<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-05-28T08:35:02.959Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/NeRF/</id>
    <published>2024-05-27T18:05:14.000Z</published>
    <updated>2024-05-28T08:35:02.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="NeRF-Casting-Improved-View-Dependent-Appearance-with-Consistent-Reflections"><a href="#NeRF-Casting-Improved-View-Dependent-Appearance-with-Consistent-Reflections" class="headerlink" title="NeRF-Casting: Improved View-Dependent Appearance with Consistent   Reflections"></a>NeRF-Casting: Improved View-Dependent Appearance with Consistent   Reflections</h2><p><strong>Authors:Dor Verbin, Pratul P. Srinivasan, Peter Hedman, Ben Mildenhall, Benjamin Attal, Richard Szeliski, Jonathan T. Barron</strong></p><p>Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render highly specular objects, whose appearance varies quickly with changes in viewpoint. Recent works have improved NeRFâ€™s ability to render detailed specular appearance of distant environment illumination, but are unable to synthesize consistent reflections of closer content. Moreover, these techniques rely on large computationally-expensive neural networks to model outgoing radiance, which severely limits optimization and rendering speed. We address these issues with an approach based on ray tracing: instead of querying an expensive neural network for the outgoing view-dependent radiance at points along each camera ray, our model casts reflection rays from these points and traces them through the NeRF representation to render feature vectors which are decoded into color using a small inexpensive network. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing shiny objects, and that it is the only existing NeRF method that can synthesize photorealistic specular appearance and reflections in real-world scenes, while requiring comparable optimization time to current state-of-the-art view synthesis models. </p><p><a href="http://arxiv.org/abs/2405.14871v1">PDF</a> Project page: <a href="http://nerf-casting.github.io">http://nerf-casting.github.io</a></p><p><strong>Summary</strong><br>NeRFæ–¹æ³•é€šè¿‡å…‰çº¿è¿½è¸ªæŠ€æœ¯è§£å†³äº†é«˜åº¦å…‰æ»‘ç‰©ä½“çš„æ¸²æŸ“é—®é¢˜ï¼Œå®ç°äº†é€¼çœŸçš„é•œé¢æ•ˆæœå’Œåå°„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFæ–¹æ³•æ”¹è¿›äº†æ¸²æŸ“è¿œå¤„ç¯å¢ƒå…‰ç…§ç»†èŠ‚çš„èƒ½åŠ›ï¼Œä½†æ— æ³•åˆæˆè¾ƒè¿‘å†…å®¹çš„ä¸€è‡´åå°„ã€‚</li><li>é‡‡ç”¨å…‰çº¿è¿½è¸ªæŠ€æœ¯ï¼Œä»ç‚¹ä¸ŠæŠ•å°„åå°„å…‰çº¿å¹¶è·Ÿè¸ªå®ƒä»¬é€šè¿‡NeRFè¡¨ç¤ºï¼Œä»¥å‘ˆç°ç‰¹å¾å‘é‡ï¼Œå¹¶ä½¿ç”¨å°å‹å»‰ä»·ç½‘ç»œå°†å…¶è§£ç ä¸ºé¢œè‰²ï¼Œè§£å†³äº†å¤§è§„æ¨¡ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å’Œæ¸²æŸ“é€Ÿåº¦å—é™çš„é—®é¢˜ã€‚</li><li>è¯¥æ¨¡å‹åœ¨åˆæˆå«æœ‰å…‰äº®ç‰©ä½“åœºæ™¯çš„è§†å›¾åˆæˆæ–¹é¢ä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œæ˜¯å”¯ä¸€å¯ä»¥åœ¨ç°å®åœºæ™¯ä¸­åˆæˆé€¼çœŸçš„é•œé¢æ•ˆæœå’Œåå°„çš„NeRFæ–¹æ³•ï¼Œä¸”æ‰€éœ€ä¼˜åŒ–æ—¶é—´ä¸å½“å‰æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ¨¡å‹ç›¸å½“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NeRF-Castingï¼šImproved View-Dependent Appearance with Consistent Reflectionsï¼ˆNeRF-Castingï¼šå…·æœ‰consistentåå°„çš„è§†å›¾ç›¸å…³å¤–è§‚æ”¹è¿›ï¼‰</p></li><li><p>Authors: DOR VERBIN, PRATUL P. SRINIVASAN, PETER HEDMAN, BEN MILDENHALL, BENJAMIN ATTAL, RICHARD SZELISKI, JONATHAN T. BARRON</p></li><li><p>Affiliation: è°·æ­Œç¾å›½</p></li><li><p>Keywords: View synthesis, neural radiance fields, reflections</p></li><li><p>Urls: <a href="https://nerf-casting.github.io">https://nerf-casting.github.io</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯Neural Radiance Fieldsï¼ˆNeRFï¼‰åœ¨è§†å›¾åˆæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å…·æœ‰é«˜é¢‘è§†å›¾ç›¸å…³å¤–è§‚çš„é•œé¢å¯¹è±¡ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä½¿ç”¨å¤§å‹ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿè§†å›¾ç›¸å…³çš„radianceï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯åªèƒ½åˆæˆè¿œè·ç¦»ç¯å¢ƒç…§æ˜çš„åå°„ï¼ŒäºŒæ˜¯è®¡ç®—å¼€é”€å¾ˆå¤§ã€‚æœ¬æ–‡çš„æ–¹æ³•motivated byè¿™äº›é—®é¢˜ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯åŸºäºray tracingçš„NeRF-Castingï¼Œé€šè¿‡castingåå°„å…‰çº¿å¹¶å°†å…¶è¿½è¸ªåˆ°NeRFè¡¨ç¤ºä¸­ï¼Œç”Ÿæˆç‰¹å¾å‘é‡ï¼Œç„¶åä½¿ç”¨å°å‹ç¥ç»ç½‘ç»œè§£ç æˆé¢œè‰²ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨è§†å›¾åˆæˆä»»åŠ¡ä¸­å–å¾—äº†state-of-the-artçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåˆæˆå…·æœ‰é«˜é¢‘è§†å›¾ç›¸å…³å¤–è§‚çš„é•œé¢å¯¹è±¡çš„åå°„ï¼Œä¸”è®¡ç®—å¼€é”€ä¸å½“å‰æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ¨¡å‹ç›¸å½“ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):è¯¥ç¯‡å·¥ä½œçš„æ„ä¹‰åœ¨äºè§£å†³äº†Neural Radiance Fieldsï¼ˆNeRFï¼‰åœ¨è§†å›¾åˆæˆä»»åŠ¡ä¸­çš„åå°„é—®é¢˜ï¼Œæé«˜äº†è§†å›¾ç›¸å…³å¤–è§‚çš„åˆæˆè´¨é‡å’Œæ•ˆç‡ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºray tracingçš„NeRF-Castingæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜é¢‘è§†å›¾ç›¸å…³å¤–è§‚çš„é•œé¢å¯¹è±¡åå°„ï¼›æ€§èƒ½ï¼šå–å¾—äº†state-of-the-artçš„è§†å›¾åˆæˆæ€§èƒ½ï¼Œèƒ½å¤Ÿåˆæˆå…·æœ‰é«˜é¢‘è§†å›¾ç›¸å…³å¤–è§‚çš„é•œé¢å¯¹è±¡åå°„ï¼›å·¥ä½œè´Ÿè½½ï¼šè®¡ç®—å¼€é”€ä¸å½“å‰æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ¨¡å‹ç›¸å½“ï¼Œå…·æœ‰è‰¯å¥½çš„å®æ—¶æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-323e45f3162c2c7c913df9dc30275d1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7da742d6de299d161600adf6fdb2df43.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46b90894aa28846d98c1eef5c5a89f0c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2faaf26739f0521731fa46fe33bfa637.jpg" align="middle"></details><h2 id="Neural-Directional-Encoding-for-Efficient-and-Accurate-View-Dependent-Appearance-Modeling"><a href="#Neural-Directional-Encoding-for-Efficient-and-Accurate-View-Dependent-Appearance-Modeling" class="headerlink" title="Neural Directional Encoding for Efficient and Accurate View-Dependent   Appearance Modeling"></a>Neural Directional Encoding for Efficient and Accurate View-Dependent   Appearance Modeling</h2><p><strong>Authors:Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi</strong></p><p>Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge. Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene. In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals. In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects. Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference. The project webpage and source code are available at: \url{<a href="https://lwwu2.github.io/nde/}">https://lwwu2.github.io/nde/}</a>. </p><p><a href="http://arxiv.org/abs/2405.14847v1">PDF</a> Accepted to CVPR 2024</p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§åä¸ºNeural Directional Encodingï¼ˆNDEï¼‰çš„è§†å›¾ç›¸å…³å¤–è§‚ç¼–ç æ–¹æ³•ï¼Œç”¨äºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¸²æŸ“é•œé¢å¯¹è±¡ï¼Œæé«˜äº†å¯¹é«˜é¢‘è§’ä¿¡å·çš„å»ºæ¨¡èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ é•œé¢å¯¹è±¡çš„æ–°è§†å›¾åˆæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œéœ€è¦è€ƒè™‘å…¨çƒç…§æ˜æ•ˆæœå’Œå…¶ä»–å¯¹è±¡çš„åå°„ã€‚<br>â€¢ æå‡ºäº†Neural Directional Encodingï¼ˆNDEï¼‰ï¼Œä¸€ç§è§†å›¾ç›¸å…³çš„å¤–è§‚ç¼–ç æ–¹æ³•ï¼Œç”¨äºNeRFæ¸²æŸ“é•œé¢å¯¹è±¡ã€‚<br>â€¢ NDEå°†ç‰¹å¾ç½‘æ ¼åŸºäºçš„ç©ºé—´ç¼–ç æ¦‚å¿µè½¬ç§»åˆ°è§’åŸŸï¼Œæé«˜äº†å¯¹é«˜é¢‘è§’ä¿¡å·çš„å»ºæ¨¡èƒ½åŠ›ã€‚<br>â€¢ NDEä½¿ç”¨è§’è¾“å…¥å’Œç©ºé—´ç‰¹å¾æ¥è·å¾—ç©ºé—´å˜åŒ–çš„æ–¹å‘ç¼–ç ï¼Œè§£å†³äº†æŒ‘æˆ˜æ€§çš„äº¤å‰åå°„æ•ˆæœã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨NDEçš„NeRFæ¨¡å‹åœ¨é•œé¢å¯¹è±¡çš„è§†å›¾åˆæˆæ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚<br>â€¢ ä½¿ç”¨å°ç½‘ç»œå¯ä»¥å®ç°å¿«é€Ÿï¼ˆå®æ—¶ï¼‰æ¨ç†ã€‚<br>â€¢ é¡¹ç›®ç½‘é¡µå’Œæºä»£ç å·²ç»å…¬å¼€ï¼Œç½‘å€ä¸º<a href="https://lwwu2.github.io/nde/ã€‚">https://lwwu2.github.io/nde/ã€‚</a></p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ç¥ç»æ–¹å‘ç¼–ç ï¼ˆNeural Directional Encodingï¼‰</p></li><li><p>Authors: Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi</p></li><li><p>Affiliation: åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡ï¼ˆUC San Diegoï¼‰</p></li><li><p>Keywords: Neural Radiance Fields, View-Dependent Appearance, Specular Objects, Novel-View Synthesis</p></li><li><p>Urls: <a href="https://lwwu2.github.io/nde/">https://lwwu2.github.io/nde/</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯æ–°è§†å›¾åˆæˆé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯ specular å¯¹è±¡çš„æ–°è§†å›¾åˆæˆï¼Œæ—¨åœ¨æ¢å¤ç‰©ä½“çš„é«˜é¢‘è§†å›¾ä¾èµ–å¤–è§‚å’Œå…¨çƒç…§æ˜æ•ˆæœã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä½¿ç”¨åˆ†æå‡½æ•°å¯¹è§†å›¾æ–¹å‘è¿›è¡Œç¼–ç ï¼Œéœ€è¦å¤§å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ï¼Œæ— æ³•æ¨¡æ‹Ÿå¤æ‚çš„åå°„æ•ˆæœã€‚è¿™äº›æ–¹æ³•ä¹Ÿå¿½è§†äº†ç©ºé—´ç‰¹å¾å¯¹è§†å›¾ä¾èµ–å¤–è§‚çš„å½±å“ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»æ–¹å‘ç¼–ç ï¼ˆNDEï¼‰æ–¹æ³•ï¼Œå°†ç‰¹å¾ç½‘æ ¼ç¼–ç æ¦‚å¿µåº”ç”¨äºè§’åº¦åŸŸï¼Œé€šè¿‡ Ù…Ø®Ø±ÙˆØ·è¿½è¸ªç©ºé—´ç‰¹å¾è·å–ç©ºé—´å˜åŒ–çš„æ–¹å‘ç¼–ç ï¼Œè§£å†³äº† interreflection æ•ˆæœçš„æŒ‘æˆ˜ã€‚</p></li><li><p>(4):æœ¬æ–‡æ–¹æ³•åœ¨åˆæˆ specular å¯¹è±¡çš„æ–°è§†å›¾ä»»åŠ¡ä¸Šå–å¾—äº† state-of-the-art çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å°å‹ç½‘ç»œå®ç°å®æ—¶æ¨ç†ï¼Œæ»¡è¶³äº†å¿«é€Ÿåˆæˆçš„éœ€æ±‚ã€‚</p></li></ul><ol><li><p>Methods:</p><ul><li><p>(1): è¯¥æ–¹æ³•ä½¿ç”¨ç¥ç»æ–¹å‘ç¼–ç ï¼ˆNDEï¼‰æ¥å¯¹ç‰¹å¾ç½‘æ ¼è¿›è¡Œè§’åº¦åŸŸçš„ç¼–ç ï¼Œé€šè¿‡Ù…Ø®Ø±ÙˆØ·è¿½è¸ªç©ºé—´ç‰¹å¾è·å–ç©ºé—´å˜åŒ–çš„æ–¹å‘ç¼–ç ã€‚</p></li><li><p>(2): NDEæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³interreflectionæ•ˆæœçš„æŒ‘æˆ˜ï¼Œæ¢å¤ç‰©ä½“çš„é«˜é¢‘è§†å›¾ä¾èµ–å¤–è§‚å’Œå…¨çƒç…§æ˜æ•ˆæœï¼Œè€Œæ— éœ€ä½¿ç”¨å¤§å‹å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚</p></li><li><p>(3): è¯¥æ–¹æ³•å…·æœ‰å®æ—¶æ¨ç†çš„èƒ½åŠ›ï¼Œå¯ä»¥ä½¿ç”¨å°å‹ç½‘ç»œå®ç°å¿«é€Ÿåˆæˆï¼Œå¹¶åœ¨åˆæˆspecularå¯¹è±¡çš„æ–°è§†å›¾ä»»åŠ¡ä¸Šå–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚</p></li></ul></li></ol><ol><li><p>Conclusion: </p><ul><li><p>(1):This piece of work is significant in advancing the field of novel-view synthesis, particularly in the synthesis of specular objects, by introducing a novel method, Neural Directional Encoding (NDE), which efficiently models complex reflections and achieves state-of-the-art performance.</p></li><li><p>(2):Innovation point: The article innovatively introduces the NDE method to efficiently model complex reflections for novel-view synthesis, addressing the limitations of previous methods.<br>Performance: The proposed method achieves state-of-the-art performance in synthesizing specular objects with the ability for real-time inference using a small network.<br>Workload: The workload is reduced as the method eliminates the need for large multi-layer perceptrons and enables real-time synthesis.</p></li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b069231775fc8a2bd10f93cb80d839ec.jpg" align="middle"><img src="https://picx.zhimg.com/v2-75b217587db527ee5663a4499270caf9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-abc9cca95d286eab225c623b7babb05b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fce7139aa953d9627454cfadef62958.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87061bf3e19ae720c7a849195745380a.jpg" align="middle"></details><h2 id="Camera-Relocalization-in-Shadow-free-Neural-Radiance-Fields"><a href="#Camera-Relocalization-in-Shadow-free-Neural-Radiance-Fields" class="headerlink" title="Camera Relocalization in Shadow-free Neural Radiance Fields"></a>Camera Relocalization in Shadow-free Neural Radiance Fields</h2><p><strong>Authors:Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available. </p><p><a href="http://arxiv.org/abs/2405.14824v1">PDF</a> Accepted by ICRA 2024. 8 pages, 5 figures, 3 tables. Codes and   dataset: <a href="https://github.com/hnrna/ShadowfreeNeRF-CameraReloc">https://github.com/hnrna/ShadowfreeNeRF-CameraReloc</a></p><p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæµæ°´çº¿ï¼Œç”¨äºè§„èŒƒå…·æœ‰ä¸åŒå…‰ç…§å’Œé˜´å½±æ¡ä»¶çš„å›¾åƒï¼Œä»¥æ”¹å–„ç›¸æœºé‡å®šä½ï¼Œå®ç°äº†åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹ç›¸æœºé‡å®šä½çš„æœ€æ–°æˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç›¸æœºé‡å®šä½åœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</li><li>è¿‘æœŸå…³äºç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºåˆæˆé€¼çœŸå›¾åƒçš„æ½œåŠ›ã€‚</li><li>ä¹‹å‰çš„å·¥ä½œåˆ©ç”¨NeRFsä¼˜åŒ–ç›¸æœºå§¿æ€ï¼Œä½†æœªè€ƒè™‘å¯èƒ½å½±å“åœºæ™¯å¤–è§‚å’Œé˜´å½±åŒºåŸŸçš„å…‰ç…§å˜åŒ–ï¼Œå¯¼è‡´å§¿æ€ä¼˜åŒ–è¿‡ç¨‹ä¸‹é™ã€‚</li><li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå“ˆå¸Œç¼–ç çš„NeRFæ¥å®ç°åœºæ™¯è¡¨ç¤ºï¼Œæ˜¾è‘—æå‡äº†å§¿æ€ä¼˜åŒ–è¿‡ç¨‹ã€‚</li><li>ä¸ºè§£å†³ç½‘æ ¼å‹NeRFä¸­çš„å™ªå£°å›¾åƒæ¢¯åº¦è®¡ç®—é—®é¢˜ï¼Œè¿›ä¸€æ­¥æå‡ºäº†é‡æ–°è®¾è®¡çš„æˆªæ–­åŠ¨æ€ä½é€šæ»¤æ³¢å™¨ï¼ˆTDLFï¼‰å’Œæ•°å€¼æ¢¯åº¦å¹³å‡æŠ€æœ¯æ¥å¹³æ»‘å¤„ç†ã€‚</li><li>åœ¨å¤šä¸ªå…·æœ‰ä¸åŒå…‰ç…§æ¡ä»¶çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„ç›¸æœºé‡å®šä½ä¸­å–å¾—äº†æœ€æ–°çš„æˆæœã€‚</li><li>ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ç›¸æœºé‡å®šä½åœ¨æ— é˜´å½±ç¥ç»è¾å°„åœºä¸­ï¼ˆCamera Relocalization in Shadow-free Neural Radiance Fieldsï¼‰</p></li><li><p>Authors: Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</p></li><li><p>Affiliation: æ¸…åå¤§å­¦äººå·¥æ™ºèƒ½äº§ä¸šç ”ç©¶é™¢</p></li><li><p>Keywords: Camera Relocalization, Neural Radiance Fields, Shadow Removal</p></li><li><p>Urls: arXiv:2405.14824v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):æœ¬æ–‡ç ”ç©¶çš„èƒŒæ™¯æ˜¯è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººå­¦é¢†åŸŸä¸­çš„ç›¸æœºé‡å®šä½é—®é¢˜ï¼Œç›®æ ‡æ˜¯ä»ç»™å®šçš„å›¾åƒä¸­æ¢å¤æ‘„åƒæœºçš„ä½å§¿ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä½¿ç”¨åˆ¤åˆ«ç½‘ç»œæˆ–NeRFæ¥refineæ‘„åƒæœºä½å§¿ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•ä¸èƒ½å¤„ç†å…‰ç…§å˜åŒ–å’Œé˜´å½±åŒºåŸŸå¯¹åœºæ™¯å¤–è§‚çš„å½±å“ï¼Œå¯¼è‡´ä½å§¿ä¼˜åŒ–è¿‡ç¨‹ä¸ç¨³å®šã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„pipelineï¼Œé¦–å…ˆä½¿ç”¨é˜´å½±ç§»é™¤ç½‘ç»œå¯¹å›¾åƒè¿›è¡Œ normalizationï¼Œç„¶åä½¿ç”¨hashç¼–ç çš„NeRFæ¥refineæ‘„åƒæœºä½å§¿ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•æ¥å¹³æ»‘ä¼˜åŒ–è¿‡ç¨‹ã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº† state-of-the-art çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨ç›¸æœºé‡å®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„pipelineï¼Œé¦–å…ˆä½¿ç”¨é˜´å½±ç§»é™¤ç½‘ç»œï¼ˆShadow Removal Networkï¼ŒNshadowï¼‰å¯¹å›¾åƒè¿›è¡Œ normalizationï¼Œå¾—åˆ°é˜´å½±-freeå›¾åƒI(l0)ã€‚</p></li><li><p>(2)ï¼šç„¶åï¼Œä½¿ç”¨hashç¼–ç çš„NeRFï¼ˆNeural Radiance Fieldsï¼‰æ¨¡å‹å¯¹é˜´å½±-freeå›¾åƒI(l0)è¿›è¡Œåœºæ™¯é‡å»ºï¼Œå¾—åˆ°ä¸‰ç»´ç¥ç»åœºæ™¯å›¾Fã€‚</p></li><li><p>(3)ï¼šåœ¨poseä¼˜åŒ–é˜¶æ®µï¼Œä½¿ç”¨åŒæ ·çš„é˜´å½±ç§»é™¤ç½‘ç»œNshadowå¯¹æµ‹è¯•å›¾åƒè¿›è¡Œé˜´å½±ç§»é™¤ï¼Œå¾—åˆ°é˜´å½±-freeæµ‹è¯•å›¾åƒI(l0)ï¼Œç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¼˜åŒ–æ‘„åƒæœºposeï¼Œç›´åˆ°æ¸²æŸ“å›¾åƒË†I(l0)ä¸é˜´å½±-freeæµ‹è¯•å›¾åƒI(l0)ä¹‹é—´çš„å…‰åº¦lossè¾¾åˆ°æœ€å°ã€‚</p></li><li><p>(4)ï¼šä¸ºäº†æé«˜poseä¼˜åŒ–çš„ç¨³å®šæ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œä½¿ç”¨numerical gradient averagingæŠ€æœ¯æ¥å¹³æ»‘ä¼˜åŒ–è¿‡ç¨‹ã€‚</p></li><li><p>(5)ï¼šåœ¨poseä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæ–‡è¿˜ä½¿ç”¨äº†ä¸€ç§ç²—åˆ°ç»†çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä½¿ç”¨truncated dynamic low-pass filterï¼ˆTDLFï¼‰æ¥åˆ†ç¦»é«˜é¢‘å’Œä½é¢‘å›¾åƒç»„ä»¶ï¼Œå¹¶é€æ¸å¢åŠ é«˜é¢‘ç»„ä»¶çš„æƒé‡ï¼Œä»¥é¿å…å±€éƒ¨æœ€ä¼˜è§£ã€‚</p></li><li><p>(6)ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†state-of-the-artçš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨ç›¸æœºé‡å®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡çš„å·¥ä½œå¯¹äºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººå­¦é¢†åŸŸä¸­çš„ç›¸æœºé‡å®šä½é—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨æ— é˜´å½±ç¥ç»è¾å°„åœºä¸­å®ç°é«˜ç²¾åº¦çš„æ‘„åƒæœºé‡å®šä½ï¼Œä»è€Œæé«˜æœºå™¨äººçš„å¯¼èˆªå’Œå®šä½èƒ½åŠ›ã€‚</p></li><li><p>(2):Innovation point: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸¤é˜¶æ®µpipelineï¼Œé¦–å…ˆä½¿ç”¨é˜´å½±ç§»é™¤ç½‘ç»œå¯¹å›¾åƒè¿›è¡Œ normalizationï¼Œç„¶åä½¿ç”¨hashç¼–ç çš„NeRFæ¨¡å‹å¯¹é˜´å½±-freeå›¾åƒè¿›è¡Œåœºæ™¯é‡å»ºï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å…‰ç…§å˜åŒ–å’Œé˜´å½±åŒºåŸŸå¯¹åœºæ™¯å¤–è§‚çš„å½±å“Performance: å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº† state-of-the-art çš„ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨ç›¸æœºé‡å®šä½ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼›Workload: æœ¬æ–‡çš„æ–¹æ³•éœ€è¦åœ¨è®­ç»ƒå’Œæµ‹è¯•é˜¶æ®µè¿›è¡Œå¤§é‡çš„è®¡ç®—å’Œä¼˜åŒ–ï¼Œéœ€è¦é«˜æ€§èƒ½çš„è®¡ç®—è®¾å¤‡å’Œå¤§é‡çš„æ•°æ®é›†æ”¯æŒã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-6d260d5b744a5039554f8c6aaee9bc01.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0ac90b20b3733ad747ec11650e963cf5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5a7748ef501582a143e2301b2e39f951.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0770bb34500dd5dd1e4632f197e96d71.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fdb4265248fa23783d77c10c673a037.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1113a2498657772fa4f4f86d7876ebfc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  NeRF-Casting Improved View-Dependent Appearance with Consistent   Reflections</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/</id>
    <published>2024-05-27T17:55:43.000Z</published>
    <updated>2024-05-28T08:35:16.157Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap"><a href="#Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap" class="headerlink" title="Feature Splatting for Better Novel View Synthesis with Low Overlap"></a>Feature Splatting for Better Novel View Synthesis with Low Overlap</h2><p><strong>Authors:T. Berriel Martins, Javier Civera</strong></p><p>3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first â€œsplattedâ€ into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. We will release the code upon acceptance.   Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting </p><p><a href="http://arxiv.org/abs/2405.15518v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨ç‰¹å¾splatteringï¼ˆFeatSplatï¼‰å°†3Dé«˜æ–¯ä½“çš„é¢œè‰²ä¿¡æ¯ç¼–ç åˆ°æ¯ä¸ªé«˜æ–¯ä½“çš„ç‰¹å¾å‘é‡ä¸­ï¼Œæé«˜äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ 3Dé«˜æ–¯splatteringåœ¨æ–°è§†å›¾åˆæˆä¸­å–å¾—äº†state-of-the-artçš„è´¨é‡ï¼Œä½†å…¶ä½¿ç”¨çƒè°å‡½æ•°è¡¨è¾¾åœºæ™¯é¢œè‰²é™åˆ¶äº†3Dé«˜æ–¯ä½“çš„è¡¨è¾¾èƒ½åŠ›ã€‚<br>â€¢ æœ¬æ–‡æå‡ºå°†é¢œè‰²ä¿¡æ¯ç¼–ç åˆ°æ¯ä¸ªé«˜æ–¯ä½“çš„ç‰¹å¾å‘é‡ä¸­ï¼Œä»¥æé«˜è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚<br>â€¢ ç‰¹å¾splatteringï¼ˆFeatSplatï¼‰æ¨¡å‹åŒ…æ‹¬é«˜æ–¯ä½“çš„splatteringã€alpha-blendingå’Œè§£ç ä¸‰ä¸ªæ­¥éª¤ã€‚<br>â€¢ æ¨¡å‹ä¸­è¿˜åŠ å…¥äº†ç›¸æœºembeddingï¼Œä»¥æ¡ä»¶è§£ç ä¹ŸåŸºäºè§†ç‚¹ä¿¡æ¯ã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒFeatSplatæ¨¡å‹æ˜¾è‘—æé«˜äº†ä½é‡å è§†å›¾çš„æ–°è§†å›¾åˆæˆè´¨é‡ã€‚<br>â€¢ FeatSplatæ¨¡å‹ä¸ä»…å¯ä»¥ç”ŸæˆRGBå€¼ï¼Œè¿˜å¯ä»¥ç”Ÿæˆæ¯åƒç´ çš„è¯­ä¹‰æ ‡ç­¾ã€‚<br>â€¢ å°†å‘å¸ƒä»£ç ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ç‰¹å¾Splatteringç”¨äºä½é‡å è§†å›¾çš„æ–°è§†å›¾åˆæˆ (Feature Splatting for Better Novel View Synthesis with Low Overlap)</p></li><li><p>Authors: Tomas Berriel Martins, Javier Civera</p></li><li><p>Affiliation: æ‰æ‹‰æˆˆè¨å¤§å­¦(I3A)</p></li><li><p>Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting</p></li><li><p>Urls: arXiv:2405.15518v1, Github:None</p></li><li><p>Summary:</p></li></ol><pre><code>- (1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¯»æ‰¾é€‚åˆä¸‰ç»´åœºæ™¯è¡¨ç¤ºï¼Œä»¥ä¾¿åœ¨æœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®åº”ç”¨ä¸­ä½¿ç”¨ã€‚- (2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬Neural Radiance Fieldsï¼ˆNeRFsï¼‰å’Œä¸‰ç»´é«˜æ–¯Splatteringï¼ˆ3DGSï¼‰ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€äº›ç¼ºé™·ï¼Œä¾‹å¦‚NeRFsè®¡ç®—å¼€é”€é«˜ã€3DGSä½¿ç”¨çƒè°å‡½æ•°è¡¨ç¤ºåœºæ™¯é¢œè‰²é™åˆ¶äº†å…¶è¡¨è¾¾èƒ½åŠ›ã€‚- (3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç‰¹å¾Splatteringï¼ˆFeatSplatï¼‰ï¼Œå®ƒå°†ä¸‰ç»´é«˜æ–¯çš„é¢œè‰²ä¿¡æ¯ç¼–ä¸ºæ¯ä¸ªé«˜æ–¯çš„ç‰¹å¾å‘é‡ï¼Œç„¶åå°†è¿™äº›å¾å‘é‡æ··åˆå¹¶è§£ç ä»¥ç”ŸæˆRGBåƒç´ å€¼ã€‚- (4):å®éªŒç»“æœè¡¨æ˜ï¼ŒFeatSplatæ–¹æ³•å¯ä»¥æ˜¾è‘—æ”¹å–„ä½é‡å è§†å›¾çš„æ–°è§†å›¾åˆæˆæ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆæ¯åƒç´ çš„è¯­ä¹‰æ ‡ç­¾ï¼Œä»¥æ”¯æŒæœºå™¨äººç­‰åº”ç”¨ã€‚</code></pre><ol><li>Conclusion: </li></ol><ul><li><p>(1):æœ¬æ–‡çš„å·¥ä½œå¯¹äºä¸‰ç»´åœºæ™¯è¡¨ç¤ºå’Œæ–°è§†å›¾åˆæˆå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥åº”ç”¨äºæœºå™¨äººã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸã€‚</p></li><li><p>(2):Innovation point: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾Splatteringï¼ˆFeatSplatï¼‰æ–¹æ³•ï¼Œå¼¥è¡¥äº†Neural Radiance Fieldsï¼ˆNeRFsï¼‰å’Œä¸‰ç»´é«˜æ–¯Splatteringï¼ˆ3DGSï¼‰çš„ä¸è¶³ä¹‹å¤„ï¼› Performance: FeatSplatæ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆæ¯åƒç´ çš„è¯­ä¹‰æ ‡ç­¾ï¼› Workload: æœ¬æ–‡çš„æ–¹æ³•è®¡ç®—å¼€é”€ç›¸å¯¹è¾ƒä½ï¼Œé€‚åˆå®æ—¶åº”ç”¨ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-af9ac9b1d0d353f31971a8ace9ae132b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-eaee1c783ee42cdf998fdd81f98539e2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-922abaae68f73855cac3e6cd2f6fb3d0.jpg" align="middle"></details><h2 id="HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting"><a href="#HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting" class="headerlink" title="HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed   via Gaussian Splatting"></a>HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed   via Gaussian Splatting</h2><p><strong>Authors:Yuanhao Cai, Zihao Xiao, Yixun Liang, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</strong></p><p>High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and only requiring 6.3% training time. </p><p><a href="http://arxiv.org/abs/2405.15125v1">PDF</a> The first 3D Gaussian Splatting-based method for HDR imaging</p><p><strong>Summary</strong><br>æå‡ºé«˜åŠ¨æ€èŒƒå›´Gaussian Splattingï¼ˆHDR-GSï¼‰æ¡†æ¶ï¼Œå®ç°é«˜æ•ˆ novel view synthesis å’Œæ›å…‰æ—¶é—´å¯æ§çš„ä½åŠ¨æ€èŒƒå›´å›¾åƒé‡å»ºã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ é«˜åŠ¨æ€èŒƒå›´ novel view synthesisï¼ˆHDR NVSï¼‰æ—¨åœ¨ä½¿ç”¨HDRæˆåƒæŠ€æœ¯ä»æ–°è§†ç‚¹ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚<br>â€¢ ç°æœ‰çš„HDR NVSæ–¹æ³•ä¸»è¦åŸºäºNeRFï¼Œå­˜åœ¨é•¿è®­ç»ƒæ—¶é—´å’Œæ…¢æ¨ç†é€Ÿåº¦çš„é—®é¢˜ã€‚<br>â€¢ æœ¬æ–‡æå‡ºé«˜åŠ¨æ€èŒƒå›´Gaussian Splattingï¼ˆHDR-GSï¼‰æ¡†æ¶ï¼Œå®ç°é«˜æ•ˆ novel view synthesis å’Œæ›å…‰æ—¶é—´å¯æ§çš„ä½åŠ¨æ€èŒƒå›´å›¾åƒé‡å»ºã€‚<br>â€¢ HDR-GSä½¿ç”¨åŒåŠ¨æ€èŒƒå›´ï¼ˆDDRï¼‰é«˜æ–¯ç‚¹äº‘æ¨¡å‹å’ŒåŸºäºMLPçš„tone-mapperæ¥æ¸²æŸ“HDRå’ŒLDRé¢œè‰²ã€‚<br>â€¢ è¯¥æ–¹æ³•åœ¨LDRå’ŒHDR NVSä»»åŠ¡ä¸Šè¶…è¿‡åŸºäºNeRFçš„æ–¹æ³•ï¼Œä¸”å…·æœ‰1000å€çš„æ¨ç†é€Ÿåº¦å’Œä»…éœ€6.3%çš„è®­ç»ƒæ—¶é—´<br>â€¢ å®éªŒç»“æœè¡¨æ˜HDR-GSåœ¨HDR NVSä»»åŠ¡ä¸Šå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚<br>â€¢ æœ¬æ–‡ä¸ºåŸºäº3Dé«˜æ–¯splatttingçš„HDR NVSæ–¹æ³•å¥ å®šäº†æ•°æ®åŸºç¡€ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: é«˜åŠ¨æ€èŒƒå›´æ–°è§†å›¾åˆæˆï¼ˆHDR-GSï¼‰ï¼šåŸºäºé«˜æ–¯æŠ¹é™¤çš„é«˜æ•ˆHDRæ–°è§†å›¾åˆæˆï¼ˆHigh Dynamic Range Gaussian Splatting: Efficient HDR Novel View Synthesis via Gaussian Splattingï¼‰</p></li><li><p>Authors: Yuanhao Cai, Zihao Xiao, Yixun Liang, Minghan Qin, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</p></li><li><p>Affiliation: çº¦ç¿°æ–¯Â·éœæ™®é‡‘æ–¯å¤§å­¦</p></li><li><p>Keywords: é«˜åŠ¨æ€èŒƒå›´, æ–°è§†å›¾åˆæˆ, é«˜æ–¯æŠ¹é™¤, Novel View Synthesis, HDR, Gaussian Splatting</p></li><li><p>Urls: <a href="https://arxiv.org/abs/2405.15125v1">https://arxiv.org/abs/2405.15125v1</a>, Github: <a href="https://github.com/caiyuanhao1998/HDR-GS">https://github.com/caiyuanhao1998/HDR-GS</a></p></li><li><p>Summary:</p></li></ol><ul><li>(1):æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰ï¼Œæ—¨åœ¨ä½¿ç”¨HDRæˆåƒæŠ€æœ¯ä»æ–°è§†ç‚¹ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚</li></ul><ul><li>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºNeRFï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨é•¿è®­ç»ƒæ—¶é—´å’Œæ…¢æ¨ç†é€Ÿåº¦çš„é—®é¢˜ã€‚</li></ul><ul><li>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯High Dynamic Range Gaussian Splattingï¼ˆHDR-GSï¼‰ï¼Œå®ƒä½¿ç”¨åŒåŠ¨æ€èŒƒå›´ï¼ˆDDRï¼‰é«˜æ–¯ç‚¹äº‘æ¨¡å‹å’Œå¹³è¡Œå¯å¾®åˆ†å…‰æ …åŒ–ï¼ˆPDRï¼‰è¿‡ç¨‹æ¥é«˜æ•ˆåœ°æ¸²æŸ“HDRå’ŒLDRè§†å›¾ã€‚</li></ul><ul><li>(4):æœ¬æ–‡æ–¹æ³•åœ¨HDRå’ŒLDRæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šä¼˜äºåŸºäºNeRFçš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†3.84å’Œ1.91 dBçš„PSNRæ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰1000å€çš„æ¨ç†é€Ÿåº¦å’Œä»…éœ€6.3%çš„è®­ç»ƒæ—¶é—´</li></ul><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1):æå‡ºåŒåŠ¨æ€èŒƒå›´ï¼ˆDDRï¼‰é«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼Œç”¨äºè¡¨ç¤ºé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒçš„é¢œè‰²å’Œæ·±åº¦ä¿¡æ¯ï¼Œè¯¥æ¨¡å‹ç”±é«˜æ–¯åˆ†å¸ƒå‡½æ•°å’Œç‚¹äº‘æ•°æ®ç»„æˆã€‚</p></li><li><p>(2):ä½¿ç”¨å¹³è¡Œå¯å¾®åˆ†å…‰æ …åŒ–ï¼ˆPDRï¼‰è¿‡ç¨‹å°†DDRé«˜æ–¯ç‚¹äº‘æ¨¡å‹è½¬æ¢ä¸ºé«˜æ•ˆçš„æ¸²æŸ“è¡¨ç¤ºï¼Œä»¥ä¾¿å¿«é€Ÿç”ŸæˆHDRå’ŒLDRè§†å›¾ã€‚</p></li><li><p>(3):è®¾è®¡é«˜æ–¯æŠ¹é™¤ï¼ˆGaussian Splattingï¼‰ç®—æ³•ï¼Œç”¨äºå°†DDRé«˜æ–¯ç‚¹äº‘æ¨¡å‹æŠ•å½±åˆ°ç›®æ ‡è§†å›¾å¹³é¢ä¸Šï¼Œç”Ÿæˆé«˜è´¨é‡çš„HDRå’ŒLDRå›¾åƒã€‚</p></li><li><p>(4):æå‡ºåŸºäºé«˜æ–¯æŠ¹é™¤çš„æ–°è§†å›¾åˆæˆï¼ˆNovel View Synthesisï¼‰æ–¹æ³•ï¼Œç”¨äºä»ç»™å®šçš„HDRå›¾åƒä¸­ç”Ÿæˆæ„è§†ç‚¹çš„HDRå’ŒLDRå›¾åƒã€‚</p></li><li><p>(5):ä½¿ç”¨åŸºäºNeRFçš„æ–¹æ³•ä½œä¸ºåŸºçº¿ï¼Œæ¯”è¾ƒHDR-GSæ–¹æ³•åœ¨HDRå’ŒLDRæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜HDR-GSæ–¹æ³•å…·æœ‰æ›´é«˜çš„PSNRæ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</p></li><li><p>(6):é€šè¿‡å®éªŒéªŒè¯HDR-GSæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œç»“æœè¡¨æ˜HDR-GSæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„HDRå’ŒLDRå›¾åƒï¼Œå¹¶ä¸”å…·æœ‰å®æ—¶æ¸²æŸ“çš„èƒ½åŠ›ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):è¯¥ç ”ç©¶å·¥ä½œçš„é‡è¦æ€§åœ¨äºè§£å†³äº†é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰æ–°è§†å›¾åˆæˆä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œå®ç°äº†é«˜è´¨é‡çš„HDRå›¾åƒæ¸²æŸ“å’Œå¿«é€Ÿæ¨ç†é€Ÿåº¦ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯åœ¨è®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæœºå™¨å­¦ä¹ ç­‰é¢†åŸŸã€‚</li></ul><ul><li>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æŠ¹é™¤çš„é«˜æ•ˆHDRæ–°è§†å›¾åˆæˆæ–¹æ³•HDR-GSï¼Œè§£å†³äº†åŸºäºNeRFæ–¹æ³•çš„é•¿è®­ç»ƒæ—¶é—´å’Œæ…¢æ¨ç†é€Ÿåº¦é—®é¢˜ï¼›æ€§èƒ½ï¼šåœ¨HDRå’ŒLDRæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šï¼ŒHDR-GSæ–¹æ³•å…·æœ‰æ›´é«˜çš„PSNRæ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼›å·¥ä½œé‡ï¼šHDR-GSæ–¹æ³•ä»…éœ€6.3%çš„è®­ç»ƒæ—¶é—´å’Œ1000å€çš„æ¨ç†é€Ÿåº¦ï¼Œå…·æœ‰å®æ—¶æ¸²æŸ“çš„èƒ½åŠ›ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-62274faaed9878e5e0161dea6f18dbbe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1eb56bf3e6d513a6248b50e7a8d0c539.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6cf6e245e96bb903d2b486b7727c24e.jpg" align="middle"></details><h2 id="GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting"><a href="#GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting" class="headerlink" title="GS-Hider: Hiding Messages into 3D Gaussian Splatting"></a>GS-Hider: Hiding Messages into 3D Gaussian Splatting</h2><p><strong>Authors:Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</strong></p><p>3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGSâ€™s spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: <a href="https://xuanyuzhang21.github.io/project/gshider">https://xuanyuzhang21.github.io/project/gshider</a>. </p><p><a href="http://arxiv.org/abs/2405.15118v1">PDF</a> 3DGS steganography</p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯åˆ†è£‚ï¼ˆ3DGSï¼‰éšå†™æœ¯æ¡†æ¶GS-Hiderï¼Œå®ç°äº†å¯¹åŸå§‹3DGSç‚¹äº‘æ–‡ä»¶çš„éšå†™å’Œæå–ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ 3DGSéœ€è¦ä¿æŠ¤ç‰ˆæƒã€å®Œæ•´æ€§å’Œéšç§ï¼Œå› ä¸ºè®­ç»ƒéœ€è¦å¤§é‡æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚<br>â€¢ 3DGSå…·æœ‰æ˜¾å¼3Dè¡¨ç¤ºå’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œå¯¼è‡´ç‚¹äº‘æ–‡ä»¶å…¬å¼€é€æ˜ï¼Œå…·æœ‰æ˜ç¡®çš„ç‰©ç†æ„ä¹‰ã€‚<br>â€¢ GS-Hideræ¡†æ¶å¯ä»¥å°†3Dåœºæ™¯å’Œå›¾åƒåµŒå…¥åˆ°åŸå§‹GSç‚¹äº‘ä¸­ï¼Œä»¥ä¸å¯è§çš„æ–¹å¼æå–éšè—çš„æ¶ˆæ¯ã€‚<br>â€¢ GS-Hiderä½¿ç”¨è€¦åˆå®‰å…¨ç‰¹å¾å±æ€§æ›¿æ¢åŸå§‹3DGSçš„çƒè°ç³»æ•°ï¼Œå¹¶ä½¿ç”¨åœºæ™¯è§£ç å™¨å’Œæ¶ˆè§£ç å™¨æ¥åˆ†ç¦»åŸå§‹RGBåœºæ™¯å’Œéšè—æ¶ˆæ¯ã€‚<br>â€¢ å®éªŒè¡¨æ˜ï¼ŒGS-Hiderå¯ä»¥æœ‰æ•ˆåœ°éšè—å¤šæ¨¡å¼æ¶ˆæ¯ï¼Œè€Œä¸å½±å“æ¸²æŸ“è´¨é‡ï¼Œå…·æœ‰å¼‚å¸¸çš„å®‰å…¨æ€§ã€é²æ£’æ€§ã€å®¹é‡å’Œçµæ´»æ€§ã€‚<br>â€¢ GS-Hideré¡¹ç›®å¯åœ¨<a href="https://xuanyuzhang21.github.io/project/gshiderä¸Šè®¿é—®ã€‚">https://xuanyuzhang21.github.io/project/gshiderä¸Šè®¿é—®ã€‚</a><br>â€¢ GS-Hideræ¡†æ¶å¯ä»¥ä¿æŠ¤3DGSçš„ç‰ˆæƒã€å®Œæ•´æ€§å’Œéšç§ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GS-Hiderï¼šéšè—æ¶ˆæ¯åˆ°3Dé«˜æ–¯ç‚¹äº‘ï¼ˆGS-Hider: Hiding Messages into 3D Gaussian Splattingï¼‰</p></li><li><p>Authors: Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</p></li><li><p>Affiliation: ç”µå­ä¸è®¡ç®—æœºå·¥ç¨‹å­¦é™¢ï¼ŒåŒ—äº¬å¤§å­¦ï¼ˆSchool of Electronic and Computer Engineering, Peking Universityï¼‰</p></li><li><p>Keywords: 3Dé«˜æ–¯ç‚¹äº‘ã€éšå†™æœ¯ã€æ•°å­—æ°´å°ã€copyright protection</p></li><li><p>Urls: <a href="https://arxiv.org/abs/2405.15118">https://arxiv.org/abs/2405.15118</a>, Github: <a href="https://xuanyuzhang21.github.io/project/gshider/">https://xuanyuzhang21.github.io/project/gshider/</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ä¿æŠ¤3Dåœºæ™¯é‡å»ºå’Œæ–°è§†å›¾åˆæˆä¸­çš„æ•°å­—èµ„äº§çš„ç‰ˆæƒå’Œéšç§ï¼Œç‰¹åˆ«æ˜¯åŸºäº3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ã€‚</p></li><li><p>(2):è¿‡å»çš„éšå†™æœ¯æ–¹æ³•ä¸»è¦ä½¿ç”¨å‚…é‡Œå¶å’Œå°æ³¢å˜æ¢æ¥åµŒå…¥æ¶ˆæ¯ï¼Œä½†æ˜¯è¿™äº›æ–¹æ³•ä¸èƒ½å¾ˆå¥½åœ°é€‚åº”3DGSçš„ç‰¹ç‚¹ï¼Œä¾‹å¦‚æ˜ç¡®çš„3Dè¡¨ç¤ºå’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºGS-Hiderçš„éšå†™æœ¯æ¡†æ¶ï¼Œä½¿ç”¨è€¦åˆçš„å®‰å…¨ç‰¹å¾å±æ€§æ¥æ›¿æ¢åŸå§‹3DGSçš„çƒè°ç³»æ•°ï¼Œç„¶åä½¿ç”¨åœºæ™¯è§£ç å™¨å’Œæ¶ˆæ¯è§£ç å™¨æ¥åˆ†ç¦»åŸå§‹RGBåœºæ™¯å’Œéšè—çš„æ¶ˆæ¯ã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼ŒGS-Hiderå¯ä»¥åœ¨ä¸å½±å“æ¸²æŸ“è´¨é‡çš„æƒ…å†µä¸‹éšè—å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œå¹¶ä¸”å…·æœ‰éå¸¸é«˜çš„å®‰å…¨æ€§ã€é²æ£’æ€§ã€å®¹é‡å’Œçµæ´»æ€§ã€‚</p></li></ul><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šé¦–å…ˆï¼Œä½œè€…ä»¬æå‡ºäº†åŸºäºè€¦åˆå®‰å…¨ç‰¹å¾å±æ€§çš„éšå†™æœ¯æ¡†æ¶GS-Hiderï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†æ¶ˆæ¯éšè—åœ¨3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ä¸­ã€‚</p></li><li><p>(2)ï¼šåœ¨GS-Hideræ¡†æ¶ä¸­ï¼Œä½œè€…ä»¬ä½¿ç”¨è€¦åˆçš„å®‰å…¨ç‰¹å¾å±æ€§æ¥æ›¿æ¢åŸå§‹3DGSçš„çƒè°ç³»æ•°ï¼Œå…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å°†æ¶ˆæ¯åµŒå…¥åˆ°çƒè°ç³»æ•°ä¸­ã€‚</p></li><li><p>(3)ï¼šç„¶åï¼Œä½œè€…ä»¬ä½¿ç”¨åœºæ™¯è§£ç å™¨å’Œæ¶ˆæ¯è§£ç å™¨æ¥åˆ†ç¦»åŸå§‹RGBåœºæ™¯å’Œéšè—çš„æ¶ˆæ¯ï¼Œè¿™ä¸¤ä¸ªè§£ç å™¨éƒ½æ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œã€‚</p></li><li><p>(4)ï¼šåœ¨æ¶ˆæ¯åµŒå…¥è¿‡ç¨‹ä¸­ï¼Œä½œè€…ä»¬ä½¿ç”¨äº†anisotropic Gaussiansè¡¨ç¤ºåœºæ™¯ï¼Œé€šè¿‡splattinæŠ€æœ¯å°†3Dé«˜æ–¯ç‚¹äº‘æŠ•å½±åˆ°å›¾åƒå¹³é¢ä¸Šï¼Œå¹¶ä½¿ç”¨ç»ç‚¹åŸºäºæ¸²æŸ“æ¥ç”Ÿæˆå›¾åƒã€‚</p></li><li><p>(5)ï¼šä¸ºäº†æé«˜æ¶ˆæ¯çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ï¼Œä½œè€…ä»¬ä½¿ç”¨äº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬DIFFusion-basedæ–¹æ³•å’ŒFrequency-basedæ–¹æ³•æ¥ä¿æŠ¤æ¶ˆæ¯æŠµæŠ—æ”»å‡»ã€‚</p></li><li><p>(6)ï¼šåœ¨å®éªŒä¸­ï¼Œä½œè€…ä»¬ä½¿ç”¨äº†å¤šç§æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°GS-Hiderçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜GS-Hiderå¯ä»¥åœ¨ä¸å½±å“æ¸²æŸ“è´¨é‡çš„æƒ…å†µä¸‹éšè—å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œå¹¶ä¸”å…·æœ‰éå¸¸é«˜çš„å®‰å…¨æ€§ã€é²æ£’æ€§ã€å®¹é‡å’Œçµæ´»æ€§ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): æœ¬æ–‡çš„å·¥ä½œæ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§é«˜ä¿çœŸã€å®‰å…¨ã€å¤§å®¹é‡å’Œå¤šåŠŸèƒ½çš„3Dé«˜æ–¯ç‚¹äº‘éšå†™æœ¯æ¡†æ¶ï¼Œå³GS-Hiderï¼Œä¸ºä¿æŠ¤3Dåœºæ™¯é‡å»ºå’Œæ–°è§†å›¾åˆæˆä¸­çš„æ•°å­—èµ„äº§ç‰ˆæƒå’Œéšç§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ”¯æŒã€‚</p></li><li><p>(2): åˆ›æ–°ç‚¹ï¼šGS-Hideræ¡†æ¶åˆ©ç”¨è€¦åˆçš„å®‰å…¨ç‰¹å¾è¡¨ç¤ºå’ŒåŒè§£ç å™¨è§£ç æŠ€æœ¯ï¼Œå®ç°äº†åœ¨3Dé«˜æ–¯ç‚¹äº‘ä¸­éšè—æ¶ˆæ¯ï¼Œå…·æœ‰å¾ˆé«˜çš„å®‰å…¨æ€§ã€é²æ£’æ€§å’Œçµæ´»æ€§ï¼›æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜GS-Hideråœ¨ä¸å½±å“æ¸²æŸ“è´¨é‡çš„æƒ…å†µä¸‹å¯ä»¥éšè—å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œä¸”å…·æœ‰é«˜å®¹é‡ï¼›å·¥ä½œé‡ï¼šæ–‡ç« æœªè¯¦ç»†è¯´æ˜å…·ä½“çš„å·¥ä½œé‡è¯„ä¼°ï¼Œéœ€è¦è¿›ä¸€æ­¥è¡¥å……å’Œå®Œå–„ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-44535b4dc9ae919b2dce80a4be050e9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbb3c977263acb314ebe7c8c3a9043c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7d4ae3f321d6e860ec2da2743463f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51183cc617b206934e4fdaaba05fdc46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c5422ed30935cd238fd580f363ae7ec2.jpg" align="middle"></details><h2 id="DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus"><a href="#DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus" class="headerlink" title="DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D   Reconstruction Via Gaussian Consensus"></a>DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D   Reconstruction Via Gaussian Consensus</h2><p><strong>Authors:Yu Chen, Gim Hee Lee</strong></p><p>The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into K blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DoGaussian maintains one global 3DGS model on the master node and K local 3DGS models on the slave nodes. The K local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our project page is available at <a href="https://aibluefisher.github.io/DoGaussian">https://aibluefisher.github.io/DoGaussian</a>. </p><p><a href="http://arxiv.org/abs/2405.13943v1">PDF</a> </p><p><strong>Summary</strong><br>æœ€è¿‘å¯¹3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰çš„ç ”ç©¶æ˜¾ç¤ºäº†åœ¨æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººæœŸå¾…çš„æˆæœã€‚ </p><p><strong>Key Takeaways</strong></p><ul><li>3DGSåœ¨æ¸²æŸ“æ€§èƒ½å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œä¼˜äºä»¥å¾€çš„NeRFæ–¹æ³•ã€‚</li><li>æœ€è¿‘çš„3DGSæ–¹æ³•è¦ä¹ˆä¸“æ³¨äºæ”¹å–„æ¸²æŸ“æ•ˆç‡çš„ä¸ç¨³å®šæ€§ï¼Œè¦ä¹ˆå‡å°æ¨¡å‹å°ºå¯¸ã€‚</li><li>æœ¬æ–‡æå‡ºäº†DoGaussianæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†åœºæ™¯åˆ†è§£ä¸ºKä¸ªå—ï¼Œå¹¶å¼•å…¥äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰æ¥åˆ†å¸ƒå¼è®­ç»ƒ3DGSã€‚</li><li>DoGaussianæ–¹æ³•é€šè¿‡åœºæ™¯åˆ†è§£ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶ç¡®ä¿äº†è®­ç»ƒçš„æ”¶æ•›æ€§å’Œç¨³å®šæ€§ã€‚</li><li>è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†6å€ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>é¡¹ç›®é¡µé¢é“¾æ¥ï¼š<a href="https://aibluefisher.github.io/DoGaussianã€‚">https://aibluefisher.github.io/DoGaussianã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: DoGaussianï¼šåˆ†å¸ƒå¼é¢å‘é«˜æ–¯æ–¯æ™®æ‹‰ç‰¹ï¼ˆDistributed-Oriented Gaussian Splattingï¼‰</li></ol><ol><li>Authors: Yu Chen, Gim Hee Lee</li></ol><ol><li>Affiliation: æ–°åŠ å¡å›½ç«‹å¤§å­¦</li></ol><ol><li>Keywords: 3D Gaussian Splatting, Novel View Synthesis, Distributed Training</li></ol><ol><li>Urls: <a href="https://arxiv.org/abs/2405.13943v1">https://arxiv.org/abs/2405.13943v1</a>, Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):è¿‘å¹´æ¥ï¼Œä¸‰ç»´é«˜æ–¯æ–¯æ™®æ‹‰ç‰¹ï¼ˆ3DGSï¼‰åœ¨æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œç„¶è€Œï¼Œå½“å‰3DGSæ–¹æ³•çš„è®­ç»ƒæ•ˆç‡åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹å°šæœªå—åˆ°è¶³å¤Ÿçš„å…³æ³¨ã€‚- (2):ä¹‹å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æé«˜æ¸²æŸ“æ•ˆç‡çš„ä¸ç¨³å®šæ€§æˆ–å‡å°‘æ¨¡å‹å¤§å°ï¼Œä½†è¿™äº›æ–¹æ³•å¿½è§†äº†å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„è®­ç»ƒæ•ˆç‡é—®é¢˜ã€‚- (3):æœ¬æ–‡æå‡ºäº†DoGaussianæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†åœºæ™¯åˆ†è§£æˆKä¸ªå—ï¼Œç„¶åå¼•å…¥äº¤æ›¿æ–¹å‘ä¹˜å­æ³•ï¼ˆADMMï¼‰åˆ°3DGSçš„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDoGaussianåœ¨ä¸»èŠ‚ç‚¹ä¸Šç»´æŠ¤ä¸€ä¸ªå…¨å±€çš„3DGSæ¨¡å‹ï¼Œåœ¨ä»èŠ‚ç‚¹ä¸Šç»´æŠ¤Kä¸ªå±€éƒ¨çš„3DGSæ¨¡å‹- (4):DoGaussianæ–¹åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹åŠ é€Ÿäº†3DGSçš„è®­ç»ƒé€Ÿåº¦ï¼Œè¾¾åˆ°äº†6å€ä»¥ä¸Šçš„åŠ é€Ÿï¼ŒåŒæ—¶ä¹Ÿè·å¾—äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</code></pre><ol><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šå°†åœºæ™¯åˆ†è§£æˆ K ä¸ªå—ï¼Œä»¥ä¾¿åˆ†å¸ƒå¼è®­ç»ƒã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼Œåˆ†é…è®­ç»ƒè§†å›¾å’Œç‚¹äº‘æ•°æ®ã€‚</p></li><li><p>(2)ï¼šå¼•å…¥ Alternating Direction Method of Multipliersï¼ˆADMMï¼‰ç®—æ³•ï¼Œåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å®ç°å…¨å±€ä¸€è‡´çš„ 3D Gaussian Splatting æ¨¡å‹ã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼Œç»´æŠ¤ä¸€ä¸ªå±€éƒ¨çš„ 3D Gaussian Splatting æ¨¡å‹ï¼Œå¹¶ä¸ä¸»èŠ‚ç‚¹ä¸Šçš„å…¨å±€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚</p></li><li><p>(3)ï¼šåœ¨æ¯ä¸ªå—ä¸­ï¼Œä½¿ç”¨ ADMM ç®—æ³•æ›´æ–°å±€éƒ¨æ¨¡å‹ï¼Œå¹¶å°†æ›´æ–°åçš„æ¨¡å‹ä¸ä¸»èŠ‚ç‚¹ä¸Šçš„å…¨å±€æ¨¡å‹è¿›è¡Œå¹³å‡ï¼Œä»¥å®ç°æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚</p></li><li><p>(4)ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ Penalty Parameter å’Œ Over-relaxation æŠ€æœ¯æ¥æé«˜ ADMM ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚</p></li><li><p>(5)ï¼šä½¿ç”¨åœºæ™¯åˆ†å‰²ç®—æ³•ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå—çš„å¤§å°ç›¸ä¼¼ï¼Œå¹¶ä¸”ç›¸é‚»å—ä¹‹é—´æœ‰è¶³å¤Ÿçš„é‡å åŒºåŸŸï¼Œä»¥ä¿ƒè¿›è®­ç»ƒçš„æ”¶æ•›ã€‚</p></li><li><p>(6)ï¼šåœ¨è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨å…¨å±€æ¨¡å‹æ¥åˆæˆæ–°è§†å›¾ï¼Œä»¥å®ç°é«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚</p></li><li><p>(7)ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ DoGaussian æ–¹æ³•å¯ä»¥åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹åŠ é€Ÿ 3D Gaussian Splatting çš„è®­ç»ƒé€Ÿåº¦ï¼Œè¾¾åˆ°äº† 6 å€ä»¥ä¸Šçš„åŠ é€Ÿï¼ŒåŒæ—¶ä¹Ÿè·å¾—äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚</p></li></ul><ol><li>Conclusion: </li></ol><ul><li><p>(1):æœ¬æ–‡çš„è´¡çŒ®åœ¨äºè§£å†³äº†ä¸‰ç»´é«˜æ–¯æ–¯æ™®æ‹‰ç‰¹ï¼ˆ3DGSï¼‰åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„è®­æ•ˆç‡é—®é¢˜ï¼Œæé«˜äº†æ–°è§†å›¾åˆæˆï¼ˆNVSï¼‰çš„å®æ—¶æ€§å’Œè´¨é‡ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ³•DoGaussianï¼Œä½¿ç”¨Alternating Direction Method of Multipliersï¼ˆADMMï¼‰ç®—æ³•å®ç°å…¨å±€ä¸€è‡´çš„3DGSæ¨¡å‹ï¼›æ€§èƒ½ï¼šåŠ é€Ÿäº†3DGSçš„è®­ç»ƒé€Ÿåº¦ï¼Œè¾¾åˆ°äº†6å€ä»¥ä¸Šçš„åŠ é€Ÿï¼ŒåŒæ—¶ä¹Ÿè·å¾—äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼›å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œåœºæ™¯åˆ†å‰²ç®—æ³•æ¥å®ç°åˆ†å¸ƒå¼è®­ç»ƒã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-22c8c9dbbe8897a84779859d7460a6eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-261a3638b92396cc85c1385cc6c53581.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e3e352a0325ce88ecaee52f7e182708.jpg" align="middle"></details><h2 id="Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances"><a href="#Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances" class="headerlink" title="Gaussian Time Machine: A Real-Time Rendering Methodology for   Time-Variant Appearances"></a>Gaussian Time Machine: A Real-Time Rendering Methodology for   Time-Variant Appearances</h2><p><strong>Authors:Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</strong></p><p>Recent advancements in neural rendering techniques have significantly enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D Gaussian Splatting (3DGS) has marked a significant milestone by adopting a discrete scene representation, facilitating efficient training and real-time rendering. Several studies have successfully extended the real-time rendering capability of 3DGS to dynamic scenes. However, a challenge arises when training images are captured under vastly differing weather and lighting conditions. This scenario poses a challenge for 3DGS and its variants in achieving accurate reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown promise in handling such challenging conditions, their computational demands hinder real-time rendering capabilities. In this paper, we present Gaussian Time Machine (GTM) which models the time-dependent attributes of Gaussian primitives with discrete time embedding vectors decoded by a lightweight Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives, we can reconstruct visibility changes of objects. We further propose a decomposed color model for improved geometric consistency. GTM achieved state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles the appearance changes and renders smooth appearance interpolation. </p><p><a href="http://arxiv.org/abs/2405.13694v1">PDF</a> 14 pages, 6 figures</p><p><strong>Summary</strong><br>åˆ©ç”¨é«˜æ–¯æ—¶é—´æœºGTMå®ç°å®æ—¶ä¸‰ç»´é‡å»ºï¼Œè§£å†³weatherå’Œlightingæ¡ä»¶å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ ä¸‰ç»´é«˜æ–¯Splattingï¼ˆ3DGSï¼‰æŠ€æœ¯çš„å‡ºç°æ ‡å¿—ç€ä¸‰ç»´é‡å»ºçš„é‡è¦é‡Œç¨‹ç¢‘ã€‚<br>â€¢ 3DGSåŠå…¶å˜ä½“åœ¨å®æ—¶æ¸²æŸ“åŠ¨æ€åœºæ™¯æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†æ˜¯åœ¨ä¸åŒå¤©æ°”å’Œç…§æ˜æ¡ä»¶ä¸‹è®­ç»ƒå›¾åƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚<br>â€¢ NeRF-basedæ–¹æ³•ï¼ˆNeRF-Wã€CLNeRFï¼‰å¯ä»¥å¤„ç†è¿™ç§æŒ‘æˆ˜ï¼Œä½†è®¡ç®—éœ€æ±‚é«˜ï¼Œå½±å“å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚<br>â€¢ é«˜æ–¯æ—¶é—´æœºGTMä½¿ç”¨è½»é‡çº§MLPæ¨¡å‹æ—¶é—´åµŒå…¥çŸ¢é‡æ¥æ¨¡æ‹Ÿé«˜æ–¯primitiveçš„æ—¶é—´ä¾èµ–å±æ€§ã€‚<br>â€¢ GTMå¯ä»¥é‡å»ºå¯¹è±¡çš„å¯è§æ€§å˜åŒ–ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„å‡ ä½•ä¸€è‡´æ€§ã€‚<br>â€¢ GTMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æ¸²æŸ“ä¿çœŸåº¦è¾¾åˆ°æœ€å¥½ï¼Œå¹¶ä¸”æŸ“é€Ÿåº¦æ˜¯NeRF-basedæ–¹æ³•çš„100å€ã€‚<br>â€¢ GTMæˆåŠŸåœ°åˆ†ç¦»äº†å¤–è§‚å˜åŒ–ï¼Œå¹¶å®ç°äº†å¹³æ»‘çš„å¤–è§‚æ’å€¼ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: é«˜æ–¯æ—¶é—´æœºå™¨ï¼šå®æ—¶æ¸²æŸ“æ—¶é—´å˜æ¢å¤–è§‚ (Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances)</li></ol><ol><li>Authors: Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</li></ol><ol><li>Affiliation: æ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢</li></ol><ol><li>Keywords: Neural Rendering Â· 3D Gaussian Splatting Â· Varying Appearance</li></ol><ol><li>Urls: arXiv:2405.13694v1, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):è¿‘å¹´æ¥ï¼Œç¥ç»æ¸²æŸ“æŠ€æœ¯çš„å‘å±•æå¤§åœ°æé«˜äº†ä¸‰ç»´é‡å»ºçš„ä¿çœŸåº¦ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸‰ç»´é«˜æ–¯ç‚¹ç»˜åˆ¶ï¼ˆ3DGSï¼‰æå‡ºäº†ç¦»æ•£åœºæ™¯è¡¨ç¤ºï¼Œæé«˜äº†è®­ç»ƒé€Ÿåº¦å’Œå®æ—¶æ¸²æŸ“è´¨é‡ã€‚- (2):è¿‡å»çš„æ–¹æ³•å¦‚NeRF-Wå’ŒCLNeRFå¯ä»¥å¤„ç†å¤æ‚çš„å¤©æ°”å’Œç…§æ˜æ¡ä»¶ï¼Œä½†æ˜¯å®ƒä»¬çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚3DGSå’Œå…¶å˜ä½“ä¹Ÿå­˜åœ¨ç€å‡†ç¡®é‡å»ºçš„æŒ‘æˆ˜ã€‚- (3):æœ¬æ–‡æå‡ºäº†é«˜æ–¯æ—¶é—´æœºå™¨ï¼ˆGTMï¼‰ï¼Œå®ƒä½¿ç”¨ç¦»æ•£æ—¶é—´åµŒå…¥å‘é‡å’Œè½»é‡çº§å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¥å»ºæ¨¡é«˜æ–¯primitiveçš„æ—¶é—´ç›¸å…³å±æ€§ã€‚é€šè¿‡è°ƒæ•´é«˜æ–¯primitiveçš„ä¸é€æ˜åº¦ï¼Œå¯ä»¥é‡å»ºå¯¹è±¡çš„å¯è§æ€§å˜åŒ–ã€‚- (4):GTMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“ä¿çœŸåº¦ï¼Œæ¸²æŸ“é€Ÿåº¦æ˜¯NeRF-basedæ–¹æ³•çš„100å€ã€‚æ­¤å¤–ï¼ŒGTMè¿˜æˆåŠŸåœ°åˆ†ç¦»äº†å¤–è§‚å˜åŒ–å¹¶å®ç°äº†å¹³æ»‘çš„å¤–è§‚æ’å€¼ã€‚</code></pre><ol><li>Methods:</li></ol><ul><li><p>(1): æœ¬æ–‡æå‡ºçš„é«˜æ–¯æ—¶é—´æœºå™¨ï¼ˆGaussian Time Machineï¼ŒGTMï¼‰é‡‡ç”¨ç¦»æ•£æ—¶é—´åµŒå…¥å‘é‡å’Œè½»é‡çº§å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¥å»ºæ¨¡é«˜æ–¯primitiveçš„æ—¶é—´ç›¸å…³å±æ€§ã€‚</p></li><li><p>(2): GTMé€šè¿‡è°ƒæ•´é«˜æ–¯primitiveçš„ä¸é€æ˜åº¦ï¼Œå®ç°äº†å¯¹è±¡å¯è§æ€§çš„å˜åŒ–ï¼Œå¹¶æˆåŠŸåœ°åˆ†ç¦»äº†å¤–è§‚å˜åŒ–ã€‚</p></li><li><p>(3): åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šï¼ŒGTMå±•ç°å‡ºäº†æœ€å…ˆè¿›çš„æ¸²æŸ“ä¿çœŸåº¦ï¼Œä¸”æ¸²æŸ“é€Ÿåº¦æ˜¯åŸºäºNeRFçš„æ–¹æ³•çš„100å€ã€‚æ­¤å¤–ï¼ŒGTMè¿˜èƒ½å¤Ÿå®ç°å¹³æ»‘çš„å¤–è§‚æ’å€¼ã€‚</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):æœ¬æ–‡æå‡ºçš„é«˜æ–¯æ—¶é—´æœºå™¨ï¼ˆGaussian Time Machineï¼ŒGTMï¼‰åœ¨è§£å†³æ—¶é—´å˜æ¢å¤–è§‚é—®é¢˜æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ•°å­—å­ªç”Ÿç­‰é¢†åŸŸã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šGTM æå‡ºäº†ç¦»æ•£æ—¶é—´åµŒå…¥å‘é‡å’Œè½»é‡çº§å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¥å»ºæ¨¡é«˜æ–¯primitiveçš„æ—¶é—´ç›¸å…³å±æ€§ï¼Œå®ç°äº†å¯¹è±¡å¯è§æ€§çš„å˜åŒ–å’Œå¤–è§‚å˜åŒ–çš„åˆ†ç¦»ï¼›æ€§èƒ½ï¼šGTM åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“ä¿çœŸåº¦ï¼Œæ¸²æŸ“é€Ÿåº¦æ˜¯ NeRF-based æ–¹æ³•çš„ 100 å€ï¼›å·¥ä½œé‡ï¼šGTM éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶æ¸²æŸ“ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e37e39f80d95d9753e062031ea071292.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8d45eb05bc11e95b4d1a05a781ee482b.jpg" align="middle"></details><h2 id="GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting"><a href="#GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting" class="headerlink" title="GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting"></a>GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting</h2><p><strong>Authors:Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</strong></p><p>The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing. </p><p><a href="http://arxiv.org/abs/2405.07472v2">PDF</a> On-going work</p><p><strong>Summary</strong><br>ç”µå­å•†åŠ¡çš„æ—¥ç›Šçªå‡ºå½°æ˜¾äº†è™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰çš„é‡è¦æ€§ã€‚æœ¬æ–‡æå‡ºäº†GaussianVTONï¼Œå°†é«˜æ–¯ç‚¹ç»˜åˆ¶ï¼ˆGSï¼‰ç¼–è¾‘ä¸2D VTONç›¸ç»“åˆï¼Œé¦–æ¬¡æå‡ºä½¿ç”¨å›¾åƒä½œä¸º3Dç¼–è¾‘æç¤ºï¼Œä»¥åŠå¼•å…¥äº†ERRç¼–è¾‘ç­–ç•¥ï¼Œä¸º3D VTONæä¾›äº†æ–°è§†è§’ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç”µå­å•†åŠ¡çš„æ—¥ç›Šçªå‡ºå½°æ˜¾äº†è™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰çš„é‡è¦æ€§ã€‚</li><li>GaussianVTONå°†é«˜æ–¯ç‚¹ç»˜åˆ¶ï¼ˆGSï¼‰ç¼–è¾‘ä¸2D VTONç›¸ç»“åˆï¼Œé¦–æ¬¡æå‡ºä½¿ç”¨å›¾åƒä½œä¸º3Dç¼–è¾‘æç¤ºã€‚</li><li>é€šè¿‡ä¸‰é˜¶æ®µçš„ç²¾ç»†åŒ–ç­–ç•¥é€æ­¥ç¼“è§£æ½œåœ¨é—®é¢˜ï¼Œè¿›ä¸€æ­¥è§£å†³äº†é¢éƒ¨æ¨¡ç³Šã€æœè£…ä¸å‡†ç¡®å’Œç¼–è¾‘è¿‡ç¨‹ä¸­è§†è§’è´¨é‡ä¸‹é™ç­‰é—®é¢˜ã€‚</li><li>å¼•å…¥äº†ERRç¼–è¾‘ç­–ç•¥æ¥åº”å¯¹ä¹‹å‰ç¼–è¾‘ç­–ç•¥çš„å±€é™æ€§ï¼Œè§£å†³äº†å¤æ‚å‡ ä½•å˜åŒ–å¸¦æ¥çš„é—®é¢˜ã€‚</li><li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGaussianVTONå…·æœ‰å“è¶Šæ€§èƒ½ï¼Œä¸º3D VTONæä¾›äº†æ–°è§†è§’ï¼Œå¹¶å»ºç«‹äº†å›¾åƒæç¤º3Dåœºæ™¯ç¼–è¾‘çš„æ–°èµ·ç‚¹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: é«˜æ–¯ Virtual Try-Onï¼šåŸºäºå¤šé˜¶æ®µé«˜æ–¯ Splatting çš„ 3D äººä½“è™šæ‹Ÿè¯•è¡£ï¼ˆGaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splattingï¼‰</p></li><li><p>Authors: Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</p></li><li><p>Affiliation: è¥¿åŒ—å·¥ä¸šå¤§å­¦</p></li><li><p>Keywords: Virtual Try-On, 3D Human, Gaussian Splatting, Image Prompting</p></li><li><p>Urls: <a href="https://haroldchen19.github.io/gsvton/">https://haroldchen19.github.io/gsvton/</a>, Github:None</p></li><li><p>Summary:</p><ul><li><p>(1):éšç€ç”µå­å•†åŠ¡çš„å…´èµ·ï¼Œè™šæ‹Ÿè¯•è¡£ï¼ˆVirtual Try-On, VTONï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ 2D é¢†åŸŸï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ 2D VTON é¢†åŸŸï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚è¿™äº›æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°è§£å†³ 3D VTON é—®é¢˜ï¼Œä¾‹å¦‚æœè£…å½¢çŠ¶ä¸äººä½“å½¢çŠ¶çš„ä¸å…¼å®¹é—®é¢˜</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº† GaussianVTONï¼Œä¸€ç§åŸºäºå¤šé˜¶æ®µé«˜æ–¯ Splatting çš„ 3D VTON ç®¡é“ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å›¾åƒä½œä¸ºç¼–è¾‘æç¤ºï¼Œå®ç°äº†ä» 2D åˆ° 3D VTON çš„æ— ç¼è¿‡æ¸¡ã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼ŒGaussianVTON æ–¹æ³•åœ¨ 3D VTON ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul></li><li>æ–¹æ³•ï¼š</li></ol><ul><li><p>(1)ï¼šè¾“å…¥é‡å»ºçš„ 3D åœºæ™¯å’Œç›¸åº”çš„æ•°æ®ï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—æ‹æ‘„çš„å›¾åƒã€ç›¸åº”çš„ç›¸æœºå§¿æ€å’Œç›¸æœºæ ‡å®šå‚æ•°ã€‚</p></li><li><p>(2)ï¼šä½¿ç”¨å›¾åƒç¼–è¾‘æç¤ºæ¥æŒ‡å¯¼ 3D åœºæ™¯çš„ç¼–è¾‘è¿‡ç¨‹ï¼Œä»¥å®ç°è™šæ‹Ÿè¯•è¡£ã€‚é¦–å…ˆï¼Œå¼•å…¥ 3D é«˜æ–¯ Splatting æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„ 2D VTON æ¨¡å‹ã€‚</p></li><li><p>(3)ï¼šæå‡ºäº† Editing Recall Reconstruction (ERR) ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­æ¸²æŸ“æ•´ä¸ªæ•°æ®é›†ï¼Œä»¥è§£å†³ç¼–è¾‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</p></li><li><p>(4)ï¼šé‡‡ç”¨ä¸‰é˜¶æ®µç»†åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬äººè„¸ä¸€è‡´æ€§ã€å±‚æ¬¡ç¨€ç–ç¼–è¾‘å’Œå›¾åƒè´¨é‡æ”¹è¿›ä¸‰ä¸ªé˜¶æ®µï¼Œä»¥è§£å†³ç¼–è¾‘è¿‡ç¨‹ä¸­é‡åˆ°çš„å„ç§é—®é¢˜ã€‚</p></li><li><p>(5)ï¼šåœ¨ ERR ç­–ç•¥ä¸­ï¼Œå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œç¼–è¾‘å’Œç»†åŒ–ï¼Œç„¶åå¯¹æ•°æ®é›†è¿›è¡Œæ›´æ–°ï¼Œä»¥ç¡®ä¿ç¼–è¾‘çš„ä¸€è‡´æ€§ã€‚</p></li><li><p>(6)ï¼šä½¿ç”¨ LaDI-VTON æ¨¡å‹å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œç¼–è¾‘ï¼Œå¹¶å°†ç¼–è¾‘ç»“æœä¸åŸå§‹å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯„ä¼°ç¼–è¾‘çš„æ•ˆæœã€‚</p></li><li><p>(7)ï¼šå¯¹ç¼–è¾‘ç»“æœè¿›è¡Œå¯è§†åŒ–å’Œè¯„ä¼°ï¼Œä»¥éªŒè¯ GaussianVTON æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li></ul><ol><li><p>Conclusion: </p><pre><code>             - (1):æœ¬æ–‡çš„å·¥ä½œå¯¹ç”µå­å•†åŠ¡è™šæ‹Ÿè¯•è¡£é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›æ›´åŠ çœŸå®çš„è¯•è¡£ä½“éªŒã€‚             - (2):åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šé˜¶æ®µé«˜æ–¯ Splatting çš„ 3D äººä½“è™šæ‹Ÿè¯•è¡£æ–¹æ³•ï¼Œè§£å†³äº† 2D åˆ° 3D è™šæ‹Ÿè¯•è¡£çš„æŠ€æœ¯ç“¶é¢ˆï¼›æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒGaussianVTON æ–¹æ³•åœ¨ 3D VTON ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡çš„æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ™®åŠæ€§ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e12873404001a9a09d996899cdfe1fc3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c590805a84c00f53de63efe5b169e438.jpg" align="middle"><img src="https://picx.zhimg.com/v2-28127860f8d303f51aff59430d547019.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  Feature Splatting for Better Novel View Synthesis with Low Overlap</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/</id>
    <published>2024-05-27T17:24:49.000Z</published>
    <updated>2024-05-28T08:33:26.824Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation"><a href="#InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation" class="headerlink" title="InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation"></a>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation</h2><p><strong>Authors:Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian</strong></p><p>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2405.15758v1">PDF</a> Project page: <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a></p><p><strong>Summary</strong><br>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬æŒ‡å¯¼æ–¹æ³•ï¼Œç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œçš„2Dè™šæ‹Ÿå¤´åƒï¼Œå®ç°ç»†ç²’åº¦æ§åˆ¶ã€äº¤äº’æ€§å’Œé€šç”¨æ€§ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å½“å‰è™šæ‹Ÿå¤´åƒç”Ÿæˆæ¨¡å‹åœ¨å”‡éŸ³åŒæ­¥ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†åœ¨è¡¨æƒ…å’Œæƒ…æ„Ÿæ§åˆ¶æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚<br>â€¢ æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬æŒ‡å¯¼æ–¹æ³•ï¼Œç”Ÿæˆæƒ…æ„Ÿä¸°å¯Œçš„2Dè™šæ‹Ÿå¤´åƒï¼Œå®ç°ç»†ç²’åº¦æ§åˆ¶å’Œäº¤äº’æ€§ã€‚<br>â€¢ è¯¥æ–¹æ³•ä½¿ç”¨è‡ªç„¶è¯­è¨€ç•Œé¢æ§åˆ¶è™šæ‹Ÿå¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨è¿åŠ¨ã€‚<br>â€¢ è¯¥æ–¹æ³•ä½¿ç”¨è‡ªåŠ¨æ³¨é‡Šç®¡é“æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨åŒåˆ†æ”¯æ‰©æ•£ç”Ÿæˆå™¨é¢„æµ‹è™šæ‹Ÿå¤´åƒã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructAvataræ–¹æ³•åœ¨ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶ã€å”‡éŸ³åŒæ­¥è´¨é‡å’Œè‡ªç„¶åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚<br>â€¢ è¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´åŠ ç”ŸåŠ¨å’Œå¯æ§çš„è™šæ‹Ÿå¤´åƒè§†é¢‘ã€‚<br>â€¢ é¡¹ç›®é¡µé¢ä¸º<a href="https://wangyuchi369.github.io/InstructAvatar/ã€‚">https://wangyuchi369.github.io/InstructAvatar/ã€‚</a></p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: InstructAvatarï¼šåŸºäºæ–‡æœ¬çš„è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶çš„Avatarç”Ÿæˆï¼ˆText-Guided Emotion and Motion Control for Avatar Generationï¼‰</p></li><li><p>Authors: Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, and Jiang Bian</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦</p></li><li><p>Keywords: Emotional Talking Avatar Â· Facial Motion Control Â· Text Guided Â· Diffusion Model</p></li><li><p>Urls: https://wangyuchi369.github.io/InstructAvatar/, Github: https://wangyuchi369.github.io/InstructAvatar/</p></li><li><p>Summary:</p></li><li><p>(1):è¿‘å¹´æ¥ï¼Œè°ˆè¯å¤´åƒç”Ÿæˆæ¨¡å‹å–å¾—äº†å®è´¨æ€§çš„è¿›å±•ï¼Œç„¶è€Œå®ƒä»¬åœ¨æ§åˆ¶å’Œè¡¨è¾¾å¤´åƒçš„æƒ…æ„Ÿå’Œè¡¨æƒ…æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ï¼Œç”Ÿæˆçš„è§†é¢‘å› æ­¤ç¼ºä¹ç”ŸåŠ¨æ€§å’Œå¯æ§æ€§ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨éŸ³é¢‘åŒæ­¥æ–¹é¢ï¼Œä½†æ˜¯åœ¨æ§åˆ¶å’Œè¡¨è¾¾å¤´åƒçš„æƒ…æ„Ÿå’Œè¡¨æƒ…æ–¹é¢æ•ˆæœä¸ä½³ï¼Œæ— æ³•æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶æ–¹æ³•ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥å£æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨è¿åŠ¨ï¼Œè®¾è®¡äº†ä¸€æ¡è‡ªåŠ¨æ³¨é‡Šæµæ°´çº¿æ¥æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå™¨æ¥é¢„æµ‹å¤´åƒã€‚</p></li><li><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructAvatarç”Ÿæˆçš„ç»“æœä¸æ¡ä»¶é«˜åº¦ä¸€è‡´ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•åœ¨ç»†ç²’åº¦çš„æƒ…æ„Ÿæ§åˆ¶ã€å”‡å½¢åŒæ­¥è´¨é‡å’Œè‡ªç„¶åº¦æ–¹é¢çš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†ç ”ç©¶ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li><li><p>(1)ï¼šé¦–å…ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶æ–¹æ³•ï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥å£æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨è¿åŠ¨ã€‚</p></li><li><p>(2)ï¼šè®¾è®¡äº†ä¸€æ¡è‡ªåŠ¨æ³¨é‡Šæµæ°´çº¿æ¥æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿæ ‡ç­¾æ‰©å±•ã€åŠ¨ä½œå•å…ƒæå–å’Œå¤§è¯­è¨€æ¨¡å‹ paraphraseã€‚</p></li><li><p>(3)ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹ä½œä¸ºæ–‡æœ¬æŒ‡å¯¼è¿åŠ¨ç”Ÿæˆå™¨ï¼Œå­¦ä¹ æ¡ä»¶äºéŸ³é¢‘å’Œæ–‡æœ¬æŒ‡ä»¤çš„è¿åŠ¨æ½œå˜é‡ã€‚</p></li><li><p>(4)ï¼šåœ¨è¿åŠ¨ç”Ÿæˆå™¨ä¸­ï¼Œè®¾è®¡äº†ä¸€ä¸ªä¸¤åˆ†æ”¯äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œinjecting æƒ…æ„Ÿå’Œè¿åŠ¨æ§åˆ¶ä¿¡æ¯åˆ°æ¨¡å‹ä¸­ã€‚</p></li><li><p>(5)ï¼šä½¿ç”¨Conformerä½œä¸ºæ‰©æ•£æ¨¡å‹çš„ä¸»å¹²ç½‘ç»œï¼Œç»“åˆéŸ³é¢‘ç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå­¦ä¹ éŸ³é¢‘å’Œæ–‡æœ¬æŒ‡å¯¼çš„è¿åŠ¨ç”Ÿæˆã€‚</p></li><li><p>(6)ï¼šåœ¨ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨DDIMç­–ç•¥ï¼Œè¿­ä»£å»å™ªéŸ³é¢‘æŒ‡å¯¼çš„è¿åŠ¨æ½œå˜é‡ï¼Œè·å¾—æœ€ç»ˆçš„è¿åŠ¨ç»“æœã€‚</p></li><li><p>(7)ï¼šåœ¨å®éªŒä¸­ï¼Œä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ç»†ç²’åº¦çš„æƒ…æ„Ÿæ§åˆ¶ã€å”‡å½¢åŒæ­¥è´¨é‡å’Œè‡ªç„¶åº¦æ–¹é¢çš„æ€§èƒ½ã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1):æœ¬æ–‡æå‡ºçš„InstructAvataræ–¹æ³•å¯¹å¤´åƒç”Ÿæˆé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯ä»¥å®ç°ç»†ç²’åº¦çš„æƒ…æ„Ÿæ§åˆ¶å’Œå”‡å½¢åŒæ­¥ï¼Œæ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ–‡æœ¬çš„è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶æ–¹æ³•ï¼Œå®ç°äº†å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨è¿åŠ¨æ§åˆ¶ï¼›æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒInstructAvatarç”Ÿæˆçš„ç»“æœä¸æ¡ä»¶é«˜åº¦ä¸€è‡´ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•åœ¨ç»†ç²’åº¦çš„æƒ…æ„Ÿæ§åˆ¶ã€å”‡å½¢åŒæ­¥è´¨é‡å’Œè‡ªç„¶åº¦æ–¹é¢çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šè®¾è®¡äº†ä¸€æ¡è‡ªåŠ¨æ³¨é‡Šæµæ°´çº¿æ¥æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼Œä½¿ç”¨äº†æ‰©æ•£æ¨¡å‹å’ŒConformerç½‘ç»œï¼Œéœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºå’Œæ•°æ®æ”¯æŒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-444c4a6d0fe06756aad4ae2d015fe594.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/</id>
    <published>2024-05-27T17:19:08.000Z</published>
    <updated>2024-05-28T08:34:32.613Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation"><a href="#DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation" class="headerlink" title="DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"></a>DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation</h2><p><strong>Authors:Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</strong></p><p>Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image. </p><p><a href="http://arxiv.org/abs/2405.15619v1">PDF</a> </p><p><strong>Summary</strong><br>å•ç›®ç›¸æœºæ ¡å‡†æ˜¯ä¼—å¤š3Dè§†è§‰åº”ç”¨çš„å…³é”®å…ˆå†³æ¡ä»¶ã€‚æœ€è¿‘ï¼ŒåŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«è¯å®èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒï¼Œä¸ºå•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡æä¾›æ›´å¼ºå¤§å’Œå‡†ç¡®çš„æ”¯æŒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å•ç›®ç›¸æœºæ ¡å‡†å¯¹äºå¤šç§3Dè§†è§‰åº”ç”¨è‡³å…³é‡è¦</li><li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„å›¾åƒ</li><li>é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„è§†è§‰çŸ¥è¯†ï¼Œèƒ½å¤Ÿå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„å•ç›®ç›¸æœºå†…åœ¨ä¼°è®¡</li><li>é€šè¿‡å°†ä¼°è®¡ç›¸æœºå†…åœ¨å‚æ•°çš„é—®é¢˜é‡æ–°æ„å»ºä¸ºå¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œèƒ½å¤Ÿå®ç°æ›´ç®€å•çš„æ¨æ–­è¿‡ç¨‹</li><li>è”åˆä¼°è®¡æ·±åº¦å›¾èƒ½å¤Ÿè¿›ä¸€æ­¥æå‡æ€§èƒ½</li><li>å®éªŒè¯æ˜è¯¥æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¢„æµ‹è¯¯å·®é™ä½äº†40%</li><li>ç²¾ç¡®çš„ç›¸æœºå†…åœ¨å’Œæ·±åº¦å›¾èƒ½å¤Ÿæå¤§åœ°ä¿ƒè¿›ä»å•å¼ é‡å¤–å›¾åƒè¿›è¡Œçš„3Dé‡å»ºç­‰å®é™…åº”ç”¨</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation (DiffCalibï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆ)</p></li><li><p>Authors: Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</p></li><li><p>Affiliation: æµ™æ±Ÿå·¥ä¸šå¤§å­¦</p></li><li><p>Keywords: monocular camera calibration, diffusion models, incident map generation</p></li><li><p>Urls: arXiv:2405.15619v1, Github:None</p></li><li><p>Summary:</p></li><li><p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å•ç›®æ‘„åƒæœºæ ¡å‡†ï¼Œè¿™æ˜¯è®¸å¤šä¸‰ç»´è§†è§‰åº”ç”¨çš„å…³é”®å‰ææ¡ä»¶ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•å­˜åœ¨ä¸€äº›å‡è®¾å’Œé™åˆ¶ï¼Œæ— æ³•åœ¨ä¸åŒçš„çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æ³›åŒ–ï¼Œå¹¶ä¸”å—é™äºè®­ç»ƒæ•°æ®çš„ä¸è¶³ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œè¿™å¯å‘äº†æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å®ç°æ›´é²æ£’å’Œå‡†ç¡®çš„å•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯å°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ï¼Œç„¶åä½¿ç”¨RANSACç®—æ³•æ¨æ–­æ‘„åƒæœºå‚ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨å•ç›®æ‘„åƒæœºæ ¡å‡†ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç†è§£è§†è§‰ä¿¡æ¯æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºåœ¨é‡ä¸‰ç»´é‡å»ºä»»åŠ¡ä¸­ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li><li><p>(1)ï¼šå°†å•ç›®æ‘„åƒæœºæ ¡å‡†é—®é¢˜é‡æ„ä¸ºåŸºäºæ‰©æ•£çš„å¯†é›†å…¥å°„å›¾ç”Ÿæˆä»»åŠ¡ï¼Œä»¥ä¾¿èƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå…¥å°„å›¾ã€‚</p></li><li><p>(2)ï¼šä½¿ç”¨Stable Diffusion v2.1æ¨¡å‹å¯¹å…¥å°„å›¾è¿›è¡Œç¼–ç å’Œè§£ç ï¼Œç”Ÿæˆå™ªå£°åçš„å…¥å°„å›¾latent codesï¼Œå¹¶è®­ç»ƒU-Netæ¨¡å‹æ¥é¢„æµ‹å™ªå£°ã€‚</p></li><li><p>(3)ï¼šå°†æ·±åº¦å›¾å’Œå…¥å°„å›¾è”åˆå­¦ä¹ ï¼Œä»¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p></li><li><p>(4)ï¼šä½¿ç”¨RANSACç®—æ³•ä»ç”Ÿæˆçš„å…¥å°„å›¾ä¸­æ¢å¤æ‘„åƒæœºçš„å†…å‚æ•°çŸ©é˜µKã€‚</p></li><li><p>(5)ï¼šä½¿ç”¨ensembleæ–¹æ³•æ¥æé«˜å…¥å°„å›¾ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</p></li><li><p>(6)ï¼šä½¿ç”¨æ¢å¤çš„æ‘„åƒæœºå†…å‚æ•°çŸ©é˜µKæ¥è¿›è¡Œå•ç›®æ‘„åƒæœºæ ¡å‡†ã€‚</p></li><li><p>Conclusion: </p></li><li><p>(1): è¿™ç¯‡æ–‡ç« çš„æ„ä¹‰åœ¨äºæå‡ºäº†å¯¹äº[é¢†åŸŸ]çš„æ–°æ€è·¯ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•å¸¦æ¥äº†æ–°çš„å¯å‘å’Œæ–¹å‘ï¼›</p></li><li>(2): Innovation point: è¯¥æ–‡ç« çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§å…¨æ–°çš„[åˆ›æ–°ç‚¹]ï¼Œçªç ´äº†ä¼ ç»Ÿçš„[åˆ›æ–°ç‚¹]æ–¹å¼ï¼› Performance: è¯¥æ–‡ç« åœ¨å®éªŒè¡¨ç°æ–¹é¢å±•ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œä½†ä»æœ‰å¾…è¿›ä¸€æ­¥æå‡ï¼› Workload: è¯¥æ–‡ç« çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦æ›´å¤šçš„å®éªŒæ•°æ®å’Œåˆ†ææ¥æ”¯æ’‘å…¶ç»“è®ºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-02a306a749ab4f7167af1ae9e9bd38f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3354b1c0f182b11d7a2fe0d1f53745ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3bcd389775a3247ad6697fadd1fd9cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a6244aa42d8f424a5319ca260b17f35.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" align="middle"></details><h2 id="Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models"><a href="#Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models"></a>Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models</h2><p><strong>Authors:Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</strong></p><p>Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMsâ€™ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: <a href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a> </p><p><a href="http://arxiv.org/abs/2405.15234v1">PDF</a> Codes are available at <a href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a></p><p><strong>Summary</strong><br>åŸºäºå¯¹æŠ—è®­ç»ƒå¢å¼ºæœºå™¨unlearningï¼Œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong><br>â€¢  Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿è§„ã€‚<br>â€¢  æœºå™¨unlearningæŠ€æœ¯å¯ä»¥è§£å†³è¿™äº›é£é™©ï¼Œä½†æ˜“å—åˆ°å¯¹æŠ—promptæ”»å‡»ã€‚<br>â€¢  æœ¬å·¥ä½œæå‡ºAdvUnlearnæ¡†æ¶ï¼Œé€šè¿‡å°†å¯¹æŠ—è®­ç»ƒåŸåˆ™é›†æˆåˆ°æœºå™¨unlearningä¸­ï¼Œä»¥æé«˜æ¦‚å¿µæ“¦é™¤çš„é²æ£’æ€§ã€‚<br>â€¢ AdvUnlearnæ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤é²æ£’æ€§å’Œæ¨¡å‹å®ç”¨æ€§ã€‚<br>â€¢  æ–‡æœ¬ç¼–ç å™¨æ˜¯å®ç°æœºå™¨unlearningçš„æ›´é€‚åˆæ¨¡å—ã€‚<br>â€¢  AdvUnlearnæ¡†æ¶å¯ä»¥åœ¨å„ç§Diffusionæ¨¡å‹unlearningåœºæ™¯ä¸‹å®ç°é²æ£’çš„æ¦‚å¿µæ“¦é™¤ã€‚<br>â€¢  æœ¬å·¥ä½œæ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢é€šè¿‡å¯¹æŠ—è®­ç»ƒå®ç°é²æ£’çš„Diffusionæ¨¡å‹unlearningã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: AdvUnlearn: Robust Unlearning for Diffusion Models (Diffusionæ¨¡å‹çš„é²æ£’unlearning)</p></li><li><p>Authors: (no authors listed)</p></li><li><p>Affiliation: æ— </p></li><li><p>Keywords: Diffusion Models, Machine Unlearning, Adversarial Training, Text-to-Image Generation</p></li><li><p>Urls: https://github.com/OPTML-Group/AdvUnlearn</p></li><li><p>Summary:</p><ul><li><p>(1):éšç€Diffusionæ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„æˆåŠŸï¼Œå®ƒä»¬ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œå¦‚ç”Ÿæˆæœ‰å®³å†…å®¹å’Œç‰ˆæƒè¿åã€‚ä¸ºè§£å†³è¿™äº›é£é™©ï¼Œæœºå™¨unlearningæŠ€æœ¯è¢«å¼€å‘å‡ºæ¥ï¼Œä½†æ˜¯è¿™äº›æŠ€æœ¯ä»æ˜“å—å¯¹æŠ—æ€§promptæ”»å‡»çš„å½±å“ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ï¼Œå¦‚ScissorHandså’ŒEraseDiffï¼Œè™½ç„¶å¯ä»¥å®ç°é«˜çš„unlearning robustnessï¼Œä½†æ˜¯å®ƒä»¬å›¾åƒç”Ÿæˆè´¨é‡ä¸‹é™æ˜æ˜¾ã€‚è¿™äº›æ–¹æ³•çš„motivationä¸è¶³ï¼Œæ— æ³•è§£å†³æœºå™¨unlearningä¸­çš„å®‰å…¨é£é™©ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºäº†AdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessã€‚è¯¥æ¡†æ¶ä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨æ€§ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p></li><li><p>(4):æœ¬æ–‡åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ã€‚ç»“æœè¡¨æ˜ï¼ŒAdvUnlearnæ¡†æ¶å¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1):æå‡ºAdvUnlearnæ¡†æ¶ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼Œä½¿ç”¨utility-retaining regularizationæ¥å¹³è¡¡æ¦‚å¿µæ“¦é™¤çš„robustnesså’Œæ¨¡å®ç”¨ï¼Œå¹¶å°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºrobustificationçš„æ¨¡å—ã€‚</p></li><li><p>(2):ä½¿ç”¨large language model (LLM)ä½œä¸ºjudgeæ¥ç­›é€‰ä¿ç•™promptï¼Œæ’é™¤ä¸ç›®æ ‡æ¦‚å¿µæ“¦é™¤ç›¸å…³çš„promptï¼Œä»è€Œç¡®ä¿å›¾åƒç”Ÿæˆè´¨é‡ä¸å—æŸå®³ã€‚</p></li><li><p>(3):å®šä¹‰utility-retaining regularizationæŸå¤±å‡½æ•°â„“ESDï¼Œpenalizeså›¾åƒç”Ÿæˆè´¨é‡çš„ä¸‹é™ï¼Œä½¿ç”¨å½“å‰Diffusionæ¨¡å‹Î¸ä¸åŸå§‹Î¸oä¸‹çš„ä¿ç•™æ¦‚å¿µËœcæ¥è®¡ç®—ã€‚</p></li><li><p>(4):ä½¿ç”¨fast attack generationæ–¹æ³•æ¥ç®€åŒ–AdvUnlearnçš„lower-levelä¼˜åŒ–ï¼Œä½¿ç”¨fast gradient sign method (FGSM)æ¥è§£å†³quadratic programï¼Œå¹¶ç”Ÿæˆå¯¹æŠ—æ€§promptã€‚</p></li><li><p>(5):å°†AdvUnlearnåº”ç”¨äºä¸åŒçš„Diffusionæ¨¡å‹unlearningåœºæ™¯ï¼ŒåŒ…æ‹¬è£¸ä½“ã€å¯¹è±¡å’Œé£æ ¼æ¦‚å¿µçš„æ“¦é™¤ï¼Œå¹¶è¯„ä¼°å…¶robustnesså’Œå›¾åƒç”Ÿæˆè´¨é‡ã€‚</p></li><li><p>(6):æ¯”è¾ƒAdvUnlearnä¸å…¶æ–¹æ³•ï¼ˆå¦‚ESDå’ŒAT-ESDï¼‰çš„æ€§èƒ½ï¼Œè¯æ˜AdvUnlearnå¯ä»¥å®ç°robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§</p></li><li><p>(7):æ¢ç´¢AdvUnlearnçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œè®¨è®ºå°†æ–‡æœ¬ç¼–ç å™¨ä½œä¸ºplug-in unlearnerçš„å¯èƒ½æ€§ï¼Œä»¥æé«˜æœºå™¨unlearningçš„æ•ˆç‡å’Œæ™®é€‚æ€§ã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1):æœ¬æ–‡æå‡ºçš„AdvUnlearnæ¡†æ¶å¯¹Diffusionæ¨¡å‹çš„æœºå™¨unlearningé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¯ä»¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚</p></li><li><p>(2):Innovation point: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨unlearningæ–¹æ³•ï¼Œç»“åˆå¯¹æŠ—æ€§è®­ç»ƒå’Œutility-retaining regularizationæ¥å¢å¼ºæœºå™¨unlearningçš„robustnessï¼›Performance: AdvUnlearnæ¡†æ¶åœ¨å¤šä¸ªDiffusionæ¨¡å‹unlearningåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†robustçš„æœºå™¨unlearningï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ï¼›Workload: æœ¬æ–‡çš„å®éªŒè®¾è®¡å’Œå®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-12bc7afe95c87708c06799dd505c46da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c3f86497a08db26b9953f1bc30dad1c3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ef67ded1db4d01263a65cdacd20797a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-202a39b4f890f5df5c6e0f34c4f7a6a7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89575cd27c93753bf34b1aebf5ce8aef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-005e6d2cd8b93a64b356e1bd2dd224c9.jpg" align="middle"></details><h2 id="DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><a href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception" class="headerlink" title="DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception"></a>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception</h2><p><strong>Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui</strong></p><p>The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the modelâ€™s resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size. </p><p><a href="http://arxiv.org/abs/2405.15232v1">PDF</a> 25 pages</p><p><strong>Summary</strong><br>é€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDEEMçš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹å¯¹äºè¶…å‡ºåˆ†å¸ƒæ•°æ®çš„é²æ£’æ€§ï¼Œå‡å°‘äº†è§†è§‰å¹»è§‰ï¼ŒåŒæ—¶æ— éœ€é¢å¤–çš„è®­ç»ƒæ¨¡å—å’Œæ›´å°‘çš„è®­ç»ƒå‚æ•°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‡ºç°ï¼›</li><li>LMMsåœ¨ä¿ƒè¿›å¤šæ¨¡æ€ç†è§£å’Œåˆ›ä½œæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤„ç†è¶…å‡ºåˆ†å¸ƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼›</li><li>DEEMåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥è°ƒæ•´å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œè§£å†³äº†ä»¥å¾€ä»…ä¾èµ–äºå›¾åƒç¼–ç å™¨çš„æ–¹æ³•çš„ç¼ºé™·ï¼›</li><li>DEEMåœ¨RobustVQAåŸºå‡†å’ŒPOPEåŸºå‡†ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œå‡å°‘æ¨¡å‹å¹»è§‰çš„èƒ½åŠ›ï¼›</li><li>DEEMç›¸è¾ƒäºæœ€å…ˆè¿›çš„äº¤æ›¿å†…å®¹ç”Ÿæˆæ¨¡å‹ï¼Œå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼Œå¹¶åˆ©ç”¨æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€æ›´å°‘çš„é¢„è®­ç»ƒæ•°æ®ï¼ˆ10%ï¼‰å’Œæ›´å°çš„åŸºç¡€æ¨¡å‹å°ºå¯¸ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: DEEMï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å›¾åƒæ„ŸçŸ¥è¿›è¡Œå¢å¼º (DEEM: Enhancing Image Perception of Large Multimodal Models with Diffusion Models)</p></li><li><p>Authors: (no author names provided)</p></li><li><p>Affiliation: æ—  (no affiliation provided)</p></li><li><p>Keywords: large language models, large multimodal models, diffusion models, image perception, robustness, hallucination</p></li><li><p>Urls: arXiv:2405.15232v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å‘å±•ï¼Œåè€…é€šè¿‡ç®€å•çš„æ˜ å°„æ¨¡å—å°†LLMsä¸å›¾åƒç¼–ç å™¨è¿æ¥èµ·æ¥ï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–å›¾åƒç¼–ç å™¨æ¥å°†å›¾åƒç¼–ç ä¸ºä»»åŠ¡ç›¸å…³ç‰¹å¾ï¼Œå¯èƒ½å¿½è§†æ— å…³ç»†èŠ‚ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œhallucinationé—®é¢˜ã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯DEEMï¼Œå®ƒä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥å¯¹é½å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnesså’Œå‡å°‘hallucinationã€‚</p></li><li><p>(4):è¯¥æ–¹æ³•åœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜DEEMç›¸æ¯”äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å…·æœ‰æ›´å¥½çš„robustnesså’Œå‡å°‘hallucinationèƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å¯ä»¥åœ¨å¤šæ¨¡æ€ä»»åŠ¡å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰æ–¹é¢å–å¾—ç«äº‰æ€§çš„ç»“æœã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1)ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼Œç”Ÿæˆå›¾åƒç›¸å…³çš„æ–‡æœ¬ç‰¹å¾ï¼Œä»¥ä¾¿ä¸å›¾åƒç¼–ç å™¨è¿›è¡Œå¯¹é½ã€‚</p></li><li><p>(2)ï¼šç„¶åï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelï¼‰å¯¹å›¾åƒç¼–ç å™¨çš„è¾“å‡ºè¿›è¡Œç”Ÿæˆåé¦ˆï¼Œä»¥è°ƒæ•´å›¾åƒç¼–ç å™¨è¯­ä¹‰åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹å¯¹å¤–åˆ†å¸ƒæ•°æ®çš„robustnessã€‚</p></li><li><p>(3)ï¼šåœ¨ç”Ÿæˆåé¦ˆè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æŠ—è®­ç»ƒï¼ˆAdversarial Trainingï¼‰æ¥é¼“åŠ±å›¾åƒç¼–ç å™¨ç”Ÿæˆæ›´åŠ robustçš„ç‰¹å¾ï¼Œå‡å°‘hallucinationçš„å¯èƒ½æ€§ã€‚</p></li><li><p>(4)ï¼šæ¥ç€ï¼Œå¯¹DEEMæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ä»»åŠ¡çš„fine-tuningï¼Œä¾‹å¦‚è§†è§‰é—®ç­”ã€å›¾åƒå­—å¹•ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒåˆæˆç­‰ï¼Œä»¥æé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p></li><li><p>(5)ï¼šæœ€åï¼Œåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°DEEMæ¨¡å‹çš„robustnesså’Œhallucinationèƒ½åŠ›ï¼Œä¸¦ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p></li><li><p>Conclusion: </p></li><li><p>(1): æœ¬ç ”ç©¶çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ˆDEEMï¼‰ï¼Œé€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå›¾åƒæ„ŸçŸ¥å¢å¼ºï¼Œæœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œå‡å°‘äº†è™šå‡æ„ŸçŸ¥ï¼Œä¸ºå¤šæ¨¡æ€ä»»åŠ¡çš„æ€§èƒ½æå‡æä¾›äº†æ–°çš„æ€è·¯ã€‚</p></li><li><p>(2): åˆ›æ–°ç‚¹ï¼šDEEMæ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œåœ¨æé«˜æ¨¡å‹é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚æ€§èƒ½ï¼šDEEMåœ¨RobustVQAå’ŒPOPEä¸¤ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šç›¸æ¯”å½“å‰æœ€å…ˆè¿›æ¨¡å‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§å’Œå‡å°‘è™šå‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šå–å¾—äº†ç«äº‰æ€§çš„ç»“æœã€‚å·¥ä½œé‡ï¼šè®ºæ–‡æ‰€æå‡ºçš„DEEMæ–¹æ³•éœ€è¦è¿›ä¸€æ­¥å®éªŒå’ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å…¶åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™å¯èƒ½éœ€è¦æ›´å¤šçš„å·¥ä½œé‡æ¥æ”¯æŒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle"></details><h2 id="NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation"><a href="#NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation" class="headerlink" title="NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation"></a>NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</h2><p><strong>Authors:Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac</strong></p><p>The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics â€” mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art. </p><p><a href="http://arxiv.org/abs/2405.15217v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§è§£å†³æ–¹æ¡ˆNIVeLã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å»å™ªæ‰©æ•£æ¨¡å‹åœ¨2D rasterå›¾åƒä¸Šçš„æˆåŠŸä¿ƒä½¿ç ”ç©¶å°†å…¶æ‰©å±•åˆ°å…¶ä»–æ•°æ®è¡¨ç¤ºå½¢å¼ï¼Œå¦‚çŸ¢é‡å›¾å½¢ã€‚<br>â€¢ ç›´æ¥å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢é¢†åŸŸæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºçŸ¢é‡å›¾å½¢å…·æœ‰å¯å˜ç»“æ„å’Œç¨€ç–çš„è®­ç»ƒæ•°æ®ã€‚<br>â€¢ ä½¿ç”¨Score Distillation Samplingï¼ˆSDSï¼‰ç­‰ä¼˜åŒ–æ–¹æ³•ä¹Ÿå­˜åœ¨å›°éš¾ï¼Œå› ä¸ºçŸ¢é‡è¡¨ç¤ºéš¾ä»¥ç›´æ¥ä¼˜åŒ–ï¼Œå®¹æ˜“äº§ç”Ÿä¸å¯ä¿¡çš„å‡ ä½•å½¢çŠ¶ã€‚<br>â€¢ NIVeLé€šè¿‡é‡æ–°è§£é‡Šé—®é¢˜åœ¨ä¸­é—´åŸŸä¸Šï¼Œä¿ç•™çŸ¢é‡å›¾å½¢çš„è‰¯å¥½å±æ€§ï¼Œä¾‹å¦‚ç¨€ç–è¡¨ç¤ºå’Œåˆ†è¾¨ç‡ç‹¬ç«‹æ€§ã€‚<br>â€¢ ä¸­é—´åŸŸåŸºäºå¯åˆ†è§£ã€å¯ç¼–è¾‘çš„ç¥ç»éšå¼å­—æ®µå±‚ã€‚<br>â€¢ å®éªŒç»“æœè¡¨æ˜ï¼ŒNIVeLç”Ÿæˆçš„æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ç»“æœè¿œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ç»“æœã€‚<br>â€¢ NIVeLè§£å†³äº†æ‰©å±•å»å™ªæ‰©æ•£æ¨¡å‹åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸçš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NIVeL: ç¥ç»éšå¼çŸ¢é‡å›¾å½¢ç”Ÿæˆï¼ˆNeural Implicit Vector Graphics Generationï¼‰</p></li><li><p>Authors: Not provided</p></li><li><p>Affiliation: ä¸æä¾›ï¼ˆNot providedï¼‰</p></li><li><p>Keywords: denoising diffusion models, vector graphics, neural implicit fields</p></li><li><p>Urls: Not provided, Github: None</p></li><li><p>Summary:</p></li><li><p>(1):è¯¥è®ºæ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯å°†å»å™ªæ‰©æ•£æ¨¡å‹ä»2D rasterå›¾åƒæ‰©å±•åˆ°çŸ¢é‡å›¾å½¢é¢†åŸŸï¼Œä½†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ä½¿å¾—ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å˜å¾—å›°éš¾ã€‚</p></li><li><p>(2):è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ç›´æ¥åº”ç”¨å»å™ªæ‰©æ•£æ¨¡å‹å’ŒScore Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚ç”Ÿæˆçš„çŸ¢é‡å›¾å½¢å¯èƒ½åŒ…å«å†—ä½™æˆ–è‡ªç›¸äº¤çš„å½¢çŠ¶ã€‚</p></li><li><p>(3):æœ¬è®ºæ–‡æå‡ºäº†NIVeLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å°†é—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µçš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚æ¥ç”ŸæˆçŸ¢é‡å›¾å½¢ã€‚</p></li><li><p>(4):æœ¬è®ºæ–‡çš„æ–¹æ³•åœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œè¯æ˜äº†NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li><li><p>(1):å°†çŸ¢é‡å›¾å½¢ç”Ÿæˆé—®é¢˜é‡æ–°è§£é‡Šåœ¨ä¸­é—´åŸŸä¸Šï¼Œå³åŸºäºç¥ç»éšå¼å­—æ®µï¼ˆNeural Implicit Fieldsï¼‰çš„å¯åˆ†è§£ã€å¯ç¼–è¾‘çš„å±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ã€‚</p></li><li><p>(2):ä½¿ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ï¼ˆDenoising Diffusion Modelsï¼‰åœ¨ä¸­é—´åŸŸä¸Šç”Ÿæˆéšå¼è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ç¥ç»éšå¼å­—æ®µå°†å…¶è½¬æ¢ä¸ºçŸ¢é‡å›¾å½¢ã€‚</p></li><li><p>(3):å¼•å…¥ Score Distillation Samplingï¼ˆSDSï¼‰ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p></li><li><p>(4):åœ¨ä¸­é—´åŸŸä¸Šåº”ç”¨ç¼–è¾‘æ“ä½œï¼Œå¦‚å½¢çŠ¶å˜æ¢ã€æ‹“æ‰‘å˜åŒ–ç­‰ï¼Œä»¥å¢å¼ºç”ŸæˆçŸ¢é‡å›¾å½¢çš„å¯ç¼–è¾‘æ€§å’Œçµæ´»æ€§ã€‚</p></li><li><p>(5):ä½¿ç”¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡çš„å®éªŒç»“æœéªŒè¯NIVeLæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡çŸ¢é‡å›¾å½¢æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li><li><p>(1):è¯¥ç¯‡å·¥ä½œçš„é‡è¦æ€§åœ¨äºå°†å»å™ªæ‰©æ•£æ¨¡å‹åº”ç”¨äºçŸ¢é‡å›¾å½¢ç”Ÿæˆé¢†åŸŸï¼Œè§£å†³äº†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®é—®é¢˜ï¼Œæé«˜äº†ç”ŸæˆçŸ¢é‡å›¾å½¢çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»éšå¼å­—æ®µçš„çŸ¢é‡å›¾å½¢ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†çŸ¢é‡å›¾å½¢çš„å¯å˜ç»“æ„å’Œç¨€ç¼ºçš„è®­ç»ƒæ•°æ®ï¼›æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°çŸ¢é‡å›¾å½¢ä»»åŠ¡ä¸Šå–å¾—äº†æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä¸”å½“å‰çš„è¡¨ç¤ºæ–¹å¼è¿˜å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œå¦‚å±‚çš„æ•°é‡é™åˆ¶ç­‰ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-deb0bce750c823b45864a06b1f2fdf37.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b05c16791ff3624415d2ca5a4bb2b01d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ddb20e736aa45d7da426d42c0386fcb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a127e1927a9826d4a5a6449d4ce7f25e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ef7a2dd3802c3e38639f59aa13e5305.jpg" align="middle"></details><h2 id="TerDiT-Ternary-Diffusion-Models-with-Transformers"><a href="#TerDiT-Ternary-Diffusion-Models-with-Transformers" class="headerlink" title="TerDiT: Ternary Diffusion Models with Transformers"></a>TerDiT: Ternary Diffusion Models with Transformers</h2><p><strong>Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</strong></p><p>Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at <a href="https://github.com/Lucky-Lance/TerDiT">https://github.com/Lucky-Lance/TerDiT</a>. </p><p><a href="http://arxiv.org/abs/2405.14854v1">PDF</a> 18 pages, 13 figures</p><p><strong>Summary</strong><br>å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œæå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆTerDiTï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚</p><p><strong>Key Takeaways</strong><br>â€¢ å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäº transformer æ¶æ„çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiTsï¼‰ï¼Œç”Ÿæˆé«˜ä¿çœŸå›¾åƒçš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚<br>â€¢ æ‰©æ•£å˜å‹å™¨æ¨¡å‹å±•ç¤ºå‡ºä¼˜è¶Šçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰è¾ƒä½çš„ FID åˆ†æ•°å’Œæ›´é«˜çš„å¯æ‰©å±•æ€§ã€‚<br>â€¢ éƒ¨ç½²å¤§è§„æ¨¡ DiT æ¨¡å‹å¯èƒ½å¾ˆæ˜‚è´µï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åºå¤§çš„å‚æ•°æ•°é‡ã€‚<br>â€¢ ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†å¯¹äº DiT åŸºç¡€æ¨¡å‹çš„ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚<br>â€¢ æœ¬æ–‡æå‡ºäº† TerDiTï¼Œä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºä¸‰çº§æ‰©æ•£æ¨¡å‹çš„ transformersã€‚<br>â€¢ è¯¥æ–¹æ¡ˆå…³æ³¨ DiT ç½‘ç»œçš„ä¸‰çº§åŒ–ï¼Œå¹¶å°†æ¨¡å‹å¤§å°ä» 600M æ‰©å±•åˆ° 4.2Bã€‚<br>â€¢ æœ¬å·¥ä½œä¸ºå¤§è§„æ¨¡ DiT æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ç­–ç•¥åšå‡ºäº†è´¡çŒ®ï¼Œè¯æ˜äº†ä»å¤´è®­ç»ƒæä½ä½æ‰©æ•£å˜å‹å™¨æ¨¡å‹çš„å¯è¡Œæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸ä¼¼çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: TerDiTï¼šå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ (TerDiT: Ternary Diffusion Models with Transformers)</p></li><li><p>Authors: Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</p></li><li><p>Affiliation: é¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</p></li><li><p>Keywords: diffusion models, transformer architecture, quantization-aware training, efficient deployment</p></li><li><p>Urls: https://arxiv.org/abs/2405.14854, Github: https://github.com/Lucky-Lance/TerDiT</p></li><li><p>Summary:</p><ul><li><p>(1):æœ€è¿‘ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‘å±•æå¤§åœ°æ”¹å–„äº†é«˜ä¿çœŸå›¾åƒçš„ç”Ÿæˆï¼Œç‰¹åˆ«æ˜¯åŸºäºå˜å‹å™¨æ¶æ„ï¼ˆDiTsï¼‰çš„æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(2):ç°æœ‰çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²æŠ€æœ¯ï¼Œå¦‚æ¨¡å‹é‡åŒ–ï¼Œä½†æ˜¯åœ¨DiTæ¨¡å‹æ–¹é¢ä»ç„¶å­˜åœ¨ç ”ç©¶gapã€‚</p></li><li><p>(3):æœ¬æ–‡æå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(4):æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥è®­ç»ƒæä½æ¯”ç‰¹æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼Œä»è€Œå®ç°ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ã€‚</p></li><li>æ–¹æ³•ï¼š</li></ul></li><li><p>(1)ï¼šé‡‡ç”¨å‡é‡å‡½æ•°ï¼ˆfake quant functionï¼‰å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œè®¾ç½®n_bits=4ï¼Œä¸è¿›è¡Œæ¿€æ´»é‡åŒ–ã€‚</p></li><li><p>(2)ï¼šå¯¹åŸDiTå—ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚æƒé‡è¿›è¡Œé‡åŒ–ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„ã€å‰é¦ˆå’ŒMLPã€‚</p></li><li><p>(3)ï¼šä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹é‡‡æ ·å›¾åƒï¼Œå¹¶ä¸å…¨ç²¾åº¦æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</p></li><li><p>(4)ï¼šæå‡ºTerDiTï¼Œä¸€ä¸ªé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œç”¨äºå…·æœ‰å˜å‹å™¨çš„ä¸‰è¿›åˆ¶æ‰©æ•£æ¨¡å‹ã€‚</p></li><li><p>(5)ï¼šé‡‡ç”¨å­¦ä¹ ç‡å‡å°ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„è®­ç»ƒç»“æœã€‚</p></li><li><p>(6)ï¼šä½¿ç”¨RMS Normalized adaLNæ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</p></li><li><p>(7)ï¼šè¿›è¡Œå®éªŒæ¯”è¾ƒï¼ŒéªŒè¯TerDiTæ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²å’Œå›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li><li><p>(1):è¯¥å·¥ä½œçš„é‡è¦æ€§åœ¨äºå®ƒæ¨åŠ¨äº†å…·æœ‰å˜å‹å™¨æ¶æ„çš„æ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²ï¼Œæ»¡è¶³äº†å®é™…åº”ç”¨ä¸­çš„ä½å»¶è¿Ÿå’Œä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚</p></li><li><p>(2):åˆ›æ–°ç‚¹ï¼šTerDiT æ¨¡å‹æå‡ºäº†ä¸€ç§é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œé«˜æ•ˆéƒ¨ç½²æ–¹æ¡ˆï¼Œè§£å†³äº†ç°æœ‰DiT æ¨¡å‹åœ¨é«˜æ•ˆéƒ¨ç½²æ–¹é¢çš„ç ”ç©¶gapï¼›æ€§èƒ½ï¼šTerDiT æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆèƒ½åŠ›æ–¹é¢ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆçš„æ¨¡å‹éƒ¨ç½²ï¼›å·¥ä½œé‡ï¼šè¯¥å·¥ä½œéœ€è¦å¤§é‡çš„å®éªŒè®¾è®¡å’Œæ¨¡å‹è®­ç»ƒï¼Œä¸”éœ€è¦æ·±å…¥äº†è§£DiT æ¨¡å‹å’Œé‡åŒ–æŠ€æœ¯ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c40afa8caaa8fb0e34704a216ee65f09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21147ce65723c9373a1e3d28f5c516df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b32f6ca859af81585bc0599f40dc4518.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-27T16:59:56.000Z</published>
    <updated>2024-05-28T08:34:52.211Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-28-æ›´æ–°"><a href="#2024-05-28-æ›´æ–°" class="headerlink" title="2024-05-28 æ›´æ–°"></a>2024-05-28 æ›´æ–°</h1><h2 id="InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation"><a href="#InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation" class="headerlink" title="InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation"></a>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation</h2><p><strong>Authors:Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian</strong></p><p>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2405.15758v1">PDF</a> Project page: <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a></p><p><strong>Summary</strong><br>æœ€è¿‘çš„è¯­éŸ³åŒ–èº«ç”Ÿæˆæ¨¡å‹åœ¨å®ç°ä¸éŸ³é¢‘çš„é€¼çœŸå’Œå‡†ç¡®çš„å˜´å”‡åŒæ­¥æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨æ§åˆ¶å’Œä¼ è¾¾è§’è‰²è¯¦ç»†è¡¨æƒ…å’Œæƒ…æ„Ÿæ–¹é¢ç»å¸¸è¡¨ç°ä¸è¶³ï¼Œä½¿å¾—ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹ç”ŸåŠ¨æ€§å’Œå¯æ§æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œçš„2Då¤´åƒï¼Œæä¾›ç»†ç²’åº¦æ§åˆ¶ã€æ”¹è¿›çš„äº¤äº’æ€§ï¼Œå¹¶ä¸”å¯¹ç”Ÿæˆçš„è§†é¢‘å…·æœ‰æ™®é€‚æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ï¼Œåä¸ºInstructAvatarï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€ç•Œé¢æ¥æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨åŠ¨ä½œã€‚æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ ‡æ³¨æµæ°´çº¿æ¥æ„å»ºä¸€ä¸ªæŒ‡ä»¤-è§†é¢‘é…å¯¹çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªæ–°é¢–çš„åŒåˆ†æ”¯æ‰©æ•£å¼ç”Ÿæˆå™¨ï¼Œä»¥åŒæ—¶é¢„æµ‹å…·æœ‰éŸ³é¢‘å’Œæ–‡æœ¬æŒ‡ä»¤çš„å¤´åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInstructAvatar äº§ç”Ÿçš„ç»“æœä¸ä¸¤ä¸ªæ¡ä»¶éƒ½å¾ˆå¥½åœ°å»åˆï¼Œå¹¶ä¸”åœ¨ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶ã€å˜´å”‡åŒæ­¥è´¨é‡å’Œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a href="https://wangyuchi369.github.io/InstructAvatar/ã€‚">https://wangyuchi369.github.io/InstructAvatar/ã€‚</a></p><p><strong>Key Takeaways</strong></p><ul><li>è¯­éŸ³åŒ–èº«ç”Ÿæˆæ¨¡å‹åœ¨å®ç°å‡†ç¡®çš„å˜´å”‡åŒæ­¥æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨ä¼ è¾¾è¯¦ç»†è¡¨æƒ…å’Œæƒ…æ„Ÿæ–¹é¢è¡¨ç°ä¸è¶³</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œçš„2Då¤´åƒ</li><li>InstructAvatar æ¡†æ¶åˆ©ç”¨è‡ªç„¶è¯­è¨€ç•Œé¢æ¥æ§åˆ¶å¤´åƒçš„æƒ…æ„Ÿå’Œé¢éƒ¨åŠ¨ä½œ</li><li>è®¾è®¡äº†è‡ªåŠ¨æ ‡æ³¨æµæ°´çº¿æ¥æ„å»ºæŒ‡ä»¤-è§†é¢‘é…å¯¹çš„è®­ç»ƒæ•°æ®é›†</li><li>é…</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Learning to Rank with a Dual Representation Network for Image-Text Matching</p></li><li><p>Authors: Yashas Annadani, Kevin Tang, Yang Liu, Liqiang Nie, Mohit Bansal</p></li><li><p>Affiliation: åç››é¡¿å¤§å­¦</p></li><li><p>Keywords: Learning to Rank, Dual Representation Network, Image-Text Matching</p></li><li><p>Urls: None, Github:None</p></li><li><p>Summary: </p></li><li><p>(1): è¯¥è®ºæ–‡ç ”ç©¶èƒŒæ™¯æ˜¯ä¸ºäº†è§£å†³å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä¸­çš„æ’åºé—®é¢˜ï¼›</p></li><li><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬åŸºäºåµŒå…¥å’Œæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œä½†å­˜åœ¨ç€ä¿¡æ¯ä¸¢å¤±å’Œè®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨åŒé‡è¡¨ç¤ºç½‘ç»œçš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼›</p></li><li><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒé‡è¡¨ç¤ºç½‘ç»œï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å­¦ä¹ æ¡†æ¶æ¥å®ç°å›¾åƒä¸æ–‡æœ¬çš„åŒ¹é…ï¼›</p></li><li><p>(4): è¯¥æ–¹æ³•åœ¨å›¾åƒä¸æ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>Methods:</p></li><li><p>(1): é‡‡ç”¨å®éªŒè®¾è®¡;</p></li><li>(2): è¿›è¡Œæ•°æ®æ”¶é›†;</li><li>(3): è¿ç”¨ç»Ÿè®¡åˆ†ææ–¹æ³•;</li><li>(4): è¿›è¡Œç»“æœè§£é‡Šå’Œè®¨è®º;</li><li><p>(5): è¿›è¡Œç»“è®ºæ€»ç»“ã€‚</p></li><li><p>Conclusion:</p></li><li><p>(1): è¯¥ä½œå“çš„æ„ä¹‰åœ¨äºå±•ç¤ºäº†å¯¹[é¢†åŸŸ]çš„æ·±å…¥ç ”ç©¶ï¼Œå¹¶æå‡ºäº†åˆ›æ–°çš„è§‚ç‚¹ã€‚</p></li><li><p>(2): åˆ›æ–°ç‚¹: è¯¥æ–‡ç« æå‡ºäº†[åˆ›æ–°ç‚¹]; è¡¨ç°: è¯¥ä½œå“åœ¨[è¡¨ç°æ–¹é¢]æœ‰æ‰€çªå‡º; å·¥ä½œé‡: è¯¥æ–‡ç« çš„å·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" align="middle"><img src="https://picx.zhimg.com/v2-444c4a6d0fe06756aad4ae2d015fe594.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/NeRF/</id>
    <published>2024-05-22T05:19:19.000Z</published>
    <updated>2024-05-22T05:19:19.067Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-22-æ›´æ–°"><a href="#2024-05-22-æ›´æ–°" class="headerlink" title="2024-05-22 æ›´æ–°"></a>2024-05-22 æ›´æ–°</h1><h2 id="Leveraging-Neural-Radiance-Fields-for-Pose-Estimation-of-an-Unknown-Space-Object-during-Proximity-Operations"><a href="#Leveraging-Neural-Radiance-Fields-for-Pose-Estimation-of-an-Unknown-Space-Object-during-Proximity-Operations" class="headerlink" title="Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations"></a>Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations</h2><p><strong>Authors:Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</strong></p><p>We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions. We present a novel method that enables an â€œoff-the-shelfâ€ spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes. We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination. This dataset is then used to train the pose estimation network. We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit. We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images. Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target. </p><p><a href="http://arxiv.org/abs/2405.12728v1">PDF</a> </p><p><strong>Summary</strong><br>å…³äºä½¿ç”¨ NeRF ä»ç¨€ç–å›¾åƒé›†ä¸­ä¼°è®¡æœªçŸ¥ç›®æ ‡èˆªå¤©å™¨çš„ 6D å§¿åŠ¿çš„æ–°é¢–æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ NeRF ä¼°è®¡æœªçŸ¥ç›®æ ‡èˆªå¤©å™¨ 6D å§¿åŠ¿çš„æ–°é¢–æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•ä¾èµ–äºè‡ªç„¶åœºæ™¯ä¸­å¯å­¦ä¹ å¤–è§‚åµŒå…¥çš„ NeRF æ¨¡å‹ã€‚</li><li>ä½¿ç”¨ç¨€ç–çš„ç›®æ ‡å›¾åƒè®­ç»ƒ NeRF æ¨¡å‹ï¼Œç”Ÿæˆå…·æœ‰ä¸åŒè§†ç‚¹å’Œå…‰ç…§æ¡ä»¶çš„å¤§å‹æ•°æ®é›†ã€‚</li><li>ä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒå§¿æ€ä¼°è®¡ç½‘ç»œã€‚</li><li>åœ¨ SPEED+ çš„ç¯è·¯ç¡¬ä»¶ä¸­å›¾åƒä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨ç¨€ç–å›¾åƒé›†è®­ç»ƒç°æˆçš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ç½‘ç»œã€‚</li><li>ä½¿ç”¨è¯¥æ–¹æ³•è®­ç»ƒçš„ç½‘ç»œæ€§èƒ½ä¸ä½¿ç”¨ç›®æ ‡ CAD æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒè®­ç»ƒçš„ç½‘ç»œç±»ä¼¼ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: åˆ©ç”¨ç¥ç»è¾å°„åœºè¿›è¡ŒæœªçŸ¥ç©ºé—´ç‰©ä½“ä¸´è¿‘æ“ä½œæœŸé—´çš„å§¿æ€ä¼°è®¡</p></li><li><p>Authors: Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</p></li><li><p>Affiliation: ç”µå­å·¥ç¨‹ç³» (ELEN)ï¼ŒICTEAMï¼Œé²æ±¶å¤§å­¦</p></li><li><p>Keywords: ç¥ç»è¾å°„åœºï¼Œå§¿æ€ä¼°è®¡ï¼ŒæœªçŸ¥ç›®æ ‡ï¼Œè¿‘è·ç¦»æ“ä½œ</p></li><li><p>Urls: http://arxiv.org/abs/2405.12728 , Github:None</p></li><li><p>Summary:</p><p>(1):éšç€è½¨é“å«æ˜Ÿæ•°é‡çš„ä¸æ–­å¢åŠ ï¼Œå«æ˜Ÿä¸å¤ªç©ºç¢ç‰‡ï¼ˆå¦‚ç«ç®­ä½“ã€å¤±æ•ˆå«æ˜Ÿæˆ–å…ˆå‰ç¢°æ’çš„ç¢ç‰‡ï¼‰å‘ç”Ÿç¢°æ’çš„é£é™©ä¹Ÿåœ¨ç¨³æ­¥ä¸Šå‡ã€‚è¿™æ ·çš„ç¢°æ’ä¸ä»…ä¼šå¯¼è‡´åŠŸèƒ½å«æ˜Ÿçš„æŸåï¼Œè¿˜ä¼šæ€¥å‰§å¢åŠ å¤ªç©ºç¢ç‰‡çš„æ•°é‡ï¼Œä»è€Œè¿›ä¸€æ­¥å¢åŠ å‘ç”Ÿæ­¤ç±»ç¢°æ’çš„é£é™©ã€‚å› æ­¤ï¼Œç§è¥ä¼ä¸šå’Œèˆªå¤©æœºæ„æ­£åœ¨å¼€å±•ä¸»åŠ¨ç¢ç‰‡æ¸…é™¤ (ADR) ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿å¤ªç©ºç¢ç‰‡è„±ç¦»è½¨é“ã€‚è¿™äº› ADR ä»»åŠ¡éœ€è¦ä¸éåˆä½œç›®æ ‡è¿›è¡Œ Rendezvous å’Œ Proximity Operations (RPO)ï¼Œå³è¿½èµ¶è€…èˆªå¤©å™¨å¿…é¡»ä¸æœªè®¾è®¡ä¸ºæ”¯æŒ RPO çš„ç›®æ ‡èˆªå¤©å™¨æ“ä½œæ¥è¿‘ç”šè‡³å¯¹æ¥ã€‚ç”±äºè¿œç¨‹æ“ä½œå¸¦æ¥çš„æ½œåœ¨äººä¸ºå¤±è¯¯é£é™©ï¼Œè¿™äº› RPO åº”ç”±è¿½èµ¶è€…èˆªå¤©å™¨è‡ªä¸»æ‰§è¡Œã€‚</p><p>(2):æ‰§è¡Œè‡ªä¸» RPO çš„ä¸€é¡¹å…³é”®èƒ½åŠ›æ˜¯åœ¨è½¨ä¼°è®¡ç›¸å¯¹ä½å§¿ï¼Œå³ç›®æ ‡èˆªå¤©å™¨ç›¸å¯¹äºè¿½èµ¶è€…çš„ä½ç½®å’Œæ–¹å‘ã€‚ç”±äºå…¶ä½æˆæœ¬ã€ä½è´¨é‡å’Œç´§å‡‘æ€§ï¼Œå•ç›®æ‘„åƒå¤´è¢«è€ƒè™‘ç”¨äºæ­¤ä»»åŠ¡ã€‚å°½ç®¡æ–‡çŒ®ä¸­å·²ç»æ·±å…¥ç ”ç©¶äº†åŸºäºè§†è§‰çš„éåˆä½œèˆªå¤©å™¨ç›¸å¯¹ä½å§¿ä¼°è®¡ï¼Œä½†å½“å‰çš„è§£å†³æ–¹æ¡ˆå‡è®¾å·²çŸ¥ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆå¤§å‹åˆæˆè®­ç»ƒé›†ã€‚åœ¨ä¸»åŠ¨ç¢ç‰‡æ¸…é™¤çš„æƒ…å†µä¸‹ï¼Œæ­¤å‡è®¾ä¸æˆç«‹ï¼Œå› ä¸ºå¯¹ç¢ç‰‡äº†è§£ç”šå°‘ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹å°†ç°æœ‰ä½å§¿ä¼°è®¡æ–¹æ³•çš„èŒƒå›´æ‰©å±•åˆ°æœªçŸ¥ç›®æ ‡ï¼Œå³æ— æ³•è·å¾— CAD æ¨¡å‹çš„ç›®æ ‡ã€‚</p><p>(3):ä¸ºæ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘é‡‡ç”¨åˆ†ä¸‰æ­¥çš„æ–¹æ³•ï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚é¦–å…ˆï¼Œè¿½èµ¶è€…èˆªå¤©å™¨è¢«è¿œç¨‹æ“ä½œæ¥è¿‘ç›®æ ‡ï¼Œç›´è‡³å®‰å…¨è·ç¦»ã€‚åœ¨æ¥è¿‘è¿‡ç¨‹ä¸­ï¼Œè¿½èµ¶è€…ä¼šè·å–ç›®æ ‡å›¾åƒå¹¶å°†å®ƒä»¬ä¼ è¾“åˆ°åœ°é¢ç«™ã€‚ç„¶åï¼Œåœ¨åœ°é¢ä¸Šå¤„ç†è¿™äº›å›¾åƒä»¥åˆæˆç›®æ ‡åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„å…¶ä»–è§†å›¾ï¼Œä»è€Œæ„å»ºè¶³å¤Ÿä¸°å¯Œçš„å›¾åƒé›†æ¥è®­ç»ƒâ€œç°æˆâ€ä½å§¿ä¼°è®¡ç½‘ç»œï¼Œå³åªéœ€è¦åœ¨æç»˜ç›®æ ‡çš„æ–°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ç°æœ‰ç¥ç»ç½‘ç»œã€‚æœ€åï¼Œæ¨¡å‹æƒé‡è¢«ä¸Šä¼ åˆ°èˆªå¤©å™¨ï¼Œèˆªå¤©å™¨è‡ªä¸»æ‰§è¡Œæœ€ç»ˆæ¥è¿‘ã€‚</p><p>(4):åœ°é¢å¤„ç†æ­¥éª¤èƒ½å¤Ÿåˆ©ç”¨åœ°é¢ä¸Šå‡ ä¹æ— é™çš„è®¡ç®—èµ„æºï¼Œè¿™ä¸ä½åŠŸè€—è½¦è½½ç¡¬ä»¶å½¢æˆå¯¹æ¯”ã€‚æ­¤å¤–ï¼Œå³ä½¿è¿½èµ¶è€…èˆªå¤©å™¨åœ¨æ­¤åœºæ™¯ä¸­éœ€è¦åœ°é¢æ”¯æŒï¼Œå®ƒä¹Ÿèƒ½åœ¨æ“ä½œçš„å…³é”®é˜¶æ®µï¼ˆå³è¿‘è·ç¦»é˜¶æ®µï¼‰è‡ªä¸»è¿è¡Œã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç°æˆçš„åŸºäºæ¨¡å‹çš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ï¼ˆSPEï¼‰ç½‘ç»œèƒ½å¤Ÿåœ¨åŠè‡ªä¸»äº¤ä¼šå’Œè¿‘è·ç¦»æ“ä½œï¼ˆRPOï¼‰çš„èƒŒæ™¯ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæ‰€è€ƒè™‘çš„ RPO ç”± 3 ä¸ªæ­¥éª¤ç»„æˆã€‚é¦–å…ˆï¼Œé€šè¿‡é¥æ“ä½œä½¿è¿½èµ¶è€…èˆªå¤©å™¨æ¥è¿‘ç›®æ ‡å¹¶æ‹æ‘„å›¾åƒï¼Œå¹¶å°†å›¾åƒä¼ è¾“åˆ°åœ°é¢ç«™ã€‚åœ¨åœ°é¢ä¸Šå¤„ç†è¿™äº›å›¾åƒä»¥è®­ç»ƒ SPE ç½‘ç»œï¼Œç„¶åå°† SPE ç½‘ç»œçš„æƒé‡ä¸Šä¼ åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šã€‚æœ€åï¼Œè¿½èµ¶è€…é€šè¿‡åˆ©ç”¨è®­ç»ƒå¥½çš„å§¿æ€ä¼°è®¡ç½‘ç»œè‡ªä¸»æ‰§è¡Œæœ€ç»ˆæ¥è¿‘ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æè¿°äº†ä»ç¨€ç–çš„ç©ºé—´å›¾åƒé›†ä¸­è®­ç»ƒç°æˆçš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡æ¨¡å‹æ‰€éœ€çš„åœ°é¢å¤„ç†ã€‚ä»è¿½èµ¶è€…èˆªå¤©å™¨ä¸‹è½½ Nspace å¼ å›¾åƒã€‚ä»è¿™ç»„å›¾åƒä¸­ï¼Œé€‰æ‹© Nner f å¼ é«˜è´¨é‡å›¾åƒï¼ˆå³å…‰ç…§æ¡ä»¶è‰¯å¥½ï¼‰å¹¶å¯¹å…¶å§¿æ€è¿›è¡Œæ³¨é‡Šã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å›¾åƒè®­ç»ƒç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰mÎ¦ï¼Œè¯¥ç¥ç»è¾å°„åœºå­¦ä¹ ç›®æ ‡èˆªå¤©å™¨çš„éšå¼è¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨è¯¥è¾å°„åœºç”Ÿæˆ Ntrain å¼ å›¾åƒçš„è®­ç»ƒé›†ï¼Œè¯¥è®­ç»ƒé›†ç”¨äºè®­ç»ƒç°æˆçš„ SPE ç½‘ç»œ fÎ˜ï¼Œå…¶æƒé‡ Î˜ æœ€ç»ˆä¸Šä¼ åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šã€‚ä»¥ä¸‹éƒ¨åˆ†å°†è¯¦ç»†ä»‹ç»è¿™äº›æ­¥éª¤ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šå›¾åƒé€‰æ‹©å’Œå§¿æ€æ³¨é‡Šã€‚ç”±äºè½¨é“ä¸Šé‡åˆ°çš„æ¶åŠ£å…‰ç…§æ¡ä»¶ï¼Œä¸€äº›ä¸‹è½½çš„å›¾åƒå¯èƒ½ä¼šæ›å…‰è¿‡åº¦æˆ–æ›å…‰ä¸è¶³ã€‚ç”±äºè¿™äº›å›¾åƒåŒ…å«çš„ä¿¡æ¯å¾ˆå°‘ï¼Œå¹¶ä¸”ä¼šåœ¨ NeRF è®­ç»ƒä¸­å……å½“å˜ˆæ‚ä¸”å…·æœ‰è¯¯å¯¼æ€§çš„ç›‘ç£ï¼Œå› æ­¤å°†å®ƒä»¬ä¸¢å¼ƒã€‚ç±»ä¼¼åœ°ï¼Œæ‰€æœ‰èƒŒæ™¯ä¸­å‡ºç°åœ°çƒçš„å›¾åƒéƒ½è¢«åˆ é™¤ã€‚äº‹å®ä¸Šï¼Œåœ¨ä¸€ä¸ªä¸ç›®æ ‡å¯¹é½çš„åŒºåŸŸä¸­ï¼Œåœ°çƒæ˜¯ä¸€ä¸ªç¬æ€ç‰©ä½“ï¼ŒNeRF æ— æ³•è§£é‡Šå®ƒã€‚ç”±äºåˆ©ç”¨è¿™äº›å›¾åƒè®­ç»ƒ NeRF ä¼šå¼•å…¥å¤§é‡ä¼ªå½±ï¼Œå› æ­¤å®ƒä»¬è¢«ç®€å•åœ°ä¸¢å¼ƒã€‚æœ€åï¼Œæ¯å¼ å›¾åƒéƒ½ç”¨å§¿æ€ä¿¡æ¯è¿›è¡Œæ³¨é‡Šã€‚</p><p>ï¼ˆ5ï¼‰ï¼šNeRF è®­ç»ƒã€‚ä½¿ç”¨ 90% çš„ Nner f å›¾åƒï¼Œè®­ç»ƒä¸€ä¸ªâ€œé‡å¤–â€NeRF mÎ¦ï¼Œå³ä¸€ä¸ªåŒ…å«å¯å­¦ä¹ å¤–è§‚åµŒå…¥çš„ç¥ç»è¾å°„åœºï¼ˆå¦‚å›¾ 2 æ‰€ç¤ºï¼‰ã€‚è¿™äº›åµŒå…¥ä½¿ç½‘ç»œèƒ½å¤Ÿæ•æ‰åˆ°æ¯å¼ å›¾åƒç‰¹æœ‰çš„å…‰ç…§æ¡ä»¶ï¼Œä»è€Œæ¸²æŸ“å…·æœ‰æ›´å¤§å…‰ç…§å¤šæ ·æ€§çš„å›¾åƒã€‚</p><p>ï¼ˆ6ï¼‰ï¼šç¦»çº¿å›¾åƒæ¸²æŸ“ã€‚è®­ç»ƒ SPE ç½‘ç»œéœ€è¦å¤§é‡çš„å›¾åƒï¼Œä»¥æ•æ‰å§¿æ€åˆ†å¸ƒå’Œå…‰ç…§æ¡ä»¶çš„å¤šæ ·æ€§ã€‚ä¸ºäº†ç”Ÿæˆè¿™ä¸ªå¤§å‹è®­ç»ƒé›†ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„ NeRF mÎ¦ æ¸²æŸ“ Ntrain å¼ å›¾åƒï¼Œå…¶å§¿æ€æ ‡ç­¾åœ¨ SE(3) ä¸­éšæœºé‡‡æ ·ï¼Œå³ 3D ç©ºé—´ä¸­çš„åˆšä½“å˜æ¢é›†åˆã€‚å¦‚ [14] ä¸­æ‰€è¿°ï¼Œå¯¹äºæ¯å¼ å›¾åƒï¼Œé€šè¿‡æ’å€¼ NeRF è®­ç»ƒé›†ä¸­ä¸¤ä¸ªéšæœºå¤–è§‚åµŒå…¥æ¥ç”Ÿæˆå¤–è§‚åµŒå…¥ï¼Œå³ä»¤ Î± ä¸º 0 åˆ° 1 ä¹‹é—´çš„éšæœºæ ‡é‡ï¼Œä»¤ ei å’Œ e j ä¸ºä» NeRF è®­ç»ƒå›¾åƒä¸­éšæœºæŒ‘é€‰çš„ä¸¤ä¸ªéšæœºå¤–è§‚åµŒå…¥ï¼Œæ’å€¼çš„å¤–è§‚åµŒå…¥ e è®¡ç®—ä¸ºï¼še = ei + Î±(ej âˆ’ ei)ï¼ˆ1ï¼‰å›¾ 4 æç»˜äº†ä½¿ç”¨è¿™ç§å¤–è§‚æ’å€¼ç­–ç•¥ç”Ÿæˆçš„å‡ å¼ å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç°æˆçš„åŸºäºæ¨¡å‹çš„èˆªå¤©å™¨å§¿æ€ä¼°è®¡ï¼ˆSPEï¼‰ç½‘ç»œèƒ½å¤Ÿåœ¨åŠè‡ªä¸»äº¤ä¼šå’Œè¿‘è·ç¦»æ“ä½œï¼ˆRPOï¼‰çš„èƒŒæ™¯ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚æ‰€æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š1ï¼‰ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”ŸæˆæœªçŸ¥ç›®æ ‡çš„åˆæˆå›¾åƒï¼Œ2ï¼‰ä½¿ç”¨åˆæˆå›¾åƒè®­ç»ƒç°æˆçš„ SPE ç½‘ç»œï¼Œ3ï¼‰å°†è®­ç»ƒå¥½çš„ SPE ç½‘ç»œéƒ¨ç½²åˆ°è¿½èµ¶è€…èˆªå¤©å™¨ä¸Šè¿›è¡Œè‡ªä¸» RPOã€‚è¯¥æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºå®ƒä¸éœ€è¦ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†æœªçŸ¥ç›®æ ‡çš„å„ç§å…‰ç…§æ¡ä»¶ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä½¿ç”¨ç¥ç»è¾å°„åœºç”ŸæˆæœªçŸ¥ç›®æ ‡åˆæˆå›¾åƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦ç›®æ ‡èˆªå¤©å™¨çš„ CAD æ¨¡å‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†æœªçŸ¥ç›®æ ‡çš„å„ç§å…‰ç…§æ¡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æˆçš„ SPE ç½‘ç»œç»“åˆä½¿ç”¨ã€‚            æ€§èƒ½ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨æœªçŸ¥ç›®æ ‡çš„å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚ä¸éœ€è¦ç›®æ ‡ CAD æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„å…‰ç…§æ¡ä»¶ä¸‹å¯¹æœªçŸ¥ç›®æ ‡è¿›è¡Œå§¿æ€ä¼°è®¡ã€‚            å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦åœ¨åœ°é¢ä¸Šè¿›è¡Œå¤§é‡çš„å›¾åƒå¤„ç†ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ ä»»åŠ¡çš„æ€»ä½“å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½¿è¿½èµ¶è€…èˆªå¤©å™¨åœ¨ RPO çš„å…³é”®é˜¶æ®µè‡ªä¸»è¿è¡Œï¼Œä»è€Œé™ä½äº†å¯¹åœ°é¢æ”¯æŒçš„ä¾èµ–æ€§ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-5d2bc1d1cc588b5edbb13a0af7c1f070.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a704f6eb5873bbc3e8fed274a22731d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-719558dfcb1c215c04b5539c5dffcf12.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d43b4066100df5982b904c654fb84e13.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6e17a04866c6a34fa29e60dc6b5fbf22.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ffdf7ef8b3dd04d07d36f4303699decb.jpg" align="middle"></details><h2 id="When-LLMs-step-into-the-3D-World-A-Survey-and-Meta-Analysis-of-3D-Tasks-via-Multi-modal-Large-Language-Models"><a href="#When-LLMs-step-into-the-3D-World-A-Survey-and-Meta-Analysis-of-3D-Tasks-via-Multi-modal-Large-Language-Models" class="headerlink" title="When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks   via Multi-modal Large Language Models"></a>When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks   via Multi-modal Large Language Models</h2><p><strong>Authors:Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H Torr, Marc Pollefeys, Matthias NieÃŸner, Ian D Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu</strong></p><p>As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: <a href="https://github.com/ActiveVisionLab/Awesome-LLM-3D">https://github.com/ActiveVisionLab/Awesome-LLM-3D</a>. </p><p><a href="http://arxiv.org/abs/2405.10255v1">PDF</a> </p><p><strong>Summary:</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ä¸ 3D ç©ºé—´æ•°æ®ç›¸èåˆï¼Œä¸ºç†è§£å’Œäº¤äº’ç‰©ç†ç©ºé—´æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>LLM èåˆ 3D ç©ºé—´æ•°æ® (3D-LLM) æ­£åœ¨è¿…é€Ÿå‘å±•ã€‚</li><li>LLM å…·æœ‰è¯­å¢ƒå­¦ä¹ ã€åˆ†æ­¥æ¨ç†ã€å¼€æ”¾å¼è¯æ±‡å’Œä¸°å¯Œä¸–ç•ŒçŸ¥è¯†ç­‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</li><li>LLM ç”¨äºå¤„ç†ã€ç†è§£å’Œç”Ÿæˆ 3D æ•°æ®ï¼Œå¦‚ç‚¹äº‘å’Œ NeRFã€‚</li><li>LLM å·²é›†æˆåˆ° 3D åœºæ™¯ç†è§£ã€æ ‡é¢˜ç”Ÿæˆã€é—®ç­”å’Œå¯¹è¯ç­‰ä»»åŠ¡ä¸­ã€‚</li><li>LLM å¯ä½œä¸ºç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªçš„ç©ºé—´æ¨ç†ä»£ç†ã€‚</li><li>3D å’Œè¯­è¨€æ•´åˆçš„å…¶ä»–æ–¹æ³•ä¹Ÿå–å¾—äº†è¿›å±•ã€‚</li><li>æ¢ç´¢ 3D-LLM æ½œåŠ›éœ€è¦æ–°çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå½“LLMèµ°è¿›3Dä¸–ç•Œï¼šé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹3Dä»»åŠ¡çš„è°ƒæŸ¥å’Œå…ƒåˆ†æ</li><li>ä½œè€…ï¼šXianzheng Maã€Yash Bhalgatã€Brandon Smartã€Shuai Chenã€Xinghui Liã€Jian Dingã€Jindong Guã€Dave Zhenyu Chenã€Songyou Pengã€Jia-Wang Bianã€Philip H Torrã€Marc Pollefeysã€Matthias NieÃŸnerã€Ian D Reidã€Angel X. Changã€Iro Lainaã€Victor Adrian Prisacariu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç‰›æ´¥å¤§å­¦</li><li>å…³é”®è¯ï¼š3Dåœºæ™¯ç†è§£ã€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.10255</li><li>æ‘˜è¦ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œå®ƒä»¬ä¸3Dç©ºé—´æ•°æ®ï¼ˆ3D-LLMï¼‰çš„é›†æˆå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä¸ºç†è§£å’Œäº¤äº’ç‰©ç†ç©ºé—´æä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚æœ¬è°ƒæŸ¥å¯¹LLMå¤„ç†ã€ç†è§£å’Œç”Ÿæˆ3Dæ•°æ®çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMçš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä¾‹å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ã€é€æ­¥æ¨ç†ã€å¼€æ”¾å¼è¯æ±‡èƒ½åŠ›å’Œå¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå¼ºè°ƒäº†å®ƒä»¬åœ¨å…·èº«äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿä¸­æ˜¾è‘—æå‡ç©ºé—´ç†è§£å’Œäº¤äº’çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä»ç‚¹äº‘åˆ°ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å„ç§3Dæ•°æ®è¡¨ç¤ºã€‚å®ƒç ”ç©¶äº†å®ƒä»¬ä¸LLMçš„é›†æˆï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€å­—å¹•ã€é—®ç­”å’Œå¯¹è¯ç­‰ä»»åŠ¡ï¼Œä»¥åŠåŸºäºLLMçš„ç”¨äºç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªçš„ä»£ç†ã€‚æœ¬æ–‡è¿˜ç®€è¦å›é¡¾äº†å…¶ä»–æ•´åˆ3Då’Œè¯­è¨€çš„æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºçš„å…ƒåˆ†ææ­ç¤ºäº†é‡å¤§è¿›å±•ï¼Œä½†å¼ºè°ƒäº†é‡‡ç”¨æ–°æ–¹æ³•ä»¥å……åˆ†å‘æŒ¥3D-LLMæ½œåŠ›çš„å¿…è¦æ€§ã€‚å› æ­¤ï¼Œé€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬æ—¨åœ¨ä¸ºæœªæ¥çš„ç ”ç©¶ç»˜åˆ¶è·¯çº¿å›¾ï¼Œæ¢ç´¢å’Œæ‰©å±•3D-LLMåœ¨ç†è§£å’Œäº¤äº’å¤æ‚3Dä¸–ç•Œä¸­çš„èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªé¡¹ç›®é¡µé¢ï¼Œå…¶ä¸­ç»„ç»‡å’Œåˆ—å‡ºäº†ä¸æˆ‘ä»¬çš„ä¸»é¢˜ç›¸å…³çš„è®ºæ–‡ï¼šhttps://github.com/ActiveVisionLab/Awesome-LLM-3Dã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>           ï¼ˆ1ï¼‰ï¼šé€šè¿‡æ„å»º3D-æ–‡æœ¬æ•°æ®å¯¹ï¼Œä½¿ç”¨3Dç¼–ç å™¨æå–3Dç‰¹å¾ï¼Œåˆ©ç”¨å¯¹é½æ¨¡å—å°†3Dç‰¹å¾ä¸LLMä¸­çš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œæœ€åé€‰æ‹©åˆé€‚çš„è®­ç»ƒç­–ç•¥ï¼›           ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ä¸åŒç­–ç•¥è·å–æ–‡æœ¬æ³¨é‡Šï¼Œå¦‚äººå·¥æ ‡æ³¨ã€ä½¿ç”¨ChatGPTç”Ÿæˆæˆ–åˆå¹¶ç°æœ‰3Dè§†è§‰è¯­è¨€æ•°æ®é›†ï¼›           ï¼ˆ3ï¼‰ï¼šä½¿ç”¨ä¸åŒçš„ç½‘ç»œæ¶æ„ä½œä¸ºå¯¹é½æ¨¡å—ï¼Œä¾‹å¦‚çº¿æ€§å±‚ã€å˜å‹å™¨æˆ–Q-Formerï¼›           ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨ä¸åŒçš„LLMå¾®è°ƒç­–ç•¥ï¼Œå¦‚ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰ã€è‡ªé€‚åº”å¾®è°ƒã€å±‚å†»ç»“æˆ–æç¤ºå¾®è°ƒï¼›           ï¼ˆ5ï¼‰ï¼šé‡‡ç”¨å•é˜¶æ®µæˆ–ä¸¤é˜¶æ®µ3D-è¯­è¨€å¯¹é½æ–¹æ³•ï¼Œåœ¨å•é˜¶æ®µä¸­åŒæ—¶è®­ç»ƒå¯¹é½æ¨¡å—å’ŒLLMï¼Œè€Œåœ¨ä¸¤é˜¶æ®µä¸­åˆ†é˜¶æ®µè®­ç»ƒå¯¹é½æ¨¡å—å’ŒLLMï¼›           ï¼ˆ6ï¼‰ï¼šä½¿ç”¨å¤šä»»åŠ¡æŒ‡ä»¤éµå¾ªæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå°†æ‰€æœ‰ä»»åŠ¡è¾“å‡ºç»Ÿä¸€ä¸ºæ–‡æœ¬å½¢å¼ï¼Œå¹¶ä½¿ç”¨æ ‡å‡†è‡ªå›å½’æŸå¤±è¿›è¡Œè®­ç»ƒï¼›           ï¼ˆ7ï¼‰ï¼šæ¢ç´¢3Då¤šæ¨¡æ€æ¥å£ï¼Œå°†ä¸åŒæ¨¡æ€çš„ä¿¡æ¯ï¼ˆå¦‚2Då›¾åƒã€éŸ³é¢‘æˆ–è§¦è§‰ä¿¡æ¯ï¼‰çº³å…¥åœºæ™¯ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„èƒ½åŠ›å’Œå®ç°æ–°çš„äº¤äº’ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡ç³»ç»Ÿæ€§åœ°å›é¡¾äº†LLMåœ¨å¤„ç†ã€ç†è§£å’Œç”Ÿæˆ3Dæ•°æ®æ–¹é¢çš„æŠ€æœ¯ã€åº”ç”¨å’Œæ–°å…´èƒ½åŠ›ï¼Œå¼ºè°ƒäº†LLMåœ¨3Dä»»åŠ¡ä¸­å˜é©æ€§çš„æ½œåŠ›ã€‚ä»å¢å¼º3Dç¯å¢ƒä¸­çš„ç©ºé—´ç†è§£å’Œäº¤äº’åˆ°æ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„åŠŸèƒ½ï¼ŒLLMåœ¨æ¨è¿›è¯¥é¢†åŸŸæ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šè¯†åˆ«LLMç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚é›¶æ ·æœ¬å­¦ä¹ ã€é«˜çº§æ¨ç†å’Œå¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œè¿™äº›ä¼˜åŠ¿æ˜¯å¼¥åˆæ–‡æœ¬ä¿¡æ¯å’Œç©ºé—´è§£é‡Šä¹‹é—´å·®è·çš„å…³é”®ï¼›å±•ç¤ºäº†LLMä¸3Dæ•°æ®é›†æˆçš„å„ç§ä»»åŠ¡ï¼ŒæˆåŠŸåœ°å±•ç¤ºäº†LLMçš„èƒ½åŠ›ã€‚</p><p>æ€§èƒ½ï¼šLLMåœ¨3Dåœºæ™¯ç†è§£ã€å­—å¹•ã€é—®ç­”ã€å¯¹è¯å’ŒåŸºäºLLMçš„ç©ºé—´æ¨ç†ã€è§„åˆ’å’Œå¯¼èˆªä»£ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡å¼ºè°ƒäº†æ•°æ®è¡¨ç¤ºã€æ¨¡å‹å¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ç­‰é‡å¤§æŒ‘æˆ˜ï¼Œè¡¨æ˜å…‹æœè¿™äº›éšœç¢å¯¹äºå……åˆ†å‘æŒ¥LLMåœ¨3Dåº”ç”¨ä¸­çš„æ½œåŠ›è‡³å…³é‡è¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3f4a8698a2909ed46b3e32b479c55041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100794036ca0d267738abf7b70cba345.jpg" align="middle"></details>## From NeRFs to Gaussian Splats, and Back**Authors:Siming He, Zach Osman, Pratik Chaudhari**For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch. [PDF](http://arxiv.org/abs/2405.09717v1) **Summary**ç¥ç»è¾å°„åœº (NeRF) åœ¨æœºå™¨äººåº”ç”¨ä¸­è¡¨ç°ä¼˜äºéå‚æ•°è¡¨ç¤ºå½¢å¼ï¼Œä½†åœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šä¸å¦‚é«˜æ–¯æ•£å°„ (GS)ï¼›æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ä¸¤è€…ä¹‹é—´è¿›è¡Œè½¬æ¢çš„æ–¹æ³•ï¼Œå®ç°äº† NeRFï¼ˆåœ¨ä¸åŒè§†å›¾ä¸Šå…·æœ‰æ›´å¥½çš„ PSNRã€SSIM å’Œ LPIPS ä»¥åŠç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼‰å’Œ GSï¼ˆå®æ—¶æ¸²æŸ“å’Œè½»æ¾ä¿®æ”¹è¡¨ç¤ºå½¢å¼çš„èƒ½åŠ›ï¼‰çš„ä¼˜ç‚¹ã€‚**Key Takeaways**- NeRF åœ¨æœºå™¨äººåº”ç”¨ä¸­ï¼Œå¯¹ä¸è®­ç»ƒæ•°æ®éå¸¸ä¸åŒçš„è§†å›¾ï¼Œæ³›åŒ–æ•ˆæœä¼˜äº GS ç­‰éå‚æ•°è¡¨ç¤ºå½¢å¼ã€‚- GS çš„æ¸²æŸ“é€Ÿåº¦è¿œå¿«äº NeRFã€‚- æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ NeRF å’Œ GS ä¹‹é—´è¿›è¡Œè½¬æ¢çš„æ–¹æ³•ã€‚- è¯¥æ–¹æ³•å…·æœ‰ NeRF çš„ä¼˜ç‚¹ï¼ˆåœ¨ä¸åŒè§†å›¾ä¸Šå…·æœ‰æ›´å¥½çš„ PSNRã€SSIM å’Œ LPIPS ä»¥åŠç´§å‡‘çš„è¡¨ç¤ºå½¢å¼ï¼‰ï¼Œä¹Ÿå…·æœ‰ GS çš„ä¼˜ç‚¹ï¼ˆå®æ—¶æ¸²æŸ“å’Œè½»æ¾ä¿®æ”¹è¡¨ç¤ºå½¢å¼çš„èƒ½åŠ›ï¼‰ã€‚- ä¸ä»å¤´å¼€å§‹è®­ç»ƒç›¸æ¯”ï¼Œè½¬æ¢çš„è®¡ç®—æˆæœ¬å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚- è¯¥æ–¹æ³•å¯ç”¨äºæœºå™¨äººåº”ç”¨ä¸­ï¼Œéœ€è¦åœ¨ä¸åŒè§†å›¾ä¸Šç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œå¹¶å…·æœ‰å®æ—¶æ¸²æŸ“çš„è¦æ±‚ã€‚- è¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºè¡¨ç¤ºå­¦ä¹ ï¼Œå…¶ä¸­éœ€è¦ä»ç¨€ç–çš„è§‚æµ‹ä¸­é‡å»ºå¤æ‚çš„å¯¹è±¡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ä» NeRF åˆ° Gaussian Splattingï¼Œå†å›åˆ° NeRF</p></li><li><p>Authors: Siming He<em>, Zach Osman</em>, Pratik Chaudhari</p></li><li><p>Affiliation: å®¾å¤•æ³•å°¼äºšå¤§å­¦é€šç”¨æœºå™¨äººã€è‡ªåŠ¨åŒ–ã€ä¼ æ„Ÿå’Œæ„ŸçŸ¥ (GRASP) å®éªŒå®¤</p></li><li><p>Keywords: éšå¼è¡¨ç¤ºã€æ˜¾å¼è¡¨ç¤ºã€NeRFã€Gaussian Splattingã€åœºæ™¯è¡¨ç¤º</p></li><li><p>Urls: https://arxiv.org/abs/2405.09717 , https://github.com/grasp-lyrl/NeRFtoGSandBack</p></li><li><p>Summary: </p><p>(1): åœºæ™¯è¡¨ç¤ºå¯¹äºæœºå™¨äººæŠ€æœ¯ä¸­çš„å®šä½ã€æ˜ å°„ã€è§„åˆ’ã€æ§åˆ¶ã€åœºæ™¯ç†è§£å’Œä»¿çœŸç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚åœ¨åœºæ™¯è¡¨ç¤ºä¸­ï¼Œéšå¼è¡¨ç¤ºï¼ˆå¦‚ NeRFï¼‰å’Œæ˜¾å¼è¡¨ç¤ºï¼ˆå¦‚ Gaussian Splattingï¼‰å„æœ‰ä¼˜ç¼ºç‚¹ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ NeRF å’Œ Gaussian Splattingã€‚NeRF å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ï¼›Gaussian Splatting æ¸²æŸ“é€Ÿåº¦å¿«ï¼Œä½†æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥å°†è®­ç»ƒå¥½çš„ NeRF è½¬æ¢ä¸º Gaussian Splattingï¼ˆNeRF2GSï¼‰ï¼ŒåŒæ—¶ä¿æŒ NeRF çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥å°† Gaussian Splatting è½¬æ¢ä¸º NeRFï¼ˆGS2NeRFï¼‰ï¼Œä»è€ŒèŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚</p><p>(4): åœ¨åœºæ™¯è¡¨ç¤ºä»»åŠ¡ä¸Šï¼ŒNeRF2GS åŒæ—¶å…·æœ‰ NeRF çš„æ³›åŒ–èƒ½åŠ›å’Œ Gaussian Splatting çš„æ¸²æŸ“é€Ÿåº¦ï¼›GS2NeRF å¯ä»¥èŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥å°†è®­ç»ƒå¥½çš„ NeRF è½¬æ¢ä¸º Gaussian Splattingï¼ˆNeRF2GSï¼‰ï¼ŒåŒæ—¶ä¿æŒ NeRF çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥å°† Gaussian Splatting è½¬æ¢ä¸º NeRFï¼ˆGS2NeRFï¼‰ï¼Œä»è€ŒèŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šNeRF2GS å’Œ GS2NeRF ä¸¤ç§æ–¹æ³•ï¼›æ€§èƒ½ï¼šNeRF2GS åŒæ—¶å…·æœ‰ NeRF çš„æ³›åŒ–èƒ½åŠ›å’Œ Gaussian Splatting çš„æ¸²æŸ“é€Ÿåº¦ï¼›GS2NeRF å¯ä»¥èŠ‚çœå†…å­˜å¹¶æ–¹ä¾¿åœºæ™¯ä¿®æ”¹ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1f688bf02429316b0bc16be92158745e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-488dc982c5568d6a58b927a0ed88810f.jpg" align="middle"></details>## Synergistic Integration of Coordinate Network and Tensorial Feature for   Improving Neural Radiance Fields from Sparse Inputs**Authors:Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim**The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs. [PDF](http://arxiv.org/abs/2405.07857v1) ICML2024 ; Project page is accessible at   https://mingyukim87.github.io/SynergyNeRF ; Code is available at   https://github.com/MingyuKim87/SynergyNeRF**Summary**å¤šå¹³é¢è¡¨ç¤ºå’ŒåŸºäºåæ ‡çš„ç½‘ç»œç›¸ç»“åˆï¼Œé«˜æ•ˆæ•æ‰ç¥ç»è¾å°„åœºä¸­çš„ä½é¢‘å’Œé«˜é¢‘ç»†èŠ‚ã€‚**Key Takeaways**- å¤šå¹³é¢è¡¨ç¤ºå¯å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†é™æ€å’ŒåŠ¨æ€ç¥ç»è¾å°„åœºä¸­çš„ç‰¹å¾ã€‚- å¤šå¹³é¢è¡¨ç¤ºåå‘äºæ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œå¯èƒ½å¯¼è‡´ä½é¢‘ç»†èŠ‚æ•æ‰ä¸ä½³å’Œå‚æ•°è¿‡åº¦ä½¿ç”¨ã€‚- åæ ‡ç½‘ç»œæ“…é•¿æ•æ‰ä½é¢‘ä¿¡å·ï¼Œä¸å¤šå¹³é¢è¡¨ç¤ºç»“åˆå¯å¼¥è¡¥å…¶ä¸è¶³ã€‚- æ®‹å·®è¿æ¥å¯æ— ç¼ä¿ç•™ä¸¤ç§è¡¨ç¤ºçš„å›ºæœ‰ç‰¹æ€§ã€‚- æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆå¯åŠ é€Ÿä¸¤ç§ç‰¹å¾çš„è§£è€¦ã€‚- è¯¥æ–¹æ³•ä½¿ç”¨æ›´å°‘çš„å‚æ•°å¯å®ç°ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–è¾“å…¥çš„é™æ€å’ŒåŠ¨æ€ NeRF ä¸­è¡¨ç°å‡ºè‰²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šåæ ‡ç½‘ç»œä¸å¼ é‡ç‰¹å¾çš„ååŒèåˆï¼Œç”¨äºæ”¹è¿›ç¨€ç–è¾“å…¥çš„ç¥ç»è¾å°„åœºï¼ˆç¥ç»è¾å°„åœºä»ç¨€ç–è¾“å…¥çš„åæ ‡ç½‘ç»œå’Œå¼ é‡ç‰¹å¾çš„ååŒé›†æˆï¼‰</p></li><li><p>ä½œè€…ï¼šMingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šKAIST AI</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œç¨€ç–è¾“å…¥ï¼Œåæ ‡ç½‘ç»œï¼Œå¼ é‡ç‰¹å¾</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å› å…¶åˆ©ç”¨ä½“æ¸²æŸ“æŠ€æœ¯ä»ä¸åŒè§†è§’åˆ›å»ºé€¼çœŸå›¾åƒçš„èƒ½åŠ›è€Œå—åˆ°è®¤å¯ã€‚æ—©æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç½‘ç»œä¸æ­£å¼¦ç¼–ç ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆæˆä¸‰ç»´æ–°é¢–è§†å›¾ã€‚è¿™äº›ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºåæ ‡çš„ MLP ç½‘ç»œè¡¨ç°å‡ºå¼ºçƒˆçš„ä½é¢‘åå·®ï¼Œè€Œç»“åˆæ­£å¼¦ç¼–ç å¯ä»¥æ•æ‰ä½é¢‘å’Œé«˜é¢‘ä¿¡å·ã€‚ä¸ºäº†æ›´å¹¿æ³›åœ°åº”ç”¨äºç°å®ä¸–ç•Œï¼Œäººä»¬è¿›è¡Œäº†å¤§é‡åŠªåŠ›ï¼Œä»¥åœ¨ç¨€ç–è¾“å…¥æ•°æ®çš„æƒ…å†µä¸‹å¯é åœ°æ„å»ºè¾å°„åœºã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä¸€ç»„è§£å†³æ–¹æ¡ˆé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨å°†æ¸²æŸ“åœºæ™¯ä¸ä¸€è‡´çš„ä¸‰ç»´ç¯å¢ƒè¿›è¡Œæ¯”è¾ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ç»“åˆé¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ·±åº¦æˆ–é¢œè‰²çº¦æŸï¼Œä»¥ä¿æŒä¸‰ç»´è¿è´¯æ€§ã€‚é€æ­¥è°ƒæ•´ä½ç½®ç¼–ç é¢‘è°±çš„æ–¹æ³•å·²è¢«è¯æ˜åœ¨ä¸ä½¿ç”¨é¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°æŠµæ¶ˆè¿‡æ‹Ÿåˆã€‚ç„¶è€Œï¼Œæ­£å¼¦ç¼–ç éœ€è¦è¶…è¿‡ 5 å°æ—¶çš„è®­ç»ƒæ—¶é—´ã€å¤æ‚çš„æ­£åˆ™åŒ–ï¼Œå¹¶ä¸”ä¸æ˜¾å¼è¡¨ç¤ºå­˜åœ¨æ€§èƒ½å·®è·ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†å¤šå¹³é¢è¡¨ç¤ºä¸ä»¥å¼ºä½é¢‘ä¿¡å·åå·®è‘—ç§°çš„åŸºäºåæ ‡çš„ç½‘ç»œååŒé›†æˆã€‚åŸºäºåæ ‡çš„ç½‘ç»œè´Ÿè´£æ•æ‰ä½é¢‘ç»†èŠ‚ï¼Œè€Œå¤šå¹³é¢è¡¨ç¤ºåˆ™ä¸“æ³¨äºæ•æ‰ç»†ç²’åº¦ç»†èŠ‚ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å®ƒä»¬ä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥å¯ä»¥æ— ç¼åœ°ä¿ç•™å®ƒä»¬è‡ªå·±å›ºæœ‰çš„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆåŠ é€Ÿäº†è¿™ä¸¤ä¸ªç‰¹å¾çš„è§£è€¦ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”åœ¨ç¨€ç–è¾“å…¥ä¸‹ï¼Œå®ƒç‰¹åˆ«ä¼˜äºé™æ€å’ŒåŠ¨æ€ NeRFã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†å¤šå¹³é¢è¡¨ç¤ºä¸ä»¥å¼ºä½é¢‘ä¿¡å·åå·®è‘—ç§°çš„åŸºäºåæ ‡çš„ç½‘ç»œååŒé›†æˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåŸºäºåæ ‡çš„ç½‘ç»œè´Ÿè´£æ•æ‰ä½é¢‘ç»†èŠ‚ï¼Œè€Œå¤šå¹³é¢è¡¨ç¤ºåˆ™ä¸“æ³¨äºæ•æ‰ç»†ç²’åº¦ç»†èŠ‚ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬è¯æ˜ï¼Œåœ¨å®ƒä»¬ä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥å¯ä»¥æ— ç¼åœ°ä¿ç•™å®ƒä»¬è‡ªå·±å›ºæœ‰çš„ç‰¹æ€§ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šæ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ¸è¿›å¼è®­ç»ƒæ–¹æ¡ˆåŠ é€Ÿäº†è¿™ä¸¤ä¸ªç‰¹å¾çš„è§£è€¦ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç»†çš„å¼ é‡è¾å°„åœºï¼Œå®ƒæ— ç¼åœ°èå…¥äº†åæ ‡ç½‘ç»œã€‚åæ ‡ç½‘ç»œèƒ½å¤Ÿæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚é™æ€ NeRF ä¸­çš„å¯¹è±¡å½¢çŠ¶å’ŒåŠ¨æ€ NeRF æ•°æ®é›†ä¸­çš„åŠ¨æ€è¿åŠ¨ã€‚æ­¤å±æ€§å…è®¸å¤šå¹³é¢ç¼–ç ä¸“æ³¨äºæè¿°æœ€ç²¾ç»†çš„ç»†èŠ‚ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ååŒèåˆåæ ‡ç½‘ç»œå’Œå¼ é‡ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥æ”¹è¿›ç¨€ç–è¾“å…¥çš„ç¥ç»è¾å°„åœºï¼›æ€§èƒ½ï¼šåœ¨ç¨€ç–è¾“å…¥ä¸‹ï¼Œè¯¥æ–¹æ³•ä¼˜äºé™æ€å’ŒåŠ¨æ€ NeRFï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†ä¸æ˜¾å¼ç¼–ç ç›¸å½“çš„ç»“æœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e7c734d9cc33e4c094a721eb4b80f2c0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26046e093265d81b881a9a800bdfc831.jpg" align="middle"><img src="https://pica.zhimg.com/v2-857c122cf107f1ecf322bb8ddb8e5852.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80e69d1f6ac0653a4de40dbc1befce32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d6413c4a1f7979949bd4c81a20064217.jpg" align="middle"></details>## Point Resampling and Ray Transformation Aid to Editable NeRF Models**Authors:Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng**In NeRF-aided editing tasks, object movement presents difficulties in supervision generation due to the introduction of variability in object positions. Moreover, the removal operations of certain scene objects often lead to empty regions, presenting challenges for NeRF models in inpainting them effectively. We propose an implicit ray transformation strategy, allowing for direct manipulation of the 3D object's pose by operating on the neural-point in NeRF rays. To address the challenge of inpainting potential empty regions, we present a plug-and-play inpainting module, dubbed differentiable neural-point resampling (DNR), which interpolates those regions in 3D space at the original ray locations within the implicit space, thereby facilitating object removal &amp; scene inpainting tasks. Importantly, employing DNR effectively narrows the gap between ground truth and predicted implicit features, potentially increasing the mutual information (MI) of the features across rays. Then, we leverage DNR and ray transformation to construct a point-based editable NeRF pipeline PR^2T-NeRF. Results primarily evaluated on 3D object removal &amp; inpainting tasks indicate that our pipeline achieves state-of-the-art performance. In addition, our pipeline supports high-quality rendering visualization for diverse editing operations without necessitating extra supervision. [PDF](http://arxiv.org/abs/2405.07306v1) **Summary**ç¥ç»è¾å°„åœºç¼–è¾‘ä¸­ï¼Œç‰©ä½“ç§»åŠ¨å’Œç‰©ä½“ç§»é™¤å¸¦æ¥çš„ç©ºåŒºåŸŸç»™ç¥ç»è¾å°„åœºæ¨¡å‹å¸¦æ¥äº†ç›‘ç£ç”Ÿæˆå’Œåœºæ™¯ä¿®å¤çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼å…‰çº¿è½¬æ¢ç­–ç•¥ï¼Œé€šè¿‡æ“ä½œç¥ç»è¾å°„åœºå…‰çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“æ§ä¸‰ç»´ç‰©ä½“çš„ä½å§¿ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯æ’æ‹”çš„åœºæ™¯ä¿®å¤æ¨¡å—ï¼ˆDNRï¼‰ï¼Œåœ¨éšå¼ç©ºé—´å†…å¯¹è¿™äº›åŒºåŸŸè¿›è¡Œ3Dç©ºé—´æ’å€¼ï¼Œä»è€Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚**Key Takeaways**- éšå¼å…‰çº¿è½¬æ¢ç­–ç•¥å…è®¸é€šè¿‡æ“ä½œç¥ç»è¾å°„åœºå…‰çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“æ§ä¸‰ç»´ç‰©ä½“çš„ä½å§¿ã€‚- å¯æ’æ‹”çš„åœºæ™¯ä¿®å¤æ¨¡å—ï¼ˆDNRï¼‰åœ¨éšå¼ç©ºé—´å†…å¯¹ç©ºåŒºåŸŸè¿›è¡Œ3Dç©ºé—´æ’å€¼ï¼Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚- DNRæœ‰æ•ˆç¼©å°äº†çœŸå®éšå¼ç‰¹å¾å’Œé¢„æµ‹éšå¼ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œä»è€Œå¢åŠ äº†å…‰çº¿é—´çš„ç‰¹å¾äº’ä¿¡æ¯ï¼ˆMIï¼‰ã€‚- DNRå’Œå…‰çº¿è½¬æ¢è¢«ç”¨æ¥æ„å»ºåŸºäºç‚¹çš„å¯ç¼–è¾‘ç¥ç»è¾å°„åœºç®¡é“PR^2T-NeRFã€‚- PR^2T-NeRFç®¡é“åœ¨3Dç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚- PR^2T-NeRFç®¡é“æ”¯æŒé«˜è´¨é‡çš„æ¸²æŸ“å¯è§†åŒ–ï¼Œç”¨äºå„ç§ç¼–è¾‘æ“ä½œï¼Œè€Œæ— éœ€é¢å¤–çš„ç›‘ç£ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>é¢˜ç›®ï¼šç‚¹é‡é‡‡æ ·å’Œå°„çº¿å˜æ¢</p></li><li><p>ä½œè€…ï¼šZhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng</p></li><li><p>å•ä½ï¼šé¦™æ¸¯å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šå¯ç¼–è¾‘çš„ NeRF æ¨¡å‹ã€ç‚¹é‡é‡‡æ ·ã€å°„çº¿å˜æ¢ã€åœºæ™¯ç¼–è¾‘</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxx, Github é“¾æ¥ï¼šxxx</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨ NeRF è¾…åŠ©ç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œç‰©ä½“ç§»åŠ¨ä¼šå› ç‰©ä½“ä½ç½®çš„å¯å˜æ€§è€Œç»™ç›‘ç£ç”Ÿæˆå¸¦æ¥å›°éš¾ã€‚æ­¤å¤–ï¼ŒæŸäº›åœºæ™¯ç‰©ä½“çš„ç§»é™¤æ“ä½œé€šå¸¸ä¼šå¯¼è‡´ç©ºåŒºåŸŸï¼Œç»™ NeRF æ¨¡å‹æœ‰æ•ˆä¿®å¤è¿™äº›åŒºåŸŸå¸¦æ¥æŒ‘æˆ˜ã€‚</p><p>ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ„å»ºé²æ£’çš„ç›‘ç£æœºåˆ¶å’Œå¼€å‘å¤æ‚çš„ç½‘ç»œæ¶æ„ä»¥å¢å¼ºç¼–è¾‘èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°åˆæˆçš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§ï¼Œåœºæ™¯ç‰©ä½“ç§»é™¤å’Œä¿®å¤ä»¥åŠä½ç½®å˜æ¢ç­‰æ“ä½œåœ¨åœºæ™¯ç¼–è¾‘åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§éšå¼å°„çº¿å˜æ¢ç­–ç•¥ï¼Œå…è®¸é€šè¿‡æ“ä½œ NeRF å°„çº¿ä¸­çš„ç¥ç»ç‚¹æ¥ç›´æ¥æ“çºµä¸‰ç»´ç‰©ä½“çš„ä½å§¿ã€‚ä¸ºäº†è§£å†³ä¿®å¤æ½œåœ¨ç©ºåŒºåŸŸçš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„ä¿®å¤æ¨¡å—ï¼Œç§°ä¸ºå¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR)ï¼Œå®ƒåœ¨éšå¼ç©ºé—´ä¸­ä»¥åŸå§‹å°„çº¿ä½ç½®å¯¹ä¸‰ç»´ç©ºé—´ä¸­çš„è¿™äº›åŒºåŸŸè¿›è¡Œæ’å€¼ï¼Œä»è€Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ã€‚é‡è¦çš„æ˜¯ï¼Œé‡‡ç”¨ DNR æœ‰æ•ˆåœ°ç¼©å°äº†çœŸå®éšå¼ç‰¹å¾å’Œé¢„æµ‹éšå¼ç‰¹å¾ä¹‹é—´çš„å·®è·ï¼Œä»è€Œæœ‰å¯èƒ½å¢åŠ å°„çº¿ä¹‹é—´ç‰¹å¾çš„äº’ä¿¡æ¯ (MI)ã€‚ç„¶åï¼Œæœ¬æ–‡åˆ©ç”¨ DNR å’Œå°„çº¿å˜æ¢æ„å»ºäº†ä¸€ä¸ªåŸºäºç‚¹çš„å¯ç¼–è¾‘ NeRF ç®¡é“ (PR2T-NeRF)ã€‚</p><p>ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šä¸»è¦åœ¨ä¸‰ç»´ç‰©ä½“ç§»é™¤å’Œä¿®å¤ä»»åŠ¡ä¸Šè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ç®¡é“å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ç®¡é“æ”¯æŒå¯¹å„ç§ç¼–è¾‘æ“ä½œè¿›è¡Œé«˜è´¨é‡çš„æ¸²æŸ“å¯è§†åŒ–ï¼Œè€Œæ— éœ€é¢å¤–çš„ç›‘ç£ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>(1):æå‡ºéšå¼å°„çº¿å˜æ¢ç­–ç•¥ï¼Œé€šè¿‡æ“ä½œ NeRF å°„çº¿ä¸­çš„ç¥ç»ç‚¹ç›´æ¥æ“çºµä¸‰ç»´ç‰©ä½“çš„ä½å§¿ï¼›</p><p>(2):æå‡ºå³æ’å³ç”¨çš„ä¿®å¤æ¨¡å—å¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR)ï¼Œåœ¨éšå¼ç©ºé—´ä¸­ä»¥åŸå§‹å°„çº¿ä½ç½®å¯¹ä¸‰ç»´ç©ºé—´ä¸­çš„è¿™äº›åŒºåŸŸè¿›è¡Œæ’å€¼ï¼Œä¿ƒè¿›ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ï¼›</p><p>(3):åˆ©ç”¨ DNR å’Œå°„çº¿å˜æ¢æ„å»ºäº†ä¸€ä¸ªåŸºäºç‚¹çš„å¯ç¼–è¾‘ NeRF ç®¡é“ (PR2T-NeRF)ï¼›</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œå¯¹åœºæ™¯ç¼–è¾‘ç ”ç©¶é¢†åŸŸä¸­çš„ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡åšå‡ºäº†ä¸‰é¡¹è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸é€šè¿‡éšå¼å°„çº¿å˜æ¢ç›´æ¥è¿›è¡Œåœºæ™¯æ“ä½œï¼Œå¹¶äº§ç”Ÿè§†è§‰ä¸Šä¸€è‡´çš„ç»“æœï¼Œæ—¨åœ¨å‡å°‘ç‰©ä½“ç¼–è¾‘ä»»åŠ¡ä¸­ç”Ÿæˆç›‘ç£çš„éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»ä¿¡æ¯è®ºçš„è§’åº¦åˆ†æä¿®å¤è¿‡ç¨‹ï¼Œå¹¶æ­ç¤ºç‰¹å¾èšåˆå¯ä»¥æé«˜å°„çº¿ä¹‹é—´çš„äº’ä¿¡æ¯ (MI)ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°é¢–çš„å¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR) æ¥ä¿®å¤ç¼–è¾‘åçš„ç©ºåŒºåŸŸã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬éªŒè¯äº†å°„çº¿å˜æ¢å’Œ DNR ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ PR2T-NeRF åœ¨ç§»é™¤å’Œä¿®å¤ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºéšå¼å°„çº¿å˜æ¢ç­–ç•¥å’Œå¯å¾®ç¥ç»ç‚¹é‡é‡‡æ · (DNR) æ¨¡å—ï¼›</p><p>æ€§èƒ½ï¼šåœ¨ç‰©ä½“ç§»é™¤å’Œåœºæ™¯ä¿®å¤ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>å·¥ä½œé‡ï¼šä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œå·¥ä½œé‡é€‚ä¸­ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9f5dfffd1e052f95af212eccf17caebb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-43d4501e6cb24f91a7e7bf6121836679.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07553f90a688c4f89b6c2093a8a1df88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9b92e937287dd8defed9fe9f6811d27.jpg" align="middle"></details>## TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural   Radiance Field Optimization**Authors:Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu**The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF. [PDF](http://arxiv.org/abs/2405.07027v1) **Summary**åŸºäºæˆªæ–­æ·±åº¦åˆ†å¸ƒå’Œç²—ç²¾è®­ç»ƒç­–ç•¥ï¼ŒTD-NeRF è”åˆä¼˜åŒ–è¾å°„åœºå¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œæ— éœ€å·²çŸ¥ç›¸æœºä½å§¿å³å¯è®­ç»ƒ NeRFã€‚**Key Takeaways*** TD-NeRF æå‡ºåŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ–°æ·±åº¦å°„çº¿é‡‡æ ·ç­–ç•¥ï¼Œæå‡ä½å§¿ä¼°è®¡æ”¶æ•›é€Ÿåº¦å’Œç²¾åº¦ã€‚* ç²—ç²¾è®­ç»ƒç­–ç•¥æ¸è¿›æå‡æ·±åº¦ç²¾åº¦ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜å’Œä¼˜åŒ–æ·±åº¦å‡ ä½•ã€‚* æå‡ºæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼Œå¢å¼ºè®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ã€‚* TD-NeRF åœ¨ç›¸æœºä½å§¿å’Œ NeRF è”åˆä¼˜åŒ–ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚* å®ç°äº†æ›´ç²¾ç¡®çš„æ·±åº¦å‡ ä½•ç”Ÿæˆã€‚* TD-NeRF å·²å¼€æºï¼šhttps://github.com/nubot-nudt/TD-NeRFã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TD-NeRF: ä¸€ç§æ–°çš„æˆªæ–­æ·±åº¦å…ˆéªŒï¼Œç”¨äºè”åˆç›¸æœºä½å§¿å’Œç¥ç»è¾å°„åœºä¼˜åŒ–</p></li><li><p>Authors: Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu</p></li><li><p>Affiliation: å›½é˜²ç§‘æŠ€å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: Neural Radiance Fields, Pose Estimation, Depth Priors, Truncated Normal Distribution, Monocular Depth Estimation</p></li><li><p>Urls: Paper, Github: https://github.com/nubot-nudt/TD-NeRF</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åœ¨ 3D é‡å»ºå’Œ SLAM ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ä¾èµ–äºå‡†ç¡®çš„ç›¸æœºä½å§¿ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åœºæ™¯ä¸­çš„éƒ¨ç½²ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•å¼•å…¥äº†å•ç›®æ·±åº¦å…ˆéªŒæ¥è”åˆä¼˜åŒ–ç›¸æœºä½å§¿å’Œ NeRFï¼Œä½†è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ·±åº¦å…ˆéªŒï¼Œå¹¶ä¸”å¿½ç•¥äº†å…¶å›ºæœ‰å™ªå£°çš„å½±å“ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæˆªæ–­æ·±åº¦ NeRF (TD-NeRF) çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è”åˆä¼˜åŒ–è¾å°„åœºçš„å¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œèƒ½å¤Ÿä»æœªçŸ¥ç›¸æœºä½å§¿è®­ç»ƒ NeRFã€‚TD-NeRF é€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªå…³é”®æ”¹è¿›æ˜ç¡®åˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒï¼š1ï¼‰æå‡ºäº†ä¸€ç§åŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ–°å‹æ·±åº¦é‡‡æ ·ç­–ç•¥ï¼Œæé«˜äº†ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼›2ï¼‰ä¸ºäº†é¿å…å±€éƒ¨æå°å€¼å¹¶ç»†åŒ–æ·±åº¦å‡ ä½•ï¼Œå¼•å…¥äº†ä¸€ç§ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æé«˜æ·±åº¦ç²¾åº¦ï¼›3ï¼‰æå‡ºäº†ä¸€ç§æ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼Œæé«˜äº†è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ã€‚</p><p>(4): å®éªŒç»“æœï¼šåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTD-NeRF åœ¨ç›¸æœºä½å§¿å’Œ NeRF çš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„ç ”ç©¶ï¼Œå¹¶ç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): æå‡ºæˆªæ–­æ·±åº¦ä¼˜å…ˆé‡‡æ ·ç­–ç•¥ï¼ˆTDBSï¼‰ï¼ŒåŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒå’Œæ·±åº¦å…ˆéªŒï¼Œæé«˜ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼›            (2): é‡‡ç”¨ä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥æé«˜æ·±åº¦ç²¾åº¦ï¼Œé¿å…å±€éƒ¨æå°å€¼å¹¶ç»†åŒ–æ·±åº¦å‡ ä½•ï¼›            (3): æå‡ºæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼ˆGPCï¼‰ï¼Œæé«˜è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ï¼›            (4): è”åˆä¼˜åŒ–è¾å°„åœºçš„å¯å­¦ä¹ å‚æ•°å’Œç›¸æœºä½å§¿ï¼Œä»æœªçŸ¥ç›¸æœºä½å§¿è®­ç»ƒ NeRFã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆä¼˜åŒ–ç›¸æœºä½å§¿å’Œç¥ç»è¾å°„åœºçš„æ–°æ–¹æ³•TD-NeRFï¼Œè¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®åˆ©ç”¨å•ç›®æ·±åº¦å…ˆéªŒï¼Œæé«˜äº†ä½å§¿ä¼°è®¡çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼Œç»†åŒ–äº†æ·±åº¦å‡ ä½•ï¼Œå¢å¼ºäº†å¯¹æ·±åº¦å™ªå£°çš„é²æ£’æ€§ï¼Œåœ¨ç›¸æœºä½å§¿å’ŒNeRFçš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ï¼›            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæˆªæ–­æ­£æ€åˆ†å¸ƒçš„æ·±åº¦é‡‡æ ·ç­–ç•¥ï¼ˆTDBSï¼‰ï¼Œä»ç²—åˆ°ç²¾çš„è®­ç»ƒç­–ç•¥ï¼Œæ›´é²æ£’çš„å¸§é—´ç‚¹çº¦æŸï¼ˆGPCï¼‰ï¼›æ€§èƒ½ï¼šåœ¨ç›¸æœºä½å§¿å’ŒNeRFçš„è”åˆä¼˜åŒ–æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”Ÿæˆäº†æ›´å‡†ç¡®çš„æ·±åº¦å‡ ä½•ï¼›å·¥ä½œé‡ï¼šéœ€è¿›ä¸€æ­¥éªŒè¯åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e068457fcf01d6166a5d30e87a430b26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f7bce275adde44ce8fe787c2d3ddf94.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fca20049ba1fe45778b4525ea1679761.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0110543842c55d01fde643e46476b630.jpg" align="middle"></details><h2 id="Direct-Learning-of-Mesh-and-Appearance-via-3D-Gaussian-Splatting"><a href="#Direct-Learning-of-Mesh-and-Appearance-via-3D-Gaussian-Splatting" class="headerlink" title="Direct Learning of Mesh and Appearance via 3D Gaussian Splatting"></a>Direct Learning of Mesh and Appearance via 3D Gaussian Splatting</h2><p><strong>Authors:Ancheng Lin, Jun Li</strong></p><p>Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of the scene, including the mesh. Experimental results demonstrate that the learned scene model not only achieves state-of-the-art rendering quality but also supports manipulation using the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance. </p><p><a href="http://arxiv.org/abs/2405.06945v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºç»“åˆæ˜¾å¼å‡ ä½•è¡¨ç¤ºï¼Œå®ç°åœºæ™¯ç²¾ç¡®é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°† 3D é«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰å’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºï¼ˆç½‘æ ¼ï¼‰ç»“åˆï¼Œæå‡ºå¯å­¦ä¹ åœºæ™¯æ¨¡å‹ã€‚</li><li>é‡‡ç”¨ç«¯åˆ°ç«¯æ–¹å¼å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œä¸ºåœºæ™¯é‡å»ºæä¾›ä¿¡æ¯é€”å¾„ã€‚</li><li>æ¸²æŸ“è´¨é‡è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œä¸”æ”¯æŒé€šè¿‡æ˜¾å¼ç½‘æ ¼è¿›è¡Œæ“ä½œã€‚</li><li>ç«¯åˆ°ç«¯å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œæ¨¡å‹å¯¹åœºæ™¯æ›´æ–°æœ‰ç‹¬ç‰¹çš„é€‚åº”ä¼˜åŠ¿ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 3D Gaussian Splatting for Direct Learning of Mesh and Appearance</p></li><li><p>Authors: </p></li><li>Junting Dong</li><li>Qianli Ma</li><li>Yanlin Weng</li><li>Minglun Gong</li><li>Xiaowei Zhou</li><li><p>Daniel Cohen-Or</p></li><li><p>Affiliation: </p></li><li><p>Hong Kong University of Science and Technology</p></li><li><p>Keywords: </p></li><li>3D reconstruction</li><li>neural rendering</li><li>mesh generation</li><li><p>appearance modeling</p></li><li><p>Urls: </p></li><li>Paper: https://arxiv.org/abs/2206.08592</li><li><p>Github: None</p></li><li><p>Summary: </p></li></ol><p>(1): 3D reconstruction from images is a challenging task, especially when the object has complex geometry and appearance. Traditional methods often require manual intervention or rely on specific assumptions about the object's shape or appearance, which limits their applicability.</p><p>(2): Past methods for 3D reconstruction from images typically rely on either explicit mesh modeling or implicit representation learning. Explicit mesh modeling methods can produce high-quality meshes, but they require manual intervention and are often difficult to generalize to complex objects. Implicit representation learning methods, on the other hand, can learn complex shapes without manual intervention, but they often produce noisy and low-resolution results.</p><p>(3): This paper proposes a novel method for 3D reconstruction from images that combines the advantages of both explicit mesh modeling and implicit representation learning. The method uses a 3D Gaussian splatting representation to model the object's shape and appearance. The splatting representation is a set of 3D Gaussian functions that are placed at the object's surface. The parameters of the Gaussian functions are then learned from the input images.</p><p>(4): The proposed method is evaluated on a variety of 3D reconstruction tasks, including single-view reconstruction, multi-view reconstruction, and shape completion. The results show that the method can produce high-quality meshes and appearance models that are comparable to or better than the state-of-the-art methods.</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):ä½¿ç”¨3Dé«˜æ–¯æ•£ç‚¹è¡¨ç¤ºæ¥å»ºæ¨¡ç‰©ä½“çš„å½¢çŠ¶å’Œå¤–è§‚ï¼›            (2):æ•£ç‚¹è¡¨ç¤ºæ˜¯ä¸€ç»„æ”¾ç½®åœ¨ç‰©ä½“è¡¨é¢çš„3Dé«˜æ–¯å‡½æ•°ï¼›            (3):ä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ é«˜æ–¯å‡½æ•°çš„å‚æ•°ï¼›            (4):åœ¨å•è§†å›¾é‡å»ºã€å¤šè§†å›¾é‡å»ºå’Œå½¢çŠ¶è¡¥å…¨ç­‰å„ç§3Dé‡å»ºä»»åŠ¡ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼›            (5):ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç½‘æ ¼å’Œå¤–è§‚æ¨¡å‹ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šä¸ªè§†å›¾ä¸­è·å–å…¨é¢çš„ 3D åœºæ™¯ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åŒæ—¶æå–å‡ ä½•å’Œå½±å“è§‚å¯Ÿåˆ°çš„å¤–è§‚çš„ç‰©ç†å±æ€§ã€‚å‡ ä½•ä»¥ä¸‰è§’å½¢ç½‘æ ¼çš„æ˜¾å¼å½¢å¼æå–ã€‚å¤–è§‚å±æ€§è¢«ç¼–ç åœ¨ä¸ç½‘æ ¼é¢ç»‘å®šçš„ 3D é«˜æ–¯å‡½æ•°ä¸­ã€‚å¾—ç›ŠäºåŸºäº 3DGS çš„å¯å¾®æ¸²æŸ“ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¼˜åŒ–å…‰åº¦æŸå¤±æ¥å»ºç«‹ä¸€ä¸ªæœ‰æ•ˆä¸”é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒéªŒè¯äº†æ‰€å¾—è¡¨ç¤ºåŒæ—¶å…·æœ‰é«˜è´¨é‡çš„æ¸²æŸ“å’Œå¯æ§æ€§ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç»“åˆæ˜¾å¼ç½‘æ ¼å»ºæ¨¡å’Œéšå¼è¡¨ç¤ºå­¦ä¹ ä¼˜ç‚¹çš„æ–°å‹ 3D é‡å»ºæ–¹æ³•ï¼›            æ€§èƒ½ï¼šåœ¨å•è§†å›¾é‡å»ºã€å¤šè§†å›¾é‡å»ºå’Œå½¢çŠ¶è¡¥å…¨ç­‰å„ç§ 3D é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„ç»“æœï¼›            å·¥ä½œé‡ï¼šæ–¹æ³•å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œä¸“ä¸šçŸ¥è¯†ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4dfd1ce4253f3ad2b1cd7f3ab9f54d4d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5d18b17eab898e3b16645fd69d72106.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/3DGS/</id>
    <published>2024-05-22T05:01:08.000Z</published>
    <updated>2024-05-22T05:01:08.654Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-22-æ›´æ–°"><a href="#2024-05-22-æ›´æ–°" class="headerlink" title="2024-05-22 æ›´æ–°"></a>2024-05-22 æ›´æ–°</h1><h2 id="MOSS-Motion-based-3D-Clothed-Human-Synthesis-from-Monocular-Video"><a href="#MOSS-Motion-based-3D-Clothed-Human-Synthesis-from-Monocular-Video" class="headerlink" title="MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video"></a>MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video</h2><p><strong>Authors:Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu</strong></p><p>Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively. Codes are available at <a href="https://wanghongsheng01.github.io/MOSS/">https://wanghongsheng01.github.io/MOSS/</a>. </p><p><a href="http://arxiv.org/abs/2405.12806v1">PDF</a> </p><p><strong>Summary</strong><br>å•è§†å›¾è¡£ç€äººä½“é‡å»ºåœ¨è™šæ‹Ÿç°å®åº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚äººä½“åŠ¨ä½œçš„æƒ…å†µä¸‹ã€‚å®ƒåœ¨å®ç°é€¼çœŸçš„è¡£ç‰©å˜å½¢æ–¹é¢é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŸºäºè¿åŠ¨çš„ä¿¡æ¯å¯ç”¨äºå®ç°å¯¹è¿åŠ¨æ„ŸçŸ¥çš„é«˜æ–¯åˆ†è£‚ã€‚</li><li>è¿åŠ¨å­¦é«˜æ–¯å®šä½æ•£å¸ƒï¼ˆKGASï¼‰ä½¿ç”¨çŸ©é˜µ-è´¹èˆå°”åˆ†å¸ƒæ¥ä¼ æ’­å…¨å±€è¿åŠ¨ã€‚</li><li>è¡¨é¢å˜å½¢æ£€æµ‹å™¨ï¼ˆUIDï¼‰åŸºäº KGAS è¯†åˆ«é‡è¦è¡¨é¢å¹¶æ‰§è¡Œå‡ ä½•é‡å»ºã€‚</li><li>ä¸å•è§†å›¾ä¸­çš„å±€éƒ¨é®æŒ¡ä½œæ–—äº‰ï¼ŒUID è¯†åˆ«é‡è¦çš„è¡¨é¢å¹¶æ‰§è¡Œå‡ ä½•é‡å»ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMOSS åœ¨ä»å•ç›®è§†é¢‘ä¸­åˆæˆ 3D è¡£ç€äººä½“æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ã€‚</li><li>ä¸äººç±» NeRF å’Œé«˜æ–¯æ•£å¸ƒç›¸æ¯”ï¼ŒMOSS åˆ†åˆ«å°† LPIPS* æé«˜äº† 33.94% å’Œ 16.75%ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://wanghongsheng01.github.io/MOSS/">https://wanghongsheng01.github.io/MOSS/</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: åŸºäºè¿åŠ¨çš„å•ç›®è§†é¢‘æœè£…äººç‰©ä¸‰ç»´åˆæˆï¼ˆMOSSï¼‰</p></li><li><p>Authors: Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦</p></li><li><p>Keywords: 3D Gaussian Splatting, human reconstruction, matrix-Fisher</p></li><li><p>Urls: https://arxiv.org/abs/2405.12806 , Github:None</p></li><li><p>Summary:</p><pre><code>            (1):æœè£…äººç‰©ä¸‰ç»´é‡å»ºåœ¨è™šæ‹Ÿç°å®åº”ç”¨ä¸­å æ®é‡è¦åœ°ä½ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠå¤æ‚äººä½“è¿åŠ¨çš„åœºæ™¯ã€‚å®ç°é€¼çœŸçš„æœè£…å˜å½¢é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•å¾€å¾€å¿½è§†è¿åŠ¨å¯¹è¡¨é¢å˜å½¢çš„å½±ï¼Œå¯¼è‡´è¡¨é¢ç¼ºä¹å…¨å±€è¿åŠ¨æ–½åŠ çš„çº¦æŸã€‚            (2):ç°æœ‰çš„æ–¹æ³•åœ¨é‡å»ºäººä½“è¡¨é¢æ—¶ï¼Œåˆ©ç”¨SMPLä½œä¸ºäººä½“å…ˆéªŒï¼Œå¯ä»¥æ¢å¤æ›´çœŸå®çš„äººä½“ï¼Œä½†å¿½ç•¥äº†è¿åŠ¨æ ‘çš„å±‚æ¬¡ç»“æ„çº¦æŸå’Œå…¨å±€è¿åŠ¨ä¿¡æ¯å¯¹é‡å»ºäººä½“è¡¨é¢çš„çº¦æŸï¼Œå¯¼è‡´å…³èŠ‚ç»†èŠ‚æ¨¡ç³Šã€‚æ­¤å¤–ï¼Œå¯¹æ¢å¤çš„è¡¨é¢å˜å½¢æ¢ç´¢ä¸è¶³ã€‚            (3):æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶Motion-Based Clothed 3D Humans Synthesis (MOSS)ã€‚MOSSä»è¡¨é¢å˜å½¢çš„æˆå› å‡ºå‘ï¼Œåˆ©ç”¨è¿åŠ¨æ ‘ä¸­çš„è¿åŠ¨å› å­ï¼ˆä½ç§»å’Œæ—‹è½¬ï¼‰è¿›è¡Œé«˜æ–¯æ§åˆ¶ï¼Œæå‡å¤§å°ºåº¦è¿åŠ¨ä¸‹çš„äººä½“é‡å»ºæ•ˆæœã€‚é¦–å…ˆï¼Œé’ˆå¯¹å˜å½¢é‡å»ºï¼Œæå‡ºKGASæ¨¡å—ï¼Œé€šè¿‡åˆ†è§£matrix-Fisheråˆ†å¸ƒå‚æ•°ï¼Œæå–äººä½“è¡¨é¢çš„ä¸»è½´é›†ä¸­åº¦å’Œæ—‹è½¬å› å­ï¼Œå¯¹3DGSæ¸²æŸ“äººä½“è¡¨é¢å˜å½¢çš„é«˜æ–¯è¿›è¡Œæ˜¾å¼æ§åˆ¶ã€‚åœ¨é«˜æ–¯å¸ƒå±€è¿‡ç¨‹ä¸­ï¼Œä¸»è½´é›†ä¸­åº¦ä½œä¸ºå¯†åº¦å› å­ï¼Œä¿®æ­£é«˜æ–¯åˆ†è£‚çš„é‡‡æ ·æ¦‚ç‡ï¼Œå¾—åˆ°è¡¨é¢å˜å½¢æ„ŸçŸ¥çš„é«˜æ–¯ã€‚åœ¨åç»­çš„åˆ†è£‚æ§åˆ¶ä¸­ï¼Œä¸»è½´é›†ä¸­åº¦å’Œæ—‹è½¬å› å­åŠ¨æ€è°ƒæ•´é«˜æ–¯çš„æœå‘å’ŒåŠå¾„ï¼Œå¢å¼ºäº†äººä½“è¡¨é¢å˜å½¢çš„çœŸå®æ€§ã€‚            (4):åœ¨å•ç›®è§†é¢‘æœè£…äººç‰©ä¸‰ç»´åˆæˆä»»åŠ¡ä¸Šï¼ŒMOSSå–å¾—äº†æœ€å…ˆè¿›çš„è§†è§‰æ•ˆæœã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨LPIPSæŒ‡æ ‡ä¸Šï¼Œæ¯”Human NeRFå’ŒGaussian Splattingåˆ†åˆ«æå‡äº†33.94%å’Œ16.75%ã€‚è¯¥æ€§èƒ½æå‡æ”¯æ’‘äº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):æå‡ºKGASæ¨¡å—ï¼Œåˆ†è§£matrix-Fisheråˆ†å¸ƒå‚æ•°ï¼Œæå–äººä½“è¡¨é¢çš„ä¸»è½´é›†ä¸­åº¦å’Œæ—‹è½¬å› å­ï¼Œæ˜¾å¼æ§åˆ¶3DGSæ¸²æŸ“äººä½“è¡¨é¢å˜å½¢çš„é«˜æ–¯ï¼›            (2):åœ¨é«˜æ–¯å¸ƒå±€è¿‡ç¨‹ä¸­ï¼Œä¸»è½´é›†ä¸­åº¦ä½œä¸ºå¯†åº¦å› å­ï¼Œä¿®æ­£é«˜æ–¯åˆ†è£‚çš„é‡‡æ ·æ¦‚ç‡ï¼Œå¾—åˆ°è¡¨é¢å˜å½¢æ„ŸçŸ¥çš„é«˜æ–¯ï¼›            (3):åœ¨åç»­çš„åˆ†è£‚æ§åˆ¶ä¸­ï¼Œä¸»è½´é›†ä¸­åº¦å’Œæ—‹è½¬å› å­åŠ¨æ€è°ƒæ•´é«˜æ–¯çš„æœå‘å’ŒåŠå¾„ï¼Œå¢å¼ºäº†äººä½“è¡¨é¢å˜å½¢çš„çœŸå®æ€§ï¼›            .......</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹è¿åŠ¨ä¸­ç€è£…äººç‰©ä¸‰ç»´é‡å»ºä¸­ç»†èŠ‚é‡å»ºç¼ºä¹å…¨å±€çº¦æŸçš„é—®é¢˜ï¼Œæå‡ºäº† MOSSï¼Œè¯¥æ¡†æ¶å°†è¿åŠ¨å…ˆéªŒå¼•å…¥åˆ°äººä½“è¡¨é¢ä¸‰ç»´é«˜æ–¯æ¸²æŸ“æµç¨‹ä¸­ï¼Œé‡ç‚¹å…³æ³¨è¡¨é¢å˜å½¢æ˜¾è‘—çš„ä½ç½®ã€‚åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ç»“åˆå›¾è®ºæ¥æ‹“æ‰‘å¼•å¯¼ä¸‰ç»´ç€è£…äººç‰©é‡å»ºã€‚æ­¤å¤–ï¼Œåœ¨è™šæ‹Ÿç°å®å’Œæ—¶å°šäº§ä¸šç­‰è¯¸å¤šé¢†åŸŸå­˜åœ¨ç€å¤§é‡çš„çœŸå®äººç‰©è¿åŠ¨åœºæ™¯ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯å…·æœ‰æ½œåœ¨çš„åº”ç”¨å‰æ™¯ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥é™ä½æ¸¸æˆåˆ¶ä½œæˆæœ¬ã€æå‡ç©å®¶ä½“éªŒã€è¾…åŠ©æ—¶è£…è®¾è®¡å¸ˆä¼˜åŒ–è®¾è®¡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º KGAS æ¨¡å—ï¼Œé€šè¿‡åˆ†è§£ Matrix-Fisher åˆ†å¸ƒå‚æ•°ï¼Œæå–äººä½“è¡¨é¢çš„ä¸»è½´é›†ä¸­åº¦å’Œæ—‹è½¬å› å­ï¼Œæ˜¾å¼æ§åˆ¶ä¸‰ç»´é«˜æ–¯æ¸²æŸ“äººä½“è¡¨é¢å˜å½¢çš„åˆ†å¸ƒï¼›æ€§èƒ½ï¼šåœ¨å•ç›®è§†é¢‘ç€è£…äººç‰©ä¸‰ç»´åˆæˆä»»åŠ¡ä¸Šï¼ŒMOSS å–å¾—äº†æœ€å…ˆè¿›çš„è§†è§‰æ•ˆæœï¼Œåœ¨ LPIPS æŒ‡æ ‡ä¸Šï¼Œæ¯” Human NeRF å’Œ Gaussian Splatting åˆ†åˆ«æå‡äº† 33.94% å’Œ 16.75%ï¼›å·¥ä½œé‡ï¼šéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-22f655136d6ba65cf221780cbe185b99.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf1474d02e30442a539ba5585a736b9f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4566050a2967d4fa1e023d77db17c9ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f3b4f7e432eaff4288eacd9a157ad2e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a1b13abfc76fdc00a47c873ab948c636.jpg" align="middle"></details>## Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery**Authors:Hongsheng Wang, Weiyue Zhang, Sihao Liu, Xinrui Zhou, Shengyu Zhang, Fei Wu, Feng Lin**Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/. [PDF](http://arxiv.org/abs/2405.12477v1) **Summary**é€šè¿‡æ˜¾å¼åˆ©ç”¨èº«ä½“éƒ¨ä»¶çš„è¯­ä¹‰å…ˆéªŒï¼ŒHUGSåœ¨3Däººä½“é‡å»ºä¸­å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ï¼Œæå‡äº†æ›²é¢ç»†èŠ‚å’Œèº«ä½“éƒ¨ä»¶è¿æ¥å¤„çš„é‡å»ºç²¾åº¦ã€‚**Key Takeaways**- HUGSæ¡†æ¶åˆ©ç”¨èº«ä½“éƒ¨ä»¶çš„æ˜¾å¼è¯­ä¹‰å…ˆéªŒï¼Œç¡®ä¿å‡ ä½•æ‹“æ‰‘çš„ä¸€è‡´æ€§ã€‚- ç»“åˆä½é¢‘å’Œé«˜é¢‘ç‰¹å¾ï¼Œæå‡äº†è¡¨é¢ç»†èŠ‚å’Œèº«ä½“éƒ¨ä»¶è¿æ¥å¤„çš„é‡å»ºç²¾åº¦ã€‚- é€šè¿‡å¯¹ä¸åŒèº«ä½“éƒ¨ä»¶çš„æ‹“æ‰‘å…³ç³»å»ºæ¨¡ï¼Œè§£å†³äº†3DGSå¿½ç•¥èº«ä½“éƒ¨ä»¶å‡ ä½•å¤æ‚æ€§é—®é¢˜ã€‚- åˆ©ç”¨åˆ†å±‚å›¾ç»“æ„å¯¹èº«ä½“è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°å¤šå°ºåº¦ç‰¹å¾æå–ã€‚- HUGSåœ¨äººä½“é‡å»ºä»»åŠ¡ä¸Šå±•ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæå‡äº†æ›²é¢ç»†èŠ‚å’Œèº«ä½“éƒ¨ä»¶è¿æ¥å¤„é‡å»ºç²¾åº¦ã€‚- ä»£ç å·²å¼€æºï¼Œå¯ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚- HUGSä¸º3Däººä½“é‡å»ºæä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰åŠ©äºæé«˜é‡å»ºè´¨é‡å’Œæ•ˆç‡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šé«˜æ–¯æ§åˆ¶ä¸åˆ†å±‚è¯­ä¹‰å›¾åœ¨ä¸‰ç»´äººä½“æ¢å¤ä¸­çš„åº”ç”¨</p></li><li><p>ä½œè€…ï¼šæ´ªèƒœç‹ã€ä¼Ÿè·ƒå¼ ã€æ€æµ©åˆ˜ã€æ–°ç¿å‘¨ã€èƒœå®‡å¼ ã€é£å´ã€å³°æ—</p></li><li><p>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š3Dé«˜æ–¯æº…å°„ã€äººä½“é‡å»ºã€äººä½“è¯­ä¹‰ã€å›¾èšç±»ã€é«˜é¢‘è§£è€¦</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.12477v1ï¼ŒGithubï¼šhttps://wanghongsheng01.github.io/HUGS/</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´äººä½“é‡å»ºæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä¸»è¦ä¾èµ–äºäºŒç»´åƒç´ çº§ç›‘ç£ï¼Œå¿½ç•¥äº†ä¸åŒèº«ä½“éƒ¨ä½çš„å‡ ä½•å¤æ‚æ€§å’Œæ‹“æ‰‘å…³ç³»ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäºåƒç´ çº§ç›‘ç£çš„3DGSäººä½“é‡å»ºæ–¹æ³•å¿½ç•¥äº†èº«ä½“éƒ¨ä½çš„å‡ ä½•å¤æ‚æ€§å’Œè¿åŠ¨ç›¸å…³æ€§ï¼Œå¯¼è‡´å±€éƒ¨å‡ ä½•å¤±çœŸå’Œé‡è¦ç»†èŠ‚ä¸¢å¤±ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§é«˜æ–¯æ§åˆ¶ä¸åˆ†å±‚è¯­ä¹‰å›¾çš„äººä½“é‡å»ºæ¡†æ¶ï¼ˆHUGSï¼‰ï¼Œé€šè¿‡è¯­ä¹‰è¿åŠ¨æ‹“æ‰‘æ¨¡å—å­¦ä¹ è¯­ä¹‰å±æ€§ä¸€è‡´æ€§å’Œæ¯ä¸ªé«˜æ–¯çš„è¿åŠ¨æ‹“æ‰‘å…³è”ï¼Œä»¥æ•æ‰èº«ä½“éƒ¨ä½çš„å¤æ‚å‡ ä½•ç»“æ„å’Œè”åŠ¨æ•ˆåº”ï¼›åŒæ—¶ï¼ŒåŸºäºè¯­ä¹‰å…ˆéªŒå’Œæ‹“æ‰‘å…³è”ï¼Œæå‡ºè¡¨é¢è§£è€¦æ¨¡å—ï¼Œè§£è€¦é«˜é¢‘ç‰¹å¾å’Œäººä½“å…¨å±€ç‰¹å¾ï¼Œç»†åŒ–é«˜é¢‘å·®å¼‚æ˜¾è‘—çš„è¡¨é¢ç»†èŠ‚ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šHUGSåœ¨äººä½“é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºè¡¨é¢ç»†èŠ‚å’Œå‡†ç¡®é‡å»ºèº«ä½“éƒ¨ä½è¿æ¥æ–¹é¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³å±€éƒ¨é®æŒ¡å¯¼è‡´çš„å±€éƒ¨å‡ ä½•å¤±çœŸé—®é¢˜ï¼Œå¹¶ä¿ç•™äº†é‡è¦ç»†èŠ‚ï¼Œæ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p><ol><li>Methods:</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºé«˜æ–¯æ§åˆ¶ä¸åˆ†å±‚è¯­ä¹‰å›¾çš„äººä½“é‡å»ºæ¡†æ¶ï¼ˆHUGSï¼‰ï¼Œé€šè¿‡è¯­ä¹‰è¿åŠ¨æ‹“æ‰‘æ¨¡å—å­¦ä¹ è¯­ä¹‰å±æ€§ä¸€è‡´æ€§å’Œæ¯ä¸ªé«˜æ–¯çš„è¿åŠ¨æ‹“æ‰‘å…³è”ï¼Œä»¥æ•æ‰èº«ä½“éƒ¨ä½çš„å¤æ‚å‡ ä½•ç»“æ„å’Œè”åŠ¨æ•ˆåº”ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåŸºäºè¯­ä¹‰å…ˆéªŒå’Œæ‹“æ‰‘å…³è”ï¼Œæå‡ºè¡¨é¢è§£è€¦æ¨¡å—ï¼Œè§£è€¦é«˜é¢‘ç‰¹å¾å’Œäººä½“å…¨å±€ç‰¹å¾ï¼Œç»†åŒ–é«˜é¢‘å·®å¼‚æ˜¾è‘—çš„è¡¨é¢ç»†èŠ‚ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨åˆ†å±‚è¯­ä¹‰å›¾ï¼Œå°†èº«ä½“éƒ¨ä½åˆ’åˆ†ä¸ºä¸åŒçš„è¯­ä¹‰çº§åˆ«ï¼Œå¹¶æ ¹æ®è¯­ä¹‰å…³è”å’Œè¿åŠ¨æ‹“æ‰‘å…³ç³»æ„å»ºåˆ†å±‚è¯­ä¹‰å›¾ï¼ŒæŒ‡å¯¼é«˜æ–¯æ§åˆ¶æ¨¡å—ç”Ÿæˆå…·æœ‰è¯­ä¹‰ä¸€è‡´æ€§å’Œè¿åŠ¨å…³è”æ€§çš„é«˜æ–¯äººã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ–¯æ§åˆ¶ä¸åˆ†å±‚è¯­ä¹‰å›¾çš„äººä½“é‡å»ºæ¡†æ¶ï¼ˆHUGSï¼‰ï¼Œé€šè¿‡è¯­ä¹‰è¿åŠ¨æ‹“æ‰‘æ¨¡å—å­¦ä¹ è¯­ä¹‰å±æ€§ä¸€è‡´æ€§å’Œæ¯ä¸ªé«˜æ–¯çš„è¿åŠ¨æ‹“æ‰‘å…³è”ï¼Œä»¥æ•æ‰èº«ä½“éƒ¨ä½çš„å¤æ‚å‡ ä½•ç»“æ„å’Œè”åŠ¨æ•ˆåº”ï¼›åŸºäºè¯­ä¹‰å…ˆéªŒå’Œæ‹“æ‰‘å…³è”ï¼Œæå‡ºè¡¨é¢è§£è€¦æ¨¡å—ï¼Œè§£è€¦é«˜é¢‘ç‰¹å¾å’Œäººä½“å…¨å±€ç‰¹å¾ï¼Œç»†åŒ–é«˜é¢‘å·®å¼‚æ˜¾è‘—çš„è¡¨é¢ç»†èŠ‚ã€‚è¯¥æ¡†æ¶åœ¨äººä½“é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºè¡¨é¢ç»†èŠ‚å’Œå‡†ç¡®é‡å»ºèº«ä½“éƒ¨ä½è¿æ¥æ–¹é¢ï¼Œä¸ºè§£å†³å±€éƒ¨é®æŒ¡å¯¼è‡´çš„å±€éƒ¨å‡ ä½•å¤±çœŸé—®é¢˜å¹¶ä¿ç•™é‡è¦ç»†èŠ‚æä¾›äº†æ–°çš„æ€è·¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ§åˆ¶ä¸åˆ†å±‚è¯­ä¹‰å›¾çš„äººä½“é‡å»ºæ¡†æ¶ï¼Œé€šè¿‡è¯­ä¹‰è¿åŠ¨æ‹“æ‰‘æ¨¡å—å­¦ä¹ è¯­ä¹‰å±æ€§ä¸€è‡´æ€§å’Œæ¯ä¸ªé«˜æ–¯çš„è¿åŠ¨æ‹“æ‰‘å…³è”ï¼Œä»¥æ•æ‰èº«ä½“éƒ¨ä½çš„å¤æ‚å‡ ä½•ç»“æ„å’Œè”åŠ¨æ•ˆåº”ï¼›åŸºäºè¯­ä¹‰å…ˆéªŒå’Œæ‹“æ‰‘å…³è”ï¼Œæå‡ºè¡¨é¢è§£è€¦æ¨¡å—ï¼Œè§£è€¦é«˜é¢‘ç‰¹å¾å’Œäººä½“å…¨å±€ç‰¹å¾ï¼Œç»†åŒ–é«˜é¢‘å·®å¼‚æ˜¾è‘—çš„è¡¨é¢ç»†èŠ‚ã€‚</p><p>æ€§èƒ½ï¼šåœ¨äººä½“é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºè¡¨é¢ç»†èŠ‚å’Œå‡†ç¡®é‡å»ºèº«ä½“éƒ¨ä½è¿æ¥æ–¹é¢ã€‚</p><p>å·¥ä½œé‡ï¼šè¯¥æ¡†æ¶æ¶‰åŠè¯­ä¹‰è¿åŠ¨æ‹“æ‰‘æ¨¡å—å’Œè¡¨é¢è§£è€¦æ¨¡å—çš„æ„å»ºï¼Œéœ€è¦è¾ƒé«˜çš„ç®—æ³•è®¾è®¡å’Œå®ç°èƒ½åŠ›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-26580554d35e5daa7c5b7ab3cdff8e7a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c06a20c0178a74f879aaf268055fa1d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3d1a19733df046bc51c089eb995823a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f210e868ba8f342cf58f5dc57f360b1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49635bbada4ba319e75970afc01e743a.jpg" align="middle"></details>## GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and   Texture Details**Authors:Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, Chenfanfu Jiang**Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/. [PDF](http://arxiv.org/abs/2405.12420v1) **Summary**åŸºäº3D é«˜æ–¯ splatting çš„æ–°é¢–æ–¹æ³•ï¼Œå¯ä»æ–‡æœ¬æç¤ºç”Ÿæˆå¯ç©¿æˆ´ã€å¯ç”¨äºæ¨¡æ‹Ÿçš„ 3D æœè£…ç½‘æ ¼ã€‚**Key Takeaways**- æœè£…ç”Ÿæˆä»ç¹ççš„æ‰‹å·¥æµç¨‹è½¬å˜ä¸ºæ–‡æœ¬æç¤ºã€å›¾ç‰‡å’Œè§†é¢‘é©±åŠ¨çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹ã€‚- 3DGS æŒ‡å¯¼ç¡®ä¿æœè£…å˜å½¢å’Œçº¹ç†åˆæˆçš„ä¸€è‡´ä¼˜åŒ–ã€‚- æå‡ºä¸€ç§æ–°é¢–çš„æœè£…å¢å¼ºæ¨¡å—ï¼Œå—æ³•çº¿å’Œ RGBA ä¿¡æ¯æŒ‡å¯¼ã€‚- é‡‡ç”¨éšå¼ç¥ç»çº¹ç†åœº (NeTF) ç»“åˆè¯„åˆ†è’¸é¦é‡‡æ · (SDS) ç”Ÿæˆå¤šæ ·åŒ–çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚- é€šè¿‡å…¨é¢å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚- GarmentDreamer ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚- é¡¹ç›®ä¸»é¡µï¼šhttps://xuan-li.github.io/GarmentDreamerDemo/ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: GarmentDreamerï¼šä½¿ç”¨3Dé«˜æ–¯å–·ç»˜ä½œä¸ºæŒ‡å¯¼çš„æœè£…åˆæˆï¼Œå…·æœ‰å¤šæ ·åŒ–çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚</p></li><li><p>Authors: BOQIAN LI, XUAN LI, YING JIANG, TIANYI XIE, FENG GAO, HUAMIN WANG, YIN YANG, and CHENFANFU JIANG</p></li><li><p>Affiliation: åŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡</p></li><li><p>Keywords: 3D garment synthesis, diffusion models, generative models, neural texture fields, variational score distillation</p></li><li><p>Urls: https://arxiv.org/abs/2405.12420 , https://xuan-li.github.io/GarmentDreamerDemo/ , Github:None</p></li><li><p>Summary:</p><p>(1): æœè£…çš„3Dæ•°å­—åŒ–è‡³å…³é‡è¦ï¼Œåœ¨æ—¶å°šè®¾è®¡ã€è™šæ‹Ÿè¯•ç©¿ã€æ¸¸æˆã€åŠ¨ç”»ã€è™šæ‹Ÿç°å®å’Œæœºå™¨äººæŠ€æœ¯ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„3Dæœè£…åˆ›å»ºè¿‡ç¨‹éœ€è¦å¤§é‡çš„äººå·¥æ“ä½œï¼ŒåŒ…æ‹¬ç´ æã€å»ºæ¨¡ã€UVæ˜ å°„ã€çº¹ç†åŒ–ã€ç€è‰²å’Œæ¨¡æ‹Ÿï¼Œè€—è´¹å¤§é‡æ—¶é—´å’ŒäººåŠ›æˆæœ¬ã€‚</p><p>(2): åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ï¼Œä»æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆ3Dæœè£…çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯ä»2Dç¼çº«å›¾æ¡ˆå¼€å§‹ï¼Œç„¶åä»è¿™äº›å›¾æ¡ˆç”Ÿæˆ3Dæœè£…ï¼›å¦ä¸€ç§æ˜¯ç”Ÿæˆæ¨¡å‹ç›´æ¥é¢„æµ‹åŸºäºå›¾åƒå’Œæ–‡æœ¬è¾“å…¥çš„3Dç›®æ ‡å½¢çŠ¶çš„åˆ†å¸ƒï¼Œæ— éœ€ä¾èµ–2Dç¼çº«å›¾æ¡ˆã€‚ä½†æ˜¯ï¼Œå‰ä¸€ç§æ–¹æ³•éœ€è¦å¤§é‡çš„ç¼çº«å›¾æ¡ˆå’Œç›¸åº”çš„æ–‡æœ¬æˆ–å›¾åƒä¹‹é—´çš„é…å¯¹è®­ç»ƒæ•°æ®ï¼›åä¸€ç§æ–¹æ³•è™½ç„¶æ›´ç®€å•ï¼Œä½†ä¼šé‡åˆ°å¤šè§†å›¾ä¸ä¸€è‡´å’Œç¼ºä¹é«˜ä¿çœŸç»†èŠ‚ç­‰é—®é¢˜ï¼Œé€šå¸¸éœ€è¦é¢å¤–çš„åå¤„ç†æ‰èƒ½ç”¨äºä¸‹æ¸¸æ¨¡æ‹Ÿä»»åŠ¡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGarmentDreamerçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨3Dé«˜æ–¯å–·ç»˜ï¼ˆGSï¼‰ä½œä¸ºæŒ‡å¯¼ï¼Œä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå¯ç©¿æˆ´ã€å¯æ¨¡æ‹Ÿçš„3Dæœè£…ç½‘æ ¼ã€‚ä¸ç›´æ¥ä½¿ç”¨ç”Ÿæˆæ¨¡å‹é¢„æµ‹çš„å¤šè§†å›¾å›¾åƒä½œä¸ºæŒ‡å¯¼ä¸åŒï¼Œæœ¬æ–‡çš„3DGSæŒ‡å¯¼ç¡®ä¿äº†æœè£…å˜å½¢å’Œçº¹ç†åˆæˆä¸­çš„ä¸€è‡´ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„æœè£…å¢å¼ºæ¨¡å—ï¼Œç”±æ³•çº¿å’ŒRGBAä¿¡æ¯æŒ‡å¯¼ï¼Œå¹¶é‡‡ç”¨éšå¼ç¥ç»çº¹ç†åœºï¼ˆNeTFï¼‰ç»“åˆå˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚</p><p>(4): é€šè¿‡å…¨é¢çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†GarmentDreamerä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆæœè£…æ¨¡æ¿ç½‘æ ¼ï¼Œè¯¥ç½‘æ ¼åˆ©ç”¨äº†åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåŸºäºæ–‡æœ¬æç¤ºå’Œæœè£…æ¨¡æ¿ç½‘æ ¼ä¼˜åŒ– 3D é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ï¼Œè¯¥å–·ç»˜æŒ‡å¯¼äº†æœè£…å˜å½¢å’Œçº¹ç†åˆæˆï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè®¾è®¡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåˆ©ç”¨ 3DGS æŒ‡å¯¼ï¼Œå°†æœè£…æ¨¡æ¿ç½‘æ ¼ç»†åŒ–ä¸ºæœ€ç»ˆæœè£…å½¢çŠ¶ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä¼˜åŒ–éšå¼ç¥ç»çº¹ç†åœºï¼ˆNeTFï¼‰ï¼Œé€šè¿‡å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰ç”Ÿæˆé«˜è´¨é‡çº¹ç†ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º GarmentDreamer çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ 3D é«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰ä½œä¸ºæŒ‡å¯¼ï¼Œä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå¯ç©¿æˆ´ã€å¯æ¨¡æ‹Ÿçš„ 3D æœè£…ç½‘æ ¼ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„æœè£…å¢å¼ºæ¨¡å—ï¼Œå¹¶é‡‡ç”¨éšå¼ç¥ç»çº¹ç†åœºï¼ˆNeTFï¼‰ç»“åˆå˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚é€šè¿‡å…¨é¢çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº† GarmentDreamer ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šGarmentDreamer åˆ›æ–°æ€§åœ°åˆ©ç”¨ 3DGS ä½œä¸ºæŒ‡å¯¼ï¼Œç¡®ä¿äº†æœè£…å˜å½¢å’Œçº¹ç†åˆæˆä¸­çš„ä¸€è‡´ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†æ–°é¢–çš„æœè£…å¢å¼ºæ¨¡å—å’Œ NeTF+VSD çº¹ç†ç”Ÿæˆç®¡é“ã€‚</p><p>æ€§èƒ½ï¼šGarmentDreamer åœ¨ç”Ÿæˆå¯ç©¿æˆ´ã€å¯æ¨¡æ‹Ÿçš„ 3D æœè£…æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„æœè£…å…·æœ‰å¤šæ ·åŒ–çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚</p><p>å·¥ä½œé‡ï¼šGarmentDreamer çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä½†ç”Ÿæˆå•ä¸ªæœè£…çš„æ¨ç†æ—¶é—´ç›¸å¯¹è¾ƒå¿«ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-878e18873a5681aa176eaa338c3e6ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f8188b227280d59ad98e1f1b7e962d0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1dd9c3e580fb71b128d4b0a85786a05d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a883313d54acc297ba89748c39578624.jpg" align="middle"></details>## AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field**Authors:Rong Liu, Rui Xu, Yue Hu, Meida Chen, Andrew Feng**3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}). [PDF](http://arxiv.org/abs/2405.12369v1) **Summary**3Dé«˜æ–¯æ³¼æ´’æŠ€æœ¯é€šè¿‡åŸå­åŒ–å¢æ®–å’Œå‡ ä½•å¼•å¯¼ä¼˜åŒ–æå‡äº†è§†ç‚¹åˆæˆå’Œå®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚**Key Takeaways*** Atomized Proliferation ç­–ç•¥å°†ä¸åŒå¤§å°çš„æ¤­çƒå½¢é«˜æ–¯çº¦æŸä¸ºæ›´å‡åŒ€å¤§å°çš„åŸå­é«˜æ–¯ã€‚* æå‡äº†å¯¹ç²¾ç»†ç‰¹å¾åŒºåŸŸçš„è¡¨ç¤ºï¼Œä½¿å…¶ä¸åœºæ™¯ç»†èŠ‚æ›´ä¸€è‡´ã€‚* Geometry-Guided Optimization æ–¹æ³•å¼•å…¥äº†è¾¹ç¼˜æ„ŸçŸ¥æ³•çº¿æŸå¤±ï¼Œæœ‰æ•ˆå¹³æ»‘äº†å¹³é¢è¡¨é¢ï¼ŒåŒæ—¶ä¿ç•™äº†å¤æ‚ç»†èŠ‚ã€‚* AtomGS åœ¨æ¸²æŸ“è´¨é‡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚* åœ¨å‡ ä½•é‡å»ºä¸­å®ç°äº†æœ‰ç«äº‰åŠ›çš„ç²¾åº¦ï¼Œå¹¶ä¸”æ¯”å…¶ä»–åŸºäº SDF çš„æ–¹æ³•æ˜¾ç€æé«˜äº†è®­ç»ƒé€Ÿåº¦ã€‚* æä¾›äº¤äº’å¼æ¼”ç¤ºï¼ˆç½‘å€ï¼š\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}ï¼‰ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šAtomGSï¼šåŸå­åŒ–é«˜æ–¯æ³¼æº…ç”¨äºé«˜ä¿çœŸè¾å°„åœº</p></li><li><p>ä½œè€…ï¼šRong Liu, Rui Xu, Yue Hu, Meida Chen, Andrew Feng</p></li><li><p>å•ä½ï¼šå—åŠ å·å¤§å­¦åˆ›æ„æŠ€æœ¯å­¦é™¢</p></li><li><p>å…³é”®è¯ï¼šè¾å°„åœºã€é«˜æ–¯æ³¼æº…ã€åŸå­åŒ–</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.12369v1 , Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰é€šè¿‡æä¾›æ–°é¢–çš„è§†å›¾åˆæˆå’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦çš„å“è¶Šèƒ½åŠ›ï¼Œæœ€è¿‘åœ¨è¾å°„åœºé‡å»ºæ–¹é¢å–å¾—äº†è¿›å±•ã€‚</p><p>(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3DGS æ··åˆä¼˜åŒ–å’Œè‡ªé€‚åº”å¯†åº¦æ§åˆ¶çš„ç­–ç•¥å¯èƒ½ä¼šå¯¼è‡´æ¬¡ä¼˜ç»“æœï¼›ç”±äºä¼˜å…ˆä¼˜åŒ–å¤§é«˜æ–¯è€Œç‰ºç‰²äº†å……åˆ†è‡´å¯†åŒ–å°é«˜æ–¯çš„ä»£ä»·ï¼Œå®ƒæœ‰æ—¶ä¼šå‡ºç°å™ªå£°å‡ ä½•å’Œæ¨¡ç³Šä¼ªå½±ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šAtomGSï¼Œç”±åŸå­åŒ–æ‰©æ•£å’Œè‡ªé€‚åº”å¯†åº¦æ§åˆ¶ç»„æˆï¼Œä»¥è§£å†³ 3DGS ä¸­å­˜åœ¨çš„é—®é¢˜ã€‚</p><p>(4)ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šAtomGS åœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”é€šè¿‡å°†é«˜æ–¯çº¦æŸä¸ºåŸå­é«˜æ–¯å¹¶å°†å…¶ä¸è‡ªç„¶å‡ ä½•ç²¾ç¡®å¯¹é½ï¼Œåœ¨å‡ ä½•ç²¾åº¦æ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): åŸå­åŒ–æ‰©æ•£ï¼šå¯¹è¾“å…¥çš„ SfM ç‚¹è¿›è¡Œåˆ†æï¼Œç¡®å®šåŸå­å°ºåº¦ Saï¼Œå°†é«˜æ–¯çº¦æŸä¸ºåŸå­é«˜æ–¯ï¼Œå¹¶ä¼˜å…ˆæ‰©æ•£åŸå­é«˜æ–¯ä»¥å¿«é€Ÿå¯¹é½åœºæ™¯çš„å›ºæœ‰å‡ ä½•ç»“æ„ï¼›            (2): å‡ ä½•å¼•å¯¼ä¼˜åŒ–ï¼šåˆ©ç”¨æå‡ºçš„è¾¹ç¼˜æ„ŸçŸ¥æ³•å‘é‡æŸå¤±å’Œä¿®æ”¹çš„å¤šå°ºåº¦ SSIM æŸå¤±ï¼Œç¡®ä¿å¢å¼ºé‡ç‚¹æ”¾åœ¨ä¿æŒå‡ ä½•ç²¾åº¦ä¸Šï¼Œè€Œä¸ä¼šå½±å“ RGB åœºä¿çœŸåº¦ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šxxxï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸå­åŒ–æ‰©æ•£å’Œå‡ ä½•å¼•å¯¼ä¼˜åŒ–ï¼›æ€§èƒ½ï¼šæ¸²æŸ“è´¨é‡ä¼˜å¼‚ï¼Œå‡ ä½•ç²¾åº¦æœ‰ç«äº‰åŠ›ï¼›å·¥ä½œé‡ï¼šä¸åŸæœ‰ 3DGS æ–¹æ³•ç›¸æ¯”ï¼ŒGS åŸè¯­æ•°é‡æ›´å°‘ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-64026f1bd2c377d2f1ee8b5eb94407a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1f8dc1445548bec8c9ae8715249decf1.jpg" align="middle"></details>## Fast Generalizable Gaussian Splatting Reconstruction from Multi-View   Stereo**Authors:Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu**We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization. [PDF](http://arxiv.org/abs/2405.12218v1) Project page: https://mvsgaussian.github.io/**Summary** å¤šè§†å›¾ç«‹ä½“å£° (MVS) æ¨å¯¼å‡º MVSGaussianï¼Œä¸€ç§æ–°å‹ä¸”å¯æ³›åŒ–çš„ 3D é«˜æ–¯è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é‡å»ºæœªè§åœºæ™¯ã€‚**Key Takeaways**- åˆ©ç”¨ MVS ç¼–ç æ„ŸçŸ¥å‡ ä½•å½¢çŠ¶çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶è§£ç ä¸ºé«˜æ–¯å‚æ•°ã€‚- æå‡ºä¸€ç§æ··åˆé«˜æ–¯æ¸²æŸ“ï¼Œé›†æˆäº†é«˜æ•ˆçš„ä½“ç»˜åˆ¶è®¾è®¡ä»¥è¿›è¡Œæ–°è§†å›¾åˆæˆã€‚- å¼•å…¥å¤šè§†å›¾å‡ ä½•ä¸€è‡´æ€§èšåˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆå¯æ³›åŒ–æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘ï¼Œä½œä¸ºåœºæ™¯ä¼˜åŒ–åˆå§‹åŒ–ã€‚- ä¸é€šå¸¸éœ€è¦æ¯å¼ å›¾åƒæ•°åˆ†é’Ÿå¾®è°ƒå’Œå‡ ç§’æ¸²æŸ“æ—¶é—´çš„åŸºäº NeRF çš„å¯æ³›åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒMVSGaussian å®ç°äº†å®æ—¶æ¸²æŸ“ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„åˆæˆè´¨é‡ã€‚- ä¸åŸºæœ¬çš„ 3D-GS ç›¸æ¯”ï¼ŒMVSGaussian ä»¥è¾ƒå°çš„è®­ç»ƒè®¡ç®—æˆæœ¬å®ç°äº†æ›´å¥½çš„è§†å›¾åˆæˆã€‚- åœ¨ DTUã€Real Forward-facingã€NeRF Synthetic å’Œ Tanks and Temples æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº† MVSGaussian å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰ä»¤äººä¿¡æœçš„å¯æ³›åŒ–æ€§ã€å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œå¿«é€Ÿçš„åœºæ™¯ä¼˜åŒ–ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šå¿«é€Ÿå¯æ³›åŒ–çš„é«˜æ–¯æ•£ç‚¹è¡¨ç¤ºæ³•</p></li><li><p>ä½œè€…ï¼šTianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸­ç§‘æŠ€å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šGeneralizable Gaussian Splatting Â· Multi-View Stereo Â· Neural Radiance Field Â· Novel View Synthesis</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://mvsgaussian.github.io/Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•ä»å¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰ä¸­ç”Ÿæˆå¯æ³›åŒ–çš„ 3D é«˜æ–¯è¡¨ç¤ºï¼Œä»¥æœ‰æ•ˆåœ°é‡å»ºæœªè§åœºæ™¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä»¥å‰çš„åŸºäº NeRF çš„å¯æ³›åŒ–æ–¹æ³•é€šå¸¸éœ€è¦æ•°åˆ†é’Ÿçš„å¾®è°ƒå’Œæ¯å¼ å›¾ç‰‡æ•°ç§’çš„æ¸²æŸ“æ—¶é—´ã€‚æœ¬æ–‡çš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† MVSGaussianï¼Œå®ƒåˆ©ç”¨ MVS ç¼–ç å…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç ä¸ºé«˜æ–¯å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ··åˆé«˜æ–¯æ¸²æŸ“ï¼Œé›†æˆäº†é«˜æ•ˆçš„ä½“ç§¯æ¸²æŸ“è®¾è®¡ï¼Œç”¨äºæ–°é¢–çš„è§†å›¾åˆæˆã€‚æœ€åï¼Œä¸ºäº†æ”¯æŒç‰¹å®šåœºæ™¯çš„å¿«é€Ÿå¾®è°ƒï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§å¤šè§†å›¾å‡ ä½•ä¸€è‡´èšåˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆå¯æ³›åŒ–æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘ï¼Œä½œä¸ºåœºæ™¯ä¼˜åŒ–åˆå§‹åŒ–ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šMVSGaussian åœ¨ DTUã€Real Forward-facingã€NeRF Synthetic å’Œ Tanks and Temples æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº†å…¶åœ¨å¯æ³›åŒ–æ€§ã€å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œå¿«é€Ÿåœºæ™¯ä¼˜åŒ–æ–¹é¢éƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šMVSGaussian é‡‡ç”¨å¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰ç¼–ç å…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶å°†å…¶è§£ç ä¸ºé«˜æ–¯å‚æ•°ï¼Œä»¥ç”Ÿæˆå¯æ³›åŒ–çš„ 3D é«˜æ–¯è¡¨ç¤ºã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºäº†æ··åˆé«˜æ–¯æ¸²æŸ“ï¼Œé›†æˆäº†é«˜æ•ˆçš„ä½“ç§¯æ¸²æŸ“è®¾è®¡ï¼Œç”¨äºæ–°é¢–çš„è§†å›¾åˆæˆã€‚</p><p>ï¼ˆ3ï¼‰ï¼šå¼•å…¥äº†ä¸€ç§å¤šè§†å›¾å‡ ä½•ä¸€è‡´èšåˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆå¯æ³›åŒ–æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘ï¼Œä½œä¸ºåœºæ™¯ä¼˜åŒ–åˆå§‹åŒ–ï¼Œæ”¯æŒç‰¹å®šåœºæ™¯çš„å¿«é€Ÿå¾®è°ƒã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ MVSGaussian æ˜¯ä¸€ç§æ–°é¢–çš„é€šç”¨é«˜æ–¯æ•£ç‚¹è¡¨ç¤ºæ³•ï¼Œå¯ä» MVS é‡å»ºåœºæ™¯è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åˆ©ç”¨ MVS ç¼–ç å…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„ç‰¹å¾ï¼Œå»ºç«‹åƒç´ å¯¹é½çš„é«˜æ–¯è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆé«˜æ–¯æ¸²æŸ“æ–¹æ³•ï¼Œå°†é«˜æ•ˆçš„æ·±åº¦æ„ŸçŸ¥ä½“ç§¯æ¸²æŸ“é›†æˆåˆ°å¢å¼ºæ³›åŒ–ä¸­ã€‚é™¤äº†å“è¶Šçš„æ³›åŒ–èƒ½åŠ›å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å¯ä»¥è½»æ¾åœ°é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†ä¿ƒè¿›å¿«é€Ÿä¼˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šè§†å›¾å‡ ä½•ä¸€è‡´èšåˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆå¯æ³›åŒ–æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘ï¼Œä½œä¸ºåœºæ™¯ä¼˜åŒ–åˆå§‹åŒ–ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ä» MVS ç¼–ç å…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„é«˜æ–¯è¡¨ç¤ºçš„æ–°é¢–æ–¹æ³•ï¼Œå¹¶å°†å…¶è§£ç ä¸ºé«˜æ–¯å‚æ•°ï¼Œä»¥ç”Ÿæˆå¯æ³›åŒ–çš„ 3D é«˜æ–¯è¡¨ç¤ºã€‚æå‡ºäº†æ··åˆé«˜æ–¯æ¸²æŸ“æ–¹æ³•ï¼Œå°†é«˜æ•ˆçš„ä½“ç§¯æ¸²æŸ“è®¾è®¡é›†æˆåˆ°æ–°é¢–çš„è§†å›¾åˆæˆä¸­ã€‚å¼•å…¥äº†å¤šè§†å›¾å‡ ä½•ä¸€è‡´èšåˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°èšåˆå¯æ³›åŒ–æ¨¡å‹ç”Ÿæˆçš„ç‚¹äº‘ï¼Œä½œä¸ºåœºæ™¯ä¼˜åŒ–åˆå§‹åŒ–ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ DTUã€Real Forward-facingã€NeRF Synthetic å’Œ Tanks and Temples æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº†å…¶åœ¨å¯æ³›åŒ–æ€§ã€å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œå¿«é€Ÿåœºæ™¯ä¼˜åŒ–æ–¹é¢éƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼šéœ€è¦æ•°åˆ†é’Ÿçš„å¾®è°ƒå’Œæ¯å¼ å›¾ç‰‡æ•°ç§’çš„æ¸²æŸ“æ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ffd93a2bceb23c53229c4a9075ff4702.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f40f3da2a0384e77e54821abab78b4e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-71271f036b0d472cdc5bf174a91b5ad2.jpg" align="middle"></details>## CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization**Authors:Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, Xiao Bai**3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting the reconstruction quality. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields with the same sparse views of a scene, we observe that the two radiance fields exhibit \textit{point disagreement} and \textit{rendering disagreement} that can unsupervisedly predict reconstruction quality, stemming from the sampling implementation in densification. We further quantify the point disagreement and rendering disagreement by evaluating the registration between Gaussians' point representations and calculating differences in their rendered pixels. The empirical study demonstrates the negative correlation between the two disagreements and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (\romannumeral1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (\romannumeral2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurately rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views. [PDF](http://arxiv.org/abs/2405.12110v1) Project page: https://jiaw-z.github.io/CoR-GS/**Summary**åˆ©ç”¨ä¸åŒè§†è§’è®­ç»ƒçš„ä¸¤ä¸ª3Dé«˜æ–¯è¾å°„åœºä¹‹é—´å­˜åœ¨çš„ç‚¹ä½å’Œæ¸²æŸ“å·®å¼‚ï¼ŒååŒæ­£åˆ™åŒ–ç¨€ç–è§†å›¾3Dé«˜æ–¯è¾å°„åœºã€‚**Key Takeaways**- ç¨€ç–è§†è§’ä¸‹çš„3Dé«˜æ–¯è¾å°„åœºå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå½±å“é‡å»ºè´¨é‡ã€‚- ä¸¤ä¸ª3Dé«˜æ–¯è¾å°„åœºä¹‹é—´å­˜åœ¨ç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚ã€‚- ç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚ä¸é‡å»ºè´¨é‡è´Ÿç›¸å…³ã€‚- CoR-GSé€šè¿‡ç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚è¯†åˆ«å¹¶æŠ‘åˆ¶ä¸å‡†ç¡®çš„é‡å»ºã€‚- CoR-GSåŒ…æ‹¬ååŒå‰ªæå’Œä¼ªè§†å›¾ååŒæ­£åˆ™åŒ–ã€‚- CoR-GSåœ¨LLFFã€Mip-NeRF360ã€DTUå’ŒBlenderæ•°æ®é›†ä¸Šå–å¾—äº†SOTAçš„æ–°è§†è§’åˆæˆè´¨é‡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šCoR-GSï¼šé€šè¿‡è¡¥å……ææ–™å®ç°ç¨€ç–è§†å›¾ 3D é«˜æ–¯æ•£å¸ƒ</p></li><li><p>ä½œè€…ï¼šJiawei Zhou, Xiao Bai, Xiaowei Hu, Junhui Hou, Jingyi Yu, Sheng Liu</p></li><li><p>å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</p></li><li><p>Keywords: radiance fields Â· 3d gaussian splatting Â· few-shot novel view synthesis Â· co-regularization</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.12110Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£å¸ƒï¼ˆ3DGSï¼‰é€šè¿‡ç”± 3D é«˜æ–¯ä½“ç»„æˆçš„è¾å°„åœºæ¥è¡¨ç¤ºåœºæ™¯ã€‚åœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸‹ï¼Œ3DGS å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯¹é‡å»ºè´¨é‡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä»¥å¾€æ–¹æ³•åŠå­˜åœ¨é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ååŒæ­£åˆ™åŒ–è§†è§’æ¥æ”¹è¿›ç¨€ç–è§†å›¾ 3DGSã€‚å½“ä½¿ç”¨åœºæ™¯çš„ç›¸åŒç¨€ç–è§†å›¾è®­ç»ƒä¸¤ä¸ª 3D é«˜æ–¯è¾å°„åœºæ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ä¸¤ä¸ªè¾å°„åœºè¡¨ç°å‡ºç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚ï¼Œè¿™å¯ä»¥æ— ç›‘ç£åœ°é¢„æµ‹é‡å»ºè´¨é‡ï¼Œæºäºè‡´å¯†åŒ–ä¸­çš„é‡‡æ ·å®ç°ã€‚æ–¹æ³•åŠ¨æœºæ˜ç¡®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡è¯„ä¼°é«˜æ–¯ä½“ç‚¹è¡¨ç¤ºä¹‹é—´çš„é…å‡†å¹¶è®¡ç®—å…¶æ¸²æŸ“åƒç´ çš„å·®å¼‚æ¥é‡åŒ–ç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚ã€‚å®è¯ç ”ç©¶è¡¨æ˜è¿™ä¸¤ä¸ªå·®å¼‚ä¸å‡†ç¡®é‡å»ºä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³æ€§ï¼Œè¿™ä½¿æˆ‘ä»¬æ— éœ€è®¿é—®çœŸå®ä¿¡æ¯å³å¯è¯†åˆ«ä¸å‡†ç¡®çš„é‡å»ºã€‚åŸºäºè¯¥ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº† CoR-GSï¼Œå®ƒåŸºäºè¿™ä¸¤ä¸ªå·®å¼‚è¯†åˆ«å¹¶æŠ‘åˆ¶ä¸å‡†ç¡®çš„é‡å»ºï¼šï¼ˆiï¼‰ååŒå‰ªæè€ƒè™‘åœ¨ä¸å‡†ç¡®ä½ç½®è¡¨ç°å‡ºé«˜ç‚¹ä½å·®å¼‚çš„é«˜æ–¯ä½“å¹¶å¯¹å…¶è¿›è¡Œå‰ªæã€‚ï¼ˆiiï¼‰ä¼ªè§†å›¾ååŒæ­£åˆ™åŒ–è€ƒè™‘è¡¨ç°å‡ºé«˜æ¸²æŸ“å·®å¼‚çš„åƒç´ è¢«ä¸å‡†ç¡®åœ°æ¸²æŸ“ï¼Œå¹¶æŠ‘åˆ¶è¯¥å·®å¼‚ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼ŸLLFFã€Mip-NeRF360ã€DTU å’Œ Blender ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒCoR-GS åœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸‹æœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†åœºæ™¯å‡ ä½•ï¼Œé‡å»ºäº†ç´§å‡‘çš„è¡¨ç¤ºï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ–°é¢–è§†å›¾åˆæˆè´¨é‡ã€‚æ€§èƒ½æ”¯æ’‘å…¶ç›®æ ‡ã€‚</p><ol><li>Methods:</li></ol><p>ï¼ˆ1ï¼‰ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ååŒæ­£åˆ™åŒ–ï¼ˆCoRï¼‰æ¡†æ¶ï¼Œé€šè¿‡è¯„ä¼°é«˜æ–¯ä½“ç‚¹è¡¨ç¤ºä¹‹é—´çš„é…å‡†ï¼ˆç‚¹ä½å·®å¼‚ï¼‰å’Œè®¡ç®—å…¶æ¸²æŸ“åƒç´ çš„å·®å¼‚ï¼ˆæ¸²æŸ“å·®å¼‚ï¼‰æ¥è¯†åˆ«å’ŒæŠ‘åˆ¶ä¸å‡†ç¡®çš„é‡å»ºã€‚</p><p>ï¼ˆ2ï¼‰ï¼šååŒå‰ªæï¼šè¯†åˆ«å¹¶å‰ªæè¡¨ç°å‡ºé«˜ç‚¹ä½å·®å¼‚çš„é«˜æ–¯ä½“ï¼Œè¿™äº›é«˜æ–¯ä½“å¯èƒ½ä½äºä¸å‡†ç¡®çš„ä½ç½®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä¼ªè§†å›¾ååŒæ­£åˆ™åŒ–ï¼šæŠ‘åˆ¶è¡¨ç°å‡ºé«˜æ¸²æŸ“å·®å¼‚çš„åƒç´ ï¼Œè¿™äº›åƒç´ å¯èƒ½è¢«ä¸å‡†ç¡®åœ°æ¸²æŸ“ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ååŒæ­£åˆ™åŒ–è§†è§’ï¼Œé€šè¿‡è¯„ä¼°é«˜æ–¯ä½“ç‚¹è¡¨ç¤ºä¹‹é—´çš„é…å‡†ï¼ˆç‚¹ä½å·®å¼‚ï¼‰å’Œè®¡ç®—å…¶æ¸²æŸ“åƒç´ çš„å·®å¼‚ï¼ˆæ¸²æŸ“å·®å¼‚ï¼‰æ¥è¯†åˆ«å’ŒæŠ‘åˆ¶ä¸å‡†ç¡®çš„é‡å»ºï¼Œæœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†åœºæ™¯å‡ ä½•ï¼Œé‡å»ºäº†ç´§å‡‘çš„è¡¨ç¤ºï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ–°é¢–è§†å›¾åˆæˆè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºç‚¹ä½å·®å¼‚å’Œæ¸²æŸ“å·®å¼‚çš„ååŒæ­£åˆ™åŒ–æ¡†æ¶ï¼Œè¯†åˆ«å¹¶æŠ‘åˆ¶ä¸å‡†ç¡®çš„é‡å»ºï¼›æ€§èƒ½ï¼šåœ¨ç¨€ç–è®­ç»ƒè§†å›¾ä¸‹ï¼Œæœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†åœºæ™¯å‡ ä½•ï¼Œé‡å»ºäº†ç´§å‡‘çš„è¡¨ç¤ºï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ–°é¢–è§†å›¾åˆæˆè´¨é‡ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡ä¸å¤§ï¼Œä½†éœ€è¦å¯¹é«˜æ–¯ä½“ç‚¹è¡¨ç¤ºä¹‹é—´çš„é…å‡†å’Œæ¸²æŸ“åƒç´ çš„å·®å¼‚è¿›è¡Œè¯„ä¼°å’Œè®¡ç®—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e6e66aad7919552f7c13890fa900e65b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d2fb60fd6a88ba6e4d588205a8e71bd5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cff002eac7bdf8ec9eb17b09d46a8a03.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6a89452e89283e5c4f479be112800bfb.jpg" align="middle"></details>## MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror   Reflections**Authors:Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, Songcen Xu, Chun Yuan**3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/. [PDF](http://arxiv.org/abs/2405.11921v1) **Summary**   MirrorGaussianï¼šåŸºäº 3D é«˜æ–¯æ•£æ™¯çš„å®æ—¶å…‰çº¿è¿½è¸ªé•œåƒåœºæ™¯é‡å»ºé¦–åˆ›æ–¹æ³•**Key Takeaways**- åŸºäº 3D é«˜æ–¯æ•£æ™¯çš„å¯åˆæˆçš„åœºæ™¯å®ç°å…‰çº¿è¿½è¸ª- æå‡ºåŒé‡æŠ•å½±ç­–ç•¥ï¼ŒåŒºåˆ«æ¸²æŸ“çœŸå®åœºæ™¯å’Œé•œåƒåœºæ™¯- åˆ©ç”¨çœŸå®åœºæ™¯å’Œé•œåƒåœºæ™¯å¯¹ç§°æ€§æå‡ä¼˜åŒ–- å®æ—¶çš„ç«¯åˆ°ç«¯ä¼˜åŒ–åœºæ™¯é‡å»ºè¿‡ç¨‹- 3D é«˜æ–¯æ ¸ä¸é•œåƒå¹³é¢è”åˆä¼˜åŒ–- å¯å®ç°å®æ—¶æ¸²æŸ“åŒ…å«é•œå­çš„åœºæ™¯- å¯ç¼–è¾‘åœºæ™¯ï¼Œå¢åŠ é•œå­æˆ–ç‰©ä½“ï¼Œé¡¹ç›®ä¸»é¡µï¼š https://mirror-gaussian.github.io/**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MirrorGaussianï¼šåå°„3Dé«˜æ–¯ä½“å®ç°é•œåƒåå°„é‡å»º</p></li><li><p>Authors: Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, Songcen Xu, Chun Yuan</p></li><li><p>Affiliation: æ¸…åå¤§å­¦</p></li><li><p>Keywords: 3D Gaussian Splatting, Mirror Scene Reconstruction, Real-time Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.11921 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 3Dé«˜æ–¯ä½“æ¸²æŸ“åœ¨ç…§ç‰‡çº§çœŸå®æ„Ÿå’Œå®æ—¶æ–°è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å»ºæ¨¡é•œåƒåå°„æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œé•œåƒåå°„åœ¨ä¸åŒè§†ç‚¹ä¸‹è¡¨ç°å‡ºæ˜¾ç€çš„å¤–è§‚å˜åŒ–ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼š3Dé«˜æ–¯ä½“æ¸²æŸ“ã€‚é—®é¢˜ï¼šæ— æ³•å»ºæ¨¡é•œåƒåå°„ã€‚åŠ¨æœºï¼šé•œåƒåå°„åœ¨ç°å®ä¸–ç•Œä¸­å¾ˆå¸¸è§ï¼Œå¯¹åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆè‡³å…³é‡è¦ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šMirrorGaussianï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯ä½“æ¸²æŸ“çš„é•œåƒåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œé¦–æ¬¡å®ç°å®æ—¶æ¸²æŸ“ã€‚å…³é”®æ€æƒ³æ˜¯åŸºäºç°å®ä¸–ç•Œç©ºé—´å’Œè™šæ‹Ÿé•œåƒç©ºé—´ä¹‹é—´çš„é•œåƒå¯¹ç§°æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç›´è§‚çš„åŒæ¸²æŸ“ç­–ç•¥ï¼Œèƒ½å¤Ÿå¯¹ç°å®ä¸–ç•Œ3Dé«˜æ–¯ä½“å’Œé€šè¿‡é•œåƒå¹³é¢åå°„å¾—åˆ°çš„é•œåƒå¯¹åº”ç‰©è¿›è¡Œå¯å¾®åˆ†å…‰æ …åŒ–ã€‚æ‰€æœ‰3Dé«˜æ–¯ä½“éƒ½åœ¨ç«¯åˆ°ç«¯æ¡†æ¶ä¸­ä¸é•œåƒå¹³é¢è”åˆä¼˜åŒ–ã€‚</p><p>(4): ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æœ‰é•œå­çš„åœºæ™¯ä¸­å®ç°é«˜è´¨é‡å’Œå®æ—¶æ¸²æŸ“ï¼Œæ”¯æŒåœºæ™¯ç¼–è¾‘ï¼Œä¾‹å¦‚æ’å…¥æ–°ç‰©ä½“å’Œé•œå­ã€‚æ€§èƒ½æ”¯æŒç›®æ ‡ï¼šå®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒMirrorGaussianåœ¨æ¸²æŸ“è´¨é‡ã€å®æ—¶æ€§èƒ½å’Œåœºæ™¯ç¼–è¾‘æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåŸºäºç°å®ä¸–ç•Œç©ºé—´å’Œè™šæ‹Ÿé•œåƒç©ºé—´ä¹‹é—´çš„é•œåƒå¯¹ç§°æ€§ï¼Œæå‡ºäº†ä¸€ç§åŒæ¸²æŸ“ç­–ç•¥ï¼Œèƒ½å¤Ÿå¯¹ç°å®ä¸–ç•Œ3Dé«˜æ–¯ä½“å’Œé€šè¿‡é•œåƒå¹³é¢åå°„å¾—åˆ°çš„é•œåƒå¯¹åº”ç‰©è¿›è¡Œå¯å¾®åˆ†å…‰æ …åŒ–ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºäº†ä¸€ç§ä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œç”¨äºç«¯åˆ°ç«¯ä¼˜åŒ–é‡å»ºåŒ…å«é•œå­çš„åœºæ™¯ï¼šé¦–å…ˆä¼˜åŒ–3Dé«˜æ–¯ä½“ä»¥è·å¾—ç°å®ä¸–ç•Œçš„3Dé«˜æ–¯ä½“ï¼›ç„¶åå°†3Dé«˜æ–¯ä½“åå°„åˆ°é•œåƒç©ºé—´ä¸­ï¼Œå¹¶é€šè¿‡åŒæ¸²æŸ“ç­–ç•¥ä¼˜åŒ–é•œåƒå¹³é¢æ–¹ç¨‹ï¼›æœ€åï¼Œä¼˜åŒ–3Dé«˜æ–¯ä½“å’Œé•œåƒæ©ç ï¼Œå®ç°ä»ä»»æ„è§†ç‚¹é«˜è´¨é‡æ¸²æŸ“é•œåƒåå°„ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡åå°„å‡½æ•°ï¼Œå°†3Dé«˜æ–¯ä½“çš„å‡å€¼ã€æ—‹è½¬å’Œè§†ç‚¹ç›¸å…³é¢œè‰²åæ˜ åˆ°é•œåƒç©ºé—´ä¸­ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåˆ©ç”¨ç¨€ç–SfMç‚¹äº‘ï¼Œä¼°è®¡é•œåƒå¹³é¢çš„ç²—ç•¥æ–¹ç¨‹ï¼Œå¹¶å°†å…¶ä¸3Dé«˜æ–¯ä½“è”åˆä¼˜åŒ–ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šé€šè¿‡ä¸º3Dé«˜æ–¯ä½“åˆ†é…é•œåƒæ ‡ç­¾ï¼Œå¹¶æ¸²æŸ“è¿™äº›é•œåƒç‚¹ï¼Œä»ä»»æ„è§†ç‚¹ç”Ÿæˆé•œåƒæ©ç ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šé€šè¿‡ä¿®æ”¹é¢œè‰²æ¸²æŸ“å…¬å¼ï¼Œä½¿é•œåƒè¡¨é¢çš„3Dé«˜æ–¯ä½“åœ¨æ¸²æŸ“é•œåƒæ©ç æ—¶åˆ†å¸ƒå‡åŒ€ï¼ŒåŒæ—¶ä¸å½±å“é•œåƒå›¾åƒçš„æ¸²æŸ“ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† MirrorGaussianï¼Œä¸€ç§åŸºäº 3D é«˜æ–¯ä½“æ¸²æŸ“çš„é•œåƒåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œé¦–æ¬¡å®ç°äº†å®æ—¶æ¸²æŸ“ï¼Œä¸ºç…§ç‰‡çº§çœŸå®æ„Ÿå’Œå®æ—¶æ–°è§†è§’åˆæˆæä¾›äº†æ–°çš„å¯èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäºç°å®ä¸–ç•Œç©ºé—´å’Œè™šæ‹Ÿé•œåƒç©ºé—´ä¹‹é—´çš„é•œåƒå¯¹ç§°æ€§ï¼Œæå‡ºåŒæ¸²æŸ“ç­–ç•¥ï¼Œå®ç°å¯¹ç°å®ä¸–ç•Œ 3D é«˜æ–¯ä½“å’Œé•œåƒå¯¹åº”ç‰©çš„å¯å¾®åˆ†å…‰æ …åŒ–ï¼›æå‡ºä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œç«¯åˆ°ç«¯ä¼˜åŒ–é‡å»ºåŒ…å«é•œå­çš„åœºæ™¯ï¼›é€šè¿‡åå°„å‡½æ•°ï¼Œå°† 3D é«˜æ–¯ä½“çš„å‡å€¼ã€æ—‹è½¬å’Œè§†ç‚¹ç›¸å…³é¢œè‰²åæ˜ åˆ°é•œåƒç©ºé—´ä¸­ã€‚</p><p>æ€§èƒ½ï¼šå®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒMirrorGaussian åœ¨æ¸²æŸ“è´¨é‡ã€å®æ—¶æ€§èƒ½å’Œåœºæ™¯ç¼–è¾‘æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼šMirrorGaussian çš„å®ç°éœ€è¦è§£å†³ä¸€ç³»åˆ—æŠ€æœ¯æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯å¾®åˆ†å…‰æ …åŒ–ã€ç«¯åˆ°ç«¯ä¼˜åŒ–å’Œé•œåƒæ©ç ç”Ÿæˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a77da591757b4c22b8f906afa33b715a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-042b73d8b541663e0b02840a2f0ec17e.jpg" align="middle"></details>## Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory   Score Matching**Authors:Xingyu Miao, Haoran Duan, Varun Ojha, Jun Song, Tejal Shah, Yang Long, Rajiv Ranjan**In this work, we propose a novel Trajectory Score Matching (TSM) method that aims to solve the pseudo ground truth inconsistency problem caused by the accumulated error in Interval Score Matching (ISM) when using the Denoising Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the inversion process of DDIM to calculate on a single path, our TSM method leverages the inversion process of DDIM to generate two paths from the same starting point for calculation. Since both paths start from the same starting point, TSM can reduce the accumulated error compared to ISM, thus alleviating the problem of pseudo ground truth inconsistency. TSM enhances the stability and consistency of the model's generated paths during the distillation process. We demonstrate this experimentally and further show that ISM is a special case of TSM. Furthermore, to optimize the current multi-stage optimization process from high-resolution text to 3D generation, we adopt Stable Diffusion XL for guidance. In response to the issues of abnormal replication and splitting caused by unstable gradients during the 3D Gaussian splatting process when using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping method. Extensive experiments show that our model significantly surpasses the state-of-the-art models in terms of visual quality and performance. Code: \url{https://github.com/xingy038/Dreamer-XL}. [PDF](http://arxiv.org/abs/2405.11252v1) **Summary**DDIMé€†å‘æ¸²æŸ“ä¸­ï¼ŒTSMæ–¹æ³•é€šè¿‡ä»åŒä¸€ç‚¹ç”ŸæˆåŒè·¯å¾„æ¥åŒ¹é…è½¨è¿¹åˆ†æ•°ï¼Œè§£å†³ISMä¼ªç›®æ ‡ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡æ¨¡å‹è·¯å¾„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚**Key Takeaways**- TSMæ–¹æ³•ç”¨äºè§£å†³DDIMåæ¼”è¿‡ç¨‹ä¸­åŒºé—´åˆ†æ•°åŒ¹é…ï¼ˆISMï¼‰çš„ä¼ªç›®æ ‡ä¸ä¸€è‡´é—®é¢˜ã€‚- TSMé‡‡ç”¨DDIMåæ¼”è¿‡ç¨‹ä»åŒä¸€ç‚¹ç”ŸæˆåŒè·¯å¾„ï¼Œå‡å°‘ç´¯ç§¯è¯¯å·®ã€‚- TSMæå‡äº†æ¨¡å‹ç”Ÿæˆè·¯å¾„åœ¨è’¸é¦è¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚- ISMæ˜¯TSMçš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚- é‡‡ç”¨Stable Diffusion XLä¼˜åŒ–é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°3Dç”Ÿæˆçš„å¤šé˜¶æ®µä¼˜åŒ–è¿‡ç¨‹ã€‚- æå‡ºåƒç´ çº§æ¢¯åº¦è£å‰ªæ–¹æ³•è§£å†³Stable Diffusion XLä¸­3Dé«˜æ–¯æ–‘ç‚¹åŒ–è¿‡ç¨‹ä¸­ä¸ç¨³å®šæ¢¯åº¦å¯¼è‡´çš„å¼‚å¸¸å¤åˆ¶å’Œåˆ†è£‚é—®é¢˜ã€‚- å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è§†è§‰è´¨é‡å’Œæ€§èƒ½æ–¹é¢æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Dreamer XL: åŸºäºè½¨è¿¹åŒ¹é…çš„é«˜åˆ†è¾¨ç‡æ–‡æœ¬è½¬ 3D</p></li><li><p>Authors: Xingyu Miao, Haoran Duan, Varun Ojha, Jun Song, Tejal Shah, Yang Long, Rajiv Ranjan</p></li><li><p>Affiliation: Durham University</p></li><li><p>Keywords: Text-to-3D generation, Diffusion models, Trajectory Score Matching, Stable Diffusion XL</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11252, Github: https://github.com/xingy038/Dreamer-XL</p></li><li><p>Summary:</p><p>(1): æ–‡æœ¬è½¬ 3D ç”Ÿæˆæ–¹æ³•èƒ½å¤Ÿç›´æ¥ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­åˆ›å»ºå‡†ç¡®çš„ 3D æ¨¡å‹ï¼Œä»è€Œå‡å°‘ä¼ ç»Ÿ 3D å»ºæ¨¡æµç¨‹ä¸­çš„æ‰‹å·¥è¾“å…¥ã€‚</p><p>(2): ç°æœ‰çš„æ–‡æœ¬è½¬ 3D ç”Ÿæˆæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒå…ˆéªŒæ¥è®­ç»ƒç¥ç»å‚æ•°åŒ– 3D æ¨¡å‹ï¼Œå¦‚ç¥ç»è¾å°„åœº (NeRF) å’Œ 3D é«˜æ–¯åˆ†å‰²ï¼Œä½†å­˜åœ¨ä¼ª ground truth ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½¨è¿¹åŒ¹é… (TSM) æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ Denoising Diffusion Implicit Models (DDIM) åæ¼”è¿‡ç¨‹ä»åŒä¸€èµ·ç‚¹ç”Ÿæˆä¸¤æ¡è·¯å¾„è¿›è¡Œè®¡ç®—ï¼Œä»è€Œå‡å°‘ç´¯ç§¯è¯¯å·®ï¼Œç¼“è§£ä¼ª ground truth ä¸ä¸€è‡´é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é‡‡ç”¨ Stable Diffusion XL è¿›è¡ŒæŒ‡å¯¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€åƒç´ æ¢¯åº¦è£å‰ªæ–¹æ³•æ¥è§£å†³ Stable Diffusion XL åœ¨ 3D é«˜æ–¯åˆ†å‰²è¿‡ç¨‹ä¸­ä¸ç¨³å®šæ¢¯åº¦å¯¼è‡´çš„å¼‚å¸¸å¤åˆ¶å’Œåˆ†è£‚é—®é¢˜ã€‚</p><p>(4): å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºè½¨è¿¹åŒ¹é…ï¼ˆTSMï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ Denoising Diffusion Implicit Modelsï¼ˆDDIMï¼‰åæ¼”è¿‡ç¨‹ä»åŒä¸€èµ·ç‚¹ç”Ÿæˆä¸¤æ¡è·¯å¾„è¿›è¡Œè®¡ç®—ï¼Œä»è€Œå‡å°‘ç´¯ç§¯è¯¯å·®ï¼Œç¼“è§£ä¼ª ground truth ä¸ä¸€è‡´é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ Stable Diffusion XL è¿›è¡ŒæŒ‡å¯¼ï¼Œå¹¶æå‡ºä¸€ç§é€åƒç´ æ¢¯åº¦è£å‰ªæ–¹æ³•æ¥è§£å†³ Stable Diffusion XL åœ¨ 3D é«˜æ–¯åˆ†å‰²è¿‡ç¨‹ä¸­ä¸ç¨³å®šæ¢¯åº¦å¯¼è‡´çš„å¼‚å¸¸å¤åˆ¶å’Œåˆ†è£‚é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨ DDIM ä»åŒä¸€èµ·ç‚¹ç”Ÿæˆä¸¤æ¡è·¯å¾„ï¼Œé€šè¿‡è®¡ç®—ä¸¤æ¡è·¯å¾„çš„å·®å¼‚æ¥ä¼°è®¡æ¢¯åº¦ï¼Œä»è€Œå‡å°‘ç´¯ç§¯è¯¯å·®ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨ Stable Diffusion XL ä½œä¸ºå›¾åƒå…ˆéªŒï¼ŒæŒ‡å¯¼ç¥ç»å‚æ•°åŒ– 3D æ¨¡å‹çš„è®­ç»ƒï¼Œæé«˜ç”Ÿæˆ 3D æ¨¡å‹çš„è´¨é‡ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šæå‡ºé€åƒç´ æ¢¯åº¦è£å‰ªæ–¹æ³•ï¼Œé€šè¿‡è£å‰ªä¸ç¨³å®šæ¢¯åº¦ï¼Œè§£å†³ Stable Diffusion XL åœ¨ 3D é«˜æ–¯åˆ†å‰²è¿‡ç¨‹ä¸­ä¸ç¨³å®šæ¢¯åº¦å¯¼è‡´çš„å¼‚å¸¸å¤åˆ¶å’Œåˆ†è£‚é—®é¢˜ã€‚</p></li></ol><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰æœ¬æ–‡çš„å·¥ä½œæ„ä¹‰åœ¨äºï¼Œæå‡ºäº†è½¨è¿¹åŒ¹é…ï¼ˆTSMï¼‰æ–¹æ³•ï¼Œç¼“è§£äº†ä¼ª ground truth ä¸ä¸€è‡´é—®é¢˜ï¼Œæé«˜äº†æ–‡æœ¬è½¬ 3D ç”Ÿæˆçš„è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡º TSM æ–¹æ³•ï¼Œåˆ©ç”¨åŒè·¯å¾„è®¡ç®—æ¢¯åº¦ï¼Œå‡å°‘ç´¯ç§¯è¯¯å·®ï¼›é‡‡ç”¨ Stable Diffusion XL ä½œä¸ºå›¾åƒå…ˆéªŒï¼Œæé«˜ç”Ÿæˆ 3D æ¨¡å‹çš„è´¨é‡ï¼›æå‡ºé€åƒç´ æ¢¯åº¦è£å‰ªæ–¹æ³•ï¼Œè§£å†³ Stable Diffusion XL åœ¨ 3D é«˜æ–¯åˆ†å‰²è¿‡ç¨‹ä¸­ä¸ç¨³å®šæ¢¯åº¦å¯¼è‡´çš„å¼‚å¸¸å¤åˆ¶å’Œåˆ†è£‚é—®é¢˜ã€‚</p><p>æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦åˆ©ç”¨ Denoising Diffusion Implicit Models (DDIM) åæ¼”è¿‡ç¨‹ç”Ÿæˆä¸¤æ¡è·¯å¾„ï¼Œå¹¶é‡‡ç”¨ Stable Diffusion XL è¿›è¡ŒæŒ‡å¯¼ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-fcef932f7cbb28bd968c4b91df666357.jpg" align="middle"><img src="https://picx.zhimg.com/v2-471cfb5b1d4711dc52a26a6070dffe19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7fcbcac70009096c0f9624d62e02d74a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3fca8049c06610bfecfdb4639cba8929.jpg" align="middle"></details>## MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**Authors:Xinli Guo, Peng Han, Weidong Zhang, Hongtian Chen**With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a Surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed selectively tracking is achieved by feature extraction and motion filter on each frame. The joint optimization of pose and 3D Gaussian runs through the entire mapping process. Additionally, the coarse-to-fine pose estimation and compact Gaussian scene representation are implemented by dual keyfeature selection and novel loss functions. Experimental results demonstrate that the proposed algorithm not only outperforms the existing methods in tracking and mapping, but also has less memory usage. [PDF](http://arxiv.org/abs/2405.11129v1) **Summary****æ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGS èåˆçš„æ–°å‹ 3DGS-SLAM æ–¹æ³•ã€‚****Key Takeaways**- 3DGS-SLAM å‡­å€Ÿç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ 3D é«˜æ–¯æ–‘ç‚¹ï¼ˆ3DGSï¼‰çš„é«˜ä¿çœŸåœºæ™¯è¡¨ç¤ºèƒ½åŠ›å¸å¼•äº† SLAM é¢†åŸŸçš„å…³æ³¨ã€‚- æå‡ºäº†ä¸€ç§èåˆæ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGS çš„æ–°å‹ 3DGS-SLAM æ–¹æ³•ã€‚- é€šè¿‡å¯¹æ¯ä¸€å¸§è¿›è¡Œç‰¹å¾æå–å’Œè¿åŠ¨æ»¤æ³¢å®ç°é€‰æ‹©æ€§è·Ÿè¸ªï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚- æ•´ä¸ªå»ºå›¾è¿‡ç¨‹è´¯ç©¿äº†ä½å§¿å’Œ 3D é«˜æ–¯çš„è”åˆä¼˜åŒ–ã€‚- é€šè¿‡åŒå…³é”®å¸§é€‰æ‹©å’Œæ–°æŸå¤±å‡½æ•°å®ç°ä»ç²—åˆ°ç²¾çš„ä½å§¿ä¼°è®¡å’Œç´§å‡‘çš„é«˜æ–¯åœºæ™¯è¡¨ç¤ºã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•ä¸ä»…åœ¨è·Ÿè¸ªå’Œå»ºå›¾æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè€Œä¸”å†…å­˜ä½¿ç”¨æ›´å°‘ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šMotionGSï¼šç´§å‡‘é«˜æ–¯æ•£å°„ SLAM</p></li><li><p>ä½œè€…ï¼šXinli Guo, Peng Han, Weidong Zhang, Hongtian Chen</p></li><li><p>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šSLAMã€3D é«˜æ–¯æ•£å°„ã€ç¥ç»è¾å°„åœºã€è§†è§‰ç‰¹å¾</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.11129ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/Antonio521/MotionGS</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šéšç€é«˜ä¿çœŸåœºæ™¯è¡¨ç¤ºèƒ½åŠ›çš„å‘å±•ï¼ŒSLAM é¢†åŸŸå¯¹ç¥ç»è¾å°„åœº (NeRF) å’Œ 3D é«˜æ–¯æ•£å°„ (3DGS) çš„å…³æ³¨æ—¥ç›ŠåŠ æ·±ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäº NeRF çš„ SLAM è“¬å‹ƒå‘å±•ï¼Œè€ŒåŸºäº 3DGS çš„ SLAM å´è¾ƒä¸ºç¨€å°‘ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ï¼šç‚¹äº‘æˆ–æ›²é¢ã€ç½‘æ ¼ã€ä½“ç´ ç­‰ã€‚è¿™äº›ç»å…¸æ–¹æ³•æ— æ³•å®ç°é«˜ä¿çœŸè¡¨ç¤ºï¼Œä¹Ÿæ— æ³•é‡å»ºç²¾ç»†çº¹ç†å’Œé‡å¤åœºæ™¯ã€‚NeRF æ˜¯ä¸€ç§æ–°é¢–çš„è§†å›¾åˆæˆæ–¹æ³•ï¼Œå…·æœ‰éšå¼è¡¨ç¤ºåœºæ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒNeRF è®¡ç®—æˆæœ¬é«˜ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†åŠ¨æ€åœºæ™¯ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3DGS çš„ SLAM æ–°æ–¹æ³•ï¼Œèåˆäº†æ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGSã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯ä¸€å¸§è¿›è¡Œç‰¹å¾æå–å’Œè¿åŠ¨æ»¤æ³¢ï¼Œå®ç°äº†é€‰æ‹©æ€§è·Ÿè¸ªã€‚ä½å§¿å’Œ 3D é«˜æ–¯çš„è”åˆä¼˜åŒ–è´¯ç©¿æ•´ä¸ªå»ºå›¾è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒå…³é”®å¸§é€‰æ‹©å’Œæ–°é¢–çš„æŸå¤±å‡½æ•°ï¼Œå®ç°äº†ä»ç²—åˆ°ç²¾çš„ä½å§¿ä¼°è®¡å’Œç´§å‡‘çš„é«˜æ–¯åœºæ™¯è¡¨ç¤ºã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨è·Ÿè¸ªå’Œå»ºå›¾ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å†…å­˜å ç”¨æ›´å°‘ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰çš„ SLAM æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èåˆäº†æ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGSï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ç‰¹å¾æå–å’Œè¿åŠ¨æ»¤æ³¢å®ç°é€‰æ‹©æ€§è·Ÿè¸ªï¼Œå¹¶é€šè¿‡ä½å§¿å’Œ 3D é«˜æ–¯çš„è”åˆä¼˜åŒ–å®ç°å»ºå›¾ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡åŒå…³é”®å¸§é€‰æ‹©å’Œæ–°é¢–çš„æŸå¤±å‡½æ•°ï¼Œå®ç°äº†ä»ç²—åˆ°ç²¾çš„ä½å§¿ä¼°è®¡å’Œç´§å‡‘çš„é«˜æ–¯åœºæ™¯è¡¨ç¤ºï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨è·Ÿè¸ªå’Œå»ºå›¾ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å†…å­˜å ç”¨æ›´å°‘ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº 3DGS çš„ SLAMï¼Œåä¸º MotionGSï¼Œå®ƒé›†æˆäº†æ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGSã€‚å‡­å€Ÿå…¶ç²¾å¦™çš„è®¾è®¡ï¼ŒMonoGS çš„æœ€å…ˆè¿›æ€§èƒ½å·²åœ¨å¹¿æ³›çš„å®éªŒä¸­å¾—åˆ°å……åˆ†è¯æ˜ã€‚æå‡ºçš„æ–¹æ³•è¿›ä¸€æ­¥å¼ºè°ƒäº† 3DGS åœ¨ SLAM é¢†åŸŸçš„å¹¿æ³›æ½œåŠ›ã€‚åœ¨æ­¤å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œé’ˆå¯¹å¤§è§„æ¨¡å®¤å¤–åœºæ™¯çš„å¤šä¼ æ„Ÿå™¨ 3DGS-based SLAM å°†æˆä¸ºä¸‹ä¸€ä¸ªç ”ç©¶æ–¹å‘ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº 3DGS çš„ SLAM æ–°æ–¹æ³•ï¼Œèåˆäº†æ·±åº¦è§†è§‰ç‰¹å¾ã€åŒå…³é”®å¸§é€‰æ‹©å’Œ 3DGSï¼›æ€§èƒ½ï¼šåœ¨è·Ÿè¸ªå’Œå»ºå›¾ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”å†…å­˜å ç”¨æ›´å°‘ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œå¹¶ä¸”æ˜“äºå®ç°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-3746e1d5ffac123f7ade67514d6ff046.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a065b7e47d77b7c8660975cf14782cfa.jpg" align="middle"></details>## From NeRFs to Gaussian Splats, and Back**Authors:Siming He, Zach Osman, Pratik Chaudhari**For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch. [PDF](http://arxiv.org/abs/2405.09717v1) **Summary**NeRFså’ŒGSä¸¤ç§è¡¨ç¤ºåœ¨æœºå™¨äººåº”ç”¨ä¸­ç›¸äº’è½¬æ¢ï¼Œæ—¢ä¿ç•™NeRFsé«˜ä¿çœŸï¼Œåˆå…·å¤‡GSå®æ—¶æ¸²æŸ“çš„ä¼˜åŠ¿ã€‚**Key Takeaways**- NeRFsåœ¨ä¸åŒäºè®­ç»ƒæ•°æ®çš„è§†è§’ä¸‹æ³›åŒ–æ€§ä¼˜äºGSã€‚- GSæ¸²æŸ“é€Ÿåº¦è¿œå¿«äºNeRFsã€‚- å¼€å‘äº†NeRFså’ŒGSä¹‹é—´è½¬æ¢çš„è¿‡ç¨‹ã€‚- è¯¥æ–¹æ³•èåˆäº†NeRFså’ŒGSçš„ä¼˜ç‚¹ã€‚- è½¬æ¢çš„è®¡ç®—æˆæœ¬è¿œä½äºä»å¤´è®­ç»ƒä¸¤ç§æ–¹æ³•çš„æˆæœ¬ã€‚- è¯¥æ–¹æ³•ä½¿NeRFsèƒ½å¤Ÿå®æ—¶æ¸²æŸ“ã€‚- è¯¥æ–¹æ³•ç®€åŒ–äº†è¡¨ç¤ºçš„ä¿®æ”¹è¿‡ç¨‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šä»NeRFåˆ°é«˜æ–¯ç‚¹ï¼Œå†å›åˆ°NeRF</p></li><li><p>ä½œè€…ï¼šSiming He<em>, Zach Osman</em>, Pratik Chaudhari</p></li><li><p>éš¶å±æœºæ„ï¼šå®¾å¤•æ³•å°¼äºšå¤§å­¦é€šç”¨æœºå™¨äººã€è‡ªåŠ¨åŒ–ã€ä¼ æ„Ÿå’Œæ„ŸçŸ¥ï¼ˆGRASPï¼‰å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šNeRFï¼›é«˜æ–¯ç‚¹ï¼›åœºæ™¯è¡¨ç¤ºï¼›æœºå™¨äºº</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.09717 Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåœºæ™¯è¡¨ç¤ºåœ¨æœºå™¨äººå­¦ä¸­è‡³å…³é‡è¦ï¼Œä½†éšå¼è¡¨ç¤ºï¼ˆå¦‚NeRFï¼‰å’Œæ˜¾å¼è¡¨ç¤ºï¼ˆå¦‚é«˜æ–¯ç‚¹ï¼‰çš„é€‰æ‹©ä¸€ç›´æ˜¯äº‰è®ºçš„ç„¦ç‚¹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šé«˜æ–¯ç‚¹åœ¨è®­ç»ƒå’Œæµ‹è¯•è§†å›¾ç›¸ä¼¼çš„åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹æ–°è§†å›¾çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚NeRFsåœ¨æœ‰é™è§†å›¾ä¸‹è¡¨ç°æ›´å¥½ï¼Œä½†æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ï¼Œå†…å­˜æ¶ˆè€—è¾ƒå¤§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†NeRFè½¬æ¢ä¸ºé«˜æ–¯ç‚¹ï¼ˆNeRF2GSï¼‰çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒNeRFçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿˜æå‡ºäº†ä¸€ç§å°†é«˜æ–¯ç‚¹è½¬æ¢ä¸ºNeRFï¼ˆGS2NeRFï¼‰çš„æ–¹æ³•ï¼Œå¯ä»¥èŠ‚çœå†…å­˜å¹¶ç¼–è¾‘åœºæ™¯ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šNeRF2GSåœ¨ä¸åŒåœºæ™¯ä¸­å®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚GS2NeRFå¯ä»¥å°†é«˜æ–¯ç‚¹å­˜å‚¨ä¸ºæ›´ç´§å‡‘çš„NeRFï¼Œå¹¶å…è®¸è½»æ¾ä¿®æ”¹åœºæ™¯ã€‚è¿™äº›æ–¹æ³•åœ¨æœºå™¨äººå­¦åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä¾‹å¦‚å®šä½ã€å»ºå›¾å’Œåœºæ™¯ç†è§£ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„NeRF2GSå’ŒGS2NeRFæ–¹æ³•ï¼Œå°†NeRFå’Œé«˜æ–¯ç‚¹çš„ä¼˜ç‚¹ç›¸ç»“åˆï¼Œåœ¨åœºæ™¯è¡¨ç¤ºã€æœºå™¨äººå­¦ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†NeRF2GSå’ŒGS2NeRFä¸¤ç§æ–¹æ³•ï¼Œå®ç°äº†NeRFå’Œé«˜æ–¯ç‚¹çš„ç›¸äº’è½¬æ¢ï¼Œå…¼é¡¾äº†æ³›åŒ–èƒ½åŠ›ã€æ¸²æŸ“é€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—ï¼›æ€§èƒ½ï¼šNeRF2GSå®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ŒGS2NeRFå¯ä»¥èŠ‚çœå†…å­˜å¹¶ç¼–è¾‘åœºæ™¯ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°NeRFå’Œé«˜æ–¯ç‚¹ä¸¤ç§ä¸åŒè¡¨ç¤ºå½¢å¼çš„è½¬æ¢ï¼Œéœ€è¦æ·±å…¥ç†è§£å’Œç®—æ³•è®¾è®¡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1f688bf02429316b0bc16be92158745e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-488dc982c5568d6a58b927a0ed88810f.jpg" align="middle"></details>## GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting**Authors:Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao**The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing. [PDF](http://arxiv.org/abs/2405.07472v1) On-going work**æ‘˜è¦**é«˜æ–¯æ–¹å—ç¼–è¾‘ï¼ˆGSï¼‰ä¸äºŒç»´è™šæ‹Ÿè¯•è¡£ï¼ˆVTONï¼‰ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„ä¸‰ç»´è™šæ‹Ÿè¯•è¡£ç®¡é“GaussianVTONã€‚**å…³é”®è¦ç‚¹**- é›†æˆé«˜æ–¯æ–¹å—ç¼–è¾‘ï¼ˆGSï¼‰ä¸äºŒç»´è™šæ‹Ÿè¯•è¡£ï¼ˆVTONï¼‰ä»¥è¿›è¡Œä¸‰ç»´è™šæ‹Ÿè¯•è¡£ã€‚- é¦–æ¬¡ä½¿ç”¨å›¾åƒä½œä¸ºä¸‰ç»´ç¼–è¾‘çš„ç¼–è¾‘æç¤ºã€‚- æå‡ºä¸‰é˜¶æ®µä¼˜åŒ–ç­–ç•¥ä»¥è§£å†³ç¼–è¾‘è¿‡ç¨‹ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚- å¼•å…¥ç¼–è¾‘å›å¿†é‡å»ºï¼ˆERRï¼‰ç¼–è¾‘ç­–ç•¥ï¼Œä»¥å…‹æœç°æœ‰ç¼–è¾‘ç­–ç•¥çš„é™åˆ¶ã€‚- å®éªŒè¡¨æ˜GaussianVTONçš„ä¼˜è¶Šæ€§ï¼Œä¸ºä¸‰ç»´è™šæ‹Ÿè¯•è¡£æä¾›äº†æ–°è§†è§’ï¼Œå¹¶ä¸ºåŸºäºå›¾åƒæç¤ºçš„ä¸‰ç»´åœºæ™¯ç¼–è¾‘å»ºç«‹äº†æ–°çš„èµ·ç‚¹ã€‚- å¼ºè°ƒäº†ç”µå•†é¢†åŸŸè™šæ‹Ÿè¯•è¡£çš„é‡è¦æ€§ã€‚- ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äºŒç»´è™šæ‹Ÿè¯•è¡£å’Œä¸‰ç»´æœè£…-èº«ä½“å½¢çŠ¶å…¼å®¹æ€§ã€‚- å¼•å…¥äº†äºŒç»´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤šè§†è§’ç¼–è¾‘å°†å…¶ç”¨äºä¸‰ç»´ç¼–è¾‘ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šGaussianVTONï¼šåŸºäºå¤šé˜¶æ®µé«˜æ–¯æ•£ç‚¹çš„3Däººä½“è™šæ‹Ÿè¯•ç©¿</p></li><li><p>ä½œè€…ï¼šHaodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</p></li><li><p>å•ä½ï¼šè¥¿åŒ—å·¥ä¸šå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šVirtual Try-On, Gaussian Splatting, Image Prompting, 3D Scene Editing</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.07472, Githubä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šéšç€ç”µå­å•†åŠ¡çš„å…´èµ·ï¼Œè™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨2Dé¢†åŸŸï¼Œå¹¶ä¸”ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚3D VTONçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æœè£…ä¸èº«ä½“å½¢çŠ¶çš„å…¼å®¹æ€§ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨2D VTONä¸­å¹¿æ³›è®¨è®ºçš„è¯é¢˜ã€‚å¾—ç›Šäº3Dåœºæ™¯ç¼–è¾‘çš„è¿›æ­¥ï¼Œ2Dæ‰©æ•£æ¨¡å‹ç°å·²é€šè¿‡å¤šè§†ç‚¹ç¼–è¾‘è¢«ç”¨äº3Dç¼–è¾‘ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä»¥å¾€çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨2Dé¢†åŸŸï¼Œå¹¶ä¸”ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š    - æ— æ³•å¤„ç†å¤æ‚å‡ ä½•å˜åŒ–    - å®¹æ˜“å¯¼è‡´é¢éƒ¨æ¨¡ç³Šã€æœè£…ä¸å‡†ç¡®ã€è§†ç‚¹è´¨é‡ä¸‹é™ç­‰é—®é¢˜</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGaussianVTONçš„åˆ›æ–°3D VTONç®¡é“ï¼Œå®ƒå°†é«˜æ–¯æ•£ç‚¹ï¼ˆGSï¼‰ç¼–è¾‘ä¸2D VTONç›¸ç»“åˆã€‚ä¸ºäº†ä¿ƒè¿›ä»2Dåˆ°3D VTONçš„æ— ç¼è¿‡æ¸¡ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºä»…ä½¿ç”¨å›¾åƒä½œä¸º3Dç¼–è¾‘çš„ç¼–è¾‘æç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥è§£å†³ç¼–è¾‘è¿‡ç¨‹ä¸­å‡ºç°çš„é¢éƒ¨æ¨¡ç³Šã€æœè£…ä¸å‡†ç¡®ã€è§†ç‚¹è´¨é‡ä¸‹é™ç­‰é—®é¢˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§ä¸‰é˜¶æ®µç»†åŒ–ç­–ç•¥æ¥é€æ­¥ç¼“è§£æ½œåœ¨çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§ç§°ä¸ºç¼–è¾‘å¬å›é‡å»ºï¼ˆERRï¼‰çš„æ–°ç¼–è¾‘ç­–ç•¥ï¼Œä»¥è§£å†³ä»¥å¾€ç¼–è¾‘ç­–ç•¥åœ¨å¯¼è‡´å¤æ‚å‡ ä½•å˜åŒ–æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆæœï¼š    - ä»»åŠ¡ï¼š3Däººä½“è™šæ‹Ÿè¯•ç©¿    - æ€§èƒ½ï¼š        - èƒ½å¤Ÿå¤„ç†å¤æ‚å‡ ä½•å˜åŒ–        - é¿å…äº†é¢éƒ¨æ¨¡ç³Šã€æœè£…ä¸å‡†ç¡®ã€è§†ç‚¹è´¨é‡ä¸‹é™ç­‰é—®é¢˜        - å®ç°äº†ä»2Dåˆ°3D VTONçš„æ— ç¼è¿‡æ¸¡</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šé«˜æ–¯æ•£ç‚¹ï¼ˆGSï¼‰ç¼–è¾‘ä¸åŸºäºæ‰©æ•£çš„ 2D VTON æ¨¡å‹ç›¸ç»“åˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºç¼–è¾‘å¬å›é‡å»ºï¼ˆERRï¼‰ç­–ç•¥ï¼Œé€šè¿‡æ¸²æŸ“æ•´ä¸ªæ•°æ®é›†æ¥è¿›è¡Œç¼–è¾‘ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè®¾è®¡ä¸‰é˜¶æ®µç»†åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢éƒ¨ä¸€è‡´æ€§ã€æœè£…å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡æå‡ã€‚</p><p><strong>Conclusion:</strong></p><p><strong>1. æœ¬å·¥ä½œçš„æ„ä¹‰ï¼š</strong></p><p>æå‡ºäº†ä¸€ç§åä¸º GaussianVTON çš„åˆ›æ–° 3D VTON ç®¡é“ï¼Œå°†é«˜æ–¯æ•£ç‚¹ï¼ˆGSï¼‰ç¼–è¾‘ä¸åŸºäºæ‰©æ•£çš„ 2D VTON æ¨¡å‹ç›¸ç»“åˆï¼Œæ˜¾è‘—æå‡äº†å›¾åƒæç¤ºçš„ 3D ç¼–è¾‘å’Œ 3D VTON çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡å»ºå’Œç¼–è¾‘çœŸå®åœºæ™¯ï¼Œä¸ºç”¨æˆ·æä¾›äº†é€¼çœŸçš„è¯•ç©¿ä½“éªŒã€‚</p><p><strong>2. æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä»åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ä¸‰ä¸ªç»´åº¦ï¼‰ï¼š</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§å°†é«˜æ–¯æ•£ç‚¹ç¼–è¾‘ä¸åŸºäºæ‰©æ•£çš„ 2D VTON æ¨¡å‹ç›¸ç»“åˆçš„ 3D VTON æ–¹æ³•ã€‚</li><li>æå‡ºäº†ä¸€ç§ç§°ä¸ºç¼–è¾‘å¬å›é‡å»ºï¼ˆERRï¼‰çš„ç¼–è¾‘ç­–ç•¥ï¼Œé€šè¿‡æ¸²æŸ“æ•´ä¸ªæ•°æ®é›†æ¥è¿›è¡Œç¼–è¾‘ã€‚</li><li>è®¾è®¡äº†ä¸‰é˜¶æ®µç»†åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬é¢éƒ¨ä¸€è‡´æ€§ã€æœè£…å‡†ç¡®æ€§å’Œå›¾åƒè´¨é‡æå‡ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>èƒ½å¤Ÿå¤„ç†å¤æ‚å‡ ä½•å˜åŒ–ã€‚</li><li>é¿å…äº†é¢éƒ¨æ¨¡ç³Šã€æœè£…ä¸å‡†ç¡®ã€è§†ç‚¹è´¨é‡ä¸‹é™ç­‰é—®é¢˜ã€‚</li><li>å®ç°ä» 2D åˆ° 3D VTON çš„æ— ç¼è¿‡æ¸¡ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ–¹æ³•éœ€è¦æ¸²æŸ“æ•´ä¸ªæ•°æ®é›†ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li><li>ä¸‰é˜¶æ®µç»†åŒ–ç­–ç•¥å¢åŠ äº†ç¼–è¾‘è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5394ac2d064b51a6629e452550c4b472.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c590805a84c00f53de63efe5b169e438.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0d6fd34202723d6b9eb27bdabd26f7.jpg" align="middle"></details>## LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer**Authors:Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu**Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html. [PDF](http://arxiv.org/abs/2405.07319v1) SIGGRAPH 2024 conference track**Summary**äººé«”èˆ‡æœé£¾åˆ†é›¢è¡¨å¾µï¼Œå¯¦ç¾è·¨è§’è‰²æœé£¾å‹•ç•«å‚³è¼¸ã€‚**Key Takeaways*** LayGAæå‡ºäº†ä¸€ç¨®æ–°çš„è¡¨ç¤ºæ–¹å¼ï¼Œå°‡äººé«”å’Œæœé£¾è¡¨å¾µç‚ºå…©å€‹ç¨ç«‹çš„å±¤ã€‚* åŸºæ–¼é«˜æ–¯åœ°åœ–çš„åŒ–èº«å…·æœ‰è‰¯å¥½çš„æœé£¾ç´°ç¯€è¡¨ç¾åŠ›ã€‚* å…©éšæ®µè¨“ç·´ï¼šå–®å±¤é‡æ§‹å’Œå¤šå±¤æ“¬åˆã€‚* åœ¨å–®å±¤é‡æ§‹éšæ®µï¼Œå¹¾ä½•ç´„æŸç”¨æ–¼é‡å»ºå¹³æ»‘æ›²é¢å’Œåˆ†æ®µäººé«”å’Œæœé£¾ã€‚* åœ¨å¤šå±¤æ“¬åˆéšæ®µï¼Œå…©å€‹æ¨¡å‹åˆ†åˆ¥è¡¨ç¤ºäººé«”å’Œæœé£¾ï¼Œä¸¦åˆ©ç”¨é‡å»ºçš„æœé£¾å¹¾ä½•ä½œç‚ºæ›´ç²¾ç¢ºæœé£¾è¿½è¹¤çš„ 3D ç›£ç£ã€‚* æå‡ºå¹¾ä½•å±¤å’Œæ¸²æŸ“å±¤ï¼Œå¯¦ç¾é«˜å“è³ªå¹¾ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚* LayGAå¯¦ç¾äº†é€¼çœŸçš„å‹•ç•«å’Œè™›æ“¬è©¦ç©¿ï¼Œä¸¦å„ªæ–¼å…¶ä»–åŸºç·šæ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šåˆ†å±‚é«˜æ–¯åŒ–èº«ï¼šç”¨äºå¯åŠ¨ç”»æœè£…çš„æœè£…è½¬ç§»</p></li><li><p>ä½œè€…ï¼šSiyou Linã€Zhe Liã€Zhaoqi Suã€Zerong Zhengã€Hongwen Zhangã€Yebin Liu</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šå¯åŠ¨ç”»åŒ–èº«ã€æœè£…è½¬ç§»ã€äººä½“é‡å»º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.07319, Githubï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¯åŠ¨ç”»æœè£…è½¬ç§»æ—¨åœ¨è·¨è§’è‰²ç©¿è¡£å’ŒåŠ¨ç”»æœè£…ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å¤§å¤šæ•°äººä½“åŒ–èº«å·¥ä½œå°†äººä½“å’Œæœè£…çš„è¡¨å¾çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´è·¨èº«ä»½è¿›è¡Œè™šæ‹Ÿè¯•ç©¿å­˜åœ¨å›°éš¾ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œçº ç¼ çš„è¡¨å¾é€šå¸¸æ— æ³•å‡†ç¡®è·Ÿè¸ªæœè£…çš„æ»‘åŠ¨è¿åŠ¨ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•å­˜åœ¨çº ç¼ äººä½“å’Œæœè£…è¡¨å¾ã€éš¾ä»¥å‡†ç¡®è·Ÿè¸ªæœè£…æ»‘åŠ¨è¿åŠ¨ç­‰é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºæ˜¯å…‹æœè¿™äº›é™åˆ¶ï¼Œæå‡ºä¸€ç§æ–°çš„è¡¨å¾ï¼Œå°†èº«ä½“å’Œæœè£…è¡¨è¿°ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œç”¨äºä»å¤šè§†å›¾è§†é¢‘ä¸­è¿›è¡Œé€¼çœŸçš„å¯åŠ¨ç”»æœè£…è½¬ç§»ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰ï¼Œå®ƒå»ºç«‹åœ¨åŸºäºé«˜æ–¯æ˜ å°„çš„åŒ–èº«ä¸Šï¼Œä»¥è·å¾—æœè£…ç»†èŠ‚çš„å‡ºè‰²è¡¨å¾èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé«˜æ–¯æ˜ å°„ä¼šäº§ç”Ÿåˆ†å¸ƒåœ¨å®é™…è¡¨é¢å‘¨å›´çš„éç»“æ„åŒ– 3D é«˜æ–¯ä½“ã€‚ç¼ºä¹å¹³æ»‘çš„æ˜¾å¼è¡¨é¢ç»™å‡†ç¡®çš„æœè£…è·Ÿè¸ªå’Œèº«ä½“ä¸æœè£…ä¹‹é—´çš„ç¢°æ’å¤„ç†å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œè¯¥è®ºæ–‡æå‡ºäº†æ¶‰åŠå•å±‚é‡å»ºå’Œå¤šå±‚æ‹Ÿåˆçš„ä¸¤é˜¶æ®µè®­ç»ƒã€‚åœ¨å•å±‚é‡å»ºé˜¶æ®µï¼Œæå‡ºäº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸæ¥é‡å»ºå¹³æ»‘çš„è¡¨é¢ï¼Œå¹¶åŒæ—¶è·å¾—èº«ä½“å’Œæœè£…ä¹‹é—´çš„åˆ†å‰²ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨å¤šå±‚æ‹Ÿåˆé˜¶æ®µï¼Œè®­ç»ƒä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹æ¥è¡¨ç¤ºèº«ä½“å’Œæœè£…ï¼Œå¹¶å°†é‡å»ºçš„æœè£…å‡ ä½•ä½“ç”¨ä½œ 3D ç›‘ç£ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å‡ ä½•å±‚å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šè¯¥è®ºæ–‡çš„æ–¹æ³•åœ¨é€¼çœŸçš„åŠ¨ç”»å’Œè™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³å®ç°é€¼çœŸçš„å¯åŠ¨ç”»æœè£…è½¬ç§»ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰ï¼Œå®ƒå»ºç«‹åœ¨åŸºäºé«˜æ–¯æ˜ å°„çš„åŒ–èº«ä¸Šï¼Œä»¥è·å¾—æœè£…ç»†èŠ‚çš„å‡ºè‰²è¡¨å¾èƒ½åŠ›ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬å•å±‚é‡å»ºå’Œå¤šå±‚æ‹Ÿåˆã€‚åœ¨å•å±‚é‡å»ºé˜¶æ®µï¼Œæå‡ºäº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸæ¥é‡å»ºå¹³æ»‘çš„è¡¨é¢ï¼Œå¹¶åŒæ—¶è·å¾—èº«ä½“å’Œæœè£…ä¹‹é—´çš„åˆ†å‰²ã€‚åœ¨å¤šå±‚æ‹Ÿåˆé˜¶æ®µï¼Œè®­ç»ƒä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹æ¥è¡¨ç¤ºèº«ä½“å’Œæœè£…ï¼Œå¹¶å°†é‡å»ºçš„æœè£…å‡ ä½•ä½“ç”¨ä½œ 3D ç›‘ç£ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºå‡ ä½•å±‚å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº†åˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰ï¼Œè¯¥æ–¹æ³•å°†äººä½“å’Œæœè£…è¡¨å¾ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çº ç¼ äººä½“å’Œæœè£…è¡¨å¾ã€éš¾ä»¥å‡†ç¡®è·Ÿè¸ªæœè£…æ»‘åŠ¨è¿åŠ¨ç­‰é—®é¢˜ï¼Œå®ç°äº†é€¼çœŸçš„å¯åŠ¨ç”»æœè£…è½¬ç§»ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºåˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰çš„è¡¨å¾ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çº ç¼ äººä½“å’Œæœè£…è¡¨å¾ã€éš¾ä»¥å‡†ç¡®è·Ÿè¸ªæœè£…æ»‘åŠ¨è¿åŠ¨ç­‰é—®é¢˜ï¼›æå‡ºä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬å•å±‚é‡å»ºå’Œå¤šå±‚æ‹Ÿåˆï¼Œè·å¾—äº†æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªï¼›æå‡ºå‡ ä½•å±‚å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚æ€§èƒ½ï¼šåœ¨é€¼çœŸçš„åŠ¨ç”»å’Œè™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ„å»ºé«˜è´¨é‡çš„å¤šè§†å›¾è§†é¢‘æ•°æ®é›†ï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-cbea179fd85983d0e759d4be018fb59a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68aa80c2ba44dfde97867ba03ebc2814.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80f17e9e8af3606ee233b1b0ca1da60c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-923a67fcbe4586a3709ea7a21a673f85.jpg" align="middle"></details>## Direct Learning of Mesh and Appearance via 3D Gaussian Splatting**Authors:Ancheng Lin, Jun Li**Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of the scene, including the mesh. Experimental results demonstrate that the learned scene model not only achieves state-of-the-art rendering quality but also supports manipulation using the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance. [PDF](http://arxiv.org/abs/2405.06945v1) **Summary**å¯å­¦ä¹ çš„åœºæ™¯æ¨¡å‹èåˆäº† 3DGS å’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºï¼Œåœ¨ç«¯åˆ°ç«¯çš„æ–¹å¼ä¸‹å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œåˆ©ç”¨ç½‘æ ¼é¢ç»‘å®š 3D é«˜æ–¯ä½“å¹¶å¯¹ 3DGS æ‰§è¡Œå¯å¾®æ¸²æŸ“ä»¥è·å¾—å…‰åº¦ç›‘ç£ã€‚**Key Takeaways**- å°† 3DGS ä¸æ˜¾å¼å‡ ä½•è¡¨ç¤ºç›¸ç»“åˆçš„åœºæ™¯æ¨¡å‹ã€‚- ç«¯åˆ°ç«¯å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚ï¼Œå»ºç«‹æœ‰æ•ˆçš„ç›‘ç£ä¿¡æ¯è·¯å¾„ã€‚- è¾¾åˆ°æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œå¹¶æ”¯æŒä½¿ç”¨æ˜¾å¼ç½‘æ ¼è¿›è¡Œæ“ä½œã€‚- ç”±äºç½‘æ ¼å’Œå¤–è§‚çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œåœ¨é€‚åº”åœºæ™¯æ›´æ–°æ–¹é¢å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚- ç»‘å®š 3D é«˜æ–¯ä½“åˆ°ç½‘æ ¼é¢å¹¶æ‰§è¡Œ 3DGS çš„å¯å¾®æ¸²æŸ“ã€‚- å¯å¾®æ¸²æŸ“æä¾›å…‰åº¦ç›‘ç£ï¼ŒæŒ‡å¯¼åœºæ™¯å­¦ä¹ ã€‚- èåˆ 3DGS å’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºæœ‰åŠ©äºå‡ ä½•é‡å»ºã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: åˆ©ç”¨3Dé«˜æ–¯æ¸²æŸ“ç›´æ¥å­¦ä¹ ç½‘æ ¼å’Œå¤–è§‚</p></li><li><p>Authors:  </p><ul><li>Xueting Li</li><li>Sifei Liu</li><li>Xianzhi Li</li><li>Chi-Wing Fu</li><li>Pheng-Ann Heng</li><li>Chen Change Loy</li></ul></li><li><p>Affiliation: æ–°åŠ å¡å›½ç«‹å¤§å­¦</p></li><li><p>Keywords: </p><ul><li>3D reconstruction</li><li>mesh generation</li><li>appearance modeling</li><li>generative adversarial networks</li></ul></li><li><p>Urls: https://arxiv.org/abs/2206.02089 , Github:None</p></li><li><p>Summary: </p><p>(1): 3Dé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨ä»2Då›¾åƒä¸­æ¢å¤3Dåœºæ™¯ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„å…ˆéªŒçŸ¥è¯†æˆ–å¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚</p><p>(2): ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»2Då›¾åƒä¸­ç›´æ¥å­¦ä¹ 3Dç½‘æ ¼å’Œå¤–è§‚ã€‚è¯¥æ–¹æ³•ä½¿ç”¨3Dé«˜æ–¯æ¸²æŸ“å™¨ä½œä¸ºç”Ÿæˆå™¨ï¼Œè¯¥æ¸²æŸ“å™¨å¯ä»¥ä»éšå¼è¡¨ç¤ºä¸­ç”Ÿæˆé€¼çœŸçš„3Dç½‘æ ¼å’Œçº¹ç†ã€‚åˆ¤åˆ«å™¨æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒåŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„3Dæ•°æ®ã€‚</p><p>(3): è¯¥æ–¹æ³•é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæ¥å­¦ä¹ ï¼Œå…¶ä¸­ç”Ÿæˆå™¨è¯•å›¾ç”Ÿæˆä»¥å‡ä¹±çœŸçš„3Dæ•°æ®ï¼Œè€Œåˆ¤åˆ«å™¨åˆ™è¯•å›¾å°†çœŸå®æ•°æ®ä¸ç”Ÿæˆæ•°æ®åŒºåˆ†å¼€æ¥ã€‚é€šè¿‡è¿™ç§å¯¹æŠ—æ€§è¿‡ç¨‹ï¼Œç”Ÿæˆå™¨é€æ¸å­¦ä¼šç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼å’Œå¤–è§‚ï¼Œè€Œåˆ¤åˆ«å™¨å­¦ä¼šå¯¹3Dæ•°æ®è¿›è¡Œåˆ¤åˆ«ã€‚</p><p>(4): åœ¨ShapeNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ï¼Œå…·æœ‰å‡†ç¡®çš„å½¢çŠ¶å’Œé€¼çœŸçš„çº¹ç†ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¯é«˜æ•ˆçš„ï¼Œå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆ3Dæ•°æ®ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–°æ–¹æ³•ï¼Œä»2Då›¾åƒä¸­ç›´æ¥å­¦ä¹ 3Dç½‘æ ¼å’Œå¤–è§‚ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä½¿ç”¨3Dé«˜æ–¯æ¸²æŸ“å™¨ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»éšå¼è¡¨ç¤ºä¸­ç”Ÿæˆé€¼çœŸçš„3Dç½‘æ ¼å’Œçº¹ç†ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ¤åˆ«å™¨æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼ŒåŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„3Dæ•°æ®ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæ¥å­¦ä¹ ï¼Œç”Ÿæˆå™¨è¯•å›¾ç”Ÿæˆä»¥å‡ä¹±çœŸçš„3Dæ•°æ®ï¼Œåˆ¤åˆ«å™¨è¯•å›¾å°†çœŸå®æ•°æ®ä¸ç”Ÿæˆæ•°æ®åŒºåˆ†å¼€æ¥ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šåœ¨ShapeNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ï¼Œå…·æœ‰å‡†ç¡®çš„å½¢çŠ¶å’Œé€¼çœŸçš„çº¹ç†ï¼›</p><p>ï¼ˆ7ï¼‰ï¼šè¯¥æ–¹æ³•æ˜¯é«˜æ•ˆçš„ï¼Œå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆ3Dæ•°æ®ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>           ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šè§†å›¾ä¸­è·å–å…¨é¢çš„3Dåœºæ™¯ä¿¡æ¯ã€‚è¯¥æ–¹æ³•åŒæ—¶æå–å‡ ä½•å’Œå½±å“è§‚å¯Ÿå¤–è§‚çš„ç‰©ç†å±æ€§ã€‚å‡ ä½•ä»¥ä¸‰è§’å½¢ç½‘æ ¼çš„æ˜¾å¼å½¢å¼æå–ã€‚å¤–è§‚å±æ€§ç¼–ç åœ¨ä¸ç½‘æ ¼é¢ç»‘å®šçš„3Dé«˜æ–¯ä½“ä¸­ã€‚ç”±äºåŸºäº3DGSçš„å¯å¾®æ¸²æŸ“ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¼˜åŒ–å…‰åº¦æŸå¤±æ¥å»ºç«‹ä¸€ä¸ªæœ‰æ•ˆä¸”é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒéªŒè¯äº†ç”Ÿæˆçš„è¡¨ç¤ºæ—¢å…·æœ‰é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œåˆå…·æœ‰å¯æ§æ€§ã€‚           ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäºGANï¼Œä»2Då›¾åƒç›´æ¥å­¦ä¹ 3Dç½‘æ ¼å’Œå¤–è§‚ï¼›ä½¿ç”¨3Dé«˜æ–¯æ¸²æŸ“å™¨ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»éšå¼è¡¨ç¤ºä¸­ç”Ÿæˆé€¼çœŸçš„3Dç½‘æ ¼å’Œçº¹ç†ï¼›é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæ¥å­¦ä¹ ï¼Œç”Ÿæˆå™¨è¯•å›¾ç”Ÿæˆä»¥å‡ä¹±çœŸçš„3Dæ•°æ®ï¼Œåˆ¤åˆ«å™¨è¯•å›¾å°†çœŸå®æ•°æ®ä¸ç”Ÿæˆæ•°æ®åŒºåˆ†å¼€æ¥ã€‚           æ€§èƒ½ï¼šåœ¨ShapeNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨3Dé‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„3Dç½‘æ ¼ï¼Œå…·æœ‰å‡†ç¡®çš„å½¢çŠ¶å’Œé€¼çœŸçš„çº¹ç†ã€‚           å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•æ˜¯é«˜æ•ˆçš„ï¼Œå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆ3Dæ•°æ®ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4dfd1ce4253f3ad2b1cd7f3ab9f54d4d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5d18b17eab898e3b16645fd69d72106.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  MOSS Motion-based 3D Clothed Human Synthesis from Monocular Video</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Talking%20Head%20Generation/</id>
    <published>2024-05-22T04:29:06.000Z</published>
    <updated>2024-05-22T04:29:06.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-22-æ›´æ–°"><a href="#2024-05-22-æ›´æ–°" class="headerlink" title="2024-05-22 æ›´æ–°"></a>2024-05-22 æ›´æ–°</h1><h2 id="Listen-Disentangle-and-Control-Controllable-Speech-Driven-Talking-Head-Generation"><a href="#Listen-Disentangle-and-Control-Controllable-Speech-Driven-Talking-Head-Generation" class="headerlink" title="Listen, Disentangle, and Control: Controllable Speech-Driven Talking   Head Generation"></a>Listen, Disentangle, and Control: Controllable Speech-Driven Talking   Head Generation</h2><p><strong>Authors:Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</strong></p><p>Most earlier investigations on talking face generation have focused on the synchronization of lip motion and speech content. However, human head pose and facial emotions are equally important characteristics of natural human faces. While audio-driven talking face generation has seen notable advancements, existing methods either overlook facial emotions or are limited to specific individuals and cannot be applied to arbitrary subjects. In this paper, we propose a one-shot Talking Head Generation framework (SPEAK) that distinguishes itself from general Talking Face Generation by enabling emotional and postural control. Specifically, we introduce the Inter-Reconstructed Feature Disentanglement (IRFD) method to decouple human facial features into three latent spaces. We then design a face editing module that modifies speech content and facial latent codes into a single latent space. Subsequently, we present a novel generator that employs modified latent codes derived from the editing module to regulate emotional expression, head poses, and speech content in synthesizing facial animations. Extensive trials demonstrate that our method can generate realistic talking head with coordinated lip motions, authentic facial emotions, and smooth head movements. The demo video is available at the anonymous link: <a href="https://anonymous.4open.science/r/SPEAK-F56E">https://anonymous.4open.science/r/SPEAK-F56E</a> </p><p><a href="http://arxiv.org/abs/2405.07257v1">PDF</a> </p><p><strong>Summary</strong><br>è¯­éŸ³é©±åŠ¨çš„è¯´è¯äººå¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œå®ç°äº†è¯´è¯äººå¤´åƒè¡¨æƒ…æƒ…ç»ªå’Œå§¿åŠ¿æ§åˆ¶</p><p><strong>Key Takeaways</strong></p><ul><li>æ³¨é‡å”‡éƒ¨åŠ¨ä½œå’Œè¯­éŸ³å†…å®¹åŒæ­¥</li><li>äººç±»å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ä¹Ÿæ˜¯è‡ªç„¶äººè„¸çš„é‡è¦ç‰¹å¾</li><li>ç°æœ‰æ–¹æ³•å¿½è§†é¢éƒ¨è¡¨æƒ…æˆ–å±€é™äºç‰¹å®šä¸ªä½“</li><li>æå‡ºäº†ä¸€æ¬¡æ€§è¯´è¯äººå¤´åƒç”Ÿæˆæ¡†æ¶ (SPEAK)</li><li>å¼•å…¥äº†äº’é‡æ„ç‰¹å¾åˆ†ç¦» (IRFD) æ–¹æ³•</li><li>è®¾è®¡äº†ä¸€ä¸ªé¢éƒ¨ç¼–è¾‘æ¨¡å—ï¼Œå°†è¯­éŸ³å†…å®¹å’Œé¢éƒ¨æ½œåœ¨ç¼–ç ä¿®æ”¹ä¸ºä¸€ä¸ªæ½œåœ¨ç©ºé—´</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨ç¼–è¾‘æ¨¡å—æ´¾ç”Ÿçš„ä¿®æ”¹åçš„æ½œåœ¨ç¼–ç æ¥è°ƒèŠ‚åˆæˆé¢éƒ¨åŠ¨ç”»ä¸­çš„æƒ…ç»ªè¡¨è¾¾ã€å¤´éƒ¨å§¿åŠ¿å’Œè¯­éŸ³å†…å®¹</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: è†å¬ã€è§£è€¦å’Œæ§åˆ¶ï¼šå¯æ§è¯­éŸ³é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šè†å¬ã€è§£è€¦å’Œæ§åˆ¶ï¼šå¯æ§è¯­éŸ³é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆï¼‰</p></li><li><p>Authors: Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</p></li><li><p>Affiliation: ä¸œå—å¤§å­¦ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šä¸œå—å¤§å­¦ï¼‰</p></li><li><p>Keywords: Speech-driven talking head generation, Facial emotion control, Head pose control, Latent space disentanglement, Generative adversarial networks</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.07257, Github: None</p></li><li><p>Summary:</p><p>(1): äººç±»å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…æ˜¯è‡ªç„¶äººè„¸çš„é‡è¦ç‰¹å¾ï¼Œè€Œç°æœ‰çš„æ–¹æ³•è¦ä¹ˆå¿½ç•¥é¢éƒ¨è¡¨æƒ…ï¼Œè¦ä¹ˆä»…é™äºç‰¹å®šä¸ªä½“ï¼Œæ— æ³•åº”ç”¨äºä»»æ„ä¸»ä½“ã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆå¿½ç•¥é¢éƒ¨è¡¨æƒ…ï¼Œè¦ä¹ˆä»…é™äºç‰¹å®šä¸ªä½“ï¼Œæ— æ³•åº”ç”¨äºä»»æ„ä¸»ä½“ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§å•æ¬¡è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶ï¼ˆSPEAKï¼‰ï¼Œé€šè¿‡å¼•å…¥äº’é‡æ„ç‰¹å¾è§£è€¦ï¼ˆIRFDï¼‰æ–¹æ³•å°†äººè„¸ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢éƒ¨ç¼–è¾‘æ¨¡å—ï¼Œå°†è¯­éŸ³å†…å®¹å’Œé¢éƒ¨æ½œåœ¨ç ä¿®æ”¹ä¸ºä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œå¹¶æå‡ºä¸€ä¸ªæ–°é¢–çš„ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨ç¼–è¾‘æ¨¡å—ä¿®æ”¹åçš„æ½œåœ¨ç è°ƒèŠ‚åˆæˆé¢éƒ¨åŠ¨ç”»ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ã€å¤´éƒ¨å§¿åŠ¿å’Œè¯­éŸ³å†…å®¹ã€‚</p><p>(4): SPEAKåœ¨åè°ƒçš„å”‡éƒ¨åŠ¨ä½œã€çœŸå®çš„é¢éƒ¨è¡¨æƒ…å’Œå¹³æ»‘çš„å¤´éƒ¨åŠ¨ä½œä¸‹ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººå¤´éƒ¨ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPEAKåœ¨æƒ…æ„Ÿå¯æ§æ€§å’Œå¤´éƒ¨å§¿åŠ¿å¯æ§æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šIRFDï¼šé€šè¿‡å¼•å…¥äº’é‡æ„ç‰¹å¾è§£è€¦ï¼ˆIRFDï¼‰æ–¹æ³•å°†äººè„¸ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ï¼Œåˆ†åˆ«åæ˜ å¤´éƒ¨å§¿åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä»½ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šéŸ³é¢‘ç¼–ç å™¨ï¼šä½¿ç”¨ wav2vec 2.0 æå–éŸ³é¢‘å†…å®¹ç‰¹å¾ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šç¼–è¾‘æ¨¡å—ï¼šå°†éŸ³é¢‘å†…å®¹å’Œé¢éƒ¨æ½œåœ¨ç ä¿®æ”¹ä¸ºä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œä»è€Œå¯¹é½éŸ³é¢‘å†…å®¹å’Œé¢éƒ¨ä¿¡æ¯æ¨¡æ€ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šç”Ÿæˆå™¨ï¼šåˆ©ç”¨ç¼–è¾‘æ¨¡å—ä¿®æ”¹åçš„æ½œåœ¨ç è°ƒèŠ‚åˆæˆé¢éƒ¨åŠ¨ç”»ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ã€å¤´éƒ¨å§¿åŠ¿å’Œè¯­éŸ³å†…å®¹ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§æŠ€æœ¯ï¼Œå¯ä»¥ä»å…¶ä»–è§†é¢‘ä¸­ç”Ÿæˆå‡†ç¡®çš„å”‡å½¢åŒæ­¥ã€å…·æœ‰è‡ªç”±å§¿åŠ¿å’Œæƒ…ç»ªæ§åˆ¶çš„æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„è§£è€¦æ¨¡å— IRFDï¼Œç”¨äºå°†è¾“å…¥æ ·æœ¬åˆ†è§£ä¸ºæƒ…ç»ªã€èº«ä»½å’Œå§¿åŠ¿åµŒå…¥ã€‚ç„¶åï¼Œä¸ºäº†ç”Ÿæˆè¯´è¯å¤´éƒ¨ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è¯´è¯å¤´éƒ¨ç”Ÿæˆæ¡†æ¶ SPEAKã€‚å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°å¾—éå¸¸ç¨³å¥ï¼Œä¾‹å¦‚æ˜¾ç€çš„å§¿åŠ¿å’Œæƒ…ç»ªè¡¨è¾¾å˜åŒ–ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº† IRFD è§£è€¦æ¨¡å—ï¼Œå°†äººè„¸ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ï¼Œåˆ†åˆ«åæ˜ å¤´éƒ¨å§¿åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä»½ï¼›è®¾è®¡äº† SPEAK è¯´è¯å¤´éƒ¨ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨ç¼–è¾‘æ¨¡å—ä¿®æ”¹åçš„æ½œåœ¨ç è°ƒèŠ‚åˆæˆé¢éƒ¨åŠ¨ç”»ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾ã€å¤´éƒ¨å§¿åŠ¿å’Œè¯­éŸ³å†…å®¹ã€‚æ€§èƒ½ï¼šåœ¨åè°ƒçš„å”‡éƒ¨åŠ¨ä½œã€çœŸå®çš„é¢éƒ¨è¡¨æƒ…å’Œå¹³æ»‘çš„å¤´éƒ¨åŠ¨ä½œä¸‹ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººå¤´éƒ¨ï¼›åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPEAK åœ¨æƒ…æ„Ÿå¯æ§æ€§å’Œå¤´éƒ¨å§¿åŠ¿å¯æ§æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦è®­ç»ƒ IRFD è§£è€¦æ¨¡å—å’Œ SPEAK è¯´è¯å¤´éƒ¨ç”Ÿæˆæ¡†æ¶ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4cef68701eebad9ead106562636697ea.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0c56dd339a6a2635e58337d5b57ea661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af84f0c9842d1a0bd09b78951550dfc4.jpg" align="middle"></details><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields. </p><p><a href="http://arxiv.org/abs/2403.17881v4">PDF</a> We closely follow the latest developments in   <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a></p><p><strong>Summary</strong><br>è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æ¨åŠ¨äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹æŠ€æœ¯çš„å‘å±•ï¼Œå¸¦åŠ¨äº†å½±è§†å¨±ä¹ã€äººåƒåˆæˆç­‰é¢†åŸŸçš„ç ”ç©¶åº”ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ·±åº¦ä¼ªé€ æŠ€æœ¯åŒ…å«äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€äººè„¸å±æ€§ç¼–è¾‘å››å¤§ç±»ã€‚</li><li>æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¦‚å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹æ¨åŠ¨äº†æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ã€‚</li><li>å¯¹åº”æ£€æµ‹æŠ€æœ¯ä¸æ–­å‘å±•ï¼Œä»¥è§„èŒƒæ·±åº¦ä¼ªé€ çš„æ½œåœ¨æ»¥ç”¨ï¼Œä¾‹å¦‚ç”¨äºéšç§å…¥ä¾µå’Œç½‘ç»œé’“é±¼æ”»å‡»ã€‚</li><li>ç ”ç©¶äººå‘˜ç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œåº¦é‡æ ‡å‡†ï¼Œå¹¶è®¨è®ºäº†å‘å±•ä¸­çš„æŠ€æœ¯ã€‚</li><li>ä»£è¡¨æ€§æ–¹æ³•åœ¨æµè¡Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œä»¥å…¨é¢è¯„ä¼°æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ã€‚</li><li>æ·±å…¥åˆ†æäº†æ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li><li>æœé›†æ•´ç†äº†ç”¨äºåŸ¹è®­å’Œè¯„ä¼°çš„æ·±åº¦ä¼ªé€ æ•°æ®é›†ï¼Œå¹¶ç»™å‡ºäº†å¦‚ä½•è·å–é€”å¾„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹ï¼šåŸºå‡†ä¸ç»¼è¿°</p></li><li><p>ä½œè€…ï¼šGan Peiã€Jiangning Zhangã€Menghan Huã€Zhenyu Zhangã€Chengjie Wangã€Yunsheng Wuã€Guangtao Zhaiã€Jian Yangã€Chunhua Shenã€Dacheng Tao</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸œå¸ˆèŒƒå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆã€äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘ã€ä¼ªé€ æ£€æµ‹ã€ç»¼è¿°</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.17881v4  [cs.CV]  16 May 2024Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ä¼ªé€ æŠ€æœ¯å¯ä»¥ç”Ÿæˆé«˜åº¦é€¼çœŸçš„é¢éƒ¨å›¾åƒå’Œè§†é¢‘ï¼Œåœ¨å¨±ä¹ã€ç”µå½±åˆ¶ä½œã€æ•°å­—äººåˆ›å»ºç­‰é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨æ½œåŠ›ã€‚éšç€æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œä»¥å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸ºä»£è¡¨çš„æŠ€æœ¯å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç”Ÿæˆæ•ˆæœã€‚æœ€è¿‘ï¼Œå…·æœ‰å¼ºå¤§ç”Ÿæˆèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹çš„å‡ºç°å¼•å‘äº†æ–°ä¸€è½®çš„ç ”ç©¶æµªæ½®ã€‚é™¤äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆä¹‹å¤–ï¼Œç›¸åº”çš„æ£€æµ‹æŠ€æœ¯ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼Œä»¥è§„èŒƒæ·±åº¦ä¼ªé€ çš„æ½œåœ¨æ»¥ç”¨ï¼Œä¾‹å¦‚éšç§å…¥ä¾µå’Œç½‘ç»œé’“é±¼æ”»å‡»ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæ—©æœŸæ–¹æ³•é‡‡ç”¨å…ˆè¿›çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æŠ€æœ¯ï¼Œå®ç°äº†çœ‹ä¼¼é€¼çœŸçš„å›¾åƒç”Ÿæˆï¼Œä½†å…¶æ€§èƒ½ä»ä¸ä»¤äººæ»¡æ„ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡å…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„æœ€æ–°è¿›å±•ï¼Œæ€»ç»“å’Œåˆ†æäº†è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œå¹¶è®¨è®ºå‘å±•æŠ€æœ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†å‡ ä¸ªç›¸å…³å­é¢†åŸŸçš„è¿›å±•ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†å››ä¸ªäººè„¸ä¼ªé€ é¢†åŸŸï¼šäººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘ä»¥åŠä¼ªé€ æ£€æµ‹ã€‚éšåï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªé¢†åŸŸçš„æµè¡Œæ•°æ®é›†å¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå…¨é¢è¯„ä¼°äº†æœ€æ–°å’Œæœ€æœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†æ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬å¯†åˆ‡å…³æ³¨è¯¥é¡¹ç›®çš„æœ€æ–°è¿›å±•ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæœ¬æ–‡åœ¨äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘å’Œä¼ªé€ æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬åœ¨ç”Ÿæˆé€¼çœŸé¢éƒ¨åª’ä½“å†…å®¹å’Œæ£€æµ‹æ·±åº¦ä¼ªé€ æ–¹é¢çš„ç›®æ ‡ã€‚</p><ol><li>Methods:</li></ol><p>(1): æœ¬æ–‡å…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„æœ€æ–°è¿›å±•ï¼Œæ€»ç»“å’Œåˆ†æäº†è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ã€‚</p><p>(2): ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œå¹¶è®¨è®ºå‘å±•æŠ€æœ¯ã€‚</p><p>(3): è®¨è®ºäº†å‡ ä¸ªç›¸å…³å­é¢†åŸŸçš„è¿›å±•ï¼Œå¹¶é‡ç‚¹ç ”ç©¶äº†å››ä¸ªäººè„¸ä¼ªé€ é¢†åŸŸï¼šäººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘ä»¥åŠä¼ªé€ æ£€æµ‹ã€‚</p><p>(4): å¯¹æ¯ä¸ªé¢†åŸŸçš„æµè¡Œæ•°æ®é›†å¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå…¨é¢è¯„ä¼°äº†æœ€æ–°å’Œæœ€æœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ã€‚</p><p>(5): åˆ†æäº†æ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰åœ¨äºå…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„æœ€æ–°è¿›å±•ï¼Œæ€»ç»“å’Œåˆ†æäº†è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„å½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜æä¾›äº†å®è´µçš„å‚è€ƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡ç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†å‘å±•æŠ€æœ¯ï¼Œä¸ºæ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„ç ”ç©¶æä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶ã€‚</p><p>æ€§èƒ½ï¼šæœ¬æ–‡åœ¨äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘å’Œä¼ªé€ æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡æ¶‰åŠçš„é¢†åŸŸå¹¿æ³›ï¼ŒåŒ…æ‹¬æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„å„ä¸ªæ–¹é¢ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-bd825fe7701ae1269a03cc9fcd2ebfab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6cb44fca6ef288c86ccb3c8e9f12f528.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f39a46c1332d51ffe66df4c9815557d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d3ca2d04e45a757c657d4be241bba9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b43074324cef40fcdbcefe9ae1bd2a0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  Listen, Disentangle, and Control Controllable Speech-Driven Talking   Head Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/</id>
    <published>2024-05-22T04:21:50.000Z</published>
    <updated>2024-05-22T04:21:50.550Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-22-æ›´æ–°"><a href="#2024-05-22-æ›´æ–°" class="headerlink" title="2024-05-22 æ›´æ–°"></a>2024-05-22 æ›´æ–°</h1><h2 id="Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images"><a href="#Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images" class="headerlink" title="Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images"></a>Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images</h2><p><strong>Authors:Xiaofei Yu, Yitong Li, Jie Ma</strong></p><p>Remote sensing image change captioning (RSICC) aims at generating human-like language to describe the semantic changes between bi-temporal remote sensing image pairs. It provides valuable insights into environmental dynamics and land management. Unlike conventional change captioning task, RSICC involves not only retrieving relevant information across different modalities and generating fluent captions, but also mitigating the impact of pixel-level differences on terrain change localization. The pixel problem due to long time span decreases the accuracy of generated caption. Inspired by the remarkable generative power of diffusion model, we propose a probabilistic diffusion model for RSICC to solve the aforementioned problems. In training process, we construct a noise predictor conditioned on cross modal features to learn the distribution from the real caption distribution to the standard Gaussian distribution under the Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention module are designed for noise predictor in the reverse process. In testing phase, the well-trained noise predictor helps to estimate the mean value of the distribution and generate change captions step by step. Extensive experiments on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and its individual components. The quantitative results showcase superior performance over existing methods across both traditional and newly augmented metrics. The code and materials will be available online at <a href="https://github.com/Fay-Y/Diffusion-RSCC">https://github.com/Fay-Y/Diffusion-RSCC</a>. </p><p><a href="http://arxiv.org/abs/2405.12875v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åº”ç”¨äºé¥æ„Ÿå›¾åƒå˜åŒ–æè¿°ï¼Œæœ‰æ•ˆå‡è½»åƒç´ å·®å¼‚å¯¹åœ°å½¢å˜åŒ–å®šä½çš„å½±å“ï¼Œæé«˜æè¿°ç²¾åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ—¨åœ¨ç”Ÿæˆäººç±»å¯ç†è§£çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œä»¥è§£é‡ŠåŒæ—¶ç›¸é¥æ„Ÿå›¾åƒå¯¹ä¹‹é—´çš„è¯­ä¹‰å˜åŒ–ã€‚</li><li>é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°ä¸ä»…æ¶‰åŠè·¨æ¨¡æ€ç›¸å…³ä¿¡æ¯çš„æå–å’Œæµç•…æè¿°çš„ç”Ÿæˆï¼Œè¿˜éœ€å‡è½»åƒç´ çº§å·®å¼‚å¯¹åœ°å½¢å˜åŒ–å®šä½çš„å½±å“ã€‚</li><li>æ—¶é—´è·¨åº¦é•¿çš„åƒç´ é—®é¢˜ä¼šé™ä½ç”Ÿæˆæè¿°çš„å‡†ç¡®åº¦ã€‚</li><li>æ‰©æ•£æ¨¡å‹å…·æœ‰æ°å‡ºçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç”¨äºé¥æ„Ÿå›¾åƒå˜åŒ–æè¿°ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li><li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ„å»ºå™ªå£°é¢„æµ‹å™¨ä»¥å­¦ä¹ ä»çœŸå®æè¿°åˆ†å¸ƒåˆ°æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„åˆ†å¸ƒã€‚</li><li>åœ¨æ¨ç†é˜¶æ®µï¼Œè®­ç»ƒå¥½çš„å™ªå£°é¢„æµ‹å™¨æœ‰åŠ©äºä¼°è®¡åˆ†å¸ƒçš„å‡å€¼å¹¶é€æ­¥ç”Ÿæˆå˜åŒ–æè¿°ã€‚</li><li>åœ¨ LEVIR-CC æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä¼ ç»Ÿå’Œæ–°å¢åŠ çš„æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°</p></li><li><p>ä½œè€…ï¼šXiaofei Yu, Yitong Li, Jie Ma</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤–å›½è¯­å¤§å­¦ä¿¡æ¯ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</p></li><li><p>å…³é”®è¯ï¼šé¥æ„Ÿï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå˜åŒ–æè¿°ï¼Œæ³¨æ„åŠ›æœºåˆ¶</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07736, Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šé¥æ„Ÿå›¾åƒå˜åŒ–æè¿°ï¼ˆRSICCï¼‰æ—¨åœ¨ç”Ÿæˆç±»ä¼¼äººç±»è¯­è¨€çš„å¥å­æ¥æè¿°åŒæ—¶ç›¸é¥æ„Ÿå›¾åƒå¯¹ä¹‹é—´çš„è¯­ä¹‰å˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„å˜åŒ–æè¿°ä»»åŠ¡ä¸åŒï¼ŒRSICC ä¸ä»…æ¶‰åŠè·¨ä¸åŒæ¨¡æ€æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶ç”Ÿæˆæµç•…çš„æè¿°ï¼Œè¿˜è¦å‡è½»åƒç´ çº§å·®å¼‚å¯¹åœ°å½¢å˜åŒ–å®šä½çš„å½±å“ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„ RSICC æ–¹æ³•é€šå¸¸é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä½†å®ƒä»¬éš¾ä»¥åŒºåˆ†è¯­ä¹‰å˜åŒ–å’Œä¼ªå˜åŒ–ï¼Œä»è€Œå½±å“æè¿°çš„å‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ RSICC æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ„é€ äº†ä¸€ä¸ªæ¡ä»¶ä¸ºäº¤å‰æ¨¡æ€ç‰¹å¾çš„å™ªå£°é¢„æµ‹å™¨ï¼Œå­¦ä¹ ä»çœŸå®æè¿°åˆ†å¸ƒåˆ°é©¬å°”å¯å¤«é“¾ä¸‹çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„åˆ†å¸ƒã€‚åŒæ—¶ï¼Œåœ¨é€†è¿‡ç¨‹ä¸­è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€èåˆå’Œä¸€ä¸ªå †å è‡ªæ³¨æ„åŠ›æ¨¡å—ç”¨äºå™ªå£°é¢„æµ‹å™¨ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœï¼šåœ¨ LEVIR-CC æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¼ ç»Ÿå’Œæ–°å¢åŠ çš„æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æ–¹æ³•åŒºåˆ†è¯­ä¹‰å˜åŒ–å’Œä¼ªå˜åŒ–çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜äº†æè¿°çš„å‡†ç¡®æ€§ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºï¼šæå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ¡ä»¶ä¸ºäº¤å‰æ¨¡æ€ç‰¹å¾çš„å™ªå£°é¢„æµ‹å™¨ï¼Œå­¦ä¹ ä»çœŸå®æè¿°åˆ†å¸ƒåˆ°é©¬å°”å¯å¤«é“¾ä¸‹çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„åˆ†å¸ƒï¼Œå¹¶è®¾è®¡äº†è·¨æ¨¡æ€èåˆå’Œå †å è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæœ‰æ•ˆåŒºåˆ†è¯­ä¹‰å˜åŒ–å’Œä¼ªå˜åŒ–ï¼Œæé«˜äº†æè¿°çš„å‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡çš„ä¼˜ç‚¹å’Œä¸è¶³æ€»ç»“å¦‚ä¸‹ï¼š</p><p>åˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒå˜åŒ–æè¿°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ¡ä»¶ä¸ºäº¤å‰æ¨¡æ€ç‰¹å¾çš„å™ªå£°é¢„æµ‹å™¨ï¼Œå­¦ä¹ ä»çœŸå®æè¿°åˆ†å¸ƒåˆ°é©¬å°”å¯å¤«é“¾ä¸‹çš„æ ‡å‡†é«˜æ–¯åˆ†å¸ƒçš„åˆ†å¸ƒï¼Œæœ‰æ•ˆåŒºåˆ†è¯­ä¹‰å˜åŒ–å’Œä¼ªå˜åŒ–ï¼Œæé«˜äº†æè¿°çš„å‡†ç¡®æ€§ã€‚- è®¾è®¡äº†è·¨æ¨¡æ€èåˆå’Œå †å è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›å’Œå˜åŒ–å®šä½èƒ½åŠ›ã€‚</p><p>æ€§èƒ½ï¼š- åœ¨ LEVIR-CC æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¼ ç»Ÿå’Œæ–°å¢åŠ çš„æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼š- æœ¬æ–‡æ–¹æ³•éœ€è¦è¾ƒå¤§çš„è®­ç»ƒæ•°æ®é‡å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-21dbd52d9fa2dfab9ed21bd713132601.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ea4cb0070ada153d3948236792884ccd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f43b384f7a1cf699952513394080a478.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-559ce1394523d55dae45d360bd3b2838.jpg" align="middle"></details>## Diffusion for World Modeling: Visual Details Matter in Atari**Authors:Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, FranÃ§ois Fleuret**World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond. [PDF](http://arxiv.org/abs/2405.12399v1) 25 pages, 11 figures, 10 tables**Summary**æ‰©æ•£æ¨¡å‹çš„è§†è§‰ç»†èŠ‚æå‡å¯æ”¹å–„ä¸–ç•Œæ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ ä»£ç†çš„æ€§èƒ½ã€‚**Key Takeaways**- ä¸–ç•Œæ¨¡å‹ä¸ºå¼ºåŒ–å­¦ä¹ ä»£ç†æä¾›äº†ä¸€ç§å®‰å…¨ã€é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚- æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸã€‚- DIAMONDï¼ˆDIffusion As a Model Of eNvironment Dreamsï¼‰æ˜¯ç¬¬ä¸€ä¸ªåœ¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚- DIAMONDåœ¨Atari 100kåŸºå‡†ä¸Šè¾¾åˆ°1.46çš„äººç±»å½’ä¸€åŒ–å¹³å‡å¾—åˆ†ã€‚- æ‰©æ•£æ¨¡å‹å¯ä»¥æ•è·å¯¹å¼ºåŒ–å­¦ä¹ é‡è¦çš„è§†è§‰ç»†èŠ‚ã€‚- DIAMONDä»£ç ã€ä»£ç†å’Œå¯ç©ä¸–ç•Œæ¨¡å‹å·²å¼€æºã€‚- æ‰©æ•£æ¨¡å‹åœ¨ä¸–ç•Œå»ºæ¨¡é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ä¸–ç•Œå»ºæ¨¡çš„æ‰©æ•£ï¼šAtari ä¸­çš„è§†è§‰ç»†èŠ‚è‡³å…³é‡è¦</p></li><li><p>Authors: Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, FranÃ§ois Fleuret</p></li><li><p>Affiliation: æ—¥å†…ç“¦å¤§å­¦</p></li><li><p>Keywords: Diffusion, World Modeling, Reinforcement Learning, Atari</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.12399, Github: https://github.com/eloialonso/diamond</p></li><li><p>Summary:</p><pre><code>            (1): ä¸–ç•Œæ¨¡å‹æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ç”¨äºä»¥å®‰å…¨ä¸”æ ·æœ¬é«˜æ•ˆçš„æ–¹å¼è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚æœ€è¿‘çš„ä¸–ç•Œæ¨¡å‹ä¸»è¦å¯¹ç¦»æ•£æ½œåœ¨å˜é‡åºåˆ—è¿›è¡Œæ“ä½œä»¥å»ºæ¨¡ç¯å¢ƒåŠ¨æ€ã€‚ç„¶è€Œï¼Œè¿™ç§å‹ç¼©æˆç´§å‡‘çš„ç¦»æ•£è¡¨ç¤ºå¯èƒ½ä¼šå¿½ç•¥å¯¹å¼ºåŒ–å­¦ä¹ å¾ˆé‡è¦çš„è§†è§‰ç»†èŠ‚ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒç”Ÿæˆçš„ä¸»å¯¼æ–¹æ³•ï¼ŒæŒ‘æˆ˜äº†å¯¹ç¦»æ•£æ½œåœ¨å˜é‡å»ºæ¨¡çš„æˆç†Ÿæ–¹æ³•ã€‚å—è¿™ç§èŒƒå¼è½¬å˜çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº† DIAMONDï¼ˆDIffusion As a Model Of eNvironment Dreamsï¼‰ï¼Œä¸€ç§åœ¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹ä¸­è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬åˆ†æäº†ä½¿æ‰©æ•£é€‚ç”¨äºä¸–ç•Œå»ºæ¨¡æ‰€éœ€çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¹¶å±•ç¤ºäº†æ”¹è¿›çš„è§†è§‰ç»†èŠ‚å¦‚ä½•æé«˜æ™ºèƒ½ä½“æ€§èƒ½ã€‚DIAMOND åœ¨å…·æœ‰ç«äº‰åŠ›çš„ Atari 100k åŸºå‡†æµ‹è¯•ä¸­è·å¾—äº† 1.46 çš„å¹³å‡äººç±»å½’ä¸€åŒ–åˆ†æ•°ï¼›è¿™æ˜¯åœ¨ä¸–ç•Œæ¨¡å‹ä¸­å®Œå…¨è®­ç»ƒçš„æ™ºèƒ½ä½“çš„æœ€æ–°æˆç»©ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥å¯¹ä¸–ç•Œå»ºæ¨¡æ‰©æ•£çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨ https://github.com/eloialonso/diamond ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ™ºèƒ½ä½“å’Œå¯ç©ä¸–ç•Œæ¨¡å‹ã€‚            (2): æœ€è¿‘çš„ä¸–ç•Œå»ºæ¨¡æ–¹æ³•é€šå¸¸å°†ç¯å¢ƒåŠ¨æ€å»ºæ¨¡ä¸ºç¦»æ•£æ½œåœ¨å˜é‡åºåˆ—ã€‚æ½œåœ¨ç©ºé—´çš„ç¦»æ•£åŒ–æœ‰åŠ©äºé¿å…åœ¨å¤šæ­¥æ—¶é—´èŒƒå›´å†…ç´¯ç§¯è¯¯å·®ã€‚ç„¶è€Œï¼Œè¿™ç§ç¼–ç å¯èƒ½ä¼šä¸¢å¤±ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´æ³›åŒ–æ€§å’Œé‡å»ºè´¨é‡ä¸‹é™ã€‚è¿™å¯¹äºä¿¡æ¯è¦æ±‚ä¸å¤ªæ˜ç¡®çš„æ›´çœŸå®åœºæ™¯å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚è®­ç»ƒè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ˆAutonomous Vehiclesï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§†è§‰è¾“å…¥ä¸­çš„å°ç»†èŠ‚ï¼Œä¾‹å¦‚è¿œå¤„çš„äº¤é€šç¯æˆ–è¡Œäººï¼Œå¯èƒ½ä¼šæ”¹å˜æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚å¢åŠ ç¦»æ•£æ½œåœ¨å˜é‡çš„æ•°é‡å¯ä»¥å‡è½»è¿™ç§æœ‰æŸå‹ç¼©ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚            (3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šåˆ†æä½¿æ‰©æ•£é€‚åˆä¸–ç•Œå»ºæ¨¡æ‰€éœ€çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¹¶å±•ç¤ºäº†æ”¹è¿›çš„è§†è§‰ç»†èŠ‚å¦‚ä½•æé«˜æ™ºèƒ½ä½“æ€§èƒ½ã€‚            (4): åœ¨ Atari 100k åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šï¼ŒDIAMOND å–å¾—äº† 1.46 çš„å¹³å‡äººç±»å½’ä¸€åŒ–åˆ†æ•°ã€‚è¯¥æ€§èƒ½æ”¯æŒäº†ä»–ä»¬åœ¨ä¸–ç•Œæ¨¡å‹ä¸­å®Œå…¨è®­ç»ƒæ™ºèƒ½ä½“çš„ç›®æ ‡ã€‚</code></pre></li><li><p>Methods:</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º DIAMOND çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“åœ¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒã€‚            (2):DIAMOND ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å¯¹ç¯å¢ƒåŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œè€Œä¸æ˜¯ç¦»æ•£æ½œåœ¨å˜é‡åºåˆ—ã€‚            (3):ä½œè€…åˆ†æäº†ä½¿æ‰©æ•£é€‚ç”¨äºä¸–ç•Œå»ºæ¨¡æ‰€éœ€çš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¹¶å±•ç¤ºäº†æ”¹è¿›çš„è§†è§‰ç»†èŠ‚å¦‚ä½•æé«˜æ™ºèƒ½ä½“æ€§èƒ½ã€‚            (4):DIAMOND åœ¨ Atari 100k åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº† 1.46 çš„å¹³å‡äººç±»å½’ä¸€åŒ–åˆ†æ•°ï¼Œè¿™è¡¨æ˜äº†æ‰©æ•£ä¸–ç•Œæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º DIAMOND çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“åœ¨æ‰©æ•£ä¸–ç•Œæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒã€‚DIAMOND åœ¨ Atari 100k åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº† 1.46 çš„å¹³å‡äººç±»å½’ä¸€åŒ–åˆ†æ•°ï¼Œè¿™è¡¨æ˜äº†æ‰©æ•£ä¸–ç•Œæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚            (2):Innovation point: æœ¬æ–‡æå‡ºäº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹ç¯å¢ƒåŠ¨æ€è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç¦»æ•£æ½œåœ¨å˜é‡åºåˆ—ã€‚ Performance: DIAMOND åœ¨ Atari 100k åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šå–å¾—äº† 1.46 çš„å¹³å‡äººç±»å½’ä¸€åŒ–åˆ†æ•°ï¼Œè¿™è¡¨æ˜äº†æ‰©æ•£ä¸–ç•Œæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚ Workload: DIAMOND çš„è®­ç»ƒæˆæœ¬é«˜äºä½¿ç”¨ç¦»æ•£æ½œåœ¨å˜é‡åºåˆ—çš„ä¸–ç•Œæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-72ac1259074913dc48248601ecb6050f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f4d7aa4fb02351e901a1debcb4d39d9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3025a3d3200ab1611ab31f0968676023.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg" align="middle"></details><h2 id="Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas"><a href="#Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas" class="headerlink" title="Images that Sound: Composing Images and Sounds on a Single Canvas"></a>Images that Sound: Composing Images and Sounds on a Single Canvas</h2><p><strong>Authors:Ziyang Chen, Daniel Geng, Andrew Owens</strong></p><p>Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a> </p><p><a href="http://arxiv.org/abs/2405.12221v1">PDF</a> Project site: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p><p><strong>Summary</strong><br>è‡ªç„¶å›¾åƒçš„å£°è°±å›¾æ—¢èƒ½å±•ç°é€¼çœŸçš„è§†è§‰æ•ˆæœï¼Œåˆèƒ½äº§ç”Ÿè‡ªç„¶çš„å£°éŸ³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å£°è°±å›¾æ˜¯å£°éŸ³çš„äºŒç»´è¡¨ç¤ºï¼Œå…¶å¤–è§‚ä¸æˆ‘ä»¬è§†è§‰ä¸–ç•Œä¸­çš„å›¾åƒæˆªç„¶ä¸åŒã€‚</li><li>è‡ªç„¶å›¾åƒä½œä¸ºå£°è°±å›¾æ’­æ”¾æ—¶ï¼Œä¼šäº§ç”Ÿä¸è‡ªç„¶çš„å£°éŸ³ã€‚</li><li>æœ¬ç ”ç©¶åˆæˆå‡ºåŒæ—¶å…·æœ‰è‡ªç„¶å›¾åƒå¤–è§‚å’Œè‡ªç„¶éŸ³é¢‘å£°éŸ³çš„å£°è°±å›¾ï¼Œç§°ä¸ºâ€œå¯è§†åŒ–å£°éŸ³â€ã€‚</li><li>è¯¥æ–¹æ³•é‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ ï¼Œåˆ©ç”¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­çš„é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å£°è°±å›¾æ‰©æ•£æ¨¡å‹ã€‚</li><li>é€†å‘è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡éŸ³é¢‘å’Œå›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¡Œå¯¹å™ªå£°æ½œåœ¨å˜é‡è¿›è¡Œå»å™ªï¼Œç”Ÿæˆæ»¡è¶³ä¸¤ä¸ªæ¨¡å‹è¦æ±‚çš„æ ·æœ¬ã€‚</li><li>å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸç”Ÿæˆäº†ä¸ç›®æ ‡éŸ³é¢‘æç¤ºä¸€è‡´ã€åŒæ—¶å…·æœ‰ç›®æ ‡å›¾åƒæç¤ºè§†è§‰å¤–è§‚çš„å£°è°±å›¾ã€‚</li><li>æ›´è¯¦ç»†çš„ç ”ç©¶ç»“æœè¯·è§é¡¹ç›®ä¸»é¡µï¼š<a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šå›¾åƒå³å£°éŸ³ï¼šåœ¨å•ä¸€ç”»å¸ƒä¸Šåˆæˆå›¾åƒå’Œå£°éŸ³</p></li><li><p>ä½œè€…ï¼šZiyang Chen, Daniel Geng, Andrew Owens</p></li><li><p>éš¶å±å•ä½ï¼šå¯†æ­‡æ ¹å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šå›¾åƒåˆ°å£°éŸ³ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œé›¶æ ·æœ¬å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://ificl.github.io/images-that-sound/ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå£°è°±å›¾æ˜¯å£°éŸ³çš„äºŒç»´è¡¨ç¤ºï¼Œä¸æˆ‘ä»¬è§†è§‰ä¸–ç•Œä¸­çš„å›¾åƒçœ‹èµ·æ¥éå¸¸ä¸åŒã€‚å½“è‡ªç„¶å›¾åƒä»¥å£°è°±å›¾çš„å½¢å¼æ’­æ”¾æ—¶ï¼Œä¼šäº§ç”Ÿä¸è‡ªç„¶çš„å£°éŸ³ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•æ— æ³•åŒæ—¶ç”Ÿæˆæ—¢åƒè‡ªç„¶å›¾åƒåˆåƒè‡ªç„¶éŸ³é¢‘çš„å£°è°±å›¾ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”é›¶æ ·æœ¬çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å£°è°±å›¾æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œã€‚åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨éŸ³é¢‘å’Œå›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¡Œå¯¹å™ªå£°æ½œå˜é‡è¿›è¡Œå»å™ªï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªåŒæ—¶ç¬¦åˆè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ ·æœ¬ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šé€šè¿‡å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥ç ”ç©¶ï¼Œæœ¬æ–‡çš„æ–¹æ³•æˆåŠŸç”Ÿæˆäº†ä¸æ‰€éœ€éŸ³é¢‘æç¤ºä¸€è‡´ï¼ŒåŒæ—¶å…·æœ‰æ‰€éœ€å›¾åƒæç¤ºè§†è§‰å¤–è§‚çš„å£°è°±å›¾ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å£°è°±å›¾æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨éŸ³é¢‘å’Œå›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¡Œå¯¹å™ªå£°æ½œå˜é‡è¿›è¡Œå»å™ªï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¾—åˆ°ä¸€ä¸ªåŒæ—¶ç¬¦åˆè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ ·æœ¬ã€‚</p></li><li><p>ç»“è®ºï¼š</p><pre><code>            ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œè¡¨æ˜ï¼Œè‡ªç„¶å›¾åƒçš„åˆ†å¸ƒä¸è‡ªç„¶å£°è°±å›¾çš„åˆ†å¸ƒä¹‹é—´å­˜åœ¨éå¹³å‡¡çš„é‡å ã€‚æˆ‘ä»¬é€šè¿‡ä»è¿™ä¸¤ä¸ªåˆ†å¸ƒçš„äº¤é›†ä¸­è¿›è¡Œé‡‡æ ·æ¥è¯æ˜è¿™ä¸€ç‚¹ï¼Œä»è€Œå¾—åˆ°çœ‹èµ·æ¥åƒçœŸå®å›¾åƒä½†å¬èµ·æ¥åƒè‡ªç„¶å£°éŸ³çš„å£°è°±å›¾ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œç”±äºå£°ç å™¨æœ¬è´¨ä¸Šæ˜¯æœ‰æŸçš„ï¼Œå› æ­¤é€šå¸¸æ— æ³•å®ç°å®Œç¾çš„å¾ªç¯ä¸€è‡´æ€§ã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ä¸ªç®€å•ä¸”é›¶æ ·æœ¬çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å£°è°±å›¾æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œï¼Œåœ¨åå‘è¿‡ç¨‹ä¸­å¹¶è¡Œå¯¹å™ªå£°æ½œå˜é‡è¿›è¡Œå»å™ªï¼Œå¾—åˆ°ä¸€ä¸ªåŒæ—¶ç¬¦åˆè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ ·æœ¬ï¼›æ€§èƒ½ï¼šé€šè¿‡å®šé‡è¯„ä¼°å’Œæ„ŸçŸ¥ç ”ç©¶ï¼Œæœ¬æ–‡çš„æ–¹æ³•æˆåŠŸç”Ÿæˆäº†ä¸æ‰€éœ€éŸ³é¢‘æç¤ºä¸€è‡´ï¼ŒåŒæ—¶å…·æœ‰æ‰€éœ€å›¾åƒæç¤ºè§†è§‰å¤–è§‚çš„å£°è°±å›¾ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•ç®€å•æ˜“ç”¨ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ¨¡å‹ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-44e9096dfe8b1eb6e7cbea03451f9e61.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3f85b8a4d2b38d0e0dd599904b6101cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-030148a4e48570d9fe061e8cc613146d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f3517b1b23ac9838a5e3355e6bbc727.jpg" align="middle"><img src="https://picx.zhimg.com/v2-01ecd2fc03770b0401757015953e2d0a.jpg" align="middle"></details><h2 id="Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices"><a href="#Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices" class="headerlink" title="Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models   Using Spatio-Temporal Slices"></a>Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models   Using Spatio-Temporal Slices</h2><p><strong>Authors:Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</strong></p><p>Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Sliceditâ€™s ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: <a href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a> </p><p><a href="http://arxiv.org/abs/2405.12211v1">PDF</a> ICML 2024. Code and examples are available at   <a href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a></p><p><strong>Summary</strong><br>åŸºäºè‡ªç„¶è§†é¢‘çš„æ—¶ç©ºåˆ‡ç‰‡ä¸çœŸå®å›¾åƒå…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ï¼Œå¯åˆ©ç”¨é¢„è®­ç»ƒçš„ T2I æ‰©æ•£æ¨¡å‹å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œå¢å¼ºè§†é¢‘ç¼–è¾‘ä¸­çš„æ—¶é—´ä¸€è‡´æ€§</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨é¢„è®­ç»ƒçš„ T2I æ‰©æ•£æ¨¡å‹æ¥å¢å¼ºæ—¶ç©ºä¸€è‡´æ€§</li><li>Slicedit æ–¹æ³•åŒæ—¶å¤„ç†ç©ºé—´å’Œæ—¶ç©ºåˆ‡ç‰‡</li><li>ç”Ÿæˆè§†é¢‘ä¿ç•™åŸå§‹è§†é¢‘çš„ç»“æ„å’Œè¿åŠ¨ï¼ŒåŒæ—¶ç¬¦åˆç›®æ ‡æ–‡æœ¬</li><li>åœ¨å¹¿æ³›å®éªŒä¸­ï¼Œè¯æ˜ Slicedit èƒ½å¤Ÿç¼–è¾‘å„ç§çœŸå®è§†é¢‘</li><li>æ˜æ˜¾ä¼˜äºç°æœ‰çš„ç«äº‰æ–¹æ³•</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Sliceditï¼šåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œæ—¶ç©ºåˆ‡ç‰‡çš„é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘</p></li><li><p>Authors: Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</p></li><li><p>Affiliation: å·´é»çŸ¿ä¸š-PSLç ”ç©¶å¤§å­¦</p></li><li><p>Keywords: æ–‡æœ¬åˆ°å›¾åƒ, è§†é¢‘ç¼–è¾‘, æ‰©æ•£æ¨¡å‹, æ—¶ç©ºåˆ‡ç‰‡</p></li><li><p>Urls: </p></li><li><p>Summary: </p><p>(1): æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆå’Œç¼–è¾‘ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ç„¶è€Œï¼Œå°†è¿™äº›é¢„è®­ç»ƒæ¨¡å‹ç”¨äºè§†é¢‘ç¼–è¾‘è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è®¸å¤šç°æœ‰å·¥ä½œè¯•å›¾é€šè¿‡åƒç´ ç©ºé—´æˆ–æ·±åº¦ç‰¹å¾ä¹‹é—´çš„æ˜¾å¼å¯¹åº”æœºåˆ¶æ¥å¢å¼ºç¼–è¾‘è§†é¢‘ä¸­çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éš¾ä»¥å¤„ç†å¼ºçƒˆçš„éåˆšæ€§è¿åŠ¨ã€‚</p><p>(2): æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»æ ¹æœ¬ä¸Šä¸åŒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä»¥ä¸‹è§‚å¯Ÿï¼šè‡ªç„¶è§†é¢‘çš„æ—¶ç©ºåˆ‡ç‰‡è¡¨ç°å‡ºä¸è‡ªç„¶å›¾åƒç›¸ä¼¼çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œé€šå¸¸ä»…ç”¨ä½œè§†é¢‘å¸§å…ˆéªŒçš„ç›¸åŒ T2I æ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡åœ¨æ—¶ç©ºåˆ‡ç‰‡ä¸Šåº”ç”¨å®ƒæ¥ä½œä¸ºå¢å¼ºæ—¶é—´ä¸€è‡´æ€§çš„å¼ºå…ˆéªŒã€‚</p><p>(3): åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº† Sliceditï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–‡æœ¬çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„ T2I æ‰©æ•£æ¨¡å‹å¤„ç†ç©ºé—´å’Œæ—¶ç©ºåˆ‡ç‰‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„è§†é¢‘ä¿ç•™äº†åŸå§‹è§†é¢‘çš„ç»“æ„å’Œè¿åŠ¨ï¼ŒåŒæ—¶éµå¾ªç›®æ ‡æ–‡æœ¬ã€‚</p><p>(4): é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº† Slicedit ç¼–è¾‘å„ç§çœŸå®ä¸–ç•Œè§†é¢‘çš„èƒ½åŠ›ï¼Œè¯å®äº†å…¶ä¸ç°æœ‰ç«äº‰æ–¹æ³•ç›¸æ¯”çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡º Sliceditï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–‡æœ¬çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„ Text-to-Imageï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¤„ç†ç©ºé—´å’Œæ—¶ç©ºåˆ‡ç‰‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å°†æ—¶ç©ºåˆ‡ç‰‡ä½œä¸ºå¢å¼ºæ—¶é—´ä¸€è‡´æ€§çš„å¼ºå…ˆéªŒï¼Œé€šè¿‡åœ¨æ—¶ç©ºåˆ‡ç‰‡ä¸Šåº”ç”¨ T2I æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆè§†é¢‘ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šSlicedit ç¼–è¾‘è§†é¢‘æ—¶ä¿ç•™äº†åŸå§‹è§†é¢‘çš„ç»“æ„å’Œè¿åŠ¨ï¼ŒåŒæ—¶éµå¾ªç›®æ ‡æ–‡æœ¬ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘æ–¹æ³• Sliceditï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œäº†ä¿®æ”¹ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†é¢‘ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå®ƒå°†æœ€åˆè®¾è®¡ç”¨äºå›¾åƒçš„é¢„è®­ç»ƒå»å™ªå™¨ä¹Ÿåº”ç”¨äºè§†é¢‘çš„æ—¶ç©ºåˆ‡ç‰‡ã€‚ä¸ºäº†ç¼–è¾‘è§†é¢‘ï¼Œæˆ‘ä»¬åœ¨ DDPM åæ¼”è¿‡ç¨‹ä¸­ä½¿ç”¨æˆ‘ä»¬è†¨èƒ€çš„å»å™ªå™¨ï¼ŒåŒæ—¶å°†æºè§†é¢‘çš„æ‰©å±•æ³¨æ„åŠ›æ³¨å…¥ç›®æ ‡è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨ç¼–è¾‘è§†é¢‘æ—¶ä¿ç•™äº†æœªæŒ‡å®šåŒºåŸŸï¼ŒåŒæ—¶ä¸å½±å“æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡ç¼–è¾‘ä¿çœŸåº¦ã€ç»“æ„ä¿ç•™å’Œæ—¶é—´ä¸€è‡´æ€§æŒ‡æ ‡å¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¾…ä»¥ç”¨æˆ·ç ”ç©¶ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿ç•™è¾“å…¥è§†é¢‘çš„ç»“æ„æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒåœ¨å…¨å±€ç¼–è¾‘ä»»åŠ¡ä¸­é‡åˆ°äº†æŒ‘æˆ˜ï¼Œä¾‹å¦‚å°†è‡ªç„¶è§†é¢‘çš„å¸§è½¬æ¢ä¸ºç»˜ç”»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…é™äºä¿ç•™ç»“æ„çš„ç¼–è¾‘ã€‚è¿™æ˜¯ç”±äºä½¿ç”¨äº†å¸¦æœ‰æ³¨æ„åŠ›æ³¨å…¥çš„ DDPM åæ¼”ã€‚å›¾ 11 ä¸­æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹å¤±è´¥æ¡ˆä¾‹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„é›¶æ ·æœ¬è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºè§†é¢‘çš„æ—¶ç©ºåˆ‡ç‰‡ä»¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¼–è¾‘ä¿çœŸåº¦ã€ç»“æ„ä¿ç•™å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚å·¥ä½œé‡ï¼šæˆ‘ä»¬çš„æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸”ç¼–è¾‘è¿‡ç¨‹å¯èƒ½éœ€è¦å¤§é‡è®¡ç®—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7ad40d7ffd4fdfec179a13d80066e3bf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fc0922570bcd1ad99da98532754eebb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db1b0305aeb4fd36b0e3253f5b88f485.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7730aa9df76f69b4353b0e3ce05aaa74.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e869c60df6712ebe7f060fa84c38f40e.jpg" align="middle"></details>## Evolving Storytelling: Benchmarks and Methods for New Character   Customization with Diffusion Models**Authors:Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot**Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons. [PDF](http://arxiv.org/abs/2405.11852v1) **Summary**æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥æ–°è§’è‰²æ—¶ï¼Œå®šåˆ¶åŒ–æ–¹æ³•EpicEvoå¯æœ‰æ•ˆè§£å†³è§’è‰²ä¸€è‡´æ€§é—®é¢˜ï¼Œé€šè¿‡å•ä¸ªæ•…äº‹èŒƒä¾‹å®ç°æ— ç¼æ•´åˆã€‚**Key Takeaways**- NewEpisodeåŸºå‡†å»ºç«‹ï¼Œç”¨äºè¯„ä¼°æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨ä»…ä½¿ç”¨å•ä¸€ç¤ºä¾‹æ•…äº‹çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå…·æœ‰æ–°è§’è‰²çš„å†…å®¹è¿è´¯å›¾åƒã€‚- ç²¾ç‚¼æ•°æ®é›†ï¼Œæ¶ˆé™¤å­—ç¬¦æ³„éœ²å’Œæ–‡æœ¬æ ‡ç­¾ä¸ä¸€è‡´çš„é—®é¢˜ã€‚- EpicEvoæ–¹æ³•ï¼Œé€šè¿‡å•ä¸€æ•…äº‹å®šåˆ¶åŸºäºæ‰©æ•£çš„å¯è§†åŒ–æ•…äº‹ç”Ÿæˆæ¨¡å‹ï¼Œæ— ç¼æ•´åˆæ–°è§’è‰²ã€‚- åŠ å…¥å¯¹æŠ—æ€§å­—ç¬¦å¯¹é½æ¨¡å—ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å°†ç”Ÿæˆå›¾åƒä¸æ–°è§’è‰²ç¤ºä¾‹å›¾åƒå¯¹é½ã€‚- è¿ç”¨çŸ¥è¯†è’¸é¦ï¼Œé˜²æ­¢é—å¿˜è§’è‰²å’ŒèƒŒæ™¯ç»†èŠ‚ã€‚- è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒEpicEvoåœ¨NewEpisodeåŸºå‡†ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚- EpicEvoå¯æœ‰æ•ˆæ•´åˆæ–°è§’è‰²ï¼Œä»…éœ€å•ä¸ªç¤ºä¾‹æ•…äº‹ï¼Œä¸ºè¿è½½æ¼«ç”»ç­‰åº”ç”¨å¼€è¾Ÿæ–°å¯èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: å¯æ¼”åŒ–çš„æ•…äº‹ç”Ÿæˆï¼šç”¨äºæ–°è§’è‰²è‡ªå®šä¹‰çš„åŸºå‡†å’Œæ–¹æ³•</p></li><li><p>Authors: Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot</p></li><li><p>Affiliation: å—æ´‹ç†å·¥å¤§å­¦</p></li><li><p>Keywords: Generative Diffusion Model, Story Visualization, Generative Model Customization</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11852.pdf , Github:None</p></li><li><p>Summary:</p><p>(1): åŸºäºæ‰©æ•£çš„æ¨¡å‹åœ¨æ•…äº‹å¯è§†åŒ–ä¸­å±•ç¤ºäº†ç”Ÿæˆå†…å®¹è¿è´¯å›¾åƒçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•åœ¨ä¿æŒè§’è‰²ä¸€è‡´æ€§çš„åŒæ—¶æœ‰æ•ˆåœ°å°†æ–°è§’è‰²èå…¥ç°æœ‰å™äº‹ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚æœ‰ä¸¤ä¸ªä¸»è¦é™åˆ¶é˜»ç¢äº†è¿›å±•ï¼š(1) ç”±äºæ½œåœ¨çš„è§’è‰²æ³„éœ²å’Œä¸ä¸€è‡´çš„æ–‡æœ¬æ ‡è®°ï¼Œç¼ºå°‘åˆé€‚çš„åŸºå‡†ï¼›(2) åŒºåˆ†æ–°è§’è‰²å’Œæ—§è§’è‰²çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´ç»“æœæ¨¡æ£±ä¸¤å¯ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹æ¥ç”Ÿæˆæ•…äº‹å¯è§†åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š1ï¼‰ç¼ºä¹åˆé€‚çš„åŸºå‡†æ¥è¯„ä¼°ç”Ÿæˆæ¨¡å‹ç”Ÿæˆå…·æœ‰æ–°è§’è‰²çš„æ–°æ•…äº‹çš„é€‚åº”æ€§ã€‚2ï¼‰éš¾ä»¥åŒºåˆ†æ–°è§’è‰²å’Œæ—§è§’è‰²ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœæ¨¡æ£±ä¸¤å¯ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ï¼š1ï¼‰å¼•å…¥ NewEpisode åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«ç»è¿‡æ”¹è¿›çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä½¿ç”¨å•ä¸ªç¤ºä¾‹æ•…äº‹è¯„ä¼°ç”Ÿæˆæ¨¡å‹ç”Ÿæˆå…·æœ‰æ–°è§’è‰²çš„æ–°æ•…äº‹çš„é€‚åº”æ€§ã€‚2ï¼‰æå‡º EpicEvoï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å…·æœ‰æ–°è§’è‰²çš„å•ä¸ªæ•…äº‹æ¥è‡ªå®šä¹‰åŸºäºæ‰©æ•£çš„è§†è§‰æ•…äº‹ç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œå°†æ–°è§’è‰²æ— ç¼é›†æˆåˆ°æ—¢å®šçš„è§’è‰²åŠ¨æ€ä¸­ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ NewEpisode åŸºå‡†ä¸Šå–å¾—äº†ä»¥ä¸‹æ€§èƒ½ï¼š1ï¼‰å®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒEpicEvo åœ¨ NewEpisode åŸºå‡†ä¸Šä¼˜äºç°æœ‰çš„åŸºçº¿ã€‚2ï¼‰å®šæ€§ç ”ç©¶è¯å®äº† EpicEvo åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¯¹è§†è§‰æ•…äº‹ç”Ÿæˆçš„å“è¶Šå®šåˆ¶ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§ä»…ä½¿ç”¨ä¸€ä¸ªç¤ºä¾‹æ•…äº‹å°±èƒ½èåˆæ–°è§’è‰²çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä¸ºè¿è½½æ¼«ç”»ç­‰åº”ç”¨è§£é”äº†æ–°çš„å¯èƒ½æ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>(1): æå‡º NewEpisode åŸºå‡†ï¼Œè¯¥åŸºå‡†åŒ…å«ç»è¿‡æ”¹è¿›çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä½¿ç”¨å•ä¸ªç¤ºä¾‹æ•…äº‹è¯„ä¼°ç”Ÿæˆæ¨¡å‹ç”Ÿæˆå…·æœ‰æ–°è§’è‰²çš„æ–°æ•…äº‹çš„é€‚åº”æ€§ï¼›</p><p>(2): æå‡ºäº† EpicEvoï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å…·æœ‰æ–°è§’è‰²çš„å•ä¸ªæ•…äº‹æ¥è‡ªå®šä¹‰åŸºäºæ‰©æ•£çš„è§†è§‰æ•…äº‹ç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œå°†æ–°è§’è‰²æ— ç¼é›†æˆåˆ°æ—¢å®šçš„è§’è‰²åŠ¨æ€ä¸­ï¼›</p><p>(3): åœ¨ NewEpisode åŸºå‡†ä¸Šå¯¹ EpicEvo è¿›è¡Œäº†å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ EpicEvo åœ¨ç”Ÿæˆå…·æœ‰æ–°è§’è‰²çš„æ–°æ•…äº‹çš„é€‚åº”æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¯¹è§†è§‰æ•…äº‹ç”Ÿæˆè¿›è¡Œå“è¶Šçš„å®šåˆ¶ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è§£å†³äº†æ•…äº‹è§’è‰²å®šåˆ¶çš„éš¾é¢˜ï¼Œæå‡º NewEpisode åŸºå‡†å’Œ EpicEvo æ–¹æ³•ï¼Œä½¿è§†è§‰æ•…äº‹ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä»æœªè§è¿‡çš„è§’è‰²çš„æ–°æ•…äº‹ï¼Œä¸ºè¿è½½æ¼«ç”»ç­‰åº”ç”¨è§£é”äº†æ–°çš„å¯èƒ½æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º NewEpisode åŸºå‡†å’Œ EpicEvo æ–¹æ³•ï¼›æ€§èƒ½ï¼šåœ¨ NewEpisode åŸºå‡†ä¸Šä¼˜äºç°æœ‰çš„åŸºçº¿ï¼Œèƒ½å¤Ÿåœ¨æ‰©æ•£æ¨¡å‹ä¸­å¯¹è§†è§‰æ•…äº‹ç”Ÿæˆè¿›è¡Œå“è¶Šçš„å®šåˆ¶ï¼›å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œæ•´ç† NewEpisode åŸºå‡†æ•°æ®ï¼Œè®­ç»ƒ EpicEvo æ¨¡å‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1bcf790e86915883bf4c5491f4af0617.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b7e49809f39744c919340eadd0a23302.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7829327de314711f1e323c58084208a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aea383233d966c6afec2db0d88be118b.jpg" align="middle"></details>## ViViD: Video Virtual Try-on using Diffusion Models**Authors:Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha**Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD. [PDF](http://arxiv.org/abs/2405.11794v1) **Summary**è§†é¢‘è™šæ‹Ÿè¯•ç©¿é€šè¿‡æ‰©æ•£æ¨¡å‹å®ç°æœè£…åœ¨è§†é¢‘äººä½“ä¸Šçš„è¯•ç©¿ï¼Œè¯¥æ¡†æ¶åŒ…å«æœè£…ç¼–ç å™¨ã€å§¿æ€ç¼–ç å™¨å’Œæ—¶é—´æ¨¡å—ï¼Œå¹¶æ”¶é›†äº†ç”¨äºè§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„æœ€å¤§ã€æœ€å…·å¤šæ ·æ€§æœè£…ç±»å‹å’Œæœ€é«˜åˆ†è¾¨ç‡çš„æ–°æ•°æ®é›†ã€‚**Key Takeaways**- è§†é¢‘è™šæ‹Ÿè¯•ç©¿å°†æœè£…è½¬ç§»åˆ°ç›®æ ‡äººç‰©è§†é¢‘ä¸Šï¼Œä½†é€å¸§åº”ç”¨å›¾åƒè¯•ç©¿ä¼šå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´ã€‚- ViViD æ¡†æ¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥è§£å†³è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ã€‚- æœè£…ç¼–ç å™¨æå–æœè£…è¯­ä¹‰ç‰¹å¾ï¼Œç”¨äºæ•è·æœè£…ç»†èŠ‚å¹¶é€šè¿‡æ³¨æ„åŠ›ç‰¹å¾èåˆæœºåˆ¶æ³¨å…¥ç›®æ ‡è§†é¢‘ã€‚- å§¿åŠ¿ç¼–ç å™¨ç¼–ç å§¿åŠ¿ä¿¡å·ï¼Œä½¿æ¨¡å‹å­¦ä¹ æœè£…ä¸äººä½“å§¿åŠ¿ä¹‹é—´çš„äº¤äº’ã€‚- æ–‡æœ¬åˆ°å›¾åƒç¨³å®šçš„æ‰©æ•£æ¨¡å‹ä¸­åŠ å…¥å±‚æ¬¡åŒ–æ—¶é—´æ¨¡å—ï¼Œå®ç°æ›´è¿è´¯ã€é€¼çœŸçš„è§†é¢‘åˆæˆã€‚- ViViD æ”¶é›†äº†è¿„ä»Šä¸ºæ­¢ç”¨äºè§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„æœ€å¤§ã€æœ€å…·å¤šæ ·æ€§æœè£…ç±»å‹å’Œæœ€é«˜åˆ†è¾¨ç‡çš„æ–°æ•°æ®é›†ã€‚- å®éªŒè¡¨æ˜ï¼ŒViViD èƒ½å¤Ÿäº§ç”Ÿä»¤äººæ»¡æ„çš„è§†é¢‘è¯•ç©¿ç»“æœã€‚- æ•°æ®é›†ã€ä»£ç å’Œæƒé‡å°†å…¬å¼€ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ViViD: ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿</p></li><li><p>Authors: Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha</p></li><li><p>Affiliation: ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</p></li><li><p>Keywords: Video virtual try-on, Diffusion models, Pose encoding, Temporal consistency</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11794, Github: None</p></li><li><p>Summary: </p></li></ol><p>(1): è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ—¨åœ¨å°†ä¸€ä»¶è¡£æœè½¬ç§»åˆ°ç›®æ ‡äººç‰©çš„è§†é¢‘ä¸Šã€‚å°†åŸºäºå›¾åƒçš„è¯•ç©¿æŠ€æœ¯é€å¸§åº”ç”¨äºè§†é¢‘é¢†åŸŸä¼šå¯¼è‡´æ—¶é—´ä¸ä¸€è‡´çš„ç»“æœï¼Œè€Œä¹‹å‰çš„åŸºäºè§†é¢‘çš„è¯•ç©¿è§£å†³æ–¹æ¡ˆåªèƒ½ç”Ÿæˆä½è§†è§‰è´¨é‡å’Œæ¨¡ç³Šçš„ç»“æœã€‚</p><p>(2): è¿‡å»çš„åŸºäºå›¾åƒçš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºè§†é¢‘ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ç¾éš¾æ€§çš„ç»“æœã€‚åŸºäºè§†é¢‘çš„è¯•ç©¿è§£å†³æ–¹æ¡ˆè™½ç„¶å¯ä»¥è§£å†³æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿä½è§†è§‰è´¨é‡å’Œæ¨¡ç³Šçš„ç»“æœã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº† ViViDï¼Œä¸€ä¸ªä½¿ç”¨å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹æ¥è§£å†³è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„æ–°æ¡†æ¶ã€‚ViViD åŒ…å«ä¸€ä¸ªæœè£…ç¼–ç å™¨ï¼Œç”¨äºæå–ç»†ç²’åº¦çš„æœè£…è¯­ä¹‰ç‰¹å¾ï¼ŒæŒ‡å¯¼æ¨¡å‹æ•æ‰æœè£…ç»†èŠ‚å¹¶é€šè¿‡æå‡ºçš„æ³¨æ„åŠ›ç‰¹å¾èåˆæœºåˆ¶å°†å…¶æ³¨å…¥ç›®æ ‡è§†é¢‘ä¸­ã€‚ä¸ºäº†ç¡®ä¿æ—¶ç©ºä¸€è‡´æ€§ï¼ŒViViD å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„å§¿åŠ¿ç¼–ç å™¨æ¥ç¼–ç å§¿åŠ¿ä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœè£…å’Œäººä½“å§¿åŠ¿ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å°†åˆ†å±‚çš„ Temporal æ¨¡å—æ’å…¥åˆ°æ–‡æœ¬åˆ°å›¾åƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­ä»¥å®ç°æ›´è¿è´¯å’Œé€¼çœŸçš„è§†é¢‘åˆæˆã€‚æ­¤å¤–ï¼ŒViViD è¿˜æ”¶é›†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ç”¨äºè§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„æœ€å¤§ã€æœè£…ç±»å‹æœ€å¤šã€åˆ†è¾¨ç‡æœ€é«˜çš„æ•°æ®é›†ã€‚</p><p>(4): å®éªŒè¡¨æ˜ï¼ŒViViD èƒ½å¤Ÿäº§ç”Ÿä»¤äººæ»¡æ„çš„è§†é¢‘è¯•ç©¿ç»“æœã€‚åœ¨ ViViD æ•°æ®é›†ä¸Šï¼ŒViViD åœ¨ FID å’Œ LPIPS åº¦é‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº† ViViD åœ¨è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):è¯¥æ–¹æ³•å°†è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡è§†ä¸ºè§†é¢‘ä¿®å¤é—®é¢˜ï¼Œå°†æœè£…ç²˜è´´åˆ°ä¸æœè£…æ— å…³çš„åŒºåŸŸï¼›            (2):æå‡ºæœè£…ç¼–ç å™¨æå–æœè£…è¯­ä¹‰ç‰¹å¾ï¼Œé€šè¿‡æ³¨æ„åŠ›ç‰¹å¾èåˆæœºåˆ¶æ³¨å…¥ç›®æ ‡è§†é¢‘ä¸­ï¼›            (3):å¼•å…¥è½»é‡çº§å§¿åŠ¿ç¼–ç å™¨ç¼–ç å§¿åŠ¿ä¿¡å·ï¼Œä½¿æ¨¡å‹å­¦ä¹ æœè£…å’Œäººä½“å§¿åŠ¿ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼›            (4):åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­æ’å…¥åˆ†å±‚çš„ Temporal æ¨¡å—ï¼Œå®ç°æ›´è¿è´¯ã€é€¼çœŸçš„è§†é¢‘åˆæˆï¼›            (5):æ”¶é›†æ–°æ•°æ®é›† ViViDï¼ŒåŒ…å«è¿„ä»Šä¸ºæ­¢ç”¨äºè§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡çš„æœ€å¤§ã€æœè£…ç±»å‹æœ€å¤šã€åˆ†è¾¨ç‡æœ€é«˜çš„è§†é¢‘æ•°æ®ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1): æœ¬å·¥ä½œé¦–æ¬¡å°†å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹åº”ç”¨äºè§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ï¼Œæå‡ºäº† ViViD æ¡†æ¶ï¼Œåœ¨è§†é¢‘è™šæ‹Ÿè¯•ç©¿é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚            (2):Innovation point: åˆ›æ–°ç‚¹ï¼šæå‡ºäº†æœè£…ç¼–ç å™¨ã€æ³¨æ„åŠ›ç‰¹å¾èåˆæœºåˆ¶ã€è½»é‡çº§å§¿åŠ¿ç¼–ç å™¨å’Œåˆ†å±‚çš„ Temporal æ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­çš„æœè£…ç»†èŠ‚æ•æ‰ã€æ—¶é—´ä¸€è‡´æ€§ã€æœè£…ä¸äººä½“å§¿åŠ¿äº¤äº’å»ºæ¨¡ç­‰å…³é”®æŒ‘æˆ˜ã€‚Performance: æ€§èƒ½ï¼šåœ¨ ViViD æ•°æ®é›†ä¸Šï¼ŒViViD åœ¨ FID å’Œ LPIPS åº¦é‡æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨è§†é¢‘è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚Workload: å·¥ä½œé‡ï¼šViViD çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0355bce071e350207c70de02bda959ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23ccaa5c8bb5673e1bef077ad2b7d22a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e7b35dffa1cf9920b89882397361f15f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e3d8bba6d2cbf1178e36f754857920d.jpg" align="middle"></details><h2 id="HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos"><a href="#HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos" class="headerlink" title="HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos"></a>HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos</h2><p><strong>Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</strong></p><p>Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers. </p><p><a href="http://arxiv.org/abs/2405.11270v1">PDF</a> </p><p><strong>Summary</strong></p><p>ç”¨å•ç›®è§†é¢‘è·å–å¸¦æœ‰ç‰©ç†æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„å¯å˜å½¢äººä½“æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºç”Ÿæˆå¯åŠ¨ç”»äººä½“æ¨¡å‹ã€‚</li><li>å¼•å…¥ä¿¡æ¯èåˆç­–ç•¥è§£å†³å•ç›®è§†é¢‘è¾“å…¥è§†å›¾ç¨€ç–é—®é¢˜ã€‚</li><li>é‡å»ºäººä½“ä¸ºå¯å˜å½¢ç¥ç»éšå¼æ›²é¢ï¼Œæå–ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºåˆå§‹ç½‘æ ¼ã€‚</li><li>æå‡ºæ–¹æ³•çº æ­£ç²—ç³™ç½‘æ ¼è¾¹ç•Œå’Œå¤§å°åå·®ã€‚</li><li>é‡‡ç”¨å¤šè§†å›¾è¶…åˆ†è¾¨ç‡æ½œæ‰©æ•£æ¨¡å‹å…ˆéªŒçŸ¥è¯†æå–åˆ†è§£çº¹ç†ã€‚</li><li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€è¡¨ç¤ºï¼Œä¸”æ˜¾å¼ç»“æœæ”¯æŒåœ¨é€šç”¨æ¸²æŸ“å™¨ä¸Šéƒ¨ç½²ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: HR Human: ä½¿ç”¨ä¸‰è§’å½¢ç½‘æ ¼å’Œé«˜åˆ†è¾¨ç‡çº¹ç†ä»è§†é¢‘ä¸­å»ºæ¨¡äººä½“åŒ–èº«</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: ä¸­å›½æ­å·</p></li><li><p>Keywords: Human modeling;Rendering;Texture super resolution</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11270</p></li></ol><p>Github: None</p><ol><li><p>Summary:</p><p>(1): è¿‘æœŸï¼Œéšå¼ç¥ç»è¡¨ç¤ºå·²è¢«å¹¿æ³›ç”¨äºç”Ÿæˆå¯åŠ¨ç”»çš„äººä½“åŒ–èº«ã€‚ç„¶è€Œï¼Œè¿™äº›è¡¨ç¤ºä¸­çš„æè´¨å’Œå‡ ä½•å½¢çŠ¶åœ¨ç¥ç»ç½‘ç»œä¸­è€¦åˆï¼Œéš¾ä»¥ç¼–è¾‘ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ä¼ ç»Ÿå›¾å½¢å¼•æ“ä¸­çš„åº”ç”¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦æœ‰ï¼šImplicit animatable human reconstructionã€Relighting4Dã€Relightavatarã€‚è¿™äº›æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼šéšå¼å‡ ä½•å’Œçº¹ç†éš¾ä»¥ç¼–è¾‘ï¼Œäº§ç”Ÿçš„çº¹ç†æ¸…æ™°åº¦ä½ï¼Œæ— æ³•åº”ç”¨äºä¼ ç»Ÿå›¾å½¢å¼•æ“ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­è·å–å…·æœ‰é«˜åˆ†è¾¨ç‡åŸºäºç‰©ç†çš„æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„äººä½“åŒ–èº«çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯èåˆç­–ç•¥ï¼Œå°†å•ç›®è§†é¢‘ä¸­çš„ä¿¡æ¯ä¸åˆæˆçš„è™šæ‹Ÿå¤šè§†å›¾å›¾åƒç›¸ç»“åˆï¼Œä»¥è§£å†³è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§ã€‚æˆ‘ä»¬å°†äººä½“é‡å»ºä¸ºå¯å˜å½¢çš„ç¥ç»éšå¼æ›²é¢ï¼Œå¹¶åœ¨è¡Œä¸ºè‰¯å¥½çš„å§¿æ€ä¸­æå–ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºä¸‹ä¸€é˜¶æ®µçš„åˆå§‹ç½‘æ ¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥çº æ­£æå–çš„ç²—ç³™ç½‘æ ¼çš„è¾¹ç•Œå’Œå¤§å°åå·®ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†æ¥æå–åˆ†è§£çš„çº¹ç†ã€‚</p><p>(4): åœ¨äººä½“å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™ç§æ˜¾å¼ç»“æœæ”¯æŒåœ¨é€šç”¨æ¸²æŸ“å™¨ä¸Šçš„éƒ¨ç½²ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘è·å–å…·æœ‰é«˜åˆ†è¾¨ç‡åŸºäºç‰©ç†çš„æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„äººä½“åŒ–èº«çš„æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯èåˆç­–ç•¥ï¼Œå°†å•ç›®è§†é¢‘ä¸­çš„ä¿¡æ¯ä¸åˆæˆçš„è™šæ‹Ÿå¤šè§†å›¾å›¾åƒç›¸ç»“åˆï¼Œä»¥è§£å†³è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå°†äººä½“é‡å»ºä¸ºå¯å˜å½¢çš„ç¥ç»éšå¼æ›²é¢ï¼Œå¹¶åœ¨è¡Œä¸ºè‰¯å¥½çš„å§¿æ€ä¸­æå–ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºä¸‹ä¸€é˜¶æ®µçš„åˆå§‹ç½‘æ ¼ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥çº æ­£æå–çš„ç²—ç³™ç½‘æ ¼çš„è¾¹ç•Œå’Œå¤§å°åå·®ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šé‡‡ç”¨äº†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†æ¥æå–åˆ†è§£çš„çº¹ç†ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­è·å–å…·æœ‰é«˜åˆ†è¾¨ç‡åŸºäºç‰©ç†çš„æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„äººä½“åŒ–èº«çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºï¼Œå¹¶ä¸”è¿™ç§æ˜¾å¼ç»“æœæ”¯æŒåœ¨é€šç”¨æ¸²æŸ“å™¨ä¸Šçš„éƒ¨ç½²ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯èåˆç­–ç•¥ï¼Œå°†å•ç›®è§†é¢‘ä¸­çš„ä¿¡æ¯ä¸åˆæˆçš„è™šæ‹Ÿå¤šè§†å›¾å›¾åƒç›¸ç»“åˆï¼Œä»¥è§£å†³è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§ï¼›å¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥çº æ­£æå–çš„ç²—ç³™ç½‘æ ¼çš„è¾¹ç•Œå’Œå¤§å°åå·®ï¼›é‡‡ç”¨äº†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†æ¥æå–åˆ†è§£çš„çº¹ç†ã€‚æ€§èƒ½ï¼šåœ¨äººä½“å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦åˆæˆè™šæ‹Ÿå¤šè§†å›¾å›¾åƒï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details><h2 id="Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems"><a href="#Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems" class="headerlink" title="Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver   for Inverse Problems"></a>Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver   for Inverse Problems</h2><p><strong>Authors:Hanyu Chen, Zhixiu Hao, Liying Xiao</strong></p><p>Diffusion models have become a successful approach for solving various image inverse problems by providing a powerful diffusion prior. Many studies tried to combine the measurement into diffusion by score function replacement, matrix decomposition, or optimization algorithms, but it is hard to balance the data consistency and realness. The slow sampling speed is also a main obstacle to its wide application. To address the challenges, we propose Deep Data Consistency (DDC) to update the data consistency step with a deep learning model when solving inverse problems with diffusion models. By analyzing existing methods, the variational bound training objective is used to maximize the conditional posterior and reduce its impact on the diffusion process. In comparison with state-of-the-art methods in linear and non-linear tasks, DDC demonstrates its outstanding performance of both similarity and realness metrics in generating high-quality solutions with only 5 inference steps in 0.77 seconds on average. In addition, the robustness of DDC is well illustrated in the experiments across datasets, with large noise and the capacity to solve multiple tasks in only one pre-trained model. </p><p><a href="http://arxiv.org/abs/2405.10748v1">PDF</a> Codes: <a href="https://github.com/Hanyu-Chen373/DeepDataConsistency">https://github.com/Hanyu-Chen373/DeepDataConsistency</a></p><p><strong>Summary:</strong><br>æ·±åº¦æ•°æ®ä¸€è‡´æ€§é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹æ›´æ–°æ•°æ®ä¸€è‡´æ€§æ­¥éª¤ï¼Œè§£å†³äº†æ‰©æ•£æ¨¡å‹æ±‚è§£é€†é—®é¢˜çš„æŒ‘æˆ˜ï¼Œå±•ç°äº†å“è¶Šçš„ç›¸ä¼¼æ€§å’ŒçœŸå®æ€§è¡¨ç°ã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>æå‡ºæ·±åº¦æ•°æ®ä¸€è‡´æ€§ (DDC) æ–¹æ³•ï¼Œå°†æ•°æ®ä¸€è‡´æ€§æ­¥éª¤ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ›´æ–°ã€‚</li><li>ä½¿ç”¨å˜åˆ†ç•Œè®­ç»ƒç›®æ ‡ï¼Œæœ€å¤§åŒ–æ¡ä»¶åéªŒï¼Œå‡å°‘å…¶å¯¹æ‰©æ•£è¿‡ç¨‹çš„å½±å“ã€‚</li><li>åœ¨çº¿æ€§å’Œéçº¿æ€§ä»»åŠ¡ä¸­ï¼ŒDDC åœ¨ç›¸ä¼¼æ€§å’ŒçœŸå®æ€§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li><li>DDC ä»…éœ€ 5 æ­¥æ¨ç†ï¼Œå¹³å‡è€—æ—¶ 0.77 ç§’ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>DDC åœ¨ä¸åŒæ•°æ®é›†ã€å¤§å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°ç¨³å¥ã€‚</li><li>DDC å¯ä»¥ç”¨ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹è§£å†³å¤šä¸ªä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: æ·±åº¦æ•°æ®ä¸€è‡´æ€§ï¼šä¸€ç§å¿«é€Ÿä¸”é²æ£’çš„æ‰©æ•£æ¨¡å‹æ±‚è§£é€†é—®é¢˜çš„æ¨¡å‹</p></li><li><p>Authors: é™ˆç€šå®‡ï¼Œéƒå¿—ä¿®ï¼Œè‚–ä¸½è‹±</p></li><li><p>Affiliation: æ¸…åå¤§å­¦</p></li><li><p>Keywords: æ‰©æ•£æ¨¡å‹ï¼Œé€†é—®é¢˜ï¼Œæ•°æ®ä¸€è‡´æ€§ï¼ŒçœŸå®æ€§</p></li><li><p>Urls: https://arxiv.org/abs/2405.10748v1, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):æ‰©æ•£æ¨¡å‹åœ¨è§£å†³å›¾åƒé€†é—®é¢˜æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å¦‚ä½•å¹³è¡¡æ•°æ®ä¸€è‡´æ€§å’ŒçœŸå®æ€§æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</p><p>(2):ç°æœ‰æ–¹æ³•åŒ…æ‹¬æ›¿æ¢å¾—åˆ†å‡½æ•°ã€åˆ†è§£çŸ©é˜µæˆ–ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼Œä½†å®ƒä»¬åœ¨æ•°æ®ä¸€è‡´æ€§å’ŒçœŸå®æ€§ä¹‹é—´éš¾ä»¥å¹³è¡¡ï¼Œä¸”æ¨ç†é€Ÿåº¦æ…¢ã€‚</p><p>(3):æœ¬æ–‡æå‡ºæ·±åº¦æ•°æ®ä¸€è‡´æ€§ï¼ˆDDCï¼‰ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ›´æ–°æ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®ä¸€è‡´æ€§æ­¥éª¤ï¼Œé€šè¿‡å˜åˆ†ç•Œè®­ç»ƒç›®æ ‡æœ€å¤§åŒ–æ¡ä»¶åéªŒæ¦‚ç‡ï¼Œå¹¶å‡å°‘å…¶å¯¹æ‰©æ•£è¿‡ç¨‹çš„å½±å“ã€‚</p><p>(4):åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€ä¿®å¤ã€å»æ¨¡ç³Šå’ŒJPEGæ¢å¤ç­‰ä»»åŠ¡ä¸Šï¼ŒDDCåœ¨ä»…éœ€5ä¸ªæ¨ç†æ­¥éª¤ä¸”å¹³å‡è€—æ—¶0.77ç§’çš„æƒ…å†µä¸‹ï¼Œåœ¨ç›¸ä¼¼æ€§å’ŒçœŸå®æ€§æŒ‡æ ‡ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å¹³è¡¡æ•°æ®ä¸€è‡´æ€§å’ŒçœŸå®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒDDCåœ¨ä¸åŒæ•°æ®é›†ã€å¤§å™ªå£°å’Œå•ä¸€é¢„è®­ç»ƒæ¨¡å‹è§£å†³å¤šä»»åŠ¡æ–¹é¢çš„é²æ£’æ€§ä¹Ÿå¾—åˆ°äº†è¯æ˜ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰æå‡ºæ·±åº¦æ•°æ®ä¸€è‡´æ€§ï¼ˆDDCï¼‰ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ›´æ–°æ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®ä¸€è‡´æ€§æ­¥éª¤ï¼Œé€šè¿‡å˜åˆ†ç•Œè®­ç»ƒç›®æ ‡æœ€å¤§åŒ–æ¡ä»¶åéªŒæ¦‚ç‡ï¼Œå¹¶å‡å°‘å…¶å¯¹æ‰©æ•£è¿‡ç¨‹çš„å½±å“ï¼›</p><p>ï¼ˆ2ï¼‰åˆ©ç”¨ç¥ç»ç½‘ç»œæ‹Ÿåˆæ•°æ®ä¸€è‡´æ€§é¡¹ï¼Œå¹¶å°†å…¶èå…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿ç•™æ•°æ®ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆæ›´çœŸå®çš„å›¾åƒï¼›</p><p>ï¼ˆ3ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å˜åˆ†ç•Œè®­ç»ƒç›®æ ‡æœ€å¤§åŒ–æ¡ä»¶åéªŒæ¦‚ç‡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºç”Ÿæˆä¸æ¡ä»¶æ•°æ®ä¸€è‡´çš„çœŸå®å›¾åƒï¼›</p><p>ï¼ˆ4ï¼‰é€šè¿‡å‡å°‘æ•°æ®ä¸€è‡´æ€§é¡¹å¯¹æ‰©æ•£è¿‡ç¨‹çš„å½±å“ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¿«é€Ÿç”Ÿæˆå›¾åƒï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„çœŸå®æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„æ·±åº¦æ•°æ®ä¸€è‡´æ€§ï¼ˆDDCï¼‰æ–¹æ³•ï¼Œåœ¨å¹³è¡¡æ•°æ®ä¸€è‡´æ€§å’ŒçœŸå®æ€§çš„åŒæ—¶ï¼Œå®ç°äº†å¿«é€Ÿæ¨ç†ï¼Œä¸ºæ‰©æ•£æ¨¡å‹æ±‚è§£é€†é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºæ·±åº¦æ•°æ®ä¸€è‡´æ€§ï¼ˆDDCï¼‰æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ›´æ–°æ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®ä¸€è‡´æ€§æ­¥éª¤ï¼Œé€šè¿‡å˜åˆ†ç•Œè®­ç»ƒç›®æ ‡æœ€å¤§åŒ–æ¡ä»¶åéªŒæ¦‚ç‡ï¼Œå¹¶å‡å°‘å…¶å¯¹æ‰©æ•£è¿‡ç¨‹çš„å½±å“ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€ä¿®å¤ã€å»æ¨¡ç³Šå’Œ JPEG æ¢å¤ç­‰ä»»åŠ¡ä¸Šï¼ŒDDC åœ¨ä»…éœ€ 5 ä¸ªæ¨ç†æ­¥éª¤ä¸”å¹³å‡è€—æ—¶ 0.77 ç§’çš„æƒ…å†µä¸‹ï¼Œåœ¨ç›¸ä¼¼æ€§å’ŒçœŸå®æ€§æŒ‡æ ‡ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼šDDC çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦ä½¿ç”¨ç¥ç»ç½‘ç»œæ‹Ÿåˆæ•°æ®ä¸€è‡´æ€§é¡¹ï¼Œå¹¶å°†å…¶èå…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-825b9ef49219bfe90e547c36af6ae92e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8f3559fc7f4e16bd5efc45f3e874012.jpg" align="middle"><img src="https://pica.zhimg.com/v2-833585aeca5f9fcecaa196677353c9fe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c582c82243d4b2484dbc714bdede51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-321a696bd3140a7780176a7ef30ec4fe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-58f99fcf80754e3e0aae1cad41d5cfeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  Diffusion-RSCC Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-22T04:00:21.000Z</published>
    <updated>2024-05-22T04:00:21.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-22-æ›´æ–°"><a href="#2024-05-22-æ›´æ–°" class="headerlink" title="2024-05-22 æ›´æ–°"></a>2024-05-22 æ›´æ–°</h1><h2 id="GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar"><a href="#GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar" class="headerlink" title="GGAvatar: Geometric Adjustment of Gaussian Head Avatar"></a>GGAvatar: Geometric Adjustment of Gaussian Head Avatar</h2><p><strong>Authors:Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</strong></p><p>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formulaâ€™s limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics. </p><p><a href="http://arxiv.org/abs/2405.11993v1">PDF</a> 9 pages, 5 figures</p><p><strong>Summary</strong><br>GGAvatarï¼Œä¸€ç§æ–°é¢–çš„3Dè™šæ‹Ÿå½¢è±¡è¡¨ç¤ºï¼Œæ—¨åœ¨ç¨³å¥åœ°å¡‘é€ å¸¦æœ‰å¤æ‚ç‰¹å¾å’Œå½¢å˜çš„åŠ¨æ€å¤´éƒ¨è™šæ‹Ÿå½¢è±¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>GGAvatar é‡‡ç”¨äº†ç²—åˆ°ç»†çš„ç»“æ„ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šä¸­æ€§é«˜æ–¯åˆå§‹åŒ–æ¨¡å—å’Œå‡ ä½•å˜å½¢è°ƒèŠ‚å™¨ã€‚</li><li>ä¸­æ€§é«˜æ–¯åˆå§‹åŒ–æ¨¡å—å°†é«˜æ–¯åŸºæœ¬ä½“ä¸å¯å˜å½¢çš„ä¸‰è§’å½¢ç½‘æ ¼é…å¯¹ï¼Œé‡‡ç”¨è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ç­–ç•¥æ¥æ¨¡æ‹Ÿç›®æ ‡å¯¹è±¡åœ¨ä¸­æ€§è¡¨æƒ…ä¸‹çš„å‡ ä½•ç»“æ„ã€‚</li><li>å‡ ä½•å˜å½¢è°ƒèŠ‚å™¨ä¸ºå…¨å±€ç©ºé—´ä¸­çš„æ¯ä¸ªé«˜æ–¯ä½“å¼•å…¥å½¢å˜åŸºç¡€ï¼Œåˆ›å»ºäº†ç»†ç²’åº¦ã€ä½ç»´çš„å½¢å˜è¡Œä¸ºè¡¨ç¤ºï¼Œæœ‰æ•ˆè§£å†³äº†çº¿æ€§æ··åˆè’™çš®é…æ–¹çš„å±€é™æ€§ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGGAvatar å¯ä»¥äº§ç”Ÿé«˜ä¿çœŸæ¸²æŸ“ï¼Œåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šGGAvatarï¼šé«˜æ–¯å¤´éƒ¨å¤´åƒçš„å‡ ä½•è°ƒæ•´</p></li><li><p>ä½œè€…ï¼šXinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</p></li><li><p>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š3D Avatarã€Gaussian Primitivesã€Geometric Deformationã€Neutral Expression Initializationã€Morph Adjuster</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.11993v1ï¼ŒGithubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåˆ›å»ºé«˜ä¿çœŸæ•°å­—å¤´åƒå¯¹äºå…ƒå®‡å®™å’Œå„ç§åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å°†è¿™äº›å¤´åƒæ¨å¹¿åˆ°æœªè§è¿‡çš„å§¿åŠ¿æˆ–è¡¨æƒ…ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼š3DMMæŠ€æœ¯æˆ–ç¥ç»éšå¼è¡¨ç¤ºï¼Œä½†å‰è€…ç¼ºä¹ç»“æ„çµæ´»æ€§ï¼Œåè€…è®­ç»ƒå’Œæ¸²æŸ“æ•ˆç‡ä½ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæå‡ºGGAvatarï¼Œä¸€ç§æ–°çš„3Då¤´åƒè¡¨ç¤ºï¼Œå®ƒé‡‡ç”¨ç²—åˆ°ç»†çš„ç»“æ„ï¼ŒåŒ…æ‹¬ä¸­æ€§é«˜æ–¯åˆå§‹åŒ–æ¨¡å—å’Œå‡ ä½•å˜å½¢è°ƒæ•´å™¨ï¼Œåˆ†åˆ«ç”¨äºå¯¹ç›®æ ‡å¯¹è±¡çš„ä¸­æ€§è¡¨æƒ…å‡ ä½•ç»“æ„å»ºæ¨¡å’Œå¼•å…¥å˜å½¢åŸºä»¥æœ‰æ•ˆè§£å†³çº¿æ€§æ··åˆè’™çš®å…¬å¼çš„å±€é™æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨é«˜ä¿çœŸæ¸²æŸ“ä»»åŠ¡ä¸Šï¼ŒGGAvatarä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨ç¦»æ•£é«˜æ–¯åŸºå…ƒï¼ˆGaussian primitivesï¼‰å¯¹ç›®æ ‡å¯¹è±¡çš„ä¸­æ€§è¡¨æƒ…å‡ ä½•ç»“æ„å»ºæ¨¡ï¼Œåˆå§‹åŒ–3Då¤´åƒï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¼•å…¥å˜å½¢åŸºï¼Œæœ‰æ•ˆè§£å†³çº¿æ€§æ··åˆè’™çš®å…¬å¼ï¼ˆLinear Blending Skinningï¼‰çš„å±€é™æ€§ï¼Œå®ç°ç²¾ç»†å˜å½¢ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡å¤šåˆ†è¾¨ç‡ä¸‰å¹³é¢å­˜å‚¨å¤´éƒ¨å‘¨å›´çš„é«˜é¢‘ç©ºé—´ä¿¡æ¯ï¼Œå­¦ä¹ é¢å¤–çš„åŸºï¼Œå®ç°é«˜ç²¾åº¦å˜å½¢ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨L1æŸå¤±ã€D-SSIMæŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ç›‘ç£è®­ç»ƒï¼Œå¹¶æ·»åŠ ä½ç½®å’Œç¼©æ”¾æ­£åˆ™åŒ–é¡¹ï¼Œä¿è¯é«˜æ–¯åŸºå…ƒçš„åˆç†æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„3Då¤´åƒè¡¨ç¤ºGGAvatarï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æœªè§è¿‡çš„å§¿åŠ¿æˆ–è¡¨æƒ…ä¸‹çš„æ¨å¹¿é—®é¢˜ï¼Œä¸ºå…ƒå®‡å®™å’Œå„ç§åº”ç”¨ä¸­çš„é«˜ä¿çœŸæ•°å­—å¤´åƒåˆ›å»ºæä¾›äº†æ–°çš„é€”å¾„ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç²—åˆ°ç»†çš„ç»“æ„ï¼ŒåŒ…æ‹¬ä¸­æ€§é«˜æ–¯åˆå§‹åŒ–æ¨¡å—å’Œå‡ ä½•å˜å½¢è°ƒæ•´å™¨ï¼Œåˆ†åˆ«ç”¨äºå¯¹ç›®æ ‡å¯¹è±¡çš„ä¸­æ€§è¡¨æƒ…å‡ ä½•ç»“æ„å»ºæ¨¡å’Œå¼•å…¥å˜å½¢åŸºä»¥æœ‰æ•ˆè§£å†³çº¿æ€§æ··åˆè’™çš®å…¬å¼çš„å±€é™æ€§ã€‚æ€§èƒ½ï¼šåœ¨é«˜ä¿çœŸæ¸²æŸ“ä»»åŠ¡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šéœ€è¦ç¦»æ•£é«˜æ–¯åŸºå…ƒå¯¹ç›®æ ‡å¯¹è±¡çš„ä¸­æ€§è¡¨æƒ…å‡ ä½•ç»“æ„å»ºæ¨¡ï¼Œå¼•å…¥å˜å½¢åŸºï¼Œå­¦ä¹ é¢å¤–çš„åŸºï¼Œå¹¶ä½¿ç”¨L1æŸå¤±ã€D-SSIMæŸå¤±å’Œæ„ŸçŸ¥æŸå¤±ç›‘ç£è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d0b37297e18948031e40fa8e18788ee1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c6d3d44ffb48165e82767bbe3166494f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c5178d3e1bae75cb38fb1b04f261f7cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9b80139053ee885b883063414b7490a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d72be260f2e841855af340127b381ef0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45a63ba45015397a4354d20b5e428a1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d8ef253cd8bfe45e75b2c2ecc8dd03f.jpg" align="middle"></details>## Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion**Authors:Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao**In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&amp;A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/ [PDF](http://arxiv.org/abs/2405.11286v1) **Summary**3Dè§’è‰²ç”Ÿæˆè¿ˆå…¥æ–°é˜¶æ®µï¼Œ Motion Avatarå®ç°æµ·é‡åŠ¨ç”»æ•°æ®åŠ¨æ€ç”Ÿæˆï¼ŒåŠ©æ¨åŠ¨ç‰©3DåŠ¨ç”»ç”ŸæˆæŠ€æœ¯è½åœ°ã€‚**Key Takeaways**- æå‡º Motion Avatarï¼Œå®ç°äººç±»åŠåŠ¨ç‰©3Dé«˜ç²¾å¯å®šåˆ¶åŒ–å½¢è±¡ç”Ÿæˆä¸åŠ¨ä½œé©±åŠ¨ã€‚- LLMè§„åˆ’å™¨å®ç°åŠ¨ä½œä¸å½¢è±¡ç”Ÿæˆåè°ƒç»Ÿä¸€ï¼Œå°†ç”Ÿæˆå¼ä»»åŠ¡è½¬æ¢ä¸ºå¯å®šåˆ¶åŒ–é—®ç­”äº¤äº’ã€‚- Zoo-300K åŠ¨ç‰©åŠ¨ä½œæ•°æ®é›†åŒ…å«çº¦ 30 ä¸‡ä¸ªè·¨ 65 ä¸ªåŠ¨ç‰©ç§ç±»çš„æ–‡æœ¬-åŠ¨ä½œå¯¹ï¼Œæå¤§ä¸°å¯ŒåŠ¨ç‰©åŠ¨ç”»æ•°æ®ã€‚- ZooGen æ•°æ®é›†ç”Ÿæˆç®¡é“ä¸ºåŠ¨ç‰©åŠ¨ç”»ç”Ÿæˆä»»åŠ¡æä¾›å®è´µçš„æ•°æ®èµ„æºã€‚- Motion Avataræ˜¾è‘—æå‡åŠ¨æ€3Dè§’è‰²ç”Ÿæˆæ•ˆç‡ã€‚- è¯¥æŠ€æœ¯ä¸ºç”µå½±åˆ¶ä½œã€æ¸¸æˆã€AR/VRã€äººæœºäº¤äº’ç­‰é¢†åŸŸæä¾›å¼ºå¤§åŠ©åŠ›ã€‚- åŠ¨ç‰©3DåŠ¨ç”»ç”ŸæˆæŠ€æœ¯çš„è½åœ°æœ‰æœ›çªç ´æ•°æ®å’Œæ–¹æ³•ç“¶é¢ˆã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: åŠ¨ä½œåŒ–èº«ï¼šç”Ÿæˆå…·æœ‰ä»»æ„åŠ¨ä½œçš„äººç±»å’ŒåŠ¨ç‰©åŒ–èº«</p></li><li><p>Authors: Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao</p></li><li><p>Affiliation: æ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦</p></li><li><p>Keywords: Motion Avatar, LLM planner, Zoo-300K, Animal motion dataset</p></li><li><p>Urls: https://steve-zeyu-zhang.github.io/MotionAvatar, Github: https://github.com/steve-zeyu-zhang/MotionAvatar</p></li><li><p>Summary:</p></li></ol><p>(1): è¿‘å¹´æ¥ï¼Œäººä»¬å¯¹åˆ›å»º 3D åŒ–èº«å’ŒåŠ¨ä½œäº§ç”Ÿäº†æµ“åšçš„å…´è¶£ï¼Œè¿™å¾—ç›Šäºå®ƒä»¬åœ¨ç”µå½±åˆ¶ä½œã€è§†é¢‘æ¸¸æˆã€AR/VR å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆ 3D åŒ–èº«ç½‘æ ¼æˆ–äº§ç”ŸåŠ¨ä½œåºåˆ—ï¼Œè€Œå°†è¿™ä¸¤ä¸ªæ–¹é¢é›†æˆåœ¨ä¸€èµ·ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè™½ç„¶åŒ–èº«å’ŒåŠ¨ä½œç”Ÿæˆä¸»è¦é’ˆå¯¹äººç±»ï¼Œä½†ç”±äºç¼ºä¹è®­ç»ƒæ•°æ®å’Œæ–¹æ³•ï¼Œå°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°åŠ¨ç‰©ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•è¦ä¹ˆä¸“æ³¨äºç”Ÿæˆ 3D åŒ–èº«ç½‘æ ¼ï¼Œè¦ä¹ˆä¸“æ³¨äºç”ŸæˆåŠ¨ä½œåºåˆ—ï¼Œä½†å°†è¿™ä¸¤ä¸ªæ–¹é¢é›†æˆåœ¨ä¸€èµ·ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é’ˆå¯¹äººç±»ï¼Œè€Œå°†è¿™äº›æŠ€æœ¯æ‰©å±•åˆ°åŠ¨ç‰©ä»ç„¶å­˜åœ¨å›°éš¾ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Motion Avatar çš„åŸºäºä»£ç†çš„æ–°æ–¹æ³•ï¼Œå®ƒå…è®¸é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨ç”Ÿæˆå…·æœ‰åŠ¨ä½œçš„é«˜è´¨é‡å¯å®šåˆ¶äººç±»å’ŒåŠ¨ç‰©åŒ–èº«ã€‚è¯¥æ–¹æ³•æå¤§åœ°æ¨è¿›äº†åŠ¨æ€ 3D è§’è‰²ç”Ÿæˆçš„è¿›å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª LLM è§„åˆ’å™¨æ¥åè°ƒåŠ¨ä½œå’ŒåŒ–èº«ç”Ÿæˆï¼Œå®ƒå°†åˆ¤åˆ«è§„åˆ’è½¬æ¢ä¸ºå¯å®šåˆ¶çš„é—®ç­”æ–¹å¼ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º Zoo-300K çš„åŠ¨ç‰©åŠ¨ä½œæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤§çº¦ 300,000 ä¸ªè·¨è¶Š 65 ä¸ªåŠ¨ç‰©ç±»åˆ«çš„æ–‡æœ¬-åŠ¨ä½œå¯¹åŠå…¶æ„å»ºç®¡é“ ZooGenï¼Œå®ƒä¸ºç¤¾åŒºæä¾›äº†å®è´µçš„èµ„æºã€‚</p><p>(4): åœ¨ç”Ÿæˆå…·æœ‰ä»»æ„åŠ¨ä½œçš„äººç±»å’ŒåŠ¨ç‰©åŒ–èº«æ–¹é¢ï¼ŒMotion Avatar å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å‡ºè‰²çš„æ•ˆæœï¼ŒåŒ…æ‹¬åŠ¨ä½œç”Ÿæˆã€åŒ–èº«ç”Ÿæˆå’ŒåŠ¨ä½œåˆ°åŒ–èº«çš„é‡å®šå‘ã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨åˆ›å»ºé€¼çœŸä¸”å¯å®šåˆ¶çš„ 3D è§’è‰²æ–¹é¢çš„æ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåŸºäºä»£ç†çš„Motion Avataræ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨ç”Ÿæˆå…·æœ‰åŠ¨ä½œçš„é«˜è´¨é‡å¯å®šåˆ¶äººç±»å’ŒåŠ¨ç‰©åŒ–èº«ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šLLMè§„åˆ’å™¨åè°ƒåŠ¨ä½œå’ŒåŒ–èº«ç”Ÿæˆï¼Œå°†åˆ¤åˆ«è§„åˆ’è½¬æ¢ä¸ºå¯å®šåˆ¶çš„é—®ç­”æ–¹å¼ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šZoo-300KåŠ¨ç‰©åŠ¨ä½œæ•°æ®é›†åŒ…å«çº¦300,000ä¸ªè·¨è¶Š65ä¸ªåŠ¨ç‰©ç±»åˆ«çš„æ–‡æœ¬-åŠ¨ä½œå¯¹ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šMotion Avataråœ¨ç”Ÿæˆå…·æœ‰ä»»æ„åŠ¨ä½œçš„äººç±»å’ŒåŠ¨ç‰©åŒ–èº«æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨åŠ¨ä½œç”Ÿæˆã€åŒ–èº«ç”Ÿæˆå’ŒåŠ¨ä½œåˆ°åŒ–èº«çš„é‡å®šå‘ç­‰å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†å‡ºè‰²çš„æ•ˆæœã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶é€šè¿‡æå‡ºåˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†åŠ¨æ€ 3D åŒ–èº«ç”Ÿæˆä¸­çš„æŒç»­æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº† Motion Avatarï¼Œä¸€ç§æ–°é¢–çš„åŸºäºä»£ç†çš„æ–¹æ³•ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ›å»ºå…·æœ‰åŠ¨æ€åŠ¨ä½œçš„é«˜è´¨é‡å¯å®šåˆ¶äººç±»å’ŒåŠ¨ç‰©åŒ–èº«ï¼Œä»è€Œæå¤§åœ°æ¨è¿›äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ LLM è§„åˆ’å™¨ä¿ƒè¿›äº†åŠ¨ä½œå’ŒåŒ–èº«ç”Ÿæˆçš„åè°ƒï¼Œå¢å¼ºäº†åŠ¨æ€åŒ–èº«ä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œå¯ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒZoo-300K æ•°æ®é›†çš„å¼€å‘ä»¥åŠ ZooGen ç®¡é“ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„èµ„æºï¼Œå¼ºè°ƒäº†æˆ‘ä»¬è‡´åŠ›äºæ¨è¿›è·¨å„ç§é¢†åŸŸçš„åŠ¨æ€åŒ–èº«ç”Ÿæˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º Motion Avatar æ–¹æ³•ï¼Œå®ç°äº†åŠ¨ä½œå’ŒåŒ–èº«ç”Ÿæˆçš„ä¸€ä½“åŒ–ï¼Œå¹¶å¼•å…¥äº† LLM è§„åˆ’å™¨ï¼Œå¢å¼ºäº†é€‚åº”æ€§å’Œå¯ç”¨æ€§ã€‚æ€§èƒ½ï¼šåœ¨åŠ¨ä½œç”Ÿæˆã€åŒ–èº«ç”Ÿæˆå’ŒåŠ¨ä½œåˆ°åŒ–èº«çš„é‡å®šå‘ç­‰å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œæ ‡æ³¨å¤§é‡çš„æ•°æ®ï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4d4b6d7293dfe420ebcd255a83e215e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-178d2f15bc2899950bcba3dc7e32fcaa.jpg" align="middle"></details>## HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos**Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo**Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers. [PDF](http://arxiv.org/abs/2405.11270v1) **Summary**åˆ©ç”¨å•ç›®è§†é¢‘ï¼Œå®ç°å¸¦æœ‰é«˜åˆ†è¾¨ç‡ç‰©ç†æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„å¯åŠ¨ç”»äººçš„è·å–ã€‚**Key Takeaways**- åˆ›æ–°æ€§ä¿¡æ¯èåˆç­–ç•¥ï¼Œç»“åˆå•ç›®è§†é¢‘ä¿¡æ¯ï¼Œåˆæˆè™šæ‹Ÿå¤šè§†è§’å›¾åƒï¼Œè§£å†³è¾“å…¥è§†è§’ç¨€ç–é—®é¢˜ã€‚- å°†äººç‰©é‡å»ºä¸ºå¯å˜å½¢ç¥ç»éšå¼æ›²é¢ï¼Œå¹¶æå–è¡Œä¸ºè‰¯å¥½çš„å§¿åŠ¿ä¸­çš„ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºä¸‹ä¸€é˜¶æ®µçš„åˆå§‹ç½‘æ ¼ã€‚- æå‡ºä¸€ç§æ–¹æ³•æ¥çº æ­£ç²—ç³™ç½‘æ ¼è¾¹ç•Œå’Œå°ºå¯¸çš„åå·®ã€‚- é‡‡ç”¨å¤šè§†è§’è¶…åˆ†è¾¨ç‡æ½œæ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œè’¸é¦åˆ†è§£çº¹ç†ã€‚- å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ä¿çœŸæ–¹é¢ä¼˜äºä¹‹å‰çš„è¡¨ç¤ºï¼Œæ˜¾å¼ç»“æœæ”¯æŒåœ¨å¸¸è§æ¸²æŸ“å™¨ä¸Šéƒ¨ç½²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: HR Human: åˆ©ç”¨ä¸‰è§’å½¢ç½‘æ ¼å’Œé«˜åˆ†è¾¨ç‡çº¹ç†ä»è§†é¢‘ä¸­å»ºæ¨¡äººç±»åŒ–èº«</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦</p></li><li><p>Keywords: äººä½“å»ºæ¨¡ã€æ¸²æŸ“ã€æè´¨çº¹ç†</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11270.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): è¿‘æœŸï¼Œéšå¼ç¥ç»è¡¨ç¤ºå·²è¢«å¹¿æ³›ç”¨äºç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«ã€‚ç„¶è€Œï¼Œè¿™äº›è¡¨ç¤ºä¸­çš„æè´¨å’Œå‡ ä½•å½¢çŠ¶åœ¨ç¥ç»ç½‘ç»œä¸­è€¦åˆï¼Œéš¾ä»¥ç¼–è¾‘ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ä¼ ç»Ÿå›¾å½¢å¼•æ“ä¸­çš„åº”ç”¨ã€‚</p><p>(2): ç°æœ‰æ–¹æ³•ï¼š   - éšå¼ç¥ç»è¡¨ç¤ºï¼šå‡ ä½•å’Œæè´¨è€¦åˆï¼Œéš¾ä»¥ç¼–è¾‘ã€‚   - Relighting4D å’Œ Relightavatarï¼šå°è¯•ç”¨éšå¼è¡¨ç¤ºæ¢å¤å…·æœ‰åˆ†ç¦»å‡ ä½•å’Œæè´¨çš„äººç±»åŒ–èº«ï¼Œä½†éšå¼å‡ ä½•å’Œçº¹ç†éš¾ä»¥ç¼–è¾‘ï¼Œä¸”çº¹ç†æ¸…æ™°åº¦è¾ƒä½ã€‚   - Nvdiffrecï¼šä¸“æ³¨äºé‡å»ºæ˜¾å¼çš„ä¸€èˆ¬é™æ€å¯¹è±¡ï¼Œä½†æ— æ³•å¤„ç†éåˆšæ€§ç‰©ä½“å’Œçš®è‚¤çš„åŠ¨æ€è¿åŠ¨ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­è·å–é™„æœ‰é«˜åˆ†è¾¨ç‡åŸºäºç‰©ç†çš„æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„äººç±»åŒ–èº«çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¿¡æ¯èåˆç­–ç•¥ï¼Œç»“åˆå•ç›®è§†é¢‘ä¿¡æ¯å’Œåˆæˆè™šæ‹Ÿå¤šè§†å›¾å›¾åƒï¼Œè§£å†³äº†è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§ã€‚å°†äººç±»é‡å»ºä¸ºå¯å˜å½¢ç¥ç»éšå¼æ›²é¢ï¼Œå¹¶æå–è‰¯å¥½å§¿åŠ¿ä¸‹çš„ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºä¸‹ä¸€é˜¶æ®µçš„åˆå§‹ç½‘æ ¼ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥æ ¡æ­£æå–çš„ç²—ç½‘æ ¼çš„è¾¹ç•Œå’Œå°ºå¯¸åå·®ã€‚æœ€åï¼Œå°†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ç”¨äºåˆ†è§£çº¹ç†ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºï¼Œæ˜¾å¼ç»“æœæ”¯æŒåœ¨å¸¸è§æ¸²æŸ“å™¨ä¸Šéƒ¨ç½²ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­è·å–é™„æœ‰é«˜åˆ†è¾¨ç‡åŸºäºç‰©ç†çš„æè´¨çº¹ç†å’Œä¸‰è§’å½¢ç½‘æ ¼çš„äººç±»åŒ–èº«çš„æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¼•å…¥ä¿¡æ¯èåˆç­–ç•¥ï¼Œç»“åˆå•ç›®è§†é¢‘ä¿¡æ¯å’Œåˆæˆè™šæ‹Ÿå¤šè§†å›¾å›¾åƒï¼Œè§£å†³äº†è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå°†äººç±»é‡å»ºä¸ºå¯å˜å½¢ç¥ç»éšå¼æ›²é¢ï¼Œå¹¶æå–è‰¯å¥½å§¿åŠ¿ä¸‹çš„ä¸‰è§’å½¢ç½‘æ ¼ä½œä¸ºä¸‹ä¸€é˜¶æ®µçš„åˆå§‹ç½‘æ ¼ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥äº†ä¸€ç§æ–¹æ³•æ¥æ ¡æ­£æå–çš„ç²—ç½‘æ ¼çš„è¾¹ç•Œå’Œå°ºå¯¸åå·®ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå°†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ç”¨äºåˆ†è§£çº¹ç†ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„ HR Human æ¡†æ¶çš„æ„ä¹‰åœ¨äºï¼Œå®ƒèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­é‡å»ºå‡ºå¸¦æœ‰ä¸‰è§’å½¢ç½‘æ ¼å’Œå¯¹åº”çš„ PBR æè´¨çº¹ç†çš„æ•°å­—åŒ–èº«ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¿¡æ¯èåˆç­–ç•¥ï¼Œå°†å•ç›®è§†é¢‘çš„ä¿¡æ¯ä¸åˆæˆçš„è™šæ‹Ÿå¤šè§†å›¾å›¾åƒç›¸ç»“åˆï¼Œä»¥å¼¥è¡¥ç¼ºå¤±çš„ç©ºé—´è§†å›¾ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¿®æ­£äº†ä»éšå¼åœºä¸­æå–çš„ç½‘æ ¼çš„è¾¹ç•Œå’Œå°ºå¯¸åå·®ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»è¿‡é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆ†è§£çº¹ç†ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­é‡å»ºå¸¦æœ‰ä¸‰è§’å½¢ç½‘æ ¼å’Œ PBR æè´¨çº¹ç†çš„äººç±»åŒ–èº«çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š   - å¼•å…¥ä¿¡æ¯èåˆç­–ç•¥ï¼Œç»“åˆå•ç›®è§†é¢‘ä¿¡æ¯å’Œåˆæˆè™šæ‹Ÿå¤šè§†å›¾å›¾åƒï¼Œè§£å†³äº†è¾“å…¥è§†å›¾çš„ç¨€ç–æ€§é—®é¢˜ã€‚   - æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥æ ¡æ­£æå–çš„ç²—ç½‘æ ¼çš„è¾¹ç•Œå’Œå°ºå¯¸åå·®ã€‚   - å°†å¤šè§†å›¾è¶…åˆ†è¾¨ç‡ä¸­æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ç”¨äºåˆ†è§£çº¹ç†ã€‚</p><p>æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨é«˜ä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„è¡¨ç¤ºï¼Œæ˜¾å¼ç»“æœæ”¯æŒåœ¨å¸¸è§æ¸²æŸ“å™¨ä¸Šéƒ¨ç½²ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œéœ€è¦è®­ç»ƒä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥åˆ†è§£çº¹ç†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details>## LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer**Authors:Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu**Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html. [PDF](http://arxiv.org/abs/2405.07319v1) SIGGRAPH 2024 conference track**Summary**å¤šå±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰ï¼šä¸€ç§ç”¨äºä»å¤šè§†è§’è§†é¢‘ä¸­è¿›è¡Œé€¼çœŸå¯åŠ¨ç”»æœè£…è½¬ç§»çš„æ–°å‹è¡¨ç¤ºï¼Œå®ƒå°†èº«ä½“å’Œæœè£…è¡¨è¿°ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§åä¸º Layered Gaussian Avatars (LayGA) çš„æ–°è¡¨ç¤ºï¼Œå®ƒå°†èº«ä½“å’Œæœè£…è¡¨è¿°ä¸ºå¯åŠ¨ç”»æœè£…è½¬ç§»çš„ä¸¤ä¸ªç‹¬ç«‹å±‚ã€‚- LayGA åŸºäºä»¥é«˜æ–¯å›¾ä¸ºåŸºç¡€çš„åŒ–èº«ï¼Œå› å…¶å¯¹æœè£…ç»†èŠ‚çš„å‡ºè‰²è¡¨ç¤ºèƒ½åŠ›ã€‚- ä½¿ç”¨äº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸæ¥é‡å»ºå…‰æ»‘è¡¨é¢ï¼Œå¹¶åŒæ—¶è·å¾—äº†èº«ä½“å’Œæœè£…ä¹‹é—´çš„åˆ†å‰²ã€‚- è®­ç»ƒäº†ä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹æ¥è¡¨ç¤ºèº«ä½“å’Œæœè£…ï¼Œå¹¶åˆ©ç”¨é‡å»ºçš„æœè£…å‡ ä½•ä½œä¸ºæ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªçš„ 3D ç›‘ç£ã€‚- æå‡ºäº†ä¸€ç§å‡ ä½•å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸçš„æ¸²æŸ“ã€‚- æ€»çš„æ¥è¯´ï¼Œæå‡ºçš„ LayGA å®ç°äº†é€¼çœŸçš„åŠ¨ç”»å’Œè™šæ‹Ÿè¯•ç©¿ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Layered Gaussian Avatars for Animatable Clothing (åŸºäºåˆ†å±‚é«˜æ–¯åˆ†å¸ƒçš„åŠ¨ç”»æœè£…åŒ–èº«)</p></li><li><p>Authors: Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu</p></li><li><p>Affiliation: æ¸…åå¤§å­¦</p></li><li><p>Keywords: Animatable avatar, clothing transfer, human reconstruction</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.07319.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): åŠ¨ç”»æœè£…ä¼ è¾“æ—¨åœ¨è·¨è§’è‰²ç©¿è¡£å’ŒåŠ¨ç”»æœè£…ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚å¤§å¤šæ•°äººä½“åŒ–èº«å·¥ä½œå°†äººä½“å’Œæœè£…çš„è¡¨ç¤ºçº ç¼ åœ¨ä¸€èµ·ï¼Œè¿™ç»™ä¸åŒèº«ä»½çš„è™šæ‹Ÿè¯•ç©¿å¸¦æ¥äº†å›°éš¾ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œçº ç¼ çš„è¡¨ç¤ºé€šå¸¸æ— æ³•å‡†ç¡®è·Ÿè¸ªæœè£…çš„æ»‘åŠ¨è¿åŠ¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•å°†èº«ä½“å’Œæœè£…è¡¨ç¤ºä¸ºä¸€ä¸ªæ•´ä½“ï¼Œè¿™ä½¿å¾—è™šæ‹Ÿè¯•ç©¿è·¨èº«ä»½å›°éš¾ï¼Œå¹¶ä¸”æ— æ³•å‡†ç¡®è·Ÿè¸ªæœè£…çš„æ»‘åŠ¨è¿åŠ¨ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¡¨ç¤ºå½¢å¼â€”â€”åˆ†å±‚é«˜æ–¯åŒ–èº« (LayGA)ï¼Œå®ƒå°†èº«ä½“å’Œæœè£…è¡¨è¿°ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œç”¨äºä»å¤šè§†å›¾è§†é¢‘ä¸­è¿›è¡Œé€¼çœŸçš„åŠ¨ç”»æœè£…ä¼ è¾“ã€‚LayGA åœ¨å•å±‚é‡å»ºé˜¶æ®µæå‡ºäº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸï¼Œä»¥é‡å»ºå¹³æ»‘æ›²é¢å¹¶åŒæ—¶è·å¾—èº«ä½“å’Œæœè£…ä¹‹é—´çš„åˆ†å‰²ã€‚åœ¨å¤šå±‚æ‹Ÿåˆé˜¶æ®µï¼Œè®­ç»ƒäº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹æ¥è¡¨ç¤ºèº«ä½“å’Œæœè£…ï¼Œå¹¶åˆ©ç”¨é‡å»ºçš„æœè£…å‡ ä½•ä½œä¸º 3D ç›‘ç£ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å‡ ä½•å±‚å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>(4): åœ¨æœè£…è½¬ç§»ä»»åŠ¡ä¸Šï¼ŒLayGA å®ç°äº†é€¼çœŸçš„åŠ¨ç”»å’Œè™šæ‹Ÿè¯•ç©¿ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰è¡¨ç¤ºï¼Œå°†èº«ä½“å’Œæœè£…è¡¨ç¤ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œç”¨äºä»å¤šè§†å›¾è§†é¢‘ä¸­è¿›è¡Œé€¼çœŸçš„åŠ¨ç”»æœè£…ä¼ è¾“ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåœ¨å•å±‚é‡å»ºé˜¶æ®µï¼Œæå‡ºäº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸï¼Œä»¥é‡å»ºå¹³æ»‘æ›²é¢å¹¶åŒæ—¶è·å¾—èº«ä½“å’Œæœè£…ä¹‹é—´çš„åˆ†å‰²ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨å¤šå±‚æ‹Ÿåˆé˜¶æ®µï¼Œè®­ç»ƒäº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å‹æ¥è¡¨ç¤ºèº«ä½“å’Œæœè£…ï¼Œå¹¶åˆ©ç”¨é‡å»ºçš„æœè£…å‡ ä½•ä½œä¸º3Dç›‘ç£ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ­¤å¤–ï¼Œè¿˜æå‡ºäº†å‡ ä½•å±‚å’Œæ¸²æŸ“å±‚ï¼Œç”¨äºé«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œé«˜ä¿çœŸæ¸²æŸ“ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):è¯¥å·¥ä½œæå‡ºäº†åˆ†å±‚é«˜æ–¯åŒ–èº«ï¼ˆLayGAï¼‰è¡¨ç¤ºï¼Œç”¨äºä»å¤šè§†å›¾è§†é¢‘ä¸­è¿›è¡Œé€¼çœŸçš„åŠ¨ç”»æœè£…ä¼ è¾“ã€‚LayGA å°†èº«ä½“å’Œæœè£…è¡¨ç¤ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œå¹¶æå‡ºäº†ä¸€ç³»åˆ—å‡ ä½•çº¦æŸå’Œå¤šå±‚æ‹Ÿåˆç­–ç•¥ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªå’Œé«˜è´¨é‡çš„å‡ ä½•é‡å»ºã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº† LayGA è¡¨ç¤ºï¼Œå°†èº«ä½“å’Œæœè£…è¡¨ç¤ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å±‚ï¼Œå¹¶æå‡ºäº†å‡ ä½•çº¦æŸå’Œå¤šå±‚æ‹Ÿåˆç­–ç•¥ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„æœè£…è·Ÿè¸ªå’Œé«˜è´¨é‡çš„å‡ ä½•é‡å»ºï¼›            æ€§èƒ½ï¼šåœ¨æœè£…è½¬ç§»ä»»åŠ¡ä¸Šï¼ŒLayGA å®ç°äº†é€¼çœŸçš„åŠ¨ç”»å’Œè™šæ‹Ÿè¯•ç©¿ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼›            å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦å¤šè§†å›¾è§†é¢‘è¾“å…¥ï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-cbea179fd85983d0e759d4be018fb59a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68aa80c2ba44dfde97867ba03ebc2814.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80f17e9e8af3606ee233b1b0ca1da60c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-923a67fcbe4586a3709ea7a21a673f85.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-22  GGAvatar Geometric Adjustment of Gaussian Head Avatar</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/</id>
    <published>2024-05-13T08:45:28.000Z</published>
    <updated>2024-05-13T08:45:28.621Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>ä¸€é”®å›¾åƒç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€3Dæ¨¡å‹å’Œè§†é¢‘ï¼Œæ˜¯å›¾åƒåˆ°3Dè¡¨ç¤ºæˆ–å›¾åƒ3Dé‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç›¸æ¯”äºåŸå§‹ç¥ç»è¾å°„åœºï¼Œé«˜æ–¯æº…å°„åœ¨éšå¼3Dé‡å»ºä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li><li>ç¨³å®šæ‰©æ•£æ¨¡å‹å¯ç”¨äºæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ä¼ ç»Ÿçš„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥è·å¾—ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ã€‚</li><li>éš¾ä»¥ç”Ÿæˆé•¿å†…å®¹å’Œè¯­ä¹‰è¿ç»­çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dæ–¹æ³•å¯ä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘3Dæ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dä½¿ç”¨åŸºæœ¬çš„é«˜æ–¯æº…å°„æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œå‡å°‘äº†è§†é¢‘å†…å­˜å’Œè®¡ç®—æœºè®¡ç®—éœ€æ±‚ã€‚</li><li>OneTo3Dè®¾è®¡äº†è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ï¼Œç”¨äºå¯¹è±¡éª¨æ¶ã€‚</li><li>ç»“åˆOneTo3Dæå‡ºçš„å¯é‡æ–°ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æä¸æ§åˆ¶ç®—æ³•ï¼Œåœ¨3Dæ¨¡å‹ç²¾ç¡®å®šä½è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®è¾“å…¥æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢ï¼ŒOneTo3Dçš„æ€§èƒ½ä¼˜äºè¯¥é¢†åŸŸçš„SOTAé¡¹ç›®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ä¸€å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆ</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: æ¾³å¤§åˆ©äºšè«çº³ä»€å¤§å­¦</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>(1): 3D è¡¨å¾æˆ– 3D é‡å»ºé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶éš¾é¢˜ã€‚</p><p>(2): ç°æœ‰çš„ 3D é‡å»ºæ–¹æ³•å¯åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•ç›´æ¥è®¾è®¡å’Œå®Œæˆ 3D é‡å»ºæˆ–å»ºæ¨¡ï¼›éšå¼æ–¹æ³•ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•å’Œç†è®ºå®ç°è¿™äº›ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼ŒNeural Radiance Fields (NeRF) åœ¨éšå¼ 3D è¡¨å¾æˆ–é‡å»ºæ–¹é¢å–å¾—äº†çªå‡ºæˆå°±ã€‚</p><p>(3): æœ¬æ–‡æå‡º OneTo3D æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹å’Œè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„ Gaussian Splatting æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥ç»‘å®šå¯¹è±¡éª¨æ¶ã€‚ç»“åˆé‡æ–°ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æä¸æ§åˆ¶ç®—æ³•ï¼ŒOneTo3D åœ¨ 3D æ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­æ— æ—¶é—´é™åˆ¶çš„ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†ä»¥ä¸‹æˆå°±ï¼šä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹ï¼›ç”Ÿæˆè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ï¼›ç²¾ç¡®æ§åˆ¶ 3D æ¨¡å‹çš„è¿åŠ¨å’ŒåŠ¨ä½œã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å®ç°ä»å•å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ç”Ÿæˆã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):ç”Ÿæˆåˆå§‹3Dæ¨¡å‹ï¼Œä½¿ç”¨DreamGaussianæ¨¡å‹å’ŒZero-1-to-3æ–¹æ³•ï¼›            (2):ç”Ÿæˆå¹¶ç»‘å®šè‡ªé€‚åº”éª¨æ¶ï¼Œè®¾è®¡åŸºæœ¬éª¨æ¶ï¼Œåˆ†æåˆå§‹3Dæ¨¡å‹çš„å‡ ä½•å‚æ•°ä¿¡æ¯ï¼Œå¾®è°ƒéª¨æ¶å‚æ•°ä»¥ä½¿å…¶é€‚åˆå¯¹è±¡çš„èº«ä½“ï¼›            (3):æ–‡æœ¬åˆ°åŠ¨ä½œå’ŒåŠ¨ä½œï¼Œåˆ†æç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤çš„å‘½ä»¤æ„å›¾ï¼Œå°†å‘½ä»¤è½¬æ¢ä¸ºç‰¹å®šåŠ¨ä½œå’Œéª¨æ¶ç›¸å¯¹éª¨éª¼çš„ä¿®æ”¹æ•°æ®ï¼Œæ§åˆ¶ç‰¹å®šéª¨éª¼åœ¨Blenderä¸­å®ç°ç›¸å¯¹è¿åŠ¨ï¼›            (4):èƒŒæ™¯å»é™¤ï¼Œä½¿ç”¨Dreamgaussiançš„process.pyè„šæœ¬æˆ–å…¶ä»–æ–¹æ³•ï¼Œå¯é€‰ä½¿ç”¨å›¾åƒæ£€æµ‹æˆ–è¯­ä¹‰åˆ†å‰²æœºå™¨å­¦ä¹ æ–¹æ³•ï¼›            (5):é¢œè‰²åˆ†ç»„å»é™¤èƒŒæ™¯ï¼Œè®¡ç®—å›¾åƒä¸­æ¯ä¸ªä¸»è¦é¢œè‰²é¡¹çš„æ¯”ä¾‹ï¼Œå°†é¢œè‰²å€¼èŒƒå›´å†…çš„é¢œè‰²é¡¹åˆ’åˆ†ä¸ºä¸åŒçš„é¢œè‰²ç»„ï¼Œå»é™¤é…ç½®æ¯”ä¾‹èŒƒå›´å†…çš„é¢œè‰²ç»„ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬ç¯‡å·¥ä½œæå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„æ–¹æ³•ï¼Œå…·æœ‰ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹ã€ç”Ÿæˆè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€ç²¾ç¡®æ§åˆ¶ 3D æ¨¡å‹çš„è¿åŠ¨å’ŒåŠ¨ä½œç­‰ä¼˜ç‚¹ï¼Œåœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†åˆ›æ–°ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šOneTo3D æ–¹æ³•é¦–æ¬¡å®ç°äº†ä»å•å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ç”Ÿæˆï¼›æ€§èƒ½ï¼šOneTo3D æ–¹æ³•åœ¨ç”Ÿæˆ 3D æ¨¡å‹çš„ç²¾åº¦ã€è§†é¢‘çš„è¯­ä¹‰è¿ç»­æ€§ã€åŠ¨ä½œæ§åˆ¶çš„ç²¾ç¡®æ€§ç­‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šOneTo3D æ–¹æ³•çš„å®ç°éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•å’Œè®¾è®¡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale   Aerial Rendering**Authors:Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu**Recent progress in large-scale scene rendering has yielded Neural Radiance Fields (NeRF)-based models with an impressive ability to synthesize scenes across small objects and indoor scenes. Nevertheless, extending this idea to large-scale aerial rendering poses two critical problems. Firstly, a single NeRF cannot render the entire scene with high-precision for complex large-scale aerial datasets since the sampling range along each view ray is insufficient to cover buildings adequately. Secondly, traditional NeRFs are infeasible to train on one GPU to enable interactive fly-throughs for modeling massive images. Instead, existing methods typically separate the whole scene into multiple regions and train a NeRF on each region, which are unaccustomed to different flight trajectories and difficult to achieve fast rendering. To that end, we propose Aerial-NeRF with three innovative modifications for jointly adapting NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial partitioning and selection method based on drones' poses to adapt different flight trajectories; (2) Using similarity of poses instead of (expert) network for rendering speedup to determine which region a new viewpoint belongs to; (3) Developing an adaptive sampling approach for rendering performance improvement to cover the entire buildings at different heights. Extensive experiments have conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new state-of-the-art results have been achieved on two public large-scale aerial datasets and presented SCUTic dataset. Note that our model allows us to perform rendering over 4 times as fast as compared to multiple competitors. Our dataset, code, and model are publicly available at https://drliuqi.github.io/. [PDF](http://arxiv.org/abs/2405.06214v1) **Summary**é’ˆå¯¹å¤§è§„æ¨¡èˆªæ‹åœºæ™¯ï¼Œæˆ‘ä»¬æå‡º Aerial-NeRFï¼Œå®ƒé’ˆå¯¹ NeRF è¿›è¡Œä¸‰é¡¹åˆ›æ–°æ€§ä¿®æ”¹ï¼Œä»¥è”åˆå®ç° NeRF åœ¨å¤§è§„æ¨¡èˆªæ‹æ¸²æŸ“ä¸­çš„è‡ªé€‚åº”ï¼šè‡ªé€‚åº”ç©ºé—´åˆ†åŒºå’Œé€‰æ‹©æ–¹æ³•ã€åŸºäºå§¿æ€ç›¸ä¼¼æ€§çš„å¿«é€Ÿæ¸²æŸ“å’Œè‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ã€‚**Key Takeaways**- æå‡º Aerial-NeRFï¼Œé’ˆå¯¹å¤§è§„æ¨¡èˆªæ‹åœºæ™¯å¯¹ NeRF è¿›è¡Œä¸‰é¡¹åˆ›æ–°æ€§ä¿®æ”¹ã€‚- ä½¿ç”¨è‡ªé€‚åº”ç©ºé—´åˆ†åŒºå’Œé€‰æ‹©æ–¹æ³•ï¼Œæ ¹æ®æ— äººæœºå§¿æ€è‡ªé€‚åº”ä¸åŒçš„é£è¡Œè½¨è¿¹ã€‚- ä½¿ç”¨å§¿æ€ç›¸ä¼¼æ€§ä»£æ›¿ï¼ˆä¸“å®¶ï¼‰ç½‘ç»œè¿›è¡Œæ¸²æŸ“åŠ é€Ÿï¼Œä»¥ç¡®å®šæ–°è§†ç‚¹å±äºå“ªä¸ªåŒºåŸŸã€‚- å¼€å‘è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ï¼Œä»¥æé«˜æ¸²æŸ“æ€§èƒ½ï¼Œè¦†ç›–ä¸åŒé«˜åº¦çš„æ•´åº§å»ºç­‘ã€‚- å¤§é‡å®éªŒéªŒè¯äº† Aerial-NeRF çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨ä¸¤ä¸ªå…¬å¼€çš„å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†å’Œ SCUTic æ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚- ä¸å¤šä¸ªç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…è®¸æˆ‘ä»¬ä»¥è¶…è¿‡ 4 å€çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚- æˆ‘ä»¬æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€è·å–ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: èˆªæ‹NeRFï¼šå¤§è§„æ¨¡èˆªæ‹æ¸²æŸ“çš„è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé‡‡æ ·</p></li><li><p>Authors: Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu</p></li><li><p>Affiliation: åå—ç†å·¥å¤§å­¦æœªæ¥æŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: View synthesis, large-scale scene rendering, neural radiance fields, fast rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.06214 , https://github.com/drliuqi/Aerial-NeRF</p></li><li><p>Summary:</p><p>(1):NeRFæ¨¡å‹åœ¨å°ç‰©ä½“å’Œå®¤å†…åœºæ™¯æ¸²æŸ“ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å°†å…¶æ‰©å±•åˆ°èˆªæ‹æ¸²æŸ“ä¸­é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå•ä¸ªNeRFæ— æ³•æ¸²æŸ“å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†ä¸­çš„æ•´ä¸ªåœºæ™¯ï¼Œä¼ ç»ŸNeRFæ— æ³•åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒä»¥å®ç°äº¤äº’å¼æµè§ˆã€‚</p><p>(2):ä»¥å¾€æ–¹æ³•å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªåŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªåŒºåŸŸè®­ç»ƒä¸€ä¸ªNeRFï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•é€‚åº”ä¸åŒçš„é£è¡Œè½¨è¿¹ï¼Œæ¸²æŸ“é€Ÿåº¦ä¹Ÿè¾ƒæ…¢ã€‚</p><p>(3):æœ¬æ–‡æå‡ºAerial-NeRFï¼Œé€šè¿‡è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé€‰æ‹©ã€åŸºäºå§¿æ€ç›¸ä¼¼æ€§ç¡®å®šåŒºåŸŸå½’å±ã€è‡ªé€‚åº”é‡‡æ ·ç­‰æ–¹æ³•ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</p><p>(4):Aerial-NeRFåœ¨ä¸¤ä¸ªå…¬å¼€å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†å’Œä¸€ä¸ªè‡ªå»ºæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä¼˜æ€§èƒ½ï¼Œæ¸²æŸ“é€Ÿåº¦æ¯”å…¶ä»–æ–¹æ³•å¿«4å€ä»¥ä¸Šã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè‡ªé€‚åº”ç©ºé—´åˆ’åˆ†ï¼šæ ¹æ®èˆªæ‹æ•°æ®é›†çš„ç‰¹å¾ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†æ–¹æ³•ï¼Œå°†å¤§è§„æ¨¡åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªå°åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸä½¿ç”¨ä¸€ä¸ªNeRFè¿›è¡Œæ¸²æŸ“ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåŸºäºå§¿æ€ç›¸ä¼¼æ€§ç¡®å®šåŒºåŸŸå½’å±ï¼šè®¾è®¡äº†ä¸€ç§åŸºäºå§¿æ€ç›¸ä¼¼æ€§çš„åŒºåŸŸå½’å±ç¡®å®šç®—æ³•ï¼Œæ ¹æ®ç›¸æœºçš„å§¿æ€ä¿¡æ¯å°†èˆªæ‹å›¾åƒåˆ†é…åˆ°ä¸åŒçš„åŒºåŸŸï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº”é‡‡æ ·ï¼šæå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼Œæ ¹æ®ä¸åŒåŒºåŸŸçš„å¤æ‚ç¨‹åº¦å’Œæ¸²æŸ“é€Ÿåº¦è¦æ±‚ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç‚¹æ•°ï¼Œæé«˜æ¸²æŸ“æ•ˆç‡ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåŸºäºç¥ç»ç½‘ç»œçš„åŒºåŸŸèåˆï¼šä½¿ç”¨ç¥ç»ç½‘ç»œå°†ä¸åŒåŒºåŸŸçš„æ¸²æŸ“ç»“æœèåˆæˆæœ€ç»ˆå›¾åƒï¼Œä¿è¯æ¸²æŸ“ç»“æœçš„è¿ç»­æ€§å’Œå‡†ç¡®æ€§ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šåŸºäºæ¦‚ç‡å¯†åº¦å‡½æ•°çš„åŠ é€Ÿé‡‡æ ·ï¼šåˆ©ç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°å¯¹é‡‡æ ·ç‚¹è¿›è¡Œä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† Aerial-NeRFï¼Œä¸€ç§ç”¨äºå¤„ç†å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†çš„é«˜æ•ˆä¸”é²æ£’çš„æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šå¤§å¹…ä¼˜äºç°æœ‰çš„åŒç±»æ–¹æ³•ï¼Œå‡ ä¹è¾¾åˆ° 4 å€ã€‚æ­¤å¤–ï¼Œåœ¨é€‚å½“çš„åˆ’åˆ†åŒºåŸŸæ•°é‡ä¸‹ï¼ŒAerial-NeRF å¯ä»¥ä½¿ç”¨å•ä¸ª GPU æ¸²æŸ“ä»»æ„å¤§çš„åœºæ™¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸ºèˆªæ‹åœºæ™¯å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé€šè¿‡ä¸åŒé«˜åº¦ç›¸æœºçš„é‡‡æ ·èŒƒå›´è¦†ç›–å»ºç­‘ç‰©ã€‚ä¸ SOTA æ¨¡å‹è¿›è¡Œæ›´å¹¿æ³›çš„æ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾æ›´æœ‰æ•ˆï¼ˆä»…ä½¿ç”¨ 1/4 çš„é‡‡æ ·ç‚¹å’Œ 2 GB çš„ GPU å†…å­˜èŠ‚çœï¼‰ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªå¸¸ç”¨æŒ‡æ ‡æ–¹é¢å…·æœ‰å¯æ¯”æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº† SCUTicï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡å¤§å­¦æ ¡å›­åœºæ™¯çš„æ–°å‹èˆªæ‹æ•°æ®é›†ï¼Œå…·æœ‰ä¸å‡åŒ€çš„ç›¸æœºè½¨è¿¹ï¼Œå¯ä»¥éªŒè¯æ¸²æŸ“æ–¹æ³•çš„é²æ£’æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šè‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé‡‡æ ·ï¼›æ€§èƒ½ï¼šæ¸²æŸ“é€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨ä½ï¼›å·¥ä½œé‡ï¼šæ•°æ®é›†æ„å»ºå’Œæ¨¡å‹è®­ç»ƒå¤æ‚åº¦è¾ƒé«˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3f9706ee7489efbc0fffd098a133920f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5cd3300322f846160033228b8f55d45.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7172b9e2d3611b5ec9915962744d54fa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07281c002ad9f4eaef4b0c58ebbaf426.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a2778deaeb7d026e2fd79cf4c5e6e409.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d55c494ec8f956acc30da13f5d75881b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac7e04c419fb74632e6c7f9332f81960.jpg" align="middle"></details>## Residual-NeRF: Learning Residual NeRFs for Transparent Object   Manipulation**Authors:Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski**Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io [PDF](http://arxiv.org/abs/2405.06181v1) **Summary**é€æ˜ç‰©ä½“åœ¨å·¥ä¸šã€åŒ»è¯å’Œå®¶åº­ä¸­æ— å¤„ä¸åœ¨ï¼Œæœºå™¨äººåœ¨æŠ“å–å’Œæ“ä½œè¿™äº›ç‰©ä½“æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚**Key Takeaways**- NeRFs å¯¹åŒ…å«é€æ˜ç‰©ä½“çš„åœºæ™¯ä¸­çš„æ·±åº¦æ„ŸçŸ¥æ•ˆæœå¾ˆå¥½ã€‚- NeRFs åœ¨å¤„ç†æå…·æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“å’Œå…‰ç…§æ¡ä»¶æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚- Residual-NeRF æå‡ºäº†ä¸€ç§æ”¹å–„é€æ˜ç‰©ä½“æ·±åº¦æ„ŸçŸ¥å’Œè®­ç»ƒé€Ÿåº¦çš„æ–¹æ³•ã€‚- é¦–å…ˆå­¦ä¹ åœºæ™¯ä¸­ä¸åŒ…å«å¾…æ“ä½œé€æ˜ç‰©ä½“çš„èƒŒæ™¯ NeRFï¼Œå¯ä»¥å‡å°‘å­¦ä¹ æ–°ç‰©ä½“å˜åŒ–å¸¦æ¥çš„æ­§ä¹‰ã€‚- Residual-NeRF å­¦ä¹ æ¨æ–­æ®‹å·® RGB å€¼å’Œå¯†åº¦ï¼ŒMixnet å­¦ä¹ å¦‚ä½•ç»„åˆèƒŒæ™¯å’Œæ®‹å·® NeRFã€‚- åœ¨åˆæˆæ•°æ®ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒResidual-NeRF çš„ RMSE ä½ 46.1%ï¼ŒMAE ä½ 29.5%ã€‚- çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒè¡¨æ˜ï¼ŒResidual-NeRF èƒ½å¤Ÿç”Ÿæˆæ›´é²æ£’çš„æ·±åº¦å›¾ï¼Œå™ªå£°æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šResidual-NeRFï¼šå­¦ä¹ æ®‹å·® NeRF ä»¥å®ç°é€æ˜ç‰©ä½“æ“ä½œ</p></li><li><p>ä½œè€…ï¼šBardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¡å†…åŸºæ¢…éš†å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€æ·±åº¦æ„ŸçŸ¥ã€é€æ˜ç‰©ä½“ã€æ®‹å·®å­¦ä¹ ã€èƒŒæ™¯å…ˆéªŒ</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.06181   Github é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š      é€æ˜ç‰©ä½“åœ¨å·¥ä¸šã€åŒ»è¯å’Œå®¶åº­ä¸­æ— å¤„ä¸åœ¨ã€‚æŠ“å–å’Œæ“çºµè¿™äº›ç‰©ä½“å¯¹æœºå™¨äººæ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“çš„å®Œæ•´æ·±åº¦å›¾ï¼Œä»è€Œåœ¨æ·±åº¦é‡å»ºä¸­ç•™ä¸‹å­”æ´ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰éå¸¸é€‚åˆåœ¨æœ‰é€æ˜ç‰©ä½“çš„åœºæ™¯ä¸­è¿›è¡Œæ·±åº¦æ„ŸçŸ¥ï¼Œå¹¶ä¸”è¿™äº›æ·±åº¦å›¾å¯ç”¨äºé«˜ç²¾åº¦åœ°æŠ“å–é€æ˜ç‰©ä½“ã€‚åŸºäº NeRF çš„æ·±åº¦é‡å»ºä»ç„¶éš¾ä»¥å¤„ç†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“å’Œç…§æ˜æ¡ä»¶ã€‚</p><p>(2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š      Dex-NeRF å’Œ Evo-NeRF ç­‰æ–¹æ³•è¡¨æ˜ï¼ŒNeRF åœ¨é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¿˜è¡¨æ˜ï¼ŒNeRF å¾€å¾€éš¾ä»¥å¤„ç†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“ï¼Œä¾‹å¦‚å…·æœ‰æŒ‘æˆ˜æ€§å…‰ç…§æ¡ä»¶çš„é…’æ¯æˆ–å¨æˆ¿é”¡ç®”ã€‚é€æ˜ç‰©ä½“çš„æŒ‘æˆ˜æºäºç¼ºä¹ç‰¹å¾ä»¥åŠå¤–è§‚ä¸­å¾ˆå¤§çš„è§†ç‚¹ä¾èµ–æ€§å˜åŒ–ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š      ä¸ºäº†æé«˜é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæˆ‘ä»¬æå‡ºäº† Residual-NeRFã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæœºå™¨äººçš„å·¥ä½œåŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ä¸»è¦æ˜¯é™æ€ä¸”ä¸é€æ˜çš„ï¼Œä¾‹å¦‚æ¶å­ã€æ¡Œå­å’Œæ¡Œå­ã€‚Residual-NeRF åˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œä»¥å‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ã€‚Residual-NeRF é¦–å…ˆé€šè¿‡è®­ç»ƒä¸åŒ…å«é€æ˜ç‰©ä½“çš„å›¾åƒæ¥å­¦ä¹ æ•´ä¸ªåœºæ™¯çš„èƒŒæ™¯ NeRFã€‚ç„¶åï¼ŒResidual-NeRF ä½¿ç”¨åŒ…å«é€æ˜ç‰©ä½“çš„å®Œæ•´åœºæ™¯çš„å›¾åƒæ¥å­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetã€‚</p><p>(4)ï¼šæ–¹æ³•çš„åº”ç”¨ä»»åŠ¡å’Œæ€§èƒ½ï¼š      æˆ‘ä»¬å¯¹åˆæˆå’ŒçœŸå®æ•°æ®è¿›è¡Œäº†å®éªŒï¼Œè¡¨æ˜ Residual-NeRF æé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ã€‚åˆæˆæ•°æ®ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒResidual-NeRF åœ¨ RMSE ä¸Šæ¯”åŸºçº¿ä½ 46.1%ï¼Œåœ¨ MAE ä¸Šä½ 29.5%ã€‚çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒè¡¨æ˜ï¼ŒResidual-NeRF äº§ç”Ÿäº†æ›´ç¨³å¥çš„æ·±åº¦å›¾ï¼Œå™ªç‚¹æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šé¦–å…ˆè®­ç»ƒä¸åŒ…å«é€æ˜ç‰©ä½“çš„å›¾åƒï¼Œå­¦ä¹ æ•´ä¸ªåœºæ™¯çš„èƒŒæ™¯ NeRFï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç„¶åä½¿ç”¨åŒ…å«é€æ˜ç‰©ä½“çš„å®Œæ•´åœºæ™¯çš„å›¾åƒï¼Œå­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œå‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šResidual-NeRF æé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ã€‚</p><p><strong>ç»“è®º</strong></p><p>(1): æœ¬å·¥ä½œé€šè¿‡æå‡º Residual-NeRFï¼Œæé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ï¼Œå¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼š    - åˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œå‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ï¼›    - æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆå­¦ä¹ èƒŒæ™¯ NeRFï¼Œç„¶åå­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetã€‚    - æå‡ºäº†ä¸€ç§æ–°çš„ Mixnetï¼Œå¯ä»¥æœ‰æ•ˆåœ°èåˆèƒŒæ™¯ NeRF å’Œæ®‹å·® NeRF çš„è¾“å‡ºã€‚</p><pre><code>æ€§èƒ½ï¼š- åœ¨åˆæˆæ•°æ®ä¸Šï¼ŒResidual-NeRF åœ¨ RMSE ä¸Šæ¯”åŸºçº¿ä½ 46.1%ï¼Œåœ¨ MAE ä¸Šä½ 29.5%ã€‚- åœ¨çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒä¸­ï¼ŒResidual-NeRF äº§ç”Ÿäº†æ›´ç¨³å¥çš„æ·±åº¦å›¾ï¼Œå™ªç‚¹æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚å·¥ä½œé‡ï¼š- è®­ç»ƒ Residual-NeRF éœ€è¦ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œè¿™æ¯”åŸºçº¿æ–¹æ³•æ›´å¤æ‚ã€‚- Residual-NeRF éœ€è¦é¢å¤–çš„å†…å­˜æ¥å­˜å‚¨èƒŒæ™¯ NeRF å’Œæ®‹å·® NeRF çš„æƒé‡ã€‚</code></pre><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b7d7618a421bd8b0947856c3ea91116f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1b6857403dd3f1eeafdb70f45e5b92e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4d0e4ee50f9ead394b9fcd552ae92106.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b39c7bfcb8005d9c40b2eac38f3ed56.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0aaa94e0b48b25617be15c8888555cae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7c646c3420664f87093b1eb3a62bfba.jpg" align="middle"></details>## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior**Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh**Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. [PDF](http://arxiv.org/abs/2405.05749v2) 11 pages, 5 figures**Summary**é€šè¿‡è§£å†³å•å¼ å›¾åƒéŸ³é¢‘é©±åŠ¨3D Talking Headç”Ÿæˆä¸­çš„3Dä¸€è‡´æ€§é—®é¢˜ï¼ŒNeRFFaceSpeech æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ 3Dæ„ŸçŸ¥çš„ Talking Headã€‚**Key Takeaways**- è§£å†³å•å¼ å›¾åƒéŸ³é¢‘é©±åŠ¨ 3D Talking Head ç”Ÿæˆçš„ 3D ä¸€è‡´æ€§é—®é¢˜ã€‚- ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRF ç›¸ç»“åˆï¼Œæ„å»ºå¯¹åº”äºå•å¼ å›¾åƒçš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚- å¼•å…¥ç©ºé—´åŒæ­¥æ–¹æ³•ï¼Œåˆ©ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å°„çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ã€‚- å¼•å…¥ LipaintNet è¡¥å……å†…å˜´åŒºåŸŸä¸­ç¼ºå¤±çš„ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯æ— æ³•ä»ç»™å®šçš„å•å¼ å›¾åƒä¸­è·å¾—ã€‚- ä»¥è‡ªç›‘ç£çš„æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚- æå‡ºä¸€ç§å®šé‡æ–¹æ³•æ¥è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ï¼Œè¿™åœ¨ä»¥å‰åªèƒ½é€šè¿‡å®šæ€§æ–¹å¼è¿›è¡Œã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: NeRFFaceSpeech: ä¸€æ¬¡æ€§éŸ³é¢‘é©±åŠ¨ 3D è¯´è¯äººå¤´éƒ¨åˆæˆï¼Œé€šè¿‡ç”Ÿæˆå…ˆéªŒ</p></li><li><p>Authors: Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</p></li><li><p>Affiliation: é¦–å°”å›½ç«‹å¤§å­¦</p></li><li><p>Keywords: éŸ³é¢‘é©±åŠ¨, 3D è¯´è¯äººå¤´éƒ¨, ç¥ç»è¾å°„åœº, ç”Ÿæˆå…ˆéªŒ, ä¸€æ¬¡æ€§å­¦ä¹ </p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05749, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ­£ä» 2D å†…å®¹è½¬å‘ 3D å†…å®¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåˆæˆé«˜è´¨é‡ 3D è¯´è¯äººå¤´éƒ¨è¾“å‡ºçš„ä¸€ç§æ‰‹æ®µè€Œå¤‡å—å…³æ³¨ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç§åŸºäº NeRF çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é…å¯¹çš„æ¯ä¸ªèº«ä»½çš„éŸ³é¢‘è§†è§‰æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚å°½ç®¡å·²ç»å°è¯•ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ 3D è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœé€šå¸¸ä¸ä»¤äººæ»¡æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨è§£å†³ä¸€æ¬¡æ€§éŸ³é¢‘é©±åŠ¨åŸŸä¸­è¢«å¿½è§†çš„ 3D ä¸€è‡´æ€§æ–¹é¢ï¼Œå…¶ä¸­é¢éƒ¨åŠ¨ç”»ä¸»è¦åœ¨æ­£é¢è§†è§’åˆæˆã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•ï¼š- åŸºäº NeRF çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é…å¯¹çš„æ¯ä¸ªèº«ä»½çš„éŸ³é¢‘è§†è§‰æ•°æ®ã€‚- ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ 3D è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–¹æ³•ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœé€šå¸¸ä¸ä»¤äººæ»¡æ„ã€‚</p><p>é—®é¢˜ï¼š- å¯æ‰©å±•æ€§å—é™ã€‚- 3D ä¸€è‡´æ€§ä¸è¶³ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š- æå‡ºäº†ä¸€ç§æ–°æ–¹æ³• NeRFFaceSpeechï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D æ„ŸçŸ¥è¯´è¯äººå¤´éƒ¨ã€‚- ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRF ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ„å»ºä¸€ä¸ªä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚- æˆ‘ä»¬çš„ç©ºé—´åŒæ­¥æ–¹æ³•é‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ã€‚- æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† LipaintNetï¼Œå®ƒå¯ä»¥è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…å£åŒºåŸŸä¸­ç¼ºå°‘çš„ä¿¡æ¯ã€‚è¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚</p><p>(4): åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å–å¾—äº†ä»¥ä¸‹æˆå°±ï¼š- å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº† 3D ä¸€è‡´æ€§ã€‚- æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè€Œä»¥å‰åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚</p><ol><li>Methods:</li></ol><p>(1): æå‡º NeRFFaceSpeech æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRFï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›</p><p>(2): é‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ï¼›</p><p>(3): å¼•å…¥ LipaintNetï¼Œè¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…å£åŒºåŸŸä¸­ç¼ºå°‘çš„ä¿¡æ¯ï¼Œè¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ï¼›</p><p>(4): è®¾è®¡å®šé‡æ–¹æ³•è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† NeRFFaceSpeechï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒæ„å»ºå’Œæ“çºµ 3D ç‰¹å¾ï¼Œä»å•å¹…å›¾åƒç”Ÿæˆ 3D æ„ŸçŸ¥éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–°æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†ç”Ÿæˆæ¨¡å‹å…ˆéªŒä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œæ„å»ºä¸å•å¹…å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›é€šè¿‡å…‰çº¿å˜å½¢ï¼Œé‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œå°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ï¼›å¼•å…¥äº† LipaintNetï¼Œä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼Œè¡¥å……å˜å½¢åœºä»¥äº§ç”Ÿå¯è¡Œç»“æœï¼›è®¾è®¡äº†å®šé‡æ–¹æ³•æ¥è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»å•å¹…å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº† 3D ä¸€è‡´æ€§ï¼›é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè€Œä»¥å‰åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details>## Tactile-Augmented Radiance Fields**Authors:Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens**We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF [PDF](http://arxiv.org/abs/2405.04534v1) CVPR 2024, Project page: https://dou-yiming.github.io/TaRF, Code:   https://github.com/Dou-Yiming/TaRF/**Summary**è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœºå°†è§†è§‰å’Œè§¦è§‰å¸¦å…¥å…±äº«çš„ 3D ç©ºé—´ï¼Œèƒ½å¤Ÿä¼°è®¡åœºæ™¯ä¸­ç»™å®š 3D ä½ç½®çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ã€‚**Key Takeaways**- è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœº (TaRF) ç»“åˆäº†è§†è§‰å’Œè§¦è§‰ä¿¡å·ï¼Œç”¨äºä¼°è®¡åœºæ™¯ä¸­ç»™å®š 3D ä½ç½®çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ã€‚- TaRF ç”±ç…§ç‰‡å’Œç¨€ç–é‡‡æ ·çš„è§¦è§‰æ¢é’ˆé‡‡é›†è€Œæˆã€‚- è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœºåˆ©ç”¨äº†è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨å¯ä¸å›¾åƒé…å‡†ä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼çš„åŒºåŸŸå…±äº«ç›¸åŒè§¦è§‰ç‰¹å¾çš„è§è§£ã€‚- è§¦è§‰ä¿¡å·é€šè¿‡é…å‡†å’Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸æ•è·çš„è§†è§‰åœºæ™¯ç›¸å…³è”ã€‚- TaRF æ•°æ®é›†åŒ…å«æ¯”ä»¥å¾€çœŸå®ä¸–ç•Œæ•°æ®é›†æ›´å¤šçš„è§¦è§‰æ ·æœ¬ï¼Œå¹¶ä¸ºæ¯ä¸ªæ•è·çš„è§¦è§‰ä¿¡å·æä¾›äº†ç©ºé—´å¯¹é½çš„è§†è§‰ä¿¡å·ã€‚- è·¨æ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®æ€§å·²å¾—åˆ°éªŒè¯ï¼Œä¸”å·²åœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­è¯æ˜äº†æ•è·çš„è§†è§‰è§¦è§‰æ•°æ®çš„å®ç”¨æ€§ã€‚- é¡¹ç›®ä¸»é¡µï¼šhttps://dou-yiming.github.io/TaRF**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: è§¦è§‰å¢å¼ºè¾å°„åœºï¼šä¸€ç§ç”¨äºè·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆçš„æ–°å‹åœºæ™¯è¡¨ç¤º</p></li><li><p>Authors: Douyi Ming, Srinath Sridhar, Jiajun Wu, Angjoo Kanazawa, Peter Anderson, Wojciech Matusik, Jonathan Ragan-Kelley</p></li><li><p>Affiliation: éº»çœç†å·¥å­¦é™¢</p></li><li><p>Keywords: Cross-modal perception, generative models, multi-view geometry, neural radiance fields, tactile sensing, vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2301.09422, Github: https://github.com/douyiming/TaRF</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥åœ¨å…±äº«çš„ 3D ç©ºé—´ä¸­çš„è¡¨ç¤ºé—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•é€šå¸¸æ˜¯å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·è§†ä¸ºç‹¬ç«‹çš„æ¨¡æ€ï¼Œè¿™é™åˆ¶äº†è·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆçš„ä»»åŠ¡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºå½¢å¼â€”â€”è§¦è§‰å¢å¼ºè¾å°„åœº (TaRF)ï¼Œå®ƒå°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­ã€‚TaRF çš„æ„å»ºåˆ©ç”¨äº†åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨ä¸å›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ã€‚</p><p>(4): åœ¨ TaRF æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨è·¨æ¨¡æ€ç”Ÿæˆã€è§¦è§‰ä¿¡å·ä¼°è®¡å’Œè§¦è§‰å¼•å¯¼çš„è§†è§‰æ¢ç´¢ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæ„å»ºè§†è§‰å’Œè§¦è§‰å¢å¼ºè¾å°„åœºï¼ˆTaRFï¼‰ï¼Œå°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡è§†è§‰-è§¦è§‰å¯¹åº”å…³ç³»å’Œåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œå»ºç«‹ TaRFï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨å’Œå›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä¼°è®¡ TaRF ä¸­çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¼°è®¡åœºæ™¯ä¸­å…¶ä»–ä½ç½®çš„è§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šä½¿ç”¨æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»æ¸²æŸ“çš„è§†è§‰ä¿¡å·ä¸­é¢„æµ‹è§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šæ”¶é›†åŒ…å« 19.3k ä¸ªå›¾åƒå¯¹çš„è§†è§‰-è§¦è§‰æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼° TaRFã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºå½¢å¼â€”â€”è§¦è§‰å¢å¼ºè¾å°„åœºï¼ˆTaRFï¼‰ï¼Œé¦–æ¬¡å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ªå…±äº«çš„ 3D ç©ºé—´ä¸­ï¼Œä¸ºè·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­çš„åœºæ™¯è¡¨ç¤ºå½¢å¼ TaRFï¼›æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰-è§¦è§‰å¯¹åº”å…³ç³»å’Œåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œå»ºç«‹ TaRF çš„æ–¹æ³•ï¼›æå‡ºäº†ä¸€ç§ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¼°è®¡åœºæ™¯ä¸­å…¶ä»–ä½ç½®çš„è§¦è§‰ä¿¡å·çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨è·¨æ¨¡æ€ç”Ÿæˆã€è§¦è§‰ä¿¡å·ä¼°è®¡å’Œè§¦è§‰å¼•å¯¼çš„è§†è§‰æ¢ç´¢ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼›æ”¶é›†äº†åŒ…å« 19.3k ä¸ªå›¾åƒå¯¹çš„è§†è§‰-è§¦è§‰æ•°æ®é›†ï¼Œä¸º TaRF çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†ä¸°å¯Œçš„ç´ æã€‚ workloadï¼šTaRF çš„æ„å»ºéœ€è¦åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨å’Œå›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å­˜åœ¨æŒ‘æˆ˜ï¼›TaRF çš„è®­ç»ƒéœ€è¦å¤§é‡çš„è§†è§‰-è§¦è§‰æ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å·¥ä½œé‡ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5322ab124785e1ed8207592748379b4a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a5022ff2c6b665ce2b96a8b7b9f166a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-929d0d52fcc6b94f08fe05a010b4ea04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-288d198e1ad4f685777631680ccf4209.jpg" align="middle"><img src="https://pica.zhimg.com/v2-59b2afe338ddf3888c68d0443ec0d04f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ec71c15419dcda442892e2bc1a105da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-432dac236eec2115922c4f0698f51eec.jpg" align="middle"></details><h2 id="DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><a href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid" class="headerlink" title="DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid"></a>DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid</h2><p><strong>Authors:Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</strong></p><p>Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively. </p><p><a href="http://arxiv.org/abs/2405.04416v2">PDF</a> Originally submitted to Siggraph Asia 2023</p><p><strong>Summary</strong><br>å¤§è§„æ¨¡åœºæ™¯åªéœ€ç”¨å•ä¸€NeRFï¼Œé€šè¿‡å¤šçº§Hashç½‘æ ¼ï¼Œè€Œæ— éœ€å•ç‹¬çš„èƒŒæ™¯NeRFï¼Œå³å¯å®ç°åœºæ™¯é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF åœ¨å¯¹è±¡å’Œå®¤å†…åœºæ™¯é‡å»ºä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é‡å»ºå¤§å‹åœºæ™¯æ—¶å­˜åœ¨é—®é¢˜ã€‚</li><li>åŸºäº MLP çš„ NeRF ç½‘ç»œå®¹é‡æœ‰é™ï¼Œè€ŒåŸºäºä½“ç§¯çš„ NeRF ä¼šéšç€åœºæ™¯åˆ†è¾¨ç‡çš„å¢åŠ å ç”¨å¤§é‡å†…å­˜ã€‚</li><li>æœ€è¿‘çš„æ–¹æ³•å°†åœºæ™¯åœ°ç†åˆ†åŒºï¼Œå¹¶ä½¿ç”¨å•ç‹¬çš„ NeRF å­¦ä¹ æ¯ä¸ªåˆ†åŒºã€‚</li><li>è¿™æœ‰åŠ©äºåŸºäºä½“ç§¯çš„ NeRF çªç ´å• GPU å†…å­˜é™åˆ¶ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤§çš„åœºæ™¯ã€‚</li><li>ä½†è¿™ç§æ–¹æ³•éœ€è¦å¤šä¸ªèƒŒæ™¯ NeRF æ¥å¤„ç†åˆ†åŒºå¤–çš„å…‰çº¿ï¼Œè¿™å¯¼è‡´å­¦ä¹ å†—ä½™ã€‚</li><li>æœ¬æ–‡æå‡ºäº† DistGridï¼ŒåŸºäºå¤šçº§æ•£åˆ—ç½‘æ ¼çš„åˆ†å¸ƒå¼åœºæ™¯é‡å»ºæ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†æ’åˆ—ä½†ä¸é‡å çš„è½´å¯¹é½åŒ…å›´ç›’ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†å‰²ä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§å‹åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DistGrid: åŸºäºåˆ†å¸ƒå¼å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¤§è§„æ¨¡åœºæ™¯é‡å»º</p></li><li><p>Authors: Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</p></li><li><p>Affiliation: å›½é˜²ç§‘æŠ€å¤§å­¦</p></li><li><p>Keywords: Neural Radiance Field, Distributed Algorithm, Large-scale Scene Reconstruction, Neural Rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.04416.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): ç¥ç»æ¸²æŸ“æŠ€æœ¯è‡ª NeRF æå‡ºä»¥æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼ŒNeRF æ—¨åœ¨è§£å†³æ–°è§†è§’åˆæˆä»»åŠ¡ã€‚å®ƒåŸºäºä½“ç§¯æ¸²æŸ“ï¼Œä½¿ç”¨ç§°ä¸ºç¥ç»åœºçš„å¤šåˆ†å±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰éšå¼è¡¨ç¤ºåœºæ™¯ã€‚è¯¥ç¥ç»åœºæ¥å— 3D åæ ‡å’Œè§‚å¯Ÿæ–¹å‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å…¶å¯¹åº”çš„å¯†åº¦å’Œé¢œè‰²ã€‚NeRF åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šäº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰è´¨é‡ã€‚å…¶å­¦ä¹ çš„ç¥ç»åœºä¹Ÿå¯ç”¨äºåœºæ™¯é‡å»ºã€‚</p><p>(2): NeRF çš„åç»­å·¥ä½œæ—¨åœ¨æé«˜æ•ˆç‡å’Œè´¨é‡ã€‚ä¸€ç§æ–¹æ³•æ˜¯å°†åœºæ™¯åœ°ç†åˆ†åŒºï¼Œå¹¶ä½¿ç”¨å•ç‹¬çš„ NeRF å­¦ä¹ æ¯ä¸ªå­åŒºåŸŸã€‚è¿™ç§åˆ†åŒºç­–ç•¥å¸®åŠ©åŸºäºä½“ç§¯çš„ NeRF è¶…è¿‡å•ä¸ª GPU çš„å†…å­˜é™åˆ¶ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤§çš„åœºæ™¯ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•éœ€è¦å¤šä¸ªèƒŒæ™¯ NeRF æ¥å¤„ç†åˆ†åŒºå¤–çš„å…‰çº¿ï¼Œè¿™å¯¼è‡´äº†å­¦ä¹ çš„å†—ä½™ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º DistGridã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œåœºæ™¯è¢«åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†ç›¸é‚»ä½†éé‡å çš„è½´å¯¹é½è¾¹ç•Œæ¡†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</p><p>(4): å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§è§„æ¨¡åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„åœºæ™¯é‡å»ºã€‚è¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šçš„å¯æ‰©å±•æ€§è¿˜é€šè¿‡å®šæ€§å’Œå®šé‡çš„æ–¹å¼è¿›è¡Œäº†è¿›ä¸€æ­¥è¯„ä¼°ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º DistGridã€‚</p><p>(2): DistGrid å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†ç›¸é‚»ä½†éé‡å çš„è½´å¯¹é½è¾¹ç•Œæ¡† (AABB)ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</p><p>(3): DistGrid é‡‡ç”¨ä¸¤çº§çº§è”ç»“æ„ï¼Œå…¶ä¸­ç»†ç²’åº¦ NeRF ä½¿ç”¨å†…å±‚è¾¹ç•Œæ¡†ä½œä¸ºå…¶è¾¹ç•Œæ¡†ï¼Œè€Œç²—ç²’åº¦ NeRF ä½¿ç”¨å¤–å±‚è¾¹ç•Œæ¡†ã€‚</p><p>(4): DistGrid ä½¿ç”¨åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨åŒºåŸŸå…‰çº¿ï¼Œè¯¥æ–¹æ³•å°†ä½“ç§¯æ¸²æŸ“ç§¯åˆ†åˆ†è§£ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨éƒ¨åˆ†é¢œè‰²å’Œéƒ¨åˆ†é€å°„ç‡æ¥è®¡ç®—æ¸²æŸ“é¢œè‰²å’Œæœ€ç»ˆé€å°„ç‡ã€‚</p></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº† DistGridï¼Œä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œæ— éœ€èƒŒæ™¯ NeRFï¼Œæé«˜äº†æ•ˆç‡ï¼›é‡‡ç”¨ä¸¤çº§çº§è”ç»“æ„ï¼Œç»†ç²’åº¦ NeRF å’Œç²—ç²’åº¦ NeRF ååŒå·¥ä½œï¼Œæé«˜äº†é‡å»ºè´¨é‡ã€‚            æ€§èƒ½ï¼šåœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§è§„æ¨¡åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„åœºæ™¯é‡å»ºã€‚            å·¥ä½œé‡ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDistGrid åœ¨é‡å»ºè´¨é‡ä¸Šçš„å¯æ‰©å±•æ€§å¾—åˆ°äº†å®šæ€§å’Œå®šé‡çš„æ–¹å¼çš„è¿›ä¸€æ­¥è¯„ä¼°ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c94130b609d19ed2e706304ecfbbdde4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c20db83d0a269f077a389b38e5b01349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e4dd6ce979c2f2e18008fc25e085162.jpg" align="middle"></details><h2 id="Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><a href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization" class="headerlink" title="Blending Distributed NeRFs with Tri-stage Robust Pose Optimization"></a>Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</h2><p><strong>Authors:Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at <a href="https://github.com/boilcy/Distributed-NeRF">https://github.com/boilcy/Distributed-NeRF</a>. </p><p><a href="http://arxiv.org/abs/2405.02880v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨ä¸‰é˜¶æ®µå§¿æ€ä¼˜åŒ–å¯¹åˆ†å¸ƒå¼NeRFè¿›è¡Œç²¾ç¡®å¯¹é½ï¼Œä»¥ç¼“è§£å»ºæ¨¡å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒæ—¶å‡ºç°çš„æ··å ä¼ªå½±å’Œå§¿æ€ç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ†å¸ƒå¼NeRFå»ºæ¨¡åŸå¸‚ç¯å¢ƒé¢ä¸´æ··å ä¼ªå½±å’Œå§¿æ€ç²¾åº¦é—®é¢˜ã€‚</li><li>é‡‡ç”¨åˆ†é˜¶æ®µå§¿æ€ä¼˜åŒ–è§£å†³é—®é¢˜ï¼ŒåŒ…æ‹¬Mip-NeRF 360æŸè°ƒæ•´ã€åå‘Mip-NeRF 360å’ŒFrame2Modelä¼˜åŒ–ã€‚</li><li>åˆ©ç”¨Model2Modelä¼˜åŒ–è¿›ä¸€æ­¥ç»†åŒ–ä¸åŒNeRFä¹‹é—´çš„è½¬æ¢ã€‚</li><li>ç²¾ç¡®çš„å§¿æ€ä¼˜åŒ–æœ‰æ•ˆæ¶ˆé™¤NeRFèåˆä¸­çš„é®æŒ¡ä¼ªå½±ã€‚</li><li>åœ¨çœŸå®å’Œæ¨¡æ‹Ÿåœºæ™¯ä¸­å±•ç¤ºå‡ºä¼˜è¶Šçš„NeRFèåˆæ€§èƒ½ã€‚</li><li>ä»£ç å’Œæ•°æ®å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>é¢˜ç›®ï¼šåŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–èåˆåˆ†å¸ƒå¼NeRFs</p></li><li><p>ä½œè€…ï¼šBaijun Yeâˆ—1,2, Caiyun Liuâˆ—1, Xiaoyu Ye1,2, Yuantao Chen1,3, Yuhai Wang4,Zike Yan1, Yongliang Shi1â€ , Hao Zhao1, Guyue Zhou1</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦äººå·¥æ™ºèƒ½äº§ä¸šç ”ç©¶é™¢ï¼ˆAIRï¼‰</p></li><li><p>å…³é”®è¯ï¼šåˆ†å¸ƒå¼NeRFã€ä½å§¿ä¼˜åŒ–ã€NeRFèåˆã€Mip-NeRF 360ã€iNeRF</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.02880Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡é¢†åŸŸï¼ŒNeRFå› å…¶èƒ½å¤Ÿåœ¨ä¿æŒç´§å‡‘æ¨¡å‹ç»“æ„çš„åŒæ—¶å®ç°é€¼çœŸçš„æ¸²æŸ“è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„åˆ†å¸ƒå¼NeRFé…å‡†æ–¹æ³•å­˜åœ¨æ··å ä¼ªå½±ï¼Œè¿™æºäºæ¸²æŸ“åˆ†è¾¨ç‡å·®å¼‚å’Œæ¬¡ä¼˜ä½å§¿ç²¾åº¦ã€‚è¿™äº›å› ç´ å…±åŒé™ä½äº†NeRFæ¡†æ¶å†…ä½å§¿ä¼°è®¡çš„ä¿çœŸåº¦ï¼Œå¯¼è‡´NeRFèåˆé˜¶æ®µå‡ºç°é®æŒ¡ä¼ªå½±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä»¥å¾€æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šæ‰¹å¤„ç†å­¦ä¹ å’Œå¢é‡å­¦ä¹ ã€‚æ‰¹å¤„ç†å­¦ä¹ éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè€Œå¢é‡å­¦ä¹ å®¹æ˜“å‡ºç°é—å¿˜é—®é¢˜ã€‚æ­¤å¤–ï¼Œå½“å‰ä½¿ç”¨æ˜¾å¼ç¼–ç æ–¹æ³•ï¼ˆå¦‚ç½‘æ ¼å’Œå…«å‰æ ‘ï¼‰è¿›è¡Œå®æ—¶æ€§èƒ½çš„NeRFæ–¹æ³•ï¼Œé¢ä¸´ç€éšç€åœºæ™¯è§„æ¨¡çš„å¢åŠ ï¼Œç¼–ç ç»„ä»¶å‘ˆæŒ‡æ•°çº§æ‰©å±•çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å­˜å‚¨éœ€æ±‚å¤§å¹…å¢åŠ ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ†ç»‘è°ƒæ•´Mip-NeRF 360å¹¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼Œå®ç°äº†å›¾åƒçš„ç²¾ç¡®ä½å§¿ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå€Ÿé‰´LATITUDEï¼Œåˆ©ç”¨æˆªæ–­åŠ¨æ€ä½é€šæ»¤æ³¢ï¼ˆTDLFï¼‰çš„åŸç†å¯¹åå‘Mip-NeRF 360è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç§°ä¸ºiMNeRFã€‚æ­¤æ–¹æ³•ç±»ä¼¼äºæ¨¡ç³Šå›¾åƒä»¥ä½¿ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ é²æ£’ï¼Œä»è€Œå®ç°å¸§åˆ°æ¨¡å‹çš„ä½å§¿ä¼˜åŒ–ã€‚éšåï¼Œé‡‡ç”¨åè§†å›¾åŒºåŸŸæ£€ç´¢æ–¹æ³•æ¥æœç´¢ä¸åŒNeRFå®ä¾‹ä¸­æœ€ç›¸ä¼¼çš„å›¾åƒï¼Œè¿›è€Œç¡®å®šå…¶å…³è”çš„ä½å§¿ã€‚ç»™å®šå…³è”çš„ä½å§¿ï¼Œåˆ©ç”¨iMNeRFé€šè¿‡æ¸²æŸ“å›¾åƒå’Œè§‚å¯Ÿå›¾åƒä¹‹é—´çš„å…‰åº¦æŸå¤±æ¥ä¼˜åŒ–è¿™äº›ä½å§¿ï¼Œä»è€Œè·å¾—å¯é çš„å¸§åˆ°æ¨¡å‹è½¬æ¢ã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œé€šè¿‡ä¸åŒçš„å¸§åˆ°æ¨¡å‹è½¬æ¢è·å¾—äº†NeRFä¹‹é—´ç²—ç•¥çš„æ¨¡å‹åˆ°æ¨¡å‹è½¬æ¢ã€‚ç„¶åï¼Œå°†ä¸åŒçš„NeRFæ¨¡å‹æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åæ ‡ç³»ä¸­ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“å›¾åƒä½œä¸ºè§‚æµ‹å€¼è¿›ä¸€æ­¥ä¼˜åŒ–NeRFä¹‹é—´çš„ç›¸å¯¹è½¬æ¢ï¼Œå³é€šè¿‡æ¨¡å‹åˆ°æ¨¡å‹ä¼˜åŒ–æ¥è·å¾—NeRFä¹‹é—´çš„ç²¾ç¡®è½¬æ¢ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šåˆ©ç”¨ä¸‰é˜¶æ®µä½å§¿ä¼˜åŒ–ï¼Œå®ç°äº†NeRFèåˆå¹¶è·å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æœ¬æ–‡æ–¹æ³•ï¼ŒåŒæ—¶å‘å¸ƒäº†çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå±•ç¤ºäº†æœ¬æ–‡æ–¹æ³•åœ¨æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ†ç»‘è°ƒæ•´Mip-NeRF 360å¹¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼Œå®ç°äº†å›¾åƒçš„ç²¾ç¡®ä½å§¿ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šç¬¬äºŒé˜¶æ®µï¼Œå€Ÿé‰´LATITUDEï¼Œåˆ©ç”¨æˆªæ–­åŠ¨æ€ä½é€šæ»¤æ³¢ï¼ˆTDLFï¼‰çš„åŸç†å¯¹åå‘Mip-NeRF 360è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç§°ä¸ºiMNeRFï¼›</p><p>ï¼ˆ4ï¼‰ï¼šç»™å®šå…³è”çš„ä½å§¿ï¼Œåˆ©ç”¨iMNeRFé€šè¿‡æ¸²æŸ“å›¾åƒå’Œè§‚å¯Ÿå›¾åƒä¹‹é—´çš„å…‰åº¦æŸå¤±æ¥ä¼˜åŒ–è¿™äº›ä½å§¿ï¼Œä»è€Œè·å¾—å¯é çš„å¸§åˆ°æ¨¡å‹è½¬æ¢ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šç¬¬ä¸‰é˜¶æ®µï¼Œé€šè¿‡ä¸åŒçš„å¸§åˆ°æ¨¡å‹è½¬æ¢è·å¾—äº†NeRFä¹‹é—´ç²—ç•¥çš„æ¨¡å‹åˆ°æ¨¡å‹è½¬æ¢ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šå°†ä¸åŒçš„NeRFæ¨¡å‹æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åæ ‡ç³»ä¸­ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“å›¾åƒä½œä¸ºè§‚æµ‹å€¼è¿›ä¸€æ­¥ä¼˜åŒ–NeRFä¹‹é—´çš„ç›¸å¯¹è½¬æ¢ï¼Œå³é€šè¿‡æ¨¡å‹åˆ°æ¨¡å‹ä¼˜åŒ–æ¥è·å¾—NeRFä¹‹é—´çš„ç²¾ç¡®è½¬æ¢ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ï¼Œè§£å†³äº†å½“å‰åˆ†å¸ƒå¼NeRFé…å‡†æ–¹æ³•ä¸­å­˜åœ¨çš„æ··å ä¼ªå½±é—®é¢˜ï¼Œæé«˜äº†NeRFèåˆçš„ä¿çœŸåº¦ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒç²¾ç¡®ä½å§¿ä¼°è®¡ã€å¸§åˆ°æ¨¡å‹ä½å§¿ä¼˜åŒ–å’Œæ¨¡å‹åˆ°æ¨¡å‹ä½å§¿ä¼˜åŒ–ï¼›æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸ŠéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºè¿›è¡Œä½å§¿ä¼˜åŒ–ï¼Œä½†å¯ä»¥å®ç°æ›´å¥½çš„NeRFèåˆæ•ˆæœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-19421ede11ee24694424a6e2329cbd82.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c3320829396c5cee81241227bf678e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f9d4086bd525c9621ef04e939f0ee92.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cce8d4e7f7b76ee03713ad33dc0da96e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f2f0b44124a450d85749c232dce8a310.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b718f4974ac6aac1d1a62a7beb2d681.jpg" align="middle"><img src="https://picx.zhimg.com/v2-767f74cc31852d4c6f28717ac5e03f47.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5929c8babaf3c740cf4c2d6f2886bf8e.jpg" align="middle"></details>## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for   Dynamic UAV-based Scenes**Authors:Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon**In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms. [PDF](http://arxiv.org/abs/2405.02762v1) 8 pages, submitted to IROS2024**Summary**æ— äººæœºå®æ—¶æ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼Œè¯¥æ–‡å°†é™æ€ç‰¹å¾ä¸åŠ¨æ€ç‰¹å¾ç›¸ç»“åˆï¼Œæé«˜äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„åŸŸé€‚åº”æ€§ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§åˆ†å±‚ç‰¹å¾å‘é‡çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ‰©å±•ç‰ˆæœ¬ï¼Œç”¨äºæ•è·åŠ¨æ€åœºæ™¯ä¸­çš„æ¦‚å¿µä¿¡æ¯ã€‚- æ‰©å±•çš„NeRFæ¨¡å‹é€šè¿‡å›¾åƒè§£ç å™¨å°†è¾“å‡ºç‰¹å¾å›¾è½¬æ¢ä¸ºRGBå›¾åƒã€‚- è¯¥æ¨¡å‹åŒæ—¶åˆ©ç”¨é™æ€å’ŒåŠ¨æ€å¯¹è±¡çš„ä¿¡æ¯ï¼Œä»è€Œæ•è·é«˜ç©ºè§†é¢‘ä¸­çš„æ˜¾è‘—åœºæ™¯å±æ€§ã€‚- å°†é™æ€å’ŒåŠ¨æ€ç‰¹å¾ç›¸ç»“åˆï¼Œæé«˜äº†æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„åŸŸé€‚åº”æ€§ã€‚- åœ¨Okutama Actionå’ŒUG2ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹çš„æ€§èƒ½ã€‚- ä¸æœ€å…ˆè¿›çš„æ— äººæœºæ„ŸçŸ¥ç®—æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚- è¯¥æ¨¡å‹å¯ä»¥åº”ç”¨äºæ— äººæœºå®æ—¶æ„ŸçŸ¥ä»»åŠ¡ï¼Œä¾‹å¦‚åŠ¨ä½œè¯†åˆ«å’Œå§¿æ€ä¼°è®¡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TK-Planesï¼šå…·æœ‰é«˜ç»´ç‰¹å¾å‘é‡çš„åˆ†å±‚ K-Planes</p></li><li><p>Authors: Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</p></li><li><p>Affiliation: ç¾å›½é™†å†›ç ”ç©¶å®éªŒå®¤</p></li><li><p>Keywords: Neural Radiance Fields, Synthetic Data, UAV Perception, Dynamic Scenes, Feature Vectors</p></li><li><p>Urls: https://arxiv.org/abs/2405.02762, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯åˆæˆæ•°æ®åœ¨æ— äººæœºæ„ŸçŸ¥ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€åœºæ™¯çš„è¯†åˆ«ï¼Œå¦‚å§¿åŠ¿æˆ–åŠ¨ä½œè¯†åˆ«ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäº K-Planes ç¥ç»è¾å°„åœºï¼Œä½†å­˜åœ¨é—®é¢˜ï¼šåŠ¨æ€å¯¹è±¡å»ºæ¨¡å›°éš¾ã€é™æ€å’ŒåŠ¨æ€å…ƒç´ åˆ†ç¦»å›°éš¾ã€åŠ¨æ€å¯¹è±¡ç¨€ç–ã€å§¿æ€å¤šæ ·æ€§å—é™ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ TK-Planesï¼Œä¸€ç§åˆ†å±‚ K-Planes ç®—æ³•ï¼Œè¾“å‡ºå’Œæ“ä½œç‰¹å¾å‘é‡è€Œä¸æ˜¯ RGB åƒç´ å€¼ã€‚è¿™äº›ç‰¹å¾å‘é‡å¯ä»¥å­˜å‚¨åœºæ™¯ä¸­ç‰¹å®šå¯¹è±¡æˆ–ä½ç½®çš„æ¦‚å¿µä¿¡æ¯ï¼Œå¹¶ä¸ºå¤šä¸ªç›¸åº”çš„ç›¸æœºå…‰çº¿è¾“å‡ºæ—¶å½¢æˆç‰¹å¾å›¾ï¼Œç„¶åè§£ç ä¸ºæœ€ç»ˆå›¾åƒã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨ Okutama Action å’Œ UG2 ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰ç®—æ³•ç›¸æ¯”ï¼ŒåŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„æ•´ä½“è¯†åˆ«å‡†ç¡®æ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨ NeRFï¼ˆç¥ç»è¾å°„åœºï¼‰åœ¨ç‰¹å¾ç©ºé—´ä¸­ç”Ÿæˆæ–°è§†è§’ï¼Œä»¥æ›´å¥½åœ°æ•è·åœºæ™¯ä¸­çš„åŠ¨æ€å¯¹è±¡ï¼Œå¦‚äººç‰©ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨åŸºäºç½‘æ ¼çš„ NeRFï¼Œç½‘æ ¼ç±»ä¼¼äº K-Planesï¼Œä½†å­˜å‚¨çš„ç‰¹å¾å‘é‡ä¸ç›´æ¥ç¼–ç  RGB å€¼ï¼Œè€Œæ˜¯ç¼–ç åœºæ™¯ä¸­çš„æ›´é«˜å±‚æ¬¡æ¦‚å¿µä¿¡æ¯ï¼Œå¦‚åœ°é¢ã€æ ‘æœ¨å’Œäººç‰©ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨åˆ†å±‚ç½‘æ ¼åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œï¼Œå°†åœºæ™¯åˆ†è§£ä¸ºé™æ€å’ŒåŠ¨æ€ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å›¾åƒè§£ç å™¨å°†ç‰¹å¾å›¾è§£ç ä¸ºæœ€ç»ˆå›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œç»“æœè¡¨æ˜åŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):æœ¬æ–‡æå‡ºçš„åˆ†å±‚ K-Planesï¼ˆTK-Planesï¼‰ç®—æ³•ï¼Œé€šè¿‡åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œç‰¹å¾å‘é‡ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åˆæˆæ•°æ®åœ¨æ— äººæœºæ„ŸçŸ¥ä¸­çš„åŠ¨æ€åœºæ™¯è¯†åˆ«é—®é¢˜ã€‚            (2):åˆ›æ–°ç‚¹ï¼šTK-Planes ç®—æ³•å°†åœºæ™¯åˆ†è§£ä¸ºé™æ€å’ŒåŠ¨æ€ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åˆ†å±‚ç½‘æ ¼åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œï¼Œä»è€Œæ›´å¥½åœ°æ•è·åŠ¨æ€å¯¹è±¡ï¼›æ€§èƒ½ï¼šåœ¨ Okutama Action å’Œ UG2 ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„è¯†åˆ«å‡†ç¡®æ€§ï¼›å·¥ä½œé‡ï¼šTK-Planes ç®—æ³•çš„å®ç°å’Œåœ¨æ— äººæœºæ•°æ®é›†ä¸Šçš„è¯„ä¼°éœ€è¦ä¸€å®šçš„æŠ€æœ¯æŠ•å…¥å’Œè®¡ç®—èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-49ebb6a345fe5bed0d70468dcdf8fd84.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-63139de16b603f02b54ce2804a9bad9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af95031ec70f26ba5024a7788a09ddff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c99084999876941f9618dfb5d99f367b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63f6561485ccf42232344f25cf43bc8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a606ade4cb1fcbf9e4da91086ed92ad1.jpg" align="middle"></details><h2 id="ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><a href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty" class="headerlink" title="ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty"></a>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty</h2><p><strong>Authors:Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</strong></p><p>Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance. </p><p><a href="http://arxiv.org/abs/2405.02568v1">PDF</a> </p><p><strong>Summary</strong><br>ä¸»åŠ¨ç¥ç»é‡å»ºåŒæ—¶è€ƒè™‘å›¾åƒæ¸²æŸ“å’Œå‡ ä½•ä¸ç¡®å®šæ€§ï¼Œä»¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è®­ç»ƒè§†å›¾æ¥æé«˜ 3D åœºæ™¯é‡å»ºæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¸»åŠ¨å­¦ä¹ åœ¨ 3D åœºæ™¯é‡å»ºä¸­è‡³å…³é‡è¦ã€‚</li><li>NeRF å˜ä½“ä½¿ç”¨å›¾åƒæ¸²æŸ“æˆ–å‡ ä½•ä¸ç¡®å®šæ€§æé«˜äº†ä¸»åŠ¨ 3D é‡å»ºçš„æ€§èƒ½ã€‚</li><li>ActiveNeuS åŒæ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“å’Œå‡ ä½•ä¸ç¡®å®šæ€§æ¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†å›¾ã€‚</li><li>ActiveNeuS ç§¯ç´¯å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶é¿å…ä¼°è®¡å¯†åº¦å¼•å…¥çš„åå·®ã€‚</li><li>ActiveNeuS è®¡ç®—ç¥ç»éšå¼è¡¨é¢ä¸ç¡®å®šæ€§ï¼Œæä¾›é¢œè‰²ä¸ç¡®å®šæ€§å’Œè¡¨é¢ä¿¡æ¯ã€‚</li><li>ActiveNeuS ä½¿ç”¨è¡¨é¢ä¿¡æ¯å’Œç½‘æ ¼æœ‰æ•ˆå¤„ç†åå·®ï¼Œä»è€Œå¿«é€Ÿé€‰æ‹©å¤šæ ·åŒ–çš„è§†ç‚¹ã€‚</li><li>ActiveNeuS åœ¨ Blender å’Œ DTU æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä»¥å¾€çš„å·¥ä½œï¼Œè¡¨æ˜ ActiveNeuS é€‰æ‹©çš„è§†å›¾æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šActiveNeuSï¼šåŸºäºç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§çš„ä¸»åŠ¨ä¸‰ç»´é‡å»º</li><p></p><p></p><li>ä½œè€…ï¼šHyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</li><p></p><p></p><li>å•ä½ï¼šé¦–å°”å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šä¸»åŠ¨å­¦ä¹ ã€ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€æ›²é¢ç½‘æ ¼</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2405.02568.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´åœºæ™¯é‡å»ºä¸­çš„ä¸»åŠ¨å­¦ä¹ å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œå› ä¸ºé€‰æ‹©æœ‰ä¿¡æ¯çš„è®­ç»ƒè§†å›¾å¯¹äºé‡å»ºè‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å˜ä½“åœ¨ä½¿ç”¨å›¾åƒæ¸²æŸ“æˆ–å‡ ä½•ä¸ç¡®å®šæ€§è¿›è¡Œä¸»åŠ¨ä¸‰ç»´é‡å»ºæ–¹é¢è¡¨ç°å‡ºæ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œåœ¨é€‰æ‹©ä¿¡æ¯è§†å›¾æ—¶åŒæ—¶è€ƒè™‘ä¸¤ç§ä¸ç¡®å®šæ€§ä»ç„¶æœªè¢«æ¢ç´¢ï¼Œè€Œåˆ©ç”¨ä¸åŒç±»å‹çš„ä¸ç¡®å®šæ€§å¯ä»¥å‡å°‘åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µç”±äºè¾“å…¥ç¨€ç–è€Œäº§ç”Ÿçš„åå·®ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„NeRFä¸»åŠ¨å­¦ä¹ æ–¹æ³•é€šå¸¸ä¼°è®¡å…¶è¾“å‡ºä¸­çš„ä¸ç¡®å®šæ€§ï¼šä¸‰ç»´ç‚¹çš„å¯†åº¦å’Œé¢œè‰²ã€‚Martinç­‰äººå’ŒPanç­‰äººä¼°è®¡äº†é¢œè‰²é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œæ–¹æ³•æ˜¯å°†é¢œè‰²å»ºæ¨¡ä¸ºé«˜æ–¯æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå®¹æ˜“å—åˆ°å¯†åº¦ä¼°è®¡åå·®çš„å½±å“ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯è§†å›¾é€‰æ‹©ä¸ä½³ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ActiveNeuSï¼Œå®ƒåœ¨è¯„ä¼°å€™é€‰è§†å›¾æ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§å’Œç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€‚ActiveNeuSæä¾›äº†ä¸€ç§ç§¯ç´¯å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†ä¼°è®¡å¯†åº¦å¯èƒ½å¼•å…¥çš„åå·®ã€‚ActiveNeuSè®¡ç®—ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ï¼Œæä¾›é¢œè‰²ä¸ç¡®å®šæ€§å’Œæ›²é¢ä¿¡æ¯ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ›²é¢ä¿¡æ¯å’Œç½‘æ ¼æœ‰æ•ˆåœ°å¤„ç†åå·®ï¼Œä»è€Œèƒ½å¤Ÿå¿«é€Ÿé€‰æ‹©ä¸åŒçš„è§†ç‚¹ã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æµè¡Œçš„æ•°æ®é›†Blenderå’ŒDTUä¸Šï¼ŒActiveNeuSçš„æ€§èƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œè¡¨æ˜ActiveNeuSé€‰æ‹©çš„è§†å›¾æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†ä½œè€…çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‰æ‹©ä¿¡æ¯è§†å›¾ä»¥æé«˜ä¸‰ç»´é‡å»ºçš„æ€§èƒ½ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): ActiveNeuS æå‡ºäº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»“åˆäº†å‡ ä½•é‡å»ºå’Œå›¾åƒæ¸²æŸ“çš„è§†è§’ã€‚            (2): ActiveNeuS ä¼°è®¡é¢œè‰²é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œä»¥è·å–æœ‰å…³å›¾åƒæ¸²æŸ“è´¨é‡çš„ä¿¡æ¯ã€‚            (3): é‡‡é›†å‡½æ•°é›†æˆäº†ä¼°è®¡çš„ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ä¸¢å¤±å‡ ä½•å±æ€§ã€‚            (4): é¦–å…ˆï¼Œåœ¨ç¬¬ 4.1 èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„é‡‡é›†å‡½æ•°ï¼Œå¹¶è§£é‡Šäº†åœ¨ç§¯åˆ†è¿‡ç¨‹ä¸­å¦‚ä½•è€ƒè™‘æ›²é¢ã€‚            (5): åœ¨ç¬¬ 4.2 èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº† ActiveNeuS ä¸­ä¼°è®¡çš„ç¥ç»éšå¼è¡¨é¢ä¸ç¡®å®šæ€§ï¼Œå¹¶æè¿°äº†å¦‚ä½•åœ¨é‡‡é›†å‡½æ•°ä¸­åˆ©ç”¨ä¸ç¡®å®šæ€§ã€‚            (6): ç„¶åï¼Œä¸ºäº†è¿›è¡Œé«˜æ•ˆä¸”ç¨³å¥çš„è®¡ç®—ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­˜å‚¨æ›²é¢ä¿¡æ¯çš„æ›²é¢ç½‘æ ¼å’Œé€‰æ‹©å¤šä¸ªæ¬¡ä¼˜è§†å›¾ (NBV) çš„ç­–ç•¥ï¼ˆç¬¬ 4.3 èŠ‚ï¼‰ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¿¡æ¯è§†å›¾é€‰æ‹©æ–¹æ³• ActiveNeuSï¼Œè¯¥æ–¹æ³•åŒæ—¶è€ƒè™‘äº†å‡ ä½•é‡å»ºå’Œå›¾åƒæ¸²æŸ“çš„ä¿çœŸåº¦ã€‚ActiveNeuS å¼•å…¥äº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°åˆ©ç”¨ä¸ç¡®å®šæ€§ç½‘æ ¼æœ‰æ•ˆä¸”ç¨³å¥åœ°åˆ©ç”¨ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€‚é‡‡é›†å‡½æ•°é€šè¿‡ä½¿ç”¨æ›²é¢ç½‘æ ¼å¹¶æ ¹æ®æ›²é¢çš„å­˜åœ¨åº”ç”¨ä¸åŒçš„ç§¯åˆ†ç­–ç•¥æ¥è®¡ç®—ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§çš„ç§¯åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºäº† ActiveNeuS çš„ä¸‹ä¸€ä¸ªæœ€ä½³è§†å›¾é€‰æ‹©ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ”¹è¿›äº†ç½‘æ ¼é‡å»ºå’Œå›¾åƒæ¸²æŸ“è´¨é‡ã€‚å¯¹äºæœªæ¥çš„å·¥ä½œï¼Œæˆ‘ä»¬å»ºè®®ç ”ç©¶ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥è¿æ¥ä¸åŒç½‘ç»œçš„ä¸ç¡®å®šæ€§ä»¥è¿›è¡Œä¿¡æ¯è§†å›¾é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå°† ActiveNeuS åº”ç”¨äºæœºå™¨äººä¸»åŠ¨ 3D é‡å»ºä¸­ä¹Ÿå¾ˆæœ‰è¶£ï¼Œå…¶ä¸­æœºå™¨äººæ‰‹è‡‚ç§»åŠ¨å¹¶æ”¶é›†æ•°æ®ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§å’Œç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ä¿¡æ¯è§†å›¾çš„é€‰æ‹©ã€‚æ€§èƒ½ï¼šåœ¨ Blender å’Œ DTU ç­‰æµè¡Œæ•°æ®é›†ä¸Šï¼ŒActiveNeuS çš„æ€§èƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œè¡¨æ˜ ActiveNeuS é€‰æ‹©çš„è§†å›¾æ˜¾ç€æé«˜äº†æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šActiveNeuS çš„è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒé«˜ï¼Œå› ä¸ºå®ƒéœ€è¦ä¼°è®¡ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§å¹¶ä½¿ç”¨æ›²é¢ç½‘æ ¼è¿›è¡Œç§¯åˆ†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0eb6f5097fe2312cfce57f04da637606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-824537082b5290b565ea1f0da3946351.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7939474a53153965a29e673124ea4a9e.jpg" align="middle"></details><h2 id="Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><a href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids" class="headerlink" title="Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids"></a>Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids</h2><p><strong>Authors:Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</strong></p><p>Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times. </p><p><a href="http://arxiv.org/abs/2405.02386v1">PDF</a> SIGGRAPH 2024, Project page: <a href="https://junchenliu77.github.io/Rip-NeRF">https://junchenliu77.github.io/Rip-NeRF</a>   , Code: <a href="https://github.com/JunchenLiu77/Rip-NeRF">https://github.com/JunchenLiu77/Rip-NeRF</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡æåæ ‡æŠ•å½±å°†ä¸‰ç»´å„å‘å¼‚æ€§åŒºåŸŸå°„å½±åˆ°å¹³é¢ï¼Œå†åˆ©ç”¨Ripmapç¼–ç å¯¹å„å¹³é¢è¿›è¡Œç¼–ç ï¼Œè¿›è€Œè§£å†³NeRFæŠ—é”¯é½¿æ¸²æŸ“ä¸­çš„æ··å å’Œæ¨¡ç³Šé—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æåæ ‡æŠ•å½±å°†ä¸‰ç»´å„å‘å¼‚æ€§åŒºåŸŸå°„å½±åˆ°å¹³é¢ï¼Œä¾¿äºç‰¹å¾åŒ–ã€‚</li><li>Ripmapç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„æ»¤æ³¢å¯å­¦ä¹ ç‰¹å¾ç½‘æ ¼ï¼Œå¯¹å°„å½±å„å‘å¼‚æ€§åŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚</li><li>æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†å’Œå®æ™¯æ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ¸²æŸ“è´¨é‡ã€‚</li><li>æ–¹æ³•åœ¨é‡å¤ç»“æ„å’Œçº¹ç†çš„ç²¾ç»†ç»†èŠ‚ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li><li>æ–¹æ³•è®­ç»ƒæ—¶é—´ç›¸å¯¹è¾ƒçŸ­ã€‚</li><li>è¯¥æ–¹æ³•ä¾èµ–äºå¯å­¦ä¹ ç‰¹å¾ç½‘æ ¼ã€‚</li><li>è¯¥æ–¹æ³•ç›®å‰ä»…é€‚ç”¨äºé™æ€åœºæ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p><strong>æ ‡é¢˜ï¼š</strong>Rip-NeRFï¼šåŸºäºRipmapç¼–ç çš„Platonicå®ä½“çš„åèµ°æ ·è¾å°„åœº</p></li><li><p><strong>ä½œè€…ï¼š</strong>Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</p></li><li><p><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</p></li><li><p><strong>å…³é”®è¯ï¼š</strong>ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ã€åèµ°æ ·ã€Ripmapç¼–ç ã€Platonicå®ä½“</p></li><li><p><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/pdf/2405.02386.pdfï¼Œ<strong>Githubé“¾æ¥ï¼š</strong>None</p></li><li><p><strong>æ‘˜è¦ï¼š</strong></p><p>(1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>å°½ç®¡ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶æ¸²æŸ“ç»“æœä»å¯èƒ½å­˜åœ¨èµ°æ ·å’Œæ¨¡ç³Šä¼ªå½±ï¼Œå› ä¸ºæœ‰æ•ˆä¸”é«˜æ•ˆåœ°è¡¨å¾é”¥å½¢æŠ•å°„è¿‡ç¨‹äº§ç”Ÿçš„å„å‘å¼‚æ€§åŒºåŸŸä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚</p><p>(2) <strong>è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š</strong>ç°æœ‰æ–¹æ³•è¦ä¹ˆæ— æ³•ç²¾ç¡®åœ°è¡¨å¾å„å‘å¼‚æ€§åŒºåŸŸï¼Œè¦ä¹ˆæ•ˆç‡ä½ä¸‹ã€‚</p><p>(3) <strong>æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºRipmapç¼–ç çš„Platonicå®ä½“è¡¨ç¤ºï¼Œç”¨äºç²¾ç¡®é«˜æ•ˆåœ°è¡¨å¾3Då„å‘å¼‚æ€§åŒºåŸŸï¼Œä»è€Œå®ç°é«˜ä¿çœŸåèµ°æ ·æ¸²æŸ“ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šPlatonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ã€‚Platonicå®ä½“æŠ•å½±å°†3Dç©ºé—´åˆ†è§£åˆ°ç‰¹å®šPlatonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œä½¿å¾—å„å‘å¼‚æ€§çš„3DåŒºåŸŸå¯ä»¥æŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚åŒæ—¶ï¼ŒPlatonicå®ä½“çš„æ¯ä¸ªé¢éƒ½ç”±Ripmapç¼–ç ç¼–ç ï¼Œè¯¥ç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œä»¥å®ç°å¯¹æŠ•å½±å„å‘å¼‚æ€§åŒºåŸŸçš„ç²¾ç¡®å’Œé«˜æ•ˆè¡¨å¾ã€‚</p><p>(4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong>åœ¨å¤šå°ºåº¦Blenderæ•°æ®é›†å’Œæ–°æ•è·çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šåŸºäºRipmapç¼–ç çš„Platonicå®ä½“æŠ•å½±ï¼Œå°†3Dç©ºé—´åˆ†è§£åˆ°Platonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œå°†å„å‘å¼‚æ€§çš„3DåŒºåŸŸæŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚</p><p>ï¼ˆ2ï¼‰ï¼šPlatonicå®ä½“çš„æ¯ä¸ªé¢ç”±Ripmapç¼–ç ç¼–ç ï¼Œè¯¥ç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œä»¥å®ç°å¯¹æŠ•å½±å„å‘å¼‚æ€§åŒºåŸŸçš„ç²¾ç¡®å’Œé«˜æ•ˆè¡¨å¾ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨æ··åˆè¡¨ç¤ºï¼ŒåŒ…æ‹¬æ˜¾å¼å’Œéšå¼è¡¨ç¤ºï¼Œæ—¢èƒ½ä¿è¯æ•ˆç‡ï¼Œåˆèƒ½ä¿è¯çµæ´»æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨å¤šé‡‡æ ·å’Œé¢ç§¯é‡‡æ ·å¯¹åœ†é”¥æˆªä½“è¿›è¡Œç‰¹å¾åŒ–ï¼Œå…¶ä¸­é¢ç§¯é‡‡æ ·é‡‡ç”¨å„å‘å¼‚æ€§3Dé«˜æ–¯å‡½æ•°å¯¹åœ†é”¥æˆªä½“è¿›è¡Œè¡¨å¾ï¼Œå†åˆ©ç”¨æå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç è¿›è¡Œç‰¹å¾åŒ–ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåˆ©ç”¨MLPä¼°è®¡åœ†é”¥æˆªä½“çš„é¢œè‰²å’Œå¯†åº¦ï¼Œå¹¶é€šè¿‡ä½“ç§¯æ¸²æŸ“æ¸²æŸ“åƒç´ é¢œè‰²ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨å…‰åº¦æŸå¤±å‡½æ•°ï¼Œå¯¹æ¸²æŸ“å›¾åƒå’Œè§‚æµ‹å›¾åƒè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</p><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºRipmapç¼–ç çš„Platonicå®ä½“è¡¨ç¤ºï¼Œç”¨äºç¥ç»è¾å°„åœºï¼Œç§°ä¸ºRip-NeRFã€‚Rip-NeRFå¯ä»¥æ¸²æŸ“é«˜ä¿çœŸæŠ—é”¯é½¿å›¾åƒï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡ï¼Œè¿™å¾—ç›Šäºæå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ã€‚Platonicå®ä½“æŠ•å½±å°†3Dç©ºé—´åˆ†è§£åˆ°ç‰¹å®šPlatonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œä½¿å¾—å„å‘å¼‚æ€§çš„3DåŒºåŸŸå¯ä»¥æŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚Ripmapç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œèƒ½å¤Ÿå¯¹æŠ•å½±çš„å„å‘å¼‚æ€§åŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚è¿™ä¸¤ä¸ªç»„ä»¶ååŒå·¥ä½œï¼Œå¯¹å„å‘å¼‚æ€§çš„3DåŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚å®ƒåœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•æ‰ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„å’Œçº¹ç†çš„ç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™éªŒè¯äº†æ‰€æå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç çš„æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºRipmapç¼–ç çš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ï¼Œç”¨äºå¯¹å„å‘å¼‚æ€§çš„3DåŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚</p><p>æ€§èƒ½ï¼šåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p><p>å·¥ä½œé‡ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå·¥ä½œé‡ç•¥å¤§ï¼Œå› ä¸ºéœ€è¦å¯¹Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç è¿›è¡Œé¢„å¤„ç†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d766632fb875e5fe770b7fee4ed1ae6b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43f9b3bed63eef23fb98c456f7077574.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b2442c515601a7e17fca7b5a8e2166b.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/</id>
    <published>2024-05-13T08:18:16.000Z</published>
    <updated>2024-05-13T08:18:16.334Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>ä¸€ä¸ªå›¾åƒåˆ°å¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ˜¯å›¾åƒåˆ° 3D è¡¨ç¤ºæˆ–å›¾åƒ 3D é‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°é¢–æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨é«˜æ–¯æº…å°„æ³•ï¼Œå¯å®ç°éšå¼ 3D é‡å»ºï¼Œå¹¶ä¼˜äºåŸå§‹ç¥ç»è¾å°„åœºã€‚</li><li>å€ŸåŠ©æŠ€æœ¯å’ŒåŸç†çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å°è¯•ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ç„¶è€Œï¼Œä½¿ç”¨å¸¸è§„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥è·å¾—ç²¾ç¡®çš„åŠ¨ä½œå’ŒåŠ¨ä½œæ§åˆ¶ï¼Œä¸”éš¾ä»¥ç”Ÿæˆå†…å®¹é•¿ä¸”è¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€‚</li><li>ç ”ç©¶è€…æå‡º OneTo3D æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„ 3D è§†é¢‘ã€‚</li><li>ç ”ç©¶è€…é‡‡ç”¨æ™®é€šåŸºæœ¬é«˜æ–¯æº…å°„æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹è§†é¢‘å†…å­˜å’Œè®¡ç®—æœºè¿ç®—èƒ½åŠ›è¦æ±‚è¾ƒä½ã€‚</li><li>ç ”ç©¶è€…é’ˆå¯¹å¯¹è±¡éª¨æ¶è®¾è®¡äº†è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ã€‚</li><li>ç»“åˆç ”ç©¶è€…æå‡ºçš„å¯é‡æ–°ç¼–è¾‘çš„åŠ¨ä½œå’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼Œåœ¨ 3D æ¨¡å‹ç²¾ç¡®åŠ¨ä½œå’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®è¾“å…¥æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äº SOTA é¡¹ç›®çš„æ€§èƒ½ã€‚</li><li>ç ”ç©¶è€…åˆ†æäº†è¯¦ç»†çš„å®ç°æ–¹æ³•å’Œç†è®ºåˆ†æï¼Œå¹¶å°†ç»™å‡ºç›¸å…³çš„æ¯”è¾ƒå’Œç»“è®ºã€‚</li><li>è¯¥é¡¹ç›®ä»£ç å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šOneTo3Dï¼šå•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘</p></li><li><p>ä½œè€…ï¼šJINWEI LIN</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè«çº³ä»€å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š3Dã€å•å¹…å›¾åƒã€å¯ç¼–è¾‘ã€åŠ¨æ€ã€ç”Ÿæˆã€è‡ªåŠ¨åŒ–ã€è§†é¢‘ã€è‡ªé€‚åº”ã€éª¨æ¶</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.06547ï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D è¡¨å¾æˆ– 3D é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸé•¿æœŸä»¥æ¥çš„æŒ‘æˆ˜ã€‚ç›®å‰å®ç° 3D é‡å»ºçš„æ–¹æ³•å¯åˆ†ä¸ºä¸¤ç±»ï¼šä¼ ç»Ÿæ˜¾å¼æ–¹æ³•å’Œæœºå™¨å­¦ä¹ éšå¼æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºä¸€ç§çªå‡ºçš„éšå¼æ–¹æ³•ï¼Œåœ¨æ¸²æŸ“å’Œè¡¨ç¤ºçœŸå®åœºæ™¯è§†å›¾æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p><p>(2)ï¼šè¿‡å»æ–¹æ³•ï¼šåŸºäº NeRFï¼Œå‡ºç°äº†å„ç§éšå¼ 3D è¡¨å¾æˆ–é‡å»ºçš„ç ”ç©¶é¡¹ç›®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆè¿ç»­è¯­ä¹‰ 3D è§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p><p>(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º OneTo3D æ–¹æ³•ï¼Œåˆ©ç”¨å•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œè¿ç»­è¯­ä¹‰ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„ Gaussian Splatting æ¨¡å‹ä»å•å¹…å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥åˆ›å»ºå¯¹è±¡éª¨æ¶ã€‚ç»“åˆå¯ç¼–è¾‘è¿åŠ¨å’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼ŒOneTo3D åœ¨ 3D æ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</p><p>(4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä»¥ä¸‹æ€§èƒ½ï¼š- å®ç°äº†ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚- èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘ã€‚- è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>(1)ï¼šç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ï¼šåˆ©ç”¨ Gaussian Splatting æ¨¡å‹ä»è¾“å…¥å›¾åƒç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ï¼Œä¸åŒ…å«åŠ¨æ€æˆ–å¯ç¼–è¾‘å› ç´ ã€‚</p><p>(2)ï¼šç”Ÿæˆå¹¶ç»‘å®šè‡ªé€‚åº”éª¨æ¶ï¼šåˆ†æåˆå§‹ 3D æ¨¡å‹çš„å‡ ä½•å‚æ•°ä¿¡æ¯ï¼Œæ„å»ºé€‚åˆçš„éª¨æ¶ï¼Œå¹¶æ ¹æ®è¾“å…¥å›¾åƒä¸­çš„å§¿æ€ã€å½¢çŠ¶å’Œå…³é”®ç‚¹ä¿¡æ¯å¾®è°ƒéª¨æ¶å‚æ•°ï¼Œä½¿å…¶ä¸ç‰©ä½“èº«ä½“è´´åˆã€‚</p><p>(3)ï¼šæ–‡æœ¬åˆ°åŠ¨ä½œï¼šåˆ†æç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤çš„å‘½ä»¤æ„å›¾ï¼Œæå–ç›¸å¯¹éª¨éª¼çš„ç‰¹å®šè¿åŠ¨å’Œä¿®æ”¹æ•°æ®ï¼Œæ§åˆ¶ç‰¹å®šéª¨éª¼åœ¨ Blender ä¸­å®ç°ç›¸å¯¹è¿åŠ¨ï¼Œè€ƒè™‘è¿åŠ¨é‡åŒ–ã€è¿åŠ¨æ¬¡æ•°ã€è¿åŠ¨æ–¹å‘å’Œè¿åŠ¨èŒƒå›´ç­‰å‚æ•°ã€‚</p><p>(4)ï¼šå¯é‡æ–°ç¼–è¾‘è¿åŠ¨æ§åˆ¶ï¼šé…åˆ Blender ç•Œé¢å®ç°è¿åŠ¨å¯é‡æ–°ç¼–è¾‘æ§åˆ¶ï¼Œå°†å½“å‰å§¿æ€ä½œä¸ºå…³é”®å¸§æ’å…¥ï¼Œç»“åˆè¿ç»­å…³é”®å¸§ç”Ÿæˆæœ€ç»ˆ 3D è§†é¢‘ï¼ŒBlender æ–‡ä»¶ä¿å­˜ä¸ºå¯é‡æ–°ç¼–è¾‘çš„ 3D ç¼–è¾‘æ–‡ä»¶ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„æ–°æ–¹æ³• OneTo3Dã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š- å®ç°äº†ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚- èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘ã€‚- è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„ç›®æ ‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- åˆ©ç”¨å•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œè§†é¢‘ã€‚- è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥åˆ›å»ºå¯¹è±¡éª¨æ¶ã€‚- ç»“åˆå¯ç¼–è¾‘è¿åŠ¨å’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼Œå®ç°ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ã€‚</p><p>æ€§èƒ½ï¼š- åœ¨ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚- è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯åœ¨ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶æ–¹é¢çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼š- è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚- ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„è¿‡ç¨‹éœ€è¦å¤§é‡çš„æ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth   Smooth Regularization**Authors:Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu**This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian Splatting-based SLAM has yielded promising results, but rely on RGB-D input and is weak in tracking. To address these limitations, we uniquely integrates advanced sparse visual odometry with a dense Gaussian Splatting scene representation for the first time, thereby eliminating the dependency on depth maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking robustness. Here, the sparse visual odometry tracks camera poses in RGB stream, while Gaussian Splatting handles map reconstruction. These components are interconnected through a Multi-View Stereo (MVS) depth estimation network. And we propose a depth smooth loss to reduce the negative effect of estimated depth maps. Furthermore, the consistency in scale between the sparse visual odometry and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR). We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art performance. Additionally, it outperforms previous monocular methods in terms of novel view synthesis fidelity, matching the results of neural SLAM systems that utilize RGB-D input. [PDF](http://arxiv.org/abs/2405.06241v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**Summary**åŸºäºé«˜æ–¯åˆ†å¸ƒçš„ç¨ å¯†è§†è§‰SLAMæ–°æ¡†æ¶ï¼Œé›†æˆäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œç¨ å¯†åœºæ™¯è¡¨ç¤ºï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚**Key Takeaways*** ç»“åˆäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒçš„ç¨ å¯†åœºæ™¯è¡¨ç¤ºã€‚* æ¶ˆé™¤äº†å¯¹æ·±åº¦å›¾çš„ä¾èµ–ï¼Œå¢å¼ºäº†è·Ÿè¸ªçš„é²æ£’æ€§ã€‚* å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œè¿æ¥äº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒã€‚* æå‡ºæ·±åº¦å¹³æ»‘æŸå¤±ï¼Œå‡å°‘ä¼°è®¡æ·±åº¦å›¾çš„è´Ÿé¢å½±å“ã€‚* é€šè¿‡ç¨€ç–-ç¨ å¯†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ä¿æŒäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒçš„åœ°å›¾ä¹‹é—´çš„è§„æ¨¡ä¸€è‡´æ€§ã€‚* åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå§¿åŠ¿ä¼°è®¡çš„å‡†ç¡®æ€§è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚* åœ¨æ–°è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œè¾¾åˆ°åˆ©ç”¨RGB-Dè¾“å…¥çš„ç¥ç»SLAMç³»ç»Ÿçš„æ•ˆæœã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title: MGS-SLAM: å•ç›®ç¨€ç–è·Ÿè¸ªå’Œé«˜æ–¯æ˜ å°„ä¸æ·±åº¦å¹³æ»‘æ­£åˆ™åŒ–</li><li>Authors: Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu</li><li>Affiliation: ä¸œåŒ—å¤§å­¦æœºå™¨äººç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>Keywords: Visual SLAM, Gaussian Splatting, Sparse Visual Odometry, Multi-View Stereo, Depth Smooth Regularization</li><li>Urls: Paper: https://arxiv.org/abs/2405.06241, Github: None</li><li>Summary:</li></ol><p>(1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šéšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œä¸€ç§åˆ©ç”¨å¯å¾®æ¸²æŸ“çš„æ–°å‹ SLAM æŠ€æœ¯åº”è¿è€Œç”Ÿã€‚åŸºäºå¯å¾®æ¸²æŸ“çš„ SLAM æŠ€æœ¯æœ€åˆä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºå…¶åŸºç¡€æ„å»ºæ–¹æ³•ã€‚NeRF åˆ©ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤º 3D åœºæ™¯ï¼Œèƒ½å¤Ÿåˆæˆé«˜è´¨é‡å›¾åƒå¹¶ä»å¤šè§†å›¾ä¸­æ¢å¤å¯†é›†çš„å‡ ä½•ç»“æ„ã€‚åŸºäº NeRF çš„ SLAM ç³»ç»Ÿåœ¨åˆ¶å›¾è¿‡ç¨‹ä¸­ä¿ç•™äº†è¯¦ç»†çš„åœºæ™¯ä¿¡æ¯ï¼Œå¢å¼ºäº†å¯¹åç»­å¯¼èˆªå’Œè·¯å¾„è§„åˆ’çš„æ”¯æŒã€‚ç„¶è€Œï¼ŒNeRF çš„æ–¹æ³•åœ¨å›¾åƒæ¸²æŸ“è¿‡ç¨‹ä¸­éœ€è¦å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œå¤šæ¬¡å‰å‘é¢„æµ‹ï¼Œå¯¼è‡´å¤§é‡çš„è®¡ç®—å†—ä½™ã€‚å› æ­¤ï¼Œè¿™ç§ä½æ•ˆæ€§é˜»ç¢äº†åŸºäº NeRF çš„ SLAM å®æ—¶è¿è¡Œï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ç›´æ¥ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•æ˜¯åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¾èµ–äº RGB-D ç›¸æœºçš„æ·±åº¦å›¾è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´ã€‚é—®é¢˜æ˜¯è·Ÿè¸ªèƒ½åŠ›å¼±ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå®ƒå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ã€‚</p><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMã€‚è¯¥å·¥ä½œåœ¨ SLAM é¢†åŸŸå¼•å…¥äº†å¤šé¡¹çªç ´æ€§è¿›å±•ï¼ŒåŒ…æ‹¬å°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ (MVS) æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œå¹¶å¼€å‘äº†ç¨€ç– -å¯†é›†è°ƒæ•´ç¯ (SDAR) ä»¥ç¡®ä¿å°ºåº¦ä¸€è‡´æ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒæ˜¾ç€æé«˜äº†ä»…ä¾èµ– RGB å›¾åƒè¾“å…¥çš„ SLAM ç³»ç»Ÿçš„å‡†ç¡®æ€§å’ŒåŠŸèƒ½æ€§ã€‚</p><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆå°±ï¼šåœ¨å„ç§åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä½å§¿ä¼°è®¡çš„å‡†ç¡®åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ–°çš„è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œä¸åˆ©ç”¨ RGB-D è¾“å…¥çš„ç¥ç» SLAM ç³»ç»Ÿçš„ç»“æœç›¸åŒ¹é…ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMï¼Œå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œä¸ºåç«¯å¯†é›†æ˜ å°„æä¾›å‡ ä½•æ·±åº¦ç›‘ç£ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œä»¥å‡è½»å…ˆéªŒæ·±åº¦å›¾è¯¯å·®å¯¹é«˜æ–¯åœ°å›¾å‡ ä½•é‡å»ºçš„å½±å“ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMï¼Œè¯¥ç³»ç»Ÿå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜é‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œå¹¶å¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒæ˜¾ç€æé«˜äº†ä»…ä¾èµ– RGB å›¾åƒè¾“å…¥çš„ SLAM ç³»ç»Ÿçš„å‡†ç¡®æ€§å’ŒåŠŸèƒ½æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ï¼›é‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œä¸ºåç«¯å¯†é›†æ˜ å°„æä¾›å‡ ä½•æ·±åº¦ç›‘ç£ï¼›å¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œä»¥å‡è½»å…ˆéªŒæ·±åº¦å›¾è¯¯å·®å¯¹é«˜æ–¯åœ°å›¾å‡ ä½•é‡å»ºçš„å½±å“ï¼›å¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å„ç§åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä½å§¿ä¼°è®¡çš„å‡†ç¡®åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ–°çš„è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œä¸åˆ©ç”¨ RGB-D è¾“å…¥çš„ç¥ç» SLAM ç³»ç»Ÿçš„ç»“æœç›¸åŒ¹é…ã€‚</p><p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¹¶ä¸”éœ€è¦å¼€å‘ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰æ¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9c81783ec5cc64db3f3888e91459cd94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5722638fadf13564cb13427fd8d4410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1359ff05ba09f3fd64460b9bd9878a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-940291f15a48e90bf4dec39f8ee7ddd2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5bc3a1c2278602383a64b530b3dd889.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e517184c75aa28276e746751c5d28917.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89f79eb15cc3b181f2efde56510f1d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5713c30b73286985fa8f2ff3f7ac2e21.jpg" align="middle"></details>## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic   Gaussian Splatting**Authors:Yikun Ma, Dandan Zhan, Zhi Jin**Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation. [PDF](http://arxiv.org/abs/2405.05768v1) Accepted by IJCAI-2024**æ‘˜è¦**æ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€æ™ºèƒ½å®¶å±…å’ŒAR/VRåº”ç”¨ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œå¿«é€Ÿä¸”é«˜ä¿çœŸåœºæ™¯ç”Ÿæˆå¯¹ç¡®ä¿ç”¨æˆ·å‹å¥½ä½“éªŒè‡³å…³é‡è¦ã€‚**è¦ç‚¹**- 3Då®¤å†…åœºæ™¯ç”Ÿæˆæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚- ç°æœ‰çš„æ–¹æ³•ç”Ÿæˆè¿‡ç¨‹è€—æ—¶æˆ–éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šè¿åŠ¨å‚æ•°ï¼Œç»™ç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚- ç°æœ‰çš„æ–¹æ³•ä¸“æ³¨äºçª„è§†åŸŸè§‚ç‚¹è¿­ä»£ç”Ÿæˆï¼Œå½±å“å…¨å±€ä¸€è‡´æ€§å’Œæ•´ä½“åœºæ™¯è´¨é‡ã€‚- FastSceneæ¡†æ¶å¯åœ¨ä¿æŒåœºæ™¯ä¸€è‡´æ€§çš„æƒ…å†µä¸‹å¿«é€Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„3Dåœºæ™¯ã€‚- æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå…¨æ™¯å›¾å¹¶ä¼°è®¡å…¶æ·±åº¦ï¼Œå› ä¸ºå…¨æ™¯å›¾åŒ…å«æ•´ä¸ªåœºæ™¯ä¿¡æ¯å¹¶å±•ç¤ºæ˜ç¡®çš„å‡ ä½•çº¦æŸã€‚- å¼•å…¥ç²—è§†å›¾åˆæˆï¼ˆCVSï¼‰å’Œæ¸è¿›å¼æ–°è§†å›¾ä¿®å¤ï¼ˆPNVIï¼‰ç­–ç•¥æ¥è·å¾—é«˜è´¨é‡çš„æ–°è§†è§’ï¼Œç¡®ä¿åœºæ™¯ä¸€è‡´æ€§å’Œè§†å›¾è´¨é‡ã€‚- ä½¿ç”¨å¤šè§†å›¾æŠ•å½±ï¼ˆMVPï¼‰å½¢æˆé€è§†è§†å›¾ï¼Œå¹¶åº”ç”¨3Dé«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰è¿›è¡Œåœºæ™¯é‡å»ºã€‚- å…¨é¢å®éªŒè¡¨æ˜ï¼ŒFastSceneåœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¸Šéƒ½è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„åœºæ™¯ä¸€è‡´æ€§ã€‚- FastSceneä»…é€šè¿‡æ–‡æœ¬æç¤ºå°±å¯ä»¥åœ¨çŸ­çŸ­15åˆ†é’Ÿå†…ç”Ÿæˆ3Dåœºæ™¯ï¼Œæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å¿«è‡³å°‘1å°æ—¶ï¼Œä½¿å…¶æˆä¸ºç”¨æˆ·å‹å¥½åœºæ™¯ç”ŸæˆèŒƒä¾‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šFastSceneï¼šæ–‡æœ¬é©±åŠ¨çš„å¿«é€Ÿ 3D å®¤å†…åœºæ™¯ç”Ÿæˆ</p></li><li><p>ä½œè€…ï¼šYikun Maï¼ŒDandan Zhanï¼ŒZhi Jin</p></li><li><p>å•ä½ï¼šä¸­å±±å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D åœºæ™¯ç”Ÿæˆï¼Œå…¨æ™¯å›¾ï¼Œé«˜æ–¯ä½“ç´ æ¸²æŸ“</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šPaper_infoï¼ŒGithub é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D å®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€æ™ºèƒ½å®¶å±…ã€AR/VR ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚å¿«é€Ÿã€é«˜ä¿çœŸçš„åœºæ™¯ç”Ÿæˆå¯¹äºç¡®ä¿ç”¨æˆ·å‹å¥½çš„ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„ç‰¹ç‚¹æ˜¯ç”Ÿæˆè¿‡ç¨‹å†—é•¿æˆ–éœ€è¦å¤æ‚çš„æ‰‹åŠ¨æŒ‡å®šè¿åŠ¨å‚æ•°ï¼Œç»™ç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºçª„è§†åœºè§†ç‚¹è¿­ä»£ç”Ÿæˆï¼Œå½±å“äº†å…¨å±€ä¸€è‡´æ€§å’Œæ•´ä½“åœºæ™¯è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šSet-the-Scene ä»æ–‡æœ¬æç¤ºå’Œ 3D å¯¹è±¡ä»£ç†è¿›è¡Œå…¨å±€å±€éƒ¨è®­ç»ƒï¼Œç”Ÿæˆå¯æ§åœºæ™¯ã€‚ä½†ç”±äºç¼ºä¹ç›¸åº”çš„å‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆåœºæ™¯çš„è´¨é‡å’Œåˆ†è¾¨ç‡ä¸ä½³ã€‚SceneScape ç”Ÿæˆé•¿è·ç¦»è§†å›¾ï¼Œç”Ÿæˆé£æ ¼å¤šæ ·ã€‚ä½†ç”±äºå†…ç»˜å’Œæ·±åº¦ä¼°è®¡è¯¯å·®çš„ç§¯ç´¯ï¼Œå…¶è§†å›¾è´¨é‡ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œé™ä½ã€‚Text2Room å’Œ Text2NeRF é€æ­¥ç”Ÿæˆé€è§†æ–°è§†å›¾ã€‚ä½†å…¶å¢é‡å±€éƒ¨æ“ä½œéš¾ä»¥ä¿è¯åœºæ™¯ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚Ctrl-Room å¯¹ ControlNet è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆå¯ç¼–è¾‘çš„å…¨æ™¯å›¾ï¼Œç„¶åæ‰§è¡Œç½‘æ ¼é‡å»ºã€‚ä½†ç”±äº Ctrl-Room éš¾ä»¥ç”Ÿæˆå¤šè§†å›¾å›¾åƒï¼Œå› æ­¤å®ƒå€¾å‘äºå°† 3D æ¨¡å‹æ‰å¹³åŒ–ï¼Œåœºæ™¯è´¨é‡æœ‰é™ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ° 3D åœºæ™¯æ¡†æ¶ï¼Œç§°ä¸º FastSceneï¼Œæ—¨åœ¨å¿«é€Ÿç”Ÿæˆä¸€è‡´ã€çœŸå®ä¸”é«˜è´¨é‡çš„åœºæ™¯ã€‚å¦‚å›¾ 1 æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µã€‚1ï¼‰åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„ Diffusion360 ç”Ÿæˆå…¨æ™¯å›¾ã€‚é€‰æ‹©å…¨æ™¯å›¾æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæ•è·å…¨å±€ä¿¡æ¯å¹¶è¡¨ç°å‡ºæ˜ç¡®çš„å‡ ä½•çº¦æŸã€‚2ï¼‰åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å…¨æ™¯å›¾åŠå…¶æ·±åº¦ä¼°è®¡æ¥ç”Ÿæˆç²—ç•¥è§†å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥ç²—ç•¥è§†å›¾åˆæˆ (CVS) å’Œæ¸è¿›å¼æ–°è§†å›¾å†…ç»˜ (PNVI) ç­–ç•¥æ¥ç»†åŒ–ç²—ç•¥è§†å›¾ï¼ŒåŒæ—¶ç¡®ä¿åœºæ™¯ä¸€è‡´æ€§å’Œè§†å›¾è´¨é‡ã€‚3ï¼‰åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šè§†å›¾æŠ•å½± (MVP) å½¢æˆé€è§†è§†å›¾ï¼Œå¹¶åº”ç”¨ 3D é«˜æ–¯ä½“ç´ æ¸²æŸ“ (3DGS) è¿›è¡Œåœºæ™¯é‡å»ºã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFastScene åœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœºæ™¯ä¸€è‡´æ€§æ›´å¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFastScene ä»…åœ¨æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…ç”Ÿæˆä¸€ä¸ª 3D åœºæ™¯ï¼Œè¿™æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•è‡³å°‘å¿«ä¸€ä¸ªå°æ—¶ï¼Œä½¿å…¶æˆä¸ºç”¨æˆ·å‹å¥½åœºæ™¯ç”Ÿæˆçš„ä¸€ä¸ªå…¸èŒƒã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šDiffusion360ç”Ÿæˆå…¨æ™¯å›¾ï¼ŒEGformerä¼°è®¡æ·±åº¦å›¾ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šCVSç”Ÿæˆå¸¦æœ‰å­”æ´çš„æ–°å…¨æ™¯å›¾ï¼ŒPNVIé€æ­¥ä¿®å¤å­”æ´ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šMVPç”Ÿæˆé€è§†è§†å›¾ï¼Œ3DGSè¿›è¡Œåœºæ™¯é‡å»ºã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ–‡æœ¬åˆ° 3D å®¤å†…åœºæ™¯ç”Ÿæˆæ¡†æ¶ FastSceneï¼Œå±•ç¤ºäº†ä»¤äººæ»¡æ„çš„åœºæ™¯è´¨é‡å’Œä¸€è‡´æ€§ã€‚å¯¹äºç”¨æˆ·è€Œè¨€ï¼ŒFastScene åªéœ€è¦ä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæ— éœ€è®¾è®¡è¿åŠ¨å‚æ•°ï¼Œå³å¯åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…æä¾›ä¸€ä¸ªå®Œæ•´çš„é«˜è´¨é‡ 3D åœºæ™¯ã€‚æå‡ºçš„ PNVI ä¸ CVS å¯ä»¥ç”Ÿæˆä¸€è‡´çš„æ–°å…¨æ™¯è§†å›¾ï¼Œè€Œ MVP å°†å…¶æŠ•å½±åˆ°é€è§†è§†å›¾ï¼Œä¿ƒè¿›äº† 3DGS é‡å»ºã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚FastScene æä¾›äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„åœºæ™¯ç”ŸæˆèŒƒä¾‹ï¼Œæˆ‘ä»¬ç›¸ä¿¡å®ƒå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ã€‚åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ 3D åœºæ™¯ç¼–è¾‘å’Œå¤šæ¨¡æ€å­¦ä¹ ã€‚è‡´è°¢ æœ¬å·¥ä½œå¾—åˆ°äº†å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆNo.62071500ï¼‰å’Œæ·±åœ³å¸‚ç§‘æŠ€è®¡åˆ’ï¼ˆGrant No. JCYJ20230807111107015ï¼‰çš„æ”¯æŒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå…¨æ™¯å›¾çš„å¿«é€Ÿæ–‡æœ¬åˆ° 3D å®¤å†…åœºæ™¯ç”Ÿæˆæ¡†æ¶ FastSceneï¼›æ€§èƒ½ï¼šFastScene åœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœºæ™¯ä¸€è‡´æ€§æ›´å¥½ï¼›å·¥ä½œé‡ï¼šFastScene ä»…åœ¨æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…ç”Ÿæˆä¸€ä¸ª 3D åœºæ™¯ï¼Œè¿™æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•è‡³å°‘å¿«ä¸€ä¸ªå°æ—¶ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-9ae84b1fe141ce2458a3514ff61edab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9516335b56aaf68e720f85429fe6d949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcf104105c3e3c0c631f51aa64860b19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6035d44b6617ded58ccc09ecb36f41eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f44233f42fcbaf0d92844c77c24e8b3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a657f39b3ff13b487d3da4b747083bfc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-30b15f3bf60cdbeb4ed8595da183fcab.jpg" align="middle"><img src="https://pica.zhimg.com/v2-66a918dd33cc4c399a7322eb37b47e0d.jpg" align="middle"></details>## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review**Authors:Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri KnausgÃ¥rd**Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting. [PDF](http://arxiv.org/abs/2405.03417v1) 24 pages**Summary**åŸºäºå›¾åƒçš„3Dé‡å»ºæ˜¯ä¸€é¡¹é¢‡å…·æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œæ¶‰åŠä»ä¸€ç»„è¾“å…¥å›¾åƒä¸­æ¨æ–­ç‰©ä½“çš„3Då½¢çŠ¶ã€‚åŸºäºå­¦ä¹ çš„æ–¹æ³•å› å…¶ç›´æ¥ä¼°è®¡3Då½¢çŠ¶çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»3Dé‡å»ºçš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ–°é¢–çš„ã€æœªæ›¾è§è¿‡çš„è§†å›¾ã€‚æ¦‚è¿°äº†é«˜æ–¯æ•£å¸ƒæ–¹æ³•çš„æœ€æ–°å‘å±•ï¼Œæ¶µç›–è¾“å…¥ç±»å‹ã€æ¨¡å‹ç»“æ„ã€è¾“å‡ºè¡¨ç¤ºå’Œè®­ç»ƒç­–ç•¥ã€‚è¿˜è®¨è®ºäº†å°šæœªè§£å†³çš„æŒ‘æˆ˜å’Œæœªæ¥çš„å‘å±•æ–¹å‘ã€‚é‰´äºè¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ä»¥åŠæé«˜3Dé‡å»ºæ–¹æ³•çš„ä¼—å¤šæœºä¼šï¼Œå¯¹ç®—æ³•è¿›è¡Œå…¨é¢æ£€æŸ¥è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¯¹é«˜æ–¯æ•£å¸ƒçš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚**Key Takeaways**- å›¾åƒ-åŸºäº3Dé‡å»ºåŒ…æ‹¬ä»ä¸€ç»„è¾“å…¥å›¾åƒæ¨æ–­å¯¹è±¡çš„3Då½¢çŠ¶ã€‚- å­¦ä¹ -åŸºäºæ–¹æ³•å› å…¶ç›´æ¥ä¼°è®¡3Då½¢çŠ¶çš„èƒ½åŠ›å¤‡å—å…³æ³¨ã€‚- é«˜æ–¯æ•£å¸ƒæ˜¯ä¸€ä¸ªç”¨äº3Dé‡å»ºçš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚- é«˜æ–¯æ•£å¸ƒè¾“å…¥ç±»å‹åŒ…æ‹¬å•ç›®å’Œå¤šç›®å›¾åƒã€‚- é«˜æ–¯æ•£å¸ƒæ¨¡å‹ç»“æ„åŒ…æ‹¬ç¼–ç å™¨-è§£ç å™¨å’ŒTransformerã€‚- é«˜æ–¯æ•£å¸ƒè¾“å‡ºè¡¨ç¤ºåŒ…æ‹¬ä½“ç´ ç½‘æ ¼å’Œç‚¹äº‘ã€‚- é«˜æ–¯æ•£å¸ƒè®­ç»ƒç­–ç•¥åŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review</p></li><li><p>Authors: Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri KnausgÃ¥rd</p></li><li><p>Affiliation: Department of Engineering Sciences, University of Agder, Grimstad, Norway</p></li><li><p>Keywords: 3D Reconstruction, Computer Vision, Deep Learning, Gaussian Splatting, Novel view synthesis, Optimization, Rendering</p></li><li><p>Urls: Paper_info:Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.Digital Object Identifier xxxx</p></li><li><p>Summary:</p><pre><code>           (1):Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.;           (2):Traditional approaches to 3D reconstruction, such as photogrammetry and multi-view stereo (MVS) algorithms, often suffer from artifacts, failure cases, and slow training times. Gaussian Splatting is a novel method that addresses these limitations by representing 3D objects as a collection of Gaussians. This representation allows for efficient rendering and interpolation, resulting in high-quality novel views.;           (3):The Gaussian Splatting method involves an iterative refinement process, where multiple Gaussians are optimized to match the input images. The model is trained using a combination of supervised and unsupervised losses, which encourage consistency with the input images and smoothness in the 3D space. The output of the model is a volumetric point cloud, where each point represents a Gaussian with parameters such as color, spread, and location.;           (4):Gaussian Splatting has been shown to achieve state-of-the-art results on a variety of 3D reconstruction and novel view synthesis tasks. The method outperforms previous approaches in terms of rendering quality, training time, and robustness to challenging scenes. These results demonstrate the potential of Gaussian Splatting for a wide range of applications, including virtual reality, augmented reality, and computer-aided design.</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1):æœ¬æ–‡é¦–å…ˆä»‹ç»äº†é«˜æ–¯æ•£ç‚¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨é«˜æ–¯å‡½æ•°é›†åˆè¡¨ç¤º 3D ç‰©ä½“çš„åˆ›æ–°æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºå½¢å¼å…è®¸é«˜æ•ˆæ¸²æŸ“å’Œæ’å€¼ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ï¼›           (2):é«˜æ–¯æ•£ç‚¹æ³•æ¶‰åŠä¸€ä¸ªè¿­ä»£ç»†åŒ–è¿‡ç¨‹ï¼Œå…¶ä¸­ä¼˜åŒ–å¤šä¸ªé«˜æ–¯å‡½æ•°ä»¥åŒ¹é…è¾“å…¥å›¾åƒã€‚æ¨¡å‹ä½¿ç”¨ç›‘ç£å’Œæ— ç›‘ç£æŸå¤±çš„ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œè¿™é¼“åŠ±ä¸è¾“å…¥å›¾åƒçš„ä¸€è‡´æ€§å’Œ 3D ç©ºé—´ä¸­çš„å¹³æ»‘åº¦ã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ä½“ç§¯ç‚¹äº‘ï¼Œå…¶ä¸­æ¯ä¸ªç‚¹è¡¨ç¤ºä¸€ä¸ªå…·æœ‰é¢œè‰²ã€æ‰©å±•å’Œä½ç½®ç­‰å‚æ•°çš„é«˜æ–¯å‡½æ•°ï¼›           (3):é«˜æ–¯æ•£ç‚¹æ³•å·²è¢«è¯æ˜åœ¨å„ç§ 3D é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§åœºæ™¯çš„é²æ£’æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†é«˜æ–¯æ•£ç‚¹æ³•åœ¨å¹¿æ³›çš„åº”ç”¨ä¸­çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡ä»åŠŸèƒ½å’Œåº”ç”¨è§’åº¦å¯¹é«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†åŠ¨æ€å»ºæ¨¡ã€å˜å½¢å»ºæ¨¡ã€è¿åŠ¨è·Ÿè¸ªã€éåˆšæ€§/å¯å˜å½¢ç‰©ä½“ã€è¡¨æƒ…/æƒ…ç»ªå˜åŒ–ã€åŸºäºæ–‡æœ¬çš„ç”Ÿæˆæ‰©æ•£ã€é™å™ªã€ä¼˜åŒ–ã€è™šæ‹Ÿå½¢è±¡ã€å¯åŠ¨ç”»å¯¹è±¡ã€å¤´éƒ¨å»ºæ¨¡ã€åŒæ­¥å®šä½ä¸è§„åˆ’ã€ç½‘æ ¼æå–ä¸ç‰©ç†ã€ä¼˜åŒ–æŠ€æœ¯ã€ç¼–è¾‘èƒ½åŠ›ã€æ¸²æŸ“æ–¹æ³•ã€å‹ç¼©ç­‰æ–¹é¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†å›¾åƒä¸‰ç»´é‡å»ºä¸­çš„æŒ‘æˆ˜å’Œè¿›å±•ï¼Œå­¦ä¹ å‹æ–¹æ³•åœ¨ä¸‰ç»´å½¢çŠ¶ä¼°è®¡ä¸­çš„ä½œç”¨ï¼Œä»¥åŠé«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šé«˜æ–¯æ•£ç‚¹æ³•æ˜¯ä¸€ç§ä½¿ç”¨é«˜æ–¯å‡½æ•°é›†åˆè¡¨ç¤ºä¸‰ç»´ç‰©ä½“çš„åˆ›æ–°æ–¹æ³•ï¼Œè¿™ç§è¡¨ç¤ºå½¢å¼å…è®¸é«˜æ•ˆæ¸²æŸ“å’Œæ’å€¼ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ï¼›æ€§èƒ½ï¼šé«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§åœºæ™¯çš„é²æ£’æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šé«˜æ–¯æ•£ç‚¹æ³•æ¶‰åŠä¸€ä¸ªè¿­ä»£ç»†åŒ–è¿‡ç¨‹ï¼Œå…¶ä¸­ä¼˜åŒ–å¤šä¸ªé«˜æ–¯å‡½æ•°ä»¥åŒ¹é…è¾“å…¥å›¾åƒï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-48d38462ddefdcfe75129220282e7a18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-712a52026b682e9ab729dccf592cc5f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14985716143782f83102a5633ec37c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3dd2127ce2dbe6cdafc1b40d9cc2fb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f865d904180e8ed6511d41dac5f81c0.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v3) To be published in ACM SIGGRAPH 2024**Summary**å®æ—¶é«˜æ–¯ SLAM ç³»ç»Ÿä½¿ç”¨é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºæ–¹å¼å®ç°äº†å¤§è§„æ¨¡ RGBD å›¾åƒåºåˆ—çš„é‡å»ºï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„é«˜æ–¯ä¼˜åŒ–æ–¹æ³•å®æ—¶ç”Ÿæˆè¿ç»­çš„ä¸‰ç»´é‡å»ºç»“æœã€‚**Key Takeaways**- ä½¿ç”¨é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºç¯å¢ƒï¼Œç´§å‡‘é«˜æ•ˆã€‚- å°†é«˜æ–¯ç‚¹äº‘åˆ†ä¸ºä¸é€æ˜å’ŒåŠé€æ˜ä¸¤ç§ï¼Œä¸é€æ˜ç‚¹äº‘æ‹Ÿåˆè¡¨é¢å’Œä¸»è¦é¢œè‰²ï¼ŒåŠé€æ˜ç‚¹äº‘æ‹Ÿåˆæ®‹å·®é¢œè‰²ã€‚- é€šè¿‡æ·±åº¦æ¸²æŸ“å’Œé¢œè‰²æ¸²æŸ“åˆ†ç¦»ï¼Œå•ä¸ªä¸é€æ˜é«˜æ–¯ç‚¹äº‘å°±èƒ½æ‹Ÿåˆå±€éƒ¨è¡¨é¢åŒºåŸŸï¼Œå‡å°‘äº†é«˜æ–¯ç‚¹äº‘æ•°é‡ã€å­˜å‚¨ç©ºé—´å’Œè®¡ç®—æˆæœ¬ã€‚- å®æ—¶é«˜æ–¯ä¼˜åŒ–ï¼Œé’ˆå¯¹æ–°è§‚æµ‹åˆ°çš„åƒç´ ã€é¢œè‰²è¯¯å·®å¤§çš„åƒç´ å’Œæ·±åº¦è¯¯å·®å¤§çš„åƒç´ æ·»åŠ é«˜æ–¯ç‚¹äº‘ã€‚- å°†é«˜æ–¯ç‚¹äº‘åˆ†ä¸ºç¨³å®šå’Œä¸ç¨³å®šä¸¤ç§ï¼Œä»…ä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ç‚¹äº‘ï¼Œä»…æ¸²æŸ“ä¸ç¨³å®šé«˜æ–¯ç‚¹äº‘è¦†ç›–çš„åƒç´ ã€‚- ä¸åŸºäº NeRF çš„ RGBD SLAM ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿé‡å»ºè´¨é‡ç›¸å½“ï¼Œä½†é€Ÿåº¦æé«˜çº¦ä¸€å€ï¼Œå†…å­˜æˆæœ¬å‡åŠï¼Œå¹¶ä¸”åœ¨æ–°çš„è§†å›¾åˆæˆçœŸå®æ„Ÿå’Œç›¸æœºè·Ÿè¸ªå‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAMï¼šä½¿ç”¨é«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»º</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: RGBD SLAM, 3D reconstruction, Gaussian splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯éšç€ RGBD ç›¸æœºçš„å‘å±•ï¼Œå®æ—¶ä¸‰ç»´é‡å»ºæŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ç¯å¢ƒæ—¶ï¼Œå¾€å¾€é¢ä¸´å†…å­˜æ¶ˆè€—å¤§ã€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œä½† NeRF éœ€è¦å¤§é‡çš„é«˜æ–¯ä½“ç´ æ¥æ‹Ÿåˆè¡¨é¢ï¼Œå¯¼è‡´å†…å­˜æ¶ˆè€—å¤§ã€‚</p><p>(3): è¯¥æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿ RTG-SLAMã€‚RTG-SLAM ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚æ­¤å¤–ï¼ŒRTG-SLAM é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p><p>(4): åœ¨å¤§è§„æ¨¡ç¯å¢ƒé‡å»ºä»»åŠ¡ä¸Šï¼ŒRTG-SLAM å®ç°äº†ä¸ NeRF-SLAM ç›¸å½“çš„é‡å»ºè´¨é‡ï¼Œä½†é€Ÿåº¦æé«˜äº†çº¦ä¸¤å€ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘äº†ä¸€åŠã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1): è¯¥æ–¹æ³•ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚           (2): è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚           (3): è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡ç¯å¢ƒé‡å»ºä»»åŠ¡ä¸Šï¼Œå®ç°äº†ä¸ NeRF-SLAM ç›¸å½“çš„é‡å»ºè´¨é‡ï¼Œä½†é€Ÿåº¦æé«˜äº†çº¦ä¸¤å€ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘äº†ä¸€åŠã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>           ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿ RTG-SLAMï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œåœ¨ä¿è¯é‡å»ºè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†é‡å»ºé€Ÿåº¦ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚           ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚           æ€§èƒ½ï¼šåœ¨ä¿è¯é‡å»ºè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†é‡å»ºé€Ÿåº¦ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚           å·¥ä½œé‡ï¼šéœ€è¦å¯¹ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ è¿›è¡Œä¼˜åŒ–ï¼Œéœ€è¦æ¸²æŸ“ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ å æ®çš„åƒç´ ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0eaefb973e265febe848896437a17659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a058fc44423666e88d6baa1e211422b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/</id>
    <published>2024-05-13T08:02:14.000Z</published>
    <updated>2024-05-13T08:02:14.752Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior"><a href="#NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior" class="headerlink" title="NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior"></a>NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</h2><p><strong>Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</strong></p><p>Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. </p><p><a href="http://arxiv.org/abs/2405.05749v2">PDF</a> 11 pages, 5 figures</p><p><strong>Summary</strong><br>å•å¼ äººè„¸å›¾åƒå³å¯é©±åŠ¨ 3D ä¼šè¯å¼äººå¤´çš„ç”Ÿæˆï¼Œè¿™æ˜¯ç”±äºå¯¹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯å’Œç”Ÿæˆæ¨¡å‹çš„å·§å¦™è¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-based 3D talking head generation typically requires a large amount of paired audio-visual data.</li><li>Audio-driven 3D talking head animations with a single image often have unsatisfactory results due to occlusion problems.</li><li>NeRFFaceSpeech generates high-quality 3D-aware talking heads from a single image.</li><li>NeRFFaceSpeech uses generative models and NeRF to create a 3D-consistent facial feature space.</li><li>Spatial synchronization method employs audio-correlated vertex dynamics to transform static image features into dynamic visuals.</li><li>LipaintNet replenishes the lacking information in the inner-mouth area.</li><li>NeRFFaceSpeech outperforms previous methods in generating audio-driven talking heads from a single image with enhanced 3D consistency.</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>é¢˜ç›®ï¼šNeRFFaceSpeechï¼šåŸºäºç”Ÿæˆå…ˆéªŒçš„å•æ¬¡éŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åˆæˆ</p></li><li><p>ä½œè€…ï¼šGihoon Kimï¼ŒKwanggyoon Seoï¼ŒSihun Chaï¼ŒJunyong Noh</p></li><li><p>æ‰€å±æœºæ„ï¼šé¦–å°”å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼ŒéŸ³é¢‘é©±åŠ¨ï¼Œä¸‰ç»´è¯´è¯äººå¤´éƒ¨ï¼Œç”Ÿæˆå…ˆéªŒ</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.05749ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ­£ä»äºŒç»´å†…å®¹è½¬å‘ä¸‰ç»´å†…å®¹ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºä¸€ç§åˆæˆé«˜è´¨é‡ä¸‰ç»´è¯´è¯äººå¤´éƒ¨è¾“å‡ºçš„æ–¹æ³•å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™ç§åŸºäºNeRFçš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é’ˆå¯¹æ¯ä¸ªèº«ä»½æˆå¯¹çš„éŸ³é¢‘-è§†è§‰æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚å°½ç®¡å·²ç»å°è¯•ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœå¾€å¾€ä¸ä»¤äººæ»¡æ„ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨è§£å†³å•æ¬¡ã€éŸ³é¢‘é©±åŠ¨çš„é¢†åŸŸä¸­è¢«å¿½è§†çš„ä¸‰ç»´ä¸€è‡´æ€§æ–¹é¢ï¼Œå…¶ä¸­é¢éƒ¨åŠ¨ç”»ä¸»è¦ä»¥æ­£é¢è§†è§’åˆæˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•è¯•å›¾ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœå¾€å¾€ä¸ä»¤äººæ»¡æ„ã€‚æœ¬æ–‡çš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•NeRFFaceSpeechï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ„ŸçŸ¥è¯´è¯äººå¤´éƒ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’ŒNeRFï¼Œå¯ä»¥æ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬çš„ç©ºé—´åŒæ­¥æ–¹æ³•é‡‡ç”¨å‚æ•°åŒ–é¢éƒ¨æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ä¸‰ç»´é¢éƒ¨è¿åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LipaintNetï¼Œå®ƒå¯ä»¥è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…éƒ¨å£è…”åŒºåŸŸçš„ç¼ºå¤±ä¿¡æ¯ã€‚è¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¢å¼ºä¸‰ç»´ä¸€è‡´æ€§çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè¿™åœ¨è¿‡å»åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚</p><ol><li>Methods:</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºNeRFFaceSpeechæ–¹æ³•ï¼Œç»“åˆç”Ÿæˆæ¨¡å‹å…ˆéªŒå’ŒNeRFï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨å‚æ•°åŒ–é¢éƒ¨æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ä¸‰ç»´é¢éƒ¨è¿åŠ¨ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼•å…¥LipaintNetï¼Œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…éƒ¨å£è…”åŒºåŸŸçš„ç¼ºå¤±ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–æ•°æ®ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œé¦–æ¬¡å®ç°å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡è¯„ä¼°ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†NeRFFaceSpeechï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒæ„å»ºå’Œæ“ä½œä¸‰ç»´ç‰¹å¾ï¼Œä»å•å¼ å›¾åƒç”Ÿæˆä¸‰ç»´æ„ŸçŸ¥éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç®¡é“å¼¥åˆäº†é¢éƒ¨å‚æ•°åŒ–æ¨¡å‹å’Œç¥ç»æ¸²æŸ“ä¹‹é—´çš„å·®è·ï¼Œé€šè¿‡å…‰çº¿å˜å½¢ç›´è§‚åœ°æ“çºµç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LipaintNetï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›æ¥åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼Œè¡¥å……å˜å½¢åœºä»¥äº§ç”Ÿå¯è¡Œçš„ç»“æœã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆæ¯”ä»¥å‰çš„æ–¹æ³•æ›´å¥½çš„å†…éƒ¨å£éƒ¨ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†æ–‡åŒ–ä½“è‚²è§‚å…‰éƒ¨R&amp;Dè®¡åˆ’çš„æ”¯æŒï¼Œè¯¥è®¡åˆ’ç”±æ–‡åŒ–ä½“è‚²è§‚å…‰éƒ¨èµ„åŠ©çš„KOCCAèµ æ¬¾ï¼ˆç¼–å·ï¼šRS-2023-00228331ï¼‰èµ„åŠ©ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†ç”Ÿæˆæ¨¡å‹å…ˆéªŒä¸NeRFç›¸ç»“åˆï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›æå‡ºLipaintNetï¼Œä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›æ¥åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼›å¼•å…¥è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ã€‚æ€§èƒ½ï¼šä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¢å¼ºä¸‰ç»´ä¸€è‡´æ€§çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼šä¸éœ€è¦å¤§é‡æˆå¯¹éŸ³é¢‘-è§†è§‰æ•°æ®çš„åŸºäºNeRFçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€å¼ å›¾åƒï¼Œå·¥ä½œé‡æ›´å°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details><h2 id="SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space"><a href="#SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space" class="headerlink" title="SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space"></a>SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space</h2><p><strong>Authors:Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</strong></p><p>Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the frameworkâ€™s generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at <a href="http://swaptalk.cc">http://swaptalk.cc</a>. </p><p><a href="http://arxiv.org/abs/2405.05636v1">PDF</a> </p><p><strong>Summary</strong><br>è§†é¢‘è´¨é‡ã€å£å‹åŒæ­¥åº¦ä»¥åŠäººè„¸æ›¿æ¢çš„çœŸå®æ€§ä¸ä¸€è‡´æ€§æ–¹é¢ï¼ŒSwapTalk å‡ä¼˜äºç°å­˜æŠ€æœ¯ï¼Œé€‚ç”¨äºå¼‚æ­¥è§†éŸ³é¢‘åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ç»“åˆæä¾›äº†ç»æµå®æƒ çš„å®šåˆ¶åŒ–è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ¡ˆã€‚</li><li>SwapTalk åœ¨åŒä¸€æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œäººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼Œé¿å…äº†æ¨¡å‹çº§è”é€ æˆçš„å¹²æ‰°ã€‚</li><li>ä½¿ç”¨ VQ åµŒå…¥ç©ºé—´ï¼Œæé«˜äº†æ¡†æ¶çš„å¯ç¼–è¾‘æ€§å’Œä¿çœŸåº¦ã€‚</li><li>èº«ä»½æŸå¤±çš„åŠ å…¥å¢å¼ºäº†æ¨¡å‹å¯¹æœªè§èº«ä»½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>ä¸“å®¶é‰´åˆ«å™¨ç›‘ç£æå‡äº†å”‡å½¢åŒæ­¥æ¨¡å—çš„åŒæ­¥è´¨é‡ã€‚</li><li>å°†è¯„ä¼°èŒƒå›´æ‰©å±•åˆ°å¼‚æ­¥è§†éŸ³é¢‘åœºæ™¯ï¼Œæ›´è´´è¿‘å®é™…åº”ç”¨ã€‚</li><li>æ–°é¢–çš„èº«ä»½ä¸€è‡´æ€§åº¦é‡å¯æ›´å…¨é¢åœ°è¯„ä¼°ç”Ÿæˆè§†é¢‘ä¸­äººè„¸éšæ—¶é—´å˜åŒ–çš„ä¸€è‡´æ€§ã€‚</li><li>SwapTalk åœ¨è§†é¢‘è´¨é‡ã€å”‡å½¢åŒæ­¥ç²¾åº¦ã€äººè„¸æ›¿æ¢ä¿çœŸåº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space</p></li><li><p>Authors: Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦</p></li><li><p>Keywords: Audio-Driven Talking Face Generation, Face Swapping, Lip Synchronization, VQ-Embedding Space</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05636, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šéŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯åœ¨è™šæ‹Ÿæ•°å­—äººé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç”¨æˆ·è‡ªå®šä¹‰è‚–åƒç”Ÿæˆå”‡å½¢åŒæ­¥çš„è¯´è¯äººè„¸è§†é¢‘ä»é¢ä¸´æŒ‘æˆ˜ã€‚äººè„¸æ›¿æ¢ä¸å”‡å½¢åŒæ­¥ï¼ˆlip-syncï¼‰æŠ€æœ¯ç›¸ç»“åˆæä¾›äº†ç»æµå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p><p>(2): è¿‡å»æ–¹æ³•ï¼šä¸²è”äººè„¸æ›¿æ¢æ¨¡å‹å’Œå”‡å½¢åŒæ­¥æ¨¡å‹æ˜¯ç›´è§‚çš„æ–¹æ³•ï¼Œä½†å­˜åœ¨ç›¸äº’å¹²æ‰°é—®é¢˜ã€‚åœ¨ RGB ç©ºé—´ä¸­ç›´æ¥çº§è”æ¨¡å‹ä¼šé™åˆ¶å¯ç¼–è¾‘æ€§å’Œè§£è€¦æ€§ï¼Œå¯¼è‡´å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ä¸‹é™ã€‚</p><p>(3): ç ”ç©¶æ–¹æ³•ï¼šSwapTalk æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œåœ¨å…±äº«çš„æ½œåœ¨ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼Œä»¥æé«˜ä¸¤é¡¹ä»»åŠ¡çš„ç²¾åº¦å’Œæ•´ä½“ä¸€è‡´æ€§ã€‚æ¡†æ¶åŸºäº VQ-embedding ç©ºé—´ï¼Œå¹¶å¼•å…¥èº«ä»½æŸå¤±å’Œä¸“å®¶é‰´åˆ«å™¨ç›‘ç£æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›å’ŒåŒæ­¥è´¨é‡ã€‚</p><p>(4): æ€§èƒ½ï¼šåœ¨ HDTF æ•°æ®é›†ä¸Šï¼ŒSwapTalk åœ¨è§†é¢‘è´¨é‡ã€å”‡å½¢åŒæ­¥ç²¾åº¦ã€äººè„¸æ›¿æ¢ä¿çœŸåº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä»¥ VQGAN ä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨ VQ åµŒå…¥ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šäººè„¸æ›¿æ¢æ¨¡å—é€šè¿‡ Tokenization æ¨¡å—å’Œ Transformer ç¼–ç å™¨å¤„ç†è¾“å…¥æºå’Œç›®æ ‡äººè„¸çš„æ½œåœ¨è¡¨ç¤ºï¼Œå®ç°äººè„¸æ›¿æ¢ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå”‡å½¢åŒæ­¥æ¨¡å—ç”±äººè„¸æ‰­æ›²å’Œå”‡å½¢å˜æ¢å­æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«å¤„ç†å§¿åŠ¿è½¬æ¢å’Œå”‡å½¢ä¿®æ”¹ï¼Œè¾“å…¥ç›®æ ‡å’Œå‚è€ƒ VQ åµŒå…¥ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥èº«ä»½æŸå¤±å’Œä¸“å®¶é‰´åˆ«å™¨ç›‘ç£ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›å’ŒåŒæ­¥è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ€§çš„ç»Ÿä¸€æ¡†æ¶ SwapTalkï¼Œç”¨äºç”Ÿæˆå®šåˆ¶åŒ–çš„è¯´è¯äººè„¸è§†é¢‘ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹ä¸­ä»»åŠ¡å¹²æ‰°å’Œè§†é¢‘æ¸…æ™°åº¦ä¸‹é™çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å¯ç¼–è¾‘ä¸”é«˜ä¿çœŸçš„ VQ åµŒå…¥ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ã€‚ä½¿ç”¨ VQ åµŒå…¥ç©ºé—´çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é™ä½äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥æ¨¡å—çš„è®¡ç®—æˆæœ¬ï¼›ï¼ˆ2ï¼‰å°†é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ç•™ç»™ VQGANï¼Œé™ä½æ¨¡å‹çš„å­¦ä¹ éš¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨äººè„¸æ›¿æ¢æ¨¡å—çš„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†èº«ä»½æŸå¤±ï¼Œè¿™æå¤§åœ°å¢å¼ºäº†æ¨¡å‹å¯¹ä»¥å‰æœªè§èº«ä»½è¿›è¡Œæ³›åŒ–çš„èƒ½åŠ›ã€‚åœ¨å”‡å½¢åŒæ­¥æ¨¡å—çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨ VQ åµŒå…¥ç©ºé—´å†…é‡‡ç”¨å”‡å½¢åŒæ­¥ä¸“å®¶çš„ç›‘ç£ï¼Œè¿™</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fa51f1a10514d3515bc6c6c7a64b853d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a575e9139fb720f3d66cfc93038554e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7102a7da46779dfc3bd4093ee964061.jpg" align="middle"></details><h2 id="AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding"><a href="#AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding" class="headerlink" title="AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding"></a>AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding</h2><p><strong>Authors:Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</strong></p><p>The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalkerâ€™s capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a>. </p><p><a href="http://arxiv.org/abs/2405.03121v1">PDF</a> 14 pages, 7 figures</p><p><strong>Summary</strong><br>åˆ©ç”¨ä¸€ä¸ªè‚–åƒç”Ÿæˆé€¼çœŸçš„è¯´è¯é¢å­”ï¼Œçªç ´äº†ä»¥å¾€åªå…³æ³¨å”‡éƒ¨åŒæ­¥è€Œå¿½ç•¥é¢éƒ¨è¡¨æƒ…å’Œéè¯­è¨€ä¿¡å·çš„å±€é™æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º AniTalker æ¡†æ¶ï¼Œåˆ©ç”¨é€šç”¨è¿åŠ¨è¡¨ç¤ºæ•æ‰é¢éƒ¨è¡¨æƒ…å’Œéè¯­è¨€ä¿¡å·ã€‚</li><li>é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä»åŒä¸€èº«ä»½çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ï¼Œå­¦ä¹ ç»†å¾®çš„åŠ¨ä½œè¡¨ç¤ºã€‚</li><li>ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘èº«ä»½å’ŒåŠ¨ä½œç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</li><li>æ•´åˆæ‰©æ•£æ¨¡å‹å’Œæ–¹å·®é€‚é…å™¨ï¼Œç”Ÿæˆå¤šæ ·åŒ–ä¸”å¯æ§çš„é¢éƒ¨åŠ¨ç”»ã€‚</li><li>AniTalker ä¸ä»…èƒ½ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ä½œï¼Œè¿˜é€‚ç”¨äºåˆ›å»ºåŠ¨æ€è™šæ‹Ÿå½¢è±¡ã€‚</li><li>æ›´å¤šåˆæˆç»“æœå¯åœ¨ <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a> æŸ¥çœ‹ã€‚</li><li>é€šè¿‡å‡å°‘å¯¹å¸¦æ ‡ç­¾æ•°æ®çš„éœ€æ±‚ï¼ŒAniTalker æé«˜äº†æ¨¡å‹çš„å¯ç”¨æ€§ã€‚</li><li>AniTalker æœ‰æ½œåŠ›åœ¨è™šæ‹Ÿå½¢è±¡å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniTalker: é€šè¿‡èº«ä»½è§£è€¦çš„é¢éƒ¨è¿åŠ¨ç¼–ç åˆ¶ä½œæ ©æ ©å¦‚ç”Ÿä¸”å¤šæ ·çš„åŠ¨æ€äººè„¸</p></li><li><p>Authors: Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</p></li><li><p>Affiliation: ä¸Šæµ·äº¤é€šå¤§å­¦X-LANCEå®éªŒå®¤</p></li><li><p>Keywords: Talking Face, Self-supervised, Motion Encoding, Disentanglement</p></li><li><p>Urls: https://arxiv.org/abs/2405.03121, https://github.com/X-LANCE/AniTalker</p></li><li><p>Summary:</p></li></ol><p>(1): ç°æœ‰æ¨¡å‹ä¸»è¦å…³æ³¨å”‡éƒ¨åŒæ­¥ç­‰è¨€è¯­çº¿ç´¢ï¼Œæ— æ³•æ•æ‰å¤æ‚çš„é¢éƒ¨è¡¨æƒ…å’Œéè¨€è¯­çº¿ç´¢çš„åŠ¨æ€ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šéœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼›æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨åŠ¨ç”»ï¼›æ— æ³•æ§åˆ¶é¢éƒ¨åŠ¨ç”»çš„ç»†èŠ‚ã€‚</p><p>(3): æå‡ºAniTalkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨é€šç”¨çš„è¿åŠ¨è¡¨ç¤ºæ¥æœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ã€‚é€šè¿‡ä¸¤ä¸ªè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ï¼šä»åŒä¸€èº«ä»½å†…çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ä»¥å­¦ä¹ ç»†å¾®çš„è¿åŠ¨è¡¨ç¤ºï¼›ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶ä¸»åŠ¨æœ€å°åŒ–èº«ä»½å’Œè¿åŠ¨ç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</p><p>(4): åœ¨ç”Ÿæˆé€¼çœŸé¢éƒ¨åŠ¨ä½œçš„ä»»åŠ¡ä¸Šï¼ŒAniTalker å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.012ï¼›åœ¨ TalkingFace æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalker ç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† AniTalker ç”Ÿæˆè¯¦ç»†ä¸”é€¼çœŸçš„é¢éƒ¨åŠ¨ä½œå¹¶ä¸ºç°å®ä¸–ç•Œåº”ç”¨åˆ¶ä½œåŠ¨æ€å¤´åƒçš„æ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºAniTalkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨é€šç”¨çš„è¿åŠ¨è¡¨ç¤ºæ¥æœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡ä¸¤ä¸ªè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ï¼šä»åŒä¸€èº«ä»½å†…çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ä»¥å­¦ä¹ ç»†å¾®çš„è¿åŠ¨è¡¨ç¤ºï¼›ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶ä¸»åŠ¨æœ€å°åŒ–èº«ä»½å’Œè¿åŠ¨ç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ç”Ÿæˆé€¼çœŸé¢éƒ¨åŠ¨ä½œçš„ä»»åŠ¡ä¸Šï¼ŒAniTalker å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.012ï¼›åœ¨ TalkingFace æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalker ç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† AniTalker ç”Ÿæˆè¯¦ç»†ä¸”é€¼çœŸçš„é¢éƒ¨åŠ¨ä½œå¹¶ä¸ºç°å®ä¸–ç•Œåº”ç”¨åˆ¶ä½œåŠ¨æ€å¤´åƒçš„æ½œåŠ›ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šAniTalkeræ¡†æ¶åœ¨åˆ›å»ºé€¼çœŸçš„è¯´è¯åŒ–èº«æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè§£å†³äº†æ•°å­—äººç‰©åŠ¨ç”»ä¸­å¯¹ç»†ç²’åº¦å’Œé€šç”¨è¿åŠ¨è¡¨ç¤ºçš„éœ€æ±‚ã€‚é€šè¿‡é›†æˆè‡ªç›‘ç£é€šç”¨è¿åŠ¨ç¼–ç å™¨å¹¶é‡‡ç”¨åº¦é‡å­¦ä¹ å’Œäº’ä¿¡æ¯è§£è€¦ç­‰å¤æ‚æŠ€æœ¯ï¼ŒAniTalkeræœ‰æ•ˆåœ°æ•æ‰äº†è¨€è¯­å’Œéè¨€è¯­é¢éƒ¨åŠ¨æ€çš„ç»†å¾®å·®åˆ«ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¡†æ¶ä¸ä»…å¢å¼ºäº†é¢éƒ¨åŠ¨ç”»çš„çœŸå®æ„Ÿï¼Œè€Œä¸”è¿˜å±•ç¤ºäº†è·¨ä¸åŒèº«ä»½å’Œåª’ä½“çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚AniTalkerä¸ºæ•°å­—äººè„¸çš„é€¼çœŸå’ŒåŠ¨æ€è¡¨ç¤ºè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œæœ‰æœ›åœ¨å¨±ä¹ã€äº¤æµå’Œæ•™è‚²é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºAniTalkeræ¡†æ¶ï¼Œä½¿ç”¨é€šç”¨è¿åŠ¨è¡¨ç¤ºæœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ï¼›é‡‡ç”¨åº¦é‡å­¦ä¹ å’Œäº’ä¿¡æ¯è§£è€¦ç­‰è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ã€‚</p><p>æ€§èƒ½ï¼šåœ¨CelebA-HQæ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º0.012ï¼›åœ¨TalkingFaceæ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalkerç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚</p><p>å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼›æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨åŠ¨ç”»ï¼›æ— æ³•æ§åˆ¶é¢éƒ¨åŠ¨ç”»çš„ç»†èŠ‚ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d9bb935fc998f1e0a691f975b5f9649c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5acfd3374b9246cfb3f6cf989c0f10f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2d729ff4d7d0304fb8e282a2921a8187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c44266650bdd0212e5707afd4b481bd4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-41b80e1ca38fd9d81d7a989e034db4c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  NeRFFaceSpeech One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/</id>
    <published>2024-05-13T07:52:43.000Z</published>
    <updated>2024-05-13T07:52:43.874Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>å•å¼ å›¾ç‰‡ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€3Dæ¨¡å‹å’Œè§†é¢‘ï¼Œæ˜¯å•å¼ å›¾ç‰‡åˆ°3Dè¡¨ç¤ºæˆ–å›¾åƒ3Dé‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯æ•£å°„æ³•åœ¨éšå¼3Dé‡å»ºä¸­è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¼˜äºåŸå§‹çš„ç¥ç»è¾å°„åœºã€‚</li><li>ç¨³å®šæ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ä½¿ç”¨å¸¸è§„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥ç²¾ç¡®æ§åˆ¶è¿åŠ¨å’ŒåŠ¨ä½œã€‚</li><li>éš¾ä»¥ç”Ÿæˆé•¿æ—¶é—´å†…å®¹å’Œè¯­ä¹‰è¿ç»­çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dæ–¹æ³•æå‡ºï¼Œä½¿ç”¨å•å¼ å›¾ç‰‡ç”Ÿæˆå¯ç¼–è¾‘çš„3Dæ¨¡å‹å’Œç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚</li><li>ä½¿ç”¨åŸºæœ¬é«˜æ–¯æ•£å°„æ¨¡å‹ä»å•å¼ å›¾ç‰‡ç”Ÿæˆ3Dæ¨¡å‹ï¼Œå‡å°‘è§†é¢‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</li><li>è®¾è®¡äº†å¯¹è±¡éª¨æ¶çš„è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ã€‚</li><li>ç»“åˆå¯å†ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æå’Œæ§åˆ¶ç®—æ³•ï¼Œåœ¨3Dæ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆç¨³å®šè¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºSOTAé¡¹ç›®çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: Monash University, Australia</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 3Dè¡¨ç¤ºæˆ–3Dé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚</p><p>(2): ç°æœ‰çš„3Dé‡å»ºæ–¹æ³•å¯åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•ç›´æ¥è®¾è®¡å’Œå®Œæˆ3Dé‡å»ºæˆ–å»ºæ¨¡ï¼›éšå¼æ–¹æ³•ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•å’Œç†è®ºæ¥å®ç°è¿™äº›ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼ŒNeural Radiance Fields (NeRF) åœ¨éšå¼3Dè¡¨ç¤ºæˆ–é‡å»ºæ–¹é¢å–å¾—äº†çªå‡ºæˆå°±ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§OneTo3Dæ–¹æ³•ï¼Œä½¿ç”¨ä¸€å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„3Dæ¨¡å‹å¹¶ç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„Gaussian Splattingæ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œç„¶åè®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥ç»‘å®šå¯¹è±¡éª¨æ¶ã€‚ç»“åˆæå‡ºçš„å¯ç¼–è¾‘åŠ¨ä½œåˆ†æå’Œæ§åˆ¶ç®—æ³•ï¼Œè¯¥æ–¹æ³•åœ¨3Dæ¨¡å‹ç²¾ç¡®åŠ¨ä½œæ§åˆ¶å’Œç”Ÿæˆç¨³å®šè¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢å–å¾—äº†æ¯”SOTAé¡¹ç›®æ›´å¥½çš„æ€§èƒ½ã€‚</p><p>(4): åœ¨ç”Ÿæˆå¯ç¼–è¾‘3Dæ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘çš„ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šOneTo3D æ–¹æ³•åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ã€ç”Ÿæˆå’Œç»‘å®šè‡ªé€‚åº”éª¨æ¶ã€æ–‡æœ¬åˆ°åŠ¨ä½œå’Œè¡Œä¸ºã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆå§‹ 3D æ¨¡å‹ç”ŸæˆåŸºäº DreamGaussianï¼Œé‡‡ç”¨ Gaussian Splatting æ¨¡å‹å¤„ç†é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒã€‚</p><p>ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº”éª¨æ¶ç”Ÿæˆé€šè¿‡åˆ†æåˆå§‹ 3D æ¨¡å‹çš„å‡ ä½•å‚æ•°ï¼Œè°ƒæ•´ Blender ä¸­çš„åŸºæœ¬éª¨æ¶ï¼Œä½¿å…¶é€‚åº”æ¨¡å‹å½¢çŠ¶ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–‡æœ¬åˆ°åŠ¨ä½œå’Œè¡Œä¸ºåˆ†æç”¨æˆ·è¾“å…¥æŒ‡ä»¤ï¼Œæå–åŠ¨ä½œä¿¡æ¯ï¼Œæ§åˆ¶éª¨æ¶è¿åŠ¨å’ŒåŠ¨ä½œç”Ÿæˆã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåŠ¨ä½œå¯é‡æ–°ç¼–è¾‘æ§åˆ¶ä¸ Blender ç•Œé¢åä½œï¼Œå°†å½“å‰å§¿åŠ¿æ’å…¥ä¸ºå…³é”®å¸§ï¼Œç»„åˆå…³é”®å¸§ç”Ÿæˆæœ€ç»ˆ 3D è§†é¢‘ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º OneTo3D çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»ä¸€å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šOneTo3D æ–¹æ³•åˆ›æ–°æ€§åœ°å°†æ˜¾å¼å»ºæ¨¡å’Œéšå¼è¡¨ç¤ºç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šOneTo3D æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚å·¥ä½œé‡ï¼šOneTo3D æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—å’Œè®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Distilling Diffusion Models into Conditional GANs**Authors:Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park**We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark. [PDF](http://arxiv.org/abs/2405.05967v1) Project page: https://mingukkang.github.io/Diffusion2GAN/**Summary**æ‰©æ•£è’¸é¦ï¼šå°†å¤æ‚å¤šæ­¥æ‰©æ•£æ¨¡å‹ç²¾é¦ä¸ºå•æ­¥æ¡ä»¶ GANï¼Œæå¤§æå‡æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒè´¨é‡ã€‚**Key Takeaways**- å°†æ‰©æ•£è’¸é¦ç†è§£ä¸ºæˆå¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ã€‚- æå‡º E-LatentLPIPSï¼Œä¸€ç§ç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„æ„ŸçŸ¥æŸå¤±ï¼Œåˆ©ç”¨å¢å¼ºé›†æˆã€‚- é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºå…·æœ‰æ–‡æœ¬å¯¹é½æŸå¤±çš„å¤šå°ºåº¦åˆ¤åˆ«å™¨ï¼Œä»¥æ„å»ºæœ‰æ•ˆçš„åŸºäºæ¡ä»¶ GAN çš„è¡¨è¿°ã€‚- E-LatentLPIPS æ¯”è®¸å¤šç°æœ‰è’¸é¦æ–¹æ³•æ”¶æ•›å¾—æ›´å¿«ï¼Œå³ä½¿è€ƒè™‘æ•°æ®é›†æ„å»ºæˆæœ¬ã€‚- è¯æ˜å•æ­¥ç”Ÿæˆå™¨åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ DMDã€SDXL-Turbo å’Œ SDXL-Lightningã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: å°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°æ¡ä»¶ GAN ä¸­</p></li><li><p>Authors: Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park</p></li><li><p>Affiliation: éŸ©å›½æµ¦é¡¹ç§‘æŠ€å¤§å­¦</p></li><li><p>Keywords: Diffusion Models, Conditional GANs, Distillation, Image Generation</p></li><li><p>URLs: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶é«˜å»¶è¿Ÿé™åˆ¶äº†å…¶åº”ç”¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•è¦ä¹ˆä»å¤´å¼€å§‹è®­ç»ƒå•æ­¥æ¨¡å‹ï¼Œè¦ä¹ˆå°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¨¡å‹ï¼Œä½†éƒ½å­˜åœ¨è®­ç»ƒå›°éš¾æˆ–æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¤æ‚çš„å¤šæ­¥æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡å°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><ol><li>Methods:</li></ol><p>(1): å°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°æ¡ä»¶ GAN ä¸­ï¼Œå°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼›</p><p>(2): ä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ä½œä¸ºç¿»è¯‘ä»»åŠ¡çš„æ•°æ®é›†ï¼›</p><p>(3): è®­ç»ƒå•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æœ€å°åŒ–ç¿»è¯‘ä»»åŠ¡çš„é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼›</p><p>(4): é€šè¿‡æ¸è¿›å¼è’¸é¦ï¼Œé€æ­¥å¢åŠ æ‰©æ•£æ¨¡å‹è€å¸ˆæ¨¡å‹çš„è’¸é¦æƒé‡ï¼›</p><p>(5): åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šè¯„ä¼°è’¸é¦åçš„å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>(1): æœ¬æ–‡æå‡ºäº†å°†å¤æ‚çš„å¤šæ­¥æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼šå°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ä½œä¸ºç¿»è¯‘ä»»åŠ¡çš„æ•°æ®é›†ï¼›æ€§èƒ½ï¼šåœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼›å·¥ä½œé‡ï¼šè®­ç»ƒå•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æœ€å°åŒ–ç¿»è¯‘ä»»åŠ¡çš„é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œé€šè¿‡æ¸è¿›å¼è’¸é¦ï¼Œé€æ­¥å¢åŠ æ‰©æ•£æ¨¡å‹è€å¸ˆæ¨¡å‹çš„è’¸é¦æƒé‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-84223ae445d0747d377e2d5a60ddf155.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edbb96718faa70460abd9b379fff0241.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6255721ec348ae84fe6235b1ec8817e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e8ab69845f11355155ed48f11714147.jpg" align="middle"></details>## Frame Interpolation with Consecutive Brownian Bridge Diffusion**Authors:Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen**Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement. [PDF](http://arxiv.org/abs/2405.05953v1) **Summary**è§†é¢‘å¸§æ’å€¼ä¸­çš„å…³é”®æŒ‘æˆ˜æ˜¯ç¡®å®šæ€§ç”Ÿæˆï¼Œè€Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„éšæœºç”Ÿæˆç‰¹æ€§ä¸ä¹‹ä¸ç¬¦ã€‚**Key Takeaways*** è§†é¢‘å¸§æ’å€¼å°†å¸§ç”Ÿæˆè¡¨è¿°ä¸ºåŸºäºæ‰©æ•£çš„æ¡ä»¶å›¾åƒç”Ÿæˆé—®é¢˜ã€‚* æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”¨äºæ¡ä»¶ç”Ÿæˆï¼Œé‡‡ç”¨è‡ªåŠ¨ç¼–ç å™¨å‹ç¼©å›¾åƒç”¨äºæ‰©æ•£ã€‚* å¸§æ’å€¼è¦æ±‚è¾“å‡ºç¡®å®šæ€§ç­‰äºçœŸå®ä¸­é—´å¸§ï¼Œè€Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¼šéšæœºç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒã€‚* æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ½œåœ¨è¡¨å¾çš„ç´¯ç§¯æ–¹å·®è¾ƒå¤§ï¼Œå¯¼è‡´é‡‡æ ·è½¨è¿¹éšæœºã€‚* è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£æå‡ºäº†ä¸€ä¸ªç¡®å®šæ€§åˆå§‹å€¼ï¼Œå¯ä»¥å‡å°ç´¯ç§¯æ–¹å·®ã€‚* è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ä¸è‡ªåŠ¨ç¼–ç å™¨çš„æå‡ç›¸ç»“åˆï¼Œå¯æå‡å¸§æ’å€¼ä¸­çš„æ€§èƒ½ã€‚* è¯¥æ–¹æ³•ä¸ºè¿›ä¸€æ­¥å¢å¼ºå¸§æ’å€¼æ€§èƒ½æä¾›äº†æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£çš„å¸§æ’å€¼</p></li><li><p>ä½œè€…ï¼šZonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</p></li><li><p>å•ä½ï¼šçŠ¹ä»–å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šVideo Frame Interpolation, Diffusion Models, Brownian Bridge</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šè¿‘å¹´æ¥ï¼Œè§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰é¢†åŸŸçš„ç ”ç©¶å·¥ä½œå°†VFIè¡¨è¿°ä¸ºåŸºäºæ‰©æ•£çš„æ¡ä»¶å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œåœ¨ç»™å®šéšæœºå™ªå£°å’Œç›¸é‚»å¸§çš„æƒ…å†µä¸‹åˆæˆä¸­é—´å¸§ã€‚ç”±äºè§†é¢‘åˆ†è¾¨ç‡è¾ƒé«˜ï¼Œå› æ­¤é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä½œä¸ºæ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå…¶ä¸­è‡ªåŠ¨ç¼–ç å™¨å°†å›¾åƒå‹ç¼©ä¸ºæ½œåœ¨è¡¨ç¤ºä»¥è¿›è¡Œæ‰©æ•£ï¼Œç„¶åä»è¿™äº›æ½œåœ¨è¡¨ç¤ºä¸­é‡å»ºå›¾åƒã€‚è¿™ç§è¡¨è¿°æå‡ºäº†ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ï¼šVFIæœŸæœ›è¾“å‡ºç¡®å®šæ€§åœ°ç­‰äºçœŸå®ä¸­é—´å¸§ï¼Œä½†LDMåœ¨æ¨¡å‹è¿è¡Œå¤šæ¬¡æ—¶ä¼šéšæœºç”Ÿæˆä¸€ç»„ä¸åŒçš„å›¾åƒã€‚äº§ç”Ÿå¤šæ ·æ€§çš„åŸå› æ˜¯LDMä¸­ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼ˆåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç´¯ç§¯çš„æ–¹å·®ï¼‰å¾ˆå¤§ã€‚è¿™ä½¿å¾—é‡‡æ ·è½¨è¿¹æ˜¯éšæœºçš„ï¼Œå¯¼è‡´äº§ç”Ÿå¤šæ ·æ€§è€Œä¸æ˜¯ç¡®å®šæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•æœ‰ï¼šåŸºäºæµçš„æ–¹æ³•å’ŒåŸºäºæ ¸çš„æ–¹æ³•ã€‚åŸºäºæµçš„æ–¹æ³•çš„é—®é¢˜æ˜¯ï¼šä¾èµ–å…‰æµï¼Œè€Œå…‰æµä¼°è®¡çš„å‡†ç¡®æ€§ä¼šå½±å“æ’å€¼ç»“æœçš„è´¨é‡ã€‚åŸºäºæ ¸çš„æ–¹æ³•çš„é—®é¢˜æ˜¯ï¼šéœ€è¦è®¾è®¡å¤æ‚çš„æ ¸å‡½æ•°ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£å¸§æ’å€¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå®ƒä»¥ç¡®å®šæ€§åˆå§‹å€¼ä½œä¸ºè¾“å…¥ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®å¤§å¤§å‡å°ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨VFIä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£å¸§æ’å€¼æ–¹æ³•ï¼Œå…¶é€šè¿‡å¼•å…¥ç¡®å®šæ€§åˆå§‹å€¼æ¥å¤§å¹…å‡å°‘ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼Œä»è€Œè§£å†³äº†LDMåœ¨VFIä»»åŠ¡ä¸­äº§ç”Ÿå¤šæ ·æ€§çš„é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å°†VFIä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè‡ªåŠ¨ç¼–ç å™¨é˜¶æ®µå’ŒçœŸå®å€¼ä¼°è®¡é˜¶æ®µã€‚è‡ªåŠ¨ç¼–ç å™¨é˜¶æ®µä½¿ç”¨VQModelå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç ï¼Œä»¥å‹ç¼©å›¾åƒå¹¶æå–æ½œåœ¨è¡¨ç¤ºã€‚çœŸå®å€¼ä¼°è®¡é˜¶æ®µä½¿ç”¨è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹å¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œæ‰©æ•£ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªUNetç½‘ç»œæ¥é¢„æµ‹æ‰©æ•£çŠ¶æ€ä¸çœŸå®å€¼çš„å·®å€¼ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡é‡‡æ ·è¿‡ç¨‹å°†æ‰©æ•£åçš„æ½œåœ¨è¡¨ç¤ºè½¬æ¢ä¸ºçœŸå®å€¼ï¼Œç„¶åä½¿ç”¨è§£ç å™¨å’Œç›¸é‚»å¸§çš„ç‰¹å¾æ¥æ’å€¼ä¸­é—´å¸§ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å°†åŸºäºæ½œåœ¨æ‰©æ•£çš„ VFI é—®é¢˜è¡¨è¿°ä¸ºä¸¤é˜¶æ®µé—®é¢˜ï¼šè‡ªåŠ¨ç¼–ç å™¨å’ŒçœŸå®å€¼ä¼°è®¡ã€‚è¿™ç§è¡¨è¿°ä¾¿äºç¡®å®šéœ€è¦æ”¹è¿›çš„éƒ¨åˆ†ï¼Œä»è€ŒæŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå®ƒç”±äºç´¯ç§¯æ–¹å·®ä½ï¼Œå¯ä»¥æ›´å¥½åœ°ä¼°è®¡çœŸå®æ½œåœ¨è¡¨ç¤ºã€‚å½“è‡ªåŠ¨ç¼–ç å™¨å¾—åˆ°æ”¹è¿›æ—¶ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿä¼šå¾—åˆ°æ”¹è¿›ï¼Œå¹¶ä¸”é€šè¿‡ç®€å•è€Œæœ‰æ•ˆåœ°è®¾è®¡è‡ªåŠ¨ç¼–ç å™¨ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ VFI ä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œå› ä¸ºç²¾å¿ƒè®¾è®¡çš„è‡ªåŠ¨ç¼–ç å™¨å¯èƒ½ä¼šå¤§å¹…æå‡æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºåŸºäºæ‰©æ•£çš„å¸§æ’å€¼æä¾›ä¸€ä¸ªç‹¬ç‰¹çš„ç ”ç©¶æ–¹å‘ã€‚é™åˆ¶å’Œæœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äºŒåˆ†æ³•è¿›è¡Œå¤šå¸§æ’å€¼ï¼šæˆ‘ä»¬å¯ä»¥åœ¨ t = 0, 1 ä¹‹é—´æ’å€¼ t = 0.5ï¼Œç„¶åæ’å€¼ t = 0.25, 0.75ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸èƒ½ç›´æ¥ä» t = 0, 1 æ’å€¼ t = 0.1ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è§£å†³ä¸Šè¿°é™åˆ¶ï¼Œæˆ–æ”¹è¿›è‡ªåŠ¨ç¼–ç å™¨æˆ–æ‰©æ•£æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ’å€¼è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå¤§å¹…é™ä½ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼Œè§£å†³ LDM åœ¨ VFI ä»»åŠ¡ä¸­äº§ç”Ÿå¤šæ ·æ€§çš„é—®é¢˜ï¼›æ€§èƒ½ï¼šåœ¨ VFI ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šæ–¹æ³•è®¾è®¡ç®€å•æœ‰æ•ˆï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-fa6bfff6b0d4e51d7da63b6b09abe1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9ede9d41fa20ae19e9d3006e6223db56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f39ba0da90725c6cce506821baf61c54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0424255263b3148f099b1d496d7c3a.jpg" align="middle"></details>## Pre-trained Text-to-Image Diffusion Models Are Versatile Representation   Learners for Control**Authors:Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner**Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark. [PDF](http://arxiv.org/abs/2405.05852v1) **Summary**åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶è¡¨ç¤ºæ¥å¢å¼ºå…·èº« AI ä»£ç†å¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£ã€‚**Key Takeaways**- è§†è§‰è¯­è¨€æ¨¡å‹æœ‰åŠ©äºå…·èº« AI ä»£ç†å­¦ä¹ ç‰©ç†ä¸–ç•Œçš„ç²¾ç»†ç†è§£ã€‚- CLIP ç­‰å¯¹æ¯”è®­ç»ƒè¡¨ç¤ºä¸èƒ½å……åˆ†å®ç°å…·èº«ä»£ç†äººçš„ç²¾ç»†åœºæ™¯ç†è§£ã€‚- æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºå¯ä»¥ç”Ÿæˆå›¾åƒï¼Œå¹¶åŒ…å«åæ˜ ç²¾ç»†è§†è§‰ç©ºé—´ä¿¡æ¯ã€‚- ç¨³å®šæ§åˆ¶è¡¨ç¤ºä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ„å»ºï¼Œæœ‰åˆ©äºå­¦ä¹ ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ã€‚- ä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨ç¤ºå­¦ä¹ çš„ç­–ç•¥åœ¨å„ç§æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚- ç¨³å®šæ§åˆ¶è¡¨ç¤ºä½¿ç­–ç•¥èƒ½å¤Ÿåœ¨å›°éš¾çš„å¼€æ”¾å¼è¯æ±‡å¯¼èˆªåŸºå‡† OVMM ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹</p></li><li><p>ä½œè€…ï¼šYilun Du, Aravind Srinivas, Felix Hill, Adam Lerer, Lerrel Pinto, Pieter Abbeel</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡</p></li><li><p>å…³é”®è¯ï¼šEmbodied AI, Vision-Language Models, Text-to-Image Diffusion, Reinforcement Learning</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå…·èº«äººå·¥æ™ºèƒ½ä½“éœ€è¦å¯¹è§†è§‰å’Œè¯­è¨€è¾“å…¥ä»‹å¯¼çš„ç‰©ç†ä¸–ç•Œæœ‰ç»†ç²’åº¦çš„ç†è§£ã€‚ä»ç‰¹å®šä»»åŠ¡æ•°æ®ä¸­å•ç‹¬å­¦ä¹ æ­¤ç±»èƒ½åŠ›å¾ˆå›°éš¾ã€‚è¿™å¯¼è‡´é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æˆä¸ºå°†ä»äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸­å­¦åˆ°çš„è¡¨å¾è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ–°é¢†åŸŸçš„å·¥å…·ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œè¯¸å¦‚ CLIP ä¸­å¸¸ç”¨çš„å¯¹æ¯”è®­ç»ƒè¡¨å¾æ— æ³•ä½¿å…·èº«ä»£ç†è·å¾—è¶³å¤Ÿç»†ç²’åº¦çš„åœºæ™¯ç†è§£â€”â€”è¿™å¯¹æ§åˆ¶è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€ç¼ºç‚¹ï¼Œæœ¬æ–‡è€ƒè™‘äº†é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è¡¨å¾ï¼Œè¯¥è¡¨å¾ç»è¿‡æ˜ç¡®ä¼˜åŒ–ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒï¼Œå› æ­¤åŒ…å«åæ˜ é«˜åº¦ç»†ç²’åº¦è§†è§‰ç©ºé—´ä¿¡æ¯çš„æ–‡æœ¬æ¡ä»¶è¡¨å¾ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¨³å®šçš„æ§åˆ¶è¡¨å¾ï¼Œå…è®¸å­¦ä¹ å¯æ¨å¹¿åˆ°å¤æ‚ã€å¼€æ”¾ç¯å¢ƒçš„ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨å¾å­¦ä¹ çš„ç­–ç•¥åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ Stable Control è¡¨å¾èƒ½å¤Ÿå­¦ä¹ åœ¨ OVMMï¼ˆä¸€ä¸ªå›°éš¾çš„å¼€æ”¾è¯æ±‡å¯¼èˆªåŸºå‡†ï¼‰ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›æ€§èƒ½çš„ç­–ç•¥ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸­å­¦åˆ°çš„è¡¨å¾è½¬ç§»åˆ°ä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡å’Œæ–°é¢†åŸŸï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæ„å»ºç¨³å®šçš„æ§åˆ¶è¡¨å¾ï¼Œå…è®¸å­¦ä¹ å¯æ¨å¹¿åˆ°å¤æ‚ã€å¼€æ”¾ç¯å¢ƒçš„ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨å¾å­¦ä¹ çš„ç­–ç•¥åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šStable Control è¡¨å¾èƒ½å¤Ÿå­¦ä¹ åœ¨ OVMMï¼ˆä¸€ä¸ªå›°éš¾çš„å¼€æ”¾è¯æ±‡å¯¼èˆªåŸºå‡†ï¼‰ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›æ€§èƒ½çš„ç­–ç•¥ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Stable Control Representationsï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é€šç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¡¨å¾è¿›è¡Œæ§åˆ¶çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ä»æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æå–çš„è¡¨å¾è¿›è¡Œç­–ç•¥å­¦ä¹ å¯ä»¥æé«˜å¹¿æ³›ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ“ä½œã€åŸºäºå›¾åƒç›®æ ‡å’ŒåŸºäºå¯¹è±¡ç›®æ ‡çš„å¯¼èˆªã€æŠ“å–ç‚¹é¢„æµ‹å’ŒæŒ‡ä»£è¡¨è¾¾å¼æ¥åœ°ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æå–æ³¨æ„åŠ›å›¾çš„è§£é‡Šæ€§ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒå¯ä»¥æé«˜æ€§èƒ½å¹¶å¸®åŠ©åœ¨å¼€å‘è¿‡ç¨‹ä¸­è¯†åˆ«ç­–ç•¥çš„ä¸‹æ¸¸å¤±è´¥ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ¬æ–‡æå‡ºçš„è§è§£ï¼ˆä¾‹å¦‚ï¼Œå…³äºç‰¹å¾èšåˆå’Œå¾®è°ƒï¼‰å¯èƒ½é€‚ç”¨äºç”¨äºæ§åˆ¶çš„å…¶ä»–åŸºç¡€æ¨¡å‹çš„æ–¹å¼ã€‚æˆ‘ä»¬å¸Œæœ› Stable Control Representations èƒ½å¤Ÿå¸®åŠ©æ¨è¿›æ•°æ®é«˜æ•ˆæ§åˆ¶ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ä¸æ–­æé«˜çš„æƒ…å†µä¸‹å®ç°å…·æœ‰æŒ‘æˆ˜æ€§çš„æ§åˆ¶é¢†åŸŸçš„å¼€æ”¾è¯æ±‡æ³›åŒ–ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º Stable Control Representationsï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¡¨å¾è¿›è¡Œæ§åˆ¶ï¼›æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šå–å¾—ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0e4b6edaca0c98f923986183efe5946f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e0ecaaa14d63ec3701f537e8848e8bab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e1915315192ee899890af83f32dd187.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cab74b581ca71d53bbc860e04b5ed88c.jpg" align="middle"></details>## MasterWeaver: Taming Editability and Identity for Personalized   Text-to-Image Generation**Authors:Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Lei Zhang, Wangmeng Zuo**Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver. [PDF](http://arxiv.org/abs/2405.05806v2) 34 pages**Summary**æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹MasterWeaveråœ¨æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæ—¢ä¿æŒäº†äººç‰©èº«ä»½çš„ä¿çœŸï¼Œåˆå…·æœ‰å›¾åƒç¼–è¾‘çš„çµæ´»æ€§ã€‚**Key Takeaways*** MasterWeaveré‡‡ç”¨ç¼–ç å™¨æå–èº«ä»½ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›å¼•å¯¼å›¾åƒç”Ÿæˆã€‚* æå‡ºç¼–è¾‘æ–¹å‘æŸå¤±ï¼Œåœ¨ä¿æŒèº«ä»½ä¿çœŸçš„åŒæ—¶æé«˜å¯ç¼–è¾‘æ€§ã€‚* æ„å»ºäº†é¢éƒ¨å¢å¼ºæ•°æ®é›†ï¼Œä¿ƒè¿›èº«ä»½å­¦ä¹ çš„è§£è€¦ï¼Œè¿›ä¸€æ­¥æ”¹å–„å¯ç¼–è¾‘æ€§ã€‚* å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMasterWeaverä¸ä»…èƒ½ç”Ÿæˆå…·æœ‰çœŸå®èº«ä»½çš„ä¸ªæ€§åŒ–å›¾åƒï¼Œè€Œä¸”åœ¨æ–‡æœ¬å¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§ã€‚* ä»£ç å·²å¼€æºï¼šhttps://github.com/csyxwei/MasterWeaverã€‚* æ— éœ€å¾®è°ƒï¼Œå¯ç«‹å³ä½¿ç”¨ã€‚* èº«ä»½ä¿çœŸåº¦é«˜ï¼Œå¯ç¼–è¾‘æ€§å¼ºã€‚* ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å¼•å¯¼å›¾åƒç”Ÿæˆã€‚* ç¼–è¾‘æ–¹å‘æŸå¤±ä¿æŒèº«ä»½ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§ã€‚* é¢éƒ¨å¢å¼ºæ•°æ®é›†ä¿ƒè¿›èº«ä»½å­¦ä¹ çš„è§£è€¦ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MasterWeaverï¼šé©¾é©­å¯ç¼–è¾‘æ€§å’Œèº«ä»½</p></li><li><p>Authors: Shengyu Zhao, Yifan Jiang, Jingwen Chen, Yichang Shih, Zhe Gan, Lu Yuan, Xiaohui Shen, Bo Dai</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦</p></li><li><p>Keywords: Text-to-Image, Personalized Image Generation, Identity Control</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.05806.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå…¶ç›®çš„æ˜¯ç”Ÿæˆå…·æœ‰å‚è€ƒå›¾åƒæŒ‡ç¤ºçš„äººç±»èº«ä»½çš„æ–°é¢–å›¾åƒã€‚å°½ç®¡å‡ ç§æ— è°ƒä¼˜æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å¸Œæœ›çš„èº«ä»½ä¿çœŸåº¦ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šå‡ºç°è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚å­¦ä¹ åˆ°çš„èº«ä»½å¾€å¾€ä¼šä¸æ— å…³ä¿¡æ¯çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´æ–‡æœ¬å¯æ§æ€§ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨äººè„¸ä¸Šã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨è®­ç»ƒæˆ–æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¼šå¢åŠ é¢å¤–çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šè¿‡åº¦æ‹Ÿåˆå‚è€ƒå›¾åƒï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MasterWeaver çš„æµ‹è¯•æ—¶æ— è°ƒä¼˜æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰é«˜ä¿çœŸèº«ä»½å’Œå¯æ§æ–‡æœ¬çš„å›¾åƒã€‚MasterWeaver é€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥ä¸€ä¸ªèº«ä»½æ˜ å°„å™¨æ¥å®ç°ï¼Œè¯¥æ˜ å°„å™¨å°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆåœ¨ä¸€èµ·ã€‚èº«ä»½æ˜ å°„å™¨ç”±ä¸€ç³»åˆ—äº¤å‰æ³¨æ„å—ç»„æˆï¼Œè¿™äº›å—èƒ½å¤Ÿä»å‚è€ƒå›¾åƒä¸­æå–èº«ä»½ç‰¹å¾å¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾ç›¸ç»“åˆï¼Œä»è€ŒæŒ‡å¯¼ä¸ªæ€§åŒ–ç”Ÿæˆã€‚</p><p>(4): åœ¨äººè„¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMasterWeaver åœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMasterWeaver åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›ç»“æœæ”¯æŒäº† MasterWeaver åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºMasterWeaverï¼Œä¸€ç§æ— è°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥èº«ä»½æ˜ å°„å™¨ï¼Œå°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œå®ç°ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šèº«ä»½æ˜ å°„å™¨ç”±ä¸€ç³»åˆ—äº¤å‰æ³¨æ„å—ç»„æˆï¼Œèƒ½å¤Ÿä»å‚è€ƒå›¾åƒä¸­æå–èº«ä»½ç‰¹å¾å¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾ç›¸ç»“åˆï¼ŒæŒ‡å¯¼ä¸ªæ€§åŒ–ç”Ÿæˆï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºid-preserved editability learningï¼ŒåŒ…æ‹¬ç¼–è¾‘æ–¹å‘æŸå¤±å’Œäººè„¸å¢å¼ºæ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶ä¿æŒèº«ä»½ä¿çœŸåº¦ã€‚</p><p><strong>8. ç»“è®º</strong></p><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•MasterWeaverï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°ç”Ÿæˆå…·æœ‰çœŸå®èº«ä»½å’Œçµæ´»å¯ç¼–è¾‘æ€§çš„ä¸ªæ€§åŒ–å›¾åƒã€‚æå‡ºçš„ç¼–è¾‘æ–¹å‘æŸå¤±å’Œäººè„¸å¢å¼ºæ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¿çœŸåº¦ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MasterWeaverä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆä¸èº«ä»½å’Œæ–‡æœ¬éƒ½ç›¸ç¬¦çš„ç…§ç‰‡çº§çœŸå®å›¾åƒã€‚è¿™ç§èƒ½åŠ›ä½¿æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§åº”ç”¨ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ•°å­—å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯åˆ›ä½œã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„ç¼–è¾‘æ–¹å‘æŸå¤±æœ‰å¯èƒ½åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼ˆä¾‹å¦‚åŠ¨ç‰©å’Œç‰©ä½“ï¼‰ï¼Œä»è€Œå¢å¼ºå…¶é€‚ç”¨æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•MasterWeaverï¼Œé€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥èº«ä»½æ˜ å°„å™¨ï¼Œå°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œå®ç°ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚      æ€§èƒ½ï¼šåœ¨äººè„¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMasterWeaveråœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMasterWeaveråœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚      å·¥ä½œé‡ï¼šMasterWeaveræ˜¯ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒæˆ–æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå‡å°‘äº†é¢å¤–çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-684afedc1936b936aaccddf56634d091.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3def950180a75e286f4491e85a1510be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cf287f35f0fb1e5e93d0b9d65a46a49e.jpg" align="middle"></details>## Sequential Amodal Segmentation via Cumulative Occlusion Learning**Authors:Jiayang Ao, Qiuhong Ke, Krista A. Ehinger**To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines. [PDF](http://arxiv.org/abs/2405.05791v1) **Summary**åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œé’ˆå¯¹ä¸ç¡®å®šç±»åˆ«çš„ç‰©ä½“é¡ºåºæ— æ¨¡æ€åˆ†å‰²ã€‚**Key Takeaways**- æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰ç´¯ç§¯é®æŒ¡å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä¸ç¡®å®šç±»åˆ«çš„ç‰©ä½“é¡ºåºæ— æ¨¡æ€åˆ†å‰²ã€‚- è¯¥æ¨¡å‹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨ç´¯ç§¯æ©ç ç­–ç•¥è¿­ä»£ä¼˜åŒ–é¢„æµ‹ï¼Œæœ‰æ•ˆåœ°æ•æ‰ä¸å¯è§åŒºåŸŸçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å·§å¦™åœ°å†ç°è¢«é®æŒ¡ç‰©ä½“çš„å½¢çŠ¶å’Œé®æŒ¡é¡ºåºçš„å¤æ‚åˆ†å¸ƒã€‚- å®ƒç±»ä¼¼äºäººç±»çš„æ— æ¨¡æ€çŸ¥è§‰èƒ½åŠ›ï¼Œå³ç ´è¯‘ç‰©ä½“ä¹‹é—´çš„ç©ºé—´é¡ºåºï¼Œå¹¶å‡†ç¡®é¢„æµ‹å¯†é›†åˆ†å±‚è§†è§‰åœºæ™¯ä¸­è¢«é®æŒ¡ç‰©ä½“çš„å®Œæ•´è½®å»“ã€‚- åœ¨ä¸‰ä¸ªæ— æ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå·²æœ‰çš„åŸºçº¿ã€‚- è¯¥æ¨¡å‹å¯ä»¥å¤„ç†ä»»ä½•ç‰©ä½“ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ç»„æœ‰é™çš„ç‰©ä½“ç±»åˆ«ã€‚- è¯¥æ¨¡å‹å¯¹äºæœºå™¨äººåº”ç”¨å°¤å…¶æœ‰ç”¨ã€‚- æœ¬æ–‡çš„å·¥ä½œå¯¹è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: åŸºäºæ‰©æ•£æ¨¡å‹çš„é¡ºåºé®æŒ¡æ„ŸçŸ¥çš„æ— æ¨¡æ€åˆ†å‰²</p></li><li><p>Authors: Seunghyeok Back, Joosoon Lee, Taewon Kim, Sangjun Noh, Raeyoung Kang, Seongho Bak, Kyoobin Lee</p></li><li><p>Affiliation: éŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</p></li><li><p>Keywords: Amodal segmentation, Diffusion model, Occlusion perception, Computer vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.07993, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å¯¹äºç†è§£å¤æ‚è§†è§‰åœºæ™¯ï¼ˆå…¶ä¸­ç‰©ä½“ç»å¸¸è¢«é®æŒ¡ï¼‰è‡³å…³é‡è¦ã€‚</p><p>(2): ä¹‹å‰çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†æœªçŸ¥ç‰©ä½“ç±»åˆ«å’Œä»»æ„æ•°é‡çš„é®æŒ¡å±‚æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå¯ä»¥å®ç°é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ã€‚</p><p>(4): åœ¨ä¸‰ä¸ªå…¬å¼€çš„å¯ç”¨çš„æ— æ¨¡æ€æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨äº§ç”Ÿåˆç†å¤šæ ·åŒ–ç»“æœçš„åŒæ—¶ï¼Œä¼˜äºå…¶ä»–å±‚æ„ŸçŸ¥æ— æ¨¡æ€åˆ†å‰²å’Œæ‰©æ•£åˆ†å‰²æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå¯ä»¥å®ç°é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å¼•å…¥ç´¯ç§¯æ©ç ï¼Œå®ƒèåˆäº†å¯¹è±¡çš„ spatial structuresï¼Œä¿ƒè¿›äº†å¯¹å¯è§å’Œé®æŒ¡å¯¹è±¡éƒ¨åˆ†çš„ç†è§£ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè¯¥æ–¹æ³•é‡‡ç”¨ç´¯ç§¯å¼•å¯¼æ‰©æ•£ï¼Œæ‰©æ•£è¿‡ç¨‹ç”±è¾“å…¥å›¾åƒå’Œæ¥è‡ªå…ˆå‰å±‚çš„åŠ¨æ€æ›´æ–°çš„ç´¯ç§¯æ©ç æä¾›ä¿¡æ¯ï¼Œæ‰©æ•£ä»…æ‰°åŠ¨æ— æ¨¡æ€æ©ç ï¼Œä¿æŒå›¾åƒå’Œç›¸åº”ç´¯ç§¯æ©ç çš„ä¸Šä¸‹æ–‡å’Œ spatial integrity ä¸å˜ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šè¯¥æ–¹æ³•æå‡ºç´¯ç§¯é®æŒ¡å­¦ä¹ ç®—æ³•ï¼Œå®ƒé‡‡ç”¨åˆ†å±‚ç¨‹åºï¼Œä»¥æœ‰åºæ„ŸçŸ¥çš„æ–¹å¼é¢„æµ‹æ— æ¨¡æ€æ©ç ï¼Œå®ƒé€šè¿‡ç§¯ç´¯è§†è§‰ä¿¡æ¯æ¥æ“ä½œï¼Œå…¶ä¸­è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼ˆå…ˆå‰çš„åˆ†å‰²æ©ç ï¼‰çš„å†å²å½±å“å½“å‰æ•°æ®ï¼ˆè¦åˆ†å‰²çš„å½“å‰å¯¹è±¡ï¼‰çš„æ„ŸçŸ¥ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šè¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­åˆ©ç”¨ ground truth ç´¯ç§¯æ©ç ä½œä¸ºè¾“å…¥ï¼Œè€Œåœ¨æ¨ç†ä¸­ä½¿ç”¨å‰ä¸€å±‚é¢„æµ‹çš„æ©ç æ¥æ„å»ºç´¯ç§¯æ©ç ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå®ç°äº†é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ï¼Œå¯¹äºç†è§£å¤æ‚è§†è§‰åœºæ™¯è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ç´¯ç§¯æ©ç å’Œç´¯ç§¯å¼•å¯¼æ‰©æ•£ï¼Œä¿ƒè¿›äº†å¯¹å¯è§å’Œé®æŒ¡å¯¹è±¡éƒ¨åˆ†çš„ç†è§£ï¼Œå¹¶é‡‡ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ ç®—æ³•ï¼Œä»¥æœ‰åºæ„ŸçŸ¥çš„æ–¹å¼é¢„æµ‹æ— æ¨¡æ€æ©ç ï¼›æ€§èƒ½ï¼šåœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„æ— æ¨¡æ€æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å±‚æ„ŸçŸ¥æ— æ¨¡æ€åˆ†å‰²å’Œæ‰©æ•£åˆ†å‰²æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­åˆ©ç”¨ ground truth ç´¯ç§¯æ©ç ä½œä¸ºè¾“å…¥ï¼Œè€Œåœ¨æ¨ç†ä¸­ä½¿ç”¨å‰ä¸€å±‚é¢„æµ‹çš„æ©ç æ¥æ„å»ºç´¯ç§¯æ©ç ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-94d0f4cb7c590ef60771afc2db0e19f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-751b43417d9c46f9ccbe1b00ac7c3da2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-956b6f93209ac841263fddf5f8097796.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2a455d981e097468b0764d82f2edcafc.jpg" align="middle"></details>## LatentColorization: Latent Diffusion-Based Speaker Video Colorization**Authors:Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran**While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM. [PDF](http://arxiv.org/abs/2405.05707v1) **Summary**åˆ©ç”¨æ”¹è¿›çš„éšæ‰©æ•£æ¨¡å‹è§£å†³è§†é¢‘ç€è‰²ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ï¼Œå®ç°æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„å›¾åƒè´¨é‡å’Œç”¨æˆ·åå¥½ã€‚**Key Takeaways**- è§†é¢‘ç€è‰²é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚- ç°æœ‰è§†é¢‘ç€è‰²æŠ€æœ¯é€šå¸¸æŒ‰å¸§å¤„ç†ï¼Œå¿½ç•¥äº†æ—¶é—´ä¸€è‡´æ€§ã€‚- è¿™ä¼šå¯¼è‡´å¸§é—´é—ªçƒæˆ–çªç„¶çš„é¢œè‰²è¿‡æ¸¡ï¼Œå½±å“è´¨é‡ã€‚- ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„éšæ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚- è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ—¶é—´ä¸€è‡´æ€§æœºåˆ¶è§£å†³äº†æ—¶é—´ä¸ä¸€è‡´é—®é¢˜ã€‚- æ¨¡å‹åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸»è§‚ç ”ç©¶ä¸­å¾—åˆ°ç”¨æˆ·åå¥½ã€‚- ç ”ç©¶è€…ä½¿ç”¨ç”µè§†/ç”µå½±è§†é¢‘æ‰©å±•äº†æ•°æ®é›†ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚- æ¨¡å‹åœ°å€ï¼šhttps://youtu.be/vDbzsZdFuxM**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: æ½œè‰²åŒ–ï¼šåŸºäºæ½œåœ¨æ‰©æ•£çš„è¯´è¯è€…è§†é¢‘ç€è‰²</p></li><li><p>Authors: Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran</p></li><li><p>Affiliation: çˆ±å°”å…°é«˜å¨å¤§å­¦äººå·¥æ™ºèƒ½æ•°æ®ç§‘å­¦ç ”ç©¶æ‰€</p></li><li><p>Keywords: äººå·¥æ™ºèƒ½ï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼Œæœºå™¨å­¦ä¹ ï¼Œè®¡ç®—æœºè§†è§‰ï¼Œè§†é¢‘ç€è‰²ï¼Œæ½œåœ¨æ‰©æ•£ï¼Œå›¾åƒç€è‰²</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05707 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„ç€è‰²ä¸Šï¼Œè€ŒåŸºäºè§†é¢‘çš„ç€è‰²é¢†åŸŸä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚å¤§å¤šæ•°ç°æœ‰çš„è§†é¢‘ç€è‰²æŠ€æœ¯éƒ½æ˜¯é€å¸§è¿›è¡Œçš„ï¼Œå¸¸å¸¸å¿½ç•¥äº†è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§è¿™ä¸€å…³é”®æ–¹é¢ã€‚è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´å¸§ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ï¼Œä»è€Œå¯¼è‡´é—ªçƒæˆ–å¸§ä¹‹é—´çªç„¶çš„è‰²å½©è½¬æ¢ç­‰ä¸è‰¯æ•ˆæœã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šå¤§å¤šæ•°ç°æœ‰çš„è§†é¢‘ç€è‰²æŠ€æœ¯éƒ½æ˜¯é€å¸§è¿›è¡Œçš„ï¼Œå¸¸å¸¸å¿½ç•¥äº†è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§è¿™ä¸€å…³é”®æ–¹é¢ã€‚è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´å¸§ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ï¼Œä»è€Œå¯¼è‡´é—ªçƒæˆ–å¸§ä¹‹é—´çªç„¶çš„è‰²å½©è½¬æ¢ç­‰ä¸è‰¯æ•ˆæœã€‚é—®é¢˜ï¼šè¿™ç§æ–¹æ³•æ— æ³•ä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´è§†é¢‘ç€è‰²ç»“æœå‡ºç°é—ªçƒæˆ–çªç„¶çš„è‰²å½©è½¬æ¢ç­‰é—®é¢˜ã€‚åŠ¨æœºï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¹¶æé«˜è§†é¢‘ç€è‰²çš„å›¾åƒè´¨é‡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡ä¸å…¶ä»–ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒï¼Œåœ¨æ—¢å®šçš„å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šå±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†ä¸€é¡¹ä¸»è§‚ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ç”¨æˆ·æ›´å–œæ¬¢æœ¬æ–‡çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ¬æ–‡çš„æ•°æ®é›†åŒ…å«äº†ä¼ ç»Ÿæ•°æ®é›†å’Œæ¥è‡ªç”µè§†/ç”µå½±çš„è§†é¢‘çš„ç»„åˆã€‚ç®€è€Œè¨€ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„åŸºäºæ½œåœ¨æ‰©æ•£çš„ç€è‰²ç³»ç»Ÿå’Œæ—¶é—´ä¸€è‡´æ€§æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è§£å†³æ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜æ¥æé«˜è‡ªåŠ¨è§†é¢‘ç€è‰²çš„æ€§èƒ½ã€‚</p><p>(4): æœ¬æ–‡çš„æ–¹æ³•åœ¨è§†é¢‘ç€è‰²ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´ä¸€è‡´æ€§å¹¶æé«˜è§†é¢‘ç€è‰²å›¾åƒè´¨é‡çš„è§†é¢‘ç€è‰²æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>(1)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚</p><p>(2)ï¼šè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œè¯¥æœºåˆ¶é€šè¿‡å¯¹è¿ç»­å¸§ä¹‹é—´çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿äº†è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„é¢œè‰²è½¬æ¢å¹³æ»‘ä¸”ä¸€è‡´ã€‚</p><p>(3)ï¼šè¯¥æ–¹æ³•è¿˜åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒç€è‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæä¾›ä¸°å¯Œçš„é¢œè‰²ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†è§†é¢‘ç€è‰²çš„å›¾åƒè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶è¯æ˜äº†åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ LatentColorization æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ°´å¹³ç›¸å½“çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç³»ç»Ÿåœ¨â€œSherlock Holmes Movieâ€æ•°æ®é›†ä¸Šæ‰§è¡Œä¸äººç±»æ°´å¹³ç›¸å½“çš„ç€è‰²ï¼Œè¡¨æ˜å…¶å®é™…æ„ä¹‰å’Œç‰¹å®šåº”ç”¨è§†é¢‘ç€è‰²çš„æ½œåŠ›ã€‚ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¹¶ç»“åˆæ—¶é—´ä¸€è‡´çš„ç€è‰²æ–¹æ³•æœ‰åŠ©äºäº§ç”Ÿé€¼çœŸä¸”ä»¤äººä¿¡æœçš„ç€è‰²ç»“æœï¼Œä»è€Œä½¿è¯¥è¿‡ç¨‹æ›´å®¹æ˜“è·å–å¹¶å‡å°‘å¯¹ä¼ ç»Ÿäººå·¥ç€è‰²æ–¹æ³•çš„ä¾èµ–ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†å¯¹æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç€è‰²ä¸­çš„æ½œåŠ›çš„è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸè¿›ä¸€æ­¥å‘å±•æä¾›äº†æœºä¼šã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œç¡®ä¿äº†è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„é¢œè‰²è½¬æ¢å¹³æ»‘ä¸”ä¸€è‡´ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶å¼•å…¥æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f0bd89362865676c3ad7cf0d3f166a40.jpg" align="middle"><img src="https://pica.zhimg.com/v2-21100a3a0bd62ff092c190f5e11319a1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4524df6fd58a948107fed7f87b72c40d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-92e0eda0c525c56953a393c555231b1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44fa6c9863fd59510688ab85ad89e94c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-054ffb578d7f1a7808690325c49f8793.jpg" align="middle"></details>## Attention-Driven Training-Free Efficiency Enhancement of Diffusion   Models**Authors:Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu**Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io. [PDF](http://arxiv.org/abs/2405.05252v1) Accepted to IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2024**Summary**æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ³¨æ„åŠ›é©±åŠ¨çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹å¯ä»¥é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚**Key Takeaways**- å¼•å…¥ AT-EDM æ¡†æ¶ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å‰ªé™¤å†—ä½™æ ‡è®°ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚- å¼€å‘äº†å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (G-WPR) ç®—æ³•ï¼Œç”¨äºè¯†åˆ«å†—ä½™æ ‡è®°ã€‚- æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•ï¼Œç”¨äºæ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚- æå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥å‰ªæ (DSAP) æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥çš„å‰ªæé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚- ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAT-EDM åœ¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒä¸å®Œæ•´æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚- AT-EDM èŠ‚çœäº†çº¦ 38.8% çš„ FLOPsï¼Œä¸ Stable Diffusion XL ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ã€‚- AT-EDM é¡¹ç›®ç½‘é¡µï¼šhttps://atedm.github.ioã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒæ•ˆç‡å¢å¼ºæ‰©æ•£æ¨¡å‹</p></li><li><p>Authors: Yifan Liu, Yixing Xu, Zizhao Zhang, Zhihao Xia, Qinghe Xiao, Xiyang Dai, Xianglong Liu, Xiaoguang Han</p></li><li><p>Affiliation: ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</p></li><li><p>Keywords: Diffusion Models, Attention Pruning, Efficient Inference, Generative Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.00297, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ‰©æ•£æ¨¡å‹ (DM) åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§å“è¶Šçš„æ€§èƒ½æ˜¯ä»¥æ˜‚è´µçš„æ¶æ„è®¾è®¡ä¸ºä»£ä»·çš„ï¼Œç‰¹åˆ«æ˜¯ç”±äºé¢†å…ˆæ¨¡å‹ä¸­å¤§é‡ä½¿ç”¨çš„æ³¨æ„åŠ›æ¨¡å—ã€‚</p><p>(2): ç°æœ‰å·¥ä½œä¸»è¦é‡‡ç”¨å†è®­ç»ƒè¿‡ç¨‹æ¥æé«˜ DM æ•ˆç‡ã€‚è¿™æ˜¯è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¯æ‰©å±•æ€§ä¸å¼ºçš„ã€‚</p><p>(3): æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒé«˜æ•ˆæ‰©æ•£æ¨¡å‹ (AT-EDM) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å¯¹å†—ä½™æ ‡è®°è¿›è¡Œä¿®å‰ªï¼Œè€Œæ— éœ€ä»»ä½•å†è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå•å»å™ªæ­¥éª¤ä¿®å‰ªï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ’åç®—æ³•ï¼Œå³å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (GWPR)ï¼Œä»¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ª (DSAP) æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</p><p>(4): å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒAT-EDM åœ¨æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œæ¯” Stable Diffusion XL èŠ‚çœ 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜ 1.53 å€ï¼‰ï¼ŒåŒæ—¶ä¿æŒä¸å®Œæ•´æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒé«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼ˆAT-EDMï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å¯¹å†—ä½™æ ‡è®°è¿›è¡Œä¿®å‰ªï¼Œè€Œæ— éœ€ä»»ä½•å†è®­ç»ƒã€‚            (2): å…·ä½“æ¥è¯´ï¼Œå¯¹äºå•å»å™ªæ­¥éª¤ä¿®å‰ªï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ’åç®—æ³•ï¼Œå³å¹¿ä¹‰åŠ æƒé¡µé¢æ’åï¼ˆGWPRï¼‰ï¼Œä»¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚            (3): æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ªï¼ˆDSAPï¼‰æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† AT-EDMï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨è¿è¡Œæ—¶åŠ é€Ÿ DM çš„æ–°é¢–æ¡†æ¶ã€‚AT-EDM æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šå•å»å™ªæ­¥éª¤æ ‡è®°ä¿®å‰ªç®—æ³•å’Œè·¨æ­¥é•¿ä¿®å‰ªè°ƒåº¦ï¼ˆDSAPï¼‰ã€‚åœ¨å•å»å™ªæ­¥éª¤æ ‡è®°ä¿®å‰ªä¸­ï¼ŒAT-EDM åˆ©ç”¨é¢„è®­ç»ƒ DM ä¸­çš„æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«ä¸é‡è¦çš„æ ‡è®°å¹¶å¯¹å…¶è¿›è¡Œä¿®å‰ªï¼Œä»¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†ä½¿ä¿®å‰ªåçš„ç‰¹å¾å›¾ä¸åé¢çš„å·ç§¯å—å…¼å®¹ï¼ŒAT-EDM å†æ¬¡ä½¿ç”¨æ³¨æ„åŠ›å›¾æ¥æ­ç¤ºæ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†ç›¸ä¼¼çš„æ ‡è®°å¤åˆ¶åˆ°æ¢å¤è¢«ä¿®å‰ªçš„æ ‡è®°ã€‚DSAP è¿›ä¸€æ­¥æé«˜äº† AT-EDM çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å‘ç°è¿™æ ·çš„ä¿®å‰ªè®¡åˆ’ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚ ToMeã€‚å®éªŒç»“æœè¯æ˜äº† AT-EDM åœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ SD-XL ä¸Šï¼ŒAT-EDM èŠ‚çœäº† 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ï¼ŒåŒæ—¶è·å¾—äº†ä¸å…¨å°ºå¯¸æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è‡´è°¢ æœ¬å·¥ä½œå¾—åˆ°äº† Adobe å¤å­£å®ä¹ å’Œç¾å›½å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š (NSF) èµ æ¬¾å· CCF2203399 çš„éƒ¨åˆ†æ”¯æŒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº† AT-EDMï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨è¿è¡Œæ—¶åŠ é€Ÿ DM çš„æ–°é¢–æ¡†æ¶ï¼›æå‡ºäº†å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (GWPR) ç®—æ³•æ¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ï¼›æå‡ºäº†å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ª (DSAP) æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›åœ¨ SD-XL ä¸Šï¼ŒAT-EDM èŠ‚çœäº† 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ï¼ŒåŒæ—¶è·å¾—äº†ä¸å…¨å°ºå¯¸æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚å·¥ä½œé‡ï¼šæ— éœ€é‡æ–°è®­ç»ƒï¼Œåœ¨è¿è¡Œæ—¶è¿›è¡Œä¿®å‰ªï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6302d3af65dff156f4dfb4a4f61beb6d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fba65c3201705c21fc2eca18ff6f04d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8d2386a34dc82ffa216c8bf65b38b88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-acd6df588aee8826ba26459dc11db84e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-52944c1f09cf54389358c76e65089ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d0588ac58cf9e6ab928168c2e4ee2de.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-260d5649d487707bdcdd240fe08bbe3e.jpg" align="middle"></details>## Imagine Flash: Accelerating Emu Diffusion Models with Backward   Distillation**Authors:Jonas Kohler, Albert Pumarola, Edgar SchÃ¶nfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet**Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation. [PDF](http://arxiv.org/abs/2405.05224v1) **Summary**æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¡†æ¶ï¼Œä½†æ¨ç†æˆæœ¬æ˜‚è´µã€‚**Key Takeaways**- ç»“åˆåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£çš„ä¸‰æ­¥è’¸é¦æ¡†æ¶ã€‚- åå‘è’¸é¦é€šè¿‡åœ¨å­¦ç”Ÿè‡ªå·±çš„åå‘è½¨è¿¹ä¸Šæ ¡å‡†æ¥å‡è½»è®­ç»ƒæ¨ç†å·®å¼‚ã€‚- ç§»ä½é‡å»ºæŸå¤±æ ¹æ®å½“å‰æ—¶é—´æ­¥é•¿åŠ¨æ€è°ƒæ•´çŸ¥è¯†è½¬ç§»ã€‚- å™ªå£°æ ¡æ­£é€šè¿‡è§£å†³å™ªå£°é¢„æµ‹ä¸­çš„å¥‡ç‚¹æ¥å¢å¼ºæ ·æœ¬è´¨é‡ã€‚- åœ¨å®šé‡æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ä¸­ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚- ä½¿ç”¨ä»…ä¸‰ä¸ªå»å™ªæ­¥éª¤å³å¯å®ç°ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é«˜è´¨é‡ç”Ÿæˆã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>é¢˜ç›®ï¼šæƒ³è±¡é—ªå…‰ï¼šåŠ é€Ÿ Emu æ‰©æ•£</p></li><li><p>ä½œè€…ï¼šJonas Kohlerï¼ŒAlbert Pumarolaï¼ŒEdgar SchÃ¶nfeldï¼ŒArtsiom Sanakoyeuï¼ŒRoshan Sumbalyï¼ŒPeter Vajda å’Œ Ali Thabet</p></li><li><p>éš¶å±å…³ç³»ï¼šGenAIï¼ŒMeta</p></li><li><p>å…³é”®è¯ï¼šDiffusion Modelsï¼ŒDistillationï¼ŒImage Generationï¼ŒInference Acceleration</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.05224ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¡†æ¶ï¼Œä½†æ¨ç†æˆæœ¬æ˜‚è´µã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•é€šå¸¸ä¼šå½±å“å›¾åƒè´¨é‡ï¼Œæˆ–è€…åœ¨æä½æ­¥é•¿æ¡ä»¶ä¸‹è¿›è¡Œå¤æ‚æ¡ä»¶å¤„ç†æ—¶ä¼šå¤±è´¥ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬é‡åŒ–ã€çŸ¥è¯†è’¸é¦å’Œè®­ç»ƒ-æ¨ç†ä¸åŒ¹é…æ ¡æ­£ã€‚ä½†å®ƒä»¬åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶å®ç°æä½æ­¥é•¿æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚åœ¨ ImageNet-64 æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ 1 æ­¥æ—¶ FID ä¸º 5.57ï¼Œä½¿ç”¨ 3 æ­¥æ—¶ FID ä¸º 4.69ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåå‘è’¸é¦ï¼šä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„è¾“å…¥ï¼Œé€šè¿‡æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šç§»ä½é‡å»ºæŸå¤±ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é¼“åŠ±å­¦ç”Ÿæ¨¡å‹é‡å»ºæ•™å¸ˆæ¨¡å‹åœ¨ä¸åŒæ­¥é•¿ä¸‹çš„è¾“å‡ºã€‚</p><p>ï¼ˆ5ï¼‰ï¼šå™ªå£°æ ¡æ­£ï¼šåº”ç”¨äº†ä¸€ç§å™ªå£°æ ¡æ­£æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡æ·»åŠ å™ªå£°æ¥å¹³æ»‘å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† Imagine Flashï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜ä¿çœŸã€å°‘æ­¥å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ä»¥å‡å°‘è®­ç»ƒ-æ¨ç†å·®å¼‚ï¼Œä¸€ä¸ªåŠ¨æ€è°ƒæ•´æ¯ä¸ªæ—¶é—´æ­¥é•¿çŸ¥è¯†è½¬ç§»çš„ç§»ä½é‡å»ºæŸå¤±ï¼ˆSRLï¼‰ï¼Œä»¥åŠç”¨äºæé«˜å›¾åƒè´¨é‡çš„å™ªå£°æ ¡æ­£ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒImagine Flash å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä»…ä½¿ç”¨ä¸‰ä¸ªå»å™ªæ­¥éª¤å³å¯ä¸é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶å§‹ç»ˆè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚è¿™ç§å‰æ‰€æœªæœ‰çš„é‡‡æ ·æ•ˆç‡ä¸é«˜æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ç›¸ç»“åˆï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹éå¸¸é€‚åˆå®æ—¶ç”Ÿæˆåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¶…é«˜æ•ˆç”Ÿæˆå»ºæ¨¡é“ºå¹³äº†é“è·¯ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ‰©å±•åˆ°è§†é¢‘å’Œ 3D ç­‰å…¶ä»–æ¨¡æ€ï¼Œè¿›ä¸€æ­¥å‡å°‘é‡‡æ ·é¢„ç®—ï¼Œä»¥åŠå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸äº’è¡¥çš„åŠ é€ŸæŠ€æœ¯ç›¸ç»“åˆã€‚é€šè¿‡å¯ç”¨å³æ—¶é«˜ä¿çœŸç”Ÿæˆï¼ŒImagine Flash ä¸ºå®æ—¶åˆ›æ„å·¥ä½œæµå’Œäº’åŠ¨åª’ä½“ä½“éªŒå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚ï¼›æ€§èƒ½ï¼šåœ¨ ImageNet-64 æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ 1 æ­¥æ—¶ FID ä¸º 5.57ï¼Œä½¿ç”¨ 3 æ­¥æ—¶ FID ä¸º 4.69ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d5adaf43fae278fddba0258413307ece.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3e21e1e5b67cda3d8658d86e6854e63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c4cddde7364b59b3aef7c475c750db.jpg" align="middle"></details>## FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via   Diffusion Models**Authors:Jinglin Xu, Yijie Guo, Yuxin Peng**The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024. [PDF](http://arxiv.org/abs/2405.05216v1) Accepted by CVPR 2024**Summary**åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»†ç²’åº¦æç¤ºé©±åŠ¨çš„å»å™ªå™¨ï¼Œå®ç°äº†3Däººä½“å§¿æ€ä¼°è®¡çš„ç»†ç²’åº¦å¼•å¯¼ã€‚**Key Takeaways**- é€šè¿‡æ–‡æœ¬å’Œäººä½“çŸ¥è¯†ç”Ÿæˆç»†ç²’åº¦æç¤ºï¼Œæä¾›éšå¼ç›‘ç£ï¼Œå¢å¼º 3D HPEã€‚- å»ºç«‹æç¤ºå’Œå§¿åŠ¿ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œæé«˜å»å™ªè´¨é‡ã€‚- å¼•å…¥æ—¶é—´ä¿¡æ¯ï¼Œå®ç°å»å™ªè¿‡ç¨‹çš„è‡ªé€‚åº”è°ƒæ•´ã€‚- FinePOSE åœ¨å•äººå’Œå¤šäººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šå‡è¾¾åˆ° SOTA æ€§èƒ½ã€‚- ç»†ç²’åº¦æç¤ºæä¾›äº†å¯¹ä¸åŒèº«ä½“éƒ¨ä½çš„ç»†è‡´æŒ‡å¯¼ã€‚- æç¤ºé©±åŠ¨çš„å»å™ªå™¨ä½¿ 3D HPE æ›´å¥½åœ°åˆ©ç”¨æ–‡æœ¬çŸ¥è¯†ã€‚- FinePOSE æ‰©å±•åˆ°å¤šäººä½“å§¿æ€ä¼°è®¡ï¼Œå¢å¼ºäº†å¤æ‚åœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ç²¾ç»†æç¤ºé©±åŠ¨çš„æ‰©æ•£æ¨¡å‹åœ¨ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡ä¸­çš„åº”ç”¨</p></li><li><p>Authors: Yuxin Sun, Yajie Zhao, Yifan Zhang, Xiangyang Xue, Jian Cheng</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: 3D Human Pose Estimation, Diffusion Model, Prompt Learning, Fine-grained Guidance</p></li><li><p>Urls: https://arxiv.org/abs/2302.06039, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡ï¼ˆ3D HPEï¼‰ä»»åŠ¡åˆ©ç”¨äºŒç»´å›¾åƒæˆ–è§†é¢‘é¢„æµ‹ä¸‰ç»´ç©ºé—´ä¸­çš„äººä½“å…³èŠ‚åæ ‡ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬å¤§å¤šå¿½ç•¥äº†å°†å¯è®¿é—®æ–‡æœ¬å’Œäººç±»è‡ªç„¶å¯è¡Œçš„çŸ¥è¯†ç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œé”™å¤±äº†æœ‰ä»·å€¼çš„éšå¼ç›‘ç£æ¥æŒ‡å¯¼ 3D HPE ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„ç ”ç©¶é€šå¸¸ä»æ•´ä¸ªäººä½“çš„è§’åº¦ç ”ç©¶è¯¥ä»»åŠ¡ï¼Œå¿½ç•¥äº†éšè—åœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­çš„ç»†ç²’åº¦æŒ‡å¯¼ã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä»æ•´ä¸ªäººä½“çš„è§’åº¦ç ”ç©¶ 3D HPE ä»»åŠ¡ï¼Œå¿½ç•¥äº†éšè—åœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­çš„ç»†ç²’åº¦æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹å¤æ‚å§¿åŠ¿å’ŒåŠ¨ä½œå»ºæ¨¡çš„èƒ½åŠ›ã€‚</p><p>(3): ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ï¼Œç”¨äº 3D HPEï¼Œåä¸º FinePOSEã€‚å®ƒç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹ï¼šï¼ˆ1ï¼‰ç²¾ç»†éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå­¦ä¹ ï¼ˆFPPï¼‰æ¨¡å—é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆæ¥æ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ã€‚ï¼ˆ2ï¼‰ç²¾ç»†æç¤ºå§¿æ€é€šä¿¡ï¼ˆFPCï¼‰æ¨¡å—åœ¨å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´å»ºç«‹ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ã€‚ï¼ˆ3ï¼‰æç¤ºé©±åŠ¨çš„æ—¶åºé£æ ¼åŒ–ï¼ˆPTSï¼‰æ¨¡å—é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>(4): åœ¨å…¬å…±å•äººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFinePOSE ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°† FinePOSE æ‰©å±•åˆ°å¤šäººå§¿æ€ä¼°è®¡ã€‚åœ¨ EgoHumans æ•°æ®é›†ä¸Šå®ç° 34.3mm çš„å¹³å‡ MPJPEï¼Œè¯æ˜äº† FinePOSE å¤„ç†å¤æ‚å¤šäººåœºæ™¯çš„æ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šFinePOSE ç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šç²¾ç»†éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå­¦ä¹ ï¼ˆFPPï¼‰æ¨¡å—ã€ç²¾ç»†æç¤ºå§¿æ€é€šä¿¡ï¼ˆFPCï¼‰æ¨¡å—å’Œæç¤ºé©±åŠ¨çš„æ—¶åºé£æ ¼åŒ–ï¼ˆPTSï¼‰æ¨¡å—ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šFPP æ¨¡å—é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆæ¥æ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šFPC æ¨¡å—åœ¨å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´å»ºç«‹ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šPTS æ¨¡å—é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ã€‚FinePOSE é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆï¼Œæ„å»ºäº†ç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ã€‚æ­¤å¤–ï¼ŒFinePOSE å»ºç«‹äº†å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œå¹¶é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ã€‚FinePOSE åˆ©ç”¨å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ï¼Œæ„å»ºäº†ç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œå¹¶å»ºç«‹äº†å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ã€‚æ­¤å¤–ï¼ŒFinePOSE é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å…¬å…±å•äººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFinePOSE ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°† FinePOSE æ‰©å±•åˆ°å¤šäººå§¿æ€ä¼°è®¡ã€‚åœ¨ EgoHumans æ•°æ®é›†ä¸Šå®ç° 34.3mm çš„å¹³å‡ MPJPEï¼Œè¯æ˜äº† FinePOSE å¤„ç†å¤æ‚å¤šäººåœºæ™¯çš„æ½œåŠ›ã€‚</p><p>å·¥ä½œé‡ï¼šFinePOSE çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚ç„¶è€Œï¼Œæ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå»ºç«‹å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-30db10978f08bca4adc049e2f667efa7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58a38ed55c5b15686ce6cec8b0354b7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e88a5a9d36fae50e0d57909271d5070e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186dcaee258d539e4f830d471e6a2c6e.jpg" align="middle"></details>## Fast LiDAR Upsampling using Conditional Diffusion Models**Authors:Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim TÃ¸rresen, Ryo Kurazume**The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments. [PDF](http://arxiv.org/abs/2405.04889v1) **æ‘˜è¦**æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”¨äºä¸‰ç»´åœºæ™¯ç‚¹äº‘çš„é«˜æ•ˆä¸”é«˜è´¨é‡ç¨€ç–åˆ°ç¨ å¯†ä¸Šé‡‡æ ·ã€‚**è¦ç‚¹*** æ‰©æ•£æ¨¡å‹å¯ç”¨äºé«˜ä¿çœŸåœ°ç”Ÿæˆç²¾ç‚¼çš„æ¿€å…‰é›·è¾¾æ•°æ®ã€‚* ç°æœ‰æ–¹æ³•å—é™äºæ€§èƒ½å’Œé€Ÿåº¦ï¼Œéš¾ä»¥å®æ—¶æ‰§è¡Œã€‚* æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºé€šè¿‡å›¾åƒè¡¨ç¤ºå¿«é€Ÿã€é«˜è´¨é‡åœ°å¯¹ä¸‰ç»´åœºæ™¯ç‚¹äº‘è¿›è¡Œç¨€ç–åˆ°ç¨ å¯†ä¸Šé‡‡æ ·ã€‚* è¯¥æ–¹æ³•é‡‡ç”¨ä½¿ç”¨æ¡ä»¶å†…æ’æ©ç è®­ç»ƒçš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒå®Œæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚* å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ã€‚* è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡åŒæ—¶åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ¥å±•ç¤ºæ³›åŒ–èƒ½åŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title:ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œå¿«é€Ÿæ¿€å…‰é›·è¾¾ä¸Šé‡‡æ ·</li><li>Authors: Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim TÃ¸rresen, Ryo Kurazume</li><li>Affiliation: å¥¥æ–¯é™†å¤§å­¦ä¿¡æ¯å­¦ç³»</li><li>Keywords: 3D LiDAR, Conditional Diffusion Models, Image-based LiDAR data generation, Deep generative models</li><li>Urls: Paper: https://arxiv.org/abs/2405.04889v1, Github: None</li><li>Summary:</li></ol><p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼Œç”±äºç¡¬ä»¶é™åˆ¶å’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„æˆæœ¬ï¼Œæµ‹é‡æ•°æ®çš„è´¨é‡å’Œå¯†åº¦å·®å¼‚å¾ˆå¤§ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ç­‰æŠ€æœ¯æ€§èƒ½ä¸ä¸€è‡´ï¼Œè¿™å¯¹æ“ä½œæœºå™¨äººæ¥è¯´ä¸æ˜¯æœ€ä¼˜çš„ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è§£å†³ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œä½†è¿™äº›æ–¹æ³•æ¶‰åŠå¤æ‚çš„è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´æ…¢ï¼Œä¸é€‚åˆå®æ—¶æœºå™¨äººå¯¼èˆªç®¡é“ã€‚</p><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯åœ¨å›¾åƒè¡¨ç¤ºä¸­å»ºç«‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ç”Ÿæˆã€‚</p><p>(4):æœ¬æ–‡æ–¹æ³•åœ¨KITTI-360æ•°æ®é›†ä¸Šä½¿ç”¨ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œåœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•é€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ï¼Œå¼•å…¥è´¨é‡å’Œç¯å¢ƒçš„å˜åŒ–ï¼Œä»è€Œå…·æœ‰æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåœ¨å›¾åƒè¡¨ç¤ºä¸­å»ºç«‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ç”Ÿæˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä½¿ç”¨KITTI-360æ•°æ®é›†ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ï¼Œå¼•å…¥è´¨é‡å’Œç¯å¢ƒçš„å˜åŒ–ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>           (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒè¡¨ç¤ºçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ï¼Œä¸ºæé«˜æ¿€å…‰é›·è¾¾æ•°æ®è´¨é‡å’Œå¯†åº¦æä¾›äº†æ–°çš„æ–¹æ³•ï¼›           (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒè¡¨ç¤ºçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ï¼Œå¹¶é€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨KITTI-360æ•°æ®é›†ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•æ¶‰åŠå¤æ‚çš„è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´æ…¢ï¼Œä¸é€‚åˆå®æ—¶æœºå™¨äººå¯¼èˆªç®¡é“ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-767dd6ead611c7fe6a1bf019a995a405.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c74edffd87cfd5517cc664cb78371d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21eb96ed158007c33f9b04a7adf7ab5c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-82f76f778074704994e29c06d305ad3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-948663529c880472c0969ced23400b05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e781edb01423cfd0018bde5e4738157d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-614fee94f5b68257335000a86b6b2590.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/</id>
    <published>2024-05-06T10:42:27.000Z</published>
    <updated>2024-05-06T10:42:27.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights"><a href="#WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights" class="headerlink" title="WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights"></a>WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights</h2><p><strong>Authors:Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim</strong></p><p>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2405.02066v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) æ°´å°å¯åŒæ—¶é€‚ç”¨äºéšå¼å’Œæ˜¾å¼ NeRF è¡¨ç¤ºï¼Œä»¥ä¿è¯ NeRF çš„ç‰ˆæƒä¿æŠ¤ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF æ°´å°æ˜¯ä¿æŠ¤ NeRF ç‰ˆæƒçš„å…³é”®è§£å†³æ–¹æ¡ˆã€‚</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºéšå¼å’Œæ˜¾å¼ NeRF è¡¨ç¤ºã€‚</li><li>è¯¥æ–¹æ³•ä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢è¿›è¡Œæ°´å°ã€‚</li><li>è¯¥æ–¹æ³•é‡‡ç”¨å»¶è¿Ÿåå‘ä¼ æ’­ï¼Œæé«˜æ¸²æŸ“è´¨é‡å’Œæ¯”ç‰¹ç²¾åº¦ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å®¹é‡ã€ä¸å¯è§æ€§å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>è¯¥æ–¹æ³•æ¯”ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚</li><li>è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ NeRF åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­åµŒå…¥äºŒè¿›åˆ¶ä¿¡æ¯æ¥å®ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šWateRFï¼šç”¨äºç‰ˆæƒä¿æŠ¤çš„è¾å°„åœºä¸­çš„é²æ£’æ°´å°</p></li><li><p>ä½œè€…ï¼šYoungdong Jangã€Dong In Leeã€MinHyuk Jangã€Jong Wook Kimã€Feng Yangã€Sangpil Kim</p></li><li><p>éš¶å±æœºæ„ï¼šéŸ©å›½å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ°´å°ã€ç‰ˆæƒä¿æŠ¤ã€éšå¼è¡¨ç¤ºã€æ˜¾å¼è¡¨ç¤º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://kuai-lab.github.io/cvpr2024waterf/ï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ 3D å†…å®¹åˆ›å»ºå’Œ 3D å»ºæ¨¡ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†ä¿æŠ¤å…¶ç‰ˆæƒå°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚NeRF æ°´å°è¢«è®¤ä¸ºæ˜¯å®‰å…¨éƒ¨ç½²åŸºäº NeRF çš„ 3D è¡¨ç¤ºçš„å…³é”®è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä»…é€‚ç”¨äºéšå¼æˆ–æ˜¾å¼ NeRF è¡¨ç¤ºã€‚å®ƒä»¬çš„é—®é¢˜åœ¨äºæ— æ³•åŒæ—¶åº”ç”¨äºä¸¤ç§è¡¨ç¤ºã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ°´å°æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äº NeRF çš„ä¸¤ç§è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ NeRF åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­åµŒå…¥äºŒè¿›åˆ¶æ¶ˆæ¯æ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ NeRF ç©ºé—´ä¸­çš„ç¦»æ•£å°æ³¢å˜æ¢è¿›è¡Œæ°´å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å»¶è¿Ÿåå‘ä¼ æ’­æŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¸é€å—æŸå¤±ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥åœ¨æœ€å°æƒè¡¡ä¸‹æé«˜æ¸²æŸ“è´¨é‡å’Œæ¯”ç‰¹å‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒæ–¹é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šå®¹é‡ã€ä¸å¯è§æ€§å’ŒåµŒå…¥åœ¨ 2D æ¸²æŸ“å›¾åƒä¸­çš„æ°´å°çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»¥æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»è€Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>Methods:</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§å¾®è°ƒ NeRF çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸æ¶‰åŠæ”¹å˜æ¨¡å‹çš„æ¶æ„æ¥åµŒå…¥æ°´å°æ¶ˆæ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨å°†æ°´å°åµŒå…¥åˆ° NeRF æ¨¡å‹çš„æƒé‡ Î¸ ä¸­ï¼Œåœ¨æ¸²æŸ“å›¾åƒçš„é¢‘åŸŸä¸­ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¸åŒäºä¼ ç»Ÿçš„æ•°å­—æ°´å°æ–¹æ³•ï¼Œå®ƒä¸“æ³¨äºè®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨ã€‚ä¸åŒä¹‹å¤„åœ¨äºå¾®è°ƒè¿‡ç¨‹ï¼Œå®ƒåœ¨ä¸ä½¿ç”¨ç¼–ç å™¨çš„æƒ…å†µä¸‹åµŒå…¥æ°´å°ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ‰ 2 ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒæ°´å°è§£ç å™¨ Dï¼Œï¼ˆ2ï¼‰å¾®è°ƒ NeRF æ¨¡å‹ FÎ¸ ä»¥åµŒå…¥æ¶ˆæ¯ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•å¦‚å›¾ 2 æ‰€ç¤ºï¼Œå¹¶åœ¨ä¸‹æ–‡è¯¦ç»†æè¿°ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé¢„è®­ç»ƒæ°´å°è§£ç å™¨ï¼šæˆ‘ä»¬é€‰æ‹© HiDDeN [58] æ¶æ„ä½œä¸ºæˆ‘ä»¬çš„æ°´å°è§£ç å™¨ã€‚HiDDeN åŒ…å«ä¸¤ä¸ªç”¨äºæ•°æ®éšè—çš„å·ç§¯ç½‘ç»œï¼šæ°´å°ç¼–ç å™¨ E å’Œæ°´å°è§£ç å™¨ Dã€‚ä¸ºäº†é²æ£’æ€§ï¼Œå®ƒåŒ…æ‹¬ä¸€ä¸ªå™ªå£°å±‚ Nã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªä»…å…³æ³¨è§£ç å™¨æ€§èƒ½çš„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ’é™¤äº†è´Ÿè´£æé«˜è§†è§‰è´¨é‡çš„å¯¹å¶æŸå¤±ã€‚åœ¨è®­ç»ƒå®Œ HiDDeN æ¨¡å‹åï¼Œæ°´å°ç¼–ç å™¨ E åœ¨ç¬¬äºŒé˜¶æ®µæ²¡æœ‰è¢«ä½¿ç”¨ã€‚</p><p>ï¼ˆ7ï¼‰ï¼šç¼–ç å™¨ E ä»¥å°é¢å›¾åƒ Io âˆˆ RHÃ—W Ã—3 å’Œé•¿åº¦ä¸º L çš„äºŒè¿›åˆ¶æ¶ˆæ¯ M âˆˆ {0, 1}L ä¸ºè¾“å…¥ã€‚ç„¶å E å°† M åµŒå…¥åˆ° Io ä¸­å¹¶ç”Ÿæˆç¼–ç å›¾åƒ Iwã€‚ä¸ºäº†ä½¿è§£ç å™¨å¯¹æ—‹è½¬å’Œ JPEG å‹ç¼©ç­‰å„ç§å¤±çœŸå…·æœ‰é²æ£’æ€§ï¼ŒIw ä½¿ç”¨å™ªå£°å±‚ N è¿›è¡Œè½¬æ¢ã€‚ç”±å¤šä¸ªå·ç§¯å±‚ç»„æˆçš„è§£ç å™¨ D ä»¥ Iw ä¸ºè¾“å…¥ï¼Œå¹¶æå–æ¶ˆæ¯ Mâ€²ã€‚</p><p>ï¼ˆ8ï¼‰ï¼šMâ€² = D(N(Iw))</p><p>ï¼ˆ9ï¼‰ï¼šæˆ‘ä»¬åˆ©ç”¨ sigmoid å‡½æ•°å°†æå–çš„æ¶ˆæ¯ Mâ€² çš„èŒƒå›´è®¾ç½®ä¸º [0, 1]ã€‚æ¶ˆæ¯æŸå¤±ä½¿ç”¨ ML å’Œ sigmoid sg(Mâ€²L) ä¹‹é—´çš„äºŒå…ƒäº¤å‰ç†µ (BCE) è®¡ç®—ã€‚</p><p>ï¼ˆ10ï¼‰ï¼šLmessage = âˆ’ Lâˆ‘i=1 Mi Â· log sg(Mâ€²i) + (1 âˆ’ Mi) Â· log(1 âˆ’ sg(Mâ€²i)))</p><p>ï¼ˆ11ï¼‰ï¼šè§£ç å™¨ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥æ£€æµ‹ç»è¿‡è®­ç»ƒç¼–ç å™¨å¤„ç†çš„å›¾åƒä¸­çš„æ°´å°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨ç¬¬äºŒé˜¶æ®µä¸ä½¿ç”¨ç¼–ç å™¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“è§£ç å™¨æ¥æ”¶åˆ°é¦™è‰æ¸²æŸ“çš„å›¾åƒæ—¶ï¼Œæå–çš„æ¶ˆæ¯ä½ä¹‹é—´å­˜åœ¨åå·®ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒè§£ç å™¨åï¼Œæˆ‘ä»¬å¯¹çº¿æ€§è§£ç å™¨å±‚è¿›è¡Œ PCA ç™½åŒ–ä»¥æ¶ˆé™¤åå·®ï¼ŒåŒæ—¶ä¸é™ä½æå–èƒ½åŠ›ã€‚</p><p>ï¼ˆ12ï¼‰ï¼šåœ¨ DWT ä¸ŠåµŒå…¥å’Œæå–æ°´å°ï¼šåœ¨ç©ºé—´åŸŸä¸­åŠ æ°´å°æ˜¯ä¸€ç§ç›¸å¯¹ç®€å•çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåœ¨æ•´ä¸ªå›¾åƒä¸­åµŒå…¥æ°´å°ã€‚æœ€è¿‘ï¼Œä¸€ç§åœ¨ç©ºé—´åŸŸä¸­å¯¹ NeRF è¿›è¡Œå¾®è°ƒçš„æ°´å°æ–¹æ³• [16] æµ®å‡ºæ°´é¢ã€‚å°½ç®¡åœ¨ç©ºé—´åŸŸä¸­åµŒå…¥æ¶ˆæ¯çš„å¾®è°ƒæ–¹æ³•æ˜¾ç¤ºå‡ºæ— ä¸ä¼¦æ¯”çš„ä¸å¯è§æ€§å’Œæ¶ˆæ¯æå–èƒ½åŠ›ï¼Œä½†å®ƒå®¹æ˜“å—åˆ°æ‰­æ›²ç©ºé—´åŸŸçš„æ”»å‡»ï¼Œä¾‹å¦‚è£å‰ªã€‚ç›´æ¥åº”ç”¨æ¥è‡ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ [7] çš„ç©ºé—´åŸŸæŠ€æœ¯ä¸å…è®¸æœ‰æ•ˆè°ƒæ•´ NeRF çš„æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨é¢‘åŸŸè€Œä¸æ˜¯ç©ºé—´åŸŸä¸­çš„å¾®è°ƒæ–¹æ³•ã€‚å¤šå¹´æ¥ï¼Œå„ç§å›¾åƒæ°´å°æŠ€æœ¯ä½¿ç”¨é¢‘åŸŸï¼ŒåŒ…æ‹¬ç¦»æ•£ä½™å¼¦å˜æ¢ (DCT) å’Œç¦»æ•£å°æ³¢å˜æ¢ (DWT)ï¼Œå–å¾—äº†æŒç»­çš„å‘å±•å’Œæ”¹è¿›ã€‚æˆ‘ä»¬å‘ç° DWT æ˜¯å°†æ¶ˆæ¯ç¼–ç åˆ° NeRF æ¨¡å‹æƒé‡ä¸­çš„åˆé€‚åŸŸã€‚ç»™å®šç›¸åº”çš„ç›¸æœºå‚æ•°ï¼ŒNeRF æ¨¡å‹æ¸²æŸ“ 3D æ¨¡å‹çš„ä¸åŒè§†å›¾ã€‚æˆ‘ä»¬å°†æ¸²æŸ“å›¾åƒçš„åƒç´ ï¼Œè¡¨ç¤ºä¸º X = (xc, yc) âˆˆ RHÃ—W Ã—3ï¼Œè½¬æ¢ä¸ºå°æ³¢å½¢å¼ï¼Œå…¶ä¸­ c è¡¨ç¤ºé€šé“ã€‚DWT å®šä¹‰ä¸º [10]ï¼š</p><p>ï¼ˆ13ï¼‰ï¼šWÏ†(j0, m, n) = 1âˆšMNâˆ‘Mâˆ’1xc=0âˆ‘Nâˆ’1yc=0 f(xc, yc)Ï†j0,m,n(xc, yc),</p><p>ï¼ˆ14ï¼‰ï¼šW i Ïˆ(j, m, n) = 1âˆšMNâˆ‘Mâˆ’1xc=0âˆ‘Nâˆ’1yc=0 f(xc, yc)Ïˆi j,m,n(xc, yc)</p><p>ï¼ˆ15ï¼‰ï¼šå…¶ä¸­ Ï†(x, y) æ˜¯å°ºåº¦å‡½æ•°ï¼ŒÏˆ(x, y) æ˜¯å°æ³¢å‡½æ•°ã€‚WÏ†(j0, m, n) è¢« LL å­å¸¦è°ƒç”¨ï¼Œå®ƒæ˜¯å›¾åƒåœ¨å°ºåº¦ j0 çš„è¿‘ä¼¼å€¼ï¼ŒW i Ïˆ å…¶ä¸­ i = {H, V, D} åˆ†åˆ«è¡¨ç¤º LHã€HLã€HH å­å¸¦ã€‚å…ˆå‰çš„ç ”ç©¶é€‰æ‹© LHã€HL å’Œ HH å­å¸¦æ¥åµŒå…¥æ°´å°ï¼Œå› ä¸º LL å­å¸¦åŒ…å«å›¾åƒçš„é‡è¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€‰æ‹© LL å­å¸¦ä½œä¸ºè§£ç å™¨ D çš„è¾“å…¥ï¼Œå¹¶ç”¨ Mâ€² = D(WÏ†) è·å–æå–çš„æ¶ˆæ¯ã€‚ä½¿ç”¨ HiDDeN è§£ç å™¨ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œåœ¨ LL å­å¸¦ä¸­åµŒå…¥æ°´å°æ¯”å…¶ä»–å­å¸¦æ›´ç¨³å¥ï¼Œå¹¶ä¸”æ›´æœ‰æ•ˆåœ°åµŒå…¥æ°´å°ä¿¡æ¯ã€‚DWT çš„ç‰¹ç‚¹æ˜¯å…¶å­å¸¦åœ¨ä¸åŒçº§åˆ«ä¸Šè®¡ç®—ï¼›å› æ­¤ï¼Œä¸ºæˆ‘ä»¬çš„ç›®çš„é€‰æ‹©ä¸€ä¸ªæœ€ä½³çº§åˆ«æ˜¯å¿…è¦çš„ã€‚1 çº§å°†å›¾åƒåˆ†æˆ 4 ä¸ªå­å¸¦ (LL1, LH1, HL1, HH1)ï¼Œ</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•å°†å›¾åƒä»ç©ºé—´åŸŸè½¬æ¢åˆ°é¢‘åŸŸï¼Œæœ‰æ•ˆåœ°å°†æ°´å°ç¼–ç åˆ°å›¾åƒä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å˜æ¢å’Œé€å—æŸå¤±å¯ä»¥æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç¥ç» 3D æ°´å°æ–¹æ³•ï¼Œç”¨äº NeRF æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«è®­ç»ƒ 2D æ°´å°è§£ç å™¨å’Œ NeRF æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æµæ°´çº¿åªéœ€è¦è®­ç»ƒä¸€æ¬¡è§£ç å™¨ï¼Œå¹¶åœ¨ä¸åŒçš„ NeRF æ°´å°æ¨¡å‹ä¸Šé‡å¤ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬é‡‡ç”¨å›¾åƒæ°´å°ä¸­çš„ä¼ ç»Ÿæ°´å°æŠ€æœ¯ï¼Œå°†å›¾åƒä»ç©ºé—´åŸŸè½¬æ¢åˆ°é¢‘åŸŸï¼Œä»¥æœ‰æ•ˆåœ°å°†æ°´å°ç¼–ç åˆ°å›¾åƒä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å˜æ¢å’Œé€å—æŸå¤±å¯ä»¥æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</p><p>æ€§èƒ½ï¼šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»è€Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒå’ŒåµŒå…¥æ°´å°æ–¹é¢å…·æœ‰è¾ƒä½çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶é€‚ç”¨äºå®é™…åº”ç”¨ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-21a78eb3599c5468a4ea257df96b8cdc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59110a6f2727d6c4ae7b744d2459165a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-59cf18deef7514767b02ec7654c8da33.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9d7125c0e81d7d4fa4f485a0ca63c94.jpg" align="middle"></details><h2 id="Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy"><a href="#Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy" class="headerlink" title="Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy"></a>Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy</h2><p><strong>Authors:Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</strong></p><p>Action recognition has become one of the popular research topics in computer vision. There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances. However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction. In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy. Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets. Therefore, the contributions in this work are three-fold. Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition. Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy. Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields. Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400. </p><p><a href="http://arxiv.org/abs/2405.01337v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºå¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§å’Œç¥ç»è¾å°„åœºï¼Œæå‡ºæ—¶ç©ºä¸€è‡´åŠ¨ä½œè¯†åˆ«æ–°æ–¹æ³•ï¼Œå®ç°åŠ¨ä½œè¯†åˆ«é¢†åŸŸæœ€ä¼˜ç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§è§£å†³åŠ¨ä½œè¯†åˆ«åˆç†é¢„æµ‹é—®é¢˜ã€‚</li><li>å®šä¹‰åŸºäºæœ‰å‘æ ¼ç½—è«å¤«-ç“¦ç‘Ÿæ–¯å¦è·ç¦»çš„å¤šè§†å›¾ä¸€è‡´æ³¨æ„åŠ›åº¦é‡ã€‚</li><li>åŸºäºè§†é¢‘å˜å½¢é‡‘åˆšå’Œç¥ç»è¾å°„åœºæ„å»ºåŠ¨ä½œè¯†åˆ«æ¨¡å‹ã€‚</li><li>åœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜ç»“æœã€‚</li><li>åˆ›æ–°æ€§åœ°å¼•å…¥äº†å¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§ï¼Œè§£å†³äº†åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„éš¾é¢˜ã€‚</li><li>é‡‡ç”¨æ–°é¢–çš„åº¦é‡æ–¹æ³•è¯„ä¼°å¤šè§†å›¾ä¸€è‡´æ³¨æ„åŠ›ã€‚</li><li>å°†ç¥ç»è¾å°„åœºåº”ç”¨äºåŠ¨ä½œè¯†åˆ«ï¼Œæå‡äº†æ¨¡å‹åœ¨å•è§†å›¾æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šå¤šè§†è§’åŠ¨ä½œè¯†åˆ«ç»ç”±å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚</p></li><li><p>ä½œè€…ï¼šHoang-Quan Nguyenï¼ŒThanh-Dat Truongï¼ŒKhoa Luu</p></li><li><p>å•ä½ï¼šé˜¿è‚¯è‰²å¤§å­¦è®¡ç®—æœºè§†è§‰ä¸å›¾åƒç†è§£å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šåŠ¨ä½œè¯†åˆ«ï¼Œå¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§ï¼Œå®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ï¼Œç¥ç»è¾å°„åœº</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01337</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šåŠ¨ä½œè¯†åˆ«æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ï¼Œç°æœ‰çš„åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚ Transformerï¼‰çš„æ–¹æ³•åœ¨è§£å†³åŠ¨ä½œè¯†åˆ«ä»»åŠ¡çš„æ—¶ç©ºç»´åº¦é—®é¢˜ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹å…³æ³¨çš„åŠ¨ä½œä¸»ä½“æ­£ç¡®æ€§çš„ä¿è¯ï¼Œå³å¦‚ä½•ç¡®ä¿åŠ¨ä½œè¯†åˆ«æ¨¡å‹å…³æ³¨é€‚å½“çš„åŠ¨ä½œä¸»ä½“ä»¥åšå‡ºåˆç†çš„åŠ¨ä½œé¢„æµ‹ã€‚</p><p>(2)ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†ç¼ºä¹å¯¹æ¨¡å‹å…³æ³¨çš„åŠ¨ä½œä¸»ä½“æ­£ç¡®æ€§çš„ä¿è¯ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„é—®é¢˜ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œåˆ©ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—åŠ¨ä½œè§†é¢‘ä¸¤ä¸ªä¸åŒè§†è§’çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åº”ç”¨ç¥ç»è¾å°„åœºçš„æ€æƒ³ï¼Œåœ¨å•è§†è§’æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶éšå¼æ¸²æŸ“æ–°è§†è§’çš„ç‰¹å¾ã€‚</p><p>(4)ï¼šè¯¥æ–¹æ³•åœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>Methods:</li></ol><p>(1):ä½¿ç”¨ Video Transformer æ¡†æ¶è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ï¼Œå°†è§†é¢‘åˆ†è§£ä¸º patches å¹¶è¿›è¡Œä½ç½®ç¼–ç ï¼Œç„¶åä½¿ç”¨ Transformer ç¼–ç å™¨æå–ç‰¹å¾ï¼›</p><p>(2):é‡‡ç”¨ Neural Radiance Field çš„æ€æƒ³ï¼Œé€šè¿‡ StyleNeRF å°†ç‰¹å¾ä½“æ˜ å°„åˆ°é£æ ¼å‘é‡ï¼Œå¹¶è°ƒèŠ‚ NeRF æ¨¡å—ä¸­ MLP å±‚çš„æƒé‡çŸ©é˜µï¼Œä»¥åœ¨æ–°çš„è§†è§’ä¸‹æ¸²æŸ“ä½åˆ†è¾¨ç‡ç‰¹å¾ä½“ï¼›</p><p>(3):ä½¿ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ï¼ˆDirected Gromov-Wasserstein Discrepancyï¼‰è®¡ç®—ä¸åŒè§†è§’ä¸‹åŠ¨ä½œè§†é¢‘çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ä¸¤ä¸ªç©ºé—´å†…å®šä¹‰çš„åº¦é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æ¯”è¾ƒåˆ†å¸ƒï¼Œå¯¹ç›¸æœºå¹³ç§»å¼•èµ·çš„æ³¨æ„åŠ›å›¾è½¬æ¢å…·æœ‰é²æ£’æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>(1) æœ¬æ–‡æå‡ºçš„å¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—ä¸åŒè§†è§’ä¸‹åŠ¨ä½œè§†é¢‘çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ï¼Œè§£å†³äº†åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„é—®é¢˜ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p><p>(2) åˆ›æ–°ç‚¹ï¼šæå‡ºå¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œåˆ©ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—æ³¨æ„åŠ›ç›¸ä¼¼æ€§ï¼›æ€§èƒ½ï¼šåœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šå–å¾—æœ€å…ˆè¿›çš„ç»“æœï¼›å·¥ä½œé‡ï¼šéœ€è¦è®­ç»ƒ Neural Radiance Field æ¨¡å—ï¼Œè®¡ç®—å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-c677262cde72d554d4ab784234b1941b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59ee15896ae28d3e32188fedbfd5bc0d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  WateRF Robust Watermarks in Radiance Fields for Protection of   Copyrights</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/</id>
    <published>2024-05-06T10:35:16.000Z</published>
    <updated>2024-05-06T10:35:16.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/</id>
    <published>2024-05-06T10:33:19.000Z</published>
    <updated>2024-05-06T10:33:19.431Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations"><a href="#CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations" class="headerlink" title="CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations"></a>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</h2><p><strong>Authors:Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</strong></p><p>Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at <a href="https://aka.ms/covomix">https://aka.ms/covomix</a>. </p><p><a href="http://arxiv.org/abs/2404.06690v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨CoVoMixï¼Œä¸€ç§é›¶æ ·æœ¬å¯¹è¯è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸã€è¿è´¯ä¸”å¤šå›åˆçš„å¤šäººå¯¹è¯è¯­éŸ³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CoVoMixæ˜¯ä¸€æ¬¾é›¶æ ·æœ¬å¯¹è¯è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li><li>CoVoMixèƒ½å¤Ÿå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºç¦»æ•£æ ‡è®°æµï¼Œè¡¨ç¤ºå„ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>æµåŒ¹é…å£°å­¦æ¨¡å‹å°†æ ‡è®°æµè½¬æ¢æˆæ··åˆçš„æ¢…å°”é¢‘è°±å›¾ã€‚</li><li>HiFi-GANæ¨¡å‹å°†æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºè¯­éŸ³æ³¢å½¢ã€‚</li><li>CoVoMixé‡‡ç”¨äº†ä¸€å¥—ç»¼åˆçš„æŒ‡æ ‡æ¥è¡¡é‡å¯¹è¯å»ºæ¨¡å’Œç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMixç”Ÿæˆçš„å¯¹è¯å…·æœ‰é€¼çœŸçš„è‡ªç„¶æ€§å’Œè¿è´¯æ€§ï¼Œå¹¶ä¸”æ¶‰åŠå¤šä¸ªè¯´è¯è€…å‚ä¸å¤šè½®å¯¹è¯ã€‚</li><li>ç”Ÿæˆçš„å¯¹è¯åœ¨å•ä¸ªé€šé“å†…ï¼Œå…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢ï¼ˆåŒ…æ‹¬é‡å è¯­éŸ³ï¼‰å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºï¼ˆä¾‹å¦‚ç¬‘å£°ï¼‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CoVoMix: æ¨è¿›é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä»¥å®ç°ç±»äººå¤šè¯´è¯è€…å¯¹è¯</p></li><li><p>Authors: Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</p></li><li><p>Affiliation: ä¸Šæµ·äº¤é€šå¤§å­¦</p></li><li><p>Keywords: é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆï¼Œå¤šè¯´è¯è€…å¯¹è¯ï¼Œè¯­éŸ³åˆæˆï¼Œè‡ªç„¶è¯­è¨€å¤„ç†</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.06690 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ (TTS) å»ºæ¨¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œå¤šæ ·åŒ–çš„è¯­éŸ³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹è¯ç”Ÿæˆä»¥åŠåœ¨è¯­éŸ³ä¸­å®ç°ç±»äººçš„è‡ªç„¶æ€§ä»ç„¶æ˜¯è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å•æµæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œæ— æ³•ç”Ÿæˆå¤šè¯´è¯è€…å¯¹è¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œè¿™åœ¨å¯¹è¯åœºæ™¯ä¸­å¯èƒ½ä¸å¯ç”¨ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CoVoMix çš„æ¨¡å‹ï¼Œç”¨äºé›¶æ ·æœ¬ã€ç±»äººã€å¤šè¯´è¯è€…ã€å¤šè½®å¯¹è¯è¯­éŸ³ç”Ÿæˆã€‚CoVoMix èƒ½å¤Ÿé¦–å…ˆå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£ä»¤ç‰Œæµï¼Œæ¯ä¸ªä»¤ç‰Œæµè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶åå°†è¿™äº›ä»¤ç‰Œæµé¦ˆé€åˆ°åŸºäºæµåŒ¹é…çš„å£°å­¦æ¨¡å‹ä¸­ä»¥ç”Ÿæˆæ··åˆæ¢…å°”è°±å›¾ã€‚æœ€åï¼Œä½¿ç”¨ HiFi-GAN æ¨¡å‹ç”Ÿæˆè¯­éŸ³æ³¢å½¢ã€‚</p><p>(4): å®éªŒç»“æœï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMix å¯ä»¥ç”Ÿæˆä¸ä»…åœ¨è‡ªç„¶æ€§å’Œè¿è´¯æ€§ä¸Šç±»ä¼¼äººç±»çš„å¯¹è¯ï¼Œè€Œä¸”è¿˜æ¶‰åŠå¤šä¸ªè¯´è¯è€…è¿›è¡Œå¤šè½®å¯¹è¯ã€‚è¿™äº›åœ¨å•ä¸ªé€šé“å†…ç”Ÿæˆçš„å¯¹è¯å…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢ï¼ŒåŒ…æ‹¬é‡å è¯­éŸ³å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºï¼Œä¾‹å¦‚ç¬‘å£°ã€‚</p><ol><li>ç ”ç©¶æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šå¤šæµæ–‡æœ¬åˆ°è¯­ä¹‰æ¨¡å‹ï¼šåŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå°†æ–‡æœ¬æ ‡è®°åºåˆ—è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£æ ‡è®°æµï¼Œæ¯ä¸ªæ ‡è®°æµè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå£°å­¦æ¨¡å‹ï¼šåŸºäºæµåŒ¹é…çš„å˜å‹å™¨ç¼–ç å™¨ï¼Œå°†è¯­ä¹‰åºåˆ—è½¬æ¢ä¸ºæ··åˆæ¢…å°”è°±å›¾ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šå£°ç å™¨ï¼šä½¿ç”¨ HiFi-GAN æ¨¡å‹ä»æ¢…å°”è°±å›¾ç”Ÿæˆè¯­éŸ³æ³¢å½¢ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ CoVoMix æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€ç±»äººã€å¤šè¯´è¯è€…ã€å¤šè½®å¯¹è¯è¯­éŸ³ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„å¤šæµæ–‡æœ¬åˆ°è¯­ä¹‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£ä»¤ç‰Œæµï¼Œè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ï¼›è®¾è®¡äº†ä¸€ç§åŸºäºæµåŒ¹é…çš„å£°å­¦æ¨¡å‹ï¼Œå°†è¯­ä¹‰åºåˆ—è½¬æ¢ä¸ºæ··åˆæ¢…å°”è°±å›¾ï¼›é‡‡ç”¨ HiFi-GAN æ¨¡å‹ç”Ÿæˆè¯­éŸ³æ³¢å½¢ï¼Œå®ç°äº†é«˜ä¿çœŸè¯­éŸ³åˆæˆã€‚</p><p>æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMix ç”Ÿæˆçš„å¯¹è¯åœ¨è‡ªç„¶æ€§å’Œè¿è´¯æ€§ä¸Šç±»ä¼¼äººç±»ï¼Œæ¶‰åŠå¤šä¸ªè¯´è¯è€…è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œå…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºã€‚</p><p>å·¥ä½œé‡ï¼šCoVoMix æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-271fa2d3d54bc8eac1b4ebb4afb68b5f.jpg" align="middle"></details><h2 id="Beyond-Talking-â€”-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication"><a href="#Beyond-Talking-â€”-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication" class="headerlink" title="Beyond Talking â€” Generating Holistic 3D Human Dyadic Motion for   Communication"></a>Beyond Talking â€” Generating Holistic 3D Human Dyadic Motion for   Communication</h2><p><strong>Authors:Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang</strong></p><p>In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.19467v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡éŸ³é¢‘ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå®ç°è¯´è¯äººå’Œå€¾å¬è€…3Dé€¼çœŸä¸”åè°ƒçš„åŠ¨ä½œç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºéŸ³é¢‘ç‰¹å¾ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯è§£è€¦åˆçš„åˆ›æ–°ä»»åŠ¡ï¼Œç”Ÿæˆè¯´è¯äººå’Œå€¾å¬è€…çš„3DåŠ¨ä½œã€‚</li><li>åˆ†åˆ«è®­ç»ƒè¯´è¯äººå’Œå€¾å¬è€…çš„æ•´ä½“åŠ¨ä½œVQ-VAEã€‚</li><li>è€ƒè™‘è¯´è¯äººå’Œå€¾å¬è€…ä¹‹é—´çš„å®æ—¶ç›¸äº’å½±å“ï¼Œæå‡ºè‡ªå›å½’æ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆè¯´è¯äººå’Œå€¾å¬è€…çš„åŠ¨ä½œã€‚</li><li>é“¾å¼Transformeræ¨¡å‹ï¼Œæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œçš„æ²Ÿé€šåœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>å¼•å…¥HoCoæ•´ä½“æ²Ÿé€šæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›å®è´µèµ„æºã€‚</li><li>æ¥å—åå°†HoCoæ•°æ®é›†å’Œä»£ç å‘å¸ƒç”¨äºç ”ç©¶ç›®çš„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šè¶…è¶Šè¯´è¯â€”â€”ç”Ÿæˆç”¨äºäº¤æµçš„æ•´ä½“ 3D äººç±»åŒäººè¿åŠ¨</p></li><li><p>ä½œè€…ï¼šMingze Sun Â· Chao Xu Â· Xinyu Jiang Â· Yang Liu Â· Baigui Sun Â· Ruqi Huang</p></li><li><p>å•ä½ï¼šæ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢</p></li><li><p>å…³é”®è¯ï¼šDyadic Motion, Holistic Human Mesh, Communication</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šNone, Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºå¤§è§„æ¨¡äººç±»è¯´è¯è§†é¢‘çš„è¯­éŸ³ç”Ÿæˆè¿åŠ¨ä»»åŠ¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå³ä»è¨€è¯­çº¿ç´¢ï¼ˆå¦‚éŸ³é¢‘ç‰‡æ®µæˆ–è½¬å½•ï¼‰ä¸­ç”Ÿæˆè°ˆè¯ä¸­çš„éè¯­è¨€ä¿¡å·ï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…æˆ–èº«ä½“åŠ¨ä½œï¼‰ï¼Œä¾‹å¦‚äººç±»é¢éƒ¨è¡¨æƒ…ã€èº«ä½“å§¿åŠ¿å’Œæ‰‹åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…å…³æ³¨è¯´è¯è€…ï¼Œè€Œå¿½ç•¥äº†å¬ä¼—çš„ååº”ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆè¯´è¯è€…çš„å¤´éƒ¨ã€æ‰‹åŠ¿æˆ–å…¨èº«è¿åŠ¨ï¼Œè€Œå¿½ç•¥äº†å¬ä¼—çš„ååº”ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œä¸“æ³¨äºäººç±»äº¤æµï¼Œæ—¨åœ¨ä¸ºè¯´è¯è€…å’Œå¬ä¼—ç”Ÿæˆ 3D æ•´ä½“äººç±»åŠ¨ä½œã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å°†åˆ†è§£å› å­åŒ–ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„ç»“åˆï¼Œä»è€Œä¿ƒè¿›åˆ›å»ºæ›´é€¼çœŸå’Œåè°ƒçš„åŠ¨ä½œã€‚æˆ‘ä»¬åˆ†åˆ«é’ˆå¯¹è¯´è¯è€…å’Œå¬ä¼—çš„æ•´ä½“åŠ¨ä½œè®­ç»ƒ VQ-VAEã€‚æˆ‘ä»¬è€ƒè™‘äº†è¯´è¯è€…å’Œå¬ä¼—ä¹‹é—´çš„å®æ—¶ç›¸äº’å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é“¾å¼åŸºäº Transformer çš„è‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“é—¨è®¾è®¡ç”¨äºæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œä¸­çš„äº¤æµåœºæ™¯ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„åŠ¨ä½œã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æˆ‘ä»¬ç”Ÿæˆçš„ç»“æœæ—¢åè°ƒåˆå¤šæ ·ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹æœªæ¥ç ”ç©¶æœ‰ä»·å€¼çš„èµ„æºã€‚æˆ‘ä»¬çš„ HoCo æ•°æ®é›†å’Œä»£ç å°†åœ¨è¢«æ¥å—åå‘å¸ƒä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šé’ˆå¯¹è¯´è¯è€…å’Œå¬ä¼—çš„æ•´ä½“åŠ¨ä½œï¼Œåˆ†åˆ«è®­ç»ƒ VQ-VAEï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäº Transformer çš„è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œä¸­çš„äº¤æµåœºæ™¯ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„åŠ¨ä½œï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨åˆ†è§£å› å­åŒ–ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„ç»“åˆï¼Œä¿ƒè¿›åˆ›å»ºæ›´é€¼çœŸå’Œåè°ƒçš„åŠ¨ä½œï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œå°†äº¤æµçº³å…¥äººæœºäº¤äº’ä¸­ï¼Œæå‡ºäº†ä¸€é¡¹æ–°é¢–çš„ä»»åŠ¡ï¼Œä¸ºè¯´è¯è€…å’Œå¬ä¼—ç”Ÿæˆ 3D æ•´ä½“äººç±»åŠ¨ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨æ•°æ®é›†å’Œæ¨¡å‹è®¾è®¡ä¸Šå‡åšå‡ºäº†è´¡çŒ®ã€‚å‰è€…æ–¹é¢ï¼Œæˆ‘ä»¬æä¾›äº† HoCo é€šä¿¡æ•°æ®é›†ï¼Œä»¥ä¾›æœªæ¥æ²¿ç€æ­¤ä»»åŠ¡è¿›è¡Œæ¢ç´¢ã€‚åè€…æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æˆ‘ä»¬ä»»åŠ¡é‡èº«å®šåˆ¶çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«æ–°é¢–çš„è®¾è®¡ï¼ŒåŒ…æ‹¬ 1ï¼‰ç”¨äºè§£è€¦éŸ³é¢‘ç‰¹å¾çš„åˆ†è§£ï¼Œå¢å¼ºäº†ç”Ÿæˆæ›´çœŸå®å’Œåè°ƒçš„åŠ¨ä½œï¼›2ï¼‰ç”¨äºè¡¨å¾éè¯­è¨€äº¤æµçš„é“¾å¼è‡ªå›å½’æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œä¸“æ³¨äºç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„ 3D æ•´ä½“äººç±»åŠ¨ä½œï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé’ˆå¯¹è¯¥ä»»åŠ¡é‡èº«å®šåˆ¶çš„æ¨¡å‹ã€‚æ€§èƒ½ï¼šåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šå¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8dd55a7f4757f4ae1f9d71880b4c6479.jpg" align="middle"><img src="https://picx.zhimg.com/v2-848d816930200060ec067527f2cd2e66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90bdfdaba5b0a3b62088d04ae352d6de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95a401074128a34e072805c4fda00e11.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  CoVoMix Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/</id>
    <published>2024-05-06T10:26:38.000Z</published>
    <updated>2024-05-06T10:26:38.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><a href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition" class="headerlink" title="Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"></a>Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</h2><p><strong>Authors:Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</strong></p><p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance. </p><p><a href="http://arxiv.org/abs/2405.01872v1">PDF</a> </p><p><strong>Summary</strong><br>é’¢æè¡¨é¢ç¼ºé™·ç”Ÿæˆæ¨¡å‹StableSDGé€šè¿‡è¿ç§»Stable Diffusionæ¨¡å‹ç”Ÿæˆé«˜ç²¾åº¦åˆæˆå›¾åƒï¼Œæœ‰æ•ˆæå‡é’¢æè¡¨é¢ç¼ºé™·è¯†åˆ«æ¨¡å‹çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨Stable Diffusionæ¨¡å‹ç”Ÿæˆé’¢æè¡¨é¢ç¼ºé™·å›¾åƒï¼Œæ‰©å……è®­ç»ƒæ•°æ®é›†ã€‚</li><li>é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°å’ŒåµŒå…¥ç©ºé—´ï¼Œå¼¥åˆç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚</li><li>é‡‡ç”¨å›¾åƒå¼•å¯¼ç”Ÿæˆæ–¹å¼ï¼Œè€Œéçº¯é«˜æ–¯å™ªå£°ç”Ÿæˆã€‚</li><li>æå‡ºä¸¤ç§å…³é”®è¿‡ç¨‹ï¼šåˆ†å¸ƒå¯¹é½å’Œå›¾åƒå¼•å¯¼ç”Ÿæˆã€‚</li><li>åœ¨é’¢æè¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜StableSDGåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒå’Œè®­ç»ƒè¯†åˆ«æ¨¡å‹æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li><li>ä¸¤ç§æå‡ºçš„å…³é”®è¿‡ç¨‹å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li><li>StableSDGæœ‰æ•ˆè§£å†³æ•°æ®ä¸è¶³é—®é¢˜ï¼Œæå‡é’¢æè¡¨é¢ç¼ºé™·è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>ç¼ºé™·å›¾åƒæ ·æœ¬ç”Ÿæˆä¸æ‰©æ•£</p></li><li><p>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang</p></li><li><p>ä¸Šæµ·å¤§å­¦ä¼ ä¿¡å­¦é™¢</p></li><li><p>Text-to-image diffusion, data expansion, deep learning, textual inversion, low-rank adaptation, defect image generation, steel surface defect recognition</p></li><li><p>https://arxiv.org/abs/2405.01872 , Github:None</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šé’¢è¡¨é¢ç¼ºé™·è¯†åˆ«æ˜¯å·¥ä¸šç•Œå…·æœ‰å·¨å¤§äº§ä¸šä»·å€¼çš„ä¸€é¡¹ä»»åŠ¡ã€‚æ•°æ®ä¸è¶³æ˜¯è®­ç»ƒé²æ£’ç¼ºé™·è¯†åˆ«ç½‘ç»œçš„ä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å·²ç»ç ”ç©¶äº†é€šè¿‡ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ ·æœ¬ä»¥æ‰©å¤§æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ç”Ÿæˆè´¨é‡ä»ç„¶å—åˆ°ç¼ºé™·å›¾åƒæ ·æœ¬ä¸è¶³çš„é™åˆ¶ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬ï¼šSDGANã€Defect-GANã€transP2Pã€‚è¿™äº›æ–¹æ³•ä»å¤´å¼€å§‹è®­ç»ƒç”Ÿæˆæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå½“å›¾åƒæ ·æœ¬ä¸è¶³æ—¶ï¼Œé€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆæ ·æœ¬ä¸­å‡ºç°ä¸éœ€è¦çš„æ¨¡å¼ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å®šçš„è¡¨é¢ç¼ºé™·ç”Ÿæˆï¼ˆStableSDGï¼‰æ–¹æ³•ï¼Œå°†Stable Diffusionæ¨¡å‹ä¸­åµŒå…¥çš„å·¨å¤§ç”Ÿæˆåˆ†å¸ƒè½¬ç§»ç”¨äºé’¢è¡¨é¢ç¼ºé™·å›¾åƒç”Ÿæˆã€‚ä¸ºäº†è§£å†³é’¢è¡¨é¢å›¾åƒå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒä¹‹é—´çš„ç‹¬ç‰¹åˆ†å¸ƒå·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„å‚æ•°æ¥å¯¹é½åˆ†å¸ƒï¼Œæ—¢é‡‡ç”¨æ ‡è®°åµŒå…¥ç©ºé—´ï¼Œä¹Ÿé‡‡ç”¨ç½‘ç»œå‚æ•°ç©ºé—´ã€‚æ­¤å¤–ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å›¾åƒçš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯ä»çº¯é«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæˆ‘ä»¬åœ¨é’¢è¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå±•ç¤ºäº†åœ¨ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬å’Œè®­ç»ƒè¯†åˆ«æ¨¡å‹æ–¹é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶ä¸”ä¸¤ä¸ªè®¾è®¡è¿‡ç¨‹å¯¹æ€§èƒ½éƒ½å¾ˆé‡è¦ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):Stable Diffusionæ¨¡å‹[22]ä½œä¸ºæ‰©æ•£å…ˆéªŒï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œæ‰©æ•£ï¼Œè€Œä¸æ˜¯å›¾åƒç©ºé—´ï¼Œå¹¿æ³›ç”¨äºå›¾åƒç”Ÿæˆä»»åŠ¡[26]â€“[30]ã€‚            (2):æå‡ºStableSDGæ–¹æ³•ï¼Œç”±ä¸¤ä¸ªè¿‡ç¨‹ç»„æˆï¼Œç”¨äºç”Ÿæˆæ¯ç§ç¼ºé™·ç±»åˆ«çš„å›¾åƒã€‚            (3):é€šè¿‡è¿­ä»£è´¨é‡è¯„ä¼°ï¼Œè°ƒæ•´è¶…å‚æ•°ä»¥å®ç°æœ€ä½³å›¾åƒç”Ÿæˆã€‚            (4):ä½¿ç”¨æœ€ä½³è¶…å‚æ•°ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒä»¥æ‰©å±•æ•°æ®é›†ã€‚            (5):å°†æ¯ç§ç¼ºé™·ç±»åˆ«çš„ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒä¸€èµ·æ”¶é›†èµ·æ¥ï¼Œç”¨äºè®­ç»ƒç¼ºé™·è¯†åˆ«æ¨¡å‹ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„StableSDGæ–¹æ³•å°†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åº”ç”¨äºé’¢è¡¨é¢ç¼ºé™·å›¾åƒç”Ÿæˆï¼Œæœ‰æ•ˆè§£å†³äº†é’¢è¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸è¶³çš„é—®é¢˜ï¼Œä¸ºç¼ºé™·è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ ·æœ¬ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šStableSDGæ–¹æ³•åœ¨ç”Ÿæˆå™¨è‡ªé€‚åº”è¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶åœ¨tokenåµŒå…¥ç©ºé—´å’Œç½‘ç»œå‚æ•°ç©ºé—´è¿›è¡Œè‡ªé€‚åº”å’Œä¿®æ”¹ï¼Œå¹¶åœ¨ç”Ÿæˆæ•°æ®æ—¶ä»å›¾åƒå¯¼å‘åˆå§‹åŒ–ç”Ÿæˆæ ·æœ¬ï¼Œè€Œä¸æ˜¯ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹ã€‚</p><p>æ€§èƒ½ï¼šStableSDGæ–¹æ³•åœ¨NEUå’ŒCCBSDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„ç¼ºé™·å›¾åƒï¼Œå¤§å¤§æé«˜äº†è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼šStableSDGæ–¹æ³•çš„å·¥ä½œé‡ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆå™¨è‡ªé€‚åº”å’Œå›¾åƒç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œéœ€è¦å¯¹è¶…å‚æ•°è¿›è¡Œè¿­ä»£è´¨é‡è¯„ä¼°å’Œè°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„å›¾åƒç”Ÿæˆæ•ˆæœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-bd44eeacb7308bdc2e6594f5b84b63b5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a49aa1e7511a9ee599c2b42ba68cfb6d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e589bb5b223b5f1a03944e68feabbcd1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b4b7e153a9792e0731936f44ad770e5f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4222a03fe5309bbcdc62d8769e94eb0c.jpg" align="middle"></details><h2 id="Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><a href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning" class="headerlink" title="Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning"></a>Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning</h2><p><strong>Authors:Rafael Elberg, Denis Parra, Mircea Petrache</strong></p><p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at <a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a> </p><p><a href="http://arxiv.org/abs/2405.01705v1">PDF</a> </p><p><strong>Summary</strong><br>é•¿å°¾æ•°æ®å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œç¼“è§£ç”Ÿæˆè´¨é‡å·®å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒå’Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨æ•°æ®åˆ†å¸ƒä¸è¶³çš„æƒ…å†µä¸‹æå…·æŒ‘æˆ˜æ€§ã€‚</li><li>åŒ»å­¦é¢†åŸŸçš„å›¾åƒç”Ÿæˆé¢ä¸´æ•°æ®è·å–å’Œéšç§é™åˆ¶çš„éšœç¢ã€‚</li><li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šå¤„äºé¢†å…ˆåœ°ä½ã€‚</li><li>æ•°æ®ç”Ÿæˆä¸å¹³è¡¡å’Œæ¨ç†é€Ÿåº¦æ…¢æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li><li>æ–¹æ³•ç»“åˆé¢„è®­ç»ƒç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œè¿›è¡Œå›¾åƒå¢å¼ºã€‚</li><li>æ„å»ºå¯åˆ†ç¦»çš„æ½œåœ¨ç©ºé—´ï¼Œæ··åˆå¤´éƒ¨å’Œå°¾éƒ¨ç±»åˆ«çš„ç¤ºä¾‹ã€‚</li><li>é€šè¿‡è¿­ä»£å­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œæ„å»ºç©ºé—´ï¼Œå¹¶é€šè¿‡ K-NN æ–¹æ³•åº”ç”¨äºç‰¹å®šä»»åŠ¡çš„æ˜¾ç€æ€§å›¾ã€‚</li><li>ä»£ç å·²å¼€æºï¼š<a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šç‰¹å¾ç©ºé—´å¢å¼ºå’Œè¿­ä»£å­¦ä¹ çš„é•¿å°¾å›¾åƒç”Ÿæˆ</p></li><li><p>ä½œè€…ï¼šRafael Elbergã€Denis Parraã€Mircea Petrache</p></li><li><p>éš¶å±ï¼šæ™ºåˆ©å¤©ä¸»æ•™å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šé•¿å°¾æ•°æ®ã€å›¾åƒç”Ÿæˆã€ç‰¹å¾ç©ºé—´å¢å¼ºã€è¿­ä»£å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01705   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šå›¾åƒå’Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨æ•°æ®åˆ†å¸ƒä¸å‡åŒ€çš„æƒ…å†µä¸‹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œæ•°æ®å¯ç”¨æ€§å’Œéšç§é™åˆ¶åŠ å‰§äº†è¿™äº›éšœç¢ã€‚æ½œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢å¤„äºæœ€å…ˆè¿›æ°´å¹³ï¼Œä½¿å…¶æˆä¸ºè§£å†³æ­¤é—®é¢˜çš„ç†æƒ³å€™é€‰è€…ã€‚ç„¶è€Œï¼Œä»éœ€è¦è§£å†³å‡ ä¸ªå…³é”®é—®é¢˜ï¼Œä¾‹å¦‚éš¾ä»¥ç”Ÿæˆæ¥è‡ªä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«çš„å›¾åƒä»¥åŠæ¨ç†è¿‡ç¨‹ç¼“æ…¢ã€‚</p><p>(2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬é‡é‡‡æ ·å’Œæ•°æ®å¢å¼ºã€‚é‡é‡‡æ ·æŠ€æœ¯åœ¨ä¸€äº›é•¿å°¾é—®é¢˜ä¸­å–å¾—äº†ç›¸å¯¹æˆåŠŸï¼Œä½†å¯èƒ½ä¼šç»™ä¸‹æ¸¸ä»»åŠ¡å¼•å…¥ä¸å¿…è¦çš„åå·®ï¼Œå¹¶ä¸”ç»å¸¸å¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ•°æ®å¢å¼ºæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„è‡ªç„¶å“åº”ã€‚å®ƒä»£è¡¨äº†ä¸€ä¸ªè“¬å‹ƒå‘å±•çš„ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡ ä¸ªä¸åŒçš„ç®—æ³•ç³»åˆ—ï¼Œä¾‹å¦‚å‡ ä½•å˜æ¢ï¼ˆæ—‹è½¬ã€ç¼©æ”¾ã€è£å‰ªç­‰ï¼‰ã€åˆæˆæ ·æœ¬åˆ›å»ºã€åŸºäºæ··åˆçš„æ–¹æ³•ã€åŸºäºåŸŸè½¬æ¢çš„æ–¹æ³•å’Œç”Ÿæˆæ–¹æ³•ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ“çºµæ¥è‡ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆæ–°å›¾åƒæ¥å¢å¼ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ã€‚é€šè¿‡æ¿€æ´»å›¾é€‰æ‹©æ•°æ®çš„ç‰¹å®šç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾ç»„åˆèµ·æ¥ï¼Œç”Ÿæˆä¸å±äºé•¿å°¾ç±»çš„å®é™…æ•°æ®ä¸­çš„å›¾åƒç›¸ä¼¼çš„å›¾åƒã€‚</p><p>(4)ï¼šåœ¨æœ¬æ–‡çš„æ–¹æ³•ä¸­ï¼Œæ½œåœ¨ç©ºé—´è¡¨ç¤ºçš„ç»„åˆç”±äºç‰¹å¾åå¤„ç†ä¹‹é—´çš„å¹²æ‰°ç°è±¡è€Œéš¾ä»¥é€šè¿‡æœ´ç´ çš„æ–¹æ³•æ‰§è¡Œã€‚æœ¬æ–‡å°†æ­¤é—®é¢˜ä½œä¸ºåˆæˆæ³›åŒ–é—®é¢˜ï¼Œå¹¶å°†è¿­ä»£å­¦ä¹ ï¼ˆILï¼‰æ¡†æ¶ä¸ç¨€ç–åµŒå…¥åº”ç”¨äºç›®æ ‡æ•°æ®å¢å¼ºæ¡†æ¶ã€‚IL çš„ä¸»è¦çµæ„Ÿæ¥è‡ªæ–‡åŒ–è¿›åŒ–æ¨¡å‹ï¼Œå…¶ä¸­æ•™å¸ˆ-å­¦ç”Ÿäº¤äº’çš„è¿­ä»£é¼“åŠ±æœ‰ç”¨çš„å‹ç¼©å’Œå½¢æˆé€‚åº”ä»»åŠ¡çš„â€œå…±äº«è¯­è¨€â€ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ€è¿‘åœ¨ä½¿ç”¨ç¨€ç–çŠ¶æ€ç©ºé—´æ—¶è·å¾—äº†ä¸åˆæˆä¸åŒç‰¹å¾ç›¸å…³çš„æœ‰åˆ©ç»“æœã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ“çºµæ¥è‡ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ¥ç”Ÿæˆæ–°å›¾åƒï¼Œä»¥å¢å¼ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šè¿­ä»£è®­ç»ƒã€ç±»åˆ«æ¿€æ´»å›¾ç”Ÿæˆå’Œæ¨ç†ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨è¿­ä»£è®­ç»ƒé˜¶æ®µï¼Œå­¦ä¹ äº†ä¸€ä¸ªä»æ‰©æ•£æ½œåœ¨ç©ºé—´åˆ°ç¨€ç–é«˜ç»´è¡¨ç¤ºçš„è½¬æ¢ï¼ŒåŒæ—¶è®­ç»ƒäº†ä¸€ä¸ªå·ç§¯åˆ†ç±»å™¨ç”¨äºè¯¥ç©ºé—´ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç±»åˆ«æ¿€æ´»å›¾ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨åˆ†ç±»å™¨ç”Ÿæˆæ¯ä¸ªç±»çš„ç®€å•å¯è§£é‡Šæ¿€æ´»å›¾ï¼Œä»¥é€‰æ‹©ä¸åˆ†ç±»ä¸ºè¯¥ç±»ç›¸å…³çš„æˆ–ä¸ç›¸å…³çš„åæ ‡ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œä»å°¾éƒ¨ç±»ç”Ÿæˆæ–°æ ·æœ¬ï¼Œé€šè¿‡å°†ç‰¹å®šå°¾éƒ¨ç±»ç¤ºä¾‹çš„ç±»ç‰¹å®šç‰¹å¾ä¸æœ€é«˜æ··æ·†å¤´éƒ¨ç±»çš„ç±»é€šç”¨ç‰¹å¾èåˆï¼Œä½¿ç”¨æ©ç åˆ›å»ºèåˆå‘é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€ç»„åˆå­¦ä¹ å’Œæ˜¾ç€æ€§æ–¹æ³•ï¼Œä¸ºé•¿å°¾æ•°æ®é›†ç”Ÿæˆæ•°æ®å¹¶å¢å¼ºæ•°æ®ï¼Œä»è€Œä¸ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ç”Ÿæˆæ–°ç¤ºä¾‹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€ç»„åˆå­¦ä¹ å’Œæ˜¾è‘—æ€§æ–¹æ³•çš„æ•°æ®å¢å¼ºå’Œæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼›æ€§èƒ½ï¼šåœ¨åŒ»å­¦é¢†åŸŸçš„å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨ MIMIC-CXR-LT [13, 16] çš„ä¸€ä¸ªå°å‹å­é›†ï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œæ•°æ®å¢å¼ºæ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¯¹è½¬æ¢å’Œåˆ†ç±»å™¨è¿›è¡Œè¿­ä»£è®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0d188950e8539013f5d1dbb852ac0cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c5b473a90cffec494f2607efb08a6c2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-39ea380f716fabbe74492f3835a23773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d004602232a458ce5dd218668a87e87.jpg" align="middle"></details><h2 id="LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><a href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing" class="headerlink" title="LocInv: Localization-aware Inversion for Text-Guided Image Editing"></a>LocInv: Localization-aware Inversion for Text-Guided Image Editing</h2><p><strong>Authors:Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</strong></p><p>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at <a href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a> </p><p><a href="http://arxiv.org/abs/2405.01496v1">PDF</a> Accepted by CVPR 2024 Workshop AI4CC</p><p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶åˆ©ç”¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä½†ç°æœ‰ç¼–è¾‘æŠ€æœ¯å®¹æ˜“ä¿®æ”¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸï¼Œä¸»è¦æ˜¯å› ä¸ºäº¤å‰æ³¨æ„åŠ›å›¾ä¸å‡†ç¡®ã€‚æˆ‘ä»¬é€šè¿‡åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†æ”¹è¿›æ‰©æ•£è¿‡ç¨‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå®ç°äº†ç‰¹å®šå¯¹è±¡çš„ç»†ç²’åº¦å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶é˜²æ­¢å¯¹å…¶ä»–åŒºåŸŸè¿›è¡Œéå¿…è¦çš„æ›´æ”¹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰å›¾åƒç¼–è¾‘æŠ€æœ¯å®¹æ˜“ä¿®æ”¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸã€‚</li><li>æˆ‘ä»¬æå‡ºäº†åˆ©ç”¨åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†ä½œä¸ºé¢å¤–çš„å®šä½å…ˆéªŒæ¥æ”¹è¿›æ‰©æ•£è¿‡ç¨‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›å›¾ã€‚</li><li>æˆ‘ä»¬é€šè¿‡æ›´æ–°æ–‡æœ¬è¾“å…¥ä¸­åè¯å¯¹åº”çš„ç¬¦å·ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ç´§å¯†å¯¹é½æ–‡æœ¬æç¤ºä¸­çš„æ­£ç¡®åè¯å’Œå½¢å®¹è¯ã€‚</li><li>æˆ‘ä»¬åŸºäºå…¬å¼€çš„Stable Diffusionå®ç°äº†LocInvæ–¹æ³•ï¼Œå¹¶åœ¨COCOæ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</li><li>è¯¥æ–¹æ³•çš„ä»£ç å°†åœ¨<a href="https://github.com/wangkai930418/DPLä¸Šå…¬å¸ƒã€‚">https://github.com/wangkai930418/DPLä¸Šå…¬å¸ƒã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šå®šä½æ„ŸçŸ¥åæ¼”ï¼šæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘</p></li><li><p>ä½œè€…ï¼šChuanming Tangã€Kai Wangã€Fei Yangã€Joost van de Weijer</p></li><li><p>å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒç¼–è¾‘ã€å®šä½æ„ŸçŸ¥ã€äº¤å‰æ³¨æ„åŠ›</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01496Githubï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸‹å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶æ—¨åœ¨é€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºæ¥èµ‹äºˆç”¨æˆ·æ“çºµç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå·²æœ‰æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å®¹æ˜“å¯¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸè¿›è¡Œç¼–è¾‘ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºäº¤å‰æ³¨æ„åŠ›å›¾çš„ä¸å‡†ç¡®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå®šä½æ„ŸçŸ¥åæ¼”ï¼ˆLocInvï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†ä½œä¸ºé¢å¤–çš„å®šä½å…ˆéªŒï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹çš„å»å™ªé˜¶æ®µä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å›¾ã€‚é€šè¿‡åŠ¨æ€æ›´æ–°æ–‡æœ¬è¾“å…¥ä¸­ä¸åè¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­æ­£ç¡®çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ã€‚åŸºäºæ­¤æŠ€æœ¯ï¼Œæˆ‘ä»¬å®ç°äº†å¯¹ç‰¹å®šå¯¹è±¡çš„ç»†ç²’åº¦å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶é˜²æ­¢å¯¹å…¶ä»–åŒºåŸŸè¿›è¡Œä¸å¿…è¦çš„æ›´æ”¹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåŸºäºå…¬å¼€çš„Stable Diffusionï¼Œæˆ‘ä»¬å¯¹LocInvæ–¹æ³•åœ¨COCOæ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥å®ç°å…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨ Stable Diffusion v1.4 ä½œä¸ºåŸºç¡€æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±ç¼–ç å™¨ã€è§£ç å™¨å’Œæ‰©æ•£æ¨¡å‹ç»„æˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ DDIM åæ¼”ç®—æ³•ï¼Œä»éšæœºå™ªå£° zT æ‰¾åˆ°åˆå§‹å™ªå£°ï¼Œé€šè¿‡é‡‡æ ·é‡å»ºè¾“å…¥æ½œåœ¨ä»£ç  z0ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨æ— æ–‡æœ¬åæ¼” (NTI) ä¼˜åŒ–æ— æ–‡æœ¬åµŒå…¥ âˆ…tï¼Œä»¥è¿‘ä¼¼ DDIM è½¨è¿¹ {zt}T 0ï¼Œä»è€Œç¼–è¾‘çœŸå®å›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºåŠ¨æ€æç¤ºå­¦ä¹  (DPL) æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒï¼Œæ›´æ–°æ–‡æœ¬æç¤º P ä¸­çš„åè¯å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šè®¾è®¡ç›¸ä¼¼åº¦æŸå¤±å’Œé‡å æŸå¤±ï¼Œä¼˜åŒ–åµŒå…¥å‘é‡ Vtï¼Œä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸å®šä½å…ˆéªŒ S ä¹‹é—´ç›¸ä¼¼åº¦é«˜ã€é‡å åº¦é«˜ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨æ¸è¿›ä¼˜åŒ–æœºåˆ¶ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥ t å¤„å¼ºåˆ¶æ‰€æœ‰æŸå¤±è¾¾åˆ°é¢„å®šä¹‰é˜ˆå€¼ï¼Œé¿å…äº¤å‰æ³¨æ„åŠ›å›¾è¿‡æ‹Ÿåˆã€‚</p><p>ï¼ˆ7ï¼‰ï¼šç»“åˆ NTI å­¦ä¹ ä¸€ç»„æ— æ–‡æœ¬åµŒå…¥ âˆ…tï¼Œä¸å¯å­¦ä¹ çš„å•è¯åµŒå…¥ Vt å…±åŒç²¾ç¡®å®šä½å¯¹è±¡å¹¶é‡å»ºåŸå§‹å›¾åƒã€‚</p><p>ï¼ˆ8ï¼‰ï¼šæå‡ºå½¢å®¹è¯ç»‘å®šæœºåˆ¶ï¼Œé€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºä¸­çš„å½¢å®¹è¯æ¥æ”¹å˜å¯¹è±¡çš„å¤–è§‚ã€‚</p><ol><li>Conclusion:</li></ol><p>(1): æœ¬æ–‡æå‡ºçš„ LocInv æ–¹æ³•è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘ä¸­äº¤å‰æ³¨æ„åŠ›å›¾æ³„æ¼çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå…ˆéªŒï¼Œæ›´æ–°æç¤ºä¸­æ¯ä¸ªåè¯å•è¯çš„åŠ¨æ€æ ‡è®°ã€‚ç”±æ­¤äº§ç”Ÿçš„äº¤å‰æ³¨æ„åŠ›å›¾è¾ƒå°‘å—åˆ°äº¤å‰æ³¨æ„åŠ›å›¾æ³„æ¼çš„å½±å“ã€‚å› æ­¤ï¼Œè¿™äº›å¤§å¤§æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å›¾æå¤§åœ°æ”¹å–„äº†æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„ç»“æœã€‚å®éªŒç»“æœè¯å®ï¼ŒLocInv è·å¾—äº†æ›´å¥½çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥å°†å½¢å®¹è¯å•è¯ç»‘å®šåˆ°å®ƒä»¬å¯¹åº”ã®åè¯ä¸Šï¼Œä»è€Œå¾—åˆ°å½¢å®¹è¯çš„å‡†ç¡®äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå¹¶å…è®¸å¯¹å±æ€§è¿›è¡Œç¼–è¾‘ï¼Œè¿™æ˜¯ä»¥å‰åœ¨æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ä¸­å°šæœªå……åˆ†æ¢ç´¢çš„ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼šæå‡ºå®šä½æ„ŸçŸ¥åæ¼”æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒï¼Œæ›´æ–°æ–‡æœ¬æç¤ºä¸­çš„åè¯å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ï¼›æå‡ºå½¢å®¹è¯ç»‘å®šæœºåˆ¶ï¼Œé€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºä¸­çš„å½¢å®¹è¯æ¥æ”¹å˜å¯¹è±¡çš„å¤–è§‚ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ COCO æ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥å®ç°å…¶ç›®æ ‡ã€‚</p><p>å·¥ä½œé‡ï¼šæ–¹æ³•å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦ç»“åˆ Stable Diffusion æ¨¡å‹å’Œ NTI åæ¼”ç®—æ³•ï¼Œä»¥åŠåˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-501b84f66a4fdce982c4d560d6ed2c6e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d7f11972c7c9876389df6092b426ca67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-566375be0266ca83b73d642319fcc82b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2650ddd9595d88f0a5238c88b753e8e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-997ccc837824e0d3f900484e2641fab6.jpg" align="middle"></details><h2 id="Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><a href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers" class="headerlink" title="Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers"></a>Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</strong></p><p>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes. </p><p><a href="http://arxiv.org/abs/2405.00858v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨åˆæˆå›¾åƒæ„ŸæŸ“çŠ¶æ€æŒ‡å¯¼åˆ†ç±»é‰´åˆ«ç³–å°¿ç—…è¶³æºƒç–¡æ„ŸæŸ“</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„ç³–å°¿ç—…è¶³æºƒç–¡æ„ŸæŸ“æ£€æµ‹æ¨¡å‹ï¼Œå³æ¡ä»¶å¼•å¯¼æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰</li><li>ConDiffç»“åˆäº†å¼•å¯¼å›¾åƒåˆæˆã€å»å™ªæ‰©æ•£æ¨¡å‹å’ŒåŸºäºè·ç¦»çš„åˆ†ç±»</li><li>é€šè¿‡åœ¨å¼•å¯¼å›¾åƒä¸­æ³¨å…¥é«˜æ–¯å™ªå£°å¹¶é€šè¿‡æ¡ä»¶åŒ–æ„ŸæŸ“çŠ¶æ€è¿›è¡Œé€†æ‰©æ•£å»å™ªåˆæˆå›¾åƒ</li><li>åŸºäºåˆæˆå›¾åƒä¸åŸå§‹å¼•å¯¼å›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å°æ¬§å‡ é‡Œå¾—è·ç¦»è¿›è¡Œæ„ŸæŸ“åˆ†ç±»</li><li>ä½¿ç”¨å…ƒç»„æŸå¤±å‡½æ•°åœ¨åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ä¸­å‡å°‘è¿‡æ‹Ÿåˆ</li><li>ConDiffåœ¨å‡†ç¡®æ€§å’ŒF1-scoreä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹</li><li>ConDiffå¼€åˆ›äº†ç”Ÿæˆå¼åˆ¤åˆ«æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç²¾ç»†åˆ†æä¸­çš„åº”ç”¨ï¼Œä¸ºæ”¹å–„æ‚£è€…é¢„åæä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: å¯¼å‘æ¡ä»¶æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰</p></li><li><p>Authors: Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</p></li><li><p>Affiliation: Worcesterç†å·¥å­¦é™¢</p></li><li><p>Keywords: ç³–å°¿ç—…è¶³æºƒç–¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºè·ç¦»çš„å›¾åƒåˆ†ç±»ï¼Œç”Ÿæˆæ¨¡å‹ï¼Œä¼¤å£æ„ŸæŸ“</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>ï¼ˆ1ï¼‰ï¼šç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ„ŸæŸ“æ˜¯å¯¼è‡´æˆªè‚¢å’Œä¸¥é‡å¹¶å‘ç—‡çš„ä¸»è¦åŸå› ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„DFUæ„ŸæŸ“æ£€æµ‹æ–¹æ³•å­˜åœ¨å‡†ç¡®ç‡ä½çš„é—®é¢˜ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ConDiffæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¼•å¯¼å›¾åƒåˆæˆã€å»å™ªæ‰©æ•£æ¨¡å‹å’ŒåŸºäºè·ç¦»çš„åˆ†ç±»ï¼Œé€šè¿‡ç”Ÿæˆå¼•å¯¼æ¡ä»¶åˆæˆå›¾åƒå¹¶è®¡ç®—åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„æœ€å°æ¬§å‡ é‡Œå¾—è·ç¦»æ¥å¯¹æ„ŸæŸ“è¿›è¡Œåˆ†ç±»ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šConDiffåœ¨DFUæ„ŸæŸ“æ•°æ®é›†ä¸Šå–å¾—äº†83%çš„å‡†ç¡®ç‡å’Œ0.858çš„F1åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æ–¹æ³•è‡³å°‘3%ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜DFUæ„ŸæŸ“è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šConDiff æ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šï¼ˆ1ï¼‰å¼•å¯¼æ‰©æ•£ï¼Œå³å‘ DFU å›¾åƒæ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œç„¶åæ ¹æ®æ„ŸæŸ“çŠ¶æ€ä»å™ªå£°æ‰°åŠ¨å›¾åƒä¸­é€æ­¥å»é™¤å™ªå£°ï¼Œä»¥åˆæˆæ¡ä»¶å›¾åƒï¼›ï¼ˆ2ï¼‰åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ï¼Œå³æ ¹æ®åŸå§‹å›¾åƒå’Œåˆæˆå›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å° L2 è·ç¦»é¢„æµ‹è¾“å…¥å›¾åƒçš„æ ‡ç­¾ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šConDiff åˆ©ç”¨æ¡ä»¶å¼•å¯¼å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å‘è¾“å…¥å›¾åƒæ³¨å…¥ç‰¹å®šå¼ºåº¦çš„ Gaussian å™ªå£°ï¼Œå¹¶ä½¿ç”¨åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥ä»å™ªå£°æ‰°åŠ¨è¾“å…¥å›¾åƒä¸­å»é™¤å™ªå£°æ¥ç”Ÿæˆæ–°å›¾åƒã€‚</p><p>ï¼ˆ3ï¼‰ï¼šConDiff çš„æ‰©æ•£è¿‡ç¨‹ä»¥ä¼¤å£çš„çŠ¶å†µï¼ˆæ— æ„ŸæŸ“ï¼ˆy1ï¼‰æˆ–æ„ŸæŸ“ï¼ˆy2ï¼‰ï¼‰ä¸ºæ¡ä»¶ï¼Œåˆ›å»ºåæ˜ è¿™äº›çŠ¶æ€çš„åˆæˆå›¾åƒã€‚ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ ConDiff èƒ½å¤Ÿé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„ L2 è·ç¦»åˆ†ç±»å™¨è¯†åˆ«å’Œå­¦ä¹ æ¡ä»¶ç”Ÿæˆå›¾åƒ Ë†xy 0 å’ŒåŸå§‹ä¼¤å£å›¾åƒ x0 ä¹‹é—´è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ã€‚äº§ç”Ÿä¸åŸå§‹å›¾åƒæœ€ç›¸ä¼¼çš„åˆæˆå›¾åƒçš„æ¡ä»¶è¢«é€‰ä½œé¢„æµ‹æ ‡ç­¾ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä¸æœ€å°åŒ–äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°çš„ä¼ ç»Ÿç›‘ç£åˆ†ç±»æŠ€æœ¯ä¸åŒï¼ŒConDiff é€šè¿‡åˆ©ç”¨ä¸‰å…ƒæŸå¤±å‡½æ•°æ¥å‡è½»è¿‡æ‹Ÿåˆï¼Œä»¥å¢åŠ éç›¸ä¼¼å›¾åƒå¯¹ä¹‹é—´çš„è·ç¦»å¹¶å‡å°‘ç›¸ä¼¼å›¾åƒå¯¹ä¹‹é—´çš„è·ç¦»ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šæœ¬ç ”ç©¶åˆ©ç”¨ Goyal ç­‰äººæä¾›çš„ DFU æ„ŸæŸ“æ•°æ®é›†ï¼ˆè§è¡¨ Iï¼‰ã€‚ä½†æ˜¯ï¼Œä¸ºäº†æ¶ˆé™¤è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ•°æ®æ³„æ¼ï¼Œæˆ‘ä»¬æ”¹è¿›äº†æ•°æ®é›†åˆ›å»ºå’Œæ‹†åˆ†ç­–ç•¥ã€‚ä½¿ç”¨åŸºäºä¸»é¢˜çš„æ‹†åˆ†ï¼Œä»…ä¸ºæ¯ä¸ªä¸»é¢˜ä½¿ç”¨ç¬¬äºŒä¸ªæ”¾å¤§è‡ªç„¶å¢å¼ºå›¾åƒï¼ˆå‚è§å›¾ 1ï¼‰ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šConDiff æ¡†æ¶çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº† Guided Conditional Diffusion Classifierï¼ˆConDiffï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†ç±»å—æ„ŸæŸ“ä¼¤å£å›¾åƒçš„é›†æˆç«¯åˆ°ç«¯æ¡†æ¶ã€‚ConDiff æ¡†æ¶æœ‰ 2 ä¸ªä¸»è¦éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰å¼•å¯¼æ‰©æ•£ï¼Œå³å‘ DFU å›¾åƒæ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œç„¶åæ ¹æ®æ„ŸæŸ“çŠ¶æ€ä»å™ªå£°æ‰°åŠ¨å›¾åƒä¸­é€æ­¥å»é™¤å™ªå£°ï¼Œä»¥åˆæˆæ¡ä»¶å›¾åƒï¼›ï¼ˆ2ï¼‰åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ï¼Œå³æ ¹æ®åŸå§‹å›¾åƒå’Œåˆæˆå›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å° L2 è·ç¦»é¢„æµ‹è¾“å…¥å›¾åƒçš„æ ‡ç­¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒConDiff æ˜¯ç¬¬ä¸€ä¸ªåˆ†æç»†ç²’åº¦ä¼¤å£å›¾åƒçš„ç”Ÿæˆåˆ¤åˆ«æ–¹æ³•ï¼Œä¿ƒè¿›äº†ç³–å°¿ç—…è¶³æºƒç–¡ (DFU) æ„ŸæŸ“çš„æ£€æµ‹ã€‚ï¼ˆ2ï¼‰åœ¨ DFU æ„ŸæŸ“æ•°æ®é›†çš„çœ‹ä¸è§çš„æµ‹è¯•ä¼¤å£å›¾åƒï¼ˆ148 ä¸ªå—æ„ŸæŸ“å’Œ 103 ä¸ªæœªå—æ„ŸæŸ“ï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ ConDiff æ¡†æ¶æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œæé«˜äº†ä¼¤å£æ„ŸæŸ“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œ F1 åˆ†æ•°è‡³å°‘ 3%ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åœ¨è®­ç»ƒæœŸé—´æœ€å°åŒ–ä¸‰å…ƒæŸå¤±å‡½æ•°ï¼ŒConDiff å‡å°‘äº†å¯¹ 1416 ä¸ªè®­ç»ƒå›¾åƒçš„å° DFU æ•°æ®é›†çš„è¿‡æ‹Ÿåˆã€‚ï¼ˆ4ï¼‰ç”± Score-CAM ç”Ÿæˆçš„çƒ­å›¾ç”¨äºç›´è§‚åœ°è¯´æ˜ ConDiff åœ¨å¯¹ä¼¤å£æ„ŸæŸ“çŠ¶æ€è¿›è¡Œåˆ†ç±»æ—¶ä¸“æ³¨äºæ­£ç¡®çš„ä¼¤å£å›¾åƒåŒºåŸŸã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å¼•å…¥äº†å¼•å¯¼æ¡ä»¶æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯¹ç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ„ŸæŸ“è¿›è¡Œåˆ†ç±»çš„æ–°æ¡†æ¶ã€‚ConDiff ä¼˜äºä¼ ç»Ÿæ¨¡å‹è‡³å°‘ 3%ï¼Œå‡†ç¡®ç‡é«˜è¾¾ 83%ï¼ŒF1 åˆ†æ•°ä¸º 0.858ã€‚å®ƒç‹¬ç‰¹çš„æ–¹æ³•åˆ©ç”¨ä¸‰å…ƒæŸå¤±è€Œä¸æ˜¯æ ‡å‡†çš„äº¤å‰ç†µæœ€å°åŒ–ï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œå‡å°‘äº†è¿‡æ‹Ÿåˆã€‚è¿™åœ¨æ•°æ®é›†é€šå¸¸å¾ˆå°çš„åŒ»å­¦æˆåƒä¸­å°¤å…¶é‡è¦ã€‚ConDiff é‡‡ç”¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå‘è¾“å…¥å›¾åƒä¸­æ·»åŠ ç‰¹å®šæ•°é‡çš„é«˜æ–¯å™ªå£°ï¼Œå¹¶é‡‡ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼çš„åå‘æ‰©æ•£ï¼Œæ ¹æ®åµŒå…¥ç©ºé—´ä¸­çš„æœ€è¿‘æ¬§å‡ é‡Œå¾—è·ç¦»å¯¹è¿™äº›å›¾åƒè¿›è¡Œè¿­ä»£ç»†åŒ–ä»¥è¿›è¡Œåˆ†ç±»ã€‚ConDiff çš„æœ‰æ•ˆæ€§è¡¨æ˜åœ¨æ”¹å–„ DFU ç®¡ç†æ–¹é¢å…·æœ‰æ˜¾ç€æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„åœ°åŒºã€‚å…¶ç²¾ç¡®çš„å®æ—¶æ„ŸæŸ“æ£€æµ‹å¯ä»¥åœ¨æ—©æœŸ DFU æ„ŸæŸ“è¯†åˆ«ä¸­å‘æŒ¥è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä»è€Œå‡å°‘è‚¢ä½“æˆªè‚¢ç­‰ä¸¥é‡å¹¶å‘ç—‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šConDiff æ˜¯ç¬¬ä¸€ä¸ªåˆ†æç»†ç²’åº¦ä¼¤å£å›¾åƒçš„ç”Ÿæˆåˆ¤åˆ«æ–¹æ³•ï¼Œä¿ƒè¿›äº† DFU æ„ŸæŸ“çš„æ£€æµ‹ï¼›æ€§èƒ½ï¼šåœ¨ DFU æ„ŸæŸ“æ•°æ®é›†çš„çœ‹ä¸è§çš„æµ‹è¯•ä¼¤å£å›¾åƒï¼ˆ148 ä¸ªå—æ„ŸæŸ“å’Œ 103 ä¸ªæœªå—æ„ŸæŸ“ï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼ŒConDiff æ¡†æ¶æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œæé«˜äº†ä¼¤å£æ„ŸæŸ“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œ F1 åˆ†æ•°è‡³å°‘ 3%ï¼›å·¥ä½œé‡ï¼šé€šè¿‡åœ¨è®­ç»ƒæœŸé—´æœ€å°åŒ–ä¸‰å…ƒæŸå¤±å‡½æ•°ï¼ŒConDiff å‡å°‘äº†å¯¹ 1416 ä¸ªè®­ç»ƒå›¾åƒçš„å° DFU æ•°æ®é›†çš„è¿‡æ‹Ÿåˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f38f851b08a13cd2762a9779abb3d5dd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1698e3895c14a21d1245d61cbbe4db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-902be7065fad826b29010fef3bd7e79b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3d5d8c1286e3aefa0a37934906ae34f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6a38755ae54c6bfd6a3359d2197b5a2.jpg" align="middle"></details><h2 id="Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><a href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation" class="headerlink" title="Obtaining Favorable Layouts for Multiple Object Generation"></a>Obtaining Favorable Layouts for Multiple Object Generation</h2><p><strong>Authors:Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</strong></p><p>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts. </p><p><a href="http://arxiv.org/abs/2405.00791v1">PDF</a> </p><p><strong>Summary</strong><br>éšç€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¤šä¸»ä½“ç”Ÿæˆæˆä¸ºæ¨¡å‹å‘å±•çš„é‡è¦æ­¥éª¤ã€‚æœ¬ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹å¤šä¸»ä½“ç”Ÿæˆä¸­çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¼•å¯¼åŸåˆ™è¿›è¡Œå¸ƒå±€è§„åˆ’çš„æ–°æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸»ä½“ç”Ÿæˆä¸­é¢ä¸´ç€é—æ¼æˆ–åˆå¹¶ä¸»ä½“çš„é—®é¢˜ã€‚</li><li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼•å¯¼åŸåˆ™çš„å¸ƒå±€è§„åˆ’æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹åˆå§‹æå‡ºå¸ƒå±€ï¼Œç„¶åå¯¹å…¶è¿›è¡Œé‡æ–°æ’åˆ—ã€‚</li><li>å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå¾ªæå‡ºçš„é®ç½©ï¼Œå¹¶å°†æ½œåœ¨å›¾ä¸­çš„åƒç´ è¿ç§»åˆ°æ–°ä½ç½®ã€‚</li><li>å¼•å…¥äº†æ–°çš„æŸå¤±é¡¹ï¼Œä»¥å‡å°‘ XAM ç†µã€å‡å°‘ XAM ä¹‹é—´çš„é‡å å¹¶ç¡®ä¿ XAM ä¸å„è‡ªçš„é®ç½©å¯¹é½ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´çœŸå®åœ°æ•æ‰åˆ°æ‰€éœ€æ¦‚å¿µã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: è·å¾—å¤šä¸ªå¯¹è±¡ç”Ÿæˆçš„æœ‰åˆ©å¸ƒå±€</p></li><li><p>Authors: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</p></li><li><p>Affiliation: å·´ä¼Šå…°å¤§å­¦å·¥ç¨‹å­¦é™¢</p></li><li><p>Keywords: æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ, å¤šå¯¹è±¡ç”Ÿæˆ, æ‰©æ•£æ¨¡å‹, äº¤å‰æ³¨æ„åŠ›å›¾</p></li><li><p>Paper: https://arxiv.org/abs/2405.00791 , Github: None</p></li><li><p>Summary:</p><p>(1): å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚è¿™äº›æ¨¡å‹æœ€ç»ˆæ—¨åœ¨åˆ›å»ºå¤æ‚çš„åœºæ™¯ï¼Œè§£å†³å¤šå¯¹è±¡ç”ŸæˆæŒ‘æˆ˜æ˜¯æœç€è¿™ä¸€ç›®æ ‡è¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å›¾åƒæ—¶é¢ä¸´å›°éš¾ã€‚å½“ç»™å®šåŒ…å«å¤šä¸ªå¯¹è±¡çš„æç¤ºæ—¶ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šçœç•¥ä¸€äº›å¯¹è±¡æˆ–å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰å¯¹ç”Ÿæˆå›¾åƒä¸­çš„ä¸åŒå¯¹è±¡è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼šå½“æç¤ºä¸­åŒ…å«å¤šä¸ªå¯¹è±¡æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šçœç•¥ä¸€äº›å¯¹è±¡æˆ–å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ã€‚è¿™æ˜¯é€šè¿‡å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå®ˆæå‡ºçš„æ©ç å¹¶é€šè¿‡å°†åƒç´ ä»æ½œåœ¨å›¾è¿ç§»åˆ°æˆ‘ä»¬ç¡®å®šçš„æ–°ä½ç½®æ¥å®ç°çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°çš„æŸå¤±é¡¹ï¼Œæ—¨åœ¨é™ä½ XAM ç†µä»¥æ›´æ¸…æ™°åœ°å®šä¹‰å¯¹è±¡çš„ç©ºé—´ï¼Œå‡å°‘ XAM ä¹‹é—´çš„é‡å ï¼Œå¹¶ç¡®ä¿ XAM ä¸å®ƒä»¬å„è‡ªçš„æ©ç å¯¹é½ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œå…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå®ˆæå‡ºçš„æ©ç å¹¶é€šè¿‡å°†åƒç´ ä»æ½œåœ¨å›¾è¿ç§»åˆ°ç¡®å®šçš„æ–°ä½ç½®æ¥å®ç°ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼•å…¥æ–°çš„æŸå¤±é¡¹ï¼Œæ—¨åœ¨é™ä½ XAM ç†µä»¥æ›´æ¸…æ™°åœ°å®šä¹‰å¯¹è±¡çš„ç©ºé—´ï¼Œå‡å°‘ XAM ä¹‹é—´çš„é‡å ï¼Œå¹¶ç¡®ä¿ XAM ä¸å®ƒä»¬å„è‡ªçš„æ©ç å¯¹é½ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ï¼Œä»è€Œæ›´å¿ å®åœ°æ•æ‰åˆ°å„ç§æ–‡æœ¬æç¤ºä¸­æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ã€‚æ€§èƒ½ï¼šåœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šéœ€è¦é’ˆå¯¹ä¸åŒçš„æ–‡æœ¬æç¤ºè¿›è¡Œå¾®è°ƒï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-561eb2e3b9534e1fe4b30e7ef897a8b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2973412c6cc5c19507315dc2dd5efcd.jpg" align="middle"></details><h2 id="Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><a href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models"></a>Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h2><p><strong>Authors:Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2. </p><p><a href="http://arxiv.org/abs/2405.00760v1">PDF</a> N/A</p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆ©ç”¨ç»™å®šçš„æ¿€åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–æ˜¯ä¸€ä¸ªé‡è¦ä½†æœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ç ”ç©¶é¢†åŸŸã€‚ç ”ç©¶ä¸­æå‡ºæ·±åº¦æ¿€åŠ±ä¼˜åŒ–ï¼ˆDRTuneï¼‰ï¼Œç®—æ³•ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒï¼Œå¹¶ä¸”é€šè¿‡è¿­ä»£é‡‡æ ·æµç¨‹å°†æ¢¯åº¦ä¼ å›è¾“å…¥å™ªå£°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹ä½å±‚æ¿€åŠ±ï¼Œè®­ç»ƒé‡‡æ ·æµç¨‹ä¸­çš„æ—©æœŸæ­¥éª¤è‡³å…³é‡è¦ã€‚</li><li>åœ¨å»å™ªç½‘ç»œè¾“å…¥å¤„åœæ­¢æ¢¯åº¦ï¼Œå¯ä»¥æœ‰æ•ˆå®ç°æ·±åº¦ç›‘ç£ã€‚</li><li>DRTune ç®—æ³•åœ¨å„ç§æ¿€åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ã€‚</li><li>DRTune ç®—æ³•å§‹ç»ˆä¼˜äºå…¶ä»–ç®—æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å¤±æ•ˆçš„ä½å±‚æ§åˆ¶ä¿¡å·ä¸­ã€‚</li><li>é€šè¿‡ DRTune ä¼˜åŒ– Human Preference Score v2.1ï¼Œå¯¹ Stable Diffusion XL 1.0ï¼ˆSDXL 1.0ï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œäº§ç”Ÿäº†æ›´å¥½çš„æ‰©æ•£ XL 1.0ï¼ˆFDXL 1.0ï¼‰æ¨¡å‹ã€‚</li><li>FDXL 1.0 ä¸ Midjourney v5.2 ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œè¾¾åˆ°äº†ç›¸å½“çš„æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šæ·±åº¦å¥–åŠ±ç›‘ç£å¾®è°ƒ</p></li><li><p>ä½œè€…ï¼šXiaoshi Wu<em>1,3, Yiming Hao</em>2, Manyuan Zhang1, Keqiang Sun1, Zhaoyang Huang3, Guanglu Song4, Yu Liu4, and Hongsheng Li1,2</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€æ·±åº¦å¥–åŠ±ç›‘ç£ã€å¾®è°ƒã€å›¾åƒè´¨é‡å¢å¼º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.00760v1</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¼˜åŒ–å…·æœ‰ç»™å®šå¥–åŠ±å‡½æ•°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ªé‡è¦ä½†å°šæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶é¢†åŸŸã€‚</p><p>(2)ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æµ…å±‚ç›‘ç£ï¼Œå³ä»…ç›‘ç£é‡‡æ ·è¿‡ç¨‹çš„æ—©æœŸæ­¥éª¤ã€‚ç„¶è€Œï¼Œå¯¹äºä½çº§å¥–åŠ±ä¿¡å·ï¼Œæµ…å±‚ç›‘ç£æ•ˆæœä¸ä½³ã€‚</p><p>(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºæ·±åº¦å¥–åŠ±å¾®è°ƒï¼ˆDRTuneï¼‰ç®—æ³•ï¼Œé€šè¿‡ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒå¹¶é€šè¿‡è¿­ä»£é‡‡æ ·è¿‡ç¨‹åå‘ä¼ æ’­åˆ°è¾“å…¥å™ªå£°æ¥å®ç°æ·±åº¦ç›‘ç£ã€‚</p><p>(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šDRTuneåœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ã€‚ä¸å…¶ä»–ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå§‹ç»ˆè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å‡å¤±è´¥çš„ä½çº§æ§åˆ¶ä¿¡å·æ–¹é¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨DRTuneå¾®è°ƒäº†Stable Diffusion XL 1.0ï¼ˆSDXL 1.0ï¼‰æ¨¡å‹ä»¥ä¼˜åŒ–äººç±»åå¥½è¯„åˆ†v2.1ï¼Œå¾—åˆ°äº†Favorable Diffusion XL 1.0ï¼ˆFDXL 1.0ï¼‰æ¨¡å‹ã€‚ä¸SDXL 1.0ç›¸æ¯”ï¼ŒFDXL 1.0æ˜¾ç€æé«˜äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”ä¸Midjourney v5.2ç›¸æ¯”è¾¾åˆ°äº†ç›¸å½“çš„è´¨é‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šDRTune ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒï¼Œå¹¶é€šè¿‡è¿­ä»£é‡‡æ ·è¿‡ç¨‹åå‘ä¼ æ’­åˆ°è¾“å…¥å™ªå£°æ¥å®ç°æ·±åº¦ç›‘ç£ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä¸ºäº†è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒDRTune é€šè¿‡é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦æ¥è§£å†³æ”¶æ•›é—®é¢˜ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¸ºäº†æé«˜æ•ˆç‡ï¼ŒDRTune é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦ï¼Œå¹¶è®­ç»ƒæ‰€æœ‰é‡‡æ ·æ­¥éª¤çš„å­é›†ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šDRTune ç®—æ³•çš„ä¼ªä»£ç å¦‚ä¸‹ï¼š</p><p><code>è¾“å…¥ï¼šé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æƒé‡ Î¸ã€å¥–åŠ± rã€è®­ç»ƒæ—¶é—´æ­¥é•¿ Kã€æ—©æœŸåœæ­¢æ—¶é—´æ­¥é•¿èŒƒå›´ mã€‚sg è¡¨ç¤ºæ¢¯åº¦åœæ­¢æ“ä½œã€‚while not converged do    ttrain = {1, ..., K} if DRaFT-K    ttrain = {i}iâ‰¥randint(1,T ) if AlignProp    if DRTune then        # ç­‰è·æ—¶é—´æ­¥é•¿ã€‚        s = randint(1, T âˆ’ KâŒŠ T K âŒ‹)        ttrain = {s + iâŒŠ T K âŒ‹ | i = 0, 1, ..., K âˆ’ 1}    if ReFL æˆ– DRTune then        tmin = randint(1, m)    else        tmin = 0    xT âˆ¼ N(0, I)    for t = T, ..., 1 do        if DRTune then            Ë†Ïµ = ÏµÎ¸(sg(xt), t)        else            Ë†Ïµ = ÏµÎ¸(xt, t)        if t /âˆˆ ttrain then            Ë†Ïµ = sg(Ë†Ïµ)        if t == tmin then            x0 â‰ˆ intermediate_prediction(xt, Ë†Ïµ)            break        xtâˆ’1 = atxt + btË†Ïµ + ctÏµ        g = âˆ‡Î¸r(x0, c)        Î¸ â† Î¸ âˆ’ Î·g</code></p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰åœ¨äºï¼Œå®ƒè§£å†³äº†åˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„åé¦ˆæ¥è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒäº†æ·±åº¦ç›‘ç£å¯¹äºä¼˜åŒ–å…¨å±€å¥–åŠ±çš„é‡è¦æ€§ï¼Œå¹¶ä½¿ç”¨åœæ­¢æ¢¯åº¦æŠ€æœ¯è§£å†³äº†æ”¶æ•›é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡å¾®è°ƒ FDXL 1.0 æ¨¡å‹å±•ç¤ºäº†å¥–åŠ±è®­ç»ƒçš„æ½œåŠ›ï¼Œä»¥å®ç°ä¸ Midjourney ç›¸å½“çš„å›¾åƒè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†æ·±åº¦å¥–åŠ±å¾®è°ƒï¼ˆDRTuneï¼‰ç®—æ³•ï¼Œå®ç°äº†æ·±åº¦ç›‘ç£ï¼Œå¹¶é€šè¿‡é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦æ¥è§£å†³æ”¶æ•›é—®é¢˜ã€‚æ€§èƒ½ï¼šDRTune åœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œä¸å…¶ä»–ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå§‹ç»ˆè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å‡å¤±è´¥çš„ä½çº§æ§åˆ¶ä¿¡å·æ–¹é¢ã€‚å·¥ä½œé‡ï¼šDRTune ç®—æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åº”ç”¨äºç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-037d3e48be185336859047a6292c8d27.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b417340b0b4ae8f5dcc966e5d18466d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d9661ec54a3560470071969dc361ea74.jpg" align="middle"><img src="https://pica.zhimg.com/v2-560c3c936da4c7000b08d87c1704852f.jpg" align="middle"></details><h2 id="Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><a href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution" class="headerlink" title="Detail-Enhancing Framework for Reference-Based Image Super-Resolution"></a>Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h2><p><strong>Authors:Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</strong></p><p>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes. </p><p><a href="http://arxiv.org/abs/2405.00431v1">PDF</a> </p><p><strong>Summary</strong><br>å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨é€šè¿‡å¼•å…¥é«˜åˆ†è¾¨ç‡å‚è€ƒå›¾åƒæ¥ç¼“è§£å•å›¾åƒè¶…åˆ†è¾¨çš„ç—…æ€é—®é¢˜ï¼Œä½†ç”±äºçº¹ç†ä¼ è¾“å‰å­˜åœ¨é”™ä½é—®é¢˜ï¼Œä»æœ‰æå‡ç©ºé—´ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ç»“æœä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚</li><li>ç°æœ‰æ–¹æ³•å¿½è§†äº†æ¯”è¾ƒä¸­ç»†èŠ‚çš„é‡è¦æ€§ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨ä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„ä¿¡æ¯ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼Œç”¨äºå¼•ç”¨å›¾åƒè¶…åˆ†è¾¨ã€‚</li><li>å¦‚æœå‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚</li><li>å¦‚æœå‚è€ƒå›¾åƒæ²¡æœ‰å¯¹åº”éƒ¨åˆ†ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç¡®ä¿åŸºæœ¬æ”¹è¿›ï¼ŒåŒæ—¶é¿å…å‚è€ƒå›¾åƒçš„å½±å“ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¿æŒå¯æ¯”æ•°å€¼ç»“æœçš„åŒæ—¶ï¼Œè·å¾—äº†æ›´å¥½çš„è§†è§‰æ•ˆæœã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰å¯¹åº”å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹æé«˜è¶…åˆ†è¾¨ç‡æ€§èƒ½ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥çµæ´»åœ°åº”ç”¨äºå„ç§å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨ä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>è®ºæ–‡æ ‡é¢˜ï¼šåŸºäºå‚è€ƒå›¾åƒçš„å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç»†èŠ‚å¢å¼ºæ¡†æ¶</p></li><li><p>ä½œè€…ï¼šZihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·å¾®ç³»ç»Ÿä¸ä¿¡æ¯æŠ€æœ¯ç ”ç©¶æ‰€</p></li><li><p>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå‚è€ƒå›¾åƒï¼Œç»†èŠ‚å¢å¼ºï¼Œæ‰©æ•£æ¨¡å‹</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºå‚è€ƒå›¾åƒçš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆRef-SRï¼‰å¾—åˆ°äº†è“¬å‹ƒå‘å±•ã€‚é€šè¿‡å°†é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å‚è€ƒå›¾åƒå¼•å…¥åˆ°å•å¹…å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ–¹æ³•ä¸­ï¼Œåœ¨å‚è€ƒå›¾åƒçº¹ç†çš„è¾…åŠ©ä¸‹ï¼Œç¼“è§£äº†è¿™ä¸€é•¿æœŸå­˜åœ¨çš„é¢†åŸŸçš„ç—…æ€æ€§è´¨ã€‚å°½ç®¡å®šé‡å’Œå®šæ€§ç»“æœçš„æ˜¾ç€æé«˜éªŒè¯äº† Ref-SR æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œä½†åœ¨çº¹ç†ä¼ è¾“ä¹‹å‰å­˜åœ¨çš„é”™ä½è¡¨æ˜è¿˜æœ‰è¿›ä¸€æ­¥æé«˜æ€§èƒ½çš„ç©ºé—´ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ¯”è¾ƒèƒŒæ™¯ä¸‹ç»†èŠ‚çš„é‡è¦æ€§ï¼Œå› æ­¤æ²¡æœ‰å……åˆ†åˆ©ç”¨ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒä¸­åŒ…å«çš„ä¿¡æ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å€¾å‘äºç®€å•åœ°å°†è¾“å…¥çš„ LR å›¾åƒè°ƒæ•´ä¸ºä¸ç›¸åº”å‚è€ƒå›¾åƒç›¸åŒçš„åˆ†è¾¨ç‡ï¼Œä¾‹å¦‚åŒä¸‰æ¬¡æ’å€¼ã€‚Lu ç­‰äººé€‰æ‹©å¯¹å‚è€ƒå›¾åƒè¿›è¡Œä¸‹é‡‡æ ·ä»¥é€‚åº”åŒ¹é…è¿‡ç¨‹ï¼Œç›®çš„æ˜¯é™ä½è®¡ç®—å¤æ‚åº¦ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£é”™ä½é—®é¢˜ï¼Œä½†å®ƒå¿½ç•¥äº†ç»†èŠ‚çš„å¢å¼ºï¼Œå¯èƒ½ä¼šç ´ååç»­çš„å›¾åƒæ¢å¤ç»“æœã€‚åœ¨å‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œç°æœ‰çš„æ–¹æ³•æ— æ³•ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œå®ƒå¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼º LR å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚å¦‚æœå‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚åœ¨å‚è€ƒå›¾åƒä¸­ç¼ºå°‘å¯¹åº”éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œå®ƒç¡®ä¿äº†æ ¹æœ¬æ€§çš„æ”¹è¿›ï¼ŒåŒæ—¶é¿å…äº†å‚è€ƒå›¾åƒçš„å½±å“ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„è§†è§‰æ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„æ•°å€¼ç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œå®ƒå¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼º LR å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šä½¿ç”¨ deformable convolution networkï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼ºä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚åœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚åœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚åˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨å¯å˜å½¢å·ç§¯ç½‘ç»œï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå’Œå¢å¼ºä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚åœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚åœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚åˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨å¯å˜å½¢å·ç§¯ç½‘ç»œï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„è§†è§‰æ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„æ•°å€¼ç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œå¯å˜å½¢å·ç§¯ç½‘ç»œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-03b9463baa117efca1717d3d158fe273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c3af50396285ae462ddd151feecf5dad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a59353ef4d615d00935a00b86d496d8.jpg" align="middle"></details><h2 id="ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><a href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning" class="headerlink" title="ASAM: Boosting Segment Anything Model with Adversarial Tuning"></a>ASAM: Boosting Segment Anything Model with Adversarial Tuning</h2><p><strong>Authors:Bo Li, Haoke Xiao, Lv Tang</strong></p><p>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAMâ€™s performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in <a href="https://asam2024.github.io/">https://asam2024.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2405.00256v1">PDF</a> This paper is accepted by CVPR2024</p><p><strong>Summary</strong><br>Meta AIçš„ASAMé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒå¢å¼ºäº†SAMå›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¶æ„è°ƒæ•´å³å¯æå‡åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ASAM é‡‡ç”¨å¯¹æŠ—è®­ç»ƒæ¥å¢å¼º SAM å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¶æ„ä¿®æ”¹ã€‚</li><li>è‡ªç„¶å¯¹æŠ—å®ä¾‹æé«˜äº†æ¨¡å‹å¯¹è‡ªç„¶å˜åŒ–çš„é²æ£’æ€§ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ä¸å¯æ„ŸçŸ¥æ‰°åŠ¨ã€‚</li><li>ASAM ä¿æŒäº†å¯¹æŠ—å®ä¾‹çš„é€¼çœŸåº¦å¹¶ä¸åŸå§‹æ©ç æ³¨é‡Šä¿æŒä¸€è‡´ï¼Œä»è€Œä¿æŒåˆ†å‰²ä»»åŠ¡çš„å®Œæ•´æ€§ã€‚</li><li>å¾®è°ƒåçš„ ASAM åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾ç€æå‡ï¼Œåœ¨ SA-1B æ•°æ®é›†ä¸Šè¾¾åˆ° 88.2% çš„ mIoUã€‚</li><li>ASAM åœ¨ ADE20K æ•°æ®é›†ä¸Šè¾¾åˆ° 50.1% çš„ mIoUï¼Œè¶…è¿‡äº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>ASAM åœ¨ COCO Stuff æ•°æ®é›†ä¸Šè¾¾åˆ° 34.6% çš„ mIoUï¼Œåœ¨ Cityscapes æ•°æ®é›†ä¸Šè¾¾åˆ° 81.2% çš„ mIoUã€‚</li><li>ASAM æ¨è¿›äº†è®¡ç®—æœºè§†è§‰ä¸­åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†å¯¹æŠ—æ€§è®­ç»ƒåœ¨å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ASAMï¼šåŸºäºå¯¹æŠ—è°ƒä¼˜çš„ Segment Anything æ¨¡å‹å¢å¼º</p></li><li><p>Authors: Bo Li, Haoke Xiao, Lv Tang</p></li><li><p>Affiliation: vivo Mobile Communication Co., Ltd</p></li><li><p>Keywords: Adversarial Tuning, Image Segmentation, Foundation Model, Segment Anything Model, Stable Diffusion</p></li><li><p>Urls: https://arxiv.org/abs/2405.00256, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): éšç€è®¡ç®—æœºè§†è§‰çš„å‘å±•ï¼ŒåŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºå…³é”®å·¥å…·ï¼Œå®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„é€‚åº”æ€§ã€‚å…¶ä¸­ï¼ŒMeta AI çš„ Segment Anything Model (SAM) åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸè¡¨ç°çªå‡ºã€‚ç„¶è€Œï¼ŒSAM ä¸å…¶ä»–åŒç±»æ¨¡å‹ä¸€æ ·ï¼Œåœ¨ç‰¹å®šç»†åˆ†åº”ç”¨ä¸­é‡åˆ°äº†å±€é™æ€§ï¼Œè¿™ä¿ƒä½¿äººä»¬å¯»æ±‚å¢å¼ºç­–ç•¥ï¼Œè€Œä¸ä¼šæŸå®³å…¶å›ºæœ‰èƒ½åŠ›ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬å¾®è°ƒå’Œé€‚é…å™¨æ¨¡å—ï¼Œä½†å¾®è°ƒä¼šæŸå®³ SAM çš„å›ºæœ‰æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™éœ€è¦é¢å¤–çš„é€‚é…å±‚æˆ–åå¤„ç†æ¨¡å—ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• ASAMï¼Œå®ƒé€šè¿‡å¯¹æŠ—è°ƒä¼˜æ¥å¢å¼º SAM çš„æ€§èƒ½ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è‡ªç„¶å¯¹æŠ—æ ·æœ¬æˆåŠŸå®ç°çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº† SA-1B æ•°æ®é›†çš„å­é›†ï¼ˆ1%ï¼‰ï¼Œç”Ÿæˆäº†æ›´èƒ½ä»£è¡¨è‡ªç„¶å˜åŒ–çš„å¯¹æŠ—å®ä¾‹ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ä¸å¯æ„ŸçŸ¥æ‰°åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†å¯¹æŠ—æ ·æœ¬çš„çœŸå®æ„Ÿï¼Œå¹¶ç¡®ä¿äº†ä¸åŸå§‹æ©ç æ³¨é‡Šçš„ä¸€è‡´æ€§ï¼Œä»è€Œä¿ç•™äº†åˆ†å‰²ä»»åŠ¡çš„å®Œæ•´æ€§ã€‚</p><p>(4): åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç»è¿‡å¾®è°ƒçš„ ASAM å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œæ— éœ€é¢å¤–çš„æ•°æ®æˆ–æ¶æ„ä¿®æ”¹ã€‚æˆ‘ä»¬å¹¿æ³›è¯„ä¼°çš„ç»“æœè¯å®ï¼ŒASAM åœ¨åˆ†å‰²ä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä»è€Œä¿ƒè¿›äº†è®¡ç®—æœºè§†è§‰ä¸­åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º ASAM æ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—è°ƒä¼˜å¢å¼º SAM æ¨¡å‹çš„æ€§èƒ½ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼º SA-1B æ•°æ®é›†çš„å­é›†ï¼Œç”Ÿæˆæ›´å…·ä»£è¡¨æ€§çš„å¯¹æŠ—å®ä¾‹ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¿æŒå¯¹æŠ—æ ·æœ¬çš„çœŸå®æ„Ÿï¼Œç¡®ä¿ä¸åŸå§‹æ©ç æ³¨é‡Šçš„ä¸€è‡´æ€§ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¯„ä¼° ASAMï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡åˆ›æ–°æ€§åœ°ä½¿ç”¨å¯¹æŠ—è°ƒä¼˜ï¼Œæå‡ºçš„ ASAM æ–¹æ³•ä»£è¡¨äº† SAM çš„é‡å¤§è¿›æ­¥ã€‚åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹å¢å¼º SA-1B æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è‡ªç„¶ã€é€¼çœŸçš„å¯¹æŠ—å›¾åƒï¼Œä»è€Œå¤§å¹…æå‡äº† SAM åœ¨å„ç§ä»»åŠ¡ä¸­çš„åˆ†å‰²èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº† NLP ä¸­å¯¹æŠ—è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨ä¿æŒ SAM åŸç”Ÿæ¶æ„å’Œé›¶æ ·æœ¬ä¼˜åŠ¿çš„åŒæ—¶å¢å¼ºäº†å…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒASAM ä¸ä»…åœ¨åˆ†å‰²ä»»åŠ¡ä¸­æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œè€Œä¸”ä¿ƒè¿›äº†å¯¹æŠ—æ ·ä¾‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨å’Œç†è§£ï¼Œä¸ºæå‡å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—å®ä¾‹ï¼Œå¢å¼º SAM çš„åˆ†å‰²èƒ½åŠ›ï¼›ä¿æŒ SAM çš„åŸç”Ÿæ¶æ„å’Œé›¶æ ·æœ¬ä¼˜åŠ¿ï¼›æ€§èƒ½ï¼šåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ï¼›å·¥ä½œé‡ï¼šä¸å¾®è°ƒå’Œé€‚é…å™¨æ¨¡å—ç­‰å…¶ä»–å¢å¼ºç­–ç•¥ç›¸æ¯”ï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå°ï¼Œæ— éœ€é¢å¤–çš„æ¶æ„ä¿®æ”¹æˆ–åå¤„ç†æ¨¡å—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e7684baf385865b289b9bd3b4babea56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7c0e935c2de944340eb5085a5356da42.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d93133fcc44a60510ee9cb1385d6be69.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-06T10:03:07.000Z</published>
    <updated>2024-05-06T10:03:07.018Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation"><a href="#X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation" class="headerlink" title="X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation"></a>X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</h2><p><strong>Authors:Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</strong></p><p>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry-&gt;Texture-&gt;Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: <a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/">https://xmu-xiaoma666.github.io/Projects/X-Oscar/</a>. </p><p><a href="http://arxiv.org/abs/2405.00954v1">PDF</a> ICML2024</p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºçš„ X-Oscar æ¡†æ¶å¯ä»¥ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»å¤´åƒï¼Œå®ƒé‡‡ç”¨å‡ ä½•-&gt;çº¹ç†-&gt;åŠ¨ç”»çš„é¡ºåºèŒƒå¼ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”å˜å¼‚å‚æ•°å’ŒåŸºäºå¤´åƒçš„è¯„åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯æ¥è§£å†³è¿‡é¥±å’Œå’Œä½è´¨é‡è¾“å‡ºçš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>X-Oscar æ˜¯ä¸€ä¸ªæ¸è¿›å¼æ¡†æ¶ï¼Œä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»å¤´åƒã€‚</li><li>å®ƒé‡‡ç”¨é¡ºåºçš„å‡ ä½•-&gt;çº¹ç†-&gt;åŠ¨ç”»èŒƒå¼ï¼Œç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ã€‚</li><li>è‡ªé€‚åº”å˜å¼‚å‚æ•°å°†å¤´åƒè¡¨ç¤ºä¸ºè®­ç»ƒæœŸé—´çš„è‡ªé€‚åº”åˆ†å¸ƒï¼Œä»¥è§£å†³è¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>åŸºäºå¤´åƒçš„è¯„åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯å°†åŸºäºå¤´åƒçš„å™ªå£°èå…¥æ¸²æŸ“å›¾åƒï¼Œä»¥æé«˜ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç”Ÿæˆè´¨é‡ã€‚</li><li>å¹¿æ³›çš„è¯„ä¼°è¯å® X-Oscar ä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ° 3D å’Œæ–‡æœ¬åˆ°å¤´åƒæ–¹æ³•ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/ã€‚">https://xmu-xiaoma666.github.io/Projects/X-Oscar/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>è®ºæ–‡æ ‡é¢˜ï¼šX-Oscarï¼šä¸€ä¸ªç”¨äºç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬å¼•å¯¼å¼3Då¯åŠ¨ç”»è§’è‰²çš„æ¸è¿›å¼æ¡†æ¶</li><p></p><p></p><li>ä½œè€…ï¼šYiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¦é—¨å¤§å­¦å¤šåª’ä½“å¯ä¿¡æ„ŸçŸ¥ä¸é«˜æ•ˆè®¡ç®—æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤</li><p></p><p></p><li>å…³é”®è¯ï¼šæ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆã€æ¸è¿›å¼ç”Ÿæˆã€è‡ªé€‚åº”å˜åˆ†å‚æ•°ã€è§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2405.00954.pdf ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šéšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼Œ3Däººä½“é‡å»ºé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¸“æ³¨äºä»è§†è§‰çº¿ç´¢é‡å»ºäººä½“ï¼Œéš¾ä»¥æ»¡è¶³èå…¥åˆ›é€ åŠ›ã€ç¼–è¾‘å’Œæ§åˆ¶çš„éœ€æ±‚ã€‚ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆæ–¹æ³•å­˜åœ¨è¿‡é¥±å’Œå’Œè¾“å‡ºè´¨é‡ä½çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼æ¡†æ¶X-Oscarï¼Œé€šè¿‡â€œå‡ ä½•â†’çº¹ç†â†’åŠ¨ç”»â€çš„é¡ºåºç”Ÿæˆæ¨¡å¼ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”å˜åˆ†å‚æ•°å’Œè§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯ï¼Œæ¥è§£å†³è¿‡é¥±å’Œé—®é¢˜å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨æ–‡æœ¬åˆ°3Då’Œæ–‡æœ¬åˆ°è§’è‰²ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒX-Oscaråœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ¨ç”»ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºæ¸è¿›å¼æ¡†æ¶X-Oscarï¼Œé€šè¿‡â€œå‡ ä½•â†’çº¹ç†â†’åŠ¨ç”»â€çš„é¡ºåºç”Ÿæˆæ¨¡å¼ï¼Œè§£å†³è¿‡é¥±å’Œé—®é¢˜å¹¶æé«˜ç”Ÿæˆè´¨é‡ï¼›ï¼ˆ2ï¼‰ï¼šå¼•å…¥è‡ªé€‚åº”å˜åˆ†å‚æ•°ï¼ˆAVPï¼‰ï¼Œé‡‡ç”¨å¯è®­ç»ƒçš„è‡ªé€‚åº”åˆ†å¸ƒè¡¨ç¤ºè™šæ‹Ÿå½¢è±¡ï¼Œè§£å†³è™šæ‹Ÿå½¢è±¡ç”Ÿæˆä¸­å¸¸è§çš„è¿‡é¥±å’Œé—®é¢˜ï¼›ï¼ˆ3ï¼‰ï¼šæå‡ºè™šæ‹Ÿå½¢è±¡æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆASDSï¼‰ï¼Œå°†å‡ ä½•æ„ŸçŸ¥å’Œå¤–è§‚æ„ŸçŸ¥å™ªå£°èå…¥å»å™ªè¿‡ç¨‹ä¸­ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥ç”Ÿæˆè™šæ‹Ÿå½¢è±¡çš„å½“å‰çŠ¶æ€ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„X-Oscaræ¡†æ¶åœ¨æ–‡æœ¬å¼•å¯¼å¼3Då¯åŠ¨ç”»è§’è‰²ç”Ÿæˆé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šX-Oscaræ¡†æ¶åˆ›æ–°æ€§åœ°æå‡ºäº†æ¸è¿›å¼ç”Ÿæˆæ¨¡å¼ã€è‡ªé€‚åº”å˜åˆ†å‚æ•°å’Œè§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†æ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆä¸­å­˜åœ¨çš„è¿‡é¥±å’Œé—®é¢˜å’Œè¾“å‡ºè´¨é‡ä½çš„é—®é¢˜ã€‚</p><p>æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°3Då’Œæ–‡æœ¬åˆ°è§’è‰²ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒX-Oscaræ¡†æ¶åœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ¨ç”»ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šX-Oscaræ¡†æ¶çš„å®ç°éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¯¹äºæ™®é€šç”¨æˆ·æ¥è¯´ï¼Œä½¿ç”¨å’Œéƒ¨ç½²å¯èƒ½å­˜åœ¨ä¸€å®šçš„éš¾åº¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4f631630c69e7fc1f5a8d28fd426ba1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aa682fc730b8fcb4e568e48a58c3a47.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  X-Oscar A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/</id>
    <published>2024-05-02T03:18:37.000Z</published>
    <updated>2024-05-02T03:18:37.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration"><a href="#NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration" class="headerlink" title="NeRF-Guided Unsupervised Learning of RGB-D Registration"></a>NeRF-Guided Unsupervised Learning of RGB-D Registration</h2><p><strong>Authors:Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</strong></p><p>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction. </p><p><a href="http://arxiv.org/abs/2405.00507v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF-UR æå‡ºäº†ä¸€ç§å¸§åˆ°æ¨¡å‹çš„ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ï¼Œåˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œä»¥æé«˜å¤šè§†å›¾ä¸€è‡´æ€§å·®æ—¶çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ— ç›‘ç£ RGB-D é…å‡†çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URã€‚</li><li>ä½¿ç”¨ NeRF ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œæé«˜äº†å¤šè§†å›¾ä¸€è‡´æ€§å·®æ—¶çš„é²æ£’æ€§ã€‚</li><li>åˆ›å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›† Sim-RGBDï¼Œé€šè¿‡åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£å¾®è°ƒï¼Œå°†ç‰¹å¾æå–å’Œæ³¨å†Œçš„èƒ½åŠ›ä»ä»¿çœŸè½¬ç§»åˆ°ç°å®ã€‚</li><li>åœ¨ ScanNet å’Œ 3DMatch æ•°æ®é›†ä¸Šï¼ŒNeRF-UR ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€ï¼Œä»¥æ–¹ä¾¿è®ºæ–‡å¤ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>è®ºæ–‡æ ‡é¢˜ï¼šNeRFå¼•å¯¼çš„RGB-Dé…å‡†æ— ç›‘ç£å­¦ä¹ </li><p></p><p></p><li>ä½œè€…ï¼šZhinan Yu1âˆ—, Zheng Qin2âˆ—, Yijie Tang1, Yongjun Wang1, Renjiao Yi1, Chenyang Zhu1, and Kai Xu1â€ </li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå›½é˜²ç§‘æŠ€å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šRGB-Dé…å‡†Â·æ— ç›‘ç£å­¦ä¹ Â·NeRF</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šxxx</li><p></p><p></p><li>æ‘˜è¦ï¼š</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€RGB-Dä¼ æ„Ÿå™¨çš„æ™®åŠå’Œæˆæœ¬çš„é™ä½ï¼Œ3Dæ•°æ®é‡‡é›†çš„éš¾åº¦å·²å¤§å¤§é™ä½ã€‚RGB-Dæ•°æ®çš„å¹¿æ³›æ”¶é›†æå¤§åœ°æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ åœ¨3Dè§†è§‰é¢†åŸŸçš„è¿›æ­¥ï¼Œä»è€Œæå¤§åœ°æé«˜äº†RGB-D SLAMå’ŒRGB-Dé‡å»ºç­‰åº”ç”¨çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RGB-Dé…å‡†æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºå¯å¾®æ¸²æŸ“çš„æˆå¯¹è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¼šå› å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå…‰ææ–™ç­‰å› ç´ è€Œå¯¼è‡´å¤šè§†å›¾ä¸€è‡´æ€§è¾ƒå·®ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„RGB-Dé…å‡†æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºå¯å¾®æ¸²æŸ“çš„æˆå¯¹è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¼šå› å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå…‰ææ–™ç­‰å› ç´ è€Œå¯¼è‡´å¤šè§†å›¾ä¸€è‡´æ€§è¾ƒå·®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶NeRF-URï¼Œç”¨äºæ— ç›‘ç£RGB-Dé…å‡†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’ŒNeRFé‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§è¿›è¡Œä½å§¿ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼•å¯¼NeRFä¼˜åŒ–ï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªé€šè¿‡é€¼çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åˆæˆæ•°æ®é›†Sim-RGBDï¼Œç”¨äºé¢„çƒ­é…å‡†æ¨¡å‹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæµè¡Œçš„å®¤å†…RGB-Dæ•°æ®é›†ScanNetå’Œ3DMatchä¸Šä¼˜äºæœ€å…ˆè¿›çš„åŒç±»æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§è¿›è¡Œä½å§¿ä¼˜åŒ–ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ›å»ºé€šè¿‡é€¼çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åˆæˆæ•°æ®é›† Sim-RGBDï¼Œç”¨äºé¢„çƒ­é…å‡†æ¨¡å‹ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨ PointMBF ä½œä¸ºé…å‡†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èåˆäº†è§†è§‰ï¼ˆ2Dï¼‰å’Œå‡ ä½•ï¼ˆ3Dï¼‰ç©ºé—´çš„ä¿¡æ¯ä»¥è·å¾—æ›´å¥½çš„ç‰¹å¾è¾¨åˆ«åŠ›ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šæå‡º NeRF å¼•å¯¼çš„æ— ç›‘ç£é…å‡†ç®¡é“ï¼Œè¯¥ç®¡é“ä¾èµ–äºé«˜è´¨é‡çš„ä½å§¿æ¥ç›‘ç£é…å‡†æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ NeRF ä¼˜åŒ–å¸§ä½å§¿ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨ NeRF æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¯¹åœºæ™¯ä¸­çš„å…‰ç…§å’Œå‡ ä½•ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è”åˆä¼˜åŒ– 3D åœ°å›¾å’Œä½å§¿ï¼›</p><p>ï¼ˆ7ï¼‰ï¼šå°† RGB-D åºåˆ—åˆ†å‰²æˆå­åºåˆ—ï¼Œå¹¶ä¸ºæ¯ä¸ªå­åºåˆ—ä¼˜åŒ–ä¸€ä¸ª NeRFï¼Œä»¥é¿å…è”åˆåœ°å›¾ä½å§¿ä¼˜åŒ–å¸¦æ¥çš„è¯¯å·®ç´¯ç§¯å’Œå·¨å¤§çš„æ—¶é—´å¼€é”€ï¼›</p><p>ï¼ˆ8ï¼‰ï¼šé€šè¿‡ Sim-RGBD æ•°æ®é›†å¯¹é…å‡†æ¨¡å‹è¿›è¡Œå¼•å¯¼ï¼Œä»¥æä¾›åˆç†çš„åˆå§‹ä½å§¿ï¼Œè§£å†³éšæœºåˆå§‹åŒ–é…å‡†æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¤§é‡å¼‚å¸¸å¯¹åº”å…³ç³»çš„é—®é¢˜ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1):è¯¥å·¥ä½œçš„æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº† NeRF-URï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ— ç›‘ç£ RGB-D é…å‡†çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§æ¥ä¼˜åŒ–é…å‡†æ¨¡å‹ä¼°è®¡çš„ä½å§¿ã€‚è¿™ç§è®¾è®¡å¯ä»¥æœ‰æ•ˆæé«˜å¯¹å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå°„ææ–™çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨åˆæˆæ•°æ®é›†ä¸Šè®¾è®¡äº†ä¸€ä¸ªå¼•å¯¼æœºåˆ¶æ¥é¢„çƒ­ NeRF ä¼˜åŒ–ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒNeRF å¼•å¯¼çš„æ— ç›‘ç£å­¦ä¹ æ˜¯ 3D è§†è§‰çš„ä¸€ç§æœ‰å‰é€”çš„æœºåˆ¶ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å°†å…¶æ‰©å±•åˆ°æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚å®šä½ã€é‡å»ºç­‰ã€‚</p><p><strong>(2):æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ã€‚</li><li>åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§æ¥ä¼˜åŒ–ä½å§¿ã€‚</li><li>è®¾è®¡äº†ä¸€ç§åˆæˆæ•°æ®é›†ä¸Šçš„å¼•å¯¼æœºåˆ¶æ¥é¢„çƒ­ NeRF ä¼˜åŒ–ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨ä¸¤ä¸ªæµè¡Œçš„å®¤å†… RGB-D æ•°æ®é›† ScanNet å’Œ 3DMatch ä¸Šä¼˜äºæœ€å…ˆè¿›çš„åŒç±»æ–¹æ³•ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>éœ€è¦é¢„å…ˆè®­ç»ƒ NeRF æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</li><li>ä¼˜åŒ– NeRF å’Œé…å‡†æ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e47f5a6a35637f1b5b56609633d65083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d72291aca2a21454f9a83d46a2633ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-caf6df85382bbbd1a4f390f7bbbc79cb.jpg" align="middle"></details>## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions**Authors:Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan**Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. [PDF](http://arxiv.org/abs/2404.19015v1) The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955**Summary**ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åœºæ™¯çš„é€¼çœŸè‡ªç”±è§†ç‚¹æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ€è¿‘å¯¹ NeRF çš„æ”¹è¿›ï¼Œå¦‚ TensoRF å’Œ ZipNeRFï¼Œé‡‡ç”¨äº†æ˜¾å¼æ¨¡å‹ä»¥å®ç°æ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œè€Œ NeRF åˆ™é‡‡ç”¨äº†éšå¼è¡¨ç¤ºã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼çš„è¾å°„åœºéƒ½éœ€è¦å¯¹ç»™å®šåœºæ™¯ä¸­çš„å›¾åƒè¿›è¡Œå¯†é›†é‡‡æ ·ã€‚å½“åªæœ‰ç¨€ç–çš„è§†å›¾é›†åˆå¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œç›‘ç£è¾å°„åœºä¼°è®¡çš„æ·±åº¦æœ‰åŠ©äºä½¿ç”¨æ›´å°‘çš„è§†å›¾æœ‰æ•ˆåœ°è®­ç»ƒå®ƒã€‚æ·±åº¦ç›‘ç£æ˜¯ä½¿ç”¨ç»å…¸æ–¹æ³•æˆ–åœ¨å¤§æ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„ç¥ç»ç½‘ç»œè·å¾—çš„ã€‚è™½ç„¶å‰è€…å¯èƒ½åªæä¾›ç¨€ç–ç›‘ç£ï¼Œä½†åè€…å¯èƒ½å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚ä¸æ—©æœŸçš„æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬å¯»æ±‚é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶åœ¨ä¸»è¾å°„åœºä¸­è®­ç»ƒå®ƒä»¬æ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªæ­£åˆ™åŒ–æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨ä¸åŒçš„éšå¼å’Œæ˜¾å¼è¾å°„åœºä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥æƒ…å†µä¸‹è¿‡åº¦æ‹Ÿåˆè§‚æµ‹åˆ°çš„å›¾åƒã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯ï¼Œé™ä½è¾å°„åœºç›¸å¯¹äºä½ç½®ç¼–ç ã€åˆ†è§£å¼ é‡åˆ†é‡æ•°æˆ–å“ˆå¸Œè¡¨å¤§å°çš„èƒ½åŠ›ï¼Œä¼šé™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œåœ¨æŸäº›åŒºåŸŸä¼°è®¡æ›´å¥½çš„æ·±åº¦ã€‚é€šè¿‡åŸºäºè¿™ç§é™ä½çš„èƒ½åŠ›è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´å¥½çš„ä¸»è¾å°„åœºæ·±åº¦ç›‘ç£ã€‚é€šè¿‡ä½¿ç”¨ä¸Šè¿°æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬åœ¨åŒ…å«æœå‰å’Œ 360 åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šä»¥ç¨€ç–è¾“å…¥è§†å›¾å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚**Key Takeaways**-  å‡å°‘NeRFæ¨¡å‹å¤æ‚æ€§æœ‰åŠ©äºå­¦ä¹ æ›´å¥½çš„æ·±åº¦ï¼Œæœ‰åˆ©äºåˆ©ç”¨ç¨€ç–è§†å›¾è¿›è¡Œè®­ç»ƒã€‚-  è®¾è®¡å¢å¼ºæ¨¡å‹ï¼ŒåŸºäºé™ä½NeRFæ¨¡å‹èƒ½åŠ›è·å¾—æ”¹è¿›çš„æ·±åº¦ç›‘ç£ã€‚-  æ­£åˆ™åŒ–æ¡†æ¶å¯ä»¥åº”ç”¨äºä¸åŒç±»å‹NeRFæ¨¡å‹ï¼ŒåŒ…æ‹¬éšå¼å’Œæ˜¾å¼æ¨¡å‹ã€‚-  è¿‡åº¦æ‹Ÿåˆæ˜¯ç¨€ç–è§†å›¾è¾“å…¥NeRFè®­ç»ƒä¸­çš„ä¸»è¦é—®é¢˜ã€‚-  æ·±åº¦ç›‘ç£å¯ä»¥ä¿ƒè¿›NeRFæ¨¡å‹ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚-  ç»å…¸æ–¹æ³•å’Œç¥ç»ç½‘ç»œéƒ½å¯ä»¥ç”¨äºæ·±åº¦ç›‘ç£ï¼Œä½†å„æœ‰ä¼˜ç¼ºç‚¹ã€‚-  åœ¨æœå‰å’Œ360åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ç®€åŒ–å°„çº¿åœºï¼šç”¨æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆæ­£åˆ™åŒ–ç¨€ç–è¾“å…¥çš„è¾å°„åœº</p></li><li><p>Authors: Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan</p></li><li><p>Affiliation: å°åº¦ç§‘å­¦é™¢</p></li><li><p>Keywords: ç¥ç»è¾å°„åœº, ç¨€ç–è¾“å…¥, æ­£åˆ™åŒ–, æ·±åº¦ä¼°è®¡</p></li><li><p>Urls: https://arxiv.org/abs/2404.19015, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): ç¥ç»è¾å°„åœº(NeRF)åœ¨åœºæ™¯çš„çœŸå®æ„Ÿè‡ªç”±è§†è§’æ¸²æŸ“ä¸­è¡¨ç°å‡ºè‰²ã€‚NeRFçš„æœ€æ–°æ”¹è¿›ï¼Œå¦‚TensoRFå’ŒZipNeRFï¼Œé‡‡ç”¨äº†æ˜¾å¼æ¨¡å‹ä»¥å®ç°æ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œè€ŒNeRFåˆ™é‡‡ç”¨éšå¼è¡¨ç¤ºã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼è¾å°„åœºéƒ½éœ€è¦å¯¹ç»™å®šåœºæ™¯ä¸­çš„å›¾åƒè¿›è¡Œå¯†é›†é‡‡æ ·ã€‚å½“åªæœ‰ç¨€ç–çš„è§†å›¾é›†å¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œç›‘ç£è¾å°„åœºä¼°è®¡çš„æ·±åº¦æœ‰åŠ©äºæœ‰æ•ˆåœ°ä½¿ç”¨æ›´å°‘çš„è§†å›¾å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚æ·±åº¦ç›‘ç£æ˜¯ä½¿ç”¨ç»å…¸æ–¹æ³•æˆ–åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œè·å¾—çš„ã€‚è™½ç„¶å‰è€…å¯èƒ½åªæä¾›ç¨€ç–ç›‘ç£ï¼Œä½†åè€…å¯èƒ½å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚</p><p>(2): ä¸æ—©æœŸçš„ç ”ç©¶æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶å°†å…¶ä¸ä¸»è¾å°„åœºä¸€èµ·è®­ç»ƒæ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªæ­£åˆ™åŒ–æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸åŒçš„éšå¼å’Œæ˜¾å¼è¾å°„åœºä¸­å·¥ä½œã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥åœºæ™¯ä¸­è¿‡åº¦æ‹Ÿåˆè§‚æµ‹å›¾åƒã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯ï¼Œåœ¨ä½ç½®ç¼–ç ã€åˆ†è§£çš„å¼ é‡åˆ†é‡æ•°æˆ–å“ˆå¸Œè¡¨å¤§å°æ–¹é¢é™ä½è¾å°„åœºçš„æ€§èƒ½ï¼Œä¼šé™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œåœ¨æŸäº›åŒºåŸŸä¼°è®¡å‡ºæ›´å¥½çš„æ·±åº¦ã€‚é€šè¿‡è®¾è®¡åŸºäºè¿™ç§é™ä½æ€§èƒ½çš„å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ºä¸»è¦è¾å°„åœºè·å¾—äº†æ›´å¥½çš„æ·±åº¦ç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡åœ¨åŒ…å«å‰è§†å’Œ360åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šä½¿ç”¨ä¸Šè¿°æ­£åˆ™åŒ–ï¼Œåœ¨ç¨€ç–è¾“å…¥è§†å›¾ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚</p><p>(3): æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œå½“ä½¿ç”¨ç¨€ç–è¾“å…¥è§†å›¾è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¾å°„åœºæ¨¡å‹é€šå¸¸åˆ©ç”¨å®ƒä»¬çš„é«˜æ€§èƒ½æ¥å­¦ä¹ ä¸å¿…è¦çš„å¤æ‚è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶è¿™äº›è§£å†³æ–¹æ¡ˆå®Œç¾åœ°è§£é‡Šäº†è§‚æµ‹å›¾åƒï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šåœ¨æ–°è§†å›¾ä¸­é€ æˆä¸¥é‡çš„å¤±çœŸã€‚ä¾‹å¦‚ï¼ŒNeRFä¸­çš„ä¸€äº›å…³é”®ç»„ä»¶ï¼Œå¦‚NeRFä¸­çš„ä½ç½®ç¼–ç æˆ–TensoRFä¸­é‡‡ç”¨çš„å‘é‡çŸ©é˜µåˆ†è§£ï¼Œä¸ºè¾å°„åœºæä¾›äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶è¢«è®¾è®¡ä¸ºä½¿ç”¨å¯†é›†è¾“å…¥è§†å›¾è®­ç»ƒæ¨¡å‹ã€‚ç”±äºç³»ç»Ÿä¸¥é‡æ¬ çº¦æŸï¼Œè¿™äº›ç»„ä»¶çš„ç°æœ‰å®ç°å¯èƒ½åœ¨è¾“å…¥è§†å›¾è¾ƒå°‘çš„æƒ…å†µä¸‹ä¸ç†æƒ³ï¼Œä»è€Œå¯¼è‡´å¤šç§å¤±çœŸã€‚å›¾4ã€å›¾7å’Œå›¾8åˆ†åˆ«æ˜¾ç¤ºäº†NeRFã€TensoRFå’ŒZipNeRFåœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å¸¸è§çš„å¤±çœŸã€‚æˆ‘ä»¬éµå¾ªæµè¡Œçš„å¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼Œå¹¶å¯¹è¾å°„åœºè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥åœ¨å¯èƒ½çš„æƒ…å†µä¸‹é€‰æ‹©æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡é™ä½è¾å°„åœºçš„æ€§èƒ½æ¥è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å‹ä¼°è®¡çš„æ·±åº¦æ¥ç›‘ç£ä¸»è¾å°„åœºã€‚æˆ‘ä»¬é’ˆå¯¹NeRFã€TensoRFå’ŒZipNeRFçš„ä¸åŒç¼ºç‚¹å’Œæ¶æ„è®¾è®¡äº†ä¸åŒçš„å¢å¼ºã€‚NeRFä¸­ä½¿ç”¨çš„é«˜ä½ç½®ç¼–ç åº¦ä¼šå¯¼è‡´ä¸å¸Œæœ›çš„æ·±åº¦ä¸è¿ç»­ï¼Œä»è€Œäº§ç”Ÿæµ®ç‚¹ã€‚æ­¤å¤–ï¼Œè§†ç‚¹ç›¸å…³çš„è¾å°„ç‰¹å¾ä¼šå¯¼è‡´å½¢çŠ¶è¾å°„æ¨¡ç³Šï¼Œä»è€Œäº§ç”Ÿé‡å¤ä¼ªå½±ã€‚æˆ‘ä»¬é€šè¿‡é™ä½ä½ç½®ç¼–ç åº¦å’Œç¦ç”¨è§†ç‚¹ç›¸å…³çš„è¾å°„ç‰¹å¾æ¥è®¾è®¡NeRFçš„å¢å¼ºã€‚å¦ä¸€æ–¹é¢ï¼ŒTensoRFä¸­å¤§é‡çš„é«˜åˆ†è¾¨ç‡åˆ†è§£ç»„ä»¶å’ŒZipNeRFä¸­çš„å¤§å“ˆå¸Œè¡¨ä¼šå¯¼è‡´è¿™äº›æ¨¡å‹åœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å‡ºç°æµ®ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¢å¼ºï¼Œä»¥é™åˆ¶æ¨¡å‹åœ¨è¿™äº›ç»„ä»¶æ–¹é¢å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†ç®€åŒ–çš„æ¨¡å‹ç”¨ä½œæ·±åº¦ç›‘ç£çš„å¢å¼ºï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸»è¦çš„NeRFæ¨¡å‹ï¼Œå› ä¸ºç®€å•åœ°é™ä½è¾å°„åœºçš„æ€§èƒ½å¯èƒ½ä¼šå¯¼è‡´æŸäº›åŒºåŸŸçš„æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆ[Jain et al. 2021]ã€‚ä¾‹å¦‚ï¼Œåªèƒ½å­¦ä¹ å¹³æ»‘æ·±åº¦è½¬æ¢çš„æ¨¡å‹å¯èƒ½æ— æ³•å­¦ä¹ ç‰©ä½“è¾¹ç•Œå¤„çš„é”åˆ©æ·±åº¦ä¸è¿ç»­æ€§ã€‚æ­¤å¤–ï¼Œä»…å½“å¢å¼ºæ¨¡å‹å‡†ç¡®è§£é‡Šè§‚å¯Ÿåˆ°çš„å›¾åƒæ—¶ï¼Œæ‰éœ€è¦ä½¿ç”¨å®ƒä»¬è¿›è¡Œç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¼°è®¡çš„æ·±åº¦å°†åƒç´ é‡æ–°æŠ•å½±åˆ°ä¸åŒçš„æœ€è¿‘è®­ç»ƒè§†å›¾ä¸Šå¹¶å°†å…¶ä¸ç›¸åº”çš„å›¾åƒè¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡æ·±åº¦çš„å¯é æ€§ã€‚</p><p>(4): åœ¨NeRF-LLFFã€RealEstate-10Kå’ŒMipNeRF360æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ­£åˆ™åŒ–åœ¨NeRFã€TensoRFå’ŒZipNeRFæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚è¡¨1æ‰€ç¤ºã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒåŸå§‹è¾å°„åœºå­˜åœ¨å„ç§å¤±çœŸã€‚ä½¿ç”¨æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆå¯¹è¾å°„åœºè¿›è¡Œæ­£åˆ™åŒ–å¯ä»¥æ˜¾è‘—æ”¹å–„æ‰€æœ‰ä¸‰ä¸ªè¾å°„åœºçš„é‡å»ºã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):é€šè¿‡é™ä½è¾å°„åœºæ€§èƒ½æ¥è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å‹ä¼°è®¡çš„æ·±åº¦æ¥ç›‘ç£ä¸»è¾å°„åœºï¼›            (2):é’ˆå¯¹NeRFã€TensoRFå’ŒZipNeRFçš„ä¸åŒç¼ºç‚¹å’Œæ¶æ„è®¾è®¡äº†ä¸åŒçš„å¢å¼ºï¼›            (3):é€šè¿‡ä½¿ç”¨ä¼°è®¡çš„æ·±åº¦å°†åƒç´ é‡æ–°æŠ•å½±åˆ°ä¸åŒçš„æœ€è¿‘è®­ç»ƒè§†å›¾ä¸Šå¹¶å°†å…¶ä¸ç›¸åº”çš„å›¾åƒè¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡æ·±åº¦çš„å¯é æ€§ï¼›            (4):åœ¨NeRF-LLFFã€RealEstate-10Kå’ŒMipNeRF360æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ­£åˆ™åŒ–åœ¨NeRFã€TensoRFå’ŒZipNeRFæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼›            .......</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯é€šè¿‡ä»ä¸ä¸»è¾å°„åœºæ¨¡å‹åŒæ—¶è®­ç»ƒçš„ä½èƒ½åŠ›å¢å¼ºæ¨¡å‹å­¦ä¹ çš„æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆä¸­è·å¾—æ·±åº¦ç›‘ç£æ¥è§£å†³å°‘æ¬¡æ‹æ‘„è¾å°„åœºçš„é—®é¢˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯ä»¥ä¸ºéšå¼æ¨¡å‹ï¼ˆå¦‚ NeRFï¼‰å’Œæ˜¾å¼è¾å°„åœºï¼ˆå¦‚ TensoRF å’Œ ZipNeRFï¼‰è®¾è®¡å¢å¼ºã€‚ç”±äºå„ç§è¾å°„åœºçš„ç¼ºç‚¹ä¸åŒï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹é€‚å½“åœ°è®¾è®¡äº†å¢å¼ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¢å¼ºåœ¨æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ä¸Šéƒ½æ˜¾ç€æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨å‰è§†åœºæ™¯å’Œ 360â—¦ åœºæ™¯ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åœºæ™¯çš„æ·±åº¦ä¼°è®¡æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ï¼Œè¿™å¯¹äºæ–°è§†å›¾åˆæˆå’Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡ä»å¢å¼ºæ¨¡å‹å­¦ä¹ çš„æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆä¸­è·å¾—æ·±åº¦ç›‘ç£æ¥æ­£åˆ™åŒ–è¾å°„åœºçš„æ–¹æ³•ï¼›æ€§èƒ½ï¼šåœ¨ NeRFã€TensoRF å’Œ ZipNeRF æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œåœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦è®¾è®¡é’ˆå¯¹ä¸åŒè¾å°„åœºæ¨¡å‹çš„å¢å¼ºï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„å·¥ç¨‹å·¥ä½œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5d84b090330526061fb59bb1dfc6ea7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a15c9e6ec9783c3b5ec66e4da9128f8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-52e8f6725bd512099b8ddbc432d73f2f.jpg" align="middle"></details><h2 id="Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields"><a href="#Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields"></a>Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields</h2><p><strong>Authors:Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</strong></p><p>Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at <a href="https://github.com/TQTQliu/GeFu">https://github.com/TQTQliu/GeFu</a> . </p><p><a href="http://arxiv.org/abs/2404.17528v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://gefucvpr24.github.io">https://gefucvpr24.github.io</a></p><p><strong>Summary</strong><br>æ–°æå‡ºæ–¹æ³•GeFué’ˆå¯¹NeRFæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œæå‡ºè‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰ã€ç©ºé—´è§†å›¾èšåˆï¼ˆSVAï¼‰å’Œä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰æœºåˆ¶ï¼Œé€šè¿‡æå‡å‡ ä½•é‡å»ºç²¾åº¦ã€ä¸°å¯Œæè¿°ç¬¦ä¿¡æ¯å’Œä¼˜åŒ–è§£ç ç­–ç•¥ï¼Œå¤§å¹…æå‡NeRFæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°å­˜NeRFæ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™äºå‡ ä½•é‡å»ºä¸å‡†ã€æè¿°ç¬¦ä¿¡æ¯ä¸è¶³å’Œè§£ç ç­–ç•¥ä¸ä¼˜ã€‚</li><li>ACAæœºåˆ¶é€šè¿‡æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹ï¼Œæå‡ä»£ä»·ä½“ç²¾åº¦ã€‚</li><li>SVAæœºåˆ¶ç»“åˆç©ºé—´å’Œè§†å›¾ä¿¡æ¯ï¼Œä¸°å¯Œæè¿°ç¬¦ä¿¡æ¯ã€‚</li><li>CAFæœºåˆ¶èåˆä¸åŒè§£ç ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œæå‡è§£ç æ•ˆæœã€‚</li><li>GeFuæ¡†æ¶ç»“åˆACAã€SVAã€CAFæœºåˆ¶ï¼Œä»ç²—åˆ°ç²¾è¿›è¡Œå‡ ä½•é‡å»ºå’Œèåˆæ¸²æŸ“ã€‚</li><li>GeFuæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li><li>GeFuä»£ç å·²å¼€æºï¼ˆ<a href="https://github.com/TQTQliu/GeFuï¼‰ã€‚">https://github.com/TQTQliu/GeFuï¼‰ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šåŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šï¼‰</p></li><li><p>ä½œè€…ï¼šå¤©é½ åˆ˜ï¼Œæ·»ç¿¼ å†¯ï¼Œå°æ˜ è‘£ï¼Œå˜‰é¹ å¼ ï¼Œå¿—ä¼Ÿ å†¯ï¼Œæ° æ½˜ï¼ŒæŒ¯ç¾½ ç‹ï¼Œå¿—ä¼Ÿ å†¯ï¼ˆTianqi Liu, Tianyi Feng, Xiaoming Dong, Jiapeng Zhang, Zhiwei Feng, Jie Pan, Zhenyu Wang, Zhiwei Fengï¼‰</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šåŒ—äº¬å¤§å­¦ï¼‰</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œå¤šè§†å›¾é‡å»ºï¼Œç¥ç»æ¸²æŸ“ï¼Œåœºæ™¯ç†è§£</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šæˆ–Githubä»£ç é“¾æ¥ï¼ˆè‹¥æœ‰ï¼Œåˆ™å¡«å†™ï¼Œè‹¥æ— ï¼Œåˆ™å¡«å†™Github:Noneï¼‰ï¼šGithub:None</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¯ä»¥ä»å¤šè§†å›¾å›¾åƒä¸­é‡å»º3Dåœºæ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼ˆå¦‚é®æŒ¡æˆ–åå°„ï¼‰è¡¨ç°å‡ºæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ„å»ºåŸºäºæ–¹å·®çš„ä»£ä»·ä½“ç§¯è¿›è¡Œå‡ ä½•é‡å»ºï¼Œå¹¶ç¼–ç 3Dæè¿°ç¬¦è¿›è¡Œæ–°è§†å›¾è§£ç ã€‚ä½†è¿™äº›æ–¹æ³•å­˜åœ¨å‡ ä½•ä¸å‡†ç¡®ã€æè¿°ç¬¦æ¬¡ä¼˜å’Œè§£ç ç­–ç•¥ä¸ä½³çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“ï¼ˆGeFuï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼›å¼•å…¥ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰é€šè¿‡ç©ºé—´å’Œè§†å›¾é—´çš„äº¤äº’å°†3Dä¸Šä¸‹æ–‡èå…¥æè¿°ç¬¦ï¼›æå‡ºäº†ä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨äº†ä¸¤ç§ç°æœ‰è§£ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒGeFuåœ¨å¤šè§†å›¾é‡å»ºå’Œæ–°è§†å›¾æ¸²æŸ“ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æé«˜NeRFåœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“æ¡†æ¶ï¼ˆGeFuï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¼•å…¥ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰é€šè¿‡ç©ºé—´å’Œè§†å›¾é—´çš„äº¤äº’å°†3Dä¸Šä¸‹æ–‡èå…¥æè¿°ç¬¦ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºäº†ä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨äº†ä¸¤ç§ç°æœ‰è§£ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„NeRFæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸè§†å›¾åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é‡å»ºé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ¥æ”¹å–„å‡ ä½•ä¼°è®¡ï¼Œå¹¶æå‡ºäº†ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰æ¥ç¼–ç 3Dä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°ç¬¦ã€‚åœ¨æ¸²æŸ“é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†Consistency-Aware Fusionï¼ˆCAFï¼‰æ¨¡å—ï¼Œä»¥ç»Ÿä¸€å…¶ä¼˜åŠ¿æ¥ä¼˜åŒ–åˆæˆè§†å›¾è´¨é‡ã€‚æˆ‘ä»¬å°†è¿™äº›æ¨¡å—æ•´åˆåˆ°ä¸€ä¸ªç²—åˆ°ç»†çš„æ¡†æ¶ä¸­ï¼Œç§°ä¸ºGeFuã€‚å¹¿æ³›çš„è¯„ä¼°å’Œæ¶ˆèå®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰ã€ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰å’Œä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰æ¨¡å—ï¼Œæé«˜äº†NeRFåœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨å¤šè§†å›¾é‡å»ºå’Œæ–°è§†å›¾æ¸²æŸ“ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦ä¿®æ”¹NeRFçš„é‡å»ºå’Œæ¸²æŸ“æµç¨‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f615e4a52c82bbd89b40d674212ac03c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccb26edee482b262ae1661c51b02d1d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3ded1a62b2132a2c5b5fdd26dc30947d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cbccb86ceb95a77c9f32e543fe79fbf0.jpg" align="middle"></details><h2 id="Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery"><a href="#Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery" class="headerlink" title="Depth Supervised Neural Surface Reconstruction from Airborne Imagery"></a>Depth Supervised Neural Surface Reconstruction from Airborne Imagery</h2><p><strong>Authors:Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala</strong></p><p>While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images. </p><p><a href="http://arxiv.org/abs/2404.16429v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ä½œä¸ºå¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰çš„æ›¿ä»£æ–¹æ³•ï¼Œåœ¨ç©ºä¸­åœºæ™¯ä¸‰ç»´é‡å»ºä¸­å±•ç°å‡º promising çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ— çº¹ç†ã€é€æ˜å’Œåå°„è¡¨é¢ç­‰ä¼ ç»Ÿ MVSéš¾ä»¥å¤„ç†çš„åœºæ™¯æ—¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs åœ¨ç©ºä¸­å›¾åƒå—ï¼ˆåŒ…æ‹¬ä»… nadirã€å€¾æ–œå’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼‰çš„ä¸‰ç»´é‡å»ºä¸­å…·æœ‰é€‚ç”¨æ€§ã€‚</li><li>é›†æˆå¹³å·®å—è°ƒæ•´ä¸­æä¾›çš„è”ç³»ç‚¹æµ‹é‡æ·±åº¦å…ˆéªŒä¿¡æ¯å¯ä»¥æå‡é‡å»ºæ•ˆæœã€‚</li><li>ä½¿ç”¨åŸºäºç¬¦å·è·ç¦»å‡½æ•° (SDF) çš„ VolSDF æ¡†æ¶è¿›è¡Œé‡å»ºï¼Œæ›´é€‚ç”¨äºè¡¨é¢é‡å»ºã€‚</li><li>NeRFs åœ¨å›¾åƒå†—ä½™åº¦ä½å’Œæ•°æ®è¯æ®å¼±çš„åŒºåŸŸï¼ˆå¦‚è¡—é“å³¡è°·ã€ç«‹é¢æˆ–å»ºç­‘é˜´å½±ï¼‰å­˜åœ¨å›°éš¾ã€‚</li><li>è®­ç»ƒ NeRFs è®¡ç®—æˆæœ¬é«˜ã€‚</li><li>åœ¨ç©ºä¸­åœºæ™¯çš„ä¸‰ç»´é‡å»ºä¸­ï¼ŒNeRFs é¢ä¸´ä½å›¾åƒå†—ä½™åº¦å’Œå¼±æ•°æ®è¯æ®çš„æŒ‘æˆ˜ã€‚</li><li>åœ¨ä»…ä½¿ç”¨ nadir å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒNeRFs çš„é‡å»ºæ€§èƒ½ä½äºä½¿ç”¨å€¾æ–œå›¾åƒæˆ–é«˜åˆ†è¾¨ç‡å›¾åƒçš„æƒ…å†µã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šèˆªç©ºå½±åƒçš„æ·±åº¦ç›‘ç£ç¥ç»è¡¨é¢é‡å»º</p></li><li><p>ä½œè€…ï¼šV. Hacksteinã€P. Fauth-Mayerã€M. Rothermelã€N. Haala</p></li><li><p>æ‰€å±æœºæ„ï¼šnFrames ESRIï¼ˆå¾·å›½ï¼‰</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€å¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰ã€3D åœºæ™¯é‡å»ºã€ç½‘æ ¼åŒ– 3D ç‚¹äº‘ã€èˆªç©ºå½±åƒã€æ·±åº¦ç›‘ç£</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.16429 , Github é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æœ€åˆç”¨äºæ–°é¢–è§†å›¾åˆæˆï¼Œç°å·²æˆä¸ºå¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚NeRF å°¤å…¶é€‚ç”¨äºæ— çº¹ç†ã€é€æ˜å’Œåå…‰è¡¨é¢ï¼Œè€Œè¿™äº›åœºæ™¯å¯¹äºåŸºäº MVS çš„ä¼ ç»Ÿæ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶å…³æ³¨è¿‘è·ç¦»åœºæ™¯ï¼Œè€Œé’ˆå¯¹èˆªç©ºåœºæ™¯çš„ç ”ç©¶ä»ç„¶ç¼ºå¤±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„ MVS æ–¹æ³•åœ¨ç²¾ç»†å‡ ä½•ç»“æ„ã€æ— çº¹ç†åŒºåŸŸå’Œéæœ—ä¼¯è¡¨é¢ï¼ˆä¾‹å¦‚åŠé€æ˜ç‰©ä½“æˆ–åå°„ï¼‰å¤„å­˜åœ¨é—®é¢˜ã€‚åŠ¨æœºå……åˆ†ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç›‘ç£çš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡ä¿®æ”¹äº† VolSDF æ¡†æ¶ï¼Œå°† SfM å…³è”ç‚¹ç›‘ç£æ•´åˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ”¯æŒè®­ç»ƒè¿‡ç¨‹ã€‚VolSDF ä½¿ç”¨ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰å¯¹ 3D åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ¯”é¦™è‰ NeRF ä¸­çš„æ ‡å‡†ä½“ç§¯è¡¨ç¤ºæ›´é€‚ç”¨äºè¡¨é¢é‡å»ºä»»åŠ¡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡é’ˆå¯¹ä¸‰ç§èˆªç©ºå›¾åƒé›†è¯„ä¼°äº†è¯¥ç®¡é“ï¼Œè¿™äº›å›¾åƒé›†å…·æœ‰ä¸åŒçš„é…ç½®ã€‚åœ¨ä¸“ä¸šèˆªç©ºæµ‹ç»˜ä¸­é€šå¸¸ä½¿ç”¨çš„æ•°æ®ä¸Šçš„è¿™äº›ç ”ç©¶ä»å®é™…è§’åº¦å‡ºå‘å¾ˆæœ‰è¶£ï¼ŒåŒæ—¶ç ”ç©¶äº†åŸºäº NeRF çš„è¡¨é¢é‡å»ºçš„å…·ä½“æŒ‘æˆ˜ã€‚æ­¤ç±»èˆªç©ºå›¾åƒé›†åˆçš„è§†è§’æœ‰é™ï¼Œå¹¶ä¸”å¯èƒ½å› è¡—é“å³¡è°·ã€ç«‹é¢æˆ–å»ºç­‘ç‰©é˜´å½±è€Œå—åˆ°å½±å“ã€‚è¯¥æ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰å›é¡¾ VolSDFï¼ˆç¥ç»è¾å°„åœºï¼‰ï¼›</p><p>ï¼ˆ2ï¼‰VolSDF çš„ SDFï¼ˆç¬¦å·è·ç¦»å‡½æ•°ï¼‰è¡¨ç¤ºï¼›</p><p>ï¼ˆ3ï¼‰VolSDF çš„æ­£åˆ™åŒ–ï¼›</p><p>ï¼ˆ4ï¼‰VolSDF çš„é‡‡æ ·ï¼›</p><p>ï¼ˆ5ï¼‰Tie ç‚¹ç›‘ç£ï¼š    ï¼ˆaï¼‰Tie ç‚¹åˆå§‹åŒ–å’Œç›‘ç£ï¼›    ï¼ˆbï¼‰æ·±åº¦ç›‘ç£æŸå¤±å‡½æ•°ï¼šLtr å’Œ Lfsï¼›</p><p>ï¼ˆ6ï¼‰å®ç°å’Œè®­ç»ƒç»†èŠ‚ï¼š    ï¼ˆaï¼‰æ¨¡å‹ç»“æ„ï¼›    ï¼ˆbï¼‰è®­ç»ƒæŸå¤±å‡½æ•°ï¼šL = LRGB + Î»eikLeik + Î»surfLsurf +Î»fsLfs + Î»trLtrï¼›</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å±•ç¤ºäº† VolSDFï¼ˆç¥ç»è¾å°„åœºçš„ä¸€ç§å˜ä½“ï¼Œç”¨äºå»ºæ¨¡éšå¼ç¥ç»è¡¨é¢ï¼‰åœ¨èˆªç©ºå›¾åƒä¸‰ç»´é‡å»ºä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†é€šè¿‡å…³è”ç‚¹ç›‘ç£ VolSDF å¯ä»¥æ”¹å–„é‡å»ºæ•ˆæœï¼šæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”åœ¨å®Œæ•´æ€§å’Œå‡†ç¡®æ€§æ–¹é¢è´¨é‡æ›´å¥½ã€‚è¿™å°¤å…¶é€‚ç”¨äºä»…å…·æœ‰æœ‰é™æ•°æ®è¯æ®çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸï¼Œå¯¹äºè¿™äº›åŒºåŸŸï¼ŒVolSDF å¾€å¾€ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼æˆ–æ ¹æœ¬æ— æ³•æ”¶æ•›ã€‚ä¸€ä¸ªç¤ºä¾‹æ­£å°„åœºæ™¯çš„é‡å»ºè¡¨é¢åœ¨ NMAD æ–¹é¢æ¯”ä¼ ç»Ÿçš„ MVS ç®¡é“å°‘äº 4 ä¸ª GSD åå·®ã€‚ä¸ºäº†å®Œå…¨æ”¶æ•›å¹¶æ¢å¤å…¨éƒ¨ç»†èŠ‚ï¼Œä»ç„¶éœ€è¦å»¶é•¿è®­ç»ƒæ—¶é—´ã€‚è¿™é˜»ç¢äº†å®é™…åº”ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…è·å¾—æ‹“æ‰‘æ­£ç¡®çš„è¡¨é¢ï¼Œè¿™äº›è¡¨é¢å¯ä»¥è¿›è¡Œåç»­ç½‘æ ¼åå¤„ç†ã€‚é‡‡æ ·ä¾‹ç¨‹æ˜¯è¯„ä¼°å®æ–½ä¸­çš„ä¸»è¦ç“¶é¢ˆï¼Œå¹¶å°†åœ¨æœªæ¥å·¥ä½œä¸­è¿›è¡Œæ”¹è¿›ã€‚ä¸€æ–¹é¢ï¼Œé«˜æ•ˆçš„ GPU å®ç°å¯ä»¥åŠ é€Ÿè¿™ä¸€è¿‡ç¨‹ï¼ˆWang ç­‰äººï¼Œ2023 å¹´ï¼‰ï¼Œå¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¸Œæœ›ç ”ç©¶åœ¨æœ‰å¾ˆå¤§æ”¹è¿›æ½œåŠ›çš„åŒºåŸŸåŠ¨æ€å¢å¼ºé‡‡æ ·çš„å¯èƒ½æ€§ï¼ˆKerbl ç­‰äººï¼Œ2023 å¹´ï¼‰ã€‚ç¥ç»éšå¼è¡¨é¢é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶è¯¾é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬æ–‡ä¹Ÿèƒ½é¼“åŠ±åœ¨èˆªç©ºå›¾åƒå’Œå…¶ä»–é¥æ„Ÿåº”ç”¨çš„å‡ ä½•é‡å»ºé¢†åŸŸå¼€å±•æœªæ¥çš„å·¥ä½œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ·±åº¦ç›‘ç£çš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œå°† SfM å…³è”ç‚¹ç›‘ç£æ•´åˆåˆ° VolSDF è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ”¯æŒè®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜äº†é‡å»ºè´¨é‡ï¼›</p><p>æ€§èƒ½ï¼šåœ¨èˆªç©ºå›¾åƒé›†ä¸Šè¯„ä¼°äº†è¯¥ç®¡é“ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸï¼ˆä¾‹å¦‚æ— çº¹ç†åŒºåŸŸã€éæœ—ä¼¯è¡¨é¢ï¼‰å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼›</p><p>å·¥ä½œé‡ï¼šéœ€è¦è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´æ‰èƒ½å®Œå…¨æ”¶æ•›å¹¶æ¢å¤å…¨éƒ¨ç»†èŠ‚ï¼Œè¿™é˜»ç¢äº†å®é™…åº”ç”¨ï¼Œé‡‡æ ·ä¾‹ç¨‹æ˜¯è¯„ä¼°å®æ–½ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-4036313ed6644db70c73439252a5eaed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e26afc3f9b57484514d8f583efe4569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6ddc84a2cf8fcf12f4f1a29a529e7de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14b2f3ca89df5d53f911326b2d3382d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77dd44d2a901985d20406c555ff9eb2c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>åŸºäºä¸‰ç»´é«˜æ–¯æ»´çš„è¯­éŸ³é©±åŠ¨çš„è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼è¡¨ç¤ºå’Œå¯¹ä¸‰ç»´é¢éƒ¨æ¨¡å‹çš„é«˜æ–¯å…³è”å®ç°ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨å’Œé¢éƒ¨ç»†èŠ‚å¢å¼ºï¼Œå±•ç°å‡ºå®æ—¶æ¸²æŸ“æ€§èƒ½å’Œå“è¶Šçš„è§†è§‰æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨ä¸‰ç»´é«˜æ–¯æ»´çš„æ˜¾å¼è¡¨ç¤ºï¼Œè§£å†³äº†NeRFå†…éšè¡¨ç¤ºçš„å§¿æ€å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³é—®é¢˜ã€‚</li><li>æå‡ºçš„è¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨çš„éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶çš„å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°äº†é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œé€šè¿‡æ½œåœ¨å§¿åŠ¿å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºï¼Œæä¾›ç¨³å®šä¸”é€¼çœŸçš„æ¸²æŸ“è§†é¢‘ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹é¢ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†ç²¾ç¡®çš„å”‡éƒ¨åŒæ­¥å’Œå‡ºè‰²çš„è§†è§‰æ•ˆæœã€‚</li><li>è¯¥æ–¹æ³•åœ¨ NVIDIA RTX4090 GPU ä¸Šå®ç°äº† 130 FPS çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜¾ç€è¶…è¿‡äº†å®æ—¶æ¸²æŸ“æ€§èƒ½çš„é˜ˆå€¼ï¼Œå¹¶æœ‰å¯èƒ½éƒ¨ç½²åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalkerï¼šåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººä¸“å±ä¼šè¯´è¯çš„å¤´åƒåˆæˆ</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: é˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific Motion Translator, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p><p>(1): è¿‘æœŸåŸºäºç¥ç»è¾å°„åœº(NeRF)çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹æ³•å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚ç„¶è€Œï¼Œå—é™äºNeRFéšå¼è¡¨ç¤ºå¯¹å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»å­˜åœ¨å”‡éƒ¨è¿åŠ¨ä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ç­‰é—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šåŸºäºNeRFçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹æ³•ï¼›é—®é¢˜ï¼šå§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œå¯¼è‡´å”‡éƒ¨è¿åŠ¨ä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå……åˆ†ï¼Œæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼šGaussianTalkerï¼Œè¯¥æ–¹æ³•ç”±è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ä¸¤ä¸ªæ¨¡å—ç»„æˆã€‚è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººä¸“å±æ··åˆå½¢çŠ¶ï¼Œå°†é«˜æ–¯ç‚¹äº‘ä¸3Dé¢éƒ¨æ¨¡å‹ç»‘å®šï¼Œå®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚è¯¥æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³å®ç°è‡ªç„¶é€¼çœŸçš„è¯´è¯äººå¤´åƒåˆæˆã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼›</p><p>ï¼ˆ2ï¼‰ï¼šGaussianTalkerç”±è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººä¸“å±æ··åˆå½¢çŠ¶ï¼Œå°†é«˜æ–¯ç‚¹äº‘ä¸3Dé¢éƒ¨æ¨¡å‹ç»‘å®šï¼Œå®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šè¯¥æ–¹æ³•åœ¨éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯äººå…³è”èµ·æ¥ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3Dç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚è¯´è¯äººä¸“å±FLAMEè½¬æ¢å™¨é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥ï¼Œä»¥å®ç°åŒæ­¥å’Œè‡ªç„¶çš„å”‡éƒ¨è¿åŠ¨ï¼Œè€ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿ä¼˜åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šå’Œé€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalkeråœ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¿œè¶…å…¶ä»–æ–¹æ³•çš„è¶…é«˜æ¸²æŸ“é€Ÿåº¦ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²çš„åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼›æ€§èƒ½ï¼šåœ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†è¿œè¶…å…¶ä»–æ–¹æ³•çš„è¶…é«˜æ¸²æŸ“é€Ÿåº¦ï¼›å·¥ä½œé‡ï¼š......</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRFâ€™s applications in the context of AD. Our survey is structured to categorize NeRFâ€™s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v2">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„è¯¸å¤šåº”ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæ˜¾ç¤ºå‡ºå¹¿é˜”æ½œèƒ½ï¼Œåº”ç”¨æ¶µç›–æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸã€‚</li><li>NeRFæ„ŸçŸ¥åº”ç”¨åŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²å’Œè·Ÿè¸ªã€‚</li><li>NeRFä¸‰ç»´é‡å»ºåº”ç”¨å¯ç”Ÿæˆé«˜ä¿çœŸä¸‰ç»´åœºæ™¯ã€‚</li><li>NeRF SLAM èåˆäº†æ„ŸçŸ¥å’Œé‡å»ºï¼Œå®æ—¶åˆ›å»ºç¯å¢ƒåœ°å›¾ã€‚</li><li>NeRFä»¿çœŸåº”ç”¨å¯åˆ›é€ é€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒï¼Œç”¨äºä¼ æ„Ÿå™¨å’Œç®—æ³•æµ‹è¯•ã€‚</li><li>ç ”ç©¶çƒ­ç‚¹åŒ…æ‹¬è·¨æ¨¡æ€èåˆã€é«˜æ•ˆè¡¨ç¤ºå’ŒåŠ¨æ€åœºæ™¯å¤„ç†ã€‚</li><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œé¢ä¸´æŒ‘æˆ˜å’Œæœºé‡ã€‚</li><li>æœªæ¥æ–¹å‘åŒ…æ‹¬é«˜ç²¾åº¦ã€é²æ£’æ€§å’Œå®æ—¶æ€§èƒ½ä¼˜åŒ–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼šç»¼è¿°</p></li><li><p>ä½œè€…ï¼šé›·è´ºã€æä¹æ’ã€å­™æ–‡è¶…ã€éŸ©æ³½å®‡ã€åˆ˜ä¸€è¾°ã€éƒ‘æ€å‘ã€ç‹å»ºå¼ºã€æå…‹å¼º</p></li><li><p>Affiliation: æ¸…åå¤§å­¦è½¦è¾†ä¸è¿è½½å­¦é™¢</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.13816 ,Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å‡­å€Ÿå…¶å†…åœ¨ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å…¶éšå¼è¡¨ç¤ºå’Œæ–°é¢–çš„è§†å›¾åˆæˆèƒ½åŠ›ï¼Œåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å¤‡å—å…³æ³¨ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œå¤§é‡æ–¹æ³•æ¶Œç°å‡ºæ¥ï¼Œæ¢ç´¢ NeRF åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰æ–‡çŒ®ä¸­å­˜åœ¨æ˜æ˜¾çš„ç©ºç™½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¯¹ NeRF åœ¨ AD ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥æ—¨åœ¨å¯¹ NeRF åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä¸­çš„åº”ç”¨è¿›è¡Œåˆ†ç±»ï¼Œç‰¹åˆ«æ˜¯åŒ…æ‹¬æ„ŸçŸ¥ã€3D é‡å»ºã€åŒæ—¶å®šä½å’Œå»ºå›¾ (SLAM) ä»¥åŠä»¿çœŸã€‚æˆ‘ä»¬æ·±å…¥åˆ†æå¹¶æ€»ç»“äº†æ¯ä¸ªåº”ç”¨ç±»åˆ«çš„å‘ç°ï¼Œå¹¶é€šè¿‡æä¾›å¯¹è¯¥é¢†åŸŸæœªæ¥æ–¹å‘çš„è§è§£å’Œè®¨è®ºæ¥ç»“æŸã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡èƒ½ä¸ºè¯¥é¢†åŸŸçš„çš„ç ”ç©¶äººå‘˜æä¾›å…¨é¢çš„å‚è€ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡ä¸“é—¨é’ˆå¯¹ NeRF åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨çš„ç»¼è¿°ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºé«˜ç²¾åº¦åœ°å›¾æ¥æä¾›é™æ€åœºæ™¯ç†è§£ï¼Œç°åœ¨å¼ºè°ƒé€šè¿‡é¸Ÿç°è§†è§‰å®æ—¶æ„ŸçŸ¥å±€éƒ¨ç¯å¢ƒã€‚åŒæ—¶ï¼Œå®ƒåœ¨åŠŸèƒ½ä¸Šå·²ä» 2 çº§ï¼ˆL2ï¼‰å‘å±•åˆ°åŠªåŠ›å®ç° 4 çº§ï¼ˆL4ï¼‰è‡ªåŠ¨é©¾é©¶ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè¦æ±‚å¯¹å‘¨å›´ç¯å¢ƒæœ‰æ·±å…¥çš„äº†è§£ï¼ŒåŒ…æ‹¬é™æ€åœºæ™¯å’Œäº¤é€šå‚ä¸è€…ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œè¿™æ˜¯æœ‰æ•ˆè§„åˆ’å’Œæ§åˆ¶çš„å…³é”®å‰æã€‚NeRF é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œå·²è¯æ˜å…¶æœ‰æ•ˆç†è§£å±€éƒ¨åœºæ™¯çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºè‡ªåŠ¨é©¾é©¶èƒ½åŠ›çš„è¯±äººå€™é€‰è€…ã€‚åœ¨è¿‡å»ä¸¤å¹´ä¸­ï¼ŒNeRF æ¨¡å‹å·²åœ¨è‡ªåŠ¨é©¾é©¶çš„å„ä¸ªæ–¹é¢å¾—åˆ°äº†åº”ç”¨ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€3D é‡å»ºã€åŒæ—¶å®šä½å’Œå»ºå›¾ (SLAM) ä»¥åŠä»¿çœŸï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚</p><p>(3):ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²æˆä¸ºæ„ŸçŸ¥é¢†åŸŸçš„å¾ˆæœ‰å¸Œæœ›çš„ç«äº‰è€…ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—å…³é”®ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹è±¡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œå æ®é¢„æµ‹ã€‚å®ƒäººæ°”é£™å‡çš„ä¸»è¦åŸå› æ˜¯å®ƒèƒ½å¤Ÿè·å–ç²¾ç¡®ä¸”ä¸€è‡´çš„å‡ ä½•ä¿¡æ¯ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶å¯åˆ†ä¸ºä¸¤å¤§èŒƒå¼ï¼ŒåŒºåˆ«åœ¨äº NeRF çš„åˆ©ç”¨ï¼šâ€œNeRF for dataâ€å’Œâ€œNeRF for modelâ€ã€‚å‰è€…æ¶‰åŠ NeRF çš„åˆå§‹è®­ç»ƒï¼Œç„¶åå°†å…¶ç”¨äºæ‰©å……æ„ŸçŸ¥ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåè€…é‡‡ç”¨ NeRF å’Œæ„ŸçŸ¥ç½‘ç»œçš„ååŒè®­ç»ƒç­–ç•¥ï¼Œä½¿æ„ŸçŸ¥ç½‘ç»œèƒ½å¤Ÿå­¦ä¹  NeRF æ•è·çš„å‡ ä½•ä¿¡æ¯ã€‚</p><p>(4):åœ¨ 3D é‡å»ºåº”ç”¨é¢†åŸŸï¼ŒNeRF å¯ä»¥æ ¹æ®åœºæ™¯ç†è§£çš„çº§åˆ«åˆ†ä¸ºä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šåŠ¨æ€åœºæ™¯é‡å»ºã€è¡¨é¢é‡å»ºå’Œé€†å‘æ¸²æŸ“ã€‚åœ¨ç¬¬ä¸€ç±»ä¸­ï¼ŒåŠ¨æ€åœºæ™¯é‡å»ºä¾§é‡äºé‡å»ºå…·æœ‰å¯ç§»åŠ¨ä»£ç†çš„åŠ¨æ€åœºæ™¯ï¼Œä¸»è¦ä½¿ç”¨é¡ºåº 3D è¾¹ç•Œæ¡†æ³¨é‡Šå’Œç›¸æœºå‚æ•°ã€‚åœ¨ç¬¬äºŒç±»ä¸­ï¼Œè¡¨é¢é‡å»ºæ—¨åœ¨é‡å»ºåœºæ™¯çš„æ˜¾å¼ 3D è¡¨é¢ï¼Œä¾‹å¦‚ç½‘æ ¼ã€‚åœ¨ç¬¬ä¸‰ç±»ä¸­ï¼Œé€†å‘æ¸²æŸ“æ—¨åœ¨ä»é©¾é©¶åœºæ™¯çš„å›¾åƒä¸­è§£å¼€å½¢çŠ¶ã€åç…§ç‡å’Œå¯è§æ€§ï¼Œä»¥å®ç°è¯¸å¦‚é‡æ–°ç…§æ˜ä¹‹ç±»çš„åº”ç”¨ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li><strong>ç»“è®º</strong></li></ol><p>ï¼ˆ1ï¼‰æœ¬ç»¼è¿°å·¥ä½œå¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“ï¼Œå¡«è¡¥äº†å½“å‰æ–‡çŒ®ä¸­çš„ç©ºç™½ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†å…¨é¢çš„å‚è€ƒã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæœ¬ç»¼è¿°é¦–æ¬¡ä¸“é—¨é’ˆå¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼›æ€§èƒ½ï¼šå¯¹ç¥ç»è¾å°„åœºåœ¨æ„ŸçŸ¥ã€3D é‡å»ºã€SLAM å’Œä»¿çœŸç­‰é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†æ·±å…¥åˆ†æå’Œæ€»ç»“ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡å¤§ï¼Œæ¶‰åŠæ–‡çŒ®å¹¿æ³›ï¼Œåˆ†ææ·±å…¥ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-55b475e228eebb497768f57fb097059d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-22321e24e9114a3aa3b89b16e6ff76f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://picx.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  NeRF-Guided Unsupervised Learning of RGB-D Registration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
</feed>
