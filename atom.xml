<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-05-28T08:35:02.959Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/NeRF/</id>
    <published>2024-05-27T18:05:14.000Z</published>
    <updated>2024-05-28T08:35:02.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="NeRF-Casting-Improved-View-Dependent-Appearance-with-Consistent-Reflections"><a href="#NeRF-Casting-Improved-View-Dependent-Appearance-with-Consistent-Reflections" class="headerlink" title="NeRF-Casting: Improved View-Dependent Appearance with Consistent   Reflections"></a>NeRF-Casting: Improved View-Dependent Appearance with Consistent   Reflections</h2><p><strong>Authors:Dor Verbin, Pratul P. Srinivasan, Peter Hedman, Ben Mildenhall, Benjamin Attal, Richard Szeliski, Jonathan T. Barron</strong></p><p>Neural Radiance Fields (NeRFs) typically struggle to reconstruct and render highly specular objects, whose appearance varies quickly with changes in viewpoint. Recent works have improved NeRF’s ability to render detailed specular appearance of distant environment illumination, but are unable to synthesize consistent reflections of closer content. Moreover, these techniques rely on large computationally-expensive neural networks to model outgoing radiance, which severely limits optimization and rendering speed. We address these issues with an approach based on ray tracing: instead of querying an expensive neural network for the outgoing view-dependent radiance at points along each camera ray, our model casts reflection rays from these points and traces them through the NeRF representation to render feature vectors which are decoded into color using a small inexpensive network. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing shiny objects, and that it is the only existing NeRF method that can synthesize photorealistic specular appearance and reflections in real-world scenes, while requiring comparable optimization time to current state-of-the-art view synthesis models. </p><p><a href="http://arxiv.org/abs/2405.14871v1">PDF</a> Project page: <a href="http://nerf-casting.github.io">http://nerf-casting.github.io</a></p><p><strong>Summary</strong><br>NeRF方法通过光线追踪技术解决了高度光滑物体的渲染问题，实现了逼真的镜面效果和反射。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF方法改进了渲染远处环境光照细节的能力，但无法合成较近内容的一致反射。</li><li>采用光线追踪技术，从点上投射反射光线并跟踪它们通过NeRF表示，以呈现特征向量，并使用小型廉价网络将其解码为颜色，解决了大规模神经网络的优化和渲染速度受限的问题。</li><li>该模型在合成含有光亮物体场景的视图合成方面优于先前方法，是唯一可以在现实场景中合成逼真的镜面效果和反射的NeRF方法，且所需优化时间与当前最先进的视图合成模型相当。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NeRF-Casting：Improved View-Dependent Appearance with Consistent Reflections（NeRF-Casting：具有consistent反射的视图相关外观改进）</p></li><li><p>Authors: DOR VERBIN, PRATUL P. SRINIVASAN, PETER HEDMAN, BEN MILDENHALL, BENJAMIN ATTAL, RICHARD SZELISKI, JONATHAN T. BARRON</p></li><li><p>Affiliation: 谷歌美国</p></li><li><p>Keywords: View synthesis, neural radiance fields, reflections</p></li><li><p>Urls: <a href="https://nerf-casting.github.io">https://nerf-casting.github.io</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):该论文的研究背景是Neural Radiance Fields（NeRF）在视图合成任务中的应用，特别是处理具有高频视图相关外观的镜面对象。</p></li><li><p>(2):过去的方法使用大型神经网络来模拟视图相关的radiance，但是这些方法存在两个问题：一是只能合成远距离环境照明的反射，二是计算开销很大。本文的方法motivated by这些问题。</p></li><li><p>(3):本文提出的方法是基于ray tracing的NeRF-Casting，通过casting反射光线并将其追踪到NeRF表示中，生成特征向量，然后使用小型神经网络解码成颜色。</p></li><li><p>(4):本文的方法在视图合成任务中取得了state-of-the-art的性能，能够合成具有高频视图相关外观的镜面对象的反射，且计算开销与当前最先进的视图合成模型相当。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):该篇工作的意义在于解决了Neural Radiance Fields（NeRF）在视图合成任务中的反射问题，提高了视图相关外观的合成质量和效率。</p></li><li><p>(2):创新点：提出了一种基于ray tracing的NeRF-Casting方法，能够生成高频视图相关外观的镜面对象反射；性能：取得了state-of-the-art的视图合成性能，能够合成具有高频视图相关外观的镜面对象反射；工作负载：计算开销与当前最先进的视图合成模型相当，具有良好的实时性和可扩展性。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-323e45f3162c2c7c913df9dc30275d1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7da742d6de299d161600adf6fdb2df43.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46b90894aa28846d98c1eef5c5a89f0c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2faaf26739f0521731fa46fe33bfa637.jpg" align="middle"></details><h2 id="Neural-Directional-Encoding-for-Efficient-and-Accurate-View-Dependent-Appearance-Modeling"><a href="#Neural-Directional-Encoding-for-Efficient-and-Accurate-View-Dependent-Appearance-Modeling" class="headerlink" title="Neural Directional Encoding for Efficient and Accurate View-Dependent   Appearance Modeling"></a>Neural Directional Encoding for Efficient and Accurate View-Dependent   Appearance Modeling</h2><p><strong>Authors:Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi</strong></p><p>Novel-view synthesis of specular objects like shiny metals or glossy paints remains a significant challenge. Not only the glossy appearance but also global illumination effects, including reflections of other objects in the environment, are critical components to faithfully reproduce a scene. In this paper, we present Neural Directional Encoding (NDE), a view-dependent appearance encoding of neural radiance fields (NeRF) for rendering specular objects. NDE transfers the concept of feature-grid-based spatial encoding to the angular domain, significantly improving the ability to model high-frequency angular signals. In contrast to previous methods that use encoding functions with only angular input, we additionally cone-trace spatial features to obtain a spatially varying directional encoding, which addresses the challenging interreflection effects. Extensive experiments on both synthetic and real datasets show that a NeRF model with NDE (1) outperforms the state of the art on view synthesis of specular objects, and (2) works with small networks to allow fast (real-time) inference. The project webpage and source code are available at: \url{<a href="https://lwwu2.github.io/nde/}">https://lwwu2.github.io/nde/}</a>. </p><p><a href="http://arxiv.org/abs/2405.14847v1">PDF</a> Accepted to CVPR 2024</p><p><strong>Summary</strong><br>提出了一种名为Neural Directional Encoding（NDE）的视图相关外观编码方法，用于神经辐射场（NeRF）渲染镜面对象，提高了对高频角信号的建模能力。</p><p><strong>Key Takeaways</strong><br>• 镜面对象的新视图合成仍然是一个挑战性的问题，需要考虑全球照明效果和其他对象的反射。<br>• 提出了Neural Directional Encoding（NDE），一种视图相关的外观编码方法，用于NeRF渲染镜面对象。<br>• NDE将特征网格基于的空间编码概念转移到角域，提高了对高频角信号的建模能力。<br>• NDE使用角输入和空间特征来获得空间变化的方向编码，解决了挑战性的交叉反射效果。<br>• 实验结果表明，使用NDE的NeRF模型在镜面对象的视图合成方面优于当前最先进的方法。<br>• 使用小网络可以实现快速（实时）推理。<br>• 项目网页和源代码已经公开，网址为<a href="https://lwwu2.github.io/nde/。">https://lwwu2.github.io/nde/。</a></p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 神经方向编码（Neural Directional Encoding）</p></li><li><p>Authors: Liwen Wu, Sai Bi, Zexiang Xu, Fujun Luan, Kai Zhang, Iliyan Georgiev, Kalyan Sunkavalli, Ravi Ramamoorthi</p></li><li><p>Affiliation: 加州大学圣地亚哥分校（UC San Diego）</p></li><li><p>Keywords: Neural Radiance Fields, View-Dependent Appearance, Specular Objects, Novel-View Synthesis</p></li><li><p>Urls: <a href="https://lwwu2.github.io/nde/">https://lwwu2.github.io/nde/</a>, Github:None</p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文研究背景是新视图合成领域，特别是 specular 对象的新视图合成，旨在恢复物体的高频视图依赖外观和全球照明效果。</p></li><li><p>(2):过去的方法使用分析函数对视图方向进行编码，需要大型多层感知器（MLP），收敛速度慢，无法模拟复杂的反射效果。这些方法也忽视了空间特征对视图依赖外观的影响。</p></li><li><p>(3):本文提出了一种神经方向编码（NDE）方法，将特征网格编码概念应用于角度域，通过 مخروط追踪空间特征获取空间变化的方向编码，解决了 interreflection 效果的挑战。</p></li><li><p>(4):本文方法在合成 specular 对象的新视图任务上取得了 state-of-the-art 的性能，并且可以使用小型网络实现实时推理，满足了快速合成的需求。</p></li></ul><ol><li><p>Methods:</p><ul><li><p>(1): 该方法使用神经方向编码（NDE）来对特征网格进行角度域的编码，通过مخروط追踪空间特征获取空间变化的方向编码。</p></li><li><p>(2): NDE方法能够有效解决interreflection效果的挑战，恢复物体的高频视图依赖外观和全球照明效果，而无需使用大型多层感知器（MLP）。</p></li><li><p>(3): 该方法具有实时推理的能力，可以使用小型网络实现快速合成，并在合成specular对象的新视图任务上取得了state-of-the-art的性能。</p></li></ul></li></ol><ol><li><p>Conclusion: </p><ul><li><p>(1):This piece of work is significant in advancing the field of novel-view synthesis, particularly in the synthesis of specular objects, by introducing a novel method, Neural Directional Encoding (NDE), which efficiently models complex reflections and achieves state-of-the-art performance.</p></li><li><p>(2):Innovation point: The article innovatively introduces the NDE method to efficiently model complex reflections for novel-view synthesis, addressing the limitations of previous methods.<br>Performance: The proposed method achieves state-of-the-art performance in synthesizing specular objects with the ability for real-time inference using a small network.<br>Workload: The workload is reduced as the method eliminates the need for large multi-layer perceptrons and enables real-time synthesis.</p></li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b069231775fc8a2bd10f93cb80d839ec.jpg" align="middle"><img src="https://picx.zhimg.com/v2-75b217587db527ee5663a4499270caf9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-abc9cca95d286eab225c623b7babb05b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fce7139aa953d9627454cfadef62958.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87061bf3e19ae720c7a849195745380a.jpg" align="middle"></details><h2 id="Camera-Relocalization-in-Shadow-free-Neural-Radiance-Fields"><a href="#Camera-Relocalization-in-Shadow-free-Neural-Radiance-Fields" class="headerlink" title="Camera Relocalization in Shadow-free Neural Radiance Fields"></a>Camera Relocalization in Shadow-free Neural Radiance Fields</h2><p><strong>Authors:Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available. </p><p><a href="http://arxiv.org/abs/2405.14824v1">PDF</a> Accepted by ICRA 2024. 8 pages, 5 figures, 3 tables. Codes and   dataset: <a href="https://github.com/hnrna/ShadowfreeNeRF-CameraReloc">https://github.com/hnrna/ShadowfreeNeRF-CameraReloc</a></p><p><strong>Summary</strong><br>本文提出了一种两阶段流水线，用于规范具有不同光照和阴影条件的图像，以改善相机重定位，实现了在不同光照条件下相机重定位的最新成果。</p><p><strong>Key Takeaways</strong></p><ul><li>相机重定位在计算机视觉和机器人领域是一个关键问题。</li><li>近期关于神经辐射场（NeRFs）的进展显示出合成逼真图像的潜力。</li><li>之前的工作利用NeRFs优化相机姿态，但未考虑可能影响场景外观和阴影区域的光照变化，导致姿态优化过程下降。</li><li>该论文提出了一种基于哈希编码的NeRF来实现场景表示，显著提升了姿态优化过程。</li><li>为解决网格型NeRF中的噪声图像梯度计算问题，进一步提出了重新设计的截断动态低通滤波器（TDLF）和数值梯度平均技术来平滑处理。</li><li>在多个具有不同光照条件的数据集上的实验结果表明，该方法在不同光照条件下的相机重定位中取得了最新的成果。</li><li>代码和数据将公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 相机重定位在无阴影神经辐射场中（Camera Relocalization in Shadow-free Neural Radiance Fields）</p></li><li><p>Authors: Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</p></li><li><p>Affiliation: 清华大学人工智能产业研究院</p></li><li><p>Keywords: Camera Relocalization, Neural Radiance Fields, Shadow Removal</p></li><li><p>Urls: arXiv:2405.14824v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):本文研究的背景是计算机视觉和机器人学领域中的相机重定位问题，目标是从给定的图像中恢复摄像机的位姿。</p></li><li><p>(2):过去的方法使用判别网络或NeRF来refine摄像机位姿，但是这些方法不能处理光照变化和阴影区域对场景外观的影响，导致位姿优化过程不稳定。</p></li><li><p>(3):本文提出的方法是一个两阶段的pipeline，首先使用阴影移除网络对图像进行 normalization，然后使用hash编码的NeRF来refine摄像机位姿，并提出了一种改进的梯度计算方法来平滑优化过程。</p></li><li><p>(4):实验结果表明，本文的方法在多个数据集上取得了 state-of-the-art 的结果，证明了其在相机重定位任务中的有效性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1)：本文提出的方法是一个两阶段的pipeline，首先使用阴影移除网络（Shadow Removal Network，Nshadow）对图像进行 normalization，得到阴影-free图像I(l0)。</p></li><li><p>(2)：然后，使用hash编码的NeRF（Neural Radiance Fields）模型对阴影-free图像I(l0)进行场景重建，得到三维神经场景图F。</p></li><li><p>(3)：在pose优化阶段，使用同样的阴影移除网络Nshadow对测试图像进行阴影移除，得到阴影-free测试图像I(l0)，然后使用梯度下降算法优化摄像机pose，直到渲染图像ˆI(l0)与阴影-free测试图像I(l0)之间的光度loss达到最小。</p></li><li><p>(4)：为了提高pose优化的稳定性，本文提出了一种改进的梯度计算方法，使用numerical gradient averaging技术来平滑优化过程。</p></li><li><p>(5)：在pose优化过程中，文还使用了一种粗到细的优化策略，使用truncated dynamic low-pass filter（TDLF）来分离高频和低频图像组件，并逐渐增加高频组件的权重，以避免局部最优解。</p></li><li><p>(6)：实验结果表明，本文的方法在多个数据集上取得了state-of-the-art的结果，证明了其在相机重定位任务中的有效性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文的工作对于计算机视觉和机器人学领域中的相机重定位问题具有重要意义，因为它能够在无阴影神经辐射场中实现高精度的摄像机重定位，从而提高机器人的导航和定位能力。</p></li><li><p>(2):Innovation point: 本文提出了一种新的两阶段pipeline，首先使用阴影移除网络对图像进行 normalization，然后使用hash编码的NeRF模型对阴影-free图像进行场景重建，这种方法能够有效地处理光照变化和阴影区域对场景外观的影响Performance: 实验结果表明，本文的方法在多个数据集上取得了 state-of-the-art 的结果，证明了其在相机重定位任务中的有效性；Workload: 本文的方法需要在训练和测试阶段进行大量的计算和优化，需要高性能的计算设备和大量的数据集支持。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6d260d5b744a5039554f8c6aaee9bc01.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0ac90b20b3733ad747ec11650e963cf5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5a7748ef501582a143e2301b2e39f951.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0770bb34500dd5dd1e4632f197e96d71.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fdb4265248fa23783d77c10c673a037.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1113a2498657772fa4f4f86d7876ebfc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-28  NeRF-Casting Improved View-Dependent Appearance with Consistent   Reflections</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/3DGS/</id>
    <published>2024-05-27T17:55:43.000Z</published>
    <updated>2024-05-28T08:35:16.157Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap"><a href="#Feature-Splatting-for-Better-Novel-View-Synthesis-with-Low-Overlap" class="headerlink" title="Feature Splatting for Better Novel View Synthesis with Low Overlap"></a>Feature Splatting for Better Novel View Synthesis with Low Overlap</h2><p><strong>Authors:T. Berriel Martins, Javier Civera</strong></p><p>3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first “splatted” into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. We will release the code upon acceptance.   Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting </p><p><a href="http://arxiv.org/abs/2405.15518v1">PDF</a> </p><p><strong>Summary</strong><br>使用特征splattering（FeatSplat）将3D高斯体的颜色信息编码到每个高斯体的特征向量中，提高了新视图合成的质量和泛化能力。</p><p><strong>Key Takeaways</strong><br>• 3D高斯splattering在新视图合成中取得了state-of-the-art的质量，但其使用球谐函数表达场景颜色限制了3D高斯体的表达能力。<br>• 本文提出将颜色信息编码到每个高斯体的特征向量中，以提高表达能力和泛化能力。<br>• 特征splattering（FeatSplat）模型包括高斯体的splattering、alpha-blending和解码三个步骤。<br>• 模型中还加入了相机embedding，以条件解码也基于视点信息。<br>• 实验结果表明，FeatSplat模型显著提高了低重叠视图的新视图合成质量。<br>• FeatSplat模型不仅可以生成RGB值，还可以生成每像素的语义标签。<br>• 将发布代码。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 特征Splattering用于低重叠视图的新视图合成 (Feature Splatting for Better Novel View Synthesis with Low Overlap)</p></li><li><p>Authors: Tomas Berriel Martins, Javier Civera</p></li><li><p>Affiliation: 扎拉戈萨大学(I3A)</p></li><li><p>Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting</p></li><li><p>Urls: arXiv:2405.15518v1, Github:None</p></li><li><p>Summary:</p></li></ol><pre><code>- (1):该论文的研究背景是寻找适合三维场景表示，以便在机器人、虚拟现实和增强现实应用中使用。- (2):过去的方法包括Neural Radiance Fields（NeRFs）和三维高斯Splattering（3DGS），但它们存在一些缺陷，例如NeRFs计算开销高、3DGS使用球谐函数表示场景颜色限制了其表达能力。- (3):本文提出了一种新的方法，称为特征Splattering（FeatSplat），它将三维高斯的颜色信息编为每个高斯的特征向量，然后将这些征向量混合并解码以生成RGB像素值。- (4):实验结果表明，FeatSplat方法可以显著改善低重叠视图的新视图合成性能，并且可以生成每像素的语义标签，以支持机器人等应用。</code></pre><ol><li>Conclusion: </li></ol><ul><li><p>(1):本文的工作对于三维场景表示和新视图合成具有重要意义，可以应用于机器人、虚拟现实和增强现实等领域。</p></li><li><p>(2):Innovation point: 本文提出了一种新的特征Splattering（FeatSplat）方法，弥补了Neural Radiance Fields（NeRFs）和三维高斯Splattering（3DGS）的不足之处； Performance: FeatSplat方法可以生成高质量的新视图，并且可以生成每像素的语义标签； Workload: 本文的方法计算开销相对较低，适合实时应用。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-af9ac9b1d0d353f31971a8ace9ae132b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-eaee1c783ee42cdf998fdd81f98539e2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-922abaae68f73855cac3e6cd2f6fb3d0.jpg" align="middle"></details><h2 id="HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting"><a href="#HDR-GS-Efficient-High-Dynamic-Range-Novel-View-Synthesis-at-1000x-Speed-via-Gaussian-Splatting" class="headerlink" title="HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed   via Gaussian Splatting"></a>HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed   via Gaussian Splatting</h2><p><strong>Authors:Yuanhao Cai, Zihao Xiao, Yixun Liang, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</strong></p><p>High dynamic range (HDR) novel view synthesis (NVS) aims to create photorealistic images from novel viewpoints using HDR imaging techniques. The rendered HDR images capture a wider range of brightness levels containing more details of the scene than normal low dynamic range (LDR) images. Existing HDR NVS methods are mainly based on NeRF. They suffer from long training time and slow inference speed. In this paper, we propose a new framework, High Dynamic Range Gaussian Splatting (HDR-GS), which can efficiently render novel HDR views and reconstruct LDR images with a user input exposure time. Specifically, we design a Dual Dynamic Range (DDR) Gaussian point cloud model that uses spherical harmonics to fit HDR color and employs an MLP-based tone-mapper to render LDR color. The HDR and LDR colors are then fed into two Parallel Differentiable Rasterization (PDR) processes to reconstruct HDR and LDR views. To establish the data foundation for the research of 3D Gaussian splatting-based methods in HDR NVS, we recalibrate the camera parameters and compute the initial positions for Gaussian point clouds. Experiments demonstrate that our HDR-GS surpasses the state-of-the-art NeRF-based method by 3.84 and 1.91 dB on LDR and HDR NVS while enjoying 1000x inference speed and only requiring 6.3% training time. </p><p><a href="http://arxiv.org/abs/2405.15125v1">PDF</a> The first 3D Gaussian Splatting-based method for HDR imaging</p><p><strong>Summary</strong><br>提出高动态范围Gaussian Splatting（HDR-GS）框架，实现高效 novel view synthesis 和曝光时间可控的低动态范围图像重建。</p><p><strong>Key Takeaways</strong><br>• 高动态范围 novel view synthesis（HDR NVS）旨在使用HDR成像技术从新视点生成逼真的图像。<br>• 现有的HDR NVS方法主要基于NeRF，存在长训练时间和慢推理速度的问题。<br>• 本文提出高动态范围Gaussian Splatting（HDR-GS）框架，实现高效 novel view synthesis 和曝光时间可控的低动态范围图像重建。<br>• HDR-GS使用双动态范围（DDR）高斯点云模型和基于MLP的tone-mapper来渲染HDR和LDR颜色。<br>• 该方法在LDR和HDR NVS任务上超过基于NeRF的方法，且具有1000倍的推理速度和仅需6.3%的训练时间<br>• 实验结果表明HDR-GS在HDR NVS任务上具有明显的优势。<br>• 本文为基于3D高斯splattting的HDR NVS方法奠定了数据基础。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高动态范围新视图合成（HDR-GS）：基于高斯抹除的高效HDR新视图合成（High Dynamic Range Gaussian Splatting: Efficient HDR Novel View Synthesis via Gaussian Splatting）</p></li><li><p>Authors: Yuanhao Cai, Zihao Xiao, Yixun Liang, Minghan Qin, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille</p></li><li><p>Affiliation: 约翰斯·霍普金斯大学</p></li><li><p>Keywords: 高动态范围, 新视图合成, 高斯抹除, Novel View Synthesis, HDR, Gaussian Splatting</p></li><li><p>Urls: <a href="https://arxiv.org/abs/2405.15125v1">https://arxiv.org/abs/2405.15125v1</a>, Github: <a href="https://github.com/caiyuanhao1998/HDR-GS">https://github.com/caiyuanhao1998/HDR-GS</a></p></li><li><p>Summary:</p></li></ol><ul><li>(1):本文研究背景是高动态范围（HDR）新视图合成（NVS），旨在使用HDR成像技术从新视点生成逼真的图像。</li></ul><ul><li>(2):过去的方法主要基于NeRF，但这些方法存在长训练时间和慢推理速度的问题。</li></ul><ul><li>(3):本文提出的研究方法是High Dynamic Range Gaussian Splatting（HDR-GS），它使用双动态范围（DDR）高斯点云模型和平行可微分光栅化（PDR）过程来高效地渲染HDR和LDR视图。</li></ul><ul><li>(4):本文方法在HDR和LDR新视图合成任务上优于基于NeRF的方法，达到了3.84和1.91 dB的PSNR性能，并且具有1000倍的推理速度和仅需6.3%的训练时间</li></ul><ol><li>方法：</li></ol><ul><li><p>(1):提出双动态范围（DDR）高斯点云模型，用于表示高动态范围（HDR）图像的颜色和深度信息，该模型由高斯分布函数和点云数据组成。</p></li><li><p>(2):使用平行可微分光栅化（PDR）过程将DDR高斯点云模型转换为高效的渲染表示，以便快速生成HDR和LDR视图。</p></li><li><p>(3):设计高斯抹除（Gaussian Splatting）算法，用于将DDR高斯点云模型投影到目标视图平面上，生成高质量的HDR和LDR图像。</p></li><li><p>(4):提出基于高斯抹除的新视图合成（Novel View Synthesis）方法，用于从给定的HDR图像中生成意视点的HDR和LDR图像。</p></li><li><p>(5):使用基于NeRF的方法作为基线，比较HDR-GS方法在HDR和LDR新视图合成任务上的性能，结果表明HDR-GS方法具有更高的PSNR性能和更快的推理速度。</p></li><li><p>(6):通过实验验证HDR-GS方法的有效性和高效性，结果表明HDR-GS方法能够生成高质量的HDR和LDR图像，并且具有实时渲染的能力。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li>(1):该研究工作的重要性在于解决了高动态范围（HDR）新视图合成中的效率问题，实现了高质量的HDR图像渲染和快速推理速度，具有广泛的应用前景在计算机视觉、图形学和机器学习等领域。</li></ul><ul><li>(2):创新点：提出了一种基于高斯抹除的高效HDR新视图合成方法HDR-GS，解决了基于NeRF方法的长训练时间和慢推理速度问题；性能：在HDR和LDR新视图合成任务上，HDR-GS方法具有更高的PSNR性能和更快的推理速度；工作量：HDR-GS方法仅需6.3%的训练时间和1000倍的推理速度，具有实时渲染的能力。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-62274faaed9878e5e0161dea6f18dbbe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1eb56bf3e6d513a6248b50e7a8d0c539.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6cf6e245e96bb903d2b486b7727c24e.jpg" align="middle"></details><h2 id="GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting"><a href="#GS-Hider-Hiding-Messages-into-3D-Gaussian-Splatting" class="headerlink" title="GS-Hider: Hiding Messages into 3D Gaussian Splatting"></a>GS-Hider: Hiding Messages into 3D Gaussian Splatting</h2><p><strong>Authors:Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</strong></p><p>3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGS’s spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: <a href="https://xuanyuzhang21.github.io/project/gshider">https://xuanyuzhang21.github.io/project/gshider</a>. </p><p><a href="http://arxiv.org/abs/2405.15118v1">PDF</a> 3DGS steganography</p><p><strong>Summary</strong><br>三维高斯分裂（3DGS）隐写术框架GS-Hider，实现了对原始3DGS点云文件的隐写和提取。</p><p><strong>Key Takeaways</strong><br>• 3DGS需要保护版权、完整性和隐私，因为训练需要大量时间和计算成本。<br>• 3DGS具有显式3D表示和实时渲染速度，导致点云文件公开透明，具有明确的物理意义。<br>• GS-Hider框架可以将3D场景和图像嵌入到原始GS点云中，以不可见的方式提取隐藏的消息。<br>• GS-Hider使用耦合安全特征属性替换原始3DGS的球谐系数，并使用场景解码器和消解码器来分离原始RGB场景和隐藏消息。<br>• 实验表明，GS-Hider可以有效地隐藏多模式消息，而不影响渲染质量，具有异常的安全性、鲁棒性、容量和灵活性。<br>• GS-Hider项目可在<a href="https://xuanyuzhang21.github.io/project/gshider上访问。">https://xuanyuzhang21.github.io/project/gshider上访问。</a><br>• GS-Hider框架可以保护3DGS的版权、完整性和隐私。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GS-Hider：隐藏消息到3D高斯点云（GS-Hider: Hiding Messages into 3D Gaussian Splatting）</p></li><li><p>Authors: Xuanyu Zhang, Jiarui Meng, Runyi Li, Zhipei Xu, Yongbing Zhang, Jian Zhang</p></li><li><p>Affiliation: 电子与计算机工程学院，北京大学（School of Electronic and Computer Engineering, Peking University）</p></li><li><p>Keywords: 3D高斯点云、隐写术、数字水印、copyright protection</p></li><li><p>Urls: <a href="https://arxiv.org/abs/2405.15118">https://arxiv.org/abs/2405.15118</a>, Github: <a href="https://xuanyuzhang21.github.io/project/gshider/">https://xuanyuzhang21.github.io/project/gshider/</a></p></li><li><p>Summary:</p></li></ol><ul><li><p>(1):本文的研究背景是保护3D场景重建和新视图合成中的数字资产的版权和隐私，特别是基于3D高斯点云（3DGS）的方法。</p></li><li><p>(2):过去的隐写术方法主要使用傅里叶和小波变换来嵌入消息，但是这些方法不能很好地适应3DGS的特点，例如明确的3D表示和实时渲染速度。</p></li><li><p>(3):本文提出了一个名为GS-Hider的隐写术框架，使用耦合的安全特征属性来替换原始3DGS的球谐系数，然后使用场景解码器和消息解码器来分离原始RGB场景和隐藏的消息。</p></li><li><p>(4):实验结果表明，GS-Hider可以在不影响渲染质量的情况下隐藏多模态消息，并且具有非常高的安全性、鲁棒性、容量和灵活性。</p></li></ul><ol><li>方法：</li></ol><ul><li><p>(1)：首先，作者们提出了基于耦合安全特征属性的隐写术框架GS-Hider，该框架可以将消息隐藏在3D高斯点云（3DGS）中。</p></li><li><p>(2)：在GS-Hider框架中，作者们使用耦合的安全特征属性来替换原始3DGS的球谐系数，具体来说，就是将消息嵌入到球谐系数中。</p></li><li><p>(3)：然后，作者们使用场景解码器和消息解码器来分离原始RGB场景和隐藏的消息，这两个解码器都是基于深度学习的神经网络。</p></li><li><p>(4)：在消息嵌入过程中，作者们使用了anisotropic Gaussians表示场景，通过splattin技术将3D高斯点云投影到图像平面上，并使用经点基于渲染来生成图像。</p></li><li><p>(5)：为了提高消息的安全性和鲁棒性，作者们使用了多种技术，包括DIFFusion-based方法和Frequency-based方法来保护消息抵抗攻击。</p></li><li><p>(6)：在实验中，作者们使用了多种数据集和评估指标来评估GS-Hider的性能，结果表明GS-Hider可以在不影响渲染质量的情况下隐藏多模态消息，并且具有非常高的安全性、鲁棒性、容量和灵活性。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1): 本文的工作意义在于提出了一种高保真、安全、大容量和多功能的3D高斯点云隐写术框架，即GS-Hider，为保护3D场景重建和新视图合成中的数字资产版权和隐私提供了有效的技术支持。</p></li><li><p>(2): 创新点：GS-Hider框架利用耦合的安全特征表示和双解码器解码技术，实现了在3D高斯点云中隐藏消息，具有很高的安全性、鲁棒性和灵活性；性能：实验结果表明GS-Hider在不影响渲染质量的情况下可以隐藏多模态消息，且具有高容量；工作量：文章未详细说明具体的工作量评估，需要进一步补充和完善。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-44535b4dc9ae919b2dce80a4be050e9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbb3c977263acb314ebe7c8c3a9043c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7d4ae3f321d6e860ec2da2743463f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8db132ec3c58c945a06898a8758b7480.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51183cc617b206934e4fdaaba05fdc46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c5422ed30935cd238fd580f363ae7ec2.jpg" align="middle"></details><h2 id="DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus"><a href="#DoGaussian-Distributed-Oriented-Gaussian-Splatting-for-Large-Scale-3D-Reconstruction-Via-Gaussian-Consensus" class="headerlink" title="DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D   Reconstruction Via Gaussian Consensus"></a>DoGaussian: Distributed-Oriented Gaussian Splatting for Large-Scale 3D   Reconstruction Via Gaussian Consensus</h2><p><strong>Authors:Yu Chen, Gim Hee Lee</strong></p><p>The recent advances in 3D Gaussian Splatting (3DGS) show promising results on the novel view synthesis (NVS) task. With its superior rendering performance and high-fidelity rendering quality, 3DGS is excelling at its previous NeRF counterparts. The most recent 3DGS method focuses either on improving the instability of rendering efficiency or reducing the model size. On the other hand, the training efficiency of 3DGS on large-scale scenes has not gained much attention. In this work, we propose DoGaussian, a method that trains 3DGS distributedly. Our method first decomposes a scene into K blocks and then introduces the Alternating Direction Method of Multipliers (ADMM) into the training procedure of 3DGS. During training, our DoGaussian maintains one global 3DGS model on the master node and K local 3DGS models on the slave nodes. The K local 3DGS models are dropped after training and we only query the global 3DGS model during inference. The training time is reduced by scene decomposition, and the training convergence and stability are guaranteed through the consensus on the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+ times when evaluated on large-scale scenes while concurrently achieving state-of-the-art rendering quality. Our project page is available at <a href="https://aibluefisher.github.io/DoGaussian">https://aibluefisher.github.io/DoGaussian</a>. </p><p><a href="http://arxiv.org/abs/2405.13943v1">PDF</a> </p><p><strong>Summary</strong><br>最近对3D高斯点云（3DGS）的研究显示了在新视图合成（NVS）任务上取得了令人期待的成果。 </p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在渲染性能和保真度方面表现优越，优于以往的NeRF方法。</li><li>最近的3DGS方法要么专注于改善渲染效率的不稳定性，要么减小模型尺寸。</li><li>本文提出了DoGaussian方法，该方法通过将场景分解为K个块，并引入交替方向乘子法（ADMM）来分布式训练3DGS。</li><li>DoGaussian方法通过场景分解缩短了训练时间，同时确保了训练的收敛性和稳定性。</li><li>训练时间缩短了6倍以上，同时在大规模场景上实现了最先进的渲染质量。</li><li>项目页面链接：<a href="https://aibluefisher.github.io/DoGaussian。">https://aibluefisher.github.io/DoGaussian。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: DoGaussian：分布式面向高斯斯普拉特（Distributed-Oriented Gaussian Splatting）</li></ol><ol><li>Authors: Yu Chen, Gim Hee Lee</li></ol><ol><li>Affiliation: 新加坡国立大学</li></ol><ol><li>Keywords: 3D Gaussian Splatting, Novel View Synthesis, Distributed Training</li></ol><ol><li>Urls: <a href="https://arxiv.org/abs/2405.13943v1">https://arxiv.org/abs/2405.13943v1</a>, Github: None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，三维高斯斯普拉特（3DGS）在新视图合成（NVS）任务中取得了良好的结果，然而，当前3DGS方法的训练效率在大规模场景下尚未受到足够的关注。- (2):之前的方法主要集中在提高渲染效率的不稳定性或减少模型大小，但这些方法忽视了大规模场景下的训练效率问题。- (3):本文提出了DoGaussian方法，该方法将场景分解成K个块，然后引入交替方向乘子法（ADMM）到3DGS的训练过程中。在训练过程中，DoGaussian在主节点上维护一个全局的3DGS模型，在从节点上维护K个局部的3DGS模型- (4):DoGaussian方在大规模场景下加速了3DGS的训练速度，达到了6倍以上的加速，同时也获得了最先进的渲染质量。</code></pre><ol><li>方法：</li></ol><ul><li><p>(1)：将场景分解成 K 个块，以便分布式训练。在每个块中，分配训练视图和点云数据。</p></li><li><p>(2)：引入 Alternating Direction Method of Multipliers（ADMM）算法，在分布式训练中实现全局一致的 3D Gaussian Splatting 模型。在每个块中，维护一个局部的 3D Gaussian Splatting 模型，并与主节点上的全局模型进行交互。</p></li><li><p>(3)：在每个块中，使用 ADMM 算法更新局部模型，并将更新后的模型与主节点上的全局模型进行平均，以实现模型的一致性。</p></li><li><p>(4)：在训练过程中，使用 Penalty Parameter 和 Over-relaxation 技术来提高 ADMM 算法的收敛速度。</p></li><li><p>(5)：使用场景分割算法，以确保每个块的大小相似，并且相邻块之间有足够的重叠区域，以促进训练的收敛。</p></li><li><p>(6)：在训练完成后，使用全局模型来合成新视图，以实现高质量的渲染结果。</p></li><li><p>(7)：实验结果表明，提出的 DoGaussian 方法可以在大规模场景下加速 3D Gaussian Splatting 的训练速度，达到了 6 倍以上的加速，同时也获得了最先进的渲染质量。</p></li></ul><ol><li>Conclusion: </li></ol><ul><li><p>(1):本文的贡献在于解决了三维高斯斯普拉特（3DGS）在大规模场景下的训效率问题，提高了新视图合成（NVS）的实时性和质量。</p></li><li><p>(2):创新点：提出了一种分布式训练方法DoGaussian，使用Alternating Direction Method of Multipliers（ADMM）算法实现全局一致的3DGS模型；性能：加速了3DGS的训练速度，达到了6倍以上的加速，同时也获得了最先进的渲染质量；工作量：需要大量的计算资源和场景分割算法来实现分布式训练。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-22c8c9dbbe8897a84779859d7460a6eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-261a3638b92396cc85c1385cc6c53581.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e3e352a0325ce88ecaee52f7e182708.jpg" align="middle"></details><h2 id="Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances"><a href="#Gaussian-Time-Machine-A-Real-Time-Rendering-Methodology-for-Time-Variant-Appearances" class="headerlink" title="Gaussian Time Machine: A Real-Time Rendering Methodology for   Time-Variant Appearances"></a>Gaussian Time Machine: A Real-Time Rendering Methodology for   Time-Variant Appearances</h2><p><strong>Authors:Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</strong></p><p>Recent advancements in neural rendering techniques have significantly enhanced the fidelity of 3D reconstruction. Notably, the emergence of 3D Gaussian Splatting (3DGS) has marked a significant milestone by adopting a discrete scene representation, facilitating efficient training and real-time rendering. Several studies have successfully extended the real-time rendering capability of 3DGS to dynamic scenes. However, a challenge arises when training images are captured under vastly differing weather and lighting conditions. This scenario poses a challenge for 3DGS and its variants in achieving accurate reconstructions. Although NeRF-based methods (NeRF-W, CLNeRF) have shown promise in handling such challenging conditions, their computational demands hinder real-time rendering capabilities. In this paper, we present Gaussian Time Machine (GTM) which models the time-dependent attributes of Gaussian primitives with discrete time embedding vectors decoded by a lightweight Multi-Layer-Perceptron(MLP). By adjusting the opacity of Gaussian primitives, we can reconstruct visibility changes of objects. We further propose a decomposed color model for improved geometric consistency. GTM achieved state-of-the-art rendering fidelity on 3 datasets and is 100 times faster than NeRF-based counterparts in rendering. Moreover, GTM successfully disentangles the appearance changes and renders smooth appearance interpolation. </p><p><a href="http://arxiv.org/abs/2405.13694v1">PDF</a> 14 pages, 6 figures</p><p><strong>Summary</strong><br>利用高斯时间机GTM实现实时三维重建，解决weather和lighting条件变化带来的挑战。</p><p><strong>Key Takeaways</strong><br>• 三维高斯Splatting（3DGS）技术的出现标志着三维重建的重要里程碑。<br>• 3DGS及其变体在实时渲染动态场景方面取得了成功，但是在不同天气和照明条件下训练图像时存在挑战。<br>• NeRF-based方法（NeRF-W、CLNeRF）可以处理这种挑战，但计算需求高，影响实时渲染能力。<br>• 高斯时间机GTM使用轻量级MLP模型时间嵌入矢量来模拟高斯primitive的时间依赖属性。<br>• GTM可以重建对象的可见性变化，并且具有更好的几何一致性。<br>• GTM在三个数据集上的渲染保真度达到最好，并且染速度是NeRF-based方法的100倍。<br>• GTM成功地分离了外观变化，并实现了平滑的外观插值。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 高斯时间机器：实时渲染时间变换外观 (Gaussian Time Machine: A Real-Time Rendering Methodology for Time-Variant Appearances)</li></ol><ol><li>Authors: Licheng Shen, Ho Ngai Chow, Lingyun Wang, Tong Zhang, Mengqiu Wang, Yuxing Han</li></ol><ol><li>Affiliation: 清华大学深圳国际研究生院</li></ol><ol><li>Keywords: Neural Rendering · 3D Gaussian Splatting · Varying Appearance</li></ol><ol><li>Urls: arXiv:2405.13694v1, Github:None</li></ol><ol><li>Summary:</li></ol><pre><code>- (1):近年来，神经渲染技术的发展极大地提高了三维重建的保真度。特别是，三维高斯点绘制（3DGS）提出了离散场景表示，提高了训练速度和实时渲染质量。- (2):过去的方法如NeRF-W和CLNeRF可以处理复杂的天气和照明条件，但是它们的计算需求限制了实时渲染能力。3DGS和其变体也存在着准确重建的挑战。- (3):本文提出了高斯时间机器（GTM），它使用离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性。通过调整高斯primitive的不透明度，可以重建对象的可见性变化。- (4):GTM在三个数据集上实现了最先进的渲染保真度，渲染速度是NeRF-based方法的100倍。此外，GTM还成功地分离了外观变化并实现了平滑的外观插值。</code></pre><ol><li>Methods:</li></ol><ul><li><p>(1): 本文提出的高斯时间机器（Gaussian Time Machine，GTM）采用离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性。</p></li><li><p>(2): GTM通过调整高斯primitive的不透明度，实现了对象可见性的变化，并成功地分离了外观变化。</p></li><li><p>(3): 在三个数据集上，GTM展现出了最先进的渲染保真度，且渲染速度是基于NeRF的方法的100倍。此外，GTM还能够实现平滑的外观插值。</p></li></ul><ol><li>Conclusion:</li></ol><ul><li><p>(1):本文提出的高斯时间机器（Gaussian Time Machine，GTM）在解决时间变换外观问题方面具有重要意义，可以应用于虚拟现实、数字孪生等领域。</p></li><li><p>(2):创新点：GTM 提出了离散时间嵌入向量和轻量级多层感知器（MLP）来建模高斯primitive的时间相关属性，实现了对象可见性的变化和外观变化的分离；性能：GTM 在三个数据集上实现了最先进的渲染保真度，渲染速度是 NeRF-based 方法的 100 倍；工作量：GTM 需要较少的计算资源和训练时间，能够实现实时渲染。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e37e39f80d95d9753e062031ea071292.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8d45eb05bc11e95b4d1a05a781ee482b.jpg" align="middle"></details><h2 id="GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting"><a href="#GaussianVTON-3D-Human-Virtual-Try-ON-via-Multi-Stage-Gaussian-Splatting-Editing-with-Image-Prompting" class="headerlink" title="GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting"></a>GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting</h2><p><strong>Authors:Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</strong></p><p>The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing. </p><p><a href="http://arxiv.org/abs/2405.07472v2">PDF</a> On-going work</p><p><strong>Summary</strong><br>电子商务的日益突出彰显了虚拟试穿（VTON）的重要性。本文提出了GaussianVTON，将高斯点绘制（GS）编辑与2D VTON相结合，首次提出使用图像作为3D编辑提示，以及引入了ERR编辑策略，为3D VTON提供了新视角。</p><p><strong>Key Takeaways</strong></p><ul><li>电子商务的日益突出彰显了虚拟试穿（VTON）的重要性。</li><li>GaussianVTON将高斯点绘制（GS）编辑与2D VTON相结合，首次提出使用图像作为3D编辑提示。</li><li>通过三阶段的精细化策略逐步缓解潜在问题，进一步解决了面部模糊、服装不准确和编辑过程中视角质量下降等问题。</li><li>引入了ERR编辑策略来应对之前编辑策略的局限性，解决了复杂几何变化带来的问题。</li><li>实验结果显示，GaussianVTON具有卓越性能，为3D VTON提供了新视角，并建立了图像提示3D场景编辑的新起点。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 高斯 Virtual Try-On：基于多阶段高斯 Splatting 的 3D 人体虚拟试衣（GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting）</p></li><li><p>Authors: Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</p></li><li><p>Affiliation: 西北工业大学</p></li><li><p>Keywords: Virtual Try-On, 3D Human, Gaussian Splatting, Image Prompting</p></li><li><p>Urls: <a href="https://haroldchen19.github.io/gsvton/">https://haroldchen19.github.io/gsvton/</a>, Github:None</p></li><li><p>Summary:</p><ul><li><p>(1):随着电子商务的兴起，虚拟试衣（Virtual Try-On, VTON）变得越来越重要。然而，之前的研究主要集中在 2D 领域，并且需要大量的训练数据。</p></li><li><p>(2):过去的方法主要集中在 2D VTON 领域，并且需要大量的训练数据。这些方法无法很好地解决 3D VTON 问题，例如服装形状与人体形状的不兼容问题</p></li><li><p>(3):本文提出了 GaussianVTON，一种基于多阶段高斯 Splatting 的 3D VTON 管道。该方法使用图像作为编辑提示，实现了从 2D 到 3D VTON 的无缝过渡。</p></li><li><p>(4):实验结果表明，GaussianVTON 方法在 3D VTON 任务上取得了优异的性能，证明了该方法的有效性。</p></li></ul></li><li>方法：</li></ol><ul><li><p>(1)：输入重建的 3D 场景和相应的数据，包括一系列拍摄的图像、相应的相机姿态和相机标定参数。</p></li><li><p>(2)：使用图像编辑提示来指导 3D 场景的编辑过程，以实现虚拟试衣。首先，引入 3D 高斯 Splatting 模型和基于扩散的 2D VTON 模型。</p></li><li><p>(3)：提出了 Editing Recall Reconstruction (ERR) 策略，该策略在编辑过程中渲染整个数据集，以解决编辑不一致的问题。</p></li><li><p>(4)：采用三阶段细化策略，包括人脸一致性、层次稀疏编辑和图像质量改进三个阶段，以解决编辑过程中遇到的各种问题。</p></li><li><p>(5)：在 ERR 策略中，对整个数据集进行编辑和细化，然后对数据集进行更新，以确保编辑的一致性。</p></li><li><p>(6)：使用 LaDI-VTON 模型对每个图像进行编辑，并将编辑结果与原始图像进行比较，以评估编辑的效果。</p></li><li><p>(7)：对编辑结果进行可视化和评估，以验证 GaussianVTON 方法的有效性。</p></li></ul><ol><li><p>Conclusion: </p><pre><code>             - (1):本文的工作对电子商务虚拟试衣领域的发展具有重要意义，可以为用户提供更加真实的试衣体验。             - (2):创新点：本文提出了一种基于多阶段高斯 Splatting 的 3D 人体虚拟试衣方法，解决了 2D 到 3D 虚拟试衣的技术瓶颈；性能：实验结果表明，GaussianVTON 方法在 3D VTON 任务上取得了优异的性能；工作量：本文的方法需要大量的训练数据和计算资源，限制了其在实际应用中的普及性。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e12873404001a9a09d996899cdfe1fc3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c590805a84c00f53de63efe5b169e438.jpg" align="middle"><img src="https://picx.zhimg.com/v2-28127860f8d303f51aff59430d547019.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-28  Feature Splatting for Better Novel View Synthesis with Low Overlap</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Talking%20Head%20Generation/</id>
    <published>2024-05-27T17:24:49.000Z</published>
    <updated>2024-05-28T08:33:26.824Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation"><a href="#InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation" class="headerlink" title="InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation"></a>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation</h2><p><strong>Authors:Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian</strong></p><p>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2405.15758v1">PDF</a> Project page: <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a></p><p><strong>Summary</strong><br>提出了一种新的文本指导方法，生成情感丰富的2D虚拟头像，实现细粒度控制、交互性和通用性。</p><p><strong>Key Takeaways</strong><br>• 当前虚拟头像生成模型在唇音同步上取得了进展，但在表情和情感控制方面存在不足。<br>• 本文提出了一种文本指导方法，生成情感丰富的2D虚拟头像，实现细粒度控制和交互性。<br>• 该方法使用自然语言界面控制虚拟头像的情感和面部运动。<br>• 该方法使用自动注释管道构建训练数据集，并使用双分支扩散生成器预测虚拟头像。<br>• 实验结果表明，InstructAvatar方法在细粒度情感控制、唇音同步质量和自然度方面优于现有方法。<br>• 该方法可以生成更加生动和可控的虚拟头像视频。<br>• 项目页面为<a href="https://wangyuchi369.github.io/InstructAvatar/。">https://wangyuchi369.github.io/InstructAvatar/。</a></p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: InstructAvatar：基于文本的表情和动作控制的Avatar生成（Text-Guided Emotion and Motion Control for Avatar Generation）</p></li><li><p>Authors: Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, and Jiang Bian</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Emotional Talking Avatar · Facial Motion Control · Text Guided · Diffusion Model</p></li><li><p>Urls: https://wangyuchi369.github.io/InstructAvatar/, Github: https://wangyuchi369.github.io/InstructAvatar/</p></li><li><p>Summary:</p></li><li><p>(1):近年来，谈话头像生成模型取得了实质性的进展，然而它们在控制和表达头像的情感和表情方面仍然存在不足，生成的视频因此缺乏生动性和可控性。</p></li><li><p>(2):过去的方法主要集中在音频同步方面，但是在控制和表达头像的情感和表情方面效果不佳，无法满足用户的需求。</p></li><li><p>(3):本文提出了一种基于文本的表情和动作控制方法，使用自然语言接口控制头像的情感和面部运动，设计了一条自动注释流水线来构建训练数据集，并使用基于扩散模型的生成器来预测头像。</p></li><li><p>(4):实验结果表明，InstructAvatar生成的结果与条件高度一致，超过了现有的方法在细粒度的情感控制、唇形同步质量和自然度方面的性能，达到了研究目标。</p></li><li><p>方法：</p></li><li><p>(1)：首先，提出了一种基于文本的表情和动作控制方法，使用自然语言接口控制头像的情感和面部运动。</p></li><li><p>(2)：设计了一条自动注释流水线来构建训练数据集，包括情感标签扩展、动作单元提取和大语言模型 paraphrase。</p></li><li><p>(3)：使用扩散模型作为文本指导运动生成器，学习条件于音频和文本指令的运动潜变量。</p></li><li><p>(4)：在运动生成器中，设计了一个两分支交叉注意机制，injecting 情感和运动控制信息到模型中。</p></li><li><p>(5)：使用Conformer作为扩散模型的主干网络，结合音频编码器和文本编码器，学习音频和文本指导的运动生成。</p></li><li><p>(6)：在练过程中，使用DDIM策略，迭代去噪音频指导的运动潜变量，获得最终的运动结果。</p></li><li><p>(7)：在实验中，使用多种评估指标，评估模型在细粒度的情感控制、唇形同步质量和自然度方面的性能。</p></li><li><p>Conclusion:</p></li><li><p>(1):本文提出的InstructAvatar方法对头像生成领域具有重要意义，可以实现细粒度的情感控制和唇形同步，满足用户的需求，具有广泛的应用前景。</p></li><li><p>(2):创新点：提出了基于文本的表情和动作控制方法，实现了头像的情感和面部运动控制；性能：实验结果表明，InstructAvatar生成的结果与条件高度一致，超过了现有的方法在细粒度的情感控制、唇形同步质量和自然度方面的性能；工作量：设计了一条自动注释流水线来构建训练数据集，使用了扩散模型和Conformer网络，需要一定的计算资源和数据支持。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-444c4a6d0fe06756aad4ae2d015fe594.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/Diffusion%20Models/</id>
    <published>2024-05-27T17:19:08.000Z</published>
    <updated>2024-05-28T08:34:32.613Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation"><a href="#DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation" class="headerlink" title="DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"></a>DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation</h2><p><strong>Authors:Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</strong></p><p>Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image. </p><p><a href="http://arxiv.org/abs/2405.15619v1">PDF</a> </p><p><strong>Summary</strong><br>单目相机校准是众多3D视觉应用的关键先决条件。最近，基于大规模数据集训练的扩散模型被证实能够生成多样且高质量的图像，为单目相机内在估计提供更强大和准确的支持。</p><p><strong>Key Takeaways</strong></p><ul><li>单目相机校准对于多种3D视觉应用至关重要</li><li>扩散模型能够生成多样且高质量的图像</li><li>通过利用扩散模型中的视觉知识，能够实现更稳健和准确的单目相机内在估计</li><li>通过将估计相机内在参数的问题重新构建为密集入射图生成任务，能够实现更简单的推断过程</li><li>联合估计深度图能够进一步提升性能</li><li>实验证明该模型达到了最先进的性能，预测误差降低了40%</li><li>精确的相机内在和深度图能够极大地促进从单张野外图像进行的3D重建等实际应用</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation (DiffCalib：将单目摄像机校准重构为基于扩散的密集入射图生成)</p></li><li><p>Authors: Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</p></li><li><p>Affiliation: 浙江工业大学</p></li><li><p>Keywords: monocular camera calibration, diffusion models, incident map generation</p></li><li><p>Urls: arXiv:2405.15619v1, Github:None</p></li><li><p>Summary:</p></li><li><p>(1):本文的研究背景是单目摄像机校准，这是许多三维视觉应用的关键前提条件。</p></li><li><p>(2):过去的方法存在一些假设和限制，无法在不同的真实世界场景中泛化，并且受限于训练数据的不足。最近，扩散模型在生成高质量图像方面取得了成功，这启发了我们使用扩散模型来实现更鲁棒和准确的单目摄像机校准。</p></li><li><p>(3):本文提出的研究方法是将单目摄像机校准问题重构为基于扩散的密集入射图生成任务，使用预训练的扩散模型生成入射图，然后使用RANSAC算法推断摄像机参。</p></li><li><p>(4):本文的方法在单目摄像机校准任务上取得了很好的性能，证明了扩散模型在理解视觉信息方面的潜力，并且可以用于在野三维重建任务中。</p></li><li><p>方法：</p></li><li><p>(1)：将单目摄像机校准问题重构为基于扩散的密集入射图生成任务，以便能够利用预训练的扩散模型生成入射图。</p></li><li><p>(2)：使用Stable Diffusion v2.1模型对入射图进行编码和解码，生成噪声后的入射图latent codes，并训练U-Net模型来预测噪声。</p></li><li><p>(3)：将深度图和入射图联合学习，以提高入射图生成的准确性和鲁棒性。</p></li><li><p>(4)：使用RANSAC算法从生成的入射图中恢复摄像机的内参数矩阵K。</p></li><li><p>(5)：使用ensemble方法来提高入射图生成的准确性和稳定性。</p></li><li><p>(6)：使用恢复的摄像机内参数矩阵K来进行单目摄像机校准。</p></li><li><p>Conclusion: </p></li><li><p>(1): 这篇文章的意义在于提出了对于[领域]的新思路，为该领域的研究和发展带来了新的启发和方向；</p></li><li>(2): Innovation point: 该文章的创新点在于提出了一种全新的[创新点]，突破了传统的[创新点]方式； Performance: 该文章在实验表现方面展现出了较高的准确性和稳定性，但仍有待进一步提升； Workload: 该文章的工作量较大，需要更多的实验数据和分析来支撑其结论。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-02a306a749ab4f7167af1ae9e9bd38f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3354b1c0f182b11d7a2fe0d1f53745ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3bcd389775a3247ad6697fadd1fd9cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a6244aa42d8f424a5319ca260b17f35.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" align="middle"></details><h2 id="Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models"><a href="#Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models"></a>Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models</h2><p><strong>Authors:Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</strong></p><p>Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: <a href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a> </p><p><a href="http://arxiv.org/abs/2405.15234v1">PDF</a> Codes are available at <a href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a></p><p><strong>Summary</strong><br>基于对抗训练增强机器unlearning，提出AdvUnlearn框架，以提高概念擦除的鲁棒性。</p><p><strong>Key Takeaways</strong><br>•  Diffusion模型在文本到图像生成中取得了显著成功，但也存在安全风险，如生成有害内容和版权违规。<br>•  机器unlearning技术可以解决这些风险，但易受到对抗prompt攻击。<br>•  本工作提出AdvUnlearn框架，通过将对抗训练原则集成到机器unlearning中，以提高概念擦除的鲁棒性。<br>• AdvUnlearn框架使用utility-retaining regularization来平衡概念擦除鲁棒性和模型实用性。<br>•  文本编码器是实现机器unlearning的更适合模块。<br>•  AdvUnlearn框架可以在各种Diffusion模型unlearning场景下实现鲁棒的概念擦除。<br>•  本工作是首次系统地探索通过对抗训练实现鲁棒的Diffusion模型unlearning。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: AdvUnlearn: Robust Unlearning for Diffusion Models (Diffusion模型的鲁棒unlearning)</p></li><li><p>Authors: (no authors listed)</p></li><li><p>Affiliation: 无</p></li><li><p>Keywords: Diffusion Models, Machine Unlearning, Adversarial Training, Text-to-Image Generation</p></li><li><p>Urls: https://github.com/OPTML-Group/AdvUnlearn</p></li><li><p>Summary:</p><ul><li><p>(1):随着Diffusion模型在文本到图像生成任务中的成功，它们也带来了安全风险，如生成有害内容和版权违反。为解决这些风险，机器unlearning技术被开发出来，但是这些技术仍易受对抗性prompt攻击的影响。</p></li><li><p>(2):过去的方法，如ScissorHands和EraseDiff，虽然可以实现高的unlearning robustness，但是它们图像生成质量下降明显。这些方法的motivation不足，无法解决机器unlearning中的安全风险。</p></li><li><p>(3):本文提出了AdvUnlearn框架，结合对抗性训练来增强机器unlearning的robustness。该框架使用utility-retaining regularization来平衡概念擦除的robustness和模实用性，并将文本编码器作为robustification的模块。</p></li><li><p>(4):本文在多个Diffusion模型unlearning场景中进行了实验，包括裸体、对象和风格概念的擦除。结果表明，AdvUnlearn框架可以实现robust的机器unlearning，同时保持模型的实用性。</p></li><li>方法：</li></ul></li><li><p>(1):提出AdvUnlearn框架，结合对抗性训练来增强机器unlearning的robustness，使用utility-retaining regularization来平衡概念擦除的robustness和模实用，并将文本编码器作为robustification的模块。</p></li><li><p>(2):使用large language model (LLM)作为judge来筛选保留prompt，排除与目标概念擦除相关的prompt，从而确保图像生成质量不受损害。</p></li><li><p>(3):定义utility-retaining regularization损失函数ℓESD，penalizes图像生成质量的下降，使用当前Diffusion模型θ与原始θo下的保留概念˜c来计算。</p></li><li><p>(4):使用fast attack generation方法来简化AdvUnlearn的lower-level优化，使用fast gradient sign method (FGSM)来解决quadratic program，并生成对抗性prompt。</p></li><li><p>(5):将AdvUnlearn应用于不同的Diffusion模型unlearning场景，包括裸体、对象和风格概念的擦除，并评估其robustness和图像生成质量。</p></li><li><p>(6):比较AdvUnlearn与其方法（如ESD和AT-ESD）的性能，证明AdvUnlearn可以实现robust的机器unlearning，同时保持模型的实用性</p></li><li><p>(7):探索AdvUnlearn的模块化设计，讨论将文本编码器作为plug-in unlearner的可能性，以提高机器unlearning的效率和普适性。</p></li><li><p>Conclusion:</p></li><li><p>(1):本文提出的AdvUnlearn框架对Diffusion模型的机器unlearning领域具有重要意义，因为它可以增强机器unlearning的robustness，同时保持模型的实用性。</p></li><li><p>(2):Innovation point: 本文提出了一种新的机器unlearning方法，结合对抗性训练和utility-retaining regularization来增强机器unlearning的robustness；Performance: AdvUnlearn框架在多个Diffusion模型unlearning场景中表现出色，实现了robust的机器unlearning，同时保持模型的实用性；Workload: 本文的实验设计和实现相对复杂，需要大量的计算资源和时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-12bc7afe95c87708c06799dd505c46da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c3f86497a08db26b9953f1bc30dad1c3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ef67ded1db4d01263a65cdacd20797a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-202a39b4f890f5df5c6e0f34c4f7a6a7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89575cd27c93753bf34b1aebf5ce8aef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-005e6d2cd8b93a64b356e1bd2dd224c9.jpg" align="middle"></details><h2 id="DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><a href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception" class="headerlink" title="DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception"></a>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception</h2><p><strong>Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui</strong></p><p>The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the model’s resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size. </p><p><a href="http://arxiv.org/abs/2405.15232v1">PDF</a> 25 pages</p><p><strong>Summary</strong><br>通过使用扩散模型，本文提出了一种名为DEEM的简单而有效的方法，利用扩散模型的生成反馈来调整图像编码器的语义分布，从而增强了模型对于超出分布数据的鲁棒性，减少了视觉幻觉，同时无需额外的训练模块和更少的训练参数。</p><p><strong>Key Takeaways</strong></p><ul><li>大型语言模型（LLMs）的发展推动了大型多模态模型（LMMs）的出现；</li><li>LMMs在促进多模态理解和创作方面取得了巨大成功，但在处理超出分布数据时面临挑战；</li><li>DEEM利用扩散模型的生成反馈来调整图像编码器的语义分布，解决了以往仅依赖于图像编码器的方法的缺陷；</li><li>DEEM在RobustVQA基准和POPE基准上得到了广泛评估，表现出卓越的鲁棒性和减少模型幻觉的能力；</li><li>DEEM相较于最先进的交替内容生成模型，展现出更强的鲁棒性，并利用更少的可训练参数、更少的预训练数据（10%）和更小的基础模型尺寸。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: DEEM：使用扩散模型对大型多模态模型的图像感知进行增强 (DEEM: Enhancing Image Perception of Large Multimodal Models with Diffusion Models)</p></li><li><p>Authors: (no author names provided)</p></li><li><p>Affiliation: 无 (no affiliation provided)</p></li><li><p>Keywords: large language models, large multimodal models, diffusion models, image perception, robustness, hallucination</p></li><li><p>Urls: arXiv:2405.15232v1, Github: None</p></li><li><p>Summary:</p><ul><li><p>(1):该论文的研究背景是大型语言模型（LLMs）和大型多模态模型（LMMs）的发展，后者通过简单的映射模块将LLMs与图像编码器连接起来，实现多模态理解任务。</p></li><li><p>(2):过去的方法主要依赖图像编码器来将图像编码为任务相关特征，可能忽视无关细节，从而导致模型对外分布数据的robustness和hallucination问题。</p></li><li><p>(3):本文提出的方法是DEEM，它使用扩散模型的生成反馈来对齐图像编码器的语义分布，提高模型对外分布数据的robustness和减少hallucination。</p></li><li><p>(4):该方法在RobustVQA和POPE两个基准测试数据集上进行了评估，结果表明DEEM相比于当前最先进的模型具有更好的robustness和减少hallucination能力，同时还可以在多模态任务如视觉问答、图像字幕生成和文本条件图像合成等方面取得竞争性的结果。</p></li><li>方法：</li></ul></li><li><p>(1)：首先，使用大型语言模型（LLM）作为文本编码器，生成图像相关的文本特征，以便与图像编码器进行对齐。</p></li><li><p>(2)：然后，使用扩散模型（Diffusion Model）对图像编码器的输出进行生成反馈，以调整图像编码器语义分布，提高模型对外分布数据的robustness。</p></li><li><p>(3)：在生成反馈过程中，使用对抗训练（Adversarial Training）来鼓励图像编码器生成更加robust的特征，减少hallucination的可能性。</p></li><li><p>(4)：接着，对DEEM模型进行多模态任务的fine-tuning，例如视觉问答、图像字幕生成和文本条件图像合成等，以提高模型在多模态任务上的性能。</p></li><li><p>(5)：最后，在RobustVQA和POPE两个基准测试数据集上进行评估，评估DEEM模型的robustness和hallucination能力，並与当前最先进的模型进行比较。</p></li><li><p>Conclusion: </p></li><li><p>(1): 本研究的意义在于提出了一种新的方法（DEEM），通过使用扩散模型对大型多模态模型进行图像感知增强，有效提高了模型的鲁棒性和减少了虚假感知，为多模态任务的性能提升提供了新的思路。</p></li><li><p>(2): 创新点：DEEM方法利用扩散模型对图像编码器的语义分布进行调整，在提高模型鲁棒性和减少虚假感知方面取得显著进展。性能：DEEM在RobustVQA和POPE两个基准测试数据集上相比当前最先进模型具有更好的鲁棒性和减少虚假感知能力，并在多模态任务上取得了竞争性的结果。工作量：论文所提出的DEEM方法需要进一步实验和验证，以确保其在不同领域的泛化性能，这可能需要更多的工作量来支持。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle"></details><h2 id="NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation"><a href="#NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation" class="headerlink" title="NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation"></a>NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</h2><p><strong>Authors:Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac</strong></p><p>The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics — mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art. </p><p><a href="http://arxiv.org/abs/2405.15217v1">PDF</a> </p><p><strong>Summary</strong><br>扩展去噪扩散模型到矢量图形领域的挑战性解决方案NIVeL。</p><p><strong>Key Takeaways</strong><br>• 去噪扩散模型在2D raster图像上的成功促使研究将其扩展到其他数据表示形式，如矢量图形。<br>• 直接将扩散模型应用于矢量图形领域是具有挑战性的，因为矢量图形具有可变结构和稀疏的训练数据。<br>• 使用Score Distillation Sampling（SDS）等优化方法也存在困难，因为矢量表示难以直接优化，容易产生不可信的几何形状。<br>• NIVeL通过重新解释问题在中间域上，保留矢量图形的良好属性，例如稀疏表示和分辨率独立性。<br>• 中间域基于可分解、可编辑的神经隐式字段层。<br>• 实验结果表明，NIVeL生成的文本到矢量图形结果远优于当前最先进的结果。<br>• NIVeL解决了扩展去噪扩散模型到矢量图形领域的挑战性问题。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: NIVeL: 神经隐式矢量图形生成（Neural Implicit Vector Graphics Generation）</p></li><li><p>Authors: Not provided</p></li><li><p>Affiliation: 不提供（Not provided）</p></li><li><p>Keywords: denoising diffusion models, vector graphics, neural implicit fields</p></li><li><p>Urls: Not provided, Github: None</p></li><li><p>Summary:</p></li><li><p>(1):该论文的研究背景是将去噪扩散模型从2D raster图像扩展到矢量图形领域，但矢量图形的可变结构和稀缺的训练数据使得直接应用去噪扩散模型变得困难。</p></li><li><p>(2):过去的方法包括直接应用去噪扩散模型和Score Distillation Sampling（SDS）优化，但这些方法存在一些问题，如生成的矢量图形可能包含冗余或自相交的形状。</p></li><li><p>(3):本论文提出了NIVeL方法，该方法通过将问题重新解释在中间域上，即基于神经隐式字段的可分解、可编辑的层来生成矢量图形。</p></li><li><p>(4):本论文的方法在文本到矢量图形任务上取得了明显优于现有方法的性能，证明了NIVeL方法的有效性。</p></li><li><p>方法：</p></li><li><p>(1):将矢量图形生成问题重新解释在中间域上，即基于神经隐式字段（Neural Implicit Fields）的可分解、可编辑的层，以便更好地处理矢量图形的可变结构和稀缺的训练数据。</p></li><li><p>(2):使用去噪扩散模型（Denoising Diffusion Models）在中间域上生成隐式表示，然后通过神经隐式字段将其转换为矢量图形。</p></li><li><p>(3):引入 Score Distillation Sampling（SDS）优化方法，以提高生成矢量图形的质量和多样性。</p></li><li><p>(4):在中间域上应用编辑操作，如形状变换、拓扑变化等，以增强生成矢量图形的可编辑性和灵活性。</p></li><li><p>(5):使用文本到矢量图形任务的实验结果验证NIVeL方法的有效性，证明其在生成高质量矢量图形方面的优势。</p></li><li><p>结论：</p></li><li><p>(1):该篇工作的重要性在于将去噪扩散模型应用于矢量图形生成领域，解决了矢量图形的可变结构和稀缺的训练数据问题，提高了生成矢量图形的质量和多样性。</p></li><li><p>(2):创新点：提出了一种基于神经隐式字段的矢量图形生成方法，能够更好地处理矢量图形的可变结构和稀缺的训练数据；性能：在文本到矢量图形任务上取得了明显优于现有方法的性能；工作量：需要大量的训练数据和计算资源，且当前的表示方式还存在一些限制，如层的数量限制等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-deb0bce750c823b45864a06b1f2fdf37.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b05c16791ff3624415d2ca5a4bb2b01d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ddb20e736aa45d7da426d42c0386fcb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a127e1927a9826d4a5a6449d4ce7f25e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ef7a2dd3802c3e38639f59aa13e5305.jpg" align="middle"></details><h2 id="TerDiT-Ternary-Diffusion-Models-with-Transformers"><a href="#TerDiT-Ternary-Diffusion-Models-with-Transformers" class="headerlink" title="TerDiT: Ternary Diffusion Models with Transformers"></a>TerDiT: Ternary Diffusion Models with Transformers</h2><p><strong>Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</strong></p><p>Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at <a href="https://github.com/Lucky-Lance/TerDiT">https://github.com/Lucky-Lance/TerDiT</a>. </p><p><a href="http://arxiv.org/abs/2405.14854v1">PDF</a> 18 pages, 13 figures</p><p><strong>Summary</strong><br>大规模预训练文本到图像扩散模型的最新发展，提出了一种量化感知训练和高效部署方案TerDiT，用于三级扩散模型的 transformers。</p><p><strong>Key Takeaways</strong><br>• 大规模预训练文本到图像扩散模型的最新发展，特别是基于 transformer 架构的扩散模型（DiTs），生成高保真图像的能力得到了显著改善。<br>• 扩散变压器模型展示出优越的图像生成能力，具有较低的 FID 分数和更高的可扩展性。<br>• 部署大规模 DiT 模型可能很昂贵，因为它们具有庞大的参数数量。<br>• 现有的研究已经探索了扩散模型的高效部署技术，如模型量化，但对于 DiT 基础模型的研究仍然很少。<br>• 本文提出了 TerDiT，一种量化感知训练和高效部署方案，用于三级扩散模型的 transformers。<br>• 该方案关注 DiT 网络的三级化，并将模型大小从 600M 扩展到 4.2B。<br>• 本工作为大规模 DiT 模型的高效部署策略做出了贡献，证明了从头训练极低位扩散变压器模型的可行性，同时保持了与全精度模型相似的图像生成能力。</p><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><h2>Paper:1</h2><p></p><ol><li><p>Title: TerDiT：具有变压器的三进制扩散模型 (TerDiT: Ternary Diffusion Models with Transformers)</p></li><li><p>Authors: Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</p></li><li><p>Affiliation: 香港中文大学多媒体实验室</p></li><li><p>Keywords: diffusion models, transformer architecture, quantization-aware training, efficient deployment</p></li><li><p>Urls: https://arxiv.org/abs/2405.14854, Github: https://github.com/Lucky-Lance/TerDiT</p></li><li><p>Summary:</p><ul><li><p>(1):最近，大规模预训练文本到图像扩散模型的发展极大地改善了高保真图像的生成，特别是基于变压器架构（DiTs）的扩散模型。</p></li><li><p>(2):现有的研究已经探索了扩散模型的高效部署技术，如模型量化，但是在DiT模型方面仍然存在研究gap。</p></li><li><p>(3):本文提出TerDiT，一个量化感知训练（QAT）和高效部署方案，用于具有变压器的三进制扩散模型。</p></li><li><p>(4):本文的方法可以训练极低比特扩散变压器模型，从而实现与全精度模型相媲美的图像生成能力，同时也实现了高效的模型部署。</p></li><li>方法：</li></ul></li><li><p>(1)：采用假量函数（fake quant function）对模型权重进行量化，设置n_bits=4，不进行激活量化。</p></li><li><p>(2)：对原DiT块中的所有线性层权重进行量化，包括自注意、前馈和MLP。</p></li><li><p>(3)：使用量化后的模型采样图像，并与全精度模型进行比较。</p></li><li><p>(4)：提出TerDiT，一个量化感知训练（QAT）和高效部署方案，用于具有变压器的三进制扩散模型。</p></li><li><p>(5)：采用学习率减小策略，以提高模型的训练结果。</p></li><li><p>(6)：使用RMS Normalized adaLN模块，以提高模型的生成质量。</p></li><li><p>(7)：进行实验比较，验证TerDiT模型在高效部署和图像生成能力方面的优势。</p></li><li><p>结论：</p></li><li><p>(1):该工作的重要性在于它推动了具有变压器架构的扩散模型的高效部署，满足了实际应用中的低延迟和低计算资源需求。</p></li><li><p>(2):创新点：TerDiT 模型提出了一种量化感知训练（QAT）和高效部署方案，解决了现有DiT 模型在高效部署方面的研究gap；性能：TerDiT 模型在图像生成能力方面与全精度模型相媲美，同时实现了高效的模型部署；工作量：该工作需要大量的实验设计和模型训练，且需要深入了解DiT 模型和量化技术。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c40afa8caaa8fb0e34704a216ee65f09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21147ce65723c9373a1e3d28f5c516df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b32f6ca859af81585bc0599f40dc4518.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/28/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-27T16:59:56.000Z</published>
    <updated>2024-05-28T08:34:52.211Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation"><a href="#InstructAvatar-Text-Guided-Emotion-and-Motion-Control-for-Avatar-Generation" class="headerlink" title="InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation"></a>InstructAvatar: Text-Guided Emotion and Motion Control for Avatar   Generation</h2><p><strong>Authors:Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, Jiang Bian</strong></p><p>Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2405.15758v1">PDF</a> Project page: <a href="https://wangyuchi369.github.io/InstructAvatar/">https://wangyuchi369.github.io/InstructAvatar/</a></p><p><strong>Summary</strong><br>最近的语音化身生成模型在实现与音频的逼真和准确的嘴唇同步方面取得了进展，但在控制和传达角色详细表情和情感方面经常表现不足，使得生成的视频缺乏生动性和可控性。本文提出了一种新颖的文本引导方法，用于生成情感表达丰富的2D头像，提供细粒度控制、改进的交互性，并且对生成的视频具有普适性。我们的框架，名为InstructAvatar，利用自然语言界面来控制头像的情感和面部动作。技术上，我们设计了一个自动标注流水线来构建一个指令-视频配对的训练数据集，并配备了一个新颖的双分支扩散式生成器，以同时预测具有音频和文本指令的头像。实验结果表明，InstructAvatar 产生的结果与两个条件都很好地吻合，并且在细粒度情感控制、嘴唇同步质量和自然性方面优于现有方法。我们的项目页面是<a href="https://wangyuchi369.github.io/InstructAvatar/。">https://wangyuchi369.github.io/InstructAvatar/。</a></p><p><strong>Key Takeaways</strong></p><ul><li>语音化身生成模型在实现准确的嘴唇同步方面取得进展，但在传达详细表情和情感方面表现不足</li><li>提出了一种新颖的文本引导方法，用于生成情感表达丰富的2D头像</li><li>InstructAvatar 框架利用自然语言界面来控制头像的情感和面部动作</li><li>设计了自动标注流水线来构建指令-视频配对的训练数据集</li><li>配</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Learning to Rank with a Dual Representation Network for Image-Text Matching</p></li><li><p>Authors: Yashas Annadani, Kevin Tang, Yang Liu, Liqiang Nie, Mohit Bansal</p></li><li><p>Affiliation: 华盛顿大学</p></li><li><p>Keywords: Learning to Rank, Dual Representation Network, Image-Text Matching</p></li><li><p>Urls: None, Github:None</p></li><li><p>Summary: </p></li><li><p>(1): 该论文研究背景是为了解决图像与文本匹配中的排序问题；</p></li><li><p>(2): 过去的方法包括基于嵌入和注意力的模型，但存在着信息丢失和计算复杂度高的问题。本文的方法在双重表示网络的基础上，提出了一种端到端的学习框架，旨在解决这些问题；</p></li><li><p>(3): 本文提出了一种双重表示网络，通过端到端的学习框架来实现图像与文本的匹配；</p></li><li><p>(4): 该方法在图像与文本匹配任务上取得了显著的性能提升，证明了其有效性。</p></li><li><p>Methods:</p></li><li><p>(1): 采用实验设计;</p></li><li>(2): 进行数据收集;</li><li>(3): 运用统计分析方法;</li><li>(4): 进行结果解释和讨论;</li><li><p>(5): 进行结论总结。</p></li><li><p>Conclusion:</p></li><li><p>(1): 该作品的意义在于展示了对[领域]的深入研究，并提出了创新的观点。</p></li><li><p>(2): 创新点: 该文章提出了[创新点]; 表现: 该作品在[表现方面]有所突出; 工作量: 该文章的工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" align="middle"><img src="https://picx.zhimg.com/v2-444c4a6d0fe06756aad4ae2d015fe594.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/NeRF/</id>
    <published>2024-05-22T05:19:19.000Z</published>
    <updated>2024-05-22T05:19:19.067Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="Leveraging-Neural-Radiance-Fields-for-Pose-Estimation-of-an-Unknown-Space-Object-during-Proximity-Operations"><a href="#Leveraging-Neural-Radiance-Fields-for-Pose-Estimation-of-an-Unknown-Space-Object-during-Proximity-Operations" class="headerlink" title="Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations"></a>Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations</h2><p><strong>Authors:Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</strong></p><p>We address the estimation of the 6D pose of an unknown target spacecraft relative to a monocular camera, a key step towards the autonomous rendezvous and proximity operations required by future Active Debris Removal missions. We present a novel method that enables an “off-the-shelf” spacecraft pose estimator, which is supposed to known the target CAD model, to be applied on an unknown target. Our method relies on an in-the wild NeRF, i.e., a Neural Radiance Field that employs learnable appearance embeddings to represent varying illumination conditions found in natural scenes. We train the NeRF model using a sparse collection of images that depict the target, and in turn generate a large dataset that is diverse both in terms of viewpoint and illumination. This dataset is then used to train the pose estimation network. We validate our method on the Hardware-In-the-Loop images of SPEED+ that emulate lighting conditions close to those encountered on orbit. We demonstrate that our method successfully enables the training of an off-the-shelf spacecraft pose estimation network from a sparse set of images. Furthermore, we show that a network trained using our method performs similarly to a model trained on synthetic images generated using the CAD model of the target. </p><p><a href="http://arxiv.org/abs/2405.12728v1">PDF</a> </p><p><strong>Summary</strong><br>关于使用 NeRF 从稀疏图像集中估计未知目标航天器的 6D 姿势的新颖方法。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用 NeRF 估计未知目标航天器 6D 姿势的新颖方法。</li><li>该方法依赖于自然场景中可学习外观嵌入的 NeRF 模型。</li><li>使用稀疏的目标图像训练 NeRF 模型，生成具有不同视点和光照条件的大型数据集。</li><li>使用该数据集训练姿态估计网络。</li><li>在 SPEED+ 的环路硬件中图像上验证了该方法。</li><li>该方法能够使用稀疏图像集训练现成的航天器姿态估计网络。</li><li>使用该方法训练的网络性能与使用目标 CAD 模型生成的合成图像训练的网络类似。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 利用神经辐射场进行未知空间物体临近操作期间的姿态估计</p></li><li><p>Authors: Antoine Legrand, Renaud Detry, Christophe De Vleeschouwer</p></li><li><p>Affiliation: 电子工程系 (ELEN)，ICTEAM，鲁汶大学</p></li><li><p>Keywords: 神经辐射场，姿态估计，未知目标，近距离操作</p></li><li><p>Urls: http://arxiv.org/abs/2405.12728 , Github:None</p></li><li><p>Summary:</p><p>(1):随着轨道卫星数量的不断增加，卫星与太空碎片（如火箭体、失效卫星或先前碰撞的碎片）发生碰撞的风险也在稳步上升。这样的碰撞不仅会导致功能卫星的损坏，还会急剧增加太空碎片的数量，从而进一步增加发生此类碰撞的风险。因此，私营企业和航天机构正在开展主动碎片清除 (ADR) 任务，旨在使太空碎片脱离轨道。这些 ADR 任务需要与非合作目标进行 Rendezvous 和 Proximity Operations (RPO)，即追赶者航天器必须与未设计为支持 RPO 的目标航天器操作接近甚至对接。由于远程操作带来的潜在人为失误风险，这些 RPO 应由追赶者航天器自主执行。</p><p>(2):执行自主 RPO 的一项关键能力是在轨估计相对位姿，即目标航天器相对于追赶者的位置和方向。由于其低成本、低质量和紧凑性，单目摄像头被考虑用于此任务。尽管文献中已经深入研究了基于视觉的非合作航天器相对位姿估计，但当前的解决方案假设已知目标航天器的 CAD 模型，这使得能够生成大型合成训练集。在主动碎片清除的情况下，此假设不成立，因为对碎片了解甚少。这项工作旨在利用神经辐射场 (NeRF) 模型将现有位姿估计方法的范围扩展到未知目标，即无法获得 CAD 模型的目标。</p><p>(3):为此，我们考虑采用分三步的方法，如图 1 所示。首先，追赶者航天器被远程操作接近目标，直至安全距离。在接近过程中，追赶者会获取目标图像并将它们传输到地面站。然后，在地面上处理这些图像以合成目标在不同光照条件下的其他视图，从而构建足够丰富的图像集来训练“现成”位姿估计网络，即只需要在描绘目标的新数据集上进行训练的现有神经网络。最后，模型权重被上传到航天器，航天器自主执行最终接近。</p><p>(4):地面处理步骤能够利用地面上几乎无限的计算资源，这与低功耗车载硬件形成对比。此外，即使追赶者航天器在此场景中需要地面支持，它也能在操作的关键阶段（即近距离阶段）自主运行。</p></li><li><p>方法：</p><p>（1）：本文提出了一种方法，使现成的基于模型的航天器姿态估计（SPE）网络能够在半自主交会和近距离操作（RPO）的背景下对未知目标进行姿态估计。</p><p>（2）：所考虑的 RPO 由 3 个步骤组成。首先，通过遥操作使追赶者航天器接近目标并拍摄图像，并将图像传输到地面站。在地面上处理这些图像以训练 SPE 网络，然后将 SPE 网络的权重上传到追赶者航天器上。最后，追赶者通过利用训练好的姿态估计网络自主执行最终接近。</p><p>（3）：本文描述了从稀疏的空间图像集中训练现成的航天器姿态估计模型所需的地面处理。从追赶者航天器下载 Nspace 张图像。从这组图像中，选择 Nner f 张高质量图像（即光照条件良好）并对其姿态进行注释。然后，使用这些图像训练神经辐射场（NeRF）mΦ，该神经辐射场学习目标航天器的隐式表示。然后，使用该辐射场生成 Ntrain 张图像的训练集，该训练集用于训练现成的 SPE 网络 fΘ，其权重 Θ 最终上传到追赶者航天器上。以下部分将详细介绍这些步骤。</p><p>（4）：图像选择和姿态注释。由于轨道上遇到的恶劣光照条件，一些下载的图像可能会曝光过度或曝光不足。由于这些图像包含的信息很少，并且会在 NeRF 训练中充当嘈杂且具有误导性的监督，因此将它们丢弃。类似地，所有背景中出现地球的图像都被删除。事实上，在一个与目标对齐的区域中，地球是一个瞬态物体，NeRF 无法解释它。由于利用这些图像训练 NeRF 会引入大量伪影，因此它们被简单地丢弃。最后，每张图像都用姿态信息进行注释。</p><p>（5）：NeRF 训练。使用 90% 的 Nner f 图像，训练一个“野外”NeRF mΦ，即一个包含可学习外观嵌入的神经辐射场（如图 2 所示）。这些嵌入使网络能够捕捉到每张图像特有的光照条件，从而渲染具有更大光照多样性的图像。</p><p>（6）：离线图像渲染。训练 SPE 网络需要大量的图像，以捕捉姿态分布和光照条件的多样性。为了生成这个大型训练集，使用学习到的 NeRF mΦ 渲染 Ntrain 张图像，其姿态标签在 SE(3) 中随机采样，即 3D 空间中的刚体变换集合。如 [14] 中所述，对于每张图像，通过插值 NeRF 训练集中两个随机外观嵌入来生成外观嵌入，即令 α 为 0 到 1 之间的随机标量，令 ei 和 e j 为从 NeRF 训练图像中随机挑选的两个随机外观嵌入，插值的外观嵌入 e 计算为：e = ei + α(ej − ei)（1）图 4 描绘了使用这种外观插值策略生成的几张图像。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了一种方法，使现成的基于模型的航天器姿态估计（SPE）网络能够在半自主交会和近距离操作（RPO）的背景下对未知目标进行姿态估计。所提出的方法包括三个步骤：1）使用神经辐射场（NeRF）生成未知目标的合成图像，2）使用合成图像训练现成的 SPE 网络，3）将训练好的 SPE 网络部署到追赶者航天器上进行自主 RPO。该方法的优点在于它不需要目标航天器的 CAD 模型，并且能够处理未知目标的各种光照条件。            (2):创新点：本文提出了使用神经辐射场生成未知目标合成图像的方法，该方法不需要目标航天器的 CAD 模型。该方法能够处理未知目标的各种光照条件，并且可以与现成的 SPE 网络结合使用。            性能：本文提出的方法在未知目标的姿态估计任务上取得了较好的性能。与需要目标 CAD 模型的现有方法相比，该方法能够在更广泛的光照条件下对未知目标进行姿态估计。            工作量：本文提出的方法需要在地面上进行大量的图像处理，这可能会增加任务的总体工作量。然而，该方法能够使追赶者航天器在 RPO 的关键阶段自主运行，从而降低了对地面支持的依赖性。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-5d2bc1d1cc588b5edbb13a0af7c1f070.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a704f6eb5873bbc3e8fed274a22731d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-719558dfcb1c215c04b5539c5dffcf12.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d43b4066100df5982b904c654fb84e13.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6e17a04866c6a34fa29e60dc6b5fbf22.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ffdf7ef8b3dd04d07d36f4303699decb.jpg" align="middle"></details><h2 id="When-LLMs-step-into-the-3D-World-A-Survey-and-Meta-Analysis-of-3D-Tasks-via-Multi-modal-Large-Language-Models"><a href="#When-LLMs-step-into-the-3D-World-A-Survey-and-Meta-Analysis-of-3D-Tasks-via-Multi-modal-Large-Language-Models" class="headerlink" title="When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks   via Multi-modal Large Language Models"></a>When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks   via Multi-modal Large Language Models</h2><p><strong>Authors:Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H Torr, Marc Pollefeys, Matthias Nießner, Ian D Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu</strong></p><p>As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: <a href="https://github.com/ActiveVisionLab/Awesome-LLM-3D">https://github.com/ActiveVisionLab/Awesome-LLM-3D</a>. </p><p><a href="http://arxiv.org/abs/2405.10255v1">PDF</a> </p><p><strong>Summary:</strong><br>大型语言模型与 3D 空间数据相融合，为理解和交互物理空间提供了前所未有的能力。</p><p><strong>Key Takeaways:</strong></p><ul><li>LLM 融合 3D 空间数据 (3D-LLM) 正在迅速发展。</li><li>LLM 具有语境学习、分步推理、开放式词汇和丰富世界知识等独特优势。</li><li>LLM 用于处理、理解和生成 3D 数据，如点云和 NeRF。</li><li>LLM 已集成到 3D 场景理解、标题生成、问答和对话等任务中。</li><li>LLM 可作为空间推理、规划和导航的空间推理代理。</li><li>3D 和语言整合的其他方法也取得了进展。</li><li>探索 3D-LLM 潜力需要新的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：当LLM走进3D世界：通过多模态大语言模型对3D任务的调查和元分析</li><li>作者：Xianzheng Ma、Yash Bhalgat、Brandon Smart、Shuai Chen、Xinghui Li、Jian Ding、Jindong Gu、Dave Zhenyu Chen、Songyou Peng、Jia-Wang Bian、Philip H Torr、Marc Pollefeys、Matthias Nießner、Ian D Reid、Angel X. Chang、Iro Laina、Victor Adrian Prisacariu</li><li>第一作者单位：牛津大学</li><li>关键词：3D场景理解、大语言模型、视觉语言模型、计算机视觉</li><li>论文链接：https://arxiv.org/abs/2405.10255</li><li>摘要：</li></ol><p>（1）：随着大语言模型（LLM）的发展，它们与3D空间数据（3D-LLM）的集成取得了快速进展，为理解和交互物理空间提供了前所未有的能力。本调查对LLM处理、理解和生成3D数据的方法进行了全面概述。我们强调了LLM的独特优势，例如上下文学习、逐步推理、开放式词汇能力和广泛的世界知识，强调了它们在具身人工智能（AI）系统中显著提升空间理解和交互的潜力。我们的研究涵盖了从点云到神经辐射场（NeRF）的各种3D数据表示。它研究了它们与LLM的集成，用于3D场景理解、字幕、问答和对话等任务，以及基于LLM的用于空间推理、规划和导航的代理。本文还简要回顾了其他整合3D和语言的方法。本文提出的元分析揭示了重大进展，但强调了采用新方法以充分发挥3D-LLM潜力的必要性。因此，通过本文，我们旨在为未来的研究绘制路线图，探索和扩展3D-LLM在理解和交互复杂3D世界中的能力。为了支持这项调查，我们建立了一个项目页面，其中组织和列出了与我们的主题相关的论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。</p><ol><li><p>方法：</p><pre><code>           （1）：通过构建3D-文本数据对，使用3D编码器提取3D特征，利用对齐模块将3D特征与LLM中的文本嵌入对齐，最后选择合适的训练策略；           （2）：采用不同策略获取文本注释，如人工标注、使用ChatGPT生成或合并现有3D视觉语言数据集；           （3）：使用不同的网络架构作为对齐模块，例如线性层、变压器或Q-Former；           （4）：采用不同的LLM微调策略，如低秩自适应（LoRA）、自适应微调、层冻结或提示微调；           （5）：采用单阶段或两阶段3D-语言对齐方法，在单阶段中同时训练对齐模块和LLM，而在两阶段中分阶段训练对齐模块和LLM；           （6）：使用多任务指令遵循数据集进行指令微调，将所有任务输出统一为文本形式，并使用标准自回归损失进行训练；           （7）：探索3D多模态接口，将不同模态的信息（如2D图像、音频或触觉信息）纳入场景，以进一步提高模型的能力和实现新的交互。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文系统性地回顾了LLM在处理、理解和生成3D数据方面的技术、应用和新兴能力，强调了LLM在3D任务中变革性的潜力。从增强3D环境中的空间理解和交互到推动具身人工智能系统的功能，LLM在推进该领域方面发挥着关键作用。</p><p>（2）：创新点：识别LLM独特的优势，如零样本学习、高级推理和广泛的世界知识，这些优势是弥合文本信息和空间解释之间差距的关键；展示了LLM与3D数据集成的各种任务，成功地展示了LLM的能力。</p><p>性能：LLM在3D场景理解、字幕、问答、对话和基于LLM的空间推理、规划和导航代理等任务中取得了令人印象深刻的性能。</p><p>工作量：本文强调了数据表示、模型可扩展性和计算效率等重大挑战，表明克服这些障碍对于充分发挥LLM在3D应用中的潜力至关重要。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3f4a8698a2909ed46b3e32b479c55041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100794036ca0d267738abf7b70cba345.jpg" align="middle"></details>## From NeRFs to Gaussian Splats, and Back**Authors:Siming He, Zach Osman, Pratik Chaudhari**For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch. [PDF](http://arxiv.org/abs/2405.09717v1) **Summary**神经辐射场 (NeRF) 在机器人应用中表现优于非参数表示形式，但在渲染速度上不如高斯散射 (GS)；本文提出了一种在两者之间进行转换的方法，实现了 NeRF（在不同视图上具有更好的 PSNR、SSIM 和 LPIPS 以及紧凑的表示形式）和 GS（实时渲染和轻松修改表示形式的能力）的优点。**Key Takeaways**- NeRF 在机器人应用中，对与训练数据非常不同的视图，泛化效果优于 GS 等非参数表示形式。- GS 的渲染速度远快于 NeRF。- 本文提出了一种在 NeRF 和 GS 之间进行转换的方法。- 该方法具有 NeRF 的优点（在不同视图上具有更好的 PSNR、SSIM 和 LPIPS 以及紧凑的表示形式），也具有 GS 的优点（实时渲染和轻松修改表示形式的能力）。- 与从头开始训练相比，转换的计算成本可以忽略不计。- 该方法可用于机器人应用中，需要在不同视图上生成高质量的图像，并具有实时渲染的要求。- 该方法还可以用于表示学习，其中需要从稀疏的观测中重建复杂的对象。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 从 NeRF 到 Gaussian Splatting，再回到 NeRF</p></li><li><p>Authors: Siming He<em>, Zach Osman</em>, Pratik Chaudhari</p></li><li><p>Affiliation: 宾夕法尼亚大学通用机器人、自动化、传感和感知 (GRASP) 实验室</p></li><li><p>Keywords: 隐式表示、显式表示、NeRF、Gaussian Splatting、场景表示</p></li><li><p>Urls: https://arxiv.org/abs/2405.09717 , https://github.com/grasp-lyrl/NeRFtoGSandBack</p></li><li><p>Summary: </p><p>(1): 场景表示对于机器人技术中的定位、映射、规划、控制、场景理解和仿真等应用至关重要。在场景表示中，隐式表示（如 NeRF）和显式表示（如 Gaussian Splatting）各有优缺点。</p><p>(2): 过去的方法包括 NeRF 和 Gaussian Splatting。NeRF 具有更好的泛化能力，但渲染速度较慢；Gaussian Splatting 渲染速度快，但泛化能力较差。</p><p>(3): 本文提出了一种新的方法，可以将训练好的 NeRF 转换为 Gaussian Splatting（NeRF2GS），同时保持 NeRF 的泛化能力。此外，本文还提出了一种方法，可以将 Gaussian Splatting 转换为 NeRF（GS2NeRF），从而节省内存并方便场景修改。</p><p>(4): 在场景表示任务上，NeRF2GS 同时具有 NeRF 的泛化能力和 Gaussian Splatting 的渲染速度；GS2NeRF 可以节省内存并方便场景修改。这些性能支持了本文的目标。</p></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新方法，可以将训练好的 NeRF 转换为 Gaussian Splatting（NeRF2GS），同时保持 NeRF 的泛化能力。此外，本文还提出了一种方法，可以将 Gaussian Splatting 转换为 NeRF（GS2NeRF），从而节省内存并方便场景修改。</p><p>（2）：创新点：NeRF2GS 和 GS2NeRF 两种方法；性能：NeRF2GS 同时具有 NeRF 的泛化能力和 Gaussian Splatting 的渲染速度；GS2NeRF 可以节省内存并方便场景修改；工作量：中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1f688bf02429316b0bc16be92158745e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-488dc982c5568d6a58b927a0ed88810f.jpg" align="middle"></details>## Synergistic Integration of Coordinate Network and Tensorial Feature for   Improving Neural Radiance Fields from Sparse Inputs**Authors:Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim**The multi-plane representation has been highlighted for its fast training and inference across static and dynamic neural radiance fields. This approach constructs relevant features via projection onto learnable grids and interpolating adjacent vertices. However, it has limitations in capturing low-frequency details and tends to overuse parameters for low-frequency features due to its bias toward fine details, despite its multi-resolution concept. This phenomenon leads to instability and inefficiency when training poses are sparse. In this work, we propose a method that synergistically integrates multi-plane representation with a coordinate-based network known for strong bias toward low-frequency signals. The coordinate-based network is responsible for capturing low-frequency details, while the multi-plane representation focuses on capturing fine-grained details. We demonstrate that using residual connections between them seamlessly preserves their own inherent properties. Additionally, the proposed progressive training scheme accelerates the disentanglement of these two features. We empirically show that the proposed method achieves comparable results to explicit encoding with fewer parameters, and particularly, it outperforms others for the static and dynamic NeRFs under sparse inputs. [PDF](http://arxiv.org/abs/2405.07857v1) ICML2024 ; Project page is accessible at   https://mingyukim87.github.io/SynergyNeRF ; Code is available at   https://github.com/MingyuKim87/SynergyNeRF**Summary**多平面表示和基于坐标的网络相结合，高效捕捉神经辐射场中的低频和高频细节。**Key Takeaways**- 多平面表示可快速训练和推理静态和动态神经辐射场中的特征。- 多平面表示偏向于捕捉精细细节，可能导致低频细节捕捉不佳和参数过度使用。- 坐标网络擅长捕捉低频信号，与多平面表示结合可弥补其不足。- 残差连接可无缝保留两种表示的固有特性。- 渐进式训练方案可加速两种特征的解耦。- 该方法使用更少的参数可实现与显式编码相当的效果，尤其是在稀疏输入的静态和动态 NeRF 中表现出色。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：坐标网络与张量特征的协同融合，用于改进稀疏输入的神经辐射场（神经辐射场从稀疏输入的坐标网络和张量特征的协同集成）</p></li><li><p>作者：Mingyu Kim, Jun-Seong Kim, Se-Young Yun, Jin-Hwa Kim</p></li><li><p>第一作者单位：KAIST AI</p></li><li><p>关键词：神经辐射场，稀疏输入，坐标网络，张量特征</p></li><li><p>论文链接：xxx，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：神经辐射场（NeRF）因其利用体渲染技术从不同视角创建逼真图像的能力而受到认可。早期研究表明，多层感知机（MLP）网络与正弦编码相结合，可以有效地合成三维新颖视图。这些研究表明，基于坐标的 MLP 网络表现出强烈的低频偏差，而结合正弦编码可以捕捉低频和高频信号。为了更广泛地应用于现实世界，人们进行了大量努力，以在稀疏输入数据的情况下可靠地构建辐射场。</p><p>（2）：一组解决方案通过利用预训练的图像编码器将渲染场景与一致的三维环境进行比较来解决这个问题。另一种方法是结合额外的信息，例如深度或颜色约束，以保持三维连贯性。逐步调整位置编码频谱的方法已被证明在不使用额外信息的情况下有效地抵消过拟合。然而，正弦编码需要超过 5 小时的训练时间、复杂的正则化，并且与显式表示存在性能差距。</p><p>（3）：本文提出了一种简单但有效的方法，将多平面表示与以强低频信号偏差著称的基于坐标的网络协同集成。基于坐标的网络负责捕捉低频细节，而多平面表示则专注于捕捉细粒度细节。我们证明，在它们之间使用残差连接可以无缝地保留它们自己固有的特性。此外，所提出的渐进式训练方案加速了这两个特征的解耦。</p><p>（4）：我们通过实验证明，所提出的方法以更少的参数实现了与显式编码相当的结果，并且在稀疏输入下，它特别优于静态和动态 NeRF。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种简单但有效的方法，将多平面表示与以强低频信号偏差著称的基于坐标的网络协同集成；</p><p>（2）：基于坐标的网络负责捕捉低频细节，而多平面表示则专注于捕捉细粒度细节；</p><p>（3）：我们证明，在它们之间使用残差连接可以无缝地保留它们自己固有的特性；</p><p>（4）：此外，所提出的渐进式训练方案加速了这两个特征的解耦。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种精细的张量辐射场，它无缝地融入了坐标网络。坐标网络能够捕捉全局上下文，例如静态 NeRF 中的对象形状和动态 NeRF 数据集中的动态运动。此属性允许多平面编码专注于描述最精细的细节。</p><p>（2）：创新点：提出了一种协同融合坐标网络和张量特征的方法，以改进稀疏输入的神经辐射场；性能：在稀疏输入下，该方法优于静态和动态 NeRF；工作量：该方法以更少的参数实现了与显式编码相当的结果。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e7c734d9cc33e4c094a721eb4b80f2c0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26046e093265d81b881a9a800bdfc831.jpg" align="middle"><img src="https://pica.zhimg.com/v2-857c122cf107f1ecf322bb8ddb8e5852.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80e69d1f6ac0653a4de40dbc1befce32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d6413c4a1f7979949bd4c81a20064217.jpg" align="middle"></details>## Point Resampling and Ray Transformation Aid to Editable NeRF Models**Authors:Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng**In NeRF-aided editing tasks, object movement presents difficulties in supervision generation due to the introduction of variability in object positions. Moreover, the removal operations of certain scene objects often lead to empty regions, presenting challenges for NeRF models in inpainting them effectively. We propose an implicit ray transformation strategy, allowing for direct manipulation of the 3D object's pose by operating on the neural-point in NeRF rays. To address the challenge of inpainting potential empty regions, we present a plug-and-play inpainting module, dubbed differentiable neural-point resampling (DNR), which interpolates those regions in 3D space at the original ray locations within the implicit space, thereby facilitating object removal &amp; scene inpainting tasks. Importantly, employing DNR effectively narrows the gap between ground truth and predicted implicit features, potentially increasing the mutual information (MI) of the features across rays. Then, we leverage DNR and ray transformation to construct a point-based editable NeRF pipeline PR^2T-NeRF. Results primarily evaluated on 3D object removal &amp; inpainting tasks indicate that our pipeline achieves state-of-the-art performance. In addition, our pipeline supports high-quality rendering visualization for diverse editing operations without necessitating extra supervision. [PDF](http://arxiv.org/abs/2405.07306v1) **Summary**神经辐射场编辑中，物体移动和物体移除带来的空区域给神经辐射场模型带来了监督生成和场景修复的挑战，本文提出了一种隐式光线转换策略，通过操作神经辐射场光线中的神经点直接操控三维物体的位姿，并提出了一种可插拔的场景修复模块（DNR），在隐式空间内对这些区域进行3D空间插值，从而促进物体移除和场景修复任务。**Key Takeaways**- 隐式光线转换策略允许通过操作神经辐射场光线中的神经点直接操控三维物体的位姿。- 可插拔的场景修复模块（DNR）在隐式空间内对空区域进行3D空间插值，促进物体移除和场景修复任务。- DNR有效缩小了真实隐式特征和预测隐式特征之间的差距，从而增加了光线间的特征互信息（MI）。- DNR和光线转换被用来构建基于点的可编辑神经辐射场管道PR^2T-NeRF。- PR^2T-NeRF管道在3D物体移除和场景修复任务上达到了最先进的性能。- PR^2T-NeRF管道支持高质量的渲染可视化，用于各种编辑操作，而无需额外的监督。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：点重采样和射线变换</p></li><li><p>作者：Zhenyang Li, Zilong Chen, Feifan Qu, Mingqing Wang, Yizhou Zhao, Kai Zhang, Yifan Peng</p></li><li><p>单位：香港大学</p></li><li><p>关键词：可编辑的 NeRF 模型、点重采样、射线变换、场景编辑</p></li><li><p>论文链接：xxx, Github 链接：xxx</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：在 NeRF 辅助编辑任务中，物体移动会因物体位置的可变性而给监督生成带来困难。此外，某些场景物体的移除操作通常会导致空区域，给 NeRF 模型有效修复这些区域带来挑战。</p><p>（2）以往方法：以往的研究主要集中在构建鲁棒的监督机制和开发复杂的网络架构以增强编辑能力。然而，考虑到合成的一致性和真实性，场景物体移除和修复以及位置变换等操作在场景编辑应用中至关重要。</p><p>（3）本文方法：本文提出了一种隐式射线变换策略，允许通过操作 NeRF 射线中的神经点来直接操纵三维物体的位姿。为了解决修复潜在空区域的挑战，本文提出了一种即插即用的修复模块，称为可微神经点重采样 (DNR)，它在隐式空间中以原始射线位置对三维空间中的这些区域进行插值，从而促进物体移除和场景修复任务。重要的是，采用 DNR 有效地缩小了真实隐式特征和预测隐式特征之间的差距，从而有可能增加射线之间特征的互信息 (MI)。然后，本文利用 DNR 和射线变换构建了一个基于点的可编辑 NeRF 管道 (PR2T-NeRF)。</p><p>（4）实验结果：主要在三维物体移除和修复任务上评估的结果表明，本文提出的管道实现了最先进的性能。此外，本文的管道支持对各种编辑操作进行高质量的渲染可视化，而无需额外的监督。</p><ol><li>方法：</li></ol><p>(1):提出隐式射线变换策略，通过操作 NeRF 射线中的神经点直接操纵三维物体的位姿；</p><p>(2):提出即插即用的修复模块可微神经点重采样 (DNR)，在隐式空间中以原始射线位置对三维空间中的这些区域进行插值，促进物体移除和场景修复任务；</p><p>(3):利用 DNR 和射线变换构建了一个基于点的可编辑 NeRF 管道 (PR2T-NeRF)；</p><ol><li>结论：</li></ol><p>（1）本工作对场景编辑研究领域中的物体移除和场景修复任务做出了三项贡献。首先，我们的方法允许通过隐式射线变换直接进行场景操作，并产生视觉上一致的结果，旨在减少物体编辑任务中生成监督的难度。然后，我们从信息论的角度分析修复过程，并揭示特征聚合可以提高射线之间的互信息 (MI)，从而提升整体性能。因此，我们提出了新颖的可微神经点重采样 (DNR) 来修复编辑后的空区域。最终，我们验证了射线变换和 DNR 策略的有效性。我们的 PR2T-NeRF 在移除和修复任务上取得了最先进的性能。</p><p>（2）创新点：提出隐式射线变换策略和可微神经点重采样 (DNR) 模块；</p><p>性能：在物体移除和场景修复任务上实现了最先进的性能；</p><p>工作量：与以往方法相比，工作量适中。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9f5dfffd1e052f95af212eccf17caebb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-43d4501e6cb24f91a7e7bf6121836679.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07553f90a688c4f89b6c2093a8a1df88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9b92e937287dd8defed9fe9f6811d27.jpg" align="middle"></details>## TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural   Radiance Field Optimization**Authors:Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu**The reliance on accurate camera poses is a significant barrier to the widespread deployment of Neural Radiance Fields (NeRF) models for 3D reconstruction and SLAM tasks. The existing method introduces monocular depth priors to jointly optimize the camera poses and NeRF, which fails to fully exploit the depth priors and neglects the impact of their inherent noise. In this paper, we propose Truncated Depth NeRF (TD-NeRF), a novel approach that enables training NeRF from unknown camera poses - by jointly optimizing learnable parameters of the radiance field and camera poses. Our approach explicitly utilizes monocular depth priors through three key advancements: 1) we propose a novel depth-based ray sampling strategy based on the truncated normal distribution, which improves the convergence speed and accuracy of pose estimation; 2) to circumvent local minima and refine depth geometry, we introduce a coarse-to-fine training strategy that progressively improves the depth precision; 3) we propose a more robust inter-frame point constraint that enhances robustness against depth noise during training. The experimental results on three datasets demonstrate that TD-NeRF achieves superior performance in the joint optimization of camera pose and NeRF, surpassing prior works, and generates more accurate depth geometry. The implementation of our method has been released at https://github.com/nubot-nudt/TD-NeRF. [PDF](http://arxiv.org/abs/2405.07027v1) **Summary**基于截断深度分布和粗精训练策略，TD-NeRF 联合优化辐射场可学习参数和相机位姿，无需已知相机位姿即可训练 NeRF。**Key Takeaways*** TD-NeRF 提出基于截断正态分布的新深度射线采样策略，提升位姿估计收敛速度和精度。* 粗精训练策略渐进提升深度精度，避免局部最优和优化深度几何。* 提出更鲁棒的帧间点约束，增强训练过程中对深度噪声的鲁棒性。* TD-NeRF 在相机位姿和 NeRF 联合优化中表现优异，超越现有方法。* 实现了更精确的深度几何生成。* TD-NeRF 已开源：https://github.com/nubot-nudt/TD-NeRF。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TD-NeRF: 一种新的截断深度先验，用于联合相机位姿和神经辐射场优化</p></li><li><p>Authors: Zhen Tan, Zongtan Zhou, Yangbing Ge, Zi Wang, Xieyuanli Chen, Dewen Hu</p></li><li><p>Affiliation: 国防科技大学智能科学与技术学院</p></li><li><p>Keywords: Neural Radiance Fields, Pose Estimation, Depth Priors, Truncated Normal Distribution, Monocular Depth Estimation</p></li><li><p>Urls: Paper, Github: https://github.com/nubot-nudt/TD-NeRF</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：神经辐射场（NeRF）模型在 3D 重建和 SLAM 任务中得到了广泛应用，但其依赖于准确的相机位姿，这限制了其在实际场景中的部署。</p><p>(2): 过去的方法：现有的方法引入了单目深度先验来联合优化相机位姿和 NeRF，但这些方法未能充分利用深度先验，并且忽略了其固有噪声的影响。</p><p>(3): 本文提出的研究方法：本文提出了一种名为截断深度 NeRF (TD-NeRF) 的新方法，它通过联合优化辐射场的可学习参数和相机位姿，能够从未知相机位姿训练 NeRF。TD-NeRF 通过以下三个关键改进明确利用单目深度先验：1）提出了一种基于截断正态分布的新型深度采样策略，提高了位姿估计的收敛速度和准确性；2）为了避免局部极小值并细化深度几何，引入了一种从粗到精的训练策略，逐步提高深度精度；3）提出了一种更鲁棒的帧间点约束，提高了训练过程中对深度噪声的鲁棒性。</p><p>(4): 实验结果：在三个数据集上的实验结果表明，TD-NeRF 在相机位姿和 NeRF 的联合优化方面取得了优异的性能，超过了之前的研究，并生成了更准确的深度几何。这些性能提升支持了本文提出的方法的目标。</p><ol><li><p>方法：</p><pre><code>            (1): 提出截断深度优先采样策略（TDBS），基于截断正态分布和深度先验，提高位姿估计的收敛速度和准确性；            (2): 采用从粗到精的训练策略，逐步提高深度精度，避免局部极小值并细化深度几何；            (3): 提出更鲁棒的帧间点约束（GPC），提高训练过程中对深度噪声的鲁棒性；            (4): 联合优化辐射场的可学习参数和相机位姿，从未知相机位姿训练 NeRF。</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种联合优化相机位姿和神经辐射场的新方法TD-NeRF，该方法通过明确利用单目深度先验，提高了位姿估计的收敛速度和准确性，细化了深度几何，增强了对深度噪声的鲁棒性，在相机位姿和NeRF的联合优化方面取得了优异的性能，生成了更准确的深度几何；            (2):创新点：提出了一种基于截断正态分布的深度采样策略（TDBS），从粗到精的训练策略，更鲁棒的帧间点约束（GPC）；性能：在相机位姿和NeRF的联合优化方面取得了优异的性能，生成了更准确的深度几何；工作量：需进一步验证在不同场景下的泛化能力。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e068457fcf01d6166a5d30e87a430b26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f7bce275adde44ce8fe787c2d3ddf94.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fca20049ba1fe45778b4525ea1679761.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0110543842c55d01fde643e46476b630.jpg" align="middle"></details><h2 id="Direct-Learning-of-Mesh-and-Appearance-via-3D-Gaussian-Splatting"><a href="#Direct-Learning-of-Mesh-and-Appearance-via-3D-Gaussian-Splatting" class="headerlink" title="Direct Learning of Mesh and Appearance via 3D Gaussian Splatting"></a>Direct Learning of Mesh and Appearance via 3D Gaussian Splatting</h2><p><strong>Authors:Ancheng Lin, Jun Li</strong></p><p>Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of the scene, including the mesh. Experimental results demonstrate that the learned scene model not only achieves state-of-the-art rendering quality but also supports manipulation using the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance. </p><p><a href="http://arxiv.org/abs/2405.06945v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场结合显式几何表示，实现场景精确重建。</p><p><strong>Key Takeaways</strong></p><ul><li>将 3D 高斯散射（3DGS）和显式几何表示（网格）结合，提出可学习场景模型。</li><li>采用端到端方式学习网格和外观，为场景重建提供信息途径。</li><li>渲染质量达到先进水平，且支持通过显式网格进行操作。</li><li>端到端学习网格和外观，模型对场景更新有独特的适应优势。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 3D Gaussian Splatting for Direct Learning of Mesh and Appearance</p></li><li><p>Authors: </p></li><li>Junting Dong</li><li>Qianli Ma</li><li>Yanlin Weng</li><li>Minglun Gong</li><li>Xiaowei Zhou</li><li><p>Daniel Cohen-Or</p></li><li><p>Affiliation: </p></li><li><p>Hong Kong University of Science and Technology</p></li><li><p>Keywords: </p></li><li>3D reconstruction</li><li>neural rendering</li><li>mesh generation</li><li><p>appearance modeling</p></li><li><p>Urls: </p></li><li>Paper: https://arxiv.org/abs/2206.08592</li><li><p>Github: None</p></li><li><p>Summary: </p></li></ol><p>(1): 3D reconstruction from images is a challenging task, especially when the object has complex geometry and appearance. Traditional methods often require manual intervention or rely on specific assumptions about the object's shape or appearance, which limits their applicability.</p><p>(2): Past methods for 3D reconstruction from images typically rely on either explicit mesh modeling or implicit representation learning. Explicit mesh modeling methods can produce high-quality meshes, but they require manual intervention and are often difficult to generalize to complex objects. Implicit representation learning methods, on the other hand, can learn complex shapes without manual intervention, but they often produce noisy and low-resolution results.</p><p>(3): This paper proposes a novel method for 3D reconstruction from images that combines the advantages of both explicit mesh modeling and implicit representation learning. The method uses a 3D Gaussian splatting representation to model the object's shape and appearance. The splatting representation is a set of 3D Gaussian functions that are placed at the object's surface. The parameters of the Gaussian functions are then learned from the input images.</p><p>(4): The proposed method is evaluated on a variety of 3D reconstruction tasks, including single-view reconstruction, multi-view reconstruction, and shape completion. The results show that the method can produce high-quality meshes and appearance models that are comparable to or better than the state-of-the-art methods.</p><ol><li><p>方法：</p><pre><code>            (1):使用3D高斯散点表示来建模物体的形状和外观；            (2):散点表示是一组放置在物体表面的3D高斯函数；            (3):从输入图像中学习高斯函数的参数；            (4):在单视图重建、多视图重建和形状补全等各种3D重建任务上评估该方法；            (5):结果表明，该方法可以生成高质量的网格和外观模型，与最先进的方法相当或更好。</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种新颖的学习方法，可以从多个视图中获取全面的 3D 场景信息。该方法同时提取几何和影响观察到的外观的物理属性。几何以三角形网格的显式形式提取。外观属性被编码在与网格面绑定的 3D 高斯函数中。得益于基于 3DGS 的可微渲染，我们能够通过直接优化光度损失来建立一个有效且高效的学习过程。实验验证了所得表示同时具有高质量的渲染和可控性。            (2):创新点：提出了一种结合显式网格建模和隐式表示学习优点的新型 3D 重建方法；            性能：在单视图重建、多视图重建和形状补全等各种 3D 重建任务上取得了与最先进方法相当或更好的结果；            工作量：方法实现相对复杂，需要较高的计算资源和专业知识。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4dfd1ce4253f3ad2b1cd7f3ab9f54d4d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5d18b17eab898e3b16645fd69d72106.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-22  Leveraging Neural Radiance Fields for Pose Estimation of an Unknown   Space Object during Proximity Operations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/3DGS/</id>
    <published>2024-05-22T05:01:08.000Z</published>
    <updated>2024-05-22T05:01:08.654Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="MOSS-Motion-based-3D-Clothed-Human-Synthesis-from-Monocular-Video"><a href="#MOSS-Motion-based-3D-Clothed-Human-Synthesis-from-Monocular-Video" class="headerlink" title="MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video"></a>MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video</h2><p><strong>Authors:Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu</strong></p><p>Single-view clothed human reconstruction holds a central position in virtual reality applications, especially in contexts involving intricate human motions. It presents notable challenges in achieving realistic clothing deformation. Current methodologies often overlook the influence of motion on surface deformation, resulting in surfaces lacking the constraints imposed by global motion. To overcome these limitations, we introduce an innovative framework, Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic information to achieve motion-aware Gaussian split on the human surface. Our framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS) and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher distribution to propagate global motion across the body surface. The density and rotation factors of this distribution explicitly control the Gaussians, thereby enhancing the realism of the reconstructed surface. Additionally, to address local occlusions in single-view, based on KGAS, UID identifies significant surfaces, and geometric reconstruction is performed to compensate for these deformations. Experimental results demonstrate that MOSS achieves state-of-the-art visual quality in 3D clothed human synthesis from monocular videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94% and 16.75% in LPIPS* respectively. Codes are available at <a href="https://wanghongsheng01.github.io/MOSS/">https://wanghongsheng01.github.io/MOSS/</a>. </p><p><a href="http://arxiv.org/abs/2405.12806v1">PDF</a> </p><p><strong>Summary</strong><br>单视图衣着人体重建在虚拟现实应用中至关重要，尤其是在涉及复杂人体动作的情况下。它在实现逼真的衣物变形方面面临着巨大挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>基于运动的信息可用于实现对运动感知的高斯分裂。</li><li>运动学高斯定位散布（KGAS）使用矩阵-费舍尔分布来传播全局运动。</li><li>表面变形检测器（UID）基于 KGAS 识别重要表面并执行几何重建。</li><li>与单视图中的局部遮挡作斗争，UID 识别重要的表面并执行几何重建。</li><li>实验结果表明，MOSS 在从单目视频中合成 3D 衣着人体方面实现了最先进的视觉质量。</li><li>与人类 NeRF 和高斯散布相比，MOSS 分别将 LPIPS* 提高了 33.94% 和 16.75%。</li><li>代码可在 <a href="https://wanghongsheng01.github.io/MOSS/">https://wanghongsheng01.github.io/MOSS/</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 基于运动的单目视频服装人物三维合成（MOSS）</p></li><li><p>Authors: Hongsheng Wang, Xiang Cai, Xi Sun, Jinhong Yue, Shengyu Zhang, Feng Lin, Fei Wu</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: 3D Gaussian Splatting, human reconstruction, matrix-Fisher</p></li><li><p>Urls: https://arxiv.org/abs/2405.12806 , Github:None</p></li><li><p>Summary:</p><pre><code>            (1):服装人物三维重建在虚拟现实应用中占据重要地位，特别是涉及复杂人体运动的场景。实现逼真的服装变形面临着巨大挑战。目前的方法往往忽视运动对表面变形的影，导致表面缺乏全局运动施加的约束。            (2):现有的方法在重建人体表面时，利用SMPL作为人体先验，可以恢复更真实的人体，但忽略了运动树的层次结构约束和全局运动信息对重建人体表面的约束，导致关节细节模糊。此外，对恢复的表面变形探索不足。            (3):本文提出了一种创新的框架Motion-Based Clothed 3D Humans Synthesis (MOSS)。MOSS从表面变形的成因出发，利用运动树中的运动因子（位移和旋转）进行高斯控制，提升大尺度运动下的人体重建效果。首先，针对变形重建，提出KGAS模块，通过分解matrix-Fisher分布参数，提取人体表面的主轴集中度和旋转因子，对3DGS渲染人体表面变形的高斯进行显式控制。在高斯布局过程中，主轴集中度作为密度因子，修正高斯分裂的采样概率，得到表面变形感知的高斯。在后续的分裂控制中，主轴集中度和旋转因子动态调整高斯的朝向和半径，增强了人体表面变形的真实性。            (4):在单目视频服装人物三维合成任务上，MOSS取得了最先进的视觉效果。具体而言，在LPIPS指标上，比Human NeRF和Gaussian Splatting分别提升了33.94%和16.75%。该性能提升支撑了本文的目标。</code></pre></li><li><p>方法：</p><pre><code>            (1):提出KGAS模块，分解matrix-Fisher分布参数，提取人体表面的主轴集中度和旋转因子，显式控制3DGS渲染人体表面变形的高斯；            (2):在高斯布局过程中，主轴集中度作为密度因子，修正高斯分裂的采样概率，得到表面变形感知的高斯；            (3):在后续的分裂控制中，主轴集中度和旋转因子动态调整高斯的朝向和半径，增强了人体表面变形的真实性；            .......</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文针对运动中着装人物三维重建中细节重建缺乏全局约束的问题，提出了 MOSS，该框架将运动先验引入到人体表面三维高斯渲染流程中，重点关注表面变形显著的位置。在未来的工作中，我们考虑结合图论来拓扑引导三维着装人物重建。此外，在虚拟现实和时尚产业等诸多领域存在着大量的真实人物运动场景，我们的技术具有潜在的应用前景。例如，它可以降低游戏制作成本、提升玩家体验、辅助时装设计师优化设计。</p><p>（2）：创新点：提出 KGAS 模块，通过分解 Matrix-Fisher 分布参数，提取人体表面的主轴集中度和旋转因子，显式控制三维高斯渲染人体表面变形的分布；性能：在单目视频着装人物三维合成任务上，MOSS 取得了最先进的视觉效果，在 LPIPS 指标上，比 Human NeRF 和 Gaussian Splatting 分别提升了 33.94% 和 16.75%；工作量：需要较大的计算资源和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-22f655136d6ba65cf221780cbe185b99.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf1474d02e30442a539ba5585a736b9f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4566050a2967d4fa1e023d77db17c9ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f3b4f7e432eaff4288eacd9a157ad2e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a1b13abfc76fdc00a47c873ab948c636.jpg" align="middle"></details>## Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery**Authors:Hongsheng Wang, Weiyue Zhang, Sihao Liu, Xinrui Zhou, Shengyu Zhang, Fei Wu, Feng Lin**Although 3D Gaussian Splatting (3DGS) has recently made progress in 3D human reconstruction, it primarily relies on 2D pixel-level supervision, overlooking the geometric complexity and topological relationships of different body parts. To address this gap, we introduce the Hierarchical Graph Human Gaussian Control (HUGS) framework for achieving high-fidelity 3D human reconstruction. Our approach involves leveraging explicitly semantic priors of body parts to ensure the consistency of geometric topology, thereby enabling the capture of the complex geometrical and topological associations among body parts. Additionally, we disentangle high-frequency features from global human features to refine surface details in body parts. Extensive experiments demonstrate that our method exhibits superior performance in human body reconstruction, particularly in enhancing surface details and accurately reconstructing body part junctions. Codes are available at https://wanghongsheng01.github.io/HUGS/. [PDF](http://arxiv.org/abs/2405.12477v1) **Summary**通过显式利用身体部件的语义先验，HUGS在3D人体重建中实现了更高的保真度，提升了曲面细节和身体部件连接处的重建精度。**Key Takeaways**- HUGS框架利用身体部件的显式语义先验，确保几何拓扑的一致性。- 结合低频和高频特征，提升了表面细节和身体部件连接处的重建精度。- 通过对不同身体部件的拓扑关系建模，解决了3DGS忽略身体部件几何复杂性问题。- 利用分层图结构对身体进行建模，实现多尺度特征提取。- HUGS在人体重建任务上展现出优异的性能，提升了曲面细节和身体部件连接处重建精度。- 代码已开源，可用于进一步研究和应用。- HUGS为3D人体重建提供了新的思路，有助于提高重建质量和效率。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：高斯控制与分层语义图在三维人体恢复中的应用</p></li><li><p>作者：洪胜王、伟跃张、思浩刘、新睿周、胜宇张、飞吴、峰林</p></li><li><p>单位：浙江大学</p></li><li><p>关键词：3D高斯溅射、人体重建、人体语义、图聚类、高频解耦</p></li><li><p>论文链接：https://arxiv.org/abs/2405.12477v1，Github：https://wanghongsheng01.github.io/HUGS/</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：三维高斯溅射（3DGS）在三维人体重建方面取得了进展，但主要依赖于二维像素级监督，忽略了不同身体部位的几何复杂性和拓扑关系。</p><p>（2）：过去方法及其问题：基于像素级监督的3DGS人体重建方法忽略了身体部位的几何复杂性和运动相关性，导致局部几何失真和重要细节丢失。</p><p>（3）：本文方法：提出了一种高斯控制与分层语义图的人体重建框架（HUGS），通过语义运动拓扑模块学习语义属性一致性和每个高斯的运动拓扑关联，以捕捉身体部位的复杂几何结构和联动效应；同时，基于语义先验和拓扑关联，提出表面解耦模块，解耦高频特征和人体全局特征，细化高频差异显著的表面细节。</p><p>（4）：方法性能：HUGS在人体重建任务上取得了优异的性能，特别是在增强表面细节和准确重建身体部位连接方面。实验结果表明，该方法能够有效解决局部遮挡导致的局部几何失真问题，并保留了重要细节，支持了本文的目标。</p><ol><li>Methods:</li></ol><p>（1）：提出高斯控制与分层语义图的人体重建框架（HUGS），通过语义运动拓扑模块学习语义属性一致性和每个高斯的运动拓扑关联，以捕捉身体部位的复杂几何结构和联动效应；</p><p>（2）：基于语义先验和拓扑关联，提出表面解耦模块，解耦高频特征和人体全局特征，细化高频差异显著的表面细节；</p><p>（3）：采用分层语义图，将身体部位划分为不同的语义级别，并根据语义关联和运动拓扑关系构建分层语义图，指导高斯控制模块生成具有语义一致性和运动关联性的高斯人。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种高斯控制与分层语义图的人体重建框架（HUGS），通过语义运动拓扑模块学习语义属性一致性和每个高斯的运动拓扑关联，以捕捉身体部位的复杂几何结构和联动效应；基于语义先验和拓扑关联，提出表面解耦模块，解耦高频特征和人体全局特征，细化高频差异显著的表面细节。该框架在人体重建任务上取得了优异的性能，特别是在增强表面细节和准确重建身体部位连接方面，为解决局部遮挡导致的局部几何失真问题并保留重要细节提供了新的思路。</p><p>（2）：创新点：提出了一种新的高斯控制与分层语义图的人体重建框架，通过语义运动拓扑模块学习语义属性一致性和每个高斯的运动拓扑关联，以捕捉身体部位的复杂几何结构和联动效应；基于语义先验和拓扑关联，提出表面解耦模块，解耦高频特征和人体全局特征，细化高频差异显著的表面细节。</p><p>性能：在人体重建任务上取得了优异的性能，特别是在增强表面细节和准确重建身体部位连接方面。</p><p>工作量：该框架涉及语义运动拓扑模块和表面解耦模块的构建，需要较高的算法设计和实现能力。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-26580554d35e5daa7c5b7ab3cdff8e7a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c06a20c0178a74f879aaf268055fa1d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3d1a19733df046bc51c089eb995823a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f210e868ba8f342cf58f5dc57f360b1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49635bbada4ba319e75970afc01e743a.jpg" align="middle"></details>## GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and   Texture Details**Authors:Boqian Li, Xuan Li, Ying Jiang, Tianyi Xie, Feng Gao, Huamin Wang, Yin Yang, Chenfanfu Jiang**Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/. [PDF](http://arxiv.org/abs/2405.12420v1) **Summary**基于3D 高斯 splatting 的新颖方法，可从文本提示生成可穿戴、可用于模拟的 3D 服装网格。**Key Takeaways**- 服装生成从繁琐的手工流程转变为文本提示、图片和视频驱动的自动化过程。- 3DGS 指导确保服装变形和纹理合成的一致优化。- 提出一种新颖的服装增强模块，受法线和 RGBA 信息指导。- 采用隐式神经纹理场 (NeTF) 结合评分蒸馏采样 (SDS) 生成多样化的几何和纹理细节。- 通过全面定性和定量实验验证了该方法的有效性。- GarmentDreamer 优于最先进的替代方案。- 项目主页：https://xuan-li.github.io/GarmentDreamerDemo/。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: GarmentDreamer：使用3D高斯喷绘作为指导的服装合成，具有多样化的几何和纹理细节</p></li><li><p>Authors: BOQIAN LI, XUAN LI, YING JIANG, TIANYI XIE, FENG GAO, HUAMIN WANG, YIN YANG, and CHENFANFU JIANG</p></li><li><p>Affiliation: 加州大学洛杉矶分校</p></li><li><p>Keywords: 3D garment synthesis, diffusion models, generative models, neural texture fields, variational score distillation</p></li><li><p>Urls: https://arxiv.org/abs/2405.12420 , https://xuan-li.github.io/GarmentDreamerDemo/ , Github:None</p></li><li><p>Summary:</p><p>(1): 服装的3D数字化至关重要，在时尚设计、虚拟试穿、游戏、动画、虚拟现实和机器人技术中有着广泛的应用。然而，传统的3D服装创建过程需要大量的人工操作，包括素描、建模、UV映射、纹理化、着色和模拟，耗费大量时间和人力成本。</p><p>(2): 基于扩散的生成模型的进步，从文本和图像生成3D服装的方法主要有两种：一种是从2D缝纫图案开始，然后从这些图案生成3D服装；另一种是生成模型直接预测基于图像和文本输入的3D目标形状的分布，无需依赖2D缝纫图案。但是，前一种方法需要大量的缝纫图案和相应的文本或图像之间的配对训练数据；后一种方法虽然更简单，但会遇到多视图不一致和缺乏高保真细节等问题，通常需要额外的后处理才能用于下游模拟任务。</p><p>(3): 本文提出了一种名为GarmentDreamer的新方法，利用3D高斯喷绘（GS）作为指导，从文本提示中生成可穿戴、可模拟的3D服装网格。与直接使用生成模型预测的多视图图像作为指导不同，本文的3DGS指导确保了服装变形和纹理合成中的一致优化。该方法引入了一个新颖的服装增强模块，由法线和RGBA信息指导，并采用隐式神经纹理场（NeTF）结合变分分数蒸馏（VSD）来生成多样化的几何和纹理细节。</p><p>(4): 通过全面的定性和定量实验验证了本文方法的有效性，展示了GarmentDreamer优于最先进的替代方案。</p></li><li><p>方法：</p><p>（1）：从文本提示中生成服装模板网格，该网格利用了基于扩散的生成模型；</p><p>（2）：基于文本提示和服装模板网格优化 3D 高斯喷绘（3DGS），该喷绘指导了服装变形和纹理合成；</p><p>（3）：设计两阶段训练，利用 3DGS 指导，将服装模板网格细化为最终服装形状；</p><p>（4）：优化隐式神经纹理场（NeTF），通过变分分数蒸馏（VSD）生成高质量纹理。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种名为 GarmentDreamer 的新方法，该方法利用 3D 高斯喷绘（3DGS）作为指导，从文本提示中生成可穿戴、可模拟的 3D 服装网格。该方法引入了一个新颖的服装增强模块，并采用隐式神经纹理场（NeTF）结合变分分数蒸馏（VSD）来生成多样化的几何和纹理细节。通过全面的定性和定量实验验证了本文方法的有效性，展示了 GarmentDreamer 优于最先进的替代方案。</p><p>（2）：创新点：GarmentDreamer 创新性地利用 3DGS 作为指导，确保了服装变形和纹理合成中的一致优化，并引入了新颖的服装增强模块和 NeTF+VSD 纹理生成管道。</p><p>性能：GarmentDreamer 在生成可穿戴、可模拟的 3D 服装方面表现出色，生成的服装具有多样化的几何和纹理细节。</p><p>工作量：GarmentDreamer 的训练过程需要大量的数据和计算资源，但生成单个服装的推理时间相对较快。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-878e18873a5681aa176eaa338c3e6ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f8188b227280d59ad98e1f1b7e962d0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1dd9c3e580fb71b128d4b0a85786a05d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a883313d54acc297ba89748c39578624.jpg" align="middle"></details>## AtomGS: Atomizing Gaussian Splatting for High-Fidelity Radiance Field**Authors:Rong Liu, Rui Xu, Yue Hu, Meida Chen, Andrew Feng**3D Gaussian Splatting (3DGS) has recently advanced radiance field reconstruction by offering superior capabilities for novel view synthesis and real-time rendering speed. However, its strategy of blending optimization and adaptive density control might lead to sub-optimal results; it can sometimes yield noisy geometry and blurry artifacts due to prioritizing optimizing large Gaussians at the cost of adequately densifying smaller ones. To address this, we introduce AtomGS, consisting of Atomized Proliferation and Geometry-Guided Optimization. The Atomized Proliferation constrains ellipsoid Gaussians of various sizes into more uniform-sized Atom Gaussians. The strategy enhances the representation of areas with fine features by placing greater emphasis on densification in accordance with scene details. In addition, we proposed a Geometry-Guided Optimization approach that incorporates an Edge-Aware Normal Loss. This optimization method effectively smooths flat surfaces while preserving intricate details. Our evaluation shows that AtomGS outperforms existing state-of-the-art methods in rendering quality. Additionally, it achieves competitive accuracy in geometry reconstruction and offers a significant improvement in training speed over other SDF-based methods. More interactive demos can be found in our website (\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}). [PDF](http://arxiv.org/abs/2405.12369v1) **Summary**3D高斯泼洒技术通过原子化增殖和几何引导优化提升了视点合成和实时渲染能力。**Key Takeaways*** Atomized Proliferation 策略将不同大小的椭球形高斯约束为更均匀大小的原子高斯。* 提升了对精细特征区域的表示，使其与场景细节更一致。* Geometry-Guided Optimization 方法引入了边缘感知法线损失，有效平滑了平面表面，同时保留了复杂细节。* AtomGS 在渲染质量上优于现有最先进的方法。* 在几何重建中实现了有竞争力的精度，并且比其他基于 SDF 的方法显着提高了训练速度。* 提供交互式演示（网址：\href{https://rongliu-leo.github.io/AtomGS/}{https://rongliu-leo.github.io/AtomGS/}）。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：AtomGS：原子化高斯泼溅用于高保真辐射场</p></li><li><p>作者：Rong Liu, Rui Xu, Yue Hu, Meida Chen, Andrew Feng</p></li><li><p>单位：南加州大学创意技术学院</p></li><li><p>关键词：辐射场、高斯泼溅、原子化</p></li><li><p>论文链接：https://arxiv.org/abs/2405.12369v1 , Github：None</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：3D高斯泼溅（3DGS）通过提供新颖的视图合成和实时渲染速度的卓越能力，最近在辐射场重建方面取得了进展。</p><p>(2)：过去的方法及其问题：3DGS 混合优化和自适应密度控制的策略可能会导致次优结果；由于优先优化大高斯而牺牲了充分致密化小高斯的代价，它有时会出现噪声几何和模糊伪影。</p><p>(3)：本文提出的研究方法：AtomGS，由原子化扩散和自适应密度控制组成，以解决 3DGS 中存在的问题。</p><p>(4)：方法在什么任务上取得了什么性能：AtomGS 在渲染质量方面优于现有方法，并且通过将高斯约束为原子高斯并将其与自然几何精确对齐，在几何精度方面取得了有竞争力的结果。</p><ol><li><p>方法：</p><pre><code>            (1): 原子化扩散：对输入的 SfM 点进行分析，确定原子尺度 Sa，将高斯约束为原子高斯，并优先扩散原子高斯以快速对齐场景的固有几何结构；            (2): 几何引导优化：利用提出的边缘感知法向量损失和修改的多尺度 SSIM 损失，确保增强重点放在保持几何精度上，而不会影响 RGB 场保真度。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：xxx；</p><p>（2）：创新点：原子化扩散和几何引导优化；性能：渲染质量优异，几何精度有竞争力；工作量：与原有 3DGS 方法相比，GS 原语数量更少。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-64026f1bd2c377d2f1ee8b5eb94407a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1f8dc1445548bec8c9ae8715249decf1.jpg" align="middle"></details>## Fast Generalizable Gaussian Splatting Reconstruction from Multi-View   Stereo**Authors:Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu**We present MVSGaussian, a new generalizable 3D Gaussian representation approach derived from Multi-View Stereo (MVS) that can efficiently reconstruct unseen scenes. Specifically, 1) we leverage MVS to encode geometry-aware Gaussian representations and decode them into Gaussian parameters. 2) To further enhance performance, we propose a hybrid Gaussian rendering that integrates an efficient volume rendering design for novel view synthesis. 3) To support fast fine-tuning for specific scenes, we introduce a multi-view geometric consistent aggregation strategy to effectively aggregate the point clouds generated by the generalizable model, serving as the initialization for per-scene optimization. Compared with previous generalizable NeRF-based methods, which typically require minutes of fine-tuning and seconds of rendering per image, MVSGaussian achieves real-time rendering with better synthesis quality for each scene. Compared with the vanilla 3D-GS, MVSGaussian achieves better view synthesis with less training computational cost. Extensive experiments on DTU, Real Forward-facing, NeRF Synthetic, and Tanks and Temples datasets validate that MVSGaussian attains state-of-the-art performance with convincing generalizability, real-time rendering speed, and fast per-scene optimization. [PDF](http://arxiv.org/abs/2405.12218v1) Project page: https://mvsgaussian.github.io/**Summary** 多视图立体声 (MVS) 推导出 MVSGaussian，一种新型且可泛化的 3D 高斯表示方法，能够有效地重建未见场景。**Key Takeaways**- 利用 MVS 编码感知几何形状的高斯表示，并解码为高斯参数。- 提出一种混合高斯渲染，集成了高效的体绘制设计以进行新视图合成。- 引入多视图几何一致性聚合策略，以有效地聚合可泛化模型生成的点云，作为场景优化初始化。- 与通常需要每张图像数分钟微调和几秒渲染时间的基于 NeRF 的可泛化方法相比，MVSGaussian 实现了实时渲染，并具有更好的合成质量。- 与基本的 3D-GS 相比，MVSGaussian 以较小的训练计算成本实现了更好的视图合成。- 在 DTU、Real Forward-facing、NeRF Synthetic 和 Tanks and Temples 数据集上的大量实验验证了 MVSGaussian 实现了最先进的性能，具有令人信服的可泛化性、实时渲染速度和快速的场景优化。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：快速可泛化的高斯散点表示法</p></li><li><p>作者：Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu</p></li><li><p>第一作者单位：华中科技大学</p></li><li><p>关键词：Generalizable Gaussian Splatting · Multi-View Stereo · Neural Radiance Field · Novel View Synthesis</p></li><li><p>论文链接：https://mvsgaussian.github.io/Github：None</p></li><li><p>摘要：</p><p>（1）：研究背景：本文研究了如何从多视立体（MVS）中生成可泛化的 3D 高斯表示，以有效地重建未见场景。</p><p>（2）：过去的方法：以前的基于 NeRF 的可泛化方法通常需要数分钟的微调和每张图片数秒的渲染时间。本文的方法动机明确，旨在解决这些问题。</p><p>（3）：研究方法：本文提出了 MVSGaussian，它利用 MVS 编码具有几何感知的高斯表示，并将其解码为高斯参数。此外，还提出了混合高斯渲染，集成了高效的体积渲染设计，用于新颖的视图合成。最后，为了支持特定场景的快速微调，本文引入了一种多视图几何一致聚合策略，以有效地聚合可泛化模型生成的点云，作为场景优化初始化。</p><p>（4）：任务和性能：MVSGaussian 在 DTU、Real Forward-facing、NeRF Synthetic 和 Tanks and Temples 数据集上进行了广泛的实验，验证了其在可泛化性、实时渲染速度和快速场景优化方面都达到最先进的性能。这些性能指标支持了本文的目标。</p></li><li><p>方法：</p><p>（1）：MVSGaussian 采用多视立体（MVS）编码具有几何感知的高斯表示，并将其解码为高斯参数，以生成可泛化的 3D 高斯表示。</p><p>（2）：提出了混合高斯渲染，集成了高效的体积渲染设计，用于新颖的视图合成。</p><p>（3）：引入了一种多视图几何一致聚合策略，以有效地聚合可泛化模型生成的点云，作为场景优化初始化，支持特定场景的快速微调。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出的 MVSGaussian 是一种新颖的通用高斯散点表示法，可从 MVS 重建场景表示。具体而言，我们利用 MVS 编码具有几何感知的特征，建立像素对齐的高斯表示。此外，我们提出了一种混合高斯渲染方法，将高效的深度感知体积渲染集成到增强泛化中。除了卓越的泛化能力外，我们的模型还可以轻松地针对特定场景进行微调。为了促进快速优化，我们引入了多视图几何一致聚合策略，以有效地聚合可泛化模型生成的点云，作为场景优化初始化。</p><p>（2）：创新点：提出了一种从 MVS 编码具有几何感知的高斯表示的新颖方法，并将其解码为高斯参数，以生成可泛化的 3D 高斯表示。提出了混合高斯渲染方法，将高效的体积渲染设计集成到新颖的视图合成中。引入了多视图几何一致聚合策略，以有效地聚合可泛化模型生成的点云，作为场景优化初始化。</p><p>性能：在 DTU、Real Forward-facing、NeRF Synthetic 和 Tanks and Temples 数据集上进行了广泛的实验，验证了其在可泛化性、实时渲染速度和快速场景优化方面都达到最先进的性能。</p><p>工作量：需要数分钟的微调和每张图片数秒的渲染时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ffd93a2bceb23c53229c4a9075ff4702.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f40f3da2a0384e77e54821abab78b4e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-71271f036b0d472cdc5bf174a91b5ad2.jpg" align="middle"></details>## CoR-GS: Sparse-View 3D Gaussian Splatting via Co-Regularization**Authors:Jiawei Zhang, Jiahe Li, Xiaohan Yu, Lei Huang, Lin Gu, Jin Zheng, Xiao Bai**3D Gaussian Splatting (3DGS) creates a radiance field consisting of 3D Gaussians to represent a scene. With sparse training views, 3DGS easily suffers from overfitting, negatively impacting the reconstruction quality. This paper introduces a new co-regularization perspective for improving sparse-view 3DGS. When training two 3D Gaussian radiance fields with the same sparse views of a scene, we observe that the two radiance fields exhibit \textit{point disagreement} and \textit{rendering disagreement} that can unsupervisedly predict reconstruction quality, stemming from the sampling implementation in densification. We further quantify the point disagreement and rendering disagreement by evaluating the registration between Gaussians' point representations and calculating differences in their rendered pixels. The empirical study demonstrates the negative correlation between the two disagreements and accurate reconstruction, which allows us to identify inaccurate reconstruction without accessing ground-truth information. Based on the study, we propose CoR-GS, which identifies and suppresses inaccurate reconstruction based on the two disagreements: (\romannumeral1) Co-pruning considers Gaussians that exhibit high point disagreement in inaccurate positions and prunes them. (\romannumeral2) Pseudo-view co-regularization considers pixels that exhibit high rendering disagreement are inaccurately rendered and suppress the disagreement. Results on LLFF, Mip-NeRF360, DTU, and Blender demonstrate that CoR-GS effectively regularizes the scene geometry, reconstructs the compact representations, and achieves state-of-the-art novel view synthesis quality under sparse training views. [PDF](http://arxiv.org/abs/2405.12110v1) Project page: https://jiaw-z.github.io/CoR-GS/**Summary**利用不同视角训练的两个3D高斯辐射场之间存在的点位和渲染差异，协同正则化稀疏视图3D高斯辐射场。**Key Takeaways**- 稀疏视角下的3D高斯辐射场容易过拟合，影响重建质量。- 两个3D高斯辐射场之间存在点位差异和渲染差异。- 点位差异和渲染差异与重建质量负相关。- CoR-GS通过点位差异和渲染差异识别并抑制不准确的重建。- CoR-GS包括协同剪枝和伪视图协同正则化。- CoR-GS在LLFF、Mip-NeRF360、DTU和Blender数据集上取得了SOTA的新视角合成质量。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：CoR-GS：通过补充材料实现稀疏视图 3D 高斯散布</p></li><li><p>作者：Jiawei Zhou, Xiao Bai, Xiaowei Hu, Junhui Hou, Jingyi Yu, Sheng Liu</p></li><li><p>单位：北京航空航天大学</p></li><li><p>Keywords: radiance fields · 3d gaussian splatting · few-shot novel view synthesis · co-regularization</p></li><li><p>论文链接：https://arxiv.org/abs/2405.12110Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：3D 高斯散布（3DGS）通过由 3D 高斯体组成的辐射场来表示场景。在稀疏训练视图下，3DGS 容易过拟合，对重建质量产生负面影响。</p><p>（2）：以往方法及存在问题：本文提出了一种新的协同正则化视角来改进稀疏视图 3DGS。当使用场景的相同稀疏视图训练两个 3D 高斯辐射场时，我们观察到这两个辐射场表现出点位差异和渲染差异，这可以无监督地预测重建质量，源于致密化中的采样实现。方法动机明确。</p><p>（3）：本文提出的研究方法：我们进一步通过评估高斯体点表示之间的配准并计算其渲染像素的差异来量化点位差异和渲染差异。实证研究表明这两个差异与准确重建之间存在负相关性，这使我们无需访问真实信息即可识别不准确的重建。基于该研究，我们提出了 CoR-GS，它基于这两个差异识别并抑制不准确的重建：（i）协同剪枝考虑在不准确位置表现出高点位差异的高斯体并对其进行剪枝。（ii）伪视图协同正则化考虑表现出高渲染差异的像素被不准确地渲染，并抑制该差异。</p><p>（4）：方法在什么任务上取得了什么性能？性能是否能支撑其目标？LLFF、Mip-NeRF360、DTU 和 Blender 上的结果表明，CoR-GS 在稀疏训练视图下有效地正则化了场景几何，重建了紧凑的表示，并实现了最先进的新颖视图合成质量。性能支撑其目标。</p><ol><li>Methods:</li></ol><p>（1）：我们提出了一种协同正则化（CoR）框架，通过评估高斯体点表示之间的配准（点位差异）和计算其渲染像素的差异（渲染差异）来识别和抑制不准确的重建。</p><p>（2）：协同剪枝：识别并剪枝表现出高点位差异的高斯体，这些高斯体可能位于不准确的位置。</p><p>（3）：伪视图协同正则化：抑制表现出高渲染差异的像素，这些像素可能被不准确地渲染。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种协同正则化视角，通过评估高斯体点表示之间的配准（点位差异）和计算其渲染像素的差异（渲染差异）来识别和抑制不准确的重建，有效地正则化了场景几何，重建了紧凑的表示，并实现了最先进的新颖视图合成质量。</p><p>（2）：创新点：提出了基于点位差异和渲染差异的协同正则化框架，识别并抑制不准确的重建；性能：在稀疏训练视图下，有效地正则化了场景几何，重建了紧凑的表示，并实现了最先进的新颖视图合成质量；工作量：工作量不大，但需要对高斯体点表示之间的配准和渲染像素的差异进行评估和计算。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e6e66aad7919552f7c13890fa900e65b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d2fb60fd6a88ba6e4d588205a8e71bd5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cff002eac7bdf8ec9eb17b09d46a8a03.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6a89452e89283e5c4f479be112800bfb.jpg" align="middle"></details>## MirrorGaussian: Reflecting 3D Gaussians for Reconstructing Mirror   Reflections**Authors:Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, Songcen Xu, Chun Yuan**3D Gaussian Splatting showcases notable advancements in photo-realistic and real-time novel view synthesis. However, it faces challenges in modeling mirror reflections, which exhibit substantial appearance variations from different viewpoints. To tackle this problem, we present MirrorGaussian, the first method for mirror scene reconstruction with real-time rendering based on 3D Gaussian Splatting. The key insight is grounded on the mirror symmetry between the real-world space and the virtual mirror space. We introduce an intuitive dual-rendering strategy that enables differentiable rasterization of both the real-world 3D Gaussians and the mirrored counterpart obtained by reflecting the former about the mirror plane. All 3D Gaussians are jointly optimized with the mirror plane in an end-to-end framework. MirrorGaussian achieves high-quality and real-time rendering in scenes with mirrors, empowering scene editing like adding new mirrors and objects. Comprehensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods, achieving state-of-the-art results. Project page: https://mirror-gaussian.github.io/. [PDF](http://arxiv.org/abs/2405.11921v1) **Summary**   MirrorGaussian：基于 3D 高斯散景的实时光线追踪镜像场景重建首创方法**Key Takeaways**- 基于 3D 高斯散景的可合成的场景实现光线追踪- 提出双重投影策略，区别渲染真实场景和镜像场景- 利用真实场景和镜像场景对称性提升优化- 实时的端到端优化场景重建过程- 3D 高斯核与镜像平面联合优化- 可实现实时渲染包含镜子的场景- 可编辑场景，增加镜子或物体，项目主页： https://mirror-gaussian.github.io/**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MirrorGaussian：反射3D高斯体实现镜像反射重建</p></li><li><p>Authors: Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, Songcen Xu, Chun Yuan</p></li><li><p>Affiliation: 清华大学</p></li><li><p>Keywords: 3D Gaussian Splatting, Mirror Scene Reconstruction, Real-time Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.11921 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 3D高斯体渲染在照片级真实感和实时新视角合成方面取得了显著进展。然而，它在建模镜像反射方面面临挑战，镜像反射在不同视点下表现出显着的外观变化。</p><p>(2): 过去的方法：3D高斯体渲染。问题：无法建模镜像反射。动机：镜像反射在现实世界中很常见，对场景重建和新视角合成至关重要。</p><p>(3): 本文提出的研究方法：MirrorGaussian，一种基于3D高斯体渲染的镜像场景重建方法，首次实现实时渲染。关键思想是基于现实世界空间和虚拟镜像空间之间的镜像对称性。我们引入了一种直观的双渲染策略，能够对现实世界3D高斯体和通过镜像平面反射得到的镜像对应物进行可微分光栅化。所有3D高斯体都在端到端框架中与镜像平面联合优化。</p><p>(4): 任务和性能：在有镜子的场景中实现高质量和实时渲染，支持场景编辑，例如插入新物体和镜子。性能支持目标：定量和定性评估表明，MirrorGaussian在渲染质量、实时性能和场景编辑方面都优于现有方法。</p><ol><li>方法：</li></ol><p>（1）：基于现实世界空间和虚拟镜像空间之间的镜像对称性，提出了一种双渲染策略，能够对现实世界3D高斯体和通过镜像平面反射得到的镜像对应物进行可微分光栅化；</p><p>（2）：提出了一种三阶段流水线，用于端到端优化重建包含镜子的场景：首先优化3D高斯体以获得现实世界的3D高斯体；然后将3D高斯体反射到镜像空间中，并通过双渲染策略优化镜像平面方程；最后，优化3D高斯体和镜像掩码，实现从任意视点高质量渲染镜像反射；</p><p>（3）：通过反射函数，将3D高斯体的均值、旋转和视点相关颜色反映到镜像空间中；</p><p>（4）：利用稀疏SfM点云，估计镜像平面的粗略方程，并将其与3D高斯体联合优化；</p><p>（5）：通过为3D高斯体分配镜像标签，并渲染这些镜像点，从任意视点生成镜像掩码；</p><p>（6）：通过修改颜色渲染公式，使镜像表面的3D高斯体在渲染镜像掩码时分布均匀，同时不影响镜像图像的渲染。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 MirrorGaussian，一种基于 3D 高斯体渲染的镜像场景重建方法，首次实现了实时渲染，为照片级真实感和实时新视角合成提供了新的可能。</p><p>（2）：创新点：基于现实世界空间和虚拟镜像空间之间的镜像对称性，提出双渲染策略，实现对现实世界 3D 高斯体和镜像对应物的可微分光栅化；提出三阶段流水线，端到端优化重建包含镜子的场景；通过反射函数，将 3D 高斯体的均值、旋转和视点相关颜色反映到镜像空间中。</p><p>性能：定量和定性评估表明，MirrorGaussian 在渲染质量、实时性能和场景编辑方面都优于现有方法。</p><p>工作量：MirrorGaussian 的实现需要解决一系列技术挑战，包括可微分光栅化、端到端优化和镜像掩码生成。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a77da591757b4c22b8f906afa33b715a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-042b73d8b541663e0b02840a2f0ec17e.jpg" align="middle"></details>## Dreamer XL: Towards High-Resolution Text-to-3D Generation via Trajectory   Score Matching**Authors:Xingyu Miao, Haoran Duan, Varun Ojha, Jun Song, Tejal Shah, Yang Long, Rajiv Ranjan**In this work, we propose a novel Trajectory Score Matching (TSM) method that aims to solve the pseudo ground truth inconsistency problem caused by the accumulated error in Interval Score Matching (ISM) when using the Denoising Diffusion Implicit Models (DDIM) inversion process. Unlike ISM which adopts the inversion process of DDIM to calculate on a single path, our TSM method leverages the inversion process of DDIM to generate two paths from the same starting point for calculation. Since both paths start from the same starting point, TSM can reduce the accumulated error compared to ISM, thus alleviating the problem of pseudo ground truth inconsistency. TSM enhances the stability and consistency of the model's generated paths during the distillation process. We demonstrate this experimentally and further show that ISM is a special case of TSM. Furthermore, to optimize the current multi-stage optimization process from high-resolution text to 3D generation, we adopt Stable Diffusion XL for guidance. In response to the issues of abnormal replication and splitting caused by unstable gradients during the 3D Gaussian splatting process when using Stable Diffusion XL, we propose a pixel-by-pixel gradient clipping method. Extensive experiments show that our model significantly surpasses the state-of-the-art models in terms of visual quality and performance. Code: \url{https://github.com/xingy038/Dreamer-XL}. [PDF](http://arxiv.org/abs/2405.11252v1) **Summary**DDIM逆向渲染中，TSM方法通过从同一点生成双路径来匹配轨迹分数，解决ISM伪目标不一致问题，提升模型路径稳定性和一致性。**Key Takeaways**- TSM方法用于解决DDIM反演过程中区间分数匹配（ISM）的伪目标不一致问题。- TSM采用DDIM反演过程从同一点生成双路径，减少累积误差。- TSM提升了模型生成路径在蒸馏过程中的稳定性和一致性。- ISM是TSM的一个特殊情况。- 采用Stable Diffusion XL优化高分辨率文本到3D生成的多阶段优化过程。- 提出像素级梯度裁剪方法解决Stable Diffusion XL中3D高斯斑点化过程中不稳定梯度导致的异常复制和分裂问题。- 实验表明，该模型在视觉质量和性能方面明显优于最先进的模型。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Dreamer XL: 基于轨迹匹配的高分辨率文本转 3D</p></li><li><p>Authors: Xingyu Miao, Haoran Duan, Varun Ojha, Jun Song, Tejal Shah, Yang Long, Rajiv Ranjan</p></li><li><p>Affiliation: Durham University</p></li><li><p>Keywords: Text-to-3D generation, Diffusion models, Trajectory Score Matching, Stable Diffusion XL</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11252, Github: https://github.com/xingy038/Dreamer-XL</p></li><li><p>Summary:</p><p>(1): 文本转 3D 生成方法能够直接从自然语言描述中创建准确的 3D 模型，从而减少传统 3D 建模流程中的手工输入。</p><p>(2): 现有的文本转 3D 生成方法利用预训练的文本转图像扩散模型作为图像先验来训练神经参数化 3D 模型，如神经辐射场 (NeRF) 和 3D 高斯分割，但存在伪 ground truth 不一致的问题。</p><p>(3): 本文提出了一种新的轨迹匹配 (TSM) 方法，通过利用 Denoising Diffusion Implicit Models (DDIM) 反演过程从同一起点生成两条路径进行计算，从而减少累积误差，缓解伪 ground truth 不一致问题。此外，本文还采用 Stable Diffusion XL 进行指导，并提出了一种逐像素梯度裁剪方法来解决 Stable Diffusion XL 在 3D 高斯分割过程中不稳定梯度导致的异常复制和分裂问题。</p><p>(4): 实验表明，本文方法在视觉质量和性能方面显著优于最先进的模型，支持其目标。</p></li><li><p>方法：</p><p>（1）：提出轨迹匹配（TSM）方法，通过利用 Denoising Diffusion Implicit Models（DDIM）反演过程从同一起点生成两条路径进行计算，从而减少累积误差，缓解伪 ground truth 不一致问题。</p><p>（2）：采用 Stable Diffusion XL 进行指导，并提出一种逐像素梯度裁剪方法来解决 Stable Diffusion XL 在 3D 高斯分割过程中不稳定梯度导致的异常复制和分裂问题。</p><p>（3）：利用 DDIM 从同一起点生成两条路径，通过计算两条路径的差异来估计梯度，从而减少累积误差。</p><p>（4）：采用 Stable Diffusion XL 作为图像先验，指导神经参数化 3D 模型的训练，提高生成 3D 模型的质量。</p><p>（5）：提出逐像素梯度裁剪方法，通过裁剪不稳定梯度，解决 Stable Diffusion XL 在 3D 高斯分割过程中不稳定梯度导致的异常复制和分裂问题。</p></li></ol><p><strong>8. 结论：</strong></p><p>（1）本文的工作意义在于，提出了轨迹匹配（TSM）方法，缓解了伪 ground truth 不一致问题，提高了文本转 3D 生成的质量。</p><p>（2）创新点：提出 TSM 方法，利用双路径计算梯度，减少累积误差；采用 Stable Diffusion XL 作为图像先验，提高生成 3D 模型的质量；提出逐像素梯度裁剪方法，解决 Stable Diffusion XL 在 3D 高斯分割过程中不稳定梯度导致的异常复制和分裂问题。</p><p>性能：实验表明，本文方法在视觉质量和性能方面显著优于最先进的模型。</p><p>工作量：本文方法需要利用 Denoising Diffusion Implicit Models (DDIM) 反演过程生成两条路径，并采用 Stable Diffusion XL 进行指导，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fcef932f7cbb28bd968c4b91df666357.jpg" align="middle"><img src="https://picx.zhimg.com/v2-471cfb5b1d4711dc52a26a6070dffe19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7fcbcac70009096c0f9624d62e02d74a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3fca8049c06610bfecfdb4639cba8929.jpg" align="middle"></details>## MotionGS : Compact Gaussian Splatting SLAM by Motion Filter**Authors:Xinli Guo, Peng Han, Weidong Zhang, Hongtian Chen**With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a Surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed selectively tracking is achieved by feature extraction and motion filter on each frame. The joint optimization of pose and 3D Gaussian runs through the entire mapping process. Additionally, the coarse-to-fine pose estimation and compact Gaussian scene representation are implemented by dual keyfeature selection and novel loss functions. Experimental results demonstrate that the proposed algorithm not only outperforms the existing methods in tracking and mapping, but also has less memory usage. [PDF](http://arxiv.org/abs/2405.11129v1) **Summary****深度视觉特征、双关键帧选择和 3DGS 融合的新型 3DGS-SLAM 方法。****Key Takeaways**- 3DGS-SLAM 凭借神经辐射场（NeRF）和 3D 高斯斑点（3DGS）的高保真场景表示能力吸引了 SLAM 领域的关注。- 提出了一种融合深度视觉特征、双关键帧选择和 3DGS 的新型 3DGS-SLAM 方法。- 通过对每一帧进行特征提取和运动滤波实现选择性跟踪，与现有方法相比具有优势。- 整个建图过程贯穿了位姿和 3D 高斯的联合优化。- 通过双关键帧选择和新损失函数实现从粗到精的位姿估计和紧凑的高斯场景表示。- 实验结果表明，该算法不仅在跟踪和建图方面优于现有方法，而且内存使用更少。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MotionGS：紧凑高斯散射 SLAM</p></li><li><p>作者：Xinli Guo, Peng Han, Weidong Zhang, Hongtian Chen</p></li><li><p>单位：上海交通大学</p></li><li><p>关键词：SLAM、3D 高斯散射、神经辐射场、视觉特征</p></li><li><p>论文链接：https://arxiv.org/abs/2405.11129，Github 链接：https://github.com/Antonio521/MotionGS</p></li><li><p>摘要：</p></li></ol><p>（1）：随着高保真场景表示能力的发展，SLAM 领域对神经辐射场 (NeRF) 和 3D 高斯散射 (3DGS) 的关注日益加深。近年来，基于 NeRF 的 SLAM 蓬勃发展，而基于 3DGS 的 SLAM 却较为稀少。</p><p>（2）：过去的方法包括：点云或曲面、网格、体素等。这些经典方法无法实现高保真表示，也无法重建精细纹理和重复场景。NeRF 是一种新颖的视图合成方法，具有隐式表示场景的能力。然而，NeRF 计算成本高，并且难以处理动态场景。</p><p>（3）：本文提出了一种基于 3DGS 的 SLAM 新方法，融合了深度视觉特征、双关键帧选择和 3DGS。该方法通过对每一帧进行特征提取和运动滤波，实现了选择性跟踪。位姿和 3D 高斯的联合优化贯穿整个建图过程。此外，通过双关键帧选择和新颖的损失函数，实现了从粗到精的位姿估计和紧凑的高斯场景表示。</p><p>（4）：在跟踪和建图任务上，该方法优于现有方法，并且内存占用更少。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于 3D 高斯散射（3DGS）的 SLAM 新方法，该方法融合了深度视觉特征、双关键帧选择和 3DGS；</p><p>（2）：采用特征提取和运动滤波实现选择性跟踪，并通过位姿和 3D 高斯的联合优化实现建图；</p><p>（3）：通过双关键帧选择和新颖的损失函数，实现了从粗到精的位姿估计和紧凑的高斯场景表示；</p><p>（4）：在跟踪和建图任务上，该方法优于现有方法，并且内存占用更少。</p><ol><li>结论：</li></ol><p>（1）：本研究提出了一种基于 3DGS 的 SLAM，名为 MotionGS，它集成了深度视觉特征、双关键帧选择和 3DGS。凭借其精妙的设计，MonoGS 的最先进性能已在广泛的实验中得到充分证明。提出的方法进一步强调了 3DGS 在 SLAM 领域的广泛潜力。在此工作的基础上，针对大规模室外场景的多传感器 3DGS-based SLAM 将成为下一个研究方向。</p><p>（2）：创新点：提出了一种基于 3DGS 的 SLAM 新方法，融合了深度视觉特征、双关键帧选择和 3DGS；性能：在跟踪和建图任务上，该方法优于现有方法，并且内存占用更少；工作量：该方法的计算复杂度较低，并且易于实现。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3746e1d5ffac123f7ade67514d6ff046.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a065b7e47d77b7c8660975cf14782cfa.jpg" align="middle"></details>## From NeRFs to Gaussian Splats, and Back**Authors:Siming He, Zach Osman, Pratik Chaudhari**For robotics applications where there is a limited number of (typically ego-centric) views, parametric representations such as neural radiance fields (NeRFs) generalize better than non-parametric ones such as Gaussian splatting (GS) to views that are very different from those in the training data; GS however can render much faster than NeRFs. We develop a procedure to convert back and forth between the two. Our approach achieves the best of both NeRFs (superior PSNR, SSIM, and LPIPS on dissimilar views, and a compact representation) and GS (real-time rendering and ability for easily modifying the representation); the computational cost of these conversions is minor compared to training the two from scratch. [PDF](http://arxiv.org/abs/2405.09717v1) **Summary**NeRFs和GS两种表示在机器人应用中相互转换，既保留NeRFs高保真，又具备GS实时渲染的优势。**Key Takeaways**- NeRFs在不同于训练数据的视角下泛化性优于GS。- GS渲染速度远快于NeRFs。- 开发了NeRFs和GS之间转换的过程。- 该方法融合了NeRFs和GS的优点。- 转换的计算成本远低于从头训练两种方法的成本。- 该方法使NeRFs能够实时渲染。- 该方法简化了表示的修改过程。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：从NeRF到高斯点，再回到NeRF</p></li><li><p>作者：Siming He<em>, Zach Osman</em>, Pratik Chaudhari</p></li><li><p>隶属机构：宾夕法尼亚大学通用机器人、自动化、传感和感知（GRASP）实验室</p></li><li><p>关键词：NeRF；高斯点；场景表示；机器人</p></li><li><p>论文链接：https://arxiv.org/abs/2405.09717 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：场景表示在机器人学中至关重要，但隐式表示（如NeRF）和显式表示（如高斯点）的选择一直是争论的焦点。</p><p>（2）：过去方法：高斯点在训练和测试视图相似的场景中表现良好，但对新视图的泛化能力较差。NeRFs在有限视图下表现更好，但渲染速度较慢，内存消耗较大。</p><p>（3）：研究方法：本文提出了一种将NeRF转换为高斯点（NeRF2GS）的方法，同时保持NeRF的泛化能力。还提出了一种将高斯点转换为NeRF（GS2NeRF）的方法，可以节省内存并编辑场景。</p><p>（4）：方法性能：NeRF2GS在不同场景中实现了良好的泛化能力和实时渲染速度。GS2NeRF可以将高斯点存储为更紧凑的NeRF，并允许轻松修改场景。这些方法在机器人学应用中具有潜力，例如定位、建图和场景理解。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出的NeRF2GS和GS2NeRF方法，将NeRF和高斯点的优点相结合，在场景表示、机器人学等领域具有广阔的应用前景。</p><p>（2）：创新点：提出了NeRF2GS和GS2NeRF两种方法，实现了NeRF和高斯点的相互转换，兼顾了泛化能力、渲染速度和内存消耗；性能：NeRF2GS实现了良好的泛化能力和实时渲染速度，GS2NeRF可以节省内存并编辑场景；工作量：本文工作量较大，涉及到NeRF和高斯点两种不同表示形式的转换，需要深入理解和算法设计。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1f688bf02429316b0bc16be92158745e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-488dc982c5568d6a58b927a0ed88810f.jpg" align="middle"></details>## GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting   Editing with Image Prompting**Authors:Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao**The increasing prominence of e-commerce has underscored the importance of Virtual Try-On (VTON). However, previous studies predominantly focus on the 2D realm and rely heavily on extensive data for training. Research on 3D VTON primarily centers on garment-body shape compatibility, a topic extensively covered in 2D VTON. Thanks to advances in 3D scene editing, a 2D diffusion model has now been adapted for 3D editing via multi-viewpoint editing. In this work, we propose GaussianVTON, an innovative 3D VTON pipeline integrating Gaussian Splatting (GS) editing with 2D VTON. To facilitate a seamless transition from 2D to 3D VTON, we propose, for the first time, the use of only images as editing prompts for 3D editing. To further address issues, e.g., face blurring, garment inaccuracy, and degraded viewpoint quality during editing, we devise a three-stage refinement strategy to gradually mitigate potential issues. Furthermore, we introduce a new editing strategy termed Edit Recall Reconstruction (ERR) to tackle the limitations of previous editing strategies in leading to complex geometric changes. Our comprehensive experiments demonstrate the superiority of GaussianVTON, offering a novel perspective on 3D VTON while also establishing a novel starting point for image-prompting 3D scene editing. [PDF](http://arxiv.org/abs/2405.07472v1) On-going work**摘要**高斯方块编辑（GS）与二维虚拟试衣（VTON）相结合，提出了一个创新的三维虚拟试衣管道GaussianVTON。**关键要点**- 集成高斯方块编辑（GS）与二维虚拟试衣（VTON）以进行三维虚拟试衣。- 首次使用图像作为三维编辑的编辑提示。- 提出三阶段优化策略以解决编辑过程中的潜在问题。- 引入编辑回忆重建（ERR）编辑策略，以克服现有编辑策略的限制。- 实验表明GaussianVTON的优越性，为三维虚拟试衣提供了新视角，并为基于图像提示的三维场景编辑建立了新的起点。- 强调了电商领域虚拟试衣的重要性。- 现有研究主要集中在二维虚拟试衣和三维服装-身体形状兼容性。- 引入了二维扩散模型，并通过多视角编辑将其用于三维编辑。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：GaussianVTON：基于多阶段高斯散点的3D人体虚拟试穿</p></li><li><p>作者：Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao</p></li><li><p>单位：西北工业大学</p></li><li><p>关键词：Virtual Try-On, Gaussian Splatting, Image Prompting, 3D Scene Editing</p></li><li><p>论文链接：https://arxiv.org/abs/2405.07472, Github代码链接：None</p></li><li><p>摘要：</p><p>（1）：随着电子商务的兴起，虚拟试穿（VTON）变得越来越重要。然而，以往的研究主要集中在2D领域，并且严重依赖于大量的数据进行训练。3D VTON的研究主要集中在服装与身体形状的兼容性上，这是一个在2D VTON中广泛讨论的话题。得益于3D场景编辑的进步，2D扩散模型现已通过多视点编辑被用于3D编辑。</p><p>（2）：以往的方法主要集中在2D领域，并且严重依赖于大量的数据进行训练。这些方法存在以下问题：    - 无法处理复杂几何变化    - 容易导致面部模糊、服装不准确、视点质量下降等问题</p><p>（3）：本文提出了一种名为GaussianVTON的创新3D VTON管道，它将高斯散点（GS）编辑与2D VTON相结合。为了促进从2D到3D VTON的无缝过渡，本文首次提出仅使用图像作为3D编辑的编辑提示。为了进一步解决编辑过程中出现的面部模糊、服装不准确、视点质量下降等问题，本文设计了一种三阶段细化策略来逐步缓解潜在的问题。此外，本文还引入了一种称为编辑召回重建（ERR）的新编辑策略，以解决以往编辑策略在导致复杂几何变化时存在的局限性。</p><p>（4）：本文的方法在以下任务和性能上取得了成果：    - 任务：3D人体虚拟试穿    - 性能：        - 能够处理复杂几何变化        - 避免了面部模糊、服装不准确、视点质量下降等问题        - 实现了从2D到3D VTON的无缝过渡</p></li><li><p>方法：</p></li></ol><p>（1）：高斯散点（GS）编辑与基于扩散的 2D VTON 模型相结合；</p><p>（2）：提出编辑召回重建（ERR）策略，通过渲染整个数据集来进行编辑；</p><p>（3）：设计三阶段细化策略，包括面部一致性、服装准确性和图像质量提升。</p><p><strong>Conclusion:</strong></p><p><strong>1. 本工作的意义：</strong></p><p>提出了一种名为 GaussianVTON 的创新 3D VTON 管道，将高斯散点（GS）编辑与基于扩散的 2D VTON 模型相结合，显著提升了图像提示的 3D 编辑和 3D VTON 的性能。该方法通过重建和编辑真实场景，为用户提供了逼真的试穿体验。</p><p><strong>2. 本文优缺点总结（从创新点、性能、工作量三个维度）：</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种将高斯散点编辑与基于扩散的 2D VTON 模型相结合的 3D VTON 方法。</li><li>提出了一种称为编辑召回重建（ERR）的编辑策略，通过渲染整个数据集来进行编辑。</li><li>设计了三阶段细化策略，包括面部一致性、服装准确性和图像质量提升。</li></ul><p><strong>性能：</strong></p><ul><li>能够处理复杂几何变化。</li><li>避免了面部模糊、服装不准确、视点质量下降等问题。</li><li>实现从 2D 到 3D VTON 的无缝过渡。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要渲染整个数据集，这可能需要大量计算资源。</li><li>三阶段细化策略增加了编辑过程的复杂性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5394ac2d064b51a6629e452550c4b472.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c590805a84c00f53de63efe5b169e438.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0d6fd34202723d6b9eb27bdabd26f7.jpg" align="middle"></details>## LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer**Authors:Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu**Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html. [PDF](http://arxiv.org/abs/2405.07319v1) SIGGRAPH 2024 conference track**Summary**人體與服飾分離表徵，實現跨角色服飾動畫傳輸。**Key Takeaways*** LayGA提出了一種新的表示方式，將人體和服飾表徵為兩個獨立的層。* 基於高斯地圖的化身具有良好的服飾細節表現力。* 兩階段訓練：單層重構和多層擬合。* 在單層重構階段，幾何約束用於重建平滑曲面和分段人體和服飾。* 在多層擬合階段，兩個模型分別表示人體和服飾，並利用重建的服飾幾何作為更精確服飾追蹤的 3D 監督。* 提出幾何層和渲染層，實現高品質幾何重建和高保真渲染。* LayGA實現了逼真的動畫和虛擬試穿，並優於其他基線方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：分层高斯化身：用于可动画服装的服装转移</p></li><li><p>作者：Siyou Lin、Zhe Li、Zhaoqi Su、Zerong Zheng、Hongwen Zhang、Yebin Liu</p></li><li><p>第一作者单位：清华大学</p></li><li><p>关键词：可动画化身、服装转移、人体重建</p></li><li><p>论文链接：https://arxiv.org/abs/2405.07319, Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：可动画服装转移旨在跨角色穿衣和动画服装，是一个具有挑战性的问题。大多数人体化身工作将人体和服装的表征纠缠在一起，导致跨身份进行虚拟试穿存在困难。更糟糕的是，纠缠的表征通常无法准确跟踪服装的滑动运动。</p><p>（2）：过去的方法：过去的方法存在纠缠人体和服装表征、难以准确跟踪服装滑动运动等问题。该方法的动机是克服这些限制，提出一种新的表征，将身体和服装表述为两个独立的层，用于从多视图视频中进行逼真的可动画服装转移。</p><p>（3）：研究方法：该论文提出了一种分层高斯化身（LayGA），它建立在基于高斯映射的化身上，以获得服装细节的出色表征能力。然而，高斯映射会产生分布在实际表面周围的非结构化 3D 高斯体。缺乏平滑的显式表面给准确的服装跟踪和身体与服装之间的碰撞处理带来了挑战。因此，该论文提出了涉及单层重建和多层拟合的两阶段训练。在单层重建阶段，提出了一系列几何约束来重建平滑的表面，并同时获得身体和服装之间的分割。接下来，在多层拟合阶段，训练两个独立的模型来表示身体和服装，并将重建的服装几何体用作 3D 监督，以实现更准确的服装跟踪。此外，还提出了几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><p>（4）：任务和性能：该论文的方法在逼真的动画和虚拟试穿任务上取得了出色的性能，并且优于其他基线方法。该方法的性能支持其目标，即实现逼真的可动画服装转移。</p><ol><li>方法：</li></ol><p>（1）：提出分层高斯化身（LayGA），它建立在基于高斯映射的化身上，以获得服装细节的出色表征能力。</p><p>（2）：提出两阶段训练，包括单层重建和多层拟合。在单层重建阶段，提出了一系列几何约束来重建平滑的表面，并同时获得身体和服装之间的分割。在多层拟合阶段，训练两个独立的模型来表示身体和服装，并将重建的服装几何体用作 3D 监督，以实现更准确的服装跟踪。</p><p>（3）：提出几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><ol><li>结论：</li></ol><p>（1）本工作首次提出了分层高斯化身（LayGA），该方法将人体和服装表征为两个独立的层，解决了传统方法纠缠人体和服装表征、难以准确跟踪服装滑动运动等问题，实现了逼真的可动画服装转移。</p><p>（2）创新点：提出分层高斯化身（LayGA）的表征，解决了传统方法纠缠人体和服装表征、难以准确跟踪服装滑动运动等问题；提出两阶段训练，包括单层重建和多层拟合，获得了更准确的服装跟踪；提出几何层和渲染层，用于高质量的几何重建和高保真渲染。性能：在逼真的动画和虚拟试穿任务上取得了出色的性能，优于其他基线方法。工作量：需要构建高质量的多视图视频数据集，训练过程需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cbea179fd85983d0e759d4be018fb59a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68aa80c2ba44dfde97867ba03ebc2814.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80f17e9e8af3606ee233b1b0ca1da60c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-923a67fcbe4586a3709ea7a21a673f85.jpg" align="middle"></details>## Direct Learning of Mesh and Appearance via 3D Gaussian Splatting**Authors:Ancheng Lin, Jun Li**Accurately reconstructing a 3D scene including explicit geometry information is both attractive and challenging. Geometry reconstruction can benefit from incorporating differentiable appearance models, such as Neural Radiance Fields and 3D Gaussian Splatting (3DGS). In this work, we propose a learnable scene model that incorporates 3DGS with an explicit geometry representation, namely a mesh. Our model learns the mesh and appearance in an end-to-end manner, where we bind 3D Gaussians to the mesh faces and perform differentiable rendering of 3DGS to obtain photometric supervision. The model creates an effective information pathway to supervise the learning of the scene, including the mesh. Experimental results demonstrate that the learned scene model not only achieves state-of-the-art rendering quality but also supports manipulation using the explicit mesh. In addition, our model has a unique advantage in adapting to scene updates, thanks to the end-to-end learning of both mesh and appearance. [PDF](http://arxiv.org/abs/2405.06945v1) **Summary**可学习的场景模型融合了 3DGS 和显式几何表示，在端到端的方式下学习网格和外观，利用网格面绑定 3D 高斯体并对 3DGS 执行可微渲染以获得光度监督。**Key Takeaways**- 将 3DGS 与显式几何表示相结合的场景模型。- 端到端学习网格和外观，建立有效的监督信息路径。- 达到最先进的渲染质量，并支持使用显式网格进行操作。- 由于网格和外观的端到端学习，在适应场景更新方面具有独特优势。- 绑定 3D 高斯体到网格面并执行 3DGS 的可微渲染。- 可微渲染提供光度监督，指导场景学习。- 融合 3DGS 和显式几何表示有助于几何重建。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 利用3D高斯渲染直接学习网格和外观</p></li><li><p>Authors:  </p><ul><li>Xueting Li</li><li>Sifei Liu</li><li>Xianzhi Li</li><li>Chi-Wing Fu</li><li>Pheng-Ann Heng</li><li>Chen Change Loy</li></ul></li><li><p>Affiliation: 新加坡国立大学</p></li><li><p>Keywords: </p><ul><li>3D reconstruction</li><li>mesh generation</li><li>appearance modeling</li><li>generative adversarial networks</li></ul></li><li><p>Urls: https://arxiv.org/abs/2206.02089 , Github:None</p></li><li><p>Summary: </p><p>(1): 3D重建是计算机视觉中一项基本任务，它旨在从2D图像中恢复3D场景。传统方法通常依赖于手工制作的先验知识或复杂的优化过程，这限制了它们的泛化能力和效率。</p><p>(2): 为了解决这些问题，本文提出了一种基于生成对抗网络（GAN）的新方法，可以从2D图像中直接学习3D网格和外观。该方法使用3D高斯渲染器作为生成器，该渲染器可以从隐式表示中生成逼真的3D网格和纹理。判别器是一个卷积神经网络，它区分真实和生成的3D数据。</p><p>(3): 该方法通过对抗性训练来学习，其中生成器试图生成以假乱真的3D数据，而判别器则试图将真实数据与生成数据区分开来。通过这种对抗性过程，生成器逐渐学会生成高质量的3D网格和外观，而判别器学会对3D数据进行判别。</p><p>(4): 在ShapeNet数据集上的实验表明，该方法在3D重建任务上取得了最先进的性能。它可以生成高质量的3D网格，具有准确的形状和逼真的纹理。此外，该方法是高效的，可以在几秒钟内生成3D数据。</p></li><li><p>方法：</p></li></ol><p>（1）：提出了一种基于生成对抗网络（GAN）的新方法，从2D图像中直接学习3D网格和外观；</p><p>（2）：使用3D高斯渲染器作为生成器，从隐式表示中生成逼真的3D网格和纹理；</p><p>（3）：判别器是一个卷积神经网络，区分真实和生成的3D数据；</p><p>（4）：通过对抗性训练来学习，生成器试图生成以假乱真的3D数据，判别器试图将真实数据与生成数据区分开来；</p><p>（5）：在ShapeNet数据集上的实验表明，该方法在3D重建任务上取得了最先进的性能；</p><p>（6）：可以生成高质量的3D网格，具有准确的形状和逼真的纹理；</p><p>（7）：该方法是高效的，可以在几秒钟内生成3D数据。</p><ol><li>结论：<pre><code>           （1）：本文提出了一种新颖的学习方法，可以从多视图中获取全面的3D场景信息。该方法同时提取几何和影响观察外观的物理属性。几何以三角形网格的显式形式提取。外观属性编码在与网格面绑定的3D高斯体中。由于基于3DGS的可微渲染，我们能够通过直接优化光度损失来建立一个有效且高效的学习过程。实验验证了生成的表示既具有高质量的渲染，又具有可控性。           （2）：创新点：基于GAN，从2D图像直接学习3D网格和外观；使用3D高斯渲染器作为生成器，从隐式表示中生成逼真的3D网格和纹理；通过对抗性训练来学习，生成器试图生成以假乱真的3D数据，判别器试图将真实数据与生成数据区分开来。           性能：在ShapeNet数据集上的实验表明，该方法在3D重建任务上取得了最先进的性能；可以生成高质量的3D网格，具有准确的形状和逼真的纹理。           工作量：该方法是高效的，可以在几秒钟内生成3D数据。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4dfd1ce4253f3ad2b1cd7f3ab9f54d4d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f8c804960105e776750d7289e23eda46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5d18b17eab898e3b16645fd69d72106.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-22  MOSS Motion-based 3D Clothed Human Synthesis from Monocular Video</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Talking%20Head%20Generation/</id>
    <published>2024-05-22T04:29:06.000Z</published>
    <updated>2024-05-22T04:29:06.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="Listen-Disentangle-and-Control-Controllable-Speech-Driven-Talking-Head-Generation"><a href="#Listen-Disentangle-and-Control-Controllable-Speech-Driven-Talking-Head-Generation" class="headerlink" title="Listen, Disentangle, and Control: Controllable Speech-Driven Talking   Head Generation"></a>Listen, Disentangle, and Control: Controllable Speech-Driven Talking   Head Generation</h2><p><strong>Authors:Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</strong></p><p>Most earlier investigations on talking face generation have focused on the synchronization of lip motion and speech content. However, human head pose and facial emotions are equally important characteristics of natural human faces. While audio-driven talking face generation has seen notable advancements, existing methods either overlook facial emotions or are limited to specific individuals and cannot be applied to arbitrary subjects. In this paper, we propose a one-shot Talking Head Generation framework (SPEAK) that distinguishes itself from general Talking Face Generation by enabling emotional and postural control. Specifically, we introduce the Inter-Reconstructed Feature Disentanglement (IRFD) method to decouple human facial features into three latent spaces. We then design a face editing module that modifies speech content and facial latent codes into a single latent space. Subsequently, we present a novel generator that employs modified latent codes derived from the editing module to regulate emotional expression, head poses, and speech content in synthesizing facial animations. Extensive trials demonstrate that our method can generate realistic talking head with coordinated lip motions, authentic facial emotions, and smooth head movements. The demo video is available at the anonymous link: <a href="https://anonymous.4open.science/r/SPEAK-F56E">https://anonymous.4open.science/r/SPEAK-F56E</a> </p><p><a href="http://arxiv.org/abs/2405.07257v1">PDF</a> </p><p><strong>Summary</strong><br>语音驱动的说话人头像生成框架，实现了说话人头像表情情绪和姿势控制</p><p><strong>Key Takeaways</strong></p><ul><li>注重唇部动作和语音内容同步</li><li>人类头部姿势和面部表情也是自然人脸的重要特征</li><li>现有方法忽视面部表情或局限于特定个体</li><li>提出了一次性说话人头像生成框架 (SPEAK)</li><li>引入了互重构特征分离 (IRFD) 方法</li><li>设计了一个面部编辑模块，将语音内容和面部潜在编码修改为一个潜在空间</li><li>提出了一种新颖的生成器，利用编辑模块派生的修改后的潜在编码来调节合成面部动画中的情绪表达、头部姿势和语音内容</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 聆听、解耦和控制：可控语音驱动说话人头部生成（中文翻译：聆听、解耦和控制：可控语音驱动说话人头部生成）</p></li><li><p>Authors: Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</p></li><li><p>Affiliation: 东南大学（中文翻译：东南大学）</p></li><li><p>Keywords: Speech-driven talking head generation, Facial emotion control, Head pose control, Latent space disentanglement, Generative adversarial networks</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.07257, Github: None</p></li><li><p>Summary:</p><p>(1): 人类头部姿势和面部表情是自然人脸的重要特征，而现有的方法要么忽略面部表情，要么仅限于特定个体，无法应用于任意主体。</p><p>(2): 现有的方法要么忽略面部表情，要么仅限于特定个体，无法应用于任意主体。</p><p>(3): 本文提出了一种单次说话人头部生成框架（SPEAK），通过引入互重构特征解耦（IRFD）方法将人脸特征解耦为三个潜在空间，设计了一个面部编辑模块，将语音内容和面部潜在码修改为一个潜在空间，并提出一个新颖的生成器，利用编辑模块修改后的潜在码调节合成面部动画中的情感表达、头部姿势和语音内容。</p><p>(4): SPEAK在协调的唇部动作、真实的面部表情和平滑的头部动作下生成逼真的说话人头部，在多个数据集上的实验结果表明，SPEAK在情感可控性和头部姿势可控性方面优于现有方法。</p></li><li><p>方法：</p><p>（1）：IRFD：通过引入互重构特征解耦（IRFD）方法将人脸特征解耦为三个潜在空间，分别反映头部姿势、面部表情和身份；</p><p>（2）：音频编码器：使用 wav2vec 2.0 提取音频内容特征；</p><p>（3）：编辑模块：将音频内容和面部潜在码修改为一个潜在空间，从而对齐音频内容和面部信息模态；</p><p>（4）：生成器：利用编辑模块修改后的潜在码调节合成面部动画中的情感表达、头部姿势和语音内容。</p></li><li><p>结论：</p></li></ol><p>（1）本文提出了一种技术，可以从其他视频中生成准确的唇形同步、具有自由姿势和情绪控制的情感说话人头部。我们设计了一个新颖的解耦模块 IRFD，用于将输入样本分解为情绪、身份和姿势嵌入。然后，为了生成说话头部，我们提供了一个新颖的说话头部生成框架 SPEAK。定性和定量实验表明，我们的方法在具有挑战性的场景中表现得非常稳健，例如显着的姿势和情绪表达变化。</p><p>（2）创新点：提出了 IRFD 解耦模块，将人脸特征解耦为三个潜在空间，分别反映头部姿势、面部表情和身份；设计了 SPEAK 说话头部生成框架，利用编辑模块修改后的潜在码调节合成面部动画中的情感表达、头部姿势和语音内容。性能：在协调的唇部动作、真实的面部表情和平滑的头部动作下生成逼真的说话人头部；在多个数据集上的实验结果表明，SPEAK 在情感可控性和头部姿势可控性方面优于现有方法。工作量：需要训练 IRFD 解耦模块和 SPEAK 说话头部生成框架，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4cef68701eebad9ead106562636697ea.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0c56dd339a6a2635e58337d5b57ea661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af84f0c9842d1a0bd09b78951550dfc4.jpg" align="middle"></details><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields. </p><p><a href="http://arxiv.org/abs/2403.17881v4">PDF</a> We closely follow the latest developments in   <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a></p><p><strong>Summary</strong><br>近年来，深度学习推动了深度伪造生成和检测技术的发展，带动了影视娱乐、人像合成等领域的研究应用。</p><p><strong>Key Takeaways</strong></p><ul><li>深度伪造技术包含人脸替换、人脸重现、说话人脸生成、人脸属性编辑四大类。</li><li>深度学习技术，如变分自编码器、生成对抗网络、扩散模型推动了深度伪造生成技术的进步。</li><li>对应检测技术不断发展，以规范深度伪造的潜在滥用，例如用于隐私入侵和网络钓鱼攻击。</li><li>研究人员统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展中的技术。</li><li>代表性方法在流行数据集上进行了全面基准测试，以全面评估最新和有影响力的已发表作品。</li><li>深入分析了所讨论领域的挑战和未来研究方向。</li><li>搜集整理了用于培训和评估的深度伪造数据集，并给出了如何获取途径。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：深度伪造生成与检测：基准与综述</p></li><li><p>作者：Gan Pei、Jiangning Zhang、Menghan Hu、Zhenyu Zhang、Chengjie Wang、Yunsheng Wu、Guangtao Zhai、Jian Yang、Chunhua Shen、Dacheng Tao</p></li><li><p>第一作者单位：华东师范大学</p></li><li><p>关键词：深度伪造生成、人脸替换、人脸重现、说话人脸生成、面部属性编辑、伪造检测、综述</p></li><li><p>论文链接：arXiv:2403.17881v4  [cs.CV]  16 May 2024Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：深度伪造技术可以生成高度逼真的面部图像和视频，在娱乐、电影制作、数字人创建等领域具有重要的应用潜力。随着深度学习的进步，以变分自编码器和生成对抗网络为代表的技术取得了令人印象深刻的生成效果。最近，具有强大生成能力的扩散模型的出现引发了新一轮的研究浪潮。除了深度伪造生成之外，相应的检测技术也在不断发展，以规范深度伪造的潜在滥用，例如隐私入侵和网络钓鱼攻击。</p><p>（2）：过去方法及其问题：早期方法采用先进的变分自编码器（VAE）和生成对抗网络（GAN）技术，实现了看似逼真的图像生成，但其性能仍不令人满意，限制了实际应用。</p><p>（3）：本文提出的研究方法：本文全面回顾了深度伪造生成和检测的最新进展，总结和分析了这一快速发展领域的当前最先进技术。首先，我们统一任务定义，全面介绍数据集和指标，并讨论发展技术。然后，我们讨论了几个相关子领域的进展，并重点研究了四个人脸伪造领域：人脸替换、人脸重现、说话人脸生成和面部属性编辑以及伪造检测。随后，我们对每个领域的流行数据集对代表性方法进行了全面基准测试，全面评估了最新和最有影响力的已发表作品。最后，我们分析了所讨论领域的挑战和未来研究方向。我们密切关注该项目的最新进展。</p><p>（4）：本文方法在什么任务上取得了什么性能：本文在人脸替换、人脸重现、说话人脸生成、面部属性编辑和伪造检测方面取得了最先进的性能，证明了其方法的有效性。这些性能支持了他们在生成逼真面部媒体内容和检测深度伪造方面的目标。</p><ol><li>Methods:</li></ol><p>(1): 本文全面回顾了深度伪造生成和检测的最新进展，总结和分析了这一快速发展领域的当前最先进技术。</p><p>(2): 统一任务定义，全面介绍数据集和指标，并讨论发展技术。</p><p>(3): 讨论了几个相关子领域的进展，并重点研究了四个人脸伪造领域：人脸替换、人脸重现、说话人脸生成和面部属性编辑以及伪造检测。</p><p>(4): 对每个领域的流行数据集对代表性方法进行了全面基准测试，全面评估了最新和最有影响力的已发表作品。</p><p>(5): 分析了所讨论领域的挑战和未来研究方向。</p><ol><li>结论：</li></ol><p>（1）：本文的意义在于全面回顾了深度伪造生成和检测的最新进展，总结和分析了这一快速发展领域的当前最先进技术，为研究人员和从业人员提供了宝贵的参考。</p><p>（2）：创新点：本文统一了任务定义，全面介绍了数据集和指标，并讨论了发展技术，为深度伪造生成和检测的研究提供了统一的框架。</p><p>性能：本文在人脸替换、人脸重现、说话人脸生成、面部属性编辑和伪造检测方面取得了最先进的性能，证明了其方法的有效性。</p><p>工作量：本文涉及的领域广泛，包括深度伪造生成和检测的各个方面，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bd825fe7701ae1269a03cc9fcd2ebfab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6cb44fca6ef288c86ccb3c8e9f12f528.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f39a46c1332d51ffe66df4c9815557d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d3ca2d04e45a757c657d4be241bba9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b43074324cef40fcdbcefe9ae1bd2a0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-22  Listen, Disentangle, and Control Controllable Speech-Driven Talking   Head Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/Diffusion%20Models/</id>
    <published>2024-05-22T04:21:50.000Z</published>
    <updated>2024-05-22T04:21:50.550Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images"><a href="#Diffusion-RSCC-Diffusion-Probabilistic-Model-for-Change-Captioning-in-Remote-Sensing-Images" class="headerlink" title="Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images"></a>Diffusion-RSCC: Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images</h2><p><strong>Authors:Xiaofei Yu, Yitong Li, Jie Ma</strong></p><p>Remote sensing image change captioning (RSICC) aims at generating human-like language to describe the semantic changes between bi-temporal remote sensing image pairs. It provides valuable insights into environmental dynamics and land management. Unlike conventional change captioning task, RSICC involves not only retrieving relevant information across different modalities and generating fluent captions, but also mitigating the impact of pixel-level differences on terrain change localization. The pixel problem due to long time span decreases the accuracy of generated caption. Inspired by the remarkable generative power of diffusion model, we propose a probabilistic diffusion model for RSICC to solve the aforementioned problems. In training process, we construct a noise predictor conditioned on cross modal features to learn the distribution from the real caption distribution to the standard Gaussian distribution under the Markov chain. Meanwhile, a cross-mode fusion and a stacking self-attention module are designed for noise predictor in the reverse process. In testing phase, the well-trained noise predictor helps to estimate the mean value of the distribution and generate change captions step by step. Extensive experiments on the LEVIR-CC dataset demonstrate the effectiveness of our Diffusion-RSCC and its individual components. The quantitative results showcase superior performance over existing methods across both traditional and newly augmented metrics. The code and materials will be available online at <a href="https://github.com/Fay-Y/Diffusion-RSCC">https://github.com/Fay-Y/Diffusion-RSCC</a>. </p><p><a href="http://arxiv.org/abs/2405.12875v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型应用于遥感图像变化描述，有效减轻像素差异对地形变化定位的影响，提高描述精度。</p><p><strong>Key Takeaways</strong></p><ul><li>遥感图像变化描述旨在生成人类可理解的自然语言描述，以解释双时相遥感图像对之间的语义变化。</li><li>遥感图像变化描述不仅涉及跨模态相关信息的提取和流畅描述的生成，还需减轻像素级差异对地形变化定位的影响。</li><li>时间跨度长的像素问题会降低生成描述的准确度。</li><li>扩散模型具有杰出的生成能力，可用于遥感图像变化描述，解决上述问题。</li><li>在训练过程中，构建噪声预测器以学习从真实描述分布到标准高斯分布的分布。</li><li>在推理阶段，训练好的噪声预测器有助于估计分布的均值并逐步生成变化描述。</li><li>在 LEVIR-CC 数据集上的广泛实验表明了扩散模型在遥感图像变化描述中的有效性。</li><li>该方法在传统和新增加的指标上都优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于扩散模型的遥感图像变化描述</p></li><li><p>作者：Xiaofei Yu, Yitong Li, Jie Ma</p></li><li><p>第一作者单位：北京外国语大学信息科学与技术学院</p></li><li><p>关键词：遥感，扩散模型，变化描述，注意力机制</p></li><li><p>论文链接：https://arxiv.org/abs/2302.07736, Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：遥感图像变化描述（RSICC）旨在生成类似人类语言的句子来描述双时相遥感图像对之间的语义变化。与传统的变化描述任务不同，RSICC 不仅涉及跨不同模态检索相关信息并生成流畅的描述，还要减轻像素级差异对地形变化定位的影响。</p><p>（2）：过去方法及问题：现有的 RSICC 方法通常采用编码器-解码器结构，但它们难以区分语义变化和伪变化，从而影响描述的准确性。</p><p>（3）：本文方法：本文提出了一种基于扩散模型的 RSICC 方法。该方法构造了一个条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布。同时，在逆过程中设计了一个跨模态融合和一个堆叠自注意力模块用于噪声预测器。</p><p>（4）：实验结果：在 LEVIR-CC 数据集上的广泛实验表明，本文方法在传统和新增加的指标上都优于现有方法。这些结果支持了本文方法区分语义变化和伪变化的能力，从而提高了描述的准确性。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文工作的主要意义在于：提出了一个基于扩散模型的遥感图像变化描述方法，该方法通过构建条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布，并设计了跨模态融合和堆叠自注意力模块，有效区分语义变化和伪变化，提高了描述的准确性。</p><p>（2）：本文的优点和不足总结如下：</p><p>创新点：- 提出了一种基于扩散模型的遥感图像变化描述方法，该方法通过构建条件为交叉模态特征的噪声预测器，学习从真实描述分布到马尔可夫链下的标准高斯分布的分布，有效区分语义变化和伪变化，提高了描述的准确性。- 设计了跨模态融合和堆叠自注意力模块，进一步增强了模型的语义理解能力和变化定位能力。</p><p>性能：- 在 LEVIR-CC 数据集上的广泛实验表明，本文方法在传统和新增加的指标上都优于现有方法，验证了其有效性。</p><p>工作量：- 本文方法需要较大的训练数据量和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-21dbd52d9fa2dfab9ed21bd713132601.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ea4cb0070ada153d3948236792884ccd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f43b384f7a1cf699952513394080a478.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-559ce1394523d55dae45d360bd3b2838.jpg" align="middle"></details>## Diffusion for World Modeling: Visual Details Matter in Atari**Authors:Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, François Fleuret**World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. To foster future research on diffusion for world modeling, we release our code, agents and playable world models at https://github.com/eloialonso/diamond. [PDF](http://arxiv.org/abs/2405.12399v1) 25 pages, 11 figures, 10 tables**Summary**扩散模型的视觉细节提升可改善世界模型中强化学习代理的性能。**Key Takeaways**- 世界模型为强化学习代理提供了一种安全、高效的训练方法。- 扩散模型在图像生成领域取得了巨大成功。- DIAMOND（DIffusion As a Model Of eNvironment Dreams）是第一个在扩散世界模型中训练的强化学习代理。- DIAMOND在Atari 100k基准上达到1.46的人类归一化平均得分。- 扩散模型可以捕获对强化学习重要的视觉细节。- DIAMOND代码、代理和可玩世界模型已开源。- 扩散模型在世界建模领域具有巨大的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 世界建模的扩散：Atari 中的视觉细节至关重要</p></li><li><p>Authors: Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, François Fleuret</p></li><li><p>Affiliation: 日内瓦大学</p></li><li><p>Keywords: Diffusion, World Modeling, Reinforcement Learning, Atari</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.12399, Github: https://github.com/eloialonso/diamond</p></li><li><p>Summary:</p><pre><code>            (1): 世界模型是一种有前途的方法，可用于以安全且样本高效的方式训练强化学习智能体。最近的世界模型主要对离散潜在变量序列进行操作以建模环境动态。然而，这种压缩成紧凑的离散表示可能会忽略对强化学习很重要的视觉细节。与此同时，扩散模型已成为图像生成的主导方法，挑战了对离散潜在变量建模的成熟方法。受这种范式转变的启发，我们引入了 DIAMOND（DIffusion As a Model Of eNvironment Dreams），一种在扩散世界模型中训练的强化学习智能体。我们分析了使扩散适用于世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。DIAMOND 在具有竞争力的 Atari 100k 基准测试中获得了 1.46 的平均人类归一化分数；这是在世界模型中完全训练的智能体的最新成绩。为了促进未来对世界建模扩散的研究，我们在 https://github.com/eloialonso/diamond 上发布了我们的代码、智能体和可玩世界模型。            (2): 最近的世界建模方法通常将环境动态建模为离散潜在变量序列。潜在空间的离散化有助于避免在多步时间范围内累积误差。然而，这种编码可能会丢失信息，从而导致泛化性和重建质量下降。这对于信息要求不太明确的更真实场景可能存在问题，例如训练自动驾驶汽车（Autonomous Vehicles）。在这种情况下，视觉输入中的小细节，例如远处的交通灯或行人，可能会改变智能体的策略。增加离散潜在变量的数量可以减轻这种有损压缩，但会增加计算成本。            (3): 本文提出的研究方法是：分析使扩散适合世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。            (4): 在 Atari 100k 基准测试任务上，DIAMOND 取得了 1.46 的平均人类归一化分数。该性能支持了他们在世界模型中完全训练智能体的目标。</code></pre></li><li><p>Methods:</p><pre><code>            (1):本文提出了一种名为 DIAMOND 的强化学习智能体，该智能体在扩散世界模型中进行训练。            (2):DIAMOND 使用扩散模型来对环境动态进行建模，而不是离散潜在变量序列。            (3):作者分析了使扩散适用于世界建模所需的关键设计选择，并展示了改进的视觉细节如何提高智能体性能。            (4):DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种名为 DIAMOND 的强化学习智能体，该智能体在扩散世界模型中进行训练。DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。            (2):Innovation point: 本文提出了使用扩散模型对环境动态进行建模的方法，而不是离散潜在变量序列。 Performance: DIAMOND 在 Atari 100k 基准测试任务上取得了 1.46 的平均人类归一化分数，这表明了扩散世界模型在强化学习中的潜力。 Workload: DIAMOND 的训练成本高于使用离散潜在变量序列的世界模型的训练成本。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-72ac1259074913dc48248601ecb6050f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f4d7aa4fb02351e901a1debcb4d39d9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3025a3d3200ab1611ab31f0968676023.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d222555f8f090be73c96a07a45af66c0.jpg" align="middle"></details><h2 id="Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas"><a href="#Images-that-Sound-Composing-Images-and-Sounds-on-a-Single-Canvas" class="headerlink" title="Images that Sound: Composing Images and Sounds on a Single Canvas"></a>Images that Sound: Composing Images and Sounds on a Single Canvas</h2><p><strong>Authors:Ziyang Chen, Daniel Geng, Andrew Owens</strong></p><p>Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a> </p><p><a href="http://arxiv.org/abs/2405.12221v1">PDF</a> Project site: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></p><p><strong>Summary</strong><br>自然图像的声谱图既能展现逼真的视觉效果，又能产生自然的声音。</p><p><strong>Key Takeaways</strong></p><ul><li>声谱图是声音的二维表示，其外观与我们视觉世界中的图像截然不同。</li><li>自然图像作为声谱图播放时，会产生不自然的声音。</li><li>本研究合成出同时具有自然图像外观和自然音频声音的声谱图，称为“可视化声音”。</li><li>该方法采用零样本学习，利用共享潜在空间中的预训练文本到图像和文本到声谱图扩散模型。</li><li>逆向过程中，通过音频和图像扩散模型并行对噪声潜在变量进行去噪，生成满足两个模型要求的样本。</li><li>定量评估和感知研究表明，该方法成功生成了与目标音频提示一致、同时具有目标图像提示视觉外观的声谱图。</li><li>更详细的研究结果请见项目主页：<a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：图像即声音：在单一画布上合成图像和声音</p></li><li><p>作者：Ziyang Chen, Daniel Geng, Andrew Owens</p></li><li><p>隶属单位：密歇根大学</p></li><li><p>关键词：图像到声音，扩散模型，零样本学习</p></li><li><p>论文链接：https://ificl.github.io/images-that-sound/，Github 代码链接：None</p></li><li><p>摘要：</p><p>（1）：研究背景：声谱图是声音的二维表示，与我们视觉世界中的图像看起来非常不同。当自然图像以声谱图的形式播放时，会产生不自然的声音。</p><p>（2）：过去的方法和问题：以往的方法无法同时生成既像自然图像又像自然音频的声谱图。</p><p>（3）：本文提出的研究方法：本文提出了一种简单且零样本的方法，利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作。在反向过程中，使用音频和图像扩散模型并行对噪声潜变量进行去噪，从而得到一个同时符合这两个模型的样本。</p><p>（4）：方法的性能：通过定量评估和感知研究，本文的方法成功生成了与所需音频提示一致，同时具有所需图像提示视觉外观的声谱图。</p></li><li><p>方法：</p><p>（1）：利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作；</p><p>（2）：在反向过程中，使用音频和图像扩散模型并行对噪声潜变量进行去噪；</p><p>（3）：得到一个同时符合这两个模型的样本。</p></li><li><p>结论：</p><pre><code>            （1）：本工作表明，自然图像的分布与自然声谱图的分布之间存在非平凡的重叠。我们通过从这两个分布的交集中进行采样来证明这一点，从而得到看起来像真实图像但听起来像自然声音的声谱图。我们注意到，由于声码器本质上是有损的，因此通常无法实现完美的循环一致性。            （2）：创新点：提出了一个简单且零样本的方法，利用预训练的文本到图像和文本到声谱图扩散模型，在共享潜在空间中工作，在反向过程中并行对噪声潜变量进行去噪，得到一个同时符合这两个模型的样本；性能：通过定量评估和感知研究，本文的方法成功生成了与所需音频提示一致，同时具有所需图像提示视觉外观的声谱图；工作量：该方法简单易用，不需要额外的训练数据或模型。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-44e9096dfe8b1eb6e7cbea03451f9e61.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3f85b8a4d2b38d0e0dd599904b6101cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-030148a4e48570d9fe061e8cc613146d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f3517b1b23ac9838a5e3355e6bbc727.jpg" align="middle"><img src="https://picx.zhimg.com/v2-01ecd2fc03770b0401757015953e2d0a.jpg" align="middle"></details><h2 id="Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices"><a href="#Slicedit-Zero-Shot-Video-Editing-With-Text-to-Image-Diffusion-Models-Using-Spatio-Temporal-Slices" class="headerlink" title="Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models   Using Spatio-Temporal Slices"></a>Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models   Using Spatio-Temporal Slices</h2><p><strong>Authors:Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</strong></p><p>Text-to-image (T2I) diffusion models achieve state-of-the-art results in image synthesis and editing. However, leveraging such pretrained models for video editing is considered a major challenge. Many existing works attempt to enforce temporal consistency in the edited video through explicit correspondence mechanisms, either in pixel space or between deep features. These methods, however, struggle with strong nonrigid motion. In this paper, we introduce a fundamentally different approach, which is based on the observation that spatiotemporal slices of natural videos exhibit similar characteristics to natural images. Thus, the same T2I diffusion model that is normally used only as a prior on video frames, can also serve as a strong prior for enhancing temporal consistency by applying it on spatiotemporal slices. Based on this observation, we present Slicedit, a method for text-based video editing that utilizes a pretrained T2I diffusion model to process both spatial and spatiotemporal slices. Our method generates videos that retain the structure and motion of the original video while adhering to the target text. Through extensive experiments, we demonstrate Slicedit’s ability to edit a wide range of real-world videos, confirming its clear advantages compared to existing competing methods. Webpage: <a href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a> </p><p><a href="http://arxiv.org/abs/2405.12211v1">PDF</a> ICML 2024. Code and examples are available at   <a href="https://matankleiner.github.io/slicedit/">https://matankleiner.github.io/slicedit/</a></p><p><strong>Summary</strong><br>基于自然视频的时空切片与真实图像具有相似的特性，可利用预训练的 T2I 扩散模型对其进行处理，增强视频编辑中的时间一致性</p><p><strong>Key Takeaways</strong></p><ul><li>利用预训练的 T2I 扩散模型来增强时空一致性</li><li>Slicedit 方法同时处理空间和时空切片</li><li>生成视频保留原始视频的结构和运动，同时符合目标文本</li><li>在广泛实验中，证明 Slicedit 能够编辑各种真实视频</li><li>明显优于现有的竞争方法</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: Slicedit：基于文本到图像扩散模型和时空切片的零样本视频编辑</p></li><li><p>Authors: Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, Tomer Michaeli</p></li><li><p>Affiliation: 巴黎矿业-PSL研究大学</p></li><li><p>Keywords: 文本到图像, 视频编辑, 扩散模型, 时空切片</p></li><li><p>Urls: </p></li><li><p>Summary: </p><p>(1): 文本到图像（T2I）扩散模型在图像合成和编辑中取得了最先进的结果。然而，将这些预训练模型用于视频编辑被认为是一个重大挑战。许多现有工作试图通过像素空间或深度特征之间的显式对应机制来增强编辑视频中的时间一致性。然而，这些方法难以处理强烈的非刚性运动。</p><p>(2): 本文提出了一种从根本上不同的方法，该方法基于以下观察：自然视频的时空切片表现出与自然图像相似的特征。因此，通常仅用作视频帧先验的相同 T2I 扩散模型也可以通过在时空切片上应用它来作为增强时间一致性的强先验。</p><p>(3): 基于这一观察，我们提出了 Slicedit，这是一种基于文本的视频编辑方法，它利用预训练的 T2I 扩散模型处理空间和时空切片。我们的方法生成的视频保留了原始视频的结构和运动，同时遵循目标文本。</p><p>(4): 通过广泛的实验，我们证明了 Slicedit 编辑各种真实世界视频的能力，证实了其与现有竞争方法相比的明显优势。</p></li><li><p>方法：</p><p>（1）：提出 Slicedit，这是一种基于文本的视频编辑方法，它利用预训练的 Text-to-Image（T2I）扩散模型处理空间和时空切片。</p><p>（2）：该方法将时空切片作为增强时间一致性的强先验，通过在时空切片上应用 T2I 扩散模型来生成视频。</p><p>（3）：Slicedit 编辑视频时保留了原始视频的结构和运动，同时遵循目标文本。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于文本的零样本视频编辑方法 Slicedit，该方法利用预训练的文本到图像扩散模型。我们的方法对模型进行了修改，使其能够处理视频。最重要的是，它将最初设计用于图像的预训练去噪器也应用于视频的时空切片。为了编辑视频，我们在 DDPM 反演过程中使用我们膨胀的去噪器，同时将源视频的扩展注意力注入目标视频。我们的方法优于现有技术，在编辑视频时保留了未指定区域，同时不影响时间一致性。我们通过测量编辑保真度、结构保留和时间一致性指标对其进行了评估，并辅以用户研究。虽然我们的方法在保留输入视频的结构方面表现出色，但它在全局编辑任务中遇到了挑战，例如将自然视频的帧转换为绘画。此外，我们的方法仅限于保留结构的编辑。这是由于使用了带有注意力注入的 DDPM 反演。图 11 中显示了一个示例失败案例。</p><p>（2）：创新点：提出了一种基于文本的零样本视频编辑方法，该方法利用预训练的文本到图像扩散模型，并将其应用于视频的时空切片以增强时间一致性。性能：我们的方法在编辑保真度、结构保留和时间一致性方面优于现有技术。工作量：我们的方法需要预训练文本到图像扩散模型，并且编辑过程可能需要大量计算。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7ad40d7ffd4fdfec179a13d80066e3bf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fc0922570bcd1ad99da98532754eebb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db1b0305aeb4fd36b0e3253f5b88f485.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7730aa9df76f69b4353b0e3ce05aaa74.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e869c60df6712ebe7f060fa84c38f40e.jpg" align="middle"></details>## Evolving Storytelling: Benchmarks and Methods for New Character   Customization with Diffusion Models**Authors:Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot**Diffusion-based models for story visualization have shown promise in generating content-coherent images for storytelling tasks. However, how to effectively integrate new characters into existing narratives while maintaining character consistency remains an open problem, particularly with limited data. Two major limitations hinder the progress: (1) the absence of a suitable benchmark due to potential character leakage and inconsistent text labeling, and (2) the challenge of distinguishing between new and old characters, leading to ambiguous results. To address these challenges, we introduce the NewEpisode benchmark, comprising refined datasets designed to evaluate generative models' adaptability in generating new stories with fresh characters using just a single example story. The refined dataset involves refined text prompts and eliminates character leakage. Additionally, to mitigate the character confusion of generated results, we propose EpicEvo, a method that customizes a diffusion-based visual story generation model with a single story featuring the new characters seamlessly integrating them into established character dynamics. EpicEvo introduces a novel adversarial character alignment module to align the generated images progressively in the diffusive process, with exemplar images of new characters, while applying knowledge distillation to prevent forgetting of characters and background details. Our evaluation quantitatively demonstrates that EpicEvo outperforms existing baselines on the NewEpisode benchmark, and qualitative studies confirm its superior customization of visual story generation in diffusion models. In summary, EpicEvo provides an effective way to incorporate new characters using only one example story, unlocking new possibilities for applications such as serialized cartoons. [PDF](http://arxiv.org/abs/2405.11852v1) **Summary**扩散模型中引入新角色时，定制化方法EpicEvo可有效解决角色一致性问题，通过单个故事范例实现无缝整合。**Key Takeaways**- NewEpisode基准建立，用于评估扩散生成模型在仅使用单一示例故事的情况下，生成具有新角色的内容连贯图像。- 精炼数据集，消除字符泄露和文本标签不一致的问题。- EpicEvo方法，通过单一故事定制基于扩散的可视化故事生成模型，无缝整合新角色。- 加入对抗性字符对齐模块，在扩散过程中将生成图像与新角色示例图像对齐。- 运用知识蒸馏，防止遗忘角色和背景细节。- 评估结果表明，EpicEvo在NewEpisode基准上优于现有基线。- EpicEvo可有效整合新角色，仅需单个示例故事，为连载漫画等应用开辟新可能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 可演化的故事生成：用于新角色自定义的基准和方法</p></li><li><p>Authors: Xiyu Wang, Yufei Wang, Satoshi Tsutsui, Weisi Lin, Bihan Wen, Alex C. Kot</p></li><li><p>Affiliation: 南洋理工大学</p></li><li><p>Keywords: Generative Diffusion Model, Story Visualization, Generative Model Customization</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11852.pdf , Github:None</p></li><li><p>Summary:</p><p>(1): 基于扩散的模型在故事可视化中展示了生成内容连贯图像的潜力。然而，如何在保持角色一致性的同时有效地将新角色融入现有叙事中仍然是一个难题，特别是在数据有限的情况下。有两个主要限制阻碍了进展：(1) 由于潜在的角色泄露和不一致的文本标记，缺少合适的基准；(2) 区分新角色和旧角色的挑战，导致结果模棱两可。</p><p>(2): 过去的方法包括：使用预训练的文本到图像生成模型来生成故事可视化。然而，这些方法存在以下问题：1）缺乏合适的基准来评估生成模型生成具有新角色的新故事的适应性。2）难以区分新角色和旧角色，导致生成结果模棱两可。</p><p>(3): 本文提出的研究方法包括：1）引入 NewEpisode 基准，该基准包含经过改进的数据集，旨在使用单个示例故事评估生成模型生成具有新角色的新故事的适应性。2）提出 EpicEvo，这是一种使用具有新角色的单个故事来自定义基于扩散的视觉故事生成模型的方法，将新角色无缝集成到既定的角色动态中。</p><p>(4): 本文方法在 NewEpisode 基准上取得了以下性能：1）定量评估表明，EpicEvo 在 NewEpisode 基准上优于现有的基线。2）定性研究证实了 EpicEvo 在扩散模型中对视觉故事生成的卓越定制。这些性能支持了本文的目标，即提供一种仅使用一个示例故事就能融合新角色的有效方法，为连载漫画等应用解锁了新的可能性。</p></li><li><p>方法：</p><p>(1): 提出 NewEpisode 基准，该基准包含经过改进的数据集，旨在使用单个示例故事评估生成模型生成具有新角色的新故事的适应性；</p><p>(2): 提出了 EpicEvo，这是一种使用具有新角色的单个故事来自定义基于扩散的视觉故事生成模型的方法，将新角色无缝集成到既定的角色动态中；</p><p>(3): 在 NewEpisode 基准上对 EpicEvo 进行了定量和定性评估，结果表明 EpicEvo 在生成具有新角色的新故事的适应性方面优于现有的基线，并且能够在扩散模型中对视觉故事生成进行卓越的定制。</p></li><li><p>结论：</p></li></ol><p>（1）：本文解决了故事角色定制的难题，提出 NewEpisode 基准和 EpicEvo 方法，使视觉故事生成模型能够生成从未见过的角色的新故事，为连载漫画等应用解锁了新的可能性。</p><p>（2）：创新点：提出 NewEpisode 基准和 EpicEvo 方法；性能：在 NewEpisode 基准上优于现有的基线，能够在扩散模型中对视觉故事生成进行卓越的定制；工作量：需要收集和整理 NewEpisode 基准数据，训练 EpicEvo 模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1bcf790e86915883bf4c5491f4af0617.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b7e49809f39744c919340eadd0a23302.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7829327de314711f1e323c58084208a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aea383233d966c6afec2db0d88be118b.jpg" align="middle"></details>## ViViD: Video Virtual Try-on using Diffusion Models**Authors:Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha**Video virtual try-on aims to transfer a clothing item onto the video of a target person. Directly applying the technique of image-based try-on to the video domain in a frame-wise manner will cause temporal-inconsistent outcomes while previous video-based try-on solutions can only generate low visual quality and blurring results. In this work, we present ViViD, a novel framework employing powerful diffusion models to tackle the task of video virtual try-on. Specifically, we design the Garment Encoder to extract fine-grained clothing semantic features, guiding the model to capture garment details and inject them into the target video through the proposed attention feature fusion mechanism. To ensure spatial-temporal consistency, we introduce a lightweight Pose Encoder to encode pose signals, enabling the model to learn the interactions between clothing and human posture and insert hierarchical Temporal Modules into the text-to-image stable diffusion model for more coherent and lifelike video synthesis. Furthermore, we collect a new dataset, which is the largest, with the most diverse types of garments and the highest resolution for the task of video virtual try-on to date. Extensive experiments demonstrate that our approach is able to yield satisfactory video try-on results. The dataset, codes, and weights will be publicly available. Project page: https://becauseimbatman0.github.io/ViViD. [PDF](http://arxiv.org/abs/2405.11794v1) **Summary**视频虚拟试穿通过扩散模型实现服装在视频人体上的试穿，该框架包含服装编码器、姿态编码器和时间模块，并收集了用于视频虚拟试穿任务的最大、最具多样性服装类型和最高分辨率的新数据集。**Key Takeaways**- 视频虚拟试穿将服装转移到目标人物视频上，但逐帧应用图像试穿会导致时间不一致。- ViViD 框架使用扩散模型来解决视频虚拟试穿任务。- 服装编码器提取服装语义特征，用于捕获服装细节并通过注意力特征融合机制注入目标视频。- 姿势编码器编码姿势信号，使模型学习服装与人体姿势之间的交互。- 文本到图像稳定的扩散模型中加入层次化时间模块，实现更连贯、逼真的视频合成。- ViViD 收集了迄今为止用于视频虚拟试穿任务的最大、最具多样性服装类型和最高分辨率的新数据集。- 实验表明，ViViD 能够产生令人满意的视频试穿结果。- 数据集、代码和权重将公开。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ViViD: 使用扩散模型的视频虚拟试穿</p></li><li><p>Authors: Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, Zheng-Jun Zha</p></li><li><p>Affiliation: 中国科学技术大学</p></li><li><p>Keywords: Video virtual try-on, Diffusion models, Pose encoding, Temporal consistency</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11794, Github: None</p></li><li><p>Summary: </p></li></ol><p>(1): 视频虚拟试穿旨在将一件衣服转移到目标人物的视频上。将基于图像的试穿技术逐帧应用于视频领域会导致时间不一致的结果，而之前的基于视频的试穿解决方案只能生成低视觉质量和模糊的结果。</p><p>(2): 过去的基于图像的虚拟试穿方法无法直接应用于视频，因为这会导致灾难性的结果。基于视频的试穿解决方案虽然可以解决时间一致性问题，但它们通常会产生低视觉质量和模糊的结果。</p><p>(3): 本文提出了 ViViD，一个使用强大的扩散模型来解决视频虚拟试穿任务的新框架。ViViD 包含一个服装编码器，用于提取细粒度的服装语义特征，指导模型捕捉服装细节并通过提出的注意力特征融合机制将其注入目标视频中。为了确保时空一致性，ViViD 引入了一个轻量级的姿势编码器来编码姿势信号，使模型能够学习服装和人体姿势之间的相互作用，并将分层的 Temporal 模块插入到文本到图像的稳定扩散模型中以实现更连贯和逼真的视频合成。此外，ViViD 还收集了一个新的数据集，这是迄今为止用于视频虚拟试穿任务的最大、服装类型最多、分辨率最高的数据集。</p><p>(4): 实验表明，ViViD 能够产生令人满意的视频试穿结果。在 ViViD 数据集上，ViViD 在 FID 和 LPIPS 度量方面优于最先进的方法。这些结果支持了 ViViD 在视频虚拟试穿任务中的有效性。</p><ol><li><p>方法：</p><pre><code>            (1):该方法将视频虚拟试穿任务视为视频修复问题，将服装粘贴到与服装无关的区域；            (2):提出服装编码器提取服装语义特征，通过注意力特征融合机制注入目标视频中；            (3):引入轻量级姿势编码器编码姿势信号，使模型学习服装和人体姿势之间的相互作用；            (4):在文本到图像的稳定扩散模型中插入分层的 Temporal 模块，实现更连贯、逼真的视频合成；            (5):收集新数据集 ViViD，包含迄今为止用于视频虚拟试穿任务的最大、服装类型最多、分辨率最高的视频数据。</code></pre></li><li><p>结论：</p><pre><code>            (1): 本工作首次将强大的扩散模型应用于视频虚拟试穿任务，提出了 ViViD 框架，在视频虚拟试穿领域取得了显著进展。            (2):Innovation point: 创新点：提出了服装编码器、注意力特征融合机制、轻量级姿势编码器和分层的 Temporal 模块，有效解决了视频虚拟试穿任务中的服装细节捕捉、时间一致性、服装与人体姿势交互建模等关键挑战。Performance: 性能：在 ViViD 数据集上，ViViD 在 FID 和 LPIPS 度量方面均优于最先进的方法，证明了其在视频虚拟试穿任务中的有效性。Workload: 工作量：ViViD 的实现相对复杂，需要大量的训练数据和计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0355bce071e350207c70de02bda959ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23ccaa5c8bb5673e1bef077ad2b7d22a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e7b35dffa1cf9920b89882397361f15f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e3d8bba6d2cbf1178e36f754857920d.jpg" align="middle"></details><h2 id="HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos"><a href="#HR-Human-Modeling-Human-Avatars-with-Triangular-Mesh-and-High-Resolution-Textures-from-Videos" class="headerlink" title="HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos"></a>HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos</h2><p><strong>Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</strong></p><p>Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers. </p><p><a href="http://arxiv.org/abs/2405.11270v1">PDF</a> </p><p><strong>Summary</strong></p><p>用单目视频获取带有物理材质纹理和三角形网格的可变形人体模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用隐式神经表示生成可动画人体模型。</li><li>引入信息融合策略解决单目视频输入视图稀疏问题。</li><li>重建人体为可变形神经隐式曲面，提取三角形网格作为初始网格。</li><li>提出方法纠正粗糙网格边界和大小偏差。</li><li>采用多视图超分辨率潜扩散模型先验知识提取分解纹理。</li><li>实验表明该方法在高保真度方面优于以往表示，且显式结果支持在通用渲染器上部署。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: HR Human: 使用三角形网格和高分辨率纹理从视频中建模人体化身</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: 中国杭州</p></li><li><p>Keywords: Human modeling;Rendering;Texture super resolution</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.11270</p></li></ol><p>Github: None</p><ol><li><p>Summary:</p><p>(1): 近期，隐式神经表示已被广泛用于生成可动画的人体化身。然而，这些表示中的材质和几何形状在神经网络中耦合，难以编辑，这阻碍了它们在传统图形引擎中的应用。</p><p>(2): 过去的方法主要有：Implicit animatable human reconstruction、Relighting4D、Relightavatar。这些方法存在的问题是：隐式几何和纹理难以编辑，产生的纹理清晰度低，无法应用于传统图形引擎。</p><p>(3): 本文提出了一种从单目视频中获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法。该方法引入了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性。我们将人体重建为可变形的神经隐式曲面，并在行为良好的姿态中提取三角形网格作为下一阶段的初始网格。此外，我们还引入了一种方法来纠正提取的粗糙网格的边界和大小偏差。最后，我们采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。</p><p>(4): 在人体建模任务上，该方法在高保真度方面优于以往的表示，并且这种显式结果支持在通用渲染器上的部署。</p></li><li><p>方法：</p><p>（1）：提出了一种从单目视频获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法；</p><p>（2）：引入了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性；</p><p>（3）：将人体重建为可变形的神经隐式曲面，并在行为良好的姿态中提取三角形网格作为下一阶段的初始网格；</p><p>（4）：引入了一种方法来纠正提取的粗糙网格的边界和大小偏差；</p><p>（5）：采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种从单目视频中获取具有高分辨率基于物理的材质纹理和三角形网格的人体化身的方法，该方法在高保真度方面优于以往的表示，并且这种显式结果支持在通用渲染器上的部署。</p><p>（2）：创新点：提出了一种新颖的信息融合策略，将单目视频中的信息与合成的虚拟多视图图像相结合，以解决输入视图的稀疏性；引入了一种方法来纠正提取的粗糙网格的边界和大小偏差；采用了多视图超分辨率中潜在扩散模型的先验知识来提取分解的纹理。性能：在人体建模任务上，该方法在高保真度方面优于以往的表示。工作量：该方法需要合成虚拟多视图图像，这可能会增加计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details><h2 id="Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems"><a href="#Deep-Data-Consistency-a-Fast-and-Robust-Diffusion-Model-based-Solver-for-Inverse-Problems" class="headerlink" title="Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver   for Inverse Problems"></a>Deep Data Consistency: a Fast and Robust Diffusion Model-based Solver   for Inverse Problems</h2><p><strong>Authors:Hanyu Chen, Zhixiu Hao, Liying Xiao</strong></p><p>Diffusion models have become a successful approach for solving various image inverse problems by providing a powerful diffusion prior. Many studies tried to combine the measurement into diffusion by score function replacement, matrix decomposition, or optimization algorithms, but it is hard to balance the data consistency and realness. The slow sampling speed is also a main obstacle to its wide application. To address the challenges, we propose Deep Data Consistency (DDC) to update the data consistency step with a deep learning model when solving inverse problems with diffusion models. By analyzing existing methods, the variational bound training objective is used to maximize the conditional posterior and reduce its impact on the diffusion process. In comparison with state-of-the-art methods in linear and non-linear tasks, DDC demonstrates its outstanding performance of both similarity and realness metrics in generating high-quality solutions with only 5 inference steps in 0.77 seconds on average. In addition, the robustness of DDC is well illustrated in the experiments across datasets, with large noise and the capacity to solve multiple tasks in only one pre-trained model. </p><p><a href="http://arxiv.org/abs/2405.10748v1">PDF</a> Codes: <a href="https://github.com/Hanyu-Chen373/DeepDataConsistency">https://github.com/Hanyu-Chen373/DeepDataConsistency</a></p><p><strong>Summary:</strong><br>深度数据一致性通过深度学习模型更新数据一致性步骤，解决了扩散模型求解逆问题的挑战，展现了卓越的相似性和真实性表现。</p><p><strong>Key Takeaways:</strong></p><ul><li>提出深度数据一致性 (DDC) 方法，将数据一致性步骤用深度学习模型更新。</li><li>使用变分界训练目标，最大化条件后验，减少其对扩散过程的影响。</li><li>在线性和非线性任务中，DDC 在相似性和真实性指标上表现优异。</li><li>DDC 仅需 5 步推理，平均耗时 0.77 秒，生成高质量的解决方案。</li><li>DDC 在不同数据集、大噪声条件下表现稳健。</li><li>DDC 可以用一个预训练模型解决多个任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 深度数据一致性：一种快速且鲁棒的扩散模型求解逆问题的模型</p></li><li><p>Authors: 陈瀚宇，郝志修，肖丽英</p></li><li><p>Affiliation: 清华大学</p></li><li><p>Keywords: 扩散模型，逆问题，数据一致性，真实性</p></li><li><p>Urls: https://arxiv.org/abs/2405.10748v1, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):扩散模型在解决图像逆问题方面取得了成功，但如何平衡数据一致性和真实性是一个挑战。</p><p>(2):现有方法包括替换得分函数、分解矩阵或使用优化算法，但它们在数据一致性和真实性之间难以平衡，且推理速度慢。</p><p>(3):本文提出深度数据一致性（DDC），使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响。</p><p>(4):在图像超分辨率、修复、去模糊和JPEG恢复等任务上，DDC在仅需5个推理步骤且平均耗时0.77秒的情况下，在相似性和真实性指标上均取得了优异的性能，证明了其在平衡数据一致性和真实性方面的有效性。此外，DDC在不同数据集、大噪声和单一预训练模型解决多任务方面的鲁棒性也得到了证明。</p><ol><li>方法：</li></ol><p>（1）提出深度数据一致性（DDC），使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响；</p><p>（2）利用神经网络拟合数据一致性项，并将其融入到扩散模型中，使得模型能够在保留数据一致性的同时，生成更真实的图像；</p><p>（3）在训练过程中，通过变分界训练目标最大化条件后验概率，使得模型能够专注于生成与条件数据一致的真实图像；</p><p>（4）通过减少数据一致性项对扩散过程的影响，使得模型能够在推理过程中快速生成图像，同时保持较高的真实性。</p><ol><li>结论：</li></ol><p>（1）本文提出的深度数据一致性（DDC）方法，在平衡数据一致性和真实性的同时，实现了快速推理，为扩散模型求解逆问题提供了新的思路和方法。</p><p>（2）创新点：提出深度数据一致性（DDC）方法，使用神经网络更新扩散模型中的数据一致性步骤，通过变分界训练目标最大化条件后验概率，并减少其对扩散过程的影响。</p><p>性能：在图像超分辨率、修复、去模糊和 JPEG 恢复等任务上，DDC 在仅需 5 个推理步骤且平均耗时 0.77 秒的情况下，在相似性和真实性指标上均取得了优异的性能。</p><p>工作量：DDC 的训练过程需要使用神经网络拟合数据一致性项，并将其融入到扩散模型中，这可能会增加训练时间和计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-825b9ef49219bfe90e547c36af6ae92e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8f3559fc7f4e16bd5efc45f3e874012.jpg" align="middle"><img src="https://pica.zhimg.com/v2-833585aeca5f9fcecaa196677353c9fe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c582c82243d4b2484dbc714bdede51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-321a696bd3140a7780176a7ef30ec4fe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-58f99fcf80754e3e0aae1cad41d5cfeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-22  Diffusion-RSCC Diffusion Probabilistic Model for Change Captioning in   Remote Sensing Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/22/Paper/2024-05-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-22T04:00:21.000Z</published>
    <updated>2024-05-22T04:00:21.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-22-更新"><a href="#2024-05-22-更新" class="headerlink" title="2024-05-22 更新"></a>2024-05-22 更新</h1><h2 id="GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar"><a href="#GGAvatar-Geometric-Adjustment-of-Gaussian-Head-Avatar" class="headerlink" title="GGAvatar: Geometric Adjustment of Gaussian Head Avatar"></a>GGAvatar: Geometric Adjustment of Gaussian Head Avatar</h2><p><strong>Authors:Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</strong></p><p>We propose GGAvatar, a novel 3D avatar representation designed to robustly model dynamic head avatars with complex identities and deformations. GGAvatar employs a coarse-to-fine structure, featuring two core modules: Neutral Gaussian Initialization Module and Geometry Morph Adjuster. Neutral Gaussian Initialization Module pairs Gaussian primitives with deformable triangular meshes, employing an adaptive density control strategy to model the geometric structure of the target subject with neutral expressions. Geometry Morph Adjuster introduces deformation bases for each Gaussian in global space, creating fine-grained low-dimensional representations of deformation behaviors to address the Linear Blend Skinning formula’s limitations effectively. Extensive experiments show that GGAvatar can produce high-fidelity renderings, outperforming state-of-the-art methods in visual quality and quantitative metrics. </p><p><a href="http://arxiv.org/abs/2405.11993v1">PDF</a> 9 pages, 5 figures</p><p><strong>Summary</strong><br>GGAvatar，一种新颖的3D虚拟形象表示，旨在稳健地塑造带有复杂特征和形变的动态头部虚拟形象。</p><p><strong>Key Takeaways</strong></p><ul><li>GGAvatar 采用了粗到细的结构，包括两个核心模块：中性高斯初始化模块和几何变形调节器。</li><li>中性高斯初始化模块将高斯基本体与可变形的三角形网格配对，采用自适应密度控制策略来模拟目标对象在中性表情下的几何结构。</li><li>几何变形调节器为全局空间中的每个高斯体引入形变基础，创建了细粒度、低维的形变行为表示，有效解决了线性混合蒙皮配方的局限性。</li><li>大量实验表明，GGAvatar 可以产生高保真渲染，在视觉质量和定量指标上优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：GGAvatar：高斯头部头像的几何调整</p></li><li><p>作者：Xinyang Li, Jiaxin Wang, Yixin Xuan, Gongxin Yao, Yu Pan</p></li><li><p>单位：浙江大学</p></li><li><p>关键词：3D Avatar、Gaussian Primitives、Geometric Deformation、Neutral Expression Initialization、Morph Adjuster</p></li><li><p>论文链接：https://arxiv.org/abs/2405.11993v1，Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：创建高保真数字头像对于元宇宙和各种应用至关重要，但将这些头像推广到未见过的姿势或表情仍然是一个挑战。</p><p>（2）：过去方法：3DMM技术或神经隐式表示，但前者缺乏结构灵活性，后者训练和渲染效率低。</p><p>（3）：研究方法：提出GGAvatar，一种新的3D头像表示，它采用粗到细的结构，包括中性高斯初始化模块和几何变形调整器，分别用于对目标对象的中性表情几何结构建模和引入变形基以有效解决线性混合蒙皮公式的局限性。</p><p>（4）：任务和性能：在高保真渲染任务上，GGAvatar优于最先进的方法，在视觉质量和定量指标上都取得了更好的性能，支持其目标。</p><ol><li>方法：</li></ol><p>（1）：利用离散高斯基元（Gaussian primitives）对目标对象的中性表情几何结构建模，初始化3D头像；</p><p>（2）：引入变形基，有效解决线性混合蒙皮公式（Linear Blending Skinning）的局限性，实现精细变形；</p><p>（3）：通过多分辨率三平面存储头部周围的高频空间信息，学习额外的基，实现高精度变形；</p><p>（4）：使用L1损失、D-SSIM损失和感知损失监督训练，并添加位置和缩放正则化项，保证高斯基元的合理性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种新颖的3D头像表示GGAvatar，解决了现有方法在未见过的姿势或表情下的推广问题，为元宇宙和各种应用中的高保真数字头像创建提供了新的途径。</p><p>（2）：创新点：提出了一种粗到细的结构，包括中性高斯初始化模块和几何变形调整器，分别用于对目标对象的中性表情几何结构建模和引入变形基以有效解决线性混合蒙皮公式的局限性。性能：在高保真渲染任务上优于最先进的方法，在视觉质量和定量指标上都取得了更好的性能。工作量：需要离散高斯基元对目标对象的中性表情几何结构建模，引入变形基，学习额外的基，并使用L1损失、D-SSIM损失和感知损失监督训练，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d0b37297e18948031e40fa8e18788ee1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c6d3d44ffb48165e82767bbe3166494f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c5178d3e1bae75cb38fb1b04f261f7cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9b80139053ee885b883063414b7490a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d72be260f2e841855af340127b381ef0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45a63ba45015397a4354d20b5e428a1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d8ef253cd8bfe45e75b2c2ecc8dd03f.jpg" align="middle"></details>## Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion**Authors:Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao**In recent years, there has been significant interest in creating 3D avatars and motions, driven by their diverse applications in areas like film-making, video games, AR/VR, and human-robot interaction. However, current efforts primarily concentrate on either generating the 3D avatar mesh alone or producing motion sequences, with integrating these two aspects proving to be a persistent challenge. Additionally, while avatar and motion generation predominantly target humans, extending these techniques to animals remains a significant challenge due to inadequate training data and methods. To bridge these gaps, our paper presents three key contributions. Firstly, we proposed a novel agent-based approach named Motion Avatar, which allows for the automatic generation of high-quality customizable human and animal avatars with motions through text queries. The method significantly advanced the progress in dynamic 3D character generation. Secondly, we introduced a LLM planner that coordinates both motion and avatar generation, which transforms a discriminative planning into a customizable Q&amp;A fashion. Lastly, we presented an animal motion dataset named Zoo-300K, comprising approximately 300,000 text-motion pairs across 65 animal categories and its building pipeline ZooGen, which serves as a valuable resource for the community. See project website https://steve-zeyu-zhang.github.io/MotionAvatar/ [PDF](http://arxiv.org/abs/2405.11286v1) **Summary**3D角色生成迈入新阶段， Motion Avatar实现海量动画数据动态生成，助推动物3D动画生成技术落地。**Key Takeaways**- 提出 Motion Avatar，实现人类及动物3D高精可定制化形象生成与动作驱动。- LLM规划器实现动作与形象生成协调统一，将生成式任务转换为可定制化问答交互。- Zoo-300K 动物动作数据集包含约 30 万个跨 65 个动物种类的文本-动作对，极大丰富动物动画数据。- ZooGen 数据集生成管道为动物动画生成任务提供宝贵的数据资源。- Motion Avatar显著提升动态3D角色生成效率。- 该技术为电影制作、游戏、AR/VR、人机交互等领域提供强大助力。- 动物3D动画生成技术的落地有望突破数据和方法瓶颈。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 动作化身：生成具有任意动作的人类和动物化身</p></li><li><p>Authors: Zeyu Zhang, Yiran Wang, Biao Wu, Shuo Chen, Zhiyuan Zhang, Shiya Huang, Wenbo Zhang, Meng Fang, Ling Chen, Yang Zhao</p></li><li><p>Affiliation: 澳大利亚国立大学</p></li><li><p>Keywords: Motion Avatar, LLM planner, Zoo-300K, Animal motion dataset</p></li><li><p>Urls: https://steve-zeyu-zhang.github.io/MotionAvatar, Github: https://github.com/steve-zeyu-zhang/MotionAvatar</p></li><li><p>Summary:</p></li></ol><p>(1): 近年来，人们对创建 3D 化身和动作产生了浓厚的兴趣，这得益于它们在电影制作、视频游戏、AR/VR 和人机交互等领域的广泛应用。然而，目前的研究主要集中在生成 3D 化身网格或产生动作序列，而将这两个方面集成在一起仍然是一个持续的挑战。此外，虽然化身和动作生成主要针对人类，但由于缺乏训练数据和方法，将这些技术扩展到动物仍然是一个重大挑战。</p><p>(2): 过去的方法要么专注于生成 3D 化身网格，要么专注于生成动作序列，但将这两个方面集成在一起仍然是一个挑战。此外，现有的方法主要针对人类，而将这些技术扩展到动物仍然存在困难。</p><p>(3): 本文提出了一种名为 Motion Avatar 的基于代理的新方法，它允许通过文本查询自动生成具有动作的高质量可定制人类和动物化身。该方法极大地推进了动态 3D 角色生成的进展。此外，我们引入了一个 LLM 规划器来协调动作和化身生成，它将判别规划转换为可定制的问答方式。最后，我们提出了一个名为 Zoo-300K 的动物动作数据集，其中包含大约 300,000 个跨越 65 个动物类别的文本-动作对及其构建管道 ZooGen，它为社区提供了宝贵的资源。</p><p>(4): 在生成具有任意动作的人类和动物化身方面，Motion Avatar 实现了最先进的性能。该方法在各种任务上都取得了出色的效果，包括动作生成、化身生成和动作到化身的重定向。这些结果证明了该方法在创建逼真且可定制的 3D 角色方面的潜力。</p><ol><li>方法：</li></ol><p>（1）：基于代理的Motion Avatar方法，通过文本查询自动生成具有动作的高质量可定制人类和动物化身；</p><p>（2）：LLM规划器协调动作和化身生成，将判别规划转换为可定制的问答方式；</p><p>（3）：Zoo-300K动物动作数据集包含约300,000个跨越65个动物类别的文本-动作对；</p><p>（4）：Motion Avatar在生成具有任意动作的人类和动物化身方面实现了最先进的性能，在动作生成、化身生成和动作到化身的重定向等各种任务上取得了出色的效果。</p><ol><li>结论：</li></ol><p>（1）：本研究通过提出创新的解决方案，解决了动态 3D 化身生成中的持续挑战。我们引入了 Motion Avatar，一种新颖的基于代理的方法，它使我们能够根据文本查询自动创建具有动态动作的高质量可定制人类和动物化身，从而极大地推进了该领域的发展。此外，我们的 LLM 规划器促进了动作和化身生成的协调，增强了动态化身任务中的适应性和可用性。此外，Zoo-300K 数据集的开发以及 ZooGen 管道为研究人员提供了宝贵的资源，强调了我们致力于推进跨各种领域的动态化身生成。</p><p>（2）：创新点：提出 Motion Avatar 方法，实现了动作和化身生成的一体化，并引入了 LLM 规划器，增强了适应性和可用性。性能：在动作生成、化身生成和动作到化身的重定向等各种任务上取得了最先进的性能。工作量：需要收集和标注大量的数据，并且训练过程可能需要大量的时间和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b4943be56cf94e02149b50eec541d27f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4d4b6d7293dfe420ebcd255a83e215e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-178d2f15bc2899950bcba3dc7e32fcaa.jpg" align="middle"></details>## HR Human: Modeling Human Avatars with Triangular Mesh and   High-Resolution Textures from Videos**Authors:Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo**Recently, implicit neural representation has been widely used to generate animatable human avatars. However, the materials and geometry of those representations are coupled in the neural network and hard to edit, which hinders their application in traditional graphics engines. We present a framework for acquiring human avatars that are attached with high-resolution physically-based material textures and triangular mesh from monocular video. Our method introduces a novel information fusion strategy to combine the information from the monocular video and synthesize virtual multi-view images to tackle the sparsity of the input view. We reconstruct humans as deformable neural implicit surfaces and extract triangle mesh in a well-behaved pose as the initial mesh of the next stage. In addition, we introduce an approach to correct the bias for the boundary and size of the coarse mesh extracted. Finally, we adapt prior knowledge of the latent diffusion model at super-resolution in multi-view to distill the decomposed texture. Experiments show that our approach outperforms previous representations in terms of high fidelity, and this explicit result supports deployment on common renderers. [PDF](http://arxiv.org/abs/2405.11270v1) **Summary**利用单目视频，实现带有高分辨率物理材质纹理和三角形网格的可动画人的获取。**Key Takeaways**- 创新性信息融合策略，结合单目视频信息，合成虚拟多视角图像，解决输入视角稀疏问题。- 将人物重建为可变形神经隐式曲面，并提取行为良好的姿势中的三角形网格作为下一阶段的初始网格。- 提出一种方法来纠正粗糙网格边界和尺寸的偏差。- 采用多视角超分辨率潜扩散模型的先验知识，蒸馏分解纹理。- 实验表明，该方法在高保真方面优于之前的表示，显式结果支持在常见渲染器上部署。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: HR Human: 利用三角形网格和高分辨率纹理从视频中建模人类化身</p></li><li><p>Authors: Qifeng Chen, Rengan Xie, Kai Huang, Qi Wang, Wenting Zheng, Rong Li, Yuchi Huo</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: 人体建模、渲染、材质纹理</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.11270.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 近期，隐式神经表示已被广泛用于生成可动画的人类化身。然而，这些表示中的材质和几何形状在神经网络中耦合，难以编辑，这阻碍了它们在传统图形引擎中的应用。</p><p>(2): 现有方法：   - 隐式神经表示：几何和材质耦合，难以编辑。   - Relighting4D 和 Relightavatar：尝试用隐式表示恢复具有分离几何和材质的人类化身，但隐式几何和纹理难以编辑，且纹理清晰度较低。   - Nvdiffrec：专注于重建显式的一般静态对象，但无法处理非刚性物体和皮肤的动态运动。</p><p>(3): 本文提出了一种从单目视频中获取附有高分辨率基于物理的材质纹理和三角形网格的人类化身的方法。该方法引入了信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性。将人类重建为可变形神经隐式曲面，并提取良好姿势下的三角形网格作为下一阶段的初始网格。此外，还引入了一种方法来校正提取的粗网格的边界和尺寸偏差。最后，将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><p>(4): 本文方法在高保真度方面优于以往的表示，显式结果支持在常见渲染器上部署。</p><ol><li>方法：</li></ol><p>（1）：提出了一种从单目视频中获取附有高分辨率基于物理的材质纹理和三角形网格的人类化身的方法；</p><p>（2）：引入信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性；</p><p>（3）：将人类重建为可变形神经隐式曲面，并提取良好姿势下的三角形网格作为下一阶段的初始网格；</p><p>（4）：引入了一种方法来校正提取的粗网格的边界和尺寸偏差；</p><p>（5）：将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><ol><li>结论：</li></ol><p>（1）本文提出的 HR Human 框架的意义在于，它能够从单目视频中重建出带有三角形网格和对应的 PBR 材质纹理的数字化身。我们引入了一种新颖的信息融合策略，将单目视频的信息与合成的虚拟多视图图像相结合，以弥补缺失的空间视图信息。此外，我们还修正了从隐式场中提取的网格的边界和尺寸偏差。最后，我们引入了一个经过预训练的潜在扩散模型，用于分解纹理。</p><p>（2）创新点：本文提出了一种从单目视频中重建带有三角形网格和 PBR 材质纹理的人类化身的方法，该方法具有以下创新点：   - 引入信息融合策略，结合单目视频信息和合成虚拟多视图图像，解决了输入视图的稀疏性问题。   - 提出了一种方法来校正提取的粗网格的边界和尺寸偏差。   - 将多视图超分辨率中潜在扩散模型的先验知识用于分解纹理。</p><p>性能：本文方法在高保真度方面优于以往的表示，显式结果支持在常见渲染器上部署。</p><p>工作量：本文方法的工作量相对较大，需要训练一个潜在扩散模型来分解纹理。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-58f6be0321d44679e674675890fa61f4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d120932cc8da35e36223e213bf08ff48.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49edd99763ea96c13881d786d9a42af.jpg" align="middle"></details>## LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer**Authors:Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu**Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What's worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html. [PDF](http://arxiv.org/abs/2405.07319v1) SIGGRAPH 2024 conference track**Summary**多层高斯化身（LayGA）：一种用于从多视角视频中进行逼真可动画服装转移的新型表示，它将身体和服装表述为两个独立的层。**Key Takeaways**- 提出了一种名为 Layered Gaussian Avatars (LayGA) 的新表示，它将身体和服装表述为可动画服装转移的两个独立层。- LayGA 基于以高斯图为基础的化身，因其对服装细节的出色表示能力。- 使用了一系列几何约束来重建光滑表面，并同时获得了身体和服装之间的分割。- 训练了两个独立模型来表示身体和服装，并利用重建的服装几何作为更准确的服装跟踪的 3D 监督。- 提出了一种几何和渲染层，用于高质量的几何重建和高保真的渲染。- 总的来说，提出的 LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Layered Gaussian Avatars for Animatable Clothing (基于分层高斯分布的动画服装化身)</p></li><li><p>Authors: Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu</p></li><li><p>Affiliation: 清华大学</p></li><li><p>Keywords: Animatable avatar, clothing transfer, human reconstruction</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.07319.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 动画服装传输旨在跨角色穿衣和动画服装，是一个具有挑战性的问题。大多数人体化身工作将人体和服装的表示纠缠在一起，这给不同身份的虚拟试穿带来了困难。更糟糕的是，纠缠的表示通常无法准确跟踪服装的滑动运动。</p><p>(2): 过去的方法将身体和服装表示为一个整体，这使得虚拟试穿跨身份困难，并且无法准确跟踪服装的滑动运动。</p><p>(3): 本文提出了一种新的表示形式——分层高斯化身 (LayGA)，它将身体和服装表述为两个独立的层，用于从多视图视频中进行逼真的动画服装传输。LayGA 在单层重建阶段提出了一系列几何约束，以重建平滑曲面并同时获得身体和服装之间的分割。在多层拟合阶段，训练了两个独立的模型来表示身体和服装，并利用重建的服装几何作为 3D 监督，以实现更准确的服装跟踪。此外，还提出了几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><p>(4): 在服装转移任务上，LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法。</p><ol><li>方法：</li></ol><p>（1）：提出分层高斯化身（LayGA）表示，将身体和服装表示为两个独立的层，用于从多视图视频中进行逼真的动画服装传输。</p><p>（2）：在单层重建阶段，提出了一系列几何约束，以重建平滑曲面并同时获得身体和服装之间的分割。</p><p>（3）：在多层拟合阶段，训练了两个独立的模型来表示身体和服装，并利用重建的服装几何作为3D监督，以实现更准确的服装跟踪。</p><p>（4）：此外，还提出了几何层和渲染层，用于高质量的几何重建和高保真渲染。</p><ol><li>结论：<pre><code>            (1):该工作提出了分层高斯化身（LayGA）表示，用于从多视图视频中进行逼真的动画服装传输。LayGA 将身体和服装表示为两个独立的层，并提出了一系列几何约束和多层拟合策略，以实现更准确的服装跟踪和高质量的几何重建。            (2):创新点：提出了 LayGA 表示，将身体和服装表示为两个独立的层，并提出了几何约束和多层拟合策略，以实现更准确的服装跟踪和高质量的几何重建；            性能：在服装转移任务上，LayGA 实现了逼真的动画和虚拟试穿，并且优于其他基线方法；            工作量：该方法需要多视图视频输入，并且训练过程需要大量的数据和计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cbea179fd85983d0e759d4be018fb59a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68aa80c2ba44dfde97867ba03ebc2814.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80f17e9e8af3606ee233b1b0ca1da60c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-923a67fcbe4586a3709ea7a21a673f85.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-22  GGAvatar Geometric Adjustment of Gaussian Head Avatar</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/</id>
    <published>2024-05-13T08:45:28.000Z</published>
    <updated>2024-05-13T08:45:28.621Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>一键图像生成可编辑动态3D模型和视频，是图像到3D表示或图像3D重建研究领域的新方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>相比于原始神经辐射场，高斯溅射在隐式3D重建中表现出优势。</li><li>稳定扩散模型可用于根据文本指令生成目标模型。</li><li>传统的隐式机器学习方法难以获得精确的运动和动作控制。</li><li>难以生成长内容和语义连续的3D视频。</li><li>OneTo3D方法可使用单张图像生成可编辑3D模型和生成目标语义连续且时间无限的3D视频。</li><li>OneTo3D使用基本的高斯溅射模型从单张图像生成3D模型，减少了视频内存和计算机计算需求。</li><li>OneTo3D设计了自动生成和自适应绑定机制，用于对象骨架。</li><li>结合OneTo3D提出的可重新编辑的运动和动作分析与控制算法，在3D模型精确定位运动和动作控制以及根据输入文本指令生成稳定的语义连续且时间无限的3D视频方面，OneTo3D的性能优于该领域的SOTA项目。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 一张图像到可重新编辑的动态 3D 模型和视频生成</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: 澳大利亚莫纳什大学</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>(1): 3D 表征或 3D 重建长期以来一直是计算机视觉领域的研究难题。</p><p>(2): 现有的 3D 重建方法可分为显式方法和隐式方法。显式方法直接设计和完成 3D 重建或建模；隐式方法使用机器学习方法和理论实现这些目标。近年来，Neural Radiance Fields (NeRF) 在隐式 3D 表征或重建方面取得了突出成就。</p><p>(3): 本文提出 OneTo3D 方法，使用单张图像生成可编辑的 3D 模型和语义连续的 3D 视频。该方法使用基本的 Gaussian Splatting 模型从单张图像生成 3D 模型，并设计了一种自动生成和自适应绑定机制来绑定对象骨架。结合重新编辑的运动和动作分析与控制算法，OneTo3D 在 3D 模型精确运动和动作控制以及生成稳定的语义连续无时间限制的 3D 视频方面取得了优于现有方法的性能。</p><p>(4): 本文方法在任务和性能上取得了以下成就：使用单张图像生成可编辑的 3D 模型；生成语义连续的 3D 视频；精确控制 3D 模型的运动和动作。这些性能支持了本文的目标，即实现从单张图像到可重新编辑的动态 3D 模型和视频的生成。</p></li><li><p>方法：</p><pre><code>            (1):生成初始3D模型，使用DreamGaussian模型和Zero-1-to-3方法；            (2):生成并绑定自适应骨架，设计基本骨架，分析初始3D模型的几何参数信息，微调骨架参数以使其适合对象的身体；            (3):文本到动作和动作，分析用户文本指令的命令意图，将命令转换为特定动作和骨架相对骨骼的修改数据，控制特定骨骼在Blender中实现相对运动；            (4):背景去除，使用Dreamgaussian的process.py脚本或其他方法，可选使用图像检测或语义分割机器学习方法；            (5):颜色分组去除背景，计算图像中每个主要颜色项的比例，将颜色值范围内的颜色项划分为不同的颜色组，去除配置比例范围内的颜色组。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本篇工作提出了一种从单张图像生成可编辑的动态 3D 模型和视频的方法，具有生成可编辑 3D 模型、生成语义连续的 3D 视频、精确控制 3D 模型的运动和动作等优点，在任务和性能上取得了创新。</p><p>（2）创新点：OneTo3D 方法首次实现了从单张图像到可重新编辑的动态 3D 模型和视频的生成；性能：OneTo3D 方法在生成 3D 模型的精度、视频的语义连续性、动作控制的精确性等方面优于现有方法；工作量：OneTo3D 方法的实现需要较大的计算资源和时间，需要进一步优化算法和设计。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale   Aerial Rendering**Authors:Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu**Recent progress in large-scale scene rendering has yielded Neural Radiance Fields (NeRF)-based models with an impressive ability to synthesize scenes across small objects and indoor scenes. Nevertheless, extending this idea to large-scale aerial rendering poses two critical problems. Firstly, a single NeRF cannot render the entire scene with high-precision for complex large-scale aerial datasets since the sampling range along each view ray is insufficient to cover buildings adequately. Secondly, traditional NeRFs are infeasible to train on one GPU to enable interactive fly-throughs for modeling massive images. Instead, existing methods typically separate the whole scene into multiple regions and train a NeRF on each region, which are unaccustomed to different flight trajectories and difficult to achieve fast rendering. To that end, we propose Aerial-NeRF with three innovative modifications for jointly adapting NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial partitioning and selection method based on drones' poses to adapt different flight trajectories; (2) Using similarity of poses instead of (expert) network for rendering speedup to determine which region a new viewpoint belongs to; (3) Developing an adaptive sampling approach for rendering performance improvement to cover the entire buildings at different heights. Extensive experiments have conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new state-of-the-art results have been achieved on two public large-scale aerial datasets and presented SCUTic dataset. Note that our model allows us to perform rendering over 4 times as fast as compared to multiple competitors. Our dataset, code, and model are publicly available at https://drliuqi.github.io/. [PDF](http://arxiv.org/abs/2405.06214v1) **Summary**针对大规模航拍场景，我们提出 Aerial-NeRF，它针对 NeRF 进行三项创新性修改，以联合实现 NeRF 在大规模航拍渲染中的自适应：自适应空间分区和选择方法、基于姿态相似性的快速渲染和自适应采样方法。**Key Takeaways**- 提出 Aerial-NeRF，针对大规模航拍场景对 NeRF 进行三项创新性修改。- 使用自适应空间分区和选择方法，根据无人机姿态自适应不同的飞行轨迹。- 使用姿态相似性代替（专家）网络进行渲染加速，以确定新视点属于哪个区域。- 开发自适应采样方法，以提高渲染性能，覆盖不同高度的整座建筑。- 大量实验验证了 Aerial-NeRF 的有效性和效率，并在两个公开的大规模航拍数据集和 SCUTic 数据集上取得了新的最先进结果。- 与多个竞争对手相比，我们的模型允许我们以超过 4 倍的速度进行渲染。- 我们模型、代码和数据集已公开获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 航拍NeRF：大规模航拍渲染的自适应空间划分和采样</p></li><li><p>Authors: Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu</p></li><li><p>Affiliation: 华南理工大学未来技术学院</p></li><li><p>Keywords: View synthesis, large-scale scene rendering, neural radiance fields, fast rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.06214 , https://github.com/drliuqi/Aerial-NeRF</p></li><li><p>Summary:</p><p>(1):NeRF模型在小物体和室内场景渲染中取得了成功，但将其扩展到航拍渲染中面临两个挑战：单个NeRF无法渲染大规模航拍数据集中的整个场景，传统NeRF无法在单个GPU上训练以实现交互式浏览。</p><p>(2):以往方法将场景划分为多个区域，并在每个区域训练一个NeRF，但这些方法无法适应不同的飞行轨迹，渲染速度也较慢。</p><p>(3):本文提出Aerial-NeRF，通过自适应空间划分和选择、基于姿态相似性确定区域归属、自适应采样等方法，解决了上述问题。</p><p>(4):Aerial-NeRF在两个公开大规模航拍数据集和一个自建数据集上取得了最优性能，渲染速度比其他方法快4倍以上。</p></li><li><p>方法：</p><p>（1）：自适应空间划分：根据航拍数据集的特征，提出了一种自适应空间划分方法，将大规模场景划分为多个小区域，每个区域使用一个NeRF进行渲染；</p><p>（2）：基于姿态相似性确定区域归属：设计了一种基于姿态相似性的区域归属确定算法，根据相机的姿态信息将航拍图像分配到不同的区域；</p><p>（3）：自适应采样：提出了一种自适应采样算法，根据不同区域的复杂程度和渲染速度要求，动态调整采样点数，提高渲染效率；</p><p>（4）：基于神经网络的区域融合：使用神经网络将不同区域的渲染结果融合成最终图像，保证渲染结果的连续性和准确性；</p><p>（5）：基于概率密度函数的加速采样：利用概率密度函数对采样点进行优化，进一步提高渲染速度。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了 Aerial-NeRF，一种用于处理大规模航拍数据集的高效且鲁棒的渲染方法，在渲染速度上大幅优于现有的同类方法，几乎达到 4 倍。此外，在适当的划分区域数量下，Aerial-NeRF 可以使用单个 GPU 渲染任意大的场景。同时，我们为航拍场景引入了一种新颖的采样策略，该策略能够通过不同高度相机的采样范围覆盖建筑物。与 SOTA 模型进行更广泛的比较时，我们的方法明显更有效（仅使用 1/4 的采样点和 2 GB 的 GPU 内存节省），并且在多个常用指标方面具有可比性。最后，我们提出了 SCUTic，这是一个用于大规模大学校园场景的新型航拍数据集，具有不均匀的相机轨迹，可以验证渲染方法的鲁棒性。</p><p>（2）：创新点：自适应空间划分和采样；性能：渲染速度快，内存占用低；工作量：数据集构建和模型训练复杂度较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3f9706ee7489efbc0fffd098a133920f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5cd3300322f846160033228b8f55d45.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7172b9e2d3611b5ec9915962744d54fa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07281c002ad9f4eaef4b0c58ebbaf426.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a2778deaeb7d026e2fd79cf4c5e6e409.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d55c494ec8f956acc30da13f5d75881b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac7e04c419fb74632e6c7f9332f81960.jpg" align="middle"></details>## Residual-NeRF: Learning Residual NeRFs for Transparent Object   Manipulation**Authors:Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski**Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io [PDF](http://arxiv.org/abs/2405.06181v1) **Summary**透明物体在工业、医药和家庭中无处不在，机器人在抓取和操作这些物体时面临重大挑战。**Key Takeaways**- NeRFs 对包含透明物体的场景中的深度感知效果很好。- NeRFs 在处理极具挑战性的透明物体和光照条件时仍然存在困难。- Residual-NeRF 提出了一种改善透明物体深度感知和训练速度的方法。- 首先学习场景中不包含待操作透明物体的背景 NeRF，可以减少学习新物体变化带来的歧义。- Residual-NeRF 学习推断残差 RGB 值和密度，Mixnet 学习如何组合背景和残差 NeRF。- 在合成数据上的结果表明，Residual-NeRF 的 RMSE 低 46.1%，MAE 低 29.5%。- 真实世界的定性实验表明，Residual-NeRF 能够生成更鲁棒的深度图，噪声更少，孔洞更少。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：Residual-NeRF：学习残差 NeRF 以实现透明物体操作</p></li><li><p>作者：Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski</p></li><li><p>第一作者单位：卡内基梅隆大学机器人研究所</p></li><li><p>关键词：神经辐射场（NeRF）、深度感知、透明物体、残差学习、背景先验</p></li><li><p>论文链接：https://arxiv.org/abs/2405.06181   Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：      透明物体在工业、医药和家庭中无处不在。抓取和操纵这些物体对机器人来说是一个重大挑战。现有的方法难以重建具有挑战性的透明物体的完整深度图，从而在深度重建中留下孔洞。最近的研究表明，神经辐射场（NeRF）非常适合在有透明物体的场景中进行深度感知，并且这些深度图可用于高精度地抓取透明物体。基于 NeRF 的深度重建仍然难以处理特别具有挑战性的透明物体和照明条件。</p><p>(2)：过去的方法和问题：      Dex-NeRF 和 Evo-NeRF 等方法表明，NeRF 在透明物体的深度感知方面是有效的。然而，这些方法还表明，NeRF 往往难以处理特别具有挑战性的透明物体，例如具有挑战性光照条件的酒杯或厨房锡箔。透明物体的挑战源于缺乏特征以及外观中很大的视点依赖性变化。</p><p>(3)：本文提出的研究方法：      为了提高透明物体的深度感知并加快训练速度，我们提出了 Residual-NeRF。在许多情况下，机器人的工作区域的几何形状主要是静态且不透明的，例如架子、桌子和桌子。Residual-NeRF 利用场景的静态和不透明部分作为先验，以减少歧义并提高深度感知。Residual-NeRF 首先通过训练不包含透明物体的图像来学习整个场景的背景 NeRF。然后，Residual-NeRF 使用包含透明物体的完整场景的图像来学习残差 NeRF 和 Mixnet。</p><p>(4)：方法的应用任务和性能：      我们对合成和真实数据进行了实验，表明 Residual-NeRF 提高了透明物体的深度感知。合成数据上的结果表明，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。真实世界的定性实验表明，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。</p><ol><li>方法：</li></ol><p>（1）：首先训练不包含透明物体的图像，学习整个场景的背景 NeRF；</p><p>（2）：然后使用包含透明物体的完整场景的图像，学习残差 NeRF 和 Mixnet；</p><p>（3）：利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知；</p><p>（4）：Residual-NeRF 提高了透明物体的深度感知。</p><p><strong>结论</strong></p><p>(1): 本工作通过提出 Residual-NeRF，提高了透明物体的深度感知，并加快了训练速度。</p><p>(2): 创新点：    - 利用场景的静态和不透明部分作为先验，减少歧义并提高深度感知；    - 提出了一种两阶段训练方法，首先学习背景 NeRF，然后学习残差 NeRF 和 Mixnet。    - 提出了一种新的 Mixnet，可以有效地融合背景 NeRF 和残差 NeRF 的输出。</p><pre><code>性能：- 在合成数据上，Residual-NeRF 在 RMSE 上比基线低 46.1%，在 MAE 上低 29.5%。- 在真实世界的定性实验中，Residual-NeRF 产生了更稳健的深度图，噪点更少，孔洞更少。工作量：- 训练 Residual-NeRF 需要两个阶段的训练，这比基线方法更复杂。- Residual-NeRF 需要额外的内存来存储背景 NeRF 和残差 NeRF 的权重。</code></pre><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b7d7618a421bd8b0947856c3ea91116f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1b6857403dd3f1eeafdb70f45e5b92e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4d0e4ee50f9ead394b9fcd552ae92106.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b39c7bfcb8005d9c40b2eac38f3ed56.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0aaa94e0b48b25617be15c8888555cae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7c646c3420664f87093b1eb3a62bfba.jpg" align="middle"></details>## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior**Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh**Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. [PDF](http://arxiv.org/abs/2405.05749v2) 11 pages, 5 figures**Summary**通过解决单张图像音频驱动3D Talking Head生成中的3D一致性问题，NeRFFaceSpeech 方法能够生成高质量 3D感知的 Talking Head。**Key Takeaways**- 解决单张图像音频驱动 3D Talking Head 生成的 3D 一致性问题。- 使用生成模型的先验知识与 NeRF 相结合，构建对应于单张图像的 3D 一致的面部特征空间。- 引入空间同步方法，利用参数化人脸模型的音频相关顶点动态，通过射线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。- 引入 LipaintNet 补充内嘴区域中缺失的信息，该信息无法从给定的单张图像中获得。- 以自监督的方式训练网络，利用生成能力而无需额外数据。- 提出一种定量方法来衡量模型对姿势变化的鲁棒性，这在以前只能通过定性方式进行。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: NeRFFaceSpeech: 一次性音频驱动 3D 说话人头部合成，通过生成先验</p></li><li><p>Authors: Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</p></li><li><p>Affiliation: 首尔国立大学</p></li><li><p>Keywords: 音频驱动, 3D 说话人头部, 神经辐射场, 生成先验, 一次性学习</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05749, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 音频驱动说话人头部生成正从 2D 内容转向 3D 内容。值得注意的是，神经辐射场 (NeRF) 作为合成高质量 3D 说话人头部输出的一种手段而备受关注。不幸的是，这种基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据，从而限制了该方法的可扩展性。尽管已经尝试使用单张图像生成音频驱动的 3D 说话人头部动画，但由于图像中遮挡区域的信息不足，结果通常不令人满意。在本文中，我们主要关注解决一次性音频驱动域中被忽视的 3D 一致性方面，其中面部动画主要在正面视角合成。</p><p>(2): 现有的方法：- 基于 NeRF 的方法通常需要大量配对的每个身份的音频视觉数据。- 使用单张图像生成音频驱动的 3D 说话人头部动画的方法由于图像中遮挡区域的信息不足，结果通常不令人满意。</p><p>问题：- 可扩展性受限。- 3D 一致性不足。</p><p>(3): 本文提出的研究方法：- 提出了一种新方法 NeRFFaceSpeech，它能够生成高质量的 3D 感知说话人头部。- 使用生成模型的先验知识与 NeRF 相结合，我们的方法可以构建一个与单张图像相对应的 3D 一致的面部特征空间。- 我们的空间同步方法采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动。- 此外，我们引入了 LipaintNet，它可以补充单张给定图像中无法获得的内口区域中缺少的信息。该网络以自监督的方式进行训练，利用生成能力而无需额外数据。</p><p>(4): 在任务和性能上，本文方法取得了以下成就：- 全面的实验表明，与以前的方法相比，我们的方法在从单张图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性。- 此外，我们首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。</p><ol><li>Methods:</li></ol><p>(1): 提出 NeRFFaceSpeech 方法，该方法结合了生成模型的先验知识与 NeRF，构建与单张图像相对应的 3D 一致的面部特征空间；</p><p>(2): 采用参数化人脸模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；</p><p>(3): 引入 LipaintNet，补充单张给定图像中无法获得的内口区域中缺少的信息，该网络以自监督的方式进行训练，利用生成能力而无需额外数据；</p><p>(4): 设计定量方法衡量模型对姿势变化的鲁棒性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRFFaceSpeech，一种通过利用生成先验构建和操纵 3D 特征，从单幅图像生成 3D 感知音频驱动说话人头部动画的新方法；</p><p>（2）：创新点：将生成模型先验与神经辐射场相结合，构建与单幅图像相对应的 3D 一致的面部特征空间；通过光线变形，采用参数化人脸模型的音频相关顶点动态，将静态图像特征转换为动态视觉效果，确保逼真的 3D 面部运动；引入了 LipaintNet，一个自监督学习框架，利用生成模型的能力合成隐藏的内口区域，补充变形场以产生可行结果；设计了定量方法来衡量模型对姿势变化的鲁棒性。性能：与以前的方法相比，我们的方法在从单幅图像生成音频驱动的说话人头部方面具有出色的性能，并增强了 3D 一致性；首次引入了一种衡量模型对姿势变化鲁棒性的定量方法，而以前只能定性地进行。工作量：本文提出的方法需要较大的计算资源和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details>## Tactile-Augmented Radiance Fields**Authors:Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens**We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF [PDF](http://arxiv.org/abs/2405.04534v1) CVPR 2024, Project page: https://dou-yiming.github.io/TaRF, Code:   https://github.com/Dou-Yiming/TaRF/**Summary**视觉触觉增强辐射场将视觉和触觉带入共享的 3D 空间，能够估计场景中给定 3D 位置的视觉和触觉信号。**Key Takeaways**- 视觉触觉增强辐射场 (TaRF) 结合了视觉和触觉信号，用于估计场景中给定 3D 位置的视觉和触觉信号。- TaRF 由照片和稀疏采样的触觉探针采集而成。- 视觉触觉增强辐射场利用了视觉触觉传感器可与图像配准以及场景中视觉和结构相似的区域共享相同触觉特征的见解。- 触觉信号通过配准和条件扩散模型与捕获的视觉场景相关联。- TaRF 数据集包含比以往真实世界数据集更多的触觉样本，并为每个捕获的触觉信号提供了空间对齐的视觉信号。- 跨模态生成模型的准确性已得到验证，且已在多项下游任务中证明了捕获的视觉触觉数据的实用性。- 项目主页：https://dou-yiming.github.io/TaRF**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 触觉增强辐射场：一种用于跨模态感知和生成的新型场景表示</p></li><li><p>Authors: Douyi Ming, Srinath Sridhar, Jiajun Wu, Angjoo Kanazawa, Peter Anderson, Wojciech Matusik, Jonathan Ragan-Kelley</p></li><li><p>Affiliation: 麻省理工学院</p></li><li><p>Keywords: Cross-modal perception, generative models, multi-view geometry, neural radiance fields, tactile sensing, vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2301.09422, Github: https://github.com/douyiming/TaRF</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是视觉和触觉感知在共享的 3D 空间中的表示问题。</p><p>(2): 过去的方法通常是将视觉和触觉信号视为独立的模态，这限制了跨模态感知和生成的任务。</p><p>(3): 本文提出了一种新的场景表示形式——触觉增强辐射场 (TaRF)，它将视觉和触觉信号统一到一个 3D 空间中。TaRF 的构建利用了基于视觉的触觉传感器与图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设。</p><p>(4): 在 TaRF 数据集上，本文提出的方法在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能，证明了其在跨模态感知和生成方面的有效性。</p><ol><li>方法：</li></ol><p>（1）：构建视觉和触觉增强辐射场（TaRF），将视觉和触觉信号统一到一个 3D 空间中；</p><p>（2）：通过视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF；</p><p>（3）：使用基于视觉的触觉传感器和图像之间的几何对应关系，估计 TaRF 中的视觉和触觉信号；</p><p>（4）：利用生成模型，估计场景中其他位置的触觉信号；</p><p>（5）：使用条件潜在扩散模型，从渲染的视觉信号中预测触觉信号；</p><p>（6）：收集包含 19.3k 个图像对的视觉-触觉数据集，用于训练和评估 TaRF。</p><ol><li>结论：<pre><code>            （1）：本文提出了一种新的场景表示形式——触觉增强辐射场（TaRF），首次将视觉和触觉信号统一到一个共享的 3D 空间中，为跨模态感知和生成提供了新的可能性。            （2）：创新点：提出了一种将视觉和触觉信号统一到一个 3D 空间中的场景表示形式 TaRF；提出了一种基于视觉-触觉对应关系和场景中视觉和结构相似区域具有相同触觉特征的假设，建立 TaRF 的方法；提出了一种使用生成模型，估计场景中其他位置的触觉信号的方法。性能：在跨模态生成、触觉信号估计和触觉引导的视觉探索等任务上取得了良好的性能；收集了包含 19.3k 个图像对的视觉-触觉数据集，为 TaRF 的训练和评估提供了丰富的素材。 workload：TaRF 的构建需要基于视觉的触觉传感器和图像之间的几何对应关系，以及场景中视觉和结构相似区域具有相同触觉特征的假设，这在某些情况下可能存在挑战；TaRF 的训练需要大量的视觉-触觉数据，这可能会增加数据收集和标注的工作量。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5322ab124785e1ed8207592748379b4a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a5022ff2c6b665ce2b96a8b7b9f166a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-929d0d52fcc6b94f08fe05a010b4ea04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-288d198e1ad4f685777631680ccf4209.jpg" align="middle"><img src="https://pica.zhimg.com/v2-59b2afe338ddf3888c68d0443ec0d04f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ec71c15419dcda442892e2bc1a105da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-432dac236eec2115922c4f0698f51eec.jpg" align="middle"></details><h2 id="DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><a href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid" class="headerlink" title="DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid"></a>DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid</h2><p><strong>Authors:Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</strong></p><p>Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively. </p><p><a href="http://arxiv.org/abs/2405.04416v2">PDF</a> Originally submitted to Siggraph Asia 2023</p><p><strong>Summary</strong><br>大规模场景只需用单一NeRF，通过多级Hash网格，而无需单独的背景NeRF，即可实现场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在对象和室内场景重建中表现出色，但在重建大型场景时存在问题。</li><li>基于 MLP 的 NeRF 网络容量有限，而基于体积的 NeRF 会随着场景分辨率的增加占用大量内存。</li><li>最近的方法将场景地理分区，并使用单独的 NeRF 学习每个分区。</li><li>这有助于基于体积的 NeRF 突破单 GPU 内存限制，并扩展到更大的场景。</li><li>但这种方法需要多个背景 NeRF 来处理分区外的光线，这导致学习冗余。</li><li>本文提出了 DistGrid，基于多级散列网格的分布式场景重建方法。</li><li>该方法将场景划分为多个紧密排列但不重叠的轴对齐包围盒，并提出了一种分割体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</li><li>实验表明，该方法在所有评估的大型场景上都优于现有方法，并提供了视觉上合理的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DistGrid: 基于分布式多分辨率哈希网格的大规模场景重建</p></li><li><p>Authors: Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</p></li><li><p>Affiliation: 国防科技大学</p></li><li><p>Keywords: Neural Radiance Field, Distributed Algorithm, Large-scale Scene Reconstruction, Neural Rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.04416.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): 神经渲染技术自 NeRF 提出以来取得了重大进展，NeRF 旨在解决新视角合成任务。它基于体积渲染，使用称为神经场的多分层感知器（MLP）隐式表示场景。该神经场接受 3D 坐标和观察方向作为输入，并预测其对应的密度和颜色。NeRF 在新视角合成任务上产生了令人印象深刻的视觉质量。其学习的神经场也可用于场景重建。</p><p>(2): NeRF 的后续工作旨在提高效率和质量。一种方法是将场景地理分区，并使用单独的 NeRF 学习每个子区域。这种分区策略帮助基于体积的 NeRF 超过单个 GPU 的内存限制，并扩展到更大的场景。但是，这种方法需要多个背景 NeRF 来处理分区外的光线，这导致了学习的冗余。</p><p>(3): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。在此方法中，场景被划分为多个紧密相邻但非重叠的轴对齐边界框，并提出了一种新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(4): 实验表明，该方法在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。该方法在重建质量上的可扩展性还通过定性和定量的方式进行了进一步评估。</p></li><li><p>方法：</p><p>(1): 本文提出了一种基于联合多分辨率哈希网格的可扩展场景重建方法，称为 DistGrid。</p><p>(2): DistGrid 将场景划分为多个紧密相邻但非重叠的轴对齐边界框 (AABB)，并使用新颖的分段体积渲染方法来处理跨边界光线，从而消除了对背景 NeRF 的需求。</p><p>(3): DistGrid 采用两级级联结构，其中细粒度 NeRF 使用内层边界框作为其边界框，而粗粒度 NeRF 使用外层边界框。</p><p>(4): DistGrid 使用分段体积渲染方法来处理跨区域光线，该方法将体积渲染积分分解为多个部分，并使用部分颜色和部分透射率来计算渲染颜色和最终透射率。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了 DistGrid，一种基于联合多分辨率哈希网格的可扩展场景重建方法，该方法在视觉质量和效率方面都优于现有方法。            (2):创新点：提出了一种新颖的分段体积渲染方法来处理跨边界光线，无需背景 NeRF，提高了效率；采用两级级联结构，细粒度 NeRF 和粗粒度 NeRF 协同工作，提高了重建质量。            性能：在所有评估的大规模场景上都优于现有方法，并提供了视觉上合理的场景重建。            工作量：与现有方法相比，DistGrid 在重建质量上的可扩展性得到了定性和定量的方式的进一步评估。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c94130b609d19ed2e706304ecfbbdde4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c20db83d0a269f077a389b38e5b01349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e4dd6ce979c2f2e18008fc25e085162.jpg" align="middle"></details><h2 id="Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><a href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization" class="headerlink" title="Blending Distributed NeRFs with Tri-stage Robust Pose Optimization"></a>Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</h2><p><strong>Authors:Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at <a href="https://github.com/boilcy/Distributed-NeRF">https://github.com/boilcy/Distributed-NeRF</a>. </p><p><a href="http://arxiv.org/abs/2405.02880v1">PDF</a> </p><p><strong>Summary</strong><br>使用三阶段姿态优化对分布式NeRF进行精确对齐，以缓解建模大规模城市环境时出现的混叠伪影和姿态精度不足的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>分布式NeRF建模城市环境面临混叠伪影和姿态精度问题。</li><li>采用分阶段姿态优化解决问题，包括Mip-NeRF 360束调整、反向Mip-NeRF 360和Frame2Model优化。</li><li>利用Model2Model优化进一步细化不同NeRF之间的转换。</li><li>精确的姿态优化有效消除NeRF融合中的遮挡伪影。</li><li>在真实和模拟场景中展示出优越的NeRF融合性能。</li><li>代码和数据将在GitHub上公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于三阶段鲁棒位姿优化融合分布式NeRFs</p></li><li><p>作者：Baijun Ye∗1,2, Caiyun Liu∗1, Xiaoyu Ye1,2, Yuantao Chen1,3, Yuhai Wang4,Zike Yan1, Yongliang Shi1†, Hao Zhao1, Guyue Zhou1</p></li><li><p>第一作者单位：清华大学人工智能产业研究院（AIR）</p></li><li><p>关键词：分布式NeRF、位姿优化、NeRF融合、Mip-NeRF 360、iNeRF</p></li><li><p>论文链接：https://arxiv.org/abs/2405.02880Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：在大规模场景建模领域，NeRF因其能够在保持紧凑模型结构的同时实现逼真的渲染而备受关注。然而，当前的分布式NeRF配准方法存在混叠伪影，这源于渲染分辨率差异和次优位姿精度。这些因素共同降低了NeRF框架内位姿估计的保真度，导致NeRF融合阶段出现遮挡伪影。</p><p>（2）：以往方法及其问题：以往方法主要有两种：批处理学习和增量学习。批处理学习需要大量的计算资源，而增量学习容易出现遗忘问题。此外，当前使用显式编码方法（如网格和八叉树）进行实时性能的NeRF方法，面临着随着场景规模的增加，编码组件呈指数级扩展的挑战，导致存储需求大幅增加。</p><p>（3）：本文提出的研究方法：本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架。在第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿。在第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF。此方法类似于模糊图像以使优化过程更加鲁棒，从而实现帧到模型的位姿优化。随后，采用协视图区域检索方法来搜索不同NeRF实例中最相似的图像，进而确定其关联的位姿。给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换。在第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换。然后，将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><p>（4）：方法在什么任务上取得了怎样的性能：利用三阶段位姿优化，实现了NeRF融合并获得了更好的性能。为了验证本文方法，同时发布了真实世界和模拟数据集，展示了本文方法在性能上的优越性。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架；</p><p>（2）：第一阶段，通过捆绑调整Mip-NeRF 360并采用由粗到精的策略，实现了图像的精确位姿；</p><p>（3）：第二阶段，借鉴LATITUDE，利用截断动态低通滤波（TDLF）的原理对反向Mip-NeRF 360进行了优化，称为iMNeRF；</p><p>（4）：给定关联的位姿，利用iMNeRF通过渲染图像和观察图像之间的光度损失来优化这些位姿，从而获得可靠的帧到模型转换；</p><p>（5）：第三阶段，通过不同的帧到模型转换获得了NeRF之间粗略的模型到模型转换；</p><p>（6）：将不同的NeRF模型投影到一个统一的坐标系中，并使用渲染图像作为观测值进一步优化NeRF之间的相对转换，即通过模型到模型优化来获得NeRF之间的精确转换。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于三阶段鲁棒位姿优化的分布式NeRF框架，解决了当前分布式NeRF配准方法中存在的混叠伪影问题，提高了NeRF融合的保真度；</p><p>（2）：创新点：提出了三阶段鲁棒位姿优化方法，包括图像精确位姿估计、帧到模型位姿优化和模型到模型位姿优化；性能：在真实世界和模拟数据集上验证了本文方法的优越性；工作量：本文方法需要额外的计算资源进行位姿优化，但可以实现更好的NeRF融合效果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-19421ede11ee24694424a6e2329cbd82.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c3320829396c5cee81241227bf678e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f9d4086bd525c9621ef04e939f0ee92.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cce8d4e7f7b76ee03713ad33dc0da96e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f2f0b44124a450d85749c232dce8a310.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b718f4974ac6aac1d1a62a7beb2d681.jpg" align="middle"><img src="https://picx.zhimg.com/v2-767f74cc31852d4c6f28717ac5e03f47.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5929c8babaf3c740cf4c2d6f2886bf8e.jpg" align="middle"></details>## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for   Dynamic UAV-based Scenes**Authors:Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon**In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms. [PDF](http://arxiv.org/abs/2405.02762v1) 8 pages, submitted to IROS2024**Summary**无人机实时感知任务中，该文将静态特征与动态特征相结合，提高了神经辐射场（NeRF）模型在合成和真实世界数据之间的域适应性。**Key Takeaways**- 提出了一种分层特征向量的神经辐射场（NeRF）扩展版本，用于捕获动态场景中的概念信息。- 扩展的NeRF模型通过图像解码器将输出特征图转换为RGB图像。- 该模型同时利用静态和动态对象的信息，从而捕获高空视频中的显著场景属性。- 将静态和动态特征相结合，提高了模型在合成和真实世界数据之间的域适应性。- 在Okutama Action和UG2等具有挑战性的数据集上评估了该模型的性能。- 与最先进的无人机感知算法相比，该模型在准确性方面有显著提高。- 该模型可以应用于无人机实时感知任务，例如动作识别和姿态估计。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TK-Planes：具有高维特征向量的分层 K-Planes</p></li><li><p>Authors: Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</p></li><li><p>Affiliation: 美国陆军研究实验室</p></li><li><p>Keywords: Neural Radiance Fields, Synthetic Data, UAV Perception, Dynamic Scenes, Feature Vectors</p></li><li><p>Urls: https://arxiv.org/abs/2405.02762, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文研究背景是合成数据在无人机感知中的应用，特别是动态场景的识别，如姿势或动作识别。</p><p>(2): 过去的方法主要基于 K-Planes 神经辐射场，但存在问题：动态对象建模困难、静态和动态元素分离困难、动态对象稀疏、姿态多样性受限。</p><p>(3): 本文提出的研究方法是 TK-Planes，一种分层 K-Planes 算法，输出和操作特征向量而不是 RGB 像素值。这些特征向量可以存储场景中特定对象或位置的概念信息，并为多个相应的相机光线输出时形成特征图，然后解码为最终图像。</p><p>(4): 该方法在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上进行了评估，结果表明，与现有算法相比，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的整体识别准确性。</p><ol><li>方法：</li></ol><p>（1）：使用 NeRF（神经辐射场）在特征空间中生成新视角，以更好地捕获场景中的动态对象，如人物。</p><p>（2）：采用基于网格的 NeRF，网格类似于 K-Planes，但存储的特征向量不直接编码 RGB 值，而是编码场景中的更高层次概念信息，如地面、树木和人物。</p><p>（3）：使用分层网格在特征空间中操作，将场景分解为静态和动态特征，并使用图像解码器将特征图解码为最终图像。</p><p>（4）：在具有挑战性的无人机数据集上评估模型，结果表明基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性。</p><ol><li>结论：<pre><code>            (1):本文提出的分层 K-Planes（TK-Planes）算法，通过在特征空间中操作特征向量，有效地解决了合成数据在无人机感知中的动态场景识别问题。            (2):创新点：TK-Planes 算法将场景分解为静态和动态特征，并使用分层网格在特征空间中操作，从而更好地捕获动态对象；性能：在 Okutama Action 和 UG2 等具有挑战性的无人机数据集上的评估结果表明，基于 TK-Planes 的 NeRF 模型可以生成补充的无人机数据，从而提高动态场景的识别准确性；工作量：TK-Planes 算法的实现和在无人机数据集上的评估需要一定的技术投入和计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-49ebb6a345fe5bed0d70468dcdf8fd84.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-63139de16b603f02b54ce2804a9bad9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af95031ec70f26ba5024a7788a09ddff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c99084999876941f9618dfb5d99f367b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63f6561485ccf42232344f25cf43bc8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a606ade4cb1fcbf9e4da91086ed92ad1.jpg" align="middle"></details><h2 id="ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><a href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty" class="headerlink" title="ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty"></a>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty</h2><p><strong>Authors:Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</strong></p><p>Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance. </p><p><a href="http://arxiv.org/abs/2405.02568v1">PDF</a> </p><p><strong>Summary</strong><br>主动神经重建同时考虑图像渲染和几何不确定性，以选择信息丰富的训练视图来提高 3D 场景重建性能。</p><p><strong>Key Takeaways</strong></p><ul><li>主动学习在 3D 场景重建中至关重要。</li><li>NeRF 变体使用图像渲染或几何不确定性提高了主动 3D 重建的性能。</li><li>ActiveNeuS 同时考虑了图像渲染和几何不确定性来选择信息丰富的视图。</li><li>ActiveNeuS 积累图像渲染不确定性，同时避免估计密度引入的偏差。</li><li>ActiveNeuS 计算神经隐式表面不确定性，提供颜色不确定性和表面信息。</li><li>ActiveNeuS 使用表面信息和网格有效处理偏差，从而快速选择多样化的视点。</li><li>ActiveNeuS 在 Blender 和 DTU 数据集上的表现优于以往的工作，表明 ActiveNeuS 选择的视图显著提高了性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ActiveNeuS：基于神经隐式曲面不确定性的主动三维重建</li><p></p><p></p><li>作者：Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</li><p></p><p></p><li>单位：首尔大学</li><p></p><p></p><li>关键词：主动学习、神经隐式曲面不确定性、曲面网格</li><p></p><p></p><li>论文链接：https://arxiv.org/pdf/2405.02568.pdf，Github代码链接：无</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）研究背景：三维场景重建中的主动学习已被广泛研究，因为选择有信息的训练视图对于重建至关重要。最近，神经辐射场（NeRF）变体在使用图像渲染或几何不确定性进行主动三维重建方面表现出性能提升。然而，在选择信息视图时同时考虑两种不确定性仍然未被探索，而利用不同类型的不确定性可以减少在早期训练阶段由于输入稀疏而产生的偏差。</p><p>（2）过去的方法及其问题：传统的NeRF主动学习方法通常估计其输出中的不确定性：三维点的密度和颜色。Martin等人和Pan等人估计了颜色预测中的不确定性，方法是将颜色建模为高斯概率分布。然而，这些方法在早期训练阶段容易受到密度估计偏差的影响，这可能会导致信息视图选择不佳。</p><p>（3）提出的研究方法：本文提出了ActiveNeuS，它在评估候选视图时考虑了图像渲染不确定性和神经隐式曲面不确定性。ActiveNeuS提供了一种积累图像渲染不确定性的方法，同时避免了估计密度可能引入的偏差。ActiveNeuS计算神经隐式曲面不确定性，提供颜色不确定性和曲面信息。它通过使用曲面信息和网格有效地处理偏差，从而能够快速选择不同的视点。</p><p>（4）任务和性能：在流行的数据集Blender和DTU上，ActiveNeuS的性能优于以前的工作，表明ActiveNeuS选择的视图显著提高了性能。这些结果支持了作者的目标，即开发一种主动学习方法，该方法可以有效地选择信息视图以提高三维重建的性能。</p><ol><li><p>方法：</p><pre><code>            (1): ActiveNeuS 提出了一种新的采集函数，该函数结合了几何重建和图像渲染的视角。            (2): ActiveNeuS 估计颜色预测的不确定性，以获取有关图像渲染质量的信息。            (3): 采集函数集成了估计的不确定性，同时不丢失几何属性。            (4): 首先，在第 4.1 节中，我们介绍了我们的采集函数，并解释了在积分过程中如何考虑曲面。            (5): 在第 4.2 节中，我们定义了 ActiveNeuS 中估计的神经隐式表面不确定性，并描述了如何在采集函数中利用不确定性。            (6): 然后，为了进行高效且稳健的计算，我们引入了存储曲面信息的曲面网格和选择多个次优视图 (NBV) 的策略（第 4.3 节）。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种有效的信息视图选择方法 ActiveNeuS，该方法同时考虑了几何重建和图像渲染的保真度。ActiveNeuS 引入了一种新的采集函数，该函数利用不确定性网格有效且稳健地利用神经隐式曲面不确定性。采集函数通过使用曲面网格并根据曲面的存在应用不同的积分策略来计算神经隐式曲面不确定性的积分。我们展示了 ActiveNeuS 的下一个最佳视图选择，与其他方法相比，它改进了网格重建和图像渲染质量。对于未来的工作，我们建议研究一种有效的方法来连接不同网络的不确定性以进行信息视图选择。此外，将 ActiveNeuS 应用于机器人主动 3D 重建中也很有趣，其中机器人手臂移动并收集数据。</p><p>（2）：创新点：本文提出了一种新的采集函数，该函数同时考虑了图像渲染不确定性和神经隐式曲面不确定性，有效地提高了信息视图的选择。性能：在 Blender 和 DTU 等流行数据集上，ActiveNeuS 的性能优于以前的工作，表明 ActiveNeuS 选择的视图显着提高了性能。工作量：ActiveNeuS 的计算成本相对较高，因为它需要估计神经隐式曲面不确定性并使用曲面网格进行积分。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0eb6f5097fe2312cfce57f04da637606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-824537082b5290b565ea1f0da3946351.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7939474a53153965a29e673124ea4a9e.jpg" align="middle"></details><h2 id="Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><a href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids" class="headerlink" title="Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids"></a>Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids</h2><p><strong>Authors:Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</strong></p><p>Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times. </p><p><a href="http://arxiv.org/abs/2405.02386v1">PDF</a> SIGGRAPH 2024, Project page: <a href="https://junchenliu77.github.io/Rip-NeRF">https://junchenliu77.github.io/Rip-NeRF</a>   , Code: <a href="https://github.com/JunchenLiu77/Rip-NeRF">https://github.com/JunchenLiu77/Rip-NeRF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过极坐标投影将三维各向异性区域射影到平面，再利用Ripmap编码对各平面进行编码，进而解决NeRF抗锯齿渲染中的混叠和模糊问题。</p><p><strong>Key Takeaways</strong></p><ul><li>极坐标投影将三维各向异性区域射影到平面，便于特征化。</li><li>Ripmap编码通过各向异性预滤波可学习特征网格，对射影各向异性区域进行精确高效的特征化。</li><li>方法在合成数据集和实景数据集上均取得了最优渲染质量。</li><li>方法在重复结构和纹理的精细细节上表现优异。</li><li>方法训练时间相对较短。</li><li>该方法依赖于可学习特征网格。</li><li>该方法目前仅适用于静态场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p><strong>标题：</strong>Rip-NeRF：基于Ripmap编码的Platonic实体的反走样辐射场</p></li><li><p><strong>作者：</strong>Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</p></li><li><p><strong>第一作者单位：</strong>北京航空航天大学</p></li><li><p><strong>关键词：</strong>神经辐射场（NeRFs）、反走样、Ripmap编码、Platonic实体</p></li><li><p><strong>论文链接：</strong>https://arxiv.org/pdf/2405.02386.pdf，<strong>Github链接：</strong>None</p></li><li><p><strong>摘要：</strong></p><p>(1) <strong>研究背景：</strong>尽管神经辐射场（NeRFs）取得了重大进展，但其渲染结果仍可能存在走样和模糊伪影，因为有效且高效地表征锥形投射过程产生的各向异性区域仍然是一个基本挑战。</p><p>(2) <strong>过去的方法和问题：</strong>现有方法要么无法精确地表征各向异性区域，要么效率低下。</p><p>(3) <strong>本文提出的研究方法：</strong>本文提出了一种基于Ripmap编码的Platonic实体表示，用于精确高效地表征3D各向异性区域，从而实现高保真反走样渲染。该方法的核心是两个关键组件：Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。同时，Platonic实体的每个面都由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>(4) <strong>方法性能：</strong>在多尺度Blender数据集和新捕获的真实世界数据集上，该方法在渲染质量和效率方面均优于现有方法。</p></li><li><p>方法：</p></li></ol><p>（1）：基于Ripmap编码的Platonic实体投影，将3D空间分解到Platonic实体的不平行面上，将各向异性的3D区域投影到具有可区分特征的平面上。</p><p>（2）：Platonic实体的每个面由Ripmap编码编码，该编码通过各向异性预过滤可学习的特征网格构建，以实现对投影各向异性区域的精确和高效表征。</p><p>（3）：采用混合表示，包括显式和隐式表示，既能保证效率，又能保证灵活性。</p><p>（4）：采用多采样和面积采样对圆锥截体进行特征化，其中面积采样采用各向异性3D高斯函数对圆锥截体进行表征，再利用提出的Platonic实体投影和Ripmap编码进行特征化。</p><p>（5）：利用MLP估计圆锥截体的颜色和密度，并通过体积渲染渲染像素颜色。</p><p>（6）：采用光度损失函数，对渲染图像和观测图像进行端到端优化。</p><p><strong>8. 结论：</strong></p><p>（1）：本工作提出了一种基于Ripmap编码的Platonic实体表示，用于神经辐射场，称为Rip-NeRF。Rip-NeRF可以渲染高保真抗锯齿图像，同时保持效率，这得益于提出的Platonic实体投影和Ripmap编码。Platonic实体投影将3D空间分解到特定Platonic实体的不平行面上，使得各向异性的3D区域可以投影到具有可区分特征的平面上。Ripmap编码通过各向异性预过滤可学习的特征网格构建，能够对投影的各向异性区域进行精确高效的特征化。这两个组件协同工作，对各向异性的3D区域进行精确高效的特征化。它在合成数据集和真实世界捕捉上都实现了最先进的渲染质量，特别是在结构和纹理的精细细节方面表现出色，这验证了所提出的Platonic实体投影和Ripmap编码的有效性。</p><p>（2）：创新点：提出了基于Ripmap编码的Platonic实体投影和Ripmap编码，用于对各向异性的3D区域进行精确高效的特征化。</p><p>性能：在渲染质量和效率方面均优于现有方法，特别是在精细细节方面表现出色。</p><p>工作量：与现有方法相比，工作量略大，因为需要对Platonic实体投影和Ripmap编码进行预处理。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d766632fb875e5fe770b7fee4ed1ae6b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43f9b3bed63eef23fb98c456f7077574.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b2442c515601a7e17fca7b5a8e2166b.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/</id>
    <published>2024-05-13T08:18:16.000Z</published>
    <updated>2024-05-13T08:18:16.334Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>一个图像到可编辑动态 3D 模型和视频生成是图像到 3D 表示或图像 3D 重建研究领域的新颖方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>采用高斯溅射法，可实现隐式 3D 重建，并优于原始神经辐射场。</li><li>借助技术和原理的快速发展，人们尝试使用稳定扩散模型通过文本指令生成目标模型。</li><li>然而，使用常规隐式机器学习方法难以获得精确的动作和动作控制，且难以生成内容长且语义连续的 3D 视频。</li><li>研究者提出 OneTo3D 方法，使用单张图像生成可编辑 3D 模型和目标语义连续且时间无限的 3D 视频。</li><li>研究者采用普通基本高斯溅射模型从单张图像生成 3D 模型，该模型对视频内存和计算机运算能力要求较低。</li><li>研究者针对对象骨架设计了自动生成和自适应绑定机制。</li><li>结合研究者提出的可重新编辑的动作和动作分析控制算法，在 3D 模型精确动作和动作控制以及根据输入文本指令生成稳定的语义连续且时间无限的 3D 视频方面取得了优于 SOTA 项目的性能。</li><li>研究者分析了详细的实现方法和理论分析，并将给出相关的比较和结论。</li><li>该项目代码开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：OneTo3D：单幅图像生成可编辑动态 3D 模型和视频</p></li><li><p>作者：JINWEI LIN</p></li><li><p>第一作者单位：莫纳什大学</p></li><li><p>关键词：3D、单幅图像、可编辑、动态、生成、自动化、视频、自适应、骨架</p></li><li><p>论文链接：https://arxiv.org/abs/2405.06547，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：3D 表征或 3D 重建是计算机视觉领域长期以来的挑战。目前实现 3D 重建的方法可分为两类：传统显式方法和机器学习隐式方法。近年来，神经辐射场 (NeRF) 作为一种突出的隐式方法，在渲染和表示真实场景视图方面取得了优异的性能。</p><p>(2)：过去方法：基于 NeRF，出现了各种隐式 3D 表征或重建的研究项目。然而，现有的方法在精确运动和动作控制以及生成连续语义 3D 视频方面存在困难。</p><p>(3)：本文方法：本文提出 OneTo3D 方法，利用单幅图像生成可编辑 3D 模型和连续语义 3D 视频。该方法使用基本的 Gaussian Splatting 模型从单幅图像生成 3D 模型，并设计了一种自动生成和自适应绑定机制来创建对象骨架。结合可编辑运动和动作分析控制算法，OneTo3D 在 3D 模型精确运动和动作控制以及根据文本指令生成稳定连续的语义 3D 视频方面取得了优于现有技术的性能。</p><p>(4)：任务和性能：本文方法在生成可编辑动态 3D 模型和视频的任务上取得了以下性能：- 实现了精确的运动和动作控制，超越了现有技术。- 能够根据文本指令生成稳定连续的语义 3D 视频。- 这些性能支持了本文提出的目标。</p><ol><li>方法：</li></ol><p>(1)：生成初始 3D 模型：利用 Gaussian Splatting 模型从输入图像生成初始 3D 模型，不包含动态或可编辑因素。</p><p>(2)：生成并绑定自适应骨架：分析初始 3D 模型的几何参数信息，构建适合的骨架，并根据输入图像中的姿态、形状和关键点信息微调骨架参数，使其与物体身体贴合。</p><p>(3)：文本到动作：分析用户文本指令的命令意图，提取相对骨骼的特定运动和修改数据，控制特定骨骼在 Blender 中实现相对运动，考虑运动量化、运动次数、运动方向和运动范围等参数。</p><p>(4)：可重新编辑运动控制：配合 Blender 界面实现运动可重新编辑控制，将当前姿态作为关键帧插入，结合连续关键帧生成最终 3D 视频，Blender 文件保存为可重新编辑的 3D 编辑文件。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种生成可编辑动态 3D 模型和视频的新方法 OneTo3D。该方法具有以下优点：- 实现了精确的运动和动作控制，超越了现有技术。- 能够根据文本指令生成稳定连续的语义 3D 视频。- 这些性能支持了本文提出的目标。</p><p>（2）：创新点：- 利用单幅图像生成可编辑 3D 模型和视频。- 设计了一种自动生成和自适应绑定机制来创建对象骨架。- 结合可编辑运动和动作分析控制算法，实现精确的运动和动作控制。</p><p>性能：- 在生成可编辑动态 3D 模型和视频的任务上取得了优异的性能。- 超越了现有技术在精确运动和动作控制方面的性能。</p><p>工作量：- 该方法需要大量的训练数据和计算资源。- 生成可编辑动态 3D 模型和视频的过程需要大量的时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth   Smooth Regularization**Authors:Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu**This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian Splatting-based SLAM has yielded promising results, but rely on RGB-D input and is weak in tracking. To address these limitations, we uniquely integrates advanced sparse visual odometry with a dense Gaussian Splatting scene representation for the first time, thereby eliminating the dependency on depth maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking robustness. Here, the sparse visual odometry tracks camera poses in RGB stream, while Gaussian Splatting handles map reconstruction. These components are interconnected through a Multi-View Stereo (MVS) depth estimation network. And we propose a depth smooth loss to reduce the negative effect of estimated depth maps. Furthermore, the consistency in scale between the sparse visual odometry and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR). We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art performance. Additionally, it outperforms previous monocular methods in terms of novel view synthesis fidelity, matching the results of neural SLAM systems that utilize RGB-D input. [PDF](http://arxiv.org/abs/2405.06241v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**Summary**基于高斯分布的稠密视觉SLAM新框架，集成了稀疏视觉里程计和稠密场景表示，增强了鲁棒性和准确性。**Key Takeaways*** 结合了稀疏视觉里程计和高斯分布的稠密场景表示。* 消除了对深度图的依赖，增强了跟踪的鲁棒性。* 多视图立体（MVS）深度估计网络连接了稀疏视觉里程计和高斯分布。* 提出深度平滑损失，减少估计深度图的负面影响。* 通过稀疏-稠密调整环（SDAR）保持了稀疏视觉里程计和高斯分布的地图之间的规模一致性。* 在合成数据集和真实世界数据集上评估，姿势估计的准确性超过了现有方法，达到最先进的水平。* 在新视图合成保真度方面优于之前的单目方法，达到利用RGB-D输入的神经SLAM系统的效果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title: MGS-SLAM: 单目稀疏跟踪和高斯映射与深度平滑正则化</li><li>Authors: Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu</li><li>Affiliation: 东北大学机器人科学与工程学院</li><li>Keywords: Visual SLAM, Gaussian Splatting, Sparse Visual Odometry, Multi-View Stereo, Depth Smooth Regularization</li><li>Urls: Paper: https://arxiv.org/abs/2405.06241, Github: None</li><li>Summary:</li></ol><p>(1):该文章的研究背景是：随着深度学习的快速发展，一种利用可微渲染的新型 SLAM 技术应运而生。基于可微渲染的 SLAM 技术最初使用神经辐射场 (NeRF) 作为其基础构建方法。NeRF 利用神经网络表示 3D 场景，能够合成高质量图像并从多视图中恢复密集的几何结构。基于 NeRF 的 SLAM 系统在制图过程中保留了详细的场景信息，增强了对后续导航和路径规划的支持。然而，NeRF 的方法在图像渲染过程中需要对每个像素进行多次前向预测，导致大量的计算冗余。因此，这种低效性阻碍了基于 NeRF 的 SLAM 实时运行，从而限制了其在直接下游任务中的实用性。</p><p>(2):过去的方法是基于高斯散射的 SLAM 系统依赖于 RGB-D 相机的深度图输入，这限制了它们的应用范围。问题是跟踪能力弱。该方法的动机很好，它将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性。</p><p>(3):本文提出的研究方法是：提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM。该工作在 SLAM 领域引入了多项突破性进展，包括将基于高斯散射的技术与稀疏视觉里程计相结合，采用预训练的多视图立体 (MVS) 深度估计网络，开创了一种几何平滑深度损失，并开发了稀疏 -密集调整环 (SDAR) 以确保尺度一致性。这些创新共同显着提高了仅依赖 RGB 图像输入的 SLAM 系统的准确性和功能性。</p><p>(4):本文的方法在以下任务和性能上取得了成就：在各种合成和真实世界数据集上对我们的系统进行了评估。我们的位姿估计的准确度超过了现有方法，并取得了最先进的性能。此外，它在新的视图合成保真度方面优于之前的单目方法，与利用 RGB-D 输入的神经 SLAM 系统的结果相匹配。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM，将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性；</p><p>（2）：采用预训练的多视图立体（MVS）深度估计网络，为后端密集映射提供几何深度监督；</p><p>（3）：开创了一种几何平滑深度损失，以减轻先验深度图误差对高斯地图几何重建的影响；</p><p>（4）：开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><ol><li>结论：</li></ol><p>（1）：该工作提出了一种新颖的单目高斯散射 SLAM 系统 MGS-SLAM，该系统将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对基于高斯散射的 SLAM 系统中典型的深度图的依赖性，并增强了跟踪鲁棒性。此外，该工作还采用预训练的多视图立体（MVS）深度估计网络，开创了一种几何平滑深度损失，并开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。这些创新共同显着提高了仅依赖 RGB 图像输入的 SLAM 系统的准确性和功能性。</p><p>（2）：创新点：将基于高斯散射的技术与稀疏视觉里程计相结合，消除了对深度图的依赖性，并增强了跟踪鲁棒性；采用预训练的多视图立体（MVS）深度估计网络，为后端密集映射提供几何深度监督；开创了一种几何平滑深度损失，以减轻先验深度图误差对高斯地图几何重建的影响；开发了稀疏-密集调整环（SDAR），以确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><p>性能：在各种合成和真实世界数据集上对我们的系统进行了评估。我们的位姿估计的准确度超过了现有方法，并取得了最先进的性能。此外，它在新的视图合成保真度方面优于之前的单目方法，与利用 RGB-D 输入的神经 SLAM 系统的结果相匹配。</p><p>工作量：该方法需要预训练的多视图立体（MVS）深度估计网络，并且需要开发稀疏-密集调整环（SDAR）来确保稀疏点云地图和密集高斯地图之间的尺度一致性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9c81783ec5cc64db3f3888e91459cd94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5722638fadf13564cb13427fd8d4410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1359ff05ba09f3fd64460b9bd9878a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-940291f15a48e90bf4dec39f8ee7ddd2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5bc3a1c2278602383a64b530b3dd889.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e517184c75aa28276e746751c5d28917.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89f79eb15cc3b181f2efde56510f1d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5713c30b73286985fa8f2ff3f7ac2e21.jpg" align="middle"></details>## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic   Gaussian Splatting**Authors:Yikun Ma, Dandan Zhan, Zhi Jin**Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation. [PDF](http://arxiv.org/abs/2405.05768v1) Accepted by IJCAI-2024**摘要**文本驱动的3D室内场景生成在游戏、智能家居和AR/VR应用中有着广泛的应用，快速且高保真场景生成对确保用户友好体验至关重要。**要点**- 3D室内场景生成有着广泛的应用。- 现有的方法生成过程耗时或需要用户手动指定运动参数，给用户带来不便。- 现有的方法专注于窄视域观点迭代生成，影响全局一致性和整体场景质量。- FastScene框架可在保持场景一致性的情况下快速生成更高质量的3D场景。- 根据文本提示生成全景图并估计其深度，因为全景图包含整个场景信息并展示明确的几何约束。- 引入粗视图合成（CVS）和渐进式新视图修复（PNVI）策略来获得高质量的新视角，确保场景一致性和视图质量。- 使用多视图投影（MVP）形成透视视图，并应用3D高斯散射（3DGS）进行场景重建。- 全面实验表明，FastScene在生成速度和质量上都超过了其他方法，并具有更好的场景一致性。- FastScene仅通过文本提示就可以在短短15分钟内生成3D场景，比最先进的方法快至少1小时，使其成为用户友好场景生成范例。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：FastScene：文本驱动的快速 3D 室内场景生成</p></li><li><p>作者：Yikun Ma，Dandan Zhan，Zhi Jin</p></li><li><p>单位：中山大学</p></li><li><p>关键词：文本驱动的 3D 场景生成，全景图，高斯体素渲染</p></li><li><p>论文链接：Paper_info，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：文本驱动的 3D 室内场景生成在游戏、智能家居、AR/VR 等领域有着广泛的应用。快速、高保真的场景生成对于确保用户友好的体验至关重要。然而，现有方法的特点是生成过程冗长或需要复杂的手动指定运动参数，给用户带来不便。此外，这些方法通常依赖于窄视场视点迭代生成，影响了全局一致性和整体场景质量。</p><p>（2）：过去的方法及问题：Set-the-Scene 从文本提示和 3D 对象代理进行全局局部训练，生成可控场景。但由于缺乏相应的几何信息，生成场景的质量和分辨率不佳。SceneScape 生成长距离视图，生成风格多样。但由于内绘和深度估计误差的积累，其视图质量会随着时间的推移而降低。Text2Room 和 Text2NeRF 逐步生成透视新视图。但其增量局部操作难以保证场景一致性和连贯性。Ctrl-Room 对 ControlNet 进行微调以生成可编辑的全景图，然后执行网格重建。但由于 Ctrl-Room 难以生成多视图图像，因此它倾向于将 3D 模型扁平化，场景质量有限。</p><p>（3）：提出的研究方法：本文提出了一种新颖的文本到 3D 场景框架，称为 FastScene，旨在快速生成一致、真实且高质量的场景。如图 1 所示，我们的方法主要包括三个阶段。1）在第一阶段，给定一个文本提示，我们利用预训练的 Diffusion360 生成全景图。选择全景图是因为它能够捕获全局信息并表现出明确的几何约束。2）在第二阶段，我们利用全景图及其深度估计来生成粗略视图。然后，我们引入粗略视图合成 (CVS) 和渐进式新视图内绘 (PNVI) 策略来细化粗略视图，同时确保场景一致性和视图质量。3）在第三阶段，我们利用多视图投影 (MVP) 形成透视视图，并应用 3D 高斯体素渲染 (3DGS) 进行场景重建。</p><p>（4）：方法在什么任务上取得了什么性能：综合实验表明，FastScene 在生成速度和质量方面都优于其他方法，并且场景一致性更好。值得注意的是，FastScene 仅在文本提示的指导下，可以在短短 15 分钟内生成一个 3D 场景，这比最先进的方法至少快一个小时，使其成为用户友好场景生成的一个典范。</p><ol><li>方法：</li></ol><p>（1）：Diffusion360生成全景图，EGformer估计深度图；</p><p>（2）：CVS生成带有孔洞的新全景图，PNVI逐步修复孔洞；</p><p>（3）：MVP生成透视视图，3DGS进行场景重建。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种快速文本到 3D 室内场景生成框架 FastScene，展示了令人满意的场景质量和一致性。对于用户而言，FastScene 只需要一个文本提示，无需设计运动参数，即可在短短 15 分钟内提供一个完整的高质量 3D 场景。提出的 PNVI 与 CVS 可以生成一致的新全景视图，而 MVP 将其投影到透视视图，促进了 3DGS 重建。大量的实验证明了我们方法的有效性。FastScene 提供了一个用户友好的场景生成范例，我们相信它具有广泛的潜在应用。在未来的工作中，我们将重点关注 3D 场景编辑和多模态学习。致谢 本工作得到了国家自然科学基金（No.62071500）和深圳市科技计划（Grant No. JCYJ20230807111107015）的支持。</p><p>（2）：创新点：提出了一种基于全景图的快速文本到 3D 室内场景生成框架 FastScene；性能：FastScene 在生成速度和质量方面均优于其他方法，并且场景一致性更好；工作量：FastScene 仅在文本提示的指导下，可以在短短 15 分钟内生成一个 3D 场景，这比最先进的方法至少快一个小时。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9ae84b1fe141ce2458a3514ff61edab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9516335b56aaf68e720f85429fe6d949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcf104105c3e3c0c631f51aa64860b19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6035d44b6617ded58ccc09ecb36f41eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f44233f42fcbaf0d92844c77c24e8b3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a657f39b3ff13b487d3da4b747083bfc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-30b15f3bf60cdbeb4ed8595da183fcab.jpg" align="middle"><img src="https://pica.zhimg.com/v2-66a918dd33cc4c399a7322eb37b47e0d.jpg" align="middle"></details>## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review**Authors:Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård**Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting. [PDF](http://arxiv.org/abs/2405.03417v1) 24 pages**Summary**基于图像的3D重建是一项颇具挑战的任务，涉及从一组输入图像中推断物体的3D形状。基于学习的方法因其直接估计3D形状的能力而备受关注。本文重点介绍3D重建的最先进技术，包括生成新颖的、未曾见过的视图。概述了高斯散布方法的最新发展，涵盖输入类型、模型结构、输出表示和训练策略。还讨论了尚未解决的挑战和未来的发展方向。鉴于该领域的快速发展以及提高3D重建方法的众多机会，对算法进行全面检查至关重要。因此，本研究对高斯散布的最新进展进行了全面概述。**Key Takeaways**- 图像-基于3D重建包括从一组输入图像推断对象的3D形状。- 学习-基于方法因其直接估计3D形状的能力备受关注。- 高斯散布是一个用于3D重建的最先进技术。- 高斯散布输入类型包括单目和多目图像。- 高斯散布模型结构包括编码器-解码器和Transformer。- 高斯散布输出表示包括体素网格和点云。- 高斯散布训练策略包括监督学习和自监督学习。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review</p></li><li><p>Authors: Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri Knausgård</p></li><li><p>Affiliation: Department of Engineering Sciences, University of Agder, Grimstad, Norway</p></li><li><p>Keywords: 3D Reconstruction, Computer Vision, Deep Learning, Gaussian Splatting, Novel view synthesis, Optimization, Rendering</p></li><li><p>Urls: Paper_info:Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.Digital Object Identifier xxxx</p></li><li><p>Summary:</p><pre><code>           (1):Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.;           (2):Traditional approaches to 3D reconstruction, such as photogrammetry and multi-view stereo (MVS) algorithms, often suffer from artifacts, failure cases, and slow training times. Gaussian Splatting is a novel method that addresses these limitations by representing 3D objects as a collection of Gaussians. This representation allows for efficient rendering and interpolation, resulting in high-quality novel views.;           (3):The Gaussian Splatting method involves an iterative refinement process, where multiple Gaussians are optimized to match the input images. The model is trained using a combination of supervised and unsupervised losses, which encourage consistency with the input images and smoothness in the 3D space. The output of the model is a volumetric point cloud, where each point represents a Gaussian with parameters such as color, spread, and location.;           (4):Gaussian Splatting has been shown to achieve state-of-the-art results on a variety of 3D reconstruction and novel view synthesis tasks. The method outperforms previous approaches in terms of rendering quality, training time, and robustness to challenging scenes. These results demonstrate the potential of Gaussian Splatting for a wide range of applications, including virtual reality, augmented reality, and computer-aided design.</code></pre></li><li><p>方法：</p><pre><code>           (1):本文首先介绍了高斯散点法，这是一种使用高斯函数集合表示 3D 物体的创新方法。这种表示形式允许高效渲染和插值，从而产生高质量的新颖视图；           (2):高斯散点法涉及一个迭代细化过程，其中优化多个高斯函数以匹配输入图像。模型使用监督和无监督损失的组合进行训练，这鼓励与输入图像的一致性和 3D 空间中的平滑度。模型的输出是体积点云，其中每个点表示一个具有颜色、扩展和位置等参数的高斯函数；           (3):高斯散点法已被证明在各种 3D 重建和新颖视图合成任务上实现了最先进的结果。该方法在渲染质量、训练时间和对具有挑战性场景的鲁棒性方面优于以前的方法。这些结果证明了高斯散点法在广泛的应用中的潜力，包括虚拟现实、增强现实和计算机辅助设计。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文从功能和应用角度对高斯散点法在三维重建和新颖视图合成中的应用进行了全面的综述，涵盖了动态建模、变形建模、运动跟踪、非刚性/可变形物体、表情/情绪变化、基于文本的生成扩散、降噪、优化、虚拟形象、可动画对象、头部建模、同步定位与规划、网格提取与物理、优化技术、编辑能力、渲染方法、压缩等方面。特别是，本文深入探讨了图像三维重建中的挑战和进展，学习型方法在三维形状估计中的作用，以及高斯散点法在三维重建中的优势和局限性。</p><p>（2）：创新点：高斯散点法是一种使用高斯函数集合表示三维物体的创新方法，这种表示形式允许高效渲染和插值，从而产生高质量的新颖视图；性能：高斯散点法在三维重建和新颖视图合成方面取得了最先进的结果，在渲染质量、训练时间和对具有挑战性场景的鲁棒性方面优于以前的方法；工作量：高斯散点法涉及一个迭代细化过程，其中优化多个高斯函数以匹配输入图像，训练过程需要较大的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-48d38462ddefdcfe75129220282e7a18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-712a52026b682e9ab729dccf592cc5f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14985716143782f83102a5633ec37c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3dd2127ce2dbe6cdafc1b40d9cc2fb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f865d904180e8ed6511d41dac5f81c0.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v3) To be published in ACM SIGGRAPH 2024**Summary**实时高斯 SLAM 系统使用高斯点云表示方式实现了大规模 RGBD 图像序列的重建，并采用高效的高斯优化方法实时生成连续的三维重建结果。**Key Takeaways**- 使用高斯点云表示环境，紧凑高效。- 将高斯点云分为不透明和半透明两种，不透明点云拟合表面和主要颜色，半透明点云拟合残差颜色。- 通过深度渲染和颜色渲染分离，单个不透明高斯点云就能拟合局部表面区域，减少了高斯点云数量、存储空间和计算成本。- 实时高斯优化，针对新观测到的像素、颜色误差大的像素和深度误差大的像素添加高斯点云。- 将高斯点云分为稳定和不稳定两种，仅优化不稳定的高斯点云，仅渲染不稳定高斯点云覆盖的像素。- 与基于 NeRF 的 RGBD SLAM 系统相比，该系统重建质量相当，但速度提高约一倍，内存成本减半，并且在新的视图合成真实感和相机跟踪准确性方面表现更出色。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAM：使用高斯渲染的大规模实时三维重建</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: RGBD SLAM, 3D reconstruction, Gaussian splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 该文章的研究背景是随着 RGBD 相机的发展，实时三维重建技术得到了广泛应用。然而，现有的方法在处理大规模环境时，往往面临内存消耗大、计算成本高的问题。</p><p>(2): 过去的方法主要使用神经辐射场（NeRF）来表示三维场景，但 NeRF 需要大量的高斯体素来拟合表面，导致内存消耗大。</p><p>(3): 该文章提出了一种基于高斯渲染的实时三维重建系统 RTG-SLAM。RTG-SLAM 使用高斯体素来表示三维场景，并通过强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗。此外，RTG-SLAM 采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。</p><p>(4): 在大规模环境重建任务上，RTG-SLAM 实现了与 NeRF-SLAM 相当的重建质量，但速度提高了约两倍，内存消耗减少了一半。</p><ol><li><p>方法：</p><pre><code>           (1): 该方法使用高斯体素表示三维场景，并通过强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗。           (2): 该方法采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。           (3): 该方法在大规模环境重建任务上，实现了与 NeRF-SLAM 相当的重建质量，但速度提高了约两倍，内存消耗减少了一半。</code></pre></li><li><p>结论：</p><pre><code>           （1）：本文提出了一种基于高斯渲染的大规模实时三维重建系统 RTG-SLAM，该系统通过使用高斯体素表示三维场景，并强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗，并采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本，在保证重建质量的情况下，提高了重建速度，降低了内存消耗。           （2）：创新点：提出了一种基于高斯渲染的大规模实时三维重建系统，该系统通过使用高斯体素表示三维场景，并强制每个高斯体素要么不透明要么近乎透明，来减少内存消耗，并采用了一种高效的在线高斯优化方案，只优化不稳定的高斯体素，进一步降低了计算成本。           性能：在保证重建质量的情况下，提高了重建速度，降低了内存消耗。           工作量：需要对不稳定的高斯体素进行优化，需要渲染不稳定的高斯体素占据的像素。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0eaefb973e265febe848896437a17659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a058fc44423666e88d6baa1e211422b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/</id>
    <published>2024-05-13T08:02:14.000Z</published>
    <updated>2024-05-13T08:02:14.752Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior"><a href="#NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior" class="headerlink" title="NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior"></a>NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</h2><p><strong>Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</strong></p><p>Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. </p><p><a href="http://arxiv.org/abs/2405.05749v2">PDF</a> 11 pages, 5 figures</p><p><strong>Summary</strong><br>单张人脸图像即可驱动 3D 会话式人头的生成，这是由于对神经辐射场（NeRF）技术和生成模型的巧妙运用。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-based 3D talking head generation typically requires a large amount of paired audio-visual data.</li><li>Audio-driven 3D talking head animations with a single image often have unsatisfactory results due to occlusion problems.</li><li>NeRFFaceSpeech generates high-quality 3D-aware talking heads from a single image.</li><li>NeRFFaceSpeech uses generative models and NeRF to create a 3D-consistent facial feature space.</li><li>Spatial synchronization method employs audio-correlated vertex dynamics to transform static image features into dynamic visuals.</li><li>LipaintNet replenishes the lacking information in the inner-mouth area.</li><li>NeRFFaceSpeech outperforms previous methods in generating audio-driven talking heads from a single image with enhanced 3D consistency.</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>题目：NeRFFaceSpeech：基于生成先验的单次音频驱动的三维说话人头部合成</p></li><li><p>作者：Gihoon Kim，Kwanggyoon Seo，Sihun Cha，Junyong Noh</p></li><li><p>所属机构：首尔大学</p></li><li><p>关键词：神经辐射场（NeRF），音频驱动，三维说话人头部，生成先验</p></li><li><p>论文链接：https://arxiv.org/abs/2405.05749，Github代码链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：音频驱动的说话人头部生成正从二维内容转向三维内容。神经辐射场（NeRF）作为一种合成高质量三维说话人头部输出的方法备受关注。然而，这种基于NeRF的方法通常需要大量针对每个身份成对的音频-视觉数据，从而限制了该方法的可扩展性。尽管已经尝试使用单张图像生成音频驱动的三维说话人头部动画，但由于图像中遮挡区域的信息不足，结果往往不令人满意。本文主要关注解决单次、音频驱动的领域中被忽视的三维一致性方面，其中面部动画主要以正面视角合成。</p><p>（2）：过去方法及其问题：现有方法试图使用单张图像生成音频驱动的三维说话人头部动画，但由于图像中遮挡区域的信息不足，结果往往不令人满意。本文的方法动机明确，旨在解决这一问题。</p><p>（3）：本文提出的研究方法：我们提出了一种新方法NeRFFaceSpeech，能够生成高质量的三维感知说话人头部。该方法结合了生成模型的先验知识和NeRF，可以构建与单张图像相对应的三维一致的面部特征空间。我们的空间同步方法采用参数化面部模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的三维面部运动。此外，我们还引入了LipaintNet，它可以补充单张给定图像中无法获得的内部口腔区域的缺失信息。该网络以自监督的方式进行训练，利用生成能力而无需额外数据。</p><p>（4）：方法在什么任务上取得了什么性能：综合实验表明，与以往方法相比，我们的方法在生成具有增强三维一致性的音频驱动的说话人头部方面具有优势。此外，我们首次引入了一种衡量模型对姿态变化鲁棒性的定量方法，这在过去只能定性地进行。</p><ol><li>Methods:</li></ol><p>（1）：提出NeRFFaceSpeech方法，结合生成模型先验和NeRF，构建与单张图像相对应的三维一致的面部特征空间；</p><p>（2）：采用参数化面部模型的音频相关顶点动态，通过光线变形将静态图像特征转换为动态视觉效果，确保逼真的三维面部运动；</p><p>（3）：引入LipaintNet，以自监督的方式补充单张给定图像中无法获得的内部口腔区域的缺失信息，无需额外数据；</p><p>（4）：引入衡量模型对姿态变化鲁棒性的定量方法，首次实现对姿态变化鲁棒性的定量评估。</p><ol><li>结论：</li></ol><p>（1）：本文的意义：本文提出了NeRFFaceSpeech，一种通过利用生成先验构建和操作三维特征，从单张图像生成三维感知音频驱动说话人头部动画的新方法。我们的管道弥合了面部参数化模型和神经渲染之间的差距，通过光线变形直观地操纵特征空间。我们还提出了LipaintNet，这是一个自监督学习框架，利用生成模型的能力来合成隐藏的内口区域，补充变形场以产生可行的结果。通过广泛的实验和用户研究，我们证明了我们的模型对姿势变化具有鲁棒性，并且可以生成比以前的方法更好的内部口部信息，从而产生更好的结果。致谢。这项工作得到了文化体育观光部R&amp;D计划的支持，该计划由文化体育观光部资助的KOCCA赠款（编号：RS-2023-00228331）资助。</p><p>（2）：创新点：将生成模型先验与NeRF相结合，构建与单张图像相对应的三维一致的面部特征空间；提出LipaintNet，一个自监督学习框架，利用生成模型的能力来合成隐藏的内口区域；引入衡量模型对姿态变化鲁棒性的定量方法。性能：与以往方法相比，我们的方法在生成具有增强三维一致性的音频驱动说话人头部方面具有优势。工作量：与需要大量成对音频-视觉数据的基于NeRF的方法相比，我们的方法只需要一张图像，工作量更小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details><h2 id="SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space"><a href="#SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space" class="headerlink" title="SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space"></a>SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space</h2><p><strong>Authors:Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</strong></p><p>Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the framework’s generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at <a href="http://swaptalk.cc">http://swaptalk.cc</a>. </p><p><a href="http://arxiv.org/abs/2405.05636v1">PDF</a> </p><p><strong>Summary</strong><br>视频质量、口型同步度以及人脸替换的真实性与一致性方面，SwapTalk 均优于现存技术，适用于异步视音频场景。</p><p><strong>Key Takeaways</strong></p><ul><li>人脸替换和唇形同步结合提供了经济实惠的定制化说话人脸生成方案。</li><li>SwapTalk 在同一潜在空间中执行人脸替换和唇形同步任务，避免了模型级联造成的干扰。</li><li>使用 VQ 嵌入空间，提高了框架的可编辑性和保真度。</li><li>身份损失的加入增强了模型对未见身份的泛化能力。</li><li>专家鉴别器监督提升了唇形同步模块的同步质量。</li><li>将评估范围扩展到异步视音频场景，更贴近实际应用。</li><li>新颖的身份一致性度量可更全面地评估生成视频中人脸随时间变化的一致性。</li><li>SwapTalk 在视频质量、唇形同步精度、人脸替换保真度和身份一致性方面显著优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space</p></li><li><p>Authors: Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Audio-Driven Talking Face Generation, Face Swapping, Lip Synchronization, VQ-Embedding Space</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05636, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：音频驱动说话人脸生成技术在虚拟数字人领域取得了显著进展，但从用户自定义肖像生成唇形同步的说话人脸视频仍面临挑战。人脸替换与唇形同步（lip-sync）技术相结合提供了经济实用的解决方案。</p><p>(2): 过去方法：串联人脸替换模型和唇形同步模型是直观的方法，但存在相互干扰问题。在 RGB 空间中直接级联模型会限制可编辑性和解耦性，导致准确性和清晰度下降。</p><p>(3): 研究方法：SwapTalk 提出了一种统一的框架，在共享的潜在空间中处理人脸替换和唇形同步任务，以提高两项任务的精度和整体一致性。框架基于 VQ-embedding 空间，并引入身份损失和专家鉴别器监督来增强泛化能力和同步质量。</p><p>(4): 性能：在 HDTF 数据集上，SwapTalk 在视频质量、唇形同步精度、人脸替换保真度和身份一致性方面都显著优于现有技术，验证了其方法的有效性。</p><ol><li>方法：</li></ol><p>（1）：以 VQGAN 为基础模型，在 VQ 嵌入空间中处理人脸替换和唇形同步任务；</p><p>（2）：人脸替换模块通过 Tokenization 模块和 Transformer 编码器处理输入源和目标人脸的潜在表示，实现人脸替换；</p><p>（3）：唇形同步模块由人脸扭曲和唇形变换子模块组成，分别处理姿势转换和唇形修改，输入目标和参考 VQ 嵌入；</p><p>（4）：引入身份损失和专家鉴别器监督，增强泛化能力和同步质量。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一个创新性的统一框架 SwapTalk，用于生成定制化的说话人脸视频。为了解决现有模型中任务干扰和视频清晰度下降的问题，我们在可编辑且高保真的 VQ 嵌入空间中处理人脸替换和唇形同步任务。使用 VQ 嵌入空间的优势包括：（1）降低人脸替换和唇形同步模块的计算成本；（2）将高分辨率图像生成任务留给 VQGAN，降低模型的学习难度。此外，我们在人脸替换模块的训练阶段引入了身份损失，这极大地增强了模型对以前未见身份进行泛化的能力。在唇形同步模块的训练过程中，我们在 VQ 嵌入空间内采用唇形同步专家的监督，这</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fa51f1a10514d3515bc6c6c7a64b853d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a575e9139fb720f3d66cfc93038554e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7102a7da46779dfc3bd4093ee964061.jpg" align="middle"></details><h2 id="AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding"><a href="#AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding" class="headerlink" title="AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding"></a>AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding</h2><p><strong>Authors:Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</strong></p><p>The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalker’s capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a>. </p><p><a href="http://arxiv.org/abs/2405.03121v1">PDF</a> 14 pages, 7 figures</p><p><strong>Summary</strong><br>利用一个肖像生成逼真的说话面孔，突破了以往只关注唇部同步而忽略面部表情和非语言信号的局限性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 AniTalker 框架，利用通用运动表示捕捉面部表情和非语言信号。</li><li>采用自监督学习策略，从同一身份的源帧重建目标视频帧，学习细微的动作表示。</li><li>使用度量学习开发身份编码器，同时最大程度地减少身份和动作编码器之间的互信息。</li><li>整合扩散模型和方差适配器，生成多样化且可控的面部动画。</li><li>AniTalker 不仅能生成逼真的面部动作，还适用于创建动态虚拟形象。</li><li>更多合成结果可在 <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a> 查看。</li><li>通过减少对带标签数据的需求，AniTalker 提高了模型的可用性。</li><li>AniTalker 有潜力在虚拟形象和人机交互等领域得到广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniTalker: 通过身份解耦的面部运动编码制作栩栩如生且多样的动态人脸</p></li><li><p>Authors: Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</p></li><li><p>Affiliation: 上海交通大学X-LANCE实验室</p></li><li><p>Keywords: Talking Face, Self-supervised, Motion Encoding, Disentanglement</p></li><li><p>Urls: https://arxiv.org/abs/2405.03121, https://github.com/X-LANCE/AniTalker</p></li><li><p>Summary:</p></li></ol><p>(1): 现有模型主要关注唇部同步等言语线索，无法捕捉复杂的面部表情和非言语线索的动态。</p><p>(2): 过去的方法存在以下问题：需要大量标记数据；无法生成多样化的面部动画；无法控制面部动画的细节。</p><p>(3): 提出AniTalker框架，该框架使用通用的运动表示来有效捕捉广泛的面部动态。通过两个自监督学习策略增强运动描述：从同一身份内的源帧重建目标视频帧以学习细微的运动表示；使用度量学习开发身份编码器，同时主动最小化身份和运动编码器之间的互信息。</p><p>(4): 在生成逼真面部动作的任务上，AniTalker 实现了以下性能：在 CelebA-HQ 数据集上，平均重建误差为 0.012；在 TalkingFace 数据集上，平均重建误差为 0.015；用户研究表明，AniTalker 生成的人脸动画比基线方法更逼真、更自然。这些性能支持了 AniTalker 生成详细且逼真的面部动作并为现实世界应用制作动态头像的潜力。</p><ol><li>方法：</li></ol><p>（1）：提出AniTalker框架，该框架使用通用的运动表示来有效捕捉广泛的面部动态。</p><p>（2）：通过两个自监督学习策略增强运动描述：从同一身份内的源帧重建目标视频帧以学习细微的运动表示；使用度量学习开发身份编码器，同时主动最小化身份和运动编码器之间的互信息。</p><p>（3）：在生成逼真面部动作的任务上，AniTalker 实现了以下性能：在 CelebA-HQ 数据集上，平均重建误差为 0.012；在 TalkingFace 数据集上，平均重建误差为 0.015；用户研究表明，AniTalker 生成的人脸动画比基线方法更逼真、更自然。这些性能支持了 AniTalker 生成详细且逼真的面部动作并为现实世界应用制作动态头像的潜力。</p><ol><li>结论：</li></ol><p>（1）：AniTalker框架在创建逼真的说话化身方面取得了重大进展，解决了数字人物动画中对细粒度和通用运动表示的需求。通过集成自监督通用运动编码器并采用度量学习和互信息解耦等复杂技术，AniTalker有效地捕捉了言语和非言语面部动态的细微差别。由此产生的框架不仅增强了面部动画的真实感，而且还展示了跨不同身份和媒体的强大泛化能力。AniTalker为数字人脸的逼真和动态表示设定了新的基准，有望在娱乐、交流和教育领域得到广泛应用。</p><p>（2）：创新点：提出AniTalker框架，使用通用运动表示有效捕捉广泛的面部动态；采用度量学习和互信息解耦等自监督学习策略增强运动描述。</p><p>性能：在CelebA-HQ数据集上，平均重建误差为0.012；在TalkingFace数据集上，平均重建误差为0.015；用户研究表明，AniTalker生成的人脸动画比基线方法更逼真、更自然。</p><p>工作量：需要大量标记数据；无法生成多样化的面部动画；无法控制面部动画的细节。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d9bb935fc998f1e0a691f975b5f9649c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5acfd3374b9246cfb3f6cf989c0f10f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2d729ff4d7d0304fb8e282a2921a8187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c44266650bdd0212e5707afd4b481bd4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-41b80e1ca38fd9d81d7a989e034db4c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-13  NeRFFaceSpeech One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/</id>
    <published>2024-05-13T07:52:43.000Z</published>
    <updated>2024-05-13T07:52:43.874Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-13-更新"><a href="#2024-05-13-更新" class="headerlink" title="2024-05-13 更新"></a>2024-05-13 更新</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>单张图片生成可编辑动态3D模型和视频，是单张图片到3D表示或图像3D重建研究领域的新方向和变革。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯散射法在隐式3D重建中表现出优势，优于原始的神经辐射场。</li><li>稳定扩散模型可以根据文本指令生成目标模型。</li><li>使用常规隐式机器学习方法难以精确控制运动和动作。</li><li>难以生成长时间内容和语义连续的3D视频。</li><li>OneTo3D方法提出，使用单张图片生成可编辑的3D模型和目标语义连续且时间无限的3D视频。</li><li>使用基本高斯散射模型从单张图片生成3D模型，减少视频内存和计算需求。</li><li>设计了对象骨架的自动生成和自适应绑定机制。</li><li>结合可再编辑的运动和动作分析和控制算法，在3D模型精确运动和动作控制以及生成稳定语义连续时间无限的3D视频方面取得了优于SOTA项目的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: Monash University, Australia</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 3D表示或3D重建是计算机视觉领域长期存在的挑战。</p><p>(2): 现有的3D重建方法可分为显式方法和隐式方法。显式方法直接设计和完成3D重建或建模；隐式方法使用机器学习方法和理论来实现这些目标。近年来，Neural Radiance Fields (NeRF) 在隐式3D表示或重建方面取得了突出成就。</p><p>(3): 本文提出了一种OneTo3D方法，使用一张图像生成可编辑的3D模型并生成目标语义连续时间无限的3D视频。该方法使用基本的Gaussian Splatting模型从单张图像生成3D模型，然后设计了一种自动生成和自适应绑定机制来绑定对象骨架。结合提出的可编辑动作分析和控制算法，该方法在3D模型精确动作控制和生成稳定语义连续时间无限的3D视频方面取得了比SOTA项目更好的性能。</p><p>(4): 在生成可编辑3D模型和生成目标语义连续时间无限的3D视频的任务上，该方法取得了优异的性能，证明了其目标的可实现性。</p><ol><li>方法：</li></ol><p>（1）：OneTo3D 方法包含三个主要阶段：生成初始 3D 模型、生成和绑定自适应骨架、文本到动作和行为。</p><p>（2）：初始 3D 模型生成基于 DreamGaussian，采用 Gaussian Splatting 模型处理预处理后的输入图像。</p><p>（3）：自适应骨架生成通过分析初始 3D 模型的几何参数，调整 Blender 中的基本骨架，使其适应模型形状。</p><p>（4）：文本到动作和行为分析用户输入指令，提取动作信息，控制骨架运动和动作生成。</p><p>（5）：动作可重新编辑控制与 Blender 界面协作，将当前姿势插入为关键帧，组合关键帧生成最终 3D 视频。</p><ol><li>结论：</li></ol><p>（1）本工作提出了一种名为 OneTo3D 的方法，该方法可以从一张图像生成可编辑的 3D 模型和生成目标语义连续时间无限的 3D 视频。该方法在生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的任务上取得了优异的性能，证明了其目标的可实现性。</p><p>（2）创新点：OneTo3D 方法创新性地将显式建模和隐式表示相结合，提出了一种从单张图像生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的方法。性能：OneTo3D 方法在生成可编辑 3D 模型和生成目标语义连续时间无限的 3D 视频的任务上取得了优异的性能，证明了其目标的可实现性。工作量：OneTo3D 方法的工作量相对较大，需要大量的计算和训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Distilling Diffusion Models into Conditional GANs**Authors:Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park**We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark. [PDF](http://arxiv.org/abs/2405.05967v1) Project page: https://mingukkang.github.io/Diffusion2GAN/**Summary**扩散蒸馏：将复杂多步扩散模型精馏为单步条件 GAN，极大提升推理速度，同时保留图像质量。**Key Takeaways**- 将扩散蒸馏理解为成对图像到图像翻译任务，利用扩散模型 ODE 轨迹的噪声到图像对。- 提出 E-LatentLPIPS，一种直接在扩散模型潜在空间中运行的感知损失，利用增强集成。- 采用扩散模型构建具有文本对齐损失的多尺度判别器，以构建有效的基于条件 GAN 的表述。- E-LatentLPIPS 比许多现有蒸馏方法收敛得更快，即使考虑数据集构建成本。- 证明单步生成器在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型 DMD、SDXL-Turbo 和 SDXL-Lightning。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 将扩散模型蒸馏到条件 GAN 中</p></li><li><p>Authors: Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park</p></li><li><p>Affiliation: 韩国浦项科技大学</p></li><li><p>Keywords: Diffusion Models, Conditional GANs, Distillation, Image Generation</p></li><li><p>URLs: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型在图像合成方面取得了显著进展，但其高延迟限制了其应用。</p><p>(2): 过去的方法要么从头开始训练单步模型，要么将扩散模型蒸馏到单步模型，但都存在训练困难或性能不足的问题。</p><p>(3): 本文提出了一种将复杂的多步扩散模型蒸馏到单步条件 GAN 学生模型的方法，通过将扩散蒸馏解释为配对图像到图像的翻译任务，并使用扩散模型 ODE 轨迹的噪声到图像对。</p><p>(4): 该方法在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型，证明了其有效性。</p><ol><li>Methods:</li></ol><p>(1): 将扩散模型蒸馏到条件 GAN 中，将扩散蒸馏解释为配对图像到图像的翻译任务；</p><p>(2): 使用扩散模型 ODE 轨迹的噪声到图像对作为翻译任务的数据集；</p><p>(3): 训练单步条件 GAN 学生模型，以最小化翻译任务的重建损失和对抗损失；</p><p>(4): 通过渐进式蒸馏，逐步增加扩散模型老师模型的蒸馏权重；</p><p>(5): 在零样本 COCO 基准上评估蒸馏后的单步条件 GAN 学生模型的性能。</p><ol><li>结论：</li></ol><p>(1): 本文提出了将复杂的多步扩散模型蒸馏到单步条件 GAN 学生模型的方法，在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型，证明了其有效性。</p><p>(2): 创新点：将扩散蒸馏解释为配对图像到图像的翻译任务，使用扩散模型 ODE 轨迹的噪声到图像对作为翻译任务的数据集；性能：在零样本 COCO 基准上优于最先进的单步扩散蒸馏模型；工作量：训练单步条件 GAN 学生模型，以最小化翻译任务的重建损失和对抗损失，通过渐进式蒸馏，逐步增加扩散模型老师模型的蒸馏权重。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-84223ae445d0747d377e2d5a60ddf155.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edbb96718faa70460abd9b379fff0241.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6255721ec348ae84fe6235b1ec8817e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e8ab69845f11355155ed48f11714147.jpg" align="middle"></details>## Frame Interpolation with Consecutive Brownian Bridge Diffusion**Authors:Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen**Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement. [PDF](http://arxiv.org/abs/2405.05953v1) **Summary**视频帧插值中的关键挑战是确定性生成，而潜在扩散模型的随机生成特性与之不符。**Key Takeaways*** 视频帧插值将帧生成表述为基于扩散的条件图像生成问题。* 潜在扩散模型用于条件生成，采用自动编码器压缩图像用于扩散。* 帧插值要求输出确定性等于真实中间帧，而潜在扩散模型会随机生成多样化的图像。* 潜在扩散模型中生成潜在表征的累积方差较大，导致采样轨迹随机。* 连续布朗桥扩散提出了一个确定性初始值，可以减小累积方差。* 连续布朗桥扩散与自动编码器的提升相结合，可提升帧插值中的性能。* 该方法为进一步增强帧插值性能提供了潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：连续布朗桥扩散的帧插值</p></li><li><p>作者：Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</p></li><li><p>单位：犹他大学</p></li><li><p>关键词：Video Frame Interpolation, Diffusion Models, Brownian Bridge</p></li><li><p>论文链接：xxx，Github代码链接：None</p></li><li><p>摘要：</p><p>（1）：该文章的研究背景是：近年来，视频帧插值（VFI）领域的研究工作将VFI表述为基于扩散的条件图像生成问题，在给定随机噪声和相邻帧的情况下合成中间帧。由于视频分辨率较高，因此采用潜在扩散模型（LDM）作为条件生成模型，其中自动编码器将图像压缩为潜在表示以进行扩散，然后从这些潜在表示中重建图像。这种表述提出了一个关键的挑战：VFI期望输出确定性地等于真实中间帧，但LDM在模型运行多次时会随机生成一组不同的图像。产生多样性的原因是LDM中生成潜在表示的累积方差（在生成过程中累积的方差）很大。这使得采样轨迹是随机的，导致产生多样性而不是确定性。</p><p>（2）：过去的方法有：基于流的方法和基于核的方法。基于流的方法的问题是：依赖光流，而光流估计的准确性会影响插值结果的质量。基于核的方法的问题是：需要设计复杂的核函数，并且计算成本较高。</p><p>（3）：本文提出的研究方法是：连续布朗桥扩散帧插值。具体来说，我们提出了连续布朗桥扩散，它以确定性初始值作为输入，从而导致生成潜在表示的累积方差大大减小。</p><p>（4）：本文方法在VFI任务上取得了最先进的性能，证明了其有效性。</p></li><li><p>方法：</p><p>（1）：本研究提出了一种连续布朗桥扩散帧插值方法，其通过引入确定性初始值来大幅减少生成潜在表示的累积方差，从而解决了LDM在VFI任务中产生多样性的问题。</p><p>（2）：该方法将VFI任务分为两个阶段：自动编码器阶段和真实值估计阶段。自动编码器阶段使用VQModel对图像进行编码和解码，以压缩图像并提取潜在表示。真实值估计阶段使用连续布朗桥扩散模型对潜在表示进行扩散，并训练一个UNet网络来预测扩散状态与真实值的差值。</p><p>（3）：在推理阶段，通过采样过程将扩散后的潜在表示转换为真实值，然后使用解码器和相邻帧的特征来插值中间帧。</p></li><li><p>结论：</p></li></ol><p>（1）：本研究将基于潜在扩散的 VFI 问题表述为两阶段问题：自动编码器和真实值估计。这种表述便于确定需要改进的部分，从而指导未来的研究。我们提出了连续布朗桥扩散，它由于累积方差低，可以更好地估计真实潜在表示。当自动编码器得到改进时，这种方法也会得到改进，并且通过简单而有效地设计自动编码器，实现了最先进的性能，展示了其在 VFI 中的强大潜力，因为精心设计的自动编码器可能会大幅提升性能。因此，我们相信我们的工作将为基于扩散的帧插值提供一个独特的研究方向。限制和未来研究。我们的方法使用二分法进行多帧插值：我们可以在 t = 0, 1 之间插值 t = 0.5，然后插值 t = 0.25, 0.75。然而，我们的方法不能直接从 t = 0, 1 插值 t = 0.1。未来的研究可以解决上述限制，或改进自动编码器或扩散模型以获得更好的插值质量。</p><p>（2）：创新点：提出连续布朗桥扩散，大幅降低生成潜在表示的累积方差，解决 LDM 在 VFI 任务中产生多样性的问题；性能：在 VFI 任务上取得最先进的性能；工作量：方法设计简单有效，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-fa6bfff6b0d4e51d7da63b6b09abe1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9ede9d41fa20ae19e9d3006e6223db56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f39ba0da90725c6cce506821baf61c54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0424255263b3148f099b1d496d7c3a.jpg" align="middle"></details>## Pre-trained Text-to-Image Diffusion Models Are Versatile Representation   Learners for Control**Authors:Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner**Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark. [PDF](http://arxiv.org/abs/2405.05852v1) **Summary**利用预训练的文本到图像扩散模型的文本条件表示来增强具身 AI 代理对复杂环境的理解。**Key Takeaways**- 视觉语言模型有助于具身 AI 代理学习物理世界的精细理解。- CLIP 等对比训练表示不能充分实现具身代理人的精细场景理解。- 文本到图像扩散模型的表示可以生成图像，并包含反映精细视觉空间信息。- 稳定控制表示使用文本到图像扩散模型构建，有利于学习下游控制策略。- 使用稳定控制表示学习的策略在各种模拟控制设置中具有竞争力。- 稳定控制表示使策略能够在困难的开放式词汇导航基准 OVMM 上表现出最先进的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：预训练文本到图像扩散模型</p></li><li><p>作者：Yilun Du, Aravind Srinivas, Felix Hill, Adam Lerer, Lerrel Pinto, Pieter Abbeel</p></li><li><p>第一作者单位：加州大学伯克利分校</p></li><li><p>关键词：Embodied AI, Vision-Language Models, Text-to-Image Diffusion, Reinforcement Learning</p></li><li><p>论文链接：None，Github代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：具身人工智能体需要对视觉和语言输入介导的物理世界有细粒度的理解。从特定任务数据中单独学习此类能力很困难。这导致预训练视觉语言模型成为将从互联网规模数据中学到的表征转移到下游任务和新领域的工具。然而，事实证明，诸如 CLIP 中常用的对比训练表征无法使具身代理获得足够细粒度的场景理解——这对控制至关重要。</p><p>（2）：过去的方法及问题：为了解决这一缺点，本文考虑了预训练文本到图像扩散模型中的表征，该表征经过明确优化以根据文本提示生成图像，因此包含反映高度细粒度视觉空间信息的文本条件表征。使用预训练文本到图像扩散模型，我们构建了稳定的控制表征，允许学习可推广到复杂、开放环境的下游控制策略。我们表明，使用稳定控制表征学习的策略在广泛的模拟控制设置中具有与最先进的表征学习方法相当的竞争力，包括具有挑战性的操作和导航任务。最值得注意的是，我们表明 Stable Control 表征能够学习在 OVMM（一个困难的开放词汇导航基准）上表现出最先进性能的策略。</p><ol><li>方法：</li></ol><p>（1）：使用预训练文本到图像扩散模型，从互联网规模数据中学到的表征转移到下游控制任务和新领域；</p><p>（2）：构建稳定的控制表征，允许学习可推广到复杂、开放环境的下游控制策略；</p><p>（3）：使用稳定控制表征学习的策略在广泛的模拟控制设置中具有与最先进的表征学习方法相当的竞争力，包括具有挑战性的操作和导航任务；</p><p>（4）：Stable Control 表征能够学习在 OVMM（一个困难的开放词汇导航基准）上表现出最先进性能的策略。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 Stable Control Representations，这是一种利用通用预训练扩散模型的表征进行控制的方法。我们展示了使用从文本到图像扩散模型中提取的表征进行策略学习可以提高广泛任务的泛化能力，包括操作、基于图像目标和基于对象目标的导航、抓取点预测和指代表达式接地。我们还展示了从预训练文本到图像扩散模型中提取注意力图的解释性优势，我们展示了它可以提高性能并帮助在开发过程中识别策略的下游失败。最后，我们讨论了本文提出的见解（例如，关于特征聚合和微调）可能适用于用于控制的其他基础模型的方式。我们希望 Stable Control Representations 能够帮助推进数据高效控制，并在扩散模型的能力不断提高的情况下实现具有挑战性的控制领域的开放词汇泛化。</p><p>（2）：创新点：提出 Stable Control Representations，利用预训练扩散模型的表征进行控制；性能：在广泛的任务上取得与最先进的表征学习方法相当或更好的性能；工作量：需要预训练文本到图像扩散模型，计算成本较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0e4b6edaca0c98f923986183efe5946f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e0ecaaa14d63ec3701f537e8848e8bab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e1915315192ee899890af83f32dd187.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cab74b581ca71d53bbc860e04b5ed88c.jpg" align="middle"></details>## MasterWeaver: Taming Editability and Identity for Personalized   Text-to-Image Generation**Authors:Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Lei Zhang, Wangmeng Zuo**Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver. [PDF](http://arxiv.org/abs/2405.05806v2) 34 pages**Summary**文本到图像扩散模型MasterWeaver在文本指导的图像生成中表现出色，既保持了人物身份的保真，又具有图像编辑的灵活性。**Key Takeaways*** MasterWeaver采用编码器提取身份特征，并通过交叉注意力引导图像生成。* 提出编辑方向损失，在保持身份保真的同时提高可编辑性。* 构建了面部增强数据集，促进身份学习的解耦，进一步改善可编辑性。* 大量实验表明，MasterWeaver不仅能生成具有真实身份的个性化图像，而且在文本可控性方面表现出优异性。* 代码已开源：https://github.com/csyxwei/MasterWeaver。* 无需微调，可立即使用。* 身份保真度高，可编辑性强。* 使用交叉注意力引导图像生成。* 编辑方向损失保持身份保真度和可编辑性。* 面部增强数据集促进身份学习的解耦。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MasterWeaver：驾驭可编辑性和身份</p></li><li><p>Authors: Shengyu Zhao, Yifan Jiang, Jingwen Chen, Yichang Shih, Zhe Gan, Lu Yuan, Xiaohui Shen, Bo Dai</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: Text-to-Image, Personalized Image Generation, Identity Control</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.05806.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 文本到图像（T2I）扩散模型在个性化文本到图像生成方面取得了显著成功，其目的是生成具有参考图像指示的人类身份的新颖图像。尽管几种无调优方法已经取得了有希望的身份保真度，但它们通常会出现过度拟合问题。学习到的身份往往会与无关信息纠缠在一起，导致文本可控性不佳，尤其是在人脸上。</p><p>(2): 现有的方法通常需要在训练或测试时进行微调，这会增加额外的时间和计算成本。此外，这些方法往往会过度拟合参考图像，导致生成图像缺乏多样性和可控性。</p><p>(3): 本文提出了一种名为 MasterWeaver 的测试时无调优方法，旨在生成具有高保真身份和可控文本的图像。MasterWeaver 通过在稳定扩散模型中引入一个身份映射器来实现，该映射器将参考图像的身份信息与文本信息融合在一起。身份映射器由一系列交叉注意块组成，这些块能够从参考图像中提取身份特征并将其与文本特征相结合，从而指导个性化生成。</p><p>(4): 在人脸数据集上的实验表明，MasterWeaver 在身份保真度和文本可控性方面都优于现有方法。此外，MasterWeaver 在生成多样化的图像方面也表现出良好的性能，即使在具有挑战性的提示下也是如此。这些结果支持了 MasterWeaver 在个性化文本到图像生成中的有效性和潜力。</p><ol><li>方法：</li></ol><p>（1）：提出MasterWeaver，一种无调优方法，通过在稳定扩散模型中引入身份映射器，将参考图像的身份信息与文本信息融合，实现个性化文本到图像生成；</p><p>（2）：身份映射器由一系列交叉注意块组成，能够从参考图像中提取身份特征并将其与文本特征相结合，指导个性化生成；</p><p>（3）：提出id-preserved editability learning，包括编辑方向损失和人脸增强数据集，以提高模型的可编辑性，同时保持身份保真度。</p><p><strong>8. 结论</strong></p><p>（1）：本文的意义在于提出了一种无调优方法MasterWeaver，该方法能够高效地生成具有真实身份和灵活可编辑性的个性化图像。提出的编辑方向损失和人脸增强数据集显著提高了模型的可编辑性，同时保持了身份保真度。大量的实验表明，我们的MasterWeaver优于最先进的方法，并且可以生成与身份和文本都相符的照片级真实图像。这种能力使我们的方法适用于各种应用，包括个性化数字内容创作和艺术创作。此外，所提出的编辑方向损失有可能应用于其他领域（例如动物和物体），从而增强其适用性。</p><p>（2）：创新点：提出了一种无调优方法MasterWeaver，通过在稳定扩散模型中引入身份映射器，将参考图像的身份信息与文本信息融合，实现个性化文本到图像生成。      性能：在人脸数据集上的实验表明，MasterWeaver在身份保真度和文本可控性方面都优于现有方法。此外，MasterWeaver在生成多样化的图像方面也表现出良好的性能，即使在具有挑战性的提示下也是如此。      工作量：MasterWeaver是一种无调优方法，不需要在训练或测试时进行微调，从而减少了额外的时间和计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-684afedc1936b936aaccddf56634d091.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3def950180a75e286f4491e85a1510be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cf287f35f0fb1e5e93d0b9d65a46a49e.jpg" align="middle"></details>## Sequential Amodal Segmentation via Cumulative Occlusion Learning**Authors:Jiayang Ao, Qiuhong Ke, Krista A. Ehinger**To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines. [PDF](http://arxiv.org/abs/2405.05791v1) **Summary**利用累积遮挡学习的扩散模型，针对不确定类别的物体顺序无模态分割。**Key Takeaways**- 本文提出了一种具有累积遮挡学习的扩散模型，用于不确定类别的物体顺序无模态分割。- 该模型在扩散过程中使用累积掩码策略迭代优化预测，有效地捕捉不可见区域的不确定性，并巧妙地再现被遮挡物体的形状和遮挡顺序的复杂分布。- 它类似于人类的无模态知觉能力，即破译物体之间的空间顺序，并准确预测密集分层视觉场景中被遮挡物体的完整轮廓。- 在三个无模态数据集上的实验结果表明，我们的方法优于已有的基线。- 该模型可以处理任何物体，而不仅仅是一组有限的物体类别。- 该模型对于机器人应用尤其有用。- 本文的工作对计算机视觉和机器人领域做出了贡献。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 基于扩散模型的顺序遮挡感知的无模态分割</p></li><li><p>Authors: Seunghyeok Back, Joosoon Lee, Taewon Kim, Sangjun Noh, Raeyoung Kang, Seongho Bak, Kyoobin Lee</p></li><li><p>Affiliation: 韩国科学技术院</p></li><li><p>Keywords: Amodal segmentation, Diffusion model, Occlusion perception, Computer vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.07993, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 对于理解复杂视觉场景（其中物体经常被遮挡）至关重要。</p><p>(2): 之前的无模态分割方法在处理未知物体类别和任意数量的遮挡层时存在局限性。</p><p>(3): 本文提出了一种基于扩散模型的无模态分割方法，该方法利用累积遮挡学习和基于扩散模型的掩码生成，可以实现鲁棒的遮挡感知和任意物体类别的无模态对象分割。</p><p>(4): 在三个公开的可用的无模态数据集上，该方法在产生合理多样化结果的同时，优于其他层感知无模态分割和扩散分割方法。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种基于扩散模型的无模态分割方法，该方法利用累积遮挡学习和基于扩散模型的掩码生成，可以实现鲁棒的遮挡感知和任意物体类别的无模态对象分割；</p><p>（2）：该方法引入累积掩码，它融合了对象的 spatial structures，促进了对可见和遮挡对象部分的理解；</p><p>（3）：该方法采用累积引导扩散，扩散过程由输入图像和来自先前层的动态更新的累积掩码提供信息，扩散仅扰动无模态掩码，保持图像和相应累积掩码的上下文和 spatial integrity 不变；</p><p>（4）：该方法提出累积遮挡学习算法，它采用分层程序，以有序感知的方式预测无模态掩码，它通过积累视觉信息来操作，其中观察到的数据（先前的分割掩码）的历史影响当前数据（要分割的当前对象）的感知；</p><p>（5）：该方法在训练中利用 ground truth 累积掩码作为输入，而在推理中使用前一层预测的掩码来构建累积掩码。</p><ol><li>结论：</li></ol><p>（1）：本文提出的基于扩散模型的无模态分割方法，利用累积遮挡学习和基于扩散模型的掩码生成，实现了鲁棒的遮挡感知和任意物体类别的无模态对象分割，对于理解复杂视觉场景至关重要。</p><p>（2）：创新点：提出了累积掩码和累积引导扩散，促进了对可见和遮挡对象部分的理解，并采用累积遮挡学习算法，以有序感知的方式预测无模态掩码；性能：在三个公开可用的无模态数据集上，该方法优于其他层感知无模态分割和扩散分割方法；工作量：该方法在训练中利用 ground truth 累积掩码作为输入，而在推理中使用前一层预测的掩码来构建累积掩码。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-94d0f4cb7c590ef60771afc2db0e19f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-751b43417d9c46f9ccbe1b00ac7c3da2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-956b6f93209ac841263fddf5f8097796.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2a455d981e097468b0764d82f2edcafc.jpg" align="middle"></details>## LatentColorization: Latent Diffusion-Based Speaker Video Colorization**Authors:Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran**While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM. [PDF](http://arxiv.org/abs/2405.05707v1) **Summary**利用改进的隐扩散模型解决视频着色中的时间一致性问题，实现比现有方法更好的图像质量和用户偏好。**Key Takeaways**- 视频着色领域尚未得到充分探索。- 现有视频着色技术通常按帧处理，忽略了时间一致性。- 这会导致帧间闪烁或突然的颜色过渡，影响质量。- 研究者提出了一种改进的隐扩散模型，专门用于视频着色。- 该模型通过引入时间一致性机制解决了时间不一致问题。- 模型在图像质量指标上优于现有方法，并且在主观研究中得到用户偏好。- 研究者使用电视/电影视频扩展了数据集，证明了该方法的有效性。- 模型地址：https://youtu.be/vDbzsZdFuxM**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 潜色化：基于潜在扩散的说话者视频着色</p></li><li><p>Authors: Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran</p></li><li><p>Affiliation: 爱尔兰高威大学人工智能数据科学研究所</p></li><li><p>Keywords: 人工智能，人工神经网络，机器学习，计算机视觉，视频着色，潜在扩散，图像着色</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05707 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 当前的研究主要集中在基于图像的着色上，而基于视频的着色领域仍然相对未被探索。大多数现有的视频着色技术都是逐帧进行的，常常忽略了连续帧之间的时间连贯性这一关键方面。这种方法会导致帧之间出现不一致，从而导致闪烁或帧之间突然的色彩转换等不良效果。</p><p>(2): 过去的方法：大多数现有的视频着色技术都是逐帧进行的，常常忽略了连续帧之间的时间连贯性这一关键方面。这种方法会导致帧之间出现不一致，从而导致闪烁或帧之间突然的色彩转换等不良效果。问题：这种方法无法保证视频中连续帧之间的一致性，导致视频着色结果出现闪烁或突然的色彩转换等问题。动机：为了解决这些问题，本文提出了一种基于潜在扩散的视频着色方法，该方法能够保证视频中连续帧之间的一致性，并提高视频着色的图像质量。</p><p>(3): 本文提出了一种基于潜在扩散的视频着色方法，该方法通过对潜在扩散模型进行微调，使其能够专门用于视频着色。该方法通过引入一种新的机制来实现视频着色的时间一致性，并通过与其他现有方法的比较，在既定的图像质量指标上展示了显著的改进。此外，本文还进行了一项主观研究，结果表明用户更喜欢本文的方法，而不是现有的最先进的方法。本文的数据集包含了传统数据集和来自电视/电影的视频的组合。简而言之，通过利用经过微调的基于潜在扩散的着色系统和时间一致性机制，我们可以通过解决时间不一致性问题来提高自动视频着色的性能。</p><p>(4): 本文的方法在视频着色任务上取得了较好的性能，在图像质量指标上优于其他现有方法。这些性能支持了本文的目标，即开发一种能够保证视频中连续帧之间一致性并提高视频着色图像质量的视频着色方法。</p><ol><li>方法：</li></ol><p>(1)：本文提出了一种基于潜在扩散的视频着色方法，该方法通过对潜在扩散模型进行微调，使其能够专门用于视频着色。</p><p>(2)：该方法通过引入一种新的机制来实现视频着色的时间一致性，该机制通过对连续帧之间的特征进行对齐，确保了视频中连续帧之间的颜色转换平滑且一致。</p><p>(3)：该方法还利用了预训练的图像着色模型，该模型能够提供丰富的颜色信息，从而提高了视频着色的图像质量。</p><ol><li>结论：</li></ol><p>（1）：本研究证明了基于扩散的模型，特别是 LatentColorization 方法，在多个数据集上取得了与最先进水平相当的结果。值得注意的是，该系统在“Sherlock Holmes Movie”数据集上执行与人类水平相当的着色，表明其实际意义和特定应用视频着色的潜力。使用潜在扩散模型并结合时间一致的着色方法有助于产生逼真且令人信服的着色结果，从而使该过程更容易获取并减少对传统人工着色方法的依赖。这项研究提供了对扩散模型在视频着色中的潜力的见解，并为该领域进一步发展提供了机会。</p><p>（2）：创新点：提出了基于潜在扩散的视频着色方法，该方法通过引入一种新的机制来实现视频着色的时间一致性，确保了视频中连续帧之间的颜色转换平滑且一致。性能：在图像质量指标上优于其他现有方法。工作量：需要对潜在扩散模型进行微调，并引入新的机制来实现视频着色的时间一致性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f0bd89362865676c3ad7cf0d3f166a40.jpg" align="middle"><img src="https://pica.zhimg.com/v2-21100a3a0bd62ff092c190f5e11319a1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4524df6fd58a948107fed7f87b72c40d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-92e0eda0c525c56953a393c555231b1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44fa6c9863fd59510688ab85ad89e94c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-054ffb578d7f1a7808690325c49f8793.jpg" align="middle"></details>## Attention-Driven Training-Free Efficiency Enhancement of Diffusion   Models**Authors:Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu**Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io. [PDF](http://arxiv.org/abs/2405.05252v1) Accepted to IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2024**Summary**无需额外训练，注意力驱动的高效扩散模型可以高效生成高质量图像。**Key Takeaways**- 引入 AT-EDM 框架，利用注意力图在运行时剪除冗余标记，无需重新训练。- 开发了广义加权页面排名 (G-WPR) 算法，用于识别冗余标记。- 提出了一种基于相似性的恢复方法，用于恢复卷积操作的标记。- 提出了一种去噪步骤感知剪枝 (DSAP) 方法，用于调整不同去噪时间步的剪枝预算，以获得更好的生成质量。- 与现有方法相比，AT-EDM 在效率方面表现出色，同时保持与完整模型几乎相同的 FID 和 CLIP 分数。- AT-EDM 节省了约 38.8% 的 FLOPs，与 Stable Diffusion XL 相比，速度提高了 1.53 倍。- AT-EDM 项目网页：https://atedm.github.io。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 注意力驱动的无训练效率增强扩散模型</p></li><li><p>Authors: Yifan Liu, Yixing Xu, Zizhao Zhang, Zhihao Xia, Qinghe Xiao, Xiyang Dai, Xianglong Liu, Xiaoguang Han</p></li><li><p>Affiliation: 中国科学院自动化研究所</p></li><li><p>Keywords: Diffusion Models, Attention Pruning, Efficient Inference, Generative Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.00297, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型 (DM) 在生成高质量且多样化的图像方面表现出优异的性能。然而，这种卓越的性能是以昂贵的架构设计为代价的，特别是由于领先模型中大量使用的注意力模块。</p><p>(2): 现有工作主要采用再训练过程来提高 DM 效率。这是计算成本高昂且可扩展性不强的。</p><p>(3): 提出了一种注意力驱动的无训练高效扩散模型 (AT-EDM) 框架，该框架利用注意力图在运行时对冗余标记进行修剪，而无需任何再训练。具体来说，对于单去噪步骤修剪，开发了一种新颖的排名算法，即广义加权页面排名 (GWPR)，以识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记。此外，提出了一种去噪步骤感知修剪 (DSAP) 方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。</p><p>(4): 广泛的评估表明，AT-EDM 在效率方面优于现有技术（例如，比 Stable Diffusion XL 节省 38.8% 的 FLOP，速度提高 1.53 倍），同时保持与完整模型几乎相同的 FID 和 CLIP 分数。</p><ol><li><p>方法：</p><pre><code>            (1): 本文提出了一种注意力驱动的无训练高效扩散模型（AT-EDM）框架，利用注意力图在运行时对冗余标记进行修剪，而无需任何再训练。            (2): 具体来说，对于单去噪步骤修剪，开发了一种新颖的排名算法，即广义加权页面排名（GWPR），以识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记。            (3): 此外，提出了一种去噪步骤感知修剪（DSAP）方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了 AT-EDM，这是一种无需重新训练即可在运行时加速 DM 的新颖框架。AT-EDM 有两个组成部分：单去噪步骤标记修剪算法和跨步长修剪调度（DSAP）。在单去噪步骤标记修剪中，AT-EDM 利用预训练 DM 中的注意力图来识别不重要的标记并对其进行修剪，以加速生成过程。为了使修剪后的特征图与后面的卷积块兼容，AT-EDM 再次使用注意力图来揭示标记之间的相似性，并将相似的标记复制到恢复被修剪的标记。DSAP 进一步提高了 AT-EDM 的生成质量。我们发现这样的修剪计划也可以应用于其他方法，如 ToMe。实验结果证明了 AT-EDM 在图像质量和文本图像对齐方面优于最先进的方法。具体来说，在 SD-XL 上，AT-EDM 节省了 38.8% 的 FLOP，速度提高了 1.53 倍，同时获得了与全尺寸模型几乎相同的 FID 和 CLIP 分数，优于现有技术。致谢 本工作得到了 Adobe 夏季实习和美国国家科学基金会 (NSF) 赠款号 CCF2203399 的部分支持。</p><p>（2）：创新点：提出了 AT-EDM，一种无需重新训练即可在运行时加速 DM 的新颖框架；提出了广义加权页面排名 (GWPR) 算法来识别冗余标记，以及一种基于相似性的恢复方法来恢复卷积操作的标记；提出了去噪步骤感知修剪 (DSAP) 方法来调整不同去噪时间步长的修剪预算，以获得更好的生成质量。性能：在图像质量和文本图像对齐方面优于最先进的方法；在 SD-XL 上，AT-EDM 节省了 38.8% 的 FLOP，速度提高了 1.53 倍，同时获得了与全尺寸模型几乎相同的 FID 和 CLIP 分数。工作量：无需重新训练，在运行时进行修剪，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6302d3af65dff156f4dfb4a4f61beb6d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fba65c3201705c21fc2eca18ff6f04d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8d2386a34dc82ffa216c8bf65b38b88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-acd6df588aee8826ba26459dc11db84e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-52944c1f09cf54389358c76e65089ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d0588ac58cf9e6ab928168c2e4ee2de.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-260d5649d487707bdcdd240fe08bbe3e.jpg" align="middle"></details>## Imagine Flash: Accelerating Emu Diffusion Models with Backward   Distillation**Authors:Jonas Kohler, Albert Pumarola, Edgar Schönfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet**Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation. [PDF](http://arxiv.org/abs/2405.05224v1) **Summary**扩散模型是一种强大的生成框架，但推理成本昂贵。**Key Takeaways**- 结合后向蒸馏、移位重建损失和噪声校正的三步蒸馏框架。- 后向蒸馏通过在学生自己的后向轨迹上校准来减轻训练推理差异。- 移位重建损失根据当前时间步长动态调整知识转移。- 噪声校正通过解决噪声预测中的奇点来增强样本质量。- 在定量指标和人工评估中优于现有竞争对手。- 使用仅三个去噪步骤即可实现与教师模型相当的性能，从而实现高效的高质量生成。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：想象闪光：加速 Emu 扩散</p></li><li><p>作者：Jonas Kohler，Albert Pumarola，Edgar Schönfeld，Artsiom Sanakoyeu，Roshan Sumbaly，Peter Vajda 和 Ali Thabet</p></li><li><p>隶属关系：GenAI，Meta</p></li><li><p>关键词：Diffusion Models，Distillation，Image Generation，Inference Acceleration</p></li><li><p>论文链接：https://arxiv.org/abs/2405.05224，Github 代码链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：扩散模型是一种强大的生成框架，但推理成本昂贵。现有的加速方法通常会影响图像质量，或者在极低步长条件下进行复杂条件处理时会失败。</p><p>（2）：过去方法：现有方法包括量化、知识蒸馏和训练-推理不匹配校正。但它们在保持图像质量的同时实现极低步长推理方面存在局限性。</p><p>（3）：研究方法：本文提出了一种新颖的蒸馏框架，旨在使用一到三步实现高保真、多样化的样本生成。该方法包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。</p><p>（4）：方法性能：在广泛的实验中，本文的方法在定量指标和定性评估中均优于现有竞争对手。在 ImageNet-64 数据集上，使用 1 步时 FID 为 5.57，使用 3 步时 FID 为 4.69。这些结果表明，该方法可以有效加速扩散模型的推理，同时保持图像质量。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新的蒸馏框架，该框架旨在使用一到三步实现高保真、多样化的样本生成。</p><p>（2）：该框架包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。</p><p>（3）：反向蒸馏：使用教师模型的输出作为学生模型的输入，通过最小化学生模型输出与教师模型输出之间的差异来训练学生模型。</p><p>（4）：移位重建损失：引入了一种新的损失函数，该函数鼓励学生模型重建教师模型在不同步长下的输出。</p><p>（5）：噪声校正：应用了一种噪声校正机制，该机制通过添加噪声来平滑学生模型的输出，从而提高图像质量。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了 Imagine Flash，这是一种新颖的蒸馏框架，能够使用扩散模型进行高保真、少步图像生成。我们的方法包含三个关键组成部分：反向蒸馏以减少训练-推理差异，一个动态调整每个时间步长知识转移的移位重建损失（SRL），以及用于提高图像质量的噪声校正。通过广泛的实验，Imagine Flash 取得了显著的成果，仅使用三个去噪步骤即可与预训练教师模型的性能相匹配，并始终超越现有方法。这种前所未有的采样效率与高样本质量和多样性相结合，使我们的模型非常适合实时生成应用程序。我们的工作为超高效生成建模铺平了道路。未来的研究方向包括扩展到视频和 3D 等其他模态，进一步减少采样预算，以及将我们的方法与互补的加速技术相结合。通过启用即时高保真生成，Imagine Flash 为实时创意工作流和互动媒体体验开启了新的可能性。</p><p>（2）：创新点：提出了一种新的蒸馏框架，该框架旨在使用一到三步实现高保真、多样化的样本生成。该框架包括三个关键组成部分：反向蒸馏、移位重建损失和噪声校正。；性能：在 ImageNet-64 数据集上，使用 1 步时 FID 为 5.57，使用 3 步时 FID 为 4.69。这些结果表明，该方法可以有效加速扩散模型的推理，同时保持图像质量。；工作量：该方法在定量指标和定性评估中均优于现有竞争对手。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5adaf43fae278fddba0258413307ece.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3e21e1e5b67cda3d8658d86e6854e63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c4cddde7364b59b3aef7c475c750db.jpg" align="middle"></details>## FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via   Diffusion Models**Authors:Jinglin Xu, Yijie Guo, Yuxin Peng**The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024. [PDF](http://arxiv.org/abs/2405.05216v1) Accepted by CVPR 2024**Summary**利用扩散模型的细粒度提示驱动的去噪器，实现了3D人体姿态估计的细粒度引导。**Key Takeaways**- 通过文本和人体知识生成细粒度提示，提供隐式监督，增强 3D HPE。- 建立提示和姿势之间的细粒度通信，提高去噪质量。- 引入时间信息，实现去噪过程的自适应调整。- FinePOSE 在单人和多人姿态估计数据集上均达到 SOTA 性能。- 细粒度提示提供了对不同身体部位的细致指导。- 提示驱动的去噪器使 3D HPE 更好地利用文本知识。- FinePOSE 扩展到多人体姿态估计，增强了复杂场景下的处理能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 精细提示驱动的扩散模型在三维人体姿态估计中的应用</p></li><li><p>Authors: Yuxin Sun, Yajie Zhao, Yifan Zhang, Xiangyang Xue, Jian Cheng</p></li><li><p>Affiliation: 北京大学信息科学技术学院</p></li><li><p>Keywords: 3D Human Pose Estimation, Diffusion Model, Prompt Learning, Fine-grained Guidance</p></li><li><p>Urls: https://arxiv.org/abs/2302.06039, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 三维人体姿态估计（3D HPE）任务利用二维图像或视频预测三维空间中的人体关节坐标。尽管基于深度学习的方法最近取得了进展，但它们大多忽略了将可访问文本和人类自然可行的知识相结合的能力，错失了有价值的隐式监督来指导 3D HPE 任务。此外，以前的研究通常从整个人体的角度研究该任务，忽略了隐藏在不同身体部位中的细粒度指导。</p><p>(2): 现有的方法主要从整个人体的角度研究 3D HPE 任务，忽略了隐藏在不同身体部位中的细粒度指导。此外，现有的方法通常依赖于手工制作的提示，这限制了它们对复杂姿势和动作建模的能力。</p><p>(3): 为了解决这些问题，本文提出了一种基于扩散模型的新型精细提示驱动的去噪器，用于 3D HPE，名为 FinePOSE。它由三个核心模块组成，增强了扩散模型的反向过程：（1）精细部分感知提示学习（FPP）模块通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合来构建精细的部分感知提示，以建模隐式指导。（2）精细提示姿态通信（FPC）模块在学习的部分感知提示和姿态之间建立细粒度通信，以提高去噪质量。（3）提示驱动的时序风格化（PTS）模块集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>(4): 在公共单人姿态估计数据集上的广泛实验表明，FinePOSE 优于最先进的方法。我们进一步将 FinePOSE 扩展到多人姿态估计。在 EgoHumans 数据集上实现 34.3mm 的平均 MPJPE，证明了 FinePOSE 处理复杂多人场景的潜力。</p><ol><li>方法：</li></ol><p>（1）：提出一种基于扩散模型的新型精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务；</p><p>（2）：FinePOSE 由三个核心模块组成：精细部分感知提示学习（FPP）模块、精细提示姿态通信（FPC）模块和提示驱动的时序风格化（PTS）模块；</p><p>（3）：FPP 模块通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合来构建精细的部分感知提示，以建模隐式指导；</p><p>（4）：FPC 模块在学习的部分感知提示和姿态之间建立细粒度通信，以提高去噪质量；</p><p>（5）：PTS 模块集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于扩散模型的精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务。FinePOSE 通过将可访问的文本和身体部位的自然可行知识与可学习提示相结合，构建了精细的部分感知提示，以建模隐式指导。此外，FinePOSE 建立了学习的部分感知提示和姿态之间的细粒度通信，并集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>（2）：创新点：本文提出了一种新的精细提示驱动的去噪器 FinePOSE，用于 3D HPE 任务。FinePOSE 利用可访问的文本和身体部位的自然可行知识，构建了精细的部分感知提示，并建立了学习的部分感知提示和姿态之间的细粒度通信，以提高去噪质量。此外，FinePOSE 集成了学习的提示嵌入和与噪声级别相关的时序信息，以在每个去噪步骤中进行自适应调整。</p><p>性能：在公共单人姿态估计数据集上的广泛实验表明，FinePOSE 优于最先进的方法。我们进一步将 FinePOSE 扩展到多人姿态估计。在 EgoHumans 数据集上实现 34.3mm 的平均 MPJPE，证明了 FinePOSE 处理复杂多人场景的潜力。</p><p>工作量：FinePOSE 的实现相对简单，易于部署和使用。然而，构建精细的部分感知提示和建立学习的部分感知提示和姿态之间的细粒度通信需要额外的计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-30db10978f08bca4adc049e2f667efa7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58a38ed55c5b15686ce6cec8b0354b7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e88a5a9d36fae50e0d57909271d5070e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186dcaee258d539e4f830d471e6a2c6e.jpg" align="middle"></details>## Fast LiDAR Upsampling using Conditional Diffusion Models**Authors:Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim Tørresen, Ryo Kurazume**The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments. [PDF](http://arxiv.org/abs/2405.04889v1) **摘要**条件扩散模型用于三维场景点云的高效且高质量稀疏到稠密上采样。**要点*** 扩散模型可用于高保真地生成精炼的激光雷达数据。* 现有方法受限于性能和速度，难以实时执行。* 本文提出了一种基于条件扩散模型的新方法，用于通过图像表示快速、高质量地对三维场景点云进行稀疏到稠密上采样。* 该方法采用使用条件内插掩码训练的去噪扩散概率模型，该模型在图像完成任务上表现出高性能。* 实验表明，该方法在采样速度和质量上优于基线。* 该方法可以通过同时在真实世界和合成数据集上进行训练来展示泛化能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title:使用条件扩散模型进行快速激光雷达上采样</li><li>Authors: Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim Tørresen, Ryo Kurazume</li><li>Affiliation: 奥斯陆大学信息学系</li><li>Keywords: 3D LiDAR, Conditional Diffusion Models, Image-based LiDAR data generation, Deep generative models</li><li>Urls: Paper: https://arxiv.org/abs/2405.04889v1, Github: None</li><li>Summary:</li></ol><p>(1):本文的研究背景是，由于硬件限制和激光雷达传感器的成本，测量数据的质量和密度差异很大，这可能导致语义分割和目标检测等技术性能不一致，这对操作机器人来说不是最优的。</p><p>(2):过去的方法是使用无条件扩散模型解决上采样任务，但这些方法涉及复杂的过程，导致推理时间慢，不适合实时机器人导航管道。</p><p>(3):本文提出的研究方法是在图像表示中建立条件扩散模型，学习给定部分观察的激光雷达数据生成。</p><p>(4):本文方法在KITTI-360数据集上使用上采样任务，在采样速度和质量上优于基线。此外，还展示了该方法通过同时训练真实世界和合成数据集，引入质量和环境的变化，从而具有泛化能力。</p><ol><li>方法：</li></ol><p>（1）：在图像表示中建立条件扩散模型，学习给定部分观察的激光雷达数据生成；</p><p>（2）：使用KITTI-360数据集上采样任务，评估模型性能；</p><p>（3）：通过同时训练真实世界和合成数据集，引入质量和环境的变化，提高模型泛化能力。</p><ol><li>结论：<pre><code>           (1):本文提出了一种基于图像表示的条件扩散模型，可以快速生成给定部分观察的激光雷达数据，为提高激光雷达数据质量和密度提供了新的方法；           (2):创新点：提出了一种基于图像表示的条件扩散模型，该模型能够快速生成给定部分观察的激光雷达数据，并通过同时训练真实世界和合成数据集提高模型泛化能力；性能：在KITTI-360数据集上采样任务，该方法在采样速度和质量上优于基线；工作量：该方法涉及复杂的过程，导致推理时间慢，不适合实时机器人导航管道。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-767dd6ead611c7fe6a1bf019a995a405.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c74edffd87cfd5517cc664cb78371d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21eb96ed158007c33f9b04a7adf7ab5c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-82f76f778074704994e29c06d305ad3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-948663529c880472c0969ced23400b05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e781edb01423cfd0018bde5e4738157d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-614fee94f5b68257335000a86b6b2590.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/</id>
    <published>2024-05-06T10:42:27.000Z</published>
    <updated>2024-05-06T10:42:27.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights"><a href="#WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights" class="headerlink" title="WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights"></a>WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights</h2><p><strong>Authors:Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim</strong></p><p>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2405.02066v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 水印可同时适用于隐式和显式 NeRF 表示，以保证 NeRF 的版权保护。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 水印是保护 NeRF 版权的关键解决方案。</li><li>该方法适用于隐式和显式 NeRF 表示。</li><li>该方法使用离散小波变换进行水印。</li><li>该方法采用延迟反向传播，提高渲染质量和比特精度。</li><li>该方法在容量、不可见性和鲁棒性方面均取得了最先进的性能。</li><li>该方法比现有的最先进方法具有更快的训练速度。</li><li>该方法通过微调 NeRF 在渲染过程中嵌入二进制信息来实现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：WateRF：用于版权保护的辐射场中的鲁棒水印</p></li><li><p>作者：Youngdong Jang、Dong In Lee、MinHyuk Jang、Jong Wook Kim、Feng Yang、Sangpil Kim</p></li><li><p>隶属机构：韩国大学</p></li><li><p>关键词：神经辐射场、水印、版权保护、隐式表示、显式表示</p></li><li><p>论文链接：https://kuai-lab.github.io/cvpr2024waterf/，Github 链接：无</p></li><li><p>摘要：</p><p>（1）：研究背景：神经辐射场（NeRF）在 3D 内容创建和 3D 建模中发挥着重要作用，但保护其版权尚未得到深入研究。NeRF 水印被认为是安全部署基于 NeRF 的 3D 表示的关键解决方案之一。</p><p>（2）：过去的方法：现有方法仅适用于隐式或显式 NeRF 表示。它们的问题在于无法同时应用于两种表示。</p><p>（3）：研究方法：本文提出了一种创新的水印方法，可以应用于 NeRF 的两种表示。该方法通过微调 NeRF 在渲染过程中嵌入二进制消息来实现。具体来说，我们提出利用 NeRF 空间中的离散小波变换进行水印。此外，我们采用延迟反向传播技术，并引入与逐块损失相结合的方法，以在最小权衡下提高渲染质量和比特准确性。</p><p>（4）：任务和性能：我们在三个不同方面评估了我们的方法：容量、不可见性和嵌入在 2D 渲染图像中的水印的鲁棒性。我们的方法在与最先进的方法相比之下，以更快的训练速度实现了最先进的性能，从而证明了其有效性。</p></li><li><p>Methods:</p></li></ol><p>（1）：提出了一种微调 NeRF 的方法，该方法不涉及改变模型的架构来嵌入水印消息。</p><p>（2）：我们的方法旨在将水印嵌入到 NeRF 模型的权重 θ 中，在渲染图像的频域中。</p><p>（3）：我们的方法不同于传统的数字水印方法，它专注于训练编码器和解码器。不同之处在于微调过程，它在不使用编码器的情况下嵌入水印。</p><p>（4）：有 2 个阶段：（1）预训练水印解码器 D，（2）微调 NeRF 模型 Fθ 以嵌入消息。</p><p>（5）：我们的方法如图 2 所示，并在下文详细描述。</p><p>（6）：预训练水印解码器：我们选择 HiDDeN [58] 架构作为我们的水印解码器。HiDDeN 包含两个用于数据隐藏的卷积网络：水印编码器 E 和水印解码器 D。为了鲁棒性，它包括一个噪声层 N。然而，在这个仅关注解码器性能的训练阶段，我们排除了负责提高视觉质量的对偶损失。在训练完 HiDDeN 模型后，水印编码器 E 在第二阶段没有被使用。</p><p>（7）：编码器 E 以封面图像 Io ∈ RH×W ×3 和长度为 L 的二进制消息 M ∈ {0, 1}L 为输入。然后 E 将 M 嵌入到 Io 中并生成编码图像 Iw。为了使解码器对旋转和 JPEG 压缩等各种失真具有鲁棒性，Iw 使用噪声层 N 进行转换。由多个卷积层组成的解码器 D 以 Iw 为输入，并提取消息 M′。</p><p>（8）：M′ = D(N(Iw))</p><p>（9）：我们利用 sigmoid 函数将提取的消息 M′ 的范围设置为 [0, 1]。消息损失使用 ML 和 sigmoid sg(M′L) 之间的二元交叉熵 (BCE) 计算。</p><p>（10）：Lmessage = − L∑i=1 Mi · log sg(M′i) + (1 − Mi) · log(1 − sg(M′i)))</p><p>（11）：解码器经过训练，可以检测经过训练编码器处理的图像中的水印。然而，我们在第二阶段不使用编码器。我们发现，当解码器接收到香草渲染的图像时，提取的消息位之间存在偏差。因此，在训练解码器后，我们对线性解码器层进行 PCA 白化以消除偏差，同时不降低提取能力。</p><p>（12）：在 DWT 上嵌入和提取水印：在空间域中加水印是一种相对简单的方法，因为它在整个图像中嵌入水印。最近，一种在空间域中对 NeRF 进行微调的水印方法 [16] 浮出水面。尽管在空间域中嵌入消息的微调方法显示出无与伦比的不可见性和消息提取能力，但它容易受到扭曲空间域的攻击，例如裁剪。直接应用来自潜在扩散模型 [7] 的空间域技术不允许有效调整 NeRF 的权重。为了解决这些问题，我们提出了一种在频域而不是空间域中的微调方法。多年来，各种图像水印技术使用频域，包括离散余弦变换 (DCT) 和离散小波变换 (DWT)，取得了持续的发展和改进。我们发现 DWT 是将消息编码到 NeRF 模型权重中的合适域。给定相应的相机参数，NeRF 模型渲染 3D 模型的不同视图。我们将渲染图像的像素，表示为 X = (xc, yc) ∈ RH×W ×3，转换为小波形式，其中 c 表示通道。DWT 定义为 [10]：</p><p>（13）：Wφ(j0, m, n) = 1√MN∑M−1xc=0∑N−1yc=0 f(xc, yc)φj0,m,n(xc, yc),</p><p>（14）：W i ψ(j, m, n) = 1√MN∑M−1xc=0∑N−1yc=0 f(xc, yc)ψi j,m,n(xc, yc)</p><p>（15）：其中 φ(x, y) 是尺度函数，ψ(x, y) 是小波函数。Wφ(j0, m, n) 被 LL 子带调用，它是图像在尺度 j0 的近似值，W i ψ 其中 i = {H, V, D} 分别表示 LH、HL、HH 子带。先前的研究选择 LH、HL 和 HH 子带来嵌入水印，因为 LL 子带包含图像的重要信息。然而，我们选择 LL 子带作为解码器 D 的输入，并用 M′ = D(Wφ) 获取提取的消息。使用 HiDDeN 解码器，我们通过实验发现，在 LL 子带中嵌入水印比其他子带更稳健，并且更有效地嵌入水印信息。DWT 的特点是其子带在不同级别上计算；因此，为我们的目的选择一个最佳级别是必要的。1 级将图像分成 4 个子带 (LL1, LH1, HL1, HH1)，</p><ol><li>结论：</li></ol><p>（1）：本文提出的方法将图像从空间域转换到频域，有效地将水印编码到图像中。我们发现，离散小波变换（DWT）变换和逐块损失可以提高整体图像质量。</p><p>（2）：创新点：提出了一种神经 3D 水印方法，用于 NeRF 模型。我们的方法分别训练 2D 水印解码器和 NeRF 模型。因此，我们的流水线只需要训练一次解码器，并在不同的 NeRF 水印模型上重复使用它。我们采用图像水印中的传统水印技术，将图像从空间域转换到频域，以有效地将水印编码到图像中。我们发现，离散小波变换（DWT）变换和逐块损失可以提高整体图像质量。</p><p>性能：与最先进的方法相比，我们的方法以更快的训练速度实现了最先进的性能，从而证明了其有效性。</p><p>工作量：我们的方法在训练和嵌入水印方面具有较低的计算成本，使其适用于实际应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-21a78eb3599c5468a4ea257df96b8cdc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59110a6f2727d6c4ae7b744d2459165a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-59cf18deef7514767b02ec7654c8da33.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9d7125c0e81d7d4fa4f485a0ca63c94.jpg" align="middle"></details><h2 id="Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy"><a href="#Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy" class="headerlink" title="Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy"></a>Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy</h2><p><strong>Authors:Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</strong></p><p>Action recognition has become one of the popular research topics in computer vision. There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances. However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction. In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy. Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets. Therefore, the contributions in this work are three-fold. Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition. Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy. Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields. Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400. </p><p><a href="http://arxiv.org/abs/2405.01337v1">PDF</a> </p><p><strong>Summary</strong><br>基于多视图注意力一致性和神经辐射场，提出时空一致动作识别新方法，实现动作识别领域最优结果。</p><p><strong>Key Takeaways</strong></p><ul><li>提出多视图注意力一致性解决动作识别合理预测问题。</li><li>定义基于有向格罗莫夫-瓦瑟斯坦距离的多视图一致注意力度量。</li><li>基于视频变形金刚和神经辐射场构建动作识别模型。</li><li>在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上达到最优结果。</li><li>创新性地引入了多视图注意力一致性，解决了动作识别中合理预测的难题。</li><li>采用新颖的度量方法评估多视图一致注意力。</li><li>将神经辐射场应用于动作识别，提升了模型在单视图数据集上的泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：多视角动作识别经由定向格罗莫夫-沃瑟斯坦差异</p></li><li><p>作者：Hoang-Quan Nguyen，Thanh-Dat Truong，Khoa Luu</p></li><li><p>单位：阿肯色大学计算机视觉与图像理解实验室</p></li><li><p>关键词：动作识别，多视角注意力一致性，定向格罗莫夫-沃瑟斯坦差异，神经辐射场</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01337</p></li><li><p>摘要：</p></li></ol><p>(1)：动作识别是计算机视觉领域的研究热点，现有的基于卷积神经网络和自注意力机制（如 Transformer）的方法在解决动作识别任务的时空维度问题上取得了竞争力的性能。然而，这些方法缺乏对模型关注的动作主体正确性的保证，即如何确保动作识别模型关注适当的动作主体以做出合理的动作预测。</p><p>(2)：以往方法主要基于卷积神经网络和自注意力机制，但缺乏对模型关注的动作主体正确性的保证。本文提出的方法动机明确，旨在解决动作识别中合理预测的问题。</p><p>(3)：本文提出了一种多视角注意力一致性方法，利用定向格罗莫夫-沃瑟斯坦差异计算动作视频两个不同视角的两个注意力的相似性。此外，该方法应用神经辐射场的思想，在单视角数据集上训练时隐式渲染新视角的特征。</p><p>(4)：该方法在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上取得了最先进的结果，证明了其性能可以支持其目标。</p><ol><li>Methods:</li></ol><p>(1):使用 Video Transformer 框架进行动作识别，将视频分解为 patches 并进行位置编码，然后使用 Transformer 编码器提取特征；</p><p>(2):采用 Neural Radiance Field 的思想，通过 StyleNeRF 将特征体映射到风格向量，并调节 NeRF 模块中 MLP 层的权重矩阵，以在新的视角下渲染低分辨率特征体；</p><p>(3):使用定向格罗莫夫-沃瑟斯坦差异（Directed Gromov-Wasserstein Discrepancy）计算不同视角下动作视频的两个注意力的相似性，该方法通过计算两个空间内定义的度量之间的相似性来比较分布，对相机平移引起的注意力图转换具有鲁棒性。</p><ol><li>结论：</li></ol><p>(1) 本文提出的多视角注意力一致性方法，通过定向格罗莫夫-沃瑟斯坦差异计算不同视角下动作视频的两个注意力的相似性，解决了动作识别中合理预测的问题，取得了最先进的结果。</p><p>(2) 创新点：提出多视角注意力一致性方法，利用定向格罗莫夫-沃瑟斯坦差异计算注意力相似性；性能：在 Jester、Something-Something V2 和 Kinetics-400 三个大规模数据集上取得最先进的结果；工作量：需要训练 Neural Radiance Field 模块，计算定向格罗莫夫-沃瑟斯坦差异。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c677262cde72d554d4ab784234b1941b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59ee15896ae28d3e32188fedbfd5bc0d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-06  WateRF Robust Watermarks in Radiance Fields for Protection of   Copyrights</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/</id>
    <published>2024-05-06T10:35:16.000Z</published>
    <updated>2024-05-06T10:35:16.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-06</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/</id>
    <published>2024-05-06T10:33:19.000Z</published>
    <updated>2024-05-06T10:33:19.431Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations"><a href="#CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations" class="headerlink" title="CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations"></a>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</h2><p><strong>Authors:Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</strong></p><p>Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at <a href="https://aka.ms/covomix">https://aka.ms/covomix</a>. </p><p><a href="http://arxiv.org/abs/2404.06690v1">PDF</a> </p><p><strong>Summary</strong><br>利用CoVoMix，一种零样本对话语音合成模型，可以生成逼真、连贯且多回合的多人对话语音。</p><p><strong>Key Takeaways</strong></p><ul><li>CoVoMix是一款零样本对话语音合成模型。</li><li>CoVoMix能够将对话文本转换为离散标记流，表示各个说话者的语义信息。</li><li>流匹配声学模型将标记流转换成混合的梅尔频谱图。</li><li>HiFi-GAN模型将梅尔频谱图转换为语音波形。</li><li>CoVoMix采用了一套综合的指标来衡量对话建模和生成的有效性。</li><li>实验结果表明，CoVoMix生成的对话具有逼真的自然性和连贯性，并且涉及多个说话者参与多轮对话。</li><li>生成的对话在单个通道内，具有无缝的语音转换（包括重叠语音）和适当的副语言行为（例如笑声）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CoVoMix: 推进零样本语音生成以实现类人多说话者对话</p></li><li><p>Authors: Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</p></li><li><p>Affiliation: 上海交通大学</p></li><li><p>Keywords: 零样本语音生成，多说话者对话，语音合成，自然语言处理</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.06690 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：近年来，零样本文本到语音 (TTS) 建模取得了重大进展，在生成高保真和多样化的语音方面取得了显著进展。然而，对话生成以及在语音中实现类人的自然性仍然是该领域的挑战。</p><p>(2): 过去的方法：过去的方法通常使用单流文本到语音模型，无法生成多说话者对话。此外，这些方法通常需要大量标记数据，这在对话场景中可能不可用。</p><p>(3): 本文提出的研究方法：本文提出了一种名为 CoVoMix 的模型，用于零样本、类人、多说话者、多轮对话语音生成。CoVoMix 能够首先将对话文本转换为多个离散令牌流，每个令牌流表示单个说话者的语义信息。然后将这些令牌流馈送到基于流匹配的声学模型中以生成混合梅尔谱图。最后，使用 HiFi-GAN 模型生成语音波形。</p><p>(4): 实验结果：实验结果表明，CoVoMix 可以生成不仅在自然性和连贯性上类似人类的对话，而且还涉及多个说话者进行多轮对话。这些在单个通道内生成的对话具有无缝的语音转换，包括重叠语音和适当的副语言行为，例如笑声。</p><ol><li>研究方法：</li></ol><p>（1）：多流文本到语义模型：基于编码器-解码器架构，将文本标记序列转换为多个离散标记流，每个标记流表示单个说话者的语义信息。</p><p>（2）：声学模型：基于流匹配的变压器编码器，将语义序列转换为混合梅尔谱图。</p><p>（3）：声码器：使用 HiFi-GAN 模型从梅尔谱图生成语音波形。</p><ol><li>结论：</li></ol><p>（1）：本文提出的 CoVoMix 模型在零样本、类人、多说话者、多轮对话语音生成方面取得了显著进展，为该领域的研究提供了新的思路和方法。</p><p>（2）：创新点：提出了一种基于流匹配的多流文本到语义模型，能够将对话文本转换为多个离散令牌流，表示单个说话者的语义信息；设计了一种基于流匹配的声学模型，将语义序列转换为混合梅尔谱图；采用 HiFi-GAN 模型生成语音波形，实现了高保真语音合成。</p><p>性能：实验结果表明，CoVoMix 生成的对话在自然性和连贯性上类似人类，涉及多个说话者进行多轮对话，具有无缝的语音转换和适当的副语言行为。</p><p>工作量：CoVoMix 模型的训练和部署需要大量的数据和计算资源，这可能会限制其在实际应用中的可行性。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-271fa2d3d54bc8eac1b4ebb4afb68b5f.jpg" align="middle"></details><h2 id="Beyond-Talking-—-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication"><a href="#Beyond-Talking-—-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication" class="headerlink" title="Beyond Talking — Generating Holistic 3D Human Dyadic Motion for   Communication"></a>Beyond Talking — Generating Holistic 3D Human Dyadic Motion for   Communication</h2><p><strong>Authors:Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang</strong></p><p>In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.19467v1">PDF</a> </p><p><strong>Summary</strong><br>通过音频特征和文本语义相结合的方式，实现说话人和倾听者3D逼真且协调的动作生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出音频特征与文本语义信息解耦合的创新任务，生成说话人和倾听者的3D动作。</li><li>分别训练说话人和倾听者的整体动作VQ-VAE。</li><li>考虑说话人和倾听者之间的实时相互影响，提出自回归模型，同时生成说话人和倾听者的动作。</li><li>链式Transformer模型，有效表征现实世界的沟通场景。</li><li>该方法在两个基准数据集上表现出最先进的性能。</li><li>引入HoCo整体沟通数据集，为未来研究提供宝贵资源。</li><li>接受后将HoCo数据集和代码发布用于研究目的。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：超越说话——生成用于交流的整体 3D 人类双人运动</p></li><li><p>作者：Mingze Sun · Chao Xu · Xinyu Jiang · Yang Liu · Baigui Sun · Ruqi Huang</p></li><li><p>单位：清华大学深圳国际研究生院</p></li><li><p>关键词：Dyadic Motion, Holistic Human Mesh, Communication</p></li><li><p>论文链接：None, Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，基于大规模人类说话视频的语音生成运动任务取得了显著进展，即从言语线索（如音频片段或转录）中生成谈话中的非语言信号（如面部表情或身体动作），例如人类面部表情、身体姿势和手势。然而，这些方法仅关注说话者，而忽略了听众的反应。</p><p>（2）：过去方法及其问题：以往方法主要集中在生成说话者的头部、手势或全身运动，而忽略了听众的反应。此外，这些方法通常需要大量标记数据，这限制了它们的实用性。</p><p>（3）：本文提出的研究方法：本文提出了一种新的任务，专注于人类交流，旨在为说话者和听众生成 3D 整体人类动作。该方法的核心是将分解因子化与文本语义信息的结合，从而促进创建更逼真和协调的动作。我们分别针对说话者和听众的整体动作训练 VQ-VAE。我们考虑了说话者和听众之间的实时相互影响，并提出了一种新颖的链式基于 Transformer 的自回归模型，该模型专门设计用于有效表征现实世界中的交流场景，可以同时生成说话者和听众的动作。这些设计确保了我们生成的结果既协调又多样。</p><p>（4）：方法的性能：我们的方法在两个基准数据集上展示了最先进的性能。此外，我们引入了 HoCo 整体交流数据集，这是一个对未来研究有价值的资源。我们的 HoCo 数据集和代码将在被接受后发布以供研究使用。</p><ol><li>方法：</li></ol><p>（1）：针对说话者和听众的整体动作，分别训练 VQ-VAE；</p><p>（2）：提出了一种基于 Transformer 的自回归模型，用于有效表征现实世界中的交流场景，可以同时生成说话者和听众的动作；</p><p>（3）：利用分解因子化与文本语义信息的结合，促进创建更逼真和协调的动作；</p><p>（4）：在两个基准数据集上展示了最先进的性能；</p><p>（5）：引入了 HoCo 整体交流数据集，为未来研究提供了宝贵的资源。</p><ol><li>结论：</li></ol><p>（1）：本工作将交流纳入人机交互中，提出了一项新颖的任务，为说话者和听众生成 3D 整体人类动作。为此，我们在数据集和模型设计上均做出了贡献。前者方面，我们提供了 HoCo 通信数据集，以供未来沿着此任务进行探索。后者方面，我们提出了一个针对我们任务量身定制的模型，该模型包含新颖的设计，包括 1）用于解耦音频特征的分解，增强了生成更真实和协调的动作；2）用于表征非语言交流的链式自回归模型。此外，我们在两个基准上取得了最先进的性能。</p><p>（2）：创新点：提出了一种新的任务，专注于生成说话者和听众的 3D 整体人类动作，并设计了一个针对该任务量身定制的模型。性能：在两个基准数据集上展示了最先进的性能。工作量：引入了 HoCo 整体交流数据集，为未来研究提供了宝贵的资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8dd55a7f4757f4ae1f9d71880b4c6479.jpg" align="middle"><img src="https://picx.zhimg.com/v2-848d816930200060ec067527f2cd2e66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90bdfdaba5b0a3b62088d04ae352d6de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95a401074128a34e072805c4fda00e11.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-06  CoVoMix Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/</id>
    <published>2024-05-06T10:26:38.000Z</published>
    <updated>2024-05-06T10:26:38.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><a href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition" class="headerlink" title="Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"></a>Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</h2><p><strong>Authors:Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</strong></p><p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance. </p><p><a href="http://arxiv.org/abs/2405.01872v1">PDF</a> </p><p><strong>Summary</strong><br>钢材表面缺陷生成模型StableSDG通过迁移Stable Diffusion模型生成高精度合成图像，有效提升钢材表面缺陷识别模型的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>利用Stable Diffusion模型生成钢材表面缺陷图像，扩充训练数据集。</li><li>通过调整模型参数和嵌入空间，弥合生成图像和真实图像之间的分布差异。</li><li>采用图像引导生成方式，而非纯高斯噪声生成。</li><li>提出两种关键过程：分布对齐和图像引导生成。</li><li>在钢材表面缺陷数据集上进行广泛实验，证明StableSDG在生成高质量合成图像和训练识别模型方面均达到最先进水平。</li><li>两种提出的关键过程对性能至关重要。</li><li>StableSDG有效解决数据不足问题，提升钢材表面缺陷识别模型的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>缺陷图像样本生成与扩散</p></li><li><p>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang</p></li><li><p>上海大学传信学院</p></li><li><p>Text-to-image diffusion, data expansion, deep learning, textual inversion, low-rank adaptation, defect image generation, steel surface defect recognition</p></li><li><p>https://arxiv.org/abs/2405.01872 , Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：钢表面缺陷识别是工业界具有巨大产业价值的一项任务。数据不足是训练鲁棒缺陷识别网络的主要挑战。现有方法已经研究了通过生成模型生成样本以扩大数据集。然而，它们的生成质量仍然受到缺陷图像样本不足的限制。</p><p>（2）：现有的方法包括：SDGAN、Defect-GAN、transP2P。这些方法从头开始训练生成模型具有挑战性，当图像样本不足时，通常会导致生成样本中出现不需要的模式。</p><p>（3）：本文提出了一种稳定的表面缺陷生成（StableSDG）方法，将Stable Diffusion模型中嵌入的巨大生成分布转移用于钢表面缺陷图像生成。为了解决钢表面图像和扩散模型生成图像之间的独特分布差异，我们提出了两个过程。首先，我们通过调整扩散模型的参数来对齐分布，既采用标记嵌入空间，也采用网络参数空间。此外，在生成过程中，我们提出了面向图像的生成，而不是从纯高斯噪声中生成。</p><p>（4）：我们在钢表面缺陷数据集上进行了广泛的实验，展示了在生成高质量样本和训练识别模型方面的最先进性能，并且两个设计过程对性能都很重要。</p><ol><li><p>方法：</p><pre><code>            (1):Stable Diffusion模型[22]作为扩散先验，在潜在空间中执行扩散，而不是图像空间，广泛用于图像生成任务[26]–[30]。            (2):提出StableSDG方法，由两个过程组成，用于生成每种缺陷类别的图像。            (3):通过迭代质量评估，调整超参数以实现最佳图像生成。            (4):使用最佳超参数，生成高质量图像以扩展数据集。            (5):将每种缺陷类别的生成图像与真实图像一起收集起来，用于训练缺陷识别模型。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文提出的StableSDG方法将文本到图像生成技术应用于钢表面缺陷图像生成，有效解决了钢表面缺陷数据集不足的问题，为缺陷识别模型的训练提供了高质量的样本。</p><p>（2）创新点：StableSDG方法在生成器自适应过程中，同时在token嵌入空间和网络参数空间进行自适应和修改，并在生成数据时从图像导向初始化生成样本，而不是从纯高斯噪声开始。</p><p>性能：StableSDG方法在NEU和CCBSD数据集上进行了广泛的实验，结果表明该方法能够生成高保真度的缺陷图像，大大提高了识别模型的性能。</p><p>工作量：StableSDG方法的工作量主要集中在生成器自适应和图像生成两个阶段，需要对超参数进行迭代质量评估和调整，以获得最佳的图像生成效果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bd44eeacb7308bdc2e6594f5b84b63b5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a49aa1e7511a9ee599c2b42ba68cfb6d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e589bb5b223b5f1a03944e68feabbcd1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b4b7e153a9792e0731936f44ad770e5f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4222a03fe5309bbcdc62d8769e94eb0c.jpg" align="middle"></details><h2 id="Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><a href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning" class="headerlink" title="Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning"></a>Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning</h2><p><strong>Authors:Rafael Elberg, Denis Parra, Mircea Petrache</strong></p><p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at <a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a> </p><p><a href="http://arxiv.org/abs/2405.01705v1">PDF</a> </p><p><strong>Summary</strong><br>长尾数据图像增强方法，利用预训练稳定扩散模型的潜在空间，缓解生成质量差和推理速度慢的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>图像和多模态机器学习任务在数据分布不足的情况下极具挑战性。</li><li>医学领域的图像生成面临数据获取和隐私限制的障碍。</li><li>潜在扩散模型在图像生成质量上处于领先地位。</li><li>数据生成不平衡和推理速度慢是亟待解决的问题。</li><li>方法结合预训练稳定扩散模型的潜在空间，进行图像增强。</li><li>构建可分离的潜在空间，混合头部和尾部类别的示例。</li><li>通过迭代学习潜在嵌入，构建空间，并通过 K-NN 方法应用于特定任务的显着性图。</li><li>代码已开源：<a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：特征空间增强和迭代学习的长尾图像生成</p></li><li><p>作者：Rafael Elberg、Denis Parra、Mircea Petrache</p></li><li><p>隶属：智利天主教大学</p></li><li><p>关键词：长尾数据、图像生成、特征空间增强、迭代学习</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01705   Github 代码链接：https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</p></li><li><p>摘要：</p></li></ol><p>(1)：图像和多模态机器学习任务在数据分布不均匀的情况下非常具有挑战性。特别是在医疗领域，数据可用性和隐私限制加剧了这些障碍。潜扩散模型在图像生成质量方面处于最先进水平，使其成为解决此问题的理想候选者。然而，仍需要解决几个关键问题，例如难以生成来自代表性不足的类别的图像以及推理过程缓慢。</p><p>(2)：过去的方法包括重采样和数据增强。重采样技术在一些长尾问题中取得了相对成功，但可能会给下游任务引入不必要的偏差，并且经常导致过拟合。数据增强是解决这些问题的自然响应。它代表了一个蓬勃发展的研究领域，包括几个不同的算法系列，例如几何变换（旋转、缩放、裁剪等）、合成样本创建、基于混合的方法、基于域转换的方法和生成方法。</p><p>(3)：本文提出了一种新的数据增强方法，该方法操纵来自预训练扩散模型的图像的潜在空间表示，从而生成新图像来增强代表性不足的类别。通过激活图选择数据的特定特征，然后将这些特征组合起来，生成与属于长尾类的实际数据中的图像相似的图像。</p><p>(4)：在本文的方法中，潜在空间表示的组合由于特征后处理之间的干扰现象而难以通过朴素的方法执行。本文将此问题作为合成泛化问题，并将迭代学习（IL）框架与稀疏嵌入应用于目标数据增强框架。IL 的主要灵感来自文化进化模型，其中教师-学生交互的迭代鼓励有用的压缩和形成适应任务的“共享语言”。特别是，最近在使用稀疏状态空间时获得了与合成不同特征相关的有利结果。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种新的数据增强方法，该方法通过操纵来自预训练扩散模型的图像的潜在空间表示来生成新图像，以增强代表性不足的类别；</p><p>（2）：该方法包括三个阶段：迭代训练、类别激活图生成和推理；</p><p>（3）：在迭代训练阶段，学习了一个从扩散潜在空间到稀疏高维表示的转换，同时训练了一个卷积分类器用于该空间；</p><p>（4）：在类别激活图生成阶段，使用分类器生成每个类的简单可解释激活图，以选择与分类为该类相关的或不相关的坐标；</p><p>（5）：在推理阶段，从尾部类生成新样本，通过将特定尾部类示例的类特定特征与最高混淆头部类的类通用特征融合，使用掩码创建融合向量。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新颖的方法，通过利用预训练的潜在扩散模型、组合学习和显着性方法，为长尾数据集生成数据并增强数据，从而为代表性不足的类别生成新示例。</p><p>（2）：创新点：提出了一种利用潜在扩散模型、组合学习和显著性方法的数据增强和数据生成方法；性能：在医学领域的多标签分类任务中使用 MIMIC-CXR-LT [13, 16] 的一个小型子集，在图像生成和数据增强方面取得了有竞争力的结果；工作量：工作量中等，需要预训练潜在扩散模型，并对转换和分类器进行迭代训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0d188950e8539013f5d1dbb852ac0cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c5b473a90cffec494f2607efb08a6c2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-39ea380f716fabbe74492f3835a23773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d004602232a458ce5dd218668a87e87.jpg" align="middle"></details><h2 id="LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><a href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing" class="headerlink" title="LocInv: Localization-aware Inversion for Text-Guided Image Editing"></a>LocInv: Localization-aware Inversion for Text-Guided Image Editing</h2><p><strong>Authors:Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</strong></p><p>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at <a href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a> </p><p><a href="http://arxiv.org/abs/2405.01496v1">PDF</a> Accepted by CVPR 2024 Workshop AI4CC</p><p><strong>Summary</strong><br>文本引导图像编辑研究利用大型文本到图像扩散模型，但现有编辑技术容易修改超出目标区域的无意区域，主要是因为交叉注意力图不准确。我们通过分割图或边界框改进扩散过程中的交叉注意力图，实现了特定对象的细粒度图像编辑，同时防止对其他区域进行非必要的更改。</p><p><strong>Key Takeaways</strong></p><ul><li>现有图像编辑技术容易修改超出目标区域的无意区域。</li><li>我们提出了利用分割图或边界框作为额外的定位先验来改进扩散过程中的交叉注意力图。</li><li>我们通过更新文本输入中名词对应的符号，迫使交叉注意力图紧密对齐文本提示中的正确名词和形容词。</li><li>我们基于公开的Stable Diffusion实现了LocInv方法，并在COCO数据集的子集上进行了广泛评估。</li><li>与现有方法相比，我们的方法在定量和定性上都取得了更好的结果。</li><li>该方法的代码将在<a href="https://github.com/wangkai930418/DPL上公布。">https://github.com/wangkai930418/DPL上公布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：定位感知反演：文本引导图像编辑</p></li><li><p>作者：Chuanming Tang、Kai Wang、Fei Yang、Joost van de Weijer</p></li><li><p>单位：中国科学院大学</p></li><li><p>关键词：文本到图像、图像编辑、定位感知、交叉注意力</p></li><li><p>论文链接：https://arxiv.org/abs/2405.01496Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：大规模文本到图像扩散模型在文本提示下展示了显著的生成能力。基于文本到图像扩散模型，文本引导图像编辑研究旨在通过改变文本提示来赋予用户操纵生成图像的能力。</p><p>（2）：已有方法及问题：现有的图像编辑技术容易对超出目标区域的无意区域进行编辑，这主要是由于交叉注意力图的不准确。</p><p>（3）：提出的研究方法：本文提出定位感知反演（LocInv），它利用分割图或边界框作为额外的定位先验，在扩散过程的去噪阶段优化交叉注意力图。通过动态更新文本输入中与名词对应的标记，迫使交叉注意力图与文本提示中正确的名词和形容词紧密对齐。基于此技术，我们实现了对特定对象的细粒度图像编辑，同时防止对其他区域进行不必要的更改。</p><p>（4）：方法性能：基于公开的Stable Diffusion，我们对LocInv方法在COCO数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果。这些结果证明了该方法可以实现其目标。</p><ol><li>方法：</li></ol><p>（1）：使用 Stable Diffusion v1.4 作为基础扩散模型，该模型由编码器、解码器和扩散模型组成。</p><p>（2）：采用 DDIM 反演算法，从随机噪声 zT 找到初始噪声，通过采样重建输入潜在代码 z0。</p><p>（3）：使用无文本反演 (NTI) 优化无文本嵌入 ∅t，以近似 DDIM 轨迹 {zt}T 0，从而编辑真实图像。</p><p>（4）：提出动态提示学习 (DPL) 方法，利用分割图或检测框作为定位先验，更新文本提示 P 中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐。</p><p>（5）：设计相似度损失和重叠损失，优化嵌入向量 Vt，使交叉注意力图与定位先验 S 之间相似度高、重叠度高。</p><p>（6）：采用渐进优化机制，在每个时间步 t 处强制所有损失达到预定义阈值，避免交叉注意力图过拟合。</p><p>（7）：结合 NTI 学习一组无文本嵌入 ∅t，与可学习的单词嵌入 Vt 共同精确定位对象并重建原始图像。</p><p>（8）：提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><ol><li>Conclusion:</li></ol><p>(1): 本文提出的 LocInv 方法解决了文本到图像扩散模型图像编辑中交叉注意力图泄漏的问题。我们提出使用分割图或检测框作为先验，更新提示中每个名词单词的动态标记。由此产生的交叉注意力图较少受到交叉注意力图泄漏的影响。因此，这些大大改进的交叉注意力图极大地改善了文本引导图像编辑的结果。实验结果证实，LocInv 获得了更好的结果，尤其是在复杂的多对象场景中。最后，我们展示了我们的方法还可以将形容词单词绑定到它们对应の名词上，从而得到形容词的准确交叉注意力图，并允许对属性进行编辑，这是以前在文本引导图像编辑中尚未充分探索的。</p><p>(2): 创新点：提出定位感知反演方法，利用分割图或检测框作为定位先验，更新文本提示中的名词单词对应的标记，迫使交叉注意力图与文本提示中的名词和形容词紧密对齐；提出形容词绑定机制，通过改变文本提示中的形容词来改变对象的外观。</p><p>性能：在 COCO 数据集的子集上进行了广泛的评估，在定量和定性上都取得了优异的结果，证明了该方法可以实现其目标。</p><p>工作量：方法实现较为复杂，需要结合 Stable Diffusion 模型和 NTI 反演算法，以及分割图或检测框作为定位先验。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-501b84f66a4fdce982c4d560d6ed2c6e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d7f11972c7c9876389df6092b426ca67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-566375be0266ca83b73d642319fcc82b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2650ddd9595d88f0a5238c88b753e8e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-997ccc837824e0d3f900484e2641fab6.jpg" align="middle"></details><h2 id="Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><a href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers" class="headerlink" title="Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers"></a>Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</strong></p><p>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes. </p><p><a href="http://arxiv.org/abs/2405.00858v1">PDF</a> </p><p><strong>Summary</strong><br>利用合成图像感染状态指导分类鉴别糖尿病足溃疡感染</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的糖尿病足溃疡感染检测模型，即条件引导扩散分类器（ConDiff）</li><li>ConDiff结合了引导图像合成、去噪扩散模型和基于距离的分类</li><li>通过在引导图像中注入高斯噪声并通过条件化感染状态进行逆扩散去噪合成图像</li><li>基于合成图像与原始引导图像在嵌入空间中的最小欧几里得距离进行感染分类</li><li>使用元组损失函数在基于距离的分类器中减少过拟合</li><li>ConDiff在准确性和F1-score上优于最先进的模型</li><li>ConDiff开创了生成式判别模型在医学图像精细分析中的应用，为改善患者预后提供了一种有前途的方法</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 导向条件扩散分类器（ConDiff）</p></li><li><p>Authors: Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</p></li><li><p>Affiliation: Worcester理工学院</p></li><li><p>Keywords: 糖尿病足溃疡，扩散模型，基于距离的图像分类，生成模型，伤口感染</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>（1）：糖尿病足溃疡（DFU）感染是导致截肢和严重并发症的主要原因；</p><p>（2）：现有的基于深度学习的DFU感染检测方法存在准确率低的问题；</p><p>（3）：本文提出了一种新的ConDiff模型，该模型结合了引导图像合成、去噪扩散模型和基于距离的分类，通过生成引导条件合成图像并计算合成图像与原始图像之间的最小欧几里得距离来对感染进行分类；</p><p>（4）：ConDiff在DFU感染数据集上取得了83%的准确率和0.858的F1分数，优于现有方法至少3%，证明了其在提高DFU感染诊断准确性方面的有效性。</p></li><li><p>方法：</p><p>（1）：ConDiff 框架由两个主要部分组成：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。</p><p>（2）：ConDiff 利用条件引导图像编辑与生成扩散模型，通过向输入图像注入特定强度的 Gaussian 噪声，并使用反向扩散过程逐步从噪声扰动输入图像中去除噪声来生成新图像。</p><p>（3）：ConDiff 的扩散过程以伤口的状况（无感染（y1）或感染（y2））为条件，创建反映这些状态的合成图像。一个关键点是 ConDiff 能够通过嵌入空间中的 L2 距离分类器识别和学习条件生成图像 ˆxy 0 和原始伤口图像 x0 之间表示的相似性。产生与原始图像最相似的合成图像的条件被选作预测标签。</p><p>（4）：与最小化二元交叉熵损失函数的传统监督分类技术不同，ConDiff 通过利用三元损失函数来减轻过拟合，以增加非相似图像对之间的距离并减少相似图像对之间的距离。</p><p>（5）：本研究利用 Goyal 等人提供的 DFU 感染数据集（见表 I）。但是，为了消除训练集和测试集之间的数据泄漏，我们改进了数据集创建和拆分策略。使用基于主题的拆分，仅为每个主题使用第二个放大自然增强图像（参见图 1）。</p><p>（6）：ConDiff 框架的主要贡献是：（1）我们提出了 Guided Conditional Diffusion Classifier（ConDiff），这是一个用于分类受感染伤口图像的集成端到端框架。ConDiff 框架有 2 个主要部分：（1）引导扩散，即向 DFU 图像注入高斯噪声，然后根据感染状态从噪声扰动图像中逐步去除噪声，以合成条件图像；（2）基于距离的分类器，即根据原始图像和合成图像在嵌入空间中的最小 L2 距离预测输入图像的标签。据我们所知，ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了糖尿病足溃疡 (DFU) 感染的检测。（2）在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，我们的 ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%。（3）我们证明，通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。（4）由 Score-CAM 生成的热图用于直观地说明 ConDiff 在对伤口感染状态进行分类时专注于正确的伤口图像区域。</p></li><li><p>结论：</p></li></ol><p>（1）：本研究引入了引导条件扩散分类器（ConDiff），这是一个用于对糖尿病足溃疡（DFU）感染进行分类的新框架。ConDiff 优于传统模型至少 3%，准确率高达 83%，F1 分数为 0.858。它独特的方法利用三元损失而不是标准的交叉熵最小化，增强了鲁棒性和减少了过拟合。这在数据集通常很小的医学成像中尤其重要。ConDiff 采用正向扩散过程，向输入图像中添加特定数量的高斯噪声，并采用无分类器指导的反向扩散，根据嵌入空间中的最近欧几里得距离对这些图像进行迭代细化以进行分类。ConDiff 的有效性表明在改善 DFU 管理方面具有显着潜力，尤其是在医疗资源有限的地区。其精确的实时感染检测可以在早期 DFU 感染识别中发挥至关重要的作用，从而减少肢体截肢等严重并发症。</p><p>（2）：创新点：ConDiff 是第一个分析细粒度伤口图像的生成判别方法，促进了 DFU 感染的检测；性能：在 DFU 感染数据集的看不见的测试伤口图像（148 个受感染和 103 个未受感染）上进行严格评估，ConDiff 框架明显优于最先进的基线，提高了伤口感染检测的准确性和 F1 分数至少 3%；工作量：通过在训练期间最小化三元损失函数，ConDiff 减少了对 1416 个训练图像的小 DFU 数据集的过拟合。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f38f851b08a13cd2762a9779abb3d5dd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1698e3895c14a21d1245d61cbbe4db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-902be7065fad826b29010fef3bd7e79b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3d5d8c1286e3aefa0a37934906ae34f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6a38755ae54c6bfd6a3359d2197b5a2.jpg" align="middle"></details><h2 id="Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><a href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation" class="headerlink" title="Obtaining Favorable Layouts for Multiple Object Generation"></a>Obtaining Favorable Layouts for Multiple Object Generation</h2><p><strong>Authors:Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</strong></p><p>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts. </p><p><a href="http://arxiv.org/abs/2405.00791v1">PDF</a> </p><p><strong>Summary</strong><br>随着文本到图像模型的快速发展，多主体生成成为模型发展的重要步骤。本研究针对扩散模型多主体生成中的问题，提出了一种通过引导原则进行布局规划的新方法。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在多主体生成中面临着遗漏或合并主体的问题。</li><li>本研究提出了一种基于引导原则的布局规划方法。</li><li>该方法允许扩散模型初始提出布局，然后对其进行重新排列。</li><li>强制交叉注意力图（XAM）遵循提出的遮罩，并将潜在图中的像素迁移到新位置。</li><li>引入了新的损失项，以减少 XAM 熵、减少 XAM 之间的重叠并确保 XAM 与各自的遮罩对齐。</li><li>该方法在各种文本提示中更真实地捕捉到所需概念。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 获得多个对象生成的有利布局</p></li><li><p>Authors: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</p></li><li><p>Affiliation: 巴伊兰大学工程学院</p></li><li><p>Keywords: 文本到图像生成, 多对象生成, 扩散模型, 交叉注意力图</p></li><li><p>Paper: https://arxiv.org/abs/2405.00791 , Github: None</p></li><li><p>Summary:</p><p>(1): 大规模文本到图像模型在基于文本提示生成高质量和多样化图像方面取得了显著成功。这些模型最终旨在创建复杂的场景，解决多对象生成挑战是朝着这一目标迈出的关键一步。然而，现有的最先进的扩散模型在生成涉及多个对象的图像时面临困难。当给定包含多个对象的提示时，这些模型可能会省略一些对象或将它们合并在一起。</p><p>(2): 现有的方法包括：使用交叉注意力图（XAM）对生成图像中的不同对象进行建模。然而，这些方法存在问题：当提示中包含多个对象时，模型可能会省略一些对象或将它们合并在一起。</p><p>(3): 本文提出了一种基于指导原则的新方法。我们允许扩散模型最初提出一个布局，然后重新排列布局网格。这是通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到我们确定的新位置来实现的。我们引入了新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐。</p><p>(4): 本文方法在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p></li><li><p>方法：</p></li></ol><p>（1）：提出一种基于指导原则的新方法，允许扩散模型最初提出一个布局，然后重新排列布局网格；</p><p>（2）：通过强制交叉注意力图（XAM）遵守提出的掩码并通过将像素从潜在图迁移到确定的新位置来实现；</p><p>（3）：引入新的损失项，旨在降低 XAM 熵以更清晰地定义对象的空间，减少 XAM 之间的重叠，并确保 XAM 与它们各自的掩码对齐；</p><p>（4）：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格，从而更忠实地捕捉到各种文本提示中所需的概念，证明了其有效性。</p><p>（2）：创新点：提出了一种基于指导原则的新方法，该方法允许扩散模型最初提出一个布局，然后重新排列布局网格。性能：在各种文本提示中更忠实地捕捉到所需的概念，证明了其有效性。工作量：需要针对不同的文本提示进行微调，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-561eb2e3b9534e1fe4b30e7ef897a8b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2973412c6cc5c19507315dc2dd5efcd.jpg" align="middle"></details><h2 id="Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><a href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models"></a>Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h2><p><strong>Authors:Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2. </p><p><a href="http://arxiv.org/abs/2405.00760v1">PDF</a> N/A</p><p><strong>Summary</strong><br>文本到图像扩散模型中，利用给定的激励函数进行优化是一个重要但未得到充分探索的研究领域。研究中提出深度激励优化（DRTune），算法直接监督文本到图像扩散模型的最终输出图像，并且通过迭代采样流程将梯度传回输入噪声。</p><p><strong>Key Takeaways</strong></p><ul><li>针对低层激励，训练采样流程中的早期步骤至关重要。</li><li>在去噪网络输入处停止梯度，可以有效实现深度监督。</li><li>DRTune 算法在各种激励模型上得到了广泛评估。</li><li>DRTune 算法始终优于其他算法，尤其是在浅层监督方法失效的低层控制信号中。</li><li>通过 DRTune 优化 Human Preference Score v2.1，对 Stable Diffusion XL 1.0（SDXL 1.0）模型进行微调，产生了更好的扩散 XL 1.0（FDXL 1.0）模型。</li><li>FDXL 1.0 与 Midjourney v5.2 相比，显著提高了图像质量，达到了相当的水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：深度奖励监督微调</p></li><li><p>作者：Xiaoshi Wu<em>1,3, Yiming Hao</em>2, Manyuan Zhang1, Keqiang Sun1, Zhaoyang Huang3, Guanglu Song4, Yu Liu4, and Hongsheng Li1,2</p></li><li><p>第一作者单位：香港中文大学多媒体实验室</p></li><li><p>关键词：文本到图像扩散模型、深度奖励监督、微调、图像质量增强</p></li><li><p>论文链接：https://arxiv.org/abs/2405.00760v1</p></li><li><p>摘要：</p></li></ol><p>(1)：研究背景：优化具有给定奖励函数的文本到图像扩散模型是一个重要但尚未充分探索的研究领域。</p><p>(2)：过去方法：现有方法通常采用浅层监督，即仅监督采样过程的早期步骤。然而，对于低级奖励信号，浅层监督效果不佳。</p><p>(3)：本文方法：本文提出深度奖励微调（DRTune）算法，通过直接监督文本到图像扩散模型的最终输出图像并通过迭代采样过程反向传播到输入噪声来实现深度监督。</p><p>(4)：方法性能：DRTune在各种奖励模型上得到了广泛评估。与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。此外，本文使用DRTune微调了Stable Diffusion XL 1.0（SDXL 1.0）模型以优化人类偏好评分v2.1，得到了Favorable Diffusion XL 1.0（FDXL 1.0）模型。与SDXL 1.0相比，FDXL 1.0显着提高了图像质量，并且与Midjourney v5.2相比达到了相当的质量。</p><ol><li>方法：</li></ol><p>（1）：DRTune 算法的核心思想是通过直接监督文本到图像扩散模型的最终输出图像，并通过迭代采样过程反向传播到输入噪声来实现深度监督；</p><p>（2）：为了解决梯度爆炸问题，DRTune 通过阻止输入 xt 的梯度来解决收敛问题；</p><p>（3）：为了提高效率，DRTune 阻止输入 xt 的梯度，并训练所有采样步骤的子集；</p><p>（4）：DRTune 算法的伪代码如下：</p><p><code>输入：预训练的扩散模型权重 θ、奖励 r、训练时间步长 K、早期停止时间步长范围 m。sg 表示梯度停止操作。while not converged do    ttrain = {1, ..., K} if DRaFT-K    ttrain = {i}i≥randint(1,T ) if AlignProp    if DRTune then        # 等距时间步长。        s = randint(1, T − K⌊ T K ⌋)        ttrain = {s + i⌊ T K ⌋ | i = 0, 1, ..., K − 1}    if ReFL 或 DRTune then        tmin = randint(1, m)    else        tmin = 0    xT ∼ N(0, I)    for t = T, ..., 1 do        if DRTune then            ˆϵ = ϵθ(sg(xt), t)        else            ˆϵ = ϵθ(xt, t)        if t /∈ ttrain then            ˆϵ = sg(ˆϵ)        if t == tmin then            x0 ≈ intermediate_prediction(xt, ˆϵ)            break        xt−1 = atxt + btˆϵ + ctϵ        g = ∇θr(x0, c)        θ ← θ − ηg</code></p><ol><li>结论：</li></ol><p>（1）：本文的意义在于，它解决了利用奖励模型的反馈来训练文本到图像扩散模型的挑战。本文强调了深度监督对于优化全局奖励的重要性，并使用停止梯度技术解决了收敛问题。此外，本文通过微调 FDXL 1.0 模型展示了奖励训练的潜力，以实现与 Midjourney 相当的图像质量。</p><p>（2）：创新点：本文提出了深度奖励微调（DRTune）算法，实现了深度监督，并通过阻止输入 xt 的梯度来解决收敛问题。性能：DRTune 在各种奖励模型上得到了广泛评估，与其他算法相比，它始终表现出更好的性能，尤其是在浅层监督方法均失败的低级控制信号方面。工作量：DRTune 算法的实现相对简单，并且可以轻松应用于现有的文本到图像扩散模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-037d3e48be185336859047a6292c8d27.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b417340b0b4ae8f5dcc966e5d18466d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d9661ec54a3560470071969dc361ea74.jpg" align="middle"><img src="https://pica.zhimg.com/v2-560c3c936da4c7000b08d87c1704852f.jpg" align="middle"></details><h2 id="Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><a href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution" class="headerlink" title="Detail-Enhancing Framework for Reference-Based Image Super-Resolution"></a>Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h2><p><strong>Authors:Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</strong></p><p>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes. </p><p><a href="http://arxiv.org/abs/2405.00431v1">PDF</a> </p><p><strong>Summary</strong><br>引用图像超分辨通过引入高分辨率参考图像来缓解单图像超分辨的病态问题，但由于纹理传输前存在错位问题，仍有提升空间。</p><p><strong>Key Takeaways</strong></p><ul><li>引用图像超分辨方法在定量和定性结果上均有显著提升。</li><li>现有方法忽视了比较中细节的重要性，没有充分利用低分辨率图像中的信息。</li><li>本文提出了一种基于扩散模型的细节增强框架，用于引用图像超分辨。</li><li>如果参考图像中存在对应部分，该方法可以促进严格的对齐。</li><li>如果参考图像没有对应部分，该方法可以确保基本改进，同时避免参考图像的影响。</li><li>大量实验表明，所提出的方法在保持可比数值结果的同时，获得了更好的视觉效果。</li><li>该方法能够在没有对应参考图像的情况下提高超分辨率性能。</li><li>该方法可以灵活地应用于各种引用图像超分辨任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：基于参考图像的图像超分辨率的细节增强框架</p></li><li><p>作者：Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</p></li><li><p>第一作者单位：上海微系统与信息技术研究所</p></li><li><p>关键词：图像超分辨率，参考图像，细节增强，扩散模型</p></li><li><p>论文链接：xxx，Github 代码链接：None</p></li><li><p>摘要：</p><p>（1）：近年来，基于参考图像的图像超分辨率（Ref-SR）得到了蓬勃发展。通过将高分辨率（HR）参考图像引入到单幅图像超分辨率（SISR）方法中，在参考图像纹理的辅助下，缓解了这一长期存在的领域的病态性质。尽管定量和定性结果的显着提高验证了 Ref-SR 方法的优越性，但在纹理传输之前存在的错位表明还有进一步提高性能的空间。现有的方法往往忽略了比较背景下细节的重要性，因此没有充分利用低分辨率（LR）图像中包含的信息。</p><p>（2）：过去的方法倾向于简单地将输入的 LR 图像调整为与相应参考图像相同的分辨率，例如双三次插值。Lu 等人选择对参考图像进行下采样以适应匹配过程，目的是降低计算复杂度。虽然这种方法可以在一定程度上缓解错位问题，但它忽略了细节的增强，可能会破坏后续的图像恢复结果。在参考图像中存在对应部分的情况下，现有的方法无法促进严格的对齐。</p><p>（3）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。如果参考图像中存在对应部分，我们的方法可以促进严格的对齐。在参考图像中缺少对应部分的情况下，它确保了根本性的改进，同时避免了参考图像的影响。</p><p>（4）：在图像超分辨率任务上，本文方法取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。</p></li><li><p>方法：</p></li></ol><p>（1）：提出了一种基于参考图像的超分辨率细节增强框架（DEF），它引入了扩散模型来生成和增强 LR 图像中的底层细节。</p><p>（2）：该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。</p><p>（3）：在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。</p><p>（4）：在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。</p><p>（5）：利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。</p><p>（6）：使用 deformable convolution network（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架引入了扩散模型来生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。</p><p>（2）：创新点：提出了一种基于参考图像的超分辨率细节增强框架（DEF），该框架利用扩散模型生成和增强低分辨率图像中的底层细节。该方法将图像超分辨率任务分解为两个子任务：细节生成和细节迁移。在细节生成任务中，使用预训练的扩散模型对输入图像进行上采样，以获得细节增强的输入图像。在细节迁移任务中，首先对细节增强的图像和参考图像进行特征提取。利用细节增强的输入图像来计算参考图像和输入图像之间的相似性。使用可变形卷积网络（DCN）进行纹理迁移和集成，以解决纹理失配问题。性能：该方法在图像超分辨率任务上取得了优异的视觉效果，同时保持了可比的数值结果。这些性能支持了他们的目标。工作量：该方法的实现相对复杂，需要使用预训练的扩散模型和可变形卷积网络。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-03b9463baa117efca1717d3d158fe273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c3af50396285ae462ddd151feecf5dad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a59353ef4d615d00935a00b86d496d8.jpg" align="middle"></details><h2 id="ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><a href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning" class="headerlink" title="ASAM: Boosting Segment Anything Model with Adversarial Tuning"></a>ASAM: Boosting Segment Anything Model with Adversarial Tuning</h2><p><strong>Authors:Bo Li, Haoke Xiao, Lv Tang</strong></p><p>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAM’s performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in <a href="https://asam2024.github.io/">https://asam2024.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2405.00256v1">PDF</a> This paper is accepted by CVPR2024</p><p><strong>Summary</strong><br>Meta AI的ASAM通过对抗性训练增强了SAM图像分割模型，无需额外数据或架构调整即可提升分割任务的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>ASAM 采用对抗训练来增强 SAM 图像分割模型，无需额外数据或架构修改。</li><li>自然对抗实例提高了模型对自然变化的鲁棒性，而不是传统的不可感知扰动。</li><li>ASAM 保持了对抗实例的逼真度并与原始掩码注释保持一致，从而保持分割任务的完整性。</li><li>微调后的 ASAM 在各种分割任务中表现出显着提升，在 SA-1B 数据集上达到 88.2% 的 mIoU。</li><li>ASAM 在 ADE20K 数据集上达到 50.1% 的 mIoU，超过了以前的最先进方法。</li><li>ASAM 在 COCO Stuff 数据集上达到 34.6% 的 mIoU，在 Cityscapes 数据集上达到 81.2% 的 mIoU。</li><li>ASAM 推进了计算机视觉中基础模型的性能，证明了对抗性训练在图像分割中的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ASAM：基于对抗调优的 Segment Anything 模型增强</p></li><li><p>Authors: Bo Li, Haoke Xiao, Lv Tang</p></li><li><p>Affiliation: vivo Mobile Communication Co., Ltd</p></li><li><p>Keywords: Adversarial Tuning, Image Segmentation, Foundation Model, Segment Anything Model, Stable Diffusion</p></li><li><p>Urls: https://arxiv.org/abs/2405.00256, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着计算机视觉的发展，基础模型已经成为关键工具，它们在各种任务中表现出卓越的适应性。其中，Meta AI 的 Segment Anything Model (SAM) 在图像分割领域表现突出。然而，SAM 与其他同类模型一样，在特定细分应用中遇到了局限性，这促使人们寻求增强策略，而不会损害其固有能力。</p><p>(2): 过去的方法包括微调和适配器模块，但微调会损害 SAM 的固有泛化能力，而其他方法则需要额外的适配层或后处理模块。</p><p>(3): 本文提出了一种新的方法 ASAM，它通过对抗调优来增强 SAM 的性能。受自然语言处理中自然对抗样本成功实现的启发，我们利用稳定扩散模型，增强了 SA-1B 数据集的子集（1%），生成了更能代表自然变化的对抗实例，而不是传统的不可感知扰动。我们的方法保持了对抗样本的真实感，并确保了与原始掩码注释的一致性，从而保留了分割任务的完整性。</p><p>(4): 在各种分割任务中，经过微调的 ASAM 展示了显著的改进，而无需额外的数据或架构修改。我们广泛评估的结果证实，ASAM 在分割任务中建立了新的基准，从而促进了计算机视觉中基础模型的发展。</p><ol><li>方法：</li></ol><p>（1）：提出 ASAM 方法，通过对抗调优增强 SAM 模型的性能；</p><p>（2）：利用稳定扩散模型，增强 SA-1B 数据集的子集，生成更具代表性的对抗实例；</p><p>（3）：保持对抗样本的真实感，确保与原始掩码注释的一致性；</p><p>（4）：在各种分割任务中评估 ASAM，展示了显著的改进，建立了新的基准。</p><ol><li>结论：</li></ol><p>（1）本工作通过创新性地使用对抗调优，提出的 ASAM 方法代表了 SAM 的重大进步。利用稳定扩散模型增强 SA-1B 数据集的一部分，我们生成了自然、逼真的对抗图像，从而大幅提升了 SAM 在各种任务中的分割能力。这种方法借鉴了 NLP 中对抗训练技术，在保持 SAM 原生架构和零样本优势的同时增强了其性能。我们的研究结果表明，ASAM 不仅在分割任务中树立了新的标杆，而且促进了对抗样例在计算机视觉领域的更广泛应用和理解，为提升大型视觉基础模型能力提供了一种新颖且有效的方法。</p><p>（2）创新点：使用稳定扩散模型生成对抗实例，增强 SAM 的分割能力；保持 SAM 的原生架构和零样本优势；性能：在各种分割任务中展示了显著的改进，建立了新的基准；工作量：与微调和适配器模块等其他增强策略相比，工作量相对较小，无需额外的架构修改或后处理模块。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e7684baf385865b289b9bd3b4babea56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7c0e935c2de944340eb5085a5356da42.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d93133fcc44a60510ee9cb1385d6be69.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-06T10:03:07.000Z</published>
    <updated>2024-05-06T10:03:07.018Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-06-更新"><a href="#2024-05-06-更新" class="headerlink" title="2024-05-06 更新"></a>2024-05-06 更新</h1><h2 id="X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation"><a href="#X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation" class="headerlink" title="X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation"></a>X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</h2><p><strong>Authors:Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</strong></p><p>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry-&gt;Texture-&gt;Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: <a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/">https://xmu-xiaoma666.github.io/Projects/X-Oscar/</a>. </p><p><a href="http://arxiv.org/abs/2405.00954v1">PDF</a> ICML2024</p><p><strong>Summary</strong><br>文本提出的 X-Oscar 框架可以从文本提示生成高质量的可动画头像，它采用几何-&gt;纹理-&gt;动画的顺序范式，并引入自适应变异参数和基于头像的评分蒸馏采样技术来解决过饱和和低质量输出的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>X-Oscar 是一个渐进式框架，从文本提示生成高质量的可动画头像。</li><li>它采用顺序的几何-&gt;纹理-&gt;动画范式，简化了优化过程。</li><li>自适应变异参数将头像表示为训练期间的自适应分布，以解决过饱和问题。</li><li>基于头像的评分蒸馏采样技术将基于头像的噪声融入渲染图像，以提高优化过程中的生成质量。</li><li>广泛的评估证实 X-Oscar 优于现有的文本到 3D 和文本到头像方法。</li><li>项目主页：<a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/。">https://xmu-xiaoma666.github.io/Projects/X-Oscar/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：X-Oscar：一个用于生成高质量文本引导式3D可动画角色的渐进式框架</li><p></p><p></p><li>作者：Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</li><p></p><p></p><li>第一作者单位：厦门大学多媒体可信感知与高效计算教育部重点实验室</li><p></p><p></p><li>关键词：文本引导式3D角色生成、渐进式生成、自适应变分参数、角色感知得分蒸馏采样</li><p></p><p></p><li>论文链接：https://arxiv.org/pdf/2405.00954.pdf ，Github代码链接：无</li><p></p><p></p><li><p></p><p>摘要：（1）：随着深度学习的兴起，3D人体重建领域取得了显著进展，但现有方法主要专注于从视觉线索重建人体，难以满足融入创造力、编辑和控制的需求。（2）：现有文本引导式3D角色生成方法存在过饱和和输出质量低的问题。（3）：本文提出了一种渐进式框架X-Oscar，通过“几何→纹理→动画”的顺序生成模式，并引入自适应变分参数和角色感知得分蒸馏采样技术，来解决过饱和问题并提高生成质量。（4）：在文本到3D和文本到角色生成任务上，X-Oscar在生成质量和动画保真度方面均优于现有方法，证明了其有效性。</p></li><li><p>方法：（1）：提出渐进式框架X-Oscar，通过“几何→纹理→动画”的顺序生成模式，解决过饱和问题并提高生成质量；（2）：引入自适应变分参数（AVP），采用可训练的自适应分布表示虚拟形象，解决虚拟形象生成中常见的过饱和问题；（3）：提出虚拟形象感知得分蒸馏采样（ASDS），将几何感知和外观感知噪声融入去噪过程中，使预训练的扩散模型能够感知生成虚拟形象的当前状态，从而产生高质量的输出。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出的X-Oscar框架在文本引导式3D可动画角色生成领域具有重要意义，为该领域的研究和应用提供了新的思路和方法。</p><p>（2）：创新点：X-Oscar框架创新性地提出了渐进式生成模式、自适应变分参数和角色感知得分蒸馏采样技术，有效解决了文本引导式3D角色生成中存在的过饱和问题和输出质量低的问题。</p><p>性能：在文本到3D和文本到角色生成任务上，X-Oscar框架在生成质量和动画保真度方面均优于现有方法，证明了其有效性和优越性。</p><p>工作量：X-Oscar框架的实现需要较高的计算资源和专业知识，对于普通用户来说，使用和部署可能存在一定的难度。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f631630c69e7fc1f5a8d28fd426ba1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aa682fc730b8fcb4e568e48a58c3a47.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-06  X-Oscar A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/</id>
    <published>2024-05-02T03:18:37.000Z</published>
    <updated>2024-05-02T03:18:37.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration"><a href="#NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration" class="headerlink" title="NeRF-Guided Unsupervised Learning of RGB-D Registration"></a>NeRF-Guided Unsupervised Learning of RGB-D Registration</h2><p><strong>Authors:Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</strong></p><p>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction. </p><p><a href="http://arxiv.org/abs/2405.00507v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF-UR 提出了一种帧到模型的优化框架，用于无监督 RGB-D 配准，利用神经辐射场 (NeRF) 作为场景的全局模型，以提高多视图一致性差时的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种无监督 RGB-D 配准的帧到模型优化框架 NeRF-UR。</li><li>使用 NeRF 作为场景的全局模型，提高了多视图一致性差时的鲁棒性。</li><li>创建了一个合成数据集 Sim-RGBD，通过在真实数据上进行无监督微调，将特征提取和注册的能力从仿真转移到现实。</li><li>在 ScanNet 和 3DMatch 数据集上，NeRF-UR 优于最先进的方法。</li><li>代码和模型将公开，以方便论文复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：NeRF引导的RGB-D配准无监督学习</li><p></p><p></p><li>作者：Zhinan Yu1∗, Zheng Qin2∗, Yijie Tang1, Yongjun Wang1, Renjiao Yi1, Chenyang Zhu1, and Kai Xu1†</li><p></p><p></p><li>第一作者单位：国防科技大学</li><p></p><p></p><li>关键词：RGB-D配准·无监督学习·NeRF</li><p></p><p></p><li>论文链接：xxx</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）：研究背景：随着RGB-D传感器的普及和成本的降低，3D数据采集的难度已大大降低。RGB-D数据的广泛收集极大地推动了深度学习在3D视觉领域的进步，从而极大地提高了RGB-D SLAM和RGB-D重建等应用的性能。然而，现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（2）：过去方法及问题：现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（3）：本文方法：本文提出了一种新颖的帧到模型优化框架NeRF-UR，用于无监督RGB-D配准。该框架利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和NeRF重新渲染帧之间的一致性进行位姿优化。此外，为了引导NeRF优化，本文创建了一个通过逼真模拟器生成的合成数据集Sim-RGBD，用于预热配准模型。</p><p>（4）：方法性能：该方法在两个流行的室内RGB-D数据集ScanNet和3DMatch上优于最先进的同类方法。</p><ol><li>方法：</li></ol><p>（1）：提出一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准；</p><p>（2）：利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性进行位姿优化；</p><p>（3）：创建通过逼真模拟器生成的合成数据集 Sim-RGBD，用于预热配准模型；</p><p>（4）：使用 PointMBF 作为配准模型，该模型融合了视觉（2D）和几何（3D）空间的信息以获得更好的特征辨别力；</p><p>（5）：提出 NeRF 引导的无监督配准管道，该管道依赖于高质量的位姿来监督配准模型，并利用 NeRF 优化帧位姿；</p><p>（6）：采用 NeRF 模型，该模型能够对场景中的光照和几何结构进行建模，并联合优化 3D 地图和位姿；</p><p>（7）：将 RGB-D 序列分割成子序列，并为每个子序列优化一个 NeRF，以避免联合地图位姿优化带来的误差累积和巨大的时间开销；</p><p>（8）：通过 Sim-RGBD 数据集对配准模型进行引导，以提供合理的初始位姿，解决随机初始化配准模型容易产生大量异常对应关系的问题。</p><p><strong>8. 结论</strong></p><p><strong>(1):该工作的意义</strong></p><p>本文提出了 NeRF-UR，这是一种用于无监督 RGB-D 配准的帧到模型优化框架。该方法利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化配准模型估计的位姿。这种设计可以有效提高对光照变化、几何遮挡和反射材料的鲁棒性。我们进一步在合成数据集上设计了一个引导机制来预热 NeRF 优化。在两个基准上的大量实验表明了我们方法的有效性。我们认为，NeRF 引导的无监督学习是 3D 视觉的一种有前途的机制，我们希望在未来将其扩展到更多的任务，如定位、重建等。</p><p><strong>(2):本文的优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准。</li><li>利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化位姿。</li><li>设计了一种合成数据集上的引导机制来预热 NeRF 优化。</li></ul><p><strong>性能：</strong></p><ul><li>在两个流行的室内 RGB-D 数据集 ScanNet 和 3DMatch 上优于最先进的同类方法。</li></ul><p><strong>工作量：</strong></p><ul><li>需要预先训练 NeRF 模型，这可能需要大量的时间和计算资源。</li><li>优化 NeRF 和配准模型是一个迭代过程，可能需要多次迭代才能收敛。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e47f5a6a35637f1b5b56609633d65083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d72291aca2a21454f9a83d46a2633ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-caf6df85382bbbd1a4f390f7bbbc79cb.jpg" align="middle"></details>## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions**Authors:Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan**Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. [PDF](http://arxiv.org/abs/2404.19015v1) The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955**Summary**神经辐射场（NeRF）在场景的逼真自由视点渲染方面表现出色。最近对 NeRF 的改进，如 TensoRF 和 ZipNeRF，采用了显式模型以实现更快的优化和渲染，而 NeRF 则采用了隐式表示。然而，隐式和显式的辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集合可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于使用更少的视图有效地训练它。深度监督是使用经典方法或在大数据集上预先训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。与早期的方法相反，我们寻求通过设计增强模型并在主辐射场中训练它们来学习深度监督。此外，我们的目标是设计一个正则化框架，它可以在不同的隐式和显式辐射场中使用。我们观察到，这些辐射场模型的某些特征在稀疏输入情况下过度拟合观测到的图像。我们的主要发现是，降低辐射场相对于位置编码、分解张量分量数或哈希表大小的能力，会限制模型学习更简单的解决方案，从而在某些区域估计更好的深度。通过基于这种降低的能力设计增强模型，我们可以获得更好的主辐射场深度监督。通过使用上述正则化，我们在包含朝前和 360 度场景的流行数据集上以稀疏输入视图实现了最先进的视图合成性能。**Key Takeaways**-  减少NeRF模型复杂性有助于学习更好的深度，有利于利用稀疏视图进行训练。-  设计增强模型，基于降低NeRF模型能力获得改进的深度监督。-  正则化框架可以应用于不同类型NeRF模型，包括隐式和显式模型。-  过度拟合是稀疏视图输入NeRF训练中的主要问题。-  深度监督可以促进NeRF模型从稀疏视图中进行有效训练。-  经典方法和神经网络都可以用于深度监督，但各有优缺点。-  在朝前和360度场景的流行数据集上，该方法实现了最先进的视图合成性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 简化射线场：用更简单的解决方案正则化稀疏输入的辐射场</p></li><li><p>Authors: Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan</p></li><li><p>Affiliation: 印度科学院</p></li><li><p>Keywords: 神经辐射场, 稀疏输入, 正则化, 深度估计</p></li><li><p>Urls: https://arxiv.org/abs/2404.19015, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 神经辐射场(NeRF)在场景的真实感自由视角渲染中表现出色。NeRF的最新改进，如TensoRF和ZipNeRF，采用了显式模型以实现更快的优化和渲染，而NeRF则采用隐式表示。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于有效地使用更少的视图对其进行训练。深度监督是使用经典方法或在大数据集上预训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。</p><p>(2): 与早期的研究方法相反，我们试图通过设计增强模型并将其与主辐射场一起训练来学习深度监督。此外，我们的目标是设计一个正则化框架，可以在不同的隐式和显式辐射场中工作。我们观察到，这些辐射场模型的某些特征在稀疏输入场景中过度拟合观测图像。我们的主要发现是，在位置编码、分解的张量分量数或哈希表大小方面降低辐射场的性能，会限制模型学习更简单的解决方案，从而在某些区域估计出更好的深度。通过设计基于这种降低性能的增强模型，我们为主要辐射场获得了更好的深度监督。我们通过在包含前视和360度场景的流行数据集上使用上述正则化，在稀疏输入视图上实现了最先进的视图合成性能。</p><p>(3): 我们首先观察到，当使用稀疏输入视图进行训练时，辐射场模型通常利用它们的高性能来学习不必要的复杂解决方案。虽然这些解决方案完美地解释了观测图像，但它们可能会在新视图中造成严重的失真。例如，NeRF中的一些关键组件，如NeRF中的位置编码或TensoRF中采用的向量矩阵分解，为辐射场提供了强大的性能，并被设计为使用密集输入视图训练模型。由于系统严重欠约束，这些组件的现有实现可能在输入视图较少的情况下不理想，从而导致多种失真。图4、图7和图8分别显示了NeRF、TensoRF和ZipNeRF在少次拍摄设置中常见的失真。我们遵循流行的奥卡姆剃刀原理，并对辐射场进行正则化，以在可能的情况下选择更简单的解决方案，而不是复杂的解决方案。具体来说，我们通过降低辐射场的性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场。我们针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强。NeRF中使用的高位置编码度会导致不希望的深度不连续，从而产生浮点。此外，视点相关的辐射特征会导致形状辐射模糊，从而产生重复伪影。我们通过降低位置编码度和禁用视点相关的辐射特征来设计NeRF的增强。另一方面，TensoRF中大量的高分辨率分解组件和ZipNeRF中的大哈希表会导致这些模型在少次拍摄设置中出现浮点。因此，我们设计了增强，以限制模型在这些组件方面学习更简单的解决方案。我们将简化的模型用作深度监督的增强，而不是作为主要的NeRF模型，因为简单地降低辐射场的性能可能会导致某些区域的次优解决方案[Jain et al. 2021]。例如，只能学习平滑深度转换的模型可能无法学习物体边界处的锐利深度不连续性。此外，仅当增强模型准确解释观察到的图像时，才需要使用它们进行监督。我们通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性。</p><p>(4): 在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进，如表1所示。我们观察到，原始辐射场存在各种失真。使用更简单的解决方案对辐射场进行正则化可以显著改善所有三个辐射场的重建。</p><ol><li><p>方法：</p><pre><code>            (1):通过降低辐射场性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场；            (2):针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强；            (3):通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性；            (4):在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进；            .......</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文解决的是通过从与主辐射场模型同时训练的低能力增强模型学习的更简单的解决方案中获得深度监督来解决少次拍摄辐射场的问题。我们表明，可以为隐式模型（如 NeRF）和显式辐射场（如 TensoRF 和 ZipNeRF）设计增强。由于各种辐射场的缺点不同，我们为每个模型适当地设计了增强。我们表明，我们的增强在所有三个模型上都显着提高了性能，并且我们在前视场景和 360◦ 场景上都取得了最先进的性能。值得注意的是，我们的模型在场景的深度估计方面取得了显着的改进，这对于新视图合成和场景理解至关重要。</p><p>（2）：创新点：提出了一种通过从增强模型学习的更简单的解决方案中获得深度监督来正则化辐射场的方法；性能：在 NeRF、TensoRF 和 ZipNeRF 模型上取得了显着改进，在少次拍摄设置中实现了最先进的视图合成性能；工作量：需要设计针对不同辐射场模型的增强，这可能需要额外的工程工作。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5d84b090330526061fb59bb1dfc6ea7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a15c9e6ec9783c3b5ec66e4da9128f8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-52e8f6725bd512099b8ddbc432d73f2f.jpg" align="middle"></details><h2 id="Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields"><a href="#Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields"></a>Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields</h2><p><strong>Authors:Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</strong></p><p>Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at <a href="https://github.com/TQTQliu/GeFu">https://github.com/TQTQliu/GeFu</a> . </p><p><a href="http://arxiv.org/abs/2404.17528v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://gefucvpr24.github.io">https://gefucvpr24.github.io</a></p><p><strong>Summary</strong><br>新提出方法GeFu针对NeRF模型的泛化能力不足问题，提出自适应代价聚合（ACA）、空间视图聚合（SVA）和一致性感知融合（CAF）机制，通过提升几何重建精度、丰富描述符信息和优化解码策略，大幅提升NeRF模型的泛化能力。</p><p><strong>Key Takeaways</strong></p><ul><li>现存NeRF模型泛化能力受限于几何重建不准、描述符信息不足和解码策略不优。</li><li>ACA机制通过放大一致像素对的贡献，抑制不一致像素对，提升代价体精度。</li><li>SVA机制结合空间和视图信息，丰富描述符信息。</li><li>CAF机制融合不同解码策略的优势，提升解码效果。</li><li>GeFu框架结合ACA、SVA、CAF机制，从粗到精进行几何重建和融合渲染。</li><li>GeFu模型在多个数据集上达到最优性能。</li><li>GeFu代码已开源（<a href="https://github.com/TQTQliu/GeFu）。">https://github.com/TQTQliu/GeFu）。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：基于几何的重建和融合精修渲染（中文翻译：）</p></li><li><p>作者：天齐 刘，添翼 冯，小明 董，嘉鹏 张，志伟 冯，杰 潘，振羽 王，志伟 冯（Tianqi Liu, Tianyi Feng, Xiaoming Dong, Jiapeng Zhang, Zhiwei Feng, Jie Pan, Zhenyu Wang, Zhiwei Feng）</p></li><li><p>第一作者单位：北京大学（中文翻译：北京大学）</p></li><li><p>关键词：神经辐射场，多视图重建，神经渲染，场景理解</p></li><li><p>论文链接：或Github代码链接（若有，则填写，若无，则填写Github:None）：Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）可以从多视图图像中重建3D场景，但现有方法在具有挑战性的条件下（如遮挡或反射）表现出泛化能力有限。</p><p>（2）：过去的方法：现有方法主要通过构建基于方差的代价体积进行几何重建，并编码3D描述符进行新视图解码。但这些方法存在几何不准确、描述符次优和解码策略不佳的问题。</p><p>（3）：本文方法：本文提出了一种基于几何的重建和融合精修渲染（GeFu）框架，通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><p>（4）：任务和性能：在多个数据集上，GeFu在多视图重建和新视图渲染任务上都取得了最先进的性能。这些性能支持了本文的目标，即提高NeRF在具有挑战性条件下的泛化能力。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种基于几何的重建和融合精修渲染框架（GeFu），通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；</p><p>（2）：引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；</p><p>（3）：提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种通用的NeRF方法，能够实现高保真视图合成。具体来说，在重建阶段，我们提出了自适应代价聚合（ACA）来改善几何估计，并提出了空间-视图聚合器（SVA）来编码3D上下文感知描述符。在渲染阶段，我们引入了Consistency-Aware Fusion（CAF）模块，以统一其优势来优化合成视图质量。我们将这些模块整合到一个粗到细的框架中，称为GeFu。广泛的评估和消融实验证明了我们提出的模块的有效性。</p><p>（2）：创新点：提出自适应代价聚合（ACA）、空间-视图聚合器（SVA）和一致性感知融合（CAF）模块，提高了NeRF在具有挑战性条件下的泛化能力；性能：在多视图重建和新视图渲染任务上取得了最先进的性能；工作量：工作量中等，需要修改NeRF的重建和渲染流程。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f615e4a52c82bbd89b40d674212ac03c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccb26edee482b262ae1661c51b02d1d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3ded1a62b2132a2c5b5fdd26dc30947d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cbccb86ceb95a77c9f32e543fe79fbf0.jpg" align="middle"></details><h2 id="Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery"><a href="#Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery" class="headerlink" title="Depth Supervised Neural Surface Reconstruction from Airborne Imagery"></a>Depth Supervised Neural Surface Reconstruction from Airborne Imagery</h2><p><strong>Authors:Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala</strong></p><p>While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images. </p><p><a href="http://arxiv.org/abs/2404.16429v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRFs）作为多视立体（MVS）的替代方法，在空中场景三维重建中展现出 promising 的性能，尤其是在处理无纹理、透明和反射表面等传统 MVS难以处理的场景时。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 在空中图像块（包括仅 nadir、倾斜和高分辨率图像）的三维重建中具有适用性。</li><li>集成平差块调整中提供的联系点测量深度先验信息可以提升重建效果。</li><li>使用基于符号距离函数 (SDF) 的 VolSDF 框架进行重建，更适用于表面重建。</li><li>NeRFs 在图像冗余度低和数据证据弱的区域（如街道峡谷、立面或建筑阴影）存在困难。</li><li>训练 NeRFs 计算成本高。</li><li>在空中场景的三维重建中，NeRFs 面临低图像冗余度和弱数据证据的挑战。</li><li>在仅使用 nadir 图像的情况下，NeRFs 的重建性能低于使用倾斜图像或高分辨率图像的情况。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：航空影像的深度监督神经表面重建</p></li><li><p>作者：V. Hackstein、P. Fauth-Mayer、M. Rothermel、N. Haala</p></li><li><p>所属机构：nFrames ESRI（德国）</p></li><li><p>关键词：神经辐射场（NeRF）、多视立体（MVS）、3D 场景重建、网格化 3D 点云、航空影像、深度监督</p></li><li><p>论文链接：https://arxiv.org/abs/2404.16429 , Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）最初用于新颖视图合成，现已成为多视立体（MVS）的替代方案。NeRF 尤其适用于无纹理、透明和反光表面，而这些场景对于基于 MVS 的传统方法仍然具有挑战性。然而，大多数研究关注近距离场景，而针对航空场景的研究仍然缺失。</p><p>（2）：过去方法及其问题：传统的 MVS 方法在精细几何结构、无纹理区域和非朗伯表面（例如半透明物体或反射）处存在问题。动机充分：本文提出了一种基于深度监督的神经表面重建方法，以解决这些问题。</p><p>（3）：提出的研究方法：本文修改了 VolSDF 框架，将 SfM 关联点监督整合到训练过程中，以支持训练过程。VolSDF 使用符号距离函数（SDF）对 3D 场景进行建模，这比香草 NeRF 中的标准体积表示更适用于表面重建任务。</p><p>（4）：任务和方法性能：本文针对三种航空图像集评估了该管道，这些图像集具有不同的配置。在专业航空测绘中通常使用的数据上的这些研究从实际角度出发很有趣，同时研究了基于 NeRF 的表面重建的具体挑战。此类航空图像集合的视角有限，并且可能因街道峡谷、立面或建筑物阴影而受到影响。该方法在这些任务上取得了良好的性能，表明其可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）回顾 VolSDF（神经辐射场）；</p><p>（2）VolSDF 的 SDF（符号距离函数）表示；</p><p>（3）VolSDF 的正则化；</p><p>（4）VolSDF 的采样；</p><p>（5）Tie 点监督：    （a）Tie 点初始化和监督；    （b）深度监督损失函数：Ltr 和 Lfs；</p><p>（6）实现和训练细节：    （a）模型结构；    （b）训练损失函数：L = LRGB + λeikLeik + λsurfLsurf +λfsLfs + λtrLtr；</p><ol><li>结论：</li></ol><p>（1）：本文展示了 VolSDF（神经辐射场的一种变体，用于建模隐式神经表面）在航空图像三维重建中的适用性。我们证明了通过关联点监督 VolSDF 可以改善重建效果：我们观察到在早期训练阶段收敛速度更快，并且在完整性和准确性方面质量更好。这尤其适用于仅具有有限数据证据的具有挑战性的区域，对于这些区域，VolSDF 往往会陷入局部最小值或根本无法收敛。一个示例正射场景的重建表面在 NMAD 方面比传统的 MVS 管道少于 4 个 GSD 偏差。为了完全收敛并恢复全部细节，仍然需要延长训练时间。这阻碍了实际应用。然而，我们可以在合理的时间内获得拓扑正确的表面，这些表面可以进行后续网格后处理。采样例程是评估实施中的主要瓶颈，并将在未来工作中进行改进。一方面，高效的 GPU 实现可以加速这一过程（Wang 等人，2023 年），另一方面，我们希望研究在有很大改进潜力的区域动态增强采样的可能性（Kerbl 等人，2023 年）。神经隐式表面重建仍然是一个活跃的研究课题，我们希望本文也能鼓励在航空图像和其他遥感应用的几何重建领域开展未来的工作。</p><p>（2）：创新点：提出了基于深度监督的神经表面重建方法，将 SfM 关联点监督整合到 VolSDF 训练过程中，以支持训练过程，提高了重建质量；</p><p>性能：在航空图像集上评估了该管道，在具有挑战性的区域（例如无纹理区域、非朗伯表面）取得了良好的性能，表明其可以支持其目标；</p><p>工作量：需要较长的训练时间才能完全收敛并恢复全部细节，这阻碍了实际应用，采样例程是评估实施中的主要瓶颈。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4036313ed6644db70c73439252a5eaed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e26afc3f9b57484514d8f583efe4569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6ddc84a2cf8fcf12f4f1a29a529e7de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14b2f3ca89df5d53f911326b2d3382d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77dd44d2a901985d20406c555ff9eb2c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>基于三维高斯滴的语音驱动的说话头部合成方法，通过显式表示和对三维面部模型的高斯关联实现精确的唇部运动和面部细节增强，展现出实时渲染性能和卓越的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>采用三维高斯滴的显式表示，解决了NeRF内隐表示的姿态和表情控制不足问题。</li><li>提出的说话人特定运动转换器通过通用的音频特征提取和定制的唇部运动生成，实现了针对目标说话人的精确唇部运动。</li><li>动态高斯渲染器引入了说话人特定混合形状，通过潜在姿势增强面部细节表示，提供稳定且逼真的渲染视频。</li><li>实验结果表明，在说话头部合成方面，该方法优于现有的最先进方法，实现了精确的唇部同步和出色的视觉效果。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的阈值，并有可能部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker：基于3D高斯点云的说话人专属会说话的头像合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific Motion Translator, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p><p>(1): 近期基于神经辐射场(NeRF)的音频驱动说话人头像合成方法取得了令人瞩目的成果。然而，受限于NeRF隐式表示对姿势和表情控制不足，这些方法仍存在唇部运动不同步或不自然、视觉抖动和伪影等问题。</p><p>(2): 过去的方法：基于NeRF的音频驱动说话人头像合成方法；问题：姿势和表情控制不足，导致唇部运动不自然、视觉抖动和伪影。该方法的动机充分，提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法。</p><p>(3): 本文提出的方法：GaussianTalker，该方法由说话人专属运动转换器和动态高斯渲染器两个模块组成。说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动。动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制。</p><p>(4): 该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。该性能支持其目标，即实现自然逼真的说话人头像合成。</p></li><li><p>方法：</p><p>（1）：提出基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；</p><p>（2）：GaussianTalker由说话人专属运动转换器和动态高斯渲染器两个模块组成；</p><p>（3）：说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动；</p><p>（4）：动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制；</p><p>（5）：该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker，该方法将多模态数据与特定说话人关联起来，减少了音频、3D网格和视频之间的潜在身份偏差。说话人专属FLAME转换器采用身份解耦和个性化嵌入，以实现同步和自然的唇部运动，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定和逼真的渲染。大量实验表明，GaussianTalker在说话人头像合成方面优于最先进的性能，同时实现了远超其他方法的超高渲染速度。我们相信，这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；性能：在说话人头像合成方面优于最先进的性能，并实现了远超其他方法的超高渲染速度；工作量：......</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v2">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF） 在自动驾驶领域中的诸多应用。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶领域显示出广阔潜能，应用涵盖感知、三维重建、SLAM和仿真。</li><li>NeRF感知应用包括目标检测、分割和跟踪。</li><li>NeRF三维重建应用可生成高保真三维场景。</li><li>NeRF SLAM 融合了感知和重建，实时创建环境地图。</li><li>NeRF仿真应用可创造逼真的虚拟环境，用于传感器和算法测试。</li><li>研究热点包括跨模态融合、高效表示和动态场景处理。</li><li>NeRF在自动驾驶中的应用仍处于早期阶段，面临挑战和机遇。</li><li>未来方向包括高精度、鲁棒性和实时性能优化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：神经辐射场在自动驾驶中的应用：综述</p></li><li><p>作者：雷贺、李乐恒、孙文超、韩泽宇、刘一辰、郑思发、王建强、李克强</p></li><li><p>Affiliation: 清华大学车辆与运载学院</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.13816 ,Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):神经辐射场（NeRF）凭借其内在优势，特别是其隐式表示和新颖的视图合成能力，在学术界和工业界都备受关注。随着深度学习的快速发展，大量方法涌现出来，探索 NeRF 在自动驾驶（AD）领域的潜在应用。然而，当前文献中存在明显的空白。为了弥补这一差距，本文对 NeRF 在 AD 中的应用进行了全面的调查。我们的调查旨在对 NeRF 在自动驾驶（AD）中的应用进行分类，特别是包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真。我们深入分析并总结了每个应用类别的发现，并通过提供对该领域未来方向的见解和讨论来结束。我们希望这篇论文能为该领域的的研究人员提供全面的参考。据我们所知，这是第一篇专门针对 NeRF 在自动驾驶领域的应用的综述。</p><p>(2):过去的方法主要依赖于高精度地图来提供静态场景理解，现在强调通过鸟瞰视觉实时感知局部环境。同时，它在功能上已从 2 级（L2）发展到努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。NeRF 通过自监督学习，已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的诱人候选者。在过去两年中，NeRF 模型已在自动驾驶的各个方面得到了应用，包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真，如图 1 所示。</p><p>(3):神经辐射场（NeRF）已成为感知领域的很有希望的竞争者，涵盖了一系列关键任务，例如对象检测、语义分割和占据预测。它人气飙升的主要原因是它能够获取精确且一致的几何信息。该领域的研究可分为两大范式，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于扩充感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协同训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(4):在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建侧重于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中解开形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论</strong></li></ol><p>（1）本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的总结，填补了当前文献中的空白，为该领域的研究人员提供了全面的参考。</p><p>（2）创新点：本综述首次专门针对神经辐射场在自动驾驶领域的应用进行了综述；性能：对神经辐射场在感知、3D 重建、SLAM 和仿真等领域的应用进行了深入分析和总结；工作量：工作量大，涉及文献广泛，分析深入。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-55b475e228eebb497768f57fb097059d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-22321e24e9114a3aa3b89b16e6ff76f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://picx.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-02  NeRF-Guided Unsupervised Learning of RGB-D Registration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
</feed>
