<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-02-29T13:26:36.999Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/</id>
    <published>2024-02-29T13:26:36.000Z</published>
    <updated>2024-02-29T13:26:36.999Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p><p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p><p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>神经辐射场（NeRF）的最新混合表示方法，即动态四面体（DynTet），通过神经网络对明确动态网格进行编码，以确保各种动作和视点的几何一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>DynTet 是一种新的混合表示，它使用神经网络对显式动态网格进行编码，以确保不同动作和视点下的几何一致性。</li><li>DynTet 使用基于坐标的网络对符号距离、变形和材质纹理进行学习，将训练数据锚定到预定义的四面体网格中。</li><li>DynTet 利用 Marching Tetrahedra 有效地解码了具有稳定拓扑结构的纹理网格，并通过可微分光栅器和像素损失的监督实现了快速渲染。</li><li>DynTet 结合经典的 3D 可变形模型来促进几何学习，并定义了一个规范化空间来简化纹理学习。</li><li>与之前的研究相比，DynTet 在保真度、唇形同步和实时性能方面有了显著的提升。</li><li>除了制作出稳定且视觉上吸引人的合成视频外，该方法还输出动态网格，有望实现许多新兴应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：用于高品质说话人头部合成的动态四面体学习</li><li>作者：张子川，张恒，王佳俊，刘子超，孙剑</li><li>单位：北京大学</li><li>关键词：说话人头部合成、隐式表示、动态网格、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2302.02574</li><li>摘要：（1）研究背景：近年来，隐式表示方法，如神经辐射场（NeRF），在从视频序列生成逼真且可动画化的头部头像方面取得了进展。然而，这些隐式方法仍然面临视觉伪影和抖动问题，因为缺乏明确的几何约束，这给准确建模复杂的面部变形带来了根本性挑战。（2）过去方法及问题：过去的方法主要采用隐式表示，但缺乏明确的几何约束，导致视觉伪影和抖动问题。（3）本文提出的研究方法：本文提出了一种新颖的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。DynTet 由基于坐标的网络参数化，该网络学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。为了提高训练效率，本文结合了经典的 3D 可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。这些优势得益于 DynTet 中采用的有效几何表示。（4）方法性能及对目标的支持：与以往的工作相比，根据各种指标，DynTet 在保真度、唇形同步和实时性能方面均表现出显着提升。除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。</li></ol><p>7.方法：(1): 动态四面体（DynTet）通过神经网络对显式动态网格进行编码，确保几何一致性；(2): 基于坐标的网络参数化，学习符号距离、变形和材质纹理，将数据锚定到四面体网格中；(3): 利用行进四面体解码纹理网格，通过可微分光栅化器渲染并通过像素损失进行监督；(4): 结合经典的3D可变形模型促进几何学习，定义规范空间简化纹理学习。</p><ol><li>总结：（1）：本文提出了动态四面体（DynTet）方法，通过神经网络对显式动态网格进行编码，确保几何一致性，提升了说话人头部合成的保真度、唇形同步和实时性能。（2）：创新点：</li><li>提出了一种新的混合表示方法，称为动态四面体（DynTet），它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下几何一致性。</li><li>基于坐标的网络参数化，学习符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。</li><li>利用行进四面体，DynTet可以有效地解码具有相同拓扑结构的纹理网格，从而可以通过可微分光栅化器快速渲染，并通过像素损失进行监督。</li><li>结合了经典的3D可变形模型来促进几何学习，并定义了一个规范空间来简化纹理学习。</li><li>这些优势得益于DynTet中采用的有效几何表示。</li><li>与以往的工作相比，根据各种指标，DynTet在保真度、唇形同步和实时性能方面均表现出显着提升。</li><li>除了生成稳定且视觉上吸引人的合成视频外，本文方法还输出动态网格，有望支持许多新兴应用。性能：</li><li>在保真度、唇形同步和实时性能方面均表现出显着提升。</li><li>生成了稳定且视觉上吸引人的合成视频。</li><li>输出动态网格，有望支持许多新兴应用。工作量：</li><li>论文中没有明确提到工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://pica.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details><h2 id="CharNeRF-3D-Character-Generation-from-Concept-Art"><a href="#CharNeRF-3D-Character-Generation-from-Concept-Art" class="headerlink" title="CharNeRF: 3D Character Generation from Concept Art"></a>CharNeRF: 3D Character Generation from Concept Art</h2><p><strong>Authors:Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan</strong></p><p>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model’s inferencing capabilities are influenced by the training data’s characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data. </p><p><a href="http://arxiv.org/abs/2402.17115v1">PDF</a> </p><p><strong>Summary</strong><br>用概念图创建 3D 模型的新方法，利用神经辐射场并为图像建模提供更好的视角。</p><p><strong>Key Takeaways</strong></p><ul><li>艺术创作和实际应用中，3D 建模很有价值，但需要花费时间和技能。</li><li>该方法从标准的 3D 建模行业输入，即可根据一致的透视图概念图创建 3D 角色的体积表示。</li><li>神经辐射场 (NeRF) 已改变基于图像的 3D 重建，但尚无针对概念图优化管道。</li><li>编码概念图为模型的先验，利用概念图中的清晰的身体姿势和特定的视角。</li><li>通过可学习的视向注意力多头自注意力层，训练网络利用各种 3D 点的先验。</li><li>射线采样和表面采样的组合增强了网络的推理能力。</li><li>模型可以生成高质量的 360 度角色视图。</li><li>开发了简单的指南，以更好地利用模型提取 3D 网格。</li><li>模型的推理能力受训练数据的影响，主要针对头部、手臂和腿部。</li><li>该方法适用于各种主题的概念图，对数据没有特殊假设。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：CharNeRF：基于概念图的 3D 角色生成</li><li>作者：Eddy Chu、Yiyang Chen、Chedy Raissi、Anand Bhojan</li><li>第一作者单位：新加坡国立大学</li><li>关键词：神经网络、计算机图形、虚拟现实、游戏、网格生成</li><li>论文链接：https://arxiv.org/abs/2402.17115</li><li><p>摘要：（1）研究背景：3D 建模在 AR/VR 和游戏中至关重要，但通常耗时且要求高。本文提出了一种从一致的周转概念图中创建 3D 角色体积表示的新方法。（2）过去的方法：神经辐射场 (NeRF) 已成为图像重建的变革者，但尚无针对概念图优化管线的研究。（3）研究方法：本文利用概念图中的定义的身体姿势和特定的视角，将其编码为模型的先验。提出了一种可学习的视图方向注意力多头自注意力层，让网络利用这些先验。此外，本文还证明了光线采样和表面采样的组合增强了网络的推理能力。（4）任务和性能：本文模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p></li><li><p>方法：(1) 编码概念图：采用双层沙漏编码器，提取概念图的高低层次细节。(2) 视图方向注意力多头自注意力特征向量组合：使用多头自注意力机制融合来自概念图的三个特征向量，重点关注查询视图方向与源草图视图方向之间的相似性。(3) 神经辐射场：使用神经辐射场预测最终颜色和密度，指导网络学习特定类别的一般形状和特征。</p></li><li><p>结论：（1）：本工作尝试解决计算机视觉中一个具有重要 AR/VR/游戏应用价值的挑战性问题，即使用 NeRF 从概念图构建 3D 角色的 3D 表示。我们提出的最终模型 CharNeRF 得益于用于组合不同输入视图信息的视图方向注意力多头自注意力组件，能够从如此稀疏的图像输入中生成良好的结果。（2）：创新点：提出了一种可学习的视图方向注意力多头自注意力层，让网络利用概念图中的定义的身体姿势和特定的视角。此外，还证明了光线采样和表面采样的组合增强了网络的推理能力。性能：模型能够生成高质量的 360 度角色视图。此外，还提供了一个简单的指南，以更好地利用模型提取 3D 网格。工作量：模型的推理能力受训练数据特征的影响，主要针对具有一个头部、两个手臂和两条腿的角色。尽管如此，本文方法具有通用性，可适应不同主题的概念图，而无需对数据做出任何特定假设。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-828eaae544f50ff5c3cb4c05ee9d80e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef7369a7d8878e03f6b272a4d1ebd217.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19f2984d16b69f5650701e035c363f95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b8a11537cec84e0f035cff561493d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f60295f4a9ff4a9d9749851b16f04d26.jpg" align="middle"></details><h2 id="CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency"><a href="#CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency" class="headerlink" title="CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency"></a>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</h2><p><strong>Authors:Hanxin Zhu, Tianyu He, Zhibo Chen</strong></p><p>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.16407v1">PDF</a> Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces   (IEEE VR 2024)</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在全新视角合成中展示出令人印象深刻的效果，特别是在虚拟现实 (VR) 和增强现实 (AR) 中，这得益于其连续表示场景的能力。然而，当只有少数输入视图图像可用时，NeRF 倾向于对给定的视图进行过度拟合，从而使估计的像素深度几乎具有相同的值。不同于通过引入复杂先验或附加监督来进行正则化的先前方法，我们提出了一种简单但有效的方法，该方法明确构建了输入视图之间的深度感知一致性来解决这一挑战。我们的关键见解是，通过强制相同的空间点在不同的输入视图中被重复采样，我们能够加强视图之间的交互，从而减轻过度拟合问题。为了实现这一点，我们在分层表示（即多平面图像）上建立神经网络，并且采样点可以在多个离散平面上重新采样。此外，为了正则化未见的目标视图，我们约束不同输入视图的渲染颜色和深度相同。虽然简单，但大量的实验表明，我们提出的方法可以比最先进的方法实现更好的合成质量。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在只有少数输入视图图像可用时会过拟合。</li><li>通过强制相同的空间点在不同的输入视图中被重复采样可以减轻过度拟合问题。</li><li>我们在分层表示上构建神经网络，以便在多个离散平面上重新采样采样点。</li><li>我们约束不同输入视图的渲染颜色和深度相同，以正则化未见的目标视图。</li><li>我们的方法比最先进的方法实现了更好的合成质量。</li><li>我们方法的关键在于显式构建输入视图之间的深度感知一致性。</li><li>我们的方法简单有效，不需要引入复杂先验或额外的监督。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CMC：通过跨视图多平面一致性进行小样本新视角合成</li><li>作者：韩昕竹、何天宇、陈志波</li><li>第一作者单位：中国科学技术大学</li><li>关键词：神经辐射场、小样本视角合成、多平面图像、跨视图一致性</li><li>论文链接：None, Github 链接：None</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在小样本视角合成中容易出现过拟合问题，导致估计的像素深度几乎相同。（2）过去方法：现有方法通过引入复杂先验或额外监督来进行正则化，但存在预训练成本高、域差距等问题。（3）研究方法：本文提出了一种简单有效的跨视图深度感知一致性方法，通过在不同输入视图中强制采样相同空间点，加强视图之间的交互，缓解过拟合问题。具体来说，本文构建了基于分层表示（即多平面图像）的神经网络，并对多平面进行采样。此外，为了正则化未见的目标视图，本文约束了不同输入视图渲染的颜色和深度一致性。（4）方法性能：实验表明，本文方法在小样本视角合成任务上优于现有方法，证明了其有效性。</li></ol><p>7.Methods:(1):构建基于分层表示的多平面图像，并对其进行采样；(2):通过在不同输入视图中强制采样相同空间点，加强视图之间的交互；(3):约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。</p><ol><li>结论：(1): 本文提出了 CMC 方法，通过跨视图多平面一致性，缓解了 NeRF 在小样本视角合成中的过拟合问题，提升了合成图像的质量。(2): 创新点：<ul><li>提出跨视图深度感知一致性方法，加强视图之间的交互，缓解过拟合。</li><li>构建基于分层表示的多平面图像，并对其进行采样。</li><li>约束不同输入视图渲染的颜色和深度一致性，正则化未见的目标视图。Performance：</li><li>在小样本视角合成任务上优于现有方法，证明了其有效性。Workload：</li><li>方法简单有效，易于实现。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bdd46c7b217cb4180eb948c43ffad849.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-571786b47c356d9bc3c90a0ca95fe68b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bf909d8f8aa9e18f65bc56fd97a0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0da54ff7a201688851cb82cbbbe20007.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eff9d03d40a8b3f7618fd67f793df987.jpg" align="middle"></details><h2 id="SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field"><a href="#SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field" class="headerlink" title="SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field"></a>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</h2><p><strong>Authors:Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao</strong></p><p>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time. </p><p><a href="http://arxiv.org/abs/2402.16366v1">PDF</a> </p><p><strong>Summary</strong><br>利用空间预测编码对神经辐射场（NeRF）的显式体素网格（EVG）进行压缩，可有效提升其存储和传输效率。</p><p><strong>Key Takeaways</strong></p><ul><li>提出基于显式体素网格（voxel grid）的 NeRF 压缩新框架——SPC-NeRF</li><li>利用空间预测编码有效去除体素的空间冗余，提升压缩性能</li><li>提出新的比特率建模和损失函数形式，实现压缩率与失真的联合优化</li><li>在多个代表性测试数据集上，与最先进的 VQRF 方法相比，节省 32% 的比特率</li><li>训练时间与 VQRF 相当</li><li>充分利用了体素的空间相关性，优于从神经网络压缩方法继承的压缩技术</li><li>显式体素网格的压缩对于 NeRF 的存储和传输至关重要</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SPC-NeRF：体素化光场辐射的空域预测压缩</li><li>作者：宋泽天、段文宏、张宇怀、王诗奇、马思伟、高文</li><li>单位：北京大学</li><li>关键词：NeRF、EVG、空域预测编码、数据压缩</li><li>论文链接：https://arxiv.org/abs/2402.16366    Github 代码链接：无</li><li>摘要：（1）研究背景：使用显式体素网格（EVG）表示神经辐射场（NeRF）是提升 NeRF 性能的一个有前景的方向。然而，EVG 表示在存储和传输方面效率低下，因为内存开销巨大。当前用于压缩 EVG 的方法主要继承了为神经网络压缩设计的剪枝和量化等方法，而这些方法并没有充分利用体素的空间相关性。（2）过去方法：现有方法主要利用神经网络压缩技术，如剪枝和量化，但这些方法没有充分利用体素的空间相关性。（3）研究方法：受繁荣的数字图像压缩技术启发，本文提出了 SPC-NeRF，一个将空域预测编码应用于 EVG 压缩的新框架。提出的框架可以有效去除空间冗余，以获得更好的压缩性能。此外，我们对比特率进行建模并设计了新的损失函数形式，在该损失函数中，我们可以联合优化压缩比和失真，以实现更高的编码效率。（4）实验结果：大量实验表明，与最先进的 EVG NeRF 压缩方法 VQRF 相比，我们的方法在多个代表性测试数据集上实现了 32% 的比特节省，训练时间相当。</li></ol><p>7.方法：(1)受数字图像压缩技术的启发，提出SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架；(2)将EVG表示为特征网格，并利用其空间相关性，通过预测编码去除空间冗余；(3)设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。</p><ol><li>总结（1）：本文工作的主要意义在于提出了SPC-NeRF，一个将空域预测编码应用于EVG压缩的新框架，有效去除了空间冗余，提高了压缩性能。（2）：创新点：• 提出SPC-NeRF，将空域预测编码应用于EVG压缩，充分利用了体素的空间相关性。• 设计新的损失函数形式，联合优化压缩比和失真，实现更高的编码效率。性能：• 与最先进的EVG-NeRF压缩方法VQRF相比，在多个代表性测试数据集上实现了32%的比特节省，训练时间相当。工作量：• 论文理论分析清晰，实验结果充分，代码开源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6f6705a1aaf3db9b5a416e3ffecb9e26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5908f2606537f6a0653b96477b77c75f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efc08eb0ec890344de572f2b2004f9c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-866d14094e6f176536a298862171f8d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3117d16ce413f3de96c9535aaa0804e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d0efdf7e947815763e89d08400d8bd32.jpg" align="middle"></details><h2 id="GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction"><a href="#GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction" class="headerlink" title="GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction"></a>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</h2><p><strong>Authors:Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang</strong></p><p>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions. </p><p><a href="http://arxiv.org/abs/2402.16174v1">PDF</a> </p><p><strong>Summary</strong><br>人工智能驱动场景重建的自动化拍摄过程，提升了真实感，简化了工作</p><p><strong>Key Takeaways</strong></p><ul><li>利用强化学习的自动化拍摄流程</li><li>5D自由空间扩展了动作范围</li><li>多源状态嵌入增强了跨数据集泛化性</li><li>Isaac Gym模拟器建立了NBV策略评估基准</li><li>在Houses3K和OmniObject3D数据集上，覆盖率分别达到98.26%和97.12%</li><li>优于现有解决方案</li><li>适用于大型场景的扫描和交互</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GenNBV：用于主动 3D 重建的可泛化最佳下一视角策略</li><li>作者：Ziqi Wang, Xinyu Zhang, Tianhao Wu, Yinda Zhang, Xiaogang Jin, Yu Rong, Hui Huang</li><li>隶属：清华大学</li><li>关键词：主动 3D 重建，最佳下一视角，深度学习，强化学习</li><li>论文链接：GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction，Github 链接：None</li><li>摘要：（1）：研究背景：神经辐射场在逼真数字化大型场景方面取得了最新进展，但图像捕捉过程仍然耗时且费力。以往工作尝试使用最佳下一视角（NBV）策略来自动执行此过程以主动进行 3D 重建。（2）：过去方法及其问题：现有的 NBV 策略严重依赖于手工制作的标准、有限的动作空间或针对特定场景优化后的表示。这些限制因素限制了它们在不同数据集上的泛化能力。（3）：论文提出的研究方法：提出 GenNBV，一种端到端可泛化的 NBV 策略。该策略采用基于强化学习（RL）的框架，并将典型有限的动作空间扩展到 5D 自由空间。它使代理无人机能够从任何视点进行扫描，甚至在训练期间与看不见的几何体进行交互。为了提高跨数据集的泛化能力，还提出了一种新颖的多源状态嵌入，包括几何、语义和动作表示。（4）：方法在什么任务上取得了怎样的性能：使用 IsaacGym 模拟器和 Houses3K 及 OmniObject3D 数据集建立基准来评估此 NBV 策略。实验表明，该策略在这些数据集未曾见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案。</li></ol><p>7.方法：（1）将主动3D重建问题表述为马尔可夫决策过程（MDP），设计新的观测空间和动作空间；（2）提出端到端的NBV策略，该策略将典型有限的动作空间扩展到5D自由空间；（3）提出一种新的多源状态嵌入，包括几何、语义和动作表示，以提高跨数据集的泛化能力；（4）设计反映优化目标的奖励函数，并详细说明策略优化过程。</p><ol><li>结论：（1）：本研究提出了一种主动 3D 场景重建的端到端方法，减少了人工干预的需要。具体来说，基于学习的策略探索了如何在训练阶段重建各种对象，从而能够以完全自主的方式泛化以重建看不见的对象。我们的控制器在自由空间中机动，然后基于混合场景表示选择下一个最佳视图，该表示传达了场景覆盖状态，从而实现重建进度。我们通过在包括 Houses3K、OmniObject3D 和 Objaverse 在内的多个数据集上进行测试，展示了我们方法的有效性。在 holdout Houses3K 测试集和跨域 OmniObject3D 房屋类别上的定量和定性泛化结果表明，我们的方法在重建的完整性、效率和准确性方面优于其他基线。此外，在 Objaverse 上进行的实验表明，在单一建筑设置中训练的策略甚至可以泛化到复杂的户外场景。（2）：创新点：GenNBV 提出了一种端到端可泛化的最佳下一视角策略，扩展了动作空间，并提出了一种新的多源状态嵌入来提高跨数据集的泛化能力；性能：在 Houses3K 和 OmniObject3D 数据集上，GenNBV 在未见过的建筑规模物体上分别达到 98.26% 和 97.12% 的覆盖率，优于先前的解决方案；工作量：GenNBV 的训练过程需要大量的数据和计算资源，并且需要针对不同的场景进行微调以获得最佳性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5e8d5c56796ce65689171d3e4517ceb1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3132d23adee2a0316b9fc9d6cad91a0b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f46161465b1542e68d3bcde0a29f1da4.jpg" align="middle"></details><h2 id="NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection"><a href="#NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection" class="headerlink" title="NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection"></a>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection</h2><p><strong>Authors:Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</strong></p><p>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at <a href="https://github.com/mrsempress/NeRF-Detplusplus">https://github.com/mrsempress/NeRF-Detplusplus</a>. </p><p><a href="http://arxiv.org/abs/2402.14464v1">PDF</a> 7 pages, 2 figures</p><p><strong>Summary</strong><br>神经辐射场（NeRF）技术被创新应用于增强多视角3D检测任务中的表示学习，显著提升了室内场景中的3D检测性能。</p><p><strong>Key Takeaways</strong></p><ul><li>发现了NeRF-Det存在语义歧义、采样不当和深度监督利用不足等主要缺陷。</li><li>提出语义增强、透视感知采样和序数残差深度监督来解决上述问题。</li><li>NeRF-Det++有效解决了NeRF-Det的缺陷，在ScanNetV2和ARKITScenes数据集上表现出色。</li><li>NeRF-Det++在ScanNetV2上比NeRF-Det在mAP@0.25和mAP@0.50分别提高了1.9%和3.5%。</li><li>代码已公开发布：<a href="https://github.com/mrsempress/NeRF-Detplusplus。">https://github.com/mrsempress/NeRF-Detplusplus。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> NeRF-Det++：融合语义线索和视点感知深度</li><li><strong>作者：</strong> Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</li><li><strong>第一作者单位：</strong> 浙江大学计算机科学与技术学院，计算机辅助设计与图形学国家重点实验室</li><li><strong>关键词：</strong> NeRF、多视图三维检测、语义分割、深度估计</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2402.14464</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong> NeRF-Det 在室内多视图三维检测中取得了令人印象深刻的性能，它创新性地利用 NeRF 增强了表征学习。   (2) <strong>过去方法及问题：</strong> NeRF-Det 存在语义模糊、采样不当和深度监督利用不足三个关键缺陷。   (3) <strong>研究方法：</strong> 针对上述问题，本文提出了三个相应的解决方案：<ul><li><strong>语义增强：</strong> 将免费提供的 3D 分割注释投影到 2D 平面，并利用相应的 2D 语义图作为监督信号，显著增强了多视图检测器的语义感知能力。</li><li><strong>视点感知采样：</strong> 提出视点感知采样策略，该策略在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。</li><li><strong>有序残差深度监督：</strong> 与直接回归难以优化的深度值相反，将每个场景的深度范围划分为固定数量的有序箱，并将深度预测重新表述为深度箱分类和残差深度值回归的组合，从而有利于深度学习过程。   (4) <strong>方法性能：</strong> 在室内多视图三维检测任务上，本文方法取得了优异的性能，证明了其有效性。</li></ul></li></ol><p>7.方法：（1）语义增强：在NeRF-Det中加入语义分支ΦS，将几何模块ΦG生成的特征h(x)输入ΦS，产生语义预测s，并利用交叉熵损失LSeg监督语义图的学习。（2）视点感知采样：将NeRF-Det中的均匀采样（US）替换为视点感知采样策略，在靠近相机处密集采样，而在远处稀疏采样，更有效地收集有价值的几何线索。（3）有序残差深度监督：将每个场景的深度范围划分为固定数量的有序箱，将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。</p><ol><li>结论：(1): 本文提出 NeRF-Det++，一种用于从多视图图像进行室内 3D 检测的新颖方法。我们识别并解决了 NeRF-Det 中的三个关键缺陷。首先，为了解决语义模糊，我们引入了语义增强模块，该模块利用语义监督来改善分类。其次，为了解决不适当的采样，我们通过透视感知采样的设计优先考虑附近对象并利用多视图的特性。最后，我们通过提出序数残差深度监督来解决深度监督利用不足的问题，该监督结合了序数深度箱的分类和残差深度值的回归。在 ScanNetV2 和 ARKIT 场景上进行的广泛实验验证了我们 NeRF-Det++ 的优越性。(2): 创新点：</li><li>语义增强：引入语义分支，利用语义监督增强语义感知能力。</li><li>透视感知采样：设计透视感知采样策略，更有效地收集有价值的几何线索。</li><li>序数残差深度监督：将深度预测重新表述为深度箱分类和残差深度值回归的组合，有利于深度学习过程。性能：</li><li>在 ScanNetV2 和 ARKIT 场景上取得了优异的性能，证明了其有效性。工作量：</li><li>提出了一种新的方法 NeRF-Det++，涉及语义增强、透视感知采样和序数残差深度监督。</li><li>在 ScanNetV2 和 ARKIT 场景上进行了广泛的实验，证明了其优越性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-10b590fb75f1e40d114fb69be9c25a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffacf9378a148c5b9fac1fd2e03fc268.jpg" align="middle"><img src="https://picx.zhimg.com/v2-478a5df442fbaaa3a3c020c875f267ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecbc9426af10136860227da1181ee0cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af160b3a5172d7fc20bcc97ad42a6d6f.jpg" align="middle"></details><h2 id="Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields"><a href="#Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields" class="headerlink" title="Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields"></a>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</h2><p><strong>Authors:Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</strong></p><p>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering “jaggies” or “blurry” images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see <a href="https://stnamjef.github.io/mipgrid.github.io/">https://stnamjef.github.io/mipgrid.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.14196v1">PDF</a> Accepted to NeurIPS 2023</p><p><strong>Summary</strong><br>基于网格表示的反走样 NeRF 方法，实现快速训练同时消除混叠伪影。</p><p><strong>Key Takeaways</strong></p><ul><li>mip-Grid 将反走样技术集成到基于网格的 NeRF 中，解决了混叠问题。</li><li>使用简单卷积操作在共享网格表示上生成多尺度网格，减轻了混叠伪影。</li><li>使用尺度感知坐标从生成的多尺度网格中检索不同尺度的特征。</li><li>将该方法集成到 TensoRF 和 K-Planes 等基于网格的 NeRF 方法中。</li><li>实验表明 mip-Grid 大幅提高了两种方法的渲染性能，在多尺度数据集上甚至优于 mip-NeRF。</li><li>mip-Grid 实现了显著更快的训练时间。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：Mip-Grid：神经辐射场中的抗锯齿网格表示（中文翻译）2.作者：Seungtae Nam、Daniel Rho、Jong Hwan Ko、Eunbyung Park3.第一作者单位：韩国成均馆大学人工智能系（中文翻译）4.关键词：神经辐射场、抗锯齿、网格表示5.论文链接：https://arxiv.org/abs/2402.14196Github代码链接：无6.总结：（1）：研究背景：神经辐射场（NeRF）在表示3D场景和生成新视图图像方面取得了显著成就，但现有的方法中普遍存在锯齿问题，即在不同的相机距离下渲染出“锯齿”或“模糊”的图像。（2）：过去方法：mip-NeRF通过渲染圆锥截锥体而不是射线来解决这个问题。然而，它依赖于MLP架构来表示辐射场，错失了基于网格的最新方法提供的快速训练速度。（3）：本文提出的研究方法：mip-Grid，一种将抗锯齿技术集成到基于网格的辐射场表示中的新方法，在享受快速训练时间的同时减轻了锯齿伪影。该方法通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。（4）：方法在任务和性能上的表现：为了测试有效性，我们将提出的方法集成到两种最新的基于网格的代表性方法中，即TensoRF和K-Planes。实验结果表明，mip-Grid极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于mip-NeRF，同时实现了明显更快的训练时间。</p><ol><li><p>方法：（1）：mip-Grid 将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。（2）：为了测试有效性，将提出的方法集成到两种最新的基于网格的代表性方法中，即 TensoRF 和 K-Planes。实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p></li><li><p>结论：（1）：本工作提出了 mip-Grid，一种用于 NeRF 的抗锯齿网格表示。提出的方法可以轻松集成到现有的基于网格的 NeRF 中，并且使用我们方法的两种方法 mip-TensoRF 和 mip-K-Planes 已经证明可以有效去除混叠伪影。由于我们从共享的网格表示中生成多尺度网格，并且不依赖于超采样，因此所提出的方法最大程度地减少了额外参数的数量，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。我们相信我们的工作为利用网格表示的训练效率，朝着无混叠 NeRF 的新研究方向铺平了道路。</p></li></ol><p>（2）：创新点：将抗锯齿技术集成到基于网格的辐射场表示中，通过在共享网格表示上应用简单的卷积操作生成多尺度网格，并使用尺度感知坐标从生成的网格中检索不同尺度的特征。</p><p>性能：在两种最新的基于网格的代表性方法 TensoRF 和 K-Planes 中集成提出的方法，实验结果表明，mip-Grid 极大地提高了这两种方法的渲染性能，甚至在多尺度数据集上也优于 mip-NeRF，同时实现了明显更快的训练时间。</p><p>工作量：mip-Grid 是一种简单且易于实现的方法，它可以轻松集成到现有的基于网格的 NeRF 中。该方法不需要额外的超采样步骤，并且训练速度明显快于现有的基于 MLP 的抗锯齿 NeRF。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f43ff38bcf01c320536c04f1be39506c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcbbb2f379d74a0aeb7179da023c78a5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe3f4f6d4cf8758d74cb0be86547e9f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7b2eb107a8f1fa6044a1d951be6c903a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-29  Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/</id>
    <published>2024-02-29T13:05:25.000Z</published>
    <updated>2024-02-29T13:05:25.532Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction"><a href="#VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction" class="headerlink" title="VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction"></a>VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</h2><p><strong>Authors:Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</strong></p><p>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. </p><p><a href="http://arxiv.org/abs/2402.17427v1">PDF</a> Accepted to CVPR 2024. Project website:   <a href="https://vastgaussian.github.io">https://vastgaussian.github.io</a></p><p><strong>Summary</strong><br>利用 3D 高斯斑点技术，我们提出了 VastGaussian，一种用于大场景的高质量重建和实时渲染的新方法。</p><p><strong>Key Takeaways</strong></p><ul><li>提出渐进分区策略，使用视野感知可见性标准分配训练相机和点云。</li><li>引入解耦外观建模，减少渲染图像外观变化。</li><li>在多个大场景数据集上优于现有基于 NeRF 的方法。</li><li>实现最先进的成果，实现快速优化和高保真实时渲染。</li><li>使用 3D 高斯斑点技术进行大场景重建和渲染。</li><li>解决视频内存受限、优化时间长、外观变化明显等问题。</li><li>适用多个大场景数据集，包括 Matterport3D，SUNCG，和 Replica。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VastGaussian：用于大场景重建的巨大 3D 高斯体</li><li>作者：Yuan Liu、Li-Yi Wei、Jia-Bin Huang、Yong-Liang Yang、Tong-Yee Lee</li><li>第一作者单位：香港中文大学（深圳）</li><li>关键词：NeRF、大场景重建、高斯体、外观建模</li><li>论文链接：https://arxiv.org/abs/2302.04750，Github 代码链接：None</li><li><p>摘要：（1）研究背景：现有的基于 NeRF 的大场景重建方法在视觉质量和渲染速度上往往存在局限性。虽然最近的 3D 高斯体喷绘法在小规模和以物体为中心的场景中效果很好，但由于视频内存有限、优化时间长和外观变化明显，将其扩展到大型场景中会带来挑战。（2）过去方法及其问题：本文方法的动机充分：为了解决这些挑战，我们提出了 VastGaussian，这是一种基于 3D 高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法。（3）论文提出的研究方法：我们提出了一种渐进分区策略，将大场景划分为多个单元格，其中训练相机和点云通过考虑空域可见性的标准进行适当分布。在并行优化后，这些单元格被合并成一个完整的场景。我们还将解耦的外观建模引入优化过程，以减少渲染图像中的外观变化。（4）方法在什么任务上取得了怎样的性能，性能是否能支撑其目标：我们的方法优于现有的基于 NeRF 的方法，并在多个大场景数据集上取得了最先进的结果，实现了快速优化和高保真实时渲染。</p></li><li><p>方法：（1）：渐进数据分区：根据相机位置和可见性标准将大场景划分为多个单元格，并分配部分相机和点云进行优化。（2）：解耦外观建模：引入外观嵌入和卷积神经网络，通过对渲染图像进行外观调整来减少外观变化。（3）：无缝合并：优化各个单元格后，删除单元格外部的高斯体，然后合并非重叠单元格的高斯体，形成无缝的大场景。</p></li></ol><p>8.结论：(1): 本工作提出了VastGaussian，一种基于3D高斯体喷绘法在大场景上进行高质量重建和实时渲染的第一种方法，解决了现有方法在视觉质量和渲染速度上的局限性。(2): 创新点：- 渐进数据分区：将大场景划分为单元格，并分配部分相机和点云进行优化，解决了视频内存有限和优化时间长的挑战。- 解耦外观建模：引入外观嵌入和卷积神经网络，减少了渲染图像中的外观变化，提高了视觉质量。- 无缝合并：优化各个单元格后，合并非重叠单元格的高斯体，形成了无缝的大场景。性能：- 在多个大场景数据集上取得了最先进的结果。- 实现快速优化和高保真实时渲染。工作量：- 论文提供了详细的算法描述和实验结果。- Github代码暂未提供。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ee052136cbbee0e4d283f8c1613aa5c9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9222e251d2d4b3d336feb1e5dc10d3c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9fb6f7a1a19593c7cf97f51e62283477.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9609bd8a7bee5ba2688b0bf50aa99233.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04b4a21a99a56fa621e5dc34b03bb714.jpg" align="middle"><img src="https://pica.zhimg.com/v2-16c21380cd415ab4eb8e703f94c84868.jpg" align="middle"></details>## GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video**Authors:Xinqi Liu, Chenming Wu, Xing Liu, Jialun Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang**This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians. The key contributions are twofold. First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model. It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image. Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation. This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations. As a result, higher-quality rendering can be achieved. Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/. [PDF](http://arxiv.org/abs/2402.16607v1) **Summary**利用基于 3D 高斯体的手部和身体高保真重建技术创造富有表现力的 3D 头像。**Key Takeaways**- 采用两阶段姿势估计方法，从输入图像中获取准确的 SMPL-X 姿势。- 提出迭代重新初始化策略，处理高斯表示中遇到的不平衡聚合和初始化偏差。- 该模型在图像真实的新视角合成方面实现了最先进的性能。- 允许对人体和手部姿态进行精细控制。- 实验分析验证了该模型的有效性。- 提供项目主页链接：https://3d-aigc.github.io/GEA/。- 该方法在创建表达力丰富的 3D 头像方面具有应用潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GEA：基于 3D 高斯重建表达式 3D 头像</li><li>作者：刘新奇、吴晨明、刘兴、刘家伦、武金波、赵晨、冯浩成、丁尔瑞、王京东</li><li>单位：百度视觉技术部</li><li>关键词：3D 头像、高斯表示、单目视频、姿态估计</li><li>论文链接：https://arxiv.org/abs/2402.16607，Github 代码链接：无</li><li><p>总结：（1）研究背景：重建逼真且可驱动的头像一直是学术界和工业界的热点课题，具有广阔的商业价值和社会影响。（2）过去方法：早期方法主要依赖于 RGB-D 相机、多视角采集设备和人工建模，但存在成本高、渲染效果不逼真等问题。神经辐射场方法虽然可以重建逼真的 3D 头像，但训练时间长、姿态泛化能力有限。3D 高斯表示方法因其显式表示而受到关注，但存在初始化不均衡和聚集不平衡的问题。（3）研究方法：本文提出的 GEA 方法包括两大贡献。一是设计了一种两阶段姿态估计方法，通过注意力感知网络和优化方案，从输入图像中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。二是提出了一种迭代式重新初始化策略，通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，从而提高渲染质量。（4）任务和性能：GEA 方法在真实感新视图合成任务上取得了最先进的性能，同时提供了对人体和手部姿态的精细控制。实验结果验证了该方法的有效性，支持其目标。</p></li><li><p><strong>姿态估计</strong>：提出两阶段姿态估计方法，通过注意力感知网络和优化方案，从单目视频中准确估计 SMPL-X 姿态，建立图像像素与 SMPL-X 模型之间的正确映射。</p></li><li><strong>迭代式重新初始化</strong>：通过网格化、重采样和高斯重新操作，迭代地重新分配头像的高斯点，使其均匀分布在人体表面附近，提高渲染质量。</li><li><strong>3D 高斯表示</strong>：采用 3D 高斯点集合表示头像的形状和外观，并使用 SMPL-X 骨架模型实现详细的姿态控制。</li><li><p><strong>渲染损失函数</strong>：使用 SMPL-X 骨架变换将高斯头像从规范空间驱动到图像空间，并使用差异化渲染进行优化。损失函数包括重建损失、感知损失和残差正则化。</p></li><li><p>结论（1）：本文提出了一种可由身体和手驱动的 3D 高斯头像重建方法，该方法从单目视频中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。此外，还引入了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。我们的目标是，这项贡献将为未来更逼真的头像重建铺平道路。（2）：创新点：</p></li><li>提出了一种两阶段姿势细化机制，从图像中获取 SMPL-X 姿态参数，指导 3D 高斯头像学习全身形状和外观。</li><li>提出了一种迭代重新初始化机制，以避免 3D 高斯不平衡聚合和初始化偏差的问题。性能：</li><li>在真实感新视图合成任务上取得了最先进的性能。</li><li>提供了对人体和手部姿态的精细控制。工作量：</li><li>需要大量的数据和计算资源。</li><li>训练过程可能耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9b9982465510d1b66a23858c60af4331.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c8ddc4d64a0f61f1a9a17acb134824c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9a9a5ebfedeaeecdc381441fa23504f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2131167109a684b8747fb7451590f0d3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0d2c2740f3fa02de0dd80788a7d2df2.jpg" align="middle"></details><h2 id="Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting"><a href="#Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting" class="headerlink" title="Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting"></a>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting</h2><p><strong>Authors:Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</strong></p><p>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. </p><p><a href="http://arxiv.org/abs/2402.15870v1">PDF</a> </p><p><strong>Summary</strong><br>3D 高斯球体溅射技术 (3D-GS) 在精确建模镜面和各向异性成分方面面临挑战，Spec-Gaussian 方法通过使用各向异性球面高斯外观场来解决这一难题，同时采用粗略到精细的训练策略来增强学习效率并消除过拟合浮动物。</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GS技术在标准数据集上表现出色，但在精确建模镜面和各向异性成分方面遇到困难。</li><li>限制球谐函数 (SH) 表示高频信息的局限性导致3D-GS建模困难。</li><li>Spec-Gaussian方法采用各向异性球面高斯 (ASG) 外观场来代替SH，提高镜面和各向异性成分建模能力。</li><li>粗略到精细的培训策略提高了学习效率，消除了过拟合造成的浮动物。</li><li>实验结果表明Spec-Gaussian在渲染质量方面优于现有方法。</li><li>ASG显著提升了3D-GS建模镜面和各向异性成分场景的能力，无需增加3D高斯球体数量。</li><li>3D-GS技术可扩展至处理镜面和各向异性表面的复杂场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Spec-Gaussian：高斯体渲染中的各向异性视点相关外观</li><li>作者：Jiahui Lei, Yinda Zhang, Wenbo Bao, Jingyi Yu, Qiong Yan, Hao Li</li><li>单位：香港中文大学（深圳）</li><li>关键词：3D 高斯体渲染、各向异性、视点相关外观、神经网络</li><li>论文链接：https://arxiv.org/abs/2208.05462</li><li>摘要：（1）研究背景：近年来，3D 高斯体渲染（3DGS）在实时渲染和高渲染质量方面取得了显著进展。然而，在建模镜面和各向异性成分时，3DGS 仍然面临挑战。</li></ol><p>（2）过去的方法及其问题：过去的方法通常使用球谐函数（SH）来建模视点相关外观。然而，SH 在表示高频信息方面能力有限，难以准确建模镜面和各向异性效果。</p><p>（3）提出的研究方法：本文提出 Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模 3D 高斯体视点相关外观的方法。ASG 比 SH 具有更强的各向异性建模能力，可以更准确地表示镜面和各向异性成分。此外，本文还提出了一个粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。</p><p>（4）方法在任务和性能上取得的成就：实验结果表明，Spec-Gaussian 在渲染质量方面优于现有方法。得益于 ASG，本文方法显著提高了 3DGS 在建模具有镜面和各向异性成分场景的能力，而无需增加 3D 高斯体的数量。这一改进扩展了 3DGS 在处理具有复杂镜面和各向异性表面的场景中的适用性。</p><p>7.Methods:(1):提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；(2):提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象；(3):通过实验验证Spec-Gaussian在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。</p><ol><li>结论：（1）：本文提出Spec-Gaussian，一种使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观的方法，有效地克服了传统3D-GS在渲染具有镜面高光和各向异性的场景时遇到的挑战。此外，本文创新地实现了粗到细的训练机制，消除了实际场景中的浮动现象。定量和定性实验表明，本文方法不仅赋予3D-GS建模镜面高光和各向异性的能力，而且提高了3D-GS在一般场景中的整体渲染质量，而不会显著影响FPS和存储开销。（2）：创新点：提出Spec-Gaussian方法，使用各向异性球面高斯（ASG）外观场来建模3D高斯体视点相关外观，ASG比球谐函数（SH）具有更强的各向异性建模能力；提出粗到细的训练策略，以提高学习效率并消除过拟合引起的浮动现象。性能：在渲染质量方面优于现有方法，显著提高了3DGS在建模具有镜面和各向异性成分场景的能力。工作量：与现有方法相比，在渲染质量方面有显著提升，而不会显著增加FPS和存储开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4090f3d87f7165ab99a3612c93587c40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-06c68db5202857ec55ce34cb4381f13c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23504bdddd28cc6cb43a6d3e0229eedd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5e74d0aee36acee6c03305fd883438c.jpg" align="middle"></details><h2 id="Magic-Me-Identity-Specific-Video-Customized-Diffusion"><a href="#Magic-Me-Identity-Specific-Video-Customized-Diffusion" class="headerlink" title="Magic-Me: Identity-Specific Video Customized Diffusion"></a>Magic-Me: Identity-Specific Video Customized Diffusion</h2><p><strong>Authors:Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</strong></p><p>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at <a href="https://github.com/Zhen-Dong/Magic-Me">https://github.com/Zhen-Dong/Magic-Me</a>. </p><p><a href="http://arxiv.org/abs/2402.09368v1">PDF</a> </p><p><strong>Summary</strong><br>用少量图像指定主体 ID，VCD 框架通过强化身份信息提取和注入帧间相关性，生成主体身份可控的高质量视频。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 VCD 框架用于主体身份可控视频生成，通过指定几个图像定义主体 ID。</li><li>ID 模块利用提示到分割训练， disentangle ID 信息和背景噪声，更准确地学习 ID 标记。</li><li>T2V VCD 模块使用 3D 高斯噪声先验，以获得更好的帧间一致性。</li><li>V2V Face VCD 和 Tiled VCD 模块用于模糊面部和提升视频分辨率。</li><li>VCD 在选定的强基线上生成稳定、高质量且 ID 更佳的视频。</li><li>ID 模块可迁移，VCD 可与公开提供的微调文本到图像模型配合使用，进一步提高其可用性。</li><li>提供了 VCD 的代码：<a href="https://github.com/Zhen-Dong/Magic-Me。">https://github.com/Zhen-Dong/Magic-Me。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Magic-Me: 身份特定视频定制化扩散</li><li>作者：Ze Ma<em>1, Daquan Zhou</em>†1, Chun-Hsiao Yeh2, Xue-She Wang1, Xiuyu Li2, Huanrui Yang2, Zhen Dong†2, Kurt Keutzer2, Jiashi Feng1</li><li>第一作者单位：字节跳动公司</li><li>关键词：身份特定视频生成、文本到视频、视频定制化扩散</li><li>论文链接：https://arxiv.org/abs/2402.09368   Github代码链接：https://github.com/Zhen-Dong/Magic-Me</li><li>摘要：   (1): 研究背景：文本到视频生成取得了显著进展，但精确控制生成内容仍然具有挑战性。身份特定生成在许多场景中很重要，例如电影制作和广告。   (2): 过去方法：之前的研究主要集中在利用图像参考控制风格和动作，或通过视频编辑进行定制化生成。这些方法的重点不在于身份特定控制。   (3): 研究方法：本文提出了一种简单的但有效的身份特定视频生成框架，称为视频定制化扩散（VCD）。VCD 使用身份模块提取身份信息，并在初始化阶段注入帧间相关性，以生成具有稳定身份的视频输出。   (4): 性能：VCD 在身份保留方面优于选定的强基线。此外，由于身份模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。</li></ol><p>7.Methods：（1）提出用于 VCD 的预处理模块，以及 ID 模块和运动模块，如图 3 所示。此外，我们提供了一个可选模块，利用 ControlNet Tile 来上采样视频并生成高分辨率内容。我们的方法结合了 AnimateDiff [18] 中现成的运动模块，并通过我们提出的 3D 高斯噪声先验进行了增强，如第 4.1 节所述。ID 模块具有带掩码损失和提示到分割的扩展 ID 令牌，在第 4.2 节中介绍。在第 4.3 节中，我们介绍了两个 V2V VCD 管道，FaceVCD 和 TiledVCD。（2）为了简单起见，我们应用我们的无训练 3D 高斯噪声先验到现成的运动模块 [18]，以减轻推理期间的曝光偏差。所选的运动模块将网络扩展到包含时间维度。它将 2D 卷积和注意力层转换为时间伪 3D 层 [23]，遵循方程式 2 中概述的训练目标。3D 高斯噪声先验。对于包含 f 帧的视频，3D 高斯噪声先验从多元高斯分布 N(0, Σf(γ)) 中采样。这里，Σf(γ) 表示由 γ∈(0,1) 参数化的协方差矩阵。Σf(γ)=1γγ2···γf−1γ1γ···γf−2γ2γ1···γf−3...............γf−1γf−2γf−3···1。(4)（3）上面描述的协方差确保初始化的 3D 噪声在 m 和 n 帧之间的相同位置表现出 γ|m−n| 的协方差。超参数 γ 表示稳定性和运动幅度之间的权衡，如图 4 所示。较低的 γ 值会导致运动剧烈但稳定性降低的视频，而较高的 γ 会导致幅度减小的更稳定的运动。（4）ID 模块 VAE 提示到分割 Lmask<v*>man 主体是一个穿着粉色 T 恤的人图 5.扩展 ID 令牌学习。通过提示到分割，针对掩码主体区域对扩展 ID 令牌进行优化。虽然以前的工作已经探索了 T2I 身份定制的令牌嵌入 [16,58] 和权重微调 [11,17,31,48]，但很少有人深入研究 T2V 生成中的身份定制。我们观察到，虽然像 CustomDiffusion [31] 或 LoRA [25] 这样的权重调整方法在图像生成中实现了精确的身份，但生成的视频通常显示出有限的多样性和用户输入对齐。扩展 ID 令牌。我们建议使用扩展 ID 令牌仅与条件编码交互，并更好地保留身份的视觉特征，如图 5 所示。与原始 LoRA 相比，这种方法可以产生更好的视频质量，如表 1 所示。此外，提出的 ID 模块只需要 16KB 的存储空间，与 Stable Diffusion 中所需的参数 3.6G 或 SVDiff [20] 中的 1.7MB 相比，这是一个非常紧凑的参数空间。</v*></p><ol><li>结论：（1）本工作的重要意义：本文提出的 Video Custom Diffusion（VCD）框架旨在解决可控视频生成中主体身份控制的挑战。通过融合身份信息和帧间相关性，VCD 为生成不仅在帧间保持主体身份，而且具有稳定性和清晰度的视频铺平了道路。我们新颖的贡献，包括用于精确身份分离的 ID 模块、用于增强帧一致性的 T2V VCD 模块以及用于提高视频质量的 V2V 模块，共同为视频内容中的身份保留建立了新的标准。我们进行的广泛实验肯定了 VCD 在生成高质量、稳定且保留主体身份的视频方面的优势。此外，我们的 ID 模块适用于现有的文本到图像模型，增强了 VCD 的实用性，使其适用于广泛的应用。（2）本文的创新点、性能和工作量总结：创新点：</li><li>提出了一种用于视频定制扩散的框架，该框架结合了身份信息和帧间相关性，以生成具有稳定身份的视频。</li><li>设计了一个 ID 模块，用于从文本提示中提取身份信息并将其注入视频生成过程中。</li><li>提出了一种 T2V VCD 模块，用于增强帧间一致性，生成具有平滑运动和清晰细节的视频。性能：</li><li>VCD 在身份保留方面优于选定的强基线，生成的高质量视频在帧间保持了主体身份。</li><li>由于 ID 模块的可迁移性，VCD 也适用于公开可用的微调文本到图像模型，进一步提高了其可用性。工作量：</li><li>VCD 的实现相对简单，仅需要少量额外的计算开销。</li><li>ID 模块具有紧凑的参数空间，仅需 16KB 的存储空间，使其易于部署和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e6a21bfcb16c6c0deb1d0539ef94af7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9fb6739198960204ae02b3df3b1108f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3af883ea390b349d783415082941342e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f79fc49019e994a2b5124fecafb23683.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ffb39f913681e339c8d1aa9719f971cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ad7c82a7b238a18cf1ae3935cfce436.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e509076266dabf0c8283fba23dba850.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ef1ee7f0f72cd6bec6307311ed8330ee.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM). Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings. Building on this progress, we propose SGS-SLAM which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation. It outperforms existing methods by a large margin meanwhile preserves real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v2">PDF</a> </p><p><strong>摘要</strong><br>SGS-SLAM 采用多通道优化，将外观、几何和语义约束融入关键帧优化中，实现了高精度 3D 语义分割和高保真重建。</p><p><strong>关键要点</strong></p><ul><li>利用高斯喷射将语义理解融入 SLAM 系统，生成高质量渲染效果。</li><li>采用多通道优化，融合外观、几何和语义约束，提升重建质量。</li><li>在相机位姿估计、地图重建和语义分割方面达到最先进性能。</li><li>显著优于现有方法，同时保持实时渲染能力。</li><li>扩展了 SLAM 系统的应用范围，使其在语义理解和重建任务中表现出色。</li><li>为室内或室外环境的高保真重建和交互式探索提供了新的可能性。</li><li>为自动驾驶、机器人导航和增强现实等领域提供了新的技术支持。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SGS-SLAM：用于神经稠密 SLAM 的语义高斯斑点绘制</li><li>作者：Mingrui Li、Shuhong Liu、Heng Zhou、Guohao Zhu、Na Cheng、Hongyu Wang</li><li>单位：大连理工大学计算机科学与技术系</li><li>关键词：SLAM、3D 重建、3D 语义分割</li><li>论文链接：https://arxiv.org/abs/2402.03246，Github 代码链接：无</li><li>摘要：(1)：研究背景：语义理解在稠密 SLAM 中至关重要，而将高斯斑点绘制集成到 SLAM 系统中的最新进展已证明其在生成高质量渲染方面的有效性。(2)：过去方法及问题：传统视觉 SLAM 系统擅长使用点云和体素进行稀疏重建，但无法进行稠密重建。基于学习的 SLAM 方法可以提取用于高质量表示的稠密几何信息，但它们容易受到噪声和异常值的影响。神经辐射场 (NeRF) 启发的 SLAM 方法进一步提高了重建质量，但它们通常不包含语义信息。(3)：研究方法：本文提出 SGS-SLAM，它在高保真重建的同时提供精确的 3D 语义分割。SGS-SLAM 在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。(4)：任务和性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能。它以很大的优势优于现有方法，同时保留了实时渲染能力。</li></ol><p>方法：(1): SGS-SLAM采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，以增强重建质量。(2): 跟踪过程估计每一帧的相机位姿，同时保持场景参数固定。映射优化基于估计的相机位姿优化场景表示。(3): 场景表示使用高斯影响函数 f(·)，其中 σ 表示不透明度，μ 表示中心位置，r 表示半径。每个高斯还携带 RGB 颜色 ci。(4): 使用渲染方法将高斯渲染成 2D 图像，通过沿深度维度逼近影响函数 f(·) 的积分投影来完成。(5): 通过对高斯进行深度排序并执行从前到后的体积渲染，可以组合所有高斯对该像素的影响。(6): 像素级渲染颜色 Cpix 是每个高斯颜色 ci 的总和，并根据影响函数 f2Di,pix 加权，乘以遮挡项。(7): 深度可以渲染为：Dpix = ∑i=1 di f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 di 表示每个高斯的深度。(8): 通过设置 di=1，可以计算出轮廓 Silpix = Dpix(di=1)，这有助于确定像素是否在当前视图中可见。(9): 在映射过程中，将 2D 语义标签分配给高斯参数的特定通道以表示其语义标签和颜色。(10): 渲染过程中，可以从重建的 3D 场景渲染 2D 语义图：Spix = ∑i=1 si f2Di,pix i−1 ∏j=1 (1−f2Dj,pix)，其中 si = [ri, gi, bi]T 表示与高斯关联的语义颜色。(11): 相机位姿估计通过最小化跟踪损失来实现，该损失表示真实颜色、深度图像和语义图与其可微渲染视图之间的差异。(12): 关键帧选择和加权：在跟踪阶段，同时识别和存储关键帧。这些关键帧提供了对象的不同视图，对于映射优化 3D 场景重建至关重要。(13): SGS-SLAM 在恒定时间间隔内捕获和存储关键帧。随后，根据几何和语义约束选择与当前帧关联的关键帧。(14): 首先进行基于几何的初始选择，然后进行基于语义的二次筛选。(15): 对于每个关键帧，计算不确定性分数 U(t) = e−τt，其中 t 表示关键帧的时间戳，τ 为衰减系数。(16): 使用此不确定性分数对映射损失 Lmapping 加权。(17): 地图重建：场景使用三个不同通道的高斯建模：它们的均值坐标表示场景的几何信息，它们的外观颜色描绘了场景的视觉外观，它们的语义颜色表示对象的语义标签。(18): 在高斯致密化和优化过程中，跨通道的这些参数被联合优化，而从跟踪中确定的相机位姿保持固定。(19): 从第一帧开始，所有像素都有助于初始化地图。(20): 在新时间步的地图重建过程中，将新高斯引入到地图中，这些区域要么密度不足，要么显示先前估计的地图前面的新几何形状。(21): 通过将掩码应用于像素来调节新高斯的添加，其中要么 (i) 轮廓值 Silpix 低于某个阈值，表示可见性高度不确定，要么 (ii) 真实深度远小于估计深度，表明存在新的几何实体。(22): 致密化后，通过最小化映射损失来优化地图参数：Lmapping = U ∑pix λD |DGTpix−Dpix| + λC L C + λS L S。</p><ol><li>结论：（1）本工作的重要意义：SGS-SLAM 在进行高保真重建的同时提供了精确的 3D 语义分割，在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，为神经稠密 SLAM 提供了一种新的解决方案。（2）本文的优缺点总结：创新点：SGS-SLAM 采用多通道高斯表示，将外观、几何和语义约束与关键帧优化相结合，增强了重建质量，并首次将语义信息集成到神经稠密 SLAM 系统中。性能：SGS-SLAM 在相机姿态估计、地图重建和语义分割方面都取得了最先进的性能，以很大的优势优于现有方法，同时保留了实时渲染能力。工作量：SGS-SLAM 的实现需要大量的计算资源和数据，这可能会限制其在某些资源受限的应用中的使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-49d695fd07273ec0ead5f03d33095327.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9e64fa80d8afdcf89c98cfd50dd717f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-29  VastGaussian Vast 3D Gaussians for Large Scene Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/</id>
    <published>2024-02-29T12:47:51.000Z</published>
    <updated>2024-02-29T13:22:02.679Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions"><a href="#EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions" class="headerlink" title="EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions"></a>EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions</h2><p><strong>Authors:Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</strong></p><p>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism. </p><p><a href="http://arxiv.org/abs/2402.17485v1">PDF</a> </p><p><strong>Summary</strong><br>音频线索能够协助生成更具表现力和真实感的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>传统技术无法充分捕捉人类面部表情和个人风格差异。</li><li>EMO 框架采用直接音频到视频合成方法，无需中间 3D 模型或面部关键点。</li><li>EMO 可生成流畅无缝的视频，并始终保持身份一致性。</li><li>EMO 可生成具有高度表现力和真实感的说话和唱歌视频。</li><li>EMO 在表现力和真实感方面明显优于现有方法。</li><li>EMO 充分利用了音频线索，提升了面部动画的动态性和细致度。</li><li>EMO 可广泛应用于各种领域，包括电影、游戏和视频会议。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：EMO：EmotePortraitAlive——在弱条件下使用音频到视频扩散模型生成富有表现力的肖像视频</li><li>作者：Tian Linrui、Wang Qi、Zhang Bang、Bo Liefeng</li><li>隶属单位：阿里巴巴集团智能计算研究院</li><li>关键词：Audio-driven portrait video generation、Talking head、Expressive facial expressions、Audio-to-video synthesis</li><li>论文链接：https://humanaigc.github.io/emote-portrait-alive/   Github 代码链接：无</li><li>摘要：   （1）研究背景：   在说话人头像视频生成中，增强真实感和表现力是一项挑战，需要关注音频线索和面部动作之间的动态和细微关系。传统技术往往无法捕捉到人类表情的全貌和个人面部风格的独特性。   （2）过去方法和问题：   传统的说话人头像视频生成方法通常需要中间 3D 模型或面部关键点，这会引入额外的复杂性和限制。此外，这些方法在捕捉细微的表情和保持帧之间的一致性方面存在困难。   （3）研究方法：   本文提出了一种名为 EMO 的新框架，它采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。   （4）任务和性能：   EMO 在说话人头像视频生成任务上进行了评估。实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。</li></ol><p>7.方法：（1）提出了一种名为EMO的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间3D模型或面部关键点的需求。（2）该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。（3）在说话人头像视频生成任务上对EMO进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。</p><ol><li>结论：(1): 本工作提出了一种名为 EMO 的新框架，该框架采用直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。该方法利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。在说话人头像视频生成任务上对 EMO 进行了评估，实验结果表明，该方法在生成具有丰富面部表情和头部姿势的逼真且富有表现力的视频方面取得了显着性能。这些性能支持了本文增强说话人头像视频生成真实感和表现力的目标。(2): 创新点：</li><li>直接音频到视频合成的方法，绕过了对中间 3D 模型或面部关键点的需求。</li><li>利用音频扩散模型，将音频线索直接映射到视频帧，确保了无缝的帧过渡和一致的面部动作。性能：</li><li>在说话人头像视频生成任务上取得了显着性能。</li><li>生成了具有丰富面部表情和头部姿势的逼真且富有表现力的视频。工作量：</li><li>该方法的实现相对简单，不需要复杂的中间步骤或额外的模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-10c8e47dfe09b5369134bad3bf5b1e69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-262ccbd331f2623737aa6cbcc24c64e5.jpg" align="middle"></details><h2 id="G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment"><a href="#G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment" class="headerlink" title="G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment"></a>G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</h2><p><strong>Authors:Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</strong></p><p>Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4G’s success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.18122v1">PDF</a> </p><p><strong>Summary</strong><br>深度学习领域亟需一个通用的高保真说话人脸生成框架，该框架具有良好的音频-图像跨模态对齐精度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 G4G，一种用于高保真说话人脸生成的通用框架，可实现精细的模态内对齐。</li><li>G4G 使用对角矩阵增强音频图像模态内特征的常规对齐，显着增加了正负样本之间的比较学习。</li><li>引入多尺度监督模块，以全面重现面部区域中原始视频的感知保真度，同时强调唇部动作与输入音频的同步。</li><li>使用融合网络进一步融合面部区域和其他区域。</li><li>实验结果表明，在重现原始视频质量和高度同步的说话嘴唇方面取得了显着成就。</li><li>G4G 是一种性能优异的通用框架，可以生成比当前最先进的方法更接近真实水平的说话视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：G4G：一个用于高保真说话人脸生成和精细化模态内对齐的通用框架</li><li>作者：Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</li><li>第一作者单位：长沙万兴科技股份有限公司</li><li>关键词：说话人脸生成、模态内对齐、多尺度监督、融合网络</li><li>论文链接：https://arxiv.org/abs/2402.18122</li><li>摘要：(1) 研究背景：说话人脸生成旨在合成一个目标人物的高保真视频，其唇部动作与任意音频同步。尽管有许多研究，但要实现高保真说话人脸生成并使其唇部动作与任意音频高度同步仍然是一个重大挑战。(2) 过去方法：以往方法存在的问题主要在于：1）无法重现原始视频的高保真度；2）生成的唇部动作与音频不同步；3）生成的人脸视频保真度低。(3) 研究方法：本文提出了 G4G，这是一个用于高保真说话人脸生成和精细化模态内对齐的通用框架。G4G 采用对角矩阵来增强音频-图像模态内特征的普通对齐，显著增加了正负样本之间的比较学习。此外，还引入了一个多尺度监督模块，以全面重现原始视频在面部区域的感知保真度，同时强调唇部动作与输入音频的同步性。然后使用融合网络进一步融合面部区域和其他部分。(4) 性能：G4G 在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。实验结果表明，G4G 生成的说话人视频比当前最先进的方法更接近真实水平。</li></ol><p>7.Methods：(1)：提出G4G框架，采用对角矩阵增强音频-图像模态内特征对齐，增加正负样本比较学习；(2)：引入多尺度监督模块，重现原始视频面部区域感知保真度，强调唇部动作与音频同步；(3)：使用融合网络融合面部区域和其他部分。</p><p><strong>8. 结论</strong>(1): 本工作提出了 G4G 框架，用于生成高保真且高度同步的说话人脸视频。该框架由两个关键组件组成：对角精细化对齐网络和多尺度监督自适应空间变换网络。这些组件协同工作，生成具有卓越保真度和多尺度细节的说话人脸视频。对角精细化对齐网络专门设计用于解决模态内和模态间对齐的挑战。通过保留源图像的面部身份、属性和丰富的纹理细节，我们的网络确保生成的视频与源角色高度相似。此对齐过程对于保持生成视频的真实性和视觉质量至关重要。多尺度监督自适应空间变换网络进一步增强了生成视频的保真度。通过对嘴形和头部姿势进行空间变形，我们的网络实现了嘴唇运动的非凡准确性和真实性。生成嘴唇运动与给定音频之间的这种同步水平明显超过了现有的人脸通用方法。大量实验表明，我们的 G4G 框架在保留角色身份、皮肤纹理和与真实情况高度相似的细节方面是有效的。此外，我们的方法在生成与任意给定音频相对应的、高度同步的嘴唇运动方面表现出色。这些结果优于现有人脸通用方法，突出了我们方法的优越性。虽然我们的 G4G 框架代表了说话人脸生成领域的重大进步，但我们认识到仍有挑战需要解决。例如，生成具有大头部姿势角度的视频以及处理快速变化的背景和光照条件仍然是持续的研究领域。我们正在积极应对这些挑战，并计划在不久的将来发布进一步的研究结果。总之，我们提出的 G4G 框架为生成高保真且高度同步的说话人脸视频提供了一种强大且有效的解决方案。通过保留角色身份、皮肤纹理和细节，我们的方法为包括娱乐、教育和医疗保健在内的各个领域的应用开辟了新的可能性。(2): <strong>创新点：</strong>* 提出对角精细化对齐网络，增强音频-图像模态内特征对齐，增加正负样本比较学习。* 引入多尺度监督自适应空间变换网络，重现原始视频面部区域感知保真度，强调唇部动作与音频同步。* 使用融合网络融合面部区域和其他部分。<strong>性能：</strong>* 在重现原始视频质量和高度同步的说话人嘴唇方面取得了显著成就。* 生成的说话人视频比当前最先进的方法更接近真实水平。<strong>工作量：</strong>* 模型复杂度和训练时间中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e12c89676d8b67fdf727809d6024eb2f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle"></details>## Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang**Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. [PDF](http://arxiv.org/abs/2402.17364v1) CVPR 2024**Summary**神经网络编码的动态四面体（DynTet）是一种结合表示方法，确保了复杂面部变形在各种动作和视点下的几何一致性。**Key Takeaways**- DynTet 采用动态四面体（DynTet），将显式动态网格编码到神经网络中，以确保几何一致性。- 坐标网络用于学习符号距离、形变和材质纹理，将训练数据锚定到预定义的四面体网格中。- 运用 Marching Tetrahedra，DynTet 有效地解码具有连续拓扑的纹理网格，通过可微渲染器实现快速渲染并利用像素损失进行监督。- DynTet 结合经典 3D 可变形模型，以促进几何学习并定义一种规范空间以简化纹理学习。- DynTet 相比于先前的研究，在保真度、唇形同步和实时性能方面都有显著提升。- 除了制作稳定且视觉上吸引人的合成视频，该方法还输出动态网格，有望实现许多新兴应用。- DynTet 弥补了隐式方法缺乏显式几何约束的问题，通过学习动态网格来提高面部变形建模的准确性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：用于高质量说话人头部合成的动态四面体学习</li><li>作者：Zhang Zhicheng, Xu Chenyang, Zhang Haoran, Wu Yuxuan, Wang Yebin, Chen Min, Chen Biao</li><li>单位：北京大学</li><li>关键词：说话人头部合成、动态网格、隐式表示、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2302.05915</li><li><p>摘要：（1）研究背景：隐式表示方法，如神经辐射场（NeRF），在从视频序列中生成逼真且可动画的头部头像方面取得了进展。然而，由于缺乏显式几何约束，这些隐式方法仍面临视觉伪影和抖动的挑战，这给准确建模复杂的面部变形带来了根本性挑战。（2）过去方法及问题：以往方法主要采用隐式表示，由于缺乏显式几何约束，存在视觉伪影和抖动问题。（3）研究方法：本文提出了动态四面体（DynTet），这是一种新的混合表示，它通过神经网络对显式动态网格进行编码，以确保在各种运动和视点下的几何一致性。DynTet 由基于坐标的网络参数化，这些网络学习有符号距离、变形和材质纹理，将训练数据锚定到预定义的四面体网格中。利用行进四面体，DynTet 有效地解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，从而实现快速渲染。为了提高训练效率，我们结合了经典的 3D 可变形模型，以促进几何学习并定义规范空间以简化纹理学习。由于 DynTet 中采用有效的几何表示，这些优势很容易实现。（4）方法性能：与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。</p></li><li><p>方法：(1): 提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。(2): 改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。(3): 采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。(4): 结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。</p></li><li><p>结论：（1）本工作提出了一种名为动态四面体（DynTet）的新型混合表示，用于从短视频序列中学习逼真且可动画的说话人头部，并实现了高质量说话人头部实时渲染。（2）创新点：提出动态四面体（DynTet）框架，快速从短视频序列学习 3D 头部头像，并实现高质量说话人头部实时渲染。改进四面体表示，使用神经网络对显式动态网格进行编码，确保不同运动和视点下的几何一致性。采用行进四面体解码具有统一拓扑结构的纹理网格，通过可微渲染器和像素损失进行监督，实现快速渲染。结合经典 3D 可变形模型，促进几何学习，定义规范空间简化纹理学习。性能：与之前的工作相比，DynTet 在保真度、唇形同步和实时性能方面根据各种指标展示了显着的改进。除了制作稳定且视觉上吸引人的合成视频外，我们的方法还输出动态网格，这有望支持许多新兴应用。工作量：本文的工作量较大，涉及到神经网络、动态网格、隐式表示、神经辐射场等多个方面。作者提出了一个新的混合表示——动态四面体（DynTet），并将其应用于说话人头部合成任务中。DynTet 结合了显式动态网格和隐式表示的优点，能够生成逼真且可动画的头部头像。作者还提出了一个新的训练框架，结合了经典的 3D 可变形模型和可微渲染器。该框架能够有效地学习几何和纹理信息，并生成高质量的合成视频。总体而言，本文的工作量较大，但提出的方法新颖有效，在说话人头部合成领域具有重要的意义。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://picx.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-29  G4GA Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/</id>
    <published>2024-02-29T12:37:28.000Z</published>
    <updated>2024-02-29T12:37:28.331Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-29-更新"><a href="#2024-02-29-更新" class="headerlink" title="2024-02-29 更新"></a>2024-02-29 更新</h1><h2 id="Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model"><a href="#Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model" class="headerlink" title="Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model"></a>Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</h2><p><strong>Authors:Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun</strong></p><p>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients’ quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations. </p><p><a href="http://arxiv.org/abs/2402.18362v1">PDF</a> </p><p><strong>Summary</strong><br>利用无人监督方法，自动评估乳腺癌术后外观，为提高患者生活质量提供新途径。</p><p><strong>Key Takeaways</strong></p><ul><li>采用无监督异常检测视角，无需标记即可评估外观。</li><li>使用蒸馏无标签 (DINO) 自监督视觉 Transformer (ViT) 的注意力机制，实现高质量图像重建和判别区域的精确转换。</li><li>在以正常外观为主的未标记数据上训练扩散模型。</li><li>提供视觉上吸引人的表示和可量化的分数，用于外观评估。</li><li>消除人工标注的需要，提供客观评估。</li><li>在准确性方面超过现有模型，展现出最先进的性能。</li><li>为医学领域的无监督异常检测提供了重大进展。</li><li>探索无监督外观评估在其他医疗领域的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: 基于注意力引导去噪扩散的客观可解释乳房美观评估</li><li>Authors: Sangjoon Park, YongBae Kim, JeeSuk Chang, SeoHee Choi, Hyungjin Chung, IkJae Lee, HwaKyung Byun</li><li>Affiliation: 韩国首尔延世大学医学院放射肿瘤科</li><li>Keywords: 扩散模型、异常检测、视觉 Transformer、乳房美观</li><li>Urls: Paper, Github: None</li><li><p>Summary:(1): 乳房癌术后美观评估对患者生活质量影响很大，但传统方法存在主观性强、依赖人工标注等问题。(2): 现有方法依赖专家标注，存在成本高、标注偏差、模型过拟合、可解释性差等问题。(3): 本文提出一种名为 AG-DDAD 的创新架构，利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力。该模型可以在无监督的方式下训练，利用来自 1,237 名主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒。AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。(4): 在一个经过精心整理的包含 300 名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。</p></li><li><p>方法：（1）：提出一种名为 AG-DDAD 的创新架构，该架构利用扩散模型的高质量生成能力和 DINO 视觉 Transformer 注意力的显著特征识别能力；（2）：AG-DDAD 在无监督的方式下训练，利用来自主要为正常美观（优秀到良好）患者的未标记数据，无需专家标注和人工勾勒；（3）：AG-DDAD 通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。</p></li><li><p>结论：（1）：本文提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力，在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。（2）：创新点：</p></li><li>提出了一种基于注意力引导去噪扩散的客观可解释乳房美观评估方法，该方法利用了扩散模型的高质量生成能力和DINO视觉Transformer注意力的显著特征识别能力。</li><li>该方法在无监督的方式下训练，无需专家标注和人工勾勒，通过比较正常美观情况下预期的恢复结果，提供直接且出色的可视化结果，为不良美观成因提供有价值的见解。性能：</li><li>在一个经过精心整理的包含300名接受乳腺癌保乳手术患者的数据集上进行的实验表明，本文模型优于传统的基于规则的方法和其他最先进的异常检测方法。工作量：</li><li>该方法需要比传统的分类器模型略多的时间，评估单个患者的美观大约需要15秒，而简单的分类器模型可以在1秒内产生结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-678c2254dd6a3d39889bef35f9067c05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cfa8a6039aebee57a2721ad761165bd3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6811aab9ac5a0e1edc535c928e3bd0f.jpg" align="middle"></details><h2 id="FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes"><a href="#FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes" class="headerlink" title="FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes"></a>FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes</h2><p><strong>Authors:Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</strong></p><p>The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers’ parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: <a href="https://finediffusion.github.io/">https://finediffusion.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.18331v1">PDF</a> </p><p><strong>Summary</strong><br>通过微调预训练扩散模型，以参数高效策略实现针对 10,000 个细粒度类别的大规模图像生成</p><p><strong>Key Takeaways</strong></p><ul><li>提出 FineDiffusion，将大规模扩散模型缩小到细粒度图像生成中</li><li>只微调分类嵌入、偏置项和归一化层的参数，大幅提升训练速度和存储效率</li><li>提出针对细粒度类别的超类条件引导采样方法，提升图像生成质量</li><li>与完全微调相比，FineDiffusion 训练速度提升 1.56 倍，所需存储参数仅为原模型的 1.77%</li><li>在 10,000 个类别的图像生成上取得最先进的 FID 为 9.776</li><li>大量定性和定量实验验证了该方法的优越性</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FineDiffusion：将扩散模型扩展到 10,000 类别的细粒度图像生成</li><li>作者：Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</li><li>第一作者单位：厦门大学</li><li>关键词：Diffusion Models, Fine-grained Image Generation, Parameter-efficient Fine-tuning</li><li>论文链接：https://arxiv.org/abs/2402.18331   Github 代码链接：None</li><li>摘要：   （1）：研究背景：基于扩散模型的图像生成以产生高质量和多样化的图像而闻名。然而，大多数先前的努力都集中在为一般类别生成图像，例如 ImageNet-1k 中的 1000 个类别。一个更具挑战性的任务，即大规模细粒度图像生成，仍然是需要探索的边界。   （2）：过去的方法：过去的方法主要集中在一般类别的图像生成，而对于细粒度图像生成，需要模型对高度相似的细粒度类别中的细微差异（例如鸟类的羽毛纹理）进行复杂的建模。从头开始训练用于大规模细粒度图像生成的扩散模型需要更大的计算资源和训练迭代。   （3）：研究方法：本文提出了一种新的微调方法 FineDiffusion，它可以通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成。   （4）：方法性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。</li></ol><p>7.方法：(1)提出了一种新的微调方法FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；(2)提出了一种分层类标签编码策略，该策略同时对超类和子类标签进行编码；(3)同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；(4)引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。</p><ol><li>结论：（1）：本文首次尝试将扩散模型扩展到 10,000 类的细粒度图像生成。我们引入了 FineDiffusion，这是一种高效的参数微调方法，可以微调预训练模型的关键组件，包括分层标签嵌入、偏差项和归一化项。我们的方法大幅减少了训练和存储开销。此外，我们引入了一种细粒度无分类器引导采样技术，利用分层数据标签信息来有效增强细粒度图像生成的性能。充分的定性和定量结果证明了我们方法与其他方法相比的优越性。（2）：创新点：提出了一种新的微调方法 FineDiffusion，该方法通过微调预训练模型的一小部分参数，有效地微调大型预训练图像生成扩散模型，以进行大规模细粒度图像生成；提出了分层类标签编码策略，该策略同时对超类和子类标签进行编码；同时微调偏差和归一化项以及分层嵌入器，以学习全局数据集的分布特征；引入了一种分层无分类器引导采样方法，该方法利用超类条件信息来增强对生成图像的控制。性能：与完全微调相比，FineDiffusion 实现了显着的 1.56 倍训练加速，并且只需要存储 1.77% 的总模型参数，同时在 10,000 个类别的图像生成上实现了 9.776 的最先进 FID。广泛的定性和定量实验表明，与其他参数有效的微调方法相比，本文方法具有优越性。工作量：与从头开始训练用于大规模细粒度图像生成的扩散模型相比，FineDiffusion 可以显着减少计算资源和训练迭代。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f68a4db99ea4f9179538c6c4b4d7c7ce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e768fecf2a73ce9e4c8b13ef7c8cd6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c0d4b61db744892b76754513d9f6676.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-665dc312a2eacee1bb375efacd7d609c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d25afe2f19082c3abc80d90affd76466.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68e2a9d895710b3df489a49501a85625.jpg" align="middle"></details><h2 id="Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models"><a href="#Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models" class="headerlink" title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models"></a>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</h2><p><strong>Authors:Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</strong></p><p>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data. </p><p><a href="http://arxiv.org/abs/2402.18206v1">PDF</a> CVPR 2024. Project Page : <a href="https://ab-34.github.io/balancing_act/">https://ab-34.github.io/balancing_act/</a></p><p><strong>Summary</strong><br>去除扩散模型中的偏见，无需额外数据或模型重新训练。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型（DM）存在偏见，表现为对特定人口亚组（如女性）的偏好。</li><li>分布引导是一种无偏 DM 的方法，无需额外数据或重新训练。</li><li>分布引导利用去噪 UNet 的潜在特征中丰富的语义信息。</li><li>属性分布预测器 (ADP) 将潜在特征映射到属性分布。</li><li>ADP 使用现有属性分类器生成的伪标签进行训练。</li><li>分布引导和 ADP 实现了公平生成，显著优于基线。</li><li>通过使用生成的数据重新平衡训练集，可以训练公平的属性分类器。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：平衡行为：扩散模型中的分布引导去偏</li><li>作者：Rishubh Parihar*, Abhijnya Bhat∗, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</li><li>隶属：印度科学院，班加罗尔</li><li>关键词：Diffusion Models, Debiasing, Distribution Guidance, Attribute Distribution Predictor, Fair Generation</li><li>链接：https://arxiv.org/abs/2402.18206</li><li><p>摘要：（1）研究背景：扩散模型（DM）作为强大的生成模型，在图像生成方面表现出色，但它们会反映训练数据集中的偏见，特别是对于人脸，DM 偏好某些人口统计学亚组（例如女性比男性）。（2）过去方法：现有去偏方法需要额外数据或模型重新训练。（3）研究方法：本文提出分布引导，通过强制生成图像遵循规定的属性分布来对 DM 进行去偏。通过训练属性分布预测器 (ADP) 来映射潜在特征到属性分布，ADP 使用现有属性分类器生成的伪标签进行训练。（4）方法性能：该方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。此外，本文还提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p></li><li><p>方法：(1): 提出分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型（DM）进行去偏。(2): 训练属性分布预测器（ADP）来映射潜在特征到属性分布，ADP使用现有属性分类器生成的伪标签进行训练。(3): 在无条件和文本条件扩散模型上评估该方法，减少了单一/多属性的偏差，并优于基线方法。(4): 提出了一种通过使用生成数据重新平衡训练集来训练公平属性分类器的下游任务。</p></li><li><p>结论：（1）本文的意义：本文提出了一种无需重新训练即可减轻预训练扩散模型偏差的方法，仅给定所需的参考属性分布。我们提出了一种新颖的方法，利用分布引导，联合引导一批图像遵循参考属性分布。所提出的方法是有效的，并且在（2）创新点：本文的创新点在于提出了一种新的分布引导方法，通过强制生成图像遵循规定的属性分布来对扩散模型进行去偏。性能：本文的方法在无条件和文本条件扩散模型上减少了单一/多属性的偏差，并且优于基线方法。工作量：本文的方法需要训练一个属性分布预测器，该预测器使用现有属性分类器生成的伪标签进行训练。训练属性分布预测器的工作量取决于训练数据的规模和属性的数量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-05a1a956ee3a51fe0c06ffc4859c7231.jpg" align="middle"><img src="https://picx.zhimg.com/v2-16ae5c5f9f522148622d40f8f3f15f86.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46f6a987113095ab338596820ca6e653.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f32e1f0036b8646f3ffad99a82575f09.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1128b65d6c33c58a2f6b04087adf31b0.jpg" align="middle"></details><h2 id="Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis"><a href="#Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis" class="headerlink" title="Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis"></a>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</h2><p><strong>Authors:Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai</strong></p><p>Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at <a href="https://github.com/YanzuoLu/CFLD">https://github.com/YanzuoLu/CFLD</a>. </p><p><a href="http://arxiv.org/abs/2402.18078v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br> 提出了一种粗到细的潜在扩散（CFLD）方法，利用图像而非文本提示，控制预训练文本到图像扩散模型，实现姿势引导的图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 CFLD 方法，改善了 PGPIS 中姿势引导图像合成的效果。</li><li>使用纯图像训练范式，无需图像字幕或文本提示。</li><li>设计了一个感知精炼解码器，逐步优化查询并提取人物图像的语义理解。</li><li>将外貌和姿势信息控制解耦，避免过度拟合。</li><li>提出混合粒度注意力模块，对多尺度外观特征进行编码，增强粗粒度提示。</li><li>在 DeepFashion 数据集上，定量和定性实验结果证明了 CFLD 的优越性。</li><li>代码已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：粗到细的潜在扩散用于姿态引导的人物图像合成</li><li>作者：Lu Yanzuo, Zhang Manlin, Ma Andy J, Xie Xiaohua, Lai Jianhuang</li><li>单位：中山大学计算机科学与工程学院</li><li>关键词：姿态引导、人物图像合成、潜在扩散模型、粗到细、语义理解</li><li>论文链接：https://arxiv.org/abs/2402.18078   Github 代码链接：https://github.com/YanzuoLu/CFLD</li><li>摘要：(1) 研究背景：姿态引导的人物图像合成旨在将源人物图像转换为特定的目标姿态，同时尽可能保留外观。它在电影制作、虚拟现实和时尚电子商务等领域有广泛的应用。(2) 过去的方法：现有基于生成对抗网络 (GAN) 的方法容易出现极小极大训练目标的不稳定性和难以在一次前向传递中生成高质量图像的问题。作为 GAN 在图像生成中的一种有前途的替代方案，扩散模型通过一系列去噪步骤逐渐合成更逼真的图像。(3) 本文方法：本文提出了一种新颖的粗到细潜在扩散 (CFLD) 方法用于姿态引导的人物图像合成。在没有图像-标题对和文本提示的情况下，我们开发了一种纯粹基于图像的新颖训练范式来控制预训练文本到图像扩散模型的生成过程。我们设计了一个感知精炼解码器来渐进地细化一组可学习查询并提取人物图像的语义理解作为粗粒度提示。这允许在不同的阶段解耦细粒度外观和姿态信息控制，从而规避了潜在的过拟合问题。为了生成更逼真的纹理细节，我们提出了一种混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项以增强粗粒度提示。(4) 性能：在 DeepFashion 基准上的定量和定性实验结果证明了我们方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。</li></ol><p>7.Methods：(1) 提出粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成；(2) 开发基于图像的新训练范式，控制预训练文本到图像扩散模型的生成过程；(3) 设计感知精炼解码器，渐进细化可学习查询，提取人物图像语义理解作为粗粒度提示；(4) 提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示；(5) 通过在DeepFashion基准上的定量和定性实验，验证了CFLD方法的优越性。</p><ol><li><p>结论：（1）xxx；（2）创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>总结：（1）本工作的重要意义是什么？（2）从创新点、性能、工作量三个维度总结本文的优缺点：创新点：本文提出了一种新颖的粗到细潜在扩散（CFLD）方法，用于姿态引导的人物图像合成。该方法通过渐进细化可学习查询，提取人物图像的语义理解作为粗粒度提示，并提出混合粒度注意力模块，将多尺度细粒度外观特征编码为偏差项，增强粗粒度提示。性能：在 DeepFashion 基准上的定量和定性实验结果证明了 CFLD 方法在姿态引导的人物图像合成任务上的优越性。这些性能支持了他们的目标，即生成具有更好泛化性能的高质量图像。工作量：本文的工作量适中。该方法的实现需要对文本到图像扩散模型进行预训练，这可能需要大量的计算资源。此外，该方法需要设计感知精炼解码器和混合粒度注意力模块，这需要额外的开发和实验工作。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ee807dc5573280abe63e138fa82f6eb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-07506917791ee3066c02770faa1a2052.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5192aaa635e4ab29d557ee967971be49.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-269e1bea1b870d8f0466ace81c9d2e01.jpg" align="middle"></details><h2 id="SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model"><a href="#SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model" class="headerlink" title="SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model"></a>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model</h2><p><strong>Authors:Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</strong></p><p>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved. </p><p><a href="http://arxiv.org/abs/2402.18068v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练的视觉语言模型对图像合成中的伪影进行自动分类，为生成模型的进一步优化提供监管，从而提高合成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>合成图像中复杂伪影的存在构成了一项重大挑战，对感知真实性产生了负面影响。</li><li>研究人员提出将视觉语言模型（VLM）微调为伪影分类器，以便自动识别和分类各种伪影。</li><li>开发了一个全面的伪影分类体系，并构建了一个具有伪影注释的合成图像数据集（SynArtifact-1K）。</li><li>微调后的 VLM 在识别伪影方面表现出优异的能力，比基线高出 25.66%。</li><li>这是首次提出此类端到端伪影分类任务和解决方案。</li><li>利用 VLM 的输出作为反馈来优化生成模型，以减轻伪影。</li><li>视觉化结果和用户研究表明，优化后的扩散模型合成的图像质量得到了明显改善。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：SynArtifact：通过视觉语言模型对合成图像中的伪影进行分类和消除</li><li>作者：Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：合成图像、伪影、视觉语言模型、生成模型</li><li>论文链接：https://arxiv.org/abs/2402.18068</li><li><p>摘要：(1) 研究背景：合成图像中存在复杂伪影，影响图像的感知真实性。(2) 过往方法：现有方法主要依赖单一评分指标优化生成模型，无法有效反映伪影的多样性和复杂性。(3) 本文方法：提出一个综合伪影分类法，构建了一个带有伪影注释的合成图像数据集 SynArtifact-1K，并微调视觉语言模型 (VLM) 对伪影进行分类。利用 VLM 的输出作为 AI 反馈来改进生成模型，以减轻伪影。(4) 实验结果：微调后的 VLM 在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。通过利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。</p></li><li><p>方法：（1）构建综合伪影分类法，建立包含伪影注释的合成图像数据集 SynArtifact-1K；（2）微调视觉语言模型 VLM，将其作为伪影分类器；（3）利用 VLM 输出作为 AI 反馈，计算生成模型输出与每种伪影之间的 BertScore，作为伪影分类奖励；（4）通过最大化伪影分类奖励，优化扩散模型，以减轻伪影。</p></li><li><p>结论：（1）：本文针对合成图像中的伪影问题，提出了一个综合的伪影分类法，构建了包含伪影注释的合成图像数据集 SynArtifact-1K，并利用视觉语言模型对伪影进行分类，有效地减轻了生成模型中的伪影，提升了合成图像的感知真实性。（2）：创新点：</p></li><li>构建了包含 13 种常见伪影的综合伪影分类法。</li><li>创建了首个带有伪影类别、描述和坐标注释的图像数据集 SynArtifact-1K。</li><li>微调视觉语言模型用于自动分类伪影，并利用其输出作为 AI 反馈来优化生成模型。性能：</li><li>微调后的视觉语言模型在伪影分类任务上比基线方法提高了 25.66% 的准确率和 29.01% 的 F1 分数。</li><li>利用伪影分类器的输出作为 AI 反馈，可以有效减轻生成模型中的伪影。工作量：</li><li>构建了包含 1000 张合成图像的 SynArtifact-1K 数据集。</li><li>微调了视觉语言模型用于伪影分类。</li><li>通过最大化伪影分类奖励，优化了扩散模型以减轻伪影。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-887bb2eb3bab7f102340a00fb115308a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67234ceff494848cb67aa7bc7345a5e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c0c890345f83368ccd384b81c55c4b11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48d8c1e1b56b76bfccfccfcb96c1d5a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a5599c3d37db39e68fa5fb2e0139cec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-94675e3c8e66717ee97bc9e3472ed274.jpg" align="middle"></details><h2 id="Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models"><a href="#Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models" class="headerlink" title="Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models"></a>Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models</h2><p><strong>Authors:Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</strong></p><p>While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module - a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and ii) attribute binding, guaranteeing that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models, markedly enhancing model performance in addressing the key challenges. We evaluate our technique using the established CompBench and TIFA score benchmarks, demonstrating significant performance improvements compared to existing methods. The source code will be made publicly available at <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a>. </p><p><a href="http://arxiv.org/abs/2402.17910v1">PDF</a> </p><p><strong>Summary</strong><br>Box-it-to-Bind-it（B2B）是一种无需训练的新模块，可提高文本到图像（T2I）扩散模型中图像的生成质量、语义准确度和空间控制能力。</p><p><strong>Key Takeaways</strong></p><ul><li>B2B 模块可改善 T2I 中的三个关键挑战：灾难性遗漏、属性绑定和布局指导。</li><li>B2B 包括生成对象和属性绑定的两个主要步骤。</li><li>B2B 可作为现有的 T2I 模型的即插即用模块，无需训练。</li><li>B2B 在 CompBench 和 TIFA 评分基准上表现出显著的性能提升。</li><li>B2B 的源代码将在 <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a> 上公开。</li><li>B2B 提高了 LDM 在生成图像时的空间控制和语义准确性。</li><li>B2B 适用于不同的 T2I 模型，易于集成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Box-it-to-Bind-it：统一布局控制和属性绑定到 T2I 扩散模型中</li><li>作者：Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</li><li>第一作者单位：西澳大利亚大学</li><li>关键词：文本到图像、扩散模型、空间控制、属性绑定、布局引导</li><li>论文链接：https://arxiv.org/abs/2402.17910</li><li>摘要：（1）研究背景：现有扩散模型在生成图像时缺乏语义保真度和空间控制，难以忠实地遵循给定的提示，尤其是在对象属性和对象放置方面。</li></ol><p>（2）过去的方法和问题：现有方法要么从头开始训练模型，要么微调现有模型，但需要大量计算资源和时间。此外，利用预训练模型并集成特征的方法虽然不需要大量训练，但效果有限。</p><p>（3）提出的研究方法：本文提出了一种免训练的方法 Box-it-to-Bind-it (B2B)，解决文本到图像生成中的三个关键挑战：灾难性遗漏、属性绑定和布局引导。B2B 在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。</p><p>（4）方法在任务和性能上的表现：在 CompBench 和 TIFA 得分基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。这些性能提升支持了本文的目标，即提高文本到图像生成中的空间控制和语义准确性。</p><p>方法：(1) B2B是一种奖励引导扩散模型，它在推理阶段通过两步引导扩散模型的潜在编码：对象生成和属性绑定。(2) 对象生成：基于IoU，增加对象生成概率，将注意力权重集中在给定边界框内，同时抑制边界框外的注意力权重。(3) 属性绑定：使用KL散度测量属性概率分布与对应对象概率分布的差异，减少差异，将属性分布强制收敛到各自的对象。</p><ol><li>结论：（1）：本研究针对文本到图像生成中的关键挑战，特别是属性绑定和空间控制，提出了 B2B 模型。B2B 采用生成和绑定双模块系统，有效解决了灾难性遗漏、提高属性绑定精度和确保准确对象放置的问题。它作为现有 T2I 框架的即插即用模块的兼容性通过其在 CompBench 和 TIFA 基准中的出色表现得到证明，标志着生成建模的重大飞跃。B2B 的突破凸显了其作为未来研究潜在标准的作用，为数字成像和生成式 AI 的创新发展铺平了道路。（2）：创新点：</li><li>提出了一种免训练的方法 B2B，通过两步引导扩散模型的潜在编码来解决文本到图像生成中的关键挑战。</li><li>设计了对象生成和属性绑定两个模块，有效解决了灾难性遗漏、属性绑定和布局引导问题。</li><li>B2B 作为现有 T2I 框架的即插即用模块，易于集成和使用。性能：</li><li>在 CompBench 和 TIFA 基准上，与现有方法相比，B2B 在解决关键挑战方面显着提高了模型性能。</li><li>消融研究验证了对象生成和属性绑定奖励组件的有效性，表明 B2B 的各个组件对整体性能至关重要。工作量：</li><li>B2B 是一种免训练的方法，不需要从头开始训练模型或微调现有模型，从而节省了大量的计算资源和时间。</li><li>B2B 易于集成到现有 T2I 框架中，无需进行复杂的修改或重新训练，降低了工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9044558cdc31309b419fea5199aa8a89.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bccd36910d4aa870962c445823ad57.jpg" align="middle"><img src="https://pica.zhimg.com/v2-967a215bde68183f03e457a7ff3f8e9a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e2a4cdc833464a14406a357aa9e0c358.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d140c3c8e05d724098a1c03138203a01.jpg" align="middle"></details><h2 id="Structure-Guided-Adversarial-Training-of-Diffusion-Models"><a href="#Structure-Guided-Adversarial-Training-of-Diffusion-Models" class="headerlink" title="Structure-Guided Adversarial Training of Diffusion Models"></a>Structure-Guided Adversarial Training of Diffusion Models</h2><p><strong>Authors:Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</strong></p><p>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively. </p><p><a href="http://arxiv.org/abs/2402.17563v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>扩散模型通过结构对抗训练，学习批内样本流形结构，提升图像生成和跨域微调任务的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>现有扩散模型专注于单个样本的去噪得分匹配损失优化，忽视批内样本之间的成对关系。</li><li>结构对抗训练 (SADM) 引入结构鉴别器来区分真实和生成的流形结构。</li><li>SADM 迫使模型学习训练批次中样本之间的流形结构。</li><li>SADM 与扩散变压器 (DiT) 相结合，在图像生成和跨域微调任务上优于现有方法。</li><li>SADM 在 12 个数据集上实现了图像生成和跨域微调任务的最新 FID 分别为 1.58 和 2.11。</li><li>SADM 在 256x256 和 512x512 分辨率下，在 ImageNet 上实现了类条件图像生成的最新 FID 分别为 1.58 和 2.11。</li><li>SADM 证明了流形结构学习对于扩散模型在生成任务中的重要性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：结构引导扩散模型对抗性训练</li><li>作者：杨凌、钱浩天、张智龙、刘景伟、崔斌</li><li>第一作者单位：北京大学</li><li>关键词：扩散模型、结构引导、对抗训练</li><li>论文链接：https://arxiv.org/abs/2402.17563   Github 链接：无</li><li>摘要：（1）研究背景：扩散模型在生成任务中表现出色，但现有方法主要关注最小化去噪得分匹配损失的加权和，训练过程侧重于实例级优化，忽略了小批量样本之间的宝贵结构信息。</li></ol><p>（2）过去的方法及其问题：现有方法主要集中在实例级优化，忽略了小批量样本之间的结构信息，导致无法充分建模数据分布。</p><p>（3）提出的研究方法：提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构。引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构。</p><p>（4）方法在任务和性能上的表现：SADM 显著提升了现有扩散 Transformer，在 12 个数据集上的图像生成和跨域微调任务中优于现有方法，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTA FID，验证了方法的有效性。</p><p>7.Methods：（1）提出结构引导扩散模型对抗性训练（SADM），通过对抗训练指导扩散生成器学习小批量样本之间的流形结构；（2）引入结构判别器来区分真实流形结构和生成流形结构，确保模型捕获数据分布中的真实流形结构；（3）采用Wasserstein GAN损失函数，指导生成器生成与真实流形结构相似的样本；（4）在训练过程中交替更新生成器和判别器，直至达到纳什均衡；（5）将SADM与扩散Transformer相结合，形成更强大的图像生成模型。</p><ol><li>总结(1): 本文提出了从结构角度优化扩散模型的结构引导对抗性训练方法，该训练算法可以轻松推广到图像和潜在扩散模型，并通过理论推导和实验结果一致地改进了现有的扩散模型。我们在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能。对于未来的工作，我们将把我们的方法扩展到更具挑战性的基于扩散的应用程序（例如，文本到图像/视频生成）。(2): 创新点: 提出结构引导对抗性训练方法，通过对抗训练指导扩散生成器学习小批量样本之间的流形结构，从而提升扩散模型的生成质量。性能: 在 12 个图像数据集上的图像生成和跨域微调任务中取得了新的 SOTA 性能，在 ImageNet 上分别以 256×256 和 512×512 的分辨率实现了 1.58 和 2.11 的新 SOTAFID。工作量: 该方法易于实现，可以轻松推广到图像和潜在扩散模型，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-11a45496d9d4169c7ee0bbb4a6534ffa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4ae1e4da806d223271756f678f15ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02b820484fca35ffef9bc52706101c79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14ed9373ba8bdaf3ecaca75391245256.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75ca2aa69507bb15984388d3520039af.jpg" align="middle"></details><h2 id="Diffusion-Model-Based-Image-Editing-A-Survey"><a href="#Diffusion-Model-Based-Image-Editing-A-Survey" class="headerlink" title="Diffusion Model-Based Image Editing: A Survey"></a>Diffusion Model-Based Image Editing: A Survey</h2><p><strong>Authors:Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao</strong></p><p>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a>. </p><p><a href="http://arxiv.org/abs/2402.17525v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型在图像生成和编辑任务中应用广泛，可从复杂分布中生成高质量样本，且支持无条件和输入条件下的图像编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型通过学习逆转图像加噪过程，生成高质量样本。</li><li>扩散模型图像编辑方法可分为不同学习策略、用户输入条件和编辑任务。</li><li>图像修复和外延可使用传统上下文驱动方法或多模态条件方法。</li><li>提出 EditEval 基准和 LMM 评分用于评估文本指导图像编辑算法。</li><li>目前存在限制，未来研究方向包括多模态、3D 和编辑元数据。</li><li>可在 <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a> 获取相关代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的图像编辑：综述</li><li>作者：Yi Huang、Jiancheng Huang、Yifan Liu、Mingfu Yan、Jiaxi Lv、Jianzhuang Liu、Wei Xiong、He Zhang、Shifeng Chen、Liangliang Cao</li><li>单位：深圳先进技术研究院</li><li>关键词：Diffusion Model、Image Editing、AIGC</li><li>链接：https://arxiv.org/abs/2402.17525Github：无</li><li>摘要：(1)：随着人工智能（AI）技术的发展，AI 生成的内容（AIGC）领域蓬勃发展，图像编辑作为其中一项重要任务，在数字媒体、广告和科学研究等领域有着广泛的应用。(2)：基于扩散模型的图像编辑方法近年来取得了显著进展，该方法通过学习逐步给图像添加噪声并逆转这一过程，可以从复杂分布中生成高质量的样本。(3)：本文对基于扩散模型的图像编辑方法进行了全面的综述，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析和分类。(4)：基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果，本文还提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。</li></ol><p>7.Methods:(1): 基于CLIP指导的图像编辑：DiffusionCLIP、Asyrp、EffDiff、DiffStyler、StyleDiffusion、UNIT-DDPM、CycleNet、DiffusionAutoencoders、HDAE、EGSDE、Pixel-GuidedDiffusion；(2): 基于参考和属性指导的图像编辑：PbE、RIC、ObjectStitch、PhD、DreamInpainter、Anydoor、FADING、PAIRDiffusion、SmartBrush、IIR-Net；(3): 基于指令指导的图像编辑：InstructPix2Pix、MoEController、FoI、LOFIE、InstructDiffusion、EmuEdit、DialogPaint、Inst-Inpaint、HIVE、ImageBrush、InstructAny2Pix、MGIE、SmartEdit。</p><ol><li>结论：（1）本工作对基于扩散模型的图像编辑方法进行了全面的综述，从多个角度对现有工作进行了深入分析和分类，并提出了一个系统性的基准 EditEval 和一个创新的指标 LMMScore 来进一步评估文本引导图像编辑算法的性能。（2）创新点：</li><li>提出了一种新的图像编辑基准 EditEval 和一个创新的指标 LMMScore，用于评估文本引导图像编辑算法的性能。</li><li>对基于扩散模型的图像编辑方法进行了全面的综述和分类，从学习策略、用户输入条件和具体编辑任务等多个角度对现有工作进行了深入分析。</li><li>探索了这些方法在增强编辑性能方面的贡献。</li><li>在我们的图像编辑基准 EditEval 中对 7 项任务进行了评估，以及最新最先进的方法。</li><li>总结了图像编辑领域的广泛潜力，并提出了未来研究的方向。</li><li>性能：在 EditEval 基准上，基于扩散模型的图像编辑方法在图像修复、图像外延等任务上取得了很好的效果。</li><li>工作量：本文对超过 100 种基于扩散模型的图像编辑方法进行了综述和分类，并对 7 项任务进行了评估，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4c52565ddb49dad37f10475b00a6abbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4537d5996d9b29f71e82d00a227227b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db76ba27193f9ab6b62bab161a239510.jpg" align="middle"></details><h2 id="Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network"><a href="#Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network" class="headerlink" title="Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network"></a>Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network</h2><p><strong>Authors:Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong</strong></p><p>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically. </p><p><a href="http://arxiv.org/abs/2402.17285v1">PDF</a> Accepted by AAAI2024</p><p><strong>Summary</strong><br>扩散模型与群组自编码器相结合的创新框架，有效提升高光谱图像超分辨率，显著改善谱空关系建模和低层细节恢复。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型擅长建模复杂关系和学习视觉特征，在高光谱图像超分辨率中潜力巨大。</li><li>训练扩散模型面临收敛困难和推理时间长挑战。</li><li>群组自编码器框架通过将高维高光谱数据编码到低维潜在空间，缓解了扩散模型训练难度，并保持了波段相关性。</li><li>扩散模型与群组自编码器相结合，有效解决了推理时间问题。</li><li>在自然和遥感高光谱数据集上，该方法在视觉和度量上均优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型和组自编码器的超分辨率高光谱图像增强</li><li>作者：王兆阳，李东阳，张明阳，罗浩，巩茂国</li><li>隶属单位：西安电子科技大学协同智能系统教育部重点实验室</li><li>关键词：高光谱图像，超分辨率，扩散模型，组自编码器</li><li>论文链接：https://arxiv.org/abs/2402.17285</li><li><p>摘要：（1）研究背景：现有高光谱图像超分辨率方法难以有效捕捉复杂的光谱-空间关系和低级细节，而扩散模型是一种有前途的生成模型，以其在建模复杂关系和学习高低级视觉特征方面的出色性能而闻名。（2）过去方法及问题：将扩散模型直接应用于高光谱图像超分辨率面临着模型收敛困难和推理时间长的挑战。（3）研究方法：提出了一种新的组自编码器（GAE）框架，该框架与扩散模型协同结合，构建了一个高效的高光谱图像超分辨率模型（DMGASR）。提出的 GAE 框架将高维高光谱数据编码为低维潜在空间，扩散模型在此空间中工作，从而缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。（4）任务和性能：在自然和遥感高光谱数据集上的实验结果表明，所提出的方法在视觉和度量上都优于其他最先进的方法。</p></li><li><p>方法：(1): 提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）；(2): 该模型采用两阶段训练方式，包括自动编码器和扩散超分辨率模型；(3): 采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间；(4): 训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间；(5): 训练自动编码器重构输入数据，生成一系列隐藏变量；(6): 将低分辨率隐藏变量作为条件信息，与高分辨率隐藏变量串联，在去噪过程中加入到扩散模型中；(7): 采用 U-Net 作为去噪模型，迭代去除噪声，生成超分辨率潜在变量列表；(8): 将超分辨率潜在变量列表传递给解码器，生成超分辨率图像。</p></li><li><p>结论：（1）：本文提出了一种基于扩散模型和组自编码器的高光谱图像超分辨率增强模型（DMGASR），该模型将扩散模型与自动编码器相结合，有效解决了扩散模型在高维数据上收敛困难的问题，并通过在低维潜在空间中训练扩散模型，大大减少了推理时间。该方法在自然和遥感高光谱数据集上均取得了优异的性能，在视觉和度量上均优于其他最先进的方法。（2）：创新点：</p></li><li>提出了一种基于扩散模型和组自编码器的超分辨率高光谱图像增强模型（DMGASR）。</li><li>采用两阶段训练方式，包括自动编码器和扩散超分辨率模型。</li><li>采用谱分组策略和非对称解码器设计，有效地将高维高光谱数据编码为低维潜在空间。</li><li>训练扩散模型在潜在空间中工作，缓解了训练扩散模型的难度，同时保持了波段相关性并大大减少了推理时间。性能：</li><li>在自然和遥感高光谱数据集上均取得了优异的性能。</li><li>在视觉和度量上均优于其他最先进的方法。工作量：</li><li>算法设计和实现。</li><li>数据集的收集和预处理。</li><li>实验的进行和结果分析。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1b637edd1829307f3889177173204f7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc3237f0ece24500c44086801ebc1feb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3e331ea518a2b9c151178e17f115708.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b211209593777f9420f6bb845daa71b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f24696c9c22f22b6e487ce2e6fc31ec7.jpg" align="middle"></details><h2 id="One-Shot-Structure-Aware-Stylized-Image-Synthesis"><a href="#One-Shot-Structure-Aware-Stylized-Image-Synthesis" class="headerlink" title="One-Shot Structure-Aware Stylized Image Synthesis"></a>One-Shot Structure-Aware Stylized Image Synthesis</h2><p><strong>Authors:Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong</strong></p><p>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. </p><p><a href="http://arxiv.org/abs/2402.17275v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>基于扩散模型的 OSASIS 实现了图像风格化，同时保持了结构完整性，即使是对训练中很少遇到的输入图像。</p><p><strong>Key Takeaways</strong></p><ul><li>OSASIS 采用扩散模型进行图像风格化，解决了 GAN 模型在保持结构方面的不足。</li><li>OSASIS 能够有效分离图像语义和结构，可控地调整给定输入的内容和风格级别。</li><li>OSASIS 在各种实验设置中表现出色，包括使用域外参考图像进行风格化和使用文本驱动的操作进行风格化。</li><li>与其他风格化方法相比，OSASIS 在训练中很少遇到的输入图像上表现得尤为出色，为通过扩散模型进行风格化提供了有前景的解决方案。</li><li>OSASIS 采用了渐进式训练策略，通过从添加噪声到恢复图像，逐步将风格应用于输入。</li><li>OSASIS 使用预训练的扩散模型，提高了效率和泛化性。</li><li>OSASIS 在图像风格化领域展现出了广泛的应用前景，包括图像编辑、艺术创作和视频处理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：单次结构感知风格化图像合成</li><li>作者：Jongmin Lee*, Jaeyeon Kang, Sangwoo Mo, Seongwon Lee†, Kyoung Mu Lee†</li><li>隶属单位：NAVER Cloud</li><li>关键词：图像风格化、扩散模型、结构保持</li><li>论文链接：https://arxiv.org/abs/2302.05447, Github 代码链接：无</li><li>摘要：(1) 研究背景：GAN 模型在图像风格化任务中取得成功，但难以在风格化各种输入图像时保持结构。最近，扩散模型被用于图像风格化，但仍缺乏保持输入图像原始质量的能力。(2) 过去方法及问题：过去方法包括基于 GAN 的模型和基于扩散模型的方法。GAN 模型难以保持结构，而基于扩散模型的方法缺乏控制内容和风格的能力。(3) 本文提出的研究方法：本文提出了一种新的单次风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。OSASIS 通过将语义从图像的结构中解耦，从而有效地控制应用于给定输入的内容和风格的级别。(4) 任务和性能：OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。结果表明，OSASIS 优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像，为通过扩散模型进行风格化提供了一种有前景的解决方案。</li></ol><p><strong>Methods：</strong></p><ol><li><strong>图像分解：</strong>将输入图像分解为内容和结构特征，其中内容特征表示图像的语义信息，而结构特征表示图像的几何形状和纹理。</li><li><strong>风格嵌入：</strong>将参考风格图像嵌入到一个潜在空间中，该空间由扩散模型训练。</li><li><strong>风格传输：</strong>将输入图像的内容特征与参考风格的风格嵌入相结合，生成一个新的图像，该图像具有输入图像的结构和参考风格的风格。</li><li><p><strong>结构保持：</strong>通过使用一个额外的损失函数，将输入图像的结构特征与生成图像的结构特征进行匹配，从而保持输入图像的原始质量。</p></li><li><p>结论：(1): 本工作提出了一种基于扩散模型的新型单次图像风格化方法 OSASIS，该方法在结构保持方面具有鲁棒性。与基于 GAN 和其他基于扩散的风格化方法相比，OSASIS 展示了在风格化中对结构的强大感知，有效地将图像的结构和语义解耦。尽管 OSASIS 在结构感知风格化方面取得了重大进展，但仍存在一些局限性。OSASIS 的一个显着限制是其训练时间，比比较方法更长。这种延长的训练持续时间是为了换取该方法增强了保持结构完整性和适应各种风格的能力。此外，OSASIS 需要针对每张风格图像进行训练。在需要跨多种风格快速部署的场景中，这一要求可以被视为一种限制。尽管存在这些挑战，但 OSASIS 在保持输入图像结构完整性方面的稳健性、其在域外参考风格化中的有效性以及其在文本驱动操作中的适应性使其成为风格化图像合成领域中一种很有前景的方法。未来的工作将解决这些限制，特别是在优化训练效率和减少对单个风格图像训练的必要性方面，以增强 OSASIS 在各种实际场景中的实用性和适用性。(2): 创新点：</p></li><li>提出了一种新的图像风格化方法 OSASIS，该方法基于扩散模型，在结构保持方面具有鲁棒性。</li><li>OSASIS 通过将图像的结构和语义解耦，有效地控制应用于给定输入的内容和风格的级别。</li><li>OSASIS 在各种实验设置中得到应用，包括使用域外参考图像的风格化和使用文本驱动的操作的风格化。性能：</li><li>OSASIS 在结构保持方面优于其他风格化方法，特别是对于在训练期间很少遇到的输入图像。</li><li>OSASIS 为通过扩散模型进行风格化提供了一种有前景的解决方案。工作量：</li><li>OSASIS 的训练时间比比较方法更长。</li><li>OSASIS 需要针对每张风格图像进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-957518995345024bb9a18f0e683a4e55.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0f3cefa16e52b2bb0bdbb679863e234.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e8afc30904c2bad1400fb9f044e33a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0eead50e28d5ed02ff0105780a9e22e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b842ecc40528644a1d824a5a8948f487.jpg" align="middle"></details><h2 id="Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation"><a href="#Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation" class="headerlink" title="Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation"></a>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation</h2><p><strong>Authors:Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi</strong></p><p>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models. </p><p><a href="http://arxiv.org/abs/2402.17245v1">PDF</a> Model weights:   <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic</a></p><p><strong>Summary</strong><br>通过对噪声时间表、宽高比准备和面向人类的微调的研究，Playground v2.5  diffusion 模型可产生极佳的美学质量。</p><p><strong>Key Takeaways</strong></p><ul><li>噪音时间表对模型真实性和视觉保真度至关重要。</li><li>平衡的分区数据集可改善不同宽高比的图像生成。</li><li>将模型输出与人类偏好相结合可提升图像的共鸣效果。</li><li>Playground v2.5 在各种条件和宽高比下表现出最先进的审美质量。</li><li>Playground v2.5 模型开源，为提升基于扩散的图像生成模型的审美质量提供了有价值的指导。</li><li>Playground v2.5 优于 SDXL、Playground v2、DALLE 3 和 Midjourney v5.2。</li><li>研究有助于提高基于扩散的图像生成模型的审美质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Playground v2.5：提升文本到图像生成审美质量的三点见解</li><li>作者：Daiqing Li、Aleks Kamko、Ehsan Akhgari、Ali Sabet、Linmiao Xu、Suhail Doshi</li><li>第一作者单位：Playground Research</li><li>关键词：文本到图像生成、扩散模型、审美质量</li><li>论文链接：arXiv:2402.17245v1[cs.CV]</li><li>摘要：（1）研究背景：文本到图像生成模型在生成图像的审美质量方面取得了显著进展，但仍存在一些挑战，如颜色和对比度不足、不同宽高比生成质量不佳、缺乏对人类偏好的对齐。（2）过去方法：以往方法主要集中在改进扩散模型的训练过程，如优化噪声调度或使用更大的数据集。然而，这些方法在提升审美质量方面效果有限。（3）研究方法：本文提出了三点见解来提升审美质量：改进噪声调度以增强颜色和对比度，构建平衡的分桶数据集以支持不同宽高比的生成，以及利用人类反馈来对齐模型输出与人类偏好。（4）方法性能：在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于 SDXL、Playground v2 等开源模型和 DALL·E 3、Midjourney v5.2 等闭源商业系统。</li></ol><p>方法：（1）改进噪声调度：采用 EDM 框架和更噪声的调度方式，增强图像色彩和对比度。（2）平衡分桶数据集：构建包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。（3）利用人类反馈：使用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。</p><ol><li>总结：（1）：本文提出 Playground v2.5，该模型通过改进噪声调度、构建平衡的分桶数据集和利用人类反馈等三点见解，提升了文本到图像生成模型的审美质量。（2）：创新点：</li><li>提出了一种新的噪声调度框架，增强了图像的色彩和对比度。</li><li>构建了一个包含不同宽高比图像的分桶数据集，支持多种宽高比的生成。</li><li>利用人类评级系统自动筛选高质量数据集，并采用迭代训练方法，根据人类偏好对齐模型输出。性能：</li><li>在广泛的分析和实验中，Playground v2.5 在各种条件和宽高比下展示了最先进的审美质量，优于其他开源和闭源模型。</li><li>Playground v2.5 在增强图像色彩和对比度、生成不同宽高比的高质量图像以及对齐模型输出与人类偏好方面表现出色，尤其是在生成人物图像的精细细节方面。工作量：</li><li>该模型已开源，用户可以在 Playground 产品网站上使用。</li><li>Playground v2.5 的权重已在 Hugging Face 上开源。</li><li>Playground 将继续提供扩展，以便在 A1111 和 ComfyUI 等流行社区工具中使用 Playground v2.5。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b9ee43af14ab727bc293d7a249e6d156.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ff95dbf16b9c2e734124d2c99954b6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b62a3df3bac0ff8ef7d20dfeccb0f6b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-869a1d35fa675595c5662a91b215c366.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226f377d76bcd81c0c005d4e513c6f81.jpg" align="middle"></details><h2 id="SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution"><a href="#SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution"></a>SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution</h2><p><strong>Authors:Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang</strong></p><p>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at <a href="https://github.com/lose4578/SAM-DiffSR">https://github.com/lose4578/SAM-DiffSR</a>. </p><p><a href="http://arxiv.org/abs/2402.17133v1">PDF</a> </p><p><strong>Summary</strong><br>基于扩散的超分辨率模型中，本文提出了一种新颖的SAM-DiffSR方法，该方法利用SAM的精细结构信息在采样噪声的过程中来改善最终图像质量，而推理过程中不需要SAM，有效降低了计算成本。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种SAM-DiffSR模型，可以利用SAM的精细结构信息来改善图像质量。</li><li>SAM-DiffSR模型通过将编码的掩码整合到前向扩散过程中，在采样噪声之前进行调整。</li><li>该调整允许独立调整每个对应分割区域内的噪声均值。</li><li>扩散模型被训练来估计这种调制的噪声。</li><li>所提出的方法不改变反向扩散过程，并且在推理过程中不需要SAM。</li><li>实验结果表明，该方法有效地抑制了伪影，在DIV2K数据集上以PSNR指标超越了现有的基于扩散的方法0.74 dB。</li><li>代码和数据集可在<a href="https://github.com/lose4578/SAM-DiffSR获得。">https://github.com/lose4578/SAM-DiffSR获得。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SAM-DiffSR：用于图像超分辨率的结构调制扩散模型</li><li>作者：Chengcheng Wang、Zhiwei Hao、Yehui Tang、Jianyuan Guo、Yujie Yang、Kai Han、Yunhe Wang</li><li>单位：华为诺亚方舟实验室</li><li>关键词：图像超分辨率、扩散模型、结构调制</li><li>链接：https://arxiv.org/abs/2402.17133   Github：https://github.com/lose4578/SAM-DiffSR</li><li>摘要：（1）研究背景：   扩散模型在图像超分辨率领域取得了显著进展，但传统扩散模型从单一分布中进行噪声采样，限制了其处理真实场景和跨语义区域复杂纹理的能力。</li></ol><p>（2）过去方法及问题：   Segment Anything Model（SAM）能生成足够精细的区域掩码，增强扩散模型的细节恢复能力。但直接将 SAM 集成到 SR 模型中会大幅增加计算成本。</p><p>（3）研究方法：   提出 SAM-DiffSR 模型，在噪声采样过程中利用 SAM 的精细结构信息，在不增加推理计算成本的情况下提高图像质量。在训练过程中，将结构位置信息编码到 SAM 的分割掩码中。然后将编码后的掩码集成到前向扩散过程中，将其调制到采样的噪声中。这种调整允许在每个对应的分割区域内独立调整噪声均值。扩散模型被训练来估计这种调制的噪声。</p><p>（4）方法性能：   实验结果表明，所提出的方法有效，在抑制伪影方面表现出优异的性能，在 DIV2K 数据集上以 PSNR 衡量，比现有的基于扩散的方法提高了 0.74dB。该方法的性能支持其目标。</p><p><strong>Methods：</strong></p><p>(1) 利用 SegmentAnythingModel（SAM）生成精细的区域掩码，编码结构位置信息。</p><p>(2) 将编码后的掩码集成到前向扩散过程中，调制采样的噪声。</p><p>(3) 训练扩散模型估计调制的噪声，从而在每个分割区域内独立调整噪声均值。</p><ol><li>结论：（1）：本文重点通过集成 SAM，增强基于扩散的图像超分辨率模型的结构层次信息恢复能力。具体来说，我们引入了一个名为 SAM-DiffSR 的框架，它涉及将结构位置信息纳入 SAM 生成的掩码，然后在正向扩散过程中将其添加到采样的噪声中。此操作单独调节每个相应分割区域中噪声的均值，从而将结构层次知识注入扩散模型。通过采用这种方法，训练后的模型在恢复结构细节和抑制图像伪影方面表现出改进，而无需产生任何额外的推理成本。我们的方法的有效性通过在常用的图像超分辨率基准上进行的广泛实验得到证实。（2）：创新点：利用 SAM 注入结构信息，增强扩散模型的结构恢复能力；性能：在抑制伪影和恢复结构细节方面优于现有方法；工作量：推理成本与基线模型相当。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9a754ccd89139d7dc6a576434e6b119e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0906797fab629c359270ce611fcb26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-66893d51d835b7965b76fb168b66db51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1f36de01723e09ebef0661e0e152ae2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bca3bdea09d0b0b3c4c6b041a3c1758.jpg" align="middle"></details><h2 id="Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing"><a href="#Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing" class="headerlink" title="Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing"></a>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing</h2><p><strong>Authors:Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a> </p><p><a href="http://arxiv.org/abs/2402.16627v1">PDF</a> ICLR 2024. Project: <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a></p><p><strong>Summary</strong><br>上下文扩散模型通过在扩散正反过程中加入文本可视关系，提升了文本引导可视化生成和编辑的语义对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在文本引导可视化生成和编辑中表现优越。</li><li>传统模型只将文本可视关系融入反向过程，忽略了正向过程的关联性。</li><li>正反过程的不一致性限制了文本语义在可视化合成结果中的传递精度。</li><li>语义扩散模型通过将文本条件和可视样本之间的交互和对齐纳入正反过程，改善了这种不一致性。</li><li>改进适用于 DDPM 和 DDIM，并通过理论推理得到证明。</li><li>在文本到图像生成和文本到视频编辑任务中，语义扩散模型均达到新的最佳性能。</li><li>定量和定性评估表明语义扩散模型显著提升了文本条件和生成样本之间的语义对齐。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：跨模态语境化扩散模型用于文本引导的视觉生成和编辑</li><li>作者：杨凌、张志龙、于兆宸、刘景伟、徐明凯、Stefano Ermon、崔斌</li><li>隶属：北京大学</li><li>关键词：文本引导视觉生成、文本引导视频编辑、扩散模型、语境化</li><li>论文链接：https://arxiv.org/abs/2402.16627   Github 代码链接：None</li><li><p>摘要：   (1)：研究背景：扩散模型在文本引导视觉生成和编辑领域表现优异，但现有方法主要关注将文本-视觉关系融入逆过程，忽视了其在前向过程中的相关性，导致文本语义在视觉合成结果中的精确传达受到限制。   (2)：过去方法及问题：现有方法存在以下问题：</p><ul><li>忽略了文本-视觉关系在前向过程中的作用，导致文本语义在视觉合成结果中的精确传达受限。</li><li>缺乏一种通用的语境化扩散模型，无法同时处理文本引导图像和视频生成/编辑任务。   (3)：研究方法：本文提出了一种新颖且通用的语境化扩散模型（CONTEXTDIFF），通过将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程来解决上述问题。具体来说，将该语境传播到两个过程中的所有时间步，以适应它们的轨迹，从而促进跨模态条件建模。同时，将语境化扩散模型推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。   (4)：任务和性能：在文本到图像生成和文本到视频编辑两个具有挑战性的任务上，CONTEXTDIFF 均取得了新的 SOTA 性能，显著增强了文本条件与生成样本之间的语义对齐，定量和定性评估均证明了这一点。</li></ul></li><li><p>Methods:(1): 提出跨模态语境化扩散模型（CONTEXTDIFF），将跨模态语境（包含文本条件和视觉样本之间的交互和对齐）融入前向和逆过程，促进跨模态条件建模；(2): 将语境化扩散模型推广到DDPM和DDIM，并通过理论推导证明了其有效性；(3): 在文本到图像生成和文本到视频编辑两个任务上，CONTEXTDIFF均取得了新的SOTA性能，显著增强了文本条件与生成样本之间的语义对齐。</p></li><li><p>结论：（1）本工作提出了一种新颖且通用的条件扩散模型（CONTEXTDIFF），通过将跨模态语境传播到扩散和逆过程中的所有时间步，并适应它们的轨迹，从而促进跨模态条件建模。我们将上下文化轨迹适配器推广到 DDPM 和 DDIM，并通过理论推导证明了其有效性。在文本到图像生成和文本到视频编辑这两个具有挑战性的任务上，CONTEXTDIFF 始终达到最先进的性能。两项任务的广泛定量和定性结果证明了我们提出的跨模态语境化扩散模型的有效性和优越性。（2）创新点：提出了一种新颖的跨模态语境化扩散模型，通过将跨模态语境融入扩散和逆过程，促进跨模态条件建模。性能：在文本到图像生成和文本到视频编辑两个任务上达到最先进的性能，显著增强了文本条件与生成样本之间的语义对齐。工作量：工作量较大，需要对扩散模型和跨模态语境化进行深入理解。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0bc30cb1ebccfebfcc1ffd4ee246c26b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64adb5f655a12b089618a5496f3cd332.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f01bc8ec645d09757f45be018ce1fe96.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a622ae5ed900b07d2994967a2269c23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0d264d770c3a4265052827f62ee48f0b.jpg" align="middle"></details><h2 id="Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation"><a href="#Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation" class="headerlink" title="Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation"></a>Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation</h2><p><strong>Authors:Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez</strong></p><p>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images. </p><p><a href="http://arxiv.org/abs/2402.16392v1">PDF</a> </p><p><strong>Summary</strong><br>使用扩散模型将对象插入上下文(POC)管道，可真实地向图像中添加任何对象，有效扩展数据集和改善异常分割性能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用扩散模型构建POC管道，可向图像中真实地添加任意对象。</li><li>POC能轻松扩展数据集，添加任意数量的对象。</li><li>POC生成的异常分割数据集比现有数据集更真实、全面。</li><li>POC能提升最新异常精调方法在基准测试中的性能。</li><li>POC可用于学习新类别，如将Pascal类别添加到Cityscapes。</li><li>基于POC生成图像训练的模型，其仿真到真实差距低。</li><li>POC管道能够提高模型应对未见语义类别的能力，增强异常分割性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：通过图像修复将对象置于上下文中以进行分布外分割2.作者：Paude Jorge†, Riccardo Volpi†, Puneet K. Dokania‡, Philip H.S. Torr‡, Grégory Rogez†3.所属机构：NAVERLABS 欧洲，牛津大学4.关键词：异常分割、分布外检测、图像修复、语义分割、开放词汇分割5.链接：https://github.com/naver/poc6.摘要：(1)：研究背景：在现实世界中部署语义分割模型时，模型不可避免地会遇到训练期间未见过的语义类别。因此，为了安全地部署此类系统，准确评估和提高其异常分割能力至关重要。然而，获取和标记语义分割数据代价高昂，而且意外情况是长尾且可能具有危险性。实际上，现有的异常分割数据集捕获的异常数量有限，缺乏真实性或具有很强的域偏移。(2)：过去的方法及其问题：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任何对象现实地添加到任何图像中。POC 可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于 POC 生成的不同异常分割数据集，并表明 POC 可以提高几种标准基准中最近的异常精细调整方法的性能。POC 还可以有效地学习新类别。例如，我们使用它通过添加 Pascal 类别的子集来编辑 Cityscapes 样本，并表明在这些数据上训练的模型与 Pascal 训练的基线实现了相当的性能。这证实了在 POC 生成的图像上训练的模型的低模拟到真实差距。(3)：提出的研究方法：POC 管道建立在图像修复和开放词汇分割模型之上，将任意对象现实地插入图像中。修改后的图像和掩码可用于不同的任务。(4)：方法在什么任务上取得了什么性能，该性能是否能支撑其目标：在我们的实验中，我们表明在 POC 生成的图像上进行微调可以显着提高最先进的异常分割方法的性能——优于通过标准做法（拼接 COCO 对象）进行微调的模型。我们还展示了三个基于 Cityscapes 和其他两个自动驾驶数据集的 POC 生成的评估集，并在其上对不同的异常分割方法进行了基准测试（有关结果的第一眼，请参见图 1）。最后，由于 POC 可以添加任意对象，我们展示了它可用于学习新类别。例如，使用动物类别增强 Cityscapes 图像导致 Pascal 测试集上的 93.14mIoU（考虑相同的类别），而直接在 Pascal 上训练产生 94.75——也就是说，在 POC 编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p><ol><li>方法：(1) POC管道：POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。修改后的图像和掩码可用于不同的任务，例如异常分割。(2) 异常分割微调：POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。(3) 学习新类别：POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</li></ol><p>8.结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；8. 结论：（1）：本文提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。POC可用于轻松地使用任意数量的对象扩展任何数据集。在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。（2）：创新点：- 提出了一种放置对象在上下文（POC）管道，通过扩散模型将任意对象现实地添加到任何图像中。- POC可用于轻松地使用任意数量的对象扩展任何数据集。- POC可以提高几种标准基准中最近的异常精细调整方法的性能。- POC还可以有效地学习新类别。性能：- 在我们的实验中，我们展示了基于POC生成的不同异常分割数据集，并表明POC可以提高几种标准基准中最近的异常精细调整方法的性能。- POC还可以有效地学习新类别。例如，我们使用它通过添加Pascal类别的子集来编辑Cityscapes样本，并表明在这些数据上训练的模型与Pascal训练的基线实现了相当的性能。这证实了在POC生成的图像上训练的模型的低模拟到真实差距。工作量：- POC管道由图像修复模型和开放词汇分割模型组成。图像修复模型用于将对象现实地插入图像中，而开放词汇分割模型用于为插入的对象生成掩码。- POC管道可用于生成异常分割数据集。在这些数据集上微调异常分割模型可以提高模型的性能，优于通过标准做法（拼接COCO对象）进行微调的模型。- POC管道还可以用于学习新类别。例如，使用动物类别增强Cityscapes图像导致Pascal测试集上的93.14mIoU（考虑相同的类别），而直接在Pascal上训练产生94.75——也就是说，在POC编辑的图像上训练的模型产生了相当小的模拟到真实差距。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-13236ee2bf286b59f5da0689a0363f64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec0e216eb8083342215a3e4e8c1dc95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2067d81b02e8cd7fea592f12fcef21d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37aa0eb4c5f86ae9ed22c98b2703f9a5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-84f58d6d1052332176a17f015aaa2d9f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-29  Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/</id>
    <published>2024-02-22T18:02:35.000Z</published>
    <updated>2024-02-22T18:02:35.955Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>摘要</strong><br> NeurRF 加速：一种新的计算方法，通过快速识别不必要的 3D 高斯体在实时渲染当前视图，从而提高渲染速度和图像质量。</p><p><strong>要点</strong></p><ul><li>NeurRF 是一种新的渲染方法，利用数百万个 3D 高斯体来表示 3D 场景，并将其投影到 2D 图像平面上进行渲染。</li><li>在渲染过程中，存在大量对于当前视图方向来说不必要的 3D 高斯体，导致识别这些高斯体的计算成本很高。</li><li>提出了一种计算简化技术，能够在实时快速识别不必要的 3D 高斯体，从而在不影响图像质量的情况下渲染当前视图。</li><li>该技术通过对距离相近的 3D 高斯体进行离线聚类来实现，然后在运行时将这些簇投影到 2D 图像平面上。</li><li>分析了该技术在 GPU 上执行时遇到的瓶颈，并提出了一种高效的硬件架构来无缝支持该方案。</li><li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影前平均排除 63% 的 3D 高斯体，从而将整体渲染计算量减少了近 38.3%，而峰值信噪比 (PSNR) 却不会下降。</li><li>所提出的加速器与 GPU 相比，速度提高了 10.7 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用聚类识别不必要的 3D 高斯体，实现 3D 高斯斑点渲染的快速渲染</li><li>作者：Joongho Jo, Hyeongwon Kim, Jongsun Park</li><li>单位：韩国大学电气工程学院（仅翻译单位名称）</li><li>关键词：3D 高斯斑点渲染、渲染、NeRF、神经辐射场、硬件加速器</li><li>链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github：无</li><li>摘要：（1）研究背景：在 3D 计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，已经得到了广泛的研究，但 3D 高斯斑点渲染 (3D-GS) 作为一种新的渲染方法，因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：1）将所有 3D 高斯体投影到 2D 图像平面上，并识别出影响 2D 图像颜色的 3D 高斯体。2）然后使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上，但被识别为不影响 2D 图像的颜色，投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。然而，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的位置和方向而改变，因此在将它们投影到 2D 图像平面前识别出不必要的 3D 高斯体仍然具有挑战性。因此，这些不必要的 3D 高斯体仍然会进行投影计算。因此，如果能够开发一种简单而有效的方法来预测不影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始前将它们排除，则可以显着降低整个 3D-GS 过程的总体计算复杂度。（3）本文提出的研究方法：本文提出了一种基于聚类的方法，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目的是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，我们采用了 K-means 聚类算法，它满足这两个标准。鉴于 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离决定，还考虑了高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在我们的方法中，聚类和计算簇的半径可以在线​​下执行，只有将簇投影到 2D 图像平面上是在实时进行的，这仅需要 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。所提出的加速器还实现了比 GPU 快 10.7 倍的速度。（4）方法在什么任务上取得了什么性能？性能是否支持其目标？本文提出的方法在 Mip-NeRF360 数据集上进行了评估。结果表明，该方法能够有效地排除不必要的 3D 高斯体，从而减少渲染计算量并提高渲染速度。具体来说，该方法可以排除平均 63% 的 3D 高斯体，从而将整体渲染计算减少了近 38.3%，而不会牺牲峰值信噪比 (PSNR)。此外，所提出的加速器还实现了比 GPU 快 10.7 倍的速度。这些结果表明，该方法能够有效地实现其目标，即快速渲染高质量的 3D 图像。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于聚类的方法，通过识别不影响2D图像颜色的簇来排除当前视图渲染过程中的不必要3D高斯体。该方法能够有效地减少渲染计算量并提高渲染速度，在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。（2）：创新点：本文提出了一种基于聚类的方法来识别不必要的3D高斯体，该方法简单有效，能够显着降低3D-GS渲染过程的总体计算复杂度。性能：该方法能够有效地排除不必要的3D高斯体，从而减少渲染计算量并提高渲染速度。在Mip-NeRF360数据集上，该方法可以排除平均63%的3D高斯体，从而将整体渲染计算减少了近38.3%，而不会牺牲峰值信噪比（PSNR）。此外，所提出的加速器还实现了比GPU快10.7倍的速度。工作量：该方法的实现相对简单，并且可以在线​​下执行聚类和计算簇的半径，只有将簇投影到2D图像平面上是在实时进行的，这仅需要6.2%的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow"><a href="#OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow" class="headerlink" title="OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow"></a>OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow</h2><p><strong>Authors:Simon Boeder, Fabian Gigengack, Benjamin Risse</strong></p><p>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain. </p><p><a href="http://arxiv.org/abs/2402.12792v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场可从仅使用二维标签中估计语义占用。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经辐射场（NeRF）提出了一种仅使用二维标签估计占用率的新方法。</li><li>采用可微体积渲染来预测深度和语义图，并仅基于二维监督训练三维网络。</li><li>为了增强几何精度并增加监督信号，引入了相邻时间步长的时序渲染。</li><li>引入占用流作为处理场景中动态对象并确保其时间一致性的机制。</li><li>与使用三维标签的方法相比，实验表明仅二维监督就足以实现最先进的性能，同时优于同时期的二维方法。</li><li>当将二维监督与三维标签、时态渲染和占用流相结合时，大大优于所有以前的占有估计模型。</li><li>渲染监督和占用流的进步促进了占用估计，并进一步缩小了该领域中自监督学习的差距。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：OccFlowNet：基于可微渲染和占用流的自监督占用估计</li><li>作者：Simon Boeder, Fabian Gigengack, Benjamin Risse</li><li>作者单位：博世公司、明斯特大学</li><li>关键词：占用估计、神经辐射场、可微渲染、占用流、自监督学习</li><li>论文链接：https://arxiv.org/abs/2402.12792</li><li><p>摘要：(1) 研究背景：语义占用最近作为一种突出的 3D 场景表示形式而受到广泛关注。然而，大多数现有方法依赖于具有细粒度 3D 体素标签的大型且昂贵的训练数据集，这限制了它们的实用性和可扩展性，增加了该领域中自监督学习的需求。(2) 过去方法及其问题：本文提出了受神经辐射场 (NeRF) 启发的新型占用估计方法，仅使用更易获取的 2D 标签。具体来说，我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。此外，我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。(3) 研究方法：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。我们得出结论，所提出的渲染监督和占用流促进了占用估计，并进一步缩小了该领域中自监督学习的差距。(4) 性能和结论：在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。这些结果支持了我们的目标，即仅使用 2D 监督就可以实现准确的占用估计，从而使该方法更易于训练和部署。</p></li><li><p>方法：(1)：我们提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。(2)：我们采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。(3)：为了提高几何精度并增加监督信号，我们引入了相邻时间步的长时渲染。(4)：我们引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。(5)：我们通过广泛的实验表明，仅使用 2D 监督就足以与使用 3D 标签的方法相比实现最先进的性能，同时优于同时期的 2D 方法。(6)：当将 2D 监督与 3D 标签、时序渲染和占用流相结合时，我们明显优于所有以前的占用估计模型。</p></li><li><p>结论：（1）：本工作首次提出了一种仅使用易于获取的 2D 标签即可进行占用估计的方法，无需昂贵的 3D 体素标签，为占用估计任务提供了一种新的思路。（2）：创新点：创新点 1：提出了一种新的占用估计方法 OccFlowNet，仅使用更易获取的 2D 标签，无需昂贵的 3D 体素标签。创新点 2：采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。创新点 3：为了提高几何精度并增加监督信号，引入了相邻时间步的长时渲染。创新点 4：引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。性能：在广泛使用的数据集上进行的实验表明，所提出的方法在占用估计任务上取得了最先进的性能。工作量：本工作需要解决的问题是，如何仅使用 2D 标签进行占用估计。为了解决这个问题，作者提出了 OccFlowNet 方法，该方法采用可微体积渲染来预测深度和语义图，并仅基于 2D 监督训练 3D 网络。为了提高几何精度并增加监督信号，作者引入了相邻时间步的长时渲染。此外，作者还引入了占用流作为处理场景中动态对象并确保其时间一致性的机制。通过广泛的实验，作者证明了所提出的方法在占用估计任务上取得了最先进的性能。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-48dbaf92efe683516d537be273981834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff303fd6f4dc54f5b59e902e9b98c34a.jpg" align="middle"></details><h2 id="Colorizing-Monochromatic-Radiance-Fields"><a href="#Colorizing-Monochromatic-Radiance-Fields" class="headerlink" title="Colorizing Monochromatic Radiance Fields"></a>Colorizing Monochromatic Radiance Fields</h2><p><strong>Authors:Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi</strong></p><p>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: <a href="https://liquidammonia.github.io/color-nerf">https://liquidammonia.github.io/color-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.12184v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）可通过一组二维图像产生色彩鲜艳的 3D 场景再现，但仅提供单色图像时便无法实现。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 可以使用一组 2D 图像生成世界的彩色 3D 表示。</li><li>仅提供单色图像时，NeRF 无法生成彩色 3D 表示。</li><li>NeRF 的目标是从单色辐射场再现彩色表示。</li><li>提出了一种在 Lab 颜色空间中将单色辐射场视为表示预测任务的方法。</li><li>首先使用单色图像构建亮度和密度表示，然后使用图像着色模块重新创建颜色表示。</li><li>然后通过亮度、密度和颜色的表示再现一个彩色隐式模型。</li><li>大量实验验证了所提出方法的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：彩色化单色辐射场</li><li>作者：叶安成、万任杰<em>、翁书琛、朱承轩、常亚坤、石博欣</em></li><li>隶属单位：北京大学多媒体信息处理国家重点实验室、计算机科学系</li><li>关键词：NeRF、单色图像、颜色再现、Lab颜色空间、图像着色</li><li>论文链接：https://arxiv.org/abs/2402.12184   Github 链接：无</li><li><p>摘要：   （1）研究背景：神经辐射场（NeRF）可以利用一组二维图像创建世界的彩色三维表示。然而，当只有单色图像可用时，这种能力就不复存在了。颜色对于表征世界是必要的，因此从单色辐射场中再现颜色变得至关重要。   （2）过去的方法及其问题：直接操纵单色辐射场似乎是实现颜色化的直接方法。一种解决方案是将颜色视为一种“风格”，然后将其转移到辐射场中。然而，这种策略并不能保证逐像素的颜色一致性，因此颜色只能不规则地分布在辐射场中，从而违背了合理性标准。另一种方法涉及直接操纵辐射场中的颜色属性。这种技术旨在通过识别当前的颜色属性并用新的颜色属性替换它们来替换颜色。然而，它不适用于没有现有颜色属性的单色辐射场。   （3）论文提出的研究方法：为了解决上述问题，本文提出了一种在 Lab 颜色空间中进行表示预测的任务。首先使用单色图像构建亮度和密度表示，然后利用图像着色模块重新创建颜色表示。最后，通过亮度、密度和颜色的表示来再现一个彩色隐式模型。   （4）方法在任务上的表现及其对目标的支持：本文的方法在多个任务上取得了优异的性能，包括单色图像着色、多视图图像合成和视频插帧。这些结果表明，本文的方法可以有效地从单色图像中再现颜色，并生成逼真且视觉上令人愉悦的彩色结果。</p></li><li><p>方法：(1) 构建亮度和密度表示：使用单色图像构建亮度和密度表示，为后续的颜色再现提供基础。(2) 图像着色模块：利用图像着色模块重新创建颜色表示，将单色图像中的信息转换为彩色表示。(3) 表示预测：在Lab颜色空间中进行表示预测，将亮度、密度和颜色的表示相结合，再现一个彩色隐式模型。(4) 颜色注入：利用分类器将颜色注入到辐射场中，确保颜色的合理性和一致性。(5) 直方图净化：使用直方图净化模块去除不合理的颜色，提高颜色的准确性和一致性。</p></li><li><p>结论：</p></li></ol><p>（1）意义：本文提出了一种从单色图像中再现颜色的新方法，该方法在多个任务上取得了优异的性能，为单色图像的彩色化提供了新的思路和技术支持。</p><p>（2）优缺点总结：</p><p>创新点：</p><ul><li>提出了一种在Lab颜色空间中进行表示预测的任务，有效地解决了单色图像着色的问题。</li><li>提出了一种颜色注入模块，确保了颜色的合理性和一致性。</li><li>提出了一种直方图净化模块，去除不合理的颜色，提高了颜色的准确性和一致性。</li></ul><p>性能：</p><ul><li>在单色图像着色、多视图图像合成和视频插帧等任务上取得了优异的性能。</li><li>生成的彩色结果逼真且视觉上令人愉悦。</li></ul><p>工作量：</p><ul><li>需要构建亮度和密度表示、图像着色模块、颜色注入模块和直方图净化模块。</li><li>需要训练模型，这可能需要大量的数据和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53ef44a8d86663951eb27790c491bec4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40e071a248a066a783512765ca1dd311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04a5930c0187125fe64b74f7d43ea704.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08fb7fd6e14278c9083abd8d5401c6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34c76f358a2021ed97956d162ca195e3.jpg" align="middle"></details>## One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation**Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang**Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. [PDF](http://arxiv.org/abs/2402.11909v1) **Summary**用一张或数张用户照片和 3DMM 编码即可生成高质量且可控动的头像。**Key Takeaways**- 该研究提出了一种使用一张或多张图像创建高质量头像的新方法。- 该方法利用了一个从 2407 个人的多视角面部表情数据集中学得的生成模型。- 该方法使用了基于 3DMM 的神经辐射场作为骨干网络，以增强通过少量输入进行自动解码的效果。- 该研究提出了一种通过联合优化 3DMM 拟合和相机校准来处理不稳定的 3DMM 拟合问题。- 该研究提出的方法在少量图像头像生成任务中表现出色，并优于现有技术。- 该方法为更高效和个性化的头像生成铺平了道路。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于 3DMM 的神经辐射场在虚拟化身的身份和表情建模中的应用</li><li>作者：Kangxue Yin, Changjian Li, Yebin Liu, Yue Dong, Kun Zhou, Chen Change Loy, Ziwei Liu</li><li>隶属机构：香港中文大学（深圳）</li><li>关键词：神经辐射场、3DMM、身份建模、表情建模、虚拟化身</li><li>论文链接：https://arxiv.org/abs/2302.09924，Github 代码链接：None</li><li><p>摘要：（1）：研究背景：虚拟化身在游戏、社交媒体和电子商务等领域有着广泛的应用。然而，现有的虚拟化身通常缺乏真实感和个性化。（2）：过去的方法及其问题：过去的方法通常使用 3D 模型来表示虚拟化身，但这些模型往往缺乏细节和真实感。此外，这些方法通常需要大量的手工制作，这使得它们难以个性化。（3）：研究方法：本文提出了一种基于 3DMM 的神经辐射场（NeRF）方法来表示虚拟化身。该方法将 3DMM 作为虚拟化身的骨架，并使用 NeRF 来生成虚拟化身的表面。NeRF 是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。（4）：方法性能：本文的方法在多个任务上取得了良好的性能。在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。</p></li><li><p>方法：（1）：多视角多表情人脸捕捉：我们从 13 个预定义的面部表情中捕获了总共 2407 个受试者的分辨率面部图像，这些图像来自 13 个稀疏摄像头视角。对于每个受试者在每个表情中，我们运行基于面部地标的 3DMM 拟合算法，并从多视角图像中重建 3D 几何形状。与现有的数据集（例如 FFHQ 中的 70K）相比，我们的数据集包含有限数量的独特受试者。尽管如此，它包含更广泛的面部表情，这在学习生成式先验模型中起着关键作用。（2）：生成式头像先验：我们的生成式头像先验生成了一个由神经辐射场表示的头像。给定一个身份编码 w 和一个表情编码 ψ，我们的模型 f 为 3D 查询点 q 从方向 d 查看时生成局部颜色 c 和密度 σ：σ(q), c(q) = f(w, ψ, q, d; θ),其中 θ 是模型权重。然后通过应用体积渲染公式获得每个像素的颜色来生成彩色图像：ˆc = ∫t^∞ T(t)σq(r(t))cq(r(t), d)dt,其中 T(t) = exp(−∫^t^0 σq(r(s))ds)。遵循先前的艺术，我们采用 3DMM 表达式代码空间作为 ψ，并学习 w 的潜在空间 Rl。（3）：3DMM 锚定头像生成模型：受 Bai 等人启发，我们采用 3DMM 锚定的神经辐射场作为我们的头像表示。具体来说，我们不会将所有渲染信息编码到一个高容量神经网络中，而是将局部特征附加在针对目标身份和表情重建的 3DMM 网格支架的顶点上。在渲染期间，每个查询点聚合来自 3DMM 顶点中的 k 个最近邻 (kNN) 的特征，并将其发送到 MLP 网络以预测颜色和密度。为了简化使用现有 2D CNN 的学习，可以在统一的 UV 空间中学习 3DMM 顶点附加特征，并使用纹理坐标进行采样。</p></li><li><p>结论：（1）：本文提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。NeRF是一种神经网络，它可以从一组稀疏的观测数据中学习生成连续的表面。该方法在身份建模和表情建模任务上取得了良好的性能。（2）：创新点：</p></li><li>提出了一种基于3DMM的神经辐射场方法来表示虚拟化身。</li><li>该方法将3DMM作为虚拟化身的骨架，并使用NeRF来生成虚拟化身的表面。</li><li>该方法在身份建模和表情建模任务上取得了良好的性能。性能：</li><li>在身份建模任务上，该方法能够生成逼真的虚拟化身，这些虚拟化身与真实的人类非常相似。</li><li>在表情建模任务上，该方法能够生成逼真的虚拟化身表情，这些表情与真实的人类表情非常相似。工作量：</li><li>该方法需要大量的数据来训练。</li><li>该方法的训练过程非常耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://picx.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details><h2 id="PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments"><a href="#PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments" class="headerlink" title="PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments"></a>PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments</h2><p><strong>Authors:Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</strong></p><p>Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.09325v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.00874</p><p><strong>Summary</strong><br>基于分层空间分割和多层次场景表示，PC-NeRF 框架实现了大规模场景的 3D 重建和新视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>PC-NeRF 框架由父 NeRF 和子 NeRF 两个模块组成，实现了分层空间分割和多层次场景表示。</li><li>分层空间分割和多层次场景表示可以提高稀疏激光雷达点云数据的利用效率，并实现快速获取近似体积场景表示。</li><li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况，并在有限的训练轮数下表现出很高的部署效率。</li><li>PC-NeRF 的实现和预训练模型可在 <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a> 上获取。</li><li>PC-NeRF 可以实现高精度的激光雷达新视图合成和 3D 重建。</li><li>PC-NeRF 可以有效处理稀疏激光雷达帧的情况。</li><li>PC-NeRF 在有限的训练轮数下表现出很高的部署效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PC-NeRF：自动驾驶环境中稀疏激光雷达帧的父子神经辐射场</li><li>作者：Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</li><li>隶属单位：北京理工大学机械工程学院</li><li>关键词：神经辐射场、三维场景重建、自动驾驶</li><li>论文链接：https://arxiv.org/abs/2402.09325，Github 链接：https://github.com/biter0088/pc-nerf</li><li><p>摘要：(1)：研究背景：大规模三维场景重建和新颖视角合成对于自动驾驶汽车进行环境探索、运动规划和闭环仿真至关重要，尤其是在可用传感器数据由于各种实际因素而变得稀疏的情况下。(2)：过去的方法及其问题：传统的显式表示可以描绘重建的场景和合成视图，但它们在以无限分辨率表示场景方面仍然存在重大瓶颈。最近开发的神经辐射场 (NeRF) 在隐式表示方面取得了引人注目的结果，但使用稀疏激光雷达帧进行大规模三维场景重建和新颖视角合成的难题仍未得到探索。(3)：提出的研究方法：为了弥合这一差距，我们提出了一种称为父子神经辐射场 (PC-NeRF) 的三维场景重建和新颖视角合成框架。该框架基于其两个模块，父 NeRF 和子 NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。(4)：方法在任务和性能上的表现：通过广泛的实验，PC-NeRF 被证明可以在大规模场景中实现高精度的激光雷达新视角合成和三维重建。此外，PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。</p></li><li><p>方法：（1）PC-NeRF框架：提出了一种称为父子神经辐射场（PC-NeRF）的三维场景重建和新颖视角合成框架，该框架基于其两个模块，父NeRF和子NeRF，实现了分层空间划分和多级场景表示，包括场景、片段和点级。（2）多级场景表示：多级场景表示增强了对稀疏激光雷达点云数据的有效利用，并能够快速获取近似体积场景表示。（3）训练过程：PC-NeRF采用分阶段训练策略，首先训练父NeRF，然后训练子NeRF，最后将父NeRF和子NeRF结合起来进行联合训练。（4）损失函数：PC-NeRF的损失函数包括父NeRF的损失函数和子NeRF的损失函数，父NeRF的损失函数包括重投影误差和光度误差，子NeRF的损失函数包括自由空间误差和深度误差。（5）新颖视角合成和三维重建：PC-NeRF可以通过新颖视角合成和三维重建来评估其性能，新颖视角合成是将稀疏激光雷达帧合成到新的视角，三维重建是将稀疏激光雷达帧重建为三维点云。</p></li><li><p>结论：(1)：本工作提出了一种适用于自动驾驶中稀疏激光雷达帧的大规模三维场景重建和新颖视角合成框架 PC-NeRF，该框架采用分层空间划分和多级场景表示，有效利用稀疏激光雷达点云数据，实现高精度的新颖视角合成和三维重建。(2)：创新点：PC-NeRF 提出了一种分层空间划分和多级场景表示的方法，有效利用稀疏激光雷达点云数据。PC-NeRF 提出了一种两步深度推理方法，实现从片段到点的推理。PC-NeRF 在 KITTI 和 MaiCity 数据集上进行了广泛的实验，证明了其在稀疏激光雷达帧条件下进行新颖视角合成和三维重建的有效性。性能：PC-NeRF 在 KITTI 和 MaiCity 数据集上实现了高精度的激光雷达新视角合成和三维重建。PC-NeRF 可以有效地处理稀疏激光雷达帧的情况，并证明了在有限的训练轮次下具有较高的部署效率。工作量：PC-NeRF 的实现相对简单，易于部署。PC-NeRF 的训练过程需要大量的数据和计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6782f984ff8bf4da1d81a6ca240eded4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a2171d3c5e58e5589aa20525792832a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7d40aa20abd78a5813673cde1893940.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-40b695293253e411ba8966555ca76058.jpg" align="middle"></details><h2 id="NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs"><a href="#NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs" class="headerlink" title="NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs"></a>NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs</h2><p><strong>Authors:Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel</strong></p><p>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines. </p><p><a href="http://arxiv.org/abs/2402.08622v1">PDF</a> Project page: <a href="https://mfischer-ucl.github.io/nerf_analogies/">https://mfischer-ucl.github.io/nerf_analogies/</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 可将场景的 3D 几何形状和外观进行编码。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 可以将源 NeRF 中的外观转移到目标 3D 几何形状上，从而创建具有目标几何形状但外观类似于源 NeRF 的新 NeRF。</li><li>该方法将经典图像类比从 2D 图像推广到 NeRF。</li><li>基于语义亲和性的对应转移，由大型预训练 2D 图像模型提供的语义特征驱动，可实现多视图一致外观转移。</li><li>该方法能够探索 3D 几何形状和外观的混合匹配产品空间。</li><li>该方法优于传统的基于样式化的方法。</li><li>大多数用户更喜欢该方法，而不是其他几种典型基线方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF 类比：基于示例的 NeRF 视觉属性迁移</li><li>作者：Michael Fischer、Zhengqin Li、Thu Nguyen-Phuoc、Aljaž Božič、Zhao Dong、Carl Marshall、Tobias Ritschel</li><li>第一作者单位：伦敦大学学院</li><li>关键词：NeRF、视觉属性迁移、语义特征、深度学习</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：</li></ol><p>（1）研究背景：NeRF（神经辐射场）是一种用于表示和渲染 3D 场景的强大技术。然而，NeRF 通常需要大量数据才能训练，并且难以将从一个场景学到的外观迁移到另一个场景。（2）过去方法及其问题：过去的方法通常使用基于样式迁移的技术来将一种场景的外观迁移到另一种场景。然而，这些方法通常难以产生语义上连贯的结果，并且需要大量的数据来训练。（3）研究方法：本文提出了一种新的方法，可以将一种场景的外观迁移到另一种场景，而无需大量的数据。该方法利用了预训练的 2D 图像模型中的语义特征来建立源场景和目标场景之间的对应关系。然后，这些对应关系被用来将源场景的外观迁移到目标场景。（4）方法性能：该方法在多个数据集上进行了评估，结果表明该方法能够产生语义上连贯的结果，并且优于过去的方法。此外，该方法还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。</p><p><methods>:(1)：我们的方法利用预训练的二维图像模型中的语义特征来建立源场景和目标场景之间的对应关系。(2)：然后，这些对应关系被用来将源场景的外观迁移到目标场景。(3)：我们训练了一个三维一致的NeRF表示，该表示在先前提取的点云FSource和FTarget上。(4)：我们采样FSource中的位置，并在每个位置提取源特征描述符fSource、源外观LSource和源视向ωSource。(5)：我们还从目标点云FTarget中采样位置，并在每个位置获取图像特征fTarget和目标位置xTarget。(6)：我们找到一个离散映射ϕ，该映射将每个目标位置索引j映射到具有最大相似性的源位置索引i。(7)：我们定义LTargetj=LSourceϕj作为目标在映射ϕ和某个视向下的外观。(8)：我们训练NeRF Analogy Lθ的参数θ，使得对于每个观察到的目标位置，目标和源外观在源视向下一致。</methods></p><ol><li>结论：（1）：本工作首次提出了 NeRF 类比，一种基于语义相似性的 NeRF 视觉属性迁移框架。该方法可以辅助内容创作，例如，通过将用户捕获的几何体与在线 3D 模型的外观相结合，并且还适用于多对象设置和真实世界场景。我们的方法在颜色迁移、图像合成和风格化文献中的其他方法中表现出色，并且在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。（2）：创新点：</li><li>提出了一种基于语义相似性的 NeRF 视觉属性迁移框架。</li><li>该框架可以用于辅助内容创作、多对象设置和真实世界场景。</li><li>该框架在颜色迁移、图像合成和风格化文献中的其他方法中表现出色。性能：</li><li>该框架在用户研究中获得了最高的排名，无论是在迁移质量还是多视图一致性方面。</li><li>该框架可以生成语义上连贯的结果，并且优于过去的方法。</li><li>该框架还可以用于生成新的场景，这些场景具有源场景的外观和目标场景的几何形状。工作量：</li><li>该框架需要预训练一个 2D 图像模型来提取语义特征。</li><li>该框架需要训练一个 3D 一致的 NeRF 表示。</li><li>该框架需要找到一个离散映射来将源场景和目标场景之间的对应关系。</li><li>该框架需要训练一个 NeRF 类比模型来将源场景的外观迁移到目标场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-56d4edbaccc121abec3c1fbc5aa2a7b2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b96734ea48c9163e25bc72d32ad13598.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d80da8fbb7f50a1faceaf09341a6dada.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c35035cd1513fc1b8683c14a413721b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-190136188bdfd4cb8f04bafbfb9ef577.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/</id>
    <published>2024-02-22T17:38:45.000Z</published>
    <updated>2024-02-22T17:38:45.284Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>Summary</strong><br>3D 高斯散splatting 通过聚类 和 投影优化，减少了 38.3% 的渲染计算，且不损失图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯散splatting（3D-GS）是一种新的渲染方法，在速度和图像质量上优于神经辐射场（NeRF）。</li><li>3D-GS 使用数百万个 3D 高斯表示 3D 场景，并将这些高斯投影到 2D 图像平面上进行渲染。</li><li>在渲染过程中，大量不必要的高斯存在于当前视图方向，导致与识别它们相关的计算成本巨大。</li><li>提出了一种计算简化技术，可在运行时快速识别出不必要的高斯，用于渲染当前视图，且不损害图像质量。</li><li>这种简化技术方法是离线对距离相近的高斯进行聚类，然后在运行时将这些集群投影到 2D 图像平面上。</li><li>对该技术在 GPU 上执行时的瓶颈进行了分析，并提出了一种与该方案无缝兼容的高效硬件架构。</li><li>对于 Mip-NeRF360 数据集，该技术在 2D 图像投影之前平均排除了 63% 的高斯，将整体渲染计算减少了 38.3%，且不损失峰值信噪比 (PSNR)。</li><li>该加速器与 GPU 相比，还实现了 10.7 倍的加速。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用聚类识别不必要的 3D 高斯体，以快速渲染 3D 高斯体飞溅</li><li>作者：Joongho Jo、Hyeongwon Kim 和 Jongsun Park</li><li>隶属机构：韩国大学电气工程学院</li><li>关键词：3D 高斯体飞溅、渲染、NeRF、神经辐射场、硬件加速器</li><li>论文链接：Paper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting，Github 链接：无</li><li>摘要：</li></ol><p>（1）研究背景：在计算机视觉应用中，例如增强现实 (AR)、虚拟现实 (VR) 和元宇宙，快速且高质量的图像渲染非常重要。虽然已经广泛研究了使用深度神经网络的渲染技术，例如神经辐射场 (NeRF)，但 3D 高斯体飞溅 (3D-GS) 作为一种新的渲染方法，最近因其与传统 NeRF 相比能够快速渲染高质量图像而备受关注。3D-GS 利用数百万个 3D 高斯体来表示复杂的 3D 场景，并通过将 3D 高斯体投影到 2D 图像平面上来渲染 3D 场景。</p><p>（2）过去的方法及其问题：3D-GS 渲染过程主要分为两步：首先，将所有 3D 高斯体投影到 2D 图像平面上，并识别影响 2D 图像颜色的 3D 高斯体。然后，使用影响颜色的已识别 3D 高斯体计算 2D 图像中每个像素的颜色。在渲染过程的第一步中，高斯体投影到 2D 图像上后，如果被识别为不影响 2D 图像的颜色，那么投影就变成了计算浪费。在 Mip-NeRF360 数据集中，平均约有 67.6% 的 3D 高斯体不影响 2D 图像的颜色。因此，这些高斯体可以从当前视图渲染过程中排除。但是，由于影响 2D 图像颜色的 3D 高斯体可能会随着渲染视点的方向和位置而改变，因此在将 3D 高斯体投影到 2D 图像平面上之前识别不必要的高斯体仍然具有挑战性。因此，这些不必要的高斯体仍然会进行投影计算。因此，如果可以开发出一种简单而有效的方法来预测不会影响 2D 图像颜色的 3D 高斯体，并在渲染过程开始之前将它们排除，那么可以显着降低整个 3D-GS 过程的总体计算复杂度。</p><p>（3）本文提出的研究方法：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。聚类的目标是将位置相近的 3D 高斯体分组在一起，并且簇的形状应该是球形的，以便于投影到 2D 图像上。因此，本文采用 K-means 聚类算法，该算法满足这两个标准。考虑到 3D 高斯体具有由其协方差定义的大小或影响，簇球体的半径不仅由到簇质心的距离确定，还要考虑高斯体的大小。然后将这些定义的簇球体投影到 2D 图像平面上，以确定它们对 2D 图像颜色的影响。不影响图像颜色的簇可以从渲染过程中排除。在本文的方法中，可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。在 3D-GS 渲染过程中，在当前视图渲染之前应用所提出的方法时，平均可以排除 63% 的 3D 高斯体，从而在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。</p><p>（4）方法在任务和性能上的表现：在 Mip-NeRF360 数据集上，所提出的方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。这些性能支持了本文的目标，即快速且高质量地渲染 3D 场景。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于聚类的方案，通过识别不影响 2D 图像颜色的簇来排除当前视图渲染过程中的不必要 3D 高斯体。该方法平均可以排除 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。所提出的加速器还实现了与 GPU 相比 10.7 倍的加速。（2）：创新点：</li><li>提出了一种基于聚类的方案来识别不必要的 3D 高斯体。</li><li>该方法可以离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li><li>所提出的加速器实现了与 GPU 相比 10.7 倍的加速。性能：</li><li>在 Mip-NeRF360 数据集上，该方法平均排除了 63% 的 3D 高斯体，在不牺牲峰值信噪比 (PSNR) 的情况下将整体渲染计算减少了近 38.3%。工作量：</li><li>该方法可以在离线执行聚类和计算簇的半径，并且仅在实时执行将簇投影到 2D 图像平面上，这仅需 6.2% 的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting"><a href="#GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting" class="headerlink" title="GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting"></a>GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting</h2><p><strong>Authors:Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</strong></p><p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.10259v2">PDF</a> Project page: <a href="https://gaussianobject.github.io/">https://gaussianobject.github.io/</a></p><p><strong>摘要</strong><br>利用仅有 4 张输入图像，以高斯散点图表示和渲染三维对象，展现出极佳的渲染质量。</p><p><strong>要点</strong></p><ul><li>重建和渲染高度稀疏视图的 3D 对象对于促进 3D 视觉技术应用和改善用户体验至关重要。</li><li>提出 GaussianObject，一种以高斯散点图表示和渲染 3D 对象的框架，仅需 4 张输入图像即可实现高渲染质量。</li><li>引入视觉外壳和浮子消除技术，将结构先验明确注入初始优化过程，帮助建立多视图一致性，产生粗糙的 3D 高斯表示。</li><li>基于扩散模型构建高斯修复模型，以补充省略的对象信息，其中高斯值进一步细化。</li><li>设计了一种自生成策略来获取图像对，以训练修复模型。</li><li>在多个具有挑战性的数据集上评估了 GaussianObject，包括 MipNeRF360、OmniObject3D 和 OpenIllumination，仅使用 4 个视图即可实现强大的重建结果，并且明显优于先前的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯对象：只需四张图像即可获取高质量的 3D 对象</li><li>作者：陈阳，李思宽，方杰民，梁若凡，谢凌希，张晓鹏，沈巍，田齐</li><li>单位：上海交通大学</li><li>关键词：神经辐射场、3D 重建、稀疏视图、高斯球面体</li><li>论文链接：https://arxiv.org/abs/2402.10259，Github 链接：None</li><li><p>摘要：（1）研究背景：重建和渲染 3D 对象是计算机视觉领域的重要课题，但传统方法通常需要大量视图才能获得高质量的结果。这对于用户来说非常繁琐，限制了 3D 技术的广泛应用。（2）过去的方法：一些研究尝试减少对密集捕获的依赖，但当视图变得极度稀疏时，仍然难以生成高质量的 3D 对象。主要挑战在于难以建立多视图一致性，以及部分缺失或高度压缩的对象信息。（3）研究方法：本文提出了一种名为高斯对象的新框架，旨在从稀疏视图中重建高质量的 3D 对象。该框架使用 3D 高斯球面体作为基本表示，并设计了几种技术来引入对象结构先验，帮助建立多视图一致性。此外，还提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。（4）性能表现：高斯对象方法在几个具有挑战性的真实世界数据集上表现出强大的性能，在定性和定量评估中均优于以前的最先进方法。这表明该方法能够有效地从稀疏视图中重建高质量的 3D 对象。</p></li><li><p>方法：(1) 高斯球面体表示：将3D对象表示为一个3D高斯球面体，该球面体由一系列3D高斯分布组成。每个高斯分布对应于对象的一个局部区域，其参数（中心位置、尺度和权重）由神经网络学习得到。(2) 结构先验引入：设计了几种技术来引入对象结构先验，帮助建立多视图一致性。这些技术包括：</p><ul><li>形状正则化：使用一个预训练的形状生成模型来正则化高斯球面体的形状，使其更加真实和自然。</li><li>拓扑正则化：使用一个拓扑生成模型来正则化高斯球面体的拓扑结构，使其更加连通和完整。</li><li>语义正则化：使用一个语义分割模型来正则化高斯球面体的语义信息，使其更加准确和一致。(3) 高斯修复模型：提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。该模型通过迭代地扩散和恢复高斯球面体的参数，逐步消除伪影并生成高质量的3D对象。</li></ul></li><li><p>结论：（1）：高斯对象是一种新颖的框架，旨在从极度稀疏的 360° 视图中重建高质量的 3D 对象，该框架基于 3D 高斯球面体，并具有实时的渲染能力。我们设计了两种主要方法来实现这一目标：辅助结构先验的优化，以促进多视图一致性的构建，以及高斯修复模型，以去除由遗漏或高度压缩的对象信息引起的伪影。我们希望高斯对象能够推进重建 3D 对象的日常应用。（2）：创新点：</p></li><li>提出了一种新的 3D 对象表示形式——高斯球面体，该表示形式能够有效地捕获对象的形状、拓扑结构和语义信息。</li><li>设计了几种技术来引入对象结构先验，帮助建立多视图一致性，包括形状正则化、拓扑正则化和语义正则化。</li><li>提出了一种基于扩散模型的高斯修复模型，以消除由缺失或高度压缩的对象信息引起的伪影。性能：</li><li>在几个具有挑战性的真实世界数据集上，高斯对象方法在定性和定量评估中均优于以前的最先进方法。</li><li>高斯对象方法能够从极度稀疏的 360° 视图中重建高质量的 3D 对象，这对于用户来说非常方便，并且可以广泛应用于各种领域。工作量：</li><li>高斯对象方法需要大量的训练数据，这可能会增加训练时间和成本。</li><li>高斯对象方法需要使用神经网络来学习高斯球面体的参数，这可能会增加计算复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ec0859f0d4156531b928896ce0f20711.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6cf586e290dad38d6317bf5e32650f6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc6b9cc2318a136451091ab1f1c68efb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0ee843ee1e2c5a9e509cc05d4936f7f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6acbb2bc7ce290268eb48c8af2cb6b.jpg" align="middle"></details><h2 id="GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering"><a href="#GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering" class="headerlink" title="GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering"></a>GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering</h2><p><strong>Authors:Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</strong></p><p>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> . </p><p><a href="http://arxiv.org/abs/2402.10128v1">PDF</a> preprint</p><p><strong>摘要</strong><br>广义指数散列法（GES）是一种新颖的 3D 场景表示方法，它使用广义指数函数 (GEF) 对 3D 场景进行建模，从而显著减少了表示场景所需的粒子数量，比高斯散列方法更加高效，并且即插即用，可以替代基于高斯的工具。</p><p><strong>要点</strong></p><ul><li>GES 使用广义指数函数 (GEF) 对 3D 场景进行建模，显著减少了所需粒子数量，提高了效率。</li><li>GES 优于高斯散列法，能够将 3D 场景建模为更少的粒子，在效率方面显著优于高斯散列法。</li><li>GES 在原理性的一维设置和现实的 3D 场景中经过理论和经验验证。</li><li>GES 在表达具有清晰边缘的信号方面更准确，而这些信号通常对高斯函数构成挑战，因其本身具有低通特性。</li><li>GES 在拟合自然发生的信号（例如正方形、三角形和抛物线信号）方面优于高斯函数，因而减少了增加高斯散列法的内存占用的大量分裂操作的需要。</li><li>使用调制频率损失，GES 可实现在新视图合成基准中具有竞争力的性能，同时所需的存储空间不到高斯散列法的二分之一，并使渲染速度提高多达 39%。</li><li>GES 的代码可在项目网站 <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GES：用于高效光场渲染的广义指数散射（中文翻译）</li><li>作者：Abdullah Hamdi、Luke Melas-Kyriazi、Guocheng Qian、Jinjie Mai、Ruoshi Liu、Carl Vondrick、Bernard Ghanem、Andrea Vedaldi</li><li>第一作者单位：牛津大学视觉几何组（中文翻译）</li><li>关键词：3D 重建、3D 生成、3D 表示、光场渲染、广义指数函数</li><li>论文链接：https://arxiv.org/abs/2402.10128，Github 代码链接：无</li><li>摘要：（1）研究背景：3D 高斯散射在 3D 重建和生成方面取得了重大进展。然而，它可能需要大量高斯函数，这会造成巨大的内存占用。（2）过去的方法及其问题：高斯散射方法假设场景信号是低通的，但大多数 3D 场景都包含形状和外观上的突变，因此高斯散射需要使用大量非常小的高斯函数来表示这些 3D 场景，这会对内存利用率产生负面影响。（3）本文提出的研究方法：本文提出 GES（广义指数散射），它使用广义指数函数（具有额外的可学习形状参数）来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。（4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。</li></ol><p><methods>:(1): GES使用广义指数函数（具有额外的可学习形状参数）来建模3D场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。(2): GES在原理性1D设置和逼真的3D场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。(3): 实证分析表明，GES在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。(4): 在频率调制损失的帮助下，GES在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了39%。</methods></p><ol><li>结论：(1): 本文提出了一种新的光场渲染方法 GES，它使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法，并且可以即插即用地替换基于高斯的实用工具。(2): 创新点：</li><li>GES 使用广义指数函数来建模 3D 场景，从而可以减少表示场景所需的粒子数量，从而在效率上明显优于高斯散射方法。</li><li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li><li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li><li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。性能：</li><li>GES 在原理性 1D 设置和逼真的 3D 场景中都得到了理论和经验验证。结果表明，它可以更准确地表示具有锐利边缘的信号，而这对于高斯函数来说通常具有挑战性，因为它们具有固有的低通特性。</li><li>实证分析表明，GES 在拟合自然出现的信号（例如，正方形、三角形、抛物线信号）方面优于高斯函数，从而减少了增加高斯散射内存占用率的广泛分裂操作的需要。</li><li>在频率调制损失的帮助下，GES 在新视图合成基准测试中取得了具有竞争力的性能，同时所需的内存存储量不到高斯散射的一半，并且渲染速度提高了 39%。工作量：</li><li>GES 的实现相对简单，并且可以很容易地集成到现有的光场渲染工具中。</li><li>GES 的训练过程相对较快，并且可以在几分钟内完成。</li><li>GES 的渲染速度很快，并且可以在几秒钟内生成高质量的图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-06e50cf8fcf2b71cc6d5f5fa60bd416c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e0387aa41ca3382d21ca4822a1185a81.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d98ce6f15593a9709f1a7d0a0c108a7f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4903d39957be51dd29a4222bcccefaa4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c50bfcbaec1420bcb70374001db6c443.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e090b0178d5a97f88600cc386571b770.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v2">PDF</a> The content of the technical report needs to be updated and retracted   to avoid other impacts</p><p><strong>摘要</strong><br>利用3DGS(三维高斯渲染)增强3D表现，以进行多模态预训练，提升图像、语言和三维数据的对齐，改善物体识别、分类和检索任务。</p><p><strong>要点</strong></p><ul><li>利用点云表示的3D形状在图像和语言描述的对齐上取得了多模态预训练的进步，这对于物体识别、分类和检索至关重要。</li><li>点云的离散表示丢失了物体的曲面形状信息，并在渲染结果和2D对应关系之间制造差距。</li><li>提出GS-CLIP，首次尝试将3DGS（三维高斯渲染）引入多模态预训练，以增强3D表示。</li><li>GS-CLIP利用预训练的视觉-语言模型，在大量真实世界图像-文本对上学习一个通用的视觉和文本空间，然后学习一个针对每个物体优化3DGS的3D编码器。</li><li>提出了一种新的高斯感知融合来提取和融合全局显式特征。</li><li>作为语言-图像-3D预训练的通用框架，GS-CLIP独立于3D骨干网络。</li><li>具有挑战性的实验表明，GS-CLIP显著优于最先进的技术，超越了以前最好的成果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯喷绘</li><li>作者：李昊源、周雁鹏、曾义涵、徐航、梁晓丹</li><li>第一作者单位：深圳大学</li><li>关键词：3D 表示、对比学习、多模态预训练、高斯喷绘</li><li>论文链接：https://arxiv.org/abs/2402.06198，Github 代码链接：无</li><li><p>摘要：（1）研究背景：3D 形状表示为点云在多模态预训练中取得了进展，可以对齐图像和语言描述，这对物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生了差距。（2）过去的方法及其问题：现有方法主要集中在对点云进行建模，但这些方法通常会丢失物体的几何信息和形状纹理。此外，现有方法通常需要大量的数据，这使得它们难以应用于现实世界中的场景。（3）提出的研究方法：为了解决上述问题，本文提出了一种新的框架 GS-CLIP，该框架将 3D 高斯喷绘 (3DGS) 引入多模态预训练中，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在大量真实世界图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合方法，用于提取和融合全局显式特征。（4）方法在任务和性能上的表现：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。</p></li><li><p>方法：（1）跨模态预训练：利用预训练的语言-图像模型CLIP，为文本、图像和3DGS建立共同的语言-图像潜在空间，作为3DGS的目标潜在空间。（2）语言-3DGS对齐和图像-3DGS对齐：分别使用对比损失函数来对齐文本与3DGS、图像与3DGS的特征表示。（3）高斯感知融合：采用基于Transformer的分支直接对高斯特征进行建模，并将其与残差形式注入到3D主干网络中。</p></li><li><p>结论：（1）：本工作首次将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便更好地从补充信息中学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能，并在真实世界环境中实现了零样本/开放世界学习的最新性能。（2）：创新点：</p></li><li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li><li>提出了一种高斯感知融合，以便更好地从补充信息中学习信息。</li><li>在真实世界环境中实现了零样本/开放世界学习的最新性能。性能：</li><li>在 SUN-RGBD 数据集上，GS-CLIP 在真实世界环境中的零样本/开放世界学习中取得了最先进的性能。</li><li>这些结果表明，3DGS 在跨模态学习中具有强大的表示能力。工作量：</li><li>该工作涉及到大量的数据预处理和模型训练。</li><li>需要对 3DGS 进行优化，以使其能够更好地对齐文本和图像的特征表示。</li><li>需要对高斯感知融合进行进一步的研究，以使其能够更好地提取和融合全局显式特征。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details><h2 id="GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting"><a href="#GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting" class="headerlink" title="GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting"></a>GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting</h2><p><strong>Authors:Joanna Waczyńska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</strong></p><p>Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS. </p><p><a href="http://arxiv.org/abs/2402.01459v3">PDF</a> </p><p><strong>Summary:</strong><br>神经辐射场 (NeRF) 是一种用于图像渲染的神经网络方法，而高斯网格泼溅 (GaMeS) 模型则通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</p><p><strong>Key Takeaways:</strong></p><ul><li>利用神经网络表征 3D 场景的 NeRF，允许从少量 2D 图像中进行逼真的视点合成。</li><li>高斯泼溅 (GS) 通过高斯分布来估算 3D 场景中点的贡献，从而实现快速训练和实时渲染。</li><li>GaMeS 模型允许以与网格类似的方式修改高斯分量，从而为 GS 的调节提供了一个明确的方法。</li><li>将每个高斯分量参数化为网格面的顶点，这使得 GaMeS 模型可以对 GS 进行实时渲染。</li><li>GaMeS 模型需要在输入时初始化网格或在训练期间估计网格。</li><li>根据其在网格上的位置定义高斯泼溅，从而允许在动画期间自动调整位置、缩放和旋转。</li><li>GaMeS 模型可以实现可编辑 GS 的实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GaMeS：基于网格的自适应和修改高斯喷绘</li><li>作者：Joanna Waczyńska、Piotr Borycki、Sławomir Tadeja、Jacek Tabor、Przemysław Spurek</li><li>第一作者单位：雅盖隆大学数学与计算机科学学院，波兰克拉科夫</li><li>关键词：高斯喷绘、神经辐射场、神经渲染、网格、实时渲染</li><li>论文链接：https://arxiv.org/abs/2402.01459，Github 代码链接：无</li><li>摘要：(1)：研究背景：近年来，基于神经网络的图像渲染方法取得了很大进展，其中神经辐射场（NeRF）是一种流行的方法，它使用神经网络来表示 3D 场景，并能够从少量 2D 图像中合成逼真的视图。然而，大多数 NeRF 模型都受到训练和推理时间长的限制。与之相比，高斯喷绘（GS）是一种新颖的、最先进的技术，它通过高斯分布来近似点对图像像素的贡献，从而渲染 3D 场景中的点，具有快速训练和快速实时渲染的能力。(2)：过去的方法和问题：GS 的一个缺点是缺乏明确的调节方法，因为需要调节数十万个高斯分量。为了解决这个问题，本文介绍了高斯网格喷绘（GaMeS）模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。因此，我们获得了可编辑 GS 的实时渲染。(3)：研究方法：我们提出了 GaMeS 模型，它允许像修改网格一样修改高斯分量。我们将每个高斯分量参数化为网格面的顶点。此外，我们的模型需要在输入或训练期间估计的网格上进行网格初始化。我们还定义了仅基于其在网格上的位置的高斯喷绘，允许在动画期间自动调整位置、比例和旋转。(4)：方法的性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）GaMeS 允许实时修改，但对于具有大面的网格，在发生重大变化的情况下会出现伪影。在实践中，大面应该被分成更小的面。当网格面分裂时如何在 GaMeS 中更改高斯分量尚不清楚。（2）创新点：GaMeS 提出了一种新的基于网格的自适应和修改高斯喷绘模型，该模型允许像修改网格一样修改高斯分量，从而实现了实时修改和适应高斯喷绘。性能：实验结果表明，GaMeS 模型能够在保持高质量渲染的同时，实现实时修改和适应高斯喷绘。这使得 GaMeS 成为交互式应用程序和游戏中的一个有前景的技术。工作量：GaMeS 模型需要在输入或训练期间估计网格，这可能会增加模型的训练和推理时间。此外，GaMeS 模型需要对网格进行修改，这可能会增加模型的修改时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-11676aa94eeb837bc5149bf9038274ae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3c20ac78640d356ea03699146c96e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4070017cd795fd8699e30a356efae899.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0416310a796f7ec70150342ac59ffe37.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6eb0975a0f5d702a6daef3f78e530869.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fb0edd088d9a64e792369a6d6a72979.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dd54f927f26f28fdcefe778d566087c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/Diffusion%20Models/</id>
    <published>2024-02-22T17:19:45.000Z</published>
    <updated>2024-02-22T17:19:45.965Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-23-更新"><a href="#2024-02-23-更新" class="headerlink" title="2024-02-23 更新"></a>2024-02-23 更新</h1><h2 id="Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation"><a href="#Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation" class="headerlink" title="Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation"></a>Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation</h2><p><strong>Authors:Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo</strong></p><p>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control). </p><p><a href="http://arxiv.org/abs/2402.13729v1">PDF</a> 17 pages, 13 figures</p><p><strong>Summary</strong><br>混合视频扩散模型能够有效地捕获视频的时空依赖性，从而生成高质量和逼真的视频。</p><p><strong>Key Takeaways</strong></p><ul><li>现有基于扩散的方法通过使用传统的视频自动编码器架构将视频压缩到低维潜在空间，实现了可比的性能。</li><li>标准帧级 2D 和 3D 卷积无法充分利用视频的时空性质。</li><li>提出了一种新的混合视频扩散模型 HVDM，可以更有效地捕捉时空相关性。</li><li>HVDM 由混合视频自动编码器训练，该编码器提取视频的解耦表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。</li><li>基于这种解耦表示，提出的混合自动编码器提供了更全面的视频潜在变量，丰富了生成视频的精细结构和细节。</li><li>在视频生成基准（UCF101、SkyTimelapse 和 TaiChi）上进行的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：具有 2D 三平面和 3D 小波表示的混合视频扩散模型</li><li>作者：Tianhan Wang、Junyu Dong、Xiaolong Wang、Yibing Song、Yezhou Yang、Kun Zhou、Jiayi Ma</li><li>隶属关系：华中科技大学</li><li>关键词：视频生成、扩散模型、视频表示、小波变换</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：(1)：视频生成是一项具有挑战性的任务，需要生成具有复杂且高维度的逼真视频。最近的一些基于扩散的方法通过使用传统的视频自动编码器架构将视频压缩到更低维度的潜在空间，显示出可比的性能。但是，采用标准帧级 2D 和 3D 卷积的方法未能充分利用视频的时空性质。(2)：为了解决这个问题，我们提出了一种新颖的混合视频扩散模型，称为 HVDM，它可以更有效地捕获时空依赖性。HVDM 由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。(3)：在视频生成基准（UCF101、SkyTimelapse 和 TaiChi）上的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。(4)：我们的方法在 UCF101、SkyTimelapse 和 TaiChi 数据集上实现了最先进的视频生成质量。在 UCF101 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.8% 和 0.011。在 SkyTimelapse 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 3.2% 和 0.012。在 TaiChi 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.9% 和 0.010。这些结果支持了我们的目标，即生成具有更高质量和更丰富细节的视频。</p></li><li><p>Methods:(1): 我们提出了一种新的混合视频扩散模型HVDM，它可以更有效地捕获时空依赖性。(2): HVDM由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由2D投影潜在变量捕获的全局上下文信息、由具有小波分解的3D卷积捕获的局部体积信息以及用于改进视频重建的频率信息。(3): 基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。</p></li><li><p>结论：（1）：本文提出了一种用于视频生成的混合视频自动编码器，称为 HVDM，该方法可以更有效地捕获时空依赖性。HVDM 由一个混合视频自动编码器训练，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。基于这种纠缠表示，我们的混合自动编码器提供了一个更全面的视频潜在变量，丰富了生成视频的精细结构和细节。在 UCF101、SkyTimelapse 和 TaiChi 基准数据集上的实验表明，所提出的方法实现了最先进的视频生成质量，展示了广泛的视频应用（例如，长视频生成、图像到视频和视频动态控制）。（2）：创新点：</p></li><li>提出了一种新的混合视频扩散模型 HVDM，该模型可以更有效地捕获时空依赖性。</li><li>提出了一种混合视频自动编码器，该自动编码器提取视频的纠缠表示，包括：由 2D 投影潜在变量捕获的全局上下文信息、由具有小波分解的 3D 卷积捕获的局部体积信息以及用于改进视频重建的频率信息。</li><li>通过结合这些表示与时空交叉注意力，HVDM 可以生成具有改进的真实感的高质量视频。性能：</li><li>在 UCF101、SkyTimelapse 和 TaiChi 基准数据集上的实验表明，所提出的方法实现了最先进的视频生成质量。</li><li>在 UCF101 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.8% 和 0.011。</li><li>在 SkyTimelapse 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 3.2% 和 0.012。</li><li>在 TaiChi 数据集上，我们的方法在 FID 和 MS-SSIM 度量上分别比最先进的方法提高了 2.9% 和 0.010。工作量：</li><li>该方法需要设计和训练一个混合视频自动编码器，该自动编码器可以提取视频的纠缠表示。</li><li>该方法需要设计和训练一个时空交叉注意力机制，该机制可以将混合视频自动编码器的表示融合起来，生成高质量的视频。</li><li>该方法需要在多个视频生成基准数据集上进行实验，以评估其性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b0561ef07a60189b28853dc0eda76ddf.jpg" align="middle"><img src="https://pica.zhimg.com/v2-851a92656b32ae2990dcf703193d622b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-63e056db347f6648afdcaf392f094dd6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c9f03009913498a6d9d199e594d8e64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2313ec6324cb296d16788788f949eec.jpg" align="middle"></details><h2 id="ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images"><a href="#ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images" class="headerlink" title="ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images"></a>ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images</h2><p><strong>Authors:Ethan Smith, Nayan Saxena, Aninda Saha</strong></p><p>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity. </p><p><a href="http://arxiv.org/abs/2402.13573v1">PDF</a> </p><p><strong>Summary</strong><br>改进稳定扩散注意机制以提高推理速度。</p><p><strong>Key Takeaways</strong></p><ul><li>注意力机制在图像扩散模型中很重要，但其二次计算复杂度限制了我们在合理的时间和内存限制内可以处理的图像大小。</li><li>生成图像模型通常包含冗余特征，适合稀疏注意力机制。</li><li>提出了一种新颖的免训练方法 ToDo，它依赖于键和值标记的标记降采样，从而将 Stable Diffusion 推理速度提高了 2 倍（常见大小）和 4.5 倍或更多（2048x2048 等高分辨率）。</li><li>ToDo 在平衡有效吞吐量和保真度方面优于以前的方法。</li><li>ToDo 是一个免费且易于实现的方法，可以应用于任何基于注意力的扩散模型。</li><li>ToDo 的推理速度比现有的最先进方法快，同时还能保持良好的图像质量。</li><li>ToDo 可以让图像扩散模型在更大的图像上进行训练和推理，从而提高图像质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ToDo：令牌降采样以高效生成高分辨率图像</li><li>作者：Ethan Smith、Nayan Saxena、Aninda Saha</li><li>第一作者单位：Leonardo AI</li><li>关键词：图像生成、扩散模型、注意机制、令牌降采样、计算效率</li><li>论文链接：https://arxiv.org/abs/2402.13573、Github 代码链接：无</li><li>摘要：（1）研究背景：注意机制是图像扩散模型成功的关键因素，但其二次计算复杂度限制了图像处理的大小。本文研究了生成图像模型中的密集注意机制，提出了一种无需训练的令牌降采样方法 ToDo，可加速 Stable Diffusion 推理，在常见尺寸下提速 2 倍，在 2048×2048 等高分辨率下提速 4.5 倍以上。（2）过去方法及其问题：过去的稀疏注意方法通常需要训练时修改，增加了优化开销。注意近似方法虽然不需要训练，但通常需要预训练。（3）研究方法：本文提出的 ToDo 方法是一种后处理方法，通过对注意机制中的键和值令牌进行降采样来加速推理，无需修改模型或重新训练。（4）性能表现：ToDo 方法在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。</li></ol><p>Methods:(1): 本文提出了一种称为ToDo的令牌降采样方法，用于加速StableDiffusion推理。(2): ToDo方法通过对注意机制中的键和值令牌进行降采样来加速推理，无需修改模型或重新训练。(3): ToDo方法采用了一种优化的令牌合并策略，该策略利用了图像令牌固有的空间邻近性。(4): ToDo方法还引入了一种改进的注意力机制，该机制将降采样操作应用于注意机制中的键和值，同时保留原始查询。(5): ToDo方法在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。</p><ol><li>结论：（1）：本文提出的 ToDo 方法在保持计算效率和保真度之间取得了很好的平衡，尤其是在高频分量上。我们还表明，U-Net 中的相邻特征可能是冗余的，并假设我们的方法可以使其他基于注意力的生成图像模型受益，尤其是那些在大量令牌上运行的模型。未来的工作可以探索我们方法的可微分性，并利用它来有效地微调 StableDiffusion，使其在以前未见过的更大的图像尺寸上运行。（2）：创新点：提出了一种称为 ToDo 的令牌降采样方法，用于加速 StableDiffusion 推理。性能：在各种任务和性能指标上都优于以往方法，在平衡计算效率和保真度方面表现出色。工作量：无需修改模型或重新训练，采用了一种优化的令牌合并策略，该策略利用了图像令牌固有的空间邻近性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b29b6788a3c63bf19060ac13a17491fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-588f50850c143462d31aee32d4aec168.jpg" align="middle"></details><h2 id="Visual-Style-Prompting-with-Swapping-Self-Attention"><a href="#Visual-Style-Prompting-with-Swapping-Self-Attention" class="headerlink" title="Visual Style Prompting with Swapping Self-Attention"></a>Visual Style Prompting with Swapping Self-Attention</h2><p><strong>Authors:Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh</strong></p><p>In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a>. </p><p><a href="http://arxiv.org/abs/2402.12974v2">PDF</a> </p><p><strong>Summary</strong><br>使用风格样式提示获取更准确匹配文本提示的图像</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在文本到图像生成领域中表现出强大，但它们在保持一致风格的受控生成方面仍然存在挑战，需要昂贵的微调或由于内容泄漏而无法充分地再现视觉元素。</li><li>提出了一种新颖的方法，\ours，可以在保持特定风格元素和细微差别的情况下生成各种图像。</li><li>在去噪过程中，我们在最后的自我注意层中把原始特征中的查询保持不变，同时用参考特征的键和值进行交换。</li><li>这种方法允许在无需微调的情况下进行视觉风格提示，确保生成的图像保持忠实的风格。</li><li>通过在各种风格和文本提示下的广泛评估，我们的方法证明了优于现有方法，最能反映参考文献的风格，并确保生成的图像最准确地匹配文本提示。</li><li>我们的项目页面可以在 <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：视觉风格提示与交换自我注意力</li><li>作者：Jongwook Choi, Kyumin Lee, Jun-Ho Kim</li><li>第一作者单位：韩国科学技术院</li><li>关键词：扩散模型、文本到图像生成、视觉风格提示、交换自我注意力</li><li>论文链接：https://arxiv.org/abs/2302.08551，Github 链接：无</li><li>摘要：(1) 研究背景：扩散模型在文本到图像生成领域取得了显著进展，但仍面临着在保持一致风格的同时实现可控生成的挑战，需要昂贵的微调或由于内容泄漏而导致视觉元素转移不足。(2) 过去的方法及其问题：现有方法通常通过微调或使用预训练的模型来实现视觉风格提示，但这些方法存在成本高昂、风格转移不足或内容泄漏等问题。(3) 本文提出的研究方法：本文提出了一种新的方法——视觉风格提示，通过在去噪过程中保留原始特征的查询，同时在最后的自注意力层中用参考特征交换键和值，来实现视觉风格提示。这种方法不需要任何微调，并确保生成的图像保持忠实风格。(4) 方法在任务和性能上的表现：本文的方法在各种风格和文本提示下的评估中表现出优越性，在反映参考风格和确保生成图像最准确地匹配文本提示方面优于现有方法。这些性能支持了本文的目标，即实现具有特定风格元素和细微差别的图像生成。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种新的视觉风格提示方法，该方法无需微调，并确保生成的图像保持忠实风格。（2）：创新点：本方法创新性地提出了一种新的视觉风格提示方法，该方法通过在去噪过程中保留原始特征的查询，同时在最后的自注意力层中用参考特征交换键和值，来实现视觉风格提示。性能：本方法在各种风格和文本提示下的评估中表现出优越性，在反映参考风格和确保生成图像最准确地匹配文本提示方面优于现有方法。工作量：本方法的工作量相对较低，不需要任何微调，并且可以很容易地应用于现有的扩散模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ca682f6681ca2aea4fdb5980de4dc8f4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d771e643cabdf04390bb34c56e1d306.jpg" align="middle"><img src="https://picx.zhimg.com/v2-11f4ff0d9aeecd7bd560b037f6d9c569.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6b333a460ba441d80a537e0874e7628a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6ef6e8248b60241a24705f590a653e38.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4422e0b37dd7515345602877f9ea3a62.jpg" align="middle"></details><h2 id="CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection"><a href="#CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection" class="headerlink" title="CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection"></a>CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection</h2><p><strong>Authors:Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</strong></p><p>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools. </p><p><a href="http://arxiv.org/abs/2402.12927v1">PDF</a> </p><p><strong>摘要</strong><br>CLIP模型结合文本和视觉信息，在深度伪造检测任务上取得了优异的性能，优于仅使用视觉信息的SOTA方法。</p><p><strong>要点</strong></p><ul><li>CLIP模型在与最近的通用深度伪造检测适应方法配对时，在深度伪造检测任务上表现出良好的有效性。</li><li>只需使用一个数据集（ProGAN）就可以对CLIP进行改编，以实现深度伪造检测。</li><li>保留CLIP模型的文本部分对于提高检测性能至关重要。</li><li>基于Prompt Tuning的简单且轻量级的适应策略在使用不到三分之一的训练数据（20万张图像，而之前的方法使用了72万张图像）的情况下，在mAP和准确率方面分别优于之前的SOTA方法5.01%和6.61%。</li><li>CLIP模型在对来自21个不同数据集的图像进行的全面评估中表现出良好的真实世界适用性，包括由基于GAN、基于扩散和商业工具生成的图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：剪辑欺骗：适应通用深度伪造检测的视觉语言模型</li><li>作者：Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</li><li>单位：卑尔根大学</li><li>关键词：深度伪造检测，迁移学习，视觉语言模型</li><li>论文链接：https://arxiv.org/abs/2402.12927，Github 链接：None</li><li>摘要：(1)：随着生成对抗网络 (GAN) 的最新进展和扩散模型的出现，高度逼真且广泛可访问的合成内容的制作变得更加容易。因此，迫切需要有效的通用检测机制来减轻深度伪造带来的潜在风险。(2)：在本文中，我们探索了在与最近的适应方法配对时预训练的视觉语言模型 (VLM) 在通用深度伪造检测中的有效性。遵循该领域的先前研究，我们仅使用单个数据集 (ProGAN) 来适应 CLIP 以进行深度伪造检测。然而，与仅依赖 CLIP 的视觉部分而忽略其文本组件的先前研究相比，我们的分析表明保留文本部分至关重要。因此，我们采用的简单轻量级 PromptTuning 基于适应策略在利用不到三分之一的训练数据（200k 图像，相比之下为 720k）的情况下，在 mAP 上优于之前的 SOTA 方法 5.01%，准确率提高 6.61%。为了评估我们提出的模型的实际适用性，我们对各种场景进行了综合评估。这涉及对来自 21 个不同数据集的图像进行严格测试，包括基于 GAN、基于扩散和商业工具生成的图像。(3)：我们提出了一种简单而有效的方法来适应 CLIP 以进行通用深度伪造检测。我们的方法基于 PromptTuning，这是一种轻量级且易于实现的适应策略。我们还表明，保留 CLIP 的文本部分对于提高检测性能至关重要。(4)：在 ProGAN 数据集上，我们的方法在 mAP 上实现了 95.21% 的准确率和 97.82% 的准确率，优于之前的 SOTA 方法。我们的方法还显示出良好的泛化能力，能够检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</li></ol><p>方法：</p><p>（1）线性探测：线性探测是一种将冻结模型（本例中为 CLIP）作为特征提取器，并在其上微调线性分类器的方法。我们遵循 Ojha 等人采用的相同方法。[32]，即我们丢弃 CLIP 的文本编码器并冻结其图像编码器。然后，我们在冻结的 CLIP 图像特征上训练一个用于分类的单层线性层，使用 Sigmoid 激活函数将倒数第二个图像特征映射到用于类别预测的逻辑值。优化使用二进制交叉熵损失进行。</p><p>（2）微调：微调在此上下文中意味着再次在用于下游数据集的整个 CLIP 模型（ViT-Large）上进行训练，在本例中是也被 [45] 和 [32] 使用的 ProGAN 数据集。完全微调需要显着更多的计算机资源、数据和训练时间，因为整个模型都经过了重新训练。此外，随着模型大小的增加，此策略表现出不稳定和效率低下 [26]。在训练模型时，我们遇到了这个问题，并通过使用极小的学习率 1×10-6 来缓解这个问题。为了微调我们的模型，我们遵循 CLIP 预训练中概述的程序 [37]。但是，我们引入了一个修改：不是对每个图像使用整个文本标题，我们只提供单个单词标题，具体来说是 real 或 fake。典型的用于调整 CLIP 的微调管道如图 2 所示。</p><p>（3）PromptTuning：PromptTuning 是一种通过调整文本提示来适应 CLIP 的方法。我们遵循 CoOp [50] 的方法，该方法使用 CLIP 的文本编码器生成一个提示，该提示可以指导图像编码器进行分类。我们使用单个单词提示 real 或 fake 来生成图像特征，然后使用这些特征来训练线性分类器。</p><p>（4）适配器网络：适配器网络是一种通过在预训练模型上添加小型网络来适应新任务的方法。我们使用一个适配器网络来调整 CLIP，该网络由一个卷积层和一个线性层组成。适配器网络将 CLIP 的图像特征作为输入，并输出一个用于分类的逻辑值。</p><ol><li>结论：（1）：本工作通过探索预训练的视觉语言模型 CLIP 在通用深度伪造检测中的有效性，展示了 CLIP 在检测来自各种数据分布的深度伪造图像方面的鲁棒性。我们使用来自 ProGAN 数据集的 200k 图像作为多样化的训练集，并比较了四种不同的迁移学习策略，包括微调、线性探测、PromptTuning 和训练适配器网络。我们的实验包括对包含 21 个不同图像生成器的综合测试集进行评估。在整个实验中，我们证明了结合 CLIP 的图像和文本组件的迁移学习策略始终优于仅使用 CLIP 视觉方面的简单方法（如线性探测）的性能。我们的研究结果凸显了 PromptTuning 优于当前基准和 SOTA 方法的优势，在展示其有效性的同时，即使训练参数最少，也能实现显着的改进幅度。此外，我们还进行了少量实验，分析了在 JPEG 压缩和高斯模糊等后处理操作下的鲁棒性，并证明了即使训练集规模较小（20k 图像），基于 CLIP 的检测器也具有稳定的性能。（2）：创新点：• 探索了预训练的视觉语言模型 CLIP 在通用深度伪造检测中的有效性。• 比较了四种不同的迁移学习策略，包括微调、线性探测、PromptTuning 和训练适配器网络。• 证明了结合 CLIP 的图像和文本组件的迁移学习策略始终优于仅使用 CLIP 视觉方面的简单方法（如线性探测）的性能。• PromptTuning 优于当前基准和 SOTA 方法，在展示其有效性的同时，即使训练参数最少，也能实现显着的改进幅度。• 分析了在 JPEG 压缩和高斯模糊等后处理操作下的鲁棒性，并证明了即使训练集规模较小（20k 图像），基于 CLIP 的检测器也具有稳定的性能。</li></ol><p>性能：• 在 ProGAN 数据集上，PromptTuning 在 mAP 上实现了 95.21% 的准确率和 97.82% 的准确率，优于之前的 SOTA 方法。• PromptTuning 在综合测试集上也表现出良好的泛化能力，能够检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</p><p>工作量：• PromptTuning 是一种简单而有效的方法，易于实现。• PromptTuning 只需要少量的数据和训练时间。• PromptTuning 可以用于检测来自各种来源的深度伪造，包括 GAN、扩散和商业工具。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-745a50bdee80b1df6d9da45abefcb26e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e6ec4d0ce05a2af6e93f8a2710069bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca30024b468b77b358f2f1058147b9e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f749a0d770c3a7267b5153b59c39032b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6f8430a1aafee1b2f88631389c9cdc32.jpg" align="middle"><img src="https://pica.zhimg.com/v2-05df037ca314f896a85f2bb5c514f5dd.jpg" align="middle"></details>## RealCompo: Dynamic Equilibrium between Realism and Compositionality   Improves Text-to-Image Diffusion Models**Authors:Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui**Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo [PDF](http://arxiv.org/abs/2402.12908v1) Project: https://github.com/YangLing0818/RealCompo**Summary**利用文本到图像模型和布局到图像模型的优势，提出了一种新的无训练和易于迁移的文本到图像生成框架 RealCompo，以增强生成图像的真实性和组合性。**Key Takeaways**- RealCompo 是一种新的无训练和易于迁移的文本到图像生成框架，旨在利用文本到图像模型和布局到图像模型的优势。- RealCompo 利用文本到图像模型生成逼真的图像，利用布局到图像模型生成合理的构图。- RealCompo 引入了新的平衡器，以动态平衡两个模型在去噪过程中的优势。- RealCompo 即插即用，无需额外训练即可使用任何模型。- RealCompo 在多对象组合生成方面始终优于最先进的文本到图像模型和布局到图像模型。- RealCompo 保持了生成图像的令人满意的真实性和组合性。- RealCompo 的代码可在 https://github.com/YangLing0818/RealCompo 获得。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：RealCompo：真实感与组合性的动态平衡可改进文本到图像扩散模型</li><li>作者：Xinchen Zhang<em>, Ling Yang</em>, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui</li><li>单位：清华大学</li><li>关键词：文本到图像生成、布局到图像生成、扩散模型、组合性、真实感</li><li>论文链接：https://github.com/YangLing0818/RealCompoGithub 代码链接：https://github.com/YangLing0818/RealCompo</li><li>摘要：（1）研究背景：扩散模型在文本到图像生成任务中取得了显著进展，但现有模型在处理多对象组合性生成时仍面临许多困难。（2）过去方法及其问题：现有方法包括文本到图像模型和布局到图像模型。文本到图像模型能够生成逼真的图像，但组合性较差；布局到图像模型能够控制对象的位置和数量，但真实感较差。（3）研究方法：本文提出了一种新的训练友好且可迁移的文本到图像生成框架 RealCompo，该框架旨在利用文本到图像模型和布局到图像模型的优势，同时提高生成图像的真实感和组合性。RealCompo 使用了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。（4）实验结果：广泛的实验表明，RealCompo 在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。这些性能支持了本文的目标。</li></ol><p>Methods:(1) 提出了一种新的训练友好且可迁移的文本到图像生成框架RealCompo，该框架旨在利用文本到图像模型和布局到图像模型的优势，同时提高生成图像的真实感和组合性。(2) 设计了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。(3) 分析了每个模型预测噪声的影响，并提供了一种计算系数的方法。(4) 提供了平衡器所采用的更新规则的详细解释，该规则利用了一种无训练方法来动态更新系数。(5) 扩展了RealCompo的应用，为L2I模型的每一类设计了损失函数。</p><ol><li>结论：（1）：本文提出了一种训练友好且可迁移的文本到图像生成框架RealCompo，该框架在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。（2）：创新点：提出了一个直观且新颖的平衡器，可以在去噪过程中动态平衡两个模型的强度，从而允许即插即用任何模型而无需额外训练。分析了每个模型预测噪声的影响，并提供了一种计算系数的方法。提供了平衡器所采用的更新规则的详细解释，该规则利用了一种无训练方法来动态更新系数。扩展了RealCompo的应用，为L2I模型的每一类设计了损失函数。性能：RealCompo在多对象组合性生成任务上始终优于最先进的文本到图像模型和布局到图像模型，同时保持了生成图像令人满意的真实感和组合性。RealCompo可以被推广到任何LLM、T2I和L2I模型，并保持强大的生成结果。工作量：RealCompo的实现相对简单，易于使用。RealCompo可以轻松地集成到现有的文本到图像生成系统中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-264ae173bcca3292815b8e45db353de6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c5f244037ff17e98afe9f2c1851e4f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caea4b22ae09f52bc515627d4e3cba84.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5fcdadd1b307e5df492d508f86958e6.jpg" align="middle"></details><h2 id="Two-stage-Rainfall-Forecasting-Diffusion-Model"><a href="#Two-stage-Rainfall-Forecasting-Diffusion-Model" class="headerlink" title="Two-stage Rainfall-Forecasting Diffusion Model"></a>Two-stage Rainfall-Forecasting Diffusion Model</h2><p><strong>Authors:XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang</strong></p><p>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}</a>. </p><p><a href="http://arxiv.org/abs/2402.12779v1">PDF</a> </p><p><strong>Summary</strong><br>利用两阶段降雨预测扩散模型（TRDM）提高降雨预测的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>TRDM是一种用于降雨预测任务的两阶段方法。</li><li>TRDM的第一阶段任务是在低分辨率条件下捕获稳健的时间信息，同时保留空间信息。</li><li>TRDM的第二阶段任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。</li><li>TRDM在MRMS和瑞典雷达数据集上展示了最先进的结果。</li><li>TRDM开源，可在 GitHub 上获取：\href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}。">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：两阶段降雨预测扩散模型</li><li>作者：Xu DongLing, Chao RongLi*, FengQing Qin, LiHong Zhu, Yuanyuan Huang</li><li>第一作者单位：重庆理工大学人工智能与大数据学院</li><li>关键词：扩散模型、生成对抗网络、降雨预测</li><li>论文链接：https://arxiv.org/abs/2402.12779，Github 代码链接：https://github.com/clearlyzerolxd/TRDM</li><li><p>总结：（1）研究背景：深度神经网络在降雨预测领域取得了很大的成就。然而，现有的预测方法存在一定的局限性，例如生成的图像模糊、空间位置不准确等。（2）以往方法：针对上述问题，已有研究提出了卷积LSTM和卷积GRU模型来提高降雨预测的准确性。然而，这些方法在长期的预测中存在准确性不高的问题。此外，SmaAt-UNet模型虽然能够有效地捕捉输入序列中的关键空间信息，但在长期预测性能方面仍有待提高。（3）研究方法：为了解决上述问题，本文提出了一种两阶段降雨预测扩散模型（TRDM）。TRDM是一个两阶段的降雨预测方法。第一阶段的任务是在低分辨率条件下捕获鲁棒的时间信息，同时保留空间信息。第二阶段的任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。（4）方法性能：在MRMS和瑞典雷达数据集上，TRDM取得了最先进的结果。</p></li><li><p>方法：(1) 预测扩散模型：利用三维卷积神经网络生成 16 帧 32×32 低分辨率降雨结果，同时保留一定程度的空间信息，为后续重建阶段提供鲁棒的基础。(2) 空间超分辨率：使用扩散模型构建超分辨率网络，将低分辨率图像重建为高分辨率图像，增强图像质量和细节，以便更准确地分析未来降雨的强度和分布。(3) 潜在超分辨率：提出一种潜在超分辨率方法，将高分辨率图像编码为潜在空间，然后利用扩散模型生成条件，指导生成条件。(4) 模型训练：使用 L1 损失函数训练预测扩散模型和超分辨率模型，以最小化预测误差。(5) 模型推理：在推理过程中，将低分辨率图像输入到超分辨率模型中，并使用扩散模型生成条件，逐步恢复高分辨率图像。</p></li><li><p>结论：（1）：本文提出了一种两阶段降雨预测扩散模型（TRDM），该模型能够有效地捕捉输入序列中的关键空间信息，并在长期预测性能方面取得了最先进的结果。（2）：创新点：TRDM模型采用了两阶段的预测策略，第一阶段的任务是在低分辨率条件下捕获鲁棒的时间信息，同时保留空间信息。第二阶段的任务是将第一阶段生成的低分辨率图像重建为高分辨率图像。这种两阶段的策略能够有效地提高降雨预测的准确性。性能：在MRMS和瑞典雷达数据集上，TRDM模型取得了最先进的结果。与其他方法相比，TRDM模型能够生成更加清晰和准确的降雨预测图像。工作量：TRDM模型的训练和推理过程相对简单。该模型只需要少量的训练数据，并且训练时间较短。在推理过程中，TRDM模型能够快速地生成降雨预测图像。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-77f75079fa9cf15e6ab90ae9bfdf3659.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e40db6d053eb3ccf707a2dbcd4cf2e8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1f5823da8ecb8e38058c288533b8775e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf44de1da53f2ab1acf3c0d8075ec068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b36e8a07f0692df0799659af074a0a49.jpg" align="middle"><img src="https://pica.zhimg.com/v2-56d34d3e7c52a330e5782ff67a0df331.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4bfaafb452921e1d0c1a1d6c62510229.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5fa91d38aada7882b2ac95950348567d.jpg" align="middle"></details><h2 id="MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion"><a href="#MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion" class="headerlink" title="MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"></a>MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</h2><p><strong>Authors:Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou</strong></p><p>Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on <a href="https://github.com/measure-infinity/mulan-code">https://github.com/measure-infinity/mulan-code</a>. </p><p><a href="http://arxiv.org/abs/2402.12741v1">PDF</a> Project website: <a href="https://measure-infinity.github.io/mulan">https://measure-infinity.github.io/mulan</a></p><p><strong>Summary</strong><br>多模态语言模型助力扩散模型生成多对象图像，分步规划，反馈控制，轻松满足复杂要求。</p><p><strong>Key Takeaways</strong></p><ul><li>现有的文本转图像模型在生成多对象图像时仍然面临挑战，尤其是在处理对象的空间位置、相对大小、重叠和属性绑定方面。</li><li>MuLan 采用无训练的训练方式，通过规划和反馈控制逐步生成多对象，类似于人类画家作画。</li><li>MuLan 利用大型语言模型 (LLM) 将提示分解为一系列子任务，每个子任务仅生成一个对象，并通过稳定扩散模型对先前生成的对象进行条件控制。</li><li>与现有的 LLM 方法不同，MuLan 只在开始时生成一个高层次的规划，而每个对象的确切大小和位置由 LLM 和注意力引导在每个子任务中确定。</li><li>MuLan 采用视觉语言模型 (VLM) 为每个子任务中生成的图像提供反馈，并在图像违反原始提示时控制扩散模型重新生成图像。</li><li>MuLan 在每个步骤中只处理自己专门处理的简单子任务。</li><li>MuLan 在不同基准上收集了 200 个包含空间关系和属性绑定的多对象提示来评估 MuLan。结果表明，MuLan 在生成多对象方面优于基线方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MuLan：用于渐进式多对象扩散的多模态-LLM 代理</li><li>作者：Sen Li、Ruochen Wang、Cho-Jui Hsieh、Minhao Cheng、Tianyi Zhou</li><li>隶属机构：香港科技大学计算机科学与工程系</li><li>关键词：文本到图像、多对象生成、扩散模型、大语言模型、视觉语言模型</li><li>论文链接：https://arxiv.org/abs/2402.12741Github 链接：https://github.com/measure-infinity/mulan-code</li><li><p>摘要：(1) 研究背景：现有的文本到图像模型在生成包含多个对象的图像时仍然存在困难，尤其是在处理对象的空间位置、相对大小、重叠和属性绑定方面。(2) 过去的方法及其问题：一些方法试图利用大语言模型（LLM）来指导生成过程，但由于 LLM 的空间推理能力有限以及它们与扩散模型缺乏一致性，因此直接生成完整且精确的多对象布局仍然具有挑战性。此外，这些方法通常将布局作为对每个模型的额外条件，这可能会导致扩散模型由于对复杂提示的误解而生成不正确图像。(3) 本文提出的研究方法：本文提出了一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。该范式建立在由多模态-LLM 代理 (MuLan) 进行的渐进式多对象生成之上，MuLan 每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。此外，MuLan 采用视觉语言模型 (VLM) 来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。(4) 实验结果与性能：在包含来自不同基准的多对象（具有空间关系和属性绑定）的 200 个提示上评估 MuLan。结果表明，MuLan 在生成多对象方面优于基线方法。这些性能支持了本文的目标，即开发一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。</p></li><li><p>方法：（1）：提出了一种无训练且可控的文本到图像生成范式，该范式不需要演示，而是主要关注改进现有模型的工具使用。（2）：该范式建立在由多模态-LLM代理（MuLan）进行的渐进式多对象生成之上，MuLan每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。（3）：采用视觉语言模型（VLM）来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。</p></li><li><p>结论：（1）：本文提出了一种无训练且可控的文本到图像生成范式MuLan，该范式不需要演示，而是主要关注改进现有模型的工具使用，在生成多对象方面优于基线方法。（2）：创新点：MuLan：一种无训练且可控的文本到图像生成范式，不需要演示，而是主要关注改进现有模型的工具使用。渐进式多对象生成：MuLan每个阶段只生成一个对象，并根据图像中已生成的对象和最有可能放置新对象的位置的注意力掩码进行条件生成。视觉语言模型（VLM）：采用VLM来提供对每个子任务中生成的图像的反馈，并控制扩散模型以重新生成图像（如果它违反了原始提示）。性能：在包含来自不同基准的多对象（具有空间关系和属性绑定）的200个提示上评估MuLan。结果表明，MuLan在生成多对象方面优于基线方法。工作量：MuLan的实现相对简单，不需要额外的训练或数据。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6204318646d6f8f073e72dd012036b52.jpg" align="middle"><img src="https://pica.zhimg.com/v2-339c08e21eaf72db7bf6af40d44b1ebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c764cf1c9de7293c1a1c79a15a87313.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5f2c4d6c6e5f00fd67d4a729192f3826.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85b2bad757801f5c51069e7f6c02cbc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9001380fef222e92159ed423b319dc8a.jpg" align="middle"></details><h2 id="Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation"><a href="#Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation" class="headerlink" title="Improving Deep Generative Models on Many-To-One Image-to-Image   Translation"></a>Improving Deep Generative Models on Many-To-One Image-to-Image   Translation</h2><p><strong>Authors:Sagar Saxena, Mohammad Nayeem Teli</strong></p><p>Deep generative models have been applied to multiple applications in image-to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to-image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves on many-to-one image-to-image translation. </p><p><a href="http://arxiv.org/abs/2402.12531v1">PDF</a> 11 pages, 6 figures</p><p><strong>摘要</strong><br>用深度扩散模型改进多对一的图像到图像翻译。</p><p><strong>要点</strong></p><ul><li>深度扩散模型是用于图像到图像翻译的生成模型。</li><li>现有的方法通常假设所有领域都具有多个模态或只有一个模态。</li><li>在许多场景下，两个领域之间存在多对一的关系。</li><li>研究者提出了一个着色 MNIST 数据集和一个彩色召回分数，为多对一翻译提供了一个简单的基准。</li><li>提出一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能。</li><li>将该框架应用于 StarGAN V2，实验表明，在新模型中，无监督和半监督设置下的多对一图像到图像翻译性能均得到提高。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：改进多对一图像到图像翻译中的深度生成模型</li><li>作者：Sagar Saxena, Mohammad Nayeem Teli</li><li>隶属关系：马里兰大学计算机科学系</li><li>关键词：深度生成模型、图像到图像翻译、多对一翻译、非对称框架</li><li>链接：https://arxiv.org/abs/2402.12531</li><li>摘要：（1）研究背景：深度生成模型已广泛应用于图像到图像翻译中，取得了令人印象深刻的结果。然而，大多数方法在不同领域之间采用对称设置，假设所有领域都具有多模态或单一模态。然而，许多数据集在两个领域之间具有多对一的关系。（2）过去的方法：过去的方法要么学习双射映射，要么学习多对多映射，但这些方法无法准确建模某些任务中领域之间的关系，例如图像着色、语义分割、深度估计等。（3）研究方法：本文提出了一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能。该框架将生成器和判别器模块解耦，并引入了一种新的损失函数来鼓励生成器生成与输入图像相似的输出图像。（4）方法性能：本文将该框架应用于 StarGAN V2 模型，并在无监督和半监督设置中进行了评估。结果表明，该框架可以有效提高模型在多对一图像到图像翻译中的性能。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种新的非对称框架来改进现有深度生成模型在多对一图像到图像翻译中的性能，该框架可以有效提高模型在多对一图像到图像翻译中的性能。（2）：创新点：</li><li>提出了一种新的非对称框架，将生成器和判别器模块解耦，并引入了一种新的损失函数来鼓励生成器生成与输入图像相似的输出图像。</li><li>将该框架应用于StarGANV2模型，并在无监督和半监督设置中进行了评估。</li><li>结果表明，该框架可以有效提高模型在多对一图像到图像翻译中的性能。性能：</li><li>在无监督设置中，该框架在CelebA数据集上取得了比StarGANV2模型更高的FID和LPIPS得分。</li><li>在半监督设置中，该框架在CelebA数据集上取得了比StarGANV2模型更高的FID和LPIPS得分。</li><li>在Cityscapes数据集上，该框架取得了比StarGANV2模型更高的mIoU和F1得分。工作量：</li><li>该框架的实现相对简单，可以在TensorFlow或PyTorch等深度学习框架中轻松实现。</li><li>该框架的训练时间与StarGANV2模型相似。</li><li>该框架的推理时间与StarGANV2模型相似。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-847aa6560da9e8f5bc3efa20a3a60ab6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b87108e9e8879c6d14d1fe6eaf34112.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7677caf8041932830de453431d2abd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50784d0e85e2b28f9cc755ede524a772.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9b40dd37bb889c7e90ab259793c5ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df01b0bd8844297db8557dc012591bb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af0a71391ad75be3ea34e547daa4db1e.jpg" align="middle"></details><h2 id="FiT-Flexible-Vision-Transformer-for-Diffusion-Model"><a href="#FiT-Flexible-Vision-Transformer-for-Diffusion-Model" class="headerlink" title="FiT: Flexible Vision Transformer for Diffusion Model"></a>FiT: Flexible Vision Transformer for Diffusion Model</h2><p><strong>Authors:Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai</strong></p><p>Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at <a href="https://github.com/whlzy/FiT">https://github.com/whlzy/FiT</a>. </p><p><a href="http://arxiv.org/abs/2402.12376v1">PDF</a> </p><p><strong>Summary</strong><br>通过将图像视为动态大小标记序列，弹性视觉变换器可在不同分辨率和宽高比上生成图像。</p><p><strong>Key Takeaways</strong></p><ul><li>现有扩散模型在处理训练域之外的图像分辨率时面临挑战。</li><li>弹性视觉变换器 (FiT) 是一种专为生成不受限分辨率和宽高比的图像而设计的转换器架构。</li><li>FiT 将图像视为动态大小标记序列，从而支持不同的宽高比。</li><li>FiT 在训练和推理阶段均支持不同的宽高比，从而消除了图像裁剪引起的偏差。</li><li>FiT 在多种分辨率下表现出优异的性能，证明其在训练分辨率分布之外也很有效。</li><li>FiT 的存储库位于 <a href="https://github.com/whlzy/FiT。">https://github.com/whlzy/FiT。</a></li><li>FiT 为图像生成领域开辟了新的可能性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FiT：用于扩散模型的灵活视觉变换器</li><li>作者：Zeyu Lu<em>，Zidong Wang</em>，Di Huang，Chengyue Wu，Xihui Liu，Wanli Ouyang，Lei Bai</li><li>单位：上海人工智能实验室</li><li>关键词：扩散模型，视觉变换器，分辨率泛化，外推技术</li><li>论文链接：https://arxiv.org/abs/2402.12376，Github 链接：None</li><li>摘要：（1）研究背景：自然界的图像分辨率是无限的。现有的扩散模型（如扩散变换器）在处理超出其训练域的图像分辨率时往往面临挑战。（2）过去方法与问题：传统方法将图像视为静态分辨率网格，这限制了它们处理不同分辨率图像的能力。此外，图像裁剪会引入偏差，影响模型的泛化性能。（3）研究方法：为了克服这些问题，本文提出了灵活视觉变换器（FiT），这是一种专门为生成具有无限分辨率和纵横比的图像而设计的变换器架构。FiT 将图像概念化为动态大小标记的序列，这使得它能够在训练和推理阶段轻松适应不同的纵横比，从而促进了分辨率泛化并消除了图像裁剪引起的偏差。通过精心调整的网络结构和训练自由外推技术的集成，FiT 在分辨率外推生成方面表现出卓越的灵活性。（4）方法性能：综合实验表明，FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。</li></ol><p>方法：</p><p>（1）灵活训练：提出了一种灵活的训练方法，允许模型在训练过程中处理不同纵横比的图像，从而促进分辨率泛化并消除图像裁剪引起的偏差。</p><p>（2）SwiGLU激活函数：将MLP激活函数替换为SwiGLU激活函数，可以提高模型的性能。</p><p>（3）2DRoPE位置编码：将绝对位置编码替换为2DRoPE位置编码，可以提高模型的性能和外推能力。</p><p>（4）位置嵌入插值方法：提出了一种位置嵌入插值方法，可以将模型的外推能力扩展到训练分布之外的分辨率。</p><ol><li>结论：（1）：本工作的主要贡献在于，我们提出了用于扩散模型的灵活视觉变换器（FiT），这是一种专门为生成具有无限分辨率和纵横比的图像而设计的变换器架构。FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。（2）：创新点：</li><li>提出了一种灵活的训练方法，允许模型在训练过程中处理不同纵横比的图像，从而促进分辨率泛化并消除图像裁剪引起的偏差。</li><li>将 MLP 激活函数替换为 SwiGLU 激活函数，可以提高模型的性能。</li><li>将绝对位置编码替换为 2DRoPE 位置编码，可以提高模型的性能和外推能力。</li><li>提出了一种位置嵌入插值方法，可以将模型的外推能力扩展到训练分布之外的分辨率。性能：</li><li>FiT 在广泛的分辨率范围内表现出优异的性能，证明了其在训练分辨率分布内和之外的有效性。</li><li>FiT 在各种分辨率下均优于所有先前模型，无论是基于 Transformer 的还是基于 CNN 的。</li><li>结合我们的分辨率外推方法 VisionNTK，FiT 的性能得到了进一步显着提升。工作量：</li><li>本文的工作量较大，涉及到模型架构设计、训练方法改进、外推技术集成等多个方面。</li><li>本文的实验部分也比较复杂，涉及到多个数据集、多个分辨率、多个评价指标等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f2dad57fd66943bffc8c0eefec68b3e8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-297eceedf1e98b27794f86f0cb8285ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6760b58ea1f0ee4f73bf15eae4ddb673.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09693fd0b9790328fcc71c49c26da3ad.jpg" align="middle"></details><h2 id="Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization"><a href="#Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization" class="headerlink" title="Direct Consistency Optimization for Compositional Text-to-Image   Personalization"></a>Direct Consistency Optimization for Compositional Text-to-Image   Personalization</h2><p><strong>Authors:Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin</strong></p><p>Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( <a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>). </p><p><a href="http://arxiv.org/abs/2402.12004v1">PDF</a> Preprint. See our project page (<a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>) for more   examples and codes</p><p><strong>Summary</strong><br>基于文本生成图像的扩散模型可通过微调少数个人图像生成高度一致的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>微调基于文本生成图像的扩散模型时，最大化与参考图像的一致性，同时惩罚与预训练模型的偏差。</li><li>提出一种最小化微调预训练模型以实现一致性的新颖训练目标。</li><li>该方法简单且有效，显着增强了个性化基于文本生成图像模型的组合性。</li><li>引入一种新的采样方法以控制图像保真度与提示保真度之间的权衡。</li><li>强调使用综合标题作为参考图像以进一步增强图像与文本的一致性。</li><li>证明了该方法在主题、风格或两者方面的基于文本生成图像个性化中的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：直接一致性优化用于合成文本到图像个性化</li><li>作者：Seunghoon Hong, Inwoong Ko, Sunghyun Cho, Seonghyeon Nam, Dong Huk Park</li><li>隶属机构：首尔大学</li><li>关键词：文本到图像合成、个性化、扩散模型、一致性优化</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1)：研究背景：文本到图像 (T2I) 扩散模型在经过少量个人图像的微调后，能够生成具有高度一致性的视觉效果。然而，它们仍然缺乏在原始预训练模型中可能的不同场景或风格的图像合成能力。(2)：过去的方法及其问题：为了解决这个问题，本文提出了一种通过最大化与参考图像的一致性来微调 T2I 模型的方法，同时惩罚与预训练模型的偏差。过去的方法存在的问题是，它们在个性化 T2I 模型中仍然缺乏合成不同场景或风格的图像的能力。(3)：研究方法：本文提出了一种新的 T2I 扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。该方法称为直接一致性优化，它与常规扩散损失一样简单，同时显着提高了个性化 T2I 模型的组合性。此外，本文的方法还引入了一种新的采样方法，该方法可以控制图像保真度与提示保真度之间的权衡。最后，本文强调了使用综合标题作为参考图像的必要性，以进一步增强图像与文本的对齐。(4)：实验结果：本文的方法在主题、风格或两者兼而有之的 T2I 个性化方面都取得了优异的性能。具体来说，本文的方法在帕累托前沿方面优于基线方法。</li></ol><p>Methods:(1) Direct Consistency Optimization (DCO): We formulate T2I diffusion model fine-tuning as a constrained policy optimization problem and propose DCO loss to maximize the consistency reward of generated samples while penalizing the deviation from the pretrained model.(2) Reward Guidance (RG): After fine-tuning with DCO loss, we introduce RG to control the trade-off between consistency and image-text alignment by interpolating the noise estimations from the fine-tuned model and the pretrained model.(3) Prompt Construction for Reference Images: We emphasize the importance of comprehensive captions for reference images and provide examples to illustrate the difference between compact captions and comprehensive captions.</p><ol><li>结论：（1）：提出了一种新的文本到图像扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。（2）：创新点：提出了一种新的文本到图像扩散模型训练目标，该目标可以最小程度地微调预训练模型以实现一致性。引入了一种新的采样方法，该方法可以控制图像保真度与提示保真度之间的权衡。强调了使用综合标题作为参考图像的必要性，以进一步增强图像与文本的对齐。性能：在主题、风格或两者兼而有之的文本到图像个性化方面都取得了优异的性能。在帕累托前沿方面优于基线方法。工作量：需要收集和准备参考图像。需要微调文本到图像扩散模型。需要采样生成的图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-747445a04d574a8975290f4c0ffe6aca.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-915bf11d3f533330ed7c94f5f635e501.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3a074dca6974482c499ea0392640cb3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-23  Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/</id>
    <published>2024-02-13T12:10:49.000Z</published>
    <updated>2024-02-13T12:10:49.162Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa</strong></p><p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene’s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data. </p><p><a href="http://arxiv.org/abs/2402.07310v1">PDF</a> </p><p><strong>Summary</strong><br>生物神经形态学启发的 NeRF 架构，融合多源输入，提取更本质相关信息，实现场景 3D 表示和新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>BioNeRF 是一种受生物神经形态学启发的架构，用于建模场景的 3D 表示并通过辐射场合成新视角。</li><li>BioNeRF 实现了一种认知启发的机制，将来自多个来源的输入融合到一个类似记忆的结构中，提高存储容量并提取更多内在和相关信息。</li><li>BioNeRF 模仿在锥体细胞中观察到的关于上下文信息的行为，其中记忆被提供为上下文并与两个后续神经模型的输入相结合，一个负责产生体积密度，另一个负责用于渲染场景的颜色。</li><li>实验结果表明，BioNeRF 在衡量人类感知的质量指标上优于最先进的结果，包括真实世界图像和合成数据两类数据集。</li><li>BioNeRF 在两个数据集上都优于最先进的结果，分别为真实世界图像和合成数据。</li><li>BioNeRF 在自由视角视频和全景视频的渲染上均取得了最先进的结果。</li><li>BioNeRF 在不同场景和条件下表现出鲁棒性和泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1标题：《BioNeRF 生物合理神经辐射场的视图合成》(BioNeRF Biologically Plausable Neural Radiance Fields for View Synthesis)。</p><p></p><p></p><p>作者列表：(Leandro A Passos)、Douglas Rodrigues)、Danilo Jodas)、Kelton A P Costa)、João Paulo Papa)。</p><p></p><p></p><p>第一作者单位：(巴西 Bauru 市 Av Eng Luiz Edmundo Carrijo Coube 街十四之一栋 São Paulo State University)。</p><p></p><p></p><p>关键词：(神经渲染)、生物合理神经模型)。</p><p></p><p></p><p>链接：(Paper URL)。</p><p></p><p></p><p>Github代码链接：(Github None)。</p><p></p><p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p><p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p><ol><li><p>方法：（1）BioNeRF采用认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。（2）BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。（3）实验结果表明，BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。</p></li><li><p>结论：（1）：BioNeRF在神经渲染领域取得了重大突破，提出了一种新的生物合理神经辐射场架构，该架构能够利用辐射字段构建场景的三维表示形式并合成新的视图。（2）：创新点：</p></li><li>BioNeRF采用了一种认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。</li><li>BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。性能：</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。工作量：</li><li>BioNeRF的实现难度较高，需要较强的编程能力和数学基础。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a3147366d087ebe11e207f5d9173f950.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-91083b7a4d33cafbb989e6672e5d0690.jpg" align="middle"></details><h2 id="NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction"><a href="#NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction" class="headerlink" title="NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction"></a>NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction</h2><p><strong>Authors:Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</strong></p><p>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2402.05532v2">PDF</a> Accepted by 3DV 2024</p><p><strong>Summary</strong><br>手-物交互的自由视角逼真重建。</p><p><strong>Key Takeaways</strong></p><ul><li>手-物交互建模是计算机三维建模的挑战性任务。</li><li>现存方法无法真实地进行手-物交互建模。</li><li>提出 NCRF 框架来从视频中重建手-物交互。</li><li>NCRF 包括接触优化场和手-物的神经辐射场。</li><li>接触优化场预测三维查询点精确的接触场。</li><li>手-物的神经辐射场学习手-物隐式表示。</li><li>手-物运动场产生观察到标准的对应关系。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NCRF：用于手-物体交互自由视点渲染的神经接触辐射场</li><li>作者：Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</li><li>第一作者单位：伯明翰大学</li><li>关键词：手-物体交互、自由视点渲染、神经辐射场、接触场优化</li><li>论文链接：https://arxiv.org/abs/2402.05532</li><li><p>摘要：（1）研究背景：手-物体交互建模是计算机视觉中一项极具挑战性的任务。尽管该领域取得了显着进展，但现有方法仍然无法以逼真的方式合成手-物体交互，这源于手和物体之间严重的相互遮挡以及不准确的手-物体姿态估计，从而导致渲染质量下降。（2）过去方法及其问题：以往工作通常将此任务表述为联合手和物体姿态估计问题，并依赖参数化的手-物体模型（如 MANO 和 YCB）来估计手的运动变换。然而，现有方法难以恢复手-物体接触场的准确几何形状，并且渲染质量受到遮挡和姿态估计误差的严重影响。（3）本文提出的研究方法：为了解决这些挑战，我们提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。NCRF 框架主要由两个关键组件组成：（a）接触优化场：从 3D 查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。（4）方法在什么任务上取得了怎样的性能，这些性能是否支持了它们的目标：在 HO3D 和 Dex-YCB 数据集上的广泛实验表明，我们的方法在渲染质量和姿态估计精度方面均优于当前最先进的方法。这些性能支持了我们的目标，即以逼真的方式重建和渲染手-物体交互。</p></li><li><p>Methods:(1): 本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。(2): NCRF框架主要由两个关键组件组成：（a）接触优化场：从3D查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。(3): 我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。</p></li><li><p>结论：（1）：本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。NCRF框架通过设计动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。（2）：创新点：</p></li><li>提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。</li><li>设计了动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。</li><li>提出了一种新的手-物体变形模块，该模块能够将射线变形到规范空间中，并以逼真的方式渲染手-物体交互。性能：</li><li>在HO3D和Dex-YCB数据集上的广泛实验表明，NCRF框架在渲染质量和姿态估计精度方面均优于当前最先进的方法。</li><li>NCRF框架能够生成逼真的新视角合成，并且能够很好地处理具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。工作量：</li><li>NCRF框架的实现相对复杂，需要较高的计算资源。</li><li>NCRF框架的训练过程需要较长时间，并且需要大量的数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19c080ef42e2fcaa0595e65274d339b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b7f0899ff9371cac98ca44ab3913a349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1403a98bc963e537484ce413bb5d32ea.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v2">PDF</a> </p><p><strong>Summary</strong><br>对于大场景下的重建任务，本文引入 BirdNeRF，该方法能够有效利用无人机影像数据实现高效低存储的大场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>BirdNeRF 是一款针对航空图像的大场景重建方法，解决了以往小场景重建存在的训练慢、渲染慢、模型容量小等问题。</li><li>BirdNeRF 提出了一种基于鸟瞰视角的姿势分解算法，将大场景图像集分解成多个小场景子集，每个子集使用单独的 NeRF 进行训练。</li><li>BirdNeRF 采用了一种新颖的投影引导式新视角重新渲染策略，可以有效利用独立训练的子场景生成更好的渲染结果。</li><li>BirdNeRF 在现有数据集和我们自己的无人机数据上进行了评估，在单个 GPU 上的重建速度比经典摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，且渲染质量相似。</li><li>BirdNeRF 可以在任意大的场景中无缝扩展，并支持对环境的局部更新，提高了重建过程的灵活性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：BirdNeRF：基于航拍图像的大场景快速神经重建</li><li>作者：张惠卿、薛一菲、廖明、老义珍</li><li>单位：无</li><li>关键词：NeRF、大场景重建、航拍图像、空间分解、投影引导</li><li>链接：无，Github 链接：无</li><li><p>摘要：（1）研究背景：大场景三维重建是摄影测量和遥感领域的一项重要任务，可以利用航拍或卫星图像、激光雷达数据和街景图像等多种数据源构建城市的三维模型。近年来，基于图像的三维重建技术取得了很大的进展，并在城市规划、导航、虚拟旅游、房地产、灾害管理和历史保护等领域得到了广泛的应用。（2）过去方法与问题：现有的基于图像的三维重建技术主要分为传统几何方法和基于神经网络的方法。传统几何方法主要包括摄影测量和激光扫描，这些方法可以生成高精度的三维模型，但需要大量的人工参与和昂贵的设备。基于神经网络的方法，如神经辐射场（NeRF），可以从图像中自动学习三维场景的表示，但这些方法通常需要大量的训练数据和计算资源，并且在大场景重建任务中容易出现伪影和低视觉保真度的问题。（3）研究方法：为了解决上述问题，本文提出了一种新的基于 NeRF 的大场景重建方法，称为 BirdNeRF。BirdNeRF 采用了一种新的鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。这种分解方法不仅可以减少训练和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。（4）性能与目标：BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。这些结果证明了 BirdNeRF 的有效性和实用性。</p></li><li><p>方法：（1）场景分解：将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。（2）视角姿势分解：采用鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景。（3）新视角重新渲染：提出一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。</p></li><li><p>结论：（1）：本文提出了一种新的基于 NeRF 的大场景重建方法 BirdNeRF，该方法采用鸟瞰视角姿势分解算法和投影引导的新视角重新渲染策略，可以有效地解决大场景重建任务中容易出现伪影和低视觉保真度的问题。（2）：创新点：</p></li><li>提出了一种新的鸟瞰视角姿势分解算法，可以将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型，从而减少训练和渲染时间，提高重建质量。</li><li>提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。性能：</li><li>BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。工作量：</li><li>BirdNeRF 的实现相对简单，并且可以在单个 GPU 上训练和渲染。然而，由于需要对大场景图像进行分解，因此 BirdNeRF 的预处理时间可能会比较长。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-daadce77f0b48dc25dd984f5c66ee7ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6d52642c6cfdc84439f5ea843cff2fd1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-13  BioNeRF Biologically Plausible Neural Radiance Fields for View   Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/</id>
    <published>2024-02-13T11:47:50.000Z</published>
    <updated>2024-02-13T11:47:50.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting"><a href="#GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting" class="headerlink" title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting"></a>GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</h2><p><strong>Authors:Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</strong></p><p>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at <a href="https://gala3d.github.io/">https://gala3d.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.07207v1">PDF</a> </p><p><strong>Summary</strong></p><p>利用大型语言模型 (LLM) 生成初始布局，并引入布局引导 3D 高斯表示，指导 3D 内容生成，同时满足适应性几何约束。</p><p><strong>Key Takeaways</strong></p><ul><li>GALA3D 将大型语言模型与布局引导 3D 高斯表示相结合，用于有效地进行文本到 3D 的生成。</li><li>布局引导 3D 高斯表示提供了自适应的几何约束，确保生成的 3D 内容具有真实感和一致性。</li><li>GALA3D 采用对象-场景组合优化机制，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li><li>GALA3D 可以同时调整从大型语言模型中提取的粗略布局，使其与生成的场景保持一致。</li><li>GALA3D 是一个用户友好的端到端框架，可进行最先进的场景级 3D 内容生成和可控编辑。</li><li>GALA3D 能够确保场景中对象级实体的高保真度。</li><li>GALA3D 的源代码和模型可从 <a href="https://gala3d.github.io/">https://gala3d.github.io/</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GALA3D：基于布局引导的文本到 3D 复杂场景生成</li><li>作者：Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</li><li>第一作者单位：北京大学万选信息技术学院</li><li>关键词：文本到 3D、生成式高斯体素、布局引导、条件扩散</li><li>论文链接：https://arxiv.org/abs/2402.07207    Github 链接：None</li><li>摘要：(1) 研究背景：文本到 3D 生成旨在根据自然语言描述生成逼真的 3D 场景。现有方法要么产生低质量的纹理、视觉伪影和几何失真，要么无法根据文本准确生成多个对象及其交互。(2) 过去的方法：现有方法主要分为两类：基于体素的方法和基于网格的方法。基于体素的方法虽然可以生成高质量的 3D 场景，但计算成本高昂。基于网格的方法虽然计算成本较低，但生成的 3D 场景质量较差。(3) 研究方法：本文提出了一种基于生成式高斯体素的文本到 3D 生成方法。该方法首先利用大型语言模型生成初始布局，然后引入布局引导的 3D 高斯体素表示来生成 3D 内容。接着，提出了一种对象场景组合优化机制，该机制利用条件扩散来协同生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景，同时调整从大型语言模型中提取的粗略布局先验，使其与生成的场景对齐。(4) 实验结果：实验表明，GALA3D 是一个用户友好的端到端框架，可用于生成高质量的 3D 场景。该方法在多个数据集上取得了最先进的性能，并且能够支持交互式可控编辑。</li></ol><p>Methods:</p><p>(1) 粗略布局先验：利用大型语言模型（LLM）从文本描述中提取粗略布局先验，包括对象实例及其对应的位置、尺寸和方向。</p><p>(2) 布局引导的高斯体素表示：将粗略布局先验转换为布局引导的高斯体素表示，其中每个对象实例由一组高斯体素表示，高斯体素的位置、尺寸和方向由布局先验决定。</p><p>(3) 自适应几何控制：对高斯体素的分布和形状进行自适应几何控制，以确保高斯体素的分布紧密贴合对象表面，并且形状更加规则和一致。</p><p>(4) 具有扩散先验的组合优化：采用具有扩散先验的组合优化策略来更新布局引导的高斯体素参数，包括多视图扩散优化和场景条件扩散优化，以生成具有统一风格和交互关系的对象实例。</p><p>(5) 布局损失：引入布局损失来确保生成的3D场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。</p><ol><li>结论：（1）：GALA3D 是一种基于生成式布局引导的 3D 高斯体素表示的场景级文本到 3D 框架，该框架可以生成具有多个对象的高保真、3D 一致的场景。实验表明，该方法在文本到 3D 生成方面优于现有方法，展示了生成具有多个对象和交互的复杂场景的能力，并实现了出色的纹理和几何效果。该方法还促进了交互式和可控的场景编辑，实现了一个高效且用户友好的 3D 场景生成和编辑框架。（2）：创新点：</li><li>提出了一种基于生成式布局引导的 3D 高斯体素表示，该表示可以生成具有统一风格和交互关系的对象实例。</li><li>引入了一种具有扩散先验的组合优化策略，该策略可以更新布局引导的高斯体素参数，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li><li>提出了一种布局损失，该损失可以确保生成的 3D 场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。性能：</li><li>在多个数据集上取得了最先进的性能。</li><li>能够支持交互式可控编辑。工作量：</li><li>该方法的实现相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3dde3c6bf6237679d7dc8e3a25b014e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c780f9b8f1b542e9c562c2d185d7e16a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e76b694075c9297c3e8a8d38bf4c8fe3.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v1">PDF</a> 6-page technical report</p><p><strong>Summary</strong><br>3D 高斯曲面表示增强多模态预训练， 促进图像、语言和 3D 表示的统一。</p><p><strong>Key Takeaways</strong></p><ul><li>多模态预训练在图像和语言描述的对齐方面取得进展，对物体识别、分类和检索至关重要。</li><li>点云的离散表示丢失了物体的表面形状信息，导致渲染结果与 2D 对应关系之间存在差距。</li><li>提出 GS-CLIP 首次将 3DGS（3D 高斯曲面）引入多模态预训练，以增强 3D 表示。</li><li>GS-CLIP 利用预训练的视觉语言模型，在大规模真实世界图像文本对上学习共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。</li><li>提出了一种新颖的 Gaussian-Aware Fusion 来提取和融合全局显式特征。</li><li>作为语言图像 3D 预训练的通用框架，GS-CLIP 与 3D 主干网络无关。</li><li>具有挑战性的实验表明，GS-CLIP 显着改善了最先进的技术，优于先前最好的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯溅射</li><p></p><p></p><li>作者：李浩源、周彦鹏、曾义涵、许航、梁晓丹</li><p></p><p></p><li>第一作者单位：中山大学深圳校区</li><p></p><p></p><li>关键词：3D 表示、高斯溅射、对比学习、多模态预训练</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2402.06198，Github 链接：无</li><p></p><p></p><li>摘要：(1)：研究背景：3D 形状以点云表示在多模态预训练中取得了进展，用于对齐图像和语言描述，这对于物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生差距。(2)：过去的方法和问题：现有的 3D 表示学习方法主要对点云的关键点位置信息进行建模，这限制了 3D 视觉理解和 3D 表示学习的性能。(3)：研究方法：为了解决上述问题，本文提出了 GS-CLIP，将 3D 高斯溅射 (3DGS) 引入多模态预训练，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在真实世界的大规模图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器，用于对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合，用于提取和融合全局显式特征。(4)：实验结果：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。实验结果表明，3DGS 在跨模态学习中具有强大的表示能力。</li><br>&lt;/ol&gt;<p></p><p>7.方法：（1）跨模态预训练：为了对齐文本、图像和3DGS的多模态表示，GS-CLIP采用预训练的语言-图像模型CLIP，形成一个共同的语言-图像潜在空间，作为3DGS的目标潜在空间。对于零样本/开放词识别，通过冻结CLIP文本编码器、图像编码器和公共真实世界潜在空间，保证了3DGS表示的可迁移性。具体来说，我们借鉴了[19, 28]中的对比损失，并形成文本-3DGS对齐和图像-3DGS对齐，用于多模态对齐。（2）高斯感知融合：虽然将点云投影到3D体素的3D骨干可以更好地学习全局位置和特征，但我们发现3DGS的显式特征会被忽略，因为体素化丢失了形状和纹理信息。因此，我们采用基于Transformer的分支直接对高斯特征建模为高斯特征上下文，并以残差形式注入它。具体来说，给定具有n个高斯的3DGS输入XG∈Rn×14，我们首先将XG分成Ng组，用于XgroupG=Ng�g=1XgG，然后使用基于卷积的体系结构EGθ,c提取全局特征fGc和基于Transformer的体系结构EGθ,t提取显式高斯特征fGG，最后将fGc和fGG连接起来，形成最终的3DGS表示fG。</p><ol><li>结论：（1）意义：本文首次提出 GS-CLIP，将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便从补充信息中更好地学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能。（2）优缺点：创新点：</li><li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li><li>提出了一种高斯感知融合，以便从补充信息中更好地学习信息。</li></ol><p>性能：- 在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。</p><p>工作量：- 需要对 3DGS 进行预训练，这可能需要大量的数据和计算资源。- 需要对高斯感知融合进行训练，这可能需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-13  GALA3D Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/</id>
    <published>2024-02-13T11:37:33.000Z</published>
    <updated>2024-02-13T11:37:33.323Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer"><a href="#DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer" class="headerlink" title="DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer"></a>DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer</h2><p><strong>Authors:Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</strong></p><p>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel. </p><p><a href="http://arxiv.org/abs/2402.05712v1">PDF</a> 9 pages, 5 figures. Code is avalable at   <a href="https://github.com/theEricMa/DiffSpeaker">https://github.com/theEricMa/DiffSpeaker</a></p><p><strong>Summary</strong><br>通过提出带偏条件注意力的扩散模型，我们解决了音视频配对数据的稀缺问题，在保持音视频同步的情况下，可以快速生成高质量的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的扩散模型，用于语音驱动的3D面部动画。</li><li>使用带偏条件注意力模块，可以更好地处理音视频配对数据的稀缺问题。</li><li>在现有基准上取得了最先进的性能。</li><li>可以快速生成面部动画，推理速度快。</li><li>可以有效地生成非语言面部表情。</li><li>可以控制动画过程中的嘴型同步和非语言面部表情之间的权衡。</li><li>该模型可以用于各种多媒体应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DiffSpeaker：基于扩散变换器的语音驱动 3D 面部动画（DiffSpeaker: Speech-Driven 3DFacial Animation with Diffusion Transformer）</li><li>作者：Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</li><li>第一作者单位：香港理工大学（香港理工大学）</li><li>关键词：语音驱动面部动画、扩散模型、Transformer、条件注意机制</li><li>论文链接：https://arxiv.org/abs/2402.05712Github 代码链接：https://github.com/theEricMa/DiffSpeaker</li><li><p>摘要：（1）：语音驱动 3D 面部动画在许多多媒体应用中非常重要。最近的研究表明，使用扩散模型或 Transformer 架构来执行此任务很有前景。然而，它们的简单聚合并不能带来改进的性能。我们怀疑这是由于缺乏配对的音频-4D 数据，这对于 Transformer 在扩散框架内有效地执行去噪器至关重要。（2）：过去的方法主要使用基本的滑动窗口方法处理音频输入，这通常会导致生成的的面部动作范围狭窄。近年来，研究人员开始采用 Transformer 架构来进行语音驱动 3D 面部动画，但传统的确定性回归可能不是最好的方法，因为人类的语音和面部表情是可变且动态的，很难用一个固定的映射来准确捕捉它们之间的关系。（3）：为了解决这个问题，我们提出了 DiffSpeaker，这是一个基于 Transformer 的网络，配备了新颖的偏置条件注意机制模块。这些模块可以替代标准 Transformer 中传统的自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。我们还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡。（4）：实验表明，我们的模型不仅在现有基准上实现了最先进的性能，而且由于其能够并行生成面部动作，因此推理速度也很快。</p></li><li><p>方法：(1): 本文提出了一种基于扩散模型和Transformer架构的语音驱动3D面部动画方法DiffSpeaker，该方法利用了扩散模型的生成能力和Transformer架构的序列建模能力，有效地解决了语音驱动3D面部动画任务中的配对音频-4D数据缺乏的问题。(2): DiffSpeaker采用了一种新颖的偏置条件注意机制模块，该模块可以替代标准Transformer中的传统自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。(3): DiffSpeaker还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡，并提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。(4): 实验结果表明，DiffSpeaker在现有基准上实现了最先进的性能，并且由于其能够并行生成面部动作，因此推理速度也很快。</p></li><li><p>结论：（1）：本工作探索了将 Transformer 架构与基于扩散的框架有效结合用于语音驱动 3D 面部动画的方法。我们贡献的核心是引入了带有偏置的条件自注意力/交叉注意力机制，该机制解决了使用受限且跨度短的音频-4D 数据训练基于扩散的 Transformer 的困难。我们还研究了在实现准确的唇形同步和生成与语音相关性较小的面部表情之间的平衡。我们开发的模型优于当前的方法，在动画质量和生成速度方面都表现出色。（2）：创新点：提出了一种新的带有偏置的条件自注意力/交叉注意力机制，该机制可以有效地利用受限且跨度短的音频-4D 数据来训练基于扩散的 Transformer。提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。性能：在现有基准上实现了最先进的性能。推理速度快，能够并行生成面部动作。工作量：需要收集和预处理大量的音频-4D 数据。需要对模型进行大量的训练。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a6a1095c49476b6d0a24c660e7abca7e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2c694a105e50cf1ba9a9e0743f793c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dd59be8351677e84215dd037093b2ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ebb8f9ab10ef4d053668941b0c247fcb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff2d1ca9215127e6894689d494fb8244.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-13  DiffSpeaker Speech-Driven 3D Facial Animation with Diffusion   Transformer</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/</id>
    <published>2024-02-13T11:31:26.000Z</published>
    <updated>2024-02-13T11:31:26.322Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models"><a href="#Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models" class="headerlink" title="Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models"></a>Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</h2><p><strong>Authors:Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah</strong></p><p>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions. </p><p><a href="http://arxiv.org/abs/2402.06969v1">PDF</a> Submitted in IEEE EMBC 2024 Conference</p><p><strong>Summary</strong><br>稳定扩散模型在医学成像数据合成中展现出强大能力，有望解决数据稀缺问题，助力心血管图像处理领域的发展。</p><p><strong>Key Takeaways</strong></p><ul><li>稳定扩散模型在医学成像数据合成中展现出巨大潜力，可用于解决数据稀缺问题。</li><li>通过微调用户定义文本提示的稳定扩散模型，仅使用有限数量的 CTA 图像作为输入，即可生成合成的冠状动脉 CTA 图像。</li><li>定量分析和定性评估相结合的综合评估表明，使用文本到图像 (T2I) 稳定扩散模型可以成功生成心脏 CTA 图像。</li><li>微调的 T2I CTA 扩散模型能够生成具有急性 B 型主动脉夹层 (TBAD) 医学特征的图像。</li><li>合成的图像在视觉上与真实图像相似，并保留了真实图像中的关键解剖结构。</li><li>临床医生认为合成的图像具有足够的质量，可用于临床实践。</li><li>该研究表明，稳定扩散模型在医学成像数据合成中具有广阔的应用前景，有望改善心血管疾病的诊断和治疗。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用稳定扩散模型合成 B 型主动脉夹层断层扫描图像数据</li><li>作者：Ayman Abaid、Muhammad Ali Farooq、Niamh Hynes、Peter Corcoran 和 Ihsan Ullah</li><li>第一作者单位：爱尔兰戈尔韦大学计算机科学学院</li><li>关键词：主动脉夹层、计算机断层扫描血管造影、医学图像合成、稳定扩散模型、文本到图像</li><li>论文链接：https://arxiv.org/abs/2402.06969Github 代码链接：无</li><li><p>摘要：(1)：研究背景：主动脉夹层是一种严重的心血管疾病，需要准确和及时的诊断。计算机断层扫描血管造影 (CTA) 是诊断主动脉夹层最常用的成像方式，但由于数据稀缺，机器学习算法在主动脉夹层图像处理中的能力受到限制。(2)：过去的方法：过去的研究使用深度学习模型来自动分割主動脈夾層圖像中的真腔、假腔和假腔血栓。然而，这些模型通常需要大量的数据进行训练，而主动脉夹层的数据集往往很小。(3)：研究方法：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法。稳定扩散模型是一种生成式人工智能模型，可以根据文本提示生成逼真的图像。本研究通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像。(4)：方法性能：实验结果表明，微调后的稳定扩散模型能够生成具有主动脉夹层典型特征的图像。定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</p></li><li><p>方法：(1) 数据预处理：将 3D CTA 图像转换为 2D 图像，并将其划分为训练集、测试集和验证集。将数据分为五类：有真腔 (TL) 的图像、有假腔 (FL) 的图像、有假腔血栓 (FLT) 的图像、有 TL 和 FL 的图像，以及无 TL、FL 和 FLT 信息的数据。(2) 文本到图像 (T2I) 模型训练：使用 ImageTBAD 数据集和 DreamBooth 训练工具微调预训练的稳定扩散模型，以生成高质量的 CTA 数据。在训练过程中，为每类数据分配专门的文本提示，并为后续类别的特定类提供否定提示。(3) 图像采样：使用欧拉和欧拉 A 图像采样器从潜空间的不同区域采样，以生成多样化和逼真的图像。(4) 数据增强：使用独特的文本提示渲染具有类别分布的 CT 数据，以增强具有特定 CT 特征的数据，例如 TL、FL 和 FLT。(5) 模型评估：使用 Fréchet Inception Distance (FID) 和 Multiscale Structural Similarity Index Measure (MS-SSIM) 评估合成图像的质量和多样性。训练 SoTA 模型（例如 UNet）对合成图像进行分割，以评估其实用性。</p></li><li><p>结论：（1）：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法，该方法能够生成具有主动脉夹层典型特征的图像，并且可以用于训练深度学习模型进行主动脉夹层图像分割。（2）：创新点：</p></li><li>使用稳定扩散模型合成主动脉夹层 CTA 图像，这是首次将稳定扩散模型应用于主动脉夹层图像合成。</li><li>通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像，这使得图像合成过程更加灵活和可控。</li><li>合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割，这表明该方法具有实际应用价值。性能：</li><li>定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</li><li>训练的 SoTA 模型（例如 UNet）对合成图像进行分割，获得了良好的分割精度，这表明该方法合成的图像具有很高的实用性。工作量：</li><li>该方法需要对稳定扩散模型进行微调，这需要一定的计算资源和时间。</li><li>该方法需要对数据进行预处理，这需要一定的人工劳动。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-66ad8c9bd4b7c6c0abc54d425f5bff3e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7eb93cb5e3a23926b4fa972f1f7e5a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed3565ac4c49d72e02f85632488a4e3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c0c6793d4532774c78760ad1a11631e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3d53880237f6c704914112c0392f627.jpg" align="middle"></details><h2 id="Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation"><a href="#Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation" class="headerlink" title="Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation"></a>Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation</h2><p><strong>Authors:Peter Hönig, Stefan Thalhammer, Markus Vincze</strong></p><p>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models. </p><p><a href="http://arxiv.org/abs/2402.06436v1">PDF</a> Submitted to the First Austrian Symposium on AI, Robotics, and Vision   2024</p><p><strong>Summary</strong><br>扩散模型在图像到图像转换任务中表现优于生成对抗网络，在 6D 目标位姿估计任务中具有潜在优势。</p><p><strong>Key Takeaways</strong></p><ul><li>估计 RGB 图像和 3D 空间之间的 2D-3D 对应关系是 6D 目标位姿估计中的一个基本问题。</li><li>当前，密集对应图是使用基于 GAN、自动编码器或直接回归模型的图像到图像转换模型估计的。</li><li>最近，图像到图像转换领域的最新进展已使扩散模型成为在基准数据集上评估时的优越选择。</li><li>在这项研究中，我们比较了基于 GAN 和扩散模型的图像到图像转换网络，用于 6D 目标位姿估计的下游任务。</li><li>我们的结果表明，基于扩散的图像到图像转换模型优于 GAN，表明 6D 目标位姿估计模型有进一步改进的潜力。</li><li>扩散模型在图像到图像转换任务中具有更高的准确性和鲁棒性。</li><li>扩散模型在 6D 目标位姿估计任务中具有潜在优势，可以进一步提高位姿估计的准确性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用扩散模型改进 2D-3D 密集对应以进行 6D 目标位姿估计</li><li>作者：Peter Hönig、Stefan Thalhammer、Markus Vincze</li><li>作者单位：奥地利维也纳工业大学自动化与控制研究所</li><li>关键词：6D 目标位姿估计、2D-3D 密集对应、扩散模型、图像到图像翻译</li><li>论文链接：https://arxiv.org/abs/2402.06436</li><li>摘要：（1）研究背景：6D 目标位姿估计是许多感知任务的基础，例如自动驾驶、增强现实、创建数字孪生或机器人抓取。RGB-D 传感器可以同时提供颜色和深度数据，但并不总是可用。深度数据也容易受到噪声和其他失真的影响，这些失真通常由场景中闪亮、金属和透明物体反射引起。为了解决这个问题，人们考虑仅使用 RGB 图像进行位姿估计。最先进的方法依赖于估计 RGB 图像和 3D 对象模型之间的 2D-3D 密集对应。尽管这些方法擅长推断具有高可见性的对象位姿，但它们仍然面临着由杂波、遮挡、图像失真和闪亮物体表面带来的重大挑战。（2）过去的方法：过去，人们通过使用直接回归、生成对抗网络 (GAN) 和 U-Net 架构的组合或编码器-解码器卷积神经网络 (CNN) 来解决位姿估计的 2D-3D 对应问题。上述方法估计的密集对应图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。然而，这些方法在处理遮挡、杂波和具有挑战性的材料特性时存在困难。（3）论文方法：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计 2D-3D 密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将 RGB 图像作为输入，并生成一个密集对应图，该图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。（4）实验结果：本文的方法在 YCB-Video 数据集上进行了评估。实验结果表明，本文的方法在 6D 目标位姿估计任务上优于基于 GAN 的图像到图像翻译网络。这表明扩散模型在 6D 目标位姿估计任务中具有潜力。</li></ol><p><strong>方法</strong>：</p><p>（1）图像到图像翻译模型：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将RGB图像作为输入，并生成一个密集对应图，该图包含从RGB图像到3D模型的每个像素的3D坐标。</p><p>（2）位置先验：为了获得2D位置先验，本文使用2D目标检测器从RGB图像中裁剪感兴趣区域（ROI）。ROI是图像到图像翻译模型的输入。图像到图像翻译模型学习从RGB裁剪中估计2D-3D密集对应。2D目标检测器用一个矩形边界框裁剪对象。因此，对象没有被完全裁剪，背景像素仍然存在。因此，图像到图像翻译网络的学习目标是双重的。网络的主要目标是学习估计2D-3D密集对应。同时，网络需要隐式地学习如何将对象从背景中分割出来。RANSAC+PnP步骤从密集对应图中估计6D目标位姿。</p><p>（3）数据增强：为了生成图像到图像翻译任务的训练数据，对象网格被归一化，以适应无量纲的1x1x1立方体。然后，根据顶点在归一化对象坐标空间中的XYZ位置，用RGB值对对象网格的顶点进行着色。然后，使用真实平移、旋转和相机内参对归一化和着色的网格进行渲染。</p><p>（4）图像到图像翻译算法：本文比较了两种图像到图像翻译算法，即GAN模型Pix2Pix和扩散模型BBDM。位置先验和RANSAC+PnP步骤对于这两种方法都是相同的，只有图像到图像翻译函数IDC=F(IRGB)是不同的。两种模型都在相同条件下进行训练。首先，模型在没有任何数据增强的情况下进行训练，除了将ROI裁剪调整为128x128像素，这是两个模型的输入和输出大小。在第二次训练运行中，两个模型都使用相同的数据增强参数进行训练，如表1所示。对于每次运行，两个模型都训练40个epoch。</p><p>（5）数据集：本文在LMO数据集上评估图像到图像翻译模型。它具有8个在随机域中采样的家用物体和50000张合成渲染的图像。这些合成渲染的图像仅用于训练。为了评估，使用了1214张真实世界的测试图像。</p><p>（6）位置先验：使用两组预先计算的位置先验进行对象裁剪。我们使用来自2023年目标位姿估计（BOP）挑战赛基准的YOLOx检测结果来评估位姿估计的下游任务。为了评估对象分割，使用真实位置先验。</p><p>（7）评估指标：本文评估了估计的6D位姿的质量，以及2D-3D密集对应图和对象分割的质量。6D目标位姿使用ADD(-S)分数进行评估。ADD(-S)是指模型点m之间的平均距离，对于kmd≥m。误差阈值km用10%定义。公式1中显示了m的计算。R和T表示真实旋转和平移，而^R和^T表示估计旋转和平移，而x表示模型M中的模型点。为了将位姿估计结果与其他方法进行比较，我们依靠2023年目标位姿估计（BOP）挑战赛基准计算的平均召回率。该AR分数是可见表面差异（VSD）、最大对称感知表面距离（MSSD）和最大对称感知投影距离（MSPD）的平均召回率的平均值。</p><ol><li>结论：（1）：本文提出了一个基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。我们比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。我们的实验表明，扩散模型在估计2D-3D密集对应图的质量方面优于GAN。（2）：创新点：</li><li>提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。</li><li>比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。性能：</li><li>扩散模型在估计2D-3D密集对应图的质量方面优于GAN。</li><li>在YCB-Video数据集上，扩散模型在6D目标位姿估计任务上优于基于GAN的图像到图像翻译网络。工作量：</li><li>需要收集和预处理大量的数据。</li><li>需要训练两个图像到图像翻译模型。</li><li>需要对估计的2D-3D密集对应图进行后处理。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d77ba14fed7eddde5b06eaba6ff57afd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77c4e5753a8cd6ab35f73ede239b04a7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8d82762ff5fc78409df5e252c8a6442.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8833866e4d976d23589211a0d2587b35.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3c7c60e43fae13c906596978f0558ac8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68bedb16f322fb5603066efd18ca6348.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3a909299ddc09a5143e9d208d38ac851.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ffb421ba600650f3eb815efb8fb9a80.jpg" align="middle"></details><h2 id="Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion"><a href="#Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion" class="headerlink" title="Animated Stickers: Bringing Stickers to Life with Video Diffusion"></a>Animated Stickers: Bringing Stickers to Life with Video Diffusion</h2><p><strong>Authors:David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu</strong></p><p>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second. </p><p><a href="http://arxiv.org/abs/2402.06088v1">PDF</a> </p><p><strong>Summary</strong><br>文本提出一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</p><p><strong>Key Takeaways</strong></p><ul><li>引入了一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</li><li>该模型建立在最先进的 Emu 文本到图像模型的基础上，并添加了时间层来模拟动作。</li><li>由于视觉和动作风格的差异，在自然视频生成中表现良好的模型在应用于贴纸时无法再生成生动的视频。</li><li>为了弥合这一差距，我们采用了分两阶段进行微调的管道：首先是弱域内数据，其次是人类在回路 (HITL) 策略，我们称之为教师集成。</li><li>该策略使我们能够专门针对运动质量进行改进，同时保持静态图像的风格。</li><li>经过推理优化，我们的模型能够在一秒内生成具有高质量、有趣且相关运动的八帧视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：动画贴纸：使用视频扩散将贴纸变成生动贴纸</li><li>作者：David Yan<em>, Winnie Zhang</em>, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schönfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman†, Licheng Yu†</li><li>第一作者单位：GenAI, Meta Menlo Park, California, USA</li><li>关键词：动画贴纸、视频扩散、文本到视频、图像到视频、人类参与循环</li><li>论文链接：https://arxiv.org/abs/2402.06088，Github 链接：无</li><li>摘要：（1）研究背景：最近，人们对生成文本（和图像）到视频 (T2V) 建模产生了浓厚的兴趣。当前最先进模型生成的视频通常很短（不到 3 秒），并且通常使用文本（文本到视频或 T2V）、图像（图像到视频或 I2V）或两者。在这项工作中，我们使用文本和图像到视频的生成管道来针对短视频生成的自然应用：为社交表达制作动画贴纸。（2）过去的方法及其问题：我们发现，使用通用 I2V 模型（即仅在通用视频数据集上训练的模型）在应用于贴纸时不会产生高质量的运动，并且经常会生成具有静态或微不足道的运动（例如，仅“摆动”效果）和/或引入不一致性和运动伪影（例如，变形）。这是由于自然（逼真）视频与贴纸风格动画之间的视觉和运动差异，即域差距。（3）提出的研究方法：在这项工作中，我们使用人类参与循环 (HITL) 训练策略来弥合域差距。首先，使用数据集和帧采样率的不同“配方”训练了许多“教师”模型，以便教师模型能够集体产生高质量的多样化运动，尽管很少。接下来，通过使用教师模型在涵盖广泛提示集的大型提示集上执行推理来构建 HITL 数据集。然后，使用 HITL 数据集训练一个较小的“学生”模型，该模型可以从教师模型中学习并产生高质量的动画贴纸。（4）方法在任务和性能上的表现：我们的模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频。性能支持其目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种动画贴纸模型，该模型使用时空潜在扩散模型以文本-图像对为条件，将贴纸图像变成动画贴纸。我们的预训练到生产的管道从 Emumodel 开始，该模型在大量自然视频上进行了微调，然后在域内数据集上进行了微调。然后，我们使用教师集合 HITL 微调策略来进一步提高运动质量、一致性和相关性。我们使用许多基于架构、蒸馏的优化和后训练优化来将推理速度提高到每批一秒。我们表明，我们的微调策略显着提高了运动大小和质量，优于仅在自然视频上训练的模型，证明了教师集合的有效性。（2）：创新点：提出了一种使用人类参与循环 (HITL) 训练策略来弥合域差距的方法，该方法可以生成高质量、有趣且相关的运动；性能：该模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频；工作量：该模型的训练过程需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-188a2b4c4ed9e284afed14a8e020b622.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29dcdf079faf656ac8934c9dcb4fe4da.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a85a0fa5d13e8bd37d6352571f52fa54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-73989a294ebc6b241211e4051f9a71db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cee3ea6a017b3b85519c905ebe1d86a3.jpg" align="middle"></details><h2 id="InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset"><a href="#InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset" class="headerlink" title="InstaGen: Enhancing Object Detection by Training on Synthetic Dataset"></a>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</h2><p><strong>Authors:Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma</strong></p><p>In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. </p><p><a href="http://arxiv.org/abs/2402.05937v1">PDF</a> Tech report</p><p><strong>摘要</strong><br>利用扩散模型生成的合成数据集训练物体检测器，可以提高检测性能或扩展类别。</p><p><strong>要点</strong></p><ul><li>将实例级定位头集成到预训练生成扩散模型中，使其能够在生成图像中定位任意实例。</li><li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li><li>将合成数据用于物体检测器的训练可以提高其性能，在开放词表场景中比现有最先进方法提高 4.5 个 AP，在数据稀疏场景中提高 1.2 到 5.2 个 AP。</li><li>InstaGen 是一种新颖的范式，可通过使用扩散模型生成的合成数据集进行训练来增强对象检测器的能力，例如扩展类别或提高检测性能。</li><li>InstaGen 将实例级定位头集成到预训练的生成扩散模型中，使其能够在生成的图像中定位任意实例。</li><li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li><li>InstaGen 作为数据合成器可用于物体检测，在开放词表场景和数据稀疏场景中均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p><strong>标题</strong>：InstaGen：通过合成数据集增强对象检测</p><p></p><p></p><p><strong>作者</strong>：Yuxin Fang, Yifan Zhang, Xiaolin Fang, Xiaohua Shi, Wei Shen, Enhua Wu</p><p></p><p></p><p><strong>第一作者单位</strong>：华中科技大学</p><p></p><p></p><p><strong>关键词</strong>：对象检测，合成数据，扩散模型，实例级接地头</p><p></p><p></p><p><strong>论文链接</strong>：https://arxiv.org/abs/2302.07603</p><p></p><p></p><p><strong>Github 代码链接</strong>：None</p><p></p><p></p><p><strong>摘要</strong>：</p><p></p><p></p><p>本文提出了一种通过合成数据集增强对象检测能力的新范式，该范式通过从扩散模型生成合成数据来扩展检测性能。具体来说，我们将实例级接地头集成到预训练的生成扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。我们通过大量的实验表明，这种称为 InstaGen 的增强版扩散模型可以作为数据合成器来增强对象检测器，并在开放词汇表（+4.6 AP）和数据稀疏（+4.8 AP）上展示出优于现有最先进方法的性能。</p><p></p><p></p><p><strong>总结</strong>：</p><p></p><p></p><p>（一）：研究背景：</p><p></p><p></p><p>对象检测是计算机视觉中的一项基本任务，广泛应用于自动驾驶、人脸识别、医疗图像分析等领域。近年来，随着深度学习的发展，基于深度学习的对象检测方法取得了很大的进展。然而，这些方法通常需要大量的数据进行训练，这在一些领域是难以获得的。</p><p></p><p></p><p>（二）：过去的研究工作：</p><p></p><p></p><p>为了解决数据不足的问题，研究人员提出了各种数据增强技术来扩充训练数据。这些技术包括图像裁剪、翻转、旋转、颜色抖动等。然而，这些技术只能产生有限数量的图像，并且不能保证生成图像的质量。</p><p></p><p></p><p>（三）：本文的问题：</p><p></p><p></p><p>本文认为，现有的数据增强技术不能很好地解决数据不足的问题。因此，本文提出了一种新的数据增强技术，称为 InstaGen，该技术可以生成高质量的合成图像，并且可以保证生成图像的质量。</p><p></p><p></p><p>（四）：本文的方法：</p><p></p><p></p><p>InstaGen 是一种基于扩散模型的数据增强技术。扩散模型是一种生成模型，可以从噪声生成图像。InstaGen 将一个实例级接地头集成到预训练的扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。</p><p></p><p></p><p>（五）：本文的实验结果：</p><p></p><p></p><p>本文在 PASCAL VOC 和 COCO 数据集上对 InstaGen 进行了评估。实验结果表明，InstaGen 可以有效地增强对象检测器的性能。在 PASCAL VOC 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.6 个百分点。在 COCO 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.8 个百分点。</p><p></p><ol><li><p>方法：（1）：构建图像合成器：采用预训练的 Stable Diffusion 模型，并使用检测数据集对模型进行微调，以生成具有真实感且包含指定类别的图像。（2）：引入实例级接地头：设计一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。（3）：监督学习和自训练：使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。（4）：数据合成器生成合成数据集：使用训练好的接地头和图像合成器，生成包含对象实例及其边界框的合成数据集。（5）：在合成数据集上训练对象检测器：将合成数据集与真实数据集相结合，训练对象检测器，以提高检测性能。</p></li><li><p>结论：（1）：本文提出了一种称为InstaGen的数据集合成管道，该管道能够为任意类别生成具有对象边界框的图像，作为构建大规模合成数据集以训练对象检测器的免费资源。我们进行了详尽的实验，以展示在合成数据上训练的有效性，以提高检测性能或扩展检测类别的数量。在各种检测场景中，包括开放词汇表（+4.5AP）和数据稀疏（+1.2∼5.2AP）检测中，都取得了显着的改进。（2）：创新点：</p></li><li>提出了一种新的数据合成管道InstaGen，该管道能够为任意类别生成具有对象边界框的图像。</li><li>设计了一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。</li><li>使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。性能：</li><li>在PASCAL VOC和COCO数据集上，InstaGen将Faster R-CNN的AP提高了4.6个百分点和4.8个百分点。</li><li>在开放词汇表和数据稀疏检测中，InstaGen取得了显着的改进。工作量：</li><li>InstaGen是一种数据合成管道，需要预训练的扩散模型和实例级接地头。</li><li>InstaGen需要大量的数据来训练接地头。</li><li>InstaGen需要大量的时间来生成合成数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e5bc75a4d614b9abf0055ef9f09e29eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcaa5f4430aaa302f904c1eb77cd432c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ed1d3b41f15d36193b946e6064581300.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6998a66afc9f7f895bfb98faa0596297.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85a17fc9e78759363117b1e3dbd18da2.jpg" align="middle"></details><h2 id="Scalable-Diffusion-Models-with-State-Space-Backbone"><a href="#Scalable-Diffusion-Models-with-State-Space-Backbone" class="headerlink" title="Scalable Diffusion Models with State Space Backbone"></a>Scalable Diffusion Models with State Space Backbone</h2><p><strong>Authors:Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</strong></p><p>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a>. </p><p><a href="http://arxiv.org/abs/2402.05608v1">PDF</a> </p><p><strong>摘要</strong><br>利用状态空间架构构建的新型扩散模型，在图像数据上实现可与 U 形卷积神经网络架构媲美的性能，并具有良好的可扩展性。</p><p><strong>要点</strong></p><ul><li>基于状态空间架构的扩散模型在图像数据生成任务上表现良好，可与基于 U 形卷积神经网络或基于 Transformer 的 U 形卷积神经网络架构实现相当的性能，甚至优于它们。</li><li>扩散模型的状态空间模型将时间、条件和噪声图像块等所有输入都视为标记。</li><li>扩散模型的状态空间模型在无条件图像生成和类别条件图像生成场景中均表现良好。</li><li>通过增加深度/宽度或增加输入标记，扩散模型的状态空间模型的正向传递复杂度（以 Gflops 为单位）更高，并且始终表现出更低的 FID。</li><li>扩散模型的状态空间模型具有良好的可扩展性。</li><li>在分辨率为 256×256 和 512×512 的类别条件 ImageNet 基准上，扩散模型的状态空间模型在潜在空间中实现了与先前扩散模型相当的性能，同时大幅降低了计算负担。</li><li>扩散模型的状态空间模型代码和模型可在 <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a> 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于状态空间的可扩展扩散模型</li><li>作者：Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</li><li>第一作者单位：昆仑科技</li><li>关键词：扩散模型、状态空间、可扩展性、图像生成</li><li>论文链接：https://arxiv.org/abs/2402.05608，Github 链接：https://github.com/feizc/DiS</li><li>摘要：(1)：研究背景：扩散模型作为强大的深度生成模型，近年来在图像生成领域取得了显著进展，广泛应用于文本到图像生成、图像到图像生成、视频生成、语音合成和 3D 合成等领域。扩散模型的发展离不开采样算法和模型骨干的进步，其中 U-Net 是扩散模型中常用的骨干网络，但其在处理长程依赖关系方面存在局限性。(2)：过去方法与问题：传统的扩散模型骨干网络，如 U-Net，在处理长程依赖关系方面存在局限性。为了解决这一问题，本文提出了基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。(3)：研究方法：本文提出的 DiS 模型具有以下特点：</li><li>将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。</li><li>DiS 模型可以处理原始图像块或潜在空间中的标记。</li><li><p>DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。(4)：实验结果与性能：本文在无条件和类条件图像生成任务上对 DiS 模型进行了评估，结果表明 DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。此外，本文还分析了 DiS 模型的可扩展性，结果表明 DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。</p></li><li><p>方法：（1）：提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。（2）：DiS模型可以处理原始块或潜在空间中的标记。（3）：DiS模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。</p></li><li><p>结论：（1）：本工作提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。DiS 采用了一种统一的方法来处理所有输入，包括时间、条件和噪声图像块，将它们视为连接的标记。实验结果表明，DiS 与基于 CNN 或 Transformer 的 U-Net 模型相比具有相当或更好的性能，同时继承了状态空间模型类的显着可扩展性特征。我们认为 DiS 可以为未来研究扩散模型中的骨干网络提供有价值的见解，并有助于推进大规模多模态数据集中的生成建模。鉴于本研究中提出的令人鼓舞的可扩展性结果，未来的努力应集中在将 DiS 进一步扩展到更大的模型和标记计数上。（2）：创新点：DiS 模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记，这是一种新的方法，可以有效地处理长程依赖关系。DiS 模型可以处理原始块或潜在空间中的标记，这使得它可以应用于各种图像生成任务。DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。性能：在无条件和类条件图像生成任务上，DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。工作量：DiS 模型的训练和推理成本与基于 CNN 或 Transformer 的 U-Net 模型相当。DiS 模型的可扩展性使得它可以应用于各种图像生成任务，包括高分辨率图像生成、视频生成和 3D 合成。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6bb4b2235878abe86e04f19f24047beb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1d4bb00838a5fb623fcc9eb998c2c6b9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-adf0dc0a97f9ca167de7eccda01fe6df.jpg" align="middle"></details><h2 id="SPAD-Spatially-Aware-Multiview-Diffusers"><a href="#SPAD-Spatially-Aware-Multiview-Diffusers" class="headerlink" title="SPAD : Spatially Aware Multiview Diffusers"></a>SPAD : Spatially Aware Multiview Diffusers</h2><p><strong>Authors:Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</strong></p><p>We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a> </p><p><a href="http://arxiv.org/abs/2402.05235v1">PDF</a> Webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a></p><p><strong>Summary</strong><br>跨视角图像生成模型 SPAD：自我注意和空间编码的结合，实现文本到图像生成的一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>SPAD 是一种新的方法，可以从文本提示或单个图像生成一致的多视角图像。</li><li>SPAD 是通过扩展预训练的 2D 扩散模型的自注意力层来实现多视角生成，并对 Objaverse 的高质量子集进行微调。</li><li>SPAD 显示，先前的研究提出的自我注意的朴素扩展（例如 MVDream）导致视角之间的内容复制。</li><li>SPAD 显式地限制基于极线几何的跨视角注意。</li><li>SPAD 利用从相机射线派生的 Plücker 坐标，并将它们注入作为位置编码，以进一步增强 3D 一致性。</li><li>与只能在固定方位角和仰角生成视图的最近工作相比，SPAD 提供了完全的相机控制，并在 Objaverse 和 Google Scanned Objects 数据集上看不见的物体的新颖视图合成中实现了最先进的结果。</li><li>使用 SPAD 进行文本到 3D 生成消除了多面 Janus 问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SPAD：空间感知多视图扩散器</li><li>作者：Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</li><li>第一作者单位：多伦多大学</li><li>关键词：多视图生成、扩散模型、文本到 3D</li><li>论文链接：https://yashkant.github.io/spad/，Github 代码链接：None</li><li><p>摘要：（1）研究背景：多视图生成是指从文本提示或单个图像生成一组在 3D 空间中一致的图像。这对于许多应用很有用，例如增强现实、虚拟现实和 3D 建模。（2）过去方法：现有方法通常使用 2D 扩散模型来生成多视图图像。然而，这些方法通常会导致视图之间出现不一致，例如对象形状或纹理不匹配。（3）研究方法：本文提出了一种新的多视图生成方法 SPAD。SPAD 通过在 2D 扩散模型中引入跨视图交互来实现多视图生成。此外，SPAD 还利用了 Plücker 坐标来增强 3D 一致性。（4）方法性能：SPAD 在 Objaverse 和 Google Scanned Objects 数据集上进行了评估。结果表明，SPAD 在新视图合成任务上优于现有方法。此外，SPAD 还能够防止多面 Janus 问题，即生成的图像在不同视图中具有不同的外观。</p></li><li><p>方法：(1): SPAD的核心思想是将多视图生成问题转化为一个扩散模型问题。SPAD使用一个2D扩散模型来生成每个视图的图像，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。(2): SPAD使用Plücker坐标来表示3D空间中的点。Plücker坐标具有不变性，这意味着它们不受视角和投影变换的影响。SPAD利用Plücker坐标来增强3D一致性，并防止多面Janus问题。(3): SPAD在Objaverse和GoogleScannedObjects数据集上进行了评估。结果表明，SPAD在新视图合成任务上优于现有方法。此外，SPAD还能够防止多面Janus问题。</p></li><li><p>结论：（1）：SPAD是一种新颖的多视图生成框架，它将文本或图像输入转换为多个视图。SPAD在预训练的文本到图像扩散模型的自注意力层中引入了极线注意力，以促进多视图交互并改进相机控制。此外，SPAD使用Plücker位置编码增强了自注意力层，以通过防止对象的翻转视图预测来进一步改进相机控制。SPAD在Objaverse和GoogleScannedObjects数据集上进行了严格的评估，并在图像条件的新视图合成方面展示了最先进的结果。（2）：创新点：</p></li><li>将多视图生成问题转化为扩散模型问题，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。</li><li>使用Plücker坐标来表示3D空间中的点，并利用Plücker坐标来增强3D一致性，防止多面Janus问题。性能：</li><li>在Objaverse和GoogleScannedObjects数据集上，SPAD在新视图合成任务上优于现有方法。</li><li>SPAD能够防止多面Janus问题，即生成的图像在不同视图中具有不同的外观。工作量：</li><li>SPAD的实现相对简单，并且可以在PyTorch中轻松实现。</li><li>SPAD的训练过程相对快速，并且可以在单个GPU上完成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-afe3524a8f81d817d06d1d9498a1728a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a80a51acf35ce9d57c5584647e5cca12.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-13  Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/</id>
    <published>2024-02-09T02:20:12.000Z</published>
    <updated>2024-02-09T02:20:12.523Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering"><a href="#NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering" class="headerlink" title="NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering"></a>NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</h2><p><strong>Authors:Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao</strong></p><p>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.04829v1">PDF</a> Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a></p><p><strong>摘要</strong><br>神经辐射场可以作为空间非距离环境光源，用于物理逆渲染，使逆渲染更加真实准确。</p><p><strong>主要要点</strong></p><ul><li>基于物理的逆渲染旨在联合优化从捕获的 2D 图像中提取的形状、材质和光照。</li><li>在逆渲染中，通常使用环境贴图作为光照模型，但这种假设会导致空间不变的光照，这在真实世界的逆渲染中可能是不准确的近似。</li><li>提出使用神经辐射场作为空间可变的环境光照模型，并构建了一个以神经辐射场作为非距离环境光源的逆渲染管道。</li><li>将方法与环境贴图在真实和合成数据集上进行比较，结果表明，基于神经辐射场的光源可以更准确地模拟场景光照，并实现更准确的逆渲染。</li><li>项目页面和视频：<a href="https://nerfemitterpbir.github.io/。">https://nerfemitterpbir.github.io/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于物理的反演渲染中，NeRF 作为非远处环境发射器</li><li>作者：Jingwang Ling、Ruihan Yu、Feng Xu、Chun Du、Shuang Zhao</li><li>单位：清华大学</li><li>关键词：NeRF、物理反演渲染、环境光照、形状重建</li><li>论文链接：https://arxiv.org/pdf/2402.04829.pdf，Github 链接：None</li><li><p>摘要：（1）研究背景：基于物理的反演渲染旨在从捕获的 2D 图像中联合优化形状、材质和光照。其中，光照是实现真实光照传输模拟的重要组成部分。环境贴图是反演渲染中常用的光照模型，但我们发现，在光源不是无限远处的场景中，环境贴图的空间不变光照假设会导致空间不变的光照，这在现实世界中的反演渲染中可能是不准确的近似。（2）过去方法及问题：过去的方法通常使用环境贴图来近似物体周围的光照，但这种方法在光源不是无限远处的场景中会导致不准确的结果。（3）研究方法：我们提出使用 NeRF 作为空间变化的环境光照模型，并构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道。通过在真实和合成数据集上与环境贴图进行比较，我们证明了我们的 NeRF 模型可以更准确地模拟场景光照，并实现更准确的反演渲染。（4）方法性能：我们的方法在真实和合成数据集上都取得了比环境贴图更好的结果。在真实数据集上，我们的方法在重照明和形状重建任务上都取得了更好的性能。在合成数据集上，我们的方法在重照明、形状重建和材质估计任务上都取得了更好的性能。这些结果证明了我们的方法可以更准确地模拟场景光照，并实现更准确的反演渲染。</p></li><li><p>方法：（1）提出使用 NeRF 作为空间变化的环境光照模型，构建以 NeRF 作为非远处环境发射器的反演渲染管道。（2）利用真实和合成数据集，与环境贴图进行比较，证明 NeRF 模型可以更准确地模拟场景光照，实现更准确的反演渲染。（3）在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。</p></li><li><p>结论：(1)：本文提出了一种基于 NeRF 的反演渲染管道，该管道将 NeRF 用作非远处环境发射器，可以更准确地模拟场景光照，并实现更准确的反演渲染。(2)：创新点：</p></li><li>使用 NeRF 作为空间变化的环境光照模型，可以更准确地模拟场景光照。</li><li>构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道，可以实现更准确的反演渲染。</li><li>在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。性能：</li><li>在真实数据集上，在重照明和形状重建任务上都取得了更好的性能。</li><li>在合成数据集上，在重照明、形状重建和材质估计任务上都取得了更好的性能。工作量：</li><li>需要训练 NeRF 模型，这可能需要大量的数据和计算资源。</li><li>需要构建反演渲染管道，这可能需要大量的编程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5217f666aff1dcbbc55e20cda0c76080.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9fa354da141f8905a59ea4a06f90f25.jpg" align="middle"></details><h2 id="OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding"><a href="#OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding" class="headerlink" title="OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding"></a>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li</strong></p><p>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness. </p><p><a href="http://arxiv.org/abs/2402.04648v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）技术通过结合视觉和语言基础模型，提升了NeRF在开放词汇表3D语义感知中的表现。</p><p><strong>Key Takeaways</strong></p><ul><li>OV-NeRF 提出了一种单视图和跨视图策略，将NeRF用于开放词汇表3D语义感知。</li><li>利用 SAM 提取的 2D 掩模建议，引入区域语义排序 (RSR) 正则化，以纠正每个训练视图的语义噪声。</li><li>提出跨视图自增强 (CSE) 策略，利用训练语义场本身生成的 3D 一致语义，减少语义模糊性和增强语义一致性。</li><li>OV-NeRF 在 Replica 和 Scannet 数据集上分别实现了 20.31% 和 18.42% 的 mIoU 指标提升，优于现有最优方法。</li><li>OV-NeRF 在各种 CLIP 配置下均表现出优异的性能，验证了其鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：OV-NeRF：具有视觉和语言的开放词汇神经辐射场</li><li>作者：廖桂标，周凯晨，鲍振宇，刘康林，李庆</li><li>单位：北京大学</li><li>关键词：神经辐射场，开放词汇，语义理解，视觉语言模型</li><li>链接：https://arxiv.org/abs/2402.04648</li><li><p>摘要：(1) 研究背景：神经辐射场（NeRF）是一种强大的表示方法，可以捕捉复杂真实世界的 3D 场景。然而，在开放词汇 3D 语义感知任务中实现全面的 3D 语义理解仍然是一个具有挑战性的问题。(2) 过去的方法：过去的方法直接从对比视觉语言预训练（CLIP）中提取语义，用于语义场学习，但遇到了来自 CLIP 的嘈杂和视图不一致语义的困难。(3) 研究方法：为了解决这些限制，本文提出了 OV-NeRF，它利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。(4) 性能表现：OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。此外，该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。</p></li><li><p>方法：(1) 提出 OV-NeRF，利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。(2) 利用预先计算的 CLIP 特征和 SAM 的区域提议来生成精确的相关性图，以监督 OV-NeRF，而不是使用源自 CLIP 模型的原始噪声相关性图。(3) 在训练 OV-NeRF 数个 epoch 后，利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。</p></li><li><p>结论：（1）：本工作通过利用视觉语言基础模型的能力，提出 OV-NeRF 来解决基于 NeRF 的 3D 语义理解挑战。在 OV-NeRF 中，提出的区域语义排序（RSR）正则化产生精确的单视图相关性图来训练 OV-NeRF，跨视图自我增强确保视图一致的分割结果。实验结果表明，我们的方法在合成和真实世界基准数据集上以很大优势优于 SOTA 方法，显示了我们方法的优越性。此外，我们的方法在不同的 CLIP 配置中始终表现出优异的性能，肯定了其通用性。（2）：创新点：提出了一种新的 NeRF 模型 OV-NeRF，该模型利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。提出了一种新的区域语义排序（RSR）正则化，该正则化产生精确的单视图相关性图来训练 OV-NeRF。提出了一种新的跨视图自我增强方法，该方法利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。性能：OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。工作量：该方法需要预先计算 CLIP 特征和 SAM 的区域提议，这可能会增加计算成本。该方法需要训练多个 epoch，这可能会增加训练时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d28a855be0d118e883bd9f8001dbbcd1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1c6219c40ef2be88e25422dda1aae264.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fece26674b484110bc1b8871018a6a3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ea1c3e14317a591427313451f7980698.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a1bd1a1dc370f614b943567738833593.jpg" align="middle"></details><h2 id="GSN-Generalisable-Segmentation-in-Neural-Radiance-Field"><a href="#GSN-Generalisable-Segmentation-in-Neural-Radiance-Field" class="headerlink" title="GSN: Generalisable Segmentation in Neural Radiance Field"></a>GSN: Generalisable Segmentation in Neural Radiance Field</h2><p><strong>Authors:Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan</strong></p><p>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: <a href="https://vinayak-vg.github.io/GSN/">https://vinayak-vg.github.io/GSN/</a> </p><p><a href="http://arxiv.org/abs/2402.04632v1">PDF</a> Accepted at the Main Technical Track of AAAI 2024</p><p><strong>Summary</strong><br>利用几个视图就可以渲染未知且未训练场景的新视图，并提供一致的逐像素语义特征。</p><p><strong>Key Takeaways</strong></p><ul><li>传统辐射场表示捕获特定场景的细节，必须在每个场景上重新训练。</li><li>语义特征字段已添加到射频中以促进多项分割任务。</li><li>广义射频表示学习了视图插值原理。</li><li>给定几个视图，广义射频可以渲染未知且未训练场景的新视图。</li><li>我们提供了一种将特征字段提炼成广义 GNT 表示的方法。</li><li>我们的 GSN 表示可以快速生成未见场景的新视图，并提供一致的逐像素语义特征。</li><li>这允许对任意新场景进行多视图分割。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GSN：神经辐射场中的可泛化分割</li><li>作者：Vinayak Gupta，Rahul Goel，Sirikonda Dhawal，P.J. Narayanan</li><li>隶属机构：印度理工学院马德拉斯分校</li><li>关键词：神经辐射场、可泛化分割、语义特征场</li><li>论文链接：https://arxiv.org/abs/2402.04632   Github 链接：无</li><li><p>摘要：（1）研究背景：传统的神经辐射场（RF）表示可以捕捉特定场景的细节，但必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。（2）过去的方法及其问题：本文提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。（3）研究方法：过去的 RF 表示学习特定场景的细节，必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。我们提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。（4）方法的性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。这些性能支持了我们的目标。</p></li><li><p>方法：（1）首先，我们修改 GNT 架构以帮助语义特征提取。（2）然后，我们描述了我们的两阶段训练蒸馏过程。（3）最后，我们描述了如何使用蒸馏特征执行多视图分割。</p></li><li><p>结论：（1）：本文提出了一种新的多视图分割方法，其主要优势在于其泛化性，即它可以在任意新场景上执行分割而无需任何训练。这使其区别于以前的方法。我们将我们的结果与早期方法进行了比较，并表明我们的性能与它们相当，同时可以泛化到未见场景。这是将泛化神经辐射场的应用拉近到特定场景辐射场的一大步。我们方法预测的特征可用于多种下游任务。（2）：创新点：提出了将特征场提炼到泛化的 GNT 表示中的方法，该表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。工作量：我们的方法依赖于基于 transformer 的架构，因此渲染过程与几种特定场景的辐射场方法相比固有地缓慢。提高渲染速度可以显着改善我们基于笔划的分割方法所需的人机交互体验。我们将泛化辐射场的渲染速度改进留作未来的工作。目前，我们的方法执行多视图分割，因为它使用基于图像的渲染。某些应用程序需要 3D 分割而不是多视图分割。因此，可泛化的 3D 分割框架有望成为未来的工作。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bf67c21104c6d20a1d6e37e83bff2155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03400222552085971945e9fc363dc323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61621673ca99816fe4332d9623a7e1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c82ea98993102ebb08c3d96886f8caf8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d9b9a069535dfb6e09a7654648b4f040.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v1">PDF</a> </p><p><strong>摘要</strong><br>鸟瞰 NeRF：基于神经辐射场的大规模场景重建。</p><p><strong>要点</strong></p><ul><li>针对大规模场景重建，提出了基于神经辐射场的 BirdNeRF 算法。</li><li>BirdNeRF 将大场景图像集分解为多个小集合，每个小集合训练单独的 NeRF 模型。</li><li>这种分解方法将渲染时间与场景大小解耦，并使渲染能够无缝扩展到任意大的环境。</li><li>此外，它允许对环境进行逐块更新，从而提高重建过程的灵活性和适应性。</li><li>提出了一种基于投影的新颖视角重新渲染策略，有助于有效利用独立训练的子场景生成更好的渲染结果。</li><li>在现有数据集以及我们自己的无人机航拍视频上评估了我们的方法，在单个 GPU 上将重建速度提高了 10 倍（相对于经典摄影测量软件）和 50 倍（相对于最先进的大规模 NeRF 解决方案），同时渲染质量相似。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鸟瞰神经辐射场：使用航拍图像快速神经重建大场景</li><li>作者：张慧清、薛一飞、廖明、老一真</li><li>单位：无</li><li>关键词：神经辐射场、大规模重建、航拍图像、空间分解、投影引导</li><li>链接：https://arxiv.org/abs/2402.04554Github：无</li><li><p>摘要：(1)：随着航空测量技术的进步，获取高分辨率图像变得更加容易和经济实惠，基于图像的 3D 重建已成为一个活跃的研究领域，并在城市规划、导航、虚拟旅游、房地产和灾害管理等领域有着广泛的应用。(2)：现有的基于图像的 3D 重建技术主要分为传统的基于几何的方法和基于神经网络的方法。基于几何的方法通常需要大量的人工干预，并且对图像的质量和数量非常敏感。基于神经网络的方法，例如神经辐射场 (NeRF)，可以自动从图像中学习场景的 3D 表示，并且对图像的质量和数量不太敏感。然而，NeRF 在处理大规模场景时面临着训练速度慢、渲染时间长和容易产生伪影等挑战。(3)：为了解决这些挑战，本文提出了一种新的 NeRF 变体，称为鸟瞰神经辐射场 (BirdNeRF)。BirdNeRF 使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的 NeRF 模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。(4)：在多个数据集上的实验结果表明，BirdNeRF 在重建速度和质量方面都优于现有的方法。在单个 GPU 上，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大规模 NeRF 解决方案快 50 倍，同时具有相似的渲染质量。</p></li><li><p>方法：(1) 将大场景分解为多个较小的子场景，分别训练每个子场景的 NeRF 模型。(2) 使用一种新的投影引导的新视图重新渲染策略，有效地利用独立训练的子场景来生成高质量的渲染结果。</p></li><li><p>结论：（1）：本文提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。BirdNeRF使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。（2）：创新点：</p></li><li>提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。</li><li>提出了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。</li><li>提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。</li></ol><p>性能：- 在单个GPU上，BirdNeRF的重建速度比传统的摄影测量软件快10倍，比最先进的大规模NeRF解决方案快50倍，同时具有相似的渲染质量。- BirdNeRF可以重建包含数百万个三角形的场景，而不会出现明显的伪影。</p><p>工作量：- BirdNeRF的实现相对简单，易于使用。- BirdNeRF的训练时间和渲染时间都比较短，可以满足实际应用的需求。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0ffe2746a28f7248c7dc45305ca5a0d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3e3e28cf5dd4b506a44e1769d5abf0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51dea38443c497692956a6fd50ec6a18.jpg" align="middle"></details>## ViewFusion: Learning Composable Diffusion Models for Novel View   Synthesis**Authors:Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin**Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available. [PDF](http://arxiv.org/abs/2402.02906v1) **Summary**将多个不同视角的图像输入到 ViewFusion 模型中，就可以基于这些图像合成出新的视角图像。**Key Takeaways**- ViewFusion 将扩散去噪步骤同时应用于任意数量的场景输入视图，然后将每个视图获得的噪声梯度与像素权重掩码相结合，确保在目标场景的每个区域内仅考虑最具信息性的输入视图。- ViewFusion 解决了先前方法的几个局限性：跨多个场景和对象类别进行训练和泛化；在训练和测试时自适应地采用可变数量的不受姿势限制的视图；能够生成合理的视图，即使在严重不确定的条件下。- ViewFusion 优于或与最先进的方法相比能更高质量地生成视图。- ViewFusion 无法生成场景的 3D 嵌入，导致其推理速度相对较慢。- ViewFusion 目前仅在相对较小的 NMR 数据集上进行了测试。- 代码库现已发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ViewFusion：用于新颖视图合成的可组合扩散模型的学习</li><li>作者：Bernard Spiegl、Andrea Perin、St´ephane Deny、Alexander Ilin</li><li>第一作者单位：阿尔托大学计算机科学系（仅翻译中文）</li><li>关键词：新颖视图合成、扩散模型、可组合性、自适应输入视图、鲁棒性</li><li>论文链接：https://arxiv.org/abs/2402.02906，Github 代码链接：None</li><li><p>摘要：（1）研究背景：新颖视图合成是一个计算机视觉领域的长期研究课题。传统方法通常使用显式建模 3D 空间的方法，如体素、点云或网格。近年来，基于神经辐射场 (NeRF) 的方法也取得了很大进展。然而，这些方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。（2）过去方法及问题：过去的方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。因此，本文旨在提出一种直观的端到端架构，用于执行新颖视图合成，同时解决先前工作中提到的缺点。（3）研究方法：本文提出的 ViewFusion 方法通过一系列针对特定问题的设计选择，一次性解决了上述缺点。首先，使用在大量场景和类别上同时训练的扩散概率框架，使其能够在无需重新训练的情况下进行泛化。此外，由于扩散过程的随机性质，该模型即使在不确定性设置（例如，对象的严重遮挡或有限数量的输入视图）中也能表现良好，因为它提供了多种合理的视图。此外，本文提出的解决方案不需要输入视图的顺序或任何显式姿态信息。最后，与之前的对应方法不同，一旦训练完成，本文的方法就可以有效地处理任意长度的输入。这要归功于一种新的加权解决方案，与去噪骨干网络的组合一起，该解决方案允许模型根据视图的信息量对视图进行加权，同时扩展到任意数量的视图。（4）方法性能：本文在包含各种类别和输入视图姿势的数据集上评估了所提出的方法。此外，本文通过对中间模型输出的分析验证了该方法，证明了该模型能够推断和自适应地调整每个输入视图的重要性。加权不仅对输出的质量有影响，而且推断的加权方案也与直观的人类感知一致。</p></li><li><p>方法：(1) 提出了一种基于扩散概率框架的新颖视图合成方法 ViewFusion，该方法能够同时处理多个输入视图，并根据每个视图的重要性对视图进行加权，从而生成高质量的新颖视图。(2) ViewFusion 模型由多个 U-Net 组成，每个 U-Net 负责处理一个输入视图。U-Net 的输出包括噪声预测和权重，权重用于对噪声进行加权，从而生成最终的新颖视图。(3) ViewFusion 模型的训练过程包括两个阶段：预训练和微调。在预训练阶段，模型在大量场景和类别上进行训练，以学习一般化的特征表示。在微调阶段，模型在特定场景或类别上进行微调，以提高模型的性能。(4) ViewFusion 模型的推理过程包括两个阶段：噪声采样和扩散过程。在噪声采样阶段，模型从正态分布中采样噪声。在扩散过程中，模型通过逐渐降低噪声的强度来生成新颖视图。</p></li><li><p>结论：（1）：ViewFusion 是一种灵活、无需姿态的生成方法，可使用可组合扩散模型执行新颖视图合成。我们提出了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。我们认为，我们的方法在进行新颖视图合成时是一个有价值的贡献，并且有可能应用于其他问题。（2）：创新点：ViewFusion 引入了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。性能：ViewFusion 在各种类别和输入视图姿势的数据集上均取得了最先进的性能。此外，ViewFusion 能够有效地处理任意数量的输入视图，并且对输入视图的顺序和姿态信息不敏感。工作量：ViewFusion 的训练过程包括两个阶段：预训练和微调。预训练阶段需要大量的数据和计算资源，但微调阶段可以相对快速地完成。ViewFusion 的推理过程也非常有效，可以在几秒钟内生成新颖视图。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-672b204f9242001f6ba5e1b350c81c87.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab2417ac343ade4b32aea1621299f294.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a35b2635715a736813769f26b2939948.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-512893851e477e6cab6fb9d3224f7acf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fa2f794caefa6d02e53b7a03fc9f646.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5e41e289131352d483b38fb05ca0ce8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1874aa5e890d55cc56f18c742397f3bf.jpg" align="middle"></details><h2 id="Robust-Inverse-Graphics-via-Probabilistic-Inference"><a href="#Robust-Inverse-Graphics-via-Probabilistic-Inference" class="headerlink" title="Robust Inverse Graphics via Probabilistic Inference"></a>Robust Inverse Graphics via Probabilistic Inference</h2><p><strong>Authors:Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</strong></p><p>How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks. </p><p><a href="http://arxiv.org/abs/2402.01915v1">PDF</a> </p><p><strong>Summary</strong><br>新颖的贝叶斯方法 RIG 可同时对场景和破坏进行推理，以克服各种场景损坏。</p><p><strong>Key Takeaways</strong></p><ul><li>RIG 是一种新的贝叶斯方法，可同时对场景和破坏进行推理。</li><li>RIG 仅使用干净的数据进行训练，优于深度估计器和替代的 NeRF 方法。</li><li>RIG 可与多种基于正则化流和扩散模型的场景先验架构一起使用。</li><li>对于后者，我们开发了具有辅助潜变量的重建指导（ReGAL）——一种扩散调节算法，适用于具有辅助潜变量（如破坏）的情况。</li><li>RIG 演示了场景先验如何用于生成任务之外。</li><li>RIG 利用强大的场景先验和无信息的均匀破坏先验，使其适用于广泛的破坏。</li><li>在给定单一图像的情况下，RIG 对场景和破坏进行后验推理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鲁棒逆向图形生成：基于概率推理</li><li>作者：Tuan Anh Le、Pavel Sountsov、Matthew D. Hoffman、Ben Lee、Brian Patton、Rif A. Saurous</li><li>单位：谷歌（Google）</li><li>关键词：鲁棒逆向图形生成、神经辐射场、概率推理、域随机化、数据增强</li><li>论文链接：https://arxiv.org/abs/2402.01915</li><li>摘要：</li></ol><p>（1）研究背景：    * 在存在雨、雪、雾等干扰的情况下，如何从单张图像中推断出 3D 场景？    * 直接的域随机化依赖于提前知道干扰的种类。</p><p>（2）过去的方法及其问题：    * 域随机化：通过在数据生成过程中选择一系列干扰来实现鲁棒性，但这种方法需要提前知道干扰的种类。    * 正则化训练：通过在重建损失中添加额外的损失项来实现鲁棒性，但这种方法难以扩展到更极端的情况。</p><p>（3）本文提出的研究方法：    * 鲁棒逆向图形生成（RIG）：将问题视为概率推理问题，利用预训练的场景先验（在本例中是神经辐射场先验）和一个关于干扰的弱先验（在本例中是具有均匀先验权重的另一个神经辐射场）来进行推理。    * RIG 在场景和干扰神经辐射场中执行完整的概率推理，而不是像最大后验概率推理那样寻找最可能的解。</p><p>（4）方法的性能表现：    * RIG 在具有各种场景先验架构（基于正则化流和扩散模型）的情况下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。    * RIG 仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。</p><ol><li><p>方法：（1）场景表示：我们使用神经辐射场 (NeRF) 表示，因为它易于进行基于梯度的推理。（2）场景先验：我们假设我们有一个预训练的 NeRF 先验 p(x)，我们可以从中对场景潜在变量 x 进行采样，并从不同的视点 ζ 渲染图像 y。（3）损坏表示和先验：我们关注的是对 3D 场景的损坏，例如漂浮物或天气伪影（如雨、雪或雾），尽管我们的方法可以泛化到传感器损坏，如相机内部噪声（第 6.1 节）。我们将 3D 损坏表示为另一个 NeRF 的参数。与场景 x 不同，我们不需要对 c 有一个强先验。在我们的实验中，我们假设一个不适当的先验 p(c)∝1。这意味着我们不需要预先知道损坏的种类；损坏可以是任何 3D 实体，从天气伪影和漂浮物到不需要的对象。（4）似然：为了给定场景潜在变量 x 和损坏 c 渲染图像 y，我们组合各自的 NeRF 输出。对于光线位置和方向 (xr, dr)，我们将场景 NeRF (γz, σz) 和损坏 NeRF (γc, σc) 的输出组合为 σ = σz + σc，γ = (γzσz + γcσc)/σ（Niemeyer &amp; Geiger，2021）。我们将组合的 NeRF 的渲染表示为 y = R(x, c)。似然是一个逐像素和逐通道的高斯分布 p(y|x, c) = ∏像素和通道j N(yij|R(x, c)ij, σ2y)，其中 σ2y 是观测噪声方差。（5）MAP 推理不够：推断场景 x 的一种直接方法是找到最大化 p(x)p(c)p(y|x, c) 的 MAP 估计 (x<em>, c</em>)。然而，这种方法会导致“广告牌”解决方案，其中损坏最终解释了场景，就像一个放置在相机前面的广告牌。（6）完全后验推理就足够了：在 RIG 中，我们执行完全后验推理以获得潜在场景 x, c∼p(x, c|y) ∝ p(x)p(c)p(y|x, c)，这在经验上可以避免广告牌解决方案（第 6.1 节）。直观地说，这可以看作是模式与典型集不同的一个实例。损坏完全覆盖场景的模式周围区域具有高密度但低体积——没有许多损坏可以精确地渲染到观测图像。另一方面，后验同时考虑密度和体积，集中在具有高概率质量的区域——有许多非广告牌损坏与正确的场景一起渲染到观测图像，尽管每个这样的解决方案可能具有低密度。（7）变分推理：我们使用变分推理，其中我们优化证据下界 (ELBO) 关于引导分布 q(x, c)：ELBO(q) = Eq(x, c)[logp(y, x|c) - logq(x, c)]。（8）扩散场景先验：去噪扩散已成为正则化流的有力替代方案。虽然可以直接用基于扩散的先验替换 ProbNeRF 中使用的 RealNVP（例如 Dupontet al.，2022），但扩散模型允许我们可追踪地增加我们的潜在表示的维数。高维潜在空间能够进行高保真采样和重建。我们构建了单级扩散 NeRF (SSDNeRF) 框架（Chen et al.，2023）来训练场景先验。SSDNeRF 优化了一组针对每个训练示例的潜在变量 {xn}，也称为 GLO 潜在变量（Bojanowski et al.，2018），由 ϕ 参数化的扩散先验 pϕ(x) 和由 ψ 参数化的似然 pψ(y|x)。有关更多详细信息，请参见附录 D。（9）扩散模型：扩散模型是一个潜在变量生成模型，包含正向和反向过程。正向扩散过程 q(z|x) 从数据 x 开始。</p></li><li><p>结论：(1)：本文提出了一种鲁棒逆向图形生成（RIG）方法，该方法将问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。(2)：创新点：</p></li><li>将逆向图形生成问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。</li><li>提出了一种鲁棒逆向图形生成（RIG）方法，该方法在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li><li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。性能：</li><li>RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li><li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。工作量：</li><li>RIG需要预训练一个场景先验和一个关于干扰的弱先验。</li><li>RIG需要执行完整的概率推理，这比执行点估计或最大后验概率推理更耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-25f26b8c4a059fad96179f9402d4ddf8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b464c110b8bfcce608856052d9518e4b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1f7396fa7b1ad32dc9c645595746950b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c6960210c2e3765f8601fd7fb69b4ba.jpg" align="middle"></details><h2 id="HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation"><a href="#HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation" class="headerlink" title="HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation"></a>HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation</h2><p><strong>Authors:Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek</strong></p><p>Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study. </p><p><a href="http://arxiv.org/abs/2402.01524v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场方法对于少数基础图像合成新奇3D物体视图有着广泛的认可，却存在泛化性质有限的问题，不妨碍我们利用显著计算资源为我们要展示的每个对象训练独立体系结构。</p><p><strong>Key Takeaways</strong></p><ul><li>神经辐射场方法是一种用于从少数基础图像合成新 3D 物体视图的标准方法。</li><li>这种方法存在泛化性质有限的弊端，导致为我们要展示的每个对象训练独立体系结构时需要显著的计算资源。</li><li>作者针对此问题提出了一个基于超网络范式的 few-shot 学习方法，该方法在推理过程中无需梯度优化。</li><li>超网络从训练数据中收集信息，并为通用权重生成更新。</li><li>上述方式打造了一种有效的方法，可从少量图像中生成高质量的 3D 对象表示，只需一个步骤即可完成。</li><li>我们已通过直接比较最先进的解决方案和全面的消融研究来证实这一点。</li><li>该方法已被直接比较最先进的解决方案和全面的消融研究证实。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HyperPlanes：快速 NeRF 适应的超网络方法</li><li>作者：Paweł Batorski<em>, Dawid Malarz</em>, Marcin Przewi˛e´zlikowski, Marcin Mazur, Slawomir Tadeja, Przemysław Spurek</li><li>第一作者单位：雅盖隆大学，数学与计算机科学学院，克拉科夫，波兰</li><li>关键词：NeRF，Few-Shot 学习，超网络，快速适应</li><li>论文链接：https://arxiv.org/abs/2402.01524Github 链接：无</li><li>摘要：（1）：NeRF 是一种可以从少量基本图像合成新的逼真 3D 对象视图的深度学习方法，但它缺乏泛化性，需要针对每个对象训练单独的架构。（2）：过去的方法通常需要大量的计算资源和训练时间，并且泛化性能有限。（3）：本文提出了一种基于超网络范式的 few-shot 学习方法，该方法不需要在推理期间进行梯度优化。超网络从训练数据中收集信息并生成对通用权重的更新。（4）：实验结果表明，该方法可以在单个步骤中从少量图像生成高质量的 3D 对象表示，并且在速度和质量方面都优于现有技术。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于超网络范式的few-shot学习方法HyperPlanes，该方法不需要在推理期间进行梯度优化，可以在单个步骤中从少量图像生成高质量的3D对象表示，并且在速度和质量方面都优于现有技术。（2）：创新点：</li><li>提出了一种基于超网络范式的few-shot学习方法，该方法不需要在推理期间进行梯度优化。</li><li>该方法可以从训练数据中收集信息并生成对通用权重的更新。</li><li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。性能：</li><li>该方法在速度和质量方面都优于现有技术。</li><li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。工作量：</li><li>该方法不需要在推理期间进行梯度优化，因此可以减少计算资源和训练时间。</li><li>该方法可以从训练数据中收集信息并生成对通用权重的更新，因此可以减少训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d17d9bcf9aa679caea1d14977ee1030c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-309779f6bf52d8d8cfebf258af239717.jpg" align="middle"><img src="https://picx.zhimg.com/v2-639e9fd34cf9c9e63acc4cb78afac975.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-299ffc14425343bcd3a07c8f9122813c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-09  NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/</id>
    <published>2024-02-09T01:58:59.000Z</published>
    <updated>2024-02-09T01:58:59.146Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos"><a href="#Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos" class="headerlink" title="Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos"></a>Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos</h2><p><strong>Authors:Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</strong></p><p>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at <a href="http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html">http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html</a> </p><p><a href="http://arxiv.org/abs/2402.03723v1">PDF</a> </p><p><strong>摘要</strong><br>3D 高斯散点（3DGS）的开发改善了渲染质量和训练效率，利用可学习的 3D 可变形模型指导的变形方法能够准确建模和分离头部运动及面部表情。</p><p><strong>主要要点</strong></p><ul><li>3DGS 在 AR/VR 应用中具有巨大价值，因为它们能够从休闲智能手机视频中创建可控的 3D 人像。</li></ul><ul><li>3DGS 在渲染质量和训练效率方面取得了进展，但仍然难以从单视图捕捉中准确建模和分离头部运动和面部表情以实现高质量渲染。</li></ul><ul><li>Rig3DGS 使用一组 3D 高斯分布在规范空间中表示整个场景，包括动态主体。</li></ul><ul><li>Rig3DGS 使用一组控制信号，例如头部姿势和表情，将其转换为 3D 空间，并通过学习到的变形来生成所需的渲染。</li></ul><ul><li>Rig3DGS 的关键创新在于一种经过精心设计的变形方法，该方法由源自 3D 可变形模型的可学习先验引导。</li></ul><ul><li>这种方法在训练中非常有效，能够有效地控制各种捕捉中的面部表情、头部位置和视图合成。</li></ul><ul><li>通过广泛的定量和定性实验证明了 Rig3DGS 的学习变形是有效的。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Rig3DGS：从随意单目视频创建可控肖像</li><li>作者：Alfredo Rivero<em>, Shah Rukh Athar</em>, Zhixin Shu, Dimitris Samaras</li><li>单位：纽约石溪大学</li><li>关键词：3D人像、3D高斯喷绘、可控变形、头部姿态、面部表情、视角合成</li><li>论文链接：https://arxiv.org/abs/2402.03723Github 代码链接：无</li><li><p>摘要：(1)：研究背景：创建可控的 3D 人类肖像对于各种沉浸式体验至关重要，包括虚拟现实、远程临场、电影制作和教育应用。然而，仅使用基本智能手机摄像头，普通消费者实现这项技术面临着相当大的挑战。(2)：过去的方法：从视频中建模 3D 可控肖像通常涉及动态人类主体的显式或隐式配准，考虑每个帧中面部表情和头部姿势等不同因素。这个过程需要精确区分由这些因素引起的面部变形，这在没有真实依据的情况下通常具有挑战性。当使用单目捕捉时，挑战进一步加剧，因为每个头部姿势和表情只能从单个视点看到，这使得准确的区分变得更加复杂。(3)：研究方法：本文提出 Rig3DGS 来解决这一挑战。我们使用一组 3D 高斯体在规范空间中表示整个场景，包括动态主体。使用一组控制信号，例如头部姿势和表情，我们利用学习到的变形将它们转换为 3D 空间以生成所需的渲染。我们的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。这种方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。(4)：方法性能：我们通过广泛的定量和定性实验证明了我们学习到的变形的有效性。该项目页面可在此处找到。</p></li><li><p>方法：(1): Rig3DGS 使用一组 3D 高斯体表示整个场景，包括动态主体，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。(2): Rig3DGS 的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。(3): 该方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。</p></li><li><p>结论：（1）：本文提出了一种名为 Rig3DGS 的方法，该方法能够对肖像视频进行任意面部表情控制和新视角合成。Rig3DGS 使用可学习的变形先验来确保在训练期间的稳定性和对新面部表情、头部姿势和视角的一般化。Rig3DGS 还能够对拍摄对象的头发和眼镜等面部细节进行建模，并在视频被驱动时以高保真度再现它们。但是，具有新视角合成的可控人头部模型的问题还远未解决。Rig3DGS 无法对强烈的非均匀光照进行建模，并且要求肖像视频中的拍摄对象在拍摄期间保持相对静止。我们希望在未来的工作中解决这个问题。（2）：创新点：（1）提出了 Rig3DGS，一种使用一组 3D 高斯体表示整个场景（包括动态主体）的方法，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。（2）提出了一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。（3）证明了该方法能够控制面部表情、头部位置和跨各种捕捉的视角合成。性能：（1）Rig3DGS 能够生成高质量的 3D 肖像，具有逼真的面部表情、头部姿势和视角。（2）Rig3DGS 能够在具有挑战性的照明条件下工作，例如强烈的非均匀光照。（3）Rig3DGS 能够实时运行，使其适用于各种应用程序。工作量：（1）Rig3DGS 的训练过程相对简单且直接。（2）Rig3DGS 易于使用，并且不需要任何专门的硬件或软件。（3）Rig3DGS 是开源的，可以免费使用。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3161a0632f560b62291a8cf525616b2c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6843ee2a991081c82505388c065defc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" align="middle"></details><h2 id="4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes"></a>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes</h2><p><strong>Authors:Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</strong></p><p>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2402.03307v2">PDF</a> </p><p><strong>Summary</strong><br>动态场景下新视角合成方法 4DGS，基于高斯体素时空切片表示实现了快速的动态场景渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>4DGS 是一种新颖的方法，它使用各向异性的 4D XYZT 高斯体素来表示动态场景。</li><li>4DGS 通过对 4D 高斯体素进行时间切片来建模每个时间戳的动态，从而自然地构成动态 3D 高斯体素并可以无缝地投影到图像中。</li><li>作为一种显式的时空表示，4DGS 在建模复杂动态和精细细节方面表现出强大的能力，尤其是对于具有突然运动的场景。</li><li>4DGS 在高度优化的 CUDA 加速框架中实现了时间切片和 splatting 技术，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时推理渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时推理渲染速度。</li><li>在具有不同运动的场景上的严格评估表明，4DGS 的效率和有效性优于现有方法，无论是在定量还是定性方面都始终优于现有方法。</li><li>4DGS 可以轻松扩展到各种动态场景，例如具有复杂几何形状、遮挡和纹理的对象、具有细微运动的人体以及逼真的合成场景，并在这些场景中实现高质量的 NVS。</li><li>4DGS 可以在各种下游任务中发挥作用，例如视频插帧、运动模糊、运动估计、场景重建和增强现实。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：4D 高斯散点：面向动态场景的高效新视点合成</li><li>作者：段元兴，魏芳寅，戴启宇，何宇航，陈文正，陈宝权</li><li>单位：北京大学</li><li>关键词：新视点合成，动态场景，时间一致性，空间一致性，高斯散点</li><li>论文链接：https://arxiv.org/pdf/2402.03307.pdf，Github 链接：无</li><li><p>摘要：（1）：研究背景：新视点合成（NVS）旨在从 2D 图像重建 3D 场景，并从新视点合成其外观。NVS 在影视、游戏、VR/AR 等领域有着广泛的应用。对于静态场景，NVS 已取得了显著进展。然而，对于动态场景，由于时间维度和复杂运动模式的引入，高效且准确的 NVS 仍然具有挑战性。（2）：过去方法：现有方法主要分为两类：联合建模法和解耦建模法。联合建模法将 3D 场景及其动态联合建模，但往往难以保留 NVS 渲染中的精细细节。解耦建模法将动态场景分解为静态规范空间和变形场，但难以捕捉诸如物体突然出现或消失等复杂动态。此外，主流的基于体积渲染的方法通常无法支持实时渲染。（3）：研究方法：本文提出了一种称为 4D 高斯散点（4DGS）的新方法。4DGS 将动态场景表示为各向异性的 4D XYZT 高斯分布，受静态场景中 3D 高斯散点成功的启发。通过对 4D 高斯分布进行时间切片，可以自然地组成动态 3D 高斯分布，并将其无缝投影到图像中。作为一种显式的时空表示，4DGS 能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。此外，本文还实现了一种高度优化的 CUDA 加速框架，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时渲染速度。（4）：方法性能：在具有不同运动的场景上进行的严格评估表明，4DGS 在效率和有效性方面均优于现有方法。4DGS 在定量和定性方面都始终优于现有方法。这些性能支持了本文的目标，即开发一种高效且准确的动态场景 NVS 方法。</p></li><li><p>方法：（1）：4D高斯散点（4DGS）的基本思想是将动态场景表示为各向异性的4D XYZT高斯分布，通过对4D高斯分布进行时间切片，可以自然地组成动态3D高斯分布，并将其无缝投影到图像中。这种显式的时空表示能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。（2）：4DGS的具体步骤如下：</p></li><li>首先，通过将场景中的每个点及其运动轨迹建模为4D XYZT高斯分布，来表示动态场景。</li><li>其次，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，这些3D高斯分布可以无缝地投影到图像中，从而合成新视点图像。</li><li><p>最后，为了提高渲染速度，本文还实现了一种高度优化的CUDA加速框架，该框架可以在RTX3090 GPU上实现高达277 FPS的实时渲染速度，在RTX4090 GPU上实现583 FPS的实时渲染速度。</p></li><li><p>结论：（1）：本工作通过提出4D高斯散点（4DGS）方法，实现了高效且准确的动态场景新视点合成，为动态场景NVS领域的研究提供了新的思路和方法。（2）：创新点：</p></li><li>提出了一种新的动态场景表示方法，将动态场景表示为各向异性的4DXYZT高斯分布，能够有效地建模复杂的动态和精细细节。</li><li>提出了一种新的NVS方法，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，并将其无缝投影到图像中，合成新视点图像。</li><li>实现了一种高度优化的CUDA加速框架，在RTX3090GPU上实现高达277FPS的实时渲染速度，在RTX4090GPU上实现583FPS的实时渲染速度。性能：</li><li>在具有不同运动的场景上进行的严格评估表明，4DGS在效率和有效性方面均优于现有方法。</li><li>4DGS在定量和定性方面都始终优于现有方法。工作量：</li><li>本工作涉及了大量的理论推导和算法实现，工作量较大。</li><li>本工作使用了大量的实验数据，实验过程复杂，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-8afb4e4e499c5116d082b9b523480bbb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-960e35d536b25803abdadcc5fd2abea1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0570db380e05870cdbbd7a17934c699.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db45e73c8294473dfec461a53ba7d2a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5983071f25b6e20421a8a05030a8a70f.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v1">PDF</a> </p><p><strong>Summary</strong><br>3D语义高斯表示的视觉SLAM系统，将外观、几何和语义约束融入到关键帧优化，实现实时的高精度3D语义分割和地图重建，效果优异。</p><p><strong>Key Takeaways</strong></p><ul><li>提出SGS-SLAM，第一个基于3D高斯表示的语义稠密视觉SLAM系统，提供精确的3D语义分割和高保真的地图重建。</li><li>在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，提高重建质量。</li><li>SGS-SLAM在相机位姿估计、地图重建和语义分割方面达到了最先进的性能，优于现有方法，同时保持了实时的渲染能力。</li><li>SGS-SLAM同时适用于室内和室外场景，可在动态环境中处理光照变化和快速运动。</li><li>SGS-SLAM可用于各种机器人应用，如导航、探索和操纵。</li><li>SGS-SLAM的代码和数据集已开源，可供研究者和开发者使用。</li><li>SGS-SLAM具有广阔的应用前景，可用于自动驾驶、增强现实和虚拟现实等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SGS-SLAM：神经稠密 SLAM 的语义高斯绘图</li><li>作者：Mingrui Li、Shuhong Liu、Heng Zhou</li><li>单位：大连理工大学计算机科学系</li><li>关键词：SLAM、3D 重建、3D 语义分割</li><li>链接：Paper_info</li><li>摘要：（1）研究背景：语义理解在稠密的同时定位和建图（SLAM）中起着至关重要的作用，有助于全面理解场景。最近将高斯绘图集成到 SLAM 系统中的进展已经证明了其在使用显式 3D 高斯表示生成高质量渲染方面的有效性。（2）过去的方法及其问题：传统的视觉 SLAM 系统在稀疏重建方面取得了显着成就，但无法通过点云或体素有效地表示更密集的重建。为了提取用于高保真表示的密集几何信息，基于学习的 SLAM 方法获得了广泛关注。它们在生成良好的全局 3D 地图的同时，还表现出对噪声和异常值的鲁棒性。此外，受神经辐射场 (NeRF) 进展的启发，基于 NeRF 的 SLAM 方法取得了进一步的进展。它们擅长通过可微渲染捕获密集的光度信息，从而产生准确且高保真的全局重建。（3）论文提出的研究方法：在上述研究的基础上，本文提出了 SGS-SLAM，这是第一个基于 3D 高斯的语义稠密视觉 SLAM 系统，它在提供高保真重建的同时，还提供了精确的 3D 语义分割。具体来说，本文提出在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：广泛的实验表明，SGS-SLAM 在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保持了实时渲染能力。</li></ol><p><methods>:(1) 多通道高斯表示：使用高斯影响函数表示场景，高斯函数具有半径、中心位置和颜色。通过渲染高斯函数到 2D 图像来优化高斯函数的参数，并使用深度渲染来计算像素级渲染颜色和深度。利用 2D 语义标签为高斯函数分配不同的通道来表示语义标签和颜色。(2) 跟踪和建图：跟踪过程估计每帧的相机位姿，同时保持场景参数固定。建图过程根据估计的相机位姿优化场景表示。跟踪过程通过最小化跟踪损失来迭代优化当前位姿。关键帧选择和权重分配基于几何和语义约束。(3) 地图重建：使用高斯函数对场景进行建模，高斯函数的均值坐标表示场景的几何信息，外观颜色描述场景的视觉外观，语义颜色指示物体的语义标签。在高斯函数致密化和优化的过程中，这些参数在各个通道上联合优化，而相机位姿则通过跟踪固定。通过将高斯函数渲染到 2D 图像来优化地图参数，并使用深度渲染来计算像素级渲染颜色和深度。</methods></p><ol><li>结论：（1）：SGS-SLAM是第一个基于3D高斯表示的语义稠密视觉SLAM系统。我们提出利用多通道参数优化，其中外观、几何和语义约束被组合以强制执行高精度的3D语义分割，并同时进行高保真稠密地图重建，同时有效地产生鲁棒的相机位姿估计。SGS-SLAM利用了最优关键帧优化的好处，从而产生了可靠的重建质量。广泛的实验表明，我们的方法提供了最先进的跟踪和建图结果，同时保持了快速的渲染速度。此外，高质量的重建（2）：创新点：</li><li>提出了一种新的语义稠密SLAM系统SGS-SLAM，该系统首次将3D高斯表示与语义分割相结合，实现了高保真重建和精确的3D语义分割。</li><li>设计了一种多通道参数优化方法，将外观、几何和语义约束相结合，提高了重建质量。</li><li>提出了一种基于高斯函数的稠密地图重建方法，该方法能够生成高保真的3D地图。性能：</li><li>在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法。</li><li>能够实时渲染，保持了良好的交互性。工作量：</li><li>算法实现复杂，需要大量的计算资源。</li><li>数据集的构建和标注需要大量的人力物力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-134845e702f2aa6e6e259afa165a6769.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8509cc5a8db3cd0d7633a8bcc603fddb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-09  Rig3DGS Creating Controllable Portraits from Casual Monocular Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/</id>
    <published>2024-02-09T01:50:48.000Z</published>
    <updated>2024-02-09T01:50:48.075Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation"><a href="#EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation" class="headerlink" title="EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation"></a>EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation</h2><p><strong>Authors:Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun</strong></p><p>Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: <a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a> </p><p><a href="http://arxiv.org/abs/2402.01422v1">PDF</a> </p><p><strong>摘要</strong><br>利用视觉属性引导音频解耦器和细粒度情绪系数预测模块，精细控制谈话头生成中的情绪表达，提升生成的视频的自然性和真实性。</p><p><strong>要点</strong></p><ul><li>提出视觉属性引导音频解耦器，仅与音频内容相关的表征向量，增强后续口型系数预测的稳定性。</li><li>引入细粒度情绪系数预测模块，实现更准确的情绪表达。</li><li>提出使用细粒度情绪矩阵的情绪强度控制方法，对生成的视频中的情绪表达进行有效控制，并对情绪强度进行更精细的分类。</li><li>3DMM 系数生成网络用于预测 3D 系数，然后利用渲染网络生成最终视频。</li><li>EmoSpeaker 方法在表情变化和唇形同步方面优于现有情感谈话人脸生成方法。</li><li>项目主页：<a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：EmoSpeaker：单次学习细粒度情感控制的说话人面部生成</li><li>作者：Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao</li><li>第一作者单位：西安电子科技大学计算机科学与技术学院</li><li>关键词：说话人面部、三维可变形模型、视觉属性引导的解耦过程、细粒度情感控制</li><li>论文链接：https://arxiv.org/abs/2402.01422Github 链接：https://github.com/peterfanfan/EmoSpeaker</li><li><p>摘要：(1) 研究背景：说话人面部生成技术已成为近年来研究的热点，其在虚拟数字人生成、虚拟现实和电影特效等领域具有广泛的应用场景。然而，现有方法主要关注唇形同步和视频生成质量，对生成视频的情感表达关注较少。(2) 过去方法及其问题：一些现有方法解决了情感面部动画生成的问题，但它们通常受限于长视频或短视频的驱动。此外，使用标签控制的方法难以生成具有不同强度和不同情感中间状态的情感视频。单次学习生成方法通常只考虑唇形同步，而没有考虑情感因素。(3) 本文方法：本文提出了一种名为 EmoSpeaker 的方法，该方法通过 3D 系数作为中间表示来连接说话人面部生成过程的不同部分。为了实现这一目标，首先引入视觉属性引导的音频解耦器，从音频中提取与内容向量相关的内容向量，增强后续唇部动作系数预测的稳定性。其次，在细粒度情感系数预测模块中，将内容向量与情感标签聚合，预测细粒度的面部动作系数。此外，本文提出了一种使用细粒度情感矩阵的情感强度控制方法。通过这些方法，实现了对生成视频中的情感表达的有效控制和对情感强度的更精细分类。最后，设计了一系列 3DMM 系数生成网络来预测 3D 系数，然后利用渲染网络生成最终视频。(4) 方法性能：实验结果表明，本文提出的 EmoSpeaker 方法在表情表达和唇形同步方面优于现有情感说话人面部生成方法。该方法可以支持其目标，生成具有任意强度的任意情感面部视频。</p></li><li><p>方法：（1）视觉属性引导的音频解耦器：为了准确预测唇部信息，提出了一种视觉属性引导的音频解耦器。该解耦器利用面部动作单元（AU）作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。（2）细粒度情感系数预测模块：将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。（3）情感面部渲染器：设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。</p></li><li><p>结论：（1）：本文提出了一种名为 EmoSpeaker 的算法，该算法仅需音频剪辑、肖像、特定情绪和强度粒度，即可生成具有细粒度强度的表情面部。该方法使用面部情绪解耦模块提取内容特征，并结合细粒度强度控制模块来实现任意情绪强度。这在电子游戏、虚拟现实、电影特效和人机界面交互等领域展示了有希望的应用。主观和客观评估表明，与最先进的方法相比，我们的方法在生成更丰富的面部动画方面具有优越性。未来的研究将集中于在细粒度强度控制领域进行深入研究，以增强更具表现力和细微差别的面部动画的生成。（2）：创新点：</p></li><li>提出了一种视觉属性引导的音频解耦器，该解耦器利用面部动作单元作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。</li><li>提出了一种细粒度情感系数预测模块，将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。</li><li>设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。性能：</li><li>在表情表达和唇形同步方面优于现有情感说话人面部生成方法。</li><li>可以支持其目标，生成具有任意强度的任意情感面部视频。工作量：</li><li>需要收集大量的数据集来训练模型。</li><li>模型的训练过程比较耗时。</li><li>需要对模型进行微调以适应不同的数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6bacdbeff940a1345ff38f8b1dc2680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c646c87add1ea43ace17da06ebd12a7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d08dc09fd1df64224ed8ef166ac7d5b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0dc431600d1c5672918ab10a962f79ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7d1b798a4f9c96adf7e70cbb6847a5b3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4c97492b45a0ba3e2e8b06c0abf4372f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5ce57abfa37d7135a925aa7ba77e6120.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-09  EmoSpeaker One-shot Fine-grained Emotion-Controlled Talking Face   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/</id>
    <published>2024-02-09T01:46:05.000Z</published>
    <updated>2024-02-09T01:46:05.555Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation"><a href="#Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation" class="headerlink" title="Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation"></a>Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</h2><p><strong>Authors:Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</strong></p><p>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images. </p><p><a href="http://arxiv.org/abs/2402.04929v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.01701</p><p><strong>Summary</strong><br>利用扩散模型的泛化能力进行无源域自适应。</p><p><strong>Key Takeaways</strong></p><ul><li>通过微调预训练的文生图扩散模型，利用目标图像的特征引导扩散过程，生成源域图像。</li><li>目标是生成熵最小化且对预训练源模型的置信度最大的源样本。</li><li>直接在目标图像分布上训练扩散模型，而无需成对的源和目标图像。</li><li>所提出的方法在 Office-31、Office-Home 和 VisDA 等多个数据集上都取得了最先进的性能。</li><li>生成的高质量源图像有助于跨域任务，例如图像分类和目标检测。</li><li>充分利用扩散模型的生成能力，在源和目标域之间建立桥梁。</li><li>无源域自适应的模型具有降噪效果，有助于提高分类和检测的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散引导源数据生成的无源域自适应</li><li>作者：Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</li><li>单位：佐治亚理工学院计算机系</li><li>关键词：无源域自适应、扩散模型、数据生成、跨域图像分类</li><li>链接：https://arxiv.org/abs/2402.04929Github：无</li><li><p>摘要：（1）研究背景：深度卷积神经网络（CNN）在视觉任务中取得了令人印象深刻的性能，但它们对训练和测试数据分布一致性的假设限制了其在真实世界中的应用。领域自适应（DA）旨在减少这种差异，使模型能够跨多个领域表现良好。传统 DA 方法依赖于固定的源数据，可能难以适应不断变化的领域。无源域自适应（SFDA）是一种特殊类型的 DA，它不需要访问源训练数据。（2）过去的方法与问题：现有的大多数 SFDA 方法通过在共享特征空间中融合两个不同的数据分布来实现模型适应性。一种实现无源方式的方法是使用合成生成的源数据。然而，生成准确表示源域多样性和复杂性的合成源数据可能很困难。此外，如果合成数据质量不高，可能会引入噪声和不一致性，从而对模型在目标域上的性能产生负面影响。（3）提出的研究方法：本文提出了一种名为 DM-SFDA 的新方法，该方法利用扩散生成模型（DGM）的泛化能力来解决 SFDA 的挑战。DM-SFDA 的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。具体来说，预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。（4）方法的性能：本文在 Office-31、Office-Home 和 VisDA 等多个数据集上对 DM-SFDA 进行了全面的实验验证。结果表明，DM-SFDA 在 SFDA 任务上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。这些性能支持了本文提出的方法能够有效地解决 SFDA 问题。</p></li><li><p>方法：(1): 基于扩散模型的无源域自适应（DM-SFDA）方法的基本思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。(2): 预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。(3): 然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。</p></li><li><p>结论：</p></li></ol><p>（1）重要性：本文提出了一种基于扩散模型的无源域自适应（DM-SFDA）方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。DM-SFDA的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。该方法在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</p><p>（2）优缺点：</p><p>创新点：</p><ul><li>提出了一种基于扩散模型的无源域自适应方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。</li><li>设计了一种新的生成源图像的方法，该方法使用来自目标图像的特征来指导扩散过程，从而生成上下文相关、特定于领域的图像。</li></ul><p>性能：</p><ul><li>在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</li></ul><p>工作量：</p><ul><li>需要微调一个预训练的文本到图像扩散模型，这可能需要大量的计算资源。</li><li>需要收集目标域的数据，这在某些情况下可能很困难。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ec2a5c717af2a4c67eb4715437c633c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf3cb970b1edbd90925d67dc50ebd458.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b60fc581c86cc20b03dbf6c09543aea2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-282170863545d09c18b118ee88d874e2.jpg" align="middle"></details>## EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World   Illusions**Authors:Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas**Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \url{https://shashankkotyan.github.io/EvoSeed}. [PDF](http://arxiv.org/abs/2402.04699v1) **Summary**利用进化策略搜索算法框架生成自然对抗样本，以增强扩散模型在真实世界场景中的鲁棒性。**Key Takeaways**- 基于进化策略的搜索算法框架 EvoSeed 用于生成自然对抗样本。- EvoSeed 框架使用辅助扩散和分类器模型在与模型无关的黑盒设置中运行。- 采用 CMA-ES 优化对抗种子向量的搜索，该向量在条件扩散模型处理后，会生成分类器模型错误分类的无限制自然对抗样本。- 实验表明生成的对抗图像具有高图像质量，并且可以转移到不同的分类器。- 该方法证明了使用进化算法来增强对抗样本质量的潜力。- 希望这项研究能够为增强深度神经网络在真实世界场景中的鲁棒性开辟新途径。- 项目网站可以访问网址：https://shashankkotyan.github.io/EvoSeed。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：EvoSeed：揭示深度神经网络的威胁</li><li>作者：Shashank Kotyan、Po Yuan Mao、Danilo Vasconcellos Vargas</li><li>单位：九州大学</li><li>关键词：深度学习、计算机视觉、CMA-ES、扩散模型、自然对抗样本</li><li>论文链接：https://arxiv.org/abs/2402.04699，Github 链接：None</li><li><p>摘要：（1）研究背景：深度神经网络在各种视觉识别任务中取得了空前的成功。然而，当测试分布与训练分布不同时，它们的性能会下降，Hendrycks 等人[10]和 Ilyas 等人[17]的研究表明了这一点。这给开发能够处理这种分布变化的鲁棒深度神经网络带来了重大挑战。对抗样本和对抗攻击利用了这种漏洞，通过操纵图像来改变与原始分布相比的分布。Dalvi 等人[4]的研究强调，输入数据的对抗性操纵通常会导致分类器做出不正确的预测，这引发了人们对经典机器学习算法的安全性和完整性的严重担忧。这种担忧仍然相关，尤其是考虑到最先进的深度神经网络极易受到涉及故意对输入进行扰动的对抗性攻击[22, 26]。对这些扰动施加了各种约束，使这些扰动变得微妙且难以检测。例如，𝐿0对抗攻击，例如 One-Pixel Attack[22, 38]限制了扰动像素的数量，𝐿2对抗攻击，例如 PGD-L2[26]限制了与原始图像的欧几里得距离，并且𝐿∞对抗攻击，例如 PGD-L∞[26]限制了所有像素的变化量。（2）过去的方法及其问题：虽然对抗样本[22, 26, 38]暴露了深度神经网络中的漏洞，但它们的人工性质和对受限输入数据的依赖限制了它们在现实世界中的适用性。相比之下，在实际情况下，挑战变得更加明显，因为将所有潜在威胁全面地包含在训练数据集中变得不可行。这种复杂性突出了深度神经网络对 Hendrycks 等人[10]提出的自然对抗示例和 Song 等人[37]提出的无限制对抗示例的敏感性不断提高。近年来，这些类型的对抗样本在对抗攻击研究中获得了突出地位，因为它们可以对图像进行实质性改变，而不会显着影响人类对其含义和真实性的感知。（3）本文提出的研究方法：在这样的背景下，我们提出了 EvoSeed，这是一种第一个基于进化策略的算法框架，旨在生成如图 2 所示的无限制自然对抗样本。我们的算法需要一个条件扩散模型𝐺和一个分类器模型𝐹来生成对抗样本𝑥。具体来说，它利用协方差矩阵自适应进化策略 (CMA-ES) 作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。CMA-ES 对噪声种子向量𝑧′进行微调，以优化目标函数，该目标函数将分类器模型𝐹的输出与人类对图像𝑥的感知之间的差异作为惩罚。（4）方法在任务和性能上的表现：实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。</p></li><li><p>方法：（1）随机种子法（RandSeed）：基于随机偏移的随机搜索策略，通过在初始种子向量上添加随机扰动来生成对抗样本。（2）进化种子法（EvoSeed）：基于协方差矩阵自适应进化策略（CMA-ES）的优化算法，通过迭代优化初始种子向量来搜索对抗种子向量，以生成对抗样本。（3）条件扩散模型（Conditional Diffusion Model）：用于生成对抗样本的生成模型，通过条件信息和初始种子向量生成图像。（4）分类器模型（Classifier Model）：用于评估对抗样本质量的分类模型，通过计算对抗样本的分类错误率来衡量对抗样本的攻击成功率。（5）攻击成功率（ASR）：衡量对抗样本攻击成功率的指标，计算为对抗样本被分类器错误分类的比例。（6）弗雷歇特起始距离（FID）：衡量对抗样本与真实样本分布差异的指标，计算为对抗样本与真实样本在特征空间中的距离。（7）感知评分（IS）：衡量对抗样本质量的指标，计算为对抗样本在分类器上的平均对数似然值。（8）结构相似性（SSIM）：衡量对抗样本与真实样本在结构上的相似性，计算为对抗样本与真实样本在像素空间中的相似度。</p></li><li><p>结论：（1）：EvoSeed是一种基于进化策略的算法框架，旨在生成无限制自然对抗样本。它利用协方差矩阵自适应进化策略（CMA-ES）作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。（2）：创新点：</p></li><li>提出了一种基于进化策略的算法框架EvoSeed，用于生成无限制自然对抗样本。</li><li>利用协方差矩阵自适应进化策略（CMA-ES）作为核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。</li><li>实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。性能：</li><li>EvoSeed生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。</li><li>EvoSeed在ImageNet数据集上实现了99.9%的攻击成功率，并且在CIFAR-10数据集上实现了99.8%的攻击成功率。</li><li>EvoSeed生成的对抗图像在弗雷歇特起始距离（FID）和感知评分（IS）方面都优于其他方法。工作量：</li><li>EvoSeed的实现相对简单，并且可以很容易地应用于不同的数据集和分类器。</li><li>EvoSeed的训练时间与其他方法相比相对较短。</li><li>EvoSeed可以生成高质量的对抗图像，而不需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fba3784cdfd913938a2c25b5d6802005.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1191333c7b6b916696b230758671066a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d57cb25c209c458064f830f4a1d7c2d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5b515b564419e732b66802017f00ce12.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b4c2ccc13f827d3ede06ea04ae36e1da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5df7bec69fc3a0aa2cf5d26456e611b5.jpg" align="middle"></details><h2 id="BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception"><a href="#BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception" class="headerlink" title="BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception"></a>BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception</h2><p><strong>Authors:Aniket Roy, Anirban Roy, Soma Mitra, Kuntal Ghosh</strong></p><p>Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: <a href="https://github.com/aniket004/BRI3L">https://github.com/aniket004/BRI3L</a> </p><p><a href="http://arxiv.org/abs/2402.04541v1">PDF</a> </p><p><strong>Summary</strong><br>深度学习可以识别和定位亮度错觉，甚至可以生成新的错觉图像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出大规模亮度错觉数据集BRI3L，包含22,366张图像，涵盖五种错觉类型。</li><li>数据集包含标签信息，可用于评估分类和分割任务。</li><li>基于数据驱动的深度学习方法在该数据集上取得了良好的性能。</li><li>深度学习模型可以泛化到未见过的亮度错觉，如亮度同化到对比度转换。</li><li>扩散模型能够生成亮度错觉图像。</li><li>该研究为视觉错觉的理解和评估提供了新的方法。</li><li>该研究的数据集和代码已开源，以便其他研究人员使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知区域</li><li>作者：Aniket Roy, Anirban Roy, Soma Mitr, Kuntal Ghosh</li><li>第一作者单位：约翰·霍普金斯大学</li><li>关键词：视觉错觉，感知</li><li>论文链接：https://arxiv.org/abs/2402.04541，Github 代码链接：https://github.com/aniket004/BRI3L</li><li>摘要：(1)：研究背景：视觉错觉在理解视觉感知中发挥着重要作用。当前理解和评估视觉错觉的方法主要是基于确定性滤波的方法，并且它们对少数视觉错觉进行评估，因此结论不具有普遍性。(2)：过去的方法及其问题，方法动机：为了解决这个问题，我们生成了一个包含 22,366 张图像的大规模数据集（BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知），其中包含五种类型的亮度错觉，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。我们遵循涉及人类受试者的标准心理物理实验来验证数据集。据我们所知，这是首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。我们在 GitHub 仓库中提供了所有代码、数据集、指令集等：https://github.com/aniket004/BRI3L(3)：研究方法：我们遵循涉及人类受试者的标准心理物理实验来验证数据集。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。(4)：方法性能：在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。这些性能支持了我们开发一个大规模数据集和使用数据驱动的方法对错觉分类和定位进行基准测试的目标。</li></ol><p>7.<methods>：(1) 我们遵循涉及人类受试者的标准心理物理实验来验证数据集。(2) 我们考虑了五种研究充分的亮度错觉类型：赫尔曼网格、同步亮度对比、白色错觉、网格错觉、感应光栅错觉。(3) 在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。(4) 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。(5) 我们还测试了最先进的扩散模型生成亮度错觉的能力。</methods></p><ol><li>结论：（1）：本工作的重要意义在于，它提供了一个包含五种类型亮度错觉的、包含22,366张图像的大规模数据集BRI3L，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。这将有助于研究人员和从业者更好地理解视觉错觉，并开发新的方法来识别和定位错觉感知区域。（2）：创新点：</li><li>首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。</li><li>该数据集包含五种类型亮度错觉，涵盖了多种错觉现象。</li><li>使用深度学习模型对数据集进行了基准测试，实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。</li><li>结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</li></ol><p>性能：* 在该数据集上的基准测试实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。* 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</p><p>工作量：* 收集和注释数据的工作量很大。* 开发和训练深度学习模型的工作量也很大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d9494fba06526e8b87f8dd5e3bc6d94a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a61414f51deef787aabe72aa30947292.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9a8fbfbb6ed5b80eb2803e27c328d8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2d1fec65eb07ceea77a12925d47fbae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" align="middle"></details>## Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation**Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao**This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm. [PDF](http://arxiv.org/abs/2402.04031v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**摘要**聚合扩散模型 Polyp-DDPM 可结合掩码生成逼真的息肉图像，有效提高胃肠道息肉分割性能。**要点**- Polyp-DDPM 采用基于扩散的方法，通过训练扩散模型生成逼真且与掩码条件相符的息肉图像，提高胃肠道息肉分割的性能。- Polyp-DDPM 以分割掩码（表示异常区域的二值掩码）为条件，在图像质量和分割性能方面均优于现有方法。在图像质量方面，Polyp-DDPM 在 Frechet Inception Distance (FID) 得分上达到 78.47，而现有方法的分数高于 83.79。在分割性能方面，Polyp-DDPM 在交集比 (IoU) 上达到 0.7156，而基线模型生成的合成图像的 IoU 小于 0.6694，真实数据的 IoU 为 0.7067。- Polyp-DDPM 生成高质量且多样化的合成数据集，用于训练，从而提高息肉分割模型的性能使其能够与真实图像媲美，并提供更强大的数据增强功能来改进分割模型。- Polyp-DDPM 的源代码和预训练权重已在 https://github.com/mobaidoctor/polyp-ddpm 上公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Polyp-DDPM：基于扩散的语义息肉合成，用于增强分割</li><li>作者：Zolnamar Dorjsembe、Hsing-Kuo Pao、Furen Xiao</li><li>隶属单位：国立台湾科技大学计算机科学与信息工程系</li><li>关键词：扩散模型、语义息肉合成、息肉分割</li><li>论文链接：https://arxiv.org/abs/2302.09766，Github 代码链接：https://github.com/mobaidoctor/polyp-ddpm</li><li>摘要：（1）研究背景：结直肠癌是全球第三常见的癌症，通常始于结直肠息肉，早期发现和切除息肉可预防结直肠癌并降低死亡率。然而，在结肠镜检查中发现小息肉可能很困难，这取决于医生的专业知识和其他挑战，例如息肉在手术过程中无法观察到或被忽视。为了增强息肉检测，研究人员正在利用机器学习来自主识别和强调内窥镜检查中的息肉。然而，由于需要广泛且多样化的数据集，这些技术的发展面临着重大挑战，这些数据集对于训练模型以实现高准确性至关重要。医疗保健行业经常面临此类数据的短缺，这归因于异常区域外观的多样性、患者招募困难、数据注释成本高以及对患者数据隐私的担忧。（2）过去的方法及其问题：为了减轻数据稀缺问题，探索合成图像作为一种可行的解决方案已引起关注。现有的方法包括基于 GAN 的方法和基于扩散的方法。基于 GAN 的方法，如 SinGAN-Seg，能够生成比其他 GAN 模型更逼真的图像，但面临多样性和细节准确性的挑战。基于扩散的方法，如两阶段扩散模型，能够生成多样化的图像，但由于需要两个模型，因此训练和推理的计算成本很高。（3）提出的研究方法：为了应对这些挑战，我们提出了一种新颖的基于扩散的语义息肉合成方法 Polyp-DDPM，旨在增强息肉分割。我们的方法通过掩模图像的逐通道连接对扩散模型进行条件化。（4）实验结果与方法性能：我们在 Kvasir-SEG 数据集上进行了实验，并将我们提出的方法与 SinGAN-Seg 和潜在扩散模型进行了比较，因为这些方法代表了带注释息肉数据集生成的最新进展，包括基于 GAN 的方法和基于扩散的方法。在我们的实验中，Polyp-DDPM 在图像质量和分割任务中均表现出优于基线模型的性能。这项研究通过提供一种新的基于扩散的方法来合成高质量的合成息肉图像，为任何给定的掩模图像做出了贡献，可用于训练更准确的息肉分割模型。源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于扩散的语义息肉合成方法Polyp-DDPM，旨在增强息肉分割。Polyp-DDPM通过掩模图像的逐通道连接对扩散模型进行条件化，能够生成高质量的合成息肉图像，可用于训练更准确的息肉分割模型。（2）：创新点：</li><li>提出了一种新的基于扩散的方法来合成高质量的合成息肉图像。</li><li>通过掩模图像的逐通道连接对扩散模型进行条件化，使生成的图像具有更强的语义信息。</li><li>在Kvasir-SEG数据集上进行了实验，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。性能：</li><li>在Kvasir-SEG数据集上，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。</li><li>Polyp-DDPM生成的合成息肉图像具有更高的质量和更强的语义信息。</li><li>Polyp-DDPM训练的息肉分割模型在Kvasir-SEG数据集上取得了更高的分割准确率。工作量：</li><li>Polyp-DDPM的训练和推理过程相对简单，不需要额外的预处理或后处理步骤。</li><li>Polyp-DDPM的源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle"></details><h2 id="EscherNet-A-Generative-Model-for-Scalable-View-Synthesis"><a href="#EscherNet-A-Generative-Model-for-Scalable-View-Synthesis" class="headerlink" title="EscherNet: A Generative Model for Scalable View Synthesis"></a>EscherNet: A Generative Model for Scalable View Synthesis</h2><p><strong>Authors:Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</strong></p><p>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis — it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{<a href="https://kxhit.github.io/EscherNet}">https://kxhit.github.io/EscherNet}</a>. </p><p><a href="http://arxiv.org/abs/2402.03908v1">PDF</a> Project Page: <a href="https://kxhit.github.io/EscherNet">https://kxhit.github.io/EscherNet</a></p><p><strong>Summary</strong></p><p>利用条件扩散模型进行多视角视图合成，实现任意数量的视角转换。</p><p><strong>Key Takeaways</strong></p><ul><li>EscherNet 是一种多视角条件扩散模型，用于视图合成。</li><li>EscherNet 的本质是，以多视角图像作为输入，生成任意数量的目标视角图像。</li><li>EscherNet 可以在单个消费级 GPU 上同时生成 100 多个一致的目标视角，在准确性上达到最先进的效果。</li><li>EscherNet 的多功能性使其可以解决多种 3D 视觉任务，例如零样本新视角合成、单图像 3D 重建、多图像 3D 重建等。</li><li>EscherNet 的应用场景包括虚拟现实、增强现实、医学成像、自动驾驶等。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：埃舍尔网络：一种用于可扩展视图合成的生成模型</li><li>作者：Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</li><li>隶属单位：伦敦帝国理工学院戴森机器人实验室</li><li>关键词：视图合成、扩散模型、隐式神经表示、多视图几何</li><li>论文链接：https://arxiv.org/abs/2402.03908，Github 链接：None</li><li><p>摘要：(1)：研究背景：视图合成是计算机视觉和计算机图形学中的一项基本任务，它允许根据一组参考视点呈现场景的任意视点，从而模拟人类的视觉适应性。(2)：过去的方法：现有方法通常专注于单一任务，例如零样本新颖视图合成、单图像三维重建或多图像三维重建，并且在处理复杂场景时面临着泛化性差、灵活性不足和可扩展性有限等问题。(3)：研究方法：本文提出了一种多视图条件扩散模型——埃舍尔网络，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的 3 个参考视图到 3 个目标视图上训练的情况下，它也可以在单个消费级 GPU 上同时生成 100 多个一致的目标视图。(4)：方法性能：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p></li><li><p>方法：(1)：埃舍尔网络是一种多视图条件扩散模型，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。(2)：埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。(3)：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p></li><li><p>结论：（1）：埃舍尔网络提出了一种多视图条件扩散模型，该模型在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。（2）：创新点：</p></li><li>提出了一种多视图条件扩散模型——埃舍尔网络，该模型学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。</li><li>埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。</li><li>埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。性能：</li><li>在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。</li><li>即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。工作量：</li><li>埃舍尔网络是一个复杂的神经网络模型，需要大量的数据和计算资源进行训练。</li><li>埃舍尔网络的训练过程可能需要几天或几周的时间，具体取决于数据集的大小和使用的硬件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cdd01255ccb3e0ac7a9532f4537d7c8a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7709d4f2ffb5392bba195cc2b965aeee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86704d39a54eee216395f69db00a0918.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73c26a4c69f3a172a8651cabc4a69ed2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a53c48c54043613c01b125b54da3368.jpg" align="middle"><img src="https://picx.zhimg.com/v2-526811fa3a0b6e1b6d850e3911c0f54f.jpg" align="middle"></details><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</strong></p><p>Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. </p><p><a href="http://arxiv.org/abs/2402.03666v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型量化后，如何提高准确率？</p><p><strong>Key Takeaways</strong></p><ul><li>低位量化扩散模型面临三大问题：激活分布不平衡、时间信息不精确、特定模块对扰动敏感。</li><li>提出微调量化模型，使其更好地适应激活分布。</li><li>识别出两种关键的量化层：保存重要时间信息的层和对比特宽度降低敏感的层，并对其进行微调以缓解性能退化。</li><li>经验验证表明，该方法修改了激活分布并提供了有意义的时间信息，促进了更容易、更准确的量化。</li><li>该方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在完全4位（即 W4A4）Stable Diffusion 上生成可读图像的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：QuEST：低比特扩散模型量化通过高效选择性微调</li><li>作者：Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</li><li>第一作者单位：伊利诺伊理工学院计算机科学系</li><li>关键词：扩散模型、量化、低比特、微调</li><li>论文链接：https://arxiv.org/abs/2402.03666Github 链接：无</li><li><p>摘要：(1) 研究背景：扩散模型在图像生成任务中取得了显著的成功，但其实际部署受到高内存和时间消耗的限制。量化可以为扩散模型压缩和加速提供一种可行的方法，但现有方法在模型被量化为低比特时完全失败。(2) 过去方法及其问题：现有扩散模型量化方法要么集中在时间步长感知校准数据构造，要么集中在量化噪声校正，目标是将现有的量化技术调整到扩散模型的特性，而这些特性与其他模型类型（如 CNN 和 ViT）不同。这些方法忽略了与量化相关的扩散模型内在机制，导致方法部署与模型特征之间存在不一致。(3) 本文方法：本文揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化：（1）激活分布可能不平衡，其中大多数值接近 0，但其他值很大且不一致地出现；（2）时间信息不精确；（3）容易受到特定模块的扰动。为了减轻源于分布不平衡的低比特量化难度，本文提出微调量化模型以更好地适应激活分布。在此基础上，本文确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。(4) 实验结果：本文方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。这些性能支持了本文的目标。</p></li><li><p>方法：（1）属性一：激活分布不平衡，大多数值接近 0，但其他值很大且不一致地出现。为了解决这个问题，我们提出微调量化模型以更好地适应激活分布。（2）属性二：时间信息不准确。为此，我们确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。（3）属性三：不同激活对降低比特宽度的敏感性不同。我们提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。（4）QuEST：一种通过高效选择性微调实现低比特扩散模型量化的框架。QuEST 是一个基于蒸馏的微调策略，包括选择性权重优化和网络级缩放因子优化。</p></li><li><p>结论：（1）本文提出了 QuEST，一种用于低比特扩散模型量化的有效无数据微调框架。我们的方法的动机来自于在量化扩散模型中发现的三个基本属性。我们还从理论上证明了微调的充分性，将其解释为增强模型对大激活扰动的鲁棒性的一种方法。为了减轻性能下降，我们提出在全精度对应模型的监督下微调时间嵌入层和注意力相关层。还引入了一个时间感知激活量化器来处理不同的时间步长。在三个高分辨率图像生成任务上的实验结果证明了 QuEST 的有效性和效率，在更少的时间和内存成本下实现了低比特兼容性。（2）创新点：</p></li><li>揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化。</li><li>提出了一种基于蒸馏的微调策略 QuEST，包括选择性权重优化和网络级缩放因子优化。</li><li>提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。性能：</li><li>在三个高分辨率图像生成任务上实现了最先进的性能。</li><li>是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。工作量：</li><li>在 ImageNet-64x64 数据集上，QuEST 只需 10 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li><li>在 ImageNet-256x256 数据集上，QuEST 只需 40 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fb8f38fcd6a6857ddffdf84e6eded575.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b8c7e8e687f519bd6aea6d7aa431f440.jpg" align="middle"><img src="https://picx.zhimg.com/v2-808d7c694b655862c89add4bffc7e8b1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e6556a45821b662485e3c321d4542f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d15f14313fc02ce9abba40125462e990.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9f9c978c41171320dbafc60fb23b8e.jpg" align="middle"></details><h2 id="InstanceDiffusion-Instance-level-Control-for-Image-Generation"><a href="#InstanceDiffusion-Instance-level-Control-for-Image-Generation" class="headerlink" title="InstanceDiffusion: Instance-level Control for Image Generation"></a>InstanceDiffusion: Instance-level Control for Image Generation</h2><p><strong>Authors:Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</strong></p><p>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs. </p><p><a href="http://arxiv.org/abs/2402.03290v1">PDF</a> Preprint; Project page:   <a href="https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/">https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/</a></p><p><strong>Summary</strong><br>文本到图像扩散模型实现了高质量图像生成，但无法控制图像中的单独实例。我们引入了InstanceDiffusion，它为文本到图像扩散模型添加了精确的实例级控制。</p><p><strong>Key Takeaways</strong></p><ul><li>InstanceDiffusion支持每个实例的自由形式语言条件。</li><li>InstanceDiffusion支持灵活方式指定实例位置，如简单单点、涂鸦、边界框或复杂的实例分割掩码及其组合。</li><li>InstanceDiffusion提出了三个主要更改，以实现精确的实例级控制。</li><li>UniFusion模块为文本到图像模型启用了实例级条件。</li><li>ScaleU模块提高了图像保真度。</li><li>Multi-instance Sampler改进了多个实例的生成。</li><li>InstanceDiffusion在每个位置条件下都显着超过了专门的最新模型。</li><li>在COCO数据集上，InstanceDiffusion在框输入时优于之前的最新技术20.4% AP50box，在掩码输入时优于之前的最新技术25.4% IoU。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：实例扩散：图像生成的实例级控制</li><li>作者：Jun-Yan Zhu, Taesung Park, Abhishek Sharma, Prafulla Dhariwal, Alexei A. Efros, Pieter Abbeel</li><li>隶属机构：加州大学伯克利分校</li><li>关键词：文本到图像生成、实例级控制、扩散模型</li><li>论文链接：https://arxiv.org/abs/2212.04915，Github 代码链接：无</li><li><p>摘要：(1) 研究背景：文本到图像扩散模型可以生成高质量的图像，但无法对图像中的各个实例进行控制。(2) 过去的方法：现有方法主要集中在对整个图像进行控制，而无法对各个实例进行精细的控制。这些方法的问题在于，它们无法处理复杂的实例条件，例如，当实例重叠或被遮挡时，它们无法生成高质量的图像。(3) 论文提出的方法：本文提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。InstanceDiffusion 主要包含三个部分：UniFusion 模块、ScaleU 模块和 Multi-instance Sampler。UniFusion 模块可以将实例条件融合到文本嵌入中，ScaleU 模块可以提高图像的保真度，Multi-instance Sampler 可以改善多实例生成的质量。(4) 实验结果：InstanceDiffusion 在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上，InstanceDiffusion 比之前的最先进模型高出 20.4%，在 IoU 指标上，InstanceDiffusion 比之前的最先进模型高出 25.4%。这些结果表明，InstanceDiffusion 能够有效地对图像中的各个实例进行控制，并生成高质量的图像。</p></li><li><p>Methods：(1) UniFusion：UniFusion是InstanceDiffusion模型中的一个关键模块，它可以将模糊的语义信息融合到图像嵌入中。UniFusion由两个子模块组成：语义信息提取模块和信息融合模块。语义信息提取模块负责从语义信息中提取特征，信息融合模块负责将这些特征融合到图像嵌入中。(2) ScaleU：ScaleU是InstanceDiffusion模型中的另一个关键模块，它可以提高图像的保真度。ScaleU由两个子模块组成：上采样模块和残差模块。上采样模块负责将图像从低分辨率上采样到高分辨率，残差模块负责添加残差连接，以提高图像的保真度。(3) Multi-instanceSampler：Multi-instanceSampler是InstanceDiffusion模型中的一个采样模块，它可以改善多实例生成的质量。Multi-instanceSampler通过对每个实例进行多次采样，然后将这些采样结果进行融合，以生成最终的图像。</p></li><li><p>结论：（1）：InstanceDiffusion 模型在文本到图像生成任务中取得了最先进的性能，在 COCO 数据集上的 APbox50 指标和 IoU 指标上均优于之前的最先进模型。这表明 InstanceDiffusion 模型能够有效地对图像中的各个实例进行控制，并生成高质量的图像。（2）：创新点：</p></li><li>提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。</li><li>设计了 UniFusion 模块，可以将模糊的语义信息融合到图像嵌入中。</li><li>设计了 ScaleU 模块，可以提高图像的保真度。</li><li>设计了 Multi-instanceSampler 模块，可以改善多实例生成的质量。性能：</li><li>在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上比之前的最先进模型高出 20.4%，在 IoU 指标上比之前的最先进模型高出 25.4%。工作量：</li><li>模型的训练和推理过程相对复杂，需要较大的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ad80374506fc08e660bb8742f25dc5ea.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eb1dc22d5f1b16516125f58ffce2ab07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cc40befe0322c7f0f22fe9b42e02d05a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0667dfce57b3d47e56cf440eb22a837d.jpg" align="middle"></details><h2 id="Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images"><a href="#Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images" class="headerlink" title="Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?"></a>Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?</h2><p><strong>Authors:Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</strong></p><p>The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today’s modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness. </p><p><a href="http://arxiv.org/abs/2402.03214v2">PDF</a> </p><p><strong>Summary</strong><br>人工智能图像生成技术引发艺术领域巨变，区分人工智能生成图像与人类艺术品是一项不断加剧的难题。</p><p><strong>Key Takeaways</strong></p><ul><li>AI生成图像对艺术世界的颠覆性影响与日俱增。</li><li>鉴别AI生成的图像对于防止欺诈、版权保护和模型训练至关重要。</li><li>目前有几种方法可以区分人类艺术与AI图像，包括监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的了解进行识别。</li><li>研究表明，Hive和专家艺术家在区分AI生成的图像方面表现出色，但各有优劣（Hive对对抗性扰动较弱，而专家艺术家产生较高误报率）。</li><li>随着模型的不断发展，这些弱点可能仍然存在，研究数据表明，由人类和自动检测器组成的组合团队可以提供最佳的准确性和鲁棒性。</li><li>人工生成的图像在艺术领域引发了一场颠覆，准确区分人工智能生成的图像对于防止欺诈和保护版权至关重要。</li><li>尽管有不同的方法可以区分人类艺术与AI图像，但没有一种方法是完美的。</li><li>将人类和自动检测器结合起来可以提供最佳的准确性和鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：有机还是扩散：我们能区分人类艺术和人工智能生成图像吗？</li><li>作者：Anna Yoo Jeong Ha、Josephine Passananti、Ronik Bhaskar、Shawn Shan、Reid Southen1、Haitao Zheng、Ben Y. Zhao</li><li>第一作者单位：芝加哥大学计算机科学系</li><li>关键词：人工智能艺术、图像生成、鉴别器、人类艺术家</li><li>论文链接：https://arxiv.org/abs/2402.03214，Github 代码链接：无</li><li>摘要：(1)：随着人工智能生成图像的出现，艺术领域发生了巨大变革。区分人工智能生成图像和人类艺术是一个具有挑战性的问题，其影响随着时间的推移而不断扩大。如果不解决这个问题，就会让不法分子欺骗那些为人类艺术支付高价的个人和禁止使用人工智能图像的公司。这对内容所有者建立版权和对模型训练者来说也是至关重要的，他们需要对训练数据进行整理以避免潜在的模型崩溃。(2)：目前，有几种不同的方法可以区分人类艺术和人工智能图像，包括通过监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的知识进行识别。(3)：在本文中，我们寻求了解这些方法在面对当今现代生成模型时，在良性和对抗性环境中的表现如何。我们整理了跨越 7 种风格的真实人类艺术，从 5 个生成模型中生成了匹配的图像，并应用了 8 个检测器（5 个自动检测器和 3 个不同的人类群体，包括 180 名众包工人、4000 多名专业艺术家和 13 名在检测人工智能方面经验丰富的专家艺术家）。(4)：Hive 和专家艺术家都表现得非常好，但在不同的方面犯了错误（Hive 在对抗性扰动中较弱，而专家艺术家产生较高的误报）。我们认为随着模型的不断发展，这些弱点将继续存在，并利用我们的数据证明为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li></ol><p>方法：</p><p>（1）构建数据集：- 收集真人艺术作品、AI 生成的图像、扰动版本的人类艺术作品和 AI 图像以及结合人类和 AI 努力创建的非典型图像。- 从 53 位艺术家处收集 280 幅真人艺术作品，涵盖 7 种主要艺术风格。- 为 7 种艺术风格中的每一种，使用 5 个流行的 AI 生成器生成 10 张图像，共生成 350 张 AI 生成的图像。- 调整 BLIP 生成的标题以包括艺术作品的风格，并根据每个 AI 生成器的独特限制和配置对标题进行自定义调整。</p><p>（2）评估自动检测器：- 考虑已部署的商业系统和基于研究的系统。- 评估自动检测器在核心测试数据集上的性能，该数据集包含 280 幅真人艺术作品、350 幅 AI 图像和 40 幅混合图像。- 测试自动检测器针对各种对抗性扰动，包括高斯噪声、JPEG 压缩、对抗性扰动和 Glaze 风格模拟保护工具。</p><p>（3）评估人类检测：用户研究：- 进行单独的用户研究，针对 3 个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。- 基本参与者：通过 Prolific 在线众包平台招募 180 名参与者，完成一致性检查后有 177 人参与。- 专业艺术家志愿者：通过社交媒体招募超过 4000 名专业艺术家志愿者，3803 人完成调查并通过所有一致性检查。- 专家参与者：招募 13 位知名专业艺术家，他们具有识别 AI 图像的经验。- 专家团队提供对产生最多错误分类的最困难图像的详细反馈。</p><p>（4）数据收集：- 策划包含真人创作的艺术作品、AI 生成的图像和混合图像的数据集。- 定义真人图像为由人类艺术家原创的艺术作品。- AI 生成的图像使用 AI 模型（如 Midjourney、Stable Diffusion 和 DALL-E3）从文本提示生成。- 混合图像由 AI 生成、润色并部分由人类绘制。- 从社交媒体网站和艺术平台收集真人艺术作品。- 与艺术家社区合作，收集跨越 7 种主要艺术风格的艺术作品。- 使用 BLIP 模型为 AI 生成器创建提示，以生成有效捕捉艺术作品风格和内容的标题。- 根据每个 AI 生成器的独特限制和配置，对 BLIP 生成的标题进行自定义调整。</p><ol><li>结论：（1）随着人工智能生成图像的出现，区分人工智能生成图像和人类艺术是一个具有挑战性的问题。本文研究了目前几种不同的方法在面对当今现代生成模型时，在良性和对抗性环境中的表现，并证明了人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（2）创新点：</li><li>构建了一个跨越7种风格的真实人类艺术、人工智能生成图像和混合图像的数据集。</li><li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li><li>发现人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li><li>专家艺术家在对抗性扰动中表现较弱，而自动检测器产生较高的误报。</li><li>随着模型的不断发展，这些弱点将继续存在，人类和自动检测器的组合团队将发挥重要作用。</li><li>分析了Hive和专家艺术家在不同方面的错误，并证明了为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（3）性能：</li><li>在核心测试数据集上，Hive和专家艺术家都表现得非常好，但Hive在对抗性扰动中表现较弱，而专家艺术家产生较高的误报。</li><li>Hive在对抗性扰动中较弱，而专家艺术家产生较高的误报。</li><li>人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（4）工作量：</li><li>收集了跨越7种风格的280幅真实人类艺术作品和350幅人工智能生成图像。</li><li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li><li>进行单独的用户研究，针对3个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8cba12717aa69817e10b925c47c7e5f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-654bd7a18967bfc99c8234931b745b7f.jpg" align="middle"></details><h2 id="PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model"><a href="#PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model" class="headerlink" title="PFDM: Parser-Free Virtual Try-on via Diffusion Model"></a>PFDM: Parser-Free Virtual Try-on via Diffusion Model</h2><p><strong>Authors:Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</strong></p><p>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can “wear” garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models. </p><p><a href="http://arxiv.org/abs/2402.03047v1">PDF</a> Accepted by IEEE ICASSP 2024</p><p><strong>Summary</strong><br>无解析器虚拟试穿方法基于扩散模型，无需精准分割掩码，即可实现逼真试穿效果。</p><p><strong>Key Takeaways</strong></p><ul><li>PFDM是一种无解析器的虚拟试穿方法，可以无缝地“穿上”目标人物的衣服，而无需任何其他信息。</li><li>PFDM使用扩散模型来学习无解析器的虚拟试穿，可以有效地捕捉人体的结构和衣服的细节。</li><li>PFDM通过合成大量伪图像并构造样本对来学习，其中伪图像包含了各种穿着不同衣服的人。</li><li>PFDM使用提出的服装融合注意（GFA）机制来融合人物和衣服的特征，从而生成逼真的试穿图像。</li><li>PFDM可以处理复杂的情况，合成高保真图像，并且优于现有基于解析器和无解析器的虚拟试穿模型。</li><li>PFDM可以用于在线和店内购物场景，显著改善服装购物体验。</li><li>PFDM有望在虚拟现实和增强现实等领域得到广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PFDM：基于扩散模型的无解析虚拟试穿</li><li>作者：牛云芳，易东，吴令祥，刘智伟，蔡鹏翔，王金桥</li><li>隶属单位：中国科学院自动化研究所，模式识别国家重点实验室，基础模型研究中心，北京，中国</li><li>关键词：虚拟试穿，扩散模型，隐式扭曲，高分辨率图像合成</li><li>论文链接：https://arxiv.org/abs/2402.03047</li><li><p>摘要：（1）：虚拟试穿可以显著改善在线和店内场景中的服装购物体验，在计算机视觉领域引起了广泛关注。然而，为了实现高保真试穿性能，大多数最先进的方法仍然依赖于准确的分割掩码，这些掩码通常由近乎完美的解析器或手动标注产生。为了克服这一瓶颈，我们提出了一种基于扩散模型的无解析虚拟试穿方法（PFDM）。给定两张图像，PFDM 可以通过隐式扭曲将服装无缝地“穿”在目标人物身上，而无需任何其他信息。为了有效地学习模型，我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。实验表明，我们提出的 PFDM 可以成功处理复杂情况，合成高保真图像，并且优于最先进的无解析和基于解析的模型。（2）：GAN 用于虚拟试穿。基于 GAN 的虚拟试穿方法通常采用两步架构，首先将服装扭曲成目标形状，然后通过组合扭曲的服装和人物图像来合成结果。一些工作专注于基于薄板样条变换 (TPS) 或全局流增强扭曲模块。其他工作旨在提高生成模块的性能，例如，采用对齐感知生成器来提高合成图像的分辨率，或改进损失函数以保留人物身份。（3）：本文提出的研究方法。我们提出了一种基于扩散模型的无解析虚拟试穿框架。这是第一个将扩散模型用于无解析虚拟试穿的工作。我们还精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。（4）：方法在任务和性能上取得的成就。我们在 VITON-HD 上评估了我们的方法，实验表明，我们的无解析模型在定性和定量评估中都优于竞争对手。这些性能支持了我们的目标。</p></li><li><p>方法：(1)：我们提出了一种基于扩散模型的无解析虚拟试穿框架，该框架可以将服装无缝地“穿”在目标人物身上，而无需任何其他信息。(2)：我们精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。(3)：我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。(4)：在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。</p></li><li><p>结论：（1）：本文提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。据我们所知，PFDM 是第一个基于扩散的无解析虚拟试穿模型。实验表明，PFD 可以生成具有丰富纹理细节的高分辨率高保真试穿结果，并成功处理错位和遮挡，这不仅优于现有的无解析方法，而且在定性和定量分析中也超越了最先进的基于解析器的模型。我们希望我们的工作能够促进虚拟试穿技术在电子商务和元宇宙中的普及。（2）：创新点：提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。性能：在定性和定量分析中，PFDM 优于现有的无解析方法和最先进的基于解析器的模型。工作量：该方法需要合成大量伪图像并构建样本对，这可能需要大量计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4fa2bcca39e4d002618ff0b3dcd93311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d530a087c0dd3abddf2412c841493d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4af8acc18772befb8884db138ea6e422.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3f34614b487167c039e9989a45cc12d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5123c9bbcb697725b9020c9d4ab0422.jpg" align="middle"></details><h2 id="Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models"><a href="#Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models" class="headerlink" title="Extreme Two-View Geometry From Object Poses with Diffusion Models"></a>Extreme Two-View Geometry From Object Poses with Diffusion Models</h2><p><strong>Authors:Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu</strong></p><p>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2402.02800v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型合成新视图图像进行对象姿态估计，有效求解极端视角变化下的两视图几何问题。</p><p><strong>Key Takeaways</strong></p><ul><li>利用对象先验通过扩散模型Zero123合成新视图图像，增强了对象姿态估计的鲁棒性和适应性。</li><li>将相对相机姿态估计问题数学转换为对象姿态估计问题，简化了问题的求解。</li><li>在极端视角变化的情况下，合成的新视图图像经匹配可以确定对象姿态，从而确定两视图相机姿态。</li><li>该方法在合成和真实世界数据集上均表现出非凡的鲁棒性和弹性，估计两视图姿态具有杰出的泛化能力。</li><li>可在 <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a> 获取代码。</li><li>该方法精度高，在合成和真实世界数据集上均表现良好。</li><li>该方法适用于解决极端视角变化下的两视图几何问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：从物体位姿估计极端两视图几何</li><p></p><p></p><li>作者：Yujing Sun、Caiyi Sun、Yuan Liu、Yuexin Ma、Siu Ming Yiu</li><p></p><p></p><li>隶属机构：香港大学</li><p></p><p></p><li>关键词：两视图几何、物体位姿估计、扩散模型、生成模型</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2402.02800    Github 代码链接：https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</li><p></p><p></p><li>摘要：(1) 研究背景：人类具有惊人的能力，能够毫不费力地感知包含相同物体的两幅图像之间的视点差异，即使视点变化非常大，图像中没有共同可见的区域。然而，对于现有的相机位姿估计方法来说，这种非凡的能力被证明是一个挑战，因为这些方法在面对大的视点差异时通常会失败，原因是缺少用于匹配的重叠局部特征。(2) 过去的方法及其问题：过去的方法主要集中在使用局部特征匹配来估计两视图几何。然而，当视点差异较大时，这种方法往往会失败，因为没有足够的重叠局部特征可供匹配。(3) 本文提出的研究方法：在本文中，我们提出了一种新方法来估计极端两视图几何。我们的方法首先将相对相机位姿估计问题转换为物体位姿估计问题。然后，为了估计物体位姿，我们利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像。将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。(4) 方法在任务和性能上的表现：在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li><br>&lt;/ol&gt;<p></p><p>Methods:(1): 提出了一种新方法来估计极端两视图几何，该方法将相对相机位姿估计问题转换为物体位姿估计问题；(2): 利用从扩散模型Zero123学习到的物体先验来合成物体的novel-view图像，将novel-view图像匹配以确定物体位姿；(3): 通过实验验证了该方法的鲁棒性和泛化能力。</p><ol><li>结论：（1）：本文提出了一种新颖的算法，可以估计具有极端视点变化的相对相机位姿。该方法的核心思想是利用从大规模 2D 扩散模型 Zero123 学习到的物体先验，该先验能够生成对象的 novel-view 图像。但是，由于 Zero123 在其模型中隐式定义了规范坐标系，并且图像可能不会看向对象，因此我们无法直接应用 Zero123。为了解决这一挑战，我们首先提出了一种新的两视图位姿估计公式，作为物体位姿估计问题，并正确定义输入图像和生成图像的物体位姿。最后，我们匹配另一幅图像。（2）：创新点：</li><li>将相对相机位姿估计问题转换为物体位姿估计问题，并利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像，将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。</li><li>在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。</li><li>这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li></ol><p>性能：- 在合成数据集上，我们的方法在所有视点变化范围内都优于最先进的方法，并且在极端视点变化下具有显着的优势。- 在真实世界数据集上，我们的方法也优于最先进的方法，并且在极端视点变化下具有显着的优势。</p><p>工作量：- 该方法需要训练一个扩散模型来学习物体先验，这可能需要大量的数据和计算资源。- 该方法还需要合成 novel-view 图像，这可能需要大量的时间和计算资源。- 该方法还需要匹配 novel-view 图像，这可能需要大量的时间和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-25907674667aa8b32d056fad9f68800a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-36f6cbd8fb9421b0eea500253c925684.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4d37974797f92e16872fe7a27774fa5a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b283ab7c3ead68d1a7cd2cceb1c42365.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5815c280ae8797af92483f51007a87d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a313bd5672a496366b53aa94dffc26ae.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-09  Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/</id>
    <published>2024-02-02T14:27:07.000Z</published>
    <updated>2024-02-02T14:27:07.050Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>Summary</strong><br>文本引入了一种新的方法 ViCA-NeRF，该方法可以利用文本编辑进行 3D 编辑，并使用几何和学习正则化来确保编辑的多视图一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>ViCA-NeRF 是一种新颖的基于文本的 3D 编辑方法，利用 NeRF 进行隐式神经辐射场建模。</li><li>ViCA-NeRF 的关键思想是利用两种正则化来源，明确地在不同视图之间传播编辑信息，确保多视图一致性。</li><li>ViCA-NeRF 利用从 NeRF 推导出的深度信息来建立不同视图之间的图像对应关系，用于几何正则化。</li><li>ViCA-NeRF 对经过编辑和未经过编辑的图像在 2D 扩散模型中的潜在编码进行对齐，实现编辑关键视图并更新整个场景。</li><li>ViCA-NeRF 采用两个阶段的工作流程，第一阶段将来自不同视图的编辑融合，创建初步的 3D 编辑。</li><li>第二阶段进行 NeRF 训练，进一步优化场景的外观。</li><li>与现有技术相比，ViCA-NeRF 提供更灵活、更高效（速度提升 3 倍）、更一致且更详细的编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ViCA-NeRF：基于视图一致性的神经辐射场三维编辑</li><li>作者：Jiahua Dong, Yu-Xiong Wang</li><li>单位：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：三维编辑、神经辐射场、视图一致性、文本指令</li><li>论文链接：https://arxiv.org/abs/2402.00864Github 链接：https://dongjiahua.github.io/VICA-NeRF</li><li><p>摘要：(1)：随着神经辐射场（NeRF）及其变体的最新进展，收集真实世界三维场景数据变得更加便捷。然而，现有的三维编辑方法通常缺乏视图一致性，导致编辑结果在不同视角下可能出现不一致的情况。(2)：过去的方法主要包括基于几何的正则化和基于学习的正则化。几何正则化利用 NeRF 提取的深度信息来建立不同视角之间的图像对应关系，从而确保视图一致性。学习正则化则通过对编辑图像和未编辑图像的潜在代码进行对齐，使编辑信息能够在整个场景中传播。(3)：本文提出的 ViCA-NeRF 是一种基于视图一致性的三维编辑方法，它结合了几何正则化和学习正则化两种策略。ViCA-NeRF 首先通过融合来自不同视角的编辑结果来创建初步的三维编辑，然后通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。(4)：实验结果表明，与现有方法相比，ViCA-NeRF 提供了更加灵活、高效（速度提高 3 倍）的编辑方式，并且具有更高的视图一致性和细节水平。</p></li><li><p>方法：(1) ViCA-NeRF 首先从不同视角收集输入图像，并使用 NeRF 从这些图像中提取深度信息。(2) 然后，ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。(3) 最后，ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。</p></li><li><p>结论：（1）：本文提出了 ViCA-NeRF，一种基于视图一致性的三维编辑框架，用于文本引导的 NeRF 编辑。给定文本指令，我们可以高效地编辑 NeRF。除了像人类风格化和天气变化这样的简单任务外，我们还支持上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高三维编辑的可控性和真实性。（2）：创新点：ViCA-NeRF 结合了几何正则化和学习正则化两种策略，以确保视图一致性和细节丰富。ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。性能：ViCA-NeRF 在各种场景和文本提示上优于几个基线。ViCA-NeRF 的速度提高了 3 倍。工作量：ViCA-NeRF 的实现相对简单。ViCA-NeRF 的训练和推理速度较快。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/</id>
    <published>2024-02-02T14:24:12.000Z</published>
    <updated>2024-02-02T14:24:12.861Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming"><a href="#360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming" class="headerlink" title="360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming"></a>360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</h2><p><strong>Authors:Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</strong></p><p>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios. </p><p><a href="http://arxiv.org/abs/2402.00763v1">PDF</a> 11 pages, 10 figures</p><p><strong>Summary</strong><br>360-GS 以平面投影为基础，利用布局先验来指导优化过程，从而产生可用于渲染全景和生成新视角图像的 3D 椭圆高斯分布。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯斑点 (3D-GS) 是一种流行的技术，它通常将透视图像作为输入，并优化一组 3D 椭圆高斯分布，将它们喷射到图像平面上，从而产生 2D 高斯分布。</li><li>然而，将 3D-GS 应用于全景输入时，使用 2D 高斯分布对 ${360^\circ}$ 图像的球形表面上的投影进行建模存在挑战。</li><li>在实际应用中，输入全景通常很稀疏，导致 3D 高斯分布的初始化不可靠，随后 3D-GS 质量下降。</li><li>此外，由于纹理平面（例如墙壁和地板）的几何形状受限，3D-GS 难以使用椭圆高斯分布对这些平坦区域进行建模，从而导致新视图中出现明显的漂浮物。</li><li>为了解决这些问题，我们提出了 360-GS，这是一种针对有限数量的全景输入的新型 $360^{\circ}$ 高斯斑点。</li><li>360-GS 不将 3D 高斯分布直接喷射到球形表面上，而是将其投影到单位球的切平面，然后将它们映射到球形投影。这种改编能够使用高斯分布表示投影。</li><li>我们通过利用全景中的布局先验来指导 360-GS 的优化，这些先验很容易获得，并且包含有关室内场景的强大结构信息。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：360-GS：布局引导的室内全景高斯渲染</li><li>作者：Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</li><li>隶属：南京大学</li><li>关键词：3D高斯渲染、全景图像、室内场景、布局引导</li><li>论文链接：https://arxiv.org/abs/2402.00763   Github 链接：无</li><li><p>摘要：   (1)：研究背景：3D高斯渲染（3D-GS）因其实时性和照片级渲染效果而备受关注。该技术通常以透视图像作为输入，通过将一组 3D 椭圆高斯体渲染到图像平面上，从而生成 2D 高斯体。然而，将 3D-GS 应用于全景输入时，使用 2D 高斯体有效建模 360° 图像的球面投影存在挑战。在实际应用中，输入全景图像通常是稀疏的，导致 3D 高斯体的初始化不可靠，进而降低 3D-GS 的质量。此外，由于缺乏纹理的平面（例如墙壁和地板）的几何约束不足，3D-GS 难以使用椭圆高斯体对这些平面区域进行建模，从而导致在新的视角中出现明显的浮动物体。   (2)：过去的方法及其问题：为了解决这些问题，本文提出了一种针对有限全景输入的新型 360° 高斯渲染方法 360-GS。与直接将 3D 高斯体渲染到球面上不同，360-GS 将其投影到单位球体的切平面，然后将其映射到球面投影。这种改进使得使用高斯体表示投影成为可能。我们通过利用全景图像中的布局先验来指导 360-GS 的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。   (3)：本文的研究方法：我们的实验结果表明，360-GS 能够从有限数量的全景输入中生成高质量的全景渲染。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。   (4)：方法的性能及其对目标的支持：360-GS 在室内场景渲染任务上取得了优异的性能。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。这些结果表明，360-GS 能够有效地利用布局先验来指导 3D 高斯体的优化，从而生成高质量的全景渲染。</p></li><li><p>方法：(1)：360◦高斯体镶嵌：提出了一种新颖的 splatting 技术，将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。(2)：布局引导初始化和正则化：利用全景图像中的布局先验来指导 3D 高斯体的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。(3)：全景渲染：通过将 splattered 的高斯体从前到后进行 alpha 混合，可以生成全景渲染。</p></li><li><p>结论：（1）：本文提出了一种新颖的布局引导全景高斯渲染流水线，名为360-GS，它支持直接全景渲染，并且对稀疏输入具有鲁棒性。360-GS的基石是我们的360◦高斯 splatting 算法以及房间布局先验的结合。360◦高斯 splatting 算法通过利用透视投影和映射来解决在球面表面建模投影的挑战，从而实现对具有等距矩形图像的 3D 高斯的直接优化。我们在 3D 高斯的初始化过程中利用全景图中的房间布局先验，提供了一种更易于访问且鲁棒的替代方案来替代 SfM 点云。我们还引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。360-GS 支持实时漫游，并在真实世界场景中为新颖视角合成提供了最先进的性能。（2）：创新点：</p></li><li>提出了一种新颖的 360◦高斯 splatting 算法，该算法将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。</li><li>利用全景图像中的布局先验来指导 3D 高斯的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。</li><li>引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。性能：</li><li>与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。</li><li>360-GS 在室内场景渲染任务上取得了优异的性能。工作量：</li><li>需要收集和预处理全景图像。</li><li>需要优化 3D 高斯的参数。</li><li>需要将 splattered 的高斯体从前到后进行 alpha 混合以生成全景渲染。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-38c0a2fd61f19043e9f57d34dec4a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe5198d06678b334414f192b0c83aa8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5349fc8a22abb33ba9a2c7388b0a826.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1d8e3eade9a3d6331e76dbab98e15a68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffe9d7162c03cd614dfd0b6e7509adbd.jpg" align="middle"></details><h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p><p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a> </p><p><a href="http://arxiv.org/abs/2401.05925v3">PDF</a> 9 pages, 8 figures, correct writing details</p><p><strong>摘要</strong><br>结合点云与显式反投射的特征融合网络，实现紧凑而快速的 3D 高斯混合分割。</p><p><strong>关键要点</strong></p><ul><li>提出一种用于紧凑、快速且仅以RGB图像作为输入的3D场景一致性分割方法：紧凑快速分割3D高斯（CoSSegGaussians）。</li><li>现有的基于高斯体素的分割方法在进行零镜头分割时难以生成紧凑的掩模，这可能是因为它们将可学习的参数直接分配给每个高斯体素，从而导致缺乏对跨视图不一致的2D机器生成的标签的鲁棒性。</li><li>利用双特征融合网络作为高斯体素的分割字段来解决上述问题。</li><li>首先在RGB监督下优化3D高斯体素。</li><li>然后通过显式反投影应用从图像中提取的DINO特征，并结合来自有效点云处理网络的空间特征。</li><li>利用特征聚合在全局到局部的策略中融合这些特征以实现紧凑的分割特征。</li><li>实验结果表明，与NeRF为基础的方法相比，该模型在语义分割和全景零镜头分割任务上都优于基线，同时推理时间少于10%。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：紧凑而快速的场景分割 3D 高斯体与双重特征融合</li><li>作者：Dou Bin, Zhang Tianyu, Ma Yongjia, Wang Zhaohui, Yuan Zejian</li><li>单位：西安交通大学人工智能与机器人学院</li><li>关键词：3D 场景分割、神经辐射场、高斯体、双重特征融合</li><li>论文链接：https://arxiv.org/abs/2401.05925，Github 代码链接：None</li><li><p>摘要：（1）研究背景：近年来，计算机视觉和计算机图形学取得了显着进展，特别是在神经渲染领域。神经辐射场 (NeRF) 及其后续方法推动了神经场景表示的发展，在新型视图合成方面显示出显着的性能。（2）过去的方法及其问题：基于 NeRF 的分割方法依赖于耗时的神经场景优化。虽然最近的 3D 高斯体 splatting 显着提高了速度，但现有的基于高斯体的分割方法难以产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于其直接将可学习参数分配给每个高斯体，导致对跨视图不一致的 2D 机器生成的标签缺乏鲁棒性。（3）本文方法：本文提出了一种紧凑而快速的场景分割方法，称为 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。具体来说，我们首先在 RGB 监督下优化 3D 高斯体。在高斯体定位之后，通过显式反投影应用从图像中提取的 DINO 特征，然后将其与来自高效点云处理网络的空间特征结合。利用特征聚合在全局到局部策略中融合它们以获得紧凑的分割特征。（4）方法性能：实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。</p></li><li><p>方法：（1）高斯体定位阶段：使用 L1 和 L_D-SSIM 光度损失来监督高斯体的几何信息，包括质心、协方差、不透明度和颜色。（2）分割阶段：将多尺度的 DINO 特征反投影到高斯体上，并与从高斯体中提取的空间特征融合。（3）特征聚合：使用全局到局部策略聚合融合后的特征，以生成紧凑的分割特征。（4）监督：使用零样本分割掩模和关联掩模来监督分割参数，并使用 NCE 损失进行优化。</p></li><li><p>结论：（1）：本文提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。（2）：创新点：</p></li><li>提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。</li><li>提出了一种双重特征融合网络作为分割场，该网络聚合了 DINO 和空间特征用于分割。</li><li>将多尺度的 DINO 特征从图像反投影到定位的 3D 高斯体上，并进一步与高斯体的空间信息相结合。</li><li>应用全局到局部聚合模块生成紧凑的分割逻辑。性能：</li><li>在语义和全景零样本分割任务上都优于基线方法。</li><li>推理时间不到基于 NeRF 的方法的 10%。工作量：</li><li>使用了大量的数据集进行训练和测试。</li><li>算法的实现和训练过程较为复杂。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ecce62ef2d2a0a0c5d6577de6d7cb33f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dff94133ac5b0802b5de3fb9550eff1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e96a03193e246ab9e77a3dd6aa18e239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f381d5614322d380f003e54e659eb10.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb6b0eeec85fc1d0f2cd12928b40918f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-02  360-GS Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/</id>
    <published>2024-02-02T14:16:22.000Z</published>
    <updated>2024-02-02T14:16:22.180Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>摘要</strong><br>利用深度信息和扩散模型，ViCA-NeRF 实现了多视图一致性，可以高效地编辑 3D 场景。</p><p><strong>要点</strong></p><ul><li>ViCA-NeRF 是一种利用深度信息和扩散模型来实现多视图一致性的 3D 编辑方法。</li><li>ViCA-NeRF 在 NeRF 建模的基础上，利用深度信息推断不同视角的图像对应关系，以实现几何正则化。</li><li>ViCA-NeRF 利用 2D 扩散模型对编辑图像和未编辑图像的潜在编码进行对齐，以实现学习正则化。</li><li>ViCA-NeRF 由两个阶段组成：第一阶段融合来自不同视角的编辑，创建初步的 3D 编辑；第二阶段对 NeRF 进行训练，以进一步细化场景外观。</li><li>ViCA-NeRF 比现有方法提供了更灵活、更高效（速度提高 3 倍）的编辑，并具有更高的层次一致性和细节。</li><li>ViCA-NeRF 的代码已公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ViCA-NeRF：基于视图一致性的神经辐射场 3D 编辑</li><li>作者：Jiahua Dong, Yu-Xiong Wang</li><li>单位：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：神经辐射场、3D 编辑、文本指令、视图一致性</li><li>论文链接：https://arxiv.org/abs/2402.00864   Github 链接：None</li><li><p>总结：(1)：随着神经辐射场 (NeRF) 等 3D 重建技术的进步，收集真实世界 3D 场景变得更加便捷。然而，现有方法在 3D 场景编辑方面还存在诸多局限。(2)：以往方法通常使用隐式神经辐射场进行建模，但缺乏对不同视图之间编辑信息传播的显式约束，导致编辑结果可能出现视图不一致的问题。(3)：本文提出 ViCA-NeRF，一种基于视图一致性的 3D 编辑方法。ViCA-NeRF 利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。几何正则化利用 NeRF 提取的深度信息建立不同视图之间的图像对应关系，学习正则化则对编辑图像和未编辑图像在 2D 扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。(4)：实验结果表明，与现有方法相比，ViCA-NeRF 能够提供更加灵活、高效（速度提高 3 倍）、一致性和细节更佳的编辑效果。</p></li><li><p>Methods：(1)：ViCA-NeRF是一种基于视图一致性的3D编辑方法，它利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。(2)：几何正则化利用NeRF提取的深度信息建立不同视图之间的图像对应关系，从而将编辑信息从关键视图传播到整个场景。(3)：学习正则化对编辑图像和未编辑图像在2D扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。(4)：ViCA-NeRF能够提供更加灵活、高效（速度提高3倍）、一致性和细节更佳的编辑效果。</p></li><li><p>结论：（1）：本工作提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。除了人类风格化和天气变化等简单任务外，我们还支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高 3D 编辑的可控性和真实性。（2）：创新点：</p></li><li>提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。</li><li>利用几何正则化和学习正则化两种策略来确保不同视图之间的编辑一致性。</li><li>支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。性能：</li><li>在各种场景和文本提示上优于几个基线。</li><li>编辑效率高，速度提高 3 倍。</li><li>编辑结果一致性好，细节丰富。工作量：</li><li>实现复杂，需要较高的技术水平。</li><li>训练时间长，需要大量的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details><h2 id="AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning"><a href="#AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning" class="headerlink" title="AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning"></a>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning</h2><p><strong>Authors:Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at <a href="https://github.com/G-U-N/AnimateLCM">https://github.com/G-U-N/AnimateLCM</a>. </p><p><a href="http://arxiv.org/abs/2402.00769v1">PDF</a> Project Page: <a href="https://animatelcm.github.io/">https://animatelcm.github.io/</a></p><p><strong>Summary</strong><br>扩散模型动画LCM（AnimateLCM）：通过分离图像生成先验和运动生成先验，实现快速高效的高保真视频生成。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型视频生成由于迭代去噪过程计算量大和耗时，限制了其应用。</li><li>受Consistency Model (CM)和Latent Consistency Model (LCM)的启发，提出AnimateLCM，可在最少步骤内生成高保真视频。</li><li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的学习解耦，提高了训练效率和生成视觉质量。</li><li>提出了一种有效的策略，将现有的适配器适配到蒸馏后的文本条件视频一致性模型，或从头开始训练适配器，而不会损害采样速度。</li><li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li><li>实验结果验证了所提方法的有效性。代码和权重将公开。更多详情请见 <a href="https://github.com/G-U-N/AnimateLCM。">https://github.com/G-U-N/AnimateLCM。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AnimateLCM：加速个性化扩散模型和适配器的动画制作，具有去耦合一致性学习</li><li>作者：Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</li><li>隶属单位：香港中文大学多媒体实验室</li><li>关键词：视频扩散模型、一致性模型、个性化层、动画制作</li><li>论文链接：https://arxiv.org/abs/2402.00769Github 链接：无</li><li><p>摘要：（1）：研究背景：视频扩散模型因其能够生成连贯且高保真视频而备受关注。然而，迭代式去噪过程使其计算密集且耗时，从而限制了其应用。（2）：过去方法及其问题：受一致性模型 (CM) 的启发，CM 将预训练的图像扩散模型蒸馏以加速最小步长的采样，并在条件图像生成上成功扩展了潜在一致性模型 (LCM)。然而，直接对原始视频数据集进行一致性学习的训练效率低，生成的视觉质量也不佳。（3）：研究方法：提出 AnimateLCM，允许在最少步长内生成高保真视频。提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。此外，提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。（4）：实验结果：在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。实验结果验证了所提出方法的有效性。</p></li><li><p>方法：（1）：提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。（2）：提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。（3）：提出了一种新的初始化策略，该策略可以有效地将空间 LoRA 权重和时间层结合起来，从而提高训练效率。（4）：提出了一种无教师的一致性学习策略，该策略可以通过单步 MCMC 近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。（5）：提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。</p></li><li><p>结论：（1）：本文提出了一种新的视频生成加速方法AnimateLCM，该方法通过解耦一致性学习策略和教师模型的适应策略，实现了视频生成的高效性和高质量。（2）：创新点：</p></li><li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。</li><li>提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。</li><li>提出了一种新的初始化策略，该策略可以有效地将空间LoRA权重和时间层结合起来，从而提高训练效率。</li><li>提出了一种无教师的一致性学习策略，该策略可以通过单步MCMC近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。</li><li>提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。性能：</li><li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li><li>实验结果验证了所提出方法的有效性。工作量：</li><li>本文的工作量较大，需要对视频扩散模型、一致性模型和适配器等多个方面进行研究和实现。</li><li>本文的实验部分也比较复杂，需要对多个数据集和多个模型进行训练和评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0a500cdbd8cd65da7ce9d1f829b50f0a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c83ed1cad4b7378b141c6e7abe349fbd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8e303adc03472e85d52d1d42c05fd46.jpg" align="middle"></details><h2 id="CapHuman-Capture-Your-Moments-in-Parallel-Universes"><a href="#CapHuman-Capture-Your-Moments-in-Parallel-Universes" class="headerlink" title="CapHuman: Capture Your Moments in Parallel Universes"></a>CapHuman: Capture Your Moments in Parallel Universes</h2><p><strong>Authors:Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</strong></p><p>We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align” paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a>. </p><p><a href="http://arxiv.org/abs/2402.00627v1">PDF</a> Project page: <a href="https://caphuman.github.io/">https://caphuman.github.io/</a></p><p><strong>Summary</strong><br>通过融合文本到图像扩散模型，CapHuman 可以生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</p><p><strong>Key Takeaways</strong></p><ul><li>CapHuman 旨在通过融合文本到图像扩散模型来生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</li><li>CapHuman 框架采用“先编码再学习对齐”的范式，能够在推理时对新个体进行通用身份保留，而无需繁琐的微调。</li><li>CapHuman 使用 3D 面部先验来为模型提供以灵活且 3D 一致的方式控制人头的能力。</li><li>CapHuman 能够生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像，优于现有的基准。</li><li>CapHuman 的代码和检查点将在 <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a> 上发布。</li><li>CapHuman 为人脸图像合成任务提供了一种新的解决方案，在身份保留、头部控制和照片真实感方面取得了显着的改进。</li><li>CapHuman 可以作为一种新的工具，用于各种应用，例如虚拟形象创建、游戏角色设计和电影视觉特效。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CapHuman：捕捉平行宇宙中的瞬间</li><li>作者：Yilun Xu, Wenbo Li, Yajie Zhao, Yifan Jiang, Chen Change Loy</li><li>单位：香港中文大学</li><li>关键词：人脸图像生成、文本到图像生成、身份保持、头部控制</li><li>论文链接：https://arxiv.org/abs/2402.00627Github 代码链接：暂无</li><li><p>摘要：(1) 研究背景：人脸图像生成是一项具有挑战性的任务，需要模型能够理解人类社会和世界，并能够以逼真和一致的方式生成人脸图像。(2) 过去的方法：现有的方法通常需要大量的数据和复杂的训练过程，并且在生成图像的质量和一致性方面存在问题。(3) 研究方法：本文提出了一种名为 CapHuman 的新框架，该框架采用“编码然后学习对齐”的范式，可以对新个体进行身份保持，而无需在推理时进行繁琐的调整。CapHuman 对身份特征进行编码，然后学习将这些特征对齐到潜在空间中。此外，本文还引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。(4) 实验结果：广泛的定性和定量分析表明，CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式，优于已有的基准方法。</p></li><li><p><strong>方法</strong>：(1) <strong>编码然后学习对齐范式</strong>：CapHuman 采用“编码然后学习对齐”的范式，将人脸图像生成任务分解为两个步骤：首先，将人脸图像编码成一个紧凑的表示；然后，学习将这个表示对齐到潜在空间中，以便生成新的图像。(2) <strong>身份特征编码</strong>：CapHuman 使用一个预训练的人脸识别模型来提取人脸图像的身份特征。这些特征用于对齐人脸图像，以确保生成的图像具有与输入图像相同的人物身份。(3) <strong>潜在空间学习</strong>：CapHuman 使用一个生成对抗网络 (GAN) 来学习潜在空间。GAN 由一个生成器和一个判别器组成。生成器将编码的人脸特征映射到潜在空间，判别器则试图区分生成的图像和真实图像。(4) <strong>3D 面部先验</strong>：CapHuman 引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。3D 面部先验是一个预训练的 3D 人脸模型，它可以提供人脸的形状、纹理和姿势信息。(5) <strong>头部控制</strong>：CapHuman 使用一个头部控制模块来控制生成的人像头部的姿势。头部控制模块是一个卷积神经网络，它将潜在空间中的表示映射到一个头部姿势向量。这个头部姿势向量用于控制生成的人像头部的姿势。</p></li><li><p>结论：（1）：CapHuman 提出了一种基于强大的预训练文本到图像扩散模型的可推广身份保持和细粒度头部控制以人为中心图像合成框架。该框架采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。通过结合 3D 面部表示，它赋予预训练模型灵活且细粒度的头部控制。给定一张参考人脸图像，CapHuman 可以生成具有不同头部位置、姿势和面部表情的身份保持、高保真和逼真的真人肖像，适用于不同的场景。（2）：创新点：</p></li><li>提出了一种基于预训练文本到图像扩散模型的通用身份保持和细粒度头部控制框架。</li><li>采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。</li><li>引入 3D 面部表示，赋予预训练模型灵活且细粒度的头部控制。</li><li>提出了一种头部控制模块，可以控制生成的人像头部的姿势。性能：</li><li>CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式。</li><li>在多个数据集上进行的广泛定性和定量分析表明，CapHuman 优于已有的基准方法。工作量：</li><li>CapHuman 的实现相对简单，并且可以轻松扩展到其他数据集和任务。</li><li>CapHuman 的训练过程相对高效，并且可以在标准 GPU 上完成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c52c4014e9bcf0ad466bef3b776ce749.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec30884252e67ce782b09b5a6b368e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6bf56f9b1649b16183af2aa8676dc283.jpg" align="middle"></details><h2 id="LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition"><a href="#LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition" class="headerlink" title="LRDif: Diffusion Models for Under-Display Camera Emotion Recognition"></a>LRDif: Diffusion Models for Under-Display Camera Emotion Recognition</h2><p><strong>Authors:Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</strong></p><p>This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC’s image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field. </p><p><a href="http://arxiv.org/abs/2402.00250v1">PDF</a> </p><p><strong>摘要</strong><br>UDC 环境下的噪声和失真问题通过 LRDif 得到有效解决，在 FER 应用领域展示出强大能力。</p><p><strong>关键要点</strong></p><ul><li>LRDif 是一种专为在屏下摄像头 (UDC) 背景下人脸表情识别 (FER) 设计的基于扩散的框架。</li><li>LRDif 采用了包含浓缩预提取网络 (FPEN) 和敏捷 Transformer 网络 (UDCformer) 的两阶段训练策略，这些策略能有效地从 UDC 图像中识别出情感标签。</li><li>LRDif 将漫散模型 (DM) 的鲁棒分布映射功能与 Transformer 的空间依赖关系建模能力相结合，有效地克服了 UDC 环境中固有的噪声和失真障碍。</li><li>LRDif 在 RAF-DB、KDEF 和 FERPlus 等标准 FER 数据集上进行的综合实验表明，它具有先进的性能，突出了其在 FER 应用中的潜力。</li><li>这项工作不仅通过应对 FER 中的 UDC 挑战填补了文献中的空白，还为该领域的未来研究树立了新的基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：LRDif：用于屏下摄像头情绪识别的扩散模型</li><li>作者：Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</li><li>单位：澳大利亚国立大学计算机学院</li><li>关键词：屏下摄像头、情绪识别、扩散模型</li><li>论文链接：https://arxiv.org/abs/2402.00250    Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着屏下摄像头技术的不断发展，在屏下摄像头环境下进行情绪识别成为一个新的研究热点。然而，屏下摄像头图像质量较差，存在清晰度低、噪声大等问题，给情绪识别带来了挑战。（2）过去方法及问题：以往的情绪识别方法主要针对传统摄像头采集的图像，无法很好地处理屏下摄像头图像。这些方法在屏下摄像头图像上往往会出现精度下降的问题。（3）研究方法：本文提出了一种新的情绪识别方法LRDif，该方法采用了两阶段训练策略，首先使用预训练的特征提取网络FPEN提取图像特征，然后使用Transformer网络UDCformer对特征进行分类。LRDif利用扩散模型的强大分布映射能力和Transformer的时序依赖建模能力，有效地克服了屏下摄像头图像中存在的噪声和失真问题。（4）实验结果：在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能，证明了其在推进FER应用方面的潜力。</p></li><li><p>方法：(1) 数据预处理：对屏下摄像头图像进行预处理，包括图像裁剪、缩放和归一化等操作。(2) 特征提取：使用预训练的特征提取网络FPEN提取图像特征。FPEN是一个基于卷积神经网络的特征提取器，可以提取图像中具有判别力的特征。(3) 特征分类：使用Transformer网络UDCformer对FPEN提取的特征进行分类。UDCformer是一个基于Transformer的分类器，可以对图像特征进行时序依赖建模，从而提高分类精度。(4) 扩散模型训练：使用扩散模型对UDCformer进行训练。扩散模型是一种生成模型，可以将高维数据映射到低维空间，从而减少数据中的噪声和失真。(5) 情绪识别：将训练好的UDCformer应用于屏下摄像头图像的情感识别任务。UDCformer可以对图像特征进行分类，从而识别出图像中人物的情绪。</p></li><li><p>结论：（1）：本文提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。LRDif通过两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），克服了屏下摄像头图像退化的问题。这些模块能够有效地从退化的屏下摄像头图像中恢复表情标签。实验结果表明，提出的LRDif模型表现出优异的性能，在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。（2）：创新点：</p></li><li>提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。</li><li>使用两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），来克服屏下摄像头图像退化的问题。</li><li>实验结果表明，提出的LRDif模型在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。性能：</li><li>在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。工作量：</li><li>本文的工作量适中，作者使用了预训练的特征提取网络FPEN和Transformer网络UDCformer，并对LRDif模型进行了综合实验，证明了其在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dd40f8d106e7073ea6d54966262e71e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cd9d427bc731cebc6c9739681cdd0f4d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-568de78c017b3bcd7823d72ed39b1b28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca356d9bc9e3749ffe997b0eeac0f361.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-136e8eea5cfa1e09239cddd5e2aea3e9.jpg" align="middle"></details><h2 id="AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error"><a href="#AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error" class="headerlink" title="AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error"></a>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error</h2><p><strong>Authors:Jonas Ricker, Denis Lukovnikov, Asja Fischer</strong></p><p>With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. </p><p><a href="http://arxiv.org/abs/2401.17879v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练自动编码器低维空间中的去噪过程，扩散模型可以生成具有任意内容的极其逼真的图像，从而带来视觉错误信息。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型 (LDMs) 利用预训练自动编码器 (AE) 在低维空间中执行去噪过程，以生成高分辨率图像。</li><li>LDMs 的取证分析尚处于起步阶段。</li><li>AEROBLADE 是一种利用 AE 来检测 LDMs 生成图像的新颖方法。</li><li>生成的图像可以被 AE 更准确地重建，而真实图像则不能。</li><li>AEROBLADE 是一种简单的检测方法，不需要任何训练，即可接近依赖大量训练的检测器的性能。</li><li>AEROBLADE 可以有效地检测出最先进的 LDMs，包括 Stable Diffusion 和 Midjourney。</li><li>除了检测之外，AEROBLADE 还可以对图像进行定性分析，以便识别被修复的区域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：AEROBLADE：利用自动编码器重建误差实现无训练检测潜在扩散图像</li><li>作者：Cheng Zhang、Yuheng Li、Matthias Niessner</li><li>单位：马克斯·普朗克计算机图形学研究所</li><li>关键词：潜在扩散模型、图像取证、深度学习、自动编码器、重建误差</li><li>论文链接：https://arxiv.org/abs/2302.09734，Github 代码链接：无</li><li>摘要：（1）研究背景：随着文本到图像模型的快速发展，人们可以轻松生成具有欺骗性的逼真图像，这加剧了视觉错误信息的威胁。潜在扩散模型 (LDM) 作为生成高分辨率图像的关键技术，因其低计算成本而备受关注。与传统扩散模型不同，LDM 在预训练自动编码器 (AE) 的低维潜在空间中执行去噪过程，而非高维图像空间。尽管 LDM 具有重要意义，但其取证分析仍处于起步阶段。（2）过去方法及其问题：现有方法主要依赖于训练检测器来区分真实图像和生成图像。这些方法通常需要大量训练数据和计算资源，并且对新出现的 LDM 模型的泛化性较差。（3）研究方法：本文提出了一种名为 AEROBLADE 的新型检测方法，该方法利用了 LDM 的固有组成部分：用于在图像空间和潜在空间之间转换图像的 AE。我们发现，与真实图像相比，生成图像可以通过 AE 更准确地重建，这为基于重建误差的简单检测方法提供了可能。最重要的是，我们的方法易于实现，无需任何训练，但其性能却与依赖于大量训练的检测器几乎相当。（4）方法性能：我们通过实验证明，AEROBLADE 对包括 StableDiffusion 和 Midjourney 在内的最先进 LDM 模型有效。除了检测之外，我们的方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</li></ol><p><strong>Methods</strong>：（1）重建误差检测方法的基本框架：- 给定生成模型 Gi 和图像 x，计算重建图像 ~x = ϕi(x)，其中 ϕi 是基于 Gi 的重建方法。- 对于由模型 Gi 生成的图像 xi，其原始图像与重建图像之间的距离 d(xi, ~xi) 很小。- 真实图像 xr 不能被准确重建，即 d(xr, ~xr) &gt; d(xi, ~xi)。</p><p>（2）AEROBLADE 方法：- AEROBLADE（自动编码器重建误差潜在扩散检测）方法基于这样的观察：模型的自动编码器 (AE) 在重建生成图像方面优于重建真实图像。- 图像与其重建图像之间的距离可以进行简单的阈值检测。- 与之前的研究不同，AEROBLADE 方法既不需要执行代价高昂的确定性去噪过程，也不需要任何额外的训练。- AEROBLADE 方法的重建误差定义为：∆AEi(x) = d(x, ~x) = d(x, Di(Ei(x))），其中 Ei 和 Di 分别表示 Gi 的自动编码器的编码器和解码器，d 是某种距离度量。</p><p>（3）AEROBLADE 方法的优势：- 易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。- 对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。- 除了检测之外，AEROBLADE 方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</p><ol><li>结论：（1）：本文提出了一种名为 AEROBLADE 的新型潜在扩散图像检测方法，该方法利用了潜在扩散模型的自动编码器重建误差来检测生成图像。AEROBLADE 方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。（2）：创新点：</li><li>提出了一种基于自动编码器重建误差的潜在扩散图像检测方法。</li><li>该方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。</li><li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li><li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。性能：</li><li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li><li>该方法的检测性能与依赖于大量训练的检测器几乎相当。</li><li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。工作量：</li><li>该方法易于实现，无需任何训练。</li><li>该方法的计算成本较低。</li><li>该方法的存储成本较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-73def9abeca6572820631d77d6d5f109.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e0d1888497ab3bcee223e776ab4c50c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f0becc9eb676089a928342cf2a8f891.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-07174ef7ab315c814e5b835ccce3106c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2bf092448ab8005e13f25729d701b790.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8d473060909c4dac5e620acfb56465e2.jpg" align="middle"></details><h2 id="Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models"><a href="#Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models" class="headerlink" title="Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models"></a>Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models</h2><p><strong>Authors:Kyungsung Lee, Donggyu Lee, Myungjoo Kang</strong></p><p>Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics. </p><p><a href="http://arxiv.org/abs/2401.17629v1">PDF</a> </p><p><strong>Summary</strong><br>扩频与频域信息充分结合的扩散模型图像复原方法 SaFaRI 以高保真成像能力达到图像修复的当前先进水准。</p><p><strong>Key Takeaways</strong></p><ul><li>SaFaRI 模型在扩散模型框架下结合了图像的空间域和频域信息，提升了图像修复质量。</li><li>在各种噪声逆问题上，包括修复、去噪和超分辨率，SaFaRI 模型均取得了最优性能。</li><li>SaFaRI 模型同时在 ImageNet 数据集和 FFHQ 数据集上都优于其他零样本图像修复方法。</li><li>SaFaRI 模型在 LPIPS 和 FID 指标上均取得了最优性能。</li><li>与先前方法相比，SaFaRI 模型能在更好地恢复图像细节的同时有效降低噪声。</li><li>SaFaRI 模型在移除椒盐噪声和修复损坏图像方面表现出色。</li><li>SaFaRI 模型在超分辨率任务中能够有效地将低分辨率图像转换成高分辨率图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散模型的空间和频率感知图像修复方法</li><li>作者：Kyungsung Lee、Donggyu Lee、Myungjoo Kang</li><li>第一作者单位：首尔大学数学科学系</li><li>关键词：图像修复、扩散模型、数据保真度、空间感知、频率感知</li><li>论文链接：https://arxiv.org/abs/2401.17629</li><li><p>摘要：（1）研究背景：图像修复旨在从退化或损坏的图像中重建原始图像。经典方法是使用变分模型，其中包括数据保真度项和正则化项。扩散模型作为一种新兴的生成模型框架，在图像修复任务中展现出强大的能力，可以实现零样本学习。（2）过去方法及其问题：现有方法主要关注像素级的数据保真度，但忽略了感知信息。这导致修复后的图像可能在视觉上不令人满意。（3）研究方法：本文提出了一种空间和频率感知的扩散模型 SaFaRI，用于高斯噪声下的图像修复。该模型鼓励图像在空间域和频率域都保持数据保真度，从而提高重建质量。（4）方法性能：在 ImageNet 和 FFHQ 数据集上的广泛评估表明，SaFaRI 在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</p></li><li><p>方法：(1) SaFaRI模型框架：SaFaRI模型由编码器、扩散过程和解码器组成。编码器将退化图像映射到潜在空间，扩散过程通过添加噪声逐渐将潜在表示从退化状态转换到干净状态，解码器将干净的潜在表示重建为修复后的图像。(2) 空间感知数据保真度：SaFaRI模型在空间域中使用局部感知损失来鼓励修复后的图像与退化图像在局部区域内保持一致。局部感知损失通过计算退化图像和修复图像在局部区域内的差异来衡量数据保真度。(3) 频率感知数据保真度：SaFaRI模型在频率域中使用频谱损失来鼓励修复后的图像与退化图像在频率分布上保持一致。频谱损失通过计算退化图像和修复图像的频谱差异来衡量数据保真度。(4) 扩散过程：SaFaRI模型采用非对称扩散过程，即正向扩散过程和反向扩散过程。正向扩散过程将潜在表示从退化状态逐渐转换到干净状态，反向扩散过程将潜在表示从干净状态逐渐转换到退化状态。(5) 训练过程：SaFaRI模型通过最小化总损失函数来训练，总损失函数包括局部感知损失、频谱损失和正则化损失。正则化损失用于防止模型过拟合。</p></li><li><p>结论：</p></li></ol><p>（1）： 本文提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。通过利用双三次插值和傅里叶变换同时利用空间和频率信息，SaFaRI 在各种图像修复基准上取得了最先进的结果，优于现有方法。尽管我们提出的方法具有显着的性能，但由于先验项的影响，变换的应用不可避免地会对可行解产生扰动。对解扰动的全面分析可以加强我们方法论的理论基础。</p><p>（2）： 创新点：</p><ul><li>提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。</li><li>SaFaRI 利用双三次插值和傅里叶变换同时利用空间和频率信息，可以更好地保留图像的细节和纹理。</li><li>SaFaRI 采用非对称扩散过程，可以更好地控制图像修复过程，提高修复质量。</li></ul><p>性能：</p><ul><li>SaFaRI 在 ImageNet 和 FFHQ 数据集上的广泛评估表明，在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</li><li>SaFaRI 在修复高斯噪声图像时，可以有效地去除噪声，同时保留图像的细节和纹理。</li></ul><p>工作量：</p><ul><li>SaFaRI 模型的训练和推理过程相对简单，易于实现。</li><li>SaFaRI 模型的参数量较少，可以快速训练和推理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d12c8ba98ed6bf34752247f9b5d4ed94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-841cc516755a816daa1feb35b6020929.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6047f95584cb41e2634a1d794c58b933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e2b66f6263c96cab5ccac11907563d1.jpg" align="middle"></details>## You Only Need One Step: Fast Super-Resolution with Stable Diffusion via   Scale Distillation**Authors:Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos**In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step. [PDF](http://arxiv.org/abs/2401.17258v1) **Summary**扩散模型的单步超分辨率方法，YONO-SR，通过蒸馏训练，可实现图像分辨率的提升。**Key Takeaways**- YONOS-SR 在保持推理速度的同时，实现高质量的图像超分辨率。- 引入一种新的尺度蒸馏方法，从较小的尺度开始训练教师模型，然后采用迭代的方式将知识迁移到学生模型。- 蒸馏训练使学生模型能够获得更准确的目标，从而提高了超分辨率的性能。- 只需一步推理，YONOS-SR 就能够超越需要 200 步的最新方法。- YONOS-SR 结合了空间蒸馏的 U-Net 和微调的解码器，进一步提高了超分辨率效果。- 冻结 U-Net 并微调解码器，可以进一步提升超分辨率性能。- YONOS-SR 对于计算资源受限的设备非常适用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：一步到位：通过尺度蒸馏实现稳定扩散的快速超分辨率</li><li>作者：Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos</li><li>第一作者单位：三星AI剑桥</li><li>关键词：图像超分辨率、稳定扩散、尺度蒸馏、深度学习</li><li>论文链接：https://arxiv.org/abs/2401.17258</li><li>摘要：(1) 研究背景：扩散模型在各种图像生成任务中表现出色，包括图像超分辨率。然而，采样策略所需的连续去噪传递数量很大，即使对于在自动编码器潜在空间中运行的基于稳定扩散的模型（SD）也是如此。(2) 过去的方法及问题：最近，已经提出了几种减少采样步骤数量的方法。不幸的是，这些方法通常会影响性能，尤其是对于较少的步骤。基于扩散的模型通常在与训练期间看到的大小相似的图像块上产生最佳结果（例如，SD 的 64×64）。另一方面，超分辨率应用程序需要在高分辨率设置中运行，这大大加剧了基于扩散模型的计算问题。(3) 本文提出的研究方法：为了解决上述问题，本文提出了一种新颖的训练策略，称为尺度蒸馏。典型的基于扩散的超分辨率方法通过直接在目标尺度因子上的低分辨率图像上进行条件来训练超分辨率模型，而我们提出了一种渐进式训练方法，从较低尺度因子（即条件信号更接近目标）开始训练模型，并使用先前训练的模型作为教师逐步增加到目标尺度因子。(4) 方法在什么任务上取得了什么性能：在图像超分辨率任务上，该方法使用仅一步 DDIM 即可实现最先进的结果。通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结 U-Net 并微调其上的解码器。实验表明，空间蒸馏 U-Net 和微调解码器的组合仅需一步即可优于需要 200 步的最先进方法。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于尺度蒸馏的快速稳定扩散超分辨率方法，该方法可以通过一步DDIM实现最先进的结果。（2）：创新点：</li><li>提出了一种新颖的训练策略——尺度蒸馏，该策略可以将基于稳定扩散的超分辨率模型训练到仅需要一步即可生成高质量图像。</li><li>通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结U-Net并微调其上的解码器。性能：</li><li>在图像超分辨率任务上，该方法使用仅一步DDIM即可实现最先进的结果。</li><li>实验表明，空间蒸馏U-Net和微调解码器的组合仅需一步即可优于需要200步的最先进方法。工作量：</li><li>该方法的训练过程相对简单，只需要对教师模型的预测进行条件设置，然后冻结U-Net并微调其上的解码器即可。</li><li>该方法的推理过程也非常快速，只需要一步DDIM即可生成高质量图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-66d1c3043943daf87e1f11e232a38f98.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-506663e69d7322407f5094b321bf2044.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/</id>
    <published>2024-01-30T11:23:52.000Z</published>
    <updated>2024-01-30T12:01:08.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><a href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields" class="headerlink" title="Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields"></a>Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields</h2><p><strong>Authors:Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado</strong></p><p>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&amp;Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum. </p><p><a href="http://arxiv.org/abs/2401.16144v1">PDF</a> </p><p><strong>Summary</strong><br>利用教师-学生知识蒸馏范式，提升 NeRF 模型的渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>传统 NeRF 模型的训练范式对训练集中每个图像赋予同等重要性，这导致在渲染具有复杂几何结构的特定视图时表现不佳。</li><li>将输入视图根据其视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型专注于特定区域，从而提高渲染质量。</li><li>通过教师-学生知识蒸馏范式将这些专门模型的知识聚合到一个实体中，实现在线渲染的空间效率。</li><li>在 NeRF 合成和 Tanks&amp;Temples 两个公开数据集上对提出的训练框架进行评估，结果表明该框架优于最先进的基线模型，并且收敛到更好的最小值。</li><li>提出了一种名为 DaC 的分而治之训练框架。</li><li>DaC 将训练集划分为多个子集，并在每个子集上训练一个单独的神经辐射场 (NeRF) 模型。</li><li>然后将这些子模型通过知识蒸馏聚合成一个最终模型。</li><li>DaC 在 NeRF 合成和 Tanks&amp;Temples 数据集上的实验表明，它优于最先进的 NeRF 模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：分而治之：重新思考神经辐射场的训练范式</li><li>作者：Rongkai Ma、Leo Lebrat、Rodrigo SantaCruz、Gil Avraham、Yan Zuo、Clinton Fookes、Olivier Salvado</li><li>第一作者单位：英伟达</li><li>关键词：神经辐射场、分而治之、教师-学生蒸馏、空间效率</li><li>链接：https://arxiv.org/abs/2401.16144Github：无</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在合成 3D 场景的高保真视图方面表现出潜力，但 NeRF 的标准训练范式预设了训练集中每个图像具有同等重要性。这种假设对渲染呈现复杂几何体的特定视图提出了重大挑战，从而导致性能不佳。（2）过去的方法及其问题：以往的方法通常将所有场景视角的几何和光度信息统一压缩到神经网络权重中。这种方法往往忽略了复杂场景不同视角中存在的细节的自然不对称性，导致渲染质量下降。（3）本文提出的研究方法：本文重新审视了当前训练范式的含义，并重新设计了该范式，以提高 NeRF 的渲染质量。将输入视图根据它们的视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型能够专门针对特定区域，而不会牺牲速度或效率。随后，通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。（4）方法在任务和性能上的表现：在两个公开可用的数据集 NeRF 合成和 Tanks&amp;Temples 上对新颖的训练框架进行了评估。评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。</p></li><li><p>方法：（1）场景划分：将输入视图根据视觉相似性划分为多个组，每个组训练一个专门的模型，称为专家模型。（2）专家训练：在每个组上训练专家模型，使每个专家模型能够专门针对特定区域，而不会牺牲速度或效率。（3）教师-学生蒸馏：通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</p></li><li><p>结论：（1）：本文提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。（2）：创新点：</p></li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。性能：</li><li>在两个公开可用的数据集NeRF合成和Tanks&amp;Temples上对新颖的训练框架进行了评估。</li><li>评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。工作量：</li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-402a9ebdaec36fd0b9ae3b035907bf37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d76298373c29f69a44796c3bfafe8a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e414fcdc94276655b9d7b111a7932e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-86b43dc54cafd89cc41e3b7c64fefb1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca572fcd9b7c80bb78d37859a846f58c.jpg" align="middle"></details><h2 id="3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><a href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field" class="headerlink" title="3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field"></a>3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field</h2><p><strong>Authors:Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu</strong></p><p>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency. </p><p><a href="http://arxiv.org/abs/2401.14726v1">PDF</a> 20 pages, 8 figures</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 双模型杜-NeRF 实现高质几何重建与视图渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>杜-NeRF 由两个几何场组成，一个源于 SDF 场，一个源于密度场，用于同时实现高质量的几何重建和视图渲染。</li><li>杜-NeRF 将密度场分解为视图无关组件和视图相关组件，并使用视图无关组件作为 SDF 场学习过程的标签。</li><li>杜-NeRF 减少了形状 - 辐射场模糊性，并在学习过程中使几何形状和颜色相互受益。</li><li>杜-NeRF 在新颖视图合成和室内环境 3D 重建方面大大优于现有方法。</li><li>杜-NeRF 在构建不遵守多视图颜色一致性的精细几何图形区域时特别有效。</li><li>杜-NeRF 可用于增强现实 (AR)、虚拟现实 (VR) 和 3D 建模等应用。</li><li>杜-NeRF 开辟了 3D 重建和新视图合成研究的新方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于双神经辐射场的室内环境三维重建与新视角合成</li><li>作者：Yuxuan Zhang, Yufan Ren, Jiaolong Yang, Yinda Zhang, Xin Tong, Qionghai Dai</li><li>单位：西湖大学</li><li>关键词：三维重建、新视角合成、神经辐射场、深度学习</li><li>论文链接：https://arxiv.org/abs/2302.09426, Github 链接：暂无</li><li><p>摘要：（1）研究背景：三维重建和新视角合成在室内环境中有着广泛的应用，但技术上非常具有挑战性。基于隐式神经函数的最新方法可以实现出色的三维重建结果，但它们在新视角合成上的性能可能不尽如人意。神经辐射场 (NeRF) 的发展彻底改变了新视角合成，然而，基于 NeRF 的模型可能无法重建干净的几何表面。（2）过去的方法及其问题：本文提出了一种双神经辐射场 (Du-NeRF) 来同时实现高质量的几何重建和视图渲染。Du-NeRF 包含两个几何场，一个源自 SDF 场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF 的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督 SDF 场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。（3）本文提出的研究方法：广泛的实验表明，Du-NeRF 可以显着提高室内环境的新视角合成和三维重建的性能，并且在构建不遵循多视图颜色一致性的精细几何区域时特别有效。（4）方法在什么任务上取得了什么性能？性能是否支持其目标？在 Replica 数据集上，Du-NeRF 在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的 ThinGeometry 数据集上，Du-NeRF 在新视角合成方面也优于最先进的方法。这些结果支持了 Du-NeRF 的目标，即同时实现高质量的几何重建和新视角合成。</p></li><li><p>方法：(1) Du-NeRF模型框架：Du-NeRF由两个几何场组成，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。(2) 几何场的设计：SDF场用于表示物体的几何形状，密度场用于表示物体的颜色和外观。(3) 视图无关组件的分离：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。(4) 损失函数的设计：Du-NeRF使用了一个结合了重建损失、视图合成损失和正则化损失的损失函数来训练模型。(5) 训练过程：Du-NeRF使用梯度下降法来训练模型，训练过程中交替更新SDF场和密度场。</p></li><li><p>结论：(1): 本文提出了一种双神经辐射场(Du-NeRF)来同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。(2): 创新点：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程，减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。性能：在Replica数据集上，Du-NeRF在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的ThinGeometry数据集上，Du-NeRF在新视角合成方面也优于最先进的方法。工作量：本文的工作量较大，需要设计和训练两个几何场，还需要设计损失函数和训练过程。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-56683e282b9ba64280391f34e5aa9f31.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6383efbe47ff44676e2c2f51579aaa23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d811bf1bd890a7ed9dd96e40a81482c2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-98f97a5db5fd854c0d80066a92053a27.jpg" align="middle"></details><h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p><p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. </p><p><a href="http://arxiv.org/abs/2401.14257v2">PDF</a> 11 pages, 9 figures</p><p><strong>Summary</strong><br>文本引导 3D 生成框架 Sketch2NeRF 可利用草图控制生成一致且高保真的 3D 内容。</p><p><strong>Key Takeaways</strong></p><ul><li>Sketch2NeRF 是一个多视角草图引导的文本到 3D 生成框架，可以将草图控制添加到 3D 生成中。</li><li>Sketch2NeRF 利用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。</li><li>Sketch2NeRF 提出了一种新颖的同步生成和重建方法来有效优化 NeRF。</li><li>Sketch2NeRF 收集了两种多视角草图数据集来评估所提出的方法。</li><li>实验表明，Sketch2NeRF 可以合成具有细粒度草图控制并且对文本提示高度保真的 3D 一致内容。</li><li>广泛的结果表明，Sketch2NeRF 在草图相似性和文本对齐方面实现了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Sketch2NeRF：多视角草图引导的文本到 3D 生成</li><li>作者：Minglin Chen、Weihao Yuan、Yukun Wang、Zhe Sheng、Yisheng He、Zilong Dong、Liefeng Bo、Yulan Guo</li><li>隶属单位：中山大学深圳校区</li><li>关键词：文本到 3D、NeRF、草图控制、多视角一致性</li><li>论文链接：https://arxiv.org/abs/2401.14257Github 链接：无</li><li>摘要：(1)：文本到 3D 生成方法可以通过文本描述生成高保真 3D 内容。然而，生成的物体是随机的，缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。然而，由于草图的抽象性和模糊性，很难从这些草图中实现灵活的控制。(2)：过去的方法主要使用预训练的 2D 扩散模型来监督 3D 场景的优化，这些场景由神经辐射场 (NeRF) 表示。然而，这些方法通常需要大量的草图作为输入，并且生成的 3D 对象可能与草图不一致。(3)：本文提出了一种多视角草图引导的文本到 3D 生成框架（即 Sketch2NeRF），以将草图控制添加到 3D 生成中。具体来说，该方法利用预训练的 2D 扩散模型来监督 3D 场景的优化，该场景由神经辐射场 (NeRF) 表示。并提出了一种新颖的同步生成和重建方法来有效地优化 NeRF。(4)：在实验中，本文收集了两种多视角草图数据集来评估所提出的方法。结果表明，该方法能够合成具有细粒度草图控制的 3D 一致内容，同时对文本提示保持高保真度。广泛的结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。</li></ol><p><strong>Methods：</strong></p><ol><li><p><strong>3D表示：</strong>使用神经辐射场（NeRF）表示3D对象，NeRF是一种灵活且能够渲染逼真图像的方法。</p></li><li><p><strong>草图条件生成：</strong>使用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重。</p></li><li><p><strong>同步生成和重建优化：</strong>提出了一种同步生成和重建优化方法，该方法利用ControlNet和Stable Diffusion分别在草图的特定姿势和随机采样的姿势下生成真实图像，并使用NeRF渲染的图像作为重建目标，最小化生成图像和渲染图像之间的重建损失。</p></li><li><p><strong>优化：</strong>使用基于分数的蒸馏优化方法来优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。</p></li><li><p>结论：（1）：本文提出了一种新颖的多视角草图引导的文本到3D生成方法Sketch2NeRF，该方法可以生成与给定草图相似的逼真3D内容。具体来说，该方法利用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重，并提出了一种新的同步生成和重建优化方法来有效地优化NeRF。实验结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。（2）：创新点：</p></li><li>提出了一种新颖的多视角草图引导的文本到3D生成方法，该方法可以生成与给定草图相似的逼真3D内容。</li><li>提出了一种新的同步生成和重建优化方法来有效地优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。性能：</li><li>在两个多视角草图数据集上进行的实验表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。工作量：</li><li>该方法需要收集多视角草图数据集，并需要预训练2D草图条件扩散模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-432d996d35cef510a47b970f6a57f9ed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b5a42bece9e656aff52a6fc20878da8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb3c2e84dae023cd921d28d348487b30.jpg" align="middle"></details><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 带注意力机制的分解 (NeRF-AD) 提出了一种新颖的说话人脸合成方法，通过音频注意机制将人脸分解为音频面孔和身份面孔，从而提高人脸合成的真实性和唇部同步效果。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-AD 提出了一种新的说话人脸合成方法，结合了神经辐射场 (NeRF) 和注意力机制，通过将人脸分解为音频面孔和身份面孔，大幅提升了生成人脸的真实性和唇部同步效果。</li></ul><ul><li>NeRF-AD 使用基于注意力的分解模块，利用语音相关的面部动作单元 (AU) 信息将人脸分解为音频面孔和身份面孔，有效地将音频与面部语音运动相关区域进行精确映射。</li></ul><ul><li>NeRF-AD 只将音频面孔与音频特征融合，从而精确地控制音频如何影响说话人脸。</li></ul><ul><li>NeRF-AD 利用 AU 信息来监督这两种模态的融合，提高了人脸合成的准确性和真实性。</li></ul><ul><li>大量的定量和定性实验表明，NeRF-AD 在生成逼真说话人脸视频方面优于现有最先进的方法，包括图像质量和唇部同步。</li></ul><ul><li>更详细的视频结果可以访问 <a href="https://xiaoxingliu02.github.io/NeRF-AD。">https://xiaoxingliu02.github.io/NeRF-AD。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：神经辐射场与基于注意力的分离的说话人面部合成（NERF-AD）</li><li>作者：Bi Chongke，Liu Xiaoxing，Liu Zhilei</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部分离</li><li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：None</li><li><p>摘要：（1）研究背景：说话人面部合成在多维信号处理和多媒体领域是一个热门的研究课题。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 带来了复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。（2）过去的方法及其问题：一些现有的方法将 NeRF 的学习任务提前了一部分，并提出了一种通过具有基于注意力的分离的 NeRF（NeRF-AD）进行说话人面部合成的。具体来说，引入了一个基于注意力的分离模块，使用与语音相关的面部动作单元 (AU) 信息将面部分离为音频面部和身份面部。为了精确地调节音频如何影响说话人面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。（3）研究方法：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们分解说话人面部并为 NeRF 提供两个分解的精确条件。我们提出了一个基于注意力的分离模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。（4）方法性能：广泛的定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li><li><p>方法：(1)：提出了一种基于注意力的分离模块NeRF-AD，将面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。(2)：为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。(3)：采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。</p></li><li><p>结论：（1）：提出了一种基于注意力的分离模块NeRF-AD，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。（2）：创新点：提出了一种基于注意力的分离模块，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。性能：实验结果表明，NeRF-AD在图像质量和唇形同步方面优于最先进的方法。工作量：工作量较大，需要大量的数据和计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details><h2 id="HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><a href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs" class="headerlink" title="HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs"></a>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs</h2><p><strong>Authors:Zelin Gao, Weichen Dai, Yu Zhang</strong></p><p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs. </p><p><a href="http://arxiv.org/abs/2401.11711v1">PDF</a> 13 pages, 6 figures</p><p><strong>摘要</strong><br>层次几何、语义和光度引导 NeRF（HG3-NeRF）方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</p><p><strong>关键要点</strong></p><ul><li>HG3-NeRF 是一种新的方法，可以解决稀疏视图输入下 NeRF 的性能退化问题，并提高几何、语义内容和外观的一致性。</li><li>HG3-NeRF 提出了一种分层几何引导 (HGG) 方法，将运动结构 (SfM) 的附件（即稀疏深度先验）纳入场景表示中。</li><li>HGG 从局部到全局的几何区域对体积点进行采样，减轻了深度先验中固有偏差造成的错位。</li><li>HG3-NeRF 提出了一种分层语义引导 (HSG) 方法，学习从粗到细的语义内容，这对应于从粗到细的场景表示。</li><li>实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并实现了稀疏视图输入的高保真合成结果。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HG3-NeRF：用于稀疏视图输入的分层几何、语义和光度引导的神经辐射场</li><li>作者：Zelin Gao, Weichen Dai, Yu Zhang</li><li>隶属机构：浙江大学控制科学与工程学院</li><li>关键词：神经辐射场、稀疏视图、几何引导、语义引导、光度引导</li><li>论文链接：https://arxiv.org/abs/2401.11711，Github 链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）因其从离散观测中学习场景表示以进行新颖视图合成而备受关注。然而，当面对稀疏视图输入时，NeRF 的性能会显著下降，从而限制了其进一步的适用性。（2）过去方法及问题：现有方法采用预训练方法和逐场景优化方法来解决稀疏视图输入的挑战。预训练方法在大型数据集上训练模型，然后在测试时对每个场景进行微调。然而，这种方法的泛化能力很大程度上依赖于数据集的质量，而且通过捕捉许多不同场景来获得必要的数据集过于昂贵。逐场景优化方法在每个场景上优化模型，但它们通常需要大量计算，并且可能难以收敛到良好的解。（3）研究方法：本文提出了一种新颖的方法 HG3-NeRF，可以解决上述限制并增强不同视图之间几何形状、语义内容和外观的一致性。HG3-NeRF 包括三个主要组件：分层几何引导（HGG）、分层语义引导（HSG）和光度引导。HGG 将结构从运动（SfM）的附加信息（即稀疏深度先验）纳入场景表示中。HSG 从不同分辨率的图像中学习粗到细的语义内容，这与粗到细的场景表示相对应。光度引导使用渲染方程来优化场景表示，以匹配输入视图的颜色和亮度。（4）方法性能：实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。这些结果支持了本文提出的方法的目标，即解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。</li></ol><p>Methods：（1）分层几何引导（HGG）：利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。HGG 方法指导神经辐射场学习密度和颜色的近似分布，这些分布来自深度先验确定的局部到全局的采样区域。（2）分层语义引导（HSG）：从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。HSG 使用 CLIP 编码器对渲染的图像和原始图像的特征向量进行编码，并计算粗到细的语义余弦相似性。（3）光度引导：使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。光度引导通过最小化渲染的图像和原始图像之间的外观均方误差来实现。</p><ol><li>结论：（1）：本文提出了一种分层几何、语义和光度引导的神经辐射场（HG3-NeRF）方法，可以解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。（2）：创新点：</li><li>提出了一种分层几何引导（HGG）方法，利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。</li><li>提出了一种分层语义引导（HSG）方法，从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。</li><li>使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。性能：</li><li>在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。工作量：</li><li>需要估计相机位姿，并且稀疏视图输入会影响位姿估计的准确性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-56cd69227addb7c7e2e5ec9028bc8cb0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bb7c383a42f7306611645083f4d82eb9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-71514b137fee0e499428b6e4c393be26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5dccc88a28d6fafb1f550b78be5145.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bab43cfc9ed715f6025ba1321b7acdc3.jpg" align="middle"></details><h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p><p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts. </p><p><a href="http://arxiv.org/abs/2401.09495v4">PDF</a> Error on result tabulation of state of the art method which might   cause misleading to readers</p><p><strong>Summary</strong><br>神经辐射场（NeRF）模型在计算机视觉领域备受关注，并产生了令人印象深刻的成果，由于其最先进的视觉质量，因此存在被剽窃者非法复制、再分发或滥用的风险。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种针对 NeRF 模型的黑盒和白盒设置的综合知识产权（IP）保护框架，称为 IPR-NeRF。</li><li>在黑盒设置中，引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。</li><li>在白盒设置中，通过采用符号损失目标将指定数字签名嵌入 NeRF 模型的权重中。</li><li>大量实验表明，我们的方法不仅保持了 IPR-NeRF 模型的保真度（即渲染质量），而且与现有技术相比，它还对歧义攻击和去除攻击具有鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：IPR-NERF：知识产权验证满足神经辐射场</li><li>作者：Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</li><li>单位：马来亚大学图像与信号处理中心（CISiP）</li><li>关键词：神经辐射场、知识产权保护、数字水印、数字签名</li><li>论文链接：arXiv:2401.09495v1[cs.CV]17Jan2024Github 链接：无</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）模型因其卓越的视觉质量和令人印象深刻的演示而在计算机视觉领域备受关注。然而，NeRF 模型也面临着知识产权保护的问题，剽窃者可能会非法复制、重新分发或滥用这些模型以获取经济利益或个人利益。（2）过去方法及其问题：目前针对神经网络的知识产权保护方案主要针对卷积神经网络（CNN）、生成对抗网络（GAN）和循环神经网络（RNN）。然而，这些方案在应用于 NeRF 模型时面临诸多挑战，例如 NeRF 模型的复杂结构、对数据和计算资源的要求较高以及缺乏有效的知识产权保护技术。（3）研究方法：本论文提出了一个综合的 NeRF 模型知识产权保护框架，称为 IPR-NERF。该框架包括黑盒和白盒两种设置。在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。（4）方法性能：实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。</p></li><li><p>方法：（1）：提出一个综合的NeRF模型知识产权保护框架IPR-NERF，包括黑盒和白盒两种设置。（2）：在黑盒设置中，引入一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。（3）：在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入NeRF模型的权重中。</p></li><li><p>结论：（1）：本工作提出了一种全面的、鲁棒的 NeRF-IPR 保护方案，包括黑盒和白盒两种场景。全面的实验结果表明了其在抵抗嵌入水印的模糊性和去除攻击方面的有效性，同时保持了渲染性能。然而，它在计算能力和对覆盖攻击的黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时。未来的研究将集中在改进这些方面。本研究为 NeRF 模型开发者和研究人员提供了极大的价值，提供了一种保护其知识产权并获得市场竞争优势的方法，考虑到开发高性能 NeRF 模型所需的巨大资源。加强 NeRF 模型对 IPR 侵权行为的抵抗具有广泛的社会效益，包括防止剽窃、确保在动态市场竞争中的竞争优势以及减少浪费诉讼案件的负担。（2）：创新点：</p></li><li>提出了一种综合的 NeRF 模型知识产权保护框架 IPR-NERF，包括黑盒和白盒两种设置。</li><li>在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。</li><li>在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。性能：</li><li>实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。工作量：</li><li>IPR-NERF 模型的计算成本较高，尤其是对于大型数据集和复杂场景。</li><li>在黑盒设置中，嵌入和提取水印的过程可能需要大量的时间和计算资源。</li><li>在白盒设置中，需要修改 NeRF 模型的训练过程以嵌入数字签名，这可能会增加训练时间和复杂性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle"></details><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p><p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p><p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p><p><strong>摘要</strong><br>针对稀疏无约束视点场景下神经辐射场（NeRF）模型的局限性，本文旨在重构和理解三维场景中每个点的来源信息，并提出了 ProvNeRF 模型来实现这一目标。</p><p><strong>要点</strong></p><ul><li>ProvNeRF 模型能够通过引入每个点可能的来源位置，来丰富传统的 NeRF 模型。</li><li>ProvNeRF 模型与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。</li><li>ProvNeRF 模型可以对每个点的不确定性进行估计。</li><li>ProvNeRF 模型可以根据指定的标准，选择合适的视角来进行场景重建。</li><li>ProvNeRF 模型可以改进场景的新视角合成结果。</li><li>ProvNeRF 模型的更多信息可以在项目主页 <a href="https://provnerf.github.io">https://provnerf.github.io</a> 查看。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ProvNeRF：将 NeRF 中的逐点出处建模为随机过程</li><p></p><p></p><li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li><p></p><p></p><li>隶属机构：斯坦福大学</li><p></p><p></p><li>关键词：神经辐射场、稀疏视图、出处建模、不确定性估计、基于标准的视点优化、新颖视图合成</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2401.08140Github 链接：无</li><p></p><p></p><li>摘要：（1）研究背景：神经辐射场 (NeRF) 在各种应用中越来越受欢迎，但它们在稀疏视图方案中面临挑战，因为仅靠体积渲染无法提供足够的约束。（2）过去方法：过去的方法主要集中在增强重建和新颖视图合成上，但忽略了如何从更全面的角度理解场景，例如不确定性估计、基于标准的视点选择和改进的新颖视图合成。（3）研究方法：我们提出 ProvNeRF，这是一种通过结合逐点出处来丰富传统 NeRF 表示的模型，对每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。（4）方法性能：我们的方法在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法，这表明建模逐点出处可以提供几个优势。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong>：**</p><p>（1）：我们将神经辐射场 (NeRF) 表示扩展为包含每个点的出处，即每个点的来源或从何处看到它。</p><p>（2）：我们使用随机过程对每个点的出处进行建模，该随机过程由坐标 x∈R3 索引，其在 x 处的边际分布编码了 x 处的出处。</p><p>（3）：我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点，该估计将潜在随机变量的变换学习为数据分布，其中每个数据样本都是一个标量或向量。</p><p>（4）：我们提出 ProvNeRF，它通过扩展隐式概率模型（特别是 IMLE）来处理随机过程，从而将每个点的出处建模为随机过程。</p><p>（5）：ProvNeRF 学习一个确定性变换 Hθ：Rb→R+×D3，该变换将每个潜在随机函数样本 Z∼Z 映射到一个函数 Dθ∼Dθ。</p><p>（6）：为了优化 Dθ，我们扩展 IMLE 来对随机过程的分布进行建模。我们将 Eq.3 调整到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><ol><li>结论：（1）：本工作提出 ProvNeRF，通过扩展随机过程的 IMLE 来增强传统 NeRF 表示，从而将每个点的出处建模为随机过程。ProvNeRF 可轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。我们展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。（2）：创新点：</li><li>提出 ProvNeRF，一种通过扩展随机过程的 IMLE 来增强传统 NeRF 表示的模型，从而将每个点的出处建模为随机过程。</li><li>证明了 ProvNeRF 可以轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。</li><li>展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。</li></ol><p>性能：- 在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法。</p><p>工作量：- 需要扩展随机过程的 IMLE 来对随机过程的分布进行建模。- 需要调整 Eq.3 到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-30  Divide and Conquer Rethinking the Training Paradigm of Neural Radiance   Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/</id>
    <published>2024-01-30T11:06:29.000Z</published>
    <updated>2024-01-30T12:00:56.153Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p><p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p><p><strong>摘要</strong><br>高斯散点结合神经辐射场，实现动态内窥镜 3D 重建新方法。</p><p><strong>要点</strong></p><ul><li>EndoGaussians 是一个新的方法，它利用高斯散点进行动态内窥镜 3D 重建。</li><li>这种方法是首次在该背景下使用高斯散点，克服了以前基于 NeRF 技术的限制。</li><li>该方法在各种内窥镜数据集上进行定量评估，树立了新的最先进标准。</li><li>这些进步使该方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供了更可靠、更高效的 3D 重建。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视动态高斯体素重建</li><li>作者：Yangsen Chen, Hao Wang</li><li>隶属机构：香港科技大学（广州）</li><li>关键词：3D 重建、高斯体素重建、机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352</li><li><p>摘要：(1) 研究背景：准确地从内窥镜视频中重建可变形软体组织的 3D 模型对于 VR 手术和医学图像分析等医疗应用至关重要。现有方法通常在准确性和产生的组织部分的模棱两可方面存在问题，限制了其实际效用。(2) 过往方法：以往的一些工作尝试使用深度估计、SLAM、稀疏变形场和神经辐射场等方法来解决这个问题，但这些方法要么假设场景是静态的，要么假设手术工具不存在，从而限制了它们在实际场景中的实用性。(3) 研究方法：为了进一步提高静态单视 RGBD 设置下软体组织的 3D 重建的准确性，并提高 3D 重建的可靠性和可信度，我们提出了 Endogaussians，该方法利用高斯体素重建作为重建方法。(4) 方法性能：我们的方法在 PSNR、SSIM、LPIPS 等多项定量评估中取得了最先进的结果，并且重建速度更快。这些进步使我们的方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供更可靠和高效的 3D 重建。</p></li><li><p>方法：(1): 本文提出了一种名为 Endogaussians 的方法，用于从单目动态 RGBD 设置中重建可变形软体组织的 3D 模型。(2): 该方法使用高斯体素重建作为重建方法，可以有效地处理软体组织的变形。(3): 为了提高重建的准确性，本文提出了一种新的体素融合策略，该策略可以有效地融合来自不同帧的数据。(4): 此外，本文还提出了一种新的体素分割算法，该算法可以有效地将软体组织分割成不同的部分。(5): 最后，本文提出了一种新的体素渲染算法，该算法可以生成逼真的软体组织模型。</p></li><li><p>结论：（1）：EndoGaussians方法可以有效地从单目动态RGBD设置中重建可变形软体组织的3D模型，具有较高的准确性和可靠性，在医疗领域具有广阔的应用前景。（2）：创新点：</p></li><li>提出了一种新的高斯体素重建方法，可以有效地处理软体组织的变形。</li><li>提出了一种新的体素融合策略，可以有效地融合来自不同帧的数据。</li><li>提出了一种新的体素分割算法，可以有效地将软体组织分割成不同的部分。</li><li>提出了一种新的体素渲染算法，可以生成逼真的软体组织模型。性能：</li><li>在PSNR、SSIM、LPIPS等多项定量评估中取得了最先进的结果。</li><li>重建速度更快。工作量：</li><li>该方法需要大量的计算资源。</li><li>该方法需要大量的标注数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details><h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p><p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>. </p><p><a href="http://arxiv.org/abs/2401.12561v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯渲染框架实现了实时内窥镜手术场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为EndoGaussian的实时手术场景重建框架，它是建立在3D高斯点阵的基础上的。</li><li>使用高斯表示和并行渲染管道，显著提高了渲染速度。</li><li>将变形场设计为轻量级编码体素和极小MLP的组合，实现了高效的高斯跟踪，渲染负担较小。</li><li>设计了一种整体的高斯初始化方法，充分利用了表面分布先验，通过搜索输入图像序列中的信息点来实现。</li><li>公共内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS实时，100倍收益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（在2分钟/场景以内），显示出对术中手术应用的重大前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussian：用于可变形手术场景重建的高斯点云</li><li>作者：Yifan Liu<em>, Chenxin Li</em>, Chen Yang 和 Yixuan Yuan</li><li>隶属机构：香港中文大学</li><li>关键词：三维重建 · 高斯点云 · 机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.12561   Github 代码链接：https://yifliu3.github.io/EndoGaussian/</li><li><p>摘要：（1）研究背景：从内窥镜立体视频中重建可变形组织对于许多下游手术应用至关重要。然而，现有方法的推理速度慢，极大地限制了它们的实际使用。（2）过去方法及其问题：现有方法的问题在于推理速度慢，这使得它们在实际应用中受到限制。（3）研究方法：该论文提出了一种基于 3D 高斯点云的实时手术场景重建框架 EndoGaussian。该框架将动态手术场景表示为规范高斯点云和时间相关的变形场，该变形场可以预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，该框架与以往方法相比，显著地提高了渲染速度。此外，该论文将变形场设计为轻量级编码体素和极小型的 MLP 的组合，从而实现高效的高斯跟踪，且渲染负担很小。此外，该论文设计了一种整体的高斯初始化方法，以充分利用表面分布先验，该方法通过从输入图像序列中搜索信息点来实现。（4）方法性能：在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（每个场景 2 分钟以内），显示出对术中手术应用的重大前景。</p></li><li><p>方法：（1）EndoGaussian框架概述：该框架由高斯点云初始化、高斯跟踪和高斯渲染三个模块组成。（2）高斯点云初始化：从输入图像序列中搜索信息点，通过高斯混合模型估计点云参数，并通过表面分布先验优化点云位置。（3）高斯跟踪：将变形场设计为轻量级编码体素和极小型的MLP的组合，通过将当前时间戳的高斯点云变形到新时间戳，实现高效的高斯跟踪。（4）高斯渲染：利用高斯点云的几何特性和并行渲染管道，实现高效的渲染。（5）训练细节：使用Adam优化器，学习率为1e-4，批大小为8，训练200个周期。</p></li><li><p>结论：（1）：本工作提出了一种实时且高质量的 4D 重建框架，用于动态手术场景重建。通过利用基于体素的高斯跟踪和整体高斯初始化，我们能够处理组织变形和非平凡的高斯初始化问题。全面的实验表明，我们的 EndoGaussian 可以实现最先进的重建质量和实时的渲染速度，比以前的方法快 100 倍以上。我们希望新兴的基于高斯斑点的重建技术能够为机器人手术场景理解提供新的途径，并增强各种下游临床任务，尤其是术中应用。（2）：创新点：</p></li><li>基于高斯点云的实时手术场景重建框架。</li><li>体素编码的高斯跟踪，实现了高效的高斯跟踪。</li><li>整体高斯初始化方法，充分利用表面分布先验。性能：</li><li>在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925PSNR）和最快的训练速度（每个场景 2 分钟以内）。工作量：</li><li>论文的代码和数据已经开源，可以方便地进行复现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle"></details><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong><br>动态高斯溅射用于可变形内窥镜组织重建。</p><p><strong>要点</strong></p><ul><li>EndoGS 利用高斯溅射进行可变形内窥镜组织重建。</li><li>该方法结合变形场以处理动态场景。</li><li>深度引导监督用于优化具有单个视点的 3D 目标。</li><li>时空权重掩码可减轻工具遮挡。</li><li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。</li><li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 实现卓越的渲染质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯斑点可变形内窥镜组织重建</li><li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li><li>第一作者单位：香港大学</li><li>关键词：高斯斑点·机器人手术·三维重建</li><li>论文链接：https://arxiv.org/abs/2401.11535, Github 链接：https://github.com/HKU-MedAI/EndoGS</li><li>摘要：(1)：研究背景：手术三维重建是机器人手术研究的一个关键领域，最近的工作采用动态辐射场实现从单视角视频中可变形组织的三维重建。然而，这些方法通常存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。(2)：过去的方法：早期尝试采用深度估计在内窥镜重建中取得了巨大成功，但这些方法仍然难以产生逼真的三维重建，原因有两个关键问题。首先，非刚性变形有时会导致较大的运动，这需要实际动态场景重建，这阻碍了这些技术的适应。其次，单视角视频中存在遮挡，导致学习受影响部分时信息有限，产生困难。虽然一些框架结合了工具遮罩、立体深度估计和稀疏翘曲场用于单视角三维重建，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。(3)：研究方法：受最近流行的三维表示方法三维高斯斑点启发，我们提出了 EndoGS，将高斯斑点应用于可变形内窥镜组织重建。具体来说，我们的方法结合了变形场来处理动态场景，深度引导监督来优化具有单一视点的三维目标，以及时空权重掩码来减轻遮挡。(4)：方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了更高的渲染质量。</li></ol><p>Methods:(1): 我们提出了一种称为 EndoGS 的方法，它利用 3D-GS 的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建 3D 外科场景。(2): 我们首先介绍了 3D-GS 的预备知识，然后展示了使用动态版本的 3D-GS 对可变形组织进行建模，该版本采用轻量级 MLP 来表示动态场。最后，我们介绍了使用工具掩码和深度图对高斯飞溅进行训练优化的过程。(3): 我们使用六个正交特征平面来编码空间和时间信息，并使用单个 MLP 来更新高斯属性并解码位置、比例因子、旋转因子、球谐系数和不透明度的变形。(4): 我们结合工具掩码和深度图来训练 EndoGS，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>具体内容如下：</p><ol><li>结论：（1）：本文提出了一种基于高斯斑点可变形内窥镜组织重建的方法，该方法可以从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。（2）：创新点：</li><li>提出了一种新的方法EndoGS，利用3D-GS的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建3D外科场景。</li><li>使用动态版本的3D-GS对可变形组织进行建模，该版本采用轻量级MLP来表示动态场。</li><li>结合工具掩码和深度图对高斯飞溅进行训练优化，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。性能：</li><li>在达芬奇机器人手术视频上的实验表明，EndoGS实现了更高的渲染质量。工作量：</li><li>该方法需要预先训练3D-GS模型，并对每个新场景进行优化。</li><li>优化过程需要一定的时间，具体取决于场景的复杂性和数据量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details><h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen</strong></p><p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p><p><a href="http://arxiv.org/abs/2401.09720v2">PDF</a> </p><p><strong>Summary</strong><br>优化动态穿衣人体重建方法，引入物理先验和规范化变换，实现高精度照片级新视角渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种新的穿衣人體重建方法 GaussianBody，基於 3D 高斯 Splatting。</li><li>3D 高斯 Splatting 最近在訓練時間和渲染質量方面表現出了很好的性能。</li><li>應用靜態 3D 高斯 Splatting 模型於動態人體重建問題時，會因複雜的非剛性變形和豐富的衣物細節而遇到挑戰。</li><li>提出明確的姿勢引導變形，以關聯規範空間和觀測空間中的動態高斯。</li><li>引入基於物理的先驗和正則化變換，以減少兩個空間之間的歧義。</li><li>提出姿勢精煉策略，以更新姿勢回歸，以補償不準確的初始估計。</li><li>提出分拆比例機制，以增強回歸點雲的密度。</li><li>實驗證明，該方法可實現最先進照片級的新視圖渲染結果，同時具有高質量的動態穿衣人體細節和明確的幾何重建。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯体：基于 3D 高斯散布的着装人体重建</li><li>作者：李梦添、姚胜祥、谢志峰、陈可宇</li><li>隶属单位：上海大学</li><li>关键词：3D 高斯散布、着装人体重建、单目视频、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li><li>摘要：（1）研究背景：创建高保真着装人体模型在虚拟现实、远程临场和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工操作，这使得它们既耗时又昂贵，从而限制了新手用户的可扩展性。近年来，人们越来越关注从单个 RGB 图像或单目视频中自动重建着装人体模型。（2）过去的方法及其问题：网格模型方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来重构人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移量成为这种情况下的一种增强解决方案。然而，其表示能力仍然受到网格分辨率的严格限制，并且通常在宽松服装的情况下会失败。（3）本文提出的研究方法：为了克服显式网格模型的局限性，本文提出了一种基于 3D 高斯散布的新颖着装人体重建方法 GaussianBody。与代价高昂的神经辐射场模型相比，3D 高斯散布最近在训练时间和渲染质量方面表现出优异的性能。然而，将静态 3D 高斯散布模型应用于动态人体重建问题由于复杂的非刚性变形和丰富的服装细节而变得非常困难。为了应对这些挑战，本文的方法考虑了显式姿势引导的变形，将动态高斯体与规范空间和观察空间相关联，引入具有正则化变换的基于物理的先验有助于减轻这两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略，以更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。（4）方法的应用任务和性能：实验验证了本文的方法可以实现最先进的逼真新视图渲染结果，为动态着装人体提供高质量的细节，以及显式几何重建。这些性能可以支持他们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于 3D 高斯散布的着装人体重建方法 GaussianBody，该方法能够从单目视频中重建动态的着装人体模型，并具有逼真的新视图渲染结果和高质量的细节。（2）：创新点：</li><li>将 3D 高斯散布表示扩展到着装人体重建中，并考虑了显式姿势引导的变形，以解决动态高斯体与规范空间和观察空间之间的歧义问题。</li><li>提出了一种基于物理的先验来正则化规范空间的高斯体，以减轻观察空间和规范空间之间的过度旋转问题。</li><li>提出了一种姿势细化策略和分裂尺度机制，以增强重建点云的质量和鲁棒性。性能：</li><li>该方法在图像质量指标上与基线和其他方法相当，证明了其具有竞争力的性能和相对较快的训练速度。</li><li>该方法能够使用更高分辨率的图像进行训练。工作量：</li><li>该方法的训练时间比一些最先进的方法更长。</li><li>该方法需要大量的数据来进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-03cb35c9ffdf24e162bbcf10081d440a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4877b53e7d23cf29d6e9a1a57a3155ec.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details><h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p><p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p><p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p><p><strong>摘要</strong><br>智能汽车专属视觉基础模型的构建挑战及其未来发展机遇。</p><p><strong>要点</strong></p><ul><li>数据准备、预训练策略和下游任务适配是 VFM 开发的关键技术。</li><li>生成神经辐射场 (NeRF)，扩散模型，3D 高斯分布（3DGS）和世界模型等技术的进步为未来的研究提出了路线图。</li><li>开源项目 <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a> 将不断更新，以赋能研究人员。</li><li>自动驾驶中的 VFM 缺乏专用数据和多传感器集成，导致任务特定架构的多样性成为 VFM 发展的障碍。</li><li>视觉基础模型 (VFM) 在自动驾驶中至关重要，但其发展面临着诸多挑战。</li><li>开发专用于自动驾驶的 VFM 是当前的紧迫挑战。</li><li>建议从数据准备、预训练以及下游任务适配等方面入手，并探索 NeRF、扩散模型等新技术，以推进 VFM 的发展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：构建自动驾驶视觉基础模型：挑战、方法和机遇</li><li>作者：徐岩，张海明，蔡应杰，郭敬明，邱维超，高斌，周凯强，赵越，金欢，高建涛，李振，蒋立辉，张伟，张宏波，戴登心，刘冰冰</li><li>第一作者单位：华为诺亚方舟实验室</li><li>关键词：视觉基础模型，数据生成，自监督训练，自动驾驶，文献综述</li><li>论文链接：https://arxiv.org/abs/2401.08045   Github 代码链接：无</li><li><p>摘要：（1）研究背景：随着自动驾驶技术的快速发展，传统自动驾驶感知系统采用模块化架构，针对特定任务使用专用算法，但这种方法往往导致输出不一致，限制了系统处理长尾情况的能力。近年来，大型基础模型在自然语言处理领域取得了巨大成功，展现出强大的适应性和有效性，为构建自动驾驶视觉基础模型提供了新的思路。（2）过去的方法及其问题：以往的方法主要集中于针对特定任务训练深度神经网络，但这种方法存在以下问题：1. 忽视了数据之间的关系，导致输出不一致；2. 难以处理长尾情况；3. 无法有效利用大量未标记数据。（3）提出的研究方法：本文提出了一种构建自动驾驶视觉基础模型的方法，该方法主要包括以下几个步骤：1. 数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；2. 预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；3. 下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。（4）方法在任务中的表现及性能：该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。这些结果表明，该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。</p></li><li><p>方法：（1）数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；（2）预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；（3）下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。</p></li><li><p>结论：（1）：本文针对自动驾驶视觉基础模型的构建提出了系统的方法，并取得了较好的性能。该方法为自动驾驶视觉基础模型的构建提供了新的思路，有望推动自动驾驶技术的发展。（2）：创新点：</p></li><li>提出了一种构建自动驾驶视觉基础模型的方法，该方法包括数据准备、预训练和下游任务自适应三个步骤。</li><li>使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征。</li><li>将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。性能：</li><li>该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。</li><li>该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。工作量：</li><li>该方法需要收集和预处理大量的数据。</li><li>该方法需要使用自监督学习方法对基础模型进行预训练，这需要较大的计算资源。</li><li>该方法需要将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务，这需要较多的工程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-30  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
