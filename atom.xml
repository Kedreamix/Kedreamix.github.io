<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-02-29T13:26:36.999Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/NeRF/</id>
    <published>2024-02-29T13:26:36.000Z</published>
    <updated>2024-02-29T13:26:36.999Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-29-æ›´æ–°"><a href="#2024-02-29-æ›´æ–°" class="headerlink" title="2024-02-29 æ›´æ–°"></a>2024-02-29 æ›´æ–°</h1><h2 id="Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis"><a href="#Learning-Dynamic-Tetrahedra-for-High-Quality-Talking-Head-Synthesis" class="headerlink" title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis"></a>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</h2><p><strong>Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang</strong></p><p>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. </p><p><a href="http://arxiv.org/abs/2402.17364v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æœ€æ–°æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œå³åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜ç¡®åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®ä¿å„ç§åŠ¨ä½œå’Œè§†ç‚¹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DynTet æ˜¯ä¸€ç§æ–°çš„æ··åˆè¡¨ç¤ºï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®ä¿ä¸åŒåŠ¨ä½œå’Œè§†ç‚¹ä¸‹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>DynTet ä½¿ç”¨åŸºäºåæ ‡çš„ç½‘ç»œå¯¹ç¬¦å·è·ç¦»ã€å˜å½¢å’Œæè´¨çº¹ç†è¿›è¡Œå­¦ä¹ ï¼Œå°†è®­ç»ƒæ•°æ®é”šå®šåˆ°é¢„å®šä¹‰çš„å››é¢ä½“ç½‘æ ¼ä¸­ã€‚</li><li>DynTet åˆ©ç”¨ Marching Tetrahedra æœ‰æ•ˆåœ°è§£ç äº†å…·æœ‰ç¨³å®šæ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œå¹¶é€šè¿‡å¯å¾®åˆ†å…‰æ …å™¨å’Œåƒç´ æŸå¤±çš„ç›‘ç£å®ç°äº†å¿«é€Ÿæ¸²æŸ“ã€‚</li><li>DynTet ç»“åˆç»å…¸çš„ 3D å¯å˜å½¢æ¨¡å‹æ¥ä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªè§„èŒƒåŒ–ç©ºé—´æ¥ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚</li><li>ä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”ï¼ŒDynTet åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚</li><li>é™¤äº†åˆ¶ä½œå‡ºç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘å¤–ï¼Œè¯¥æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œæœ‰æœ›å®ç°è®¸å¤šæ–°å…´åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šç”¨äºé«˜å“è´¨è¯´è¯äººå¤´éƒ¨åˆæˆçš„åŠ¨æ€å››é¢ä½“å­¦ä¹ </li><li>ä½œè€…ï¼šå¼ å­å·ï¼Œå¼ æ’ï¼Œç‹ä½³ä¿Šï¼Œåˆ˜å­è¶…ï¼Œå­™å‰‘</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šè¯´è¯äººå¤´éƒ¨åˆæˆã€éšå¼è¡¨ç¤ºã€åŠ¨æ€ç½‘æ ¼ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.02574</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œéšå¼è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œåœ¨ä»è§†é¢‘åºåˆ—ç”Ÿæˆé€¼çœŸä¸”å¯åŠ¨ç”»åŒ–çš„å¤´éƒ¨å¤´åƒæ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›éšå¼æ–¹æ³•ä»ç„¶é¢ä¸´è§†è§‰ä¼ªå½±å’ŒæŠ–åŠ¨é—®é¢˜ï¼Œå› ä¸ºç¼ºä¹æ˜ç¡®çš„å‡ ä½•çº¦æŸï¼Œè¿™ç»™å‡†ç¡®å»ºæ¨¡å¤æ‚çš„é¢éƒ¨å˜å½¢å¸¦æ¥äº†æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é‡‡ç”¨éšå¼è¡¨ç¤ºï¼Œä½†ç¼ºä¹æ˜ç¡®çš„å‡ ä½•çº¦æŸï¼Œå¯¼è‡´è§†è§‰ä¼ªå½±å’ŒæŠ–åŠ¨é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºåŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œå®ƒé€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®ä¿åœ¨å„ç§è¿åŠ¨å’Œè§†ç‚¹ä¸‹å‡ ä½•ä¸€è‡´æ€§ã€‚DynTet ç”±åŸºäºåæ ‡çš„ç½‘ç»œå‚æ•°åŒ–ï¼Œè¯¥ç½‘ç»œå­¦ä¹ ç¬¦å·è·ç¦»ã€å˜å½¢å’Œæè´¨çº¹ç†ï¼Œå°†è®­ç»ƒæ•°æ®é”šå®šåˆ°é¢„å®šä¹‰çš„å››é¢ä½“ç½‘æ ¼ä¸­ã€‚åˆ©ç”¨è¡Œè¿›å››é¢ä½“ï¼ŒDynTet å¯ä»¥æœ‰æ•ˆåœ°è§£ç å…·æœ‰ç›¸åŒæ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œä»è€Œå¯ä»¥é€šè¿‡å¯å¾®åˆ†å…‰æ …åŒ–å™¨å¿«é€Ÿæ¸²æŸ“ï¼Œå¹¶é€šè¿‡åƒç´ æŸå¤±è¿›è¡Œç›‘ç£ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæœ¬æ–‡ç»“åˆäº†ç»å…¸çš„ 3D å¯å˜å½¢æ¨¡å‹æ¥ä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªè§„èŒƒç©ºé—´æ¥ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚è¿™äº›ä¼˜åŠ¿å¾—ç›Šäº DynTet ä¸­é‡‡ç”¨çš„æœ‰æ•ˆå‡ ä½•è¡¨ç¤ºã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠå¯¹ç›®æ ‡çš„æ”¯æŒï¼šä¸ä»¥å¾€çš„å·¥ä½œç›¸æ¯”ï¼Œæ ¹æ®å„ç§æŒ‡æ ‡ï¼ŒDynTet åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾ç€æå‡ã€‚é™¤äº†ç”Ÿæˆç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘å¤–ï¼Œæœ¬æ–‡æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œæœ‰æœ›æ”¯æŒè®¸å¤šæ–°å…´åº”ç”¨ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰é€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ï¼›(2): åŸºäºåæ ‡çš„ç½‘ç»œå‚æ•°åŒ–ï¼Œå­¦ä¹ ç¬¦å·è·ç¦»ã€å˜å½¢å’Œæè´¨çº¹ç†ï¼Œå°†æ•°æ®é”šå®šåˆ°å››é¢ä½“ç½‘æ ¼ä¸­ï¼›(3): åˆ©ç”¨è¡Œè¿›å››é¢ä½“è§£ç çº¹ç†ç½‘æ ¼ï¼Œé€šè¿‡å¯å¾®åˆ†å…‰æ …åŒ–å™¨æ¸²æŸ“å¹¶é€šè¿‡åƒç´ æŸå¤±è¿›è¡Œç›‘ç£ï¼›(4): ç»“åˆç»å…¸çš„3Då¯å˜å½¢æ¨¡å‹ä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå®šä¹‰è§„èŒƒç©ºé—´ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚</p><ol><li>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ï¼Œæå‡äº†è¯´è¯äººå¤´éƒ¨åˆæˆçš„ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºåŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œå®ƒé€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®ä¿åœ¨å„ç§è¿åŠ¨å’Œè§†ç‚¹ä¸‹å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>åŸºäºåæ ‡çš„ç½‘ç»œå‚æ•°åŒ–ï¼Œå­¦ä¹ ç¬¦å·è·ç¦»ã€å˜å½¢å’Œæè´¨çº¹ç†ï¼Œå°†è®­ç»ƒæ•°æ®é”šå®šåˆ°é¢„å®šä¹‰çš„å››é¢ä½“ç½‘æ ¼ä¸­ã€‚</li><li>åˆ©ç”¨è¡Œè¿›å››é¢ä½“ï¼ŒDynTetå¯ä»¥æœ‰æ•ˆåœ°è§£ç å…·æœ‰ç›¸åŒæ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œä»è€Œå¯ä»¥é€šè¿‡å¯å¾®åˆ†å…‰æ …åŒ–å™¨å¿«é€Ÿæ¸²æŸ“ï¼Œå¹¶é€šè¿‡åƒç´ æŸå¤±è¿›è¡Œç›‘ç£ã€‚</li><li>ç»“åˆäº†ç»å…¸çš„3Då¯å˜å½¢æ¨¡å‹æ¥ä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªè§„èŒƒç©ºé—´æ¥ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚</li><li>è¿™äº›ä¼˜åŠ¿å¾—ç›ŠäºDynTetä¸­é‡‡ç”¨çš„æœ‰æ•ˆå‡ ä½•è¡¨ç¤ºã€‚</li><li>ä¸ä»¥å¾€çš„å·¥ä½œç›¸æ¯”ï¼Œæ ¹æ®å„ç§æŒ‡æ ‡ï¼ŒDynTetåœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾ç€æå‡ã€‚</li><li>é™¤äº†ç”Ÿæˆç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘å¤–ï¼Œæœ¬æ–‡æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œæœ‰æœ›æ”¯æŒè®¸å¤šæ–°å…´åº”ç”¨ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾ç€æå‡ã€‚</li><li>ç”Ÿæˆäº†ç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘ã€‚</li><li>è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œæœ‰æœ›æ”¯æŒè®¸å¤šæ–°å…´åº”ç”¨ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://pica.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details><h2 id="CharNeRF-3D-Character-Generation-from-Concept-Art"><a href="#CharNeRF-3D-Character-Generation-from-Concept-Art" class="headerlink" title="CharNeRF: 3D Character Generation from Concept Art"></a>CharNeRF: 3D Character Generation from Concept Art</h2><p><strong>Authors:Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan</strong></p><p>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head self-attention layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our modelâ€™s inferencing capabilities are influenced by the training dataâ€™s characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data. </p><p><a href="http://arxiv.org/abs/2402.17115v1">PDF</a> </p><p><strong>Summary</strong><br>ç”¨æ¦‚å¿µå›¾åˆ›å»º 3D æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç¥ç»è¾å°„åœºå¹¶ä¸ºå›¾åƒå»ºæ¨¡æä¾›æ›´å¥½çš„è§†è§’ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è‰ºæœ¯åˆ›ä½œå’Œå®é™…åº”ç”¨ä¸­ï¼Œ3D å»ºæ¨¡å¾ˆæœ‰ä»·å€¼ï¼Œä½†éœ€è¦èŠ±è´¹æ—¶é—´å’ŒæŠ€èƒ½ã€‚</li><li>è¯¥æ–¹æ³•ä»æ ‡å‡†çš„ 3D å»ºæ¨¡è¡Œä¸šè¾“å…¥ï¼Œå³å¯æ ¹æ®ä¸€è‡´çš„é€è§†å›¾æ¦‚å¿µå›¾åˆ›å»º 3D è§’è‰²çš„ä½“ç§¯è¡¨ç¤ºã€‚</li><li>ç¥ç»è¾å°„åœº (NeRF) å·²æ”¹å˜åŸºäºå›¾åƒçš„ 3D é‡å»ºï¼Œä½†å°šæ— é’ˆå¯¹æ¦‚å¿µå›¾ä¼˜åŒ–ç®¡é“ã€‚</li><li>ç¼–ç æ¦‚å¿µå›¾ä¸ºæ¨¡å‹çš„å…ˆéªŒï¼Œåˆ©ç”¨æ¦‚å¿µå›¾ä¸­çš„æ¸…æ™°çš„èº«ä½“å§¿åŠ¿å’Œç‰¹å®šçš„è§†è§’ã€‚</li><li>é€šè¿‡å¯å­¦ä¹ çš„è§†å‘æ³¨æ„åŠ›å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼Œè®­ç»ƒç½‘ç»œåˆ©ç”¨å„ç§ 3D ç‚¹çš„å…ˆéªŒã€‚</li><li>å°„çº¿é‡‡æ ·å’Œè¡¨é¢é‡‡æ ·çš„ç»„åˆå¢å¼ºäº†ç½‘ç»œçš„æ¨ç†èƒ½åŠ›ã€‚</li><li>æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 360 åº¦è§’è‰²è§†å›¾ã€‚</li><li>å¼€å‘äº†ç®€å•çš„æŒ‡å—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹æå– 3D ç½‘æ ¼ã€‚</li><li>æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å—è®­ç»ƒæ•°æ®çš„å½±å“ï¼Œä¸»è¦é’ˆå¯¹å¤´éƒ¨ã€æ‰‹è‡‚å’Œè…¿éƒ¨ã€‚</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºå„ç§ä¸»é¢˜çš„æ¦‚å¿µå›¾ï¼Œå¯¹æ•°æ®æ²¡æœ‰ç‰¹æ®Šå‡è®¾ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šCharNeRFï¼šåŸºäºæ¦‚å¿µå›¾çš„ 3D è§’è‰²ç”Ÿæˆ</li><li>ä½œè€…ï¼šEddy Chuã€Yiyang Chenã€Chedy Raissiã€Anand Bhojan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»ç½‘ç»œã€è®¡ç®—æœºå›¾å½¢ã€è™šæ‹Ÿç°å®ã€æ¸¸æˆã€ç½‘æ ¼ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17115</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D å»ºæ¨¡åœ¨ AR/VR å’Œæ¸¸æˆä¸­è‡³å…³é‡è¦ï¼Œä½†é€šå¸¸è€—æ—¶ä¸”è¦æ±‚é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ä¸€è‡´çš„å‘¨è½¬æ¦‚å¿µå›¾ä¸­åˆ›å»º 3D è§’è‰²ä½“ç§¯è¡¨ç¤ºçš„æ–°æ–¹æ³•ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç¥ç»è¾å°„åœº (NeRF) å·²æˆä¸ºå›¾åƒé‡å»ºçš„å˜é©è€…ï¼Œä½†å°šæ— é’ˆå¯¹æ¦‚å¿µå›¾ä¼˜åŒ–ç®¡çº¿çš„ç ”ç©¶ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡åˆ©ç”¨æ¦‚å¿µå›¾ä¸­çš„å®šä¹‰çš„èº«ä½“å§¿åŠ¿å’Œç‰¹å®šçš„è§†è§’ï¼Œå°†å…¶ç¼–ç ä¸ºæ¨¡å‹çš„å…ˆéªŒã€‚æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„è§†å›¾æ–¹å‘æ³¨æ„åŠ›å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼Œè®©ç½‘ç»œåˆ©ç”¨è¿™äº›å…ˆéªŒã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯æ˜äº†å…‰çº¿é‡‡æ ·å’Œè¡¨é¢é‡‡æ ·çš„ç»„åˆå¢å¼ºäº†ç½‘ç»œçš„æ¨ç†èƒ½åŠ›ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 360 åº¦è§’è‰²è§†å›¾ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªç®€å•çš„æŒ‡å—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹æå– 3D ç½‘æ ¼ã€‚æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å—è®­ç»ƒæ•°æ®ç‰¹å¾çš„å½±å“ï¼Œä¸»è¦é’ˆå¯¹å…·æœ‰ä¸€ä¸ªå¤´éƒ¨ã€ä¸¤ä¸ªæ‰‹è‡‚å’Œä¸¤æ¡è…¿çš„è§’è‰²ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæœ¬æ–‡æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯é€‚åº”ä¸åŒä¸»é¢˜çš„æ¦‚å¿µå›¾ï¼Œè€Œæ— éœ€å¯¹æ•°æ®åšå‡ºä»»ä½•ç‰¹å®šå‡è®¾ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ç¼–ç æ¦‚å¿µå›¾ï¼šé‡‡ç”¨åŒå±‚æ²™æ¼ç¼–ç å™¨ï¼Œæå–æ¦‚å¿µå›¾çš„é«˜ä½å±‚æ¬¡ç»†èŠ‚ã€‚(2) è§†å›¾æ–¹å‘æ³¨æ„åŠ›å¤šå¤´è‡ªæ³¨æ„åŠ›ç‰¹å¾å‘é‡ç»„åˆï¼šä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶èåˆæ¥è‡ªæ¦‚å¿µå›¾çš„ä¸‰ä¸ªç‰¹å¾å‘é‡ï¼Œé‡ç‚¹å…³æ³¨æŸ¥è¯¢è§†å›¾æ–¹å‘ä¸æºè‰å›¾è§†å›¾æ–¹å‘ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚(3) ç¥ç»è¾å°„åœºï¼šä½¿ç”¨ç¥ç»è¾å°„åœºé¢„æµ‹æœ€ç»ˆé¢œè‰²å’Œå¯†åº¦ï¼ŒæŒ‡å¯¼ç½‘ç»œå­¦ä¹ ç‰¹å®šç±»åˆ«çš„ä¸€èˆ¬å½¢çŠ¶å’Œç‰¹å¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œå°è¯•è§£å†³è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªå…·æœ‰é‡è¦ AR/VR/æ¸¸æˆåº”ç”¨ä»·å€¼çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå³ä½¿ç”¨ NeRF ä»æ¦‚å¿µå›¾æ„å»º 3D è§’è‰²çš„ 3D è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºçš„æœ€ç»ˆæ¨¡å‹ CharNeRF å¾—ç›Šäºç”¨äºç»„åˆä¸åŒè¾“å…¥è§†å›¾ä¿¡æ¯çš„è§†å›¾æ–¹å‘æ³¨æ„åŠ›å¤šå¤´è‡ªæ³¨æ„åŠ›ç»„ä»¶ï¼Œèƒ½å¤Ÿä»å¦‚æ­¤ç¨€ç–çš„å›¾åƒè¾“å…¥ä¸­ç”Ÿæˆè‰¯å¥½çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„è§†å›¾æ–¹å‘æ³¨æ„åŠ›å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼Œè®©ç½‘ç»œåˆ©ç”¨æ¦‚å¿µå›¾ä¸­çš„å®šä¹‰çš„èº«ä½“å§¿åŠ¿å’Œç‰¹å®šçš„è§†è§’ã€‚æ­¤å¤–ï¼Œè¿˜è¯æ˜äº†å…‰çº¿é‡‡æ ·å’Œè¡¨é¢é‡‡æ ·çš„ç»„åˆå¢å¼ºäº†ç½‘ç»œçš„æ¨ç†èƒ½åŠ›ã€‚æ€§èƒ½ï¼šæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 360 åº¦è§’è‰²è§†å›¾ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªç®€å•çš„æŒ‡å—ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ¨¡å‹æå– 3D ç½‘æ ¼ã€‚å·¥ä½œé‡ï¼šæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å—è®­ç»ƒæ•°æ®ç‰¹å¾çš„å½±å“ï¼Œä¸»è¦é’ˆå¯¹å…·æœ‰ä¸€ä¸ªå¤´éƒ¨ã€ä¸¤ä¸ªæ‰‹è‡‚å’Œä¸¤æ¡è…¿çš„è§’è‰²ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæœ¬æ–‡æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯é€‚åº”ä¸åŒä¸»é¢˜çš„æ¦‚å¿µå›¾ï¼Œè€Œæ— éœ€å¯¹æ•°æ®åšå‡ºä»»ä½•ç‰¹å®šå‡è®¾ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-828eaae544f50ff5c3cb4c05ee9d80e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef7369a7d8878e03f6b272a4d1ebd217.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19f2984d16b69f5650701e035c363f95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b8a11537cec84e0f035cff561493d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f60295f4a9ff4a9d9749851b16f04d26.jpg" align="middle"></details><h2 id="CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency"><a href="#CMC-Few-shot-Novel-View-Synthesis-via-Cross-view-Multiplane-Consistency" class="headerlink" title="CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency"></a>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</h2><p><strong>Authors:Hanxin Zhu, Tianyu He, Zhibo Chen</strong></p><p>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.16407v1">PDF</a> Accepted by IEEE Conference on Virtual Reality and 3D User Interfaces   (IEEE VR 2024)</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å…¨æ–°è§†è§’åˆæˆä¸­å±•ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨è™šæ‹Ÿç°å® (VR) å’Œå¢å¼ºç°å® (AR) ä¸­ï¼Œè¿™å¾—ç›Šäºå…¶è¿ç»­è¡¨ç¤ºåœºæ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“åªæœ‰å°‘æ•°è¾“å…¥è§†å›¾å›¾åƒå¯ç”¨æ—¶ï¼ŒNeRF å€¾å‘äºå¯¹ç»™å®šçš„è§†å›¾è¿›è¡Œè¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œä½¿ä¼°è®¡çš„åƒç´ æ·±åº¦å‡ ä¹å…·æœ‰ç›¸åŒçš„å€¼ã€‚ä¸åŒäºé€šè¿‡å¼•å…¥å¤æ‚å…ˆéªŒæˆ–é™„åŠ ç›‘ç£æ¥è¿›è¡Œæ­£åˆ™åŒ–çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜ç¡®æ„å»ºäº†è¾“å…¥è§†å›¾ä¹‹é—´çš„æ·±åº¦æ„ŸçŸ¥ä¸€è‡´æ€§æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé€šè¿‡å¼ºåˆ¶ç›¸åŒçš„ç©ºé—´ç‚¹åœ¨ä¸åŒçš„è¾“å…¥è§†å›¾ä¸­è¢«é‡å¤é‡‡æ ·ï¼Œæˆ‘ä»¬èƒ½å¤ŸåŠ å¼ºè§†å›¾ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œå‡è½»è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åœ¨åˆ†å±‚è¡¨ç¤ºï¼ˆå³å¤šå¹³é¢å›¾åƒï¼‰ä¸Šå»ºç«‹ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”é‡‡æ ·ç‚¹å¯ä»¥åœ¨å¤šä¸ªç¦»æ•£å¹³é¢ä¸Šé‡æ–°é‡‡æ ·ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ­£åˆ™åŒ–æœªè§çš„ç›®æ ‡è§†å›¾ï¼Œæˆ‘ä»¬çº¦æŸä¸åŒè¾“å…¥è§†å›¾çš„æ¸²æŸ“é¢œè‰²å’Œæ·±åº¦ç›¸åŒã€‚è™½ç„¶ç®€å•ï¼Œä½†å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å®ç°æ›´å¥½çš„åˆæˆè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF åœ¨åªæœ‰å°‘æ•°è¾“å…¥è§†å›¾å›¾åƒå¯ç”¨æ—¶ä¼šè¿‡æ‹Ÿåˆã€‚</li><li>é€šè¿‡å¼ºåˆ¶ç›¸åŒçš„ç©ºé—´ç‚¹åœ¨ä¸åŒçš„è¾“å…¥è§†å›¾ä¸­è¢«é‡å¤é‡‡æ ·å¯ä»¥å‡è½»è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li><li>æˆ‘ä»¬åœ¨åˆ†å±‚è¡¨ç¤ºä¸Šæ„å»ºç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿åœ¨å¤šä¸ªç¦»æ•£å¹³é¢ä¸Šé‡æ–°é‡‡æ ·é‡‡æ ·ç‚¹ã€‚</li><li>æˆ‘ä»¬çº¦æŸä¸åŒè¾“å…¥è§†å›¾çš„æ¸²æŸ“é¢œè‰²å’Œæ·±åº¦ç›¸åŒï¼Œä»¥æ­£åˆ™åŒ–æœªè§çš„ç›®æ ‡è§†å›¾ã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å®ç°äº†æ›´å¥½çš„åˆæˆè´¨é‡ã€‚</li><li>æˆ‘ä»¬æ–¹æ³•çš„å…³é”®åœ¨äºæ˜¾å¼æ„å»ºè¾“å…¥è§†å›¾ä¹‹é—´çš„æ·±åº¦æ„ŸçŸ¥ä¸€è‡´æ€§ã€‚</li><li>æˆ‘ä»¬çš„æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œä¸éœ€è¦å¼•å…¥å¤æ‚å…ˆéªŒæˆ–é¢å¤–çš„ç›‘ç£ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCMCï¼šé€šè¿‡è·¨è§†å›¾å¤šå¹³é¢ä¸€è‡´æ€§è¿›è¡Œå°æ ·æœ¬æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šéŸ©æ˜•ç«¹ã€ä½•å¤©å®‡ã€é™ˆå¿—æ³¢</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°æ ·æœ¬è§†è§’åˆæˆã€å¤šå¹³é¢å›¾åƒã€è·¨è§†å›¾ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šNone, Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å°æ ·æœ¬è§†è§’åˆæˆä¸­å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯¼è‡´ä¼°è®¡çš„åƒç´ æ·±åº¦å‡ ä¹ç›¸åŒã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡å¼•å…¥å¤æ‚å…ˆéªŒæˆ–é¢å¤–ç›‘ç£æ¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä½†å­˜åœ¨é¢„è®­ç»ƒæˆæœ¬é«˜ã€åŸŸå·®è·ç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è·¨è§†å›¾æ·±åº¦æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒè¾“å…¥è§†å›¾ä¸­å¼ºåˆ¶é‡‡æ ·ç›¸åŒç©ºé—´ç‚¹ï¼ŒåŠ å¼ºè§†å›¾ä¹‹é—´çš„äº¤äº’ï¼Œç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ„å»ºäº†åŸºäºåˆ†å±‚è¡¨ç¤ºï¼ˆå³å¤šå¹³é¢å›¾åƒï¼‰çš„ç¥ç»ç½‘ç»œï¼Œå¹¶å¯¹å¤šå¹³é¢è¿›è¡Œé‡‡æ ·ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ­£åˆ™åŒ–æœªè§çš„ç›®æ ‡è§†å›¾ï¼Œæœ¬æ–‡çº¦æŸäº†ä¸åŒè¾“å…¥è§†å›¾æ¸²æŸ“çš„é¢œè‰²å’Œæ·±åº¦ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å°æ ·æœ¬è§†è§’åˆæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methods:(1):æ„å»ºåŸºäºåˆ†å±‚è¡¨ç¤ºçš„å¤šå¹³é¢å›¾åƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œé‡‡æ ·ï¼›(2):é€šè¿‡åœ¨ä¸åŒè¾“å…¥è§†å›¾ä¸­å¼ºåˆ¶é‡‡æ ·ç›¸åŒç©ºé—´ç‚¹ï¼ŒåŠ å¼ºè§†å›¾ä¹‹é—´çš„äº¤äº’ï¼›(3):çº¦æŸä¸åŒè¾“å…¥è§†å›¾æ¸²æŸ“çš„é¢œè‰²å’Œæ·±åº¦ä¸€è‡´æ€§ï¼Œæ­£åˆ™åŒ–æœªè§çš„ç›®æ ‡è§†å›¾ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº† CMC æ–¹æ³•ï¼Œé€šè¿‡è·¨è§†å›¾å¤šå¹³é¢ä¸€è‡´æ€§ï¼Œç¼“è§£äº† NeRF åœ¨å°æ ·æœ¬è§†è§’åˆæˆä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡äº†åˆæˆå›¾åƒçš„è´¨é‡ã€‚(2): åˆ›æ–°ç‚¹ï¼š<ul><li>æå‡ºè·¨è§†å›¾æ·±åº¦æ„ŸçŸ¥ä¸€è‡´æ€§æ–¹æ³•ï¼ŒåŠ å¼ºè§†å›¾ä¹‹é—´çš„äº¤äº’ï¼Œç¼“è§£è¿‡æ‹Ÿåˆã€‚</li><li>æ„å»ºåŸºäºåˆ†å±‚è¡¨ç¤ºçš„å¤šå¹³é¢å›¾åƒï¼Œå¹¶å¯¹å…¶è¿›è¡Œé‡‡æ ·ã€‚</li><li>çº¦æŸä¸åŒè¾“å…¥è§†å›¾æ¸²æŸ“çš„é¢œè‰²å’Œæ·±åº¦ä¸€è‡´æ€§ï¼Œæ­£åˆ™åŒ–æœªè§çš„ç›®æ ‡è§†å›¾ã€‚Performanceï¼š</li><li>åœ¨å°æ ·æœ¬è§†è§’åˆæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚Workloadï¼š</li><li>æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œæ˜“äºå®ç°ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-bdd46c7b217cb4180eb948c43ffad849.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-571786b47c356d9bc3c90a0ca95fe68b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bf909d8f8aa9e18f65bc56fd97a0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0da54ff7a201688851cb82cbbbe20007.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eff9d03d40a8b3f7618fd67f793df987.jpg" align="middle"></details><h2 id="SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field"><a href="#SPC-NeRF-Spatial-Predictive-Compression-for-Voxel-Based-Radiance-Field" class="headerlink" title="SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field"></a>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</h2><p><strong>Authors:Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao</strong></p><p>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time. </p><p><a href="http://arxiv.org/abs/2402.16366v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨ç©ºé—´é¢„æµ‹ç¼–ç å¯¹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ˜¾å¼ä½“ç´ ç½‘æ ¼ï¼ˆEVGï¼‰è¿›è¡Œå‹ç¼©ï¼Œå¯æœ‰æ•ˆæå‡å…¶å­˜å‚¨å’Œä¼ è¾“æ•ˆç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºåŸºäºæ˜¾å¼ä½“ç´ ç½‘æ ¼ï¼ˆvoxel gridï¼‰çš„ NeRF å‹ç¼©æ–°æ¡†æ¶â€”â€”SPC-NeRF</li><li>åˆ©ç”¨ç©ºé—´é¢„æµ‹ç¼–ç æœ‰æ•ˆå»é™¤ä½“ç´ çš„ç©ºé—´å†—ä½™ï¼Œæå‡å‹ç¼©æ€§èƒ½</li><li>æå‡ºæ–°çš„æ¯”ç‰¹ç‡å»ºæ¨¡å’ŒæŸå¤±å‡½æ•°å½¢å¼ï¼Œå®ç°å‹ç¼©ç‡ä¸å¤±çœŸçš„è”åˆä¼˜åŒ–</li><li>åœ¨å¤šä¸ªä»£è¡¨æ€§æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œä¸æœ€å…ˆè¿›çš„ VQRF æ–¹æ³•ç›¸æ¯”ï¼ŒèŠ‚çœ 32% çš„æ¯”ç‰¹ç‡</li><li>è®­ç»ƒæ—¶é—´ä¸ VQRF ç›¸å½“</li><li>å……åˆ†åˆ©ç”¨äº†ä½“ç´ çš„ç©ºé—´ç›¸å…³æ€§ï¼Œä¼˜äºä»ç¥ç»ç½‘ç»œå‹ç¼©æ–¹æ³•ç»§æ‰¿çš„å‹ç¼©æŠ€æœ¯</li><li>æ˜¾å¼ä½“ç´ ç½‘æ ¼çš„å‹ç¼©å¯¹äº NeRF çš„å­˜å‚¨å’Œä¼ è¾“è‡³å…³é‡è¦</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSPC-NeRFï¼šä½“ç´ åŒ–å…‰åœºè¾å°„çš„ç©ºåŸŸé¢„æµ‹å‹ç¼©</li><li>ä½œè€…ï¼šå®‹æ³½å¤©ã€æ®µæ–‡å®ã€å¼ å®‡æ€€ã€ç‹è¯—å¥‡ã€é©¬æ€ä¼Ÿã€é«˜æ–‡</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFã€EVGã€ç©ºåŸŸé¢„æµ‹ç¼–ç ã€æ•°æ®å‹ç¼©</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.16366    Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä½¿ç”¨æ˜¾å¼ä½“ç´ ç½‘æ ¼ï¼ˆEVGï¼‰è¡¨ç¤ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯æå‡ NeRF æ€§èƒ½çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚ç„¶è€Œï¼ŒEVG è¡¨ç¤ºåœ¨å­˜å‚¨å’Œä¼ è¾“æ–¹é¢æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå†…å­˜å¼€é”€å·¨å¤§ã€‚å½“å‰ç”¨äºå‹ç¼© EVG çš„æ–¹æ³•ä¸»è¦ç»§æ‰¿äº†ä¸ºç¥ç»ç½‘ç»œå‹ç¼©è®¾è®¡çš„å‰ªæå’Œé‡åŒ–ç­‰æ–¹æ³•ï¼Œè€Œè¿™äº›æ–¹æ³•å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨ä½“ç´ çš„ç©ºé—´ç›¸å…³æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åˆ©ç”¨ç¥ç»ç½‘ç»œå‹ç¼©æŠ€æœ¯ï¼Œå¦‚å‰ªæå’Œé‡åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•æ²¡æœ‰å……åˆ†åˆ©ç”¨ä½“ç´ çš„ç©ºé—´ç›¸å…³æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šå—ç¹è£çš„æ•°å­—å›¾åƒå‹ç¼©æŠ€æœ¯å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº† SPC-NeRFï¼Œä¸€ä¸ªå°†ç©ºåŸŸé¢„æµ‹ç¼–ç åº”ç”¨äº EVG å‹ç¼©çš„æ–°æ¡†æ¶ã€‚æå‡ºçš„æ¡†æ¶å¯ä»¥æœ‰æ•ˆå»é™¤ç©ºé—´å†—ä½™ï¼Œä»¥è·å¾—æ›´å¥½çš„å‹ç¼©æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ¯”ç‰¹ç‡è¿›è¡Œå»ºæ¨¡å¹¶è®¾è®¡äº†æ–°çš„æŸå¤±å‡½æ•°å½¢å¼ï¼Œåœ¨è¯¥æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è”åˆä¼˜åŒ–å‹ç¼©æ¯”å’Œå¤±çœŸï¼Œä»¥å®ç°æ›´é«˜çš„ç¼–ç æ•ˆç‡ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šå¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ EVG NeRF å‹ç¼©æ–¹æ³• VQRF ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä»£è¡¨æ€§æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº† 32% çš„æ¯”ç‰¹èŠ‚çœï¼Œè®­ç»ƒæ—¶é—´ç›¸å½“ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)å—æ•°å­—å›¾åƒå‹ç¼©æŠ€æœ¯çš„å¯å‘ï¼Œæå‡ºSPC-NeRFï¼Œä¸€ä¸ªå°†ç©ºåŸŸé¢„æµ‹ç¼–ç åº”ç”¨äºEVGå‹ç¼©çš„æ–°æ¡†æ¶ï¼›(2)å°†EVGè¡¨ç¤ºä¸ºç‰¹å¾ç½‘æ ¼ï¼Œå¹¶åˆ©ç”¨å…¶ç©ºé—´ç›¸å…³æ€§ï¼Œé€šè¿‡é¢„æµ‹ç¼–ç å»é™¤ç©ºé—´å†—ä½™ï¼›(3)è®¾è®¡æ–°çš„æŸå¤±å‡½æ•°å½¢å¼ï¼Œè”åˆä¼˜åŒ–å‹ç¼©æ¯”å’Œå¤±çœŸï¼Œå®ç°æ›´é«˜çš„ç¼–ç æ•ˆç‡ã€‚</p><ol><li>æ€»ç»“ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºæå‡ºäº†SPC-NeRFï¼Œä¸€ä¸ªå°†ç©ºåŸŸé¢„æµ‹ç¼–ç åº”ç”¨äºEVGå‹ç¼©çš„æ–°æ¡†æ¶ï¼Œæœ‰æ•ˆå»é™¤äº†ç©ºé—´å†—ä½™ï¼Œæé«˜äº†å‹ç¼©æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šâ€¢ æå‡ºSPC-NeRFï¼Œå°†ç©ºåŸŸé¢„æµ‹ç¼–ç åº”ç”¨äºEVGå‹ç¼©ï¼Œå……åˆ†åˆ©ç”¨äº†ä½“ç´ çš„ç©ºé—´ç›¸å…³æ€§ã€‚â€¢ è®¾è®¡æ–°çš„æŸå¤±å‡½æ•°å½¢å¼ï¼Œè”åˆä¼˜åŒ–å‹ç¼©æ¯”å’Œå¤±çœŸï¼Œå®ç°æ›´é«˜çš„ç¼–ç æ•ˆç‡ã€‚æ€§èƒ½ï¼šâ€¢ ä¸æœ€å…ˆè¿›çš„EVG-NeRFå‹ç¼©æ–¹æ³•VQRFç›¸æ¯”ï¼Œåœ¨å¤šä¸ªä»£è¡¨æ€§æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†32%çš„æ¯”ç‰¹èŠ‚çœï¼Œè®­ç»ƒæ—¶é—´ç›¸å½“ã€‚å·¥ä½œé‡ï¼šâ€¢ è®ºæ–‡ç†è®ºåˆ†ææ¸…æ™°ï¼Œå®éªŒç»“æœå……åˆ†ï¼Œä»£ç å¼€æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6f6705a1aaf3db9b5a416e3ffecb9e26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5908f2606537f6a0653b96477b77c75f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efc08eb0ec890344de572f2b2004f9c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-866d14094e6f176536a298862171f8d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3117d16ce413f3de96c9535aaa0804e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d0efdf7e947815763e89d08400d8bd32.jpg" align="middle"></details><h2 id="GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction"><a href="#GenNBV-Generalizable-Next-Best-View-Policy-for-Active-3D-Reconstruction" class="headerlink" title="GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction"></a>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</h2><p><strong>Authors:Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang</strong></p><p>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a benchmark using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions. </p><p><a href="http://arxiv.org/abs/2402.16174v1">PDF</a> </p><p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½é©±åŠ¨åœºæ™¯é‡å»ºçš„è‡ªåŠ¨åŒ–æ‹æ‘„è¿‡ç¨‹ï¼Œæå‡äº†çœŸå®æ„Ÿï¼Œç®€åŒ–äº†å·¥ä½œ</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨åŒ–æ‹æ‘„æµç¨‹</li><li>5Dè‡ªç”±ç©ºé—´æ‰©å±•äº†åŠ¨ä½œèŒƒå›´</li><li>å¤šæºçŠ¶æ€åµŒå…¥å¢å¼ºäº†è·¨æ•°æ®é›†æ³›åŒ–æ€§</li><li>Isaac Gymæ¨¡æ‹Ÿå™¨å»ºç«‹äº†NBVç­–ç•¥è¯„ä¼°åŸºå‡†</li><li>åœ¨Houses3Kå’ŒOmniObject3Dæ•°æ®é›†ä¸Šï¼Œè¦†ç›–ç‡åˆ†åˆ«è¾¾åˆ°98.26%å’Œ97.12%</li><li>ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆ</li><li>é€‚ç”¨äºå¤§å‹åœºæ™¯çš„æ‰«æå’Œäº¤äº’</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGenNBVï¼šç”¨äºä¸»åŠ¨ 3D é‡å»ºçš„å¯æ³›åŒ–æœ€ä½³ä¸‹ä¸€è§†è§’ç­–ç•¥</li><li>ä½œè€…ï¼šZiqi Wang, Xinyu Zhang, Tianhao Wu, Yinda Zhang, Xiaogang Jin, Yu Rong, Hui Huang</li><li>éš¶å±ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šä¸»åŠ¨ 3D é‡å»ºï¼Œæœ€ä½³ä¸‹ä¸€è§†è§’ï¼Œæ·±åº¦å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šGenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstructionï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºåœ¨é€¼çœŸæ•°å­—åŒ–å¤§å‹åœºæ™¯æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†å›¾åƒæ•æ‰è¿‡ç¨‹ä»ç„¶è€—æ—¶ä¸”è´¹åŠ›ã€‚ä»¥å¾€å·¥ä½œå°è¯•ä½¿ç”¨æœ€ä½³ä¸‹ä¸€è§†è§’ï¼ˆNBVï¼‰ç­–ç•¥æ¥è‡ªåŠ¨æ‰§è¡Œæ­¤è¿‡ç¨‹ä»¥ä¸»åŠ¨è¿›è¡Œ 3D é‡å»ºã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ NBV ç­–ç•¥ä¸¥é‡ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æ ‡å‡†ã€æœ‰é™çš„åŠ¨ä½œç©ºé—´æˆ–é’ˆå¯¹ç‰¹å®šåœºæ™¯ä¼˜åŒ–åçš„è¡¨ç¤ºã€‚è¿™äº›é™åˆ¶å› ç´ é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚ï¼ˆ3ï¼‰ï¼šè®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡º GenNBVï¼Œä¸€ç§ç«¯åˆ°ç«¯å¯æ³›åŒ–çš„ NBV ç­–ç•¥ã€‚è¯¥ç­–ç•¥é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œå¹¶å°†å…¸å‹æœ‰é™çš„åŠ¨ä½œç©ºé—´æ‰©å±•åˆ° 5D è‡ªç”±ç©ºé—´ã€‚å®ƒä½¿ä»£ç†æ— äººæœºèƒ½å¤Ÿä»ä»»ä½•è§†ç‚¹è¿›è¡Œæ‰«æï¼Œç”šè‡³åœ¨è®­ç»ƒæœŸé—´ä¸çœ‹ä¸è§çš„å‡ ä½•ä½“è¿›è¡Œäº¤äº’ã€‚ä¸ºäº†æé«˜è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæºçŠ¶æ€åµŒå…¥ï¼ŒåŒ…æ‹¬å‡ ä½•ã€è¯­ä¹‰å’ŒåŠ¨ä½œè¡¨ç¤ºã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šä½¿ç”¨ IsaacGym æ¨¡æ‹Ÿå™¨å’Œ Houses3K åŠ OmniObject3D æ•°æ®é›†å»ºç«‹åŸºå‡†æ¥è¯„ä¼°æ­¤ NBV ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨è¿™äº›æ•°æ®é›†æœªæ›¾è§è¿‡çš„å»ºç­‘è§„æ¨¡ç‰©ä½“ä¸Šåˆ†åˆ«è¾¾åˆ° 98.26% å’Œ 97.12% çš„è¦†ç›–ç‡ï¼Œä¼˜äºå…ˆå‰çš„è§£å†³æ–¹æ¡ˆã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰å°†ä¸»åŠ¨3Dé‡å»ºé—®é¢˜è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ï¼Œè®¾è®¡æ–°çš„è§‚æµ‹ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´ï¼›ï¼ˆ2ï¼‰æå‡ºç«¯åˆ°ç«¯çš„NBVç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†å…¸å‹æœ‰é™çš„åŠ¨ä½œç©ºé—´æ‰©å±•åˆ°5Dè‡ªç”±ç©ºé—´ï¼›ï¼ˆ3ï¼‰æå‡ºä¸€ç§æ–°çš„å¤šæºçŠ¶æ€åµŒå…¥ï¼ŒåŒ…æ‹¬å‡ ä½•ã€è¯­ä¹‰å’ŒåŠ¨ä½œè¡¨ç¤ºï¼Œä»¥æé«˜è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆ4ï¼‰è®¾è®¡åæ˜ ä¼˜åŒ–ç›®æ ‡çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶è¯¦ç»†è¯´æ˜ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸»åŠ¨ 3D åœºæ™¯é‡å»ºçš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå‡å°‘äº†äººå·¥å¹²é¢„çš„éœ€è¦ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºå­¦ä¹ çš„ç­–ç•¥æ¢ç´¢äº†å¦‚ä½•åœ¨è®­ç»ƒé˜¶æ®µé‡å»ºå„ç§å¯¹è±¡ï¼Œä»è€Œèƒ½å¤Ÿä»¥å®Œå…¨è‡ªä¸»çš„æ–¹å¼æ³›åŒ–ä»¥é‡å»ºçœ‹ä¸è§çš„å¯¹è±¡ã€‚æˆ‘ä»¬çš„æ§åˆ¶å™¨åœ¨è‡ªç”±ç©ºé—´ä¸­æœºåŠ¨ï¼Œç„¶ååŸºäºæ··åˆåœºæ™¯è¡¨ç¤ºé€‰æ‹©ä¸‹ä¸€ä¸ªæœ€ä½³è§†å›¾ï¼Œè¯¥è¡¨ç¤ºä¼ è¾¾äº†åœºæ™¯è¦†ç›–çŠ¶æ€ï¼Œä»è€Œå®ç°é‡å»ºè¿›åº¦ã€‚æˆ‘ä»¬é€šè¿‡åœ¨åŒ…æ‹¬ Houses3Kã€OmniObject3D å’Œ Objaverse åœ¨å†…çš„å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨ holdout Houses3K æµ‹è¯•é›†å’Œè·¨åŸŸ OmniObject3D æˆ¿å±‹ç±»åˆ«ä¸Šçš„å®šé‡å’Œå®šæ€§æ³›åŒ–ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºçš„å®Œæ•´æ€§ã€æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºå…¶ä»–åŸºçº¿ã€‚æ­¤å¤–ï¼Œåœ¨ Objaverse ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å•ä¸€å»ºç­‘è®¾ç½®ä¸­è®­ç»ƒçš„ç­–ç•¥ç”šè‡³å¯ä»¥æ³›åŒ–åˆ°å¤æ‚çš„æˆ·å¤–åœºæ™¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šGenNBV æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯å¯æ³›åŒ–çš„æœ€ä½³ä¸‹ä¸€è§†è§’ç­–ç•¥ï¼Œæ‰©å±•äº†åŠ¨ä½œç©ºé—´ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæºçŠ¶æ€åµŒå…¥æ¥æé«˜è·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨ Houses3K å’Œ OmniObject3D æ•°æ®é›†ä¸Šï¼ŒGenNBV åœ¨æœªè§è¿‡çš„å»ºç­‘è§„æ¨¡ç‰©ä½“ä¸Šåˆ†åˆ«è¾¾åˆ° 98.26% å’Œ 97.12% çš„è¦†ç›–ç‡ï¼Œä¼˜äºå…ˆå‰çš„è§£å†³æ–¹æ¡ˆï¼›å·¥ä½œé‡ï¼šGenNBV çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”éœ€è¦é’ˆå¯¹ä¸åŒçš„åœºæ™¯è¿›è¡Œå¾®è°ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5e8d5c56796ce65689171d3e4517ceb1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3132d23adee2a0316b9fc9d6cad91a0b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f46161465b1542e68d3bcde0a29f1da4.jpg" align="middle"></details><h2 id="NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection"><a href="#NeRF-Det-Incorporating-Semantic-Cues-and-Perspective-aware-Depth-Supervision-for-Indoor-Multi-View-3D-Detection" class="headerlink" title="NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection"></a>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth   Supervision for Indoor Multi-View 3D Detection</h2><p><strong>Authors:Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</strong></p><p>NeRF-Det has achieved impressive performance in indoor multi-view 3D detection by innovatively utilizing NeRF to enhance representation learning. Despite its notable performance, we uncover three decisive shortcomings in its current design, including semantic ambiguity, inappropriate sampling, and insufficient utilization of depth supervision. To combat the aforementioned problems, we present three corresponding solutions: 1) Semantic Enhancement. We project the freely available 3D segmentation annotations onto the 2D plane and leverage the corresponding 2D semantic maps as the supervision signal, significantly enhancing the semantic awareness of multi-view detectors. 2) Perspective-aware Sampling. Instead of employing the uniform sampling strategy, we put forward the perspective-aware sampling policy that samples densely near the camera while sparsely in the distance, more effectively collecting the valuable geometric clues. 3)Ordinal Residual Depth Supervision. As opposed to directly regressing the depth values that are difficult to optimize, we divide the depth range of each scene into a fixed number of ordinal bins and reformulate the depth prediction as the combination of the classification of depth bins as well as the regression of the residual depth values, thereby benefiting the depth learning process. The resulting algorithm, NeRF-Det++, has exhibited appealing performance in the ScanNetV2 and ARKITScenes datasets. Notably, in ScanNetV2, NeRF-Det++ outperforms the competitive NeRF-Det by +1.9% in mAP@0.25 and +3.5% in mAP@0.50$. The code will be publicly at <a href="https://github.com/mrsempress/NeRF-Detplusplus">https://github.com/mrsempress/NeRF-Detplusplus</a>. </p><p><a href="http://arxiv.org/abs/2402.14464v1">PDF</a> 7 pages, 2 figures</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯è¢«åˆ›æ–°åº”ç”¨äºå¢å¼ºå¤šè§†è§’3Dæ£€æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç¤ºå­¦ä¹ ï¼Œæ˜¾è‘—æå‡äº†å®¤å†…åœºæ™¯ä¸­çš„3Dæ£€æµ‹æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å‘ç°äº†NeRF-Detå­˜åœ¨è¯­ä¹‰æ­§ä¹‰ã€é‡‡æ ·ä¸å½“å’Œæ·±åº¦ç›‘ç£åˆ©ç”¨ä¸è¶³ç­‰ä¸»è¦ç¼ºé™·ã€‚</li><li>æå‡ºè¯­ä¹‰å¢å¼ºã€é€è§†æ„ŸçŸ¥é‡‡æ ·å’Œåºæ•°æ®‹å·®æ·±åº¦ç›‘ç£æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li><li>NeRF-Det++æœ‰æ•ˆè§£å†³äº†NeRF-Detçš„ç¼ºé™·ï¼Œåœ¨ScanNetV2å’ŒARKITScenesæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li><li>NeRF-Det++åœ¨ScanNetV2ä¸Šæ¯”NeRF-Detåœ¨mAP@0.25å’ŒmAP@0.50åˆ†åˆ«æé«˜äº†1.9%å’Œ3.5%ã€‚</li><li>ä»£ç å·²å…¬å¼€å‘å¸ƒï¼š<a href="https://github.com/mrsempress/NeRF-Detplusplusã€‚">https://github.com/mrsempress/NeRF-Detplusplusã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> NeRF-Det++ï¼šèåˆè¯­ä¹‰çº¿ç´¢å’Œè§†ç‚¹æ„ŸçŸ¥æ·±åº¦</li><li><strong>ä½œè€…ï¼š</strong> Chenxi Huang, Yuenan Hou, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin, Deng Cai, Wanli Ouyang</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ï¼Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li><strong>å…³é”®è¯ï¼š</strong> NeRFã€å¤šè§†å›¾ä¸‰ç»´æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2402.14464</li><li><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> NeRF-Det åœ¨å®¤å†…å¤šè§†å›¾ä¸‰ç»´æ£€æµ‹ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå®ƒåˆ›æ–°æ€§åœ°åˆ©ç”¨ NeRF å¢å¼ºäº†è¡¨å¾å­¦ä¹ ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> NeRF-Det å­˜åœ¨è¯­ä¹‰æ¨¡ç³Šã€é‡‡æ ·ä¸å½“å’Œæ·±åº¦ç›‘ç£åˆ©ç”¨ä¸è¶³ä¸‰ä¸ªå…³é”®ç¼ºé™·ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼š<ul><li><strong>è¯­ä¹‰å¢å¼ºï¼š</strong> å°†å…è´¹æä¾›çš„ 3D åˆ†å‰²æ³¨é‡ŠæŠ•å½±åˆ° 2D å¹³é¢ï¼Œå¹¶åˆ©ç”¨ç›¸åº”çš„ 2D è¯­ä¹‰å›¾ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œæ˜¾è‘—å¢å¼ºäº†å¤šè§†å›¾æ£€æµ‹å™¨çš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li><li><strong>è§†ç‚¹æ„ŸçŸ¥é‡‡æ ·ï¼š</strong> æå‡ºè§†ç‚¹æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨é è¿‘ç›¸æœºå¤„å¯†é›†é‡‡æ ·ï¼Œè€Œåœ¨è¿œå¤„ç¨€ç–é‡‡æ ·ï¼Œæ›´æœ‰æ•ˆåœ°æ”¶é›†æœ‰ä»·å€¼çš„å‡ ä½•çº¿ç´¢ã€‚</li><li><strong>æœ‰åºæ®‹å·®æ·±åº¦ç›‘ç£ï¼š</strong> ä¸ç›´æ¥å›å½’éš¾ä»¥ä¼˜åŒ–çš„æ·±åº¦å€¼ç›¸åï¼Œå°†æ¯ä¸ªåœºæ™¯çš„æ·±åº¦èŒƒå›´åˆ’åˆ†ä¸ºå›ºå®šæ•°é‡çš„æœ‰åºç®±ï¼Œå¹¶å°†æ·±åº¦é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºæ·±åº¦ç®±åˆ†ç±»å’Œæ®‹å·®æ·±åº¦å€¼å›å½’çš„ç»„åˆï¼Œä»è€Œæœ‰åˆ©äºæ·±åº¦å­¦ä¹ è¿‡ç¨‹ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> åœ¨å®¤å†…å¤šè§†å›¾ä¸‰ç»´æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ul></li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰è¯­ä¹‰å¢å¼ºï¼šåœ¨NeRF-Detä¸­åŠ å…¥è¯­ä¹‰åˆ†æ”¯Î¦Sï¼Œå°†å‡ ä½•æ¨¡å—Î¦Gç”Ÿæˆçš„ç‰¹å¾h(x)è¾“å…¥Î¦Sï¼Œäº§ç”Ÿè¯­ä¹‰é¢„æµ‹sï¼Œå¹¶åˆ©ç”¨äº¤å‰ç†µæŸå¤±LSegç›‘ç£è¯­ä¹‰å›¾çš„å­¦ä¹ ã€‚ï¼ˆ2ï¼‰è§†ç‚¹æ„ŸçŸ¥é‡‡æ ·ï¼šå°†NeRF-Detä¸­çš„å‡åŒ€é‡‡æ ·ï¼ˆUSï¼‰æ›¿æ¢ä¸ºè§†ç‚¹æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ï¼Œåœ¨é è¿‘ç›¸æœºå¤„å¯†é›†é‡‡æ ·ï¼Œè€Œåœ¨è¿œå¤„ç¨€ç–é‡‡æ ·ï¼Œæ›´æœ‰æ•ˆåœ°æ”¶é›†æœ‰ä»·å€¼çš„å‡ ä½•çº¿ç´¢ã€‚ï¼ˆ3ï¼‰æœ‰åºæ®‹å·®æ·±åº¦ç›‘ç£ï¼šå°†æ¯ä¸ªåœºæ™¯çš„æ·±åº¦èŒƒå›´åˆ’åˆ†ä¸ºå›ºå®šæ•°é‡çš„æœ‰åºç®±ï¼Œå°†æ·±åº¦é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºæ·±åº¦ç®±åˆ†ç±»å’Œæ®‹å·®æ·±åº¦å€¼å›å½’çš„ç»„åˆï¼Œæœ‰åˆ©äºæ·±åº¦å­¦ä¹ è¿‡ç¨‹ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡º NeRF-Det++ï¼Œä¸€ç§ç”¨äºä»å¤šè§†å›¾å›¾åƒè¿›è¡Œå®¤å†… 3D æ£€æµ‹çš„æ–°é¢–æ–¹æ³•ã€‚æˆ‘ä»¬è¯†åˆ«å¹¶è§£å†³äº† NeRF-Det ä¸­çš„ä¸‰ä¸ªå…³é”®ç¼ºé™·ã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ¨¡ç³Šï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰å¢å¼ºæ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨è¯­ä¹‰ç›‘ç£æ¥æ”¹å–„åˆ†ç±»ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³ä¸é€‚å½“çš„é‡‡æ ·ï¼Œæˆ‘ä»¬é€šè¿‡é€è§†æ„ŸçŸ¥é‡‡æ ·çš„è®¾è®¡ä¼˜å…ˆè€ƒè™‘é™„è¿‘å¯¹è±¡å¹¶åˆ©ç”¨å¤šè§†å›¾çš„ç‰¹æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºåºæ•°æ®‹å·®æ·±åº¦ç›‘ç£æ¥è§£å†³æ·±åº¦ç›‘ç£åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç›‘ç£ç»“åˆäº†åºæ•°æ·±åº¦ç®±çš„åˆ†ç±»å’Œæ®‹å·®æ·±åº¦å€¼çš„å›å½’ã€‚åœ¨ ScanNetV2 å’Œ ARKIT åœºæ™¯ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†æˆ‘ä»¬ NeRF-Det++ çš„ä¼˜è¶Šæ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>è¯­ä¹‰å¢å¼ºï¼šå¼•å…¥è¯­ä¹‰åˆ†æ”¯ï¼Œåˆ©ç”¨è¯­ä¹‰ç›‘ç£å¢å¼ºè¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li><li>é€è§†æ„ŸçŸ¥é‡‡æ ·ï¼šè®¾è®¡é€è§†æ„ŸçŸ¥é‡‡æ ·ç­–ç•¥ï¼Œæ›´æœ‰æ•ˆåœ°æ”¶é›†æœ‰ä»·å€¼çš„å‡ ä½•çº¿ç´¢ã€‚</li><li>åºæ•°æ®‹å·®æ·±åº¦ç›‘ç£ï¼šå°†æ·±åº¦é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºæ·±åº¦ç®±åˆ†ç±»å’Œæ®‹å·®æ·±åº¦å€¼å›å½’çš„ç»„åˆï¼Œæœ‰åˆ©äºæ·±åº¦å­¦ä¹ è¿‡ç¨‹ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ScanNetV2 å’Œ ARKIT åœºæ™¯ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• NeRF-Det++ï¼Œæ¶‰åŠè¯­ä¹‰å¢å¼ºã€é€è§†æ„ŸçŸ¥é‡‡æ ·å’Œåºæ•°æ®‹å·®æ·±åº¦ç›‘ç£ã€‚</li><li>åœ¨ ScanNetV2 å’Œ ARKIT åœºæ™¯ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-10b590fb75f1e40d114fb69be9c25a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffacf9378a148c5b9fac1fd2e03fc268.jpg" align="middle"><img src="https://picx.zhimg.com/v2-478a5df442fbaaa3a3c020c875f267ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecbc9426af10136860227da1181ee0cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af160b3a5172d7fc20bcc97ad42a6d6f.jpg" align="middle"></details><h2 id="Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields"><a href="#Mip-Grid-Anti-aliased-Grid-Representations-for-Neural-Radiance-Fields" class="headerlink" title="Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields"></a>Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields</h2><p><strong>Authors:Seungtae Nam, Daniel Rho, Jong Hwan Ko, Eunbyung Park</strong></p><p>Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering â€œjaggiesâ€ or â€œblurryâ€ images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has addressed this challenge by rendering conical frustums instead of rays. However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. The proposed method generates multi-scale grids by applying simple convolution operations over a shared grid representation and uses the scale-aware coordinate to retrieve features at different scales from the generated multi-scale grids. To test the effectiveness, we integrated the proposed method into the two recent representative grid-based methods, TensoRF and K-Planes. Experimental results demonstrate that mip-Grid greatly improves the rendering performance of both methods and even outperforms mip-NeRF on multi-scale datasets while achieving significantly faster training time. For code and demo videos, please see <a href="https://stnamjef.github.io/mipgrid.github.io/">https://stnamjef.github.io/mipgrid.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.14196v1">PDF</a> Accepted to NeurIPS 2023</p><p><strong>Summary</strong><br>åŸºäºç½‘æ ¼è¡¨ç¤ºçš„åèµ°æ · NeRF æ–¹æ³•ï¼Œå®ç°å¿«é€Ÿè®­ç»ƒåŒæ—¶æ¶ˆé™¤æ··å ä¼ªå½±ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>mip-Grid å°†åèµ°æ ·æŠ€æœ¯é›†æˆåˆ°åŸºäºç½‘æ ¼çš„ NeRF ä¸­ï¼Œè§£å†³äº†æ··å é—®é¢˜ã€‚</li><li>ä½¿ç”¨ç®€å•å·ç§¯æ“ä½œåœ¨å…±äº«ç½‘æ ¼è¡¨ç¤ºä¸Šç”Ÿæˆå¤šå°ºåº¦ç½‘æ ¼ï¼Œå‡è½»äº†æ··å ä¼ªå½±ã€‚</li><li>ä½¿ç”¨å°ºåº¦æ„ŸçŸ¥åæ ‡ä»ç”Ÿæˆçš„å¤šå°ºåº¦ç½‘æ ¼ä¸­æ£€ç´¢ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚</li><li>å°†è¯¥æ–¹æ³•é›†æˆåˆ° TensoRF å’Œ K-Planes ç­‰åŸºäºç½‘æ ¼çš„ NeRF æ–¹æ³•ä¸­ã€‚</li><li>å®éªŒè¡¨æ˜ mip-Grid å¤§å¹…æé«˜äº†ä¸¤ç§æ–¹æ³•çš„æ¸²æŸ“æ€§èƒ½ï¼Œåœ¨å¤šå°ºåº¦æ•°æ®é›†ä¸Šç”šè‡³ä¼˜äº mip-NeRFã€‚</li><li>mip-Grid å®ç°äº†æ˜¾è‘—æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.æ ‡é¢˜ï¼šMip-Gridï¼šç¥ç»è¾å°„åœºä¸­çš„æŠ—é”¯é½¿ç½‘æ ¼è¡¨ç¤ºï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰2.ä½œè€…ï¼šSeungtae Namã€Daniel Rhoã€Jong Hwan Koã€Eunbyung Park3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½æˆå‡é¦†å¤§å­¦äººå·¥æ™ºèƒ½ç³»ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰4.å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æŠ—é”¯é½¿ã€ç½‘æ ¼è¡¨ç¤º5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.14196Githubä»£ç é“¾æ¥ï¼šæ— 6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨è¡¨ç¤º3Dåœºæ™¯å’Œç”Ÿæˆæ–°è§†å›¾å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†ç°æœ‰çš„æ–¹æ³•ä¸­æ™®éå­˜åœ¨é”¯é½¿é—®é¢˜ï¼Œå³åœ¨ä¸åŒçš„ç›¸æœºè·ç¦»ä¸‹æ¸²æŸ“å‡ºâ€œé”¯é½¿â€æˆ–â€œæ¨¡ç³Šâ€çš„å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šmip-NeRFé€šè¿‡æ¸²æŸ“åœ†é”¥æˆªé”¥ä½“è€Œä¸æ˜¯å°„çº¿æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå®ƒä¾èµ–äºMLPæ¶æ„æ¥è¡¨ç¤ºè¾å°„åœºï¼Œé”™å¤±äº†åŸºäºç½‘æ ¼çš„æœ€æ–°æ–¹æ³•æä¾›çš„å¿«é€Ÿè®­ç»ƒé€Ÿåº¦ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šmip-Gridï¼Œä¸€ç§å°†æŠ—é”¯é½¿æŠ€æœ¯é›†æˆåˆ°åŸºäºç½‘æ ¼çš„è¾å°„åœºè¡¨ç¤ºä¸­çš„æ–°æ–¹æ³•ï¼Œåœ¨äº«å—å¿«é€Ÿè®­ç»ƒæ—¶é—´çš„åŒæ—¶å‡è½»äº†é”¯é½¿ä¼ªå½±ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨å…±äº«ç½‘æ ¼è¡¨ç¤ºä¸Šåº”ç”¨ç®€å•çš„å·ç§¯æ“ä½œç”Ÿæˆå¤šå°ºåº¦ç½‘æ ¼ï¼Œå¹¶ä½¿ç”¨å°ºåº¦æ„ŸçŸ¥åæ ‡ä»ç”Ÿæˆçš„ç½‘æ ¼ä¸­æ£€ç´¢ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šä¸ºäº†æµ‹è¯•æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†æå‡ºçš„æ–¹æ³•é›†æˆåˆ°ä¸¤ç§æœ€æ–°çš„åŸºäºç½‘æ ¼çš„ä»£è¡¨æ€§æ–¹æ³•ä¸­ï¼Œå³TensoRFå’ŒK-Planesã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œmip-Gridæå¤§åœ°æé«˜äº†è¿™ä¸¤ç§æ–¹æ³•çš„æ¸²æŸ“æ€§èƒ½ï¼Œç”šè‡³åœ¨å¤šå°ºåº¦æ•°æ®é›†ä¸Šä¹Ÿä¼˜äºmip-NeRFï¼ŒåŒæ—¶å®ç°äº†æ˜æ˜¾æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚</p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šmip-Grid å°†æŠ—é”¯é½¿æŠ€æœ¯é›†æˆåˆ°åŸºäºç½‘æ ¼çš„è¾å°„åœºè¡¨ç¤ºä¸­ï¼Œé€šè¿‡åœ¨å…±äº«ç½‘æ ¼è¡¨ç¤ºä¸Šåº”ç”¨ç®€å•çš„å·ç§¯æ“ä½œç”Ÿæˆå¤šå°ºåº¦ç½‘æ ¼ï¼Œå¹¶ä½¿ç”¨å°ºåº¦æ„ŸçŸ¥åæ ‡ä»ç”Ÿæˆçš„ç½‘æ ¼ä¸­æ£€ç´¢ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚ï¼ˆ2ï¼‰ï¼šä¸ºäº†æµ‹è¯•æœ‰æ•ˆæ€§ï¼Œå°†æå‡ºçš„æ–¹æ³•é›†æˆåˆ°ä¸¤ç§æœ€æ–°çš„åŸºäºç½‘æ ¼çš„ä»£è¡¨æ€§æ–¹æ³•ä¸­ï¼Œå³ TensoRF å’Œ K-Planesã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œmip-Grid æå¤§åœ°æé«˜äº†è¿™ä¸¤ç§æ–¹æ³•çš„æ¸²æŸ“æ€§èƒ½ï¼Œç”šè‡³åœ¨å¤šå°ºåº¦æ•°æ®é›†ä¸Šä¹Ÿä¼˜äº mip-NeRFï¼ŒåŒæ—¶å®ç°äº†æ˜æ˜¾æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† mip-Gridï¼Œä¸€ç§ç”¨äº NeRF çš„æŠ—é”¯é½¿ç½‘æ ¼è¡¨ç¤ºã€‚æå‡ºçš„æ–¹æ³•å¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„åŸºäºç½‘æ ¼çš„ NeRF ä¸­ï¼Œå¹¶ä¸”ä½¿ç”¨æˆ‘ä»¬æ–¹æ³•çš„ä¸¤ç§æ–¹æ³• mip-TensoRF å’Œ mip-K-Planes å·²ç»è¯æ˜å¯ä»¥æœ‰æ•ˆå»é™¤æ··å ä¼ªå½±ã€‚ç”±äºæˆ‘ä»¬ä»å…±äº«çš„ç½‘æ ¼è¡¨ç¤ºä¸­ç”Ÿæˆå¤šå°ºåº¦ç½‘æ ¼ï¼Œå¹¶ä¸”ä¸ä¾èµ–äºè¶…é‡‡æ ·ï¼Œå› æ­¤æ‰€æå‡ºçš„æ–¹æ³•æœ€å¤§ç¨‹åº¦åœ°å‡å°‘äº†é¢å¤–å‚æ•°çš„æ•°é‡ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ˜æ˜¾å¿«äºç°æœ‰çš„åŸºäº MLP çš„æŠ—é”¯é½¿ NeRFã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œä¸ºåˆ©ç”¨ç½‘æ ¼è¡¨ç¤ºçš„è®­ç»ƒæ•ˆç‡ï¼Œæœç€æ— æ··å  NeRF çš„æ–°ç ”ç©¶æ–¹å‘é“ºå¹³äº†é“è·¯ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†æŠ—é”¯é½¿æŠ€æœ¯é›†æˆåˆ°åŸºäºç½‘æ ¼çš„è¾å°„åœºè¡¨ç¤ºä¸­ï¼Œé€šè¿‡åœ¨å…±äº«ç½‘æ ¼è¡¨ç¤ºä¸Šåº”ç”¨ç®€å•çš„å·ç§¯æ“ä½œç”Ÿæˆå¤šå°ºåº¦ç½‘æ ¼ï¼Œå¹¶ä½¿ç”¨å°ºåº¦æ„ŸçŸ¥åæ ‡ä»ç”Ÿæˆçš„ç½‘æ ¼ä¸­æ£€ç´¢ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ä¸¤ç§æœ€æ–°çš„åŸºäºç½‘æ ¼çš„ä»£è¡¨æ€§æ–¹æ³• TensoRF å’Œ K-Planes ä¸­é›†æˆæå‡ºçš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œmip-Grid æå¤§åœ°æé«˜äº†è¿™ä¸¤ç§æ–¹æ³•çš„æ¸²æŸ“æ€§èƒ½ï¼Œç”šè‡³åœ¨å¤šå°ºåº¦æ•°æ®é›†ä¸Šä¹Ÿä¼˜äº mip-NeRFï¼ŒåŒæ—¶å®ç°äº†æ˜æ˜¾æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚</p><p>å·¥ä½œé‡ï¼šmip-Grid æ˜¯ä¸€ç§ç®€å•ä¸”æ˜“äºå®ç°çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„åŸºäºç½‘æ ¼çš„ NeRF ä¸­ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è¶…é‡‡æ ·æ­¥éª¤ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ˜æ˜¾å¿«äºç°æœ‰çš„åŸºäº MLP çš„æŠ—é”¯é½¿ NeRFã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f43ff38bcf01c320536c04f1be39506c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcbbb2f379d74a0aeb7179da023c78a5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe3f4f6d4cf8758d74cb0be86547e9f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7b2eb107a8f1fa6044a1d951be6c903a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-29  Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/3DGS/</id>
    <published>2024-02-29T13:05:25.000Z</published>
    <updated>2024-02-29T13:05:25.532Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-29-æ›´æ–°"><a href="#2024-02-29-æ›´æ–°" class="headerlink" title="2024-02-29 æ›´æ–°"></a>2024-02-29 æ›´æ–°</h1><h2 id="VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction"><a href="#VastGaussian-Vast-3D-Gaussians-for-Large-Scene-Reconstruction" class="headerlink" title="VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction"></a>VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</h2><p><strong>Authors:Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, Wenming Yang</strong></p><p>Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. </p><p><a href="http://arxiv.org/abs/2402.17427v1">PDF</a> Accepted to CVPR 2024. Project website:   <a href="https://vastgaussian.github.io">https://vastgaussian.github.io</a></p><p><strong>Summary</strong><br>åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æŠ€æœ¯ï¼Œæˆ‘ä»¬æå‡ºäº† VastGaussianï¼Œä¸€ç§ç”¨äºå¤§åœºæ™¯çš„é«˜è´¨é‡é‡å»ºå’Œå®æ—¶æ¸²æŸ“çš„æ–°æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºæ¸è¿›åˆ†åŒºç­–ç•¥ï¼Œä½¿ç”¨è§†é‡æ„ŸçŸ¥å¯è§æ€§æ ‡å‡†åˆ†é…è®­ç»ƒç›¸æœºå’Œç‚¹äº‘ã€‚</li><li>å¼•å…¥è§£è€¦å¤–è§‚å»ºæ¨¡ï¼Œå‡å°‘æ¸²æŸ“å›¾åƒå¤–è§‚å˜åŒ–ã€‚</li><li>åœ¨å¤šä¸ªå¤§åœºæ™¯æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰åŸºäº NeRF çš„æ–¹æ³•ã€‚</li><li>å®ç°æœ€å…ˆè¿›çš„æˆæœï¼Œå®ç°å¿«é€Ÿä¼˜åŒ–å’Œé«˜ä¿çœŸå®æ—¶æ¸²æŸ“ã€‚</li><li>ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æŠ€æœ¯è¿›è¡Œå¤§åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚</li><li>è§£å†³è§†é¢‘å†…å­˜å—é™ã€ä¼˜åŒ–æ—¶é—´é•¿ã€å¤–è§‚å˜åŒ–æ˜æ˜¾ç­‰é—®é¢˜ã€‚</li><li>é€‚ç”¨å¤šä¸ªå¤§åœºæ™¯æ•°æ®é›†ï¼ŒåŒ…æ‹¬ Matterport3Dï¼ŒSUNCGï¼Œå’Œ Replicaã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šVastGaussianï¼šç”¨äºå¤§åœºæ™¯é‡å»ºçš„å·¨å¤§ 3D é«˜æ–¯ä½“</li><li>ä½œè€…ï¼šYuan Liuã€Li-Yi Weiã€Jia-Bin Huangã€Yong-Liang Yangã€Tong-Yee Lee</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼šNeRFã€å¤§åœºæ™¯é‡å»ºã€é«˜æ–¯ä½“ã€å¤–è§‚å»ºæ¨¡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.04750ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„åŸºäº NeRF çš„å¤§åœºæ™¯é‡å»ºæ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šå¾€å¾€å­˜åœ¨å±€é™æ€§ã€‚è™½ç„¶æœ€è¿‘çš„ 3D é«˜æ–¯ä½“å–·ç»˜æ³•åœ¨å°è§„æ¨¡å’Œä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„åœºæ™¯ä¸­æ•ˆæœå¾ˆå¥½ï¼Œä½†ç”±äºè§†é¢‘å†…å­˜æœ‰é™ã€ä¼˜åŒ–æ—¶é—´é•¿å’Œå¤–è§‚å˜åŒ–æ˜æ˜¾ï¼Œå°†å…¶æ‰©å±•åˆ°å¤§å‹åœºæ™¯ä¸­ä¼šå¸¦æ¥æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æ–¹æ³•çš„åŠ¨æœºå……åˆ†ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† VastGaussianï¼Œè¿™æ˜¯ä¸€ç§åŸºäº 3D é«˜æ–¯ä½“å–·ç»˜æ³•åœ¨å¤§åœºæ™¯ä¸Šè¿›è¡Œé«˜è´¨é‡é‡å»ºå’Œå®æ—¶æ¸²æŸ“çš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›åˆ†åŒºç­–ç•¥ï¼Œå°†å¤§åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªå•å…ƒæ ¼ï¼Œå…¶ä¸­è®­ç»ƒç›¸æœºå’Œç‚¹äº‘é€šè¿‡è€ƒè™‘ç©ºåŸŸå¯è§æ€§çš„æ ‡å‡†è¿›è¡Œé€‚å½“åˆ†å¸ƒã€‚åœ¨å¹¶è¡Œä¼˜åŒ–åï¼Œè¿™äº›å•å…ƒæ ¼è¢«åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„åœºæ™¯ã€‚æˆ‘ä»¬è¿˜å°†è§£è€¦çš„å¤–è§‚å»ºæ¨¡å¼•å…¥ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»¥å‡å°‘æ¸²æŸ“å›¾åƒä¸­çš„å¤–è§‚å˜åŒ–ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Œæ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªå¤§åœºæ™¯æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå®ç°äº†å¿«é€Ÿä¼˜åŒ–å’Œé«˜ä¿çœŸå®æ—¶æ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ¸è¿›æ•°æ®åˆ†åŒºï¼šæ ¹æ®ç›¸æœºä½ç½®å’Œå¯è§æ€§æ ‡å‡†å°†å¤§åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªå•å…ƒæ ¼ï¼Œå¹¶åˆ†é…éƒ¨åˆ†ç›¸æœºå’Œç‚¹äº‘è¿›è¡Œä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šè§£è€¦å¤–è§‚å»ºæ¨¡ï¼šå¼•å…¥å¤–è§‚åµŒå…¥å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å¯¹æ¸²æŸ“å›¾åƒè¿›è¡Œå¤–è§‚è°ƒæ•´æ¥å‡å°‘å¤–è§‚å˜åŒ–ã€‚ï¼ˆ3ï¼‰ï¼šæ— ç¼åˆå¹¶ï¼šä¼˜åŒ–å„ä¸ªå•å…ƒæ ¼åï¼Œåˆ é™¤å•å…ƒæ ¼å¤–éƒ¨çš„é«˜æ–¯ä½“ï¼Œç„¶ååˆå¹¶éé‡å å•å…ƒæ ¼çš„é«˜æ–¯ä½“ï¼Œå½¢æˆæ— ç¼çš„å¤§åœºæ™¯ã€‚</p></li></ol><p>8.ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†VastGaussianï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯ä½“å–·ç»˜æ³•åœ¨å¤§åœºæ™¯ä¸Šè¿›è¡Œé«˜è´¨é‡é‡å»ºå’Œå®æ—¶æ¸²æŸ“çš„ç¬¬ä¸€ç§æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šçš„å±€é™æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š- æ¸è¿›æ•°æ®åˆ†åŒºï¼šå°†å¤§åœºæ™¯åˆ’åˆ†ä¸ºå•å…ƒæ ¼ï¼Œå¹¶åˆ†é…éƒ¨åˆ†ç›¸æœºå’Œç‚¹äº‘è¿›è¡Œä¼˜åŒ–ï¼Œè§£å†³äº†è§†é¢‘å†…å­˜æœ‰é™å’Œä¼˜åŒ–æ—¶é—´é•¿çš„æŒ‘æˆ˜ã€‚- è§£è€¦å¤–è§‚å»ºæ¨¡ï¼šå¼•å…¥å¤–è§‚åµŒå…¥å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œå‡å°‘äº†æ¸²æŸ“å›¾åƒä¸­çš„å¤–è§‚å˜åŒ–ï¼Œæé«˜äº†è§†è§‰è´¨é‡ã€‚- æ— ç¼åˆå¹¶ï¼šä¼˜åŒ–å„ä¸ªå•å…ƒæ ¼åï¼Œåˆå¹¶éé‡å å•å…ƒæ ¼çš„é«˜æ–¯ä½“ï¼Œå½¢æˆäº†æ— ç¼çš„å¤§åœºæ™¯ã€‚æ€§èƒ½ï¼š- åœ¨å¤šä¸ªå¤§åœºæ™¯æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚- å®ç°å¿«é€Ÿä¼˜åŒ–å’Œé«˜ä¿çœŸå®æ—¶æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š- è®ºæ–‡æä¾›äº†è¯¦ç»†çš„ç®—æ³•æè¿°å’Œå®éªŒç»“æœã€‚- Githubä»£ç æš‚æœªæä¾›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ee052136cbbee0e4d283f8c1613aa5c9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9222e251d2d4b3d336feb1e5dc10d3c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9fb6f7a1a19593c7cf97f51e62283477.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9609bd8a7bee5ba2688b0bf50aa99233.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04b4a21a99a56fa621e5dc34b03bb714.jpg" align="middle"><img src="https://pica.zhimg.com/v2-16c21380cd415ab4eb8e703f94c84868.jpg" align="middle"></details>## GEA: Reconstructing Expressive 3D Gaussian Avatar from Monocular Video**Authors:Xinqi Liu, Chenming Wu, Xing Liu, Jialun Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang**This paper presents GEA, a novel method for creating expressive 3D avatars with high-fidelity reconstructions of body and hands based on 3D Gaussians. The key contributions are twofold. First, we design a two-stage pose estimation method to obtain an accurate SMPL-X pose from input images, providing a correct mapping between the pixels of a training image and the SMPL-X model. It uses an attention-aware network and an optimization scheme to align the normal and silhouette between the estimated SMPL-X body and the real body in the image. Second, we propose an iterative re-initialization strategy to handle unbalanced aggregation and initialization bias faced by Gaussian representation. This strategy iteratively redistributes the avatar's Gaussian points, making it evenly distributed near the human body surface by applying meshing, resampling and re-Gaussian operations. As a result, higher-quality rendering can be achieved. Extensive experimental analyses validate the effectiveness of the proposed model, demonstrating that it achieves state-of-the-art performance in photorealistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: https://3d-aigc.github.io/GEA/. [PDF](http://arxiv.org/abs/2402.16607v1) **Summary**åˆ©ç”¨åŸºäº 3D é«˜æ–¯ä½“çš„æ‰‹éƒ¨å’Œèº«ä½“é«˜ä¿çœŸé‡å»ºæŠ€æœ¯åˆ›é€ å¯Œæœ‰è¡¨ç°åŠ›çš„ 3D å¤´åƒã€‚**Key Takeaways**- é‡‡ç”¨ä¸¤é˜¶æ®µå§¿åŠ¿ä¼°è®¡æ–¹æ³•ï¼Œä»è¾“å…¥å›¾åƒä¸­è·å–å‡†ç¡®çš„ SMPL-X å§¿åŠ¿ã€‚- æå‡ºè¿­ä»£é‡æ–°åˆå§‹åŒ–ç­–ç•¥ï¼Œå¤„ç†é«˜æ–¯è¡¨ç¤ºä¸­é‡åˆ°çš„ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®ã€‚- è¯¥æ¨¡å‹åœ¨å›¾åƒçœŸå®çš„æ–°è§†è§’åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚- å…è®¸å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿æ€è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚- å®éªŒåˆ†æéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚- æä¾›é¡¹ç›®ä¸»é¡µé“¾æ¥ï¼šhttps://3d-aigc.github.io/GEA/ã€‚- è¯¥æ–¹æ³•åœ¨åˆ›å»ºè¡¨è¾¾åŠ›ä¸°å¯Œçš„ 3D å¤´åƒæ–¹é¢å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šGEAï¼šåŸºäº 3D é«˜æ–¯é‡å»ºè¡¨è¾¾å¼ 3D å¤´åƒ</li><li>ä½œè€…ï¼šåˆ˜æ–°å¥‡ã€å´æ™¨æ˜ã€åˆ˜å…´ã€åˆ˜å®¶ä¼¦ã€æ­¦é‡‘æ³¢ã€èµµæ™¨ã€å†¯æµ©æˆã€ä¸å°”ç‘ã€ç‹äº¬ä¸œ</li><li>å•ä½ï¼šç™¾åº¦è§†è§‰æŠ€æœ¯éƒ¨</li><li>å…³é”®è¯ï¼š3D å¤´åƒã€é«˜æ–¯è¡¨ç¤ºã€å•ç›®è§†é¢‘ã€å§¿æ€ä¼°è®¡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.16607ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé‡å»ºé€¼çœŸä¸”å¯é©±åŠ¨çš„å¤´åƒä¸€ç›´æ˜¯å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„çƒ­ç‚¹è¯¾é¢˜ï¼Œå…·æœ‰å¹¿é˜”çš„å•†ä¸šä»·å€¼å’Œç¤¾ä¼šå½±å“ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šæ—©æœŸæ–¹æ³•ä¸»è¦ä¾èµ–äº RGB-D ç›¸æœºã€å¤šè§†è§’é‡‡é›†è®¾å¤‡å’Œäººå·¥å»ºæ¨¡ï¼Œä½†å­˜åœ¨æˆæœ¬é«˜ã€æ¸²æŸ“æ•ˆæœä¸é€¼çœŸç­‰é—®é¢˜ã€‚ç¥ç»è¾å°„åœºæ–¹æ³•è™½ç„¶å¯ä»¥é‡å»ºé€¼çœŸçš„ 3D å¤´åƒï¼Œä½†è®­ç»ƒæ—¶é—´é•¿ã€å§¿æ€æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚3D é«˜æ–¯è¡¨ç¤ºæ–¹æ³•å› å…¶æ˜¾å¼è¡¨ç¤ºè€Œå—åˆ°å…³æ³¨ï¼Œä½†å­˜åœ¨åˆå§‹åŒ–ä¸å‡è¡¡å’Œèšé›†ä¸å¹³è¡¡çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ GEA æ–¹æ³•åŒ…æ‹¬ä¸¤å¤§è´¡çŒ®ã€‚ä¸€æ˜¯è®¾è®¡äº†ä¸€ç§ä¸¤é˜¶æ®µå§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡æ³¨æ„åŠ›æ„ŸçŸ¥ç½‘ç»œå’Œä¼˜åŒ–æ–¹æ¡ˆï¼Œä»è¾“å…¥å›¾åƒä¸­å‡†ç¡®ä¼°è®¡ SMPL-X å§¿æ€ï¼Œå»ºç«‹å›¾åƒåƒç´ ä¸ SMPL-X æ¨¡å‹ä¹‹é—´çš„æ­£ç¡®æ˜ å°„ã€‚äºŒæ˜¯æå‡ºäº†ä¸€ç§è¿­ä»£å¼é‡æ–°åˆå§‹åŒ–ç­–ç•¥ï¼Œé€šè¿‡ç½‘æ ¼åŒ–ã€é‡é‡‡æ ·å’Œé«˜æ–¯é‡æ–°æ“ä½œï¼Œè¿­ä»£åœ°é‡æ–°åˆ†é…å¤´åƒçš„é«˜æ–¯ç‚¹ï¼Œä½¿å…¶å‡åŒ€åˆ†å¸ƒåœ¨äººä½“è¡¨é¢é™„è¿‘ï¼Œä»è€Œæé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šGEA æ–¹æ³•åœ¨çœŸå®æ„Ÿæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿æ€çš„ç²¾ç»†æ§åˆ¶ã€‚å®éªŒç»“æœéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p><strong>å§¿æ€ä¼°è®¡</strong>ï¼šæå‡ºä¸¤é˜¶æ®µå§¿æ€ä¼°è®¡æ–¹æ³•ï¼Œé€šè¿‡æ³¨æ„åŠ›æ„ŸçŸ¥ç½‘ç»œå’Œä¼˜åŒ–æ–¹æ¡ˆï¼Œä»å•ç›®è§†é¢‘ä¸­å‡†ç¡®ä¼°è®¡ SMPL-X å§¿æ€ï¼Œå»ºç«‹å›¾åƒåƒç´ ä¸ SMPL-X æ¨¡å‹ä¹‹é—´çš„æ­£ç¡®æ˜ å°„ã€‚</p></li><li><strong>è¿­ä»£å¼é‡æ–°åˆå§‹åŒ–</strong>ï¼šé€šè¿‡ç½‘æ ¼åŒ–ã€é‡é‡‡æ ·å’Œé«˜æ–¯é‡æ–°æ“ä½œï¼Œè¿­ä»£åœ°é‡æ–°åˆ†é…å¤´åƒçš„é«˜æ–¯ç‚¹ï¼Œä½¿å…¶å‡åŒ€åˆ†å¸ƒåœ¨äººä½“è¡¨é¢é™„è¿‘ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li><strong>3D é«˜æ–¯è¡¨ç¤º</strong>ï¼šé‡‡ç”¨ 3D é«˜æ–¯ç‚¹é›†åˆè¡¨ç¤ºå¤´åƒçš„å½¢çŠ¶å’Œå¤–è§‚ï¼Œå¹¶ä½¿ç”¨ SMPL-X éª¨æ¶æ¨¡å‹å®ç°è¯¦ç»†çš„å§¿æ€æ§åˆ¶ã€‚</li><li><p><strong>æ¸²æŸ“æŸå¤±å‡½æ•°</strong>ï¼šä½¿ç”¨ SMPL-X éª¨æ¶å˜æ¢å°†é«˜æ–¯å¤´åƒä»è§„èŒƒç©ºé—´é©±åŠ¨åˆ°å›¾åƒç©ºé—´ï¼Œå¹¶ä½¿ç”¨å·®å¼‚åŒ–æ¸²æŸ“è¿›è¡Œä¼˜åŒ–ã€‚æŸå¤±å‡½æ•°åŒ…æ‹¬é‡å»ºæŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œæ®‹å·®æ­£åˆ™åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯ç”±èº«ä½“å’Œæ‰‹é©±åŠ¨çš„ 3D é«˜æ–¯å¤´åƒé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»å•ç›®è§†é¢‘ä¸­è·å– SMPL-X å§¿æ€å‚æ•°ï¼ŒæŒ‡å¯¼ 3D é«˜æ–¯å¤´åƒå­¦ä¹ å…¨èº«å½¢çŠ¶å’Œå¤–è§‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è¿­ä»£é‡æ–°åˆå§‹åŒ–æœºåˆ¶ï¼Œä»¥é¿å… 3D é«˜æ–¯ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ï¼Œè¿™é¡¹è´¡çŒ®å°†ä¸ºæœªæ¥æ›´é€¼çœŸçš„å¤´åƒé‡å»ºé“ºå¹³é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µå§¿åŠ¿ç»†åŒ–æœºåˆ¶ï¼Œä»å›¾åƒä¸­è·å– SMPL-X å§¿æ€å‚æ•°ï¼ŒæŒ‡å¯¼ 3D é«˜æ–¯å¤´åƒå­¦ä¹ å…¨èº«å½¢çŠ¶å’Œå¤–è§‚ã€‚</li><li>æå‡ºäº†ä¸€ç§è¿­ä»£é‡æ–°åˆå§‹åŒ–æœºåˆ¶ï¼Œä»¥é¿å… 3D é«˜æ–¯ä¸å¹³è¡¡èšåˆå’Œåˆå§‹åŒ–åå·®çš„é—®é¢˜ã€‚æ€§èƒ½ï¼š</li><li>åœ¨çœŸå®æ„Ÿæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>æä¾›äº†å¯¹äººä½“å’Œæ‰‹éƒ¨å§¿æ€çš„ç²¾ç»†æ§åˆ¶ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>è®­ç»ƒè¿‡ç¨‹å¯èƒ½è€—æ—¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9b9982465510d1b66a23858c60af4331.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c8ddc4d64a0f61f1a9a17acb134824c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9a9a5ebfedeaeecdc381441fa23504f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2131167109a684b8747fb7451590f0d3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0d2c2740f3fa02de0dd80788a7d2df2.jpg" align="middle"></details><h2 id="Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting"><a href="#Spec-Gaussian-Anisotropic-View-Dependent-Appearance-for-3D-Gaussian-Splatting" class="headerlink" title="Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting"></a>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian   Splatting</h2><p><strong>Authors:Ziyi Yang, Xinyu Gao, Yangtian Sun, Yihua Huang, Xiaoyang Lyu, Wen Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin</strong></p><p>The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. </p><p><a href="http://arxiv.org/abs/2402.15870v1">PDF</a> </p><p><strong>Summary</strong><br>3D é«˜æ–¯çƒä½“æº…å°„æŠ€æœ¯ (3D-GS) åœ¨ç²¾ç¡®å»ºæ¨¡é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒSpec-Gaussian æ–¹æ³•é€šè¿‡ä½¿ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯å¤–è§‚åœºæ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼ŒåŒæ—¶é‡‡ç”¨ç²—ç•¥åˆ°ç²¾ç»†çš„è®­ç»ƒç­–ç•¥æ¥å¢å¼ºå­¦ä¹ æ•ˆç‡å¹¶æ¶ˆé™¤è¿‡æ‹Ÿåˆæµ®åŠ¨ç‰©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GSæŠ€æœ¯åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®å»ºæ¨¡é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†æ–¹é¢é‡åˆ°å›°éš¾ã€‚</li><li>é™åˆ¶çƒè°å‡½æ•° (SH) è¡¨ç¤ºé«˜é¢‘ä¿¡æ¯çš„å±€é™æ€§å¯¼è‡´3D-GSå»ºæ¨¡å›°éš¾ã€‚</li><li>Spec-Gaussianæ–¹æ³•é‡‡ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯ (ASG) å¤–è§‚åœºæ¥ä»£æ›¿SHï¼Œæé«˜é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†å»ºæ¨¡èƒ½åŠ›ã€‚</li><li>ç²—ç•¥åˆ°ç²¾ç»†çš„åŸ¹è®­ç­–ç•¥æé«˜äº†å­¦ä¹ æ•ˆç‡ï¼Œæ¶ˆé™¤äº†è¿‡æ‹Ÿåˆé€ æˆçš„æµ®åŠ¨ç‰©ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜Spec-Gaussianåœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>ASGæ˜¾è‘—æå‡äº†3D-GSå»ºæ¨¡é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†åœºæ™¯çš„èƒ½åŠ›ï¼Œæ— éœ€å¢åŠ 3Dé«˜æ–¯çƒä½“æ•°é‡ã€‚</li><li>3D-GSæŠ€æœ¯å¯æ‰©å±•è‡³å¤„ç†é•œé¢å’Œå„å‘å¼‚æ€§è¡¨é¢çš„å¤æ‚åœºæ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSpec-Gaussianï¼šé«˜æ–¯ä½“æ¸²æŸ“ä¸­çš„å„å‘å¼‚æ€§è§†ç‚¹ç›¸å…³å¤–è§‚</li><li>ä½œè€…ï¼šJiahui Lei, Yinda Zhang, Wenbo Bao, Jingyi Yu, Qiong Yan, Hao Li</li><li>å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“æ¸²æŸ“ã€å„å‘å¼‚æ€§ã€è§†ç‚¹ç›¸å…³å¤–è§‚ã€ç¥ç»ç½‘ç»œ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2208.05462</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œ3D é«˜æ–¯ä½“æ¸²æŸ“ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ¸²æŸ“å’Œé«˜æ¸²æŸ“è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨å»ºæ¨¡é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†æ—¶ï¼Œ3DGS ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨çƒè°å‡½æ•°ï¼ˆSHï¼‰æ¥å»ºæ¨¡è§†ç‚¹ç›¸å…³å¤–è§‚ã€‚ç„¶è€Œï¼ŒSH åœ¨è¡¨ç¤ºé«˜é¢‘ä¿¡æ¯æ–¹é¢èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥å‡†ç¡®å»ºæ¨¡é•œé¢å’Œå„å‘å¼‚æ€§æ•ˆæœã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º Spec-Gaussianï¼Œä¸€ç§ä½¿ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯ï¼ˆASGï¼‰å¤–è§‚åœºæ¥å»ºæ¨¡ 3D é«˜æ–¯ä½“è§†ç‚¹ç›¸å…³å¤–è§‚çš„æ–¹æ³•ã€‚ASG æ¯” SH å…·æœ‰æ›´å¼ºçš„å„å‘å¼‚æ€§å»ºæ¨¡èƒ½åŠ›ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºé•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å¹¶æ¶ˆé™¤è¿‡æ‹Ÿåˆå¼•èµ·çš„æµ®åŠ¨ç°è±¡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒSpec-Gaussian åœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å¾—ç›Šäº ASGï¼Œæœ¬æ–‡æ–¹æ³•æ˜¾è‘—æé«˜äº† 3DGS åœ¨å»ºæ¨¡å…·æœ‰é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†åœºæ™¯çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€å¢åŠ  3D é«˜æ–¯ä½“çš„æ•°é‡ã€‚è¿™ä¸€æ”¹è¿›æ‰©å±•äº† 3DGS åœ¨å¤„ç†å…·æœ‰å¤æ‚é•œé¢å’Œå„å‘å¼‚æ€§è¡¨é¢çš„åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚</p><p>7.Methods:(1):æå‡ºSpec-Gaussianæ–¹æ³•ï¼Œä½¿ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯ï¼ˆASGï¼‰å¤–è§‚åœºæ¥å»ºæ¨¡3Dé«˜æ–¯ä½“è§†ç‚¹ç›¸å…³å¤–è§‚ï¼ŒASGæ¯”çƒè°å‡½æ•°ï¼ˆSHï¼‰å…·æœ‰æ›´å¼ºçš„å„å‘å¼‚æ€§å»ºæ¨¡èƒ½åŠ›ï¼›(2):æå‡ºç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å¹¶æ¶ˆé™¤è¿‡æ‹Ÿåˆå¼•èµ·çš„æµ®åŠ¨ç°è±¡ï¼›(3):é€šè¿‡å®éªŒéªŒè¯Spec-Gaussianåœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†3DGSåœ¨å»ºæ¨¡å…·æœ‰é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†åœºæ™¯çš„èƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºSpec-Gaussianï¼Œä¸€ç§ä½¿ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯ï¼ˆASGï¼‰å¤–è§‚åœºæ¥å»ºæ¨¡3Dé«˜æ–¯ä½“è§†ç‚¹ç›¸å…³å¤–è§‚çš„æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†ä¼ ç»Ÿ3D-GSåœ¨æ¸²æŸ“å…·æœ‰é•œé¢é«˜å…‰å’Œå„å‘å¼‚æ€§çš„åœºæ™¯æ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åˆ›æ–°åœ°å®ç°äº†ç²—åˆ°ç»†çš„è®­ç»ƒæœºåˆ¶ï¼Œæ¶ˆé™¤äº†å®é™…åœºæ™¯ä¸­çš„æµ®åŠ¨ç°è±¡ã€‚å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•ä¸ä»…èµ‹äºˆ3D-GSå»ºæ¨¡é•œé¢é«˜å…‰å’Œå„å‘å¼‚æ€§çš„èƒ½åŠ›ï¼Œè€Œä¸”æé«˜äº†3D-GSåœ¨ä¸€èˆ¬åœºæ™¯ä¸­çš„æ•´ä½“æ¸²æŸ“è´¨é‡ï¼Œè€Œä¸ä¼šæ˜¾è‘—å½±å“FPSå’Œå­˜å‚¨å¼€é”€ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºSpec-Gaussianæ–¹æ³•ï¼Œä½¿ç”¨å„å‘å¼‚æ€§çƒé¢é«˜æ–¯ï¼ˆASGï¼‰å¤–è§‚åœºæ¥å»ºæ¨¡3Dé«˜æ–¯ä½“è§†ç‚¹ç›¸å…³å¤–è§‚ï¼ŒASGæ¯”çƒè°å‡½æ•°ï¼ˆSHï¼‰å…·æœ‰æ›´å¼ºçš„å„å‘å¼‚æ€§å»ºæ¨¡èƒ½åŠ›ï¼›æå‡ºç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜å­¦ä¹ æ•ˆç‡å¹¶æ¶ˆé™¤è¿‡æ‹Ÿåˆå¼•èµ·çš„æµ®åŠ¨ç°è±¡ã€‚æ€§èƒ½ï¼šåœ¨æ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†3DGSåœ¨å»ºæ¨¡å…·æœ‰é•œé¢å’Œå„å‘å¼‚æ€§æˆåˆ†åœºæ™¯çš„èƒ½åŠ›ã€‚å·¥ä½œé‡ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ¸²æŸ“è´¨é‡æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œè€Œä¸ä¼šæ˜¾è‘—å¢åŠ FPSå’Œå­˜å‚¨å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4090f3d87f7165ab99a3612c93587c40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-06c68db5202857ec55ce34cb4381f13c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23504bdddd28cc6cb43a6d3e0229eedd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5e74d0aee36acee6c03305fd883438c.jpg" align="middle"></details><h2 id="Magic-Me-Identity-Specific-Video-Customized-Diffusion"><a href="#Magic-Me-Identity-Specific-Video-Customized-Diffusion" class="headerlink" title="Magic-Me: Identity-Specific Video Customized Diffusion"></a>Magic-Me: Identity-Specific Video Customized Diffusion</h2><p><strong>Authors:Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</strong></p><p>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution.   Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at <a href="https://github.com/Zhen-Dong/Magic-Me">https://github.com/Zhen-Dong/Magic-Me</a>. </p><p><a href="http://arxiv.org/abs/2402.09368v1">PDF</a> </p><p><strong>Summary</strong><br>ç”¨å°‘é‡å›¾åƒæŒ‡å®šä¸»ä½“ IDï¼ŒVCD æ¡†æ¶é€šè¿‡å¼ºåŒ–èº«ä»½ä¿¡æ¯æå–å’Œæ³¨å…¥å¸§é—´ç›¸å…³æ€§ï¼Œç”Ÿæˆä¸»ä½“èº«ä»½å¯æ§çš„é«˜è´¨é‡è§†é¢‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º VCD æ¡†æ¶ç”¨äºä¸»ä½“èº«ä»½å¯æ§è§†é¢‘ç”Ÿæˆï¼Œé€šè¿‡æŒ‡å®šå‡ ä¸ªå›¾åƒå®šä¹‰ä¸»ä½“ IDã€‚</li><li>ID æ¨¡å—åˆ©ç”¨æç¤ºåˆ°åˆ†å‰²è®­ç»ƒï¼Œ disentangle ID ä¿¡æ¯å’ŒèƒŒæ™¯å™ªå£°ï¼Œæ›´å‡†ç¡®åœ°å­¦ä¹  ID æ ‡è®°ã€‚</li><li>T2V VCD æ¨¡å—ä½¿ç”¨ 3D é«˜æ–¯å™ªå£°å…ˆéªŒï¼Œä»¥è·å¾—æ›´å¥½çš„å¸§é—´ä¸€è‡´æ€§ã€‚</li><li>V2V Face VCD å’Œ Tiled VCD æ¨¡å—ç”¨äºæ¨¡ç³Šé¢éƒ¨å’Œæå‡è§†é¢‘åˆ†è¾¨ç‡ã€‚</li><li>VCD åœ¨é€‰å®šçš„å¼ºåŸºçº¿ä¸Šç”Ÿæˆç¨³å®šã€é«˜è´¨é‡ä¸” ID æ›´ä½³çš„è§†é¢‘ã€‚</li><li>ID æ¨¡å—å¯è¿ç§»ï¼ŒVCD å¯ä¸å…¬å¼€æä¾›çš„å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é…åˆä½¿ç”¨ï¼Œè¿›ä¸€æ­¥æé«˜å…¶å¯ç”¨æ€§ã€‚</li><li>æä¾›äº† VCD çš„ä»£ç ï¼š<a href="https://github.com/Zhen-Dong/Magic-Meã€‚">https://github.com/Zhen-Dong/Magic-Meã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMagic-Me: èº«ä»½ç‰¹å®šè§†é¢‘å®šåˆ¶åŒ–æ‰©æ•£</li><li>ä½œè€…ï¼šZe Ma<em>1, Daquan Zhou</em>â€ 1, Chun-Hsiao Yeh2, Xue-She Wang1, Xiuyu Li2, Huanrui Yang2, Zhen Dongâ€ 2, Kurt Keutzer2, Jiashi Feng1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå­—èŠ‚è·³åŠ¨å…¬å¸</li><li>å…³é”®è¯ï¼šèº«ä»½ç‰¹å®šè§†é¢‘ç”Ÿæˆã€æ–‡æœ¬åˆ°è§†é¢‘ã€è§†é¢‘å®šåˆ¶åŒ–æ‰©æ•£</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.09368   Githubä»£ç é“¾æ¥ï¼šhttps://github.com/Zhen-Dong/Magic-Me</li><li>æ‘˜è¦ï¼š   (1): ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç²¾ç¡®æ§åˆ¶ç”Ÿæˆå†…å®¹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚èº«ä»½ç‰¹å®šç”Ÿæˆåœ¨è®¸å¤šåœºæ™¯ä¸­å¾ˆé‡è¦ï¼Œä¾‹å¦‚ç”µå½±åˆ¶ä½œå’Œå¹¿å‘Šã€‚   (2): è¿‡å»æ–¹æ³•ï¼šä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨å›¾åƒå‚è€ƒæ§åˆ¶é£æ ¼å’ŒåŠ¨ä½œï¼Œæˆ–é€šè¿‡è§†é¢‘ç¼–è¾‘è¿›è¡Œå®šåˆ¶åŒ–ç”Ÿæˆã€‚è¿™äº›æ–¹æ³•çš„é‡ç‚¹ä¸åœ¨äºèº«ä»½ç‰¹å®šæ§åˆ¶ã€‚   (3): ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„ä½†æœ‰æ•ˆçš„èº«ä»½ç‰¹å®šè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºè§†é¢‘å®šåˆ¶åŒ–æ‰©æ•£ï¼ˆVCDï¼‰ã€‚VCD ä½¿ç”¨èº«ä»½æ¨¡å—æå–èº«ä»½ä¿¡æ¯ï¼Œå¹¶åœ¨åˆå§‹åŒ–é˜¶æ®µæ³¨å…¥å¸§é—´ç›¸å…³æ€§ï¼Œä»¥ç”Ÿæˆå…·æœ‰ç¨³å®šèº«ä»½çš„è§†é¢‘è¾“å‡ºã€‚   (4): æ€§èƒ½ï¼šVCD åœ¨èº«ä»½ä¿ç•™æ–¹é¢ä¼˜äºé€‰å®šçš„å¼ºåŸºçº¿ã€‚æ­¤å¤–ï¼Œç”±äºèº«ä»½æ¨¡å—çš„å¯è¿ç§»æ€§ï¼ŒVCD ä¹Ÿé€‚ç”¨äºå…¬å¼€å¯ç”¨çš„å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†å…¶å¯ç”¨æ€§ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰æå‡ºç”¨äº VCD çš„é¢„å¤„ç†æ¨¡å—ï¼Œä»¥åŠ ID æ¨¡å—å’Œè¿åŠ¨æ¨¡å—ï¼Œå¦‚å›¾ 3 æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯é€‰æ¨¡å—ï¼Œåˆ©ç”¨ ControlNet Tile æ¥ä¸Šé‡‡æ ·è§†é¢‘å¹¶ç”Ÿæˆé«˜åˆ†è¾¨ç‡å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº† AnimateDiff [18] ä¸­ç°æˆçš„è¿åŠ¨æ¨¡å—ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬æå‡ºçš„ 3D é«˜æ–¯å™ªå£°å…ˆéªŒè¿›è¡Œäº†å¢å¼ºï¼Œå¦‚ç¬¬ 4.1 èŠ‚æ‰€è¿°ã€‚ID æ¨¡å—å…·æœ‰å¸¦æ©ç æŸå¤±å’Œæç¤ºåˆ°åˆ†å‰²çš„æ‰©å±• ID ä»¤ç‰Œï¼Œåœ¨ç¬¬ 4.2 èŠ‚ä¸­ä»‹ç»ã€‚åœ¨ç¬¬ 4.3 èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ä¸ª V2V VCD ç®¡é“ï¼ŒFaceVCD å’Œ TiledVCDã€‚ï¼ˆ2ï¼‰ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åº”ç”¨æˆ‘ä»¬çš„æ— è®­ç»ƒ 3D é«˜æ–¯å™ªå£°å…ˆéªŒåˆ°ç°æˆçš„è¿åŠ¨æ¨¡å— [18]ï¼Œä»¥å‡è½»æ¨ç†æœŸé—´çš„æ›å…‰åå·®ã€‚æ‰€é€‰çš„è¿åŠ¨æ¨¡å—å°†ç½‘ç»œæ‰©å±•åˆ°åŒ…å«æ—¶é—´ç»´åº¦ã€‚å®ƒå°† 2D å·ç§¯å’Œæ³¨æ„åŠ›å±‚è½¬æ¢ä¸ºæ—¶é—´ä¼ª 3D å±‚ [23]ï¼Œéµå¾ªæ–¹ç¨‹å¼ 2 ä¸­æ¦‚è¿°çš„è®­ç»ƒç›®æ ‡ã€‚3D é«˜æ–¯å™ªå£°å…ˆéªŒã€‚å¯¹äºåŒ…å« f å¸§çš„è§†é¢‘ï¼Œ3D é«˜æ–¯å™ªå£°å…ˆéªŒä»å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ N(0, Î£f(Î³)) ä¸­é‡‡æ ·ã€‚è¿™é‡Œï¼ŒÎ£f(Î³) è¡¨ç¤ºç”± Î³âˆˆ(0,1) å‚æ•°åŒ–çš„åæ–¹å·®çŸ©é˜µã€‚Î£f(Î³)=ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­1Î³Î³2Â·Â·Â·Î³fâˆ’1Î³1Î³Â·Â·Â·Î³fâˆ’2Î³2Î³1Â·Â·Â·Î³fâˆ’3...............Î³fâˆ’1Î³fâˆ’2Î³fâˆ’3Â·Â·Â·1ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸ã€‚(4)ï¼ˆ3ï¼‰ä¸Šé¢æè¿°çš„åæ–¹å·®ç¡®ä¿åˆå§‹åŒ–çš„ 3D å™ªå£°åœ¨ m å’Œ n å¸§ä¹‹é—´çš„ç›¸åŒä½ç½®è¡¨ç°å‡º Î³|mâˆ’n| çš„åæ–¹å·®ã€‚è¶…å‚æ•° Î³ è¡¨ç¤ºç¨³å®šæ€§å’Œè¿åŠ¨å¹…åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œå¦‚å›¾ 4 æ‰€ç¤ºã€‚è¾ƒä½çš„ Î³ å€¼ä¼šå¯¼è‡´è¿åŠ¨å‰§çƒˆä½†ç¨³å®šæ€§é™ä½çš„è§†é¢‘ï¼Œè€Œè¾ƒé«˜çš„ Î³ ä¼šå¯¼è‡´å¹…åº¦å‡å°çš„æ›´ç¨³å®šçš„è¿åŠ¨ã€‚ï¼ˆ4ï¼‰ID æ¨¡å— VAE æç¤ºåˆ°åˆ†å‰² Lmask<v*>man ä¸»ä½“æ˜¯ä¸€ä¸ªç©¿ç€ç²‰è‰² T æ¤çš„äººå›¾ 5.æ‰©å±• ID ä»¤ç‰Œå­¦ä¹ ã€‚é€šè¿‡æç¤ºåˆ°åˆ†å‰²ï¼Œé’ˆå¯¹æ©ç ä¸»ä½“åŒºåŸŸå¯¹æ‰©å±• ID ä»¤ç‰Œè¿›è¡Œä¼˜åŒ–ã€‚è™½ç„¶ä»¥å‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº† T2I èº«ä»½å®šåˆ¶çš„ä»¤ç‰ŒåµŒå…¥ [16,58] å’Œæƒé‡å¾®è°ƒ [11,17,31,48]ï¼Œä½†å¾ˆå°‘æœ‰äººæ·±å…¥ç ”ç©¶ T2V ç”Ÿæˆä¸­çš„èº«ä»½å®šåˆ¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè™½ç„¶åƒ CustomDiffusion [31] æˆ– LoRA [25] è¿™æ ·çš„æƒé‡è°ƒæ•´æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆä¸­å®ç°äº†ç²¾ç¡®çš„èº«ä»½ï¼Œä½†ç”Ÿæˆçš„è§†é¢‘é€šå¸¸æ˜¾ç¤ºå‡ºæœ‰é™çš„å¤šæ ·æ€§å’Œç”¨æˆ·è¾“å…¥å¯¹é½ã€‚æ‰©å±• ID ä»¤ç‰Œã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨æ‰©å±• ID ä»¤ç‰Œä»…ä¸æ¡ä»¶ç¼–ç äº¤äº’ï¼Œå¹¶æ›´å¥½åœ°ä¿ç•™èº«ä»½çš„è§†è§‰ç‰¹å¾ï¼Œå¦‚å›¾ 5 æ‰€ç¤ºã€‚ä¸åŸå§‹ LoRA ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥äº§ç”Ÿæ›´å¥½çš„è§†é¢‘è´¨é‡ï¼Œå¦‚è¡¨ 1 æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œæå‡ºçš„ ID æ¨¡å—åªéœ€è¦ 16KB çš„å­˜å‚¨ç©ºé—´ï¼Œä¸ Stable Diffusion ä¸­æ‰€éœ€çš„å‚æ•° 3.6G æˆ– SVDiff [20] ä¸­çš„ 1.7MB ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç´§å‡‘çš„å‚æ•°ç©ºé—´ã€‚</v*></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºçš„ Video Custom Diffusionï¼ˆVCDï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³å¯æ§è§†é¢‘ç”Ÿæˆä¸­ä¸»ä½“èº«ä»½æ§åˆ¶çš„æŒ‘æˆ˜ã€‚é€šè¿‡èåˆèº«ä»½ä¿¡æ¯å’Œå¸§é—´ç›¸å…³æ€§ï¼ŒVCD ä¸ºç”Ÿæˆä¸ä»…åœ¨å¸§é—´ä¿æŒä¸»ä½“èº«ä»½ï¼Œè€Œä¸”å…·æœ‰ç¨³å®šæ€§å’Œæ¸…æ™°åº¦çš„è§†é¢‘é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬æ–°é¢–çš„è´¡çŒ®ï¼ŒåŒ…æ‹¬ç”¨äºç²¾ç¡®èº«ä»½åˆ†ç¦»çš„ ID æ¨¡å—ã€ç”¨äºå¢å¼ºå¸§ä¸€è‡´æ€§çš„ T2V VCD æ¨¡å—ä»¥åŠç”¨äºæé«˜è§†é¢‘è´¨é‡çš„ V2V æ¨¡å—ï¼Œå…±åŒä¸ºè§†é¢‘å†…å®¹ä¸­çš„èº«ä»½ä¿ç•™å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚æˆ‘ä»¬è¿›è¡Œçš„å¹¿æ³›å®éªŒè‚¯å®šäº† VCD åœ¨ç”Ÿæˆé«˜è´¨é‡ã€ç¨³å®šä¸”ä¿ç•™ä¸»ä½“èº«ä»½çš„è§†é¢‘æ–¹é¢çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ ID æ¨¡å—é€‚ç”¨äºç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¢å¼ºäº† VCD çš„å®ç”¨æ€§ï¼Œä½¿å…¶é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„åˆ›æ–°ç‚¹ã€æ€§èƒ½å’Œå·¥ä½œé‡æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘å®šåˆ¶æ‰©æ•£çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†èº«ä»½ä¿¡æ¯å’Œå¸§é—´ç›¸å…³æ€§ï¼Œä»¥ç”Ÿæˆå…·æœ‰ç¨³å®šèº«ä»½çš„è§†é¢‘ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ª ID æ¨¡å—ï¼Œç”¨äºä»æ–‡æœ¬æç¤ºä¸­æå–èº«ä»½ä¿¡æ¯å¹¶å°†å…¶æ³¨å…¥è§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚</li><li>æå‡ºäº†ä¸€ç§ T2V VCD æ¨¡å—ï¼Œç”¨äºå¢å¼ºå¸§é—´ä¸€è‡´æ€§ï¼Œç”Ÿæˆå…·æœ‰å¹³æ»‘è¿åŠ¨å’Œæ¸…æ™°ç»†èŠ‚çš„è§†é¢‘ã€‚æ€§èƒ½ï¼š</li><li>VCD åœ¨èº«ä»½ä¿ç•™æ–¹é¢ä¼˜äºé€‰å®šçš„å¼ºåŸºçº¿ï¼Œç”Ÿæˆçš„é«˜è´¨é‡è§†é¢‘åœ¨å¸§é—´ä¿æŒäº†ä¸»ä½“èº«ä»½ã€‚</li><li>ç”±äº ID æ¨¡å—çš„å¯è¿ç§»æ€§ï¼ŒVCD ä¹Ÿé€‚ç”¨äºå…¬å¼€å¯ç”¨çš„å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜äº†å…¶å¯ç”¨æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>VCD çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œä»…éœ€è¦å°‘é‡é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li><li>ID æ¨¡å—å…·æœ‰ç´§å‡‘çš„å‚æ•°ç©ºé—´ï¼Œä»…éœ€ 16KB çš„å­˜å‚¨ç©ºé—´ï¼Œä½¿å…¶æ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e6a21bfcb16c6c0deb1d0539ef94af7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9fb6739198960204ae02b3df3b1108f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3af883ea390b349d783415082941342e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f79fc49019e994a2b5124fecafb23683.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ffb39f913681e339c8d1aa9719f971cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ad7c82a7b238a18cf1ae3935cfce436.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e509076266dabf0c8283fba23dba850.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ef1ee7f0f72cd6bec6307311ed8330ee.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM). Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings. Building on this progress, we propose SGS-SLAM which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation. It outperforms existing methods by a large margin meanwhile preserves real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v2">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>SGS-SLAM é‡‡ç”¨å¤šé€šé“ä¼˜åŒ–ï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸèå…¥å…³é”®å¸§ä¼˜åŒ–ä¸­ï¼Œå®ç°äº†é«˜ç²¾åº¦ 3D è¯­ä¹‰åˆ†å‰²å’Œé«˜ä¿çœŸé‡å»ºã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>åˆ©ç”¨é«˜æ–¯å–·å°„å°†è¯­ä¹‰ç†è§£èå…¥ SLAM ç³»ç»Ÿï¼Œç”Ÿæˆé«˜è´¨é‡æ¸²æŸ“æ•ˆæœã€‚</li><li>é‡‡ç”¨å¤šé€šé“ä¼˜åŒ–ï¼Œèåˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸï¼Œæå‡é‡å»ºè´¨é‡ã€‚</li><li>åœ¨ç›¸æœºä½å§¿ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</li><li>æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li><li>æ‰©å±•äº† SLAM ç³»ç»Ÿçš„åº”ç”¨èŒƒå›´ï¼Œä½¿å…¶åœ¨è¯­ä¹‰ç†è§£å’Œé‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>ä¸ºå®¤å†…æˆ–å®¤å¤–ç¯å¢ƒçš„é«˜ä¿çœŸé‡å»ºå’Œäº¤äº’å¼æ¢ç´¢æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li><li>ä¸ºè‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººå¯¼èˆªå’Œå¢å¼ºç°å®ç­‰é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯æ”¯æŒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSGS-SLAMï¼šç”¨äºç¥ç»ç¨ å¯† SLAM çš„è¯­ä¹‰é«˜æ–¯æ–‘ç‚¹ç»˜åˆ¶</li><li>ä½œè€…ï¼šMingrui Liã€Shuhong Liuã€Heng Zhouã€Guohao Zhuã€Na Chengã€Hongyu Wang</li><li>å•ä½ï¼šå¤§è¿ç†å·¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ç³»</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€3D è¯­ä¹‰åˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03246ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¯­ä¹‰ç†è§£åœ¨ç¨ å¯† SLAM ä¸­è‡³å…³é‡è¦ï¼Œè€Œå°†é«˜æ–¯æ–‘ç‚¹ç»˜åˆ¶é›†æˆåˆ° SLAM ç³»ç»Ÿä¸­çš„æœ€æ–°è¿›å±•å·²è¯æ˜å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡æ¸²æŸ“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿè§†è§‰ SLAM ç³»ç»Ÿæ“…é•¿ä½¿ç”¨ç‚¹äº‘å’Œä½“ç´ è¿›è¡Œç¨€ç–é‡å»ºï¼Œä½†æ— æ³•è¿›è¡Œç¨ å¯†é‡å»ºã€‚åŸºäºå­¦ä¹ çš„ SLAM æ–¹æ³•å¯ä»¥æå–ç”¨äºé«˜è´¨é‡è¡¨ç¤ºçš„ç¨ å¯†å‡ ä½•ä¿¡æ¯ï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°å™ªå£°å’Œå¼‚å¸¸å€¼çš„å½±å“ã€‚ç¥ç»è¾å°„åœº (NeRF) å¯å‘çš„ SLAM æ–¹æ³•è¿›ä¸€æ­¥æé«˜äº†é‡å»ºè´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸ä¸åŒ…å«è¯­ä¹‰ä¿¡æ¯ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º SGS-SLAMï¼Œå®ƒåœ¨é«˜ä¿çœŸé‡å»ºçš„åŒæ—¶æä¾›ç²¾ç¡®çš„ 3D è¯­ä¹‰åˆ†å‰²ã€‚SGS-SLAM åœ¨æ˜ å°„è¿‡ç¨‹ä¸­é‡‡ç”¨å¤šé€šé“ä¼˜åŒ–ï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸä¸å…³é”®å¸§ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥å¢å¼ºé‡å»ºè´¨é‡ã€‚(4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šSGS-SLAM åœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒä»¥å¾ˆå¤§çš„ä¼˜åŠ¿ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li></ol><p>æ–¹æ³•ï¼š(1): SGS-SLAMé‡‡ç”¨å¤šé€šé“é«˜æ–¯è¡¨ç¤ºï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸä¸å…³é”®å¸§ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥å¢å¼ºé‡å»ºè´¨é‡ã€‚(2): è·Ÿè¸ªè¿‡ç¨‹ä¼°è®¡æ¯ä¸€å¸§çš„ç›¸æœºä½å§¿ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯å‚æ•°å›ºå®šã€‚æ˜ å°„ä¼˜åŒ–åŸºäºä¼°è®¡çš„ç›¸æœºä½å§¿ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºã€‚(3): åœºæ™¯è¡¨ç¤ºä½¿ç”¨é«˜æ–¯å½±å“å‡½æ•° f(Â·)ï¼Œå…¶ä¸­ Ïƒ è¡¨ç¤ºä¸é€æ˜åº¦ï¼ŒÎ¼ è¡¨ç¤ºä¸­å¿ƒä½ç½®ï¼Œr è¡¨ç¤ºåŠå¾„ã€‚æ¯ä¸ªé«˜æ–¯è¿˜æºå¸¦ RGB é¢œè‰² ciã€‚(4): ä½¿ç”¨æ¸²æŸ“æ–¹æ³•å°†é«˜æ–¯æ¸²æŸ“æˆ 2D å›¾åƒï¼Œé€šè¿‡æ²¿æ·±åº¦ç»´åº¦é€¼è¿‘å½±å“å‡½æ•° f(Â·) çš„ç§¯åˆ†æŠ•å½±æ¥å®Œæˆã€‚(5): é€šè¿‡å¯¹é«˜æ–¯è¿›è¡Œæ·±åº¦æ’åºå¹¶æ‰§è¡Œä»å‰åˆ°åçš„ä½“ç§¯æ¸²æŸ“ï¼Œå¯ä»¥ç»„åˆæ‰€æœ‰é«˜æ–¯å¯¹è¯¥åƒç´ çš„å½±å“ã€‚(6): åƒç´ çº§æ¸²æŸ“é¢œè‰² Cpix æ˜¯æ¯ä¸ªé«˜æ–¯é¢œè‰² ci çš„æ€»å’Œï¼Œå¹¶æ ¹æ®å½±å“å‡½æ•° f2Di,pix åŠ æƒï¼Œä¹˜ä»¥é®æŒ¡é¡¹ã€‚(7): æ·±åº¦å¯ä»¥æ¸²æŸ“ä¸ºï¼šDpix = âˆ‘i=1 di f2Di,pix iâˆ’1 âˆj=1 (1âˆ’f2Dj,pix)ï¼Œå…¶ä¸­ di è¡¨ç¤ºæ¯ä¸ªé«˜æ–¯çš„æ·±åº¦ã€‚(8): é€šè¿‡è®¾ç½® di=1ï¼Œå¯ä»¥è®¡ç®—å‡ºè½®å»“ Silpix = Dpix(di=1)ï¼Œè¿™æœ‰åŠ©äºç¡®å®šåƒç´ æ˜¯å¦åœ¨å½“å‰è§†å›¾ä¸­å¯è§ã€‚(9): åœ¨æ˜ å°„è¿‡ç¨‹ä¸­ï¼Œå°† 2D è¯­ä¹‰æ ‡ç­¾åˆ†é…ç»™é«˜æ–¯å‚æ•°çš„ç‰¹å®šé€šé“ä»¥è¡¨ç¤ºå…¶è¯­ä¹‰æ ‡ç­¾å’Œé¢œè‰²ã€‚(10): æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥ä»é‡å»ºçš„ 3D åœºæ™¯æ¸²æŸ“ 2D è¯­ä¹‰å›¾ï¼šSpix = âˆ‘i=1 si f2Di,pix iâˆ’1 âˆj=1 (1âˆ’f2Dj,pix)ï¼Œå…¶ä¸­ si = [ri, gi, bi]T è¡¨ç¤ºä¸é«˜æ–¯å…³è”çš„è¯­ä¹‰é¢œè‰²ã€‚(11): ç›¸æœºä½å§¿ä¼°è®¡é€šè¿‡æœ€å°åŒ–è·Ÿè¸ªæŸå¤±æ¥å®ç°ï¼Œè¯¥æŸå¤±è¡¨ç¤ºçœŸå®é¢œè‰²ã€æ·±åº¦å›¾åƒå’Œè¯­ä¹‰å›¾ä¸å…¶å¯å¾®æ¸²æŸ“è§†å›¾ä¹‹é—´çš„å·®å¼‚ã€‚(12): å…³é”®å¸§é€‰æ‹©å’ŒåŠ æƒï¼šåœ¨è·Ÿè¸ªé˜¶æ®µï¼ŒåŒæ—¶è¯†åˆ«å’Œå­˜å‚¨å…³é”®å¸§ã€‚è¿™äº›å…³é”®å¸§æä¾›äº†å¯¹è±¡çš„ä¸åŒè§†å›¾ï¼Œå¯¹äºæ˜ å°„ä¼˜åŒ– 3D åœºæ™¯é‡å»ºè‡³å…³é‡è¦ã€‚(13): SGS-SLAM åœ¨æ’å®šæ—¶é—´é—´éš”å†…æ•è·å’Œå­˜å‚¨å…³é”®å¸§ã€‚éšåï¼Œæ ¹æ®å‡ ä½•å’Œè¯­ä¹‰çº¦æŸé€‰æ‹©ä¸å½“å‰å¸§å…³è”çš„å…³é”®å¸§ã€‚(14): é¦–å…ˆè¿›è¡ŒåŸºäºå‡ ä½•çš„åˆå§‹é€‰æ‹©ï¼Œç„¶åè¿›è¡ŒåŸºäºè¯­ä¹‰çš„äºŒæ¬¡ç­›é€‰ã€‚(15): å¯¹äºæ¯ä¸ªå…³é”®å¸§ï¼Œè®¡ç®—ä¸ç¡®å®šæ€§åˆ†æ•° U(t) = eâˆ’Ï„tï¼Œå…¶ä¸­ t è¡¨ç¤ºå…³é”®å¸§çš„æ—¶é—´æˆ³ï¼ŒÏ„ ä¸ºè¡°å‡ç³»æ•°ã€‚(16): ä½¿ç”¨æ­¤ä¸ç¡®å®šæ€§åˆ†æ•°å¯¹æ˜ å°„æŸå¤± Lmapping åŠ æƒã€‚(17): åœ°å›¾é‡å»ºï¼šåœºæ™¯ä½¿ç”¨ä¸‰ä¸ªä¸åŒé€šé“çš„é«˜æ–¯å»ºæ¨¡ï¼šå®ƒä»¬çš„å‡å€¼åæ ‡è¡¨ç¤ºåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œå®ƒä»¬çš„å¤–è§‚é¢œè‰²æç»˜äº†åœºæ™¯çš„è§†è§‰å¤–è§‚ï¼Œå®ƒä»¬çš„è¯­ä¹‰é¢œè‰²è¡¨ç¤ºå¯¹è±¡çš„è¯­ä¹‰æ ‡ç­¾ã€‚(18): åœ¨é«˜æ–¯è‡´å¯†åŒ–å’Œä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè·¨é€šé“çš„è¿™äº›å‚æ•°è¢«è”åˆä¼˜åŒ–ï¼Œè€Œä»è·Ÿè¸ªä¸­ç¡®å®šçš„ç›¸æœºä½å§¿ä¿æŒå›ºå®šã€‚(19): ä»ç¬¬ä¸€å¸§å¼€å§‹ï¼Œæ‰€æœ‰åƒç´ éƒ½æœ‰åŠ©äºåˆå§‹åŒ–åœ°å›¾ã€‚(20): åœ¨æ–°æ—¶é—´æ­¥çš„åœ°å›¾é‡å»ºè¿‡ç¨‹ä¸­ï¼Œå°†æ–°é«˜æ–¯å¼•å…¥åˆ°åœ°å›¾ä¸­ï¼Œè¿™äº›åŒºåŸŸè¦ä¹ˆå¯†åº¦ä¸è¶³ï¼Œè¦ä¹ˆæ˜¾ç¤ºå…ˆå‰ä¼°è®¡çš„åœ°å›¾å‰é¢çš„æ–°å‡ ä½•å½¢çŠ¶ã€‚(21): é€šè¿‡å°†æ©ç åº”ç”¨äºåƒç´ æ¥è°ƒèŠ‚æ–°é«˜æ–¯çš„æ·»åŠ ï¼Œå…¶ä¸­è¦ä¹ˆ (i) è½®å»“å€¼ Silpix ä½äºæŸä¸ªé˜ˆå€¼ï¼Œè¡¨ç¤ºå¯è§æ€§é«˜åº¦ä¸ç¡®å®šï¼Œè¦ä¹ˆ (ii) çœŸå®æ·±åº¦è¿œå°äºä¼°è®¡æ·±åº¦ï¼Œè¡¨æ˜å­˜åœ¨æ–°çš„å‡ ä½•å®ä½“ã€‚(22): è‡´å¯†åŒ–åï¼Œé€šè¿‡æœ€å°åŒ–æ˜ å°„æŸå¤±æ¥ä¼˜åŒ–åœ°å›¾å‚æ•°ï¼šLmapping = U âˆ‘pix Î»D |DGTpixâˆ’Dpix| + Î»C L C + Î»S L Sã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šSGS-SLAM åœ¨è¿›è¡Œé«˜ä¿çœŸé‡å»ºçš„åŒæ—¶æä¾›äº†ç²¾ç¡®çš„ 3D è¯­ä¹‰åˆ†å‰²ï¼Œåœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºç¥ç»ç¨ å¯† SLAM æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼šSGS-SLAM é‡‡ç”¨å¤šé€šé“é«˜æ–¯è¡¨ç¤ºï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸä¸å…³é”®å¸§ä¼˜åŒ–ç›¸ç»“åˆï¼Œå¢å¼ºäº†é‡å»ºè´¨é‡ï¼Œå¹¶é¦–æ¬¡å°†è¯­ä¹‰ä¿¡æ¯é›†æˆåˆ°ç¥ç»ç¨ å¯† SLAM ç³»ç»Ÿä¸­ã€‚æ€§èƒ½ï¼šSGS-SLAM åœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»¥å¾ˆå¤§çš„ä¼˜åŠ¿ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚å·¥ä½œé‡ï¼šSGS-SLAM çš„å®ç°éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨æŸäº›èµ„æºå—é™çš„åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-49d695fd07273ec0ead5f03d33095327.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9e64fa80d8afdcf89c98cfd50dd717f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-29  VastGaussian Vast 3D Gaussians for Large Scene Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Talking%20Head%20Generation/</id>
    <published>2024-02-29T12:47:51.000Z</published>
    <updated>2024-02-29T13:22:02.679Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-29-æ›´æ–°"><a href="#2024-02-29-æ›´æ–°" class="headerlink" title="2024-02-29 æ›´æ–°"></a>2024-02-29 æ›´æ–°</h1><h2 id="EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions"><a href="#EMO-Emote-Portrait-Alive-Generating-Expressive-Portrait-Videos-with-Audio2Video-Diffusion-Model-under-Weak-Conditions" class="headerlink" title="EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions"></a>EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with   Audio2Video Diffusion Model under Weak Conditions</h2><p><strong>Authors:Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</strong></p><p>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism. </p><p><a href="http://arxiv.org/abs/2402.17485v1">PDF</a> </p><p><strong>Summary</strong><br>éŸ³é¢‘çº¿ç´¢èƒ½å¤ŸååŠ©ç”Ÿæˆæ›´å…·è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿçš„é¢éƒ¨åŠ¨ç”»ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¼ ç»ŸæŠ€æœ¯æ— æ³•å……åˆ†æ•æ‰äººç±»é¢éƒ¨è¡¨æƒ…å’Œä¸ªäººé£æ ¼å·®å¼‚ã€‚</li><li>EMO æ¡†æ¶é‡‡ç”¨ç›´æ¥éŸ³é¢‘åˆ°è§†é¢‘åˆæˆæ–¹æ³•ï¼Œæ— éœ€ä¸­é—´ 3D æ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹ã€‚</li><li>EMO å¯ç”Ÿæˆæµç•…æ— ç¼çš„è§†é¢‘ï¼Œå¹¶å§‹ç»ˆä¿æŒèº«ä»½ä¸€è‡´æ€§ã€‚</li><li>EMO å¯ç”Ÿæˆå…·æœ‰é«˜åº¦è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿçš„è¯´è¯å’Œå”±æ­Œè§†é¢‘ã€‚</li><li>EMO åœ¨è¡¨ç°åŠ›å’ŒçœŸå®æ„Ÿæ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>EMO å……åˆ†åˆ©ç”¨äº†éŸ³é¢‘çº¿ç´¢ï¼Œæå‡äº†é¢éƒ¨åŠ¨ç”»çš„åŠ¨æ€æ€§å’Œç»†è‡´åº¦ã€‚</li><li>EMO å¯å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸï¼ŒåŒ…æ‹¬ç”µå½±ã€æ¸¸æˆå’Œè§†é¢‘ä¼šè®®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šEMOï¼šEmotePortraitAliveâ€”â€”åœ¨å¼±æ¡ä»¶ä¸‹ä½¿ç”¨éŸ³é¢‘åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è‚–åƒè§†é¢‘</li><li>ä½œè€…ï¼šTian Linruiã€Wang Qiã€Zhang Bangã€Bo Liefeng</li><li>éš¶å±å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢æ™ºèƒ½è®¡ç®—ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šAudio-driven portrait video generationã€Talking headã€Expressive facial expressionsã€Audio-to-video synthesis</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://humanaigc.github.io/emote-portrait-alive/   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   åœ¨è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆä¸­ï¼Œå¢å¼ºçœŸå®æ„Ÿå’Œè¡¨ç°åŠ›æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦å…³æ³¨éŸ³é¢‘çº¿ç´¢å’Œé¢éƒ¨åŠ¨ä½œä¹‹é—´çš„åŠ¨æ€å’Œç»†å¾®å…³ç³»ã€‚ä¼ ç»ŸæŠ€æœ¯å¾€å¾€æ— æ³•æ•æ‰åˆ°äººç±»è¡¨æƒ…çš„å…¨è²Œå’Œä¸ªäººé¢éƒ¨é£æ ¼çš„ç‹¬ç‰¹æ€§ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼š   ä¼ ç»Ÿçš„è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦ä¸­é—´ 3D æ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹ï¼Œè¿™ä¼šå¼•å…¥é¢å¤–çš„å¤æ‚æ€§å’Œé™åˆ¶ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨æ•æ‰ç»†å¾®çš„è¡¨æƒ…å’Œä¿æŒå¸§ä¹‹é—´çš„ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º EMO çš„æ–°æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ç›´æ¥éŸ³é¢‘åˆ°è§†é¢‘åˆæˆçš„æ–¹æ³•ï¼Œç»•è¿‡äº†å¯¹ä¸­é—´ 3D æ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†éŸ³é¢‘çº¿ç´¢ç›´æ¥æ˜ å°„åˆ°è§†é¢‘å¸§ï¼Œç¡®ä¿äº†æ— ç¼çš„å¸§è¿‡æ¸¡å’Œä¸€è‡´çš„é¢éƒ¨åŠ¨ä½œã€‚   ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼š   EMO åœ¨è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰ä¸°å¯Œé¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾ç€æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡å¢å¼ºè¯´è¯äººå¤´åƒè§†é¢‘ç”ŸæˆçœŸå®æ„Ÿå’Œè¡¨ç°åŠ›çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºäº†ä¸€ç§åä¸ºEMOçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ç›´æ¥éŸ³é¢‘åˆ°è§†é¢‘åˆæˆçš„æ–¹æ³•ï¼Œç»•è¿‡äº†å¯¹ä¸­é—´3Dæ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹çš„éœ€æ±‚ã€‚ï¼ˆ2ï¼‰è¯¥æ–¹æ³•åˆ©ç”¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†éŸ³é¢‘çº¿ç´¢ç›´æ¥æ˜ å°„åˆ°è§†é¢‘å¸§ï¼Œç¡®ä¿äº†æ— ç¼çš„å¸§è¿‡æ¸¡å’Œä¸€è‡´çš„é¢éƒ¨åŠ¨ä½œã€‚ï¼ˆ3ï¼‰åœ¨è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå¯¹EMOè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰ä¸°å¯Œé¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾ç€æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º EMO çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ç›´æ¥éŸ³é¢‘åˆ°è§†é¢‘åˆæˆçš„æ–¹æ³•ï¼Œç»•è¿‡äº†å¯¹ä¸­é—´ 3D æ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†éŸ³é¢‘çº¿ç´¢ç›´æ¥æ˜ å°„åˆ°è§†é¢‘å¸§ï¼Œç¡®ä¿äº†æ— ç¼çš„å¸§è¿‡æ¸¡å’Œä¸€è‡´çš„é¢éƒ¨åŠ¨ä½œã€‚åœ¨è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå¯¹ EMO è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰ä¸°å¯Œé¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾ç€æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡å¢å¼ºè¯´è¯äººå¤´åƒè§†é¢‘ç”ŸæˆçœŸå®æ„Ÿå’Œè¡¨ç°åŠ›çš„ç›®æ ‡ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>ç›´æ¥éŸ³é¢‘åˆ°è§†é¢‘åˆæˆçš„æ–¹æ³•ï¼Œç»•è¿‡äº†å¯¹ä¸­é—´ 3D æ¨¡å‹æˆ–é¢éƒ¨å…³é”®ç‚¹çš„éœ€æ±‚ã€‚</li><li>åˆ©ç”¨éŸ³é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†éŸ³é¢‘çº¿ç´¢ç›´æ¥æ˜ å°„åˆ°è§†é¢‘å¸§ï¼Œç¡®ä¿äº†æ— ç¼çš„å¸§è¿‡æ¸¡å’Œä¸€è‡´çš„é¢éƒ¨åŠ¨ä½œã€‚æ€§èƒ½ï¼š</li><li>åœ¨è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾ç€æ€§èƒ½ã€‚</li><li>ç”Ÿæˆäº†å…·æœ‰ä¸°å¯Œé¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è§†é¢‘ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„ä¸­é—´æ­¥éª¤æˆ–é¢å¤–çš„æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-10c8e47dfe09b5369134bad3bf5b1e69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-262ccbd331f2623737aa6cbcc24c64e5.jpg" align="middle"></details><h2 id="G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment"><a href="#G4G-A-Generic-Framework-for-High-Fidelity-Talking-Face-Generation-with-Fine-grained-Intra-modal-Alignment" class="headerlink" title="G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment"></a>G4G:A Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</h2><p><strong>Authors:Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</strong></p><p>Despite numerous completed studies, achieving high fidelity talking face generation with highly synchronized lip movements corresponding to arbitrary audio remains a significant challenge in the field. The shortcomings of published studies continue to confuse many researchers. This paper introduces G4G, a generic framework for high fidelity talking face generation with fine-grained intra-modal alignment. G4G can reenact the high fidelity of original video while producing highly synchronized lip movements regardless of given audio tones or volumes. The key to G4Gâ€™s success is the use of a diagonal matrix to enhance the ordinary alignment of audio-image intra-modal features, which significantly increases the comparative learning between positive and negative samples. Additionally, a multi-scaled supervision module is introduced to comprehensively reenact the perceptional fidelity of original video across the facial region while emphasizing the synchronization of lip movements and the input audio. A fusion network is then used to further fuse the facial region and the rest. Our experimental results demonstrate significant achievements in reenactment of original video quality as well as highly synchronized talking lips. G4G is an outperforming generic framework that can produce talking videos competitively closer to ground truth level than current state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.18122v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ é¢†åŸŸäºŸéœ€ä¸€ä¸ªé€šç”¨çš„é«˜ä¿çœŸè¯´è¯äººè„¸ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„éŸ³é¢‘-å›¾åƒè·¨æ¨¡æ€å¯¹é½ç²¾åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º G4Gï¼Œä¸€ç§ç”¨äºé«˜ä¿çœŸè¯´è¯äººè„¸ç”Ÿæˆçš„é€šç”¨æ¡†æ¶ï¼Œå¯å®ç°ç²¾ç»†çš„æ¨¡æ€å†…å¯¹é½ã€‚</li><li>G4G ä½¿ç”¨å¯¹è§’çŸ©é˜µå¢å¼ºéŸ³é¢‘å›¾åƒæ¨¡æ€å†…ç‰¹å¾çš„å¸¸è§„å¯¹é½ï¼Œæ˜¾ç€å¢åŠ äº†æ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„æ¯”è¾ƒå­¦ä¹ ã€‚</li><li>å¼•å…¥å¤šå°ºåº¦ç›‘ç£æ¨¡å—ï¼Œä»¥å…¨é¢é‡ç°é¢éƒ¨åŒºåŸŸä¸­åŸå§‹è§†é¢‘çš„æ„ŸçŸ¥ä¿çœŸåº¦ï¼ŒåŒæ—¶å¼ºè°ƒå”‡éƒ¨åŠ¨ä½œä¸è¾“å…¥éŸ³é¢‘çš„åŒæ­¥ã€‚</li><li>ä½¿ç”¨èåˆç½‘ç»œè¿›ä¸€æ­¥èåˆé¢éƒ¨åŒºåŸŸå’Œå…¶ä»–åŒºåŸŸã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é‡ç°åŸå§‹è§†é¢‘è´¨é‡å’Œé«˜åº¦åŒæ­¥çš„è¯´è¯å˜´å”‡æ–¹é¢å–å¾—äº†æ˜¾ç€æˆå°±ã€‚</li><li>G4G æ˜¯ä¸€ç§æ€§èƒ½ä¼˜å¼‚çš„é€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ç”Ÿæˆæ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´æ¥è¿‘çœŸå®æ°´å¹³çš„è¯´è¯è§†é¢‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šG4Gï¼šä¸€ä¸ªç”¨äºé«˜ä¿çœŸè¯´è¯äººè„¸ç”Ÿæˆå’Œç²¾ç»†åŒ–æ¨¡æ€å†…å¯¹é½çš„é€šç”¨æ¡†æ¶</li><li>ä½œè€…ï¼šJuan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Di Wu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé•¿æ²™ä¸‡å…´ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸</li><li>å…³é”®è¯ï¼šè¯´è¯äººè„¸ç”Ÿæˆã€æ¨¡æ€å†…å¯¹é½ã€å¤šå°ºåº¦ç›‘ç£ã€èåˆç½‘ç»œ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.18122</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººè„¸ç”Ÿæˆæ—¨åœ¨åˆæˆä¸€ä¸ªç›®æ ‡äººç‰©çš„é«˜ä¿çœŸè§†é¢‘ï¼Œå…¶å”‡éƒ¨åŠ¨ä½œä¸ä»»æ„éŸ³é¢‘åŒæ­¥ã€‚å°½ç®¡æœ‰è®¸å¤šç ”ç©¶ï¼Œä½†è¦å®ç°é«˜ä¿çœŸè¯´è¯äººè„¸ç”Ÿæˆå¹¶ä½¿å…¶å”‡éƒ¨åŠ¨ä½œä¸ä»»æ„éŸ³é¢‘é«˜åº¦åŒæ­¥ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ä¸»è¦åœ¨äºï¼š1ï¼‰æ— æ³•é‡ç°åŸå§‹è§†é¢‘çš„é«˜ä¿çœŸåº¦ï¼›2ï¼‰ç”Ÿæˆçš„å”‡éƒ¨åŠ¨ä½œä¸éŸ³é¢‘ä¸åŒæ­¥ï¼›3ï¼‰ç”Ÿæˆçš„äººè„¸è§†é¢‘ä¿çœŸåº¦ä½ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† G4Gï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜ä¿çœŸè¯´è¯äººè„¸ç”Ÿæˆå’Œç²¾ç»†åŒ–æ¨¡æ€å†…å¯¹é½çš„é€šç”¨æ¡†æ¶ã€‚G4G é‡‡ç”¨å¯¹è§’çŸ©é˜µæ¥å¢å¼ºéŸ³é¢‘-å›¾åƒæ¨¡æ€å†…ç‰¹å¾çš„æ™®é€šå¯¹é½ï¼Œæ˜¾è‘—å¢åŠ äº†æ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„æ¯”è¾ƒå­¦ä¹ ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šå°ºåº¦ç›‘ç£æ¨¡å—ï¼Œä»¥å…¨é¢é‡ç°åŸå§‹è§†é¢‘åœ¨é¢éƒ¨åŒºåŸŸçš„æ„ŸçŸ¥ä¿çœŸåº¦ï¼ŒåŒæ—¶å¼ºè°ƒå”‡éƒ¨åŠ¨ä½œä¸è¾“å…¥éŸ³é¢‘çš„åŒæ­¥æ€§ã€‚ç„¶åä½¿ç”¨èåˆç½‘ç»œè¿›ä¸€æ­¥èåˆé¢éƒ¨åŒºåŸŸå’Œå…¶ä»–éƒ¨åˆ†ã€‚(4) æ€§èƒ½ï¼šG4G åœ¨é‡ç°åŸå§‹è§†é¢‘è´¨é‡å’Œé«˜åº¦åŒæ­¥çš„è¯´è¯äººå˜´å”‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG4G ç”Ÿæˆçš„è¯´è¯äººè§†é¢‘æ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´æ¥è¿‘çœŸå®æ°´å¹³ã€‚</li></ol><p>7.Methodsï¼š(1)ï¼šæå‡ºG4Gæ¡†æ¶ï¼Œé‡‡ç”¨å¯¹è§’çŸ©é˜µå¢å¼ºéŸ³é¢‘-å›¾åƒæ¨¡æ€å†…ç‰¹å¾å¯¹é½ï¼Œå¢åŠ æ­£è´Ÿæ ·æœ¬æ¯”è¾ƒå­¦ä¹ ï¼›(2)ï¼šå¼•å…¥å¤šå°ºåº¦ç›‘ç£æ¨¡å—ï¼Œé‡ç°åŸå§‹è§†é¢‘é¢éƒ¨åŒºåŸŸæ„ŸçŸ¥ä¿çœŸåº¦ï¼Œå¼ºè°ƒå”‡éƒ¨åŠ¨ä½œä¸éŸ³é¢‘åŒæ­¥ï¼›(3)ï¼šä½¿ç”¨èåˆç½‘ç»œèåˆé¢éƒ¨åŒºåŸŸå’Œå…¶ä»–éƒ¨åˆ†ã€‚</p><p><strong>8. ç»“è®º</strong>(1): æœ¬å·¥ä½œæå‡ºäº† G4G æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸä¸”é«˜åº¦åŒæ­¥çš„è¯´è¯äººè„¸è§†é¢‘ã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šå¯¹è§’ç²¾ç»†åŒ–å¯¹é½ç½‘ç»œå’Œå¤šå°ºåº¦ç›‘ç£è‡ªé€‚åº”ç©ºé—´å˜æ¢ç½‘ç»œã€‚è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œç”Ÿæˆå…·æœ‰å“è¶Šä¿çœŸåº¦å’Œå¤šå°ºåº¦ç»†èŠ‚çš„è¯´è¯äººè„¸è§†é¢‘ã€‚å¯¹è§’ç²¾ç»†åŒ–å¯¹é½ç½‘ç»œä¸“é—¨è®¾è®¡ç”¨äºè§£å†³æ¨¡æ€å†…å’Œæ¨¡æ€é—´å¯¹é½çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä¿ç•™æºå›¾åƒçš„é¢éƒ¨èº«ä»½ã€å±æ€§å’Œä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ï¼Œæˆ‘ä»¬çš„ç½‘ç»œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘ä¸æºè§’è‰²é«˜åº¦ç›¸ä¼¼ã€‚æ­¤å¯¹é½è¿‡ç¨‹å¯¹äºä¿æŒç”Ÿæˆè§†é¢‘çš„çœŸå®æ€§å’Œè§†è§‰è´¨é‡è‡³å…³é‡è¦ã€‚å¤šå°ºåº¦ç›‘ç£è‡ªé€‚åº”ç©ºé—´å˜æ¢ç½‘ç»œè¿›ä¸€æ­¥å¢å¼ºäº†ç”Ÿæˆè§†é¢‘çš„ä¿çœŸåº¦ã€‚é€šè¿‡å¯¹å˜´å½¢å’Œå¤´éƒ¨å§¿åŠ¿è¿›è¡Œç©ºé—´å˜å½¢ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå®ç°äº†å˜´å”‡è¿åŠ¨çš„éå‡¡å‡†ç¡®æ€§å’ŒçœŸå®æ€§ã€‚ç”Ÿæˆå˜´å”‡è¿åŠ¨ä¸ç»™å®šéŸ³é¢‘ä¹‹é—´çš„è¿™ç§åŒæ­¥æ°´å¹³æ˜æ˜¾è¶…è¿‡äº†ç°æœ‰çš„äººè„¸é€šç”¨æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ G4G æ¡†æ¶åœ¨ä¿ç•™è§’è‰²èº«ä»½ã€çš®è‚¤çº¹ç†å’Œä¸çœŸå®æƒ…å†µé«˜åº¦ç›¸ä¼¼çš„ç»†èŠ‚æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆä¸ä»»æ„ç»™å®šéŸ³é¢‘ç›¸å¯¹åº”çš„ã€é«˜åº¦åŒæ­¥çš„å˜´å”‡è¿åŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿™äº›ç»“æœä¼˜äºç°æœ‰äººè„¸é€šç”¨æ–¹æ³•ï¼Œçªå‡ºäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚è™½ç„¶æˆ‘ä»¬çš„ G4G æ¡†æ¶ä»£è¡¨äº†è¯´è¯äººè„¸ç”Ÿæˆé¢†åŸŸçš„é‡å¤§è¿›æ­¥ï¼Œä½†æˆ‘ä»¬è®¤è¯†åˆ°ä»æœ‰æŒ‘æˆ˜éœ€è¦è§£å†³ã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆå…·æœ‰å¤§å¤´éƒ¨å§¿åŠ¿è§’åº¦çš„è§†é¢‘ä»¥åŠå¤„ç†å¿«é€Ÿå˜åŒ–çš„èƒŒæ™¯å’Œå…‰ç…§æ¡ä»¶ä»ç„¶æ˜¯æŒç»­çš„ç ”ç©¶é¢†åŸŸã€‚æˆ‘ä»¬æ­£åœ¨ç§¯æåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶è®¡åˆ’åœ¨ä¸ä¹…çš„å°†æ¥å‘å¸ƒè¿›ä¸€æ­¥çš„ç ”ç©¶ç»“æœã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºçš„ G4G æ¡†æ¶ä¸ºç”Ÿæˆé«˜ä¿çœŸä¸”é«˜åº¦åŒæ­¥çš„è¯´è¯äººè„¸è§†é¢‘æä¾›äº†ä¸€ç§å¼ºå¤§ä¸”æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ä¿ç•™è§’è‰²èº«ä»½ã€çš®è‚¤çº¹ç†å’Œç»†èŠ‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºåŒ…æ‹¬å¨±ä¹ã€æ•™è‚²å’ŒåŒ»ç–—ä¿å¥åœ¨å†…çš„å„ä¸ªé¢†åŸŸçš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong>* æå‡ºå¯¹è§’ç²¾ç»†åŒ–å¯¹é½ç½‘ç»œï¼Œå¢å¼ºéŸ³é¢‘-å›¾åƒæ¨¡æ€å†…ç‰¹å¾å¯¹é½ï¼Œå¢åŠ æ­£è´Ÿæ ·æœ¬æ¯”è¾ƒå­¦ä¹ ã€‚* å¼•å…¥å¤šå°ºåº¦ç›‘ç£è‡ªé€‚åº”ç©ºé—´å˜æ¢ç½‘ç»œï¼Œé‡ç°åŸå§‹è§†é¢‘é¢éƒ¨åŒºåŸŸæ„ŸçŸ¥ä¿çœŸåº¦ï¼Œå¼ºè°ƒå”‡éƒ¨åŠ¨ä½œä¸éŸ³é¢‘åŒæ­¥ã€‚* ä½¿ç”¨èåˆç½‘ç»œèåˆé¢éƒ¨åŒºåŸŸå’Œå…¶ä»–éƒ¨åˆ†ã€‚<strong>æ€§èƒ½ï¼š</strong>* åœ¨é‡ç°åŸå§‹è§†é¢‘è´¨é‡å’Œé«˜åº¦åŒæ­¥çš„è¯´è¯äººå˜´å”‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚* ç”Ÿæˆçš„è¯´è¯äººè§†é¢‘æ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ›´æ¥è¿‘çœŸå®æ°´å¹³ã€‚<strong>å·¥ä½œé‡ï¼š</strong>* æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒæ—¶é—´ä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e12c89676d8b67fdf727809d6024eb2f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153d9657273ba05cfef190ef2e389848.jpg" align="middle"></details>## Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**Authors:Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang**Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate geometry learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications. [PDF](http://arxiv.org/abs/2402.17364v1) CVPR 2024**Summary**ç¥ç»ç½‘ç»œç¼–ç çš„åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰æ˜¯ä¸€ç§ç»“åˆè¡¨ç¤ºæ–¹æ³•ï¼Œç¡®ä¿äº†å¤æ‚é¢éƒ¨å˜å½¢åœ¨å„ç§åŠ¨ä½œå’Œè§†ç‚¹ä¸‹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚**Key Takeaways**- DynTet é‡‡ç”¨åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œå°†æ˜¾å¼åŠ¨æ€ç½‘æ ¼ç¼–ç åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥ç¡®ä¿å‡ ä½•ä¸€è‡´æ€§ã€‚- åæ ‡ç½‘ç»œç”¨äºå­¦ä¹ ç¬¦å·è·ç¦»ã€å½¢å˜å’Œæè´¨çº¹ç†ï¼Œå°†è®­ç»ƒæ•°æ®é”šå®šåˆ°é¢„å®šä¹‰çš„å››é¢ä½“ç½‘æ ¼ä¸­ã€‚- è¿ç”¨ Marching Tetrahedraï¼ŒDynTet æœ‰æ•ˆåœ°è§£ç å…·æœ‰è¿ç»­æ‹“æ‰‘çš„çº¹ç†ç½‘æ ¼ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“å™¨å®ç°å¿«é€Ÿæ¸²æŸ“å¹¶åˆ©ç”¨åƒç´ æŸå¤±è¿›è¡Œç›‘ç£ã€‚- DynTet ç»“åˆç»å…¸ 3D å¯å˜å½¢æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å‡ ä½•å­¦ä¹ å¹¶å®šä¹‰ä¸€ç§è§„èŒƒç©ºé—´ä»¥ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚- DynTet ç›¸æ¯”äºå…ˆå‰çš„ç ”ç©¶ï¼Œåœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚- é™¤äº†åˆ¶ä½œç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘ï¼Œè¯¥æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œæœ‰æœ›å®ç°è®¸å¤šæ–°å…´åº”ç”¨ã€‚- DynTet å¼¥è¡¥äº†éšå¼æ–¹æ³•ç¼ºä¹æ˜¾å¼å‡ ä½•çº¦æŸçš„é—®é¢˜ï¼Œé€šè¿‡å­¦ä¹ åŠ¨æ€ç½‘æ ¼æ¥æé«˜é¢éƒ¨å˜å½¢å»ºæ¨¡çš„å‡†ç¡®æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç”¨äºé«˜è´¨é‡è¯´è¯äººå¤´éƒ¨åˆæˆçš„åŠ¨æ€å››é¢ä½“å­¦ä¹ </li><li>ä½œè€…ï¼šZhang Zhicheng, Xu Chenyang, Zhang Haoran, Wu Yuxuan, Wang Yebin, Chen Min, Chen Biao</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šè¯´è¯äººå¤´éƒ¨åˆæˆã€åŠ¨æ€ç½‘æ ¼ã€éšå¼è¡¨ç¤ºã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.05915</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšå¼è¡¨ç¤ºæ–¹æ³•ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œåœ¨ä»è§†é¢‘åºåˆ—ä¸­ç”Ÿæˆé€¼çœŸä¸”å¯åŠ¨ç”»çš„å¤´éƒ¨å¤´åƒæ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ˜¾å¼å‡ ä½•çº¦æŸï¼Œè¿™äº›éšå¼æ–¹æ³•ä»é¢ä¸´è§†è§‰ä¼ªå½±å’ŒæŠ–åŠ¨çš„æŒ‘æˆ˜ï¼Œè¿™ç»™å‡†ç¡®å»ºæ¨¡å¤æ‚çš„é¢éƒ¨å˜å½¢å¸¦æ¥äº†æ ¹æœ¬æ€§æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é‡‡ç”¨éšå¼è¡¨ç¤ºï¼Œç”±äºç¼ºä¹æ˜¾å¼å‡ ä½•çº¦æŸï¼Œå­˜åœ¨è§†è§‰ä¼ªå½±å’ŒæŠ–åŠ¨é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ··åˆè¡¨ç¤ºï¼Œå®ƒé€šè¿‡ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®ä¿åœ¨å„ç§è¿åŠ¨å’Œè§†ç‚¹ä¸‹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚DynTet ç”±åŸºäºåæ ‡çš„ç½‘ç»œå‚æ•°åŒ–ï¼Œè¿™äº›ç½‘ç»œå­¦ä¹ æœ‰ç¬¦å·è·ç¦»ã€å˜å½¢å’Œæè´¨çº¹ç†ï¼Œå°†è®­ç»ƒæ•°æ®é”šå®šåˆ°é¢„å®šä¹‰çš„å››é¢ä½“ç½‘æ ¼ä¸­ã€‚åˆ©ç”¨è¡Œè¿›å››é¢ä½“ï¼ŒDynTet æœ‰æ•ˆåœ°è§£ç å…·æœ‰ç»Ÿä¸€æ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“å™¨å’Œåƒç´ æŸå¤±è¿›è¡Œç›‘ç£ï¼Œä»è€Œå®ç°å¿«é€Ÿæ¸²æŸ“ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ç»“åˆäº†ç»å…¸çš„ 3D å¯å˜å½¢æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å‡ ä½•å­¦ä¹ å¹¶å®šä¹‰è§„èŒƒç©ºé—´ä»¥ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚ç”±äº DynTet ä¸­é‡‡ç”¨æœ‰æ•ˆçš„å‡ ä½•è¡¨ç¤ºï¼Œè¿™äº›ä¼˜åŠ¿å¾ˆå®¹æ˜“å®ç°ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒDynTet åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢æ ¹æ®å„ç§æŒ‡æ ‡å±•ç¤ºäº†æ˜¾ç€çš„æ”¹è¿›ã€‚é™¤äº†åˆ¶ä½œç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œè¿™æœ‰æœ›æ”¯æŒè®¸å¤šæ–°å…´åº”ç”¨ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºåŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰æ¡†æ¶ï¼Œå¿«é€Ÿä»çŸ­è§†é¢‘åºåˆ—å­¦ä¹  3D å¤´éƒ¨å¤´åƒï¼Œå¹¶å®ç°é«˜è´¨é‡è¯´è¯äººå¤´éƒ¨å®æ—¶æ¸²æŸ“ã€‚(2): æ”¹è¿›å››é¢ä½“è¡¨ç¤ºï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œç¡®ä¿ä¸åŒè¿åŠ¨å’Œè§†ç‚¹ä¸‹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚(3): é‡‡ç”¨è¡Œè¿›å››é¢ä½“è§£ç å…·æœ‰ç»Ÿä¸€æ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“å™¨å’Œåƒç´ æŸå¤±è¿›è¡Œç›‘ç£ï¼Œå®ç°å¿«é€Ÿæ¸²æŸ“ã€‚(4): ç»“åˆç»å…¸ 3D å¯å˜å½¢æ¨¡å‹ï¼Œä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå®šä¹‰è§„èŒƒç©ºé—´ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰çš„æ–°å‹æ··åˆè¡¨ç¤ºï¼Œç”¨äºä»çŸ­è§†é¢‘åºåˆ—ä¸­å­¦ä¹ é€¼çœŸä¸”å¯åŠ¨ç”»çš„è¯´è¯äººå¤´éƒ¨ï¼Œå¹¶å®ç°äº†é«˜è´¨é‡è¯´è¯äººå¤´éƒ¨å®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºåŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰æ¡†æ¶ï¼Œå¿«é€Ÿä»çŸ­è§†é¢‘åºåˆ—å­¦ä¹  3D å¤´éƒ¨å¤´åƒï¼Œå¹¶å®ç°é«˜è´¨é‡è¯´è¯äººå¤´éƒ¨å®æ—¶æ¸²æŸ“ã€‚æ”¹è¿›å››é¢ä½“è¡¨ç¤ºï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹æ˜¾å¼åŠ¨æ€ç½‘æ ¼è¿›è¡Œç¼–ç ï¼Œç¡®ä¿ä¸åŒè¿åŠ¨å’Œè§†ç‚¹ä¸‹çš„å‡ ä½•ä¸€è‡´æ€§ã€‚é‡‡ç”¨è¡Œè¿›å››é¢ä½“è§£ç å…·æœ‰ç»Ÿä¸€æ‹“æ‰‘ç»“æ„çš„çº¹ç†ç½‘æ ¼ï¼Œé€šè¿‡å¯å¾®æ¸²æŸ“å™¨å’Œåƒç´ æŸå¤±è¿›è¡Œç›‘ç£ï¼Œå®ç°å¿«é€Ÿæ¸²æŸ“ã€‚ç»“åˆç»å…¸ 3D å¯å˜å½¢æ¨¡å‹ï¼Œä¿ƒè¿›å‡ ä½•å­¦ä¹ ï¼Œå®šä¹‰è§„èŒƒç©ºé—´ç®€åŒ–çº¹ç†å­¦ä¹ ã€‚æ€§èƒ½ï¼šä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒDynTet åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢æ ¹æ®å„ç§æŒ‡æ ‡å±•ç¤ºäº†æ˜¾ç€çš„æ”¹è¿›ã€‚é™¤äº†åˆ¶ä½œç¨³å®šä¸”è§†è§‰ä¸Šå¸å¼•äººçš„åˆæˆè§†é¢‘å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜è¾“å‡ºåŠ¨æ€ç½‘æ ¼ï¼Œè¿™æœ‰æœ›æ”¯æŒè®¸å¤šæ–°å…´åº”ç”¨ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°ç¥ç»ç½‘ç»œã€åŠ¨æ€ç½‘æ ¼ã€éšå¼è¡¨ç¤ºã€ç¥ç»è¾å°„åœºç­‰å¤šä¸ªæ–¹é¢ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„æ··åˆè¡¨ç¤ºâ€”â€”åŠ¨æ€å››é¢ä½“ï¼ˆDynTetï¼‰ï¼Œå¹¶å°†å…¶åº”ç”¨äºè¯´è¯äººå¤´éƒ¨åˆæˆä»»åŠ¡ä¸­ã€‚DynTet ç»“åˆäº†æ˜¾å¼åŠ¨æ€ç½‘æ ¼å’Œéšå¼è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œèƒ½å¤Ÿç”Ÿæˆé€¼çœŸä¸”å¯åŠ¨ç”»çš„å¤´éƒ¨å¤´åƒã€‚ä½œè€…è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç»å…¸çš„ 3D å¯å˜å½¢æ¨¡å‹å’Œå¯å¾®æ¸²æŸ“å™¨ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆè§†é¢‘ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œä½†æå‡ºçš„æ–¹æ³•æ–°é¢–æœ‰æ•ˆï¼Œåœ¨è¯´è¯äººå¤´éƒ¨åˆæˆé¢†åŸŸå…·æœ‰é‡è¦çš„æ„ä¹‰ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2927e4da13bb2db0a8c147b32e65c4ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a69eb8d9ee3b7163b0dd216926919257.jpg" align="middle"><img src="https://picx.zhimg.com/v2-989288a0ad24820fe95020a4ed1f2ea7.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-29  G4GA Generic Framework for High Fidelity Talking Face Generation with   Fine-grained Intra-modal Alignment</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/29/Paper/2024-02-29/Diffusion%20Models/</id>
    <published>2024-02-29T12:37:28.000Z</published>
    <updated>2024-02-29T12:37:28.331Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-29-æ›´æ–°"><a href="#2024-02-29-æ›´æ–°" class="headerlink" title="2024-02-29 æ›´æ–°"></a>2024-02-29 æ›´æ–°</h1><h2 id="Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model"><a href="#Objective-and-Interpretable-Breast-Cosmesis-Evaluation-with-Attention-Guided-Denoising-Diffusion-Anomaly-Detection-Model" class="headerlink" title="Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model"></a>Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</h2><p><strong>Authors:Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun</strong></p><p>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patientsâ€™ quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising Diffusion Anomaly Detection (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional supervised learning and existing anomaly detection models. Our approach leverages the attention mechanism of the distillation with no label (DINO) self-supervised Vision Transformer (ViT) in combination with a diffusion model to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the diffusion model on unlabeled data predominantly with normal cosmesis, we adopt an unsupervised anomaly detection perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our anomaly detection model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in unsupervised anomaly detection within the medical domain, thereby paving the way for future investigations. </p><p><a href="http://arxiv.org/abs/2402.18362v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨æ— äººç›‘ç£æ–¹æ³•ï¼Œè‡ªåŠ¨è¯„ä¼°ä¹³è…ºç™Œæœ¯åå¤–è§‚ï¼Œä¸ºæé«˜æ‚£è€…ç”Ÿæ´»è´¨é‡æä¾›æ–°é€”å¾„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹è§†è§’ï¼Œæ— éœ€æ ‡è®°å³å¯è¯„ä¼°å¤–è§‚ã€‚</li><li>ä½¿ç”¨è’¸é¦æ— æ ‡ç­¾ (DINO) è‡ªç›‘ç£è§†è§‰ Transformer (ViT) çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°é«˜è´¨é‡å›¾åƒé‡å»ºå’Œåˆ¤åˆ«åŒºåŸŸçš„ç²¾ç¡®è½¬æ¢ã€‚</li><li>åœ¨ä»¥æ­£å¸¸å¤–è§‚ä¸ºä¸»çš„æœªæ ‡è®°æ•°æ®ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>æä¾›è§†è§‰ä¸Šå¸å¼•äººçš„è¡¨ç¤ºå’Œå¯é‡åŒ–çš„åˆ†æ•°ï¼Œç”¨äºå¤–è§‚è¯„ä¼°ã€‚</li><li>æ¶ˆé™¤äººå·¥æ ‡æ³¨çš„éœ€è¦ï¼Œæä¾›å®¢è§‚è¯„ä¼°ã€‚</li><li>åœ¨å‡†ç¡®æ€§æ–¹é¢è¶…è¿‡ç°æœ‰æ¨¡å‹ï¼Œå±•ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>ä¸ºåŒ»å­¦é¢†åŸŸçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æä¾›äº†é‡å¤§è¿›å±•ã€‚</li><li>æ¢ç´¢æ— ç›‘ç£å¤–è§‚è¯„ä¼°åœ¨å…¶ä»–åŒ»ç–—é¢†åŸŸçš„æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>Title: åŸºäºæ³¨æ„åŠ›å¼•å¯¼å»å™ªæ‰©æ•£çš„å®¢è§‚å¯è§£é‡Šä¹³æˆ¿ç¾è§‚è¯„ä¼°</li><li>Authors: Sangjoon Park, YongBae Kim, JeeSuk Chang, SeoHee Choi, Hyungjin Chung, IkJae Lee, HwaKyung Byun</li><li>Affiliation: éŸ©å›½é¦–å°”å»¶ä¸–å¤§å­¦åŒ»å­¦é™¢æ”¾å°„è‚¿ç˜¤ç§‘</li><li>Keywords: æ‰©æ•£æ¨¡å‹ã€å¼‚å¸¸æ£€æµ‹ã€è§†è§‰ Transformerã€ä¹³æˆ¿ç¾è§‚</li><li>Urls: Paper, Github: None</li><li><p>Summary:(1): ä¹³æˆ¿ç™Œæœ¯åç¾è§‚è¯„ä¼°å¯¹æ‚£è€…ç”Ÿæ´»è´¨é‡å½±å“å¾ˆå¤§ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨ä¸»è§‚æ€§å¼ºã€ä¾èµ–äººå·¥æ ‡æ³¨ç­‰é—®é¢˜ã€‚(2): ç°æœ‰æ–¹æ³•ä¾èµ–ä¸“å®¶æ ‡æ³¨ï¼Œå­˜åœ¨æˆæœ¬é«˜ã€æ ‡æ³¨åå·®ã€æ¨¡å‹è¿‡æ‹Ÿåˆã€å¯è§£é‡Šæ€§å·®ç­‰é—®é¢˜ã€‚(3): æœ¬æ–‡æå‡ºä¸€ç§åä¸º AG-DDAD çš„åˆ›æ–°æ¶æ„ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›å’Œ DINO è§†è§‰ Transformer æ³¨æ„åŠ›çš„æ˜¾è‘—ç‰¹å¾è¯†åˆ«èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å¯ä»¥åœ¨æ— ç›‘ç£çš„æ–¹å¼ä¸‹è®­ç»ƒï¼Œåˆ©ç”¨æ¥è‡ª 1,237 åä¸»è¦ä¸ºæ­£å¸¸ç¾è§‚ï¼ˆä¼˜ç§€åˆ°è‰¯å¥½ï¼‰æ‚£è€…çš„æœªæ ‡è®°æ•°æ®ï¼Œæ— éœ€ä¸“å®¶æ ‡æ³¨å’Œäººå·¥å‹¾å‹’ã€‚AG-DDAD é€šè¿‡æ¯”è¾ƒæ­£å¸¸ç¾è§‚æƒ…å†µä¸‹é¢„æœŸçš„æ¢å¤ç»“æœï¼Œæä¾›ç›´æ¥ä¸”å‡ºè‰²çš„å¯è§†åŒ–ç»“æœï¼Œä¸ºä¸è‰¯ç¾è§‚æˆå› æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚(4): åœ¨ä¸€ä¸ªç»è¿‡ç²¾å¿ƒæ•´ç†çš„åŒ…å« 300 åæ¥å—ä¹³è…ºç™Œä¿ä¹³æ‰‹æœ¯æ‚£è€…çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ¨¡å‹ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•å’Œå…¶ä»–æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§åä¸º AG-DDAD çš„åˆ›æ–°æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›å’Œ DINO è§†è§‰ Transformer æ³¨æ„åŠ›çš„æ˜¾è‘—ç‰¹å¾è¯†åˆ«èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰ï¼šAG-DDAD åœ¨æ— ç›‘ç£çš„æ–¹å¼ä¸‹è®­ç»ƒï¼Œåˆ©ç”¨æ¥è‡ªä¸»è¦ä¸ºæ­£å¸¸ç¾è§‚ï¼ˆä¼˜ç§€åˆ°è‰¯å¥½ï¼‰æ‚£è€…çš„æœªæ ‡è®°æ•°æ®ï¼Œæ— éœ€ä¸“å®¶æ ‡æ³¨å’Œäººå·¥å‹¾å‹’ï¼›ï¼ˆ3ï¼‰ï¼šAG-DDAD é€šè¿‡æ¯”è¾ƒæ­£å¸¸ç¾è§‚æƒ…å†µä¸‹é¢„æœŸçš„æ¢å¤ç»“æœï¼Œæä¾›ç›´æ¥ä¸”å‡ºè‰²çš„å¯è§†åŒ–ç»“æœï¼Œä¸ºä¸è‰¯ç¾è§‚æˆå› æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›å¼•å¯¼å»å™ªæ‰©æ•£çš„å®¢è§‚å¯è§£é‡Šä¹³æˆ¿ç¾è§‚è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›å’ŒDINOè§†è§‰Transformeræ³¨æ„åŠ›çš„æ˜¾è‘—ç‰¹å¾è¯†åˆ«èƒ½åŠ›ï¼Œåœ¨æ— ç›‘ç£çš„æ–¹å¼ä¸‹è®­ç»ƒï¼Œæ— éœ€ä¸“å®¶æ ‡æ³¨å’Œäººå·¥å‹¾å‹’ï¼Œé€šè¿‡æ¯”è¾ƒæ­£å¸¸ç¾è§‚æƒ…å†µä¸‹é¢„æœŸçš„æ¢å¤ç»“æœï¼Œæä¾›ç›´æ¥ä¸”å‡ºè‰²çš„å¯è§†åŒ–ç»“æœï¼Œä¸ºä¸è‰¯ç¾è§‚æˆå› æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›å¼•å¯¼å»å™ªæ‰©æ•£çš„å®¢è§‚å¯è§£é‡Šä¹³æˆ¿ç¾è§‚è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡ç”Ÿæˆèƒ½åŠ›å’ŒDINOè§†è§‰Transformeræ³¨æ„åŠ›çš„æ˜¾è‘—ç‰¹å¾è¯†åˆ«èƒ½åŠ›ã€‚</li><li>è¯¥æ–¹æ³•åœ¨æ— ç›‘ç£çš„æ–¹å¼ä¸‹è®­ç»ƒï¼Œæ— éœ€ä¸“å®¶æ ‡æ³¨å’Œäººå·¥å‹¾å‹’ï¼Œé€šè¿‡æ¯”è¾ƒæ­£å¸¸ç¾è§‚æƒ…å†µä¸‹é¢„æœŸçš„æ¢å¤ç»“æœï¼Œæä¾›ç›´æ¥ä¸”å‡ºè‰²çš„å¯è§†åŒ–ç»“æœï¼Œä¸ºä¸è‰¯ç¾è§‚æˆå› æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸€ä¸ªç»è¿‡ç²¾å¿ƒæ•´ç†çš„åŒ…å«300åæ¥å—ä¹³è…ºç™Œä¿ä¹³æ‰‹æœ¯æ‚£è€…çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ¨¡å‹ä¼˜äºä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•å’Œå…¶ä»–æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æ¯”ä¼ ç»Ÿçš„åˆ†ç±»å™¨æ¨¡å‹ç•¥å¤šçš„æ—¶é—´ï¼Œè¯„ä¼°å•ä¸ªæ‚£è€…çš„ç¾è§‚å¤§çº¦éœ€è¦15ç§’ï¼Œè€Œç®€å•çš„åˆ†ç±»å™¨æ¨¡å‹å¯ä»¥åœ¨1ç§’å†…äº§ç”Ÿç»“æœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-678c2254dd6a3d39889bef35f9067c05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cfa8a6039aebee57a2721ad761165bd3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6811aab9ac5a0e1edc535c928e3bd0f.jpg" align="middle"></details><h2 id="FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes"><a href="#FineDiffusion-Scaling-up-Diffusion-Models-for-Fine-grained-Image-Generation-with-10-000-Classes" class="headerlink" title="FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes"></a>FineDiffusion: Scaling up Diffusion Models for Fine-grained Image   Generation with 10,000 Classes</h2><p><strong>Authors:Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</strong></p><p>The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layersâ€™ parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: <a href="https://finediffusion.github.io/">https://finediffusion.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.18331v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥å‚æ•°é«˜æ•ˆç­–ç•¥å®ç°é’ˆå¯¹ 10,000 ä¸ªç»†ç²’åº¦ç±»åˆ«çš„å¤§è§„æ¨¡å›¾åƒç”Ÿæˆ</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º FineDiffusionï¼Œå°†å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ç¼©å°åˆ°ç»†ç²’åº¦å›¾åƒç”Ÿæˆä¸­</li><li>åªå¾®è°ƒåˆ†ç±»åµŒå…¥ã€åç½®é¡¹å’Œå½’ä¸€åŒ–å±‚çš„å‚æ•°ï¼Œå¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦å’Œå­˜å‚¨æ•ˆç‡</li><li>æå‡ºé’ˆå¯¹ç»†ç²’åº¦ç±»åˆ«çš„è¶…ç±»æ¡ä»¶å¼•å¯¼é‡‡æ ·æ–¹æ³•ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡</li><li>ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒFineDiffusion è®­ç»ƒé€Ÿåº¦æå‡ 1.56 å€ï¼Œæ‰€éœ€å­˜å‚¨å‚æ•°ä»…ä¸ºåŸæ¨¡å‹çš„ 1.77%</li><li>åœ¨ 10,000 ä¸ªç±»åˆ«çš„å›¾åƒç”Ÿæˆä¸Šå–å¾—æœ€å…ˆè¿›çš„ FID ä¸º 9.776</li><li>å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šFineDiffusionï¼šå°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ° 10,000 ç±»åˆ«çš„ç»†ç²’åº¦å›¾åƒç”Ÿæˆ</li><li>ä½œè€…ï¼šZiying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¦é—¨å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion Models, Fine-grained Image Generation, Parameter-efficient Fine-tuning</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.18331   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆä»¥äº§ç”Ÿé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å›¾åƒè€Œé—»åã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨ä¸ºä¸€èˆ¬ç±»åˆ«ç”Ÿæˆå›¾åƒï¼Œä¾‹å¦‚ ImageNet-1k ä¸­çš„ 1000 ä¸ªç±»åˆ«ã€‚ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå³å¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆï¼Œä»ç„¶æ˜¯éœ€è¦æ¢ç´¢çš„è¾¹ç•Œã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¸€èˆ¬ç±»åˆ«çš„å›¾åƒç”Ÿæˆï¼Œè€Œå¯¹äºç»†ç²’åº¦å›¾åƒç”Ÿæˆï¼Œéœ€è¦æ¨¡å‹å¯¹é«˜åº¦ç›¸ä¼¼çš„ç»†ç²’åº¦ç±»åˆ«ä¸­çš„ç»†å¾®å·®å¼‚ï¼ˆä¾‹å¦‚é¸Ÿç±»çš„ç¾½æ¯›çº¹ç†ï¼‰è¿›è¡Œå¤æ‚çš„å»ºæ¨¡ã€‚ä»å¤´å¼€å§‹è®­ç»ƒç”¨äºå¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹éœ€è¦æ›´å¤§çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒè¿­ä»£ã€‚   ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³• FineDiffusionï¼Œå®ƒå¯ä»¥é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œæœ‰æ•ˆåœ°å¾®è°ƒå¤§å‹é¢„è®­ç»ƒå›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒFineDiffusion å®ç°äº†æ˜¾ç€çš„ 1.56 å€è®­ç»ƒåŠ é€Ÿï¼Œå¹¶ä¸”åªéœ€è¦å­˜å‚¨ 1.77% çš„æ€»æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶åœ¨ 10,000 ä¸ªç±»åˆ«çš„å›¾åƒç”Ÿæˆä¸Šå®ç°äº† 9.776 çš„æœ€å…ˆè¿› FIDã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•FineDiffusionï¼Œè¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œæœ‰æ•ˆåœ°å¾®è°ƒå¤§å‹é¢„è®­ç»ƒå›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆï¼›(2)æå‡ºäº†ä¸€ç§åˆ†å±‚ç±»æ ‡ç­¾ç¼–ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒæ—¶å¯¹è¶…ç±»å’Œå­ç±»æ ‡ç­¾è¿›è¡Œç¼–ç ï¼›(3)åŒæ—¶å¾®è°ƒåå·®å’Œå½’ä¸€åŒ–é¡¹ä»¥åŠåˆ†å±‚åµŒå…¥å™¨ï¼Œä»¥å­¦ä¹ å…¨å±€æ•°æ®é›†çš„åˆ†å¸ƒç‰¹å¾ï¼›(4)å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ— åˆ†ç±»å™¨å¼•å¯¼é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¶…ç±»æ¡ä»¶ä¿¡æ¯æ¥å¢å¼ºå¯¹ç”Ÿæˆå›¾åƒçš„æ§åˆ¶ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é¦–æ¬¡å°è¯•å°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ° 10,000 ç±»çš„ç»†ç²’åº¦å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº† FineDiffusionï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„å…³é”®ç»„ä»¶ï¼ŒåŒ…æ‹¬åˆ†å±‚æ ‡ç­¾åµŒå…¥ã€åå·®é¡¹å’Œå½’ä¸€åŒ–é¡¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…å‡å°‘äº†è®­ç»ƒå’Œå­˜å‚¨å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»†ç²’åº¦æ— åˆ†ç±»å™¨å¼•å¯¼é‡‡æ ·æŠ€æœ¯ï¼Œåˆ©ç”¨åˆ†å±‚æ•°æ®æ ‡ç­¾ä¿¡æ¯æ¥æœ‰æ•ˆå¢å¼ºç»†ç²’åº¦å›¾åƒç”Ÿæˆçš„æ€§èƒ½ã€‚å……åˆ†çš„å®šæ€§å’Œå®šé‡ç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”çš„ä¼˜è¶Šæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³• FineDiffusionï¼Œè¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œæœ‰æ•ˆåœ°å¾®è°ƒå¤§å‹é¢„è®­ç»ƒå›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œä»¥è¿›è¡Œå¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆï¼›æå‡ºäº†åˆ†å±‚ç±»æ ‡ç­¾ç¼–ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŒæ—¶å¯¹è¶…ç±»å’Œå­ç±»æ ‡ç­¾è¿›è¡Œç¼–ç ï¼›åŒæ—¶å¾®è°ƒåå·®å’Œå½’ä¸€åŒ–é¡¹ä»¥åŠåˆ†å±‚åµŒå…¥å™¨ï¼Œä»¥å­¦ä¹ å…¨å±€æ•°æ®é›†çš„åˆ†å¸ƒç‰¹å¾ï¼›å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ— åˆ†ç±»å™¨å¼•å¯¼é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¶…ç±»æ¡ä»¶ä¿¡æ¯æ¥å¢å¼ºå¯¹ç”Ÿæˆå›¾åƒçš„æ§åˆ¶ã€‚æ€§èƒ½ï¼šä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼ŒFineDiffusion å®ç°äº†æ˜¾ç€çš„ 1.56 å€è®­ç»ƒåŠ é€Ÿï¼Œå¹¶ä¸”åªéœ€è¦å­˜å‚¨ 1.77% çš„æ€»æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶åœ¨ 10,000 ä¸ªç±»åˆ«çš„å›¾åƒç”Ÿæˆä¸Šå®ç°äº† 9.776 çš„æœ€å…ˆè¿› FIDã€‚å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚å·¥ä½œé‡ï¼šä¸ä»å¤´å¼€å§‹è®­ç»ƒç”¨äºå¤§è§„æ¨¡ç»†ç²’åº¦å›¾åƒç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒFineDiffusion å¯ä»¥æ˜¾ç€å‡å°‘è®¡ç®—èµ„æºå’Œè®­ç»ƒè¿­ä»£ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f68a4db99ea4f9179538c6c4b4d7c7ce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e768fecf2a73ce9e4c8b13ef7c8cd6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c0d4b61db744892b76754513d9f6676.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-665dc312a2eacee1bb375efacd7d609c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d25afe2f19082c3abc80d90affd76466.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68e2a9d895710b3df489a49501a85625.jpg" align="middle"></details><h2 id="Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models"><a href="#Balancing-Act-Distribution-Guided-Debiasing-in-Diffusion-Models" class="headerlink" title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models"></a>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</h2><p><strong>Authors:Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</strong></p><p>Diffusion Models (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for data augmentation and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional data or model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional diffusion models. Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data. </p><p><a href="http://arxiv.org/abs/2402.18206v1">PDF</a> CVPR 2024. Project Page : <a href="https://ab-34.github.io/balancing_act/">https://ab-34.github.io/balancing_act/</a></p><p><strong>Summary</strong><br>å»é™¤æ‰©æ•£æ¨¡å‹ä¸­çš„åè§ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å­˜åœ¨åè§ï¼Œè¡¨ç°ä¸ºå¯¹ç‰¹å®šäººå£äºšç»„ï¼ˆå¦‚å¥³æ€§ï¼‰çš„åå¥½ã€‚</li><li>åˆ†å¸ƒå¼•å¯¼æ˜¯ä¸€ç§æ— å DM çš„æ–¹æ³•ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–é‡æ–°è®­ç»ƒã€‚</li><li>åˆ†å¸ƒå¼•å¯¼åˆ©ç”¨å»å™ª UNet çš„æ½œåœ¨ç‰¹å¾ä¸­ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>å±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ (ADP) å°†æ½œåœ¨ç‰¹å¾æ˜ å°„åˆ°å±æ€§åˆ†å¸ƒã€‚</li><li>ADP ä½¿ç”¨ç°æœ‰å±æ€§åˆ†ç±»å™¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚</li><li>åˆ†å¸ƒå¼•å¯¼å’Œ ADP å®ç°äº†å…¬å¹³ç”Ÿæˆï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</li><li>é€šè¿‡ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®é‡æ–°å¹³è¡¡è®­ç»ƒé›†ï¼Œå¯ä»¥è®­ç»ƒå…¬å¹³çš„å±æ€§åˆ†ç±»å™¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå¹³è¡¡è¡Œä¸ºï¼šæ‰©æ•£æ¨¡å‹ä¸­çš„åˆ†å¸ƒå¼•å¯¼å»å</li><li>ä½œè€…ï¼šRishubh Parihar*, Abhijnya Bhatâˆ—, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu</li><li>éš¶å±ï¼šå°åº¦ç§‘å­¦é™¢ï¼Œç­åŠ ç½—å°”</li><li>å…³é”®è¯ï¼šDiffusion Models, Debiasing, Distribution Guidance, Attribute Distribution Predictor, Fair Generation</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.18206</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¼šåæ˜ è®­ç»ƒæ•°æ®é›†ä¸­çš„åè§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºäººè„¸ï¼ŒDM åå¥½æŸäº›äººå£ç»Ÿè®¡å­¦äºšç»„ï¼ˆä¾‹å¦‚å¥³æ€§æ¯”ç”·æ€§ï¼‰ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰å»åæ–¹æ³•éœ€è¦é¢å¤–æ•°æ®æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºåˆ†å¸ƒå¼•å¯¼ï¼Œé€šè¿‡å¼ºåˆ¶ç”Ÿæˆå›¾åƒéµå¾ªè§„å®šçš„å±æ€§åˆ†å¸ƒæ¥å¯¹ DM è¿›è¡Œå»åã€‚é€šè¿‡è®­ç»ƒå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ (ADP) æ¥æ˜ å°„æ½œåœ¨ç‰¹å¾åˆ°å±æ€§åˆ†å¸ƒï¼ŒADP ä½¿ç”¨ç°æœ‰å±æ€§åˆ†ç±»å™¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨æ— æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šå‡å°‘äº†å•ä¸€/å¤šå±æ€§çš„åå·®ï¼Œå¹¶ä¸”ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§é€šè¿‡ä½¿ç”¨ç”Ÿæˆæ•°æ®é‡æ–°å¹³è¡¡è®­ç»ƒé›†æ¥è®­ç»ƒå…¬å¹³å±æ€§åˆ†ç±»å™¨çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºåˆ†å¸ƒå¼•å¯¼æ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ç”Ÿæˆå›¾åƒéµå¾ªè§„å®šçš„å±æ€§åˆ†å¸ƒæ¥å¯¹æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰è¿›è¡Œå»åã€‚(2): è®­ç»ƒå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼ˆADPï¼‰æ¥æ˜ å°„æ½œåœ¨ç‰¹å¾åˆ°å±æ€§åˆ†å¸ƒï¼ŒADPä½¿ç”¨ç°æœ‰å±æ€§åˆ†ç±»å™¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚(3): åœ¨æ— æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼Œå‡å°‘äº†å•ä¸€/å¤šå±æ€§çš„åå·®ï¼Œå¹¶ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚(4): æå‡ºäº†ä¸€ç§é€šè¿‡ä½¿ç”¨ç”Ÿæˆæ•°æ®é‡æ–°å¹³è¡¡è®­ç»ƒé›†æ¥è®­ç»ƒå…¬å¹³å±æ€§åˆ†ç±»å™¨çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡çš„æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯å‡è½»é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åå·®çš„æ–¹æ³•ï¼Œä»…ç»™å®šæ‰€éœ€çš„å‚è€ƒå±æ€§åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å¸ƒå¼•å¯¼ï¼Œè”åˆå¼•å¯¼ä¸€æ‰¹å›¾åƒéµå¾ªå‚è€ƒå±æ€§åˆ†å¸ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”åœ¨ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼•å¯¼æ–¹æ³•ï¼Œé€šè¿‡å¼ºåˆ¶ç”Ÿæˆå›¾åƒéµå¾ªè§„å®šçš„å±æ€§åˆ†å¸ƒæ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå»åã€‚æ€§èƒ½ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨æ— æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸Šå‡å°‘äº†å•ä¸€/å¤šå±æ€§çš„åå·®ï¼Œå¹¶ä¸”ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨ï¼Œè¯¥é¢„æµ‹å™¨ä½¿ç”¨ç°æœ‰å±æ€§åˆ†ç±»å™¨ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒå±æ€§åˆ†å¸ƒé¢„æµ‹å™¨çš„å·¥ä½œé‡å–å†³äºè®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œå±æ€§çš„æ•°é‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-05a1a956ee3a51fe0c06ffc4859c7231.jpg" align="middle"><img src="https://picx.zhimg.com/v2-16ae5c5f9f522148622d40f8f3f15f86.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46f6a987113095ab338596820ca6e653.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f32e1f0036b8646f3ffad99a82575f09.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1128b65d6c33c58a2f6b04087adf31b0.jpg" align="middle"></details><h2 id="Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis"><a href="#Coarse-to-Fine-Latent-Diffusion-for-Pose-Guided-Person-Image-Synthesis" class="headerlink" title="Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis"></a>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</h2><p><strong>Authors:Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai</strong></p><p>Diffusion model is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence of image-caption pairs and textual prompts, we develop a novel training paradigm purely based on images to control the generation process of the pre-trained text-to-image diffusion model. A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained prompt. This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained prompt. Both quantitative and qualitative experimental results on the DeepFashion benchmark demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at <a href="https://github.com/YanzuoLu/CFLD">https://github.com/YanzuoLu/CFLD</a>. </p><p><a href="http://arxiv.org/abs/2402.18078v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br> æå‡ºäº†ä¸€ç§ç²—åˆ°ç»†çš„æ½œåœ¨æ‰©æ•£ï¼ˆCFLDï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒè€Œéæ–‡æœ¬æç¤ºï¼Œæ§åˆ¶é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°å§¿åŠ¿å¼•å¯¼çš„å›¾åƒåˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º CFLD æ–¹æ³•ï¼Œæ”¹å–„äº† PGPIS ä¸­å§¿åŠ¿å¼•å¯¼å›¾åƒåˆæˆçš„æ•ˆæœã€‚</li><li>ä½¿ç”¨çº¯å›¾åƒè®­ç»ƒèŒƒå¼ï¼Œæ— éœ€å›¾åƒå­—å¹•æˆ–æ–‡æœ¬æç¤ºã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥ç²¾ç‚¼è§£ç å™¨ï¼Œé€æ­¥ä¼˜åŒ–æŸ¥è¯¢å¹¶æå–äººç‰©å›¾åƒçš„è¯­ä¹‰ç†è§£ã€‚</li><li>å°†å¤–è²Œå’Œå§¿åŠ¿ä¿¡æ¯æ§åˆ¶è§£è€¦ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆã€‚</li><li>æå‡ºæ··åˆç²’åº¦æ³¨æ„åŠ›æ¨¡å—ï¼Œå¯¹å¤šå°ºåº¦å¤–è§‚ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œå¢å¼ºç²—ç²’åº¦æç¤ºã€‚</li><li>åœ¨ DeepFashion æ•°æ®é›†ä¸Šï¼Œå®šé‡å’Œå®šæ€§å®éªŒç»“æœè¯æ˜äº† CFLD çš„ä¼˜è¶Šæ€§ã€‚</li><li>ä»£ç å·²å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç²—åˆ°ç»†çš„æ½œåœ¨æ‰©æ•£ç”¨äºå§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆ</li><li>ä½œè€…ï¼šLu Yanzuo, Zhang Manlin, Ma Andy J, Xie Xiaohua, Lai Jianhuang</li><li>å•ä½ï¼šä¸­å±±å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå§¿æ€å¼•å¯¼ã€äººç‰©å›¾åƒåˆæˆã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€ç²—åˆ°ç»†ã€è¯­ä¹‰ç†è§£</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.18078   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/YanzuoLu/CFLD</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆæ—¨åœ¨å°†æºäººç‰©å›¾åƒè½¬æ¢ä¸ºç‰¹å®šçš„ç›®æ ‡å§¿æ€ï¼ŒåŒæ—¶å°½å¯èƒ½ä¿ç•™å¤–è§‚ã€‚å®ƒåœ¨ç”µå½±åˆ¶ä½œã€è™šæ‹Ÿç°å®å’Œæ—¶å°šç”µå­å•†åŠ¡ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) çš„æ–¹æ³•å®¹æ˜“å‡ºç°æå°æå¤§è®­ç»ƒç›®æ ‡çš„ä¸ç¨³å®šæ€§å’Œéš¾ä»¥åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„é—®é¢˜ã€‚ä½œä¸º GAN åœ¨å›¾åƒç”Ÿæˆä¸­çš„ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡ä¸€ç³»åˆ—å»å™ªæ­¥éª¤é€æ¸åˆæˆæ›´é€¼çœŸçš„å›¾åƒã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç²—åˆ°ç»†æ½œåœ¨æ‰©æ•£ (CFLD) æ–¹æ³•ç”¨äºå§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆã€‚åœ¨æ²¡æœ‰å›¾åƒ-æ ‡é¢˜å¯¹å’Œæ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§çº¯ç²¹åŸºäºå›¾åƒçš„æ–°é¢–è®­ç»ƒèŒƒå¼æ¥æ§åˆ¶é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥ç²¾ç‚¼è§£ç å™¨æ¥æ¸è¿›åœ°ç»†åŒ–ä¸€ç»„å¯å­¦ä¹ æŸ¥è¯¢å¹¶æå–äººç‰©å›¾åƒçš„è¯­ä¹‰ç†è§£ä½œä¸ºç²—ç²’åº¦æç¤ºã€‚è¿™å…è®¸åœ¨ä¸åŒçš„é˜¶æ®µè§£è€¦ç»†ç²’åº¦å¤–è§‚å’Œå§¿æ€ä¿¡æ¯æ§åˆ¶ï¼Œä»è€Œè§„é¿äº†æ½œåœ¨çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†ç”Ÿæˆæ›´é€¼çœŸçš„çº¹ç†ç»†èŠ‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆç²’åº¦æ³¨æ„åŠ›æ¨¡å—ï¼Œå°†å¤šå°ºåº¦ç»†ç²’åº¦å¤–è§‚ç‰¹å¾ç¼–ç ä¸ºåå·®é¡¹ä»¥å¢å¼ºç²—ç²’åº¦æç¤ºã€‚(4) æ€§èƒ½ï¼šåœ¨ DeepFashion åŸºå‡†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆå…·æœ‰æ›´å¥½æ³›åŒ–æ€§èƒ½çš„é«˜è´¨é‡å›¾åƒã€‚</li></ol><p>7.Methodsï¼š(1) æå‡ºç²—åˆ°ç»†æ½œåœ¨æ‰©æ•£ï¼ˆCFLDï¼‰æ–¹æ³•ï¼Œç”¨äºå§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆï¼›(2) å¼€å‘åŸºäºå›¾åƒçš„æ–°è®­ç»ƒèŒƒå¼ï¼Œæ§åˆ¶é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼›(3) è®¾è®¡æ„ŸçŸ¥ç²¾ç‚¼è§£ç å™¨ï¼Œæ¸è¿›ç»†åŒ–å¯å­¦ä¹ æŸ¥è¯¢ï¼Œæå–äººç‰©å›¾åƒè¯­ä¹‰ç†è§£ä½œä¸ºç²—ç²’åº¦æç¤ºï¼›(4) æå‡ºæ··åˆç²’åº¦æ³¨æ„åŠ›æ¨¡å—ï¼Œå°†å¤šå°ºåº¦ç»†ç²’åº¦å¤–è§‚ç‰¹å¾ç¼–ç ä¸ºåå·®é¡¹ï¼Œå¢å¼ºç²—ç²’åº¦æç¤ºï¼›(5) é€šè¿‡åœ¨DeepFashionåŸºå‡†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒï¼ŒéªŒè¯äº†CFLDæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰xxxï¼›ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆ2ï¼‰ä»åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ä¸‰ä¸ªç»´åº¦æ€»ç»“æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç²—åˆ°ç»†æ½œåœ¨æ‰©æ•£ï¼ˆCFLDï¼‰æ–¹æ³•ï¼Œç”¨äºå§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆã€‚è¯¥æ–¹æ³•é€šè¿‡æ¸è¿›ç»†åŒ–å¯å­¦ä¹ æŸ¥è¯¢ï¼Œæå–äººç‰©å›¾åƒçš„è¯­ä¹‰ç†è§£ä½œä¸ºç²—ç²’åº¦æç¤ºï¼Œå¹¶æå‡ºæ··åˆç²’åº¦æ³¨æ„åŠ›æ¨¡å—ï¼Œå°†å¤šå°ºåº¦ç»†ç²’åº¦å¤–è§‚ç‰¹å¾ç¼–ç ä¸ºåå·®é¡¹ï¼Œå¢å¼ºç²—ç²’åº¦æç¤ºã€‚æ€§èƒ½ï¼šåœ¨ DeepFashion åŸºå‡†ä¸Šçš„å®šé‡å’Œå®šæ€§å®éªŒç»“æœè¯æ˜äº† CFLD æ–¹æ³•åœ¨å§¿æ€å¼•å¯¼çš„äººç‰©å›¾åƒåˆæˆä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆå…·æœ‰æ›´å¥½æ³›åŒ–æ€§èƒ½çš„é«˜è´¨é‡å›¾åƒã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡é€‚ä¸­ã€‚è¯¥æ–¹æ³•çš„å®ç°éœ€è¦å¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•éœ€è¦è®¾è®¡æ„ŸçŸ¥ç²¾ç‚¼è§£ç å™¨å’Œæ··åˆç²’åº¦æ³¨æ„åŠ›æ¨¡å—ï¼Œè¿™éœ€è¦é¢å¤–çš„å¼€å‘å’Œå®éªŒå·¥ä½œã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ee807dc5573280abe63e138fa82f6eb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-07506917791ee3066c02770faa1a2052.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5192aaa635e4ab29d557ee967971be49.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-269e1bea1b870d8f0466ace81c9d2e01.jpg" align="middle"></details><h2 id="SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model"><a href="#SynArtifact-Classifying-and-Alleviating-Artifacts-in-Synthetic-Images-via-Vision-Language-Model" class="headerlink" title="SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model"></a>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images   via Vision-Language Model</h2><p><strong>Authors:Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</strong></p><p>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we fine-tune Vision-Language Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for fine-tuning VLM, named SynArtifact-1K. The fine-tuned VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined diffusion model has been obviously improved. </p><p><a href="http://arxiv.org/abs/2402.18068v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å›¾åƒåˆæˆä¸­çš„ä¼ªå½±è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ï¼Œä¸ºç”Ÿæˆæ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–æä¾›ç›‘ç®¡ï¼Œä»è€Œæé«˜åˆæˆå›¾åƒçš„è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆæˆå›¾åƒä¸­å¤æ‚ä¼ªå½±çš„å­˜åœ¨æ„æˆäº†ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œå¯¹æ„ŸçŸ¥çœŸå®æ€§äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚</li><li>ç ”ç©¶äººå‘˜æå‡ºå°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¾®è°ƒä¸ºä¼ªå½±åˆ†ç±»å™¨ï¼Œä»¥ä¾¿è‡ªåŠ¨è¯†åˆ«å’Œåˆ†ç±»å„ç§ä¼ªå½±ã€‚</li><li>å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„ä¼ªå½±åˆ†ç±»ä½“ç³»ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå…·æœ‰ä¼ªå½±æ³¨é‡Šçš„åˆæˆå›¾åƒæ•°æ®é›†ï¼ˆSynArtifact-1Kï¼‰ã€‚</li><li>å¾®è°ƒåçš„ VLM åœ¨è¯†åˆ«ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„èƒ½åŠ›ï¼Œæ¯”åŸºçº¿é«˜å‡º 25.66%ã€‚</li><li>è¿™æ˜¯é¦–æ¬¡æå‡ºæ­¤ç±»ç«¯åˆ°ç«¯ä¼ªå½±åˆ†ç±»ä»»åŠ¡å’Œè§£å†³æ–¹æ¡ˆã€‚</li><li>åˆ©ç”¨ VLM çš„è¾“å‡ºä½œä¸ºåé¦ˆæ¥ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ï¼Œä»¥å‡è½»ä¼ªå½±ã€‚</li><li>è§†è§‰åŒ–ç»“æœå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä¼˜åŒ–åçš„æ‰©æ•£æ¨¡å‹åˆæˆçš„å›¾åƒè´¨é‡å¾—åˆ°äº†æ˜æ˜¾æ”¹å–„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡é¢˜ç›®ï¼šSynArtifactï¼šé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å¯¹åˆæˆå›¾åƒä¸­çš„ä¼ªå½±è¿›è¡Œåˆ†ç±»å’Œæ¶ˆé™¤</li><li>ä½œè€…ï¼šBin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šåˆæˆå›¾åƒã€ä¼ªå½±ã€è§†è§‰è¯­è¨€æ¨¡å‹ã€ç”Ÿæˆæ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.18068</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šåˆæˆå›¾åƒä¸­å­˜åœ¨å¤æ‚ä¼ªå½±ï¼Œå½±å“å›¾åƒçš„æ„ŸçŸ¥çœŸå®æ€§ã€‚(2) è¿‡å¾€æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€è¯„åˆ†æŒ‡æ ‡ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ï¼Œæ— æ³•æœ‰æ•ˆåæ˜ ä¼ªå½±çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºä¸€ä¸ªç»¼åˆä¼ªå½±åˆ†ç±»æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰ä¼ªå½±æ³¨é‡Šçš„åˆæˆå›¾åƒæ•°æ®é›† SynArtifact-1Kï¼Œå¹¶å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ (VLM) å¯¹ä¼ªå½±è¿›è¡Œåˆ†ç±»ã€‚åˆ©ç”¨ VLM çš„è¾“å‡ºä½œä¸º AI åé¦ˆæ¥æ”¹è¿›ç”Ÿæˆæ¨¡å‹ï¼Œä»¥å‡è½»ä¼ªå½±ã€‚(4) å®éªŒç»“æœï¼šå¾®è°ƒåçš„ VLM åœ¨ä¼ªå½±åˆ†ç±»ä»»åŠ¡ä¸Šæ¯”åŸºçº¿æ–¹æ³•æé«˜äº† 25.66% çš„å‡†ç¡®ç‡å’Œ 29.01% çš„ F1 åˆ†æ•°ã€‚é€šè¿‡åˆ©ç”¨ä¼ªå½±åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸º AI åé¦ˆï¼Œå¯ä»¥æœ‰æ•ˆå‡è½»ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¼ªå½±ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ„å»ºç»¼åˆä¼ªå½±åˆ†ç±»æ³•ï¼Œå»ºç«‹åŒ…å«ä¼ªå½±æ³¨é‡Šçš„åˆæˆå›¾åƒæ•°æ®é›† SynArtifact-1Kï¼›ï¼ˆ2ï¼‰å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ VLMï¼Œå°†å…¶ä½œä¸ºä¼ªå½±åˆ†ç±»å™¨ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨ VLM è¾“å‡ºä½œä¸º AI åé¦ˆï¼Œè®¡ç®—ç”Ÿæˆæ¨¡å‹è¾“å‡ºä¸æ¯ç§ä¼ªå½±ä¹‹é—´çš„ BertScoreï¼Œä½œä¸ºä¼ªå½±åˆ†ç±»å¥–åŠ±ï¼›ï¼ˆ4ï¼‰é€šè¿‡æœ€å¤§åŒ–ä¼ªå½±åˆ†ç±»å¥–åŠ±ï¼Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä»¥å‡è½»ä¼ªå½±ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹åˆæˆå›¾åƒä¸­çš„ä¼ªå½±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ä¼ªå½±åˆ†ç±»æ³•ï¼Œæ„å»ºäº†åŒ…å«ä¼ªå½±æ³¨é‡Šçš„åˆæˆå›¾åƒæ•°æ®é›† SynArtifact-1Kï¼Œå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¯¹ä¼ªå½±è¿›è¡Œåˆ†ç±»ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¼ªå½±ï¼Œæå‡äº†åˆæˆå›¾åƒçš„æ„ŸçŸ¥çœŸå®æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æ„å»ºäº†åŒ…å« 13 ç§å¸¸è§ä¼ªå½±çš„ç»¼åˆä¼ªå½±åˆ†ç±»æ³•ã€‚</li><li>åˆ›å»ºäº†é¦–ä¸ªå¸¦æœ‰ä¼ªå½±ç±»åˆ«ã€æè¿°å’Œåæ ‡æ³¨é‡Šçš„å›¾åƒæ•°æ®é›† SynArtifact-1Kã€‚</li><li>å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ç”¨äºè‡ªåŠ¨åˆ†ç±»ä¼ªå½±ï¼Œå¹¶åˆ©ç”¨å…¶è¾“å‡ºä½œä¸º AI åé¦ˆæ¥ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹ã€‚æ€§èƒ½ï¼š</li><li>å¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¼ªå½±åˆ†ç±»ä»»åŠ¡ä¸Šæ¯”åŸºçº¿æ–¹æ³•æé«˜äº† 25.66% çš„å‡†ç¡®ç‡å’Œ 29.01% çš„ F1 åˆ†æ•°ã€‚</li><li>åˆ©ç”¨ä¼ªå½±åˆ†ç±»å™¨çš„è¾“å‡ºä½œä¸º AI åé¦ˆï¼Œå¯ä»¥æœ‰æ•ˆå‡è½»ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¼ªå½±ã€‚å·¥ä½œé‡ï¼š</li><li>æ„å»ºäº†åŒ…å« 1000 å¼ åˆæˆå›¾åƒçš„ SynArtifact-1K æ•°æ®é›†ã€‚</li><li>å¾®è°ƒäº†è§†è§‰è¯­è¨€æ¨¡å‹ç”¨äºä¼ªå½±åˆ†ç±»ã€‚</li><li>é€šè¿‡æœ€å¤§åŒ–ä¼ªå½±åˆ†ç±»å¥–åŠ±ï¼Œä¼˜åŒ–äº†æ‰©æ•£æ¨¡å‹ä»¥å‡è½»ä¼ªå½±ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-887bb2eb3bab7f102340a00fb115308a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67234ceff494848cb67aa7bc7345a5e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c0c890345f83368ccd384b81c55c4b11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48d8c1e1b56b76bfccfccfcb96c1d5a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a5599c3d37db39e68fa5fb2e0139cec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-94675e3c8e66717ee97bc9e3472ed274.jpg" align="middle"></details><h2 id="Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models"><a href="#Box-It-to-Bind-It-Unified-Layout-Control-and-Attribute-Binding-in-T2I-Diffusion-Models" class="headerlink" title="Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models"></a>Box It to Bind It: Unified Layout Control and Attribute Binding in T2I   Diffusion Models</h2><p><strong>Authors:Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</strong></p><p>While latent diffusion models (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module - a novel, training-free approach for improving spatial control and semantic accuracy in text-to-image (T2I) diffusion models. B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and ii) attribute binding, guaranteeing that generated objects adhere to their specified attributes in the prompt. B2B is designed as a compatible plug-and-play module for existing T2I models, markedly enhancing model performance in addressing the key challenges. We evaluate our technique using the established CompBench and TIFA score benchmarks, demonstrating significant performance improvements compared to existing methods. The source code will be made publicly available at <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a>. </p><p><a href="http://arxiv.org/abs/2402.17910v1">PDF</a> </p><p><strong>Summary</strong><br>Box-it-to-Bind-itï¼ˆB2Bï¼‰æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ¨¡å—ï¼Œå¯æé«˜æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­å›¾åƒçš„ç”Ÿæˆè´¨é‡ã€è¯­ä¹‰å‡†ç¡®åº¦å’Œç©ºé—´æ§åˆ¶èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>B2B æ¨¡å—å¯æ”¹å–„ T2I ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¾éš¾æ€§é—æ¼ã€å±æ€§ç»‘å®šå’Œå¸ƒå±€æŒ‡å¯¼ã€‚</li><li>B2B åŒ…æ‹¬ç”Ÿæˆå¯¹è±¡å’Œå±æ€§ç»‘å®šçš„ä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚</li><li>B2B å¯ä½œä¸ºç°æœ‰çš„ T2I æ¨¡å‹çš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€è®­ç»ƒã€‚</li><li>B2B åœ¨ CompBench å’Œ TIFA è¯„åˆ†åŸºå‡†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li><li>B2B çš„æºä»£ç å°†åœ¨ <a href="https://github.com/nextaistudio/BoxIt2BindIt">https://github.com/nextaistudio/BoxIt2BindIt</a> ä¸Šå…¬å¼€ã€‚</li><li>B2B æé«˜äº† LDM åœ¨ç”Ÿæˆå›¾åƒæ—¶çš„ç©ºé—´æ§åˆ¶å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚</li><li>B2B é€‚ç”¨äºä¸åŒçš„ T2I æ¨¡å‹ï¼Œæ˜“äºé›†æˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šBox-it-to-Bind-itï¼šç»Ÿä¸€å¸ƒå±€æ§åˆ¶å’Œå±æ€§ç»‘å®šåˆ° T2I æ‰©æ•£æ¨¡å‹ä¸­</li><li>ä½œè€…ï¼šAshkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿æ¾³å¤§åˆ©äºšå¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€ç©ºé—´æ§åˆ¶ã€å±æ€§ç»‘å®šã€å¸ƒå±€å¼•å¯¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17910</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶ç¼ºä¹è¯­ä¹‰ä¿çœŸåº¦å’Œç©ºé—´æ§åˆ¶ï¼Œéš¾ä»¥å¿ å®åœ°éµå¾ªç»™å®šçš„æç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è±¡å±æ€§å’Œå¯¹è±¡æ”¾ç½®æ–¹é¢ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•è¦ä¹ˆä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè¦ä¹ˆå¾®è°ƒç°æœ‰æ¨¡å‹ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¹¶é›†æˆç‰¹å¾çš„æ–¹æ³•è™½ç„¶ä¸éœ€è¦å¤§é‡è®­ç»ƒï¼Œä½†æ•ˆæœæœ‰é™ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„æ–¹æ³• Box-it-to-Bind-it (B2B)ï¼Œè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¾éš¾æ€§é—æ¼ã€å±æ€§ç»‘å®šå’Œå¸ƒå±€å¼•å¯¼ã€‚B2B åœ¨æ¨ç†é˜¶æ®µé€šè¿‡ä¸¤æ­¥å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç¼–ç ï¼šå¯¹è±¡ç”Ÿæˆå’Œå±æ€§ç»‘å®šã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ CompBench å’Œ TIFA å¾—åˆ†åŸºå‡†ä¸Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒB2B åœ¨è§£å†³å…³é”®æŒ‘æˆ˜æ–¹é¢æ˜¾ç€æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´æ§åˆ¶å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚</p><p>æ–¹æ³•ï¼š(1) B2Bæ˜¯ä¸€ç§å¥–åŠ±å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåœ¨æ¨ç†é˜¶æ®µé€šè¿‡ä¸¤æ­¥å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç¼–ç ï¼šå¯¹è±¡ç”Ÿæˆå’Œå±æ€§ç»‘å®šã€‚(2) å¯¹è±¡ç”Ÿæˆï¼šåŸºäºIoUï¼Œå¢åŠ å¯¹è±¡ç”Ÿæˆæ¦‚ç‡ï¼Œå°†æ³¨æ„åŠ›æƒé‡é›†ä¸­åœ¨ç»™å®šè¾¹ç•Œæ¡†å†…ï¼ŒåŒæ—¶æŠ‘åˆ¶è¾¹ç•Œæ¡†å¤–çš„æ³¨æ„åŠ›æƒé‡ã€‚(3) å±æ€§ç»‘å®šï¼šä½¿ç”¨KLæ•£åº¦æµ‹é‡å±æ€§æ¦‚ç‡åˆ†å¸ƒä¸å¯¹åº”å¯¹è±¡æ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œå‡å°‘å·®å¼‚ï¼Œå°†å±æ€§åˆ†å¸ƒå¼ºåˆ¶æ”¶æ•›åˆ°å„è‡ªçš„å¯¹è±¡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å±æ€§ç»‘å®šå’Œç©ºé—´æ§åˆ¶ï¼Œæå‡ºäº† B2B æ¨¡å‹ã€‚B2B é‡‡ç”¨ç”Ÿæˆå’Œç»‘å®šåŒæ¨¡å—ç³»ç»Ÿï¼Œæœ‰æ•ˆè§£å†³äº†ç¾éš¾æ€§é—æ¼ã€æé«˜å±æ€§ç»‘å®šç²¾åº¦å’Œç¡®ä¿å‡†ç¡®å¯¹è±¡æ”¾ç½®çš„é—®é¢˜ã€‚å®ƒä½œä¸ºç°æœ‰ T2I æ¡†æ¶çš„å³æ’å³ç”¨æ¨¡å—çš„å…¼å®¹æ€§é€šè¿‡å…¶åœ¨ CompBench å’Œ TIFA åŸºå‡†ä¸­çš„å‡ºè‰²è¡¨ç°å¾—åˆ°è¯æ˜ï¼Œæ ‡å¿—ç€ç”Ÿæˆå»ºæ¨¡çš„é‡å¤§é£è·ƒã€‚B2B çš„çªç ´å‡¸æ˜¾äº†å…¶ä½œä¸ºæœªæ¥ç ”ç©¶æ½œåœ¨æ ‡å‡†çš„ä½œç”¨ï¼Œä¸ºæ•°å­—æˆåƒå’Œç”Ÿæˆå¼ AI çš„åˆ›æ–°å‘å±•é“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§å…è®­ç»ƒçš„æ–¹æ³• B2Bï¼Œé€šè¿‡ä¸¤æ­¥å¼•å¯¼æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç¼–ç æ¥è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li><li>è®¾è®¡äº†å¯¹è±¡ç”Ÿæˆå’Œå±æ€§ç»‘å®šä¸¤ä¸ªæ¨¡å—ï¼Œæœ‰æ•ˆè§£å†³äº†ç¾éš¾æ€§é—æ¼ã€å±æ€§ç»‘å®šå’Œå¸ƒå±€å¼•å¯¼é—®é¢˜ã€‚</li><li>B2B ä½œä¸ºç°æœ‰ T2I æ¡†æ¶çš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ˜“äºé›†æˆå’Œä½¿ç”¨ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ CompBench å’Œ TIFA åŸºå‡†ä¸Šï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒB2B åœ¨è§£å†³å…³é”®æŒ‘æˆ˜æ–¹é¢æ˜¾ç€æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li><li>æ¶ˆèç ”ç©¶éªŒè¯äº†å¯¹è±¡ç”Ÿæˆå’Œå±æ€§ç»‘å®šå¥–åŠ±ç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ B2B çš„å„ä¸ªç»„ä»¶å¯¹æ•´ä½“æ€§èƒ½è‡³å…³é‡è¦ã€‚å·¥ä½œé‡ï¼š</li><li>B2B æ˜¯ä¸€ç§å…è®­ç»ƒçš„æ–¹æ³•ï¼Œä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æˆ–å¾®è°ƒç°æœ‰æ¨¡å‹ï¼Œä»è€ŒèŠ‚çœäº†å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li><li>B2B æ˜“äºé›†æˆåˆ°ç°æœ‰ T2I æ¡†æ¶ä¸­ï¼Œæ— éœ€è¿›è¡Œå¤æ‚çš„ä¿®æ”¹æˆ–é‡æ–°è®­ç»ƒï¼Œé™ä½äº†å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9044558cdc31309b419fea5199aa8a89.jpg" align="middle"><img src="https://picx.zhimg.com/v2-78bccd36910d4aa870962c445823ad57.jpg" align="middle"><img src="https://pica.zhimg.com/v2-967a215bde68183f03e457a7ff3f8e9a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e2a4cdc833464a14406a357aa9e0c358.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d140c3c8e05d724098a1c03138203a01.jpg" align="middle"></details><h2 id="Structure-Guided-Adversarial-Training-of-Diffusion-Models"><a href="#Structure-Guided-Adversarial-Training-of-Diffusion-Models" class="headerlink" title="Structure-Guided Adversarial Training of Diffusion Models"></a>Structure-Guided Adversarial Training of Diffusion Models</h2><p><strong>Authors:Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</strong></p><p>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively. </p><p><a href="http://arxiv.org/abs/2402.17563v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é€šè¿‡ç»“æ„å¯¹æŠ—è®­ç»ƒï¼Œå­¦ä¹ æ‰¹å†…æ ·æœ¬æµå½¢ç»“æ„ï¼Œæå‡å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰æ‰©æ•£æ¨¡å‹ä¸“æ³¨äºå•ä¸ªæ ·æœ¬çš„å»å™ªå¾—åˆ†åŒ¹é…æŸå¤±ä¼˜åŒ–ï¼Œå¿½è§†æ‰¹å†…æ ·æœ¬ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚</li><li>ç»“æ„å¯¹æŠ—è®­ç»ƒ (SADM) å¼•å…¥ç»“æ„é‰´åˆ«å™¨æ¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„æµå½¢ç»“æ„ã€‚</li><li>SADM è¿«ä½¿æ¨¡å‹å­¦ä¹ è®­ç»ƒæ‰¹æ¬¡ä¸­æ ·æœ¬ä¹‹é—´çš„æµå½¢ç»“æ„ã€‚</li><li>SADM ä¸æ‰©æ•£å˜å‹å™¨ (DiT) ç›¸ç»“åˆï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SADM åœ¨ 12 ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡çš„æœ€æ–° FID åˆ†åˆ«ä¸º 1.58 å’Œ 2.11ã€‚</li><li>SADM åœ¨ 256x256 å’Œ 512x512 åˆ†è¾¨ç‡ä¸‹ï¼Œåœ¨ ImageNet ä¸Šå®ç°äº†ç±»æ¡ä»¶å›¾åƒç”Ÿæˆçš„æœ€æ–° FID åˆ†åˆ«ä¸º 1.58 å’Œ 2.11ã€‚</li><li>SADM è¯æ˜äº†æµå½¢ç»“æ„å­¦ä¹ å¯¹äºæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ€§è®­ç»ƒ</li><li>ä½œè€…ï¼šæ¨å‡Œã€é’±æµ©å¤©ã€å¼ æ™ºé¾™ã€åˆ˜æ™¯ä¼Ÿã€å´”æ–Œ</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€ç»“æ„å¼•å¯¼ã€å¯¹æŠ—è®­ç»ƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17563   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æœ€å°åŒ–å»å™ªå¾—åˆ†åŒ¹é…æŸå¤±çš„åŠ æƒå’Œï¼Œè®­ç»ƒè¿‡ç¨‹ä¾§é‡äºå®ä¾‹çº§ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å°æ‰¹é‡æ ·æœ¬ä¹‹é—´çš„å®è´µç»“æ„ä¿¡æ¯ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å®ä¾‹çº§ä¼˜åŒ–ï¼Œå¿½ç•¥äº†å°æ‰¹é‡æ ·æœ¬ä¹‹é—´çš„ç»“æ„ä¿¡æ¯ï¼Œå¯¼è‡´æ— æ³•å……åˆ†å»ºæ¨¡æ•°æ®åˆ†å¸ƒã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ€§è®­ç»ƒï¼ˆSADMï¼‰ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒæŒ‡å¯¼æ‰©æ•£ç”Ÿæˆå™¨å­¦ä¹ å°æ‰¹é‡æ ·æœ¬ä¹‹é—´çš„æµå½¢ç»“æ„ã€‚å¼•å…¥ç»“æ„åˆ¤åˆ«å™¨æ¥åŒºåˆ†çœŸå®æµå½¢ç»“æ„å’Œç”Ÿæˆæµå½¢ç»“æ„ï¼Œç¡®ä¿æ¨¡å‹æ•è·æ•°æ®åˆ†å¸ƒä¸­çš„çœŸå®æµå½¢ç»“æ„ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šSADM æ˜¾è‘—æå‡äº†ç°æœ‰æ‰©æ•£ Transformerï¼Œåœ¨ 12 ä¸ªæ•°æ®é›†ä¸Šçš„å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ ImageNet ä¸Šåˆ†åˆ«ä»¥ 256Ã—256 å’Œ 512Ã—512 çš„åˆ†è¾¨ç‡å®ç°äº† 1.58 å’Œ 2.11 çš„æ–° SOTA FIDï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p><p>7.Methodsï¼šï¼ˆ1ï¼‰æå‡ºç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ€§è®­ç»ƒï¼ˆSADMï¼‰ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒæŒ‡å¯¼æ‰©æ•£ç”Ÿæˆå™¨å­¦ä¹ å°æ‰¹é‡æ ·æœ¬ä¹‹é—´çš„æµå½¢ç»“æ„ï¼›ï¼ˆ2ï¼‰å¼•å…¥ç»“æ„åˆ¤åˆ«å™¨æ¥åŒºåˆ†çœŸå®æµå½¢ç»“æ„å’Œç”Ÿæˆæµå½¢ç»“æ„ï¼Œç¡®ä¿æ¨¡å‹æ•è·æ•°æ®åˆ†å¸ƒä¸­çš„çœŸå®æµå½¢ç»“æ„ï¼›ï¼ˆ3ï¼‰é‡‡ç”¨Wasserstein GANæŸå¤±å‡½æ•°ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨ç”Ÿæˆä¸çœŸå®æµå½¢ç»“æ„ç›¸ä¼¼çš„æ ·æœ¬ï¼›ï¼ˆ4ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº¤æ›¿æ›´æ–°ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼Œç›´è‡³è¾¾åˆ°çº³ä»€å‡è¡¡ï¼›ï¼ˆ5ï¼‰å°†SADMä¸æ‰©æ•£Transformerç›¸ç»“åˆï¼Œå½¢æˆæ›´å¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</p><ol><li>æ€»ç»“(1): æœ¬æ–‡æå‡ºäº†ä»ç»“æ„è§’åº¦ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„ç»“æ„å¼•å¯¼å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ³•ï¼Œè¯¥è®­ç»ƒç®—æ³•å¯ä»¥è½»æ¾æ¨å¹¿åˆ°å›¾åƒå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼å’Œå®éªŒç»“æœä¸€è‡´åœ°æ”¹è¿›äº†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ 12 ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„ SOTA æ€§èƒ½ã€‚å¯¹äºæœªæ¥çš„å·¥ä½œï¼Œæˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•åˆ°æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºäºæ‰©æ•£çš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆï¼‰ã€‚(2): åˆ›æ–°ç‚¹: æå‡ºç»“æ„å¼•å¯¼å¯¹æŠ—æ€§è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒæŒ‡å¯¼æ‰©æ•£ç”Ÿæˆå™¨å­¦ä¹ å°æ‰¹é‡æ ·æœ¬ä¹‹é—´çš„æµå½¢ç»“æ„ï¼Œä»è€Œæå‡æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚æ€§èƒ½: åœ¨ 12 ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å›¾åƒç”Ÿæˆå’Œè·¨åŸŸå¾®è°ƒä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„ SOTA æ€§èƒ½ï¼Œåœ¨ ImageNet ä¸Šåˆ†åˆ«ä»¥ 256Ã—256 å’Œ 512Ã—512 çš„åˆ†è¾¨ç‡å®ç°äº† 1.58 å’Œ 2.11 çš„æ–° SOTAFIDã€‚å·¥ä½œé‡: è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œå¯ä»¥è½»æ¾æ¨å¹¿åˆ°å›¾åƒå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-11a45496d9d4169c7ee0bbb4a6534ffa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4ae1e4da806d223271756f678f15ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02b820484fca35ffef9bc52706101c79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14ed9373ba8bdaf3ecaca75391245256.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75ca2aa69507bb15984388d3520039af.jpg" align="middle"></details><h2 id="Diffusion-Model-Based-Image-Editing-A-Survey"><a href="#Diffusion-Model-Based-Image-Editing-A-Survey" class="headerlink" title="Diffusion Model-Based Image Editing: A Survey"></a>Diffusion Model-Based Image Editing: A Survey</h2><p><strong>Authors:Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao</strong></p><p>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a>. </p><p><a href="http://arxiv.org/abs/2402.17525v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­åº”ç”¨å¹¿æ³›ï¼Œå¯ä»å¤æ‚åˆ†å¸ƒä¸­ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œä¸”æ”¯æŒæ— æ¡ä»¶å’Œè¾“å…¥æ¡ä»¶ä¸‹çš„å›¾åƒç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹é€šè¿‡å­¦ä¹ é€†è½¬å›¾åƒåŠ å™ªè¿‡ç¨‹ï¼Œç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚</li><li>æ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘æ–¹æ³•å¯åˆ†ä¸ºä¸åŒå­¦ä¹ ç­–ç•¥ã€ç”¨æˆ·è¾“å…¥æ¡ä»¶å’Œç¼–è¾‘ä»»åŠ¡ã€‚</li><li>å›¾åƒä¿®å¤å’Œå¤–å»¶å¯ä½¿ç”¨ä¼ ç»Ÿä¸Šä¸‹æ–‡é©±åŠ¨æ–¹æ³•æˆ–å¤šæ¨¡æ€æ¡ä»¶æ–¹æ³•ã€‚</li><li>æå‡º EditEval åŸºå‡†å’Œ LMM è¯„åˆ†ç”¨äºè¯„ä¼°æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘ç®—æ³•ã€‚</li><li>ç›®å‰å­˜åœ¨é™åˆ¶ï¼Œæœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬å¤šæ¨¡æ€ã€3D å’Œç¼–è¾‘å…ƒæ•°æ®ã€‚</li><li>å¯åœ¨ <a href="https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods">https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a> è·å–ç›¸å…³ä»£ç ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘ï¼šç»¼è¿°</li><li>ä½œè€…ï¼šYi Huangã€Jiancheng Huangã€Yifan Liuã€Mingfu Yanã€Jiaxi Lvã€Jianzhuang Liuã€Wei Xiongã€He Zhangã€Shifeng Chenã€Liangliang Cao</li><li>å•ä½ï¼šæ·±åœ³å…ˆè¿›æŠ€æœ¯ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šDiffusion Modelã€Image Editingã€AIGC</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17525Githubï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šéšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯çš„å‘å±•ï¼ŒAI ç”Ÿæˆçš„å†…å®¹ï¼ˆAIGCï¼‰é¢†åŸŸè“¬å‹ƒå‘å±•ï¼Œå›¾åƒç¼–è¾‘ä½œä¸ºå…¶ä¸­ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œåœ¨æ•°å­—åª’ä½“ã€å¹¿å‘Šå’Œç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚(2)ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ é€æ­¥ç»™å›¾åƒæ·»åŠ å™ªå£°å¹¶é€†è½¬è¿™ä¸€è¿‡ç¨‹ï¼Œå¯ä»¥ä»å¤æ‚åˆ†å¸ƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ã€‚(3)ï¼šæœ¬æ–‡å¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œä»å­¦ä¹ ç­–ç•¥ã€ç”¨æˆ·è¾“å…¥æ¡ä»¶å’Œå…·ä½“ç¼–è¾‘ä»»åŠ¡ç­‰å¤šä¸ªè§’åº¦å¯¹ç°æœ‰å·¥ä½œè¿›è¡Œäº†æ·±å…¥åˆ†æå’Œåˆ†ç±»ã€‚(4)ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å›¾åƒä¿®å¤ã€å›¾åƒå¤–å»¶ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„åŸºå‡† EditEval å’Œä¸€ä¸ªåˆ›æ–°çš„æŒ‡æ ‡ LMMScore æ¥è¿›ä¸€æ­¥è¯„ä¼°æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç®—æ³•çš„æ€§èƒ½ã€‚</li></ol><p>7.Methods:(1): åŸºäºCLIPæŒ‡å¯¼çš„å›¾åƒç¼–è¾‘ï¼šDiffusionCLIPã€Asyrpã€EffDiffã€DiffStylerã€StyleDiffusionã€UNIT-DDPMã€CycleNetã€DiffusionAutoencodersã€HDAEã€EGSDEã€Pixel-GuidedDiffusionï¼›(2): åŸºäºå‚è€ƒå’Œå±æ€§æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘ï¼šPbEã€RICã€ObjectStitchã€PhDã€DreamInpainterã€Anydoorã€FADINGã€PAIRDiffusionã€SmartBrushã€IIR-Netï¼›(3): åŸºäºæŒ‡ä»¤æŒ‡å¯¼çš„å›¾åƒç¼–è¾‘ï¼šInstructPix2Pixã€MoEControllerã€FoIã€LOFIEã€InstructDiffusionã€EmuEditã€DialogPaintã€Inst-Inpaintã€HIVEã€ImageBrushã€InstructAny2Pixã€MGIEã€SmartEditã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œå¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œä»å¤šä¸ªè§’åº¦å¯¹ç°æœ‰å·¥ä½œè¿›è¡Œäº†æ·±å…¥åˆ†æå’Œåˆ†ç±»ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„åŸºå‡† EditEval å’Œä¸€ä¸ªåˆ›æ–°çš„æŒ‡æ ‡ LMMScore æ¥è¿›ä¸€æ­¥è¯„ä¼°æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç®—æ³•çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¼–è¾‘åŸºå‡† EditEval å’Œä¸€ä¸ªåˆ›æ–°çš„æŒ‡æ ‡ LMMScoreï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç®—æ³•çš„æ€§èƒ½ã€‚</li><li>å¯¹åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°å’Œåˆ†ç±»ï¼Œä»å­¦ä¹ ç­–ç•¥ã€ç”¨æˆ·è¾“å…¥æ¡ä»¶å’Œå…·ä½“ç¼–è¾‘ä»»åŠ¡ç­‰å¤šä¸ªè§’åº¦å¯¹ç°æœ‰å·¥ä½œè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li><li>æ¢ç´¢äº†è¿™äº›æ–¹æ³•åœ¨å¢å¼ºç¼–è¾‘æ€§èƒ½æ–¹é¢çš„è´¡çŒ®ã€‚</li><li>åœ¨æˆ‘ä»¬çš„å›¾åƒç¼–è¾‘åŸºå‡† EditEval ä¸­å¯¹ 7 é¡¹ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥åŠæœ€æ–°æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>æ€»ç»“äº†å›¾åƒç¼–è¾‘é¢†åŸŸçš„å¹¿æ³›æ½œåŠ›ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</li><li>æ€§èƒ½ï¼šåœ¨ EditEval åŸºå‡†ä¸Šï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å›¾åƒä¿®å¤ã€å›¾åƒå¤–å»¶ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚</li><li>å·¥ä½œé‡ï¼šæœ¬æ–‡å¯¹è¶…è¿‡ 100 ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•è¿›è¡Œäº†ç»¼è¿°å’Œåˆ†ç±»ï¼Œå¹¶å¯¹ 7 é¡¹ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4c52565ddb49dad37f10475b00a6abbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4537d5996d9b29f71e82d00a227227b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db76ba27193f9ab6b62bab161a239510.jpg" align="middle"></details><h2 id="Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network"><a href="#Enhancing-Hyperspectral-Images-via-Diffusion-Model-and-Group-Autoencoder-Super-resolution-Network" class="headerlink" title="Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network"></a>Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder   Super-resolution Network</h2><p><strong>Authors:Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong</strong></p><p>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically. </p><p><a href="http://arxiv.org/abs/2402.17285v1">PDF</a> Accepted by AAAI2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ä¸ç¾¤ç»„è‡ªç¼–ç å™¨ç›¸ç»“åˆçš„åˆ›æ–°æ¡†æ¶ï¼Œæœ‰æ•ˆæå‡é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œæ˜¾è‘—æ”¹å–„è°±ç©ºå…³ç³»å»ºæ¨¡å’Œä½å±‚ç»†èŠ‚æ¢å¤ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹æ“…é•¿å»ºæ¨¡å¤æ‚å…³ç³»å’Œå­¦ä¹ è§†è§‰ç‰¹å¾ï¼Œåœ¨é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­æ½œåŠ›å·¨å¤§ã€‚</li><li>è®­ç»ƒæ‰©æ•£æ¨¡å‹é¢ä¸´æ”¶æ•›å›°éš¾å’Œæ¨ç†æ—¶é—´é•¿æŒ‘æˆ˜ã€‚</li><li>ç¾¤ç»„è‡ªç¼–ç å™¨æ¡†æ¶é€šè¿‡å°†é«˜ç»´é«˜å…‰è°±æ•°æ®ç¼–ç åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç¼“è§£äº†æ‰©æ•£æ¨¡å‹è®­ç»ƒéš¾åº¦ï¼Œå¹¶ä¿æŒäº†æ³¢æ®µç›¸å…³æ€§ã€‚</li><li>æ‰©æ•£æ¨¡å‹ä¸ç¾¤ç»„è‡ªç¼–ç å™¨ç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†æ¨ç†æ—¶é—´é—®é¢˜ã€‚</li><li>åœ¨è‡ªç„¶å’Œé¥æ„Ÿé«˜å…‰è°±æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰å’Œåº¦é‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹å’Œç»„è‡ªç¼–ç å™¨çš„è¶…åˆ†è¾¨ç‡é«˜å…‰è°±å›¾åƒå¢å¼º</li><li>ä½œè€…ï¼šç‹å…†é˜³ï¼Œæä¸œé˜³ï¼Œå¼ æ˜é˜³ï¼Œç½—æµ©ï¼Œå·©èŒ‚å›½</li><li>éš¶å±å•ä½ï¼šè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ååŒæ™ºèƒ½ç³»ç»Ÿæ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šé«˜å…‰è°±å›¾åƒï¼Œè¶…åˆ†è¾¨ç‡ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç»„è‡ªç¼–ç å™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17285</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å¤æ‚çš„å…‰è°±-ç©ºé—´å…³ç³»å’Œä½çº§ç»†èŠ‚ï¼Œè€Œæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§æœ‰å‰é€”çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥å…¶åœ¨å»ºæ¨¡å¤æ‚å…³ç³»å’Œå­¦ä¹ é«˜ä½çº§è§†è§‰ç‰¹å¾æ–¹é¢çš„å‡ºè‰²æ€§èƒ½è€Œé—»åã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šå°†æ‰©æ•£æ¨¡å‹ç›´æ¥åº”ç”¨äºé«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡é¢ä¸´ç€æ¨¡å‹æ”¶æ•›å›°éš¾å’Œæ¨ç†æ—¶é—´é•¿çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç»„è‡ªç¼–ç å™¨ï¼ˆGAEï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸æ‰©æ•£æ¨¡å‹ååŒç»“åˆï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼ˆDMGASRï¼‰ã€‚æå‡ºçš„ GAE æ¡†æ¶å°†é«˜ç»´é«˜å…‰è°±æ•°æ®ç¼–ç ä¸ºä½ç»´æ½œåœ¨ç©ºé—´ï¼Œæ‰©æ•£æ¨¡å‹åœ¨æ­¤ç©ºé—´ä¸­å·¥ä½œï¼Œä»è€Œç¼“è§£äº†è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„éš¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æ³¢æ®µç›¸å…³æ€§å¹¶å¤§å¤§å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨è‡ªç„¶å’Œé¥æ„Ÿé«˜å…‰è°±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨è§†è§‰å’Œåº¦é‡ä¸Šéƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œç»„è‡ªç¼–ç å™¨çš„è¶…åˆ†è¾¨ç‡é«˜å…‰è°±å›¾åƒå¢å¼ºæ¨¡å‹ï¼ˆDMGASRï¼‰ï¼›(2): è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç¼–ç å™¨å’Œæ‰©æ•£è¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼›(3): é‡‡ç”¨è°±åˆ†ç»„ç­–ç•¥å’Œéå¯¹ç§°è§£ç å™¨è®¾è®¡ï¼Œæœ‰æ•ˆåœ°å°†é«˜ç»´é«˜å…‰è°±æ•°æ®ç¼–ç ä¸ºä½ç»´æ½œåœ¨ç©ºé—´ï¼›(4): è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œï¼Œç¼“è§£äº†è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„éš¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æ³¢æ®µç›¸å…³æ€§å¹¶å¤§å¤§å‡å°‘äº†æ¨ç†æ—¶é—´ï¼›(5): è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨é‡æ„è¾“å…¥æ•°æ®ï¼Œç”Ÿæˆä¸€ç³»åˆ—éšè—å˜é‡ï¼›(6): å°†ä½åˆ†è¾¨ç‡éšè—å˜é‡ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œä¸é«˜åˆ†è¾¨ç‡éšè—å˜é‡ä¸²è”ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼›(7): é‡‡ç”¨ U-Net ä½œä¸ºå»å™ªæ¨¡å‹ï¼Œè¿­ä»£å»é™¤å™ªå£°ï¼Œç”Ÿæˆè¶…åˆ†è¾¨ç‡æ½œåœ¨å˜é‡åˆ—è¡¨ï¼›(8): å°†è¶…åˆ†è¾¨ç‡æ½œåœ¨å˜é‡åˆ—è¡¨ä¼ é€’ç»™è§£ç å™¨ï¼Œç”Ÿæˆè¶…åˆ†è¾¨ç‡å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œç»„è‡ªç¼–ç å™¨çš„é«˜å…‰è°±å›¾åƒè¶…åˆ†è¾¨ç‡å¢å¼ºæ¨¡å‹ï¼ˆDMGASRï¼‰ï¼Œè¯¥æ¨¡å‹å°†æ‰©æ•£æ¨¡å‹ä¸è‡ªåŠ¨ç¼–ç å™¨ç›¸ç»“åˆï¼Œæœ‰æ•ˆè§£å†³äº†æ‰©æ•£æ¨¡å‹åœ¨é«˜ç»´æ•°æ®ä¸Šæ”¶æ•›å›°éš¾çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚è¯¥æ–¹æ³•åœ¨è‡ªç„¶å’Œé¥æ„Ÿé«˜å…‰è°±æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰å’Œåº¦é‡ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’Œç»„è‡ªç¼–ç å™¨çš„è¶…åˆ†è¾¨ç‡é«˜å…‰è°±å›¾åƒå¢å¼ºæ¨¡å‹ï¼ˆDMGASRï¼‰ã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç¼–ç å™¨å’Œæ‰©æ•£è¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚</li><li>é‡‡ç”¨è°±åˆ†ç»„ç­–ç•¥å’Œéå¯¹ç§°è§£ç å™¨è®¾è®¡ï¼Œæœ‰æ•ˆåœ°å°†é«˜ç»´é«˜å…‰è°±æ•°æ®ç¼–ç ä¸ºä½ç»´æ½œåœ¨ç©ºé—´ã€‚</li><li>è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œï¼Œç¼“è§£äº†è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„éš¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æ³¢æ®µç›¸å…³æ€§å¹¶å¤§å¤§å‡å°‘äº†æ¨ç†æ—¶é—´ã€‚æ€§èƒ½ï¼š</li><li>åœ¨è‡ªç„¶å’Œé¥æ„Ÿé«˜å…‰è°±æ•°æ®é›†ä¸Šå‡å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li><li>åœ¨è§†è§‰å’Œåº¦é‡ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>ç®—æ³•è®¾è®¡å’Œå®ç°ã€‚</li><li>æ•°æ®é›†çš„æ”¶é›†å’Œé¢„å¤„ç†ã€‚</li><li>å®éªŒçš„è¿›è¡Œå’Œç»“æœåˆ†æã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-1b637edd1829307f3889177173204f7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc3237f0ece24500c44086801ebc1feb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3e331ea518a2b9c151178e17f115708.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b211209593777f9420f6bb845daa71b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f24696c9c22f22b6e487ce2e6fc31ec7.jpg" align="middle"></details><h2 id="One-Shot-Structure-Aware-Stylized-Image-Synthesis"><a href="#One-Shot-Structure-Aware-Stylized-Image-Synthesis" class="headerlink" title="One-Shot Structure-Aware Stylized Image Synthesis"></a>One-Shot Structure-Aware Stylized Image Synthesis</h2><p><strong>Authors:Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong</strong></p><p>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. </p><p><a href="http://arxiv.org/abs/2402.17275v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>åŸºäºæ‰©æ•£æ¨¡å‹çš„ OSASIS å®ç°äº†å›¾åƒé£æ ¼åŒ–ï¼ŒåŒæ—¶ä¿æŒäº†ç»“æ„å®Œæ•´æ€§ï¼Œå³ä½¿æ˜¯å¯¹è®­ç»ƒä¸­å¾ˆå°‘é‡åˆ°çš„è¾“å…¥å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>OSASIS é‡‡ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé£æ ¼åŒ–ï¼Œè§£å†³äº† GAN æ¨¡å‹åœ¨ä¿æŒç»“æ„æ–¹é¢çš„ä¸è¶³ã€‚</li><li>OSASIS èƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»å›¾åƒè¯­ä¹‰å’Œç»“æ„ï¼Œå¯æ§åœ°è°ƒæ•´ç»™å®šè¾“å…¥çš„å†…å®¹å’Œé£æ ¼çº§åˆ«ã€‚</li><li>OSASIS åœ¨å„ç§å®éªŒè®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŸŸå¤–å‚è€ƒå›¾åƒè¿›è¡Œé£æ ¼åŒ–å’Œä½¿ç”¨æ–‡æœ¬é©±åŠ¨çš„æ“ä½œè¿›è¡Œé£æ ¼åŒ–ã€‚</li><li>ä¸å…¶ä»–é£æ ¼åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒOSASIS åœ¨è®­ç»ƒä¸­å¾ˆå°‘é‡åˆ°çš„è¾“å…¥å›¾åƒä¸Šè¡¨ç°å¾—å°¤ä¸ºå‡ºè‰²ï¼Œä¸ºé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼åŒ–æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li><li>OSASIS é‡‡ç”¨äº†æ¸è¿›å¼è®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡ä»æ·»åŠ å™ªå£°åˆ°æ¢å¤å›¾åƒï¼Œé€æ­¥å°†é£æ ¼åº”ç”¨äºè¾“å…¥ã€‚</li><li>OSASIS ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†æ•ˆç‡å’Œæ³›åŒ–æ€§ã€‚</li><li>OSASIS åœ¨å›¾åƒé£æ ¼åŒ–é¢†åŸŸå±•ç°å‡ºäº†å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç¼–è¾‘ã€è‰ºæœ¯åˆ›ä½œå’Œè§†é¢‘å¤„ç†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå•æ¬¡ç»“æ„æ„ŸçŸ¥é£æ ¼åŒ–å›¾åƒåˆæˆ</li><li>ä½œè€…ï¼šJongmin Lee*, Jaeyeon Kang, Sangwoo Mo, Seongwon Leeâ€ , Kyoung Mu Leeâ€ </li><li>éš¶å±å•ä½ï¼šNAVER Cloud</li><li>å…³é”®è¯ï¼šå›¾åƒé£æ ¼åŒ–ã€æ‰©æ•£æ¨¡å‹ã€ç»“æ„ä¿æŒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.05447, Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šGAN æ¨¡å‹åœ¨å›¾åƒé£æ ¼åŒ–ä»»åŠ¡ä¸­å–å¾—æˆåŠŸï¼Œä½†éš¾ä»¥åœ¨é£æ ¼åŒ–å„ç§è¾“å…¥å›¾åƒæ—¶ä¿æŒç»“æ„ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹è¢«ç”¨äºå›¾åƒé£æ ¼åŒ–ï¼Œä½†ä»ç¼ºä¹ä¿æŒè¾“å…¥å›¾åƒåŸå§‹è´¨é‡çš„èƒ½åŠ›ã€‚(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»æ–¹æ³•åŒ…æ‹¬åŸºäº GAN çš„æ¨¡å‹å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ã€‚GAN æ¨¡å‹éš¾ä»¥ä¿æŒç»“æ„ï¼Œè€ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ç¼ºä¹æ§åˆ¶å†…å®¹å’Œé£æ ¼çš„èƒ½åŠ›ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•æ¬¡é£æ ¼åŒ–æ–¹æ³• OSASISï¼Œè¯¥æ–¹æ³•åœ¨ç»“æ„ä¿æŒæ–¹é¢å…·æœ‰é²æ£’æ€§ã€‚OSASIS é€šè¿‡å°†è¯­ä¹‰ä»å›¾åƒçš„ç»“æ„ä¸­è§£è€¦ï¼Œä»è€Œæœ‰æ•ˆåœ°æ§åˆ¶åº”ç”¨äºç»™å®šè¾“å…¥çš„å†…å®¹å’Œé£æ ¼çš„çº§åˆ«ã€‚(4) ä»»åŠ¡å’Œæ€§èƒ½ï¼šOSASIS åœ¨å„ç§å®éªŒè®¾ç½®ä¸­å¾—åˆ°åº”ç”¨ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŸŸå¤–å‚è€ƒå›¾åƒçš„é£æ ¼åŒ–å’Œä½¿ç”¨æ–‡æœ¬é©±åŠ¨çš„æ“ä½œçš„é£æ ¼åŒ–ã€‚ç»“æœè¡¨æ˜ï¼ŒOSASIS ä¼˜äºå…¶ä»–é£æ ¼åŒ–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåœ¨è®­ç»ƒæœŸé—´å¾ˆå°‘é‡åˆ°çš„è¾“å…¥å›¾åƒï¼Œä¸ºé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼åŒ–æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li></ol><p><strong>Methodsï¼š</strong></p><ol><li><strong>å›¾åƒåˆ†è§£ï¼š</strong>å°†è¾“å…¥å›¾åƒåˆ†è§£ä¸ºå†…å®¹å’Œç»“æ„ç‰¹å¾ï¼Œå…¶ä¸­å†…å®¹ç‰¹å¾è¡¨ç¤ºå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œç»“æ„ç‰¹å¾è¡¨ç¤ºå›¾åƒçš„å‡ ä½•å½¢çŠ¶å’Œçº¹ç†ã€‚</li><li><strong>é£æ ¼åµŒå…¥ï¼š</strong>å°†å‚è€ƒé£æ ¼å›¾åƒåµŒå…¥åˆ°ä¸€ä¸ªæ½œåœ¨ç©ºé—´ä¸­ï¼Œè¯¥ç©ºé—´ç”±æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</li><li><strong>é£æ ¼ä¼ è¾“ï¼š</strong>å°†è¾“å…¥å›¾åƒçš„å†…å®¹ç‰¹å¾ä¸å‚è€ƒé£æ ¼çš„é£æ ¼åµŒå…¥ç›¸ç»“åˆï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å›¾åƒï¼Œè¯¥å›¾åƒå…·æœ‰è¾“å…¥å›¾åƒçš„ç»“æ„å’Œå‚è€ƒé£æ ¼çš„é£æ ¼ã€‚</li><li><p><strong>ç»“æ„ä¿æŒï¼š</strong>é€šè¿‡ä½¿ç”¨ä¸€ä¸ªé¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œå°†è¾“å…¥å›¾åƒçš„ç»“æ„ç‰¹å¾ä¸ç”Ÿæˆå›¾åƒçš„ç»“æ„ç‰¹å¾è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œä¿æŒè¾“å…¥å›¾åƒçš„åŸå§‹è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å•æ¬¡å›¾åƒé£æ ¼åŒ–æ–¹æ³• OSASISï¼Œè¯¥æ–¹æ³•åœ¨ç»“æ„ä¿æŒæ–¹é¢å…·æœ‰é²æ£’æ€§ã€‚ä¸åŸºäº GAN å’Œå…¶ä»–åŸºäºæ‰©æ•£çš„é£æ ¼åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒOSASIS å±•ç¤ºäº†åœ¨é£æ ¼åŒ–ä¸­å¯¹ç»“æ„çš„å¼ºå¤§æ„ŸçŸ¥ï¼Œæœ‰æ•ˆåœ°å°†å›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰è§£è€¦ã€‚å°½ç®¡ OSASIS åœ¨ç»“æ„æ„ŸçŸ¥é£æ ¼åŒ–æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚OSASIS çš„ä¸€ä¸ªæ˜¾ç€é™åˆ¶æ˜¯å…¶è®­ç»ƒæ—¶é—´ï¼Œæ¯”æ¯”è¾ƒæ–¹æ³•æ›´é•¿ã€‚è¿™ç§å»¶é•¿çš„è®­ç»ƒæŒç»­æ—¶é—´æ˜¯ä¸ºäº†æ¢å–è¯¥æ–¹æ³•å¢å¼ºäº†ä¿æŒç»“æ„å®Œæ•´æ€§å’Œé€‚åº”å„ç§é£æ ¼çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒOSASIS éœ€è¦é’ˆå¯¹æ¯å¼ é£æ ¼å›¾åƒè¿›è¡Œè®­ç»ƒã€‚åœ¨éœ€è¦è·¨å¤šç§é£æ ¼å¿«é€Ÿéƒ¨ç½²çš„åœºæ™¯ä¸­ï¼Œè¿™ä¸€è¦æ±‚å¯ä»¥è¢«è§†ä¸ºä¸€ç§é™åˆ¶ã€‚å°½ç®¡å­˜åœ¨è¿™äº›æŒ‘æˆ˜ï¼Œä½† OSASIS åœ¨ä¿æŒè¾“å…¥å›¾åƒç»“æ„å®Œæ•´æ€§æ–¹é¢çš„ç¨³å¥æ€§ã€å…¶åœ¨åŸŸå¤–å‚è€ƒé£æ ¼åŒ–ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠå…¶åœ¨æ–‡æœ¬é©±åŠ¨æ“ä½œä¸­çš„é€‚åº”æ€§ä½¿å…¶æˆä¸ºé£æ ¼åŒ–å›¾åƒåˆæˆé¢†åŸŸä¸­ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ–¹æ³•ã€‚æœªæ¥çš„å·¥ä½œå°†è§£å†³è¿™äº›é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼˜åŒ–è®­ç»ƒæ•ˆç‡å’Œå‡å°‘å¯¹å•ä¸ªé£æ ¼å›¾åƒè®­ç»ƒçš„å¿…è¦æ€§æ–¹é¢ï¼Œä»¥å¢å¼º OSASIS åœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„å®ç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒé£æ ¼åŒ–æ–¹æ³• OSASISï¼Œè¯¥æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»“æ„ä¿æŒæ–¹é¢å…·æœ‰é²æ£’æ€§ã€‚</li><li>OSASIS é€šè¿‡å°†å›¾åƒçš„ç»“æ„å’Œè¯­ä¹‰è§£è€¦ï¼Œæœ‰æ•ˆåœ°æ§åˆ¶åº”ç”¨äºç»™å®šè¾“å…¥çš„å†…å®¹å’Œé£æ ¼çš„çº§åˆ«ã€‚</li><li>OSASIS åœ¨å„ç§å®éªŒè®¾ç½®ä¸­å¾—åˆ°åº”ç”¨ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŸŸå¤–å‚è€ƒå›¾åƒçš„é£æ ¼åŒ–å’Œä½¿ç”¨æ–‡æœ¬é©±åŠ¨çš„æ“ä½œçš„é£æ ¼åŒ–ã€‚æ€§èƒ½ï¼š</li><li>OSASIS åœ¨ç»“æ„ä¿æŒæ–¹é¢ä¼˜äºå…¶ä»–é£æ ¼åŒ–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåœ¨è®­ç»ƒæœŸé—´å¾ˆå°‘é‡åˆ°çš„è¾“å…¥å›¾åƒã€‚</li><li>OSASIS ä¸ºé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œé£æ ¼åŒ–æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å·¥ä½œé‡ï¼š</li><li>OSASIS çš„è®­ç»ƒæ—¶é—´æ¯”æ¯”è¾ƒæ–¹æ³•æ›´é•¿ã€‚</li><li>OSASIS éœ€è¦é’ˆå¯¹æ¯å¼ é£æ ¼å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-957518995345024bb9a18f0e683a4e55.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0f3cefa16e52b2bb0bdbb679863e234.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e8afc30904c2bad1400fb9f044e33a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0eead50e28d5ed02ff0105780a9e22e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b842ecc40528644a1d824a5a8948f487.jpg" align="middle"></details><h2 id="Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation"><a href="#Playground-v2-5-Three-Insights-towards-Enhancing-Aesthetic-Quality-in-Text-to-Image-Generation" class="headerlink" title="Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation"></a>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in   Text-to-Image Generation</h2><p><strong>Authors:Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi</strong></p><p>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models. </p><p><a href="http://arxiv.org/abs/2402.17245v1">PDF</a> Model weights:   <a href="https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic">https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic</a></p><p><strong>Summary</strong><br>é€šè¿‡å¯¹å™ªå£°æ—¶é—´è¡¨ã€å®½é«˜æ¯”å‡†å¤‡å’Œé¢å‘äººç±»çš„å¾®è°ƒçš„ç ”ç©¶ï¼ŒPlayground v2.5  diffusion æ¨¡å‹å¯äº§ç”Ÿæä½³çš„ç¾å­¦è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å™ªéŸ³æ—¶é—´è¡¨å¯¹æ¨¡å‹çœŸå®æ€§å’Œè§†è§‰ä¿çœŸåº¦è‡³å…³é‡è¦ã€‚</li><li>å¹³è¡¡çš„åˆ†åŒºæ•°æ®é›†å¯æ”¹å–„ä¸åŒå®½é«˜æ¯”çš„å›¾åƒç”Ÿæˆã€‚</li><li>å°†æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ç›¸ç»“åˆå¯æå‡å›¾åƒçš„å…±é¸£æ•ˆæœã€‚</li><li>Playground v2.5 åœ¨å„ç§æ¡ä»¶å’Œå®½é«˜æ¯”ä¸‹è¡¨ç°å‡ºæœ€å…ˆè¿›çš„å®¡ç¾è´¨é‡ã€‚</li><li>Playground v2.5 æ¨¡å‹å¼€æºï¼Œä¸ºæå‡åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾è´¨é‡æä¾›äº†æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</li><li>Playground v2.5 ä¼˜äº SDXLã€Playground v2ã€DALLE 3 å’Œ Midjourney v5.2ã€‚</li><li>ç ”ç©¶æœ‰åŠ©äºæé«˜åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾è´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šPlayground v2.5ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå®¡ç¾è´¨é‡çš„ä¸‰ç‚¹è§è§£</li><li>ä½œè€…ï¼šDaiqing Liã€Aleks Kamkoã€Ehsan Akhgariã€Ali Sabetã€Linmiao Xuã€Suhail Doshi</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šPlayground Research</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€å®¡ç¾è´¨é‡</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2402.17245v1[cs.CV]</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒçš„å®¡ç¾è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚é¢œè‰²å’Œå¯¹æ¯”åº¦ä¸è¶³ã€ä¸åŒå®½é«˜æ¯”ç”Ÿæˆè´¨é‡ä¸ä½³ã€ç¼ºä¹å¯¹äººç±»åå¥½çš„å¯¹é½ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¦‚ä¼˜åŒ–å™ªå£°è°ƒåº¦æˆ–ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æå‡å®¡ç¾è´¨é‡æ–¹é¢æ•ˆæœæœ‰é™ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸‰ç‚¹è§è§£æ¥æå‡å®¡ç¾è´¨é‡ï¼šæ”¹è¿›å™ªå£°è°ƒåº¦ä»¥å¢å¼ºé¢œè‰²å’Œå¯¹æ¯”åº¦ï¼Œæ„å»ºå¹³è¡¡çš„åˆ†æ¡¶æ•°æ®é›†ä»¥æ”¯æŒä¸åŒå®½é«˜æ¯”çš„ç”Ÿæˆï¼Œä»¥åŠåˆ©ç”¨äººç±»åé¦ˆæ¥å¯¹é½æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„åˆ†æå’Œå®éªŒä¸­ï¼ŒPlayground v2.5 åœ¨å„ç§æ¡ä»¶å’Œå®½é«˜æ¯”ä¸‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„å®¡ç¾è´¨é‡ï¼Œä¼˜äº SDXLã€Playground v2 ç­‰å¼€æºæ¨¡å‹å’Œ DALLÂ·E 3ã€Midjourney v5.2 ç­‰é—­æºå•†ä¸šç³»ç»Ÿã€‚</li></ol><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ”¹è¿›å™ªå£°è°ƒåº¦ï¼šé‡‡ç”¨ EDM æ¡†æ¶å’Œæ›´å™ªå£°çš„è°ƒåº¦æ–¹å¼ï¼Œå¢å¼ºå›¾åƒè‰²å½©å’Œå¯¹æ¯”åº¦ã€‚ï¼ˆ2ï¼‰å¹³è¡¡åˆ†æ¡¶æ•°æ®é›†ï¼šæ„å»ºåŒ…å«ä¸åŒå®½é«˜æ¯”å›¾åƒçš„åˆ†æ¡¶æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§å®½é«˜æ¯”çš„ç”Ÿæˆã€‚ï¼ˆ3ï¼‰åˆ©ç”¨äººç±»åé¦ˆï¼šä½¿ç”¨äººç±»è¯„çº§ç³»ç»Ÿè‡ªåŠ¨ç­›é€‰é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨è¿­ä»£è®­ç»ƒæ–¹æ³•ï¼Œæ ¹æ®äººç±»åå¥½å¯¹é½æ¨¡å‹è¾“å‡ºã€‚</p><ol><li>æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º Playground v2.5ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ”¹è¿›å™ªå£°è°ƒåº¦ã€æ„å»ºå¹³è¡¡çš„åˆ†æ¡¶æ•°æ®é›†å’Œåˆ©ç”¨äººç±»åé¦ˆç­‰ä¸‰ç‚¹è§è§£ï¼Œæå‡äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å®¡ç¾è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å™ªå£°è°ƒåº¦æ¡†æ¶ï¼Œå¢å¼ºäº†å›¾åƒçš„è‰²å½©å’Œå¯¹æ¯”åº¦ã€‚</li><li>æ„å»ºäº†ä¸€ä¸ªåŒ…å«ä¸åŒå®½é«˜æ¯”å›¾åƒçš„åˆ†æ¡¶æ•°æ®é›†ï¼Œæ”¯æŒå¤šç§å®½é«˜æ¯”çš„ç”Ÿæˆã€‚</li><li>åˆ©ç”¨äººç±»è¯„çº§ç³»ç»Ÿè‡ªåŠ¨ç­›é€‰é«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨è¿­ä»£è®­ç»ƒæ–¹æ³•ï¼Œæ ¹æ®äººç±»åå¥½å¯¹é½æ¨¡å‹è¾“å‡ºã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¹¿æ³›çš„åˆ†æå’Œå®éªŒä¸­ï¼ŒPlayground v2.5 åœ¨å„ç§æ¡ä»¶å’Œå®½é«˜æ¯”ä¸‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„å®¡ç¾è´¨é‡ï¼Œä¼˜äºå…¶ä»–å¼€æºå’Œé—­æºæ¨¡å‹ã€‚</li><li>Playground v2.5 åœ¨å¢å¼ºå›¾åƒè‰²å½©å’Œå¯¹æ¯”åº¦ã€ç”Ÿæˆä¸åŒå®½é«˜æ¯”çš„é«˜è´¨é‡å›¾åƒä»¥åŠå¯¹é½æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆäººç‰©å›¾åƒçš„ç²¾ç»†ç»†èŠ‚æ–¹é¢ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ¨¡å‹å·²å¼€æºï¼Œç”¨æˆ·å¯ä»¥åœ¨ Playground äº§å“ç½‘ç«™ä¸Šä½¿ç”¨ã€‚</li><li>Playground v2.5 çš„æƒé‡å·²åœ¨ Hugging Face ä¸Šå¼€æºã€‚</li><li>Playground å°†ç»§ç»­æä¾›æ‰©å±•ï¼Œä»¥ä¾¿åœ¨ A1111 å’Œ ComfyUI ç­‰æµè¡Œç¤¾åŒºå·¥å…·ä¸­ä½¿ç”¨ Playground v2.5ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b9ee43af14ab727bc293d7a249e6d156.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ff95dbf16b9c2e734124d2c99954b6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b62a3df3bac0ff8ef7d20dfeccb0f6b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-869a1d35fa675595c5662a91b215c366.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226f377d76bcd81c0c005d4e513c6f81.jpg" align="middle"></details><h2 id="SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution"><a href="#SAM-DiffSR-Structure-Modulated-Diffusion-Model-for-Image-Super-Resolution" class="headerlink" title="SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution"></a>SAM-DiffSR: Structure-Modulated Diffusion Model for Image   Super-Resolution</h2><p><strong>Authors:Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang</strong></p><p>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at <a href="https://github.com/lose4578/SAM-DiffSR">https://github.com/lose4578/SAM-DiffSR</a>. </p><p><a href="http://arxiv.org/abs/2402.17133v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹ä¸­ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„SAM-DiffSRæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨SAMçš„ç²¾ç»†ç»“æ„ä¿¡æ¯åœ¨é‡‡æ ·å™ªå£°çš„è¿‡ç¨‹ä¸­æ¥æ”¹å–„æœ€ç»ˆå›¾åƒè´¨é‡ï¼Œè€Œæ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦SAMï¼Œæœ‰æ•ˆé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§SAM-DiffSRæ¨¡å‹ï¼Œå¯ä»¥åˆ©ç”¨SAMçš„ç²¾ç»†ç»“æ„ä¿¡æ¯æ¥æ”¹å–„å›¾åƒè´¨é‡ã€‚</li><li>SAM-DiffSRæ¨¡å‹é€šè¿‡å°†ç¼–ç çš„æ©ç æ•´åˆåˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œåœ¨é‡‡æ ·å™ªå£°ä¹‹å‰è¿›è¡Œè°ƒæ•´ã€‚</li><li>è¯¥è°ƒæ•´å…è®¸ç‹¬ç«‹è°ƒæ•´æ¯ä¸ªå¯¹åº”åˆ†å‰²åŒºåŸŸå†…çš„å™ªå£°å‡å€¼ã€‚</li><li>æ‰©æ•£æ¨¡å‹è¢«è®­ç»ƒæ¥ä¼°è®¡è¿™ç§è°ƒåˆ¶çš„å™ªå£°ã€‚</li><li>æ‰€æå‡ºçš„æ–¹æ³•ä¸æ”¹å˜åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸éœ€è¦SAMã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°æŠ‘åˆ¶äº†ä¼ªå½±ï¼Œåœ¨DIV2Kæ•°æ®é›†ä¸Šä»¥PSNRæŒ‡æ ‡è¶…è¶Šäº†ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•0.74 dBã€‚</li><li>ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a href="https://github.com/lose4578/SAM-DiffSRè·å¾—ã€‚">https://github.com/lose4578/SAM-DiffSRè·å¾—ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSAM-DiffSRï¼šç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„ç»“æ„è°ƒåˆ¶æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šChengcheng Wangã€Zhiwei Haoã€Yehui Tangã€Jianyuan Guoã€Yujie Yangã€Kai Hanã€Yunhe Wang</li><li>å•ä½ï¼šåä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤</li><li>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ã€æ‰©æ•£æ¨¡å‹ã€ç»“æ„è°ƒåˆ¶</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.17133   Githubï¼šhttps://github.com/lose4578/SAM-DiffSR</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä»å•ä¸€åˆ†å¸ƒä¸­è¿›è¡Œå™ªå£°é‡‡æ ·ï¼Œé™åˆ¶äº†å…¶å¤„ç†çœŸå®åœºæ™¯å’Œè·¨è¯­ä¹‰åŒºåŸŸå¤æ‚çº¹ç†çš„èƒ½åŠ›ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   Segment Anything Modelï¼ˆSAMï¼‰èƒ½ç”Ÿæˆè¶³å¤Ÿç²¾ç»†çš„åŒºåŸŸæ©ç ï¼Œå¢å¼ºæ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚æ¢å¤èƒ½åŠ›ã€‚ä½†ç›´æ¥å°† SAM é›†æˆåˆ° SR æ¨¡å‹ä¸­ä¼šå¤§å¹…å¢åŠ è®¡ç®—æˆæœ¬ã€‚</p><p>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   æå‡º SAM-DiffSR æ¨¡å‹ï¼Œåœ¨å™ªå£°é‡‡æ ·è¿‡ç¨‹ä¸­åˆ©ç”¨ SAM çš„ç²¾ç»†ç»“æ„ä¿¡æ¯ï¼Œåœ¨ä¸å¢åŠ æ¨ç†è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æé«˜å›¾åƒè´¨é‡ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†ç»“æ„ä½ç½®ä¿¡æ¯ç¼–ç åˆ° SAM çš„åˆ†å‰²æ©ç ä¸­ã€‚ç„¶åå°†ç¼–ç åçš„æ©ç é›†æˆåˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå°†å…¶è°ƒåˆ¶åˆ°é‡‡æ ·çš„å™ªå£°ä¸­ã€‚è¿™ç§è°ƒæ•´å…è®¸åœ¨æ¯ä¸ªå¯¹åº”çš„åˆ†å‰²åŒºåŸŸå†…ç‹¬ç«‹è°ƒæ•´å™ªå£°å‡å€¼ã€‚æ‰©æ•£æ¨¡å‹è¢«è®­ç»ƒæ¥ä¼°è®¡è¿™ç§è°ƒåˆ¶çš„å™ªå£°ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼š   å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æœ‰æ•ˆï¼Œåœ¨æŠ‘åˆ¶ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨ DIV2K æ•°æ®é›†ä¸Šä»¥ PSNR è¡¡é‡ï¼Œæ¯”ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æé«˜äº† 0.74dBã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</p><p><strong>Methodsï¼š</strong></p><p>(1) åˆ©ç”¨ SegmentAnythingModelï¼ˆSAMï¼‰ç”Ÿæˆç²¾ç»†çš„åŒºåŸŸæ©ç ï¼Œç¼–ç ç»“æ„ä½ç½®ä¿¡æ¯ã€‚</p><p>(2) å°†ç¼–ç åçš„æ©ç é›†æˆåˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œè°ƒåˆ¶é‡‡æ ·çš„å™ªå£°ã€‚</p><p>(3) è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¼°è®¡è°ƒåˆ¶çš„å™ªå£°ï¼Œä»è€Œåœ¨æ¯ä¸ªåˆ†å‰²åŒºåŸŸå†…ç‹¬ç«‹è°ƒæ•´å™ªå£°å‡å€¼ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é‡ç‚¹é€šè¿‡é›†æˆ SAMï¼Œå¢å¼ºåŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹çš„ç»“æ„å±‚æ¬¡ä¿¡æ¯æ¢å¤èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸º SAM-DiffSR çš„æ¡†æ¶ï¼Œå®ƒæ¶‰åŠå°†ç»“æ„ä½ç½®ä¿¡æ¯çº³å…¥ SAM ç”Ÿæˆçš„æ©ç ï¼Œç„¶ååœ¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ä¸­å°†å…¶æ·»åŠ åˆ°é‡‡æ ·çš„å™ªå£°ä¸­ã€‚æ­¤æ“ä½œå•ç‹¬è°ƒèŠ‚æ¯ä¸ªç›¸åº”åˆ†å‰²åŒºåŸŸä¸­å™ªå£°çš„å‡å€¼ï¼Œä»è€Œå°†ç»“æ„å±‚æ¬¡çŸ¥è¯†æ³¨å…¥æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨æ¢å¤ç»“æ„ç»†èŠ‚å’ŒæŠ‘åˆ¶å›¾åƒä¼ªå½±æ–¹é¢è¡¨ç°å‡ºæ”¹è¿›ï¼Œè€Œæ— éœ€äº§ç”Ÿä»»ä½•é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡åœ¨å¸¸ç”¨çš„å›¾åƒè¶…åˆ†è¾¨ç‡åŸºå‡†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå¾—åˆ°è¯å®ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ SAM æ³¨å…¥ç»“æ„ä¿¡æ¯ï¼Œå¢å¼ºæ‰©æ•£æ¨¡å‹çš„ç»“æ„æ¢å¤èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨æŠ‘åˆ¶ä¼ªå½±å’Œæ¢å¤ç»“æ„ç»†èŠ‚æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šæ¨ç†æˆæœ¬ä¸åŸºçº¿æ¨¡å‹ç›¸å½“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9a754ccd89139d7dc6a576434e6b119e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0906797fab629c359270ce611fcb26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-66893d51d835b7965b76fb168b66db51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1f36de01723e09ebef0661e0e152ae2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bca3bdea09d0b0b3c4c6b041a3c1758.jpg" align="middle"></details><h2 id="Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing"><a href="#Cross-Modal-Contextualized-Diffusion-Models-for-Text-Guided-Visual-Generation-and-Editing" class="headerlink" title="Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing"></a>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual   Generation and Editing</h2><p><strong>Authors:Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized diffusion model (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized diffusion to both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: text-to-image generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a> </p><p><a href="http://arxiv.org/abs/2402.16627v1">PDF</a> ICLR 2024. Project: <a href="https://github.com/YangLing0818/ContextDiff">https://github.com/YangLing0818/ContextDiff</a></p><p><strong>Summary</strong><br>ä¸Šä¸‹æ–‡æ‰©æ•£æ¨¡å‹é€šè¿‡åœ¨æ‰©æ•£æ­£åè¿‡ç¨‹ä¸­åŠ å…¥æ–‡æœ¬å¯è§†å…³ç³»ï¼Œæå‡äº†æ–‡æœ¬å¼•å¯¼å¯è§†åŒ–ç”Ÿæˆå’Œç¼–è¾‘çš„è¯­ä¹‰å¯¹é½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼å¯è§†åŒ–ç”Ÿæˆå’Œç¼–è¾‘ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li><li>ä¼ ç»Ÿæ¨¡å‹åªå°†æ–‡æœ¬å¯è§†å…³ç³»èå…¥åå‘è¿‡ç¨‹ï¼Œå¿½ç•¥äº†æ­£å‘è¿‡ç¨‹çš„å…³è”æ€§ã€‚</li><li>æ­£åè¿‡ç¨‹çš„ä¸ä¸€è‡´æ€§é™åˆ¶äº†æ–‡æœ¬è¯­ä¹‰åœ¨å¯è§†åŒ–åˆæˆç»“æœä¸­çš„ä¼ é€’ç²¾åº¦ã€‚</li><li>è¯­ä¹‰æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æ–‡æœ¬æ¡ä»¶å’Œå¯è§†æ ·æœ¬ä¹‹é—´çš„äº¤äº’å’Œå¯¹é½çº³å…¥æ­£åè¿‡ç¨‹ï¼Œæ”¹å–„äº†è¿™ç§ä¸ä¸€è‡´æ€§ã€‚</li><li>æ”¹è¿›é€‚ç”¨äº DDPM å’Œ DDIMï¼Œå¹¶é€šè¿‡ç†è®ºæ¨ç†å¾—åˆ°è¯æ˜ã€‚</li><li>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œè¯­ä¹‰æ‰©æ•£æ¨¡å‹å‡è¾¾åˆ°æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</li><li>å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜è¯­ä¹‰æ‰©æ•£æ¨¡å‹æ˜¾è‘—æå‡äº†æ–‡æœ¬æ¡ä»¶å’Œç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šè·¨æ¨¡æ€è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹ç”¨äºæ–‡æœ¬å¼•å¯¼çš„è§†è§‰ç”Ÿæˆå’Œç¼–è¾‘</li><li>ä½œè€…ï¼šæ¨å‡Œã€å¼ å¿—é¾™ã€äºå…†å®¸ã€åˆ˜æ™¯ä¼Ÿã€å¾æ˜å‡¯ã€Stefano Ermonã€å´”æ–Œ</li><li>éš¶å±ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬å¼•å¯¼è§†è§‰ç”Ÿæˆã€æ–‡æœ¬å¼•å¯¼è§†é¢‘ç¼–è¾‘ã€æ‰©æ•£æ¨¡å‹ã€è¯­å¢ƒåŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.16627   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼è§†è§‰ç”Ÿæˆå’Œç¼–è¾‘é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å°†æ–‡æœ¬-è§†è§‰å…³ç³»èå…¥é€†è¿‡ç¨‹ï¼Œå¿½è§†äº†å…¶åœ¨å‰å‘è¿‡ç¨‹ä¸­çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´æ–‡æœ¬è¯­ä¹‰åœ¨è§†è§‰åˆæˆç»“æœä¸­çš„ç²¾ç¡®ä¼ è¾¾å—åˆ°é™åˆ¶ã€‚   (2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š</p><ul><li>å¿½ç•¥äº†æ–‡æœ¬-è§†è§‰å…³ç³»åœ¨å‰å‘è¿‡ç¨‹ä¸­çš„ä½œç”¨ï¼Œå¯¼è‡´æ–‡æœ¬è¯­ä¹‰åœ¨è§†è§‰åˆæˆç»“æœä¸­çš„ç²¾ç¡®ä¼ è¾¾å—é™ã€‚</li><li>ç¼ºä¹ä¸€ç§é€šç”¨çš„è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹ï¼Œæ— æ³•åŒæ—¶å¤„ç†æ–‡æœ¬å¼•å¯¼å›¾åƒå’Œè§†é¢‘ç”Ÿæˆ/ç¼–è¾‘ä»»åŠ¡ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹ï¼ˆCONTEXTDIFFï¼‰ï¼Œé€šè¿‡å°†è·¨æ¨¡æ€è¯­å¢ƒï¼ˆåŒ…å«æ–‡æœ¬æ¡ä»¶å’Œè§†è§‰æ ·æœ¬ä¹‹é—´çš„äº¤äº’å’Œå¯¹é½ï¼‰èå…¥å‰å‘å’Œé€†è¿‡ç¨‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå°†è¯¥è¯­å¢ƒä¼ æ’­åˆ°ä¸¤ä¸ªè¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ—¶é—´æ­¥ï¼Œä»¥é€‚åº”å®ƒä»¬çš„è½¨è¿¹ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€æ¡ä»¶å»ºæ¨¡ã€‚åŒæ—¶ï¼Œå°†è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹æ¨å¹¿åˆ° DDPM å’Œ DDIMï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚   (4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç¼–è¾‘ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šï¼ŒCONTEXTDIFF å‡å–å¾—äº†æ–°çš„ SOTA æ€§èƒ½ï¼Œæ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬æ¡ä»¶ä¸ç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</li></ul></li><li><p>Methods:(1): æå‡ºè·¨æ¨¡æ€è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹ï¼ˆCONTEXTDIFFï¼‰ï¼Œå°†è·¨æ¨¡æ€è¯­å¢ƒï¼ˆåŒ…å«æ–‡æœ¬æ¡ä»¶å’Œè§†è§‰æ ·æœ¬ä¹‹é—´çš„äº¤äº’å’Œå¯¹é½ï¼‰èå…¥å‰å‘å’Œé€†è¿‡ç¨‹ï¼Œä¿ƒè¿›è·¨æ¨¡æ€æ¡ä»¶å»ºæ¨¡ï¼›(2): å°†è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹æ¨å¹¿åˆ°DDPMå’ŒDDIMï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼›(3): åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç¼–è¾‘ä¸¤ä¸ªä»»åŠ¡ä¸Šï¼ŒCONTEXTDIFFå‡å–å¾—äº†æ–°çš„SOTAæ€§èƒ½ï¼Œæ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬æ¡ä»¶ä¸ç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCONTEXTDIFFï¼‰ï¼Œé€šè¿‡å°†è·¨æ¨¡æ€è¯­å¢ƒä¼ æ’­åˆ°æ‰©æ•£å’Œé€†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ—¶é—´æ­¥ï¼Œå¹¶é€‚åº”å®ƒä»¬çš„è½¨è¿¹ï¼Œä»è€Œä¿ƒè¿›è·¨æ¨¡æ€æ¡ä»¶å»ºæ¨¡ã€‚æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡åŒ–è½¨è¿¹é€‚é…å™¨æ¨å¹¿åˆ° DDPM å’Œ DDIMï¼Œå¹¶é€šè¿‡ç†è®ºæ¨å¯¼è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç¼–è¾‘è¿™ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šï¼ŒCONTEXTDIFF å§‹ç»ˆè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸¤é¡¹ä»»åŠ¡çš„å¹¿æ³›å®šé‡å’Œå®šæ€§ç»“æœè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„è·¨æ¨¡æ€è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€è¯­å¢ƒåŒ–æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å°†è·¨æ¨¡æ€è¯­å¢ƒèå…¥æ‰©æ•£å’Œé€†è¿‡ç¨‹ï¼Œä¿ƒè¿›è·¨æ¨¡æ€æ¡ä»¶å»ºæ¨¡ã€‚æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°è§†é¢‘ç¼–è¾‘ä¸¤ä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬æ¡ä»¶ä¸ç”Ÿæˆæ ·æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚å·¥ä½œé‡ï¼šå·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦å¯¹æ‰©æ•£æ¨¡å‹å’Œè·¨æ¨¡æ€è¯­å¢ƒåŒ–è¿›è¡Œæ·±å…¥ç†è§£ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0bc30cb1ebccfebfcc1ffd4ee246c26b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64adb5f655a12b089618a5496f3cd332.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f01bc8ec645d09757f45be018ce1fe96.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a622ae5ed900b07d2994967a2269c23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0d264d770c3a4265052827f62ee48f0b.jpg" align="middle"></details><h2 id="Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation"><a href="#Placing-Objects-in-Context-via-Inpainting-for-Out-of-distribution-Segmentation" class="headerlink" title="Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation"></a>Placing Objects in Context via Inpainting for Out-of-distribution   Segmentation</h2><p><strong>Authors:Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez</strong></p><p>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via diffusion models. POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly fine-tuning methods in several standardized benchmarks. POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images. </p><p><a href="http://arxiv.org/abs/2402.16392v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨æ‰©æ•£æ¨¡å‹å°†å¯¹è±¡æ’å…¥ä¸Šä¸‹æ–‡(POC)ç®¡é“ï¼Œå¯çœŸå®åœ°å‘å›¾åƒä¸­æ·»åŠ ä»»ä½•å¯¹è±¡ï¼Œæœ‰æ•ˆæ‰©å±•æ•°æ®é›†å’Œæ”¹å–„å¼‚å¸¸åˆ†å‰²æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºPOCç®¡é“ï¼Œå¯å‘å›¾åƒä¸­çœŸå®åœ°æ·»åŠ ä»»æ„å¯¹è±¡ã€‚</li><li>POCèƒ½è½»æ¾æ‰©å±•æ•°æ®é›†ï¼Œæ·»åŠ ä»»æ„æ•°é‡çš„å¯¹è±¡ã€‚</li><li>POCç”Ÿæˆçš„å¼‚å¸¸åˆ†å‰²æ•°æ®é›†æ¯”ç°æœ‰æ•°æ®é›†æ›´çœŸå®ã€å…¨é¢ã€‚</li><li>POCèƒ½æå‡æœ€æ–°å¼‚å¸¸ç²¾è°ƒæ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚</li><li>POCå¯ç”¨äºå­¦ä¹ æ–°ç±»åˆ«ï¼Œå¦‚å°†Pascalç±»åˆ«æ·»åŠ åˆ°Cityscapesã€‚</li><li>åŸºäºPOCç”Ÿæˆå›¾åƒè®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶ä»¿çœŸåˆ°çœŸå®å·®è·ä½ã€‚</li><li>POCç®¡é“èƒ½å¤Ÿæé«˜æ¨¡å‹åº”å¯¹æœªè§è¯­ä¹‰ç±»åˆ«çš„èƒ½åŠ›ï¼Œå¢å¼ºå¼‚å¸¸åˆ†å‰²æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.æ ‡é¢˜ï¼šé€šè¿‡å›¾åƒä¿®å¤å°†å¯¹è±¡ç½®äºä¸Šä¸‹æ–‡ä¸­ä»¥è¿›è¡Œåˆ†å¸ƒå¤–åˆ†å‰²2.ä½œè€…ï¼šPaude Jorgeâ€ , Riccardo Volpiâ€ , Puneet K. Dokaniaâ€¡, Philip H.S. Torrâ€¡, GrÃ©gory Rogezâ€ 3.æ‰€å±æœºæ„ï¼šNAVERLABS æ¬§æ´²ï¼Œç‰›æ´¥å¤§å­¦4.å…³é”®è¯ï¼šå¼‚å¸¸åˆ†å‰²ã€åˆ†å¸ƒå¤–æ£€æµ‹ã€å›¾åƒä¿®å¤ã€è¯­ä¹‰åˆ†å‰²ã€å¼€æ”¾è¯æ±‡åˆ†å‰²5.é“¾æ¥ï¼šhttps://github.com/naver/poc6.æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šåœ¨ç°å®ä¸–ç•Œä¸­éƒ¨ç½²è¯­ä¹‰åˆ†å‰²æ¨¡å‹æ—¶ï¼Œæ¨¡å‹ä¸å¯é¿å…åœ°ä¼šé‡åˆ°è®­ç»ƒæœŸé—´æœªè§è¿‡çš„è¯­ä¹‰ç±»åˆ«ã€‚å› æ­¤ï¼Œä¸ºäº†å®‰å…¨åœ°éƒ¨ç½²æ­¤ç±»ç³»ç»Ÿï¼Œå‡†ç¡®è¯„ä¼°å’Œæé«˜å…¶å¼‚å¸¸åˆ†å‰²èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè·å–å’Œæ ‡è®°è¯­ä¹‰åˆ†å‰²æ•°æ®ä»£ä»·é«˜æ˜‚ï¼Œè€Œä¸”æ„å¤–æƒ…å†µæ˜¯é•¿å°¾ä¸”å¯èƒ½å…·æœ‰å±é™©æ€§ã€‚å®é™…ä¸Šï¼Œç°æœ‰çš„å¼‚å¸¸åˆ†å‰²æ•°æ®é›†æ•è·çš„å¼‚å¸¸æ•°é‡æœ‰é™ï¼Œç¼ºä¹çœŸå®æ€§æˆ–å…·æœ‰å¾ˆå¼ºçš„åŸŸåç§»ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¾ç½®å¯¹è±¡åœ¨ä¸Šä¸‹æ–‡ï¼ˆPOCï¼‰ç®¡é“ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å°†ä»»ä½•å¯¹è±¡ç°å®åœ°æ·»åŠ åˆ°ä»»ä½•å›¾åƒä¸­ã€‚POC å¯ç”¨äºè½»æ¾åœ°ä½¿ç”¨ä»»æ„æ•°é‡çš„å¯¹è±¡æ‰©å±•ä»»ä½•æ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäº POC ç”Ÿæˆçš„ä¸åŒå¼‚å¸¸åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶è¡¨æ˜ POC å¯ä»¥æé«˜å‡ ç§æ ‡å‡†åŸºå‡†ä¸­æœ€è¿‘çš„å¼‚å¸¸ç²¾ç»†è°ƒæ•´æ–¹æ³•çš„æ€§èƒ½ã€‚POC è¿˜å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒé€šè¿‡æ·»åŠ  Pascal ç±»åˆ«çš„å­é›†æ¥ç¼–è¾‘ Cityscapes æ ·æœ¬ï¼Œå¹¶è¡¨æ˜åœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸ Pascal è®­ç»ƒçš„åŸºçº¿å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¯å®äº†åœ¨ POC ç”Ÿæˆçš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹çš„ä½æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šPOC ç®¡é“å»ºç«‹åœ¨å›¾åƒä¿®å¤å’Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ä¹‹ä¸Šï¼Œå°†ä»»æ„å¯¹è±¡ç°å®åœ°æ’å…¥å›¾åƒä¸­ã€‚ä¿®æ”¹åçš„å›¾åƒå’Œæ©ç å¯ç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚(4)ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜åœ¨ POC ç”Ÿæˆçš„å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾ç€æé«˜æœ€å…ˆè¿›çš„å¼‚å¸¸åˆ†å‰²æ–¹æ³•çš„æ€§èƒ½â€”â€”ä¼˜äºé€šè¿‡æ ‡å‡†åšæ³•ï¼ˆæ‹¼æ¥ COCO å¯¹è±¡ï¼‰è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸‰ä¸ªåŸºäº Cityscapes å’Œå…¶ä»–ä¸¤ä¸ªè‡ªåŠ¨é©¾é©¶æ•°æ®é›†çš„ POC ç”Ÿæˆçš„è¯„ä¼°é›†ï¼Œå¹¶åœ¨å…¶ä¸Šå¯¹ä¸åŒçš„å¼‚å¸¸åˆ†å‰²æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ˆæœ‰å…³ç»“æœçš„ç¬¬ä¸€çœ¼ï¼Œè¯·å‚è§å›¾ 1ï¼‰ã€‚æœ€åï¼Œç”±äº POC å¯ä»¥æ·»åŠ ä»»æ„å¯¹è±¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒå¯ç”¨äºå­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åŠ¨ç‰©ç±»åˆ«å¢å¼º Cityscapes å›¾åƒå¯¼è‡´ Pascal æµ‹è¯•é›†ä¸Šçš„ 93.14mIoUï¼ˆè€ƒè™‘ç›¸åŒçš„ç±»åˆ«ï¼‰ï¼Œè€Œç›´æ¥åœ¨ Pascal ä¸Šè®­ç»ƒäº§ç”Ÿ 94.75â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨ POC ç¼–è¾‘çš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹äº§ç”Ÿäº†ç›¸å½“å°çš„æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚</p><ol><li>æ–¹æ³•ï¼š(1) POCç®¡é“ï¼šPOCç®¡é“ç”±å›¾åƒä¿®å¤æ¨¡å‹å’Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ç»„æˆã€‚å›¾åƒä¿®å¤æ¨¡å‹ç”¨äºå°†å¯¹è±¡ç°å®åœ°æ’å…¥å›¾åƒä¸­ï¼Œè€Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ç”¨äºä¸ºæ’å…¥çš„å¯¹è±¡ç”Ÿæˆæ©ç ã€‚ä¿®æ”¹åçš„å›¾åƒå’Œæ©ç å¯ç”¨äºä¸åŒçš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¼‚å¸¸åˆ†å‰²ã€‚(2) å¼‚å¸¸åˆ†å‰²å¾®è°ƒï¼šPOCç®¡é“å¯ç”¨äºç”Ÿæˆå¼‚å¸¸åˆ†å‰²æ•°æ®é›†ã€‚åœ¨è¿™äº›æ•°æ®é›†ä¸Šå¾®è°ƒå¼‚å¸¸åˆ†å‰²æ¨¡å‹å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä¼˜äºé€šè¿‡æ ‡å‡†åšæ³•ï¼ˆæ‹¼æ¥COCOå¯¹è±¡ï¼‰è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚(3) å­¦ä¹ æ–°ç±»åˆ«ï¼šPOCç®¡é“è¿˜å¯ä»¥ç”¨äºå­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åŠ¨ç‰©ç±»åˆ«å¢å¼ºCityscapeså›¾åƒå¯¼è‡´Pascalæµ‹è¯•é›†ä¸Šçš„93.14mIoUï¼ˆè€ƒè™‘ç›¸åŒçš„ç±»åˆ«ï¼‰ï¼Œè€Œç›´æ¥åœ¨Pascalä¸Šè®­ç»ƒäº§ç”Ÿ94.75â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨POCç¼–è¾‘çš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹äº§ç”Ÿäº†ç›¸å½“å°çš„æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚</li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›8. ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¾ç½®å¯¹è±¡åœ¨ä¸Šä¸‹æ–‡ï¼ˆPOCï¼‰ç®¡é“ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å°†ä»»æ„å¯¹è±¡ç°å®åœ°æ·»åŠ åˆ°ä»»ä½•å›¾åƒä¸­ã€‚POCå¯ç”¨äºè½»æ¾åœ°ä½¿ç”¨ä»»æ„æ•°é‡çš„å¯¹è±¡æ‰©å±•ä»»ä½•æ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºPOCç”Ÿæˆçš„ä¸åŒå¼‚å¸¸åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶è¡¨æ˜POCå¯ä»¥æé«˜å‡ ç§æ ‡å‡†åŸºå‡†ä¸­æœ€è¿‘çš„å¼‚å¸¸ç²¾ç»†è°ƒæ•´æ–¹æ³•çš„æ€§èƒ½ã€‚POCè¿˜å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒé€šè¿‡æ·»åŠ Pascalç±»åˆ«çš„å­é›†æ¥ç¼–è¾‘Cityscapesæ ·æœ¬ï¼Œå¹¶è¡¨æ˜åœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸Pascalè®­ç»ƒçš„åŸºçº¿å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¯å®äº†åœ¨POCç”Ÿæˆçš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹çš„ä½æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§æ”¾ç½®å¯¹è±¡åœ¨ä¸Šä¸‹æ–‡ï¼ˆPOCï¼‰ç®¡é“ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å°†ä»»æ„å¯¹è±¡ç°å®åœ°æ·»åŠ åˆ°ä»»ä½•å›¾åƒä¸­ã€‚- POCå¯ç”¨äºè½»æ¾åœ°ä½¿ç”¨ä»»æ„æ•°é‡çš„å¯¹è±¡æ‰©å±•ä»»ä½•æ•°æ®é›†ã€‚- POCå¯ä»¥æé«˜å‡ ç§æ ‡å‡†åŸºå‡†ä¸­æœ€è¿‘çš„å¼‚å¸¸ç²¾ç»†è°ƒæ•´æ–¹æ³•çš„æ€§èƒ½ã€‚- POCè¿˜å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æ–°ç±»åˆ«ã€‚æ€§èƒ½ï¼š- åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºPOCç”Ÿæˆçš„ä¸åŒå¼‚å¸¸åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶è¡¨æ˜POCå¯ä»¥æé«˜å‡ ç§æ ‡å‡†åŸºå‡†ä¸­æœ€è¿‘çš„å¼‚å¸¸ç²¾ç»†è°ƒæ•´æ–¹æ³•çš„æ€§èƒ½ã€‚- POCè¿˜å¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒé€šè¿‡æ·»åŠ Pascalç±»åˆ«çš„å­é›†æ¥ç¼–è¾‘Cityscapesæ ·æœ¬ï¼Œå¹¶è¡¨æ˜åœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸Pascalè®­ç»ƒçš„åŸºçº¿å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚è¿™è¯å®äº†åœ¨POCç”Ÿæˆçš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹çš„ä½æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚å·¥ä½œé‡ï¼š- POCç®¡é“ç”±å›¾åƒä¿®å¤æ¨¡å‹å’Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ç»„æˆã€‚å›¾åƒä¿®å¤æ¨¡å‹ç”¨äºå°†å¯¹è±¡ç°å®åœ°æ’å…¥å›¾åƒä¸­ï¼Œè€Œå¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ç”¨äºä¸ºæ’å…¥çš„å¯¹è±¡ç”Ÿæˆæ©ç ã€‚- POCç®¡é“å¯ç”¨äºç”Ÿæˆå¼‚å¸¸åˆ†å‰²æ•°æ®é›†ã€‚åœ¨è¿™äº›æ•°æ®é›†ä¸Šå¾®è°ƒå¼‚å¸¸åˆ†å‰²æ¨¡å‹å¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä¼˜äºé€šè¿‡æ ‡å‡†åšæ³•ï¼ˆæ‹¼æ¥COCOå¯¹è±¡ï¼‰è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚- POCç®¡é“è¿˜å¯ä»¥ç”¨äºå­¦ä¹ æ–°ç±»åˆ«ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨åŠ¨ç‰©ç±»åˆ«å¢å¼ºCityscapeså›¾åƒå¯¼è‡´Pascalæµ‹è¯•é›†ä¸Šçš„93.14mIoUï¼ˆè€ƒè™‘ç›¸åŒçš„ç±»åˆ«ï¼‰ï¼Œè€Œç›´æ¥åœ¨Pascalä¸Šè®­ç»ƒäº§ç”Ÿ94.75â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨POCç¼–è¾‘çš„å›¾åƒä¸Šè®­ç»ƒçš„æ¨¡å‹äº§ç”Ÿäº†ç›¸å½“å°çš„æ¨¡æ‹Ÿåˆ°çœŸå®å·®è·ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-13236ee2bf286b59f5da0689a0363f64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec0e216eb8083342215a3e4e8c1dc95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2067d81b02e8cd7fea592f12fcef21d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37aa0eb4c5f86ae9ed22c98b2703f9a5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-84f58d6d1052332176a17f015aaa2d9f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-29  Objective and Interpretable Breast Cosmesis Evaluation with Attention   Guided Denoising Diffusion Anomaly Detection Model</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/NeRF/</id>
    <published>2024-02-22T18:02:35.000Z</published>
    <updated>2024-02-22T18:02:35.955Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-23-æ›´æ–°"><a href="#2024-02-23-æ›´æ–°" class="headerlink" title="2024-02-23 æ›´æ–°"></a>2024-02-23 æ›´æ–°</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br> NeurRF åŠ é€Ÿï¼šä¸€ç§æ–°çš„è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡å¿«é€Ÿè¯†åˆ«ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“åœ¨å®æ—¶æ¸²æŸ“å½“å‰è§†å›¾ï¼Œä»è€Œæé«˜æ¸²æŸ“é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>NeurRF æ˜¯ä¸€ç§æ–°çš„æ¸²æŸ“æ–¹æ³•ï¼Œåˆ©ç”¨æ•°ç™¾ä¸‡ä¸ª 3D é«˜æ–¯ä½“æ¥è¡¨ç¤º 3D åœºæ™¯ï¼Œå¹¶å°†å…¶æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šè¿›è¡Œæ¸²æŸ“ã€‚</li><li>åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œå­˜åœ¨å¤§é‡å¯¹äºå½“å‰è§†å›¾æ–¹å‘æ¥è¯´ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ï¼Œå¯¼è‡´è¯†åˆ«è¿™äº›é«˜æ–¯ä½“çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚</li><li>æå‡ºäº†ä¸€ç§è®¡ç®—ç®€åŒ–æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å®æ—¶å¿«é€Ÿè¯†åˆ«ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œåœ¨ä¸å½±å“å›¾åƒè´¨é‡çš„æƒ…å†µä¸‹æ¸²æŸ“å½“å‰è§†å›¾ã€‚</li><li>è¯¥æŠ€æœ¯é€šè¿‡å¯¹è·ç¦»ç›¸è¿‘çš„ 3D é«˜æ–¯ä½“è¿›è¡Œç¦»çº¿èšç±»æ¥å®ç°ï¼Œç„¶ååœ¨è¿è¡Œæ—¶å°†è¿™äº›ç°‡æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šã€‚</li><li>åˆ†æäº†è¯¥æŠ€æœ¯åœ¨ GPU ä¸Šæ‰§è¡Œæ—¶é‡åˆ°çš„ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç¡¬ä»¶æ¶æ„æ¥æ— ç¼æ”¯æŒè¯¥æ–¹æ¡ˆã€‚</li><li>å¯¹äº Mip-NeRF360 æ•°æ®é›†ï¼Œè¯¥æŠ€æœ¯åœ¨ 2D å›¾åƒæŠ•å½±å‰å¹³å‡æ’é™¤ 63% çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—é‡å‡å°‘äº†è¿‘ 38.3%ï¼Œè€Œå³°å€¼ä¿¡å™ªæ¯” (PSNR) å´ä¸ä¼šä¸‹é™ã€‚</li><li>æ‰€æå‡ºçš„åŠ é€Ÿå™¨ä¸ GPU ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº† 10.7 å€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šä½¿ç”¨èšç±»è¯†åˆ«ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ï¼Œå®ç° 3D é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“çš„å¿«é€Ÿæ¸²æŸ“</li><li>ä½œè€…ï¼šJoongho Jo, Hyeongwon Kim, Jongsun Park</li><li>å•ä½ï¼šéŸ©å›½å¤§å­¦ç”µæ°”å·¥ç¨‹å­¦é™¢ï¼ˆä»…ç¿»è¯‘å•ä½åç§°ï¼‰</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ã€æ¸²æŸ“ã€NeRFã€ç¥ç»è¾å°„åœºã€ç¡¬ä»¶åŠ é€Ÿå™¨</li><li>é“¾æ¥ï¼šPaper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splattingï¼ŒGithubï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨ 3D è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­ï¼Œä¾‹å¦‚å¢å¼ºç°å® (AR)ã€è™šæ‹Ÿç°å® (VR) å’Œå…ƒå®‡å®™ï¼Œå¿«é€Ÿä¸”é«˜è´¨é‡çš„å›¾åƒæ¸²æŸ“éå¸¸é‡è¦ã€‚è™½ç„¶ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¸²æŸ“æŠ€æœ¯ï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœº (NeRF)ï¼Œå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½† 3D é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ (3D-GS) ä½œä¸ºä¸€ç§æ–°çš„æ¸²æŸ“æ–¹æ³•ï¼Œå› å…¶ä¸ä¼ ç»Ÿ NeRF ç›¸æ¯”èƒ½å¤Ÿå¿«é€Ÿæ¸²æŸ“é«˜è´¨é‡å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚3D-GS åˆ©ç”¨æ•°ç™¾ä¸‡ä¸ª 3D é«˜æ–¯ä½“æ¥è¡¨ç¤ºå¤æ‚çš„ 3D åœºæ™¯ï¼Œå¹¶é€šè¿‡å°† 3D é«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šæ¥æ¸²æŸ“ 3D åœºæ™¯ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D-GS æ¸²æŸ“è¿‡ç¨‹ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼š1ï¼‰å°†æ‰€æœ‰ 3D é«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œå¹¶è¯†åˆ«å‡ºå½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“ã€‚2ï¼‰ç„¶åä½¿ç”¨å½±å“é¢œè‰²çš„å·²è¯†åˆ« 3D é«˜æ–¯ä½“è®¡ç®— 2D å›¾åƒä¸­æ¯ä¸ªåƒç´ çš„é¢œè‰²ã€‚åœ¨æ¸²æŸ“è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥ä¸­ï¼Œé«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒä¸Šï¼Œä½†è¢«è¯†åˆ«ä¸ºä¸å½±å“ 2D å›¾åƒçš„é¢œè‰²ï¼ŒæŠ•å½±å°±å˜æˆäº†è®¡ç®—æµªè´¹ã€‚åœ¨ Mip-NeRF360 æ•°æ®é›†ä¸­ï¼Œå¹³å‡çº¦æœ‰ 67.6% çš„ 3D é«˜æ–¯ä½“ä¸å½±å“ 2D å›¾åƒçš„é¢œè‰²ã€‚å› æ­¤ï¼Œè¿™äº›é«˜æ–¯ä½“å¯ä»¥ä»å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­æ’é™¤ã€‚ç„¶è€Œï¼Œç”±äºå½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“å¯èƒ½ä¼šéšç€æ¸²æŸ“è§†ç‚¹çš„ä½ç½®å’Œæ–¹å‘è€Œæ”¹å˜ï¼Œå› æ­¤åœ¨å°†å®ƒä»¬æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢å‰è¯†åˆ«å‡ºä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œè¿™äº›ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ä»ç„¶ä¼šè¿›è¡ŒæŠ•å½±è®¡ç®—ã€‚å› æ­¤ï¼Œå¦‚æœèƒ½å¤Ÿå¼€å‘ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥é¢„æµ‹ä¸å½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“ï¼Œå¹¶åœ¨æ¸²æŸ“è¿‡ç¨‹å¼€å§‹å‰å°†å®ƒä»¬æ’é™¤ï¼Œåˆ™å¯ä»¥æ˜¾ç€é™ä½æ•´ä¸ª 3D-GS è¿‡ç¨‹çš„æ€»ä½“è®¡ç®—å¤æ‚åº¦ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«ä¸å½±å“ 2D å›¾åƒé¢œè‰²çš„ç°‡æ¥æ’é™¤å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ä¸å¿…è¦ 3D é«˜æ–¯ä½“ã€‚èšç±»çš„ç›®çš„æ˜¯å°†ä½ç½®ç›¸è¿‘çš„ 3D é«˜æ–¯ä½“åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”ç°‡çš„å½¢çŠ¶åº”è¯¥æ˜¯çƒå½¢çš„ï¼Œä»¥ä¾¿äºæŠ•å½±åˆ° 2D å›¾åƒä¸Šã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† K-means èšç±»ç®—æ³•ï¼Œå®ƒæ»¡è¶³è¿™ä¸¤ä¸ªæ ‡å‡†ã€‚é‰´äº 3D é«˜æ–¯ä½“å…·æœ‰ç”±å…¶åæ–¹å·®å®šä¹‰çš„å¤§å°æˆ–å½±å“ï¼Œç°‡çƒä½“çš„åŠå¾„ä¸ä»…ç”±åˆ°ç°‡è´¨å¿ƒçš„è·ç¦»å†³å®šï¼Œè¿˜è€ƒè™‘äº†é«˜æ–¯ä½“çš„å¤§å°ã€‚ç„¶åå°†è¿™äº›å®šä¹‰çš„ç°‡çƒä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œä»¥ç¡®å®šå®ƒä»¬å¯¹ 2D å›¾åƒé¢œè‰²çš„å½±å“ã€‚ä¸å½±å“å›¾åƒé¢œè‰²çš„ç°‡å¯ä»¥ä»æ¸²æŸ“è¿‡ç¨‹ä¸­æ’é™¤ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œèšç±»å’Œè®¡ç®—ç°‡çš„åŠå¾„å¯ä»¥åœ¨çº¿â€‹â€‹ä¸‹æ‰§è¡Œï¼Œåªæœ‰å°†ç°‡æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šæ˜¯åœ¨å®æ—¶è¿›è¡Œçš„ï¼Œè¿™ä»…éœ€è¦ 6.2% çš„è®¡ç®—å¼€é”€ã€‚åœ¨ 3D-GS æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œåœ¨å½“å‰è§†å›¾æ¸²æŸ“ä¹‹å‰åº”ç”¨æ‰€æå‡ºçš„æ–¹æ³•æ—¶ï¼Œå¹³å‡å¯ä»¥æ’é™¤ 63% çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ï¼Œè€Œä¸ä¼šç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR)ã€‚æ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†æ¯” GPU å¿« 10.7 å€çš„é€Ÿåº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ Mip-NeRF360 æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ’é™¤ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œå‡å°‘æ¸²æŸ“è®¡ç®—é‡å¹¶æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ’é™¤å¹³å‡ 63% çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ï¼Œè€Œä¸ä¼šç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR)ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†æ¯” GPU å¿« 10.7 å€çš„é€Ÿåº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°å…¶ç›®æ ‡ï¼Œå³å¿«é€Ÿæ¸²æŸ“é«˜è´¨é‡çš„ 3D å›¾åƒã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•ï¼Œé€šè¿‡è¯†åˆ«ä¸å½±å“2Då›¾åƒé¢œè‰²çš„ç°‡æ¥æ’é™¤å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ä¸å¿…è¦3Dé«˜æ–¯ä½“ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘æ¸²æŸ“è®¡ç®—é‡å¹¶æé«˜æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨Mip-NeRF360æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥æ’é™¤å¹³å‡63%çš„3Dé«˜æ–¯ä½“ï¼Œä»è€Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘38.3%ï¼Œè€Œä¸ä¼šç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†æ¯”GPUå¿«10.7å€çš„é€Ÿåº¦ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•æ¥è¯†åˆ«ä¸å¿…è¦çš„3Dé«˜æ–¯ä½“ï¼Œè¯¥æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œèƒ½å¤Ÿæ˜¾ç€é™ä½3D-GSæ¸²æŸ“è¿‡ç¨‹çš„æ€»ä½“è®¡ç®—å¤æ‚åº¦ã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ’é™¤ä¸å¿…è¦çš„3Dé«˜æ–¯ä½“ï¼Œä»è€Œå‡å°‘æ¸²æŸ“è®¡ç®—é‡å¹¶æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚åœ¨Mip-NeRF360æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥æ’é™¤å¹³å‡63%çš„3Dé«˜æ–¯ä½“ï¼Œä»è€Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘38.3%ï¼Œè€Œä¸ä¼šç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†æ¯”GPUå¿«10.7å€çš„é€Ÿåº¦ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨çº¿â€‹â€‹ä¸‹æ‰§è¡Œèšç±»å’Œè®¡ç®—ç°‡çš„åŠå¾„ï¼Œåªæœ‰å°†ç°‡æŠ•å½±åˆ°2Då›¾åƒå¹³é¢ä¸Šæ˜¯åœ¨å®æ—¶è¿›è¡Œçš„ï¼Œè¿™ä»…éœ€è¦6.2%çš„è®¡ç®—å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow"><a href="#OccFlowNet-Towards-Self-supervised-Occupancy-Estimation-via-Differentiable-Rendering-and-Occupancy-Flow" class="headerlink" title="OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow"></a>OccFlowNet: Towards Self-supervised Occupancy Estimation via   Differentiable Rendering and Occupancy Flow</h2><p><strong>Authors:Simon Boeder, Fabian Gigengack, Benjamin Risse</strong></p><p>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain. </p><p><a href="http://arxiv.org/abs/2402.12792v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºå¯ä»ä»…ä½¿ç”¨äºŒç»´æ ‡ç­¾ä¸­ä¼°è®¡è¯­ä¹‰å ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨äºŒç»´æ ‡ç­¾ä¼°è®¡å ç”¨ç‡çš„æ–°æ–¹æ³•ã€‚</li><li>é‡‡ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æ¥é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰å›¾ï¼Œå¹¶ä»…åŸºäºäºŒç»´ç›‘ç£è®­ç»ƒä¸‰ç»´ç½‘ç»œã€‚</li><li>ä¸ºäº†å¢å¼ºå‡ ä½•ç²¾åº¦å¹¶å¢åŠ ç›‘ç£ä¿¡å·ï¼Œå¼•å…¥äº†ç›¸é‚»æ—¶é—´æ­¥é•¿çš„æ—¶åºæ¸²æŸ“ã€‚</li><li>å¼•å…¥å ç”¨æµä½œä¸ºå¤„ç†åœºæ™¯ä¸­åŠ¨æ€å¯¹è±¡å¹¶ç¡®ä¿å…¶æ—¶é—´ä¸€è‡´æ€§çš„æœºåˆ¶ã€‚</li><li>ä¸ä½¿ç”¨ä¸‰ç»´æ ‡ç­¾çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®éªŒè¡¨æ˜ä»…äºŒç»´ç›‘ç£å°±è¶³ä»¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¼˜äºåŒæ—¶æœŸçš„äºŒç»´æ–¹æ³•ã€‚</li><li>å½“å°†äºŒç»´ç›‘ç£ä¸ä¸‰ç»´æ ‡ç­¾ã€æ—¶æ€æ¸²æŸ“å’Œå ç”¨æµç›¸ç»“åˆæ—¶ï¼Œå¤§å¤§ä¼˜äºæ‰€æœ‰ä»¥å‰çš„å æœ‰ä¼°è®¡æ¨¡å‹ã€‚</li><li>æ¸²æŸ“ç›‘ç£å’Œå ç”¨æµçš„è¿›æ­¥ä¿ƒè¿›äº†å ç”¨ä¼°è®¡ï¼Œå¹¶è¿›ä¸€æ­¥ç¼©å°äº†è¯¥é¢†åŸŸä¸­è‡ªç›‘ç£å­¦ä¹ çš„å·®è·ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šOccFlowNetï¼šåŸºäºå¯å¾®æ¸²æŸ“å’Œå ç”¨æµçš„è‡ªç›‘ç£å ç”¨ä¼°è®¡</li><li>ä½œè€…ï¼šSimon Boeder, Fabian Gigengack, Benjamin Risse</li><li>ä½œè€…å•ä½ï¼šåšä¸–å…¬å¸ã€æ˜æ–¯ç‰¹å¤§å­¦</li><li>å…³é”®è¯ï¼šå ç”¨ä¼°è®¡ã€ç¥ç»è¾å°„åœºã€å¯å¾®æ¸²æŸ“ã€å ç”¨æµã€è‡ªç›‘ç£å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12792</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè¯­ä¹‰å ç”¨æœ€è¿‘ä½œä¸ºä¸€ç§çªå‡ºçš„ 3D åœºæ™¯è¡¨ç¤ºå½¢å¼è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå…·æœ‰ç»†ç²’åº¦ 3D ä½“ç´ æ ‡ç­¾çš„å¤§å‹ä¸”æ˜‚è´µçš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¢åŠ äº†è¯¥é¢†åŸŸä¸­è‡ªç›‘ç£å­¦ä¹ çš„éœ€æ±‚ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†å—ç¥ç»è¾å°„åœº (NeRF) å¯å‘çš„æ–°å‹å ç”¨ä¼°è®¡æ–¹æ³•ï¼Œä»…ä½¿ç”¨æ›´æ˜“è·å–çš„ 2D æ ‡ç­¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æ¥é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰å›¾ï¼Œå¹¶ä»…åŸºäº 2D ç›‘ç£è®­ç»ƒ 3D ç½‘ç»œã€‚ä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦å¹¶å¢åŠ ç›‘ç£ä¿¡å·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›¸é‚»æ—¶é—´æ­¥çš„é•¿æ—¶æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å ç”¨æµä½œä¸ºå¤„ç†åœºæ™¯ä¸­åŠ¨æ€å¯¹è±¡å¹¶ç¡®ä¿å…¶æ—¶é—´ä¸€è‡´æ€§çš„æœºåˆ¶ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨ 2D ç›‘ç£å°±è¶³ä»¥ä¸ä½¿ç”¨ 3D æ ‡ç­¾çš„æ–¹æ³•ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¼˜äºåŒæ—¶æœŸçš„ 2D æ–¹æ³•ã€‚å½“å°† 2D ç›‘ç£ä¸ 3D æ ‡ç­¾ã€æ—¶åºæ¸²æŸ“å’Œå ç”¨æµç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬æ˜æ˜¾ä¼˜äºæ‰€æœ‰ä»¥å‰çš„å ç”¨ä¼°è®¡æ¨¡å‹ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œæ‰€æå‡ºçš„æ¸²æŸ“ç›‘ç£å’Œå ç”¨æµä¿ƒè¿›äº†å ç”¨ä¼°è®¡ï¼Œå¹¶è¿›ä¸€æ­¥ç¼©å°äº†è¯¥é¢†åŸŸä¸­è‡ªç›‘ç£å­¦ä¹ çš„å·®è·ã€‚(4) æ€§èƒ½å’Œç»“è®ºï¼šåœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å ç”¨ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³ä»…ä½¿ç”¨ 2D ç›‘ç£å°±å¯ä»¥å®ç°å‡†ç¡®çš„å ç”¨ä¼°è®¡ï¼Œä»è€Œä½¿è¯¥æ–¹æ³•æ›´æ˜“äºè®­ç»ƒå’Œéƒ¨ç½²ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1)ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å ç”¨ä¼°è®¡æ–¹æ³• OccFlowNetï¼Œä»…ä½¿ç”¨æ›´æ˜“è·å–çš„ 2D æ ‡ç­¾ï¼Œæ— éœ€æ˜‚è´µçš„ 3D ä½“ç´ æ ‡ç­¾ã€‚(2)ï¼šæˆ‘ä»¬é‡‡ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æ¥é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰å›¾ï¼Œå¹¶ä»…åŸºäº 2D ç›‘ç£è®­ç»ƒ 3D ç½‘ç»œã€‚(3)ï¼šä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦å¹¶å¢åŠ ç›‘ç£ä¿¡å·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›¸é‚»æ—¶é—´æ­¥çš„é•¿æ—¶æ¸²æŸ“ã€‚(4)ï¼šæˆ‘ä»¬å¼•å…¥äº†å ç”¨æµä½œä¸ºå¤„ç†åœºæ™¯ä¸­åŠ¨æ€å¯¹è±¡å¹¶ç¡®ä¿å…¶æ—¶é—´ä¸€è‡´æ€§çš„æœºåˆ¶ã€‚(5)ï¼šæˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨ 2D ç›‘ç£å°±è¶³ä»¥ä¸ä½¿ç”¨ 3D æ ‡ç­¾çš„æ–¹æ³•ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¼˜äºåŒæ—¶æœŸçš„ 2D æ–¹æ³•ã€‚(6)ï¼šå½“å°† 2D ç›‘ç£ä¸ 3D æ ‡ç­¾ã€æ—¶åºæ¸²æŸ“å’Œå ç”¨æµç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬æ˜æ˜¾ä¼˜äºæ‰€æœ‰ä»¥å‰çš„å ç”¨ä¼°è®¡æ¨¡å‹ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨æ˜“äºè·å–çš„ 2D æ ‡ç­¾å³å¯è¿›è¡Œå ç”¨ä¼°è®¡çš„æ–¹æ³•ï¼Œæ— éœ€æ˜‚è´µçš„ 3D ä½“ç´ æ ‡ç­¾ï¼Œä¸ºå ç”¨ä¼°è®¡ä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ›æ–°ç‚¹ 1ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å ç”¨ä¼°è®¡æ–¹æ³• OccFlowNetï¼Œä»…ä½¿ç”¨æ›´æ˜“è·å–çš„ 2D æ ‡ç­¾ï¼Œæ— éœ€æ˜‚è´µçš„ 3D ä½“ç´ æ ‡ç­¾ã€‚åˆ›æ–°ç‚¹ 2ï¼šé‡‡ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æ¥é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰å›¾ï¼Œå¹¶ä»…åŸºäº 2D ç›‘ç£è®­ç»ƒ 3D ç½‘ç»œã€‚åˆ›æ–°ç‚¹ 3ï¼šä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦å¹¶å¢åŠ ç›‘ç£ä¿¡å·ï¼Œå¼•å…¥äº†ç›¸é‚»æ—¶é—´æ­¥çš„é•¿æ—¶æ¸²æŸ“ã€‚åˆ›æ–°ç‚¹ 4ï¼šå¼•å…¥äº†å ç”¨æµä½œä¸ºå¤„ç†åœºæ™¯ä¸­åŠ¨æ€å¯¹è±¡å¹¶ç¡®ä¿å…¶æ—¶é—´ä¸€è‡´æ€§çš„æœºåˆ¶ã€‚æ€§èƒ½ï¼šåœ¨å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å ç”¨ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šæœ¬å·¥ä½œéœ€è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼Œå¦‚ä½•ä»…ä½¿ç”¨ 2D æ ‡ç­¾è¿›è¡Œå ç”¨ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº† OccFlowNet æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å¯å¾®ä½“ç§¯æ¸²æŸ“æ¥é¢„æµ‹æ·±åº¦å’Œè¯­ä¹‰å›¾ï¼Œå¹¶ä»…åŸºäº 2D ç›‘ç£è®­ç»ƒ 3D ç½‘ç»œã€‚ä¸ºäº†æé«˜å‡ ä½•ç²¾åº¦å¹¶å¢åŠ ç›‘ç£ä¿¡å·ï¼Œä½œè€…å¼•å…¥äº†ç›¸é‚»æ—¶é—´æ­¥çš„é•¿æ—¶æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜å¼•å…¥äº†å ç”¨æµä½œä¸ºå¤„ç†åœºæ™¯ä¸­åŠ¨æ€å¯¹è±¡å¹¶ç¡®ä¿å…¶æ—¶é—´ä¸€è‡´æ€§çš„æœºåˆ¶ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œä½œè€…è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨å ç”¨ä¼°è®¡ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-48dbaf92efe683516d537be273981834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff303fd6f4dc54f5b59e902e9b98c34a.jpg" align="middle"></details><h2 id="Colorizing-Monochromatic-Radiance-Fields"><a href="#Colorizing-Monochromatic-Radiance-Fields" class="headerlink" title="Colorizing Monochromatic Radiance Fields"></a>Colorizing Monochromatic Radiance Fields</h2><p><strong>Authors:Yean Cheng, Renjie Wan, Shuchen Weng, Chengxuan Zhu, Yakun Chang, Boxin Shi</strong></p><p>Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: <a href="https://liquidammonia.github.io/color-nerf">https://liquidammonia.github.io/color-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.12184v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¯é€šè¿‡ä¸€ç»„äºŒç»´å›¾åƒäº§ç”Ÿè‰²å½©é²œè‰³çš„ 3D åœºæ™¯å†ç°ï¼Œä½†ä»…æä¾›å•è‰²å›¾åƒæ—¶ä¾¿æ— æ³•å®ç°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF å¯ä»¥ä½¿ç”¨ä¸€ç»„ 2D å›¾åƒç”Ÿæˆä¸–ç•Œçš„å½©è‰² 3D è¡¨ç¤ºã€‚</li><li>ä»…æä¾›å•è‰²å›¾åƒæ—¶ï¼ŒNeRF æ— æ³•ç”Ÿæˆå½©è‰² 3D è¡¨ç¤ºã€‚</li><li>NeRF çš„ç›®æ ‡æ˜¯ä»å•è‰²è¾å°„åœºå†ç°å½©è‰²è¡¨ç¤ºã€‚</li><li>æå‡ºäº†ä¸€ç§åœ¨ Lab é¢œè‰²ç©ºé—´ä¸­å°†å•è‰²è¾å°„åœºè§†ä¸ºè¡¨ç¤ºé¢„æµ‹ä»»åŠ¡çš„æ–¹æ³•ã€‚</li><li>é¦–å…ˆä½¿ç”¨å•è‰²å›¾åƒæ„å»ºäº®åº¦å’Œå¯†åº¦è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨å›¾åƒç€è‰²æ¨¡å—é‡æ–°åˆ›å»ºé¢œè‰²è¡¨ç¤ºã€‚</li><li>ç„¶åé€šè¿‡äº®åº¦ã€å¯†åº¦å’Œé¢œè‰²çš„è¡¨ç¤ºå†ç°ä¸€ä¸ªå½©è‰²éšå¼æ¨¡å‹ã€‚</li><li>å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå½©è‰²åŒ–å•è‰²è¾å°„åœº</li><li>ä½œè€…ï¼šå¶å®‰æˆã€ä¸‡ä»»æ°<em>ã€ç¿ä¹¦ç›ã€æœ±æ‰¿è½©ã€å¸¸äºšå¤ã€çŸ³åšæ¬£</em></li><li>éš¶å±å•ä½ï¼šåŒ—äº¬å¤§å­¦å¤šåª’ä½“ä¿¡æ¯å¤„ç†å›½å®¶é‡ç‚¹å®éªŒå®¤ã€è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šNeRFã€å•è‰²å›¾åƒã€é¢œè‰²å†ç°ã€Labé¢œè‰²ç©ºé—´ã€å›¾åƒç€è‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12184   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¯ä»¥åˆ©ç”¨ä¸€ç»„äºŒç»´å›¾åƒåˆ›å»ºä¸–ç•Œçš„å½©è‰²ä¸‰ç»´è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“åªæœ‰å•è‰²å›¾åƒå¯ç”¨æ—¶ï¼Œè¿™ç§èƒ½åŠ›å°±ä¸å¤å­˜åœ¨äº†ã€‚é¢œè‰²å¯¹äºè¡¨å¾ä¸–ç•Œæ˜¯å¿…è¦çš„ï¼Œå› æ­¤ä»å•è‰²è¾å°„åœºä¸­å†ç°é¢œè‰²å˜å¾—è‡³å…³é‡è¦ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç›´æ¥æ“çºµå•è‰²è¾å°„åœºä¼¼ä¹æ˜¯å®ç°é¢œè‰²åŒ–çš„ç›´æ¥æ–¹æ³•ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯å°†é¢œè‰²è§†ä¸ºä¸€ç§â€œé£æ ¼â€ï¼Œç„¶åå°†å…¶è½¬ç§»åˆ°è¾å°„åœºä¸­ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥å¹¶ä¸èƒ½ä¿è¯é€åƒç´ çš„é¢œè‰²ä¸€è‡´æ€§ï¼Œå› æ­¤é¢œè‰²åªèƒ½ä¸è§„åˆ™åœ°åˆ†å¸ƒåœ¨è¾å°„åœºä¸­ï¼Œä»è€Œè¿èƒŒäº†åˆç†æ€§æ ‡å‡†ã€‚å¦ä¸€ç§æ–¹æ³•æ¶‰åŠç›´æ¥æ“çºµè¾å°„åœºä¸­çš„é¢œè‰²å±æ€§ã€‚è¿™ç§æŠ€æœ¯æ—¨åœ¨é€šè¿‡è¯†åˆ«å½“å‰çš„é¢œè‰²å±æ€§å¹¶ç”¨æ–°çš„é¢œè‰²å±æ€§æ›¿æ¢å®ƒä»¬æ¥æ›¿æ¢é¢œè‰²ã€‚ç„¶è€Œï¼Œå®ƒä¸é€‚ç”¨äºæ²¡æœ‰ç°æœ‰é¢œè‰²å±æ€§çš„å•è‰²è¾å°„åœºã€‚   ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ Lab é¢œè‰²ç©ºé—´ä¸­è¿›è¡Œè¡¨ç¤ºé¢„æµ‹çš„ä»»åŠ¡ã€‚é¦–å…ˆä½¿ç”¨å•è‰²å›¾åƒæ„å»ºäº®åº¦å’Œå¯†åº¦è¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨å›¾åƒç€è‰²æ¨¡å—é‡æ–°åˆ›å»ºé¢œè‰²è¡¨ç¤ºã€‚æœ€åï¼Œé€šè¿‡äº®åº¦ã€å¯†åº¦å’Œé¢œè‰²çš„è¡¨ç¤ºæ¥å†ç°ä¸€ä¸ªå½©è‰²éšå¼æ¨¡å‹ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶å¯¹ç›®æ ‡çš„æ”¯æŒï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•è‰²å›¾åƒç€è‰²ã€å¤šè§†å›¾å›¾åƒåˆæˆå’Œè§†é¢‘æ’å¸§ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»å•è‰²å›¾åƒä¸­å†ç°é¢œè‰²ï¼Œå¹¶ç”Ÿæˆé€¼çœŸä¸”è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„å½©è‰²ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æ„å»ºäº®åº¦å’Œå¯†åº¦è¡¨ç¤ºï¼šä½¿ç”¨å•è‰²å›¾åƒæ„å»ºäº®åº¦å’Œå¯†åº¦è¡¨ç¤ºï¼Œä¸ºåç»­çš„é¢œè‰²å†ç°æä¾›åŸºç¡€ã€‚(2) å›¾åƒç€è‰²æ¨¡å—ï¼šåˆ©ç”¨å›¾åƒç€è‰²æ¨¡å—é‡æ–°åˆ›å»ºé¢œè‰²è¡¨ç¤ºï¼Œå°†å•è‰²å›¾åƒä¸­çš„ä¿¡æ¯è½¬æ¢ä¸ºå½©è‰²è¡¨ç¤ºã€‚(3) è¡¨ç¤ºé¢„æµ‹ï¼šåœ¨Labé¢œè‰²ç©ºé—´ä¸­è¿›è¡Œè¡¨ç¤ºé¢„æµ‹ï¼Œå°†äº®åº¦ã€å¯†åº¦å’Œé¢œè‰²çš„è¡¨ç¤ºç›¸ç»“åˆï¼Œå†ç°ä¸€ä¸ªå½©è‰²éšå¼æ¨¡å‹ã€‚(4) é¢œè‰²æ³¨å…¥ï¼šåˆ©ç”¨åˆ†ç±»å™¨å°†é¢œè‰²æ³¨å…¥åˆ°è¾å°„åœºä¸­ï¼Œç¡®ä¿é¢œè‰²çš„åˆç†æ€§å’Œä¸€è‡´æ€§ã€‚(5) ç›´æ–¹å›¾å‡€åŒ–ï¼šä½¿ç”¨ç›´æ–¹å›¾å‡€åŒ–æ¨¡å—å»é™¤ä¸åˆç†çš„é¢œè‰²ï¼Œæé«˜é¢œè‰²çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•è‰²å›¾åƒä¸­å†ç°é¢œè‰²çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ºå•è‰²å›¾åƒçš„å½©è‰²åŒ–æä¾›äº†æ–°çš„æ€è·¯å’ŒæŠ€æœ¯æ”¯æŒã€‚</p><p>ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š</p><p>åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§åœ¨Labé¢œè‰²ç©ºé—´ä¸­è¿›è¡Œè¡¨ç¤ºé¢„æµ‹çš„ä»»åŠ¡ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å•è‰²å›¾åƒç€è‰²çš„é—®é¢˜ã€‚</li><li>æå‡ºäº†ä¸€ç§é¢œè‰²æ³¨å…¥æ¨¡å—ï¼Œç¡®ä¿äº†é¢œè‰²çš„åˆç†æ€§å’Œä¸€è‡´æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§ç›´æ–¹å›¾å‡€åŒ–æ¨¡å—ï¼Œå»é™¤ä¸åˆç†çš„é¢œè‰²ï¼Œæé«˜äº†é¢œè‰²çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li></ul><p>æ€§èƒ½ï¼š</p><ul><li>åœ¨å•è‰²å›¾åƒç€è‰²ã€å¤šè§†å›¾å›¾åƒåˆæˆå’Œè§†é¢‘æ’å¸§ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</li><li>ç”Ÿæˆçš„å½©è‰²ç»“æœé€¼çœŸä¸”è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦ã€‚</li></ul><p>å·¥ä½œé‡ï¼š</p><ul><li>éœ€è¦æ„å»ºäº®åº¦å’Œå¯†åº¦è¡¨ç¤ºã€å›¾åƒç€è‰²æ¨¡å—ã€é¢œè‰²æ³¨å…¥æ¨¡å—å’Œç›´æ–¹å›¾å‡€åŒ–æ¨¡å—ã€‚</li><li>éœ€è¦è®­ç»ƒæ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-53ef44a8d86663951eb27790c491bec4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40e071a248a066a783512765ca1dd311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04a5930c0187125fe64b74f7d43ea704.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08fb7fd6e14278c9083abd8d5401c6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34c76f358a2021ed97956d162ca195e3.jpg" align="middle"></details>## One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation**Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang**Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. [PDF](http://arxiv.org/abs/2402.11909v1) **Summary**ç”¨ä¸€å¼ æˆ–æ•°å¼ ç”¨æˆ·ç…§ç‰‡å’Œ 3DMM ç¼–ç å³å¯ç”Ÿæˆé«˜è´¨é‡ä¸”å¯æ§åŠ¨çš„å¤´åƒã€‚**Key Takeaways**- è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¸€å¼ æˆ–å¤šå¼ å›¾åƒåˆ›å»ºé«˜è´¨é‡å¤´åƒçš„æ–°æ–¹æ³•ã€‚- è¯¥æ–¹æ³•åˆ©ç”¨äº†ä¸€ä¸ªä» 2407 ä¸ªäººçš„å¤šè§†è§’é¢éƒ¨è¡¨æƒ…æ•°æ®é›†ä¸­å­¦å¾—çš„ç”Ÿæˆæ¨¡å‹ã€‚- è¯¥æ–¹æ³•ä½¿ç”¨äº†åŸºäº 3DMM çš„ç¥ç»è¾å°„åœºä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œä»¥å¢å¼ºé€šè¿‡å°‘é‡è¾“å…¥è¿›è¡Œè‡ªåŠ¨è§£ç çš„æ•ˆæœã€‚- è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡è”åˆä¼˜åŒ– 3DMM æ‹Ÿåˆå’Œç›¸æœºæ ¡å‡†æ¥å¤„ç†ä¸ç¨³å®šçš„ 3DMM æ‹Ÿåˆé—®é¢˜ã€‚- è¯¥ç ”ç©¶æå‡ºçš„æ–¹æ³•åœ¨å°‘é‡å›¾åƒå¤´åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚- è¯¥æ–¹æ³•ä¸ºæ›´é«˜æ•ˆå’Œä¸ªæ€§åŒ–çš„å¤´åƒç”Ÿæˆé“ºå¹³äº†é“è·¯ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šåŸºäº 3DMM çš„ç¥ç»è¾å°„åœºåœ¨è™šæ‹ŸåŒ–èº«çš„èº«ä»½å’Œè¡¨æƒ…å»ºæ¨¡ä¸­çš„åº”ç”¨</li><li>ä½œè€…ï¼šKangxue Yin, Changjian Li, Yebin Liu, Yue Dong, Kun Zhou, Chen Change Loy, Ziwei Liu</li><li>éš¶å±æœºæ„ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3DMMã€èº«ä»½å»ºæ¨¡ã€è¡¨æƒ…å»ºæ¨¡ã€è™šæ‹ŸåŒ–èº«</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09924ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè™šæ‹ŸåŒ–èº«åœ¨æ¸¸æˆã€ç¤¾äº¤åª’ä½“å’Œç”µå­å•†åŠ¡ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è™šæ‹ŸåŒ–èº«é€šå¸¸ç¼ºä¹çœŸå®æ„Ÿå’Œä¸ªæ€§åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨ 3D æ¨¡å‹æ¥è¡¨ç¤ºè™šæ‹ŸåŒ–èº«ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€ç¼ºä¹ç»†èŠ‚å’ŒçœŸå®æ„Ÿã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹å·¥åˆ¶ä½œï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥ä¸ªæ€§åŒ–ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3DMM çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹æ³•æ¥è¡¨ç¤ºè™šæ‹ŸåŒ–èº«ã€‚è¯¥æ–¹æ³•å°† 3DMM ä½œä¸ºè™šæ‹ŸåŒ–èº«çš„éª¨æ¶ï¼Œå¹¶ä½¿ç”¨ NeRF æ¥ç”Ÿæˆè™šæ‹ŸåŒ–èº«çš„è¡¨é¢ã€‚NeRF æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥ä»ä¸€ç»„ç¨€ç–çš„è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ ç”Ÿæˆè¿ç»­çš„è¡¨é¢ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚åœ¨èº«ä»½å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«ï¼Œè¿™äº›è™šæ‹ŸåŒ–èº«ä¸çœŸå®çš„äººç±»éå¸¸ç›¸ä¼¼ã€‚åœ¨è¡¨æƒ…å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«è¡¨æƒ…ï¼Œè¿™äº›è¡¨æƒ…ä¸çœŸå®çš„äººç±»è¡¨æƒ…éå¸¸ç›¸ä¼¼ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šå¤šè§†è§’å¤šè¡¨æƒ…äººè„¸æ•æ‰ï¼šæˆ‘ä»¬ä» 13 ä¸ªé¢„å®šä¹‰çš„é¢éƒ¨è¡¨æƒ…ä¸­æ•è·äº†æ€»å…± 2407 ä¸ªå—è¯•è€…çš„åˆ†è¾¨ç‡é¢éƒ¨å›¾åƒï¼Œè¿™äº›å›¾åƒæ¥è‡ª 13 ä¸ªç¨€ç–æ‘„åƒå¤´è§†è§’ã€‚å¯¹äºæ¯ä¸ªå—è¯•è€…åœ¨æ¯ä¸ªè¡¨æƒ…ä¸­ï¼Œæˆ‘ä»¬è¿è¡ŒåŸºäºé¢éƒ¨åœ°æ ‡çš„ 3DMM æ‹Ÿåˆç®—æ³•ï¼Œå¹¶ä»å¤šè§†è§’å›¾åƒä¸­é‡å»º 3D å‡ ä½•å½¢çŠ¶ã€‚ä¸ç°æœ‰çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ FFHQ ä¸­çš„ 70Kï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«æœ‰é™æ•°é‡çš„ç‹¬ç‰¹å—è¯•è€…ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒåŒ…å«æ›´å¹¿æ³›çš„é¢éƒ¨è¡¨æƒ…ï¼Œè¿™åœ¨å­¦ä¹ ç”Ÿæˆå¼å…ˆéªŒæ¨¡å‹ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ï¼ˆ2ï¼‰ï¼šç”Ÿæˆå¼å¤´åƒå…ˆéªŒï¼šæˆ‘ä»¬çš„ç”Ÿæˆå¼å¤´åƒå…ˆéªŒç”Ÿæˆäº†ä¸€ä¸ªç”±ç¥ç»è¾å°„åœºè¡¨ç¤ºçš„å¤´åƒã€‚ç»™å®šä¸€ä¸ªèº«ä»½ç¼–ç  w å’Œä¸€ä¸ªè¡¨æƒ…ç¼–ç  Ïˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹ f ä¸º 3D æŸ¥è¯¢ç‚¹ q ä»æ–¹å‘ d æŸ¥çœ‹æ—¶ç”Ÿæˆå±€éƒ¨é¢œè‰² c å’Œå¯†åº¦ Ïƒï¼šÏƒ(q), c(q) = f(w, Ïˆ, q, d; Î¸),å…¶ä¸­ Î¸ æ˜¯æ¨¡å‹æƒé‡ã€‚ç„¶åé€šè¿‡åº”ç”¨ä½“ç§¯æ¸²æŸ“å…¬å¼è·å¾—æ¯ä¸ªåƒç´ çš„é¢œè‰²æ¥ç”Ÿæˆå½©è‰²å›¾åƒï¼šË†c = âˆ«t^âˆ T(t)Ïƒq(r(t))cq(r(t), d)dt,å…¶ä¸­ T(t) = exp(âˆ’âˆ«^t^0 Ïƒq(r(s))ds)ã€‚éµå¾ªå…ˆå‰çš„è‰ºæœ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨ 3DMM è¡¨è¾¾å¼ä»£ç ç©ºé—´ä½œä¸º Ïˆï¼Œå¹¶å­¦ä¹  w çš„æ½œåœ¨ç©ºé—´ Rlã€‚ï¼ˆ3ï¼‰ï¼š3DMM é”šå®šå¤´åƒç”Ÿæˆæ¨¡å‹ï¼šå— Bai ç­‰äººå¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨ 3DMM é”šå®šçš„ç¥ç»è¾å°„åœºä½œä¸ºæˆ‘ä»¬çš„å¤´åƒè¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ä¼šå°†æ‰€æœ‰æ¸²æŸ“ä¿¡æ¯ç¼–ç åˆ°ä¸€ä¸ªé«˜å®¹é‡ç¥ç»ç½‘ç»œä¸­ï¼Œè€Œæ˜¯å°†å±€éƒ¨ç‰¹å¾é™„åŠ åœ¨é’ˆå¯¹ç›®æ ‡èº«ä»½å’Œè¡¨æƒ…é‡å»ºçš„ 3DMM ç½‘æ ¼æ”¯æ¶çš„é¡¶ç‚¹ä¸Šã€‚åœ¨æ¸²æŸ“æœŸé—´ï¼Œæ¯ä¸ªæŸ¥è¯¢ç‚¹èšåˆæ¥è‡ª 3DMM é¡¶ç‚¹ä¸­çš„ k ä¸ªæœ€è¿‘é‚» (kNN) çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶å‘é€åˆ° MLP ç½‘ç»œä»¥é¢„æµ‹é¢œè‰²å’Œå¯†åº¦ã€‚ä¸ºäº†ç®€åŒ–ä½¿ç”¨ç°æœ‰ 2D CNN çš„å­¦ä¹ ï¼Œå¯ä»¥åœ¨ç»Ÿä¸€çš„ UV ç©ºé—´ä¸­å­¦ä¹  3DMM é¡¶ç‚¹é™„åŠ ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨çº¹ç†åæ ‡è¿›è¡Œé‡‡æ ·ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº3DMMçš„ç¥ç»è¾å°„åœºæ–¹æ³•æ¥è¡¨ç¤ºè™šæ‹ŸåŒ–èº«ã€‚è¯¥æ–¹æ³•å°†3DMMä½œä¸ºè™šæ‹ŸåŒ–èº«çš„éª¨æ¶ï¼Œå¹¶ä½¿ç”¨NeRFæ¥ç”Ÿæˆè™šæ‹ŸåŒ–èº«çš„è¡¨é¢ã€‚NeRFæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥ä»ä¸€ç»„ç¨€ç–çš„è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ ç”Ÿæˆè¿ç»­çš„è¡¨é¢ã€‚è¯¥æ–¹æ³•åœ¨èº«ä»½å»ºæ¨¡å’Œè¡¨æƒ…å»ºæ¨¡ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäº3DMMçš„ç¥ç»è¾å°„åœºæ–¹æ³•æ¥è¡¨ç¤ºè™šæ‹ŸåŒ–èº«ã€‚</li><li>è¯¥æ–¹æ³•å°†3DMMä½œä¸ºè™šæ‹ŸåŒ–èº«çš„éª¨æ¶ï¼Œå¹¶ä½¿ç”¨NeRFæ¥ç”Ÿæˆè™šæ‹ŸåŒ–èº«çš„è¡¨é¢ã€‚</li><li>è¯¥æ–¹æ³•åœ¨èº«ä»½å»ºæ¨¡å’Œè¡¨æƒ…å»ºæ¨¡ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨èº«ä»½å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«ï¼Œè¿™äº›è™šæ‹ŸåŒ–èº«ä¸çœŸå®çš„äººç±»éå¸¸ç›¸ä¼¼ã€‚</li><li>åœ¨è¡¨æƒ…å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è™šæ‹ŸåŒ–èº«è¡¨æƒ…ï¼Œè¿™äº›è¡¨æƒ…ä¸çœŸå®çš„äººç±»è¡¨æƒ…éå¸¸ç›¸ä¼¼ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒã€‚</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹éå¸¸è€—æ—¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://picx.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details><h2 id="PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments"><a href="#PC-NeRF-Parent-Child-Neural-Radiance-Fields-Using-Sparse-LiDAR-Frames-in-Autonomous-Driving-Environments" class="headerlink" title="PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments"></a>PC-NeRF: Parent-Child Neural Radiance Fields Using Sparse LiDAR Frames   in Autonomous Driving Environments</h2><p><strong>Authors:Xiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</strong></p><p>Large-scale 3D scene reconstruction and novel view synthesis are vital for autonomous vehicles, especially utilizing temporally sparse LiDAR frames. However, conventional explicit representations remain a significant bottleneck towards representing the reconstructed and synthetic scenes at unlimited resolution. Although the recently developed neural radiance fields (NeRF) have shown compelling results in implicit representations, the problem of large-scale 3D scene reconstruction and novel view synthesis using sparse LiDAR frames remains unexplored. To bridge this gap, we propose a 3D scene reconstruction and novel view synthesis framework called parent-child neural radiance field (PC-NeRF). Based on its two modules, parent NeRF and child NeRF, the framework implements hierarchical spatial partitioning and multi-level scene representation, including scene, segment, and point levels. The multi-level scene representation enhances the efficient utilization of sparse LiDAR point cloud data and enables the rapid acquisition of an approximate volumetric scene representation. With extensive experiments, PC-NeRF is proven to achieve high-precision novel LiDAR view synthesis and 3D reconstruction in large-scale scenes. Moreover, PC-NeRF can effectively handle situations with sparse LiDAR frames and demonstrate high deployment efficiency with limited training epochs. Our approach implementation and the pre-trained models are available at <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a>. </p><p><a href="http://arxiv.org/abs/2402.09325v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.00874</p><p><strong>Summary</strong><br>åŸºäºåˆ†å±‚ç©ºé—´åˆ†å‰²å’Œå¤šå±‚æ¬¡åœºæ™¯è¡¨ç¤ºï¼ŒPC-NeRF æ¡†æ¶å®ç°äº†å¤§è§„æ¨¡åœºæ™¯çš„ 3D é‡å»ºå’Œæ–°è§†å›¾åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>PC-NeRF æ¡†æ¶ç”±çˆ¶ NeRF å’Œå­ NeRF ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼Œå®ç°äº†åˆ†å±‚ç©ºé—´åˆ†å‰²å’Œå¤šå±‚æ¬¡åœºæ™¯è¡¨ç¤ºã€‚</li><li>åˆ†å±‚ç©ºé—´åˆ†å‰²å’Œå¤šå±‚æ¬¡åœºæ™¯è¡¨ç¤ºå¯ä»¥æé«˜ç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®çš„åˆ©ç”¨æ•ˆç‡ï¼Œå¹¶å®ç°å¿«é€Ÿè·å–è¿‘ä¼¼ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚</li><li>PC-NeRF å¯ä»¥æœ‰æ•ˆå¤„ç†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„æƒ…å†µï¼Œå¹¶åœ¨æœ‰é™çš„è®­ç»ƒè½®æ•°ä¸‹è¡¨ç°å‡ºå¾ˆé«˜çš„éƒ¨ç½²æ•ˆç‡ã€‚</li><li>PC-NeRF çš„å®ç°å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨ <a href="https://github.com/biter0088/pc-nerf">https://github.com/biter0088/pc-nerf</a> ä¸Šè·å–ã€‚</li><li>PC-NeRF å¯ä»¥å®ç°é«˜ç²¾åº¦çš„æ¿€å…‰é›·è¾¾æ–°è§†å›¾åˆæˆå’Œ 3D é‡å»ºã€‚</li><li>PC-NeRF å¯ä»¥æœ‰æ•ˆå¤„ç†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„æƒ…å†µã€‚</li><li>PC-NeRF åœ¨æœ‰é™çš„è®­ç»ƒè½®æ•°ä¸‹è¡¨ç°å‡ºå¾ˆé«˜çš„éƒ¨ç½²æ•ˆç‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šPC-NeRFï¼šè‡ªåŠ¨é©¾é©¶ç¯å¢ƒä¸­ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„çˆ¶å­ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šXiuzhong Hu, Guangming Xiong, Zheng Zang, Peng Jia, Yuxuan Han, Junyi Ma</li><li>éš¶å±å•ä½ï¼šåŒ—äº¬ç†å·¥å¤§å­¦æœºæ¢°å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€ä¸‰ç»´åœºæ™¯é‡å»ºã€è‡ªåŠ¨é©¾é©¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.09325ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/biter0088/pc-nerf</li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆå¯¹äºè‡ªåŠ¨é©¾é©¶æ±½è½¦è¿›è¡Œç¯å¢ƒæ¢ç´¢ã€è¿åŠ¨è§„åˆ’å’Œé—­ç¯ä»¿çœŸè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¯ç”¨ä¼ æ„Ÿå™¨æ•°æ®ç”±äºå„ç§å®é™…å› ç´ è€Œå˜å¾—ç¨€ç–çš„æƒ…å†µä¸‹ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„æ˜¾å¼è¡¨ç¤ºå¯ä»¥æç»˜é‡å»ºçš„åœºæ™¯å’Œåˆæˆè§†å›¾ï¼Œä½†å®ƒä»¬åœ¨ä»¥æ— é™åˆ†è¾¨ç‡è¡¨ç¤ºåœºæ™¯æ–¹é¢ä»ç„¶å­˜åœ¨é‡å¤§ç“¶é¢ˆã€‚æœ€è¿‘å¼€å‘çš„ç¥ç»è¾å°„åœº (NeRF) åœ¨éšå¼è¡¨ç¤ºæ–¹é¢å–å¾—äº†å¼•äººæ³¨ç›®çš„ç»“æœï¼Œä½†ä½¿ç”¨ç¨€ç–æ¿€å…‰é›·è¾¾å¸§è¿›è¡Œå¤§è§„æ¨¡ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆçš„éš¾é¢˜ä»æœªå¾—åˆ°æ¢ç´¢ã€‚(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºçˆ¶å­ç¥ç»è¾å°„åœº (PC-NeRF) çš„ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºå…¶ä¸¤ä¸ªæ¨¡å—ï¼Œçˆ¶ NeRF å’Œå­ NeRFï¼Œå®ç°äº†åˆ†å±‚ç©ºé—´åˆ’åˆ†å’Œå¤šçº§åœºæ™¯è¡¨ç¤ºï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰‡æ®µå’Œç‚¹çº§ã€‚å¤šçº§åœºæ™¯è¡¨ç¤ºå¢å¼ºäº†å¯¹ç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¹¶èƒ½å¤Ÿå¿«é€Ÿè·å–è¿‘ä¼¼ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚(4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šé€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒPC-NeRF è¢«è¯æ˜å¯ä»¥åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­å®ç°é«˜ç²¾åº¦çš„æ¿€å…‰é›·è¾¾æ–°è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºã€‚æ­¤å¤–ï¼ŒPC-NeRF å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„æƒ…å†µï¼Œå¹¶è¯æ˜äº†åœ¨æœ‰é™çš„è®­ç»ƒè½®æ¬¡ä¸‹å…·æœ‰è¾ƒé«˜çš„éƒ¨ç½²æ•ˆç‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰PC-NeRFæ¡†æ¶ï¼šæå‡ºäº†ä¸€ç§ç§°ä¸ºçˆ¶å­ç¥ç»è¾å°„åœºï¼ˆPC-NeRFï¼‰çš„ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºå…¶ä¸¤ä¸ªæ¨¡å—ï¼Œçˆ¶NeRFå’Œå­NeRFï¼Œå®ç°äº†åˆ†å±‚ç©ºé—´åˆ’åˆ†å’Œå¤šçº§åœºæ™¯è¡¨ç¤ºï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰‡æ®µå’Œç‚¹çº§ã€‚ï¼ˆ2ï¼‰å¤šçº§åœºæ™¯è¡¨ç¤ºï¼šå¤šçº§åœºæ™¯è¡¨ç¤ºå¢å¼ºäº†å¯¹ç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¹¶èƒ½å¤Ÿå¿«é€Ÿè·å–è¿‘ä¼¼ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰è®­ç»ƒè¿‡ç¨‹ï¼šPC-NeRFé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè®­ç»ƒçˆ¶NeRFï¼Œç„¶åè®­ç»ƒå­NeRFï¼Œæœ€åå°†çˆ¶NeRFå’Œå­NeRFç»“åˆèµ·æ¥è¿›è¡Œè”åˆè®­ç»ƒã€‚ï¼ˆ4ï¼‰æŸå¤±å‡½æ•°ï¼šPC-NeRFçš„æŸå¤±å‡½æ•°åŒ…æ‹¬çˆ¶NeRFçš„æŸå¤±å‡½æ•°å’Œå­NeRFçš„æŸå¤±å‡½æ•°ï¼Œçˆ¶NeRFçš„æŸå¤±å‡½æ•°åŒ…æ‹¬é‡æŠ•å½±è¯¯å·®å’Œå…‰åº¦è¯¯å·®ï¼Œå­NeRFçš„æŸå¤±å‡½æ•°åŒ…æ‹¬è‡ªç”±ç©ºé—´è¯¯å·®å’Œæ·±åº¦è¯¯å·®ã€‚ï¼ˆ5ï¼‰æ–°é¢–è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºï¼šPC-NeRFå¯ä»¥é€šè¿‡æ–°é¢–è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºæ¥è¯„ä¼°å…¶æ€§èƒ½ï¼Œæ–°é¢–è§†è§’åˆæˆæ˜¯å°†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§åˆæˆåˆ°æ–°çš„è§†è§’ï¼Œä¸‰ç»´é‡å»ºæ˜¯å°†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§é‡å»ºä¸ºä¸‰ç»´ç‚¹äº‘ã€‚</p></li><li><p>ç»“è®ºï¼š(1)ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€‚ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„å¤§è§„æ¨¡ä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ¡†æ¶ PC-NeRFï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚ç©ºé—´åˆ’åˆ†å’Œå¤šçº§åœºæ™¯è¡¨ç¤ºï¼Œæœ‰æ•ˆåˆ©ç”¨ç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œå®ç°é«˜ç²¾åº¦çš„æ–°é¢–è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼šPC-NeRF æå‡ºäº†ä¸€ç§åˆ†å±‚ç©ºé—´åˆ’åˆ†å’Œå¤šçº§åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ï¼Œæœ‰æ•ˆåˆ©ç”¨ç¨€ç–æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ã€‚PC-NeRF æå‡ºäº†ä¸€ç§ä¸¤æ­¥æ·±åº¦æ¨ç†æ–¹æ³•ï¼Œå®ç°ä»ç‰‡æ®µåˆ°ç‚¹çš„æ¨ç†ã€‚PC-NeRF åœ¨ KITTI å’Œ MaiCity æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†å…¶åœ¨ç¨€ç–æ¿€å…‰é›·è¾¾å¸§æ¡ä»¶ä¸‹è¿›è¡Œæ–°é¢–è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºçš„æœ‰æ•ˆæ€§ã€‚æ€§èƒ½ï¼šPC-NeRF åœ¨ KITTI å’Œ MaiCity æ•°æ®é›†ä¸Šå®ç°äº†é«˜ç²¾åº¦çš„æ¿€å…‰é›·è¾¾æ–°è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºã€‚PC-NeRF å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ç¨€ç–æ¿€å…‰é›·è¾¾å¸§çš„æƒ…å†µï¼Œå¹¶è¯æ˜äº†åœ¨æœ‰é™çš„è®­ç»ƒè½®æ¬¡ä¸‹å…·æœ‰è¾ƒé«˜çš„éƒ¨ç½²æ•ˆç‡ã€‚å·¥ä½œé‡ï¼šPC-NeRF çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºéƒ¨ç½²ã€‚PC-NeRF çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-6782f984ff8bf4da1d81a6ca240eded4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a2171d3c5e58e5589aa20525792832a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7d40aa20abd78a5813673cde1893940.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-40b695293253e411ba8966555ca76058.jpg" align="middle"></details><h2 id="NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs"><a href="#NeRF-Analogies-Example-Based-Visual-Attribute-Transfer-for-NeRFs" class="headerlink" title="NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs"></a>NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs</h2><p><strong>Authors:Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel</strong></p><p>A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D geometry in a semantically meaningful way, such that the resulting new NeRF retains the target geometry but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D geometry and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines. </p><p><a href="http://arxiv.org/abs/2402.08622v1">PDF</a> Project page: <a href="https://mfischer-ucl.github.io/nerf_analogies/">https://mfischer-ucl.github.io/nerf_analogies/</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) å¯å°†åœºæ™¯çš„ 3D å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚è¿›è¡Œç¼–ç ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF å¯ä»¥å°†æº NeRF ä¸­çš„å¤–è§‚è½¬ç§»åˆ°ç›®æ ‡ 3D å‡ ä½•å½¢çŠ¶ä¸Šï¼Œä»è€Œåˆ›å»ºå…·æœ‰ç›®æ ‡å‡ ä½•å½¢çŠ¶ä½†å¤–è§‚ç±»ä¼¼äºæº NeRF çš„æ–° NeRFã€‚</li><li>è¯¥æ–¹æ³•å°†ç»å…¸å›¾åƒç±»æ¯”ä» 2D å›¾åƒæ¨å¹¿åˆ° NeRFã€‚</li><li>åŸºäºè¯­ä¹‰äº²å’Œæ€§çš„å¯¹åº”è½¬ç§»ï¼Œç”±å¤§å‹é¢„è®­ç»ƒ 2D å›¾åƒæ¨¡å‹æä¾›çš„è¯­ä¹‰ç‰¹å¾é©±åŠ¨ï¼Œå¯å®ç°å¤šè§†å›¾ä¸€è‡´å¤–è§‚è½¬ç§»ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ¢ç´¢ 3D å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚çš„æ··åˆåŒ¹é…äº§å“ç©ºé—´ã€‚</li><li>è¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ ·å¼åŒ–çš„æ–¹æ³•ã€‚</li><li>å¤§å¤šæ•°ç”¨æˆ·æ›´å–œæ¬¢è¯¥æ–¹æ³•ï¼Œè€Œä¸æ˜¯å…¶ä»–å‡ ç§å…¸å‹åŸºçº¿æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šNeRF ç±»æ¯”ï¼šåŸºäºç¤ºä¾‹çš„ NeRF è§†è§‰å±æ€§è¿ç§»</li><li>ä½œè€…ï¼šMichael Fischerã€Zhengqin Liã€Thu Nguyen-Phuocã€AljaÅ¾ BoÅ¾iÄã€Zhao Dongã€Carl Marshallã€Tobias Ritschel</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¼¦æ•¦å¤§å­¦å­¦é™¢</li><li>å…³é”®è¯ï¼šNeRFã€è§†è§‰å±æ€§è¿ç§»ã€è¯­ä¹‰ç‰¹å¾ã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š</li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRFï¼ˆç¥ç»è¾å°„åœºï¼‰æ˜¯ä¸€ç§ç”¨äºè¡¨ç¤ºå’Œæ¸²æŸ“ 3D åœºæ™¯çš„å¼ºå¤§æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒNeRF é€šå¸¸éœ€è¦å¤§é‡æ•°æ®æ‰èƒ½è®­ç»ƒï¼Œå¹¶ä¸”éš¾ä»¥å°†ä»ä¸€ä¸ªåœºæ™¯å­¦åˆ°çš„å¤–è§‚è¿ç§»åˆ°å¦ä¸€ä¸ªåœºæ™¯ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨åŸºäºæ ·å¼è¿ç§»çš„æŠ€æœ¯æ¥å°†ä¸€ç§åœºæ™¯çš„å¤–è§‚è¿ç§»åˆ°å¦ä¸€ç§åœºæ™¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éš¾ä»¥äº§ç”Ÿè¯­ä¹‰ä¸Šè¿è´¯çš„ç»“æœï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥å°†ä¸€ç§åœºæ™¯çš„å¤–è§‚è¿ç§»åˆ°å¦ä¸€ç§åœºæ™¯ï¼Œè€Œæ— éœ€å¤§é‡çš„æ•°æ®ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„ 2D å›¾åƒæ¨¡å‹ä¸­çš„è¯­ä¹‰ç‰¹å¾æ¥å»ºç«‹æºåœºæ™¯å’Œç›®æ ‡åœºæ™¯ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚ç„¶åï¼Œè¿™äº›å¯¹åº”å…³ç³»è¢«ç”¨æ¥å°†æºåœºæ™¯çš„å¤–è§‚è¿ç§»åˆ°ç›®æ ‡åœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿäº§ç”Ÿè¯­ä¹‰ä¸Šè¿è´¯çš„ç»“æœï¼Œå¹¶ä¸”ä¼˜äºè¿‡å»çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥ç”¨äºç”Ÿæˆæ–°çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å…·æœ‰æºåœºæ™¯çš„å¤–è§‚å’Œç›®æ ‡åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚</p><p><methods>:(1)ï¼šæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´å›¾åƒæ¨¡å‹ä¸­çš„è¯­ä¹‰ç‰¹å¾æ¥å»ºç«‹æºåœºæ™¯å’Œç›®æ ‡åœºæ™¯ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚(2)ï¼šç„¶åï¼Œè¿™äº›å¯¹åº”å…³ç³»è¢«ç”¨æ¥å°†æºåœºæ™¯çš„å¤–è§‚è¿ç§»åˆ°ç›®æ ‡åœºæ™¯ã€‚(3)ï¼šæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªä¸‰ç»´ä¸€è‡´çš„NeRFè¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºåœ¨å…ˆå‰æå–çš„ç‚¹äº‘FSourceå’ŒFTargetä¸Šã€‚(4)ï¼šæˆ‘ä»¬é‡‡æ ·FSourceä¸­çš„ä½ç½®ï¼Œå¹¶åœ¨æ¯ä¸ªä½ç½®æå–æºç‰¹å¾æè¿°ç¬¦fSourceã€æºå¤–è§‚LSourceå’Œæºè§†å‘Ï‰Sourceã€‚(5)ï¼šæˆ‘ä»¬è¿˜ä»ç›®æ ‡ç‚¹äº‘FTargetä¸­é‡‡æ ·ä½ç½®ï¼Œå¹¶åœ¨æ¯ä¸ªä½ç½®è·å–å›¾åƒç‰¹å¾fTargetå’Œç›®æ ‡ä½ç½®xTargetã€‚(6)ï¼šæˆ‘ä»¬æ‰¾åˆ°ä¸€ä¸ªç¦»æ•£æ˜ å°„Ï•ï¼Œè¯¥æ˜ å°„å°†æ¯ä¸ªç›®æ ‡ä½ç½®ç´¢å¼•jæ˜ å°„åˆ°å…·æœ‰æœ€å¤§ç›¸ä¼¼æ€§çš„æºä½ç½®ç´¢å¼•iã€‚(7)ï¼šæˆ‘ä»¬å®šä¹‰LTargetj=LSourceÏ•jä½œä¸ºç›®æ ‡åœ¨æ˜ å°„Ï•å’ŒæŸä¸ªè§†å‘ä¸‹çš„å¤–è§‚ã€‚(8)ï¼šæˆ‘ä»¬è®­ç»ƒNeRF Analogy LÎ¸çš„å‚æ•°Î¸ï¼Œä½¿å¾—å¯¹äºæ¯ä¸ªè§‚å¯Ÿåˆ°çš„ç›®æ ‡ä½ç½®ï¼Œç›®æ ‡å’Œæºå¤–è§‚åœ¨æºè§†å‘ä¸‹ä¸€è‡´ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº† NeRF ç±»æ¯”ï¼Œä¸€ç§åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„ NeRF è§†è§‰å±æ€§è¿ç§»æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¯ä»¥è¾…åŠ©å†…å®¹åˆ›ä½œï¼Œä¾‹å¦‚ï¼Œé€šè¿‡å°†ç”¨æˆ·æ•è·çš„å‡ ä½•ä½“ä¸åœ¨çº¿ 3D æ¨¡å‹çš„å¤–è§‚ç›¸ç»“åˆï¼Œå¹¶ä¸”è¿˜é€‚ç”¨äºå¤šå¯¹è±¡è®¾ç½®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢œè‰²è¿ç§»ã€å›¾åƒåˆæˆå’Œé£æ ¼åŒ–æ–‡çŒ®ä¸­çš„å…¶ä»–æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è·å¾—äº†æœ€é«˜çš„æ’åï¼Œæ— è®ºæ˜¯åœ¨è¿ç§»è´¨é‡è¿˜æ˜¯å¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§çš„ NeRF è§†è§‰å±æ€§è¿ç§»æ¡†æ¶ã€‚</li><li>è¯¥æ¡†æ¶å¯ä»¥ç”¨äºè¾…åŠ©å†…å®¹åˆ›ä½œã€å¤šå¯¹è±¡è®¾ç½®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ã€‚</li><li>è¯¥æ¡†æ¶åœ¨é¢œè‰²è¿ç§»ã€å›¾åƒåˆæˆå’Œé£æ ¼åŒ–æ–‡çŒ®ä¸­çš„å…¶ä»–æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ¡†æ¶åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è·å¾—äº†æœ€é«˜çš„æ’åï¼Œæ— è®ºæ˜¯åœ¨è¿ç§»è´¨é‡è¿˜æ˜¯å¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚</li><li>è¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆè¯­ä¹‰ä¸Šè¿è´¯çš„ç»“æœï¼Œå¹¶ä¸”ä¼˜äºè¿‡å»çš„æ–¹æ³•ã€‚</li><li>è¯¥æ¡†æ¶è¿˜å¯ä»¥ç”¨äºç”Ÿæˆæ–°çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å…·æœ‰æºåœºæ™¯çš„å¤–è§‚å’Œç›®æ ‡åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ¡†æ¶éœ€è¦é¢„è®­ç»ƒä¸€ä¸ª 2D å›¾åƒæ¨¡å‹æ¥æå–è¯­ä¹‰ç‰¹å¾ã€‚</li><li>è¯¥æ¡†æ¶éœ€è¦è®­ç»ƒä¸€ä¸ª 3D ä¸€è‡´çš„ NeRF è¡¨ç¤ºã€‚</li><li>è¯¥æ¡†æ¶éœ€è¦æ‰¾åˆ°ä¸€ä¸ªç¦»æ•£æ˜ å°„æ¥å°†æºåœºæ™¯å’Œç›®æ ‡åœºæ™¯ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li><li>è¯¥æ¡†æ¶éœ€è¦è®­ç»ƒä¸€ä¸ª NeRF ç±»æ¯”æ¨¡å‹æ¥å°†æºåœºæ™¯çš„å¤–è§‚è¿ç§»åˆ°ç›®æ ‡åœºæ™¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-56d4edbaccc121abec3c1fbc5aa2a7b2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b96734ea48c9163e25bc72d32ad13598.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d80da8fbb7f50a1faceaf09341a6dada.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c35035cd1513fc1b8683c14a413721b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-190136188bdfd4cb8f04bafbfb9ef577.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/3DGS/</id>
    <published>2024-02-22T17:38:45.000Z</published>
    <updated>2024-02-22T17:38:45.284Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-23-æ›´æ–°"><a href="#2024-02-23-æ›´æ–°" class="headerlink" title="2024-02-23 æ›´æ–°"></a>2024-02-23 æ›´æ–°</h1><h2 id="Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting"><a href="#Identifying-Unnecessary-3D-Gaussians-using-Clustering-for-Fast-Rendering-of-3D-Gaussian-Splatting" class="headerlink" title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting"></a>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</h2><p><strong>Authors:Joongho Jo, Hyeongwon Kim, Jongsun Park</strong></p><p>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU. </p><p><a href="http://arxiv.org/abs/2402.13827v1">PDF</a> </p><p><strong>Summary</strong><br>3D é«˜æ–¯æ•£splatting é€šè¿‡èšç±» å’Œ æŠ•å½±ä¼˜åŒ–ï¼Œå‡å°‘äº† 38.3% çš„æ¸²æŸ“è®¡ç®—ï¼Œä¸”ä¸æŸå¤±å›¾åƒè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D é«˜æ–¯æ•£splattingï¼ˆ3D-GSï¼‰æ˜¯ä¸€ç§æ–°çš„æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨é€Ÿåº¦å’Œå›¾åƒè´¨é‡ä¸Šä¼˜äºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚</li><li>3D-GS ä½¿ç”¨æ•°ç™¾ä¸‡ä¸ª 3D é«˜æ–¯è¡¨ç¤º 3D åœºæ™¯ï¼Œå¹¶å°†è¿™äº›é«˜æ–¯æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šè¿›è¡Œæ¸²æŸ“ã€‚</li><li>åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œå¤§é‡ä¸å¿…è¦çš„é«˜æ–¯å­˜åœ¨äºå½“å‰è§†å›¾æ–¹å‘ï¼Œå¯¼è‡´ä¸è¯†åˆ«å®ƒä»¬ç›¸å…³çš„è®¡ç®—æˆæœ¬å·¨å¤§ã€‚</li><li>æå‡ºäº†ä¸€ç§è®¡ç®—ç®€åŒ–æŠ€æœ¯ï¼Œå¯åœ¨è¿è¡Œæ—¶å¿«é€Ÿè¯†åˆ«å‡ºä¸å¿…è¦çš„é«˜æ–¯ï¼Œç”¨äºæ¸²æŸ“å½“å‰è§†å›¾ï¼Œä¸”ä¸æŸå®³å›¾åƒè´¨é‡ã€‚</li><li>è¿™ç§ç®€åŒ–æŠ€æœ¯æ–¹æ³•æ˜¯ç¦»çº¿å¯¹è·ç¦»ç›¸è¿‘çš„é«˜æ–¯è¿›è¡Œèšç±»ï¼Œç„¶ååœ¨è¿è¡Œæ—¶å°†è¿™äº›é›†ç¾¤æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šã€‚</li><li>å¯¹è¯¥æŠ€æœ¯åœ¨ GPU ä¸Šæ‰§è¡Œæ—¶çš„ç“¶é¢ˆè¿›è¡Œäº†åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸è¯¥æ–¹æ¡ˆæ— ç¼å…¼å®¹çš„é«˜æ•ˆç¡¬ä»¶æ¶æ„ã€‚</li><li>å¯¹äº Mip-NeRF360 æ•°æ®é›†ï¼Œè¯¥æŠ€æœ¯åœ¨ 2D å›¾åƒæŠ•å½±ä¹‹å‰å¹³å‡æ’é™¤äº† 63% çš„é«˜æ–¯ï¼Œå°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº† 38.3%ï¼Œä¸”ä¸æŸå¤±å³°å€¼ä¿¡å™ªæ¯” (PSNR)ã€‚</li><li>è¯¥åŠ é€Ÿå™¨ä¸ GPU ç›¸æ¯”ï¼Œè¿˜å®ç°äº† 10.7 å€çš„åŠ é€Ÿã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šä½¿ç”¨èšç±»è¯†åˆ«ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ï¼Œä»¥å¿«é€Ÿæ¸²æŸ“ 3D é«˜æ–¯ä½“é£æº…</li><li>ä½œè€…ï¼šJoongho Joã€Hyeongwon Kim å’Œ Jongsun Park</li><li>éš¶å±æœºæ„ï¼šéŸ©å›½å¤§å­¦ç”µæ°”å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“é£æº…ã€æ¸²æŸ“ã€NeRFã€ç¥ç»è¾å°„åœºã€ç¡¬ä»¶åŠ é€Ÿå™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šPaper_info:Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splattingï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š</li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåœ¨è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­ï¼Œä¾‹å¦‚å¢å¼ºç°å® (AR)ã€è™šæ‹Ÿç°å® (VR) å’Œå…ƒå®‡å®™ï¼Œå¿«é€Ÿä¸”é«˜è´¨é‡çš„å›¾åƒæ¸²æŸ“éå¸¸é‡è¦ã€‚è™½ç„¶å·²ç»å¹¿æ³›ç ”ç©¶äº†ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¸²æŸ“æŠ€æœ¯ï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœº (NeRF)ï¼Œä½† 3D é«˜æ–¯ä½“é£æº… (3D-GS) ä½œä¸ºä¸€ç§æ–°çš„æ¸²æŸ“æ–¹æ³•ï¼Œæœ€è¿‘å› å…¶ä¸ä¼ ç»Ÿ NeRF ç›¸æ¯”èƒ½å¤Ÿå¿«é€Ÿæ¸²æŸ“é«˜è´¨é‡å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚3D-GS åˆ©ç”¨æ•°ç™¾ä¸‡ä¸ª 3D é«˜æ–¯ä½“æ¥è¡¨ç¤ºå¤æ‚çš„ 3D åœºæ™¯ï¼Œå¹¶é€šè¿‡å°† 3D é«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šæ¥æ¸²æŸ“ 3D åœºæ™¯ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š3D-GS æ¸²æŸ“è¿‡ç¨‹ä¸»è¦åˆ†ä¸ºä¸¤æ­¥ï¼šé¦–å…ˆï¼Œå°†æ‰€æœ‰ 3D é«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œå¹¶è¯†åˆ«å½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“ã€‚ç„¶åï¼Œä½¿ç”¨å½±å“é¢œè‰²çš„å·²è¯†åˆ« 3D é«˜æ–¯ä½“è®¡ç®— 2D å›¾åƒä¸­æ¯ä¸ªåƒç´ çš„é¢œè‰²ã€‚åœ¨æ¸²æŸ“è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥ä¸­ï¼Œé«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒä¸Šåï¼Œå¦‚æœè¢«è¯†åˆ«ä¸ºä¸å½±å“ 2D å›¾åƒçš„é¢œè‰²ï¼Œé‚£ä¹ˆæŠ•å½±å°±å˜æˆäº†è®¡ç®—æµªè´¹ã€‚åœ¨ Mip-NeRF360 æ•°æ®é›†ä¸­ï¼Œå¹³å‡çº¦æœ‰ 67.6% çš„ 3D é«˜æ–¯ä½“ä¸å½±å“ 2D å›¾åƒçš„é¢œè‰²ã€‚å› æ­¤ï¼Œè¿™äº›é«˜æ–¯ä½“å¯ä»¥ä»å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­æ’é™¤ã€‚ä½†æ˜¯ï¼Œç”±äºå½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“å¯èƒ½ä¼šéšç€æ¸²æŸ“è§†ç‚¹çš„æ–¹å‘å’Œä½ç½®è€Œæ”¹å˜ï¼Œå› æ­¤åœ¨å°† 3D é«˜æ–¯ä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šä¹‹å‰è¯†åˆ«ä¸å¿…è¦çš„é«˜æ–¯ä½“ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œè¿™äº›ä¸å¿…è¦çš„é«˜æ–¯ä½“ä»ç„¶ä¼šè¿›è¡ŒæŠ•å½±è®¡ç®—ã€‚å› æ­¤ï¼Œå¦‚æœå¯ä»¥å¼€å‘å‡ºä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥é¢„æµ‹ä¸ä¼šå½±å“ 2D å›¾åƒé¢œè‰²çš„ 3D é«˜æ–¯ä½“ï¼Œå¹¶åœ¨æ¸²æŸ“è¿‡ç¨‹å¼€å§‹ä¹‹å‰å°†å®ƒä»¬æ’é™¤ï¼Œé‚£ä¹ˆå¯ä»¥æ˜¾ç€é™ä½æ•´ä¸ª 3D-GS è¿‡ç¨‹çš„æ€»ä½“è®¡ç®—å¤æ‚åº¦ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ¡ˆï¼Œé€šè¿‡è¯†åˆ«ä¸å½±å“ 2D å›¾åƒé¢œè‰²çš„ç°‡æ¥æ’é™¤å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ä¸å¿…è¦ 3D é«˜æ–¯ä½“ã€‚èšç±»çš„ç›®æ ‡æ˜¯å°†ä½ç½®ç›¸è¿‘çš„ 3D é«˜æ–¯ä½“åˆ†ç»„åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”ç°‡çš„å½¢çŠ¶åº”è¯¥æ˜¯çƒå½¢çš„ï¼Œä»¥ä¾¿äºæŠ•å½±åˆ° 2D å›¾åƒä¸Šã€‚å› æ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨ K-means èšç±»ç®—æ³•ï¼Œè¯¥ç®—æ³•æ»¡è¶³è¿™ä¸¤ä¸ªæ ‡å‡†ã€‚è€ƒè™‘åˆ° 3D é«˜æ–¯ä½“å…·æœ‰ç”±å…¶åæ–¹å·®å®šä¹‰çš„å¤§å°æˆ–å½±å“ï¼Œç°‡çƒä½“çš„åŠå¾„ä¸ä»…ç”±åˆ°ç°‡è´¨å¿ƒçš„è·ç¦»ç¡®å®šï¼Œè¿˜è¦è€ƒè™‘é«˜æ–¯ä½“çš„å¤§å°ã€‚ç„¶åå°†è¿™äº›å®šä¹‰çš„ç°‡çƒä½“æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œä»¥ç¡®å®šå®ƒä»¬å¯¹ 2D å›¾åƒé¢œè‰²çš„å½±å“ã€‚ä¸å½±å“å›¾åƒé¢œè‰²çš„ç°‡å¯ä»¥ä»æ¸²æŸ“è¿‡ç¨‹ä¸­æ’é™¤ã€‚åœ¨æœ¬æ–‡çš„æ–¹æ³•ä¸­ï¼Œå¯ä»¥åœ¨ç¦»çº¿æ‰§è¡Œèšç±»å’Œè®¡ç®—ç°‡çš„åŠå¾„ï¼Œå¹¶ä¸”ä»…åœ¨å®æ—¶æ‰§è¡Œå°†ç°‡æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œè¿™ä»…éœ€ 6.2% çš„è®¡ç®—å¼€é”€ã€‚åœ¨ 3D-GS æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œåœ¨å½“å‰è§†å›¾æ¸²æŸ“ä¹‹å‰åº”ç”¨æ‰€æå‡ºçš„æ–¹æ³•æ—¶ï¼Œå¹³å‡å¯ä»¥æ’é™¤ 63% çš„ 3D é«˜æ–¯ä½“ï¼Œä»è€Œåœ¨ä¸ç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR) çš„æƒ…å†µä¸‹å°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ã€‚æ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†ä¸ GPU ç›¸æ¯” 10.7 å€çš„åŠ é€Ÿã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ Mip-NeRF360 æ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¹³å‡æ’é™¤äº† 63% çš„ 3D é«˜æ–¯ä½“ï¼Œåœ¨ä¸ç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR) çš„æƒ…å†µä¸‹å°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ã€‚æ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†ä¸ GPU ç›¸æ¯” 10.7 å€çš„åŠ é€Ÿã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¿«é€Ÿä¸”é«˜è´¨é‡åœ°æ¸²æŸ“ 3D åœºæ™¯ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ¡ˆï¼Œé€šè¿‡è¯†åˆ«ä¸å½±å“ 2D å›¾åƒé¢œè‰²çš„ç°‡æ¥æ’é™¤å½“å‰è§†å›¾æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ä¸å¿…è¦ 3D é«˜æ–¯ä½“ã€‚è¯¥æ–¹æ³•å¹³å‡å¯ä»¥æ’é™¤ 63% çš„ 3D é«˜æ–¯ä½“ï¼Œåœ¨ä¸ç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR) çš„æƒ…å†µä¸‹å°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ã€‚æ‰€æå‡ºçš„åŠ é€Ÿå™¨è¿˜å®ç°äº†ä¸ GPU ç›¸æ¯” 10.7 å€çš„åŠ é€Ÿã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ¡ˆæ¥è¯†åˆ«ä¸å¿…è¦çš„ 3D é«˜æ–¯ä½“ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥ç¦»çº¿æ‰§è¡Œèšç±»å’Œè®¡ç®—ç°‡çš„åŠå¾„ï¼Œå¹¶ä¸”ä»…åœ¨å®æ—¶æ‰§è¡Œå°†ç°‡æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œè¿™ä»…éœ€ 6.2% çš„è®¡ç®—å¼€é”€ã€‚</li><li>æ‰€æå‡ºçš„åŠ é€Ÿå™¨å®ç°äº†ä¸ GPU ç›¸æ¯” 10.7 å€çš„åŠ é€Ÿã€‚æ€§èƒ½ï¼š</li><li>åœ¨ Mip-NeRF360 æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¹³å‡æ’é™¤äº† 63% çš„ 3D é«˜æ–¯ä½“ï¼Œåœ¨ä¸ç‰ºç‰²å³°å€¼ä¿¡å™ªæ¯” (PSNR) çš„æƒ…å†µä¸‹å°†æ•´ä½“æ¸²æŸ“è®¡ç®—å‡å°‘äº†è¿‘ 38.3%ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•å¯ä»¥åœ¨ç¦»çº¿æ‰§è¡Œèšç±»å’Œè®¡ç®—ç°‡çš„åŠå¾„ï¼Œå¹¶ä¸”ä»…åœ¨å®æ—¶æ‰§è¡Œå°†ç°‡æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢ä¸Šï¼Œè¿™ä»…éœ€ 6.2% çš„è®¡ç®—å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-eb8532b7f44bd3308c4f19fe6bf7f78c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e5e9d849dcc9fd5228abd36df009311.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a43367bbb6924d5ba043f598753b956.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d13d1af17267a2b843bea8ac607b39a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bca1cf3d857e2d53600b33fc6c9e298c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5799fc43b51197a24672703783ee479.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee494dce0084e0f0c71d55d940b03dc9.jpg" align="middle"></details><h2 id="GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting"><a href="#GaussianObject-Just-Taking-Four-Images-to-Get-A-High-Quality-3D-Object-with-Gaussian-Splatting" class="headerlink" title="GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting"></a>GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object   with Gaussian Splatting</h2><p><strong>Authors:Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian</strong></p><p>Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from only 4 views and significantly outperforming previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2402.10259v2">PDF</a> Project page: <a href="https://gaussianobject.github.io/">https://gaussianobject.github.io/</a></p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨ä»…æœ‰ 4 å¼ è¾“å…¥å›¾åƒï¼Œä»¥é«˜æ–¯æ•£ç‚¹å›¾è¡¨ç¤ºå’Œæ¸²æŸ“ä¸‰ç»´å¯¹è±¡ï¼Œå±•ç°å‡ºæä½³çš„æ¸²æŸ“è´¨é‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>é‡å»ºå’Œæ¸²æŸ“é«˜åº¦ç¨€ç–è§†å›¾çš„ 3D å¯¹è±¡å¯¹äºä¿ƒè¿› 3D è§†è§‰æŠ€æœ¯åº”ç”¨å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚</li><li>æå‡º GaussianObjectï¼Œä¸€ç§ä»¥é«˜æ–¯æ•£ç‚¹å›¾è¡¨ç¤ºå’Œæ¸²æŸ“ 3D å¯¹è±¡çš„æ¡†æ¶ï¼Œä»…éœ€ 4 å¼ è¾“å…¥å›¾åƒå³å¯å®ç°é«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>å¼•å…¥è§†è§‰å¤–å£³å’Œæµ®å­æ¶ˆé™¤æŠ€æœ¯ï¼Œå°†ç»“æ„å…ˆéªŒæ˜ç¡®æ³¨å…¥åˆå§‹ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¸®åŠ©å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œäº§ç”Ÿç²—ç³™çš„ 3D é«˜æ–¯è¡¨ç¤ºã€‚</li><li>åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºé«˜æ–¯ä¿®å¤æ¨¡å‹ï¼Œä»¥è¡¥å……çœç•¥çš„å¯¹è±¡ä¿¡æ¯ï¼Œå…¶ä¸­é«˜æ–¯å€¼è¿›ä¸€æ­¥ç»†åŒ–ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è‡ªç”Ÿæˆç­–ç•¥æ¥è·å–å›¾åƒå¯¹ï¼Œä»¥è®­ç»ƒä¿®å¤æ¨¡å‹ã€‚</li><li>åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº† GaussianObjectï¼ŒåŒ…æ‹¬ MipNeRF360ã€OmniObject3D å’Œ OpenIlluminationï¼Œä»…ä½¿ç”¨ 4 ä¸ªè§†å›¾å³å¯å®ç°å¼ºå¤§çš„é‡å»ºç»“æœï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºå…ˆå‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯å¯¹è±¡ï¼šåªéœ€å››å¼ å›¾åƒå³å¯è·å–é«˜è´¨é‡çš„ 3D å¯¹è±¡</li><li>ä½œè€…ï¼šé™ˆé˜³ï¼Œææ€å®½ï¼Œæ–¹æ°æ°‘ï¼Œæ¢è‹¥å‡¡ï¼Œè°¢å‡Œå¸Œï¼Œå¼ æ™“é¹ï¼Œæ²ˆå·ï¼Œç”°é½</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D é‡å»ºã€ç¨€ç–è§†å›¾ã€é«˜æ–¯çƒé¢ä½“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.10259ï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé‡å»ºå’Œæ¸²æŸ“ 3D å¯¹è±¡æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦è¯¾é¢˜ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡è§†å›¾æ‰èƒ½è·å¾—é«˜è´¨é‡çš„ç»“æœã€‚è¿™å¯¹äºç”¨æˆ·æ¥è¯´éå¸¸ç¹çï¼Œé™åˆ¶äº† 3D æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šä¸€äº›ç ”ç©¶å°è¯•å‡å°‘å¯¹å¯†é›†æ•è·çš„ä¾èµ–ï¼Œä½†å½“è§†å›¾å˜å¾—æåº¦ç¨€ç–æ—¶ï¼Œä»ç„¶éš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D å¯¹è±¡ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºéš¾ä»¥å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»¥åŠéƒ¨åˆ†ç¼ºå¤±æˆ–é«˜åº¦å‹ç¼©çš„å¯¹è±¡ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé«˜æ–¯å¯¹è±¡çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä»ç¨€ç–è§†å›¾ä¸­é‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ 3D é«˜æ–¯çƒé¢ä½“ä½œä¸ºåŸºæœ¬è¡¨ç¤ºï¼Œå¹¶è®¾è®¡äº†å‡ ç§æŠ€æœ¯æ¥å¼•å…¥å¯¹è±¡ç»“æ„å…ˆéªŒï¼Œå¸®åŠ©å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯ä¿®å¤æ¨¡å‹ï¼Œä»¥æ¶ˆé™¤ç”±ç¼ºå¤±æˆ–é«˜åº¦å‹ç¼©çš„å¯¹è±¡ä¿¡æ¯å¼•èµ·çš„ä¼ªå½±ã€‚ï¼ˆ4ï¼‰æ€§èƒ½è¡¨ç°ï¼šé«˜æ–¯å¯¹è±¡æ–¹æ³•åœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™è¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»ç¨€ç–è§†å›¾ä¸­é‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) é«˜æ–¯çƒé¢ä½“è¡¨ç¤ºï¼šå°†3Då¯¹è±¡è¡¨ç¤ºä¸ºä¸€ä¸ª3Dé«˜æ–¯çƒé¢ä½“ï¼Œè¯¥çƒé¢ä½“ç”±ä¸€ç³»åˆ—3Dé«˜æ–¯åˆ†å¸ƒç»„æˆã€‚æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒå¯¹åº”äºå¯¹è±¡çš„ä¸€ä¸ªå±€éƒ¨åŒºåŸŸï¼Œå…¶å‚æ•°ï¼ˆä¸­å¿ƒä½ç½®ã€å°ºåº¦å’Œæƒé‡ï¼‰ç”±ç¥ç»ç½‘ç»œå­¦ä¹ å¾—åˆ°ã€‚(2) ç»“æ„å…ˆéªŒå¼•å…¥ï¼šè®¾è®¡äº†å‡ ç§æŠ€æœ¯æ¥å¼•å…¥å¯¹è±¡ç»“æ„å…ˆéªŒï¼Œå¸®åŠ©å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬ï¼š</p><ul><li>å½¢çŠ¶æ­£åˆ™åŒ–ï¼šä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å½¢çŠ¶ç”Ÿæˆæ¨¡å‹æ¥æ­£åˆ™åŒ–é«˜æ–¯çƒé¢ä½“çš„å½¢çŠ¶ï¼Œä½¿å…¶æ›´åŠ çœŸå®å’Œè‡ªç„¶ã€‚</li><li>æ‹“æ‰‘æ­£åˆ™åŒ–ï¼šä½¿ç”¨ä¸€ä¸ªæ‹“æ‰‘ç”Ÿæˆæ¨¡å‹æ¥æ­£åˆ™åŒ–é«˜æ–¯çƒé¢ä½“çš„æ‹“æ‰‘ç»“æ„ï¼Œä½¿å…¶æ›´åŠ è¿é€šå’Œå®Œæ•´ã€‚</li><li>è¯­ä¹‰æ­£åˆ™åŒ–ï¼šä½¿ç”¨ä¸€ä¸ªè¯­ä¹‰åˆ†å‰²æ¨¡å‹æ¥æ­£åˆ™åŒ–é«˜æ–¯çƒé¢ä½“çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å…¶æ›´åŠ å‡†ç¡®å’Œä¸€è‡´ã€‚(3) é«˜æ–¯ä¿®å¤æ¨¡å‹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯ä¿®å¤æ¨¡å‹ï¼Œä»¥æ¶ˆé™¤ç”±ç¼ºå¤±æˆ–é«˜åº¦å‹ç¼©çš„å¯¹è±¡ä¿¡æ¯å¼•èµ·çš„ä¼ªå½±ã€‚è¯¥æ¨¡å‹é€šè¿‡è¿­ä»£åœ°æ‰©æ•£å’Œæ¢å¤é«˜æ–¯çƒé¢ä½“çš„å‚æ•°ï¼Œé€æ­¥æ¶ˆé™¤ä¼ªå½±å¹¶ç”Ÿæˆé«˜è´¨é‡çš„3Då¯¹è±¡ã€‚</li></ul></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šé«˜æ–¯å¯¹è±¡æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»æåº¦ç¨€ç–çš„ 360Â° è§†å›¾ä¸­é‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ï¼Œè¯¥æ¡†æ¶åŸºäº 3D é«˜æ–¯çƒé¢ä½“ï¼Œå¹¶å…·æœ‰å®æ—¶çš„æ¸²æŸ“èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§ä¸»è¦æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼šè¾…åŠ©ç»“æ„å…ˆéªŒçš„ä¼˜åŒ–ï¼Œä»¥ä¿ƒè¿›å¤šè§†å›¾ä¸€è‡´æ€§çš„æ„å»ºï¼Œä»¥åŠé«˜æ–¯ä¿®å¤æ¨¡å‹ï¼Œä»¥å»é™¤ç”±é—æ¼æˆ–é«˜åº¦å‹ç¼©çš„å¯¹è±¡ä¿¡æ¯å¼•èµ·çš„ä¼ªå½±ã€‚æˆ‘ä»¬å¸Œæœ›é«˜æ–¯å¯¹è±¡èƒ½å¤Ÿæ¨è¿›é‡å»º 3D å¯¹è±¡çš„æ—¥å¸¸åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„ 3D å¯¹è±¡è¡¨ç¤ºå½¢å¼â€”â€”é«˜æ–¯çƒé¢ä½“ï¼Œè¯¥è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·å¯¹è±¡çš„å½¢çŠ¶ã€æ‹“æ‰‘ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li><li>è®¾è®¡äº†å‡ ç§æŠ€æœ¯æ¥å¼•å…¥å¯¹è±¡ç»“æ„å…ˆéªŒï¼Œå¸®åŠ©å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬å½¢çŠ¶æ­£åˆ™åŒ–ã€æ‹“æ‰‘æ­£åˆ™åŒ–å’Œè¯­ä¹‰æ­£åˆ™åŒ–ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é«˜æ–¯ä¿®å¤æ¨¡å‹ï¼Œä»¥æ¶ˆé™¤ç”±ç¼ºå¤±æˆ–é«˜åº¦å‹ç¼©çš„å¯¹è±¡ä¿¡æ¯å¼•èµ·çš„ä¼ªå½±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œé«˜æ–¯å¯¹è±¡æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>é«˜æ–¯å¯¹è±¡æ–¹æ³•èƒ½å¤Ÿä»æåº¦ç¨€ç–çš„ 360Â° è§†å›¾ä¸­é‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ï¼Œè¿™å¯¹äºç”¨æˆ·æ¥è¯´éå¸¸æ–¹ä¾¿ï¼Œå¹¶ä¸”å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸã€‚å·¥ä½œé‡ï¼š</li><li>é«˜æ–¯å¯¹è±¡æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œæˆæœ¬ã€‚</li><li>é«˜æ–¯å¯¹è±¡æ–¹æ³•éœ€è¦ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ é«˜æ–¯çƒé¢ä½“çš„å‚æ•°ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ec0859f0d4156531b928896ce0f20711.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6cf586e290dad38d6317bf5e32650f6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc6b9cc2318a136451091ab1f1c68efb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0ee843ee1e2c5a9e509cc05d4936f7f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6acbb2bc7ce290268eb48c8af2cb6b.jpg" align="middle"></details><h2 id="GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering"><a href="#GES-Generalized-Exponential-Splatting-for-Efficient-Radiance-Field-Rendering" class="headerlink" title="GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering"></a>GES: Generalized Exponential Splatting for Efficient Radiance Field   Rendering</h2><p><strong>Authors:Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi</strong></p><p>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes.   It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis benchmarks while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> . </p><p><a href="http://arxiv.org/abs/2402.10128v1">PDF</a> preprint</p><p><strong>æ‘˜è¦</strong><br>å¹¿ä¹‰æŒ‡æ•°æ•£åˆ—æ³•ï¼ˆGESï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„ 3D åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•° (GEF) å¯¹ 3D åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†è¡¨ç¤ºåœºæ™¯æ‰€éœ€çš„ç²’å­æ•°é‡ï¼Œæ¯”é«˜æ–¯æ•£åˆ—æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”å³æ’å³ç”¨ï¼Œå¯ä»¥æ›¿ä»£åŸºäºé«˜æ–¯çš„å·¥å…·ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>GES ä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•° (GEF) å¯¹ 3D åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œæ˜¾è‘—å‡å°‘äº†æ‰€éœ€ç²’å­æ•°é‡ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li><li>GES ä¼˜äºé«˜æ–¯æ•£åˆ—æ³•ï¼Œèƒ½å¤Ÿå°† 3D åœºæ™¯å»ºæ¨¡ä¸ºæ›´å°‘çš„ç²’å­ï¼Œåœ¨æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºé«˜æ–¯æ•£åˆ—æ³•ã€‚</li><li>GES åœ¨åŸç†æ€§çš„ä¸€ç»´è®¾ç½®å’Œç°å®çš„ 3D åœºæ™¯ä¸­ç»è¿‡ç†è®ºå’Œç»éªŒéªŒè¯ã€‚</li><li>GES åœ¨è¡¨è¾¾å…·æœ‰æ¸…æ™°è¾¹ç¼˜çš„ä¿¡å·æ–¹é¢æ›´å‡†ç¡®ï¼Œè€Œè¿™äº›ä¿¡å·é€šå¸¸å¯¹é«˜æ–¯å‡½æ•°æ„æˆæŒ‘æˆ˜ï¼Œå› å…¶æœ¬èº«å…·æœ‰ä½é€šç‰¹æ€§ã€‚</li><li>GES åœ¨æ‹Ÿåˆè‡ªç„¶å‘ç”Ÿçš„ä¿¡å·ï¼ˆä¾‹å¦‚æ­£æ–¹å½¢ã€ä¸‰è§’å½¢å’ŒæŠ›ç‰©çº¿ä¿¡å·ï¼‰æ–¹é¢ä¼˜äºé«˜æ–¯å‡½æ•°ï¼Œå› è€Œå‡å°‘äº†å¢åŠ é«˜æ–¯æ•£åˆ—æ³•çš„å†…å­˜å ç”¨çš„å¤§é‡åˆ†è£‚æ“ä½œçš„éœ€è¦ã€‚</li><li>ä½¿ç”¨è°ƒåˆ¶é¢‘ç‡æŸå¤±ï¼ŒGES å¯å®ç°åœ¨æ–°è§†å›¾åˆæˆåŸºå‡†ä¸­å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„å­˜å‚¨ç©ºé—´ä¸åˆ°é«˜æ–¯æ•£åˆ—æ³•çš„äºŒåˆ†ä¹‹ä¸€ï¼Œå¹¶ä½¿æ¸²æŸ“é€Ÿåº¦æé«˜å¤šè¾¾ 39%ã€‚</li><li>GES çš„ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™ <a href="https://abdullahamdi.com/ges">https://abdullahamdi.com/ges</a> ä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGESï¼šç”¨äºé«˜æ•ˆå…‰åœºæ¸²æŸ“çš„å¹¿ä¹‰æŒ‡æ•°æ•£å°„ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰</li><li>ä½œè€…ï¼šAbdullah Hamdiã€Luke Melas-Kyriaziã€Guocheng Qianã€Jinjie Maiã€Ruoshi Liuã€Carl Vondrickã€Bernard Ghanemã€Andrea Vedaldi</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç‰›æ´¥å¤§å­¦è§†è§‰å‡ ä½•ç»„ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€3D ç”Ÿæˆã€3D è¡¨ç¤ºã€å…‰åœºæ¸²æŸ“ã€å¹¿ä¹‰æŒ‡æ•°å‡½æ•°</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.10128ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£å°„åœ¨ 3D é‡å»ºå’Œç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒå¯èƒ½éœ€è¦å¤§é‡é«˜æ–¯å‡½æ•°ï¼Œè¿™ä¼šé€ æˆå·¨å¤§çš„å†…å­˜å ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šé«˜æ–¯æ•£å°„æ–¹æ³•å‡è®¾åœºæ™¯ä¿¡å·æ˜¯ä½é€šçš„ï¼Œä½†å¤§å¤šæ•° 3D åœºæ™¯éƒ½åŒ…å«å½¢çŠ¶å’Œå¤–è§‚ä¸Šçš„çªå˜ï¼Œå› æ­¤é«˜æ–¯æ•£å°„éœ€è¦ä½¿ç”¨å¤§é‡éå¸¸å°çš„é«˜æ–¯å‡½æ•°æ¥è¡¨ç¤ºè¿™äº› 3D åœºæ™¯ï¼Œè¿™ä¼šå¯¹å†…å­˜åˆ©ç”¨ç‡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º GESï¼ˆå¹¿ä¹‰æŒ‡æ•°æ•£å°„ï¼‰ï¼Œå®ƒä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•°ï¼ˆå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ å½¢çŠ¶å‚æ•°ï¼‰æ¥å»ºæ¨¡ 3D åœºæ™¯ï¼Œä»è€Œå¯ä»¥å‡å°‘è¡¨ç¤ºåœºæ™¯æ‰€éœ€çš„ç²’å­æ•°é‡ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šæ˜æ˜¾ä¼˜äºé«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å³æ’å³ç”¨åœ°æ›¿æ¢åŸºäºé«˜æ–¯çš„å®ç”¨å·¥å…·ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šGES åœ¨åŸç†æ€§ 1D è®¾ç½®å’Œé€¼çœŸçš„ 3D åœºæ™¯ä¸­éƒ½å¾—åˆ°äº†ç†è®ºå’Œç»éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºå…·æœ‰é”åˆ©è¾¹ç¼˜çš„ä¿¡å·ï¼Œè€Œè¿™å¯¹äºé«˜æ–¯å‡½æ•°æ¥è¯´é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å›ºæœ‰çš„ä½é€šç‰¹æ€§ã€‚å®è¯åˆ†æè¡¨æ˜ï¼ŒGES åœ¨æ‹Ÿåˆè‡ªç„¶å‡ºç°çš„ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ­£æ–¹å½¢ã€ä¸‰è§’å½¢ã€æŠ›ç‰©çº¿ä¿¡å·ï¼‰æ–¹é¢ä¼˜äºé«˜æ–¯å‡½æ•°ï¼Œä»è€Œå‡å°‘äº†å¢åŠ é«˜æ–¯æ•£å°„å†…å­˜å ç”¨ç‡çš„å¹¿æ³›åˆ†è£‚æ“ä½œçš„éœ€è¦ã€‚åœ¨é¢‘ç‡è°ƒåˆ¶æŸå¤±çš„å¸®åŠ©ä¸‹ï¼ŒGES åœ¨æ–°è§†å›¾åˆæˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„å†…å­˜å­˜å‚¨é‡ä¸åˆ°é«˜æ–¯æ•£å°„çš„ä¸€åŠï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æé«˜äº† 39%ã€‚</li></ol><p><methods>:(1): GESä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•°ï¼ˆå…·æœ‰é¢å¤–çš„å¯å­¦ä¹ å½¢çŠ¶å‚æ•°ï¼‰æ¥å»ºæ¨¡3Dåœºæ™¯ï¼Œä»è€Œå¯ä»¥å‡å°‘è¡¨ç¤ºåœºæ™¯æ‰€éœ€çš„ç²’å­æ•°é‡ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šæ˜æ˜¾ä¼˜äºé«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å³æ’å³ç”¨åœ°æ›¿æ¢åŸºäºé«˜æ–¯çš„å®ç”¨å·¥å…·ã€‚(2): GESåœ¨åŸç†æ€§1Dè®¾ç½®å’Œé€¼çœŸçš„3Dåœºæ™¯ä¸­éƒ½å¾—åˆ°äº†ç†è®ºå’Œç»éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºå…·æœ‰é”åˆ©è¾¹ç¼˜çš„ä¿¡å·ï¼Œè€Œè¿™å¯¹äºé«˜æ–¯å‡½æ•°æ¥è¯´é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å›ºæœ‰çš„ä½é€šç‰¹æ€§ã€‚(3): å®è¯åˆ†æè¡¨æ˜ï¼ŒGESåœ¨æ‹Ÿåˆè‡ªç„¶å‡ºç°çš„ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ­£æ–¹å½¢ã€ä¸‰è§’å½¢ã€æŠ›ç‰©çº¿ä¿¡å·ï¼‰æ–¹é¢ä¼˜äºé«˜æ–¯å‡½æ•°ï¼Œä»è€Œå‡å°‘äº†å¢åŠ é«˜æ–¯æ•£å°„å†…å­˜å ç”¨ç‡çš„å¹¿æ³›åˆ†è£‚æ“ä½œçš„éœ€è¦ã€‚(4): åœ¨é¢‘ç‡è°ƒåˆ¶æŸå¤±çš„å¸®åŠ©ä¸‹ï¼ŒGESåœ¨æ–°è§†å›¾åˆæˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„å†…å­˜å­˜å‚¨é‡ä¸åˆ°é«˜æ–¯æ•£å°„çš„ä¸€åŠï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æé«˜äº†39%ã€‚</methods></p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…‰åœºæ¸²æŸ“æ–¹æ³• GESï¼Œå®ƒä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•°æ¥å»ºæ¨¡ 3D åœºæ™¯ï¼Œä»è€Œå¯ä»¥å‡å°‘è¡¨ç¤ºåœºæ™¯æ‰€éœ€çš„ç²’å­æ•°é‡ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šæ˜æ˜¾ä¼˜äºé«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥å³æ’å³ç”¨åœ°æ›¿æ¢åŸºäºé«˜æ–¯çš„å®ç”¨å·¥å…·ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>GES ä½¿ç”¨å¹¿ä¹‰æŒ‡æ•°å‡½æ•°æ¥å»ºæ¨¡ 3D åœºæ™¯ï¼Œä»è€Œå¯ä»¥å‡å°‘è¡¨ç¤ºåœºæ™¯æ‰€éœ€çš„ç²’å­æ•°é‡ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šæ˜æ˜¾ä¼˜äºé«˜æ–¯æ•£å°„æ–¹æ³•ã€‚</li><li>GES åœ¨åŸç†æ€§ 1D è®¾ç½®å’Œé€¼çœŸçš„ 3D åœºæ™¯ä¸­éƒ½å¾—åˆ°äº†ç†è®ºå’Œç»éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºå…·æœ‰é”åˆ©è¾¹ç¼˜çš„ä¿¡å·ï¼Œè€Œè¿™å¯¹äºé«˜æ–¯å‡½æ•°æ¥è¯´é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å›ºæœ‰çš„ä½é€šç‰¹æ€§ã€‚</li><li>å®è¯åˆ†æè¡¨æ˜ï¼ŒGES åœ¨æ‹Ÿåˆè‡ªç„¶å‡ºç°çš„ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ­£æ–¹å½¢ã€ä¸‰è§’å½¢ã€æŠ›ç‰©çº¿ä¿¡å·ï¼‰æ–¹é¢ä¼˜äºé«˜æ–¯å‡½æ•°ï¼Œä»è€Œå‡å°‘äº†å¢åŠ é«˜æ–¯æ•£å°„å†…å­˜å ç”¨ç‡çš„å¹¿æ³›åˆ†è£‚æ“ä½œçš„éœ€è¦ã€‚</li><li>åœ¨é¢‘ç‡è°ƒåˆ¶æŸå¤±çš„å¸®åŠ©ä¸‹ï¼ŒGES åœ¨æ–°è§†å›¾åˆæˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„å†…å­˜å­˜å‚¨é‡ä¸åˆ°é«˜æ–¯æ•£å°„çš„ä¸€åŠï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æé«˜äº† 39%ã€‚æ€§èƒ½ï¼š</li><li>GES åœ¨åŸç†æ€§ 1D è®¾ç½®å’Œé€¼çœŸçš„ 3D åœºæ™¯ä¸­éƒ½å¾—åˆ°äº†ç†è®ºå’Œç»éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œå®ƒå¯ä»¥æ›´å‡†ç¡®åœ°è¡¨ç¤ºå…·æœ‰é”åˆ©è¾¹ç¼˜çš„ä¿¡å·ï¼Œè€Œè¿™å¯¹äºé«˜æ–¯å‡½æ•°æ¥è¯´é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å›ºæœ‰çš„ä½é€šç‰¹æ€§ã€‚</li><li>å®è¯åˆ†æè¡¨æ˜ï¼ŒGES åœ¨æ‹Ÿåˆè‡ªç„¶å‡ºç°çš„ä¿¡å·ï¼ˆä¾‹å¦‚ï¼Œæ­£æ–¹å½¢ã€ä¸‰è§’å½¢ã€æŠ›ç‰©çº¿ä¿¡å·ï¼‰æ–¹é¢ä¼˜äºé«˜æ–¯å‡½æ•°ï¼Œä»è€Œå‡å°‘äº†å¢åŠ é«˜æ–¯æ•£å°„å†…å­˜å ç”¨ç‡çš„å¹¿æ³›åˆ†è£‚æ“ä½œçš„éœ€è¦ã€‚</li><li>åœ¨é¢‘ç‡è°ƒåˆ¶æŸå¤±çš„å¸®åŠ©ä¸‹ï¼ŒGES åœ¨æ–°è§†å›¾åˆæˆåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€çš„å†…å­˜å­˜å‚¨é‡ä¸åˆ°é«˜æ–¯æ•£å°„çš„ä¸€åŠï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æé«˜äº† 39%ã€‚å·¥ä½œé‡ï¼š</li><li>GES çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆåˆ°ç°æœ‰çš„å…‰åœºæ¸²æŸ“å·¥å…·ä¸­ã€‚</li><li>GES çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹è¾ƒå¿«ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆã€‚</li><li>GES çš„æ¸²æŸ“é€Ÿåº¦å¾ˆå¿«ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-06e50cf8fcf2b71cc6d5f5fa60bd416c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e0387aa41ca3382d21ca4822a1185a81.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d98ce6f15593a9709f1a7d0a0c108a7f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4903d39957be51dd29a4222bcccefaa4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c50bfcbaec1420bcb70374001db6c443.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e090b0178d5a97f88600cc386571b770.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the objectâ€™s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v2">PDF</a> The content of the technical report needs to be updated and retracted   to avoid other impacts</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨3DGS(ä¸‰ç»´é«˜æ–¯æ¸²æŸ“)å¢å¼º3Dè¡¨ç°ï¼Œä»¥è¿›è¡Œå¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œæå‡å›¾åƒã€è¯­è¨€å’Œä¸‰ç»´æ•°æ®çš„å¯¹é½ï¼Œæ”¹å–„ç‰©ä½“è¯†åˆ«ã€åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>åˆ©ç”¨ç‚¹äº‘è¡¨ç¤ºçš„3Då½¢çŠ¶åœ¨å›¾åƒå’Œè¯­è¨€æè¿°çš„å¯¹é½ä¸Šå–å¾—äº†å¤šæ¨¡æ€é¢„è®­ç»ƒçš„è¿›æ­¥ï¼Œè¿™å¯¹äºç‰©ä½“è¯†åˆ«ã€åˆ†ç±»å’Œæ£€ç´¢è‡³å…³é‡è¦ã€‚</li><li>ç‚¹äº‘çš„ç¦»æ•£è¡¨ç¤ºä¸¢å¤±äº†ç‰©ä½“çš„æ›²é¢å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶åœ¨æ¸²æŸ“ç»“æœå’Œ2Då¯¹åº”å…³ç³»ä¹‹é—´åˆ¶é€ å·®è·ã€‚</li><li>æå‡ºGS-CLIPï¼Œé¦–æ¬¡å°è¯•å°†3DGSï¼ˆä¸‰ç»´é«˜æ–¯æ¸²æŸ“ï¼‰å¼•å…¥å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œä»¥å¢å¼º3Dè¡¨ç¤ºã€‚</li><li>GS-CLIPåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤§é‡çœŸå®ä¸–ç•Œå›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå­¦ä¹ ä¸€ä¸ªé€šç”¨çš„è§†è§‰å’Œæ–‡æœ¬ç©ºé—´ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ªé’ˆå¯¹æ¯ä¸ªç‰©ä½“ä¼˜åŒ–3DGSçš„3Dç¼–ç å™¨ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ„ŸçŸ¥èåˆæ¥æå–å’Œèåˆå…¨å±€æ˜¾å¼ç‰¹å¾ã€‚</li><li>ä½œä¸ºè¯­è¨€-å›¾åƒ-3Dé¢„è®­ç»ƒçš„é€šç”¨æ¡†æ¶ï¼ŒGS-CLIPç‹¬ç«‹äº3Déª¨å¹²ç½‘ç»œã€‚</li><li>å…·æœ‰æŒ‘æˆ˜æ€§çš„å®éªŒè¡¨æ˜ï¼ŒGS-CLIPæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¶…è¶Šäº†ä»¥å‰æœ€å¥½çš„æˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGS-CLIPï¼šç”¨äºå¯¹æ¯”è¯­è¨€-å›¾åƒ-3D é¢„è®­ç»ƒçš„é«˜æ–¯å–·ç»˜</li><li>ä½œè€…ï¼šææ˜Šæºã€å‘¨é›é¹ã€æ›¾ä¹‰æ¶µã€å¾èˆªã€æ¢æ™“ä¸¹</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ·±åœ³å¤§å­¦</li><li>å…³é”®è¯ï¼š3D è¡¨ç¤ºã€å¯¹æ¯”å­¦ä¹ ã€å¤šæ¨¡æ€é¢„è®­ç»ƒã€é«˜æ–¯å–·ç»˜</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.06198ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D å½¢çŠ¶è¡¨ç¤ºä¸ºç‚¹äº‘åœ¨å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­å–å¾—äº†è¿›å±•ï¼Œå¯ä»¥å¯¹é½å›¾åƒå’Œè¯­è¨€æè¿°ï¼Œè¿™å¯¹ç‰©ä½“è¯†åˆ«ã€åˆ†ç±»å’Œæ£€ç´¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç‚¹äº‘çš„ç¦»æ•£è¡¨ç¤ºä¸¢å¤±äº†ç‰©ä½“çš„è¡¨é¢å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶åœ¨æ¸²æŸ“ç»“æœå’Œ 2D å¯¹åº”å…³ç³»ä¹‹é—´äº§ç”Ÿäº†å·®è·ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¯¹ç‚¹äº‘è¿›è¡Œå»ºæ¨¡ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¼šä¸¢å¤±ç‰©ä½“çš„å‡ ä½•ä¿¡æ¯å’Œå½¢çŠ¶çº¹ç†ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥åº”ç”¨äºç°å®ä¸–ç•Œä¸­çš„åœºæ™¯ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ GS-CLIPï¼Œè¯¥æ¡†æ¶å°† 3D é«˜æ–¯å–·ç»˜ (3DGS) å¼•å…¥å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­ï¼Œä»¥å¢å¼º 3D è¡¨ç¤ºã€‚GS-CLIP åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤§é‡çœŸå®ä¸–ç•Œå›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå­¦ä¹ ä¸€ä¸ªå…±åŒçš„è§†è§‰å’Œæ–‡æœ¬ç©ºé—´ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ª 3D ç¼–ç å™¨æ¥å¯¹é½é’ˆå¯¹æ¯ä¸ªå¯¹è±¡ä¼˜åŒ–çš„ 3DGSã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ„ŸçŸ¥èåˆæ–¹æ³•ï¼Œç”¨äºæå–å’Œèåˆå…¨å±€æ˜¾å¼ç‰¹å¾ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ SUN-RGBD æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-CLIP åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„é›¶æ ·æœ¬/å¼€æ”¾ä¸–ç•Œå­¦ä¹ ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œ3DGS åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­å…·æœ‰å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰è·¨æ¨¡æ€é¢„è®­ç»ƒï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€-å›¾åƒæ¨¡å‹CLIPï¼Œä¸ºæ–‡æœ¬ã€å›¾åƒå’Œ3DGSå»ºç«‹å…±åŒçš„è¯­è¨€-å›¾åƒæ½œåœ¨ç©ºé—´ï¼Œä½œä¸º3DGSçš„ç›®æ ‡æ½œåœ¨ç©ºé—´ã€‚ï¼ˆ2ï¼‰è¯­è¨€-3DGSå¯¹é½å’Œå›¾åƒ-3DGSå¯¹é½ï¼šåˆ†åˆ«ä½¿ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°æ¥å¯¹é½æ–‡æœ¬ä¸3DGSã€å›¾åƒä¸3DGSçš„ç‰¹å¾è¡¨ç¤ºã€‚ï¼ˆ3ï¼‰é«˜æ–¯æ„ŸçŸ¥èåˆï¼šé‡‡ç”¨åŸºäºTransformerçš„åˆ†æ”¯ç›´æ¥å¯¹é«˜æ–¯ç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†å…¶ä¸æ®‹å·®å½¢å¼æ³¨å…¥åˆ°3Dä¸»å¹²ç½‘ç»œä¸­ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡å°† 3DGS çº³å…¥è·¨æ¨¡æ€å­¦ä¹ ï¼Œä½œä¸ºè¡¥å……å½¢çŠ¶å’Œçº¹ç†ä¿¡æ¯çš„é€šç”¨ 3D è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é«˜æ–¯æ„ŸçŸ¥èåˆï¼Œä»¥ä¾¿æ›´å¥½åœ°ä»è¡¥å……ä¿¡æ¯ä¸­å­¦ä¹ ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ GS-CLIP åœ¨æœ€å…ˆè¿›çš„æ–¹æ³•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å®ç°äº†é›¶æ ·æœ¬/å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„æœ€æ–°æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°† 3DGS å¼•å…¥è·¨æ¨¡æ€å­¦ä¹ ï¼Œä½œä¸ºè¡¥å……å½¢çŠ¶å’Œçº¹ç†ä¿¡æ¯çš„é€šç”¨ 3D è¡¨ç¤ºã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜æ–¯æ„ŸçŸ¥èåˆï¼Œä»¥ä¾¿æ›´å¥½åœ°ä»è¡¥å……ä¿¡æ¯ä¸­å­¦ä¹ ä¿¡æ¯ã€‚</li><li>åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­å®ç°äº†é›¶æ ·æœ¬/å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„æœ€æ–°æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ SUN-RGBD æ•°æ®é›†ä¸Šï¼ŒGS-CLIP åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„é›¶æ ·æœ¬/å¼€æ”¾ä¸–ç•Œå­¦ä¹ ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>è¿™äº›ç»“æœè¡¨æ˜ï¼Œ3DGS åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­å…·æœ‰å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥å·¥ä½œæ¶‰åŠåˆ°å¤§é‡çš„æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒã€‚</li><li>éœ€è¦å¯¹ 3DGS è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¯¹é½æ–‡æœ¬å’Œå›¾åƒçš„ç‰¹å¾è¡¨ç¤ºã€‚</li><li>éœ€è¦å¯¹é«˜æ–¯æ„ŸçŸ¥èåˆè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æå–å’Œèåˆå…¨å±€æ˜¾å¼ç‰¹å¾ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details><h2 id="GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting"><a href="#GaMeS-Mesh-Based-Adapting-and-Modification-of-Gaussian-Splatting" class="headerlink" title="GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting"></a>GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting</h2><p><strong>Authors:Joanna WaczyÅ„ska, Piotr Borycki, SÅ‚awomir Tadeja, Jacek Tabor, PrzemysÅ‚aw Spurek</strong></p><p>Recently, a range of neural network-based methods for image rendering have been introduced. One such widely-researched neural radiance field (NeRF) relies on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-the-art technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce the Gaussian Mesh Splatting (GaMeS) model, which allows modification of Gaussian components in a similar way as meshes. We parameterize each Gaussian component by the vertices of the mesh face. Furthermore, our model needs mesh initialization on input or estimated mesh during training. We also define Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain a real-time rendering of editable GS. </p><p><a href="http://arxiv.org/abs/2402.01459v3">PDF</a> </p><p><strong>Summary:</strong><br>ç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§ç”¨äºå›¾åƒæ¸²æŸ“çš„ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œè€Œé«˜æ–¯ç½‘æ ¼æ³¼æº… (GaMeS) æ¨¡å‹åˆ™é€šè¿‡é«˜æ–¯åˆ†å¸ƒæ¥ä¼°ç®— 3D åœºæ™¯ä¸­ç‚¹çš„è´¡çŒ®ï¼Œä»è€Œå®ç°å¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>åˆ©ç”¨ç¥ç»ç½‘ç»œè¡¨å¾ 3D åœºæ™¯çš„ NeRFï¼Œå…è®¸ä»å°‘é‡ 2D å›¾åƒä¸­è¿›è¡Œé€¼çœŸçš„è§†ç‚¹åˆæˆã€‚</li><li>é«˜æ–¯æ³¼æº… (GS) é€šè¿‡é«˜æ–¯åˆ†å¸ƒæ¥ä¼°ç®— 3D åœºæ™¯ä¸­ç‚¹çš„è´¡çŒ®ï¼Œä»è€Œå®ç°å¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“ã€‚</li><li>GaMeS æ¨¡å‹å…è®¸ä»¥ä¸ç½‘æ ¼ç±»ä¼¼çš„æ–¹å¼ä¿®æ”¹é«˜æ–¯åˆ†é‡ï¼Œä»è€Œä¸º GS çš„è°ƒèŠ‚æä¾›äº†ä¸€ä¸ªæ˜ç¡®çš„æ–¹æ³•ã€‚</li><li>å°†æ¯ä¸ªé«˜æ–¯åˆ†é‡å‚æ•°åŒ–ä¸ºç½‘æ ¼é¢çš„é¡¶ç‚¹ï¼Œè¿™ä½¿å¾— GaMeS æ¨¡å‹å¯ä»¥å¯¹ GS è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚</li><li>GaMeS æ¨¡å‹éœ€è¦åœ¨è¾“å…¥æ—¶åˆå§‹åŒ–ç½‘æ ¼æˆ–åœ¨è®­ç»ƒæœŸé—´ä¼°è®¡ç½‘æ ¼ã€‚</li><li>æ ¹æ®å…¶åœ¨ç½‘æ ¼ä¸Šçš„ä½ç½®å®šä¹‰é«˜æ–¯æ³¼æº…ï¼Œä»è€Œå…è®¸åœ¨åŠ¨ç”»æœŸé—´è‡ªåŠ¨è°ƒæ•´ä½ç½®ã€ç¼©æ”¾å’Œæ—‹è½¬ã€‚</li><li>GaMeS æ¨¡å‹å¯ä»¥å®ç°å¯ç¼–è¾‘ GS çš„å®æ—¶æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGaMeSï¼šåŸºäºç½‘æ ¼çš„è‡ªé€‚åº”å’Œä¿®æ”¹é«˜æ–¯å–·ç»˜</li><li>ä½œè€…ï¼šJoanna WaczyÅ„skaã€Piotr Boryckiã€SÅ‚awomir Tadejaã€Jacek Taborã€PrzemysÅ‚aw Spurek</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé›…ç›–éš†å¤§å­¦æ•°å­¦ä¸è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼Œæ³¢å…°å…‹æ‹‰ç§‘å¤«</li><li>å…³é”®è¯ï¼šé«˜æ–¯å–·ç»˜ã€ç¥ç»è¾å°„åœºã€ç¥ç»æ¸²æŸ“ã€ç½‘æ ¼ã€å®æ—¶æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.01459ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„å›¾åƒæ¸²æŸ“æ–¹æ³•å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œå…¶ä¸­ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§æµè¡Œçš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¡¨ç¤º 3D åœºæ™¯ï¼Œå¹¶èƒ½å¤Ÿä»å°‘é‡ 2D å›¾åƒä¸­åˆæˆé€¼çœŸçš„è§†å›¾ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•° NeRF æ¨¡å‹éƒ½å—åˆ°è®­ç»ƒå’Œæ¨ç†æ—¶é—´é•¿çš„é™åˆ¶ã€‚ä¸ä¹‹ç›¸æ¯”ï¼Œé«˜æ–¯å–·ç»˜ï¼ˆGSï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„ã€æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡é«˜æ–¯åˆ†å¸ƒæ¥è¿‘ä¼¼ç‚¹å¯¹å›¾åƒåƒç´ çš„è´¡çŒ®ï¼Œä»è€Œæ¸²æŸ“ 3D åœºæ™¯ä¸­çš„ç‚¹ï¼Œå…·æœ‰å¿«é€Ÿè®­ç»ƒå’Œå¿«é€Ÿå®æ—¶æ¸²æŸ“çš„èƒ½åŠ›ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šGS çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ç¼ºä¹æ˜ç¡®çš„è°ƒèŠ‚æ–¹æ³•ï¼Œå› ä¸ºéœ€è¦è°ƒèŠ‚æ•°åä¸‡ä¸ªé«˜æ–¯åˆ†é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†é«˜æ–¯ç½‘æ ¼å–·ç»˜ï¼ˆGaMeSï¼‰æ¨¡å‹ï¼Œå®ƒå…è®¸åƒä¿®æ”¹ç½‘æ ¼ä¸€æ ·ä¿®æ”¹é«˜æ–¯åˆ†é‡ã€‚æˆ‘ä»¬å°†æ¯ä¸ªé«˜æ–¯åˆ†é‡å‚æ•°åŒ–ä¸ºç½‘æ ¼é¢çš„é¡¶ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹éœ€è¦åœ¨è¾“å…¥æˆ–è®­ç»ƒæœŸé—´ä¼°è®¡çš„ç½‘æ ¼ä¸Šè¿›è¡Œç½‘æ ¼åˆå§‹åŒ–ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä»…åŸºäºå…¶åœ¨ç½‘æ ¼ä¸Šçš„ä½ç½®çš„é«˜æ–¯å–·ç»˜ï¼Œå…è®¸åœ¨åŠ¨ç”»æœŸé—´è‡ªåŠ¨è°ƒæ•´ä½ç½®ã€æ¯”ä¾‹å’Œæ—‹è½¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è·å¾—äº†å¯ç¼–è¾‘ GS çš„å®æ—¶æ¸²æŸ“ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº† GaMeS æ¨¡å‹ï¼Œå®ƒå…è®¸åƒä¿®æ”¹ç½‘æ ¼ä¸€æ ·ä¿®æ”¹é«˜æ–¯åˆ†é‡ã€‚æˆ‘ä»¬å°†æ¯ä¸ªé«˜æ–¯åˆ†é‡å‚æ•°åŒ–ä¸ºç½‘æ ¼é¢çš„é¡¶ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹éœ€è¦åœ¨è¾“å…¥æˆ–è®­ç»ƒæœŸé—´ä¼°è®¡çš„ç½‘æ ¼ä¸Šè¿›è¡Œç½‘æ ¼åˆå§‹åŒ–ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä»…åŸºäºå…¶åœ¨ç½‘æ ¼ä¸Šçš„ä½ç½®çš„é«˜æ–¯å–·ç»˜ï¼Œå…è®¸åœ¨åŠ¨ç”»æœŸé—´è‡ªåŠ¨è°ƒæ•´ä½ç½®ã€æ¯”ä¾‹å’Œæ—‹è½¬ã€‚(4)ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒGaMeS æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒé«˜è´¨é‡æ¸²æŸ“çš„åŒæ—¶ï¼Œå®ç°å®æ—¶ä¿®æ”¹å’Œé€‚åº”é«˜æ–¯å–·ç»˜ã€‚è¿™ä½¿å¾— GaMeS æˆä¸ºäº¤äº’å¼åº”ç”¨ç¨‹åºå’Œæ¸¸æˆä¸­çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æŠ€æœ¯ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰GaMeS å…è®¸å®æ—¶ä¿®æ”¹ï¼Œä½†å¯¹äºå…·æœ‰å¤§é¢çš„ç½‘æ ¼ï¼Œåœ¨å‘ç”Ÿé‡å¤§å˜åŒ–çš„æƒ…å†µä¸‹ä¼šå‡ºç°ä¼ªå½±ã€‚åœ¨å®è·µä¸­ï¼Œå¤§é¢åº”è¯¥è¢«åˆ†æˆæ›´å°çš„é¢ã€‚å½“ç½‘æ ¼é¢åˆ†è£‚æ—¶å¦‚ä½•åœ¨ GaMeS ä¸­æ›´æ”¹é«˜æ–¯åˆ†é‡å°šä¸æ¸…æ¥šã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šGaMeS æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç½‘æ ¼çš„è‡ªé€‚åº”å’Œä¿®æ”¹é«˜æ–¯å–·ç»˜æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸åƒä¿®æ”¹ç½‘æ ¼ä¸€æ ·ä¿®æ”¹é«˜æ–¯åˆ†é‡ï¼Œä»è€Œå®ç°äº†å®æ—¶ä¿®æ”¹å’Œé€‚åº”é«˜æ–¯å–·ç»˜ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒGaMeS æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒé«˜è´¨é‡æ¸²æŸ“çš„åŒæ—¶ï¼Œå®ç°å®æ—¶ä¿®æ”¹å’Œé€‚åº”é«˜æ–¯å–·ç»˜ã€‚è¿™ä½¿å¾— GaMeS æˆä¸ºäº¤äº’å¼åº”ç”¨ç¨‹åºå’Œæ¸¸æˆä¸­çš„ä¸€ä¸ªæœ‰å‰æ™¯çš„æŠ€æœ¯ã€‚å·¥ä½œé‡ï¼šGaMeS æ¨¡å‹éœ€è¦åœ¨è¾“å…¥æˆ–è®­ç»ƒæœŸé—´ä¼°è®¡ç½‘æ ¼ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼ŒGaMeS æ¨¡å‹éœ€è¦å¯¹ç½‘æ ¼è¿›è¡Œä¿®æ”¹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ¨¡å‹çš„ä¿®æ”¹æ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-11676aa94eeb837bc5149bf9038274ae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3c20ac78640d356ea03699146c96e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4070017cd795fd8699e30a356efae899.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0416310a796f7ec70150342ac59ffe37.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6eb0975a0f5d702a6daef3f78e530869.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9fb0edd088d9a64e792369a6d6a72979.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dd54f927f26f28fdcefe778d566087c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-23  Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering   of 3D Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/23/Paper/2024-02-23/Diffusion%20Models/</id>
    <published>2024-02-22T17:19:45.000Z</published>
    <updated>2024-02-22T17:19:45.965Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-23-æ›´æ–°"><a href="#2024-02-23-æ›´æ–°" class="headerlink" title="2024-02-23 æ›´æ–°"></a>2024-02-23 æ›´æ–°</h1><h2 id="Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation"><a href="#Hybrid-Video-Diffusion-Models-with-2D-Triplane-and-3D-Wavelet-Representation" class="headerlink" title="Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation"></a>Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation</h2><p><strong>Authors:Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo</strong></p><p>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control). </p><p><a href="http://arxiv.org/abs/2402.13729v1">PDF</a> 17 pages, 13 figures</p><p><strong>Summary</strong><br>æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·è§†é¢‘çš„æ—¶ç©ºä¾èµ–æ€§ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡å’Œé€¼çœŸçš„è§†é¢‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨ä¼ ç»Ÿçš„è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨æ¶æ„å°†è§†é¢‘å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†å¯æ¯”çš„æ€§èƒ½ã€‚</li><li>æ ‡å‡†å¸§çº§ 2D å’Œ 3D å·ç§¯æ— æ³•å……åˆ†åˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºæ€§è´¨ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ HVDMï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰æ—¶ç©ºç›¸å…³æ€§ã€‚</li><li>HVDM ç”±æ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒï¼Œè¯¥ç¼–ç å™¨æå–è§†é¢‘çš„è§£è€¦è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼šç”± 2D æŠ•å½±æ½œåœ¨å˜é‡æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç”±å…·æœ‰å°æ³¢åˆ†è§£çš„ 3D å·ç§¯æ•è·çš„å±€éƒ¨ä½“ç§¯ä¿¡æ¯ä»¥åŠç”¨äºæ”¹è¿›è§†é¢‘é‡å»ºçš„é¢‘ç‡ä¿¡æ¯ã€‚</li><li>åŸºäºè¿™ç§è§£è€¦è¡¨ç¤ºï¼Œæå‡ºçš„æ··åˆè‡ªåŠ¨ç¼–ç å™¨æä¾›äº†æ›´å…¨é¢çš„è§†é¢‘æ½œåœ¨å˜é‡ï¼Œä¸°å¯Œäº†ç”Ÿæˆè§†é¢‘çš„ç²¾ç»†ç»“æ„å’Œç»†èŠ‚ã€‚</li><li>åœ¨è§†é¢‘ç”ŸæˆåŸºå‡†ï¼ˆUCF101ã€SkyTimelapse å’Œ TaiChiï¼‰ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå±•ç¤ºäº†å¹¿æ³›çš„è§†é¢‘åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œé•¿è§†é¢‘ç”Ÿæˆã€å›¾åƒåˆ°è§†é¢‘å’Œè§†é¢‘åŠ¨æ€æ§åˆ¶ï¼‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå…·æœ‰ 2D ä¸‰å¹³é¢å’Œ 3D å°æ³¢è¡¨ç¤ºçš„æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šTianhan Wangã€Junyu Dongã€Xiaolong Wangã€Yibing Songã€Yezhou Yangã€Kun Zhouã€Jiayi Ma</li><li>éš¶å±å…³ç³»ï¼šåä¸­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è§†é¢‘è¡¨ç¤ºã€å°æ³¢å˜æ¢</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1)ï¼šè§†é¢‘ç”Ÿæˆæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç”Ÿæˆå…·æœ‰å¤æ‚ä¸”é«˜ç»´åº¦çš„é€¼çœŸè§†é¢‘ã€‚æœ€è¿‘çš„ä¸€äº›åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨ä¼ ç»Ÿçš„è§†é¢‘è‡ªåŠ¨ç¼–ç å™¨æ¶æ„å°†è§†é¢‘å‹ç¼©åˆ°æ›´ä½ç»´åº¦çš„æ½œåœ¨ç©ºé—´ï¼Œæ˜¾ç¤ºå‡ºå¯æ¯”çš„æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œé‡‡ç”¨æ ‡å‡†å¸§çº§ 2D å’Œ 3D å·ç§¯çš„æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºæ€§è´¨ã€‚(2)ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç§°ä¸º HVDMï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·æ—¶ç©ºä¾èµ–æ€§ã€‚HVDM ç”±ä¸€ä¸ªæ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒï¼Œè¯¥è‡ªåŠ¨ç¼–ç å™¨æå–è§†é¢‘çš„çº ç¼ è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼šç”± 2D æŠ•å½±æ½œåœ¨å˜é‡æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç”±å…·æœ‰å°æ³¢åˆ†è§£çš„ 3D å·ç§¯æ•è·çš„å±€éƒ¨ä½“ç§¯ä¿¡æ¯ä»¥åŠç”¨äºæ”¹è¿›è§†é¢‘é‡å»ºçš„é¢‘ç‡ä¿¡æ¯ã€‚åŸºäºè¿™ç§çº ç¼ è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ··åˆè‡ªåŠ¨ç¼–ç å™¨æä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„è§†é¢‘æ½œåœ¨å˜é‡ï¼Œä¸°å¯Œäº†ç”Ÿæˆè§†é¢‘çš„ç²¾ç»†ç»“æ„å’Œç»†èŠ‚ã€‚(3)ï¼šåœ¨è§†é¢‘ç”ŸæˆåŸºå‡†ï¼ˆUCF101ã€SkyTimelapse å’Œ TaiChiï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå±•ç¤ºäº†å¹¿æ³›çš„è§†é¢‘åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œé•¿è§†é¢‘ç”Ÿæˆã€å›¾åƒåˆ°è§†é¢‘å’Œè§†é¢‘åŠ¨æ€æ§åˆ¶ï¼‰ã€‚(4)ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ UCF101ã€SkyTimelapse å’Œ TaiChi æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚åœ¨ UCF101 æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 2.8% å’Œ 0.011ã€‚åœ¨ SkyTimelapse æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 3.2% å’Œ 0.012ã€‚åœ¨ TaiChi æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 2.9% å’Œ 0.010ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆå…·æœ‰æ›´é«˜è´¨é‡å’Œæ›´ä¸°å¯Œç»†èŠ‚çš„è§†é¢‘ã€‚</p></li><li><p>Methods:(1): æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹HVDMï¼Œå®ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·æ—¶ç©ºä¾èµ–æ€§ã€‚(2): HVDMç”±ä¸€ä¸ªæ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒï¼Œè¯¥è‡ªåŠ¨ç¼–ç å™¨æå–è§†é¢‘çš„çº ç¼ è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼šç”±2DæŠ•å½±æ½œåœ¨å˜é‡æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç”±å…·æœ‰å°æ³¢åˆ†è§£çš„3Då·ç§¯æ•è·çš„å±€éƒ¨ä½“ç§¯ä¿¡æ¯ä»¥åŠç”¨äºæ”¹è¿›è§†é¢‘é‡å»ºçš„é¢‘ç‡ä¿¡æ¯ã€‚(3): åŸºäºè¿™ç§çº ç¼ è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ··åˆè‡ªåŠ¨ç¼–ç å™¨æä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„è§†é¢‘æ½œåœ¨å˜é‡ï¼Œä¸°å¯Œäº†ç”Ÿæˆè§†é¢‘çš„ç²¾ç»†ç»“æ„å’Œç»†èŠ‚ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘ç”Ÿæˆçš„æ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œç§°ä¸º HVDMï¼Œè¯¥æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·æ—¶ç©ºä¾èµ–æ€§ã€‚HVDM ç”±ä¸€ä¸ªæ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨è®­ç»ƒï¼Œè¯¥è‡ªåŠ¨ç¼–ç å™¨æå–è§†é¢‘çš„çº ç¼ è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼šç”± 2D æŠ•å½±æ½œåœ¨å˜é‡æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç”±å…·æœ‰å°æ³¢åˆ†è§£çš„ 3D å·ç§¯æ•è·çš„å±€éƒ¨ä½“ç§¯ä¿¡æ¯ä»¥åŠç”¨äºæ”¹è¿›è§†é¢‘é‡å»ºçš„é¢‘ç‡ä¿¡æ¯ã€‚åŸºäºè¿™ç§çº ç¼ è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ··åˆè‡ªåŠ¨ç¼–ç å™¨æä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„è§†é¢‘æ½œåœ¨å˜é‡ï¼Œä¸°å¯Œäº†ç”Ÿæˆè§†é¢‘çš„ç²¾ç»†ç»“æ„å’Œç»†èŠ‚ã€‚åœ¨ UCF101ã€SkyTimelapse å’Œ TaiChi åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå±•ç¤ºäº†å¹¿æ³›çš„è§†é¢‘åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œé•¿è§†é¢‘ç”Ÿæˆã€å›¾åƒåˆ°è§†é¢‘å’Œè§†é¢‘åŠ¨æ€æ§åˆ¶ï¼‰ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ HVDMï¼Œè¯¥æ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·æ—¶ç©ºä¾èµ–æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§æ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œè¯¥è‡ªåŠ¨ç¼–ç å™¨æå–è§†é¢‘çš„çº ç¼ è¡¨ç¤ºï¼ŒåŒ…æ‹¬ï¼šç”± 2D æŠ•å½±æ½œåœ¨å˜é‡æ•è·çš„å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€ç”±å…·æœ‰å°æ³¢åˆ†è§£çš„ 3D å·ç§¯æ•è·çš„å±€éƒ¨ä½“ç§¯ä¿¡æ¯ä»¥åŠç”¨äºæ”¹è¿›è§†é¢‘é‡å»ºçš„é¢‘ç‡ä¿¡æ¯ã€‚</li><li>é€šè¿‡ç»“åˆè¿™äº›è¡¨ç¤ºä¸æ—¶ç©ºäº¤å‰æ³¨æ„åŠ›ï¼ŒHVDM å¯ä»¥ç”Ÿæˆå…·æœ‰æ”¹è¿›çš„çœŸå®æ„Ÿçš„é«˜è´¨é‡è§†é¢‘ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ UCF101ã€SkyTimelapse å’Œ TaiChi åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</li><li>åœ¨ UCF101 æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 2.8% å’Œ 0.011ã€‚</li><li>åœ¨ SkyTimelapse æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 3.2% å’Œ 0.012ã€‚</li><li>åœ¨ TaiChi æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ FID å’Œ MS-SSIM åº¦é‡ä¸Šåˆ†åˆ«æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 2.9% å’Œ 0.010ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦è®¾è®¡å’Œè®­ç»ƒä¸€ä¸ªæ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨ï¼Œè¯¥è‡ªåŠ¨ç¼–ç å™¨å¯ä»¥æå–è§†é¢‘çš„çº ç¼ è¡¨ç¤ºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦è®¾è®¡å’Œè®­ç»ƒä¸€ä¸ªæ—¶ç©ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥å°†æ··åˆè§†é¢‘è‡ªåŠ¨ç¼–ç å™¨çš„è¡¨ç¤ºèåˆèµ·æ¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦åœ¨å¤šä¸ªè§†é¢‘ç”ŸæˆåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä»¥è¯„ä¼°å…¶æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b0561ef07a60189b28853dc0eda76ddf.jpg" align="middle"><img src="https://pica.zhimg.com/v2-851a92656b32ae2990dcf703193d622b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-63e056db347f6648afdcaf392f094dd6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c9f03009913498a6d9d199e594d8e64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2313ec6324cb296d16788788f949eec.jpg" align="middle"></details><h2 id="ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images"><a href="#ToDo-Token-Downsampling-for-Efficient-Generation-of-High-Resolution-Images" class="headerlink" title="ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images"></a>ToDo: Token Downsampling for Efficient Generation of High-Resolution   Images</h2><p><strong>Authors:Ethan Smith, Nayan Saxena, Aninda Saha</strong></p><p>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity. </p><p><a href="http://arxiv.org/abs/2402.13573v1">PDF</a> </p><p><strong>Summary</strong><br>æ”¹è¿›ç¨³å®šæ‰©æ•£æ³¨æ„æœºåˆ¶ä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¾ˆé‡è¦ï¼Œä½†å…¶äºŒæ¬¡è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†æˆ‘ä»¬åœ¨åˆç†çš„æ—¶é—´å’Œå†…å­˜é™åˆ¶å†…å¯ä»¥å¤„ç†çš„å›¾åƒå¤§å°ã€‚</li><li>ç”Ÿæˆå›¾åƒæ¨¡å‹é€šå¸¸åŒ…å«å†—ä½™ç‰¹å¾ï¼Œé€‚åˆç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å…è®­ç»ƒæ–¹æ³• ToDoï¼Œå®ƒä¾èµ–äºé”®å’Œå€¼æ ‡è®°çš„æ ‡è®°é™é‡‡æ ·ï¼Œä»è€Œå°† Stable Diffusion æ¨ç†é€Ÿåº¦æé«˜äº† 2 å€ï¼ˆå¸¸è§å¤§å°ï¼‰å’Œ 4.5 å€æˆ–æ›´å¤šï¼ˆ2048x2048 ç­‰é«˜åˆ†è¾¨ç‡ï¼‰ã€‚</li><li>ToDo åœ¨å¹³è¡¡æœ‰æ•ˆååé‡å’Œä¿çœŸåº¦æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li><li>ToDo æ˜¯ä¸€ä¸ªå…è´¹ä¸”æ˜“äºå®ç°çš„æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹ã€‚</li><li>ToDo çš„æ¨ç†é€Ÿåº¦æ¯”ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•å¿«ï¼ŒåŒæ—¶è¿˜èƒ½ä¿æŒè‰¯å¥½çš„å›¾åƒè´¨é‡ã€‚</li><li>ToDo å¯ä»¥è®©å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ›´å¤§çš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šToDoï¼šä»¤ç‰Œé™é‡‡æ ·ä»¥é«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒ</li><li>ä½œè€…ï¼šEthan Smithã€Nayan Saxenaã€Aninda Saha</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šLeonardo AI</li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€æ³¨æ„æœºåˆ¶ã€ä»¤ç‰Œé™é‡‡æ ·ã€è®¡ç®—æ•ˆç‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.13573ã€Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ³¨æ„æœºåˆ¶æ˜¯å›¾åƒæ‰©æ•£æ¨¡å‹æˆåŠŸçš„å…³é”®å› ç´ ï¼Œä½†å…¶äºŒæ¬¡è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å›¾åƒå¤„ç†çš„å¤§å°ã€‚æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå›¾åƒæ¨¡å‹ä¸­çš„å¯†é›†æ³¨æ„æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ä»¤ç‰Œé™é‡‡æ ·æ–¹æ³• ToDoï¼Œå¯åŠ é€Ÿ Stable Diffusion æ¨ç†ï¼Œåœ¨å¸¸è§å°ºå¯¸ä¸‹æé€Ÿ 2 å€ï¼Œåœ¨ 2048Ã—2048 ç­‰é«˜åˆ†è¾¨ç‡ä¸‹æé€Ÿ 4.5 å€ä»¥ä¸Šã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„ç¨€ç–æ³¨æ„æ–¹æ³•é€šå¸¸éœ€è¦è®­ç»ƒæ—¶ä¿®æ”¹ï¼Œå¢åŠ äº†ä¼˜åŒ–å¼€é”€ã€‚æ³¨æ„è¿‘ä¼¼æ–¹æ³•è™½ç„¶ä¸éœ€è¦è®­ç»ƒï¼Œä½†é€šå¸¸éœ€è¦é¢„è®­ç»ƒã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ ToDo æ–¹æ³•æ˜¯ä¸€ç§åå¤„ç†æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ³¨æ„æœºåˆ¶ä¸­çš„é”®å’Œå€¼ä»¤ç‰Œè¿›è¡Œé™é‡‡æ ·æ¥åŠ é€Ÿæ¨ç†ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æˆ–é‡æ–°è®­ç»ƒã€‚ï¼ˆ4ï¼‰æ€§èƒ½è¡¨ç°ï¼šToDo æ–¹æ³•åœ¨å„ç§ä»»åŠ¡å’Œæ€§èƒ½æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºä»¥å¾€æ–¹æ³•ï¼Œåœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li></ol><p>Methods:(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºToDoçš„ä»¤ç‰Œé™é‡‡æ ·æ–¹æ³•ï¼Œç”¨äºåŠ é€ŸStableDiffusionæ¨ç†ã€‚(2): ToDoæ–¹æ³•é€šè¿‡å¯¹æ³¨æ„æœºåˆ¶ä¸­çš„é”®å’Œå€¼ä»¤ç‰Œè¿›è¡Œé™é‡‡æ ·æ¥åŠ é€Ÿæ¨ç†ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹æˆ–é‡æ–°è®­ç»ƒã€‚(3): ToDoæ–¹æ³•é‡‡ç”¨äº†ä¸€ç§ä¼˜åŒ–çš„ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨äº†å›¾åƒä»¤ç‰Œå›ºæœ‰çš„ç©ºé—´é‚»è¿‘æ€§ã€‚(4): ToDoæ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†é™é‡‡æ ·æ“ä½œåº”ç”¨äºæ³¨æ„æœºåˆ¶ä¸­çš„é”®å’Œå€¼ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹æŸ¥è¯¢ã€‚(5): ToDoæ–¹æ³•åœ¨å„ç§ä»»åŠ¡å’Œæ€§èƒ½æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºä»¥å¾€æ–¹æ³•ï¼Œåœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ ToDo æ–¹æ³•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡å’Œä¿çœŸåº¦ä¹‹é—´å–å¾—äº†å¾ˆå¥½çš„å¹³è¡¡ï¼Œå°¤å…¶æ˜¯åœ¨é«˜é¢‘åˆ†é‡ä¸Šã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒU-Net ä¸­çš„ç›¸é‚»ç‰¹å¾å¯èƒ½æ˜¯å†—ä½™çš„ï¼Œå¹¶å‡è®¾æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½¿å…¶ä»–åŸºäºæ³¨æ„åŠ›çš„ç”Ÿæˆå›¾åƒæ¨¡å‹å—ç›Šï¼Œå°¤å…¶æ˜¯é‚£äº›åœ¨å¤§é‡ä»¤ç‰Œä¸Šè¿è¡Œçš„æ¨¡å‹ã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥æ¢ç´¢æˆ‘ä»¬æ–¹æ³•çš„å¯å¾®åˆ†æ€§ï¼Œå¹¶åˆ©ç”¨å®ƒæ¥æœ‰æ•ˆåœ°å¾®è°ƒ StableDiffusionï¼Œä½¿å…¶åœ¨ä»¥å‰æœªè§è¿‡çš„æ›´å¤§çš„å›¾åƒå°ºå¯¸ä¸Šè¿è¡Œã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç§°ä¸º ToDo çš„ä»¤ç‰Œé™é‡‡æ ·æ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿ StableDiffusion æ¨ç†ã€‚æ€§èƒ½ï¼šåœ¨å„ç§ä»»åŠ¡å’Œæ€§èƒ½æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºä»¥å¾€æ–¹æ³•ï¼Œåœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å·¥ä½œé‡ï¼šæ— éœ€ä¿®æ”¹æ¨¡å‹æˆ–é‡æ–°è®­ç»ƒï¼Œé‡‡ç”¨äº†ä¸€ç§ä¼˜åŒ–çš„ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨äº†å›¾åƒä»¤ç‰Œå›ºæœ‰çš„ç©ºé—´é‚»è¿‘æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b29b6788a3c63bf19060ac13a17491fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-588f50850c143462d31aee32d4aec168.jpg" align="middle"></details><h2 id="Visual-Style-Prompting-with-Swapping-Self-Attention"><a href="#Visual-Style-Prompting-with-Swapping-Self-Attention" class="headerlink" title="Visual Style Prompting with Swapping Self-Attention"></a>Visual Style Prompting with Swapping Self-Attention</h2><p><strong>Authors:Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh</strong></p><p>In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a>. </p><p><a href="http://arxiv.org/abs/2402.12974v2">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨é£æ ¼æ ·å¼æç¤ºè·å–æ›´å‡†ç¡®åŒ¹é…æ–‡æœ¬æç¤ºçš„å›¾åƒ</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸä¸­è¡¨ç°å‡ºå¼ºå¤§ï¼Œä½†å®ƒä»¬åœ¨ä¿æŒä¸€è‡´é£æ ¼çš„å—æ§ç”Ÿæˆæ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ˜‚è´µçš„å¾®è°ƒæˆ–ç”±äºå†…å®¹æ³„æ¼è€Œæ— æ³•å……åˆ†åœ°å†ç°è§†è§‰å…ƒç´ ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œ\oursï¼Œå¯ä»¥åœ¨ä¿æŒç‰¹å®šé£æ ¼å…ƒç´ å’Œç»†å¾®å·®åˆ«çš„æƒ…å†µä¸‹ç”Ÿæˆå„ç§å›¾åƒã€‚</li><li>åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨æœ€åçš„è‡ªæˆ‘æ³¨æ„å±‚ä¸­æŠŠåŸå§‹ç‰¹å¾ä¸­çš„æŸ¥è¯¢ä¿æŒä¸å˜ï¼ŒåŒæ—¶ç”¨å‚è€ƒç‰¹å¾çš„é”®å’Œå€¼è¿›è¡Œäº¤æ¢ã€‚</li><li>è¿™ç§æ–¹æ³•å…è®¸åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹è¿›è¡Œè§†è§‰é£æ ¼æç¤ºï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¿æŒå¿ å®çš„é£æ ¼ã€‚</li><li>é€šè¿‡åœ¨å„ç§é£æ ¼å’Œæ–‡æœ¬æç¤ºä¸‹çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¯æ˜äº†ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ€èƒ½åæ˜ å‚è€ƒæ–‡çŒ®çš„é£æ ¼ï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„å›¾åƒæœ€å‡†ç¡®åœ°åŒ¹é…æ–‡æœ¬æç¤ºã€‚</li><li>æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯ä»¥åœ¨ <a href="https://curryjung.github.io/VisualStylePrompt/">https://curryjung.github.io/VisualStylePrompt/</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè§†è§‰é£æ ¼æç¤ºä¸äº¤æ¢è‡ªæˆ‘æ³¨æ„åŠ›</li><li>ä½œè€…ï¼šJongwook Choi, Kyumin Lee, Jun-Ho Kim</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€è§†è§‰é£æ ¼æç¤ºã€äº¤æ¢è‡ªæˆ‘æ³¨æ„åŠ›</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08551ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ç€åœ¨ä¿æŒä¸€è‡´é£æ ¼çš„åŒæ—¶å®ç°å¯æ§ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œéœ€è¦æ˜‚è´µçš„å¾®è°ƒæˆ–ç”±äºå†…å®¹æ³„æ¼è€Œå¯¼è‡´è§†è§‰å…ƒç´ è½¬ç§»ä¸è¶³ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¾®è°ƒæˆ–ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹æ¥å®ç°è§†è§‰é£æ ¼æç¤ºï¼Œä½†è¿™äº›æ–¹æ³•å­˜åœ¨æˆæœ¬é«˜æ˜‚ã€é£æ ¼è½¬ç§»ä¸è¶³æˆ–å†…å®¹æ³„æ¼ç­‰é—®é¢˜ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”è§†è§‰é£æ ¼æç¤ºï¼Œé€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹ç‰¹å¾çš„æŸ¥è¯¢ï¼ŒåŒæ—¶åœ¨æœ€åçš„è‡ªæ³¨æ„åŠ›å±‚ä¸­ç”¨å‚è€ƒç‰¹å¾äº¤æ¢é”®å’Œå€¼ï¼Œæ¥å®ç°è§†è§‰é£æ ¼æç¤ºã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦ä»»ä½•å¾®è°ƒï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¿æŒå¿ å®é£æ ¼ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨å„ç§é£æ ¼å’Œæ–‡æœ¬æç¤ºä¸‹çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œåœ¨åæ˜ å‚è€ƒé£æ ¼å’Œç¡®ä¿ç”Ÿæˆå›¾åƒæœ€å‡†ç¡®åœ°åŒ¹é…æ–‡æœ¬æç¤ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å®ç°å…·æœ‰ç‰¹å®šé£æ ¼å…ƒç´ å’Œç»†å¾®å·®åˆ«çš„å›¾åƒç”Ÿæˆã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰é£æ ¼æç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€å¾®è°ƒï¼Œå¹¶ç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¿æŒå¿ å®é£æ ¼ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–¹æ³•åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰é£æ ¼æç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹ç‰¹å¾çš„æŸ¥è¯¢ï¼ŒåŒæ—¶åœ¨æœ€åçš„è‡ªæ³¨æ„åŠ›å±‚ä¸­ç”¨å‚è€ƒç‰¹å¾äº¤æ¢é”®å’Œå€¼ï¼Œæ¥å®ç°è§†è§‰é£æ ¼æç¤ºã€‚æ€§èƒ½ï¼šæœ¬æ–¹æ³•åœ¨å„ç§é£æ ¼å’Œæ–‡æœ¬æç¤ºä¸‹çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œåœ¨åæ˜ å‚è€ƒé£æ ¼å’Œç¡®ä¿ç”Ÿæˆå›¾åƒæœ€å‡†ç¡®åœ°åŒ¹é…æ–‡æœ¬æç¤ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šæœ¬æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒä½ï¼Œä¸éœ€è¦ä»»ä½•å¾®è°ƒï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ca682f6681ca2aea4fdb5980de4dc8f4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d771e643cabdf04390bb34c56e1d306.jpg" align="middle"><img src="https://picx.zhimg.com/v2-11f4ff0d9aeecd7bd560b037f6d9c569.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6b333a460ba441d80a537e0874e7628a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6ef6e8248b60241a24705f590a653e38.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4422e0b37dd7515345602877f9ea3a62.jpg" align="middle"></details><h2 id="CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection"><a href="#CLIPping-the-Deception-Adapting-Vision-Language-Models-for-Universal-Deepfake-Detection" class="headerlink" title="CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection"></a>CLIPping the Deception: Adapting Vision-Language Models for Universal   Deepfake Detection</h2><p><strong>Authors:Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</strong></p><p>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools. </p><p><a href="http://arxiv.org/abs/2402.12927v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>CLIPæ¨¡å‹ç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºä»…ä½¿ç”¨è§†è§‰ä¿¡æ¯çš„SOTAæ–¹æ³•ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>CLIPæ¨¡å‹åœ¨ä¸æœ€è¿‘çš„é€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹é€‚åº”æ–¹æ³•é…å¯¹æ—¶ï¼Œåœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æœ‰æ•ˆæ€§ã€‚</li><li>åªéœ€ä½¿ç”¨ä¸€ä¸ªæ•°æ®é›†ï¼ˆProGANï¼‰å°±å¯ä»¥å¯¹CLIPè¿›è¡Œæ”¹ç¼–ï¼Œä»¥å®ç°æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚</li><li>ä¿ç•™CLIPæ¨¡å‹çš„æ–‡æœ¬éƒ¨åˆ†å¯¹äºæé«˜æ£€æµ‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li><li>åŸºäºPrompt Tuningçš„ç®€å•ä¸”è½»é‡çº§çš„é€‚åº”ç­–ç•¥åœ¨ä½¿ç”¨ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®ï¼ˆ20ä¸‡å¼ å›¾åƒï¼Œè€Œä¹‹å‰çš„æ–¹æ³•ä½¿ç”¨äº†72ä¸‡å¼ å›¾åƒï¼‰çš„æƒ…å†µä¸‹ï¼Œåœ¨mAPå’Œå‡†ç¡®ç‡æ–¹é¢åˆ†åˆ«ä¼˜äºä¹‹å‰çš„SOTAæ–¹æ³•5.01%å’Œ6.61%ã€‚</li><li>CLIPæ¨¡å‹åœ¨å¯¹æ¥è‡ª21ä¸ªä¸åŒæ•°æ®é›†çš„å›¾åƒè¿›è¡Œçš„å…¨é¢è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„çœŸå®ä¸–ç•Œé€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬ç”±åŸºäºGANã€åŸºäºæ‰©æ•£å’Œå•†ä¸šå·¥å…·ç”Ÿæˆçš„å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå‰ªè¾‘æ¬ºéª—ï¼šé€‚åº”é€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹çš„è§†è§‰è¯­è¨€æ¨¡å‹</li><li>ä½œè€…ï¼šSohail Ahmed Khan, Duc-Tien Dang-Nguyen</li><li>å•ä½ï¼šå‘å°”æ ¹å¤§å­¦</li><li>å…³é”®è¯ï¼šæ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œè¿ç§»å­¦ä¹ ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12927ï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šéšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) çš„æœ€æ–°è¿›å±•å’Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œé«˜åº¦é€¼çœŸä¸”å¹¿æ³›å¯è®¿é—®çš„åˆæˆå†…å®¹çš„åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦æœ‰æ•ˆçš„é€šç”¨æ£€æµ‹æœºåˆ¶æ¥å‡è½»æ·±åº¦ä¼ªé€ å¸¦æ¥çš„æ½œåœ¨é£é™©ã€‚(2)ï¼šåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨ä¸æœ€è¿‘çš„é€‚åº”æ–¹æ³•é…å¯¹æ—¶é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) åœ¨é€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚éµå¾ªè¯¥é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å•ä¸ªæ•°æ®é›† (ProGAN) æ¥é€‚åº” CLIP ä»¥è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ç„¶è€Œï¼Œä¸ä»…ä¾èµ– CLIP çš„è§†è§‰éƒ¨åˆ†è€Œå¿½ç•¥å…¶æ–‡æœ¬ç»„ä»¶çš„å…ˆå‰ç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ä¿ç•™æ–‡æœ¬éƒ¨åˆ†è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„ç®€å•è½»é‡çº§ PromptTuning åŸºäºé€‚åº”ç­–ç•¥åœ¨åˆ©ç”¨ä¸åˆ°ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®ï¼ˆ200k å›¾åƒï¼Œç›¸æ¯”ä¹‹ä¸‹ä¸º 720kï¼‰çš„æƒ…å†µä¸‹ï¼Œåœ¨ mAP ä¸Šä¼˜äºä¹‹å‰çš„ SOTA æ–¹æ³• 5.01%ï¼Œå‡†ç¡®ç‡æé«˜ 6.61%ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„å®é™…é€‚ç”¨æ€§ï¼Œæˆ‘ä»¬å¯¹å„ç§åœºæ™¯è¿›è¡Œäº†ç»¼åˆè¯„ä¼°ã€‚è¿™æ¶‰åŠå¯¹æ¥è‡ª 21 ä¸ªä¸åŒæ•°æ®é›†çš„å›¾åƒè¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼ŒåŒ…æ‹¬åŸºäº GANã€åŸºäºæ‰©æ•£å’Œå•†ä¸šå·¥å…·ç”Ÿæˆçš„å›¾åƒã€‚(3)ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ¥é€‚åº” CLIP ä»¥è¿›è¡Œé€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäº PromptTuningï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”æ˜“äºå®ç°çš„é€‚åº”ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¿ç•™ CLIP çš„æ–‡æœ¬éƒ¨åˆ†å¯¹äºæé«˜æ£€æµ‹æ€§èƒ½è‡³å…³é‡è¦ã€‚(4)ï¼šåœ¨ ProGAN æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ mAP ä¸Šå®ç°äº† 95.21% çš„å‡†ç¡®ç‡å’Œ 97.82% çš„å‡†ç¡®ç‡ï¼Œä¼˜äºä¹‹å‰çš„ SOTA æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ£€æµ‹æ¥è‡ªå„ç§æ¥æºçš„æ·±åº¦ä¼ªé€ ï¼ŒåŒ…æ‹¬ GANã€æ‰©æ•£å’Œå•†ä¸šå·¥å…·ã€‚</li></ol><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰çº¿æ€§æ¢æµ‹ï¼šçº¿æ€§æ¢æµ‹æ˜¯ä¸€ç§å°†å†»ç»“æ¨¡å‹ï¼ˆæœ¬ä¾‹ä¸­ä¸º CLIPï¼‰ä½œä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶åœ¨å…¶ä¸Šå¾®è°ƒçº¿æ€§åˆ†ç±»å™¨çš„æ–¹æ³•ã€‚æˆ‘ä»¬éµå¾ª Ojha ç­‰äººé‡‡ç”¨çš„ç›¸åŒæ–¹æ³•ã€‚[32]ï¼Œå³æˆ‘ä»¬ä¸¢å¼ƒ CLIP çš„æ–‡æœ¬ç¼–ç å™¨å¹¶å†»ç»“å…¶å›¾åƒç¼–ç å™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å†»ç»“çš„ CLIP å›¾åƒç‰¹å¾ä¸Šè®­ç»ƒä¸€ä¸ªç”¨äºåˆ†ç±»çš„å•å±‚çº¿æ€§å±‚ï¼Œä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°å°†å€’æ•°ç¬¬äºŒä¸ªå›¾åƒç‰¹å¾æ˜ å°„åˆ°ç”¨äºç±»åˆ«é¢„æµ‹çš„é€»è¾‘å€¼ã€‚ä¼˜åŒ–ä½¿ç”¨äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±è¿›è¡Œã€‚</p><p>ï¼ˆ2ï¼‰å¾®è°ƒï¼šå¾®è°ƒåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­æ„å‘³ç€å†æ¬¡åœ¨ç”¨äºä¸‹æ¸¸æ•°æ®é›†çš„æ•´ä¸ª CLIP æ¨¡å‹ï¼ˆViT-Largeï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯ä¹Ÿè¢« [45] å’Œ [32] ä½¿ç”¨çš„ ProGAN æ•°æ®é›†ã€‚å®Œå…¨å¾®è°ƒéœ€è¦æ˜¾ç€æ›´å¤šçš„è®¡ç®—æœºèµ„æºã€æ•°æ®å’Œè®­ç»ƒæ—¶é—´ï¼Œå› ä¸ºæ•´ä¸ªæ¨¡å‹éƒ½ç»è¿‡äº†é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œéšç€æ¨¡å‹å¤§å°çš„å¢åŠ ï¼Œæ­¤ç­–ç•¥è¡¨ç°å‡ºä¸ç¨³å®šå’Œæ•ˆç‡ä½ä¸‹ [26]ã€‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬é‡åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶é€šè¿‡ä½¿ç”¨æå°çš„å­¦ä¹ ç‡ 1Ã—10-6 æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ä¸ºäº†å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éµå¾ª CLIP é¢„è®­ç»ƒä¸­æ¦‚è¿°çš„ç¨‹åº [37]ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¿®æ”¹ï¼šä¸æ˜¯å¯¹æ¯ä¸ªå›¾åƒä½¿ç”¨æ•´ä¸ªæ–‡æœ¬æ ‡é¢˜ï¼Œæˆ‘ä»¬åªæä¾›å•ä¸ªå•è¯æ ‡é¢˜ï¼Œå…·ä½“æ¥è¯´æ˜¯ real æˆ– fakeã€‚å…¸å‹çš„ç”¨äºè°ƒæ•´ CLIP çš„å¾®è°ƒç®¡é“å¦‚å›¾ 2 æ‰€ç¤ºã€‚</p><p>ï¼ˆ3ï¼‰PromptTuningï¼šPromptTuning æ˜¯ä¸€ç§é€šè¿‡è°ƒæ•´æ–‡æœ¬æç¤ºæ¥é€‚åº” CLIP çš„æ–¹æ³•ã€‚æˆ‘ä»¬éµå¾ª CoOp [50] çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ CLIP çš„æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆä¸€ä¸ªæç¤ºï¼Œè¯¥æç¤ºå¯ä»¥æŒ‡å¯¼å›¾åƒç¼–ç å™¨è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨å•ä¸ªå•è¯æç¤º real æˆ– fake æ¥ç”Ÿæˆå›¾åƒç‰¹å¾ï¼Œç„¶åä½¿ç”¨è¿™äº›ç‰¹å¾æ¥è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ã€‚</p><p>ï¼ˆ4ï¼‰é€‚é…å™¨ç½‘ç»œï¼šé€‚é…å™¨ç½‘ç»œæ˜¯ä¸€ç§é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ·»åŠ å°å‹ç½‘ç»œæ¥é€‚åº”æ–°ä»»åŠ¡çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé€‚é…å™¨ç½‘ç»œæ¥è°ƒæ•´ CLIPï¼Œè¯¥ç½‘ç»œç”±ä¸€ä¸ªå·ç§¯å±‚å’Œä¸€ä¸ªçº¿æ€§å±‚ç»„æˆã€‚é€‚é…å™¨ç½‘ç»œå°† CLIP çš„å›¾åƒç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªç”¨äºåˆ†ç±»çš„é€»è¾‘å€¼ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡æ¢ç´¢é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ CLIP åœ¨é€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº† CLIP åœ¨æ£€æµ‹æ¥è‡ªå„ç§æ•°æ®åˆ†å¸ƒçš„æ·±åº¦ä¼ªé€ å›¾åƒæ–¹é¢çš„é²æ£’æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª ProGAN æ•°æ®é›†çš„ 200k å›¾åƒä½œä¸ºå¤šæ ·åŒ–çš„è®­ç»ƒé›†ï¼Œå¹¶æ¯”è¾ƒäº†å››ç§ä¸åŒçš„è¿ç§»å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å¾®è°ƒã€çº¿æ€§æ¢æµ‹ã€PromptTuning å’Œè®­ç»ƒé€‚é…å™¨ç½‘ç»œã€‚æˆ‘ä»¬çš„å®éªŒåŒ…æ‹¬å¯¹åŒ…å« 21 ä¸ªä¸åŒå›¾åƒç”Ÿæˆå™¨çš„ç»¼åˆæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ•´ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ç»“åˆ CLIP çš„å›¾åƒå’Œæ–‡æœ¬ç»„ä»¶çš„è¿ç§»å­¦ä¹ ç­–ç•¥å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨ CLIP è§†è§‰æ–¹é¢çš„ç®€å•æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ¢æµ‹ï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå‡¸æ˜¾äº† PromptTuning ä¼˜äºå½“å‰åŸºå‡†å’Œ SOTA æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œåœ¨å±•ç¤ºå…¶æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œå³ä½¿è®­ç»ƒå‚æ•°æœ€å°‘ï¼Œä¹Ÿèƒ½å®ç°æ˜¾ç€çš„æ”¹è¿›å¹…åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å°‘é‡å®éªŒï¼Œåˆ†æäº†åœ¨ JPEG å‹ç¼©å’Œé«˜æ–¯æ¨¡ç³Šç­‰åå¤„ç†æ“ä½œä¸‹çš„é²æ£’æ€§ï¼Œå¹¶è¯æ˜äº†å³ä½¿è®­ç»ƒé›†è§„æ¨¡è¾ƒå°ï¼ˆ20k å›¾åƒï¼‰ï¼ŒåŸºäº CLIP çš„æ£€æµ‹å™¨ä¹Ÿå…·æœ‰ç¨³å®šçš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šâ€¢ æ¢ç´¢äº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ CLIP åœ¨é€šç”¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚â€¢ æ¯”è¾ƒäº†å››ç§ä¸åŒçš„è¿ç§»å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å¾®è°ƒã€çº¿æ€§æ¢æµ‹ã€PromptTuning å’Œè®­ç»ƒé€‚é…å™¨ç½‘ç»œã€‚â€¢ è¯æ˜äº†ç»“åˆ CLIP çš„å›¾åƒå’Œæ–‡æœ¬ç»„ä»¶çš„è¿ç§»å­¦ä¹ ç­–ç•¥å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨ CLIP è§†è§‰æ–¹é¢çš„ç®€å•æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ¢æµ‹ï¼‰çš„æ€§èƒ½ã€‚â€¢ PromptTuning ä¼˜äºå½“å‰åŸºå‡†å’Œ SOTA æ–¹æ³•ï¼Œåœ¨å±•ç¤ºå…¶æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œå³ä½¿è®­ç»ƒå‚æ•°æœ€å°‘ï¼Œä¹Ÿèƒ½å®ç°æ˜¾ç€çš„æ”¹è¿›å¹…åº¦ã€‚â€¢ åˆ†æäº†åœ¨ JPEG å‹ç¼©å’Œé«˜æ–¯æ¨¡ç³Šç­‰åå¤„ç†æ“ä½œä¸‹çš„é²æ£’æ€§ï¼Œå¹¶è¯æ˜äº†å³ä½¿è®­ç»ƒé›†è§„æ¨¡è¾ƒå°ï¼ˆ20k å›¾åƒï¼‰ï¼ŒåŸºäº CLIP çš„æ£€æµ‹å™¨ä¹Ÿå…·æœ‰ç¨³å®šçš„æ€§èƒ½ã€‚</li></ol><p>æ€§èƒ½ï¼šâ€¢ åœ¨ ProGAN æ•°æ®é›†ä¸Šï¼ŒPromptTuning åœ¨ mAP ä¸Šå®ç°äº† 95.21% çš„å‡†ç¡®ç‡å’Œ 97.82% çš„å‡†ç¡®ç‡ï¼Œä¼˜äºä¹‹å‰çš„ SOTA æ–¹æ³•ã€‚â€¢ PromptTuning åœ¨ç»¼åˆæµ‹è¯•é›†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ£€æµ‹æ¥è‡ªå„ç§æ¥æºçš„æ·±åº¦ä¼ªé€ ï¼ŒåŒ…æ‹¬ GANã€æ‰©æ•£å’Œå•†ä¸šå·¥å…·ã€‚</p><p>å·¥ä½œé‡ï¼šâ€¢ PromptTuning æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ˜“äºå®ç°ã€‚â€¢ PromptTuning åªéœ€è¦å°‘é‡çš„æ•°æ®å’Œè®­ç»ƒæ—¶é—´ã€‚â€¢ PromptTuning å¯ä»¥ç”¨äºæ£€æµ‹æ¥è‡ªå„ç§æ¥æºçš„æ·±åº¦ä¼ªé€ ï¼ŒåŒ…æ‹¬ GANã€æ‰©æ•£å’Œå•†ä¸šå·¥å…·ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-745a50bdee80b1df6d9da45abefcb26e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e6ec4d0ce05a2af6e93f8a2710069bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca30024b468b77b358f2f1058147b9e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f749a0d770c3a7267b5153b59c39032b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6f8430a1aafee1b2f88631389c9cdc32.jpg" align="middle"><img src="https://pica.zhimg.com/v2-05df037ca314f896a85f2bb5c514f5dd.jpg" align="middle"></details>## RealCompo: Dynamic Equilibrium between Realism and Compositionality   Improves Text-to-Image Diffusion Models**Authors:Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui**Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo [PDF](http://arxiv.org/abs/2402.12908v1) Project: https://github.com/YangLing0818/RealCompo**Summary**åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ— è®­ç»ƒå’Œæ˜“äºè¿ç§»çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ RealCompoï¼Œä»¥å¢å¼ºç”Ÿæˆå›¾åƒçš„çœŸå®æ€§å’Œç»„åˆæ€§ã€‚**Key Takeaways**- RealCompo æ˜¯ä¸€ç§æ–°çš„æ— è®­ç»ƒå’Œæ˜“äºè¿ç§»çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„ä¼˜åŠ¿ã€‚- RealCompo åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œåˆ©ç”¨å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆåˆç†çš„æ„å›¾ã€‚- RealCompo å¼•å…¥äº†æ–°çš„å¹³è¡¡å™¨ï¼Œä»¥åŠ¨æ€å¹³è¡¡ä¸¤ä¸ªæ¨¡å‹åœ¨å»å™ªè¿‡ç¨‹ä¸­çš„ä¼˜åŠ¿ã€‚- RealCompo å³æ’å³ç”¨ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ä½¿ç”¨ä»»ä½•æ¨¡å‹ã€‚- RealCompo åœ¨å¤šå¯¹è±¡ç»„åˆç”Ÿæˆæ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ã€‚- RealCompo ä¿æŒäº†ç”Ÿæˆå›¾åƒçš„ä»¤äººæ»¡æ„çš„çœŸå®æ€§å’Œç»„åˆæ€§ã€‚- RealCompo çš„ä»£ç å¯åœ¨ https://github.com/YangLing0818/RealCompo è·å¾—ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šRealCompoï¼šçœŸå®æ„Ÿä¸ç»„åˆæ€§çš„åŠ¨æ€å¹³è¡¡å¯æ”¹è¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šXinchen Zhang<em>, Ling Yang</em>, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€ç»„åˆæ€§ã€çœŸå®æ„Ÿ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://github.com/YangLing0818/RealCompoGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/YangLing0818/RealCompo</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šå¯¹è±¡ç»„åˆæ€§ç”Ÿæˆæ—¶ä»é¢ä¸´è®¸å¤šå›°éš¾ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ã€‚æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†ç»„åˆæ€§è¾ƒå·®ï¼›å¸ƒå±€åˆ°å›¾åƒæ¨¡å‹èƒ½å¤Ÿæ§åˆ¶å¯¹è±¡çš„ä½ç½®å’Œæ•°é‡ï¼Œä½†çœŸå®æ„Ÿè¾ƒå·®ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒå‹å¥½ä¸”å¯è¿ç§»çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ RealCompoï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æé«˜ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿå’Œç»„åˆæ€§ã€‚RealCompo ä½¿ç”¨äº†ä¸€ä¸ªç›´è§‚ä¸”æ–°é¢–çš„å¹³è¡¡å™¨ï¼Œå¯ä»¥åœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡ä¸¤ä¸ªæ¨¡å‹çš„å¼ºåº¦ï¼Œä»è€Œå…è®¸å³æ’å³ç”¨ä»»ä½•æ¨¡å‹è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒRealCompo åœ¨å¤šå¯¹è±¡ç»„åˆæ€§ç”Ÿæˆä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå›¾åƒä»¤äººæ»¡æ„çš„çœŸå®æ„Ÿå’Œç»„åˆæ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</li></ol><p>Methods:(1) æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒå‹å¥½ä¸”å¯è¿ç§»çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶RealCompoï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æé«˜ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿå’Œç»„åˆæ€§ã€‚(2) è®¾è®¡äº†ä¸€ä¸ªç›´è§‚ä¸”æ–°é¢–çš„å¹³è¡¡å™¨ï¼Œå¯ä»¥åœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡ä¸¤ä¸ªæ¨¡å‹çš„å¼ºåº¦ï¼Œä»è€Œå…è®¸å³æ’å³ç”¨ä»»ä½•æ¨¡å‹è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚(3) åˆ†æäº†æ¯ä¸ªæ¨¡å‹é¢„æµ‹å™ªå£°çš„å½±å“ï¼Œå¹¶æä¾›äº†ä¸€ç§è®¡ç®—ç³»æ•°çš„æ–¹æ³•ã€‚(4) æä¾›äº†å¹³è¡¡å™¨æ‰€é‡‡ç”¨çš„æ›´æ–°è§„åˆ™çš„è¯¦ç»†è§£é‡Šï¼Œè¯¥è§„åˆ™åˆ©ç”¨äº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥åŠ¨æ€æ›´æ–°ç³»æ•°ã€‚(5) æ‰©å±•äº†RealCompoçš„åº”ç”¨ï¼Œä¸ºL2Iæ¨¡å‹çš„æ¯ä¸€ç±»è®¾è®¡äº†æŸå¤±å‡½æ•°ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒå‹å¥½ä¸”å¯è¿ç§»çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶RealCompoï¼Œè¯¥æ¡†æ¶åœ¨å¤šå¯¹è±¡ç»„åˆæ€§ç”Ÿæˆä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå›¾åƒä»¤äººæ»¡æ„çš„çœŸå®æ„Ÿå’Œç»„åˆæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ä¸ªç›´è§‚ä¸”æ–°é¢–çš„å¹³è¡¡å™¨ï¼Œå¯ä»¥åœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡ä¸¤ä¸ªæ¨¡å‹çš„å¼ºåº¦ï¼Œä»è€Œå…è®¸å³æ’å³ç”¨ä»»ä½•æ¨¡å‹è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚åˆ†æäº†æ¯ä¸ªæ¨¡å‹é¢„æµ‹å™ªå£°çš„å½±å“ï¼Œå¹¶æä¾›äº†ä¸€ç§è®¡ç®—ç³»æ•°çš„æ–¹æ³•ã€‚æä¾›äº†å¹³è¡¡å™¨æ‰€é‡‡ç”¨çš„æ›´æ–°è§„åˆ™çš„è¯¦ç»†è§£é‡Šï¼Œè¯¥è§„åˆ™åˆ©ç”¨äº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥åŠ¨æ€æ›´æ–°ç³»æ•°ã€‚æ‰©å±•äº†RealCompoçš„åº”ç”¨ï¼Œä¸ºL2Iæ¨¡å‹çš„æ¯ä¸€ç±»è®¾è®¡äº†æŸå¤±å‡½æ•°ã€‚æ€§èƒ½ï¼šRealCompoåœ¨å¤šå¯¹è±¡ç»„åˆæ€§ç”Ÿæˆä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå¸ƒå±€åˆ°å›¾åƒæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆå›¾åƒä»¤äººæ»¡æ„çš„çœŸå®æ„Ÿå’Œç»„åˆæ€§ã€‚RealCompoå¯ä»¥è¢«æ¨å¹¿åˆ°ä»»ä½•LLMã€T2Iå’ŒL2Iæ¨¡å‹ï¼Œå¹¶ä¿æŒå¼ºå¤§çš„ç”Ÿæˆç»“æœã€‚å·¥ä½œé‡ï¼šRealCompoçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚RealCompoå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç³»ç»Ÿä¸­ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-264ae173bcca3292815b8e45db353de6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c5f244037ff17e98afe9f2c1851e4f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caea4b22ae09f52bc515627d4e3cba84.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5fcdadd1b307e5df492d508f86958e6.jpg" align="middle"></details><h2 id="Two-stage-Rainfall-Forecasting-Diffusion-Model"><a href="#Two-stage-Rainfall-Forecasting-Diffusion-Model" class="headerlink" title="Two-stage Rainfall-Forecasting Diffusion Model"></a>Two-stage Rainfall-Forecasting Diffusion Model</h2><p><strong>Authors:XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang</strong></p><p>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}</a>. </p><p><a href="http://arxiv.org/abs/2402.12779v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨ä¸¤é˜¶æ®µé™é›¨é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTRDMï¼‰æé«˜é™é›¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TRDMæ˜¯ä¸€ç§ç”¨äºé™é›¨é¢„æµ‹ä»»åŠ¡çš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚</li><li>TRDMçš„ç¬¬ä¸€é˜¶æ®µä»»åŠ¡æ˜¯åœ¨ä½åˆ†è¾¨ç‡æ¡ä»¶ä¸‹æ•è·ç¨³å¥çš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚</li><li>TRDMçš„ç¬¬äºŒé˜¶æ®µä»»åŠ¡æ˜¯å°†ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li><li>TRDMåœ¨MRMSå’Œç‘å…¸é›·è¾¾æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>TRDMå¼€æºï¼Œå¯åœ¨ GitHub ä¸Šè·å–ï¼š\href{<a href="https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}ã€‚">https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šä¸¤é˜¶æ®µé™é›¨é¢„æµ‹æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šXu DongLing, Chao RongLi*, FengQing Qin, LiHong Zhu, Yuanyuan Huang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé‡åº†ç†å·¥å¤§å­¦äººå·¥æ™ºèƒ½ä¸å¤§æ•°æ®å­¦é™¢</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€é™é›¨é¢„æµ‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12779ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/clearlyzerolxd/TRDM</li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåœ¨é™é›¨é¢„æµ‹é¢†åŸŸå–å¾—äº†å¾ˆå¤§çš„æˆå°±ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢„æµ‹æ–¹æ³•å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ï¼Œä¾‹å¦‚ç”Ÿæˆçš„å›¾åƒæ¨¡ç³Šã€ç©ºé—´ä½ç½®ä¸å‡†ç¡®ç­‰ã€‚ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šé’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œå·²æœ‰ç ”ç©¶æå‡ºäº†å·ç§¯LSTMå’Œå·ç§¯GRUæ¨¡å‹æ¥æé«˜é™é›¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨é•¿æœŸçš„é¢„æµ‹ä¸­å­˜åœ¨å‡†ç¡®æ€§ä¸é«˜çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒSmaAt-UNetæ¨¡å‹è™½ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è¾“å…¥åºåˆ—ä¸­çš„å…³é”®ç©ºé—´ä¿¡æ¯ï¼Œä½†åœ¨é•¿æœŸé¢„æµ‹æ€§èƒ½æ–¹é¢ä»æœ‰å¾…æé«˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µé™é›¨é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTRDMï¼‰ã€‚TRDMæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„é™é›¨é¢„æµ‹æ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µçš„ä»»åŠ¡æ˜¯åœ¨ä½åˆ†è¾¨ç‡æ¡ä»¶ä¸‹æ•è·é²æ£’çš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µçš„ä»»åŠ¡æ˜¯å°†ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨MRMSå’Œç‘å…¸é›·è¾¾æ•°æ®é›†ä¸Šï¼ŒTRDMå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼šåˆ©ç”¨ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œç”Ÿæˆ 16 å¸§ 32Ã—32 ä½åˆ†è¾¨ç‡é™é›¨ç»“æœï¼ŒåŒæ—¶ä¿ç•™ä¸€å®šç¨‹åº¦çš„ç©ºé—´ä¿¡æ¯ï¼Œä¸ºåç»­é‡å»ºé˜¶æ®µæä¾›é²æ£’çš„åŸºç¡€ã€‚(2) ç©ºé—´è¶…åˆ†è¾¨ç‡ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºè¶…åˆ†è¾¨ç‡ç½‘ç»œï¼Œå°†ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¢å¼ºå›¾åƒè´¨é‡å’Œç»†èŠ‚ï¼Œä»¥ä¾¿æ›´å‡†ç¡®åœ°åˆ†ææœªæ¥é™é›¨çš„å¼ºåº¦å’Œåˆ†å¸ƒã€‚(3) æ½œåœ¨è¶…åˆ†è¾¨ç‡ï¼šæå‡ºä¸€ç§æ½œåœ¨è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œå°†é«˜åˆ†è¾¨ç‡å›¾åƒç¼–ç ä¸ºæ½œåœ¨ç©ºé—´ï¼Œç„¶ååˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¡ä»¶ï¼ŒæŒ‡å¯¼ç”Ÿæˆæ¡ä»¶ã€‚(4) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨ L1 æŸå¤±å‡½æ•°è®­ç»ƒé¢„æµ‹æ‰©æ•£æ¨¡å‹å’Œè¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼Œä»¥æœ€å°åŒ–é¢„æµ‹è¯¯å·®ã€‚(5) æ¨¡å‹æ¨ç†ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå°†ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥åˆ°è¶…åˆ†è¾¨ç‡æ¨¡å‹ä¸­ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¡ä»¶ï¼Œé€æ­¥æ¢å¤é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µé™é›¨é¢„æµ‹æ‰©æ•£æ¨¡å‹ï¼ˆTRDMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰è¾“å…¥åºåˆ—ä¸­çš„å…³é”®ç©ºé—´ä¿¡æ¯ï¼Œå¹¶åœ¨é•¿æœŸé¢„æµ‹æ€§èƒ½æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šTRDMæ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„é¢„æµ‹ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µçš„ä»»åŠ¡æ˜¯åœ¨ä½åˆ†è¾¨ç‡æ¡ä»¶ä¸‹æ•è·é²æ£’çš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µçš„ä»»åŠ¡æ˜¯å°†ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„ä½åˆ†è¾¨ç‡å›¾åƒé‡å»ºä¸ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¿™ç§ä¸¤é˜¶æ®µçš„ç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜é™é›¨é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ€§èƒ½ï¼šåœ¨MRMSå’Œç‘å…¸é›·è¾¾æ•°æ®é›†ä¸Šï¼ŒTRDMæ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒTRDMæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´åŠ æ¸…æ™°å’Œå‡†ç¡®çš„é™é›¨é¢„æµ‹å›¾åƒã€‚å·¥ä½œé‡ï¼šTRDMæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹ç®€å•ã€‚è¯¥æ¨¡å‹åªéœ€è¦å°‘é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”è®­ç»ƒæ—¶é—´è¾ƒçŸ­ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒTRDMæ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿåœ°ç”Ÿæˆé™é›¨é¢„æµ‹å›¾åƒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-77f75079fa9cf15e6ab90ae9bfdf3659.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e40db6d053eb3ccf707a2dbcd4cf2e8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1f5823da8ecb8e38058c288533b8775e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf44de1da53f2ab1acf3c0d8075ec068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b36e8a07f0692df0799659af074a0a49.jpg" align="middle"><img src="https://pica.zhimg.com/v2-56d34d3e7c52a330e5782ff67a0df331.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4bfaafb452921e1d0c1a1d6c62510229.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5fa91d38aada7882b2ac95950348567d.jpg" align="middle"></details><h2 id="MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion"><a href="#MuLan-Multimodal-LLM-Agent-for-Progressive-Multi-Object-Diffusion" class="headerlink" title="MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion"></a>MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</h2><p><strong>Authors:Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou</strong></p><p>Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on <a href="https://github.com/measure-infinity/mulan-code">https://github.com/measure-infinity/mulan-code</a>. </p><p><a href="http://arxiv.org/abs/2402.12741v1">PDF</a> Project website: <a href="https://measure-infinity.github.io/mulan">https://measure-infinity.github.io/mulan</a></p><p><strong>Summary</strong><br>å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åŠ©åŠ›æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šå¯¹è±¡å›¾åƒï¼Œåˆ†æ­¥è§„åˆ’ï¼Œåé¦ˆæ§åˆ¶ï¼Œè½»æ¾æ»¡è¶³å¤æ‚è¦æ±‚ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰çš„æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆå¤šå¯¹è±¡å›¾åƒæ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¯¹è±¡çš„ç©ºé—´ä½ç½®ã€ç›¸å¯¹å¤§å°ã€é‡å å’Œå±æ€§ç»‘å®šæ–¹é¢ã€‚</li><li>MuLan é‡‡ç”¨æ— è®­ç»ƒçš„è®­ç»ƒæ–¹å¼ï¼Œé€šè¿‡è§„åˆ’å’Œåé¦ˆæ§åˆ¶é€æ­¥ç”Ÿæˆå¤šå¯¹è±¡ï¼Œç±»ä¼¼äºäººç±»ç”»å®¶ä½œç”»ã€‚</li><li>MuLan åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å°†æç¤ºåˆ†è§£ä¸ºä¸€ç³»åˆ—å­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡ä»…ç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶é€šè¿‡ç¨³å®šæ‰©æ•£æ¨¡å‹å¯¹å…ˆå‰ç”Ÿæˆçš„å¯¹è±¡è¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚</li><li>ä¸ç°æœ‰çš„ LLM æ–¹æ³•ä¸åŒï¼ŒMuLan åªåœ¨å¼€å§‹æ—¶ç”Ÿæˆä¸€ä¸ªé«˜å±‚æ¬¡çš„è§„åˆ’ï¼Œè€Œæ¯ä¸ªå¯¹è±¡çš„ç¡®åˆ‡å¤§å°å’Œä½ç½®ç”± LLM å’Œæ³¨æ„åŠ›å¼•å¯¼åœ¨æ¯ä¸ªå­ä»»åŠ¡ä¸­ç¡®å®šã€‚</li><li>MuLan é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä¸ºæ¯ä¸ªå­ä»»åŠ¡ä¸­ç”Ÿæˆçš„å›¾åƒæä¾›åé¦ˆï¼Œå¹¶åœ¨å›¾åƒè¿ååŸå§‹æç¤ºæ—¶æ§åˆ¶æ‰©æ•£æ¨¡å‹é‡æ–°ç”Ÿæˆå›¾åƒã€‚</li><li>MuLan åœ¨æ¯ä¸ªæ­¥éª¤ä¸­åªå¤„ç†è‡ªå·±ä¸“é—¨å¤„ç†çš„ç®€å•å­ä»»åŠ¡ã€‚</li><li>MuLan åœ¨ä¸åŒåŸºå‡†ä¸Šæ”¶é›†äº† 200 ä¸ªåŒ…å«ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šçš„å¤šå¯¹è±¡æç¤ºæ¥è¯„ä¼° MuLanã€‚ç»“æœè¡¨æ˜ï¼ŒMuLan åœ¨ç”Ÿæˆå¤šå¯¹è±¡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMuLanï¼šç”¨äºæ¸è¿›å¼å¤šå¯¹è±¡æ‰©æ•£çš„å¤šæ¨¡æ€-LLM ä»£ç†</li><li>ä½œè€…ï¼šSen Liã€Ruochen Wangã€Cho-Jui Hsiehã€Minhao Chengã€Tianyi Zhou</li><li>éš¶å±æœºæ„ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹ç³»</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€å¤šå¯¹è±¡ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ã€è§†è§‰è¯­è¨€æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12741Github é“¾æ¥ï¼šhttps://github.com/measure-infinity/mulan-code</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”ŸæˆåŒ…å«å¤šä¸ªå¯¹è±¡çš„å›¾åƒæ—¶ä»ç„¶å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¯¹è±¡çš„ç©ºé—´ä½ç½®ã€ç›¸å¯¹å¤§å°ã€é‡å å’Œå±æ€§ç»‘å®šæ–¹é¢ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸€äº›æ–¹æ³•è¯•å›¾åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œä½†ç”±äº LLM çš„ç©ºé—´æ¨ç†èƒ½åŠ›æœ‰é™ä»¥åŠå®ƒä»¬ä¸æ‰©æ•£æ¨¡å‹ç¼ºä¹ä¸€è‡´æ€§ï¼Œå› æ­¤ç›´æ¥ç”Ÿæˆå®Œæ•´ä¸”ç²¾ç¡®çš„å¤šå¯¹è±¡å¸ƒå±€ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å°†å¸ƒå±€ä½œä¸ºå¯¹æ¯ä¸ªæ¨¡å‹çš„é¢å¤–æ¡ä»¶ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ‰©æ•£æ¨¡å‹ç”±äºå¯¹å¤æ‚æç¤ºçš„è¯¯è§£è€Œç”Ÿæˆä¸æ­£ç¡®å›¾åƒã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å¯æ§çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œæ˜¯ä¸»è¦å…³æ³¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ã€‚è¯¥èŒƒå¼å»ºç«‹åœ¨ç”±å¤šæ¨¡æ€-LLM ä»£ç† (MuLan) è¿›è¡Œçš„æ¸è¿›å¼å¤šå¯¹è±¡ç”Ÿæˆä¹‹ä¸Šï¼ŒMuLan æ¯ä¸ªé˜¶æ®µåªç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶æ ¹æ®å›¾åƒä¸­å·²ç”Ÿæˆçš„å¯¹è±¡å’Œæœ€æœ‰å¯èƒ½æ”¾ç½®æ–°å¯¹è±¡çš„ä½ç½®çš„æ³¨æ„åŠ›æ©ç è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒMuLan é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æ¥æä¾›å¯¹æ¯ä¸ªå­ä»»åŠ¡ä¸­ç”Ÿæˆçš„å›¾åƒçš„åé¦ˆï¼Œå¹¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ä»¥é‡æ–°ç”Ÿæˆå›¾åƒï¼ˆå¦‚æœå®ƒè¿åäº†åŸå§‹æç¤ºï¼‰ã€‚(4) å®éªŒç»“æœä¸æ€§èƒ½ï¼šåœ¨åŒ…å«æ¥è‡ªä¸åŒåŸºå‡†çš„å¤šå¯¹è±¡ï¼ˆå…·æœ‰ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šï¼‰çš„ 200 ä¸ªæç¤ºä¸Šè¯„ä¼° MuLanã€‚ç»“æœè¡¨æ˜ï¼ŒMuLan åœ¨ç”Ÿæˆå¤šå¯¹è±¡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§æ— è®­ç»ƒä¸”å¯æ§çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œæ˜¯ä¸»è¦å…³æ³¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å¯æ§çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆèŒƒå¼ï¼Œè¯¥èŒƒå¼ä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œæ˜¯ä¸»è¦å…³æ³¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šè¯¥èŒƒå¼å»ºç«‹åœ¨ç”±å¤šæ¨¡æ€-LLMä»£ç†ï¼ˆMuLanï¼‰è¿›è¡Œçš„æ¸è¿›å¼å¤šå¯¹è±¡ç”Ÿæˆä¹‹ä¸Šï¼ŒMuLanæ¯ä¸ªé˜¶æ®µåªç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶æ ¹æ®å›¾åƒä¸­å·²ç”Ÿæˆçš„å¯¹è±¡å’Œæœ€æœ‰å¯èƒ½æ”¾ç½®æ–°å¯¹è±¡çš„ä½ç½®çš„æ³¨æ„åŠ›æ©ç è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æä¾›å¯¹æ¯ä¸ªå­ä»»åŠ¡ä¸­ç”Ÿæˆçš„å›¾åƒçš„åé¦ˆï¼Œå¹¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ä»¥é‡æ–°ç”Ÿæˆå›¾åƒï¼ˆå¦‚æœå®ƒè¿åäº†åŸå§‹æç¤ºï¼‰ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å¯æ§çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆèŒƒå¼MuLanï¼Œè¯¥èŒƒå¼ä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œæ˜¯ä¸»è¦å…³æ³¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ï¼Œåœ¨ç”Ÿæˆå¤šå¯¹è±¡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šMuLanï¼šä¸€ç§æ— è®­ç»ƒä¸”å¯æ§çš„æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆèŒƒå¼ï¼Œä¸éœ€è¦æ¼”ç¤ºï¼Œè€Œæ˜¯ä¸»è¦å…³æ³¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ã€‚æ¸è¿›å¼å¤šå¯¹è±¡ç”Ÿæˆï¼šMuLanæ¯ä¸ªé˜¶æ®µåªç”Ÿæˆä¸€ä¸ªå¯¹è±¡ï¼Œå¹¶æ ¹æ®å›¾åƒä¸­å·²ç”Ÿæˆçš„å¯¹è±¡å’Œæœ€æœ‰å¯èƒ½æ”¾ç½®æ–°å¯¹è±¡çš„ä½ç½®çš„æ³¨æ„åŠ›æ©ç è¿›è¡Œæ¡ä»¶ç”Ÿæˆã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼šé‡‡ç”¨VLMæ¥æä¾›å¯¹æ¯ä¸ªå­ä»»åŠ¡ä¸­ç”Ÿæˆçš„å›¾åƒçš„åé¦ˆï¼Œå¹¶æ§åˆ¶æ‰©æ•£æ¨¡å‹ä»¥é‡æ–°ç”Ÿæˆå›¾åƒï¼ˆå¦‚æœå®ƒè¿åäº†åŸå§‹æç¤ºï¼‰ã€‚æ€§èƒ½ï¼šåœ¨åŒ…å«æ¥è‡ªä¸åŒåŸºå‡†çš„å¤šå¯¹è±¡ï¼ˆå…·æœ‰ç©ºé—´å…³ç³»å’Œå±æ€§ç»‘å®šï¼‰çš„200ä¸ªæç¤ºä¸Šè¯„ä¼°MuLanã€‚ç»“æœè¡¨æ˜ï¼ŒMuLanåœ¨ç”Ÿæˆå¤šå¯¹è±¡æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šMuLançš„å®ç°ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–æ•°æ®ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6204318646d6f8f073e72dd012036b52.jpg" align="middle"><img src="https://pica.zhimg.com/v2-339c08e21eaf72db7bf6af40d44b1ebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c764cf1c9de7293c1a1c79a15a87313.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5f2c4d6c6e5f00fd67d4a729192f3826.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85b2bad757801f5c51069e7f6c02cbc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9001380fef222e92159ed423b319dc8a.jpg" align="middle"></details><h2 id="Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation"><a href="#Improving-Deep-Generative-Models-on-Many-To-One-Image-to-Image-Translation" class="headerlink" title="Improving Deep Generative Models on Many-To-One Image-to-Image   Translation"></a>Improving Deep Generative Models on Many-To-One Image-to-Image   Translation</h2><p><strong>Authors:Sagar Saxena, Mohammad Nayeem Teli</strong></p><p>Deep generative models have been applied to multiple applications in image-to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to-image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves on many-to-one image-to-image translation. </p><p><a href="http://arxiv.org/abs/2402.12531v1">PDF</a> 11 pages, 6 figures</p><p><strong>æ‘˜è¦</strong><br>ç”¨æ·±åº¦æ‰©æ•£æ¨¡å‹æ”¹è¿›å¤šå¯¹ä¸€çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ·±åº¦æ‰©æ•£æ¨¡å‹æ˜¯ç”¨äºå›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„ç”Ÿæˆæ¨¡å‹ã€‚</li><li>ç°æœ‰çš„æ–¹æ³•é€šå¸¸å‡è®¾æ‰€æœ‰é¢†åŸŸéƒ½å…·æœ‰å¤šä¸ªæ¨¡æ€æˆ–åªæœ‰ä¸€ä¸ªæ¨¡æ€ã€‚</li><li>åœ¨è®¸å¤šåœºæ™¯ä¸‹ï¼Œä¸¤ä¸ªé¢†åŸŸä¹‹é—´å­˜åœ¨å¤šå¯¹ä¸€çš„å…³ç³»ã€‚</li><li>ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç€è‰² MNIST æ•°æ®é›†å’Œä¸€ä¸ªå½©è‰²å¬å›åˆ†æ•°ï¼Œä¸ºå¤šå¯¹ä¸€ç¿»è¯‘æä¾›äº†ä¸€ä¸ªç®€å•çš„åŸºå‡†ã€‚</li><li>æå‡ºä¸€ç§æ–°çš„éå¯¹ç§°æ¡†æ¶æ¥æ”¹è¿›ç°æœ‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ã€‚</li><li>å°†è¯¥æ¡†æ¶åº”ç”¨äº StarGAN V2ï¼Œå®éªŒè¡¨æ˜ï¼Œåœ¨æ–°æ¨¡å‹ä¸­ï¼Œæ— ç›‘ç£å’ŒåŠç›‘ç£è®¾ç½®ä¸‹çš„å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ€§èƒ½å‡å¾—åˆ°æé«˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ”¹è¿›å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹</li><li>ä½œè€…ï¼šSagar Saxena, Mohammad Nayeem Teli</li><li>éš¶å±å…³ç³»ï¼šé©¬é‡Œå…°å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šæ·±åº¦ç”Ÿæˆæ¨¡å‹ã€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€å¤šå¯¹ä¸€ç¿»è¯‘ã€éå¯¹ç§°æ¡†æ¶</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12531</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ç”Ÿæˆæ¨¡å‹å·²å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸä¹‹é—´é‡‡ç”¨å¯¹ç§°è®¾ç½®ï¼Œå‡è®¾æ‰€æœ‰é¢†åŸŸéƒ½å…·æœ‰å¤šæ¨¡æ€æˆ–å•ä¸€æ¨¡æ€ã€‚ç„¶è€Œï¼Œè®¸å¤šæ•°æ®é›†åœ¨ä¸¤ä¸ªé¢†åŸŸä¹‹é—´å…·æœ‰å¤šå¯¹ä¸€çš„å…³ç³»ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆå­¦ä¹ åŒå°„æ˜ å°„ï¼Œè¦ä¹ˆå­¦ä¹ å¤šå¯¹å¤šæ˜ å°„ï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•å‡†ç¡®å»ºæ¨¡æŸäº›ä»»åŠ¡ä¸­é¢†åŸŸä¹‹é—´çš„å…³ç³»ï¼Œä¾‹å¦‚å›¾åƒç€è‰²ã€è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ç­‰ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éå¯¹ç§°æ¡†æ¶æ¥æ”¹è¿›ç°æœ‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å—è§£è€¦ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±ç”Ÿæˆå™¨ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸ä¼¼çš„è¾“å‡ºå›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡å°†è¯¥æ¡†æ¶åº”ç”¨äº StarGAN V2 æ¨¡å‹ï¼Œå¹¶åœ¨æ— ç›‘ç£å’ŒåŠç›‘ç£è®¾ç½®ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éå¯¹ç§°æ¡†æ¶æ¥æ”¹è¿›ç°æœ‰æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„éå¯¹ç§°æ¡†æ¶ï¼Œå°†ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å—è§£è€¦ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥é¼“åŠ±ç”Ÿæˆå™¨ç”Ÿæˆä¸è¾“å…¥å›¾åƒç›¸ä¼¼çš„è¾“å‡ºå›¾åƒã€‚</li><li>å°†è¯¥æ¡†æ¶åº”ç”¨äºStarGANV2æ¨¡å‹ï¼Œå¹¶åœ¨æ— ç›‘ç£å’ŒåŠç›‘ç£è®¾ç½®ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚</li><li>ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨å¤šå¯¹ä¸€å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä¸­çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ— ç›‘ç£è®¾ç½®ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨CelebAæ•°æ®é›†ä¸Šå–å¾—äº†æ¯”StarGANV2æ¨¡å‹æ›´é«˜çš„FIDå’ŒLPIPSå¾—åˆ†ã€‚</li><li>åœ¨åŠç›‘ç£è®¾ç½®ä¸­ï¼Œè¯¥æ¡†æ¶åœ¨CelebAæ•°æ®é›†ä¸Šå–å¾—äº†æ¯”StarGANV2æ¨¡å‹æ›´é«˜çš„FIDå’ŒLPIPSå¾—åˆ†ã€‚</li><li>åœ¨Cityscapesæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶å–å¾—äº†æ¯”StarGANV2æ¨¡å‹æ›´é«˜çš„mIoUå’ŒF1å¾—åˆ†ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ¡†æ¶çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¯ä»¥åœ¨TensorFlowæˆ–PyTorchç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­è½»æ¾å®ç°ã€‚</li><li>è¯¥æ¡†æ¶çš„è®­ç»ƒæ—¶é—´ä¸StarGANV2æ¨¡å‹ç›¸ä¼¼ã€‚</li><li>è¯¥æ¡†æ¶çš„æ¨ç†æ—¶é—´ä¸StarGANV2æ¨¡å‹ç›¸ä¼¼ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-847aa6560da9e8f5bc3efa20a3a60ab6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b87108e9e8879c6d14d1fe6eaf34112.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7677caf8041932830de453431d2abd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50784d0e85e2b28f9cc755ede524a772.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9b40dd37bb889c7e90ab259793c5ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df01b0bd8844297db8557dc012591bb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af0a71391ad75be3ea34e547daa4db1e.jpg" align="middle"></details><h2 id="FiT-Flexible-Vision-Transformer-for-Diffusion-Model"><a href="#FiT-Flexible-Vision-Transformer-for-Diffusion-Model" class="headerlink" title="FiT: Flexible Vision Transformer for Diffusion Model"></a>FiT: Flexible Vision Transformer for Diffusion Model</h2><p><strong>Authors:Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai</strong></p><p>Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at <a href="https://github.com/whlzy/FiT">https://github.com/whlzy/FiT</a>. </p><p><a href="http://arxiv.org/abs/2402.12376v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å°†å›¾åƒè§†ä¸ºåŠ¨æ€å¤§å°æ ‡è®°åºåˆ—ï¼Œå¼¹æ€§è§†è§‰å˜æ¢å™¨å¯åœ¨ä¸åŒåˆ†è¾¨ç‡å’Œå®½é«˜æ¯”ä¸Šç”Ÿæˆå›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†è®­ç»ƒåŸŸä¹‹å¤–çš„å›¾åƒåˆ†è¾¨ç‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li><li>å¼¹æ€§è§†è§‰å˜æ¢å™¨ (FiT) æ˜¯ä¸€ç§ä¸“ä¸ºç”Ÿæˆä¸å—é™åˆ†è¾¨ç‡å’Œå®½é«˜æ¯”çš„å›¾åƒè€Œè®¾è®¡çš„è½¬æ¢å™¨æ¶æ„ã€‚</li><li>FiT å°†å›¾åƒè§†ä¸ºåŠ¨æ€å¤§å°æ ‡è®°åºåˆ—ï¼Œä»è€Œæ”¯æŒä¸åŒçš„å®½é«˜æ¯”ã€‚</li><li>FiT åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå‡æ”¯æŒä¸åŒçš„å®½é«˜æ¯”ï¼Œä»è€Œæ¶ˆé™¤äº†å›¾åƒè£å‰ªå¼•èµ·çš„åå·®ã€‚</li><li>FiT åœ¨å¤šç§åˆ†è¾¨ç‡ä¸‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜å…¶åœ¨è®­ç»ƒåˆ†è¾¨ç‡åˆ†å¸ƒä¹‹å¤–ä¹Ÿå¾ˆæœ‰æ•ˆã€‚</li><li>FiT çš„å­˜å‚¨åº“ä½äº <a href="https://github.com/whlzy/FiTã€‚">https://github.com/whlzy/FiTã€‚</a></li><li>FiT ä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šFiTï¼šç”¨äºæ‰©æ•£æ¨¡å‹çš„çµæ´»è§†è§‰å˜æ¢å™¨</li><li>ä½œè€…ï¼šZeyu Lu<em>ï¼ŒZidong Wang</em>ï¼ŒDi Huangï¼ŒChengyue Wuï¼ŒXihui Liuï¼ŒWanli Ouyangï¼ŒLei Bai</li><li>å•ä½ï¼šä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ï¼Œè§†è§‰å˜æ¢å™¨ï¼Œåˆ†è¾¨ç‡æ³›åŒ–ï¼Œå¤–æ¨æŠ€æœ¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.12376ï¼ŒGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè‡ªç„¶ç•Œçš„å›¾åƒåˆ†è¾¨ç‡æ˜¯æ— é™çš„ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£å˜æ¢å™¨ï¼‰åœ¨å¤„ç†è¶…å‡ºå…¶è®­ç»ƒåŸŸçš„å›¾åƒåˆ†è¾¨ç‡æ—¶å¾€å¾€é¢ä¸´æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šä¼ ç»Ÿæ–¹æ³•å°†å›¾åƒè§†ä¸ºé™æ€åˆ†è¾¨ç‡ç½‘æ ¼ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¤„ç†ä¸åŒåˆ†è¾¨ç‡å›¾åƒçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå›¾åƒè£å‰ªä¼šå¼•å…¥åå·®ï¼Œå½±å“æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†çµæ´»è§†è§‰å˜æ¢å™¨ï¼ˆFiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç”Ÿæˆå…·æœ‰æ— é™åˆ†è¾¨ç‡å’Œçºµæ¨ªæ¯”çš„å›¾åƒè€Œè®¾è®¡çš„å˜æ¢å™¨æ¶æ„ã€‚FiT å°†å›¾åƒæ¦‚å¿µåŒ–ä¸ºåŠ¨æ€å¤§å°æ ‡è®°çš„åºåˆ—ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè½»æ¾é€‚åº”ä¸åŒçš„çºµæ¨ªæ¯”ï¼Œä»è€Œä¿ƒè¿›äº†åˆ†è¾¨ç‡æ³›åŒ–å¹¶æ¶ˆé™¤äº†å›¾åƒè£å‰ªå¼•èµ·çš„åå·®ã€‚é€šè¿‡ç²¾å¿ƒè°ƒæ•´çš„ç½‘ç»œç»“æ„å’Œè®­ç»ƒè‡ªç”±å¤–æ¨æŠ€æœ¯çš„é›†æˆï¼ŒFiT åœ¨åˆ†è¾¨ç‡å¤–æ¨ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„çµæ´»æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFiT åœ¨å¹¿æ³›çš„åˆ†è¾¨ç‡èŒƒå›´å†…è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è®­ç»ƒåˆ†è¾¨ç‡åˆ†å¸ƒå†…å’Œä¹‹å¤–çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰çµæ´»è®­ç»ƒï¼šæå‡ºäº†ä¸€ç§çµæ´»çš„è®­ç»ƒæ–¹æ³•ï¼Œå…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†ä¸åŒçºµæ¨ªæ¯”çš„å›¾åƒï¼Œä»è€Œä¿ƒè¿›åˆ†è¾¨ç‡æ³›åŒ–å¹¶æ¶ˆé™¤å›¾åƒè£å‰ªå¼•èµ·çš„åå·®ã€‚</p><p>ï¼ˆ2ï¼‰SwiGLUæ¿€æ´»å‡½æ•°ï¼šå°†MLPæ¿€æ´»å‡½æ•°æ›¿æ¢ä¸ºSwiGLUæ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ3ï¼‰2DRoPEä½ç½®ç¼–ç ï¼šå°†ç»å¯¹ä½ç½®ç¼–ç æ›¿æ¢ä¸º2DRoPEä½ç½®ç¼–ç ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¤–æ¨èƒ½åŠ›ã€‚</p><p>ï¼ˆ4ï¼‰ä½ç½®åµŒå…¥æ’å€¼æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§ä½ç½®åµŒå…¥æ’å€¼æ–¹æ³•ï¼Œå¯ä»¥å°†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›æ‰©å±•åˆ°è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„åˆ†è¾¨ç‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ‰©æ•£æ¨¡å‹çš„çµæ´»è§†è§‰å˜æ¢å™¨ï¼ˆFiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºç”Ÿæˆå…·æœ‰æ— é™åˆ†è¾¨ç‡å’Œçºµæ¨ªæ¯”çš„å›¾åƒè€Œè®¾è®¡çš„å˜æ¢å™¨æ¶æ„ã€‚FiT åœ¨å¹¿æ³›çš„åˆ†è¾¨ç‡èŒƒå›´å†…è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è®­ç»ƒåˆ†è¾¨ç‡åˆ†å¸ƒå†…å’Œä¹‹å¤–çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§çµæ´»çš„è®­ç»ƒæ–¹æ³•ï¼Œå…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†ä¸åŒçºµæ¨ªæ¯”çš„å›¾åƒï¼Œä»è€Œä¿ƒè¿›åˆ†è¾¨ç‡æ³›åŒ–å¹¶æ¶ˆé™¤å›¾åƒè£å‰ªå¼•èµ·çš„åå·®ã€‚</li><li>å°† MLP æ¿€æ´»å‡½æ•°æ›¿æ¢ä¸º SwiGLU æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>å°†ç»å¯¹ä½ç½®ç¼–ç æ›¿æ¢ä¸º 2DRoPE ä½ç½®ç¼–ç ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¤–æ¨èƒ½åŠ›ã€‚</li><li>æå‡ºäº†ä¸€ç§ä½ç½®åµŒå…¥æ’å€¼æ–¹æ³•ï¼Œå¯ä»¥å°†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›æ‰©å±•åˆ°è®­ç»ƒåˆ†å¸ƒä¹‹å¤–çš„åˆ†è¾¨ç‡ã€‚æ€§èƒ½ï¼š</li><li>FiT åœ¨å¹¿æ³›çš„åˆ†è¾¨ç‡èŒƒå›´å†…è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è®­ç»ƒåˆ†è¾¨ç‡åˆ†å¸ƒå†…å’Œä¹‹å¤–çš„æœ‰æ•ˆæ€§ã€‚</li><li>FiT åœ¨å„ç§åˆ†è¾¨ç‡ä¸‹å‡ä¼˜äºæ‰€æœ‰å…ˆå‰æ¨¡å‹ï¼Œæ— è®ºæ˜¯åŸºäº Transformer çš„è¿˜æ˜¯åŸºäº CNN çš„ã€‚</li><li>ç»“åˆæˆ‘ä»¬çš„åˆ†è¾¨ç‡å¤–æ¨æ–¹æ³• VisionNTKï¼ŒFiT çš„æ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æ˜¾ç€æå‡ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°æ¨¡å‹æ¶æ„è®¾è®¡ã€è®­ç»ƒæ–¹æ³•æ”¹è¿›ã€å¤–æ¨æŠ€æœ¯é›†æˆç­‰å¤šä¸ªæ–¹é¢ã€‚</li><li>æœ¬æ–‡çš„å®éªŒéƒ¨åˆ†ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼Œæ¶‰åŠåˆ°å¤šä¸ªæ•°æ®é›†ã€å¤šä¸ªåˆ†è¾¨ç‡ã€å¤šä¸ªè¯„ä»·æŒ‡æ ‡ç­‰ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f2dad57fd66943bffc8c0eefec68b3e8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-297eceedf1e98b27794f86f0cb8285ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6760b58ea1f0ee4f73bf15eae4ddb673.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09693fd0b9790328fcc71c49c26da3ad.jpg" align="middle"></details><h2 id="Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization"><a href="#Direct-Consistency-Optimization-for-Compositional-Text-to-Image-Personalization" class="headerlink" title="Direct Consistency Optimization for Compositional Text-to-Image   Personalization"></a>Direct Consistency Optimization for Compositional Text-to-Image   Personalization</h2><p><strong>Authors:Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin</strong></p><p>Text-to-image (T2I) diffusion models, when fine-tuned on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to fine-tune the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I diffusion models that minimally fine-tunes the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular diffusion loss, while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the image-text alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( <a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>). </p><p><a href="http://arxiv.org/abs/2402.12004v1">PDF</a> Preprint. See our project page (<a href="https://dco-t2i.github.io/">https://dco-t2i.github.io/</a>) for more   examples and codes</p><p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒçš„æ‰©æ•£æ¨¡å‹å¯é€šè¿‡å¾®è°ƒå°‘æ•°ä¸ªäººå›¾åƒç”Ÿæˆé«˜åº¦ä¸€è‡´çš„è§†è§‰æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¾®è°ƒåŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒçš„æ‰©æ•£æ¨¡å‹æ—¶ï¼Œæœ€å¤§åŒ–ä¸å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æƒ©ç½šä¸é¢„è®­ç»ƒæ¨¡å‹çš„åå·®ã€‚</li><li>æå‡ºä¸€ç§æœ€å°åŒ–å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥å®ç°ä¸€è‡´æ€§çš„æ–°é¢–è®­ç»ƒç›®æ ‡ã€‚</li><li>è¯¥æ–¹æ³•ç®€å•ä¸”æœ‰æ•ˆï¼Œæ˜¾ç€å¢å¼ºäº†ä¸ªæ€§åŒ–åŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹çš„ç»„åˆæ€§ã€‚</li><li>å¼•å…¥ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•ä»¥æ§åˆ¶å›¾åƒä¿çœŸåº¦ä¸æç¤ºä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li><li>å¼ºè°ƒä½¿ç”¨ç»¼åˆæ ‡é¢˜ä½œä¸ºå‚è€ƒå›¾åƒä»¥è¿›ä¸€æ­¥å¢å¼ºå›¾åƒä¸æ–‡æœ¬çš„ä¸€è‡´æ€§ã€‚</li><li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä¸»é¢˜ã€é£æ ¼æˆ–ä¸¤è€…æ–¹é¢çš„åŸºäºæ–‡æœ¬ç”Ÿæˆå›¾åƒä¸ªæ€§åŒ–ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šç›´æ¥ä¸€è‡´æ€§ä¼˜åŒ–ç”¨äºåˆæˆæ–‡æœ¬åˆ°å›¾åƒä¸ªæ€§åŒ–</li><li>ä½œè€…ï¼šSeunghoon Hong, Inwoong Ko, Sunghyun Cho, Seonghyeon Nam, Dong Huk Park</li><li>éš¶å±æœºæ„ï¼šé¦–å°”å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€ä¸ªæ€§åŒ–ã€æ‰©æ•£æ¨¡å‹ã€ä¸€è‡´æ€§ä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒ (T2I) æ‰©æ•£æ¨¡å‹åœ¨ç»è¿‡å°‘é‡ä¸ªäººå›¾åƒçš„å¾®è°ƒåï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰é«˜åº¦ä¸€è‡´æ€§çš„è§†è§‰æ•ˆæœã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶ç¼ºä¹åœ¨åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ä¸­å¯èƒ½çš„ä¸åŒåœºæ™¯æˆ–é£æ ¼çš„å›¾åƒåˆæˆèƒ½åŠ›ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æœ€å¤§åŒ–ä¸å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§æ¥å¾®è°ƒ T2I æ¨¡å‹çš„æ–¹æ³•ï¼ŒåŒæ—¶æƒ©ç½šä¸é¢„è®­ç»ƒæ¨¡å‹çš„åå·®ã€‚è¿‡å»çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼Œå®ƒä»¬åœ¨ä¸ªæ€§åŒ– T2I æ¨¡å‹ä¸­ä»ç„¶ç¼ºä¹åˆæˆä¸åŒåœºæ™¯æˆ–é£æ ¼çš„å›¾åƒçš„èƒ½åŠ›ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ T2I æ‰©æ•£æ¨¡å‹è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡å¯ä»¥æœ€å°ç¨‹åº¦åœ°å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥å®ç°ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç§°ä¸ºç›´æ¥ä¸€è‡´æ€§ä¼˜åŒ–ï¼Œå®ƒä¸å¸¸è§„æ‰©æ•£æŸå¤±ä¸€æ ·ç®€å•ï¼ŒåŒæ—¶æ˜¾ç€æé«˜äº†ä¸ªæ€§åŒ– T2I æ¨¡å‹çš„ç»„åˆæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ§åˆ¶å›¾åƒä¿çœŸåº¦ä¸æç¤ºä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ã€‚æœ€åï¼Œæœ¬æ–‡å¼ºè°ƒäº†ä½¿ç”¨ç»¼åˆæ ‡é¢˜ä½œä¸ºå‚è€ƒå›¾åƒçš„å¿…è¦æ€§ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ã€‚(4)ï¼šå®éªŒç»“æœï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨ä¸»é¢˜ã€é£æ ¼æˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹çš„ T2I ä¸ªæ€§åŒ–æ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å¸•ç´¯æ‰˜å‰æ²¿æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li></ol><p>Methods:(1) Direct Consistency Optimization (DCO): We formulate T2I diffusion model fine-tuning as a constrained policy optimization problem and propose DCO loss to maximize the consistency reward of generated samples while penalizing the deviation from the pretrained model.(2) Reward Guidance (RG): After fine-tuning with DCO loss, we introduce RG to control the trade-off between consistency and image-text alignment by interpolating the noise estimations from the fine-tuned model and the pretrained model.(3) Prompt Construction for Reference Images: We emphasize the importance of comprehensive captions for reference images and provide examples to illustrate the difference between compact captions and comprehensive captions.</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡å¯ä»¥æœ€å°ç¨‹åº¦åœ°å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥å®ç°ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹è®­ç»ƒç›®æ ‡ï¼Œè¯¥ç›®æ ‡å¯ä»¥æœ€å°ç¨‹åº¦åœ°å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥å®ç°ä¸€è‡´æ€§ã€‚å¼•å…¥äº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ§åˆ¶å›¾åƒä¿çœŸåº¦ä¸æç¤ºä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡ã€‚å¼ºè°ƒäº†ä½¿ç”¨ç»¼åˆæ ‡é¢˜ä½œä¸ºå‚è€ƒå›¾åƒçš„å¿…è¦æ€§ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ã€‚æ€§èƒ½ï¼šåœ¨ä¸»é¢˜ã€é£æ ¼æˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹çš„æ–‡æœ¬åˆ°å›¾åƒä¸ªæ€§åŒ–æ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨å¸•ç´¯æ‰˜å‰æ²¿æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œå‡†å¤‡å‚è€ƒå›¾åƒã€‚éœ€è¦å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚éœ€è¦é‡‡æ ·ç”Ÿæˆçš„å›¾åƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-747445a04d574a8975290f4c0ffe6aca.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-915bf11d3f533330ed7c94f5f635e501.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3a074dca6974482c499ea0392640cb3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-23  Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet   Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/</id>
    <published>2024-02-13T12:10:49.000Z</published>
    <updated>2024-02-13T12:10:49.162Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-13-æ›´æ–°"><a href="#2024-02-13-æ›´æ–°" class="headerlink" title="2024-02-13 æ›´æ–°"></a>2024-02-13 æ›´æ–°</h1><h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, JoÃ£o Paulo Papa</strong></p><p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the sceneâ€™s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data. </p><p><a href="http://arxiv.org/abs/2402.07310v1">PDF</a> </p><p><strong>Summary</strong><br>ç”Ÿç‰©ç¥ç»å½¢æ€å­¦å¯å‘çš„ NeRF æ¶æ„ï¼Œèåˆå¤šæºè¾“å…¥ï¼Œæå–æ›´æœ¬è´¨ç›¸å…³ä¿¡æ¯ï¼Œå®ç°åœºæ™¯ 3D è¡¨ç¤ºå’Œæ–°è§†è§’åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>BioNeRF æ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»å½¢æ€å­¦å¯å‘çš„æ¶æ„ï¼Œç”¨äºå»ºæ¨¡åœºæ™¯çš„ 3D è¡¨ç¤ºå¹¶é€šè¿‡è¾å°„åœºåˆæˆæ–°è§†è§’ã€‚</li><li>BioNeRF å®ç°äº†ä¸€ç§è®¤çŸ¥å¯å‘çš„æœºåˆ¶ï¼Œå°†æ¥è‡ªå¤šä¸ªæ¥æºçš„è¾“å…¥èåˆåˆ°ä¸€ä¸ªç±»ä¼¼è®°å¿†çš„ç»“æ„ä¸­ï¼Œæé«˜å­˜å‚¨å®¹é‡å¹¶æå–æ›´å¤šå†…åœ¨å’Œç›¸å…³ä¿¡æ¯ã€‚</li><li>BioNeRF æ¨¡ä»¿åœ¨é”¥ä½“ç»†èƒä¸­è§‚å¯Ÿåˆ°çš„å…³äºä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡Œä¸ºï¼Œå…¶ä¸­è®°å¿†è¢«æä¾›ä¸ºä¸Šä¸‹æ–‡å¹¶ä¸ä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹çš„è¾“å…¥ç›¸ç»“åˆï¼Œä¸€ä¸ªè´Ÿè´£äº§ç”Ÿä½“ç§¯å¯†åº¦ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒBioNeRF åœ¨è¡¡é‡äººç±»æ„ŸçŸ¥çš„è´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒå’Œåˆæˆæ•°æ®ä¸¤ç±»æ•°æ®é›†ã€‚</li><li>BioNeRF åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½ä¼˜äºæœ€å…ˆè¿›çš„ç»“æœï¼Œåˆ†åˆ«ä¸ºçœŸå®ä¸–ç•Œå›¾åƒå’Œåˆæˆæ•°æ®ã€‚</li><li>BioNeRF åœ¨è‡ªç”±è§†è§’è§†é¢‘å’Œå…¨æ™¯è§†é¢‘çš„æ¸²æŸ“ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>BioNeRF åœ¨ä¸åŒåœºæ™¯å’Œæ¡ä»¶ä¸‹è¡¨ç°å‡ºé²æ£’æ€§å’Œæ³›åŒ–æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1æ ‡é¢˜ï¼šã€ŠBioNeRF ç”Ÿç‰©åˆç†ç¥ç»è¾å°„åœºçš„è§†å›¾åˆæˆã€‹(BioNeRF Biologically Plausable Neural Radiance Fields for View Synthesis)ã€‚</p><p></p><p></p><p>ä½œè€…åˆ—è¡¨ï¼š(Leandro A Passos)ã€Douglas Rodrigues)ã€Danilo Jodas)ã€Kelton A P Costa)ã€JoÃ£o Paulo Papa)ã€‚</p><p></p><p></p><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼š(å·´è¥¿ Bauru å¸‚ Av Eng Luiz Edmundo Carrijo Coube è¡—åå››ä¹‹ä¸€æ ‹ SÃ£o Paulo State University)ã€‚</p><p></p><p></p><p>å…³é”®è¯ï¼š(ç¥ç»æ¸²æŸ“)ã€ç”Ÿç‰©åˆç†ç¥ç»æ¨¡å‹)ã€‚</p><p></p><p></p><p>é“¾æ¥ï¼š(Paper URL)ã€‚</p><p></p><p></p><p>Githubä»£ç é“¾æ¥ï¼š(Github None)ã€‚</p><p></p><p></p><p>æ‘˜è¦ï¼š(BioNeRFæ˜¯ä¸€ç§ç”Ÿç‰©åˆç†æ¶æ„)ï¼Œå¯ä»¥åˆ©ç”¨è¾å°„å­—æ®µæ„å»ºåœºæ™¯çš„ä¸‰ D è¡¨ç¤ºå½¢å¼å¹¶ä¸”åˆæˆæ–°çš„è§†å›¾)ã€‚ç”±äº NeRF åˆ©ç”¨ç½‘ç»œä¸­çš„å„ç§å‚æ•°å­˜å‚¨åœºæ™¯çš„ä¸‰ D è¡¨ç¤ºå½¢å¼)ï¼ŒBioNeRF ä¾¿é‡‡ç”¨ä¸€ç§è®¤çŸ¥æ¿€åŠ±æ–¹æ³•)ï¼Œé€šè¿‡èåˆå¤šä¸ªæ¥æºä¸­çš„ä¿¡æ¯ç”Ÿæˆè®°å¿†ç»“æ„)ï¼Œä»è€Œæé«˜å‚¨å­˜å®¹é‡å¹¶ä¸”æå–æ›´å¤šæœ¬è´¨ä¿¡æ¯ä»¥åŠç›¸å…³ä¿¡æ¯)ã€‚BioNeRF è¿˜æ¨¡ä»¿é”¥ä½“å‹ç¥ç»ç»†èƒæœ‰å…³ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡Œä¸º)ï¼Œå…¶ä¸­è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›)ï¼Œå¹¶ä¸”ç»“åˆä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹ä¸­çš„ä¿¡æ¯)ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆå®¹é‡å¯†åº¦)ï¼Œå¦ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²)ã€‚å®éªŒç»“æœè¡¨æ˜)ï¼ŒBioNeRF åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„è´¨é‡è¯„ä¼°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯)ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒä»¥åŠåˆæˆæ•°æ®)ã€‚</p><p></p><p></p><p>æ‘˜è¦ï¼š(BioNeRFæ˜¯ä¸€ç§ç”Ÿç‰©åˆç†æ¶æ„)ï¼Œå¯ä»¥åˆ©ç”¨è¾å°„å­—æ®µæ„å»ºåœºæ™¯çš„ä¸‰ D è¡¨ç¤ºå½¢å¼å¹¶ä¸”åˆæˆæ–°çš„è§†å›¾)ã€‚ç”±äº NeRF åˆ©ç”¨ç½‘ç»œä¸­çš„å„ç§å‚æ•°å­˜å‚¨åœºæ™¯çš„ä¸‰ D è¡¨ç¤ºå½¢å¼)ï¼ŒBioNeRF ä¾¿é‡‡ç”¨ä¸€ç§è®¤çŸ¥æ¿€åŠ±æ–¹æ³•)ï¼Œé€šè¿‡èåˆå¤šä¸ªæ¥æºä¸­çš„ä¿¡æ¯ç”Ÿæˆè®°å¿†ç»“æ„)ï¼Œä»è€Œæé«˜å‚¨å­˜å®¹é‡å¹¶ä¸”æå–æ›´å¤šæœ¬è´¨ä¿¡æ¯ä»¥åŠç›¸å…³ä¿¡æ¯)ã€‚BioNeRF è¿˜æ¨¡ä»¿é”¥ä½“å‹ç¥ç»ç»†èƒæœ‰å…³ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡Œä¸º)ï¼Œå…¶ä¸­è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›)ï¼Œå¹¶ä¸”ç»“åˆä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹ä¸­çš„ä¿¡æ¯)ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆå®¹é‡å¯†åº¦)ï¼Œå¦ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²)ã€‚å®éªŒç»“æœè¡¨æ˜)ï¼ŒBioNeRF åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„è´¨é‡è¯„ä¼°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯)ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒä»¥åŠåˆæˆæ•°æ®)ã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰BioNeRFé‡‡ç”¨è®¤çŸ¥å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡èåˆå¤šä¸ªæ¥æºä¸­çš„ä¿¡æ¯ç”Ÿæˆè®°å¿†ç»“æ„ï¼Œä»è€Œæé«˜å­˜å‚¨å®¹é‡å¹¶æå–æ›´å¤šæœ¬è´¨ä¿¡æ¯å’Œç›¸å…³ä¿¡æ¯ã€‚ï¼ˆ2ï¼‰BioNeRFæ¨¡ä»¿é”¥ä½“å‹ç¥ç»ç»†èƒæœ‰å…³ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡Œä¸ºï¼Œå…¶ä¸­è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ï¼Œå¹¶ç»“åˆä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹ä¸­çš„ä¿¡æ¯ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆå®¹é‡å¯†åº¦ï¼Œå¦ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²ã€‚ï¼ˆ3ï¼‰å®éªŒç»“æœè¡¨æ˜ï¼ŒBioNeRFåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„è´¨é‡è¯„ä¼°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒä»¥åŠåˆæˆæ•°æ®ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šBioNeRFåœ¨ç¥ç»æ¸²æŸ“é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿç‰©åˆç†ç¥ç»è¾å°„åœºæ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿåˆ©ç”¨è¾å°„å­—æ®µæ„å»ºåœºæ™¯çš„ä¸‰ç»´è¡¨ç¤ºå½¢å¼å¹¶åˆæˆæ–°çš„è§†å›¾ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>BioNeRFé‡‡ç”¨äº†ä¸€ç§è®¤çŸ¥å¯å‘çš„æ–¹æ³•ï¼Œé€šè¿‡èåˆå¤šä¸ªæ¥æºä¸­çš„ä¿¡æ¯ç”Ÿæˆè®°å¿†ç»“æ„ï¼Œä»è€Œæé«˜å­˜å‚¨å®¹é‡å¹¶æå–æ›´å¤šæœ¬è´¨ä¿¡æ¯å’Œç›¸å…³ä¿¡æ¯ã€‚</li><li>BioNeRFæ¨¡ä»¿é”¥ä½“å‹ç¥ç»ç»†èƒæœ‰å…³ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è¡Œä¸ºï¼Œå…¶ä¸­è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ï¼Œå¹¶ç»“åˆä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹ä¸­çš„ä¿¡æ¯ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆå®¹é‡å¯†åº¦ï¼Œå¦ä¸€ä¸ªæ¨¡å‹è´Ÿè´£ç”Ÿæˆç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²ã€‚</li><li>BioNeRFåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„è´¨é‡è¯„ä¼°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒä»¥åŠåˆæˆæ•°æ®ã€‚æ€§èƒ½ï¼š</li><li>BioNeRFåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„è´¨é‡è¯„ä¼°æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œè¿™äº›æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒä»¥åŠåˆæˆæ•°æ®ã€‚å·¥ä½œé‡ï¼š</li><li>BioNeRFçš„å®ç°éš¾åº¦è¾ƒé«˜ï¼Œéœ€è¦è¾ƒå¼ºçš„ç¼–ç¨‹èƒ½åŠ›å’Œæ•°å­¦åŸºç¡€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a3147366d087ebe11e207f5d9173f950.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-91083b7a4d33cafbb989e6672e5d0690.jpg" align="middle"></details><h2 id="NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction"><a href="#NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction" class="headerlink" title="NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction"></a>NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction</h2><p><strong>Authors:Zhongqun Zhang, Jifei Song, Eduardo PÃ©rez-Pellitero, Yiren Zhou, Hyung Jin Chang, AleÅ¡ Leonardis</strong></p><p>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2402.05532v2">PDF</a> Accepted by 3DV 2024</p><p><strong>Summary</strong><br>æ‰‹-ç‰©äº¤äº’çš„è‡ªç”±è§†è§’é€¼çœŸé‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰‹-ç‰©äº¤äº’å»ºæ¨¡æ˜¯è®¡ç®—æœºä¸‰ç»´å»ºæ¨¡çš„æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚</li><li>ç°å­˜æ–¹æ³•æ— æ³•çœŸå®åœ°è¿›è¡Œæ‰‹-ç‰©äº¤äº’å»ºæ¨¡ã€‚</li><li>æå‡º NCRF æ¡†æ¶æ¥ä»è§†é¢‘ä¸­é‡å»ºæ‰‹-ç‰©äº¤äº’ã€‚</li><li>NCRF åŒ…æ‹¬æ¥è§¦ä¼˜åŒ–åœºå’Œæ‰‹-ç‰©çš„ç¥ç»è¾å°„åœºã€‚</li><li>æ¥è§¦ä¼˜åŒ–åœºé¢„æµ‹ä¸‰ç»´æŸ¥è¯¢ç‚¹ç²¾ç¡®çš„æ¥è§¦åœºã€‚</li><li>æ‰‹-ç‰©çš„ç¥ç»è¾å°„åœºå­¦ä¹ æ‰‹-ç‰©éšå¼è¡¨ç¤ºã€‚</li><li>æ‰‹-ç‰©è¿åŠ¨åœºäº§ç”Ÿè§‚å¯Ÿåˆ°æ ‡å‡†çš„å¯¹åº”å…³ç³»ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šNCRFï¼šç”¨äºæ‰‹-ç‰©ä½“äº¤äº’è‡ªç”±è§†ç‚¹æ¸²æŸ“çš„ç¥ç»æ¥è§¦è¾å°„åœº</li><li>ä½œè€…ï¼šZhongqun Zhang, Jifei Song, Eduardo PÃ©rez-Pellitero, Yiren Zhou, Hyung Jin Chang, AleÅ¡ Leonardis</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¼¯æ˜ç¿°å¤§å­¦</li><li>å…³é”®è¯ï¼šæ‰‹-ç‰©ä½“äº¤äº’ã€è‡ªç”±è§†ç‚¹æ¸²æŸ“ã€ç¥ç»è¾å°„åœºã€æ¥è§¦åœºä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.05532</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰‹-ç‰©ä½“äº¤äº’å»ºæ¨¡æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡è¯¥é¢†åŸŸå–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶æ— æ³•ä»¥é€¼çœŸçš„æ–¹å¼åˆæˆæ‰‹-ç‰©ä½“äº¤äº’ï¼Œè¿™æºäºæ‰‹å’Œç‰©ä½“ä¹‹é—´ä¸¥é‡çš„ç›¸äº’é®æŒ¡ä»¥åŠä¸å‡†ç¡®çš„æ‰‹-ç‰©ä½“å§¿æ€ä¼°è®¡ï¼Œä»è€Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€å·¥ä½œé€šå¸¸å°†æ­¤ä»»åŠ¡è¡¨è¿°ä¸ºè”åˆæ‰‹å’Œç‰©ä½“å§¿æ€ä¼°è®¡é—®é¢˜ï¼Œå¹¶ä¾èµ–å‚æ•°åŒ–çš„æ‰‹-ç‰©ä½“æ¨¡å‹ï¼ˆå¦‚ MANO å’Œ YCBï¼‰æ¥ä¼°è®¡æ‰‹çš„è¿åŠ¨å˜æ¢ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ¢å¤æ‰‹-ç‰©ä½“æ¥è§¦åœºçš„å‡†ç¡®å‡ ä½•å½¢çŠ¶ï¼Œå¹¶ä¸”æ¸²æŸ“è´¨é‡å—åˆ°é®æŒ¡å’Œå§¿æ€ä¼°è®¡è¯¯å·®çš„ä¸¥é‡å½±å“ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“æ¡†æ¶â€”â€”ç¥ç»æ¥è§¦è¾å°„åœºï¼ˆNCRFï¼‰ï¼Œä»¥ä»ä¸€ç»„ç¨€ç–è§†é¢‘ä¸­é‡å»ºæ‰‹-ç‰©ä½“äº¤äº’ã€‚NCRF æ¡†æ¶ä¸»è¦ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šï¼ˆaï¼‰æ¥è§¦ä¼˜åŒ–åœºï¼šä» 3D æŸ¥è¯¢ç‚¹é¢„æµ‹å‡†ç¡®çš„æ¥è§¦åœºï¼Œä»¥å®ç°æ‰‹å’Œç‰©ä½“ä¹‹é—´çš„ç†æƒ³æ¥è§¦ã€‚ï¼ˆbï¼‰æ‰‹-ç‰©ä½“ç¥ç»è¾å°„åœºï¼šä¸ä¸“é—¨è®¾è®¡çš„æ‰‹-ç‰©ä½“è¿åŠ¨åœºååŒå·¥ä½œï¼Œå­¦ä¹ é™æ€è§„èŒƒç©ºé—´ä¸­çš„éšå¼æ‰‹-ç‰©ä½“è¡¨ç¤ºï¼Œä»¥äº§ç”Ÿè§‚æµ‹åˆ°è§„èŒƒçš„å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬è”åˆå­¦ä¹ è¿™äº›å…³é”®ç»„ä»¶ï¼Œå®ƒä»¬é€šè¿‡è§†è§‰å’Œå‡ ä½•çº¦æŸç›¸äº’å¸®åŠ©å’Œæ­£åˆ™åŒ–ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ‰‹-ç‰©ä½“é‡å»ºï¼Œå®ç°é€¼çœŸçš„æ–°è§†è§’åˆæˆã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒäº†å®ƒä»¬çš„ç›®æ ‡ï¼šåœ¨ HO3D å’Œ Dex-YCB æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œå§¿æ€ä¼°è®¡ç²¾åº¦æ–¹é¢å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³ä»¥é€¼çœŸçš„æ–¹å¼é‡å»ºå’Œæ¸²æŸ“æ‰‹-ç‰©ä½“äº¤äº’ã€‚</p></li><li><p>Methods:(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“æ¡†æ¶â€”â€”ç¥ç»æ¥è§¦è¾å°„åœºï¼ˆNCRFï¼‰ï¼Œä»¥ä»ä¸€ç»„ç¨€ç–è§†é¢‘ä¸­é‡å»ºæ‰‹-ç‰©ä½“äº¤äº’ã€‚(2): NCRFæ¡†æ¶ä¸»è¦ç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šï¼ˆaï¼‰æ¥è§¦ä¼˜åŒ–åœºï¼šä»3DæŸ¥è¯¢ç‚¹é¢„æµ‹å‡†ç¡®çš„æ¥è§¦åœºï¼Œä»¥å®ç°æ‰‹å’Œç‰©ä½“ä¹‹é—´çš„ç†æƒ³æ¥è§¦ã€‚ï¼ˆbï¼‰æ‰‹-ç‰©ä½“ç¥ç»è¾å°„åœºï¼šä¸ä¸“é—¨è®¾è®¡çš„æ‰‹-ç‰©ä½“è¿åŠ¨åœºååŒå·¥ä½œï¼Œå­¦ä¹ é™æ€è§„èŒƒç©ºé—´ä¸­çš„éšå¼æ‰‹-ç‰©ä½“è¡¨ç¤ºï¼Œä»¥äº§ç”Ÿè§‚æµ‹åˆ°è§„èŒƒçš„å¯¹åº”å…³ç³»ã€‚(3): æˆ‘ä»¬è”åˆå­¦ä¹ è¿™äº›å…³é”®ç»„ä»¶ï¼Œå®ƒä»¬é€šè¿‡è§†è§‰å’Œå‡ ä½•çº¦æŸç›¸äº’å¸®åŠ©å’Œæ­£åˆ™åŒ–ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ‰‹-ç‰©ä½“é‡å»ºï¼Œå®ç°é€¼çœŸçš„æ–°è§†è§’åˆæˆã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“æ¡†æ¶â€”â€”ç¥ç»æ¥è§¦è¾å°„åœºï¼ˆNCRFï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä¸€ç»„ç¨€ç–è§†é¢‘ä¸­é‡å»ºæ‰‹-ç‰©ä½“äº¤äº’ï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„æ–°è§†è§’åˆæˆã€‚NCRFæ¡†æ¶é€šè¿‡è®¾è®¡åŠ¨æ€æ‰‹-ç‰©ä½“ç¥ç»è¾å°„åœºå’Œæ¥è§¦ä¼˜åŒ–åœºï¼Œèƒ½å¤Ÿå»ºæ¨¡å…·æœ‰å¤æ‚æ‰‹éƒ¨æŠ“æ¡åŠ¨ä½œå’Œé¢‘ç¹ç›¸äº’é®æŒ¡çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹-ç‰©ä½“äº¤äº’ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“æ¡†æ¶â€”â€”ç¥ç»æ¥è§¦è¾å°„åœºï¼ˆNCRFï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»ä¸€ç»„ç¨€ç–è§†é¢‘ä¸­é‡å»ºæ‰‹-ç‰©ä½“äº¤äº’ï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„æ–°è§†è§’åˆæˆã€‚</li><li>è®¾è®¡äº†åŠ¨æ€æ‰‹-ç‰©ä½“ç¥ç»è¾å°„åœºå’Œæ¥è§¦ä¼˜åŒ–åœºï¼Œèƒ½å¤Ÿå»ºæ¨¡å…·æœ‰å¤æ‚æ‰‹éƒ¨æŠ“æ¡åŠ¨ä½œå’Œé¢‘ç¹ç›¸äº’é®æŒ¡çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹-ç‰©ä½“äº¤äº’ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰‹-ç‰©ä½“å˜å½¢æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå°†å°„çº¿å˜å½¢åˆ°è§„èŒƒç©ºé—´ä¸­ï¼Œå¹¶ä»¥é€¼çœŸçš„æ–¹å¼æ¸²æŸ“æ‰‹-ç‰©ä½“äº¤äº’ã€‚æ€§èƒ½ï¼š</li><li>åœ¨HO3Då’ŒDex-YCBæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒNCRFæ¡†æ¶åœ¨æ¸²æŸ“è´¨é‡å’Œå§¿æ€ä¼°è®¡ç²¾åº¦æ–¹é¢å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>NCRFæ¡†æ¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œå¹¶ä¸”èƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†å…·æœ‰å¤æ‚æ‰‹éƒ¨æŠ“æ¡åŠ¨ä½œå’Œé¢‘ç¹ç›¸äº’é®æŒ¡çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‰‹-ç‰©ä½“äº¤äº’ã€‚å·¥ä½œé‡ï¼š</li><li>NCRFæ¡†æ¶çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li><li>NCRFæ¡†æ¶çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„æ•°æ®ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-19c080ef42e2fcaa0595e65274d339b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b7f0899ff9371cac98ca44ab3913a349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1403a98bc963e537484ce413bb5d32ea.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v2">PDF</a> </p><p><strong>Summary</strong><br>å¯¹äºå¤§åœºæ™¯ä¸‹çš„é‡å»ºä»»åŠ¡ï¼Œæœ¬æ–‡å¼•å…¥ BirdNeRFï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ— äººæœºå½±åƒæ•°æ®å®ç°é«˜æ•ˆä½å­˜å‚¨çš„å¤§åœºæ™¯é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>BirdNeRF æ˜¯ä¸€æ¬¾é’ˆå¯¹èˆªç©ºå›¾åƒçš„å¤§åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œè§£å†³äº†ä»¥å¾€å°åœºæ™¯é‡å»ºå­˜åœ¨çš„è®­ç»ƒæ…¢ã€æ¸²æŸ“æ…¢ã€æ¨¡å‹å®¹é‡å°ç­‰é—®é¢˜ã€‚</li><li>BirdNeRF æå‡ºäº†ä¸€ç§åŸºäºé¸Ÿç°è§†è§’çš„å§¿åŠ¿åˆ†è§£ç®—æ³•ï¼Œå°†å¤§åœºæ™¯å›¾åƒé›†åˆ†è§£æˆå¤šä¸ªå°åœºæ™¯å­é›†ï¼Œæ¯ä¸ªå­é›†ä½¿ç”¨å•ç‹¬çš„ NeRF è¿›è¡Œè®­ç»ƒã€‚</li><li>BirdNeRF é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„æŠ•å½±å¼•å¯¼å¼æ–°è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯ç”Ÿæˆæ›´å¥½çš„æ¸²æŸ“ç»“æœã€‚</li><li>BirdNeRF åœ¨ç°æœ‰æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ— äººæœºæ•°æ®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å•ä¸ª GPU ä¸Šçš„é‡å»ºé€Ÿåº¦æ¯”ç»å…¸æ‘„å½±æµ‹é‡è½¯ä»¶å¿« 10 å€ï¼Œæ¯”æœ€å…ˆè¿›çš„å¤§åœºæ™¯ NeRF è§£å†³æ–¹æ¡ˆå¿« 50 å€ï¼Œä¸”æ¸²æŸ“è´¨é‡ç›¸ä¼¼ã€‚</li><li>BirdNeRF å¯ä»¥åœ¨ä»»æ„å¤§çš„åœºæ™¯ä¸­æ— ç¼æ‰©å±•ï¼Œå¹¶æ”¯æŒå¯¹ç¯å¢ƒçš„å±€éƒ¨æ›´æ–°ï¼Œæé«˜äº†é‡å»ºè¿‡ç¨‹çš„çµæ´»æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šBirdNeRFï¼šåŸºäºèˆªæ‹å›¾åƒçš„å¤§åœºæ™¯å¿«é€Ÿç¥ç»é‡å»º</li><li>ä½œè€…ï¼šå¼ æƒ å¿ã€è–›ä¸€è²ã€å»–æ˜ã€è€ä¹‰ç</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šNeRFã€å¤§åœºæ™¯é‡å»ºã€èˆªæ‹å›¾åƒã€ç©ºé—´åˆ†è§£ã€æŠ•å½±å¼•å¯¼</li><li>é“¾æ¥ï¼šæ— ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§åœºæ™¯ä¸‰ç»´é‡å»ºæ˜¯æ‘„å½±æµ‹é‡å’Œé¥æ„Ÿé¢†åŸŸçš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œå¯ä»¥åˆ©ç”¨èˆªæ‹æˆ–å«æ˜Ÿå›¾åƒã€æ¿€å…‰é›·è¾¾æ•°æ®å’Œè¡—æ™¯å›¾åƒç­‰å¤šç§æ•°æ®æºæ„å»ºåŸå¸‚çš„ä¸‰ç»´æ¨¡å‹ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå›¾åƒçš„ä¸‰ç»´é‡å»ºæŠ€æœ¯å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œå¹¶åœ¨åŸå¸‚è§„åˆ’ã€å¯¼èˆªã€è™šæ‹Ÿæ—…æ¸¸ã€æˆ¿åœ°äº§ã€ç¾å®³ç®¡ç†å’Œå†å²ä¿æŠ¤ç­‰é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šç°æœ‰çš„åŸºäºå›¾åƒçš„ä¸‰ç»´é‡å»ºæŠ€æœ¯ä¸»è¦åˆ†ä¸ºä¼ ç»Ÿå‡ ä½•æ–¹æ³•å’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚ä¼ ç»Ÿå‡ ä½•æ–¹æ³•ä¸»è¦åŒ…æ‹¬æ‘„å½±æµ‹é‡å’Œæ¿€å…‰æ‰«æï¼Œè¿™äº›æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜ç²¾åº¦çš„ä¸‰ç»´æ¨¡å‹ï¼Œä½†éœ€è¦å¤§é‡çš„äººå·¥å‚ä¸å’Œæ˜‚è´µçš„è®¾å¤‡ã€‚åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œå¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œå¯ä»¥ä»å›¾åƒä¸­è‡ªåŠ¨å­¦ä¹ ä¸‰ç»´åœºæ™¯çš„è¡¨ç¤ºï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨å¤§åœºæ™¯é‡å»ºä»»åŠ¡ä¸­å®¹æ˜“å‡ºç°ä¼ªå½±å’Œä½è§†è§‰ä¿çœŸåº¦çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº NeRF çš„å¤§åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º BirdNeRFã€‚BirdNeRF é‡‡ç”¨äº†ä¸€ç§æ–°çš„é¸Ÿç°è§†è§’å§¿åŠ¿åˆ†è§£ç®—æ³•ï¼Œå°†å¤§åœºæ™¯å›¾åƒåˆ†è§£æˆå¤šä¸ªå°åœºæ™¯ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå°åœºæ™¯çš„ NeRF æ¨¡å‹ã€‚è¿™ç§åˆ†è§£æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ï¼Œè¿˜å¯ä»¥æé«˜é‡å»ºçš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒBirdNeRF è¿˜æå‡ºäº†ä¸€ç§æŠ•å½±å¼•å¯¼çš„æ–°è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å°åœºæ™¯æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚ï¼ˆ4ï¼‰æ€§èƒ½ä¸ç›®æ ‡ï¼šBirdNeRF åœ¨ç°æœ‰æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ— äººæœºèˆªæ‹æ•°æ®ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒBirdNeRF çš„é‡å»ºé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ‘„å½±æµ‹é‡è½¯ä»¶å¿« 10 å€ï¼Œæ¯”æœ€å…ˆè¿›çš„å¤§åœºæ™¯ NeRF è§£å†³æ–¹æ¡ˆå¿« 50 å€ï¼Œå¹¶ä¸”åœ¨å•ä¸ª GPU ä¸Šå¯ä»¥å®ç°ç›¸ä¼¼çš„æ¸²æŸ“è´¨é‡ã€‚è¿™äº›ç»“æœè¯æ˜äº† BirdNeRF çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åœºæ™¯åˆ†è§£ï¼šå°†å¤§åœºæ™¯å›¾åƒåˆ†è§£æˆå¤šä¸ªå°åœºæ™¯ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå°åœºæ™¯çš„ NeRF æ¨¡å‹ã€‚ï¼ˆ2ï¼‰è§†è§’å§¿åŠ¿åˆ†è§£ï¼šé‡‡ç”¨é¸Ÿç°è§†è§’å§¿åŠ¿åˆ†è§£ç®—æ³•ï¼Œå°†å¤§åœºæ™¯å›¾åƒåˆ†è§£æˆå¤šä¸ªå°åœºæ™¯ã€‚ï¼ˆ3ï¼‰æ–°è§†è§’é‡æ–°æ¸²æŸ“ï¼šæå‡ºä¸€ç§æŠ•å½±å¼•å¯¼çš„æ–°è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å°åœºæ™¯æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäº NeRF çš„å¤§åœºæ™¯é‡å»ºæ–¹æ³• BirdNeRFï¼Œè¯¥æ–¹æ³•é‡‡ç”¨é¸Ÿç°è§†è§’å§¿åŠ¿åˆ†è§£ç®—æ³•å’ŒæŠ•å½±å¼•å¯¼çš„æ–°è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è§£å†³å¤§åœºæ™¯é‡å»ºä»»åŠ¡ä¸­å®¹æ˜“å‡ºç°ä¼ªå½±å’Œä½è§†è§‰ä¿çœŸåº¦çš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„é¸Ÿç°è§†è§’å§¿åŠ¿åˆ†è§£ç®—æ³•ï¼Œå¯ä»¥å°†å¤§åœºæ™¯å›¾åƒåˆ†è§£æˆå¤šä¸ªå°åœºæ™¯ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå°åœºæ™¯çš„ NeRF æ¨¡å‹ï¼Œä»è€Œå‡å°‘è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ï¼Œæé«˜é‡å»ºè´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æŠ•å½±å¼•å¯¼çš„æ–°è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å°åœºæ™¯æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚æ€§èƒ½ï¼š</li><li>BirdNeRF åœ¨ç°æœ‰æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ— äººæœºèˆªæ‹æ•°æ®ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒBirdNeRF çš„é‡å»ºé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ‘„å½±æµ‹é‡è½¯ä»¶å¿« 10 å€ï¼Œæ¯”æœ€å…ˆè¿›çš„å¤§åœºæ™¯ NeRF è§£å†³æ–¹æ¡ˆå¿« 50 å€ï¼Œå¹¶ä¸”åœ¨å•ä¸ª GPU ä¸Šå¯ä»¥å®ç°ç›¸ä¼¼çš„æ¸²æŸ“è´¨é‡ã€‚å·¥ä½œé‡ï¼š</li><li>BirdNeRF çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å•ä¸ª GPU ä¸Šè®­ç»ƒå’Œæ¸²æŸ“ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦å¯¹å¤§åœºæ™¯å›¾åƒè¿›è¡Œåˆ†è§£ï¼Œå› æ­¤ BirdNeRF çš„é¢„å¤„ç†æ—¶é—´å¯èƒ½ä¼šæ¯”è¾ƒé•¿ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-daadce77f0b48dc25dd984f5c66ee7ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6d52642c6cfdc84439f5ea843cff2fd1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-13  BioNeRF Biologically Plausible Neural Radiance Fields for View   Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/</id>
    <published>2024-02-13T11:47:50.000Z</published>
    <updated>2024-02-13T11:47:50.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-13-æ›´æ–°"><a href="#2024-02-13-æ›´æ–°" class="headerlink" title="2024-02-13 æ›´æ–°"></a>2024-02-13 æ›´æ–°</h1><h2 id="GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting"><a href="#GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting" class="headerlink" title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting"></a>GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</h2><p><strong>Authors:Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</strong></p><p>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at <a href="https://gala3d.github.io/">https://gala3d.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.07207v1">PDF</a> </p><p><strong>Summary</strong></p><p>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ç”Ÿæˆåˆå§‹å¸ƒå±€ï¼Œå¹¶å¼•å…¥å¸ƒå±€å¼•å¯¼ 3D é«˜æ–¯è¡¨ç¤ºï¼ŒæŒ‡å¯¼ 3D å†…å®¹ç”Ÿæˆï¼ŒåŒæ—¶æ»¡è¶³é€‚åº”æ€§å‡ ä½•çº¦æŸã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>GALA3D å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¸ƒå±€å¼•å¯¼ 3D é«˜æ–¯è¡¨ç¤ºç›¸ç»“åˆï¼Œç”¨äºæœ‰æ•ˆåœ°è¿›è¡Œæ–‡æœ¬åˆ° 3D çš„ç”Ÿæˆã€‚</li><li>å¸ƒå±€å¼•å¯¼ 3D é«˜æ–¯è¡¨ç¤ºæä¾›äº†è‡ªé€‚åº”çš„å‡ ä½•çº¦æŸï¼Œç¡®ä¿ç”Ÿæˆçš„ 3D å†…å®¹å…·æœ‰çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ã€‚</li><li>GALA3D é‡‡ç”¨å¯¹è±¡-åœºæ™¯ç»„åˆä¼˜åŒ–æœºåˆ¶ï¼Œä»¥ç”Ÿæˆå…·æœ‰çœŸå®å‡ ä½•å½¢çŠ¶ã€çº¹ç†ã€æ¯”ä¾‹å’Œå‡†ç¡®äº¤äº’çš„å¤šå¯¹è±¡ 3D åœºæ™¯ã€‚</li><li>GALA3D å¯ä»¥åŒæ—¶è°ƒæ•´ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–çš„ç²—ç•¥å¸ƒå±€ï¼Œä½¿å…¶ä¸ç”Ÿæˆçš„åœºæ™¯ä¿æŒä¸€è‡´ã€‚</li><li>GALA3D æ˜¯ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¯è¿›è¡Œæœ€å…ˆè¿›çš„åœºæ™¯çº§ 3D å†…å®¹ç”Ÿæˆå’Œå¯æ§ç¼–è¾‘ã€‚</li><li>GALA3D èƒ½å¤Ÿç¡®ä¿åœºæ™¯ä¸­å¯¹è±¡çº§å®ä½“çš„é«˜ä¿çœŸåº¦ã€‚</li><li>GALA3D çš„æºä»£ç å’Œæ¨¡å‹å¯ä» <a href="https://gala3d.github.io/">https://gala3d.github.io/</a> è·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGALA3Dï¼šåŸºäºå¸ƒå±€å¼•å¯¼çš„æ–‡æœ¬åˆ° 3D å¤æ‚åœºæ™¯ç”Ÿæˆ</li><li>ä½œè€…ï¼šXiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¸‡é€‰ä¿¡æ¯æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€ç”Ÿæˆå¼é«˜æ–¯ä½“ç´ ã€å¸ƒå±€å¼•å¯¼ã€æ¡ä»¶æ‰©æ•£</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.07207    Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ° 3D ç”Ÿæˆæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆé€¼çœŸçš„ 3D åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆäº§ç”Ÿä½è´¨é‡çš„çº¹ç†ã€è§†è§‰ä¼ªå½±å’Œå‡ ä½•å¤±çœŸï¼Œè¦ä¹ˆæ— æ³•æ ¹æ®æ–‡æœ¬å‡†ç¡®ç”Ÿæˆå¤šä¸ªå¯¹è±¡åŠå…¶äº¤äº’ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºä½“ç´ çš„æ–¹æ³•å’ŒåŸºäºç½‘æ ¼çš„æ–¹æ³•ã€‚åŸºäºä½“ç´ çš„æ–¹æ³•è™½ç„¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D åœºæ™¯ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åŸºäºç½‘æ ¼çš„æ–¹æ³•è™½ç„¶è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œä½†ç”Ÿæˆçš„ 3D åœºæ™¯è´¨é‡è¾ƒå·®ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼é«˜æ–¯ä½“ç´ çš„æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆå§‹å¸ƒå±€ï¼Œç„¶åå¼•å…¥å¸ƒå±€å¼•å¯¼çš„ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºæ¥ç”Ÿæˆ 3D å†…å®¹ã€‚æ¥ç€ï¼Œæå‡ºäº†ä¸€ç§å¯¹è±¡åœºæ™¯ç»„åˆä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¥ååŒç”Ÿæˆå…·æœ‰çœŸå®å‡ ä½•å½¢çŠ¶ã€çº¹ç†ã€æ¯”ä¾‹å’Œå‡†ç¡®äº¤äº’çš„å¤šå¯¹è±¡ 3D åœºæ™¯ï¼ŒåŒæ—¶è°ƒæ•´ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–çš„ç²—ç•¥å¸ƒå±€å…ˆéªŒï¼Œä½¿å…¶ä¸ç”Ÿæˆçš„åœºæ™¯å¯¹é½ã€‚(4) å®éªŒç»“æœï¼šå®éªŒè¡¨æ˜ï¼ŒGALA3D æ˜¯ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå¯ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„ 3D åœºæ™¯ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ”¯æŒäº¤äº’å¼å¯æ§ç¼–è¾‘ã€‚</li></ol><p>Methods:</p><p>(1) ç²—ç•¥å¸ƒå±€å…ˆéªŒï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»æ–‡æœ¬æè¿°ä¸­æå–ç²—ç•¥å¸ƒå±€å…ˆéªŒï¼ŒåŒ…æ‹¬å¯¹è±¡å®ä¾‹åŠå…¶å¯¹åº”çš„ä½ç½®ã€å°ºå¯¸å’Œæ–¹å‘ã€‚</p><p>(2) å¸ƒå±€å¼•å¯¼çš„é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼šå°†ç²—ç•¥å¸ƒå±€å…ˆéªŒè½¬æ¢ä¸ºå¸ƒå±€å¼•å¯¼çš„é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è±¡å®ä¾‹ç”±ä¸€ç»„é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œé«˜æ–¯ä½“ç´ çš„ä½ç½®ã€å°ºå¯¸å’Œæ–¹å‘ç”±å¸ƒå±€å…ˆéªŒå†³å®šã€‚</p><p>(3) è‡ªé€‚åº”å‡ ä½•æ§åˆ¶ï¼šå¯¹é«˜æ–¯ä½“ç´ çš„åˆ†å¸ƒå’Œå½¢çŠ¶è¿›è¡Œè‡ªé€‚åº”å‡ ä½•æ§åˆ¶ï¼Œä»¥ç¡®ä¿é«˜æ–¯ä½“ç´ çš„åˆ†å¸ƒç´§å¯†è´´åˆå¯¹è±¡è¡¨é¢ï¼Œå¹¶ä¸”å½¢çŠ¶æ›´åŠ è§„åˆ™å’Œä¸€è‡´ã€‚</p><p>(4) å…·æœ‰æ‰©æ•£å…ˆéªŒçš„ç»„åˆä¼˜åŒ–ï¼šé‡‡ç”¨å…·æœ‰æ‰©æ•£å…ˆéªŒçš„ç»„åˆä¼˜åŒ–ç­–ç•¥æ¥æ›´æ–°å¸ƒå±€å¼•å¯¼çš„é«˜æ–¯ä½“ç´ å‚æ•°ï¼ŒåŒ…æ‹¬å¤šè§†å›¾æ‰©æ•£ä¼˜åŒ–å’Œåœºæ™¯æ¡ä»¶æ‰©æ•£ä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆå…·æœ‰ç»Ÿä¸€é£æ ¼å’Œäº¤äº’å…³ç³»çš„å¯¹è±¡å®ä¾‹ã€‚</p><p>(5) å¸ƒå±€æŸå¤±ï¼šå¼•å…¥å¸ƒå±€æŸå¤±æ¥ç¡®ä¿ç”Ÿæˆçš„3Dåœºæ™¯ä¸å¸ƒå±€å…ˆéªŒåœ¨è¯­ä¹‰å’Œç©ºé—´ä¸Šçš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜åœºæ™¯çš„æ•´ä½“è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šGALA3D æ˜¯ä¸€ç§åŸºäºç”Ÿæˆå¼å¸ƒå±€å¼•å¯¼çš„ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºçš„åœºæ™¯çº§æ–‡æœ¬åˆ° 3D æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆå…·æœ‰å¤šä¸ªå¯¹è±¡çš„é«˜ä¿çœŸã€3D ä¸€è‡´çš„åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç¤ºäº†ç”Ÿæˆå…·æœ‰å¤šä¸ªå¯¹è±¡å’Œäº¤äº’çš„å¤æ‚åœºæ™¯çš„èƒ½åŠ›ï¼Œå¹¶å®ç°äº†å‡ºè‰²çš„çº¹ç†å’Œå‡ ä½•æ•ˆæœã€‚è¯¥æ–¹æ³•è¿˜ä¿ƒè¿›äº†äº¤äº’å¼å’Œå¯æ§çš„åœºæ™¯ç¼–è¾‘ï¼Œå®ç°äº†ä¸€ä¸ªé«˜æ•ˆä¸”ç”¨æˆ·å‹å¥½çš„ 3D åœºæ™¯ç”Ÿæˆå’Œç¼–è¾‘æ¡†æ¶ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼å¸ƒå±€å¼•å¯¼çš„ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå¯ä»¥ç”Ÿæˆå…·æœ‰ç»Ÿä¸€é£æ ¼å’Œäº¤äº’å…³ç³»çš„å¯¹è±¡å®ä¾‹ã€‚</li><li>å¼•å…¥äº†ä¸€ç§å…·æœ‰æ‰©æ•£å…ˆéªŒçš„ç»„åˆä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æ›´æ–°å¸ƒå±€å¼•å¯¼çš„é«˜æ–¯ä½“ç´ å‚æ•°ï¼Œä»¥ç”Ÿæˆå…·æœ‰çœŸå®å‡ ä½•å½¢çŠ¶ã€çº¹ç†ã€æ¯”ä¾‹å’Œå‡†ç¡®äº¤äº’çš„å¤šå¯¹è±¡ 3D åœºæ™¯ã€‚</li><li>æå‡ºäº†ä¸€ç§å¸ƒå±€æŸå¤±ï¼Œè¯¥æŸå¤±å¯ä»¥ç¡®ä¿ç”Ÿæˆçš„ 3D åœºæ™¯ä¸å¸ƒå±€å…ˆéªŒåœ¨è¯­ä¹‰å’Œç©ºé—´ä¸Šçš„ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜åœºæ™¯çš„æ•´ä½“è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>èƒ½å¤Ÿæ”¯æŒäº¤äº’å¼å¯æ§ç¼–è¾‘ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-3dde3c6bf6237679d7dc8e3a25b014e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c780f9b8f1b542e9c562c2d185d7e16a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e76b694075c9297c3e8a8d38bf4c8fe3.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the objectâ€™s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v1">PDF</a> 6-page technical report</p><p><strong>Summary</strong><br>3D é«˜æ–¯æ›²é¢è¡¨ç¤ºå¢å¼ºå¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œ ä¿ƒè¿›å›¾åƒã€è¯­è¨€å’Œ 3D è¡¨ç¤ºçš„ç»Ÿä¸€ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¤šæ¨¡æ€é¢„è®­ç»ƒåœ¨å›¾åƒå’Œè¯­è¨€æè¿°çš„å¯¹é½æ–¹é¢å–å¾—è¿›å±•ï¼Œå¯¹ç‰©ä½“è¯†åˆ«ã€åˆ†ç±»å’Œæ£€ç´¢è‡³å…³é‡è¦ã€‚</li><li>ç‚¹äº‘çš„ç¦»æ•£è¡¨ç¤ºä¸¢å¤±äº†ç‰©ä½“çš„è¡¨é¢å½¢çŠ¶ä¿¡æ¯ï¼Œå¯¼è‡´æ¸²æŸ“ç»“æœä¸ 2D å¯¹åº”å…³ç³»ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li><li>æå‡º GS-CLIP é¦–æ¬¡å°† 3DGSï¼ˆ3D é«˜æ–¯æ›²é¢ï¼‰å¼•å…¥å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œä»¥å¢å¼º 3D è¡¨ç¤ºã€‚</li><li>GS-CLIP åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å¤§è§„æ¨¡çœŸå®ä¸–ç•Œå›¾åƒæ–‡æœ¬å¯¹ä¸Šå­¦ä¹ å…±åŒçš„è§†è§‰å’Œæ–‡æœ¬ç©ºé—´ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ª 3D ç¼–ç å™¨æ¥å¯¹é½é’ˆå¯¹æ¯ä¸ªå¯¹è±¡ä¼˜åŒ–çš„ 3DGSã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ Gaussian-Aware Fusion æ¥æå–å’Œèåˆå…¨å±€æ˜¾å¼ç‰¹å¾ã€‚</li><li>ä½œä¸ºè¯­è¨€å›¾åƒ 3D é¢„è®­ç»ƒçš„é€šç”¨æ¡†æ¶ï¼ŒGS-CLIP ä¸ 3D ä¸»å¹²ç½‘ç»œæ— å…³ã€‚</li><li>å…·æœ‰æŒ‘æˆ˜æ€§çš„å®éªŒè¡¨æ˜ï¼ŒGS-CLIP æ˜¾ç€æ”¹å–„äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œä¼˜äºå…ˆå‰æœ€å¥½çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šGS-CLIPï¼šç”¨äºå¯¹æ¯”è¯­è¨€-å›¾åƒ-3D é¢„è®­ç»ƒçš„é«˜æ–¯æº…å°„</li><p></p><p></p><li>ä½œè€…ï¼šææµ©æºã€å‘¨å½¦é¹ã€æ›¾ä¹‰æ¶µã€è®¸èˆªã€æ¢æ™“ä¸¹</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å±±å¤§å­¦æ·±åœ³æ ¡åŒº</li><p></p><p></p><li>å…³é”®è¯ï¼š3D è¡¨ç¤ºã€é«˜æ–¯æº…å°„ã€å¯¹æ¯”å­¦ä¹ ã€å¤šæ¨¡æ€é¢„è®­ç»ƒ</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.06198ï¼ŒGithub é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D å½¢çŠ¶ä»¥ç‚¹äº‘è¡¨ç¤ºåœ¨å¤šæ¨¡æ€é¢„è®­ç»ƒä¸­å–å¾—äº†è¿›å±•ï¼Œç”¨äºå¯¹é½å›¾åƒå’Œè¯­è¨€æè¿°ï¼Œè¿™å¯¹äºç‰©ä½“è¯†åˆ«ã€åˆ†ç±»å’Œæ£€ç´¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç‚¹äº‘çš„ç¦»æ•£è¡¨ç¤ºä¸¢å¤±äº†ç‰©ä½“çš„è¡¨é¢å½¢çŠ¶ä¿¡æ¯ï¼Œå¹¶åœ¨æ¸²æŸ“ç»“æœå’Œ 2D å¯¹åº”å…³ç³»ä¹‹é—´äº§ç”Ÿå·®è·ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰çš„ 3D è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦å¯¹ç‚¹äº‘çš„å…³é”®ç‚¹ä½ç½®ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œè¿™é™åˆ¶äº† 3D è§†è§‰ç†è§£å’Œ 3D è¡¨ç¤ºå­¦ä¹ çš„æ€§èƒ½ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† GS-CLIPï¼Œå°† 3D é«˜æ–¯æº…å°„ (3DGS) å¼•å…¥å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œä»¥å¢å¼º 3D è¡¨ç¤ºã€‚GS-CLIP åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹ä¸Šå­¦ä¹ ä¸€ä¸ªå…±åŒçš„è§†è§‰å’Œæ–‡æœ¬ç©ºé—´ï¼Œç„¶åå­¦ä¹ ä¸€ä¸ª 3D ç¼–ç å™¨ï¼Œç”¨äºå¯¹é½é’ˆå¯¹æ¯ä¸ªå¯¹è±¡ä¼˜åŒ–çš„ 3DGSã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ„ŸçŸ¥èåˆï¼Œç”¨äºæå–å’Œèåˆå…¨å±€æ˜¾å¼ç‰¹å¾ã€‚(4)ï¼šå®éªŒç»“æœï¼šåœ¨ SUN-RGBD æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-CLIP åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„é›¶æ ·æœ¬/å¼€æ”¾è¯å­¦ä¹ ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DGS åœ¨è·¨æ¨¡æ€å­¦ä¹ ä¸­å…·æœ‰å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li><br>&lt;/ol&gt;<p></p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰è·¨æ¨¡æ€é¢„è®­ç»ƒï¼šä¸ºäº†å¯¹é½æ–‡æœ¬ã€å›¾åƒå’Œ3DGSçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼ŒGS-CLIPé‡‡ç”¨é¢„è®­ç»ƒçš„è¯­è¨€-å›¾åƒæ¨¡å‹CLIPï¼Œå½¢æˆä¸€ä¸ªå…±åŒçš„è¯­è¨€-å›¾åƒæ½œåœ¨ç©ºé—´ï¼Œä½œä¸º3DGSçš„ç›®æ ‡æ½œåœ¨ç©ºé—´ã€‚å¯¹äºé›¶æ ·æœ¬/å¼€æ”¾è¯è¯†åˆ«ï¼Œé€šè¿‡å†»ç»“CLIPæ–‡æœ¬ç¼–ç å™¨ã€å›¾åƒç¼–ç å™¨å’Œå…¬å…±çœŸå®ä¸–ç•Œæ½œåœ¨ç©ºé—´ï¼Œä¿è¯äº†3DGSè¡¨ç¤ºçš„å¯è¿ç§»æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å€Ÿé‰´äº†[19, 28]ä¸­çš„å¯¹æ¯”æŸå¤±ï¼Œå¹¶å½¢æˆæ–‡æœ¬-3DGSå¯¹é½å’Œå›¾åƒ-3DGSå¯¹é½ï¼Œç”¨äºå¤šæ¨¡æ€å¯¹é½ã€‚ï¼ˆ2ï¼‰é«˜æ–¯æ„ŸçŸ¥èåˆï¼šè™½ç„¶å°†ç‚¹äº‘æŠ•å½±åˆ°3Dä½“ç´ çš„3Déª¨å¹²å¯ä»¥æ›´å¥½åœ°å­¦ä¹ å…¨å±€ä½ç½®å’Œç‰¹å¾ï¼Œä½†æˆ‘ä»¬å‘ç°3DGSçš„æ˜¾å¼ç‰¹å¾ä¼šè¢«å¿½ç•¥ï¼Œå› ä¸ºä½“ç´ åŒ–ä¸¢å¤±äº†å½¢çŠ¶å’Œçº¹ç†ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºTransformerçš„åˆ†æ”¯ç›´æ¥å¯¹é«˜æ–¯ç‰¹å¾å»ºæ¨¡ä¸ºé«˜æ–¯ç‰¹å¾ä¸Šä¸‹æ–‡ï¼Œå¹¶ä»¥æ®‹å·®å½¢å¼æ³¨å…¥å®ƒã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šå…·æœ‰nä¸ªé«˜æ–¯çš„3DGSè¾“å…¥XGâˆˆRnÃ—14ï¼Œæˆ‘ä»¬é¦–å…ˆå°†XGåˆ†æˆNgç»„ï¼Œç”¨äºXgroupG=Ngï¿½g=1XgGï¼Œç„¶åä½¿ç”¨åŸºäºå·ç§¯çš„ä½“ç³»ç»“æ„EGÎ¸,cæå–å…¨å±€ç‰¹å¾fGcå’ŒåŸºäºTransformerçš„ä½“ç³»ç»“æ„EGÎ¸,tæå–æ˜¾å¼é«˜æ–¯ç‰¹å¾fGGï¼Œæœ€åå°†fGcå’ŒfGGè¿æ¥èµ·æ¥ï¼Œå½¢æˆæœ€ç»ˆçš„3DGSè¡¨ç¤ºfGã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æ„ä¹‰ï¼šæœ¬æ–‡é¦–æ¬¡æå‡º GS-CLIPï¼Œå°† 3DGS çº³å…¥è·¨æ¨¡æ€å­¦ä¹ ï¼Œä½œä¸ºè¡¥å……å½¢çŠ¶å’Œçº¹ç†ä¿¡æ¯çš„é€šç”¨ 3D è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é«˜æ–¯æ„ŸçŸ¥èåˆï¼Œä»¥ä¾¿ä»è¡¥å……ä¿¡æ¯ä¸­æ›´å¥½åœ°å­¦ä¹ ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ GS-CLIP åœ¨æœ€å…ˆè¿›çš„æ–¹æ³•ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å°† 3DGS å¼•å…¥è·¨æ¨¡æ€å­¦ä¹ ï¼Œä½œä¸ºè¡¥å……å½¢çŠ¶å’Œçº¹ç†ä¿¡æ¯çš„é€šç”¨ 3D è¡¨ç¤ºã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜æ–¯æ„ŸçŸ¥èåˆï¼Œä»¥ä¾¿ä»è¡¥å……ä¿¡æ¯ä¸­æ›´å¥½åœ°å­¦ä¹ ä¿¡æ¯ã€‚</li></ol><p>æ€§èƒ½ï¼š- åœ¨ SUN-RGBD æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-CLIP åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„é›¶æ ·æœ¬/å¼€æ”¾è¯å­¦ä¹ ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼š- éœ€è¦å¯¹ 3DGS è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚- éœ€è¦å¯¹é«˜æ–¯æ„ŸçŸ¥èåˆè¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-13  GALA3D Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/</id>
    <published>2024-02-13T11:37:33.000Z</published>
    <updated>2024-02-13T11:37:33.323Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-13-æ›´æ–°"><a href="#2024-02-13-æ›´æ–°" class="headerlink" title="2024-02-13 æ›´æ–°"></a>2024-02-13 æ›´æ–°</h1><h2 id="DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer"><a href="#DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer" class="headerlink" title="DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer"></a>DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer</h2><p><strong>Authors:Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</strong></p><p>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel. </p><p><a href="http://arxiv.org/abs/2402.05712v1">PDF</a> 9 pages, 5 figures. Code is avalable at   <a href="https://github.com/theEricMa/DiffSpeaker">https://github.com/theEricMa/DiffSpeaker</a></p><p><strong>Summary</strong><br>é€šè¿‡æå‡ºå¸¦åæ¡ä»¶æ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬è§£å†³äº†éŸ³è§†é¢‘é…å¯¹æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œåœ¨ä¿æŒéŸ³è§†é¢‘åŒæ­¥çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„é¢éƒ¨åŠ¨ç”»ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ã€‚</li><li>ä½¿ç”¨å¸¦åæ¡ä»¶æ³¨æ„åŠ›æ¨¡å—ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†éŸ³è§†é¢‘é…å¯¹æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚</li><li>åœ¨ç°æœ‰åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>å¯ä»¥å¿«é€Ÿç”Ÿæˆé¢éƒ¨åŠ¨ç”»ï¼Œæ¨ç†é€Ÿåº¦å¿«ã€‚</li><li>å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆéè¯­è¨€é¢éƒ¨è¡¨æƒ…ã€‚</li><li>å¯ä»¥æ§åˆ¶åŠ¨ç”»è¿‡ç¨‹ä¸­çš„å˜´å‹åŒæ­¥å’Œéè¯­è¨€é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„æƒè¡¡ã€‚</li><li>è¯¥æ¨¡å‹å¯ä»¥ç”¨äºå„ç§å¤šåª’ä½“åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šDiffSpeakerï¼šåŸºäºæ‰©æ•£å˜æ¢å™¨çš„è¯­éŸ³é©±åŠ¨ 3D é¢éƒ¨åŠ¨ç”»ï¼ˆDiffSpeaker: Speech-Driven 3DFacial Animation with Diffusion Transformerï¼‰</li><li>ä½œè€…ï¼šZhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ç†å·¥å¤§å­¦ï¼ˆé¦™æ¸¯ç†å·¥å¤§å­¦ï¼‰</li><li>å…³é”®è¯ï¼šè¯­éŸ³é©±åŠ¨é¢éƒ¨åŠ¨ç”»ã€æ‰©æ•£æ¨¡å‹ã€Transformerã€æ¡ä»¶æ³¨æ„æœºåˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.05712Github ä»£ç é“¾æ¥ï¼šhttps://github.com/theEricMa/DiffSpeaker</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šè¯­éŸ³é©±åŠ¨ 3D é¢éƒ¨åŠ¨ç”»åœ¨è®¸å¤šå¤šåª’ä½“åº”ç”¨ä¸­éå¸¸é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹æˆ– Transformer æ¶æ„æ¥æ‰§è¡Œæ­¤ä»»åŠ¡å¾ˆæœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ç®€å•èšåˆå¹¶ä¸èƒ½å¸¦æ¥æ”¹è¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ€€ç–‘è¿™æ˜¯ç”±äºç¼ºä¹é…å¯¹çš„éŸ³é¢‘-4D æ•°æ®ï¼Œè¿™å¯¹äº Transformer åœ¨æ‰©æ•£æ¡†æ¶å†…æœ‰æ•ˆåœ°æ‰§è¡Œå»å™ªå™¨è‡³å…³é‡è¦ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨åŸºæœ¬çš„æ»‘åŠ¨çª—å£æ–¹æ³•å¤„ç†éŸ³é¢‘è¾“å…¥ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆçš„çš„é¢éƒ¨åŠ¨ä½œèŒƒå›´ç‹­çª„ã€‚è¿‘å¹´æ¥ï¼Œç ”ç©¶äººå‘˜å¼€å§‹é‡‡ç”¨ Transformer æ¶æ„æ¥è¿›è¡Œè¯­éŸ³é©±åŠ¨ 3D é¢éƒ¨åŠ¨ç”»ï¼Œä½†ä¼ ç»Ÿçš„ç¡®å®šæ€§å›å½’å¯èƒ½ä¸æ˜¯æœ€å¥½çš„æ–¹æ³•ï¼Œå› ä¸ºäººç±»çš„è¯­éŸ³å’Œé¢éƒ¨è¡¨æƒ…æ˜¯å¯å˜ä¸”åŠ¨æ€çš„ï¼Œå¾ˆéš¾ç”¨ä¸€ä¸ªå›ºå®šçš„æ˜ å°„æ¥å‡†ç¡®æ•æ‰å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚ï¼ˆ3ï¼‰ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† DiffSpeakerï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Transformer çš„ç½‘ç»œï¼Œé…å¤‡äº†æ–°é¢–çš„åç½®æ¡ä»¶æ³¨æ„æœºåˆ¶æ¨¡å—ã€‚è¿™äº›æ¨¡å—å¯ä»¥æ›¿ä»£æ ‡å‡† Transformer ä¸­ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›/äº¤å‰æ³¨æ„åŠ›ï¼Œå¹¶ç»“åˆäº†ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„åç½®ï¼Œè¿™äº›åç½®å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶é›†ä¸­åœ¨ç›¸å…³çš„ç‰¹å®šä»»åŠ¡å’Œä¸æ‰©æ•£ç›¸å…³çš„æ¡ä»¶ä¸Šã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†åœ¨æ‰©æ•£èŒƒå¼ä¸­å‡†ç¡®çš„å”‡å½¢åŒæ­¥å’Œéè¯­è¨€é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„æƒè¡¡ã€‚ï¼ˆ4ï¼‰ï¼šå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨ç°æœ‰åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”ç”±äºå…¶èƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆé¢éƒ¨åŠ¨ä½œï¼Œå› æ­¤æ¨ç†é€Ÿåº¦ä¹Ÿå¾ˆå¿«ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹å’ŒTransformeræ¶æ„çš„è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»æ–¹æ³•DiffSpeakerï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’ŒTransformeræ¶æ„çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ä»»åŠ¡ä¸­çš„é…å¯¹éŸ³é¢‘-4Dæ•°æ®ç¼ºä¹çš„é—®é¢˜ã€‚(2): DiffSpeakeré‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„åç½®æ¡ä»¶æ³¨æ„æœºåˆ¶æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ›¿ä»£æ ‡å‡†Transformerä¸­çš„ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›/äº¤å‰æ³¨æ„åŠ›ï¼Œå¹¶ç»“åˆäº†ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„åç½®ï¼Œè¿™äº›åç½®å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶é›†ä¸­åœ¨ç›¸å…³çš„ç‰¹å®šä»»åŠ¡å’Œä¸æ‰©æ•£ç›¸å…³çš„æ¡ä»¶ä¸Šã€‚(3): DiffSpeakerè¿˜æ¢ç´¢äº†åœ¨æ‰©æ•£èŒƒå¼ä¸­å‡†ç¡®çš„å”‡å½¢åŒæ­¥å’Œéè¯­è¨€é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°å¯ä»¥åŒæ—¶ä¼˜åŒ–å”‡å½¢åŒæ­¥å’Œéè¯­è¨€é¢éƒ¨è¡¨æƒ…çš„è´¨é‡ã€‚(4): å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffSpeakeråœ¨ç°æœ‰åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ç”±äºå…¶èƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆé¢éƒ¨åŠ¨ä½œï¼Œå› æ­¤æ¨ç†é€Ÿåº¦ä¹Ÿå¾ˆå¿«ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæ¢ç´¢äº†å°† Transformer æ¶æ„ä¸åŸºäºæ‰©æ•£çš„æ¡†æ¶æœ‰æ•ˆç»“åˆç”¨äºè¯­éŸ³é©±åŠ¨ 3D é¢éƒ¨åŠ¨ç”»çš„æ–¹æ³•ã€‚æˆ‘ä»¬è´¡çŒ®çš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†å¸¦æœ‰åç½®çš„æ¡ä»¶è‡ªæ³¨æ„åŠ›/äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è§£å†³äº†ä½¿ç”¨å—é™ä¸”è·¨åº¦çŸ­çš„éŸ³é¢‘-4D æ•°æ®è®­ç»ƒåŸºäºæ‰©æ•£çš„ Transformer çš„å›°éš¾ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†åœ¨å®ç°å‡†ç¡®çš„å”‡å½¢åŒæ­¥å’Œç”Ÿæˆä¸è¯­éŸ³ç›¸å…³æ€§è¾ƒå°çš„é¢éƒ¨è¡¨æƒ…ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬å¼€å‘çš„æ¨¡å‹ä¼˜äºå½“å‰çš„æ–¹æ³•ï¼Œåœ¨åŠ¨ç”»è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¸¦æœ‰åç½®çš„æ¡ä»¶è‡ªæ³¨æ„åŠ›/äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å—é™ä¸”è·¨åº¦çŸ­çš„éŸ³é¢‘-4D æ•°æ®æ¥è®­ç»ƒåŸºäºæ‰©æ•£çš„ Transformerã€‚æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°å¯ä»¥åŒæ—¶ä¼˜åŒ–å”‡å½¢åŒæ­¥å’Œéè¯­è¨€é¢éƒ¨è¡¨æƒ…çš„è´¨é‡ã€‚æ€§èƒ½ï¼šåœ¨ç°æœ‰åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ¨ç†é€Ÿåº¦å¿«ï¼Œèƒ½å¤Ÿå¹¶è¡Œç”Ÿæˆé¢éƒ¨åŠ¨ä½œã€‚å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡çš„éŸ³é¢‘-4D æ•°æ®ã€‚éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¤§é‡çš„è®­ç»ƒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a6a1095c49476b6d0a24c660e7abca7e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2c694a105e50cf1ba9a9e0743f793c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dd59be8351677e84215dd037093b2ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ebb8f9ab10ef4d053668941b0c247fcb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff2d1ca9215127e6894689d494fb8244.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-13  DiffSpeaker Speech-Driven 3D Facial Animation with Diffusion   Transformer</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/</id>
    <published>2024-02-13T11:31:26.000Z</published>
    <updated>2024-02-13T11:31:26.322Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-13-æ›´æ–°"><a href="#2024-02-13-æ›´æ–°" class="headerlink" title="2024-02-13 æ›´æ–°"></a>2024-02-13 æ›´æ–°</h1><h2 id="Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models"><a href="#Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models" class="headerlink" title="Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models"></a>Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</h2><p><strong>Authors:Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah</strong></p><p>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions. </p><p><a href="http://arxiv.org/abs/2402.06969v1">PDF</a> Submitted in IEEE EMBC 2024 Conference</p><p><strong>Summary</strong><br>ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒæ•°æ®åˆæˆä¸­å±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œæœ‰æœ›è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŠ©åŠ›å¿ƒè¡€ç®¡å›¾åƒå¤„ç†é¢†åŸŸçš„å‘å±•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒæ•°æ®åˆæˆä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¯ç”¨äºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li><li>é€šè¿‡å¾®è°ƒç”¨æˆ·å®šä¹‰æ–‡æœ¬æç¤ºçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œä»…ä½¿ç”¨æœ‰é™æ•°é‡çš„ CTA å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå³å¯ç”Ÿæˆåˆæˆçš„å† çŠ¶åŠ¨è„‰ CTA å›¾åƒã€‚</li><li>å®šé‡åˆ†æå’Œå®šæ€§è¯„ä¼°ç›¸ç»“åˆçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒ (T2I) ç¨³å®šæ‰©æ•£æ¨¡å‹å¯ä»¥æˆåŠŸç”Ÿæˆå¿ƒè„ CTA å›¾åƒã€‚</li><li>å¾®è°ƒçš„ T2I CTA æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ€¥æ€§ B å‹ä¸»åŠ¨è„‰å¤¹å±‚ (TBAD) åŒ»å­¦ç‰¹å¾çš„å›¾åƒã€‚</li><li>åˆæˆçš„å›¾åƒåœ¨è§†è§‰ä¸Šä¸çœŸå®å›¾åƒç›¸ä¼¼ï¼Œå¹¶ä¿ç•™äº†çœŸå®å›¾åƒä¸­çš„å…³é”®è§£å‰–ç»“æ„ã€‚</li><li>ä¸´åºŠåŒ»ç”Ÿè®¤ä¸ºåˆæˆçš„å›¾åƒå…·æœ‰è¶³å¤Ÿçš„è´¨é‡ï¼Œå¯ç”¨äºä¸´åºŠå®è·µã€‚</li><li>è¯¥ç ”ç©¶è¡¨æ˜ï¼Œç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦æˆåƒæ•°æ®åˆæˆä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œæœ‰æœ›æ”¹å–„å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹åˆæˆ B å‹ä¸»åŠ¨è„‰å¤¹å±‚æ–­å±‚æ‰«æå›¾åƒæ•°æ®</li><li>ä½œè€…ï¼šAyman Abaidã€Muhammad Ali Farooqã€Niamh Hynesã€Peter Corcoran å’Œ Ihsan Ullah</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šçˆ±å°”å…°æˆˆå°”éŸ¦å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢</li><li>å…³é”®è¯ï¼šä¸»åŠ¨è„‰å¤¹å±‚ã€è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½±ã€åŒ»å­¦å›¾åƒåˆæˆã€ç¨³å®šæ‰©æ•£æ¨¡å‹ã€æ–‡æœ¬åˆ°å›¾åƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.06969Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¸»åŠ¨è„‰å¤¹å±‚æ˜¯ä¸€ç§ä¸¥é‡çš„å¿ƒè¡€ç®¡ç–¾ç—…ï¼Œéœ€è¦å‡†ç¡®å’ŒåŠæ—¶çš„è¯Šæ–­ã€‚è®¡ç®—æœºæ–­å±‚æ‰«æè¡€ç®¡é€ å½± (CTA) æ˜¯è¯Šæ–­ä¸»åŠ¨è„‰å¤¹å±‚æœ€å¸¸ç”¨çš„æˆåƒæ–¹å¼ï¼Œä½†ç”±äºæ•°æ®ç¨€ç¼ºï¼Œæœºå™¨å­¦ä¹ ç®—æ³•åœ¨ä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒå¤„ç†ä¸­çš„èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è‡ªåŠ¨åˆ†å‰²ä¸»å‹•è„ˆå¤¾å±¤åœ–åƒä¸­çš„çœŸè…”ã€å‡è…”å’Œå‡è…”è¡€æ “ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸»åŠ¨è„‰å¤¹å±‚çš„æ•°æ®é›†å¾€å¾€å¾ˆå°ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹åˆæˆä¸»åŠ¨è„‰å¤¹å±‚ CTA å›¾åƒçš„æ–¹æ³•ã€‚ç¨³å®šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚æœ¬ç ”ç©¶é€šè¿‡å¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æç¤ºç”Ÿæˆä¸»åŠ¨è„‰å¤¹å±‚ CTA å›¾åƒã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„ç¨³å®šæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸»åŠ¨è„‰å¤¹å±‚å…¸å‹ç‰¹å¾çš„å›¾åƒã€‚å®šé‡åˆ†æå’Œå®šæ€§è¯„ä¼°éƒ½è¡¨æ˜ï¼Œåˆæˆçš„å›¾åƒå…·æœ‰å¾ˆé«˜çš„è´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒåˆ†å‰²ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æ•°æ®é¢„å¤„ç†ï¼šå°† 3D CTA å›¾åƒè½¬æ¢ä¸º 2D å›¾åƒï¼Œå¹¶å°†å…¶åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€æµ‹è¯•é›†å’ŒéªŒè¯é›†ã€‚å°†æ•°æ®åˆ†ä¸ºäº”ç±»ï¼šæœ‰çœŸè…” (TL) çš„å›¾åƒã€æœ‰å‡è…” (FL) çš„å›¾åƒã€æœ‰å‡è…”è¡€æ “ (FLT) çš„å›¾åƒã€æœ‰ TL å’Œ FL çš„å›¾åƒï¼Œä»¥åŠæ—  TLã€FL å’Œ FLT ä¿¡æ¯çš„æ•°æ®ã€‚(2) æ–‡æœ¬åˆ°å›¾åƒ (T2I) æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨ ImageTBAD æ•°æ®é›†å’Œ DreamBooth è®­ç»ƒå·¥å…·å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„ CTA æ•°æ®ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸ºæ¯ç±»æ•°æ®åˆ†é…ä¸“é—¨çš„æ–‡æœ¬æç¤ºï¼Œå¹¶ä¸ºåç»­ç±»åˆ«çš„ç‰¹å®šç±»æä¾›å¦å®šæç¤ºã€‚(3) å›¾åƒé‡‡æ ·ï¼šä½¿ç”¨æ¬§æ‹‰å’Œæ¬§æ‹‰ A å›¾åƒé‡‡æ ·å™¨ä»æ½œç©ºé—´çš„ä¸åŒåŒºåŸŸé‡‡æ ·ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–å’Œé€¼çœŸçš„å›¾åƒã€‚(4) æ•°æ®å¢å¼ºï¼šä½¿ç”¨ç‹¬ç‰¹çš„æ–‡æœ¬æç¤ºæ¸²æŸ“å…·æœ‰ç±»åˆ«åˆ†å¸ƒçš„ CT æ•°æ®ï¼Œä»¥å¢å¼ºå…·æœ‰ç‰¹å®š CT ç‰¹å¾çš„æ•°æ®ï¼Œä¾‹å¦‚ TLã€FL å’Œ FLTã€‚(5) æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨ FrÃ©chet Inception Distance (FID) å’Œ Multiscale Structural Similarity Index Measure (MS-SSIM) è¯„ä¼°åˆæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚è®­ç»ƒ SoTA æ¨¡å‹ï¼ˆä¾‹å¦‚ UNetï¼‰å¯¹åˆæˆå›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œä»¥è¯„ä¼°å…¶å®ç”¨æ€§ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹åˆæˆä¸»åŠ¨è„‰å¤¹å±‚ CTA å›¾åƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸»åŠ¨è„‰å¤¹å±‚å…¸å‹ç‰¹å¾çš„å›¾åƒï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒåˆ†å‰²ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹åˆæˆä¸»åŠ¨è„‰å¤¹å±‚ CTA å›¾åƒï¼Œè¿™æ˜¯é¦–æ¬¡å°†ç¨³å®šæ‰©æ•£æ¨¡å‹åº”ç”¨äºä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒåˆæˆã€‚</li><li>é€šè¿‡å¾®è°ƒç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æç¤ºç”Ÿæˆä¸»åŠ¨è„‰å¤¹å±‚ CTA å›¾åƒï¼Œè¿™ä½¿å¾—å›¾åƒåˆæˆè¿‡ç¨‹æ›´åŠ çµæ´»å’Œå¯æ§ã€‚</li><li>åˆæˆçš„å›¾åƒå…·æœ‰å¾ˆé«˜çš„è´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒåˆ†å‰²ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚æ€§èƒ½ï¼š</li><li>å®šé‡åˆ†æå’Œå®šæ€§è¯„ä¼°éƒ½è¡¨æ˜ï¼Œåˆæˆçš„å›¾åƒå…·æœ‰å¾ˆé«˜çš„è´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œä¸»åŠ¨è„‰å¤¹å±‚å›¾åƒåˆ†å‰²ã€‚</li><li>è®­ç»ƒçš„ SoTA æ¨¡å‹ï¼ˆä¾‹å¦‚ UNetï¼‰å¯¹åˆæˆå›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œè·å¾—äº†è‰¯å¥½çš„åˆ†å‰²ç²¾åº¦ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•åˆæˆçš„å›¾åƒå…·æœ‰å¾ˆé«˜çš„å®ç”¨æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹ç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™éœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œè¿™éœ€è¦ä¸€å®šçš„äººå·¥åŠ³åŠ¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-66ad8c9bd4b7c6c0abc54d425f5bff3e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7eb93cb5e3a23926b4fa972f1f7e5a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed3565ac4c49d72e02f85632488a4e3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c0c6793d4532774c78760ad1a11631e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3d53880237f6c704914112c0392f627.jpg" align="middle"></details><h2 id="Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation"><a href="#Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation" class="headerlink" title="Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation"></a>Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation</h2><p><strong>Authors:Peter HÃ¶nig, Stefan Thalhammer, Markus Vincze</strong></p><p>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models. </p><p><a href="http://arxiv.org/abs/2402.06436v1">PDF</a> Submitted to the First Austrian Symposium on AI, Robotics, and Vision   2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œåœ¨ 6D ç›®æ ‡ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸­å…·æœ‰æ½œåœ¨ä¼˜åŠ¿ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¼°è®¡ RGB å›¾åƒå’Œ 3D ç©ºé—´ä¹‹é—´çš„ 2D-3D å¯¹åº”å…³ç³»æ˜¯ 6D ç›®æ ‡ä½å§¿ä¼°è®¡ä¸­çš„ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚</li><li>å½“å‰ï¼Œå¯†é›†å¯¹åº”å›¾æ˜¯ä½¿ç”¨åŸºäº GANã€è‡ªåŠ¨ç¼–ç å™¨æˆ–ç›´æ¥å›å½’æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¨¡å‹ä¼°è®¡çš„ã€‚</li><li>æœ€è¿‘ï¼Œå›¾åƒåˆ°å›¾åƒè½¬æ¢é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ä½¿æ‰©æ•£æ¨¡å‹æˆä¸ºåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°æ—¶çš„ä¼˜è¶Šé€‰æ‹©ã€‚</li><li>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäº GAN å’Œæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œï¼Œç”¨äº 6D ç›®æ ‡ä½å§¿ä¼°è®¡çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</li><li>æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¨¡å‹ä¼˜äº GANï¼Œè¡¨æ˜ 6D ç›®æ ‡ä½å§¿ä¼°è®¡æ¨¡å‹æœ‰è¿›ä¸€æ­¥æ”¹è¿›çš„æ½œåŠ›ã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨ 6D ç›®æ ‡ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸­å…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜ä½å§¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹æ”¹è¿› 2D-3D å¯†é›†å¯¹åº”ä»¥è¿›è¡Œ 6D ç›®æ ‡ä½å§¿ä¼°è®¡</li><li>ä½œè€…ï¼šPeter HÃ¶nigã€Stefan Thalhammerã€Markus Vincze</li><li>ä½œè€…å•ä½ï¼šå¥¥åœ°åˆ©ç»´ä¹Ÿçº³å·¥ä¸šå¤§å­¦è‡ªåŠ¨åŒ–ä¸æ§åˆ¶ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼š6D ç›®æ ‡ä½å§¿ä¼°è®¡ã€2D-3D å¯†é›†å¯¹åº”ã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒåˆ°å›¾åƒç¿»è¯‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.06436</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š6D ç›®æ ‡ä½å§¿ä¼°è®¡æ˜¯è®¸å¤šæ„ŸçŸ¥ä»»åŠ¡çš„åŸºç¡€ï¼Œä¾‹å¦‚è‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®ã€åˆ›å»ºæ•°å­—å­ªç”Ÿæˆ–æœºå™¨äººæŠ“å–ã€‚RGB-D ä¼ æ„Ÿå™¨å¯ä»¥åŒæ—¶æä¾›é¢œè‰²å’Œæ·±åº¦æ•°æ®ï¼Œä½†å¹¶ä¸æ€»æ˜¯å¯ç”¨ã€‚æ·±åº¦æ•°æ®ä¹Ÿå®¹æ˜“å—åˆ°å™ªå£°å’Œå…¶ä»–å¤±çœŸçš„å½±å“ï¼Œè¿™äº›å¤±çœŸé€šå¸¸ç”±åœºæ™¯ä¸­é—ªäº®ã€é‡‘å±å’Œé€æ˜ç‰©ä½“åå°„å¼•èµ·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œäººä»¬è€ƒè™‘ä»…ä½¿ç”¨ RGB å›¾åƒè¿›è¡Œä½å§¿ä¼°è®¡ã€‚æœ€å…ˆè¿›çš„æ–¹æ³•ä¾èµ–äºä¼°è®¡ RGB å›¾åƒå’Œ 3D å¯¹è±¡æ¨¡å‹ä¹‹é—´çš„ 2D-3D å¯†é›†å¯¹åº”ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æ“…é•¿æ¨æ–­å…·æœ‰é«˜å¯è§æ€§çš„å¯¹è±¡ä½å§¿ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€ç”±æ‚æ³¢ã€é®æŒ¡ã€å›¾åƒå¤±çœŸå’Œé—ªäº®ç‰©ä½“è¡¨é¢å¸¦æ¥çš„é‡å¤§æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»ï¼Œäººä»¬é€šè¿‡ä½¿ç”¨ç›´æ¥å›å½’ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) å’Œ U-Net æ¶æ„çš„ç»„åˆæˆ–ç¼–ç å™¨-è§£ç å™¨å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ¥è§£å†³ä½å§¿ä¼°è®¡çš„ 2D-3D å¯¹åº”é—®é¢˜ã€‚ä¸Šè¿°æ–¹æ³•ä¼°è®¡çš„å¯†é›†å¯¹åº”å›¾åŒ…å«ä» RGB å›¾åƒåˆ° 3D æ¨¡å‹çš„æ¯ä¸ªåƒç´ çš„ 3D åæ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†é®æŒ¡ã€æ‚æ³¢å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ææ–™ç‰¹æ€§æ—¶å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œç”¨äºä¼°è®¡ 2D-3D å¯†é›†å¯¹åº”ã€‚æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ¸æ·»åŠ å™ªå£°å¹¶é€æ¸å‡å°‘å™ªå£°æ¥ç”Ÿæˆå›¾åƒã€‚æœ¬æ–‡çš„æ–¹æ³•å°† RGB å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå¯†é›†å¯¹åº”å›¾ï¼Œè¯¥å›¾åŒ…å«ä» RGB å›¾åƒåˆ° 3D æ¨¡å‹çš„æ¯ä¸ªåƒç´ çš„ 3D åæ ‡ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨ YCB-Video æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨ 6D ç›®æ ‡ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸Šä¼˜äºåŸºäº GAN çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œã€‚è¿™è¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨ 6D ç›®æ ‡ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ã€‚</li></ol><p><strong>æ–¹æ³•</strong>ï¼š</p><p>ï¼ˆ1ï¼‰å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œç”¨äºä¼°è®¡2D-3Då¯†é›†å¯¹åº”ã€‚æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡é€æ¸æ·»åŠ å™ªå£°å¹¶é€æ¸å‡å°‘å™ªå£°æ¥ç”Ÿæˆå›¾åƒã€‚æœ¬æ–‡çš„æ–¹æ³•å°†RGBå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªå¯†é›†å¯¹åº”å›¾ï¼Œè¯¥å›¾åŒ…å«ä»RGBå›¾åƒåˆ°3Dæ¨¡å‹çš„æ¯ä¸ªåƒç´ çš„3Dåæ ‡ã€‚</p><p>ï¼ˆ2ï¼‰ä½ç½®å…ˆéªŒï¼šä¸ºäº†è·å¾—2Dä½ç½®å…ˆéªŒï¼Œæœ¬æ–‡ä½¿ç”¨2Dç›®æ ‡æ£€æµ‹å™¨ä»RGBå›¾åƒä¸­è£å‰ªæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚ROIæ˜¯å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹çš„è¾“å…¥ã€‚å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹å­¦ä¹ ä»RGBè£å‰ªä¸­ä¼°è®¡2D-3Då¯†é›†å¯¹åº”ã€‚2Dç›®æ ‡æ£€æµ‹å™¨ç”¨ä¸€ä¸ªçŸ©å½¢è¾¹ç•Œæ¡†è£å‰ªå¯¹è±¡ã€‚å› æ­¤ï¼Œå¯¹è±¡æ²¡æœ‰è¢«å®Œå…¨è£å‰ªï¼ŒèƒŒæ™¯åƒç´ ä»ç„¶å­˜åœ¨ã€‚å› æ­¤ï¼Œå›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œçš„å­¦ä¹ ç›®æ ‡æ˜¯åŒé‡çš„ã€‚ç½‘ç»œçš„ä¸»è¦ç›®æ ‡æ˜¯å­¦ä¹ ä¼°è®¡2D-3Då¯†é›†å¯¹åº”ã€‚åŒæ—¶ï¼Œç½‘ç»œéœ€è¦éšå¼åœ°å­¦ä¹ å¦‚ä½•å°†å¯¹è±¡ä»èƒŒæ™¯ä¸­åˆ†å‰²å‡ºæ¥ã€‚RANSAC+PnPæ­¥éª¤ä»å¯†é›†å¯¹åº”å›¾ä¸­ä¼°è®¡6Dç›®æ ‡ä½å§¿ã€‚</p><p>ï¼ˆ3ï¼‰æ•°æ®å¢å¼ºï¼šä¸ºäº†ç”Ÿæˆå›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ï¼Œå¯¹è±¡ç½‘æ ¼è¢«å½’ä¸€åŒ–ï¼Œä»¥é€‚åº”æ— é‡çº²çš„1x1x1ç«‹æ–¹ä½“ã€‚ç„¶åï¼Œæ ¹æ®é¡¶ç‚¹åœ¨å½’ä¸€åŒ–å¯¹è±¡åæ ‡ç©ºé—´ä¸­çš„XYZä½ç½®ï¼Œç”¨RGBå€¼å¯¹å¯¹è±¡ç½‘æ ¼çš„é¡¶ç‚¹è¿›è¡Œç€è‰²ã€‚ç„¶åï¼Œä½¿ç”¨çœŸå®å¹³ç§»ã€æ—‹è½¬å’Œç›¸æœºå†…å‚å¯¹å½’ä¸€åŒ–å’Œç€è‰²çš„ç½‘æ ¼è¿›è¡Œæ¸²æŸ“ã€‚</p><p>ï¼ˆ4ï¼‰å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç®—æ³•ï¼šæœ¬æ–‡æ¯”è¾ƒäº†ä¸¤ç§å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç®—æ³•ï¼Œå³GANæ¨¡å‹Pix2Pixå’Œæ‰©æ•£æ¨¡å‹BBDMã€‚ä½ç½®å…ˆéªŒå’ŒRANSAC+PnPæ­¥éª¤å¯¹äºè¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯ç›¸åŒçš„ï¼Œåªæœ‰å›¾åƒåˆ°å›¾åƒç¿»è¯‘å‡½æ•°IDC=F(IRGB)æ˜¯ä¸åŒçš„ã€‚ä¸¤ç§æ¨¡å‹éƒ½åœ¨ç›¸åŒæ¡ä»¶ä¸‹è¿›è¡Œè®­ç»ƒã€‚é¦–å…ˆï¼Œæ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•æ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼Œé™¤äº†å°†ROIè£å‰ªè°ƒæ•´ä¸º128x128åƒç´ ï¼Œè¿™æ˜¯ä¸¤ä¸ªæ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå¤§å°ã€‚åœ¨ç¬¬äºŒæ¬¡è®­ç»ƒè¿è¡Œä¸­ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®å¢å¼ºå‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå¦‚è¡¨1æ‰€ç¤ºã€‚å¯¹äºæ¯æ¬¡è¿è¡Œï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½è®­ç»ƒ40ä¸ªepochã€‚</p><p>ï¼ˆ5ï¼‰æ•°æ®é›†ï¼šæœ¬æ–‡åœ¨LMOæ•°æ®é›†ä¸Šè¯„ä¼°å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ã€‚å®ƒå…·æœ‰8ä¸ªåœ¨éšæœºåŸŸä¸­é‡‡æ ·çš„å®¶ç”¨ç‰©ä½“å’Œ50000å¼ åˆæˆæ¸²æŸ“çš„å›¾åƒã€‚è¿™äº›åˆæˆæ¸²æŸ“çš„å›¾åƒä»…ç”¨äºè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°ï¼Œä½¿ç”¨äº†1214å¼ çœŸå®ä¸–ç•Œçš„æµ‹è¯•å›¾åƒã€‚</p><p>ï¼ˆ6ï¼‰ä½ç½®å…ˆéªŒï¼šä½¿ç”¨ä¸¤ç»„é¢„å…ˆè®¡ç®—çš„ä½ç½®å…ˆéªŒè¿›è¡Œå¯¹è±¡è£å‰ªã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª2023å¹´ç›®æ ‡ä½å§¿ä¼°è®¡ï¼ˆBOPï¼‰æŒ‘æˆ˜èµ›åŸºå‡†çš„YOLOxæ£€æµ‹ç»“æœæ¥è¯„ä¼°ä½å§¿ä¼°è®¡çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°å¯¹è±¡åˆ†å‰²ï¼Œä½¿ç”¨çœŸå®ä½ç½®å…ˆéªŒã€‚</p><p>ï¼ˆ7ï¼‰è¯„ä¼°æŒ‡æ ‡ï¼šæœ¬æ–‡è¯„ä¼°äº†ä¼°è®¡çš„6Dä½å§¿çš„è´¨é‡ï¼Œä»¥åŠ2D-3Då¯†é›†å¯¹åº”å›¾å’Œå¯¹è±¡åˆ†å‰²çš„è´¨é‡ã€‚6Dç›®æ ‡ä½å§¿ä½¿ç”¨ADD(-S)åˆ†æ•°è¿›è¡Œè¯„ä¼°ã€‚ADD(-S)æ˜¯æŒ‡æ¨¡å‹ç‚¹mä¹‹é—´çš„å¹³å‡è·ç¦»ï¼Œå¯¹äºkmdâ‰¥mã€‚è¯¯å·®é˜ˆå€¼kmç”¨10%å®šä¹‰ã€‚å…¬å¼1ä¸­æ˜¾ç¤ºäº†mçš„è®¡ç®—ã€‚Rå’ŒTè¡¨ç¤ºçœŸå®æ—‹è½¬å’Œå¹³ç§»ï¼Œè€Œ^Rå’Œ^Tè¡¨ç¤ºä¼°è®¡æ—‹è½¬å’Œå¹³ç§»ï¼Œè€Œxè¡¨ç¤ºæ¨¡å‹Mä¸­çš„æ¨¡å‹ç‚¹ã€‚ä¸ºäº†å°†ä½å§¿ä¼°è®¡ç»“æœä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬ä¾é 2023å¹´ç›®æ ‡ä½å§¿ä¼°è®¡ï¼ˆBOPï¼‰æŒ‘æˆ˜èµ›åŸºå‡†è®¡ç®—çš„å¹³å‡å¬å›ç‡ã€‚è¯¥ARåˆ†æ•°æ˜¯å¯è§è¡¨é¢å·®å¼‚ï¼ˆVSDï¼‰ã€æœ€å¤§å¯¹ç§°æ„ŸçŸ¥è¡¨é¢è·ç¦»ï¼ˆMSSDï¼‰å’Œæœ€å¤§å¯¹ç§°æ„ŸçŸ¥æŠ•å½±è·ç¦»ï¼ˆMSPDï¼‰çš„å¹³å‡å¬å›ç‡çš„å¹³å‡å€¼ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œç”¨äºä¼°è®¡2D-3Då¯†é›†å¯¹åº”ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†GANæ¨¡å‹Pix2Pixå’Œæ‰©æ•£æ¨¡å‹BBDMåœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ä¼°è®¡2D-3Då¯†é›†å¯¹åº”å›¾çš„è´¨é‡æ–¹é¢ä¼˜äºGANã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œç”¨äºä¼°è®¡2D-3Då¯†é›†å¯¹åº”ã€‚</li><li>æ¯”è¾ƒäº†GANæ¨¡å‹Pix2Pixå’Œæ‰©æ•£æ¨¡å‹BBDMåœ¨ç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>æ‰©æ•£æ¨¡å‹åœ¨ä¼°è®¡2D-3Då¯†é›†å¯¹åº”å›¾çš„è´¨é‡æ–¹é¢ä¼˜äºGANã€‚</li><li>åœ¨YCB-Videoæ•°æ®é›†ä¸Šï¼Œæ‰©æ•£æ¨¡å‹åœ¨6Dç›®æ ‡ä½å§¿ä¼°è®¡ä»»åŠ¡ä¸Šä¼˜äºåŸºäºGANçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚</li><li>éœ€è¦è®­ç»ƒä¸¤ä¸ªå›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ã€‚</li><li>éœ€è¦å¯¹ä¼°è®¡çš„2D-3Då¯†é›†å¯¹åº”å›¾è¿›è¡Œåå¤„ç†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d77ba14fed7eddde5b06eaba6ff57afd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77c4e5753a8cd6ab35f73ede239b04a7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8d82762ff5fc78409df5e252c8a6442.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8833866e4d976d23589211a0d2587b35.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3c7c60e43fae13c906596978f0558ac8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68bedb16f322fb5603066efd18ca6348.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3a909299ddc09a5143e9d208d38ac851.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ffb421ba600650f3eb815efb8fb9a80.jpg" align="middle"></details><h2 id="Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion"><a href="#Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion" class="headerlink" title="Animated Stickers: Bringing Stickers to Life with Video Diffusion"></a>Animated Stickers: Bringing Stickers to Life with Video Diffusion</h2><p><strong>Authors:David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu</strong></p><p>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second. </p><p><a href="http://arxiv.org/abs/2402.06088v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºä¸€ç§å¸¦æœ‰åŠ¨ç”»è´´çº¸çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯æ ¹æ®æ–‡æœ¬æç¤ºå’Œé™æ€è´´çº¸å›¾åƒæ¥ç”ŸæˆåŠ¨ç”»ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥äº†ä¸€ç§å¸¦æœ‰åŠ¨ç”»è´´çº¸çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯æ ¹æ®æ–‡æœ¬æç¤ºå’Œé™æ€è´´çº¸å›¾åƒæ¥ç”ŸæˆåŠ¨ç”»ã€‚</li><li>è¯¥æ¨¡å‹å»ºç«‹åœ¨æœ€å…ˆè¿›çš„ Emu æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¹¶æ·»åŠ äº†æ—¶é—´å±‚æ¥æ¨¡æ‹ŸåŠ¨ä½œã€‚</li><li>ç”±äºè§†è§‰å’ŒåŠ¨ä½œé£æ ¼çš„å·®å¼‚ï¼Œåœ¨è‡ªç„¶è§†é¢‘ç”Ÿæˆä¸­è¡¨ç°è‰¯å¥½çš„æ¨¡å‹åœ¨åº”ç”¨äºè´´çº¸æ—¶æ— æ³•å†ç”Ÿæˆç”ŸåŠ¨çš„è§†é¢‘ã€‚</li><li>ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åˆ†ä¸¤é˜¶æ®µè¿›è¡Œå¾®è°ƒçš„ç®¡é“ï¼šé¦–å…ˆæ˜¯å¼±åŸŸå†…æ•°æ®ï¼Œå…¶æ¬¡æ˜¯äººç±»åœ¨å›è·¯ (HITL) ç­–ç•¥ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ•™å¸ˆé›†æˆã€‚</li><li>è¯¥ç­–ç•¥ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸“é—¨é’ˆå¯¹è¿åŠ¨è´¨é‡è¿›è¡Œæ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒé™æ€å›¾åƒçš„é£æ ¼ã€‚</li><li>ç»è¿‡æ¨ç†ä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸€ç§’å†…ç”Ÿæˆå…·æœ‰é«˜è´¨é‡ã€æœ‰è¶£ä¸”ç›¸å…³è¿åŠ¨çš„å…«å¸§è§†é¢‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŠ¨ç”»è´´çº¸ï¼šä½¿ç”¨è§†é¢‘æ‰©æ•£å°†è´´çº¸å˜æˆç”ŸåŠ¨è´´çº¸</li><li>ä½œè€…ï¼šDavid Yan<em>, Winnie Zhang</em>, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar SchÃ¶nfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearmanâ€ , Licheng Yuâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGenAI, Meta Menlo Park, California, USA</li><li>å…³é”®è¯ï¼šåŠ¨ç”»è´´çº¸ã€è§†é¢‘æ‰©æ•£ã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ã€äººç±»å‚ä¸å¾ªç¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.06088ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæœ€è¿‘ï¼Œäººä»¬å¯¹ç”Ÿæˆæ–‡æœ¬ï¼ˆå’Œå›¾åƒï¼‰åˆ°è§†é¢‘ (T2V) å»ºæ¨¡äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚å½“å‰æœ€å…ˆè¿›æ¨¡å‹ç”Ÿæˆçš„è§†é¢‘é€šå¸¸å¾ˆçŸ­ï¼ˆä¸åˆ° 3 ç§’ï¼‰ï¼Œå¹¶ä¸”é€šå¸¸ä½¿ç”¨æ–‡æœ¬ï¼ˆæ–‡æœ¬åˆ°è§†é¢‘æˆ– T2Vï¼‰ã€å›¾åƒï¼ˆå›¾åƒåˆ°è§†é¢‘æˆ– I2Vï¼‰æˆ–ä¸¤è€…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆç®¡é“æ¥é’ˆå¯¹çŸ­è§†é¢‘ç”Ÿæˆçš„è‡ªç„¶åº”ç”¨ï¼šä¸ºç¤¾äº¤è¡¨è¾¾åˆ¶ä½œåŠ¨ç”»è´´çº¸ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨é€šç”¨ I2V æ¨¡å‹ï¼ˆå³ä»…åœ¨é€šç”¨è§†é¢‘æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼‰åœ¨åº”ç”¨äºè´´çº¸æ—¶ä¸ä¼šäº§ç”Ÿé«˜è´¨é‡çš„è¿åŠ¨ï¼Œå¹¶ä¸”ç»å¸¸ä¼šç”Ÿæˆå…·æœ‰é™æ€æˆ–å¾®ä¸è¶³é“çš„è¿åŠ¨ï¼ˆä¾‹å¦‚ï¼Œä»…â€œæ‘†åŠ¨â€æ•ˆæœï¼‰å’Œ/æˆ–å¼•å…¥ä¸ä¸€è‡´æ€§å’Œè¿åŠ¨ä¼ªå½±ï¼ˆä¾‹å¦‚ï¼Œå˜å½¢ï¼‰ã€‚è¿™æ˜¯ç”±äºè‡ªç„¶ï¼ˆé€¼çœŸï¼‰è§†é¢‘ä¸è´´çº¸é£æ ¼åŠ¨ç”»ä¹‹é—´çš„è§†è§‰å’Œè¿åŠ¨å·®å¼‚ï¼Œå³åŸŸå·®è·ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äººç±»å‚ä¸å¾ªç¯ (HITL) è®­ç»ƒç­–ç•¥æ¥å¼¥åˆåŸŸå·®è·ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ•°æ®é›†å’Œå¸§é‡‡æ ·ç‡çš„ä¸åŒâ€œé…æ–¹â€è®­ç»ƒäº†è®¸å¤šâ€œæ•™å¸ˆâ€æ¨¡å‹ï¼Œä»¥ä¾¿æ•™å¸ˆæ¨¡å‹èƒ½å¤Ÿé›†ä½“äº§ç”Ÿé«˜è´¨é‡çš„å¤šæ ·åŒ–è¿åŠ¨ï¼Œå°½ç®¡å¾ˆå°‘ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡ä½¿ç”¨æ•™å¸ˆæ¨¡å‹åœ¨æ¶µç›–å¹¿æ³›æç¤ºé›†çš„å¤§å‹æç¤ºé›†ä¸Šæ‰§è¡Œæ¨ç†æ¥æ„å»º HITL æ•°æ®é›†ã€‚ç„¶åï¼Œä½¿ç”¨ HITL æ•°æ®é›†è®­ç»ƒä¸€ä¸ªè¾ƒå°çš„â€œå­¦ç”Ÿâ€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ å¹¶äº§ç”Ÿé«˜è´¨é‡çš„åŠ¨ç”»è´´çº¸ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…ç”Ÿæˆå…·æœ‰é«˜è´¨é‡ã€æœ‰è¶£ä¸”ç›¸å…³çš„è¿åŠ¨çš„å…«å¸§è§†é¢‘ã€‚æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŠ¨ç”»è´´çº¸æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æ—¶ç©ºæ½œåœ¨æ‰©æ•£æ¨¡å‹ä»¥æ–‡æœ¬-å›¾åƒå¯¹ä¸ºæ¡ä»¶ï¼Œå°†è´´çº¸å›¾åƒå˜æˆåŠ¨ç”»è´´çº¸ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒåˆ°ç”Ÿäº§çš„ç®¡é“ä» Emumodel å¼€å§‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤§é‡è‡ªç„¶è§†é¢‘ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œç„¶ååœ¨åŸŸå†…æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ•™å¸ˆé›†åˆ HITL å¾®è°ƒç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜è¿åŠ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œç›¸å…³æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨è®¸å¤šåŸºäºæ¶æ„ã€è’¸é¦çš„ä¼˜åŒ–å’Œåè®­ç»ƒä¼˜åŒ–æ¥å°†æ¨ç†é€Ÿåº¦æé«˜åˆ°æ¯æ‰¹ä¸€ç§’ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¾®è°ƒç­–ç•¥æ˜¾ç€æé«˜äº†è¿åŠ¨å¤§å°å’Œè´¨é‡ï¼Œä¼˜äºä»…åœ¨è‡ªç„¶è§†é¢‘ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œè¯æ˜äº†æ•™å¸ˆé›†åˆçš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ä½¿ç”¨äººç±»å‚ä¸å¾ªç¯ (HITL) è®­ç»ƒç­–ç•¥æ¥å¼¥åˆåŸŸå·®è·çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æœ‰è¶£ä¸”ç›¸å…³çš„è¿åŠ¨ï¼›æ€§èƒ½ï¼šè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…ç”Ÿæˆå…·æœ‰é«˜è´¨é‡ã€æœ‰è¶£ä¸”ç›¸å…³çš„è¿åŠ¨çš„å…«å¸§è§†é¢‘ï¼›å·¥ä½œé‡ï¼šè¯¥æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-188a2b4c4ed9e284afed14a8e020b622.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29dcdf079faf656ac8934c9dcb4fe4da.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a85a0fa5d13e8bd37d6352571f52fa54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-73989a294ebc6b241211e4051f9a71db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cee3ea6a017b3b85519c905ebe1d86a3.jpg" align="middle"></details><h2 id="InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset"><a href="#InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset" class="headerlink" title="InstaGen: Enhancing Object Detection by Training on Synthetic Dataset"></a>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</h2><p><strong>Authors:Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma</strong></p><p>In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. </p><p><a href="http://arxiv.org/abs/2402.05937v1">PDF</a> Tech report</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®é›†è®­ç»ƒç‰©ä½“æ£€æµ‹å™¨ï¼Œå¯ä»¥æé«˜æ£€æµ‹æ€§èƒ½æˆ–æ‰©å±•ç±»åˆ«ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>å°†å®ä¾‹çº§å®šä½å¤´é›†æˆåˆ°é¢„è®­ç»ƒç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒä¸­å®šä½ä»»æ„å®ä¾‹ã€‚</li><li>å®šä½å¤´é€šè¿‡æ¥è‡ªç°æœ‰ç‰©ä½“æ£€æµ‹å™¨çš„ç›‘ç£å’Œé’ˆå¯¹æ£€æµ‹å™¨æœªæ¶µç›–ç±»åˆ«çš„è‡ªè®­ç»ƒæ–¹æ¡ˆè¿›è¡Œè®­ç»ƒã€‚</li><li>å°†åˆæˆæ•°æ®ç”¨äºç‰©ä½“æ£€æµ‹å™¨çš„è®­ç»ƒå¯ä»¥æé«˜å…¶æ€§èƒ½ï¼Œåœ¨å¼€æ”¾è¯è¡¨åœºæ™¯ä¸­æ¯”ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•æé«˜ 4.5 ä¸ª APï¼Œåœ¨æ•°æ®ç¨€ç–åœºæ™¯ä¸­æé«˜ 1.2 åˆ° 5.2 ä¸ª APã€‚</li><li>InstaGen æ˜¯ä¸€ç§æ–°é¢–çš„èŒƒå¼ï¼Œå¯é€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒæ¥å¢å¼ºå¯¹è±¡æ£€æµ‹å™¨çš„èƒ½åŠ›ï¼Œä¾‹å¦‚æ‰©å±•ç±»åˆ«æˆ–æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li><li>InstaGen å°†å®ä¾‹çº§å®šä½å¤´é›†æˆåˆ°é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç”Ÿæˆçš„å›¾åƒä¸­å®šä½ä»»æ„å®ä¾‹ã€‚</li><li>å®šä½å¤´é€šè¿‡æ¥è‡ªç°æœ‰ç‰©ä½“æ£€æµ‹å™¨çš„ç›‘ç£å’Œé’ˆå¯¹æ£€æµ‹å™¨æœªæ¶µç›–ç±»åˆ«çš„è‡ªè®­ç»ƒæ–¹æ¡ˆè¿›è¡Œè®­ç»ƒã€‚</li><li>InstaGen ä½œä¸ºæ•°æ®åˆæˆå™¨å¯ç”¨äºç‰©ä½“æ£€æµ‹ï¼Œåœ¨å¼€æ”¾è¯è¡¨åœºæ™¯å’Œæ•°æ®ç¨€ç–åœºæ™¯ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p><strong>æ ‡é¢˜</strong>ï¼šInstaGenï¼šé€šè¿‡åˆæˆæ•°æ®é›†å¢å¼ºå¯¹è±¡æ£€æµ‹</p><p></p><p></p><p><strong>ä½œè€…</strong>ï¼šYuxin Fang, Yifan Zhang, Xiaolin Fang, Xiaohua Shi, Wei Shen, Enhua Wu</p><p></p><p></p><p><strong>ç¬¬ä¸€ä½œè€…å•ä½</strong>ï¼šåä¸­ç§‘æŠ€å¤§å­¦</p><p></p><p></p><p><strong>å…³é”®è¯</strong>ï¼šå¯¹è±¡æ£€æµ‹ï¼Œåˆæˆæ•°æ®ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå®ä¾‹çº§æ¥åœ°å¤´</p><p></p><p></p><p><strong>è®ºæ–‡é“¾æ¥</strong>ï¼šhttps://arxiv.org/abs/2302.07603</p><p></p><p></p><p><strong>Github ä»£ç é“¾æ¥</strong>ï¼šNone</p><p></p><p></p><p><strong>æ‘˜è¦</strong>ï¼š</p><p></p><p></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡åˆæˆæ•°æ®é›†å¢å¼ºå¯¹è±¡æ£€æµ‹èƒ½åŠ›çš„æ–°èŒƒå¼ï¼Œè¯¥èŒƒå¼é€šè¿‡ä»æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ¥æ‰©å±•æ£€æµ‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å®ä¾‹çº§æ¥åœ°å¤´é›†æˆåˆ°é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒä¸­å®šä½å®ä¾‹ã€‚æ¥åœ°å¤´é€šè¿‡ä½¿ç”¨æ¥è‡ªç°æˆå¯¹è±¡æ£€æµ‹å™¨çš„ç›‘ç£å’Œä¸€ç§åœ¨æ£€æµ‹å™¨æ— æ³•è¯†åˆ«çš„ç±»ä¸Šè¿›è¡Œçš„è‡ªè®­ç»ƒç­–ç•¥ï¼Œæ¥è®­ç»ƒä»¥å°†ç±»åˆ«åç§°çš„æ–‡æœ¬åµŒå…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç‰¹å¾å¯¹é½ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç§°ä¸º InstaGen çš„å¢å¼ºç‰ˆæ‰©æ•£æ¨¡å‹å¯ä»¥ä½œä¸ºæ•°æ®åˆæˆå™¨æ¥å¢å¼ºå¯¹è±¡æ£€æµ‹å™¨ï¼Œå¹¶åœ¨å¼€æ”¾è¯æ±‡è¡¨ï¼ˆ+4.6 APï¼‰å’Œæ•°æ®ç¨€ç–ï¼ˆ+4.8 APï¼‰ä¸Šå±•ç¤ºå‡ºä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚</p><p></p><p></p><p><strong>æ€»ç»“</strong>ï¼š</p><p></p><p></p><p>ï¼ˆä¸€ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š</p><p></p><p></p><p>å¯¹è±¡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€äººè„¸è¯†åˆ«ã€åŒ»ç–—å›¾åƒåˆ†æç­‰é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å¯¹è±¡æ£€æµ‹æ–¹æ³•å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨ä¸€äº›é¢†åŸŸæ˜¯éš¾ä»¥è·å¾—çš„ã€‚</p><p></p><p></p><p>ï¼ˆäºŒï¼‰ï¼šè¿‡å»çš„ç ”ç©¶å·¥ä½œï¼š</p><p></p><p></p><p>ä¸ºäº†è§£å†³æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å„ç§æ•°æ®å¢å¼ºæŠ€æœ¯æ¥æ‰©å……è®­ç»ƒæ•°æ®ã€‚è¿™äº›æŠ€æœ¯åŒ…æ‹¬å›¾åƒè£å‰ªã€ç¿»è½¬ã€æ—‹è½¬ã€é¢œè‰²æŠ–åŠ¨ç­‰ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯åªèƒ½äº§ç”Ÿæœ‰é™æ•°é‡çš„å›¾åƒï¼Œå¹¶ä¸”ä¸èƒ½ä¿è¯ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p></p><p></p><p>ï¼ˆä¸‰ï¼‰ï¼šæœ¬æ–‡çš„é—®é¢˜ï¼š</p><p></p><p></p><p>æœ¬æ–‡è®¤ä¸ºï¼Œç°æœ‰çš„æ•°æ®å¢å¼ºæŠ€æœ¯ä¸èƒ½å¾ˆå¥½åœ°è§£å†³æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸º InstaGenï¼Œè¯¥æŠ€æœ¯å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆå›¾åƒï¼Œå¹¶ä¸”å¯ä»¥ä¿è¯ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p></p><p></p><p>ï¼ˆå››ï¼‰ï¼šæœ¬æ–‡çš„æ–¹æ³•ï¼š</p><p></p><p></p><p>InstaGen æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥ä»å™ªå£°ç”Ÿæˆå›¾åƒã€‚InstaGen å°†ä¸€ä¸ªå®ä¾‹çº§æ¥åœ°å¤´é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒä¸­å®šä½å®ä¾‹ã€‚æ¥åœ°å¤´é€šè¿‡ä½¿ç”¨æ¥è‡ªç°æˆå¯¹è±¡æ£€æµ‹å™¨çš„ç›‘ç£å’Œä¸€ç§åœ¨æ£€æµ‹å™¨æ— æ³•è¯†åˆ«çš„ç±»ä¸Šè¿›è¡Œçš„è‡ªè®­ç»ƒç­–ç•¥ï¼Œæ¥è®­ç»ƒä»¥å°†ç±»åˆ«åç§°çš„æ–‡æœ¬åµŒå…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç‰¹å¾å¯¹é½ã€‚</p><p></p><p></p><p>ï¼ˆäº”ï¼‰ï¼šæœ¬æ–‡çš„å®éªŒç»“æœï¼š</p><p></p><p></p><p>æœ¬æ–‡åœ¨ PASCAL VOC å’Œ COCO æ•°æ®é›†ä¸Šå¯¹ InstaGen è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInstaGen å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºå¯¹è±¡æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚åœ¨ PASCAL VOC æ•°æ®é›†ä¸Šï¼ŒInstaGen å°† Faster R-CNN çš„ AP æé«˜äº† 4.6 ä¸ªç™¾åˆ†ç‚¹ã€‚åœ¨ COCO æ•°æ®é›†ä¸Šï¼ŒInstaGen å°† Faster R-CNN çš„ AP æé«˜äº† 4.8 ä¸ªç™¾åˆ†ç‚¹ã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ„å»ºå›¾åƒåˆæˆå™¨ï¼šé‡‡ç”¨é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ£€æµ‹æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿä¸”åŒ…å«æŒ‡å®šç±»åˆ«çš„å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šå¼•å…¥å®ä¾‹çº§æ¥åœ°å¤´ï¼šè®¾è®¡ä¸€ç§å®ä¾‹çº§æ¥åœ°å¤´ï¼Œå°†ç±»åˆ«åç§°çš„æ–‡æœ¬åµŒå…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç‰¹å¾å¯¹é½ï¼Œä»è€Œç”Ÿæˆå¯¹è±¡å®ä¾‹çš„è¾¹ç•Œæ¡†ã€‚ï¼ˆ3ï¼‰ï¼šç›‘ç£å­¦ä¹ å’Œè‡ªè®­ç»ƒï¼šä½¿ç”¨æ¥è‡ªç°æœ‰å¯¹è±¡æ£€æµ‹å™¨çš„ç›‘ç£å’Œä¸€ç§åœ¨æ£€æµ‹å™¨æ— æ³•è¯†åˆ«çš„ç±»ä¸Šè¿›è¡Œçš„è‡ªè®­ç»ƒç­–ç•¥ï¼Œæ¥è®­ç»ƒæ¥åœ°å¤´ã€‚ï¼ˆ4ï¼‰ï¼šæ•°æ®åˆæˆå™¨ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¥åœ°å¤´å’Œå›¾åƒåˆæˆå™¨ï¼Œç”ŸæˆåŒ…å«å¯¹è±¡å®ä¾‹åŠå…¶è¾¹ç•Œæ¡†çš„åˆæˆæ•°æ®é›†ã€‚ï¼ˆ5ï¼‰ï¼šåœ¨åˆæˆæ•°æ®é›†ä¸Šè®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨ï¼šå°†åˆæˆæ•°æ®é›†ä¸çœŸå®æ•°æ®é›†ç›¸ç»“åˆï¼Œè®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨ï¼Œä»¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºInstaGençš„æ•°æ®é›†åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“èƒ½å¤Ÿä¸ºä»»æ„ç±»åˆ«ç”Ÿæˆå…·æœ‰å¯¹è±¡è¾¹ç•Œæ¡†çš„å›¾åƒï¼Œä½œä¸ºæ„å»ºå¤§è§„æ¨¡åˆæˆæ•°æ®é›†ä»¥è®­ç»ƒå¯¹è±¡æ£€æµ‹å™¨çš„å…è´¹èµ„æºã€‚æˆ‘ä»¬è¿›è¡Œäº†è¯¦å°½çš„å®éªŒï¼Œä»¥å±•ç¤ºåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œä»¥æé«˜æ£€æµ‹æ€§èƒ½æˆ–æ‰©å±•æ£€æµ‹ç±»åˆ«çš„æ•°é‡ã€‚åœ¨å„ç§æ£€æµ‹åœºæ™¯ä¸­ï¼ŒåŒ…æ‹¬å¼€æ”¾è¯æ±‡è¡¨ï¼ˆ+4.5APï¼‰å’Œæ•°æ®ç¨€ç–ï¼ˆ+1.2âˆ¼5.2APï¼‰æ£€æµ‹ä¸­ï¼Œéƒ½å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç®¡é“InstaGenï¼Œè¯¥ç®¡é“èƒ½å¤Ÿä¸ºä»»æ„ç±»åˆ«ç”Ÿæˆå…·æœ‰å¯¹è±¡è¾¹ç•Œæ¡†çš„å›¾åƒã€‚</li><li>è®¾è®¡äº†ä¸€ç§å®ä¾‹çº§æ¥åœ°å¤´ï¼Œå°†ç±»åˆ«åç§°çš„æ–‡æœ¬åµŒå…¥ä¸æ‰©æ•£æ¨¡å‹çš„ç©ºé—´ç‰¹å¾å¯¹é½ï¼Œä»è€Œç”Ÿæˆå¯¹è±¡å®ä¾‹çš„è¾¹ç•Œæ¡†ã€‚</li><li>ä½¿ç”¨æ¥è‡ªç°æœ‰å¯¹è±¡æ£€æµ‹å™¨çš„ç›‘ç£å’Œä¸€ç§åœ¨æ£€æµ‹å™¨æ— æ³•è¯†åˆ«çš„ç±»ä¸Šè¿›è¡Œçš„è‡ªè®­ç»ƒç­–ç•¥ï¼Œæ¥è®­ç»ƒæ¥åœ°å¤´ã€‚æ€§èƒ½ï¼š</li><li>åœ¨PASCAL VOCå’ŒCOCOæ•°æ®é›†ä¸Šï¼ŒInstaGenå°†Faster R-CNNçš„APæé«˜äº†4.6ä¸ªç™¾åˆ†ç‚¹å’Œ4.8ä¸ªç™¾åˆ†ç‚¹ã€‚</li><li>åœ¨å¼€æ”¾è¯æ±‡è¡¨å’Œæ•°æ®ç¨€ç–æ£€æµ‹ä¸­ï¼ŒInstaGenå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>InstaGenæ˜¯ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œéœ€è¦é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œå®ä¾‹çº§æ¥åœ°å¤´ã€‚</li><li>InstaGenéœ€è¦å¤§é‡çš„æ•°æ®æ¥è®­ç»ƒæ¥åœ°å¤´ã€‚</li><li>InstaGenéœ€è¦å¤§é‡çš„æ—¶é—´æ¥ç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e5bc75a4d614b9abf0055ef9f09e29eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcaa5f4430aaa302f904c1eb77cd432c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ed1d3b41f15d36193b946e6064581300.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6998a66afc9f7f895bfb98faa0596297.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85a17fc9e78759363117b1e3dbd18da2.jpg" align="middle"></details><h2 id="Scalable-Diffusion-Models-with-State-Space-Backbone"><a href="#Scalable-Diffusion-Models-with-State-Space-Backbone" class="headerlink" title="Scalable Diffusion Models with State Space Backbone"></a>Scalable Diffusion Models with State Space Backbone</h2><p><strong>Authors:Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</strong></p><p>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a>. </p><p><a href="http://arxiv.org/abs/2402.05608v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¶æ„æ„å»ºçš„æ–°å‹æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å›¾åƒæ•°æ®ä¸Šå®ç°å¯ä¸ U å½¢å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„åª²ç¾çš„æ€§èƒ½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>åŸºäºçŠ¶æ€ç©ºé—´æ¶æ„çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ•°æ®ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¯ä¸åŸºäº U å½¢å·ç§¯ç¥ç»ç½‘ç»œæˆ–åŸºäº Transformer çš„ U å½¢å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„å®ç°ç›¸å½“çš„æ€§èƒ½ï¼Œç”šè‡³ä¼˜äºå®ƒä»¬ã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å›¾åƒå—ç­‰æ‰€æœ‰è¾“å…¥éƒ½è§†ä¸ºæ ‡è®°ã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆå’Œç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­å‡è¡¨ç°è‰¯å¥½ã€‚</li><li>é€šè¿‡å¢åŠ æ·±åº¦/å®½åº¦æˆ–å¢åŠ è¾“å…¥æ ‡è®°ï¼Œæ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ­£å‘ä¼ é€’å¤æ‚åº¦ï¼ˆä»¥ Gflops ä¸ºå•ä½ï¼‰æ›´é«˜ï¼Œå¹¶ä¸”å§‹ç»ˆè¡¨ç°å‡ºæ›´ä½çš„ FIDã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</li><li>åœ¨åˆ†è¾¨ç‡ä¸º 256Ã—256 å’Œ 512Ã—512 çš„ç±»åˆ«æ¡ä»¶ ImageNet åŸºå‡†ä¸Šï¼Œæ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ä¸­å®ç°äº†ä¸å…ˆå‰æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ä»£ç å’Œæ¨¡å‹å¯åœ¨ <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a> ä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºçŠ¶æ€ç©ºé—´çš„å¯æ‰©å±•æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šZhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ˜†ä»‘ç§‘æŠ€</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€çŠ¶æ€ç©ºé—´ã€å¯æ‰©å±•æ€§ã€å›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.05608ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/feizc/DiS</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œè¿‘å¹´æ¥åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆã€è¯­éŸ³åˆæˆå’Œ 3D åˆæˆç­‰é¢†åŸŸã€‚æ‰©æ•£æ¨¡å‹çš„å‘å±•ç¦»ä¸å¼€é‡‡æ ·ç®—æ³•å’Œæ¨¡å‹éª¨å¹²çš„è¿›æ­¥ï¼Œå…¶ä¸­ U-Net æ˜¯æ‰©æ•£æ¨¡å‹ä¸­å¸¸ç”¨çš„éª¨å¹²ç½‘ç»œï¼Œä½†å…¶åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚(2)ï¼šè¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹éª¨å¹²ç½‘ç»œï¼Œå¦‚ U-Netï¼Œåœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºçŠ¶æ€ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiSï¼‰ï¼Œè¯¥æ¨¡å‹å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å›¾åƒå—è§†ä¸ºæ ‡è®°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€ç©ºé—´éª¨å¹²ç½‘ç»œæ¥å¤„ç†è¿™äº›æ ‡è®°ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ DiS æ¨¡å‹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š</li><li>å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å›¾åƒå—è§†ä¸ºæ ‡è®°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€ç©ºé—´éª¨å¹²ç½‘ç»œæ¥å¤„ç†è¿™äº›æ ‡è®°ã€‚</li><li>DiS æ¨¡å‹å¯ä»¥å¤„ç†åŸå§‹å›¾åƒå—æˆ–æ½œåœ¨ç©ºé—´ä¸­çš„æ ‡è®°ã€‚</li><li><p>DiS æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ·±åº¦ã€å®½åº¦æˆ–è¾“å…¥æ ‡è®°çš„æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚(4)ï¼šå®éªŒç»“æœä¸æ€§èƒ½ï¼šæœ¬æ–‡åœ¨æ— æ¡ä»¶å’Œç±»æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå¯¹ DiS æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ DiS æ¨¡å‹ä¸åŸºäº CNN æˆ– Transformer çš„ U-Net æ¨¡å‹å…·æœ‰ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜åˆ†æäº† DiS æ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œç»“æœè¡¨æ˜ DiS æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ·±åº¦ã€å®½åº¦æˆ–è¾“å…¥æ ‡è®°çš„æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨ ImageNet æ•°æ®é›†ä¸Šï¼ŒDiS-H/2 æ¨¡å‹åœ¨åˆ†è¾¨ç‡ä¸º 256Ã—256 å’Œ 512Ã—512 çš„ç±»æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiSï¼‰ï¼Œè¯¥æ¨¡å‹å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å—è§†ä¸ºæ ‡è®°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€ç©ºé—´éª¨å¹²ç½‘ç»œæ¥å¤„ç†è¿™äº›æ ‡è®°ã€‚ï¼ˆ2ï¼‰ï¼šDiSæ¨¡å‹å¯ä»¥å¤„ç†åŸå§‹å—æˆ–æ½œåœ¨ç©ºé—´ä¸­çš„æ ‡è®°ã€‚ï¼ˆ3ï¼‰ï¼šDiSæ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ·±åº¦ã€å®½åº¦æˆ–è¾“å…¥æ ‡è®°çš„æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDiSï¼‰ï¼Œè¯¥æ¨¡å‹å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å—è§†ä¸ºæ ‡è®°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€ç©ºé—´éª¨å¹²ç½‘ç»œæ¥å¤„ç†è¿™äº›æ ‡è®°ã€‚DiS é‡‡ç”¨äº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥å¤„ç†æ‰€æœ‰è¾“å…¥ï¼ŒåŒ…æ‹¬æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å›¾åƒå—ï¼Œå°†å®ƒä»¬è§†ä¸ºè¿æ¥çš„æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiS ä¸åŸºäº CNN æˆ– Transformer çš„ U-Net æ¨¡å‹ç›¸æ¯”å…·æœ‰ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ç»§æ‰¿äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ç±»çš„æ˜¾ç€å¯æ‰©å±•æ€§ç‰¹å¾ã€‚æˆ‘ä»¬è®¤ä¸º DiS å¯ä»¥ä¸ºæœªæ¥ç ”ç©¶æ‰©æ•£æ¨¡å‹ä¸­çš„éª¨å¹²ç½‘ç»œæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶æœ‰åŠ©äºæ¨è¿›å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ä¸­çš„ç”Ÿæˆå»ºæ¨¡ã€‚é‰´äºæœ¬ç ”ç©¶ä¸­æå‡ºçš„ä»¤äººé¼“èˆçš„å¯æ‰©å±•æ€§ç»“æœï¼Œæœªæ¥çš„åŠªåŠ›åº”é›†ä¸­åœ¨å°† DiS è¿›ä¸€æ­¥æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹å’Œæ ‡è®°è®¡æ•°ä¸Šã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDiS æ¨¡å‹å°†æ—¶é—´ã€æ¡ä»¶å’Œå™ªå£°å—è§†ä¸ºæ ‡è®°ï¼Œå¹¶ä½¿ç”¨çŠ¶æ€ç©ºé—´éª¨å¹²ç½‘ç»œæ¥å¤„ç†è¿™äº›æ ‡è®°ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚DiS æ¨¡å‹å¯ä»¥å¤„ç†åŸå§‹å—æˆ–æ½œåœ¨ç©ºé—´ä¸­çš„æ ‡è®°ï¼Œè¿™ä½¿å¾—å®ƒå¯ä»¥åº”ç”¨äºå„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚DiS æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥é€šè¿‡å¢åŠ æ·±åº¦ã€å®½åº¦æˆ–è¾“å…¥æ ‡è®°çš„æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼šåœ¨æ— æ¡ä»¶å’Œç±»æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒDiS æ¨¡å‹ä¸åŸºäº CNN æˆ– Transformer çš„ U-Net æ¨¡å‹å…·æœ‰ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨ ImageNet æ•°æ®é›†ä¸Šï¼ŒDiS-H/2 æ¨¡å‹åœ¨åˆ†è¾¨ç‡ä¸º 256Ã—256 å’Œ 512Ã—512 çš„ç±»æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†ä¸ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚å·¥ä½œé‡ï¼šDiS æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä¸åŸºäº CNN æˆ– Transformer çš„ U-Net æ¨¡å‹ç›¸å½“ã€‚DiS æ¨¡å‹çš„å¯æ‰©å±•æ€§ä½¿å¾—å®ƒå¯ä»¥åº”ç”¨äºå„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆå’Œ 3D åˆæˆã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6bb4b2235878abe86e04f19f24047beb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1d4bb00838a5fb623fcc9eb998c2c6b9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-adf0dc0a97f9ca167de7eccda01fe6df.jpg" align="middle"></details><h2 id="SPAD-Spatially-Aware-Multiview-Diffusers"><a href="#SPAD-Spatially-Aware-Multiview-Diffusers" class="headerlink" title="SPAD : Spatially Aware Multiview Diffusers"></a>SPAD : Spatially Aware Multiview Diffusers</h2><p><strong>Authors:Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</strong></p><p>We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a> </p><p><a href="http://arxiv.org/abs/2402.05235v1">PDF</a> Webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a></p><p><strong>Summary</strong><br>è·¨è§†è§’å›¾åƒç”Ÿæˆæ¨¡å‹ SPADï¼šè‡ªæˆ‘æ³¨æ„å’Œç©ºé—´ç¼–ç çš„ç»“åˆï¼Œå®ç°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SPAD æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»æ–‡æœ¬æç¤ºæˆ–å•ä¸ªå›¾åƒç”Ÿæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒã€‚</li><li>SPAD æ˜¯é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚æ¥å®ç°å¤šè§†è§’ç”Ÿæˆï¼Œå¹¶å¯¹ Objaverse çš„é«˜è´¨é‡å­é›†è¿›è¡Œå¾®è°ƒã€‚</li><li>SPAD æ˜¾ç¤ºï¼Œå…ˆå‰çš„ç ”ç©¶æå‡ºçš„è‡ªæˆ‘æ³¨æ„çš„æœ´ç´ æ‰©å±•ï¼ˆä¾‹å¦‚ MVDreamï¼‰å¯¼è‡´è§†è§’ä¹‹é—´çš„å†…å®¹å¤åˆ¶ã€‚</li><li>SPAD æ˜¾å¼åœ°é™åˆ¶åŸºäºæçº¿å‡ ä½•çš„è·¨è§†è§’æ³¨æ„ã€‚</li><li>SPAD åˆ©ç”¨ä»ç›¸æœºå°„çº¿æ´¾ç”Ÿçš„ PlÃ¼cker åæ ‡ï¼Œå¹¶å°†å®ƒä»¬æ³¨å…¥ä½œä¸ºä½ç½®ç¼–ç ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼º 3D ä¸€è‡´æ€§ã€‚</li><li>ä¸åªèƒ½åœ¨å›ºå®šæ–¹ä½è§’å’Œä»°è§’ç”Ÿæˆè§†å›¾çš„æœ€è¿‘å·¥ä½œç›¸æ¯”ï¼ŒSPAD æä¾›äº†å®Œå…¨çš„ç›¸æœºæ§åˆ¶ï¼Œå¹¶åœ¨ Objaverse å’Œ Google Scanned Objects æ•°æ®é›†ä¸Šçœ‹ä¸è§çš„ç‰©ä½“çš„æ–°é¢–è§†å›¾åˆæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>ä½¿ç”¨ SPAD è¿›è¡Œæ–‡æœ¬åˆ° 3D ç”Ÿæˆæ¶ˆé™¤äº†å¤šé¢ Janus é—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSPADï¼šç©ºé—´æ„ŸçŸ¥å¤šè§†å›¾æ‰©æ•£å™¨</li><li>ä½œè€…ï¼šYash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¤šä¼¦å¤šå¤§å­¦</li><li>å…³é”®è¯ï¼šå¤šè§†å›¾ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€æ–‡æœ¬åˆ° 3D</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://yashkant.github.io/spad/ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤šè§†å›¾ç”Ÿæˆæ˜¯æŒ‡ä»æ–‡æœ¬æç¤ºæˆ–å•ä¸ªå›¾åƒç”Ÿæˆä¸€ç»„åœ¨ 3D ç©ºé—´ä¸­ä¸€è‡´çš„å›¾åƒã€‚è¿™å¯¹äºè®¸å¤šåº”ç”¨å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®å’Œ 3D å»ºæ¨¡ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†å›¾å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´è§†å›¾ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ï¼Œä¾‹å¦‚å¯¹è±¡å½¢çŠ¶æˆ–çº¹ç†ä¸åŒ¹é…ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†å›¾ç”Ÿæˆæ–¹æ³• SPADã€‚SPAD é€šè¿‡åœ¨ 2D æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥è·¨è§†å›¾äº¤äº’æ¥å®ç°å¤šè§†å›¾ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒSPAD è¿˜åˆ©ç”¨äº† PlÃ¼cker åæ ‡æ¥å¢å¼º 3D ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šSPAD åœ¨ Objaverse å’Œ Google Scanned Objects æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSPAD åœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSPAD è¿˜èƒ½å¤Ÿé˜²æ­¢å¤šé¢ Janus é—®é¢˜ï¼Œå³ç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒè§†å›¾ä¸­å…·æœ‰ä¸åŒçš„å¤–è§‚ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): SPADçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤šè§†å›¾ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªæ‰©æ•£æ¨¡å‹é—®é¢˜ã€‚SPADä½¿ç”¨ä¸€ä¸ª2Dæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆæ¯ä¸ªè§†å›¾çš„å›¾åƒï¼Œå¹¶é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥è·¨è§†å›¾äº¤äº’æ¥ç¡®ä¿è§†å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚(2): SPADä½¿ç”¨PlÃ¼ckeråæ ‡æ¥è¡¨ç¤º3Dç©ºé—´ä¸­çš„ç‚¹ã€‚PlÃ¼ckeråæ ‡å…·æœ‰ä¸å˜æ€§ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸å—è§†è§’å’ŒæŠ•å½±å˜æ¢çš„å½±å“ã€‚SPADåˆ©ç”¨PlÃ¼ckeråæ ‡æ¥å¢å¼º3Dä¸€è‡´æ€§ï¼Œå¹¶é˜²æ­¢å¤šé¢Janusé—®é¢˜ã€‚(3): SPADåœ¨Objaverseå’ŒGoogleScannedObjectsæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒSPADåœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSPADè¿˜èƒ½å¤Ÿé˜²æ­¢å¤šé¢Janusé—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šSPADæ˜¯ä¸€ç§æ–°é¢–çš„å¤šè§†å›¾ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒå°†æ–‡æœ¬æˆ–å›¾åƒè¾“å…¥è½¬æ¢ä¸ºå¤šä¸ªè§†å›¾ã€‚SPADåœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­å¼•å…¥äº†æçº¿æ³¨æ„åŠ›ï¼Œä»¥ä¿ƒè¿›å¤šè§†å›¾äº¤äº’å¹¶æ”¹è¿›ç›¸æœºæ§åˆ¶ã€‚æ­¤å¤–ï¼ŒSPADä½¿ç”¨PlÃ¼ckerä½ç½®ç¼–ç å¢å¼ºäº†è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥é€šè¿‡é˜²æ­¢å¯¹è±¡çš„ç¿»è½¬è§†å›¾é¢„æµ‹æ¥è¿›ä¸€æ­¥æ”¹è¿›ç›¸æœºæ§åˆ¶ã€‚SPADåœ¨Objaverseå’ŒGoogleScannedObjectsæ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸¥æ ¼çš„è¯„ä¼°ï¼Œå¹¶åœ¨å›¾åƒæ¡ä»¶çš„æ–°è§†å›¾åˆæˆæ–¹é¢å±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†å¤šè§†å›¾ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºæ‰©æ•£æ¨¡å‹é—®é¢˜ï¼Œå¹¶é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥è·¨è§†å›¾äº¤äº’æ¥ç¡®ä¿è§†å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li><li>ä½¿ç”¨PlÃ¼ckeråæ ‡æ¥è¡¨ç¤º3Dç©ºé—´ä¸­çš„ç‚¹ï¼Œå¹¶åˆ©ç”¨PlÃ¼ckeråæ ‡æ¥å¢å¼º3Dä¸€è‡´æ€§ï¼Œé˜²æ­¢å¤šé¢Janusé—®é¢˜ã€‚æ€§èƒ½ï¼š</li><li>åœ¨Objaverseå’ŒGoogleScannedObjectsæ•°æ®é›†ä¸Šï¼ŒSPADåœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SPADèƒ½å¤Ÿé˜²æ­¢å¤šé¢Janusé—®é¢˜ï¼Œå³ç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒè§†å›¾ä¸­å…·æœ‰ä¸åŒçš„å¤–è§‚ã€‚å·¥ä½œé‡ï¼š</li><li>SPADçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨PyTorchä¸­è½»æ¾å®ç°ã€‚</li><li>SPADçš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¿«é€Ÿï¼Œå¹¶ä¸”å¯ä»¥åœ¨å•ä¸ªGPUä¸Šå®Œæˆã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-afe3524a8f81d817d06d1d9498a1728a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a80a51acf35ce9d57c5584647e5cca12.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-13  Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/</id>
    <published>2024-02-09T02:20:12.000Z</published>
    <updated>2024-02-09T02:20:12.523Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-09-æ›´æ–°"><a href="#2024-02-09-æ›´æ–°" class="headerlink" title="2024-02-09 æ›´æ–°"></a>2024-02-09 æ›´æ–°</h1><h2 id="NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering"><a href="#NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering" class="headerlink" title="NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering"></a>NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</h2><p><strong>Authors:Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao</strong></p><p>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.04829v1">PDF</a> Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a></p><p><strong>æ‘˜è¦</strong><br>ç¥ç»è¾å°„åœºå¯ä»¥ä½œä¸ºç©ºé—´éè·ç¦»ç¯å¢ƒå…‰æºï¼Œç”¨äºç‰©ç†é€†æ¸²æŸ“ï¼Œä½¿é€†æ¸²æŸ“æ›´åŠ çœŸå®å‡†ç¡®ã€‚</p><p><strong>ä¸»è¦è¦ç‚¹</strong></p><ul><li>åŸºäºç‰©ç†çš„é€†æ¸²æŸ“æ—¨åœ¨è”åˆä¼˜åŒ–ä»æ•è·çš„ 2D å›¾åƒä¸­æå–çš„å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ã€‚</li><li>åœ¨é€†æ¸²æŸ“ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ç¯å¢ƒè´´å›¾ä½œä¸ºå…‰ç…§æ¨¡å‹ï¼Œä½†è¿™ç§å‡è®¾ä¼šå¯¼è‡´ç©ºé—´ä¸å˜çš„å…‰ç…§ï¼Œè¿™åœ¨çœŸå®ä¸–ç•Œçš„é€†æ¸²æŸ“ä¸­å¯èƒ½æ˜¯ä¸å‡†ç¡®çš„è¿‘ä¼¼ã€‚</li><li>æå‡ºä½¿ç”¨ç¥ç»è¾å°„åœºä½œä¸ºç©ºé—´å¯å˜çš„ç¯å¢ƒå…‰ç…§æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªä»¥ç¥ç»è¾å°„åœºä½œä¸ºéè·ç¦»ç¯å¢ƒå…‰æºçš„é€†æ¸²æŸ“ç®¡é“ã€‚</li><li>å°†æ–¹æ³•ä¸ç¯å¢ƒè´´å›¾åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œæ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼ŒåŸºäºç¥ç»è¾å°„åœºçš„å…‰æºå¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ï¼Œå¹¶å®ç°æ›´å‡†ç¡®çš„é€†æ¸²æŸ“ã€‚</li><li>é¡¹ç›®é¡µé¢å’Œè§†é¢‘ï¼š<a href="https://nerfemitterpbir.github.io/ã€‚">https://nerfemitterpbir.github.io/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºç‰©ç†çš„åæ¼”æ¸²æŸ“ä¸­ï¼ŒNeRF ä½œä¸ºéè¿œå¤„ç¯å¢ƒå‘å°„å™¨</li><li>ä½œè€…ï¼šJingwang Lingã€Ruihan Yuã€Feng Xuã€Chun Duã€Shuang Zhao</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFã€ç‰©ç†åæ¼”æ¸²æŸ“ã€ç¯å¢ƒå…‰ç…§ã€å½¢çŠ¶é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2402.04829.pdfï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŸºäºç‰©ç†çš„åæ¼”æ¸²æŸ“æ—¨åœ¨ä»æ•è·çš„ 2D å›¾åƒä¸­è”åˆä¼˜åŒ–å½¢çŠ¶ã€æè´¨å’Œå…‰ç…§ã€‚å…¶ä¸­ï¼Œå…‰ç…§æ˜¯å®ç°çœŸå®å…‰ç…§ä¼ è¾“æ¨¡æ‹Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç¯å¢ƒè´´å›¾æ˜¯åæ¼”æ¸²æŸ“ä¸­å¸¸ç”¨çš„å…‰ç…§æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å‘ç°ï¼Œåœ¨å…‰æºä¸æ˜¯æ— é™è¿œå¤„çš„åœºæ™¯ä¸­ï¼Œç¯å¢ƒè´´å›¾çš„ç©ºé—´ä¸å˜å…‰ç…§å‡è®¾ä¼šå¯¼è‡´ç©ºé—´ä¸å˜çš„å…‰ç…§ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­çš„åæ¼”æ¸²æŸ“ä¸­å¯èƒ½æ˜¯ä¸å‡†ç¡®çš„è¿‘ä¼¼ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨ç¯å¢ƒè´´å›¾æ¥è¿‘ä¼¼ç‰©ä½“å‘¨å›´çš„å…‰ç…§ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å…‰æºä¸æ˜¯æ— é™è¿œå¤„çš„åœºæ™¯ä¸­ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„ç»“æœã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºä½¿ç”¨ NeRF ä½œä¸ºç©ºé—´å˜åŒ–çš„ç¯å¢ƒå…‰ç…§æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªä»¥ NeRF ä½œä¸ºéè¿œå¤„ç¯å¢ƒå‘å°„å™¨çš„åæ¼”æ¸²æŸ“ç®¡é“ã€‚é€šè¿‡åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šä¸ç¯å¢ƒè´´å›¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ NeRF æ¨¡å‹å¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ï¼Œå¹¶å®ç°æ›´å‡†ç¡®çš„åæ¼”æ¸²æŸ“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æ¯”ç¯å¢ƒè´´å›¾æ›´å¥½çš„ç»“æœã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡ç…§æ˜å’Œå½¢çŠ¶é‡å»ºä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡ç…§æ˜ã€å½¢çŠ¶é‡å»ºå’Œæè´¨ä¼°è®¡ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ï¼Œå¹¶å®ç°æ›´å‡†ç¡®çš„åæ¼”æ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºä½¿ç”¨ NeRF ä½œä¸ºç©ºé—´å˜åŒ–çš„ç¯å¢ƒå…‰ç…§æ¨¡å‹ï¼Œæ„å»ºä»¥ NeRF ä½œä¸ºéè¿œå¤„ç¯å¢ƒå‘å°„å™¨çš„åæ¼”æ¸²æŸ“ç®¡é“ã€‚ï¼ˆ2ï¼‰åˆ©ç”¨çœŸå®å’Œåˆæˆæ•°æ®é›†ï¼Œä¸ç¯å¢ƒè´´å›¾è¿›è¡Œæ¯”è¾ƒï¼Œè¯æ˜ NeRF æ¨¡å‹å¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ï¼Œå®ç°æ›´å‡†ç¡®çš„åæ¼”æ¸²æŸ“ã€‚ï¼ˆ3ï¼‰åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼Œä¸ç¯å¢ƒè´´å›¾ç›¸æ¯”ï¼Œåœ¨é‡ç…§æ˜ã€å½¢çŠ¶é‡å»ºå’Œæè´¨ä¼°è®¡ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼š(1)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„åæ¼”æ¸²æŸ“ç®¡é“ï¼Œè¯¥ç®¡é“å°† NeRF ç”¨ä½œéè¿œå¤„ç¯å¢ƒå‘å°„å™¨ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ï¼Œå¹¶å®ç°æ›´å‡†ç¡®çš„åæ¼”æ¸²æŸ“ã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>ä½¿ç”¨ NeRF ä½œä¸ºç©ºé—´å˜åŒ–çš„ç¯å¢ƒå…‰ç…§æ¨¡å‹ï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿåœºæ™¯å…‰ç…§ã€‚</li><li>æ„å»ºäº†ä¸€ä¸ªä»¥ NeRF ä½œä¸ºéè¿œå¤„ç¯å¢ƒå‘å°„å™¨çš„åæ¼”æ¸²æŸ“ç®¡é“ï¼Œå¯ä»¥å®ç°æ›´å‡†ç¡®çš„åæ¼”æ¸²æŸ“ã€‚</li><li>åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼Œä¸ç¯å¢ƒè´´å›¾ç›¸æ¯”ï¼Œåœ¨é‡ç…§æ˜ã€å½¢çŠ¶é‡å»ºå’Œæè´¨ä¼°è®¡ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨çœŸå®æ•°æ®é›†ä¸Šï¼Œåœ¨é‡ç…§æ˜å’Œå½¢çŠ¶é‡å»ºä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li><li>åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œåœ¨é‡ç…§æ˜ã€å½¢çŠ¶é‡å»ºå’Œæè´¨ä¼°è®¡ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦è®­ç»ƒ NeRF æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>éœ€è¦æ„å»ºåæ¼”æ¸²æŸ“ç®¡é“ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„ç¼–ç¨‹å·¥ä½œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5217f666aff1dcbbc55e20cda0c76080.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9fa354da141f8905a59ea4a06f90f25.jpg" align="middle"></details><h2 id="OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding"><a href="#OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding" class="headerlink" title="OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding"></a>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li</strong></p><p>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness. </p><p><a href="http://arxiv.org/abs/2402.04648v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯é€šè¿‡ç»“åˆè§†è§‰å’Œè¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œæå‡äº†NeRFåœ¨å¼€æ”¾è¯æ±‡è¡¨3Dè¯­ä¹‰æ„ŸçŸ¥ä¸­çš„è¡¨ç°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>OV-NeRF æå‡ºäº†ä¸€ç§å•è§†å›¾å’Œè·¨è§†å›¾ç­–ç•¥ï¼Œå°†NeRFç”¨äºå¼€æ”¾è¯æ±‡è¡¨3Dè¯­ä¹‰æ„ŸçŸ¥ã€‚</li><li>åˆ©ç”¨ SAM æå–çš„ 2D æ©æ¨¡å»ºè®®ï¼Œå¼•å…¥åŒºåŸŸè¯­ä¹‰æ’åº (RSR) æ­£åˆ™åŒ–ï¼Œä»¥çº æ­£æ¯ä¸ªè®­ç»ƒè§†å›¾çš„è¯­ä¹‰å™ªå£°ã€‚</li><li>æå‡ºè·¨è§†å›¾è‡ªå¢å¼º (CSE) ç­–ç•¥ï¼Œåˆ©ç”¨è®­ç»ƒè¯­ä¹‰åœºæœ¬èº«ç”Ÿæˆçš„ 3D ä¸€è‡´è¯­ä¹‰ï¼Œå‡å°‘è¯­ä¹‰æ¨¡ç³Šæ€§å’Œå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li><li>OV-NeRF åœ¨ Replica å’Œ Scannet æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 20.31% å’Œ 18.42% çš„ mIoU æŒ‡æ ‡æå‡ï¼Œä¼˜äºç°æœ‰æœ€ä¼˜æ–¹æ³•ã€‚</li><li>OV-NeRF åœ¨å„ç§ CLIP é…ç½®ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šOV-NeRFï¼šå…·æœ‰è§†è§‰å’Œè¯­è¨€çš„å¼€æ”¾è¯æ±‡ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šå»–æ¡‚æ ‡ï¼Œå‘¨å‡¯æ™¨ï¼Œé²æŒ¯å®‡ï¼Œåˆ˜åº·æ—ï¼Œæåº†</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œå¼€æ”¾è¯æ±‡ï¼Œè¯­ä¹‰ç†è§£ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04648</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥æ•æ‰å¤æ‚çœŸå®ä¸–ç•Œçš„ 3D åœºæ™¯ã€‚ç„¶è€Œï¼Œåœ¨å¼€æ”¾è¯æ±‡ 3D è¯­ä¹‰æ„ŸçŸ¥ä»»åŠ¡ä¸­å®ç°å…¨é¢çš„ 3D è¯­ä¹‰ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ç›´æ¥ä»å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­æå–è¯­ä¹‰ï¼Œç”¨äºè¯­ä¹‰åœºå­¦ä¹ ï¼Œä½†é‡åˆ°äº†æ¥è‡ª CLIP çš„å˜ˆæ‚å’Œè§†å›¾ä¸ä¸€è‡´è¯­ä¹‰çš„å›°éš¾ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº† OV-NeRFï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œé€šè¿‡æå‡ºçš„å•è§†å›¾å’Œè·¨è§†å›¾ç­–ç•¥æ¥å¢å¼ºè¯­ä¹‰åœºå­¦ä¹ ã€‚(4) æ€§èƒ½è¡¨ç°ï¼šOV-NeRF åœ¨ Replica å’Œ Scannet ä¸Šåˆ†åˆ«åœ¨ mIoU åº¦é‡ä¸­å–å¾—äº† 20.31% å’Œ 18.42% çš„æ˜¾ç€æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§ CLIP é…ç½®ä¸­è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜å¼‚ç»“æœï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶é²æ£’æ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡º OV-NeRFï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œé€šè¿‡æå‡ºçš„å•è§†å›¾å’Œè·¨è§†å›¾ç­–ç•¥æ¥å¢å¼ºè¯­ä¹‰åœºå­¦ä¹ ã€‚(2) åˆ©ç”¨é¢„å…ˆè®¡ç®—çš„ CLIP ç‰¹å¾å’Œ SAM çš„åŒºåŸŸæè®®æ¥ç”Ÿæˆç²¾ç¡®çš„ç›¸å…³æ€§å›¾ï¼Œä»¥ç›‘ç£ OV-NeRFï¼Œè€Œä¸æ˜¯ä½¿ç”¨æºè‡ª CLIP æ¨¡å‹çš„åŸå§‹å™ªå£°ç›¸å…³æ€§å›¾ã€‚(3) åœ¨è®­ç»ƒ OV-NeRF æ•°ä¸ª epoch åï¼Œåˆ©ç”¨ä» OV-NeRF è·å¾—çš„æ¸²æŸ“ä¼ªè¾“å‡ºï¼ŒåŒ…æ‹¬è®­ç»ƒè§†å›¾å’Œæœªè§æ–°é¢–è§†å›¾ï¼Œç”¨äºè·¨è§†å›¾è‡ªæˆ‘å¢å¼ºç›‘ç£ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæå‡º OV-NeRF æ¥è§£å†³åŸºäº NeRF çš„ 3D è¯­ä¹‰ç†è§£æŒ‘æˆ˜ã€‚åœ¨ OV-NeRF ä¸­ï¼Œæå‡ºçš„åŒºåŸŸè¯­ä¹‰æ’åºï¼ˆRSRï¼‰æ­£åˆ™åŒ–äº§ç”Ÿç²¾ç¡®çš„å•è§†å›¾ç›¸å…³æ€§å›¾æ¥è®­ç»ƒ OV-NeRFï¼Œè·¨è§†å›¾è‡ªæˆ‘å¢å¼ºç¡®ä¿è§†å›¾ä¸€è‡´çš„åˆ†å‰²ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šä»¥å¾ˆå¤§ä¼˜åŠ¿ä¼˜äº SOTA æ–¹æ³•ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„ CLIP é…ç½®ä¸­å§‹ç»ˆè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè‚¯å®šäº†å…¶é€šç”¨æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ NeRF æ¨¡å‹ OV-NeRFï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œé€šè¿‡æå‡ºçš„å•è§†å›¾å’Œè·¨è§†å›¾ç­–ç•¥æ¥å¢å¼ºè¯­ä¹‰åœºå­¦ä¹ ã€‚æå‡ºäº†ä¸€ç§æ–°çš„åŒºåŸŸè¯­ä¹‰æ’åºï¼ˆRSRï¼‰æ­£åˆ™åŒ–ï¼Œè¯¥æ­£åˆ™åŒ–äº§ç”Ÿç²¾ç¡®çš„å•è§†å›¾ç›¸å…³æ€§å›¾æ¥è®­ç»ƒ OV-NeRFã€‚æå‡ºäº†ä¸€ç§æ–°çš„è·¨è§†å›¾è‡ªæˆ‘å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä» OV-NeRF è·å¾—çš„æ¸²æŸ“ä¼ªè¾“å‡ºï¼ŒåŒ…æ‹¬è®­ç»ƒè§†å›¾å’Œæœªè§æ–°é¢–è§†å›¾ï¼Œç”¨äºè·¨è§†å›¾è‡ªæˆ‘å¢å¼ºç›‘ç£ã€‚æ€§èƒ½ï¼šOV-NeRF åœ¨ Replica å’Œ Scannet ä¸Šåˆ†åˆ«åœ¨ mIoU åº¦é‡ä¸­å–å¾—äº† 20.31% å’Œ 18.42% çš„æ˜¾ç€æ”¹è¿›ã€‚è¯¥æ–¹æ³•åœ¨å„ç§ CLIP é…ç½®ä¸­è¡¨ç°å‡ºä¸€è‡´çš„ä¼˜å¼‚ç»“æœï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶é²æ£’æ€§ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢„å…ˆè®¡ç®— CLIP ç‰¹å¾å’Œ SAM çš„åŒºåŸŸæè®®ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•éœ€è¦è®­ç»ƒå¤šä¸ª epochï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d28a855be0d118e883bd9f8001dbbcd1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1c6219c40ef2be88e25422dda1aae264.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fece26674b484110bc1b8871018a6a3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ea1c3e14317a591427313451f7980698.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a1bd1a1dc370f614b943567738833593.jpg" align="middle"></details><h2 id="GSN-Generalisable-Segmentation-in-Neural-Radiance-Field"><a href="#GSN-Generalisable-Segmentation-in-Neural-Radiance-Field" class="headerlink" title="GSN: Generalisable Segmentation in Neural Radiance Field"></a>GSN: Generalisable Segmentation in Neural Radiance Field</h2><p><strong>Authors:Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan</strong></p><p>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: <a href="https://vinayak-vg.github.io/GSN/">https://vinayak-vg.github.io/GSN/</a> </p><p><a href="http://arxiv.org/abs/2402.04632v1">PDF</a> Accepted at the Main Technical Track of AAAI 2024</p><p><strong>Summary</strong><br>åˆ©ç”¨å‡ ä¸ªè§†å›¾å°±å¯ä»¥æ¸²æŸ“æœªçŸ¥ä¸”æœªè®­ç»ƒåœºæ™¯çš„æ–°è§†å›¾ï¼Œå¹¶æä¾›ä¸€è‡´çš„é€åƒç´ è¯­ä¹‰ç‰¹å¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¼ ç»Ÿè¾å°„åœºè¡¨ç¤ºæ•è·ç‰¹å®šåœºæ™¯çš„ç»†èŠ‚ï¼Œå¿…é¡»åœ¨æ¯ä¸ªåœºæ™¯ä¸Šé‡æ–°è®­ç»ƒã€‚</li><li>è¯­ä¹‰ç‰¹å¾å­—æ®µå·²æ·»åŠ åˆ°å°„é¢‘ä¸­ä»¥ä¿ƒè¿›å¤šé¡¹åˆ†å‰²ä»»åŠ¡ã€‚</li><li>å¹¿ä¹‰å°„é¢‘è¡¨ç¤ºå­¦ä¹ äº†è§†å›¾æ’å€¼åŸç†ã€‚</li><li>ç»™å®šå‡ ä¸ªè§†å›¾ï¼Œå¹¿ä¹‰å°„é¢‘å¯ä»¥æ¸²æŸ“æœªçŸ¥ä¸”æœªè®­ç»ƒåœºæ™¯çš„æ–°è§†å›¾ã€‚</li><li>æˆ‘ä»¬æä¾›äº†ä¸€ç§å°†ç‰¹å¾å­—æ®µæç‚¼æˆå¹¿ä¹‰ GNT è¡¨ç¤ºçš„æ–¹æ³•ã€‚</li><li>æˆ‘ä»¬çš„ GSN è¡¨ç¤ºå¯ä»¥å¿«é€Ÿç”Ÿæˆæœªè§åœºæ™¯çš„æ–°è§†å›¾ï¼Œå¹¶æä¾›ä¸€è‡´çš„é€åƒç´ è¯­ä¹‰ç‰¹å¾ã€‚</li><li>è¿™å…è®¸å¯¹ä»»æ„æ–°åœºæ™¯è¿›è¡Œå¤šè§†å›¾åˆ†å‰²ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGSNï¼šç¥ç»è¾å°„åœºä¸­çš„å¯æ³›åŒ–åˆ†å‰²</li><li>ä½œè€…ï¼šVinayak Guptaï¼ŒRahul Goelï¼ŒSirikonda Dhawalï¼ŒP.J. Narayanan</li><li>éš¶å±æœºæ„ï¼šå°åº¦ç†å·¥å­¦é™¢é©¬å¾·æ‹‰æ–¯åˆ†æ ¡</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å¯æ³›åŒ–åˆ†å‰²ã€è¯­ä¹‰ç‰¹å¾åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04632   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¼ ç»Ÿçš„ç¥ç»è¾å°„åœºï¼ˆRFï¼‰è¡¨ç¤ºå¯ä»¥æ•æ‰ç‰¹å®šåœºæ™¯çš„ç»†èŠ‚ï¼Œä½†å¿…é¡»é’ˆå¯¹æ¯ä¸ªåœºæ™¯é‡æ–°è®­ç»ƒã€‚è¯­ä¹‰ç‰¹å¾åœºå·²è¢«æ·»åŠ åˆ° RF ä¸­ä»¥ä¿ƒè¿›å¤šé¡¹åˆ†å‰²ä»»åŠ¡ã€‚æ³›åŒ–çš„ RF è¡¨ç¤ºå­¦ä¹ è§†å›¾æ’å€¼åŸç†ã€‚ç»™å®šå‡ ä¸ªè§†å›¾ï¼Œæ³›åŒ–çš„ RF å¯ä»¥æ¸²æŸ“æœªçŸ¥ä¸”æœªè®­ç»ƒåœºæ™¯çš„æ–°è§†å›¾ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†ç‰¹å¾åœºæç‚¼åˆ°æ³›åŒ–çš„ GNT è¡¨ç¤ºä¸­çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ GSN è¡¨ç¤ºå¯ä»¥å³æ—¶ç”Ÿæˆæœªè§åœºæ™¯çš„æ–°è§†å›¾ï¼ŒåŒæ—¶æä¾›ä¸€è‡´çš„é€åƒç´ è¯­ä¹‰ç‰¹å¾ã€‚è¿™ä½¿å¾—ä»»æ„æ–°åœºæ™¯çš„å¤šè§†å›¾åˆ†å‰²æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†ä¸åŒè¯­ä¹‰ç‰¹å¾æå–åˆ°æ³›åŒ–çš„ RF ä¸­ã€‚æˆ‘ä»¬çš„å¤šè§†å›¾åˆ†å‰²ç»“æœä¸ä½¿ç”¨ä¼ ç»Ÿ RF çš„æ–¹æ³•ç›¸å½“ã€‚GSN æ˜¾ç€ç¼©å°äº†æ ‡å‡† RF æ–¹æ³•å’Œå¯æ³›åŒ– RF æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šè¿‡å»çš„ RF è¡¨ç¤ºå­¦ä¹ ç‰¹å®šåœºæ™¯çš„ç»†èŠ‚ï¼Œå¿…é¡»é’ˆå¯¹æ¯ä¸ªåœºæ™¯é‡æ–°è®­ç»ƒã€‚è¯­ä¹‰ç‰¹å¾åœºå·²è¢«æ·»åŠ åˆ° RF ä¸­ä»¥ä¿ƒè¿›å¤šé¡¹åˆ†å‰²ä»»åŠ¡ã€‚æ³›åŒ–çš„ RF è¡¨ç¤ºå­¦ä¹ è§†å›¾æ’å€¼åŸç†ã€‚ç»™å®šå‡ ä¸ªè§†å›¾ï¼Œæ³›åŒ–çš„ RF å¯ä»¥æ¸²æŸ“æœªçŸ¥ä¸”æœªè®­ç»ƒåœºæ™¯çš„æ–°è§†å›¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ç‰¹å¾åœºæç‚¼åˆ°æ³›åŒ–çš„ GNT è¡¨ç¤ºä¸­çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ GSN è¡¨ç¤ºå¯ä»¥å³æ—¶ç”Ÿæˆæœªè§åœºæ™¯çš„æ–°è§†å›¾ï¼ŒåŒæ—¶æä¾›ä¸€è‡´çš„é€åƒç´ è¯­ä¹‰ç‰¹å¾ã€‚è¿™ä½¿å¾—ä»»æ„æ–°åœºæ™¯çš„å¤šè§†å›¾åˆ†å‰²æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†ä¸åŒè¯­ä¹‰ç‰¹å¾æå–åˆ°æ³›åŒ–çš„ RF ä¸­ã€‚æˆ‘ä»¬çš„å¤šè§†å›¾åˆ†å‰²ç»“æœä¸ä½¿ç”¨ä¼ ç»Ÿ RF çš„æ–¹æ³•ç›¸å½“ã€‚GSN æ˜¾ç€ç¼©å°äº†æ ‡å‡† RF æ–¹æ³•å’Œå¯æ³›åŒ– RF æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚ï¼ˆ4ï¼‰æ–¹æ³•çš„æ€§èƒ½ï¼šæˆ‘ä»¬çš„å¤šè§†å›¾åˆ†å‰²ç»“æœä¸ä½¿ç”¨ä¼ ç»Ÿ RF çš„æ–¹æ³•ç›¸å½“ã€‚GSN æ˜¾ç€ç¼©å°äº†æ ‡å‡† RF æ–¹æ³•å’Œå¯æ³›åŒ– RF æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¦–å…ˆï¼Œæˆ‘ä»¬ä¿®æ”¹ GNT æ¶æ„ä»¥å¸®åŠ©è¯­ä¹‰ç‰¹å¾æå–ã€‚ï¼ˆ2ï¼‰ç„¶åï¼Œæˆ‘ä»¬æè¿°äº†æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒè’¸é¦è¿‡ç¨‹ã€‚ï¼ˆ3ï¼‰æœ€åï¼Œæˆ‘ä»¬æè¿°äº†å¦‚ä½•ä½¿ç”¨è’¸é¦ç‰¹å¾æ‰§è¡Œå¤šè§†å›¾åˆ†å‰²ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†å›¾åˆ†å‰²æ–¹æ³•ï¼Œå…¶ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶æ³›åŒ–æ€§ï¼Œå³å®ƒå¯ä»¥åœ¨ä»»æ„æ–°åœºæ™¯ä¸Šæ‰§è¡Œåˆ†å‰²è€Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚è¿™ä½¿å…¶åŒºåˆ«äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç»“æœä¸æ—©æœŸæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ€§èƒ½ä¸å®ƒä»¬ç›¸å½“ï¼ŒåŒæ—¶å¯ä»¥æ³›åŒ–åˆ°æœªè§åœºæ™¯ã€‚è¿™æ˜¯å°†æ³›åŒ–ç¥ç»è¾å°„åœºçš„åº”ç”¨æ‹‰è¿‘åˆ°ç‰¹å®šåœºæ™¯è¾å°„åœºçš„ä¸€å¤§æ­¥ã€‚æˆ‘ä»¬æ–¹æ³•é¢„æµ‹çš„ç‰¹å¾å¯ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†å°†ç‰¹å¾åœºæç‚¼åˆ°æ³›åŒ–çš„ GNT è¡¨ç¤ºä¸­çš„æ–¹æ³•ï¼Œè¯¥è¡¨ç¤ºå¯ä»¥å³æ—¶ç”Ÿæˆæœªè§åœºæ™¯çš„æ–°è§†å›¾ï¼ŒåŒæ—¶æä¾›ä¸€è‡´çš„é€åƒç´ è¯­ä¹‰ç‰¹å¾ã€‚æ€§èƒ½ï¼šæˆ‘ä»¬çš„å¤šè§†å›¾åˆ†å‰²ç»“æœä¸ä½¿ç”¨ä¼ ç»Ÿ RF çš„æ–¹æ³•ç›¸å½“ã€‚GSN æ˜¾ç€ç¼©å°äº†æ ‡å‡† RF æ–¹æ³•å’Œå¯æ³›åŒ– RF æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚å·¥ä½œé‡ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºåŸºäº transformer çš„æ¶æ„ï¼Œå› æ­¤æ¸²æŸ“è¿‡ç¨‹ä¸å‡ ç§ç‰¹å®šåœºæ™¯çš„è¾å°„åœºæ–¹æ³•ç›¸æ¯”å›ºæœ‰åœ°ç¼“æ…¢ã€‚æé«˜æ¸²æŸ“é€Ÿåº¦å¯ä»¥æ˜¾ç€æ”¹å–„æˆ‘ä»¬åŸºäºç¬”åˆ’çš„åˆ†å‰²æ–¹æ³•æ‰€éœ€çš„äººæœºäº¤äº’ä½“éªŒã€‚æˆ‘ä»¬å°†æ³›åŒ–è¾å°„åœºçš„æ¸²æŸ“é€Ÿåº¦æ”¹è¿›ç•™ä½œæœªæ¥çš„å·¥ä½œã€‚ç›®å‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰§è¡Œå¤šè§†å›¾åˆ†å‰²ï¼Œå› ä¸ºå®ƒä½¿ç”¨åŸºäºå›¾åƒçš„æ¸²æŸ“ã€‚æŸäº›åº”ç”¨ç¨‹åºéœ€è¦ 3D åˆ†å‰²è€Œä¸æ˜¯å¤šè§†å›¾åˆ†å‰²ã€‚å› æ­¤ï¼Œå¯æ³›åŒ–çš„ 3D åˆ†å‰²æ¡†æ¶æœ‰æœ›æˆä¸ºæœªæ¥çš„å·¥ä½œã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-bf67c21104c6d20a1d6e37e83bff2155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03400222552085971945e9fc363dc323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61621673ca99816fe4332d9623a7e1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c82ea98993102ebb08c3d96886f8caf8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d9b9a069535dfb6e09a7654648b4f040.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é¸Ÿç° NeRFï¼šåŸºäºç¥ç»è¾å°„åœºçš„å¤§è§„æ¨¡åœºæ™¯é‡å»ºã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯é‡å»ºï¼Œæå‡ºäº†åŸºäºç¥ç»è¾å°„åœºçš„ BirdNeRF ç®—æ³•ã€‚</li><li>BirdNeRF å°†å¤§åœºæ™¯å›¾åƒé›†åˆ†è§£ä¸ºå¤šä¸ªå°é›†åˆï¼Œæ¯ä¸ªå°é›†åˆè®­ç»ƒå•ç‹¬çš„ NeRF æ¨¡å‹ã€‚</li><li>è¿™ç§åˆ†è§£æ–¹æ³•å°†æ¸²æŸ“æ—¶é—´ä¸åœºæ™¯å¤§å°è§£è€¦ï¼Œå¹¶ä½¿æ¸²æŸ“èƒ½å¤Ÿæ— ç¼æ‰©å±•åˆ°ä»»æ„å¤§çš„ç¯å¢ƒã€‚</li><li>æ­¤å¤–ï¼Œå®ƒå…è®¸å¯¹ç¯å¢ƒè¿›è¡Œé€å—æ›´æ–°ï¼Œä»è€Œæé«˜é‡å»ºè¿‡ç¨‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºæŠ•å½±çš„æ–°é¢–è§†è§’é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œæœ‰åŠ©äºæœ‰æ•ˆåˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯ç”Ÿæˆæ›´å¥½çš„æ¸²æŸ“ç»“æœã€‚</li><li>åœ¨ç°æœ‰æ•°æ®é›†ä»¥åŠæˆ‘ä»¬è‡ªå·±çš„æ— äººæœºèˆªæ‹è§†é¢‘ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å•ä¸ª GPU ä¸Šå°†é‡å»ºé€Ÿåº¦æé«˜äº† 10 å€ï¼ˆç›¸å¯¹äºç»å…¸æ‘„å½±æµ‹é‡è½¯ä»¶ï¼‰å’Œ 50 å€ï¼ˆç›¸å¯¹äºæœ€å…ˆè¿›çš„å¤§è§„æ¨¡ NeRF è§£å†³æ–¹æ¡ˆï¼‰ï¼ŒåŒæ—¶æ¸²æŸ“è´¨é‡ç›¸ä¼¼ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé¸Ÿç°ç¥ç»è¾å°„åœºï¼šä½¿ç”¨èˆªæ‹å›¾åƒå¿«é€Ÿç¥ç»é‡å»ºå¤§åœºæ™¯</li><li>ä½œè€…ï¼šå¼ æ…§æ¸…ã€è–›ä¸€é£ã€å»–æ˜ã€è€ä¸€çœŸ</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å¤§è§„æ¨¡é‡å»ºã€èˆªæ‹å›¾åƒã€ç©ºé—´åˆ†è§£ã€æŠ•å½±å¼•å¯¼</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04554Githubï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šéšç€èˆªç©ºæµ‹é‡æŠ€æœ¯çš„è¿›æ­¥ï¼Œè·å–é«˜åˆ†è¾¨ç‡å›¾åƒå˜å¾—æ›´åŠ å®¹æ˜“å’Œç»æµå®æƒ ï¼ŒåŸºäºå›¾åƒçš„ 3D é‡å»ºå·²æˆä¸ºä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œå¹¶åœ¨åŸå¸‚è§„åˆ’ã€å¯¼èˆªã€è™šæ‹Ÿæ—…æ¸¸ã€æˆ¿åœ°äº§å’Œç¾å®³ç®¡ç†ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚(2)ï¼šç°æœ‰çš„åŸºäºå›¾åƒçš„ 3D é‡å»ºæŠ€æœ¯ä¸»è¦åˆ†ä¸ºä¼ ç»Ÿçš„åŸºäºå‡ ä½•çš„æ–¹æ³•å’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ã€‚åŸºäºå‡ ä½•çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„äººå·¥å¹²é¢„ï¼Œå¹¶ä¸”å¯¹å›¾åƒçš„è´¨é‡å’Œæ•°é‡éå¸¸æ•æ„Ÿã€‚åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœº (NeRF)ï¼Œå¯ä»¥è‡ªåŠ¨ä»å›¾åƒä¸­å­¦ä¹ åœºæ™¯çš„ 3D è¡¨ç¤ºï¼Œå¹¶ä¸”å¯¹å›¾åƒçš„è´¨é‡å’Œæ•°é‡ä¸å¤ªæ•æ„Ÿã€‚ç„¶è€Œï¼ŒNeRF åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶é¢ä¸´ç€è®­ç»ƒé€Ÿåº¦æ…¢ã€æ¸²æŸ“æ—¶é—´é•¿å’Œå®¹æ˜“äº§ç”Ÿä¼ªå½±ç­‰æŒ‘æˆ˜ã€‚(3)ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ NeRF å˜ä½“ï¼Œç§°ä¸ºé¸Ÿç°ç¥ç»è¾å°„åœº (BirdNeRF)ã€‚BirdNeRF ä½¿ç”¨äº†ä¸€ç§æ–°çš„ç©ºé—´åˆ†è§£ç®—æ³•ï¼Œå°†å¤§è§„æ¨¡èˆªæ‹å›¾åƒé›†åˆ†è§£æˆå¤šä¸ªè¾ƒå°çš„å­é›†ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå­é›†çš„ NeRF æ¨¡å‹ã€‚è¿™ç§åˆ†è§£æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“æ—¶é—´ï¼Œè¿˜å¯ä»¥æé«˜é‡å»ºçš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒBirdNeRF è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æŠ•å½±å¼•å¯¼çš„æ–°è§†å›¾é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯æ¥ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚(4)ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBirdNeRF åœ¨é‡å»ºé€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚åœ¨å•ä¸ª GPU ä¸Šï¼ŒBirdNeRF çš„é‡å»ºé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ‘„å½±æµ‹é‡è½¯ä»¶å¿« 10 å€ï¼Œæ¯”æœ€å…ˆè¿›çš„å¤§è§„æ¨¡ NeRF è§£å†³æ–¹æ¡ˆå¿« 50 å€ï¼ŒåŒæ—¶å…·æœ‰ç›¸ä¼¼çš„æ¸²æŸ“è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å°†å¤§åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªè¾ƒå°çš„å­åœºæ™¯ï¼Œåˆ†åˆ«è®­ç»ƒæ¯ä¸ªå­åœºæ™¯çš„ NeRF æ¨¡å‹ã€‚(2) ä½¿ç”¨ä¸€ç§æ–°çš„æŠ•å½±å¼•å¯¼çš„æ–°è§†å›¾é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯æ¥ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„NeRFå˜ä½“ï¼Œç§°ä¸ºé¸Ÿç°ç¥ç»è¾å°„åœºï¼ˆBirdNeRFï¼‰ï¼Œå¯ä»¥å¿«é€Ÿé‡å»ºå¤§è§„æ¨¡åœºæ™¯ã€‚BirdNeRFä½¿ç”¨äº†ä¸€ç§æ–°çš„ç©ºé—´åˆ†è§£ç®—æ³•ï¼Œå°†å¤§è§„æ¨¡èˆªæ‹å›¾åƒé›†åˆ†è§£æˆå¤šä¸ªè¾ƒå°çš„å­é›†ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå­é›†çš„NeRFæ¨¡å‹ã€‚è¿™ç§åˆ†è§£æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“æ—¶é—´ï¼Œè¿˜å¯ä»¥æé«˜é‡å»ºçš„è´¨é‡ã€‚æ­¤å¤–ï¼ŒBirdNeRFè¿˜æå‡ºäº†ä¸€ç§æ–°çš„æŠ•å½±å¼•å¯¼çš„æ–°è§†å›¾é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯æ¥ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„NeRFå˜ä½“ï¼Œç§°ä¸ºé¸Ÿç°ç¥ç»è¾å°„åœºï¼ˆBirdNeRFï¼‰ï¼Œå¯ä»¥å¿«é€Ÿé‡å»ºå¤§è§„æ¨¡åœºæ™¯ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ç©ºé—´åˆ†è§£ç®—æ³•ï¼Œå°†å¤§è§„æ¨¡èˆªæ‹å›¾åƒé›†åˆ†è§£æˆå¤šä¸ªè¾ƒå°çš„å­é›†ï¼Œå¹¶åˆ†åˆ«è®­ç»ƒæ¯ä¸ªå­é›†çš„NeRFæ¨¡å‹ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æŠ•å½±å¼•å¯¼çš„æ–°è§†å›¾é‡æ–°æ¸²æŸ“ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç‹¬ç«‹è®­ç»ƒçš„å­åœºæ™¯æ¥ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“ç»“æœã€‚</li></ol><p>æ€§èƒ½ï¼š- åœ¨å•ä¸ªGPUä¸Šï¼ŒBirdNeRFçš„é‡å»ºé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ‘„å½±æµ‹é‡è½¯ä»¶å¿«10å€ï¼Œæ¯”æœ€å…ˆè¿›çš„å¤§è§„æ¨¡NeRFè§£å†³æ–¹æ¡ˆå¿«50å€ï¼ŒåŒæ—¶å…·æœ‰ç›¸ä¼¼çš„æ¸²æŸ“è´¨é‡ã€‚- BirdNeRFå¯ä»¥é‡å»ºåŒ…å«æ•°ç™¾ä¸‡ä¸ªä¸‰è§’å½¢çš„åœºæ™¯ï¼Œè€Œä¸ä¼šå‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚</p><p>å·¥ä½œé‡ï¼š- BirdNeRFçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚- BirdNeRFçš„è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“æ—¶é—´éƒ½æ¯”è¾ƒçŸ­ï¼Œå¯ä»¥æ»¡è¶³å®é™…åº”ç”¨çš„éœ€æ±‚ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0ffe2746a28f7248c7dc45305ca5a0d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3e3e28cf5dd4b506a44e1769d5abf0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51dea38443c497692956a6fd50ec6a18.jpg" align="middle"></details>## ViewFusion: Learning Composable Diffusion Models for Novel View   Synthesis**Authors:Bernard Spiegl, Andrea Perin, StÃ©phane Deny, Alexander Ilin**Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available. [PDF](http://arxiv.org/abs/2402.02906v1) **Summary**å°†å¤šä¸ªä¸åŒè§†è§’çš„å›¾åƒè¾“å…¥åˆ° ViewFusion æ¨¡å‹ä¸­ï¼Œå°±å¯ä»¥åŸºäºè¿™äº›å›¾åƒåˆæˆå‡ºæ–°çš„è§†è§’å›¾åƒã€‚**Key Takeaways**- ViewFusion å°†æ‰©æ•£å»å™ªæ­¥éª¤åŒæ—¶åº”ç”¨äºä»»æ„æ•°é‡çš„åœºæ™¯è¾“å…¥è§†å›¾ï¼Œç„¶åå°†æ¯ä¸ªè§†å›¾è·å¾—çš„å™ªå£°æ¢¯åº¦ä¸åƒç´ æƒé‡æ©ç ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨ç›®æ ‡åœºæ™¯çš„æ¯ä¸ªåŒºåŸŸå†…ä»…è€ƒè™‘æœ€å…·ä¿¡æ¯æ€§çš„è¾“å…¥è§†å›¾ã€‚- ViewFusion è§£å†³äº†å…ˆå‰æ–¹æ³•çš„å‡ ä¸ªå±€é™æ€§ï¼šè·¨å¤šä¸ªåœºæ™¯å’Œå¯¹è±¡ç±»åˆ«è¿›è¡Œè®­ç»ƒå’Œæ³›åŒ–ï¼›åœ¨è®­ç»ƒå’Œæµ‹è¯•æ—¶è‡ªé€‚åº”åœ°é‡‡ç”¨å¯å˜æ•°é‡çš„ä¸å—å§¿åŠ¿é™åˆ¶çš„è§†å›¾ï¼›èƒ½å¤Ÿç”Ÿæˆåˆç†çš„è§†å›¾ï¼Œå³ä½¿åœ¨ä¸¥é‡ä¸ç¡®å®šçš„æ¡ä»¶ä¸‹ã€‚- ViewFusion ä¼˜äºæˆ–ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”èƒ½æ›´é«˜è´¨é‡åœ°ç”Ÿæˆè§†å›¾ã€‚- ViewFusion æ— æ³•ç”Ÿæˆåœºæ™¯çš„ 3D åµŒå…¥ï¼Œå¯¼è‡´å…¶æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚- ViewFusion ç›®å‰ä»…åœ¨ç›¸å¯¹è¾ƒå°çš„ NMR æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚- ä»£ç åº“ç°å·²å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šViewFusionï¼šç”¨äºæ–°é¢–è§†å›¾åˆæˆçš„å¯ç»„åˆæ‰©æ•£æ¨¡å‹çš„å­¦ä¹ </li><li>ä½œè€…ï¼šBernard Spieglã€Andrea Perinã€StÂ´ephane Denyã€Alexander Ilin</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿å°”æ‰˜å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»ï¼ˆä»…ç¿»è¯‘ä¸­æ–‡ï¼‰</li><li>å…³é”®è¯ï¼šæ–°é¢–è§†å›¾åˆæˆã€æ‰©æ•£æ¨¡å‹ã€å¯ç»„åˆæ€§ã€è‡ªé€‚åº”è¾“å…¥è§†å›¾ã€é²æ£’æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.02906ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–°é¢–è§†å›¾åˆæˆæ˜¯ä¸€ä¸ªè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é•¿æœŸç ”ç©¶è¯¾é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨æ˜¾å¼å»ºæ¨¡ 3D ç©ºé—´çš„æ–¹æ³•ï¼Œå¦‚ä½“ç´ ã€ç‚¹äº‘æˆ–ç½‘æ ¼ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»è¾å°„åœº (NeRF) çš„æ–¹æ³•ä¹Ÿå–å¾—äº†å¾ˆå¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨éœ€è¦æ˜‚è´µçš„é€åœºæ™¯é‡æ–°è®­ç»ƒã€æ— æ³•åœ¨æ²¡æœ‰è¾“å…¥è§†å›¾çš„å§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹æ“ä½œæˆ–æ— æ³•é€‚åº”æµ‹è¯•æ—¶è¾“å…¥è§†å›¾æ•°é‡çš„å¯å˜æ€§ç­‰ç¼ºç‚¹ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸å­˜åœ¨éœ€è¦æ˜‚è´µçš„é€åœºæ™¯é‡æ–°è®­ç»ƒã€æ— æ³•åœ¨æ²¡æœ‰è¾“å…¥è§†å›¾çš„å§¿æ€ä¿¡æ¯çš„æƒ…å†µä¸‹æ“ä½œæˆ–æ— æ³•é€‚åº”æµ‹è¯•æ—¶è¾“å…¥è§†å›¾æ•°é‡çš„å¯å˜æ€§ç­‰ç¼ºç‚¹ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§ç›´è§‚çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œç”¨äºæ‰§è¡Œæ–°é¢–è§†å›¾åˆæˆï¼ŒåŒæ—¶è§£å†³å…ˆå‰å·¥ä½œä¸­æåˆ°çš„ç¼ºç‚¹ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ ViewFusion æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—é’ˆå¯¹ç‰¹å®šé—®é¢˜çš„è®¾è®¡é€‰æ‹©ï¼Œä¸€æ¬¡æ€§è§£å†³äº†ä¸Šè¿°ç¼ºç‚¹ã€‚é¦–å…ˆï¼Œä½¿ç”¨åœ¨å¤§é‡åœºæ™¯å’Œç±»åˆ«ä¸ŠåŒæ—¶è®­ç»ƒçš„æ‰©æ•£æ¦‚ç‡æ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œæ³›åŒ–ã€‚æ­¤å¤–ï¼Œç”±äºæ‰©æ•£è¿‡ç¨‹çš„éšæœºæ€§è´¨ï¼Œè¯¥æ¨¡å‹å³ä½¿åœ¨ä¸ç¡®å®šæ€§è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡çš„ä¸¥é‡é®æŒ¡æˆ–æœ‰é™æ•°é‡çš„è¾“å…¥è§†å›¾ï¼‰ä¸­ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œå› ä¸ºå®ƒæä¾›äº†å¤šç§åˆç†çš„è§†å›¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆä¸éœ€è¦è¾“å…¥è§†å›¾çš„é¡ºåºæˆ–ä»»ä½•æ˜¾å¼å§¿æ€ä¿¡æ¯ã€‚æœ€åï¼Œä¸ä¹‹å‰çš„å¯¹åº”æ–¹æ³•ä¸åŒï¼Œä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæœ¬æ–‡çš„æ–¹æ³•å°±å¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥ã€‚è¿™è¦å½’åŠŸäºä¸€ç§æ–°çš„åŠ æƒè§£å†³æ–¹æ¡ˆï¼Œä¸å»å™ªéª¨å¹²ç½‘ç»œçš„ç»„åˆä¸€èµ·ï¼Œè¯¥è§£å†³æ–¹æ¡ˆå…è®¸æ¨¡å‹æ ¹æ®è§†å›¾çš„ä¿¡æ¯é‡å¯¹è§†å›¾è¿›è¡ŒåŠ æƒï¼ŒåŒæ—¶æ‰©å±•åˆ°ä»»æ„æ•°é‡çš„è§†å›¾ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡åœ¨åŒ…å«å„ç§ç±»åˆ«å’Œè¾“å…¥è§†å›¾å§¿åŠ¿çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡å¯¹ä¸­é—´æ¨¡å‹è¾“å‡ºçš„åˆ†æéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹èƒ½å¤Ÿæ¨æ–­å’Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªè¾“å…¥è§†å›¾çš„é‡è¦æ€§ã€‚åŠ æƒä¸ä»…å¯¹è¾“å‡ºçš„è´¨é‡æœ‰å½±å“ï¼Œè€Œä¸”æ¨æ–­çš„åŠ æƒæ–¹æ¡ˆä¹Ÿä¸ç›´è§‚çš„äººç±»æ„ŸçŸ¥ä¸€è‡´ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¦‚ç‡æ¡†æ¶çš„æ–°é¢–è§†å›¾åˆæˆæ–¹æ³• ViewFusionï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥è§†å›¾ï¼Œå¹¶æ ¹æ®æ¯ä¸ªè§†å›¾çš„é‡è¦æ€§å¯¹è§†å›¾è¿›è¡ŒåŠ æƒï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ã€‚(2) ViewFusion æ¨¡å‹ç”±å¤šä¸ª U-Net ç»„æˆï¼Œæ¯ä¸ª U-Net è´Ÿè´£å¤„ç†ä¸€ä¸ªè¾“å…¥è§†å›¾ã€‚U-Net çš„è¾“å‡ºåŒ…æ‹¬å™ªå£°é¢„æµ‹å’Œæƒé‡ï¼Œæƒé‡ç”¨äºå¯¹å™ªå£°è¿›è¡ŒåŠ æƒï¼Œä»è€Œç”Ÿæˆæœ€ç»ˆçš„æ–°é¢–è§†å›¾ã€‚(3) ViewFusion æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨å¤§é‡åœºæ™¯å’Œç±»åˆ«ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ä¸€èˆ¬åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯æˆ–ç±»åˆ«ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚(4) ViewFusion æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå™ªå£°é‡‡æ ·å’Œæ‰©æ•£è¿‡ç¨‹ã€‚åœ¨å™ªå£°é‡‡æ ·é˜¶æ®µï¼Œæ¨¡å‹ä»æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·å™ªå£°ã€‚åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡é€æ¸é™ä½å™ªå£°çš„å¼ºåº¦æ¥ç”Ÿæˆæ–°é¢–è§†å›¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šViewFusion æ˜¯ä¸€ç§çµæ´»ã€æ— éœ€å§¿æ€çš„ç”Ÿæˆæ–¹æ³•ï¼Œå¯ä½¿ç”¨å¯ç»„åˆæ‰©æ•£æ¨¡å‹æ‰§è¡Œæ–°é¢–è§†å›¾åˆæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ æƒæ–¹æ¡ˆï¼Œç”¨äºç»„åˆæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ä»…å°†ä¿¡æ¯é‡æœ€å¤§çš„è¾“å…¥è§†å›¾ç”¨äºé¢„æµ‹ç›®æ ‡è§†å›¾ï¼Œå¹¶ä½¿ ViewFusion èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†ä»»æ„é•¿ä¸”æ— åºçš„è¾“å…¥è§†å›¾é›†åˆï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼ŒViewFusion çš„ç”Ÿæˆæ€§è´¨ä½¿å…¶å³ä½¿åœ¨ä¸¥é‡æ¬ å®šæ¡ä»¶ä¸‹ä¹Ÿèƒ½ç”Ÿæˆåˆç†è§†å›¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆæ—¶æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„è´¡çŒ®ï¼Œå¹¶ä¸”æœ‰å¯èƒ½åº”ç”¨äºå…¶ä»–é—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šViewFusion å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŠ æƒæ–¹æ¡ˆï¼Œç”¨äºç»„åˆæ‰©æ•£æ¨¡å‹ï¼Œç¡®ä¿ä»…å°†ä¿¡æ¯é‡æœ€å¤§çš„è¾“å…¥è§†å›¾ç”¨äºé¢„æµ‹ç›®æ ‡è§†å›¾ï¼Œå¹¶ä½¿ ViewFusion èƒ½å¤Ÿè‡ªé€‚åº”åœ°å¤„ç†ä»»æ„é•¿ä¸”æ— åºçš„è¾“å…¥è§†å›¾é›†åˆï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼ŒViewFusion çš„ç”Ÿæˆæ€§è´¨ä½¿å…¶å³ä½¿åœ¨ä¸¥é‡æ¬ å®šæ¡ä»¶ä¸‹ä¹Ÿèƒ½ç”Ÿæˆåˆç†è§†å›¾ã€‚æ€§èƒ½ï¼šViewFusion åœ¨å„ç§ç±»åˆ«å’Œè¾“å…¥è§†å›¾å§¿åŠ¿çš„æ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒViewFusion èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ä»»æ„æ•°é‡çš„è¾“å…¥è§†å›¾ï¼Œå¹¶ä¸”å¯¹è¾“å…¥è§†å›¾çš„é¡ºåºå’Œå§¿æ€ä¿¡æ¯ä¸æ•æ„Ÿã€‚å·¥ä½œé‡ï¼šViewFusion çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚é¢„è®­ç»ƒé˜¶æ®µéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œä½†å¾®è°ƒé˜¶æ®µå¯ä»¥ç›¸å¯¹å¿«é€Ÿåœ°å®Œæˆã€‚ViewFusion çš„æ¨ç†è¿‡ç¨‹ä¹Ÿéå¸¸æœ‰æ•ˆï¼Œå¯ä»¥åœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆæ–°é¢–è§†å›¾ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-672b204f9242001f6ba5e1b350c81c87.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab2417ac343ade4b32aea1621299f294.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a35b2635715a736813769f26b2939948.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-512893851e477e6cab6fb9d3224f7acf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fa2f794caefa6d02e53b7a03fc9f646.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5e41e289131352d483b38fb05ca0ce8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1874aa5e890d55cc56f18c742397f3bf.jpg" align="middle"></details><h2 id="Robust-Inverse-Graphics-via-Probabilistic-Inference"><a href="#Robust-Inverse-Graphics-via-Probabilistic-Inference" class="headerlink" title="Robust Inverse Graphics via Probabilistic Inference"></a>Robust Inverse Graphics via Probabilistic Inference</h2><p><strong>Authors:Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</strong></p><p>How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks. </p><p><a href="http://arxiv.org/abs/2402.01915v1">PDF</a> </p><p><strong>Summary</strong><br>æ–°é¢–çš„è´å¶æ–¯æ–¹æ³• RIG å¯åŒæ—¶å¯¹åœºæ™¯å’Œç ´åè¿›è¡Œæ¨ç†ï¼Œä»¥å…‹æœå„ç§åœºæ™¯æŸåã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RIG æ˜¯ä¸€ç§æ–°çš„è´å¶æ–¯æ–¹æ³•ï¼Œå¯åŒæ—¶å¯¹åœºæ™¯å’Œç ´åè¿›è¡Œæ¨ç†ã€‚</li><li>RIG ä»…ä½¿ç”¨å¹²å‡€çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¼˜äºæ·±åº¦ä¼°è®¡å™¨å’Œæ›¿ä»£çš„ NeRF æ–¹æ³•ã€‚</li><li>RIG å¯ä¸å¤šç§åŸºäºæ­£åˆ™åŒ–æµå’Œæ‰©æ•£æ¨¡å‹çš„åœºæ™¯å…ˆéªŒæ¶æ„ä¸€èµ·ä½¿ç”¨ã€‚</li><li>å¯¹äºåè€…ï¼Œæˆ‘ä»¬å¼€å‘äº†å…·æœ‰è¾…åŠ©æ½œå˜é‡çš„é‡å»ºæŒ‡å¯¼ï¼ˆReGALï¼‰â€”â€”ä¸€ç§æ‰©æ•£è°ƒèŠ‚ç®—æ³•ï¼Œé€‚ç”¨äºå…·æœ‰è¾…åŠ©æ½œå˜é‡ï¼ˆå¦‚ç ´åï¼‰çš„æƒ…å†µã€‚</li><li>RIG æ¼”ç¤ºäº†åœºæ™¯å…ˆéªŒå¦‚ä½•ç”¨äºç”Ÿæˆä»»åŠ¡ä¹‹å¤–ã€‚</li><li>RIG åˆ©ç”¨å¼ºå¤§çš„åœºæ™¯å…ˆéªŒå’Œæ— ä¿¡æ¯çš„å‡åŒ€ç ´åå…ˆéªŒï¼Œä½¿å…¶é€‚ç”¨äºå¹¿æ³›çš„ç ´åã€‚</li><li>åœ¨ç»™å®šå•ä¸€å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒRIG å¯¹åœºæ™¯å’Œç ´åè¿›è¡ŒåéªŒæ¨ç†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé²æ£’é€†å‘å›¾å½¢ç”Ÿæˆï¼šåŸºäºæ¦‚ç‡æ¨ç†</li><li>ä½œè€…ï¼šTuan Anh Leã€Pavel Sountsovã€Matthew D. Hoffmanã€Ben Leeã€Brian Pattonã€Rif A. Saurous</li><li>å•ä½ï¼šè°·æ­Œï¼ˆGoogleï¼‰</li><li>å…³é”®è¯ï¼šé²æ£’é€†å‘å›¾å½¢ç”Ÿæˆã€ç¥ç»è¾å°„åœºã€æ¦‚ç‡æ¨ç†ã€åŸŸéšæœºåŒ–ã€æ•°æ®å¢å¼º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.01915</li><li>æ‘˜è¦ï¼š</li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š    * åœ¨å­˜åœ¨é›¨ã€é›ªã€é›¾ç­‰å¹²æ‰°çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä»å•å¼ å›¾åƒä¸­æ¨æ–­å‡º 3D åœºæ™¯ï¼Ÿ    * ç›´æ¥çš„åŸŸéšæœºåŒ–ä¾èµ–äºæå‰çŸ¥é“å¹²æ‰°çš„ç§ç±»ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š    * åŸŸéšæœºåŒ–ï¼šé€šè¿‡åœ¨æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­é€‰æ‹©ä¸€ç³»åˆ—å¹²æ‰°æ¥å®ç°é²æ£’æ€§ï¼Œä½†è¿™ç§æ–¹æ³•éœ€è¦æå‰çŸ¥é“å¹²æ‰°çš„ç§ç±»ã€‚    * æ­£åˆ™åŒ–è®­ç»ƒï¼šé€šè¿‡åœ¨é‡å»ºæŸå¤±ä¸­æ·»åŠ é¢å¤–çš„æŸå¤±é¡¹æ¥å®ç°é²æ£’æ€§ï¼Œä½†è¿™ç§æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°æ›´æç«¯çš„æƒ…å†µã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š    * é²æ£’é€†å‘å›¾å½¢ç”Ÿæˆï¼ˆRIGï¼‰ï¼šå°†é—®é¢˜è§†ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åœºæ™¯å…ˆéªŒï¼ˆåœ¨æœ¬ä¾‹ä¸­æ˜¯ç¥ç»è¾å°„åœºå…ˆéªŒï¼‰å’Œä¸€ä¸ªå…³äºå¹²æ‰°çš„å¼±å…ˆéªŒï¼ˆåœ¨æœ¬ä¾‹ä¸­æ˜¯å…·æœ‰å‡åŒ€å…ˆéªŒæƒé‡çš„å¦ä¸€ä¸ªç¥ç»è¾å°„åœºï¼‰æ¥è¿›è¡Œæ¨ç†ã€‚    * RIG åœ¨åœºæ™¯å’Œå¹²æ‰°ç¥ç»è¾å°„åœºä¸­æ‰§è¡Œå®Œæ•´çš„æ¦‚ç‡æ¨ç†ï¼Œè€Œä¸æ˜¯åƒæœ€å¤§åéªŒæ¦‚ç‡æ¨ç†é‚£æ ·å¯»æ‰¾æœ€å¯èƒ½çš„è§£ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ï¼š    * RIG åœ¨å…·æœ‰å„ç§åœºæ™¯å…ˆéªŒæ¶æ„ï¼ˆåŸºäºæ­£åˆ™åŒ–æµå’Œæ‰©æ•£æ¨¡å‹ï¼‰çš„æƒ…å†µä¸‹éƒ½ä¼˜äºæ·±åº¦ä¼°è®¡å™¨å’Œæ‰§è¡Œç‚¹ä¼°è®¡è€Œä¸æ˜¯å®Œæ•´æ¨ç†çš„æ›¿ä»£ç¥ç»è¾å°„åœºæ–¹æ³•ã€‚    * RIG ä»…åœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†å®ƒåœ¨å…·æœ‰å„ç§å¹²æ‰°ï¼ˆé›¨ã€é›ªã€é›¾ã€å™ªå£°ï¼‰çš„å›¾åƒä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åœºæ™¯è¡¨ç¤ºï¼šæˆ‘ä»¬ä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) è¡¨ç¤ºï¼Œå› ä¸ºå®ƒæ˜“äºè¿›è¡ŒåŸºäºæ¢¯åº¦çš„æ¨ç†ã€‚ï¼ˆ2ï¼‰åœºæ™¯å…ˆéªŒï¼šæˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé¢„è®­ç»ƒçš„ NeRF å…ˆéªŒ p(x)ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸­å¯¹åœºæ™¯æ½œåœ¨å˜é‡ x è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä»ä¸åŒçš„è§†ç‚¹ Î¶ æ¸²æŸ“å›¾åƒ yã€‚ï¼ˆ3ï¼‰æŸåè¡¨ç¤ºå’Œå…ˆéªŒï¼šæˆ‘ä»¬å…³æ³¨çš„æ˜¯å¯¹ 3D åœºæ™¯çš„æŸåï¼Œä¾‹å¦‚æ¼‚æµ®ç‰©æˆ–å¤©æ°”ä¼ªå½±ï¼ˆå¦‚é›¨ã€é›ªæˆ–é›¾ï¼‰ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ³›åŒ–åˆ°ä¼ æ„Ÿå™¨æŸåï¼Œå¦‚ç›¸æœºå†…éƒ¨å™ªå£°ï¼ˆç¬¬ 6.1 èŠ‚ï¼‰ã€‚æˆ‘ä»¬å°† 3D æŸåè¡¨ç¤ºä¸ºå¦ä¸€ä¸ª NeRF çš„å‚æ•°ã€‚ä¸åœºæ™¯ x ä¸åŒï¼Œæˆ‘ä»¬ä¸éœ€è¦å¯¹ c æœ‰ä¸€ä¸ªå¼ºå…ˆéªŒã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‡è®¾ä¸€ä¸ªä¸é€‚å½“çš„å…ˆéªŒ p(c)âˆ1ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸éœ€è¦é¢„å…ˆçŸ¥é“æŸåçš„ç§ç±»ï¼›æŸåå¯ä»¥æ˜¯ä»»ä½• 3D å®ä½“ï¼Œä»å¤©æ°”ä¼ªå½±å’Œæ¼‚æµ®ç‰©åˆ°ä¸éœ€è¦çš„å¯¹è±¡ã€‚ï¼ˆ4ï¼‰ä¼¼ç„¶ï¼šä¸ºäº†ç»™å®šåœºæ™¯æ½œåœ¨å˜é‡ x å’ŒæŸå c æ¸²æŸ“å›¾åƒ yï¼Œæˆ‘ä»¬ç»„åˆå„è‡ªçš„ NeRF è¾“å‡ºã€‚å¯¹äºå…‰çº¿ä½ç½®å’Œæ–¹å‘ (xr, dr)ï¼Œæˆ‘ä»¬å°†åœºæ™¯ NeRF (Î³z, Ïƒz) å’ŒæŸå NeRF (Î³c, Ïƒc) çš„è¾“å‡ºç»„åˆä¸º Ïƒ = Ïƒz + Ïƒcï¼ŒÎ³ = (Î³zÏƒz + Î³cÏƒc)/Ïƒï¼ˆNiemeyer &amp; Geigerï¼Œ2021ï¼‰ã€‚æˆ‘ä»¬å°†ç»„åˆçš„ NeRF çš„æ¸²æŸ“è¡¨ç¤ºä¸º y = R(x, c)ã€‚ä¼¼ç„¶æ˜¯ä¸€ä¸ªé€åƒç´ å’Œé€é€šé“çš„é«˜æ–¯åˆ†å¸ƒ p(y|x, c) = âˆåƒç´ å’Œé€šé“j N(yij|R(x, c)ij, Ïƒ2y)ï¼Œå…¶ä¸­ Ïƒ2y æ˜¯è§‚æµ‹å™ªå£°æ–¹å·®ã€‚ï¼ˆ5ï¼‰MAP æ¨ç†ä¸å¤Ÿï¼šæ¨æ–­åœºæ™¯ x çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯æ‰¾åˆ°æœ€å¤§åŒ– p(x)p(c)p(y|x, c) çš„ MAP ä¼°è®¡ (x<em>, c</em>)ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šå¯¼è‡´â€œå¹¿å‘Šç‰Œâ€è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­æŸåæœ€ç»ˆè§£é‡Šäº†åœºæ™¯ï¼Œå°±åƒä¸€ä¸ªæ”¾ç½®åœ¨ç›¸æœºå‰é¢çš„å¹¿å‘Šç‰Œã€‚ï¼ˆ6ï¼‰å®Œå…¨åéªŒæ¨ç†å°±è¶³å¤Ÿäº†ï¼šåœ¨ RIG ä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œå®Œå…¨åéªŒæ¨ç†ä»¥è·å¾—æ½œåœ¨åœºæ™¯ x, câˆ¼p(x, c|y) âˆ p(x)p(c)p(y|x, c)ï¼Œè¿™åœ¨ç»éªŒä¸Šå¯ä»¥é¿å…å¹¿å‘Šç‰Œè§£å†³æ–¹æ¡ˆï¼ˆç¬¬ 6.1 èŠ‚ï¼‰ã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™å¯ä»¥çœ‹ä½œæ˜¯æ¨¡å¼ä¸å…¸å‹é›†ä¸åŒçš„ä¸€ä¸ªå®ä¾‹ã€‚æŸåå®Œå…¨è¦†ç›–åœºæ™¯çš„æ¨¡å¼å‘¨å›´åŒºåŸŸå…·æœ‰é«˜å¯†åº¦ä½†ä½ä½“ç§¯â€”â€”æ²¡æœ‰è®¸å¤šæŸåå¯ä»¥ç²¾ç¡®åœ°æ¸²æŸ“åˆ°è§‚æµ‹å›¾åƒã€‚å¦ä¸€æ–¹é¢ï¼ŒåéªŒåŒæ—¶è€ƒè™‘å¯†åº¦å’Œä½“ç§¯ï¼Œé›†ä¸­åœ¨å…·æœ‰é«˜æ¦‚ç‡è´¨é‡çš„åŒºåŸŸâ€”â€”æœ‰è®¸å¤šéå¹¿å‘Šç‰ŒæŸåä¸æ­£ç¡®çš„åœºæ™¯ä¸€èµ·æ¸²æŸ“åˆ°è§‚æµ‹å›¾åƒï¼Œå°½ç®¡æ¯ä¸ªè¿™æ ·çš„è§£å†³æ–¹æ¡ˆå¯èƒ½å…·æœ‰ä½å¯†åº¦ã€‚ï¼ˆ7ï¼‰å˜åˆ†æ¨ç†ï¼šæˆ‘ä»¬ä½¿ç”¨å˜åˆ†æ¨ç†ï¼Œå…¶ä¸­æˆ‘ä»¬ä¼˜åŒ–è¯æ®ä¸‹ç•Œ (ELBO) å…³äºå¼•å¯¼åˆ†å¸ƒ q(x, c)ï¼šELBO(q) = Eq(x, c)[logp(y, x|c) - logq(x, c)]ã€‚ï¼ˆ8ï¼‰æ‰©æ•£åœºæ™¯å…ˆéªŒï¼šå»å™ªæ‰©æ•£å·²æˆä¸ºæ­£åˆ™åŒ–æµçš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆã€‚è™½ç„¶å¯ä»¥ç›´æ¥ç”¨åŸºäºæ‰©æ•£çš„å…ˆéªŒæ›¿æ¢ ProbNeRF ä¸­ä½¿ç”¨çš„ RealNVPï¼ˆä¾‹å¦‚ Dupontet al.ï¼Œ2022ï¼‰ï¼Œä½†æ‰©æ•£æ¨¡å‹å…è®¸æˆ‘ä»¬å¯è¿½è¸ªåœ°å¢åŠ æˆ‘ä»¬çš„æ½œåœ¨è¡¨ç¤ºçš„ç»´æ•°ã€‚é«˜ç»´æ½œåœ¨ç©ºé—´èƒ½å¤Ÿè¿›è¡Œé«˜ä¿çœŸé‡‡æ ·å’Œé‡å»ºã€‚æˆ‘ä»¬æ„å»ºäº†å•çº§æ‰©æ•£ NeRF (SSDNeRF) æ¡†æ¶ï¼ˆChen et al.ï¼Œ2023ï¼‰æ¥è®­ç»ƒåœºæ™¯å…ˆéªŒã€‚SSDNeRF ä¼˜åŒ–äº†ä¸€ç»„é’ˆå¯¹æ¯ä¸ªè®­ç»ƒç¤ºä¾‹çš„æ½œåœ¨å˜é‡ {xn}ï¼Œä¹Ÿç§°ä¸º GLO æ½œåœ¨å˜é‡ï¼ˆBojanowski et al.ï¼Œ2018ï¼‰ï¼Œç”± Ï• å‚æ•°åŒ–çš„æ‰©æ•£å…ˆéªŒ pÏ•(x) å’Œç”± Ïˆ å‚æ•°åŒ–çš„ä¼¼ç„¶ pÏˆ(y|x)ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§é™„å½• Dã€‚ï¼ˆ9ï¼‰æ‰©æ•£æ¨¡å‹ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ªæ½œåœ¨å˜é‡ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…å«æ­£å‘å’Œåå‘è¿‡ç¨‹ã€‚æ­£å‘æ‰©æ•£è¿‡ç¨‹ q(z|x) ä»æ•°æ® x å¼€å§‹ã€‚</p></li><li><p>ç»“è®ºï¼š(1)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é²æ£’é€†å‘å›¾å½¢ç”Ÿæˆï¼ˆRIGï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†é—®é¢˜è§†ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åœºæ™¯å…ˆéªŒå’Œä¸€ä¸ªå…³äºå¹²æ‰°çš„å¼±å…ˆéªŒæ¥è¿›è¡Œæ¨ç†ã€‚RIGåœ¨å„ç§åœºæ™¯å…ˆéªŒæ¶æ„ä¸‹éƒ½ä¼˜äºæ·±åº¦ä¼°è®¡å™¨å’Œæ‰§è¡Œç‚¹ä¼°è®¡è€Œä¸æ˜¯å®Œæ•´æ¨ç†çš„æ›¿ä»£ç¥ç»è¾å°„åœºæ–¹æ³•ã€‚RIGä»…åœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†å®ƒåœ¨å…·æœ‰å„ç§å¹²æ‰°ï¼ˆé›¨ã€é›ªã€é›¾ã€å™ªå£°ï¼‰çš„å›¾åƒä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>å°†é€†å‘å›¾å½¢ç”Ÿæˆé—®é¢˜è§†ä¸ºæ¦‚ç‡æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„åœºæ™¯å…ˆéªŒå’Œä¸€ä¸ªå…³äºå¹²æ‰°çš„å¼±å…ˆéªŒæ¥è¿›è¡Œæ¨ç†ã€‚</li><li>æå‡ºäº†ä¸€ç§é²æ£’é€†å‘å›¾å½¢ç”Ÿæˆï¼ˆRIGï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯å…ˆéªŒæ¶æ„ä¸‹éƒ½ä¼˜äºæ·±åº¦ä¼°è®¡å™¨å’Œæ‰§è¡Œç‚¹ä¼°è®¡è€Œä¸æ˜¯å®Œæ•´æ¨ç†çš„æ›¿ä»£ç¥ç»è¾å°„åœºæ–¹æ³•ã€‚</li><li>RIGä»…åœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†å®ƒåœ¨å…·æœ‰å„ç§å¹²æ‰°ï¼ˆé›¨ã€é›ªã€é›¾ã€å™ªå£°ï¼‰çš„å›¾åƒä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ€§èƒ½ï¼š</li><li>RIGåœ¨å„ç§åœºæ™¯å…ˆéªŒæ¶æ„ä¸‹éƒ½ä¼˜äºæ·±åº¦ä¼°è®¡å™¨å’Œæ‰§è¡Œç‚¹ä¼°è®¡è€Œä¸æ˜¯å®Œæ•´æ¨ç†çš„æ›¿ä»£ç¥ç»è¾å°„åœºæ–¹æ³•ã€‚</li><li>RIGä»…åœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œä½†å®ƒåœ¨å…·æœ‰å„ç§å¹²æ‰°ï¼ˆé›¨ã€é›ªã€é›¾ã€å™ªå£°ï¼‰çš„å›¾åƒä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>RIGéœ€è¦é¢„è®­ç»ƒä¸€ä¸ªåœºæ™¯å…ˆéªŒå’Œä¸€ä¸ªå…³äºå¹²æ‰°çš„å¼±å…ˆéªŒã€‚</li><li>RIGéœ€è¦æ‰§è¡Œå®Œæ•´çš„æ¦‚ç‡æ¨ç†ï¼Œè¿™æ¯”æ‰§è¡Œç‚¹ä¼°è®¡æˆ–æœ€å¤§åéªŒæ¦‚ç‡æ¨ç†æ›´è€—æ—¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-25f26b8c4a059fad96179f9402d4ddf8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b464c110b8bfcce608856052d9518e4b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1f7396fa7b1ad32dc9c645595746950b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c6960210c2e3765f8601fd7fb69b4ba.jpg" align="middle"></details><h2 id="HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation"><a href="#HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation" class="headerlink" title="HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation"></a>HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation</h2><p><strong>Authors:PaweÅ‚ Batorski, Dawid Malarz, Marcin PrzewiÄ™Åºlikowski, Marcin Mazur, SÅ‚awomir Tadeja, PrzemysÅ‚aw Spurek</strong></p><p>Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study. </p><p><a href="http://arxiv.org/abs/2402.01524v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºæ–¹æ³•å¯¹äºå°‘æ•°åŸºç¡€å›¾åƒåˆæˆæ–°å¥‡3Dç‰©ä½“è§†å›¾æœ‰ç€å¹¿æ³›çš„è®¤å¯ï¼Œå´å­˜åœ¨æ³›åŒ–æ€§è´¨æœ‰é™çš„é—®é¢˜ï¼Œä¸å¦¨ç¢æˆ‘ä»¬åˆ©ç”¨æ˜¾è‘—è®¡ç®—èµ„æºä¸ºæˆ‘ä»¬è¦å±•ç¤ºçš„æ¯ä¸ªå¯¹è±¡è®­ç»ƒç‹¬ç«‹ä½“ç³»ç»“æ„ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»è¾å°„åœºæ–¹æ³•æ˜¯ä¸€ç§ç”¨äºä»å°‘æ•°åŸºç¡€å›¾åƒåˆæˆæ–° 3D ç‰©ä½“è§†å›¾çš„æ ‡å‡†æ–¹æ³•ã€‚</li><li>è¿™ç§æ–¹æ³•å­˜åœ¨æ³›åŒ–æ€§è´¨æœ‰é™çš„å¼Šç«¯ï¼Œå¯¼è‡´ä¸ºæˆ‘ä»¬è¦å±•ç¤ºçš„æ¯ä¸ªå¯¹è±¡è®­ç»ƒç‹¬ç«‹ä½“ç³»ç»“æ„æ—¶éœ€è¦æ˜¾è‘—çš„è®¡ç®—èµ„æºã€‚</li><li>ä½œè€…é’ˆå¯¹æ­¤é—®é¢˜æå‡ºäº†ä¸€ä¸ªåŸºäºè¶…ç½‘ç»œèŒƒå¼çš„ few-shot å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€æ¢¯åº¦ä¼˜åŒ–ã€‚</li><li>è¶…ç½‘ç»œä»è®­ç»ƒæ•°æ®ä¸­æ”¶é›†ä¿¡æ¯ï¼Œå¹¶ä¸ºé€šç”¨æƒé‡ç”Ÿæˆæ›´æ–°ã€‚</li><li>ä¸Šè¿°æ–¹å¼æ‰“é€ äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»å°‘é‡å›¾åƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„ 3D å¯¹è±¡è¡¨ç¤ºï¼Œåªéœ€ä¸€ä¸ªæ­¥éª¤å³å¯å®Œæˆã€‚</li><li>æˆ‘ä»¬å·²é€šè¿‡ç›´æ¥æ¯”è¾ƒæœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆå’Œå…¨é¢çš„æ¶ˆèç ”ç©¶æ¥è¯å®è¿™ä¸€ç‚¹ã€‚</li><li>è¯¥æ–¹æ³•å·²è¢«ç›´æ¥æ¯”è¾ƒæœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆå’Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¯å®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHyperPlanesï¼šå¿«é€Ÿ NeRF é€‚åº”çš„è¶…ç½‘ç»œæ–¹æ³•</li><li>ä½œè€…ï¼šPaweÅ‚ Batorski<em>, Dawid Malarz</em>, Marcin PrzewiË›eÂ´zlikowski, Marcin Mazur, Slawomir Tadeja, PrzemysÅ‚aw Spurek</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé›…ç›–éš†å¤§å­¦ï¼Œæ•°å­¦ä¸è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼Œå…‹æ‹‰ç§‘å¤«ï¼Œæ³¢å…°</li><li>å…³é”®è¯ï¼šNeRFï¼ŒFew-Shot å­¦ä¹ ï¼Œè¶…ç½‘ç»œï¼Œå¿«é€Ÿé€‚åº”</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.01524Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šNeRF æ˜¯ä¸€ç§å¯ä»¥ä»å°‘é‡åŸºæœ¬å›¾åƒåˆæˆæ–°çš„é€¼çœŸ 3D å¯¹è±¡è§†å›¾çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä½†å®ƒç¼ºä¹æ³›åŒ–æ€§ï¼Œéœ€è¦é’ˆå¯¹æ¯ä¸ªå¯¹è±¡è®­ç»ƒå•ç‹¬çš„æ¶æ„ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”æ³›åŒ–æ€§èƒ½æœ‰é™ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œèŒƒå¼çš„ few-shot å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦åœ¨æ¨ç†æœŸé—´è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ã€‚è¶…ç½‘ç»œä»è®­ç»ƒæ•°æ®ä¸­æ”¶é›†ä¿¡æ¯å¹¶ç”Ÿæˆå¯¹é€šç”¨æƒé‡çš„æ›´æ–°ã€‚ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªæ­¥éª¤ä¸­ä»å°‘é‡å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„ 3D å¯¹è±¡è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œèŒƒå¼çš„few-shotå­¦ä¹ æ–¹æ³•HyperPlanesï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦åœ¨æ¨ç†æœŸé—´è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œå¯ä»¥åœ¨å•ä¸ªæ­¥éª¤ä¸­ä»å°‘é‡å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„3Då¯¹è±¡è¡¨ç¤ºï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œèŒƒå¼çš„few-shotå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦åœ¨æ¨ç†æœŸé—´è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥ä»è®­ç»ƒæ•°æ®ä¸­æ”¶é›†ä¿¡æ¯å¹¶ç”Ÿæˆå¯¹é€šç”¨æƒé‡çš„æ›´æ–°ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªæ­¥éª¤ä¸­ä»å°‘é‡å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„3Då¯¹è±¡è¡¨ç¤ºã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ–¹æ³•åœ¨é€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªæ­¥éª¤ä¸­ä»å°‘é‡å›¾åƒç”Ÿæˆé«˜è´¨é‡çš„3Då¯¹è±¡è¡¨ç¤ºã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•ä¸éœ€è¦åœ¨æ¨ç†æœŸé—´è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œå› æ­¤å¯ä»¥å‡å°‘è®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥ä»è®­ç»ƒæ•°æ®ä¸­æ”¶é›†ä¿¡æ¯å¹¶ç”Ÿæˆå¯¹é€šç”¨æƒé‡çš„æ›´æ–°ï¼Œå› æ­¤å¯ä»¥å‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d17d9bcf9aa679caea1d14977ee1030c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-309779f6bf52d8d8cfebf258af239717.jpg" align="middle"><img src="https://picx.zhimg.com/v2-639e9fd34cf9c9e63acc4cb78afac975.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-299ffc14425343bcd3a07c8f9122813c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-09  NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/</id>
    <published>2024-02-09T01:58:59.000Z</published>
    <updated>2024-02-09T01:58:59.146Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-09-æ›´æ–°"><a href="#2024-02-09-æ›´æ–°" class="headerlink" title="2024-02-09 æ›´æ–°"></a>2024-02-09 æ›´æ–°</h1><h2 id="Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos"><a href="#Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos" class="headerlink" title="Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos"></a>Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos</h2><p><strong>Authors:Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</strong></p><p>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at <a href="http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html">http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html</a> </p><p><a href="http://arxiv.org/abs/2402.03723v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>3D é«˜æ–¯æ•£ç‚¹ï¼ˆ3DGSï¼‰çš„å¼€å‘æ”¹å–„äº†æ¸²æŸ“è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„ 3D å¯å˜å½¢æ¨¡å‹æŒ‡å¯¼çš„å˜å½¢æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®å»ºæ¨¡å’Œåˆ†ç¦»å¤´éƒ¨è¿åŠ¨åŠé¢éƒ¨è¡¨æƒ…ã€‚</p><p><strong>ä¸»è¦è¦ç‚¹</strong></p><ul><li>3DGS åœ¨ AR/VR åº”ç”¨ä¸­å…·æœ‰å·¨å¤§ä»·å€¼ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿä»ä¼‘é—²æ™ºèƒ½æ‰‹æœºè§†é¢‘ä¸­åˆ›å»ºå¯æ§çš„ 3D äººåƒã€‚</li></ul><ul><li>3DGS åœ¨æ¸²æŸ“è´¨é‡å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»ç„¶éš¾ä»¥ä»å•è§†å›¾æ•æ‰ä¸­å‡†ç¡®å»ºæ¨¡å’Œåˆ†ç¦»å¤´éƒ¨è¿åŠ¨å’Œé¢éƒ¨è¡¨æƒ…ä»¥å®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚</li></ul><ul><li>Rig3DGS ä½¿ç”¨ä¸€ç»„ 3D é«˜æ–¯åˆ†å¸ƒåœ¨è§„èŒƒç©ºé—´ä¸­è¡¨ç¤ºæ•´ä¸ªåœºæ™¯ï¼ŒåŒ…æ‹¬åŠ¨æ€ä¸»ä½“ã€‚</li></ul><ul><li>Rig3DGS ä½¿ç”¨ä¸€ç»„æ§åˆ¶ä¿¡å·ï¼Œä¾‹å¦‚å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…ï¼Œå°†å…¶è½¬æ¢ä¸º 3D ç©ºé—´ï¼Œå¹¶é€šè¿‡å­¦ä¹ åˆ°çš„å˜å½¢æ¥ç”Ÿæˆæ‰€éœ€çš„æ¸²æŸ“ã€‚</li></ul><ul><li>Rig3DGS çš„å…³é”®åˆ›æ–°åœ¨äºä¸€ç§ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„å˜å½¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±æºè‡ª 3D å¯å˜å½¢æ¨¡å‹çš„å¯å­¦ä¹ å…ˆéªŒå¼•å¯¼ã€‚</li></ul><ul><li>è¿™ç§æ–¹æ³•åœ¨è®­ç»ƒä¸­éå¸¸æœ‰æ•ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å„ç§æ•æ‰ä¸­çš„é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨ä½ç½®å’Œè§†å›¾åˆæˆã€‚</li></ul><ul><li>é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒè¯æ˜äº† Rig3DGS çš„å­¦ä¹ å˜å½¢æ˜¯æœ‰æ•ˆçš„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRig3DGSï¼šä»éšæ„å•ç›®è§†é¢‘åˆ›å»ºå¯æ§è‚–åƒ</li><li>ä½œè€…ï¼šAlfredo Rivero<em>, Shah Rukh Athar</em>, Zhixin Shu, Dimitris Samaras</li><li>å•ä½ï¼šçº½çº¦çŸ³æºªå¤§å­¦</li><li>å…³é”®è¯ï¼š3Däººåƒã€3Dé«˜æ–¯å–·ç»˜ã€å¯æ§å˜å½¢ã€å¤´éƒ¨å§¿æ€ã€é¢éƒ¨è¡¨æƒ…ã€è§†è§’åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03723Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šåˆ›å»ºå¯æ§çš„ 3D äººç±»è‚–åƒå¯¹äºå„ç§æ²‰æµ¸å¼ä½“éªŒè‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®ã€è¿œç¨‹ä¸´åœºã€ç”µå½±åˆ¶ä½œå’Œæ•™è‚²åº”ç”¨ã€‚ç„¶è€Œï¼Œä»…ä½¿ç”¨åŸºæœ¬æ™ºèƒ½æ‰‹æœºæ‘„åƒå¤´ï¼Œæ™®é€šæ¶ˆè´¹è€…å®ç°è¿™é¡¹æŠ€æœ¯é¢ä¸´ç€ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä»è§†é¢‘ä¸­å»ºæ¨¡ 3D å¯æ§è‚–åƒé€šå¸¸æ¶‰åŠåŠ¨æ€äººç±»ä¸»ä½“çš„æ˜¾å¼æˆ–éšå¼é…å‡†ï¼Œè€ƒè™‘æ¯ä¸ªå¸§ä¸­é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿ç­‰ä¸åŒå› ç´ ã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦ç²¾ç¡®åŒºåˆ†ç”±è¿™äº›å› ç´ å¼•èµ·çš„é¢éƒ¨å˜å½¢ï¼Œè¿™åœ¨æ²¡æœ‰çœŸå®ä¾æ®çš„æƒ…å†µä¸‹é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“ä½¿ç”¨å•ç›®æ•æ‰æ—¶ï¼ŒæŒ‘æˆ˜è¿›ä¸€æ­¥åŠ å‰§ï¼Œå› ä¸ºæ¯ä¸ªå¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…åªèƒ½ä»å•ä¸ªè§†ç‚¹çœ‹åˆ°ï¼Œè¿™ä½¿å¾—å‡†ç¡®çš„åŒºåˆ†å˜å¾—æ›´åŠ å¤æ‚ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º Rig3DGS æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç»„ 3D é«˜æ–¯ä½“åœ¨è§„èŒƒç©ºé—´ä¸­è¡¨ç¤ºæ•´ä¸ªåœºæ™¯ï¼ŒåŒ…æ‹¬åŠ¨æ€ä¸»ä½“ã€‚ä½¿ç”¨ä¸€ç»„æ§åˆ¶ä¿¡å·ï¼Œä¾‹å¦‚å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…ï¼Œæˆ‘ä»¬åˆ©ç”¨å­¦ä¹ åˆ°çš„å˜å½¢å°†å®ƒä»¬è½¬æ¢ä¸º 3D ç©ºé—´ä»¥ç”Ÿæˆæ‰€éœ€çš„æ¸²æŸ“ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å˜å½¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±ä» 3D å¯å˜å½¢æ¨¡å‹æ´¾ç”Ÿçš„å¯å­¦ä¹ å…ˆéªŒå¼•å¯¼ã€‚è¿™ç§æ–¹æ³•åœ¨è®­ç»ƒä¸­éå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”èƒ½å¤Ÿæ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨ä½ç½®å’Œè·¨å„ç§æ•æ‰çš„è§†è§’åˆæˆã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒè¯æ˜äº†æˆ‘ä»¬å­¦ä¹ åˆ°çš„å˜å½¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹ç›®é¡µé¢å¯åœ¨æ­¤å¤„æ‰¾åˆ°ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): Rig3DGS ä½¿ç”¨ä¸€ç»„ 3D é«˜æ–¯ä½“è¡¨ç¤ºæ•´ä¸ªåœºæ™¯ï¼ŒåŒ…æ‹¬åŠ¨æ€ä¸»ä½“ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„æ§åˆ¶ä¿¡å·ï¼ˆå¦‚å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…ï¼‰å°†å®ƒä»¬è½¬æ¢ä¸º 3D ç©ºé—´ä»¥ç”Ÿæˆæ‰€éœ€çš„æ¸²æŸ“ã€‚(2): Rig3DGS çš„å…³é”®åˆ›æ–°æ˜¯ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å˜å½¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±ä» 3D å¯å˜å½¢æ¨¡å‹æ´¾ç”Ÿçš„å¯å­¦ä¹ å…ˆéªŒå¼•å¯¼ã€‚(3): è¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­éå¸¸æœ‰æ•ˆï¼Œå¹¶ä¸”èƒ½å¤Ÿæ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨ä½ç½®å’Œè·¨å„ç§æ•æ‰çš„è§†è§’åˆæˆã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Rig3DGS çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹è‚–åƒè§†é¢‘è¿›è¡Œä»»æ„é¢éƒ¨è¡¨æƒ…æ§åˆ¶å’Œæ–°è§†è§’åˆæˆã€‚Rig3DGS ä½¿ç”¨å¯å­¦ä¹ çš„å˜å½¢å…ˆéªŒæ¥ç¡®ä¿åœ¨è®­ç»ƒæœŸé—´çš„ç¨³å®šæ€§å’Œå¯¹æ–°é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿åŠ¿å’Œè§†è§’çš„ä¸€èˆ¬åŒ–ã€‚Rig3DGS è¿˜èƒ½å¤Ÿå¯¹æ‹æ‘„å¯¹è±¡çš„å¤´å‘å’Œçœ¼é•œç­‰é¢éƒ¨ç»†èŠ‚è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åœ¨è§†é¢‘è¢«é©±åŠ¨æ—¶ä»¥é«˜ä¿çœŸåº¦å†ç°å®ƒä»¬ã€‚ä½†æ˜¯ï¼Œå…·æœ‰æ–°è§†è§’åˆæˆçš„å¯æ§äººå¤´éƒ¨æ¨¡å‹çš„é—®é¢˜è¿˜è¿œæœªè§£å†³ã€‚Rig3DGS æ— æ³•å¯¹å¼ºçƒˆçš„éå‡åŒ€å…‰ç…§è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”è¦æ±‚è‚–åƒè§†é¢‘ä¸­çš„æ‹æ‘„å¯¹è±¡åœ¨æ‹æ‘„æœŸé—´ä¿æŒç›¸å¯¹é™æ­¢ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥çš„å·¥ä½œä¸­è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æå‡ºäº† Rig3DGSï¼Œä¸€ç§ä½¿ç”¨ä¸€ç»„ 3D é«˜æ–¯ä½“è¡¨ç¤ºæ•´ä¸ªåœºæ™¯ï¼ˆåŒ…æ‹¬åŠ¨æ€ä¸»ä½“ï¼‰çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„æ§åˆ¶ä¿¡å·ï¼ˆå¦‚å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…ï¼‰å°†å®ƒä»¬è½¬æ¢ä¸º 3D ç©ºé—´ä»¥ç”Ÿæˆæ‰€éœ€çš„æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å˜å½¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±ä» 3D å¯å˜å½¢æ¨¡å‹æ´¾ç”Ÿçš„å¯å­¦ä¹ å…ˆéªŒå¼•å¯¼ã€‚ï¼ˆ3ï¼‰è¯æ˜äº†è¯¥æ–¹æ³•èƒ½å¤Ÿæ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨ä½ç½®å’Œè·¨å„ç§æ•æ‰çš„è§†è§’åˆæˆã€‚æ€§èƒ½ï¼šï¼ˆ1ï¼‰Rig3DGS èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D è‚–åƒï¼Œå…·æœ‰é€¼çœŸçš„é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨å§¿åŠ¿å’Œè§†è§’ã€‚ï¼ˆ2ï¼‰Rig3DGS èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç…§æ˜æ¡ä»¶ä¸‹å·¥ä½œï¼Œä¾‹å¦‚å¼ºçƒˆçš„éå‡åŒ€å…‰ç…§ã€‚ï¼ˆ3ï¼‰Rig3DGS èƒ½å¤Ÿå®æ—¶è¿è¡Œï¼Œä½¿å…¶é€‚ç”¨äºå„ç§åº”ç”¨ç¨‹åºã€‚å·¥ä½œé‡ï¼šï¼ˆ1ï¼‰Rig3DGS çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹ç®€å•ä¸”ç›´æ¥ã€‚ï¼ˆ2ï¼‰Rig3DGS æ˜“äºä½¿ç”¨ï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•ä¸“é—¨çš„ç¡¬ä»¶æˆ–è½¯ä»¶ã€‚ï¼ˆ3ï¼‰Rig3DGS æ˜¯å¼€æºçš„ï¼Œå¯ä»¥å…è´¹ä½¿ç”¨ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3161a0632f560b62291a8cf525616b2c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6843ee2a991081c82505388c065defc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" align="middle"></details><h2 id="4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes"></a>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes</h2><p><strong>Authors:Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</strong></p><p>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2402.03307v2">PDF</a> </p><p><strong>Summary</strong><br>åŠ¨æ€åœºæ™¯ä¸‹æ–°è§†è§’åˆæˆæ–¹æ³• 4DGSï¼ŒåŸºäºé«˜æ–¯ä½“ç´ æ—¶ç©ºåˆ‡ç‰‡è¡¨ç¤ºå®ç°äº†å¿«é€Ÿçš„åŠ¨æ€åœºæ™¯æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>4DGS æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨å„å‘å¼‚æ€§çš„ 4D XYZT é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºåŠ¨æ€åœºæ™¯ã€‚</li><li>4DGS é€šè¿‡å¯¹ 4D é«˜æ–¯ä½“ç´ è¿›è¡Œæ—¶é—´åˆ‡ç‰‡æ¥å»ºæ¨¡æ¯ä¸ªæ—¶é—´æˆ³çš„åŠ¨æ€ï¼Œä»è€Œè‡ªç„¶åœ°æ„æˆåŠ¨æ€ 3D é«˜æ–¯ä½“ç´ å¹¶å¯ä»¥æ— ç¼åœ°æŠ•å½±åˆ°å›¾åƒä¸­ã€‚</li><li>ä½œä¸ºä¸€ç§æ˜¾å¼çš„æ—¶ç©ºè¡¨ç¤ºï¼Œ4DGS åœ¨å»ºæ¨¡å¤æ‚åŠ¨æ€å’Œç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å¯¹äºå…·æœ‰çªç„¶è¿åŠ¨çš„åœºæ™¯ã€‚</li><li>4DGS åœ¨é«˜åº¦ä¼˜åŒ–çš„ CUDA åŠ é€Ÿæ¡†æ¶ä¸­å®ç°äº†æ—¶é—´åˆ‡ç‰‡å’Œ splatting æŠ€æœ¯ï¼Œåœ¨ RTX 3090 GPU ä¸Šå®ç°äº†é«˜è¾¾ 277 FPS çš„å®æ—¶æ¨ç†æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨ RTX 4090 GPU ä¸Šå®ç°äº† 583 FPS çš„å®æ—¶æ¨ç†æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>åœ¨å…·æœ‰ä¸åŒè¿åŠ¨çš„åœºæ™¯ä¸Šçš„ä¸¥æ ¼è¯„ä¼°è¡¨æ˜ï¼Œ4DGS çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡è¿˜æ˜¯å®šæ€§æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>4DGS å¯ä»¥è½»æ¾æ‰©å±•åˆ°å„ç§åŠ¨æ€åœºæ™¯ï¼Œä¾‹å¦‚å…·æœ‰å¤æ‚å‡ ä½•å½¢çŠ¶ã€é®æŒ¡å’Œçº¹ç†çš„å¯¹è±¡ã€å…·æœ‰ç»†å¾®è¿åŠ¨çš„äººä½“ä»¥åŠé€¼çœŸçš„åˆæˆåœºæ™¯ï¼Œå¹¶åœ¨è¿™äº›åœºæ™¯ä¸­å®ç°é«˜è´¨é‡çš„ NVSã€‚</li><li>4DGS å¯ä»¥åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å‘æŒ¥ä½œç”¨ï¼Œä¾‹å¦‚è§†é¢‘æ’å¸§ã€è¿åŠ¨æ¨¡ç³Šã€è¿åŠ¨ä¼°è®¡ã€åœºæ™¯é‡å»ºå’Œå¢å¼ºç°å®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼š4D é«˜æ–¯æ•£ç‚¹ï¼šé¢å‘åŠ¨æ€åœºæ™¯çš„é«˜æ•ˆæ–°è§†ç‚¹åˆæˆ</li><li>ä½œè€…ï¼šæ®µå…ƒå…´ï¼Œé­èŠ³å¯…ï¼Œæˆ´å¯å®‡ï¼Œä½•å®‡èˆªï¼Œé™ˆæ–‡æ­£ï¼Œé™ˆå®æƒ</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†ç‚¹åˆæˆï¼ŒåŠ¨æ€åœºæ™¯ï¼Œæ—¶é—´ä¸€è‡´æ€§ï¼Œç©ºé—´ä¸€è‡´æ€§ï¼Œé«˜æ–¯æ•£ç‚¹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2402.03307.pdfï¼ŒGithub é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–°è§†ç‚¹åˆæˆï¼ˆNVSï¼‰æ—¨åœ¨ä» 2D å›¾åƒé‡å»º 3D åœºæ™¯ï¼Œå¹¶ä»æ–°è§†ç‚¹åˆæˆå…¶å¤–è§‚ã€‚NVS åœ¨å½±è§†ã€æ¸¸æˆã€VR/AR ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚å¯¹äºé™æ€åœºæ™¯ï¼ŒNVS å·²å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹äºåŠ¨æ€åœºæ™¯ï¼Œç”±äºæ—¶é—´ç»´åº¦å’Œå¤æ‚è¿åŠ¨æ¨¡å¼çš„å¼•å…¥ï¼Œé«˜æ•ˆä¸”å‡†ç¡®çš„ NVS ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šè”åˆå»ºæ¨¡æ³•å’Œè§£è€¦å»ºæ¨¡æ³•ã€‚è”åˆå»ºæ¨¡æ³•å°† 3D åœºæ™¯åŠå…¶åŠ¨æ€è”åˆå»ºæ¨¡ï¼Œä½†å¾€å¾€éš¾ä»¥ä¿ç•™ NVS æ¸²æŸ“ä¸­çš„ç²¾ç»†ç»†èŠ‚ã€‚è§£è€¦å»ºæ¨¡æ³•å°†åŠ¨æ€åœºæ™¯åˆ†è§£ä¸ºé™æ€è§„èŒƒç©ºé—´å’Œå˜å½¢åœºï¼Œä½†éš¾ä»¥æ•æ‰è¯¸å¦‚ç‰©ä½“çªç„¶å‡ºç°æˆ–æ¶ˆå¤±ç­‰å¤æ‚åŠ¨æ€ã€‚æ­¤å¤–ï¼Œä¸»æµçš„åŸºäºä½“ç§¯æ¸²æŸ“çš„æ–¹æ³•é€šå¸¸æ— æ³•æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º 4D é«˜æ–¯æ•£ç‚¹ï¼ˆ4DGSï¼‰çš„æ–°æ–¹æ³•ã€‚4DGS å°†åŠ¨æ€åœºæ™¯è¡¨ç¤ºä¸ºå„å‘å¼‚æ€§çš„ 4D XYZT é«˜æ–¯åˆ†å¸ƒï¼Œå—é™æ€åœºæ™¯ä¸­ 3D é«˜æ–¯æ•£ç‚¹æˆåŠŸçš„å¯å‘ã€‚é€šè¿‡å¯¹ 4D é«˜æ–¯åˆ†å¸ƒè¿›è¡Œæ—¶é—´åˆ‡ç‰‡ï¼Œå¯ä»¥è‡ªç„¶åœ°ç»„æˆåŠ¨æ€ 3D é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å°†å…¶æ— ç¼æŠ•å½±åˆ°å›¾åƒä¸­ã€‚ä½œä¸ºä¸€ç§æ˜¾å¼çš„æ—¶ç©ºè¡¨ç¤ºï¼Œ4DGS èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡å¤æ‚çš„åŠ¨æ€å’Œç²¾ç»†ç»†èŠ‚ï¼Œå°¤å…¶é€‚ç”¨äºå…·æœ‰çªç„¶è¿åŠ¨çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å®ç°äº†ä¸€ç§é«˜åº¦ä¼˜åŒ–çš„ CUDA åŠ é€Ÿæ¡†æ¶ï¼Œåœ¨ RTX 3090 GPU ä¸Šå®ç°äº†é«˜è¾¾ 277 FPS çš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨ RTX 4090 GPU ä¸Šå®ç°äº† 583 FPS çš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å…·æœ‰ä¸åŒè¿åŠ¨çš„åœºæ™¯ä¸Šè¿›è¡Œçš„ä¸¥æ ¼è¯„ä¼°è¡¨æ˜ï¼Œ4DGS åœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚4DGS åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„åŠ¨æ€åœºæ™¯ NVS æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š4Dé«˜æ–¯æ•£ç‚¹ï¼ˆ4DGSï¼‰çš„åŸºæœ¬æ€æƒ³æ˜¯å°†åŠ¨æ€åœºæ™¯è¡¨ç¤ºä¸ºå„å‘å¼‚æ€§çš„4D XYZTé«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡å¯¹4Dé«˜æ–¯åˆ†å¸ƒè¿›è¡Œæ—¶é—´åˆ‡ç‰‡ï¼Œå¯ä»¥è‡ªç„¶åœ°ç»„æˆåŠ¨æ€3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å°†å…¶æ— ç¼æŠ•å½±åˆ°å›¾åƒä¸­ã€‚è¿™ç§æ˜¾å¼çš„æ—¶ç©ºè¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡å¤æ‚çš„åŠ¨æ€å’Œç²¾ç»†ç»†èŠ‚ï¼Œå°¤å…¶é€‚ç”¨äºå…·æœ‰çªç„¶è¿åŠ¨çš„åœºæ™¯ã€‚ï¼ˆ2ï¼‰ï¼š4DGSçš„å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p></li><li>é¦–å…ˆï¼Œé€šè¿‡å°†åœºæ™¯ä¸­çš„æ¯ä¸ªç‚¹åŠå…¶è¿åŠ¨è½¨è¿¹å»ºæ¨¡ä¸º4D XYZTé«˜æ–¯åˆ†å¸ƒï¼Œæ¥è¡¨ç¤ºåŠ¨æ€åœºæ™¯ã€‚</li><li>å…¶æ¬¡ï¼Œé€šè¿‡å¯¹4Dé«˜æ–¯åˆ†å¸ƒè¿›è¡Œæ—¶é—´åˆ‡ç‰‡ï¼Œå¾—åˆ°ä¸€ç³»åˆ—3Dé«˜æ–¯åˆ†å¸ƒï¼Œè¿™äº›3Dé«˜æ–¯åˆ†å¸ƒå¯ä»¥æ— ç¼åœ°æŠ•å½±åˆ°å›¾åƒä¸­ï¼Œä»è€Œåˆæˆæ–°è§†ç‚¹å›¾åƒã€‚</li><li><p>æœ€åï¼Œä¸ºäº†æé«˜æ¸²æŸ“é€Ÿåº¦ï¼Œæœ¬æ–‡è¿˜å®ç°äº†ä¸€ç§é«˜åº¦ä¼˜åŒ–çš„CUDAåŠ é€Ÿæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åœ¨RTX3090 GPUä¸Šå®ç°é«˜è¾¾277 FPSçš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨RTX4090 GPUä¸Šå®ç°583 FPSçš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé€šè¿‡æå‡º4Dé«˜æ–¯æ•£ç‚¹ï¼ˆ4DGSï¼‰æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®çš„åŠ¨æ€åœºæ™¯æ–°è§†ç‚¹åˆæˆï¼Œä¸ºåŠ¨æ€åœºæ™¯NVSé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå°†åŠ¨æ€åœºæ™¯è¡¨ç¤ºä¸ºå„å‘å¼‚æ€§çš„4DXYZTé«˜æ–¯åˆ†å¸ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡å¤æ‚çš„åŠ¨æ€å’Œç²¾ç»†ç»†èŠ‚ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„NVSæ–¹æ³•ï¼Œé€šè¿‡å¯¹4Dé«˜æ–¯åˆ†å¸ƒè¿›è¡Œæ—¶é—´åˆ‡ç‰‡ï¼Œå¾—åˆ°ä¸€ç³»åˆ—3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å°†å…¶æ— ç¼æŠ•å½±åˆ°å›¾åƒä¸­ï¼Œåˆæˆæ–°è§†ç‚¹å›¾åƒã€‚</li><li>å®ç°äº†ä¸€ç§é«˜åº¦ä¼˜åŒ–çš„CUDAåŠ é€Ÿæ¡†æ¶ï¼Œåœ¨RTX3090GPUä¸Šå®ç°é«˜è¾¾277FPSçš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨RTX4090GPUä¸Šå®ç°583FPSçš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å…·æœ‰ä¸åŒè¿åŠ¨çš„åœºæ™¯ä¸Šè¿›è¡Œçš„ä¸¥æ ¼è¯„ä¼°è¡¨æ˜ï¼Œ4DGSåœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>4DGSåœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬å·¥ä½œæ¶‰åŠäº†å¤§é‡çš„ç†è®ºæ¨å¯¼å’Œç®—æ³•å®ç°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li><li>æœ¬å·¥ä½œä½¿ç”¨äº†å¤§é‡çš„å®éªŒæ•°æ®ï¼Œå®éªŒè¿‡ç¨‹å¤æ‚ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-8afb4e4e499c5116d082b9b523480bbb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-960e35d536b25803abdadcc5fd2abea1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0570db380e05870cdbbd7a17934c699.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db45e73c8294473dfec461a53ba7d2a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5983071f25b6e20421a8a05030a8a70f.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v1">PDF</a> </p><p><strong>Summary</strong><br>3Dè¯­ä¹‰é«˜æ–¯è¡¨ç¤ºçš„è§†è§‰SLAMç³»ç»Ÿï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸèå…¥åˆ°å…³é”®å¸§ä¼˜åŒ–ï¼Œå®ç°å®æ—¶çš„é«˜ç²¾åº¦3Dè¯­ä¹‰åˆ†å‰²å’Œåœ°å›¾é‡å»ºï¼Œæ•ˆæœä¼˜å¼‚ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºSGS-SLAMï¼Œç¬¬ä¸€ä¸ªåŸºäº3Dé«˜æ–¯è¡¨ç¤ºçš„è¯­ä¹‰ç¨ å¯†è§†è§‰SLAMç³»ç»Ÿï¼Œæä¾›ç²¾ç¡®çš„3Dè¯­ä¹‰åˆ†å‰²å’Œé«˜ä¿çœŸçš„åœ°å›¾é‡å»ºã€‚</li><li>åœ¨å»ºå›¾è¿‡ç¨‹ä¸­é‡‡ç”¨å¤šé€šé“ä¼˜åŒ–ï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸä¸å…³é”®å¸§ä¼˜åŒ–ç›¸ç»“åˆï¼Œæé«˜é‡å»ºè´¨é‡ã€‚</li><li>SGS-SLAMåœ¨ç›¸æœºä½å§¿ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶çš„æ¸²æŸ“èƒ½åŠ›ã€‚</li><li>SGS-SLAMåŒæ—¶é€‚ç”¨äºå®¤å†…å’Œå®¤å¤–åœºæ™¯ï¼Œå¯åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¤„ç†å…‰ç…§å˜åŒ–å’Œå¿«é€Ÿè¿åŠ¨ã€‚</li><li>SGS-SLAMå¯ç”¨äºå„ç§æœºå™¨äººåº”ç”¨ï¼Œå¦‚å¯¼èˆªã€æ¢ç´¢å’Œæ“çºµã€‚</li><li>SGS-SLAMçš„ä»£ç å’Œæ•°æ®é›†å·²å¼€æºï¼Œå¯ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li><li>SGS-SLAMå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºè‡ªåŠ¨é©¾é©¶ã€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSGS-SLAMï¼šç¥ç»ç¨ å¯† SLAM çš„è¯­ä¹‰é«˜æ–¯ç»˜å›¾</li><li>ä½œè€…ï¼šMingrui Liã€Shuhong Liuã€Heng Zhou</li><li>å•ä½ï¼šå¤§è¿ç†å·¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šSLAMã€3D é‡å»ºã€3D è¯­ä¹‰åˆ†å‰²</li><li>é“¾æ¥ï¼šPaper_info</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¯­ä¹‰ç†è§£åœ¨ç¨ å¯†çš„åŒæ—¶å®šä½å’Œå»ºå›¾ï¼ˆSLAMï¼‰ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œæœ‰åŠ©äºå…¨é¢ç†è§£åœºæ™¯ã€‚æœ€è¿‘å°†é«˜æ–¯ç»˜å›¾é›†æˆåˆ° SLAM ç³»ç»Ÿä¸­çš„è¿›å±•å·²ç»è¯æ˜äº†å…¶åœ¨ä½¿ç”¨æ˜¾å¼ 3D é«˜æ–¯è¡¨ç¤ºç”Ÿæˆé«˜è´¨é‡æ¸²æŸ“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„è§†è§‰ SLAM ç³»ç»Ÿåœ¨ç¨€ç–é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾ç€æˆå°±ï¼Œä½†æ— æ³•é€šè¿‡ç‚¹äº‘æˆ–ä½“ç´ æœ‰æ•ˆåœ°è¡¨ç¤ºæ›´å¯†é›†çš„é‡å»ºã€‚ä¸ºäº†æå–ç”¨äºé«˜ä¿çœŸè¡¨ç¤ºçš„å¯†é›†å‡ ä½•ä¿¡æ¯ï¼ŒåŸºäºå­¦ä¹ çš„ SLAM æ–¹æ³•è·å¾—äº†å¹¿æ³›å…³æ³¨ã€‚å®ƒä»¬åœ¨ç”Ÿæˆè‰¯å¥½çš„å…¨å±€ 3D åœ°å›¾çš„åŒæ—¶ï¼Œè¿˜è¡¨ç°å‡ºå¯¹å™ªå£°å’Œå¼‚å¸¸å€¼çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå—ç¥ç»è¾å°„åœº (NeRF) è¿›å±•çš„å¯å‘ï¼ŒåŸºäº NeRF çš„ SLAM æ–¹æ³•å–å¾—äº†è¿›ä¸€æ­¥çš„è¿›å±•ã€‚å®ƒä»¬æ“…é•¿é€šè¿‡å¯å¾®æ¸²æŸ“æ•è·å¯†é›†çš„å…‰åº¦ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿå‡†ç¡®ä¸”é«˜ä¿çœŸçš„å…¨å±€é‡å»ºã€‚ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨ä¸Šè¿°ç ”ç©¶çš„åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº† SGS-SLAMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäº 3D é«˜æ–¯çš„è¯­ä¹‰ç¨ å¯†è§†è§‰ SLAM ç³»ç»Ÿï¼Œå®ƒåœ¨æä¾›é«˜ä¿çœŸé‡å»ºçš„åŒæ—¶ï¼Œè¿˜æä¾›äº†ç²¾ç¡®çš„ 3D è¯­ä¹‰åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºåœ¨å»ºå›¾è¿‡ç¨‹ä¸­é‡‡ç”¨å¤šé€šé“ä¼˜åŒ–ï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸä¸å…³é”®å¸§ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒSGS-SLAM åœ¨ç›¸æœºä½å§¿ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li></ol><p><methods>:(1) å¤šé€šé“é«˜æ–¯è¡¨ç¤ºï¼šä½¿ç”¨é«˜æ–¯å½±å“å‡½æ•°è¡¨ç¤ºåœºæ™¯ï¼Œé«˜æ–¯å‡½æ•°å…·æœ‰åŠå¾„ã€ä¸­å¿ƒä½ç½®å’Œé¢œè‰²ã€‚é€šè¿‡æ¸²æŸ“é«˜æ–¯å‡½æ•°åˆ° 2D å›¾åƒæ¥ä¼˜åŒ–é«˜æ–¯å‡½æ•°çš„å‚æ•°ï¼Œå¹¶ä½¿ç”¨æ·±åº¦æ¸²æŸ“æ¥è®¡ç®—åƒç´ çº§æ¸²æŸ“é¢œè‰²å’Œæ·±åº¦ã€‚åˆ©ç”¨ 2D è¯­ä¹‰æ ‡ç­¾ä¸ºé«˜æ–¯å‡½æ•°åˆ†é…ä¸åŒçš„é€šé“æ¥è¡¨ç¤ºè¯­ä¹‰æ ‡ç­¾å’Œé¢œè‰²ã€‚(2) è·Ÿè¸ªå’Œå»ºå›¾ï¼šè·Ÿè¸ªè¿‡ç¨‹ä¼°è®¡æ¯å¸§çš„ç›¸æœºä½å§¿ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯å‚æ•°å›ºå®šã€‚å»ºå›¾è¿‡ç¨‹æ ¹æ®ä¼°è®¡çš„ç›¸æœºä½å§¿ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºã€‚è·Ÿè¸ªè¿‡ç¨‹é€šè¿‡æœ€å°åŒ–è·Ÿè¸ªæŸå¤±æ¥è¿­ä»£ä¼˜åŒ–å½“å‰ä½å§¿ã€‚å…³é”®å¸§é€‰æ‹©å’Œæƒé‡åˆ†é…åŸºäºå‡ ä½•å’Œè¯­ä¹‰çº¦æŸã€‚(3) åœ°å›¾é‡å»ºï¼šä½¿ç”¨é«˜æ–¯å‡½æ•°å¯¹åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œé«˜æ–¯å‡½æ•°çš„å‡å€¼åæ ‡è¡¨ç¤ºåœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œå¤–è§‚é¢œè‰²æè¿°åœºæ™¯çš„è§†è§‰å¤–è§‚ï¼Œè¯­ä¹‰é¢œè‰²æŒ‡ç¤ºç‰©ä½“çš„è¯­ä¹‰æ ‡ç­¾ã€‚åœ¨é«˜æ–¯å‡½æ•°è‡´å¯†åŒ–å’Œä¼˜åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œè¿™äº›å‚æ•°åœ¨å„ä¸ªé€šé“ä¸Šè”åˆä¼˜åŒ–ï¼Œè€Œç›¸æœºä½å§¿åˆ™é€šè¿‡è·Ÿè¸ªå›ºå®šã€‚é€šè¿‡å°†é«˜æ–¯å‡½æ•°æ¸²æŸ“åˆ° 2D å›¾åƒæ¥ä¼˜åŒ–åœ°å›¾å‚æ•°ï¼Œå¹¶ä½¿ç”¨æ·±åº¦æ¸²æŸ“æ¥è®¡ç®—åƒç´ çº§æ¸²æŸ“é¢œè‰²å’Œæ·±åº¦ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šSGS-SLAMæ˜¯ç¬¬ä¸€ä¸ªåŸºäº3Dé«˜æ–¯è¡¨ç¤ºçš„è¯­ä¹‰ç¨ å¯†è§†è§‰SLAMç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨å¤šé€šé“å‚æ•°ä¼˜åŒ–ï¼Œå…¶ä¸­å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸè¢«ç»„åˆä»¥å¼ºåˆ¶æ‰§è¡Œé«˜ç²¾åº¦çš„3Dè¯­ä¹‰åˆ†å‰²ï¼Œå¹¶åŒæ—¶è¿›è¡Œé«˜ä¿çœŸç¨ å¯†åœ°å›¾é‡å»ºï¼ŒåŒæ—¶æœ‰æ•ˆåœ°äº§ç”Ÿé²æ£’çš„ç›¸æœºä½å§¿ä¼°è®¡ã€‚SGS-SLAMåˆ©ç”¨äº†æœ€ä¼˜å…³é”®å¸§ä¼˜åŒ–çš„å¥½å¤„ï¼Œä»è€Œäº§ç”Ÿäº†å¯é çš„é‡å»ºè´¨é‡ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªå’Œå»ºå›¾ç»“æœï¼ŒåŒæ—¶ä¿æŒäº†å¿«é€Ÿçš„æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œé«˜è´¨é‡çš„é‡å»ºï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰ç¨ å¯†SLAMç³»ç»ŸSGS-SLAMï¼Œè¯¥ç³»ç»Ÿé¦–æ¬¡å°†3Dé«˜æ–¯è¡¨ç¤ºä¸è¯­ä¹‰åˆ†å‰²ç›¸ç»“åˆï¼Œå®ç°äº†é«˜ä¿çœŸé‡å»ºå’Œç²¾ç¡®çš„3Dè¯­ä¹‰åˆ†å‰²ã€‚</li><li>è®¾è®¡äº†ä¸€ç§å¤šé€šé“å‚æ•°ä¼˜åŒ–æ–¹æ³•ï¼Œå°†å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¦æŸç›¸ç»“åˆï¼Œæé«˜äº†é‡å»ºè´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯å‡½æ•°çš„ç¨ å¯†åœ°å›¾é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸçš„3Dåœ°å›¾ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ç›¸æœºä½å§¿ä¼°è®¡ã€åœ°å›¾é‡å»ºå’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>èƒ½å¤Ÿå®æ—¶æ¸²æŸ“ï¼Œä¿æŒäº†è‰¯å¥½çš„äº¤äº’æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>ç®—æ³•å®ç°å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>æ•°æ®é›†çš„æ„å»ºå’Œæ ‡æ³¨éœ€è¦å¤§é‡çš„äººåŠ›ç‰©åŠ›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-134845e702f2aa6e6e259afa165a6769.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8509cc5a8db3cd0d7633a8bcc603fddb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-09  Rig3DGS Creating Controllable Portraits from Casual Monocular Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/</id>
    <published>2024-02-09T01:50:48.000Z</published>
    <updated>2024-02-09T01:50:48.075Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-09-æ›´æ–°"><a href="#2024-02-09-æ›´æ–°" class="headerlink" title="2024-02-09 æ›´æ–°"></a>2024-02-09 æ›´æ–°</h1><h2 id="EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation"><a href="#EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation" class="headerlink" title="EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation"></a>EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation</h2><p><strong>Authors:Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun</strong></p><p>Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: <a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a> </p><p><a href="http://arxiv.org/abs/2402.01422v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨è§†è§‰å±æ€§å¼•å¯¼éŸ³é¢‘è§£è€¦å™¨å’Œç»†ç²’åº¦æƒ…ç»ªç³»æ•°é¢„æµ‹æ¨¡å—ï¼Œç²¾ç»†æ§åˆ¶è°ˆè¯å¤´ç”Ÿæˆä¸­çš„æƒ…ç»ªè¡¨è¾¾ï¼Œæå‡ç”Ÿæˆçš„è§†é¢‘çš„è‡ªç„¶æ€§å’ŒçœŸå®æ€§ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æå‡ºè§†è§‰å±æ€§å¼•å¯¼éŸ³é¢‘è§£è€¦å™¨ï¼Œä»…ä¸éŸ³é¢‘å†…å®¹ç›¸å…³çš„è¡¨å¾å‘é‡ï¼Œå¢å¼ºåç»­å£å‹ç³»æ•°é¢„æµ‹çš„ç¨³å®šæ€§ã€‚</li><li>å¼•å…¥ç»†ç²’åº¦æƒ…ç»ªç³»æ•°é¢„æµ‹æ¨¡å—ï¼Œå®ç°æ›´å‡†ç¡®çš„æƒ…ç»ªè¡¨è¾¾ã€‚</li><li>æå‡ºä½¿ç”¨ç»†ç²’åº¦æƒ…ç»ªçŸ©é˜µçš„æƒ…ç»ªå¼ºåº¦æ§åˆ¶æ–¹æ³•ï¼Œå¯¹ç”Ÿæˆçš„è§†é¢‘ä¸­çš„æƒ…ç»ªè¡¨è¾¾è¿›è¡Œæœ‰æ•ˆæ§åˆ¶ï¼Œå¹¶å¯¹æƒ…ç»ªå¼ºåº¦è¿›è¡Œæ›´ç²¾ç»†çš„åˆ†ç±»ã€‚</li><li>3DMM ç³»æ•°ç”Ÿæˆç½‘ç»œç”¨äºé¢„æµ‹ 3D ç³»æ•°ï¼Œç„¶ååˆ©ç”¨æ¸²æŸ“ç½‘ç»œç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚</li><li>EmoSpeaker æ–¹æ³•åœ¨è¡¨æƒ…å˜åŒ–å’Œå”‡å½¢åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æƒ…æ„Ÿè°ˆè¯äººè„¸ç”Ÿæˆæ–¹æ³•ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šEmoSpeakerï¼šå•æ¬¡å­¦ä¹ ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶çš„è¯´è¯äººé¢éƒ¨ç”Ÿæˆ</li><li>ä½œè€…ï¼šGuanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼šè¯´è¯äººé¢éƒ¨ã€ä¸‰ç»´å¯å˜å½¢æ¨¡å‹ã€è§†è§‰å±æ€§å¼•å¯¼çš„è§£è€¦è¿‡ç¨‹ã€ç»†ç²’åº¦æƒ…æ„Ÿæ§åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.01422Github é“¾æ¥ï¼šhttps://github.com/peterfanfan/EmoSpeaker</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººé¢éƒ¨ç”ŸæˆæŠ€æœ¯å·²æˆä¸ºè¿‘å¹´æ¥ç ”ç©¶çš„çƒ­ç‚¹ï¼Œå…¶åœ¨è™šæ‹Ÿæ•°å­—äººç”Ÿæˆã€è™šæ‹Ÿç°å®å’Œç”µå½±ç‰¹æ•ˆç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å”‡å½¢åŒæ­¥å’Œè§†é¢‘ç”Ÿæˆè´¨é‡ï¼Œå¯¹ç”Ÿæˆè§†é¢‘çš„æƒ…æ„Ÿè¡¨è¾¾å…³æ³¨è¾ƒå°‘ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸€äº›ç°æœ‰æ–¹æ³•è§£å†³äº†æƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ç”Ÿæˆçš„é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸å—é™äºé•¿è§†é¢‘æˆ–çŸ­è§†é¢‘çš„é©±åŠ¨ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ ‡ç­¾æ§åˆ¶çš„æ–¹æ³•éš¾ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒå¼ºåº¦å’Œä¸åŒæƒ…æ„Ÿä¸­é—´çŠ¶æ€çš„æƒ…æ„Ÿè§†é¢‘ã€‚å•æ¬¡å­¦ä¹ ç”Ÿæˆæ–¹æ³•é€šå¸¸åªè€ƒè™‘å”‡å½¢åŒæ­¥ï¼Œè€Œæ²¡æœ‰è€ƒè™‘æƒ…æ„Ÿå› ç´ ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º EmoSpeaker çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ 3D ç³»æ•°ä½œä¸ºä¸­é—´è¡¨ç¤ºæ¥è¿æ¥è¯´è¯äººé¢éƒ¨ç”Ÿæˆè¿‡ç¨‹çš„ä¸åŒéƒ¨åˆ†ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œé¦–å…ˆå¼•å…¥è§†è§‰å±æ€§å¼•å¯¼çš„éŸ³é¢‘è§£è€¦å™¨ï¼Œä»éŸ³é¢‘ä¸­æå–ä¸å†…å®¹å‘é‡ç›¸å…³çš„å†…å®¹å‘é‡ï¼Œå¢å¼ºåç»­å”‡éƒ¨åŠ¨ä½œç³»æ•°é¢„æµ‹çš„ç¨³å®šæ€§ã€‚å…¶æ¬¡ï¼Œåœ¨ç»†ç²’åº¦æƒ…æ„Ÿç³»æ•°é¢„æµ‹æ¨¡å—ä¸­ï¼Œå°†å†…å®¹å‘é‡ä¸æƒ…æ„Ÿæ ‡ç­¾èšåˆï¼Œé¢„æµ‹ç»†ç²’åº¦çš„é¢éƒ¨åŠ¨ä½œç³»æ•°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç»†ç²’åº¦æƒ…æ„ŸçŸ©é˜µçš„æƒ…æ„Ÿå¼ºåº¦æ§åˆ¶æ–¹æ³•ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œå®ç°äº†å¯¹ç”Ÿæˆè§†é¢‘ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾çš„æœ‰æ•ˆæ§åˆ¶å’Œå¯¹æƒ…æ„Ÿå¼ºåº¦çš„æ›´ç²¾ç»†åˆ†ç±»ã€‚æœ€åï¼Œè®¾è®¡äº†ä¸€ç³»åˆ— 3DMM ç³»æ•°ç”Ÿæˆç½‘ç»œæ¥é¢„æµ‹ 3D ç³»æ•°ï¼Œç„¶ååˆ©ç”¨æ¸²æŸ“ç½‘ç»œç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„ EmoSpeaker æ–¹æ³•åœ¨è¡¨æƒ…è¡¨è¾¾å’Œå”‡å½¢åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æƒ…æ„Ÿè¯´è¯äººé¢éƒ¨ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œç”Ÿæˆå…·æœ‰ä»»æ„å¼ºåº¦çš„ä»»æ„æƒ…æ„Ÿé¢éƒ¨è§†é¢‘ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰è§†è§‰å±æ€§å¼•å¯¼çš„éŸ³é¢‘è§£è€¦å™¨ï¼šä¸ºäº†å‡†ç¡®é¢„æµ‹å”‡éƒ¨ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§è§†è§‰å±æ€§å¼•å¯¼çš„éŸ³é¢‘è§£è€¦å™¨ã€‚è¯¥è§£è€¦å™¨åˆ©ç”¨é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ä½œä¸ºè§†è§‰ä¿¡æ¯ï¼ŒæŒ‡å¯¼éŸ³é¢‘çš„æƒ…æ„Ÿè§£è€¦è¿‡ç¨‹ï¼Œå¢å¼ºè§£è€¦çš„ç²¾åº¦å’Œå¯æ§æ€§ã€‚ï¼ˆ2ï¼‰ç»†ç²’åº¦æƒ…æ„Ÿç³»æ•°é¢„æµ‹æ¨¡å—ï¼šå°†å†…å®¹å‘é‡ä¸æƒ…æ„Ÿæ ‡ç­¾èšåˆï¼Œé¢„æµ‹ç»†ç²’åº¦çš„é¢éƒ¨åŠ¨ä½œç³»æ•°ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨ç»†ç²’åº¦æƒ…æ„ŸçŸ©é˜µçš„æƒ…æ„Ÿå¼ºåº¦æ§åˆ¶æ–¹æ³•ï¼Œå®ç°å¯¹ç”Ÿæˆè§†é¢‘ä¸­æƒ…æ„Ÿè¡¨è¾¾çš„æœ‰æ•ˆæ§åˆ¶å’Œå¯¹æƒ…æ„Ÿå¼ºåº¦çš„æ›´ç²¾ç»†åˆ†ç±»ã€‚ï¼ˆ3ï¼‰æƒ…æ„Ÿé¢éƒ¨æ¸²æŸ“å™¨ï¼šè®¾è®¡äº†ä¸€ç³»åˆ—ä¸‰ç»´å¯å˜å½¢æ¨¡å‹ç³»æ•°ç”Ÿæˆç½‘ç»œæ¥é¢„æµ‹ä¸‰ç»´ç³»æ•°ï¼Œç„¶ååˆ©ç”¨æ¸²æŸ“ç½‘ç»œç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º EmoSpeaker çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•ä»…éœ€éŸ³é¢‘å‰ªè¾‘ã€è‚–åƒã€ç‰¹å®šæƒ…ç»ªå’Œå¼ºåº¦ç²’åº¦ï¼Œå³å¯ç”Ÿæˆå…·æœ‰ç»†ç²’åº¦å¼ºåº¦çš„è¡¨æƒ…é¢éƒ¨ã€‚è¯¥æ–¹æ³•ä½¿ç”¨é¢éƒ¨æƒ…ç»ªè§£è€¦æ¨¡å—æå–å†…å®¹ç‰¹å¾ï¼Œå¹¶ç»“åˆç»†ç²’åº¦å¼ºåº¦æ§åˆ¶æ¨¡å—æ¥å®ç°ä»»æ„æƒ…ç»ªå¼ºåº¦ã€‚è¿™åœ¨ç”µå­æ¸¸æˆã€è™šæ‹Ÿç°å®ã€ç”µå½±ç‰¹æ•ˆå’Œäººæœºç•Œé¢äº¤äº’ç­‰é¢†åŸŸå±•ç¤ºäº†æœ‰å¸Œæœ›çš„åº”ç”¨ã€‚ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆæ›´ä¸°å¯Œçš„é¢éƒ¨åŠ¨ç”»æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚æœªæ¥çš„ç ”ç©¶å°†é›†ä¸­äºåœ¨ç»†ç²’åº¦å¼ºåº¦æ§åˆ¶é¢†åŸŸè¿›è¡Œæ·±å…¥ç ”ç©¶ï¼Œä»¥å¢å¼ºæ›´å…·è¡¨ç°åŠ›å’Œç»†å¾®å·®åˆ«çš„é¢éƒ¨åŠ¨ç”»çš„ç”Ÿæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§è§†è§‰å±æ€§å¼•å¯¼çš„éŸ³é¢‘è§£è€¦å™¨ï¼Œè¯¥è§£è€¦å™¨åˆ©ç”¨é¢éƒ¨åŠ¨ä½œå•å…ƒä½œä¸ºè§†è§‰ä¿¡æ¯ï¼ŒæŒ‡å¯¼éŸ³é¢‘çš„æƒ…æ„Ÿè§£è€¦è¿‡ç¨‹ï¼Œå¢å¼ºè§£è€¦çš„ç²¾åº¦å’Œå¯æ§æ€§ã€‚</li><li>æå‡ºäº†ä¸€ç§ç»†ç²’åº¦æƒ…æ„Ÿç³»æ•°é¢„æµ‹æ¨¡å—ï¼Œå°†å†…å®¹å‘é‡ä¸æƒ…æ„Ÿæ ‡ç­¾èšåˆï¼Œé¢„æµ‹ç»†ç²’åº¦çš„é¢éƒ¨åŠ¨ä½œç³»æ•°ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ä½¿ç”¨ç»†ç²’åº¦æƒ…æ„ŸçŸ©é˜µçš„æƒ…æ„Ÿå¼ºåº¦æ§åˆ¶æ–¹æ³•ï¼Œå®ç°å¯¹ç”Ÿæˆè§†é¢‘ä¸­æƒ…æ„Ÿè¡¨è¾¾çš„æœ‰æ•ˆæ§åˆ¶å’Œå¯¹æƒ…æ„Ÿå¼ºåº¦çš„æ›´ç²¾ç»†åˆ†ç±»ã€‚</li><li>è®¾è®¡äº†ä¸€ç³»åˆ—ä¸‰ç»´å¯å˜å½¢æ¨¡å‹ç³»æ•°ç”Ÿæˆç½‘ç»œæ¥é¢„æµ‹ä¸‰ç»´ç³»æ•°ï¼Œç„¶ååˆ©ç”¨æ¸²æŸ“ç½‘ç»œç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚æ€§èƒ½ï¼š</li><li>åœ¨è¡¨æƒ…è¡¨è¾¾å’Œå”‡å½¢åŒæ­¥æ–¹é¢ä¼˜äºç°æœ‰æƒ…æ„Ÿè¯´è¯äººé¢éƒ¨ç”Ÿæˆæ–¹æ³•ã€‚</li><li>å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼Œç”Ÿæˆå…·æœ‰ä»»æ„å¼ºåº¦çš„ä»»æ„æƒ…æ„Ÿé¢éƒ¨è§†é¢‘ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦æ”¶é›†å¤§é‡çš„æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ã€‚</li><li>æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹æ¯”è¾ƒè€—æ—¶ã€‚</li><li>éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ä¸åŒçš„æ•°æ®é›†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-6bacdbeff940a1345ff38f8b1dc2680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c646c87add1ea43ace17da06ebd12a7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d08dc09fd1df64224ed8ef166ac7d5b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0dc431600d1c5672918ab10a962f79ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7d1b798a4f9c96adf7e70cbb6847a5b3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4c97492b45a0ba3e2e8b06c0abf4372f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5ce57abfa37d7135a925aa7ba77e6120.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-09  EmoSpeaker One-shot Fine-grained Emotion-Controlled Talking Face   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/</id>
    <published>2024-02-09T01:46:05.000Z</published>
    <updated>2024-02-09T01:46:05.555Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-09-æ›´æ–°"><a href="#2024-02-09-æ›´æ–°" class="headerlink" title="2024-02-09 æ›´æ–°"></a>2024-02-09 æ›´æ–°</h1><h2 id="Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation"><a href="#Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation" class="headerlink" title="Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation"></a>Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</h2><p><strong>Authors:Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</strong></p><p>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images. </p><p><a href="http://arxiv.org/abs/2402.04929v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.01701</p><p><strong>Summary</strong><br>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›è¿›è¡Œæ— æºåŸŸè‡ªé€‚åº”ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç›®æ ‡å›¾åƒçš„ç‰¹å¾å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œç”ŸæˆæºåŸŸå›¾åƒã€‚</li><li>ç›®æ ‡æ˜¯ç”Ÿæˆç†µæœ€å°åŒ–ä¸”å¯¹é¢„è®­ç»ƒæºæ¨¡å‹çš„ç½®ä¿¡åº¦æœ€å¤§çš„æºæ ·æœ¬ã€‚</li><li>ç›´æ¥åœ¨ç›®æ ‡å›¾åƒåˆ†å¸ƒä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œè€Œæ— éœ€æˆå¯¹çš„æºå’Œç›®æ ‡å›¾åƒã€‚</li><li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨ Office-31ã€Office-Home å’Œ VisDA ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>ç”Ÿæˆçš„é«˜è´¨é‡æºå›¾åƒæœ‰åŠ©äºè·¨åŸŸä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ã€‚</li><li>å……åˆ†åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨æºå’Œç›®æ ‡åŸŸä¹‹é—´å»ºç«‹æ¡¥æ¢ã€‚</li><li>æ— æºåŸŸè‡ªé€‚åº”çš„æ¨¡å‹å…·æœ‰é™å™ªæ•ˆæœï¼Œæœ‰åŠ©äºæé«˜åˆ†ç±»å’Œæ£€æµ‹çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£å¼•å¯¼æºæ•°æ®ç”Ÿæˆçš„æ— æºåŸŸè‡ªé€‚åº”</li><li>ä½œè€…ï¼šShivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</li><li>å•ä½ï¼šä½æ²»äºšç†å·¥å­¦é™¢è®¡ç®—æœºç³»</li><li>å…³é”®è¯ï¼šæ— æºåŸŸè‡ªé€‚åº”ã€æ‰©æ•£æ¨¡å‹ã€æ•°æ®ç”Ÿæˆã€è·¨åŸŸå›¾åƒåˆ†ç±»</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04929Githubï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§çš„å‡è®¾é™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚é¢†åŸŸè‡ªé€‚åº”ï¼ˆDAï¼‰æ—¨åœ¨å‡å°‘è¿™ç§å·®å¼‚ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè·¨å¤šä¸ªé¢†åŸŸè¡¨ç°è‰¯å¥½ã€‚ä¼ ç»Ÿ DA æ–¹æ³•ä¾èµ–äºå›ºå®šçš„æºæ•°æ®ï¼Œå¯èƒ½éš¾ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„é¢†åŸŸã€‚æ— æºåŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šç±»å‹çš„ DAï¼Œå®ƒä¸éœ€è¦è®¿é—®æºè®­ç»ƒæ•°æ®ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ä¸é—®é¢˜ï¼šç°æœ‰çš„å¤§å¤šæ•° SFDA æ–¹æ³•é€šè¿‡åœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­èåˆä¸¤ä¸ªä¸åŒçš„æ•°æ®åˆ†å¸ƒæ¥å®ç°æ¨¡å‹é€‚åº”æ€§ã€‚ä¸€ç§å®ç°æ— æºæ–¹å¼çš„æ–¹æ³•æ˜¯ä½¿ç”¨åˆæˆç”Ÿæˆçš„æºæ•°æ®ã€‚ç„¶è€Œï¼Œç”Ÿæˆå‡†ç¡®è¡¨ç¤ºæºåŸŸå¤šæ ·æ€§å’Œå¤æ‚æ€§çš„åˆæˆæºæ•°æ®å¯èƒ½å¾ˆå›°éš¾ã€‚æ­¤å¤–ï¼Œå¦‚æœåˆæˆæ•°æ®è´¨é‡ä¸é«˜ï¼Œå¯èƒ½ä¼šå¼•å…¥å™ªå£°å’Œä¸ä¸€è‡´æ€§ï¼Œä»è€Œå¯¹æ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸Šçš„æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º DM-SFDA çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰çš„æ³›åŒ–èƒ½åŠ›æ¥è§£å†³ SFDA çš„æŒ‘æˆ˜ã€‚DM-SFDA çš„æ ¸å¿ƒæ€æƒ³æ˜¯å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥ä½¿ç”¨æ¥è‡ªç›®æ ‡å›¾åƒçš„ç‰¹å¾æ¥ç”ŸæˆæºåŸŸå›¾åƒï¼Œä»è€ŒæŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«å¾®è°ƒä»¥ç”Ÿæˆæºæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æœ€å°åŒ–ç†µå¹¶æœ€å¤§åŒ–é¢„è®­ç»ƒæºæ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚ç„¶åï¼Œå°†å·²å»ºç«‹çš„æ— ç›‘ç£åŸŸé€‚åº”æŠ€æœ¯åº”ç”¨äºå°†ç”Ÿæˆçš„æºå›¾åƒä¸ç›®æ ‡åŸŸæ•°æ®å¯¹é½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•çš„æ€§èƒ½ï¼šæœ¬æ–‡åœ¨ Office-31ã€Office-Home å’Œ VisDA ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå¯¹ DM-SFDA è¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒDM-SFDA åœ¨ SFDA ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾ç€çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³ã€ç‰¹å®šäºé¢†åŸŸçš„å›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³ SFDA é—®é¢˜ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æºåŸŸè‡ªé€‚åº”ï¼ˆDM-SFDAï¼‰æ–¹æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥ä½¿ç”¨æ¥è‡ªç›®æ ‡å›¾åƒçš„ç‰¹å¾æ¥ç”ŸæˆæºåŸŸå›¾åƒï¼Œä»è€ŒæŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚(2): é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¢«å¾®è°ƒä»¥ç”Ÿæˆæºæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æœ€å°åŒ–ç†µå¹¶æœ€å¤§åŒ–é¢„è®­ç»ƒæºæ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚(3): ç„¶åï¼Œå°†å·²å»ºç«‹çš„æ— ç›‘ç£åŸŸé€‚åº”æŠ€æœ¯åº”ç”¨äºå°†ç”Ÿæˆçš„æºå›¾åƒä¸ç›®æ ‡åŸŸæ•°æ®å¯¹é½ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰é‡è¦æ€§ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æºåŸŸè‡ªé€‚åº”ï¼ˆDM-SFDAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰çš„æ³›åŒ–èƒ½åŠ›æ¥è§£å†³SFDAçš„æŒ‘æˆ˜ã€‚DM-SFDAçš„æ ¸å¿ƒæ€æƒ³æ˜¯å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥ä½¿ç”¨æ¥è‡ªç›®æ ‡å›¾åƒçš„ç‰¹å¾æ¥ç”ŸæˆæºåŸŸå›¾åƒï¼Œä»è€ŒæŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾ç€çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³ã€ç‰¹å®šäºé¢†åŸŸçš„å›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚</p><p>ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹ï¼š</p><p>åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æºåŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼ˆDGMï¼‰çš„æ³›åŒ–èƒ½åŠ›æ¥è§£å†³SFDAçš„æŒ‘æˆ˜ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„ç”Ÿæˆæºå›¾åƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ¥è‡ªç›®æ ‡å›¾åƒçš„ç‰¹å¾æ¥æŒ‡å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³ã€ç‰¹å®šäºé¢†åŸŸçš„å›¾åƒã€‚</li></ul><p>æ€§èƒ½ï¼š</p><ul><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾ç€çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³ã€ç‰¹å®šäºé¢†åŸŸçš„å›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚</li></ul><p>å·¥ä½œé‡ï¼š</p><ul><li>éœ€è¦å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>éœ€è¦æ”¶é›†ç›®æ ‡åŸŸçš„æ•°æ®ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¾ˆå›°éš¾ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-ec2a5c717af2a4c67eb4715437c633c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf3cb970b1edbd90925d67dc50ebd458.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b60fc581c86cc20b03dbf6c09543aea2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-282170863545d09c18b118ee88d874e2.jpg" align="middle"></details>## EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World   Illusions**Authors:Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas**Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \url{https://shashankkotyan.github.io/EvoSeed}. [PDF](http://arxiv.org/abs/2402.04699v1) **Summary**åˆ©ç”¨è¿›åŒ–ç­–ç•¥æœç´¢ç®—æ³•æ¡†æ¶ç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ï¼Œä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„é²æ£’æ€§ã€‚**Key Takeaways**- åŸºäºè¿›åŒ–ç­–ç•¥çš„æœç´¢ç®—æ³•æ¡†æ¶ EvoSeed ç”¨äºç”Ÿæˆè‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚- EvoSeed æ¡†æ¶ä½¿ç”¨è¾…åŠ©æ‰©æ•£å’Œåˆ†ç±»å™¨æ¨¡å‹åœ¨ä¸æ¨¡å‹æ— å…³çš„é»‘ç›’è®¾ç½®ä¸­è¿è¡Œã€‚- é‡‡ç”¨ CMA-ES ä¼˜åŒ–å¯¹æŠ—ç§å­å‘é‡çš„æœç´¢ï¼Œè¯¥å‘é‡åœ¨æ¡ä»¶æ‰©æ•£æ¨¡å‹å¤„ç†åï¼Œä¼šç”Ÿæˆåˆ†ç±»å™¨æ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ— é™åˆ¶è‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚- å®éªŒè¡¨æ˜ç”Ÿæˆçš„å¯¹æŠ—å›¾åƒå…·æœ‰é«˜å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„åˆ†ç±»å™¨ã€‚- è¯¥æ–¹æ³•è¯æ˜äº†ä½¿ç”¨è¿›åŒ–ç®—æ³•æ¥å¢å¼ºå¯¹æŠ—æ ·æœ¬è´¨é‡çš„æ½œåŠ›ã€‚- å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½å¤Ÿä¸ºå¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„é²æ£’æ€§å¼€è¾Ÿæ–°é€”å¾„ã€‚- é¡¹ç›®ç½‘ç«™å¯ä»¥è®¿é—®ç½‘å€ï¼šhttps://shashankkotyan.github.io/EvoSeedã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šEvoSeedï¼šæ­ç¤ºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¨èƒ</li><li>ä½œè€…ï¼šShashank Kotyanã€Po Yuan Maoã€Danilo Vasconcellos Vargas</li><li>å•ä½ï¼šä¹å·å¤§å­¦</li><li>å…³é”®è¯ï¼šæ·±åº¦å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€CMA-ESã€æ‰©æ•£æ¨¡å‹ã€è‡ªç„¶å¯¹æŠ—æ ·æœ¬</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04699ï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåœ¨å„ç§è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†ç©ºå‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå½“æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒä¸åŒæ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šä¸‹é™ï¼ŒHendrycks ç­‰äºº[10]å’Œ Ilyas ç­‰äºº[17]çš„ç ”ç©¶è¡¨æ˜äº†è¿™ä¸€ç‚¹ã€‚è¿™ç»™å¼€å‘èƒ½å¤Ÿå¤„ç†è¿™ç§åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å¯¹æŠ—æ ·æœ¬å’Œå¯¹æŠ—æ”»å‡»åˆ©ç”¨äº†è¿™ç§æ¼æ´ï¼Œé€šè¿‡æ“çºµå›¾åƒæ¥æ”¹å˜ä¸åŸå§‹åˆ†å¸ƒç›¸æ¯”çš„åˆ†å¸ƒã€‚Dalvi ç­‰äºº[4]çš„ç ”ç©¶å¼ºè°ƒï¼Œè¾“å…¥æ•°æ®çš„å¯¹æŠ—æ€§æ“çºµé€šå¸¸ä¼šå¯¼è‡´åˆ†ç±»å™¨åšå‡ºä¸æ­£ç¡®çš„é¢„æµ‹ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§çš„ä¸¥é‡æ‹…å¿§ã€‚è¿™ç§æ‹…å¿§ä»ç„¶ç›¸å…³ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æœ€å…ˆè¿›çš„æ·±åº¦ç¥ç»ç½‘ç»œææ˜“å—åˆ°æ¶‰åŠæ•…æ„å¯¹è¾“å…¥è¿›è¡Œæ‰°åŠ¨çš„å¯¹æŠ—æ€§æ”»å‡»[22, 26]ã€‚å¯¹è¿™äº›æ‰°åŠ¨æ–½åŠ äº†å„ç§çº¦æŸï¼Œä½¿è¿™äº›æ‰°åŠ¨å˜å¾—å¾®å¦™ä¸”éš¾ä»¥æ£€æµ‹ã€‚ä¾‹å¦‚ï¼Œğ¿0å¯¹æŠ—æ”»å‡»ï¼Œä¾‹å¦‚ One-Pixel Attack[22, 38]é™åˆ¶äº†æ‰°åŠ¨åƒç´ çš„æ•°é‡ï¼Œğ¿2å¯¹æŠ—æ”»å‡»ï¼Œä¾‹å¦‚ PGD-L2[26]é™åˆ¶äº†ä¸åŸå§‹å›¾åƒçš„æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œå¹¶ä¸”ğ¿âˆå¯¹æŠ—æ”»å‡»ï¼Œä¾‹å¦‚ PGD-Lâˆ[26]é™åˆ¶äº†æ‰€æœ‰åƒç´ çš„å˜åŒ–é‡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè™½ç„¶å¯¹æŠ—æ ·æœ¬[22, 26, 38]æš´éœ²äº†æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„æ¼æ´ï¼Œä½†å®ƒä»¬çš„äººå·¥æ€§è´¨å’Œå¯¹å—é™è¾“å…¥æ•°æ®çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨å®é™…æƒ…å†µä¸‹ï¼ŒæŒ‘æˆ˜å˜å¾—æ›´åŠ æ˜æ˜¾ï¼Œå› ä¸ºå°†æ‰€æœ‰æ½œåœ¨å¨èƒå…¨é¢åœ°åŒ…å«åœ¨è®­ç»ƒæ•°æ®é›†ä¸­å˜å¾—ä¸å¯è¡Œã€‚è¿™ç§å¤æ‚æ€§çªå‡ºäº†æ·±åº¦ç¥ç»ç½‘ç»œå¯¹ Hendrycks ç­‰äºº[10]æå‡ºçš„è‡ªç„¶å¯¹æŠ—ç¤ºä¾‹å’Œ Song ç­‰äºº[37]æå‡ºçš„æ— é™åˆ¶å¯¹æŠ—ç¤ºä¾‹çš„æ•æ„Ÿæ€§ä¸æ–­æé«˜ã€‚è¿‘å¹´æ¥ï¼Œè¿™äº›ç±»å‹çš„å¯¹æŠ—æ ·æœ¬åœ¨å¯¹æŠ—æ”»å‡»ç ”ç©¶ä¸­è·å¾—äº†çªå‡ºåœ°ä½ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¯¹å›¾åƒè¿›è¡Œå®è´¨æ€§æ”¹å˜ï¼Œè€Œä¸ä¼šæ˜¾ç€å½±å“äººç±»å¯¹å…¶å«ä¹‰å’ŒçœŸå®æ€§çš„æ„ŸçŸ¥ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨è¿™æ ·çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº† EvoSeedï¼Œè¿™æ˜¯ä¸€ç§ç¬¬ä¸€ä¸ªåŸºäºè¿›åŒ–ç­–ç•¥çš„ç®—æ³•æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå¦‚å›¾ 2 æ‰€ç¤ºçš„æ— é™åˆ¶è‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç®—æ³•éœ€è¦ä¸€ä¸ªæ¡ä»¶æ‰©æ•£æ¨¡å‹ğºå’Œä¸€ä¸ªåˆ†ç±»å™¨æ¨¡å‹ğ¹æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ğ‘¥ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ©ç”¨åæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ (CMA-ES) ä½œä¸ºå…¶æ ¸å¿ƒæ¥å¢å¼ºå¯¹èƒ½å¤Ÿç”Ÿæˆå¯¹æŠ—æ ·æœ¬ğ‘¥çš„å¯¹æŠ—ç§å­å‘é‡ğ‘§â€²çš„æœç´¢ã€‚CMA-ES å¯¹å™ªå£°ç§å­å‘é‡ğ‘§â€²è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œè¯¥ç›®æ ‡å‡½æ•°å°†åˆ†ç±»å™¨æ¨¡å‹ğ¹çš„è¾“å‡ºä¸äººç±»å¯¹å›¾åƒğ‘¥çš„æ„ŸçŸ¥ä¹‹é—´çš„å·®å¼‚ä½œä¸ºæƒ©ç½šã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒè¡¨æ˜ï¼Œç”Ÿæˆçš„å¯¹æŠ—å›¾åƒå…·æœ‰å¾ˆé«˜çš„å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯æ˜äº†ä½¿ç”¨è¿›åŒ–ç®—æ³•æé«˜å¯¹æŠ—æ ·æœ¬è´¨é‡çš„å‰æ™¯ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„é²æ£’æ€§å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰éšæœºç§å­æ³•ï¼ˆRandSeedï¼‰ï¼šåŸºäºéšæœºåç§»çš„éšæœºæœç´¢ç­–ç•¥ï¼Œé€šè¿‡åœ¨åˆå§‹ç§å­å‘é‡ä¸Šæ·»åŠ éšæœºæ‰°åŠ¨æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚ï¼ˆ2ï¼‰è¿›åŒ–ç§å­æ³•ï¼ˆEvoSeedï¼‰ï¼šåŸºäºåæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰çš„ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–åˆå§‹ç§å­å‘é‡æ¥æœç´¢å¯¹æŠ—ç§å­å‘é‡ï¼Œä»¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚ï¼ˆ3ï¼‰æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆConditional Diffusion Modelï¼‰ï¼šç”¨äºç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡æ¡ä»¶ä¿¡æ¯å’Œåˆå§‹ç§å­å‘é‡ç”Ÿæˆå›¾åƒã€‚ï¼ˆ4ï¼‰åˆ†ç±»å™¨æ¨¡å‹ï¼ˆClassifier Modelï¼‰ï¼šç”¨äºè¯„ä¼°å¯¹æŠ—æ ·æœ¬è´¨é‡çš„åˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡è®¡ç®—å¯¹æŠ—æ ·æœ¬çš„åˆ†ç±»é”™è¯¯ç‡æ¥è¡¡é‡å¯¹æŠ—æ ·æœ¬çš„æ”»å‡»æˆåŠŸç‡ã€‚ï¼ˆ5ï¼‰æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼šè¡¡é‡å¯¹æŠ—æ ·æœ¬æ”»å‡»æˆåŠŸç‡çš„æŒ‡æ ‡ï¼Œè®¡ç®—ä¸ºå¯¹æŠ—æ ·æœ¬è¢«åˆ†ç±»å™¨é”™è¯¯åˆ†ç±»çš„æ¯”ä¾‹ã€‚ï¼ˆ6ï¼‰å¼—é›·æ­‡ç‰¹èµ·å§‹è·ç¦»ï¼ˆFIDï¼‰ï¼šè¡¡é‡å¯¹æŠ—æ ·æœ¬ä¸çœŸå®æ ·æœ¬åˆ†å¸ƒå·®å¼‚çš„æŒ‡æ ‡ï¼Œè®¡ç®—ä¸ºå¯¹æŠ—æ ·æœ¬ä¸çœŸå®æ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ã€‚ï¼ˆ7ï¼‰æ„ŸçŸ¥è¯„åˆ†ï¼ˆISï¼‰ï¼šè¡¡é‡å¯¹æŠ—æ ·æœ¬è´¨é‡çš„æŒ‡æ ‡ï¼Œè®¡ç®—ä¸ºå¯¹æŠ—æ ·æœ¬åœ¨åˆ†ç±»å™¨ä¸Šçš„å¹³å‡å¯¹æ•°ä¼¼ç„¶å€¼ã€‚ï¼ˆ8ï¼‰ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰ï¼šè¡¡é‡å¯¹æŠ—æ ·æœ¬ä¸çœŸå®æ ·æœ¬åœ¨ç»“æ„ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œè®¡ç®—ä¸ºå¯¹æŠ—æ ·æœ¬ä¸çœŸå®æ ·æœ¬åœ¨åƒç´ ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šEvoSeedæ˜¯ä¸€ç§åŸºäºè¿›åŒ–ç­–ç•¥çš„ç®—æ³•æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆæ— é™åˆ¶è‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚å®ƒåˆ©ç”¨åæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰ä½œä¸ºå…¶æ ¸å¿ƒæ¥å¢å¼ºå¯¹èƒ½å¤Ÿç”Ÿæˆå¯¹æŠ—æ ·æœ¬ğ‘¥çš„å¯¹æŠ—ç§å­å‘é‡ğ‘§â€²çš„æœç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆçš„å¯¹æŠ—å›¾åƒå…·æœ‰å¾ˆé«˜çš„å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯æ˜äº†ä½¿ç”¨è¿›åŒ–ç®—æ³•æé«˜å¯¹æŠ—æ ·æœ¬è´¨é‡çš„å‰æ™¯ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„é²æ£’æ€§å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºè¿›åŒ–ç­–ç•¥çš„ç®—æ³•æ¡†æ¶EvoSeedï¼Œç”¨äºç”Ÿæˆæ— é™åˆ¶è‡ªç„¶å¯¹æŠ—æ ·æœ¬ã€‚</li><li>åˆ©ç”¨åæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ï¼ˆCMA-ESï¼‰ä½œä¸ºæ ¸å¿ƒæ¥å¢å¼ºå¯¹èƒ½å¤Ÿç”Ÿæˆå¯¹æŠ—æ ·æœ¬ğ‘¥çš„å¯¹æŠ—ç§å­å‘é‡ğ‘§â€²çš„æœç´¢ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œç”Ÿæˆçš„å¯¹æŠ—å›¾åƒå…·æœ‰å¾ˆé«˜çš„å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„åˆ†ç±»å™¨ã€‚æ€§èƒ½ï¼š</li><li>EvoSeedç”Ÿæˆçš„å¯¹æŠ—å›¾åƒå…·æœ‰å¾ˆé«˜çš„å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„åˆ†ç±»å™¨ã€‚</li><li>EvoSeedåœ¨ImageNetæ•°æ®é›†ä¸Šå®ç°äº†99.9%çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶ä¸”åœ¨CIFAR-10æ•°æ®é›†ä¸Šå®ç°äº†99.8%çš„æ”»å‡»æˆåŠŸç‡ã€‚</li><li>EvoSeedç”Ÿæˆçš„å¯¹æŠ—å›¾åƒåœ¨å¼—é›·æ­‡ç‰¹èµ·å§‹è·ç¦»ï¼ˆFIDï¼‰å’Œæ„ŸçŸ¥è¯„åˆ†ï¼ˆISï¼‰æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>EvoSeedçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºä¸åŒçš„æ•°æ®é›†å’Œåˆ†ç±»å™¨ã€‚</li><li>EvoSeedçš„è®­ç»ƒæ—¶é—´ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ç›¸å¯¹è¾ƒçŸ­ã€‚</li><li>EvoSeedå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¯¹æŠ—å›¾åƒï¼Œè€Œä¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-fba3784cdfd913938a2c25b5d6802005.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1191333c7b6b916696b230758671066a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d57cb25c209c458064f830f4a1d7c2d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5b515b564419e732b66802017f00ce12.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b4c2ccc13f827d3ede06ea04ae36e1da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5df7bec69fc3a0aa2cf5d26456e611b5.jpg" align="middle"></details><h2 id="BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception"><a href="#BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception" class="headerlink" title="BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception"></a>BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception</h2><p><strong>Authors:Aniket Roy, Anirban Roy, Soma Mitra, Kuntal Ghosh</strong></p><p>Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: <a href="https://github.com/aniket004/BRI3L">https://github.com/aniket004/BRI3L</a> </p><p><a href="http://arxiv.org/abs/2402.04541v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ å¯ä»¥è¯†åˆ«å’Œå®šä½äº®åº¦é”™è§‰ï¼Œç”šè‡³å¯ä»¥ç”Ÿæˆæ–°çš„é”™è§‰å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¤§è§„æ¨¡äº®åº¦é”™è§‰æ•°æ®é›†BRI3Lï¼ŒåŒ…å«22,366å¼ å›¾åƒï¼Œæ¶µç›–äº”ç§é”™è§‰ç±»å‹ã€‚</li><li>æ•°æ®é›†åŒ…å«æ ‡ç­¾ä¿¡æ¯ï¼Œå¯ç”¨äºè¯„ä¼°åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚</li><li>åŸºäºæ•°æ®é©±åŠ¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¯¥æ•°æ®é›†ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li><li>æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œå¦‚äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚</li><li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆäº®åº¦é”™è§‰å›¾åƒã€‚</li><li>è¯¥ç ”ç©¶ä¸ºè§†è§‰é”™è§‰çš„ç†è§£å’Œè¯„ä¼°æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li><li>è¯¥ç ”ç©¶çš„æ•°æ®é›†å’Œä»£ç å·²å¼€æºï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šBRI3Lï¼šäº®åº¦é”™è§‰å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè¯†åˆ«å’Œå®šä½é”™è§‰æ„ŸçŸ¥åŒºåŸŸ</li><li>ä½œè€…ï¼šAniket Roy, Anirban Roy, Soma Mitr, Kuntal Ghosh</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šçº¦ç¿°Â·éœæ™®é‡‘æ–¯å¤§å­¦</li><li>å…³é”®è¯ï¼šè§†è§‰é”™è§‰ï¼Œæ„ŸçŸ¥</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.04541ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/aniket004/BRI3L</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè§†è§‰é”™è§‰åœ¨ç†è§£è§†è§‰æ„ŸçŸ¥ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å½“å‰ç†è§£å’Œè¯„ä¼°è§†è§‰é”™è§‰çš„æ–¹æ³•ä¸»è¦æ˜¯åŸºäºç¡®å®šæ€§æ»¤æ³¢çš„æ–¹æ³•ï¼Œå¹¶ä¸”å®ƒä»¬å¯¹å°‘æ•°è§†è§‰é”™è§‰è¿›è¡Œè¯„ä¼°ï¼Œå› æ­¤ç»“è®ºä¸å…·æœ‰æ™®éæ€§ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼Œæ–¹æ³•åŠ¨æœºï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å« 22,366 å¼ å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆBRI3Lï¼šäº®åº¦é”™è§‰å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè¯†åˆ«å’Œå®šä½é”™è§‰æ„ŸçŸ¥ï¼‰ï¼Œå…¶ä¸­åŒ…å«äº”ç§ç±»å‹çš„äº®åº¦é”™è§‰ï¼Œå¹¶ä½¿ç”¨æ•°æ®é©±åŠ¨çš„åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•å¯¹è¯¥æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«æ ‡ç­¾ä¿¡æ¯â€”â€”ï¼ˆ1ï¼‰ç‰¹å®šå›¾åƒæ˜¯å¦å…·æœ‰é”™è§‰/éé”™è§‰ï¼Œï¼ˆ2ï¼‰å›¾åƒä¸­é”™è§‰åŒºåŸŸçš„åˆ†å‰²æ©ç ã€‚å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ•°æ®é›†è¯„ä¼°åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬éµå¾ªæ¶‰åŠäººç±»å—è¯•è€…çš„æ ‡å‡†å¿ƒç†ç‰©ç†å®éªŒæ¥éªŒè¯æ•°æ®é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å¼€å‘è§†è§‰é”™è§‰æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å¯¹é”™è§‰åˆ†ç±»å’Œå®šä½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è€ƒè™‘äº†äº”ç§ç ”ç©¶å……åˆ†çš„äº®åº¦é”™è§‰ç±»å‹ï¼š1) èµ«å°”æ›¼ç½‘æ ¼ï¼Œ2) åŒæ­¥äº®åº¦å¯¹æ¯”ï¼Œ3) ç™½è‰²é”™è§‰ï¼Œ4) ç½‘æ ¼é”™è§‰ï¼Œ5) æ„Ÿåº”å…‰æ …é”™è§‰ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•å®ç°äº† 99.56% çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ 84.37% çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº®åº¦é”™è§‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ GitHub ä»“åº“ä¸­æä¾›äº†æ‰€æœ‰ä»£ç ã€æ•°æ®é›†ã€æŒ‡ä»¤é›†ç­‰ï¼šhttps://github.com/aniket004/BRI3L(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬éµå¾ªæ¶‰åŠäººç±»å—è¯•è€…çš„æ ‡å‡†å¿ƒç†ç‰©ç†å®éªŒæ¥éªŒè¯æ•°æ®é›†ã€‚æˆ‘ä»¬è€ƒè™‘äº†äº”ç§ç ”ç©¶å……åˆ†çš„äº®åº¦é”™è§‰ç±»å‹ï¼š1) èµ«å°”æ›¼ç½‘æ ¼ï¼Œ2) åŒæ­¥äº®åº¦å¯¹æ¯”ï¼Œ3) ç™½è‰²é”™è§‰ï¼Œ4) ç½‘æ ¼é”™è§‰ï¼Œ5) æ„Ÿåº”å…‰æ …é”™è§‰ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•å®ç°äº† 99.56% çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ 84.37% çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº®åº¦é”™è§‰çš„èƒ½åŠ›ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨è¯¥æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•å®ç°äº† 99.56% çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ 84.37% çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬å¼€å‘ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†å’Œä½¿ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å¯¹é”™è§‰åˆ†ç±»å’Œå®šä½è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„ç›®æ ‡ã€‚</li></ol><p>7.<methods>ï¼š(1) æˆ‘ä»¬éµå¾ªæ¶‰åŠäººç±»å—è¯•è€…çš„æ ‡å‡†å¿ƒç†ç‰©ç†å®éªŒæ¥éªŒè¯æ•°æ®é›†ã€‚(2) æˆ‘ä»¬è€ƒè™‘äº†äº”ç§ç ”ç©¶å……åˆ†çš„äº®åº¦é”™è§‰ç±»å‹ï¼šèµ«å°”æ›¼ç½‘æ ¼ã€åŒæ­¥äº®åº¦å¯¹æ¯”ã€ç™½è‰²é”™è§‰ã€ç½‘æ ¼é”™è§‰ã€æ„Ÿåº”å…‰æ …é”™è§‰ã€‚(3) åœ¨è¯¥æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•å®ç°äº† 99.56% çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ 84.37% çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚(4) ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚(5) æˆ‘ä»¬è¿˜æµ‹è¯•äº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº®åº¦é”™è§‰çš„èƒ½åŠ›ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºï¼Œå®ƒæä¾›äº†ä¸€ä¸ªåŒ…å«äº”ç§ç±»å‹äº®åº¦é”™è§‰çš„ã€åŒ…å«22,366å¼ å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†BRI3Lï¼Œå¹¶ä½¿ç”¨æ•°æ®é©±åŠ¨çš„åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•å¯¹è¯¥æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†åŒ…å«æ ‡ç­¾ä¿¡æ¯â€”â€”ï¼ˆ1ï¼‰ç‰¹å®šå›¾åƒæ˜¯å¦å…·æœ‰é”™è§‰/éé”™è§‰ï¼Œï¼ˆ2ï¼‰å›¾åƒä¸­é”™è§‰åŒºåŸŸçš„åˆ†å‰²æ©ç ã€‚å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ•°æ®é›†è¯„ä¼°åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚è¿™å°†æœ‰åŠ©äºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£è§†è§‰é”™è§‰ï¼Œå¹¶å¼€å‘æ–°çš„æ–¹æ³•æ¥è¯†åˆ«å’Œå®šä½é”™è§‰æ„ŸçŸ¥åŒºåŸŸã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>é¦–æ¬¡å°è¯•å¼€å‘è§†è§‰é”™è§‰æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å¯¹é”™è§‰åˆ†ç±»å’Œå®šä½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚</li><li>è¯¥æ•°æ®é›†åŒ…å«äº”ç§ç±»å‹äº®åº¦é”™è§‰ï¼Œæ¶µç›–äº†å¤šç§é”™è§‰ç°è±¡ã€‚</li><li>ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå®ç°äº†99.56%çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ84.37%çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚</li><li>ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚</li></ol><p>æ€§èƒ½ï¼š* åœ¨è¯¥æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•å®ç°äº†99.56%çš„é”™è§‰è¯†åˆ«å‡†ç¡®ç‡å’Œ84.37%çš„é”™è§‰å®šä½åƒç´ å‡†ç¡®ç‡ã€‚* ç»“æœè¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åº”ç”¨è¿˜å¯æ¨å¹¿åˆ°æœªè§è¿‡çš„äº®åº¦é”™è§‰ï¼Œä¾‹å¦‚ä»äº®åº¦åŒåŒ–åˆ°å¯¹æ¯”åº¦è½¬æ¢ã€‚</p><p>å·¥ä½œé‡ï¼š* æ”¶é›†å’Œæ³¨é‡Šæ•°æ®çš„å·¥ä½œé‡å¾ˆå¤§ã€‚* å¼€å‘å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å·¥ä½œé‡ä¹Ÿå¾ˆå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-d9494fba06526e8b87f8dd5e3bc6d94a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a61414f51deef787aabe72aa30947292.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9a8fbfbb6ed5b80eb2803e27c328d8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2d1fec65eb07ceea77a12925d47fbae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" align="middle"></details>## Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation**Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao**This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm. [PDF](http://arxiv.org/abs/2402.04031v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**æ‘˜è¦**èšåˆæ‰©æ•£æ¨¡å‹ Polyp-DDPM å¯ç»“åˆæ©ç ç”Ÿæˆé€¼çœŸçš„æ¯è‚‰å›¾åƒï¼Œæœ‰æ•ˆæé«˜èƒƒè‚ é“æ¯è‚‰åˆ†å‰²æ€§èƒ½ã€‚**è¦ç‚¹**- Polyp-DDPM é‡‡ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸä¸”ä¸æ©ç æ¡ä»¶ç›¸ç¬¦çš„æ¯è‚‰å›¾åƒï¼Œæé«˜èƒƒè‚ é“æ¯è‚‰åˆ†å‰²çš„æ€§èƒ½ã€‚- Polyp-DDPM ä»¥åˆ†å‰²æ©ç ï¼ˆè¡¨ç¤ºå¼‚å¸¸åŒºåŸŸçš„äºŒå€¼æ©ç ï¼‰ä¸ºæ¡ä»¶ï¼Œåœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²æ€§èƒ½æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨å›¾åƒè´¨é‡æ–¹é¢ï¼ŒPolyp-DDPM åœ¨ Frechet Inception Distance (FID) å¾—åˆ†ä¸Šè¾¾åˆ° 78.47ï¼Œè€Œç°æœ‰æ–¹æ³•çš„åˆ†æ•°é«˜äº 83.79ã€‚åœ¨åˆ†å‰²æ€§èƒ½æ–¹é¢ï¼ŒPolyp-DDPM åœ¨äº¤é›†æ¯” (IoU) ä¸Šè¾¾åˆ° 0.7156ï¼Œè€ŒåŸºçº¿æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒçš„ IoU å°äº 0.6694ï¼ŒçœŸå®æ•°æ®çš„ IoU ä¸º 0.7067ã€‚- Polyp-DDPM ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„åˆæˆæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒï¼Œä»è€Œæé«˜æ¯è‚‰åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ä½¿å…¶èƒ½å¤Ÿä¸çœŸå®å›¾åƒåª²ç¾ï¼Œå¹¶æä¾›æ›´å¼ºå¤§çš„æ•°æ®å¢å¼ºåŠŸèƒ½æ¥æ”¹è¿›åˆ†å‰²æ¨¡å‹ã€‚- Polyp-DDPM çš„æºä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²åœ¨ https://github.com/mobaidoctor/polyp-ddpm ä¸Šå…¬å¼€å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šPolyp-DDPMï¼šåŸºäºæ‰©æ•£çš„è¯­ä¹‰æ¯è‚‰åˆæˆï¼Œç”¨äºå¢å¼ºåˆ†å‰²</li><li>ä½œè€…ï¼šZolnamar Dorjsembeã€Hsing-Kuo Paoã€Furen Xiao</li><li>éš¶å±å•ä½ï¼šå›½ç«‹å°æ¹¾ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸ä¿¡æ¯å·¥ç¨‹ç³»</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€è¯­ä¹‰æ¯è‚‰åˆæˆã€æ¯è‚‰åˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09766ï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/mobaidoctor/polyp-ddpm</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç»“ç›´è‚ ç™Œæ˜¯å…¨çƒç¬¬ä¸‰å¸¸è§çš„ç™Œç—‡ï¼Œé€šå¸¸å§‹äºç»“ç›´è‚ æ¯è‚‰ï¼Œæ—©æœŸå‘ç°å’Œåˆ‡é™¤æ¯è‚‰å¯é¢„é˜²ç»“ç›´è‚ ç™Œå¹¶é™ä½æ­»äº¡ç‡ã€‚ç„¶è€Œï¼Œåœ¨ç»“è‚ é•œæ£€æŸ¥ä¸­å‘ç°å°æ¯è‚‰å¯èƒ½å¾ˆå›°éš¾ï¼Œè¿™å–å†³äºåŒ»ç”Ÿçš„ä¸“ä¸šçŸ¥è¯†å’Œå…¶ä»–æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ¯è‚‰åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­æ— æ³•è§‚å¯Ÿåˆ°æˆ–è¢«å¿½è§†ã€‚ä¸ºäº†å¢å¼ºæ¯è‚‰æ£€æµ‹ï¼Œç ”ç©¶äººå‘˜æ­£åœ¨åˆ©ç”¨æœºå™¨å­¦ä¹ æ¥è‡ªä¸»è¯†åˆ«å’Œå¼ºè°ƒå†…çª¥é•œæ£€æŸ¥ä¸­çš„æ¯è‚‰ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦å¹¿æ³›ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œè¿™äº›æŠ€æœ¯çš„å‘å±•é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™äº›æ•°æ®é›†å¯¹äºè®­ç»ƒæ¨¡å‹ä»¥å®ç°é«˜å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚åŒ»ç–—ä¿å¥è¡Œä¸šç»å¸¸é¢ä¸´æ­¤ç±»æ•°æ®çš„çŸ­ç¼ºï¼Œè¿™å½’å› äºå¼‚å¸¸åŒºåŸŸå¤–è§‚çš„å¤šæ ·æ€§ã€æ‚£è€…æ‹›å‹Ÿå›°éš¾ã€æ•°æ®æ³¨é‡Šæˆæœ¬é«˜ä»¥åŠå¯¹æ‚£è€…æ•°æ®éšç§çš„æ‹…å¿§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†å‡è½»æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæ¢ç´¢åˆæˆå›¾åƒä½œä¸ºä¸€ç§å¯è¡Œçš„è§£å†³æ–¹æ¡ˆå·²å¼•èµ·å…³æ³¨ã€‚ç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬åŸºäº GAN çš„æ–¹æ³•å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚åŸºäº GAN çš„æ–¹æ³•ï¼Œå¦‚ SinGAN-Segï¼Œèƒ½å¤Ÿç”Ÿæˆæ¯”å…¶ä»– GAN æ¨¡å‹æ›´é€¼çœŸçš„å›¾åƒï¼Œä½†é¢ä¸´å¤šæ ·æ€§å’Œç»†èŠ‚å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œå¦‚ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒï¼Œä½†ç”±äºéœ€è¦ä¸¤ä¸ªæ¨¡å‹ï¼Œå› æ­¤è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£çš„è¯­ä¹‰æ¯è‚‰åˆæˆæ–¹æ³• Polyp-DDPMï¼Œæ—¨åœ¨å¢å¼ºæ¯è‚‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ©æ¨¡å›¾åƒçš„é€é€šé“è¿æ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœä¸æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬åœ¨ Kvasir-SEG æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶å°†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ SinGAN-Seg å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå› ä¸ºè¿™äº›æ–¹æ³•ä»£è¡¨äº†å¸¦æ³¨é‡Šæ¯è‚‰æ•°æ®é›†ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬åŸºäº GAN çš„æ–¹æ³•å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒPolyp-DDPM åœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡æä¾›ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æ¥åˆæˆé«˜è´¨é‡çš„åˆæˆæ¯è‚‰å›¾åƒï¼Œä¸ºä»»ä½•ç»™å®šçš„æ©æ¨¡å›¾åƒåšå‡ºäº†è´¡çŒ®ï¼Œå¯ç”¨äºè®­ç»ƒæ›´å‡†ç¡®çš„æ¯è‚‰åˆ†å‰²æ¨¡å‹ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€æä¾›ï¼Œä»¥ä¾¿åœ¨è¿™ä¸€é‡è¦çš„åŒ»å­¦æˆåƒé¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„è¯­ä¹‰æ¯è‚‰åˆæˆæ–¹æ³•Polyp-DDPMï¼Œæ—¨åœ¨å¢å¼ºæ¯è‚‰åˆ†å‰²ã€‚Polyp-DDPMé€šè¿‡æ©æ¨¡å›¾åƒçš„é€é€šé“è¿æ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åˆæˆæ¯è‚‰å›¾åƒï¼Œå¯ç”¨äºè®­ç»ƒæ›´å‡†ç¡®çš„æ¯è‚‰åˆ†å‰²æ¨¡å‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æ¥åˆæˆé«˜è´¨é‡çš„åˆæˆæ¯è‚‰å›¾åƒã€‚</li><li>é€šè¿‡æ©æ¨¡å›¾åƒçš„é€é€šé“è¿æ¥å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿ç”Ÿæˆçš„å›¾åƒå…·æœ‰æ›´å¼ºçš„è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>åœ¨Kvasir-SEGæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒPolyp-DDPMåœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨Kvasir-SEGæ•°æ®é›†ä¸Šï¼ŒPolyp-DDPMåœ¨å›¾åƒè´¨é‡å’Œåˆ†å‰²ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>Polyp-DDPMç”Ÿæˆçš„åˆæˆæ¯è‚‰å›¾åƒå…·æœ‰æ›´é«˜çš„è´¨é‡å’Œæ›´å¼ºçš„è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>Polyp-DDPMè®­ç»ƒçš„æ¯è‚‰åˆ†å‰²æ¨¡å‹åœ¨Kvasir-SEGæ•°æ®é›†ä¸Šå–å¾—äº†æ›´é«˜çš„åˆ†å‰²å‡†ç¡®ç‡ã€‚å·¥ä½œé‡ï¼š</li><li>Polyp-DDPMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä¸éœ€è¦é¢å¤–çš„é¢„å¤„ç†æˆ–åå¤„ç†æ­¥éª¤ã€‚</li><li>Polyp-DDPMçš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€æä¾›ï¼Œä»¥ä¾¿åœ¨è¿™ä¸€é‡è¦çš„åŒ»å­¦æˆåƒé¢†åŸŸè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle"></details><h2 id="EscherNet-A-Generative-Model-for-Scalable-View-Synthesis"><a href="#EscherNet-A-Generative-Model-for-Scalable-View-Synthesis" class="headerlink" title="EscherNet: A Generative Model for Scalable View Synthesis"></a>EscherNet: A Generative Model for Scalable View Synthesis</h2><p><strong>Authors:Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</strong></p><p>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis â€” it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{<a href="https://kxhit.github.io/EscherNet}">https://kxhit.github.io/EscherNet}</a>. </p><p><a href="http://arxiv.org/abs/2402.03908v1">PDF</a> Project Page: <a href="https://kxhit.github.io/EscherNet">https://kxhit.github.io/EscherNet</a></p><p><strong>Summary</strong></p><p>åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šè§†è§’è§†å›¾åˆæˆï¼Œå®ç°ä»»æ„æ•°é‡çš„è§†è§’è½¬æ¢ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>EscherNet æ˜¯ä¸€ç§å¤šè§†è§’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§†å›¾åˆæˆã€‚</li><li>EscherNet çš„æœ¬è´¨æ˜¯ï¼Œä»¥å¤šè§†è§’å›¾åƒä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä»»æ„æ•°é‡çš„ç›®æ ‡è§†è§’å›¾åƒã€‚</li><li>EscherNet å¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§ GPU ä¸ŠåŒæ—¶ç”Ÿæˆ 100 å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†è§’ï¼Œåœ¨å‡†ç¡®æ€§ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ•ˆæœã€‚</li><li>EscherNet çš„å¤šåŠŸèƒ½æ€§ä½¿å…¶å¯ä»¥è§£å†³å¤šç§ 3D è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚é›¶æ ·æœ¬æ–°è§†è§’åˆæˆã€å•å›¾åƒ 3D é‡å»ºã€å¤šå›¾åƒ 3D é‡å»ºç­‰ã€‚</li><li>EscherNet çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®ã€åŒ»å­¦æˆåƒã€è‡ªåŠ¨é©¾é©¶ç­‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸƒèˆå°”ç½‘ç»œï¼šä¸€ç§ç”¨äºå¯æ‰©å±•è§†å›¾åˆæˆçš„ç”Ÿæˆæ¨¡å‹</li><li>ä½œè€…ï¼šXin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</li><li>éš¶å±å•ä½ï¼šä¼¦æ•¦å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤</li><li>å…³é”®è¯ï¼šè§†å›¾åˆæˆã€æ‰©æ•£æ¨¡å‹ã€éšå¼ç¥ç»è¡¨ç¤ºã€å¤šè§†å›¾å‡ ä½•</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03908ï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè§†å›¾åˆæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒå…è®¸æ ¹æ®ä¸€ç»„å‚è€ƒè§†ç‚¹å‘ˆç°åœºæ™¯çš„ä»»æ„è§†ç‚¹ï¼Œä»è€Œæ¨¡æ‹Ÿäººç±»çš„è§†è§‰é€‚åº”æ€§ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•ä¸€ä»»åŠ¡ï¼Œä¾‹å¦‚é›¶æ ·æœ¬æ–°é¢–è§†å›¾åˆæˆã€å•å›¾åƒä¸‰ç»´é‡å»ºæˆ–å¤šå›¾åƒä¸‰ç»´é‡å»ºï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶é¢ä¸´ç€æ³›åŒ–æ€§å·®ã€çµæ´»æ€§ä¸è¶³å’Œå¯æ‰©å±•æ€§æœ‰é™ç­‰é—®é¢˜ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹â€”â€”åŸƒèˆå°”ç½‘ç»œï¼Œå®ƒå­¦ä¹ éšå¼å’Œç”Ÿæˆçš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶ç»“åˆä¸“é—¨çš„ç›¸æœºä½ç½®ç¼–ç ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„å‚è€ƒè§†å›¾å’Œç›®æ ‡è§†å›¾ä¹‹é—´çš„ç›¸æœºå˜æ¢è¿›è¡Œç²¾ç¡®å’Œè¿ç»­çš„ç›¸å¯¹æ§åˆ¶ã€‚åŸƒèˆå°”ç½‘ç»œåœ¨è§†å›¾åˆæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§ã€çµæ´»æ€§ï¼Œä»¥åŠå¯æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨å›ºå®šæ•°é‡çš„ 3 ä¸ªå‚è€ƒè§†å›¾åˆ° 3 ä¸ªç›®æ ‡è§†å›¾ä¸Šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§ GPU ä¸ŠåŒæ—¶ç”Ÿæˆ 100 å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†å›¾ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåŸƒèˆå°”ç½‘ç»œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿ä¸é’ˆå¯¹æ¯ä¸ªå•ç‹¬é—®é¢˜é‡èº«å®šåˆ¶çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å“è¶Šçš„å¤šåŠŸèƒ½æ€§ä¸ºè®¾è®¡ç”¨äºä¸‰ç»´è§†è§‰çš„å¯æ‰©å±•ç¥ç»æ¶æ„å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1)ï¼šåŸƒèˆå°”ç½‘ç»œæ˜¯ä¸€ç§å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå­¦ä¹ éšå¼å’Œç”Ÿæˆçš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶ç»“åˆä¸“é—¨çš„ç›¸æœºä½ç½®ç¼–ç ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„å‚è€ƒè§†å›¾å’Œç›®æ ‡è§†å›¾ä¹‹é—´çš„ç›¸æœºå˜æ¢è¿›è¡Œç²¾ç¡®å’Œè¿ç»­çš„ç›¸å¯¹æ§åˆ¶ã€‚(2)ï¼šåŸƒèˆå°”ç½‘ç»œåœ¨è§†å›¾åˆæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§ã€çµæ´»æ€§ï¼Œä»¥åŠå¯æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨å›ºå®šæ•°é‡çš„3ä¸ªå‚è€ƒè§†å›¾åˆ°3ä¸ªç›®æ ‡è§†å›¾ä¸Šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸ŠåŒæ—¶ç”Ÿæˆ100å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†å›¾ã€‚(3)ï¼šåŸƒèˆå°”ç½‘ç»œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿ä¸é’ˆå¯¹æ¯ä¸ªå•ç‹¬é—®é¢˜é‡èº«å®šåˆ¶çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å“è¶Šçš„å¤šåŠŸèƒ½æ€§ä¸ºè®¾è®¡ç”¨äºä¸‰ç»´è§†è§‰çš„å¯æ‰©å±•ç¥ç»æ¶æ„å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šåŸƒèˆå°”ç½‘ç»œæå‡ºäº†ä¸€ç§å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è§†å›¾åˆæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§ã€çµæ´»æ€§ï¼Œä»¥åŠå¯æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨å›ºå®šæ•°é‡çš„3ä¸ªå‚è€ƒè§†å›¾åˆ°3ä¸ªç›®æ ‡è§†å›¾ä¸Šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸ŠåŒæ—¶ç”Ÿæˆ100å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†å›¾ã€‚åŸƒèˆå°”ç½‘ç»œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿ä¸é’ˆå¯¹æ¯ä¸ªå•ç‹¬é—®é¢˜é‡èº«å®šåˆ¶çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å“è¶Šçš„å¤šåŠŸèƒ½æ€§ä¸ºè®¾è®¡ç”¨äºä¸‰ç»´è§†è§‰çš„å¯æ‰©å±•ç¥ç»æ¶æ„å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹â€”â€”åŸƒèˆå°”ç½‘ç»œï¼Œè¯¥æ¨¡å‹å­¦ä¹ éšå¼å’Œç”Ÿæˆçš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶ç»“åˆä¸“é—¨çš„ç›¸æœºä½ç½®ç¼–ç ï¼Œå…è®¸å¯¹ä»»æ„æ•°é‡çš„å‚è€ƒè§†å›¾å’Œç›®æ ‡è§†å›¾ä¹‹é—´çš„ç›¸æœºå˜æ¢è¿›è¡Œç²¾ç¡®å’Œè¿ç»­çš„ç›¸å¯¹æ§åˆ¶ã€‚</li><li>åŸƒèˆå°”ç½‘ç»œåœ¨è§†å›¾åˆæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§ã€çµæ´»æ€§ï¼Œä»¥åŠå¯æ‰©å±•æ€§ï¼Œå³ä½¿åœ¨å›ºå®šæ•°é‡çš„3ä¸ªå‚è€ƒè§†å›¾åˆ°3ä¸ªç›®æ ‡è§†å›¾ä¸Šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸ŠåŒæ—¶ç”Ÿæˆ100å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†å›¾ã€‚</li><li>åŸƒèˆå°”ç½‘ç»œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿ä¸é’ˆå¯¹æ¯ä¸ªå•ç‹¬é—®é¢˜é‡èº«å®šåˆ¶çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å“è¶Šçš„å¤šåŠŸèƒ½æ€§ä¸ºè®¾è®¡ç”¨äºä¸‰ç»´è§†è§‰çš„å¯æ‰©å±•ç¥ç»æ¶æ„å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå³ä½¿ä¸é’ˆå¯¹æ¯ä¸ªå•ç‹¬é—®é¢˜é‡èº«å®šåˆ¶çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li><li>å³ä½¿åœ¨å›ºå®šæ•°é‡çš„3ä¸ªå‚è€ƒè§†å›¾åˆ°3ä¸ªç›®æ ‡è§†å›¾ä¸Šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå¯ä»¥åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸ŠåŒæ—¶ç”Ÿæˆ100å¤šä¸ªä¸€è‡´çš„ç›®æ ‡è§†å›¾ã€‚å·¥ä½œé‡ï¼š</li><li>åŸƒèˆå°”ç½‘ç»œæ˜¯ä¸€ä¸ªå¤æ‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</li><li>åŸƒèˆå°”ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ å¤©æˆ–å‡ å‘¨çš„æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ•°æ®é›†çš„å¤§å°å’Œä½¿ç”¨çš„ç¡¬ä»¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-cdd01255ccb3e0ac7a9532f4537d7c8a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7709d4f2ffb5392bba195cc2b965aeee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86704d39a54eee216395f69db00a0918.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73c26a4c69f3a172a8651cabc4a69ed2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a53c48c54043613c01b125b54da3368.jpg" align="middle"><img src="https://picx.zhimg.com/v2-526811fa3a0b6e1b6d850e3911c0f54f.jpg" align="middle"></details><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</strong></p><p>Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. </p><p><a href="http://arxiv.org/abs/2402.03666v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é‡åŒ–åï¼Œå¦‚ä½•æé«˜å‡†ç¡®ç‡ï¼Ÿ</p><p><strong>Key Takeaways</strong></p><ul><li>ä½ä½é‡åŒ–æ‰©æ•£æ¨¡å‹é¢ä¸´ä¸‰å¤§é—®é¢˜ï¼šæ¿€æ´»åˆ†å¸ƒä¸å¹³è¡¡ã€æ—¶é—´ä¿¡æ¯ä¸ç²¾ç¡®ã€ç‰¹å®šæ¨¡å—å¯¹æ‰°åŠ¨æ•æ„Ÿã€‚</li><li>æå‡ºå¾®è°ƒé‡åŒ–æ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”æ¿€æ´»åˆ†å¸ƒã€‚</li><li>è¯†åˆ«å‡ºä¸¤ç§å…³é”®çš„é‡åŒ–å±‚ï¼šä¿å­˜é‡è¦æ—¶é—´ä¿¡æ¯çš„å±‚å’Œå¯¹æ¯”ç‰¹å®½åº¦é™ä½æ•æ„Ÿçš„å±‚ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥ç¼“è§£æ€§èƒ½é€€åŒ–ã€‚</li><li>ç»éªŒéªŒè¯è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¿®æ”¹äº†æ¿€æ´»åˆ†å¸ƒå¹¶æä¾›äº†æœ‰æ„ä¹‰çš„æ—¶é—´ä¿¡æ¯ï¼Œä¿ƒè¿›äº†æ›´å®¹æ˜“ã€æ›´å‡†ç¡®çš„é‡åŒ–ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å„ç§æ¯”ç‰¹å®½åº¦è®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªåœ¨å®Œå…¨4ä½ï¼ˆå³ W4A4ï¼‰Stable Diffusion ä¸Šç”Ÿæˆå¯è¯»å›¾åƒçš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šQuESTï¼šä½æ¯”ç‰¹æ‰©æ•£æ¨¡å‹é‡åŒ–é€šè¿‡é«˜æ•ˆé€‰æ‹©æ€§å¾®è°ƒ</li><li>ä½œè€…ï¼šHaoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¼Šåˆ©è¯ºä¼Šç†å·¥å­¦é™¢è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€é‡åŒ–ã€ä½æ¯”ç‰¹ã€å¾®è°ƒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03666Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶å®é™…éƒ¨ç½²å—åˆ°é«˜å†…å­˜å’Œæ—¶é—´æ¶ˆè€—çš„é™åˆ¶ã€‚é‡åŒ–å¯ä»¥ä¸ºæ‰©æ•£æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿæä¾›ä¸€ç§å¯è¡Œçš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹è¢«é‡åŒ–ä¸ºä½æ¯”ç‰¹æ—¶å®Œå…¨å¤±è´¥ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•è¦ä¹ˆé›†ä¸­åœ¨æ—¶é—´æ­¥é•¿æ„ŸçŸ¥æ ¡å‡†æ•°æ®æ„é€ ï¼Œè¦ä¹ˆé›†ä¸­åœ¨é‡åŒ–å™ªå£°æ ¡æ­£ï¼Œç›®æ ‡æ˜¯å°†ç°æœ‰çš„é‡åŒ–æŠ€æœ¯è°ƒæ•´åˆ°æ‰©æ•£æ¨¡å‹çš„ç‰¹æ€§ï¼Œè€Œè¿™äº›ç‰¹æ€§ä¸å…¶ä»–æ¨¡å‹ç±»å‹ï¼ˆå¦‚ CNN å’Œ ViTï¼‰ä¸åŒã€‚è¿™äº›æ–¹æ³•å¿½ç•¥äº†ä¸é‡åŒ–ç›¸å…³çš„æ‰©æ•£æ¨¡å‹å†…åœ¨æœºåˆ¶ï¼Œå¯¼è‡´æ–¹æ³•éƒ¨ç½²ä¸æ¨¡å‹ç‰¹å¾ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æ­ç¤ºäº†é‡åŒ–æ‰©æ•£æ¨¡å‹çš„ä¸‰ä¸ªå±æ€§ï¼Œè¿™äº›å±æ€§é˜»ç¢äº†æœ‰æ•ˆçš„é‡åŒ–ï¼šï¼ˆ1ï¼‰æ¿€æ´»åˆ†å¸ƒå¯èƒ½ä¸å¹³è¡¡ï¼Œå…¶ä¸­å¤§å¤šæ•°å€¼æ¥è¿‘ 0ï¼Œä½†å…¶ä»–å€¼å¾ˆå¤§ä¸”ä¸ä¸€è‡´åœ°å‡ºç°ï¼›ï¼ˆ2ï¼‰æ—¶é—´ä¿¡æ¯ä¸ç²¾ç¡®ï¼›ï¼ˆ3ï¼‰å®¹æ˜“å—åˆ°ç‰¹å®šæ¨¡å—çš„æ‰°åŠ¨ã€‚ä¸ºäº†å‡è½»æºäºåˆ†å¸ƒä¸å¹³è¡¡çš„ä½æ¯”ç‰¹é‡åŒ–éš¾åº¦ï¼Œæœ¬æ–‡æå‡ºå¾®è°ƒé‡åŒ–æ¨¡å‹ä»¥æ›´å¥½åœ°é€‚åº”æ¿€æ´»åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡ç¡®å®šäº†ä¸¤ç§å…³é”®ç±»å‹çš„é‡åŒ–å±‚ï¼šé‚£äº›æŒæœ‰é‡è¦æ—¶é—´ä¿¡æ¯å’Œé‚£äº›å¯¹é™ä½æ¯”ç‰¹å®½åº¦æ•æ„Ÿçš„å±‚ï¼Œå¹¶å¾®è°ƒå®ƒä»¬ä»¥æœ‰æ•ˆåœ°å‡è½»æ€§èƒ½ä¸‹é™ã€‚(4) å®éªŒç»“æœï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨å„ç§æ¯”ç‰¹å®½åº¦è®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…¨ 4 ä½ï¼ˆå³ W4A4ï¼‰Stable Diffusion ä¸Šç”Ÿæˆå¯è¯»å›¾åƒçš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å±æ€§ä¸€ï¼šæ¿€æ´»åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå¤§å¤šæ•°å€¼æ¥è¿‘ 0ï¼Œä½†å…¶ä»–å€¼å¾ˆå¤§ä¸”ä¸ä¸€è‡´åœ°å‡ºç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå¾®è°ƒé‡åŒ–æ¨¡å‹ä»¥æ›´å¥½åœ°é€‚åº”æ¿€æ´»åˆ†å¸ƒã€‚ï¼ˆ2ï¼‰å±æ€§äºŒï¼šæ—¶é—´ä¿¡æ¯ä¸å‡†ç¡®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ç§å…³é”®ç±»å‹çš„é‡åŒ–å±‚ï¼šé‚£äº›æŒæœ‰é‡è¦æ—¶é—´ä¿¡æ¯å’Œé‚£äº›å¯¹é™ä½æ¯”ç‰¹å®½åº¦æ•æ„Ÿçš„å±‚ï¼Œå¹¶å¾®è°ƒå®ƒä»¬ä»¥æœ‰æ•ˆåœ°å‡è½»æ€§èƒ½ä¸‹é™ã€‚ï¼ˆ3ï¼‰å±æ€§ä¸‰ï¼šä¸åŒæ¿€æ´»å¯¹é™ä½æ¯”ç‰¹å®½åº¦çš„æ•æ„Ÿæ€§ä¸åŒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥æ¿€æ´»é‡åŒ–å™¨ï¼Œä»¥è¿›ä¸€æ­¥è§£å†³ç”±äºé‡åŒ–è€Œå¯¼è‡´çš„æ—¶é—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚ï¼ˆ4ï¼‰QuESTï¼šä¸€ç§é€šè¿‡é«˜æ•ˆé€‰æ‹©æ€§å¾®è°ƒå®ç°ä½æ¯”ç‰¹æ‰©æ•£æ¨¡å‹é‡åŒ–çš„æ¡†æ¶ã€‚QuEST æ˜¯ä¸€ä¸ªåŸºäºè’¸é¦çš„å¾®è°ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§æƒé‡ä¼˜åŒ–å’Œç½‘ç»œçº§ç¼©æ”¾å› å­ä¼˜åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº† QuESTï¼Œä¸€ç§ç”¨äºä½æ¯”ç‰¹æ‰©æ•£æ¨¡å‹é‡åŒ–çš„æœ‰æ•ˆæ— æ•°æ®å¾®è°ƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„åŠ¨æœºæ¥è‡ªäºåœ¨é‡åŒ–æ‰©æ•£æ¨¡å‹ä¸­å‘ç°çš„ä¸‰ä¸ªåŸºæœ¬å±æ€§ã€‚æˆ‘ä»¬è¿˜ä»ç†è®ºä¸Šè¯æ˜äº†å¾®è°ƒçš„å……åˆ†æ€§ï¼Œå°†å…¶è§£é‡Šä¸ºå¢å¼ºæ¨¡å‹å¯¹å¤§æ¿€æ´»æ‰°åŠ¨çš„é²æ£’æ€§çš„ä¸€ç§æ–¹æ³•ã€‚ä¸ºäº†å‡è½»æ€§èƒ½ä¸‹é™ï¼Œæˆ‘ä»¬æå‡ºåœ¨å…¨ç²¾åº¦å¯¹åº”æ¨¡å‹çš„ç›‘ç£ä¸‹å¾®è°ƒæ—¶é—´åµŒå…¥å±‚å’Œæ³¨æ„åŠ›ç›¸å…³å±‚ã€‚è¿˜å¼•å…¥äº†ä¸€ä¸ªæ—¶é—´æ„ŸçŸ¥æ¿€æ´»é‡åŒ–å™¨æ¥å¤„ç†ä¸åŒçš„æ—¶é—´æ­¥é•¿ã€‚åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¯æ˜äº† QuEST çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œåœ¨æ›´å°‘çš„æ—¶é—´å’Œå†…å­˜æˆæœ¬ä¸‹å®ç°äº†ä½æ¯”ç‰¹å…¼å®¹æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æ­ç¤ºäº†é‡åŒ–æ‰©æ•£æ¨¡å‹çš„ä¸‰ä¸ªå±æ€§ï¼Œè¿™äº›å±æ€§é˜»ç¢äº†æœ‰æ•ˆçš„é‡åŒ–ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„å¾®è°ƒç­–ç•¥ QuESTï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§æƒé‡ä¼˜åŒ–å’Œç½‘ç»œçº§ç¼©æ”¾å› å­ä¼˜åŒ–ã€‚</li><li>æå‡ºäº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥æ¿€æ´»é‡åŒ–å™¨ï¼Œä»¥è¿›ä¸€æ­¥è§£å†³ç”±äºé‡åŒ–è€Œå¯¼è‡´çš„æ—¶é—´ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸‰ä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…¨ 4 ä½ï¼ˆå³ W4A4ï¼‰Stable Diffusion ä¸Šç”Ÿæˆå¯è¯»å›¾åƒçš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>åœ¨ ImageNet-64x64 æ•°æ®é›†ä¸Šï¼ŒQuEST åªéœ€ 10 ä¸ª GPU å¤©å³å¯å°† Stable Diffusion é‡åŒ–ä¸º 4 ä½ã€‚</li><li>åœ¨ ImageNet-256x256 æ•°æ®é›†ä¸Šï¼ŒQuEST åªéœ€ 40 ä¸ª GPU å¤©å³å¯å°† Stable Diffusion é‡åŒ–ä¸º 4 ä½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fb8f38fcd6a6857ddffdf84e6eded575.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b8c7e8e687f519bd6aea6d7aa431f440.jpg" align="middle"><img src="https://picx.zhimg.com/v2-808d7c694b655862c89add4bffc7e8b1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e6556a45821b662485e3c321d4542f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d15f14313fc02ce9abba40125462e990.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9f9c978c41171320dbafc60fb23b8e.jpg" align="middle"></details><h2 id="InstanceDiffusion-Instance-level-Control-for-Image-Generation"><a href="#InstanceDiffusion-Instance-level-Control-for-Image-Generation" class="headerlink" title="InstanceDiffusion: Instance-level Control for Image Generation"></a>InstanceDiffusion: Instance-level Control for Image Generation</h2><p><strong>Authors:Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</strong></p><p>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs. </p><p><a href="http://arxiv.org/abs/2402.03290v1">PDF</a> Preprint; Project page:   <a href="https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/">https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å®ç°äº†é«˜è´¨é‡å›¾åƒç”Ÿæˆï¼Œä½†æ— æ³•æ§åˆ¶å›¾åƒä¸­çš„å•ç‹¬å®ä¾‹ã€‚æˆ‘ä»¬å¼•å…¥äº†InstanceDiffusionï¼Œå®ƒä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ·»åŠ äº†ç²¾ç¡®çš„å®ä¾‹çº§æ§åˆ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>InstanceDiffusionæ”¯æŒæ¯ä¸ªå®ä¾‹çš„è‡ªç”±å½¢å¼è¯­è¨€æ¡ä»¶ã€‚</li><li>InstanceDiffusionæ”¯æŒçµæ´»æ–¹å¼æŒ‡å®šå®ä¾‹ä½ç½®ï¼Œå¦‚ç®€å•å•ç‚¹ã€æ¶‚é¸¦ã€è¾¹ç•Œæ¡†æˆ–å¤æ‚çš„å®ä¾‹åˆ†å‰²æ©ç åŠå…¶ç»„åˆã€‚</li><li>InstanceDiffusionæå‡ºäº†ä¸‰ä¸ªä¸»è¦æ›´æ”¹ï¼Œä»¥å®ç°ç²¾ç¡®çš„å®ä¾‹çº§æ§åˆ¶ã€‚</li><li>UniFusionæ¨¡å—ä¸ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¯ç”¨äº†å®ä¾‹çº§æ¡ä»¶ã€‚</li><li>ScaleUæ¨¡å—æé«˜äº†å›¾åƒä¿çœŸåº¦ã€‚</li><li>Multi-instance Sampleræ”¹è¿›äº†å¤šä¸ªå®ä¾‹çš„ç”Ÿæˆã€‚</li><li>InstanceDiffusionåœ¨æ¯ä¸ªä½ç½®æ¡ä»¶ä¸‹éƒ½æ˜¾ç€è¶…è¿‡äº†ä¸“é—¨çš„æœ€æ–°æ¨¡å‹ã€‚</li><li>åœ¨COCOæ•°æ®é›†ä¸Šï¼ŒInstanceDiffusionåœ¨æ¡†è¾“å…¥æ—¶ä¼˜äºä¹‹å‰çš„æœ€æ–°æŠ€æœ¯20.4% AP50boxï¼Œåœ¨æ©ç è¾“å…¥æ—¶ä¼˜äºä¹‹å‰çš„æœ€æ–°æŠ€æœ¯25.4% IoUã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå®ä¾‹æ‰©æ•£ï¼šå›¾åƒç”Ÿæˆçš„å®ä¾‹çº§æ§åˆ¶</li><li>ä½œè€…ï¼šJun-Yan Zhu, Taesung Park, Abhishek Sharma, Prafulla Dhariwal, Alexei A. Efros, Pieter Abbeel</li><li>éš¶å±æœºæ„ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å®ä¾‹çº§æ§åˆ¶ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2212.04915ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†æ— æ³•å¯¹å›¾åƒä¸­çš„å„ä¸ªå®ä¾‹è¿›è¡Œæ§åˆ¶ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œæ§åˆ¶ï¼Œè€Œæ— æ³•å¯¹å„ä¸ªå®ä¾‹è¿›è¡Œç²¾ç»†çš„æ§åˆ¶ã€‚è¿™äº›æ–¹æ³•çš„é—®é¢˜åœ¨äºï¼Œå®ƒä»¬æ— æ³•å¤„ç†å¤æ‚çš„å®ä¾‹æ¡ä»¶ï¼Œä¾‹å¦‚ï¼Œå½“å®ä¾‹é‡å æˆ–è¢«é®æŒ¡æ—¶ï¼Œå®ƒä»¬æ— æ³•ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚(3) è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ InstanceDiffusionï¼Œè¯¥æ¨¡å‹å¯ä»¥å¯¹å›¾åƒä¸­çš„å„ä¸ªå®ä¾‹è¿›è¡Œç²¾ç»†çš„æ§åˆ¶ã€‚InstanceDiffusion ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šUniFusion æ¨¡å—ã€ScaleU æ¨¡å—å’Œ Multi-instance Samplerã€‚UniFusion æ¨¡å—å¯ä»¥å°†å®ä¾‹æ¡ä»¶èåˆåˆ°æ–‡æœ¬åµŒå…¥ä¸­ï¼ŒScaleU æ¨¡å—å¯ä»¥æé«˜å›¾åƒçš„ä¿çœŸåº¦ï¼ŒMulti-instance Sampler å¯ä»¥æ”¹å–„å¤šå®ä¾‹ç”Ÿæˆçš„è´¨é‡ã€‚(4) å®éªŒç»“æœï¼šInstanceDiffusion åœ¨ COCO æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ APbox50 æŒ‡æ ‡ä¸Šï¼ŒInstanceDiffusion æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹é«˜å‡º 20.4%ï¼Œåœ¨ IoU æŒ‡æ ‡ä¸Šï¼ŒInstanceDiffusion æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹é«˜å‡º 25.4%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒInstanceDiffusion èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹å›¾åƒä¸­çš„å„ä¸ªå®ä¾‹è¿›è¡Œæ§åˆ¶ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚</p></li><li><p>Methodsï¼š(1) UniFusionï¼šUniFusionæ˜¯InstanceDiffusionæ¨¡å‹ä¸­çš„ä¸€ä¸ªå…³é”®æ¨¡å—ï¼Œå®ƒå¯ä»¥å°†æ¨¡ç³Šçš„è¯­ä¹‰ä¿¡æ¯èåˆåˆ°å›¾åƒåµŒå…¥ä¸­ã€‚UniFusionç”±ä¸¤ä¸ªå­æ¨¡å—ç»„æˆï¼šè¯­ä¹‰ä¿¡æ¯æå–æ¨¡å—å’Œä¿¡æ¯èåˆæ¨¡å—ã€‚è¯­ä¹‰ä¿¡æ¯æå–æ¨¡å—è´Ÿè´£ä»è¯­ä¹‰ä¿¡æ¯ä¸­æå–ç‰¹å¾ï¼Œä¿¡æ¯èåˆæ¨¡å—è´Ÿè´£å°†è¿™äº›ç‰¹å¾èåˆåˆ°å›¾åƒåµŒå…¥ä¸­ã€‚(2) ScaleUï¼šScaleUæ˜¯InstanceDiffusionæ¨¡å‹ä¸­çš„å¦ä¸€ä¸ªå…³é”®æ¨¡å—ï¼Œå®ƒå¯ä»¥æé«˜å›¾åƒçš„ä¿çœŸåº¦ã€‚ScaleUç”±ä¸¤ä¸ªå­æ¨¡å—ç»„æˆï¼šä¸Šé‡‡æ ·æ¨¡å—å’Œæ®‹å·®æ¨¡å—ã€‚ä¸Šé‡‡æ ·æ¨¡å—è´Ÿè´£å°†å›¾åƒä»ä½åˆ†è¾¨ç‡ä¸Šé‡‡æ ·åˆ°é«˜åˆ†è¾¨ç‡ï¼Œæ®‹å·®æ¨¡å—è´Ÿè´£æ·»åŠ æ®‹å·®è¿æ¥ï¼Œä»¥æé«˜å›¾åƒçš„ä¿çœŸåº¦ã€‚(3) Multi-instanceSamplerï¼šMulti-instanceSampleræ˜¯InstanceDiffusionæ¨¡å‹ä¸­çš„ä¸€ä¸ªé‡‡æ ·æ¨¡å—ï¼Œå®ƒå¯ä»¥æ”¹å–„å¤šå®ä¾‹ç”Ÿæˆçš„è´¨é‡ã€‚Multi-instanceSampleré€šè¿‡å¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œå¤šæ¬¡é‡‡æ ·ï¼Œç„¶åå°†è¿™äº›é‡‡æ ·ç»“æœè¿›è¡Œèåˆï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šInstanceDiffusion æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ COCO æ•°æ®é›†ä¸Šçš„ APbox50 æŒ‡æ ‡å’Œ IoU æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚è¿™è¡¨æ˜ InstanceDiffusion æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹å›¾åƒä¸­çš„å„ä¸ªå®ä¾‹è¿›è¡Œæ§åˆ¶ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ InstanceDiffusionï¼Œè¯¥æ¨¡å‹å¯ä»¥å¯¹å›¾åƒä¸­çš„å„ä¸ªå®ä¾‹è¿›è¡Œç²¾ç»†çš„æ§åˆ¶ã€‚</li><li>è®¾è®¡äº† UniFusion æ¨¡å—ï¼Œå¯ä»¥å°†æ¨¡ç³Šçš„è¯­ä¹‰ä¿¡æ¯èåˆåˆ°å›¾åƒåµŒå…¥ä¸­ã€‚</li><li>è®¾è®¡äº† ScaleU æ¨¡å—ï¼Œå¯ä»¥æé«˜å›¾åƒçš„ä¿çœŸåº¦ã€‚</li><li>è®¾è®¡äº† Multi-instanceSampler æ¨¡å—ï¼Œå¯ä»¥æ”¹å–„å¤šå®ä¾‹ç”Ÿæˆçš„è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ COCO æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ APbox50 æŒ‡æ ‡ä¸Šæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹é«˜å‡º 20.4%ï¼Œåœ¨ IoU æŒ‡æ ‡ä¸Šæ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ¨¡å‹é«˜å‡º 25.4%ã€‚å·¥ä½œé‡ï¼š</li><li>æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-ad80374506fc08e660bb8742f25dc5ea.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eb1dc22d5f1b16516125f58ffce2ab07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cc40befe0322c7f0f22fe9b42e02d05a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0667dfce57b3d47e56cf440eb22a837d.jpg" align="middle"></details><h2 id="Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images"><a href="#Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images" class="headerlink" title="Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?"></a>Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?</h2><p><strong>Authors:Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</strong></p><p>The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against todayâ€™s modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness. </p><p><a href="http://arxiv.org/abs/2402.03214v2">PDF</a> </p><p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½å›¾åƒç”ŸæˆæŠ€æœ¯å¼•å‘è‰ºæœ¯é¢†åŸŸå·¨å˜ï¼ŒåŒºåˆ†äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒä¸äººç±»è‰ºæœ¯å“æ˜¯ä¸€é¡¹ä¸æ–­åŠ å‰§çš„éš¾é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>AIç”Ÿæˆå›¾åƒå¯¹è‰ºæœ¯ä¸–ç•Œçš„é¢ è¦†æ€§å½±å“ä¸æ—¥ä¿±å¢ã€‚</li><li>é‰´åˆ«AIç”Ÿæˆçš„å›¾åƒå¯¹äºé˜²æ­¢æ¬ºè¯ˆã€ç‰ˆæƒä¿æŠ¤å’Œæ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦ã€‚</li><li>ç›®å‰æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åŒºåˆ†äººç±»è‰ºæœ¯ä¸AIå›¾åƒï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ è®­ç»ƒçš„åˆ†ç±»å™¨ã€é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶å·¥å…·ä»¥åŠä¸“ä¸šè‰ºæœ¯å®¶åˆ©ç”¨å…¶å¯¹è‰ºæœ¯æŠ€å·§çš„äº†è§£è¿›è¡Œè¯†åˆ«ã€‚</li><li>ç ”ç©¶è¡¨æ˜ï¼ŒHiveå’Œä¸“å®¶è‰ºæœ¯å®¶åœ¨åŒºåˆ†AIç”Ÿæˆçš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å„æœ‰ä¼˜åŠ£ï¼ˆHiveå¯¹å¯¹æŠ—æ€§æ‰°åŠ¨è¾ƒå¼±ï¼Œè€Œä¸“å®¶è‰ºæœ¯å®¶äº§ç”Ÿè¾ƒé«˜è¯¯æŠ¥ç‡ï¼‰ã€‚</li><li>éšç€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¿™äº›å¼±ç‚¹å¯èƒ½ä»ç„¶å­˜åœ¨ï¼Œç ”ç©¶æ•°æ®è¡¨æ˜ï¼Œç”±äººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨ç»„æˆçš„ç»„åˆå›¢é˜Ÿå¯ä»¥æä¾›æœ€ä½³çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li><li>äººå·¥ç”Ÿæˆçš„å›¾åƒåœ¨è‰ºæœ¯é¢†åŸŸå¼•å‘äº†ä¸€åœºé¢ è¦†ï¼Œå‡†ç¡®åŒºåˆ†äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒå¯¹äºé˜²æ­¢æ¬ºè¯ˆå’Œä¿æŠ¤ç‰ˆæƒè‡³å…³é‡è¦ã€‚</li><li>å°½ç®¡æœ‰ä¸åŒçš„æ–¹æ³•å¯ä»¥åŒºåˆ†äººç±»è‰ºæœ¯ä¸AIå›¾åƒï¼Œä½†æ²¡æœ‰ä¸€ç§æ–¹æ³•æ˜¯å®Œç¾çš„ã€‚</li><li>å°†äººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨ç»“åˆèµ·æ¥å¯ä»¥æä¾›æœ€ä½³çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæœ‰æœºè¿˜æ˜¯æ‰©æ•£ï¼šæˆ‘ä»¬èƒ½åŒºåˆ†äººç±»è‰ºæœ¯å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒå—ï¼Ÿ</li><li>ä½œè€…ï¼šAnna Yoo Jeong Haã€Josephine Passanantiã€Ronik Bhaskarã€Shawn Shanã€Reid Southen1ã€Haitao Zhengã€Ben Y. Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šèŠåŠ å“¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šäººå·¥æ™ºèƒ½è‰ºæœ¯ã€å›¾åƒç”Ÿæˆã€é‰´åˆ«å™¨ã€äººç±»è‰ºæœ¯å®¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03214ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šéšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒçš„å‡ºç°ï¼Œè‰ºæœ¯é¢†åŸŸå‘ç”Ÿäº†å·¨å¤§å˜é©ã€‚åŒºåˆ†äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒå’Œäººç±»è‰ºæœ¯æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå…¶å½±å“éšç€æ—¶é—´çš„æ¨ç§»è€Œä¸æ–­æ‰©å¤§ã€‚å¦‚æœä¸è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±ä¼šè®©ä¸æ³•åˆ†å­æ¬ºéª—é‚£äº›ä¸ºäººç±»è‰ºæœ¯æ”¯ä»˜é«˜ä»·çš„ä¸ªäººå’Œç¦æ­¢ä½¿ç”¨äººå·¥æ™ºèƒ½å›¾åƒçš„å…¬å¸ã€‚è¿™å¯¹å†…å®¹æ‰€æœ‰è€…å»ºç«‹ç‰ˆæƒå’Œå¯¹æ¨¡å‹è®­ç»ƒè€…æ¥è¯´ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ï¼Œä»–ä»¬éœ€è¦å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ•´ç†ä»¥é¿å…æ½œåœ¨çš„æ¨¡å‹å´©æºƒã€‚(2)ï¼šç›®å‰ï¼Œæœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•å¯ä»¥åŒºåˆ†äººç±»è‰ºæœ¯å’Œäººå·¥æ™ºèƒ½å›¾åƒï¼ŒåŒ…æ‹¬é€šè¿‡ç›‘ç£å­¦ä¹ è®­ç»ƒçš„åˆ†ç±»å™¨ã€é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶å·¥å…·ä»¥åŠä¸“ä¸šè‰ºæœ¯å®¶åˆ©ç”¨å…¶å¯¹è‰ºæœ¯æŠ€å·§çš„çŸ¥è¯†è¿›è¡Œè¯†åˆ«ã€‚(3)ï¼šåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯»æ±‚äº†è§£è¿™äº›æ–¹æ³•åœ¨é¢å¯¹å½“ä»Šç°ä»£ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œåœ¨è‰¯æ€§å’Œå¯¹æŠ—æ€§ç¯å¢ƒä¸­çš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬æ•´ç†äº†è·¨è¶Š 7 ç§é£æ ¼çš„çœŸå®äººç±»è‰ºæœ¯ï¼Œä» 5 ä¸ªç”Ÿæˆæ¨¡å‹ä¸­ç”Ÿæˆäº†åŒ¹é…çš„å›¾åƒï¼Œå¹¶åº”ç”¨äº† 8 ä¸ªæ£€æµ‹å™¨ï¼ˆ5 ä¸ªè‡ªåŠ¨æ£€æµ‹å™¨å’Œ 3 ä¸ªä¸åŒçš„äººç±»ç¾¤ä½“ï¼ŒåŒ…æ‹¬ 180 åä¼—åŒ…å·¥äººã€4000 å¤šåä¸“ä¸šè‰ºæœ¯å®¶å’Œ 13 ååœ¨æ£€æµ‹äººå·¥æ™ºèƒ½æ–¹é¢ç»éªŒä¸°å¯Œçš„ä¸“å®¶è‰ºæœ¯å®¶ï¼‰ã€‚(4)ï¼šHive å’Œä¸“å®¶è‰ºæœ¯å®¶éƒ½è¡¨ç°å¾—éå¸¸å¥½ï¼Œä½†åœ¨ä¸åŒçš„æ–¹é¢çŠ¯äº†é”™è¯¯ï¼ˆHive åœ¨å¯¹æŠ—æ€§æ‰°åŠ¨ä¸­è¾ƒå¼±ï¼Œè€Œä¸“å®¶è‰ºæœ¯å®¶äº§ç”Ÿè¾ƒé«˜çš„è¯¯æŠ¥ï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºéšç€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¿™äº›å¼±ç‚¹å°†ç»§ç»­å­˜åœ¨ï¼Œå¹¶åˆ©ç”¨æˆ‘ä»¬çš„æ•°æ®è¯æ˜ä¸ºä»€ä¹ˆäººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿæä¾›äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„æœ€ä½³ç»„åˆã€‚</li></ol><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰æ„å»ºæ•°æ®é›†ï¼š- æ”¶é›†çœŸäººè‰ºæœ¯ä½œå“ã€AI ç”Ÿæˆçš„å›¾åƒã€æ‰°åŠ¨ç‰ˆæœ¬çš„äººç±»è‰ºæœ¯ä½œå“å’Œ AI å›¾åƒä»¥åŠç»“åˆäººç±»å’Œ AI åŠªåŠ›åˆ›å»ºçš„éå…¸å‹å›¾åƒã€‚- ä» 53 ä½è‰ºæœ¯å®¶å¤„æ”¶é›† 280 å¹…çœŸäººè‰ºæœ¯ä½œå“ï¼Œæ¶µç›– 7 ç§ä¸»è¦è‰ºæœ¯é£æ ¼ã€‚- ä¸º 7 ç§è‰ºæœ¯é£æ ¼ä¸­çš„æ¯ä¸€ç§ï¼Œä½¿ç”¨ 5 ä¸ªæµè¡Œçš„ AI ç”Ÿæˆå™¨ç”Ÿæˆ 10 å¼ å›¾åƒï¼Œå…±ç”Ÿæˆ 350 å¼  AI ç”Ÿæˆçš„å›¾åƒã€‚- è°ƒæ•´ BLIP ç”Ÿæˆçš„æ ‡é¢˜ä»¥åŒ…æ‹¬è‰ºæœ¯ä½œå“çš„é£æ ¼ï¼Œå¹¶æ ¹æ®æ¯ä¸ª AI ç”Ÿæˆå™¨çš„ç‹¬ç‰¹é™åˆ¶å’Œé…ç½®å¯¹æ ‡é¢˜è¿›è¡Œè‡ªå®šä¹‰è°ƒæ•´ã€‚</p><p>ï¼ˆ2ï¼‰è¯„ä¼°è‡ªåŠ¨æ£€æµ‹å™¨ï¼š- è€ƒè™‘å·²éƒ¨ç½²çš„å•†ä¸šç³»ç»Ÿå’ŒåŸºäºç ”ç©¶çš„ç³»ç»Ÿã€‚- è¯„ä¼°è‡ªåŠ¨æ£€æµ‹å™¨åœ¨æ ¸å¿ƒæµ‹è¯•æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†åŒ…å« 280 å¹…çœŸäººè‰ºæœ¯ä½œå“ã€350 å¹… AI å›¾åƒå’Œ 40 å¹…æ··åˆå›¾åƒã€‚- æµ‹è¯•è‡ªåŠ¨æ£€æµ‹å™¨é’ˆå¯¹å„ç§å¯¹æŠ—æ€§æ‰°åŠ¨ï¼ŒåŒ…æ‹¬é«˜æ–¯å™ªå£°ã€JPEG å‹ç¼©ã€å¯¹æŠ—æ€§æ‰°åŠ¨å’Œ Glaze é£æ ¼æ¨¡æ‹Ÿä¿æŠ¤å·¥å…·ã€‚</p><p>ï¼ˆ3ï¼‰è¯„ä¼°äººç±»æ£€æµ‹ï¼šç”¨æˆ·ç ”ç©¶ï¼š- è¿›è¡Œå•ç‹¬çš„ç”¨æˆ·ç ”ç©¶ï¼Œé’ˆå¯¹ 3 ä¸ªç‹¬ç«‹çš„ç”¨æˆ·ç¾¤ä½“ï¼šåŸºæœ¬å‚ä¸è€…ã€ä¸“ä¸šè‰ºæœ¯å®¶å¿—æ„¿è€…å’Œä¸“å®¶å‚ä¸è€…ã€‚- åŸºæœ¬å‚ä¸è€…ï¼šé€šè¿‡ Prolific åœ¨çº¿ä¼—åŒ…å¹³å°æ‹›å‹Ÿ 180 åå‚ä¸è€…ï¼Œå®Œæˆä¸€è‡´æ€§æ£€æŸ¥åæœ‰ 177 äººå‚ä¸ã€‚- ä¸“ä¸šè‰ºæœ¯å®¶å¿—æ„¿è€…ï¼šé€šè¿‡ç¤¾äº¤åª’ä½“æ‹›å‹Ÿè¶…è¿‡ 4000 åä¸“ä¸šè‰ºæœ¯å®¶å¿—æ„¿è€…ï¼Œ3803 äººå®Œæˆè°ƒæŸ¥å¹¶é€šè¿‡æ‰€æœ‰ä¸€è‡´æ€§æ£€æŸ¥ã€‚- ä¸“å®¶å‚ä¸è€…ï¼šæ‹›å‹Ÿ 13 ä½çŸ¥åä¸“ä¸šè‰ºæœ¯å®¶ï¼Œä»–ä»¬å…·æœ‰è¯†åˆ« AI å›¾åƒçš„ç»éªŒã€‚- ä¸“å®¶å›¢é˜Ÿæä¾›å¯¹äº§ç”Ÿæœ€å¤šé”™è¯¯åˆ†ç±»çš„æœ€å›°éš¾å›¾åƒçš„è¯¦ç»†åé¦ˆã€‚</p><p>ï¼ˆ4ï¼‰æ•°æ®æ”¶é›†ï¼š- ç­–åˆ’åŒ…å«çœŸäººåˆ›ä½œçš„è‰ºæœ¯ä½œå“ã€AI ç”Ÿæˆçš„å›¾åƒå’Œæ··åˆå›¾åƒçš„æ•°æ®é›†ã€‚- å®šä¹‰çœŸäººå›¾åƒä¸ºç”±äººç±»è‰ºæœ¯å®¶åŸåˆ›çš„è‰ºæœ¯ä½œå“ã€‚- AI ç”Ÿæˆçš„å›¾åƒä½¿ç”¨ AI æ¨¡å‹ï¼ˆå¦‚ Midjourneyã€Stable Diffusion å’Œ DALL-E3ï¼‰ä»æ–‡æœ¬æç¤ºç”Ÿæˆã€‚- æ··åˆå›¾åƒç”± AI ç”Ÿæˆã€æ¶¦è‰²å¹¶éƒ¨åˆ†ç”±äººç±»ç»˜åˆ¶ã€‚- ä»ç¤¾äº¤åª’ä½“ç½‘ç«™å’Œè‰ºæœ¯å¹³å°æ”¶é›†çœŸäººè‰ºæœ¯ä½œå“ã€‚- ä¸è‰ºæœ¯å®¶ç¤¾åŒºåˆä½œï¼Œæ”¶é›†è·¨è¶Š 7 ç§ä¸»è¦è‰ºæœ¯é£æ ¼çš„è‰ºæœ¯ä½œå“ã€‚- ä½¿ç”¨ BLIP æ¨¡å‹ä¸º AI ç”Ÿæˆå™¨åˆ›å»ºæç¤ºï¼Œä»¥ç”Ÿæˆæœ‰æ•ˆæ•æ‰è‰ºæœ¯ä½œå“é£æ ¼å’Œå†…å®¹çš„æ ‡é¢˜ã€‚- æ ¹æ®æ¯ä¸ª AI ç”Ÿæˆå™¨çš„ç‹¬ç‰¹é™åˆ¶å’Œé…ç½®ï¼Œå¯¹ BLIP ç”Ÿæˆçš„æ ‡é¢˜è¿›è¡Œè‡ªå®šä¹‰è°ƒæ•´ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒçš„å‡ºç°ï¼ŒåŒºåˆ†äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒå’Œäººç±»è‰ºæœ¯æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶äº†ç›®å‰å‡ ç§ä¸åŒçš„æ–¹æ³•åœ¨é¢å¯¹å½“ä»Šç°ä»£ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œåœ¨è‰¯æ€§å’Œå¯¹æŠ—æ€§ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œå¹¶è¯æ˜äº†äººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿæä¾›äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„æœ€ä½³ç»„åˆã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æ„å»ºäº†ä¸€ä¸ªè·¨è¶Š7ç§é£æ ¼çš„çœŸå®äººç±»è‰ºæœ¯ã€äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒå’Œæ··åˆå›¾åƒçš„æ•°æ®é›†ã€‚</li><li>è¯„ä¼°äº†8ä¸ªæ£€æµ‹å™¨ï¼ˆ5ä¸ªè‡ªåŠ¨æ£€æµ‹å™¨å’Œ3ä¸ªä¸åŒçš„äººç±»ç¾¤ä½“ï¼‰åœ¨æ ¸å¿ƒæµ‹è¯•æ•°æ®é›†å’Œå„ç§å¯¹æŠ—æ€§æ‰°åŠ¨ä¸Šçš„æ€§èƒ½ã€‚</li><li>å‘ç°äººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿæä¾›äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„æœ€ä½³ç»„åˆã€‚</li><li>ä¸“å®¶è‰ºæœ¯å®¶åœ¨å¯¹æŠ—æ€§æ‰°åŠ¨ä¸­è¡¨ç°è¾ƒå¼±ï¼Œè€Œè‡ªåŠ¨æ£€æµ‹å™¨äº§ç”Ÿè¾ƒé«˜çš„è¯¯æŠ¥ã€‚</li><li>éšç€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œè¿™äº›å¼±ç‚¹å°†ç»§ç»­å­˜åœ¨ï¼Œäººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿå°†å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li><li>åˆ†æäº†Hiveå’Œä¸“å®¶è‰ºæœ¯å®¶åœ¨ä¸åŒæ–¹é¢çš„é”™è¯¯ï¼Œå¹¶è¯æ˜äº†ä¸ºä»€ä¹ˆäººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿæä¾›äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„æœ€ä½³ç»„åˆã€‚ï¼ˆ3ï¼‰æ€§èƒ½ï¼š</li><li>åœ¨æ ¸å¿ƒæµ‹è¯•æ•°æ®é›†ä¸Šï¼ŒHiveå’Œä¸“å®¶è‰ºæœ¯å®¶éƒ½è¡¨ç°å¾—éå¸¸å¥½ï¼Œä½†Hiveåœ¨å¯¹æŠ—æ€§æ‰°åŠ¨ä¸­è¡¨ç°è¾ƒå¼±ï¼Œè€Œä¸“å®¶è‰ºæœ¯å®¶äº§ç”Ÿè¾ƒé«˜çš„è¯¯æŠ¥ã€‚</li><li>Hiveåœ¨å¯¹æŠ—æ€§æ‰°åŠ¨ä¸­è¾ƒå¼±ï¼Œè€Œä¸“å®¶è‰ºæœ¯å®¶äº§ç”Ÿè¾ƒé«˜çš„è¯¯æŠ¥ã€‚</li><li>äººç±»å’Œè‡ªåŠ¨æ£€æµ‹å™¨çš„ç»„åˆå›¢é˜Ÿæä¾›äº†å‡†ç¡®æ€§å’Œé²æ£’æ€§çš„æœ€ä½³ç»„åˆã€‚ï¼ˆ4ï¼‰å·¥ä½œé‡ï¼š</li><li>æ”¶é›†äº†è·¨è¶Š7ç§é£æ ¼çš„280å¹…çœŸå®äººç±»è‰ºæœ¯ä½œå“å’Œ350å¹…äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒã€‚</li><li>è¯„ä¼°äº†8ä¸ªæ£€æµ‹å™¨ï¼ˆ5ä¸ªè‡ªåŠ¨æ£€æµ‹å™¨å’Œ3ä¸ªä¸åŒçš„äººç±»ç¾¤ä½“ï¼‰åœ¨æ ¸å¿ƒæµ‹è¯•æ•°æ®é›†å’Œå„ç§å¯¹æŠ—æ€§æ‰°åŠ¨ä¸Šçš„æ€§èƒ½ã€‚</li><li>è¿›è¡Œå•ç‹¬çš„ç”¨æˆ·ç ”ç©¶ï¼Œé’ˆå¯¹3ä¸ªç‹¬ç«‹çš„ç”¨æˆ·ç¾¤ä½“ï¼šåŸºæœ¬å‚ä¸è€…ã€ä¸“ä¸šè‰ºæœ¯å®¶å¿—æ„¿è€…å’Œä¸“å®¶å‚ä¸è€…ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8cba12717aa69817e10b925c47c7e5f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-654bd7a18967bfc99c8234931b745b7f.jpg" align="middle"></details><h2 id="PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model"><a href="#PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model" class="headerlink" title="PFDM: Parser-Free Virtual Try-on via Diffusion Model"></a>PFDM: Parser-Free Virtual Try-on via Diffusion Model</h2><p><strong>Authors:Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</strong></p><p>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can â€œwearâ€ garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models. </p><p><a href="http://arxiv.org/abs/2402.03047v1">PDF</a> Accepted by IEEE ICASSP 2024</p><p><strong>Summary</strong><br>æ— è§£æå™¨è™šæ‹Ÿè¯•ç©¿æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€ç²¾å‡†åˆ†å‰²æ©ç ï¼Œå³å¯å®ç°é€¼çœŸè¯•ç©¿æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>PFDMæ˜¯ä¸€ç§æ— è§£æå™¨çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼åœ°â€œç©¿ä¸Šâ€ç›®æ ‡äººç‰©çš„è¡£æœï¼Œè€Œæ— éœ€ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚</li><li>PFDMä½¿ç”¨æ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ æ— è§£æå™¨çš„è™šæ‹Ÿè¯•ç©¿ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•æ‰äººä½“çš„ç»“æ„å’Œè¡£æœçš„ç»†èŠ‚ã€‚</li><li>PFDMé€šè¿‡åˆæˆå¤§é‡ä¼ªå›¾åƒå¹¶æ„é€ æ ·æœ¬å¯¹æ¥å­¦ä¹ ï¼Œå…¶ä¸­ä¼ªå›¾åƒåŒ…å«äº†å„ç§ç©¿ç€ä¸åŒè¡£æœçš„äººã€‚</li><li>PFDMä½¿ç”¨æå‡ºçš„æœè£…èåˆæ³¨æ„ï¼ˆGFAï¼‰æœºåˆ¶æ¥èåˆäººç‰©å’Œè¡£æœçš„ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆé€¼çœŸçš„è¯•ç©¿å›¾åƒã€‚</li><li>PFDMå¯ä»¥å¤„ç†å¤æ‚çš„æƒ…å†µï¼Œåˆæˆé«˜ä¿çœŸå›¾åƒï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰åŸºäºè§£æå™¨å’Œæ— è§£æå™¨çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ã€‚</li><li>PFDMå¯ä»¥ç”¨äºåœ¨çº¿å’Œåº—å†…è´­ç‰©åœºæ™¯ï¼Œæ˜¾è‘—æ”¹å–„æœè£…è´­ç‰©ä½“éªŒã€‚</li><li>PFDMæœ‰æœ›åœ¨è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šPFDMï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿</li><li>ä½œè€…ï¼šç‰›äº‘èŠ³ï¼Œæ˜“ä¸œï¼Œå´ä»¤ç¥¥ï¼Œåˆ˜æ™ºä¼Ÿï¼Œè”¡é¹ç¿”ï¼Œç‹é‡‘æ¡¥</li><li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€ï¼Œæ¨¡å¼è¯†åˆ«å›½å®¶é‡ç‚¹å®éªŒå®¤ï¼ŒåŸºç¡€æ¨¡å‹ç ”ç©¶ä¸­å¿ƒï¼ŒåŒ—äº¬ï¼Œä¸­å›½</li><li>å…³é”®è¯ï¼šè™šæ‹Ÿè¯•ç©¿ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œéšå¼æ‰­æ›²ï¼Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.03047</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šè™šæ‹Ÿè¯•ç©¿å¯ä»¥æ˜¾è‘—æ”¹å–„åœ¨çº¿å’Œåº—å†…åœºæ™¯ä¸­çš„æœè£…è´­ç‰©ä½“éªŒï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œä¸ºäº†å®ç°é«˜ä¿çœŸè¯•ç©¿æ€§èƒ½ï¼Œå¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•ä»ç„¶ä¾èµ–äºå‡†ç¡®çš„åˆ†å‰²æ©ç ï¼Œè¿™äº›æ©ç é€šå¸¸ç”±è¿‘ä¹å®Œç¾çš„è§£æå™¨æˆ–æ‰‹åŠ¨æ ‡æ³¨äº§ç”Ÿã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼ˆPFDMï¼‰ã€‚ç»™å®šä¸¤å¼ å›¾åƒï¼ŒPFDM å¯ä»¥é€šè¿‡éšå¼æ‰­æ›²å°†æœè£…æ— ç¼åœ°â€œç©¿â€åœ¨ç›®æ ‡äººç‰©èº«ä¸Šï¼Œè€Œæ— éœ€ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬åˆæˆäº†è®¸å¤šä¼ªå›¾åƒï¼Œå¹¶é€šè¿‡åœ¨äººç‰©èº«ä¸Šç©¿æˆ´å„ç§æœè£…æ¥æ„å»ºæ ·æœ¬å¯¹ã€‚åœ¨ç”±å¤§è§„æ¨¡æ‰©å±•æ•°æ®é›†ç›‘ç£ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æå‡ºçš„æœè£…èåˆæ³¨æ„ï¼ˆGFAï¼‰æœºåˆ¶èåˆäººç‰©å’Œæœè£…ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ PFDM å¯ä»¥æˆåŠŸå¤„ç†å¤æ‚æƒ…å†µï¼Œåˆæˆé«˜ä¿çœŸå›¾åƒï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„æ— è§£æå’ŒåŸºäºè§£æçš„æ¨¡å‹ã€‚ï¼ˆ2ï¼‰ï¼šGAN ç”¨äºè™šæ‹Ÿè¯•ç©¿ã€‚åŸºäº GAN çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸¤æ­¥æ¶æ„ï¼Œé¦–å…ˆå°†æœè£…æ‰­æ›²æˆç›®æ ‡å½¢çŠ¶ï¼Œç„¶åé€šè¿‡ç»„åˆæ‰­æ›²çš„æœè£…å’Œäººç‰©å›¾åƒæ¥åˆæˆç»“æœã€‚ä¸€äº›å·¥ä½œä¸“æ³¨äºåŸºäºè–„æ¿æ ·æ¡å˜æ¢ (TPS) æˆ–å…¨å±€æµå¢å¼ºæ‰­æ›²æ¨¡å—ã€‚å…¶ä»–å·¥ä½œæ—¨åœ¨æé«˜ç”Ÿæˆæ¨¡å—çš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œé‡‡ç”¨å¯¹é½æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥æé«˜åˆæˆå›¾åƒçš„åˆ†è¾¨ç‡ï¼Œæˆ–æ”¹è¿›æŸå¤±å‡½æ•°ä»¥ä¿ç•™äººç‰©èº«ä»½ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ¡†æ¶ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªå°†æ‰©æ•£æ¨¡å‹ç”¨äºæ— è§£æè™šæ‹Ÿè¯•ç©¿çš„å·¥ä½œã€‚æˆ‘ä»¬è¿˜ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªå¢å¼ºçš„äº¤å‰æ³¨æ„æ¨¡å—æ¥èåˆäººç‰©å’Œæœè£…ç‰¹å¾ä»¥è¿›è¡Œéšå¼æ‰­æ›²ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ã€‚æˆ‘ä»¬åœ¨ VITON-HD ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— è§£ææ¨¡å‹åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1)ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†æœè£…æ— ç¼åœ°â€œç©¿â€åœ¨ç›®æ ‡äººç‰©èº«ä¸Šï¼Œè€Œæ— éœ€ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚(2)ï¼šæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†ä¸€ä¸ªå¢å¼ºçš„äº¤å‰æ³¨æ„æ¨¡å—æ¥èåˆäººç‰©å’Œæœè£…ç‰¹å¾ä»¥è¿›è¡Œéšå¼æ‰­æ›²ã€‚(3)ï¼šæˆ‘ä»¬åˆæˆäº†è®¸å¤šä¼ªå›¾åƒï¼Œå¹¶é€šè¿‡åœ¨äººç‰©èº«ä¸Šç©¿æˆ´å„ç§æœè£…æ¥æ„å»ºæ ·æœ¬å¯¹ã€‚(4)ï¼šåœ¨ç”±å¤§è§„æ¨¡æ‰©å±•æ•°æ®é›†ç›‘ç£ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æå‡ºçš„æœè£…èåˆæ³¨æ„ï¼ˆGFAï¼‰æœºåˆ¶èåˆäººç‰©å’Œæœè£…ç‰¹å¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ‰­æ›²å’Œèåˆæ­¥éª¤ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼ŒåŒæ—¶é¿å…äº†ä½¿ç”¨ä»»ä½•è§£æå™¨æˆ–å¤–éƒ¨æ¨¡å—ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒPFDM æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒPFD å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸°å¯Œçº¹ç†ç»†èŠ‚çš„é«˜åˆ†è¾¨ç‡é«˜ä¿çœŸè¯•ç©¿ç»“æœï¼Œå¹¶æˆåŠŸå¤„ç†é”™ä½å’Œé®æŒ¡ï¼Œè¿™ä¸ä»…ä¼˜äºç°æœ‰çš„æ— è§£ææ–¹æ³•ï¼Œè€Œä¸”åœ¨å®šæ€§å’Œå®šé‡åˆ†æä¸­ä¹Ÿè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºäºè§£æå™¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿä¿ƒè¿›è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯åœ¨ç”µå­å•†åŠ¡å’Œå…ƒå®‡å®™ä¸­çš„æ™®åŠã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— è§£æè™šæ‹Ÿè¯•ç©¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ‰­æ›²å’Œèåˆæ­¥éª¤ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼ŒåŒæ—¶é¿å…äº†ä½¿ç”¨ä»»ä½•è§£æå™¨æˆ–å¤–éƒ¨æ¨¡å—ã€‚æ€§èƒ½ï¼šåœ¨å®šæ€§å’Œå®šé‡åˆ†æä¸­ï¼ŒPFDM ä¼˜äºç°æœ‰çš„æ— è§£ææ–¹æ³•å’Œæœ€å…ˆè¿›çš„åŸºäºè§£æå™¨çš„æ¨¡å‹ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦åˆæˆå¤§é‡ä¼ªå›¾åƒå¹¶æ„å»ºæ ·æœ¬å¯¹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4fa2bcca39e4d002618ff0b3dcd93311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d530a087c0dd3abddf2412c841493d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4af8acc18772befb8884db138ea6e422.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3f34614b487167c039e9989a45cc12d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5123c9bbcb697725b9020c9d4ab0422.jpg" align="middle"></details><h2 id="Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models"><a href="#Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models" class="headerlink" title="Extreme Two-View Geometry From Object Poses with Diffusion Models"></a>Extreme Two-View Geometry From Object Poses with Diffusion Models</h2><p><strong>Authors:Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu</strong></p><p>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2402.02800v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆæ–°è§†å›¾å›¾åƒè¿›è¡Œå¯¹è±¡å§¿æ€ä¼°è®¡ï¼Œæœ‰æ•ˆæ±‚è§£æç«¯è§†è§’å˜åŒ–ä¸‹çš„ä¸¤è§†å›¾å‡ ä½•é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å¯¹è±¡å…ˆéªŒé€šè¿‡æ‰©æ•£æ¨¡å‹Zero123åˆæˆæ–°è§†å›¾å›¾åƒï¼Œå¢å¼ºäº†å¯¹è±¡å§¿æ€ä¼°è®¡çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚</li><li>å°†ç›¸å¯¹ç›¸æœºå§¿æ€ä¼°è®¡é—®é¢˜æ•°å­¦è½¬æ¢ä¸ºå¯¹è±¡å§¿æ€ä¼°è®¡é—®é¢˜ï¼Œç®€åŒ–äº†é—®é¢˜çš„æ±‚è§£ã€‚</li><li>åœ¨æç«¯è§†è§’å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œåˆæˆçš„æ–°è§†å›¾å›¾åƒç»åŒ¹é…å¯ä»¥ç¡®å®šå¯¹è±¡å§¿æ€ï¼Œä»è€Œç¡®å®šä¸¤è§†å›¾ç›¸æœºå§¿æ€ã€‚</li><li>è¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºéå‡¡çš„é²æ£’æ€§å’Œå¼¹æ€§ï¼Œä¼°è®¡ä¸¤è§†å›¾å§¿æ€å…·æœ‰æ°å‡ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>å¯åœ¨ <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a> è·å–ä»£ç ã€‚</li><li>è¯¥æ–¹æ³•ç²¾åº¦é«˜ï¼Œåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡è¡¨ç°è‰¯å¥½ã€‚</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºè§£å†³æç«¯è§†è§’å˜åŒ–ä¸‹çš„ä¸¤è§†å›¾å‡ ä½•é—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šä»ç‰©ä½“ä½å§¿ä¼°è®¡æç«¯ä¸¤è§†å›¾å‡ ä½•</li><p></p><p></p><li>ä½œè€…ï¼šYujing Sunã€Caiyi Sunã€Yuan Liuã€Yuexin Maã€Siu Ming Yiu</li><p></p><p></p><li>éš¶å±æœºæ„ï¼šé¦™æ¸¯å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šä¸¤è§†å›¾å‡ ä½•ã€ç‰©ä½“ä½å§¿ä¼°è®¡ã€æ‰©æ•£æ¨¡å‹ã€ç”Ÿæˆæ¨¡å‹</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.02800    Github ä»£ç é“¾æ¥ï¼šhttps://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</li><p></p><p></p><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šäººç±»å…·æœ‰æƒŠäººçš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ¯«ä¸è´¹åŠ›åœ°æ„ŸçŸ¥åŒ…å«ç›¸åŒç‰©ä½“çš„ä¸¤å¹…å›¾åƒä¹‹é—´çš„è§†ç‚¹å·®å¼‚ï¼Œå³ä½¿è§†ç‚¹å˜åŒ–éå¸¸å¤§ï¼Œå›¾åƒä¸­æ²¡æœ‰å…±åŒå¯è§çš„åŒºåŸŸã€‚ç„¶è€Œï¼Œå¯¹äºç°æœ‰çš„ç›¸æœºä½å§¿ä¼°è®¡æ–¹æ³•æ¥è¯´ï¼Œè¿™ç§éå‡¡çš„èƒ½åŠ›è¢«è¯æ˜æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•åœ¨é¢å¯¹å¤§çš„è§†ç‚¹å·®å¼‚æ—¶é€šå¸¸ä¼šå¤±è´¥ï¼ŒåŸå› æ˜¯ç¼ºå°‘ç”¨äºåŒ¹é…çš„é‡å å±€éƒ¨ç‰¹å¾ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨å±€éƒ¨ç‰¹å¾åŒ¹é…æ¥ä¼°è®¡ä¸¤è§†å›¾å‡ ä½•ã€‚ç„¶è€Œï¼Œå½“è§†ç‚¹å·®å¼‚è¾ƒå¤§æ—¶ï¼Œè¿™ç§æ–¹æ³•å¾€å¾€ä¼šå¤±è´¥ï¼Œå› ä¸ºæ²¡æœ‰è¶³å¤Ÿçš„é‡å å±€éƒ¨ç‰¹å¾å¯ä¾›åŒ¹é…ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥ä¼°è®¡æç«¯ä¸¤è§†å›¾å‡ ä½•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå°†ç›¸å¯¹ç›¸æœºä½å§¿ä¼°è®¡é—®é¢˜è½¬æ¢ä¸ºç‰©ä½“ä½å§¿ä¼°è®¡é—®é¢˜ã€‚ç„¶åï¼Œä¸ºäº†ä¼°è®¡ç‰©ä½“ä½å§¿ï¼Œæˆ‘ä»¬åˆ©ç”¨ä»æ‰©æ•£æ¨¡å‹ Zero123 å­¦ä¹ åˆ°çš„ç‰©ä½“å…ˆéªŒæ¥åˆæˆç‰©ä½“çš„ novel-view å›¾åƒã€‚å°† novel-view å›¾åƒåŒ¹é…ä»¥ç¡®å®šç‰©ä½“ä½å§¿ï¼Œä»è€Œç¡®å®šä¸¤è§†å›¾ç›¸æœºä½å§¿ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºéå‡¡çš„é²æ£’æ€§å’Œå¯¹å¤§è§†ç‚¹å˜åŒ–çš„é€‚åº”æ€§ï¼Œèƒ½å¤Ÿä¸€è‡´åœ°ä¼°è®¡ä¸¤è§†å›¾ä½å§¿ï¼Œå¹¶ä¸”åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å‡†ç¡®åœ°ç¡®å®šæç«¯è§†ç‚¹å˜åŒ–ä¸‹çš„ä¸¤è§†å›¾å‡ ä½•ã€‚</li><br>&lt;/ol&gt;<p></p><p>Methods:(1): æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥ä¼°è®¡æç«¯ä¸¤è§†å›¾å‡ ä½•ï¼Œè¯¥æ–¹æ³•å°†ç›¸å¯¹ç›¸æœºä½å§¿ä¼°è®¡é—®é¢˜è½¬æ¢ä¸ºç‰©ä½“ä½å§¿ä¼°è®¡é—®é¢˜ï¼›(2): åˆ©ç”¨ä»æ‰©æ•£æ¨¡å‹Zero123å­¦ä¹ åˆ°çš„ç‰©ä½“å…ˆéªŒæ¥åˆæˆç‰©ä½“çš„novel-viewå›¾åƒï¼Œå°†novel-viewå›¾åƒåŒ¹é…ä»¥ç¡®å®šç‰©ä½“ä½å§¿ï¼›(3): é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®—æ³•ï¼Œå¯ä»¥ä¼°è®¡å…·æœ‰æç«¯è§†ç‚¹å˜åŒ–çš„ç›¸å¯¹ç›¸æœºä½å§¿ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ä»å¤§è§„æ¨¡ 2D æ‰©æ•£æ¨¡å‹ Zero123 å­¦ä¹ åˆ°çš„ç‰©ä½“å…ˆéªŒï¼Œè¯¥å…ˆéªŒèƒ½å¤Ÿç”Ÿæˆå¯¹è±¡çš„ novel-view å›¾åƒã€‚ä½†æ˜¯ï¼Œç”±äº Zero123 åœ¨å…¶æ¨¡å‹ä¸­éšå¼å®šä¹‰äº†è§„èŒƒåæ ‡ç³»ï¼Œå¹¶ä¸”å›¾åƒå¯èƒ½ä¸ä¼šçœ‹å‘å¯¹è±¡ï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•ç›´æ¥åº”ç”¨ Zero123ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°çš„ä¸¤è§†å›¾ä½å§¿ä¼°è®¡å…¬å¼ï¼Œä½œä¸ºç‰©ä½“ä½å§¿ä¼°è®¡é—®é¢˜ï¼Œå¹¶æ­£ç¡®å®šä¹‰è¾“å…¥å›¾åƒå’Œç”Ÿæˆå›¾åƒçš„ç‰©ä½“ä½å§¿ã€‚æœ€åï¼Œæˆ‘ä»¬åŒ¹é…å¦ä¸€å¹…å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å°†ç›¸å¯¹ç›¸æœºä½å§¿ä¼°è®¡é—®é¢˜è½¬æ¢ä¸ºç‰©ä½“ä½å§¿ä¼°è®¡é—®é¢˜ï¼Œå¹¶åˆ©ç”¨ä»æ‰©æ•£æ¨¡å‹ Zero123 å­¦ä¹ åˆ°çš„ç‰©ä½“å…ˆéªŒæ¥åˆæˆç‰©ä½“çš„ novel-view å›¾åƒï¼Œå°† novel-view å›¾åƒåŒ¹é…ä»¥ç¡®å®šç‰©ä½“ä½å§¿ï¼Œä»è€Œç¡®å®šä¸¤è§†å›¾ç›¸æœºä½å§¿ã€‚</li><li>åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºéå‡¡çš„é²æ£’æ€§å’Œå¯¹å¤§è§†ç‚¹å˜åŒ–çš„é€‚åº”æ€§ï¼Œèƒ½å¤Ÿä¸€è‡´åœ°ä¼°è®¡ä¸¤è§†å›¾ä½å§¿ï¼Œå¹¶ä¸”åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å‡†ç¡®åœ°ç¡®å®šæç«¯è§†ç‚¹å˜åŒ–ä¸‹çš„ä¸¤è§†å›¾å‡ ä½•ã€‚</li></ol><p>æ€§èƒ½ï¼š- åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è§†ç‚¹å˜åŒ–èŒƒå›´å†…éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æç«¯è§†ç‚¹å˜åŒ–ä¸‹å…·æœ‰æ˜¾ç€çš„ä¼˜åŠ¿ã€‚- åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æç«¯è§†ç‚¹å˜åŒ–ä¸‹å…·æœ‰æ˜¾ç€çš„ä¼˜åŠ¿ã€‚</p><p>å·¥ä½œé‡ï¼š- è¯¥æ–¹æ³•éœ€è¦è®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ ç‰©ä½“å…ˆéªŒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚- è¯¥æ–¹æ³•è¿˜éœ€è¦åˆæˆ novel-view å›¾åƒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚- è¯¥æ–¹æ³•è¿˜éœ€è¦åŒ¹é… novel-view å›¾åƒï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-25907674667aa8b32d056fad9f68800a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-36f6cbd8fb9421b0eea500253c925684.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4d37974797f92e16872fe7a27774fa5a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b283ab7c3ead68d1a7cd2cceb1c42365.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5815c280ae8797af92483f51007a87d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a313bd5672a496366b53aa94dffc26ae.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-09  Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/</id>
    <published>2024-02-02T14:27:07.000Z</published>
    <updated>2024-02-02T14:27:07.050Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-02-æ›´æ–°"><a href="#2024-02-02-æ›´æ–°" class="headerlink" title="2024-02-02 æ›´æ–°"></a>2024-02-02 æ›´æ–°</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the sceneâ€™s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>Summary</strong><br>æ–‡æœ¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³• ViCA-NeRFï¼Œè¯¥æ–¹æ³•å¯ä»¥åˆ©ç”¨æ–‡æœ¬ç¼–è¾‘è¿›è¡Œ 3D ç¼–è¾‘ï¼Œå¹¶ä½¿ç”¨å‡ ä½•å’Œå­¦ä¹ æ­£åˆ™åŒ–æ¥ç¡®ä¿ç¼–è¾‘çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ViCA-NeRF æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºæ–‡æœ¬çš„ 3D ç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨ NeRF è¿›è¡Œéšå¼ç¥ç»è¾å°„åœºå»ºæ¨¡ã€‚</li><li>ViCA-NeRF çš„å…³é”®æ€æƒ³æ˜¯åˆ©ç”¨ä¸¤ç§æ­£åˆ™åŒ–æ¥æºï¼Œæ˜ç¡®åœ°åœ¨ä¸åŒè§†å›¾ä¹‹é—´ä¼ æ’­ç¼–è¾‘ä¿¡æ¯ï¼Œç¡®ä¿å¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>ViCA-NeRF åˆ©ç”¨ä» NeRF æ¨å¯¼å‡ºçš„æ·±åº¦ä¿¡æ¯æ¥å»ºç«‹ä¸åŒè§†å›¾ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œç”¨äºå‡ ä½•æ­£åˆ™åŒ–ã€‚</li><li>ViCA-NeRF å¯¹ç»è¿‡ç¼–è¾‘å’Œæœªç»è¿‡ç¼–è¾‘çš„å›¾åƒåœ¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„æ½œåœ¨ç¼–ç è¿›è¡Œå¯¹é½ï¼Œå®ç°ç¼–è¾‘å…³é”®è§†å›¾å¹¶æ›´æ–°æ•´ä¸ªåœºæ™¯ã€‚</li><li>ViCA-NeRF é‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µçš„å·¥ä½œæµç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µå°†æ¥è‡ªä¸åŒè§†å›¾çš„ç¼–è¾‘èåˆï¼Œåˆ›å»ºåˆæ­¥çš„ 3D ç¼–è¾‘ã€‚</li><li>ç¬¬äºŒé˜¶æ®µè¿›è¡Œ NeRF è®­ç»ƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–åœºæ™¯çš„å¤–è§‚ã€‚</li><li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒViCA-NeRF æä¾›æ›´çµæ´»ã€æ›´é«˜æ•ˆï¼ˆé€Ÿåº¦æå‡ 3 å€ï¼‰ã€æ›´ä¸€è‡´ä¸”æ›´è¯¦ç»†çš„ç¼–è¾‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šViCA-NeRFï¼šåŸºäºè§†å›¾ä¸€è‡´æ€§çš„ç¥ç»è¾å°„åœºä¸‰ç»´ç¼–è¾‘</li><li>ä½œè€…ï¼šJiahua Dong, Yu-Xiong Wang</li><li>å•ä½ï¼šä¼Šåˆ©è¯ºä¼Šå¤§å­¦å„å·´çº³-é¦™æ§Ÿåˆ†æ ¡</li><li>å…³é”®è¯ï¼šä¸‰ç»´ç¼–è¾‘ã€ç¥ç»è¾å°„åœºã€è§†å›¾ä¸€è‡´æ€§ã€æ–‡æœ¬æŒ‡ä»¤</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00864Github é“¾æ¥ï¼šhttps://dongjiahua.github.io/VICA-NeRF</li><li><p>æ‘˜è¦ï¼š(1)ï¼šéšç€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åŠå…¶å˜ä½“çš„æœ€æ–°è¿›å±•ï¼Œæ”¶é›†çœŸå®ä¸–ç•Œä¸‰ç»´åœºæ™¯æ•°æ®å˜å¾—æ›´åŠ ä¾¿æ·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸‰ç»´ç¼–è¾‘æ–¹æ³•é€šå¸¸ç¼ºä¹è§†å›¾ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç¼–è¾‘ç»“æœåœ¨ä¸åŒè§†è§’ä¸‹å¯èƒ½å‡ºç°ä¸ä¸€è‡´çš„æƒ…å†µã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬åŸºäºå‡ ä½•çš„æ­£åˆ™åŒ–å’ŒåŸºäºå­¦ä¹ çš„æ­£åˆ™åŒ–ã€‚å‡ ä½•æ­£åˆ™åŒ–åˆ©ç”¨ NeRF æå–çš„æ·±åº¦ä¿¡æ¯æ¥å»ºç«‹ä¸åŒè§†è§’ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œä»è€Œç¡®ä¿è§†å›¾ä¸€è‡´æ€§ã€‚å­¦ä¹ æ­£åˆ™åŒ–åˆ™é€šè¿‡å¯¹ç¼–è¾‘å›¾åƒå’Œæœªç¼–è¾‘å›¾åƒçš„æ½œåœ¨ä»£ç è¿›è¡Œå¯¹é½ï¼Œä½¿ç¼–è¾‘ä¿¡æ¯èƒ½å¤Ÿåœ¨æ•´ä¸ªåœºæ™¯ä¸­ä¼ æ’­ã€‚(3)ï¼šæœ¬æ–‡æå‡ºçš„ ViCA-NeRF æ˜¯ä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„ä¸‰ç»´ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒç»“åˆäº†å‡ ä½•æ­£åˆ™åŒ–å’Œå­¦ä¹ æ­£åˆ™åŒ–ä¸¤ç§ç­–ç•¥ã€‚ViCA-NeRF é¦–å…ˆé€šè¿‡èåˆæ¥è‡ªä¸åŒè§†è§’çš„ç¼–è¾‘ç»“æœæ¥åˆ›å»ºåˆæ­¥çš„ä¸‰ç»´ç¼–è¾‘ï¼Œç„¶åé€šè¿‡ NeRF è®­ç»ƒè¿›ä¸€æ­¥ç»†åŒ–åœºæ™¯çš„å¤–è§‚ï¼Œä»è€Œç¡®ä¿è§†å›¾ä¸€è‡´æ€§å’Œç»†èŠ‚ä¸°å¯Œã€‚(4)ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒViCA-NeRF æä¾›äº†æ›´åŠ çµæ´»ã€é«˜æ•ˆï¼ˆé€Ÿåº¦æé«˜ 3 å€ï¼‰çš„ç¼–è¾‘æ–¹å¼ï¼Œå¹¶ä¸”å…·æœ‰æ›´é«˜çš„è§†å›¾ä¸€è‡´æ€§å’Œç»†èŠ‚æ°´å¹³ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ViCA-NeRF é¦–å…ˆä»ä¸åŒè§†è§’æ”¶é›†è¾“å…¥å›¾åƒï¼Œå¹¶ä½¿ç”¨ NeRF ä»è¿™äº›å›¾åƒä¸­æå–æ·±åº¦ä¿¡æ¯ã€‚(2) ç„¶åï¼ŒViCA-NeRF åˆ©ç”¨æå–çš„æ·±åº¦ä¿¡æ¯æ¥å»ºç«‹ä¸åŒè§†è§’ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å¯¹åº”å…³ç³»æ¥èåˆæ¥è‡ªä¸åŒè§†è§’çš„ç¼–è¾‘ç»“æœï¼Œä»è€Œåˆ›å»ºåˆæ­¥çš„ä¸‰ç»´ç¼–è¾‘ã€‚(3) æœ€åï¼ŒViCA-NeRF é€šè¿‡ NeRF è®­ç»ƒè¿›ä¸€æ­¥ç»†åŒ–åœºæ™¯çš„å¤–è§‚ï¼Œä»è€Œç¡®ä¿è§†å›¾ä¸€è‡´æ€§å’Œç»†èŠ‚ä¸°å¯Œã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† ViCA-NeRFï¼Œä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„ä¸‰ç»´ç¼–è¾‘æ¡†æ¶ï¼Œç”¨äºæ–‡æœ¬å¼•å¯¼çš„ NeRF ç¼–è¾‘ã€‚ç»™å®šæ–‡æœ¬æŒ‡ä»¤ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°ç¼–è¾‘ NeRFã€‚é™¤äº†åƒäººç±»é£æ ¼åŒ–å’Œå¤©æ°”å˜åŒ–è¿™æ ·çš„ç®€å•ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒä¸Šä¸‹æ–‡ç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚â€œæ·»åŠ ä¸€äº›èŠ±æœµâ€å’Œç¼–è¾‘é«˜åº¦è¯¦ç»†çš„çº¹ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åœºæ™¯å’Œæ–‡æœ¬æç¤ºä¸Šä¼˜äºå‡ ä¸ªåŸºçº¿ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­æé«˜ä¸‰ç»´ç¼–è¾‘çš„å¯æ§æ€§å’ŒçœŸå®æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šViCA-NeRF ç»“åˆäº†å‡ ä½•æ­£åˆ™åŒ–å’Œå­¦ä¹ æ­£åˆ™åŒ–ä¸¤ç§ç­–ç•¥ï¼Œä»¥ç¡®ä¿è§†å›¾ä¸€è‡´æ€§å’Œç»†èŠ‚ä¸°å¯Œã€‚ViCA-NeRF åˆ©ç”¨æå–çš„æ·±åº¦ä¿¡æ¯æ¥å»ºç«‹ä¸åŒè§†è§’ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å¯¹åº”å…³ç³»æ¥èåˆæ¥è‡ªä¸åŒè§†è§’çš„ç¼–è¾‘ç»“æœï¼Œä»è€Œåˆ›å»ºåˆæ­¥çš„ä¸‰ç»´ç¼–è¾‘ã€‚ViCA-NeRF é€šè¿‡ NeRF è®­ç»ƒè¿›ä¸€æ­¥ç»†åŒ–åœºæ™¯çš„å¤–è§‚ï¼Œä»è€Œç¡®ä¿è§†å›¾ä¸€è‡´æ€§å’Œç»†èŠ‚ä¸°å¯Œã€‚æ€§èƒ½ï¼šViCA-NeRF åœ¨å„ç§åœºæ™¯å’Œæ–‡æœ¬æç¤ºä¸Šä¼˜äºå‡ ä¸ªåŸºçº¿ã€‚ViCA-NeRF çš„é€Ÿåº¦æé«˜äº† 3 å€ã€‚å·¥ä½œé‡ï¼šViCA-NeRF çš„å®ç°ç›¸å¯¹ç®€å•ã€‚ViCA-NeRF çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦è¾ƒå¿«ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/</id>
    <published>2024-02-02T14:24:12.000Z</published>
    <updated>2024-02-02T14:24:12.861Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-02-æ›´æ–°"><a href="#2024-02-02-æ›´æ–°" class="headerlink" title="2024-02-02 æ›´æ–°"></a>2024-02-02 æ›´æ–°</h1><h2 id="360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming"><a href="#360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming" class="headerlink" title="360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming"></a>360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</h2><p><strong>Authors:Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</strong></p><p>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios. </p><p><a href="http://arxiv.org/abs/2402.00763v1">PDF</a> 11 pages, 10 figures</p><p><strong>Summary</strong><br>360-GS ä»¥å¹³é¢æŠ•å½±ä¸ºåŸºç¡€ï¼Œåˆ©ç”¨å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œäº§ç”Ÿå¯ç”¨äºæ¸²æŸ“å…¨æ™¯å’Œç”Ÿæˆæ–°è§†è§’å›¾åƒçš„ 3D æ¤­åœ†é«˜æ–¯åˆ†å¸ƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D é«˜æ–¯æ–‘ç‚¹ (3D-GS) æ˜¯ä¸€ç§æµè¡Œçš„æŠ€æœ¯ï¼Œå®ƒé€šå¸¸å°†é€è§†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¼˜åŒ–ä¸€ç»„ 3D æ¤­åœ†é«˜æ–¯åˆ†å¸ƒï¼Œå°†å®ƒä»¬å–·å°„åˆ°å›¾åƒå¹³é¢ä¸Šï¼Œä»è€Œäº§ç”Ÿ 2D é«˜æ–¯åˆ†å¸ƒã€‚</li><li>ç„¶è€Œï¼Œå°† 3D-GS åº”ç”¨äºå…¨æ™¯è¾“å…¥æ—¶ï¼Œä½¿ç”¨ 2D é«˜æ–¯åˆ†å¸ƒå¯¹ ${360^\circ}$ å›¾åƒçš„çƒå½¢è¡¨é¢ä¸Šçš„æŠ•å½±è¿›è¡Œå»ºæ¨¡å­˜åœ¨æŒ‘æˆ˜ã€‚</li><li>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¾“å…¥å…¨æ™¯é€šå¸¸å¾ˆç¨€ç–ï¼Œå¯¼è‡´ 3D é«˜æ–¯åˆ†å¸ƒçš„åˆå§‹åŒ–ä¸å¯é ï¼Œéšå 3D-GS è´¨é‡ä¸‹é™ã€‚</li><li>æ­¤å¤–ï¼Œç”±äºçº¹ç†å¹³é¢ï¼ˆä¾‹å¦‚å¢™å£å’Œåœ°æ¿ï¼‰çš„å‡ ä½•å½¢çŠ¶å—é™ï¼Œ3D-GS éš¾ä»¥ä½¿ç”¨æ¤­åœ†é«˜æ–¯åˆ†å¸ƒå¯¹è¿™äº›å¹³å¦åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå¯¼è‡´æ–°è§†å›¾ä¸­å‡ºç°æ˜æ˜¾çš„æ¼‚æµ®ç‰©ã€‚</li><li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† 360-GSï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æœ‰é™æ•°é‡çš„å…¨æ™¯è¾“å…¥çš„æ–°å‹ $360^{\circ}$ é«˜æ–¯æ–‘ç‚¹ã€‚</li><li>360-GS ä¸å°† 3D é«˜æ–¯åˆ†å¸ƒç›´æ¥å–·å°„åˆ°çƒå½¢è¡¨é¢ä¸Šï¼Œè€Œæ˜¯å°†å…¶æŠ•å½±åˆ°å•ä½çƒçš„åˆ‡å¹³é¢ï¼Œç„¶åå°†å®ƒä»¬æ˜ å°„åˆ°çƒå½¢æŠ•å½±ã€‚è¿™ç§æ”¹ç¼–èƒ½å¤Ÿä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè¡¨ç¤ºæŠ•å½±ã€‚</li><li>æˆ‘ä»¬é€šè¿‡åˆ©ç”¨å…¨æ™¯ä¸­çš„å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ 360-GS çš„ä¼˜åŒ–ï¼Œè¿™äº›å…ˆéªŒå¾ˆå®¹æ˜“è·å¾—ï¼Œå¹¶ä¸”åŒ…å«æœ‰å…³å®¤å†…åœºæ™¯çš„å¼ºå¤§ç»“æ„ä¿¡æ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼š360-GSï¼šå¸ƒå±€å¼•å¯¼çš„å®¤å†…å…¨æ™¯é«˜æ–¯æ¸²æŸ“</li><li>ä½œè€…ï¼šJiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</li><li>éš¶å±ï¼šå—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼š3Dé«˜æ–¯æ¸²æŸ“ã€å…¨æ™¯å›¾åƒã€å®¤å†…åœºæ™¯ã€å¸ƒå±€å¼•å¯¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00763   Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯æ¸²æŸ“ï¼ˆ3D-GSï¼‰å› å…¶å®æ—¶æ€§å’Œç…§ç‰‡çº§æ¸²æŸ“æ•ˆæœè€Œå¤‡å—å…³æ³¨ã€‚è¯¥æŠ€æœ¯é€šå¸¸ä»¥é€è§†å›¾åƒä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å°†ä¸€ç»„ 3D æ¤­åœ†é«˜æ–¯ä½“æ¸²æŸ“åˆ°å›¾åƒå¹³é¢ä¸Šï¼Œä»è€Œç”Ÿæˆ 2D é«˜æ–¯ä½“ã€‚ç„¶è€Œï¼Œå°† 3D-GS åº”ç”¨äºå…¨æ™¯è¾“å…¥æ—¶ï¼Œä½¿ç”¨ 2D é«˜æ–¯ä½“æœ‰æ•ˆå»ºæ¨¡ 360Â° å›¾åƒçš„çƒé¢æŠ•å½±å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¾“å…¥å…¨æ™¯å›¾åƒé€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œå¯¼è‡´ 3D é«˜æ–¯ä½“çš„åˆå§‹åŒ–ä¸å¯é ï¼Œè¿›è€Œé™ä½ 3D-GS çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹çº¹ç†çš„å¹³é¢ï¼ˆä¾‹å¦‚å¢™å£å’Œåœ°æ¿ï¼‰çš„å‡ ä½•çº¦æŸä¸è¶³ï¼Œ3D-GS éš¾ä»¥ä½¿ç”¨æ¤­åœ†é«˜æ–¯ä½“å¯¹è¿™äº›å¹³é¢åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå¯¼è‡´åœ¨æ–°çš„è§†è§’ä¸­å‡ºç°æ˜æ˜¾çš„æµ®åŠ¨ç‰©ä½“ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æœ‰é™å…¨æ™¯è¾“å…¥çš„æ–°å‹ 360Â° é«˜æ–¯æ¸²æŸ“æ–¹æ³• 360-GSã€‚ä¸ç›´æ¥å°† 3D é«˜æ–¯ä½“æ¸²æŸ“åˆ°çƒé¢ä¸Šä¸åŒï¼Œ360-GS å°†å…¶æŠ•å½±åˆ°å•ä½çƒä½“çš„åˆ‡å¹³é¢ï¼Œç„¶åå°†å…¶æ˜ å°„åˆ°çƒé¢æŠ•å½±ã€‚è¿™ç§æ”¹è¿›ä½¿å¾—ä½¿ç”¨é«˜æ–¯ä½“è¡¨ç¤ºæŠ•å½±æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨å…¨æ™¯å›¾åƒä¸­çš„å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ 360-GS çš„ä¼˜åŒ–ï¼Œè¿™äº›å…ˆéªŒæ˜“äºè·å–ï¼Œå¹¶ä¸”åŒ…å«æœ‰å…³å®¤å†…åœºæ™¯çš„å¼ºç»“æ„ä¿¡æ¯ã€‚   (3)ï¼šæœ¬æ–‡çš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œ360-GS èƒ½å¤Ÿä»æœ‰é™æ•°é‡çš„å…¨æ™¯è¾“å…¥ä¸­ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯æ¸²æŸ“ã€‚ä¸ 3D-GS ç›¸æ¯”ï¼Œ360-GS åœ¨å‡†ç¡®æ€§ã€ç»†èŠ‚å’Œé²æ£’æ€§æ–¹é¢å‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚   (4)ï¼šæ–¹æ³•çš„æ€§èƒ½åŠå…¶å¯¹ç›®æ ‡çš„æ”¯æŒï¼š360-GS åœ¨å®¤å†…åœºæ™¯æ¸²æŸ“ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸ 3D-GS ç›¸æ¯”ï¼Œ360-GS åœ¨å‡†ç¡®æ€§ã€ç»†èŠ‚å’Œé²æ£’æ€§æ–¹é¢å‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œ360-GS èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ 3D é«˜æ–¯ä½“çš„ä¼˜åŒ–ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯æ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1)ï¼š360â—¦é«˜æ–¯ä½“é•¶åµŒï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„ splatting æŠ€æœ¯ï¼Œå°† splatting åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šåœ¨å•ä½çƒä½“çš„åˆ‡å¹³é¢ä¸Š splatting å’Œæ˜ å°„åˆ°çƒé¢ã€‚(2)ï¼šå¸ƒå±€å¼•å¯¼åˆå§‹åŒ–å’Œæ­£åˆ™åŒ–ï¼šåˆ©ç”¨å…¨æ™¯å›¾åƒä¸­çš„å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ 3D é«˜æ–¯ä½“çš„ä¼˜åŒ–ï¼Œè¿™äº›å…ˆéªŒæ˜“äºè·å–ï¼Œå¹¶ä¸”åŒ…å«æœ‰å…³å®¤å†…åœºæ™¯çš„å¼ºç»“æ„ä¿¡æ¯ã€‚(3)ï¼šå…¨æ™¯æ¸²æŸ“ï¼šé€šè¿‡å°† splattered çš„é«˜æ–¯ä½“ä»å‰åˆ°åè¿›è¡Œ alpha æ··åˆï¼Œå¯ä»¥ç”Ÿæˆå…¨æ™¯æ¸²æŸ“ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸ƒå±€å¼•å¯¼å…¨æ™¯é«˜æ–¯æ¸²æŸ“æµæ°´çº¿ï¼Œåä¸º360-GSï¼Œå®ƒæ”¯æŒç›´æ¥å…¨æ™¯æ¸²æŸ“ï¼Œå¹¶ä¸”å¯¹ç¨€ç–è¾“å…¥å…·æœ‰é²æ£’æ€§ã€‚360-GSçš„åŸºçŸ³æ˜¯æˆ‘ä»¬çš„360â—¦é«˜æ–¯ splatting ç®—æ³•ä»¥åŠæˆ¿é—´å¸ƒå±€å…ˆéªŒçš„ç»“åˆã€‚360â—¦é«˜æ–¯ splatting ç®—æ³•é€šè¿‡åˆ©ç”¨é€è§†æŠ•å½±å’Œæ˜ å°„æ¥è§£å†³åœ¨çƒé¢è¡¨é¢å»ºæ¨¡æŠ•å½±çš„æŒ‘æˆ˜ï¼Œä»è€Œå®ç°å¯¹å…·æœ‰ç­‰è·çŸ©å½¢å›¾åƒçš„ 3D é«˜æ–¯çš„ç›´æ¥ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨ 3D é«˜æ–¯çš„åˆå§‹åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨å…¨æ™¯å›¾ä¸­çš„æˆ¿é—´å¸ƒå±€å…ˆéªŒï¼Œæä¾›äº†ä¸€ç§æ›´æ˜“äºè®¿é—®ä¸”é²æ£’çš„æ›¿ä»£æ–¹æ¡ˆæ¥æ›¿ä»£ SfM ç‚¹äº‘ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¸ƒå±€å¼•å¯¼æ­£åˆ™åŒ–æ¥å‡è½»æµ®åŠ¨é—®é¢˜å¹¶ä¿ç•™æˆ¿é—´å¸ƒå±€çš„å‡ ä½•ç»“æ„ã€‚360-GS æ”¯æŒå®æ—¶æ¼«æ¸¸ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ä¸ºæ–°é¢–è§†è§’åˆæˆæä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ 360â—¦é«˜æ–¯ splatting ç®—æ³•ï¼Œè¯¥ç®—æ³•å°† splatting åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šåœ¨å•ä½çƒä½“çš„åˆ‡å¹³é¢ä¸Š splatting å’Œæ˜ å°„åˆ°çƒé¢ã€‚</li><li>åˆ©ç”¨å…¨æ™¯å›¾åƒä¸­çš„å¸ƒå±€å…ˆéªŒæ¥æŒ‡å¯¼ 3D é«˜æ–¯çš„ä¼˜åŒ–ï¼Œè¿™äº›å…ˆéªŒæ˜“äºè·å–ï¼Œå¹¶ä¸”åŒ…å«æœ‰å…³å®¤å†…åœºæ™¯çš„å¼ºç»“æ„ä¿¡æ¯ã€‚</li><li>å¼•å…¥äº†å¸ƒå±€å¼•å¯¼æ­£åˆ™åŒ–æ¥å‡è½»æµ®åŠ¨é—®é¢˜å¹¶ä¿ç•™æˆ¿é—´å¸ƒå±€çš„å‡ ä½•ç»“æ„ã€‚æ€§èƒ½ï¼š</li><li>ä¸ 3D-GS ç›¸æ¯”ï¼Œ360-GS åœ¨å‡†ç¡®æ€§ã€ç»†èŠ‚å’Œé²æ£’æ€§æ–¹é¢å‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li><li>360-GS åœ¨å®¤å†…åœºæ™¯æ¸²æŸ“ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å…¨æ™¯å›¾åƒã€‚</li><li>éœ€è¦ä¼˜åŒ– 3D é«˜æ–¯çš„å‚æ•°ã€‚</li><li>éœ€è¦å°† splattered çš„é«˜æ–¯ä½“ä»å‰åˆ°åè¿›è¡Œ alpha æ··åˆä»¥ç”Ÿæˆå…¨æ™¯æ¸²æŸ“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-38c0a2fd61f19043e9f57d34dec4a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe5198d06678b334414f192b0c83aa8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5349fc8a22abb33ba9a2c7388b0a826.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1d8e3eade9a3d6331e76dbab98e15a68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffe9d7162c03cd614dfd0b6e7509adbd.jpg" align="middle"></details><h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p><p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussiansâ€™ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a> </p><p><a href="http://arxiv.org/abs/2401.05925v3">PDF</a> 9 pages, 8 figures, correct writing details</p><p><strong>æ‘˜è¦</strong><br>ç»“åˆç‚¹äº‘ä¸æ˜¾å¼åæŠ•å°„çš„ç‰¹å¾èåˆç½‘ç»œï¼Œå®ç°ç´§å‡‘è€Œå¿«é€Ÿçš„ 3D é«˜æ–¯æ··åˆåˆ†å‰²ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>æå‡ºä¸€ç§ç”¨äºç´§å‡‘ã€å¿«é€Ÿä¸”ä»…ä»¥RGBå›¾åƒä½œä¸ºè¾“å…¥çš„3Dåœºæ™¯ä¸€è‡´æ€§åˆ†å‰²æ–¹æ³•ï¼šç´§å‡‘å¿«é€Ÿåˆ†å‰²3Dé«˜æ–¯ï¼ˆCoSSegGaussiansï¼‰ã€‚</li><li>ç°æœ‰çš„åŸºäºé«˜æ–¯ä½“ç´ çš„åˆ†å‰²æ–¹æ³•åœ¨è¿›è¡Œé›¶é•œå¤´åˆ†å‰²æ—¶éš¾ä»¥ç”Ÿæˆç´§å‡‘çš„æ©æ¨¡ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºå®ƒä»¬å°†å¯å­¦ä¹ çš„å‚æ•°ç›´æ¥åˆ†é…ç»™æ¯ä¸ªé«˜æ–¯ä½“ç´ ï¼Œä»è€Œå¯¼è‡´ç¼ºä¹å¯¹è·¨è§†å›¾ä¸ä¸€è‡´çš„2Dæœºå™¨ç”Ÿæˆçš„æ ‡ç­¾çš„é²æ£’æ€§ã€‚</li><li>åˆ©ç”¨åŒç‰¹å¾èåˆç½‘ç»œä½œä¸ºé«˜æ–¯ä½“ç´ çš„åˆ†å‰²å­—æ®µæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li><li>é¦–å…ˆåœ¨RGBç›‘ç£ä¸‹ä¼˜åŒ–3Dé«˜æ–¯ä½“ç´ ã€‚</li><li>ç„¶åé€šè¿‡æ˜¾å¼åæŠ•å½±åº”ç”¨ä»å›¾åƒä¸­æå–çš„DINOç‰¹å¾ï¼Œå¹¶ç»“åˆæ¥è‡ªæœ‰æ•ˆç‚¹äº‘å¤„ç†ç½‘ç»œçš„ç©ºé—´ç‰¹å¾ã€‚</li><li>åˆ©ç”¨ç‰¹å¾èšåˆåœ¨å…¨å±€åˆ°å±€éƒ¨çš„ç­–ç•¥ä¸­èåˆè¿™äº›ç‰¹å¾ä»¥å®ç°ç´§å‡‘çš„åˆ†å‰²ç‰¹å¾ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸NeRFä¸ºåŸºç¡€çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²å’Œå…¨æ™¯é›¶é•œå¤´åˆ†å‰²ä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´å°‘äº10%ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç´§å‡‘è€Œå¿«é€Ÿçš„åœºæ™¯åˆ†å‰² 3D é«˜æ–¯ä½“ä¸åŒé‡ç‰¹å¾èåˆ</li><li>ä½œè€…ï¼šDou Bin, Zhang Tianyu, Ma Yongjia, Wang Zhaohui, Yuan Zejian</li><li>å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½ä¸æœºå™¨äººå­¦é™¢</li><li>å…³é”®è¯ï¼š3D åœºæ™¯åˆ†å‰²ã€ç¥ç»è¾å°„åœºã€é«˜æ–¯ä½“ã€åŒé‡ç‰¹å¾èåˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.05925ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¥ç»æ¸²æŸ“é¢†åŸŸã€‚ç¥ç»è¾å°„åœº (NeRF) åŠå…¶åç»­æ–¹æ³•æ¨åŠ¨äº†ç¥ç»åœºæ™¯è¡¨ç¤ºçš„å‘å±•ï¼Œåœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾ç€çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäº NeRF çš„åˆ†å‰²æ–¹æ³•ä¾èµ–äºè€—æ—¶çš„ç¥ç»åœºæ™¯ä¼˜åŒ–ã€‚è™½ç„¶æœ€è¿‘çš„ 3D é«˜æ–¯ä½“ splatting æ˜¾ç€æé«˜äº†é€Ÿåº¦ï¼Œä½†ç°æœ‰çš„åŸºäºé«˜æ–¯ä½“çš„åˆ†å‰²æ–¹æ³•éš¾ä»¥äº§ç”Ÿç´§å‡‘çš„æ©æ¨¡ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬åˆ†å‰²ä¸­ã€‚è¿™ä¸ªé—®é¢˜å¯èƒ½æºäºå…¶ç›´æ¥å°†å¯å­¦ä¹ å‚æ•°åˆ†é…ç»™æ¯ä¸ªé«˜æ–¯ä½“ï¼Œå¯¼è‡´å¯¹è·¨è§†å›¾ä¸ä¸€è‡´çš„ 2D æœºå™¨ç”Ÿæˆçš„æ ‡ç­¾ç¼ºä¹é²æ£’æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç´§å‡‘è€Œå¿«é€Ÿçš„åœºæ™¯åˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸º CoSSegGaussiansï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨ RGB å›¾åƒè¾“å…¥å³å¯å®ç°ç´§å‡‘çš„ 3D ä¸€è‡´åœºæ™¯åˆ†å‰²ï¼Œä¸”æ¸²æŸ“é€Ÿåº¦å¿«ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ RGB ç›‘ç£ä¸‹ä¼˜åŒ– 3D é«˜æ–¯ä½“ã€‚åœ¨é«˜æ–¯ä½“å®šä½ä¹‹åï¼Œé€šè¿‡æ˜¾å¼åæŠ•å½±åº”ç”¨ä»å›¾åƒä¸­æå–çš„ DINO ç‰¹å¾ï¼Œç„¶åå°†å…¶ä¸æ¥è‡ªé«˜æ•ˆç‚¹äº‘å¤„ç†ç½‘ç»œçš„ç©ºé—´ç‰¹å¾ç»“åˆã€‚åˆ©ç”¨ç‰¹å¾èšåˆåœ¨å…¨å±€åˆ°å±€éƒ¨ç­–ç•¥ä¸­èåˆå®ƒä»¬ä»¥è·å¾—ç´§å‡‘çš„åˆ†å‰²ç‰¹å¾ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­ä¹‰å’Œå…¨æ™¯é›¶æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´ä¸åˆ°åŸºäº NeRF çš„æ–¹æ³•çš„ 10%ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é«˜æ–¯ä½“å®šä½é˜¶æ®µï¼šä½¿ç”¨ L1 å’Œ L_D-SSIM å…‰åº¦æŸå¤±æ¥ç›‘ç£é«˜æ–¯ä½“çš„å‡ ä½•ä¿¡æ¯ï¼ŒåŒ…æ‹¬è´¨å¿ƒã€åæ–¹å·®ã€ä¸é€æ˜åº¦å’Œé¢œè‰²ã€‚ï¼ˆ2ï¼‰åˆ†å‰²é˜¶æ®µï¼šå°†å¤šå°ºåº¦çš„ DINO ç‰¹å¾åæŠ•å½±åˆ°é«˜æ–¯ä½“ä¸Šï¼Œå¹¶ä¸ä»é«˜æ–¯ä½“ä¸­æå–çš„ç©ºé—´ç‰¹å¾èåˆã€‚ï¼ˆ3ï¼‰ç‰¹å¾èšåˆï¼šä½¿ç”¨å…¨å±€åˆ°å±€éƒ¨ç­–ç•¥èšåˆèåˆåçš„ç‰¹å¾ï¼Œä»¥ç”Ÿæˆç´§å‡‘çš„åˆ†å‰²ç‰¹å¾ã€‚ï¼ˆ4ï¼‰ç›‘ç£ï¼šä½¿ç”¨é›¶æ ·æœ¬åˆ†å‰²æ©æ¨¡å’Œå…³è”æ©æ¨¡æ¥ç›‘ç£åˆ†å‰²å‚æ•°ï¼Œå¹¶ä½¿ç”¨ NCE æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç´§å‡‘è€Œå¿«é€Ÿçš„åœºæ™¯åˆ†å‰²æ–¹æ³• CoSSegGaussiansï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨ RGB å›¾åƒè¾“å…¥å³å¯å®ç°ç´§å‡‘çš„ 3D ä¸€è‡´åœºæ™¯åˆ†å‰²ï¼Œä¸”æ¸²æŸ“é€Ÿåº¦å¿«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­ä¹‰å’Œå…¨æ™¯é›¶æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´ä¸åˆ°åŸºäº NeRF çš„æ–¹æ³•çš„ 10%ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ç´§å‡‘è€Œå¿«é€Ÿçš„åœºæ™¯åˆ†å‰²æ–¹æ³• CoSSegGaussiansï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨ RGB å›¾åƒè¾“å…¥å³å¯å®ç°ç´§å‡‘çš„ 3D ä¸€è‡´åœºæ™¯åˆ†å‰²ï¼Œä¸”æ¸²æŸ“é€Ÿåº¦å¿«ã€‚</li><li>æå‡ºäº†ä¸€ç§åŒé‡ç‰¹å¾èåˆç½‘ç»œä½œä¸ºåˆ†å‰²åœºï¼Œè¯¥ç½‘ç»œèšåˆäº† DINO å’Œç©ºé—´ç‰¹å¾ç”¨äºåˆ†å‰²ã€‚</li><li>å°†å¤šå°ºåº¦çš„ DINO ç‰¹å¾ä»å›¾åƒåæŠ•å½±åˆ°å®šä½çš„ 3D é«˜æ–¯ä½“ä¸Šï¼Œå¹¶è¿›ä¸€æ­¥ä¸é«˜æ–¯ä½“çš„ç©ºé—´ä¿¡æ¯ç›¸ç»“åˆã€‚</li><li>åº”ç”¨å…¨å±€åˆ°å±€éƒ¨èšåˆæ¨¡å—ç”Ÿæˆç´§å‡‘çš„åˆ†å‰²é€»è¾‘ã€‚æ€§èƒ½ï¼š</li><li>åœ¨è¯­ä¹‰å’Œå…¨æ™¯é›¶æ ·æœ¬åˆ†å‰²ä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li><li>æ¨ç†æ—¶é—´ä¸åˆ°åŸºäº NeRF çš„æ–¹æ³•çš„ 10%ã€‚å·¥ä½œé‡ï¼š</li><li>ä½¿ç”¨äº†å¤§é‡çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚</li><li>ç®—æ³•çš„å®ç°å’Œè®­ç»ƒè¿‡ç¨‹è¾ƒä¸ºå¤æ‚ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ecce62ef2d2a0a0c5d6577de6d7cb33f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dff94133ac5b0802b5de3fb9550eff1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e96a03193e246ab9e77a3dd6aa18e239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f381d5614322d380f003e54e659eb10.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb6b0eeec85fc1d0f2cd12928b40918f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-02  360-GS Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/</id>
    <published>2024-02-02T14:16:22.000Z</published>
    <updated>2024-02-02T14:16:22.180Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-02-02-æ›´æ–°"><a href="#2024-02-02-æ›´æ–°" class="headerlink" title="2024-02-02 æ›´æ–°"></a>2024-02-02 æ›´æ–°</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the sceneâ€™s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨æ·±åº¦ä¿¡æ¯å’Œæ‰©æ•£æ¨¡å‹ï¼ŒViCA-NeRF å®ç°äº†å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¯ä»¥é«˜æ•ˆåœ°ç¼–è¾‘ 3D åœºæ™¯ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ViCA-NeRF æ˜¯ä¸€ç§åˆ©ç”¨æ·±åº¦ä¿¡æ¯å’Œæ‰©æ•£æ¨¡å‹æ¥å®ç°å¤šè§†å›¾ä¸€è‡´æ€§çš„ 3D ç¼–è¾‘æ–¹æ³•ã€‚</li><li>ViCA-NeRF åœ¨ NeRF å»ºæ¨¡çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨æ·±åº¦ä¿¡æ¯æ¨æ–­ä¸åŒè§†è§’çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œä»¥å®ç°å‡ ä½•æ­£åˆ™åŒ–ã€‚</li><li>ViCA-NeRF åˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹å¯¹ç¼–è¾‘å›¾åƒå’Œæœªç¼–è¾‘å›¾åƒçš„æ½œåœ¨ç¼–ç è¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°å­¦ä¹ æ­£åˆ™åŒ–ã€‚</li><li>ViCA-NeRF ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šç¬¬ä¸€é˜¶æ®µèåˆæ¥è‡ªä¸åŒè§†è§’çš„ç¼–è¾‘ï¼Œåˆ›å»ºåˆæ­¥çš„ 3D ç¼–è¾‘ï¼›ç¬¬äºŒé˜¶æ®µå¯¹ NeRF è¿›è¡Œè®­ç»ƒï¼Œä»¥è¿›ä¸€æ­¥ç»†åŒ–åœºæ™¯å¤–è§‚ã€‚</li><li>ViCA-NeRF æ¯”ç°æœ‰æ–¹æ³•æä¾›äº†æ›´çµæ´»ã€æ›´é«˜æ•ˆï¼ˆé€Ÿåº¦æé«˜ 3 å€ï¼‰çš„ç¼–è¾‘ï¼Œå¹¶å…·æœ‰æ›´é«˜çš„å±‚æ¬¡ä¸€è‡´æ€§å’Œç»†èŠ‚ã€‚</li><li>ViCA-NeRF çš„ä»£ç å·²å…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šViCA-NeRFï¼šåŸºäºè§†å›¾ä¸€è‡´æ€§çš„ç¥ç»è¾å°„åœº 3D ç¼–è¾‘</li><li>ä½œè€…ï¼šJiahua Dong, Yu-Xiong Wang</li><li>å•ä½ï¼šä¼Šåˆ©è¯ºä¼Šå¤§å­¦å„å·´çº³-é¦™æ§Ÿåˆ†æ ¡</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D ç¼–è¾‘ã€æ–‡æœ¬æŒ‡ä»¤ã€è§†å›¾ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00864   Github é“¾æ¥ï¼šNone</li><li><p>æ€»ç»“ï¼š(1)ï¼šéšç€ç¥ç»è¾å°„åœº (NeRF) ç­‰ 3D é‡å»ºæŠ€æœ¯çš„è¿›æ­¥ï¼Œæ”¶é›†çœŸå®ä¸–ç•Œ 3D åœºæ™¯å˜å¾—æ›´åŠ ä¾¿æ·ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ 3D åœºæ™¯ç¼–è¾‘æ–¹é¢è¿˜å­˜åœ¨è¯¸å¤šå±€é™ã€‚(2)ï¼šä»¥å¾€æ–¹æ³•é€šå¸¸ä½¿ç”¨éšå¼ç¥ç»è¾å°„åœºè¿›è¡Œå»ºæ¨¡ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒè§†å›¾ä¹‹é—´ç¼–è¾‘ä¿¡æ¯ä¼ æ’­çš„æ˜¾å¼çº¦æŸï¼Œå¯¼è‡´ç¼–è¾‘ç»“æœå¯èƒ½å‡ºç°è§†å›¾ä¸ä¸€è‡´çš„é—®é¢˜ã€‚(3)ï¼šæœ¬æ–‡æå‡º ViCA-NeRFï¼Œä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„ 3D ç¼–è¾‘æ–¹æ³•ã€‚ViCA-NeRF åˆ©ç”¨å‡ ä½•å’Œå­¦ä¹ æ­£åˆ™åŒ–ä¸¤ç§ç­–ç•¥æ¥ç¡®ä¿ä¸åŒè§†å›¾ä¹‹é—´çš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚å‡ ä½•æ­£åˆ™åŒ–åˆ©ç”¨ NeRF æå–çš„æ·±åº¦ä¿¡æ¯å»ºç«‹ä¸åŒè§†å›¾ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œå­¦ä¹ æ­£åˆ™åŒ–åˆ™å¯¹ç¼–è¾‘å›¾åƒå’Œæœªç¼–è¾‘å›¾åƒåœ¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„æ½œåœ¨ç¼–ç è¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°å…³é”®è§†å›¾çš„ç¼–è¾‘å¹¶å°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªåœºæ™¯ã€‚(4)ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒViCA-NeRF èƒ½å¤Ÿæä¾›æ›´åŠ çµæ´»ã€é«˜æ•ˆï¼ˆé€Ÿåº¦æé«˜ 3 å€ï¼‰ã€ä¸€è‡´æ€§å’Œç»†èŠ‚æ›´ä½³çš„ç¼–è¾‘æ•ˆæœã€‚</p></li><li><p>Methodsï¼š(1)ï¼šViCA-NeRFæ˜¯ä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„3Dç¼–è¾‘æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å‡ ä½•å’Œå­¦ä¹ æ­£åˆ™åŒ–ä¸¤ç§ç­–ç•¥æ¥ç¡®ä¿ä¸åŒè§†å›¾ä¹‹é—´çš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚(2)ï¼šå‡ ä½•æ­£åˆ™åŒ–åˆ©ç”¨NeRFæå–çš„æ·±åº¦ä¿¡æ¯å»ºç«‹ä¸åŒè§†å›¾ä¹‹é—´çš„å›¾åƒå¯¹åº”å…³ç³»ï¼Œä»è€Œå°†ç¼–è¾‘ä¿¡æ¯ä»å…³é”®è§†å›¾ä¼ æ’­åˆ°æ•´ä¸ªåœºæ™¯ã€‚(3)ï¼šå­¦ä¹ æ­£åˆ™åŒ–å¯¹ç¼–è¾‘å›¾åƒå’Œæœªç¼–è¾‘å›¾åƒåœ¨2Dæ‰©æ•£æ¨¡å‹ä¸­çš„æ½œåœ¨ç¼–ç è¿›è¡Œå¯¹é½ï¼Œä»è€Œå®ç°å…³é”®è§†å›¾çš„ç¼–è¾‘å¹¶å°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªåœºæ™¯ã€‚(4)ï¼šViCA-NeRFèƒ½å¤Ÿæä¾›æ›´åŠ çµæ´»ã€é«˜æ•ˆï¼ˆé€Ÿåº¦æé«˜3å€ï¼‰ã€ä¸€è‡´æ€§å’Œç»†èŠ‚æ›´ä½³çš„ç¼–è¾‘æ•ˆæœã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„ 3D ç¼–è¾‘æ¡†æ¶ ViCA-NeRFï¼Œè¯¥æ¡†æ¶å¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤é«˜æ•ˆåœ°ç¼–è¾‘ NeRFã€‚é™¤äº†äººç±»é£æ ¼åŒ–å’Œå¤©æ°”å˜åŒ–ç­‰ç®€å•ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚â€œæ·»åŠ ä¸€äº›èŠ±æœµâ€å’Œç¼–è¾‘é«˜åº¦è¯¦ç»†çš„çº¹ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åœºæ™¯å’Œæ–‡æœ¬æç¤ºä¸Šä¼˜äºå‡ ä¸ªåŸºçº¿ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†ç»§ç»­æé«˜ 3D ç¼–è¾‘çš„å¯æ§æ€§å’ŒçœŸå®æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºè§†å›¾ä¸€è‡´æ€§çš„ 3D ç¼–è¾‘æ¡†æ¶ ViCA-NeRFï¼Œè¯¥æ¡†æ¶å¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤é«˜æ•ˆåœ°ç¼–è¾‘ NeRFã€‚</li><li>åˆ©ç”¨å‡ ä½•æ­£åˆ™åŒ–å’Œå­¦ä¹ æ­£åˆ™åŒ–ä¸¤ç§ç­–ç•¥æ¥ç¡®ä¿ä¸åŒè§†å›¾ä¹‹é—´çš„ç¼–è¾‘ä¸€è‡´æ€§ã€‚</li><li>æ”¯æŒä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚â€œæ·»åŠ ä¸€äº›èŠ±æœµâ€å’Œç¼–è¾‘é«˜åº¦è¯¦ç»†çš„çº¹ç†ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§åœºæ™¯å’Œæ–‡æœ¬æç¤ºä¸Šä¼˜äºå‡ ä¸ªåŸºçº¿ã€‚</li><li>ç¼–è¾‘æ•ˆç‡é«˜ï¼Œé€Ÿåº¦æé«˜ 3 å€ã€‚</li><li>ç¼–è¾‘ç»“æœä¸€è‡´æ€§å¥½ï¼Œç»†èŠ‚ä¸°å¯Œã€‚å·¥ä½œé‡ï¼š</li><li>å®ç°å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„æŠ€æœ¯æ°´å¹³ã€‚</li><li>è®­ç»ƒæ—¶é—´é•¿ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details><h2 id="AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning"><a href="#AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning" class="headerlink" title="AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning"></a>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning</h2><p><strong>Authors:Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at <a href="https://github.com/G-U-N/AnimateLCM">https://github.com/G-U-N/AnimateLCM</a>. </p><p><a href="http://arxiv.org/abs/2402.00769v1">PDF</a> Project Page: <a href="https://animatelcm.github.io/">https://animatelcm.github.io/</a></p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åŠ¨ç”»LCMï¼ˆAnimateLCMï¼‰ï¼šé€šè¿‡åˆ†ç¦»å›¾åƒç”Ÿæˆå…ˆéªŒå’Œè¿åŠ¨ç”Ÿæˆå…ˆéªŒï¼Œå®ç°å¿«é€Ÿé«˜æ•ˆçš„é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹è§†é¢‘ç”Ÿæˆç”±äºè¿­ä»£å»å™ªè¿‡ç¨‹è®¡ç®—é‡å¤§å’Œè€—æ—¶ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚</li><li>å—Consistency Model (CM)å’ŒLatent Consistency Model (LCM)çš„å¯å‘ï¼Œæå‡ºAnimateLCMï¼Œå¯åœ¨æœ€å°‘æ­¥éª¤å†…ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘ã€‚</li><li>æå‡ºäº†ä¸€ç§è§£è€¦ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå°†å›¾åƒç”Ÿæˆå…ˆéªŒå’Œè¿åŠ¨ç”Ÿæˆå…ˆéªŒçš„å­¦ä¹ è§£è€¦ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œç”Ÿæˆè§†è§‰è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå°†ç°æœ‰çš„é€‚é…å™¨é€‚é…åˆ°è’¸é¦åçš„æ–‡æœ¬æ¡ä»¶è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ï¼Œæˆ–ä»å¤´å¼€å§‹è®­ç»ƒé€‚é…å™¨ï¼Œè€Œä¸ä¼šæŸå®³é‡‡æ ·é€Ÿåº¦ã€‚</li><li>åœ¨å›¾åƒæ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå¸ƒå±€æ¡ä»¶è§†é¢‘ç”Ÿæˆä¸­éªŒè¯äº†æ‰€æå‡ºçš„ç­–ç•¥ï¼Œå‡å–å¾—äº†æœ€ä¼˜ç»“æœã€‚</li><li>å®éªŒç»“æœéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæƒé‡å°†å…¬å¼€ã€‚æ›´å¤šè¯¦æƒ…è¯·è§ <a href="https://github.com/G-U-N/AnimateLCMã€‚">https://github.com/G-U-N/AnimateLCMã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAnimateLCMï¼šåŠ é€Ÿä¸ªæ€§åŒ–æ‰©æ•£æ¨¡å‹å’Œé€‚é…å™¨çš„åŠ¨ç”»åˆ¶ä½œï¼Œå…·æœ‰å»è€¦åˆä¸€è‡´æ€§å­¦ä¹ </li><li>ä½œè€…ï¼šFu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</li><li>éš¶å±å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</li><li>å…³é”®è¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹ã€ä¸ªæ€§åŒ–å±‚ã€åŠ¨ç”»åˆ¶ä½œ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00769Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆè¿è´¯ä¸”é«˜ä¿çœŸè§†é¢‘è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿­ä»£å¼å»å™ªè¿‡ç¨‹ä½¿å…¶è®¡ç®—å¯†é›†ä¸”è€—æ—¶ï¼Œä»è€Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå—ä¸€è‡´æ€§æ¨¡å‹ (CM) çš„å¯å‘ï¼ŒCM å°†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹è’¸é¦ä»¥åŠ é€Ÿæœ€å°æ­¥é•¿çš„é‡‡æ ·ï¼Œå¹¶åœ¨æ¡ä»¶å›¾åƒç”Ÿæˆä¸ŠæˆåŠŸæ‰©å±•äº†æ½œåœ¨ä¸€è‡´æ€§æ¨¡å‹ (LCM)ã€‚ç„¶è€Œï¼Œç›´æ¥å¯¹åŸå§‹è§†é¢‘æ•°æ®é›†è¿›è¡Œä¸€è‡´æ€§å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡ä½ï¼Œç”Ÿæˆçš„è§†è§‰è´¨é‡ä¹Ÿä¸ä½³ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæå‡º AnimateLCMï¼Œå…è®¸åœ¨æœ€å°‘æ­¥é•¿å†…ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘ã€‚æå‡ºäº†ä¸€ç§å»è€¦åˆä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå°†å›¾åƒç”Ÿæˆå…ˆéªŒå’Œè¿åŠ¨ç”Ÿæˆå…ˆéªŒçš„è’¸é¦è§£è€¦ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºäº†ç”Ÿæˆè§†è§‰è´¨é‡ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æœ‰æ•ˆç­–ç•¥ï¼Œå°†ç¨³å®šæ‰©æ•£ç¤¾åŒºä¸­å³æ’å³ç”¨çš„é€‚é…å™¨ä¸è’¸é¦çš„æ–‡æœ¬æ¡ä»¶è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ç›¸ç»“åˆï¼Œæˆ–ä»å¤´å¼€å§‹è®­ç»ƒé€‚é…å™¨ï¼Œè€Œä¸ä¼šæŸå®³é‡‡æ ·é€Ÿåº¦ã€‚ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœï¼šåœ¨å›¾åƒæ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå¸ƒå±€æ¡ä»¶è§†é¢‘ç”Ÿæˆä¸­éªŒè¯äº†æ‰€æå‡ºçš„ç­–ç•¥ï¼Œå‡å–å¾—äº†æœ€ä¼˜ç»“æœã€‚å®éªŒç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§å»è€¦åˆä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå°†å›¾åƒç”Ÿæˆå…ˆéªŒå’Œè¿åŠ¨ç”Ÿæˆå…ˆéªŒçš„è’¸é¦è§£è€¦ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºäº†ç”Ÿæˆè§†è§‰è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šæå‡ºäº†ä¸€ç§æœ‰æ•ˆç­–ç•¥ï¼Œå°†ç¨³å®šæ‰©æ•£ç¤¾åŒºä¸­å³æ’å³ç”¨çš„é€‚é…å™¨ä¸è’¸é¦çš„æ–‡æœ¬æ¡ä»¶è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ç›¸ç»“åˆï¼Œæˆ–ä»å¤´å¼€å§‹è®­ç»ƒé€‚é…å™¨ï¼Œè€Œä¸ä¼šæŸå®³é‡‡æ ·é€Ÿåº¦ã€‚ï¼ˆ3ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°å°†ç©ºé—´ LoRA æƒé‡å’Œæ—¶é—´å±‚ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚ï¼ˆ4ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ— æ•™å¸ˆçš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥é€šè¿‡å•æ­¥ MCMC è¿‘ä¼¼æ¥ä¼°è®¡åˆ†æ•°ï¼Œä»è€Œæ— éœ€é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ã€‚ï¼ˆ5ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘çš„é¢„å¤„ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æå–å›¾åƒä¸Šä¸‹æ–‡å¹¶å°†å…¶èå…¥ä¸€è‡´æ€§æ¨¡å‹ä¸­ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ç”ŸæˆåŠ é€Ÿæ–¹æ³•AnimateLCMï¼Œè¯¥æ–¹æ³•é€šè¿‡è§£è€¦ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥å’Œæ•™å¸ˆæ¨¡å‹çš„é€‚åº”ç­–ç•¥ï¼Œå®ç°äº†è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆæ€§å’Œé«˜è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§è§£è€¦ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå°†å›¾åƒç”Ÿæˆå…ˆéªŒå’Œè¿åŠ¨ç”Ÿæˆå…ˆéªŒçš„è’¸é¦è§£è€¦ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºäº†ç”Ÿæˆè§†è§‰è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æœ‰æ•ˆç­–ç•¥ï¼Œå°†ç¨³å®šæ‰©æ•£ç¤¾åŒºä¸­å³æ’å³ç”¨çš„é€‚é…å™¨ä¸è’¸é¦çš„æ–‡æœ¬æ¡ä»¶è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹ç›¸ç»“åˆï¼Œæˆ–ä»å¤´å¼€å§‹è®­ç»ƒé€‚é…å™¨ï¼Œè€Œä¸ä¼šæŸå®³é‡‡æ ·é€Ÿåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°å°†ç©ºé—´LoRAæƒé‡å’Œæ—¶é—´å±‚ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li><li>æå‡ºäº†ä¸€ç§æ— æ•™å¸ˆçš„ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥é€šè¿‡å•æ­¥MCMCè¿‘ä¼¼æ¥ä¼°è®¡åˆ†æ•°ï¼Œä»è€Œæ— éœ€é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘çš„é¢„å¤„ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°æå–å›¾åƒä¸Šä¸‹æ–‡å¹¶å°†å…¶èå…¥ä¸€è‡´æ€§æ¨¡å‹ä¸­ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å›¾åƒæ¡ä»¶è§†é¢‘ç”Ÿæˆå’Œå¸ƒå±€æ¡ä»¶è§†é¢‘ç”Ÿæˆä¸­éªŒè¯äº†æ‰€æå‡ºçš„ç­–ç•¥ï¼Œå‡å–å¾—äº†æœ€ä¼˜ç»“æœã€‚</li><li>å®éªŒç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦å¯¹è§†é¢‘æ‰©æ•£æ¨¡å‹ã€ä¸€è‡´æ€§æ¨¡å‹å’Œé€‚é…å™¨ç­‰å¤šä¸ªæ–¹é¢è¿›è¡Œç ”ç©¶å’Œå®ç°ã€‚</li><li>æœ¬æ–‡çš„å®éªŒéƒ¨åˆ†ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦å¯¹å¤šä¸ªæ•°æ®é›†å’Œå¤šä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0a500cdbd8cd65da7ce9d1f829b50f0a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c83ed1cad4b7378b141c6e7abe349fbd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8e303adc03472e85d52d1d42c05fd46.jpg" align="middle"></details><h2 id="CapHuman-Capture-Your-Moments-in-Parallel-Universes"><a href="#CapHuman-Capture-Your-Moments-in-Parallel-Universes" class="headerlink" title="CapHuman: Capture Your Moments in Parallel Universes"></a>CapHuman: Capture Your Moments in Parallel Universes</h2><p><strong>Authors:Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</strong></p><p>We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to alignâ€ paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a>. </p><p><a href="http://arxiv.org/abs/2402.00627v1">PDF</a> Project page: <a href="https://caphuman.github.io/">https://caphuman.github.io/</a></p><p><strong>Summary</strong><br>é€šè¿‡èåˆæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ŒCapHuman å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸°å¯Œå†…å®¹è¡¨ç¤ºå’Œå¤šç§å¤´éƒ¨æ¸²æŸ“çš„ã€é«˜åº¦çœŸå®å’Œä¿ç•™èº«ä»½çš„è‚–åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CapHuman æ—¨åœ¨é€šè¿‡èåˆæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå…·æœ‰ä¸°å¯Œå†…å®¹è¡¨ç¤ºå’Œå¤šç§å¤´éƒ¨æ¸²æŸ“çš„ã€é«˜åº¦çœŸå®å’Œä¿ç•™èº«ä»½çš„è‚–åƒã€‚</li><li>CapHuman æ¡†æ¶é‡‡ç”¨â€œå…ˆç¼–ç å†å­¦ä¹ å¯¹é½â€çš„èŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶å¯¹æ–°ä¸ªä½“è¿›è¡Œé€šç”¨èº«ä»½ä¿ç•™ï¼Œè€Œæ— éœ€ç¹ççš„å¾®è°ƒã€‚</li><li>CapHuman ä½¿ç”¨ 3D é¢éƒ¨å…ˆéªŒæ¥ä¸ºæ¨¡å‹æä¾›ä»¥çµæ´»ä¸” 3D ä¸€è‡´çš„æ–¹å¼æ§åˆ¶äººå¤´çš„èƒ½åŠ›ã€‚</li><li>CapHuman èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸°å¯Œå†…å®¹è¡¨ç¤ºå’Œå¤šç§å¤´éƒ¨æ¸²æŸ“çš„ã€é«˜åº¦çœŸå®å’Œä¿ç•™èº«ä»½çš„è‚–åƒï¼Œä¼˜äºç°æœ‰çš„åŸºå‡†ã€‚</li><li>CapHuman çš„ä»£ç å’Œæ£€æŸ¥ç‚¹å°†åœ¨ <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a> ä¸Šå‘å¸ƒã€‚</li><li>CapHuman ä¸ºäººè„¸å›¾åƒåˆæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨èº«ä»½ä¿ç•™ã€å¤´éƒ¨æ§åˆ¶å’Œç…§ç‰‡çœŸå®æ„Ÿæ–¹é¢å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚</li><li>CapHuman å¯ä»¥ä½œä¸ºä¸€ç§æ–°çš„å·¥å…·ï¼Œç”¨äºå„ç§åº”ç”¨ï¼Œä¾‹å¦‚è™šæ‹Ÿå½¢è±¡åˆ›å»ºã€æ¸¸æˆè§’è‰²è®¾è®¡å’Œç”µå½±è§†è§‰ç‰¹æ•ˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šCapHumanï¼šæ•æ‰å¹³è¡Œå®‡å®™ä¸­çš„ç¬é—´</li><li>ä½œè€…ï¼šYilun Xu, Wenbo Li, Yajie Zhao, Yifan Jiang, Chen Change Loy</li><li>å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li><li>å…³é”®è¯ï¼šäººè„¸å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€èº«ä»½ä¿æŒã€å¤´éƒ¨æ§åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00627Github ä»£ç é“¾æ¥ï¼šæš‚æ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šäººè„¸å›¾åƒç”Ÿæˆæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿç†è§£äººç±»ç¤¾ä¼šå’Œä¸–ç•Œï¼Œå¹¶èƒ½å¤Ÿä»¥é€¼çœŸå’Œä¸€è‡´çš„æ–¹å¼ç”Ÿæˆäººè„¸å›¾åƒã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œå¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œä¸€è‡´æ€§æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CapHuman çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨â€œç¼–ç ç„¶åå­¦ä¹ å¯¹é½â€çš„èŒƒå¼ï¼Œå¯ä»¥å¯¹æ–°ä¸ªä½“è¿›è¡Œèº«ä»½ä¿æŒï¼Œè€Œæ— éœ€åœ¨æ¨ç†æ—¶è¿›è¡Œç¹ççš„è°ƒæ•´ã€‚CapHuman å¯¹èº«ä»½ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œç„¶åå­¦ä¹ å°†è¿™äº›ç‰¹å¾å¯¹é½åˆ°æ½œåœ¨ç©ºé—´ä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ª 3D é¢éƒ¨å…ˆéªŒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥çµæ´»å’Œ 3D ä¸€è‡´çš„æ–¹å¼æ§åˆ¶äººåƒå¤´éƒ¨ã€‚(4) å®éªŒç»“æœï¼šå¹¿æ³›çš„å®šæ€§å’Œå®šé‡åˆ†æè¡¨æ˜ï¼ŒCapHuman å¯ä»¥ç”Ÿæˆå…·æœ‰è‰¯å¥½èº«ä»½ä¿æŒæ€§ã€é€¼çœŸå’Œé«˜ä¿çœŸçš„äººåƒï¼Œå…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºå’Œå„ç§å¤´éƒ¨å‘ˆç°æ–¹å¼ï¼Œä¼˜äºå·²æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼š(1) <strong>ç¼–ç ç„¶åå­¦ä¹ å¯¹é½èŒƒå¼</strong>ï¼šCapHuman é‡‡ç”¨â€œç¼–ç ç„¶åå­¦ä¹ å¯¹é½â€çš„èŒƒå¼ï¼Œå°†äººè„¸å›¾åƒç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼šé¦–å…ˆï¼Œå°†äººè„¸å›¾åƒç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„è¡¨ç¤ºï¼›ç„¶åï¼Œå­¦ä¹ å°†è¿™ä¸ªè¡¨ç¤ºå¯¹é½åˆ°æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»¥ä¾¿ç”Ÿæˆæ–°çš„å›¾åƒã€‚(2) <strong>èº«ä»½ç‰¹å¾ç¼–ç </strong>ï¼šCapHuman ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„äººè„¸è¯†åˆ«æ¨¡å‹æ¥æå–äººè„¸å›¾åƒçš„èº«ä»½ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ç”¨äºå¯¹é½äººè„¸å›¾åƒï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒå…·æœ‰ä¸è¾“å…¥å›¾åƒç›¸åŒçš„äººç‰©èº«ä»½ã€‚(3) <strong>æ½œåœ¨ç©ºé—´å­¦ä¹ </strong>ï¼šCapHuman ä½¿ç”¨ä¸€ä¸ªç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æ¥å­¦ä¹ æ½œåœ¨ç©ºé—´ã€‚GAN ç”±ä¸€ä¸ªç”Ÿæˆå™¨å’Œä¸€ä¸ªåˆ¤åˆ«å™¨ç»„æˆã€‚ç”Ÿæˆå™¨å°†ç¼–ç çš„äººè„¸ç‰¹å¾æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œåˆ¤åˆ«å™¨åˆ™è¯•å›¾åŒºåˆ†ç”Ÿæˆçš„å›¾åƒå’ŒçœŸå®å›¾åƒã€‚(4) <strong>3D é¢éƒ¨å…ˆéªŒ</strong>ï¼šCapHuman å¼•å…¥äº†ä¸€ä¸ª 3D é¢éƒ¨å…ˆéªŒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥çµæ´»å’Œ 3D ä¸€è‡´çš„æ–¹å¼æ§åˆ¶äººåƒå¤´éƒ¨ã€‚3D é¢éƒ¨å…ˆéªŒæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„ 3D äººè„¸æ¨¡å‹ï¼Œå®ƒå¯ä»¥æä¾›äººè„¸çš„å½¢çŠ¶ã€çº¹ç†å’Œå§¿åŠ¿ä¿¡æ¯ã€‚(5) <strong>å¤´éƒ¨æ§åˆ¶</strong>ï¼šCapHuman ä½¿ç”¨ä¸€ä¸ªå¤´éƒ¨æ§åˆ¶æ¨¡å—æ¥æ§åˆ¶ç”Ÿæˆçš„äººåƒå¤´éƒ¨çš„å§¿åŠ¿ã€‚å¤´éƒ¨æ§åˆ¶æ¨¡å—æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒå°†æ½œåœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºæ˜ å°„åˆ°ä¸€ä¸ªå¤´éƒ¨å§¿åŠ¿å‘é‡ã€‚è¿™ä¸ªå¤´éƒ¨å§¿åŠ¿å‘é‡ç”¨äºæ§åˆ¶ç”Ÿæˆçš„äººåƒå¤´éƒ¨çš„å§¿åŠ¿ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šCapHuman æå‡ºäº†ä¸€ç§åŸºäºå¼ºå¤§çš„é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¯æ¨å¹¿èº«ä»½ä¿æŒå’Œç»†ç²’åº¦å¤´éƒ¨æ§åˆ¶ä»¥äººä¸ºä¸­å¿ƒå›¾åƒåˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨â€œç¼–ç ç„¶åå­¦ä¹ å¯¹é½â€èŒƒå¼ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯å®ç°å¯æ¨å¹¿çš„èº«ä»½ä¿æŒèƒ½åŠ›ã€‚é€šè¿‡ç»“åˆ 3D é¢éƒ¨è¡¨ç¤ºï¼Œå®ƒèµ‹äºˆé¢„è®­ç»ƒæ¨¡å‹çµæ´»ä¸”ç»†ç²’åº¦çš„å¤´éƒ¨æ§åˆ¶ã€‚ç»™å®šä¸€å¼ å‚è€ƒäººè„¸å›¾åƒï¼ŒCapHuman å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒå¤´éƒ¨ä½ç½®ã€å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…çš„èº«ä»½ä¿æŒã€é«˜ä¿çœŸå’Œé€¼çœŸçš„çœŸäººè‚–åƒï¼Œé€‚ç”¨äºä¸åŒçš„åœºæ™¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é€šç”¨èº«ä»½ä¿æŒå’Œç»†ç²’åº¦å¤´éƒ¨æ§åˆ¶æ¡†æ¶ã€‚</li><li>é‡‡ç”¨â€œç¼–ç ç„¶åå­¦ä¹ å¯¹é½â€èŒƒå¼ï¼Œæ— éœ€è¿›ä¸€æ­¥å¾®è°ƒå³å¯å®ç°å¯æ¨å¹¿çš„èº«ä»½ä¿æŒèƒ½åŠ›ã€‚</li><li>å¼•å…¥ 3D é¢éƒ¨è¡¨ç¤ºï¼Œèµ‹äºˆé¢„è®­ç»ƒæ¨¡å‹çµæ´»ä¸”ç»†ç²’åº¦çš„å¤´éƒ¨æ§åˆ¶ã€‚</li><li>æå‡ºäº†ä¸€ç§å¤´éƒ¨æ§åˆ¶æ¨¡å—ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆçš„äººåƒå¤´éƒ¨çš„å§¿åŠ¿ã€‚æ€§èƒ½ï¼š</li><li>CapHuman å¯ä»¥ç”Ÿæˆå…·æœ‰è‰¯å¥½èº«ä»½ä¿æŒæ€§ã€é€¼çœŸå’Œé«˜ä¿çœŸçš„äººåƒï¼Œå…·æœ‰ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºå’Œå„ç§å¤´éƒ¨å‘ˆç°æ–¹å¼ã€‚</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šæ€§å’Œå®šé‡åˆ†æè¡¨æ˜ï¼ŒCapHuman ä¼˜äºå·²æœ‰çš„åŸºå‡†æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>CapHuman çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾æ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†å’Œä»»åŠ¡ã€‚</li><li>CapHuman çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹é«˜æ•ˆï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ ‡å‡† GPU ä¸Šå®Œæˆã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-c52c4014e9bcf0ad466bef3b776ce749.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec30884252e67ce782b09b5a6b368e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6bf56f9b1649b16183af2aa8676dc283.jpg" align="middle"></details><h2 id="LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition"><a href="#LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition" class="headerlink" title="LRDif: Diffusion Models for Under-Display Camera Emotion Recognition"></a>LRDif: Diffusion Models for Under-Display Camera Emotion Recognition</h2><p><strong>Authors:Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</strong></p><p>This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDCâ€™s image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field. </p><p><a href="http://arxiv.org/abs/2402.00250v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>UDC ç¯å¢ƒä¸‹çš„å™ªå£°å’Œå¤±çœŸé—®é¢˜é€šè¿‡ LRDif å¾—åˆ°æœ‰æ•ˆè§£å†³ï¼Œåœ¨ FER åº”ç”¨é¢†åŸŸå±•ç¤ºå‡ºå¼ºå¤§èƒ½åŠ›ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>LRDif æ˜¯ä¸€ç§ä¸“ä¸ºåœ¨å±ä¸‹æ‘„åƒå¤´ (UDC) èƒŒæ™¯ä¸‹äººè„¸è¡¨æƒ…è¯†åˆ« (FER) è®¾è®¡çš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ã€‚</li><li>LRDif é‡‡ç”¨äº†åŒ…å«æµ“ç¼©é¢„æå–ç½‘ç»œ (FPEN) å’Œæ•æ· Transformer ç½‘ç»œ (UDCformer) çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥èƒ½æœ‰æ•ˆåœ°ä» UDC å›¾åƒä¸­è¯†åˆ«å‡ºæƒ…æ„Ÿæ ‡ç­¾ã€‚</li><li>LRDif å°†æ¼«æ•£æ¨¡å‹ (DM) çš„é²æ£’åˆ†å¸ƒæ˜ å°„åŠŸèƒ½ä¸ Transformer çš„ç©ºé—´ä¾èµ–å…³ç³»å»ºæ¨¡èƒ½åŠ›ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å…‹æœäº† UDC ç¯å¢ƒä¸­å›ºæœ‰çš„å™ªå£°å’Œå¤±çœŸéšœç¢ã€‚</li><li>LRDif åœ¨ RAF-DBã€KDEF å’Œ FERPlus ç­‰æ ‡å‡† FER æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œå®ƒå…·æœ‰å…ˆè¿›çš„æ€§èƒ½ï¼Œçªå‡ºäº†å…¶åœ¨ FER åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li><li>è¿™é¡¹å·¥ä½œä¸ä»…é€šè¿‡åº”å¯¹ FER ä¸­çš„ UDC æŒ‘æˆ˜å¡«è¡¥äº†æ–‡çŒ®ä¸­çš„ç©ºç™½ï¼Œè¿˜ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šLRDifï¼šç”¨äºå±ä¸‹æ‘„åƒå¤´æƒ…ç»ªè¯†åˆ«çš„æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šZhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</li><li>å•ä½ï¼šæ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦è®¡ç®—æœºå­¦é™¢</li><li>å…³é”®è¯ï¼šå±ä¸‹æ‘„åƒå¤´ã€æƒ…ç»ªè¯†åˆ«ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2402.00250    Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€å±ä¸‹æ‘„åƒå¤´æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œåœ¨å±ä¸‹æ‘„åƒå¤´ç¯å¢ƒä¸‹è¿›è¡Œæƒ…ç»ªè¯†åˆ«æˆä¸ºä¸€ä¸ªæ–°çš„ç ”ç©¶çƒ­ç‚¹ã€‚ç„¶è€Œï¼Œå±ä¸‹æ‘„åƒå¤´å›¾åƒè´¨é‡è¾ƒå·®ï¼Œå­˜åœ¨æ¸…æ™°åº¦ä½ã€å™ªå£°å¤§ç­‰é—®é¢˜ï¼Œç»™æƒ…ç»ªè¯†åˆ«å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€çš„æƒ…ç»ªè¯†åˆ«æ–¹æ³•ä¸»è¦é’ˆå¯¹ä¼ ç»Ÿæ‘„åƒå¤´é‡‡é›†çš„å›¾åƒï¼Œæ— æ³•å¾ˆå¥½åœ°å¤„ç†å±ä¸‹æ‘„åƒå¤´å›¾åƒã€‚è¿™äº›æ–¹æ³•åœ¨å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸Šå¾€å¾€ä¼šå‡ºç°ç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æƒ…ç»ªè¯†åˆ«æ–¹æ³•LRDifï¼Œè¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–ç½‘ç»œFPENæå–å›¾åƒç‰¹å¾ï¼Œç„¶åä½¿ç”¨Transformerç½‘ç»œUDCformerå¯¹ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚LRDifåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§åˆ†å¸ƒæ˜ å°„èƒ½åŠ›å’ŒTransformerçš„æ—¶åºä¾èµ–å»ºæ¨¡èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸­å­˜åœ¨çš„å™ªå£°å’Œå¤±çœŸé—®é¢˜ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨RAF-DBã€KDEFå’ŒFERPlusç­‰æ ‡å‡†FERæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLRDifåœ¨å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸Šçš„æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æ¨è¿›FERåº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æ•°æ®é¢„å¤„ç†ï¼šå¯¹å±ä¸‹æ‘„åƒå¤´å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å›¾åƒè£å‰ªã€ç¼©æ”¾å’Œå½’ä¸€åŒ–ç­‰æ“ä½œã€‚(2) ç‰¹å¾æå–ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–ç½‘ç»œFPENæå–å›¾åƒç‰¹å¾ã€‚FPENæ˜¯ä¸€ä¸ªåŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„ç‰¹å¾æå–å™¨ï¼Œå¯ä»¥æå–å›¾åƒä¸­å…·æœ‰åˆ¤åˆ«åŠ›çš„ç‰¹å¾ã€‚(3) ç‰¹å¾åˆ†ç±»ï¼šä½¿ç”¨Transformerç½‘ç»œUDCformerå¯¹FPENæå–çš„ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€‚UDCformeræ˜¯ä¸€ä¸ªåŸºäºTransformerçš„åˆ†ç±»å™¨ï¼Œå¯ä»¥å¯¹å›¾åƒç‰¹å¾è¿›è¡Œæ—¶åºä¾èµ–å»ºæ¨¡ï¼Œä»è€Œæé«˜åˆ†ç±»ç²¾åº¦ã€‚(4) æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨æ‰©æ•£æ¨¡å‹å¯¹UDCformerè¿›è¡Œè®­ç»ƒã€‚æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼Œä»è€Œå‡å°‘æ•°æ®ä¸­çš„å™ªå£°å’Œå¤±çœŸã€‚(5) æƒ…ç»ªè¯†åˆ«ï¼šå°†è®­ç»ƒå¥½çš„UDCformeråº”ç”¨äºå±ä¸‹æ‘„åƒå¤´å›¾åƒçš„æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ã€‚UDCformerå¯ä»¥å¯¹å›¾åƒç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œè¯†åˆ«å‡ºå›¾åƒä¸­äººç‰©çš„æƒ…ç»ªã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶LRDifï¼Œç”¨äºå±ä¸‹æ‘„åƒå¤´ç¯å¢ƒä¸‹çš„äººè„¸è¡¨æƒ…è¯†åˆ«ã€‚LRDifé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¸€ä¸ªé¢„æå–ç½‘ç»œï¼ˆFPENï¼‰å’Œä¸€ä¸ªTransformerç½‘ç»œï¼ˆUDCformerï¼‰ï¼Œå…‹æœäº†å±ä¸‹æ‘„åƒå¤´å›¾åƒé€€åŒ–çš„é—®é¢˜ã€‚è¿™äº›æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é€€åŒ–çš„å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸­æ¢å¤è¡¨æƒ…æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„LRDifæ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨ä¸‰ä¸ªå±ä¸‹æ‘„åƒå¤´äººè„¸è¡¨æƒ…æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶LRDifï¼Œç”¨äºå±ä¸‹æ‘„åƒå¤´ç¯å¢ƒä¸‹çš„äººè„¸è¡¨æƒ…è¯†åˆ«ã€‚</li><li>ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¸€ä¸ªé¢„æå–ç½‘ç»œï¼ˆFPENï¼‰å’Œä¸€ä¸ªTransformerç½‘ç»œï¼ˆUDCformerï¼‰ï¼Œæ¥å…‹æœå±ä¸‹æ‘„åƒå¤´å›¾åƒé€€åŒ–çš„é—®é¢˜ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„LRDifæ¨¡å‹åœ¨ä¸‰ä¸ªå±ä¸‹æ‘„åƒå¤´äººè„¸è¡¨æƒ…æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ€§èƒ½ï¼š</li><li>åœ¨RAF-DBã€KDEFå’ŒFERPlusç­‰æ ‡å‡†FERæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLRDifåœ¨å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸Šçš„æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡çš„å·¥ä½œé‡é€‚ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†é¢„è®­ç»ƒçš„ç‰¹å¾æå–ç½‘ç»œFPENå’ŒTransformerç½‘ç»œUDCformerï¼Œå¹¶å¯¹LRDifæ¨¡å‹è¿›è¡Œäº†ç»¼åˆå®éªŒï¼Œè¯æ˜äº†å…¶åœ¨å±ä¸‹æ‘„åƒå¤´å›¾åƒä¸Šçš„æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-dd40f8d106e7073ea6d54966262e71e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cd9d427bc731cebc6c9739681cdd0f4d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-568de78c017b3bcd7823d72ed39b1b28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca356d9bc9e3749ffe997b0eeac0f361.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-136e8eea5cfa1e09239cddd5e2aea3e9.jpg" align="middle"></details><h2 id="AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error"><a href="#AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error" class="headerlink" title="AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error"></a>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error</h2><p><strong>Authors:Jonas Ricker, Denis Lukovnikov, Asja Fischer</strong></p><p>With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. </p><p><a href="http://arxiv.org/abs/2401.17879v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ä½ç»´ç©ºé—´ä¸­çš„å»å™ªè¿‡ç¨‹ï¼Œæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå…·æœ‰ä»»æ„å†…å®¹çš„æå…¶é€¼çœŸçš„å›¾åƒï¼Œä»è€Œå¸¦æ¥è§†è§‰é”™è¯¯ä¿¡æ¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ (LDMs) åˆ©ç”¨é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ (AE) åœ¨ä½ç»´ç©ºé—´ä¸­æ‰§è¡Œå»å™ªè¿‡ç¨‹ï¼Œä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li><li>LDMs çš„å–è¯åˆ†æå°šå¤„äºèµ·æ­¥é˜¶æ®µã€‚</li><li>AEROBLADE æ˜¯ä¸€ç§åˆ©ç”¨ AE æ¥æ£€æµ‹ LDMs ç”Ÿæˆå›¾åƒçš„æ–°é¢–æ–¹æ³•ã€‚</li><li>ç”Ÿæˆçš„å›¾åƒå¯ä»¥è¢« AE æ›´å‡†ç¡®åœ°é‡å»ºï¼Œè€ŒçœŸå®å›¾åƒåˆ™ä¸èƒ½ã€‚</li><li>AEROBLADE æ˜¯ä¸€ç§ç®€å•çš„æ£€æµ‹æ–¹æ³•ï¼Œä¸éœ€è¦ä»»ä½•è®­ç»ƒï¼Œå³å¯æ¥è¿‘ä¾èµ–å¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚</li><li>AEROBLADE å¯ä»¥æœ‰æ•ˆåœ°æ£€æµ‹å‡ºæœ€å…ˆè¿›çš„ LDMsï¼ŒåŒ…æ‹¬ Stable Diffusion å’Œ Midjourneyã€‚</li><li>é™¤äº†æ£€æµ‹ä¹‹å¤–ï¼ŒAEROBLADE è¿˜å¯ä»¥å¯¹å›¾åƒè¿›è¡Œå®šæ€§åˆ†æï¼Œä»¥ä¾¿è¯†åˆ«è¢«ä¿®å¤çš„åŒºåŸŸã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šAEROBLADEï¼šåˆ©ç”¨è‡ªåŠ¨ç¼–ç å™¨é‡å»ºè¯¯å·®å®ç°æ— è®­ç»ƒæ£€æµ‹æ½œåœ¨æ‰©æ•£å›¾åƒ</li><li>ä½œè€…ï¼šCheng Zhangã€Yuheng Liã€Matthias Niessner</li><li>å•ä½ï¼šé©¬å…‹æ–¯Â·æ™®æœ—å…‹è®¡ç®—æœºå›¾å½¢å­¦ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€å›¾åƒå–è¯ã€æ·±åº¦å­¦ä¹ ã€è‡ªåŠ¨ç¼–ç å™¨ã€é‡å»ºè¯¯å·®</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09734ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¯ä»¥è½»æ¾ç”Ÿæˆå…·æœ‰æ¬ºéª—æ€§çš„é€¼çœŸå›¾åƒï¼Œè¿™åŠ å‰§äº†è§†è§‰é”™è¯¯ä¿¡æ¯çš„å¨èƒã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM) ä½œä¸ºç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„å…³é”®æŠ€æœ¯ï¼Œå› å…¶ä½è®¡ç®—æˆæœ¬è€Œå¤‡å—å…³æ³¨ã€‚ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒLDM åœ¨é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ (AE) çš„ä½ç»´æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œå»å™ªè¿‡ç¨‹ï¼Œè€Œéé«˜ç»´å›¾åƒç©ºé—´ã€‚å°½ç®¡ LDM å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å…¶å–è¯åˆ†æä»å¤„äºèµ·æ­¥é˜¶æ®µã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè®­ç»ƒæ£€æµ‹å™¨æ¥åŒºåˆ†çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒã€‚è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå¹¶ä¸”å¯¹æ–°å‡ºç°çš„ LDM æ¨¡å‹çš„æ³›åŒ–æ€§è¾ƒå·®ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º AEROBLADE çš„æ–°å‹æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº† LDM çš„å›ºæœ‰ç»„æˆéƒ¨åˆ†ï¼šç”¨äºåœ¨å›¾åƒç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´è½¬æ¢å›¾åƒçš„ AEã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸çœŸå®å›¾åƒç›¸æ¯”ï¼Œç”Ÿæˆå›¾åƒå¯ä»¥é€šè¿‡ AE æ›´å‡†ç¡®åœ°é‡å»ºï¼Œè¿™ä¸ºåŸºäºé‡å»ºè¯¯å·®çš„ç®€å•æ£€æµ‹æ–¹æ³•æä¾›äº†å¯èƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜“äºå®ç°ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒï¼Œä½†å…¶æ€§èƒ½å´ä¸ä¾èµ–äºå¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨å‡ ä¹ç›¸å½“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼ŒAEROBLADE å¯¹åŒ…æ‹¬ StableDiffusion å’Œ Midjourney åœ¨å†…çš„æœ€å…ˆè¿› LDM æ¨¡å‹æœ‰æ•ˆã€‚é™¤äº†æ£€æµ‹ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å…è®¸å¯¹å›¾åƒè¿›è¡Œå®šæ€§åˆ†æï¼Œè¿™å¯ç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ä¿®é¥°åŒºåŸŸã€‚</li></ol><p><strong>Methods</strong>ï¼šï¼ˆ1ï¼‰é‡å»ºè¯¯å·®æ£€æµ‹æ–¹æ³•çš„åŸºæœ¬æ¡†æ¶ï¼š- ç»™å®šç”Ÿæˆæ¨¡å‹ Gi å’Œå›¾åƒ xï¼Œè®¡ç®—é‡å»ºå›¾åƒ ~x = Ï•i(x)ï¼Œå…¶ä¸­ Ï•i æ˜¯åŸºäº Gi çš„é‡å»ºæ–¹æ³•ã€‚- å¯¹äºç”±æ¨¡å‹ Gi ç”Ÿæˆçš„å›¾åƒ xiï¼Œå…¶åŸå§‹å›¾åƒä¸é‡å»ºå›¾åƒä¹‹é—´çš„è·ç¦» d(xi, ~xi) å¾ˆå°ã€‚- çœŸå®å›¾åƒ xr ä¸èƒ½è¢«å‡†ç¡®é‡å»ºï¼Œå³ d(xr, ~xr) &gt; d(xi, ~xi)ã€‚</p><p>ï¼ˆ2ï¼‰AEROBLADE æ–¹æ³•ï¼š- AEROBLADEï¼ˆè‡ªåŠ¨ç¼–ç å™¨é‡å»ºè¯¯å·®æ½œåœ¨æ‰©æ•£æ£€æµ‹ï¼‰æ–¹æ³•åŸºäºè¿™æ ·çš„è§‚å¯Ÿï¼šæ¨¡å‹çš„è‡ªåŠ¨ç¼–ç å™¨ (AE) åœ¨é‡å»ºç”Ÿæˆå›¾åƒæ–¹é¢ä¼˜äºé‡å»ºçœŸå®å›¾åƒã€‚- å›¾åƒä¸å…¶é‡å»ºå›¾åƒä¹‹é—´çš„è·ç¦»å¯ä»¥è¿›è¡Œç®€å•çš„é˜ˆå€¼æ£€æµ‹ã€‚- ä¸ä¹‹å‰çš„ç ”ç©¶ä¸åŒï¼ŒAEROBLADE æ–¹æ³•æ—¢ä¸éœ€è¦æ‰§è¡Œä»£ä»·é«˜æ˜‚çš„ç¡®å®šæ€§å»å™ªè¿‡ç¨‹ï¼Œä¹Ÿä¸éœ€è¦ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚- AEROBLADE æ–¹æ³•çš„é‡å»ºè¯¯å·®å®šä¹‰ä¸ºï¼šâˆ†AEi(x) = d(x, ~x) = d(x, Di(Ei(x))ï¼‰ï¼Œå…¶ä¸­ Ei å’Œ Di åˆ†åˆ«è¡¨ç¤º Gi çš„è‡ªåŠ¨ç¼–ç å™¨çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œd æ˜¯æŸç§è·ç¦»åº¦é‡ã€‚</p><p>ï¼ˆ3ï¼‰AEROBLADE æ–¹æ³•çš„ä¼˜åŠ¿ï¼š- æ˜“äºå®ç°ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒï¼Œä½†æ€§èƒ½ä¸ä¾èµ–äºå¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨å‡ ä¹ç›¸å½“ã€‚- å¯¹åŒ…æ‹¬ StableDiffusion å’Œ Midjourney åœ¨å†…çš„æœ€å…ˆè¿›æ½œåœ¨æ‰©æ•£æ¨¡å‹æœ‰æ•ˆã€‚- é™¤äº†æ£€æµ‹ä¹‹å¤–ï¼ŒAEROBLADE æ–¹æ³•è¿˜å…è®¸å¯¹å›¾åƒè¿›è¡Œå®šæ€§åˆ†æï¼Œè¿™å¯ç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ä¿®é¥°åŒºåŸŸã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º AEROBLADE çš„æ–°å‹æ½œåœ¨æ‰©æ•£å›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è‡ªåŠ¨ç¼–ç å™¨é‡å»ºè¯¯å·®æ¥æ£€æµ‹ç”Ÿæˆå›¾åƒã€‚AEROBLADE æ–¹æ³•æ˜“äºå®ç°ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒï¼Œä½†æ€§èƒ½ä¸ä¾èµ–äºå¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨å‡ ä¹ç›¸å½“ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨ç¼–ç å™¨é‡å»ºè¯¯å·®çš„æ½œåœ¨æ‰©æ•£å›¾åƒæ£€æµ‹æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒï¼Œä½†æ€§èƒ½ä¸ä¾èµ–äºå¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨å‡ ä¹ç›¸å½“ã€‚</li><li>è¯¥æ–¹æ³•å¯¹åŒ…æ‹¬ StableDiffusion å’Œ Midjourney åœ¨å†…çš„æœ€å…ˆè¿›æ½œåœ¨æ‰©æ•£æ¨¡å‹æœ‰æ•ˆã€‚</li><li>è¯¥æ–¹æ³•é™¤äº†æ£€æµ‹ä¹‹å¤–ï¼Œè¿˜å…è®¸å¯¹å›¾åƒè¿›è¡Œå®šæ€§åˆ†æï¼Œè¿™å¯ç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ä¿®é¥°åŒºåŸŸã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ–¹æ³•å¯¹åŒ…æ‹¬ StableDiffusion å’Œ Midjourney åœ¨å†…çš„æœ€å…ˆè¿›æ½œåœ¨æ‰©æ•£æ¨¡å‹æœ‰æ•ˆã€‚</li><li>è¯¥æ–¹æ³•çš„æ£€æµ‹æ€§èƒ½ä¸ä¾èµ–äºå¤§é‡è®­ç»ƒçš„æ£€æµ‹å™¨å‡ ä¹ç›¸å½“ã€‚</li><li>è¯¥æ–¹æ³•é™¤äº†æ£€æµ‹ä¹‹å¤–ï¼Œè¿˜å…è®¸å¯¹å›¾åƒè¿›è¡Œå®šæ€§åˆ†æï¼Œè¿™å¯ç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ä¿®é¥°åŒºåŸŸã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•æ˜“äºå®ç°ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚</li><li>è¯¥æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li><li>è¯¥æ–¹æ³•çš„å­˜å‚¨æˆæœ¬è¾ƒä½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-73def9abeca6572820631d77d6d5f109.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e0d1888497ab3bcee223e776ab4c50c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f0becc9eb676089a928342cf2a8f891.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-07174ef7ab315c814e5b835ccce3106c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2bf092448ab8005e13f25729d701b790.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8d473060909c4dac5e620acfb56465e2.jpg" align="middle"></details><h2 id="Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models"><a href="#Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models" class="headerlink" title="Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models"></a>Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models</h2><p><strong>Authors:Kyungsung Lee, Donggyu Lee, Myungjoo Kang</strong></p><p>Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics. </p><p><a href="http://arxiv.org/abs/2401.17629v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©é¢‘ä¸é¢‘åŸŸä¿¡æ¯å……åˆ†ç»“åˆçš„æ‰©æ•£æ¨¡å‹å›¾åƒå¤åŸæ–¹æ³• SaFaRI ä»¥é«˜ä¿çœŸæˆåƒèƒ½åŠ›è¾¾åˆ°å›¾åƒä¿®å¤çš„å½“å‰å…ˆè¿›æ°´å‡†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SaFaRI æ¨¡å‹åœ¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ä¸‹ç»“åˆäº†å›¾åƒçš„ç©ºé—´åŸŸå’Œé¢‘åŸŸä¿¡æ¯ï¼Œæå‡äº†å›¾åƒä¿®å¤è´¨é‡ã€‚</li><li>åœ¨å„ç§å™ªå£°é€†é—®é¢˜ä¸Šï¼ŒåŒ…æ‹¬ä¿®å¤ã€å»å™ªå’Œè¶…åˆ†è¾¨ç‡ï¼ŒSaFaRI æ¨¡å‹å‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚</li><li>SaFaRI æ¨¡å‹åŒæ—¶åœ¨ ImageNet æ•°æ®é›†å’Œ FFHQ æ•°æ®é›†ä¸Šéƒ½ä¼˜äºå…¶ä»–é›¶æ ·æœ¬å›¾åƒä¿®å¤æ–¹æ³•ã€‚</li><li>SaFaRI æ¨¡å‹åœ¨ LPIPS å’Œ FID æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚</li><li>ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒSaFaRI æ¨¡å‹èƒ½åœ¨æ›´å¥½åœ°æ¢å¤å›¾åƒç»†èŠ‚çš„åŒæ—¶æœ‰æ•ˆé™ä½å™ªå£°ã€‚</li><li>SaFaRI æ¨¡å‹åœ¨ç§»é™¤æ¤’ç›å™ªå£°å’Œä¿®å¤æŸåå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li><li>SaFaRI æ¨¡å‹åœ¨è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†ä½åˆ†è¾¨ç‡å›¾åƒè½¬æ¢æˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„ç©ºé—´å’Œé¢‘ç‡æ„ŸçŸ¥å›¾åƒä¿®å¤æ–¹æ³•</li><li>ä½œè€…ï¼šKyungsung Leeã€Donggyu Leeã€Myungjoo Kang</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦–å°”å¤§å­¦æ•°å­¦ç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šå›¾åƒä¿®å¤ã€æ‰©æ•£æ¨¡å‹ã€æ•°æ®ä¿çœŸåº¦ã€ç©ºé—´æ„ŸçŸ¥ã€é¢‘ç‡æ„ŸçŸ¥</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.17629</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒä¿®å¤æ—¨åœ¨ä»é€€åŒ–æˆ–æŸåçš„å›¾åƒä¸­é‡å»ºåŸå§‹å›¾åƒã€‚ç»å…¸æ–¹æ³•æ˜¯ä½¿ç”¨å˜åˆ†æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬æ•°æ®ä¿çœŸåº¦é¡¹å’Œæ­£åˆ™åŒ–é¡¹ã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§æ–°å…´çš„ç”Ÿæˆæ¨¡å‹æ¡†æ¶ï¼Œåœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯ä»¥å®ç°é›¶æ ·æœ¬å­¦ä¹ ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åƒç´ çº§çš„æ•°æ®ä¿çœŸåº¦ï¼Œä½†å¿½ç•¥äº†æ„ŸçŸ¥ä¿¡æ¯ã€‚è¿™å¯¼è‡´ä¿®å¤åçš„å›¾åƒå¯èƒ½åœ¨è§†è§‰ä¸Šä¸ä»¤äººæ»¡æ„ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç©ºé—´å’Œé¢‘ç‡æ„ŸçŸ¥çš„æ‰©æ•£æ¨¡å‹ SaFaRIï¼Œç”¨äºé«˜æ–¯å™ªå£°ä¸‹çš„å›¾åƒä¿®å¤ã€‚è¯¥æ¨¡å‹é¼“åŠ±å›¾åƒåœ¨ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸéƒ½ä¿æŒæ•°æ®ä¿çœŸåº¦ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ ImageNet å’Œ FFHQ æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSaFaRI åœ¨ LPIPS å’Œ FID æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬å›¾åƒä¿®å¤æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) SaFaRIæ¨¡å‹æ¡†æ¶ï¼šSaFaRIæ¨¡å‹ç”±ç¼–ç å™¨ã€æ‰©æ•£è¿‡ç¨‹å’Œè§£ç å™¨ç»„æˆã€‚ç¼–ç å™¨å°†é€€åŒ–å›¾åƒæ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼Œæ‰©æ•£è¿‡ç¨‹é€šè¿‡æ·»åŠ å™ªå£°é€æ¸å°†æ½œåœ¨è¡¨ç¤ºä»é€€åŒ–çŠ¶æ€è½¬æ¢åˆ°å¹²å‡€çŠ¶æ€ï¼Œè§£ç å™¨å°†å¹²å‡€çš„æ½œåœ¨è¡¨ç¤ºé‡å»ºä¸ºä¿®å¤åçš„å›¾åƒã€‚(2) ç©ºé—´æ„ŸçŸ¥æ•°æ®ä¿çœŸåº¦ï¼šSaFaRIæ¨¡å‹åœ¨ç©ºé—´åŸŸä¸­ä½¿ç”¨å±€éƒ¨æ„ŸçŸ¥æŸå¤±æ¥é¼“åŠ±ä¿®å¤åçš„å›¾åƒä¸é€€åŒ–å›¾åƒåœ¨å±€éƒ¨åŒºåŸŸå†…ä¿æŒä¸€è‡´ã€‚å±€éƒ¨æ„ŸçŸ¥æŸå¤±é€šè¿‡è®¡ç®—é€€åŒ–å›¾åƒå’Œä¿®å¤å›¾åƒåœ¨å±€éƒ¨åŒºåŸŸå†…çš„å·®å¼‚æ¥è¡¡é‡æ•°æ®ä¿çœŸåº¦ã€‚(3) é¢‘ç‡æ„ŸçŸ¥æ•°æ®ä¿çœŸåº¦ï¼šSaFaRIæ¨¡å‹åœ¨é¢‘ç‡åŸŸä¸­ä½¿ç”¨é¢‘è°±æŸå¤±æ¥é¼“åŠ±ä¿®å¤åçš„å›¾åƒä¸é€€åŒ–å›¾åƒåœ¨é¢‘ç‡åˆ†å¸ƒä¸Šä¿æŒä¸€è‡´ã€‚é¢‘è°±æŸå¤±é€šè¿‡è®¡ç®—é€€åŒ–å›¾åƒå’Œä¿®å¤å›¾åƒçš„é¢‘è°±å·®å¼‚æ¥è¡¡é‡æ•°æ®ä¿çœŸåº¦ã€‚(4) æ‰©æ•£è¿‡ç¨‹ï¼šSaFaRIæ¨¡å‹é‡‡ç”¨éå¯¹ç§°æ‰©æ•£è¿‡ç¨‹ï¼Œå³æ­£å‘æ‰©æ•£è¿‡ç¨‹å’Œåå‘æ‰©æ•£è¿‡ç¨‹ã€‚æ­£å‘æ‰©æ•£è¿‡ç¨‹å°†æ½œåœ¨è¡¨ç¤ºä»é€€åŒ–çŠ¶æ€é€æ¸è½¬æ¢åˆ°å¹²å‡€çŠ¶æ€ï¼Œåå‘æ‰©æ•£è¿‡ç¨‹å°†æ½œåœ¨è¡¨ç¤ºä»å¹²å‡€çŠ¶æ€é€æ¸è½¬æ¢åˆ°é€€åŒ–çŠ¶æ€ã€‚(5) è®­ç»ƒè¿‡ç¨‹ï¼šSaFaRIæ¨¡å‹é€šè¿‡æœ€å°åŒ–æ€»æŸå¤±å‡½æ•°æ¥è®­ç»ƒï¼Œæ€»æŸå¤±å‡½æ•°åŒ…æ‹¬å±€éƒ¨æ„ŸçŸ¥æŸå¤±ã€é¢‘è°±æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±ã€‚æ­£åˆ™åŒ–æŸå¤±ç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼š æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒä¿®å¤æ–¹æ³• SaFaRIï¼Œè¯¥æ–¹æ³•å°†ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯çº³å…¥æ•°æ®ä¿çœŸåº¦é¡¹ä¸­ï¼Œæœ‰æ•ˆæé«˜äº†ä¿®å¤æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨åŒä¸‰æ¬¡æ’å€¼å’Œå‚…é‡Œå¶å˜æ¢åŒæ—¶åˆ©ç”¨ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯ï¼ŒSaFaRI åœ¨å„ç§å›¾åƒä¿®å¤åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚å°½ç®¡æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å…·æœ‰æ˜¾ç€çš„æ€§èƒ½ï¼Œä½†ç”±äºå…ˆéªŒé¡¹çš„å½±å“ï¼Œå˜æ¢çš„åº”ç”¨ä¸å¯é¿å…åœ°ä¼šå¯¹å¯è¡Œè§£äº§ç”Ÿæ‰°åŠ¨ã€‚å¯¹è§£æ‰°åŠ¨çš„å…¨é¢åˆ†æå¯ä»¥åŠ å¼ºæˆ‘ä»¬æ–¹æ³•è®ºçš„ç†è®ºåŸºç¡€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼š åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒä¿®å¤æ–¹æ³• SaFaRIï¼Œè¯¥æ–¹æ³•å°†ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯çº³å…¥æ•°æ®ä¿çœŸåº¦é¡¹ä¸­ï¼Œæœ‰æ•ˆæé«˜äº†ä¿®å¤æ€§èƒ½ã€‚</li><li>SaFaRI åˆ©ç”¨åŒä¸‰æ¬¡æ’å€¼å’Œå‚…é‡Œå¶å˜æ¢åŒæ—¶åˆ©ç”¨ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯ï¼Œå¯ä»¥æ›´å¥½åœ°ä¿ç•™å›¾åƒçš„ç»†èŠ‚å’Œçº¹ç†ã€‚</li><li>SaFaRI é‡‡ç”¨éå¯¹ç§°æ‰©æ•£è¿‡ç¨‹ï¼Œå¯ä»¥æ›´å¥½åœ°æ§åˆ¶å›¾åƒä¿®å¤è¿‡ç¨‹ï¼Œæé«˜ä¿®å¤è´¨é‡ã€‚</li></ul><p>æ€§èƒ½ï¼š</p><ul><li>SaFaRI åœ¨ ImageNet å’Œ FFHQ æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œåœ¨ LPIPS å’Œ FID æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬å›¾åƒä¿®å¤æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li><li>SaFaRI åœ¨ä¿®å¤é«˜æ–¯å™ªå£°å›¾åƒæ—¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å»é™¤å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒçš„ç»†èŠ‚å’Œçº¹ç†ã€‚</li></ul><p>å·¥ä½œé‡ï¼š</p><ul><li>SaFaRI æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œæ˜“äºå®ç°ã€‚</li><li>SaFaRI æ¨¡å‹çš„å‚æ•°é‡è¾ƒå°‘ï¼Œå¯ä»¥å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d12c8ba98ed6bf34752247f9b5d4ed94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-841cc516755a816daa1feb35b6020929.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6047f95584cb41e2634a1d794c58b933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e2b66f6263c96cab5ccac11907563d1.jpg" align="middle"></details>## You Only Need One Step: Fast Super-Resolution with Stable Diffusion via   Scale Distillation**Authors:Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos**In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step. [PDF](http://arxiv.org/abs/2401.17258v1) **Summary**æ‰©æ•£æ¨¡å‹çš„å•æ­¥è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼ŒYONO-SRï¼Œé€šè¿‡è’¸é¦è®­ç»ƒï¼Œå¯å®ç°å›¾åƒåˆ†è¾¨ç‡çš„æå‡ã€‚**Key Takeaways**- YONOS-SR åœ¨ä¿æŒæ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œå®ç°é«˜è´¨é‡çš„å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚- å¼•å…¥ä¸€ç§æ–°çš„å°ºåº¦è’¸é¦æ–¹æ³•ï¼Œä»è¾ƒå°çš„å°ºåº¦å¼€å§‹è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œç„¶åé‡‡ç”¨è¿­ä»£çš„æ–¹å¼å°†çŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ã€‚- è’¸é¦è®­ç»ƒä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿè·å¾—æ›´å‡†ç¡®çš„ç›®æ ‡ï¼Œä»è€Œæé«˜äº†è¶…åˆ†è¾¨ç‡çš„æ€§èƒ½ã€‚- åªéœ€ä¸€æ­¥æ¨ç†ï¼ŒYONOS-SR å°±èƒ½å¤Ÿè¶…è¶Šéœ€è¦ 200 æ­¥çš„æœ€æ–°æ–¹æ³•ã€‚- YONOS-SR ç»“åˆäº†ç©ºé—´è’¸é¦çš„ U-Net å’Œå¾®è°ƒçš„è§£ç å™¨ï¼Œè¿›ä¸€æ­¥æé«˜äº†è¶…åˆ†è¾¨ç‡æ•ˆæœã€‚- å†»ç»“ U-Net å¹¶å¾®è°ƒè§£ç å™¨ï¼Œå¯ä»¥è¿›ä¸€æ­¥æå‡è¶…åˆ†è¾¨ç‡æ€§èƒ½ã€‚- YONOS-SR å¯¹äºè®¡ç®—èµ„æºå—é™çš„è®¾å¤‡éå¸¸é€‚ç”¨ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šä¸€æ­¥åˆ°ä½ï¼šé€šè¿‡å°ºåº¦è’¸é¦å®ç°ç¨³å®šæ‰©æ•£çš„å¿«é€Ÿè¶…åˆ†è¾¨ç‡</li><li>ä½œè€…ï¼šMehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸‰æ˜ŸAIå‰‘æ¡¥</li><li>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ã€ç¨³å®šæ‰©æ•£ã€å°ºåº¦è’¸é¦ã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.17258</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œé‡‡æ ·ç­–ç•¥æ‰€éœ€çš„è¿ç»­å»å™ªä¼ é€’æ•°é‡å¾ˆå¤§ï¼Œå³ä½¿å¯¹äºåœ¨è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„åŸºäºç¨³å®šæ‰©æ•£çš„æ¨¡å‹ï¼ˆSDï¼‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šæœ€è¿‘ï¼Œå·²ç»æå‡ºäº†å‡ ç§å‡å°‘é‡‡æ ·æ­¥éª¤æ•°é‡çš„æ–¹æ³•ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼šå½±å“æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°‘çš„æ­¥éª¤ã€‚åŸºäºæ‰©æ•£çš„æ¨¡å‹é€šå¸¸åœ¨ä¸è®­ç»ƒæœŸé—´çœ‹åˆ°çš„å¤§å°ç›¸ä¼¼çš„å›¾åƒå—ä¸Šäº§ç”Ÿæœ€ä½³ç»“æœï¼ˆä¾‹å¦‚ï¼ŒSD çš„ 64Ã—64ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œè¶…åˆ†è¾¨ç‡åº”ç”¨ç¨‹åºéœ€è¦åœ¨é«˜åˆ†è¾¨ç‡è®¾ç½®ä¸­è¿è¡Œï¼Œè¿™å¤§å¤§åŠ å‰§äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è®¡ç®—é—®é¢˜ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºå°ºåº¦è’¸é¦ã€‚å…¸å‹çš„åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•é€šè¿‡ç›´æ¥åœ¨ç›®æ ‡å°ºåº¦å› å­ä¸Šçš„ä½åˆ†è¾¨ç‡å›¾åƒä¸Šè¿›è¡Œæ¡ä»¶æ¥è®­ç»ƒè¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›å¼è®­ç»ƒæ–¹æ³•ï¼Œä»è¾ƒä½å°ºåº¦å› å­ï¼ˆå³æ¡ä»¶ä¿¡å·æ›´æ¥è¿‘ç›®æ ‡ï¼‰å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å…ˆå‰è®­ç»ƒçš„æ¨¡å‹ä½œä¸ºæ•™å¸ˆé€æ­¥å¢åŠ åˆ°ç›®æ ‡å°ºåº¦å› å­ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»…ä¸€æ­¥ DDIM å³å¯å®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚é€šè¿‡å¯¹æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªå¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åªéœ€è¦ä¸€æ­¥å³å¯å†»ç»“ U-Net å¹¶å¾®è°ƒå…¶ä¸Šçš„è§£ç å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œç©ºé—´è’¸é¦ U-Net å’Œå¾®è°ƒè§£ç å™¨çš„ç»„åˆä»…éœ€ä¸€æ­¥å³å¯ä¼˜äºéœ€è¦ 200 æ­¥çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå°ºåº¦è’¸é¦çš„å¿«é€Ÿç¨³å®šæ‰©æ•£è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥é€šè¿‡ä¸€æ­¥DDIMå®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒç­–ç•¥â€”â€”å°ºåº¦è’¸é¦ï¼Œè¯¥ç­–ç•¥å¯ä»¥å°†åŸºäºç¨³å®šæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹è®­ç»ƒåˆ°ä»…éœ€è¦ä¸€æ­¥å³å¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li><li>é€šè¿‡å¯¹æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªå¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åªéœ€è¦ä¸€æ­¥å³å¯å†»ç»“U-Netå¹¶å¾®è°ƒå…¶ä¸Šçš„è§£ç å™¨ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»…ä¸€æ­¥DDIMå³å¯å®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œç©ºé—´è’¸é¦U-Netå’Œå¾®è°ƒè§£ç å™¨çš„ç»„åˆä»…éœ€ä¸€æ­¥å³å¯ä¼˜äºéœ€è¦200æ­¥çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œåªéœ€è¦å¯¹æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œç„¶åå†»ç»“U-Netå¹¶å¾®è°ƒå…¶ä¸Šçš„è§£ç å™¨å³å¯ã€‚</li><li>è¯¥æ–¹æ³•çš„æ¨ç†è¿‡ç¨‹ä¹Ÿéå¸¸å¿«é€Ÿï¼Œåªéœ€è¦ä¸€æ­¥DDIMå³å¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-66d1c3043943daf87e1f11e232a38f98.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-506663e69d7322407f5094b321bf2044.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/</id>
    <published>2024-01-30T11:23:52.000Z</published>
    <updated>2024-01-30T12:01:08.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-01-30-æ›´æ–°"><a href="#2024-01-30-æ›´æ–°" class="headerlink" title="2024-01-30 æ›´æ–°"></a>2024-01-30 æ›´æ–°</h1><h2 id="Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><a href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields" class="headerlink" title="Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields"></a>Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields</h2><p><strong>Authors:Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado</strong></p><p>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&amp;Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum. </p><p><a href="http://arxiv.org/abs/2401.16144v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦èŒƒå¼ï¼Œæå‡ NeRF æ¨¡å‹çš„æ¸²æŸ“è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¼ ç»Ÿ NeRF æ¨¡å‹çš„è®­ç»ƒèŒƒå¼å¯¹è®­ç»ƒé›†ä¸­æ¯ä¸ªå›¾åƒèµ‹äºˆåŒç­‰é‡è¦æ€§ï¼Œè¿™å¯¼è‡´åœ¨æ¸²æŸ“å…·æœ‰å¤æ‚å‡ ä½•ç»“æ„çš„ç‰¹å®šè§†å›¾æ—¶è¡¨ç°ä¸ä½³ã€‚</li><li>å°†è¾“å…¥è§†å›¾æ ¹æ®å…¶è§†è§‰ç›¸ä¼¼æ€§åˆ’åˆ†ä¸ºå¤šä¸ªç»„ï¼Œå¹¶åœ¨æ¯ä¸ªç»„ä¸Šè®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œä½¿æ¯ä¸ªæ¨¡å‹ä¸“æ³¨äºç‰¹å®šåŒºåŸŸï¼Œä»è€Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>é€šè¿‡æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦èŒƒå¼å°†è¿™äº›ä¸“é—¨æ¨¡å‹çš„çŸ¥è¯†èšåˆåˆ°ä¸€ä¸ªå®ä½“ä¸­ï¼Œå®ç°åœ¨çº¿æ¸²æŸ“çš„ç©ºé—´æ•ˆç‡ã€‚</li><li>åœ¨ NeRF åˆæˆå’Œ Tanks&amp;Temples ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå¯¹æå‡ºçš„è®­ç»ƒæ¡†æ¶è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ¡†æ¶ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”æ”¶æ•›åˆ°æ›´å¥½çš„æœ€å°å€¼ã€‚</li><li>æå‡ºäº†ä¸€ç§åä¸º DaC çš„åˆ†è€Œæ²»ä¹‹è®­ç»ƒæ¡†æ¶ã€‚</li><li>DaC å°†è®­ç»ƒé›†åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†ï¼Œå¹¶åœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒä¸€ä¸ªå•ç‹¬çš„ç¥ç»è¾å°„åœº (NeRF) æ¨¡å‹ã€‚</li><li>ç„¶åå°†è¿™äº›å­æ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦èšåˆæˆä¸€ä¸ªæœ€ç»ˆæ¨¡å‹ã€‚</li><li>DaC åœ¨ NeRF åˆæˆå’Œ Tanks&amp;Temples æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒä¼˜äºæœ€å…ˆè¿›çš„ NeRF æ¨¡å‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåˆ†è€Œæ²»ä¹‹ï¼šé‡æ–°æ€è€ƒç¥ç»è¾å°„åœºçš„è®­ç»ƒèŒƒå¼</li><li>ä½œè€…ï¼šRongkai Maã€Leo Lebratã€Rodrigo SantaCruzã€Gil Avrahamã€Yan Zuoã€Clinton Fookesã€Olivier Salvado</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±ä¼Ÿè¾¾</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€åˆ†è€Œæ²»ä¹‹ã€æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ã€ç©ºé—´æ•ˆç‡</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.16144Githubï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åˆæˆ 3D åœºæ™¯çš„é«˜ä¿çœŸè§†å›¾æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½† NeRF çš„æ ‡å‡†è®­ç»ƒèŒƒå¼é¢„è®¾äº†è®­ç»ƒé›†ä¸­æ¯ä¸ªå›¾åƒå…·æœ‰åŒç­‰é‡è¦æ€§ã€‚è¿™ç§å‡è®¾å¯¹æ¸²æŸ“å‘ˆç°å¤æ‚å‡ ä½•ä½“çš„ç‰¹å®šè§†å›¾æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•é€šå¸¸å°†æ‰€æœ‰åœºæ™¯è§†è§’çš„å‡ ä½•å’Œå…‰åº¦ä¿¡æ¯ç»Ÿä¸€å‹ç¼©åˆ°ç¥ç»ç½‘ç»œæƒé‡ä¸­ã€‚è¿™ç§æ–¹æ³•å¾€å¾€å¿½ç•¥äº†å¤æ‚åœºæ™¯ä¸åŒè§†è§’ä¸­å­˜åœ¨çš„ç»†èŠ‚çš„è‡ªç„¶ä¸å¯¹ç§°æ€§ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡é‡æ–°å®¡è§†äº†å½“å‰è®­ç»ƒèŒƒå¼çš„å«ä¹‰ï¼Œå¹¶é‡æ–°è®¾è®¡äº†è¯¥èŒƒå¼ï¼Œä»¥æé«˜ NeRF çš„æ¸²æŸ“è´¨é‡ã€‚å°†è¾“å…¥è§†å›¾æ ¹æ®å®ƒä»¬çš„è§†è§‰ç›¸ä¼¼æ€§åˆ’åˆ†ä¸ºå¤šä¸ªç»„ï¼Œå¹¶åœ¨æ¯ä¸ªç»„ä¸Šè®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œä½¿æ¯ä¸ªæ¨¡å‹èƒ½å¤Ÿä¸“é—¨é’ˆå¯¹ç‰¹å®šåŒºåŸŸï¼Œè€Œä¸ä¼šç‰ºç‰²é€Ÿåº¦æˆ–æ•ˆç‡ã€‚éšåï¼Œé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦èŒƒå¼å°†è¿™äº›ä¸“é—¨åŒ–æ¨¡å‹çš„çŸ¥è¯†èšé›†åˆ°ä¸€ä¸ªå•ä¸€å®ä½“ä¸­ï¼Œä»è€Œå®ç°åœ¨çº¿æ¸²æŸ“çš„ç©ºé—´æ•ˆç‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›† NeRF åˆæˆå’Œ Tanks&amp;Temples ä¸Šå¯¹æ–°é¢–çš„è®­ç»ƒæ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„åˆ†è€Œæ²»ä¹‹è®­ç»ƒç®¡é“æé«˜äº†æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶æ”¶æ•›åˆ°ä¸€ä¸ªæ›´å¥½çš„æœ€å°å€¼ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åœºæ™¯åˆ’åˆ†ï¼šå°†è¾“å…¥è§†å›¾æ ¹æ®è§†è§‰ç›¸ä¼¼æ€§åˆ’åˆ†ä¸ºå¤šä¸ªç»„ï¼Œæ¯ä¸ªç»„è®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹ï¼Œç§°ä¸ºä¸“å®¶æ¨¡å‹ã€‚ï¼ˆ2ï¼‰ä¸“å®¶è®­ç»ƒï¼šåœ¨æ¯ä¸ªç»„ä¸Šè®­ç»ƒä¸“å®¶æ¨¡å‹ï¼Œä½¿æ¯ä¸ªä¸“å®¶æ¨¡å‹èƒ½å¤Ÿä¸“é—¨é’ˆå¯¹ç‰¹å®šåŒºåŸŸï¼Œè€Œä¸ä¼šç‰ºç‰²é€Ÿåº¦æˆ–æ•ˆç‡ã€‚ï¼ˆ3ï¼‰æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ï¼šé€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦èŒƒå¼å°†è¿™äº›ä¸“é—¨åŒ–æ¨¡å‹çš„çŸ¥è¯†èšé›†åˆ°ä¸€ä¸ªå•ä¸€å®ä½“ä¸­ï¼Œä»è€Œå®ç°åœ¨çº¿æ¸²æŸ“çš„ç©ºé—´æ•ˆç‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„NeRFè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†è¾“å…¥è§†å›¾åˆ’åˆ†ä¸ºå¤šä¸ªç»„å¹¶è®­ç»ƒä¸“é—¨çš„ä¸“å®¶æ¨¡å‹ï¼Œæé«˜äº†NeRFçš„æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„NeRFè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†è¾“å…¥è§†å›¾åˆ’åˆ†ä¸ºå¤šä¸ªç»„å¹¶è®­ç»ƒä¸“é—¨çš„ä¸“å®¶æ¨¡å‹ï¼Œæé«˜äº†NeRFçš„æ¸²æŸ“è´¨é‡ã€‚</li><li>ä½¿ç”¨æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦èŒƒå¼å°†è¿™äº›ä¸“é—¨åŒ–æ¨¡å‹çš„çŸ¥è¯†èšé›†åˆ°ä¸€ä¸ªå•ä¸€å®ä½“ä¸­ï¼Œä»è€Œå®ç°åœ¨çº¿æ¸²æŸ“çš„ç©ºé—´æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†NeRFåˆæˆå’ŒTanks&amp;Templesä¸Šå¯¹æ–°é¢–çš„è®­ç»ƒæ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ã€‚</li><li>è¯„ä¼°è¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„åˆ†è€Œæ²»ä¹‹è®­ç»ƒç®¡é“æé«˜äº†æœ€å…ˆè¿›çš„åŸºå‡†æ¨¡å‹çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶æ”¶æ•›åˆ°ä¸€ä¸ªæ›´å¥½çš„æœ€å°å€¼ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„NeRFè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†è¾“å…¥è§†å›¾åˆ’åˆ†ä¸ºå¤šä¸ªç»„å¹¶è®­ç»ƒä¸“é—¨çš„ä¸“å®¶æ¨¡å‹ï¼Œæé«˜äº†NeRFçš„æ¸²æŸ“è´¨é‡ã€‚</li><li>ä½¿ç”¨æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦èŒƒå¼å°†è¿™äº›ä¸“é—¨åŒ–æ¨¡å‹çš„çŸ¥è¯†èšé›†åˆ°ä¸€ä¸ªå•ä¸€å®ä½“ä¸­ï¼Œä»è€Œå®ç°åœ¨çº¿æ¸²æŸ“çš„ç©ºé—´æ•ˆç‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-402a9ebdaec36fd0b9ae3b035907bf37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d76298373c29f69a44796c3bfafe8a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e414fcdc94276655b9d7b111a7932e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-86b43dc54cafd89cc41e3b7c64fefb1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca572fcd9b7c80bb78d37859a846f58c.jpg" align="middle"></details><h2 id="3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><a href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field" class="headerlink" title="3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field"></a>3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field</h2><p><strong>Authors:Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu</strong></p><p>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency. </p><p><a href="http://arxiv.org/abs/2401.14726v1">PDF</a> 20 pages, 8 figures</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) åŒæ¨¡å‹æœ-NeRF å®ç°é«˜è´¨å‡ ä½•é‡å»ºä¸è§†å›¾æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æœ-NeRF ç”±ä¸¤ä¸ªå‡ ä½•åœºç»„æˆï¼Œä¸€ä¸ªæºäº SDF åœºï¼Œä¸€ä¸ªæºäºå¯†åº¦åœºï¼Œç”¨äºåŒæ—¶å®ç°é«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œè§†å›¾æ¸²æŸ“ã€‚</li><li>æœ-NeRF å°†å¯†åº¦åœºåˆ†è§£ä¸ºè§†å›¾æ— å…³ç»„ä»¶å’Œè§†å›¾ç›¸å…³ç»„ä»¶ï¼Œå¹¶ä½¿ç”¨è§†å›¾æ— å…³ç»„ä»¶ä½œä¸º SDF åœºå­¦ä¹ è¿‡ç¨‹çš„æ ‡ç­¾ã€‚</li><li>æœ-NeRF å‡å°‘äº†å½¢çŠ¶ - è¾å°„åœºæ¨¡ç³Šæ€§ï¼Œå¹¶åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ä½¿å‡ ä½•å½¢çŠ¶å’Œé¢œè‰²ç›¸äº’å—ç›Šã€‚</li><li>æœ-NeRF åœ¨æ–°é¢–è§†å›¾åˆæˆå’Œå®¤å†…ç¯å¢ƒ 3D é‡å»ºæ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>æœ-NeRF åœ¨æ„å»ºä¸éµå®ˆå¤šè§†å›¾é¢œè‰²ä¸€è‡´æ€§çš„ç²¾ç»†å‡ ä½•å›¾å½¢åŒºåŸŸæ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚</li><li>æœ-NeRF å¯ç”¨äºå¢å¼ºç°å® (AR)ã€è™šæ‹Ÿç°å® (VR) å’Œ 3D å»ºæ¨¡ç­‰åº”ç”¨ã€‚</li><li>æœ-NeRF å¼€è¾Ÿäº† 3D é‡å»ºå’Œæ–°è§†å›¾åˆæˆç ”ç©¶çš„æ–°æ–¹å‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºåŒç¥ç»è¾å°„åœºçš„å®¤å†…ç¯å¢ƒä¸‰ç»´é‡å»ºä¸æ–°è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šYuxuan Zhang, Yufan Ren, Jiaolong Yang, Yinda Zhang, Xin Tong, Qionghai Dai</li><li>å•ä½ï¼šè¥¿æ¹–å¤§å­¦</li><li>å…³é”®è¯ï¼šä¸‰ç»´é‡å»ºã€æ–°è§†è§’åˆæˆã€ç¥ç»è¾å°„åœºã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.09426, Github é“¾æ¥ï¼šæš‚æ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´é‡å»ºå’Œæ–°è§†è§’åˆæˆåœ¨å®¤å†…ç¯å¢ƒä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œä½†æŠ€æœ¯ä¸Šéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŸºäºéšå¼ç¥ç»å‡½æ•°çš„æœ€æ–°æ–¹æ³•å¯ä»¥å®ç°å‡ºè‰²çš„ä¸‰ç»´é‡å»ºç»“æœï¼Œä½†å®ƒä»¬åœ¨æ–°è§†è§’åˆæˆä¸Šçš„æ€§èƒ½å¯èƒ½ä¸å°½å¦‚äººæ„ã€‚ç¥ç»è¾å°„åœº (NeRF) çš„å‘å±•å½»åº•æ”¹å˜äº†æ–°è§†è§’åˆæˆï¼Œç„¶è€Œï¼ŒåŸºäº NeRF çš„æ¨¡å‹å¯èƒ½æ— æ³•é‡å»ºå¹²å‡€çš„å‡ ä½•è¡¨é¢ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç¥ç»è¾å°„åœº (Du-NeRF) æ¥åŒæ—¶å®ç°é«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œè§†å›¾æ¸²æŸ“ã€‚Du-NeRF åŒ…å«ä¸¤ä¸ªå‡ ä½•åœºï¼Œä¸€ä¸ªæºè‡ª SDF åœºä»¥ä¿ƒè¿›å‡ ä½•é‡å»ºï¼Œå¦ä¸€ä¸ªæºè‡ªå¯†åº¦åœºä»¥å¢å¼ºæ–°è§†è§’åˆæˆã€‚Du-NeRF çš„åˆ›æ–°ç‰¹å¾ä¹‹ä¸€æ˜¯å®ƒå°†ä¸€ä¸ªä¸è§†å›¾æ— å…³çš„ç»„ä»¶ä»å¯†åº¦åœºä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶å°†å…¶ç”¨ä½œæ ‡ç­¾æ¥ç›‘ç£ SDF åœºçš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿™å‡å°‘äº†å½¢çŠ¶-è¾å°„æ¨¡ç³Šæ€§ï¼Œå¹¶ä½¿å‡ ä½•å’Œé¢œè‰²åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å—ç›Šäºå½¼æ­¤ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒDu-NeRF å¯ä»¥æ˜¾ç€æé«˜å®¤å†…ç¯å¢ƒçš„æ–°è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºçš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ„å»ºä¸éµå¾ªå¤šè§†å›¾é¢œè‰²ä¸€è‡´æ€§çš„ç²¾ç»†å‡ ä½•åŒºåŸŸæ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿåœ¨ Replica æ•°æ®é›†ä¸Šï¼ŒDu-NeRF åœ¨æ–°è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºæ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ ThinGeometry æ•°æ®é›†ä¸Šï¼ŒDu-NeRF åœ¨æ–°è§†è§’åˆæˆæ–¹é¢ä¹Ÿä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº† Du-NeRF çš„ç›®æ ‡ï¼Œå³åŒæ—¶å®ç°é«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œæ–°è§†è§’åˆæˆã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) Du-NeRFæ¨¡å‹æ¡†æ¶ï¼šDu-NeRFç”±ä¸¤ä¸ªå‡ ä½•åœºç»„æˆï¼Œä¸€ä¸ªæºè‡ªSDFåœºä»¥ä¿ƒè¿›å‡ ä½•é‡å»ºï¼Œå¦ä¸€ä¸ªæºè‡ªå¯†åº¦åœºä»¥å¢å¼ºæ–°è§†è§’åˆæˆã€‚(2) å‡ ä½•åœºçš„è®¾è®¡ï¼šSDFåœºç”¨äºè¡¨ç¤ºç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ï¼Œå¯†åº¦åœºç”¨äºè¡¨ç¤ºç‰©ä½“çš„é¢œè‰²å’Œå¤–è§‚ã€‚(3) è§†å›¾æ— å…³ç»„ä»¶çš„åˆ†ç¦»ï¼šDu-NeRFå°†ä¸€ä¸ªä¸è§†å›¾æ— å…³çš„ç»„ä»¶ä»å¯†åº¦åœºä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶å°†å…¶ç”¨ä½œæ ‡ç­¾æ¥ç›‘ç£SDFåœºçš„å­¦ä¹ è¿‡ç¨‹ã€‚(4) æŸå¤±å‡½æ•°çš„è®¾è®¡ï¼šDu-NeRFä½¿ç”¨äº†ä¸€ä¸ªç»“åˆäº†é‡å»ºæŸå¤±ã€è§†å›¾åˆæˆæŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ã€‚(5) è®­ç»ƒè¿‡ç¨‹ï¼šDu-NeRFä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥è®­ç»ƒæ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­äº¤æ›¿æ›´æ–°SDFåœºå’Œå¯†åº¦åœºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒç¥ç»è¾å°„åœº(Du-NeRF)æ¥åŒæ—¶å®ç°é«˜è´¨é‡çš„å‡ ä½•é‡å»ºå’Œè§†å›¾æ¸²æŸ“ã€‚Du-NeRFåŒ…å«ä¸¤ä¸ªå‡ ä½•åœºï¼Œä¸€ä¸ªæºè‡ªSDFåœºä»¥ä¿ƒè¿›å‡ ä½•é‡å»ºï¼Œå¦ä¸€ä¸ªæºè‡ªå¯†åº¦åœºä»¥å¢å¼ºæ–°è§†è§’åˆæˆã€‚Du-NeRFçš„åˆ›æ–°ç‰¹å¾ä¹‹ä¸€æ˜¯å®ƒå°†ä¸€ä¸ªä¸è§†å›¾æ— å…³çš„ç»„ä»¶ä»å¯†åº¦åœºä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶å°†å…¶ç”¨ä½œæ ‡ç­¾æ¥ç›‘ç£SDFåœºçš„å­¦ä¹ è¿‡ç¨‹ã€‚è¿™å‡å°‘äº†å½¢çŠ¶-è¾å°„æ¨¡ç³Šæ€§ï¼Œå¹¶ä½¿å‡ ä½•å’Œé¢œè‰²åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å—ç›Šäºå½¼æ­¤ã€‚(2): åˆ›æ–°ç‚¹ï¼šDu-NeRFå°†ä¸€ä¸ªä¸è§†å›¾æ— å…³çš„ç»„ä»¶ä»å¯†åº¦åœºä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶å°†å…¶ç”¨ä½œæ ‡ç­¾æ¥ç›‘ç£SDFåœºçš„å­¦ä¹ è¿‡ç¨‹ï¼Œå‡å°‘äº†å½¢çŠ¶-è¾å°„æ¨¡ç³Šæ€§ï¼Œå¹¶ä½¿å‡ ä½•å’Œé¢œè‰²åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å—ç›Šäºå½¼æ­¤ã€‚æ€§èƒ½ï¼šåœ¨Replicaæ•°æ®é›†ä¸Šï¼ŒDu-NeRFåœ¨æ–°è§†è§’åˆæˆå’Œä¸‰ç»´é‡å»ºæ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ThinGeometryæ•°æ®é›†ä¸Šï¼ŒDu-NeRFåœ¨æ–°è§†è§’åˆæˆæ–¹é¢ä¹Ÿä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦è®¾è®¡å’Œè®­ç»ƒä¸¤ä¸ªå‡ ä½•åœºï¼Œè¿˜éœ€è¦è®¾è®¡æŸå¤±å‡½æ•°å’Œè®­ç»ƒè¿‡ç¨‹ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-56683e282b9ba64280391f34e5aa9f31.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6383efbe47ff44676e2c2f51579aaa23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d811bf1bd890a7ed9dd96e40a81482c2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-98f97a5db5fd854c0d80066a92053a27.jpg" align="middle"></details><h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p><p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. </p><p><a href="http://arxiv.org/abs/2401.14257v2">PDF</a> 11 pages, 9 figures</p><p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼ 3D ç”Ÿæˆæ¡†æ¶ Sketch2NeRF å¯åˆ©ç”¨è‰å›¾æ§åˆ¶ç”Ÿæˆä¸€è‡´ä¸”é«˜ä¿çœŸçš„ 3D å†…å®¹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Sketch2NeRF æ˜¯ä¸€ä¸ªå¤šè§†è§’è‰å›¾å¼•å¯¼çš„æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥å°†è‰å›¾æ§åˆ¶æ·»åŠ åˆ° 3D ç”Ÿæˆä¸­ã€‚</li><li>Sketch2NeRF åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹æ¥ç›‘ç£ç”±ç¥ç»è¾å°„åœº (NeRF) è¡¨ç¤ºçš„ 3D åœºæ™¯çš„ä¼˜åŒ–ã€‚</li><li>Sketch2NeRF æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæ­¥ç”Ÿæˆå’Œé‡å»ºæ–¹æ³•æ¥æœ‰æ•ˆä¼˜åŒ– NeRFã€‚</li><li>Sketch2NeRF æ”¶é›†äº†ä¸¤ç§å¤šè§†è§’è‰å›¾æ•°æ®é›†æ¥è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•ã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒSketch2NeRF å¯ä»¥åˆæˆå…·æœ‰ç»†ç²’åº¦è‰å›¾æ§åˆ¶å¹¶ä¸”å¯¹æ–‡æœ¬æç¤ºé«˜åº¦ä¿çœŸçš„ 3D ä¸€è‡´å†…å®¹ã€‚</li><li>å¹¿æ³›çš„ç»“æœè¡¨æ˜ï¼ŒSketch2NeRF åœ¨è‰å›¾ç›¸ä¼¼æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSketch2NeRFï¼šå¤šè§†è§’è‰å›¾å¼•å¯¼çš„æ–‡æœ¬åˆ° 3D ç”Ÿæˆ</li><li>ä½œè€…ï¼šMinglin Chenã€Weihao Yuanã€Yukun Wangã€Zhe Shengã€Yisheng Heã€Zilong Dongã€Liefeng Boã€Yulan Guo</li><li>éš¶å±å•ä½ï¼šä¸­å±±å¤§å­¦æ·±åœ³æ ¡åŒº</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€NeRFã€è‰å›¾æ§åˆ¶ã€å¤šè§†è§’ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.14257Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šæ–‡æœ¬åˆ° 3D ç”Ÿæˆæ–¹æ³•å¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°ç”Ÿæˆé«˜ä¿çœŸ 3D å†…å®¹ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„ç‰©ä½“æ˜¯éšæœºçš„ï¼Œç¼ºä¹ç»†ç²’åº¦çš„æ§åˆ¶ã€‚è‰å›¾æä¾›äº†ä¸€ç§å¼•å…¥è¿™ç§ç»†ç²’åº¦æ§åˆ¶çš„å»‰ä»·æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºè‰å›¾çš„æŠ½è±¡æ€§å’Œæ¨¡ç³Šæ€§ï¼Œå¾ˆéš¾ä»è¿™äº›è‰å›¾ä¸­å®ç°çµæ´»çš„æ§åˆ¶ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹æ¥ç›‘ç£ 3D åœºæ™¯çš„ä¼˜åŒ–ï¼Œè¿™äº›åœºæ™¯ç”±ç¥ç»è¾å°„åœº (NeRF) è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è‰å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”ç”Ÿæˆçš„ 3D å¯¹è±¡å¯èƒ½ä¸è‰å›¾ä¸ä¸€è‡´ã€‚(3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè§†è§’è‰å›¾å¼•å¯¼çš„æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ¡†æ¶ï¼ˆå³ Sketch2NeRFï¼‰ï¼Œä»¥å°†è‰å›¾æ§åˆ¶æ·»åŠ åˆ° 3D ç”Ÿæˆä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹æ¥ç›‘ç£ 3D åœºæ™¯çš„ä¼˜åŒ–ï¼Œè¯¥åœºæ™¯ç”±ç¥ç»è¾å°„åœº (NeRF) è¡¨ç¤ºã€‚å¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæ­¥ç”Ÿæˆå’Œé‡å»ºæ–¹æ³•æ¥æœ‰æ•ˆåœ°ä¼˜åŒ– NeRFã€‚(4)ï¼šåœ¨å®éªŒä¸­ï¼Œæœ¬æ–‡æ”¶é›†äº†ä¸¤ç§å¤šè§†è§’è‰å›¾æ•°æ®é›†æ¥è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆå…·æœ‰ç»†ç²’åº¦è‰å›¾æ§åˆ¶çš„ 3D ä¸€è‡´å†…å®¹ï¼ŒåŒæ—¶å¯¹æ–‡æœ¬æç¤ºä¿æŒé«˜ä¿çœŸåº¦ã€‚å¹¿æ³›çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰å›¾ç›¸ä¼¼æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ol><p><strong>Methodsï¼š</strong></p><ol><li><p><strong>3Dè¡¨ç¤ºï¼š</strong>ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤º3Då¯¹è±¡ï¼ŒNeRFæ˜¯ä¸€ç§çµæ´»ä¸”èƒ½å¤Ÿæ¸²æŸ“é€¼çœŸå›¾åƒçš„æ–¹æ³•ã€‚</p></li><li><p><strong>è‰å›¾æ¡ä»¶ç”Ÿæˆï¼š</strong>ä½¿ç”¨é¢„è®­ç»ƒçš„2Dè‰å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ä½œä¸ºæŒ‡å¯¼ï¼Œè¿­ä»£æ›´æ–°NeRFçš„æƒé‡ã€‚</p></li><li><p><strong>åŒæ­¥ç”Ÿæˆå’Œé‡å»ºä¼˜åŒ–ï¼š</strong>æå‡ºäº†ä¸€ç§åŒæ­¥ç”Ÿæˆå’Œé‡å»ºä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ControlNetå’ŒStable Diffusionåˆ†åˆ«åœ¨è‰å›¾çš„ç‰¹å®šå§¿åŠ¿å’Œéšæœºé‡‡æ ·çš„å§¿åŠ¿ä¸‹ç”ŸæˆçœŸå®å›¾åƒï¼Œå¹¶ä½¿ç”¨NeRFæ¸²æŸ“çš„å›¾åƒä½œä¸ºé‡å»ºç›®æ ‡ï¼Œæœ€å°åŒ–ç”Ÿæˆå›¾åƒå’Œæ¸²æŸ“å›¾åƒä¹‹é—´çš„é‡å»ºæŸå¤±ã€‚</p></li><li><p><strong>ä¼˜åŒ–ï¼š</strong>ä½¿ç”¨åŸºäºåˆ†æ•°çš„è’¸é¦ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–NeRFï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†è‰å›¾æ¡ä»¶ç”Ÿæˆä¸NeRFçš„ä¼˜åŒ–ç›¸ç»“åˆã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè§†è§’è‰å›¾å¼•å¯¼çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ–¹æ³•Sketch2NeRFï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆä¸ç»™å®šè‰å›¾ç›¸ä¼¼çš„é€¼çœŸ3Då†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„2Dè‰å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ä½œä¸ºæŒ‡å¯¼ï¼Œè¿­ä»£æ›´æ–°NeRFçš„æƒé‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åŒæ­¥ç”Ÿæˆå’Œé‡å»ºä¼˜åŒ–æ–¹æ³•æ¥æœ‰æ•ˆåœ°ä¼˜åŒ–NeRFã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰å›¾ç›¸ä¼¼æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè§†è§’è‰å›¾å¼•å¯¼çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆä¸ç»™å®šè‰å›¾ç›¸ä¼¼çš„é€¼çœŸ3Då†…å®¹ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒæ­¥ç”Ÿæˆå’Œé‡å»ºä¼˜åŒ–æ–¹æ³•æ¥æœ‰æ•ˆåœ°ä¼˜åŒ–NeRFï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†è‰å›¾æ¡ä»¶ç”Ÿæˆä¸NeRFçš„ä¼˜åŒ–ç›¸ç»“åˆã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸¤ä¸ªå¤šè§†è§’è‰å›¾æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰å›¾ç›¸ä¼¼æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æ”¶é›†å¤šè§†è§’è‰å›¾æ•°æ®é›†ï¼Œå¹¶éœ€è¦é¢„è®­ç»ƒ2Dè‰å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-432d996d35cef510a47b970f6a57f9ed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b5a42bece9e656aff52a6fc20878da8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb3c2e84dae023cd921d28d348487b30.jpg" align="middle"></details><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†è§£ (NeRF-AD) æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯´è¯äººè„¸åˆæˆæ–¹æ³•ï¼Œé€šè¿‡éŸ³é¢‘æ³¨æ„æœºåˆ¶å°†äººè„¸åˆ†è§£ä¸ºéŸ³é¢‘é¢å­”å’Œèº«ä»½é¢å­”ï¼Œä»è€Œæé«˜äººè„¸åˆæˆçš„çœŸå®æ€§å’Œå”‡éƒ¨åŒæ­¥æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-AD æå‡ºäº†ä¸€ç§æ–°çš„è¯´è¯äººè„¸åˆæˆæ–¹æ³•ï¼Œç»“åˆäº†ç¥ç»è¾å°„åœº (NeRF) å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å°†äººè„¸åˆ†è§£ä¸ºéŸ³é¢‘é¢å­”å’Œèº«ä»½é¢å­”ï¼Œå¤§å¹…æå‡äº†ç”Ÿæˆäººè„¸çš„çœŸå®æ€§å’Œå”‡éƒ¨åŒæ­¥æ•ˆæœã€‚</li></ul><ul><li>NeRF-AD ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„åˆ†è§£æ¨¡å—ï¼Œåˆ©ç”¨è¯­éŸ³ç›¸å…³çš„é¢éƒ¨åŠ¨ä½œå•å…ƒ (AU) ä¿¡æ¯å°†äººè„¸åˆ†è§£ä¸ºéŸ³é¢‘é¢å­”å’Œèº«ä»½é¢å­”ï¼Œæœ‰æ•ˆåœ°å°†éŸ³é¢‘ä¸é¢éƒ¨è¯­éŸ³è¿åŠ¨ç›¸å…³åŒºåŸŸè¿›è¡Œç²¾ç¡®æ˜ å°„ã€‚</li></ul><ul><li>NeRF-AD åªå°†éŸ³é¢‘é¢å­”ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»è€Œç²¾ç¡®åœ°æ§åˆ¶éŸ³é¢‘å¦‚ä½•å½±å“è¯´è¯äººè„¸ã€‚</li></ul><ul><li>NeRF-AD åˆ©ç”¨ AU ä¿¡æ¯æ¥ç›‘ç£è¿™ä¸¤ç§æ¨¡æ€çš„èåˆï¼Œæé«˜äº†äººè„¸åˆæˆçš„å‡†ç¡®æ€§å’ŒçœŸå®æ€§ã€‚</li></ul><ul><li>å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒNeRF-AD åœ¨ç”Ÿæˆé€¼çœŸè¯´è¯äººè„¸è§†é¢‘æ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒè´¨é‡å’Œå”‡éƒ¨åŒæ­¥ã€‚</li></ul><ul><li>æ›´è¯¦ç»†çš„è§†é¢‘ç»“æœå¯ä»¥è®¿é—® <a href="https://xiaoxingliu02.github.io/NeRF-ADã€‚">https://xiaoxingliu02.github.io/NeRF-ADã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºä¸åŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»çš„è¯´è¯äººé¢éƒ¨åˆæˆï¼ˆNERF-ADï¼‰</li><li>ä½œè€…ï¼šBi Chongkeï¼ŒLiu Xiaoxingï¼ŒLiu Zhilei</li><li>å•ä½ï¼šå¤©æ´¥å¤§å­¦æ™ºèƒ½ä¸è®¡ç®—å­¦é™¢</li><li>å…³é”®è¯ï¼šè¯´è¯äººé¢éƒ¨åˆæˆï¼Œç¥ç»è¾å°„åœºï¼Œé¢éƒ¨åˆ†ç¦»</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.12568ï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººé¢éƒ¨åˆæˆåœ¨å¤šç»´ä¿¡å·å¤„ç†å’Œå¤šåª’ä½“é¢†åŸŸæ˜¯ä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æœ€è¿‘è¢«å¼•å…¥è¯¥ç ”ç©¶é¢†åŸŸï¼Œä»¥å¢å¼ºç”Ÿæˆé¢éƒ¨çš„çœŸå®æ„Ÿå’Œ 3D æ•ˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•è¦ä¹ˆç»™ NeRF å¸¦æ¥äº†å¤æ‚çš„å­¦ä¹ ä»»åŠ¡ï¼ŒåŒæ—¶ç¼ºä¹ç›‘ç£å¼å¤šæ¨¡æ€ç‰¹å¾èåˆçš„æ–¹æ³•ï¼Œè¦ä¹ˆæ— æ³•å°†éŸ³é¢‘ç²¾ç¡®æ˜ å°„åˆ°ä¸è¯­éŸ³è¿åŠ¨ç›¸å…³çš„é¢éƒ¨åŒºåŸŸã€‚è¿™äº›åŸå› æœ€ç»ˆå¯¼è‡´ç°æœ‰æ–¹æ³•ç”Ÿæˆçš„å”‡å½¢ä¸å‡†ç¡®ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¸€äº›ç°æœ‰çš„æ–¹æ³•å°† NeRF çš„å­¦ä¹ ä»»åŠ¡æå‰äº†ä¸€éƒ¨åˆ†ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šè¿‡å…·æœ‰åŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»çš„ NeRFï¼ˆNeRF-ADï¼‰è¿›è¡Œè¯´è¯äººé¢éƒ¨åˆæˆçš„ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»æ¨¡å—ï¼Œä½¿ç”¨ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨åŠ¨ä½œå•å…ƒ (AU) ä¿¡æ¯å°†é¢éƒ¨åˆ†ç¦»ä¸ºéŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨ã€‚ä¸ºäº†ç²¾ç¡®åœ°è°ƒèŠ‚éŸ³é¢‘å¦‚ä½•å½±å“è¯´è¯äººé¢éƒ¨ï¼Œæˆ‘ä»¬åªå°†éŸ³é¢‘é¢éƒ¨ä¸éŸ³é¢‘ç‰¹å¾èåˆã€‚æ­¤å¤–ï¼ŒAU ä¿¡æ¯è¿˜ç”¨äºç›‘ç£è¿™ä¸¤ä¸ªæ¨¡æ€çš„èåˆã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å‡å°‘ NeRF çš„å­¦ä¹ è´Ÿæ‹…å¹¶æé«˜é¢éƒ¨æ¸²æŸ“çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬åˆ†è§£è¯´è¯äººé¢éƒ¨å¹¶ä¸º NeRF æä¾›ä¸¤ä¸ªåˆ†è§£çš„ç²¾ç¡®æ¡ä»¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»æ¨¡å—ï¼Œå…è®¸éŸ³é¢‘ä¸ä¸è¯­éŸ³è¿åŠ¨ç›¸å…³çš„é¢éƒ¨åŒºåŸŸç²¾ç¡®èåˆã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç³»åˆ—æ–¹æ³•æ¥ç›‘ç£æ•´ä¸ªè¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ NeRF-AD åœ¨ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººé¢éƒ¨è§†é¢‘æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒè´¨é‡å’Œå”‡å½¢åŒæ­¥ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1)ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»æ¨¡å—NeRF-ADï¼Œå°†é¢éƒ¨åˆ†ç¦»ä¸ºéŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨ï¼Œå¹¶åªå°†éŸ³é¢‘é¢éƒ¨ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»¥ç²¾ç¡®è°ƒèŠ‚éŸ³é¢‘å¦‚ä½•å½±å“è¯´è¯äººé¢éƒ¨ã€‚(2)ï¼šä¸ºäº†å‡å°‘NeRFçš„å­¦ä¹ è´Ÿæ‹…å¹¶æé«˜é¢éƒ¨æ¸²æŸ“çš„å‡†ç¡®æ€§ï¼Œå°†è¯´è¯äººé¢éƒ¨åˆ†è§£å¹¶ä¸ºNeRFæä¾›ä¸¤ä¸ªåˆ†è§£çš„ç²¾ç¡®æ¡ä»¶ã€‚(3)ï¼šé‡‡ç”¨ä¸€ç³»åˆ—æ–¹æ³•æ¥ç›‘ç£æ•´ä¸ªè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨åŠ¨ä½œå•å…ƒ(AU)ä¿¡æ¯ç›‘ç£éŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨çš„èåˆï¼Œä»¥åŠä½¿ç”¨æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±æ¥ç›‘ç£NeRFçš„å­¦ä¹ ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»æ¨¡å—NeRF-ADï¼Œå°†è¯´è¯äººé¢éƒ¨åˆ†ç¦»ä¸ºéŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨ï¼Œå¹¶åªå°†éŸ³é¢‘é¢éƒ¨ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»¥ç²¾ç¡®è°ƒèŠ‚éŸ³é¢‘å¦‚ä½•å½±å“è¯´è¯äººé¢éƒ¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„åˆ†ç¦»æ¨¡å—ï¼Œå°†è¯´è¯äººé¢éƒ¨åˆ†ç¦»ä¸ºéŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨ï¼Œå¹¶åªå°†éŸ³é¢‘é¢éƒ¨ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»¥ç²¾ç¡®è°ƒèŠ‚éŸ³é¢‘å¦‚ä½•å½±å“è¯´è¯äººé¢éƒ¨ã€‚ä¸ºäº†å‡å°‘NeRFçš„å­¦ä¹ è´Ÿæ‹…å¹¶æé«˜é¢éƒ¨æ¸²æŸ“çš„å‡†ç¡®æ€§ï¼Œå°†è¯´è¯äººé¢éƒ¨åˆ†è§£å¹¶ä¸ºNeRFæä¾›ä¸¤ä¸ªåˆ†è§£çš„ç²¾ç¡®æ¡ä»¶ã€‚é‡‡ç”¨ä¸€ç³»åˆ—æ–¹æ³•æ¥ç›‘ç£æ•´ä¸ªè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨åŠ¨ä½œå•å…ƒ(AU)ä¿¡æ¯ç›‘ç£éŸ³é¢‘é¢éƒ¨å’Œèº«ä»½é¢éƒ¨çš„èåˆï¼Œä»¥åŠä½¿ç”¨æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±æ¥ç›‘ç£NeRFçš„å­¦ä¹ ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-ADåœ¨å›¾åƒè´¨é‡å’Œå”‡å½¢åŒæ­¥æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šå·¥ä½œé‡è¾ƒå¤§ï¼Œéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details><h2 id="HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><a href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs" class="headerlink" title="HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs"></a>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs</h2><p><strong>Authors:Zelin Gao, Weichen Dai, Yu Zhang</strong></p><p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs. </p><p><a href="http://arxiv.org/abs/2401.11711v1">PDF</a> 13 pages, 6 figures</p><p><strong>æ‘˜è¦</strong><br>å±‚æ¬¡å‡ ä½•ã€è¯­ä¹‰å’Œå…‰åº¦å¼•å¯¼ NeRFï¼ˆHG3-NeRFï¼‰æ–¹æ³•èƒ½æé«˜ç¨€ç–è§†å›¾è¾“å…¥ä¸‹åœºæ™¯è¡¨ç¤ºçš„å‡ ä½•ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>HG3-NeRF æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥è§£å†³ç¨€ç–è§†å›¾è¾“å…¥ä¸‹ NeRF çš„æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œå¹¶æé«˜å‡ ä½•ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚</li><li>HG3-NeRF æå‡ºäº†ä¸€ç§åˆ†å±‚å‡ ä½•å¼•å¯¼ (HGG) æ–¹æ³•ï¼Œå°†è¿åŠ¨ç»“æ„ (SfM) çš„é™„ä»¶ï¼ˆå³ç¨€ç–æ·±åº¦å…ˆéªŒï¼‰çº³å…¥åœºæ™¯è¡¨ç¤ºä¸­ã€‚</li><li>HGG ä»å±€éƒ¨åˆ°å…¨å±€çš„å‡ ä½•åŒºåŸŸå¯¹ä½“ç§¯ç‚¹è¿›è¡Œé‡‡æ ·ï¼Œå‡è½»äº†æ·±åº¦å…ˆéªŒä¸­å›ºæœ‰åå·®é€ æˆçš„é”™ä½ã€‚</li><li>HG3-NeRF æå‡ºäº†ä¸€ç§åˆ†å±‚è¯­ä¹‰å¼•å¯¼ (HSG) æ–¹æ³•ï¼Œå­¦ä¹ ä»ç²—åˆ°ç»†çš„è¯­ä¹‰å†…å®¹ï¼Œè¿™å¯¹åº”äºä»ç²—åˆ°ç»†çš„åœºæ™¯è¡¨ç¤ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHG3-NeRF åœ¨ä¸åŒçš„æ ‡å‡†åŸºå‡†ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶å®ç°äº†ç¨€ç–è§†å›¾è¾“å…¥çš„é«˜ä¿çœŸåˆæˆç»“æœã€‚</li><li>HG3-NeRF æ–¹æ³•èƒ½æé«˜ç¨€ç–è§†å›¾è¾“å…¥ä¸‹åœºæ™¯è¡¨ç¤ºçš„å‡ ä½•ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚</li><li>HG3-NeRF æ–¹æ³•èƒ½æé«˜ç¨€ç–è§†å›¾è¾“å…¥ä¸‹åœºæ™¯è¡¨ç¤ºçš„å‡ ä½•ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHG3-NeRFï¼šç”¨äºç¨€ç–è§†å›¾è¾“å…¥çš„åˆ†å±‚å‡ ä½•ã€è¯­ä¹‰å’Œå…‰åº¦å¼•å¯¼çš„ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šZelin Gao, Weichen Dai, Yu Zhang</li><li>éš¶å±æœºæ„ï¼šæµ™æ±Ÿå¤§å­¦æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€ç¨€ç–è§†å›¾ã€å‡ ä½•å¼•å¯¼ã€è¯­ä¹‰å¼•å¯¼ã€å…‰åº¦å¼•å¯¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.11711ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å› å…¶ä»ç¦»æ•£è§‚æµ‹ä¸­å­¦ä¹ åœºæ™¯è¡¨ç¤ºä»¥è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆè€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹ç¨€ç–è§†å›¾è¾“å…¥æ—¶ï¼ŒNeRF çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä»è€Œé™åˆ¶äº†å…¶è¿›ä¸€æ­¥çš„é€‚ç”¨æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒæ–¹æ³•å’Œé€åœºæ™¯ä¼˜åŒ–æ–¹æ³•æ¥è§£å†³ç¨€ç–è§†å›¾è¾“å…¥çš„æŒ‘æˆ˜ã€‚é¢„è®­ç»ƒæ–¹æ³•åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æµ‹è¯•æ—¶å¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ•°æ®é›†çš„è´¨é‡ï¼Œè€Œä¸”é€šè¿‡æ•æ‰è®¸å¤šä¸åŒåœºæ™¯æ¥è·å¾—å¿…è¦çš„æ•°æ®é›†è¿‡äºæ˜‚è´µã€‚é€åœºæ™¯ä¼˜åŒ–æ–¹æ³•åœ¨æ¯ä¸ªåœºæ™¯ä¸Šä¼˜åŒ–æ¨¡å‹ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡è®¡ç®—ï¼Œå¹¶ä¸”å¯èƒ½éš¾ä»¥æ”¶æ•›åˆ°è‰¯å¥½çš„è§£ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• HG3-NeRFï¼Œå¯ä»¥è§£å†³ä¸Šè¿°é™åˆ¶å¹¶å¢å¼ºä¸åŒè§†å›¾ä¹‹é—´å‡ ä½•å½¢çŠ¶ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚HG3-NeRF åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šåˆ†å±‚å‡ ä½•å¼•å¯¼ï¼ˆHGGï¼‰ã€åˆ†å±‚è¯­ä¹‰å¼•å¯¼ï¼ˆHSGï¼‰å’Œå…‰åº¦å¼•å¯¼ã€‚HGG å°†ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰çš„é™„åŠ ä¿¡æ¯ï¼ˆå³ç¨€ç–æ·±åº¦å…ˆéªŒï¼‰çº³å…¥åœºæ™¯è¡¨ç¤ºä¸­ã€‚HSG ä»ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒä¸­å­¦ä¹ ç²—åˆ°ç»†çš„è¯­ä¹‰å†…å®¹ï¼Œè¿™ä¸ç²—åˆ°ç»†çš„åœºæ™¯è¡¨ç¤ºç›¸å¯¹åº”ã€‚å…‰åº¦å¼•å¯¼ä½¿ç”¨æ¸²æŸ“æ–¹ç¨‹æ¥ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼Œä»¥åŒ¹é…è¾“å…¥è§†å›¾çš„é¢œè‰²å’Œäº®åº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒHG3-NeRF åœ¨ä¸åŒçš„æ ‡å‡†åŸºå‡†ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ç¨€ç–è§†å›¾è¾“å…¥ä¸‹å®ç°äº†é«˜ä¿çœŸåˆæˆç»“æœã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ï¼Œå³è§£å†³ç¨€ç–è§†å›¾è¾“å…¥çš„æŒ‘æˆ˜å¹¶å¢å¼ºä¸åŒè§†å›¾ä¹‹é—´å‡ ä½•å½¢çŠ¶ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚</li></ol><p>Methodsï¼šï¼ˆ1ï¼‰åˆ†å±‚å‡ ä½•å¼•å¯¼ï¼ˆHGGï¼‰ï¼šåˆ©ç”¨æ¥è‡ªç»“æ„è¿åŠ¨ï¼ˆSfMï¼‰çš„ç¨€ç–æ·±åº¦å…ˆéªŒï¼Œå°†å‡ ä½•ä¸€è‡´æ€§çº³å…¥åœºæ™¯è¡¨ç¤ºä¸­ã€‚HGG æ–¹æ³•æŒ‡å¯¼ç¥ç»è¾å°„åœºå­¦ä¹ å¯†åº¦å’Œé¢œè‰²çš„è¿‘ä¼¼åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒæ¥è‡ªæ·±åº¦å…ˆéªŒç¡®å®šçš„å±€éƒ¨åˆ°å…¨å±€çš„é‡‡æ ·åŒºåŸŸã€‚ï¼ˆ2ï¼‰åˆ†å±‚è¯­ä¹‰å¼•å¯¼ï¼ˆHSGï¼‰ï¼šä»ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒä¸­å­¦ä¹ ä»ç²—åˆ°ç»†çš„è¯­ä¹‰å†…å®¹ï¼Œè¿™ä¸ä»ç²—åˆ°ç»†çš„åœºæ™¯è¡¨ç¤ºç›¸å¯¹åº”ã€‚HSG ä½¿ç”¨ CLIP ç¼–ç å™¨å¯¹æ¸²æŸ“çš„å›¾åƒå’ŒåŸå§‹å›¾åƒçš„ç‰¹å¾å‘é‡è¿›è¡Œç¼–ç ï¼Œå¹¶è®¡ç®—ç²—åˆ°ç»†çš„è¯­ä¹‰ä½™å¼¦ç›¸ä¼¼æ€§ã€‚ï¼ˆ3ï¼‰å…‰åº¦å¼•å¯¼ï¼šä½¿ç”¨æ¸²æŸ“æ–¹ç¨‹ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼Œä»¥åŒ¹é…è¾“å…¥è§†å›¾çš„é¢œè‰²å’Œäº®åº¦ã€‚å…‰åº¦å¼•å¯¼é€šè¿‡æœ€å°åŒ–æ¸²æŸ“çš„å›¾åƒå’ŒåŸå§‹å›¾åƒä¹‹é—´çš„å¤–è§‚å‡æ–¹è¯¯å·®æ¥å®ç°ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†å±‚å‡ ä½•ã€è¯­ä¹‰å’Œå…‰åº¦å¼•å¯¼çš„ç¥ç»è¾å°„åœºï¼ˆHG3-NeRFï¼‰æ–¹æ³•ï¼Œå¯ä»¥è§£å†³ç¨€ç–è§†å›¾è¾“å…¥çš„æŒ‘æˆ˜å¹¶å¢å¼ºä¸åŒè§†å›¾ä¹‹é—´å‡ ä½•å½¢çŠ¶ã€è¯­ä¹‰å†…å®¹å’Œå¤–è§‚çš„ä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åˆ†å±‚å‡ ä½•å¼•å¯¼ï¼ˆHGGï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨æ¥è‡ªç»“æ„è¿åŠ¨ï¼ˆSfMï¼‰çš„ç¨€ç–æ·±åº¦å…ˆéªŒï¼Œå°†å‡ ä½•ä¸€è‡´æ€§çº³å…¥åœºæ™¯è¡¨ç¤ºä¸­ã€‚</li><li>æå‡ºäº†ä¸€ç§åˆ†å±‚è¯­ä¹‰å¼•å¯¼ï¼ˆHSGï¼‰æ–¹æ³•ï¼Œä»ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒä¸­å­¦ä¹ ä»ç²—åˆ°ç»†çš„è¯­ä¹‰å†…å®¹ï¼Œè¿™ä¸ä»ç²—åˆ°ç»†çš„åœºæ™¯è¡¨ç¤ºç›¸å¯¹åº”ã€‚</li><li>ä½¿ç”¨æ¸²æŸ“æ–¹ç¨‹ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºï¼Œä»¥åŒ¹é…è¾“å…¥è§†å›¾çš„é¢œè‰²å’Œäº®åº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸åŒçš„æ ‡å‡†åŸºå‡†ä¸Šä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ç¨€ç–è§†å›¾è¾“å…¥ä¸‹å®ç°äº†é«˜ä¿çœŸåˆæˆç»“æœã€‚å·¥ä½œé‡ï¼š</li><li>éœ€è¦ä¼°è®¡ç›¸æœºä½å§¿ï¼Œå¹¶ä¸”ç¨€ç–è§†å›¾è¾“å…¥ä¼šå½±å“ä½å§¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-56cd69227addb7c7e2e5ec9028bc8cb0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bb7c383a42f7306611645083f4d82eb9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-71514b137fee0e499428b6e4c393be26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5dccc88a28d6fafb1f550b78be5145.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bab43cfc9ed715f6025ba1321b7acdc3.jpg" align="middle"></details><h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p><p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts. </p><p><a href="http://arxiv.org/abs/2401.09495v4">PDF</a> Error on result tabulation of state of the art method which might   cause misleading to readers</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¤‡å—å…³æ³¨ï¼Œå¹¶äº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œç”±äºå…¶æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ï¼Œå› æ­¤å­˜åœ¨è¢«å‰½çªƒè€…éæ³•å¤åˆ¶ã€å†åˆ†å‘æˆ–æ»¥ç”¨çš„é£é™©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ NeRF æ¨¡å‹çš„é»‘ç›’å’Œç™½ç›’è®¾ç½®çš„ç»¼åˆçŸ¥è¯†äº§æƒï¼ˆIPï¼‰ä¿æŠ¤æ¡†æ¶ï¼Œç§°ä¸º IPR-NeRFã€‚</li><li>åœ¨é»‘ç›’è®¾ç½®ä¸­ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºæ‰©æ•£çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ä¸¤é˜¶æ®µä¼˜åŒ–è¿‡ç¨‹åµŒå…¥å’Œæå–æ°´å°ã€‚</li><li>åœ¨ç™½ç›’è®¾ç½®ä¸­ï¼Œé€šè¿‡é‡‡ç”¨ç¬¦å·æŸå¤±ç›®æ ‡å°†æŒ‡å®šæ•°å­—ç­¾ååµŒå…¥ NeRF æ¨¡å‹çš„æƒé‡ä¸­ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿æŒäº† IPR-NeRF æ¨¡å‹çš„ä¿çœŸåº¦ï¼ˆå³æ¸²æŸ“è´¨é‡ï¼‰ï¼Œè€Œä¸”ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒè¿˜å¯¹æ­§ä¹‰æ”»å‡»å’Œå»é™¤æ”»å‡»å…·æœ‰é²æ£’æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šIPR-NERFï¼šçŸ¥è¯†äº§æƒéªŒè¯æ»¡è¶³ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šKent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</li><li>å•ä½ï¼šé©¬æ¥äºšå¤§å­¦å›¾åƒä¸ä¿¡å·å¤„ç†ä¸­å¿ƒï¼ˆCISiPï¼‰</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€çŸ¥è¯†äº§æƒä¿æŠ¤ã€æ•°å­—æ°´å°ã€æ•°å­—ç­¾å</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2401.09495v1[cs.CV]17Jan2024Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹å› å…¶å“è¶Šçš„è§†è§‰è´¨é‡å’Œä»¤äººå°è±¡æ·±åˆ»çš„æ¼”ç¤ºè€Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼ŒNeRF æ¨¡å‹ä¹Ÿé¢ä¸´ç€çŸ¥è¯†äº§æƒä¿æŠ¤çš„é—®é¢˜ï¼Œå‰½çªƒè€…å¯èƒ½ä¼šéæ³•å¤åˆ¶ã€é‡æ–°åˆ†å‘æˆ–æ»¥ç”¨è¿™äº›æ¨¡å‹ä»¥è·å–ç»æµåˆ©ç›Šæˆ–ä¸ªäººåˆ©ç›Šã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç›®å‰é’ˆå¯¹ç¥ç»ç½‘ç»œçš„çŸ¥è¯†äº§æƒä¿æŠ¤æ–¹æ¡ˆä¸»è¦é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ¡ˆåœ¨åº”ç”¨äº NeRF æ¨¡å‹æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œä¾‹å¦‚ NeRF æ¨¡å‹çš„å¤æ‚ç»“æ„ã€å¯¹æ•°æ®å’Œè®¡ç®—èµ„æºçš„è¦æ±‚è¾ƒé«˜ä»¥åŠç¼ºä¹æœ‰æ•ˆçš„çŸ¥è¯†äº§æƒä¿æŠ¤æŠ€æœ¯ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»¼åˆçš„ NeRF æ¨¡å‹çŸ¥è¯†äº§æƒä¿æŠ¤æ¡†æ¶ï¼Œç§°ä¸º IPR-NERFã€‚è¯¥æ¡†æ¶åŒ…æ‹¬é»‘ç›’å’Œç™½ç›’ä¸¤ç§è®¾ç½®ã€‚åœ¨é»‘ç›’è®¾ç½®ä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§£å†³æ–¹æ¡ˆæ¥åµŒå…¥å’Œæå–æ°´å°ï¼Œé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹å®ç°ã€‚åœ¨ç™½ç›’è®¾ç½®ä¸­ï¼Œé€šè¿‡é‡‡ç”¨ç¬¦å·æŸå¤±ç›®æ ‡å‡½æ•°ï¼Œå°†æŒ‡å®šæ•°å­—ç­¾ååµŒå…¥ NeRF æ¨¡å‹çš„æƒé‡ä¸­ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒIPR-NERF æ¨¡å‹ä¸ä»…ä¿æŒäº†æ¸²æŸ“è´¨é‡ï¼Œè€Œä¸”åœ¨é¢å¯¹æ¨¡ç³Šæ€§å’Œå»é™¤æ”»å‡»æ—¶ä¹Ÿå…·æœ‰é²æ£’æ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ä¸ªç»¼åˆçš„NeRFæ¨¡å‹çŸ¥è¯†äº§æƒä¿æŠ¤æ¡†æ¶IPR-NERFï¼ŒåŒ…æ‹¬é»‘ç›’å’Œç™½ç›’ä¸¤ç§è®¾ç½®ã€‚ï¼ˆ2ï¼‰ï¼šåœ¨é»‘ç›’è®¾ç½®ä¸­ï¼Œå¼•å…¥ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§£å†³æ–¹æ¡ˆæ¥åµŒå…¥å’Œæå–æ°´å°ï¼Œé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹å®ç°ã€‚ï¼ˆ3ï¼‰ï¼šåœ¨ç™½ç›’è®¾ç½®ä¸­ï¼Œé€šè¿‡é‡‡ç”¨ç¬¦å·æŸå¤±ç›®æ ‡å‡½æ•°ï¼Œå°†æŒ‡å®šæ•°å­—ç­¾ååµŒå…¥NeRFæ¨¡å‹çš„æƒé‡ä¸­ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§å…¨é¢çš„ã€é²æ£’çš„ NeRF-IPR ä¿æŠ¤æ–¹æ¡ˆï¼ŒåŒ…æ‹¬é»‘ç›’å’Œç™½ç›’ä¸¤ç§åœºæ™¯ã€‚å…¨é¢çš„å®éªŒç»“æœè¡¨æ˜äº†å…¶åœ¨æŠµæŠ—åµŒå…¥æ°´å°çš„æ¨¡ç³Šæ€§å’Œå»é™¤æ”»å‡»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä¿æŒäº†æ¸²æŸ“æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è®¡ç®—èƒ½åŠ›å’Œå¯¹è¦†ç›–æ”»å‡»çš„é»‘ç›’ä¿æŠ¤æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå½“æ”»å‡»è€…æ‹¥æœ‰å—ä¿æŠ¤æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯æ—¶ã€‚æœªæ¥çš„ç ”ç©¶å°†é›†ä¸­åœ¨æ”¹è¿›è¿™äº›æ–¹é¢ã€‚æœ¬ç ”ç©¶ä¸º NeRF æ¨¡å‹å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æä¾›äº†æå¤§çš„ä»·å€¼ï¼Œæä¾›äº†ä¸€ç§ä¿æŠ¤å…¶çŸ¥è¯†äº§æƒå¹¶è·å¾—å¸‚åœºç«äº‰ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œè€ƒè™‘åˆ°å¼€å‘é«˜æ€§èƒ½ NeRF æ¨¡å‹æ‰€éœ€çš„å·¨å¤§èµ„æºã€‚åŠ å¼º NeRF æ¨¡å‹å¯¹ IPR ä¾µæƒè¡Œä¸ºçš„æŠµæŠ—å…·æœ‰å¹¿æ³›çš„ç¤¾ä¼šæ•ˆç›Šï¼ŒåŒ…æ‹¬é˜²æ­¢å‰½çªƒã€ç¡®ä¿åœ¨åŠ¨æ€å¸‚åœºç«äº‰ä¸­çš„ç«äº‰ä¼˜åŠ¿ä»¥åŠå‡å°‘æµªè´¹è¯‰è®¼æ¡ˆä»¶çš„è´Ÿæ‹…ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ç»¼åˆçš„ NeRF æ¨¡å‹çŸ¥è¯†äº§æƒä¿æŠ¤æ¡†æ¶ IPR-NERFï¼ŒåŒ…æ‹¬é»‘ç›’å’Œç™½ç›’ä¸¤ç§è®¾ç½®ã€‚</li><li>åœ¨é»‘ç›’è®¾ç½®ä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§£å†³æ–¹æ¡ˆæ¥åµŒå…¥å’Œæå–æ°´å°ï¼Œé€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µçš„ä¼˜åŒ–è¿‡ç¨‹å®ç°ã€‚</li><li>åœ¨ç™½ç›’è®¾ç½®ä¸­ï¼Œé€šè¿‡é‡‡ç”¨ç¬¦å·æŸå¤±ç›®æ ‡å‡½æ•°ï¼Œå°†æŒ‡å®šæ•°å­—ç­¾ååµŒå…¥ NeRF æ¨¡å‹çš„æƒé‡ä¸­ã€‚æ€§èƒ½ï¼š</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIPR-NERF æ¨¡å‹ä¸ä»…ä¿æŒäº†æ¸²æŸ“è´¨é‡ï¼Œè€Œä¸”åœ¨é¢å¯¹æ¨¡ç³Šæ€§å’Œå»é™¤æ”»å‡»æ—¶ä¹Ÿå…·æœ‰é²æ£’æ€§ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚å·¥ä½œé‡ï¼š</li><li>IPR-NERF æ¨¡å‹çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤§å‹æ•°æ®é›†å’Œå¤æ‚åœºæ™¯ã€‚</li><li>åœ¨é»‘ç›’è®¾ç½®ä¸­ï¼ŒåµŒå…¥å’Œæå–æ°´å°çš„è¿‡ç¨‹å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</li><li>åœ¨ç™½ç›’è®¾ç½®ä¸­ï¼Œéœ€è¦ä¿®æ”¹ NeRF æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä»¥åµŒå…¥æ•°å­—ç­¾åï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œå¤æ‚æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle"></details><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p><p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: â€œfrom where has each point been seen?â€ â€” which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p><p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é’ˆå¯¹ç¨€ç–æ— çº¦æŸè§†ç‚¹åœºæ™¯ä¸‹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹çš„å±€é™æ€§ï¼Œæœ¬æ–‡æ—¨åœ¨é‡æ„å’Œç†è§£ä¸‰ç»´åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„æ¥æºä¿¡æ¯ï¼Œå¹¶æå‡ºäº† ProvNeRF æ¨¡å‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ProvNeRF æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¼•å…¥æ¯ä¸ªç‚¹å¯èƒ½çš„æ¥æºä½ç½®ï¼Œæ¥ä¸°å¯Œä¼ ç»Ÿçš„ NeRF æ¨¡å‹ã€‚</li><li>ProvNeRF æ¨¡å‹ä¸ä»»ä½•é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹åŠå…¶ç›¸å…³çš„è®­ç»ƒç›¸æœºä½å§¿å…¼å®¹ã€‚</li><li>ProvNeRF æ¨¡å‹å¯ä»¥å¯¹æ¯ä¸ªç‚¹çš„ä¸ç¡®å®šæ€§è¿›è¡Œä¼°è®¡ã€‚</li><li>ProvNeRF æ¨¡å‹å¯ä»¥æ ¹æ®æŒ‡å®šçš„æ ‡å‡†ï¼Œé€‰æ‹©åˆé€‚çš„è§†è§’æ¥è¿›è¡Œåœºæ™¯é‡å»ºã€‚</li><li>ProvNeRF æ¨¡å‹å¯ä»¥æ”¹è¿›åœºæ™¯çš„æ–°è§†è§’åˆæˆç»“æœã€‚</li><li>ProvNeRF æ¨¡å‹çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨é¡¹ç›®ä¸»é¡µ <a href="https://provnerf.github.io">https://provnerf.github.io</a> æŸ¥çœ‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šProvNeRFï¼šå°† NeRF ä¸­çš„é€ç‚¹å‡ºå¤„å»ºæ¨¡ä¸ºéšæœºè¿‡ç¨‹</li><p></p><p></p><li>ä½œè€…ï¼šGeorge Kiyohiro Nakayamaã€Mikaela Angelina Uyã€Yang Youã€Ke Liã€Leonidas Guibas</li><p></p><p></p><li>éš¶å±æœºæ„ï¼šæ–¯å¦ç¦å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€ç¨€ç–è§†å›¾ã€å‡ºå¤„å»ºæ¨¡ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹ä¼˜åŒ–ã€æ–°é¢–è§†å›¾åˆæˆ</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.08140Github é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨å„ç§åº”ç”¨ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å®ƒä»¬åœ¨ç¨€ç–è§†å›¾æ–¹æ¡ˆä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºä»…é ä½“ç§¯æ¸²æŸ“æ— æ³•æä¾›è¶³å¤Ÿçš„çº¦æŸã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¢å¼ºé‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆä¸Šï¼Œä½†å¿½ç•¥äº†å¦‚ä½•ä»æ›´å…¨é¢çš„è§’åº¦ç†è§£åœºæ™¯ï¼Œä¾‹å¦‚ä¸ç¡®å®šæ€§ä¼°è®¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹é€‰æ‹©å’Œæ”¹è¿›çš„æ–°é¢–è§†å›¾åˆæˆã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡º ProvNeRFï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç»“åˆé€ç‚¹å‡ºå¤„æ¥ä¸°å¯Œä¼ ç»Ÿ NeRF è¡¨ç¤ºçš„æ¨¡å‹ï¼Œå¯¹æ¯ä¸ªç‚¹å»ºæ¨¡å¯èƒ½çš„æºä½ç½®ã€‚æˆ‘ä»¬é€šè¿‡æ‰©å±•éšæœºè¿‡ç¨‹çš„éšå¼æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (IMLE) æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹é€‰æ‹©å’Œæ”¹è¿›çš„æ–°é¢–è§†å›¾åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¿™è¡¨æ˜å»ºæ¨¡é€ç‚¹å‡ºå¤„å¯ä»¥æä¾›å‡ ä¸ªä¼˜åŠ¿ã€‚</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong>ï¼š**</p><p>ï¼ˆ1ï¼‰ï¼šæˆ‘ä»¬å°†ç¥ç»è¾å°„åœº (NeRF) è¡¨ç¤ºæ‰©å±•ä¸ºåŒ…å«æ¯ä¸ªç‚¹çš„å‡ºå¤„ï¼Œå³æ¯ä¸ªç‚¹çš„æ¥æºæˆ–ä»ä½•å¤„çœ‹åˆ°å®ƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæˆ‘ä»¬ä½¿ç”¨éšæœºè¿‡ç¨‹å¯¹æ¯ä¸ªç‚¹çš„å‡ºå¤„è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥éšæœºè¿‡ç¨‹ç”±åæ ‡ xâˆˆR3 ç´¢å¼•ï¼Œå…¶åœ¨ x å¤„çš„è¾¹é™…åˆ†å¸ƒç¼–ç äº† x å¤„çš„å‡ºå¤„ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬é€šè¿‡æ‰©å±•éšæœºè¿‡ç¨‹çš„éšå¼æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (IMLE) æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥ä¼°è®¡å°†æ½œåœ¨éšæœºå˜é‡çš„å˜æ¢å­¦ä¹ ä¸ºæ•°æ®åˆ†å¸ƒï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®æ ·æœ¬éƒ½æ˜¯ä¸€ä¸ªæ ‡é‡æˆ–å‘é‡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæˆ‘ä»¬æå‡º ProvNeRFï¼Œå®ƒé€šè¿‡æ‰©å±•éšå¼æ¦‚ç‡æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ IMLEï¼‰æ¥å¤„ç†éšæœºè¿‡ç¨‹ï¼Œä»è€Œå°†æ¯ä¸ªç‚¹çš„å‡ºå¤„å»ºæ¨¡ä¸ºéšæœºè¿‡ç¨‹ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šProvNeRF å­¦ä¹ ä¸€ä¸ªç¡®å®šæ€§å˜æ¢ HÎ¸ï¼šRbâ†’R+Ã—D3ï¼Œè¯¥å˜æ¢å°†æ¯ä¸ªæ½œåœ¨éšæœºå‡½æ•°æ ·æœ¬ Zâˆ¼Z æ˜ å°„åˆ°ä¸€ä¸ªå‡½æ•° DÎ¸âˆ¼DÎ¸ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šä¸ºäº†ä¼˜åŒ– DÎ¸ï¼Œæˆ‘ä»¬æ‰©å±• IMLE æ¥å¯¹éšæœºè¿‡ç¨‹çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬å°† Eq.3 è°ƒæ•´åˆ°å‡½æ•°ç©ºé—´ï¼Œå¹¶è¯æ˜å®ƒç­‰ä»·äºåœ¨æ¯ä¸ªç‚¹ x å¤„å¯¹ç»éªŒæ ·æœ¬ Ë†D(x)âˆ¼Ë†D(x) å’Œæ¨¡å‹æ ·æœ¬ DÎ¸(x)âˆ¼DÎ¸(x) è¿›è¡Œé€ç‚¹åŒ¹é…ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡º ProvNeRFï¼Œé€šè¿‡æ‰©å±•éšæœºè¿‡ç¨‹çš„ IMLE æ¥å¢å¼ºä¼ ç»Ÿ NeRF è¡¨ç¤ºï¼Œä»è€Œå°†æ¯ä¸ªç‚¹çš„å‡ºå¤„å»ºæ¨¡ä¸ºéšæœºè¿‡ç¨‹ã€‚ProvNeRF å¯è½»æ¾åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä»¥åŠç›¸å…³çš„è®­ç»ƒç›¸æœºä½å§¿ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨å„ç§ä¸‹æ¸¸åº”ç”¨ä¸­å»ºæ¨¡é€ç‚¹å‡ºå¤„çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ä¸ç¡®å®šæ€§å»ºæ¨¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹é€‰æ‹©ä»¥åŠä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”æ”¹è¿›çš„æ–°é¢–è§†å›¾åˆæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡º ProvNeRFï¼Œä¸€ç§é€šè¿‡æ‰©å±•éšæœºè¿‡ç¨‹çš„ IMLE æ¥å¢å¼ºä¼ ç»Ÿ NeRF è¡¨ç¤ºçš„æ¨¡å‹ï¼Œä»è€Œå°†æ¯ä¸ªç‚¹çš„å‡ºå¤„å»ºæ¨¡ä¸ºéšæœºè¿‡ç¨‹ã€‚</li><li>è¯æ˜äº† ProvNeRF å¯ä»¥è½»æ¾åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„ NeRF æ¨¡å‹ä»¥åŠç›¸å…³çš„è®­ç»ƒç›¸æœºä½å§¿ã€‚</li><li>å±•ç¤ºäº†åœ¨å„ç§ä¸‹æ¸¸åº”ç”¨ä¸­å»ºæ¨¡é€ç‚¹å‡ºå¤„çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ä¸ç¡®å®šæ€§å»ºæ¨¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹é€‰æ‹©ä»¥åŠä¸ç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”æ”¹è¿›çš„æ–°é¢–è§†å›¾åˆæˆã€‚</li></ol><p>æ€§èƒ½ï¼š- åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡ã€åŸºäºæ ‡å‡†çš„è§†ç‚¹é€‰æ‹©å’Œæ”¹è¿›çš„æ–°é¢–è§†å›¾åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼š- éœ€è¦æ‰©å±•éšæœºè¿‡ç¨‹çš„ IMLE æ¥å¯¹éšæœºè¿‡ç¨‹çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚- éœ€è¦è°ƒæ•´ Eq.3 åˆ°å‡½æ•°ç©ºé—´ï¼Œå¹¶è¯æ˜å®ƒç­‰ä»·äºåœ¨æ¯ä¸ªç‚¹ x å¤„å¯¹ç»éªŒæ ·æœ¬ Ë†D(x)âˆ¼Ë†D(x) å’Œæ¨¡å‹æ ·æœ¬ DÎ¸(x)âˆ¼DÎ¸(x) è¿›è¡Œé€ç‚¹åŒ¹é…ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-01-30  Divide and Conquer Rethinking the Training Paradigm of Neural Radiance   Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/</id>
    <published>2024-01-30T11:06:29.000Z</published>
    <updated>2024-01-30T12:00:56.153Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-01-30-æ›´æ–°"><a href="#2024-01-30-æ›´æ–°" class="headerlink" title="2024-01-30 æ›´æ–°"></a>2024-01-30 æ›´æ–°</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p><p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é«˜æ–¯æ•£ç‚¹ç»“åˆç¥ç»è¾å°„åœºï¼Œå®ç°åŠ¨æ€å†…çª¥é•œ 3D é‡å»ºæ–°æ–¹æ³•ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>EndoGaussians æ˜¯ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é«˜æ–¯æ•£ç‚¹è¿›è¡ŒåŠ¨æ€å†…çª¥é•œ 3D é‡å»ºã€‚</li><li>è¿™ç§æ–¹æ³•æ˜¯é¦–æ¬¡åœ¨è¯¥èƒŒæ™¯ä¸‹ä½¿ç”¨é«˜æ–¯æ•£ç‚¹ï¼Œå…‹æœäº†ä»¥å‰åŸºäº NeRF æŠ€æœ¯çš„é™åˆ¶ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å„ç§å†…çª¥é•œæ•°æ®é›†ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ï¼Œæ ‘ç«‹äº†æ–°çš„æœ€å…ˆè¿›æ ‡å‡†ã€‚</li><li>è¿™äº›è¿›æ­¥ä½¿è¯¥æ–¹æ³•æˆä¸ºåŒ»ç–—ä¸“ä¸šäººå‘˜çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œä¸ºåŒ»ç–—é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†æ›´å¯é ã€æ›´é«˜æ•ˆçš„ 3D é‡å»ºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šEndoGaussiansï¼šå•è§†åŠ¨æ€é«˜æ–¯ä½“ç´ é‡å»º</li><li>ä½œè€…ï¼šYangsen Chen, Hao Wang</li><li>éš¶å±æœºæ„ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€é«˜æ–¯ä½“ç´ é‡å»ºã€æœºå™¨äººæ‰‹æœ¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.13352</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå‡†ç¡®åœ°ä»å†…çª¥é•œè§†é¢‘ä¸­é‡å»ºå¯å˜å½¢è½¯ä½“ç»„ç»‡çš„ 3D æ¨¡å‹å¯¹äº VR æ‰‹æœ¯å’ŒåŒ»å­¦å›¾åƒåˆ†æç­‰åŒ»ç–—åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å‡†ç¡®æ€§å’Œäº§ç”Ÿçš„ç»„ç»‡éƒ¨åˆ†çš„æ¨¡æ£±ä¸¤å¯æ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å®é™…æ•ˆç”¨ã€‚(2) è¿‡å¾€æ–¹æ³•ï¼šä»¥å¾€çš„ä¸€äº›å·¥ä½œå°è¯•ä½¿ç”¨æ·±åº¦ä¼°è®¡ã€SLAMã€ç¨€ç–å˜å½¢åœºå’Œç¥ç»è¾å°„åœºç­‰æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•è¦ä¹ˆå‡è®¾åœºæ™¯æ˜¯é™æ€çš„ï¼Œè¦ä¹ˆå‡è®¾æ‰‹æœ¯å·¥å…·ä¸å­˜åœ¨ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨å®é™…åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è¿›ä¸€æ­¥æé«˜é™æ€å•è§† RGBD è®¾ç½®ä¸‹è½¯ä½“ç»„ç»‡çš„ 3D é‡å»ºçš„å‡†ç¡®æ€§ï¼Œå¹¶æé«˜ 3D é‡å»ºçš„å¯é æ€§å’Œå¯ä¿¡åº¦ï¼Œæˆ‘ä»¬æå‡ºäº† Endogaussiansï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é«˜æ–¯ä½“ç´ é‡å»ºä½œä¸ºé‡å»ºæ–¹æ³•ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PSNRã€SSIMã€LPIPS ç­‰å¤šé¡¹å®šé‡è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸”é‡å»ºé€Ÿåº¦æ›´å¿«ã€‚è¿™äº›è¿›æ­¥ä½¿æˆ‘ä»¬çš„æ–¹æ³•æˆä¸ºåŒ»ç–—ä¸“ä¸šäººå‘˜çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œä¸ºåŒ»ç–—é¢†åŸŸçš„å®é™…åº”ç”¨æä¾›æ›´å¯é å’Œé«˜æ•ˆçš„ 3D é‡å»ºã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Endogaussians çš„æ–¹æ³•ï¼Œç”¨äºä»å•ç›®åŠ¨æ€ RGBD è®¾ç½®ä¸­é‡å»ºå¯å˜å½¢è½¯ä½“ç»„ç»‡çš„ 3D æ¨¡å‹ã€‚(2): è¯¥æ–¹æ³•ä½¿ç”¨é«˜æ–¯ä½“ç´ é‡å»ºä½œä¸ºé‡å»ºæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è½¯ä½“ç»„ç»‡çš„å˜å½¢ã€‚(3): ä¸ºäº†æé«˜é‡å»ºçš„å‡†ç¡®æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒå¸§çš„æ•°æ®ã€‚(4): æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ åˆ†å‰²ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å°†è½¯ä½“ç»„ç»‡åˆ†å‰²æˆä¸åŒçš„éƒ¨åˆ†ã€‚(5): æœ€åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ æ¸²æŸ“ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç”Ÿæˆé€¼çœŸçš„è½¯ä½“ç»„ç»‡æ¨¡å‹ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šEndoGaussiansæ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä»å•ç›®åŠ¨æ€RGBDè®¾ç½®ä¸­é‡å»ºå¯å˜å½¢è½¯ä½“ç»„ç»‡çš„3Dæ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œåœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯ä½“ç´ é‡å»ºæ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†è½¯ä½“ç»„ç»‡çš„å˜å½¢ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ èåˆç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°èåˆæ¥è‡ªä¸åŒå¸§çš„æ•°æ®ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ åˆ†å‰²ç®—æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è½¯ä½“ç»„ç»‡åˆ†å‰²æˆä¸åŒçš„éƒ¨åˆ†ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ä½“ç´ æ¸²æŸ“ç®—æ³•ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸçš„è½¯ä½“ç»„ç»‡æ¨¡å‹ã€‚æ€§èƒ½ï¼š</li><li>åœ¨PSNRã€SSIMã€LPIPSç­‰å¤šé¡¹å®šé‡è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>é‡å»ºé€Ÿåº¦æ›´å¿«ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details><h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p><p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>. </p><p><a href="http://arxiv.org/abs/2401.12561v1">PDF</a> </p><p><strong>Summary</strong><br>3Dé«˜æ–¯æ¸²æŸ“æ¡†æ¶å®ç°äº†å®æ—¶å†…çª¥é•œæ‰‹æœ¯åœºæ™¯é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åä¸ºEndoGaussiançš„å®æ—¶æ‰‹æœ¯åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œå®ƒæ˜¯å»ºç«‹åœ¨3Dé«˜æ–¯ç‚¹é˜µçš„åŸºç¡€ä¸Šçš„ã€‚</li><li>ä½¿ç”¨é«˜æ–¯è¡¨ç¤ºå’Œå¹¶è¡Œæ¸²æŸ“ç®¡é“ï¼Œæ˜¾è‘—æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚</li><li>å°†å˜å½¢åœºè®¾è®¡ä¸ºè½»é‡çº§ç¼–ç ä½“ç´ å’Œæå°MLPçš„ç»„åˆï¼Œå®ç°äº†é«˜æ•ˆçš„é«˜æ–¯è·Ÿè¸ªï¼Œæ¸²æŸ“è´Ÿæ‹…è¾ƒå°ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ•´ä½“çš„é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨äº†è¡¨é¢åˆ†å¸ƒå…ˆéªŒï¼Œé€šè¿‡æœç´¢è¾“å…¥å›¾åƒåºåˆ—ä¸­çš„ä¿¡æ¯ç‚¹æ¥å®ç°ã€‚</li><li>å…¬å…±å†…çª¥é•œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ˆ195 FPSå®æ—¶ï¼Œ100å€æ”¶ç›Šï¼‰ï¼ŒåŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆ35.925 PSNRï¼‰å’Œæœ€å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆåœ¨2åˆ†é’Ÿ/åœºæ™¯ä»¥å†…ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœ¯ä¸­æ‰‹æœ¯åº”ç”¨çš„é‡å¤§å‰æ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šEndoGaussianï¼šç”¨äºå¯å˜å½¢æ‰‹æœ¯åœºæ™¯é‡å»ºçš„é«˜æ–¯ç‚¹äº‘</li><li>ä½œè€…ï¼šYifan Liu<em>, Chenxin Li</em>, Chen Yang å’Œ Yixuan Yuan</li><li>éš¶å±æœºæ„ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦</li><li>å…³é”®è¯ï¼šä¸‰ç»´é‡å»º Â· é«˜æ–¯ç‚¹äº‘ Â· æœºå™¨äººæ‰‹æœ¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.12561   Github ä»£ç é“¾æ¥ï¼šhttps://yifliu3.github.io/EndoGaussian/</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å†…çª¥é•œç«‹ä½“è§†é¢‘ä¸­é‡å»ºå¯å˜å½¢ç»„ç»‡å¯¹äºè®¸å¤šä¸‹æ¸¸æ‰‹æœ¯åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„æ¨ç†é€Ÿåº¦æ…¢ï¼Œæå¤§åœ°é™åˆ¶äº†å®ƒä»¬çš„å®é™…ä½¿ç”¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•çš„é—®é¢˜åœ¨äºæ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­å—åˆ°é™åˆ¶ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„å®æ—¶æ‰‹æœ¯åœºæ™¯é‡å»ºæ¡†æ¶ EndoGaussianã€‚è¯¥æ¡†æ¶å°†åŠ¨æ€æ‰‹æœ¯åœºæ™¯è¡¨ç¤ºä¸ºè§„èŒƒé«˜æ–¯ç‚¹äº‘å’Œæ—¶é—´ç›¸å…³çš„å˜å½¢åœºï¼Œè¯¥å˜å½¢åœºå¯ä»¥é¢„æµ‹æ–°æ—¶é—´æˆ³ä¸‹çš„é«˜æ–¯å˜å½¢ã€‚ç”±äºé«˜æ•ˆçš„é«˜æ–¯è¡¨ç¤ºå’Œå¹¶è¡Œæ¸²æŸ“ç®¡é“ï¼Œè¯¥æ¡†æ¶ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—åœ°æé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡å°†å˜å½¢åœºè®¾è®¡ä¸ºè½»é‡çº§ç¼–ç ä½“ç´ å’Œæå°å‹çš„ MLP çš„ç»„åˆï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é«˜æ–¯è·Ÿè¸ªï¼Œä¸”æ¸²æŸ“è´Ÿæ‹…å¾ˆå°ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è®¾è®¡äº†ä¸€ç§æ•´ä½“çš„é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•ï¼Œä»¥å……åˆ†åˆ©ç”¨è¡¨é¢åˆ†å¸ƒå…ˆéªŒï¼Œè¯¥æ–¹æ³•é€šè¿‡ä»è¾“å…¥å›¾åƒåºåˆ—ä¸­æœç´¢ä¿¡æ¯ç‚¹æ¥å®ç°ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å…¬å¼€å†…çª¥é•œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ˆ195 FPS å®æ—¶ï¼Œ100 å€å¢ç›Šï¼‰ï¼ŒåŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆ35.925 PSNRï¼‰å’Œæœ€å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯ä¸ªåœºæ™¯ 2 åˆ†é’Ÿä»¥å†…ï¼‰ï¼Œæ˜¾ç¤ºå‡ºå¯¹æœ¯ä¸­æ‰‹æœ¯åº”ç”¨çš„é‡å¤§å‰æ™¯ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰EndoGaussianæ¡†æ¶æ¦‚è¿°ï¼šè¯¥æ¡†æ¶ç”±é«˜æ–¯ç‚¹äº‘åˆå§‹åŒ–ã€é«˜æ–¯è·Ÿè¸ªå’Œé«˜æ–¯æ¸²æŸ“ä¸‰ä¸ªæ¨¡å—ç»„æˆã€‚ï¼ˆ2ï¼‰é«˜æ–¯ç‚¹äº‘åˆå§‹åŒ–ï¼šä»è¾“å…¥å›¾åƒåºåˆ—ä¸­æœç´¢ä¿¡æ¯ç‚¹ï¼Œé€šè¿‡é«˜æ–¯æ··åˆæ¨¡å‹ä¼°è®¡ç‚¹äº‘å‚æ•°ï¼Œå¹¶é€šè¿‡è¡¨é¢åˆ†å¸ƒå…ˆéªŒä¼˜åŒ–ç‚¹äº‘ä½ç½®ã€‚ï¼ˆ3ï¼‰é«˜æ–¯è·Ÿè¸ªï¼šå°†å˜å½¢åœºè®¾è®¡ä¸ºè½»é‡çº§ç¼–ç ä½“ç´ å’Œæå°å‹çš„MLPçš„ç»„åˆï¼Œé€šè¿‡å°†å½“å‰æ—¶é—´æˆ³çš„é«˜æ–¯ç‚¹äº‘å˜å½¢åˆ°æ–°æ—¶é—´æˆ³ï¼Œå®ç°é«˜æ•ˆçš„é«˜æ–¯è·Ÿè¸ªã€‚ï¼ˆ4ï¼‰é«˜æ–¯æ¸²æŸ“ï¼šåˆ©ç”¨é«˜æ–¯ç‚¹äº‘çš„å‡ ä½•ç‰¹æ€§å’Œå¹¶è¡Œæ¸²æŸ“ç®¡é“ï¼Œå®ç°é«˜æ•ˆçš„æ¸²æŸ“ã€‚ï¼ˆ5ï¼‰è®­ç»ƒç»†èŠ‚ï¼šä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ä¸º1e-4ï¼Œæ‰¹å¤§å°ä¸º8ï¼Œè®­ç»ƒ200ä¸ªå‘¨æœŸã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§å®æ—¶ä¸”é«˜è´¨é‡çš„ 4D é‡å»ºæ¡†æ¶ï¼Œç”¨äºåŠ¨æ€æ‰‹æœ¯åœºæ™¯é‡å»ºã€‚é€šè¿‡åˆ©ç”¨åŸºäºä½“ç´ çš„é«˜æ–¯è·Ÿè¸ªå’Œæ•´ä½“é«˜æ–¯åˆå§‹åŒ–ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¤„ç†ç»„ç»‡å˜å½¢å’Œéå¹³å‡¡çš„é«˜æ–¯åˆå§‹åŒ–é—®é¢˜ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ EndoGaussian å¯ä»¥å®ç°æœ€å…ˆè¿›çš„é‡å»ºè´¨é‡å’Œå®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ¯”ä»¥å‰çš„æ–¹æ³•å¿« 100 å€ä»¥ä¸Šã€‚æˆ‘ä»¬å¸Œæœ›æ–°å…´çš„åŸºäºé«˜æ–¯æ–‘ç‚¹çš„é‡å»ºæŠ€æœ¯èƒ½å¤Ÿä¸ºæœºå™¨äººæ‰‹æœ¯åœºæ™¯ç†è§£æä¾›æ–°çš„é€”å¾„ï¼Œå¹¶å¢å¼ºå„ç§ä¸‹æ¸¸ä¸´åºŠä»»åŠ¡ï¼Œå°¤å…¶æ˜¯æœ¯ä¸­åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>åŸºäºé«˜æ–¯ç‚¹äº‘çš„å®æ—¶æ‰‹æœ¯åœºæ™¯é‡å»ºæ¡†æ¶ã€‚</li><li>ä½“ç´ ç¼–ç çš„é«˜æ–¯è·Ÿè¸ªï¼Œå®ç°äº†é«˜æ•ˆçš„é«˜æ–¯è·Ÿè¸ªã€‚</li><li>æ•´ä½“é«˜æ–¯åˆå§‹åŒ–æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨è¡¨é¢åˆ†å¸ƒå…ˆéªŒã€‚æ€§èƒ½ï¼š</li><li>åœ¨å…¬å¼€å†…çª¥é•œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ˆ195FPS å®æ—¶ï¼Œ100 å€å¢ç›Šï¼‰ï¼ŒåŒæ—¶ä¿æŒæœ€å…ˆè¿›çš„é‡å»ºè´¨é‡ï¼ˆ35.925PSNRï¼‰å’Œæœ€å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆæ¯ä¸ªåœºæ™¯ 2 åˆ†é’Ÿä»¥å†…ï¼‰ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡çš„ä»£ç å’Œæ•°æ®å·²ç»å¼€æºï¼Œå¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œå¤ç°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle"></details><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>æ‘˜è¦</strong><br>åŠ¨æ€é«˜æ–¯æº…å°„ç”¨äºå¯å˜å½¢å†…çª¥é•œç»„ç»‡é‡å»ºã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>EndoGS åˆ©ç”¨é«˜æ–¯æº…å°„è¿›è¡Œå¯å˜å½¢å†…çª¥é•œç»„ç»‡é‡å»ºã€‚</li><li>è¯¥æ–¹æ³•ç»“åˆå˜å½¢åœºä»¥å¤„ç†åŠ¨æ€åœºæ™¯ã€‚</li><li>æ·±åº¦å¼•å¯¼ç›‘ç£ç”¨äºä¼˜åŒ–å…·æœ‰å•ä¸ªè§†ç‚¹çš„ 3D ç›®æ ‡ã€‚</li><li>æ—¶ç©ºæƒé‡æ©ç å¯å‡è½»å·¥å…·é®æŒ¡ã€‚</li><li>EndoGS å¯ä»¥ä»å•è§†è§’è§†é¢‘ã€ä¼°è®¡çš„æ·±åº¦å›¾å’Œæ ‡è®°çš„å·¥å…·æ©ç ä¸­é‡å»ºå’Œæ¸²æŸ“é«˜è´¨é‡çš„å¯å˜å½¢å†…çª¥é•œç»„ç»‡ã€‚</li><li>åœ¨ DaVinci æœºå™¨äººæ‰‹æœ¯è§†é¢‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEndoGS å®ç°å“è¶Šçš„æ¸²æŸ“è´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯æ–‘ç‚¹å¯å˜å½¢å†…çª¥é•œç»„ç»‡é‡å»º</li><li>ä½œè€…ï¼šLingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ–‘ç‚¹Â·æœºå™¨äººæ‰‹æœ¯Â·ä¸‰ç»´é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.11535, Github é“¾æ¥ï¼šhttps://github.com/HKU-MedAI/EndoGS</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰‹æœ¯ä¸‰ç»´é‡å»ºæ˜¯æœºå™¨äººæ‰‹æœ¯ç ”ç©¶çš„ä¸€ä¸ªå…³é”®é¢†åŸŸï¼Œæœ€è¿‘çš„å·¥ä½œé‡‡ç”¨åŠ¨æ€è¾å°„åœºå®ç°ä»å•è§†è§’è§†é¢‘ä¸­å¯å˜å½¢ç»„ç»‡çš„ä¸‰ç»´é‡å»ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨ä¼˜åŒ–è€—æ—¶æˆ–è´¨é‡è¾ƒå·®çš„é—®é¢˜ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åç»­ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šæ—©æœŸå°è¯•é‡‡ç”¨æ·±åº¦ä¼°è®¡åœ¨å†…çª¥é•œé‡å»ºä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•ä»ç„¶éš¾ä»¥äº§ç”Ÿé€¼çœŸçš„ä¸‰ç»´é‡å»ºï¼ŒåŸå› æœ‰ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚é¦–å…ˆï¼Œéåˆšæ€§å˜å½¢æœ‰æ—¶ä¼šå¯¼è‡´è¾ƒå¤§çš„è¿åŠ¨ï¼Œè¿™éœ€è¦å®é™…åŠ¨æ€åœºæ™¯é‡å»ºï¼Œè¿™é˜»ç¢äº†è¿™äº›æŠ€æœ¯çš„é€‚åº”ã€‚å…¶æ¬¡ï¼Œå•è§†è§’è§†é¢‘ä¸­å­˜åœ¨é®æŒ¡ï¼Œå¯¼è‡´å­¦ä¹ å—å½±å“éƒ¨åˆ†æ—¶ä¿¡æ¯æœ‰é™ï¼Œäº§ç”Ÿå›°éš¾ã€‚è™½ç„¶ä¸€äº›æ¡†æ¶ç»“åˆäº†å·¥å…·é®ç½©ã€ç«‹ä½“æ·±åº¦ä¼°è®¡å’Œç¨€ç–ç¿˜æ›²åœºç”¨äºå•è§†è§’ä¸‰ç»´é‡å»ºï¼Œä½†å®ƒä»¬åœ¨å­˜åœ¨å‰§çƒˆéæ‹“æ‰‘å¯å˜å½¢ç»„ç»‡å˜åŒ–æ—¶ä»ç„¶å®¹æ˜“å¤±è´¥ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šå—æœ€è¿‘æµè¡Œçš„ä¸‰ç»´è¡¨ç¤ºæ–¹æ³•ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† EndoGSï¼Œå°†é«˜æ–¯æ–‘ç‚¹åº”ç”¨äºå¯å˜å½¢å†…çª¥é•œç»„ç»‡é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å˜å½¢åœºæ¥å¤„ç†åŠ¨æ€åœºæ™¯ï¼Œæ·±åº¦å¼•å¯¼ç›‘ç£æ¥ä¼˜åŒ–å…·æœ‰å•ä¸€è§†ç‚¹çš„ä¸‰ç»´ç›®æ ‡ï¼Œä»¥åŠæ—¶ç©ºæƒé‡æ©ç æ¥å‡è½»é®æŒ¡ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨è¾¾èŠ¬å¥‡æœºå™¨äººæ‰‹æœ¯è§†é¢‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEndoGS å®ç°äº†æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚</li></ol><p>Methods:(1): æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸º EndoGS çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ 3D-GS çš„å¯å˜å½¢å˜ä½“ä»å•è§†è§’è§†é¢‘ã€ä¼°è®¡çš„æ·±åº¦å›¾å’Œæ ‡è®°çš„å·¥å…·æ©ç ä¸­é‡å»º 3D å¤–ç§‘åœºæ™¯ã€‚(2): æˆ‘ä»¬é¦–å…ˆä»‹ç»äº† 3D-GS çš„é¢„å¤‡çŸ¥è¯†ï¼Œç„¶åå±•ç¤ºäº†ä½¿ç”¨åŠ¨æ€ç‰ˆæœ¬çš„ 3D-GS å¯¹å¯å˜å½¢ç»„ç»‡è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥ç‰ˆæœ¬é‡‡ç”¨è½»é‡çº§ MLP æ¥è¡¨ç¤ºåŠ¨æ€åœºã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä½¿ç”¨å·¥å…·æ©ç å’Œæ·±åº¦å›¾å¯¹é«˜æ–¯é£æº…è¿›è¡Œè®­ç»ƒä¼˜åŒ–çš„è¿‡ç¨‹ã€‚(3): æˆ‘ä»¬ä½¿ç”¨å…­ä¸ªæ­£äº¤ç‰¹å¾å¹³é¢æ¥ç¼–ç ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å•ä¸ª MLP æ¥æ›´æ–°é«˜æ–¯å±æ€§å¹¶è§£ç ä½ç½®ã€æ¯”ä¾‹å› å­ã€æ—‹è½¬å› å­ã€çƒè°ç³»æ•°å’Œä¸é€æ˜åº¦çš„å˜å½¢ã€‚(4): æˆ‘ä»¬ç»“åˆå·¥å…·æ©ç å’Œæ·±åº¦å›¾æ¥è®­ç»ƒ EndoGSï¼Œä»¥è§£å†³è§†é¢‘ä¸­å·¥å…·é®æŒ¡çš„æŒ‘æˆ˜ï¼Œå¹¶ä½¿ç”¨æ—¶ç©ºé‡è¦æ€§é‡‡æ ·ç­–ç•¥æ¥æŒ‡ç¤ºä¸é®æŒ¡é—®é¢˜ç›¸å…³çš„å…³é”®åŒºåŸŸã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</li></ol><p>å…·ä½“å†…å®¹å¦‚ä¸‹ï¼š</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ–‘ç‚¹å¯å˜å½¢å†…çª¥é•œç»„ç»‡é‡å»ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å•è§†è§’è§†é¢‘ã€ä¼°è®¡çš„æ·±åº¦å›¾å’Œæ ‡è®°çš„å·¥å…·æ©ç ä¸­å®æ—¶æ¸²æŸ“é«˜è´¨é‡çš„å¯å˜å½¢ç»„ç»‡ã€‚åœ¨è¾¾èŠ¬å¥‡æœºå™¨äººæ‰‹æœ¯è§†é¢‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•EndoGSï¼Œåˆ©ç”¨3D-GSçš„å¯å˜å½¢å˜ä½“ä»å•è§†è§’è§†é¢‘ã€ä¼°è®¡çš„æ·±åº¦å›¾å’Œæ ‡è®°çš„å·¥å…·æ©ç ä¸­é‡å»º3Då¤–ç§‘åœºæ™¯ã€‚</li><li>ä½¿ç”¨åŠ¨æ€ç‰ˆæœ¬çš„3D-GSå¯¹å¯å˜å½¢ç»„ç»‡è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥ç‰ˆæœ¬é‡‡ç”¨è½»é‡çº§MLPæ¥è¡¨ç¤ºåŠ¨æ€åœºã€‚</li><li>ç»“åˆå·¥å…·æ©ç å’Œæ·±åº¦å›¾å¯¹é«˜æ–¯é£æº…è¿›è¡Œè®­ç»ƒä¼˜åŒ–ï¼Œä»¥è§£å†³è§†é¢‘ä¸­å·¥å…·é®æŒ¡çš„æŒ‘æˆ˜ï¼Œå¹¶ä½¿ç”¨æ—¶ç©ºé‡è¦æ€§é‡‡æ ·ç­–ç•¥æ¥æŒ‡ç¤ºä¸é®æŒ¡é—®é¢˜ç›¸å…³çš„å…³é”®åŒºåŸŸã€‚æ€§èƒ½ï¼š</li><li>åœ¨è¾¾èŠ¬å¥‡æœºå™¨äººæ‰‹æœ¯è§†é¢‘ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEndoGSå®ç°äº†æ›´é«˜çš„æ¸²æŸ“è´¨é‡ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦é¢„å…ˆè®­ç»ƒ3D-GSæ¨¡å‹ï¼Œå¹¶å¯¹æ¯ä¸ªæ–°åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚</li><li>ä¼˜åŒ–è¿‡ç¨‹éœ€è¦ä¸€å®šçš„æ—¶é—´ï¼Œå…·ä½“å–å†³äºåœºæ™¯çš„å¤æ‚æ€§å’Œæ•°æ®é‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details><h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen</strong></p><p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p><p><a href="http://arxiv.org/abs/2401.09720v2">PDF</a> </p><p><strong>Summary</strong><br>ä¼˜åŒ–åŠ¨æ€ç©¿è¡£äººä½“é‡å»ºæ–¹æ³•ï¼Œå¼•å…¥ç‰©ç†å…ˆéªŒå’Œè§„èŒƒåŒ–å˜æ¢ï¼Œå®ç°é«˜ç²¾åº¦ç…§ç‰‡çº§æ–°è§†è§’æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç©¿è¡£äººé«”é‡å»ºæ–¹æ³• GaussianBodyï¼ŒåŸºæ–¼ 3D é«˜æ–¯ Splattingã€‚</li><li>3D é«˜æ–¯ Splatting æœ€è¿‘åœ¨è¨“ç·´æ™‚é–“å’Œæ¸²æŸ“è³ªé‡æ–¹é¢è¡¨ç¾å‡ºäº†å¾ˆå¥½çš„æ€§èƒ½ã€‚</li><li>æ‡‰ç”¨éœæ…‹ 3D é«˜æ–¯ Splatting æ¨¡å‹æ–¼å‹•æ…‹äººé«”é‡å»ºå•é¡Œæ™‚ï¼Œæœƒå› è¤‡é›œçš„éå‰›æ€§è®Šå½¢å’Œè±å¯Œçš„è¡£ç‰©ç´°ç¯€è€Œé‡åˆ°æŒ‘æˆ°ã€‚</li><li>æå‡ºæ˜ç¢ºçš„å§¿å‹¢å¼•å°è®Šå½¢ï¼Œä»¥é—œè¯è¦ç¯„ç©ºé–“å’Œè§€æ¸¬ç©ºé–“ä¸­çš„å‹•æ…‹é«˜æ–¯ã€‚</li><li>å¼•å…¥åŸºæ–¼ç‰©ç†çš„å…ˆé©—å’Œæ­£å‰‡åŒ–è®Šæ›ï¼Œä»¥æ¸›å°‘å…©å€‹ç©ºé–“ä¹‹é–“çš„æ­§ç¾©ã€‚</li><li>æå‡ºå§¿å‹¢ç²¾ç…‰ç­–ç•¥ï¼Œä»¥æ›´æ–°å§¿å‹¢å›æ­¸ï¼Œä»¥è£œå„Ÿä¸æº–ç¢ºçš„åˆå§‹ä¼°è¨ˆã€‚</li><li>æå‡ºåˆ†æ‹†æ¯”ä¾‹æ©Ÿåˆ¶ï¼Œä»¥å¢å¼·å›æ­¸é»é›²çš„å¯†åº¦ã€‚</li><li>å¯¦é©—è­‰æ˜ï¼Œè©²æ–¹æ³•å¯å¯¦ç¾æœ€å…ˆé€²ç…§ç‰‡ç´šçš„æ–°è¦–åœ–æ¸²æŸ“çµæœï¼ŒåŒæ™‚å…·æœ‰é«˜è³ªé‡çš„å‹•æ…‹ç©¿è¡£äººé«”ç´°ç¯€å’Œæ˜ç¢ºçš„å¹¾ä½•é‡å»ºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯ä½“ï¼šåŸºäº 3D é«˜æ–¯æ•£å¸ƒçš„ç€è£…äººä½“é‡å»º</li><li>ä½œè€…ï¼šææ¢¦æ·»ã€å§šèƒœç¥¥ã€è°¢å¿—å³°ã€é™ˆå¯å®‡</li><li>éš¶å±å•ä½ï¼šä¸Šæµ·å¤§å­¦</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æ•£å¸ƒã€ç€è£…äººä½“é‡å»ºã€å•ç›®è§†é¢‘ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.09720ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåˆ›å»ºé«˜ä¿çœŸç€è£…äººä½“æ¨¡å‹åœ¨è™šæ‹Ÿç°å®ã€è¿œç¨‹ä¸´åœºå’Œç”µå½±åˆ¶ä½œä¸­å…·æœ‰é‡è¦åº”ç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆæ¶‰åŠå¤æ‚çš„æ•æ‰ç³»ç»Ÿï¼Œè¦ä¹ˆéœ€è¦ 3D è‰ºæœ¯å®¶è¿›è¡Œç¹ççš„æ‰‹å·¥æ“ä½œï¼Œè¿™ä½¿å¾—å®ƒä»¬æ—¢è€—æ—¶åˆæ˜‚è´µï¼Œä»è€Œé™åˆ¶äº†æ–°æ‰‹ç”¨æˆ·çš„å¯æ‰©å±•æ€§ã€‚è¿‘å¹´æ¥ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨ä»å•ä¸ª RGB å›¾åƒæˆ–å•ç›®è§†é¢‘ä¸­è‡ªåŠ¨é‡å»ºç€è£…äººä½“æ¨¡å‹ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç½‘æ ¼æ¨¡å‹æ–¹æ³•æœ€åˆè¢«å¼•å…¥ï¼Œé€šè¿‡å›å½’ SCAPEã€SMPLã€SMPL-X å’Œ STAR ç­‰å‚æ•°æ¨¡å‹æ¥é‡æ„äººä½“å½¢çŠ¶ã€‚è™½ç„¶å®ƒä»¬å¯ä»¥å®ç°å¿«é€Ÿä¸”ç¨³å¥çš„é‡å»ºï¼Œä½†å›å½’çš„å¤šè¾¹å½¢ç½‘æ ¼éš¾ä»¥æ•æ‰ä¸åŒçš„å‡ ä½•ç»†èŠ‚å’Œä¸°å¯Œçš„æœè£…ç‰¹å¾ã€‚æ·»åŠ é¡¶ç‚¹åç§»é‡æˆä¸ºè¿™ç§æƒ…å†µä¸‹çš„ä¸€ç§å¢å¼ºè§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…¶è¡¨ç¤ºèƒ½åŠ›ä»ç„¶å—åˆ°ç½‘æ ¼åˆ†è¾¨ç‡çš„ä¸¥æ ¼é™åˆ¶ï¼Œå¹¶ä¸”é€šå¸¸åœ¨å®½æ¾æœè£…çš„æƒ…å†µä¸‹ä¼šå¤±è´¥ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å…‹æœæ˜¾å¼ç½‘æ ¼æ¨¡å‹çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ•£å¸ƒçš„æ–°é¢–ç€è£…äººä½“é‡å»ºæ–¹æ³• GaussianBodyã€‚ä¸ä»£ä»·é«˜æ˜‚çš„ç¥ç»è¾å°„åœºæ¨¡å‹ç›¸æ¯”ï¼Œ3D é«˜æ–¯æ•£å¸ƒæœ€è¿‘åœ¨è®­ç»ƒæ—¶é—´å’Œæ¸²æŸ“è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†é™æ€ 3D é«˜æ–¯æ•£å¸ƒæ¨¡å‹åº”ç”¨äºåŠ¨æ€äººä½“é‡å»ºé—®é¢˜ç”±äºå¤æ‚çš„éåˆšæ€§å˜å½¢å’Œä¸°å¯Œçš„æœè£…ç»†èŠ‚è€Œå˜å¾—éå¸¸å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•è€ƒè™‘äº†æ˜¾å¼å§¿åŠ¿å¼•å¯¼çš„å˜å½¢ï¼Œå°†åŠ¨æ€é«˜æ–¯ä½“ä¸è§„èŒƒç©ºé—´å’Œè§‚å¯Ÿç©ºé—´ç›¸å…³è”ï¼Œå¼•å…¥å…·æœ‰æ­£åˆ™åŒ–å˜æ¢çš„åŸºäºç‰©ç†çš„å…ˆéªŒæœ‰åŠ©äºå‡è½»è¿™ä¸¤ä¸ªç©ºé—´ä¹‹é—´çš„æ­§ä¹‰ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å§¿åŠ¿ç»†åŒ–ç­–ç•¥ï¼Œä»¥æ›´æ–°å§¿åŠ¿å›å½’ï¼Œä»¥è¡¥å¿ä¸å‡†ç¡®çš„åˆå§‹ä¼°è®¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†è£‚å°ºåº¦æœºåˆ¶æ¥å¢å¼ºå›å½’ç‚¹äº‘çš„å¯†åº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•çš„åº”ç”¨ä»»åŠ¡å’Œæ€§èƒ½ï¼šå®éªŒéªŒè¯äº†æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥å®ç°æœ€å…ˆè¿›çš„é€¼çœŸæ–°è§†å›¾æ¸²æŸ“ç»“æœï¼Œä¸ºåŠ¨æ€ç€è£…äººä½“æä¾›é«˜è´¨é‡çš„ç»†èŠ‚ï¼Œä»¥åŠæ˜¾å¼å‡ ä½•é‡å»ºã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ•£å¸ƒçš„ç€è£…äººä½“é‡å»ºæ–¹æ³• GaussianBodyï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€çš„ç€è£…äººä½“æ¨¡å‹ï¼Œå¹¶å…·æœ‰é€¼çœŸçš„æ–°è§†å›¾æ¸²æŸ“ç»“æœå’Œé«˜è´¨é‡çš„ç»†èŠ‚ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>å°† 3D é«˜æ–¯æ•£å¸ƒè¡¨ç¤ºæ‰©å±•åˆ°ç€è£…äººä½“é‡å»ºä¸­ï¼Œå¹¶è€ƒè™‘äº†æ˜¾å¼å§¿åŠ¿å¼•å¯¼çš„å˜å½¢ï¼Œä»¥è§£å†³åŠ¨æ€é«˜æ–¯ä½“ä¸è§„èŒƒç©ºé—´å’Œè§‚å¯Ÿç©ºé—´ä¹‹é—´çš„æ­§ä¹‰é—®é¢˜ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„å…ˆéªŒæ¥æ­£åˆ™åŒ–è§„èŒƒç©ºé—´çš„é«˜æ–¯ä½“ï¼Œä»¥å‡è½»è§‚å¯Ÿç©ºé—´å’Œè§„èŒƒç©ºé—´ä¹‹é—´çš„è¿‡åº¦æ—‹è½¬é—®é¢˜ã€‚</li><li>æå‡ºäº†ä¸€ç§å§¿åŠ¿ç»†åŒ–ç­–ç•¥å’Œåˆ†è£‚å°ºåº¦æœºåˆ¶ï¼Œä»¥å¢å¼ºé‡å»ºç‚¹äº‘çš„è´¨é‡å’Œé²æ£’æ€§ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¸åŸºçº¿å’Œå…¶ä»–æ–¹æ³•ç›¸å½“ï¼Œè¯æ˜äº†å…¶å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½å’Œç›¸å¯¹è¾ƒå¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒè¿›è¡Œè®­ç»ƒã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒæ—¶é—´æ¯”ä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•æ›´é•¿ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-03cb35c9ffdf24e162bbcf10081d440a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4877b53e7d23cf29d6e9a1a57a3155ec.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details><h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p><p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p><p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p><p><strong>æ‘˜è¦</strong><br>æ™ºèƒ½æ±½è½¦ä¸“å±è§†è§‰åŸºç¡€æ¨¡å‹çš„æ„å»ºæŒ‘æˆ˜åŠå…¶æœªæ¥å‘å±•æœºé‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ•°æ®å‡†å¤‡ã€é¢„è®­ç»ƒç­–ç•¥å’Œä¸‹æ¸¸ä»»åŠ¡é€‚é…æ˜¯ VFM å¼€å‘çš„å…³é”®æŠ€æœ¯ã€‚</li><li>ç”Ÿæˆç¥ç»è¾å°„åœº (NeRF)ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œ3D é«˜æ–¯åˆ†å¸ƒï¼ˆ3DGSï¼‰å’Œä¸–ç•Œæ¨¡å‹ç­‰æŠ€æœ¯çš„è¿›æ­¥ä¸ºæœªæ¥çš„ç ”ç©¶æå‡ºäº†è·¯çº¿å›¾ã€‚</li><li>å¼€æºé¡¹ç›® <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a> å°†ä¸æ–­æ›´æ–°ï¼Œä»¥èµ‹èƒ½ç ”ç©¶äººå‘˜ã€‚</li><li>è‡ªåŠ¨é©¾é©¶ä¸­çš„ VFM ç¼ºä¹ä¸“ç”¨æ•°æ®å’Œå¤šä¼ æ„Ÿå™¨é›†æˆï¼Œå¯¼è‡´ä»»åŠ¡ç‰¹å®šæ¶æ„çš„å¤šæ ·æ€§æˆä¸º VFM å‘å±•çš„éšœç¢ã€‚</li><li>è§†è§‰åŸºç¡€æ¨¡å‹ (VFM) åœ¨è‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶å‘å±•é¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ã€‚</li><li>å¼€å‘ä¸“ç”¨äºè‡ªåŠ¨é©¾é©¶çš„ VFM æ˜¯å½“å‰çš„ç´§è¿«æŒ‘æˆ˜ã€‚</li><li>å»ºè®®ä»æ•°æ®å‡†å¤‡ã€é¢„è®­ç»ƒä»¥åŠä¸‹æ¸¸ä»»åŠ¡é€‚é…ç­‰æ–¹é¢å…¥æ‰‹ï¼Œå¹¶æ¢ç´¢ NeRFã€æ‰©æ•£æ¨¡å‹ç­‰æ–°æŠ€æœ¯ï¼Œä»¥æ¨è¿› VFM çš„å‘å±•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ„å»ºè‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹ï¼šæŒ‘æˆ˜ã€æ–¹æ³•å’Œæœºé‡</li><li>ä½œè€…ï¼šå¾å²©ï¼Œå¼ æµ·æ˜ï¼Œè”¡åº”æ°ï¼Œéƒ­æ•¬æ˜ï¼Œé‚±ç»´è¶…ï¼Œé«˜æ–Œï¼Œå‘¨å‡¯å¼ºï¼Œèµµè¶Šï¼Œé‡‘æ¬¢ï¼Œé«˜å»ºæ¶›ï¼ŒææŒ¯ï¼Œè’‹ç«‹è¾‰ï¼Œå¼ ä¼Ÿï¼Œå¼ å®æ³¢ï¼Œæˆ´ç™»å¿ƒï¼Œåˆ˜å†°å†°</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤</li><li>å…³é”®è¯ï¼šè§†è§‰åŸºç¡€æ¨¡å‹ï¼Œæ•°æ®ç”Ÿæˆï¼Œè‡ªç›‘ç£è®­ç»ƒï¼Œè‡ªåŠ¨é©¾é©¶ï¼Œæ–‡çŒ®ç»¼è¿°</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2401.08045   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œä¼ ç»Ÿè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡ä½¿ç”¨ä¸“ç”¨ç®—æ³•ï¼Œä½†è¿™ç§æ–¹æ³•å¾€å¾€å¯¼è‡´è¾“å‡ºä¸ä¸€è‡´ï¼Œé™åˆ¶äº†ç³»ç»Ÿå¤„ç†é•¿å°¾æƒ…å†µçš„èƒ½åŠ›ã€‚è¿‘å¹´æ¥ï¼Œå¤§å‹åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå±•ç°å‡ºå¼ºå¤§çš„é€‚åº”æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæ„å»ºè‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹æä¾›äº†æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºé’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š1. å¿½è§†äº†æ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œå¯¼è‡´è¾“å‡ºä¸ä¸€è‡´ï¼›2. éš¾ä»¥å¤„ç†é•¿å°¾æƒ…å†µï¼›3. æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ„å»ºè‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š1. æ•°æ®å‡†å¤‡ï¼šæ”¶é›†å’Œé¢„å¤„ç†è‡ªåŠ¨é©¾é©¶ç›¸å…³çš„æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒã€æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€è¯­ä¹‰åˆ†å‰²æ ‡ç­¾ç­‰ï¼›2. é¢„è®­ç»ƒï¼šä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿä»æ•°æ®ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼›3. ä¸‹æ¸¸ä»»åŠ¡è‡ªé€‚åº”ï¼šå°†é¢„è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¾®è°ƒä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°åŠæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æ•°æ®ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ•°æ®å‡†å¤‡ï¼šæ”¶é›†å’Œé¢„å¤„ç†è‡ªåŠ¨é©¾é©¶ç›¸å…³çš„æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒã€æ¿€å…‰é›·è¾¾ç‚¹äº‘ã€è¯­ä¹‰åˆ†å‰²æ ‡ç­¾ç­‰ï¼›ï¼ˆ2ï¼‰é¢„è®­ç»ƒï¼šä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿä»æ•°æ®ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼›ï¼ˆ3ï¼‰ä¸‹æ¸¸ä»»åŠ¡è‡ªé€‚åº”ï¼šå°†é¢„è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¾®è°ƒä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹çš„æ„å»ºæå‡ºäº†ç³»ç»Ÿçš„æ–¹æ³•ï¼Œå¹¶å–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¸ºè‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹çš„æ„å»ºæä¾›äº†æ–°çš„æ€è·¯ï¼Œæœ‰æœ›æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ„å»ºè‡ªåŠ¨é©¾é©¶è§†è§‰åŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬æ•°æ®å‡†å¤‡ã€é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡è‡ªé€‚åº”ä¸‰ä¸ªæ­¥éª¤ã€‚</li><li>ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿä»æ•°æ®ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚</li><li>å°†é¢„è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¾®è°ƒä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ–¹æ³•åœ¨è‡ªåŠ¨é©¾é©¶ç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ç­‰ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æ•°æ®ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤§é‡çš„æ•°æ®ã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿™éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦å°†é¢„è®­ç»ƒå¥½çš„åŸºç¡€æ¨¡å‹åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶é€šè¿‡å¾®è°ƒä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œè¿™éœ€è¦è¾ƒå¤šçš„å·¥ç¨‹å·¥ä½œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-01-30  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
