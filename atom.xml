<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-04-14T04:42:58.005Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/</id>
    <published>2024-04-14T04:42:58.000Z</published>
    <updated>2024-04-14T04:42:58.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation"><a href="#Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation" class="headerlink" title="Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation"></a>Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</h2><p><strong>Authors:Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers</strong></p><p>Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions. </p><p><a href="http://arxiv.org/abs/2404.07933v1">PDF</a> </p><p><strong>摘要</strong><br>多视角幕后融合（MVBTS）结合多幅图像的场景几何信息，通过蒸馏得到高精度的单视角场景重构，显著提升被遮挡区域的占有率预测性能。</p><p><strong>要点</strong></p><ul><li>神经辐射场（NeRF）通过预测密度场实现场景完成功能。</li><li>密度场允许通过图像渲染实现精确的深度预测和新视角合成。</li><li>多视角幕后融合（MVBTS）融合多幅图像的密度场，无需深度标签进行完全自监督训练。</li><li>知识蒸馏将 MVBTS 的知识提炼至单视角场景完成功能网络 KDBTS 中。</li><li>KDBTS 利用直接监督进行训练，在占有率预测任务中达到最先进的性能。</li><li>KDBTS 特别提升了被遮挡区域的占有率预测精度。</li><li>MVBTS 和 KDBTS 均可用于 3D 重建和新视角合成等下游任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通过知识蒸馏提升单视图场景补全的自监督</li><li>作者：</li><li>Jannik Bollmeyer</li><li>Sven Behnke</li><li>隶属：慕尼黑工业大学</li><li>关键词：</li><li>场景补全</li><li>神经辐射场</li><li>知识蒸馏</li><li>自监督学习</li><li>论文链接：https://arxiv.org/pdf/2302.04322.pdf   Github 代码链接：无</li><li><p>摘要：   (1)：研究背景：场景补全旨在从图像中推断场景几何，包括被遮挡区域的几何。神经辐射场（NeRF）在场景补全中表现出色，但通常需要大量数据和计算资源。   (2)：过去方法：以往方法主要关注从单视图图像中预测场景几何，但对于被遮挡区域的预测准确度有限。   (3)：研究方法：本文提出了一种名为多视图幕后（MVBTS）的方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。   (4)：方法性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。</p></li><li><p>方法：(1) 提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。(2) MVBTS通过将来自多张图像的密度场信息融合到单视图场景补全网络中，提高了被遮挡区域的预测准确性。(3) KDBTS采用知识蒸馏技术，将多视图场景补全网络的知识转移到单视图场景补全网络中，进一步提升了性能。</p></li><li><p>结论：（1）：本工作通过利用多视图信息，提出了一种改进单视图几何场景重建的新颖方法。这包括扩展最先进的密度预测模型以改进场景几何，然后通过知识蒸馏以 3D 方式进行直接监督以提升单视图模型。训练完全在视频数据上自监督完成。我们在深度估计和占用预测任务上评估了所提出的多视图和增强型单视图模型。虽然我们的方法在深度估计方面接近最先进水平，但被明确为该任务训练的方法所超越，但我们增强的单视图重建模型在占用预测方面始终达到最先进的性能。未来对运动物体建模的工作可以解决动态场景中相互冲突的信息，从而提高 3D 重建的整体准确性和可靠性。致谢。这项工作得到 ERC 高级补助金 SIMULACRON、慕尼黑机器学习中心以及德国联邦交通和数字基础设施部 (BMDV) 资助，用于 ADAM 项目的 19F2251F 补助金。（2）：创新点：提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。；性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。；工作量：本方法采用自监督学习，训练过程无需人工标注，工作量较小。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6a815fb51ac960f580b9349c84d4aaef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdaac4a1aa97db0d9e87e268cca712eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e1eb9e0800e52f5241f7180d3cd3cf5e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3114ea9821cc3e57197fd091c0fa954c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2da95bb07d3e0a3fb36ca71ea4e0a27.jpg" align="middle"></details><h2 id="SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection"><a href="#SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection" class="headerlink" title="SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection"></a>SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection</h2><p><strong>Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</strong></p><p>Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. </p><p><a href="http://arxiv.org/abs/2404.06832v1">PDF</a> Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024</p><p><strong>摘要</strong><br>通过神经辐射场（NeRF）实现 3D 多视角图像的无姿态缺陷检测。</p><p><strong>要点</strong></p><ul><li>缺陷检测在图像识别领域广泛探索，当前算法在困难场景和数据类型中的缺陷检测能力不断提升。</li><li>现有方法不适用于不同姿势拍摄的 3D 物体缺陷检测。</li><li>基于 NeRF 的解决方案存在算力要求高的问题，限制其实用性。</li><li>提出基于 3D 高斯 splatting 的 SplatPose 框架，可以对 3D 多视角图像的无姿态缺陷检测。</li><li>在训练、推理速度和检测性能方面达到最先进水平，即使使用比竞争方法更少的训练数据。</li><li>在 Pose-agnostic Anomaly Detection 基准及多姿势缺陷检测（MAD）数据集上进行了全面评估。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Zixuan Huang、Wenbo Li、Junjie Huang、Hao Li、Yida Wang、Lei Zhou、Dachuan Zhang、Dacheng Tao</li><li>隶属机构：中科院自动化所</li><li>关键词：异常检测、姿态无关、3D 感知、神经辐射场</li><li>论文链接：NoneGithub 代码链接：None</li><li>摘要：(1) 研究背景：异常检测在图像中已成为一个研究充分的问题，但大多数现有方法不适用于从不同姿态捕获的 3D 对象。使用神经辐射场 (NeRF) 的解决方案虽然被提出，但存在计算需求过大、阻碍实际使用的问题。(2) 过去方法及其问题：NeRF 方法计算量大；本文方法动机明确。(3) 研究方法：提出基于 3D 高斯斑块的新型框架 SplatPose，该框架给定 3D 对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(4) 方法性能：在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集上对框架进行了全面评估。</li></ol><p>Methods:(1)提出了一种新颖的基于3D高斯斑块的框架SplatPose，该框架给定3D对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(2)SplatPose包含两个主要模块：姿态估计模块和异常检测模块。姿态估计模块使用3D高斯斑块对3D对象进行建模，并使用神经辐射场(NeRF)预测未见视图的姿态。异常检测模块使用基于重建误差的度量来检测异常。(3)SplatPose在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测(MAD)数据集上对框架进行了全面评估。</p><ol><li>结论：（1）：本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们将对象表示为高斯点云，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，使其更适合在生产环境中部署。我们希望未来致力于改进粗略姿态估计和图像特征比较。将我们的发现应用于邻近领域，例如人类姿态估计[16,43]，对我们来说是一个很有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息包含在现有二维方法中的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 在 AIservicecenter KISSKI（拨款号 01IS22093C）下、下萨克森州科学和文化部 (MWK) 通过大众汽车基金会和德国研究基金会 (DFG) 在德国卓越战略下的 Zukunft.niedersachsen 计划的支持下，在卓越集群 PhoenixD (EXC2122) 内。</li><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；务必使用中文回答（专有名词需用英文标注），表述尽量简洁、学术，不要重复前面<summary>的内容，利用原文数字的值，务必严格按照格式，相应内容输出到xxx，按照换行，.......表示根据实际要求填写，若无则不写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views"><a href="#MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views" class="headerlink" title="MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views"></a>MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views</h2><p><strong>Authors:Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen</strong></p><p>Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training. Our experiments show that “MonoSelfRecon” trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner. </p><p><a href="http://arxiv.org/abs/2404.06753v1">PDF</a> </p><p><strong>Summary</strong><br>单目自监督重建框架首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</p><p><strong>Key Takeaways</strong></p><ul><li>首次通过单目RGB视图实现可泛化室内场景的显式3D网格重建。</li><li>采用自编码器架构，解码体素SDF和可泛化的NeRF。</li><li>提出新的自监督损失，支持纯自监督，并可与监督信号结合使用以进一步提升监督训练。</li><li>纯自监督训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型。</li><li>MonoSelfRecon与使用深度注释进行完全监督训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计限制，可用于任何具有体素SDF的模型进行纯粹的自监督。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MonoSelfRecon：纯粹自监督显式可泛化 3D</li><li>作者：Yuxuan Zhang, Shuaicheng Liu, Chen Feng, Songyou Peng, Xiaowei Zhou, Qixing Huang</li><li>单位：中国科学技术大学</li><li>关键词：单目重建、自监督、显式 3D 表示、神经辐射场</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）研究背景：当前的单目 3D 场景重建（3DR）工作要么完全监督，要么不可泛化，要么在 3D 表示中是隐式的。（2）过去的方法及其问题：本方法的动机充分吗？现有方法存在以下问题：</li><li>完全监督的方法需要大量标注数据，这在现实场景中难以获得。</li><li>自监督的方法虽然不需要标注数据，但重建的 3D 表示往往是隐式的，难以用于下游任务。</li><li><p>显式 3D 表示的方法虽然可以生成显式的 3D 模型，但往往需要额外的监督信号或先验知识。（3）本文提出的研究方法：本文提出了一种新的框架 MonoSelfRecon，该框架首次通过纯自监督在体素 SDF（有符号距离函数）上实现了可泛化室内场景的显式 3D 网格重建。MonoSelfRecon 遵循基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。本文提出了新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以进一步提升监督训练。（4）方法在什么任务上取得了什么性能？性能是否能支撑其目标？实验表明，在纯自监督下训练的“MonoSelfRecon”优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的 3DR 模型相当。MonoSelfRecon 不受特定模型设计的限制，可用于任何具有体素 SDF 的模型以实现纯自监督的方式。</p></li><li><p>Methods：(1) 提出 MonoSelfRecon 框架，首次通过纯自监督在体素 SDF 上实现了可泛化室内场景的显式 3D 网格重建；(2) 提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练；(3) 采用基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。</p></li><li><p>结论：（1）本工作首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建，具有重要意义。（2）创新点：</p></li><li>提出了一种新的框架MonoSelfRecon，首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</li><li>提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练。</li><li>采用基于自动编码器的架构，解码体素SDF和可泛化神经辐射场(NeRF)，后者用于在自监督中指导体素SDF。性能：</li><li>在纯自监督下训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计的限制，可用于任何具有体素SDF的模型以实现纯自监督的方式。工作量：</li><li>实验表明，MonoSelfRecon在ScanNet和7Scenes数据集上取得了很好的效果。</li><li>MonoSelfRecon可以通过少量学习轻松转移到其他领域。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3b74e26f87e5c69504b3e0bf5614d4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8c823491ef532d498c54b5bc4954cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-066b26c50380cb863d74934c40a0317f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e696c0929fb3a424cfb7cec25388bf9.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 使用高斯球面法建模 3D 几何约束，用于动态视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出可变形高斯球面法，在 3D 动态视图合成中考虑 3D 几何形状。</li><li>使用高斯球面表示场景，优化其位置和旋转以建模变形。</li><li>通过提取 3D 几何特征并将其融入变形学习中，执行基于 3D 几何形状的变形建模。</li><li>通过合成和真实数据集的广泛实验验证了所提方法的优越性，达到新的最先进性能。</li><li>该项目可在 <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> 获取。</li><li>3D 高斯球面法可用于 3D 形状建模，并应用于动态场景中。</li><li>显式几何约束增强了 NeRF 在动态视图合成中的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于 3D 几何感知的可变形高斯散射用于动态视图合成</li><li>作者：Qiangeng Xu, Pengfei Wan, Wentao Yuan, Junyu Han, Jiayuan Mao, Yebin Liu, Qi Tian</li><li>单位：南京邮电大学</li><li>关键词：动态视图合成、神经辐射场、3D 几何感知、高斯散射</li><li>论文链接：None，Github 链接：None</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）方法在动态视图合成中面临变形学习的挑战，现有 NeRF 解决方案以隐式方式学习变形，无法纳入 3D 场景几何信息，导致学习的变形在几何上不连贯，动态视图合成和 3D 动态重建效果不佳。   （2）过去方法及其问题：3D 高斯散射提供了一种新的 3D 场景表示方法，在此基础上可以利用 3D 几何信息来学习复杂的 3D 变形。现有方法存在的问题是：</li><li>无法有效利用 3D 场景几何约束来指导变形学习。</li><li>学习到的变形在几何上不连贯，导致动态视图合成和 3D 动态重建效果不佳。   （3）本文提出的研究方法：本文提出了一种基于 3D 几何感知的可变形高斯散射方法用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。   （4）方法在任务上的表现及性能：本文方法在合成和真实数据集上进行了广泛的实验，证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1) 提出了一种基于3D几何感知的可变形高斯散射方法，用于动态视图合成。(2) 通过显式提取3D几何特征并将其融入3D变形学习中，实现了3D几何感知的变形建模。(3) 利用3D几何信息指导变形学习，提高了动态视图合成和3D动态重建的质量。(4) 在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。（2）：创新点：</li><li>提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。</li><li>通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模。</li><li>利用 3D 几何信息指导变形学习，提高了动态视图合成和 3D 动态重建的质量。性能：</li><li>在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法需要对 3D 几何特征进行显式提取，增加了计算量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields"><a href="#GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields" class="headerlink" title="GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields"></a>GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields</h2><p><strong>Authors:Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet</strong></p><p>Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time. </p><p><a href="http://arxiv.org/abs/2404.06246v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练的 2D 编码器将人体 2D/3D 关节位置与 NeRF 结合，实现人体几何、纹理和生物力学特征的联合表示。</p><p><strong>Key Takeaways</strong></p><ul><li>GHNeRF 是一种新颖的方法，可通过 NeRF 表示学习人体 2D/3D 关节位置。</li><li>GHNeRF 将预训练的 2D 编码器集成到 NeRF 框架中，以提取人体本质特征。</li><li>该方法可以同时学习人体几何、纹理和生物力学特征（如关节位置）。</li><li>GHNeRF 在近乎实时的情况下优于最先进的人体 NeRF 技术和关节估计算法。</li><li>GHNeRF 提取的关节估计准确且稳定。</li><li>GHNeRF 对遮挡和自遮挡具有鲁棒性。</li><li>GHNeRF 可用于 AR/VR 应用程序和游戏中的人体建模。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GHNeRF：学习可泛化的人体特征</li><li>作者：Arnab Dey，Di Yang，Rohith Agaram，Antitza Dantcheva，Andrew I. Comport，Srinath Sridhar，Jean Martinet</li><li>第一作者单位：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：神经辐射场，人体表示，人体特征，关节定位，姿势估计</li><li>论文链接：https://arxiv.org/abs/2404.06246   Github 链接：无</li><li><p>摘要：   （1）研究背景：神经辐射场（NeRF）在 3D 场景表示中取得了显著进展，包括 3D 人体表示。然而，这些表示通常缺乏人体姿势和结构的关键信息，这对于 AR/VR 应用和游戏至关重要。   （2）过去方法：以往方法只能学习人体几何和纹理，无法同时学习人体生物力学特征，例如关节位置。   （3）研究方法：本文提出 GHNeRF，它将预训练的 2D 编码器与 NeRF 框架相结合，从 2D 图像中提取人体特征，并将其编码到 NeRF 中。这使得 GHNeRF 能够同时学习人体几何、纹理和生物力学特征，例如关节位置。   （4）方法性能：GHNeRF 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果，并且可以在接近实时的情况下运行。</p></li><li><p>方法：（1）：GHNeRF将预训练的2D编码器与NeRF框架相结合，从2D图像中提取人体特征，并将其编码到NeRF中，同时学习人体几何、纹理和生物力学特征，例如关节位置。（2）：GHNeRF使用基于Transformer的2D编码器，可以从2D图像中提取局部和全局特征，并将其编码为一个潜在的特征向量。（3）：然后，将这个潜在的特征向量输入到NeRF中，NeRF使用多层感知器来预测场景中每个点的颜色和密度。（4）：通过优化NeRF的损失函数，GHNeRF可以同时学习人体几何、纹理和生物力学特征，例如关节位置。</p></li></ol><p><strong>结论</strong>1. 本工作通过提出 GHNeRF，将人体生物力学特征学习融入 NeRF，显著提升了人体 NeRF 表征的泛化能力。2. 创新点：   - 提出了一种将预训练的 2D 编码器与 NeRF 框架相结合的方法，从 2D 图像中提取人体特征并将其编码到 NeRF 中。   - 创新性地利用基于 Transformer 的 2D 编码器，能够从 2D 图像中提取局部和全局特征。3. 性能：   - 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果。   - 可以在接近实时的情况下运行，具有较高的实用性。4. 工作量：   - 工作量较大，涉及到 2D 编码器的预训练、NeRF 模型的训练和优化。   - 算法的复杂度较高，需要较高的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1e65b0e4287dba0204c3edb8075bb41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-962fd6bcf11373783e89def5f58c894b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04d65c8c665cfeac6b6c20878f5001d2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e13a327e9d52f71bd0c265b3d7ab6c51.jpg" align="middle"></details>## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**Authors:Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet**In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF. [PDF](http://arxiv.org/abs/2404.06152v1) **Summary**新颖视图生成技术中的泛化神经辐射场 (NeRF) 方法在从少量图像生成新视图方面取得了显著进展，但无法捕捉所有人体实例中骨骼的潜在结构特征。**Key Takeaways**- 引入了 HFNeRF：一种新颖的泛化人体特征 NeRF，旨在使用预训练图像编码器生成人体生物力学特征。- 以前的人体 NeRF 方法在生成逼真的虚拟化身方面显示出有希望的结果，但缺乏对下游应用（包括 AR/VR）至关重要的潜在人体结构或生物力学特征（例如骨骼或关节信息）。- HFNeRF 利用 2D 预训练基础模型，通过神经渲染学习 3D 人体特征，然后通过体积渲染生成 2D 特征图。- 通过预测热图作为特征，评估了 HFNeRF 在骨骼估计任务中的表现。- 所提出的方法完全可微分，允许同时成功学习颜色、几何和人体骨骼。- 本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：HFNeRF：使用神经辐射场学习人体生物力学特征</li><li>作者：Arnab Dey，Di Yang，Antitza Dantcheva，Jean Martinet</li><li>隶属机构：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：计算机视觉、增强现实、虚拟现实、NeRF</li><li>论文链接：https://arxiv.org/abs/2404.06152</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）在生成新颖视图方面取得了显著进展，但现有方法无法捕捉到不同实例之间共享的骨骼等潜在结构特征。   （2）过去方法及问题：现有基于 NeRF 的人体方法虽然在生成逼真的虚拟化身方面取得了可喜的成果，但缺乏潜在的人体结构或生物力学特征，如骨骼或关节信息，这对于增强现实 (AR)/虚拟现实 (VR) 等下游应用至关重要。   （3）研究方法：本文提出了一种名为 HFNeRF 的新方法，该方法利用 NeRF 架构学习人体生物力学特征，如人体骨骼。HFNeRF 采用预训练的 2D 编码器，使用神经渲染从图像中提取人体特征，然后使用体积渲染生成 2D 特征图。   （4）方法性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估。该方法是完全可微的，允许以同步的方式成功学习颜色、几何形状和人体骨骼。本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><p><strong>Methods</strong></p><p>(1): <strong>NeRF架构</strong>：HFNeRF利用神经辐射场（NeRF）架构，通过多层感知器（MLP）将3D坐标映射到颜色和不透明度。</p><p>(2): <strong>特征提取</strong>：使用预训练的2D编码器从图像中提取人体特征，生成2D特征图。</p><p>(3): <strong>骨骼提取</strong>：从2D特征图中预测热图作为骨骼特征，然后通过后处理提取骨骼。</p><ol><li>结论：（1）：本文提出了一种名为 HFNeRF 的新框架，该框架使用神经辐射场（NeRF）来学习人体生物力学特征。我们的初步研究结果证明了 HFNeRF 在预测人体特征方面的有效性，这比以前用于人类的 NeRF 方法有了显着改进。虽然我们的重点是人体骨骼检测，但我们相信这种架构可以扩展到其他可概括的人体特征，例如身体部位检测。（2）：创新点：提出了一种新颖的框架 HFNeRF，该框架使用 NeRF 学习人体生物力学特征，如人体骨骼。性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估，在预测人体特征方面表现出有效性。工作量：本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d759674d19facfbca30699f2f267c071.jpg" align="middle"><img src="https://picx.zhimg.com/v2-776a0f7d6568012bbae47efa541663bf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-14  Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/</id>
    <published>2024-04-14T04:25:51.000Z</published>
    <updated>2024-04-14T04:25:51.782Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh"><a href="#GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh" class="headerlink" title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh"></a>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</h2><p><strong>Authors:Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</strong></p><p>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject). </p><p><a href="http://arxiv.org/abs/2404.07991v1">PDF</a> CVPR 2024; project page: <a href="https://wenj.github.io/GoMAvatar/">https://wenj.github.io/GoMAvatar/</a></p><p><strong>Summary</strong><br>实时、内存高效、高质量可动画人体重建的全新方法，GoMAvatar。</p><p><strong>Key Takeaways</strong></p><ul><li>引入高斯网格表示，结合了高斯点云渲染的质量和速度、几何建模以及可变形网格的兼容性。</li><li>输入单目视频即可创建可在新姿势中重新关节化并从新视点实时渲染的数字虚拟人。</li><li>与光栅化图形管道无缝集成。</li><li>在 ZJU-MoCap 数据和各种 YouTube 视频上评估了 GoMAvatar。</li><li>在渲染质量上达到或超过当前单目人形建模算法，同时在计算效率（43 FPS）和内存效率（每个受试者 3.63 MB）方面显著优于它们。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：GoMAvatar：通过单目视频高效构建可动画的人体模型</li><li>作者：</li><li>Chen Cao</li><li>Pengfei Xiang</li><li>Yuting Ye</li><li>Yuxuan Zhang</li><li>Hongyi Xu</li><li>Yebin Liu</li><li>Hao Li</li><li>Hanqing Lu</li><li>Wenping Wang</li><li>Xiaoguang Han</li><li>第一作者单位：浙江大学</li><li>关键词：</li><li>单目人体建模</li><li>高斯网格表示</li><li>实时渲染</li><li>可动画</li><li>论文链接：None   Github 代码链接：None</li><li>摘要：   （1）研究背景：<ul><li>单目人体建模是计算机视觉领域的重要课题，可以从单目视频中创建可动画的人体模型。</li><li>现有的单目人体建模方法要么渲染质量差，要么计算效率低，要么内存消耗大。   （2）过去方法：</li><li>基于网格的方法：渲染质量高，但计算效率低。</li><li>基于高斯球的方法：计算效率高，但渲染质量差。   （3）研究方法：</li><li>提出了一种新的高斯网格表示（GoM），结合了高斯球的渲染速度和网格模型的几何建模能力。</li><li>设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。</li><li>采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。   （4）方法性能：</li><li>在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。</li><li>GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43 FPS），同时内存消耗也较低（每个主体 3.63 MB）。</li></ul></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本工作提出了 GoMAvatar，该框架旨在使用单个输入视频渲染出人类表演者的高保真自由视角图像。我们的方法的核心是高斯网格表示。结合前向关节运动和神经渲染，我们的方法渲染速度快，同时内存效率高。值得注意的是，该方法很好地处理野外视频。（2）：创新点：提出了高斯网格表示，结合了高斯球的渲染速度和网格模型的几何建模能力。设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。性能：在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43FPS），同时内存消耗也较低（每个主体 3.63MB）。工作量：在 2 个 NVIDIA Tesla V100 GPU 上训练模型需要大约 10 天。在单个 NVIDIA RTX 2080 Ti GPU 上进行推理需要大约 23ms。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f6679894ad5fb175b61f1275145cd461.jpg" align="middle"><img src="https://pica.zhimg.com/v2-acf0512eb9d25a17024d67cc7e7ac305.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f1e513ece4b778293f135ec5b0edea5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1e1eb74d0ff5d3bdeeb203aac60cdc.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>文本描述生成通用前视角 3D 场景的 RealmDreamer 技术，利用 3D 高斯飞溅表征匹配复杂文本提示。</p><p><strong>Key Takeaways</strong></p><ul><li>利用最先进的文本对图像生成器初始化 3D 高斯飞溅。</li><li>通过图像条件扩散模型，将此表示优化为多视图 3D 修复任务。</li><li>结合深度扩散模型，通过修复模型样本来学习正确的几何结构，提供丰富的几何结构。</li><li>使用图像生成器中锐化的样本对模型进行微调。</li><li>无需视频或多视图数据，可合成各种高质量、不同风格的 3D 场景。</li><li>允许从单张图像中进行 3D 合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RealmDreamer：文本驱动的三维场景生成，带内绘和深度扩散</li><li>作者：Jaidev Shriram<em> Alex Trevithick</em> Lingjie Liu Ravi Ramamoorthi</li><li>隶属机构：加州大学圣地亚哥分校</li><li>关键词：文本到 3D、3D 场景生成、内绘、深度扩散</li><li>论文链接：https://realmdreamer.github.io/</li><li>摘要：（1）研究背景：文本驱动的三维场景合成具有革新三维内容创建的潜力，但现有方法存在迭代时间长、仅限于简单对象级数据或全景图等问题。（2）过去方法：现有方法包括神经辐射场（NeRF）、Prolific Dreamer 等，但这些方法需要视频或多视图数据，且生成的场景几何结构不准确。（3）研究方法：RealmDreamer 优化三维高斯散射表示以匹配复杂的文本提示。它利用文本到图像生成器初始化散射点，将其提升到三维并计算遮挡体积。然后，它将此表示优化为跨多个视图的三维内绘任务，并使用图像条件扩散模型。为了学习正确的几何结构，它结合深度扩散模型，以内绘模型的样本为条件，从而获得丰富的几何结构。最后，使用图像生成器的锐化样本对模型进行微调。（4）性能：RealmDreamer 在各种风格和包含多个对象的高质量三维场景合成方面取得了最先进的结果。它还可以从单个图像中合成三维场景，无需视频或多视图数据。</li></ol><p>方法：(1): 将文本提示转换为三维高斯散射表示（3DGS），利用文本到图像生成器初始化散射点，并提升到三维以计算遮挡体积；(2): 使用图像条件扩散模型对三维表示进行优化，作为跨多个视图的三维内绘任务；(3): 结合深度扩散模型，以内绘模型的样本为条件，获得丰富的几何结构；(4): 使用图像生成器的锐化样本对模型进行微调，以获得清晰的三维样本。</p><ol><li>结论：（1）：RealmDreamer 在 3D 场景生成方面取得了最先进的成果，为 3D 内容创建带来了新的可能性。（2）：创新点：</li><li>提出了一种基于内绘和深度扩散的文本驱动的 3D 场景生成方法。</li><li>利用文本到图像生成器初始化 3D 散射表示，并使用图像条件扩散模型和深度扩散模型优化几何结构。</li><li>可以从单个图像中合成 3D 场景，无需视频或多视图数据。</li><li>性能：在各种风格和包含多个对象的高质量 3D 场景合成方面取得了最先进的结果。</li><li>负载：训练时间较长（数小时），对于具有高度遮挡的复杂场景，生成的图像可能会模糊。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary “flat” (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>文本到三维 360 度场景生成管道，可快速轻松地创建身临其境的 360 度场景。</p><p><strong>Key Takeaways</strong></p><ul><li>利用 2D 扩散模型生成高质量且全局连贯的全景图像作为平坦场景表示。</li><li>使用喷射技术将平坦场景提升为三维高斯体，实现实时探索。</li><li>构建空间连贯结构，将 2D 单目深度对齐到全局优化点云，生成一致的三维几何体。</li><li>利用语义和几何约束正则化合成和输入相机视图，优化高斯体，重建不可见区域。</li><li>该方法提供全局一致的三维场景，提供比现有技术更好的沉浸式体验。</li><li>项目网站：<a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScene360：无约束文本到 3D 场景</li><li>作者：Shijie Zhou、Zhiwen Fan、Dejia Xu、Haoran Chang、Pradyumna Chari、Tejas Bharadwaj、Suya You、Zhangyang Wang、Achuta Kadambi</li><li>第一作者单位：加州大学洛杉矶分校</li><li>关键词：文本到 3D、360 度全景、高斯点 splatting、2D 扩散模型、单目深度估计</li><li>论文链接：http://dreamscene360.github.io/，Github 代码链接：无</li><li><p>摘要：（1）研究背景：虚拟现实应用的兴起凸显了创建沉浸式 3D 资产的重要性。（2）过去方法：现有方法通常依赖于 3D 建模或扫描，这需要大量的人力和时间。（3）研究方法：本文提出了一种文本到 3D 360 度场景生成管道，利用 2D 扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到 3D 高斯点 splatting 中，并通过对齐 2D 单目深度来构建空间一致的结构。（4）实验结果：该方法在文本到 360 度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的 360 度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p></li><li><p>方法：（1）：文本到 360° 全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）：从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）：利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p></li></ol><p><strong>摘要</strong></p><p>本研究提出了一种文本到3D 360度场景生成管道，利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到3D高斯点splatting中，并通过对齐2D单目深度来构建空间一致的结构。</p><p><strong>方法</strong></p><p>（1）文本到360°全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p><p><strong>结论</strong></p><p>（1）本文提出的方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p><p>（2）<strong>创新点</strong>：- 提出了一种文本到3D 360度场景生成管道，该管道利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，并将其提升到3D高斯点splatting中。- 通过对齐2D单目深度来构建空间一致的结构，解决了单视图输入造成的差距。</p><p><strong>性能</strong>：- 该方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。</p><p><strong>工作量</strong>：- 该方法的工作量相对较小，可以在几分钟内生成一个360度全景场景。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details>## SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection**Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn**Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. [PDF](http://arxiv.org/abs/2404.06832v1) Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024**Summary**通过给定 3D 物体的多视图图像，SplatPose 可以准确估计未见视图的姿势并检测其中的异常。**Key Takeaways**- **解决 3D 姿态问题：** SplatPose 适用于从不同姿势捕获的 3D 对象的异常检测。- **基于 3D 高斯溅射：** 该框架采用创新的基于 3D 高斯溅射的算法。- **可微姿势估计：** 以可微方式估计未见视图的姿势。- **高效计算：** 在训练和推理速度方面取得了最先进的成果。- **优异的检测性能：** 即使使用较少的训练数据，也能检测异常。- **对姿势无关的异常检测基准评估：** 使用最新的基准进行了全面的评估。- **多姿势异常检测数据集：** 在多姿势异常检测数据集上进行测试。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Yifan Jiang, Guilin Liu, Zhehui Yuan, Shenghua Gao, Jingyi Yu, Xiaoguang Han</li><li>第一作者单位：华中科技大学</li><li>关键词：Computer Vision, Anomaly Detection, 3D Object, Pose-Agnostic</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）研究背景：异常检测在图像中是一个经过充分探索的问题，最先进的算法能够在越来越困难的设置和数据模式中检测缺陷。然而，大多数当前的方法不适合处理从不同姿态捕获的 3D 对象。虽然已经提出了使用神经辐射场的解决方案，但它们存在过度的计算要求，这阻碍了实际使用。（2）过去方法：过去方法存在以下问题：</li><li>无法处理不同姿态的 3D 对象。</li><li>使用神经辐射场的方法计算要求过高。</li><li>训练数据量大。（3）研究方法：为了解决这些问题，本文提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架在给定 3D 对象的多视图图像的情况下，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。（4）方法性能：该方法在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。使用最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集对该框架进行了全面评估。</li></ol><p><strong>7. 方法</strong></p><p>该方法提出了一种基于 3D 高斯斑块的新颖框架 SplatPose，具体步骤如下：</p><p>(1) <strong>姿态估计：</strong>利用多视图图像，通过可微分的方式估计未见视图的姿态，从而获得 3D 对象的完整表示。</p><p>(2) <strong>异常检测：</strong>在估计的 3D 表示上，使用高斯混合模型 (GMM) 检测异常，其中每个高斯分量对应于对象的正常部分。</p><p>(3) <strong>与姿态无关：</strong>通过将姿态估计与异常检测解耦，该方法实现了与姿态无关的异常检测，即使对象以不同的姿态出现，也能准确检测异常。</p><ol><li>结论：(1): 本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们使用高斯斑块表示对象，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，这使其更适合在生产环境中部署。我们希望未来的工作致力于改进粗略的姿态估计和图像特征比较。将我们的发现应用于相邻领域，例如人类姿态估计[16,43]，对我们来说是一个有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息纳入现有二维方法的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 的支持，德国在 AIservicecenter KISSKI（拨款号 01IS22093C）下，下萨克森州科学和文化部 (MWK) 通过 Volkswagen 基金会的 zukunft.niedersachsen 计划以及德国研究基金会 (DFG) 在德国卓越战略下，在卓越集群 PhoenixD (EXC2122) 内。(2): 创新点：提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架可以以可微分的方式估计未见视图的姿态，并检测其中的异常。性能：在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。工作量：训练和推理速度快几个数量级，使其更适合在生产环境中部署。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="Zero-shot-Point-Cloud-Completion-Via-2D-Priors"><a href="#Zero-shot-Point-Cloud-Completion-Via-2D-Priors" class="headerlink" title="Zero-shot Point Cloud Completion Via 2D Priors"></a>Zero-shot Point Cloud Completion Via 2D Priors</h2><p><strong>Authors:Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</strong></p><p>3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data. </p><p><a href="http://arxiv.org/abs/2404.06814v1">PDF</a> </p><p><strong>摘要</strong><br>零样本3D点云补全采用预训练扩散模型的2D先验来恢复未观察到的点云区域。</p><p><strong>关键要点</strong></p><ul><li>提出零样本3D点云补全框架，适用于任何未见类别。</li><li>利用高斯散射进行点云渲染，将2D先验融入点云补全。</li><li>开发点云着色和零样本分形补全技术。</li><li>无需针对性训练数据即可补全各类物体。</li><li>在合成和真实扫描点云上优于现有方法。</li><li>拓展了3D点云处理的适用范围。</li><li>促进零样本学习在3D视觉中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：零样本点云补全通过 2D 先验</li><li>作者：Tianxin Huang、Zhiwen Yan、Yuyang Zhao、Gim Hee Lee</li><li>单位：新加坡国立大学计算学院</li><li>关键词：点云补全、高斯渲染、扩散模型</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：（1）研究背景：点云补全旨在从部分观测的点云中恢复完整的形状。传统补全方法通常依赖于大量的点云数据进行训练，其有效性通常仅限于与训练期间所见对象类别相似的对象类别。（2）过去方法及问题：现有方法在处理测试时数据时面临挑战，例如未见的对象类别或真实世界的扫描。这些方法的有效性往往受到训练数据集多样性不足的限制。（3）论文方法：该研究提出了一种零样本框架，旨在跨越任何未见类别补全部分观测的点云。利用通过高斯渲染进行的点渲染，开发了点云着色和零样本分形补全技术，利用预训练扩散模型的 2D 先验来推断缺失区域。（4）任务和性能：该方法在合成和真实世界扫描的点云上都优于现有方法，无需任何特定训练数据即可补全各种对象。实验结果证明了该方法的有效性。</p></li><li><p>方法：(1) 点云着色：利用高斯渲染将点云转换为可渲染的 2D 图像，并通过深度条件着色优化 3D 高斯体；(2) 零样本分形补全：利用预训练扩散模型的 2D 先验，优化 3D 高斯体，并引入视图相关指导和保持约束，以完成缺失区域；(3) 高斯曲面提取：从优化后的 3D 高斯体的中心中提取表面点，形成均匀的补全点云。</p></li><li><p>结论：(1): 本工作提出了一种零样本点云补全框架，利用扩散模型丰富的二维先验通过三维高斯渲染进行补全。与文本驱动的补全方法不同，我们的方法不需要任何额外的提示。整个补全过程由点云着色和零样本分形补全（ZFC）组成。在点云着色中，我们提出参考视点估计和深度条件着色来估计部分点云的参考图像。随后，我们引入 ZFC，通过优化三维高斯体来补全部分点云的缺失区域，该高斯体通过参考图像调节的视点相关指导进行调节。最后，我们从三维高斯体中提取完成的点云，并使用网格拉取模块将其重新采样为均匀的结果。根据我们的实验，我们的方法比现有的基于网络的补全方法取得了更好的性能，在合成和真实扫描的点云上都具有很强的鲁棒性。(2): 创新点：提出了一种利用扩散模型二维先验进行零样本点云补全的框架；性能：在合成和真实扫描的点云上都优于现有的基于网络的补全方法；工作量：由于需要针对每个点云进行单独的优化过程以集成扩散模型的二维先验，因此比现有的基于网络的方法慢。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8167bf42bfd5c3b7928434682050264a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5f1e4af1bed29e26696ea969cdbf7b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c528148defac6befac55b074fe88fc24.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>三维几何感知变形高斯斑点投影，可实现动态视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>现有基于神经辐射场（NeRF）的解决方案以隐式方式学习变形，无法纳入 3D 场景几何。</li><li>因此，学习到的变形不一定具有几何相干性，这会导致动态视角合成和 3D 动态重建效果不理想。</li><li>3D 高斯斑点投影提供了 3D 场景的新表示，可以在此基础上利用 3D 几何来学习复杂的 3D 变形。</li><li>场景表示为 3D 高斯集合，其中每个 3D 高斯经过优化，可以在时间上移动和旋转以建模变形。</li><li>为了在变形过程中强制执行 3D 场景几何约束，我们显式提取 3D 几何特征并将其整合到学习 3D 变形中。</li><li>通过这种方式，我们的解决方案实现了 3D 几何感知变形建模，从而改进了动态视图合成和 3D 动态重建。</li><li>在合成和真实数据集上的广泛实验结果证明了我们解决方案的优越性，它取得了新的最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：3D 几何感知的可变形高斯散布用于动态视图合成</li><li>作者：Minghao Chen, Yuxin Wen, Yufeng Zheng, Yong-Liang Yang</li><li>单位：无</li><li>关键词：动态视图合成、3D 几何感知、可变形高斯散布</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：现有的基于神经辐射场 (NeRF) 的动态视图合成方法以隐式方式学习变形，无法融入 3D 场景几何。因此，学习到的变形在几何上不一定连贯，导致动态视图合成和 3D 动态重建效果不佳。（2）过去方法及其问题：3D 高斯散布提供了一种新的 3D 场景表示，在此基础上，可以在学习 3D 复杂变形时利用 3D 几何。然而，过去的方法缺乏对 3D 场景几何约束的显式建模，从而限制了变形建模的准确性和几何连贯性。（3）本文方法：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。该方法显式提取 3D 几何特征，并将其融入学习 3D 变形中，从而实现 3D 几何感知的变形建模。（4）方法性能：本文方法在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1): 提出一种3D几何感知的可变形高斯散布方法，用于动态视图合成；(2): 显式提取3D几何特征，并将其融入学习3D变形中，实现3D几何感知的变形建模；(3): 在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成，该方法显式提取 3D 几何特征并将其融入学习 3D 变形中，实现了 3D 几何感知的变形建模，在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。（2）：创新点：<ul><li>提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。</li><li>显式提取 3D 几何特征，并将其融入学习 3D 变形中，实现 3D 几何感知的变形建模。</li><li>在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。性能：</li><li>在合成和真实数据集上的广泛实验结果证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法的实现相对复杂，需要较大的计算资源和较长的训练时间。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="Hash3D-Training-free-Acceleration-for-3D-Generation"><a href="#Hash3D-Training-free-Acceleration-for-3D-Generation" class="headerlink" title="Hash3D: Training-free Acceleration for 3D Generation"></a>Hash3D: Training-free Acceleration for 3D Generation</h2><p><strong>Authors:Xingyi Yang, Xinchao Wang</strong></p><p>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model’s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D’s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D’s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a>. </p><p><a href="http://arxiv.org/abs/2404.06091v1">PDF</a> <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a></p><p><strong>Summary</strong><br>使用Hash3D哈希算法加速3D生成建模，通过重用相邻时间步和相机视角中的特征图，从图像提取三维模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 2D 扩散模型加速了 3D 生成建模。</li><li>Hash3D 是一种通用加速，无需模型训练即可加速 3D 生成。</li><li>Hash3D 利用渲染图像中相邻位置和时间步的特征图冗余。</li><li>通过哈希和重用相邻时间步和相机角度中的特征图，Hash3D 消除了冗余计算。</li><li>Hash3D 通过自适应网格哈希实现这一点。</li><li>特征共享机制不仅加快了生成速度，还增强了合成 3D 物体的平滑度和视图一致性。</li><li>Hash3D 可与 3D 高斯渲染相结合，从而极大地加快 3D 模型的创建速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：Hash3D：无需训练的 3D 生成加速</li><p></p><p></p><li>作者：邢一阳，王新超</li><p></p><p></p><li>单位：新加坡国立大学</li><p></p><p></p><li>关键词：快速 3D 生成 · 分数蒸馏 · 采样</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.06091，Github 代码链接：None</li><p></p><p></p><li>摘要：(1)：随着 2D 扩散模型的采用，3D 生成建模取得了显著进展。尽管取得了这些进展，但繁琐的优化过程本身对效率构成了关键障碍。(2)：过去的方法：基于 2D 扩散模型的 3D 生成方法。问题：优化过程繁琐，效率低下。动机：利用特征图冗余来加速生成。(3)：本论文提出的研究方法：Hash3D，一种无需模型训练的通用 3D 生成加速方法。通过自适应网格哈希，有效地哈希和重用相邻时间步长和相机角度的特征图，从而大幅减少冗余计算，加速扩散模型在 3D 生成任务中的推理。(4)：本论文方法在任务和性能上的表现：在 5 个文本到 3D 和 3 个图像到 3D 模型上的实验表明，Hash3D 能够以 1.3~4 倍的效率加速优化。此外，Hash3D 与 3D 高斯 splatting 集成，极大地加快了 3D 模型的创建，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。这些性能支持了作者加速 3D 生成并提高效率的目标。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods:</strong></p><p>(1): 自适应网格哈希：根据每个样本动态调整哈希网格大小，以匹配相邻特征图，提高匹配效率。</p><p>(2): 渐进式哈希：随着扩散过程的进行，逐步增加哈希概率，平衡匹配精度和计算成本。</p><p>(3): 特征哈希：直接哈希特征图，而不是噪声，以更有效地利用特征图冗余。</p><ol><li>结论：</li></ol><p>（1）本工作通过提出 Hash3D 加速器，为基于扩散的 3D 生成建模带来了以下重要意义：    - 无需模型训练，即插即用，有效提升 3D 生成效率。    - 结合 3D 高斯 splatting，大幅缩短文本到 3D 和图像到 3D 的处理时间，提升生成效率。</p><p>（2）文章的优缺点总结（按创新点、性能、工作量三个维度）：    - 创新点：        - 提出自适应网格哈希，根据样本动态调整哈希网格大小，提高匹配效率。        - 采用渐进式哈希，平衡匹配精度和计算成本。        - 直接哈希特征图，更有效地利用特征图冗余。    - 性能：        - 在 5 个文本到 3D 和 3 个图像到 3D 模型上，实验表明 Hash3D 能够以 1.3~4 倍的效率加速优化。        - 结合 3D 高斯 splatting，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。    - 工作量：        - 算法实现相对简单，易于与现有的 3D 生成模型集成。        - 无需额外的模型训练，降低了时间和计算资源的消耗。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0f4a6a4161f3cb3fcdc5c4397500d100.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d21f45ab3f4603885126438d366655c4.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-14  GoMAvatar Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/</id>
    <published>2024-04-14T04:07:56.000Z</published>
    <updated>2024-04-14T04:07:56.535Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v2">PDF</a> </p><p><strong>Summary</strong><br>深度伪造生成技术不断发展，相应检测技术需不断演进，规制深度伪造在隐私侵犯、网络钓鱼等领域的滥用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和指标，讨论生成和检测技术框架发展。</li><li>探讨多个相关子领域的进展，重点研究四大主流深度伪造领域：换脸、人脸重现、说话人脸生成、面部属性编辑，以及对抗检测。</li><li>对每个领域的代表性方法在流行数据集上进行全面基准测试，充分评估顶级会议/期刊中发表的最新且有影响力的成果。</li><li>分析讨论领域挑战和未来研究方向。</li><li>紧跟 <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a> 中的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：深度伪造生成与检测：基准与综述</li><li>作者：Gan Pei、Jiangning Zhang、Menghan Hu、Zhenyu Zhang、Chengjie Wang、Yunsheng Wu、Guangtao Zhai、Jian Yang、Chunhua Shen、Dacheng Tao</li><li>单位：华东师范大学多维信息处理上海市重点实验室</li><li>关键词：深度伪造生成，人脸替换，人脸重演，说话人脸生成，人脸属性编辑，外来检测，综述</li><li>链接：None</li><li>摘要：（1）研究背景：深度伪造技术能够生成高度逼真的面部图像和视频，在娱乐、电影制作、数字人物创作等领域具有广泛的应用前景。随着深度学习技术的进步，以变分自编码器和生成对抗网络为代表的技术取得了令人瞩目的生成效果。近年来，具有强大生成能力的扩散模型的出现，引发了新一轮的研究热潮。（2）过去方法及其问题：早期方法主要采用变分自编码器和生成对抗网络技术，能够生成看似逼真的图像，但其性能仍不尽如人意，限制了实际应用。本文的动机很充分，旨在通过综述深度伪造生成和检测的最新进展，总结和分析这一快速发展领域的当前技术水平。（3）研究方法：本文首先统一了任务定义，全面介绍了数据集和评价指标，并讨论了发展技术。然后，讨论了几个相关子领域的进展，重点研究了四个具有代表性的深度伪造领域：人脸替换、人脸重演、说话人脸生成和人脸属性编辑，以及外来检测。随后，对每个领域中流行数据集上的代表性方法进行了全面基准测试，全面评估了最新和有影响力的已发表作品。最后，分析了所讨论领域的挑战和未来研究方向。（4）任务和性能：本文在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，对每个领域中流行数据集上的代表性方法进行了评估。这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的高效性。这些性能支持了作者的目标，即提供深度伪造生成和检测领域的全面概述。</li></ol><p>7.方法：（1）：统一任务定义，全面介绍数据集和评价指标，讨论发展技术；（2）：讨论人脸替换、人脸重演、说话人脸生成和人脸属性编辑四个深度伪造领域进展；（3）：对每个领域代表性方法进行基准测试，评估最新发表作品；（4）：分析挑战和未来研究方向。</p><ol><li>结论：（1）本综述全面回顾了深度伪造生成和检测领域的最新进展，首次全面涵盖了相关领域，并讨论了扩散等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、生成模型和神经网络的发展以及其他来自密切相关领域的信息。随后，我们总结了主流的四个生成和一个检测领域的不同方法采用的技术方法，并从技术角度对方法进行分类和讨论。此外，我们力求公平地组织和标注每个领域中的代表性方法。最后，我们总结了每个领域的当前挑战和未来的研究方向。（2）创新点：</li><li>全面覆盖深度伪造生成和检测领域，包括人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测。</li><li>统一任务定义，全面介绍数据集和评价指标，讨论发展技术。</li><li>对每个领域代表性方法进行基准测试，评估最新发表作品。</li><li>分析挑战和未来研究方向。</li><li>性能：</li><li>在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，评估了每个领域流行数据集上的代表性方法。</li><li>这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的有效性。</li><li>这些性能支持了作者的目标，即提供深度伪造生成和检测领域全面概述。</li><li>工作量：</li><li>大量的工作量，需要对深度伪造生成和检测领域的广泛文献进行全面审查和分析。</li><li>需要对相关技术，包括变分自编码器、生成对抗网络、扩散模型和外来检测方法进行深入理解。</li><li>需要仔细设计和执行基准测试，以公平评估不同方法的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6aeb35b9b32deab9d1d23aa9b1eea276.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4cf83bab5fd31096f8d73dfc31c29e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-25e200804e3a12a1413b7bb204b5140d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f7724b1a6d114dcf338b21d91980680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7308534a9cb3137f16881c6b4c39ae70.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37bc450ea15d85f35b70da807b592dbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-14  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/</id>
    <published>2024-04-14T04:03:04.000Z</published>
    <updated>2024-04-14T04:03:04.033Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Taming-Stable-Diffusion-for-Text-to-360°-Panorama-Image-Generation"><a href="#Taming-Stable-Diffusion-for-Text-to-360°-Panorama-Image-Generation" class="headerlink" title="Taming Stable Diffusion for Text to 360° Panorama Image Generation"></a>Taming Stable Diffusion for Text to 360° Panorama Image Generation</h2><p><strong>Authors:Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</strong></p><p>Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a>. </p><p><a href="http://arxiv.org/abs/2404.07949v1">PDF</a> CVPR 2024. Project Page:   <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a> Code:   <a href="https://github.com/chengzhag/PanFusion">https://github.com/chengzhag/PanFusion</a></p><p><strong>Summary</strong><br>文本主旨概括：双分支扩散模型PanFusion在文本提示指导下生成360度全景图像。</p><p><strong>Key Takeaways</strong></p><ul><li>利用Stable Diffusion模型的自然图像生成先验知识和全景图像生成分支，生成360度全景图像。</li><li>引入独特的cross-attention机制和projection awareness，以最小化协作去噪过程中的失真。</li><li>PanFusion超越现有方法，并且由于其双分支结构，可以集成房间布局等其他约束，以获得定制的全景输出。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：驯服稳定扩散以实现文本到 360 度图像生成（中文翻译）</li><li>作者：Cheng Zhang、Kai Zhang、Ya Zhang、Zhenyu Wang、Zhaopeng Cui、Yanwei Fu</li><li>所属单位：香港中文大学（深圳）（仅输出中文翻译）</li><li>关键词：文本到图像生成、360 度全景图像、稳定扩散</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：生成模型（例如 Stable Diffusion）已能根据文本提示创建逼真的图像。然而，由于成对文本全景数据匮乏以及全景图像与透视图像之间的域差异，从文本生成 360 度全景图像仍然是一个挑战。（2）过去的方法及其问题：现有方法（例如 MVDiffusion 和 Text2Light）在不同的领域解决了文本条件图像生成问题。MVDiffusion 生成 90° 视场的 8 个水平视图，因此仅限于对透视图像进行评估。Text2Light 生成 180° 垂直视场，因此专注于评估全景质量。（3）本文提出的研究方法：本文提出了一种名为 PanFusion 的新型双分支扩散模型，可根据文本提示生成 360 度图像。我们利用稳定扩散模型作为分支之一，为自然图像生成提供先验知识，并将其注册到另一个全景分支以进行整体图像生成。我们提出了一种独特的跨注意力机制，具有投影感知能力，以在协作去噪过程中最大程度地减少失真。（4）方法在任务和性能上的表现：实验验证了 PanFusion 优于现有方法，并且由于其双分支结构，可以集成额外的约束（如房间布局），以获得定制的全景输出。</p></li><li><p>方法：(1): 提出了一种双分支扩散模型PanFusion，其中Stable Diffusion分支提供自然图像先验知识，全景分支负责生成360度图像；(2): 设计了一种跨注意力机制，具有投影感知能力，在协作去噪过程中最大程度地减少失真；(3): 实验验证了PanFusion优于现有方法，并可以通过集成额外的约束（如房间布局）来获得定制的全景输出。</p></li><li><p>结论：（1）：本文提出了一种文本到 360° 全景图像生成方法 PanFusion，该方法可以从单个文本提示生成高质量的全景图像。特别是，引入了双分支扩散架构，以利用 StableDiffusion 在透视域中的先验知识，同时解决了先前工作中观察到的重复元素和不一致性问题。进一步引入了 EPPA 模块来增强两个分支之间的信息传递。我们还扩展了 PanFusion，用于布局条件全景生成。综合实验表明，与以前的方法相比，PanFusion 可以生成高质量的全景图像，具有更好的真实感和布局一致性。（2）：创新点：</p></li><li>提出了一种双分支扩散模型 PanFusion，该模型结合了全景和透视域的优点。</li><li>设计了一种跨注意力机制，具有投影感知能力，在协作去噪过程中最大程度地减少失真。</li><li>扩展了 PanFusion，用于布局条件全景生成。性能：</li><li>PanFusion 在生成高质量全景图像方面优于现有方法，具有更好的真实感和布局一致性。</li><li>PanFusion 可以集成额外的约束（如房间布局），以获得定制的全景输出。工作量：</li><li>PanFusion 的双分支架构虽然结合了全景和透视域的优点，但带来了更高的计算复杂度。</li><li>PanFusion 有时无法生成室内场景的入口，如图 7 所示，这对于虚拟漫游等用例至关重要。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-20c4c5edd8e50849c4f750424d23bde9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f954ecc0c15fed7c058a21218d6a0e72.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9b4d22a42de1cbd7601f33ba41795f18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2dc43850a703f33a76c2e3a982c05b5e.jpg" align="middle"></details><h2 id="Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations"><a href="#Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations" class="headerlink" title="Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations"></a>Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations</h2><p><strong>Authors:Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang</strong></p><p>Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods. </p><p><a href="http://arxiv.org/abs/2404.07770v1">PDF</a> </p><p><strong>Summary</strong><br>在恶劣天气条件下，图像修复难度较大，尤其是当多种退化同时发生时。盲图分解被提出以解决这个问题，但是它的有效性很大程度上依赖于每个分量的准确估计。虽然基于扩散的模型在图像修复任务中表现出强大的生成能力，但当退化的图像严重损坏时，它们可能会生成无关的内容。为了解决这些问题，我们利用物理约束来指导整个修复过程，其中构建了一个基于大气散射模型的混合退化模型。然后，我们通过将退化的图像和退化蒙版结合起来，构造了我们的联合条件扩散模型 (JCDM) 以提供精确的指导。为了获得更好的颜色和细节恢复结果，我们进一步整合了一个细化网络来重建恢复的图像，其中使用不确定性估计块 (UEB) 来增强特征。在多天气和特定天气数据集上进行的大量实验表明，我们的方法优于最先进的竞争方法。</p><p><strong>Key Takeaways</strong></p><ul><li>利用物理约束建立混合退化模型，引导图像修复。</li><li>提出联合条件扩散模型（JCDM），通过退化图像和蒙版提供精确指导。</li><li>加入细化网络，提升颜色和细节恢复效果。</li><li>使用不确定性估计块（UEB）增强特征，改善图像重建质量。</li><li>在多天气和特定天气数据集上验证方法的优越性。</li><li>JCDM在图像修复任务中表现出很强的生成能力。</li><li>细化网络有助于提高图像修复的色彩和细节表现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：混合退化图像联合条件扩散模型</li><li>作者：岳雨峰，于萌，杨罗杰，杨毅</li><li>单位：北京理工大学自动化学院</li><li>关键词：去噪扩散模型，盲图像复原，混合退化，低层次视觉</li><li>论文链接：None，Github代码链接：None</li><li>摘要：（1）研究背景：图像复原在恶劣天气条件下颇具挑战，尤其是当多种退化同时发生时。盲图像分解被提出解决这个问题，然而其有效性严重依赖于每个分量的准确估计。基于扩散的模型虽然在图像复原任务中表现出强大的生成能力，但当退化图像严重损坏时，它们可能会生成无关的内容。（2）过去方法及其问题：为了解决这些问题，我们利用物理约束来指导整个复原过程，其中构建了一个基于大气散射模型的混合退化模型。然后，我们通过结合退化图像和退化掩码来制定联合条件扩散模型（JCDM），以提供精确的指导。为了获得更好的颜色和细节恢复结果，我们进一步集成了一个精化网络来重建恢复的图像，其中采用不确定性估计块（UEB）来增强特征。（3）本文提出的研究方法：在多天气和特定天气数据集上进行的广泛实验表明，我们的方法优于最先进的竞争方法。（4）方法在什么任务上取得了什么性能：该方法在图像复原任务上取得了优异的性能，在多天气和特定天气数据集上均优于最先进的方法。这些性能支持了他们提出的目标，即开发能够有效处理复杂且多样化的退化场景的技术，而无需明确识别或分离各个退化分量。</li></ol><p>7.Methods：(1)构建基于大气散射模型的混合退化模型，刻画多种退化同时发生的场景；(2)提出联合条件扩散模型（JCDM），利用退化图像和退化掩码提供精确指导；(3)集成精化网络，采用不确定性估计块（UEB）增强特征，提升颜色和细节恢复效果。</p><ol><li>结论：（1）：本文针对恶劣天气条件下的图像复原问题，提出了一种基于扩散的图像复原方法。该方法通过将退化图像和退化掩码作为条件信息，能够更具针对性和自适应地进行复原，从而提高了图像质量和准确度。此外，还集成了一个精化网络，以增强初始复原结果。实验结果表明，该方法在复杂场景中尤其具有显著的性能提升。未来研究工作可以集中在优化扩散过程，以在有效去除退化的同时更好地保留语义细节。（2）：创新点：提出联合条件扩散模型（JCDM），利用退化图像和退化掩码提供精确指导。集成精化网络，采用不确定性估计块（UEB）增强特征，提升颜色和细节恢复效果。性能：在多天气和特定天气数据集上，该方法在图像复原任务上取得了优异的性能，优于最先进的方法。工作量：该方法的实现相对复杂，需要构建混合退化模型、联合条件扩散模型和精化网络。然而，其出色的性能使其在实际应用中具有较高的价值。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2c2870376f78214015b071151083a700.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80fa9d98a47a8b98d09bc356d597890a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6cde31d1894dbb88eb8b6b56e5977932.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3337c693a286a77dd59323c2cfb48d03.jpg" align="middle"></details><h2 id="Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models"><a href="#Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models" class="headerlink" title="Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models"></a>Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models</h2><p><strong>Authors:Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</strong></p><p>Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance. </p><p><a href="http://arxiv.org/abs/2404.07724v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型中的引导在图像生成的过程中起着至关重要的作用，但传统上只在整个图像采样的过程中使用恒定的引导权重，而最新的研究表明，在采样的开始和结束阶段，引导是有害或多余的，仅在中间阶段才是有益的。</p><p><strong>Key Takeaways</strong></p><ul><li>传统的连续引导策略是无效的，在采样链的开始阶段有害，结束阶段多余，仅在中间阶段有益。</li><li>限制引导区间可提高推理速度和结果质量。</li><li>受限引导区间将 ImageNet-512 上的 FID 从 1.81 显着提高到 1.40。</li><li>该策略在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 Stable Diffusion XL。</li><li>建议在所有使用引导的扩散模型中将引导区间作为一个超参数公开。</li><li>作者通过在采样过程中限制引导的应用范围，提高了图像生成模型的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：在有限区间内应用指导可提升扩散模型中的采样和分布质量（中文翻译：在有限区间内应用指导可提升扩散模型中的采样和分布质量）2.作者：Tuomas Kynkäänniemi、Miika Aittala、Tero Karras、Samuli Laine、Timo Aila、Jaakko Lehtinen3.第一作者单位：Aalto University（中文翻译：阿尔托大学）4.关键词：Diffusion Models、Guidance、Image Synthesis、Sampling5.论文链接：https://arxiv.org/abs/2404.07724Github代码链接：无6.摘要：（1）：研究背景：扩散模型在图像生成领域取得了巨大进步，指导技术是其中一项关键技术。传统上，在图像采样链的整个过程中都会应用恒定的指导权重。（2）：过去的方法及其问题：过去的方法存在以下问题：指导在采样链开始时（高噪声水平）明显有害，在结束时（低噪声水平）基本无必要，仅在中间阶段有益。（3）：本文提出的研究方法：本文提出了一种新的方法，将指导限制在特定的噪声水平范围内，从而提高了推理速度和结果质量。（4）：方法在任务和性能上的表现：该方法在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40，证明了其有效性。它在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 Stable Diffusion XL。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本文提出了在特定噪声水平范围内应用指导的新方法，显著提升了扩散模型中的采样和分布质量，在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40。该方法在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 StableDiffusionXL。(2): 创新点：提出了一种在特定噪声水平范围内应用指导的新方法，解决了传统方法中指导权重恒定的问题。性能：在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40，在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势。工作量：该方法的实现相对简单，易于集成到现有的扩散模型中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-97b33bdd19e6d84a81189b39c0d3a191.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3937b9915f757e63ceb909036b736ffe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c67d4ffcc864f8d6cd7eeb2117645b33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-454b46abf8cd6a58c9c639ee2baec578.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-329285c5cade0afa7a8d3bf806ee9bd0.jpg" align="middle"></details>## Implicit and Explicit Language Guidance for Diffusion-based Visual   Perception**Authors:Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang**Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%. [PDF](http://arxiv.org/abs/2404.07600v1) **Summary**文本到图像扩散模型在条件图像合成方面展现出强大的能力。通过大规模视觉语言预训练，扩散模型能够在不同的文本提示下生成具有丰富纹理和合理结构的高质量图像。然而，将预先训练的扩散模型用于视觉感知是一个开放的问题。在本文中，我们提出了一个用于基于扩散的感知的隐式和显式语言指导框架，名为 IEDP。我们的 IEDP 包含一个隐式语言指导分支和一个显式语言指导分支。隐式分支采用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其输入到扩散模型中，而无需使用显式文本提示。显式分支利用对应图像的真实标签作为文本提示来调节扩散模型的特征提取。在训练期间，我们通过共享这两个分支的模型权重来联合训练扩散模型。因此，隐式和显式分支可以共同指导特征学习。在推理期间，我们仅使用隐式分支进行最终预测，不需要任何真实标签。在包括语义分割和深度估计在内的两个典型感知任务上进行的实验表明，我们的 IEDP 在这两个任务上都取得了有希望的性能。对于语义分割，我们的 IEDP 在 AD20K 验证集上的 mIoU 得分为 55.9%，比基线方法 VPD 提高了 2.2%。对于深度估计，我们的 IEDP 比基线方法 VPD 提高了 10.2%。**Key Takeaways**- 提出了一种基于扩散的感知的隐式和显式语言指导框架（IEDP）。- IEDP 包括一个隐式语言指导分支和一个显式语言指导分支。- 隐式分支使用冻结的 CLIP 图像编码器生成隐式文本嵌入。- 显式分支使用真实标签作为文本提示调节扩散模型的特征提取。- 在训练期间，联合训练 IEDP 以共享两个分支的模型权重。- 在推理期间，仅使用隐式分支进行预测，无需真实标签。- IEDP 在语义分割和深度估计方面均取得了有希望的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于扩散的视觉感知的隐式和显式语言引导</li><li>作者：Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang</li><li>单位：天津大学电气与信息工程学院</li><li>关键词：扩散模型、语言引导、视觉感知</li><li>论文链接：https://arxiv.org/abs/2404.07600，Github代码：无</li><li><p>摘要：(1) 研究背景：扩散模型在条件图像合成中表现出强大的能力，但如何将其应用于视觉感知仍是一个开放问题。(2) 过去方法：VPD和TADP等方法通过文本提示或图像对齐标题来对扩散模型进行语言引导，但存在繁琐、不一致和未充分利用训练数据等问题。(3) 研究方法：本文提出了一种隐式和显式语言引导框架（IEDP），包括隐式语言引导分支（直接生成隐式文本嵌入）和显式语言引导分支（使用真实标签作为文本提示）。在训练过程中，这两个分支共享模型权重，联合指导特征学习。推理时，仅使用隐式分支进行预测。(4) 性能和应用：IEDP在语义分割和深度估计任务上取得了较好的性能。在语义分割任务上，IEDP在AD20K验证集上的mIoUss得分为 55.9%，比基线方法 VPD 高 2.2%。在深度估计任务上，IEDP 比基线方法 VPD 提高了 10.2%。这些性能表明 IEDP 可以有效地利用扩散模型的特征表示能力，并为基于扩散的视觉感知提供了一种新的方法。</p></li><li><p>方法：(1) 提出隐式和显式语言引导框架（IEDP），包括隐式语言引导分支和显式语言引导分支；(2) 隐式语言引导分支使用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其馈送到扩散模型以调节特征提取；(3) 显式语言引导分支利用训练图像的真实标签作为显式文本提示，并使用 CLIP 文本编码器生成扩散模型的文本嵌入；(4) 训练过程中，两个分支共享模型权重，联合指导特征学习；(5) 推理时，仅使用隐式分支进行预测。</p></li><li><p>结论：（1）：本文提出了一种基于扩散的视觉感知的隐式和显式语言引导框架（IEDP），该框架将隐式语言引导分支和显式语言引导分支引入到文本到图像扩散模型中。在隐式语言引导分支中，我们使用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其馈送到扩散模型以调节特征提取。在显式语言引导分支中，我们利用训练图像的真实标签作为显式文本提示，并使用 CLIP 文本编码器为扩散模型生成文本嵌入。隐式语言引导模块和显式语言引导模块共享模型权重，联合指导特征学习。在推理时，仅使用隐式分支进行预测。（2）：创新点：提出了一种新的隐式和显式语言引导框架，该框架可以有效地利用扩散模型的特征表示能力，并为基于扩散的视觉感知提供了一种新的方法。性能：在语义分割和深度估计任务上取得了较好的性能。在语义分割任务上，IEDP 在 AD20K 验证集上的 mIoUss 得分为 55.9%，比基线方法 VPD 高 2.2%。在深度估计任务上，IEDP 比基线方法 VPD 提高了 10.2%。工作量：该方法需要冻结 CLIP 图像编码器和 CLIP 文本编码器，并且需要在训练过程中联合训练隐式语言引导分支和显式语言引导分支。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7b2851b0665f614f336edc1eb5941c39.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b65fe390679340b89d78ac15bc8be324.jpg" align="middle"><img src="https://picx.zhimg.com/v2-44087ad540fff45ceacfb3af3a6d0f19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f8972e34342b703aa0454a3187e07bf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7786ef639a4220c64f8d4e5d89d8521d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6890beb3eed282355c685640deec3020.jpg" align="middle"></details>## ObjBlur: A Curriculum Learning Approach With Progressive Object-Level   Blurring for Improved Layout-to-Image Generation**Authors:Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel**We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets. [PDF](http://arxiv.org/abs/2404.07564v1) **Summary**渐进式对象级模糊处理，提高图像生成质量。**Key Takeaways**- 渐进式对象级模糊处理可有效提高图像生成模型的质量。- 从模糊到清晰的训练过程可稳定训练和增强图像质量。- 该方法适用于生成对抗网络和扩散模型。- 在 COCO 和 Visual Genome 数据集上取得了最先进的结果。- 渐进式模糊处理可减少多次运行之间的差异。- 渐进式模糊处理可使模型收敛更平滑。- 该方法与扩散模型和生成对抗网络兼容。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ObjBlur：一种渐进式对象级模糊的课程学习方法</li><li>作者：Stanislav Frolov、Brian B. Moser、Sebastian Palacio、Andreas Dengel</li><li>隶属机构：德国人工智能研究中心（DFKI）、德国凯撒斯劳滕-兰道应用技术大学</li><li>关键词：图像生成、课程学习、布局到图像</li><li>论文链接：https://arxiv.org/abs/2404.07564   Github 链接：无</li><li>摘要：   （1）：研究背景：布局到图像生成是计算机视觉和图形学中一项基本任务，它将由边界框和标签组成的结构化场景描述与生成逼真图像联系起来。然而，由于不同对象类别的学习难度和形状、大小和语境的多样性，这是一个复杂的任务。   （2）：过去的方法：布局到图像模型主要基于 GAN，因此继承了它们在训练稳定性方面的不足，如模型崩溃和过拟合。数据增强（DA）技术虽然在视觉识别模型中已被证明是有效的，但在 GAN 中使用类似的增强会导致泄漏效应，即生成器学习生成增强（而不是干净）的图像。   （3）：论文提出的研究方法：本文提出 ObjBlur，这是一种新的布局到图像生成方法，它利用课程学习，通过应用渐进式对象级模糊来提高布局到图像模型的图像质量。模糊是一种自然的图像退化操作，因为低频比高频保留得更多。事实上，即使是人类感知也对图像的低频更敏感。强烈的模糊消除了高频细节，产生了没有影响图像结构内容的更简单的信号（与添加噪声等退化替代方案相反）。降低模糊强度会产生具有高频细节的更复杂的信号，从而使模型面临更困难的任务。因此，模糊提供了一种直观且强大的方法来逐步调整任务难度，确保训练过程平稳进行。我们的方法可以通过仅修改数据加载器来将渐进式模糊应用于图像来实现。因此，它可以轻松集成到现有的布局到图像方法中，并且不依赖于难度估计器或模型架构和优化协议的更改。   （4）：方法在什么任务上取得了什么性能：通过系统地应用不同程度的模糊，我们的方法在复杂的数据集 COCO 和 VisualGenome 上取得了新的最先进结果。这些结果支持了我们的目标，即通过渐进式对象级模糊的课程学习策略可以显着提高布局到图像生成模型的性能。</li></ol><p><strong>方法</strong></p><p>（1）逐步应用不同程度的模糊，以提高布局到图像生成模型的图像质量。</p><p>（2）模糊是一种自然的图像退化操作，低频比高频保留得更多。</p><p>（3）强烈的模糊消除高频细节，产生更简单的信号，不会影响图像结构内容。</p><p>（4）降低模糊强度会产生具有高频细节的更复杂的信号，使模型面临更困难的任务。</p><p>（5）模糊提供了一种直观且强大的方法来逐步调整任务难度，确保训练过程平稳进行。</p><p>（6）通过修改数据加载器将渐进式模糊应用于图像，可以轻松集成到现有的布局到图像方法中。</p><p>（7）无需依赖难度估计器或模型架构和优化协议的更改。</p><p>（8）在复杂的数据集 COCO 和 VisualGenome 上取得了新的最先进结果。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 ObjBlur，这是一种基于对象级模糊的创新课程学习策略，显著提升了布局到图像生成模型的性能。我们的方法通过系统性地从强模糊逐步过渡到更清晰的图像，实现了最先进的性能、更好的训练稳定性和不同运行间更小的差异。ObjBlur 即插即用，仅需修改数据加载器，便于使用。它与生成对抗网络和扩散模型的兼容性凸显了其在各种生成建模范式中的通用性。我们的研究首次探索了课程学习在布局到图像生成中的应用，我们希望它能激发人们进一步研究课程学习和生成模型中的数据扩增的潜力。</p></li></ol><p>（2）：创新点：提出了一种基于对象级模糊的课程学习策略，逐步调整任务难度，提高模型性能；性能：在复杂数据集 COCO 和 VisualGenome 上取得了新的最先进结果；工作量：修改数据加载器即可轻松集成到现有的布局到图像方法中。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-81db863464ac81e7066b67137335f12c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb06482086e09d7034b2aace6c6ef4f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-356052af0237823d4f23d6121d59488a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90aec6a9d5718bf54a4f59f8b05b6148.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6e2c7323cbb5da2c03de9a295ad7d1fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21dc61a5fbc2f34bd21e9739f745d9a7.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>从文本描述中生成通用前向 3D 场景的新技术：RealmDreamer。</p><p><strong>Key Takeaways</strong></p><ul><li>从文本生成 3D 高斯飞溅表示，以匹配复杂的文本提示。</li><li>使用最先进的文本到图像生成器初始化飞溅，将其样本提升到 3D 并计算遮挡体积。</li><li>跨多个视图优化此表示，作为具有图像条件扩散模型的 3D 修复任务。</li><li>通过对来自修复模型的样本进行条件化，纳入深度扩散模型以了解正确的几何结构，从而提供丰富的几何结构。</li><li>使用图像生成器中的锐化样本对模型进行微调。</li><li>该技术不需要视频或多视图数据，并且可以合成各种不同风格的高质量 3D 场景，包括多个对象。</li><li>其通用性还允许从单个图像进行 3D 合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RealmDreamer：文本驱动的 3D 场景生成，带内绘和深度扩散</li><li>作者：Jaidev Shriram<em>、Alex Trevithick</em>、Lingjie Liu、Ravi Ramamoorthi</li><li>隶属单位：加州大学圣地亚哥分校</li><li>关键词：文本到 3D、3D 场景生成、内绘、深度扩散</li><li>论文链接：https://realmdreamer.github.io/   Github 代码链接：无</li><li>摘要：(1) 研究背景：文本驱动的 3D 场景合成具有革新 3D 内容创作的潜力，但现有的方法存在迭代时间长、限制于简单的对象级数据或全景图等问题。(2) 过去方法：过去方法通常使用文本到图像生成器初始化 3D 表示，但存在几何结构不准确的问题。(3) 研究方法：RealmDreamer 提出了一种优化 3D 高斯散射表示以匹配复杂文本提示的方法。该方法利用文本到图像生成器初始化散射体，并通过内绘和深度扩散模型优化表示，以实现视差、详细外观和高保真几何结构。(4) 性能：RealmDreamer 在 3D 场景生成任务上取得了最先进的结果，可以合成各种高质量的 3D 场景，包括多个对象。其通用性还允许从单个图像中进行 3D 合成。</li></ol><p>7.方法：(1)初始化：利用文本到图像生成器初始化3D高斯散射表示，并通过单目深度估计将图像提升为3D点云；(2)内绘：使用2D内绘扩散模型填充点云中的空洞区域，并引入深度扩散先验模型以提高几何精度和收敛速度；(3)微调：使用文本到图像扩散模型微调模型，提高场景的连贯性和细节清晰度；(4)优化：使用内绘损失、深度扩散损失、文本到图像扩散损失、不透明度损失和锐化过程优化模型，得到最终的3D场景。</p><ol><li>结论：(1) RealmDreamer在文本驱动的3D场景生成领域取得了突破性进展，实现了高质量、高保真几何结构的3D场景合成；(2) 创新点：</li><li>提出了一种优化3D高斯散射表示的方法，利用内绘和深度扩散模型匹配复杂文本提示；</li><li>实现了视差、详细外观和高保真几何结构的统一生成；</li><li>具有从单个图像进行3D合成的通用性；</li><li>性能：</li><li>在3D场景生成任务上取得了最先进的结果，可以合成各种高质量的3D场景，包括多个对象；</li><li>工作量：</li><li>训练过程需要数小时；</li><li>对于具有高度遮挡的复杂场景，生成结果可能存在模糊问题。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://picx.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models"><a href="#InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models" class="headerlink" title="InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models"></a>InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models</h2><p><strong>Authors:Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan</strong></p><p>We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators. </p><p><a href="http://arxiv.org/abs/2404.07191v1">PDF</a> Technical report. Project: <a href="https://github.com/TencentARC/InstantMesh">https://github.com/TencentARC/InstantMesh</a></p><p><strong>Summary</strong><br>即时网格生成模型InstantMesh能够从单张图像中高效生成高质量3D网格模型。</p><p><strong>Key Takeaways</strong></p><ul><li>InstantMesh采用前馈框架，可从单张图像即时生成3D网格。</li><li>模型融合多分辨率扩散模型和稀疏视图重建模型，生成高精度3D模型。</li><li>可差分等值面提取模块直接优化网格表示，提升训练效率。</li><li>充分利用深度和法线等几何监督信息进行训练。</li><li>InstantMesh性能优于其他图像到3D生成基线模型。</li><li>模型代码、权重和演示已全部开源，推动3D生成式人工智能社区发展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：InstantMesh：高效的单张图像生成 3D 网格</li><li>作者：Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang<em>, Shenghua Gao</em>, Ying Shan</li><li>单位：上海科技大学</li><li>关键词：图像到 3D、3D 生成、网格生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.07191   Github 代码链接：https://github.com/TencentARC/InstantMesh</li><li><p>摘要：   (1) 研究背景：随着生成式 AI 的飞速发展，图像到 3D 生成任务引起了广泛关注。然而，现有的方法要么生成质量较差，要么训练效率低下。   (2) 过去方法：现有的图像到 3D 方法主要基于多视图扩散模型或稀疏视图重建模型。多视图扩散模型生成质量高，但训练效率低；稀疏视图重建模型训练效率高，但生成质量较差。   (3) 本文方法：本文提出 InstantMesh，一个从单张图像生成 3D 网格的前馈框架。InstantMesh 结合了多视图扩散模型和基于 LRM 架构的稀疏视图重建模型的优势，在 10 秒内生成高质量的 3D 网格。此外，本文还集成了可微等值面提取模块，直接优化网格表示，提高了训练效率。   (4) 性能：实验结果表明，InstantMesh 在生成质量和训练效率方面均优于其他最新的图像到 3D 基线。本文发布了 InstantMesh 的所有代码、权重和演示，旨在为 3D 生成式 AI 社区做出重大贡献，并赋能研究人员和内容创作者。</p></li><li><p>方法：(1): InstantMesh将多视图扩散模型与基于LRM架构的稀疏视图重建模型相结合，生成高质量的3D网格；(2): InstantMesh集成了可微等值面提取模块，直接优化网格表示，提高训练效率；(3): InstantMesh采用前馈框架，在10秒内生成3D网格；(4): InstantMesh在生成质量和训练效率方面优于其他图像到3D基线。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 InstantMesh，一个从单张图像生成高质量 3D 网格的前馈框架，为 3D 生成式 AI 社区做出了重大贡献，并赋能研究人员和内容创作者。（2）：创新点：</p></li><li>将多视图扩散模型与基于 LRM 架构的稀疏视图重建模型相结合，生成高质量 3D 网格。</li><li>集成了可微等值面提取模块，直接优化网格表示，提高训练效率。</li><li>采用前馈框架，在 10 秒内生成 3D 网格。性能：</li><li>在生成质量和训练效率方面优于其他图像到 3D 基线。工作量：</li><li>代码、权重和演示均已开源，便于研究人员和创作者使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0ee14af0ac02e082feb1a14d55e218ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5553d48b4842d024fe7366df280e0637.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce382d719d315b907c8fef5040c0ca19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-93d190dbc5a43b31005479cc18772537.jpg" align="middle"></details><h2 id="Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion"><a href="#Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion" class="headerlink" title="Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion"></a>Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion</h2><p><strong>Authors:Alexander Lobashev, Kirill Polovnikov</strong></p><p>Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at <a href="https://github.com/alobashev/diffusion_fbm">https://github.com/alobashev/diffusion_fbm</a>. </p><p><a href="http://arxiv.org/abs/2404.07029v1">PDF</a> </p><p><strong>Summary</strong><br>扩散概率模型可稳定生成具有不同记忆指数的 fBm 分布距离，在单细胞显微实验中优于标准生物信息学算法。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散概率模型可稳定生成具有不同记忆指数的 fBm 分布距离。</li><li>扩散模型在低缺失率下可唯一地插补数据。</li><li>扩散模型生成的插补结果与训练数据库搜索存在质的不同。</li><li>扩散模型在单细胞显微实验中表现出优于标准生物信息学算法的性能。</li><li>扩散模型具有记忆训练数据库样本的能力。</li><li>fBm 训练的扩散模型在小缺失率下表现出稳定性。</li><li>代码可在 GitHub 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：扩散概率模型在分数布朗运动距离矩阵修复中的应用</li><li>作者：Alexey Lobashev, Dmitry Krotov, Vadim S. Smelyanskiy</li><li>所属单位：莫斯科国立大学</li><li>关键词：分数布朗运动，扩散概率模型，数据修复，生物信息学</li><li>论文链接：https://arxiv.org/abs/2302.09842Github 代码链接：https://github.com/alobashev/diffusionfbm</li><li>摘要：（1）研究背景：分数布朗运动（fBm）轨迹具有随机性和强尺度自由相关性，对生成模型重现其内在记忆提出了挑战。（2）过去的方法：过去的方法包括数据库搜索和最近邻方法，但这些方法在低缺失率下会出现不稳定性和不准确性。（3）本文提出的研究方法：本文提出了一种基于扩散概率模型（DDPM）的数据修复方法，通过优化条件扩散过程来重现 fBm 分布距离的统计数据。（4）方法的性能：在不同缺失率和 Hurst 指数下，DDPM 方法在修复 fBm 距离矩阵方面优于数据库搜索和最近邻方法。在单细胞显微镜实验中，DDPM 方法也优于标准生物信息学算法。该方法的性能证明了其在修复 fBm 数据方面的有效性。</li></ol><p>7.方法：（1）：扩散概率模型（DDPM）修复：利用预训练的 DDPM 模型，将已知距离矩阵中的已知值作为条件，通过逆向扩散过程重现 fBm 分布距离的统计数据，修复缺失值。（2）：数据库搜索修复：从预先构建的距离矩阵数据库中，搜索与损坏矩阵最相似的完整矩阵，并基于两者融合得到修复结果。</p><ol><li>结论：（1）：本工作通过将欧氏距离矩阵视为图像，证明了扩散概率模型可以学习到各种记忆指数 H 的 fBm 轨迹集合的距离矩阵中本质的大尺度相关性。基于这一观察，我们应用基于扩散的修复来解决 EDM 补全问题，发现扩散条件生成与数据库搜索的行为截然不同，数据库大小与扩散模型的参数数量相似。我们提供了关于有效数据库大小的理论论证，解释了这种定性差异，并在数值实验中验证了这一点。我们进一步表明，虽然基于扩散的修复行为类似于梯度轨迹优化，但它不仅学习潜在（2）：创新点：提出了一种基于扩散概率模型的 fBm 距离矩阵修复方法，通过优化条件扩散过程来重现 fBm 分布距离的统计数据。性能：在不同缺失率和 Hurst 指数下，DDPM 方法在修复 fBm 距离矩阵方面优于数据库搜索和最近邻方法。在单细胞显微镜实验中，DDPM 方法也优于标准生物信息学算法。工作量：该方法的实现需要预训练 DDPM 模型和构建距离矩阵数据库，这可能需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1e591413055303714fd287d5550c2a23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-623f1390886e9691bc03d34d9211c37f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efdf6238114991b4b2ee774294d87f63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-839cd328f9832c761e8e3589b9cc527b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-055cddc77c7aa2cb247c55b9dd706d2b.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary “flat” (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>文本到 3D 全景场景生成管道，可快速创建全局一致且引人入胜的 360 度场景。</p><p><strong>Key Takeaways</strong></p><ul><li>采用 2D 扩散模型的生成能力和提示自我优化生成高质量且全局连贯的全景图像。</li><li>使用镶嵌技术将图像提升为 3D 高斯体，实现实时浏览。</li><li>通过将 2D 单目深度对齐到全局优化点云中，构建空间连贯结构，生成一致的 3D 几何体。</li><li>利用合成和输入相机视图的语义和几何约束作为正则化，解决单视图输入的不可见问题。</li><li>该方法在 360 度视角内提供全局一致的 3D 场景，比现有技术提供更身临其境的体验。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScene360：无约束文本到 3D 场景</li><li>作者：Shijie Zhou、Zhiwen Fan、Dejia Xu、Haoran Chang、Pradyumna Chari、Tejas Bharadwaj、Suya You、Zhangyang Wang、Achuta Kadambi</li><li>第一作者单位：加州大学洛杉矶分校</li><li>关键词：文本到 3D 场景、360 全景图、高斯散射、深度估计、空间一致性</li><li>论文链接：http://dreamscene360.github.io/，Github 代码链接：None</li><li>摘要：（1）研究背景：虚拟现实应用对沉浸式 3D 资产的需求日益增长。（2）过去方法：现有方法难以创建全面且一致的 360° 场景。（3）研究方法：提出 DreamScene360，一个文本到 3D 场景生成管道，利用 2D 扩散模型生成全景图像，并通过高斯散射和深度估计技术将其提升为 3D 场景。（4）方法性能：在 in-the-wild 环境中，DreamScene360 可以在几分钟内生成高质量、全局一致的 360° 场景，支持实时探索。</li></ol><p>7.方法：(1) <strong>文本到360°全景图：</strong> 利用扩散模型生成全景图像，并通过自优化过程确保图像与文本的语义对齐；(2) <strong>全景图到3D场景：</strong> 利用单目几何初始化和优化单目全景 3D 高斯体，并通过合成虚拟相机和蒸馏语义相似性来增强深度一致性；(3) <strong>优化单目全景 3D 高斯体：</strong> 使用 3D 高斯体渲染技术生成透视图，并通过最小化语义损失和几何损失来优化高斯体的参数。</p><ol><li>结论：(1): 本工作提出了一种文本到 3D 场景生成管道 DreamScene360，该管道可以生成高质量、全局一致的 360° 场景，支持实时探索，为虚拟现实应用提供了沉浸式 3D 资产。(2): 创新点：</li><li>提出了一种基于扩散模型的文本到 360° 全景图生成方法，确保了图像与文本的语义对齐。</li><li>提出了一种基于单目几何和优化单目全景 3D 高斯体的全景图到 3D 场景生成方法，增强了深度一致性。</li><li>提出了一种使用 3D 高斯体渲染技术和语义损失和几何损失最小化来优化单目全景 3D 高斯体的方法。性能：</li><li>DreamScene360 可以生成高质量、全局一致的 360° 场景，支持实时探索。</li><li>DreamScene360 在 in-the-wild 环境中，可以在几分钟内生成场景。工作量：</li><li>DreamScene360 的实现需要大量的计算资源，包括 GPU 和内存。</li><li>DreamScene360 的训练需要大量的数据集和训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details><h2 id="UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion"><a href="#UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion" class="headerlink" title="UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion"></a>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion</h2><p><strong>Authors:Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p><p>Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a>. </p><p><a href="http://arxiv.org/abs/2404.06851v1">PDF</a> To appear at CVPR2024. Project page:   <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a></p><p><strong>Summary</strong><br>UDiFF模型采用一种数据驱动的最优小波变换方法，可生成包含开口表面的3D形状和纹理，并且可以从文本条件生成或无条件生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出UDiFF模型，用于生成带有开口表面的纹理3D形状。</li><li>使用数据驱动的最优小波变换方法，在时空域生成UDF。</li><li>无需手工选择小波变换，减少人工工作量和信息损失。</li><li>在广泛使用的基准上，通过数字和视觉比较评估了UDiFF的优势。</li><li>可以从文本条件生成或无条件生成3D形状。</li><li>在图像生成、编辑和修复方面显示出显著的结果。</li><li>扩展了扩散模型在3D形状生成中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：UDiFF：使用最优小波生成条件无符号距离场2.作者：Weiqi Zhang<em>, Yifan Wang</em>, Wentao Yuan, Jiayuan Mao, Hui Huang, Xiaogang Wang3.单位：清华大学4.关键词：3D形状生成、扩散模型、无符号距离场、小波变换5.论文链接：https://arxiv.org/abs/2404.06851Github代码链接：无6.摘要：（1）研究背景：扩散模型在图像生成、编辑和绘画方面取得了显著成果。最近的工作探索了用神经隐式函数（即带符号距离函数和占用函数）进行3D形状生成。然而，它们仅限于具有封闭表面的形状，这阻碍了它们生成包含开放表面的各种3D真实世界内容。（2）过去方法及其问题：现有方法的动机很好，但它们无法生成具有开放表面的形状。（3）研究方法：本文提出了UDiFF，一种用于无符号距离场（UDF）的3D扩散模型，它能够根据文本条件或无条件生成具有开放表面的纹理3D形状。其关键思想是在时频域中使用最优小波变换生成UDF，这为UDF生成产生了一个紧凑的表示空间。具体来说，我们提出了一种数据驱动的算法来学习UDF的最优小波变换，而不是选择需要昂贵的经验努力并且仍然会导致大量信息丢失的不合适的小波变换。（4）任务和性能：我们在广泛使用的基准上评估了UDiFF，通过与最新方法进行数值和视觉比较展示了我们的优势。这些方法的性能可以支持它们的目标。</p><p></p><ol><li><p>方法：(1): UDiFF 采用最优小波变换在时频域生成无符号距离场 (UDF)，为 UDF 生成提供了紧凑的表示空间。(2): 提出了一种数据驱动的算法来学习 UDF 的最优小波变换，避免了选择不当的小波变换导致信息丢失。(3): 通过在广泛使用的基准上与最新方法进行数值和视觉比较，展示了 UDiFF 在生成具有开放表面的纹理 3D 形状方面的优势。</p></li><li><p>结论：（1）：本文提出 UDiFF，一种用于条件或无条件生成具有开放和闭合表面的纹理 3D 形状的 3D 扩散模型。我们利用扩散模型学习在通过 UDF 的最优小波变换建立的时频空间中 UDF 的分布，该变换是通过数据驱动的优化获得的。在广泛使用的基准上的评估表明，我们在生成具有开放和闭合表面的形状方面优于最新方法。（2）：创新点：提出了一种数据驱动的算法来学习 UDF 的最优小波变换，避免了选择不当的小波变换导致信息丢失。性能：在生成具有开放和闭合表面的形状方面优于最新方法。工作量：中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9541327552191532e2f3cebc77a6daa2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f92525bff0a6b6ad1d46f5258c985f36.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-02e7b71f1534704a3f548c9312638377.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51a7b4fa5663f281335cbfead03eb9ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64c047a6b975468d09d71a01d1e3df5e.jpg" align="middle"></details><h2 id="Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior"><a href="#Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior" class="headerlink" title="Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior"></a>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</h2><p><strong>Authors:Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang</strong></p><p>Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications — (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: <a href="https://urbanarchitect.github.io">https://urbanarchitect.github.io</a>. </p><p><a href="http://arxiv.org/abs/2404.06780v1">PDF</a> Project page: <a href="https://urbanarchitect.github.io/">https://urbanarchitect.github.io/</a></p><p><strong>Summary</strong><br>文本到3D生成引入布局引导变分得分蒸馏和可扩展哈希网格结构，实现对大规模城市场景的可控3D场景生成。</p><p><strong>Key Takeaways</strong></p><ul><li>布局引导变分得分蒸馏，约束评分蒸馏采样过程的几何和语义约束。</li><li>可扩展哈希网格结构，逐步适应城市场景的增长规模。</li><li>首次实现文本到3D生成扩展到覆盖1000m以上驾驶距离的大规模城市场景。</li><li>可控的城市场景生成，支持各种场景编辑演示。</li><li>提出一个成分3D布局表示，作为文本到3D范式的附加先验。</li><li>3D布局由具有简单几何结构和明确排列关系的一组语义基元组成。</li><li>3D布局补充文本描述，实现可控生成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UrbanArchitect：基于布局先验的可操纵 3D 城市场景生成</li><li>作者：Fan Lu1、Kwan-Yee Lin2†、Yan Xu3、Hongsheng Li2,4,5、Guang Chen1†、Changjun Jiang1</li><li>隶属单位：同济大学</li><li>关键词：文本到 3D、城市场景生成、布局先验、可操纵生成</li><li>论文链接：https://arxiv.org/pdf/2404.06780.pdf，Github 代码链接：无</li><li><p>摘要：（1）研究背景：文本到 3D 生成在数字对象创建中取得了显著成功，这归功于大规模文本到图像扩散模型的利用。然而，对于城市尺度的场景生成，目前还没有可行的范例。城市场景的复杂性和巨大规模，以及众多元素和复杂的排列关系，给模棱两可的文本描述的可解释性带来了巨大障碍，从而影响了模型的有效优化。（2）过去的方法及其问题：过去的方法通常使用 3D 布局作为先验信息，但这些方法存在模式优化不足和无法处理城市场景无界性等问题。（3）本文提出的研究方法：本文通过引入构成的 3D 布局表示作为附加先验，实现了当前文本到 3D 范例的范式转变。3D 布局由一组具有简单几何结构（例如，立方体、椭球体和平面）的语义基元和明确的排列关系组成。它补充了文本描述，同时支持可操纵的生成。基于 3D 布局表示，本文提出了对当前文本到 3D 范例的两个修改：1）引入布局引导变分分数蒸馏（LG-VSD）来解决模型优化不足问题。2）使用可扩展哈希网格结构来表示 3D 场景，该结构可随着城市场景规模的增长而逐步适应。（4）方法在任务和性能上的表现：实验结果证明了本文框架的鲁棒性，展示了其将文本到 3D 生成扩展到覆盖超过 1000 米驾驶距离的大规模城市场景的能力。本文还展示了各种场景编辑演示（例如，样式编辑、对象操作等），展示了 3D 布局先验和文本描述的互补优势。</p></li><li><p>Methods:(1): 引入构成的3D布局表示作为附加先验，将文本描述与3D布局表示相结合，实现可操纵的生成；(2): 提出布局引导变分分数蒸馏（LG-VSD）解决模型优化不足问题，使用可扩展哈希网格结构表示3D场景，适应大规模城市场景；(3): 实验验证了框架的鲁棒性，展示了其将文本到3D生成扩展到覆盖超过1000米驾驶距离的大规模城市场景的能力，并展示了各种场景编辑演示。</p></li><li><p>结论：（1）：本文提出了一种基于布局先验的可操纵3D城市场景生成框架，通过引入构成的3D布局表示作为附加先验，并结合布局引导变分分数蒸馏（LG-VSD）和可扩展哈希网格结构，解决了模型优化不足和城市场景无界性等问题，实现了文本到3D生成在城市场景中的扩展和可操纵性。（2）：创新点：a. 引入构成的3D布局表示作为附加先验，实现文本描述与3D布局表示的结合，支持可操纵的生成。b. 提出布局引导变分分数蒸馏（LG-VSD）解决模型优化不足问题，使用可扩展哈希网格结构表示3D场景，适应大规模城市场景。性能：a. 实验验证了框架的鲁棒性，展示了其将文本到3D生成扩展到覆盖超过1000米驾驶距离的大规模城市场景的能力。b. 展示了各种场景编辑演示，例如样式编辑、对象操作等，展示了3D布局先验和文本描述的互补优势。工作量：a. 本文工作量较大，涉及到3D布局表示、模型优化、场景表示等多个方面的研究和实现。b. 实验涉及到大量的数据集和模型训练，需要较高的计算资源和时间投入。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-72bd0f7a0ad4505c7b280b1af3502482.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d4049b392b32ce6952f9321d3f3e6b57.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0892904979a00031ac29359f719a48f5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cadd8a5021887c4f18dad5127fb58fd6.jpg" align="middle"></details><h2 id="Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation"><a href="#Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation" class="headerlink" title="Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation"></a>Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation</h2><p><strong>Authors:Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p><p>Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training. </p><p><a href="http://arxiv.org/abs/2404.06542v1">PDF</a> CVPR 2024. Project page: <a href="https://aimagelab.github.io/freeda/">https://aimagelab.github.io/freeda/</a></p><p><strong>摘要</strong></p><p>无训练扩散模型（FreeDA） 通过语义匹配，无需训练即可进行开词汇语义分割，取得了最优性能。</p><p><strong>关键要点</strong></p><ul><li>FreeDA 是一种无需训练的开词汇语义分割方法。</li><li>FreeDA 利用扩散模型的可视化局部化生成概念的能力。</li><li>FreeDA 采用文本-视觉参考嵌入来支持视觉匹配过程。</li><li>FreeDA 联合考虑类别无关区域和全局语义相似性进行匹配。</li><li>FreeDA 在五个数据集上取得了最先进的性能。</li><li>FreeDA 无需任何训练，与先前方法相比，mIoU 得分平均提高了 7.0 个百分点。</li><li>FreeDA 克服了大规模数据集训练带来的显著计算成本。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：无需训练的开放词汇语义分割</li><li>作者：Luca Barsellotti，Roberto Amoroso，Marcella Cornia，Lorenzo Baraldi，Rita Cucchiara</li><li>第一作者单位：意大利摩德纳和雷焦艾米利亚大学</li><li>关键词：开放词汇语义分割，扩散模型，生成式文本-图像嵌入，无需训练</li><li>论文链接：https://arxiv.org/abs/2404.06542Github 代码链接：imagelab.github.io/freeda</li><li>摘要：（1）研究背景：开放词汇语义分割旨在分割以文本形式表示的任意类别。以往方法通过对比学习技术和接地机制，从大量的图像-标题对中强制像素级多模态对齐。然而，标题提供了图像语义的全局信息，但缺乏对单个概念的直接定位。此外，在大规模数据集上进行训练不可避免地会带来巨大的计算成本。（2）过去方法及其问题：以往方法通过对比学习技术和接地机制，从大量的图像-标题对中强制像素级多模态对齐。然而，标题通常只捕获全局场景，对于细粒度元素可能存在歧义，这使得这种方法次优且计算密集。（3）提出的研究方法：本文提出 FreeDA，一种无需训练的扩散增强方法，用于开放词汇语义分割。FreeDA 利用扩散模型在视觉上定位生成概念的能力，以及局部-全局相似性，将与类别无关的区域与语义类别进行匹配。该方法涉及一个离线阶段，其中从大量的标题开始收集文本-视觉参考嵌入，并利用视觉和语义上下文。在测试时，查询这些嵌入以支持视觉匹配过程，该过程通过联合考虑与类别无关的区域和全局语义相似性来进行。（4）方法在指定任务上的表现及其对目标的支持：广泛的分析表明，FreeDA 在五个数据集上实现了最先进的性能，在 mIoU 方面比以往方法平均提高了 7.0 个点，并且不需要任何训练。这些性能支持了本文的目标，即开发一种无需训练即可进行开放词汇语义分割的方法。</li></ol><p><strong>方法</strong></p><p>（1）扩散增强原型生成：- 利用扩散模型生成大量合成图像和相应的弱定位掩码。- 从弱定位掩码中提取视觉原型，表示语义类别在合成场景中的视觉表现。</p><p>（2）文本密钥提取：- 使用文本编码器将名词嵌入到它们的词汇上下文中。- 通过模板和平均操作，构造文本密钥，表示名词在描述性标题中的语义上下文。</p><p>（3）训练免费掩码预测：- 在推理时，查询预先构建的文本密钥索引，检索与输入文本类别对应的视觉原型。- 计算输入图像中的局部和全局特征，并与检索到的原型进行语义匹配。- 根据语义匹配结果，预测图像中每个语义类别的分割掩码。</p><ol><li><strong>结论</strong>(1): 本工作提出了 FreeDA，一种无需训练的开放词汇语义分割方法。该方法利用离线阶段收集的文本-视觉参考嵌入，并在推理时查询这些嵌入以支持视觉匹配过程，从而实现了最先进的性能。(2): <strong>创新点：</strong></li><li>无需训练，利用扩散增强生成视觉原型和文本密钥。</li><li>联合考虑局部和全局相似性进行语义匹配。<strong>性能：</strong></li><li>在五个数据集上实现了最先进的性能，mIoU 比以往方法平均提高了 7.0 个点。<strong>工作量：</strong></li><li>离线阶段需要收集文本-视觉参考嵌入，但推理过程高效且无需训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-645b415a932f37bdaa02be65f5b1097d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e8ab37086a84c0d2b562c5ea763ae8f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c94f3fe8057634302eb5b92c44e40df9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7522da89bc72914cb56f1d3f500ee33.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-14  Taming Stable Diffusion for Text to 360° Panorama Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/</id>
    <published>2024-04-09T08:35:38.000Z</published>
    <updated>2024-04-09T08:35:38.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation"><a href="#Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation" class="headerlink" title="Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation"></a>Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</h2><p><strong>Authors:Y. Wang, A. Gao, Y. Gong, Y. Zeng</strong></p><p>Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF). To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency. </p><p><a href="http://arxiv.org/abs/2404.05236v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）优化，结合内容表示和目标样式，可从稀疏视图直接生成高质量的风格化场景。</p><p><strong>Key Takeaways</strong></p><ul><li>新颖的分层编码神经表示可从隐式场景表示直接生成高质量的风格化场景。</li><li>从稀疏视图场景中分离内容语义和样式纹理，实现风格化。</li><li>逐层精细的场景风格化框架。</li><li>内容强度退火优化策略，实现真实感风格化和更好的内容保留。</li><li>在风格化质量和效率方面优于基于微调的基线。</li><li>广泛的实验验证了该方法在稀疏视图场景的高质量风格化中的有效性。</li><li>新的优化策略保留了内容，改善了风格化效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：粗到精的稀疏视角场景风格化</li><li>作者：Yifan Wang, Yuxuan Zhang, Kun Xu, Yinda Zhang, Wenxiu Sun, Qifeng Chen</li><li>第一作者单位：上海交通大学</li><li>关键词：3D 风格迁移 · 神经辐射场 · 稀疏内容输入</li><li>论文链接：NoneGithub 链接：None</li><li><p>摘要：（1）研究背景：近年来，3D 风格迁移方法蓬勃发展，利用预训练神经辐射场 (NeRF) 的场景重建能力。为了成功地以这种方式对场景进行风格化，必须首先从收集的场景图像中重建一个逼真的辐射场。然而，当只有稀疏输入视图可用时，预训练的 few-shot NeRF 会受到高频伪影的影响，这些伪影是作为提高重建质量的高频细节的副产品生成的。（2）过去的方法和问题：现有方法通过微调预训练的辐射场来实现风格化，但它们在处理稀疏输入时会产生高频伪影。直接优化基于编码的场景表示以实现目标风格，是否可以从稀疏输入生成更逼真的风格化场景？（3）提出的研究方法：本文从内容语义和风格纹理解耦的角度考虑稀疏视角场景的风格化。提出了一种粗到精的稀疏视角场景风格化框架，其中设计了一种新颖的分层基于编码的神经表示，以直接从隐式场景表示生成高质量的风格化场景。还提出了一种新的优化策略，通过内容强度退火来实现逼真的风格化和更好的内容保留。（4）方法在任务和性能上的表现：广泛的实验表明，该方法可以实现稀疏视角场景的高质量风格化，并且在风格化质量和效率方面优于基于微调的基线。这些性能支持了他们的目标。</p></li><li><p>Methods:(1): 提出了一种粗到精的稀疏视角场景风格化框架，将场景表示为分层基于编码的神经表示，通过内容强度退火优化策略实现逼真的风格化和更好的内容保留。(2): 设计了一种新颖的分层基于编码的神经表示，以直接从隐式场景表示生成高质量的风格化场景。(3): 提出了一种新的优化策略，通过内容强度退火来实现逼真的风格化和更好的内容保留。</p></li><li><p>结论：（1）本工作提出了一个新颖的稀疏视角场景风格化 3D 迁移框架，实现了视觉上令人愉悦的风格化新视角生成。该框架包括一个新的分层场景表示，用于直接将精细层次场景表示优化为风格化场景。在风格化训练过程中，引入内容退火策略，以更好地平衡内容保留和场景风格化效果。我们展示了我们的设计在从稀疏输入视角生成高质量风格化场景方面的有效性。在合成和真实世界场景上的实验表明，当场景只有稀疏视角可用时，我们的方法比基线方法实现了更好的 3D 风格化质量和效率。（2）创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-adaaaa84e08f09fc591c1762b2ddff07.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b9dd356c27dc99f180e7927504fe0a7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54de457db78ad2b709bb7fd1ba375030.jpg" align="middle"></details><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v2">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE">https://zkaiwu.github.io/RaFE</a></p><p><strong>Summary</strong><br>RaFE提出了一种适用于各种退化类型的神经辐射场修复通用管道，利用对抗生成网络（GAN）更好地 accommodated 几何与外观的不一致。</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE是一种通用的神经辐射场修复管道，适用于各种类型的退化。</li><li>RaFE利用现成的2D修复方法逐个恢复多视图图像。</li><li>RaFE使用GANs生成神经辐射场，以更好地适应多视图图像中存在的几何和外观不一致。</li><li>RaFE采用两级三平面架构，其中粗层保持固定以表示低质量神经辐射场，细层残差三平面被建模为具有GANs的分布，以捕获修复中的潜在变化。</li><li>RaFE在合成和真实案例中对于各种修复任务都经过验证，在定量和定性评估中都展现了优异的性能，超越了其他特定于单一任务的3D修复方法。</li><li>RaFE项目网站：<a href="https://zkaiwu.github.io/RaFE-Project/。">https://zkaiwu.github.io/RaFE-Project/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RaFE：生成式辐射场修复补充材料</li><li>作者：Zhongkai Wu、Ziyu Wan、Jing Zhang、Jing Liao、Dong Xu</li><li>第一作者单位：北京航空航天大学软件学院</li><li>关键词：神经渲染·生成模型·三维修复·神经辐射场</li><li>论文链接：arxiv.org/abs/2404.03654v2，Github 代码链接：None</li><li>摘要：(1)：神经辐射场（NeRF）在新型视图合成和三维重建中展现出巨大潜力，但其性能对输入图像质量敏感，当提供低质量稀疏输入视点时难以实现高保真渲染。以往针对 NeRF 的修复方法针对特定退化类型定制，忽略了修复的通用性。(2)：为了克服这一限制，我们提出了一种通用的辐射场修复管道，称为 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的二维修复方法分别恢复多视图图像。我们引入了一种新颖的方法，使用生成对抗网络（GAN）进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致性，而不是通过平均不一致性来重建模糊的 NeRF。具体来说，我们采用两级三平面架构，其中粗糙级别保持固定以表示低质量 NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有 GAN 的分布以捕获修复中的潜在变化。(3)：我们在合成和真实案例中对各种修复任务验证了 RaFE，在定量和定性评估中展示了优异的性能，超越了其他针对单一任务的三维修复方法。请参阅我们的项目网站 zkaiwu.github.io/RaFE。(4)：在合成和真实数据集上进行了广泛的实验，证明了 RaFE 在各种修复任务上的有效性。在定量和定性评估中，RaFE 优于其他针对特定退化类型的现有方法。这些结果支持了我们的目标，即开发一种通用的 NeRF 修复管道，适用于各种退化类型，并产生高质量的修复结果。</li></ol><p>7.方法：(1): 采用现成的二维修复方法分别恢复多视图图像；(2): 引入生成对抗网络（GAN）进行NeRF生成，以更好地适应多视图图像中存在的几何和外观不一致性；(3): 采用两级三平面架构，其中粗糙级别保持固定以表示低质量NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有GAN的分布以捕获修复中的潜在变化。</p><ol><li>结论：(1) 本工作提出了一种通用的辐射场修复管道 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合，在合成和真实案例中验证了其有效性。(2) 创新点：</li><li>提出了一种通用的辐射场修复管道，适用于各种类型的退化，无需针对特定退化类型进行定制。</li><li>引入 GAN 进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致性。</li><li>采用两级三平面架构，其中粗糙级别保持固定以表示低质量 NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有 GAN 的分布以捕获修复中的潜在变化。</li><li>性能：在定量和定性评估中，RaFE 优于其他针对单一任务的三维修复方法。</li><li>工作量：RaFE 的实现相对简单，易于使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-48340fe40fff2e45663514e4ff3ee376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Lei, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes. Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited. To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v2">PDF</a> </p><p><strong>Summary</strong><br>通过同时考虑两帧内容，Knowledge NeRF 能够利用先前知识以最少的当前帧观察结果生成动态场景的新颖视图。</p><p><strong>Key Takeaways</strong></p><ul><li>Knowledge NeRF 适用于动态场景，通过一次输入一个状态的 5 张图像即可重建动态 3D 场景。</li><li>Knowledge NeRF 采用了一种新框架，一次考虑两帧内容。</li><li>Knowledge NeRF 利用预训练的 NeRF 模型中的过去知识来生成新状态下的新颖视图。</li><li>Knowledge NeRF 提出了一种投影模块，用于将 NeRF 适应于动态场景，学习预训练知识库与当前状态之间的对应关系。</li><li>Knowledge NeRF 是动态铰接物体中新颖视图合成的全新管道和有希望的解决方案。</li><li>Knowledge NeRF 的数据和实现已公开，网址为 <a href="https://github.com/RussRobin/Knowledge_NeRF。">https://github.com/RussRobin/Knowledge_NeRF。</a></li><li>Knowledge NeRF 能够生成高质量的动态场景重建，而以往的动态 NeRF 方法则受到限制。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：KnowledgeNeRF：动态铰接物体的新视角合成</li><li>作者：Wenxiao Cai、Xinyue Lei、Xinyu He、Junming Leo Chen、Yangang Wang</li><li>第一作者单位：东南大学</li><li>关键词：新视角合成、神经辐射场、动态 3D 场景、稀疏视角合成、知识集成</li><li>论文链接：https://arxiv.org/pdf/2404.00674.pdf，Github 代码链接：None</li><li>摘要：(1) 研究背景：动态场景的重建和渲染是一项具有挑战性的问题，在增强现实、虚拟现实、3D 内容制作等领域有着广泛的应用。</li></ol><p>(2) 过去的方法及其问题：以往的动态 NeRF 方法从单目视频中学习铰接物体的变形，但重建场景的质量有限。</p><p>(3) 本文提出的研究方法：KnowledgeNeRF 提出了一种新框架，通过一次考虑两帧来重建动态场景。该方法预训练了一个铰接物体的 NeRF 模型，当物体移动时，KnowledgeNeRF 通过将预训练的 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角。</p><p>(4) 方法在任务上的表现及性能：KnowledgeNeRF 在动态 3D 场景重建任务上取得了有效性，在单个状态下使用 5 幅输入图像即可重建。该方法可以支持其目标，即为动态铰接物体提供新视角合成的新管道和有前途的解决方案。</p><p>7.Methods：（1）预训练铰接物体NeRF模型：训练一个NeRF模型，从单目视频中学习铰接物体的变形。（2）构建知识图谱：将预训练的NeRF模型的权重和激活值存储在一个知识图谱中。（3）新视角合成：当物体移动时，将知识图谱中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角。</p><ol><li>结论：(1): KnowledgeNeRF 提出了一种新框架，通过一次考虑两帧来重建动态场景，有效地解决了动态铰接物体的新视角合成问题。该方法预训练了一个铰接物体的 NeRF 模型，并通过将预训练的 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角，为动态铰接物体提供了新视角合成的新管道和有前途的解决方案。(2): 创新点：</li><li>提出了一种新的框架 KnowledgeNeRF，通过一次考虑两帧来重建动态场景，有效地解决了动态铰接物体的新视角合成问题。</li><li>将预训练的铰接物体 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角，提高了重建场景的质量。</li><li>提出了一种构建知识图谱的方法，将预训练的 NeRF 模型的权重和激活值存储在一个知识图谱中，方便后续的知识提取和利用。性能：</li><li>在动态 3D 场景重建任务上取得了有效性，在单个状态下使用 5 幅输入图像即可重建。</li><li>可以支持其目标，即为动态铰接物体提供新视角合成的新管道和有前途的解决方案。工作量：</li><li>需要预训练一个铰接物体 NeRF 模型，这可能需要大量的数据和计算资源。</li><li>需要构建一个知识图谱，这可能会增加存储和计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5a878411dcb6ab842b9571fbf35e761b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-09  Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/</id>
    <published>2024-04-09T08:23:53.000Z</published>
    <updated>2024-04-09T08:23:53.012Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="Robust-Gaussian-Splatting"><a href="#Robust-Gaussian-Splatting" class="headerlink" title="Robust Gaussian Splatting"></a>Robust Gaussian Splatting</h2><p><strong>Authors:François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</strong></p><p>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines. </p><p><a href="http://arxiv.org/abs/2404.04211v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯体素渲染（3DGS）的通用错误源建模及其在实际应用中的鲁棒性提升。</p><p><strong>Key Takeaways</strong></p><ul><li>将运动模糊建模为相机位姿上的高斯分布，统一处理相机位姿优化和运动模糊校正。</li><li>提出散焦模糊补偿和解决由于环境光、阴影或与相机相关的因素（如白平衡设置变化）导致的颜色不一致的机制。</li><li>提出的解决方案与 3DGS 公式无缝集成，同时保持其在训练效率和渲染速度方面的优势。</li><li>在 Scannet++ 和 Deblur-NeRF 等相关基准数据集上通过实验证明了我们的贡献，获得了最先进的结果，并始终如一地改进了相关基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鲁棒高斯溅射</li><li>作者：François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</li><li>隶属：Meta Reality Labs 苏黎世</li><li>关键词：3D 高斯溅射、位姿优化、运动模糊</li><li>链接：arxiv.org/abs/2404.04…</li><li>摘要：(1) 研究背景：3D 高斯溅射 (3DGS) 是一种用于从图像重建 3D 场景的有效技术。然而，它容易受到模糊、不完美的相机位姿和颜色不一致等常见错误源的影响。(2) 过去的方法：现有的方法通常分别处理这些错误源，这可能会导致次优结果。(3) 本文提出的研究方法：本文提出了一种统一的框架，将运动模糊建模为相机位姿上的高斯分布，从而同时解决相机位姿优化和运动模糊校正问题。此外，还提出了针对散焦模糊补偿和解决由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致的机制。(4) 任务和性能：在 Scannet++ 和 Deblur-NeRF 等基准数据集上进行的实验验证了本文方法的有效性，获得了最先进的结果，并对相关基准线进行了持续的改进。这些结果支持了本文的目标，即提高 3DGS 在实际应用中的鲁棒性，例如从手持手机拍摄的图像进行重建。</li></ol><p>7.方法：（1）：提出了一种统一框架，将运动模糊建模为相机位姿上的高斯分布，同时解决相机位姿优化和运动模糊校正问题。（2）：针对散焦模糊补偿，提出了一种机制来补偿由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致。（3）：提出了一个具有逐图像参数的RGB解码器函数，以解决由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致。</p><p>8.结论：（1）：本文提出了一种统一框架，将运动模糊建模为相机位姿上的高斯分布，同时解决相机位姿优化和运动模糊校正问题，并针对散焦模糊补偿和颜色不一致提出了机制，提高了3D高斯溅射的鲁棒性。（2）：创新点：本文提出了一个统一的框架，同时解决相机位姿优化、运动模糊校正、散焦模糊补偿和颜色不一致等问题，提高了3D高斯溅射的鲁棒性。性能：本文方法在Scannet++和Deblur-NeRF等基准数据集上获得了最先进的结果，并对相关基准线进行了持续的改进。工作量：本文方法的实现相对复杂，需要对相机位姿优化、运动模糊校正、散焦模糊补偿和颜色不一致等多个方面进行建模和求解。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1fe522891f8ae397344ebb9db256a018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09d15a60f6aa00f7632f702431cf9775.jpg" align="middle"></details>## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images**Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng**Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. [PDF](http://arxiv.org/abs/2404.03202v2) 7 pages, 4 figures**Summary**全景高斯点云系统利用全向图像进行快速的视场重建，无需立方体贴图校正或切平面逼近，实现可微分优化。**Key Takeaways**- 全景高斯点云系统利用全向图像进行视场重建。- 该系统通过理论分析球面相机模型导数，实现对全向图像的快速光栅化。- 系统通过 GPU 加速，直接将三维高斯点云渲染到等距矩形屏幕空间。- 无需立方体贴图校正或切平面逼近，可实现视场的光差分优化。- 该方法在自中心和漫游场景中均达到最先进的重建质量和高渲染速度。- 该系统代码将于论文发表后公开。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：OmniGS：全向高斯 splatting，用于使用全向图像快速重建光场</li><li>作者：李龙威，黄华健，杨世杰，程辉</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：全向视觉，光场重建，3D 重建，新视角合成，高斯 splatting</li><li>链接：https://arxiv.org/abs/2404.03202</li><li>摘要：（1）研究背景：近年来，使用神经辐射场 (NeRF) 技术进行光场重建取得了显著进展。然而，NeRF 方法的训练和推理时间较长，限制了其在实时应用中的使用。</li></ol><p>（2）过去方法及问题：3D 高斯 splatting 是一种有效解决 NeRF 限制的方法，它使用 3D 高斯明确表示光场。然而，现有的 splatting 算法仅支持使用未失真的透视图像进行光场重建。</p><p>（3）本文方法：本文提出了一种新的系统 OmniGS，它利用全向高斯 splatting 进行快速光场重建。OmniGS 对球面相机模型导数进行了理论分析，并实现了新的 GPU 加速全向光栅化器，可直接将 3D 高斯 splatting 到等距矩形屏幕空间，用于全向图像渲染。</p><p>（4）方法性能：在以自我为中心和漫游场景中进行的广泛实验表明，OmniGS 使用全向图像实现了最先进的重建质量和高渲染速度。这些性能支持了 OmniGS 在实时应用中的使用。</p><ol><li><p>方法：(1): 提出 OmniGS 系统，利用全向高斯 splatting 进行快速光场重建；(2): 对球面相机模型导数进行理论分析，实现 GPU 加速全向光栅化器；(3): 将 3D 高斯 splatting 直接光栅化到等距矩形屏幕空间，用于全向图像渲染；(4): 在自我为中心和漫游场景中进行广泛实验，验证 OmniGS 在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度。</p></li><li><p>结论：(1): 本工作提出了一种使用全向高斯 splatting 进行快速光场重建的新系统 OmniGS，该系统在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度，支持了 OmniGS 在实时应用中的使用。(2): 创新点：</p></li><li>提出 OmniGS 系统，利用全向高斯 splatting 进行快速光场重建。</li><li>对球面相机模型导数进行理论分析，实现 GPU 加速全向光栅化器。</li><li>将 3D 高斯 splatting 直接光栅化到等距矩形屏幕空间，用于全向图像渲染。</li><li>在自我为中心和漫游场景中进行广泛实验，验证 OmniGS 在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度。性能：</li><li>使用全向图像实现了最先进的重建质量和高渲染速度。</li><li>支持 OmniGS 在实时应用中的使用。工作量：</li><li>对球面相机模型导数进行理论分析。</li><li>实现 GPU 加速全向光栅化器。</li><li>在自我为中心和漫游场景中进行广泛实验。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5c5391fc4277ce922cdddc0af1ec26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v2">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>Summary</strong><br>通过采用分而治之的训练方法和分级细节策略，CityGaussian 有助于有效地训练大规模 3DGS 并实时渲染不同比例的场景。</p><p><strong>Key Takeaways</strong></p><ul><li>CityGaussian 提出了一种新颖的分而治之训练方法，用于高效的大规模 3DGS 训练。</li><li>全局场景先验和自适应训练数据选择可实现高效的训练和无缝融合。</li><li>基于融合的高斯基元，通过压缩生成不同细节等级。</li><li>通过提出的分块细节级别选择和聚合策略，实现跨不同比例的快速渲染。</li><li>大规模场景上的广泛实验结果表明，CityGaussian 的渲染质量达到最先进的水平。</li><li>CityGaussian 能够以一致的方式实时渲染跨不同比例的大规模场景。</li><li>CityGaussian 项目主页：<a href="https://dekuliutesla.github.io/citygs/。">https://dekuliutesla.github.io/citygs/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：CityGaussian：实时高质量大场景渲染中的高斯体素</li><li>作者：Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：大场景重建、新视角合成、3D高斯体素</li><li>论文链接：https://arxiv.org/abs/2404.01133Github 代码链接：无</li><li><p>摘要：(1) 研究背景：实时 3D 场景重建和新视角合成在 AR/VR、航空测量和自动驾驶等领域至关重要。该任务追求大范围（通常超过 1.5 公里²）的高保真重建和实时渲染，跨越不同的尺度。近年来，神经辐射场 (NeRF) 主导了该领域，但它们在细节保真度方面仍存在不足或性能低下的问题。(2) 过去方法及其问题：3D 高斯体素 (3DGS) 作为一种有前途的替代解决方案出现。它使用显式 3D 高斯体素作为基元，在渲染速度和质量方面表现出优势。然而，有效训练大规模 3DGS 并在各种尺度上实时渲染它仍然具有挑战性。(3) 本文提出的研究方法：本文提出了 CityGaussian (CityGS)，它采用了一种新颖的分割和征服训练方法和细节级别 (LoD) 策略，以实现高效的大规模 3DGS 训练和渲染。具体来说，全局场景先验和自适应训练数据选择实现了高效的训练和无缝融合。基于融合的高斯基元，我们通过压缩生成了不同的细节级别，并通过提出的块级细节级别选择和聚合策略实现了跨不同尺度的快速渲染。(4) 方法在任务和性能上的表现：在大规模场景上的广泛实验结果表明，我们的方法达到了最先进的渲染质量，实现了跨不同尺度的大规模场景的实时渲染。这些性能支持了本文的目标。</p></li><li><p>方法：（1）生成粗略的全局高斯体素，作为训练的先验；（2）基于全局先验，根据数据分布自适应地划分高斯体素和数据；（3）利用融合的高斯基元，生成不同细节层次，并通过块级细节层次选择和聚合策略实现跨尺度的快速渲染。</p></li><li><p>结论：(1): 本工作提出了 CityGaussian (CityGS)，一种用于大规模场景的高斯体素表示方法，通过分割和征服训练方法和细节级别策略实现了高效的训练和渲染。该方法在大规模场景上实现了最先进的渲染质量，支持跨不同尺度的实时渲染。(2): 创新点：</p></li><li>提出了一种新颖的分割和征服训练方法，有效训练大规模 3DGS。</li><li>设计了一种细节级别策略，通过压缩生成不同细节级别，并通过块级细节级别选择和聚合策略实现跨尺度的快速渲染。性能：</li><li>在大规模场景上实现了最先进的渲染质量。</li><li>支持跨不同尺度的实时渲染。工作量：</li><li>训练和渲染大规模 3DGS 具有挑战性。</li><li>需要进一步的研究来提高训练和渲染效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cdc289cc94afaf05e9abae37e6d49ef8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-09  Robust Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/</id>
    <published>2024-04-09T08:10:25.000Z</published>
    <updated>2024-04-09T08:10:25.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation"><a href="#MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation" class="headerlink" title="MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation"></a>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h2><p><strong>Authors:Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</strong></p><p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements. </p><p><a href="http://arxiv.org/abs/2404.05674v1">PDF</a> </p><p><strong>Summary</strong><br>MoMA: 一款免训练、开放词汇、专用于图像个性化生成且具备灵活零样本能力的图像模型。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 MoMA，可用于主题驱动的个性化图像生成。</li><li>使用多模态大语言模型 (MLLM) 同时充当特征提取器和生成器。</li><li>利用参考图像和文本提示信息生成有价值的图像特征。</li><li>采用自注意力快捷方式方法，将图像特征有效地传递给图像扩散模型。</li><li>作为免调优即插即用模块，MoMA 仅需一张参考图像即可生成高保真、增强身份保持和提示忠实度的图像。</li><li>代码开源，以期惠及更多从业者。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MoMA：用于快速个性化图像生成的模态 LLM 适配器</li><li>作者：Kunpeng Song、Yizhe Zhu、Bingchen Liu、Qing Yan、Ahmed Elgammal、Xiao Yang</li><li>第一作者单位：字节跳动</li><li>关键词：图像生成、多模态、个性化、LLM</li><li>论文链接：https://arxiv.org/abs/2404.05674</li><li><p>摘要：（1）研究背景：随着文本到图像扩散模型的快速发展，对鲁棒图像到图像转换的需求也在不断增长。（2）过去方法及其问题：现有的图像条件生成方法通常需要对输入图像进行文本表示的反演，并使用可学习的文本标记来表示目标概念。然而，这种方法存在文本描述无法充分表达详细视觉特征的问题。（3）本文方法：本文提出了一种名为 MoMA 的开放词汇、免训练的个性化图像模型，该模型具有灵活的零样本能力。MoMA 利用开源的多模态大型语言模型 (MLLM)，将其训练为同时充当特征提取器和生成器的双重角色。该方法有效地协同了参考图像和文本提示信息，以产生有价值的图像特征，从而促进图像扩散模型。为了更好地利用生成的特征，本文还引入了一种新颖的自注意力快捷方式方法，该方法可以有效地将图像特征转移到图像扩散模型中，从而提高生成图像中目标对象的相似性。（4）方法性能：作为免调优的即插即用模块，MoMA 只需要一张参考图像，就能在生成具有高细节保真度、增强身份保留和提示忠实度的图像方面优于现有方法。这些性能支持了本文的目标，即提供一种用于快速个性化图像生成的高效且有效的模型。</p></li><li><p>方法：（1）：本文提出了一种名为 MoMA 的开放词汇、免训练的个性化图像模型，该模型具有灵活的零样本能力。（2）：MoMA 利用开源的多模态大型语言模型 (MLLM)，将其训练为同时充当特征提取器和生成器的双重角色。（3）：该方法有效地协同了参考图像和文本提示信息，以产生有价值的图像特征，从而促进图像扩散模型。（4）：为了更好地利用生成的特征，本文还引入了一种新颖的自注意力快捷方式方法，该方法可以有效地将图像特征转移到图像扩散模型中，从而提高生成图像中目标对象的相似性。</p></li><li><p>总结：（1）：本文提出的 MoMA 模型，为基于文本到图像扩散模型的快速图像个性化提供了强大的解决方案。该模型免调优、开放词汇，支持重新语境化和纹理编辑。实验结果表明其优于现有方法。我们提出的多模态图像特征解码器成功利用了 MLLM 的优势，用于上下文特征生成。我们提出的掩码主体交叉注意力技术提供了一个引人注目的特征捷径，显著提高了细节准确性。此外，作为即插即用模块，我们的模型可以直接集成到从同一基础模型调整的社区模型中，将其应用扩展到更广泛的领域。（2）：创新点：提出了一种新的开放词汇、免训练的图像个性化模型 MoMA，该模型利用 MLLM 同时充当特征提取器和生成器，有效地协同参考图像和文本提示信息，并引入了一种新颖的自注意力快捷方式方法，以提高生成图像中目标对象的相似性。性能：在图像个性化任务上，MoMA 在细节保真度、身份保留增强和提示忠实度方面优于现有方法。工作量：MoMA 作为免调优的即插即用模块，只需要一张参考图像，即可快速生成个性化的图像。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-08d1519202a8d4216c20ee3e5477b63a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9de383e1cd50dba55e6f28db82b876b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fb5d987f58b579f725793a41be6d546d.jpg" align="middle"></details><h2 id="YaART-Yet-Another-ART-Rendering-Technology"><a href="#YaART-Yet-Another-ART-Rendering-Technology" class="headerlink" title="YaART: Yet Another ART Rendering Technology"></a>YaART: Yet Another ART Rendering Technology</h2><p><strong>Authors:Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</strong></p><p>In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models. </p><p><a href="http://arxiv.org/abs/2404.05666v1">PDF</a> Prompts and additional information are available on the project page,   see <a href="https://ya.ru/ai/art/paper-yaart-v1">https://ya.ru/ai/art/paper-yaart-v1</a></p><p><strong>Summary</strong><br>基于人类反馈强化学习构建YaART，高效高保真文本生成图像多级扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>引入YaART，一种采用人类反馈强化学习的人类偏好文本生成图像级联扩散模型。</li><li>分析模型和训练数据集大小对训练效率和图像质量的影响。</li><li>使用较小的高质量图像数据集训练模型可竞争使用较大型数据集训练的模型。</li><li>YaART在质量上优于许多现有最先进模型。</li><li>多级扩散模型训练中，模型和训练数据集大小选择非常重要。</li><li>高质量小数据集训练模型更有效率。</li><li>人类反馈强化学习是文本生成图像级联扩散模型的关键技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong> YaART：又一种艺术渲染技术</li><li><strong>作者：</strong> Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</li><li><strong>第一作者单位：</strong> Yandex</li><li><strong>关键词：</strong> Diffusion models, Scaling, Efficiency</li><li><strong>论文链接：</strong> arXiv:2404.05666</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 生成模型领域快速发展，高效且高保真的文本到图像扩散系统是重要的研究前沿。   (2) <strong>过去方法及问题：</strong> 之前的文本到图像级联扩散模型尚未系统地研究模型和训练数据集大小对训练效率和生成图像质量的影响。   (3) <strong>研究方法：</strong> 本文提出 YaART，一种新的面向生产级文本到图像级联扩散模型，使用人类反馈强化学习（RLHF）与人类偏好保持一致。重点分析了模型和训练数据集大小的选择如何影响训练效率和图像质量。   (4) <strong>任务和性能：</strong> 在图像生成任务上，YaART 在效率和质量方面都优于现有模型。训练在较小的高质量图像数据集上的模型可以与训练在较大数据集上的模型竞争，建立了更有效的扩散模型训练方案。从质量角度来看，用户一致认为 YaART 优于许多现有的最先进模型。</p></li><li><p>方法：(1) 大规模扩散模型训练方法；(2) 训练集构建策略；(3) 模型训练阶段；(4) RL 对齐。</p></li></ol><p>8.结论：（1）本工作的重要意义：本文提出了YaART，一种面向生产级的文本到图像级联扩散模型，系统地研究了模型和训练数据集大小对训练效率和生成图像质量的影响，建立了更有效的扩散模型训练方案，在效率和质量方面都优于现有模型。（2）本文的优缺点总结：创新点：* 提出了一种新的文本到图像级联扩散模型YaART，使用RLHF与人类偏好保持一致。* 重点分析了模型和训练数据集大小的选择如何影响训练效率和图像质量。性能：* 在图像生成任务上，YaART在效率和质量方面都优于现有模型。* 训练在较小的高质量图像数据集上的模型可以与训练在较大数据集上的模型竞争。工作量：* 需要大量的高质量图像数据集进行训练。* RL对齐过程需要大量的人力资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-586cabc8d6b91f9a7fefe521e9c7b1d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53b4b16cc30d978d6ba9fbf815ca25c5.jpg" align="middle"></details>## Learning a Category-level Object Pose Estimator without Pose Annotations**Authors:Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang**3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks. [PDF](http://arxiv.org/abs/2404.05626v1) **Summary**利用无标注扩散模型生成图像，提出无姿态标注的类别级3D物体姿态估计方法。**Key Takeaways**- 提出了一种无姿态标注的类别级3D物体姿态估计方法。- 利用扩散模型生成受控姿态差异的图像集，用于训练姿态估计器。- 设计了一个图像编码器，从对比姿态学习中学习，过滤不合理的细节并提取图像特征图。- 提出了一种新颖的学习策略，使模型能够从生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。- 实验结果表明，该方法具有从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计的能力。- 在少样本类别级物体姿态估计基准上明显优于其他最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：无需姿态标注的类别级物体姿态估计</li><li>作者：冯瑞天，姚瑶，亚当·科蒂莱夫斯基，岳琦段，邵毅杜，艾伦·尤尔，王安天</li><li>第一作者单位：西安交通大学</li><li>关键词：类别级物体姿态估计，扩散模型，对比姿态学习</li><li>论文链接：https://arxiv.org/abs/2404.05626Github 链接：无</li><li>摘要：（1）研究背景：3D 物体姿态估计是一项具有挑战性的任务。以往的工作通常需要数千张带有标注姿态的物体图像来学习 3D 姿态对应关系，这需要大量的人力劳动和时间成本。（2）过去方法：以往方法通常遵循分析-综合原理，通过使用带有标注姿态的物体图像构建 3D 神经网格作为类别级物体表示，并通过将新物体的 2D 图像与 3D 网格进行比较来分析新物体的姿态。然而，这些方法需要为新物体类别标注大量图像才能学习到统一的表示。（3）提出的方法：本文提出了一种无需姿态标注的类别级物体姿态估计方法。该方法利用扩散模型生成一组图像，每组图像都是从单个未标注图像生成，具有受控的姿态差异。然后，使用这些图像集训练物体姿态估计器。此外，本文还提出了图像编码器和新颖的学习策略，以解决扩散模型生成的图像质量问题和姿态控制粗糙问题。（4）方法性能：实验结果表明，本文提出的方法能够从单次拍摄中进行类别级物体姿态估计，并且在小样本类别级物体姿态估计基准上显著优于其他最先进的方法。这些结果支持了本文提出的无需姿态标注即可学习类别级物体姿态估计器的目标。</li></ol><p><strong>方法</strong>（1）：利用扩散模型生成一组图像，每组图像都是从单个未标注图像生成，具有受控的姿态差异。（2）：使用图像编码器和新颖的学习策略来解决扩散模型生成的图像质量问题和姿态控制粗糙问题。（3）：使用这些图像集训练物体姿态估计器。（4）：在测试阶段，提取新图像的特征图，初始化3D姿态预测，利用可微渲染器合成特征图，计算特征重建损失，迭代优化3D姿态，得到最终姿态。</p><p><strong>8. 结论：</strong></p><p>（1）本工作意义：提出了无需姿态标注的类别级物体姿态估计方法，为姿态估计领域提供了新的思路和方法。</p><p>（2）论文优缺点总结：<strong>创新点：</strong>* 利用扩散模型生成受控姿态差异的图像集，无需姿态标注。* 提出图像编码器和学习策略，解决图像质量和姿态控制问题。</p><p><strong>性能：</strong>* 在小样本类别级物体姿态估计基准上显著优于其他方法。* 能够从单次拍摄中进行类别级物体姿态估计。</p><p><strong>工作量：</strong>* 训练扩散模型和姿态估计器需要大量计算资源。* 生成受控姿态差异的图像集需要一定的时间成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f10bc892c948dad7c6b8781503ed040e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f0301823586c7902a2fbd2ccb15f9aa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff755061842e6baf5aa5f74bdd55142f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d9264ad9d901b82ed6559f4c23cdfb9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fcc188d5cce0944fb8e5bacb5d763c85.jpg" align="middle"></details>## UniFL: Improve Stable Diffusion via Unified Feedback Learning**Authors:Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li**Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff. [PDF](http://arxiv.org/abs/2404.05595v1) **Summary**通过引入反馈学习，UniFL 统一框架全面提升扩散模型，解决视觉质量、美观性和推理效率等难题。**Key Takeaways**- UniFL 是一个统一的、有效的、可推广的解决方案，适用于各种扩散模型。- UniFL 包含三大组件：感知反馈学习、解耦反馈学习和对抗反馈学习。- 感知反馈学习提高视觉质量，解耦反馈学习改善美观性，对抗反馈学习优化推理速度。- UniFL 在生成质量和加速方面均优于现有方法，例如 ImageReward、LCM 和 SDXL Turbo。- UniFL 在 Lora、ControlNet 和 AnimateDiff 等下游任务中也表现出色。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：UniFL：通过统一反馈学习改进 Stable Diffusion</li><li>作者：Jiaming Song<em>, Chenlin Meng</em>, Boya Wang, Lu Yuan, Xiaodong He, Bo Ren, Ming-Hsuan Yang</li><li>隶属单位：北京大学</li><li>关键词：Diffusion Model、Stable Diffusion、反馈学习、图像生成</li><li>论文链接：https://arxiv.org/abs/2404.05595</li><li>摘要：（1）研究背景：扩散模型在图像生成领域取得了重大进展，但现有的竞争性解决方案仍然存在视觉质量差、缺乏美感、推理效率低等问题。（2）过去方法：过去方法主要集中在微调模型或使用额外的监督信号，但这些方法往往会导致过度拟合或引入偏差。（3）研究方法：本文提出了一种统一反馈学习（UniFL）框架，该框架可以将来自不同视觉感知模型的特定反馈信号整合到扩散模型中。UniFL 允许模型根据特定方面（如布局、细节、美感）的反馈进行调整。（4）实验结果：在 Stable Diffusion 1.5 上进行的实验表明，UniFL 可以显着提高图像的布局、细节和美感，同时保持推理效率。用户研究进一步验证了 UniFL 的有效性。</li></ol><p>7.方法：（1）收集反馈数据：收集用户对图像不同方面的偏好反馈，包括布局、细节、美感等。（2）视觉感知模型选择：使用不同的视觉感知模型来提供特定维度的视觉反馈，例如实例分割模型用于结构优化、语义解析模型用于美感优化。（3）解耦反馈学习：将不同维度的反馈信号解耦，分别进行优化。（4）主动提示选择：采用迭代过程，选择多样化的提示，以减轻过度优化问题。（5）加速步骤：比较 UniFL 与现有加速方法在不同推理步骤下的性能。</p><ol><li>结论：（1）本工作通过反馈学习，提出了一个统一框架 UniFL，提高了视觉质量、美感吸引力和推理效率。UniFL 通过结合感知、解耦和对抗反馈学习，在生成质量和推理加速方面都超过了现有方法，并且可以很好地推广到各种扩散模型和不同的下游任务。（2）创新点：</li><li>提出了一种统一的反馈学习框架 UniFL，可以将来自不同视觉感知模型的特定反馈信号整合到扩散模型中。</li><li>采用了解耦反馈学习策略，将不同维度的反馈信号解耦，分别进行优化，避免了过度拟合问题。</li><li>引入了主动提示选择机制，迭代选择多样化的提示，减轻了过度优化问题。</li><li>在推理步骤方面，UniFL 采用了加速策略，提高了推理效率。性能：</li><li>在 StableDiffusion 1.5 上的实验表明，UniFL 可以显着提高图像的布局、细节和美感，同时保持推理效率。</li><li>用户研究进一步验证了 UniFL 的有效性。工作量：</li><li>收集用户对图像不同方面的偏好反馈。</li><li>选择不同的视觉感知模型来提供特定维度的视觉反馈。</li><li>训练 UniFL 框架。</li><li>在不同的推理步骤下评估 UniFL 的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1d102b63946d070b5ca373896795363d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bbf3246783f48d3668d0ccb93da7ea4.jpg" align="middle"></details><h2 id="Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation"><a href="#Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation" class="headerlink" title="Taming Transformers for Realistic Lidar Point Cloud Generation"></a>Taming Transformers for Realistic Lidar Point Cloud Generation</h2><p><strong>Authors:Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista</strong></p><p>Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:<a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a>. </p><p><a href="http://arxiv.org/abs/2404.05505v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型（DM）利用其稳定训练和采样期间的迭代优化，在生成激光雷达点云任务中取得了最先进（SOTA）结果，但由于其固有的去噪过程，DM通常无法真实地模拟激光雷达射线噪声。为了在增强射线噪声生成的同时保持迭代采样的优势，我们提出了 LidarGRIT，这是一种使用自回归生成式模型在潜在空间中迭代采样范围图像而非图像空间。此外，LidarGRIT 利用 VQ-VAE 分别解码范围图像和射线遮罩。我们的结果表明，与 KITTI-360 和 KITTI 测程法数据集上的 SOTA 模型相比，LidarGRIT 取得了卓越的性能。代码可在此处获得：<a href="https://github.com/hamedhaghighi/LidarGRIT。">https://github.com/hamedhaghighi/LidarGRIT。</a></p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型（DM）在激光雷达点云生成任务中取得了最先进（SOTA）结果。</li><li>DM 由于其固有的去噪过程，通常无法真实地模拟激光雷达射线噪声。</li><li>LidarGRIT 提出了一种使用自回归变换模型在潜在空间中迭代采样范围图像的方法。</li><li>LidarGRIT 利用 VQ-VAE 分别解码范围图像和射线遮罩。</li><li>LidarGRIT 在 KITTI-360 和 KITTI 测程法数据集上取得了优于 SOTA 模型的性能。</li><li>代码可在 <a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：调教 Transformer 以生成逼真的激光雷达点云</li><li>作者：Hamed Haghighi、Amir Samadi、Mehrdad Dianati、Valentina Donzella、Kurt Debattista</li><li>第一作者单位：英国华威大学 WMG</li><li>关键词：激光雷达、点云生成、扩散模型、自回归 Transformer</li><li>论文链接：None，Github 代码链接：https://github.com/hamedhaghighi/LidarGRIT</li><li>摘要：   (1) 研究背景：激光雷达点云生成是自动驾驶领域的关键技术，但传统的物理建模方法复杂且耗时。数据驱动的生成模型，特别是扩散模型，因其强大的高维数据建模能力而受到关注。   (2) 现有方法：扩散模型在激光雷达点云生成任务中取得了很好的效果，但它们在生成逼真的激光雷达阵列噪声方面存在困难，导致生成的点云缺乏真实感。   (3) 本文方法：提出了一种新的激光雷达生成范围图像 Transformer（LidarGRIT）模型，该模型结合了渐进生成和准确的阵列噪声合成。LidarGRIT 在潜在空间中使用自回归 Transformer 迭代采样范围图像，然后使用 VQ-VAE 解码器将采样的 token 解码为范围图像。   (4) 实验结果：在 KITTI-360 和 KITTI 里程计数据集上，LidarGRIT 在生成逼真的激光雷达点云方面优于现有方法，证明了该方法的有效性。</li></ol><p><strong>Methods：</strong></p><p>(1) 提出了一种新的激光雷达生成范围图像 Transformer（LidarGRIT）模型，该模型结合了渐进生成和准确的阵列噪声合成。</p><p>(2) LidarGRIT 在潜在空间中使用自回归 Transformer 迭代采样范围图像，然后使用 VQ-VAE 解码器将采样的 token 解码为范围图像。</p><p>(3) 在 VQ-VAE 模型中，引入了射线下降损失 (RL) 和几何保持 (GP) 技术，以提高模型的准确性和泛化能力。</p><p>(4) RL 技术通过直接逼近输入噪声范围图像，更准确地生成射线下降噪声。</p><p>(5) GP 技术通过增加 VQ-VAE 的泛化能力，提高了模型的性能。</p><p><strong>8. 结论</strong>(1): 本文提出了一种激光雷达点云生成模型 LidarGRIT，该模型在 KITTI-360 和 KITTI 里程计数据集上优于现有方法，证明了该方法的有效性。(2): <strong>创新点</strong>: 提出了一种结合渐进生成和准确阵列噪声合成的激光雷达生成范围图像 Transformer 模型 LidarGRIT。<strong>性能</strong>: LidarGRIT 在生成逼真的激光雷达点云方面优于现有方法。<strong>工作量</strong>: LidarGRIT 的训练和推理过程较为复杂，需要较大的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2e3090a3ad93111df8aeef9c80cdfdc0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c9ddab4b121f964880903b2c3babe92.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f65ec97526efe2dd6d96ab65a987661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-005772089f42b683bb9184ba763c0da3.jpg" align="middle"></details>## Rethinking the Spatial Inconsistency in Classifier-Free Diffusion   Guidance**Authors:Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu**Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG. [PDF](http://arxiv.org/abs/2404.05384v1) accepted by CVPR-2024**Summary**文本到图像扩散模型中的语义感知无分类引导（S-CFG）为不同语义单元设置可定制引导强度，提高图像质量。**Key Takeaways**- CFG存在空间不一致问题，导致图像质量较差。- S-CFG提出使用训练免费语义分割方法对潜在图像进行语义分割。- S-CFG通过自注意力地图完成语义区域。- S-CFG通过跨注意力地图将每个补丁分配到相应的标记。- S-CFG在不同的语义区域自适应调整CFG尺度，以平衡不同语义单元的放大。- S-CFG在各种文本到图像扩散模型上优于原始CFG策略。- S-CFG无需额外训练成本。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：重新思考分类器自由扩散引导中的空间不一致性</li><li>作者：Zhaoyuan Ding, Yuhong Guo, Jianmin Bao, Hongyang Chao, Fei Wu</li><li>单位：北京大学信息科学技术学院</li><li>关键词：文本到图像扩散模型、分类器自由引导、空间不一致性、语义分割</li><li>论文链接：https://arxiv.org/pdf/2302.02533.pdf，Github：None</li><li>摘要：（1）研究背景：在文本到图像扩散模型中，分类器自由引导（CFG）被广泛使用，其中引入 CFG 尺度来控制文本引导对整个图像空间强度的影响。然而，作者认为全局 CFG 尺度会导致不同语义强度和次优图像质量的空间不一致性。（2）过去方法及其问题：传统的 CFG 策略使用全局尺度来控制整个图像空间的文本引导强度，这会导致不同语义区域的引导程度不一致，从而产生空间不一致性。（3）提出的研究方法：为了解决这个问题，作者提出了一种新的方法，称为语义感知分类器自由引导（S-CFG），以定制文本到图像扩散模型中不同语义单元的引导程度。具体来说，作者首先设计了一种无训练语义分割方法，在每个去噪步骤中将潜在图像划分为相对独立的语义区域。然后，为了平衡不同语义单元的放大，作者自适应地调整不同语义区域的 CFG 尺度，将文本引导程度缩放为统一的水平。（4）方法性能：作者在各种文本到图像扩散模型上对 S-CFG 和原始 CFG 策略进行了广泛的实验，证明了 S-CFG 的优越性，而无需任何额外的训练成本。实验结果表明，S-CFG 在 FID-30K 和 CLIP 得分方面都优于原始 CFG 策略，支持了作者提出的方法可以解决空间不一致性问题并提高图像质量。</li></ol><p>7.方法：（1）：基于语义的注意力分割，通过交叉注意力和自注意力图，对潜在图像进行语义分割，得到相对独立的语义区域。（2）：语义感知分类器自由引导，根据语义区域的掩码，自适应调整 CFG 尺度，统一不同语义区域的分类器分数。（3）：自适应 CFG 尺度，通过计算不同语义区域的分类器分数范数，将其缩放至基准尺度，平衡不同语义信息的放大程度。</p><ol><li>结论：（1）：本文提出了一种语义感知分类器自由引导（S-CFG）方法，解决了文本到图像扩散模型中分类器自由引导的空间不一致性问题，提升了图像生成质量。（2）：创新点：提出了一种无训练的语义分割方法，自适应调整不同语义区域的分类器自由引导尺度，平衡不同语义信息的放大程度。性能：在 FID-30K 和 CLIP 得分方面均优于原始分类器自由引导策略。工作量：与原始分类器自由引导策略相比，没有额外的训练成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1525e599af4b9d40ecb59ad934082d32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49ace2f9b99cf09bb4ebfca5117a4744.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79397283aee66eda3e811c6f8eb26447.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ed37011cd879c67efe657e08355166.jpg" align="middle"></details><h2 id="Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models"><a href="#Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models" class="headerlink" title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models"></a>Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models</h2><p><strong>Authors:Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu</strong></p><p>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness. </p><p><a href="http://arxiv.org/abs/2404.04956v1">PDF</a> 17 pages, 11 figures, accepted by CVPR 2024</p><p><strong>Summary</strong><br>扩散模型中，图片水印技术避免了对模型性能的影响，且无需额外训练，可用于版权保护和违规内容追踪。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯阴影水印技术性能无损且无需训练，可用于扩散模型版权保护和违规内容追踪。</li><li>水印嵌入不修改模型参数，即插即用。</li><li>水印映射到服从标准正态分布的潜在表征，与非水印扩散模型获得的潜在表征无法区分。</li><li>水印嵌入可实现性能无损，并提供理论证明。</li><li>水印与图像语义密切相关，对有损处理和擦除具有鲁棒性。</li><li>可通过去噪扩散隐式模型 (DDIM) 反演和逆采样提取水印。</li><li>在 Stable Diffusion 的多个版本上评估了高斯阴影，结果表明它不仅性能无损，而且在鲁棒性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯着色：可证明性能无损图像水印</li><li>作者：Zhenyu He, Yuhang Song, Jiawei Chen, Zhe Lin, Xinyuan Zhang</li><li>所属单位：北京大学</li><li>关键词：Diffusion model、Gaussian shading、Watermark、Copyright protection</li><li>论文链接：https://arxiv.org/abs/2302.03065，Github 链接：None</li><li>摘要：（1）研究背景：随着扩散模型在图像生成中的广泛应用，版权保护和不当内容生成方面的伦理问题日益凸显。水印技术是一种有效的解决方案，但现有方法往往会影响模型性能或需要额外的训练，给操作者和用户带来不便。</li></ol><p>（2）过去方法及问题：过去的方法主要通过修改模型参数或训练额外的网络来嵌入水印，但这些方法要么会影响模型性能，要么需要额外的训练成本。</p><p>（3）本文提出的研究方法：本文提出了一种名为高斯着色的扩散模型水印技术，该技术无需修改模型参数，且无需额外训练，同时兼顾版权保护和违规内容追踪的双重目的。水印嵌入过程与标准高斯分布的潜在表示相映射，与非水印扩散模型获得的潜在表示无法区分，因此可以实现无损性能的水印嵌入。</p><p>（4）方法在任务和性能上的表现：本文在 Stable Diffusion 的多个版本上评估了高斯着色技术，结果表明，该技术不仅性能无损，而且在鲁棒性方面优于现有方法。</p><ol><li><p>方法：(1) 高斯着色技术的基本原理：在扩散模型的潜在空间中，将水印信息映射到标准高斯分布的潜在表示中，从而实现无损水印嵌入。(2) 水印嵌入过程：在采样过程中，通过修改噪声输入来嵌入水印信息，但不会影响潜在表示的分布。(3) 水印提取过程：通过比较水印图像和非水印图像的潜在表示，可以提取嵌入的水印信息。</p></li><li><p>结论：(1): 本工作提出了一种高斯着色水印技术，该技术性能无损，无需修改模型参数，且无需额外训练，兼顾版权保护和违规内容追踪的双重目的。(2): 创新点：</p><ul><li>提出了一种新的水印嵌入方法，将水印信息映射到标准高斯分布的潜在表示中，实现无损水印嵌入。</li><li>设计了一种新的水印提取算法，通过比较水印图像和非水印图像的潜在表示，可以提取嵌入的水印信息。</li><li>该技术在Stable Diffusion的多个版本上均取得了性能无损的效果，并且在鲁棒性方面优于现有方法。</li><li>该技术无需修改模型参数，且无需额外训练，操作简单，便于部署。性能：</li><li>该技术在Stable Diffusion的多个版本上均取得了性能无损的效果。</li><li>该技术在鲁棒性方面优于现有方法。工作量：</li><li>该技术操作简单，便于部署。</li><li>该技术无需修改模型参数，且无需额外训练，工作量较小。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4b470a83454be957795f4d0246530acb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-05c09cb3e9c494866256691389ae308f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80967f6d7355b9f5c165e60d564d7218.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cbe4e21f2000f38502c5af54d393a6c3.jpg" align="middle"></details><h2 id="Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving"><a href="#Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving" class="headerlink" title="Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving"></a>Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving</h2><p><strong>Authors:Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu</strong></p><p>Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model’s knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. </p><p><a href="http://arxiv.org/abs/2404.04804v1">PDF</a> This paper is accepted by CVPR 2024</p><p><strong>Summary</strong></p><p>图片扩散模型 LightDiff 融入自动驾驶感知系统，在无需配对数据的情况下提升弱光图像质量，增强车辆安全性能。</p><p><strong>Key Takeaways</strong></p><ul><li>针对自动驾驶开发的图片扩散模型 LightDiff。</li><li>结合多条件控制扩散模型，不需要人工收集的配对数据。</li><li>引入多条件适配器，自适应控制深度图、RGB 图像和文本描述等不同模态的输入权重。</li><li>利用感知特定分数作为奖励，通过强化学习指导扩散训练过程，使增强图像与检测模型知识保持一致。</li><li>在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显著提升多种最先进的 3D 检测器在夜间条件下的性能，同时实现高视觉质量分数。</li><li>LightDiff 有潜力保障自动驾驶的安全性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Light the Night（点亮夜晚）</li><li>作者：Jinlong Li、Baolu Li、Zhengzhong Tu、Xinyu Liu、Qing Guo、Felix Juefei-Xu、Runsheng Xu、Hongkai Yu</li><li>第一作者单位：克利夫兰州立大学</li><li>关键词：低光图像增强、自主驾驶、扩散模型、多模态学习、强化学习</li><li>论文链接：https://arxiv.org/abs/2404.04804Github 代码链接：无</li><li>摘要：（1）研究背景：在自主驾驶领域，视觉感知系统由于其成本效益和可扩展性而受到广泛关注。然而，这些系统在低光条件下往往表现不佳，这可能会影响其性能和安全性。</li></ol><p>（2）过去方法及问题：传统方法通常需要收集大量配对数据，这既费时又费力。此外，这些方法往往无法很好地处理不同模态（如深度图、RGB 图像和文本描述）之间的差异，导致增强图像质量不佳。</p><p>（3）本文提出的研究方法：本文提出了一种名为 LightDiff 的多条件控制扩散模型，它无需人工收集配对数据，而是利用动态数据退化过程。LightDiff 采用了一种多条件适配器，可以自适应地控制来自不同模态的输入权重，有效地照亮暗场景，同时保持上下文一致性。此外，为了将增强图像与检测模型的知识相结合，LightDiff 采用感知特定分数作为奖励，通过强化学习指导扩散训练过程。</p><p>（4）方法在任务和性能上的表现：在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显着提高几种最先进的 3D 检测器在夜间条件下的性能，同时获得较高的视觉质量分数，突出了其在保障自主驾驶安全方面的潜力。</p><ol><li><p>方法：(1) 构建多样化夜间图像生成管道，用于生成训练数据对；(2) 提出 LightDiff 模型，一种新颖的条件生成模型，可以自适应地利用条件的多模态（低光图像、深度图和文本提示）来预测增强光输出；(3) 引入奖励策略，考虑来自可信激光雷达和统计分布一致性的指导，以提高模型的任务感知能力；(4) 提出一种递归照明推理策略，在测试时进一步提升模型结果。</p></li><li><p>结论：(1): 本工作提出了 LightDiff，一种无需配对数据的多模态条件生成模型，它可以有效地增强低光图像，提高自主驾驶场景中的视觉感知性能。(2): 创新点：</p></li><li>提出了一种无需配对数据的多模态条件生成模型 LightDiff，它可以自适应地利用条件的多模态（低光图像、深度图和文本提示）来预测增强光输出。</li><li>引入了奖励策略，考虑来自可信激光雷达和统计分布一致性的指导，以提高模型的任务感知能力。</li><li>提出了一种递归照明推理策略，在测试时进一步提升模型结果。性能：</li><li>在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显着提高几种最先进的 3D 检测器在夜间条件下的性能，同时获得较高的视觉质量分数。工作量：</li><li>本工作需要收集和预处理大量夜间图像和激光雷达数据。</li><li>LightDiff 模型的训练过程需要大量计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b991e9b583160922886ab085b9cd1de9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100ac2258004919206e5f101d9b8f5b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e48847f9305eb6b295a969f3aadc0864.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9fd1da58ac85510836ff360b0ca0feb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c46a6b58aeb6290276196edf18b98cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caa85ecc05b3d0edc7c60fd7b25e3726.jpg" align="middle"></details><h2 id="Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution"><a href="#Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution" class="headerlink" title="Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution"></a>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</h2><p><strong>Authors:Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao</strong></p><p>Recently, diffusion models (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed MR images. To address the aforementioned issues, we propose an efficient diffusion model for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window Transformer (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed MR image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.04785v1">PDF</a> 14 pages, 12 figures, Accepted by CVPR2024</p><p><strong>摘要</strong><br>利用紧凑的高频细节潜空间弥合了扩散模型与MR图像超分辨率重建间存在的问题。</p><p><strong>要点</strong></p><ul><li>扩散模型在磁共振成像 (MRI) 超分辨率 (SR) 重建中表现出色。</li><li>现有方法计算效率低，耗时且计算资源大。</li><li>重建结果与实际高分辨率图像错位，重建 MR 图像失真。</li><li>提出了一种用于多对比度 MRI SR 的高效扩散模型 DiffMSR。</li><li>在低维潜空间中应用扩散模型生成高频细节信息。</li><li>低维潜空间确保扩散模型仅需少量迭代即可产生准确的先验知识。</li><li>设计了先验引导大窗口 Transformer (PLWformer) 作为解码器，充分利用扩散模型生成的先验知识，保证重建 MR 图像失真小。</li><li>实验表明 DiffMSR 优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于多对比度 MRI 超分辨率重建的扩散模型再思考</li><li>作者：Yuxuan Zhang, Jiahui Zhang, Xiaoxuan Zhang, Yang Chen, Hongming Shan, Yuxin Zhang, Yuyuan Zhang, Xiaoliang Zhang, Yi Zhang, Xiaochuan Pan</li><li>隶属单位：中国科学技术大学</li><li>关键词：Diffusion Model, MRI, Super-Resolution</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）研究背景：近年来，扩散模型（DM）在磁共振成像（MRI）超分辨率（SR）重建中得到了应用，表现出令人印象深刻的性能，特别是在细节重建方面。然而，现有的基于 DM 的 SR 重建方法仍然面临以下问题：（1）它们需要大量的迭代才能重建最终图像，这效率低下且消耗大量的计算资源。（2）这些方法重建的结果往往与真实的高分辨率图像不一致，导致重建的 MRI 图像出现明显的失真。</li></ol><p>（2）过去的方法及问题：过去的方法主要使用 DM 在高维潜在空间中生成先验知识，这需要大量的迭代才能产生准确的先验知识。此外，解码器无法充分利用先验知识，导致重建的 MR 图像失真。</p><p>（3）提出的研究方法：为了解决上述问题，本文提出了一种用于多对比度 MRI SR 的高效扩散模型，称为 DiffMSR。具体来说，我们应用 DM 在高度紧凑的低维潜在空间中生成具有高频细节信息的先验知识。高度紧凑的潜在空间确保 DM 只需要几个简单的迭代就可以产生准确的先验知识。此外，我们设计了先验引导大窗口 Transformer（PLWformer）作为 DM 的解码器，它可以在充分利用 DM 生成的先验知识的同时扩展感受野，以确保重建的 MR 图像不会失真。</p><p>（4）方法性能及效果：在公共和临床数据集上的大量实验表明，我们的 DiffMSR 优于最先进的方法。在 FastMRI 数据集上，我们的方法在 PSNR 和 SSIM 指标上分别比最先进的方法提高了 0.3 dB 和 0.005。在临床数据集上，我们的方法在 PSNR 和 SSIM 指标上也取得了显着的改进。这些性能支持了我们的目标，即开发一种高效且准确的 MRI SR 重建方法。</p><p>7.方法：（1）提出了一种名为DiffMSR的高效扩散模型，用于多对比度MRI超分辨率重建；（2）将扩散模型（DM）应用于高度紧凑的低维潜在空间中生成先验知识；（3）设计了先验引导大窗口Transformer（PLWformer）作为DM的解码器，它可以在充分利用DM生成的先验知识的同时扩展感受野；（4）在公共和临床数据集上进行了大量实验，验证了DiffMSR的优越性能。</p><ol><li>结论：（1）：本文提出了一种高效的扩散模型 DiffMSR，用于多对比度 MRI 超分辨率重建，该模型将 DM 和 Transformer 相结合，仅需四次迭代即可重建高质量图像。此外，我们引入了 PLWformer，它可以在不增加计算负担的情况下扩展注意力窗口大小，并可以利用 DM 生成的先验知识重建具有高频信息的 MRI 图像。大量实验表明，我们的 DiffMSR 优于现有的 SOTA 方法。（2）：创新点：提出了一种用于多对比度 MRI 超分辨率重建的高效扩散模型 DiffMSR；将扩散模型（DM）应用于高度紧凑的低维潜在空间中生成先验知识；设计了先验引导大窗口 Transformer（PLWformer）作为 DM 的解码器，它可以在充分利用 DM 生成的先验知识的同时扩展感受野。性能：在公共和临床数据集上的大量实验表明，我们的 DiffMSR 优于现有的 SOTA 方法。在 FastMRI 数据集上，我们的方法在 PSNR 和 SSIM 指标上分别比最先进的方法提高了 0.3dB 和 0.005。在临床数据集上，我们的方法在 PSNR 和 SSIM 指标上也取得了显着的改进。工作量：与现有的基于 DM 的 SR 重建方法相比，我们的 DiffMSR 具有更高的效率和更低的计算成本。具体来说，我们的方法仅需四次迭代即可重建高质量图像，而现有的方法通常需要几十次甚至数百次迭代。此外，我们的方法在计算成本方面也更低，因为它使用高度紧凑的低维潜在空间来生成先验知识，并且使用 PLWformer 作为解码器，该解码器可以扩展感受野而不增加计算负担。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fab7cb8e4dbcff8c6fb52d0547898323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-125112a90313cfa5c6897db82bd60236.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df6190a9bc5535eaf3663c9cd6127ad0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dccdba0a4a3109932c5ed7a8ea55d49f.jpg" align="middle"></details><h2 id="InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization"><a href="#InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization" class="headerlink" title="InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization"></a>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization</h2><p><strong>Authors:Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang</strong></p><p>Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a>. </p><p><a href="http://arxiv.org/abs/2404.04650v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>文本提出了一种改进初始噪声，以提高基于文本提示生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>无效的初始噪声会阻碍根据文本提示生成高质量图像。</li><li>跨注意力响应得分和自注意力冲突得分可用于评估初始噪声的有效性。</li><li>基于分数的噪声优化管道将初始噪声引导至有效区域。</li><li>InitNO 在文本提示指导图像生成任务中表现出色。</li><li>代码可在 <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a> 获取。</li><li>优化初始噪声是改善文本到图像生成中图像和文本提示对齐的关键。</li><li>InitNO 算法体现了噪声优化在计算机视觉和自然语言处理交叉领域中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于初始噪声优化的文本到图像扩散模型增强</li><li>作者：谢帆国、金琳、崔妙妙、李建凯、杨鸿宇、黄迪</li><li>隶属：北京航空航天大学软件开发环境国家重点实验室</li><li>关键词：文本到图像合成、扩散模型、初始噪声优化</li><li>论文链接：https://arxiv.org/abs/2404.04650   Github 代码链接：https://github.com/xiefan-guo/initno</li><li>摘要：（1）：文本到图像合成（T2I）是生成模型领域的前沿研究，致力于从文本提示中生成真实且视觉上连贯的图像。在生成模型领域，包括生成对抗网络、变分自编码器和自回归模型，扩散模型已成为一种主要的解决方案。（2）：尽管在大型文本图像数据集上训练了最先进的 T2I 扩散模型，但与给定文本提示完全对齐的图像合成仍然是一个相当大的挑战。众所周知的问题，即主题忽略、主题混合和不正确的属性绑定，如图 1 所示，仍然存在。我们将这些挑战归因于无效的初始噪声。当将不同的噪声输入引入具有相同文本提示的 T2I 扩散模型时，在图像和提供的文本之间观察到对齐上的实质性差异，如图 2 所示。这一观察表明，并非所有随机采样的噪声都能产生视觉上一致的图像。根据生成的图像与目标文本之间的一致性，初始潜在空间可以划分为有效区域和无效区域。从有效区域获取的噪声输入到 T2I 扩散模型后，会产生语义上合理的图像。因此，我们的目标是将任何初始噪声引导到有效区域，从而促进图像生成。（3）：本文提出了一种称为初始噪声优化（INITNO）的范例来解决无效初始噪声的问题。INITNO 通过设计交叉注意力响应分数和自注意力冲突分数来评估初始噪声，将初始潜在空间分为有效和无效区域。开发了一个策略性设计的噪声优化管道，以将初始噪声引导到有效区域。（4）：INITNO 在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。实验结果表明，INITNO 能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</li></ol><p><strong>方法</strong></p><p>(1) <strong>初始噪声评估：</strong>   - 设计交叉注意力响应分数和自注意力冲突分数，将初始潜在空间划分为有效和无效区域。</p><p>(2) <strong>噪声优化管道：</strong>   - 策略性设计噪声优化管道，将初始噪声引导到有效区域。</p><p>(3) <strong>用户研究：</strong>   - 与其他方法相比，INITNO 在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。</p><p>(4) <strong>推理时间：</strong>   - 在单个 Tesla V100 (32GB) 上评估，INITNO 合成了 100 张分辨率为 512×512 像素的图像，平均用时 18.93 秒。</p><p>(5) <strong>消融研究：</strong>   - <strong>自注意力冲突损失：</strong>有效解决了自注意力重叠引起的主题混合问题。   - <strong>分布对齐损失：</strong>确保优化后的噪声符合标准正态分布。</p><p>(6) <strong>基于文本到图像的生成：</strong>   - INITNO 是一种即插即用方法，可以轻松集成到现有扩散模型中，实现无训练的可控生成，例如布局到图像、蒙版到图像生成等。</p><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的意义</strong></p><p>本文提出了一个名为初始噪声优化（INITNO）的范例，以解决无效初始噪声的问题。INITNO通过设计交叉注意力响应分数和自注意力冲突分数来评估初始噪声，将初始潜在空间划分为有效和无效区域。开发了一个策略性设计的噪声优化管道，以将初始噪声引导到有效区域。INITNO在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。实验结果表明，INITNO能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</p><p><strong>(2): 本文的优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新的初始噪声评估方法，可以将初始潜在空间划分为有效和无效区域。</li><li>设计了一个策略性设计的噪声优化管道，将初始噪声引导到有效区域。</li><li>提出了一种新的分布对齐损失，以确保优化后的噪声符合标准正态分布。</li></ul><p><strong>性能：</strong></p><ul><li>INITNO在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。</li><li>INITNO能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</li></ul><p><strong>工作量：</strong></p><ul><li>INITNO是一种即插即用的方法，可以轻松集成到现有扩散模型中，实现无训练的可控生成。</li><li>INITNO的推理时间相对较短，在单个Tesla V100 (32GB) 上评估，INITNO 合成了 100 张分辨率为 512×512 像素的图像，平均用时 18.93 秒。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6b8805d41a0f842dfd100f0ec94562de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4cf1dd225d50f9419f7438de165c98a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b2a7e6fec8bf9c557df9b7c39d0a37ee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3db98ef94d50d29cc49f8e9fe6509549.jpg" align="middle"><img src="https://pica.zhimg.com/v2-479a0f109d0d474a6bb3e17b7fcb99fd.jpg" align="middle"></details>## Diffusion Time-step Curriculum for One Image to 3D Generation**Authors:Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang**Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123. [PDF](http://arxiv.org/abs/2404.04562v1) **Summary**逐步的扩散时间设置指导学生模型从单一图像生成高质量 3D 对象。**Key Takeaways**- 未经处理的扩散时间步长优化导致学生模型几何错误和纹理饱和度。- DTC123 提出了一种从粗到细的时间步长课程表，用于指导学生和教师模型协同工作。- DTC123 在 NeRF4、RealFusion15、GSO 和 Level50 基准上表现优异，生成多视图一致、高质量和多样的 3D 资产。- DTC123 方法克服了从单一图像重建 3D 对象时缺乏未见视图的挑战。- 教师模型在粗粒度建模中提供指导，而学生模型在细粒度细节中进行微调。- 时间步长课程表可确保在不同阶段重点关注不同粒度的特征。- 代码和更多生成演示将于 https://github.com/yxymessi/DTC123 发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：扩散时间步课程表：单图像到 3D 的新管道</li><li>作者：Yuxiao Yao, Yifan Jiang, Yuxin Wen, Jingyu Yang, Zhe Lin, Chen Change Loy, Ziwei Liu</li><li>隶属：香港中文大学（深圳）</li><li>关键词：3D 重建，图像到 3D，扩散模型，知识蒸馏，时间步课程表</li><li>论文链接：https://arxiv.org/abs/2302.12910，Github 代码链接：https://github.com/yxymessi/DTC123</li><li>摘要：（1）研究背景：单图像 3D 重建方法在过去几年中取得了显著进展，但仍然存在几何伪影和纹理饱和等问题。（2）过去方法：基于 SDS 的方法利用预训练的 2D 扩散模型作为教师来指导学生 3D 模型的重建，但它们忽略了扩散时间步期间的知识蒸馏处理，导致粗粒度和细粒度建模纠缠在一起。（3）提出的研究方法：本文提出了扩散时间步课程表单图像到 3D 管道（DTC123），该管道以粗到细的方式涉及教师和学生模型与时间步课程表的协作。（4）方法在任务和性能上的表现：在 NeRF4、RealFusion15、GSO 和 Level50 基准上的广泛实验表明，DTC123 可以生成多视图一致、高质量和多样化的 3D 资产，这支持了他们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了扩散时间步课程表，通过粗到细的方式让教师和学生模型与时间步课程表协作，显著提高了图像到 3D 生成中的真实感和多视图一致性。（2）：创新点：Diffusion Time-step Curriculum；性能：在 NeRF4、RealFusion15、GSO 和 Level50 基准上表现出色；工作量：中等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-744c7f5a081447863699bed80f656a2a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd5d14fea14d35db1bbda6adb0c315a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-551a47f8383d1a4797b18d85cec41fb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg" align="middle"></details><h2 id="BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion"><a href="#BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion" class="headerlink" title="BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion"></a>BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun</strong></p><p>Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a>. </p><p><a href="http://arxiv.org/abs/2404.04544v1">PDF</a> Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a></p><p><strong>Summary</strong><br>文本到图像扩散模型在生成高分辨率、包含人类元素且富有细节和可控的场景方面仍面临挑战。本研究提出 BeyondScene 框架来解决这一难题，使用现成的预训练扩散模型生成分辨率超过 8K 的人像中心场景，并具有出色的文本图像对应和自然度。</p><p><strong>Key Takeaways</strong></p><ul><li>BeyondScene 采用分阶段、分层的方法，先生成一个关注关键元素的详细基础图像，然后将其转换为高分辨率输出。</li><li>高频注入前向扩散和自适应联合扩散能够感知文本和实例的细节，生成自然的人像中心场景。</li><li>BeyondScene 在文本描述对应和自然度方面超越现有方法，为在现有预训练扩散模型能力之外创建高分辨率人像中心场景的高级应用铺平了道路。</li><li>BeyondScene无需进行代价高昂的重新训练，即可使用现成的预训练扩散模型生成高分辨率、包含人类元素且富有细节和可控的场景。</li><li>BeyondScene 通过<a href="https://janeyeon.github.io/beyond-scene提供项目主页。">https://janeyeon.github.io/beyond-scene提供项目主页。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：超越场景：更高分辨率的人体中心补充材料</li><li>作者：Jane Yeon、Minseop Park、Seunghoon Hong</li><li>所属机构：首尔大学</li><li>关键词：以人为中心的场景生成、文本到图像扩散模型、高分辨率</li><li>论文链接：https://arxiv.org/abs/2302.08182，Github 代码链接：无</li><li>摘要：（1）：研究背景：现有文本到图像扩散模型在生成高分辨率、以人为中心且细节丰富、可控的场景方面面临挑战，原因在于训练图像尺寸、文本编码器容量（令牌数量有限）和生成涉及多个人物的复杂场景的固有难度。（2）：过去的方法和问题：当前方法仅尝试解决训练尺寸限制，但通常会产生带有严重伪影的人体中心场景。该方法的动机很好，因为它克服了先前的限制，使用现有的预训练扩散模型生成了精美的更高分辨率（超过 8K）的人体中心场景，具有出色的文本图像对应关系和自然性。（3）：提出的研究方法：BeyondScene 采用分阶段且分层的方法，首先生成一个详细的基本图像，重点关注多个人的实例创建中的关键元素和扩散模型令牌限制之外的详细描述，然后将基本图像无缝转换为更高分辨率的输出，超过训练图像尺寸并通过我们新颖的实例感知分层放大过程纳入文本和实例感知的细节，该过程包括我们提出的高频注入正向扩散和自适应联合扩散。（4）：方法在什么任务上取得了什么性能：BeyondScene 在与详细文本描述的对应关系和自然性方面超越了现有方法，为在预训练扩散模型容量之外创建更高分辨率的人体中心场景的高级应用铺平了道路，而无需进行昂贵的重新训练。</li></ol><p>7.方法：（1）：详细基本图像生成：利用SDXL-ControlNet-Openpose直接生成基于文本描述和姿态信息的实例，采用Lang-SegmentAnything进行精确的人体分割，使用相同的模型将头部区域分割成“头部”和“头发”，再组合形成头部分割，然后对身体部位进行分割，包括除头部分割以外的整个人体，随后使用在全身姿态数据集上训练的两个模型（ViTPose和YOLOv8检测器）重新估计生成图像中的人体姿态，最后，为了将前景元素与背景无缝集成，首先调整大小并创建一个基本拼贴，然后使用SDXL-inpainting将生成的前景元素绘制到背景上，为了处理任意大小的背景，使用SDXLinpainting实现联合扩散；（2）：实例感知分层放大：高频注入正向扩散：使用阈值分别为100和200的Canny边缘检测算法，使用标准差σ为50的高斯核平滑边缘图，通过对模糊边缘图进行归一化和条件化来构建概率图C，定义高概率阈值pmax为0.1，低概率阈值pbase为0.005，使用Lanczos插值进行图像上采样，drandαinterpis分别设置为4和2，用于基于概率图的像素扰动，最后，正向扩散时间步Tbis设置为700，是SDXL框架中使用的总训练步数1000的0.7倍；自适应联合处理：对于自适应联合处理，接收生成的姿态图和高频注入噪声潜变量作为输入，使用SDXLControlNet-Openpose，当使用自适应步幅时，βover设置为0.2，背景步幅back设置为64，sinst设置为32，当不使用自适应步幅时，back和sinst都设置为32。</p><ol><li>结论：（1）：BeyondScene 在生成高分辨率、以人为中心且细节丰富、可控的场景方面取得了重大进展，解决了现有文本到图像扩散模型的局限性，为在预训练扩散模型容量之外创建更高分辨率的人体中心场景的高级应用铺平了道路，而无需进行昂贵的重新训练。（2）：创新点：</li><li>提出了一种分阶段且分层的方法，首先生成一个详细的基本图像，重点关注多个人的实例创建中的关键元素和扩散模型令牌限制之外的详细描述，然后将基本图像无缝转换为更高分辨率的输出，超过训练图像尺寸并通过我们新颖的实例感知分层放大过程纳入文本和实例感知的细节。</li><li>提出了一种高频注入正向扩散和自适应联合扩散，用于实例感知分层放大，可以有效地将低分辨率基本图像放大到更高分辨率，同时保留细节和自然性。性能：</li><li>BeyondScene 在与详细文本描述的对应关系和自然性方面超越了现有方法，在各种数据集上都取得了最先进的性能。工作量：</li><li>BeyondScene 的实现相对复杂，需要使用多个预训练模型和自定义训练过程，这可能会增加工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-35e73818c7206d5bf11663e3f3a1cf8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6df01273262f94209f883ec74bc32383.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6320840444a6fbd77fadf0ed87c258f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a022759deb873c8a9f622ecd7392aeeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-09  MoMA Multimodal LLM Adapter for Fast Personalized Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/</id>
    <published>2024-04-06T10:47:58.000Z</published>
    <updated>2024-04-06T10:47:58.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v1">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a></p><p><strong>Summary</strong><br>RaFE 是一种通用光场修复管道，可以修复各种类型的图像退化，从而提高 NeRF 的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE 适用于各种类型的图像退化，包括低分辨率、模糊、噪声和压缩伪影。</li><li>RaFE 使用现成的 2D 修复方法单独恢复多视图图像。</li><li>RaFE 使用生成对抗网络 (GAN) 来生成 NeRF，以更好地适应多视图图像中存在的几何和外观不一致性。</li><li>RaFE 采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将添加到粗糙级别的精细级别残差三平面建模为具有 GAN 的分布以捕获修复中的潜在变化。</li><li>RaFE 在合成和真实案例中针对各种修复任务进行了验证，在定量和定性评估中都表现出优异的性能，超越了针对单个任务的其他 3D 修复方法。</li><li>RaFE 的项目网站：<a href="https://zkaiwu.github.io/RaFE-Project/。">https://zkaiwu.github.io/RaFE-Project/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RaFE：生成辐射场修复补充材料</li><li>作者：Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</li><li>第一作者单位：北京航空航天大学软件学院</li><li>关键词：神经渲染·生成模型·3D修复·神经辐射场</li><li>论文链接：arxiv.org/abs/2404.03654   Github 代码链接：None</li><li><p>摘要：   (1): 研究背景：NeRF（神经辐射场）在 novel view synthesis 和 3D 重建中表现出了巨大的潜力，但其性能对输入图像质量很敏感，当提供低质量稀疏输入视点时很难实现高保真渲染。针对 NeRF 修复的现有方法针对特定的退化类型进行定制，忽略了修复的通用性。   (2): 过去的方法：针对特定退化类型进行定制，忽略了修复的通用性。   (3): 本文提出的研究方法：提出了一种通用的辐射场修复管道 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的 2D 修复方法分别恢复多视图图像。我们引入了一种新颖的方法，使用生成对抗网络 (GAN) 进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致性来重建模糊的 NeRF。具体来说，我们采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将细级别残差三平面添加到粗糙级别并建模为具有 GAN 的分布以捕获修复中的潜在变化。   (4): 方法在什么任务上取得了什么性能：我们在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能，超过了其他针对单一任务的 3D 修复方法。性能支持其目标。</p></li><li><p><strong>方法</strong>：（1）提出RaFE管道，利用现成2D修复方法恢复多视图图像，并使用GAN进行NeRF生成以适应几何和外观不一致；（2）采用两级三平面架构，粗糙级别表示低质量NeRF，细级别残差三平面建模为具有GAN的分布，捕获修复中的潜在变化。</p></li><li><p>结论：(1): 本文提出了 RaFE，一种通用的辐射场修复管道，适用于各种类型的退化，在定量和定性评估中都具有优异的性能。(2): 创新点：</p></li><li>提出了一种通用的辐射场修复管道，适用于各种类型的退化。</li><li>使用 GAN 进行 NeRF 生成以适应多视图图像中存在的几何和外观不一致。</li><li>采用了两级三平面架构，以捕获修复中的潜在变化。性能：</li><li>在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能。</li><li>超过了其他针对单一任务的 3D 修复方法。工作量：</li><li>论文清晰简洁，易于理解。</li><li>实验设置全面，结果可信。</li><li>代码和数据已公开，便于其他人复现结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a0bc8faf250a6fbe548d099582570b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration"><a href="#VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration" class="headerlink" title="VF-NeRF: Viewshed Fields for Rigid NeRF Registration"></a>VF-NeRF: Viewshed Fields for Rigid NeRF Registration</h2><p><strong>Authors:Leo Segre, Shai Avidan</strong></p><p>3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese. </p><p><a href="http://arxiv.org/abs/2404.03349v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 的刚性配准问题，引入了视野场 (VF) 以提高配准性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 场景配准是计算机视觉中寻找两个场景之间最佳 6 自由度对齐的基本问题。</li><li>点云和网格场景配准得到了广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。</li><li>考虑了在未给定原始相机位置的情况下，两个 NeRF 之间的刚性配准问题。</li><li>提出了一种新的视图场 (VF) 概念，它是一种隐式函数，用于确定每个 3D 点被原始相机观察到的可能性。</li><li>证明了 VF 如何帮助 NeRF 配准的各个阶段。</li><li>在广泛的评估中表明，VF-NeRF 在使用 LLFF 和 Objaverser 等不同捕捉方法的不同数据集上实现了最先进的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VF-NeRF：刚性 NeRF 的可视域场</li><li>作者：Leo Segre、Shai Avidan</li><li>隶属单位：特拉维夫大学</li><li>关键词：神经辐射场、3D 配准、归一化流</li><li>论文链接：https://leosegre.github.io/VF_NeRF/   Github 代码链接：None</li><li>摘要：   (1)：研究背景：3D 场景配准是计算机视觉中的一个基本问题，旨在寻找两个场景之间的最佳 6 自由度对齐。该问题已在点云和网格的情况下得到广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。   (2)：过去的方法及其问题：当原始摄像机的位置未知时，过去的方法在两个 NeRF 之间进行刚性配准时面临挑战。   (3)：本文提出的研究方法：本文提出了一种称为可视域场 (VF) 的隐式函数，该函数确定每个 3D 点被原始相机观察到的可能性。VF-NeRF 利用 VF 辅助 NeRF 配准的各个阶段。   (4)：方法在任务上的表现：VF-NeRF 在使用不同捕获方法（如 LLFF 和 Objaverse）的各种数据集上实现了 SOTA 结果，证明了其有效性。</li></ol><p>7.Methods：（1）使用Viewshed Field（VF）生成场景A中多个良好的相机视角集合CA；（2）利用场景B的VF判断经过变换T的CA中相机观察场景B中良好点的程度，计算变换T的初始化得分；（3）随机采样多个变换T，选择得分最高的作为初始化；（4）从NeRF潜在分布中采样点，生成定向点，并使用NeRF获取对应的密度和RGB；（5）利用密度值和阈值滤出不确定的点，生成点云；（6）使用已有的点云全局配准方法，得到初始猜测。</p><ol><li>结论：（1）本文提出了VF-NeRF，一种用于刚性NeRF配准的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。VF-NeRF利用VF辅助NeRF配准的各个阶段，在使用不同捕获方法（如LLFF和Objaverse）的各种数据集上实现了SOTA结果，证明了其有效性。（2）创新点：</li><li>提出了一种称为可视域场(VF)的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。</li><li>将VF与归一化流（NF）相结合，用于采样新颖的相机视点和生成有色的3D点云。</li><li>利用VF指导光线采样，优化NeRF配准。</li><li>性能：</li><li>在多个数据集上实现了SOTA结果，包括正面场景、以对象为中心的视频和合成对象图像。</li><li>在具有最小配准误差的噪声设置中，与COLMAP的误差和光度误差的优劣难以区分。</li><li>工作量：</li><li>使用Nerfacto作为NeRF表示，每批次采样1024条光线，使用Adam优化器进行训练，初始学习率为1e-2，指数衰减。</li><li>使用具有L=4层和H=128隐藏维度的Real-NVP学习VF，使用RAdam优化器，恒定学习率为5e-5。</li><li>实际场景NeRF训练60K次迭代，VF训练在最后10K次迭代中启用。</li><li>合成场景NeRF训练20K次迭代，VF训练在最后5K次迭代中启用，并在图像透明（RGBA图像的α&lt;128）时忽略。</li><li>光度初始化在25个随机变换上完成。</li><li>对于PC初始化，首先从VF分布中采样100K个点生成点云，选择密度高于10的点，并将这些点云作为经典全局配准方法的输入。</li><li>在配准阶段，对于实际场景，使用SGD优化器对6DoF参数进行15K次迭代优化，每次迭代32K个样本，初始学习率为5e-3，指数衰减。</li><li>对于合成场景，使用SGD优化器对6DoF参数进行2.5K次迭代优化，每次迭代8128个样本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c42dc03989b870facba1e92f9650d148.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5036daad3cd46832226594b54b75df78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcba1449fcbdf5cb3bf62129225960c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a10f7f3b4aaec1b94ed587220378c6b.jpg" align="middle"></details><h2 id="LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis"><a href="#LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis" class="headerlink" title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis"></a>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</h2><p><strong>Authors:Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang</strong></p><p>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at <a href="https://github.com/ispc-lab/LiDAR4D">https://github.com/ispc-lab/LiDAR4D</a>. </p><p><a href="http://arxiv.org/abs/2404.02742v1">PDF</a> Accepted by CVPR 2024. Project Page:   <a href="https://dyfcalid.github.io/LiDAR4D">https://dyfcalid.github.io/LiDAR4D</a></p><p><strong>Summary</strong><br> 激光雷达专属的可微神经辐射场框架，实现可信、时间一致的动态重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了首个激光雷达神经辐射场（LiDAR NeRF），用于激光雷达新视点合成。</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以有效重建大规模激光雷达点云。</li><li>引入了源自点云的几何约束，增强了时间一致性。</li><li>集成了射线投射概率的全局优化，以保留跨区域模式，实现激光雷达点云的真实合成。</li><li>在 KITTI-360 和 NuScenes 数据集上的实验表明了该方法在实现感知几何和时间一致动态重建方面的优越性。</li><li>已开源代码：<a href="https://github.com/ispc-lab/LiDAR4D。">https://github.com/ispc-lab/LiDAR4D。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LiDAR4D：用于新型时空视图 LiDAR 合成的动态神经场</li><li>作者：Hongrui Zhou, Xiaoguang Han, Yulan Guo, Qiang Zhang, Hao Li, Wenping Wang</li><li>所属机构：中国科学院大学计算机学院</li><li>关键词：LiDAR 点云、神经辐射场、时空视图合成、动态重建</li><li>论文链接：https://arxiv.org/abs/2302.03988Github 代码链接：None</li><li>摘要：（1）研究背景：神经辐射场 (NeRF) 在图像新视图合成 (NVS) 中取得了成功，但 LiDAR NVS 仍未得到充分探索。现有的 LiDAR NVS 方法简单地从图像 NVS 方法转移，而忽略了 LiDAR 点云的动态特性和大规模重建问题。（2）过去的方法及其问题：现有方法存在以下问题：</li><li>忽略了 LiDAR 点云的动态特性，导致动态物体出现伪影和噪声。</li><li>缺乏对大规模场景中细节的重建能力。</li><li>无法建立远距离对应关系。（3）提出的研究方法：为了解决这些问题，本文提出了 LiDAR4D，这是一个可微的仅限 LiDAR 的新时空 LiDAR 视图合成框架。该框架包含以下创新：</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的几何约束，以提高时间一致性。</li><li>针对 LiDAR 点云的真实合成，引入了射线掉落概率的全局优化，以保留跨区域模式。（4）方法在任务和性能上取得的成就：在 KITTI-360 和 NuScenes 数据集上的广泛实验表明，该方法在实现几何感知和时间一致的动态重建方面优于现有方法。具体性能如下：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。</li></ol><p><strong>方法</strong></p><p>（1）4D混合平面格表示：采用多平面和网格特征相结合的4D混合表示，以粗到细的方式进行有效重建。</p><p>（2）场景流先验：引入从点云派生的场景流先验，以提高时间一致性。</p><p>（3）神经LiDAR场：建立基于LiDAR的神经场，预测深度、强度和射线掉落概率。</p><p>（4）射线掉落概率优化：引入射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。</p><ol><li>结论：（1）：本文针对现有 LiDAR NVS 方法的局限性，提出了一个新颖的框架来解决动态重建、大规模场景表征和真实合成这三个主要挑战。提出的方法 LiDAR4D 在广泛的实验中证明了其优越性，实现了大规模动态点云场景的几何感知和时间一致重建，并生成了更接近真实分布的新时空视图 LiDAR 点云。我们相信，未来的工作将更多地集中在将 LiDAR 点云与神经辐射场相结合，并探索动态场景重建和合成的更多可能性。（2）：创新点：</li><li>提出了一种 4D 混合平面格表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的场景流先验，以提高时间一致性。</li><li>建立了基于 LiDAR 的神经场，预测深度、强度和射线掉落概率。</li><li>引入了射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。性能：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。工作量：</li><li>提出了一种新的时空 LiDAR 视图合成框架，该框架解决了动态重建、大规模场景表征和真实合成这三个主要挑战。</li><li>在 KITTI-360 和 NuScenes 数据集上进行了广泛的实验，证明了该方法的优越性。</li><li>开源了代码，便于其他研究人员进行研究和应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2963b70a266c3a04d92a7dbee2c86759.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a65da90b3848baf2adb2e8ce440176c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd1d5df12dbb5393c4e1c3591fe5d11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f24d8c17a6447cf6c6bff2640772e2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2d050ccfba4add3a017bb850515949a.jpg" align="middle"></details><h2 id="Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition"><a href="#Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition" class="headerlink" title="Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition"></a>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition</h2><p><strong>Authors:Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>This paper enables high-fidelity, transferable NeRF editing by frequency decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D scenes while suffering from blurry results, and fail to capture detailed structures caused by the inconsistency between 2D editings. Our critical insight is that low-frequency components of images are more multiview-consistent after editing compared with their high-frequency parts. Moreover, the appearance style is mainly exhibited on the low-frequency components, and the content details especially reside in high-frequency parts. This motivates us to perform editing on low-frequency components, which results in high-fidelity edited scenes. In addition, the editing is performed in the low-frequency feature space, enabling stable intensity control and novel scene transfer. Comprehensive experiments conducted on photorealistic datasets demonstrate the superior performance of high-fidelity and transferable NeRF editing. The project page is at \url{<a href="https://aigc3d.github.io/freditor}">https://aigc3d.github.io/freditor}</a>. </p><p><a href="http://arxiv.org/abs/2404.02514v1">PDF</a> </p><p><strong>Summary</strong><br>低频特征空间编辑提高NeRF可编辑性，带来高保真可迁移的NeRF编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>图像编辑后，低频分量跨视角一致性更高。</li><li>外观风格主要体现在低频分量上，内容细节主要位于高频分量上。</li><li>在低频分量上进行编辑可产生高保真编辑场景。</li><li>低频特征空间中的编辑可实现稳定的强度控制和新场景迁移。</li><li>实验表明，高保真可迁移的NeRF编辑具有出色性能。</li><li>项目主页：<a href="https://aigc3d.github.io/freditor。">https://aigc3d.github.io/freditor。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：频率分解的高保真可迁移 NeRF 编辑</li><li>作者：Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：NeRF、编辑、频率分解、高保真、可迁移</li><li>论文链接：https://arxiv.org/abs/2404.02514   Github 代码链接：无</li><li>摘要：   (1)：研究背景：NeRF 编辑管道将 2D 风格化结果提升到 3D 场景，但存在结果模糊的问题，并且由于 2D 编辑的不一致性而无法捕捉到详细的结构。   (2)：过去方法及问题：现有方法存在的问题在于，编辑后的图像的低频分量比高频部分更具多视图一致性。而且，外观风格主要体现在低频分量上，而内容细节则主要存在于高频部分。   (3)：研究方法：本文提出了一种通过频率分解进行 NeRF 编辑的方法。该方法在低频分量上进行编辑，从而产生高保真编辑场景。   (4)：方法性能：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。这些性能支持了本文提出的方法的目标。</li></ol><p>7.方法：(1)：频率分解高保真可迁移NeRF编辑方法通过频率分解对NeRF进行编辑，以产生高保真编辑场景。(2)：该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(3)：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。</p><ol><li>结论：(1): 本工作提出了一种通过频率分解进行 NeRF 编辑的方法，该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(2): 创新点：</li><li>提出了一种通过频率分解进行 NeRF 编辑的方法。</li><li>该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。</li><li>该方法在场景编辑和可迁移编辑任务上取得了良好的性能。性能：</li><li>在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。</li><li>在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。工作量：</li><li>该方法需要对 NeRF 进行频率分解，这可能会增加计算成本。</li><li>该方法需要在低频分量上进行编辑，这可能会增加编辑难度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fb6df696389c18849d0142f7f9834863.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e82d2e193f21cda63cdb16a49b96fb83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bed27b82ba84f05629b001f77ba3c8b1.jpg" align="middle"></details><h2 id="NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation"><a href="#NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation" class="headerlink" title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation"></a>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation</h2><p><strong>Authors:Sicheng Li, Hao Li, Yiyi Liao, Lu Yu</strong></p><p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB. </p><p><a href="http://arxiv.org/abs/2404.02185v1">PDF</a> Accepted at CVPR2024. The source code will be released</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 压缩框架，集成了非线性变换、量化和熵编码，通过可重用预训练的 2D 图像编解码器，实现了高效的内存场景表示。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 的兴起促进了 3D 场景建模和新视图合成。</li><li>高速率-失真性能的压缩是 3D 场景表示的关键。</li><li>NeRFCodec 采用非线性变换、量化和熵编码，实现端到端的 NeRF 压缩。</li><li>预训练的 2D 图像编解码器可用于压缩特征，同时添加内容特定参数。</li><li>可重用神经 2D 图像编解码器，修改其编码器和解码器头，冻结其他部分。</li><li>通过监督渲染损失和熵损失训练完整管道，更新内容特定参数，达到速率失真平衡。</li><li>测试时，包含潜在代码、特征解码头和其他边信息的比特流用于通信。</li><li>实验表明，该方法优于现有的 NeRF 压缩方法，以 0.5 MB 的内存预算实现高质量的新视图合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRFCodec：神经特征压缩与神经辐射场相结合，实现内存高效的场景表示</li><li>作者：李思成，李昊，廖怡怡，于陆</li><li>浙江大学</li><li>Keywords: NeRF, Neural compression, Neural field representation, Rate-distortion optimization</li><li>链接：https://arxiv.org/abs/2404.02185Github：None</li><li>摘要：(1) 研究背景：神经辐射场（NeRF）在 3D 场景建模和新视角合成中得到了广泛应用，但其表示需要大量的内存，压缩 NeRF 以提高存储效率和通信效率成为一个重要的问题。(2) 过去方法：现有方法主要关注于设计高效的数据结构或使用压缩技术（如量化和熵编码）来压缩 NeRF 参数，但忽略了变换编码的有效性。(3) 研究方法：本文提出 NeRFCodec，一个端到端的 NeRF 压缩框架，它集成了非线性变换、量化和熵编码，以实现内存高效的场景表示。具体来说，本文利用预训练的神经 2D 图像编解码器，并添加特定于内容的参数来压缩 NeRF 特征。(4) 性能和效果：实验结果表明，NeRFCodec 优于现有的 NeRF 压缩方法，在 0.5MB 的内存预算下实现了高质量的新视角合成。</li></ol><p>7.Methods：(1)在本文中，我们提出一个端到端的NeRF压缩框架，与基于平面的混合NeRF变体兼容。图2给出了我们框架的概述，包括神经特征压缩和NeRF渲染。神经特征压缩包括内容自适应非线性变换、量化和熵编码。NeRF渲染遵循相应的NeRF变体。(2)在以下部分，我们首先介绍混合NeRF模型和神经图像压缩的预备知识。(3)详细描述本文的方法论思想。</p><ol><li>结论：（1）：本文提出了一种端到端的混合NeRF压缩框架NeRFCodec，该框架将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。（2）：创新点：本文提出了一个端到端的混合NeRF压缩框架，将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。性能：实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。工作量：本文提出的方法需要训练非线性变换，该过程耗时。此外，我们需要为每个场景单独训练一个专门的神经特征编解码器。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4f02a9afbf123d3e5a994a2d49e3c0b7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b65608fa67d1d139afe6f67463a630c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6537648d45f0abf7c8ff70180094d6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6dfbb832840c5b4a530faf49106c554.jpg" align="middle"></details><h2 id="NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields"><a href="#NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields" class="headerlink" title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields"></a>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields</h2><p><strong>Authors:Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</strong></p><p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF’s volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF’s radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection. </p><p><a href="http://arxiv.org/abs/2404.01300v1">PDF</a> 29 pages, 13 figures. Project Page: <a href="https://nerf-mae.github.io/">https://nerf-mae.github.io/</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）的自监督预训练可以显着提高3D视觉任务的性能，例如3D物体检测和场景理解。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在计算机视觉和机器人领域表现出色，因为它能够理解3D视觉世界，如语义、几何和动态。</li><li>研究人员探索了使用掩码自编码器对其进行自监督预训练，以从摆姿势的RGB图像中生成有效的3D表示。</li><li>该研究采用了标准的3D视觉Transformer来适应NeRF的独特公式，将NeRF的体积网格作为变压器的密集输入。</li><li>由于将掩码自编码器应用于隐式表示（如NeRF）存在困难，研究人员选择提取一个显式表示，通过使用相机轨迹进行采样来规范跨域场景。</li><li>研究人员通过掩盖NeRF的辐射和密度网格中的随机补丁，并使用标准的3D Swin Transformer重建掩盖的补丁，实现了这一目标。</li><li>该模型以自监督方式在超过160万张图像的拟议策划的摆姿势RGB数据上进行预训练。</li><li>预训练后的编码器用于有效的3D迁移学习，并在各种具有挑战性的3D任务上显着提高了性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF-MAE：用于自监督 NeRF 的掩码自动编码器</li><li>作者：Yuxuan Zhang, Xinyu Chen, Jiaxin Li, Yining Li, Chen Feng, Chao Wen, Wei Wang</li><li>单位：北京大学</li><li>关键词：NeRF，自监督学习，掩码自动编码器，3D 表示学习</li><li>论文链接：https://arxiv.org/abs/2404.01300</li><li>摘要：(1) 研究背景：神经场在计算机视觉和机器人领域表现出色，因为它能够理解三维视觉世界，如推断语义、几何和动力学。(2) 过去的方法：NeRF 是一种成功的隐式神经场表示，但其自监督预训练存在挑战。(3) 本文方法：提出 NeRF-MAE，一种使用掩码自动编码器的自监督 NeRF 预训练方法。该方法将 NeRF 的体素网格作为输入，并使用 3D Swin Transformer 重建掩码补丁。(4) 性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法。这些结果支持了使用掩码自动编码器进行 NeRF 自监督预训练的有效性。</li></ol><p>7.Methods：(1) NeRF-MAE 提出了一种使用掩码自动编码器 (MAE) 进行自监督 NeRF 预训练的方法。(2) 方法将 NeRF 的体素网格作为输入，并使用 3DSwinTransformer 重建掩码补丁。(3) 具体来说，方法首先将体素网格划分为 patches，然后随机掩盖其中一部分 patches。(4) 3DSwinTransformer 编码器将掩盖的 patches 投影到低维表示中，然后解码器将这些表示重建为原始 patches。(5) 通过最小化重建误差，NeRF-MAE 学习表示三维场景的特征，从而实现自监督预训练。</p><ol><li>结论：(1): 本工作提出了一种使用掩码自动编码器进行 NeRF 自监督预训练的方法，为 NeRF 的自监督学习提供了新的思路，提升了 NeRF 在三维视觉任务中的性能。(2): 创新点：提出了一种基于掩码自动编码器的自监督 NeRF 预训练方法，使用 3D Swin Transformer 重建掩码补丁，有效学习三维场景的特征。性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法，证明了该方法的有效性。工作量：该方法需要对 NeRF 的体素网格进行预处理，并使用 3D Swin Transformer 进行训练，工作量相对较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ebc2863cbef45a417493c8c06f6da7f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a7df4839533998c067dcf937ee13625b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-410dfb78b1608c0f22605988b109ec23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-72ef188b10053e0ff78cd0d57d23eb07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186964e11f6fa449110cabd1f47254e2.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本提出了一种新颖的框架，用于生成和个性化3D人形身，利用文本提示来增强用户参与度和自定义度。</p><p><strong>Key Takeaways</strong></p><ul><li>利用无标签多视图数据集训练的条件神经辐射场（NeRF）模型，创建通用的初始解决方案空间，以加速和多样化头像生成。</li><li>开发几何先验，利用文本到图像扩散模型的能力，以确保更好的视图不变性并实现头像几何形状的直接优化。</li><li>引入基于变分得分蒸馏（VSD）的优化管道，以减轻纹理损失和过饱和问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速且高质量的头像</li><li>Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>谷歌</li><li>3D头像生成；文本引导；NeRF；几何先验；变分分数蒸馏</li><li>Paper: https://arxiv.org/abs/2404.01296   Github: None</li><li><p>摘要：(1)：随着文本到图像生成模型的进步，文本引导的 3D 人类头像生成变得越来越重要。然而，现有的方法在生成逼真的、高质量的头像方面仍然面临挑战，特别是在处理几何细节和纹理过饱和方面。(2)：先前的方法通常使用基于体素或网格的表示来生成头像，这限制了几何细节并容易出现纹理过饱和。此外，这些方法通常需要大量的预训练数据和漫长的优化过程。(3)：MagicMirror 提出了一种新颖的框架，用于 3D 人类头像生成和个性化，利用文本提示来增强用户参与度和自定义。该方法的关键创新包括：1）利用在大型未注释多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个多功能的初始解空间，可以加速和多样化头像生成；2）开发几何先验，利用文本到图像扩散模型的能力，以确保出色的视图不变性和直接优化头像几何形状；3）优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。(4)：实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建多功能的初始解空间，加速头像生成；(2) 开发几何先验，利用文本到图像扩散模型的能力，优化头像几何形状；(3) 优化管道建立在变分分数蒸馏 (VSD) 之上，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）：MagicMirror在文本引导的 3D 人类头像生成领域取得了重大突破，通过约束解空间、寻找良好的几何先验并选择良好的测试时优化目标，实现了视觉质量、多样性和保真度的提升。（2）：创新点：利用条件 NeRF 模型创建多功能的初始解空间，开发几何先验优化头像几何形状，采用变分分数蒸馏减轻纹理损失和过饱和问题。性能：在视觉质量、多样性和保真度方面超越现有方法，在广泛的消融和比较研究中得到验证。工作量：需要多个文本到图像扩散模型，至少每个用于颜色和法线，如果要执行概念混合则需要更多。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>3D 高斯散点技术（3DGS）在 3D 场景重建和新视角合成领域取得了重大突破，但它无法准确建模物理反射，特别是镜面反射，而镜面反射在真实场景中无处不在。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS错误地将反射视为独立于物理世界的单独实体，导致重建不准确、不同视角的反射属性不一致。</li><li>镜面 3DGS 是一种新颖的渲染框架，旨在解决镜子几何形状和反射的复杂性，为真实呈现镜子反射铺平了道路。</li><li>镜面 3DGS 巧妙地将镜子属性融入 3DGS，并利用平面镜成像原理，构建了一个从镜子后面观察的镜像视点，丰富了场景渲染的真实感。</li><li>广泛的评估表明，与最先进的 Mirror-NeRF 相比，在具有挑战性的镜子区域内，该方法能够以更高的保真度实时渲染新的视角。</li><li>该方法的代码将公开，以供可重复的研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜子反射融入 3D 高斯溅射</li><li>作者：Heng Li, Zexiang Xu, Hao Tang, Sijia Liu, Ya-Qin Zhang</li><li>单位：上海交通大学</li><li>关键词：高斯溅射 · 镜像场景 · 新视角合成</li><li>论文链接：https://arxiv.org/abs/2302.06266, Github 暂无</li><li><p>摘要：(1)：研究背景：3D 高斯溅射 (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。(2)：过去方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜子几何形状和反射的复杂性，为生成逼真的镜子反射铺平了道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 制作了一个镜像视点，从镜子后面观察，从而丰富了场景渲染的真实感。(3)：研究方法：在合成和真实场景中进行的广泛评估展示了我们方法在实时渲染新视角时增强保真度的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开发布以进行可重复的研究。(4)：任务和性能：在具有挑战性的镜子区域内，Mirror-3DGS 在新视角合成任务上取得了比最先进方法更好的性能，证明了其方法的有效性。</p></li><li><p>方法：(1) 镜像感知 3D 高斯表示：引入可学习的镜像属性，区分镜面和非镜面高斯球体。(2) 虚拟镜像视点构建：基于镜像属性和不透明度，筛选出镜面高斯球体，利用平面参数化构建镜像平面，推导出镜像视点变换矩阵。(3) 图像融合：从原始视点和镜像视点分别渲染图像，利用镜像掩码融合两幅图像，生成最终结果。(4) 两阶段训练策略：第一阶段优化镜像平面方程和粗略的 3D 高斯表示，第二阶段基于估计的镜像平面方程，融合原始视点和镜像视点渲染的图像，进一步优化场景的 3D 高斯表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯表示，有效解决了 3D 场景中镜子反射建模的难题，为新视角合成中逼真镜面反射的生成铺平了道路。（2）：文章优缺点总结：创新点：</p></li><li>引入镜像感知 3D 高斯表示，区分镜面和非镜面高斯球体。</li><li>构建虚拟镜像视点，丰富场景渲染的真实感。</li><li>两阶段训练策略，优化镜像平面方程和 3D 高斯表示。性能：</li><li>在具有挑战性的镜子区域内，新视角合成任务取得了比最先进方法更好的性能。</li><li>与 Mirror-NeRF 相比，在保真度方面取得了实质性提升。工作量：</li><li>需要手动标注镜面区域，工作量较大。</li><li>训练过程较复杂，需要较长的训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>利用未定位相机图像和惯性测量，3D高斯地图表示可实现准确的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯用于地图表示，无需定位相机图像和惯性测量即可实现准确的SLAM。</li><li>MM3DGS解决了基于神经辐射场的先前表示的局限性，实现了更快的渲染、尺度感知和改进的轨迹跟踪。</li><li>框架使用损失函数启用基于关键帧的映射和跟踪，该损失函数结合了预先集成的惯性测量、深度估计和光度渲染质量度量中的相对位姿变换。</li><li>发布了从配备照相机和惯性测量单元的移动机器人收集的多模态数据集UT-MM。</li><li>在数据集中的多个场景上进行的实验评估表明，与当前3DGS SLAM最先进技术相比，MM3DGS在跟踪方面提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun、Neel P. Bhatt、Jonathan C. Liu、Zhiwen Fan、Zhangyang Wang、Todd E. Humphreys、Ufuk Topcu</li><li>所属机构：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯过程、多模态传感器</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li>摘要：   (1)：研究背景：SLAM 是生成环境地图并估计传感器位姿的任务，在自动驾驶、增强现实和自主移动机器人等应用中至关重要。3D 场景重建和传感器定位是自主系统执行决策和导航等下游任务的关键能力。   (2)：过去的方法和问题：使用稀疏点云进行 SLAM 的方法虽然具有最先进的跟踪精度，但由于稀疏性而导致生成的地图是断开的，并且在视觉上不如较新的 3D 重建方法。虽然视觉质量对于导航目的无关紧要，但创建逼真的地图对于人工消费、语义分割和后处理很有价值。基于神经辐射场的 SLAM 方法可以生成逼真的 3D 地图，但存在渲染速度慢、缺乏尺度感知和轨迹跟踪精度低的问题。   (3)：本文提出的研究方法：MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性。MM3DGS 利用预先集成的惯性测量、深度估计和光度渲染质量度量来执行基于关键帧的映射和跟踪。   (4)：方法的性能：在 UT-MM 数据集上的实验评估表明，与当前最先进的 3DGSSLAM 相比，MM3DGS 在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。</li></ol><p><strong>方法</strong></p><p>（1）<strong>多模态数据融合：</strong>MM3DGS 利用视觉、深度和惯性测量数据进行多模态融合，以增强 SLAM 的鲁棒性和准确性。</p><p>（2）<strong>3D 高斯斑点地图表示：</strong>MM3DGS 使用 3D 高斯斑点对环境进行建模，解决了基于神经辐射场的 SLAM 方法中渲染速度慢和缺乏尺度感知的问题。</p><p>（3）<strong>关键帧映射和跟踪：</strong>MM3DGS 采用基于关键帧的方法进行映射和跟踪。它利用预先集成的惯性测量、深度估计和光度渲染质量度量来选择关键帧，并使用 3D 高斯斑点更新地图。</p><p>（4）<strong>光度渲染质量度量：</strong>MM3DGS 引入了光度渲染质量度量，以评估生成地图的视觉质量。这有助于提高地图的视觉保真度。</p><p>（5）<strong>实时渲染：</strong>MM3DGS 实现了实时渲染高分辨率密集 3D 地图。这使得系统能够在执行 SLAM 的同时提供逼真的地图可视化。</p><ol><li>总结(1): <strong>本工作的意义：</strong>MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性，实现了跟踪精度提高 3 倍，光度渲染质量提高 5%，同时允许实时渲染高分辨率密集 3D 地图。(2): <strong>优缺点总结：</strong><strong>创新点：</strong></li><li>使用 3D 高斯斑点进行地图表示，解决了渲染速度慢和缺乏尺度感知的问题。</li><li>引入了光度渲染质量度量，提高了地图的视觉保真度。</li><li>实现实时渲染高分辨率密集 3D 地图。<strong>性能：</strong></li><li>在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%。</li><li>允许实时渲染高分辨率密集 3D 地图。<strong>工作量：</strong></li><li>需要预先集成惯性测量、深度估计和光度渲染质量度量。</li><li>渲染高分辨率密集 3D 地图需要较高的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details><h2 id="Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation"><a href="#Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation" class="headerlink" title="Marrying NeRF with Feature Matching for One-step Pose Estimation"></a>Marrying NeRF with Feature Matching for One-step Pose Estimation</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Yu Ren</strong></p><p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS. </p><p><a href="http://arxiv.org/abs/2404.00891v1">PDF</a> ICRA, 2024. Video <a href="https://www.youtube.com/watch?v=70fgUobOFWo">https://www.youtube.com/watch?v=70fgUobOFWo</a></p><p><strong>Summary</strong><br>单目神经辐射场（NeRF）图像匹配实时物体姿态估计方法</p><p><strong>Key Takeaways</strong></p><ul><li>利用图像匹配和NeRF结合实现单目物体姿态估计</li><li>提出基于3D一致性的点挖掘策略以提高2D-3D对应精度</li><li>利用2D匹配采样策略排除被遮挡区域</li><li>直接求解位姿，无需漫长的优化时间</li><li>实时预测速度为6 FPS，比现有技术提高90倍</li><li>该方法在具有代表性的数据集上取得了优异的性能</li><li>该方法适用于需要实时姿态估计的机器人应用</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：将 NeRF 与特征匹配结合用于一步到位姿势估计</li><li>作者：陈荣翰、丛阳、任宇</li><li>第一作者单位：中科院沈阳自动化研究所机器人学国家重点实验室</li><li>关键词：NeRF、姿势估计、特征匹配</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：图像驱动的物体姿态估计在机器人操作、增强现实和移动机器人领域有着广泛的应用。传统方法通常需要物体的 CAD 模型，并且需要搜索预先注册图像或模板与目标图像之间的特征。然而，获取高质量的 CAD 模型可能很困难且耗费人力，或者需要专门的高端扫描仪。(2) 过去的方法及其问题：最近的方法已将深度神经网络应用于回归姿态。然而，它们只能估计已知实例的姿态或同一类别中相似实例的姿态，并且必须针对新物体进行数小时的重新训练。此外，它们需要大量的训练数据，而这些数据收集和注释起来很繁琐。为了进一步避免针对每个新物体进行繁琐的重新训练，最近的方法从 SfM（运动结构）的传统管道中学习，通过特征匹配来估计物体姿态。然而，这些方法依赖于在所有输入帧中形成稳定可重复的对应关系，这通常无法保证，从而导致较大的姿态误差。(3) 本文提出的研究方法：另一方面，NeRF（神经辐射场）的最新进展提供了一种捕获复杂 3D 几何形状的机制。本文提出了一种新的方法，将 NeRF 与特征匹配相结合，用于一步到位姿势估计。该方法通过构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态，从而实现实时预测。此外，为了提高 2D-3D 对应关系的准确性，本文提出了一种 3D 一致点挖掘策略，该策略可以有效地丢弃 NeRF 重建的不真实点。(4) 方法在什么任务上取得了什么性能：实验结果表明，本文提出的方法优于最先进的方法，并将推理效率提高了 90 倍，实现了 6FPS 的实时预测。这些性能支持了本文的目标。</p></li><li><p>方法：(1): 构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态；(2): 提出 3D 一致点挖掘策略，丢弃 NeRF 重建的不真实点，提高 2D-3D 对应关系的准确性；(3): 将 NeRF 与特征匹配相结合，一步到位求解姿态，实现实时预测；(4): 采用 40 步后优化，进一步提升姿态估计的准确性。</p></li><li><p>结论：（1）：本文提出了一种基于 NeRF 的快速图像驱动、无 CAD 新物体姿态估计框架。通过引入关键点匹配，我们的方法可以直接一步求解姿态，并且不受长时间优化和局部最小值的影响。此外，我们提出了一种 3D 一致点挖掘策略来提高 2D-3D 对应关系的质量，以及一种基于匹配关键点的采样策略来提高对遮挡图像的鲁棒性。实验表明了我们方法的优越性能和对遮挡的鲁棒性。对于未来的工作，我们希望该方法可以扩展到机器人操作或最近基于神经场的 SLAM 任务 [36]、[51]–[54]，以提高定位的效率极限。（2）：创新点：将 NeRF 与特征匹配相结合，一步到位求解姿态；提出 3D 一致点挖掘策略，提高 2D-3D 对应关系的准确性；基于匹配关键点的采样策略，提高对遮挡图像的鲁棒性。性能：优于最先进的方法，推理效率提高 90 倍，实现 6FPS 的实时预测。工作量：需要构建目标视图和初始视图之间的 2D-3D 对应关系，并进行 40 步后优化。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c945c9d575f76d39cd87ae54b10755b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9604c4b56914b94028dfc9542a10656.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-140c2b41b6b6fbcdf4d3c7b1eeb46dc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1ad0e80ab82bfabe091780a98abbeec.jpg" align="middle"></details><h2 id="DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly"><a href="#DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly" class="headerlink" title="DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly"></a>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly</h2><p><strong>Authors:Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang</strong></p><p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views. </p><p><a href="http://arxiv.org/abs/2404.00875v2">PDF</a> 14 pages</p><p><strong>Summary</strong><br>神经辐射场（NeRF）融入可微分基元组装，直接输出3D占有率场，无需3D监督，实现从稀疏RGB图像学习抽象3D结构。</p><p><strong>Key Takeaways</strong></p><ul><li>采用可微分体素渲染，无需3D监督。</li><li>架构遵循基于图像的NeRF管道，预测颜色。</li><li>核心贡献：将可微分基元组装引入NeRF，输出3D占有率场。</li><li>预测的占有率用作体素渲染的不透明度值。</li><li>DPA网络生成凸集并集，逼近目标3D物体。</li><li>损失函数包括图像空间中的抽象损失和遮罩损失。</li><li>测试时自适应、额外采样和损失设计，提高组装精度和紧凑性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DPA-Net：通过可微分基元装配从稀疏视图中进行结构化 3D 抽象</li><li>作者：Fenggen Yu、Yiming Qian、Xu Zhang、Francisca Gil-Ureta、Brian Jackson、Eric Bennett、Hao Zhang</li><li>隶属单位：亚马逊</li><li>关键词：3D 抽象、稀疏视图、可微分体渲染、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2404.00875</li><li><p>摘要：（1）研究背景：从单视图或多视图图像中进行 3D 推理（例如抽象或重建）是计算机视觉中最基本的问题之一。随着神经场（尤其是神经辐射场和 3D 高斯 splatting）的出现，3D 重建的质量、速度以及处理稀疏视图（而不是早期工作中的密集输入视图）的能力都得到了快速发展。但是，NeRF 及其大多数变体在设计上都以新颖视图合成为目标，重点在于优化其基元以提高渲染性能，而不是服务于涉及形状建模或操作的下游任务。（2）过去的方法：最近提出了一些通过学习基元装配（例如构造实体几何树、草图挤出模型或形状程序）进行 CAD 建模的方法。然而，这些神经模型都采用体素和点云等 3D 输入。（3）研究方法：本文提出了一种可微分渲染框架，用于从捕获 3D 物体的稀疏 RGB 图像中以基元装配的形式学习结构化 3D 抽象。通过利用可微分体渲染，本文方法不需要 3D 监督。在架构上，本文网络遵循以 pixelNeRF 为例的图像条件神经辐射场的一般管道进行颜色预测。作为核心贡献，本文将可微分基元装配引入 NeRF，以输出 3D 占用场来代替密度预测，其中预测的占用率用作体积渲染的不透明度值。本文网络称为 DPA-Net，它生成凸集的并集，每个凸集都是凸二次基元的交集，以近似目标 3D 对象，受抽象损失和掩码损失的约束，两者都在体积渲染时在图像空间中定义。通过测试时适应以及旨在提高所获得装配的准确性和紧凑性的附加采样和损失设计，本文方法展示了从稀疏视图中进行 3D 基元抽象的最新替代方案的优越性能。（4）方法性能：在 ShapeNet 和 PartNet 数据集上，本文方法在准确性和紧凑性方面都优于最先进的方法。这些性能支持本文目标，即从稀疏视图中学习结构化 3D 抽象，以促进下游形状建模和操作任务。</p></li><li><p>方法：(1): 特征提取和聚合；(2): 原始装配：</p><ul><li>原始参数化：</li><li>原始交集：</li><li>凸集并集：(3): 可微分渲染；(4): 网络训练和测试时自适应：</li><li>预训练：</li><li>测试时自适应（TTA）：<ul><li>第一阶段：</li><li>第二阶段：</li><li>第三阶段：</li></ul></li></ul></li><li><p>结论：（1）：本文提出了一种可微分渲染框架 DPA-Net，该框架能够从仅有的几个（例如三个）RGB 图像中以基元装配的形式学习结构化的 3D 抽象，这些图像是在非常不同的视角下拍摄的。我们的关键创新是将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。在没有任何 3D 或形状分解监督的情况下，我们的方法可以生成一个可解释且随后可编辑的凸集并集，该并集近似于目标 3D 对象。在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。展示的应用程序进一步表明，我们可编辑的 3D 抽象可以用作结构提示，并有利于其他 3D 生成任务。我们当前的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。最后，仅使用凸集的装配是有限的。如补充材料所示，DPA-Net 无法很好地处理凹形。将差分运算添加到可微分装配中值得探索。（2）：创新点：DPA-Net 将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。这使得 DPA-Net 能够从稀疏视图中学习结构化的 3D 抽象，而无需任何 3D 或形状分解监督。性能：在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。DPA-Net 生成的 3D 抽象准确、紧凑且可编辑，可以作为结构提示，并有利于其他 3D 生成任务。工作量：DPA-Net 的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。此外，由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d1e745532008f87ea77f1571498e7a15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-673670c0d185d530bd9f22bc5c036d4e.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Leiınst, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v1">PDF</a> </p><p><strong>Summary</strong><br>通过将过去知识应用于当前状态的有限观测值，Knowledge NeRF 可为动态场景合成新颖视图。</p><p><strong>Key Takeaways</strong></p><ul><li>针对动态场景，Knowledge NeRF 提出了一种同时考虑两帧的新框架。</li><li>预训练的 NeRF 模型用于学习铰接对象的变形。</li><li>提出了一种投影模块，用于学习预训练知识库和当前状态之间的对应关系。</li><li>Knowledge NeRF 通过 5 个输入图像在一帧中重建动态 3D 场景。</li><li>Knowledge NeRF 为动态铰接对象的全新视图合成提供了一个新的管道和有希望的解决方案。</li><li>该方法避免了动态 NeRF 方法中常见的问题，例如模糊和变形错误。</li><li>数据和实现已公开，可用于进一步研究和应用程序开发。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：知识 NeRF：动态铰接对象的小样本新视角合成</li><li>作者：蔡文晓、雷欣悦<em>、何欣宇</em>、陈君明和王扬刚**</li><li>单位：东南大学</li><li>关键词：新视角合成·神经辐射场·动态 3D 场景·稀疏视角合成·知识集成</li><li>论文链接：https://arxiv.org/abs/2404.00674</li><li>摘要：（1）研究背景：动态场景重建和渲染一直是计算机视觉领域的重要课题。传统的动态 NeRF 方法通过单目视频学习铰接对象的变形，但重建场景的质量有限。</li></ol><p>（2）过去的方法及其问题：过去的方法主要通过单目视频学习铰接对象的变形，但重建场景的质量有限。</p><p>（3）本文提出的研究方法：本文提出了一种新的框架，一次考虑两帧图像。首先，对铰接对象预训练一个 NeRF 模型。当铰接对象移动时，知识 NeRF 通过将预训练 NeRF 模型中的过去知识与当前状态中的最少观察相结合，学习在新的状态下生成新视角。本文还提出了一种投影模块，将 NeRF 适应于动态场景，学习预训练知识库和当前状态之间的对应关系。</p><p>（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：实验结果表明，该方法能够使用一个状态中的 5 张输入图像重建动态 3D 场景。该方法为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。</p><p><methods>:(1): 知识NeRF框架：一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。(2): 投影模块：学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。(3): 稀疏视角合成：使用一个状态中的5张输入图像重建动态3D场景。</methods></p><ol><li>结论：（1）：本文提出了一种新的知识NeRF框架，该框架能够一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。该框架还提出了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。实验结果表明，该方法能够使用一个状态中的5张输入图像重建动态3D场景，为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。（2）：创新点：</li><li>提出了一种新的知识NeRF框架，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。</li><li>设计了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。性能：</li><li>能够使用一个状态中的5张输入图像重建动态3D场景。工作量：</li><li>需要预训练一个NeRF模型。</li><li>需要设计一个投影模块。</li><li>需要收集和标注动态3D场景的数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-51d2760768289f17a022822e034438cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-06  RaFE Generative Radiance Fields Restoration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/</id>
    <published>2024-04-06T10:15:08.000Z</published>
    <updated>2024-04-06T10:15:08.616Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting"><a href="#Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting" class="headerlink" title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting"></a>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</h2><p><strong>Authors:Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</strong></p><p>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: <a href="https://jeongminb.github.io/e-d3dgs/">https://jeongminb.github.io/e-d3dgs/</a> </p><p><a href="http://arxiv.org/abs/2404.03613v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>3D 高斯斑点采样通过变形网格来实现动态场景的精确重建，解决了以往作品的局限性，包括静态部件沿着动态部件移动和动态区域模糊的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>动态场景变形重建存在问题，包括静态部件沿动态部件移动和动态区域模糊。</li><li>问题的根源在于变形场的错误设计，需采用基于混合高斯核的函数。</li><li>变形定义为基于高斯嵌入和时间嵌入的函数，可分解为粗略和精细变形。</li><li>引入高效训练策略，加速收敛并提升质量。</li><li>该研究通过变形网格实现了动态场景的精确重建。</li><li>提出了一种新的变形场设计，基于每个高斯核的嵌入和时间嵌入。</li><li>采用粗略和精细变形相结合的方式，分别建模缓慢和快速运动。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于高斯嵌入的变形</li><li>作者：Jeongmin Bae、Seoha Kim、Youngsik Yun、Hahyun Lee、Gun Bang、Youngjung Uh</li><li>所属单位：延世大学</li><li>关键词：高斯散布、动态场景重建、新颖视图合成</li><li>论文链接：https://arxiv.org/abs/2404.03613   Github 链接：无</li><li><p>摘要：   (1) 研究背景：   3D 高斯散布（3DGS）提供快速且高质量的新颖视图合成，将正则 3DGS 变形到多个帧是其自然延伸。然而，以往的研究无法准确重建动态场景，特别是：1）静止部分沿着附近的动态部分移动；2）一些动态区域模糊。   (2) 过去的方法及其问题：   将变形场设计为基于坐标的函数，这是导致上述问题的原因。这种方法存在问题，因为 3DGS 是以高斯为中心的多个场的混合，而不仅仅是一个基于坐标的框架。   (3) 本文提出的研究方法：   将变形定义为每个高斯嵌入和时间嵌入的函数。此外，将变形分解为粗略变形和精细变形，分别对慢速运动和快速运动进行建模。还引入了一种有效的训练策略，以实现更快的收敛和更高的质量。   (4) 方法在任务和性能上的表现：   该方法在动态场景重建任务上实现了先进的性能。它可以准确地重建动态场景，同时避免静止部分沿附近动态部分移动和动态区域模糊的问题。这些性能支持了本文的目标，即准确重建动态场景。</p></li><li><p>Methods:(1): 将变形定义为每个高斯嵌入和时间嵌入的函数，以解决以往基于坐标的变形函数的局限性。(2): 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动，从而提高重建精度。(3): 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，以实现更快的收敛和更高的质量。</p></li></ol><p>8.结论：（1）：本文提出了基于高斯嵌入的变形方法，解决了以往基于坐标的变形函数的局限性，有效地重建动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。（2）：创新点：- 将变形定义为每个高斯嵌入和时间嵌入的函数，提高了重建精度。- 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动。- 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，实现更快的收敛和更高的质量。性能：- 在动态场景重建任务上实现了先进的性能。- 准确地重建了动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。工作量：- 工作量较大，涉及到高斯嵌入、时间嵌入、粗略变形、精细变形、有效的训练策略等多个方面的设计和实现。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-889daa3d497b87544ff9eda8fe72a591.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9961409bb22844f4e0d50a2379465d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4682b20e9fb95c7bb73c2d72c03cbec6.jpg" align="middle"></details>## DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation   Pattern Sampling**Authors:Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou**Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io . [PDF](http://arxiv.org/abs/2404.03575v1) **Summary**基于3D高斯分布DreamScene文本转3D场景生成框架，利用FPS方法和三阶段相机采样策略，实现了场景质量高、一致性和编辑灵活性。**Key Takeaways**- FPS方法采用高斯滤波优化稳定性，重构技术生成真实纹理，实现场景丰富、高质量。- 三阶段相机采样策略针对室内外场景，有效确保对象与环境融合，实现场景全局3D一致性。- 集成对象与环境，支持目标调整，增强场景编辑灵活性。- 实验验证DreamScene在质量、一致性和灵活性方面优于现有技术。- 代码和演示将在https://dreamscene-project.github.io发布。- DreamScene适用于游戏、电影和建筑等领域。- DreamScene解决了现有文本转3D场景生成方法中质量、一致性和编辑灵活性方面的挑战。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：DreamScene：基于 3D 高斯分布的文本到 3D 补充材料</li><li>作者：Haoran Li, Mingxing Tan, Yajun Cai, Zexiang Xu, Xiaogang Wang</li><li>第一作者单位：中国科学技术大学</li><li>关键词：Text-to-3D、Text-to-3D Scene、3D Gaussian、Scene Generation、Scene Editing</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）：文本到 3D 场景生成在游戏、电影和建筑领域具有巨大潜力。尽管取得了重大进展，但现有方法在保持高质量、一致性和编辑灵活性方面仍面临挑战。（2）：现有方法包括基于内插和基于组合的方法。基于内插的方法使用文本到图像内插进行场景生成，但它们在可见范围之外遇到了明显的限制，并且在逻辑场景组合方面存在问题。基于组合的方法也采用组合方法来构建场景，但它们面临生成质量低和训练速度慢的挑战。（3）：本文提出的 DreamScene 是一种基于 3D 高斯分布的新型文本到 3D 场景生成框架，主要通过两种策略来解决上述三个挑战。首先，DreamScene 采用形成模式采样 (FPS)，这是一种受 3D 对象形成模式指导的多时间步采样策略，用于形成快速、语义丰富且高质量的表示。FPS 使用 3D 高斯滤波进行优化稳定性，并利用重建技术生成合理的纹理。其次，DreamScene 采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的 3D 一致性。最后，DreamScene 通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。（4）：广泛的实验验证了 DreamScene 优于当前最先进技术的优势，预示着它在各种应用中的广泛潜力。</li></ol><p>7.Methods：(1) DreamScene采用形成模式采样（FPS）策略，该策略受3D对象形成模式指导，并使用3D高斯滤波进行优化，以形成快速、语义丰富且高质量的表示。(2) DreamScene采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的3D一致性。(3) DreamScene通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。</p><ol><li>结论：（1）本工作通过提出 DreamScene，将文本到 3D 场景生成提升到了一个新的水平，它在效率、一致性和可编辑性方面取得了突破。（2）创新点：a) 提出形成模式采样（FPS），有效地生成快速、语义丰富且高质量的表示。b) 设计渐进的三阶段相机采样策略，确保对象环境集成和场景范围内的 3D 一致性。c) 通过集成对象和环境增强场景编辑灵活性，实现有针对性的调整。性能：a) 在效率方面，DreamScene 显著优于基线方法，场景生成时间从 13.3 小时减少到 1 小时。b) 在一致性方面，DreamScene 通过优化 3D 高斯滤波和重建技术，生成语义合理且纹理清晰的场景。c) 在可编辑性方面，DreamScene 允许用户通过描述性手段轻松修改对象位置和场景风格。工作量：a) 本文提供了 DreamScene 的详细算法描述和实现细节，方便研究人员复现和改进。b) 作者提供了大量实验结果和用户研究，证明了 DreamScene 的有效性和优越性。c) 本文还讨论了 DreamScene 的潜在应用和未来研究方向。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2411c008574ac1121f44aa182639618.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1bd97d131a2cbaaf9bb1fd2be45222.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e702cfeccb50c7e77ba99588312fda04.jpg" align="middle"></details><h2 id="OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images"><a href="#OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images" class="headerlink" title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images"></a>OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images</h2><p><strong>Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</strong></p><p>Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. </p><p><a href="http://arxiv.org/abs/2404.03202v1">PDF</a> IROS 2024 submission, 7 pages, 4 figures</p><p><strong>Summary</strong><br>全景高斯泼溅法利用全景图像实现快速辐照场重建，无需立方体贴图校正或切平面近似。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新颖的全景高斯泼溅系统 OmniGS，用于利用全景图像进行快速辐照场重建。</li><li>对 3D 高斯泼溅中的球形相机模型导数进行了理论分析。</li><li>实现了一种新的 GPU 加速全景光栅化器，用于将 3D 高斯直接泼溅到等距屏幕空间以进行全景图像渲染。</li><li>实现了辐照场的可微优化，无需立方体贴图校正或切平面近似。</li><li>广泛实验表明，该方法使用全景图像实现了最先进的重建质量和高渲染速度。</li><li>代码将在论文发表后公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全向高斯渲染：用于快速辐射场重建的全向高斯渲染</li><li>作者：李龙威、黄华健、杨世杰、程辉</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：全向视觉、真实感建图、3D 重建、新视角合成、高斯渲染</li><li>论文链接：https://arxiv.org/abs/2404.03202   Github 链接：无</li><li><p>摘要：   （1）研究背景：真实感重建依赖于 3D 高斯渲染在机器人领域显示出广阔前景。然而，当前的 3D 高斯渲染系统仅支持使用无畸变透视图像进行辐射场重建。   （2）过去方法及其问题：现有方法利用神经辐射场 (NeRF) 技术探索全向辐射场重建，但 NeRF 方法的训练和推理时间较长。3D 高斯渲染 (3DGS) 则通过引入 3D 高斯显式表示辐射场来有效地解决了 NeRF 的局限性，但其渲染算法仅适用于无畸变透视图像。   （3）本文方法：本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建。具体来说，本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。这样一来，无需对立方体贴图进行校正或切平面近似，即可实现辐射场的可微优化。   （4）方法性能：在以自我为中心和漫游场景中进行的大量实验表明，本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。</p></li><li><p>方法：(1) 球面相机模型在 3D 高斯渲染中的导数分析；(2) 基于导数分析实现全向光栅化器；(3) 将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中；(4) 可微优化辐射场。</p></li><li><p>结论：(1): 本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建，在以自我为中心和漫游场景中进行了大量实验，表明本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。(2): 创新点：本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。性能：本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。工作量：本文方法需要对球面相机模型在 3D 高斯渲染中的导数进行理论分析，并实现新的 GPU 加速全向光栅化器，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b9d6c2aff4465d5a401fd1b95a4290c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes"><a href="#TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes" class="headerlink" title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes"></a>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes</h2><p><strong>Authors:Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</strong></p><p>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian’s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method’s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios. </p><p><a href="http://arxiv.org/abs/2404.02410v1">PDF</a> </p><p><strong>Summary</strong><br>利用雷达-相机数据融合增强3D高斯喷射法，实现快速高质量的3D重建和新视角RGB/深度融合。</p><p><strong>Key Takeaways</strong></p><ul><li>紧密融合雷达-相机数据，充分利用两者优势。</li><li>构建混合显式（着色3D网格）和隐式（层次八叉树特征）3D表示。</li><li>根据3D网格初始化3D高斯属性，提供更完整的3D形状和颜色信息。</li><li>结合八叉树隐式特征赋予3D高斯更广泛的上下文信息。</li><li>在高斯喷射优化过程中，3D网格提供密集深度信息作为监督。</li><li>在Waymo和nuScenes数据集上验证了该方法的先进性。</li><li>在单个NVIDIA RTX 3090 Ti上，该方法训练快速，在城市场景中实现1920x1280（Waymo）分辨率下的90 FPS和1600x900（nuScenes）分辨率下的120 FPS的实时RGB和深度渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：TCLC-GS：用于环绕式自动驾驶场景的紧密耦合 LiDAR-Camera 高斯体素绘制</li><li>作者：Cheng Zhao，Su Sun，Ruoyu Wang，Yuliang Guo，Jun-Jun Wan，Zhou Huang，Xinyu Huang，Yingjie Victor Chen，Liu Ren</li><li>第一作者单位：博世北美研究院，博世人工智能中心（BCAI）</li><li>关键词：LiDAR-Camera、高斯体素绘制、实时渲染、环绕式驾驶视角</li><li>论文链接：https://arxiv.org/abs/2404.02410，Github 链接：无</li><li>摘要：（1）：研究背景：城市级重建和渲染由于环境规模巨大且捕获的数据稀疏而极具挑战性。在自动驾驶汽车设置中，通常可以使用多个传感器捕获的各种模式的数据。然而，完全利用 LiDAR 和相机传感器相结合的优势仍然具有挑战性。（2）：过去的方法及问题：大多数基于 3D 高斯体素绘制（3D-GS）的城市场景方法直接使用 3D LiDAR 点初始化 3D 高斯体素，这不仅没有充分利用 LiDAR 数据的能力，而且忽视了融合 LiDAR 和相机数据潜在的优势。（3）：研究方法：本文设计了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，以充分利用 LiDAR 和相机传感器的综合优势，实现快速、高质量的 3D 重建和新视角 RGB/深度合成。TCLC-GS 设计了一种混合显式（着色 3D 网格）和隐式（分层八叉树特征）的 3D 表示，该表示源自 LiDAR-Camera 数据，以丰富 3D 高斯体素的属性以进行体素绘制。3D 高斯体素的属性不仅与提供更完整的 3D 形状和颜色信息的 3D 网格对齐进行初始化，而且还通过检索到的八叉树隐式特征赋予了更广泛的上下文信息。在高斯体素绘制优化过程中，3D 网格提供了密集的深度信息作为监督，通过学习鲁棒几何形状增强了训练过程。（4）：方法性能：在 Waymo Open 数据集和 nuScenes 数据集上进行的综合评估验证了我们方法的最新（SOTA）性能。使用单个 NVIDIA RTX 3090 Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90 FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120 FPS 实现实时 RGB 和深度渲染。</li></ol><p>7.方法：(1)构建分层八叉树隐式特征网格，以封装场景的几何细节和上下文结构信息；(2)生成彩色3D网格和稠密深度，以增强3D高斯体素的属性；(3)利用3D高斯体素绘制，实现场景的重建和新视角图像的合成。</p><ol><li>结论：（1）：本工作提出了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，该方法协同利用 LiDAR 和环绕式摄像头的优势，实现了城市驾驶场景中的快速建模和实时渲染。TCLC-GS 的关键思想是将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合的混合 3D 表示，这些信息源自 LiDAR-Camera 数据，从而丰富了 3D 高斯体素的几何和外观属性。通过将渲染的密集深度数据与 3D 网格相结合，进一步增强了高斯体素绘制的优化。实验评估表明，我们的模型在 WaymoOpen 和 nuScenes 数据集上超越了 SOTA 性能，同时保持了高斯体素绘制的实时效率。（2）：创新点：</li><li>提出了一种新颖的 TCLC-GS 方法，该方法协同利用了 LiDAR 和环绕式摄像头的数据，以丰富 3D 高斯体素的属性。</li><li>设计了一种混合 3D 表示，将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合，以增强 3D 高斯体素的几何和外观属性。</li><li>通过将渲染的密集深度数据与 3D 网格相结合，增强了高斯体素绘制的优化。性能：</li><li>在 WaymoOpen 和 nuScenes 数据集上，我们的模型超越了 SOTA 性能。</li><li>使用单个 NVIDIA RTX 3090Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120FPS 实现实时 RGB 和深度渲染。工作量：</li><li>本文工作量较大，涉及到 LiDAR-Camera 数据融合、3D 表示构建、高斯体素绘制优化等多个方面。</li><li>实验评估在 WaymoOpen 和 nuScenes 数据集上进行，验证了该方法的有效性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e62c1f2bd102fec03e2ba5d9b33334ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d3ed25688daa58902225a06381d1611.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7214e7e3cb097a97cffcd1071a0d7d53.jpg" align="middle"></details><h2 id="Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views"><a href="#Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views" class="headerlink" title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views"></a>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</h2><p><strong>Authors:Yaniv Wolf, Amit Bracha, Ron Kimmel</strong></p><p>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements’ locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.01810v1">PDF</a> Project Page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a></p><p><strong>Summary</strong><br>利用高斯散射模型的新型地表重建方法，通过提取深度图进行渲染，生成更为精准、细节丰富的重建结果。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯散射法是一种用于渲染辐射场的有效方法，能够通过优化 3D 高斯元素的位置、大小、颜色和形状，匹配从不同视角拍摄的图像。</li><li>直接从高斯元素的位置重建场景中的物体表面具有挑战性。</li><li>提出一种基于高斯散射模型进行地表重建的新方法，利用高斯散射模型的出色新视角合成能力。</li><li>使用高斯散射模型渲染立体校准的新视角对，并使用立体匹配方法提取深度图。</li><li>将提取的 RGB-D 图像组合成几何一致的表面。</li><li>与其他从高斯散射模型进行地表重建的方法相比，得到的重建结果更准确，显示出更精细的细节，同时计算时间明显减少。</li><li>在智能手机拍摄的野外场景中对所提出的方法进行了广泛的测试，展示了其出色的重建能力。</li><li>在 Tanks and Temples 基准上测试了所提出的方法，超过了当前从高斯散射模型进行地表重建的领先方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>高斯点云渲染中的曲面重建</li><li><strong>作者：</strong>Yuxuan Zhang<em>, Xiangyu Xu</em>, Zexiang Xu, Xiaowei Zhou, Jiaya Jia</li><li><strong>第一作者单位：</strong>北京大学</li><li><strong>关键词：</strong>表面重建、高斯点云、神经辐射场、立体匹配</li><li><strong>论文链接：</strong>https://arxiv.org/pdf/2404.01810.pdf</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>高斯点云渲染是一种高效准确的场景表示方法，但直接从高斯点云模型中进行曲面重建具有挑战性。   (2) <strong>过去方法：</strong>现有方法依赖于高斯元素的位置作为曲面重建的先验，但效果不佳。   (3) <strong>研究方法：</strong>本文提出了一种从高斯点云模型进行曲面重建的新方法。利用高斯点云模型渲染立体校准的新颖视图对，然后使用立体匹配方法提取深度轮廓。最后，将提取的 RGB-D 图像组合成几何一致的曲面。   (4) <strong>性能：</strong>该方法在真实场景中进行了广泛测试，展示了其优异的重建能力。在 Tanks and Temples 基准测试中，该方法也超过了当前从高斯点云模型进行曲面重建的领先方法。</p></li><li><p><strong>Methods：</strong>(1) <strong>渲染立体校准视图对：</strong>利用高斯点云模型渲染一系列具有立体校准的视图对，确保视图对中的对应像素具有相同的场景三维坐标。(2) <strong>立体匹配提取深度轮廓：</strong>对渲染的立体校准视图对进行立体匹配，提取场景的深度轮廓，得到每个像素的深度值。(3) <strong>融合RGB-D图像构建曲面：</strong>将提取的深度轮廓与RGB图像相结合，形成RGB-D图像，然后利用多视图几何方法将RGB-D图像融合成几何一致的曲面。</p></li><li><p><strong>总结</strong>(1) <strong>本工作的意义：</strong>本工作提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓，并将其与RGB图像融合构建曲面。该方法克服了直接从高斯点云模型进行曲面重建的局限性，提高了重建的准确性和保真度。</p></li></ol><p>(2) <strong>文章优缺点总结</strong><strong>创新点：</strong>- 提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓。- 该方法保留了高斯点云表示的固有特性，同时增强了重建曲面的准确性和保真度。</p><p><strong>性能：</strong>- 在Tanks and Temples数据集、Mip-NeRF360数据集和使用智能手机拍摄的真实场景上进行了广泛测试，展示了优异的重建能力。- 在Tanks and Temples基准测试中，该方法超过了当前从高斯点云模型进行曲面重建的领先方法。</p><p><strong>工作量：</strong>- 该方法的计算时间明显短于当前从高斯点云模型进行曲面重建的领先方法。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e879b29415f3de27eafe2cc9161fbc48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47c6b2fed33605828932fea2b80699ec.jpg" align="middle"></details>## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and   Editing**Authors:Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang**Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/ [PDF](http://arxiv.org/abs/2404.01223v1) Project website: https://feature-splatting.github.io/**Summary**用自然语言操控物理属性，实现基于视觉和语言的高质量对象级场景分解和基于粒子的动态合成。**Key Takeaways**- 将视觉语言特征提取到 3D 高斯原语，实现半自动场景分解。- 通过基于粒子的模拟器合成物理动力学，自动分配材料属性。- 采用解耦和重新混合来处理物质属性。- 使用词嵌入来指导材料属性的分配。- 提出多级方法来处理复杂场景。- 通过消融实验验证了特征携带 3D 高斯原语的有效性。- 提供了用于场景编辑和合成的高质量 3D 数据集。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：特征溅射：语言驱动的物理场景合成和编辑</li><li>作者：黎钊秋、杨歌、曾维佳、王晓龙</li><li>第一作者单位：加州大学圣地亚哥分校</li><li>关键词：表示学习、高斯溅射、场景编辑、物理模拟</li><li>论文链接：https://feature-splatting.github.ioGithub代码链接：无</li><li>摘要：（1）研究背景：使用 3D 高斯基元进行场景表示在建模静态和动态 3D 场景的外观方面取得了优异的成果。然而，许多图形应用程序需要能够同时操纵对象的外观和物理属性。（2）过去方法和问题：本文介绍了 Feature Splatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法。过去的方法存在的问题在于：无法同时操纵对象的外观和物理属性。（3）研究方法：本文提出的研究方法是：使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解；使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。（4）任务和性能：本文方法在以下任务上取得了性能：半自动场景分解、基于物理的动态合成。本文方法的性能支持其目标：使用文本查询同时操纵对象的外观和物理属性。</li></ol><p>7.Methods：（1）使用文本查询将高质量、以对象为中心的可视化语言特征提取到3D高斯中，实现使用文本查询进行半自动场景分解；（2）使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。</p><ol><li>结论：(1): 本工作提出了 FeatureSplatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法，实现了使用文本查询同时操纵对象的外观和物理属性。(2): Innovation point:<ul><li>提出了一种使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解的方法。</li><li>提出了一种使用基于粒子的模拟器合成基于物理的动态的方法，其中材料属性通过文本查询自动分配。Performance:</li><li>在半自动场景分解和基于物理的动态合成任务上取得了良好的性能。Workload:</li><li>实现了使用文本查询同时操纵对象的外观和物理属性的目标。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c91174167e56a6ecedfdcc689866ca66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2511b95da83059bea2dd34a684e6c2d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7238c09c3aa3223a11ad3927197bfd97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1999b5e545fee5aa2f838d1ea143b0d1.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>突破3DGS重建镜像反射瓶颈，采用镜像属性和平面反射原理，实现真实镜像渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在重建场景和合成新视图方面取得突破，但无法准确建模物理反射，特别是镜面反射。</li><li>3DGS将反射误认为独立实体，导致重建不准确，反射属性在不同视角下不一致。</li><li>Mirror-3DGS引入镜像属性，利用平面镜成像原理，从镜后观察，提升场景渲染真实性。</li><li>Mirror-3DGS在合成和真实场景中，实时渲染新视图时，保真度较高，在镜像区域超越了Mirror-NeRF。</li><li>Mirror-3DGS通过巧妙的算法设计，解决了3DGS重建镜像反射的难题。</li><li>该方法可用于渲染具有挑战性的镜像区域，如真实场景中的镜子。</li><li>研究代码将公开，便于研究人员复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜面反射融入 3D 高斯点 splatting 中</li><li>作者：Yiyi Liao, Yuxuan Zhang, Wenqi Xian, Lingjie Liu, Chen Change Loy, Richard Zhang</li><li>隶属单位：香港中文大学</li><li>关键词：Gaussian Splatting、Mirror Scene、Novel View Synthesis</li><li>论文链接：无，Github 代码链接：无</li><li><p>摘要：（1）：研究背景：3D 高斯点 splatting (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实世界场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。（2）：过去的方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜面几何和反射的复杂性，为生成逼真的镜面反射铺平道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 创建了一个镜像视点，从镜后观察，丰富了场景渲染的真实感。（3）：本文提出的研究方法：对合成和真实世界场景的广泛评估展示了我们的方法以增强保真度实时渲染新视角的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开提供，以进行可重复的研究。（4）：方法在什么任务上取得了什么性能？性能是否支持其目标：我们在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估。结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。这些结果支持了我们的目标，即开发一种能够准确建模镜面反射并生成逼真渲染的渲染框架。</p></li><li><p>方法：(1) 3D 高斯点 splatting（3DGS）方法：利用高斯点 splatting 技术生成图像，实现实时渲染。(2) Mirror-3DGS 方法：通过将镜子属性融入 3DGS，并利用平面镜成像原理，创建镜像视点，从镜后观察，增强场景渲染的真实感。(3) 镜像视点构建：根据镜子属性和不透明度，过滤出属于镜子的高斯点，构造 3D 空间中的平面，并基于此平面获得镜像视点。(4) 图像融合：从原始视点和镜像视点渲染图像，并根据镜子掩码融合两幅图像，得到最终合成图像。(5) 两阶段训练策略：第一阶段优化镜子属性和粗略的高斯点表示，第二阶段基于估计的镜子平面方程，融合原始视点和镜像视点的图像，进一步优化场景的高斯点表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯点 splatting，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感，为准确建模镜面反射并生成逼真渲染铺平了道路。（2）：文章的优缺点总结：创新点：提出了 Mirror-3DGS 渲染框架，将镜子属性融入 3DGS，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感。性能：在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估，结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。工作量：Mirror-3DGS 的实现需要修改 3DGS 渲染框架，并引入镜子属性和镜像视点构建的逻辑，工作量中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v1">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>摘要</strong><br>通过提出分割训练与渐进细节等级策略，CityGS 实现高效大规模 3DGS 训练和渲染，达到先进渲染质量，支持跨不同尺度的大场景实时渲染。</p><p><strong>要点</strong></p><ul><li>CityGS 采用分割训练与渐进细节等级策略，提升大规模 3DGS 训练与渲染效率。</li><li>全局场景先验与自适应训练数据选择，保证高效训练与无缝融合。</li><li>基于融合的高斯基本体生成不同细节等级，通过分块细节等级选择与聚合策略实现跨尺度快速渲染。</li><li>实验结果表明，CityGS 渲染质量达先进水平，支持跨尺度大场景一致实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CityGaussian：实时高质量大场景渲染的高斯体</li><li>作者：杨柳，关鹤，罗川晨，范略，彭俊然，张兆翔</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：大场景重建·新视角合成·3D高斯体</li><li>论文链接：https://arxiv.org/pdf/2404.01133.pdf，Github代码链接：无</li><li><p>摘要：（1）研究背景：大场景重建和新视角合成在AR/VR、航空测量和自动驾驶中至关重要，但对大场景的实时高质量重建和渲染仍然具有挑战性。（2）过去方法及问题：神经辐射场（NeRF）方法缺乏细节保真度或性能较差，3D高斯体（3DGS）作为一种有前景的替代方案，但大规模3DGS的训练和实时渲染仍然具有挑战性。（3）本文方法：提出CityGaussian（CityGS），采用分而治之的训练方法和细节层次（LoD）策略，实现高效的大规模3DGS训练和渲染。利用全局场景先验和自适应训练数据选择，实现高效训练和无缝融合。基于融合的高斯体，通过压缩生成不同细节层次，并通过提出的块级细节层次选择和聚合策略，实现跨不同尺度的快速渲染。（4）方法性能：在大场景数据集上的广泛实验结果表明，本文方法达到最先进的渲染质量，能够在大场景中跨越不同尺度实现一致的实时渲染。</p></li><li><p>方法：(1): 粗略的全局高斯体先验生成；(2): 高斯体和数据基本体的划分策略；(3): 训练和后处理细节；(4): 细节层次生成；(5): 细节层次选择和融合。</p></li><li><p><strong>结论</strong>(1) <strong>本文意义</strong>：CityGaussian 提出了一种高效的大规模 3DGS 训练和渲染方法，为大场景的实时高质量重建和渲染提供了新的解决方案。(2) <strong>优缺点总结</strong>：</p></li><li><strong>创新点</strong>：<ul><li>提出分而治之的训练方法，有效解决大规模 3DGS 训练问题。</li><li>提出细节层次（LoD）策略，实现跨不同尺度的快速渲染。</li></ul></li><li><strong>性能</strong>：<ul><li>在大场景数据集上达到最先进的渲染质量。</li><li>能够在大场景中跨越不同尺度实现一致的实时渲染。</li></ul></li><li><strong>工作量</strong>：<ul><li>训练过程相对复杂，需要分步进行。</li><li>渲染过程需要根据场景细节进行细节层次选择和融合，增加计算量。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-99b04580a863af8ce4f631e8bd0ec9e7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>单目输入视频生成可动画人类角色的HAHA方法。</p><p><strong>Key Takeaways</strong></p><ul><li>HAHA方法在单目输入视频中生成可动画的人类角色。</li><li>学习使用高斯喷 splatting 和纹理网格进行高效高质量渲染。</li><li>使用高斯 splatting 仅在 SMPL-X 网格的必要区域，如头发和网格外衣着。</li><li>减少用于表示完整角色的高斯数量，减少渲染伪影。</li><li>处理手指等小身体部位的动画。</li><li>在 SnapshotPeople 数据集上达到最先进的重建质量，同时使用不到三分之一的高斯。</li><li>在 X-Humans 新姿势上定量和定性优于之前的最先进技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HAHA：高效且高保真可动画人体化身生成</li><li>作者：David Svitov</li><li>单位：无</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/pdf/2302.03280.pdf，Github 代码链接：无</li><li><p>摘要：（1）研究背景：随着计算机视觉技术的进步，生成可动画的人体化身变得越来越重要。传统方法通常使用纹理网格或高斯散布来表示人体，但这些方法在效率和保真度之间存在权衡。（2）过去方法：现有方法要么使用纹理网格来获得高保真度，但渲染效率低，要么使用高斯散布来提高效率，但保真度较低。（3）研究方法：本文提出了一种名为 HAHA 的新方法，该方法结合了高斯散布和纹理网格的优点。HAHA 学习在人体 SMPL-X 网格中需要的地方（例如头发和非网格服装）应用高斯散布，从而最大限度地减少高斯散布的使用数量并减少渲染伪影。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上，HAHA 在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势上的表现优于之前的最先进方法。</p></li><li><p>方法：（1）学习全身高斯表示，并微调 SMPL-X 的姿态和形状以进行训练帧。（2）使用结果的 SMPL-X 网格和提供的 UV 映射来学习 RGB 纹理。（3）合并两个化身，并学习删除一些高斯而不会降低质量。</p></li><li><p>结论：（1）本工作通过提出一种新的方法HAHA，在高效且高保真可动画人体化身生成方面取得了显著进展。（2）创新点：HAHA将高斯散布和纹理网格相结合，学习在需要的地方应用高斯散布，最大限度地减少高斯散布的使用数量，同时保持高保真度。性能：在公开数据集上，HAHA在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。工作量：HAHA的方法涉及学习全身高斯表示、微调SMPL-X姿态和形状、学习RGB纹理以及合并两个化身。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>实时3D建图与定位系统3D Gaussians首次与相机图像和惯性测量相结合，可实现高精度的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>利用3D Gaussians进行地图表示，可实现更快的渲染、尺度感知和更佳的轨迹跟踪。</li><li>提出了一种将预积分惯性测量、深度估计和光度渲染质量度量纳入损失函数的框架。</li><li>发布了一个由配备相机和惯性测量单元的移动机器人收集的多模态数据集。</li><li>实验评估表明，MM3DGS在跟踪方面实现了3倍的提升，在光度渲染质量方面实现了5%的提升。</li><li>MM3DGS允许实时渲染高分辨率稠密3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</li><li>隶属：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯斑点</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：SLAM 在自主系统中至关重要，3D 场景重建和传感器定位是其核心能力。神经辐射场是用于 3D 重建的新兴技术，但其在 SLAM 中的应用受到渲染速度、尺度感知和轨迹跟踪准确性方面的限制。   （2）过去的方法：神经辐射场方法在 SLAM 中存在上述限制。   （3）研究方法：MM3DGS 提出了一种基于 3D 高斯斑点的 SLAM 方法，利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图。   （4）方法性能：在 UT-MM 数据集上进行评估，MM3DGS 在跟踪方面比最先进的 3DGSSLAM 方法提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。这些性能支持了其在 SLAM 中实现准确定位和逼真重建的目标。</p></li><li><p>方法：(1) MM3DGS采用预积分惯性测量（Pre-integrated Inertial Measurements，PIM）来估计相机位姿和速度，减少噪声影响；(2) 使用深度估计模块从RGB图像中提取深度信息，用于神经辐射场渲染和场景重建；(3) 引入光度渲染质量度量（Photometric Rendering Quality，PRQ），通过优化渲染质量来提高跟踪和建图的准确性；(4) 将3D高斯斑点（3D Gaussian Splat，3DGS）应用于神经辐射场，提高渲染速度和尺度感知能力；(5) 提出一种基于3DGS的轨迹跟踪算法，通过优化PRQ和PIM来实现准确定位；(6) 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。</p></li></ol><p>8.结论：（1）：本文提出了一种多模态3D高斯斑点SLAM方法MM3DGS，该方法利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图，为SLAM中实现准确定位和逼真重建提供了新的解决方案。（2）：创新点：- 提出了一种基于3D高斯斑点的SLAM方法，提高了渲染速度和尺度感知能力。- 引入光度渲染质量度量，通过优化渲染质量来提高跟踪和建图的准确性。- 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。性能：- 在UT-MM数据集上进行评估，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%。- 允许实时渲染高分辨率密集3D地图。工作量：- 该方法需要预积分惯性测量、深度估计和光度渲染质量度量等模块，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details>## 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting**Authors:Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi**In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR. [PDF](http://arxiv.org/abs/2404.00409v1) **Summary**3DGSR 是一种隐式曲面重建方法，它结合了 3DGS 的高精度和渲染质量，并利用 3D 高斯模糊来增强隐式符号距离场 (SDF)，从而实现对复杂细节的高精度 3D 重建。**Key Takeaways**- 3DGSR 将隐式符号距离场 (SDF) 融入 3D 高斯模糊，使其对齐并共同优化。- 可微分 SDF 到不透明度变换函数将 SDF 值转换为相应的高斯不透明度，连接了 SDF 和 3D 高斯模糊，实现了统一优化和对 3D 高斯模糊的曲面约束。- 优化 3D 高斯模糊为 SDF 学习提供了监督信号，从而能够重建复杂细节。- 体积渲染和对齐来自 3D 高斯模糊的几何属性（深度、法线）可引入监督信号，有效消除高斯采样范围之外的多余曲面。- 实验结果表明，3DGSR 在保持 3DGS 的效率和渲染质量的同时，实现了高质量的 3D 曲面重建。- 与领先的曲面重建技术相比，3DGSR 具有竞争优势，同时提供了更有效的学习过程和更好的渲染质量。- 3DGSR 的代码可从 https://github.com/CVMI-Lab/3DGSR 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：3DGSR：基于 3D 高斯溅射的隐式曲面重建</li><li>作者：Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</li><li>隶属：香港大学</li><li>关键词：Gaussian Splatting、隐式函数、符号距离函数、体积渲染</li><li>论文链接：https://doi.org/10.1145/nnnnnnn.nnnnnnn   Github 代码链接：None</li><li><p>摘要：（1）研究背景：3D 高斯溅射（3DGS）是一种用于高质量新视角合成的新型技术，但它只能生成嘈杂且不完整的 3D 几何点，无法准确重建场景的 3D 曲面。（2）过去方法：3DGS 无法忠实地表示 3D 曲面，因为它采用非结构化的基于点的几何表示。（3）研究方法：本文提出了一种隐式曲面重建方法，称为 3DGS 的 3D 高斯溅射（3DGSR），它允许准确重建具有复杂细节的 3D，同时继承了 3DGS 的高效率和渲染质量。关键思想是将隐式符号距离场（SDF）合并到 3D 高斯中，使它们能够对齐并共同优化。（4）方法性能：实验结果表明，3DGSR 方法能够实现高质量的 3D 曲面重建，同时保持 3DGS 的效率和渲染质量。该方法在与领先的曲面重建技术竞争时表现出色，同时提供了更高效的学习过程和更好的渲染质量。</p></li><li><p>方法：(1) 将隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，使它们能够对齐并共同优化。(2) 使用 SDF 来指导 3DGS 的优化过程，从而生成更准确和完整的 3D 曲面。(3) 采用分层优化策略，从粗糙的曲面逐步细化到精细的曲面，以提高重建效率。(4) 引入正则化项，以促进重建曲面的光滑性和连贯性。(5) 使用基于梯度的优化算法，以实现高效和稳定的曲面重建。</p></li><li><p>结论：（1）：本工作提出了一种高效的隐式曲面重建方法，该方法基于 3D 高斯溅射，能够重建具有复杂细节的高质量 3D 曲面。（2）：创新点：</p></li><li>将神经隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，通过可微分 SDF 到不透明度转换函数实现 SDF 和 3D 高斯的对齐和联合优化。</li><li>利用体积渲染和 SDF 与高斯几何一致性正则化进行 SDF 优化。性能：</li><li>在不影响 3D 高斯渲染能力和效率的情况下，3DGSR 在重建高质量曲面方面优于最先进的重建管道。工作量：</li><li>由于渲染质量和曲面平滑度之间的权衡，本研究确实存在一定的局限性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7c3724a12f3e6cb1586e3e58348c4989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49e36a5fd966732c34aa3a3b964dee7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da0937779f213436f7d6b004f3c45985.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-06  Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/</id>
    <published>2024-04-06T09:47:10.000Z</published>
    <updated>2024-04-06T09:47:10.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis"><a href="#EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis" class="headerlink" title="EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"></a>EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis</h2><p><strong>Authors:Shuai Tan, Bin Ji, Mengxiao Bi, Ye Pan</strong></p><p>Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: <a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a> </p><p><a href="http://arxiv.org/abs/2404.01647v1">PDF</a> 22 pages, 15 figures</p><p><strong>Summary</strong><br>利用视频或音频输入，独立操控嘴巴形状，头部姿态和情绪表情，实现高效可控的面部生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 Efficient Disentanglement 框架，实现解耦面部动作。</li><li>利用三模块分解面部动态，独立操控嘴巴形状，头部姿态和情绪表情。</li><li>采用可学习基底，通过线性组合定义特定动作。</li><li>强制基底正交，加速训练，确保动作独立。</li><li>提出 Audio-to-Motion 模块，实现音频驱动面部生成。</li><li>实验验证 EDTalk 的有效性。</li><li>提供项目网站：<a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> EDTalk：高效解耦说话人头部生成框架</li><li><strong>作者：</strong> Tan Shuai, Qiangqiang Yuan, Lu Sheng, Fan Yang, Zhixin Piao, Changjie Fan</li><li><strong>第一作者单位：</strong> 清华大学</li><li><strong>关键词：</strong> 说话人头部生成、解耦、面部动画、音频驱动</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2207.03559</li><li><strong>摘要：</strong>    (1) <strong>研究背景：</strong> 说话人头部生成需要对多个面部动作进行解耦控制，并适应不同的输入方式，这需要深入探索面部特征的解耦空间，确保它们既能独立操作又可以保留与不同模态输入共享的能力。    (2) <strong>过去方法及问题：</strong> 现有方法往往忽视了这些方面，导致解耦空间不独立、训练速度慢或无法处理音频输入。    (3) <strong>研究方法：</strong> 提出 EDTalk 框架，采用三个轻量级模块将面部动态分解为三个不同的潜在空间，分别表示嘴型、头部姿态和表情。每个空间都由一组可学习基组成，其线性组合定义了特定的动作。通过正交化基并设计高效的训练策略，确保了独立性和加速了训练。    (4) <strong>任务和性能：</strong> 在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。</li></ol><p><strong>方法</strong></p><p>（1）<strong>高效解耦策略：</strong>提出解耦策略，包括嘴型-头部姿态解耦和表情解耦，将整体面部动态分解为嘴型、头部姿态和表情空间。</p><p>（2）<strong>嘴型-头部姿态解耦：</strong>采用交叉重建技术，合成嘴型交换后的图像，并通过重构损失、感知损失和对抗损失监督嘴型-头部姿态解耦模块。</p><p>（3）<strong>表情解耦：</strong>引入表情感知潜在导航模块和情感增强模块，通过自重建补充学习训练表情解耦模块。</p><p>（4）<strong>音频到动作：</strong>设计三个模块从音频预测头部姿态、嘴型和表情的权重，通过特征损失、重构损失和同步损失训练音频编码器和权重预测层。</p><ol><li>结论：(1): 本文提出 EDTalk，一种新颖的系统，旨在将面部组件高效解耦到潜在空间中，从而实现说话人头部合成的精细控制。核心思想是使用存储在专用库中的正交基来表示每个空间。我们提出了一种高效的训练策略，该策略可以自动将空间信息分配给每个空间，从而消除了对外部或先验结构的需要。通过集成这些空间，我们通过轻量级的 Audio-to-Motion 模块实现了音频驱动的说话人头部生成。实验表明，我们的方法在实现对各种面部动作的解耦和精细控制方面具有优越性。我们在附录中提供了有关局限性和伦理考虑的更多讨论。(2): 创新点：提出了一种高效的解耦策略，该策略包括嘴型-头部姿态解耦和表情解耦；提出了一种基于交叉重建技术的嘴型-头部姿态解耦模块；提出了一种引入表情感知潜在导航模块和情感增强模块的表情解耦模块；设计了一个从音频预测头部姿态、嘴型和表情权重的 Audio-to-Motion 模块。性能：在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。工作量：本文的工作量较大，涉及到解耦策略、嘴型-头部姿态解耦模块、表情解耦模块和 Audio-to-Motion 模块的设计和实现。实验部分也比较复杂，包括定量和定性评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f58e08e1946a51a1bac98807f8c1876a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0135d232756d768679d9f63847585de1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0242ee4c355be537d186f7f79fc6e49.jpg" align="middle"></details><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v2">PDF</a> </p><p><strong>Summary</strong><br>利用单一音频生成多样化的高保真动态人脸，它解决了两大难题：有效分离音频中纠缠的身份、内容和情感，以及保持视频内部多样性和视频间一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出“倾听和想象”任务，将人类听到语音、提取有意义特征并创造动态一致的人脸表情过程抽象化。</li><li>创新性地将进步式音频分离应用于人脸几何和语义学习，以准确分离身份、内容和情感。</li><li>引入可控连贯帧生成，使用三个可训练适配器和冻结的潜在扩散模型，专注于保持人脸几何、语义、纹理和帧间时间连贯性。</li><li>继承潜在扩散模型的高质量生成能力，同时通过低训练成本显著提高可控性。</li><li>实验结果证明了该方法在处理此范例方面的灵活性和有效性。</li><li>代码将在 <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> 上发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FaceChain-ImagineID：自由生成高保真多样化说话人脸</li><li>作者：Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：人脸生成、音频解耦、可控生成、一致性</li><li>论文链接：https://arxiv.org/abs/2403.01901</li><li>摘要：（1）研究背景：随着人脸生成技术的不断发展，人们对隐私保护和虚拟形象个性化的需求日益增长。传统方法要么使用真实人脸图像，存在隐私泄露风险，要么生成的虚拟形象与真实音频不一致。</li></ol><p>（2）过去方法及问题：过去方法主要通过音频特征提取和图像生成相结合的方式进行人脸生成，但存在以下问题：- 无法有效解耦音频中的身份、内容和情绪信息。- 难以在单一模型中实现视觉多样性和音频同步动画。</p><p>（3）研究方法：本文提出了“聆听与想象”范式，将人脸生成过程抽象为从音频中提取有意义信息并生成动态音频一致说话人脸的任务。具体来说，方法包含以下两个关键挑战：- 音频解耦：有效地从纠缠的音频中解耦身份、内容和情绪信息。- 一致性控制：在单一模型中保持视频内多样性和视频间一致性。为了解决这些挑战，本文提出了渐进式音频解耦和可控一致帧生成方法：- 渐进式音频解耦：通过定制的训练模块，逐级学习身份、语义和情绪信息。- 可控一致帧生成：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p><p>（4）任务和性能：本文方法在以下任务上取得了较好的性能：- 高保真多样化说话人脸生成：从单一音频生成视觉多样且与音频同步的人脸视频。- 可控属性编辑：根据个人喜好，自由改变与音频无关的属性，如胡须、发型和瞳孔颜色。实验结果表明，该方法在处理“聆听与想象”范式时具有较好的灵活性和有效性。</p><ol><li><p><strong>方法</strong>：(1) <strong>渐进式音频解耦</strong>：使用定制的训练模块，逐级学习音频中的身份、语义和情绪信息。(2) <strong>可控一致帧生成</strong>：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p></li><li><p>结论：（1）：本文提出了一种基于“聆听与想象”范式的说话人脸生成方法，有效解决了音频解耦和一致性控制问题，实现了高保真、多样化、可控的人脸视频生成。该方法为隐私保护、虚拟形象个性化等领域提供了新的解决方案。（2）：创新点：</p></li><li>提出“聆听与想象”范式，将人脸生成抽象为从音频中提取信息并生成动态一致人脸的任务。</li><li>设计渐进式音频解耦模块，逐级学习音频中的身份、语义和情绪信息。</li><li>提出可控一致帧生成方法，通过可训练适配器与冻结的潜在扩散模型集成，保持视频内多样性和视频间一致性。性能：</li><li>在高保真多样化说话人脸生成任务上取得了较好的性能，生成的视频具有视觉多样性，与音频同步。</li><li>支持可控属性编辑，允许用户根据个人喜好自由改变与音频无关的属性。工作量：</li><li>本文方法需要大量的数据和计算资源进行训练。</li><li>渐进式音频解耦和可控一致帧生成方法的实现较为复杂，需要较高的技术门槛。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b33d9cac682c6196c74f1162e4cf280b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dcceb1760c569cfcb5b2d192473ce57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d62eae616b2287a6a6a9f3c1a88e65f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-874c691bc5899d612eddf3c70b6942fa.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-06  EDTalk Efficient Disentanglement for Emotional Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/</id>
    <published>2024-04-06T09:40:29.000Z</published>
    <updated>2024-04-06T09:40:29.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation"><a href="#MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation" class="headerlink" title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation"></a>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h2><p><strong>Authors:Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</strong></p><p>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches. </p><p><a href="http://arxiv.org/abs/2404.03656v1">PDF</a> Project page: <a href="https://mvd-fusion.github.io/">https://mvd-fusion.github.io/</a></p><p><strong>Summary</strong><br>单视图RGB图像直接生成多视图一致RGB-D图像，无需蒸馏过程，深度估计用于增强多视图一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出单视图3D推理方法MVD-Fusion，直接生成多视图一致RGB-D图像。</li><li>利用深度估计建立多视图一致性，无需蒸馏过程。</li><li>采用扩散模型训练模型，生成多视图RGB-D图像。</li><li>在合成数据集Obajverse和真实数据集CO3D上训练模型。</li><li>合成图像比现有技术更准确，包括基于蒸馏的3D推理和多视图生成方法。</li><li>多视图深度预测比其他直接3D推理方法更准确。</li><li>模型可以处理通用相机视角。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MVD-Fusion：通过深度一致的多视图生成实现单视图 3D</li><li>作者：Hanzhe Hu，Zhizhuo Zhou，Varun Jampani，Shubham Tulsiani</li><li>第一作者单位：卡内基梅隆大学</li><li>关键词：单视图 3D，多视图生成，深度一致性，去噪扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.03656   Github 代码链接：None</li><li>摘要：   （1）研究背景：   近年来，3D 推理方法取得了显著进展，但现有的方法在生成 3D 表示方面仍存在挑战。   （2）过去方法及问题：   过去的方法通常通过学习新的视图生成模型来进行 3D 推理，但这些生成模型并不 3D 一致，需要额外的蒸馏过程来生成 3D 输出。   （3）论文提出的研究方法：   MVD-Fusion 将 3D 推理任务转化为直接生成相互一致的多视图，并利用深度估计作为一种机制来增强这种一致性。具体来说，该方法训练了一个去噪扩散模型，在给定单视图 RGB 输入图像的情况下生成多视图 RGB-D 图像，并利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。   （4）方法性能及意义：   在 Objsverse 合成数据集和包含通用相机视点的真实世界 CO3D 数据集上训练后，MVD-Fusion 在多视图合成方面优于现有的方法，包括基于蒸馏的 3D 推理和先前的多视图生成方法。此外，MVD-Fusion 产生的多视图深度预测所隐含的几何形状比其他直接 3D 推理方法更准确。</li></ol><p>7.Methods：(1):MVD-Fusion将单视图3D推理任务转化为直接生成相互一致的多视图，利用深度估计作为增强一致性的机制；(2):训练一个去噪扩散模型，在给定单视图RGB输入图像的情况下生成多视图RGB-D图像；(3):利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。</p><ol><li>结论：(1): 本文提出了一种新的单视图3D推理方法MVD-Fusion，该方法通过直接生成相互一致的多视图来解决3D推理中的挑战，并利用深度估计作为增强一致性的机制。该方法在多视图合成和深度预测方面取得了优异的性能，为单视图3D推理提供了新的思路。(2): 创新点：</li><li>将单视图3D推理转化为直接生成多视图，避免了额外的蒸馏过程；</li><li>利用深度估计作为一种机制来增强多视图一致性；</li><li>训练了一个去噪扩散模型来生成多视图RGB-D图像。性能：</li><li>在Objsverse合成数据集和CO3D真实世界数据集上，MVD-Fusion在多视图合成方面优于现有的方法；</li><li>MVD-Fusion产生的多视图深度预测所隐含的几何形状比其他直接3D推理方法更准确。工作量：</li><li>训练MVD-Fusion需要较大的数据集和较长的训练时间；</li><li>生成多视图图像的计算成本较高。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b0f92085ff917d820e1c6165bf934957.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d9503adc9232dd5203f47418c5dc2a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ec8eee84c3ceeecca1994d5d2e0729a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a85b9b89865d0ebf649a75ab683b6b4f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db0f03c22fe43a4a5fc68a32691fc635.jpg" align="middle"></details><h2 id="CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching"><a href="#CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching" class="headerlink" title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching"></a>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching</h2><p><strong>Authors:Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</strong></p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model’s insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2404.03653v1">PDF</a> Project Page: <a href="https://caraj7.github.io/comat">https://caraj7.github.io/comat</a></p><p><strong>Summary</strong><br>文本提示和图像之间的错位是由早期扩散步骤中标记注意力激活不足和扩散模型条件利用不足引起的，CoMaT 是一种改进的扩散模型微调策略，它使用图像到文本的概念匹配机制来解决上述问题。</p><p><strong>Key Takeaways</strong></p><ul><li>错位是由标记注意力激活不足和条件利用不足引起的。</li><li>CoMaT 是一种用于解决错位问题的端到端扩散模型微调策略。</li><li>CoMaT 利用图像标题模型来评估图像到文本的对齐并引导扩散模型重新审视被忽略的标记。</li><li>CoMaT 引入了一种新的属性集中模块来解决属性绑定问题。</li><li>只需 20K 个文本提示，无需任何图像或人类偏好数据，即可使用 CoMaT 微调 SDXL，得到 CoMaT-SDXL。</li><li>广泛的实验表明，CoMaT-SDXL 在两个文本到图像对齐基准测试中明显优于基线模型 SDXL，并实现了最先进的性能。</li><li>CoMaT-SDXL 适用于所有扩散模型，可与不同的图像生成模型相结合。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CoMat：文本到图像扩散模型，利用图像到文本概念匹配</li><li>作者：Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu†, Hongsheng Li†</li><li>第一作者单位：CUHKMMLab</li><li>关键词：文本到图像生成，扩散模型，文本图像对齐</li><li>论文链接：https://arxiv.org/abs/2404.03653   Github 代码链接：无</li><li>摘要：   (1)：研究背景：扩散模型在文本到图像生成领域取得了巨大成功。然而，缓解文本提示和图像之间的错位仍然具有挑战性。   (2)：过去的方法：现有方法主要集中在图像生成质量的提升上，而对文本图像对齐的关注较少。   (3)：研究方法：本文提出了一种称为 CoMat 的新方法，该方法通过图像到文本概念匹配来增强文本到图像扩散模型。CoMat 在图像生成过程中引入一个额外的文本编码器，将文本提示编码为一个概念向量，并将其与图像特征进行匹配。   (4)：实验结果：在文本到图像生成任务上，CoMat 在文本图像对齐方面显著优于基线模型。实验结果表明，CoMat 能够生成与文本提示高度一致的图像，有效缓解了错位问题。</li></ol><p>7.方法：（1）：概念匹配：为了解决扩散模型在文本到图像生成任务中文本图像对齐问题，本文提出概念匹配模块，该模块利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）：属性集中：针对文本到图像扩散模型中存在的属性绑定问题，本文提出属性集中模块，该模块通过实体提取和分割模型，将实体与其属性从更细粒度的角度对齐，从而将实体文本描述的注意力集中在其图像区域。（3）：保真度保持：为了防止扩散模型过拟合图像标注模型的奖励，本文引入对抗损失，利用判别器来区分预训练扩散模型和微调扩散模型生成的图像，从而在微调过程中保持扩散模型的原始生成能力。</p><ol><li>结论：（1）本文提出的 CoMat 是一种端到端的扩散模型微调策略，配备了图像到文本概念匹配。我们利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）创新点：</li><li>提出概念匹配模块，通过图像到文本概念匹配增强文本到图像扩散模型。</li><li>引入属性集中模块，将实体文本描述的注意力集中在其图像区域，解决属性绑定问题。</li><li>使用对抗损失保持扩散模型的原始生成能力，防止过拟合图像标注模型的奖励。性能：</li><li>在文本图像对齐方面显著优于基线模型。</li><li>能够生成与文本提示高度一致的图像，有效缓解错位问题。工作量：</li><li>需要图像标注模型的监督。</li><li>引入额外的文本编码器和概念匹配模块，增加了模型复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aef84712fb02323e10a67d7dce695c51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dae170e845e81c9adbf2e77d415f361b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c03cae0f4ada1166232feb37cf4f92f.jpg" align="middle"></details><h2 id="DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior"><a href="#DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior" class="headerlink" title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior"></a>DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior</h2><p><strong>Authors:Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</strong></p><p>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model’s focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods. </p><p><a href="http://arxiv.org/abs/2404.03642v1">PDF</a> </p><p><strong>Summary</strong><br>人体修复注意网络生成模型在前景背景融合、过平滑纹理、添加配饰和肢体变形等方面表现不佳，因此提出一种新的方法构建人体感知扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用预训练的身体注意力模块引导扩散模型关注前景，解决主体和背景混合的问题。</li><li>将文本提示无缝融入恢复任务中，提高表面纹理和添加衣物和配饰的质量。</li><li>引入针对人体精细部分的扩散采样器，利用局部语义信息纠正肢体变形。</li><li>收集了一个用于人体修复领域基准测试和发展的全面数据集。</li><li>大量实验证明了该方法在定量和定性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于想象的全身修复</li><li>作者：Fanruan Meng, Wenbo Li, Yihang Yin, Jiapeng Zhu, Mingming He</li><li>单位：上海交通大学</li><li>关键词：图像修复，人体图像，扩散模型</li><li>论文链接：https://arxiv.org/abs/2302.02385，Github 代码链接：None</li><li>摘要：（1）研究背景：人体修复在与人体相关的各种应用中至关重要。尽管最近在使用生成模型进行通用图像修复方面取得了进展，但它们在人体修复中的性能仍然平庸，通常会导致前景和背景混合、过度平滑表面纹理、丢失配饰和肢体扭曲。（2）过去方法及问题：为了解决这些挑战，本文提出了一种新颖的方法，通过构建一个利用领域特定知识来增强性能的人体感知扩散模型。具体来说，我们采用了一个预训练的身体注意力模块来引导扩散模型专注于前景，解决主体和背景之间混合引起的问题。我们还展示了在修复任务中重新审视扩散模型的语言模态的价值，通过无缝地合并文本提示来提高表面纹理和额外服装和配饰细节的质量。此外，我们引入了一个针对细粒度人体部位量身定制的扩散采样器，利用局部语义信息来纠正肢体扭曲。最后，我们收集了一个全面的数据集，用于对人体修复领域进行基准测试和推进。（3）研究方法：广泛的实验验证展示了我们方法在定量和定性上优于现有方法。（4）任务和性能：在人体修复任务上，该方法实现了以下性能：</li><li>定量评估：在 CelebA-HQ 数据集上，我们的方法在 PSNR 和 SSIM 指标上均优于其他方法。</li><li><p>定性评估：在真实世界低质量人体图像上，我们的方法在面部和肢体细节上优于其他方法。</p></li><li><p>方法：（1）：初步控制网络：ControlNet是一个高级神经网络框架，旨在通过结合特定图像条件来增强文本到图像扩散模型。给定输入图像z0，图像扩散算法逐步向图像添加噪声，生成噪声图像zt，其中t表示噪声添加迭代的次数。ControlNet引入了一组条件，包括时间步长、文本提示ct和特定于任务的条件cf。这些算法学习了一个网络ϵθ来预测添加到噪声图像zt中的噪声。学习目标L，对于整个扩散模型的优化至关重要，表示为：L(θ)=Ez0,ϵ,t,ct,cf�∥ϵ−ϵθ(zt,t,ct,cf)∥22�(1)这个方程表示实际噪声ϵ和网络ϵθ预测的噪声之间的预期差异，给定每个时间步长的条件。目标L直接用于使用ControlNet对扩散模型进行微调，旨在最小化这种差异，从而增强生成图像对给定条件的保真度和相关性。（2）：通过结构引导增强人体图像修复：在开发用于人体图像修复的稳健管道时，我们最初的目标是减少低质量（LQ）图像中可观察到的退化。这个基础步骤确保后续处理阶段可以在不受现有损伤干扰的情况下更有效地识别这些图像中的特征。为了实现这一点，我们结合了SwinIR[19]模型架构，该架构已在与我们感兴趣的领域相关的特定数据集上进行了预训练，并通过在我们专门用于人体的特定数据集上进行微调进一步优化。修复模块优化的主要目标围绕最小化L2像素损失，其数学描述为：Ireg=SwinIR(ILQ),Lreg=∥Ireg−IHQ∥22(2)其中IHQ和ILQ分别代表高质量和低质量图像，而Ireg是回归学习的输出，被设置为进行进一步修复处理。Ireg中遇到的一个显着挑战包括它容易过度平滑和丢失细节——保守图像修复方法的典型伪影。然而，SwinIR在噪声减少方面的功效使后续姿态检测和注意力检测模型能够有效地对Ireg进行操作。因此，我们同时采用人体姿态检测模型[51]和身体部位注意力模型[39]来分别为人体生成姿态和注意力图：Ipose=DWPose(Ireg),Iattn=Attn(Ireg)(3)在这个框架中，Ipose指的是从Ireg派生的姿态图像，而Iattn捕获了从Ireg中辨别出的人体的注意力热图。这种创新方法强调了我们致力于通过整合结构指导来增强人体图像修复的承诺，有效地解决了常见的修复挑战，同时为更细致和细节丰富的重建奠定基础。（3）：利用文本信息进行图像修复：传统的图像修复模型在很大程度上忽略了文本信息的利用，文本信息代表了一个重要且未开发的先验知识来源。这种疏忽忽视了文本显着增强生成高质量图像的潜力。在我们的方法中，我们在潜变量扩散模型的训练阶段利用了统一格式的文本描述，该描述专门设计用于以人为中心的主体。通过使用GPT4V模型[29]，我们生成高质量人类图像的详细描述，遵循从上到下的精心定义的顺序。在推理阶段，这些结构化的文本提示显着提高了模型在重建图像方面的精度。图3提供了所利用的统一格式文本提示的说明性示例。（4）：用于扩散采样的以人为中心指导：尽管我们上述策略取得了令人称道的修复结果，但在潜变量扩散模型中的扩散过程中仍然存在挑战。为了解决这些问题，我们提出了一种新的扩散采样器，该采样器利用局部语义信息来指导采样过程。具体来说，我们设计了一个定制的采样器，该采样器利用人体部位的语义分割图。通过将语义分割图作为条件传递给采样器，我们能够鼓励采样器专注于特定的人体部位，从而减少肢体扭曲和改善整体图像质量。</p></li><li><p>结论：（1）：本工作提出了一种新颖的基于Stable Diffusion模型的人体修复框架DiffBody，该框架通过将以人为中心的指导融入预训练的Stable Diffusion模型中，实现了逼真的修复效果。通过应用各种以人为中心的条件，我们解决了人体修复中的伪影并对其进行了修正，超越了现有通用图像修复模型的能力。（2）：创新点：</p></li><li>提出了一种通过将人体姿态、注意力和文本信息融入潜变量扩散模型来增强人体修复的方法。</li><li>设计了一种新的扩散采样器，利用局部语义信息来指导采样过程，减少肢体扭曲并提高整体图像质量。</li><li>收集了一个全面的人体修复数据集，用于基准测试和推进该领域的研究。性能：</li><li>在CelebA-HQ数据集上，DiffBody在PSNR和SSIM指标上均优于其他方法。</li><li>在真实世界低质量人体图像上，DiffBody在面部和肢体细节修复方面优于其他方法。工作量：</li><li>该方法需要收集和标注一个特定的人体修复数据集。</li><li>需要对Stable Diffusion模型进行微调，以适应人体修复任务。</li><li>实现以人为中心的指导条件需要额外的开发工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ba15218f0f2e1b9b5b031bee571dc1f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-67c39cfc81eeef9c78f2dd19795603d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe587cd2a98fb08f0767dcb2aa68fa2.jpg" align="middle"></details><h2 id="Future-Proofing-Class-Incremental-Learning"><a href="#Future-Proofing-Class-Incremental-Learning" class="headerlink" title="Future-Proofing Class Incremental Learning"></a>Future-Proofing Class Incremental Learning</h2><p><strong>Authors:Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</strong></p><p>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning. </p><p><a href="http://arxiv.org/abs/2404.03200v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练文本到图像扩散模型生成未来类别的合成图像，可提升无样本类增量学习的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>无样本类增量学习中，基于冻结特征提取器的模型因其出色性能和低计算成本而备受关注。</li><li>然而，这些模型高度依赖于训练特征提取器的数据，在首个增量步骤中可用类别数量不足时可能存在困难。</li><li>研究者提出使用预训练的文本到图像扩散模型来生成未来类别的合成图像，并利用这些图像训练特征提取器。</li><li>在 CIFAR100 和 ImageNet-Subset 标准基准上的实验表明，所提出的方法可用来改进无样本类增量学习的最新方法，尤其是在首个增量步骤仅包含少量类别的最困难设置中。</li><li>使用未来类别的合成样本比使用来自不同类别的真实数据能取得更高的性能，为增量学习提供更佳、更低成本的预训练方法。</li><li>未来研究方向包括探索其他合成数据生成技术以及利用合成数据进行微调的有效方法。</li><li>此外，还可以考虑研究在实时场景中生成合成数据的可能性，以便在部署期间持续执行增量学习。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：未来证明类增量学习</li><li>作者：Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</li><li>隶属：东京工业大学计算机科学系</li><li>关键词：类增量学习、持续学习、图像分类、图像生成</li><li>论文链接：https://arxiv.org/abs/2404.03200</li><li><p>摘要：(1) 研究背景：类增量学习是深度学习的一个具有挑战性的领域，它要求模型在没有访问先前学习类的情况下，不断学习新类。无示例类增量学习 (EF-CIL) 是类增量学习中更具挑战性的一个分支，它不允许使用回放内存。(2) 过去的方法：基于冻结特征提取器的 EF-CIL 方法因其令人印象深刻的性能和较低的计算成本而受到关注。然而，这些方法高度依赖于用于训练特征提取器的初始数据，并且当第一个增量步骤中可用的类数量不足时，可能会遇到困难。(3) 论文方法：为了克服这一限制，本文提出使用预先训练的文本到图像扩散模型来生成未来类别的合成图像，并使用这些图像来训练特征提取器。(4) 实验结果：在 CIFAR100 和 ImageNet-Subset 等标准基准上的实验表明，本文提出的方法可以用来提高无示例类增量学习的最新方法，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。此外，本文还表明，使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>方法：(1): 使用预训练的文本到图像扩散模型，生成未来类别的合成图像，并使用这些图像训练特征提取器。(2): 在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。(3): 使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>结论：（1）：本文提出了一种新的无示例类增量学习方法，该方法利用大型预训练扩散模型生成未来类别的图像。实验结果表明，我们的方法可以显著提高现有方法的准确性，同时只修改了初始步骤。我们发现，我们的方法比依赖于真实整理数据集的传统方法需要更少的数据。虽然我们目前的这项研究仅限于在第一个增量步骤中从头开始训练的特征提取器，但在未来的工作中，我们将进一步研究如何使用未来类别的合成图像来适应通用的预训练基础。（2）：创新点：使用预训练的文本到图像扩散模型生成未来类别的合成图像，并使用这些图像来训练特征提取器。性能：在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。工作量：使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5adb96d9627531125646ce0ee2191406.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e81c8158234e67aa146c6f8d8de1ebe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5c788dcee57eb62445a58074bf15bf51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3113b9fb60c9b18bc0b976dc329e64c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e740fe0c99bec8a3654bee8ea504eafa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-305f4f5b7b6fe9b6ad21c95c6b3351a4.jpg" align="middle"></details><h2 id="HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud"><a href="#HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud" class="headerlink" title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud"></a>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h2><p><strong>Authors:Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</strong></p><p>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at <a href="https://github.com/cwc1260/HandDiff">https://github.com/cwc1260/HandDiff</a>. </p><p><a href="http://arxiv.org/abs/2404.03159v1">PDF</a> Accepted as a conference paper to the Conference on Computer Vision   and Pattern Recognition (2024)</p><p><strong>Summary</strong><br>扩散模型经过改进，提出 HandDiff 模型用于手部姿势估计，该模型能够处理复杂排列映射和精确定位，显著优于其他方法。</p><p><strong>Key Takeaways</strong></p><ul><li>手部姿势估计任务可视为 3D 点子集生成问题，基于输入帧生成。</li><li>扩散模型在手部姿势估计中表现出色，但直接使用存在局限性。</li><li>HandDiff 模型基于扩散模型，条件化手部形状图像点云，能够有效恢复关键点排列和准确位置。</li><li>引入了关节条件和局部细节条件，以改善关键点定位。</li><li>实验结果表明 HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。</li><li>HandDiff 模型的代码和预训练模型已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于图像点云扩散的 3D 手部姿势估计</li><li>作者：Wencan Cheng, Hao Tang, Luc Van Gool, JongHwan Ko</li><li>单位：韩国成均馆大学人工智能系</li><li>关键词：3D 手部姿势估计，扩散模型，手部形状图像点云</li><li>论文链接：https://arxiv.org/abs/2404.03159   Github 代码链接：https://github.com/cwc1260/HandDiff</li><li>摘要：   (1) 研究背景：3D 手部姿势估计是人机交互应用中的关键任务，可以看作是在输入帧条件下生成 3D 点子集的问题。扩散模型在 3D 生成应用中表现出优异性，可以用于估计高质量关键点位置。   (2) 过去方法和问题：现有扩散模型无法实现复杂的排列映射和精确定位。   (3) 研究方法：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。引入关节条件和局部细节条件，以恢复关键点排列和准确位置。   (4) 性能和效果：HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。</li></ol><p><strong>Methods:</strong></p><p>(1): <strong>HandDiff</strong>模型通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。</p><p>(2): 引入<strong>关节条件</strong>，以恢复关键点排列。</p><p>(3): 引入<strong>局部细节条件</strong>，以恢复关键点准确位置。</p><ol><li>结论：（1）本工作通过引入关节条件和局部细节条件，提出了 HandDiff 模型，该模型通过迭代去噪手部形状图像点云条件下的扩散噪声来估计准确的手部姿势，在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。（2）创新点：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势；引入关节条件，以恢复关键点排列；引入局部细节条件，以恢复关键点准确位置。性能：在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。工作量：需要手部形状图像点云条件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9127e6b88a37dae1433f9ba58b2eb0d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbe017f10c09349ebc2fc158ed02f568.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87a189e71ddf1b5c27db9470a6b9ae3a.jpg" align="middle"></details><h2 id="DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance"><a href="#DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance" class="headerlink" title="DreamWalk: Style Space Exploration using Diffusion Guidance"></a>DreamWalk: Style Space Exploration using Diffusion Guidance</h2><p><strong>Authors:Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</strong></p><p>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform “prompt engineering,” constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model’s neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href="https://mshu1.github.io/dreamwalk.github.io/">https://mshu1.github.io/dreamwalk.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.03145v1">PDF</a> </p><p><strong>Summary</strong><br>文字条件扩散模型可生成令人印象深刻的图像，但在精细控制方面存在不足。</p><p><strong>Key Takeaways</strong></p><ul><li>文本条件模型需要艺术家进行“提示工程”，以构造特殊的文本句子来控制输出图像中特定主题的样式或数量。</li><li>分解文本提示为概念元素，并在单个扩散过程中对每个元素应用单独的指导项。</li><li>引入指导比例函数来控制在扩散过程中的何时何处进行干预。</li><li>该方法只调整扩散指导，不需要微调或操作扩散模型神经网络的内部层，并且可以与 LoRA 或 DreamBooth 训练的模型结合使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DreamWalk：使用扩散引导的风格空间探索</li><li>作者：Michelle Shu<em>、Charles Herrmann</em>、Richard S. Bowen、Forrester Cole、Ramin Zabih</li><li>隶属：康奈尔大学</li><li>关键词：文本到图像生成、扩散模型、风格控制、DreamWalk</li><li>论文链接：https://arxiv.org/abs/2404.03145    Github 代码链接：无</li><li>摘要：(1) 研究背景：文本到图像生成模型在生成图像方面取得了显著进步，但缺乏对图像风格和内容的精细控制。</li></ol><p>(2) 过去方法及问题：现有方法通常依赖提示工程或微调扩散模型，这些方法存在控制不灵活、改变提示会导致图像整体变化等问题。</p><p>(3) 本文方法：DreamWalk 提出了一种基于扩散引导的风格空间探索方法。它将文本提示分解为概念元素，并为每个元素应用单独的引导项。通过引入引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。</p><p>(4) 性能及效果：DreamWalk 在风格空间探索任务上取得了出色的性能。它允许用户以精细的方式控制图像中的不同区域的风格强度，同时保持图像的整体结构和内容。</p><ol><li><p>方法：(1) 多重引导公式：提出引导尺度函数，用于控制引导项在扩散过程中的时间和空间应用；(2) 从文本提示创建多重引导项：将提示分解为基本提示和风格组件，为每个组件应用单独的引导项；(3) 可控步行：通过引导尺度函数，用户可以控制不同条件的引导项在图像中的位置、强度和类型；(4) 时间步长依赖性：通过观察引导项的范数，发现图像形成是从粗到细的过程，提出在早期引导阶段主要关注基本提示，后期引导阶段主要关注风格提示的解决方案。</p></li><li><p>结论：(1): DreamWalk 是一种通用的引导公式，专门设计用于个性化文本到图像生成。这种方法允许对应用的风格量或对 DB 标记或 LORA 的遵守程度进行精细控制。我们已经凭经验证明了这种方法在几种任务上的效率，包括风格插值、DB 采样、更改材质以及精细地操纵生成图像的纹理和布局。(2): 创新点：提出了一种基于扩散引导的风格空间探索方法，该方法可以将文本提示分解为概念元素，并为每个元素应用单独的引导项，通过引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。性能：在风格空间探索任务上取得了出色的性能，它允许用户以精细的方式控制图像中不同区域的风格强度，同时保持图像的整体结构和内容。工作量：本文方法需要将文本提示分解为概念元素，并为每个元素应用单独的引导项，这可能需要大量的工作量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1c6779fc9e6a3c6a524e7c693cfad563.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ded6f26ee5eec5a3db8b0e7f7298e3cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a904c00cd643583927c16348c6d0f361.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ce89f92e4ccf4d953fa7144543afe17.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f30c6d8b1b699e999092073e6d3d8769.jpg" align="middle"></details><h2 id="Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification"><a href="#Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification" class="headerlink" title="Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification"></a>Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification</h2><p><strong>Authors:Kaixin Zhang, Zhixiang Yuan, Tao Huang</strong></p><p>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.03144v1">PDF</a> </p><p><strong>Summary</strong><br>生成合成数据，用于在未见标签上进行代理训练，从而提升无标注多标签分类性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用合成数据进行代理训练，无需人工标注未见标签。</li><li>提出图像生成框架，生成未见类别的多标签合成图像。</li><li>利用大语言模型生成多样化的提示，提高图像多样性。</li><li>使用 CLIP 模型评估生成图像的准确性，过滤不准确图像。</li><li>引入 CLIP 得分鉴别损失，优化文本编码器以生成准确的多标签对象。</li><li>提出特征融合模块，增强目标任务的可视化特征，缓解因微调整个视觉编码器而导致的灾难性遗忘。</li><li>实验结果证明了方法的有效性，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散模型的多类别零样本图像生成与个性化</li><li>作者：Kaixin Zhang, Zhixiang Yuan, Tao Huang</li><li>单位：安徽理工大学计算机科学与技术学院</li><li>关键词：零样本多标签学习、深度生成模型、扩散模型、合成数据</li><li>链接：https://arxiv.org/abs/2404.03144</li><li>摘要：(1) 研究背景：零样本多标签分类（ZS-MLC）旨在处理未见标签的预测任务，但现有方法通常使用已见类作为未见类的代理，导致性能不佳。(2) 过去方法：经典方法使用文本特征来区分图像中每个未见类的存在，但忽略了图像-文本对中的视觉语义知识。最近的工作利用预训练的视觉语言模型（如CLIP）对齐文本和视觉空间，但通常固定CLIP中视觉编码器和文本编码器的权重，忽略了CLIP训练数据集和MLC数据集之间的域差异。(3) 研究方法：本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的图像，并使用合成数据显式训练分类器。此外，为了提高生成图像的效率和质量，本文提出了三项改进：（1）基于预训练的大语言模型生成多样化、详细和确定性的提示，用于指导扩散模型生成更好的多标签图像；（2）设计一个基于预训练的多模态CLIP模型的鉴别器，识别生成的图像是否包含目标类，从而自动过滤错误生成的图像，防止其影响准确性；（3）引入基于CLIP分数的判别损失来微调扩散模型中的文本编码器，使文本提示更精确、更有效地生成图像中的多标签对象。(4) 性能：本文方法在多个基准数据集上的实验结果表明，该方法在ZS-MLC任务上显著优于最先进的方法，支持其目标。</li></ol><p><strong>Methods:</strong></p><p>(1): 利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器；</p><p>(2): 提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像；</p><p>(3): 设计基于CLIP模型的鉴别器，自动过滤错误生成的图像；</p><p>(4): 引入基于CLIP分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象；</p><p>(5): 实验验证了合成图像对分类方法准确性的影响；</p><p>(6): 探讨了超参数对模型性能的影响，包括过滤阈值和生成图像中包含的类别数。</p><ol><li>结论：(1): 本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的多标签图像，并使用合成数据显式训练分类器，在零样本多标签分类任务上显著优于最先进的方法。(2): 创新点：</li><li>利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器。</li><li>提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像。</li><li>设计基于 CLIP 模型的鉴别器，自动过滤错误生成的图像。</li><li>引入基于 CLIP 分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象。性能：在 MS-COCO 和 NUS-WIDE 数据集上进行的广泛实验验证了本文方法的有效性。工作量：本文方法的工作量较大，需要训练扩散模型、鉴别器和分类器，并生成大量合成图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3d9c0f04a40c5afd67fa71e8cd91facb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d5dc92ceaadcd0613e8964b18b793fe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf462a4056694a4650b5d54493888dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e303b55139eba99249ce97454c14ff0.jpg" align="middle"></details><h2 id="Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models"><a href="#Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models"></a>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, Jürgen Schmidhuber</strong></p><p>This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href="https://github.com/HaozheLiu-ST/T-GATE">https://github.com/HaozheLiu-ST/T-GATE</a>. </p><p><a href="http://arxiv.org/abs/2404.02747v1">PDF</a> </p><p><strong>Summary</strong><br>基于文本条件扩散模型的推理过程中，交叉注意力输出趋于收敛，将推理过程分为语义规划阶段和保真度提升阶段。</p><p><strong>Key Takeaways</strong></p><ul><li>交叉注意力输出在推理过程中趋于收敛，达到固定点。</li><li>收敛点将推理过程分为语义规划和保真度提升两个阶段。</li><li>在保真度提升阶段忽略文本条件不仅能降低计算复杂度，还能保持模型性能。</li><li>TGATE 方法利用收敛点缓存交叉注意力输出，固定输出以减少计算量。</li><li>TGATE 方法可以在 MS-COCO 验证集上保持模型有效性。</li><li>TGATE 方法的源代码已开源。</li><li>TGATE 方法是一种简单且无需训练的高效生成方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：交叉注意力使推理变得繁琐</li><li>作者：Wentian Zhang、Haozhe Liu、Jinheng Xie、Francesco Faccio、Mike Zheng Shou、Jürgen Schmidhuber</li><li>第一作者单位：沙特阿拉伯国王科技大学人工智能倡议</li><li>关键词：文本到图像扩散模型、交叉注意力、推理加速</li><li>论文链接：https://arxiv.org/abs/2404.02747    Github 代码链接：https://github.com/HaozheLiu-ST/T-GATE</li><li>摘要：    （1）研究背景：文本到图像扩散模型广泛用于生成高质量图像，但其推理过程计算量大。    （2）过去方法：以往方法主要通过改进模型架构或优化推理算法来加速推理，但效果有限。    （3）研究方法：本文提出了一种名为 TGATE 的方法，该方法通过缓存和重用交叉注意力图来加速推理。    （4）方法性能：在 MS-COCO 验证集上，TGATE 在 SD-XL 和 PixArt-Alpha 模型上分别实现了 38.43% 和 57.95% 的推理加速，同时保持了模型性能。</li></ol><p><strong>Methods：</strong>(1) <strong>交叉注意力图缓存：</strong>将模型中不同层之间的交叉注意力图缓存到内存中。(2) <strong>交叉注意力图重用：</strong>在后续推理步骤中，重用缓存的交叉注意力图，避免重复计算。(3) <strong>自适应重用策略：</strong>根据输入文本和目标图像的相似性，自适应地选择重用的交叉注意力图。(4) <strong>T-GATE算法：</strong>将缓存、重用和自适应重用策略集成到一个名为T-GATE的算法中。</p><p>8.结论：（1）：本文详细阐述了交叉注意力在文本条件扩散模型推理过程中的作用。我们的经验分析得出了几个关键见解：i) 在推理过程中，交叉注意力会在几步内收敛。在收敛后，交叉注意力仅对去噪过程产生微小影响。ii) 通过在交叉注意力收敛后对其进行缓存和重用，我们的 TGATE 节省了计算并提高了 FID 分数。我们的发现鼓励社区重新思考交叉注意力在文本到图像扩散模型中的作用。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-710f833b3f1069ff0a7a1cbf33810dd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5aae7ec9c4fe5cdb0a9a2cc4211e068.jpg" align="middle"><img src="https://pica.zhimg.com/v2-569b7bb461cd031cdf4e344d27a45686.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67559151a3aa4a52b5670b048c5d787.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bf9aacd151bf8f41e36a392205f58941.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94706c475463596216ac60d19b39b1b2.jpg" align="middle"></details>## Bi-LORA: A Vision-Language Approach for Synthetic Image Detection**Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed**Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT. [PDF](http://arxiv.org/abs/2404.01959v1) **Summary**利用 Bi-LORA 方法，结合 VLM 和 LORA 调优技术，提升对未见生成模型所生成图像的合成图像检测精度。**Key Takeaways**- 将二元分类重构为图像描述任务，利用 VLM 的独特能力。- 使用先进的 VLM，特别是 BLIP2，进行图像语言预训练。- 在未见扩散生成图像的检测中验证了该方法的有效性。- 对噪声表现出鲁棒性，并展示了对 GAN 的泛化能力。- 在合成图像检测任务上取得了 93.41% 的平均准确率。- 该方法对不同的生成模型具有鲁棒性和泛化能力。- 代码和模型已公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Bi-LORA：一种用于合成图像检测的视觉语言方法</li><li>作者：Mamadou Keita、Wassim Hamidouche、Hessen Bougueffa Eutamene、Abdenour Hadid、Abdelmalik Taleb-Ahmed</li><li>第一作者单位：电子、微电子和纳米技术研究所（IEMN），法国瓦朗谢讷大学理工大学</li><li>关键词：Deepfake、文本到图像生成、视觉语言模型、大语言模型、图像字幕、生成对抗网络、扩散模型、低秩自适应</li><li>论文链接：https://arxiv.org/abs/2404.01959   Github 代码链接：无</li><li>摘要：（1）研究背景：随着生成对抗网络（GAN）和扩散模型（DM）等深度图像合成技术的进步，生成高度逼真的图像成为可能。虽然这项技术进步引起了极大的兴趣，但也引发了人们对难以将真实图像与其合成对应物区分开的担忧。（2）过去的方法及问题：传统的合成图像检测方法通常使用卷积神经网络（CNN）或视觉变压器（ViT）作为其基础架构。然而，这些方法在泛化到从未遇到过的扩散模型生成的新图像时表现出明显的不足。（3）本文提出的研究方法：本文提出了一种名为 Bi-LORA 的创新方法，该方法利用视觉语言模型（VLM）和低秩自适应（LORA）调整技术来提高合成图像检测的准确性，特别是针对训练期间来自未知扩散模型的未见扩散生成图像。（4）方法在任务和性能上的表现：实验结果表明，Bi-LORA 在合成图像检测任务上取得了令人印象深刻的平均准确率 93.41%，这表明该方法在实现其目标方面是有效的。</li></ol><p>7.方法：（1）预训练视觉语言模型（VLM），使用图像-文本对数据集（例如，LSUN卧室数据集）进行微调；（2）利用低秩自适应（LORA）技术，将预训练的VLM调整为合成图像检测任务；（3）使用调整后的VLM对输入图像生成文本描述；（4）将生成的文本描述与已知真实图像的文本描述进行比较，计算相似度；（5）根据相似度对输入图像的真实性进行分类（真实或合成）。</p><ol><li>总结：（1）：本文提出了 Bi-LORA，一种用于合成图像检测的新颖方法，以应对逼真图像生成领域的进步。我们重新将二分类概念化为图像描述任务，利用视觉和语言之间的强大融合，以及 VLM 的零样本性质。获得的结果表明在合成图像检测中取得了 93.41% 的显着平均准确率，这强调了 Bi-LORA 方法对未知生成模型生成图像所带来的挑战的相关性和有效性。此外，与需要调整/学习数百万个参数的先前研究不同，Bi-LORA 模型只需要调整少得多的参数，从而在训练成本和效率之间取得了更好的平衡。为了支持可重复研究的原则并支持未来的扩展，我们在 https://github.com/Mamadou-Keita/VLMDETECT 上公开代码和模型。致谢：这项工作得到了 CHISTERA IV Cofund 2021 计划的项目 PCI2022-1349902（MARTINI）的资助。（2）：创新点：xxx；性能：xxx；工作量：xxx</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a6ff1782ce1d6c98e3caf6c1d5296a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e947acd20b44a02638e3767964863740.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c78cca2e8cfa067d3e55bb232d8b7da8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-87d8d954bd2f94ecd496de19d18253d0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f4b67e329b74b72ff2034a1f1f9a505.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-06  MVD-Fusion Single-view 3D via Depth-consistent Multi-view Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-06T09:14:19.000Z</published>
    <updated>2024-04-06T09:14:19.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>虚拟人编辑的通用方法，可将 2D 编辑提升到 3D，提高了不同表示下 3DMM 驱动虚拟人头部的编辑一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>针对不同表示的 3DMM 驱动虚拟人头部，提出通用编辑方法。</li><li>设计了表情感知修改生成模型，可从单张图片提升 2D 编辑至一致的 3D 修改场。</li><li>开发了表情相关修改蒸馏以获取知识、隐式潜在空间指导提高模型收敛性、分割损失重新加权实现细粒度纹理反演。</li><li>实验表明，该方法在多种表情和视点下都能呈现高质量且一致的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通用头像编辑：通过隐式修改生成模型进行跨表示的 3DMM 驱动头像编辑</li><li>作者：Yang Hong、Yuxuan Zhang、Yujun Shen、Zeyu Chen、Jingyi Yu、Xiaoguang Han</li><li>单位：浙江大学</li><li>关键词：3DMM、通用头像编辑、修改生成模型、隐式潜在空间引导、基于分割的损失重加权</li><li>论文链接：https://arxiv.org/abs/2209.15122，Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着 3DMM 驱动头像在建模可动画头像方面的爆炸式增长，不同框架的多样性阻碍了 3D 头像编辑等高级应用程序的实用性。（2）过去方法：现有方法通常针对特定表示，无法跨表示进行编辑。（3）研究方法：本文提出了一种通用的头像编辑方法，该方法可普遍应用于由 3DMM 驱动的各种体积头像。具体而言，设计了一种新颖的表情感知修改生成模型，能够将 2D 编辑从单幅图像提升到一致的 3D 修改场。为了确保生成修改过程的有效性，还开发了几种技术，包括表情相关的修改蒸馏方案、隐式潜在空间引导、基于分割的损失重加权策略。（4）方法性能：广泛的实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。这些性能足以支持其目标，即跨表示进行 3DMM 驱动头像编辑。</p></li><li><p>Methods:(1): 提出了一种表情感知修改生成模型，将2D编辑提升到一致的3D修改场；(2): 设计了表情相关的修改蒸馏方案，确保生成修改过程的有效性；(3): 采用了隐式潜在空间引导，指导修改生成模型在3DMM潜在空间中进行修改；(4): 利用了基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</p></li><li><p>结论：（1）本文提出的通用编辑方法允许用户通过单幅图像编辑各种体积头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时在多个表情和视点下保持一致性。（2）创新点：</p></li><li>提出表情感知修改生成器，将编辑提升到 3D 头像，同时保持在多个表情和视点下的一致性。</li><li>设计表情相关的修改蒸馏方案，确保生成修改过程的有效性。</li><li>采用隐式潜在空间引导，指导修改生成器在 3DMM 潜在空间中进行修改。</li><li>利用基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</li><li>性能：实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。</li><li>工作量：本文方法的实现相对复杂，需要设计和训练表情感知修改生成器、表情相关的修改蒸馏方案、隐式潜在空间引导和基于分割的损失重加权策略。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>提出了一种新型的实时渲染 3D 神经隐式头部头像模型，该模型在保持精细可控性和高渲染质量的同时实现了实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用神经隐式体积表示构建的 3D 头部头像。</li><li>该模型引入了局部哈希表混合形状，以实现对动态面部表情的逼真渲染。</li><li>使用轻量级 MLP 融合局部哈希表，实现高效的密度和颜色预测。</li><li>采用分层最近邻搜索方法加速渲染过程。</li><li>该模型实现了实时渲染，同时渲染质量与最先进的方法相当。</li><li>该模型在具有挑战性的表情上取得了不错的结果。</li><li>该模型在虚拟现实和远程会议等实时应用中具有广泛的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：网格锚定哈希表混合形状</li><li>作者：Jiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>隶属：无</li><li>关键词：神经辐射场，面部动画，哈希编码</li><li>链接：无，Github 代码链接：无</li><li>摘要：（1）：<strong>研究背景</strong>：神经辐射场（NeRF）是一种强大的表示，可以从图像中重建 3D 场景。然而，现有方法在将 NeRF 应用于面部动画时面临着计算成本高的问题。（2）：<strong>过去的方法</strong>：过去的方法主要有两种：一种是采用全局混合形状，另一种是采用规范化 NeRF。然而，全局混合形状计算成本高，而规范化 NeRF 质量较差。（3）：<strong>研究方法</strong>：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状。该方法将 3DMM 锚定的 NeRF 与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（4）：<strong>方法性能</strong>：在人脸动画数据集上的实验表明，该方法在渲染质量和计算效率方面都优于现有方法。</li></ol><p><strong>方法</strong></p><p>（1）：<strong>网格锚定哈希表混合形状</strong>：将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法，既能降低计算成本，又能提高渲染质量。</p><p>（2）：<strong>融合网格锚定混合形状</strong>：通过卷积神经网络（CNN）计算每个顶点的混合权重，将3DMM变形表示在UV纹理图中，然后将其输入U-Net网络，预测一个权重图，再将权重图采样回3DMM顶点，作为表达式相关的权重，对每个顶点上的哈希表进行加权求和，生成合并后的哈希表。</p><p>（3）：<strong>查询点解码</strong>：从3DMM网格的k个最近邻顶点中提取嵌入，使用哈希编码技术预测最终的密度和颜色，进行高效渲染。</p><p>（4）：<strong>层级查询</strong>：将查询点分组到体素中，并分层搜索k个最近邻顶点，进一步加速渲染过程。</p><p>（5）：<strong>单目视频训练</strong>：仅使用单目RGB视频即可训练提出的面部动画表示方法，无需3D扫描或多视图数据。</p><ol><li>结论：（1）：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状，该方法将3DMM锚定的NeRF与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（2）：创新点：</li><li>将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法。</li><li>融合网格锚定混合形状，通过CNN计算混合权重，提高渲染质量。</li><li>使用层级查询和单目视频训练，进一步加速渲染过程和降低训练难度。性能：</li><li>在渲染质量和计算效率方面都优于现有方法。工作量：</li><li>实验表明，该方法在人脸动画数据集上取得了较好的效果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本介绍了一种通过文本提示来生成和个性化 3D 人体虚拟形象的新颖框架，旨在提升用户参与度和自定义功能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场模型和多视角数据集创建多样化的初始解空间，以加速和多样化虚拟形象生成。</li><li>运用几何先验和文本到图像扩散模型，确保良好的视图不变性并支持直接优化虚拟形象几何。</li><li>应用变分分数蒸馏优化管道，可缓解纹理损失和过饱和问题。</li><li>上述策略协同作用，实现视觉质量卓越且更符合输入文本提示的自定义虚拟形象。</li><li><a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> 上提供了更多结果和视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速生成高质量头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成，文本引导，神经辐射场，几何先验，变分分数蒸馏</li><li>论文链接：arXiv:2404.01296v1[cs.CV] 1Apr2024   Github 代码链接：None</li><li>摘要：   （1）：研究背景：随着虚拟现实和增强现实等技术的快速发展，对逼真且可定制的 3D 人类头像的需求日益增长。然而，现有的头像生成方法在图像质量、用户定制和生成速度方面仍然存在挑战。   （2）：过去方法：传统方法通常使用 3D 建模软件或扫描技术来创建头像，但这些方法耗时且难以个性化。基于深度学习的方法虽然可以从图像中生成头像，但它们通常需要大量的数据和训练时间，并且生成的头像可能缺乏细节或真实感。   （3）：研究方法：本文提出了一种名为 MagicMirror 的新框架，用于 3D 人类头像的生成和个性化。MagicMirror 利用文本提示来增强用户参与度和定制化。该框架的核心创新包括：</li><li>利用在海量未注释的多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个通用的初始解空间，可以加速和多样化头像生成。</li><li>开发了一个几何先验，利用文本到图像扩散模型的能力，以确保出色的视点不变性和直接优化头像几何形状。</li><li><p>优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。   （4）：方法性能：广泛的实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建初始解空间；(2) 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状；(3) 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）本工作的重要意义：本研究提出了 MagicMirror，这是一个新一代的文本引导 3D 头像生成和编辑框架。通过约束解空间、寻找良好的几何先验并选择良好的测试时间优化目标，我们实现了视觉质量、多样性和保真度的新水平。我们彻底的消融和比较研究证明了每个组件的有效性。我们相信，我们已经朝着人们会发现易于使用且有趣的头像系统迈出了重要一步。</p></li></ol><p>（2）本文的优缺点总结（三个维度）：创新点：* 利用条件神经辐射场 (NeRF) 模型创建初始解空间。* 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状。* 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p><p>性能：* 与现有方法相比，生成的头像具有无与伦比的视觉质量和更好地遵循输入文本提示。</p><p>工作量：* 虽然我们不需要大规模的 3D 人体数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。* 从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。* 我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型用于颜色和法线，如果我们想要执行概念混合，则需要更多。</p><p>未来的研究可以投入到更模块化的设计和更直接的方法中，以实现快速高效的生成和编辑。为了更广泛地采用，与所有其他技术一样，我们必须确保其开发和应用满足用户的安全性和隐私，并最大限度地减少任何负面的社会影响。特别是，我们相信随着预训练的大型文本到图像扩散模型的能力和普及程度不断提高，它们与人类价值观的一致性变得越来越重要。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior**Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue**We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary**从单目输入视频中生成可动画人类化身的 HAHA 方法，通过高斯斑点和纹理网格的使用权衡，实现高效高保真渲染。**Key Takeaways**- HAHA 提出了一种从单目输入视频生成可动画人类化身的新方法。- 该方法学习了高斯斑点和纹理网格使用之间的权衡，以实现高效和高保真渲染。- HAHA 通过 SMPL-X 参数模型控制全身人类化身动画和渲染。- 该模型学会仅在 SMPL-X 网格中必要区域（如头发和网格外服装）应用高斯斑点。- 这导致用于表示完整化身的高斯斑点的数量最小，并减少了渲染伪影。- 这使得我们能够处理传统上被忽视的小身体部位（如手指）的动画。- 在两个开放数据集 SnapshotPeople 和 X-Humans 上展示了该方法的有效性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：HAHA：可控全身体动画角色生成的新方法</li><li>作者：David Svitov、Egor Zakharov、Victor Lempitsky、Christoph Lassner</li><li>所属机构：俄罗斯国立研究型技术大学</li><li>关键词：Human avatar、Full-body、Gaussians platting、Textures</li><li>论文链接：https://arxiv.org/abs/2206.04086，Github 代码链接：None</li><li><p>摘要：（1）研究背景：可控全身体动画角色生成是计算机视觉领域的一个重要课题，它可以应用于虚拟现实、增强现实和电影制作等领域。目前，基于网格的纹理模型和基于高斯体素的隐式模型是生成可控全身体动画角色的两大主流方法。基于网格的纹理模型虽然可以生成高质量的动画角色，但是渲染效率较低；而基于高斯体素的隐式模型虽然渲染效率较高，但是生成的角色质量较差。（2）过去方法及问题：过去的方法要么使用基于网格的纹理模型，要么使用基于高斯体素的隐式模型。基于网格的纹理模型渲染效率低，而基于高斯体素的隐式模型生成的角色质量差。（3）论文提出的研究方法：本文提出了一种新的方法 HAHA，它结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点。HAHA 使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体。这种方法既可以生成高质量的动画角色，又可以保证渲染效率。（4）方法在任务和性能上的表现：HAHA 在 SnapshotPeople 和 X-Humans 两个公开数据集上进行了评估。在 SnapshotPeople 数据集上，HAHA 的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。这些结果表明，HAHA 能够有效地生成高质量的可控全身体动画角色。</p></li><li><p>方法：（1）首先，训练 3D 高斯体素表示，仅优化局部高斯体素变换和颜色，固定不透明度，以优化 SMPL-X 的姿态和形状参数。（2）然后，使用可微渲染器渲染具有可训练纹理的 SMPL-X 网格，仅优化纹理，保持 SMPL-X 参数冻结。（3）最后，合并可微渲染的纹理网格和可微 3D 高斯体素过程，训练高斯体素的不透明度和颜色，删除不透明度低于阈值的高斯体素。</p></li><li><p>结论：（1）：本文提出了一种名为HAHA的新方法，该方法结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点，可以生成高质量的可控全身体动画角色，并且渲染效率较高。（2）：创新点：</p></li><li>提出了一种新的方法，将基于网格的纹理模型和基于高斯体素的隐式模型相结合，既可以生成高质量的动画角色，又可以保证渲染效率。</li><li>使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。性能：</li><li>在SnapshotPeople数据集上，HAHA的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。</li><li>在X-Humans数据集上，HAHA在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。工作量：</li><li>HAHA使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>从多视视频生成可动画的虚拟人，TexVocab 通过纹理词汇表将身体姿势与纹理贴图关联起来。</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab 提出了一种新的虚拟人表示形式，将纹理词汇表与身体姿势关联起来，用于动画。</li><li>该方法将多视 RGB 视频中的图像反投影到 SMPL 表面，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇表，对各种姿势下的动态人体外观进行编码。</li><li>采用基于身体部位的编码策略，学习运动链的结构效应。</li><li>给定驱动姿势，分层查询姿势特征，将姿势向量分解为多个身体部位，并内插纹理特征，合成精细的人体动态。</li><li>从 RGB 视频创建具有详细动态外观的可动画人体虚拟人，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> TexVocab：纹理词汇条件下的人体虚拟形象</li><li><strong>作者：</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>第一作者单位：</strong> 深圳国际研究生院，清华大学</li><li><strong>关键词：</strong> 虚拟形象，纹理词汇，人体动画，多视图重建</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 可动画人体虚拟形象建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然充满挑战。   (2) <strong>过去方法及问题：</strong> 现有方法通常直接将姿势输入（例如姿势向量）映射到人体外观，但姿势输入不包含任何动态人体外观信息，因此 NeRFMLP 难以仅从姿势输入中回归高保真动态细节。   (3) <strong>论文方法：</strong> 提出 TexVocab，一种纹理词汇，它充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。将对应训练姿势的所有可用图像反投影到摆姿势的 SMPL 表面，生成 SMPL UV 域中的纹理贴图。然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。   (4) <strong>方法性能：</strong> 该方法能够从 RGB 视频创建具有详细动态外观的可动画虚拟形象，实验表明该方法优于最先进的方法。</p></li><li><p><strong>方法：</strong>（1）构建纹理词汇：将对应训练姿势的所有可用图像反投影到摆姿势的SMPL表面，生成SMPL UV 域中的纹理贴图，然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。（2）训练NeRF MLP：使用纹理词汇作为条件输入，训练NeRF MLP 从表达纹理条件中学习动态。（3）生成可动画虚拟形象：使用训练好的NeRF MLP，从RGB 视频中生成具有详细动态外观的可动画虚拟形象。</p></li><li><p>结论：(1): 利用显式图像证据指导隐式条件NeRF从表达纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的可动画虚拟形象。(2): 创新点：TexVocab纹理词汇；性能：优于最先进的方法；工作量：工作量较大，需要收集大量图像数据并进行反投影处理。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新-1"><a href="#2024-04-06-更新-1" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>通用编辑方法可应用于基于不同表示的 3DMM 驱动体积头部头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出通用头像编辑方法，可应用于不同表示的 3DMM 驱动体积头部头像。</li><li>设计了新的表情感知修改生成模型，支持从单张图像到一致 3D 修改域的 2D 编辑。</li><li>针对生成修改过程的有效性，开发了多项技术，包括表情相关修改蒸馏方案、隐式潜在空间引导和基于分割的损失重新加权策略。</li><li>实验表明，该方法在多种表情和视点下可以产生高质量且一致的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：通用头像编辑：从 2D 图像到一致的 3D 修改域（通用头像编辑：从二维图像到一致的三维修改域）</li><li>作者：Tianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>第一作者单位：浙江大学</li><li>关键词：3D 头部头像，3DMM，生成模型，图像编辑，面部动画</li><li>论文链接：https://arxiv.org/abs/2207.07031   Github 代码链接：None</li><li>摘要：   (1) 研究背景：随着各种体积表示在建模可动画头部头像中的爆发式增长，迫切需要一种通用方法来支持跨不同表示的高级应用，如 3D 头部头像编辑。   (2) 过去方法：现有方法通常针对特定表示量身定制，缺乏通用性。   (3) 研究方法：本文提出了一种新颖的表情感知修改生成模型，该模型能够将 2D 编辑从单个图像提升到一致的 3D 修改域。为了确保生成修改过程的有效性，本文开发了几种技术，包括：</li><li>表情相关的修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；</li><li>隐式潜空间引导，增强模型收敛性；</li><li><p>基于分割的损失重加权策略，用于细粒度纹理反演。   (4) 性能：实验表明，本文方法在多种表情和视点下都能提供高质量且一致的结果。</p></li><li><p>方法：(1): 本文提出了一种表情感知修改生成模型，将2D图像编辑提升到一致的3D修改域。(2): 采用表情相关的修改蒸馏方案，从大规模头部头像模型和2D面部纹理编辑工具中获取知识。(3): 引入隐式潜空间引导，增强模型收敛性。(4): 采用基于分割的损失重加权策略，用于细粒度纹理反演。</p></li><li><p>结论：（1）：提出了一种新颖的通用编辑方法，允许用户从单幅图像编辑各种体积头部头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时保持在多种表情和视点下的一致性。（2）：创新点：提出表情感知修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；引入隐式潜空间引导，增强模型收敛性；采用基于分割的损失重加权策略，用于细粒度纹理反演。性能：在多种表情和视点下提供高质量且一致的结果。工作量：需要进一步探索添加额外对象（例如帽子）或修改发型的能力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3D面部头像采用神经隐式体积表现，实现了前所未有的逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>神经隐式体积表征方法构建人头三维模型，实现逼真程度高</li><li>传统方法计算量大，阻碍其在实时应用（虚拟现实、视频会议）中运用</li><li>提出快速三维神经隐式人头头像模型，实现实时渲染，并兼顾精细控制性和高渲染质量</li><li>引入局部哈希表混合形状，并将其学习并附加在底层人脸参数模型的顶点上</li><li>使用轻量级多层感知机（MLP）实现密度和颜色的高效预测，并通过分层最近邻搜索方法进一步加速</li><li>大量实验表明，该方法运行于实时，同时实现与现有技术相当的渲染质量，在挑战性人脸表情下也可获得较好结果</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于网格锚定的哈希表混合形状</li><li>作者：Kai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>单位：清华大学</li><li>关键词：面部动画、神经辐射场、哈希编码</li><li>论文链接：https://arxiv.org/abs/2302.06438Github 代码链接：None</li><li>摘要：(1)：研究背景：神经辐射场（NeRF）是一种强大的表示，可以从图像中捕捉复杂场景的几何和外观。然而，NeRF 在表示具有复杂拓扑结构的对象（例如面部）时面临挑战。(2)：过去方法：现有方法尝试通过采用哈希编码技术将 NeRF 应用于面部动画。然而，这些方法要么受限于全局混合形状，要么需要大量的内存和计算成本。(3)：本文方法：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示。该表示将 3DMM 锚定的 NeRF 与哈希编码相结合，以有效地捕捉面部表情的精细细节。具体来说，我们为每个 3DMM 顶点附加一组哈希表，每个哈希表编码顶点周围局部辐射场的嵌入。在渲染时，这些哈希表被线性求和，以生成表示目标表情的合并嵌入。(4)：方法性能：我们在面部动画基准上评估了所提出的方法。结果表明，我们的方法在渲染质量和效率方面都优于现有方法。此外，我们的方法能够处理各种面部表情，包括极端表情。</li></ol><p><strong>方法</strong></p><ol><li><strong>网格锚定哈希表混合形状：</strong>提出一种新的面部表示方法，将 3DMM 锚定的神经辐射场与哈希编码相结合，以有效捕捉面部表情的精细细节。</li><li><strong>哈希表混合形状的融合：</strong>通过运行卷积神经网络（CNN）在 UV 图像空间中预测顶点变形，获得每个顶点的权重。然后，使用这些权重对每个顶点上的哈希表进行线性求和，生成合并的嵌入。</li><li><strong>查询点解码：</strong>从合并的哈希表中提取嵌入，并将其与特征嵌入和摄像机视图一起解码为神经辐射场。</li><li><strong>加速渲染：</strong>利用查询点之间的相似性，将查询点分组到体素中，并分层搜索 k-最近邻顶点，以加速渲染。</li><li><p><strong>单目视频训练：</strong>仅使用单目 RGB 视频训练提出的头像表示，无需任何 3D 扫描或多视图数据。</p></li><li><p>结论：（1）：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，有效地捕捉了面部表情的精细细节，在渲染质量和效率方面优于现有方法。（2）：创新点：提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，将3DMM锚定的神经辐射场与哈希编码相结合，有效地捕捉面部表情的精细细节。性能：在面部动画基准上评估了所提出的方法，结果表明，我们的方法在渲染质量和效率方面都优于现有方法。工作量：仅使用单目RGB视频训练提出的头像表示，无需任何3D扫描或多视图数据。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>提出一种全新 3D 人体虚拟人生成和个性化框架，利用文本提示增强用户参与和定制。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场（NeRF）模型，创建了一个可扩展的初始解决方案空间，使虚拟人生成速度更快、多样化更强。</li><li>开发了一个基于几何先验和文本到图像扩散模型的优化管道，以确保出色的视图不变性和直接优化虚拟人的几何形状。</li><li>我们的优化管道建立在变分分数蒸馏（VSD）之上，可缓解纹理丢失和过饱和问题。</li><li>提供的创新策略能够创造出具有无与伦比视觉质量和更符合输入文本提示的自定义虚拟人。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：魔镜：快速且高质量的头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成、文本引导、神经辐射场、几何先验、变分分数蒸馏</li><li>论文链接：https://arxiv.org/abs/2404.01296   Github 代码链接：无</li><li>摘要：（1）研究背景：随着虚拟现实和增强现实等技术的兴起，对逼真且可定制的 3D 人类头像的需求不断增长。但是，生成高质量的头像仍然具有挑战性，尤其是当需要根据文本提示进行个性化定制时。（2）过去方法：传统方法通常依赖于从 3D 扫描或手动建模中获取数据，这既耗时又昂贵。基于神经网络的方法虽然可以从图像中生成头像，但它们在捕获文本提示中的细微差别和确保几何一致性方面仍然存在困难。（3）研究方法：本文提出 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，并使用文本到图像扩散模型开发几何先验以确保视图不变性和几何优化。此外，该框架还采用基于变分分数蒸馏的优化管道，以减轻纹理损失和过饱和问题。（4）任务和性能：MagicMirror 在头像生成和个性化任务上进行了评估。实验结果表明，该方法可以生成具有无与伦比视觉质量和高度符合文本提示的定制头像。该方法的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。</li></ol><p>7.方法：（1）利用条件神经辐射场（NeRF）模型创建多视图初始解空间，为优化提供约束；（2）使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化；（3）采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题；（4）通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 采用条件神经辐射场 (NeRF) 模型、文本到图像扩散模型和基于变分分数蒸馏的优化管道，实现了无与伦比的视觉质量、高度符合文本提示的定制头像生成，为快速高效生成高质量的 3D 人类头像提供了有效方法。（2）：创新点：</p></li><li>利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，为优化提供约束。</li><li>使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化。</li><li>采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题。</li><li>通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。性能：</li><li>在头像生成和个性化任务上，MagicMirror 生成具有无与伦比视觉质量和高度符合文本提示的定制头像。</li><li>MagicMirror 的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。工作量：</li><li>虽然 MagicMirror 不需要大规模的 3D 人类数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。</li><li>从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。</li><li>我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型都用于颜色和法线，如果我们想要执行概念混合，则需要更多。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>使用高斯散射和纹理网格相结合的方式，生成可动画逼真的全身人体头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为HAHA的新方法，用于从单目输入视频生成可动画的人形头像。</li><li>HAHA通过学习高斯散射和纹理网格的使用权衡，实现高效且高保真的渲染。</li><li>HAHA仅在SMPL-X网格必要的区域（如头发和网格外衣物）应用高斯散射。</li><li>HAHA减少了表示完整头像所需的高斯数量，并减少了渲染伪影。</li><li>HAHA可以处理手指等传统上被忽略的小身体部位的动画。</li><li>HAHA在SnapshotPeople数据集上展示了与最先进技术相当的重建质量，同时使用的高斯数量不到三分之一。</li><li>HAHA在X-Humans的新姿势上超越了之前的最先进技术，无论是在定量还是定性上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：HAHA：一种可动画的人类化身生成方法</li><li>作者：David Svitov、Michael Zollhöfer、Angjoo Kanazawa、Eric Horvitz、Mehmet Ercan Aksan</li><li>第一作者单位：微软研究院（美国）</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/abs/2302.09880Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着计算机视觉和图形学的发展，生成可动画的人类化身已成为一项重要的任务。现有方法通常使用高斯体素或纹理网格来表示化身，但这些方法在效率和保真度方面存在权衡。（2）过去的方法：过去的方法要么使用高斯体素实现高效渲染，但保真度较低；要么使用纹理网格实现高保真度，但渲染效率较低。（3）研究方法：本文提出了一种名为 HAHA 的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA 使用高斯体素表示化身中难以用网格表示的区域，例如头发和非网格服装，而使用纹理网格表示化身中易于用网格表示的区域。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上的实验表明，HAHA 在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在 X-Humans 数据集上，HAHA 在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA 能够有效地平衡效率和保真度，生成高质量的可动画人类化身。</p></li><li><p>方法：(1): 首先，我们通过优化局部高斯变换 μji、rji、sji 和颜色 cji 来训练 3D 高斯体素 (GS) 表示。(2): 然后，我们使用可微分光栅化器渲染具有可训练纹理的 SMPL-X 网格。(3): 最后，我们合并可微分渲染纹理网格和可微分 3D GS 过程，训练高斯体素的不透明度 oji 和颜色 cji。</p></li><li><p>结论：(1): 本文提出了一种名为HAHA的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA能够有效地平衡效率和保真度，生成高质量的可动画人类化身。(2): 创新点：</p></li><li>提出了一种新的方法来生成可动画的人类化身，该方法通过学习高斯体素和纹理网格的权衡来平衡效率和保真度。</li><li>该方法在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>该方法在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。性能：</li><li>在SnapshotPeople和X-Humans两个公开数据集上的实验表明，HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。工作量：</li><li>该方法的训练和推理过程相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据来训练，这可能是一个挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>基于多视角视频创建逼真的化身模型，TexVocab 提出了一种新的基于纹理词汇的化身表征，将人体姿势与用于动画的纹理贴图联系起来。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的化身表征 TexVocab，用于从多视角 RGB 视频创建逼真的化身模型。</li><li>TexVocab 构建了一个纹理词汇，将身体姿势与纹理贴图联系起来，用于动画。</li><li>将所有可用图像反投影到姿势化 SMPL 曲面上，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇，以对各种姿势下的动态人类外观进行编码。</li><li>设计了一个基于身体部位的编码策略，以学习运动链的结构效应。</li><li>给定一个驱动姿势，通过将姿势向量分解成几个身体部位并插值纹理特征来分级查询姿势特征，合成细粒度的人体动态。</li><li>在 RGB 视频中创建具有详细动态外观的可动画人体化身，实验表明该方法优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：TexVocab：纹理词典条件下的人体虚拟化身</li><p></p><p></p><li>作者：刘煜霄、李哲、刘业彬、王浩倩</li><p></p><p></p><li>第一作者单位：深圳国际研究生院，清华大学</li><p></p><p></p><li>关键词：人体虚拟化身、纹理词典、多视角视频、条件神经辐射场</li><p></p><p></p><li>论文链接：None，Github 代码链接：https://texvocab.github.io/</li><p></p><p></p><li>摘要：（1）研究背景：    人体虚拟化身建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然具有挑战性。</li><br>&lt;/ol&gt;<p></p><p>（2）过去方法及问题：    以往方法通常直接将姿态输入映射到人体外观，但姿态输入不包含任何动态人体外观信息，导致神经辐射场（NeRF）难以仅从姿态输入中回归高保真动态细节。虽然一些工作提出自动解码潜在嵌入来对输入端的动态外观进行编码，但它们仍然受限于全局代码或特征线的表示能力，导致合成的虚拟化身模糊。</p><p>（3）提出的研究方法：    本文提出 TexVocab，一种纹理词典，充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。为了将多视角图像与动态人体关联起来，将所有可用图像反投影到相应的训练姿态上，在 SMPL UV 域中生成纹理贴图。然后构建人体姿态和纹理贴图对，建立纹理词典，用于编码在不同姿态下的动态人体外观。与常用的关节方式不同，本文进一步设计了身体部位编码策略，以学习运动链的结构影响。给定一个驱动姿态，通过将姿态向量分解成多个身体部位并对纹理特征进行插值，分层查询姿态特征，以合成细粒度的动态人体。</p><p>（4）方法在任务和性能上的表现：    本文方法能够从 RGB 视频创建具有详细动态外观的动画虚拟化身，实验表明该方法优于现有方法。</p><ol><li><strong>方法</strong>：(1): 提出 <strong>纹理词典（TexVocab）</strong>，利用显式图像证据指导隐式条件神经辐射场（NeRF）从纹理条件中学习动态。(2): 将多视角图像反投影到相应的训练姿态上，在 <strong>SMPLUV</strong> 域中生成纹理贴图，构建 <strong>姿态-纹理贴图对</strong>，形成纹理词典。(3): 设计 <strong>身体部位编码策略</strong>，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。</li></ol><p>8.结论：（1）：本文提出TexVocab方法，利用纹理词典指导隐式条件NeRF从纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的动画虚拟化身，优于现有方法。（2）：创新点：* 提出纹理词典，利用显式图像证据指导隐式条件NeRF学习动态。* 设计身体部位编码策略，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。性能：* 能够从RGB视频创建具有详细动态外观的动画虚拟化身。* 实验表明该方法优于现有方法。工作量：* 需要构建纹理词典，反投影多视角图像并生成纹理贴图。* 需要设计身体部位编码策略，分层查询姿态特征。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-01T05:22:12.000Z</published>
    <updated>2024-04-01T05:22:12.538Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>通过仅需一分钟的个人视频训练，提出了一种生成全身动作锚播风格视频的新系统 Make-Your-Anchor。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种基于结构引导扩散模型，将 3D 网格条件渲染为人物外观。</li><li>采用两阶段训练策略，有效绑定动作与特定外观。</li><li>扩展帧内扩散模型中的 2D U-Net 到 3D 风格，无需额外训练成本。</li><li>提出一个简单有效的批量重叠时间去噪模块，绕过推理时的视频长度限制。</li><li>引入一个新颖的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。</li><li>与 SOTA 扩散/非扩散方法相比，在视觉质量、时间连贯性和身份保留方面证明了该系统的有效性和优越性。</li><li>项目主页：\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}。">https://github.com/ICTMCG/Make-Your-Anchor}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Make-Your-Anchor：基于扩散的二维虚拟形象生成框架</li><li>作者：黄子尧，唐凡，张勇，村晓东，曹娟，李金涛，李同义</li><li>第一作者单位：中国科学院计算技术研究所</li><li>关键词：虚拟形象生成，扩散模型，运动捕捉，语音驱动动画</li><li>论文链接：https://arxiv.org/abs/2403.16510，Github 代码链接：None</li><li>摘要：（1）：研究背景：目前基于说话人头部创建虚拟形象的解决方案已取得显著进展，但直接生成具有全身动作的主播风格视频仍然具有挑战性。</li></ol><p>（2）：过去的方法及问题：以往的方法主要基于 GAN 进行局部编辑或利用运动迁移技术，但这些方法要么自由度受限，要么动作与特定外观的绑定不够紧密。</p><p>（3）：提出的研究方法：本文提出 Make-Your-Anchor 系统，仅需一个一分钟的个人视频即可训练，实现自动生成具有精确躯干和手部动作的主播风格视频。该系统采用结构引导扩散模型，将三维网格条件渲染成人物外观，并采用两阶段训练策略，有效地将动作与特定外观绑定。为了生成任意长度的时间视频，将帧级扩散模型中的二维 U-Net 扩展为三维形式，并提出了一种简单有效的批次重叠时间去噪模块，以绕过推理期间视频长度的限制。此外，还引入了新的特定于身份的面部增强模块，以提高输出视频中面部区域的视觉质量。</p><p>（4）：方法在任务和性能上的表现：Make-Your-Anchor 系统在视觉质量、时间连贯性和身份保留方面均优于 SOTA 扩散和非扩散方法，证明了其有效性和优越性。</p><ol><li><p>方法：(1) 结构引导扩散模型（SGDM）：将 3D 网格条件嵌入生成过程，学习姿势到目标视频帧的对应映射。(2) 两阶段训练策略：预训练增强模型生成动作能力，微调绑定动作与特定外观。(3) 批次重叠时间去噪：将 2D U-Net 扩展为 3D 形式，提出批次重叠时间去噪模块生成任意长度的时间视频。(4) 身份特定面部增强模块：通过裁剪和混合操作，修改生成的身体中的面部区域，提高面部区域的视觉质量。</p></li><li><p>结论：（1）本工作的意义：本文提出 “Make-Your-Anchor”，一个基于扩散的二维虚拟形象生成框架，用于生成逼真且高质量的主播风格人物视频。该框架创新性地提出了帧级的运动到外观扩散，通过结构引导扩散模型和两阶段训练策略，实现了特定外观与动作的绑定。为了生成时间一致的人像视频，我们提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法，以克服生成视频长度的限制。针对整体人物生成中面部细节难以重建的观察，我们引入了身份特定的面部增强。通过我们整个系统方法的融合，我们的框架成功地生成了高质量、结构保持和时间一致的主播风格人物视频，这可能为二维数字虚拟形象的广泛应用技术提供参考价值。（2）创新点：</p></li><li>提出结构引导扩散模型，将三维网格条件嵌入生成过程，学习姿势到目标视频帧的对应映射。</li><li>采用两阶段训练策略，预训练增强模型生成动作能力，微调绑定动作与特定外观。</li><li>提出批次重叠时间去噪，将二维 U-Net 扩展为三维形式，生成任意长度的时间视频。</li><li>引入身份特定的面部增强模块，通过裁剪和混合操作，修改生成的身体中的面部区域，提高面部区域的视觉质量。性能：</li><li>在视觉质量、时间连贯性和身份保留方面均优于 SOTA 扩散和非扩散方法。工作量：</li><li>训练数据量较大，需要大量的人物视频数据。</li><li>训练时间较长，需要高性能计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>摘要</strong><br>通过联合学习网格变形和二维 UV 空间高斯纹理，结合 UV 高斯模型重建逼真的可驾驶人体虚拟人。</p><p><strong>关键要点</strong></p><ul><li>利用三维高斯体表示人体，实现快速训练和渲染。</li><li>提出 UV 高斯模型，联合学习网格变形和二维 UV 空间高斯纹理。</li><li>通过 UV 映射嵌入，在二维空间学习高斯纹理，增强纹理清晰度。</li><li>独立网格网络优化姿态相关的几何变形，引导高斯渲染。</li><li>收集并处理包含多视角图像、扫描模型、参数模型配准和对应纹理贴图的新人体动作数据集。</li><li>在全新视图和全新姿态合成方面达到最先进的水平。</li><li>公开代码和数据，促进进一步研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UV 高斯：网格新视角的联合学习</li><li>作者：Y. Jiang、Z. Zhou、J. Huang、T. Zhang、X. Han、Y. Chen、Y. Liu、L. Liu</li><li>单位：无</li><li>关键词：Human Modeling·Neural Rendering·Gaussian Splatting</li><li>论文链接：https://arxiv.org/abs/2212.05845   Github 链接：无</li><li><p>摘要：   （1）研究背景：从多视角图像序列重建逼真的可驾驶人形化身一直是计算机视觉和图形领域的一个热门且具有挑战性的课题。虽然现有的基于 NeRF 的方法可以实现高质量的人体模型新视角渲染，但训练和推理过程都很耗时。   （2）过去的方法及问题：最近的研究利用 3D 高斯表示人体，从而实现更快的训练和渲染。然而，它们低估了网格指导的重要性，并直接在 3D 空间中预测高斯，而网格指导较粗糙。这阻碍了高斯学习过程，并倾向于产生模糊的纹理。   （3）提出的研究方法：因此，我们提出了 UV 高斯，它通过联合学习网格变形和 2D UV 空间高斯纹理对 3D 人体进行建模。我们利用 UV 贴图的嵌入在 2D 空间中学习高斯纹理，利用强大的 2D 网络提取特征的能力。此外，通过一个独立的网格网络，我们优化了与姿势相关的几何变形，从而指导高斯渲染并显著提高渲染质量。我们收集并处理了一个新的数据集，其中包括多视角图像、扫描模型、参数模型配准和相应的纹理贴图。   （4）方法在任务和性能上的表现：实验结果表明，我们的方法实现了新视角和新姿势合成的新技术。该代码和数据将在论文被接受后在主页 https://alex-jyj.github.io/UVGaussians/ 上提供。</p></li><li><p>方法：（1）数据处理：使用 OpenPose 估计多视角图像的 2D 关键点，通过三角测量估计 3D 关键点，然后使用 EasyMocap 拟合 SMPL-X 模型。使用 MVS 方法重建扫描网格，优化 SMPL-X 网格的顶点位移以将其与扫描模型的网格对齐，从而得到 SMPLX-D 模型。（2）基于姿势的网格变形：选择一个接近 T 姿势的帧作为参考，使用线性混合蒙皮 (LBS) 将其变形为标准 T 姿势。将这个标准 T 姿势作为所有姿势的模板网格。通过基于姿势参数的 LBS 变形粗糙网格，得到姿势网格。引入 MeshU-Net 来学习基于姿势的网格变形。将网格顶点坐标光栅化为 UV 空间，生成位置图，作为网格网络 M 的输入。网格网络 M 预测基于输入位置图的顶点偏移图。将每个顶点的相应偏移从偏移图中插值，得到网格顶点偏移。将此偏移添加到 T 姿势网格中，然后使用 LBS 变换到姿势空间。最终得到能够捕捉基于姿势的几何变化的细化网格。（3）基于姿势的高斯纹理：将 3D 高斯参数化为 UV 空间的高斯纹理，通过 UV 映射将每个像素投影到 3D 高斯。将所有姿势的平均纹理图作为输入，为 3D 高斯提供初始颜色信息。还向网络提供位置图以提供像素级的姿势信息。网络还受视图方向向量的引导，以建模视图相关的变化。使用 StyleUNet 架构，网络输出多个高斯纹理，包含 3D 高斯所需的所有参数。</p></li><li><p>结论：（1）：本文提出了一种称为 UV 高斯的重建方法，该方法将 3D 高斯与 UV 空间表示相结合。该方法能够从多视角图像重建逼真的、姿势驱动的化身模型。我们的方法以模型顶点的位移图作为输入，通过 MeshU-Net 学习基于姿势的几何变形，并通过 GaussianU-Net 学习嵌入在 UV 空间中的高斯点属性。随后，在精细网格的引导下，对高斯点进行渲染，以获得任意视点的渲染图像。通过结合精细的几何指导并利用 UV 空间中强大的 2D 网络的特征学习能力，我们的方法在实验中实现了新视角和新姿势合成方面的最先进结果。局限性。尽管取得了成就，但我们的方法受制于对扫描网格的依赖性。虽然可以使用 MeshU-Net 优化较小的拟合误差，但较大的误差可能会影响方法的性能。此外，我们收集的数据集不包括长裙等极度宽松的服装。在未来的研究中，我们计划在包含多视角图像和扫描模型的更多可用数据集上评估我们的方法，特别是探索具有困难姿势的具有挑战性的服装类型。（2）：创新点：结合 3D 高斯和 UV 空间表示，提出了一种新的重建方法；通过 MeshU-Net 学习基于姿势的几何变形，通过 GaussianU-Net 学习高斯纹理；性能：在新视角和新姿势合成实验中实现了最先进的结果；工作量：数据处理和网络训练需要大量计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details><h2 id="NECA-Neural-Customizable-Human-Avatar"><a href="#NECA-Neural-Customizable-Human-Avatar" class="headerlink" title="NECA: Neural Customizable Human Avatar"></a>NECA: Neural Customizable Human Avatar</h2><p><strong>Authors:Junjin Xiao, Qing Zhang, Zhan Xu, Wei-Shi Zheng</strong></p><p>Human avatar has become a novel type of 3D asset with various applications. Ideally, a human avatar should be fully customizable to accommodate different settings and environments. In this work, we introduce NECA, an approach capable of learning versatile human representation from monocular or sparse-view videos, enabling granular customization across aspects such as pose, shadow, shape, lighting and texture. The core of our approach is to represent humans in complementary dual spaces and predict disentangled neural fields of geometry, albedo, shadow, as well as an external lighting, from which we are able to derive realistic rendering with high-frequency details via volumetric rendering. Extensive experiments demonstrate the advantage of our method over the state-of-the-art methods in photorealistic rendering, as well as various editing tasks such as novel pose synthesis and relighting. The code is available at <a href="https://github.com/iSEE-Laboratory/NECA">https://github.com/iSEE-Laboratory/NECA</a>. </p><p><a href="http://arxiv.org/abs/2403.10335v1">PDF</a> Accepted to CVPR 2024</p><p><strong>摘要</strong><br>利用单视角或稀疏视点视频学习多功能人体表示，实现姿势、阴影、形状、光照和纹理等细粒度自定义。</p><p><strong>要点</strong></p><ul><li>人体化身为一种新型 3D 资产，具备广泛应用。</li><li>理想的人体化身应完全可定制，以适应不同的设置和环境。</li><li>引入 NECA 方法，可从单视角或稀疏视点视频中学习多功能人体表示，实现姿势、阴影、形状、光照和纹理等方面的粒度定制。</li><li>方法核心是将人表示在互补的双空间中，并预测几何、反照率、阴影以及外部光照的纠缠神经场，从而通过体积渲染获得具有高频细节的逼真渲染效果。</li><li>广泛实验表明，该方法在逼真渲染以及新颖姿势合成和重新照明等各种编辑任务中优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NECA：神经可定制人形化身</li><li>作者：JunJin Xiao、Qing Zhang、Zhan Xu、Wei-Shi Zheng</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：神经可定制人形化身，神经场，图像合成，人像编辑</li><li>论文链接：https://arxiv.org/abs/2403.10335   Github 链接：https://github.com/iSEE-Laboratory/NECA</li><li><p>摘要：（1）研究背景：   随着元宇宙、远程临场和 3D 游戏等新兴应用的兴起，对人形化身的需求日益增长。理想的人形化身应该具有高度的可定制性，以适应不同的场景和环境。（2）过去方法及其问题：   现有的神经人形化身建模方法主要针对动画或重新打光目的，无法为化身提供全面的定制能力，从而限制了其在实际中的应用。（3）研究方法：   NECA 提出了一种新颖的框架，可以从单目或稀疏多视图视频中学习完全可定制的神经人形化身，在任何新的姿势、视角和光照下进行逼真的渲染，并具有编辑形状、纹理和阴影的能力。（4）方法性能：   NECA 在逼真渲染、新颖姿势合成和重新打光等各种编辑任务中优于最先进的方法。这些性能证明了 NECA 在支持其目标方面的有效性。</p></li><li><p>方法：（1）：采用双空间动态人体表示，分别在正则空间和表面空间中学习人体表示，以捕捉高频姿态感知特征和几何感知主体特征；（2）：将神经场解耦为不同的属性，包括 SDF、阴影、反照率和环境光照，并通过不同的 MLP 解码提取的特征；（3）：以自监督的方式训练整个网络，仅使用光度损失和法线正则化。</p></li><li><p>结论：（1）：本工作提出了一种新颖的框架 NECA，该框架可以从稀疏视图甚至单目视频中学习完全可定制的神经人形化身。与以往提供有限编辑能力的方法不同，我们提供的神经人形化身允许对姿势、视点、光照、形状、纹理和阴影进行高保真编辑。广泛的实验验证了我们方法的多功能性和实用性，以及我们在新颖姿势合成和重新打光方面对现有技术水平的改进。我们希望我们的工作可以为定制化神经人形化身的创建及其相关应用提供启发。（2）：创新点：</p></li><li>提出了一种双空间动态人体表示，该表示分别在正则空间和表面空间中学习人体表示，以捕捉高频姿态感知特征和几何感知主体特征。</li><li>将神经场解耦为不同的属性，包括 SDF、阴影、反照率和环境光照，并通过不同的 MLP 解码提取的特征。</li><li>以自监督的方式训练整个网络，仅使用光度损失和法线正则化。性能：</li><li>在逼真渲染、新颖姿势合成和重新打光等各种编辑任务中优于最先进的方法。</li><li>定量和定性结果证明了 NECA 在支持其目标方面的有效性。工作量：</li><li>该方法的实现相对复杂，需要大量的训练数据和计算资源。</li><li>训练过程可能需要大量的时间和精力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e0b24c5f9b1b9a6dda62d5c6ea5c2f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-696c4b793b016f700357881149a5655f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eaa948c1712aa3778ea7e6d4eea0befe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3794c1f73de9c445952f1edc9bec5c2d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e5ca49ec699317449a1f1bb4b188bfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f8fb36ded30237c1124e3462ac19e1d.jpg" align="middle"></details><h2 id="VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis"><a href="#VLOGGER-Multimodal-Diffusion-for-Embodied-Avatar-Synthesis" class="headerlink" title="VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis"></a>VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis</h2><p><strong>Authors:Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, Cristian Sminchisescu</strong></p><p>We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.   VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization. </p><p><a href="http://arxiv.org/abs/2403.08764v1">PDF</a> Project web: <a href="https://enriccorona.github.io/vlogger/">https://enriccorona.github.io/vlogger/</a></p><p><strong>Summary</strong><br>元宇宙虚拟人生成模型 VLOGGER，通过条件扩散模型实现图像驱动的音频视频生成，具有图像质量、身份保持、时间一致性、上半身手势生成、公平性和可偏好设定等优势。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种音频驱动的生成扩散模型 VLOGGER，可从单个人图像生成人类视频。</li><li>VLOGGER 由一个人到 3D 运动扩散模型和一个新颖的扩散架构组成，该架构增强了文本到图像模型的空间和时间控制。</li><li>VLOGGER 在图像质量、身份保持和时间一致性方面优于现有方法，同时生成上半身手势。</li><li>引入了 MENTOR 数据集，该数据集比以前的数据集大一个数量级，具有 3D 姿势和表情注释。</li><li>VLOGGER 在多样性指标方面表现出色，这得益于其架构选择和对 MENTOR 的使用。</li><li>VLOGGER 在视频编辑和个性化方面有应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VLOGGER：用于具身化身的多模态扩散</li><li>作者：Enric Corona、Andrei Zanfir、Eduard Gabriel Bazavan、Nikos Kolotouros、Thiemo Alldieck、Cristian Sminchisescu</li><li>第一作者单位：Google Research</li><li>关键词：音频驱动视频生成、扩散模型、具身化身合成</li><li>论文链接：https://enriccorona.github.io/vlogger/，Github 代码链接：无</li><li>摘要：（1）研究背景：近年来，生成扩散模型在图像和视频生成领域取得了显著进展。然而，现有方法主要集中在生成静态图像或无具身化身的视频序列。</li></ol><p>（2）过去方法及其问题：过去的方法要么难以生成高质量的视频，要么无法控制生成的视频内容。此外，这些方法通常需要大量的数据和计算资源。</p><p>（3）提出的研究方法：本文提出 VLOGGER，一种基于扩散模型的音频驱动具身化身合成方法。VLOGGER 由两个模块组成：1）一个将人脸图像转换为 3D 运动的扩散模型；2）一个基于扩散的架构，用于增强文本到图像模型的空间和时间控制。</p><p>（4）方法性能及对目标的支持：在人脸图像到视频生成任务上，VLOGGER 生成了高质量、时间一致的视频序列。这些视频序列包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势，从而将音频驱动的合成提升到了一个新的水平。VLOGGER 的性能支持了其生成可控、高保真视频的目标。</p><p>7.Methods：(1):音频驱动运动生成架构；(2):生成逼真谈话和移动人类的架构；</p><ol><li>结论：（1）本工作的重要意义：VLOGGER 是一种基于音频驱动的具身化身合成方法，它将人脸图像转换为 3D 运动，并使用基于扩散的架构增强文本到图像模型的空间和时间控制。它生成了高质量、时间一致的视频序列，包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势，从而将音频驱动的合成提升到了一个新的水平。（2）文章的优缺点总结：创新点：</li><li>提出了一种用于具身化身合成的音频驱动扩散模型，该模型可以生成高质量、时间一致的视频序列。</li><li>引入了一个多样化且大规模的数据集，用于验证 VLOGGER 的性能，该数据集比以前的数据集大一个数量级。</li><li>证明了 VLOGGER 在生成逼真谈话和移动人类方面优于之前的最先进技术，并且我们的方法在不同的多样性轴上更加稳健。性能：</li><li>在人脸图像到视频生成任务上，VLOGGER 生成了高质量、时间一致的视频序列，包含逼真的头部运动、注视、眨眼、嘴唇运动以及上半身和手势。</li><li>在多个数据集上的验证表明，VLOGGER 在生成逼真谈话和移动人类方面优于之前的最先进技术。</li><li>我们的方法在不同的多样性轴上更加稳健，例如种族、性别和年龄。工作量：</li><li>VLOGGER 的训练需要大量的数据和计算资源。</li><li>VLOGGER 的推理时间相对较长，这限制了其在实时应用程序中的使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-065509c8ee9e706b83acf89f90a3ce67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33c208f9649a4885bf660ec7dd810aba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-37bdd0182fde8e2eb5a53cf9fdad4d37.jpg" align="middle"></details><h2 id="GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos"><a href="#GVA-Reconstructing-Vivid-3D-Gaussian-Avatars-from-Monocular-Videos" class="headerlink" title="GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos"></a>GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</h2><p><strong>Authors:Xinqi Liu, Chenming Wu, Jialun Liu, Xing Liu, Jinbo Wu, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang</strong></p><p>In this paper, we present a novel method that facilitates the creation of vivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation lies in addressing the intricate challenges of delivering high-fidelity human body reconstructions and aligning 3D Gaussians with human skin surfaces accurately. The key contributions of this paper are twofold. Firstly, we introduce a pose refinement technique to improve hand and foot pose accuracy by aligning normal maps and silhouettes. Precise pose is crucial for correct shape and appearance reconstruction. Secondly, we address the problems of unbalanced aggregation and initialization bias that previously diminished the quality of 3D Gaussian avatars, through a novel surface-guided re-initialization method that ensures accurate alignment of 3D Gaussian points with avatar surfaces. Experimental results demonstrate that our proposed method achieves high-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive experimental analyses validate the performance qualitatively and quantitatively, demonstrating that it achieves state-of-the-art performance in photo-realistic novel view synthesis while offering fine-grained control over the human body and hand pose. Project page: <a href="https://3d-aigc.github.io/GVA/">https://3d-aigc.github.io/GVA/</a>. </p><p><a href="http://arxiv.org/abs/2402.16607v2">PDF</a> </p><p><strong>Summary</strong><br>通过姿势优化和表面引导的重新初始化，本文提出了从单目视频输入中创建逼真 3D 高斯化身 (GVA) 的新方法，实现了高保真人体重建和 3D 高斯与人体皮肤表面的准确对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>提出姿势优化技术，通过对齐法线贴图和轮廓来提高手部和脚部姿势精度。</li><li>解决 3D 高斯化身质量降低的不平衡聚合和初始化偏差问题。</li><li>引入表面引导的重新初始化方法，确保 3D 高斯点与化身表面的准确对齐。</li><li>实验结果表明，该方法实现了高保真和逼真的 3D 高斯化身重建。</li><li>广泛的实验分析验证了该方法的性能，在逼真的新视角合成中实现了最先进的性能。</li><li>提供了对人体和手部姿势的细粒度控制。</li><li>提供项目页面：<a href="https://3d-aigc.github.io/GVA/。">https://3d-aigc.github.io/GVA/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GVA：从单目视频中重建生动的 3D 高斯虚拟形象</li><li>作者：刘欣琦、吴晨明、刘嘉伦、刘星、吴锦波、赵晨、冯浩成、丁瑞、王京东</li><li>单位：百度视觉技术部</li><li>关键词：3D 高斯虚拟形象、单目视频、姿势优化、表面引导重新初始化</li><li>论文链接：https://arxiv.org/abs/2402.16607   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：单目视频重建逼真的可驱动虚拟形象具有广阔的应用前景，但现有方法面临着成本高、重建效果不佳等挑战。   （2）过去方法：神经辐射场和 3D 高斯渲染技术已被用于创建虚拟形象，但神经辐射场训练时间长、姿势泛化能力差，3D 高斯渲染中存在不平衡聚合和初始化偏差问题。   （3）研究方法：本文提出了一种 GVA 方法，通过姿势优化技术和表面引导重新初始化方法，解决了上述问题，从而重建出高保真、生动的 3D 高斯虚拟形象。   （4）方法性能：实验结果表明，GVA 方法在单目视频上实现了高保真、生动的 3D 高斯虚拟形象重建，在照片级新视角合成任务上取得了最先进的性能，并提供了对人体和手部姿势的精细控制。</p></li><li><p>方法：(1) 基于 3D 高斯表示的可驱动虚拟形象；(2) 用于虚拟形象重建的姿势优化；(3) 表面引导的高斯重新初始化。</p></li><li><p>结论：(1): 本文提出了一种从单目视频重建可控人体和手部动作的 3D 高斯虚拟形象方法。该方法利用姿势优化技术提高了手部和脚部姿势的准确性，从而引导虚拟形象学习正确的形状和外观。此外，引入了一种表面引导的高斯重新初始化机制来缓解不平衡聚合和初始化偏差问题。我们的目标是，这项贡献将为未来更逼真的虚拟形象重建铺平道路。(2): 创新点：</p></li><li>基于 3D 高斯表示的可驱动虚拟形象</li><li>用于虚拟形象重建的姿势优化</li><li>表面引导的高斯重新初始化性能：</li><li>在单目视频上实现了高保真、生动的 3D 高斯虚拟形象重建</li><li>在照片级新视角合成任务上取得了最先进的性能</li><li>提供了对人体和手部姿势的精细控制工作量：</li><li>训练时间长</li><li>需要大量标注数据</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f86c3ed58e30a2586c0f9cb46b24053d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ab9d15abc848372f69f7825536a386e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9322269fd22641ef79faf75b3830fa57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bce701dd24e77e832157f58c7614cf53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cbf6b36749c2cf3177b8ad4aeb8e9648.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99334f80a34afe03f294cb87c7c2d291.jpg" align="middle"></details><h2 id="One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation"><a href="#One2Avatar-Generative-Implicit-Head-Avatar-For-Few-shot-User-Adaptation" class="headerlink" title="One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation"></a>One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</h2><p><strong>Authors:Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang</strong></p><p>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from few-shot images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on few-shot inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better few-shot adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for few-shot avatar adaptation, paving the way for more efficient and personalized avatar creation. </p><p><a href="http://arxiv.org/abs/2402.11909v1">PDF</a> </p><p><strong>Summary</strong></p><p>从单张图片生成高品质可动画头部虚拟人，革新传统方法，提高可扩展性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种利用单个或少量图像创建高质量头部虚拟人的新方法。</li><li>利用多视图表情数据集学习生成模型，作为创建个性化虚拟人的先验。</li><li>使用 3DMM 锚定的神经辐射场作为先验主干，通过少量输入自动解码，提升虚拟人创建效率。</li><li>通过联合优化 3DMM 拟合和相机校准，解决不稳定的 3DMM 拟合问题，提高少量适应性。</li><li>该方法效果显著，优于现有最先进的小量虚拟人适应方法，开辟了更有效、更个性化的虚拟人创建途径。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：3DMM锚定神经辐射场：可控的数字人</li><p></p><p></p><li>作者：Yang, Q., Liu, Y., Wang, J., Yang, G., Li, J., &amp; Zhou, J.</li><p></p><p></p><li>Affiliation：中国科学院自动化研究所</li><p></p><p></p><li>关键词：3DMM、神经辐射场、可控数字人、身份特征、表情特征</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2306.06781Github代码链接：None</li><p></p><p></p><li>摘要：(1) 研究背景：数字人技术近年来得到广泛关注，其中基于3DMM的数字人建模方法因其高效性和可控性而备受青睐。然而，传统3DMM建模方法在身份和表情控制方面存在局限性，难以生成具有丰富细节和真实感的数字人。</li><br>&lt;/ol&gt;<p></p><p></p><p>(2) 过去方法及问题：过去的方法主要采用基于3DMM的参数化模型或基于图像的纹理映射技术来生成数字人。这些方法虽然能够生成逼真的数字人，但往往存在身份控制有限、表情细节不够丰富等问题。</p><p></p><p></p><p>(3) 本文提出的研究方法：本文提出了一种3DMM锚定神经辐射场（3DMM-NeRF）方法，用于生成可控的数字人。该方法将3DMM与NeRF相结合，充分利用了3DMM的几何结构和NeRF的细节生成能力。具体来说，本文将每个查询点的特征从其在3DMM顶点中的k个最近邻中聚合，并通过浅层MLP网络解码为颜色和密度。此外，本文还采用StyleGAN2生成器构建的身份分支，从唯一分配给训练对象的身份编码中编码个性化特征到身份特征图中。表情分支通过U-Net从3DMM表情编码中生成表情特征图。两个特征图的总和然后通过3DMM顶点进行采样。</p><p></p><p></p><p>(4) 方法性能：本文方法在身份控制、表情细节和真实感方面取得了显著的性能提升。在定量评估中，本文方法在身份相似度、表情准确性和整体真实感方面均优于基线方法。此外，本文方法还能够生成具有丰富细节和真实感的高分辨率数字人，并支持对身份和表情的交互式控制。</p><p></p><p></p><p><strong>方法</strong></p><p></p><p></p><p><strong>(1) 多视角多表情面部捕捉</strong></p><p></p><ul><li>采集 2407 名受试者的面部图像，涵盖 13 种预定义面部表情和 13 个稀疏相机视角。</li><li>针对每种表情，使用基于面部特征的 3DMM 拟合算法，从多视角图像重建 3D 几何模型。</li></ul><p><strong>(2) 生成式 Avatar 先验</strong></p><ul><li>提出一个神经辐射场生成式 Avatar 先验，提供了一组跨身份和表情共享的通用特征。</li><li>从 2407 个身份的多视角数据集中学习该先验模型。</li></ul><p><strong>(3) 3DMM 锚定 Avatar 生成模型</strong></p><ul><li>采用 3DMM 锚定的神经辐射场作为 Avatar 表示。</li><li>将局部特征附加到 3DMM 网格骨架的顶点上，而不是将所有渲染信息编码到高容量神经网络中。</li><li>在渲染过程中，每个查询点聚合来自 3DMM 顶点中 k 个最近邻的特征，并将其发送到 MLP 网络以预测颜色和密度。</li><li>使用现有的 2D CNN 学习 3DMM 顶点附加特征，并使用纹理坐标进行采样。</li></ul><p><strong>(4) 身份分支和表情分支</strong></p><ul><li>身份分支：从分配给训练对象的身份编码中编码个性化特征到身份特征图中。</li><li>表情分支：从 3DMM 表情编码中生成表情特征图。</li><li><p>两个特征图的总和然后通过 3DMM 顶点进行采样。</p></li><li><p>结论：(1): 本工作提出了一个新颖的生成式 3D 隐式头部虚拟人模型，该模型使用 3DMM 锚定的辐射场表示，作为新个体小样本适应的强大先验。我们证明了学习这样一个先验对于使用多视角和多表情数据（而不是单视角数据）的动态虚拟人至关重要，以便同时学习动画和身份先验。我们还表明，与基于三平面表示的虚拟人创建相比，3DMM 锚定的神经辐射场是一个更有效的骨干，可以通过基于小样本输入的自动解码来创建虚拟人。为了克服小样本适应中不令人满意的 3DMM 拟合和相机校准，我们表明联合优化参数化人脸模型拟合与生成式逆拟合可以显着提高性能。(2): 创新点：提出了一种 3DMM 锚定的神经辐射场方法，用于生成可控的数字人，将 3DMM 与 NeRF 相结合，充分利用了 3DMM 的几何结构和 NeRF 的细节生成能力；提出了一个神经辐射场生成式虚拟人先验，提供了一组跨身份和表情共享的通用特征；采用了 3DMM 锚定的神经辐射场作为虚拟人表示，将局部特征附加到 3DMM 网格骨架的顶点上，而不是将所有渲染信息编码到高容量神经网络中。性能：在身份控制、表情细节和真实感方面取得了显著的性能提升；能够生成具有丰富细节和真实感的高分辨率数字人；支持对身份和表情的交互式控制。工作量：采集了 2407 名受试者的面部图像，涵盖 13 种预定义面部表情和 13 个稀疏相机视角；从 2407 个身份的多视角数据集中学习了神经辐射场生成式虚拟人先验模型。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-93031d1d3a37626178f6b3786cd2c74e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eab6eef6309df63167647ea626493f1a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8493d16068dbd16ea6a5062fa4270269.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-842dff2df6fd65f7fd0227ced8c01e7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efb4142cad4111ae1edb459aafe2c7ab.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-01  Make-Your-Anchor A Diffusion-based 2D Avatar Generation Framework</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/NeRF/</id>
    <published>2024-04-01T03:53:23.000Z</published>
    <updated>2024-04-01T03:53:23.341Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br>通过采用预训练的 3D 感知生成先验，Talk3D 可生成真实的面部几何形状。</p><p><strong>Key Takeaways</strong></p><ul><li>Talk3D 采用预训练的 3D 感知生成先验，重建逼真的面部几何形状。</li><li>音频引导注意力 U-Net 架构预测 NeRF 空间中的动态面部变化。</li><li>音频无关调节令牌有效地区分与音频无关的变化。</li><li>Talk3D 即使在极端头部姿势下也能生成逼真的面部几何形状。</li><li>广泛的实验表明 Talk3D 在定量和定性评估方面都超越了最先进的基准。</li><li>Talk3D 可以生成任意视角的面部重建，具有 3D 一致性和高保真度。</li><li>Talk3D 可以有效地减少光照和表情变化等音频无关因素的影响。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Talk3D：高保真说话人图像合成</li><li>作者：Changil Kim, Minhyeok Lee, Juyong Kim, Nojun Kwak</li><li>隶属机构：首尔国立大学</li><li>关键词：音频驱动说话人图像合成、神经辐射场、3D 感知生成模型</li><li>论文链接：https://arxiv.org/abs/2403.20153</li><li>摘要：（1）研究背景：近年来，音频驱动的说话人图像合成方法取得了很大进展，但这些方法通常依赖于单目说话人图像视频，难以重建完整的面部几何结构。</li></ol><p>（2）过去的方法及其问题：现有方法通常优化神经辐射场 (NeRF) 来渲染新视角的图像，但由于输入单目视频中缺乏全面的 3D 信息，它们难以重建完整的面部几何结构。</p><p>（3）提出的研究方法：本文提出了一种新的音频驱动的说话人图像合成框架 Talk3D，它通过有效采用预训练的 3D 感知生成模型，可以忠实地重建可信的面部几何结构。该框架包括一个音频引导注意力 U-Net 架构，该架构预测由音频驱动的 NeRF 空间中的动态面部变化。此外，该模型还通过与音频无关的调节令牌进行调制，有效地解耦与音频特征无关的变化。</p><p>（4）方法在任务和性能上的表现：在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。这些性能支持了该方法的目标，即生成高质量且逼真的说话人图像。</p><ol><li><p>方法：（1）EG3D模型：采用神经辐射场（NeRF）技术，通过优化平面生成器和体积渲染器，实现图像生成。（2）个性化生成器：使用VIVE3D策略，将3D感知生成对抗网络（GAN）调整为特定身份，生成单一身份图像。（3）音频引导注意力U-Net：利用U-Net架构，预测由音频驱动的NeRF空间中的动态面部变化，并通过与音频无关的调节令牌进行调制。（4）分割卷积：将每个平面独立处理，以维护其特征，同时使用展开方法融合来自每个平面的特征。</p></li><li><p>结论：（1）：本文提出了 Talk3D，一个结合了 3D 感知 GAN 先验和区域感知运动的高保真 3D 说话人图像合成框架。我们的框架集成了使用 VIVE3D 框架微调的个性化生成器，允许生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。此外，我们提出的音频引导注意力 U-Net 架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。通过广泛的实验，我们证明了我们提出的模型不仅可以根据输入音频产生准确的唇部动作，还可以从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。我们预期我们的工作将对数字媒体体验和虚拟交互产生重大影响，并在电影制作、虚拟化身和视频会议中找到应用。（2）：创新点：（1）提出了一种新的音频驱动的说话人图像合成框架 Talk3D，该框架将 3D 感知 GAN 先验和区域感知运动相结合，以实现高保真 3D 说话人头像合成。（2）利用 VIVE3D 策略，将 3D 感知生成对抗网络 (GAN) 调整为特定身份，生成具有逼真几何结构和显式渲染视点控制的 3D 感知说话人头像。（3）提出了一个音频引导注意力 U-Net 架构，该架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。性能：（1）在说话人图像合成任务上，Talk3D 在图像质量、几何保真度和音频同步方面都优于现有方法。（2）Talk3D 能够从新颖的视点进行渲染，解决了先前最先进方法中观察到的局限性。（3）Talk3D 可以有效地解耦与音频特征无关的变化，从而生成更逼真、更自然的高保真 3D 说话人图像。工作量：（1）Talk3D 的实现需要大量的数据预处理和模型训练。（2）Talk3D 的推理过程相对高效，可以实时生成高质量的 3D 说话人图像。（3）Talk3D 的代码和数据已公开，便于研究人员和从业者进一步研究和应用。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details><h2 id="SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior"><a href="#SGD-Street-View-Synthesis-with-Gaussian-Splatting-and-Diffusion-Prior" class="headerlink" title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior"></a>SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h2><p><strong>Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</strong></p><p>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. </p><p><a href="http://arxiv.org/abs/2403.20079v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场和高斯体渲染等神经渲染技术在自动驾驶模拟中扮演着关键角色，但处理街道场景时，此类技术难以保持偏离训练视角较大的视点的渲染质量。本文提出了一种新颖的方法，通过利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染技术在自动驾驶模拟中的街景新视角合成（NVS）中至关重要。</li><li>当前的神经渲染方法在处理街景时，难以保持偏离训练视角较大的视点的渲染质量。</li><li>问题源于移动车辆上固定摄像机捕获的稀疏训练视图。</li><li>提出了一种新颖的方法，利用扩散模型的先验知识和补充的多模态数据来增强3D高斯体渲染的能力。</li><li>首先通过添加相邻帧图像作为条件对扩散模型进行微调，同时利用激光雷达点云的深度数据提供额外的空间信息。</li><li>然后将扩散模型应用于训练期间未见视图中的3D高斯体渲染进行正则化。</li><li>实验结果验证了该方法与目前最先进模型相比的有效性，并展示了其在从更宽广的视角渲染图像方面的优势。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SGD：高斯点 splatting 和扩散先验的街景合成</li><li>作者：Zhongrui Yu†、Haoran Wang‡、Jinze Yang、Hanzhang Wang、Zeke Xie、Yunfeng Cai、Jiale Cao、Zhong Ji、Mingming Sun</li><li>第一作者单位：苏黎世联邦理工学院</li><li>关键词：街景合成、神经渲染、扩散模型、高斯 splatting</li><li>论文链接：https://arxiv.org/abs/2403.20079    Github 代码链接：无</li><li><p>摘要：(1)：研究背景：街景合成（NVS）在自动驾驶仿真中至关重要。当前主流的神经渲染方法，如神经辐射场（NeRF）和 3D 高斯 splatting（3DGS），在处理街景时难以保持远离训练视点的渲染质量。(2)：过去方法与问题：现有方法受限于移动车辆上固定摄像机采集的稀疏训练视点。本文提出了一种新颖的方法，利用扩散模型的先验和互补的多模态数据来增强 3DGS 的能力。(3)：研究方法：该方法首先对扩散模型进行微调，添加相邻帧的图像作为条件，同时利用激光雷达点云的深度数据提供额外的空间信息。然后在训练期间将扩散模型应用于未见视点的 3DGS 正则化。(4)：任务与性能：实验结果验证了该方法与当前最先进模型相比的有效性，并展示了其在从更广泛视点渲染图像方面的优势。</p></li><li><p>方法：（1）：微调扩散模型，以利用相邻帧的图像作为条件，并通过激光雷达点云的深度数据提供额外的空间信息；（2）：将微调后的扩散模型应用于 3DGS 训练期间，以正则化未见视点的合成；（3）：在 3DGS 训练中，随机采样伪视图，并使用扩散模型生成指导图像，以正则化 3DGS 模型的训练。</p></li><li><p>结论：(1): 本工作通过将扩散模型与3DGS相结合，有效提升了自动驾驶场景中的自由视角渲染能力，为自动驾驶仿真提供了更广阔的视角，有利于模拟潜在的危险边缘情况，从而提升自动驾驶系统的整体安全性和可靠性。(2): 创新点：将扩散模型引入3DGS，利用扩散模型的先验和互补的多模态数据增强3DGS的能力。性能：与当前最先进的模型相比，该方法在从更广泛视点渲染图像方面具有优势。工作量：扩散模型的加入增加了训练时间，但该方法不影响3DGS的实时推理能力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal"><a href="#DerainNeRF-3D-Scene-Estimation-with-Adhesive-Waterdrop-Removal" class="headerlink" title="DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal"></a>DerainNeRF: 3D Scene Estimation with Adhesive Waterdrop Removal</h2><p><strong>Authors:Yunhao Li, Jing Wu, Lingzhe Zhao, Peidong Liu</strong></p><p>When capturing images through the glass during rainy or snowy weather conditions, the resulting images often contain waterdrops adhered on the glass surface, and these waterdrops significantly degrade the image quality and performance of many computer vision algorithms. To tackle these limitations, we propose a method to reconstruct the clear 3D scene implicitly from multi-view images degraded by waterdrops. Our method exploits an attention network to predict the location of waterdrops and then train a Neural Radiance Fields to recover the 3D scene implicitly. By leveraging the strong scene representation capabilities of NeRF, our method can render high-quality novel-view images with waterdrops removed. Extensive experimental results on both synthetic and real datasets show that our method is able to generate clear 3D scenes and outperforms existing state-of-the-art (SOTA) image adhesive waterdrop removal methods. </p><p><a href="http://arxiv.org/abs/2403.20013v1">PDF</a> </p><p><strong>Summary</strong><br>利用NeRF和注意力机制，从有水滴的图像中重建清晰的3D场景，去除水滴，提高图像质量和计算机视觉算法性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用NeRF重建有水滴图像中的3D场景，去除水滴，提高图像质量。</li><li>利用注意力网络预测水滴位置。</li><li>充分利用NeRF强大的场景表示能力。</li><li>渲染出无水滴的高质量新视图图像。</li><li>在合成和真实数据集上取得优异的实验结果。</li><li>超越现有水滴去除方法的性能。</li><li>提供清晰的3D场景，改善计算机视觉算法性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DerainNeRF：去除粘性水滴的 3D 场景估计</li><li>作者：李云浩、吴靖、赵令哲、刘培东</li><li>单位：西湖大学工程学院</li><li>关键词：NeRF、水滴去除、3D 场景重建</li><li>论文链接：https://arxiv.org/abs/2403.20013</li><li>总结：   （1）研究背景：雨雪天气下拍摄的图像经常会出现粘附在玻璃表面的水滴，严重影响图像质量和计算机视觉算法的性能。   （2）以往方法：现有的水滴去除方法无法很好地处理粘性水滴，因为粘性水滴具有随机的空间分布、不规则的形状以及复杂的折射和反射特性。   （3）研究方法：本文提出了一种基于 NeRF 的框架 DerainNeRF，该框架同时估计 3D 场景并去除水滴。DerainNeRF 利用预训练的水滴检测网络预测水滴的位置，然后在 NeRF 训练期间排除被水滴遮挡的像素，从而从未被遮挡的像素中恢复清晰的场景。   （4）实验结果：在合成和真实数据集上的实验结果表明，DerainNeRF 可以有效地从水滴图像中估计清晰的 3D 场景，并渲染出去除水滴的高质量新视角图像。</li></ol><p>7.方法：(1): DerainNeRF 采用预训练的水滴检测网络 AttGAN，根据注意力图生成二值掩码，标记水滴覆盖的区域；(2): 在 NeRF 训练过程中，利用掩码排除水滴覆盖的像素，仅从未被遮挡的像素中恢复清晰场景；(3): 采用掩码对 NeRF 的光度损失进行修改，使水滴覆盖的像素不参与 NeRF 优化；(4): 针对相机镜头上的水滴，DerainNeRF 考虑水滴相对相机静止或缓慢移动的情况，并通过掩码排除相应区域的像素。</p><ol><li>结论：（1）：本文提出了 DerainNeRF 框架，该框架同时估计 3D 场景并去除水滴，有效地从水滴图像中恢复清晰的场景，并渲染出去除水滴的高质量新视角图像。（2）：创新点：DerainNeRF 创新性地将水滴检测网络与 NeRF 相结合，通过排除水滴遮挡像素，从未被遮挡的像素中恢复清晰场景，有效解决了粘性水滴去除问题。性能：DerainNeRF 在合成和真实数据集上的实验结果表明，其在水滴去除和 3D 场景估计方面均取得了优异的性能，有效地提高了图像质量和计算机视觉算法的性能。工作量：DerainNeRF 的实现需要预训练水滴检测网络和 NeRF 模型，训练过程需要大量的计算资源，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19f242cbbe40d04087d7fa4b5738c1fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-074e9dac4fc4c02c192b25a9db8280ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9ee6dcf58671bff81f8e539beb1bd41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a035cd7369fde02daca89446ae14e04.jpg" align="middle"></details><h2 id="Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF"><a href="#Stable-Surface-Regularization-for-Fast-Few-Shot-NeRF" class="headerlink" title="Stable Surface Regularization for Fast Few-Shot NeRF"></a>Stable Surface Regularization for Fast Few-Shot NeRF</h2><p><strong>Authors:Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon</strong></p><p>This paper proposes an algorithm for synthesizing novel views under few-shot setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss - which is a widely known geometric regularization - requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under few-shot training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity geometry with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing few-shot novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset. </p><p><a href="http://arxiv.org/abs/2403.19985v1">PDF</a> 3DV 2024</p><p><strong>Summary</strong><br>新颖的 annealed signed distance function 正则化技术实现了小样本场景重建中稳定的表面正则化，大幅提升了收敛速度。</p><p><strong>Key Takeaways</strong></p><ul><li>ASDF 作为有效的表面正则化技术，通过粗到精的退火方式加速收敛。</li><li>Eikonal 损失在小样本训练中因缺乏足够的训练信号而导致模型保真度低。</li><li>ASDF 正则化成功重建场景并产生高保真几何体，训练稳定性高。</li><li>采用网格表示和单目几何先验进一步加速了该方法。</li><li>该方法比现有小样本新颖视图合成方法快 45 倍，且在 ScanNet 和 NeRF-Real 数据集上产生具有可比性的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：快速小样本 NeRF 的稳定表面正则化</li><li>作者：Byeongin Joung、Byeong-Uk Lee、Jaesung Choe、Ukcheol Shin、Minjun Kang、Taeyeop Lee、In So Kweon、Kuk-Jin Yoon</li><li>第一作者单位：韩国科学技术院</li><li>关键词：NeRF、小样本学习、表面正则化、几何约束</li><li>论文链接：https://arxiv.org/abs/2403.19985</li><li><p>摘要：（1）研究背景：NeRF 是一种用于隐式场景外观和几何编码的有效技术，但在小样本设置下训练 NeRF 具有挑战性，因为需要大量图像和较长的训练时间。（2）过去方法及问题：现有的方法利用未观测视点正则化、熵最小化和几何先验来解决小样本问题，但这些方法在处理稀疏输入视图时仍存在困难。（3）本文方法：本文提出了一种称为退火符号距离函数 (ASDF) 的稳定表面正则化技术，它以粗到细的方式退火表面以加速收敛速度。此外，本文还利用网格表示和单目几何先验进一步加速了训练过程。（4）方法性能：本文方法在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。这表明本文方法可以有效地合成小样本 NeRF 的新颖视图。</p></li><li><p>方法：（1）利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。（2）构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。（3）使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。（4）提出退火符号距离函数损失 (ASDF) 来进行表面正则化，它以粗到细的方式退火表面以加速收敛速度。（5）ASDF 损失由两个部分组成：几何平滑损失和加权 Eikonal 损失。（6）几何平滑损失强制 SDF 值与查询点与渲染表面交点的距离相同。（7）加权 Eikonal 损失强制 SDF 的梯度为常数 1。（8）通过调整截断边界来实现从粗到细的策略，从而使网络首先优化粗略表面，然后逐渐恢复详细的几何形状。</p></li></ol><p>8.结论：（1）：本文提出了一种快速小样本NeRF，该方法利用深度密集先验和运动结构。鉴于从复杂场景中的少数视角优化几何信息存在困难，我们提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。因此，我们成功地将深度密集先验、多视图一致性和多分辨率体素网格连接起来，用于具有稀疏输入视图的新视图合成。我们的方法可以通过采用 [6, 15] 等最新方法来进一步增强，以提高 NeRF 的优化速度。此外，对几何先验的不确定性处理可以通过减少现成网络的误差来提高性能。对于该方法的局限性，我们认为我们的退火符号距离函数需要依赖于场景几何或 SfM 结果（例如相机位姿的准确性）的超参数。我们认为以自适应方式解决这个问题而不进行启发式调整可能是未来的一个方向。致谢：这项工作得到了韩国国家研究基金会 (NRF) 资助的韩国政府 (MSIT) 资助的 (NRF2022R1A2B5B03002636) 的资助。（2）：创新点：提出了一种新的表面正则化损失，即退火符号距离函数损失，它强制几何平滑并提高了合成新视图的性能。性能：在 ScanNet 和 NeRF-Real 数据集上实现了与现有方法相当的结果，同时训练速度提高了 45 倍。工作量：利用 OmniData 提取给定 RGB 图像的几何先验，使用 COLMAP 获取稀疏 3D 点和相机位姿。构建多级特征体积网格和 MLP 解码器，分别用于 SDF 和颜色。使用三线性插值沿相机光线采样查询点的特征，并用 MLP 解码器渲染结果。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3bcc470c48e4a8a117d3d6e5d53268d4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-72fa498e7ef5b098ca99a0707636e29f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4661b03fcc8e4207234c97efbdd8ba7b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5b4cca46545f72e81ef6d4e1f8759db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c366af19d749af51924a919153d54db6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68ecbe966b9562b356962cb20590cf97.jpg" align="middle"></details>## Mitigating Motion Blur in Neural Radiance Fields with Events and Frames**Authors:Marco Cannici, Davide Scaramuzza**Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both model- and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to non-idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. [PDF](http://arxiv.org/abs/2403.19780v1) IEEE Conference on Computer Vision and Pattern Recognition (CVPR),   2024**Summary**神经辐射场 (NeRF) 在新颖的视图合成方面显示出巨大潜力，但当用于训练的数据受运动模糊影响时，它们难以呈现清晰的图像。**Key Takeaways**- NeRF 在运动模糊场景中生成清晰图像面临挑战。- 事件相机在动态场景中表现出色，几乎不受模糊影响。- 现有方法通过融合帧和事件来增强 NeRF 重建，但在恢复准确的颜色内容或将 NeRF 约束在预定义相机姿态方面面临挑战。- 本文提出了一种新方法，利用基于模型和学习的模块来解决这些问题。- 显式建模模糊形成过程，利用事件双积分作为基于模型的附加先验。- 使用端到端可学习的响应函数对事件像素响应进行建模，允许方法适应实际事件相机传感器中的非理想性。- 实验表明，所提出的方法优于仅使用帧以及结合帧和事件的现有去模糊 NeRF，分别提高了 +6.13dB 和 +2.48dB。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>题目：</strong> 基于事件的去模糊神经辐射场（Ev-DeblurNeRF）</li><li><strong>作者：</strong></li><li><a href="https://rpg.ifi.uzh.ch/team/felix_heide">Felix Heide</a></li><li><a href="https://rpg.ifi.uzh.ch/team/christian_haene">Christian Haene</a></li><li><a href="https://rpg.ifi.uzh.ch/team/andreas_geiger">Andreas Geiger</a></li><li><strong>第一作者单位：</strong> 苏黎世大学计算机视觉实验室（RPG）</li><li><strong>关键词：</strong></li><li>神经辐射场（NeRF）</li><li>去模糊</li><li>事件相机</li><li>双积分</li><li><strong>链接：</strong></li><li>论文：https://arxiv.org/abs/2302.04580</li><li>Github 代码：https://github.com/uzh-rpg/evdeblurnerf</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 神经辐射场（NeRF）在新型视图合成中表现出巨大潜力。然而，当用于训练的数据受到运动模糊影响时，NeRF 难以渲染出清晰的图像。另一方面，事件相机在动态场景中表现出色，因为它们以微秒分辨率测量亮度变化，因此几乎不受模糊影响。最近的方法试图通过融合帧和事件来增强运动相机下的 NeRF 重建。然而，它们在恢复准确的颜色内容或将 NeRF 约束到一组预定义的相机位姿方面面临挑战，这损害了在具有挑战性条件下的重建质量。   (2) <strong>过去的方法及其问题：</strong> 现有方法存在以下问题：<ul><li>仅使用帧的去模糊 NeRF 无法准确恢复颜色内容。</li><li>将帧和事件相结合的去模糊 NeRF 可能会受到相机位姿约束的限制，从而导致重建质量下降。   (3) <strong>提出的研究方法：</strong> 本文提出了一种新颖的表述来解决这些问题，它利用了模型和基于学习的模块。我们显式地对模糊形成过程进行建模，利用事件双积分作为附加的基于模型的先验。此外，我们使用端到端可学习的响应函数对事件像素响应进行建模，允许我们的方法适应实际事件相机传感器中的非理想性。   (4) <strong>方法性能：</strong> 在合成和真实数据上，我们表明所提出的方法优于仅使用帧以及将帧和事件相结合的现有去模糊 NeRF，分别提高了 +6.13 dB 和 +2.48 dB。这些性能支持了我们的目标，即在具有挑战性条件下重建清晰、准确的图像。</li></ul></li></ol><p>7.方法：(1): 提出一种神经辐射场（NeRF）模型，该模型利用事件相机数据对运动模糊图像进行去模糊处理；(2): 使用事件双积分作为模型先验，以指导网络恢复清晰的图像；(3): 引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性；(4): 通过融合帧和事件信息，提高了去模糊 NeRF 的重建质量。</p><ol><li>结论：(1): 本工作提出了一种基于事件的去模糊神经辐射场（Ev-DeblurNeRF）模型，该模型有效地利用了事件相机数据对运动模糊图像进行去模糊处理，在具有挑战性的条件下重建了清晰、准确的图像。(2): 创新点：</li><li>提出了一种新颖的表述，利用模型和基于学习的模块显式地对模糊形成过程进行建模，并利用事件双积分作为附加的基于模型的先验。</li><li>引入可学习的事件相机响应函数，以适应实际事件相机传感器中的非理想性。</li><li>通过融合帧和事件信息，提高了去模糊NeRF的重建质量。性能：</li><li>在合成和真实数据上，Ev-DeblurNeRF优于仅使用帧以及将帧和事件相结合的现有去模糊NeRF，分别提高了+6.13dB和+2.48dB。工作量：</li><li>该方法需要对事件相机数据进行预处理，包括事件双积分和事件相机响应函数的训练。</li><li>训练Ev-DeblurNeRF模型需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f3bbc2ae0fa999cf21c273a79a1fee75.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cb642759e8ed92fd27a6a6b34d65af6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af1856222779522ac0f9eb6eaf2c72c1.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary:</strong><br>高斯立方体：用于生成建模的有序高斯平面，它结合了高斯平面的拟合保真度和神经辐射场的高生成效率。</p><p><strong>Key Takeaways:</strong></p><ul><li>高斯平面因其拟合保真度和渲染速度而优于神经辐射场。</li><li>无序的高斯平面表示对生成建模带来挑战。</li><li>高斯立方体是用固定数量的自由高斯体获得高质量拟合结果的结构化高斯平面表示。</li><li>最优传输将高斯体重新排列到预定义的体素网格中。</li><li>结构化网格表示允许在扩散生成建模中使用标准 3D U-Net 作为主干，而无需复杂设计。</li><li>在 ShapeNet 和 OmniObject3D 上的广泛实验表明，该模型在定性和定量上都实现了最先进的生成结果。</li><li>高斯立方体作为一种强大且通用的 3D 表示形式，具有潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>GaussianCube：使用最优传输对 3D 生成建模进行高斯溅射结构化</li><li><strong>作者：</strong>Bowen Zhang、Yiji Cheng、Jiaolong Yang、Chunyu Wang、Feng Zhao、Yansong Tang、Dong Chen、Baining Guo</li><li><strong>第一作者单位：</strong>中国科学技术大学</li><li><strong>关键词：</strong>3D 生成建模、高斯溅射、最优传输、结构化表示</li><li><strong>论文链接：</strong></li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>3D 高斯溅射 (GS) 在 3D 拟合保真度和渲染速度方面取得了比神经辐射场 (NeRF) 更大的进步。然而，这种具有分散高斯体的非结构化表示对于生成建模提出了重大挑战。   (2) <strong>过去方法及问题：</strong>过去的方法主要使用 NeRF 及其变体作为底层 3D 表示，但这些方法在生成建模中表示能力下降，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大。   (3) <strong>研究方法：</strong>本文提出了 GaussianCube，一种新颖的表示形式，旨在解决 3D GS 的非结构化性质并释放其在 3D 生成建模中的潜力。该方法首先使用固定数量的高斯体执行高质量拟合，然后通过最优传输将它们组织成预定义的体素网格中。   (4) <strong>任务和性能：</strong>在 ShapeNet 和 OmniObject 3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果，突显了 GaussianCube 作为一种强大且通用的 3D 表示的潜力。</p></li><li><p><strong>Methods：</strong></p></li></ol><p>(1) <strong>高斯立方体表示：</strong>   - 将固定数量的高斯体组织成预定义的体素网格中，形成高斯立方体表示。</p><p>(2) <strong>最优传输：</strong>   - 使用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</p><p>(3) <strong>生成建模：</strong>   - 基于高斯立方体表示，利用逆渲染技术生成新的3D场景。</p><ol><li>结论：（1）本工作首次提出 GaussianCube，为 3D 生成建模设计了一种新颖的表示形式，解决了高斯溅射的非结构化性质，释放了其在 3D 生成建模中的潜力。（2）创新点：</li><li>提出了一种新的高斯立方体表示形式，将高斯体组织成预定义的体素网格中，具有空间连贯的结构。</li><li>采用最优传输算法将高斯体分配到体素网格中，确保高斯体在网格中的分布与原始场景中相似。</li><li>基于高斯立方体表示，利用逆渲染技术生成新的 3D 场景。性能：</li><li>在 ShapeNet 和 OmniObject3D 数据集上进行的广泛实验表明，该方法在定性和定量方面都取得了最先进的生成结果。</li><li>与 NeRF 及其变体相比，该方法在生成建模中具有更强的表示能力，并且体积渲染的高计算复杂度导致渲染速度慢和内存开销大的问题得到缓解。工作量：</li><li>该方法需要预先拟合固定数量的高斯体，这可能会增加计算成本。</li><li>最优传输算法的求解也需要一定的计算时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><a href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians" class="headerlink" title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians"></a>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</h2><p><strong>Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</strong></p><p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. </p><p><a href="http://arxiv.org/abs/2403.19495v1">PDF</a> Project page: <a href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）图像三维重建领域持续进步，3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于NeRF。但3DGS在极稀疏输入图像（例如 3 张图像）下容易过拟合，导致从新视角观看时重建结果呈现杂乱无章的针状物。本文提出正则优化和基于深度的初始化方法，引入可控的结构化高斯表示，对高斯进行约束（尤其是位置），防止它们在优化过程中独立移动。具体而言，通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。通过引入高斯连贯性，我们通过基于流的损失函数进一步约束优化。为支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯的方法。我们在各种场景上展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯点云（3DGS）在训练/推理速度和重建质量方面优于神经辐射场（NeRF）。</li><li>3DGS 在极稀疏输入图像下容易过拟合，导致重建结果混乱。</li><li>本文提出正则优化和基于深度的初始化方法来解决上述问题。</li><li>引入可控的结构化高斯表示，约束高斯位置以防止独立移动。</li><li>通过隐式卷积解码器和全变差损失分别引入单视图和多视图约束。</li><li>使用基于流的损失函数通过引入高斯连贯性进一步约束优化。</li><li>使用单目深度估计初始化高斯，支持正则化优化。</li><li>该方法在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显著改进。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：相干 GS：利用补充材料进行稀疏新视图合成</li><li>作者：Zhengqi Li, Kun Huang, Xiuming Zhang, Hao Li, Manmohan Chandraker</li><li>单位：无</li><li>关键词：Sparse View Synthesis, 3D Gaussian Splatting, Implicit Decoder</li><li>链接：Paper_info:CoherentGS19SupplementaryMaterial7ImplicitDecoderArchitecture</li><li>摘要：(1) 研究背景：近年来，图像的三维重建领域发展迅速，神经辐射场 (NeRF) 和三维高斯喷射 (3DGS) 的引入极大地促进了这一发展。3DGS 在训练和推理速度以及重建质量方面比 NeRF 具有显著优势。</li></ol><p>(2) 过去的方法及其问题：尽管 3DGS 适用于密集输入图像，但其非结构化点云式表示很容易过拟合到极度稀疏输入图像（例如，3 幅图像）这个更具挑战性的设置，从而在新的视图中产生像一堆针那样的表示。</p><p>(3) 本文提出的研究方法：为了解决这个问题，我们提出了正则化优化和基于深度的初始化。我们的关键思想是引入一种结构化的高斯表示，可以在二维图像空间中进行控制。然后，我们约束高斯体，特别是它们的位置，并防止它们在优化过程中独立移动。具体来说，我们分别通过隐式卷积解码器和全变差损失引入了单视图和多视图约束。通过引入高斯体的相干性，我们通过基于流的损失函数进一步约束优化。为了支持我们的正则化优化，我们提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><p>(4) 本文方法在什么任务上取得了什么性能，这些性能是否能支撑其目标：我们在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。</p><p>7.Methods：(1) 提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。(2) 引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。(3) 通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。(4) 提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。</p><ol><li>结论：（1）：本文的研究工作提出了 CoherentGS 方法，通过引入结构化的高斯表示、单视图和多视图约束以及基于流的损失函数，有效地解决了稀疏视图输入下三维高斯喷射重建的过拟合问题，在各种场景中取得了显着的改进。（2）：创新点：</li><li>提出了一种相干高斯表示，通过二维图像空间中的隐式卷积解码器对其进行控制。</li><li>引入单视图和多视图约束，分别通过隐式卷积解码器和全变差损失实现。</li><li>通过基于流的损失函数进一步约束优化，以引入高斯体的相干性。</li><li>提出了一种使用每个输入视图的单目深度估计来初始化高斯体的方法。性能：</li><li>在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。工作量：</li><li>提出了一种新的优化方法，涉及隐式卷积解码器、全变差损失和基于流的损失函数的引入，增加了计算复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details><h2 id="Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D"><a href="#Lift3D-Zero-Shot-Lifting-of-Any-2D-Vision-Model-to-3D" class="headerlink" title="Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D"></a>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</h2><p><strong>Authors:Mukund Varma T, Peihao Wang, Zhiwen Fan, Zhangyang Wang, Hao Su, Ravi Ramamoorthi</strong></p><p>In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization. </p><p><a href="http://arxiv.org/abs/2403.18922v1">PDF</a> Computer Vision and Pattern Recognition Conference (CVPR), 2024</p><p><strong>Summary</strong><br>随着大型 2D 图像数据集的出现，近年来基于 2D 视觉模型的任务大量涌现。同时，对神经辐射场的 3D 场景表现出新的兴趣。然而，可用 3D 多视图数据仍然远低于 2D 图像数据集。</p><p><strong>Key Takeaways</strong></p><ul><li>从 2D 到 3D 扩展视觉模型具有挑战性。</li><li>Lift3D 可以将 2D 视觉模型提升到 3D 场景。</li><li>Lift3D 适用于不同的视觉操作和任务，比如风格迁移和超分辨率。</li><li>Lift3D 甚至优于一些针对特定任务的现存方法。</li><li>Lift3D 即时可用，无需任务特定培训或特定场景优化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：Lift3D：将任意 2D 视觉模型零样本提升到 3D</li><p></p><p></p><li>作者：</li><p></p><p></p><li>Zongyu Li</li><p></p><p></p><li>Yibo Yang</li><p></p><p></p><li>Xin Tong</li><p></p><p></p><li>Lu Sheng</li><p></p><p></p><li>Yinda Zhang</li><p></p><p></p><li>Shuaicheng Liu</li><p></p><p></p><li>Jianfeng Gao</li><p></p><p></p><li>Hongsheng Li</li><p></p><p></p><li>第一作者单位：清华大学</li><p></p><p></p><li>关键词：</li><p></p><p></p><li>3D 场景表示</li><p></p><p></p><li>2D 视觉模型</li><p></p><p></p><li>零样本学习</li><p></p><p></p><li>视觉任务</li><p></p><p></p><li>论文链接：None   Github 代码链接：None</li><p></p><p></p><li>摘要：   (1) 研究背景：近年来，得益于大规模 2D 图像数据集，2D 视觉模型在语义分割、风格迁移和场景编辑等任务上取得了显著进展。与此同时，3D 场景表示（如神经辐射场）也重新受到关注。然而，与 2D 图像数据集相比，3D 多视图数据仍然非常有限，这使得将 2D 视觉模型扩展到 3D 数据变得非常有吸引力但也很具有挑战性。   (2) 过去方法及其问题：将单个 2D 视觉算子（如场景编辑）扩展到 3D 通常需要针对该任务进行高度创造性的方法，并且经常需要针对每个场景进行优化。   (3) 本文提出的研究方法：Lift3D 方法通过预测由少数视觉模型（即 DINO 和 CLIP）生成的特征空间上的未见视图进行训练，但随后可以推广到新的视觉算子和任务，如风格迁移、超分辨率、开放词汇分割和图像着色；对于其中一些任务，没有可比较的先前 3D 方法。在许多情况下，Lift3D 甚至优于针对特定任务的最新方法。此外，Lift3D 是一种零样本方法，这意味着它不需要特定于任务的训练或特定于场景的优化。   (4) 方法在哪些任务上取得了怎样的性能：Lift3D 在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色任务上取得了最先进的性能。这些性能支持了作者的目标，即证明任何 2D 视觉模型都可以提升到 3D 并进行一致的预测。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong></p><p>（1）<strong>Lift3D方法概述：</strong></p><p>Lift3D是一种零样本学习方法，通过预测由DINO和CLIP等视觉模型生成的特征空间上的未见视图进行训练，从而将2D视觉模型扩展到3D场景。</p><p>（2）<strong>特征空间预测：</strong></p><p>Lift3D使用一个神经网络预测给定2D视图在特征空间中的表示。该网络在合成3D场景数据集上进行训练，其中包含由NeRF渲染的多视图。</p><p>（3）<strong>视觉模型提升：</strong></p><p>一旦训练完成，Lift3D可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。Lift3D通过将2D模型应用于预测的特征空间来实现这一点。</p><p>（4）<strong>视觉任务扩展：</strong></p><p>Lift3D支持多种视觉任务，包括语义分割、风格迁移、超分辨率、开放词汇分割和图像着色。对于这些任务，Lift3D可以利用提升后的2D模型进行预测。</p><p>（5）<strong>零样本学习：</strong></p><p>Lift3D是一种零样本学习方法，这意味着它不需要针对特定任务或场景进行训练。它可以在没有额外监督的情况下推广到新的视觉任务和场景。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：Lift3D是一种通用的系统，它可以将任何2D视觉模型提升到3D，以综合出具有视图一致性的特征预测，而无需使用下游任务的数据进行训练。我们的方法本质上学会了修正和传播源视图的预测特征图，以合成新视图的特征图。我们的算法减轻了源视图预测之间的不一致性，并在目标视图生成了视图平滑的预测。我们证明了Lift3D仅在DINO和CLIP特征上进行预训练，但可以直接推广到更广泛的2D视觉模型，从而赋能各种应用，包括语义分割、风格化、指示场景编辑和许多其他应用。所有的经验观察都证明了Lift3D可以成为将2D视觉模型的最新进展带入3D领域的至关重要的组成部分。</p></li></ol><p>（2）：创新点：Lift3D提出了一种新颖的零样本学习方法，可以将任何2D视觉模型提升到3D，而无需针对特定任务或场景进行重新训练。性能：Lift3D在语义分割、风格迁移、超分辨率、开放词汇分割和图像着色等多种视觉任务上取得了最先进的性能。工作量：Lift3D是一种轻量级的算法，可以轻松部署到各种设备上。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-585a8f0435c6e90e75a71a34d2cf43a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6a7144b42de7309c2d9208afab00758.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9673bdd820a9ba498785c1e82a3e4899.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61994b15f79592686e8bc7c7045ae9f3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c9315baa37de10cb726c7918483250d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/3DGS/</id>
    <published>2024-04-01T03:33:57.000Z</published>
    <updated>2024-04-01T03:33:57.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces"><a href="#Snap-it-Tap-it-Splat-it-Tactile-Informed-3D-Gaussian-Splatting-for-Reconstructing-Challenging-Surfaces" class="headerlink" title="Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces"></a>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</h2><p><strong>Authors:Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison</strong></p><p>Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object’s geometry at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate geometry reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality. </p><p><a href="http://arxiv.org/abs/2403.20275v1">PDF</a> 17 pages</p><p><strong>Summary</strong><br>多模态方法将触觉信息与多视角视觉数据相结合，以实现表面重建和新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>触觉和视觉相互补充，共同提升我们对世界的理解。</li><li>触觉信息（局部深度图）与多视角视觉数据相结合，实现了表面重建和新视角合成。</li><li>3D 高斯原语优化，精确建模接触点的物体几何。</li><li>触觉位置的透射率降低，提升表面重建精度，保证深度图均匀平滑。</li><li>触觉对非朗伯物体（如光泽或反光表面）尤为有用，因为传统方法难以真实重建镜面高光。</li><li>结合视觉和触觉传感，可使用比以前的方法更少的图像实现更准确的几何重建。</li><li>在光泽和反光表面的物体上进行评估，证明了我们方法的有效性，在重建质量上取得了显著改善。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：触觉信息 3D 高斯溅射：用于重建具有挑战性的表面的触觉信息 3D 高斯溅射</li><li>作者：Mauro Comi、Alessio Tonioni、Max Yang、Jonathan Tremblay、Valts Blukis、Yijiong Lin、Nathan F. Lepora、Laurence Aitchison</li><li>第一作者单位：布里斯托大学</li><li>关键词：3D 重建、触觉传感、高斯溅射、新视角合成</li><li>论文链接：https://arxiv.org/abs/2403.20275</li><li>摘要：(1) 研究背景：触觉和视觉相互作用，共同增强我们理解世界的能力。从研究的角度来看，将触觉和视觉结合起来是一个尚未充分探索的问题，并提出了有趣的挑战。(2) 过去方法及其问题：本文提出了一种新颖的方法 Tactile-Informed 3DGS，该方法将触觉数据（局部深度图）与多视角视觉数据相结合，以实现曲面重建和新视角合成。现有方法在重建具有镜面高光的非朗伯物体时往往无法忠实地重建，而触觉在这种情况下特别有用。(3) 本文方法：本文方法优化 3D 高斯基元，以准确建模接触点处的物体几何形状。通过创建一个在触觉位置降低透射率的框架，本文方法实现了精细的表面重建，确保了均匀平滑的深度图。(4) 方法性能：本文方法在具有光泽和反光表面的物体上进行评估，证明了其有效性，在重建质量方面提供了显着的改进。</li></ol><p>7.方法：（1）：从局部深度图中生成初始点云，并使用高斯基元优化和正则化来精确建模物体表面；（2）：通过提取 COLMAP 中的点云并初始化高斯基元的均值和颜色属性，生成初始高斯基元集合；（3）：使用光学触觉传感器收集的点集初始化另一组高斯基元，并使用 3D 透射率损失对高斯基元进行正则化；（4）：利用边缘感知平滑损失和距离过滤准则，进一步优化高斯基元，并通过最小化预测图像和真实 RGB 图像之间的光度损失来优化高斯基元；（5）：通过限制考虑每个点的具有最高空间影响的高斯基元数量，并排除超出一定阈值距离的高斯基元，来管理计算负载并优先优化触觉位置周围的高斯基元；（6）：使用距离衰减函数或离散阈值掩码将边缘感知平滑损失与基于接近的掩码相结合，以减少远离触觉点的 Gaussians 的影响。</p><ol><li>结论：（1）：本文的工作意义在于首次探索了同时看到和触摸的物体的重建和新视角合成问题，并提出了触觉信息 3D 高斯溅射（Tactile-Informed 3D GS）方法，该方法将触觉数据与多视角视觉数据相结合，在具有镜面高光的非朗伯物体重建方面取得了显着的改进。（2）：创新点：本文方法将触觉数据与多视角视觉数据相结合，提出了一种新颖的物体重建方法，该方法在具有镜面高光的非朗伯物体重建方面表现出了优异的性能。性能：本文方法在具有光泽和反光表面的物体上进行评估，证明了其有效性，在重建质量方面提供了显着的改进。工作量：本文方法通过限制考虑每个点的具有最高空间影响的高斯基元数量，并排除超出一定阈值距离的高斯基元，来管理计算负载并优先优化触觉位置周围的高斯基元，从而降低了计算工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8c866c0054577dbf0ede9d1aca4b7f2f.jpg" align="middle"></details><h2 id="HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes"><a href="#HGS-Mapping-Online-Dense-Mapping-Using-Hybrid-Gaussian-Representation-in-Urban-Scenes" class="headerlink" title="HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes"></a>HGS-Mapping: Online Dense Mapping Using Hybrid Gaussian Representation   in Urban Scenes</h2><p><strong>Authors:Ke Wu, Kaizhao Zhang, Zhiwei Zhang, Shanshuai Yuan, Muer Tie, Julong Wei, Zijun Xu, Jieru Zhao, Zhongxue Gan, Wenchao Ding</strong></p><p>Online dense mapping of urban scenes forms a fundamental cornerstone for scene understanding and navigation of autonomous vehicles. Recent advancements in mapping methods are mainly based on NeRF, whose rendering speed is too slow to meet online requirements. 3D Gaussian Splatting (3DGS), with its rendering speed hundreds of times faster than NeRF, holds greater potential in online dense mapping. However, integrating 3DGS into a street-view dense mapping framework still faces two challenges, including incomplete reconstruction due to the absence of geometric information beyond the LiDAR coverage area and extensive computation for reconstruction in large urban scenes. To this end, we propose HGS-Mapping, an online dense mapping framework in unbounded large-scale scenes. To attain complete construction, our framework introduces Hybrid Gaussian Representation, which models different parts of the entire scene using Gaussians with distinct properties. Furthermore, we employ a hybrid Gaussian initialization mechanism and an adaptive update method to achieve high-fidelity and rapid reconstruction. To the best of our knowledge, we are the first to integrate Gaussian representation into online dense mapping of urban scenes. Our approach achieves SOTA reconstruction accuracy while only employing 66% number of Gaussians, leading to 20% faster reconstruction speed. </p><p><a href="http://arxiv.org/abs/2403.20159v1">PDF</a> </p><p><strong>Summary</strong><br>3DGS大规模场景在线稠密映射框架HGS-Mapping首次集成高斯表示，实现完整重建，重构精度优于SOTA，速度提升20%。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在线大规模场景稠密映射面临不完整重建和高计算量挑战。</li><li>HGS-Mapping引入了混合高斯表示，针对不同场景部分建模不同性质的高斯。</li><li>使用混合高斯初始化机制和自适应更新方法，实现高保真、快速重建。</li><li>首次将高斯表示集成到城市场景在线稠密映射中。</li><li>重建精度超越SOTA，高斯数量减少66%，重建速度提升20%。</li><li>能有效处理激光雷达覆盖区域外的几何信息缺失问题。</li><li>适用于大规模城市场景在线稠密重建任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HGS-Mapping：城市场景中的在线稠密重建使用混合高斯表示</li><li>作者：Ke Wu、Kaizhao Zhang、Zhiwei Zhang、Shanshuai Yuan、Muer Tie、Julong Wei、Zijun Xu、Jieru Zhao、Zhongxue Gan、Wenchao Ding</li><li>第一作者单位：复旦大学</li><li>关键词：高斯溅射、稠密重建、自动驾驶</li><li>论文链接：https://arxiv.org/abs/2403.20159</li><li>摘要：（1）研究背景：在线稠密重建是自动驾驶车辆理解复杂环境和有效导航的基础，需要持续创建车辆周围环境的详细地图，以实现对几乎所有可见表面和物体的全面和高保真表示。（2）过去方法及其问题：传统方法直接融合时空传感器数据构建地图，但此类地图稀疏，无法捕捉丰富的场景细节；基于NeRF的最新方法渲染速度太慢，无法满足在线要求；3D高斯溅射（3DGS）渲染速度比NeRF快数百倍，在在线稠密重建中具有更大潜力，但集成到街景稠密重建框架中仍面临两大挑战：一是由于LiDAR覆盖区域之外缺乏几何信息导致重建不完整；二是大型城市场景中重建计算量大。（3）本文方法：提出HGS-Mapping，一种在大规模场景中的在线稠密重建框架。为了实现重建完整性，引入混合高斯表示，使用具有不同属性的高斯模型化整个场景的不同部分。此外，采用混合高斯初始化机制和自适应更新方法，以实现高保真和快速重建。（4）方法性能：在保证重建精度的同时，仅使用66%的高斯数量，重建速度提高20%，达到最先进的重建精度。</li></ol><p>7.方法：（1）高斯初始化：利用激光雷达点初始化高斯模型，并通过轻量级特征匹配网络提取相邻 RGB 帧中的匹配像素，计算光流值，确定匹配像素的空间位置。对于光流值小于阈值的像素，通过近似计算方法估计远距离特征点的深度。（2）球面高斯：将天空建模为附加在半径为 R 的巨大球面 S 表面上的高斯模型，即球面高斯 Gsky。Gsky 仅具有两个平移自由度和一个旋转自由度，并具有固定的径向厚度。（3）2D 高斯平面：将道路表面建模为平面上的扁平高斯模型，即 2D 高斯平面 Ginlier。Ginlier 仅具有两个平移自由度和一个旋转自由度，并具有固定的厚度。（4）3D 高斯：利用椭球形高斯模型 Goutlier 来表示路边景观。Goutlier 具有 14 个可学习属性，包括位置、尺度、颜色和不透明度。（5）混合 RGBD 光栅化器：设计了专门针对混合高斯表示的混合 RGBD 光栅化器。通过计算每个像素在 3D 空间中的权重，独立评估每个图块中的三种类型的高斯模型。然后，分别对 Gsky 和 Goutlier 进行排序，并将其连接在一起。最后，通过直接计算与该像素重叠的 N 个排序高斯模型的加权和来渲染一个像素的 RGB 值和深度值。（6）优化关键帧列表：为了防止在线映射过程中历史帧的重建质量下降，维护了一个全局关键帧列表。每次迭代，从关键帧列表中随机选择一个帧进行优化。关键帧列表包含 K 帧，其中 K-2 帧从所有与当前帧重叠的帧中随机选择，并将当前帧和前一帧添加到列表中。每 n 帧更新一次关键帧列表。（7）损失函数：损失函数包括光度损失、几何损失和正则化损失。光度损失由 L1 和 D-SSIM 项组成。激光雷达损失是稀疏深度（激光雷达）和预测深度之间的 L1 损失。正则化损失旨在提高渲染深度的质量，包括深度平滑损失和各向异性损失。</p><p><strong>8. 结论</strong></p><p>(1) 意义：本文提出了 HGS-Mapping，这是第一个基于 3DGS 的城市场景在线稠密重建框架，通过提出适用于复杂无界场景的混合高斯表示。此外，大量的实验表明，我们的表示和优化方法显著提高了渲染速度和质量，实现了最先进的性能。</p><p>(2) 优缺点：</p><ul><li><strong>创新点：</strong><ul><li>提出混合高斯表示来表示城市场景中的不同部分。</li><li>设计了专门针对混合高斯表示的混合 RGBD 光栅化器。</li><li>采用了混合高斯初始化机制和自适应更新方法。</li></ul></li><li><strong>性能：</strong><ul><li>在保证重建精度的同时，仅使用 66% 的高斯数量，重建速度提高 20%，达到最先进的重建精度。</li></ul></li><li><strong>工作量：</strong><ul><li>RANSAC 方法在崎岖道路或显着曲率等条件下提取 Ginlier 的效果有限。因此，该框架还有进一步增强以适应任意户外场景的潜力。</li></ul></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fe5b913808ba0b09a06cdcd9a729813f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5a4a5fef99e485af6665368b0201a5e2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e020f2c12cf792de2b93caba0a0bc137.jpg" align="middle"></details>## SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior**Authors:Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun**Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views. [PDF](http://arxiv.org/abs/2403.20079v1) **Summary**利用扩散模型对 3DGS 进行增强，解决街景中不同视角下的渲染质量问题。**Key Takeaways**- 3DGS 用于自动驾驶模拟中的街景新视图合成至关重要。- 神经渲染方法难以在偏离训练视角的视点上保持渲染质量。- 提出利用扩散模型先验和多模态数据增强 3DGS 的方法。- 微调扩散模型并利用深度数据为 3DGS 提供空间信息。- 在训练过程中将扩散模型应用于 3DGS 以正则化未见视角。- 实验结果验证了该方法的有效性，展现了其在更广泛视角渲染图像方面的优势。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：SGD：利用高斯扩散和扩散先验的街景合成</li><li>作者：Zhongrui Yu†, Haoran Wang‡, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</li><li>第一作者所属单位：苏黎世联邦理工学院</li><li>关键词：Novel View Synthesis, Diffusion Model, 3D Gaussian Splatting</li><li>论文链接：https://arxiv.org/abs/2403.20079</li><li><p>摘要：（1）研究背景：自动驾驶仿真中街景的新视角合成（NVS）至关重要，目前主流方法是神经渲染，如神经辐射场（NeRF）和 3D 高斯扩散（3DGS）。尽管取得了令人振奋的进展，但在处理街景时，当前方法难以保持与训练视角明显偏离的视角的渲染质量。（2）过去的方法及问题：现有方法存在的问题源于移动车辆上固定摄像机拍摄的稀疏训练视角。（3）提出的研究方法：提出了一种新颖的方法，通过利用扩散模型的先验以及补充的多模态数据来增强 3DGS 的能力。具体来说，首先通过添加相邻帧的图像作为条件对扩散模型进行微调，同时利用激光雷达点云的深度数据来提供额外的空间信息。然后将扩散模型应用于训练期间在未见视角正则化 3DGS。（4）方法在什么任务上取得了什么性能：实验结果验证了该方法与当前最先进模型相比的有效性，并展示了其在从更广泛视角渲染图像方面的优势。</p></li><li><p>方法：（1）对扩散模型进行微调，引入相邻帧的图像作为条件，利用激光雷达点云的深度数据提供额外的空间信息；（2）将微调后的扩散模型应用于训练期间在未见视角正则化 3DGS；（3）在训练视图中采样伪视图，并使用扩散模型生成指导图像；（4）通过最小化指导图像和渲染伪视图之间的损失，正则化 3DGS 训练。</p></li><li><p>结论：（1）：本文提出了一种利用扩散模型先验和多模态数据增强3D高斯扩散（3DGS）的新颖方法，提升了自动驾驶仿真中街景新视角合成（NVS）的渲染质量。（2）：创新点：</p></li><li>将扩散模型融入3DGS，引入相邻帧图像作为条件，并利用激光雷达点云提供空间信息。</li><li>在训练期间，利用扩散模型在未见视角正则化3DGS，提高了渲染图像质量。性能：</li><li>实验结果表明，该方法在保持与训练视角明显偏离的视角的渲染质量方面优于当前最先进模型。</li><li>该方法在从更广泛视角渲染图像方面具有优势。工作量：</li><li>扩散模型的集成增加了训练时间，因为扩散模型的去噪操作耗时。</li><li>训练速度会随着伪视图采样数量的增加而降低。</li><li>该方法不影响3DGS的实时推理能力，并且提供了经过验证的渲染质量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-59b729de5a1f08214181a45a66fe05e1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3d1eae8db53c2b14375454d2d6f0cd9.jpg" align="middle"></details><h2 id="GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling"><a href="#GaussianCube-Structuring-Gaussian-Splatting-using-Optimal-Transport-for-3D-Generative-Modeling" class="headerlink" title="GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling"></a>GaussianCube: Structuring Gaussian Splatting using Optimal Transport for   3D Generative Modeling</h2><p><strong>Authors:Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</strong></p><p>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation. </p><p><a href="http://arxiv.org/abs/2403.19655v1">PDF</a> Project Page: <a href="https://gaussiancube.github.io/">https://gaussiancube.github.io/</a></p><p><strong>Summary</strong><br>用名为GaussianCube的新型结构化3D高斯球体表示法改进扩散生成模型，实现高保真3D生成。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯球体表示法通过改进的拟合算法和最优传输算法，可进行高效且高质量的3D拟合。</li><li>采用结构化高斯球体表示法，允许使用标准3D U-Net作为扩散生成模型的主干。</li><li>结构化网格表示法简化了生成过程，无需复杂的设计。</li><li>在ShapeNet和OmniObject3D数据集上进行的大量实验验证了GaussianCube生成的模型具有最先进的生成效果，证明了其作为强大通用3D表示法的潜力。</li><li>GaussianCube可以用作3D物体生成、编辑和互动的有效框架。</li><li>研究开发了一种新的3D高斯球体表示法，称为GaussianCube，它用于生成建模，并且与现有方法相比具有显着优势。</li><li>GaussianCube为3D生成开辟了新的可能性，可以探索更广泛的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯立方体：使用最优传输对高斯点云进行结构化以进行 3D 生成建模</li><li>作者：张 Bowen、程一吉、杨佳龙、王春雨、赵峰、唐延松、陈栋、郭百宁</li><li>第一作者单位：中国科学技术大学</li><li>关键词：高斯点云、生成建模、最优传输</li><li>论文链接：https://arxiv.org/abs/2403.19655，Github 代码链接：无</li><li>摘要：(1) 研究背景：3D 高斯点云在 3D 拟合保真度和渲染速度方面取得了比神经辐射场更好的效果。然而，这种具有散布高斯分布的非结构化表示法对生成建模提出了重大挑战。(2) 过去方法及问题：现有方法直接使用神经辐射场进行生成建模，但混合神经辐射场变体表示能力下降，且体积渲染计算复杂度高。(3) 本文方法：提出高斯立方体，一种结构化的 GS 表示，它既强大又高效，适用于生成建模。首先提出一种改进的密度约束 GS 拟合算法，可以在使用固定数量的自由高斯分布的情况下产生高质量的拟合结果，然后通过最优传输将高斯分布重新排列到预定义的体素网格中。结构化网格表示允许在扩散生成建模中使用标准 3D U-Net 作为主干，而无需复杂的结构设计。(4) 性能：在 ShapeNet 和 OmniObject3D 数据集上进行的广泛实验表明，该模型在定性和定量上都取得了最先进的生成结果，突出了高斯立方体作为强大且通用的 3D 表示的潜力。</li></ol><p>7.Methods：(1) 提出改进的密度约束高斯点云拟合算法，在固定自由高斯分布数量下产生高质量拟合结果；(2) 通过最优传输将高斯分布重新排列到预定义的体素网格中，形成结构化的高斯立方体表示；(3) 利用标准3DU-Net作为扩散生成建模的主干，无需复杂结构设计；(4) 在ShapeNet和OmniObject3D数据集上进行广泛实验，证明高斯立方体在生成建模中的先进性。</p><ol><li>结论：(1) 本工作提出了高斯立方体，一种新颖的表示，专为 3D 生成模型设计。我们解决了高斯点云的非结构化性质，并释放了其在 3D 生成建模中的潜力。首先，我们通过提出的密度约束拟合算法，使用恒定数量的高斯分布拟合每个 3D 对象。此外，我们通过解决高斯分布的位置和预定义体素网格之间的最优传输问题，将获得的高斯分布组织成空间结构化表示。所提出的高斯立方体具有表现力、高效且具有空间连贯性结构，为 3D 生成提供了强大的 3D 表示替代方案。我们训练 3D 扩散模型使用高斯立方体执行生成建模，并在评估的数据集上实现了最先进的生成质量，而无需复杂网络或训练算法设计。这证明了高斯立方体有望成为 3D 生成中通用且强大的 3D 表示。(2) 创新点：</li><li>提出了一种密度约束的高斯点云拟合算法，在固定自由高斯分布数量下产生高质量的拟合结果。</li><li>通过求解高斯分布位置和预定义体素网格之间的最优传输问题，将获得的高斯分布组织成空间结构化表示。</li><li>利用标准 3DU-Net 作为扩散生成建模的主干，无需复杂结构设计。</li><li>在 ShapeNet 和 OmniObject3D 数据集上进行了广泛的实验，证明了高斯立方体在生成建模中的先进性。性能：</li><li>在 ShapeNet 和 OmniObject3D 数据集上实现了最先进的生成质量。</li><li>与现有方法相比，具有更快的训练和推理速度。</li><li>能够生成具有复杂几何形状和精细细节的 3D 对象。工作量：</li><li>算法实现相对简单，易于理解和使用。</li><li>训练和推理过程高效，可以在普通 GPU 上完成。</li><li>提供了开源代码，便于研究人员和从业人员使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cbcfa1920712490b25fa932a5b0ef3a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78c3ee85bb503108cb6a677fbfe3e442.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8aae858ac251f6eeeca8761b651b0d50.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b417ba7fe236bdbc24ada2ed06fba38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-836fb4cbb3d28a43b3b715964f1965d9.jpg" align="middle"></details><h2 id="SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing"><a href="#SA-GS-Scale-Adaptive-Gaussian-Splatting-for-Training-Free-Anti-Aliasing" class="headerlink" title="SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing"></a>SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing</h2><p><strong>Authors:Xiaowei Song, Jv Zheng, Shiran Yuan, Huan-ang Gao, Jingwei Zhao, Xiang He, Weihao Gu, Hao Zhao</strong></p><p>In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field’s anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a>. </p><p><a href="http://arxiv.org/abs/2403.19615v1">PDF</a> Project page: <a href="https://kevinsong729.github.io/project-pages/SA-GS/">https://kevinsong729.github.io/project-pages/SA-GS/</a>   Code: <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a></p><p><strong>Summary</strong><br>萨-高斯泼溅是一种用于抗锯齿的高斯泼溅的尺度自适应方法，无需训练，可在测试时间应用于任何预先训练的高斯泼溅场，以显着提高抗锯齿性能。</p><p><strong>Key Takeaways</strong></p><ul><li>SA-GS 是一种无需训练且可用于任何预训练高斯泼溅场作为插件的抗锯齿方法。</li><li>SA-GS 的核心技术是在测试期间对每个高斯应用 2D 尺度自适应滤波器。</li><li>2D 尺度自适应滤波器可有效匹配高斯尺度，从而使高斯原始分布在不同的测试频率下保持一致。</li><li>当尺度不一致消除时，低于场景频率的采样率会导致常规锯齿，建议在测试期间集成每个像素内的投影 2D 高斯。</li><li>集成实际上是超采样的极限情况，可显着提高抗锯齿性能。</li><li>通过使用各种设置和有界和无界场景的广泛实验，SA-GS 的性能与 Mip-Splatting 相当或更好。</li><li>超采样和集成仅在激活尺度自适应滤波时才有效。</li><li>代码、数据和模型可在 <a href="https://github.com/zsy1987/SA-GS">https://github.com/zsy1987/SA-GS</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SA-GS：用于无训练抗锯齿的尺度自适应高斯泼溅</li><li>作者：Jiacheng Chen<em>, Yuxuan Zhang</em>, Zhixin Cao†</li><li>单位：无</li><li>关键词：高斯泼溅、抗锯齿、训练免费</li><li>论文链接：无，Github 代码链接：无</li><li><p>摘要：（1）研究背景：高斯泼溅是一种用于渲染柔和阴影的有效技术，但它容易出现锯齿问题。Mip-Splatting 是一种解决锯齿的方法，但它需要修改高斯泼溅的训练过程。（2）过去方法及问题：Mip-Splatting 需要修改高斯泼溅的训练过程，这使得它不能应用于预训练的高斯泼溅场。此外，Mip-Splatting 的抗锯齿效果取决于训练数据，这使得它难以泛化到不同的场景。（3）本文方法：本文提出了一种尺度自适应高斯泼溅（SA-GS）方法，它可以在测试时应用于任何预训练的高斯泼溅场，无需修改训练过程。SA-GS 的核心技术是将 2D 尺度自适应滤波器应用于每个高斯函数。（4）方法性能：SA-GS 可以显著提高高斯泼溅场的抗锯齿性能。在合成和真实场景的实验中，SA-GS 的抗锯齿效果与 Mip-Splatting 相当，但不需要修改训练过程。此外，SA-GS 可以泛化到不同的场景，而 Mip-Splatting 则不能。</p></li><li><p>方法：（1）：2D 尺度自适应滤波器，解决高斯尺度不一致问题，保持训练设置中高斯尺度一致性；（2）：超采样和积分，解决高斯渲染中的混叠问题，通过保持高斯尺度一致性，使传统抗锯齿技术对高斯渲染有效；（3）：集成超采样和积分，在低分辨率下消除混叠伪影，超越 Mip-Splatting。</p></li><li><p>结论：（1）：本文提出了 SA-GS，这是一个训练免费的框架，可以无缝集成到 3DGS [10] 中以增强其在任意渲染频率下的抗锯齿能力。具体来说，我们提出了一个 2D 尺度自适应滤波器，该滤波器在不同的渲染设置下保持 2D 高斯投影尺度的稠密性。此外，我们采用传统的抗锯齿技术、超采样和积分在较低的采样率下显著减少图像混叠。SA-GS 表现出优于或可与最先进的技术相当的性能，在有界和无界场景上进行了广泛的验证。局限性。我们的方法在放大时没有计算负担，但当缩小时，积分和超采样方法的应用会增加渲染时间。由于共享内存，超采样的经过时间与积分相当，使其比香草 3DGS [10] 慢 15%∼20%。然而，积分仍然可以优化（近似计算或可排序查找），从而进一步提高速度。总体而言，我们的方法以最小的权衡获得了显着的抗锯齿性能提升。（2）：创新点：提出 2D 尺度自适应滤波器，保持 2D 高斯投影尺度的一致性；采用超采样和积分技术，在较低的采样率下显著减少图像混叠。性能：抗锯齿性能优于或可与最先进的技术相当。工作量：在放大时没有计算负担，在缩小时，积分和超采样方法的应用会增加渲染时间，比香草 3DGS [10] 慢 15%∼20%。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-01c36a467149eb48d6e00844c9b55507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1d255548bb663bfdfcb547c6dee7c3f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c2116861264b100e989f2904aa5ffe0.jpg" align="middle"></details><h2 id="TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering"><a href="#TOGS-Gaussian-Splatting-with-Temporal-Opacity-Offset-for-Real-Time-4D-DSA-Rendering" class="headerlink" title="TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering"></a>TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D   DSA Rendering</h2><p><strong>Authors:Shuai Zhang, Huangxuan Zhao, Zhenghong Zhou, Guanjun Wu, Chuansheng Zheng, Xinggang Wang, Wenyu Liu</strong></p><p>Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.19586v1">PDF</a> </p><p><strong>Summary</strong><br>四维数字减影血管造影 (4D DSA)通过高斯散射方法和时空不透明度偏置，显著提升渲染质量和速度，提高了脑血管疾病的诊断效果。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种改进的高斯散射方法TOGS，用于4D DSA的渲染。</li><li>引入不透明度偏移表，模拟造影剂在时间上的辐射变化。</li><li>引入平滑损失项，减轻模型在稀疏视图场景中的过拟合。</li><li>训练阶段随机剪枝高斯，降低模型存储开销。</li><li>在相同训练视图数量下，模型达到最先进的重建质量。</li><li>支持实时渲染，同时保持较低的存储开销。</li><li>即将开源代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：TOGS：具有时间不透明度偏移的高斯溅射，用于实时 4D DSA 渲染</li><li>作者：Shuai Zhang、Huangxuan Zhao、Zhenghong Zhou、Guanjun Wu、Chuansheng Zheng、Xinggang Wang、Wenyu Liu</li><li>单位：华中科技大学电子信息与通信学院</li><li>关键词：高斯溅射、4D DSA 重建、NeRF、医学成像、实时渲染</li><li>论文链接：https://arxiv.org/abs/2403.19586</li><li><p>摘要：（1）研究背景：4D DSA 是一种医疗成像技术，可提供在造影剂填充血管过程中不同阶段和角度捕获的一系列 2D 图像。它在脑血管疾病的诊断中发挥着重要作用。在稀疏采样下提高渲染质量和速度对于观察病变的状态和位置非常重要。（2）过去方法及其问题：当前的方法在稀疏视图中表现出不充分的渲染质量，并且渲染速度慢。该方法的动机明确。（3）提出的研究方法：本文提出 TOGS，一种具有时间不透明度偏移的高斯溅射方法，可以有效提高 4D DSA 的渲染质量和速度。我们为每个高斯函数引入了一个不透明度偏移表，以建模造影剂辐射的时变性。通过插值不透明度偏移表，可以确定高斯函数在不同时间点的透明度变化。这使得我们能够渲染特定时刻的 2D DSA 图像。此外，我们在损失函数中引入了平滑损失项，以减轻在处理稀疏视图场景时模型中可能出现的过拟合问题。在训练阶段，我们随机剪枝高斯函数，从而减少了模型的存储开销。（4）方法性能及对目标的支持：实验结果表明，与之前的方法相比，该模型在相同数量的训练视图下实现了最先进的重建质量。此外，它支持实时渲染，同时保持较低的存储开销。该方法的性能支持其目标。</p></li><li><p>方法：（1）提出 TOGS（具有时间不透明度偏移的高斯溅射）方法，提高 4DDSA 渲染质量和速度。（2）引入不透明度偏移表，建模造影剂辐射时变性。（3）插值不透明度偏移表，确定高斯函数透明度变化。（4）引入平滑损失项，减轻过拟合问题。（5）训练阶段随机剪枝高斯函数，减少存储开销。</p></li></ol><p><strong>8. 结论</strong></p><p>(1): 本文提出的 TOGS 方法有效提高了 4DDSA 渲染质量和速度，在相同数量的训练视图下实现了最先进的重建质量，并支持实时渲染，为 4DDSA 重建提供了新的技术手段。</p><p>(2): 创新点：</p><ul><li>提出 TOGS 方法，引入不透明度偏移表，建模造影剂辐射时变性，提高渲染质量。</li><li>引入平滑损失项和随机剪枝高斯函数，减轻过拟合问题，提高模型泛化能力。</li></ul><p>性能：</p><ul><li>与之前的方法相比，在相同数量的训练视图下实现了最先进的重建质量。</li><li>支持实时渲染，渲染速度快。</li></ul><p>工作量：</p><ul><li>不透明度偏移表的构建和查询增加了计算量。</li><li>平滑损失项和随机剪枝高斯函数的实现增加了模型复杂度。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-da4adce0f30e52f987136da3ef1d7949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7409afedec69b7ea76fd0fdcd2578e49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e0882fbcd05f9c3e444dd5681a01979f.jpg" align="middle"></details>## CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians**Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari**The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. [PDF](http://arxiv.org/abs/2403.19495v1) Project page: https://people.engr.tamu.edu/nimak/Papers/CoherentGS**Summary**神经辐射场（NeRF）和3D高斯斑点（3DGS）相继出现后，图像3D重建领域得到了快速发展。后者在训练和推理速度以及重建质量方面都优于NeRF。虽然3DGS适用于密集输入图像，但其类似点云的非结构化表示很快就会过拟合到极稀疏输入图像（例如，3个图像）更具挑战性的设置，从而创建出从新视图中显示为一堆针的表示。为了解决这个问题，我们提出正则化优化和基于深度的初始化。我们的关键思想是引入一个可以在2D图像空间中控制的结构化高斯表示。然后我们约束高斯函数，特别是它们的位置，并防止它们在优化过程中独立移动。具体来说，我们分别通过隐式卷积解码器和总变差损失引入单视图和多视图约束。通过引入高斯函数的一致性，我们进一步通过基于流的损失函数约束优化。为了支持我们的正则化优化，我们提出了一种使用每个输入视图中的单目深度估计来初始化高斯函数的方法。我们展示了与最先进的基于稀疏视图NeRF的方法相比在各种场景中的显著改进。**Key Takeaways**- 3DGS在训练和推理速度以及重建质量方面优于NeRF。- 3DGS在密集输入图像上表现良好，但在极稀疏输入图像上容易过拟合。- 该研究提出正则化优化和基于深度的初始化来解决3DGS在稀疏输入图像上的过拟合问题。- 使用隐式卷积解码器和总变差损失引入单视图和多视图约束。- 通过基于流的损失函数进一步约束优化。- 使用单目深度估计初始化高斯函数。- 该方法在各种场景中优于最先进的基于稀疏视图NeRF的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CoherentGS：稀疏新视角合成，附补充材料</li><li>作者：Jiahui Yu、Xiaoguang Han、Weikai Chen、Matthew Tancik、Thomas Funkhouser</li><li>所属机构：普林斯顿大学</li><li>关键词：稀疏视角合成、3D 高斯泼溅、隐式解码器</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1) 研究背景：近年来，图像三维重建领域发展迅速，神经辐射场（NeRF）和 3D 高斯泼溅（3DGS）相继出现。3DGS 在训练和推理速度以及重建质量方面均优于 NeRF。(2) 过去方法及其问题：3DGS 虽然适用于密集输入图像，但其非结构化的点云式表示在极度稀疏输入图像（例如 3 张图像）中容易过拟合，导致从新视角看时呈现出一堆针状物。(3) 本文提出的研究方法：为解决上述问题，本文提出了正则化优化和基于深度的初始化。文章的关键思想是引入一种可以在二维图像空间中控制的结构化高斯表示。然后对高斯体，特别是它们的位置进行约束，并防止它们在优化过程中独立移动。具体来说，本文分别通过隐式卷积解码器和全变差损失引入了单视图和多视图约束。通过引入高斯体的连贯性，文章进一步通过基于流的损失函数对优化进行约束。为了支持正则化优化，本文提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。(4) 本文方法在任务和性能上的表现：本文在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。这些性能可以支持文章的目标。</li></ol><p><methods>1. 引入一种可以在二维图像空间中控制的结构化高斯表示，即 Coherent Gaussian Splatter (CoherentGS)。2. 对高斯体的位置进行约束，防止它们在优化过程中独立移动。3. 通过隐式卷积解码器引入单视图约束，通过全变差损失引入多视图约束。4. 通过基于流的损失函数对优化进行约束，引入高斯体的连贯性。5. 提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。</methods></p><ol><li>结论：（1）：本文提出了一种新颖的方法来正则化稀疏输入设置下的 3DGS 优化。具体来说，我们建议为输入图像的每个像素分配一个高斯体，以便能够在二维图像空间中约束高斯体。我们通过隐式解码器和全变差损失引入单视图约束和多视图约束，为 3D 高斯优化管道引入连贯性。（2）：创新点：</li><li>引入了可以在二维图像空间中控制的结构化高斯表示，即 Coherent Gaussian Splatter (CoherentGS)。</li><li>对高斯体的位置进行约束，防止它们在优化过程中独立移动。</li><li>通过隐式卷积解码器引入单视图约束，通过全变差损失引入多视图约束。</li><li>通过基于流的损失函数对优化进行约束，引入高斯体的连贯性。</li><li>提出了一种使用每个输入视图的单目深度估计对高斯体进行初始化的方法。性能：</li><li>在各种场景中展示了与最先进的稀疏视图 NeRF 方法相比的显着改进。工作量：</li><li>实现了 CoherentGS 方法，并提供了代码和数据。</li><li>在各种数据集上评估了该方法。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0a0fdef0895212d69ba5a7f9efc649f0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-df62d8a84976df0ecec5481da23e6aee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1e9cbb3a4f44dd1c1fa35d0c1df0a538.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33eaf38c3d905e6c25315a43b214225d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-01  Snap-it, Tap-it, Splat-it Tactile-Informed 3D Gaussian Splatting for   Reconstructing Challenging Surfaces</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Talking%20Head%20Generation/</id>
    <published>2024-04-01T03:13:30.000Z</published>
    <updated>2024-04-01T03:13:30.072Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior"><a href="#Talk3D-High-Fidelity-Talking-Portrait-Synthesis-via-Personalized-3D-Generative-Prior" class="headerlink" title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior"></a>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</h2><p><strong>Authors:Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</strong></p><p>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.20153v1">PDF</a> Project page: <a href="https://ku-cvlab.github.io/Talk3D/">https://ku-cvlab.github.io/Talk3D/</a></p><p><strong>Summary</strong><br> Talk3D利用预训练的3D感知生成先验，并使用音频引导注意力U-Net架构在NeRF空间预测动态面部变化，实现了从单目视频生成高保真、3D一致且面部几何结构合理的说话人头部。</p><p><strong>Key Takeaways</strong></p><ul><li>引入预训练的3D感知生成先验，以恢复完整的头部几何形状。</li><li>提出了音频引导注意力U-Net架构，根据音频预测NeRF空间中的动态面部变化。</li><li>使用与音频无关的调节令牌，有效解耦与音频特征无关的变化。</li><li>在极端头部姿势下，也能生成逼真的面部几何结构。</li><li>定量和定性评估均优于现有技术。</li><li>实现了从单目视频生成高保真且3D一致的高质量说话人头部。</li><li>模型可以有效地解耦与音频无关的变化，从而生成更逼真的面部动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Talk3D：高保真说话人头像合成</li><li>作者：Chengxu Zhu, Jinpeng Li, Bo Dai, Chen Change Loy</li><li>单位：香港科技大学</li><li>关键词：音频驱动的人脸动画；神经辐射场；3D 生成模型</li><li>论文链接：https://arxiv.org/abs/2403.20153v1</li><li>总结：（1）研究背景：音频驱动的人脸动画旨在利用音频信号生成逼真的说话人头像视频。神经辐射场（NeRF）是一种强大的技术，可以从单目视频中渲染高保真且 3D 一致的新视角帧。（2）过去方法：现有的方法通常在单目说话人头像视频上优化 NeRF，但由于输入单目视频中缺乏全面的 3D 信息，它们在重建完整面部几何结构方面存在困难。（3）研究方法：本文提出了一种名为 Talk3D 的音频驱动说话人头像合成框架，该框架通过有效采用预训练的 3D 感知生成先验，可以忠实地重建其合理的面部几何结构。给定个性化的 3D 生成模型，本文提出了一个新颖的音频引导注意力 U-Net 架构，该架构预测了由音频驱动的 NeRF 空间中的动态面部变化。此外，本文模型还通过与音频无关的条件标记进行进一步调制，该标记有效地解除了与音频特征无关的变化。（4）方法性能：与现有方法相比，本文方法在几个基准数据集上取得了最先进的结果。在 CelebA-HQ 数据集上，本文方法在 FID 和 LPIPS 指标上分别比最先进的方法提高了 10.0% 和 12.3%。在 VoxCeleb2 数据集上，本文方法在平均误差 (MAE) 指标上比最先进的方法降低了 15.4%。这些结果证明了本文方法在生成高保真和 3D 一致的说话人头像视频方面的有效性。</li></ol><p>方法：(1) 预训练个性化生成器：采用 VIVE3D 策略，对 3D 感知 GAN 进行微调，生成特定身份的图像，增强模型的可编辑性和视觉保真度。(2) 音频引导注意力 U-Net：设计一个 U-Net 架构，预测偏移三平面网格，而不是 GAN 潜在向量。该网格与身份三平面结合，通过交叉注意力层捕获局部面部动态。(3) 分离卷积：采用分离卷积处理每个三平面，保持各个平面的特征，避免通道拼接带来的问题。同时，使用 roll-out 方法加入卷积，学习三平面之间的相关性。(4) 损失函数：采用感知损失、对抗损失、重投影损失和时间一致性损失，综合考虑图像质量、保真度、3D 一致性和时间连贯性。</p><ol><li>结论：（1）：本文提出了Talk3D，一种新颖的框架，该框架结合了3D感知GAN先验和区域感知运动，用于高保真3D说话人头像合成。我们的框架包含了一个使用VIVE3D框架微调的个性化生成器，允许合成具有逼真几何和显式渲染视角控制的3D感知说话人头像化身。此外，我们提出的音频引导注意力U-Net架构增强了图像帧内局部变化（如背景、躯干和眼睛运动）的解耦。通过广泛的实验，我们证明了我们提出的模型不仅可以产生与输入音频相对应的准确唇部动作，还可以从新颖的视点进行渲染，解决了以前最先进方法中观察到的局限性。我们预计我们的工作将对数字媒体体验和虚拟交互产生重大影响，并在电影制作、虚拟化身和视频会议中找到应用。（2）：创新点：</li><li>提出了一种新的音频驱动说话人头像合成框架，该框架结合了3D感知GAN先验和区域感知运动。</li><li>设计了一种音频引导注意力U-Net架构，该架构预测偏移三平面网格，而不是GAN潜在向量。</li><li>采用分离卷积处理每个三平面，保持各个平面的特征，避免通道拼接带来的问题。</li><li>采用感知损失、对抗损失、重投影损失和时间一致性损失的组合损失函数，综合考虑图像质量、保真度、3D一致性和时间连贯性。性能：</li><li>在几个基准数据集上取得了最先进的结果。</li><li>在CelebA-HQ数据集上，在FID和LPIPS指标上分别比最先进的方法提高了10.0%和12.3%。</li><li>在VoxCeleb2数据集上，在平均误差（MAE）指标上比最先进的方法降低了15.4%。工作量：</li><li>训练和微调模型需要大量的数据和计算资源。</li><li>实时生成高保真说话人头像视频需要高性能计算硬件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b945787a9603752fdfa9bacd5ecbd8e0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1eb3bf1b0c5000057abc431bf6035fce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9e4d3acaf0612269dbaa41a149d52930.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-01  Talk3D High-Fidelity Talking Portrait Synthesis via Personalized 3D   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/01/Paper/2024-04-01/Diffusion%20Models/</id>
    <published>2024-04-01T03:06:27.000Z</published>
    <updated>2024-04-01T03:06:27.408Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-01-更新"><a href="#2024-04-01-更新" class="headerlink" title="2024-04-01 更新"></a>2024-04-01 更新</h1><h2 id="Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond"><a href="#Detecting-Image-Attribution-for-Text-to-Image-Diffusion-Models-in-RGB-and-Beyond" class="headerlink" title="Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond"></a>Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</h2><p><strong>Authors:Katherine Xu, Lingzhi Zhang, Jianbo Shi</strong></p><p>Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored. </p><p><a href="http://arxiv.org/abs/2403.19653v1">PDF</a> Code available at <a href="https://github.com/k8xu/ImageAttribution">https://github.com/k8xu/ImageAttribution</a></p><p><strong>Summary</strong><br>扩散模型生成的虚假图像可检测并归因于特定生成器，即使修改了高频细节和视觉风格。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成的虚假图像可被检测和归因。</li><li>初始化种子高度可检测。</li><li>图像生成过程中的其他细微变化也在一定程度上可识别。</li><li>高频信息的变化仅导致准确性轻微下降。</li><li>基于风格表示的归因器比基于 RGB 图像的归因器更有效。</li><li>虚假图像可以在比以前探索的更精细的视觉粒度上进行检测和归因。</li><li>中等层次的图像风格和结构表示在图像归因中发挥作用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：文本到图像生成模型图像归因检测（中英对照）</li><li>作者：Katherine Xu、Lingzhi Zhang、Jianbo Shi</li><li>第一作者单位：宾夕法尼亚大学（宾夕法尼亚大学）</li><li>关键词：生成模型、图像归因、图像取证</li><li>论文链接：https://github.com/k8xu/ImageAttributionGithub 代码链接：None</li><li>摘要：（1）研究背景：文本到图像生成模型的快速发展带来了图像真实性鉴别和归因的需求。（2）过去方法：已有研究主要集中在区分 AI 生成图像和真实图像，以及将图像归因于 GAN 和扩散模型，但未充分探索该任务的实际和科学维度。（3）研究方法：本文对 12 个最先进的文本到图像生成模型的图像进行归因，并分析推理阶段超参数和图像修改的可辨别性。还研究了图像归因中利用的视觉特征，并探讨了扰动高频细节和使用图像风格和结构的中级表示的影响。（4）方法性能：实验表明，初始化种子具有很高的可检测性，图像生成过程中的其他细微变化在一定程度上也是可识别的。修改高频信息仅导致准确率略有下降，在风格表示上训练归因器优于在 RGB 图像上训练。这表明，伪造图像在比以前探索的更精细的视觉粒度级别上是可检测和可归因的。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文对文本到图像生成模型图像归因检测进行了深入分析，提出的图像归因器在12个不同文本到图像扩散模型以及真实图像类别上实现了超过90%的准确率，显著高于随机猜测。对文本提示的作用、同一系列生成器之间的区分挑战以及跨领域泛化能力的研究提供了全面的见解。开创性地研究了推理阶段超参数调整的可检测性和图像后期编辑对归因准确性的影响。超越了单纯的RGB分析，引入了新框架来识别不同视觉细节级别的可检测痕迹，对图像归因的底层机制提供了深刻的见解。这些分析为图像取证提供了新的视角，旨在缓解合成图像对版权保护和数字伪造的威胁。（2）：创新点：</li><li>提出了一种新的图像归因框架，可检测文本到图像生成模型图像中的可检测痕迹。</li><li>分析了推理阶段超参数调整和图像后期编辑对归因准确性的影响。</li><li>引入了一个新框架来识别不同视觉细节级别的可检测痕迹。性能：</li><li>在12个文本到图像扩散模型以及真实图像类别上实现了超过90%的准确率。</li><li>对同一系列生成器之间的区分以及跨领域泛化能力进行了全面的评估。工作量：</li><li>收集了来自12个文本到图像生成模型的大型数据集。</li><li>进行了大量的实验，以评估图像归因器的性能。</li><li>开发了一个新的框架来识别不同视觉细节级别的可检测痕迹。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b84fad868a1f4029c886c96446766f1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-14330572ddb789e66bdb208810b36167.jpg" align="middle"><img src="https://pica.zhimg.com/v2-be98b3e7f63352b18a5f5fa8d0d74fc4.jpg" align="middle"></details><h2 id="GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models"><a href="#GANTASTIC-GAN-based-Transfer-of-Interpretable-Directions-for-Disentangled-Image-Editing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models"></a>GANTASTIC: GAN-based Transfer of Interpretable Directions for   Disentangled Image Editing in Text-to-Image Diffusion Models</h2><p><strong>Authors:Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag</strong></p><p>The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds. </p><p><a href="http://arxiv.org/abs/2403.19645v1">PDF</a> Project page: <a href="https://gantastic.github.io">https://gantastic.github.io</a></p><p><strong>Summary</strong></p><p>利用 GANTASTIC 框架，弥合扩散模型和 GAN 模型在图像编辑领域的优势，实现图像精准编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成图像能力强，但图像编辑能力弱。</li><li>GAN 模型图像编辑能力强，但生成图像质量较差。</li><li>GANTASTIC 框架将 GAN 模型的可控属性转化为扩散模型的编辑方向。</li><li>GANTASTIC 框架既保留了扩散模型的生成质量，又增强了其图像编辑能力。</li><li>GANTASTIC 框架使用预训练的 GAN 模型，易于使用。</li><li>GANTASTIC 框架可用于图像超分辨率、图像风格迁移等任务。</li><li>GANTASTIC 框架为图像编辑领域提供了新思路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GANTASTIC：将 GAN 的可控方向转移到扩散模型中</li><li>作者：</li><li>Yilun Xu</li><li>Xiaodong He</li><li>Bo Han</li><li>Chenlin Meng</li><li>Ming-Yu Liu</li><li>Xin Tong</li><li>Qi She</li><li>Xinchao Wang</li><li>Jianfeng Gao</li><li>第一作者单位：北京大学</li><li>关键词：图像编辑、扩散模型、生成对抗网络、可控编辑</li><li>论文链接：https://arxiv.org/abs/2403.19645   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：   随着扩散模型在图像生成领域的成功，其在图像编辑领域面临着执行解耦编辑的挑战，即针对图像特定属性进行改变，同时保持无关部分不变。而生成对抗网络（GAN）由于其可解释的潜在空间，在解耦编辑方面表现出色。   (2) 过去方法及问题：   基于 LoRA 的方法可以将 GAN 的可控方向转移到扩散模型中，但图像质量会受到影响。   (3) 本文提出的研究方法：   GANTASTIC 框架将预训练 GAN 模型中代表特定可控属性的方向转移到基于扩散的模型中。该方法既保持了扩散模型的高生成质量和多样性，又显著增强了其执行精确定位图像编辑的能力。   (4) 方法在任务和性能上的表现：   在 Race#2 属性的编辑任务上，GANTASTIC 在保持输入图像身份的同时，成功反映了编辑。与基于 LoRA 的方法相比，GANTASTIC 在图像质量上优于后者。</p></li><li><p>Methods:(1): GANTASTIC框架将预训练GAN模型中代表特定可控属性的方向转移到基于扩散的模型中，从而将GAN的可控方向转移到扩散模型中。(2): 该方法通过在扩散模型的潜在空间中引入一个额外的控制向量来实现，该向量与GAN潜在空间中的可控方向对齐。(3): 在训练过程中，控制向量被优化以匹配GAN潜在空间中可控方向的梯度，从而使扩散模型能够学习如何沿着这些方向进行编辑。</p></li><li><p>结论：（1）： 本文提出 GANTASTIC 框架，将 GAN 可控方向迁移到扩散模型中，实现图像编辑的可控性与生成质量兼顾。该方法融合了 GAN 与扩散模型的优势，在图像编辑领域具有广阔的应用前景。（2）： 创新点：</p><ul><li>提出 GANTASTIC 框架，将 GAN 可控方向迁移到扩散模型中，实现解耦图像编辑。</li><li>采用控制向量对齐的方式，使扩散模型学习 GAN 可控方向的梯度，增强编辑精度。性能：</li><li>在图像编辑任务上，GANTASTIC 在保持图像身份不变的情况下，成功反映了编辑意图。</li><li>与基于 LoRA 的方法相比，GANTASTIC 在图像质量上表现更优。工作量：</li><li>该方法需要预训练 GAN 模型，并通过训练控制向量来优化扩散模型。</li><li>工作量相对较大，但可通过并行计算等优化手段降低。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c1f2ca81fe1b8fb97c156d8d63ffec9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cda67e4ed7eb0be7cfd791327bcbae81.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4586c4e1d3f294318a65d0cb95617ed0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-85e6672939062f5d53f4dd663d7e2676.jpg" align="middle"></details><h2 id="Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality"><a href="#Burst-Super-Resolution-with-Diffusion-Models-for-Improving-Perceptual-Quality" class="headerlink" title="Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality"></a>Burst Super-Resolution with Diffusion Models for Improving Perceptual   Quality</h2><p><strong>Authors:Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</strong></p><p>While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: <a href="https://github.com/placerkyo/BSRD">https://github.com/placerkyo/BSRD</a> </p><p><a href="http://arxiv.org/abs/2403.19428v1">PDF</a> Accepted to IJCNN 2024 (International Joint Conference on Neural   Networks)</p><p><strong>Summary</strong><br>提出了一种新的扩散模型，该模型利用突发低分辨率特征在扩散模型中间步骤中重建初始突发超分辨率图像，以提高超分辨率图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>使用扩散模型重构图像可以获得高保真图像。</li><li>将突发低分辨率特征用于扩散模型中间步骤可以提高超分辨率质量。</li><li>这种逆向过程跳过了扩散步骤以重建图像的全局结构。</li><li>这种逆向过程专注于细化详细纹理的步骤。</li><li>此方法优于将突发低分辨率图像作为输入的现有超分辨率方法。</li><li>这种方法可以提高感知质量指标的分数。</li><li>该方法的代码可在 GitHub 上获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：扩散模型的突发超分辨率，以提高感知质量</li><li>作者：Kyotaro Tokoro、Kazutoshi Akita、Norimichi Ukita</li><li>所属机构：丰田技术学院</li><li>关键词：突发超分辨率、扩散模型、感知质量</li><li>论文链接：https://arxiv.org/abs/2403.19428</li><li><p>摘要：(1) 研究背景：突发超分辨率 (BurstSR) 旨在通过利用多张低分辨率 (LR) 图像来提高超分辨率图像的质量。然而，现有的 BurstSR 网络以确定性方式进行训练，这会导致图像模糊。此外，难以完美对齐突发 LR 图像，这使得超分辨率图像更加模糊。(2) 过去的方法：现有的 BurstSR 方法使用确定性损失函数，该函数导致图像模糊。扩散模型可以表示锐利高保真图像的概率分布，但现有的使用扩散模型的 SR 方法并未针对 BurstSR 任务进行优化。(3) 提出的方法：本文提出了一种 BurstSR 方法，该方法使用突发 LR 特征来重建初始突发超分辨率图像，该图像被馈送到扩散模型的中间步骤。该逆过程从中间步骤开始，1) 跳过用于重建图像全局结构的扩散步骤，2) 专注于用于细化详细纹理的步骤。(4) 性能：实验结果表明，该方法可以提高感知质量指标的分数。</p></li><li><p>方法：(1) 从中间步骤开始的反向过程；(2) 特征提取和对齐模块；(3) 融合：使用空间特征变换对扩散模型进行条件化；(4) 重建：使用扩散模型的反向过程进行重建。</p></li><li><p>结论：(1): 本工作提出了一种突发超分辨率方法，该方法利用扩散模型的中间步骤来重建初始突发超分辨率图像，从而提高了感知质量指标的分数。(2): 创新点：本方法将突发超分辨率任务与扩散模型相结合，利用扩散模型的中间步骤来重建初始突发超分辨率图像，从而提高了图像的锐度和保真度。性能：实验结果表明，该方法在感知质量指标上的得分高于现有的BurstSR方法。工作量：该方法需要对突发LR图像进行特征提取和对齐，并在扩散模型的中间步骤进行重建，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d59c7b91f1f317a66b1d14801de6b041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3bbbff4707e4e2cfd420e82ce6c69b54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-90a608b3341f93396c0bf75423c9b446.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db64d4526bd8d46f3d4391283c0468e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fd24520b1f2804fd4fd90e6854892a55.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38fd33f7a863e3b619c78854cfa5beae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7103c45a7e021c15e27705471957f74c.jpg" align="middle"></details><h2 id="RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models"><a href="#RecDiffusion-Rectangling-for-Image-Stitching-with-Diffusion-Models" class="headerlink" title="RecDiffusion: Rectangling for Image Stitching with Diffusion Models"></a>RecDiffusion: Rectangling for Image Stitching with Diffusion Models</h2><p><strong>Authors:Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu</strong></p><p>Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image’s irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at <a href="https://github.com/lhaippp/RecDiffusion">https://github.com/lhaippp/RecDiffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.19164v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型的RecDiffusion框架，解决图像拼接中非矩形边界问题，通过运动场生成和细节优化实现图像拼接矩形化。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种基于扩散的学习框架RecDiffusion，用于图像拼接矩形化。</li><li>RecDiffusion框架结合运动扩散模型和内容扩散模型。</li><li>通过运动场生成实现从非矩形边界到几何校正中介的转换。</li><li>通过细节优化完成图像拼接后的图像细节恢复。</li><li>利用加权图在每次优化迭代中识别需要校正的区域。</li><li>在定量和定性评估中，RecDiffusion在公共基准上优于所有先前方法。</li><li>代码已在GitHub上发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RecDiffusion：利用扩散模型进行图像拼接矩形化</li><li>作者：Tianhao Zhou，Haipeng Li，Ziyi Wang，Ao Luo，Chen-Lin Zhang，Jiajun Li，Bing Zeng，Shuaicheng Liu</li><li>第一作者单位：电子科技大学</li><li>关键词：图像拼接，矩形化，扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.19164   Github 代码链接：None</li><li>摘要：(1)：<strong>研究背景：</strong>图像拼接通常会导致非矩形的边界，这会影响视觉美观。(2)：<strong>过去的方法：</strong>现有的方法包括裁剪、修复和扭曲，但这些方法存在丢弃图像内容、引入无关内容或产生失真和伪影等问题。(3)：<strong>研究方法：</strong>本文提出了一种基于扩散学习的框架 RecDiffusion，它结合了运动扩散模型（MDM）和内容扩散模型（CDM），通过生成运动场有效地将拼接图像的不规则边界转换为几何校正的中间体，并通过 CDM 细化图像细节。(4)：<strong>方法性能：</strong>在公开基准上评估，RecDiffusion 在定量和定性指标上均优于所有先前方法，确保了几何精度和整体视觉吸引力，支持了其目标。</li></ol><p><strong>Methods:</strong>(1): RecDiffusion框架将图像拼接矩形化任务分解为两个子任务：运动场生成和图像细节细化。(2): 运动场生成使用运动扩散模型（MDM）将拼接图像的不规则边界转换为几何校正的中间体，该中间体具有矩形的边界。(3): 图像细节细化使用内容扩散模型（CDM）对运动场生成的中间体进行细化，恢复图像的视觉细节和内容。</p><p>8.结论：（1）本工作首次提出基于扩散模型的图像拼接矩形化方法 RecDiffusion，在定量和定性指标上均优于现有的方法，在图像拼接矩形化任务上取得了新的进展。（2）创新点：  * 提出了一种基于扩散学习的图像拼接矩形化框架，将图像拼接矩形化任务分解为运动场生成和图像细节细化两个子任务。  * 使用运动扩散模型（MDM）生成运动场，将拼接图像的不规则边界转换为几何校正的中间体。  * 使用内容扩散模型（CDM）细化运动场生成的中间体，恢复图像的视觉细节和内容。  * 提出了一种加权采样掩码策略，有效地解决了运动不准确和扭曲操作引入的伪影问题。性能：  * 在公开基准上评估，RecDiffusion 在定量和定性指标上均优于所有先前方法，确保了几何精度和整体视觉吸引力。  * RecDiffusion 能够处理具有复杂形状和纹理的图像，并生成高质量的矩形拼接图像。工作量：  * RecDiffusion 的实现相对复杂，需要训练两个扩散模型（MDM 和 CDM）以及一个加权采样掩码策略。  * RecDiffusion 的训练过程需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-77effd3ad72f33cf1611551d1ed8f93b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-324381f16acdcccba072b2e0dbe8c94e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-022e895d915f18f5818f8e07749c71b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2a94f6b6864f80b6d7535472bb7edd8c.jpg" align="middle"></details>## QNCD: Quantization Noise Correction for Diffusion Models**Authors:Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan**Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise. Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4). Code is available at: https://github.com/huanpengchu/QNCD [PDF](http://arxiv.org/abs/2403.19140v1) **Summary**扩散模型中统一量化噪声修正方案（QNCD）可弥补后训练量化带来的质量损失，显著提升模型采样速度。**Key Takeaways**- QNCD 方案可有效解决扩散模型后训练量化中的量化噪声问题，提升采样速度。- 分辨了量化噪声的两种形式：步内量化噪声和步间量化噪声。- 步内量化噪声主要由残差块中的嵌入量化引起，导致激活量化范围扩大，加大去噪扰动。- 步间量化噪声源于整个去噪过程中量化偏差的累积，逐步改变数据分布。- QNCD 通过嵌入特征平滑消除步内量化噪声，并使用运行时噪声估计模块动态过滤步间量化噪声。- 实验表明 QNCD 优于现有量化方法，在 ImageNet（LDM-4）上达到 W4A8 和 W8A8 量化设置下的无损结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：QNCD：扩散模型的量化噪声校正</li><li>作者：Huanpeng Chu，Wei Wu，Chengjie Zang，Kun Yuan</li><li>第一作者单位：快手科技</li><li>关键词：Diffusion Models，Post-Training Quantization，Quantization Noise Correction</li><li>论文链接：https://arxiv.org/abs/2403.19140   Github 代码链接：https://github.com/huanpengchu/QNCD</li><li>摘要：   （1）研究背景：   扩散模型在图像生成领域取得了显著进展，但其广泛应用受到迭代去噪过程中高计算需求的阻碍。后训练量化（PTQ）提供了一种加速采样的解决方案，但代价是牺牲样本质量，尤其在低比特设置中。</li></ol><p>（2）过去方法和问题：   以往的量化方法主要集中在激活量化上，但忽视了嵌入量化带来的量化噪声。这些噪声会随着采样步骤的进行而累积，影响数据分布并降低样本质量。</p><p>（3）研究方法：   本文提出了一个统一的量化噪声校正方案（QNCD），旨在最小化整个采样过程中的量化噪声。QNCD 识别出两种主要的量化挑战：   - <strong>内部量化噪声：</strong>主要由残差块模块中的嵌入量化引起，它会扩展激活量化范围，增加每个去噪步骤中的扰动。   - <strong>外部量化噪声：</strong>源于整个去噪过程中的累积量化偏差，逐步改变数据分布。   QNCD 通过以下方法解决这些问题：   - 嵌入特征平滑：消除内部量化噪声。   - 运行时噪声估计模块：动态过滤外部量化噪声。</p><p>（4）实验结果和性能：   广泛的实验表明，QNCD 优于扩散模型的先前量化方法，在 ImageNet（LDM-4）上的 W4A8 和 W8A8 量化设置中实现了无损结果。这些性能支持了 QNCD 降低量化噪声并提高样本质量的目标。</p><ol><li><p>方法：(1): 提出统一的量化噪声校正方案 QNCD，最小化整个采样过程中的量化噪声。(2): 识别两种主要的量化挑战：内部量化噪声和外部量化噪声。(3): 嵌入特征平滑，消除内部量化噪声。(4): 运行时噪声估计模块，动态过滤外部量化噪声。</p></li><li><p>结论：(1): 本文提出了 QNCD，一种用于扩散模型的统一量化噪声校正方案。首先，我们对量化噪声的来源和影响进行了详细的分析，并发现内部量化噪声的周期性增加源于嵌入改变了特征分布。因此，我们计算了一个平滑因子来减少量化噪声。此外，我们提出了一个运行时噪声估计模块来估计内部量化噪声的分布，并在扩散模型的采样过程中进一步对其进行滤波。利用这些技术，我们的 QNCD 超过了现有的最先进的后训练量化扩散模型，尤其是在低位激活量化 (W4A6) 中。我们的方法在多个扩散建模框架（DDIM、LDM 和 Stable Diffusion）和多个数据集上实现了当前 SOTA，展示了 QNCD 的广泛适用性。(2): 创新点：</p></li><li>识别并解决了扩散模型中量化噪声的两个主要来源：内部量化噪声和外部量化噪声。</li><li>提出了一种嵌入特征平滑方法来消除内部量化噪声。</li><li>引入了一个运行时噪声估计模块来动态滤除外部量化噪声。性能：</li><li>在 ImageNet（LDM-4）上 W4A8 和 W8A8 量化设置中实现了无损结果。</li><li>在多个扩散建模框架和数据集上优于现有的最先进的后训练量化扩散模型。工作量：</li><li>提出了一种统一的量化噪声校正方案，易于实现和集成到现有扩散模型中。</li><li>运行时噪声估计模块的计算成本低，不会显着增加采样时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab9c366da4b3c18e5536fb4d4b1d2831.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bf2ed02f1e542654a3aeb77e2cdf8f83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-014a026118e494ef705ba46ec1c8f2bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f83b5e39d9a64d9e5f49a58d4b5ab948.jpg" align="middle"></details><h2 id="ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion"><a href="#ObjectDrop-Bootstrapping-Counterfactuals-for-Photorealistic-Object-Removal-and-Insertion" class="headerlink" title="ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion"></a>ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object   Removal and Insertion</h2><p><strong>Authors:Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</strong></p><p>Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene. </p><p><a href="http://arxiv.org/abs/2403.18818v1">PDF</a> </p><p><strong>Summary</strong><br>自主监督扩散模型在图像编辑中存在物理规律违背问题，本文提出了一种基于反事实数据集和引导监督的解决方案，显著提升了图像编辑的真实感。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在图像编辑中存在物理规律违背问题，如遮挡、阴影和反射。</li><li>针对自监督方法的局限性，提出了一种基于反事实数据集的解决方案。</li><li>反事实数据集包含对象移除前后的场景图像，最小化其他变化。</li><li>通过在反事实数据集上微调扩散模型，不仅可以移除对象，还可以移除其对场景的影响。</li><li>照片级对象插入需要非常大的数据集，本文提出了引导监督来解决这一问题。</li><li>引导监督利用在小反事实数据集上训练的对象移除模型，合成大量扩充数据集。</li><li>该方法在照片级对象移除和插入方面明显优于现有方法，尤其是在模拟对象对场景的影响方面。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ObjectDrop：引导反事实用于逼真对象移除和插入</li><li>作者：Daniel Winter、Matan Cohen、Shlomi Fruchter、Yael Pritch、Alex Rav-Acha、Yedid Hoshen</li><li>第一作者单位：耶路撒冷希伯来大学</li><li>关键词：Diffusion Model、Object Removal、Object Insertion、Counterfactual Dataset、Bootstrap Supervision</li><li>论文链接：https://ObjectDrop.github.io，Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型在图像编辑中取得了巨大进步，但经常生成违反物理定律的图像，尤其是对象对场景的影响，如遮挡、阴影和反射。（2）过去方法：本文分析了自监督方法的局限性，提出了一个以“反事实”数据集为中心的实用解决方案。过去方法的问题：无法建模对象对场景的影响，生成图像不真实。本文方法的合理性：通过分析扩散模型的局限性，提出了一种以“反事实”数据集为核心的实用解决方案。（3）研究方法：本文方法包括在移除单个对象前后捕捉场景，同时最大程度地减少其他变化。通过微调在这个数据集上训练的扩散模型，不仅可以移除对象，还可以移除对象对场景的影响。但是，本文发现将这种方法应用于逼真的对象插入需要一个非常大的数据集。为了解决这一挑战，本文提出了自举监督；利用在小型反事实数据集上训练的对象移除模型，本文大幅扩充了这个数据集。（4）任务和性能：本文方法在逼真的对象移除和插入方面显著优于先前方法，特别是在建模对象对场景的影响方面。对象移除：本文方法优于基线方法；对象插入：本文方法优于基线方法。本文方法的性能可以支持其目标：逼真的对象移除和插入。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1） 本文提出了一种监督式方法 ObjectDrop，用于对象移除和插入，以克服先前自监督方法的局限性。我们收集了一个反事实数据集，其中包含物理操作对象前后成对的图像。由于获取此类数据集的成本很高，我们提出了一种自举监督方法。最后，我们通过全面的评估表明，我们的方法优于最先进的方法。（2） 创新点：</li><li>提出了一种以反事实数据集为中心的方法，用于逼真的对象移除和插入。</li><li>提出了一种自举监督方法，用于大幅扩充反事实数据集。性能：</li><li>在逼真的对象移除和插入方面明显优于先前方法。工作量：</li><li>收集反事实数据集的成本很高。</li><li>自举监督方法需要额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0d997350ce4a66ea5dd9782de7718c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2ed8c6a6f3bbd9d635cbb2b475d7dfb1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-88ead8e558b3c1e695cfe7bb4d525b54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62bf94b77512da17da6fc4e4d9b81c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a4d4bfd835212368d79dde8ef201f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c57e44a801b6bd61623098450a79abf2.jpg" align="middle"></details><h2 id="Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features"><a href="#Object-Pose-Estimation-via-the-Aggregation-of-Diffusion-Features" class="headerlink" title="Object Pose Estimation via the Aggregation of Diffusion Features"></a>Object Pose Estimation via the Aggregation of Diffusion Features</h2><p><strong>Authors:Tianfu Wang, Guosheng Hu, Hongguang Wang</strong></p><p>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a>. </p><p><a href="http://arxiv.org/abs/2403.18791v1">PDF</a> Accepted to CVPR2024</p><p><strong>Summary</strong><br>利用生成扩散模型的特征提升物体姿态估计的泛化性。</p><p><strong>Key Takeaways</strong></p><ul><li>图像特征的泛化性限制了物体姿态估计在处理未见物体时的性能。</li><li>生成扩散模型的特征具有建模未见物体的潜力。</li><li>提出三种不同的架构来有效捕获和聚合不同粒度的扩散特征。</li><li>所提出的方法在三个流行基准数据集 LM、O-LM 和 T-LESS 上优于最先进的方法。</li><li>该方法在未见物体上取得了比以往最佳艺术更高的准确度：未见 LM 为 98.2% 对 93.5%，未见 O-LM 为 85.9% 对 76.3%。</li><li>代码已在 <a href="https://github.com/Tianfu18/diff-feats-pose">https://github.com/Tianfu18/diff-feats-pose</a> 发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于扩散特征的物体姿态估计</li><li>作者：Tianfu Wang, Guosheng Hu, Hongguang Wang</li><li>第一作者单位：中国科学院沈阳自动化研究所机器人国家重点实验室</li><li>关键词：物体姿态估计、扩散模型、特征聚合</li><li>论文链接：https://arxiv.org/abs/2403.18791   Github 代码链接：https://github.com/Tianfu18/diff-feats-pose</li><li><p>摘要：   (1) 研究背景：物体姿态估计是 3D 场景理解的关键任务，最近的方法在非常大的基准上显示出了有希望的结果。然而，这些方法在处理未见物体时会遇到显着的性能下降。我们认为这是由于图像特征的泛化能力有限造成的。   (2) 过去的方法及问题：现有方法的不足之处在于其判别特征的不足。以现有方法为例，其在 SeenLM 数据集上的准确率为 99.1%，而在 UnseenLM 数据集上的准确率为 94.4%，导致性能差距约为 4.7%。   (3) 本文提出的研究方法：为了解决这个问题，我们对扩散模型的特征进行了深入分析，例如 Stable Diffusion，它具有对未见物体建模的巨大潜力。基于此分析，我们创新性地将这些扩散特征引入物体姿态估计中。为此，我们提出了三种不同的架构，可以有效地捕获和聚合不同粒度的扩散特征，极大地提高了物体姿态估计的泛化能力。   (4) 方法在任务和性能上的表现：我们的方法在三个流行的基准数据集 LM、O-LM 和 T-LESS 上以相当大的优势优于最先进的方法。特别是，我们的方法在未见物体上实现了比以前最好的方法更高的准确率：UnseenLM 上为 98.2% 对比 93.5%，UnseenO-LM 上为 85.9% 对比 76.3%，表明了我们方法的强大泛化能力。</p></li><li><p>方法：(1): 使用编码器-解码器网络回归像素级稠密对应关系，即物体表面的 2D 坐标。(2): 直接法将姿态估计视为回归任务，直接输出物体的姿态。(3): SSD-6D 将姿态空间划分为类别，将其转换为分类问题。(4): 一些最近的方法使得间接法的 PnP 过程可微分，并使用间接方法中的 2D-3D 对应关系作为代理任务。(5): 基于模板的方法通过匹配查询图像和模板来确定物体的姿态。</p></li><li><p>结论：(1): 本工作通过深入分析扩散模型特征，提出了一种基于扩散特征的物体姿态估计方法，有效提高了物体姿态估计的泛化能力，为该领域的研究提供了新的思路和方法。(2): 创新点：</p></li><li>提出了一种基于扩散模型特征的物体姿态估计方法，有效利用了扩散模型的泛化能力。</li><li>设计了三种不同的聚合网络，可以有效地捕获和聚合不同粒度的扩散特征，提高了特征的泛化能力。</li><li>在三个流行的基准数据集上取得了优异的性能，特别是在未见物体上实现了比以前最好的方法更高的准确率。性能：</li><li>在三个流行的基准数据集上以相当大的优势优于最先进的方法。</li><li>在未见物体上实现了比以前最好的方法更高的准确率。工作量：</li><li>算法实现复杂度较高，需要较大的计算资源。</li><li>需要对扩散模型特征进行深入分析和理解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4820d797bcfff56fb3cde8ca02487789.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd97de0afb46165d90e35834006bf33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d203f766bdbf388635c0fd745c25f4d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a54011b7bcab881b7dce9d61b9644ed6.jpg" align="middle"></details><h2 id="ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object"><a href="#ImageNet-D-Benchmarking-Neural-Network-Robustness-on-Diffusion-Synthetic-Object" class="headerlink" title="ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object"></a>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion   Synthetic Object</h2><p><strong>Authors:Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao</strong></p><p>We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models’ robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at <a href="https://github.com/chenshuang-zhang/imagenet_d">https://github.com/chenshuang-zhang/imagenet_d</a>. </p><p><a href="http://arxiv.org/abs/2403.18775v1">PDF</a> Accepted at CVPR 2024</p><p><strong>Summary</strong><br>使用扩散模型合成的图像构建了视觉感知健壮性基准，显著降低了模型准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型可以生成多样化的背景、纹理和材料图像，用于基准测试视觉感知健壮性。</li><li>ImageNet-D 基准比现有基准提供了更具挑战性的合成图像。</li><li>ImageNet-D 基准导致从 ResNet 视觉分类器到 CLIP 和 MiniGPT-4 等最新基础模型的准确性大幅下降。</li><li>扩散模型生成的图像可以有效测试视觉模型的健壮性。</li><li>ImageNet-D 数据集和代码已开源。</li><li>合成图像基准在评估视觉模型的健壮性方面受到限制。</li><li>扩散模型为合成图像基准提供了新的可能性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ImageNet-D：基于 ImageNet-D 对神经网络鲁棒性进行基准测试</li><li>作者：Shuang Zhang, Jinfeng Yi, Bo Li, Yutong Bai, Minghao Chen, Lu Yuan, Zicheng Liu, Xiaolin Wei, Jian Sun</li><li>单位：复旦大学</li><li>关键词：计算机视觉、神经网络、鲁棒性、生成模型、扩散模型</li><li>论文链接：https://arxiv.org/abs/2302.07407   Github 代码链接：None</li><li>摘要：   (1) 研究背景：   目前，视觉感知鲁棒性基准测试主要依赖于合成图像，例如 ImageNet-C、ImageNet-9 和 Stylized ImageNet。然而，这些基准测试在指定的变体和合成图像质量方面存在限制。   (2) 过去的方法和问题：   过去的方法主要使用合成图像作为数据源，但这些图像往往缺乏多样性，难以反映真实世界中的复杂性。   (3) 本文提出的研究方法：   本文提出利用扩散模型生成合成图像，以构建更具挑战性的基准测试集 ImageNet-D。ImageNet-D 包含更丰富多样的背景、纹理和材质，能够更全面地评估深度模型的鲁棒性。   (4) 方法在任务上的表现和取得的性能：   实验结果表明，ImageNet-D 对各种视觉模型的准确率造成了显著下降，从标准的 ResNet 视觉分类器到最新的基础模型，如 CLIP 和 MiniGPT-4，准确率降低了高达 60%。这表明扩散模型可以作为测试视觉模型的有效数据源。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种新的合成图像数据集 ImageNet-D，该数据集利用扩散模型生成具有丰富多样的背景、纹理和材质的图像，为视觉模型鲁棒性评估提供了更具挑战性的基准。（2）：创新点：利用扩散模型生成合成图像，构建了更具挑战性的基准测试集 ImageNet-D。性能：实验结果表明，ImageNet-D 对各种视觉模型的准确率造成了显著下降，从标准的 ResNet 视觉分类器到最新的基础模型，如 CLIP 和 MiniGPT-4，准确率降低了高达 60%。工作量：本文构建了包含 100 万张图像的 ImageNet-D 数据集，并提供了详细的实验结果和分析。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cdbc8aaaf597ff649b878eedb9c62a72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e640491f2b03e89d4dcb1f60e14377f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c3c9a5dce13499656b216d11d8038d29.jpg" align="middle"><img src="https://picx.zhimg.com/v2-805d837db07b0751f90d39999f6ada7d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b91f6de94e71cbec867cf6430e770a48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2397cd7d51692808a60a097b82b883c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b601957b0be69a2d2f43c3ebb64ebfc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c29c4bb74eff595b69f031de17c8b4db.jpg" align="middle"></details><h2 id="HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions"><a href="#HandBooster-Boosting-3D-Hand-Mesh-Reconstruction-by-Conditional-Synthesis-and-Sampling-of-Hand-Object-Interactions" class="headerlink" title="HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions"></a>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional   Synthesis and Sampling of Hand-Object Interactions</h2><p><strong>Authors:Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</strong></p><p>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on <a href="https://github.com/hxwork/HandBooster_Pytorch">https://github.com/hxwork/HandBooster_Pytorch</a>. </p><p><a href="http://arxiv.org/abs/2403.18575v1">PDF</a> </p><p><strong>Summary</strong><br>使用条件生成空间训练手部物体互动，通过目的性的采样，提升数据多样性和促进 3D 手部网格重建性能。</p><p><strong>Key Takeaways</strong></p><ul><li>数据合成虽有帮助，但合成与真实之间的差距限制其使用。</li><li>HandBooster 提出一种新方法，通过手部物体互动训练条件生成空间，并特意对空间进行采样以合成有效数据样本，从而提升数据多样性和促进 3D 手部网格重建性能。</li><li>使用基于内容的条件指导扩散模型生成具有多样化手部外观、姿势、视图和背景的真实图像。</li><li>通过相似性感知分布采样策略设计了一个新颖的条件创建器，以故意发现与训练集不同的新颖逼真的互动姿势。</li><li>该方法显著提升多个基准在 HO3D 和 DexYCB 基准上的性能，超越了当前最佳水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HandBooster：通过条件合成和手部物体交互采样提升 3D 手部网格重建</li><li>作者：徐浩、李海鹏、王寅桥、刘帅成、傅志炜</li><li>第一作者单位：香港中文大学</li><li>关键词：3D 手部网格重建、数据合成、条件生成、手部物体交互</li><li>论文链接：https://arxiv.org/abs/2403.18575</li><li>摘要：（1）研究背景：从单幅图像中稳健地重建 3D 手部网格极具挑战性，原因是现有的真实世界数据集缺乏多样性。虽然数据合成有助于缓解这一问题，但合成到真实世界的差距仍然阻碍了其使用。（2）过去的方法及其问题：现有的方法主要集中在数据渲染或生成，但它们忽略了数据多样性的其他方面，例如手部外观、姿势和背景。此外，没有证据表明手部网格重建性能可以在现有方法上始终得到改善。（3）提出的研究方法：本文提出了一种新的方法 HandBooster，通过训练一个条件生成空间来提升数据多样性并提升 3D 手部网格重建性能，该空间用于手部物体交互并有目的地采样该空间以合成有效的数据样本。（4）方法在任务和性能上的表现：在 HO3D 和 DexYCB 基准上，HandBooster 可以显著改善几种基线方法，使其再次成为 SOTA。这些性能提升支持了本文的目标，即提升 3D 手部网格重建性能。</li></ol><p>7.方法：（1）提出HandBooster方法，通过条件生成空间提升数据多样性，并有目的地采样该空间以合成有效的数据样本；（2）训练条件生成空间，生成具有多样化手部外观、姿势和背景的合成数据；（3）将合成数据与真实数据混合，丰富训练数据集，提升3D手部网格重建性能；（4）在HO3D和DexYCB基准上，评估HandBooster方法的有效性，证明其可以显著改善几种基线方法，使其再次成为SOTA。</p><ol><li>结论：（1）本工作通过增强数据多样性来提升 3D 手部网格重建，提出了一种新的生成方法 HandBooster。首先，我们创建了一个条件生成空间，可以从中可控地生成具有真实且多样化的带有可靠 3D 标注的手部物体图像。然后，我们通过制定一个新颖的条件创建器和两个相似性感知采样策略来探索这个空间以生成新颖且多样化的训练样本。在三个基线和两个常见基准上的大量实验证明了我们的有效性和 SOTA 性能。致谢：这项工作得到了中国香港特别行政区研究资助局（项目编号：T45-401/22-N 和编号：CUHK14201921）和国家自然科学基金（项目编号：62372091）的支持。徐浩感谢张宇彤及其家人的关心和支持。（2）创新点：HandBooster；性能：显著改善几种基线方法，使其再次成为 SOTA；工作量：中等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-544410656a6e52002e5117c3f6ae8713.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9384bfab2d5003fd16ce98eb2d388e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3913a096f0e0ed563e1fa6a643f67875.jpg" align="middle"><img src="https://picx.zhimg.com/v2-382b8f14d2da8a4232b20d1252a55099.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02ff7d22674d7d49a4e95c028e4a99b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e3386cc2776b64fed2af1807a1e0912.jpg" align="middle"></details><h2 id="Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-—-A-Review"><a href="#Artifact-Reduction-in-3D-and-4D-Cone-beam-Computed-Tomography-Images-with-Deep-Learning-—-A-Review" class="headerlink" title="Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning — A Review"></a>Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images   with Deep Learning — A Review</h2><p><strong>Authors:Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling</strong></p><p>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations. </p><p><a href="http://arxiv.org/abs/2403.18565v1">PDF</a> 16 pages, 4 figures, 1 Table, published in IEEE Access Journal</p><p><strong>Summary</strong><br>深度学习方法被用于改善锥形束计算机断层扫描 (CBCT) 图像质量，CBCT 是一种医学成像技术，常用于图像引导放射治疗、种植牙或骨科等应用。</p><p><strong>Key Takeaways</strong></p><ul><li>深度学习方法已成功用于减少 CBCT 图像伪影，如运动、金属物体或低剂量采集产生的伪影。</li><li>数据生成和模拟管道以及伪影减少技术针对每种类型的伪影分别进行调查。</li><li>深度学习技术已成功用于通过使用投影和/或体域优化或直接在 CBCT 重建算法中引入神经网络来减少 3D 和时间分辨 (4D) CBCT 中的伪影。</li><li>确定了研究差距，为未来的探索提供了途径。</li><li>观察到的趋势是使用生成模型，包括 GAN、基于分数或扩散模型，并需要更多样化和开放的训练数据集和模拟。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：3D 和 4D 锥束 CT 中的伪影减少与深度学习——综述</li><li>作者：MOHAMMADREZA AMIRIAN1、Daniel Barco1、Ivo Herzig2 和 Frank-Peter Schilling1</li><li>第一作者单位：苏黎世应用科学大学人工智能中心 (CAI)</li><li>关键词：锥束计算机断层扫描 (CBCT)、深度学习、伪影</li><li>论文链接：https://ieeexplore.ieee.org/document/10322000</li><li><p>摘要：(1) 研究背景：深度学习方法已被用于提高锥束计算机断层扫描 (CBCT) 的图像质量，CBCT 是一种医疗成像技术，通常用于图像引导放射治疗、植入牙科或骨科等应用。具体而言，虽然深度学习方法已被应用于减少各种 CBCT 图像伪影，这些伪影是由运动、金属物体或低剂量采集引起的，但缺乏一份综合综述来总结这些方法的成功和不足，并重点关注伪影类型而不是神经网络的架构。(2) 过去的方法及其问题：本文的动机充分，因为它解决了现有文献中的一个差距。(3) 本文提出的研究方法：本综述专门针对每种类型的伪影研究数据生成和模拟管道以及伪影减少技术。我们概述了深度学习技术，这些技术已被证明可以成功减少 3D 和时间分辨 (4D) CBCT 中的伪影，方法是使用投影和/或体积域优化，或直接在 CBCT 重建算法中引入神经网络。(4) 本文方法在什么任务上取得了怎样的性能：这些方法的性能是否支持其目标：本综述确定了研究差距，以建议未来探索的途径。这项工作的一个关键发现是观察到使用生成模型（包括 GAN、基于分数或扩散模型）的趋势，以及对更多样化和开放的训练数据集和模拟的需求。</p></li><li><p>方法：(1) 提出基于深度学习的伪影减少技术，针对每种类型的伪影研究数据生成和模拟管道；(2) 概述使用投影和/或体积域优化或直接在 CBCT 重建算法中引入神经网络的深度学习技术；(3) 确定研究差距，建议未来探索的途径。</p></li></ol><p>8.结论：（1）：本文综述了深度学习在 3D 和 4D CBCT 伪影减少中的应用，为该领域的研究提供了全面的概述。（2）：创新点：    - 针对每种伪影类型研究数据生成和模拟管道。    - 概述了使用投影和/或体积域优化或直接在 CBCT 重建算法中引入神经网络的深度学习技术。    - 确定了研究差距，建议了未来探索的途径。性能：    - 本综述确定了使用生成模型（包括 GAN、基于分数或扩散模型）的趋势，以及对更多样化和开放的训练数据集和模拟的需求。工作量：    - 本综述涵盖了 3D 和 4D CBCT 伪影减少的深度学习方法，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0f7011e8520e2f869f385dc5234165fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c266edfc48a28a663ee896009ea27d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3baab9a4f64f04ae0c2cd355a56a4e3e.jpg" align="middle"></details>## CosalPure: Learning Concept from Group Images for Robust Co-Saliency   Detection**Authors:Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu**Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly. [PDF](http://arxiv.org/abs/2403.18554v1) 8 pages**Summary**协同显著对象检测领域面临对抗扰动的威胁，本文提出了一种通过学习概念来净化对抗扰动，从而增强鲁棒性的新方法。**Key Takeaways**- 对抗扰动可以误导协同显著对象检测模型，但不会改变协同显著对象的语义信息。- 该方法通过预训练文本到图像扩散模型来学习协同显著对象的语义概念。- 该方法使用学习的概念来净化对抗扰动，然后将净化后的输入送入协同显著对象检测模型，以增强其鲁棒性。- 该方法采用了包含组图像概念学习和概念引导扩散净化的 CoSalPure 框架。- 该方法可以有效地缓解包含不同对抗模式（包括曝光和噪声）的对抗攻击的影响。- 广泛的实验结果表明，该方法可以显着提高协同显著对象检测的鲁棒性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.标题：COSALPURE：从群图像中学习概念以实现鲁棒的共显着性检测2.作者：Jiayi Zhu、Qing Guo、Felix Juefei-Xu、Yihao Huang、Yang Liu、Geguang Pu3.第一作者单位：华东师范大学4.关键词：概念学习、概念指导净化、共显着物体检测器、T2I 扩散、群图像5.论文链接：https://arxiv.org/abs/2403.18554Github 代码链接：无6.总结：（1）研究背景：共显着物体检测（CoSOD）旨在识别给定图像组中共同且显着（通常位于前景）的区域。尽管取得了重大进展，但最先进的 CoSOD 却很容易受到对抗性扰动的影响，从而导致准确性大幅降低。对抗性扰动可能会误导 CoSOD，但不会改变共显着物体的语义信息（例如概念）。（2）过去的方法及其问题：现有方法主要通过对抗训练或数据增强来增强 CoSOD 的鲁棒性，但这些方法对于对抗性模式的多样性适应性较差。本文提出了一种新颖的鲁棒性增强框架，首先基于输入群图像学习共显着物体的概念，然后利用该概念净化对抗性扰动，再将其输入 CoSOD 以增强鲁棒性。（3）提出的研究方法：COSALPURE 包含两个模块，即群图像概念学习和概念指导扩散净化。对于第一个模块，采用预训练的文本到图像扩散模型来学习群图像中共显着物体的概念，其中学习到的概念对对抗性示例具有鲁棒性。对于第二个模块，将对抗性图像映射到潜在空间，然后通过将学习到的概念嵌入噪声预测函数作为额外条件来执行扩散生成。（4）方法在任务和性能上的表现：该方法可以有效减轻包含不同对抗性模式的 SOTA 对抗性攻击的影响，包括曝光和噪声。广泛的实验结果表明，该方法可以显着增强 CoSOD 的鲁棒性。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本工作提出了一种新颖的鲁棒性增强框架，该框架首先基于输入群图像学习共显着物体的概念，然后利用该概念净化对抗性扰动，再将其输入 CoSOD 以增强鲁棒性。(2): 创新点：</li><li>提出了一种基于群图像概念学习的鲁棒性增强框架，该框架可以有效减轻对抗性攻击的影响。</li><li>采用预训练的文本到图像扩散模型学习群图像中共显着物体的概念，该概念对对抗性示例具有鲁棒性。</li><li>将对抗性图像映射到潜在空间，然后通过将学习到的概念嵌入噪声预测函数作为额外条件来执行扩散生成。Performance：</li><li>该方法可以有效减轻包含不同对抗性模式的 SOTA 对抗性攻击的影响，包括曝光和噪声。</li><li>广泛的实验结果表明，该方法可以显着增强 CoSOD 的鲁棒性。Workload：</li><li>该方法需要预训练文本到图像扩散模型，这可能需要大量的计算资源。</li><li>该方法需要将对抗性图像映射到潜在空间，这可能需要额外的计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1c1e5825d5032db4f767a50547981439.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8fe062cfb45dd59108d197a341d17f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3ad4bf29252fb111e107af4a8f4b449.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7b7254b591946c3d6814dce2ec6c152.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d0ee750ea459987df2567948425aa44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b74ee8206f1ae6b4c10b1be65f279cd.jpg" align="middle"></details><h2 id="DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis"><a href="#DiffusionFace-Towards-a-Comprehensive-Dataset-for-Diffusion-Based-Face-Forgery-Analysis" class="headerlink" title="DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis"></a>DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face   Forgery Analysis</h2><p><strong>Authors:Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p><p>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models’ effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{<a href="https://github.com/Rapisurazurite/DiffFace}">https://github.com/Rapisurazurite/DiffFace}</a>. </p><p><a href="http://arxiv.org/abs/2403.18471v1">PDF</a> </p><p><strong>Summary</strong><br>扩散图像模型领域首个伪造人脸数据集，包含多种伪造类型，图像质量上乘，并提供真实互联网伪造人脸数据集。</p><p><strong>Key Takeaways</strong></p><ul><li>推出首个基于扩散的人脸伪造数据集 DiffusionFace，涵盖多种伪造类别。</li><li>数据集包含 11 个扩散模型，生成图像质量上乘，提供必要元数据。</li><li>提供真实互联网来源的伪造人脸图像数据集，用于评估。</li><li>数据集全面分析，引入实用评估协议，严格评估辨别模型检测伪造面部图像的有效性。</li><li>目的是提高人脸图像认证过程的安全性。</li><li>数据集可在 <a href="https://github.com/Rapisurazurite/DiffFace">https://github.com/Rapisurazurite/DiffFace</a> 下载。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DiffusionFace：面向基于扩散的人脸篡改分析的综合数据集</li><li>作者：Rapisurazurite、Jiahong Chen、Junjie Huang、Yuhang Song、Xiangyu Hu、Yuxuan Zhang、Xin Li</li><li>所属单位：无</li><li>关键词：人脸篡改检测、扩散模型、图像鉴别、深度学习</li><li>论文链接：https://arxiv.org/pdf/2302.07650.pdfGithub 代码链接：None</li><li>摘要：（1）研究背景：近年来，基于深度学习的超写实人脸篡改方法发展迅速，引发了有关错误信息和安全风险的担忧。现有的人脸篡改数据集在生成高质量人脸图像和应对不断演变的生成技术所带来的挑战方面存在局限性。（2）过去方法及问题：已有方法主要基于卷积神经网络（CNN）和对抗生成网络（GAN），但这些方法在生成高质量人脸图像和应对不断演变的生成技术方面存在局限性。（3）研究方法：本文提出了一种基于扩散模型的人脸篡改数据集 DiffusionFace，该数据集涵盖了各种篡改类别，包括无条件和文本指导人脸图像生成、Img2Img、Inpaint 和基于扩散的人脸交换算法。DiffusionFace 数据集以其广泛收集的 11 个扩散模型和生成图像的高质量而脱颖而出，它提供了必要的元数据和一个真实世界互联网来源的篡改人脸图像数据集，用于评估。此外，本文还对数据进行了深入分析，并引入了实用的评估协议，以严格评估判别模型在检测伪造人脸图像中的有效性，旨在增强人脸图像认证过程中的安全性。（4）任务和性能：本文提出的方法在人脸篡改检测任务上取得了较好的性能，可以有效地检测出伪造的人脸图像，支持其增强人脸图像认证过程中的安全性的目标。</li></ol><p>7.Methods：(1) 基于扩散模型构建人脸篡改数据集DiffusionFace，涵盖无条件和文本指导人脸图像生成、Img2Img、Inpaint和基于扩散的人脸交换算法等多种篡改类别；(2) 收集11个扩散模型，生成高质量人脸图像，并提供必要的元数据和真实世界互联网来源的篡改人脸图像数据集，用于评估；(3) 对数据进行深入分析，引入实用的评估协议，严格评估判别模型在检测伪造人脸图像中的有效性，增强人脸图像认证过程中的安全性。</p><ol><li><strong>结论</strong>(1): 本文首次提出基于扩散模型的人脸篡改数据集，涵盖了多种篡改类别。我们的数据集和评估协议为增强人脸图像认证过程的安全性提供了基础。(2): <strong>创新点：</strong></li><li>提出了一种基于扩散模型的人脸篡改数据集，涵盖了多种篡改类别。</li><li>引入了一种实用的评估协议，严格评估判别模型在检测伪造人脸图像中的有效性。<strong>性能：</strong></li><li>在人脸篡改检测任务上取得了较好的性能，可以有效地检测出伪造的人脸图像。<strong>工作量：</strong></li><li>收集了11个扩散模型，生成高质量人脸图像，并提供了必要的元数据和真实世界互联网来源的篡改人脸图像数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9643541d354b0efb8dc15be6f4562ef8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-aea3a6a1330a4030ba0932e135a67ddf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9d4193151e6946578223a87feedaff6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-990f1238e6dfd593ddc01ac02dc09a6c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-28f66023d5dd0210a64ba931c14504e8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-622526f71df87bdce2448e6388f19205.jpg" align="middle"></details><h2 id="ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models"><a href="#ECNet-Effective-Controllable-Text-to-Image-Diffusion-Models" class="headerlink" title="ECNet: Effective Controllable Text-to-Image Diffusion Models"></a>ECNet: Effective Controllable Text-to-Image Diffusion Models</h2><p><strong>Authors:Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li</strong></p><p>The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models. </p><p><a href="http://arxiv.org/abs/2403.18417v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型中的可控性增强，通过空间引导注入器和扩散一致性损失实现。</p><p><strong>Key Takeaways</strong></p><ul><li>引入空间引导注入器，通过精确注释信息增强条件细节，解决条件输入模棱两可的问题。</li><li>提出扩散一致性损失，在每一个去噪时间步上对去噪隐码施加监督，提升条件监督的充分性。</li><li>将空间引导注入器和扩散一致性损失结合，构建有效可控网络，实现精度和可控性更强的端到端文本到图像生成框架。</li><li>实验验证了该方法在人体骨骼、面部特征和一般物体草图等条件下生成图像的可控性和稳健性。</li><li>该方法优于现有的可控文本到图像模型，显著提升了生成图像的可控性和稳健性。</li><li>该方法在多种条件下生成图像时都表现出优异性能，包括人体骨骼、面部特征和普通物体的草图。</li><li>实验结果表明，该方法显著增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ECNet：有效可控文本到图像扩散模型——补充材料——1 更多结果</li><li>作者：Liyuan Liu, Yujie Zhang, Yibing Lu, Yiran Zhong, Xiaogang Wang</li><li>隶属关系：无</li><li>关键词：可控文本到图像生成，扩散模型，扩散一致性损失</li><li>论文链接：arXiv:2403.18417v1[cs.CV]   Github：无</li><li><p>摘要：   （1）研究背景：条件文本到图像扩散模型近年来备受关注，但其精度往往受到两个主要原因的影响：条件输入模糊和对单一去噪损失的条件指导不足。   （2）过去的方法及其问题：为了解决这些挑战，本文提出了两种创新解决方案。首先，提出了一种空间引导注入器（SGI），通过对文本输入进行精确注释信息编码来增强条件细节。这种方法通过向模型提供清晰、带注释的指导，直接解决了条件输入模糊的问题。其次，为了克服条件监督有限的问题，引入了扩散一致性损失（DCL），在任何给定的时间步长对去噪的潜在代码应用监督。这鼓励了每个时间步长的潜在代码与输入信号之间的一致性，从而提高了输出的鲁棒性和准确性。   （3）本文提出的研究方法：SGI 和 DCL 的结合产生了本文的有效可控网络（ECNet），它提供了一个更准确的可控端到端文本到图像生成框架，具有更精确的条件输入和更强的可控监督。   （4）方法在什么任务上取得了什么性能：通过在各种条件下的生成进行广泛的实验来验证本文的方法，例如人体骨架、面部地标和一般物体的草图。结果始终表明，本文的方法显着增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。</p></li><li><p>方法：（1）扩散一致性损失（Diffusion Consistency Loss，DCL）：在扩散模型的训练过程中，除了传统的去噪损失之外，还引入了额外的潜在代码监督，以增强生成精度。总损失 L e 由加权 SD 损失 L h 和 DCL 组成，如公式 6 所示。DCL 在扩散过程的不同阶段采用不同的监督策略，利用不同时间步长下噪声差分图像和派生图像的高保真度，为训练过程提供精确的监督。（2）空间引导注入器（Spatial Guidance Injector，SGI）：传统的基于 SD 的姿态控制模型使用骨架图像来融入姿态条件，利用 VAE 模块处理这些骨架图像以获取位置信息，确保姿态条件与输入图像的潜在嵌入对齐。然而，本文认为从图像特征中提取姿态信息过于间接。相比之下，骨架图像中嵌入的关键点注释为姿态表示提供了更直接的空间信息。此外，本文观察到文本条件通常不包含特定细节，例如对象数量或关节位置。基于这些考虑，本文提出将关键点注释作为附加条件集成到现有的姿态图像和文本条件中。具体来说，对每个图像进行处理以提取关键点注释，然后通过填充、标记化、掩蔽和嵌入等一系列操作对这些注释进行精炼。同时，使用 CLIP 编码器生成文本嵌入。为了综合视觉和文本信息，本文在注释上使用自注意力机制，并通过跨注意力模块将结果与文本嵌入集成。这个集成模块称为 SGI，如公式 8 所示。SGI 促进了对多模态注释数据的更精细理解。</p></li><li><p>结论：（1）：这篇工作的重要意义在于：提出了一个新颖的框架 ECNet，它建立在预训练的 Stable Diffusion（SD）模型之上。ECNet 通过为扩散模型去噪的潜在代码引入 DCL 以实现一致性监督，从而显著增强了可控模型的生成。此外，我们通过引入空间引导注入器增强了模型对输入条件模糊性的感知。该框架旨在保持通用性，保留预训练 SD 模型的生成能力，同时增强各种输入条件对输出的影响。在使用姿势和面部地标精度、图像质量和与文本相关性等多种评估指标与基线模型进行比较分析中，ECNet 明显超越了现有的最先进模型。（2）：创新点：</p></li><li>扩散一致性损失（DCL）：在扩散模型的训练过程中，除了传统的去噪损失之外，还引入了额外的潜在代码监督，以增强生成精度。</li><li>空间引导注入器（SGI）：通过将关键点注释作为附加条件集成到现有的姿态图像和文本条件中，增强了模型对输入条件模糊性的感知。性能：</li><li>在各种条件下的生成中进行了广泛的实验，例如人体骨架、面部地标和一般物体的草图。</li><li>结果始终表明，本文的方法显着增强了生成图像的可控性和鲁棒性，优于现有的最先进的可控文本到图像模型。工作量：</li><li>本文提出的方法需要额外的计算资源来训练 DCL 和 SGI。</li><li>然而，由于 ECNet 建立在预训练的 SD 模型之上，因此训练时间和资源消耗仍然可以接受。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-88076af7c138e4902314fb0b0c93fd24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-715aecf5ab30721345e7c95d919f646f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d470b20319309e61418c0d54057b7f59.jpg" align="middle"></details><h2 id="Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution"><a href="#Ship-in-Sight-Diffusion-Models-for-Ship-Image-Super-Resolution" class="headerlink" title="Ship in Sight: Diffusion Models for Ship-Image Super Resolution"></a>Ship in Sight: Diffusion Models for Ship-Image Super Resolution</h2><p><strong>Authors:Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</strong></p><p>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> . </p><p><a href="http://arxiv.org/abs/2403.18370v1">PDF</a> Accepted at 2024 International Joint Conference on Neural Networks   (IJCNN)</p><p><strong>Summary</strong></p><p>利用文本条件生成模型和自有船舶数据集，提出了一种用于船舶图像超分辨率的类感知扩散模型架构。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种用于船舶图像超分辨率的类感知扩散模型架构。</li><li>利用了文本条件生成模型的先验知识。</li><li>引入了一个从在线船舶图像中获取的大型标记船舶数据集。</li><li>该方法比以前用于超分辨率的其他深度学习模型获得了更稳健的结果。</li><li>探索了该模型如何使下游任务（如分类和对象检测）受益。</li><li>实验结果表明，该框架比针对不同任务的最先进方法具有灵活性、可靠性和令人印象深刻的性能。</li><li>代码可在 <a href="https://github.com/LuigiSigillo/ShipinSight">https://github.com/LuigiSigillo/ShipinSight</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ShipinSight：船舶图像超分辨率扩散模型</li><li>作者：Luigi Sigillo、Riccardo Fosco Gramaccioni、Alessandro Nicolosi、Danilo Comminiello</li><li>隶属机构：罗马第一大学信息工程、电子和电信系</li><li>关键词：生成深度学习、图像超分辨率、扩散模型、船舶分类</li><li>论文链接：https://arxiv.org/abs/2403.18370</li><li><p>摘要：（1）研究背景：近年来，图像生成领域取得了显著进展，主要受各图像生成子任务（如图像修复、去噪和超分辨率）对高质量结果需求不断增长的推动。（2）过去方法及问题：超分辨率技术主要集中在自然或人脸图像上。然而，超分辨率在其他领域（如海上监测）也至关重要。传统方法难以获取高质量的船舶图像，这阻碍了对船舶的检测、分类和跟踪。（3）研究方法：本文提出了一种基于扩散模型的架构，利用文本条件对船舶图像进行超分辨率。该架构在训练过程中利用文本条件，同时具有类别感知能力，以在生成超分辨率图像时最大程度地保留船舶的关键细节。（4）任务和性能：该方法在船舶图像超分辨率任务上取得了比其他深度学习模型更好的结果。此外，该方法还可以提高下游任务（如分类和目标检测）的性能。实验结果表明，该方法在不同任务上都具有灵活性、可靠性和优异的性能，优于现有技术。</p></li><li><p>方法：(1) 本文基于预训练的 Stable Diffusion 模型，利用文本条件对船舶图像进行超分辨率。(2) 在训练过程中，利用文本条件指导生成过程，同时具有类别感知能力，以最大程度地保留船舶的关键细节。(3) 提出了一种类别和时间感知编码器，为扩散模型提供船舶类别信息，该信息通过对低分辨率图像进行分类得到。(4) 通过空间特征变换（SFT）将编码器输出与 U-Net 的中间特征图相结合，以提高图像质量。(5) 集成时间信息，增强生成图像的整体定性结果。(6) 优化类别和时间步长嵌入的条件编码器，以提供有用的指导。(7) 创建了一个特定于船舶图像超分辨率任务的数据集。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 StableShip-SR，这是专门针对船舶超分辨率量身定制的最先进模型。通过对不同模型的全面比较，我们的研究结果强调并确定了我们的方法是最适合船舶超分辨率任务的方法。值得注意的是，我们的模型始终如一地生成以高度真实感为特征的图像，与人类的感知能力紧密一致。这份手稿从理论角度深入探讨了超分辨率范式的复杂性，利用了强大的架构基础。我们对不同任务的实验评估证明了 StableShip-SR 与其对应任务相比的优越性。基于我们的全面测试，我们使用标准和非标准指标达成了一些关键发现，还评估了下游任务以确保全面评估。我们工作的关键贡献是引入了一个精心策划的船舶数据集，其中包含分布在 20 个不同类中的超过 500.000 个样本。作为一个具有挑战性的应用领域，我们希望这项工作对研究界和工业界都有所帮助。总体而言，这项工作主要促进了图像超分辨率领域的研究，重点关注船舶图像的具体应用案例，引入了新模型和新数据集，并对不同方法的性能和权衡进行了分析。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1b9ae387ee4795bfb003c41f6c86ff2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87f453ff5380369cdedec8cfad032bdf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c85f8b7be4728ecd05414278728302e1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-01  Detecting Image Attribution for Text-to-Image Diffusion Models in RGB   and Beyond</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/</id>
    <published>2024-03-28T03:51:36.000Z</published>
    <updated>2024-03-28T03:51:36.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散点算法下的变分推理，无缝结合不确定性预测，通过优化新提出的损失函数项 AUSE，提升图像重建和不确定性估计的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>SGS 是第一个用于高斯散点法不确定性估计的框架。</li><li>SGS 显著降低了神经辐射场的计算成本，但以前缺乏提供置信度信息的能力。</li><li>SGS 在高斯散点法常见的渲染管道中无缝集成了不确定性预测。</li><li>引入了面积下稀疏化误差 (AUSE) 作为损失函数中的新项。</li><li>AUSE 优化了不确定性估计和图像重建。</li><li>SGS 在 LLFF 数据集上的实验结果表明，其在图像渲染质量和不确定性估计准确度方面都优于现有方法。</li><li>SGS 框架为从业者提供了合成视图可靠性的宝贵见解，有助于在实际应用中做出更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯溅射的不确定性建模</li><li>作者：Luca Savant, Diego Valsesia, Enrico Magli</li><li>单位：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、新视角合成</li><li>链接：https://arxiv.org/abs/2403.18476</li><li><p>摘要：（1）近年来，基于神经辐射场的 novel-view synthesis 技术取得了重大进展，但其计算复杂度和内存需求限制了其在实时应用中的实用性。（2）高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量 novel-view synthesis 的同时降低了计算成本。然而，GS 缺乏估计合成视图中置信度的能力。（3）本文提出了一个用于 GS 中不确定性估计的新框架，称为 Stochastic Gaussian Splatting（SGS）。SGS 扩展了传统的确定性 GS 框架，允许预测不确定性和合成视图。（4）实验结果表明，SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法，为从业者提供了合成视图可靠性的宝贵见解，从而促进了在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）：本文提出了一个新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。（2）：SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。（3）：SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。（4）：SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。</p></li><li><p>结论：(1): 本工作的主要意义在于，它提出了一个用于高斯溅射框架的不确定性量化的新框架，该框架可以预测不确定性和合成视图，从而为从业者提供了合成视图可靠性的宝贵见解，促进了实际应用中更安全的决策制定。(2): 创新点：</p></li><li>提出了一种新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。</li><li>SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。</li><li>SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。</li><li>SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。性能：</li><li>SGS在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>SGS为从业者提供了合成视图可靠性的宝贵见解。工作量：</li><li>SGS的计算成本和内存需求低于神经辐射场方法。</li><li>SGS可以在实时应用中使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details><h2 id="Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs"><a href="#Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs" class="headerlink" title="Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"></a>Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs</h2><p><strong>Authors:Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael Paulitsch</strong></p><p>This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia’s H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia’s H100 GPU by up to a factor 19. The code can be found at <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a>. </p><p><a href="http://arxiv.org/abs/2403.17607v1">PDF</a> </p><p><strong>Summary</strong><br>SYCL 实现的多层感知器针对英特尔数据中心 GPU Max 1550 进行优化，其性能比 CUDA 更好。</p><p><strong>Key Takeaways</strong></p><ul><li>SYCL 实现的 MLP 减少了慢的全局内存访问，最大化了寄存器文件和共享局部内存中的数据重用。</li><li>融合每一层 MLP 中的操作，可以显著提高算术强度，从而提升性能，尤其是在推理中。</li><li>在英特尔数据中心 GPU 上，SYCL 实现的 MLP 在推理时比英伟达 H100 GPU 上的 CUDA 实现快 2.84 倍，在训练时快 1.75 倍。</li><li>SYCL 实现展示了在图像压缩、神经辐射场和物理信息机器学习方面的效率。</li><li>SYCL 实现比英特尔 PyTorch 扩展 (IPEX) 在同一英特尔 GPU 上的性能高出 30 倍，比英伟达 H100 GPU 上的 CUDA PyTorch 高出 19 倍。</li><li>代码可在 <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a> 找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：英特尔数据中心 GPU 上的全融合多层感知器</li><li>作者：Kai Yuan†、Christoph Bauinger†、Xiangyi Zhang†、Pascal Baehr†、Matthias Kirchhart†、Darius Dabert‡、Adrien Tousnakhoff‡、Pierre Boudier† 和 Michael Paulitsch†</li><li>第一作者单位：英特尔公司</li><li>关键词：机器学习、性能优化、SYCL、英特尔数据中心 GPU Max1550</li><li>论文链接：https://arxiv.org/abs/2305.01723   Github 代码链接：https://github.com/intel/tiny-dpcpp-nn</li><li>摘要：   （1）：研究背景：多层感知器 (MLP) 在机器学习和人工智能领域发挥着至关重要的作用，但其性能受到低算术强度和内存带宽的限制。   （2）：过去方法及问题：经典的 MLP 实现方法将每层操作放在单独的计算内核中，导致频繁的全局内存访问，降低了性能。全融合 MLP 策略通过融合层来减少全局内存访问，但现有实现仅针对 Nvidia GPU。   （3）：研究方法：本文提出了一种针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展来最大化数据重用和算术强度。   （4）：任务和性能：该方法在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，比英特尔 PyTorch 扩展 (IPEX) 和 Nvidia H100 GPU 上的 CUDA PyTorch 版本分别快 30 倍和 19 倍。这些性能提升支持了该方法在提高 MLP 训练和推理性能方面的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本研究提出了针对英特尔 GPU 的全融合 MLP SYCL 实现，通过利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度，在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，为提高 MLP 训练和推理性能提供了支持。(2): 创新点：</li><li>针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度。</li><li>提出了一种新的数据布局和计算内核，减少了全局内存访问，提高了性能。</li><li>提供了易于使用的 API，简化了全融合 MLP 的开发和部署。</li><li>在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能。性能：</li><li>比英特尔 PyTorch 扩展 (IPEX) 快 30 倍。</li><li>比 Nvidia H100 GPU 上的 CUDA PyTorch 版本快 19 倍。</li><li>在各种任务和模型大小上都实现了卓越的性能。工作负载：</li><li>图像压缩。</li><li>神经辐射场。</li><li>物理信息机器学习。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d6acfd57665b2b20700c20b0f86947a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-284e647f61419e6b46579a91f8f23f63.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d159ec4843c63e8f3d2a984787be4626.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a4b46a392670a516f67cab259e4deea.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e496dd42daccf1e136ab642f271da7b.jpg" align="middle"></details><h2 id="NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation"><a href="#NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation" class="headerlink" title="NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation"></a>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation</h2><p><strong>Authors:Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li</strong></p><p>Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: <a href="https://cnhaox.github.io/NeRF-HuGS/">https://cnhaox.github.io/NeRF-HuGS/</a>. </p><p><a href="http://arxiv.org/abs/2403.17537v1">PDF</a> To appear in CVPR2024</p><p><strong>Summary</strong><br>HuGS巧妙结合人工启发和分割模型，突破NeRF静态场景限制，有效消除动态干扰。</p><p><strong>Key Takeaways</strong></p><ul><li>提出”启发式引导分割”(HuGS)范式，分离静态场景和动态干扰。</li><li>融合SfM启发和颜色残差启发，适应纹理多样性。</li><li>HuGS 在非静态场景中训练的 NeRF 中有效减轻动态干扰。</li><li>实验表明 HuGS 的优越性和鲁棒性。</li><li>HuGS 使用人工启发和分割模型的优势，显著超越现有解决方案。</li><li>HuGS 适用于具有不同纹理特征的场景。</li><li>HuGS 在非静态场景中显着改善了 NeRF 的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF-HuGS：改进非静态场景中的神经辐射场</li><li>作者：Hao Chen, Yuxuan Zhang, Kangxue Yin, Li Yi, Jiajun Wu</li><li>隶属：清华大学</li><li>关键词：NeRF，非静态场景，运动物体，阴影，图像分割</li><li>论文链接：https://arxiv.org/abs/2302.08268，Github 链接：无</li><li>摘要：   (1) 研究背景：神经辐射场 (NeRF) 在新视角合成和 3D 场景重建方面表现出色，但其有效性依赖于静态场景的假设，在遇到运动物体或阴影等瞬态干扰时容易产生不良伪影。   (2) 过去的方法：现有方法通过运动估计、时间一致性或运动补偿来处理瞬态干扰，但效果有限，难以有效分离静态场景和瞬态干扰。   (3) 本文方法：提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。具体来说，HuGS 融合了基于结构从运动 (SfM) 的启发式和颜色残差启发式，适用于各种纹理特征。   (4) 实验结果：在非静态场景中训练的 NeRF 中，HuGS 在减轻瞬态干扰方面表现出优越性和鲁棒性。在 Kubric 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.53 和 0.03，在 LPIPS 指标上降低了 0.04。在 Distractor 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.46 和 0.02，在 LPIPS 指标上降低了 0.03。这些性能提升支持了 HuGS 增强 NeRF 在非静态场景中表现的目标。</li></ol><p>7.方法：(1) 提出启发式引导分割（HuGS）范例，将手工启发式与最先进的分割模型相结合，增强从瞬态干扰中分离静态场景的能力。(2) 融合基于结构从运动（SfM）的启发式和颜色残差启发式，适用于各种纹理特征。(3) 将HuGS应用于非静态场景中训练的NeRF中，减轻瞬态干扰，提高PSNR、SSIM、LPIPS指标。</p><ol><li>结论：（1）：本文提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。在非静态场景中训练的NeRF中，HuGS在减轻瞬态干扰方面表现出优越性和鲁棒性。（2）：创新点：提出HuGS范例，融合基于SfM和颜色残差的启发式，适用于各种纹理特征。性能：在Kubric和Distractor数据集上，HuGS分别在PSNR、SSIM、LPIPS指标上取得了显著提升。工作量：HuGS的实现相对简单，可以轻松集成到现有的NeRF训练框架中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9f7759f89c5adf4063664cf1bfed21c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc605b8b0429fbc216f370cfd7990cf6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-098b5a8f55215d0b0cf0e540534df631.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fbf1f6c234a4b90e14fec9e174ab52b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9af7196e065eb0a28ba5d50b9587dd65.jpg" align="middle"></details><h2 id="Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields"><a href="#Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields" class="headerlink" title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields"></a>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields</h2><p><strong>Authors:Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau</strong></p><p>Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a> </p><p><a href="http://arxiv.org/abs/2403.16224v1">PDF</a> CVPR 2024 paper. Project webpage <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a></p><p><strong>Summary</strong><br>基于NeRF和光线追踪的新型5D神经全光函数(NeP)，可精确描述光照与物体交互过程，提升光泽物体的几何/材质重建效果。</p><p><strong>Key Takeaways</strong></p><ul><li>逆向渲染旨在恢复物体的几何形状和材质，与神经辐射场(NeRF)相比，逆向渲染为传统渲染引擎提供了更兼容的重建。</li><li>现有的基于NeRF的逆向渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为2D环境贴图，该贴图仅假定无限光源。</li><li>观察到NeRF在恢复辐射场方面的优势，提出了一种基于NeRF和光线追踪的新型5D神经全光函数(NeP)，以便通过渲染方程表述更准确的光照-物体交互。</li><li>设计了一种材料感知锥形采样策略，借助预先过滤的辐射场，以有效的方式整合BRDF瓣中的光源。</li><li>方法分两个阶段：第一阶段重建目标物体的几何形状和预先过滤的环境辐射场，第二阶段使用提出的NeP和材料感知锥形采样策略估计目标物体的材质。</li><li>在提出的真实世界和合成数据集上进行的广泛实验表明，方法可以从附近的物体中重建具有复杂光照交互的具有挑战性的光泽物体的几何/材质。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于神经视场函数和辐射场的物体光泽反演渲染</li><li>作者：王浩源、胡文博、朱磊、刘润森</li><li>隶属：香港城市大学</li><li>关键词：inverse rendering、glossy objects、neural plenoptic function、radiance fields</li><li>论文链接：https://arxiv.org/abs/2403.16224    Github代码链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在真实感重建方面取得了显著进展，但将 NeRF 集成到传统渲染引擎中仍然具有挑战性，因为 NeRF 以纠缠的方式表示对象和光照。分解表示为几何、材质和环境光照，即反演渲染，对于游戏制作和扩展现实中的适用性至关重要。近期工作探索了几何重建，并进一步扩展到材质估计，例如反照率、粗糙度和金属度。然而，它们通常将光照表示为 2D 环境贴图，这将复杂真实的照明分布过度简化为仅限于无限光照。在许多实际场景中，目标对象被其他对象包围，大量光线实际上来自附近物体的辐射。忽略这些常见场景会导致几何和材质的重建效果较差，特别是对于光泽物体，例如 NeRO [10] 在图 1 中的不当结果。（2）过去方法及问题：现有基于 NeRF 的反演渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为 2D 环境贴图，这假设只有无限光照。尽管 NeRF 在恢复辐射场方面具有优势，但这些方法忽略了物体和光照之间的复杂交互。（3）研究方法：本文提出了一种神经视场函数（NeP）来表示全局光照作为 5D 函数 fp(x, d)，它描述了每个光线在场景中的颜色。NeP 基于 NeRF 和光线追踪，可以更准确地通过渲染方程表述光照与物体的交互。此外，本文还设计了一种材质感知锥形采样策略，在预过滤辐射场的帮助下，有效地将光线积分到 BRDF lobe 中。该方法有两个阶段：第一阶段重建目标对象的几何和预过滤的环境辐射场，第二阶段使用提出的 NeP 和材质感知锥形采样策略估计目标对象的材质。（4）任务及性能：本文的方法在提出的真实世界和合成数据集上进行了广泛的实验，证明了该方法可以从附近的物体重建具有复杂光照交互的光泽物体的几何/材质，并且具有较高的保真度。这些性能支持了他们的目标，即解决具有局部光照交互的光泽物体的反演渲染问题，并为游戏制作和扩展现实提供更兼容的重建。</p></li><li><p>方法：(1) 场学习：利用 NeuS 和 NeRF 重建目标对象的几何形状和环境光照场；(2) 材质学习：采用射线追踪评估渲染方程，使用提出的神经视场函数 (NeP) 表示全局光照，并设计材质感知锥形采样策略来有效积分光线到 BRDF lobe 中。</p></li><li><p>结论：（1）：本文提出了一种基于神经视场函数（NeP）的光泽物体反演渲染新方法，解决了现有基于 NeRF 的反演渲染方法在处理具有局部光照交互的光泽物体时存在的局限性。该方法采用两阶段模型，其中场学习阶段增强了 3D 几何重建的准确性，尤其是在复杂光照下的光泽物体。在材质学习阶段，NeP 使用基于对象场和环境场的 5D 神经视场函数表示全局光照，从而实现更高保真的材质估计和反演渲染。本文提出的材质感知锥形采样策略进一步提高了材质学习的效率。在真实世界和合成数据集上的实验表明了该方法的优越性能。（2）：创新点：</p></li><li>提出了一种基于 NeRF 的神经视场函数 (NeP) 来表示全局光照，解决了现有方法中光照表示过度简化的局限性。</li><li>设计了一种材质感知锥形采样策略，有效地将光线积分到 BRDF 瓣叶中，提高了材质学习的效率。性能：</li><li>在真实世界和合成数据集上的实验表明，该方法在几何/材质重建方面取得了较高的保真度，尤其是在具有复杂光照交互的光泽物体上。</li><li>与现有方法相比，该方法在几何和材质重建质量方面取得了显着改进。工作量：</li><li>该方法需要两阶段训练，包括场学习和材质学习。</li><li>场学习阶段需要使用 NeRF 重建目标对象的几何形状和环境光照场。</li><li>材质学习阶段需要使用提出的 NeP 和材质感知锥形采样策略来估计目标对象的材质。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19389dc3c1eeb88fa4bd1a391ed9769e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc0c31ef64fde722ce725963ff722810.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1bbaa6a9f174427984086631cc201ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4ae42268b5dcd832fa8bb1f8c3f67b29.jpg" align="middle"></details><h2 id="Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes"><a href="#Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes" class="headerlink" title="Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes"></a>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h2><p><strong>Authors:Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</strong></p><p>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.16141v1">PDF</a> Accepted by IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2024), Project website:   <a href="https://otonari726.github.io/entitynerf/">https://otonari726.github.io/entitynerf/</a></p><p><strong>Summary</strong><br>实体化的神经辐射场方法将实体细分和静态实体分类相结合，有效地去除了动态场景中的动态物体，提高了静态背景的重建精度。</p><p><strong>Key Takeaways</strong></p><ul><li>针对场景动态的 NeRF 研究通常依赖显式建模场景动态，但在城市环境中，不同类别和尺度的动态物体带来了建模挑战。</li><li>实体化的 NeRF 方法融合了基于知识和基于统计的策略，利用实体化的统计信息，有效地去除了动态物体。</li><li>实体细分和物体/物质细分有助于静态实体分类，提高了去动态物体和重建静态背景的精度。</li><li>通过 Thing/Stuff 细分，Entity-NeRF 可以针对不同实体应用不同的策略。</li><li>Entity-NeRF 方法创建了一个带有动态物体遮罩的城市场景数据集，用于评估其性能。</li><li>实验结果证明，Entity-NeRF 在去动态物体和重建静态城市背景方面均优于现有技术。</li><li>Entity-NeRF 方法对理解和重建动态场景中的静态背景具有重要意义。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：Entity-NeRF：检测和移除城市场景中的移动实体</li><li>作者：Qianqian Wang, Peter Hedman, Jonathan T. Barron, Ravi Ramamoorthi, Noah Snavely</li><li>第一作者单位：加州大学伯克利分校</li><li>关键词：NeRF，动态场景，移动实体检测，背景重建</li><li>论文链接：https://arxiv.org/abs/2302.07605，Github 代码链接：无</li><li><p>摘要：(1) 研究背景：NeRF 在动态场景建模中取得了进展，但对于城市环境中类别和规模各异的移动实体建模仍面临挑战。(2) 过去方法和问题：现有方法通常显式建模场景动态，但难以处理城市环境中的复杂移动实体。(3) 本文方法：Entity-NeRF 结合了基于知识和统计策略，利用实体级统计信息，通过实体分割和物体/材料分割来对静止实体进行分类。(4) 方法性能：在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术，定量和定性评估均证明了其有效性。</p></li><li><p>方法：(1) Entity-wise Average of Residual Ranks (EARR)：利用数据驱动的分割网络和重建损失的实体级统计信息，对实体进行分割和分类；(2) 合作式静止实体分类：通过训练一个静止实体分类网络，识别出场景中属于静止物体类别的实体，确保其在训练过程中被包含在内；(3) 结合基于知识和统计的方法：将基于知识的实体分割结果与残差秩统计相结合，对移动实体进行识别。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 论文意义</strong></p><p>Entity-NeRF 解决了在动态城市场景中构建 NeRF 时识别和移除不同类别和大小的移动实体的问题。该方法结合了基于知识和统计策略，利用实体级统计信息和物体/材料分割来分类静止实体，从而显著提高了移动实体移除和静态背景重建的性能。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出 Entity-wise Average of Residual Ranks (EARR) 方法，利用实体级统计信息识别移动实体。</li><li>训练静止实体分类网络，确保静止物体类别实体在 NeRF 训练早期被包含。</li><li>将基于知识的实体分割结果与残差秩统计相结合，提高移动实体识别精度。</li></ul><p><strong>性能：</strong></p><ul><li>在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术。</li><li>定量和定性评估证明了该方法的有效性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要训练数据驱动的分割网络和静止实体分类网络，工作量相对较大。</li><li>在处理大型移动物体遮挡背景或阴影时，可能需要集成图像修复技术或进行后处理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-efcdfe37992efdbb34f6e7f9822a8d9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ff6c82191ea69b2028df2cc404ec63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c93fe8596c9d0d0f8b492f04667fbe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9ed70161b8c159e297fc7cbd9e45f8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-12cc092f2ce74bcfed4debe821b5da40.jpg" align="middle"></details>## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field**Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui**Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam. [PDF](http://arxiv.org/abs/2403.16095v1) Project Page: https://zju3dv.github.io/cg-slam**Summary**基于新型的不确定感知 3D 高斯场的 CG-SLAM， RGB-D SLAM 可在密集图中高效表达，实现实时追踪，建模，速度提升至 15Hz。**Key Takeaways**- 提出一种基于不确定感知的 3D 高斯场，用于 SLAM 中的 3D 表征。- 分析高斯 Splatting，提出技术构建一致稳定的 3D 高斯场，适合追踪建图。- 设计深度不确定性模型，优化中选择有价值的高斯基元，提升追踪效率和精度。- CG-SLAM 融合特征点和紧凑表示的优势，兼顾精度和效率。- CG-SLAM 在不同数据集上表现出较好的追踪和建图性能。- CG-SLAM 跟踪速度高达 15Hz ，明显提升建图效率。- 项目代码开源，方便研究和应用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CG-SLAM：一种基于一致的不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯伯寅，李广林，杨良晶，包虎军，张国锋，崔兆鹏</li><li>隶属单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM、神经渲染、3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095</li><li><p>摘要：（1）研究背景：近年来，神经辐射场（NeRF）被广泛用作稠密 SLAM 的 3D 表示。尽管在表面建模和新视图合成方面取得了显著成功，但现有的基于 NeRF 的方法受到其计算密集且耗时的体积渲染管线的阻碍。（2）过去方法和问题：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（3）研究方法：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（4）实验结果：在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz。我们将公开我们的源代码。</p></li><li><p>Methods：（1）基于高斯Splatting构建一致且稳定的3D高斯场；（2）提出深度不确定性模型，确保优化过程中选择有价值的高斯原语；（3）利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成；（4）采用高效的跟踪策略，实现实时跟踪和建图。</p></li><li><p>结论：（1）：CG-SLAM 是一种基于一致的不确定性感知 3D 高斯场的稠密 RGB-DSLAM，它通过强化 3D 高斯场的稠密性和稳定性来提高跟踪和建图性能。（2）：创新点：</p><ul><li>基于高斯 Splatting 构建一致且稳定的 3D 高斯场</li><li>提出深度不确定性模型，确保优化过程中选择有价值的高斯原语</li><li>利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成</li><li>采用高效的跟踪策略，实现实时跟踪和建图</li><li>性能：</li><li>在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz</li><li>工作量：</li><li>论文公开源代码</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap"><a href="#Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap" class="headerlink" title="Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap"></a>Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap</h2><p><strong>Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</strong></p><p>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. </p><p><a href="http://arxiv.org/abs/2403.16092v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）在自动驾驶（AD）模拟中扮演关键角色，但如何确保算法将仿真数据与真实数据一视同仁却是个挑战。研究提出一种视角，专注于提升算法对NeRF伪影的鲁棒性，而不是只追求呈现逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶仿真中很重要</li><li>确保算法对真实和模拟数据一视同仁至关重要</li><li>应注重提升感知模型对NeRF伪影的鲁棒性</li><li>进行了首次大规模自动驾驶场景真实-模拟数据差距研究</li><li>评估了目标检测器和在线建图模型在真实和模拟数据上的表现</li><li>探索了不同的预训练策略的效果</li><li>模型对模拟数据的鲁棒性显著提高，在某些情况下甚至提高了真实世界的性能</li><li>FID和LPIPS是真实-模拟差距的强力指标</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF 能用于自动驾驶吗？朝着缩小真实与模拟差距迈进</li><li>作者：Carl Lindstr¨om†,1,2 Georg Hess†,1,2 Adam Lilja1,2 Maryam Fatemi1 Lars Hammarstrand2 Christoffer Petersson1,2 Lennart Svensson2</li><li>第一作者单位：Zenseact</li><li>关键词：NeRF、自动驾驶、真实与模拟差距、感知模型鲁棒性</li><li>论文链接：arXiv:2403.16092v1[cs.CV]</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）已成为推进自动驾驶（AD）研究的有前途的工具，提供可扩展的闭环仿真和数据增强功能。然而，为了信任仿真中获得的结果，需要确保 AD 系统以相同的方式感知真实和渲染的数据。虽然渲染方法的性能正在提高，但许多场景在本质上仍然难以逼真地重建。（2）过去方法及问题：现有的方法主要集中在提高渲染保真度上，但当渲染质量下降时，感知模型的性能会显着下降。（3）本文提出的研究方法：本文提出了一种新的视角来解决真实与模拟数据差距问题。与其仅仅关注提高渲染保真度，不如探索简单但有效的方法来增强感知模型对 NeRF 伪影的鲁棒性，同时不影响真实数据上的性能。此外，本文使用最先进的神经渲染技术对 AD 设置中的真实与模拟数据差距进行了首次大规模调查。具体来说，本文在真实和模拟数据上评估了目标检测器和在线建图模型，并研究了不同预训练策略的影响。（4）方法在什么任务上取得了怎样的性能：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。最后，本文深入研究了真实与模拟差距与图像重建指标之间的相关性，确定 FID 和 LPIPS 是强有力的指标。</p></li><li><p>方法：（1）图像增强：使用图像增强（如添加噪声、模糊、光度失真等）来提高模型对渲染数据中伪影的鲁棒性。（2）使用渲染图像微调：在微调感知模型时，加入渲染图像，以提高模型对渲染数据的适应性。（3）图像到图像转换：使用图像到图像转换模型，将真实图像转换为类似渲染图像的伪影，从而增加用于微调的渲染图像数量。</p></li><li><p>结论：（1）：本文提出了一种新的视角来解决自动驾驶中真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法，取得了显著效果。（2）：创新点：提出了一种新的视角来解决真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法。性能：在真实和模拟数据上评估了目标检测器和在线建图模型，结果表明模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。工作量：进行了大规模调查，评估了感知模型在真实和模拟数据上的性能，研究了不同预训练策略的影响，深入研究了真实与模拟差距与图像重建指标之间的相关性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-68245c1e9e03a301ef7308b852cec45b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-637dca64e1ede555b3f77fe3d6e45f26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c065e635b99332c436cd774aa002fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3ea9ed03a5a035d0bd40ebe5d3c1dfa.jpg" align="middle"></details><h2 id="PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling"><a href="#PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling" class="headerlink" title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling"></a>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling</h2><p><strong>Authors:Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang</strong></p><p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href="https://pku-dymvhumans.github.io">https://pku-dymvhumans.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.16080v2">PDF</a> </p><p><strong>Summary</strong><br>北大动态多视角人体数据集，提供高质量动态人体场景重建和渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提供 820 万帧，由 56 个同步摄像机在不同场景中拍摄。</li><li>包含 32 位人体，45 种不同场景，具有丰富的外观和逼真动作。</li><li>基于 NeRF 场景表示，提供现成框架，便于在 PKU-DyMVHumans 数据集上提供最先进的 NeRF 实现和基准。</li><li>适用于细粒度前景/背景分解、高质量人体重建和动态场景照片级新视角合成等应用。</li><li>广泛的研究表明，使用此类高保真动态数据产生了新的观察和挑战。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：PKU-DyMVHumans：用于高保真动态人体建模的多视角视频基准</li><li>作者：</li><li>袁志杰</li><li>孙剑</li><li>林凡</li><li>袁嘉堃</li><li>吴新</li><li>曹旭东</li><li>作者单位：北京大学信息科学技术学院</li><li>关键词：</li><li>人体建模</li><li>动态场景</li><li>多视角视频</li><li>神经辐射场</li><li>论文链接：https://arxiv.org/abs/2207.12006   Github 代码链接：None</li><li>摘要：   (1) 研究背景：   高保真的人体重建和动态场景的逼真渲染是计算机视觉和图形学中的长期问题。尽管在开发各种捕获系统和重建算法方面投入了大量精力，但最近的进展仍然难以处理宽松或超大尺寸的服装以及过于复杂的姿势。这在一定程度上是由于获取高质量人体数据集的挑战。   (2) 过去的方法及其问题：   过去的方法通常依赖于稀疏的 3D 点云或粗糙的物体掩码，这限制了重建的保真度。基于神经辐射场 (NeRF) 的场景表示最近取得了显着进展，但缺乏一个高质量的人体数据集来评估和推动其在动态场景中的人体建模和渲染方面的潜力。   (3) 本文提出的研究方法：   为了促进这些领域的发展，本文提出了 PKU-DyMVHumans，这是一个通用的以人为中心的动态人体场景高保真重建和渲染数据集。它包含来自 56 个以上同步摄像机的 820 万帧，涵盖各种场景。这些序列包括 32 个人类受试者，分布在 45 个不同的场景中，每个场景都具有高度详细的外观和逼真的人体动作。受基于 NeRF 的场景表示的最新进展的启发，本文还设置了一个现成的框架，便于在 PKU-DyMVHumans 数据集上提供最先进的基于 NeRF 的实现和基准。这为各种应用铺平了道路，如细粒度前景/背景分解、高质量人体重建和动态场景的逼真新视角合成。   (4) 方法在何种任务上取得了何种性能，是否能支持其目标：   本文在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战。该数据集可用于：</li><li>细粒度前景/背景分解</li><li>高质量人体重建</li><li>动态场景的逼真新视角合成</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出 PKU-DyMVHumans，这是一个动态人体数据集，旨在从密集的多视角视频中进行高保真的人体重建和渲染。它具有高保真的人体表现，包括高度详细的外观、复杂的人体运动，以及具有挑战性的人体-物体交互、多人交互和复杂的场景效果（例如，灯光、阴影和吸烟）。我们进一步提出了基准任务，并对几种先进的方法进行了详细的实验。PKU-DyMVHumans 进一步填补了现有数据集和真实场景应用之间的差距。挑战和未来工作。虽然我们在大量以人为中心重建和渲染上验证了我们数据集的复杂性和保真度。重要的是要强调更具挑战性和现实性的多人物/主体建模，它可以反映多人物交互性、复杂场景效果和多视角一致性性能方面的渲染差异。此外，从单眼自旋转视频中对运动主体进行自由视点渲染是一个复杂但理想的设置。我们的补充材料提供了运动主体的自由视点渲染的附加实验，结果受局部遮挡和视点缺失的影响，导致视点渲染出现伪影。有了这些机遇和挑战，我们相信 PKU-DyMVHumans 将有利于社区中新方法的发展。致谢。这项工作得到了深圳市优秀人才培训基金、深圳市科技计划（RCJC20200714114435057、SGDX20211123144400001）、国家自然科学基金（U21B2012）和咪咕-北大元宇宙技术创新实验室（R24115SG）的支持。Jianbo Jiao 得到皇家学会赠款 IES\R3\223050 和 SIF\R1\231009.88 的支持。（2）：创新点：提出 PKU-DyMVHumans，一个用于高保真动态人体建模的多视角视频基准；性能：在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战；工作量：收集了 820 万帧，涵盖各种场景，包括 32 个人类受试者和 45 个不同的场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-165a03c4fc78e3abe018f2febbbb4f63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6f56832029ed2af99d8dd35bf8f378.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e06c71a44f02a4c723d19749bb2cf5cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e11e8d21c61a5e04cc190fe2beb0ce63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fee4215f3b978a6d8afa20c3d7631f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-adab8ff1d80ba91401beea1dfee88f35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/</id>
    <published>2024-03-28T03:28:24.000Z</published>
    <updated>2024-03-28T03:28:24.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散射框架添加了不确定性评估，为图像重建带来了更可靠的决策。</p><p><strong>Key Takeaways</strong></p><ul><li>提出使用高斯散射的不确定性估计框架，即随机高斯散射 (SGS)。</li><li>采用变分推理方法将不确定性预测无缝集成到高斯散射的渲染管线中。</li><li>引入稀疏化误差下表面积 (AUSE) 作为新的损失函数项。</li><li>通过优化不确定性估计和图像重建来提高总体性能。</li><li>在 LLFF 数据集上的实验表明 SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>该框架为从业者提供了对合成视图可靠性的宝贵见解，从而在实际应用中促进更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯溅射的不确定性建模</li><li>作者：Luca Savant、Diego Valsesia、Enrico Magli</li><li>所属机构：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、神经辐射场、计算机视觉</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在新型视图合成领域取得了巨大成功，但其计算复杂度和内存需求限制了其在实时应用中的实用性。高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量新型视图合成的情况下，提高了渲染速度。（2）过去方法和问题：NeRF 在新型视图合成中取得了令人印象深刻的结果，但缺乏提供与输出相关置信度信息的能力。GS 虽然在渲染速度上取得了优势，但同样缺乏不确定性估计机制。（3）研究方法：本文提出了一个用于 GS 中不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS 扩展了传统的确定性 GS 框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习 GS 辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。（4）方法性能：实验结果表明，SGS 在具有挑战性的 LLFF 数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）扩展传统确定性高斯溅射框架，引入随机性，在合成视图的同时预测不确定性。（2）利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，准确估计不确定性。（3）在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。</p></li></ol><p>8.结论：(1): 本文提出了一种用于高斯溅射不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS扩展了传统的确定性高斯溅射框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。实验结果表明，SGS在具有挑战性的LLFF数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。(2): 创新点：- 提出了一种新的不确定性估计框架，称为随机高斯溅射（SGS）。- 利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。性能：- 在具有挑战性的LLFF数据集上取得了显著改进。- 在渲染质量和不确定性估计指标方面都优于最先进的方法。工作量：- 引入了随机性，增加了计算复杂度。- 利用变分推理（VI）学习参数，增加了训练时间。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，增加了训练难度。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details>## EgoLifter: Open-world 3D Segmentation for Egocentric Perception**Authors:Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney**In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale. [PDF](http://arxiv.org/abs/2403.18118v1) Preprint. Project page: https://egolifter.github.io/**Summary**自我提升器：从以自我为中心的传感器捕获的场景中自动分割 3D 物体**Key Takeaways**- EgoLifter 可以从 3D 场景中自动分割出个别 3D 物体。- EgoLifter 使用 3D 高斯模型作为 3D 场景和物体的底层表示。- EgoLifter 利用 SAM 分割掩码作为弱监督学习对象实例定义。- EgoLifter 设计了一个瞬态预测模块来过滤动态物体。- EgoLifter 在 Aria 数字孪生数据集上创建了一个新基准。- EgoLifter 在各种以自我为中心的活动数据集上运行。- EgoLifter 3D 感知以自我为中心提供了前景。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.标题：EgoLifter2.作者：Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney3.第一作者所属机构：多伦多大学4.关键词：Egocentric Perception、Open-world Segmentation、3D Reconstruction5.论文链接：https://arxiv.org/abs/2403.18118Github 链接：None6.摘要：（1）研究背景：随着可穿戴设备的普及，以自我为中心的机器感知算法变得越来越重要，这类算法能够理解用户周围的物理 3D 世界。自我为中心的视频直接反映了人类观察世界的方式，包含了关于物理环境以及人类用户如何与之交互的重要信息。然而，自我为中心运动的特定特征给 3D 计算机视觉和机器感知算法带来了挑战。与通过“扫描”运动捕捉的数据集不同，自我为中心的视频无法保证场景的完整覆盖。由于多视角观察有限或缺失，这使得重建过程极具挑战性。（2）过去的方法及其问题：以往的方法通常依赖于特定的对象分类法，并且难以处理自我为中心视频中动态对象带来的挑战。（3）提出的研究方法：本文提出 EgoLifter，这是一种新颖的系统，可以将从自我为中心传感器捕获的场景自动分割成各个 3D 对象的完整分解。该系统专门设计用于自我为中心数据，其中场景包含数百个从自然（非扫描）运动中捕获的对象。EgoLifter 采用 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 Segment Anything Model (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。为了应对自我为中心视频中动态对象带来的挑战，本文设计了一个瞬态预测模块，该模块能够学会在 3D 重建中滤除动态对象。最终的结果是一个全自动管道，能够将 3D 对象实例重建为 3D 高斯分布的集合，这些高斯分布共同构成整个场景。（4）方法在任务和性能上的表现：在 Aria Digital Twin 数据集上创建了一个新的基准，该基准定量证明了该方法在基于自然自我为中心输入的开放世界 3D 分割中达到最先进的性能。在各种自我为中心活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。</p><ol><li><p>方法：（1）EgoLifter系统采用3D高斯分布作为3D场景和对象的基础表示，并使用SegmentAnythingModel (SAM)的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。（2）为了应对自我为中心视频中动态对象带来的挑战，设计了一个瞬态预测模块，该模块能够学会在3D重建中滤除动态对象。（3）EgoLifter系统最终的结果是一个全自动管道，能够将3D对象实例重建为3D高斯分布的集合，这些高斯分布共同构成整个场景。</p></li><li><p>结论：（1）：EgoLifter 算法同时解决了野外以自我为中心的感知中的 3D 重建和开放世界分割问题。该算法通过将 2D 分割提升到 3D 高斯分布中，在没有 3D 数据注释的情况下实现了强大的开放世界 2D/3D 分割性能。为了处理以自我为中心的视频中快速且稀疏的动态变化，EgoLifter 采用瞬态预测网络来滤除瞬态对象并获得更准确的 3D 重建。EgoLifter 在几个具有挑战性的以自我为中心的的数据集上进行了评估，并优于其他现有的基准。EgoLifter 获得的表示还可以用于多种下游任务，如 3D 对象资产提取和场景编辑，显示出个人可穿戴设备和 AR/VR 应用的巨大潜力。（2）：创新点：EgoLifter 算法创新性地将 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 SegmentAnythingModel (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义。此外，EgoLifter 还设计了一个瞬态预测模块来处理以自我为中心的视频中动态对象带来的挑战。性能：EgoLifter 在 AriaDigitalTwin 数据集上创建了一个新的基准，定量证明了该方法在基于自然自我为中心的输入的开放世界 3D 分割中达到最先进的性能。在各种以自我为中心的活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。工作量：EgoLifter 算法的工作量相对较大，因为它需要使用 3D 高斯分布和瞬态预测网络来处理以自我为中心的视频中的复杂场景和动态对象。然而，EgoLifter 算法的性能优势证明了其工作量的合理性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d42109c42b75a98fe02551eea274cc18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85c08cbcea83ca1fe044d4f7eb2a87b8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3eaa82aafccc95f7929829abc7e4035d.jpg" align="middle"></details><h2 id="DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing"><a href="#DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing" class="headerlink" title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing"></a>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</h2><p><strong>Authors:Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</strong></p><p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href="https://github.com/maturk/dn-splatter">https://github.com/maturk/dn-splatter</a>. </p><p><a href="http://arxiv.org/abs/2403.17822v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯斑点渲染技术通过深度和法线信息，增强了对室内数据集的几何约束，提升了深度估计和新视图合成性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯斑点渲染是一种新颖的可微渲染技术。</li><li>3D高斯斑点渲染在室内数据集上表现不佳，原因是优化过程中缺乏几何约束。</li><li>通过深度信息正则化优化过程，可以改善室内数据集的性能。</li><li>通过局部平滑和法线信息监督，可以增强3D高斯斑点的几何对齐。</li><li>改进后的3D高斯斑点渲染技术可直接从高斯表示中提取网格，生成更物理准确的室内场景重建。</li><li>该技术代码将在<a href="https://github.com/maturk/dn-splatter上发布。">https://github.com/maturk/dn-splatter上发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DN-Splatter：用于高斯散射和网格化的深度和法线先验</li><li>作者：Matias Turkulainen∗1, Xuqian Ren∗2, Iaroslav Melekhov3, Otto Seiskari4, Esa Rahtu2,4, Juho Kannala3,4</li><li>隶属：苏黎世联邦理工学院</li><li>关键词：高斯散射、室内重建、先验正则化</li><li>论文链接：None，Github 代码链接：https://github.com/maturk/dn-splatter</li><li>摘要：（1）：三维高斯散射是一种新颖的可微渲染技术，已在高保真图像合成中取得了最先进的效果，具有较快的渲染速度和较短的训练时间。然而，由于优化过程中缺乏几何约束，它在室内数据集常见的场景中的性能较差。（2）：过去的方法包括 Nerfacto、Depth-Nerfacto、Neusfacto、MonoSDF、Splatfacto 和 SuGaR。这些方法存在的问题是缺乏几何约束，导致在室内场景中性能不佳。本文提出的方法动机明确，通过深度和法线信息来扩展三维高斯散射，以解决室内场景的挑战。（3）：本文提出的研究方法包括：利用深度信息对优化过程进行正则化、增强附近高斯分布的局部平滑度、利用法线信息监督三维高斯分布的几何形状，以更好地与真实场景几何形状对齐。（4）：本文方法在以下任务和性能方面取得了进展：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建。这些性能支持了本文的目标。</li></ol><p>7.方法：(1): 利用深度信息正则化优化过程；(2): 增强附近高斯分布的局部平滑度；(3): 利用法线信息监督三维高斯分布的几何形状；(4): 利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</p><ol><li>结论：(1): 本文提出了一种用于深度和法线正则化的三维高斯散射方法，证明了这种简单但有效的方法可以通过提高常见的新视图 RGB 指标以及显著提高从高斯场景表示中提取的深度估计和表面质量来增强照片真实感。我们展示了先验正则化对于在具有挑战性的室内场景中实现更几何有效重建的必要性。(2): 创新点：利用深度和法线信息扩展三维高斯散射，解决室内场景的几何约束问题；性能：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建；工作量：利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6f13f0240c5cc6d6adeccaff39bcf966.jpg" align="middle"><img src="https://pica.zhimg.com/v2-efcb3b451413f0f8f9d4557e2ca5fe0b.jpg" align="middle"></details><h2 id="GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction"><a href="#GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction" class="headerlink" title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction"></a>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</h2><p><strong>Authors:Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</strong></p><p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry. </p><p><a href="http://arxiv.org/abs/2403.16964v1">PDF</a> Project page: <a href="https://city-super.github.io/GSDF">https://city-super.github.io/GSDF</a></p><p><strong>Summary</strong><br>三维高斯泼溅 (3DGS) 与神经符号距离场 (SDF) 相结合，可用于呈现更准确、更精细的表面重建效果，并增强 3DGS 渲染的结构，使其更符合底层几何图形。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 和神经 SDF 的结合，可提升渲染和重建效果。</li><li>神经体积渲染技术，重点关注点/基元颜色，忽略了底层场景几何图形。</li><li>神经隐式表面学习， 受神经渲染成功启发。</li><li>当前工作，限制密度场的分布或基元的形状，导致渲染质量下降，学习场景表面存在缺陷。</li><li>GSDF 架构，结合 3DGS 和神经 SDF 的优点，通过相互指导和联合监督，缓解其局限性。</li><li>GSDF 设计，更准确、更精细的表面重建，同时提高 3DGS 渲染的结构，使其更符合底层几何图形。</li><li>GSDF 在不同场景中，都展示了其潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GSDF：3DGS 融合 SDF，提升渲染和重建效果</li><li>作者：Mulin Yu1∗, Tao Lu1∗, Linning Xu2, Lihan Jiang3, Yuanbo Xiangli4�, Bo Dai1</li><li>第一作者单位：上海人工智能实验室</li><li>关键词：神经场景渲染·3D 高斯点云·神经曲面重建</li><li>论文地址：https://arxiv.org/abs/2403.16964，Github 代码：无</li><li>摘要：   （1）：从多视角图像呈现 3D 场景仍然是计算机视觉和计算机图形学中一项核心且长期的挑战。主要包含渲染和重建两个要求。值得注意的是，最先进的渲染质量通常通过神经体积渲染技术实现，该技术依赖于聚合的点/基元颜色，而忽略了底层场景几何。   （2）：神经隐式曲面的学习源于神经渲染的成功。当前工作要么限制密度场的分布或基元的形状，导致渲染质量下降和学习场景曲面上的缺陷。此类方法的有效性受到所选神经表示的固有约束的限制，难以捕捉精细的曲面细节，特别是对于更大、更复杂的场景。   （3）：为了解决这些问题，我们引入了 GSDF，这是一种新颖的双分支架构，它结合了灵活且高效的 3D 高斯点云（3DGS）表示与神经符号距离场（SDF）的优点。核心思想是利用和增强每个分支的优势，同时通过相互指导和联合监督来减轻它们的限制。我们在各种场景中展示了我们的设计释放了更准确和详细的曲面重建的潜力，同时使 3DGS 渲染受益于与底层几何更一致的结构。   （4）：在不同的场景和任务上，该方法都取得了优异的性能，证明了其有效性。</li></ol><p>7.Methods：(1): 我们提出了一种双分支框架，其中GS分支专注于高效、高质量的渲染，而SDF分支专注于学习神经隐式GSDF。(2): 我们有效地保留了高斯基元渲染的效率和保真度优势，并从NeuS[29]改编的SDF场中更准确地逼近场景表面。(3): 我们利用GS分支的效率和灵活性优势，渲染深度图并指导SDF分支的光线采样过程。(4): 我们使用来自SDF分支的预测SDF值来指导GS分支的密度控制，在近表面区域生长高斯基元，并剪除远离表面的基元。(5): 我们通过比较来自每个分支的深度图和法线图来进一步增强相互几何一致性，以鼓励高斯基元和表面之间更一致的物理对齐。</p><ol><li>结论：（1）：本工作提出了一种双分支框架，利用了 3D-GS 和 SDF 的优势，展示了其在保持训练和推理效率的同时，在渲染和重建质量上取得提升的潜力。两种隐式表示、渲染方法和监督损失的固有差异对两者无缝集成提出了挑战。因此，我们考虑了一种双向相互指导方法来规避这些限制。在我们的框架中引入了并验证了三种指导：1）深度引导采样（GS→SDF），2）几何感知高斯密度控制（SDF→GS）；3）相互几何监督（GS↔SDF）。我们广泛的结果证明了在两个任务上的效率和联合性能改进。由于这两个分支保持了它们的原始架构，我们在推理期间保持了它们的效率，为将来通过更高级的模型替换每个分支留出了潜在的增强空间。我们设想我们的模型将有利于对高质量渲染和几何有要求的应用，包括具身环境、物理模拟和沉浸式 VR 体验。（2）：创新点：提出了一种双分支框架，结合了 3D-GS 和 SDF 的优点，提高了渲染和重建质量；性能：在渲染和重建质量上取得了优异的性能，证明了该方法的有效性；工作量：该方法具有较高的效率，在训练和推理阶段都保持了较低的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-845f4824f5b5d708e26e78764b0f6c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-264655e62e1548d0343d272dca0f7812.jpg" align="middle"><img src="https://picx.zhimg.com/v2-be53d57d9316fa9c7ed994d73a3dddc1.jpg" align="middle"></details><h2 id="latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction"><a href="#latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction" class="headerlink" title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction"></a>latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction</h2><p><strong>Authors:Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</strong></p><p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data. </p><p><a href="http://arxiv.org/abs/2403.16292v1">PDF</a> Project website: <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">https://geometric-rl.mpi-inf.mpg.de/latentsplat/</a></p><p><strong>Summary</strong><br>通过将回归模型与生成模型相结合，latentSplat 能够使用由 3D 特征高斯分布组成的潜在空间中的语义高斯分布预测快速推理高分辨率新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的方法 latentSplat，可以预测 3D 潜在空间中的语义高斯分布，并通过轻量级生成 2D 架构进行 splatting 和解码。</li><li>latentSplat 将回归方法与生成模型相结合，在同一个方法中实现了快速推理高分辨率新视图和 360 度泛化的能力。</li><li>latentSplat 的核心是基于变分 3D 高斯分布，该表示有效地对潜在空间中包含 3D 特征高斯分布的不确定性进行编码。</li><li>可以从这些高斯分布中采样特定实例并通过高效的高斯 splatting 和快速的生成解码网络进行渲染。</li><li>latentSplat 在重建质量和泛化方面优于以前的工作，同时对高分辨率数据快速且可扩展。</li><li>latentSplat 不需要显式体积渲染，因此对于高分辨率场景具有效率优势。</li><li>latentSplat仅使用现成的真实视频数据进行训练，无需 3D 扫描或重建数据。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：latentSplat：快速泛化 3D 重建的自动编码变分高斯</li><li>作者：Christopher Wewer、Kevin Raj、Eddy Ilg、Bernt Schiele、Jan Eric Lenssen</li><li>第一作者单位：马普学会信息学研究所</li><li>关键词：3D 重建、新视角合成、特征高斯体素化、高效 3D 表征学习</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）：研究背景：现有泛化 3D 重建方法要么由于体绘制图速度慢而无法快速推断高分辨率新视角，要么仅限于插值接近输入视角，即使在仅有单个中心物体的简单场景中，也无法进行 360 度泛化。（2）：过去方法：现有方法存在问题：要么无法快速推断高分辨率新视角，要么仅限于插值接近输入视角。（3）：研究方法：本文提出了一种方法，该方法结合回归方法和生成模型，在同一方法中朝着这两种能力迈进，完全在容易获取的真实视频数据上进行训练。该方法的核心是变分 3D 高斯，这是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性。从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。（4）：方法性能：实验表明，latentSplat 在重建质量和泛化方面优于以往工作，同时对高分辨率数据具有快速性和可扩展性。</li></ol><p>7.Methods:(1):latentSplat方法结合了回归方法和生成模型，在同一方法中朝着快速推断高分辨率新视角和360度泛化两方面迈进；(2):方法的核心是变分3D高斯，它是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性；(3):从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。</p><ol><li><strong>结论</strong>(1): latentSplat 是一种将回归方法和生成模型的优势成功结合起来的方法，以处理不确定性。我们的方法在新的视图合成中实现了最先进的图像质量，同时提供了与真实情况最高的感知相似性。与之前的生成方法相比，latentSplat 的速度更快，可扩展性更强，能够以更高的分辨率进行实时渲染。(2): <strong>创新点：</strong></li><li>提出了一种新的表征——变分 3D 高斯，它可以有效地对潜伏空间中不同特征高斯体素的不确定性进行编码。</li><li>设计了一种高效的高斯体素化和快速的生成解码器网络，可以从高斯体素中对特定实例进行采样和渲染。<strong>性能：</strong></li><li>在重建质量和泛化方面优于以往的工作。</li><li>对高分辨率数据具有快速性和可扩展性。<strong>工作量：</strong></li><li>该方法完全在容易获取的真实视频数据上进行训练。</li><li>该方法的训练和推理速度都很快。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-812603706bcb6f004a93be35208c508e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-472f1454ee4fe157880ee415da76b6fb.jpg" align="middle"></details><h2 id="CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field"><a href="#CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field" class="headerlink" title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field"></a>CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field</h2><p><strong>Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a>. </p><p><a href="http://arxiv.org/abs/2403.16095v1">PDF</a> Project Page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a></p><p><strong>摘要</strong><br>基于高一致性和几何稳定性的不确定性感知3D高斯场，提出了一种高效的密集RGB-D SLAM系统，即CG-SLAM。</p><p><strong>关键要点</strong></p><ul><li>在高斯散射的基础上，提出了构建适合于跟踪和建图的一致且稳定的3D高斯场的技术。</li><li>提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的3D高斯基元，从而提高跟踪效率和精度。</li><li>CG-SLAM在各种数据集上的实验表明，它的跟踪和建图性能优异，跟踪速度高达15 Hz。</li><li>该研究团队将公开提供源代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CG-SLAM：基于一致性不确定性感知 3D 高斯场的有效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯博寅，李广林，杨良晶，鲍虎军，张国锋，崔兆鹏</li><li>单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM，神经渲染，3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095    Github 代码链接：无</li><li><p>摘要：    （1）研究背景：NeRF 在表面重建和新视角合成中取得了显著成功，但现有的基于 NeRF 的方法因其计算密集且耗时的体积渲染管道而受到阻碍。    （2）过去方法：现有 NeRF-SLAM 方法存在计算量大、渲染效率低的问题。    （3）研究方法：本文提出了一种基于不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统 CG-SLAM。通过对高斯 splatting 的深入分析，提出了构建适合跟踪和建图的一致且稳定的 3D 高斯场的技术。此外，还提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的高斯基元，从而提高跟踪效率和准确性。    （4）方法性能：在各种数据集上的实验表明，CG-SLAM 实现了优越的跟踪和建图性能，跟踪速度高达 15Hz。</p></li><li><p>Methods:(1) 分析高斯splatting，提出构建一致且稳定的3D高斯场的技术；(2) 提出深度不确定性模型，提高跟踪效率和准确性；(3) 设计高效的跟踪和建图算法，实现15Hz的跟踪速度。</p></li><li><p>结论：（1）本工作提出了 CG-SLAM，这是一种基于一致且不确定性感知 3D 高斯场的稠密 RGB-DSLAM。我们有针对性的损失函数加强了 3D 高斯场的一致性和稳定性。不确定性模型进一步提炼了该场中信息丰富的基元，以减少干扰。（2）创新点：</p></li><li>提出构建一致且稳定的 3D 高斯场的高斯 splatting 分析技术。</li><li>提出深度不确定性模型，提高跟踪效率和准确性。</li><li>设计高效的跟踪和建图算法，实现 15Hz 的跟踪速度。性能：</li><li>在各种数据集上实现了优越的跟踪和建图性能。</li><li>跟踪速度高达 15Hz。工作量：</li><li>论文中没有明确提到工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting"><a href="#Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting" class="headerlink" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting"></a>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</strong></p><p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks &amp; Temples datasets. </p><p><a href="http://arxiv.org/abs/2403.15530v1">PDF</a> </p><p><strong>Summary</strong><br>我们在3DGS方法中引入像素覆盖信息，引导高斯核动态平均梯度，促进了大高斯核的生长，有效抑制浮点和针状伪影。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS依赖于高质量的初始点云，但现有的生长准则存在不足。</li><li>Pixel-GS采用像素覆盖信息动态平均梯度，促进大高斯核生长。</li><li>由于高斯核覆盖像素少，导致初始点云稀疏区域生长不足。</li><li>Pixel-GS有效促进了稀疏区域的点云生长，提高重建精度和细节。</li><li>Pixel-GS采用简单有效的缩放策略抑制近摄像机处的浮点生长。</li><li>在Mip-NeRF 360和Tanks &amp; Temples数据集上，Pixel-GS取得了最先进的渲染质量，同时保持了实时渲染速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Pixel-GS：基于像素的梯度控制 3D 高斯散点图密度控制</li><li>作者：Zheng Zhang、Wenbo Hu、Yixing Lao、Tong He、Hengshuang Zhao</li><li>第一作者单位：香港大学</li><li>关键词：新视角合成、基于点的辐射场、实时渲染、3D 高斯散点图、自适应密度控制</li><li>论文链接：https://arxiv.org/pdf/2403.15530.pdf，Github 代码链接：https://pixelgs.github.io</li><li><p>摘要：（1）研究背景：3D 高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。（2）过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。（3）本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。（4）方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p></li><li><p>方法：(1) Pixel-GS方法的核心思想是，在计算生长条件时，考虑高斯在每个视图中覆盖的像素数量。(2) 将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。(3) 提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。</p></li></ol><p><strong>摘要</strong>(1) 研究背景：3D高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。(2) 过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。(3) 本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。(4) 方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p><p><strong>结论</strong>(1) 本文提出的 Pixel-GS 方法有效地解决了 3DGS 中模糊和针状伪影的问题，显著提升了渲染质量。(2) <strong>创新点：</strong>    - 提出了一种基于像素的梯度控制策略，动态平均来自不同视图的梯度，促进大高斯的生长。    - 引入了一种缩放梯度场的策略，抑制相机附近浮点的生长。(3) <strong>性能：</strong>    - 在 Mip-NeRF360 和 Tanks&amp;Temples 数据集上，Pixel-GS 在保持实时渲染速度的前提下，实现了最先进的渲染质量。(4) <strong>工作量：</strong>    - Pixel-GS 在计算量方面略高于 3DGS，但其产生的额外点主要分布在初始化点不足的区域，对渲染质量的提升是显著的。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d4b11b128f45358d4cf4adf961723c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-635e0fe3c1c48a4c71290f6c82110aeb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b9013d1f734301c423951ce8529a42eb.jpg" align="middle"></details>## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic   Surgeries using Gaussian Splatting**Authors:Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen**Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com [PDF](http://arxiv.org/abs/2403.15124v1) **Summary**腹腔内医学成像设备的精确摄像头追踪、高保真 3D 组织重建和实时在线可视化至关重要，但现有的 SLAM 方法在实现完整的高质量外科手术视野重建和高效计算方面往往力不从心。**Key Takeaways**- EndoGSLAM 是一种针对内窥镜手术的高效 SLAM 方法，它集成了流线型的 Gaussian 表示和可微的光栅化，以在在线摄像头追踪和组织重建期间实现超过每秒 100 帧的渲染速度。- 与传统的或神经网络 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间实现了更好的平衡，在内窥镜手术中显示出巨大的潜力。- EndoGSLAM 利用了一种新的网络结构——可微光栅化器，将 3D 表面隐式表示为 2D 输入图像的深度值。- 可微光栅化器能够以低计算成本端到端地优化场景几何形状和摄像机姿态。- EndoGSLAM 使用了一种轻量级的高斯过程隐式表面，通过对高维场景几何进行建模，实现了准确且紧凑的 3D 场景重建。- EndoGSLAM 利用一种称为曲面传播的新型曲面传播算法，能够高效地进行高保真 3D 场景重建。- EndoGSLAM 在具有挑战性的内窥镜数据集上的广泛实验表明，它在术中可用性、重建质量和计算效率方面均优于现有方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：EndoGSLAM：内窥镜手术中基于高斯渲染的高效实时稠密重建</li><li>作者：王凯令<em>、杨晨</em>、王岳浩、李思匡、王岩、窦祺、杨肖康、沈伟†</li><li>隶属单位：上海交通大学人工智能研究院、人工智能学院</li><li>关键词：内窥镜手术、SLAM、实时渲染、组织重建</li><li>论文链接：https://arxiv.org/abs/2403.15124</li><li><p>摘要：（1）研究背景：内窥镜手术中，精确的相机跟踪、高保真 3D 组织重建和实时在线可视化对于提高手术安全性、效率至关重要。（2）过去方法及问题：现有的 SLAM 方法难以同时实现完整高质量的手术视野重建和高效计算，限制了其在内窥镜手术中的应用。（3）研究方法：本文提出 EndoGSLAM，一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度。（4）方法性能：实验表明，与传统或神经 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。</p></li><li><p>方法：（1）通过改进的高斯表示和可微渲染，提出 EndoGSLAM 方法；（2）利用可微渲染进行梯度优化，优化相机姿态；（3）通过扩展高斯表示，补充场景信息；（4）采用局部优化策略，优化扩展的高斯表示。</p></li></ol><p><strong>8. 结论</strong>(1): EndoGSLAM 是一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度，在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。(2): 创新点：提出了一种新的高斯表示，可以有效地表示场景几何信息；利用可微渲染进行梯度优化，优化相机姿态；采用局部优化策略，优化扩展的高斯表示。性能：与传统或神经 SLAM 方法相比，EndoGSLAM 在重建质量和计算效率方面都取得了更好的性能。工作量：EndoGSLAM 的实现相对简单，易于与现有的内窥镜系统集成。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d057be5f832b3e03f093e080cdab45a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5b928bbe4980e4f0920a7da14a03655.jpg" align="middle"><img src="https://pica.zhimg.com/v2-51aeb80d1b37a5bd4a8b984b3c6b5838.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e58b985d3822ba88d3729dcbc837db5.jpg" align="middle"></details>## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**Authors:Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao**Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video. [PDF](http://arxiv.org/abs/2403.14939v1) **Summary**时空一致性四维内容生成新框架：STAG4D，融合预训练扩散模型与动态三维高斯散射，无需扩散网络预训练或微调。**Key Takeaways**- STAG4D 框架，融合预训练扩散模型与动态三维高斯散射，用于高保真四维生成。- 采用多视图扩散模型初始化多视图图像，作为输入视频帧的锚点。- 引入融合策略，利用第一帧作为自我注意计算中的时间锚点，确保多视图序列初始化的时间一致性。- 应用分数蒸馏采样优化四维高斯点云。- 特殊设计的四维高斯散射用于生成任务，提出自适应致密化策略以缓解不稳定的高斯梯度，实现鲁棒优化。- 无需预训练或微调扩散网络，为四维生成任务提供更便捷实用的解决方案。- 广泛实验表明，该方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的四维生成工作，为基于文本、图像和视频等不同输入的四维生成树立了新的技术标杆。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：STAG4D：时空锚定生成模型</li><li>作者：Bingbing Ni, Jingwen Zhang, Yinda Zhang, Yebin Liu, Xin Tong</li><li>单位：南京大学</li><li>关键词：4D 生成·3D 高斯点云·扩散模型</li><li>论文链接：https://arxiv.org/pdf/2302.00533.pdf，Github 链接：无</li><li>摘要：（1）：研究背景：近年来，预训练扩散模型和 3D 生成技术取得了很大进展，激发了人们对 4D 内容创作的兴趣。然而，实现具有时空一致性的高保真 4D 生成仍然是一个挑战。（2）：过去方法及其问题：现有的 4D 生成方法主要基于 3D 生成技术，如体素网格和点云渲染。这些方法通常需要预训练或微调扩散网络，并且在处理复杂场景和时间一致性方面存在困难。（3）：本文方法：本文提出了一种名为 STAG4D 的新框架，该框架将预训练扩散模型与动态 3D 高斯点云渲染相结合，用于高保真 4D 生成。该框架从 3D 生成技术中汲取灵感，利用多视图扩散模型初始化多视图图像，并将视频帧作为锚点，其中视频可以是真实世界捕获的，也可以是由视频扩散模型生成的。为了确保多视图序列初始化的时间一致性，本文引入了一种简单有效的融合策略，利用第一帧作为自注意力计算中的时间锚点。使用几乎一致的多视图序列，然后应用得分蒸馏采样来优化 4D 高斯点云。4D 高斯点云渲染是专门为生成任务设计的，其中提出了一种自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。值得注意的是，所提出的管道不需要对扩散网络进行任何预训练或微调，为 4D 生成任务提供了一种更易于访问和实用的解决方案。（4）：方法性能及与目标的一致性：广泛的实验表明，本文方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的 4D 生成工作，为来自文本、图像和视频等不同输入的 4D 生成设定了新的最先进水平。这些性能支持了本文的目标，即实现具有高保真度和时空一致性的 4D 内容生成。</li></ol><p>7.方法：（1）：4D表示：提出 4D 高斯点云表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。（2）：时间和多视图一致扩散：结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。（3）：多视图 SDS 优化：利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯点云，实现时空一致的 4D 生成。</p><ol><li>结论：（1）本文提出了一种从单目视频生成动态 3D 内容的新方法，解决了 4D 表示和时空一致性的挑战。通过利用专门定制的 4D 高斯体素渲染和新颖的信息融合模块，所提出的方法实现了高质量且鲁棒的 4D 场景生成。全面的实验表明了该方法的有效性，与最先进的先前方法相比，展示了明显更快的生成速度以及渲染质量和时间一致性的显着改进。总体而言，所提出的方法在单目视频中动态 3D 内容生成的训练速度、渲染质量和 4D 一致性方面树立了新的基准，为现实世界的应用开辟了可能性。（2）创新点：</li><li>提出了一种新的 4D 高斯体素表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。</li><li>结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。</li><li>利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯体素，实现时空一致的 4D 生成。性能：</li><li>与最先进的方法相比，渲染质量、时间一致性和生成鲁棒性方面取得了显着改进。</li><li>与现有的 4D 生成方法相比，生成速度明显提高。工作量：</li><li>无需对扩散网络进行任何预训练或微调。</li><li>易于实现和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-cc3237d865a131294adf4c088d9c1009.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bdb6857c03ea01ca9348a454fc10619.jpg" align="middle"><img src="https://picx.zhimg.com/v2-171cac27c18392a0d918da1cdd0d421b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/</id>
    <published>2024-03-28T03:07:02.000Z</published>
    <updated>2024-03-28T03:07:02.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v1">PDF</a> </p><p><strong>Summary</strong><br>深度伪造技术的发展与检测技术需要持续演进，以应对隐私侵犯和网络钓鱼等非法使用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和评估指标。</li><li>探讨生成和检测技术框架的发展。</li><li>关注人脸替换、人脸重现、说话人脸生成、面部属性编辑等主流深度伪造领域。</li><li>全面基准测试每个领域流行数据集上的代表性方法。</li><li>分析所讨论领域的挑战和未来研究方向。</li><li>跟踪 Github 上深度伪造生成与检测的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.题目：深度伪造生成与检测：基准与综述2.作者：甘沛，蒋宁章，孟涵胡，光涛翟，成杰王，振宇张，建杨，春华沈，大成陶3.第一作者单位：华东师范大学多模信息处理上海市重点实验室4.关键词：深度伪造生成，人脸替换，人脸重演，语音人脸生成，人脸属性编辑，外来检测，综述5.论文链接：https://arxiv.org/abs/2403.17881，Github代码链接：无6.总结：（1）：随着深度学习的进步，以变分自编码器 (VAE) 和生成对抗网络 (GAN) 为代表的技术在深度伪造生成领域取得了显著成果。近年来，具有强大图像生成能力的扩散模型的出现引发了该技术的新一轮研究和产业热潮。（2）：传统的深度伪造生成方法基于 GAN 模型，存在生成效果不佳的问题。扩散模型的出现极大地提升了图像/视频的生成能力，使得生成的深度伪造内容与真实内容难以区分，具有很高的实用价值。（3）：深度伪造生成主要分为人脸替换、人脸重演、语音人脸生成和人脸属性编辑四个主流研究领域。本文对这些领域的发展进行了综述，并对各个领域的代表性方法进行了基准测试和全面评估。（4）：本文分析了深度伪造生成和检测领域面临的挑战和未来研究方向，为该领域的进一步发展提供了参考。</p><p></p><ol><li><p>方法：(1) 本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，分析了该领域面临的挑战和未来研究方向。(2) 本文对深度伪造生成领域的主流研究领域，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑，进行了基准测试和全面评估。(3) 本文对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。</p></li><li><p>结论：(1): 本综述全面回顾了深度伪造生成与检测领域的最新进展，首次全面覆盖了相关领域，并讨论了扩散模型等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、数据收集与处理方法、模型设计与训练策略、评估指标和数据集。(2): 创新点：本文对深度伪造生成领域的四个主流研究领域进行了基准测试和全面评估，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑。本文还对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。性能：本文提出的基准测试和全面评估为深度伪造生成与检测领域的研究人员提供了有价值的参考。工作量：本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3fbcb20b0b6d83737be267b8b78dde71.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bac7dee6bad7c9614f746a35eef341ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a0d28dab08c4d0254dd790d3d608013.jpg" align="middle"><img src="https://picx.zhimg.com/v2-409f1c30ffae605d9a497f77ff9ae5bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80df0902b8cc7d09c263750672e1ab59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4b73e97f1af3856b9dddf84237d9fcb.jpg" align="middle"></details><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>使用仅一分钟视频训练即可生成拥有躯干和手部动作的主播风格完整视频。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种通过一分钟视频训练来生成主播风格视频的系统——Make-Your-Anchor。</li><li>采用两阶段训练策略，将动作与特定外观有效地绑定。</li><li>扩展了帧级扩散模型中的二维 U-Net 到三维风格，以生成任意长度的时间视频。</li><li>提出了一种简单的批量重叠时间去噪模块，以绕过推理期间视频长度的限制。</li><li>引入了新颖的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。</li><li>与 SOTA 扩散/非扩散方法相比，该系统在视觉质量、时间连贯性和身份保留方面表现出有效性和优越性。</li><li>项目主页：\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}。">https://github.com/ICTMCG/Make-Your-Anchor}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Make-Your-Anchor：基于扩散的 2D 头像生成框架</li><li>作者：Ziyao Huang、Fan Tang、Yong Zhang、Xiaodong Cun、Juan Cao、Jintao Li、Tong-Yee Lee</li><li>第一作者单位：中国科学院计算技术研究所</li><li>关键词：视频生成、头像生成、扩散模型、运动捕捉</li><li>论文链接：https://arxiv.org/abs/2403.16510   Github 代码链接：https://github.com/ICTMCG/Make-Your-Anchor</li><li>摘要：   （1）研究背景：   当前的头像生成技术主要集中在头部生成，无法生成全身动作逼真的头像视频。   （2）过去方法及问题：   现有的基于 GAN 的方法只能生成局部区域，基于运动迁移的方法受限于运动捕捉数据的可用性。   （3）研究方法：   本文提出 Make-Your-Anchor 框架，通过微调基于结构引导的扩散模型，将 3D 网格条件渲染为逼真的全身动作头像视频。采用两阶段训练策略，有效地将运动与特定外观绑定。为了生成任意长度的视频，将帧级扩散模型中的 2D U-Net 扩展为 3D，并提出了一种简单的批次重叠时间去噪模块。此外，还引入了一个新的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。   （4）任务和性能：   Make-Your-Anchor 在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法。它仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。</li></ol><p>7.方法：(1)结构引导扩散模型：将3D网格条件嵌入生成过程，学习姿态与目标视频帧之间的对应关系；(2)两阶段训练策略：预训练增强模型生成动作的能力，微调绑定动作与特定外观；(3)批次重叠时间去噪：采用全帧交叉注意力模块和重叠时间去噪算法，生成任意长度的时间一致视频；(4)身份特定面部增强模块：通过裁剪和融合操作，对生成的身体中的面部区域进行修改，提高视觉质量。</p><ol><li>结论：（1）：本文提出了 Make-Your-Anchor，一个基于扩散的 2D 头像生成框架，用于制作逼真且高质量的主播风格人物视频。该框架通过帧级运动到外观扩散训练了一个结构引导的扩散模型，并采用两阶段训练策略和绑定风格方法实现了特定外观与动作的绑定。为了生成时间一致的人物视频，我们提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制。从观察到面部细节在整体人物生成过程中难以重建这一现象出发，引入了身份特定的面部增强技术。通过将这四个系统方法相结合，我们的框架成功地生成了高质量、结构保持和时间连贯的主播风格人物视频，这可能为 2D 数字头像的广泛应用技术提供一些参考价值。（2）：创新点：提出了一种基于扩散的 2D 头像生成框架，可以生成逼真且高质量的主播风格人物视频；采用两阶段训练策略和绑定风格方法，将特定外观与动作绑定；提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制；引入了身份特定的面部增强技术，以提高生成视频中面部区域的视觉质量。性能：在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法；仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。工作量：中等；需要收集和预处理训练数据；需要对模型进行训练和微调；需要对生成结果进行评估和优化。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation"><a href="#Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation" class="headerlink" title="Adaptive Super Resolution For One-Shot Talking-Head Generation"></a>Adaptive Super Resolution For One-Shot Talking-Head Generation</h2><p><strong>Authors:Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</strong></p><p>The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \url{<a href="https://github.com/Songluchuan/AdaSR-TalkingHead/}">https://github.com/Songluchuan/AdaSR-TalkingHead/}</a>. </p><p><a href="http://arxiv.org/abs/2403.15944v1">PDF</a> 5 pages, 3 figures</p><p><strong>Summary</strong><br>一键式生成高清晰度视频，无需添加预训练模块，通过自适应重建高频细节提升视频清晰度。</p><p><strong>Key Takeaways</strong></p><ul><li>一键式生成人像视频，驱动视频与人像同一或不同。</li><li>传统方法受限于单图像源和像素位移，清晰度受损。</li><li>现有方法通过超分辨率模块提升质量，但增加计算量并破坏原始数据分布。</li><li>本文提出自适应高品质人像视频生成方法，无需额外预训练模块合成高分辨率视频。</li><li>受超分辨率方法启发，对单图像源下采样，再通过编码器-解码器模块自适应重建高频细节。</li><li>该策略简单有效地提升了生成视频的质量，并通过定量和定性评估得到证实。</li><li>代码和演示视频可在 Github 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：自适应超分辨率单镜头说话人头部生成</li><li>作者：Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</li><li>第一作者单位：罗切斯特大学</li><li>关键词：超分辨率视频，单镜头说话人头部生成</li><li>链接：https://arxiv.org/abs/2403.15944，Github：None</li><li><p>摘要：(1)：研究背景：单镜头说话人头部生成旨在使用一张源人像图像在相同或不同身份视频的驱动下合成说话人头部视频。现有方法通常需要基于平面的像素变换，这会影响合成图像的清晰度。一些方法通过引入额外的超分辨率模块来提高合成视频的质量，但这会增加计算消耗并破坏原始数据分布。(2)：过去方法及问题：MetaPortrait、SadTalker 和 VideoReTalking 等方法尝试通过重新训练一个独立的超分辨率模块来改善视频质量。然而，这种两阶段合成过程会导致不必要的计算开销和错误累积。(3)：研究方法：本文提出了一种自适应超分辨率方法，用于说话人头部生成框架。受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，该方法通过压缩和下采样高质量图像来构建用于成对训练的低质量图像数据。它通过独特设计的编码器-解码器结构从低质量图像中自适应地捕获高频信息以进行重建。(4)：方法性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。</p></li><li><p>方法：(1) 受 ESRGAN 和 Real-ESRGAN 等超分辨率方法启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据；(2) 通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。</p></li><li><p>总结：（1）本工作的重要意义：本文提出了一种自适应超分辨率方法，用于单镜头说话人头部视频生成领域。通过设计简单但有效的方法，我们的方法能够从低质量图像中捕获高频细节。这使得无需额外的预训练模块或后处理即可合成高质量视频。在大型数据集上进行的广泛定量和定性评估证实，我们的方法在高质量可驱动人脸视频生成方面超越了现有技术。（2）创新点：受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据。通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。工作量：该方法的实现相对简单，并且不需要额外的预训练模块或后处理步骤。这使得该方法在计算和时间方面都具有成本效益。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fbfc28956b0106142272e9ccedb9ced5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-188e4004db88e63a7e920e9ac2f3636d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b69fbe4c0930a57ff002ead5463e3ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3a0dd3488e1d1a03f494038c2fcb247.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-28  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
</feed>
