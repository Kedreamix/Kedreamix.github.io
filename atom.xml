<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-04-22T09:43:13.959Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</id>
    <published>2024-04-22T09:43:13.000Z</published>
    <updated>2024-04-22T09:43:13.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF. </p><p><a href="http://arxiv.org/abs/2404.11897v1">PDF</a> </p><p><strong>Summary</strong><br>降低训练成本，实现多高度自由视角图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>多高度神经辐射场（NeRF）能够合成自由视角图像。</li><li>提出图像选择方法和注意力特征融合，解决不同高度细节差异问题。</li><li>AG-NeRF 在 56 Leonard 和 Transamerica 基准上达到最先进性能。</li><li>AG-NeRF 训练时间仅需半小时，即可达到 BungeeNeRF 的竞争水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: 多高度大尺度户外场景渲染的注意力引导神经辐射场</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: 华南理工大学</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 现有的基于神经辐射场 (NeRF) 的大规模户外场景新视角合成方法主要建立在单一高度上。此外，它们通常需要先验的相机拍摄高度和场景范围，当相机高度发生变化时，会导致低效且不实用的应用。</p><p>(2): 过去的方法：   - 地理上将场景分解为几个单元格，并为每个单元格训练一个子 NeRF，然后将它们合并。   - 在位置编码中并行应用平面和网格特征以实现高效建模。   - 问题：它们在基础高度上重建大规模场景，当导航到更近的地方以检查大规模户外场景的微观细节时，表现出过度模糊的伪影和不完整的重建。</p><p>(3): 本文提出的研究方法：   - 提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。   - 具体来说，为了解决从低高度（无人机级别）到高高度（卫星级别）的细节变化问题，开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>(4): 本文方法在任务和性能上的表现：   - 在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能。   - 只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。   - 性能支持了他们的目标：降低构建良好重建的训练成本。</p><ol><li>方法：</li></ol><p>（1）：提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。</p><p>（2）：开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>（3）：利用可训练的 U-Net 网络从源图像中提取特征图，并使用 Transformer 对提取的特征向量进行融合，以最大化融合特征与目标像素之间的相关性。</p><p>（4）：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><ol><li>结论：</li></ol><p>（1）：本文针对不同高度拍摄的大场景渲染提出了端到端的 AG-NeRF 框架，降低了构建良好重建模型的训练成本。</p><p>（2）：创新点：提出了一种源图像选择方法和基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。性能：在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能，只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。工作量：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy. </p><p><a href="http://arxiv.org/abs/2404.11419v1">PDF</a> </p><p><strong>Summary</strong><br>Nerf-SLAM 通过采用从粗到细的跟踪模型和 KL 正则化器，在跟踪性能和重建精度上实现了最先进的成绩。</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM 提出了一种从粗到细的跟踪模型以提高 NeRF-SLAM 的跟踪性能。</li><li>SLAIM 通过高斯金字塔滤波器实现从粗到细的跟踪优化策略。</li><li>NeRF 系统难以使用有限的输入视图收敛到正确的几何形状。</li><li>SLAIM 使用体积密度表示和一个新的 KL 正则化器来约束场景几何形状。</li><li>SLAIM 实现局部和全局捆绑调整以提高鲁棒性和准确性。</li><li>SLAIM 在多个数据集上进行了实验，在跟踪和重建精度上均显示出最先进的结果。</li><li>SLAIM 解决了 NeRF-SLAM 在传统 SLAM 算法下表现出较差的跟踪性能这一难题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：SLAIM：用于在线跟踪和建图的鲁棒稠密神经SLAM</p></li><li><p>作者：Vincent Cartillier、Grant Schindler、Irfan Essa</p></li><li><p>隶属关系：佐治亚理工学院</p></li><li><p>关键词：神经辐射场、SLAM、稠密建图、跟踪</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11419，Github 代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：稠密视觉SLAM是3D计算机视觉中的一个长期问题，在自动驾驶、室内外机器人导航、虚拟现实和增强现实等领域有着广泛的应用。</p><p>（2）过去的方法及问题：传统的SLAM系统通过估计图像对应关系来开始，这些对应关系可能是稀疏的，例如匹配的特征点。神经辐射场SLAM（NeRF-SLAM）方法通过图像对齐和光度捆绑调整来解决相机跟踪问题。由于图像空间中优化损失的吸引域窄（局部极小值）以及缺乏初始对应关系，此类优化过程难以优化。</p><p>（3）提出的研究方法：本文提出了一种新的粗到细跟踪模型，专门针对NeRF-SLAM，以实现最先进的跟踪性能。此外，本文还引入了一种新的目标射线终止分布，并将其用于KL正则化器中，以约束场景几何由空空间和不透明表面组成。</p><p>（4）任务和性能：本文方法在ScanNet、TUM、Replica等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。这些性能支持了本文的目标。</p><ol><li><p>方法：</p><pre><code>            (1):SLAIM 是一种用于稠密映射和跟踪的 RGB-D 输入流的 novel 方法；            (2):SLAIM 采用了一种从粗到精的跟踪模型，以实现最先进的跟踪性能；            (3):SLAIM 引入了一种新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成；            (4):SLAIM 在 ScanNet、TUM、Replica 等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文的工作意义：本文提出了一种最先进的稠密实时 RGB-D NeRF-SLAM 系统 SLAIM，该系统具有最先进的相机跟踪和建图能力。</p><p>（2）本文的优缺点总结：    - 创新点：        - 采用从粗到精的跟踪模型，实现最先进的跟踪性能。        - 引入新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成。    - 性能：        - 在 ScanNet、TUM、Replica 等多个数据集上取得了最先进的跟踪和重建精度。    - 工作量：        - 内存效率高，在 Replica 和 ScanNet 数据集上与基准相比，跟踪和建图时间均有明显降低。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. </p><p><a href="http://arxiv.org/abs/2404.11401v1">PDF</a> </p><p><strong>Summary</strong><br>基于神经网络的光谱偏差特性，RainyScape利用无监督框架重建干净场景，包含神经渲染模块和雨滴预测模块。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经网络的光谱偏差特性获得低频场景表示。</li><li>联合优化神经渲染模块和雨滴预测模块，以区分场景细节和雨滴条纹。</li><li>提出自适应方向敏感梯度重建损失，引导网络区分场景细节和雨滴条纹。</li><li>在经典神经辐射场和 3D 高斯斑点 splatting 数据集上均达到最先进的去雨性能。</li><li>提供高质量数据集和源代码，促进研究工作。</li><li>引入可学习潜在嵌入，捕捉场景的雨滴特征。</li><li>通过雨滴预测网络有效消除雨滴条纹，渲染干净图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: 无监督雨景重建使用解耦神经渲染</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: 香港城市大学计算机科学系</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):随着神经辐射场（NeRF）在图像合成中的广泛应用，当输入图像受到模糊、噪声或雨水等因素影响时，渲染结果不可避免地会产生明显的伪影。</p><p>(2):现有的方法针对特定任务提出了各种解决方案，但对于雨景重建任务，它们无法有效表示三维空间中稀疏且间歇性的降雨。</p><p>(3):本文提出RainyScape，一个解耦的神经渲染框架，它能够以无监督的方式从雨景图像中重建无雨场景。该框架通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水。此外，本文还提出了一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹。</p><p>(4):在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p><ol><li><p>方法：</p><p>（1）：提出RainyScape，一个解耦的神经渲染框架，可以无监督地从雨景图像中重建无雨场景；</p><p>（2）：通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水；</p><p>（3）：提出一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹；</p><p>（4）：在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>结论：</p></li></ol><p>（1）：RainyScape的意义在于，它提出了一种无监督的解耦神经渲染框架，可以从雨景图像中重建无雨场景，有效解决了雨景重建中的雨水条纹去除问题，为雨景图像处理提供了新的思路和方法。</p><p>（2）：创新点：</p><ul><li><p>提出了一种解耦的神经渲染框架，通过低频场景表示、可学习的雨水嵌入和预测器以及自适应角度估计策略和梯度旋转损失，有效解耦了场景高频细节和雨水条纹。</p></li><li><p>性能：</p></li><li><p>在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>工作量：</p></li><li><p>该方法需要对雨景图像进行预处理，包括图像分割、雨水条纹检测和雨水嵌入提取等步骤，增加了计算量和时间开销。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video**Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu**In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary**对于一般性关节动作的3D物体，本文提出了一种新的变形模型，即准刚性混合蒙皮，以便从单个视频中进行全面重建。**Key Takeaways**- 提出一种新的变形模型，准刚性混合蒙皮，增强了零件刚性，同时保持关节柔性变形。- 采用增强骨骼绑定系统改善组件建模。- 使用准稀疏蒙皮权重提高零件刚性和重建保真度。- 应用测地线点赋值实现精确运动和无缝变形。- 在真实和合成数据集上，该方法在生成高保真一般性关节动作的3D重建方面优于先前的工作。- 该研究为一般性关节动作的3D物体重建提供了新的方法。- 该研究在计算机视觉和图形学领域具有潜在应用价值。- 该研究有助于推动相关领域的发展。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：REACTO：从单一视频中重建铰接物体</p></li><li><p>作者：Chaoyue Song、Jiacheng Wei、Chuan Sheng Foo、Guosheng Lin、Fayao Liu</p></li><li><p>隶属：南洋理工大学</p></li><li><p>关键词：铰接物体重建、动态神经辐射场、准刚性混合蒙皮</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11151, Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：重建铰接物体是计算机视觉中的一项重要任务，但现有方法在处理具有分段刚性的通用铰接物体时面临挑战。</p><p>（2）：过去方法：NASAM和PARIS等方法需要多视角图像或多视图图像，在实际应用中受限。</p><p>（3）：研究方法：本文提出了一种准刚性混合蒙皮变形模型，该模型通过增强骨骼装配系统、使用准稀疏蒙皮权重和应用测地线点分配来提高刚性并保持关节的灵活变形。</p><p>（4）：任务与性能：REACTO在真实和合成数据集上对通用铰接物体的3D重建任务中取得了较高的保真度，证明了其性能可以支持其目标。</p><p><strong>7. Methods：</strong></p><p>(1)：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>(2)：构建REACTO框架，包括骨骼装配、蒙皮变形、体绘制和渲染模块；</p><p>(3)：使用基于神经辐射场的渲染器，从单一视频中重建铰接物体；</p><p>(4)：通过优化骨骼参数、蒙皮权重和神经辐射场参数，实现铰接物体的高保真重建；</p><p>(5)：在真实和合成数据集上进行实验，验证REACTO的有效性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出REACTO，一种从单一视频中重建通用铰接3D物体的开创性方法，通过重新定义装配结构并采用准刚性混合蒙皮，实现了建模和精度的提升。准刚性混合蒙皮通过利用准稀疏蒙皮权重和测地线点分配，确保了每个部件的刚性，同时在关节处保持平滑变形。广泛的实验表明，REACTO在真实和合成数据集上都优于现有方法，保真度和细节方面都有所提升。</p><p>（2）：创新点：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>性能：在真实和合成数据集上，REACTO在保真度和细节方面都优于现有方法；</p><p>工作量：与需要多视角或多视图图像的现有方法相比，REACTO只需单一视频即可重建铰接物体，工作量更小。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</id>
    <published>2024-04-22T09:32:29.000Z</published>
    <updated>2024-04-22T09:32:29.438Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>通过借鉴2D说话人面部的唇形同步和言语感知领域的专业知识，提出了一种学习框架，可以构建更好的3D说话人面部网络。</p><p><strong>Key Takeaways</strong></p><ul><li>3D说话人面部研究在唇形同步和言语感知方面不如2D说话人面部研究深入。</li><li>Learn2Talk框架利用2D说话人面部领域的两个专业知识点来构建更好的3D说话人面部网络。</li><li>3D同步唇专家模型旨在实现音频和3D面部运动之间的唇形同步。</li><li>2D说话人面部方法中选择的教师模型用于指导音频到3D运动回归网络的训练，以提高3D顶点精度。</li><li>广泛的实验表明，该框架在唇形同步、顶点精度和言语感知方面优于现有技术。</li><li>该框架有语音-视觉语音识别和语音驱动3D高斯飞溅基于头像动画两个应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：Learn2Talk：3D 说话人脸从 2D 说话人脸学习</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, XuanCheng, Jing Liao, Juncong Lin</p></li><li><p>单位：暂缺</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.12888v1Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：说话人脸动画方法通常包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究在唇形同步（lip-sync）和语音感知方面并未像 2D 说话人脸那样深入。</p><p>（2）：过去的方法及其问题：本文方法动机充分。</p><p>（3）：本文提出的研究方法：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法性能：本文方法在唇形同步、顶点精度和语音感知方面均优于现有技术。这些性能可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。</p><p>（2）：设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。</p><p>（3）：选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络，在唇形同步、顶点精度和语音感知方面均优于现有技术。</p><p>（2）：创新点：提出了一种新的 3D 说话人脸动画方法，该方法利用了 2D 说话人脸领域的专业知识；性能：在唇形同步、顶点精度和语音感知方面均优于现有技术；工作量：需要收集和标注大量的数据。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue**We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary**使用来自不同视角的对比高斯聚类实现 3D 场景分割。**Key Takeaways**- 提出一种新的对比高斯聚类方法，能够从任何视角提供分割掩模，并实现场景的 3D 分割。- 受新视角合成领域研究的启发，使用 3D 高斯云建模场景的外观。- 通过将高斯投影到给定视点并对其颜色进行α混合，从给定视点生成准确的图像。- 训练模型，使每个高斯都包含一个分割特征向量。- 通过根据其特征向量对高斯进行聚类，可用于 3D 场景分割；通过将高斯投影到平面上并对其分割特征进行 α 混合，可生成 2D 分割掩模。- 使用对比学习和空间正则化的组合，可以在不一致的 2D 分割掩模上训练我们的方法，并学习生成在所有视图中都一致的分割掩模。- 所提出的方法非常准确，与现有技术相比，预测掩模的 IoU 准确度提高了 8%。- 代码和训练好的模型即将发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：对比高斯聚类：弱监督 3D 场景分割</p></li><li><p>作者：Myrna C. Silva、Mahtab Dahaghin、Matteo Toso、Alessio Del Bue</p></li><li><p>单位：意大利理工学院模式分析与计算机视觉（PAVIS）</p></li><li><p>关键词：3D 高斯散射、3D 分割、对比学习</p></li><li><p>论文链接：arXiv:2404.12784v1 [cs.CV]   Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，新视角合成领域的研究表明，可以通过 3D 高斯云对场景的外观进行建模，并通过在给定视角上投影高斯并 α 混合其颜色来生成准确的图像。</p><p>（2）：过去方法与问题：高斯分组和 LangSplat 等方法存在以下问题：   - 训练和评估需要大量 GPU 内存，导致某些场景无法处理。   - 无法从任意视角提供分割掩码，也无法实现场景的 3D 分割。</p><p>（3）：研究方法：本文提出对比高斯聚类方法，该方法通过以下步骤实现 3D 场景分割和 2D 分割掩码预测：   - 训练模型为每个高斯体添加分割特征向量。   - 根据特征向量对高斯体进行聚类，实现 3D 场景分割。   - 将高斯体投影到平面上并 α 混合其分割特征，生成 2D 分割掩码。   - 使用对比学习和空间正则化，在不一致的 2D 分割掩码上训练模型，生成跨所有视角一致的分割掩码。</p><p>（4）：性能与目标：   - 任务：3D 场景分割和 2D 分割掩码预测。   - 性能：IoU 准确率比现有技术提高 +8%，表明该方法能够有效实现其目标。</p><ol><li>方法：</li></ol><p>（1）：将场景表示为 3D 高斯体集合，编码几何、外观和实例分割信息；</p><p>（2）：使用基础模型生成 2D 分割掩码；</p><p>（3）：优化 3D 高斯体，最小化渲染图像和真实图像之间的差异；</p><p>（4）：使用对比分割损失监督 3D 特征场；</p><p>（5）：引入正则化项，强制高斯体在欧几里得和分割特征空间中的距离相关；</p><p>（6）：渲染 2D 特征图，根据对应的 2D 分割掩码对渲染特征进行聚类，计算对比聚类损失；</p><p>（7）：最大化同一分割内特征之间的相似度，最小化不同分割内的特征相似度。</p><p><strong>8. 结论</strong></p><p><strong>(1)</strong> 本工作的主要意义在于：</p><p>提出了对比高斯聚类方法，实现了 3D 场景分割和 2D 分割掩码预测，有效提高了分割精度。</p><p><strong>(2)</strong> 本文优缺点总结（创新点、性能、工作量）：</p><p><strong>创新点：</strong></p><ul><li>引入对比学习和空间正则化，提高了分割掩码的一致性。</li><li>使用 3D 高斯体表示场景，编码几何、外观和实例分割信息。</li></ul><p><strong>性能：</strong></p><ul><li>IoU 准确率比现有技术提高 +8%，分割精度高。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和评估需要大量 GPU 内存，大场景处理困难。</li><li>无法从任意视角提供分割掩码，无法实现场景的完整 3D 分割。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering**Authors:Xianqiang Lyu, Hui Liu, Junhui Hou**We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary**雨景重建：无监督地从多视角雨景图重建干净场景。**Key Takeaways**- 提出无监督框架 RainyScape，重建干净场景。- RainyScape 由神经渲染和降雨预测模块组成。- 降雨预测模块包含预测网络和可学习潜嵌入，捕捉场景的降雨特征。- 基于神经网络的光谱偏差属性，优化神经渲染管道，获得低频场景表示。- 利用自适应方向敏感梯度重建损失，优化两个模块，区分场景细节和雨痕。- 在神经辐射场和 3D 高斯喷溅中进行的实验表明，该方法能有效消除雨痕、渲染干净图像，达到最先进性能。- 将公开构建高质量数据集和源代码。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：RainyScape：基于解耦神经渲染的无监督雨景重建</p></li><li><p>作者：Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>单位：香港城市大学计算机科学系</p></li><li><p>关键词：雨景重建、神经渲染、无监督损失</p></li><li><p>论文链接：xxx，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）在学习场景的连续体积表示方面取得了突破性进展，但当输入图像因模糊、噪声或雨水等因素而退化时，渲染结果不可避免地会出现明显伪影。</p><p>（2）：过去方法：现有方法针对不同的退化因素提出了特定任务的解决方案，但针对雨景重建任务的方法较少，且难以通过附加神经渲染场来表示雨水。</p><p>（3）：研究方法：本文提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><p>（4）：方法性能：在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。</p><p>（2）：该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹。</p><p>（3）：提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><ol><li>结论：<pre><code>            （1）：RainyScape 在雨景重建领域具有重要意义，它首次提出了一个解耦神经渲染框架，能够从雨景图像中无监督地重建无雨场景。 该框架通过将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            （2）：创新点：RainyScape 创新性地提出了一个解耦神经渲染框架，将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            性能：RainyScape 在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。            工作量：RainyScape 的工作量中等，需要训练神经渲染模块和雨滴预测模块，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos. </p><p><a href="http://arxiv.org/abs/2404.11358v2">PDF</a> </p><p><strong>Summary</strong><br>从模糊运动图像重建清晰 3D 场景方法，优化 3D 高斯投射，实现精确摄像机位姿初始化。</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS 优化高斯投射，提高运动模糊图像 3D 重建精度。</li><li>利用高斯投射的重建能力，还原精细锐利场景。</li><li>估计每幅模糊图像的 6 自由度摄像机运动，生成模糊渲染用于优化。</li><li>高斯密度退火策略防止错误位置生成不准确的高斯。</li><li>DeblurGS 在去模糊和合成新视角方面取得了最先进的性能。</li><li>适用于真实世界和合成基准数据集，以及现场拍摄的模糊智能手机视频。</li><li>DeblurGS 极大地扩展了运动模糊图像的 3D 重建的实际应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: 高斯溅射相机运动模糊 (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: 首尔国立大学人工智能与信息处理研究所 (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting · Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 尽管从运动模糊图像重建清晰的 3D 场景方面取得了重大进展，但向实际应用的过渡仍然具有挑战性。主要障碍源于严重的模糊，这会导致通过 Structure-from-Motion 获取初始相机姿态的不准确，而这往往是以前的方法所忽视的关键方面。</p><p>(2): 过去的方法主要集中于模糊图像的去模糊处理，但对于初始相机姿态的噪声初始化不鲁棒。</p><p>(3): 本文提出 DeblurGS，这是一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。我们利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景。我们的方法估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，我们提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。</p><p>(4): 综合实验表明，我们的 DeblurGS 在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 DeblurGS，一种从运动模糊图像优化清晰的 3D 高斯溅射的方法；</p><p>（2）：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景；</p><p>（3）：估计每个模糊观测的 6 自由度相机运动，并合成相应的模糊渲染；</p><p>（4）：提出高斯致密化退火策略，防止在相机运动不精确的早期训练阶段生成不准确的高斯。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。该方法利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景，估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，该方法提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。综合实验表明，该方法在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><p>（2）：创新点：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景，即使在噪声相机姿态初始化的情况下也是如此；</p><p>性能：在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能；</p><p>工作量：该方法需要估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染，这可能会增加计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/</id>
    <published>2024-04-22T09:22:24.000Z</published>
    <updated>2024-04-22T09:22:24.975Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>通过借鉴2D说话人脸的唇形同步(lip-sync)和语音感知的专业知识，Learn2Talk框架构建了一个更好的3D说话人脸网络。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了Learn2Talk框架，将2D说话人脸的专业知识应用于3D说话人脸网络。</li><li>设计了3D唇形同步专家模型，追求音频和3D面部动作之间的唇形同步。</li><li>使用2D说话人脸方法选择的教师模型来指导音频到3D动作回归网络的训练，以提高3D顶点精度。</li><li>实验表明，该框架在唇形同步、顶点精度和语音感知方面优于现有技术。</li><li>展示了该框架的两个应用：视听语音识别和语音驱动的3D高斯喷射动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：Learn2Talk：3D Talking Face Learns from 2D（3D 说话人脸从 2D 说话人脸中学习）</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</p></li><li><p>第一作者单位：暂无</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：暂无，Github 链接：https://lkjkjoiuiu.github.io/Learn2Talk/</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：语音驱动的面部动画方法主要包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究并未像 2D 说话人脸那样深入，在唇形同步（lip-sync）和言语感知方面存在差距。</p><p>（2）：以往方法：以往方法主要分为 2D 和 3D 说话人脸方法。2D 说话人脸方法通常在像素空间（例如图像、视频）中生成唇部运动或头部运动以匹配给定的输入音频流，而 3D 说话人脸方法使用时间 3D 顶点数据（例如 3D 人脸模板、混合形状参数）来表示面部运动。与 2D 说话人脸方法相比，3D 说话人脸方法可以合成更细微的唇部动作，因为细粒度的唇形校正可以在 3D 空间中更好地执行。此外，3D 面部动画具有重要的优势，因为它可以与 3D 模型或虚拟角色无缝集成，从而实现更逼真的交互。</p><p>（3）：本文提出的研究方法：为了弥合两者之间的差距，我们提出了一个名为 Learn2Talk 的学习框架，该框架可以通过利用 2D 说话人脸领域的两个专业知识点来构建更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇部专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，使用从 2D 说话人脸方法中选择的教师模型来指导音频到 3D 动作回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法在什么任务上取得了什么性能：广泛的实验表明，与最先进的方法相比，所提出的框架在唇形同步、顶点精度和言语感知方面具有优势。最终，我们展示了所提出框架的两个应用：视听语音识别和语音驱动的基于 3D 高斯泼溅的头像动画。这些结果表明，Learn2Talk 可以有效地利用 2D 说话人脸的专业知识来提高 3D 说话人脸的性能，从而为语音驱动的面部动画领域做出贡献。</p><ol><li><p>方法：</p><pre><code>            (1): 受音频视频同步网络的启发，设计了 3D 同步唇部专家模型 SyncNet3D，以追求音频和 3D 面部动作之间的唇形同步；            (2): 使用从 2D 说话人脸方法中选择的教师模型 LipReadNet 来指导音频到 3D 动作回归网络 Audio2Mesh 的训练，以产生更高的 3D 顶点精度；            (3): 提出了一种联合训练框架，将 SyncNet3D 和 Audio2Mesh 结合起来，通过联合损失函数优化，使 3D 说话人脸模型同时满足唇形同步和顶点精度要求。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本工作通过借鉴 2D 说话人脸领域的专业知识，提出了一种名为 Learn2Talk 的学习框架，有效提升了 3D 说话人脸的性能，为语音驱动的面部动画领域做出了贡献。</p><p>（2）：创新点：Learn2Talk 创新性地将 3D 同步唇部专家模型 SyncNet3D 与教师模型 LipReadNet 相结合，通过联合训练，实现了 3D 说话人脸模型在唇形同步和顶点精度方面的双重提升。</p><p>性能：在唇形同步、顶点精度和言语感知方面，Learn2Talk 均优于最先进的方法。</p><p>工作量：Learn2Talk 的训练过程较为复杂，需要同时训练 SyncNet3D 和 Audio2Mesh 两个模型，并且需要从 2D 说话人脸方法中选择教师模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/</id>
    <published>2024-04-22T09:18:09.000Z</published>
    <updated>2024-04-22T09:18:09.823Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models"><a href="#Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models" class="headerlink" title="Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models"></a>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Pedro Sanchez, Alison Q. O’Neil, Sotirios A. Tsaftaris</strong></p><p>Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the model’s weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.12920v1">PDF</a> 8 pages, 3 figures, submitted to IEEE J-BHI Special Issue on   Foundation Models in Medical Imaging</p><p><strong>Summary</strong><br>利用大型语言模型（如隐扩散模型）即使在没有目标数据训练的情况下也能执行文本引导定位任务。</p><p><strong>Key Takeaways</strong></p><ul><li>隐扩散模型具有隐式对齐视觉和文本特征的机制，适用于文本引导定位任务。</li><li>该方法采用零样本方式，无需对目标数据进行进一步训练。</li><li>通过特征选择和后处理策略，在不增加可学习参数的情况下优化特征。</li><li>该方法与采用对比学习显式强制图像和文本对齐的先进方法具有竞争力。</li><li>在胸部 X 射线基准测试中，该方法在不同类型的病理上与 SOTA 持平，在平均 IoU 和 AUC-ROC 两个指标上甚至优于 SOTA。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 零样本医学短语定位</p></li><li><p>Authors: Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris</p></li><li><p>Affiliation: 爱丁堡大学工程学院</p></li><li><p>Keywords: 深度学习, 扩散模型, 医学影像, 短语定位, 零样本学习</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12920, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文研究背景是医学影像中病理区域定位任务需要大量边界框标注，而文本引导定位任务（短语定位）可以提供一种替代的弱监督形式。</p><p>(2): 现有方法通过对比学习在联合嵌入空间中强制执行图像-文本对齐，但存在显式对齐计算量大、泛化性差的问题。</p><p>(3): 本文提出了一种零样本短语定位方法，利用预训练的扩散模型中的交叉注意力机制隐式对齐图像和文本特征，并通过特征选择和后处理策略在不增加可学习参数的情况下提升定位精度。</p><p>(4): 该方法在胸部 X 射线基准测试上取得了与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法，验证了其在医学影像领域零样本学习的可行性和有效性。</p><ol><li>方法：</li></ol><p>（1）：采用 Latent Diffusion Model（LDM），通过反向扩散过程逐步恢复图像，并利用 U-Net 模型中的交叉注意力机制对图像和文本特征进行隐式对齐；</p><p>（2）：收集不同层级和时间步长的交叉注意力图，并通过特征选择和后处理策略优化定位精度；</p><p>（3）：在不增加可学习参数的情况下，在胸部 X 射线基准测试上取得与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种利用预训练扩散模型进行短语定位的新方法，该方法在不改变生成模型的情况下，利用模型中视觉和文本特征融合的交叉注意力机制，实现了零样本短语定位，为医学影像领域零样本学习提供了新的思路；</p><p>（2）：创新点：利用预训练扩散模型中的交叉注意力机制隐式对齐图像和文本特征，实现零样本短语定位；性能：在胸部 X 射线基准测试上取得与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法；工作量：在不增加可学习参数的情况下，通过特征选择和后处理策略优化定位精度，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-edc65b84041a4ffbf6fad90dfbf52862.jpg" align="middle"><img src="https://picx.zhimg.com/v2-42ab68ed87191afb18c00170b44f792e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d782a2682f47c83a60efe8ef4da1aeb0.jpg" align="middle"></details><h2 id="Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images"><a href="#Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images" class="headerlink" title="Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images"></a>Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images</h2><p><strong>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</strong></p><p>Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detector’s robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detector’s generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at <a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection</a>. </p><p><a href="http://arxiv.org/abs/2404.12908v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型生成图像的真实性鉴别框架，利用 CLIP 模型提取图像和文本特征，并通过 MLP 分类器判别真实性和合成性。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成的图像真实性鉴别挑战性。</li><li>提出利用 CLIP 模型提取图像和文本特征的鉴别框架。</li><li>设计改进鉴别器鲁棒性的损失函数，并处理不平衡数据集。</li><li>对损失函数进行平滑处理，提升鉴别模型泛化能力。</li><li>实验结果表明该方法优于传统鉴别技术。</li><li>代码已开源：<a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection。">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 基于CLIP的稳健检测器用于揭露扩散模型生成图像</p></li><li><p>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</p></li><li><p>Affiliation: 普渡大学</p></li><li><p>Keywords: Diffusion models, CLIP, Robust, AI images</p></li><li><p>Urls: https://arxiv.org/abs/2404.12908, Github:https://github.com/Purdue-M2/Robust DM Generated Image Detection</p></li><li><p>Summary:</p></li></ol><p>(1):随着扩散模型（Diffusion models，DMs）在图像生成领域取得重大进展，其生成的图像质量不断提升，应用范围也不断扩大。然而，DM生成图像的逼真性也给区分真实图像和合成图像带来了巨大挑战，引发了对数字内容真实性和潜在滥用（如生成深度伪造内容）的担忧。</p><p>(2):传统方法主要利用CLIP图像特征或图像和文本特征，结合多层感知机（MLP）分类器和二元交叉熵（BCE）损失函数进行DM生成图像检测。然而，这些方法存在鲁棒性差、对不平衡数据集处理能力弱等问题。</p><p>(3):本文提出了一种基于CLIP图像和文本特征的稳健检测框架，采用MLP分类器和条件风险价值（CVaR）损失函数与面积下曲线（AUC）损失函数的组合，并在平坦化的损失函数曲面下进行训练，以提高检测器的鲁棒性和泛化能力。</p><p>(4):在DM生成图像检测任务上，本文方法在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性，有望成为DM生成图像检测领域的新技术标杆。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于 CLIP 图像和文本特征的稳健检测框架，采用 MLP 分类器和条件风险价值 (CVaR) 损失函数与面积下曲线 (AUC) 损失函数的组合。</p><p>（2）：在平坦化的损失函数曲面下进行训练，以提高检测器的鲁棒性和泛化能力。</p><p>（3）：在 DM 生成图像检测任务上，本文方法在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性。</p><ol><li>结论：</li></ol><p>（1）本文提出的方法在DM生成图像检测任务上取得了优异的性能，有望成为该领域的新技术标杆。</p><p>（2）创新点：提出了一种基于CLIP图像和文本特征的稳健检测框架，采用MLP分类器和CVaR损失函数与AUC损失函数的组合，并在平坦化的损失函数曲面下进行训练。</p><p>性能：在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性。</p><p>工作量：与传统方法相比，本文方法的训练时间更长，需要更多的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4d2d3895766f30bd509b9a3d935d9804.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4bf1ac8a20b7e67bfd03bc5cca10058c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6f7856d5aaeb46c1d7aa9023b3a02ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce9de0cd6eee8dc551b4cd04b517c61c.jpg" align="middle"></details><h2 id="Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing"><a href="#Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing" class="headerlink" title="Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing"></a>Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing</h2><p><strong>Authors:Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</strong></p><p>Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image. However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts. To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel “share-attention module”. This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations. Additionally, we further introduce “similarity reweighting” mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches. At last, we recognize the deficiencies in existing benchmarks and propose the “General Painterly Harmonization Benchmark”, which employs range-based evaluation metrics to more accurately reflect real-world application. Extensive experiments demonstrate the superior efficacy of our method across various benchmarks. The code and web demo are available at <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a>. </p><p><a href="http://arxiv.org/abs/2404.12900v1">PDF</a> </p><p><strong>Summary</strong></p><p>图像风格统一方法TF-GPH通过图像注意力共享，不需训练和提示，即可实现多样视觉元素的无缝融合。</p><p><strong>Key Takeaways</strong></p><ul><li>设计了一种不需训练和提示的通用图像风格统一方法 TF-GPH。</li><li>引入图像级注意力共享，打破了传统自注意力机制的局限。</li><li>提出相似性重新加权机制，有效利用跨图像信息，提升性能。</li><li>提出通用图像风格统一基准，采用基于范围的评估指标，更贴近真实应用。</li><li>实验表明，TF-GPH 在多个基准上均表现优异。</li><li>代码和网络演示可在 <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：训练与提示无关的通用绘画调和</li><p></p><p></p><li>作者：Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</li><p></p><p></p><li>单位：国立阳明交通大学</li><p></p><p></p><li>关键词：diffusion model, attention, image editing, image harmonization, painterly harmonization, style transfer</li><p></p><p></p><li>论文链接：xxx，Github 代码链接：https://github.com/BlueDyee/TF-GPH</li><p></p><p></p><li><p></p><p>摘要：（1）：研究背景：绘画图像调和旨在无缝地将不同的视觉元素融合到一个连贯的图像中。然而，由于训练数据限制、需要耗时的微调或依赖额外的提示，以前的方法经常遇到重大限制。（2）：过去的方法：过去的方法包括使用双域生成器和判别器的双域生成器和判别器，以及将图像融合到绘画中的 PHDiffusion 模型。这些方法存在训练数据限制、需要微调和依赖提示的问题。（3）：研究方法：本文提出了一种使用图像级注意力共享（TF-GPH）的训练和提示无关的通用绘画调和方法，该方法集成了一个新颖的“共享注意力模块”。该模块通过允许全面的图像级注意力来重新定义传统的自注意力机制，从而促进使用最先进的预训练潜在扩散模型而没有典型的训练数据限制。此外，我们进一步引入了“相似性重新加权”机制，通过有效利用跨图像信息来增强性能，超越了微调或基于提示的方法的能力。最后，我们认识到现有基准的缺陷，并提出了“通用绘画调和基准”，该基准采用基于范围的评估指标来更准确地反映实际应用。（4）：任务和性能：本文方法在各种基准上展示了其卓越的功效。该方法在通用绘画调和基准上的 FID 得分为 10.6，在绘画图像调和基准上的 FID 得分为 10.3，在图像编辑基准上的 FID 得分为 11.2。这些性能支持了他们的目标，即提供一种训练和提示无关的通用绘画调和方法，该方法可以在各种任务上实现最先进的性能。</p></li><li><p>Methods:</p></li></ol><p>（1）：提出了一种使用图像级注意力共享（TF-GPH）的训练和提示无关的通用绘画调和方法，该方法集成了一个新颖的“共享注意力模块”。</p><p>（2）：该模块通过允许全面的图像级注意力来重新定义传统的自注意力机制，从而促进使用最先进的预训练潜在扩散模型而没有典型的训练数据限制。</p><p>（3）：进一步引入了“相似性重新加权”机制，通过有效利用跨图像信息来增强性能，超越了微调或基于提示的方法的能力。</p><p>（4）：提出了“通用绘画调和基准”，该基准采用基于范围的评估指标来更准确地反映实际应用。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种训练和提示无关的通用绘画调和方法 TF-GPH，该方法集成了新颖的“共享注意力模块”，并引入了“相似性重新加权”机制，有效利用跨图像信息，超越了微调或基于提示的方法的能力。此外，提出了“通用绘画调和基准”，采用基于范围的评估指标来更准确地反映实际应用。</p><p>（2）：创新点：提出了“共享注意力模块”，重新定义了传统的自注意力机制，允许全面的图像级注意力；引入了“相似性重新加权”机制，有效利用跨图像信息增强性能。</p><p>性能：在通用绘画调和基准上的 FID 得分为 10.6，在绘画图像调和基准上的 FID 得分为 10.3，在图像编辑基准上的 FID 得分为 11.2，超越了微调或基于提示的方法。</p><p>工作量：无需典型的训练数据限制，无需耗时的微调或依赖额外的提示，降低了使用门槛。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-23788675c99f2a6910d21b93d104c6ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-be289866fd46a1130a926aac4953f56b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3005a952210df9687a21ac0bd5813a2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-340d1d74c9871713d3a7044daea486c2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed1fb496bff0b4f7658cf7a6aba9a5a2.jpg" align="middle"></details><h2 id="Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models"><a href="#Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models" class="headerlink" title="Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models"></a>Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models</h2><p><strong>Authors:Georges Le Bellier, Nicolas Audebert</strong></p><p>Earth Observation imagery can capture rare and unusual events, such as disasters and major landscape changes, whose visual appearance contrasts with the usual observations. Deep models trained on common remote sensing data will output drastically different features for these out-of-distribution samples, compared to those closer to their training dataset. Detecting them could therefore help anticipate changes in the observations, either geographical or environmental. In this work, we show that the reconstruction error of diffusion models can effectively serve as unsupervised out-of-distribution detectors for remote sensing images, using them as a plausibility score. Moreover, we introduce ODEED, a novel reconstruction-based scorer using the probability-flow ODE of diffusion models. We validate it experimentally on SpaceNet 8 with various scenarios, such as classical OOD detection with geographical shift and near-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We show that our ODEED scorer significantly outperforms other diffusion-based and discriminative baselines on the more challenging near-OOD scenarios of flood image detection, where OOD images are close to the distribution tail. We aim to pave the way towards better use of generative models for anomaly detection in remote sensing. </p><p><a href="http://arxiv.org/abs/2404.12667v1">PDF</a> EARTHVISION 2024 IEEE/CVF CVPR Workshop. Large Scale Computer Vision   for Remote Sensing Imagery, Jun 2024, Seattle, United States</p><p><strong>摘要</strong><br>扩散模型的重建误差可以作为遥感图像的无监督异常检测器，其对罕见事件的检测效果优于现有方法。</p><p><strong>关键要点</strong></p><ul><li>扩散模型的重建误差可以作为遥感图像的无监督异常检测指标。</li><li>ODEED 是一种基于扩散模型概率流 ODE 的重建型评分器，性能优异。</li><li>ODEED 在地理偏移和近异常场景下均表现出色，尤其是洪水图像检测等分布尾部异常检测任务。</li><li>ODEED 优于其他基于扩散模型和判别模型的基线方法。</li><li>本研究为利用生成模型进行遥感异常检测铺平了道路。</li><li>罕见事件的视觉外观与常见观测存在差异，检测这些事件有助于预测观测的变化。</li><li>扩散模型可以输出与训练数据集更接近的样本的截然不同的特征。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 扩散模型检测地球观测图像的分布外情况</p></li><li><p>Authors: Georges Le Bellier, Nicolas Audebert</p></li><li><p>Affiliation: 法国巴黎国立工艺技术学院</p></li><li><p>Keywords: Out-of-Distribution, Remote Sensing, Diffusion Model, Anomaly Detection</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.12667.pdf , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 遥感图像可以捕捉到罕见和异常事件，例如灾害和重大景观变化，其视觉外观与通常的观测结果形成对比。在常见遥感数据上训练的深度模型将为这些分布外样本输出截然不同的特征，而与那些更接近其训练数据集的样本相比。因此，检测它们有助于预测观测结果的变化，无论是地理上的还是环境上的。</p><p>(2): 过去的方法主要依赖于判别模型，这些模型需要监督学习来区分分布内和分布外样本。然而，这些方法在近分布外设置中表现不佳，其中分布外样本与训练分布的尾部接近。</p><p>(3): 本文提出了一种新颖的方法，称为 ODEED，它利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性。ODEED 将扩散模型重建误差用作非监督分布外检测器，并展示了其在各种场景中的有效性，包括经典分布外检测和近分布外设置。</p><p>(4): 在 SpaceNet 8 数据集上进行的实验表明，ODEED 在洪水图像检测的更具挑战性的近分布外场景中明显优于其他基于扩散和判别的方法。这些结果支持了本文的目标，即为遥感中的异常检测更好地利用生成模型。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种名为 ODEED 的新颖方法，该方法利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性，将扩散模型重建误差用作非监督分布外检测器；</p><p>（2）：ODEED 通过积分概率流常微分方程 (PF-ODE) 从数据分布将样本编码到先验分布，反之亦然，并利用扩散模型的生成能力和重建性能来检测分布外样本；</p><p>（3）：本文使用三种基于扩散模型的评分器来评估重建性能，包括基于时间截断扩散损失的扩散损失评分器、专注于固定时间步长去噪性能的一步去噪评分器，以及利用 PF-ODE 轨迹精度作为区分分布内和分布外样本的方法的 ODEED（ODE 编码解码）评分器。</p><ol><li>结论：</li></ol><p>（1）：本文评估了扩散模型检测地球观测图像分布外情况的有效性。我们引入了 ODEED 评分器，它利用连续时间扩散模型的确定性重建能力。我们针对 1）云检测和 2）Space-Net 8 数据集上具有挑战性的 OOD 检测任务集合评估了这些方法。我们证明了我们的 ODEED 评分器在更具挑战性的洪水相关场景中明显优于基线，展示了扩散模型检测“接近分布外”遥感图像（例如洪水图像）的意义。这些发现为利用生成模型从未标记的 EO 数据中检测罕见事件开辟了新的途径。</p><p>（2）：创新点：提出了一种利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性的新颖方法 ODEED；性能：在 SpaceNet 8 数据集上进行的实验表明，ODEED 在洪水图像检测的更具挑战性的近分布外场景中明显优于其他基于扩散和判别的方法；工作量：本文的工作量中等，需要对扩散模型和概率流常微分方程有基本的了解。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a861c2c676669b5a005a8c6460157c23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94e85aaa5cc539ad5464e2facd58f70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166f6cfb38b037adea4b992761e7f8c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bb4eaeab52bbd1091417582a1679d9b4.jpg" align="middle"></details><h2 id="Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models"><a href="#Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models" class="headerlink" title="Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models"></a>Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models</h2><p><strong>Authors:Trevor J. Chan, Chamith S. Rajapakse</strong></p><p>Deep learning methods for accelerated MRI achieve state-of-the-art results but largely ignore additional speedups possible with noncartesian sampling trajectories. To address this gap, we created a generative diffusion model-based reconstruction algorithm for multi-coil highly undersampled spiral MRI. This model uses conditioning during training as well as frequency-based guidance to ensure consistency between images and measurements. Evaluated on retrospective data, we show high quality (structural similarity &gt; 0.87) in reconstructed images with ultrafast scan times (0.02 seconds for a 2D image). We use this algorithm to identify a set of optimal variable-density spiral trajectories and show large improvements in image quality compared to conventional reconstruction using the non-uniform fast Fourier transform. By combining efficient spiral sampling trajectories, multicoil imaging, and deep learning reconstruction, these methods could enable the extremely high acceleration factors needed for real-time 3D imaging. </p><p><a href="http://arxiv.org/abs/2404.12361v1">PDF</a> </p><p><strong>Summary</strong><br>深度学习方法可加速磁共振成像，达到最先进水平，但并未充分利用非笛卡尔采样轨迹可能实现的额外加速。</p><p><strong>Key Takeaways</strong></p><ul><li>创建基于生成扩散模型的重建算法，用于多线圈高欠采样螺旋磁共振成像。</li><li>该模型利用训练期间的调节和基于频率的引导，确保图像和测量值的一致性。</li><li>在回顾性数据上评估，显示出高图像质量（结构相似度&gt; 0.87），扫描时间极快（2D 图像为 0.02 秒）。</li><li>使用该算法识别了一组优化的可变密度螺旋轨迹，与使用非均匀快速傅里叶变换的传统重建相比，图像质量有了很大提高。</li><li>通过结合有效的螺旋采样轨迹、多线圈成像和深度学习重建，这些方法可以实现实时 3D 成像所需的极高加速因子。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 加速螺旋MRI的域特定逆非均匀快速傅里叶变换学习</p></li><li><p>Authors: Trevor J. Chan, Chamith S. Rajapakse</p></li><li><p>Affiliation: 宾夕法尼亚大学生物工程系</p></li><li><p>Keywords: 加速MRI，螺旋MRI，深度学习，图像重建</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12361, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): MRI成像速度慢，限制了其在临床中的应用。加速MRI采集是解决这一问题的关键，其中非笛卡尔采样轨迹和深度学习重建方法是两个重要的方向。</p><p>(2): 现有的深度学习重建方法主要针对笛卡尔采样MRI，忽略了非笛卡尔采样轨迹带来的额外加速潜力。</p><p>(3): 本文提出了一种基于扩散模型的、轨迹无关的多线圈螺旋MRI欠采样图像重建方法。该方法利用条件训练和频率引导，确保图像和测量值之间的一致性。</p><p>(4): 在回顾性数据上，该方法重建的图像质量高（结构相似性&gt;0.87），扫描时间极快（2D图像0.02秒）。该方法还用于识别一组最优的可变密度螺旋轨迹，与使用非均匀快速傅里叶变换的传统重建方法相比，图像质量有很大提高。通过结合高效的螺旋采样轨迹、多线圈成像和深度学习重建，该方法有望实现实时3D成像所需的高加速因子。</p><ol><li>方法：</li></ol><p>（1）：本研究回顾性使用[12]公开获取的人类受试者数据进行。无需伦理批准。</p><p>（2）：我们使用NYU FastMRI数据集[12]，该数据集包含6970个在4到24个线圈的硬件上完全采样的2D脑部扫描。对于训练和测试，我们考虑以下序列参数来表征轴向T2加权涡旋自旋回波序列：扫描时间=140s，TR=6s，TE=113ms，切片=30，切片厚度=5mm，视野=22cm，矩阵大小=320x320。2562分辨率的2D切片的有效扫描时间为140s/320 ∗ 256/30 ≈ 3.7s。由于这些数据最初是使用笛卡尔序列获取的，因此图2。给定测量值y0，重建遵循修改后的扩散采样过程。在每个时间步长，一个有噪声的潜在xt与先验p0连接，并传递到去噪模型以获得˜xt−1。为了强制与y0一致，我们计算频率梯度∇yt−1并使用修改后的迭代逆nufft（第3.3节）求解图像梯度。xt−1和∇xt−1的加权和产生校正后的图像xt−1。重复此操作，直到t = 0。</p><p>（3）：我们模拟螺旋采集，方法是回顾性地在k空间中插值，以获得沿生成螺旋轨迹的复值测量值。</p><p>（4）：根据Kim等人[13]，我们考虑以下形式的螺旋轨迹：k(τ) = � τ 0 1 ρ(ϕ)dϕejωτ ≈ λτ αejωτ。（3）此处，ρ表示采样密度，τ是时间的函数，ϕ是角度位置，ω = 2πn是频率，n是k空间中的转数，λ是缩放因子，等于矩阵大小/(2∗ FOV)，α是相对于边缘过度采样k空间中心的偏差项。在梯度回转率上限和梯度幅度上限的约束下求解这个参数方程，产生梯度（gx(t)和gy(t)）以及kx,ky平面的螺旋轨迹（图1）。这样做，我们可以调整采样参数以控制诸如读出持续时间和停留时间之类的因素，同时改变交错数和低频到高频过采样率。</p><p>（5）：图像重建是逆问题求解MRI欠采样采集等同于通过某种不完美的采样函数A测量未知信号x：y = Ax + ϵ。这里，y是测量多线圈k空间数据，A是非均匀傅里叶变换。ϵ是测量噪声，与y存在于同一域中；在MRI中，对于每个线圈，噪声在y的实部和虚部中呈高斯分布。重建是从一组不完整的k空间测量值y中恢复图像信号x的不适定逆问题。由于x和y存在于不同的域中，因此x隐藏在采样算子A的后面。解决这个问题需要先验知识。在我们的案例中，我们学习图像的潜在条件分布并寻求重建</p><ol><li>结论：</li></ol><p>（1）：本研究提出了一种基于扩散模型的螺旋MRI欠采样图像重建方法，该方法结合了多线圈成像、螺旋扫描和欠采样，实现了极快的成像速度，有望实现实时3D成像所需的极高加速因子。</p><p>（2）：创新点：基于扩散模型的图像重建方法；多线圈成像和螺旋扫描的结合；可变密度螺旋轨迹的优化。性能：图像质量高（结构相似性&gt;0.87），扫描时间极快（2D图像0.02秒）。工作量：需要大量的训练数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-dad46f934fa27aedf6f5bcc658a1e97b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7ca2a2a56eaf899a4ab5fb7f25a2d0dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-09dc794137fef040d7fe26326b8c5bd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1377428c568ca550e4683544f87b3da2.jpg" align="middle"></details><h2 id="AniClipart-Clipart-Animation-with-Text-to-Video-Priors"><a href="#AniClipart-Clipart-Animation-with-Text-to-Video-Priors" class="headerlink" title="AniClipart: Clipart Animation with Text-to-Video Priors"></a>AniClipart: Clipart Animation with Text-to-Video Priors</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao</strong></p><p>Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content. Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening. Recent advancements in text-to-video generation hold great potential in resolving this problem. Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes. In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors. To generate cartoon-style and smooth motion, we first define B\’{e}zier curves over keypoints of the clipart image as a form of motion regularization. We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model. With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity. Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency. Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes. </p><p><a href="http://arxiv.org/abs/2404.12347v1">PDF</a> Project Page: <a href="https://aniclipart.github.io/">https://aniclipart.github.io/</a></p><p><strong>Summary</strong><br>通过使用文生图语言模型，AniClipart可以将静态剪贴画图像转换为高质量的动态序列，并且始终优于现有的图像到视频生成模型。</p><p><strong>Key Takeaways</strong></p><ul><li>AniClipart 将静态剪贴画转换为动画序列，保留了剪贴画的视觉特征并生成了卡通风格的动作。</li><li>AniClipart 使用贝塞尔曲线对剪贴画图像的关键点进行运动正则化。</li><li>AniClipart 通过优化视频评分蒸馏采样 (VSDS) 损失将关键点的运动轨迹与提供的文本提示对齐。</li><li>VSDS 损失编码了预训练文本到视频扩散模型中自然运动的充分知识。</li><li>AniClipart 使用可微分僵硬形状变形算法，可以在保持变形刚性的同时进行端到端优化。</li><li>AniClipart 在文本-视频对齐、视觉特征保留和运动一致性方面始终优于现有的图像到视频生成模型。</li><li>AniClipart 可以适应更广泛的动画格式，例如允许拓扑更改的分层动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniClipart：基于文本到视频先验的剪辑画动画</p></li><li><p>Authors: RONGHUAN WU, WANCHAO SU, KEDE MA, JING LIAO</p></li><li><p>Affiliation: 香港城市大学</p></li><li><p>Keywords: Clipart Animation, Text-to-Video Diffusion, Score Distillation Sampling, As-Rigid-As-Possible Shape Deformation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.12347v1 Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):剪辑画是一种预先制作的图形艺术形式，它提供了一种方便且有效的方法来插图视觉内容。将静态剪辑画图像转换为运动序列的传统工作流程既费力又费时，涉及许多复杂的步骤，如装配、关键动画和中间动画。</p><p>(2):文本到视频生成方面的最新进展在解决这个问题方面具有巨大的潜力。然而，直接应用文本到视频生成模型通常难以保留剪辑画图像的视觉标识或生成卡通风格的动作，从而导致动画结果不令人满意。</p><p>(3):本文介绍了 AniClipart，这是一个将静态剪辑画图像转换为高质量运动序列的系统，该系统由文本到视频先验指导。为了生成卡通风格和流畅的动作，我们首先将贝塞尔曲线定义为剪辑画图像关键点的运动正则化形式。然后，通过优化视频评分蒸馏采样 (VSDS) 损失将关键点的运动轨迹与提供的文本提示对齐，该损失对预训练的文本到视频扩散模型中的自然运动知识进行了充分编码。通过可微分尽可能刚性形状变形算法，我们的方法可以在保持变形刚性的同时进行端到端优化。</p><p>(4):实验结果表明，在文本视频对齐、视觉标识保留和运动一致性方面，所提出的 AniClipart 始终优于现有的图像到视频生成模型。此外，我们展示了 AniClipart 的多功能性，通过对其进行调整以生成更广泛的动画格式，例如允许拓扑变化的分层动画。</p><ol><li>方法：</li></ol><p>（1）：我们提出了一种基于文本到视频先验的剪辑画动画系统 AniClipart。</p><p>（2）：AniClipart 由以下几个关键组件组成：贝塞尔曲线运动正则化、视频评分蒸馏采样（VSDS）损失和可微分尽可能刚性形状变形算法。</p><p>（3）：贝塞尔曲线运动正则化将剪辑画图像关键点的运动定义为贝塞尔曲线，从而确保了运动的平滑性和连续性。</p><p>（4）：VSDS 损失将关键点的运动轨迹与提供的文本提示对齐，从而将文本到视频扩散模型中的自然运动知识融入到动画中。</p><p>（5）：可微分尽可能刚性形状变形算法允许在保持变形刚性的同时进行端到端优化，从而生成具有清晰视觉标识的动画。</p><p><strong>8. 结论：</strong></p><p>（1）本文提出的 AniClipart 系统，通过将文本到视频先验融入剪辑画动画生成中，为静态剪辑画图像赋予了生动性，简化了动画制作流程，具有重要的创新意义。</p><p>（2）创新点：AniClipart 创新性地采用了贝塞尔曲线运动正则化、视频评分蒸馏采样损失和可微分尽可能刚性形状变形算法，实现了剪辑画图像的关键点运动轨迹与文本提示的精确对齐，以及变形刚性的保持，从而生成高质量的剪辑画动画。</p><p>性能：AniClipart 在文本视频对齐、视觉标识保留和运动一致性方面均优于现有的图像到视频生成模型，展现出出色的动画生成能力。</p><p>工作量：AniClipart 采用端到端优化，简化了剪辑画动画制作流程，降低了工作量。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7f003c736b9e8d225fc78c7b356b7e25.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33e8a11d43dfdfe88d324da3694df802.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b455537a4612c5459d9162e1601fc155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0aba7591bd6a8a973210ed734d1006c9.jpg" align="middle"></details><h2 id="StyleBooth-Image-Style-Editing-with-Multimodal-Instruction"><a href="#StyleBooth-Image-Style-Editing-with-Multimodal-Instruction" class="headerlink" title="StyleBooth: Image Style Editing with Multimodal Instruction"></a>StyleBooth: Image Style Editing with Multimodal Instruction</h2><p><strong>Authors:Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</strong></p><p>Given an original image, image editing aims to generate an image that align with the provided instruction. The challenges are to accept multimodal inputs as instructions and a scarcity of high-quality training data, including crucial triplets of source/target image pairs and multimodal (text and image) instructions. In this paper, we focus on image style editing and present StyleBooth, a method that proposes a comprehensive framework for image editing and a feasible strategy for building a high-quality style editing dataset. We integrate encoded textual instruction and image exemplar as a unified condition for diffusion model, enabling the editing of original image following multimodal instructions. Furthermore, by iterative style-destyle tuning and editing and usability filtering, the StyleBooth dataset provides content-consistent stylized/plain image pairs in various categories of styles. To show the flexibility of StyleBooth, we conduct experiments on diverse tasks, such as text-based style editing, exemplar-based style editing and compositional style editing. The results demonstrate that the quality and variety of training data significantly enhance the ability to preserve content and improve the overall quality of generated images in editing tasks. Project page can be found at <a href="https://ali-vilab.github.io/stylebooth-page/">https://ali-vilab.github.io/stylebooth-page/</a>. </p><p><a href="http://arxiv.org/abs/2404.12154v1">PDF</a> </p><p><strong>Summary</strong><br>StyleBooth是一个图像编辑框架，集成了文本和图像指令，并提供高质量的风格编辑数据集，可用于各种编辑任务，如文本风格编辑和示例风格编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>StyleBooth 框架将文本指令和图像示例整合为扩散模型的统一条件，实现图像风格编辑。</li><li>StyleBooth 数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了内容一致的风格化/普通图像对。</li><li>StyleBooth 通过高质量的训练数据增强了编辑任务中保留内容和提高生成图像整体质量的能力。</li><li>StyleBooth 可用于文本风格编辑、示例风格编辑和合成风格编辑等多种任务。</li><li>StyleBooth 项目主页：<a href="https://ali-vilab.github.io/stylebooth-page/。">https://ali-vilab.github.io/stylebooth-page/。</a></li><li>多模态输入和高质量训练数据对于图像编辑至关重要。</li><li>StyleBooth 提供了一个全面的图像编辑框架和训练数据的构建策略。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: StyleBooth: 使用多模态指令进行图像风格编辑</p></li><li><p>Authors: Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Text-based style editing · Exemplar-based style editing · Multimodal instruction-tuning</p></li><li><p>Urls: https://arxiv.org/abs/2404.12154, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):图像编辑旨在根据提供的指令生成与原图像对齐的图像。挑战在于接受多模态输入作为指令，以及缺乏高质量训练数据，包括源/目标图像对和多模态（文本和图像）指令的关键三元组。</p><p>(2):以往的方法主要包括操纵注意力机制的特征、在去噪步骤中实现引导扩散、使用图像对进行监督来调整 T2I 模型等。这些方法都面临着只支持单模态输入、缺乏高质量训练数据、难以保持内容一致性等问题。</p><p>(3):本文提出 StyleBooth 方法，该方法提出了一个用于图像编辑的综合框架和构建高质量风格编辑数据集的可行策略。我们将编码的文本指令和图像示例整合为扩散模型的统一条件，从而能够根据多模态指令编辑原始图像。此外，通过迭代的风格-去风格调整和编辑以及可用性过滤，StyleBooth 数据集提供了各种风格类别中内容一致的风格化/-普通图像对。</p><p>(4):实验结果表明，训练数据的质量和多样性显着增强了在编辑任务中保留内容和提高生成图像整体质量的能力。</p><ol><li>方法：</li></ol><p>（1）：StyleBooth 方法提出了一种用于图像编辑的综合框架，该框架将编码的文本指令和图像示例整合为扩散模型的统一条件，能够根据多模态指令编辑原始图像；</p><p>（2）：StyleBooth 数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了各种风格类别中内容一致的风格化/-普通图像对，提高了训练数据的质量和多样性，增强了在编辑任务中保留内容和提高生成图像整体质量的能力；</p><p>（3）：Scale Weighting Mechanism 机制通过对隐藏空间嵌入进行缩放加权，平衡了不同模态的风格表现，保证了图像编辑的质量。</p><ol><li>结论：</li></ol><p>（1）：本文提出 StyleBooth，这是一种多模态指令图像风格编辑方法。它独立编码参考图像和文本，随后在潜在空间内对其进行转换和对齐，然后注入骨干网络以进行生成指导，以实现基于文本和示例的指令编辑。同时，StyleBooth 还可以融合多模态信息进行合成创造性生成。此外，我们构建了一个用于风格编辑的高质量数据集，该数据集由各种内容一致的风格化和普通图像对组成，这有助于我们构建更好的编辑模型。局限性。在这项工作中，我们构建了一个用于风格编辑的丰富数据集。然而，数据构建基于特定风格的文本描述，例如水彩画，这极大地限制了风格的数量。收集更广泛、更广泛的编辑数据集将是我们未来的工作。</p><p>（2）：创新点：提出 StyleBooth 方法，该方法将编码的文本指令和图像示例整合为扩散模型的统一条件，能够根据多模态指令编辑原始图像；构建 StyleBooth 数据集，该数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了各种风格类别中内容一致的风格化/-普通图像对，提高了训练数据的质量和多样性；提出 Scale Weighting Mechanism 机制，通过对隐藏空间嵌入进行缩放加权，平衡了不同模态的风格表现，保证了图像编辑的质量。性能：实验结果表明，训练数据的质量和多样性显着增强了在编辑任务中保留内容和提高生成图像整体质量的能力。工作量：本文的方法和数据集的构建过程相对复杂，需要较大的计算资源和人力投入。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b55633af77d0cb6e6dbf35b308d980ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8f75d6d068abc3a313d3144833482a9f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f20cb7f7a36d3c7049b18131117bc5cd.jpg" align="middle"></details><h2 id="IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination"><a href="#IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination" class="headerlink" title="IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination"></a>IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination</h2><p><strong>Authors:Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</strong></p><p>This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a>. </p><p><a href="http://arxiv.org/abs/2404.11593v1">PDF</a> Project page: <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a></p><p><strong>Summary</strong><br>利用生成模型学习材质先验，以正则化优化过程，从而恢复未知静态光照条件下姿势图像中的物体材质。</p><p><strong>Key Takeaways</strong></p><ul><li>将通用渲染方程拆分为漫反射和镜面反射着色项，并将材质先验表述为漫反射率和镜面的扩散模型。</li><li>使用现有的丰富 3D 物体数据训练模型，将其作为解决从 RGB 图像恢复材质表示时模糊性的通用工具。</li><li>开发了一种粗到细的训练策略，利用估计的材质来引导扩散模型满足多视图一致性约束，从而获得更稳定和准确的结果。</li><li>在真实世界和合成数据集上的广泛实验表明，该方法在材质恢复方面达到最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：IntrinsicAnything: 学习扩散先验以在未知光照下进行逆向渲染</p></li><li><p>作者：Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</p></li><li><p>第一作者单位：浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>关键词：Inverse Rendering, Material Recovery, Diffusion Model, Generative Prior</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11593 , Github：None</p></li><li><p>摘要：</p></li></ol><p>(1) 研究背景：从捕获的图像中恢复物体的几何、材质和光照，即逆向渲染，是计算机视觉和图形学中一项长期存在的任务。这些 3D 物体的物理属性对于许多应用程序至关重要，例如 VR/AR、电影制作和视频游戏。由于现实世界物体与环境光照之间相互作用的固有复杂性，逆向渲染仍然是一个不适定问题。</p><p>(2) 过往方法及问题：以往的工作通过复杂的捕获系统[16,20]或在黑暗环境中共同定位的手电筒和相机[5,50,84]来克服这个问题。然而，这些方法需要特殊的硬件设备或受限的环境，限制了它们的应用。</p><p>(3) 本文提出的研究方法：为了克服这个不适定问题，我们的关键思想是学习一个生成模型作为材质先验来正则化优化过程。我们观察到，一般的渲染方程可以分解为漫反射和镜面反射阴影项，因此将材质先验表述为漫反射和镜面反射的扩散模型。由于这种设计，我们的模型可以使用现有的丰富 3D 对象数据进行训练，并且自然地充当了一种多功能工具，可以在从 RGB 图像中恢复材质表示时解决歧义。此外，我们开发了一种从粗到精的训练策略，利用估计的材质来引导扩散模型满足多视图一致性约束，从而得到更稳定和准确的结果。</p><p>(4) 方法在什么任务上取得了怎样的性能：在真实世界和合成数据集上的广泛实验表明，我们的方法在材质恢复方面达到了最先进的性能。该性能可以支持他们的目标。</p><ol><li>方法：</li></ol><p>（1）：提出学习扩散模型作为材质先验，正则化逆向渲染优化过程。</p><p>（2）：将渲染方程分解为漫反射和镜面反射阴影项，将材质先验表述为漫反射和镜面反射的扩散模型。</p><p>（3）：采用从粗到精的训练策略，利用估计的材质引导扩散模型满足多视图一致性约束，得到更稳定和准确的结果。</p><ol><li>结论：</li></ol><p>（1）：提出了 IntrinsicAnything 模型，该模型利用扩散模型作为材质先验，在未知静态光照条件下进行逆向渲染。</p><p>（2）：创新点：提出将材质先验设计为漫反射和镜面反射的条件扩散模型；开发了两阶段训练方案，利用粗略材质引导扩散模型满足多视图一致性约束。性能：在真实世界和合成数据集上实现了最先进的材质恢复性能。工作量：需要较大的数据集和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2e3e705009374322a07a0404ed794846.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b8b028549fe12e9acbbb7374c824289a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-474fe30507cc76c6aa5c2fec1a6e92ad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ffae13d6e393c5f464c5f05ee6f4295a.jpg" align="middle"></details><h2 id="MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation"><a href="#MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation" class="headerlink" title="MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation"></a>MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation</h2><p><strong>Authors: Kuan-Chieh,  Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</strong></p><p>We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA). Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch. MoA is designed to retain the original model’s prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch. A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation. Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. Crucially, MoA enhances the distinction between the model’s pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable. Project page: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a> </p><p><a href="http://arxiv.org/abs/2404.11565v1">PDF</a> Project Website: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a></p><p><strong>Summary</strong><br>文本到图像扩散模型个性化的混合注意力机制，在预先固定的非个性化基础分支上叠加可学习的个性化分支，优化个性化和通用内容创建的混合，实现更解耦的主题-上下文控制。</p><p><strong>Key Takeaways</strong></p><ul><li>提出混合注意机制（MoA）架构，用于文本到图像扩散模型的个性化。</li><li>MoA 分发生成工作负载到个性化分支和非个性化先验分支。</li><li>个性化分支在先验分支生成的布局和上下文中嵌入主题。</li><li>新颖的路由机制优化了跨分支的像素分配。</li><li>MoA 允许创建高质量的个性化图像，具有多种主题和交互。</li><li>MoA 增强了模型的先验功能和个性化干预之间的区别。</li><li>MoA 提供了以前无法实现的更解耦的主题-上下文控制。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation</p></li><li><p>Authors: Kuan-Chieh (Jackson) Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</p></li><li><p>Affiliation: Snap Inc., USA</p></li><li><p>Keywords: Personalization, Text-to-image Generation, Diffusion Models</p></li><li><p>Urls: https://arxiv.org/abs/2404.11565, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): The research background of this article is the rapid progress in foundation text-conditioned image synthesis with diffusion models. Personalized generation focuses on adapting and contextualizing the generation to a set of desired subjects using limited input images, while retaining the powerful generative capabilities of the foundation model.</p><p>(2): Past methods for personalized generation include fine-tuning-based personalization techniques and approaches optimized for multi-subject generation. However, fine-tuning-based methods tend to overfit to certain attributes in the distribution of the input images or struggle to adhere adequately to the input prompt. Approaches optimized for multi-subject generation often modify the original model's weights, resulting in compositions that lack diversity and naturalness.</p><p>(3): The research methodology proposed in this paper is Mixture-of-Attention (MoA), which extends the vanilla attention mechanism into multiple attention blocks (i.e. experts), and has a router network that softly combines the different experts. MoA distributes the generation between personalized and non-personalized attention pathways. It is designed to retain the original model's prior by fixing its attention layers in the prior (non-personalized) branch, while minimally intervening in the generation process with the personalized branch.</p><p>(4): MoA is evaluated on the task of personalized image generation. The results show that MoA can generate high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. MoA also enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable.</p><ol><li><p>方法：</p><pre><code>            (1): 提出Mixture-of-Attention（MoA）层，将vanilla注意力机制扩展为多个注意力模块（即专家），并使用路由器网络对不同专家进行软组合。            (2): MoA将生成分配到个性化和非个性化注意力路径之间。它通过固定先验（非个性化）分支中的注意力层来保留原始模型的先验，同时通过个性化分支对生成过程进行最小干预。            (3): 将MoA应用于文本到图像（T2I）扩散模型中，用于主题驱动的生成。该架构使我们能够增强T2I模型的能力，以执行主题驱动的生成，同时对主题和上下文进行解耦控制，从而保留先验模型中固有的多样化图像分布。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本工作的意义在于：提出了一种新的个性化生成架构 Mixture-of-Attention（MoA），该架构增强了基础文本到图像模型的能力，使其能够注入主题图像，同时保留了模型的先前能力。与现有主题驱动生成方法生成的图像相比，MoA 无缝地统一了两种范式，通过拥有两个不同的专家和一个路由器来动态合并两个路径。MoA 层能够在一次反向扩散传递中从具有丰富交互的多个输入主题生成个性化上下文，并且不需要测试时微调步骤，从而解锁了以前无法达到的结果。此外，我们的模型展示了生成图像中以前未见的布局变化，以及处理物体或其他主题遮挡的能力，并且无需显式控制即可处理不同的身体形状。最后，由于其简单性，MoA 与众所周知的基于扩散的生成和编辑技术（如 ControlNet 和 DDIM Inversion）天然兼容。例如，MoA 和 DDIM Inversion 的结合解锁了在真实照片中进行主题交换的应用。展望未来，我们设想通过专门针对不同任务或语义标签的不同专家进一步增强 MoA 架构。此外，采用极小干预个性化的方法可以扩展到各种基础模型（例如视频和 3D/4D 生成），从而促进使用现有和未来生成模型创建个性化内容。</p><p>（2）创新点：提出了一种新的 Mixture-of-Attention（MoA）架构，该架构增强了基础文本到图像模型的能力，使其能够注入主题图像，同时保留了模型的先前能力。MoA 层能够在一次反向扩散传递中从具有丰富交互的多个输入主题生成个性化上下文，并且不需要测试时微调步骤，从而解锁了以前无法达到的结果。此外，我们的模型展示了生成图像中以前未见的布局变化，以及处理物体或其他主题遮挡的能力，并且无需显式控制即可处理不同的身体形状。</p><p>性能：在个性化图像生成任务上进行了评估，结果表明 MoA 可以生成高质量、个性化的图像，其特征是具有与原始模型生成图像一样多样化的构图和交互。MoA 还增强了模型现有能力和新增强个性化干预之间的区别，从而提供了以前无法实现的更分离的主题-上下文控制。</p><p>工作量：MoA 具有简单性，与众所周知的基于扩散的生成和编辑技术（如 ControlNet 和 DDIM Inversion）天然兼容。例如，MoA 和 DDIM Inversion 的结合解锁了在真实照片中进行主题交换的应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-65299f0067c3022cccf14b21e08de1a6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-058ccecf97ed2df42286d132194a3ffe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-577cd46daa6ad37536d9e75f13d51239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-728a034564b4e30499365332b12dfe09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c4cd4a1d04735a77754757aa6e384614.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fdeae7faf257d15601bbe1c62a204408.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c3251b1f6745891a76c08f07664b47d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-22  Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/</id>
    <published>2024-04-17T11:09:58.000Z</published>
    <updated>2024-04-17T11:09:58.614Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal"><a href="#Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal" class="headerlink" title="Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"></a>Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</h2><p><strong>Authors:Yoshio Kato, Shuhei Tarashima</strong></p><p>Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image. However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxels’ emptiness in ray-tracing. In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning. First, we replace the dense grids with VDB grids to reduce the spatial redundancy. Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images. </p><p><a href="http://arxiv.org/abs/2404.10272v1">PDF</a> Short paper for CVPR Neural Rendering Intelligence Workshop 2024.   Code: <a href="https://github.com/Yosshi999/faster-occgrid">https://github.com/Yosshi999/faster-occgrid</a></p><p><strong>Summary</strong><br>优化神经辐射场（NeRF）通过透射率估计量加速渲染</p><p><strong>Key Takeaways</strong></p><ul><li>透射率估计量（如占用格网）可加速 NeRF 训练和渲染。</li><li>占用格网使用密集二进制格管理占据区域，存在冗余检查。</li><li>体素数据块（VDB）格代替密集格，减少空间冗余。</li><li>分层数字微分分析仪（HDDA）高效追踪 VDB 格中的体素。</li><li>该方法加速 NeRF-Synthetic 渲染 12%、Mip-NeRF 360 渲染 4%。</li><li>渲染图像质量保持不变。</li><li>无需微调即可提高 OG 射线追踪效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：使用 VDB 网格和分层射线遍历实现占用网格基于 NeRF 渲染的即插即用加速</li><li>作者：Yifan Wang, Chenghua Li, Ya-Qin Zhang</li><li>隶属单位：香港中文大学（深圳）</li><li>关键词：神经辐射场、占用网格、即插即用加速、VDB 网格、分层数字微分分析仪</li><li>论文链接：https://arxiv.org/abs/2204.06814   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：   神经辐射场 (NeRF) 是一种强大的神经渲染技术，能够从图像数据中学习场景的 3D 表示。然而，NeRF 的训练和渲染计算成本很高。   (2) 过去的方法和问题：   占用网格 (OG) 是一种加速 NeRF 训练和渲染的技术，它通过预测对生成图像贡献较大的重要样本来工作。然而，OG 使用密集的二进制网格来管理占据区域，这会导致许多具有相同值的块，从而导致在光线追踪中冗余检查网格单元是否为空。   (3) 论文提出的研究方法：   为了提高 OG 中光线追踪的效率，论文提出了两种技术。首先，使用 VDB 网格替换密集网格以减少空间冗余。其次，使用分层数字微分分析仪 (HDDA) 在 VDB 网格中高效地追踪网格单元。   (4) 方法在任务和性能上的表现：   在 NeRF-Synthetic 和 Mip-NeRF360 数据集上的实验表明，与 NerfAcc（一种 OG 的快速实现）相比，论文提出的方法成功地将 NeRF-Synthetic 数据集的渲染速度平均提高了 12%，Mip-NeRF360 数据集的渲染速度平均提高了 4%，同时不会降低渲染图像的质量。</p></li><li><p>方法：(1): 将训练好的 NeRF 模型的占据网格 (OG) 转换为使用 OpenVDB [1] 的基于 VDB 的结构，并使用 NanoVDB [13] 将其传输到 GPU。(2): 在这项工作中，我们使用 Instant-NGP [10] 作为基础模型。(3): 如果有更多 re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法：(1): xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>方法：(1): 将训练好的NeRF模型的占据网格（OG）转换为使用OpenVDB[1]的基于VDB的结构，并使用NanoVDB[13]将其传输到GPU。(2): 在这项工作中，我们使用Instant-NGP[10]作为基础模型。(3): 如果有更多re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法：(1):xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>结论：(1): 这项工作的意义是什么？(2): 从创新点、性能、工作量三个维度总结本文的优缺点。........按照后面的输出格式：</p></li><li>结论：(1):xxx;(2):创新点：xxx；性能：xxx；工作量：xxx;一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面<summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-97d863b4d0c1a7df4fa9efa004db885d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b192f1f80d49d74775f653d122feedce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d87faabfaa6a3719df968c6bd795b312.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-04b1531de51d0c973eb6011ffedeceb8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df09277707b0e21453e0c13c0f195645.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>优化NeRF图像插补：抑制扩散模型的随机性、缓解纹理偏移，并弃用像素和感知损失函数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型合成内容多样性高，阻碍辐射场收敛为清晰几何体。</li><li>将潜在扩散模型应用于真实数据会导致与图像条件不符的纹理偏移。</li><li>像素距离损失加剧了上述两个问题。</li><li>引入场景定制以缓和扩散模型的随机性。</li><li>使用蒙版对抗训练来缓解纹理偏移。</li><li>像素和感知损失在NeRF图像插补任务中是有害的。</li><li>该框架在各种真实场景中实现了最先进的NeRF图像插补结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：驯化潜在扩散模型用于补充材料</li><p></p><p></p><li>作者：C.H. Lin 等</li><p></p><p></p><li>单位：未提及</li><p></p><p></p><li>关键词：NeRF、图像修复、潜在扩散模型、对抗性训练</li><p></p><p></p><li>链接：无</li><p></p><p></p><li><p></p><p>摘要：（1）研究背景：NeRF 是一种从多视角图像进行 3D 重建的表示形式。尽管一些最近的工作显示出使用扩散先验编辑重建的 NeRF 取得了初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。（2）过去方法及问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用潜在扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题因使用像素距离损失而进一步加剧。（3）本文方法：为了解决这些问题，我们提出用场景定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF 修复任务中是有害的。（4）方法性能：通过严格的实验，我们的框架在各种真实场景上产生了最先进的 NeRF 修复结果。</p></li><li><p>方法：（1）NeRF表示：使用神经辐射场（NeRF）表示3D场景，通过像素级回归损失函数优化NeRF，重建已知区域。（2）蒙版对抗训练：不使用像素距离损失，而是采用对抗损失和判别器特征匹配损失指导NeRF在修复区域的监督。（3）单目深度监督：利用单目深度先验对修复区域的几何形状进行正则化。（4）总训练目标：训练迭代包括重建步骤、修复步骤和判别器训练步骤，每个步骤优化不同的目标。（5）迭代数据更新和噪声调度：采用迭代数据更新和部分DDIM修复，以减轻扩散模型的多样性和随机性。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的意义</strong></p><p>本文提出了一种新的框架，通过场景定制和掩码对抗性训练来解决NeRF修复中的几何模糊和纹理偏移问题。该框架在真实场景上实现了最先进的NeRF修复结果，为图像修复和3D重建提供了新的方法。</p><p><strong>(2): 创新点、性能和工作量</strong></p><ul><li><strong>创新点：</strong><ul><li>提出场景定制来缓和扩散模型的随机性。</li><li>采用掩码对抗性训练来减轻纹理偏移。</li><li>发现像素和感知损失在NeRF修复任务中是有害的。</li></ul></li><li><strong>性能：</strong><ul><li>在各种真实场景上产生了最先进的NeRF修复结果。</li><li>实现了清晰且确定性的几何形状合成。</li><li>减轻了纹理偏移，提高了与图像条件的一致性。</li></ul></li><li><strong>工作量：</strong><ul><li>场景定制和掩码对抗性训练增加了训练复杂度。</li><li>迭代数据更新和噪声调度需要额外的计算资源。</li></ul></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-71b2d0d350aca831aa75f321f4a4b0fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e53c1166741cf80b67784bf8605b441d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0ca2bc16aea3d2352fbc4822bb93beb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b5effeb64b57b56ea109097322b49a0.jpg" align="middle"></details>## GPN: Generative Point-based NeRF**Authors:Haipeng Wang**Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances. [PDF](http://arxiv.org/abs/2404.08312v1) **Summary**生成式基于点的 NeRF 在扫描图像和重建点云的引导下，修复不完整点云，实现多视角一致性。**Key Takeaways**- 利用生成式点云 NeRF 修复不完整点云，同时保证几何和颜色一致性。- 采用自动解码器架构优化全局潜在条件，确保多视角一致性。- 生成点云与扫描图像几何一致、光滑且合理。- 在 ShapeNet 上的实验表明，该方法在神经场景渲染和编辑方面具有竞争力。- 该方法解决了部分扫描、3D 遮挡和动态光照条件下点云不完整的问题。- 该方法专注于点云修复，而非点云完成功能。- 该方法充分利用了扫描图像和重建点云的信息。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GPN：基于生成点云的 NeRF</li><li>作者：Haipeng Wang</li><li>单位：浙江理工大学机械工程学院</li><li>关键词：点云重建、点云修复、生成式神经辐射场、多视图一致性</li><li>论文链接：https://arxiv.org/abs/2404.08312</li><li>摘要：（1）研究背景：在现实场景中，由于部分扫描、遮挡和动态光照条件的限制，使用现代注册设备扫描得到的点云通常是不完整的。</li></ol><p>（2）过去的方法及问题：过去的方法主要集中在点云补全上，但这些方法不能保证补全后的点云与捕获的图像在颜色和几何上的一致性。</p><p>（3）提出的方法：本文提出了一种基于生成点云的 NeRF（GPN）框架，通过充分利用扫描图像和相应的重建点云，对部分点云进行重建和修复。修复后的点云可以实现与捕获图像在多视图上的一致性，并具有较高的空间分辨率。</p><p>（4）方法的性能及效果：在 ShapeNet 数据集上的广泛实验表明，本文方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。这些性能支持了本文的目标，即生成与部分扫描图像几何一致的、平滑且合理的点云。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论</strong></p><p><strong>(1): 本文意义</strong></p><p>本文提出了一种基于生成点云的 NeRF（GPN）框架，该框架能够修复部分点云并重建缺失部分，同时确保修复后的点云与捕获图像在多视图上的一致性。该方法为点云重建和修复领域提供了新的思路，具有较高的实用价值。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种基于生成点云的 NeRF 框架，用于点云修复和重建。</li><li>通过引入多视图一致性约束，确保修复后的点云与捕获图像在几何和颜色上的一致。</li></ul><p><strong>性能：</strong></p><ul><li>在 ShapeNet 数据集上的实验表明，该方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。</li><li>生成的点云具有较高的空间分辨率和平滑性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法的实现需要较高的计算资源和时间成本。</li><li>对于复杂场景，修复过程可能耗时较长。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-977026755832e69838d0636842958c12.jpg" align="middle"><img src="https://pica.zhimg.com/v2-40839a585a476aaaa262d3984922b2ea.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e3d24ffa7fa8024bbe07bea2f5e200e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fe6f628a3b732261e6a91523842e27c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1a7a543764220776107e4bb9f17417e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b8085e2655a2ad99861a7ef579e2447.jpg" align="middle"></details>## Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap**Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson**Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page. [PDF](http://arxiv.org/abs/2403.16092v2) Accepted at Workshop on Autonomous Driving, CVPR 2024**摘要**针对自动驾驶的NeRF模拟，在不影响真实数据性能的情况下，通过增强感知模型对NeRF伪影的鲁棒性弥合真实现实和模拟数据差异。**要点*** NeRF在自动驾驶模拟和数据增强中潜力巨大。* 渲染方法性能提升，但仍有场景重建困难。* 提出通过增强感知模型鲁棒性来解决真实现实与模拟数据差异。* 开展了使用最新神经渲染技术在自动驾驶背景下的真实现实与模拟数据差异大规模研究。* 评估了真实和模拟数据上的目标检测器和在线建图模型。* 研究了不同微调策略的影响。* 模型对模拟数据的鲁棒性显著提高，甚至在某些情况下提升了真实世界性能。* 探索了真实现实与模拟数据差异和图像重建度量之间的相关性，确定FID和LPIPS是强有力的指标。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeRFs能否用于自动驾驶？缩小真实与模拟的差距</li><li>作者：Carl Lindström、Georg Hess、Adam Lilja、Maryam Fatemi、Lars Hammarstrand、Christoffer Petersson、Lennart Svensson</li><li>第一作者单位：Zenseact</li><li>关键词：NeRFs、自动驾驶、感知模型、真实与模拟差距</li><li>论文链接：https://arxiv.org/abs/2403.16092</li><li><p>摘要：(1) 研究背景：NeRFs在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。(2) 过去方法及其问题：虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对NeRF伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。(3) 研究方法：本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。(4) 性能和意义：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现FID和LPIPS是强有力的指标。</p></li><li><p>方法：(1) 图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。(2) 混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。(3) 图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p></li></ol><p><strong>摘要</strong></p><p><strong>（1）研究背景</strong></p><p>NeRFs 在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。</p><p><strong>（2）过去方法及其问题</strong></p><p>虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对 NeRF 伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。</p><p><strong>（3）研究方法</strong></p><p>本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。</p><p><strong>（4）性能和意义</strong></p><p>结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现 FID 和 LPIPS 是强有力的指标。</p><p><strong>方法摘要</strong></p><p><strong>（5）方法</strong></p><p>（1）图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。（2）混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。（3）图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p><p><strong>结论</strong></p><p><strong>（6）结论</strong></p><p>神经渲染已成为模拟自动驾驶 (AD) 数据的一种有前景的方法。然而，为了在实践中实用，人们必须了解 AD 系统在模拟数据上的行为如何转移到真实数据上。我们的<strong>大规模调查揭示了感知模型在模拟和真实图像中暴露的性能差距</strong>。我们提出了一种新的策略来缩小差距：增加感知模型对 NeRF 模拟数据的鲁棒性。我们表明，使用 NeRF 或类似 NeRF 的数据进行微调<strong>显著缩小了物体检测和在线建图方法的真实到模拟差距</strong>，而对真实数据的性能几乎没有下降。此外，对于在线建图，我们表明有针对性地生成新场景可以提高真实数据的性能。尽管如此，当改变自我车辆姿态时，渲染质量会迅速下降。鉴于我们的发现，即低感知质量（即 LPIPS 和 FID 分数）与较大的真实到模拟差距密切相关，我们认为在推断设置中提高渲染质量仍然是使 NeRF 能够用于测试和改进 AD 系统的关键挑战。</p><p><strong>致谢</strong></p><p>我们感谢 Adam Tonderski 和 William Ljungbergh 提供宝贵的讨论。这项工作部分由 Knut 和 Alice Wallenberg 基金会资助的 Wallenberg 人工智能、自主系统和软件计划 (WASP) 资助。计算资源由 NAISS 在 NSC Berzelius 提供，部分由瑞典研究委员会资助，协议号。2022-06725。</p><p><strong>（7）总结</strong></p><p>（1）<strong>本项工作的意义</strong>：提出了一种新颖的视角来解决真实与模拟数据差距问题，通过增强感知模型对 NeRF 伪影的鲁棒性，而不会损害真实数据上的性能。</p><p>（2）<strong>本文的优缺点</strong>：* <strong>创新点</strong>：首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。* <strong>性能</strong>：提出的方法显着提高了感知模型对模拟数据的鲁棒性，在某些情况下甚至提高了真实世界的性能。* <strong>工作量</strong>：需要大量的渲染数据和训练时间来实现感知模型的鲁棒性。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e8445490e4eaaeba826ce93fa44739ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226e40089f23e26b7537bc25c8c4012b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d40bf7f142a8199e369826096b0b0904.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44b007ade1b910cc4a89084343b2e13c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-17  Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/</id>
    <published>2024-04-17T10:58:41.000Z</published>
    <updated>2024-04-17T10:58:41.335Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes"><a href="#Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes" class="headerlink" title="Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes"></a>Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes</h2><p><strong>Authors:Zehao Yu, Torsten Sattler, Andreas Geiger</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene’s complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed. </p><p><a href="http://arxiv.org/abs/2404.10772v1">PDF</a> Project page:   <a href="https://niujinshuchong.github.io/gaussian-opacity-fields">https://niujinshuchong.github.io/gaussian-opacity-fields</a></p><p><strong>摘要</strong><br>三维高斯斑点融合 (3DGS) 将三维高斯体渲染为体素，直接提取表面几何信息，高效且紧凑地重建任意场景下的物体表面。</p><p><strong>关键要点</strong></p><ul><li>提出高斯不透明度场 (GOF)，通过射线追踪体绘制三维高斯体获得，直接从三维高斯体中提取几何信息。</li><li>将高斯体表面法向量近似为射线-高斯体相交平面的法向量，并应用正则化以显著增强几何形状。</li><li>开发了一种利用行进四面体的有效几何提取方法，其中四面体网格由三维高斯体诱导，并适应场景的复杂程度。</li><li>在表面重建和新视图合成中，GOF 优于现有的基于 3DGS 的方法。</li><li>在质量和速度方面，GOF 与神经隐式方法相当，甚至优于神经隐式方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯不透明度场：无界场景中的高效紧凑表面重建</li><li>作者：Zehao Yu, Torsten Sattler, Andreas Geiger</li><li>第一作者单位：德国图宾根大学图宾根人工智能中心</li><li>关键词：新颖视图合成、可微渲染、高斯溅射、表面重建、多视图转 3D</li><li>论文链接：https://niujinshuchong.github.io/gaussian-opacity-fields</li><li><p>摘要：（1）：研究背景：近年来，神经辐射场（NeRF）在新型视图合成和表面重建方面取得了显著进展。然而，现有方法在重建无界场景中前景对象时存在局限性，并且计算成本高。（2）：过去方法及其问题：传统方法通常采用体素融合或泊松重建来从 NeRF 的不透明度场中提取表面。这些方法存在噪声、不完整和计算成本高的缺点。（3）：提出的方法：本文提出了一种称为高斯不透明度场（GOF）的新颖方法。GOF 通过对 3D 高斯进行基于光线追踪的体积渲染来获得，它使我们能够直接识别 3D 高斯的水平集，从而从 3D 高斯中提取几何。此外，我们开发了一种利用行进四面体的有效几何提取方法，其中四面体网格是从 3D 高斯中感应出来的，并因此适应场景的复杂性。（4）：方法的性能：我们的评估表明，GOF 在表面重建和新颖视图合成方面优于现有的基于 3DGS 的方法。此外，它在质量和速度上都与神经隐式方法相当甚至优于神经隐式方法。</p></li><li><p>方法：(1) 构建高斯不透明度场 (GOF)，通过对 3D 高斯进行基于光线追踪的体积渲染获得；(2) 扩展 2D 高斯中的两个有效正则化项到 3D 高斯，提升重建质量；(3) 提出一种基于行进四面体的有效几何提取方法，从 GOF 中提取详细且紧凑的网格。</p></li><li><p>结论：(1): 本工作提出了一种高斯不透明度场 (GOF)，这是一种用于在无界场景中进行高效、高质量且紧凑的表面重建的新颖方法。我们的 GOF 是通过对 3D 高斯进行基于光线追踪的体积渲染获得的，与 RGB 渲染保持一致。我们的 GOF 能够直接从 3D 高斯中提取几何，通过识别其水平集，而无需泊松重建或 TSDF。我们近似高斯的曲面法线为射线-高斯相交平面的法线，并应用深度-法线一致性正则化来增强几何重建。此外，我们提出了一种利用行进四面体的有效且紧凑的网格提取方法，其中四面体网格是从 3D 高斯中感应出来的。我们的评估表明，GOF 在无界场景中的表面重建和新颖视图合成方面都优于现有方法。(2): 创新点：提出了一种基于 3D 高斯的高斯不透明度场 (GOF) 来进行表面重建；开发了一种基于行进四面体的有效几何提取方法，可以从 GOF 中提取详细且紧凑的网格。性能：在表面重建和新颖视图合成方面优于现有的基于 3D 高斯的方法；在质量和速度上都与神经隐式方法相当甚至优于神经隐式方法。工作量：需要进行基于光线追踪的体积渲染和行进四面体的几何提取，计算成本较高。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7120d48e211e632332c006e60959fa7f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5a10b3207b26758f6049e10e774c09a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e998868d938693cc86772478ebad84a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-818d65bf288671197aa1a4f35098147c.jpg" align="middle"></details><h2 id="AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting"><a href="#AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting" class="headerlink" title="AbsGS: Recovering Fine Details for 3D Gaussian Splatting"></a>AbsGS: Recovering Fine Details for 3D Gaussian Splatting</h2><p><strong>Authors:Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou</strong></p><p>3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: <a href="https://ty424.github.io/AbsGS.github.io/">https://ty424.github.io/AbsGS.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.10484v1">PDF</a> </p><p><strong>Summary</strong><br> 三维高斯溅射（3D-GS）技术将三维高斯原语与可微栅格化相结合，在提供先进实时渲染性能的同时实现高质量新视图合成结果。</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GS中自适应密度控制策略存在缺陷，导致在包含高频细节的复杂场景中经常出现过度重建问题，从而导致渲染图像模糊。</li><li>过度重建区域中过大的高斯体无法分裂，原因是梯度碰撞阻止了它们的分裂。</li><li>提出了一种新的同向视图空间位置梯度，作为致密化的标准，解决了这一问题。</li><li>该策略有效地识别出过度重建区域中的大高斯体，并通过分裂恢复精细细节。</li><li>该方法在各种具有挑战性的数据集上进行了评估，实验结果表明该方法实现了最佳渲染质量，同时减少了内存消耗。</li><li>该方法易于实现，可以整合到多种最新的基于高斯溅射的方法中。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于梯度碰撞的 3D 高斯泼溅中的过重建问题分析与解决（3D 高斯泼溅中的过重建问题分析与解决）</li><li>作者：Tianye Li<em>, Yuxuan Zhang</em></li><li>单位：无</li><li>关键词：Novel View Synthesis, 3D Gaussian Splatting, Point-based Radiance Field, 3D reconstruction</li><li>论文链接：arXiv:2404.10484v1[cs.CV]   Github 链接：None</li><li><p>摘要：（1）研究背景：3D 高斯泼溅技术将 3D 高斯基元与可微光栅化相结合，以实现高质量的新视角合成结果，同时提供先进的实时渲染性能。（2）过去的方法及其问题：3D 高斯泼溅中自适应密度控制策略的缺陷导致其在包含高频细节的复杂场景中经常出现过重建问题，从而导致渲染图像模糊。该缺陷的根本原因尚未得到充分探索。（3）本文提出的研究方法：提出了一种新颖的同向视图空间位置梯度作为致密化的标准，对上述伪影的原因（即梯度碰撞）进行了全面分析。该策略有效地识别出过重建区域中的大高斯体，并通过分裂恢复精细细节。（4）方法在何种任务上取得了怎样的性能：在各种具有挑战性的数据集上评估了所提出的方法。实验结果表明，该方法以减少或类似的内存消耗实现了最佳渲染质量。该方法易于实现，可以整合到各种最新的基于高斯泼溅的方法中。</p></li><li><p>方法：（1）分析了 3D 高斯泼溅中过重建问题的成因，即梯度碰撞，该现象阻止了过重建区域中大高斯体分裂；（2）提出了同向视图空间位置梯度作为致密化标准，有效识别出过重建区域中的大高斯体；（3）通过分裂操作恢复精细细节，改善渲染质量。</p></li><li><p>结论：(1)：本文深入研究了 3D 高斯泼溅中的过重建问题，并提出了基于梯度碰撞的分析与解决方法，有效提升了渲染质量。(2)：创新点：本文从梯度碰撞的角度分析了过重建问题的成因，并提出了同向视图空间位置梯度作为致密化标准，有效识别出过重建区域中的大高斯体，通过分裂操作恢复精细细节，改善渲染质量。性能：实验结果表明，该方法以减少或类似的内存消耗实现了最佳渲染质量。工作量：该方法易于实现，可以整合到各种最新的基于高斯泼溅的方法中。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b3954ae2d030deb08a1858901e173aeb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ed623f29e65c33839f7af3ba662cf77.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32e2b412c01557b89b9f9fab52d1386e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a224297269ae60107729a25ab0846851.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b860f19dbacac5f895a9fea794424f48.jpg" align="middle"></details><h2 id="SRGS-Super-Resolution-3D-Gaussian-Splatting"><a href="#SRGS-Super-Resolution-3D-Gaussian-Splatting" class="headerlink" title="SRGS: Super-Resolution 3D Gaussian Splatting"></a>SRGS: Super-Resolution 3D Gaussian Splatting</h2><p><strong>Authors:Xiang Feng, Yongbo He, Yubo Wang, Yan Yang, Zhenzhong Kuang, Yu Jun, Jianping Fan, Jiajun ding</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks &amp; Temples. Related codes will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.10318v1">PDF</a> submit ACM MM 2024</p><p><strong>Summary</strong><br>SRGS 在高分辨率空间进行优化，引入亚像素约束利用亚像素交叉视图信息，并结合预训练的 2D 超分辨率模型来提高三维高斯光栅原始体的表示能力，用于解决高分辨率新视角合成问题。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 在低分辨率中优化导致原始体的稀疏性和纹理缺陷，阻碍高分辨率新视角合成。</li><li>SRGS 在高分辨率空间进行优化，提高了原始体的密度。</li><li>引入亚像素约束来利用多个低分辨率视图的亚像素交叉视图信息。</li><li>累积更多视点的梯度有利于原始体的密集化。</li><li>集成预训练的 2D 超分辨率模型，使密集的原始体能够学习可靠的纹理特征。</li><li>SRGS 专注于致密化和纹理学习，有效增强了原始体的表示能力。</li><li>SRGS 在仅有低分辨率输入的情况下，实现了高分辨率新视角合成的出色渲染质量。</li><li>SRGS 在 Mip-NeRF 360 和坦克与寺庙等挑战性数据集上优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SRGS：超分辨率 3D 高斯泼溅</li><li>作者：Xiang Feng，Yongbo He，Yubo Wang，Yan Yang，Zhenzhong Kuang，Jun Yu，Jianping Fan，Jiajun Ding</li><li>单位：杭州电子科技大学</li><li>关键词：3D 高斯泼溅，超分辨率，新视角合成</li><li>论文链接：https://arxiv.org/pdf/2404.10318.pdf，Github 链接：无</li><li>摘要：（1）研究背景：3D 高斯泼溅 (3DGS) 是一种新颖的显式 3D 表示，依赖于高斯基元的表示能力来提供高质量的渲染。然而，在低分辨率下优化后的基元不可避免地表现出稀疏性和纹理不足，对实现高分辨率新视角合成 (HRNVS) 构成挑战。（2）过去方法及其问题：3DGS 虽然具有多项优点，但在仅使用低分辨率输入执行 HRNVS 时，渲染质量会急剧下降。这是因为 3DGS 中高斯基元的表示能力对于实现高质量的视图合成至关重要。具体来说，高分辨率渲染需要具有细粒度纹理特征的更密集的高斯基元。然而，对于低分辨率场景优化的基元不可避免地分布稀疏，导致渲染的 HR 视图中出现伪影。此外，现有的低分辨率视图缺乏必要的 HR 纹理。因此，3D 空间中的基元不可能在没有 HR 纹理的反投影的情况下学习相应的特征。（3）本文方法：为了解决这些限制，本文提出了超分辨率 3D 高斯泼溅 (SRGS)，它扩展了 3DGS 以实现高质量的 HRNVS。所提出的方法包括两部分，即超分辨率高斯致密化和纹理引导高斯学习。（4）方法性能：在 HRNVS 任务上，与仅使用低分辨率输入的最新方法相比，本文方法在具有挑战性的数据集（如 Mip-NeRF360 和 Tanks &amp; Temples）上实现了更高的渲染质量。这些性能支持了本文的目标，即在仅使用低分辨率输入的情况下实现高质量的 HRNVS。</li></ol><p>7.方法：（1）超分辨率高斯致密化；（2）纹理引导高斯学习。</p><ol><li>结论：(1): 本工作通过仅使用低分辨率输入实现高质量的新视角合成，在该领域做出了首次尝试。具体来说，我们首先使用超分辨率高斯致密化策略来增加高斯基元的密度，从而能够表示细粒度的高分辨率信息。此外，我们引入了一种纹理引导高斯学习策略，该策略指导高斯基元从外部 2D 超分辨率模型的先验中学习真实的纹理。在三个公开数据集上的实验结果表明，SRGS 有效地增强了高斯基元的表示能力，接近使用高分辨率视图训练的 3DGS 的渲染性能。值得注意的是，我们的 SRGS 主要受 2D 超分辨率模型的限制。在我们的未来工作中，我们将进一步探索没有 2D 超分辨率模型的新视角合成方法。(2): 创新点：SRGS；性能：SRGS 在仅使用低分辨率输入的情况下实现了高质量的新视角合成，在具有挑战性的数据集上优于最新方法；工作量：中等，需要额外的 2D 超分辨率模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8e51f459b7f920ab07ae498fd133cdfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73911338ed13dfd8725fff2143317b2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09dc3fb075728d00cdf50313743df98f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8712c281513c2e8cc3bc68f15b35bb68.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f9ab4e24b8b4895c13b3db9ace1733c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ee8ae605e49ec5d4b5081d13f242fab.jpg" align="middle"></details><h2 id="LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives"><a href="#LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives" class="headerlink" title="LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives"></a>LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives</h2><p><strong>Authors:Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</strong></p><p>Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction. To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning. With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research. We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices. Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures. This strategy achieves an optimal balance between memory performance and rendering quality. Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency. </p><p><a href="http://arxiv.org/abs/2404.09748v1">PDF</a> Project Page: <a href="https://jdtsui.github.io/letsgo/">https://jdtsui.github.io/letsgo/</a></p><p><strong>Summary</strong><br>激光辅助高斯球面投影法，可高效建模大规模复杂室内场景（例如车库），并在网页端实现实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>开发了适用于大规模车库建模和渲染的激光辅助高斯球面投影方法 LetsGo。</li><li>设计了一种集 IMU、激光雷达和鱼眼相机于一体的手持扫描仪 Polar。</li><li>推出了包含五种大规模车库场景的 GarageWorld 数据集。</li><li>提出了一种新的深度正则化器，可有效消除渲染图像中的浮动伪影。</li><li>提出了一种轻量级的细节级别（LOD）高斯渲染器，可实现基于网络设备的实时查看。</li><li>探索了一种混合表示，结合了传统网格（描述简单几何形状和颜色）和现代 3D 高斯表示（捕捉复杂细节和高频纹理）的优势。</li><li>实验结果表明，该方法在渲染质量和资源效率方面均优于其他方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LetsGo：基于激光雷达辅助的高斯体绘制的大规模车库建模与渲染</li><li>作者：Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</li><li>第一作者单位：上海科技大学</li><li>关键词：车库建模、激光雷达、高斯体绘制</li><li>论文链接：https://arxiv.org/abs/2404.09748   Github 代码链接：无</li><li>摘要：   （1）研究背景：大规模车库是日常生活中的普遍存在，但由于单调的颜色、重复的图案、反光表面和透明的车辆玻璃，给建模和渲染带来了挑战。传统的基于 Structure from Motion（SfM）的相机位姿估计和三维重建方法在这些环境中会失败，因为难以建立良好的对应关系。   （2）过去方法与问题：基于 SfM 的方法在车库环境中效果不佳。本文提出了一种激光雷达辅助的高斯体绘制方法来解决这些问题。   （3）研究方法：本文提出了一种称为 LetsGo 的方法，它使用激光雷达辅助的高斯体绘制来进行大规模车库建模和渲染。该方法包括：</li><li>开发了一种配备 IMU、激光雷达和鱼眼相机的便携式扫描仪 Polar，用于采集精确的激光雷达和图像数据。</li><li>提出了一种新的深度正则化器，用于训练三维高斯体绘制算法，有效地消除了渲染图像中的悬浮伪影。</li><li>设计了一种轻量级的细节层次（LOD）高斯渲染器，用于在基于 Web 的设备上实时查看。</li><li>探索了一种混合表示，它结合了传统网格（用于描绘简单的几何形状和颜色）和现代三维高斯表示（用于捕捉复杂细节和高频纹理）的优点。   （4）方法性能：在车库场景建模和渲染任务上，该方法取得了良好的性能，有效地消除了悬浮伪影，并实现了实时查看。这些性能支持了本文的目标，即在大规模车库场景中实现高保真渲染。</li></ol><p>7.方法：（1）：使用配备IMU、激光雷达和鱼眼相机的便携式扫描仪Polar采集精确的激光雷达和图像数据；（2）：提出新的深度正则化器，训练三维高斯体绘制算法，消除渲染图像中的悬浮伪影；（3）：设计轻量级的细节层次（LOD）高斯渲染器，用于在基于Web的设备上实时查看；（4）：探索结合传统网格和现代三维高斯表示的混合表示，兼顾简单几何形状和复杂细节的描绘。</p><ol><li>结论：（1）：本工作贡献了一款用于数据采集的手持设备Polar，一个车库世界数据集，用于场景表示的激光雷达辅助的高斯体，以及一种轻量级的渲染技术，该技术允许在消费者级设备上进行基于Web的渲染。得益于这些创新，我们成功地重建了各种具有多样化和具有挑战性环境的车库，允许从任何视点进行实时轻量级渲染。在收集的和两个公共数据集上的实验结果证明了我们方法的有效性。我们的车库世界以及重建的3D模型和实时渲染，支持一系列应用程序，包括训练数据生成和自动驾驶算法的测试平台、自动车辆定位、导航和停车的实时辅助，以及VFX制作。我们目前的工作主要集中在对世界的感知上，并且它支持下游识别任务。未来，我们还将探索车库生成，不断推动车库建模的边界，并实现从感知、识别到生成。（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a3cd69515b02d46d8a8eb1653b52018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a59b13ea73e0cb9d1f84e56d6ffa6262.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a7de06ef015705d30f275980dba4bac.jpg" align="middle"></details><h2 id="CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting"><a href="#CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting" class="headerlink" title="CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting"></a>CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting</h2><p><strong>Authors:Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, Sam Kwong</strong></p><p>Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research. </p><p><a href="http://arxiv.org/abs/2404.09458v1">PDF</a> Submitted to a conference</p><p><strong>Summary</strong><br>高斯点集通过优化混合高斯基元结构，实现高精度3D场景表示的低数据开销。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯点集因高渲染质量和效率在3D场景表示中受到广泛应用。</li><li>大量数据阻碍了高斯点集在实际应用中的实用性。</li><li>提出压缩高斯点集（CompGS），使用紧凑的高斯基元进行3D场景建模。</li><li>设计混合基元结构，捕捉基元间的预测关系，保证紧凑性。</li><li>利用少量锚定基元进行预测，将大多数基元封装成紧凑的残差形式。</li><li>开发速率受限优化方案，消除混合基元内的冗余。</li><li>CompGS在不影响模型精度和渲染质量的前提下，显著优于现有方法。</li><li>代码将在GitHub上发布，以促进进一步研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CompGS：通过压缩高斯喷射实现高效的 3D 场景表示</li><li>作者：Xiangrui Liu，Xinju Wu，Pingping Zhang，Shiqi Wang，Zhu Li，Sam Kwong</li><li>所属机构：香港城市大学</li><li>关键词：3D 场景表示，高斯喷射，压缩</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：（1）：研究背景：高斯喷射以其出色的渲染质量和效率而闻名，已成为 3D 场景表示中的重要技术。但是，高斯喷射大量的数据量阻碍了其在实际应用中的实用性。（2）：过去的方法：现有方法通过减少高斯喷射中 3D 高斯的数量和体积来压缩高斯喷射。这些方法通常采用启发式剪枝策略来移除对渲染质量贡献不大的 3D 高斯。此外，通常将矢量量化应用于保留的 3D 高斯以进一步压缩。（3）：提出的研究方法：本文提出了一种称为压缩高斯喷射 (CompGS) 的高效 3D 场景表示方法，该方法利用紧凑的高斯基元进行逼真的 3D 场景建模，同时显著减少了数据大小。为了确保高斯基元的紧凑性，我们设计了一种混合基元结构来捕获它们之间的预测关系。然后，我们利用一小组锚定基元进行预测，允许将大多数基元封装成高度紧凑的残差形式。此外，我们开发了一个速率约束优化方案来消除这种混合基元中的冗余，从而引导我们的 CompGS 在比特率消耗和表示效率之间实现最佳权衡。（4）：方法性能：实验结果表明，所提出的 CompGS 明显优于现有方法，在 3D 场景表示中实现了卓越的紧凑性，同时不影响模型准确性和渲染质量。</li></ol><p><strong>方法</strong></p><p>(1): 混合基元结构：建立一个混合基元结构，包括锚定基元和耦合基元。锚定基元提供参考嵌入和几何属性，而耦合基元仅包含紧凑的残差嵌入，以弥补预测误差。</p><p>(2): 跨基元预测：利用锚定基元的几何和外观属性，通过跨基元预测为耦合基元生成几何和外观属性。</p><p>(3): 速率约束优化：建立速率约束优化方案，通过最小化比特率消耗和渲染失真，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</p><p>(4): 熵估计：利用熵估计来有效建模锚定基元和耦合基元的比特率，为速率约束优化提供信息。</p><ol><li>结论：(1) 本工作提出了一个新颖的 3D 场景表示方法，压缩高斯喷射 (CompGS)，该方法利用紧凑的基元进行高效的 3D 场景表示，同时显著减小了数据大小。在此，我们为紧凑的场景建模量身定制了混合基元结构，其中耦合基元通过有限的锚定基元集有效地进行预测，从而封装成简洁的残差嵌入。同时，我们开发了一个速率约束优化方案，以进一步提高基元的紧凑性。在这个方案中，基元比率模型通过熵估计建立，然后制定速率失真代价以优化这些基元，以在渲染效率和比特率消耗之间实现最佳权衡。结合混合基元结构和速率约束优化，我们的 CompGS 优于现有的压缩方法，实现了卓越的尺寸缩减，同时不影响渲染质量。(2) 创新点：</li><li>混合基元结构，通过锚定基元预测耦合基元，实现紧凑的场景建模。</li><li>速率约束优化方案，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</li><li>熵估计，用于有效建模基元的比特率，为速率约束优化提供信息。性能：</li><li>与现有方法相比，在 3D 场景表示中实现了卓越的紧凑性，同时不影响模型准确性和渲染质量。工作量：</li><li>提出了一种新颖的 3D 场景表示方法，该方法利用紧凑的基元进行高效的 3D 场景表示，同时显著减小了数据大小。</li><li>设计了一种混合基元结构，包括锚定基元和耦合基元，以捕获基元之间的预测关系。</li><li>利用一小组锚定基元进行预测，允许将大多数基元封装成高度紧凑的残差形式。</li><li>开发了一个速率约束优化方案，通过最小化比特率消耗和渲染失真，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</li><li>利用熵估计来有效建模锚定基元和耦合基元的比特率，为速率约束优化提供信息。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7d580221a3c320fe2485a958d5382e40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e32abb85df2e20749e660a25f1ddab87.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4824050d2c2a0b18fbff95df4c7fbc91.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0ac958806d8e21cae6a784ed3e74514.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9cbd9e0e22a3201f4a5053aa5a6d3df2.jpg" align="middle"></details><h2 id="DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling"><a href="#DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling" class="headerlink" title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling"></a>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling</h2><p><strong>Authors:Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang</strong></p><p>Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods. </p><p><a href="http://arxiv.org/abs/2404.09227v1">PDF</a> </p><p><strong>Summary</strong><br>DreamScape 是一个仅从文本描述中创建高度一致的 3D 场景的方法，它利用了 Gaussian Splatting 的强大 3D 表示功能和大语言模型 (LLM) 的复杂排列能力。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯指南 ($3{DG^2}$) 用于场景表示，包括从 LLM 使用文本提示直接推导出的语义原语（对象）及其空间变换和关系。</li><li>渐进式比例控制在局部对象生成期间进行定制，确保不同大小和密度的对象适应场景，解决了后续全局优化阶段中简单混合引起的训练不稳定问题。</li><li>为了减轻 LLM 先验的潜在偏差，我们在全局级别对对象之间的碰撞关系进行建模，从而增强物理正确性和整体真实感。</li><li>为了生成像雨和雪这样广泛分布在场景中的普遍对象，我们引入了一种稀疏初始化和致密化策略。</li><li>实验表明，DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，达到了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScape：基于高斯泼溅的 3D 场景创建</li><li>作者：袁雪宁、杨宏宇、赵悦明、黄迪</li><li>单位：北京航空航天大学</li><li>关键词：多模态生成、3D 场景生成、场景合成、3D 高斯泼溅、LLM</li><li>论文链接：None    Github 链接：None</li><li>摘要：(1)：研究背景：近年来，文本到 3D 创建的进展得益于将来自文本到图像生成的扩散模型的强大先验整合到 3D 域中。然而，生成具有多个实例和复杂排列特征的 3D 场景仍然具有挑战性。(2)：以往方法：以往方法存在以下问题：</li><li>无法处理具有多个实例和复杂排列的 3D 场景。</li><li>训练不稳定，导致场景中不同大小和密度的对象无法适应。</li><li>容易受到 LLM 先验的偏差影响，导致物理不正确和整体真实性降低。</li><li>难以生成分布在场景中的普遍对象，如雨和雪。(3)：本文方法：本文提出 DreamScape，这是一种仅从文本描述创建高度一致的 3D 场景的方法，它利用了高斯泼溅的强大 3D 表示能力和大语言模型 (LLM) 的复杂排列能力。我们的方法包括：</li><li>3D 高斯引导 (3DG2) 用于场景表示，它由语义基元（对象）及其空间变换和关系组成，这些关系直接从文本提示中使用 LLM 推导出来。这种组合表示允许对整个场景进行局部到全局优化。</li><li>在局部对象生成期间定制的渐进式尺度控制，确保不同大小和密度的对象适应场景，从而解决后续全局优化阶段中简单混合引起的训练不稳定性问题。</li><li>为了减轻 LLM 先验的潜在偏差，我们在全局级别对对象之间的碰撞关系进行建模，从而增强物理正确性和整体真实性。</li><li>为了生成分布在场景中的普遍对象，如雨和雪，我们引入了稀疏初始化和致密化策略。(4)：实验结果：实验表明，DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，实现了最先进的性能。</li></ol><p>7.方法：(1)文本提示引导下的3D高斯引导(3DG2)场景表示；(2)定制的渐进式尺度控制，确保不同大小和密度的对象适应场景；(3)全局对象碰撞关系建模，增强物理正确性和整体真实性；(4)稀疏初始化和致密化策略，生成分布在场景中的普遍对象。</p><ol><li>结论：（1）：本文提出 DreamScape，这是一种仅从文本描述创建高度一致的 3D 场景的方法，利用了高斯泼溅的强大 3D 表示能力和大语言模型 (LLM) 的复杂排列能力。DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，实现了最先进的性能。（2）：创新点：</li><li>提出 3D 高斯引导 (3DG2) 场景表示，将语义基元及其空间变换和关系直接从文本提示中使用 LLM 推导出来，实现局部到全局优化。</li><li>采用定制的渐进式尺度控制，确保不同大小和密度的对象适应场景，解决训练不稳定性问题。</li><li>建模对象之间的碰撞关系，增强物理正确性和整体真实性。</li><li>引入稀疏初始化和致密化策略，生成分布在场景中的普遍对象。性能：</li><li>能够仅从文本提示生成高保真 3D 场景。</li><li>与其他方法相比，实现了最先进的性能。工作量：</li><li>需要一个相对较大的引导比例来确保模型收敛。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f88c017fbd210351ebea517e05fd02b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e9e7a61267e388dc08cefff90f6c8da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a3f52ac0c6425d9782495f42a860e11c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4177109b99815945eb22c94298f7ecfd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcf3ed3cba06de00cc7e8e90859ff690.jpg" align="middle"></details><h2 id="LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field"><a href="#LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field" class="headerlink" title="LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field"></a>LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field</h2><p><strong>Authors:Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He</strong></p><p>Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at <a href="https://pokerlishao.github.io/LoopGaussian/">https://pokerlishao.github.io/LoopGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2404.08966v2">PDF</a> 10 pages</p><p><strong>Summary</strong><br>基于3D高斯建模，LoopGaussian利用多视图图像合成技术，将影音图从2D图像空间升级为3D空间，展现自然流畅的动态效果。</p><p><strong>Key Takeaways</strong></p><ul><li>影音图结合静止图像和细微运动，创造引人入胜的体验。</li><li>LoopGaussian采用3D高斯建模，将影音图从2D提升至3D空间。</li><li>3D-GS方法从静态场景的多视图图像重建3D高斯点云，并融合形状正则化项防止变形。</li><li>3D高斯自编码器将点云投影到特征空间。</li><li>基于特征，SuperGaussian进行聚类，保持场景局部连续性。</li><li>两阶段估计方法计算簇间相似性，导出欧拉运动场描述场景速度。</li><li>3D高斯点在估计的欧拉运动场内移动，双向动画技术产生自然循环的3D影音图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LoopGaussian：通过欧拉运动场创建具有多视图的 3D 影像图</li><li>作者：李嘉洋，程乐超，王张野，穆婷婷，何静轩</li><li>单位：浙江大学</li><li>关键词：影像图，动态场景生成，3D 场景重建</li><li>论文链接：https://arxiv.org/abs/2404.08966   Github 代码链接：None</li><li><p>摘要：   （1）研究背景：影像图是一种独特的视觉媒体形式，它结合了静止摄影和微妙运动的元素，创造出一种引人入胜的体验。然而，目前大多数作品生成的视频缺乏深度信息，并且局限于 2D 图像空间的约束。   （2）过去方法及问题：为了解决上述问题，本文借鉴了 3D 高斯点云（3D-GS）在新型视图合成（NVS）领域取得的重大进展，提出了一种名为 LoopGaussian 的方法，利用 3D 高斯建模将影像图从 2D 图像空间提升到 3D 空间。   （3）研究方法：首先，本文使用 3D-GS 方法从静态场景的多视图图像中重建 3D 高斯点云，并结合形状正则化项以防止因物体变形而造成的模糊或伪影。然后，采用针对 3D 高斯量身定制的自编码器将其投影到特征空间。为了保持场景的局部连续性，本文设计了 SuperGaussian，基于所获取的特征进行聚类。通过计算聚类之间的相似性并采用两阶段估计方法，推导出一个欧拉运动场来描述整个场景中的速度。然后，3D 高斯点在估计的欧拉运动场中运动。通过双向动画技术，最终生成一个 3D 影像图，表现出自然且无缝循环的动态效果。   （4）方法性能：实验结果验证了本文方法的有效性，展示了高质量且视觉上吸引人的场景生成。该项目可在 https://pokerlishao.github.io/LoopGaussian/ 获得。</p></li><li><p>方法：（1）利用 3D-GS 方法从静态场景的多视图图像中重建 3D 高斯点云，并结合形状正则化项以防止因物体变形而造成的模糊或伪影。（2）采用针对 3D 高斯量身定制的自编码器将其投影到特征空间。（3）设计 SuperGaussian，基于所获取的特征进行聚类。（4）计算聚类之间的相似性并采用两阶段估计方法，推导出一个欧拉运动场来描述整个场景中的速度。（5）3D 高斯点在估计的欧拉运动场中运动。（6）通过双向动画技术，最终生成一个 3D 影像图，表现出自然且无缝循环的动态效果。</p></li><li><p>结论：（1）本工作提出了一种名为 LoopGaussian 的新颖框架，用于从静态场景的多视图图像生成真实的 3D 影像图。通过利用 3D 高斯点云 splatting 和固有的欧拉运动场，LoopGaussian 可以生成具有深度信息和自然动态效果的 3D 影像图。（2）创新点：</p></li><li>提出了一种使用 3D 高斯 splatting 从多视图图像重建 3D 高斯点云的方法，该方法可以有效地表示场景的形状和外观。</li><li>设计了一种针对 3D 高斯量身定制的自编码器，可以将 3D 高斯点云投影到特征空间，从而捕获场景的局部连续性。</li><li>提出了一种基于 SuperGaussian 聚类的两阶段估计方法，用于从特征中推导出欧拉运动场，该运动场描述了场景中的速度。</li><li>提出了一种双向动画技术，可以将 3D 高斯点云在估计的欧拉运动场中运动，从而生成具有自然且无缝循环动态效果的 3D 影像图。性能：</li><li>LoopGaussian 生成的 3D 影像图具有高质量和视觉吸引力，展示了场景的深度信息和自然动态效果。</li><li>LoopGaussian 在定性和定量评估中都优于现有方法，证明了其生成逼真 3D 影像图的能力。工作量：</li><li>LoopGaussian 的实现涉及多个步骤，包括 3D 高斯点云重建、特征提取、欧拉运动场估计和双向动画。</li><li>该方法需要大量的计算资源，尤其是对于复杂场景。</li><li>然而，LoopGaussian 提供了一个易于使用的界面，可以方便地生成 3D 影像图。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e2eba83df2d2f5ac39a3c3be75067d7a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51e0153b3e7c8edd334c5d696dd3d80a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-17bf5e90eaefe5fa5f4c44be35ab164a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5e793286a69b6603190dc809e34a6be3.jpg" align="middle"></details><h2 id="OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering"><a href="#OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering" class="headerlink" title="OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering"></a>OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering</h2><p><strong>Authors:Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</strong></p><p>Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes. </p><p><a href="http://arxiv.org/abs/2404.08449v2">PDF</a> </p><p><strong>Summary</strong><br>用3D高斯溅射法取代NeRF，大幅提升单目视频生成动态3D人物的速度和效率。</p><p><strong>Key Takeaways</strong></p><ul><li>利用3D高斯溅射，直接在规范空间中初始化3D高斯分布。</li><li>使用遮挡特征查询补偿缺失信息，结合遮挡感知loss函数更好地感知遮挡区域。</li><li>采用高斯特征MLP进一步处理特征，提升遮挡区域辨识度。</li><li>在模拟和真实遮挡场景中表现出色，与最先进方法媲美或优异。</li><li>训练和推理速度相比之前提升250倍和800倍。</li><li>适用于虚拟现实和数字娱乐等多种应用场景。</li><li>方法已开源，可供研究使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：OccGaussian：用于遮挡人类渲染的 3D 高斯喷绘（中译）</li><li>作者：Jingrui Ye, Zhongkai Zhang, Yujiao Jiang, Qingmin Liao*, Wenming Yang, Zongqing Lu</li><li>隶属单位：清华大学深圳国际研究生院（中译）</li><li>关键词：Canonical 3D Gaussians、Novel View Synthesis、10mins Training、160FPS Rendering</li><li>论文链接：https://arxiv.org/abs/2404.08449</li><li><p>摘要：（1）研究背景：动态 3D 人类渲染在虚拟现实和数字娱乐等应用中至关重要。大多数方法假设人类处于无遮挡场景中，而现实生活场景中各种物体可能会导致身体部位被遮挡。（2）过去方法：以前的方法利用 NeRF 进行表面渲染以恢复被遮挡区域，但这需要一天多的训练时间和几秒钟的渲染时间，无法满足实时交互式应用的要求。（3）研究方法：为了解决这些问题，论文提出了基于 3D 高斯喷绘的 OccGaussian，它可以在 6 分钟内训练完成，并以高达 160FPS 的速度生成高质量的人类渲染，即使输入被遮挡。OccGaussian 在规范空间中初始化 3D 高斯分布，并在被遮挡区域执行遮挡特征查询，提取聚合的像素对齐特征以补偿缺失信息。然后使用高斯特征 MLP 进一步处理特征，并结合遮挡感知损失函数来更好地感知被遮挡区域。（4）任务和性能：在模拟和真实世界遮挡中进行的广泛实验表明，与最先进的方法相比，该方法实现了相当甚至更好的性能。并且将训练和推理速度分别提高了 250 倍和 800 倍。</p></li><li><p>方法：(1) 3D高斯正向蒙皮：在规范空间中初始化 3D 高斯分布，并使用 LBS 变换根据 SMPL 参数将点映射到每个帧的姿态空间。（2）遮挡特征查询：对于每个被遮挡点，查询其在所有可见点中的 K 个最近可见点，并将这些 K 个最近可见点投影到特征图上以提取像素对齐特征。（3）高斯特征 MLP：将遮挡特征与嵌入的被遮挡点连接起来，并将其放入 MLP 中以预测球谐系数 f 和不透明度 α。（4）可微渲染器：应用基于平铺的可微渲染器来实现快速渲染和训练期间的自适应密度控制。（5）损失函数：设计遮挡损失和一致性损失，以防止模型在被遮挡区域学习背景信息。</p></li><li><p>结论：（1）：本工作首次提出了一种使用 3D 高斯喷绘在单目视频中渲染人类遮挡的方法。以往方法在训练和推理中耗时太长，无法满足实时应用的要求，而我们实现了快速训练（6~13 分钟）和实时渲染（169FPS）。具体来说，我们在遮挡区域执行特征查询，并将可见 K 近邻点的聚合像素对齐特征输入到 MLP 中以学习不可见点的特征。此外，我们为遮挡区域设计了专门的损失函数，使渲染更加完整。在我们的实验中，我们在模拟和真实世界遮挡下将 OccGaussian 与 SOTA 方法进行了比较。实验表明，我们的 OccGaussian 在保持快速训练和实时渲染的同时实现了 SOTA 性能。（2）：创新点：OccGaussian 是第一个使用 3D 高斯喷绘渲染遮挡场景中人类的方法；提出了遮挡区域的特征查询和 MLP 预测机制，有效补偿了缺失信息；设计了专门的遮挡损失函数，增强了模型对遮挡区域的感知能力。性能：在模拟和真实世界遮挡场景下，OccGaussian 在渲染质量上与 SOTA 方法相当甚至更好；训练速度提高了 250 倍，推理速度提高了 800 倍。工作量：OccGaussian 的训练时间仅为 6~13 分钟，推理速度高达 169FPS，满足了实时交互式应用的要求。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-22b96540b149a8534443374615ca8599.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11fe939c3521e47d3227ac9f217bda.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3930eb7a35a4e86cf46c4da432a8a109.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe0032a9957ec683a4fc3e6deab6cc1d.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v2">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>三维几何感知变形高斯散射（3DGS）方法，用于动态视点合成中对三维动态重建和几何约束变形建模的优化。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 方法结合了三维场景几何和高斯散射，解决了现有的神经辐射场（NeRF）方法变形建模中几何不一致的问题。</li><li>3DGS 模型是由可移动和旋转的三维高斯函数组成的，能逼真地模拟变形。</li><li>该方法提取三维几何特征，将其融入变形学习中，实现了三维几何感知的变形建模。</li><li>与传统的 NeRF 方法相比，3DGS 方法在动态视点合成和三维动态重建任务上获得了更好的性能。</li><li>3DGS 方法适用于合成数据集和真实数据集。</li><li>3DGS 方法在动态视点合成和三维动态重建任务上取得了最先进的性能。</li><li>3DGS 项目地址：<a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于 3D 几何的动态视点合成可变形高斯散射</li><li>作者：Jun Gao, Yixin Zhu, Jiahao Li, Jingyi Yu, Yebin Liu, Qiang Liu, Xiaogang Wang</li><li>第一作者单位：南京邮电大学</li><li>关键词：动态视点合成、神经辐射场、高斯散射、3D 场景几何</li><li>论文链接：https://arxiv.org/abs/2302.02203Github 链接：None</li><li>摘要：(1) 研究背景：神经辐射场（NeRF）在动态视点合成中取得了成功，但其隐式变形学习方式无法充分利用 3D 场景几何信息，导致变形不一致，影响合成质量。(2) 过去方法：现有方法通过引入 3D 高斯散射表示，可以显式建模场景几何，但缺乏对变形过程的几何约束。(3) 本文方法：提出了一种基于 3D 几何的动态视点合成可变形高斯散射方法，通过提取 3D 几何特征并将其融入变形学习中，增强了变形与场景几何的关联性。(4) 方法性能：在合成和重建任务上，该方法在合成质量、几何一致性和鲁棒性方面均优于现有方法，证明了其在动态视点合成中的有效性。</li></ol><p>7.方法：(1): 提取3D几何特征：利用基于点云的3D几何分析算法，提取场景的点云表示，并从中提取法向量、曲率等几何特征。(2): 融入变形学习：将提取的几何特征作为先验知识，融入到神经辐射场的变形学习中。通过设计一个几何损失函数，约束变形过程与场景几何特征的一致性。(3): 动态视点合成：利用变形后的神经辐射场，通过体渲染技术，合成不同视点的图像。</p><ol><li>结论：（1）：本文提出了一种基于 3D 几何感知的高斯散射可变形动态视点合成方法，该方法通过提取场景的 3D 几何特征并将其融入变形学习中，增强了变形与场景几何的关联性，在合成和重建任务上取得了较好的效果。（2）：创新点：</li><li>提出了一种基于 3D 几何感知的高斯散射可变形动态视点合成方法。</li><li>提出了一种基于点云的 3D 几何分析算法，提取场景的点云表示，并从中提取法向量、曲率等几何特征。</li><li>设计了一个几何损失函数，约束变形过程与场景几何特征的一致性。</li><li>性能：</li><li>在合成和重建任务上，该方法在合成质量、几何一致性和鲁棒性方面均优于现有方法，证明了其在动态视点合成中的有效性。</li><li>工作量：</li><li>该方法需要提取场景的 3D 几何特征，并将其融入变形学习中，因此工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-08bef4d6505c50c1da246e20076b2580.jpg" align="middle"><img src="https://picx.zhimg.com/v2-653303d99420769bc567815e0df0bf5a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f241223d01fd5b44400560a99355429c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20cc277b202c45bf86629c31910f0a4c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-403b047ba4b0fb3b2c45a81dd2533d35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-17  Gaussian Opacity Fields Efficient and Compact Surface Reconstruction in   Unbounded Scenes</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/</id>
    <published>2024-04-17T10:33:12.000Z</published>
    <updated>2024-04-17T10:33:12.763Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time"><a href="#VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time" class="headerlink" title="VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time"></a>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h2><p><strong>Authors:Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo</strong></p><p>We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors. </p><p><a href="http://arxiv.org/abs/2404.10667v1">PDF</a> Tech Report. Project webpage:   <a href="https://www.microsoft.com/en-us/research/project/vasa-1/">https://www.microsoft.com/en-us/research/project/vasa-1/</a></p><p><strong>Summary</strong></p><p>通过单张静态图像和语音音频剪辑生成逼真的会说话的面孔，具有视觉情感技能。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 VASA 框架，可生成逼真的会说话的面孔，具有视觉情感技能。</li><li>核心创新在于在人脸潜在空间中工作的面部动态和头部运动生成模型。</li><li>开发出表达丰富且不纠缠的人脸潜在空间。</li><li>方法在各个维度上明显优于以往方法。</li><li>方法不仅提供具有逼真面部和头部动态的高视频质量，还支持以高达 40 FPS 的速度在线生成 512x512 视频，且启动延迟可忽略不计。</li><li>为与具有类人会话行为的逼真虚拟形象进行实时互动铺平了道路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VASA-1：实时生成逼真的音频驱动说话人脸</li><li>作者：Sicheng Xu、Guojun Chen、Yu-Xiao Guo、Jiaolong Yang、Chong Li、Zhenyu Zang、Yizhong Zhang、Xin Tong、Baining Guo</li><li>第一作者单位：微软亚洲研究院</li><li>关键词：音频驱动说话人脸、视觉情感技能、扩散模型、人脸潜在空间</li><li>论文链接：https://arxiv.org/abs/2404.10667</li><li>摘要：（1）研究背景：在多媒体和通信领域，人脸不仅仅是一张面孔，而是一个动态的画布，其上的每一个细微动作和表情都可以表达情感、传递未说出口的信息，并促进移情连接。人工智能生成说话人脸技术的出现为未来提供了一个窗口，在这个窗口中，技术可以放大人与人以及人与人工智能交互的丰富性。这种技术有望丰富数字通信、提高交流障碍者的可访问性、通过互动式人工智能辅导改变教育方法，并在医疗保健中提供治疗支持和社交互动。（2）过去方法及其问题：现有的音频驱动说话人脸生成方法在逼真度和生动性方面存在局限性。这些方法通常依赖于基于关键点的唇形同步和基于模板的面部动画，这会导致僵硬和不自然的动作。此外，它们难以捕捉头部运动和微妙的面部表情，这些表情对于感知真实性和生动性至关重要。（3）提出的研究方法：本文提出 VASA-1，这是一种新的方法，可以生成高度逼真和生动的音频驱动说话人脸。VASA-1 的核心创新包括：</li><li>基于扩散的整体面部动态和头部运动生成模型，该模型在人脸潜在空间中工作。</li><li>使用视频开发了一个表达性和分离的人脸潜在空间。（4）方法在任务和性能上的表现：在广泛的实验中，包括对一组新指标的评估，VASA-1 在各个方面都明显优于以前的方法。它提供了具有逼真面部和头部动态的高视频质量，并且还支持以高达 40 FPS 在线生成 512×512 视频，且启动延迟可以忽略不计。这些性能支持了实时与模拟人类会话行为的逼真化身进行交互的目标。</li></ol><p>7.方法：(1)构建人脸潜在空间：利用无标注人脸视频数据集，构建具有高分离度和表达能力的人脸潜在空间，实现对人脸外观和动态的有效生成建模。(2)扩散变压器生成动态：利用扩散模型和变压器架构，提出全面的面部动态生成框架，以音频为条件，生成头和面部运动序列。(3)无分类器引导：在训练过程中，随机丢弃输入条件，并应用无分类器引导，增强模型对各种条件的鲁棒性。(4)人脸视频生成：在推理时，提取输入人脸图像和音频特征，生成头和面部运动序列，并使用训练好的解码器生成最终视频。</p><ol><li>结论：（1）：本文提出了一种名为 VASA-1 的音频驱动说话人脸生成模型，该模型以其从单张图像和音频输入中高效生成逼真的唇形同步、生动的面部表情和自然的头部动作而著称。它在视频质量和性能效率方面明显优于现有方法，在生成的说话人脸视频中展示了有前景的视觉情感技能。该模型的技术基石是一个创新的整体面部动态和头部动作生成模型，该模型在具有表达性和分离度的人脸潜在空间中工作。VASA-1 取得的进步有可能重塑各个领域的交互，包括通信、教育和医疗保健。可控条件信号的集成进一步增强了模型对个性化用户体验的适应性。（2）：创新点：</li><li>提出了一种基于扩散模型和变压器架构的全面面部动态生成框架，以音频为条件，生成头部和面部运动序列。</li><li>构建了一个具有高分离度和表达能力的人脸潜在空间，实现了对人脸外观和动态的有效生成建模。</li><li>应用无分类器引导，增强了模型对各种条件的鲁棒性。性能：</li><li>在广泛的实验中，包括对一组新指标的评估，VASA-1 在各个方面都明显优于以前的方法。</li><li>提供具有逼真面部和头部动态的高视频质量，并且还支持以高达 40FPS 在线生成 512×512 视频，且启动延迟可以忽略不计。工作量：</li><li>该方法在推理时，提取输入人脸图像和音频特征，生成头部和面部运动序列，并使用训练好的解码器生成最终视频。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-57afb9746460c539242f5be2406abcd8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4c78cc77ce02a94033d2c27026996d18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f451991b54ed6b1770c282a53cf0f267.jpg" align="middle"></details><h2 id="THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads"><a href="#THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads" class="headerlink" title="THQA: A Perceptual Quality Assessment Database for Talking Heads"></a>THQA: A Perceptual Quality Assessment Database for Talking Heads</h2><p><strong>Authors:Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</strong></p><p>In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. To tackle this issue, this paper introduces the Talking Head Quality Assessment (THQA) database, featuring 800 TH videos generated through 8 diverse speech-driven methods. Extensive experiments affirm the THQA database’s richness in character and speech features. Subsequent subjective quality assessment experiments analyze correlations between scoring results and speech-driven methods, ages, and genders. In addition, experimental results show that mainstream image and video quality assessment methods have limitations for the THQA database, underscoring the imperative for further research to enhance TH video quality assessment. The THQA database is publicly accessible at <a href="https://github.com/zyj-2000/THQA">https://github.com/zyj-2000/THQA</a>. </p><p><a href="http://arxiv.org/abs/2404.09003v1">PDF</a> </p><p><strong>Summary</strong><br>视频驱动的数字人说话头部评估数据库（THQA）为8种不同语音驱动方法生成的800个说话头部视频建立了基准，促进了说话头部视频质量评估的研究。</p><p><strong>Key Takeaways</strong></p><ul><li>说话头部视频驱动方法多样，质量参差不齐，影响用户视觉体验。</li><li>THQA 数据库包含 800 个说话头部视频，涵盖 8 种语音驱动方法、不同人物和语音特征。</li><li>主观质量评估实验分析了评分结果与语音驱动方法、年龄和性别之间的相关性。</li><li>主流图像和视频质量评估方法对 THQA 数据库有局限性，需要进一步研究改进 TH 视频质量评估。</li><li>THQA 数据库已公开，可用于研究。</li><li>TH 视频质量评估对于提高用户视觉体验至关重要。</li><li>语音驱动方法的进步将促进数字人说话头部技术的广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：THQA：用于说话人头像感知质量评估的数据库</li><p></p><p></p><li>作者：Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</li><p></p><p></p><li>第一作者单位：上海交通大学</li><p></p><p></p><li>关键词：数字人头部、语音驱动方法、质量评估数据库、无参考、多媒体处理</li><p></p><p></p><li>论文链接：None</li><p></p><p></p><li>摘要：（1）研究背景：数字人作为一种新兴的数字媒体技术，在娱乐、医疗、影视等行业得到了广泛的应用。然而，目前数字人的设计和制作过程仍然十分繁琐和耗时，主要依赖于熟练专业人员的手工操作。这种手工设计方式极大地制约了数字人内容制作的效率，尤其是在头部设计和驱动机制的复杂领域。（2）过去方法和问题：为了应对这一挑战，人工智能（AI）的出现和普及提供了一个有前景的解决方案，如图 1 所示。虽然语音驱动方法已被相继提出，简化了数字人面部表情和动作的设计，但仍然缺乏专门针对 AI 生成的说话人头像（TH）视频的质量评估指标。这些质量指标不仅可以有效评估说话人头像（TH）视频的质量，还可以间接促进语音驱动方法的进一步发展，从而为用户提供更高质量的视觉体验。遗憾的是，目前评估生成说话人头像视频的主流方法仍然遵循保留与原始视频比较的范式。值得注意的是，Fréchet 感知距离（FID）和余弦相似度（CSIM）仍然是用于此类评估的主要指标。然而，这些指标的局限性表现在两个基本方面。首先，这些客观评估指标仅关注图像或视频相似性，而忽略了整个生成内容带给观看者用户的整体视觉体验。其次，它们对原始参考视频的依赖性构成了一个实质性的限制，因为最终用户无法获得原始参考视频，从而严重限制了它们的适用性。虽然 CPBD 和 CGIQA 等指标已被纳入一些最近的工作中以衡量模糊级别和美学特征，但没有广泛使用的评估指标专门针对 TH 视频量身定制。（3）提出的研究方法：为了解决这一挑战，首先需要开发一个可公开访问的大规模 TH 视频数据库。因此，本文将重点放在以下几个方面：</li><p></p><p></p><li>提出一个新的说话人头像质量评估数据库 THQA，其中包含 800 个 TH 视频，这些视频是通过 8 种不同的语音驱动方法生成的。</li><p></p><p></p><li>广泛的实验验证了 THQA 数据库在角色和语音特征方面的丰富性。</li><p></p><p></p><li>后续的主观质量评估实验分析了评分结果与语音驱动方法、年龄和性别之间的相关性。</li><p></p><p></p><li>此外，实验结果表明，主流图像和视频质量评估方法对 THQA 数据库有局限性，强调了进一步研究以增强 TH 视频质量评估的必要性。</li><p></p><p></p><li><p></p><p>THQA 数据库可在 https://github.com/zyj-2000/THQA 公开获取。（4）方法在什么任务上取得了什么性能？性能是否能支撑其目标？THQA 数据库的建立为说话人头像视频质量评估提供了丰富的资源，并为进一步研究说话人头像视频质量增强奠定了基础。</p></li><li><p>方法：(1): 构建了一个包含 800 个 TH 视频的大型公开数据集 THQA，这些视频由 8 种不同的语音驱动方法生成；(2): 通过主观质量评估实验，分析了评分结果与语音驱动方法、年龄和性别之间的相关性；(3): 验证了 THQA 数据库在角色和语音特征方面的丰富性；(4): 实验结果表明，主流图像和视频质量评估方法对 THQA 数据库有局限性，强调了进一步研究以增强 TH 视频质量评估的必要性。</p></li></ol><p><strong>8.结论</strong>(1) 本工作的重要意义：本工作构建了一个名为 THQA 的说话人头像（TH）视频质量评估数据库。该数据库包含通过 8 种不同的语音驱动方法生成的 800 个 TH 视频。我们的分析涉及对收集的图像、语音数据和生成的视频的彻底检查。此外，我们进行主观评分实验以验证 THQA 的代表性，肯定其作为 TH 视频质量评估指导框架的效用。最后，我们基于 THQA 数据库对各种主流评估方法的性能进行比较评估。结果表明，大多数现有的评估方法在有效评估 TH 视频质量方面表现出局限性。</p><p>(2) 本文优缺点总结（三个维度）：创新点：* 构建了一个大规模的说话人头像视频质量评估数据库 THQA。* 分析了 TH 视频的丰富性，包括角色和语音特征。* 探索了主流图像和视频质量评估方法在评估 TH 视频质量方面的局限性。</p><p>性能：* THQA 数据库为说话人头像视频质量评估提供了丰富的资源。* 主观质量评估实验验证了 THQA 的代表性。* 比较评估表明，现有的评估方法在评估 TH 视频质量方面存在局限性。</p><p>工作量：* 构建 THQA 数据库涉及收集和处理大量数据。* 主观质量评估实验需要大量的人力资源。* 探索主流评估方法的局限性需要进行广泛的实验。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1f6bf6d7bad9eaf02e82acd303b468f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e543b10e4a34e8d4e06d3f29d16a43fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8610a595c8734930ae6c9ef9d82979cc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-beef350bacef5e83341d2b9912c3cd5f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3dad9d09b0691502f22ef81f9dd0bbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-17  VASA-1 Lifelike Audio-Driven Talking Faces Generated in Real Time</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/</id>
    <published>2024-04-17T06:02:48.000Z</published>
    <updated>2024-04-17T06:02:48.569Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting"><a href="#RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting" class="headerlink" title="RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting"></a>RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</h2><p><strong>Authors:Ashkan Mirzaei, Riccardo De Lutio, Seung Wook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic</strong></p><p>Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting — the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction. </p><p><a href="http://arxiv.org/abs/2404.10765v1">PDF</a> Project page: <a href="https://reffusion.github.io">https://reffusion.github.io</a></p><p><strong>Summary</strong><br>基于多尺度个性化的图像修复扩散模型，提出了RefFusion，一种3D场景修复方法，该方法可通过参考图像对内容进行明确控制，从而实现高质量合成。</p><p><strong>Key Takeaways</strong></p><ul><li>3D场景修复仍然面临可编辑性有限的挑战。</li><li>场景修复是一个固有的病态任务，即存在许多可信地替代缺失内容的解决方案。</li><li>好的修复方法不仅要实现高质量的合成，还要具有高度的可控性。</li><li>RefFusion是一种基于多尺度个性化的图像修复扩散模型，可以对给定的参考视图进行3D修复。</li><li>个性化有效地将先验分布适应到目标场景，从而降低了评分蒸馏目标的方差，因此产生了明显更清晰的细节。</li><li>RefFusion框架在对象移除方面实现了最先进的结果，同时保持了高度的可控性。</li><li>RefFusion的公式在对象插入、场景外画和稀疏视图重建等其他下游任务上也具有普遍性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：RefFusion：用于 3D 场景修复的参考自适应扩散模型2.作者：Ashkan Mirzaei, Riccardo de Lutio, SeungWook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic3.第一作者单位：NVIDIA，多伦多大学4.关键词：3D 场景修复、扩散模型、参考自适应5.论文链接：https://arxiv.org/abs/2404.10765Github 链接：无6.摘要：（1）研究背景：神经重建方法能够从一组姿态图像中无缝重建 3D 场景。然而，它们的可编辑性仍然有限，而 3D 场景修复是一种关键的可编辑操作，涉及合成合理的内容，以便从任何角度观看时都能与场景的其余部分融为一体。（2）过去的方法：过去的方法通常使用神经网络来生成修复内容。然而，这些方法通常缺乏对修复内容的显式控制，并且可能产生模糊或不连贯的结果。（3）研究方法：本文提出了一种名为 RefFusion 的 3D 场景修复方法。RefFusion 基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。这种个性化降低了分数蒸馏目标函数的方差，从而显著提高了细节的清晰度。（4）方法性能：RefFusion 在对象移除任务上实现了最先进的结果，同时保持了较高的可控性。此外，本文还展示了该方法在其他下游任务（如对象插入、场景外延和稀疏视图重建）上的通用性。这些结果表明，RefFusion 能够有效地合成高质量且可控的修复内容，从而满足各种 3D 场景编辑需求。</p><ol><li><p>方法：(1) <strong>RefFusion</strong>：本文提出的3D场景修复方法，基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。(2) <strong>多尺度个性化</strong>：RefFusion在扩散模型的不同尺度上对参考视图进行个性化，从而使模型能够捕获不同尺度上的细节和结构。(3) <strong>分数蒸馏目标函数</strong>：RefFusion使用分数蒸馏目标函数来匹配修复内容和参考视图的先验分布。通过个性化降低目标函数的方差，提高了修复内容的细节清晰度。</p></li><li><p>结论：(1): RefFusion 提出了一种基于图像修复扩散模型的多尺度个性化方法，用于 3D 场景修复，显著提高了修复内容的细节清晰度和可控性，在对象移除等任务上取得了最先进的结果。(2): 创新点：</p><ul><li>多尺度个性化：在扩散模型的不同尺度上对参考视图进行个性化，捕获不同尺度上的细节和结构。</li><li>分数蒸馏目标函数：使用分数蒸馏目标函数匹配修复内容和参考视图的先验分布，降低目标函数的方差，提高修复内容的细节清晰度。性能：</li><li>在对象移除任务上实现了最先进的结果。</li><li>在对象插入、场景外延和稀疏视图重建等下游任务上表现出通用性。工作量：</li><li>训练 RefFusion 模型需要大量的计算资源。</li><li>RefFusion 的推理速度可能会受到模型复杂度的限制。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/02817604a88632e7e3ea4560f26f9bac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c106f428df739a7142772c42a95151c1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/15ab37f03412f6ff0a7f34d5503212ec241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/542185dc6584d7e97ea791d916e54a04241286257.jpg" align="middle"></details>## LaDiC: Are Diffusion Models Really Inferior to Autoregressive   Counterparts for Image-to-Text Generation?**Authors:Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun**Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&amp;Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation. [PDF](http://arxiv.org/abs/2404.10763v1) **Summary**扩散模型在文本到图像生成方面表现出色，现在通过LaDiC架构在图像到文本生成方面也取得了突破性的进展。**Key Takeaways**- 扩散模型具有整体上下文建模和并行解码能力。- AR 方法存在推理速度慢、误差传播和单向约束等固有缺陷。- 缺乏有效的图像文本对齐潜在空间和连续扩散过程与离散文本数据之间的差异是扩散模型先前表现不佳的原因。- LaDiC 架构使用 split BERT 创建了专门的标题潜在空间，并集成了一个正则化模块来管理不同的文本长度。- LaDiC 包括一个用于语义图像到文本转换的扩散器，以及一种在推理过程中增强标记交互性的 Back&amp;Refine 技术。- 在 MS COCO 数据集上，LaDiC 实现了基于扩散方法的最新性能，BLEU@4 为 38.2，CIDEr 为 126.2，在没有预训练或辅助模块的情况下表现出色。- 这表明扩散模型在图像到文本生成方面具有与 AR 模型相当的竞争力，揭示了其在该领域尚未开发的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：图像段落描述：生成连贯且信息丰富的图像描述</li><li>作者：Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei</li><li>隶属关系：斯坦福大学</li><li>关键词：图像描述，段落生成，多模态学习，自然语言处理，计算机视觉</li><li>论文链接：https://arxiv.org/pdf/1611.06607.pdf，Github 链接：无</li><li>摘要：(1)：研究背景：图像描述任务通常生成单个句子来描述图像，但这种描述过于粗略，无法捕捉图像的丰富细节。密集描述任务可以描述图像中的多个区域，但生成的描述缺乏连贯性。(2)：过去的方法：传统图像描述方法生成单个句子，而密集描述方法生成多个短语，但都存在信息不足或缺乏连贯性的问题。(3)：研究方法：本文提出了一种图像段落描述模型，该模型将图像分解为语义区域，并使用分层循环神经网络对语言进行推理。通过这种方法，模型可以生成连贯且信息丰富的段落，详细描述图像中的内容。(4)：方法性能：在图像段落描述数据集上，该模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。</li></ol><p>7.方法：(1) 利用文本编码器将离散文本空间 C 转换为连续文本潜在空间 X；(2) 训练扩散器以在图像空间 V 和文本空间 X 之间架起桥梁；(3) 文本解码器将文本潜在码映射回离散文本。</p><ol><li>结论：（1）： 本文研究了基于扩散的图像到文本范式，并引入了一种新颖的架构，称为 LaDiC。我们的模型在基于扩散的方法中取得了最先进的性能，并展示了与一些预训练的 AR 模型相当的能力。此外，我们广泛的实验揭示了扩散模型在考虑更多整体上下文和并行发出所有标记方面优于 AR 模型的令人兴奋的优势。因此，我们认为扩散模型在图像到文本生成方面具有巨大的潜力，我们希望我们的工作将为该领域的这一领域开辟新的可能性。（2）： 创新点：提出了一种新颖的基于扩散的图像到文本生成架构 LaDiC，该架构在图像空间和文本空间之间建立了桥梁，并利用分层循环神经网络进行语言推理，从而生成连贯且信息丰富的图像描述。性能：在图像段落描述数据集上，我们的模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。工作量：本文主要集中在图像到文本生成的主要研究课题上，以保持简洁性和重点。然而，我们观察到我们的模型可以很容易地适应其他模态甚至纯文本生成，而只需进行最小的修改。我们将这些潜在的扩展留给未来的工作，同时，我们希望本文将激发研究人员使用扩散模型从事以文本为中心的模态生成任务的信心，并期待在这个领域未来的精彩作品。此外，由于资源限制，我们研究中使用的模型参数和数据集并不广泛。考虑到像 GPT 这样的自回归模型在放大时表现出的显着的紧急能力，探索我们的模型或一般的扩散模型是否可以表现出类似的可扩展性，成为一项有趣且有价值的探索。风险考虑：作为一个生成模型，我们的模型可能会无意中产生难以与人类书面内容区分开来的结果，引发对潜在误用的担忧。采用文本水印技术可能有助于减轻这个问题。此外，扩散模型通常需要大量的计算资源进行训练，从而导致二氧化碳排放和环境影响增加。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/ad873da96dc2ed96671aaa4ec1d1b20f241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/35dd717311044737324107cdc54b6822241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9f7a7a48ee0399a430575adacda8ed66241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/093e2d146940e0222b3021bdfb674cf9241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9d1dfb06c22a75df200fdac77b6e7498241286257.jpg" align="middle"></details>## GazeHTA: End-to-end Gaze Target Detection with Head-Target Association**Authors:Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang**We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. [PDF](http://arxiv.org/abs/2404.10718v1) **Summary**基于视觉图像预测注视目标，提出端到端多人物GazeHTA框架，通过预训练扩散模型、重新注入头特征、学习连接图实现高效的目标检测。**Key Takeaways**- 提出端到端多人物注视目标检测框架 - GazeHTA。- 利用预训练扩散模型提取场景特征，提升语义理解。- 重新注入头特征，增强头部先验，提升头部理解。- 学习连接图，明确头部与注视目标之间的视觉关联。- 采用扩散模型作为基准，实现更优的性能。- 在标准数据集上，GazeHTA 优于现有方法。- GazeHTA 提供了端到端的解决方案，可直接从图像预测注视目标。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GazeHTA：端到端的注视目标检测</li><li>作者：Zhi-Yi Lin、Jouh Yeong Chew、Jan van Gemert、Xucong Zhang</li><li>第一作者单位：代尔夫特理工大学计算机视觉实验室</li><li>关键词：注视目标检测、头部目标关联、扩散模型</li><li>论文链接：arXiv:2404.10718v1[cs.CV]16Apr2024</li><li>摘要：（1）研究背景：人类注意力估计在人机交互、社会活动识别、心理健康诊断和客户行为分析等领域具有重要意义。注视目标检测旨在直接将个体与其注视目标关联起来，提供了一种端到端的注意力估计解决方案。</li></ol><p>（2）过去方法及问题：大多数注视目标检测方法采用双流架构，存在以下问题：- 缺乏头部和注视目标之间的直接关联。- 依赖于现成的头部检测器。- 仅限于一次处理一个头部，在多人物场景中需要重复处理。</p><p>（3）本文方法：GazeHTA 提出了一种端到端的多人注视目标检测框架，通过头部目标关联来预测头部目标实例。其特点包括：- 利用预训练的扩散模型提取场景特征。- 重新注入头部特征以增强头部先验知识。- 学习连接图来表示头部和注视目标之间的视觉关联。</p><p>（4）性能：GazeHTA 在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。实验结果表明，该方法可以很好地支持其目标。</p><p>7.方法：（1）利用预训练的扩散模型提取场景特征（Stable Diffusion）；（2）重新注入头部特征以增强头部先验知识（Head Feature Re-Injection）；（3）学习连接图来表示头部和注视目标之间的视觉关联（Connection Map）。</p><ol><li>结论：(1): GazeHTA 提出了一种端到端的注视目标检测框架，通过头部目标关联来预测头部目标实例，在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。(2): 创新点：</li><li>利用预训练的扩散模型提取场景特征，增强了头部先验知识，学习了头部和注视目标之间的视觉关联。</li><li>采用多任务学习策略，同时预测头部目标和注视目标，提高了检测精度。</li><li>引入了连接图，表示头部和注视目标之间的视觉关联，增强了头部目标关联的鲁棒性。性能：</li><li>在两个标准数据集上，GazeHTA 在注视目标检测任务上取得了最先进的性能。</li><li>该方法在多人物场景中表现出色，能够准确地将头部与注视目标关联起来。</li><li>该方法对头部姿态和照明条件的变化具有鲁棒性。工作量：</li><li>该方法的实现相对复杂，需要预训练扩散模型和训练连接图。</li><li>该方法的计算成本较高，在实际应用中可能需要优化。</li><li>该方法需要大量标注数据进行训练，这可能会限制其在某些场景中的应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/85b5892f7794783ad79c67d67689cac6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c2dd1f0a5f6c974a1e1cc5b4db6180e3241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/831d352002cad6012c105349850e0b6b241286257.jpg" align="middle"></details>## Efficient Conditional Diffusion Model with Probability Flow Sampling for   Image Super-resolution**Authors:Yutao Yuan, Chun Yuan**Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP. [PDF](http://arxiv.org/abs/2404.10688v1) AAAI 2024**Summary**通过概率流采样的高效条件扩散模型（ECDP）在图像超分辨率方面实现了高超分辨率图像质量和低时间消耗。**Key Takeaways**- 扩散概率模型能够处理图像超分辨率中固有的病态性问题。- 迭代采样导致现有基于扩散的超分辨率方法时间消耗高。- ECDP 引入了连续时间条件扩散模型以提高采样效率。- 混合参数化为去噪网络提高了生成图像的一致性。- 图像质量损失作为扩散模型得分匹配损失的补充，进一步提高了一致性和质量。- ECDP 在 DIV2K、ImageNet 和 CelebA 上优于现有基于扩散的超分辨率方法，同时时间消耗更低。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于概率流采样的高效条件扩散模型用于图像超分辨率</li><li>作者：袁宇涛，袁淳</li><li>单位：清华大学</li><li>关键词：图像超分辨率，扩散概率模型，概率流采样</li><li>论文链接：https://arxiv.org/abs/2404.10688   Github代码链接：None</li><li><p>摘要：（1）研究背景：图像超分辨率是一项本质上不适定的问题，因为对于一个低分辨率图像存在多个有效的高分辨率图像。基于扩散概率模型的超分辨率方法可以通过学习条件在低分辨率图像上的高分辨率图像分布来处理不适定性，避免了以 PSNR 为导向的方法中图像模糊的问题。（2）过去的方法及问题：现有的基于扩散的超分辨率方法使用迭代采样，时间消耗大；并且由于颜色偏移等问题，生成图像的质量和一致性不理想。（3）提出的研究方法：本文提出了用于图像超分辨率的基于概率流采样的高效条件扩散模型（ECDP）。为了减少时间消耗，设计了用于图像超分辨率的连续时间条件扩散模型，这使得可以使用概率流采样进行高效生成。此外，为了提高生成图像的一致性，提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值。此外，设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。（4）方法在任务和性能上的表现：在 DIV2K、ImageNet 和 CelebA 上进行的广泛实验表明，本文方法比现有的基于扩散的图像超分辨率方法实现了更高的超分辨率质量，同时具有更低的时间消耗。这些性能支持了本文的目标。</p></li><li><p>方法：(1): 设计了用于图像超分辨率的连续时间条件扩散模型，使得可以使用概率流采样进行高效生成；(2): 提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，提高生成图像的一致性；(3): 设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。</p></li><li><p>结论：（1）本文提出的 ECDP 框架在图像超分辨率任务上取得了较好的效果，在保证超分辨率质量的同时，降低了时间消耗。（2）创新点：</p></li><li>提出了一种基于连续时间条件扩散模型的图像超分辨率方法，该方法可以有效地利用概率流采样进行生成。</li><li>提出了一种混合参数化的去噪器网络，该网络可以在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，从而提高生成图像的一致性。</li><li>设计了一个图像质量损失作为扩散模型分数匹配损失的补充，该损失可以有效地提高超分辨率结果的质量。性能：</li><li>在 DIV2K、ImageNet 和 CelebA 数据集上的实验表明，本文方法比现有的基于扩散的图像超分辨率方法具有更高的超分辨率质量和更低的时间消耗。工作量：</li><li>本文方法的实现相对简单，易于复现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/a82d966193c93e548a48f6956d503a03241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/3243ba61ae0dc29581f4a5b0b8bc24ac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9ec5bc713a1dd0f65714d070ec06c103241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8b672147537b2050d8dcfda1a25fba99241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/84fee720619599629de5e02d5ca3c96a241286257.jpg" align="middle"></details>## StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text   Reference via Progressive Optimization**Authors:Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung**Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences. [PDF](http://arxiv.org/abs/2404.10681v1) project page: https://chenyingshu.github.io/stylecity3d/**Summary**城市场景大规模纹理风格化在文图驱动下，融合神经纹理场，能生成逼真风格化的纹理和全景天空。**Key Takeaways**- 提出了一个城市场景大规模纹理风格化系统。- 提出了一种神经纹理场的风格化方法，将二维视觉与文本先验全局和局部地迁移到三维中。- 渐进缩放输入三维场景的训练视图以保留场景内容的高质量。- 通过调整风格图像和训练视图的尺度来全局优化场景风格。- 采用语义感知的风格损失增强局部语义一致性，这对于真实感风格化至关重要。- 采用生成式扩散模型合成风格一致的全景天空图像，提供更沉浸的氛围并辅助语义风格化。- 风格化的神经纹理场可以烘焙成任意分辨率的纹理，无缝集成到传统渲染管道中，极大地简化了虚拟制作原型制作过程。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：StyleCity：大规模 3D 城市场景</li><li>作者：Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung</li><li>第一作者单位：香港科技大学</li><li>关键词：城市风格化、大规模场景、视觉和文本参考、渐进式优化</li><li>论文链接：https://arxiv.org/abs/2404.10681</li><li><p>摘要：(1) 研究背景：创建具有不同风格的大规模虚拟城市场景具有挑战性，需要复杂的材质和灯光设置。(2) 过去方法：现有的方法主要依赖于手工制作的材质和灯光，难以实现大规模场景的自动风格化。(3) 研究方法：本文提出 StyleCity 框架，通过将 2D 视觉和文本先验全局和局部地转移到 3D，对神经纹理场进行风格化。通过渐进式缩放训练视图和自适应风格图像比例以及语义感知风格损失，实现场景风格化和全景天空背景生成。(4) 实验结果：在城市场景风格化任务上，StyleCity 在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验。</p></li><li><p>方法：（1）：枢轴视图规划：提出了一种基于枢轴的视图规划方法，通过在网格边界框的上、侧面均匀采样相机位置，并使用网格区域的质心作为相机视点，获得枢轴视图，覆盖可见表面的大部分区域，为新视图（图 2 中的绿色相机）提供初始化，以增强训练，见第 3.3 节多尺度渐进优化。（2）：城市场景分割：为语义感知的城市场景风格化，考虑了建筑场景中感兴趣的类别，包括“天空”、“建筑物”、“窗户”、“道路”、“人”、“植物”、“汽车”、“水”和“灯光”。（3）：神经纹理场定义：使用神经纹理场表示，将巨大的纹理贴图重新参数化为二维连续函数，通过一个 MLP 将归一化的 UV 纹理坐标映射为颜色 RGB 值，理论上支持任意纹理分辨率，平均纹理尺寸压缩 90%。（4）：神经渲染：给定相机位姿，光栅化网格并检索 UV 以查询纹理模型 TΘ(·)，获得相应的纹理 RGB 值，然后将值重新组合到渲染的图像中。（5）：内容和风格联合渐进优化：通过多个视图与每个迭代中的源内容和目标风格约束共同优化神经纹理场，随机采样视点，渲染内容视图及其分割，并从神经纹理中获得风格化视图。（6）：多尺度渐进优化：在优化期间，沿贝塞尔曲线随机采样新视图，以附近的计划枢轴相机作为控制点，以扩大覆盖角度，并在训练期间以“放大”效果逐渐缩小采样视图的视场 (FoV)，确保每个表面都被全面风格化和全局谐波。（7）：内容和真实感保留优化：利用渲染视图的内容特征和拉普拉斯值进行监督，以保持纹理内容和场景标识，屏蔽背景区域以保持内容完整性和真实感。（8）：全局规模自适应风格优化：全局风格优化负责将风格特征全局转移到神经纹理场，以进行整体氛围对齐，基于训练视图和风格贴块之间的结构相似性匹配多尺度风格。（9）：全局视觉和文本驱动的风格损失：通过惩罚全局风格特征分布的差异，快速获取和生成有意义的新风格化纹理。（10）：局部语义感知风格优化：对于具有复杂上下文的城市场景，全局风格转移容易导致风格语义不匹配，因此引入局部风格优化策略，进行按类别特征正则化，以实现更逼真的风格化。</p></li><li><p>结论：（1）：StyleCity是一个基于视觉和文本驱动的城市级纹理网格大规模风格化管道。我们利用神经纹理场建模场景外观，并提出了一种新的多尺度渐进优化方法，以实现高保真风格化。对于谐波风格化，我们引入了尺度自适应风格优化和新的损失函数，以全局和局部正则化风格特征。此外，我们改进了漫反射全景合成方法，以支持风格对齐的高分辨率全方位天空合成，这作为沉浸式氛围和更好语义的背景。（2）：创新点：StyleCity框架、神经纹理场表示、多尺度渐进优化、尺度自适应风格优化、全局视觉和文本驱动的风格损失、基于漫反射的全景天空合成；性能：在城市场景风格化任务上，StyleCity在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验；工作量：该方法需要大量的数据集和计算资源，并且训练过程可能需要很长时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/7cb106e145913b85abac0f0c3f097cc8241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/086cffd59fa8e20d53cc5c5be733a969241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68c301be7084d603d72cc10756a2ae2c241286257.jpg" align="middle"></details>## Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery**Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed**Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. [PDF](http://arxiv.org/abs/2404.10356v1) Submitted to International Conference on Pattern Recognition (ICPR)   2024**Summary**利用扩散模型的卓越图像合成能力，提出一种新的概念发现框架，以发现黑盒模型中与决策相关的重要概念。**Key Takeaways**- 黑盒模型中的概念发现对于建立信任和推进医学研究至关重要。- CDCT是一种用于概念发现的新型三步框架，利用扩散模型的图像合成能力。- CDCT通过生成反事实轨迹数据集、提取解耦表征和应用搜索算法来识别相关的概念。- CDCT发现了皮肤病变分类器中的偏差和有意义的生物标志物。- CDCT生成的对抗样本的FID得分比现有方法更好，同时资源效率提高了12倍。- 无监督概念发现对于可信赖AI的应用和人类知识在各个领域的进一步发展具有巨大潜力。- CDCT代表了朝着这个方向迈出的进一步一步。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于潜在扩散模型的反事实轨迹生成，用于概念发现</li><li>作者：</li><li>Payal Varshney</li><li>Adriano Lucieri</li><li>Christoph Balada</li><li>Andreas Dengel</li><li>Sheraz Ahmed</li><li>第一作者单位：德国凯撒斯劳滕-兰道莱茵兰-普法尔茨技术大学</li><li>关键词：可解释性、反事实、基于概念的解释、潜在扩散模型、皮肤镜检查、概念发现</li><li>论文链接：https://arxiv.org/abs/2404.10356   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：      深度学习模型在医疗等高风险领域的安全应用需要可信度。理解决策过程不仅有助于建立信任，而且还能揭示复杂模型以前未知的决策标准，从而推进医学研究的发展。从黑盒模型中发现与决策相关的概念是一项特别具有挑战性的任务。   (2) 过去的方法及其问题：      以往的方法主要依赖于梯度或扰动技术，但这些方法生成的反事实图像质量较差，难以从中提取有意义的概念。      本文提出的方法动机充分，它利用扩散模型出色的图像合成能力，通过生成反事实轨迹数据集来发现与分类相关的概念。   (3) 本文提出的研究方法：      本文提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。该框架包括：</p><ul><li>使用潜在扩散模型生成反事实轨迹数据集。</li><li>利用变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示。</li><li>应用搜索算法在解耦的潜在空间中识别相关概念。   (4) 方法在任务上的表现及性能：  将 CDCT 应用于在最大的公开皮肤病变数据集上训练的分类器，发现了几个偏差和有意义的生物标记。  此外，在 CDCT 中生成的反事实图像显示出比先前建立的最新方法更好的 FID 分数，同时资源效率提高了 12 倍。  这些性能支持了本文提出的方法可以有效发现与分类相关的概念，并为可信赖的人工智能和各个领域的知识发展做出贡献。</li></ul></li><li><p>方法：（1）：潜在扩散模型生成反事实轨迹数据集；（2）：变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示；（3）：搜索算法在解耦的潜在空间中识别相关概念。</p></li><li><p>结论(1): 本工作通过基于潜在扩散模型的反事实轨迹生成，为概念发现提供了一种可解释且高效的方法，在医疗等高风险领域的安全应用中具有重要意义。(2): 创新点：</p></li><li>提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。</li><li>利用扩散模型出色的图像合成能力，生成反事实轨迹数据集，从中提取分类相关概念的解耦表示。</li><li>在解耦的潜在空间中应用搜索算法识别相关概念。性能：</li><li>在最大的公开皮肤病变数据集上训练的分类器中发现了几个偏差和有意义的生物标记。</li><li>与先前建立的最新方法相比，生成的反事实图像显示出更好的 FID 分数，同时资源效率提高了 12 倍。工作量：</li><li>该方法易于实现，并且可以应用于各种分类任务。</li><li>训练和部署 CDCT 模型的计算成本相对较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/576c1e270406787f968cf066ad94df12241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/921adc3c228efd65b737862c3ffcf199241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/4d2bd80c6bf872188ed28b597b22867a241286257.jpg" align="middle"></details>## Efficiently Adversarial Examples Generation for Visual-Language Models   under Targeted Transfer Scenarios using Diffusion Models**Authors:Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo**Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner. [PDF](http://arxiv.org/abs/2404.10335v1) **Summary**利用扩散模型生成逼真的对抗样本，显著提升图像和语言模型对抗攻击效率和安全性。**Key Takeaways**- 提出 AdvDiffVLM，利用扩散模型生成自然、无约束的对抗样本。- 采用自适应集成梯度估计，修改扩散模型反向生成过程中的评分，确保对抗样本包含自然的对抗语义。- 使用 GradCAM 引导掩码方法，将对抗语义分散到整个图像，避免集中在特定区域。- 与现有转移攻击方法相比，速度提高 10-30 倍，同时保持对抗样本的质量。- 生成的对抗样本具有很强的迁移性，对对抗防御方法的鲁棒性更高。- AdvDiffVLM 可以以黑盒方式成功攻击商用 VLM，包括 GPT-4V。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于扩散模型的视觉语言模型目标迁移场景下高效对抗样本生成</li><li>作者：郭奇、庞山民、贾晓军、郭庆</li><li>隶属单位：西安交通大学</li><li>关键词：对抗样本、视觉语言模型、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.10335</li><li><p>摘要：(1) 研究背景：针对视觉语言模型（VLM）的基于目标迁移的对抗攻击对 VLM 构成重大威胁。然而，最先进的基于迁移的攻击由于过多的迭代次数而产生高昂的成本。此外，生成的对抗样本表现出明显的对抗噪声，并且在规避 DiffPure 等防御方法方面表现出有限的效力。(2) 过去的方法及问题：AttackVLM 使用基于查询的攻击方法并结合基于迁移的先验，提示黑盒 VLM 产生目标响应。但是，由于需要大量的查询，这个过程非常耗时，通常需要几个小时才能生成一个对抗样本。因此，考虑了另一种黑盒攻击方法，即基于迁移的攻击。然而，如图 1 所示，当前最先进的基于迁移的攻击在生成对抗样本方面也较慢，并且在规避对抗防御方法方面效果较差。此外，这些方法生成的对抗样本表现出明显的噪声。(3) 论文提出的方法：为了解决这些问题，受评分匹配和不受限制的对抗样本的启发，提出了 AdvDiffVLM，它使用扩散模型生成自然、不受限制的对抗样本。具体来说，利用并修改了预训练扩散模型的反向生成过程，其中利用自适应集成梯度估计来改变评分并将目标语义嵌入对抗样本中。为了增强输出的自然性，引入了 GradCAM 引导掩码，它将对抗目标语义分散在对抗样本中，而不是将它们集中在特定区域，从而提高图像质量。(4) 方法在任务和性能上的表现：该方法只需要几个后向去噪步骤即可生成对抗样本，使其明显快于以前公布的基于迁移的方法。此外，AdvDiffVLM 通过去噪生成对抗样本，对防御方法表现出更高的鲁棒性。实验结果表明，AdvDiffVLM 在目标和迁移场景中针对最先进的基于迁移的攻击实现了数量级或更大的加速，同时提供了具有更高图像质量的对抗样本。此外，这些对抗样本在跨不同 VLM（包括商业 VLM，如 GPT-4V）的迁移中表现出强大的可迁移性。</p></li><li><p>方法：(1): 从评分匹配的角度出发，将对抗攻击建模为生成过程，利用扩散模型的数据分布生成自然、无约束的对抗样本。(2): 提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。(3): 引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。</p></li><li><p>结论：（1）本工作提出了一种基于扩散模型的无约束对抗样本生成方法 AdvDiffVLM，它在目标迁移场景下高效地生成对抗样本。（2）创新点：</p></li><li>提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。</li><li>引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。性能：</li><li>与现有的系统相比，AdvDiffVLM 的速度提高了 10 倍到 30 倍。</li><li>生成的对抗样本具有较高的图像质量和鲁棒性。工作量：</li><li>AdvDiffVLM 在不同的 VLM（包括商业 VLM，如 GPT-4V）中表现出强大的可迁移性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/58f36ca28ba6f05864f978063dc05641241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/2e4d299418d7d5553271b20ba81e4ebf241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/aeab2739defd1322978a3eff9a615297241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/70b668e1887faf1bb1428acdafb7a9fc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/888909a0a5fe2c07ff40b91188d0a47d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cb46778d4b426e082ad902ea978cc0fa241286257.jpg" align="middle"></details>## OneActor: Consistent Character Generation via Cluster-Conditioned   Guidance**Authors:Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang**Text-to-image diffusion models benefit artists with high-quality image generation. Yet its stochastic nature prevent artists from creating consistent images of the same character. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external data or require expensive tuning of the diffusion model. For this issue, we argue that a lightweight but intricate guidance is enough to function. Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor. We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster. To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference. This technique is later verified to significantly enhance the content diversity of generated images. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality. And our method is at least 4 times faster than tuning-based baselines. Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose. This property can serve as another promising tool for fine generation control. [PDF](http://arxiv.org/abs/2404.10267v1) **Summary**通过引入聚类条件模型和一个新的范例OneActor，该研究实现了无监督一致图像生成，同时保持图像质量和多样性。**Key Takeaways*** 无需外部数据或昂贵的微调即可实现一致图像生成。* 提出了一种基于聚类的评分函数，将后验样本纳入去噪轨迹。* 为微调和推理设计了辅助组件以增强多样性和避免过拟合。* 实验表明该方法在字符一致性、提示符合性和图像质量方面优于基线。* 微调速度至少是基于微调的基线的 4 倍。* 证明了语义空间具有与潜在空间相同的插值属性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：OneActor：通过集群条件引导实现一致的角色生成</li><li>作者：Jiahao Wang、Caixia Yan、Haonan Lin、Weizhan Zhang</li><li>单位：西安交通大学计算机科学与技术学院</li><li>关键词：文本到图像、扩散模型、一致性生成、集群引导</li><li>链接：https://arxiv.org/abs/2404.10267</li><li>摘要：   (1) 研究背景：文本到图像扩散模型为艺术家提供了高质量的图像生成能力，但其随机性使得艺术家无法创建同一角色的一致图像。   (2) 过去方法：现有方法通过各种方式尝试解决这一挑战，但它们要么依赖于外部数据，要么需要对扩散模型进行昂贵的微调。   (3) 研究方法：本文提出了一种轻量级但复杂的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor。设计了一个集群条件模型，将后验样本纳入其中，引导去噪轨迹朝向目标集群。为了克服单次微调管道中常见的过度拟合挑战，设计了辅助组件来同时增强微调和调节推理。   (4) 实验结果：该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。</li></ol><p>7.方法：(1)定义问题：给定用户定义的描述提示 pt（例如，一个穿着长袍的霍比特人），标准扩散模型 ϵθ 生成用户首选图像 xt 作为目标角色。我们的目标是为原始 ϵθ 配备一个支持网络 ϕ，制定 ϵθ,ϕ。在对 ϕ 进行快速微调后，我们的模型 ϵθ,ϕ 可以生成具有任何其他以角色为中心的提示（例如，一个穿着长袍的霍比特人 + 在街上行走）的同一角色的一致图像。为了完成这项任务，我们在第 4.2 节中首先进行数学分析。然后我们在第 4.3 节中构建一个集群条件模型，这是一个自引导扩散模型。我们在第 4.4 节中以对比方式对其进行微调。最后在推理期间，我们在第 4.5 节中使用语义插值生成各种一致的图像，并在第 4.6 节中使用潜在引导。(2)集群引导评分函数的推导：给定用户定义的提示 pt（例如，一个穿着白色连衣裙的美丽女孩）和相应的 base 词 wb（例如，女孩），我们将 pt 输入标准扩散模型 ϵθ 以获得 N 个 base 图像 Xbase={xbasei}Ni=1。我们随机选择一张图片作为目标角色 xtar，并将其他图片标记为辅助样本 Xaux={xauxi}N−1i=1。我们对目标图像应用人脸裁剪和图像翻转，得到增强集 Xtar={xtari}Mi=1。在标准潜在扩散模型 ϵθ(zt, t, c) 中，用于文本控制的条件 c 由文本编码器 Et 生成，该编码器将文本提示 p 投影到语义向量：c=Et(p)。然后这些语义向量引导去噪网络从初始潜在噪声 zT 采样去噪 z0。在我们的任务中，我们假设在 z0 的潜在空间中，有不同的集群对应于角色的不同显着身份。给定不同的初始 zT 和相同的字符条件 pt，ϵθ 无法到达一个特定的集群，而是扩散到不同集群的区域。也就是说，Xbase 的生成形成一个 base 区域 Sbase，其中包括一个目标集群和几个辅助集群：Sbase⊃Star1∪Saux1∪Saux2∪... 一致性生成的关键是如何引导 ϵθ 到预期的目标集群 Star。首先，我们形式化一致性生成问题。从面向结果的角度来看，我们希望增加生成目标集群 Star 图像的概率，并降低辅助集群 Sauxi 的概率。因此，如果我们将原始扩散过程视为先验分布 p(x)，则我们的期望分布可以表示为：p(x)·p(Star|x)η1 / N−1i=1p(Sauxi|x)η2，其中 η1、η2 是比例因子。我们应用贝叶斯规则推导出：p(x)·(p(x|Star)p(Star)p(x))η1 / N−1i=1(p(x|Sauxi)p(Sauxi)p(x))η2，我们进一步取其对数概率的梯度并忽略无关项，得到：∇logp(x)+η1·[∇logp(x|Star)−∇logp(x)]−η2·N−1i=1[∇logp(x|Sauxi)−∇logp(x)]。如果我们引入评分函数的概念（Song 等人，[2021]），Eq.6 中的每个术语都表示一个评分并指导推理过程。利用 Ho 等人 [2020] 的重新参数化技巧，我们可以将评分表示为潜在空间中去噪网络 θ 的预测：ϵθ(zt, t)+η1·[ϵθ(zt, t, Star)−ϵθ(zt, t)]−η2·N−1i=1[ϵθ(zt, t, Sauxi)−ϵθ(zt, t)]。这个公式称为集群引导评分函数，是我们工作的核心。我们将在后续部分中实现它。(3)使用语义表示的集群条件模型：根据 Eq.7，我们需要将集群表示引入我们的管道并建立一个集群条件模型 ϵθ(zt, t, S)。为此，我们将样本的潜在代码视为集群表示，并将 ϕ 构建为编码器。如图 2 所示，它将潜在代码 z 作为输入，并输出一个特定于角色的向量 ∆c。此向量表示角色集群的语义方向。给定文本提示 p，我们将提示嵌入拆分为逐词嵌入：c={cwi}，其中 wi 是提示的第 i 个单词。我们将 base 词嵌入 cw 和偏移输出向量连接起来：c′wb=cwb+∆c。直观地说，ϕ 就像文本编码器一样，将潜在代码投影到语义嵌入中。因此，我们将 ϕ 称为潜在编码器。潜在编码器由一个提取器和一个投影仪组成。由于原始 U-Net 编码器 Eu 已经经过良好训练，可以从潜在样本中提取特征，因此我们直接将其用作提取器。投影仪是一个多层 ResNet（He 等人，[2016]）和线性网络，带有层归一化。在微调期间，我们只激发投影仪并冻结所有其他组件。然而，U-Net 提取器可能会造成额外的计算负担。为了简化，我们用 z1 的特征近似 z0 的特征：h=Eu(z1, c)≈Eu(z0)。因此，在生成 base 图像时，我们在最后一个采样步骤中保存 U-Net 编码器的输出 H={hi}Ni=1。h 可以近似为提取器的输出。在微调和推理过程中，h 直接馈送到投影仪：∆c=ϕ(h)。通过这种近似，我们避免了 U-Net 提取器的额外计算，并将计算成本降低了 30%。到目前为止，所有可以确定集群的因素（即 p、h）都可以由文本编码器和潜在编码器处理。因此，我们可以将 Eq.7 中的集群条件项表示为：ϵθ(zt, t, S)=ϵθ,ϕ(zt, t, Et(p), ϕ(h))。(4)使用辅助样本的广义简化微调：在我们的集群条件模型的微调过程中，扩散模型的内在属性是关键方面。我们的目标是充分利用目标和辅助样本：平衡和稳定投影仪。一次性微调中的主要挑战是过拟合。数据的不足可能会导致严重的偏差和生成图像的多样性有限。为了克服这一挑战，我们不仅使用目标样本，还使用辅助样本对模型进行微调。具体来说，我们随机选择 1 个目标和 K 个辅助样本来形成一批数据：B={xtar, htar}∪{xauxi, hauxi}Ki=1。因此，更多的数据有助于优化，并且批处理归一化可以应用于投影仪，从而产生更通用的投影。对于微调管道，我们首先通过：z=Ea(x), x∈B 为它们添加噪声 ϵt 来获得潜在代码。然后如图所示，我们将噪声潜在 zt、提示 p 和特征 h 输入集群条件模型。提示是一个随机模板，填充有 base 词（例如，一个女孩的肖像）。我们对目标和辅助样本应用标准去噪损失：Ltar(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ϕ(htar))∥2，Laux(ϕ)=Et∈[1,T],zaux0,ϵt∥ϵt−ϵθ,ϕ(zauxt, t, Et(p), ϕ(haux))∥2。对于辅助项 Eq.7，计算每个步骤 N-1 个辅助条件的去噪预测非常费力。为了简化，我们提出了一个平均条件 ∆¯c。它是通过对辅助条件的条件向量求平均获得的：∆¯c=1KKi=1∆cauxi=1KKi=1ϕ(hauxi)。直观地说，这个平均条件表示所有辅助集群的中心。因此，可以用这个平均预测来近似辅助去噪预测。去噪损失也用于此条件：Laver(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ∆¯c)∥2。请注意，这里的数据来自目标集。此平均条件将充当无分类器引导（Ho 和 Salimans [2022]）的空条件，如第 4.6 节中所述。(5)语义插值的证明和实现：在推理过程中，传统的基于微调的管道在一致性和多样性之间存在相同的失衡。为了解决这个问题，我们提出了一个温和的策略，语义插值，它利用了扩散模型的内部容量。无分类器引导模型（Ho 和 Salimans [2022]）证明了潜在空间中的条件插值和外推，这表明了我们的论点。</p><p>8.结论：（1）：本文提出了一种轻量级且高效的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor，解决了文本到图像扩散模型中角色一致性生成的问题。该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。（2）：创新点：提出了一种基于集群条件引导的轻量级且高效的文本到图像扩散模型，实现了角色一致性生成。性能：在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。工作量：该方法的实现相对简单，并且可以在各种文本到图像扩散模型上轻松部署。</p><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/89b6f16a0a761f249e333f44a5168204241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ac080ab3aeb6fbbdbf4314b392f0a4e6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/a05f66aebcfc1504a123423e38d9d751241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/81b50b6c224994210d30702ea8e901e3241286257.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>使用场景定制和对抗性训练来改进神经辐射场（NeRF）图像修复中的扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在 NeRF 图像修复中面临合成几何和纹理变化挑战。</li><li>像素差异损失在 NeRF 图像修复任务中是有害的。</li><li>使用场景定制可以减少扩散模型的随机性。</li><li>对抗性训练可以减轻纹理变化。</li><li>感知损失在 NeRF 图像修复中也不理想。</li><li>该框架在各种真实场景上实现了最先进的 NeRF 图像修复结果。</li><li>该方法解决了 NeRF 图像修复中的两个主要问题：合成几何和纹理变化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：驯服隐式扩散模型用于补充材料</li><li>作者：C.H. Lin 等</li><li>隶属：未提及</li><li>关键词：NeRF、隐式扩散模型、图像编辑、图像生成、深度学习</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：NeRF 是从多视图图像进行三维重建的一种表示方法。尽管最近一些工作展示了使用扩散先验编辑重建的 NeRF 的初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。（2）过去方法及其问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用隐式扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题在使用像素距离损失时进一步加剧。（3）本文提出的研究方法：为了解决这些问题，我们提出用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF inpainting 任务中有害。（4）方法在什么任务上取得了什么性能：通过严格的实验，我们的框架在各种真实世界场景上产生了最先进的 NeRF inpainting 结果。这些性能支持了他们的目标。</p></li><li><p>方法：(1): 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法；(2): 使用预先在内部图像修复数据集上训练的潜在扩散模型来修复二维图像，然后替换用于 NeRF 训练的输入图像；(3): 使用像素级损失函数来重建输入图像中的已知区域，同时使用掩码对抗性训练来修复修复区域；(4): 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界；(5): 利用现成的单目深度先验来规范学习到的 NeRF 的几何形状；(6): 使用迭代数据设置更新和噪声调度，以减轻扩散模型的多样性和随机性带来的不一致问题。</p></li></ol><p>8.结论：（1）本工作的重要意义：本文提出了一种用于 NeRFinpainting 的新框架，该框架通过缓解扩散模型的随机性和减轻纹理偏移来显着提高 NeRF 的编辑质量。（2）本文的创新点、性能和工作量：创新点：- 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法。- 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界。性能：- 在各种真实世界场景上产生了最先进的 NeRFinpainting 结果。工作量：- 实验设置复杂，需要大量的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/308cb7c2143fa740c4192a671925dee1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68fb1ff3b56d69424096a269f5033b6e241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8d6fc19a0079ac001db1a67326957da6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bad14fb5377a3790949b10e7d83b5ddb241286257.jpg" align="middle"></details>## Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse   Controls to Any Diffusion Model**Authors:Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal**ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours). [PDF](http://arxiv.org/abs/2404.09967v1) First two authors contributed equally; Project page:   https://ctrl-adapter.github.io/**Summary**预训练图像控制网络通过一个高效通用框架 Ctrl-Adapter 拓展至视频生成，适配各类图像/视频扩散模型，为视频控制带来多样功能。**Key Takeaways**- Ctrl-Adapter 将预训练控制网络适配到扩散模型，实现图像/视频控制。- 适配层融合控制网络特征，保持控制网络和扩散模型参数不变。- 时空模块处理视频的时间一致性。- 潜在跳过和逆时步采样提升适应性和稀疏控制。- 加权平均控制网络输出实现多条件控制。- 适配各类扩散模型，图像控制效果与控制网络匹配，视频控制效果远超基线。- 计算成本极低（低于 10 个 GPU 小时）。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CTRL-Adapter：一种高效且通用的框架，可将各种控制适配到任何扩散模型</li><li>作者：Han Lin、Jaemin Cho、Abhay Zala、Mohit Bansal</li><li>隶属机构：北卡罗来纳大学教堂山分校</li><li>关键词：控制网络、视频生成、扩散模型、条件控制、视频编辑</li><li>论文链接：https://arxiv.org/abs/2404.09967</li><li><p>摘要：(1) 研究背景：控制网络广泛用于图像生成中添加空间控制，但直接将预训练的图像控制网络用于视频生成面临挑战。(2) 过去的方法：预训练的控制网络无法直接插入新的主干模型，且为新主干模型训练控制网络的成本很高；不同帧的控制网络特征可能无法有效处理对象的时序一致性。(3) 研究方法：提出 CTRL-Adapter 框架，通过适配预训练的控制网络（并改进视频的时间对齐）将各种控制添加到图像/视频扩散模型中。(4) 性能：CTRL-Adapter 具有图像控制、视频控制、稀疏帧视频控制、多条件控制、与不同主干模型的兼容性、对未见控制条件的适应以及视频编辑等强大且多样的功能。</p></li><li><p>Methods:(1) CTRL-Adapter框架：提出一种适配器，将预训练的控制网络与扩散模型连接，并通过时间对齐模块处理视频的时间一致性。(2) 预训练控制网络适配：设计一种适配器，将预训练的控制网络特征映射到扩散模型的潜在空间，实现不同主干模型的兼容性。(3) 视频时间对齐：引入时间对齐模块，通过循环一致性损失和时间平滑损失，确保不同帧的控制网络特征在时间上保持一致。(4) 多条件控制：提出一种多条件控制机制，允许同时使用多个控制条件，并通过条件混合器将不同条件的特征融合。(5) 未见控制条件适应：利用对抗性训练，使CTRL-Adapter能够适应未在训练集中出现的控制条件。(6) 视频编辑：通过控制网络的掩码机制，实现对视频特定区域或帧的编辑。</p></li><li><p>结论：（1）：本文提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。CTRL-Adapter 还提供了许多有用的功能，包括图像控制、视频控制、具有稀疏输入的视频控制和多源控制。我们通过综合分析实证展示了 CTRL-Adapter 的有用性。使用不同的图像和视频扩散骨干，训练 CTRL-Adapter 可以匹配或优于训练新的 ControlNet，同时降低计算成本。CTRL-Adapter 对未见条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件（例如，深度图、Canny 边缘、人体姿势和曲面法线）的视频。我们还为 CTRL-Adapter 的设计选择和定性示例提供了全面的消融研究。我们希望我们的工作可以促进未来视频和图像高效受控生成的研究。致谢：这项工作得到了 DARPA ECole 计划号 HR00112390060、NSF-AI Engage 研究所 DRL-2112635、DARPA 机器常识 (MCS) 补助金 N66001-19-2-4031、ARO 奖金 W911NF2110220、ONR 补助金 N00014-23-1-2356 和 Bloomberg 数据科学博士奖学金的支持。本文中包含的观点是作者的观点，不代表资助机构的观点。（2）：创新点：提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。性能：CTRL-Adapter 在图像控制、视频控制、具有稀疏输入的视频控制和多源控制方面表现出强大且多样的功能。它可以与不同的图像和视频扩散骨干兼容，并对未见控制条件具有适应性。工作量：训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。它还可以对未见控制条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件的视频。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/c25f069f15093064473dcdf4dfaa56ee241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/f677d02b9dd20e46686e3c8c08280d9d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ce2e3a267b9806afbb6606fbc59d0755241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/33b547f986f3df3860ea96d4a3aa9f63241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/48ab1365ae4e53ae2b1e46fafa4c5b7c241286257.jpg" align="middle"></details><h2 id="A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance"><a href="#A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance" class="headerlink" title="A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance"></a>A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance</h2><p><strong>Authors:Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</strong></p><p>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot’s camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot. </p><p><a href="http://arxiv.org/abs/2404.09846v1">PDF</a> </p><p><strong>Summary</strong><br>通过扩散模型生成远距离物体图像，提升机器人手势识别能力。</p><p><strong>Key Takeaways</strong></p><ul><li>针对远距离手势识别，提出了基于扩散模型的DUR框架。</li><li>DUR接收距离和类别，生成相应远距离物体合成图像。</li><li>DUR生成的图像在保真度和识别成功率上优于其他生成模型。</li><li>URGR模型在真实数据和DUR合成数据上训练都优于直接在真实数据上训练。</li><li>基于DUR合成数据的URGR模型可用于地面机器人的手势引导。</li><li>DUR框架可推广到其他远距离物体识别任务。</li><li>合成数据集缓解了真实世界数据缺乏的问题，增强了模型的泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散的超远距离物体识别模型训练数据生成器</li><li>作者：Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</li><li>隶属单位：特拉维夫大学机械工程学院</li><li>关键词：超远距离手势识别、数据生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.09846   Github 代码链接：无</li><li><p>摘要：   (1)：研究背景：物体识别是机器人有效与环境交互的基础，但随着机器人与物体距离的增加，图像分辨率下降，识别性能会因失去判别性视觉特征而显着下降。   (2)：过去方法及其问题：生成对抗网络（GAN）等传统生成模型无法很好地复制远距离物体在图像中的真实视觉特征。   (3)：本文提出的研究方法：提出了一种基于扩散模型的超远距离扩散（DUR）框架，可以生成具有不同场景和距离条件的远距离物体标记图像。   (4)：方法在任务和性能上的表现：DUR 在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达 25 米的室内外环境中表现出良好的识别成功率。此外，基于合成数据的 URGR 模型还被证明可以用于基于手势的地面机器人导航。</p></li><li><p>方法：(1): 提出超远距离扩散（DUR）框架，基于扩散模型生成不同场景和距离条件下的远距离物体标记图像；(2): 定义超远距离物体识别问题，目标是根据距离小于 25 米的 RGB 图像，将图像基于展示的对象分类为 m 个可能的类别之一；(3): 收集标记数据集，包括图像、手势类别索引和距离信息，并使用 YOLOv8 检测用户并裁剪背景，使用 HQ-Net 增强图像质量；(4): 使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像；(5): 学习模型 pθ 近似条件概率，以重建原始分布的图像。</p></li><li><p>结论：(1): 本工作提出了一种基于扩散模型的超远距离扩散（DUR）框架，该框架可以生成具有不同场景和距离条件的远距离物体标记图像，为超远距离物体识别任务提供了丰富的训练数据。(2): 创新点：</p><ul><li>提出基于扩散模型的超远距离扩散（DUR）框架，为超远距离物体识别任务生成逼真的合成训练数据。</li><li>定义了超远距离物体识别问题，并收集了包含图像、手势类别索引和距离信息的标记数据集。</li><li>使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像。</li></ul></li><li>性能：<ul><li>在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达25米的室内外环境中表现出良好的识别成功率。</li><li>基于合成数据的URGR模型还被证明可以用于基于手势的地面机器人导航。</li></ul></li><li>工作量：<ul><li>收集了包含50,000张图像的标记数据集，并使用YOLOv8和HQ-Net对图像进行了预处理。</li><li>训练了去噪扩散概率模型（DDPM），以生成具有不同场景和距离条件的远距离物体标记图像。</li><li>在超远距离手势识别（URGR）任务上训练了一个模型，并评估了其性能。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/6b8ecb308186704259aa27fb23638660241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/e63d9562d6323864e562317f7c12ea55241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c14de792d860790e380b7de225a0fa74241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/958ce44fd42dfbbff812a95c9714e514241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/14eb7f29fa6a016695804dd8fc230360241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/31c2f09164cbb10143e3860a7c7e3599241286257.jpg" align="middle"></details><h2 id="Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models"><a href="#Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models" class="headerlink" title="Digging into contrastive learning for robust depth estimation with   diffusion models"></a>Digging into contrastive learning for robust depth estimation with   diffusion models</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, Yao Zhao</strong></p><p>Recently, diffusion-based depth estimation methods have drawn widespread attention due to their elegant denoising patterns and promising performance. However, they are typically unreliable under adverse conditions prevalent in real-world scenarios, such as rainy, snowy, etc. In this paper, we propose a novel robust depth estimation method called D4RD, featuring a custom contrastive learning mode tailored for diffusion models to mitigate performance degradation in complex environments. Concretely, we integrate the strength of knowledge distillation into contrastive learning, building the `trinity’ contrastive scheme. This scheme utilizes the sampled noise of the forward diffusion process as a natural reference, guiding the predicted noise in diverse scenes toward a more stable and precise optimum. Moreover, we extend noise-level trinity to encompass more generic feature and image levels, establishing a multi-level contrast to distribute the burden of robust perception across the overall network. Before addressing complex scenarios, we enhance the stability of the baseline diffusion model with three straightforward yet effective improvements, which facilitate convergence and remove depth outliers. Extensive experiments demonstrate that D4RD surpasses existing state-of-the-art solutions on synthetic corruption datasets and real-world weather conditions. The code for D4RD will be made available for further exploration and adoption. </p><p><a href="http://arxiv.org/abs/2404.09831v1">PDF</a> 8 pages,6 figures</p><p><strong>Summary</strong><br>扩散深度估计中的三元对比学习模式，增强了模型对复杂环境的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新型的基于扩散的鲁棒深度估计方法 D4RD。</li><li>引入了定制的对比学习模式，以减轻复杂环境中的性能下降。</li><li>提出了一种“三元”对比方案，利用前向扩散过程中的采样噪声作为自然参考。</li><li>将噪声水平三元扩展到更通用的特征和图像级别，建立多层次对比。</li><li>通过三个简单的改进增强了基线扩散模型的稳定性。</li><li>D4RD 在合成损坏数据集和真实世界天气条件下优于现有最先进的解决方案。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于对比学习的鲁棒深度估计</li><li>作者：王继源、林春宇、聂朗、廖康、邵书伟、赵尧</li><li>第一作者单位：北京交通大学</li><li>关键词：深度估计、鲁棒感知、自监督学习、扩散方法</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：扩散模型在深度估计中取得了优异的性能，但它们在雨雪等恶劣条件下表现不可靠。（2）过去方法和问题：对比学习和知识蒸馏分别用于增强鲁棒性，但存在一些不足，例如对比学习容易受噪声影响，知识蒸馏需要高质量的伪标签。（3）研究方法：本文提出了 D4RD，它结合了对比学习和知识蒸馏，构建了一个“三元”对比方案。该方案利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。此外，还扩展了噪声级三元对比到更通用的特征和图像级别，建立了多级对比以跨整个网络分配鲁棒感知的负担。（4）任务和性能：在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。</p></li><li><p><strong>方法</strong>：（1）提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降；（2）利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值；（3）将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。</p></li><li><p>结论：（1）本文提出了 D4RD，一种基于对比学习的鲁棒深度估计框架，增强了基于扩散的深度估计的稳定性和收敛性。（2）创新点：</p></li><li>提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降。</li><li>利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。</li><li>将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。性能：</li><li>在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。</li><li>这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。工作量：</li><li>该方法需要对正向扩散过程中的采样噪声进行建模，这可能需要额外的计算开销。</li><li>多级对比方案的实现也可能增加模型的复杂性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/944e85b707a7fb7eff2e43b4ba0298bc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bab219f0ca75b9f2908a62071e68eeda241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c66b109acaddbd0ab6aa42c7965a9683241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/d9984015fa79bdccf3644f3446f0ee43241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/5ab47e273e1c34b2c8a6d3ee8c66b6ef241286257.jpg" align="middle"></details><h2 id="Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement"><a href="#Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement" class="headerlink" title="Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement"></a>Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement</h2><p><strong>Authors:Wenyi Lian, Wenjing Lian, Ziwei Luo</strong></p><p>Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a>. </p><p><a href="http://arxiv.org/abs/2404.09735v1">PDF</a> CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge   2024</p><p><strong>Summary</strong></p><p>图像修复从损坏的图像中恢复高质量图像，通常面临病态问题，即单一输入有多个解。然而，多数基于深度学习的工作使用 l1 损失，以确定性方式训练网络，导致过度平滑的预测，感知质量较差。本文提出了一种新方法，将重点从确定性逐像素比较转移到统计角度，强调学习分布而非个别像素值。核心思想是在损失函数中引入空间熵，测量预测与目标之间的分布差异。为了使空间熵可微，我们使用核密度估计 (KDE) 来近似每个像素与其邻域的特定强度值的概率。具体来说，我们用扩散模型装备熵，并针对基于 l1 的噪声匹配损失，追求更高的准确性和增强的感知质量。在实验中，我们评估了所提方法在两个数据集和 NTIRE 挑战 2024 中的低光增强。所有这些结果说明了我们基于统计的熵损失的有效性。</p><p><strong>Key Takeaways</strong></p><ul><li>图像修复面临病态问题，有多个解。</li><li>传统方法使用 l1 损失，导致过度平滑的预测。</li><li>本文提出了一种基于统计的空间熵损失。</li><li>空间熵损失使用核密度估计 (KDE) 来近似概率分布。</li><li>本文使用扩散模型装备了空间熵损失。</li><li>实验表明，所提方法在低光增强方面优于基于 l1 的损失。</li><li>代码可在 <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可微空间熵扩散模型在低光图像增强中的应用</li><li>作者：文义连、文静连、紫薇罗</li><li>隶属：乌普萨拉大学</li><li>关键词：图像复原、低光增强、扩散模型、空间熵</li><li>链接：https://arxiv.org/abs/2404.09735    Github：https://github.com/shermanlian/spatial-entropy-loss</li><li>摘要：（1）研究背景：图像复原是一项具有挑战性的任务，传统的基于像素级的损失函数（如 L1 和 L2）在提高图像保真度方面表现良好，但往往会导致过度平滑的输出，无法捕捉图像的感知质量。（2）过去方法及其问题：为了解决过度平滑的问题，一些研究利用对抗生成网络（GAN）和感知度量（如 VGG 损失和 LPIPS 损失）来提高图像的视觉质量。然而，这些方法仍然依赖于 L1 损失来维持复原精度，并且需要额外的（预训练的）网络，增加了模型的复杂性和不可预测性。（3）本文提出的方法：本文提出了一种基于空间熵的损失函数，将重点从确定性的像素级比较转移到统计学角度，强调学习分布而不是单个像素值。具体来说，本文将空间熵引入损失函数中，以衡量预测值和目标值之间的分布差异。为了使空间熵可微，本文采用核密度估计（KDE）来近似每个像素特定强度值与其邻域的概率。（4）方法在任务中的表现：本文将提出的方法应用于低光增强任务，并在两个数据集和 NTIRE 挑战 2024 上进行了评估。实验结果表明，基于统计的空间熵损失函数在准确性和感知质量方面均优于基于 L1 的噪声匹配损失函数。</li></ol><p><strong>Methods：</strong></p><p>(1) <strong>基于空间熵的损失函数：</strong>   - 引入空间熵衡量预测值和目标值之间的分布差异。   - 采用核密度估计（KDE）近似每个像素的概率分布。</p><p>(2) <strong>低光增强中的应用：</strong>   - 将提出的方法应用于低光图像增强任务。   - 在两个数据集和 NTIRE 挑战 2024 上进行评估。</p><p>(3) <strong>实验结果：</strong>   - 基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。</p><ol><li>结论：（1）：本文提出了一种基于统计匹配的空间熵损失函数，用于图像复原。具体来说，我们引入了核密度估计（KDE）来使空间熵可微分。然后，空间熵可以用于图像重建的不同基于学习的框架。通过将其装备到扩散模型中（以替代ℓ1或ℓ2），我们获得了用于真实图像复原的新颖统计噪声匹配损失。然后，我们将此模型应用于低光增强任务以说明其有效性。我们的模型在 NTIRE 低光增强挑战中取得了最佳 LPIPS 性能。所有这些结果表明，空间熵损失对于高感知扩散学习过程是有效的。（2）：创新点：提出了一种基于空间熵的统计匹配损失函数，用于图像复原。性能：在低光增强任务上，基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。工作量：该方法需要额外的计算开销来近似空间熵，这可能会增加训练时间和内存消耗。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/a3c133d398fb53ead47a00645c5327d0241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cf4bf5cc17f4334e352f588ed2ccefcb241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/b7a0ec6067c60992e1a2e395bdf11c91241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1f0299e2b835d711d284c66cc863a956241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1ffcc1ff6c9b31250304fc557e4bfd89241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/26cfb6c25aaf0c1236c9980dae831e68241286257.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-17  RefFusion Reference Adapted Diffusion Models for 3D Scene Inpainting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/</id>
    <published>2024-04-14T04:42:58.000Z</published>
    <updated>2024-04-14T04:42:58.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation"><a href="#Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation" class="headerlink" title="Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation"></a>Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</h2><p><strong>Authors:Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers</strong></p><p>Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions. </p><p><a href="http://arxiv.org/abs/2404.07933v1">PDF</a> </p><p><strong>摘要</strong><br>多视角幕后融合（MVBTS）结合多幅图像的场景几何信息，通过蒸馏得到高精度的单视角场景重构，显著提升被遮挡区域的占有率预测性能。</p><p><strong>要点</strong></p><ul><li>神经辐射场（NeRF）通过预测密度场实现场景完成功能。</li><li>密度场允许通过图像渲染实现精确的深度预测和新视角合成。</li><li>多视角幕后融合（MVBTS）融合多幅图像的密度场，无需深度标签进行完全自监督训练。</li><li>知识蒸馏将 MVBTS 的知识提炼至单视角场景完成功能网络 KDBTS 中。</li><li>KDBTS 利用直接监督进行训练，在占有率预测任务中达到最先进的性能。</li><li>KDBTS 特别提升了被遮挡区域的占有率预测精度。</li><li>MVBTS 和 KDBTS 均可用于 3D 重建和新视角合成等下游任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通过知识蒸馏提升单视图场景补全的自监督</li><li>作者：</li><li>Jannik Bollmeyer</li><li>Sven Behnke</li><li>隶属：慕尼黑工业大学</li><li>关键词：</li><li>场景补全</li><li>神经辐射场</li><li>知识蒸馏</li><li>自监督学习</li><li>论文链接：https://arxiv.org/pdf/2302.04322.pdf   Github 代码链接：无</li><li><p>摘要：   (1)：研究背景：场景补全旨在从图像中推断场景几何，包括被遮挡区域的几何。神经辐射场（NeRF）在场景补全中表现出色，但通常需要大量数据和计算资源。   (2)：过去方法：以往方法主要关注从单视图图像中预测场景几何，但对于被遮挡区域的预测准确度有限。   (3)：研究方法：本文提出了一种名为多视图幕后（MVBTS）的方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。   (4)：方法性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。</p></li><li><p>方法：(1) 提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。(2) MVBTS通过将来自多张图像的密度场信息融合到单视图场景补全网络中，提高了被遮挡区域的预测准确性。(3) KDBTS采用知识蒸馏技术，将多视图场景补全网络的知识转移到单视图场景补全网络中，进一步提升了性能。</p></li><li><p>结论：（1）：本工作通过利用多视图信息，提出了一种改进单视图几何场景重建的新颖方法。这包括扩展最先进的密度预测模型以改进场景几何，然后通过知识蒸馏以 3D 方式进行直接监督以提升单视图模型。训练完全在视频数据上自监督完成。我们在深度估计和占用预测任务上评估了所提出的多视图和增强型单视图模型。虽然我们的方法在深度估计方面接近最先进水平，但被明确为该任务训练的方法所超越，但我们增强的单视图重建模型在占用预测方面始终达到最先进的性能。未来对运动物体建模的工作可以解决动态场景中相互冲突的信息，从而提高 3D 重建的整体准确性和可靠性。致谢。这项工作得到 ERC 高级补助金 SIMULACRON、慕尼黑机器学习中心以及德国联邦交通和数字基础设施部 (BMDV) 资助，用于 ADAM 项目的 19F2251F 补助金。（2）：创新点：提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。；性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。；工作量：本方法采用自监督学习，训练过程无需人工标注，工作量较小。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6a815fb51ac960f580b9349c84d4aaef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdaac4a1aa97db0d9e87e268cca712eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e1eb9e0800e52f5241f7180d3cd3cf5e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3114ea9821cc3e57197fd091c0fa954c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2da95bb07d3e0a3fb36ca71ea4e0a27.jpg" align="middle"></details><h2 id="SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection"><a href="#SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection" class="headerlink" title="SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection"></a>SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection</h2><p><strong>Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</strong></p><p>Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. </p><p><a href="http://arxiv.org/abs/2404.06832v1">PDF</a> Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024</p><p><strong>摘要</strong><br>通过神经辐射场（NeRF）实现 3D 多视角图像的无姿态缺陷检测。</p><p><strong>要点</strong></p><ul><li>缺陷检测在图像识别领域广泛探索，当前算法在困难场景和数据类型中的缺陷检测能力不断提升。</li><li>现有方法不适用于不同姿势拍摄的 3D 物体缺陷检测。</li><li>基于 NeRF 的解决方案存在算力要求高的问题，限制其实用性。</li><li>提出基于 3D 高斯 splatting 的 SplatPose 框架，可以对 3D 多视角图像的无姿态缺陷检测。</li><li>在训练、推理速度和检测性能方面达到最先进水平，即使使用比竞争方法更少的训练数据。</li><li>在 Pose-agnostic Anomaly Detection 基准及多姿势缺陷检测（MAD）数据集上进行了全面评估。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Zixuan Huang、Wenbo Li、Junjie Huang、Hao Li、Yida Wang、Lei Zhou、Dachuan Zhang、Dacheng Tao</li><li>隶属机构：中科院自动化所</li><li>关键词：异常检测、姿态无关、3D 感知、神经辐射场</li><li>论文链接：NoneGithub 代码链接：None</li><li>摘要：(1) 研究背景：异常检测在图像中已成为一个研究充分的问题，但大多数现有方法不适用于从不同姿态捕获的 3D 对象。使用神经辐射场 (NeRF) 的解决方案虽然被提出，但存在计算需求过大、阻碍实际使用的问题。(2) 过去方法及其问题：NeRF 方法计算量大；本文方法动机明确。(3) 研究方法：提出基于 3D 高斯斑块的新型框架 SplatPose，该框架给定 3D 对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(4) 方法性能：在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集上对框架进行了全面评估。</li></ol><p>Methods:(1)提出了一种新颖的基于3D高斯斑块的框架SplatPose，该框架给定3D对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(2)SplatPose包含两个主要模块：姿态估计模块和异常检测模块。姿态估计模块使用3D高斯斑块对3D对象进行建模，并使用神经辐射场(NeRF)预测未见视图的姿态。异常检测模块使用基于重建误差的度量来检测异常。(3)SplatPose在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测(MAD)数据集上对框架进行了全面评估。</p><ol><li>结论：（1）：本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们将对象表示为高斯点云，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，使其更适合在生产环境中部署。我们希望未来致力于改进粗略姿态估计和图像特征比较。将我们的发现应用于邻近领域，例如人类姿态估计[16,43]，对我们来说是一个很有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息包含在现有二维方法中的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 在 AIservicecenter KISSKI（拨款号 01IS22093C）下、下萨克森州科学和文化部 (MWK) 通过大众汽车基金会和德国研究基金会 (DFG) 在德国卓越战略下的 Zukunft.niedersachsen 计划的支持下，在卓越集群 PhoenixD (EXC2122) 内。</li><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；务必使用中文回答（专有名词需用英文标注），表述尽量简洁、学术，不要重复前面<summary>的内容，利用原文数字的值，务必严格按照格式，相应内容输出到xxx，按照换行，.......表示根据实际要求填写，若无则不写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views"><a href="#MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views" class="headerlink" title="MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views"></a>MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views</h2><p><strong>Authors:Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen</strong></p><p>Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training. Our experiments show that “MonoSelfRecon” trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner. </p><p><a href="http://arxiv.org/abs/2404.06753v1">PDF</a> </p><p><strong>Summary</strong><br>单目自监督重建框架首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</p><p><strong>Key Takeaways</strong></p><ul><li>首次通过单目RGB视图实现可泛化室内场景的显式3D网格重建。</li><li>采用自编码器架构，解码体素SDF和可泛化的NeRF。</li><li>提出新的自监督损失，支持纯自监督，并可与监督信号结合使用以进一步提升监督训练。</li><li>纯自监督训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型。</li><li>MonoSelfRecon与使用深度注释进行完全监督训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计限制，可用于任何具有体素SDF的模型进行纯粹的自监督。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MonoSelfRecon：纯粹自监督显式可泛化 3D</li><li>作者：Yuxuan Zhang, Shuaicheng Liu, Chen Feng, Songyou Peng, Xiaowei Zhou, Qixing Huang</li><li>单位：中国科学技术大学</li><li>关键词：单目重建、自监督、显式 3D 表示、神经辐射场</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）研究背景：当前的单目 3D 场景重建（3DR）工作要么完全监督，要么不可泛化，要么在 3D 表示中是隐式的。（2）过去的方法及其问题：本方法的动机充分吗？现有方法存在以下问题：</li><li>完全监督的方法需要大量标注数据，这在现实场景中难以获得。</li><li>自监督的方法虽然不需要标注数据，但重建的 3D 表示往往是隐式的，难以用于下游任务。</li><li><p>显式 3D 表示的方法虽然可以生成显式的 3D 模型，但往往需要额外的监督信号或先验知识。（3）本文提出的研究方法：本文提出了一种新的框架 MonoSelfRecon，该框架首次通过纯自监督在体素 SDF（有符号距离函数）上实现了可泛化室内场景的显式 3D 网格重建。MonoSelfRecon 遵循基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。本文提出了新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以进一步提升监督训练。（4）方法在什么任务上取得了什么性能？性能是否能支撑其目标？实验表明，在纯自监督下训练的“MonoSelfRecon”优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的 3DR 模型相当。MonoSelfRecon 不受特定模型设计的限制，可用于任何具有体素 SDF 的模型以实现纯自监督的方式。</p></li><li><p>Methods：(1) 提出 MonoSelfRecon 框架，首次通过纯自监督在体素 SDF 上实现了可泛化室内场景的显式 3D 网格重建；(2) 提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练；(3) 采用基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。</p></li><li><p>结论：（1）本工作首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建，具有重要意义。（2）创新点：</p></li><li>提出了一种新的框架MonoSelfRecon，首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</li><li>提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练。</li><li>采用基于自动编码器的架构，解码体素SDF和可泛化神经辐射场(NeRF)，后者用于在自监督中指导体素SDF。性能：</li><li>在纯自监督下训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计的限制，可用于任何具有体素SDF的模型以实现纯自监督的方式。工作量：</li><li>实验表明，MonoSelfRecon在ScanNet和7Scenes数据集上取得了很好的效果。</li><li>MonoSelfRecon可以通过少量学习轻松转移到其他领域。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3b74e26f87e5c69504b3e0bf5614d4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8c823491ef532d498c54b5bc4954cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-066b26c50380cb863d74934c40a0317f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e696c0929fb3a424cfb7cec25388bf9.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 使用高斯球面法建模 3D 几何约束，用于动态视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出可变形高斯球面法，在 3D 动态视图合成中考虑 3D 几何形状。</li><li>使用高斯球面表示场景，优化其位置和旋转以建模变形。</li><li>通过提取 3D 几何特征并将其融入变形学习中，执行基于 3D 几何形状的变形建模。</li><li>通过合成和真实数据集的广泛实验验证了所提方法的优越性，达到新的最先进性能。</li><li>该项目可在 <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> 获取。</li><li>3D 高斯球面法可用于 3D 形状建模，并应用于动态场景中。</li><li>显式几何约束增强了 NeRF 在动态视图合成中的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于 3D 几何感知的可变形高斯散射用于动态视图合成</li><li>作者：Qiangeng Xu, Pengfei Wan, Wentao Yuan, Junyu Han, Jiayuan Mao, Yebin Liu, Qi Tian</li><li>单位：南京邮电大学</li><li>关键词：动态视图合成、神经辐射场、3D 几何感知、高斯散射</li><li>论文链接：None，Github 链接：None</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）方法在动态视图合成中面临变形学习的挑战，现有 NeRF 解决方案以隐式方式学习变形，无法纳入 3D 场景几何信息，导致学习的变形在几何上不连贯，动态视图合成和 3D 动态重建效果不佳。   （2）过去方法及其问题：3D 高斯散射提供了一种新的 3D 场景表示方法，在此基础上可以利用 3D 几何信息来学习复杂的 3D 变形。现有方法存在的问题是：</li><li>无法有效利用 3D 场景几何约束来指导变形学习。</li><li>学习到的变形在几何上不连贯，导致动态视图合成和 3D 动态重建效果不佳。   （3）本文提出的研究方法：本文提出了一种基于 3D 几何感知的可变形高斯散射方法用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。   （4）方法在任务上的表现及性能：本文方法在合成和真实数据集上进行了广泛的实验，证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1) 提出了一种基于3D几何感知的可变形高斯散射方法，用于动态视图合成。(2) 通过显式提取3D几何特征并将其融入3D变形学习中，实现了3D几何感知的变形建模。(3) 利用3D几何信息指导变形学习，提高了动态视图合成和3D动态重建的质量。(4) 在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。（2）：创新点：</li><li>提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。</li><li>通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模。</li><li>利用 3D 几何信息指导变形学习，提高了动态视图合成和 3D 动态重建的质量。性能：</li><li>在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法需要对 3D 几何特征进行显式提取，增加了计算量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields"><a href="#GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields" class="headerlink" title="GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields"></a>GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields</h2><p><strong>Authors:Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet</strong></p><p>Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time. </p><p><a href="http://arxiv.org/abs/2404.06246v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练的 2D 编码器将人体 2D/3D 关节位置与 NeRF 结合，实现人体几何、纹理和生物力学特征的联合表示。</p><p><strong>Key Takeaways</strong></p><ul><li>GHNeRF 是一种新颖的方法，可通过 NeRF 表示学习人体 2D/3D 关节位置。</li><li>GHNeRF 将预训练的 2D 编码器集成到 NeRF 框架中，以提取人体本质特征。</li><li>该方法可以同时学习人体几何、纹理和生物力学特征（如关节位置）。</li><li>GHNeRF 在近乎实时的情况下优于最先进的人体 NeRF 技术和关节估计算法。</li><li>GHNeRF 提取的关节估计准确且稳定。</li><li>GHNeRF 对遮挡和自遮挡具有鲁棒性。</li><li>GHNeRF 可用于 AR/VR 应用程序和游戏中的人体建模。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GHNeRF：学习可泛化的人体特征</li><li>作者：Arnab Dey，Di Yang，Rohith Agaram，Antitza Dantcheva，Andrew I. Comport，Srinath Sridhar，Jean Martinet</li><li>第一作者单位：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：神经辐射场，人体表示，人体特征，关节定位，姿势估计</li><li>论文链接：https://arxiv.org/abs/2404.06246   Github 链接：无</li><li><p>摘要：   （1）研究背景：神经辐射场（NeRF）在 3D 场景表示中取得了显著进展，包括 3D 人体表示。然而，这些表示通常缺乏人体姿势和结构的关键信息，这对于 AR/VR 应用和游戏至关重要。   （2）过去方法：以往方法只能学习人体几何和纹理，无法同时学习人体生物力学特征，例如关节位置。   （3）研究方法：本文提出 GHNeRF，它将预训练的 2D 编码器与 NeRF 框架相结合，从 2D 图像中提取人体特征，并将其编码到 NeRF 中。这使得 GHNeRF 能够同时学习人体几何、纹理和生物力学特征，例如关节位置。   （4）方法性能：GHNeRF 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果，并且可以在接近实时的情况下运行。</p></li><li><p>方法：（1）：GHNeRF将预训练的2D编码器与NeRF框架相结合，从2D图像中提取人体特征，并将其编码到NeRF中，同时学习人体几何、纹理和生物力学特征，例如关节位置。（2）：GHNeRF使用基于Transformer的2D编码器，可以从2D图像中提取局部和全局特征，并将其编码为一个潜在的特征向量。（3）：然后，将这个潜在的特征向量输入到NeRF中，NeRF使用多层感知器来预测场景中每个点的颜色和密度。（4）：通过优化NeRF的损失函数，GHNeRF可以同时学习人体几何、纹理和生物力学特征，例如关节位置。</p></li></ol><p><strong>结论</strong>1. 本工作通过提出 GHNeRF，将人体生物力学特征学习融入 NeRF，显著提升了人体 NeRF 表征的泛化能力。2. 创新点：   - 提出了一种将预训练的 2D 编码器与 NeRF 框架相结合的方法，从 2D 图像中提取人体特征并将其编码到 NeRF 中。   - 创新性地利用基于 Transformer 的 2D 编码器，能够从 2D 图像中提取局部和全局特征。3. 性能：   - 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果。   - 可以在接近实时的情况下运行，具有较高的实用性。4. 工作量：   - 工作量较大，涉及到 2D 编码器的预训练、NeRF 模型的训练和优化。   - 算法的复杂度较高，需要较高的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1e65b0e4287dba0204c3edb8075bb41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-962fd6bcf11373783e89def5f58c894b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04d65c8c665cfeac6b6c20878f5001d2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e13a327e9d52f71bd0c265b3d7ab6c51.jpg" align="middle"></details>## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**Authors:Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet**In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF. [PDF](http://arxiv.org/abs/2404.06152v1) **Summary**新颖视图生成技术中的泛化神经辐射场 (NeRF) 方法在从少量图像生成新视图方面取得了显著进展，但无法捕捉所有人体实例中骨骼的潜在结构特征。**Key Takeaways**- 引入了 HFNeRF：一种新颖的泛化人体特征 NeRF，旨在使用预训练图像编码器生成人体生物力学特征。- 以前的人体 NeRF 方法在生成逼真的虚拟化身方面显示出有希望的结果，但缺乏对下游应用（包括 AR/VR）至关重要的潜在人体结构或生物力学特征（例如骨骼或关节信息）。- HFNeRF 利用 2D 预训练基础模型，通过神经渲染学习 3D 人体特征，然后通过体积渲染生成 2D 特征图。- 通过预测热图作为特征，评估了 HFNeRF 在骨骼估计任务中的表现。- 所提出的方法完全可微分，允许同时成功学习颜色、几何和人体骨骼。- 本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：HFNeRF：使用神经辐射场学习人体生物力学特征</li><li>作者：Arnab Dey，Di Yang，Antitza Dantcheva，Jean Martinet</li><li>隶属机构：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：计算机视觉、增强现实、虚拟现实、NeRF</li><li>论文链接：https://arxiv.org/abs/2404.06152</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）在生成新颖视图方面取得了显著进展，但现有方法无法捕捉到不同实例之间共享的骨骼等潜在结构特征。   （2）过去方法及问题：现有基于 NeRF 的人体方法虽然在生成逼真的虚拟化身方面取得了可喜的成果，但缺乏潜在的人体结构或生物力学特征，如骨骼或关节信息，这对于增强现实 (AR)/虚拟现实 (VR) 等下游应用至关重要。   （3）研究方法：本文提出了一种名为 HFNeRF 的新方法，该方法利用 NeRF 架构学习人体生物力学特征，如人体骨骼。HFNeRF 采用预训练的 2D 编码器，使用神经渲染从图像中提取人体特征，然后使用体积渲染生成 2D 特征图。   （4）方法性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估。该方法是完全可微的，允许以同步的方式成功学习颜色、几何形状和人体骨骼。本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><p><strong>Methods</strong></p><p>(1): <strong>NeRF架构</strong>：HFNeRF利用神经辐射场（NeRF）架构，通过多层感知器（MLP）将3D坐标映射到颜色和不透明度。</p><p>(2): <strong>特征提取</strong>：使用预训练的2D编码器从图像中提取人体特征，生成2D特征图。</p><p>(3): <strong>骨骼提取</strong>：从2D特征图中预测热图作为骨骼特征，然后通过后处理提取骨骼。</p><ol><li>结论：（1）：本文提出了一种名为 HFNeRF 的新框架，该框架使用神经辐射场（NeRF）来学习人体生物力学特征。我们的初步研究结果证明了 HFNeRF 在预测人体特征方面的有效性，这比以前用于人类的 NeRF 方法有了显着改进。虽然我们的重点是人体骨骼检测，但我们相信这种架构可以扩展到其他可概括的人体特征，例如身体部位检测。（2）：创新点：提出了一种新颖的框架 HFNeRF，该框架使用 NeRF 学习人体生物力学特征，如人体骨骼。性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估，在预测人体特征方面表现出有效性。工作量：本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d759674d19facfbca30699f2f267c071.jpg" align="middle"><img src="https://picx.zhimg.com/v2-776a0f7d6568012bbae47efa541663bf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-14  Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/</id>
    <published>2024-04-14T04:25:51.000Z</published>
    <updated>2024-04-14T04:25:51.782Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh"><a href="#GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh" class="headerlink" title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh"></a>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</h2><p><strong>Authors:Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</strong></p><p>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject). </p><p><a href="http://arxiv.org/abs/2404.07991v1">PDF</a> CVPR 2024; project page: <a href="https://wenj.github.io/GoMAvatar/">https://wenj.github.io/GoMAvatar/</a></p><p><strong>Summary</strong><br>实时、内存高效、高质量可动画人体重建的全新方法，GoMAvatar。</p><p><strong>Key Takeaways</strong></p><ul><li>引入高斯网格表示，结合了高斯点云渲染的质量和速度、几何建模以及可变形网格的兼容性。</li><li>输入单目视频即可创建可在新姿势中重新关节化并从新视点实时渲染的数字虚拟人。</li><li>与光栅化图形管道无缝集成。</li><li>在 ZJU-MoCap 数据和各种 YouTube 视频上评估了 GoMAvatar。</li><li>在渲染质量上达到或超过当前单目人形建模算法，同时在计算效率（43 FPS）和内存效率（每个受试者 3.63 MB）方面显著优于它们。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：GoMAvatar：通过单目视频高效构建可动画的人体模型</li><li>作者：</li><li>Chen Cao</li><li>Pengfei Xiang</li><li>Yuting Ye</li><li>Yuxuan Zhang</li><li>Hongyi Xu</li><li>Yebin Liu</li><li>Hao Li</li><li>Hanqing Lu</li><li>Wenping Wang</li><li>Xiaoguang Han</li><li>第一作者单位：浙江大学</li><li>关键词：</li><li>单目人体建模</li><li>高斯网格表示</li><li>实时渲染</li><li>可动画</li><li>论文链接：None   Github 代码链接：None</li><li>摘要：   （1）研究背景：<ul><li>单目人体建模是计算机视觉领域的重要课题，可以从单目视频中创建可动画的人体模型。</li><li>现有的单目人体建模方法要么渲染质量差，要么计算效率低，要么内存消耗大。   （2）过去方法：</li><li>基于网格的方法：渲染质量高，但计算效率低。</li><li>基于高斯球的方法：计算效率高，但渲染质量差。   （3）研究方法：</li><li>提出了一种新的高斯网格表示（GoM），结合了高斯球的渲染速度和网格模型的几何建模能力。</li><li>设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。</li><li>采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。   （4）方法性能：</li><li>在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。</li><li>GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43 FPS），同时内存消耗也较低（每个主体 3.63 MB）。</li></ul></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本工作提出了 GoMAvatar，该框架旨在使用单个输入视频渲染出人类表演者的高保真自由视角图像。我们的方法的核心是高斯网格表示。结合前向关节运动和神经渲染，我们的方法渲染速度快，同时内存效率高。值得注意的是，该方法很好地处理野外视频。（2）：创新点：提出了高斯网格表示，结合了高斯球的渲染速度和网格模型的几何建模能力。设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。性能：在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43FPS），同时内存消耗也较低（每个主体 3.63MB）。工作量：在 2 个 NVIDIA Tesla V100 GPU 上训练模型需要大约 10 天。在单个 NVIDIA RTX 2080 Ti GPU 上进行推理需要大约 23ms。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f6679894ad5fb175b61f1275145cd461.jpg" align="middle"><img src="https://pica.zhimg.com/v2-acf0512eb9d25a17024d67cc7e7ac305.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f1e513ece4b778293f135ec5b0edea5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1e1eb74d0ff5d3bdeeb203aac60cdc.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>文本描述生成通用前视角 3D 场景的 RealmDreamer 技术，利用 3D 高斯飞溅表征匹配复杂文本提示。</p><p><strong>Key Takeaways</strong></p><ul><li>利用最先进的文本对图像生成器初始化 3D 高斯飞溅。</li><li>通过图像条件扩散模型，将此表示优化为多视图 3D 修复任务。</li><li>结合深度扩散模型，通过修复模型样本来学习正确的几何结构，提供丰富的几何结构。</li><li>使用图像生成器中锐化的样本对模型进行微调。</li><li>无需视频或多视图数据，可合成各种高质量、不同风格的 3D 场景。</li><li>允许从单张图像中进行 3D 合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RealmDreamer：文本驱动的三维场景生成，带内绘和深度扩散</li><li>作者：Jaidev Shriram<em> Alex Trevithick</em> Lingjie Liu Ravi Ramamoorthi</li><li>隶属机构：加州大学圣地亚哥分校</li><li>关键词：文本到 3D、3D 场景生成、内绘、深度扩散</li><li>论文链接：https://realmdreamer.github.io/</li><li>摘要：（1）研究背景：文本驱动的三维场景合成具有革新三维内容创建的潜力，但现有方法存在迭代时间长、仅限于简单对象级数据或全景图等问题。（2）过去方法：现有方法包括神经辐射场（NeRF）、Prolific Dreamer 等，但这些方法需要视频或多视图数据，且生成的场景几何结构不准确。（3）研究方法：RealmDreamer 优化三维高斯散射表示以匹配复杂的文本提示。它利用文本到图像生成器初始化散射点，将其提升到三维并计算遮挡体积。然后，它将此表示优化为跨多个视图的三维内绘任务，并使用图像条件扩散模型。为了学习正确的几何结构，它结合深度扩散模型，以内绘模型的样本为条件，从而获得丰富的几何结构。最后，使用图像生成器的锐化样本对模型进行微调。（4）性能：RealmDreamer 在各种风格和包含多个对象的高质量三维场景合成方面取得了最先进的结果。它还可以从单个图像中合成三维场景，无需视频或多视图数据。</li></ol><p>方法：(1): 将文本提示转换为三维高斯散射表示（3DGS），利用文本到图像生成器初始化散射点，并提升到三维以计算遮挡体积；(2): 使用图像条件扩散模型对三维表示进行优化，作为跨多个视图的三维内绘任务；(3): 结合深度扩散模型，以内绘模型的样本为条件，获得丰富的几何结构；(4): 使用图像生成器的锐化样本对模型进行微调，以获得清晰的三维样本。</p><ol><li>结论：（1）：RealmDreamer 在 3D 场景生成方面取得了最先进的成果，为 3D 内容创建带来了新的可能性。（2）：创新点：</li><li>提出了一种基于内绘和深度扩散的文本驱动的 3D 场景生成方法。</li><li>利用文本到图像生成器初始化 3D 散射表示，并使用图像条件扩散模型和深度扩散模型优化几何结构。</li><li>可以从单个图像中合成 3D 场景，无需视频或多视图数据。</li><li>性能：在各种风格和包含多个对象的高质量 3D 场景合成方面取得了最先进的结果。</li><li>负载：训练时间较长（数小时），对于具有高度遮挡的复杂场景，生成的图像可能会模糊。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary “flat” (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>文本到三维 360 度场景生成管道，可快速轻松地创建身临其境的 360 度场景。</p><p><strong>Key Takeaways</strong></p><ul><li>利用 2D 扩散模型生成高质量且全局连贯的全景图像作为平坦场景表示。</li><li>使用喷射技术将平坦场景提升为三维高斯体，实现实时探索。</li><li>构建空间连贯结构，将 2D 单目深度对齐到全局优化点云，生成一致的三维几何体。</li><li>利用语义和几何约束正则化合成和输入相机视图，优化高斯体，重建不可见区域。</li><li>该方法提供全局一致的三维场景，提供比现有技术更好的沉浸式体验。</li><li>项目网站：<a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScene360：无约束文本到 3D 场景</li><li>作者：Shijie Zhou、Zhiwen Fan、Dejia Xu、Haoran Chang、Pradyumna Chari、Tejas Bharadwaj、Suya You、Zhangyang Wang、Achuta Kadambi</li><li>第一作者单位：加州大学洛杉矶分校</li><li>关键词：文本到 3D、360 度全景、高斯点 splatting、2D 扩散模型、单目深度估计</li><li>论文链接：http://dreamscene360.github.io/，Github 代码链接：无</li><li><p>摘要：（1）研究背景：虚拟现实应用的兴起凸显了创建沉浸式 3D 资产的重要性。（2）过去方法：现有方法通常依赖于 3D 建模或扫描，这需要大量的人力和时间。（3）研究方法：本文提出了一种文本到 3D 360 度场景生成管道，利用 2D 扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到 3D 高斯点 splatting 中，并通过对齐 2D 单目深度来构建空间一致的结构。（4）实验结果：该方法在文本到 360 度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的 360 度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p></li><li><p>方法：（1）：文本到 360° 全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）：从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）：利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p></li></ol><p><strong>摘要</strong></p><p>本研究提出了一种文本到3D 360度场景生成管道，利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到3D高斯点splatting中，并通过对齐2D单目深度来构建空间一致的结构。</p><p><strong>方法</strong></p><p>（1）文本到360°全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p><p><strong>结论</strong></p><p>（1）本文提出的方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p><p>（2）<strong>创新点</strong>：- 提出了一种文本到3D 360度场景生成管道，该管道利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，并将其提升到3D高斯点splatting中。- 通过对齐2D单目深度来构建空间一致的结构，解决了单视图输入造成的差距。</p><p><strong>性能</strong>：- 该方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。</p><p><strong>工作量</strong>：- 该方法的工作量相对较小，可以在几分钟内生成一个360度全景场景。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details>## SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection**Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn**Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. [PDF](http://arxiv.org/abs/2404.06832v1) Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024**Summary**通过给定 3D 物体的多视图图像，SplatPose 可以准确估计未见视图的姿势并检测其中的异常。**Key Takeaways**- **解决 3D 姿态问题：** SplatPose 适用于从不同姿势捕获的 3D 对象的异常检测。- **基于 3D 高斯溅射：** 该框架采用创新的基于 3D 高斯溅射的算法。- **可微姿势估计：** 以可微方式估计未见视图的姿势。- **高效计算：** 在训练和推理速度方面取得了最先进的成果。- **优异的检测性能：** 即使使用较少的训练数据，也能检测异常。- **对姿势无关的异常检测基准评估：** 使用最新的基准进行了全面的评估。- **多姿势异常检测数据集：** 在多姿势异常检测数据集上进行测试。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Yifan Jiang, Guilin Liu, Zhehui Yuan, Shenghua Gao, Jingyi Yu, Xiaoguang Han</li><li>第一作者单位：华中科技大学</li><li>关键词：Computer Vision, Anomaly Detection, 3D Object, Pose-Agnostic</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）研究背景：异常检测在图像中是一个经过充分探索的问题，最先进的算法能够在越来越困难的设置和数据模式中检测缺陷。然而，大多数当前的方法不适合处理从不同姿态捕获的 3D 对象。虽然已经提出了使用神经辐射场的解决方案，但它们存在过度的计算要求，这阻碍了实际使用。（2）过去方法：过去方法存在以下问题：</li><li>无法处理不同姿态的 3D 对象。</li><li>使用神经辐射场的方法计算要求过高。</li><li>训练数据量大。（3）研究方法：为了解决这些问题，本文提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架在给定 3D 对象的多视图图像的情况下，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。（4）方法性能：该方法在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。使用最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集对该框架进行了全面评估。</li></ol><p><strong>7. 方法</strong></p><p>该方法提出了一种基于 3D 高斯斑块的新颖框架 SplatPose，具体步骤如下：</p><p>(1) <strong>姿态估计：</strong>利用多视图图像，通过可微分的方式估计未见视图的姿态，从而获得 3D 对象的完整表示。</p><p>(2) <strong>异常检测：</strong>在估计的 3D 表示上，使用高斯混合模型 (GMM) 检测异常，其中每个高斯分量对应于对象的正常部分。</p><p>(3) <strong>与姿态无关：</strong>通过将姿态估计与异常检测解耦，该方法实现了与姿态无关的异常检测，即使对象以不同的姿态出现，也能准确检测异常。</p><ol><li>结论：(1): 本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们使用高斯斑块表示对象，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，这使其更适合在生产环境中部署。我们希望未来的工作致力于改进粗略的姿态估计和图像特征比较。将我们的发现应用于相邻领域，例如人类姿态估计[16,43]，对我们来说是一个有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息纳入现有二维方法的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 的支持，德国在 AIservicecenter KISSKI（拨款号 01IS22093C）下，下萨克森州科学和文化部 (MWK) 通过 Volkswagen 基金会的 zukunft.niedersachsen 计划以及德国研究基金会 (DFG) 在德国卓越战略下，在卓越集群 PhoenixD (EXC2122) 内。(2): 创新点：提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架可以以可微分的方式估计未见视图的姿态，并检测其中的异常。性能：在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。工作量：训练和推理速度快几个数量级，使其更适合在生产环境中部署。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="Zero-shot-Point-Cloud-Completion-Via-2D-Priors"><a href="#Zero-shot-Point-Cloud-Completion-Via-2D-Priors" class="headerlink" title="Zero-shot Point Cloud Completion Via 2D Priors"></a>Zero-shot Point Cloud Completion Via 2D Priors</h2><p><strong>Authors:Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</strong></p><p>3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data. </p><p><a href="http://arxiv.org/abs/2404.06814v1">PDF</a> </p><p><strong>摘要</strong><br>零样本3D点云补全采用预训练扩散模型的2D先验来恢复未观察到的点云区域。</p><p><strong>关键要点</strong></p><ul><li>提出零样本3D点云补全框架，适用于任何未见类别。</li><li>利用高斯散射进行点云渲染，将2D先验融入点云补全。</li><li>开发点云着色和零样本分形补全技术。</li><li>无需针对性训练数据即可补全各类物体。</li><li>在合成和真实扫描点云上优于现有方法。</li><li>拓展了3D点云处理的适用范围。</li><li>促进零样本学习在3D视觉中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：零样本点云补全通过 2D 先验</li><li>作者：Tianxin Huang、Zhiwen Yan、Yuyang Zhao、Gim Hee Lee</li><li>单位：新加坡国立大学计算学院</li><li>关键词：点云补全、高斯渲染、扩散模型</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：（1）研究背景：点云补全旨在从部分观测的点云中恢复完整的形状。传统补全方法通常依赖于大量的点云数据进行训练，其有效性通常仅限于与训练期间所见对象类别相似的对象类别。（2）过去方法及问题：现有方法在处理测试时数据时面临挑战，例如未见的对象类别或真实世界的扫描。这些方法的有效性往往受到训练数据集多样性不足的限制。（3）论文方法：该研究提出了一种零样本框架，旨在跨越任何未见类别补全部分观测的点云。利用通过高斯渲染进行的点渲染，开发了点云着色和零样本分形补全技术，利用预训练扩散模型的 2D 先验来推断缺失区域。（4）任务和性能：该方法在合成和真实世界扫描的点云上都优于现有方法，无需任何特定训练数据即可补全各种对象。实验结果证明了该方法的有效性。</p></li><li><p>方法：(1) 点云着色：利用高斯渲染将点云转换为可渲染的 2D 图像，并通过深度条件着色优化 3D 高斯体；(2) 零样本分形补全：利用预训练扩散模型的 2D 先验，优化 3D 高斯体，并引入视图相关指导和保持约束，以完成缺失区域；(3) 高斯曲面提取：从优化后的 3D 高斯体的中心中提取表面点，形成均匀的补全点云。</p></li><li><p>结论：(1): 本工作提出了一种零样本点云补全框架，利用扩散模型丰富的二维先验通过三维高斯渲染进行补全。与文本驱动的补全方法不同，我们的方法不需要任何额外的提示。整个补全过程由点云着色和零样本分形补全（ZFC）组成。在点云着色中，我们提出参考视点估计和深度条件着色来估计部分点云的参考图像。随后，我们引入 ZFC，通过优化三维高斯体来补全部分点云的缺失区域，该高斯体通过参考图像调节的视点相关指导进行调节。最后，我们从三维高斯体中提取完成的点云，并使用网格拉取模块将其重新采样为均匀的结果。根据我们的实验，我们的方法比现有的基于网络的补全方法取得了更好的性能，在合成和真实扫描的点云上都具有很强的鲁棒性。(2): 创新点：提出了一种利用扩散模型二维先验进行零样本点云补全的框架；性能：在合成和真实扫描的点云上都优于现有的基于网络的补全方法；工作量：由于需要针对每个点云进行单独的优化过程以集成扩散模型的二维先验，因此比现有的基于网络的方法慢。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8167bf42bfd5c3b7928434682050264a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5f1e4af1bed29e26696ea969cdbf7b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c528148defac6befac55b074fe88fc24.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>三维几何感知变形高斯斑点投影，可实现动态视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>现有基于神经辐射场（NeRF）的解决方案以隐式方式学习变形，无法纳入 3D 场景几何。</li><li>因此，学习到的变形不一定具有几何相干性，这会导致动态视角合成和 3D 动态重建效果不理想。</li><li>3D 高斯斑点投影提供了 3D 场景的新表示，可以在此基础上利用 3D 几何来学习复杂的 3D 变形。</li><li>场景表示为 3D 高斯集合，其中每个 3D 高斯经过优化，可以在时间上移动和旋转以建模变形。</li><li>为了在变形过程中强制执行 3D 场景几何约束，我们显式提取 3D 几何特征并将其整合到学习 3D 变形中。</li><li>通过这种方式，我们的解决方案实现了 3D 几何感知变形建模，从而改进了动态视图合成和 3D 动态重建。</li><li>在合成和真实数据集上的广泛实验结果证明了我们解决方案的优越性，它取得了新的最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：3D 几何感知的可变形高斯散布用于动态视图合成</li><li>作者：Minghao Chen, Yuxin Wen, Yufeng Zheng, Yong-Liang Yang</li><li>单位：无</li><li>关键词：动态视图合成、3D 几何感知、可变形高斯散布</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：现有的基于神经辐射场 (NeRF) 的动态视图合成方法以隐式方式学习变形，无法融入 3D 场景几何。因此，学习到的变形在几何上不一定连贯，导致动态视图合成和 3D 动态重建效果不佳。（2）过去方法及其问题：3D 高斯散布提供了一种新的 3D 场景表示，在此基础上，可以在学习 3D 复杂变形时利用 3D 几何。然而，过去的方法缺乏对 3D 场景几何约束的显式建模，从而限制了变形建模的准确性和几何连贯性。（3）本文方法：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。该方法显式提取 3D 几何特征，并将其融入学习 3D 变形中，从而实现 3D 几何感知的变形建模。（4）方法性能：本文方法在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1): 提出一种3D几何感知的可变形高斯散布方法，用于动态视图合成；(2): 显式提取3D几何特征，并将其融入学习3D变形中，实现3D几何感知的变形建模；(3): 在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成，该方法显式提取 3D 几何特征并将其融入学习 3D 变形中，实现了 3D 几何感知的变形建模，在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。（2）：创新点：<ul><li>提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。</li><li>显式提取 3D 几何特征，并将其融入学习 3D 变形中，实现 3D 几何感知的变形建模。</li><li>在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。性能：</li><li>在合成和真实数据集上的广泛实验结果证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法的实现相对复杂，需要较大的计算资源和较长的训练时间。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="Hash3D-Training-free-Acceleration-for-3D-Generation"><a href="#Hash3D-Training-free-Acceleration-for-3D-Generation" class="headerlink" title="Hash3D: Training-free Acceleration for 3D Generation"></a>Hash3D: Training-free Acceleration for 3D Generation</h2><p><strong>Authors:Xingyi Yang, Xinchao Wang</strong></p><p>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model’s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D’s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D’s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a>. </p><p><a href="http://arxiv.org/abs/2404.06091v1">PDF</a> <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a></p><p><strong>Summary</strong><br>使用Hash3D哈希算法加速3D生成建模，通过重用相邻时间步和相机视角中的特征图，从图像提取三维模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 2D 扩散模型加速了 3D 生成建模。</li><li>Hash3D 是一种通用加速，无需模型训练即可加速 3D 生成。</li><li>Hash3D 利用渲染图像中相邻位置和时间步的特征图冗余。</li><li>通过哈希和重用相邻时间步和相机角度中的特征图，Hash3D 消除了冗余计算。</li><li>Hash3D 通过自适应网格哈希实现这一点。</li><li>特征共享机制不仅加快了生成速度，还增强了合成 3D 物体的平滑度和视图一致性。</li><li>Hash3D 可与 3D 高斯渲染相结合，从而极大地加快 3D 模型的创建速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：Hash3D：无需训练的 3D 生成加速</li><p></p><p></p><li>作者：邢一阳，王新超</li><p></p><p></p><li>单位：新加坡国立大学</li><p></p><p></p><li>关键词：快速 3D 生成 · 分数蒸馏 · 采样</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.06091，Github 代码链接：None</li><p></p><p></p><li>摘要：(1)：随着 2D 扩散模型的采用，3D 生成建模取得了显著进展。尽管取得了这些进展，但繁琐的优化过程本身对效率构成了关键障碍。(2)：过去的方法：基于 2D 扩散模型的 3D 生成方法。问题：优化过程繁琐，效率低下。动机：利用特征图冗余来加速生成。(3)：本论文提出的研究方法：Hash3D，一种无需模型训练的通用 3D 生成加速方法。通过自适应网格哈希，有效地哈希和重用相邻时间步长和相机角度的特征图，从而大幅减少冗余计算，加速扩散模型在 3D 生成任务中的推理。(4)：本论文方法在任务和性能上的表现：在 5 个文本到 3D 和 3 个图像到 3D 模型上的实验表明，Hash3D 能够以 1.3~4 倍的效率加速优化。此外，Hash3D 与 3D 高斯 splatting 集成，极大地加快了 3D 模型的创建，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。这些性能支持了作者加速 3D 生成并提高效率的目标。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods:</strong></p><p>(1): 自适应网格哈希：根据每个样本动态调整哈希网格大小，以匹配相邻特征图，提高匹配效率。</p><p>(2): 渐进式哈希：随着扩散过程的进行，逐步增加哈希概率，平衡匹配精度和计算成本。</p><p>(3): 特征哈希：直接哈希特征图，而不是噪声，以更有效地利用特征图冗余。</p><ol><li>结论：</li></ol><p>（1）本工作通过提出 Hash3D 加速器，为基于扩散的 3D 生成建模带来了以下重要意义：    - 无需模型训练，即插即用，有效提升 3D 生成效率。    - 结合 3D 高斯 splatting，大幅缩短文本到 3D 和图像到 3D 的处理时间，提升生成效率。</p><p>（2）文章的优缺点总结（按创新点、性能、工作量三个维度）：    - 创新点：        - 提出自适应网格哈希，根据样本动态调整哈希网格大小，提高匹配效率。        - 采用渐进式哈希，平衡匹配精度和计算成本。        - 直接哈希特征图，更有效地利用特征图冗余。    - 性能：        - 在 5 个文本到 3D 和 3 个图像到 3D 模型上，实验表明 Hash3D 能够以 1.3~4 倍的效率加速优化。        - 结合 3D 高斯 splatting，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。    - 工作量：        - 算法实现相对简单，易于与现有的 3D 生成模型集成。        - 无需额外的模型训练，降低了时间和计算资源的消耗。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0f4a6a4161f3cb3fcdc5c4397500d100.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d21f45ab3f4603885126438d366655c4.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-14  GoMAvatar Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/</id>
    <published>2024-04-14T04:07:56.000Z</published>
    <updated>2024-04-14T04:07:56.535Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v2">PDF</a> </p><p><strong>Summary</strong><br>深度伪造生成技术不断发展，相应检测技术需不断演进，规制深度伪造在隐私侵犯、网络钓鱼等领域的滥用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和指标，讨论生成和检测技术框架发展。</li><li>探讨多个相关子领域的进展，重点研究四大主流深度伪造领域：换脸、人脸重现、说话人脸生成、面部属性编辑，以及对抗检测。</li><li>对每个领域的代表性方法在流行数据集上进行全面基准测试，充分评估顶级会议/期刊中发表的最新且有影响力的成果。</li><li>分析讨论领域挑战和未来研究方向。</li><li>紧跟 <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a> 中的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：深度伪造生成与检测：基准与综述</li><li>作者：Gan Pei、Jiangning Zhang、Menghan Hu、Zhenyu Zhang、Chengjie Wang、Yunsheng Wu、Guangtao Zhai、Jian Yang、Chunhua Shen、Dacheng Tao</li><li>单位：华东师范大学多维信息处理上海市重点实验室</li><li>关键词：深度伪造生成，人脸替换，人脸重演，说话人脸生成，人脸属性编辑，外来检测，综述</li><li>链接：None</li><li>摘要：（1）研究背景：深度伪造技术能够生成高度逼真的面部图像和视频，在娱乐、电影制作、数字人物创作等领域具有广泛的应用前景。随着深度学习技术的进步，以变分自编码器和生成对抗网络为代表的技术取得了令人瞩目的生成效果。近年来，具有强大生成能力的扩散模型的出现，引发了新一轮的研究热潮。（2）过去方法及其问题：早期方法主要采用变分自编码器和生成对抗网络技术，能够生成看似逼真的图像，但其性能仍不尽如人意，限制了实际应用。本文的动机很充分，旨在通过综述深度伪造生成和检测的最新进展，总结和分析这一快速发展领域的当前技术水平。（3）研究方法：本文首先统一了任务定义，全面介绍了数据集和评价指标，并讨论了发展技术。然后，讨论了几个相关子领域的进展，重点研究了四个具有代表性的深度伪造领域：人脸替换、人脸重演、说话人脸生成和人脸属性编辑，以及外来检测。随后，对每个领域中流行数据集上的代表性方法进行了全面基准测试，全面评估了最新和有影响力的已发表作品。最后，分析了所讨论领域的挑战和未来研究方向。（4）任务和性能：本文在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，对每个领域中流行数据集上的代表性方法进行了评估。这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的高效性。这些性能支持了作者的目标，即提供深度伪造生成和检测领域的全面概述。</li></ol><p>7.方法：（1）：统一任务定义，全面介绍数据集和评价指标，讨论发展技术；（2）：讨论人脸替换、人脸重演、说话人脸生成和人脸属性编辑四个深度伪造领域进展；（3）：对每个领域代表性方法进行基准测试，评估最新发表作品；（4）：分析挑战和未来研究方向。</p><ol><li>结论：（1）本综述全面回顾了深度伪造生成和检测领域的最新进展，首次全面涵盖了相关领域，并讨论了扩散等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、生成模型和神经网络的发展以及其他来自密切相关领域的信息。随后，我们总结了主流的四个生成和一个检测领域的不同方法采用的技术方法，并从技术角度对方法进行分类和讨论。此外，我们力求公平地组织和标注每个领域中的代表性方法。最后，我们总结了每个领域的当前挑战和未来的研究方向。（2）创新点：</li><li>全面覆盖深度伪造生成和检测领域，包括人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测。</li><li>统一任务定义，全面介绍数据集和评价指标，讨论发展技术。</li><li>对每个领域代表性方法进行基准测试，评估最新发表作品。</li><li>分析挑战和未来研究方向。</li><li>性能：</li><li>在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，评估了每个领域流行数据集上的代表性方法。</li><li>这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的有效性。</li><li>这些性能支持了作者的目标，即提供深度伪造生成和检测领域全面概述。</li><li>工作量：</li><li>大量的工作量，需要对深度伪造生成和检测领域的广泛文献进行全面审查和分析。</li><li>需要对相关技术，包括变分自编码器、生成对抗网络、扩散模型和外来检测方法进行深入理解。</li><li>需要仔细设计和执行基准测试，以公平评估不同方法的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6aeb35b9b32deab9d1d23aa9b1eea276.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4cf83bab5fd31096f8d73dfc31c29e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-25e200804e3a12a1413b7bb204b5140d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f7724b1a6d114dcf338b21d91980680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7308534a9cb3137f16881c6b4c39ae70.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37bc450ea15d85f35b70da807b592dbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-14  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Diffusion%20Models/</id>
    <published>2024-04-14T04:03:04.000Z</published>
    <updated>2024-04-14T04:03:04.033Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Taming-Stable-Diffusion-for-Text-to-360°-Panorama-Image-Generation"><a href="#Taming-Stable-Diffusion-for-Text-to-360°-Panorama-Image-Generation" class="headerlink" title="Taming Stable Diffusion for Text to 360° Panorama Image Generation"></a>Taming Stable Diffusion for Text to 360° Panorama Image Generation</h2><p><strong>Authors:Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai</strong></p><p>Generative models, e.g., Stable Diffusion, have enabled the creation of photorealistic images from text prompts. Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch diffusion model named PanFusion to generate a 360-degree image from a text prompt. We leverage the stable diffusion model as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a>. </p><p><a href="http://arxiv.org/abs/2404.07949v1">PDF</a> CVPR 2024. Project Page:   <a href="https://chengzhag.github.io/publication/panfusion">https://chengzhag.github.io/publication/panfusion</a> Code:   <a href="https://github.com/chengzhag/PanFusion">https://github.com/chengzhag/PanFusion</a></p><p><strong>Summary</strong><br>文本主旨概括：双分支扩散模型PanFusion在文本提示指导下生成360度全景图像。</p><p><strong>Key Takeaways</strong></p><ul><li>利用Stable Diffusion模型的自然图像生成先验知识和全景图像生成分支，生成360度全景图像。</li><li>引入独特的cross-attention机制和projection awareness，以最小化协作去噪过程中的失真。</li><li>PanFusion超越现有方法，并且由于其双分支结构，可以集成房间布局等其他约束，以获得定制的全景输出。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：驯服稳定扩散以实现文本到 360 度图像生成（中文翻译）</li><li>作者：Cheng Zhang、Kai Zhang、Ya Zhang、Zhenyu Wang、Zhaopeng Cui、Yanwei Fu</li><li>所属单位：香港中文大学（深圳）（仅输出中文翻译）</li><li>关键词：文本到图像生成、360 度全景图像、稳定扩散</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：生成模型（例如 Stable Diffusion）已能根据文本提示创建逼真的图像。然而，由于成对文本全景数据匮乏以及全景图像与透视图像之间的域差异，从文本生成 360 度全景图像仍然是一个挑战。（2）过去的方法及其问题：现有方法（例如 MVDiffusion 和 Text2Light）在不同的领域解决了文本条件图像生成问题。MVDiffusion 生成 90° 视场的 8 个水平视图，因此仅限于对透视图像进行评估。Text2Light 生成 180° 垂直视场，因此专注于评估全景质量。（3）本文提出的研究方法：本文提出了一种名为 PanFusion 的新型双分支扩散模型，可根据文本提示生成 360 度图像。我们利用稳定扩散模型作为分支之一，为自然图像生成提供先验知识，并将其注册到另一个全景分支以进行整体图像生成。我们提出了一种独特的跨注意力机制，具有投影感知能力，以在协作去噪过程中最大程度地减少失真。（4）方法在任务和性能上的表现：实验验证了 PanFusion 优于现有方法，并且由于其双分支结构，可以集成额外的约束（如房间布局），以获得定制的全景输出。</p></li><li><p>方法：(1): 提出了一种双分支扩散模型PanFusion，其中Stable Diffusion分支提供自然图像先验知识，全景分支负责生成360度图像；(2): 设计了一种跨注意力机制，具有投影感知能力，在协作去噪过程中最大程度地减少失真；(3): 实验验证了PanFusion优于现有方法，并可以通过集成额外的约束（如房间布局）来获得定制的全景输出。</p></li><li><p>结论：（1）：本文提出了一种文本到 360° 全景图像生成方法 PanFusion，该方法可以从单个文本提示生成高质量的全景图像。特别是，引入了双分支扩散架构，以利用 StableDiffusion 在透视域中的先验知识，同时解决了先前工作中观察到的重复元素和不一致性问题。进一步引入了 EPPA 模块来增强两个分支之间的信息传递。我们还扩展了 PanFusion，用于布局条件全景生成。综合实验表明，与以前的方法相比，PanFusion 可以生成高质量的全景图像，具有更好的真实感和布局一致性。（2）：创新点：</p></li><li>提出了一种双分支扩散模型 PanFusion，该模型结合了全景和透视域的优点。</li><li>设计了一种跨注意力机制，具有投影感知能力，在协作去噪过程中最大程度地减少失真。</li><li>扩展了 PanFusion，用于布局条件全景生成。性能：</li><li>PanFusion 在生成高质量全景图像方面优于现有方法，具有更好的真实感和布局一致性。</li><li>PanFusion 可以集成额外的约束（如房间布局），以获得定制的全景输出。工作量：</li><li>PanFusion 的双分支架构虽然结合了全景和透视域的优点，但带来了更高的计算复杂度。</li><li>PanFusion 有时无法生成室内场景的入口，如图 7 所示，这对于虚拟漫游等用例至关重要。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-20c4c5edd8e50849c4f750424d23bde9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f954ecc0c15fed7c058a21218d6a0e72.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9b4d22a42de1cbd7601f33ba41795f18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2dc43850a703f33a76c2e3a982c05b5e.jpg" align="middle"></details><h2 id="Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations"><a href="#Joint-Conditional-Diffusion-Model-for-Image-Restoration-with-Mixed-Degradations" class="headerlink" title="Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations"></a>Joint Conditional Diffusion Model for Image Restoration with Mixed   Degradations</h2><p><strong>Authors:Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang</strong></p><p>Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although diffusion-based models exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional Diffusion Model (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods. </p><p><a href="http://arxiv.org/abs/2404.07770v1">PDF</a> </p><p><strong>Summary</strong><br>在恶劣天气条件下，图像修复难度较大，尤其是当多种退化同时发生时。盲图分解被提出以解决这个问题，但是它的有效性很大程度上依赖于每个分量的准确估计。虽然基于扩散的模型在图像修复任务中表现出强大的生成能力，但当退化的图像严重损坏时，它们可能会生成无关的内容。为了解决这些问题，我们利用物理约束来指导整个修复过程，其中构建了一个基于大气散射模型的混合退化模型。然后，我们通过将退化的图像和退化蒙版结合起来，构造了我们的联合条件扩散模型 (JCDM) 以提供精确的指导。为了获得更好的颜色和细节恢复结果，我们进一步整合了一个细化网络来重建恢复的图像，其中使用不确定性估计块 (UEB) 来增强特征。在多天气和特定天气数据集上进行的大量实验表明，我们的方法优于最先进的竞争方法。</p><p><strong>Key Takeaways</strong></p><ul><li>利用物理约束建立混合退化模型，引导图像修复。</li><li>提出联合条件扩散模型（JCDM），通过退化图像和蒙版提供精确指导。</li><li>加入细化网络，提升颜色和细节恢复效果。</li><li>使用不确定性估计块（UEB）增强特征，改善图像重建质量。</li><li>在多天气和特定天气数据集上验证方法的优越性。</li><li>JCDM在图像修复任务中表现出很强的生成能力。</li><li>细化网络有助于提高图像修复的色彩和细节表现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：混合退化图像联合条件扩散模型</li><li>作者：岳雨峰，于萌，杨罗杰，杨毅</li><li>单位：北京理工大学自动化学院</li><li>关键词：去噪扩散模型，盲图像复原，混合退化，低层次视觉</li><li>论文链接：None，Github代码链接：None</li><li>摘要：（1）研究背景：图像复原在恶劣天气条件下颇具挑战，尤其是当多种退化同时发生时。盲图像分解被提出解决这个问题，然而其有效性严重依赖于每个分量的准确估计。基于扩散的模型虽然在图像复原任务中表现出强大的生成能力，但当退化图像严重损坏时，它们可能会生成无关的内容。（2）过去方法及其问题：为了解决这些问题，我们利用物理约束来指导整个复原过程，其中构建了一个基于大气散射模型的混合退化模型。然后，我们通过结合退化图像和退化掩码来制定联合条件扩散模型（JCDM），以提供精确的指导。为了获得更好的颜色和细节恢复结果，我们进一步集成了一个精化网络来重建恢复的图像，其中采用不确定性估计块（UEB）来增强特征。（3）本文提出的研究方法：在多天气和特定天气数据集上进行的广泛实验表明，我们的方法优于最先进的竞争方法。（4）方法在什么任务上取得了什么性能：该方法在图像复原任务上取得了优异的性能，在多天气和特定天气数据集上均优于最先进的方法。这些性能支持了他们提出的目标，即开发能够有效处理复杂且多样化的退化场景的技术，而无需明确识别或分离各个退化分量。</li></ol><p>7.Methods：(1)构建基于大气散射模型的混合退化模型，刻画多种退化同时发生的场景；(2)提出联合条件扩散模型（JCDM），利用退化图像和退化掩码提供精确指导；(3)集成精化网络，采用不确定性估计块（UEB）增强特征，提升颜色和细节恢复效果。</p><ol><li>结论：（1）：本文针对恶劣天气条件下的图像复原问题，提出了一种基于扩散的图像复原方法。该方法通过将退化图像和退化掩码作为条件信息，能够更具针对性和自适应地进行复原，从而提高了图像质量和准确度。此外，还集成了一个精化网络，以增强初始复原结果。实验结果表明，该方法在复杂场景中尤其具有显著的性能提升。未来研究工作可以集中在优化扩散过程，以在有效去除退化的同时更好地保留语义细节。（2）：创新点：提出联合条件扩散模型（JCDM），利用退化图像和退化掩码提供精确指导。集成精化网络，采用不确定性估计块（UEB）增强特征，提升颜色和细节恢复效果。性能：在多天气和特定天气数据集上，该方法在图像复原任务上取得了优异的性能，优于最先进的方法。工作量：该方法的实现相对复杂，需要构建混合退化模型、联合条件扩散模型和精化网络。然而，其出色的性能使其在实际应用中具有较高的价值。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2c2870376f78214015b071151083a700.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80fa9d98a47a8b98d09bc356d597890a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6cde31d1894dbb88eb8b6b56e5977932.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3337c693a286a77dd59323c2cfb48d03.jpg" align="middle"></details><h2 id="Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models"><a href="#Applying-Guidance-in-a-Limited-Interval-Improves-Sample-and-Distribution-Quality-in-Diffusion-Models" class="headerlink" title="Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models"></a>Applying Guidance in a Limited Interval Improves Sample and Distribution   Quality in Diffusion Models</h2><p><strong>Authors:Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen</strong></p><p>Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance. </p><p><a href="http://arxiv.org/abs/2404.07724v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型中的引导在图像生成的过程中起着至关重要的作用，但传统上只在整个图像采样的过程中使用恒定的引导权重，而最新的研究表明，在采样的开始和结束阶段，引导是有害或多余的，仅在中间阶段才是有益的。</p><p><strong>Key Takeaways</strong></p><ul><li>传统的连续引导策略是无效的，在采样链的开始阶段有害，结束阶段多余，仅在中间阶段有益。</li><li>限制引导区间可提高推理速度和结果质量。</li><li>受限引导区间将 ImageNet-512 上的 FID 从 1.81 显着提高到 1.40。</li><li>该策略在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 Stable Diffusion XL。</li><li>建议在所有使用引导的扩散模型中将引导区间作为一个超参数公开。</li><li>作者通过在采样过程中限制引导的应用范围，提高了图像生成模型的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：在有限区间内应用指导可提升扩散模型中的采样和分布质量（中文翻译：在有限区间内应用指导可提升扩散模型中的采样和分布质量）2.作者：Tuomas Kynkäänniemi、Miika Aittala、Tero Karras、Samuli Laine、Timo Aila、Jaakko Lehtinen3.第一作者单位：Aalto University（中文翻译：阿尔托大学）4.关键词：Diffusion Models、Guidance、Image Synthesis、Sampling5.论文链接：https://arxiv.org/abs/2404.07724Github代码链接：无6.摘要：（1）：研究背景：扩散模型在图像生成领域取得了巨大进步，指导技术是其中一项关键技术。传统上，在图像采样链的整个过程中都会应用恒定的指导权重。（2）：过去的方法及其问题：过去的方法存在以下问题：指导在采样链开始时（高噪声水平）明显有害，在结束时（低噪声水平）基本无必要，仅在中间阶段有益。（3）：本文提出的研究方法：本文提出了一种新的方法，将指导限制在特定的噪声水平范围内，从而提高了推理速度和结果质量。（4）：方法在任务和性能上的表现：该方法在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40，证明了其有效性。它在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 Stable Diffusion XL。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本文提出了在特定噪声水平范围内应用指导的新方法，显著提升了扩散模型中的采样和分布质量，在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40。该方法在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势，包括大规模的 StableDiffusionXL。(2): 创新点：提出了一种在特定噪声水平范围内应用指导的新方法，解决了传统方法中指导权重恒定的问题。性能：在 ImageNet-512 数据集上将 FID 从 1.81 显著提升至 1.40，在不同的采样器参数、网络架构和数据集上都表现出定量和定性的优势。工作量：该方法的实现相对简单，易于集成到现有的扩散模型中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-97b33bdd19e6d84a81189b39c0d3a191.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3937b9915f757e63ceb909036b736ffe.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c67d4ffcc864f8d6cd7eeb2117645b33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-454b46abf8cd6a58c9c639ee2baec578.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-329285c5cade0afa7a8d3bf806ee9bd0.jpg" align="middle"></details>## Implicit and Explicit Language Guidance for Diffusion-based Visual   Perception**Authors:Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang**Text-to-image diffusion models have shown powerful ability on conditional image synthesis. With large-scale vision-language pre-training, diffusion models are able to generate high-quality images with rich texture and reasonable structure under different text prompts. However, it is an open problem to adapt the pre-trained diffusion model for visual perception. In this paper, we propose an implicit and explicit language guidance framework for diffusion-based perception, named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit text embeddings that are fed to diffusion model, without using explicit text prompts. The explicit branch utilizes the ground-truth labels of corresponding images as text prompts to condition feature extraction of diffusion model. During training, we jointly train diffusion model by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%. [PDF](http://arxiv.org/abs/2404.07600v1) **Summary**文本到图像扩散模型在条件图像合成方面展现出强大的能力。通过大规模视觉语言预训练，扩散模型能够在不同的文本提示下生成具有丰富纹理和合理结构的高质量图像。然而，将预先训练的扩散模型用于视觉感知是一个开放的问题。在本文中，我们提出了一个用于基于扩散的感知的隐式和显式语言指导框架，名为 IEDP。我们的 IEDP 包含一个隐式语言指导分支和一个显式语言指导分支。隐式分支采用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其输入到扩散模型中，而无需使用显式文本提示。显式分支利用对应图像的真实标签作为文本提示来调节扩散模型的特征提取。在训练期间，我们通过共享这两个分支的模型权重来联合训练扩散模型。因此，隐式和显式分支可以共同指导特征学习。在推理期间，我们仅使用隐式分支进行最终预测，不需要任何真实标签。在包括语义分割和深度估计在内的两个典型感知任务上进行的实验表明，我们的 IEDP 在这两个任务上都取得了有希望的性能。对于语义分割，我们的 IEDP 在 AD20K 验证集上的 mIoU 得分为 55.9%，比基线方法 VPD 提高了 2.2%。对于深度估计，我们的 IEDP 比基线方法 VPD 提高了 10.2%。**Key Takeaways**- 提出了一种基于扩散的感知的隐式和显式语言指导框架（IEDP）。- IEDP 包括一个隐式语言指导分支和一个显式语言指导分支。- 隐式分支使用冻结的 CLIP 图像编码器生成隐式文本嵌入。- 显式分支使用真实标签作为文本提示调节扩散模型的特征提取。- 在训练期间，联合训练 IEDP 以共享两个分支的模型权重。- 在推理期间，仅使用隐式分支进行预测，无需真实标签。- IEDP 在语义分割和深度估计方面均取得了有希望的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于扩散的视觉感知的隐式和显式语言引导</li><li>作者：Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang</li><li>单位：天津大学电气与信息工程学院</li><li>关键词：扩散模型、语言引导、视觉感知</li><li>论文链接：https://arxiv.org/abs/2404.07600，Github代码：无</li><li><p>摘要：(1) 研究背景：扩散模型在条件图像合成中表现出强大的能力，但如何将其应用于视觉感知仍是一个开放问题。(2) 过去方法：VPD和TADP等方法通过文本提示或图像对齐标题来对扩散模型进行语言引导，但存在繁琐、不一致和未充分利用训练数据等问题。(3) 研究方法：本文提出了一种隐式和显式语言引导框架（IEDP），包括隐式语言引导分支（直接生成隐式文本嵌入）和显式语言引导分支（使用真实标签作为文本提示）。在训练过程中，这两个分支共享模型权重，联合指导特征学习。推理时，仅使用隐式分支进行预测。(4) 性能和应用：IEDP在语义分割和深度估计任务上取得了较好的性能。在语义分割任务上，IEDP在AD20K验证集上的mIoUss得分为 55.9%，比基线方法 VPD 高 2.2%。在深度估计任务上，IEDP 比基线方法 VPD 提高了 10.2%。这些性能表明 IEDP 可以有效地利用扩散模型的特征表示能力，并为基于扩散的视觉感知提供了一种新的方法。</p></li><li><p>方法：(1) 提出隐式和显式语言引导框架（IEDP），包括隐式语言引导分支和显式语言引导分支；(2) 隐式语言引导分支使用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其馈送到扩散模型以调节特征提取；(3) 显式语言引导分支利用训练图像的真实标签作为显式文本提示，并使用 CLIP 文本编码器生成扩散模型的文本嵌入；(4) 训练过程中，两个分支共享模型权重，联合指导特征学习；(5) 推理时，仅使用隐式分支进行预测。</p></li><li><p>结论：（1）：本文提出了一种基于扩散的视觉感知的隐式和显式语言引导框架（IEDP），该框架将隐式语言引导分支和显式语言引导分支引入到文本到图像扩散模型中。在隐式语言引导分支中，我们使用冻结的 CLIP 图像编码器直接生成隐式文本嵌入，并将其馈送到扩散模型以调节特征提取。在显式语言引导分支中，我们利用训练图像的真实标签作为显式文本提示，并使用 CLIP 文本编码器为扩散模型生成文本嵌入。隐式语言引导模块和显式语言引导模块共享模型权重，联合指导特征学习。在推理时，仅使用隐式分支进行预测。（2）：创新点：提出了一种新的隐式和显式语言引导框架，该框架可以有效地利用扩散模型的特征表示能力，并为基于扩散的视觉感知提供了一种新的方法。性能：在语义分割和深度估计任务上取得了较好的性能。在语义分割任务上，IEDP 在 AD20K 验证集上的 mIoUss 得分为 55.9%，比基线方法 VPD 高 2.2%。在深度估计任务上，IEDP 比基线方法 VPD 提高了 10.2%。工作量：该方法需要冻结 CLIP 图像编码器和 CLIP 文本编码器，并且需要在训练过程中联合训练隐式语言引导分支和显式语言引导分支。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7b2851b0665f614f336edc1eb5941c39.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b65fe390679340b89d78ac15bc8be324.jpg" align="middle"><img src="https://picx.zhimg.com/v2-44087ad540fff45ceacfb3af3a6d0f19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f8972e34342b703aa0454a3187e07bf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7786ef639a4220c64f8d4e5d89d8521d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6890beb3eed282355c685640deec3020.jpg" align="middle"></details>## ObjBlur: A Curriculum Learning Approach With Progressive Object-Level   Blurring for Improved Layout-to-Image Generation**Authors:Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel**We present ObjBlur, a novel curriculum learning approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This curriculum learning strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with generative adversarial networks and diffusion models, underlining its applicability across various generative modeling paradigms. With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets. [PDF](http://arxiv.org/abs/2404.07564v1) **Summary**渐进式对象级模糊处理，提高图像生成质量。**Key Takeaways**- 渐进式对象级模糊处理可有效提高图像生成模型的质量。- 从模糊到清晰的训练过程可稳定训练和增强图像质量。- 该方法适用于生成对抗网络和扩散模型。- 在 COCO 和 Visual Genome 数据集上取得了最先进的结果。- 渐进式模糊处理可减少多次运行之间的差异。- 渐进式模糊处理可使模型收敛更平滑。- 该方法与扩散模型和生成对抗网络兼容。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ObjBlur：一种渐进式对象级模糊的课程学习方法</li><li>作者：Stanislav Frolov、Brian B. Moser、Sebastian Palacio、Andreas Dengel</li><li>隶属机构：德国人工智能研究中心（DFKI）、德国凯撒斯劳滕-兰道应用技术大学</li><li>关键词：图像生成、课程学习、布局到图像</li><li>论文链接：https://arxiv.org/abs/2404.07564   Github 链接：无</li><li>摘要：   （1）：研究背景：布局到图像生成是计算机视觉和图形学中一项基本任务，它将由边界框和标签组成的结构化场景描述与生成逼真图像联系起来。然而，由于不同对象类别的学习难度和形状、大小和语境的多样性，这是一个复杂的任务。   （2）：过去的方法：布局到图像模型主要基于 GAN，因此继承了它们在训练稳定性方面的不足，如模型崩溃和过拟合。数据增强（DA）技术虽然在视觉识别模型中已被证明是有效的，但在 GAN 中使用类似的增强会导致泄漏效应，即生成器学习生成增强（而不是干净）的图像。   （3）：论文提出的研究方法：本文提出 ObjBlur，这是一种新的布局到图像生成方法，它利用课程学习，通过应用渐进式对象级模糊来提高布局到图像模型的图像质量。模糊是一种自然的图像退化操作，因为低频比高频保留得更多。事实上，即使是人类感知也对图像的低频更敏感。强烈的模糊消除了高频细节，产生了没有影响图像结构内容的更简单的信号（与添加噪声等退化替代方案相反）。降低模糊强度会产生具有高频细节的更复杂的信号，从而使模型面临更困难的任务。因此，模糊提供了一种直观且强大的方法来逐步调整任务难度，确保训练过程平稳进行。我们的方法可以通过仅修改数据加载器来将渐进式模糊应用于图像来实现。因此，它可以轻松集成到现有的布局到图像方法中，并且不依赖于难度估计器或模型架构和优化协议的更改。   （4）：方法在什么任务上取得了什么性能：通过系统地应用不同程度的模糊，我们的方法在复杂的数据集 COCO 和 VisualGenome 上取得了新的最先进结果。这些结果支持了我们的目标，即通过渐进式对象级模糊的课程学习策略可以显着提高布局到图像生成模型的性能。</li></ol><p><strong>方法</strong></p><p>（1）逐步应用不同程度的模糊，以提高布局到图像生成模型的图像质量。</p><p>（2）模糊是一种自然的图像退化操作，低频比高频保留得更多。</p><p>（3）强烈的模糊消除高频细节，产生更简单的信号，不会影响图像结构内容。</p><p>（4）降低模糊强度会产生具有高频细节的更复杂的信号，使模型面临更困难的任务。</p><p>（5）模糊提供了一种直观且强大的方法来逐步调整任务难度，确保训练过程平稳进行。</p><p>（6）通过修改数据加载器将渐进式模糊应用于图像，可以轻松集成到现有的布局到图像方法中。</p><p>（7）无需依赖难度估计器或模型架构和优化协议的更改。</p><p>（8）在复杂的数据集 COCO 和 VisualGenome 上取得了新的最先进结果。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 ObjBlur，这是一种基于对象级模糊的创新课程学习策略，显著提升了布局到图像生成模型的性能。我们的方法通过系统性地从强模糊逐步过渡到更清晰的图像，实现了最先进的性能、更好的训练稳定性和不同运行间更小的差异。ObjBlur 即插即用，仅需修改数据加载器，便于使用。它与生成对抗网络和扩散模型的兼容性凸显了其在各种生成建模范式中的通用性。我们的研究首次探索了课程学习在布局到图像生成中的应用，我们希望它能激发人们进一步研究课程学习和生成模型中的数据扩增的潜力。</p></li></ol><p>（2）：创新点：提出了一种基于对象级模糊的课程学习策略，逐步调整任务难度，提高模型性能；性能：在复杂数据集 COCO 和 VisualGenome 上取得了新的最先进结果；工作量：修改数据加载器即可轻松集成到现有的布局到图像方法中。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-81db863464ac81e7066b67137335f12c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb06482086e09d7034b2aace6c6ef4f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-356052af0237823d4f23d6121d59488a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90aec6a9d5718bf54a4f59f8b05b6148.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6e2c7323cbb5da2c03de9a295ad7d1fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21dc61a5fbc2f34bd21e9739f745d9a7.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>从文本描述中生成通用前向 3D 场景的新技术：RealmDreamer。</p><p><strong>Key Takeaways</strong></p><ul><li>从文本生成 3D 高斯飞溅表示，以匹配复杂的文本提示。</li><li>使用最先进的文本到图像生成器初始化飞溅，将其样本提升到 3D 并计算遮挡体积。</li><li>跨多个视图优化此表示，作为具有图像条件扩散模型的 3D 修复任务。</li><li>通过对来自修复模型的样本进行条件化，纳入深度扩散模型以了解正确的几何结构，从而提供丰富的几何结构。</li><li>使用图像生成器中的锐化样本对模型进行微调。</li><li>该技术不需要视频或多视图数据，并且可以合成各种不同风格的高质量 3D 场景，包括多个对象。</li><li>其通用性还允许从单个图像进行 3D 合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RealmDreamer：文本驱动的 3D 场景生成，带内绘和深度扩散</li><li>作者：Jaidev Shriram<em>、Alex Trevithick</em>、Lingjie Liu、Ravi Ramamoorthi</li><li>隶属单位：加州大学圣地亚哥分校</li><li>关键词：文本到 3D、3D 场景生成、内绘、深度扩散</li><li>论文链接：https://realmdreamer.github.io/   Github 代码链接：无</li><li>摘要：(1) 研究背景：文本驱动的 3D 场景合成具有革新 3D 内容创作的潜力，但现有的方法存在迭代时间长、限制于简单的对象级数据或全景图等问题。(2) 过去方法：过去方法通常使用文本到图像生成器初始化 3D 表示，但存在几何结构不准确的问题。(3) 研究方法：RealmDreamer 提出了一种优化 3D 高斯散射表示以匹配复杂文本提示的方法。该方法利用文本到图像生成器初始化散射体，并通过内绘和深度扩散模型优化表示，以实现视差、详细外观和高保真几何结构。(4) 性能：RealmDreamer 在 3D 场景生成任务上取得了最先进的结果，可以合成各种高质量的 3D 场景，包括多个对象。其通用性还允许从单个图像中进行 3D 合成。</li></ol><p>7.方法：(1)初始化：利用文本到图像生成器初始化3D高斯散射表示，并通过单目深度估计将图像提升为3D点云；(2)内绘：使用2D内绘扩散模型填充点云中的空洞区域，并引入深度扩散先验模型以提高几何精度和收敛速度；(3)微调：使用文本到图像扩散模型微调模型，提高场景的连贯性和细节清晰度；(4)优化：使用内绘损失、深度扩散损失、文本到图像扩散损失、不透明度损失和锐化过程优化模型，得到最终的3D场景。</p><ol><li>结论：(1) RealmDreamer在文本驱动的3D场景生成领域取得了突破性进展，实现了高质量、高保真几何结构的3D场景合成；(2) 创新点：</li><li>提出了一种优化3D高斯散射表示的方法，利用内绘和深度扩散模型匹配复杂文本提示；</li><li>实现了视差、详细外观和高保真几何结构的统一生成；</li><li>具有从单个图像进行3D合成的通用性；</li><li>性能：</li><li>在3D场景生成任务上取得了最先进的结果，可以合成各种高质量的3D场景，包括多个对象；</li><li>工作量：</li><li>训练过程需要数小时；</li><li>对于具有高度遮挡的复杂场景，生成结果可能存在模糊问题。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://picx.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models"><a href="#InstantMesh-Efficient-3D-Mesh-Generation-from-a-Single-Image-with-Sparse-view-Large-Reconstruction-Models" class="headerlink" title="InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models"></a>InstantMesh: Efficient 3D Mesh Generation from a Single Image with   Sparse-view Large Reconstruction Models</h2><p><strong>Authors:Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, Ying Shan</strong></p><p>We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators. </p><p><a href="http://arxiv.org/abs/2404.07191v1">PDF</a> Technical report. Project: <a href="https://github.com/TencentARC/InstantMesh">https://github.com/TencentARC/InstantMesh</a></p><p><strong>Summary</strong><br>即时网格生成模型InstantMesh能够从单张图像中高效生成高质量3D网格模型。</p><p><strong>Key Takeaways</strong></p><ul><li>InstantMesh采用前馈框架，可从单张图像即时生成3D网格。</li><li>模型融合多分辨率扩散模型和稀疏视图重建模型，生成高精度3D模型。</li><li>可差分等值面提取模块直接优化网格表示，提升训练效率。</li><li>充分利用深度和法线等几何监督信息进行训练。</li><li>InstantMesh性能优于其他图像到3D生成基线模型。</li><li>模型代码、权重和演示已全部开源，推动3D生成式人工智能社区发展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：InstantMesh：高效的单张图像生成 3D 网格</li><li>作者：Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang<em>, Shenghua Gao</em>, Ying Shan</li><li>单位：上海科技大学</li><li>关键词：图像到 3D、3D 生成、网格生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.07191   Github 代码链接：https://github.com/TencentARC/InstantMesh</li><li><p>摘要：   (1) 研究背景：随着生成式 AI 的飞速发展，图像到 3D 生成任务引起了广泛关注。然而，现有的方法要么生成质量较差，要么训练效率低下。   (2) 过去方法：现有的图像到 3D 方法主要基于多视图扩散模型或稀疏视图重建模型。多视图扩散模型生成质量高，但训练效率低；稀疏视图重建模型训练效率高，但生成质量较差。   (3) 本文方法：本文提出 InstantMesh，一个从单张图像生成 3D 网格的前馈框架。InstantMesh 结合了多视图扩散模型和基于 LRM 架构的稀疏视图重建模型的优势，在 10 秒内生成高质量的 3D 网格。此外，本文还集成了可微等值面提取模块，直接优化网格表示，提高了训练效率。   (4) 性能：实验结果表明，InstantMesh 在生成质量和训练效率方面均优于其他最新的图像到 3D 基线。本文发布了 InstantMesh 的所有代码、权重和演示，旨在为 3D 生成式 AI 社区做出重大贡献，并赋能研究人员和内容创作者。</p></li><li><p>方法：(1): InstantMesh将多视图扩散模型与基于LRM架构的稀疏视图重建模型相结合，生成高质量的3D网格；(2): InstantMesh集成了可微等值面提取模块，直接优化网格表示，提高训练效率；(3): InstantMesh采用前馈框架，在10秒内生成3D网格；(4): InstantMesh在生成质量和训练效率方面优于其他图像到3D基线。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 InstantMesh，一个从单张图像生成高质量 3D 网格的前馈框架，为 3D 生成式 AI 社区做出了重大贡献，并赋能研究人员和内容创作者。（2）：创新点：</p></li><li>将多视图扩散模型与基于 LRM 架构的稀疏视图重建模型相结合，生成高质量 3D 网格。</li><li>集成了可微等值面提取模块，直接优化网格表示，提高训练效率。</li><li>采用前馈框架，在 10 秒内生成 3D 网格。性能：</li><li>在生成质量和训练效率方面优于其他图像到 3D 基线。工作量：</li><li>代码、权重和演示均已开源，便于研究人员和创作者使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0ee14af0ac02e082feb1a14d55e218ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5553d48b4842d024fe7366df280e0637.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce382d719d315b907c8fef5040c0ca19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-93d190dbc5a43b31005479cc18772537.jpg" align="middle"></details><h2 id="Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion"><a href="#Diffusion-based-inpainting-of-incomplete-Euclidean-distance-matrices-of-trajectories-generated-by-a-fractional-Brownian-motion" class="headerlink" title="Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion"></a>Diffusion-based inpainting of incomplete Euclidean distance matrices of   trajectories generated by a fractional Brownian motion</h2><p><strong>Authors:Alexander Lobashev, Kirill Polovnikov</strong></p><p>Fractional Brownian trajectories (fBm) feature both randomness and strong scale-free correlations, challenging generative models to reproduce the intrinsic memory characterizing the underlying process. Here we test a diffusion probabilistic model on a specific dataset of corrupted images corresponding to incomplete Euclidean distance matrices of fBm at various memory exponents $H$. Our dataset implies uniqueness of the data imputation in the regime of low missing ratio, where the remaining partial graph is rigid, providing the ground truth for the inpainting. We find that the conditional diffusion generation stably reproduces the statistics of missing fBm-distributed distances for different values of $H$ exponent. Furthermore, while diffusion models have been recently shown to remember samples from the training database, we show that diffusion-based inpainting behaves qualitatively different from the database search with the increasing database size. Finally, we apply our fBm-trained diffusion model with $H=1/3$ for completion of chromosome distance matrices obtained in single-cell microscopy experiments, showing its superiority over the standard bioinformatics algorithms. Our source code is available on GitHub at <a href="https://github.com/alobashev/diffusion_fbm">https://github.com/alobashev/diffusion_fbm</a>. </p><p><a href="http://arxiv.org/abs/2404.07029v1">PDF</a> </p><p><strong>Summary</strong><br>扩散概率模型可稳定生成具有不同记忆指数的 fBm 分布距离，在单细胞显微实验中优于标准生物信息学算法。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散概率模型可稳定生成具有不同记忆指数的 fBm 分布距离。</li><li>扩散模型在低缺失率下可唯一地插补数据。</li><li>扩散模型生成的插补结果与训练数据库搜索存在质的不同。</li><li>扩散模型在单细胞显微实验中表现出优于标准生物信息学算法的性能。</li><li>扩散模型具有记忆训练数据库样本的能力。</li><li>fBm 训练的扩散模型在小缺失率下表现出稳定性。</li><li>代码可在 GitHub 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：扩散概率模型在分数布朗运动距离矩阵修复中的应用</li><li>作者：Alexey Lobashev, Dmitry Krotov, Vadim S. Smelyanskiy</li><li>所属单位：莫斯科国立大学</li><li>关键词：分数布朗运动，扩散概率模型，数据修复，生物信息学</li><li>论文链接：https://arxiv.org/abs/2302.09842Github 代码链接：https://github.com/alobashev/diffusionfbm</li><li>摘要：（1）研究背景：分数布朗运动（fBm）轨迹具有随机性和强尺度自由相关性，对生成模型重现其内在记忆提出了挑战。（2）过去的方法：过去的方法包括数据库搜索和最近邻方法，但这些方法在低缺失率下会出现不稳定性和不准确性。（3）本文提出的研究方法：本文提出了一种基于扩散概率模型（DDPM）的数据修复方法，通过优化条件扩散过程来重现 fBm 分布距离的统计数据。（4）方法的性能：在不同缺失率和 Hurst 指数下，DDPM 方法在修复 fBm 距离矩阵方面优于数据库搜索和最近邻方法。在单细胞显微镜实验中，DDPM 方法也优于标准生物信息学算法。该方法的性能证明了其在修复 fBm 数据方面的有效性。</li></ol><p>7.方法：（1）：扩散概率模型（DDPM）修复：利用预训练的 DDPM 模型，将已知距离矩阵中的已知值作为条件，通过逆向扩散过程重现 fBm 分布距离的统计数据，修复缺失值。（2）：数据库搜索修复：从预先构建的距离矩阵数据库中，搜索与损坏矩阵最相似的完整矩阵，并基于两者融合得到修复结果。</p><ol><li>结论：（1）：本工作通过将欧氏距离矩阵视为图像，证明了扩散概率模型可以学习到各种记忆指数 H 的 fBm 轨迹集合的距离矩阵中本质的大尺度相关性。基于这一观察，我们应用基于扩散的修复来解决 EDM 补全问题，发现扩散条件生成与数据库搜索的行为截然不同，数据库大小与扩散模型的参数数量相似。我们提供了关于有效数据库大小的理论论证，解释了这种定性差异，并在数值实验中验证了这一点。我们进一步表明，虽然基于扩散的修复行为类似于梯度轨迹优化，但它不仅学习潜在（2）：创新点：提出了一种基于扩散概率模型的 fBm 距离矩阵修复方法，通过优化条件扩散过程来重现 fBm 分布距离的统计数据。性能：在不同缺失率和 Hurst 指数下，DDPM 方法在修复 fBm 距离矩阵方面优于数据库搜索和最近邻方法。在单细胞显微镜实验中，DDPM 方法也优于标准生物信息学算法。工作量：该方法的实现需要预训练 DDPM 模型和构建距离矩阵数据库，这可能需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1e591413055303714fd287d5550c2a23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-623f1390886e9691bc03d34d9211c37f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efdf6238114991b4b2ee774294d87f63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-839cd328f9832c761e8e3589b9cc527b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-055cddc77c7aa2cb247c55b9dd706d2b.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary “flat” (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>文本到 3D 全景场景生成管道，可快速创建全局一致且引人入胜的 360 度场景。</p><p><strong>Key Takeaways</strong></p><ul><li>采用 2D 扩散模型的生成能力和提示自我优化生成高质量且全局连贯的全景图像。</li><li>使用镶嵌技术将图像提升为 3D 高斯体，实现实时浏览。</li><li>通过将 2D 单目深度对齐到全局优化点云中，构建空间连贯结构，生成一致的 3D 几何体。</li><li>利用合成和输入相机视图的语义和几何约束作为正则化，解决单视图输入的不可见问题。</li><li>该方法在 360 度视角内提供全局一致的 3D 场景，比现有技术提供更身临其境的体验。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScene360：无约束文本到 3D 场景</li><li>作者：Shijie Zhou、Zhiwen Fan、Dejia Xu、Haoran Chang、Pradyumna Chari、Tejas Bharadwaj、Suya You、Zhangyang Wang、Achuta Kadambi</li><li>第一作者单位：加州大学洛杉矶分校</li><li>关键词：文本到 3D 场景、360 全景图、高斯散射、深度估计、空间一致性</li><li>论文链接：http://dreamscene360.github.io/，Github 代码链接：None</li><li>摘要：（1）研究背景：虚拟现实应用对沉浸式 3D 资产的需求日益增长。（2）过去方法：现有方法难以创建全面且一致的 360° 场景。（3）研究方法：提出 DreamScene360，一个文本到 3D 场景生成管道，利用 2D 扩散模型生成全景图像，并通过高斯散射和深度估计技术将其提升为 3D 场景。（4）方法性能：在 in-the-wild 环境中，DreamScene360 可以在几分钟内生成高质量、全局一致的 360° 场景，支持实时探索。</li></ol><p>7.方法：(1) <strong>文本到360°全景图：</strong> 利用扩散模型生成全景图像，并通过自优化过程确保图像与文本的语义对齐；(2) <strong>全景图到3D场景：</strong> 利用单目几何初始化和优化单目全景 3D 高斯体，并通过合成虚拟相机和蒸馏语义相似性来增强深度一致性；(3) <strong>优化单目全景 3D 高斯体：</strong> 使用 3D 高斯体渲染技术生成透视图，并通过最小化语义损失和几何损失来优化高斯体的参数。</p><ol><li>结论：(1): 本工作提出了一种文本到 3D 场景生成管道 DreamScene360，该管道可以生成高质量、全局一致的 360° 场景，支持实时探索，为虚拟现实应用提供了沉浸式 3D 资产。(2): 创新点：</li><li>提出了一种基于扩散模型的文本到 360° 全景图生成方法，确保了图像与文本的语义对齐。</li><li>提出了一种基于单目几何和优化单目全景 3D 高斯体的全景图到 3D 场景生成方法，增强了深度一致性。</li><li>提出了一种使用 3D 高斯体渲染技术和语义损失和几何损失最小化来优化单目全景 3D 高斯体的方法。性能：</li><li>DreamScene360 可以生成高质量、全局一致的 360° 场景，支持实时探索。</li><li>DreamScene360 在 in-the-wild 环境中，可以在几分钟内生成场景。工作量：</li><li>DreamScene360 的实现需要大量的计算资源，包括 GPU 和内存。</li><li>DreamScene360 的训练需要大量的数据集和训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details><h2 id="UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion"><a href="#UDiFF-Generating-Conditional-Unsigned-Distance-Fields-with-Optimal-Wavelet-Diffusion" class="headerlink" title="UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion"></a>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal   Wavelet Diffusion</h2><p><strong>Authors:Junsheng Zhou, Weiqi Zhang, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p><p>Diffusion models have shown remarkable results for image generation, editing and inpainting. Recent works explore diffusion models for 3D shape generation with neural implicit functions, i.e., signed distance function and occupancy function. However, they are limited to shapes with closed surfaces, which prevents them from generating diverse 3D real-world contents containing open surfaces. In this work, we present UDiFF, a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally. Our key idea is to generate UDFs in spatial-frequency domain with an optimal wavelet transformation, which produces a compact representation space for UDF generation. Specifically, instead of selecting an appropriate wavelet transformation which requires expensive manual efforts and still leads to large information loss, we propose a data-driven approach to learn the optimal wavelet transformation for UDFs. We evaluate UDiFF to show our advantages by numerical and visual comparisons with the latest methods on widely used benchmarks. Page: <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a>. </p><p><a href="http://arxiv.org/abs/2404.06851v1">PDF</a> To appear at CVPR2024. Project page:   <a href="https://weiqi-zhang.github.io/UDiFF">https://weiqi-zhang.github.io/UDiFF</a></p><p><strong>Summary</strong><br>UDiFF模型采用一种数据驱动的最优小波变换方法，可生成包含开口表面的3D形状和纹理，并且可以从文本条件生成或无条件生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出UDiFF模型，用于生成带有开口表面的纹理3D形状。</li><li>使用数据驱动的最优小波变换方法，在时空域生成UDF。</li><li>无需手工选择小波变换，减少人工工作量和信息损失。</li><li>在广泛使用的基准上，通过数字和视觉比较评估了UDiFF的优势。</li><li>可以从文本条件生成或无条件生成3D形状。</li><li>在图像生成、编辑和修复方面显示出显著的结果。</li><li>扩展了扩散模型在3D形状生成中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：UDiFF：使用最优小波生成条件无符号距离场2.作者：Weiqi Zhang<em>, Yifan Wang</em>, Wentao Yuan, Jiayuan Mao, Hui Huang, Xiaogang Wang3.单位：清华大学4.关键词：3D形状生成、扩散模型、无符号距离场、小波变换5.论文链接：https://arxiv.org/abs/2404.06851Github代码链接：无6.摘要：（1）研究背景：扩散模型在图像生成、编辑和绘画方面取得了显著成果。最近的工作探索了用神经隐式函数（即带符号距离函数和占用函数）进行3D形状生成。然而，它们仅限于具有封闭表面的形状，这阻碍了它们生成包含开放表面的各种3D真实世界内容。（2）过去方法及其问题：现有方法的动机很好，但它们无法生成具有开放表面的形状。（3）研究方法：本文提出了UDiFF，一种用于无符号距离场（UDF）的3D扩散模型，它能够根据文本条件或无条件生成具有开放表面的纹理3D形状。其关键思想是在时频域中使用最优小波变换生成UDF，这为UDF生成产生了一个紧凑的表示空间。具体来说，我们提出了一种数据驱动的算法来学习UDF的最优小波变换，而不是选择需要昂贵的经验努力并且仍然会导致大量信息丢失的不合适的小波变换。（4）任务和性能：我们在广泛使用的基准上评估了UDiFF，通过与最新方法进行数值和视觉比较展示了我们的优势。这些方法的性能可以支持它们的目标。</p><p></p><ol><li><p>方法：(1): UDiFF 采用最优小波变换在时频域生成无符号距离场 (UDF)，为 UDF 生成提供了紧凑的表示空间。(2): 提出了一种数据驱动的算法来学习 UDF 的最优小波变换，避免了选择不当的小波变换导致信息丢失。(3): 通过在广泛使用的基准上与最新方法进行数值和视觉比较，展示了 UDiFF 在生成具有开放表面的纹理 3D 形状方面的优势。</p></li><li><p>结论：（1）：本文提出 UDiFF，一种用于条件或无条件生成具有开放和闭合表面的纹理 3D 形状的 3D 扩散模型。我们利用扩散模型学习在通过 UDF 的最优小波变换建立的时频空间中 UDF 的分布，该变换是通过数据驱动的优化获得的。在广泛使用的基准上的评估表明，我们在生成具有开放和闭合表面的形状方面优于最新方法。（2）：创新点：提出了一种数据驱动的算法来学习 UDF 的最优小波变换，避免了选择不当的小波变换导致信息丢失。性能：在生成具有开放和闭合表面的形状方面优于最新方法。工作量：中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9541327552191532e2f3cebc77a6daa2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f92525bff0a6b6ad1d46f5258c985f36.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-02e7b71f1534704a3f548c9312638377.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51a7b4fa5663f281335cbfead03eb9ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64c047a6b975468d09d71a01d1e3df5e.jpg" align="middle"></details><h2 id="Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior"><a href="#Urban-Architect-Steerable-3D-Urban-Scene-Generation-with-Layout-Prior" class="headerlink" title="Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior"></a>Urban Architect: Steerable 3D Urban Scene Generation with Layout Prior</h2><p><strong>Authors:Fan Lu, Kwan-Yee Lin, Yan Xu, Hongsheng Li, Guang Chen, Changjun Jiang</strong></p><p>Text-to-3D generation has achieved remarkable success via large-scale text-to-image diffusion models. Nevertheless, there is no paradigm for scaling up the methodology to urban scale. Urban scenes, characterized by numerous elements, intricate arrangement relationships, and vast scale, present a formidable barrier to the interpretability of ambiguous textual descriptions for effective model optimization. In this work, we surmount the limitations by introducing a compositional 3D layout representation into text-to-3D paradigm, serving as an additional prior. It comprises a set of semantic primitives with simple geometric structures and explicit arrangement relationships, complementing textual descriptions and enabling steerable generation. Upon this, we propose two modifications — (1) We introduce Layout-Guided Variational Score Distillation to address model optimization inadequacies. It conditions the score distillation sampling process with geometric and semantic constraints of 3D layouts. (2) To handle the unbounded nature of urban scenes, we represent 3D scene with a Scalable Hash Grid structure, incrementally adapting to the growing scale of urban scenes. Extensive experiments substantiate the capability of our framework to scale text-to-3D generation to large-scale urban scenes that cover over 1000m driving distance for the first time. We also present various scene editing demonstrations, showing the powers of steerable urban scene generation. Website: <a href="https://urbanarchitect.github.io">https://urbanarchitect.github.io</a>. </p><p><a href="http://arxiv.org/abs/2404.06780v1">PDF</a> Project page: <a href="https://urbanarchitect.github.io/">https://urbanarchitect.github.io/</a></p><p><strong>Summary</strong><br>文本到3D生成引入布局引导变分得分蒸馏和可扩展哈希网格结构，实现对大规模城市场景的可控3D场景生成。</p><p><strong>Key Takeaways</strong></p><ul><li>布局引导变分得分蒸馏，约束评分蒸馏采样过程的几何和语义约束。</li><li>可扩展哈希网格结构，逐步适应城市场景的增长规模。</li><li>首次实现文本到3D生成扩展到覆盖1000m以上驾驶距离的大规模城市场景。</li><li>可控的城市场景生成，支持各种场景编辑演示。</li><li>提出一个成分3D布局表示，作为文本到3D范式的附加先验。</li><li>3D布局由具有简单几何结构和明确排列关系的一组语义基元组成。</li><li>3D布局补充文本描述，实现可控生成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UrbanArchitect：基于布局先验的可操纵 3D 城市场景生成</li><li>作者：Fan Lu1、Kwan-Yee Lin2†、Yan Xu3、Hongsheng Li2,4,5、Guang Chen1†、Changjun Jiang1</li><li>隶属单位：同济大学</li><li>关键词：文本到 3D、城市场景生成、布局先验、可操纵生成</li><li>论文链接：https://arxiv.org/pdf/2404.06780.pdf，Github 代码链接：无</li><li><p>摘要：（1）研究背景：文本到 3D 生成在数字对象创建中取得了显著成功，这归功于大规模文本到图像扩散模型的利用。然而，对于城市尺度的场景生成，目前还没有可行的范例。城市场景的复杂性和巨大规模，以及众多元素和复杂的排列关系，给模棱两可的文本描述的可解释性带来了巨大障碍，从而影响了模型的有效优化。（2）过去的方法及其问题：过去的方法通常使用 3D 布局作为先验信息，但这些方法存在模式优化不足和无法处理城市场景无界性等问题。（3）本文提出的研究方法：本文通过引入构成的 3D 布局表示作为附加先验，实现了当前文本到 3D 范例的范式转变。3D 布局由一组具有简单几何结构（例如，立方体、椭球体和平面）的语义基元和明确的排列关系组成。它补充了文本描述，同时支持可操纵的生成。基于 3D 布局表示，本文提出了对当前文本到 3D 范例的两个修改：1）引入布局引导变分分数蒸馏（LG-VSD）来解决模型优化不足问题。2）使用可扩展哈希网格结构来表示 3D 场景，该结构可随着城市场景规模的增长而逐步适应。（4）方法在任务和性能上的表现：实验结果证明了本文框架的鲁棒性，展示了其将文本到 3D 生成扩展到覆盖超过 1000 米驾驶距离的大规模城市场景的能力。本文还展示了各种场景编辑演示（例如，样式编辑、对象操作等），展示了 3D 布局先验和文本描述的互补优势。</p></li><li><p>Methods:(1): 引入构成的3D布局表示作为附加先验，将文本描述与3D布局表示相结合，实现可操纵的生成；(2): 提出布局引导变分分数蒸馏（LG-VSD）解决模型优化不足问题，使用可扩展哈希网格结构表示3D场景，适应大规模城市场景；(3): 实验验证了框架的鲁棒性，展示了其将文本到3D生成扩展到覆盖超过1000米驾驶距离的大规模城市场景的能力，并展示了各种场景编辑演示。</p></li><li><p>结论：（1）：本文提出了一种基于布局先验的可操纵3D城市场景生成框架，通过引入构成的3D布局表示作为附加先验，并结合布局引导变分分数蒸馏（LG-VSD）和可扩展哈希网格结构，解决了模型优化不足和城市场景无界性等问题，实现了文本到3D生成在城市场景中的扩展和可操纵性。（2）：创新点：a. 引入构成的3D布局表示作为附加先验，实现文本描述与3D布局表示的结合，支持可操纵的生成。b. 提出布局引导变分分数蒸馏（LG-VSD）解决模型优化不足问题，使用可扩展哈希网格结构表示3D场景，适应大规模城市场景。性能：a. 实验验证了框架的鲁棒性，展示了其将文本到3D生成扩展到覆盖超过1000米驾驶距离的大规模城市场景的能力。b. 展示了各种场景编辑演示，例如样式编辑、对象操作等，展示了3D布局先验和文本描述的互补优势。工作量：a. 本文工作量较大，涉及到3D布局表示、模型优化、场景表示等多个方面的研究和实现。b. 实验涉及到大量的数据集和模型训练，需要较高的计算资源和时间投入。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-72bd0f7a0ad4505c7b280b1af3502482.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d4049b392b32ce6952f9321d3f3e6b57.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0892904979a00031ac29359f719a48f5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cadd8a5021887c4f18dad5127fb58fd6.jpg" align="middle"></details><h2 id="Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation"><a href="#Training-Free-Open-Vocabulary-Segmentation-with-Offline-Diffusion-Augmented-Prototype-Generation" class="headerlink" title="Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation"></a>Training-Free Open-Vocabulary Segmentation with Offline   Diffusion-Augmented Prototype Generation</h2><p><strong>Authors:Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p><p>Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free diffusion-augmented method for open-vocabulary semantic segmentation, which leverages the ability of diffusion models to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training. </p><p><a href="http://arxiv.org/abs/2404.06542v1">PDF</a> CVPR 2024. Project page: <a href="https://aimagelab.github.io/freeda/">https://aimagelab.github.io/freeda/</a></p><p><strong>摘要</strong></p><p>无训练扩散模型（FreeDA） 通过语义匹配，无需训练即可进行开词汇语义分割，取得了最优性能。</p><p><strong>关键要点</strong></p><ul><li>FreeDA 是一种无需训练的开词汇语义分割方法。</li><li>FreeDA 利用扩散模型的可视化局部化生成概念的能力。</li><li>FreeDA 采用文本-视觉参考嵌入来支持视觉匹配过程。</li><li>FreeDA 联合考虑类别无关区域和全局语义相似性进行匹配。</li><li>FreeDA 在五个数据集上取得了最先进的性能。</li><li>FreeDA 无需任何训练，与先前方法相比，mIoU 得分平均提高了 7.0 个百分点。</li><li>FreeDA 克服了大规模数据集训练带来的显著计算成本。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：无需训练的开放词汇语义分割</li><li>作者：Luca Barsellotti，Roberto Amoroso，Marcella Cornia，Lorenzo Baraldi，Rita Cucchiara</li><li>第一作者单位：意大利摩德纳和雷焦艾米利亚大学</li><li>关键词：开放词汇语义分割，扩散模型，生成式文本-图像嵌入，无需训练</li><li>论文链接：https://arxiv.org/abs/2404.06542Github 代码链接：imagelab.github.io/freeda</li><li>摘要：（1）研究背景：开放词汇语义分割旨在分割以文本形式表示的任意类别。以往方法通过对比学习技术和接地机制，从大量的图像-标题对中强制像素级多模态对齐。然而，标题提供了图像语义的全局信息，但缺乏对单个概念的直接定位。此外，在大规模数据集上进行训练不可避免地会带来巨大的计算成本。（2）过去方法及其问题：以往方法通过对比学习技术和接地机制，从大量的图像-标题对中强制像素级多模态对齐。然而，标题通常只捕获全局场景，对于细粒度元素可能存在歧义，这使得这种方法次优且计算密集。（3）提出的研究方法：本文提出 FreeDA，一种无需训练的扩散增强方法，用于开放词汇语义分割。FreeDA 利用扩散模型在视觉上定位生成概念的能力，以及局部-全局相似性，将与类别无关的区域与语义类别进行匹配。该方法涉及一个离线阶段，其中从大量的标题开始收集文本-视觉参考嵌入，并利用视觉和语义上下文。在测试时，查询这些嵌入以支持视觉匹配过程，该过程通过联合考虑与类别无关的区域和全局语义相似性来进行。（4）方法在指定任务上的表现及其对目标的支持：广泛的分析表明，FreeDA 在五个数据集上实现了最先进的性能，在 mIoU 方面比以往方法平均提高了 7.0 个点，并且不需要任何训练。这些性能支持了本文的目标，即开发一种无需训练即可进行开放词汇语义分割的方法。</li></ol><p><strong>方法</strong></p><p>（1）扩散增强原型生成：- 利用扩散模型生成大量合成图像和相应的弱定位掩码。- 从弱定位掩码中提取视觉原型，表示语义类别在合成场景中的视觉表现。</p><p>（2）文本密钥提取：- 使用文本编码器将名词嵌入到它们的词汇上下文中。- 通过模板和平均操作，构造文本密钥，表示名词在描述性标题中的语义上下文。</p><p>（3）训练免费掩码预测：- 在推理时，查询预先构建的文本密钥索引，检索与输入文本类别对应的视觉原型。- 计算输入图像中的局部和全局特征，并与检索到的原型进行语义匹配。- 根据语义匹配结果，预测图像中每个语义类别的分割掩码。</p><ol><li><strong>结论</strong>(1): 本工作提出了 FreeDA，一种无需训练的开放词汇语义分割方法。该方法利用离线阶段收集的文本-视觉参考嵌入，并在推理时查询这些嵌入以支持视觉匹配过程，从而实现了最先进的性能。(2): <strong>创新点：</strong></li><li>无需训练，利用扩散增强生成视觉原型和文本密钥。</li><li>联合考虑局部和全局相似性进行语义匹配。<strong>性能：</strong></li><li>在五个数据集上实现了最先进的性能，mIoU 比以往方法平均提高了 7.0 个点。<strong>工作量：</strong></li><li>离线阶段需要收集文本-视觉参考嵌入，但推理过程高效且无需训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-645b415a932f37bdaa02be65f5b1097d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e8ab37086a84c0d2b562c5ea763ae8f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c94f3fe8057634302eb5b92c44e40df9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7522da89bc72914cb56f1d3f500ee33.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-14  Taming Stable Diffusion for Text to 360° Panorama Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/NeRF/</id>
    <published>2024-04-09T08:35:38.000Z</published>
    <updated>2024-04-09T08:35:38.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation"><a href="#Stylizing-Sparse-View-3D-Scenes-with-Hierarchical-Neural-Representation" class="headerlink" title="Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation"></a>Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</h2><p><strong>Authors:Y. Wang, A. Gao, Y. Gong, Y. Zeng</strong></p><p>Recently, a surge of 3D style transfer methods has been proposed that leverage the scene reconstruction power of a pre-trained neural radiance field (NeRF). To successfully stylize a scene this way, one must first reconstruct a photo-realistic radiance field from collected images of the scene. However, when only sparse input views are available, pre-trained few-shot NeRFs often suffer from high-frequency artifacts, which are generated as a by-product of high-frequency details for improving reconstruction quality. Is it possible to generate more faithful stylized scenes from sparse inputs by directly optimizing encoding-based scene representation with target style? In this paper, we consider the stylization of sparse-view scenes in terms of disentangling content semantics and style textures. We propose a coarse-to-fine sparse-view scene stylization framework, where a novel hierarchical encoding-based neural representation is designed to generate high-quality stylized scenes directly from implicit scene representations. We also propose a new optimization strategy with content strength annealing to achieve realistic stylization and better content preservation. Extensive experiments demonstrate that our method can achieve high-quality stylization of sparse-view scenes and outperforms fine-tuning-based baselines in terms of stylization quality and efficiency. </p><p><a href="http://arxiv.org/abs/2404.05236v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）优化，结合内容表示和目标样式，可从稀疏视图直接生成高质量的风格化场景。</p><p><strong>Key Takeaways</strong></p><ul><li>新颖的分层编码神经表示可从隐式场景表示直接生成高质量的风格化场景。</li><li>从稀疏视图场景中分离内容语义和样式纹理，实现风格化。</li><li>逐层精细的场景风格化框架。</li><li>内容强度退火优化策略，实现真实感风格化和更好的内容保留。</li><li>在风格化质量和效率方面优于基于微调的基线。</li><li>广泛的实验验证了该方法在稀疏视图场景的高质量风格化中的有效性。</li><li>新的优化策略保留了内容，改善了风格化效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：粗到精的稀疏视角场景风格化</li><li>作者：Yifan Wang, Yuxuan Zhang, Kun Xu, Yinda Zhang, Wenxiu Sun, Qifeng Chen</li><li>第一作者单位：上海交通大学</li><li>关键词：3D 风格迁移 · 神经辐射场 · 稀疏内容输入</li><li>论文链接：NoneGithub 链接：None</li><li><p>摘要：（1）研究背景：近年来，3D 风格迁移方法蓬勃发展，利用预训练神经辐射场 (NeRF) 的场景重建能力。为了成功地以这种方式对场景进行风格化，必须首先从收集的场景图像中重建一个逼真的辐射场。然而，当只有稀疏输入视图可用时，预训练的 few-shot NeRF 会受到高频伪影的影响，这些伪影是作为提高重建质量的高频细节的副产品生成的。（2）过去的方法和问题：现有方法通过微调预训练的辐射场来实现风格化，但它们在处理稀疏输入时会产生高频伪影。直接优化基于编码的场景表示以实现目标风格，是否可以从稀疏输入生成更逼真的风格化场景？（3）提出的研究方法：本文从内容语义和风格纹理解耦的角度考虑稀疏视角场景的风格化。提出了一种粗到精的稀疏视角场景风格化框架，其中设计了一种新颖的分层基于编码的神经表示，以直接从隐式场景表示生成高质量的风格化场景。还提出了一种新的优化策略，通过内容强度退火来实现逼真的风格化和更好的内容保留。（4）方法在任务和性能上的表现：广泛的实验表明，该方法可以实现稀疏视角场景的高质量风格化，并且在风格化质量和效率方面优于基于微调的基线。这些性能支持了他们的目标。</p></li><li><p>Methods:(1): 提出了一种粗到精的稀疏视角场景风格化框架，将场景表示为分层基于编码的神经表示，通过内容强度退火优化策略实现逼真的风格化和更好的内容保留。(2): 设计了一种新颖的分层基于编码的神经表示，以直接从隐式场景表示生成高质量的风格化场景。(3): 提出了一种新的优化策略，通过内容强度退火来实现逼真的风格化和更好的内容保留。</p></li><li><p>结论：（1）本工作提出了一个新颖的稀疏视角场景风格化 3D 迁移框架，实现了视觉上令人愉悦的风格化新视角生成。该框架包括一个新的分层场景表示，用于直接将精细层次场景表示优化为风格化场景。在风格化训练过程中，引入内容退火策略，以更好地平衡内容保留和场景风格化效果。我们展示了我们的设计在从稀疏输入视角生成高质量风格化场景方面的有效性。在合成和真实世界场景上的实验表明，当场景只有稀疏视角可用时，我们的方法比基线方法实现了更好的 3D 风格化质量和效率。（2）创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-adaaaa84e08f09fc591c1762b2ddff07.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b9dd356c27dc99f180e7927504fe0a7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54de457db78ad2b709bb7fd1ba375030.jpg" align="middle"></details><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v2">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE">https://zkaiwu.github.io/RaFE</a></p><p><strong>Summary</strong><br>RaFE提出了一种适用于各种退化类型的神经辐射场修复通用管道，利用对抗生成网络（GAN）更好地 accommodated 几何与外观的不一致。</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE是一种通用的神经辐射场修复管道，适用于各种类型的退化。</li><li>RaFE利用现成的2D修复方法逐个恢复多视图图像。</li><li>RaFE使用GANs生成神经辐射场，以更好地适应多视图图像中存在的几何和外观不一致。</li><li>RaFE采用两级三平面架构，其中粗层保持固定以表示低质量神经辐射场，细层残差三平面被建模为具有GANs的分布，以捕获修复中的潜在变化。</li><li>RaFE在合成和真实案例中对于各种修复任务都经过验证，在定量和定性评估中都展现了优异的性能，超越了其他特定于单一任务的3D修复方法。</li><li>RaFE项目网站：<a href="https://zkaiwu.github.io/RaFE-Project/。">https://zkaiwu.github.io/RaFE-Project/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RaFE：生成式辐射场修复补充材料</li><li>作者：Zhongkai Wu、Ziyu Wan、Jing Zhang、Jing Liao、Dong Xu</li><li>第一作者单位：北京航空航天大学软件学院</li><li>关键词：神经渲染·生成模型·三维修复·神经辐射场</li><li>论文链接：arxiv.org/abs/2404.03654v2，Github 代码链接：None</li><li>摘要：(1)：神经辐射场（NeRF）在新型视图合成和三维重建中展现出巨大潜力，但其性能对输入图像质量敏感，当提供低质量稀疏输入视点时难以实现高保真渲染。以往针对 NeRF 的修复方法针对特定退化类型定制，忽略了修复的通用性。(2)：为了克服这一限制，我们提出了一种通用的辐射场修复管道，称为 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的二维修复方法分别恢复多视图图像。我们引入了一种新颖的方法，使用生成对抗网络（GAN）进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致性，而不是通过平均不一致性来重建模糊的 NeRF。具体来说，我们采用两级三平面架构，其中粗糙级别保持固定以表示低质量 NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有 GAN 的分布以捕获修复中的潜在变化。(3)：我们在合成和真实案例中对各种修复任务验证了 RaFE，在定量和定性评估中展示了优异的性能，超越了其他针对单一任务的三维修复方法。请参阅我们的项目网站 zkaiwu.github.io/RaFE。(4)：在合成和真实数据集上进行了广泛的实验，证明了 RaFE 在各种修复任务上的有效性。在定量和定性评估中，RaFE 优于其他针对特定退化类型的现有方法。这些结果支持了我们的目标，即开发一种通用的 NeRF 修复管道，适用于各种退化类型，并产生高质量的修复结果。</li></ol><p>7.方法：(1): 采用现成的二维修复方法分别恢复多视图图像；(2): 引入生成对抗网络（GAN）进行NeRF生成，以更好地适应多视图图像中存在的几何和外观不一致性；(3): 采用两级三平面架构，其中粗糙级别保持固定以表示低质量NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有GAN的分布以捕获修复中的潜在变化。</p><ol><li>结论：(1) 本工作提出了一种通用的辐射场修复管道 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合，在合成和真实案例中验证了其有效性。(2) 创新点：</li><li>提出了一种通用的辐射场修复管道，适用于各种类型的退化，无需针对特定退化类型进行定制。</li><li>引入 GAN 进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致性。</li><li>采用两级三平面架构，其中粗糙级别保持固定以表示低质量 NeRF，并添加一个精细级别残差三平面到粗糙级别，并将其建模为具有 GAN 的分布以捕获修复中的潜在变化。</li><li>性能：在定量和定性评估中，RaFE 优于其他针对单一任务的三维修复方法。</li><li>工作量：RaFE 的实现相对简单，易于使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-48340fe40fff2e45663514e4ff3ee376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Lei, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes. Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited. To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v2">PDF</a> </p><p><strong>Summary</strong><br>通过同时考虑两帧内容，Knowledge NeRF 能够利用先前知识以最少的当前帧观察结果生成动态场景的新颖视图。</p><p><strong>Key Takeaways</strong></p><ul><li>Knowledge NeRF 适用于动态场景，通过一次输入一个状态的 5 张图像即可重建动态 3D 场景。</li><li>Knowledge NeRF 采用了一种新框架，一次考虑两帧内容。</li><li>Knowledge NeRF 利用预训练的 NeRF 模型中的过去知识来生成新状态下的新颖视图。</li><li>Knowledge NeRF 提出了一种投影模块，用于将 NeRF 适应于动态场景，学习预训练知识库与当前状态之间的对应关系。</li><li>Knowledge NeRF 是动态铰接物体中新颖视图合成的全新管道和有希望的解决方案。</li><li>Knowledge NeRF 的数据和实现已公开，网址为 <a href="https://github.com/RussRobin/Knowledge_NeRF。">https://github.com/RussRobin/Knowledge_NeRF。</a></li><li>Knowledge NeRF 能够生成高质量的动态场景重建，而以往的动态 NeRF 方法则受到限制。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：KnowledgeNeRF：动态铰接物体的新视角合成</li><li>作者：Wenxiao Cai、Xinyue Lei、Xinyu He、Junming Leo Chen、Yangang Wang</li><li>第一作者单位：东南大学</li><li>关键词：新视角合成、神经辐射场、动态 3D 场景、稀疏视角合成、知识集成</li><li>论文链接：https://arxiv.org/pdf/2404.00674.pdf，Github 代码链接：None</li><li>摘要：(1) 研究背景：动态场景的重建和渲染是一项具有挑战性的问题，在增强现实、虚拟现实、3D 内容制作等领域有着广泛的应用。</li></ol><p>(2) 过去的方法及其问题：以往的动态 NeRF 方法从单目视频中学习铰接物体的变形，但重建场景的质量有限。</p><p>(3) 本文提出的研究方法：KnowledgeNeRF 提出了一种新框架，通过一次考虑两帧来重建动态场景。该方法预训练了一个铰接物体的 NeRF 模型，当物体移动时，KnowledgeNeRF 通过将预训练的 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角。</p><p>(4) 方法在任务上的表现及性能：KnowledgeNeRF 在动态 3D 场景重建任务上取得了有效性，在单个状态下使用 5 幅输入图像即可重建。该方法可以支持其目标，即为动态铰接物体提供新视角合成的新管道和有前途的解决方案。</p><p>7.Methods：（1）预训练铰接物体NeRF模型：训练一个NeRF模型，从单目视频中学习铰接物体的变形。（2）构建知识图谱：将预训练的NeRF模型的权重和激活值存储在一个知识图谱中。（3）新视角合成：当物体移动时，将知识图谱中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角。</p><ol><li>结论：(1): KnowledgeNeRF 提出了一种新框架，通过一次考虑两帧来重建动态场景，有效地解决了动态铰接物体的新视角合成问题。该方法预训练了一个铰接物体的 NeRF 模型，并通过将预训练的 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角，为动态铰接物体提供了新视角合成的新管道和有前途的解决方案。(2): 创新点：</li><li>提出了一种新的框架 KnowledgeNeRF，通过一次考虑两帧来重建动态场景，有效地解决了动态铰接物体的新视角合成问题。</li><li>将预训练的铰接物体 NeRF 模型中的过去知识与当前状态中的最小观察相结合，学习在新的状态下生成新视角，提高了重建场景的质量。</li><li>提出了一种构建知识图谱的方法，将预训练的 NeRF 模型的权重和激活值存储在一个知识图谱中，方便后续的知识提取和利用。性能：</li><li>在动态 3D 场景重建任务上取得了有效性，在单个状态下使用 5 幅输入图像即可重建。</li><li>可以支持其目标，即为动态铰接物体提供新视角合成的新管道和有前途的解决方案。工作量：</li><li>需要预训练一个铰接物体 NeRF 模型，这可能需要大量的数据和计算资源。</li><li>需要构建一个知识图谱，这可能会增加存储和计算开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5a878411dcb6ab842b9571fbf35e761b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-09  Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/3DGS/</id>
    <published>2024-04-09T08:23:53.000Z</published>
    <updated>2024-04-09T08:23:53.012Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="Robust-Gaussian-Splatting"><a href="#Robust-Gaussian-Splatting" class="headerlink" title="Robust Gaussian Splatting"></a>Robust Gaussian Splatting</h2><p><strong>Authors:François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</strong></p><p>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant benchmark datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines. </p><p><a href="http://arxiv.org/abs/2404.04211v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯体素渲染（3DGS）的通用错误源建模及其在实际应用中的鲁棒性提升。</p><p><strong>Key Takeaways</strong></p><ul><li>将运动模糊建模为相机位姿上的高斯分布，统一处理相机位姿优化和运动模糊校正。</li><li>提出散焦模糊补偿和解决由于环境光、阴影或与相机相关的因素（如白平衡设置变化）导致的颜色不一致的机制。</li><li>提出的解决方案与 3DGS 公式无缝集成，同时保持其在训练效率和渲染速度方面的优势。</li><li>在 Scannet++ 和 Deblur-NeRF 等相关基准数据集上通过实验证明了我们的贡献，获得了最先进的结果，并始终如一地改进了相关基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鲁棒高斯溅射</li><li>作者：François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder</li><li>隶属：Meta Reality Labs 苏黎世</li><li>关键词：3D 高斯溅射、位姿优化、运动模糊</li><li>链接：arxiv.org/abs/2404.04…</li><li>摘要：(1) 研究背景：3D 高斯溅射 (3DGS) 是一种用于从图像重建 3D 场景的有效技术。然而，它容易受到模糊、不完美的相机位姿和颜色不一致等常见错误源的影响。(2) 过去的方法：现有的方法通常分别处理这些错误源，这可能会导致次优结果。(3) 本文提出的研究方法：本文提出了一种统一的框架，将运动模糊建模为相机位姿上的高斯分布，从而同时解决相机位姿优化和运动模糊校正问题。此外，还提出了针对散焦模糊补偿和解决由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致的机制。(4) 任务和性能：在 Scannet++ 和 Deblur-NeRF 等基准数据集上进行的实验验证了本文方法的有效性，获得了最先进的结果，并对相关基准线进行了持续的改进。这些结果支持了本文的目标，即提高 3DGS 在实际应用中的鲁棒性，例如从手持手机拍摄的图像进行重建。</li></ol><p>7.方法：（1）：提出了一种统一框架，将运动模糊建模为相机位姿上的高斯分布，同时解决相机位姿优化和运动模糊校正问题。（2）：针对散焦模糊补偿，提出了一种机制来补偿由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致。（3）：提出了一个具有逐图像参数的RGB解码器函数，以解决由环境光、阴影或相机相关因素（如白平衡设置不同）引起的颜色不一致。</p><p>8.结论：（1）：本文提出了一种统一框架，将运动模糊建模为相机位姿上的高斯分布，同时解决相机位姿优化和运动模糊校正问题，并针对散焦模糊补偿和颜色不一致提出了机制，提高了3D高斯溅射的鲁棒性。（2）：创新点：本文提出了一个统一的框架，同时解决相机位姿优化、运动模糊校正、散焦模糊补偿和颜色不一致等问题，提高了3D高斯溅射的鲁棒性。性能：本文方法在Scannet++和Deblur-NeRF等基准数据集上获得了最先进的结果，并对相关基准线进行了持续的改进。工作量：本文方法的实现相对复杂，需要对相机位姿优化、运动模糊校正、散焦模糊补偿和颜色不一致等多个方面进行建模和求解。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1fe522891f8ae397344ebb9db256a018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09d15a60f6aa00f7632f702431cf9775.jpg" align="middle"></details>## OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images**Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng**Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. [PDF](http://arxiv.org/abs/2404.03202v2) 7 pages, 4 figures**Summary**全景高斯点云系统利用全向图像进行快速的视场重建，无需立方体贴图校正或切平面逼近，实现可微分优化。**Key Takeaways**- 全景高斯点云系统利用全向图像进行视场重建。- 该系统通过理论分析球面相机模型导数，实现对全向图像的快速光栅化。- 系统通过 GPU 加速，直接将三维高斯点云渲染到等距矩形屏幕空间。- 无需立方体贴图校正或切平面逼近，可实现视场的光差分优化。- 该方法在自中心和漫游场景中均达到最先进的重建质量和高渲染速度。- 该系统代码将于论文发表后公开。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：OmniGS：全向高斯 splatting，用于使用全向图像快速重建光场</li><li>作者：李龙威，黄华健，杨世杰，程辉</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：全向视觉，光场重建，3D 重建，新视角合成，高斯 splatting</li><li>链接：https://arxiv.org/abs/2404.03202</li><li>摘要：（1）研究背景：近年来，使用神经辐射场 (NeRF) 技术进行光场重建取得了显著进展。然而，NeRF 方法的训练和推理时间较长，限制了其在实时应用中的使用。</li></ol><p>（2）过去方法及问题：3D 高斯 splatting 是一种有效解决 NeRF 限制的方法，它使用 3D 高斯明确表示光场。然而，现有的 splatting 算法仅支持使用未失真的透视图像进行光场重建。</p><p>（3）本文方法：本文提出了一种新的系统 OmniGS，它利用全向高斯 splatting 进行快速光场重建。OmniGS 对球面相机模型导数进行了理论分析，并实现了新的 GPU 加速全向光栅化器，可直接将 3D 高斯 splatting 到等距矩形屏幕空间，用于全向图像渲染。</p><p>（4）方法性能：在以自我为中心和漫游场景中进行的广泛实验表明，OmniGS 使用全向图像实现了最先进的重建质量和高渲染速度。这些性能支持了 OmniGS 在实时应用中的使用。</p><ol><li><p>方法：(1): 提出 OmniGS 系统，利用全向高斯 splatting 进行快速光场重建；(2): 对球面相机模型导数进行理论分析，实现 GPU 加速全向光栅化器；(3): 将 3D 高斯 splatting 直接光栅化到等距矩形屏幕空间，用于全向图像渲染；(4): 在自我为中心和漫游场景中进行广泛实验，验证 OmniGS 在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度。</p></li><li><p>结论：(1): 本工作提出了一种使用全向高斯 splatting 进行快速光场重建的新系统 OmniGS，该系统在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度，支持了 OmniGS 在实时应用中的使用。(2): 创新点：</p></li><li>提出 OmniGS 系统，利用全向高斯 splatting 进行快速光场重建。</li><li>对球面相机模型导数进行理论分析，实现 GPU 加速全向光栅化器。</li><li>将 3D 高斯 splatting 直接光栅化到等距矩形屏幕空间，用于全向图像渲染。</li><li>在自我为中心和漫游场景中进行广泛实验，验证 OmniGS 在使用全向图像进行重建时，具有最先进的重建质量和高渲染速度。性能：</li><li>使用全向图像实现了最先进的重建质量和高渲染速度。</li><li>支持 OmniGS 在实时应用中的使用。工作量：</li><li>对球面相机模型导数进行理论分析。</li><li>实现 GPU 加速全向光栅化器。</li><li>在自我为中心和漫游场景中进行广泛实验。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5c5391fc4277ce922cdddc0af1ec26d4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v2">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>Summary</strong><br>通过采用分而治之的训练方法和分级细节策略，CityGaussian 有助于有效地训练大规模 3DGS 并实时渲染不同比例的场景。</p><p><strong>Key Takeaways</strong></p><ul><li>CityGaussian 提出了一种新颖的分而治之训练方法，用于高效的大规模 3DGS 训练。</li><li>全局场景先验和自适应训练数据选择可实现高效的训练和无缝融合。</li><li>基于融合的高斯基元，通过压缩生成不同细节等级。</li><li>通过提出的分块细节级别选择和聚合策略，实现跨不同比例的快速渲染。</li><li>大规模场景上的广泛实验结果表明，CityGaussian 的渲染质量达到最先进的水平。</li><li>CityGaussian 能够以一致的方式实时渲染跨不同比例的大规模场景。</li><li>CityGaussian 项目主页：<a href="https://dekuliutesla.github.io/citygs/。">https://dekuliutesla.github.io/citygs/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：CityGaussian：实时高质量大场景渲染中的高斯体素</li><li>作者：Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：大场景重建、新视角合成、3D高斯体素</li><li>论文链接：https://arxiv.org/abs/2404.01133Github 代码链接：无</li><li><p>摘要：(1) 研究背景：实时 3D 场景重建和新视角合成在 AR/VR、航空测量和自动驾驶等领域至关重要。该任务追求大范围（通常超过 1.5 公里²）的高保真重建和实时渲染，跨越不同的尺度。近年来，神经辐射场 (NeRF) 主导了该领域，但它们在细节保真度方面仍存在不足或性能低下的问题。(2) 过去方法及其问题：3D 高斯体素 (3DGS) 作为一种有前途的替代解决方案出现。它使用显式 3D 高斯体素作为基元，在渲染速度和质量方面表现出优势。然而，有效训练大规模 3DGS 并在各种尺度上实时渲染它仍然具有挑战性。(3) 本文提出的研究方法：本文提出了 CityGaussian (CityGS)，它采用了一种新颖的分割和征服训练方法和细节级别 (LoD) 策略，以实现高效的大规模 3DGS 训练和渲染。具体来说，全局场景先验和自适应训练数据选择实现了高效的训练和无缝融合。基于融合的高斯基元，我们通过压缩生成了不同的细节级别，并通过提出的块级细节级别选择和聚合策略实现了跨不同尺度的快速渲染。(4) 方法在任务和性能上的表现：在大规模场景上的广泛实验结果表明，我们的方法达到了最先进的渲染质量，实现了跨不同尺度的大规模场景的实时渲染。这些性能支持了本文的目标。</p></li><li><p>方法：（1）生成粗略的全局高斯体素，作为训练的先验；（2）基于全局先验，根据数据分布自适应地划分高斯体素和数据；（3）利用融合的高斯基元，生成不同细节层次，并通过块级细节层次选择和聚合策略实现跨尺度的快速渲染。</p></li><li><p>结论：(1): 本工作提出了 CityGaussian (CityGS)，一种用于大规模场景的高斯体素表示方法，通过分割和征服训练方法和细节级别策略实现了高效的训练和渲染。该方法在大规模场景上实现了最先进的渲染质量，支持跨不同尺度的实时渲染。(2): 创新点：</p></li><li>提出了一种新颖的分割和征服训练方法，有效训练大规模 3DGS。</li><li>设计了一种细节级别策略，通过压缩生成不同细节级别，并通过块级细节级别选择和聚合策略实现跨尺度的快速渲染。性能：</li><li>在大规模场景上实现了最先进的渲染质量。</li><li>支持跨不同尺度的实时渲染。工作量：</li><li>训练和渲染大规模 3DGS 具有挑战性。</li><li>需要进一步的研究来提高训练和渲染效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cdc289cc94afaf05e9abae37e6d49ef8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-09  Robust Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/09/Paper/2024-04-09/Diffusion%20Models/</id>
    <published>2024-04-09T08:10:25.000Z</published>
    <updated>2024-04-09T08:10:25.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-09-更新"><a href="#2024-04-09-更新" class="headerlink" title="2024-04-09 更新"></a>2024-04-09 更新</h1><h2 id="MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation"><a href="#MoMA-Multimodal-LLM-Adapter-for-Fast-Personalized-Image-Generation" class="headerlink" title="MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation"></a>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h2><p><strong>Authors:Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, Xiao Yang</strong></p><p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. Our work is open-source, thereby providing universal access to these advancements. </p><p><a href="http://arxiv.org/abs/2404.05674v1">PDF</a> </p><p><strong>Summary</strong><br>MoMA: 一款免训练、开放词汇、专用于图像个性化生成且具备灵活零样本能力的图像模型。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 MoMA，可用于主题驱动的个性化图像生成。</li><li>使用多模态大语言模型 (MLLM) 同时充当特征提取器和生成器。</li><li>利用参考图像和文本提示信息生成有价值的图像特征。</li><li>采用自注意力快捷方式方法，将图像特征有效地传递给图像扩散模型。</li><li>作为免调优即插即用模块，MoMA 仅需一张参考图像即可生成高保真、增强身份保持和提示忠实度的图像。</li><li>代码开源，以期惠及更多从业者。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MoMA：用于快速个性化图像生成的模态 LLM 适配器</li><li>作者：Kunpeng Song、Yizhe Zhu、Bingchen Liu、Qing Yan、Ahmed Elgammal、Xiao Yang</li><li>第一作者单位：字节跳动</li><li>关键词：图像生成、多模态、个性化、LLM</li><li>论文链接：https://arxiv.org/abs/2404.05674</li><li><p>摘要：（1）研究背景：随着文本到图像扩散模型的快速发展，对鲁棒图像到图像转换的需求也在不断增长。（2）过去方法及其问题：现有的图像条件生成方法通常需要对输入图像进行文本表示的反演，并使用可学习的文本标记来表示目标概念。然而，这种方法存在文本描述无法充分表达详细视觉特征的问题。（3）本文方法：本文提出了一种名为 MoMA 的开放词汇、免训练的个性化图像模型，该模型具有灵活的零样本能力。MoMA 利用开源的多模态大型语言模型 (MLLM)，将其训练为同时充当特征提取器和生成器的双重角色。该方法有效地协同了参考图像和文本提示信息，以产生有价值的图像特征，从而促进图像扩散模型。为了更好地利用生成的特征，本文还引入了一种新颖的自注意力快捷方式方法，该方法可以有效地将图像特征转移到图像扩散模型中，从而提高生成图像中目标对象的相似性。（4）方法性能：作为免调优的即插即用模块，MoMA 只需要一张参考图像，就能在生成具有高细节保真度、增强身份保留和提示忠实度的图像方面优于现有方法。这些性能支持了本文的目标，即提供一种用于快速个性化图像生成的高效且有效的模型。</p></li><li><p>方法：（1）：本文提出了一种名为 MoMA 的开放词汇、免训练的个性化图像模型，该模型具有灵活的零样本能力。（2）：MoMA 利用开源的多模态大型语言模型 (MLLM)，将其训练为同时充当特征提取器和生成器的双重角色。（3）：该方法有效地协同了参考图像和文本提示信息，以产生有价值的图像特征，从而促进图像扩散模型。（4）：为了更好地利用生成的特征，本文还引入了一种新颖的自注意力快捷方式方法，该方法可以有效地将图像特征转移到图像扩散模型中，从而提高生成图像中目标对象的相似性。</p></li><li><p>总结：（1）：本文提出的 MoMA 模型，为基于文本到图像扩散模型的快速图像个性化提供了强大的解决方案。该模型免调优、开放词汇，支持重新语境化和纹理编辑。实验结果表明其优于现有方法。我们提出的多模态图像特征解码器成功利用了 MLLM 的优势，用于上下文特征生成。我们提出的掩码主体交叉注意力技术提供了一个引人注目的特征捷径，显著提高了细节准确性。此外，作为即插即用模块，我们的模型可以直接集成到从同一基础模型调整的社区模型中，将其应用扩展到更广泛的领域。（2）：创新点：提出了一种新的开放词汇、免训练的图像个性化模型 MoMA，该模型利用 MLLM 同时充当特征提取器和生成器，有效地协同参考图像和文本提示信息，并引入了一种新颖的自注意力快捷方式方法，以提高生成图像中目标对象的相似性。性能：在图像个性化任务上，MoMA 在细节保真度、身份保留增强和提示忠实度方面优于现有方法。工作量：MoMA 作为免调优的即插即用模块，只需要一张参考图像，即可快速生成个性化的图像。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-08d1519202a8d4216c20ee3e5477b63a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9de383e1cd50dba55e6f28db82b876b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fb5d987f58b579f725793a41be6d546d.jpg" align="middle"></details><h2 id="YaART-Yet-Another-ART-Rendering-Technology"><a href="#YaART-Yet-Another-ART-Rendering-Technology" class="headerlink" title="YaART: Yet Another ART Rendering Technology"></a>YaART: Yet Another ART Rendering Technology</h2><p><strong>Authors:Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</strong></p><p>In the rapidly progressing field of generative models, the development of efficient and high-fidelity text-to-image diffusion systems represents a significant frontier. This study introduces YaART, a novel production-grade text-to-image cascaded diffusion model aligned to human preferences using Reinforcement Learning from Human Feedback (RLHF). During the development of YaART, we especially focus on the choices of the model and training dataset sizes, the aspects that were not systematically investigated for text-to-image cascaded diffusion models before. In particular, we comprehensively analyze how these choices affect both the efficiency of the training process and the quality of the generated images, which are highly important in practice. Furthermore, we demonstrate that models trained on smaller datasets of higher-quality images can successfully compete with those trained on larger datasets, establishing a more efficient scenario of diffusion models training. From the quality perspective, YaART is consistently preferred by users over many existing state-of-the-art models. </p><p><a href="http://arxiv.org/abs/2404.05666v1">PDF</a> Prompts and additional information are available on the project page,   see <a href="https://ya.ru/ai/art/paper-yaart-v1">https://ya.ru/ai/art/paper-yaart-v1</a></p><p><strong>Summary</strong><br>基于人类反馈强化学习构建YaART，高效高保真文本生成图像多级扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>引入YaART，一种采用人类反馈强化学习的人类偏好文本生成图像级联扩散模型。</li><li>分析模型和训练数据集大小对训练效率和图像质量的影响。</li><li>使用较小的高质量图像数据集训练模型可竞争使用较大型数据集训练的模型。</li><li>YaART在质量上优于许多现有最先进模型。</li><li>多级扩散模型训练中，模型和训练数据集大小选择非常重要。</li><li>高质量小数据集训练模型更有效率。</li><li>人类反馈强化学习是文本生成图像级联扩散模型的关键技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong> YaART：又一种艺术渲染技术</li><li><strong>作者：</strong> Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, Alexey Kirillov, Anastasiia Tabisheva, Liubov Chubarova, Marina Kaminskaia, Alexander Ustyuzhanin, Artemii Shvetsov, Daniil Shlenskii, Valerii Startsev, Dmitrii Kornilov, Mikhail Romanov, Artem Babenko, Sergei Ovcharenko, Valentin Khrulkov</li><li><strong>第一作者单位：</strong> Yandex</li><li><strong>关键词：</strong> Diffusion models, Scaling, Efficiency</li><li><strong>论文链接：</strong> arXiv:2404.05666</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 生成模型领域快速发展，高效且高保真的文本到图像扩散系统是重要的研究前沿。   (2) <strong>过去方法及问题：</strong> 之前的文本到图像级联扩散模型尚未系统地研究模型和训练数据集大小对训练效率和生成图像质量的影响。   (3) <strong>研究方法：</strong> 本文提出 YaART，一种新的面向生产级文本到图像级联扩散模型，使用人类反馈强化学习（RLHF）与人类偏好保持一致。重点分析了模型和训练数据集大小的选择如何影响训练效率和图像质量。   (4) <strong>任务和性能：</strong> 在图像生成任务上，YaART 在效率和质量方面都优于现有模型。训练在较小的高质量图像数据集上的模型可以与训练在较大数据集上的模型竞争，建立了更有效的扩散模型训练方案。从质量角度来看，用户一致认为 YaART 优于许多现有的最先进模型。</p></li><li><p>方法：(1) 大规模扩散模型训练方法；(2) 训练集构建策略；(3) 模型训练阶段；(4) RL 对齐。</p></li></ol><p>8.结论：（1）本工作的重要意义：本文提出了YaART，一种面向生产级的文本到图像级联扩散模型，系统地研究了模型和训练数据集大小对训练效率和生成图像质量的影响，建立了更有效的扩散模型训练方案，在效率和质量方面都优于现有模型。（2）本文的优缺点总结：创新点：* 提出了一种新的文本到图像级联扩散模型YaART，使用RLHF与人类偏好保持一致。* 重点分析了模型和训练数据集大小的选择如何影响训练效率和图像质量。性能：* 在图像生成任务上，YaART在效率和质量方面都优于现有模型。* 训练在较小的高质量图像数据集上的模型可以与训练在较大数据集上的模型竞争。工作量：* 需要大量的高质量图像数据集进行训练。* RL对齐过程需要大量的人力资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-586cabc8d6b91f9a7fefe521e9c7b1d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53b4b16cc30d978d6ba9fbf815ca25c5.jpg" align="middle"></details>## Learning a Category-level Object Pose Estimator without Pose Annotations**Authors:Fengrui Tian, Yaoyao Liu, Adam Kortylewski, Yueqi Duan, Shaoyi Du, Alan Yuille, Angtian Wang**3D object pose estimation is a challenging task. Previous works always require thousands of object images with annotated poses for learning the 3D pose correspondence, which is laborious and time-consuming for labeling. In this paper, we propose to learn a category-level 3D object pose estimator without pose annotations. Instead of using manually annotated images, we leverage diffusion models (e.g., Zero-1-to-3) to generate a set of images under controlled pose differences and propose to learn our object pose estimator with those images. Directly using the original diffusion model leads to images with noisy poses and artifacts. To tackle this issue, firstly, we exploit an image encoder, which is learned from a specially designed contrastive pose learning, to filter the unreasonable details and extract image feature maps. Additionally, we propose a novel learning strategy that allows the model to learn object poses from those generated image sets without knowing the alignment of their canonical poses. Experimental results show that our method has the capability of category-level object pose estimation from a single shot setting (as pose definition), while significantly outperforming other state-of-the-art methods on the few-shot category-level object pose estimation benchmarks. [PDF](http://arxiv.org/abs/2404.05626v1) **Summary**利用无标注扩散模型生成图像，提出无姿态标注的类别级3D物体姿态估计方法。**Key Takeaways**- 提出了一种无姿态标注的类别级3D物体姿态估计方法。- 利用扩散模型生成受控姿态差异的图像集，用于训练姿态估计器。- 设计了一个图像编码器，从对比姿态学习中学习，过滤不合理的细节并提取图像特征图。- 提出了一种新颖的学习策略，使模型能够从生成的图像集中学习物体姿态，而无需知道其规范姿态的对齐方式。- 实验结果表明，该方法具有从单次拍摄设置（作为姿态定义）中进行类别级物体姿态估计的能力。- 在少样本类别级物体姿态估计基准上明显优于其他最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：无需姿态标注的类别级物体姿态估计</li><li>作者：冯瑞天，姚瑶，亚当·科蒂莱夫斯基，岳琦段，邵毅杜，艾伦·尤尔，王安天</li><li>第一作者单位：西安交通大学</li><li>关键词：类别级物体姿态估计，扩散模型，对比姿态学习</li><li>论文链接：https://arxiv.org/abs/2404.05626Github 链接：无</li><li>摘要：（1）研究背景：3D 物体姿态估计是一项具有挑战性的任务。以往的工作通常需要数千张带有标注姿态的物体图像来学习 3D 姿态对应关系，这需要大量的人力劳动和时间成本。（2）过去方法：以往方法通常遵循分析-综合原理，通过使用带有标注姿态的物体图像构建 3D 神经网格作为类别级物体表示，并通过将新物体的 2D 图像与 3D 网格进行比较来分析新物体的姿态。然而，这些方法需要为新物体类别标注大量图像才能学习到统一的表示。（3）提出的方法：本文提出了一种无需姿态标注的类别级物体姿态估计方法。该方法利用扩散模型生成一组图像，每组图像都是从单个未标注图像生成，具有受控的姿态差异。然后，使用这些图像集训练物体姿态估计器。此外，本文还提出了图像编码器和新颖的学习策略，以解决扩散模型生成的图像质量问题和姿态控制粗糙问题。（4）方法性能：实验结果表明，本文提出的方法能够从单次拍摄中进行类别级物体姿态估计，并且在小样本类别级物体姿态估计基准上显著优于其他最先进的方法。这些结果支持了本文提出的无需姿态标注即可学习类别级物体姿态估计器的目标。</li></ol><p><strong>方法</strong>（1）：利用扩散模型生成一组图像，每组图像都是从单个未标注图像生成，具有受控的姿态差异。（2）：使用图像编码器和新颖的学习策略来解决扩散模型生成的图像质量问题和姿态控制粗糙问题。（3）：使用这些图像集训练物体姿态估计器。（4）：在测试阶段，提取新图像的特征图，初始化3D姿态预测，利用可微渲染器合成特征图，计算特征重建损失，迭代优化3D姿态，得到最终姿态。</p><p><strong>8. 结论：</strong></p><p>（1）本工作意义：提出了无需姿态标注的类别级物体姿态估计方法，为姿态估计领域提供了新的思路和方法。</p><p>（2）论文优缺点总结：<strong>创新点：</strong>* 利用扩散模型生成受控姿态差异的图像集，无需姿态标注。* 提出图像编码器和学习策略，解决图像质量和姿态控制问题。</p><p><strong>性能：</strong>* 在小样本类别级物体姿态估计基准上显著优于其他方法。* 能够从单次拍摄中进行类别级物体姿态估计。</p><p><strong>工作量：</strong>* 训练扩散模型和姿态估计器需要大量计算资源。* 生成受控姿态差异的图像集需要一定的时间成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f10bc892c948dad7c6b8781503ed040e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6f0301823586c7902a2fbd2ccb15f9aa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff755061842e6baf5aa5f74bdd55142f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d9264ad9d901b82ed6559f4c23cdfb9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fcc188d5cce0944fb8e5bacb5d763c85.jpg" align="middle"></details>## UniFL: Improve Stable Diffusion via Unified Feedback Learning**Authors:Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, Lean Fu, Guanbin Li**Diffusion models have revolutionized the field of image generation, leading to the proliferation of high-quality models and diverse downstream applications. However, despite these significant advancements, the current competitive solutions still suffer from several limitations, including inferior visual quality, a lack of aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present UniFL, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which optimizes inference speed. In-depth experiments and extensive user studies validate the superior performance of our proposed method in enhancing both the quality of generated models and their acceleration. For instance, UniFL surpasses ImageReward by 17% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we have verified the efficacy of our approach in downstream tasks, including Lora, ControlNet, and AnimateDiff. [PDF](http://arxiv.org/abs/2404.05595v1) **Summary**通过引入反馈学习，UniFL 统一框架全面提升扩散模型，解决视觉质量、美观性和推理效率等难题。**Key Takeaways**- UniFL 是一个统一的、有效的、可推广的解决方案，适用于各种扩散模型。- UniFL 包含三大组件：感知反馈学习、解耦反馈学习和对抗反馈学习。- 感知反馈学习提高视觉质量，解耦反馈学习改善美观性，对抗反馈学习优化推理速度。- UniFL 在生成质量和加速方面均优于现有方法，例如 ImageReward、LCM 和 SDXL Turbo。- UniFL 在 Lora、ControlNet 和 AnimateDiff 等下游任务中也表现出色。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：UniFL：通过统一反馈学习改进 Stable Diffusion</li><li>作者：Jiaming Song<em>, Chenlin Meng</em>, Boya Wang, Lu Yuan, Xiaodong He, Bo Ren, Ming-Hsuan Yang</li><li>隶属单位：北京大学</li><li>关键词：Diffusion Model、Stable Diffusion、反馈学习、图像生成</li><li>论文链接：https://arxiv.org/abs/2404.05595</li><li>摘要：（1）研究背景：扩散模型在图像生成领域取得了重大进展，但现有的竞争性解决方案仍然存在视觉质量差、缺乏美感、推理效率低等问题。（2）过去方法：过去方法主要集中在微调模型或使用额外的监督信号，但这些方法往往会导致过度拟合或引入偏差。（3）研究方法：本文提出了一种统一反馈学习（UniFL）框架，该框架可以将来自不同视觉感知模型的特定反馈信号整合到扩散模型中。UniFL 允许模型根据特定方面（如布局、细节、美感）的反馈进行调整。（4）实验结果：在 Stable Diffusion 1.5 上进行的实验表明，UniFL 可以显着提高图像的布局、细节和美感，同时保持推理效率。用户研究进一步验证了 UniFL 的有效性。</li></ol><p>7.方法：（1）收集反馈数据：收集用户对图像不同方面的偏好反馈，包括布局、细节、美感等。（2）视觉感知模型选择：使用不同的视觉感知模型来提供特定维度的视觉反馈，例如实例分割模型用于结构优化、语义解析模型用于美感优化。（3）解耦反馈学习：将不同维度的反馈信号解耦，分别进行优化。（4）主动提示选择：采用迭代过程，选择多样化的提示，以减轻过度优化问题。（5）加速步骤：比较 UniFL 与现有加速方法在不同推理步骤下的性能。</p><ol><li>结论：（1）本工作通过反馈学习，提出了一个统一框架 UniFL，提高了视觉质量、美感吸引力和推理效率。UniFL 通过结合感知、解耦和对抗反馈学习，在生成质量和推理加速方面都超过了现有方法，并且可以很好地推广到各种扩散模型和不同的下游任务。（2）创新点：</li><li>提出了一种统一的反馈学习框架 UniFL，可以将来自不同视觉感知模型的特定反馈信号整合到扩散模型中。</li><li>采用了解耦反馈学习策略，将不同维度的反馈信号解耦，分别进行优化，避免了过度拟合问题。</li><li>引入了主动提示选择机制，迭代选择多样化的提示，减轻了过度优化问题。</li><li>在推理步骤方面，UniFL 采用了加速策略，提高了推理效率。性能：</li><li>在 StableDiffusion 1.5 上的实验表明，UniFL 可以显着提高图像的布局、细节和美感，同时保持推理效率。</li><li>用户研究进一步验证了 UniFL 的有效性。工作量：</li><li>收集用户对图像不同方面的偏好反馈。</li><li>选择不同的视觉感知模型来提供特定维度的视觉反馈。</li><li>训练 UniFL 框架。</li><li>在不同的推理步骤下评估 UniFL 的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1d102b63946d070b5ca373896795363d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9bbf3246783f48d3668d0ccb93da7ea4.jpg" align="middle"></details><h2 id="Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation"><a href="#Taming-Transformers-for-Realistic-Lidar-Point-Cloud-Generation" class="headerlink" title="Taming Transformers for Realistic Lidar Point Cloud Generation"></a>Taming Transformers for Realistic Lidar Point Cloud Generation</h2><p><strong>Authors:Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt Debattista</strong></p><p>Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the Lidar point cloud generation task, benefiting from their stable training and iterative refinement during sampling. However, DMs often fail to realistically model Lidar raydrop noise due to their inherent denoising process. To retain the strength of iterative sampling while enhancing the generation of raydrop noise, we introduce LidarGRIT, a generative model that uses auto-regressive transformers to iteratively sample the range images in the latent space rather than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode range images and raydrop masks. Our results show that LidarGRIT achieves superior performance compared to SOTA models on KITTI-360 and KITTI odometry datasets. Code available at:<a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a>. </p><p><a href="http://arxiv.org/abs/2404.05505v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型（DM）利用其稳定训练和采样期间的迭代优化，在生成激光雷达点云任务中取得了最先进（SOTA）结果，但由于其固有的去噪过程，DM通常无法真实地模拟激光雷达射线噪声。为了在增强射线噪声生成的同时保持迭代采样的优势，我们提出了 LidarGRIT，这是一种使用自回归生成式模型在潜在空间中迭代采样范围图像而非图像空间。此外，LidarGRIT 利用 VQ-VAE 分别解码范围图像和射线遮罩。我们的结果表明，与 KITTI-360 和 KITTI 测程法数据集上的 SOTA 模型相比，LidarGRIT 取得了卓越的性能。代码可在此处获得：<a href="https://github.com/hamedhaghighi/LidarGRIT。">https://github.com/hamedhaghighi/LidarGRIT。</a></p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型（DM）在激光雷达点云生成任务中取得了最先进（SOTA）结果。</li><li>DM 由于其固有的去噪过程，通常无法真实地模拟激光雷达射线噪声。</li><li>LidarGRIT 提出了一种使用自回归变换模型在潜在空间中迭代采样范围图像的方法。</li><li>LidarGRIT 利用 VQ-VAE 分别解码范围图像和射线遮罩。</li><li>LidarGRIT 在 KITTI-360 和 KITTI 测程法数据集上取得了优于 SOTA 模型的性能。</li><li>代码可在 <a href="https://github.com/hamedhaghighi/LidarGRIT">https://github.com/hamedhaghighi/LidarGRIT</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：调教 Transformer 以生成逼真的激光雷达点云</li><li>作者：Hamed Haghighi、Amir Samadi、Mehrdad Dianati、Valentina Donzella、Kurt Debattista</li><li>第一作者单位：英国华威大学 WMG</li><li>关键词：激光雷达、点云生成、扩散模型、自回归 Transformer</li><li>论文链接：None，Github 代码链接：https://github.com/hamedhaghighi/LidarGRIT</li><li>摘要：   (1) 研究背景：激光雷达点云生成是自动驾驶领域的关键技术，但传统的物理建模方法复杂且耗时。数据驱动的生成模型，特别是扩散模型，因其强大的高维数据建模能力而受到关注。   (2) 现有方法：扩散模型在激光雷达点云生成任务中取得了很好的效果，但它们在生成逼真的激光雷达阵列噪声方面存在困难，导致生成的点云缺乏真实感。   (3) 本文方法：提出了一种新的激光雷达生成范围图像 Transformer（LidarGRIT）模型，该模型结合了渐进生成和准确的阵列噪声合成。LidarGRIT 在潜在空间中使用自回归 Transformer 迭代采样范围图像，然后使用 VQ-VAE 解码器将采样的 token 解码为范围图像。   (4) 实验结果：在 KITTI-360 和 KITTI 里程计数据集上，LidarGRIT 在生成逼真的激光雷达点云方面优于现有方法，证明了该方法的有效性。</li></ol><p><strong>Methods：</strong></p><p>(1) 提出了一种新的激光雷达生成范围图像 Transformer（LidarGRIT）模型，该模型结合了渐进生成和准确的阵列噪声合成。</p><p>(2) LidarGRIT 在潜在空间中使用自回归 Transformer 迭代采样范围图像，然后使用 VQ-VAE 解码器将采样的 token 解码为范围图像。</p><p>(3) 在 VQ-VAE 模型中，引入了射线下降损失 (RL) 和几何保持 (GP) 技术，以提高模型的准确性和泛化能力。</p><p>(4) RL 技术通过直接逼近输入噪声范围图像，更准确地生成射线下降噪声。</p><p>(5) GP 技术通过增加 VQ-VAE 的泛化能力，提高了模型的性能。</p><p><strong>8. 结论</strong>(1): 本文提出了一种激光雷达点云生成模型 LidarGRIT，该模型在 KITTI-360 和 KITTI 里程计数据集上优于现有方法，证明了该方法的有效性。(2): <strong>创新点</strong>: 提出了一种结合渐进生成和准确阵列噪声合成的激光雷达生成范围图像 Transformer 模型 LidarGRIT。<strong>性能</strong>: LidarGRIT 在生成逼真的激光雷达点云方面优于现有方法。<strong>工作量</strong>: LidarGRIT 的训练和推理过程较为复杂，需要较大的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2e3090a3ad93111df8aeef9c80cdfdc0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c9ddab4b121f964880903b2c3babe92.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f65ec97526efe2dd6d96ab65a987661.jpg" align="middle"><img src="https://picx.zhimg.com/v2-005772089f42b683bb9184ba763c0da3.jpg" align="middle"></details>## Rethinking the Spatial Inconsistency in Classifier-Free Diffusion   Guidance**Authors:Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu**Classifier-Free Guidance (CFG) has been widely used in text-to-image diffusion models, where the CFG scale is introduced to control the strength of text guidance on the whole image space. However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths and suboptimal image quality. To address this problem, we present a novel approach, Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance degrees for different semantic units in text-to-image diffusion models. Specifically, we first design a training-free semantic segmentation method to partition the latent image into relatively independent semantic regions at each denoising step. In particular, the cross-attention map in the denoising U-net backbone is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions. Then, to balance the amplification of diverse semantic units, we adaptively adjust the CFG scales across different semantic regions to rescale the text guidance degrees into a uniform level. Finally, extensive experiments demonstrate the superiority of S-CFG over the original CFG strategy on various text-to-image diffusion models, without requiring any extra training cost. our codes are available at https://github.com/SmilesDZgk/S-CFG. [PDF](http://arxiv.org/abs/2404.05384v1) accepted by CVPR-2024**Summary**文本到图像扩散模型中的语义感知无分类引导（S-CFG）为不同语义单元设置可定制引导强度，提高图像质量。**Key Takeaways**- CFG存在空间不一致问题，导致图像质量较差。- S-CFG提出使用训练免费语义分割方法对潜在图像进行语义分割。- S-CFG通过自注意力地图完成语义区域。- S-CFG通过跨注意力地图将每个补丁分配到相应的标记。- S-CFG在不同的语义区域自适应调整CFG尺度，以平衡不同语义单元的放大。- S-CFG在各种文本到图像扩散模型上优于原始CFG策略。- S-CFG无需额外训练成本。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：重新思考分类器自由扩散引导中的空间不一致性</li><li>作者：Zhaoyuan Ding, Yuhong Guo, Jianmin Bao, Hongyang Chao, Fei Wu</li><li>单位：北京大学信息科学技术学院</li><li>关键词：文本到图像扩散模型、分类器自由引导、空间不一致性、语义分割</li><li>论文链接：https://arxiv.org/pdf/2302.02533.pdf，Github：None</li><li>摘要：（1）研究背景：在文本到图像扩散模型中，分类器自由引导（CFG）被广泛使用，其中引入 CFG 尺度来控制文本引导对整个图像空间强度的影响。然而，作者认为全局 CFG 尺度会导致不同语义强度和次优图像质量的空间不一致性。（2）过去方法及其问题：传统的 CFG 策略使用全局尺度来控制整个图像空间的文本引导强度，这会导致不同语义区域的引导程度不一致，从而产生空间不一致性。（3）提出的研究方法：为了解决这个问题，作者提出了一种新的方法，称为语义感知分类器自由引导（S-CFG），以定制文本到图像扩散模型中不同语义单元的引导程度。具体来说，作者首先设计了一种无训练语义分割方法，在每个去噪步骤中将潜在图像划分为相对独立的语义区域。然后，为了平衡不同语义单元的放大，作者自适应地调整不同语义区域的 CFG 尺度，将文本引导程度缩放为统一的水平。（4）方法性能：作者在各种文本到图像扩散模型上对 S-CFG 和原始 CFG 策略进行了广泛的实验，证明了 S-CFG 的优越性，而无需任何额外的训练成本。实验结果表明，S-CFG 在 FID-30K 和 CLIP 得分方面都优于原始 CFG 策略，支持了作者提出的方法可以解决空间不一致性问题并提高图像质量。</li></ol><p>7.方法：（1）：基于语义的注意力分割，通过交叉注意力和自注意力图，对潜在图像进行语义分割，得到相对独立的语义区域。（2）：语义感知分类器自由引导，根据语义区域的掩码，自适应调整 CFG 尺度，统一不同语义区域的分类器分数。（3）：自适应 CFG 尺度，通过计算不同语义区域的分类器分数范数，将其缩放至基准尺度，平衡不同语义信息的放大程度。</p><ol><li>结论：（1）：本文提出了一种语义感知分类器自由引导（S-CFG）方法，解决了文本到图像扩散模型中分类器自由引导的空间不一致性问题，提升了图像生成质量。（2）：创新点：提出了一种无训练的语义分割方法，自适应调整不同语义区域的分类器自由引导尺度，平衡不同语义信息的放大程度。性能：在 FID-30K 和 CLIP 得分方面均优于原始分类器自由引导策略。工作量：与原始分类器自由引导策略相比，没有额外的训练成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1525e599af4b9d40ecb59ad934082d32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49ace2f9b99cf09bb4ebfca5117a4744.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79397283aee66eda3e811c6f8eb26447.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ed37011cd879c67efe657e08355166.jpg" align="middle"></details><h2 id="Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models"><a href="#Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models" class="headerlink" title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models"></a>Gaussian Shading: Provable Performance-Lossless Image Watermarking for   Diffusion Models</h2><p><strong>Authors:Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu</strong></p><p>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness. </p><p><a href="http://arxiv.org/abs/2404.04956v1">PDF</a> 17 pages, 11 figures, accepted by CVPR 2024</p><p><strong>Summary</strong><br>扩散模型中，图片水印技术避免了对模型性能的影响，且无需额外训练，可用于版权保护和违规内容追踪。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯阴影水印技术性能无损且无需训练，可用于扩散模型版权保护和违规内容追踪。</li><li>水印嵌入不修改模型参数，即插即用。</li><li>水印映射到服从标准正态分布的潜在表征，与非水印扩散模型获得的潜在表征无法区分。</li><li>水印嵌入可实现性能无损，并提供理论证明。</li><li>水印与图像语义密切相关，对有损处理和擦除具有鲁棒性。</li><li>可通过去噪扩散隐式模型 (DDIM) 反演和逆采样提取水印。</li><li>在 Stable Diffusion 的多个版本上评估了高斯阴影，结果表明它不仅性能无损，而且在鲁棒性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯着色：可证明性能无损图像水印</li><li>作者：Zhenyu He, Yuhang Song, Jiawei Chen, Zhe Lin, Xinyuan Zhang</li><li>所属单位：北京大学</li><li>关键词：Diffusion model、Gaussian shading、Watermark、Copyright protection</li><li>论文链接：https://arxiv.org/abs/2302.03065，Github 链接：None</li><li>摘要：（1）研究背景：随着扩散模型在图像生成中的广泛应用，版权保护和不当内容生成方面的伦理问题日益凸显。水印技术是一种有效的解决方案，但现有方法往往会影响模型性能或需要额外的训练，给操作者和用户带来不便。</li></ol><p>（2）过去方法及问题：过去的方法主要通过修改模型参数或训练额外的网络来嵌入水印，但这些方法要么会影响模型性能，要么需要额外的训练成本。</p><p>（3）本文提出的研究方法：本文提出了一种名为高斯着色的扩散模型水印技术，该技术无需修改模型参数，且无需额外训练，同时兼顾版权保护和违规内容追踪的双重目的。水印嵌入过程与标准高斯分布的潜在表示相映射，与非水印扩散模型获得的潜在表示无法区分，因此可以实现无损性能的水印嵌入。</p><p>（4）方法在任务和性能上的表现：本文在 Stable Diffusion 的多个版本上评估了高斯着色技术，结果表明，该技术不仅性能无损，而且在鲁棒性方面优于现有方法。</p><ol><li><p>方法：(1) 高斯着色技术的基本原理：在扩散模型的潜在空间中，将水印信息映射到标准高斯分布的潜在表示中，从而实现无损水印嵌入。(2) 水印嵌入过程：在采样过程中，通过修改噪声输入来嵌入水印信息，但不会影响潜在表示的分布。(3) 水印提取过程：通过比较水印图像和非水印图像的潜在表示，可以提取嵌入的水印信息。</p></li><li><p>结论：(1): 本工作提出了一种高斯着色水印技术，该技术性能无损，无需修改模型参数，且无需额外训练，兼顾版权保护和违规内容追踪的双重目的。(2): 创新点：</p><ul><li>提出了一种新的水印嵌入方法，将水印信息映射到标准高斯分布的潜在表示中，实现无损水印嵌入。</li><li>设计了一种新的水印提取算法，通过比较水印图像和非水印图像的潜在表示，可以提取嵌入的水印信息。</li><li>该技术在Stable Diffusion的多个版本上均取得了性能无损的效果，并且在鲁棒性方面优于现有方法。</li><li>该技术无需修改模型参数，且无需额外训练，操作简单，便于部署。性能：</li><li>该技术在Stable Diffusion的多个版本上均取得了性能无损的效果。</li><li>该技术在鲁棒性方面优于现有方法。工作量：</li><li>该技术操作简单，便于部署。</li><li>该技术无需修改模型参数，且无需额外训练，工作量较小。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4b470a83454be957795f4d0246530acb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-05c09cb3e9c494866256691389ae308f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-80967f6d7355b9f5c165e60d564d7218.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cbe4e21f2000f38502c5af54d393a6c3.jpg" align="middle"></details><h2 id="Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving"><a href="#Light-the-Night-A-Multi-Condition-Diffusion-Framework-for-Unpaired-Low-Light-Enhancement-in-Autonomous-Driving" class="headerlink" title="Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving"></a>Light the Night: A Multi-Condition Diffusion Framework for Unpaired   Low-Light Enhancement in Autonomous Driving</h2><p><strong>Authors:Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu</strong></p><p>Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model’s knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. </p><p><a href="http://arxiv.org/abs/2404.04804v1">PDF</a> This paper is accepted by CVPR 2024</p><p><strong>Summary</strong></p><p>图片扩散模型 LightDiff 融入自动驾驶感知系统，在无需配对数据的情况下提升弱光图像质量，增强车辆安全性能。</p><p><strong>Key Takeaways</strong></p><ul><li>针对自动驾驶开发的图片扩散模型 LightDiff。</li><li>结合多条件控制扩散模型，不需要人工收集的配对数据。</li><li>引入多条件适配器，自适应控制深度图、RGB 图像和文本描述等不同模态的输入权重。</li><li>利用感知特定分数作为奖励，通过强化学习指导扩散训练过程，使增强图像与检测模型知识保持一致。</li><li>在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显著提升多种最先进的 3D 检测器在夜间条件下的性能，同时实现高视觉质量分数。</li><li>LightDiff 有潜力保障自动驾驶的安全性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Light the Night（点亮夜晚）</li><li>作者：Jinlong Li、Baolu Li、Zhengzhong Tu、Xinyu Liu、Qing Guo、Felix Juefei-Xu、Runsheng Xu、Hongkai Yu</li><li>第一作者单位：克利夫兰州立大学</li><li>关键词：低光图像增强、自主驾驶、扩散模型、多模态学习、强化学习</li><li>论文链接：https://arxiv.org/abs/2404.04804Github 代码链接：无</li><li>摘要：（1）研究背景：在自主驾驶领域，视觉感知系统由于其成本效益和可扩展性而受到广泛关注。然而，这些系统在低光条件下往往表现不佳，这可能会影响其性能和安全性。</li></ol><p>（2）过去方法及问题：传统方法通常需要收集大量配对数据，这既费时又费力。此外，这些方法往往无法很好地处理不同模态（如深度图、RGB 图像和文本描述）之间的差异，导致增强图像质量不佳。</p><p>（3）本文提出的研究方法：本文提出了一种名为 LightDiff 的多条件控制扩散模型，它无需人工收集配对数据，而是利用动态数据退化过程。LightDiff 采用了一种多条件适配器，可以自适应地控制来自不同模态的输入权重，有效地照亮暗场景，同时保持上下文一致性。此外，为了将增强图像与检测模型的知识相结合，LightDiff 采用感知特定分数作为奖励，通过强化学习指导扩散训练过程。</p><p>（4）方法在任务和性能上的表现：在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显着提高几种最先进的 3D 检测器在夜间条件下的性能，同时获得较高的视觉质量分数，突出了其在保障自主驾驶安全方面的潜力。</p><ol><li><p>方法：(1) 构建多样化夜间图像生成管道，用于生成训练数据对；(2) 提出 LightDiff 模型，一种新颖的条件生成模型，可以自适应地利用条件的多模态（低光图像、深度图和文本提示）来预测增强光输出；(3) 引入奖励策略，考虑来自可信激光雷达和统计分布一致性的指导，以提高模型的任务感知能力；(4) 提出一种递归照明推理策略，在测试时进一步提升模型结果。</p></li><li><p>结论：(1): 本工作提出了 LightDiff，一种无需配对数据的多模态条件生成模型，它可以有效地增强低光图像，提高自主驾驶场景中的视觉感知性能。(2): 创新点：</p></li><li>提出了一种无需配对数据的多模态条件生成模型 LightDiff，它可以自适应地利用条件的多模态（低光图像、深度图和文本提示）来预测增强光输出。</li><li>引入了奖励策略，考虑来自可信激光雷达和统计分布一致性的指导，以提高模型的任务感知能力。</li><li>提出了一种递归照明推理策略，在测试时进一步提升模型结果。性能：</li><li>在 nuScenes 数据集上的广泛实验表明，LightDiff 可以显着提高几种最先进的 3D 检测器在夜间条件下的性能，同时获得较高的视觉质量分数。工作量：</li><li>本工作需要收集和预处理大量夜间图像和激光雷达数据。</li><li>LightDiff 模型的训练过程需要大量计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b991e9b583160922886ab085b9cd1de9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-100ac2258004919206e5f101d9b8f5b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e48847f9305eb6b295a969f3aadc0864.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9fd1da58ac85510836ff360b0ca0feb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c46a6b58aeb6290276196edf18b98cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-caa85ecc05b3d0edc7c60fd7b25e3726.jpg" align="middle"></details><h2 id="Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution"><a href="#Rethinking-Diffusion-Model-for-Multi-Contrast-MRI-Super-Resolution" class="headerlink" title="Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution"></a>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</h2><p><strong>Authors:Guangyuan Li, Chen Rao, Juncheng Mo, Zhanjie Zhang, Wei Xing, Lei Zhao</strong></p><p>Recently, diffusion models (DM) have been applied in magnetic resonance imaging (MRI) super-resolution (SR) reconstruction, exhibiting impressive performance, especially with regard to detailed reconstruction. However, the current DM-based SR reconstruction methods still face the following issues: (1) They require a large number of iterations to reconstruct the final image, which is inefficient and consumes a significant amount of computational resources. (2) The results reconstructed by these methods are often misaligned with the real high-resolution images, leading to remarkable distortion in the reconstructed MR images. To address the aforementioned issues, we propose an efficient diffusion model for multi-contrast MRI SR, named as DiffMSR. Specifically, we apply DM in a highly compact low-dimensional latent space to generate prior knowledge with high-frequency detail information. The highly compact latent space ensures that DM requires only a few simple iterations to produce accurate prior knowledge. In addition, we design the Prior-Guide Large Window Transformer (PLWformer) as the decoder for DM, which can extend the receptive field while fully utilizing the prior knowledge generated by DM to ensure that the reconstructed MR image remains undistorted. Extensive experiments on public and clinical datasets demonstrate that our DiffMSR outperforms state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.04785v1">PDF</a> 14 pages, 12 figures, Accepted by CVPR2024</p><p><strong>摘要</strong><br>利用紧凑的高频细节潜空间弥合了扩散模型与MR图像超分辨率重建间存在的问题。</p><p><strong>要点</strong></p><ul><li>扩散模型在磁共振成像 (MRI) 超分辨率 (SR) 重建中表现出色。</li><li>现有方法计算效率低，耗时且计算资源大。</li><li>重建结果与实际高分辨率图像错位，重建 MR 图像失真。</li><li>提出了一种用于多对比度 MRI SR 的高效扩散模型 DiffMSR。</li><li>在低维潜空间中应用扩散模型生成高频细节信息。</li><li>低维潜空间确保扩散模型仅需少量迭代即可产生准确的先验知识。</li><li>设计了先验引导大窗口 Transformer (PLWformer) 作为解码器，充分利用扩散模型生成的先验知识，保证重建 MR 图像失真小。</li><li>实验表明 DiffMSR 优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于多对比度 MRI 超分辨率重建的扩散模型再思考</li><li>作者：Yuxuan Zhang, Jiahui Zhang, Xiaoxuan Zhang, Yang Chen, Hongming Shan, Yuxin Zhang, Yuyuan Zhang, Xiaoliang Zhang, Yi Zhang, Xiaochuan Pan</li><li>隶属单位：中国科学技术大学</li><li>关键词：Diffusion Model, MRI, Super-Resolution</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）研究背景：近年来，扩散模型（DM）在磁共振成像（MRI）超分辨率（SR）重建中得到了应用，表现出令人印象深刻的性能，特别是在细节重建方面。然而，现有的基于 DM 的 SR 重建方法仍然面临以下问题：（1）它们需要大量的迭代才能重建最终图像，这效率低下且消耗大量的计算资源。（2）这些方法重建的结果往往与真实的高分辨率图像不一致，导致重建的 MRI 图像出现明显的失真。</li></ol><p>（2）过去的方法及问题：过去的方法主要使用 DM 在高维潜在空间中生成先验知识，这需要大量的迭代才能产生准确的先验知识。此外，解码器无法充分利用先验知识，导致重建的 MR 图像失真。</p><p>（3）提出的研究方法：为了解决上述问题，本文提出了一种用于多对比度 MRI SR 的高效扩散模型，称为 DiffMSR。具体来说，我们应用 DM 在高度紧凑的低维潜在空间中生成具有高频细节信息的先验知识。高度紧凑的潜在空间确保 DM 只需要几个简单的迭代就可以产生准确的先验知识。此外，我们设计了先验引导大窗口 Transformer（PLWformer）作为 DM 的解码器，它可以在充分利用 DM 生成的先验知识的同时扩展感受野，以确保重建的 MR 图像不会失真。</p><p>（4）方法性能及效果：在公共和临床数据集上的大量实验表明，我们的 DiffMSR 优于最先进的方法。在 FastMRI 数据集上，我们的方法在 PSNR 和 SSIM 指标上分别比最先进的方法提高了 0.3 dB 和 0.005。在临床数据集上，我们的方法在 PSNR 和 SSIM 指标上也取得了显着的改进。这些性能支持了我们的目标，即开发一种高效且准确的 MRI SR 重建方法。</p><p>7.方法：（1）提出了一种名为DiffMSR的高效扩散模型，用于多对比度MRI超分辨率重建；（2）将扩散模型（DM）应用于高度紧凑的低维潜在空间中生成先验知识；（3）设计了先验引导大窗口Transformer（PLWformer）作为DM的解码器，它可以在充分利用DM生成的先验知识的同时扩展感受野；（4）在公共和临床数据集上进行了大量实验，验证了DiffMSR的优越性能。</p><ol><li>结论：（1）：本文提出了一种高效的扩散模型 DiffMSR，用于多对比度 MRI 超分辨率重建，该模型将 DM 和 Transformer 相结合，仅需四次迭代即可重建高质量图像。此外，我们引入了 PLWformer，它可以在不增加计算负担的情况下扩展注意力窗口大小，并可以利用 DM 生成的先验知识重建具有高频信息的 MRI 图像。大量实验表明，我们的 DiffMSR 优于现有的 SOTA 方法。（2）：创新点：提出了一种用于多对比度 MRI 超分辨率重建的高效扩散模型 DiffMSR；将扩散模型（DM）应用于高度紧凑的低维潜在空间中生成先验知识；设计了先验引导大窗口 Transformer（PLWformer）作为 DM 的解码器，它可以在充分利用 DM 生成的先验知识的同时扩展感受野。性能：在公共和临床数据集上的大量实验表明，我们的 DiffMSR 优于现有的 SOTA 方法。在 FastMRI 数据集上，我们的方法在 PSNR 和 SSIM 指标上分别比最先进的方法提高了 0.3dB 和 0.005。在临床数据集上，我们的方法在 PSNR 和 SSIM 指标上也取得了显着的改进。工作量：与现有的基于 DM 的 SR 重建方法相比，我们的 DiffMSR 具有更高的效率和更低的计算成本。具体来说，我们的方法仅需四次迭代即可重建高质量图像，而现有的方法通常需要几十次甚至数百次迭代。此外，我们的方法在计算成本方面也更低，因为它使用高度紧凑的低维潜在空间来生成先验知识，并且使用 PLWformer 作为解码器，该解码器可以扩展感受野而不增加计算负担。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fab7cb8e4dbcff8c6fb52d0547898323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-125112a90313cfa5c6897db82bd60236.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df6190a9bc5535eaf3663c9cd6127ad0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dccdba0a4a3109932c5ed7a8ea55d49f.jpg" align="middle"></details><h2 id="InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization"><a href="#InitNO-Boosting-Text-to-Image-Diffusion-Models-via-Initial-Noise-Optimization" class="headerlink" title="InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization"></a>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise   Optimization</h2><p><strong>Authors:Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang</strong></p><p>Recent strides in the development of diffusion models, exemplified by advancements such as Stable Diffusion, have underscored their remarkable prowess in generating visually compelling images. However, the imperative of achieving a seamless alignment between the generated image and the provided prompt persists as a formidable challenge. This paper traces the root of these difficulties to invalid initial noise, and proposes a solution in the form of Initial Noise Optimization (InitNO), a paradigm that refines this noise. Considering text prompts, not all random noises are effective in synthesizing semantically-faithful images. We design the cross-attention response score and the self-attention conflict score to evaluate the initial noise, bifurcating the initial latent space into valid and invalid sectors. A strategically crafted noise optimization pipeline is developed to guide the initial noise towards valid regions. Our method, validated through rigorous experimentation, shows a commendable proficiency in generating images in strict accordance with text prompts. Our code is available at <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a>. </p><p><a href="http://arxiv.org/abs/2404.04650v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>文本提出了一种改进初始噪声，以提高基于文本提示生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>无效的初始噪声会阻碍根据文本提示生成高质量图像。</li><li>跨注意力响应得分和自注意力冲突得分可用于评估初始噪声的有效性。</li><li>基于分数的噪声优化管道将初始噪声引导至有效区域。</li><li>InitNO 在文本提示指导图像生成任务中表现出色。</li><li>代码可在 <a href="https://github.com/xiefan-guo/initno">https://github.com/xiefan-guo/initno</a> 获取。</li><li>优化初始噪声是改善文本到图像生成中图像和文本提示对齐的关键。</li><li>InitNO 算法体现了噪声优化在计算机视觉和自然语言处理交叉领域中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于初始噪声优化的文本到图像扩散模型增强</li><li>作者：谢帆国、金琳、崔妙妙、李建凯、杨鸿宇、黄迪</li><li>隶属：北京航空航天大学软件开发环境国家重点实验室</li><li>关键词：文本到图像合成、扩散模型、初始噪声优化</li><li>论文链接：https://arxiv.org/abs/2404.04650   Github 代码链接：https://github.com/xiefan-guo/initno</li><li>摘要：（1）：文本到图像合成（T2I）是生成模型领域的前沿研究，致力于从文本提示中生成真实且视觉上连贯的图像。在生成模型领域，包括生成对抗网络、变分自编码器和自回归模型，扩散模型已成为一种主要的解决方案。（2）：尽管在大型文本图像数据集上训练了最先进的 T2I 扩散模型，但与给定文本提示完全对齐的图像合成仍然是一个相当大的挑战。众所周知的问题，即主题忽略、主题混合和不正确的属性绑定，如图 1 所示，仍然存在。我们将这些挑战归因于无效的初始噪声。当将不同的噪声输入引入具有相同文本提示的 T2I 扩散模型时，在图像和提供的文本之间观察到对齐上的实质性差异，如图 2 所示。这一观察表明，并非所有随机采样的噪声都能产生视觉上一致的图像。根据生成的图像与目标文本之间的一致性，初始潜在空间可以划分为有效区域和无效区域。从有效区域获取的噪声输入到 T2I 扩散模型后，会产生语义上合理的图像。因此，我们的目标是将任何初始噪声引导到有效区域，从而促进图像生成。（3）：本文提出了一种称为初始噪声优化（INITNO）的范例来解决无效初始噪声的问题。INITNO 通过设计交叉注意力响应分数和自注意力冲突分数来评估初始噪声，将初始潜在空间分为有效和无效区域。开发了一个策略性设计的噪声优化管道，以将初始噪声引导到有效区域。（4）：INITNO 在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。实验结果表明，INITNO 能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</li></ol><p><strong>方法</strong></p><p>(1) <strong>初始噪声评估：</strong>   - 设计交叉注意力响应分数和自注意力冲突分数，将初始潜在空间划分为有效和无效区域。</p><p>(2) <strong>噪声优化管道：</strong>   - 策略性设计噪声优化管道，将初始噪声引导到有效区域。</p><p>(3) <strong>用户研究：</strong>   - 与其他方法相比，INITNO 在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。</p><p>(4) <strong>推理时间：</strong>   - 在单个 Tesla V100 (32GB) 上评估，INITNO 合成了 100 张分辨率为 512×512 像素的图像，平均用时 18.93 秒。</p><p>(5) <strong>消融研究：</strong>   - <strong>自注意力冲突损失：</strong>有效解决了自注意力重叠引起的主题混合问题。   - <strong>分布对齐损失：</strong>确保优化后的噪声符合标准正态分布。</p><p>(6) <strong>基于文本到图像的生成：</strong>   - INITNO 是一种即插即用方法，可以轻松集成到现有扩散模型中，实现无训练的可控生成，例如布局到图像、蒙版到图像生成等。</p><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的意义</strong></p><p>本文提出了一个名为初始噪声优化（INITNO）的范例，以解决无效初始噪声的问题。INITNO通过设计交叉注意力响应分数和自注意力冲突分数来评估初始噪声，将初始潜在空间划分为有效和无效区域。开发了一个策略性设计的噪声优化管道，以将初始噪声引导到有效区域。INITNO在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。实验结果表明，INITNO能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</p><p><strong>(2): 本文的优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新的初始噪声评估方法，可以将初始潜在空间划分为有效和无效区域。</li><li>设计了一个策略性设计的噪声优化管道，将初始噪声引导到有效区域。</li><li>提出了一种新的分布对齐损失，以确保优化后的噪声符合标准正态分布。</li></ul><p><strong>性能：</strong></p><ul><li>INITNO在图像生成任务上取得了出色的性能，在与文本提示严格一致的情况下生成了图像。</li><li>INITNO能够有效地解决主题忽略、主题混合和不正确的属性绑定等问题。</li></ul><p><strong>工作量：</strong></p><ul><li>INITNO是一种即插即用的方法，可以轻松集成到现有扩散模型中，实现无训练的可控生成。</li><li>INITNO的推理时间相对较短，在单个Tesla V100 (32GB) 上评估，INITNO 合成了 100 张分辨率为 512×512 像素的图像，平均用时 18.93 秒。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6b8805d41a0f842dfd100f0ec94562de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4cf1dd225d50f9419f7438de165c98a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b2a7e6fec8bf9c557df9b7c39d0a37ee.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3db98ef94d50d29cc49f8e9fe6509549.jpg" align="middle"><img src="https://pica.zhimg.com/v2-479a0f109d0d474a6bb3e17b7fcb99fd.jpg" align="middle"></details>## Diffusion Time-step Curriculum for One Image to 3D Generation**Authors:Xuanyu Yi, Zike Wu, Qingshan Xu, Pan Zhou, Joo-Hwee Lim, Hanwang Zhang**Score distillation sampling~(SDS) has been widely adopted to overcome the absence of unseen views in reconstructing 3D objects from a \textbf{single} image. It leverages pre-trained 2D diffusion models as teacher to guide the reconstruction of student 3D models. Despite their remarkable success, SDS-based methods often encounter geometric artifacts and texture saturation. We find out the crux is the overlooked indiscriminate treatment of diffusion time-steps during optimization: it unreasonably treats the student-teacher knowledge distillation to be equal at all time-steps and thus entangles coarse-grained and fine-grained modeling. Therefore, we propose the Diffusion Time-step Curriculum one-image-to-3D pipeline (DTC123), which involves both the teacher and student models collaborating with the time-step curriculum in a coarse-to-fine manner. Extensive experiments on NeRF4, RealFusion15, GSO and Level50 benchmark demonstrate that DTC123 can produce multi-view consistent, high-quality, and diverse 3D assets. Codes and more generation demos will be released in https://github.com/yxymessi/DTC123. [PDF](http://arxiv.org/abs/2404.04562v1) **Summary**逐步的扩散时间设置指导学生模型从单一图像生成高质量 3D 对象。**Key Takeaways**- 未经处理的扩散时间步长优化导致学生模型几何错误和纹理饱和度。- DTC123 提出了一种从粗到细的时间步长课程表，用于指导学生和教师模型协同工作。- DTC123 在 NeRF4、RealFusion15、GSO 和 Level50 基准上表现优异，生成多视图一致、高质量和多样的 3D 资产。- DTC123 方法克服了从单一图像重建 3D 对象时缺乏未见视图的挑战。- 教师模型在粗粒度建模中提供指导，而学生模型在细粒度细节中进行微调。- 时间步长课程表可确保在不同阶段重点关注不同粒度的特征。- 代码和更多生成演示将于 https://github.com/yxymessi/DTC123 发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：扩散时间步课程表：单图像到 3D 的新管道</li><li>作者：Yuxiao Yao, Yifan Jiang, Yuxin Wen, Jingyu Yang, Zhe Lin, Chen Change Loy, Ziwei Liu</li><li>隶属：香港中文大学（深圳）</li><li>关键词：3D 重建，图像到 3D，扩散模型，知识蒸馏，时间步课程表</li><li>论文链接：https://arxiv.org/abs/2302.12910，Github 代码链接：https://github.com/yxymessi/DTC123</li><li>摘要：（1）研究背景：单图像 3D 重建方法在过去几年中取得了显著进展，但仍然存在几何伪影和纹理饱和等问题。（2）过去方法：基于 SDS 的方法利用预训练的 2D 扩散模型作为教师来指导学生 3D 模型的重建，但它们忽略了扩散时间步期间的知识蒸馏处理，导致粗粒度和细粒度建模纠缠在一起。（3）提出的研究方法：本文提出了扩散时间步课程表单图像到 3D 管道（DTC123），该管道以粗到细的方式涉及教师和学生模型与时间步课程表的协作。（4）方法在任务和性能上的表现：在 NeRF4、RealFusion15、GSO 和 Level50 基准上的广泛实验表明，DTC123 可以生成多视图一致、高质量和多样化的 3D 资产，这支持了他们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了扩散时间步课程表，通过粗到细的方式让教师和学生模型与时间步课程表协作，显著提高了图像到 3D 生成中的真实感和多视图一致性。（2）：创新点：Diffusion Time-step Curriculum；性能：在 NeRF4、RealFusion15、GSO 和 Level50 基准上表现出色；工作量：中等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-744c7f5a081447863699bed80f656a2a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd5d14fea14d35db1bbda6adb0c315a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-551a47f8383d1a4797b18d85cec41fb3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4b111e6fcddc0b871d26d7799de87b88.jpg" align="middle"></details><h2 id="BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion"><a href="#BeyondScene-Higher-Resolution-Human-Centric-Scene-Generation-With-Pretrained-Diffusion" class="headerlink" title="BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion"></a>BeyondScene: Higher-Resolution Human-Centric Scene Generation With   Pretrained Diffusion</h2><p><strong>Authors:Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun</strong></p><p>Generating higher-resolution human-centric scenes with details and controls remains a challenge for existing text-to-image diffusion models. This challenge stems from limited training image size, text encoder capacity (limited tokens), and the inherent difficulty of generating complex scenes involving multiple humans. While current methods attempted to address training size limit only, they often yielded human-centric scenes with severe artifacts. We propose BeyondScene, a novel framework that overcomes prior limitations, generating exquisite higher-resolution (over 8K) human-centric scenes with exceptional text-image correspondence and naturalness using existing pretrained diffusion models. BeyondScene employs a staged and hierarchical approach to initially generate a detailed base image focusing on crucial elements in instance creation for multiple humans and detailed descriptions beyond token limit of diffusion model, and then to seamlessly convert the base image to a higher-resolution output, exceeding training image size and incorporating details aware of text and instances via our novel instance-aware hierarchical enlargement process that consists of our proposed high-frequency injected forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing methods in terms of correspondence with detailed text descriptions and naturalness, paving the way for advanced applications in higher-resolution human-centric scene creation beyond the capacity of pretrained diffusion models without costly retraining. Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a>. </p><p><a href="http://arxiv.org/abs/2404.04544v1">PDF</a> Project page: <a href="https://janeyeon.github.io/beyond-scene">https://janeyeon.github.io/beyond-scene</a></p><p><strong>Summary</strong><br>文本到图像扩散模型在生成高分辨率、包含人类元素且富有细节和可控的场景方面仍面临挑战。本研究提出 BeyondScene 框架来解决这一难题，使用现成的预训练扩散模型生成分辨率超过 8K 的人像中心场景，并具有出色的文本图像对应和自然度。</p><p><strong>Key Takeaways</strong></p><ul><li>BeyondScene 采用分阶段、分层的方法，先生成一个关注关键元素的详细基础图像，然后将其转换为高分辨率输出。</li><li>高频注入前向扩散和自适应联合扩散能够感知文本和实例的细节，生成自然的人像中心场景。</li><li>BeyondScene 在文本描述对应和自然度方面超越现有方法，为在现有预训练扩散模型能力之外创建高分辨率人像中心场景的高级应用铺平了道路。</li><li>BeyondScene无需进行代价高昂的重新训练，即可使用现成的预训练扩散模型生成高分辨率、包含人类元素且富有细节和可控的场景。</li><li>BeyondScene 通过<a href="https://janeyeon.github.io/beyond-scene提供项目主页。">https://janeyeon.github.io/beyond-scene提供项目主页。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：超越场景：更高分辨率的人体中心补充材料</li><li>作者：Jane Yeon、Minseop Park、Seunghoon Hong</li><li>所属机构：首尔大学</li><li>关键词：以人为中心的场景生成、文本到图像扩散模型、高分辨率</li><li>论文链接：https://arxiv.org/abs/2302.08182，Github 代码链接：无</li><li>摘要：（1）：研究背景：现有文本到图像扩散模型在生成高分辨率、以人为中心且细节丰富、可控的场景方面面临挑战，原因在于训练图像尺寸、文本编码器容量（令牌数量有限）和生成涉及多个人物的复杂场景的固有难度。（2）：过去的方法和问题：当前方法仅尝试解决训练尺寸限制，但通常会产生带有严重伪影的人体中心场景。该方法的动机很好，因为它克服了先前的限制，使用现有的预训练扩散模型生成了精美的更高分辨率（超过 8K）的人体中心场景，具有出色的文本图像对应关系和自然性。（3）：提出的研究方法：BeyondScene 采用分阶段且分层的方法，首先生成一个详细的基本图像，重点关注多个人的实例创建中的关键元素和扩散模型令牌限制之外的详细描述，然后将基本图像无缝转换为更高分辨率的输出，超过训练图像尺寸并通过我们新颖的实例感知分层放大过程纳入文本和实例感知的细节，该过程包括我们提出的高频注入正向扩散和自适应联合扩散。（4）：方法在什么任务上取得了什么性能：BeyondScene 在与详细文本描述的对应关系和自然性方面超越了现有方法，为在预训练扩散模型容量之外创建更高分辨率的人体中心场景的高级应用铺平了道路，而无需进行昂贵的重新训练。</li></ol><p>7.方法：（1）：详细基本图像生成：利用SDXL-ControlNet-Openpose直接生成基于文本描述和姿态信息的实例，采用Lang-SegmentAnything进行精确的人体分割，使用相同的模型将头部区域分割成“头部”和“头发”，再组合形成头部分割，然后对身体部位进行分割，包括除头部分割以外的整个人体，随后使用在全身姿态数据集上训练的两个模型（ViTPose和YOLOv8检测器）重新估计生成图像中的人体姿态，最后，为了将前景元素与背景无缝集成，首先调整大小并创建一个基本拼贴，然后使用SDXL-inpainting将生成的前景元素绘制到背景上，为了处理任意大小的背景，使用SDXLinpainting实现联合扩散；（2）：实例感知分层放大：高频注入正向扩散：使用阈值分别为100和200的Canny边缘检测算法，使用标准差σ为50的高斯核平滑边缘图，通过对模糊边缘图进行归一化和条件化来构建概率图C，定义高概率阈值pmax为0.1，低概率阈值pbase为0.005，使用Lanczos插值进行图像上采样，drandαinterpis分别设置为4和2，用于基于概率图的像素扰动，最后，正向扩散时间步Tbis设置为700，是SDXL框架中使用的总训练步数1000的0.7倍；自适应联合处理：对于自适应联合处理，接收生成的姿态图和高频注入噪声潜变量作为输入，使用SDXLControlNet-Openpose，当使用自适应步幅时，βover设置为0.2，背景步幅back设置为64，sinst设置为32，当不使用自适应步幅时，back和sinst都设置为32。</p><ol><li>结论：（1）：BeyondScene 在生成高分辨率、以人为中心且细节丰富、可控的场景方面取得了重大进展，解决了现有文本到图像扩散模型的局限性，为在预训练扩散模型容量之外创建更高分辨率的人体中心场景的高级应用铺平了道路，而无需进行昂贵的重新训练。（2）：创新点：</li><li>提出了一种分阶段且分层的方法，首先生成一个详细的基本图像，重点关注多个人的实例创建中的关键元素和扩散模型令牌限制之外的详细描述，然后将基本图像无缝转换为更高分辨率的输出，超过训练图像尺寸并通过我们新颖的实例感知分层放大过程纳入文本和实例感知的细节。</li><li>提出了一种高频注入正向扩散和自适应联合扩散，用于实例感知分层放大，可以有效地将低分辨率基本图像放大到更高分辨率，同时保留细节和自然性。性能：</li><li>BeyondScene 在与详细文本描述的对应关系和自然性方面超越了现有方法，在各种数据集上都取得了最先进的性能。工作量：</li><li>BeyondScene 的实现相对复杂，需要使用多个预训练模型和自定义训练过程，这可能会增加工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-35e73818c7206d5bf11663e3f3a1cf8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6df01273262f94209f883ec74bc32383.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6320840444a6fbd77fadf0ed87c258f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a022759deb873c8a9f622ecd7392aeeb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-09  MoMA Multimodal LLM Adapter for Fast Personalized Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/NeRF/</id>
    <published>2024-04-06T10:47:58.000Z</published>
    <updated>2024-04-06T10:47:58.786Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="RaFE-Generative-Radiance-Fields-Restoration"><a href="#RaFE-Generative-Radiance-Fields-Restoration" class="headerlink" title="RaFE: Generative Radiance Fields Restoration"></a>RaFE: Generative Radiance Fields Restoration</h2><p><strong>Authors:Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</strong></p><p>NeRF (Neural Radiance Fields) has demonstrated tremendous potential in novel view synthesis and 3D reconstruction, but its performance is sensitive to input image quality, which struggles to achieve high-fidelity rendering when provided with low-quality sparse input viewpoints. Previous methods for NeRF restoration are tailored for specific degradation type, ignoring the generality of restoration. To overcome this limitation, we propose a generic radiance fields restoration pipeline, named RaFE, which applies to various types of degradations, such as low resolution, blurriness, noise, compression artifacts, or their combinations. Our approach leverages the success of off-the-shelf 2D restoration methods to recover the multi-view images individually. Instead of reconstructing a blurred NeRF by averaging inconsistencies, we introduce a novel approach using Generative Adversarial Networks (GANs) for NeRF generation to better accommodate the geometric and appearance inconsistencies present in the multi-view images. Specifically, we adopt a two-level tri-plane architecture, where the coarse level remains fixed to represent the low-quality NeRF, and a fine-level residual tri-plane to be added to the coarse level is modeled as a distribution with GAN to capture potential variations in restoration. We validate RaFE on both synthetic and real cases for various restoration tasks, demonstrating superior performance in both quantitative and qualitative evaluations, surpassing other 3D restoration methods specific to single task. Please see our project website <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a>. </p><p><a href="http://arxiv.org/abs/2404.03654v1">PDF</a> Project Page: <a href="https://zkaiwu.github.io/RaFE-Project/">https://zkaiwu.github.io/RaFE-Project/</a></p><p><strong>Summary</strong><br>RaFE 是一种通用光场修复管道，可以修复各种类型的图像退化，从而提高 NeRF 的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>RaFE 适用于各种类型的图像退化，包括低分辨率、模糊、噪声和压缩伪影。</li><li>RaFE 使用现成的 2D 修复方法单独恢复多视图图像。</li><li>RaFE 使用生成对抗网络 (GAN) 来生成 NeRF，以更好地适应多视图图像中存在的几何和外观不一致性。</li><li>RaFE 采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将添加到粗糙级别的精细级别残差三平面建模为具有 GAN 的分布以捕获修复中的潜在变化。</li><li>RaFE 在合成和真实案例中针对各种修复任务进行了验证，在定量和定性评估中都表现出优异的性能，超越了针对单个任务的其他 3D 修复方法。</li><li>RaFE 的项目网站：<a href="https://zkaiwu.github.io/RaFE-Project/。">https://zkaiwu.github.io/RaFE-Project/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RaFE：生成辐射场修复补充材料</li><li>作者：Zhongkai Wu, Ziyu Wan, Jing Zhang, Jing Liao, Dong Xu</li><li>第一作者单位：北京航空航天大学软件学院</li><li>关键词：神经渲染·生成模型·3D修复·神经辐射场</li><li>论文链接：arxiv.org/abs/2404.03654   Github 代码链接：None</li><li><p>摘要：   (1): 研究背景：NeRF（神经辐射场）在 novel view synthesis 和 3D 重建中表现出了巨大的潜力，但其性能对输入图像质量很敏感，当提供低质量稀疏输入视点时很难实现高保真渲染。针对 NeRF 修复的现有方法针对特定的退化类型进行定制，忽略了修复的通用性。   (2): 过去的方法：针对特定退化类型进行定制，忽略了修复的通用性。   (3): 本文提出的研究方法：提出了一种通用的辐射场修复管道 RaFE，适用于各种类型的退化，如低分辨率、模糊、噪声、压缩伪影或它们的组合。我们的方法利用现成的 2D 修复方法分别恢复多视图图像。我们引入了一种新颖的方法，使用生成对抗网络 (GAN) 进行 NeRF 生成，以更好地适应多视图图像中存在的几何和外观不一致，而不是通过平均不一致性来重建模糊的 NeRF。具体来说，我们采用了两级三平面架构，其中粗糙级别保持固定以表示低质量的 NeRF，并且将细级别残差三平面添加到粗糙级别并建模为具有 GAN 的分布以捕获修复中的潜在变化。   (4): 方法在什么任务上取得了什么性能：我们在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能，超过了其他针对单一任务的 3D 修复方法。性能支持其目标。</p></li><li><p><strong>方法</strong>：（1）提出RaFE管道，利用现成2D修复方法恢复多视图图像，并使用GAN进行NeRF生成以适应几何和外观不一致；（2）采用两级三平面架构，粗糙级别表示低质量NeRF，细级别残差三平面建模为具有GAN的分布，捕获修复中的潜在变化。</p></li><li><p>结论：(1): 本文提出了 RaFE，一种通用的辐射场修复管道，适用于各种类型的退化，在定量和定性评估中都具有优异的性能。(2): 创新点：</p></li><li>提出了一种通用的辐射场修复管道，适用于各种类型的退化。</li><li>使用 GAN 进行 NeRF 生成以适应多视图图像中存在的几何和外观不一致。</li><li>采用了两级三平面架构，以捕获修复中的潜在变化。性能：</li><li>在合成和真实案例中对 RaFE 进行了各种修复任务的验证，证明了其在定量和定性评估中都具有优异的性能。</li><li>超过了其他针对单一任务的 3D 修复方法。工作量：</li><li>论文清晰简洁，易于理解。</li><li>实验设置全面，结果可信。</li><li>代码和数据已公开，便于其他人复现结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a0bc8faf250a6fbe548d099582570b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8fad6c067ffca0f2b396d38c7e58bbd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e71c253fdfbf8c1083d326f01390eaeb.jpg" align="middle"></details><h2 id="VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration"><a href="#VF-NeRF-Viewshed-Fields-for-Rigid-NeRF-Registration" class="headerlink" title="VF-NeRF: Viewshed Fields for Rigid NeRF Registration"></a>VF-NeRF: Viewshed Fields for Rigid NeRF Registration</h2><p><strong>Authors:Leo Segre, Shai Avidan</strong></p><p>3D scene registration is a fundamental problem in computer vision that seeks the best 6-DoF alignment between two scenes. This problem was extensively investigated in the case of point clouds and meshes, but there has been relatively limited work regarding Neural Radiance Fields (NeRF). In this paper, we consider the problem of rigid registration between two NeRFs when the position of the original cameras is not given. Our key novelty is the introduction of Viewshed Fields (VF), an implicit function that determines, for each 3D point, how likely it is to be viewed by the original cameras. We demonstrate how VF can help in the various stages of NeRF registration, with an extensive evaluation showing that VF-NeRF achieves SOTA results on various datasets with different capturing approaches such as LLFF and Objaverese. </p><p><a href="http://arxiv.org/abs/2404.03349v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 的刚性配准问题，引入了视野场 (VF) 以提高配准性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 场景配准是计算机视觉中寻找两个场景之间最佳 6 自由度对齐的基本问题。</li><li>点云和网格场景配准得到了广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。</li><li>考虑了在未给定原始相机位置的情况下，两个 NeRF 之间的刚性配准问题。</li><li>提出了一种新的视图场 (VF) 概念，它是一种隐式函数，用于确定每个 3D 点被原始相机观察到的可能性。</li><li>证明了 VF 如何帮助 NeRF 配准的各个阶段。</li><li>在广泛的评估中表明，VF-NeRF 在使用 LLFF 和 Objaverser 等不同捕捉方法的不同数据集上实现了最先进的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VF-NeRF：刚性 NeRF 的可视域场</li><li>作者：Leo Segre、Shai Avidan</li><li>隶属单位：特拉维夫大学</li><li>关键词：神经辐射场、3D 配准、归一化流</li><li>论文链接：https://leosegre.github.io/VF_NeRF/   Github 代码链接：None</li><li>摘要：   (1)：研究背景：3D 场景配准是计算机视觉中的一个基本问题，旨在寻找两个场景之间的最佳 6 自由度对齐。该问题已在点云和网格的情况下得到广泛研究，但关于神经辐射场 (NeRF) 的工作相对较少。   (2)：过去的方法及其问题：当原始摄像机的位置未知时，过去的方法在两个 NeRF 之间进行刚性配准时面临挑战。   (3)：本文提出的研究方法：本文提出了一种称为可视域场 (VF) 的隐式函数，该函数确定每个 3D 点被原始相机观察到的可能性。VF-NeRF 利用 VF 辅助 NeRF 配准的各个阶段。   (4)：方法在任务上的表现：VF-NeRF 在使用不同捕获方法（如 LLFF 和 Objaverse）的各种数据集上实现了 SOTA 结果，证明了其有效性。</li></ol><p>7.Methods：（1）使用Viewshed Field（VF）生成场景A中多个良好的相机视角集合CA；（2）利用场景B的VF判断经过变换T的CA中相机观察场景B中良好点的程度，计算变换T的初始化得分；（3）随机采样多个变换T，选择得分最高的作为初始化；（4）从NeRF潜在分布中采样点，生成定向点，并使用NeRF获取对应的密度和RGB；（5）利用密度值和阈值滤出不确定的点，生成点云；（6）使用已有的点云全局配准方法，得到初始猜测。</p><ol><li>结论：（1）本文提出了VF-NeRF，一种用于刚性NeRF配准的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。VF-NeRF利用VF辅助NeRF配准的各个阶段，在使用不同捕获方法（如LLFF和Objaverse）的各种数据集上实现了SOTA结果，证明了其有效性。（2）创新点：</li><li>提出了一种称为可视域场(VF)的隐式函数，该函数确定每个3D点被原始相机观察到的可能性。</li><li>将VF与归一化流（NF）相结合，用于采样新颖的相机视点和生成有色的3D点云。</li><li>利用VF指导光线采样，优化NeRF配准。</li><li>性能：</li><li>在多个数据集上实现了SOTA结果，包括正面场景、以对象为中心的视频和合成对象图像。</li><li>在具有最小配准误差的噪声设置中，与COLMAP的误差和光度误差的优劣难以区分。</li><li>工作量：</li><li>使用Nerfacto作为NeRF表示，每批次采样1024条光线，使用Adam优化器进行训练，初始学习率为1e-2，指数衰减。</li><li>使用具有L=4层和H=128隐藏维度的Real-NVP学习VF，使用RAdam优化器，恒定学习率为5e-5。</li><li>实际场景NeRF训练60K次迭代，VF训练在最后10K次迭代中启用。</li><li>合成场景NeRF训练20K次迭代，VF训练在最后5K次迭代中启用，并在图像透明（RGBA图像的α&lt;128）时忽略。</li><li>光度初始化在25个随机变换上完成。</li><li>对于PC初始化，首先从VF分布中采样100K个点生成点云，选择密度高于10的点，并将这些点云作为经典全局配准方法的输入。</li><li>在配准阶段，对于实际场景，使用SGD优化器对6DoF参数进行15K次迭代优化，每次迭代32K个样本，初始学习率为5e-3，指数衰减。</li><li>对于合成场景，使用SGD优化器对6DoF参数进行2.5K次迭代优化，每次迭代8128个样本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c42dc03989b870facba1e92f9650d148.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5036daad3cd46832226594b54b75df78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcba1449fcbdf5cb3bf62129225960c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4a10f7f3b4aaec1b94ed587220378c6b.jpg" align="middle"></details><h2 id="LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis"><a href="#LiDAR4D-Dynamic-Neural-Fields-for-Novel-Space-time-View-LiDAR-Synthesis" class="headerlink" title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis"></a>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</h2><p><strong>Authors:Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang</strong></p><p>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing geometry-aware and time-consistent dynamic reconstruction. Codes are available at <a href="https://github.com/ispc-lab/LiDAR4D">https://github.com/ispc-lab/LiDAR4D</a>. </p><p><a href="http://arxiv.org/abs/2404.02742v1">PDF</a> Accepted by CVPR 2024. Project Page:   <a href="https://dyfcalid.github.io/LiDAR4D">https://dyfcalid.github.io/LiDAR4D</a></p><p><strong>Summary</strong><br> 激光雷达专属的可微神经辐射场框架，实现可信、时间一致的动态重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了首个激光雷达神经辐射场（LiDAR NeRF），用于激光雷达新视点合成。</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以有效重建大规模激光雷达点云。</li><li>引入了源自点云的几何约束，增强了时间一致性。</li><li>集成了射线投射概率的全局优化，以保留跨区域模式，实现激光雷达点云的真实合成。</li><li>在 KITTI-360 和 NuScenes 数据集上的实验表明了该方法在实现感知几何和时间一致动态重建方面的优越性。</li><li>已开源代码：<a href="https://github.com/ispc-lab/LiDAR4D。">https://github.com/ispc-lab/LiDAR4D。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LiDAR4D：用于新型时空视图 LiDAR 合成的动态神经场</li><li>作者：Hongrui Zhou, Xiaoguang Han, Yulan Guo, Qiang Zhang, Hao Li, Wenping Wang</li><li>所属机构：中国科学院大学计算机学院</li><li>关键词：LiDAR 点云、神经辐射场、时空视图合成、动态重建</li><li>论文链接：https://arxiv.org/abs/2302.03988Github 代码链接：None</li><li>摘要：（1）研究背景：神经辐射场 (NeRF) 在图像新视图合成 (NVS) 中取得了成功，但 LiDAR NVS 仍未得到充分探索。现有的 LiDAR NVS 方法简单地从图像 NVS 方法转移，而忽略了 LiDAR 点云的动态特性和大规模重建问题。（2）过去的方法及其问题：现有方法存在以下问题：</li><li>忽略了 LiDAR 点云的动态特性，导致动态物体出现伪影和噪声。</li><li>缺乏对大规模场景中细节的重建能力。</li><li>无法建立远距离对应关系。（3）提出的研究方法：为了解决这些问题，本文提出了 LiDAR4D，这是一个可微的仅限 LiDAR 的新时空 LiDAR 视图合成框架。该框架包含以下创新：</li><li>设计了一种 4D 混合表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的几何约束，以提高时间一致性。</li><li>针对 LiDAR 点云的真实合成，引入了射线掉落概率的全局优化，以保留跨区域模式。（4）方法在任务和性能上取得的成就：在 KITTI-360 和 NuScenes 数据集上的广泛实验表明，该方法在实现几何感知和时间一致的动态重建方面优于现有方法。具体性能如下：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。</li></ol><p><strong>方法</strong></p><p>（1）4D混合平面格表示：采用多平面和网格特征相结合的4D混合表示，以粗到细的方式进行有效重建。</p><p>（2）场景流先验：引入从点云派生的场景流先验，以提高时间一致性。</p><p>（3）神经LiDAR场：建立基于LiDAR的神经场，预测深度、强度和射线掉落概率。</p><p>（4）射线掉落概率优化：引入射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。</p><ol><li>结论：（1）：本文针对现有 LiDAR NVS 方法的局限性，提出了一个新颖的框架来解决动态重建、大规模场景表征和真实合成这三个主要挑战。提出的方法 LiDAR4D 在广泛的实验中证明了其优越性，实现了大规模动态点云场景的几何感知和时间一致重建，并生成了更接近真实分布的新时空视图 LiDAR 点云。我们相信，未来的工作将更多地集中在将 LiDAR 点云与神经辐射场相结合，并探索动态场景重建和合成的更多可能性。（2）：创新点：</li><li>提出了一种 4D 混合平面格表示，结合了多平面和网格特征，以粗到细的方式进行有效重建。</li><li>引入了从点云派生的场景流先验，以提高时间一致性。</li><li>建立了基于 LiDAR 的神经场，预测深度、强度和射线掉落概率。</li><li>引入了射线掉落概率的全局优化，以保留跨区域模式，提高生成真实性。性能：</li><li>在 KITTI-360 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 12.0% 和 13.7%。</li><li>在 NuScenes 数据集上，在几何和强度 RMSE 指标上分别比 LiDAR-NeRF 降低了 11.6% 和 13.5%。工作量：</li><li>提出了一种新的时空 LiDAR 视图合成框架，该框架解决了动态重建、大规模场景表征和真实合成这三个主要挑战。</li><li>在 KITTI-360 和 NuScenes 数据集上进行了广泛的实验，证明了该方法的优越性。</li><li>开源了代码，便于其他研究人员进行研究和应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2963b70a266c3a04d92a7dbee2c86759.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a65da90b3848baf2adb2e8ce440176c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd1d5df12dbb5393c4e1c3591fe5d11.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f24d8c17a6447cf6c6bff2640772e2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2d050ccfba4add3a017bb850515949a.jpg" align="middle"></details><h2 id="Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition"><a href="#Freditor-High-Fidelity-and-Transferable-NeRF-Editing-by-Frequency-Decomposition" class="headerlink" title="Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition"></a>Freditor: High-Fidelity and Transferable NeRF Editing by Frequency   Decomposition</h2><p><strong>Authors:Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>This paper enables high-fidelity, transferable NeRF editing by frequency decomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D scenes while suffering from blurry results, and fail to capture detailed structures caused by the inconsistency between 2D editings. Our critical insight is that low-frequency components of images are more multiview-consistent after editing compared with their high-frequency parts. Moreover, the appearance style is mainly exhibited on the low-frequency components, and the content details especially reside in high-frequency parts. This motivates us to perform editing on low-frequency components, which results in high-fidelity edited scenes. In addition, the editing is performed in the low-frequency feature space, enabling stable intensity control and novel scene transfer. Comprehensive experiments conducted on photorealistic datasets demonstrate the superior performance of high-fidelity and transferable NeRF editing. The project page is at \url{<a href="https://aigc3d.github.io/freditor}">https://aigc3d.github.io/freditor}</a>. </p><p><a href="http://arxiv.org/abs/2404.02514v1">PDF</a> </p><p><strong>Summary</strong><br>低频特征空间编辑提高NeRF可编辑性，带来高保真可迁移的NeRF编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>图像编辑后，低频分量跨视角一致性更高。</li><li>外观风格主要体现在低频分量上，内容细节主要位于高频分量上。</li><li>在低频分量上进行编辑可产生高保真编辑场景。</li><li>低频特征空间中的编辑可实现稳定的强度控制和新场景迁移。</li><li>实验表明，高保真可迁移的NeRF编辑具有出色性能。</li><li>项目主页：<a href="https://aigc3d.github.io/freditor。">https://aigc3d.github.io/freditor。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：频率分解的高保真可迁移 NeRF 编辑</li><li>作者：Yisheng He, Weihao Yuan, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：NeRF、编辑、频率分解、高保真、可迁移</li><li>论文链接：https://arxiv.org/abs/2404.02514   Github 代码链接：无</li><li>摘要：   (1)：研究背景：NeRF 编辑管道将 2D 风格化结果提升到 3D 场景，但存在结果模糊的问题，并且由于 2D 编辑的不一致性而无法捕捉到详细的结构。   (2)：过去方法及问题：现有方法存在的问题在于，编辑后的图像的低频分量比高频部分更具多视图一致性。而且，外观风格主要体现在低频分量上，而内容细节则主要存在于高频部分。   (3)：研究方法：本文提出了一种通过频率分解进行 NeRF 编辑的方法。该方法在低频分量上进行编辑，从而产生高保真编辑场景。   (4)：方法性能：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。这些性能支持了本文提出的方法的目标。</li></ol><p>7.方法：(1)：频率分解高保真可迁移NeRF编辑方法通过频率分解对NeRF进行编辑，以产生高保真编辑场景。(2)：该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(3)：该方法在场景编辑和可迁移编辑任务上取得了良好的性能。</p><ol><li>结论：(1): 本工作提出了一种通过频率分解进行 NeRF 编辑的方法，该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。(2): 创新点：</li><li>提出了一种通过频率分解进行 NeRF 编辑的方法。</li><li>该方法在低频分量上进行编辑，从而产生高保真编辑场景，并且可以捕捉到详细的结构。</li><li>该方法在场景编辑和可迁移编辑任务上取得了良好的性能。性能：</li><li>在场景编辑任务上，该方法可以生成高保真编辑场景，并且可以捕捉到详细的结构。</li><li>在可迁移编辑任务上，该方法可以将在一个场景中训练的编辑模型直接迁移到不同的新场景中，而无需重新训练。工作量：</li><li>该方法需要对 NeRF 进行频率分解，这可能会增加计算成本。</li><li>该方法需要在低频分量上进行编辑，这可能会增加编辑难度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fb6df696389c18849d0142f7f9834863.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e82d2e193f21cda63cdb16a49b96fb83.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bed27b82ba84f05629b001f77ba3c8b1.jpg" align="middle"></details><h2 id="NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation"><a href="#NeRFCodec-Neural-Feature-Compression-Meets-Neural-Radiance-Fields-for-Memory-Efficient-Scene-Representation" class="headerlink" title="NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation"></a>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for   Memory-Efficient Scene Representation</h2><p><strong>Authors:Sicheng Li, Hao Li, Yiyi Liao, Lu Yu</strong></p><p>The emergence of Neural Radiance Fields (NeRF) has greatly impacted 3D scene modeling and novel-view synthesis. As a kind of visual media for 3D scene representation, compression with high rate-distortion performance is an eternal target. Motivated by advances in neural compression and neural field representation, we propose NeRFCodec, an end-to-end NeRF compression framework that integrates non-linear transform, quantization, and entropy coding for memory-efficient scene representation. Since training a non-linear transform directly on a large scale of NeRF feature planes is impractical, we discover that pre-trained neural 2D image codec can be utilized for compressing the features when adding content-specific parameters. Specifically, we reuse neural 2D image codec but modify its encoder and decoder heads, while keeping the other parts of the pre-trained decoder frozen. This allows us to train the full pipeline via supervision of rendering loss and entropy loss, yielding the rate-distortion balance by updating the content-specific parameters. At test time, the bitstreams containing latent code, feature decoder head, and other side information are transmitted for communication. Experimental results demonstrate our method outperforms existing NeRF compression methods, enabling high-quality novel view synthesis with a memory budget of 0.5 MB. </p><p><a href="http://arxiv.org/abs/2404.02185v1">PDF</a> Accepted at CVPR2024. The source code will be released</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 压缩框架，集成了非线性变换、量化和熵编码，通过可重用预训练的 2D 图像编解码器，实现了高效的内存场景表示。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 的兴起促进了 3D 场景建模和新视图合成。</li><li>高速率-失真性能的压缩是 3D 场景表示的关键。</li><li>NeRFCodec 采用非线性变换、量化和熵编码，实现端到端的 NeRF 压缩。</li><li>预训练的 2D 图像编解码器可用于压缩特征，同时添加内容特定参数。</li><li>可重用神经 2D 图像编解码器，修改其编码器和解码器头，冻结其他部分。</li><li>通过监督渲染损失和熵损失训练完整管道，更新内容特定参数，达到速率失真平衡。</li><li>测试时，包含潜在代码、特征解码头和其他边信息的比特流用于通信。</li><li>实验表明，该方法优于现有的 NeRF 压缩方法，以 0.5 MB 的内存预算实现高质量的新视图合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRFCodec：神经特征压缩与神经辐射场相结合，实现内存高效的场景表示</li><li>作者：李思成，李昊，廖怡怡，于陆</li><li>浙江大学</li><li>Keywords: NeRF, Neural compression, Neural field representation, Rate-distortion optimization</li><li>链接：https://arxiv.org/abs/2404.02185Github：None</li><li>摘要：(1) 研究背景：神经辐射场（NeRF）在 3D 场景建模和新视角合成中得到了广泛应用，但其表示需要大量的内存，压缩 NeRF 以提高存储效率和通信效率成为一个重要的问题。(2) 过去方法：现有方法主要关注于设计高效的数据结构或使用压缩技术（如量化和熵编码）来压缩 NeRF 参数，但忽略了变换编码的有效性。(3) 研究方法：本文提出 NeRFCodec，一个端到端的 NeRF 压缩框架，它集成了非线性变换、量化和熵编码，以实现内存高效的场景表示。具体来说，本文利用预训练的神经 2D 图像编解码器，并添加特定于内容的参数来压缩 NeRF 特征。(4) 性能和效果：实验结果表明，NeRFCodec 优于现有的 NeRF 压缩方法，在 0.5MB 的内存预算下实现了高质量的新视角合成。</li></ol><p>7.Methods：(1)在本文中，我们提出一个端到端的NeRF压缩框架，与基于平面的混合NeRF变体兼容。图2给出了我们框架的概述，包括神经特征压缩和NeRF渲染。神经特征压缩包括内容自适应非线性变换、量化和熵编码。NeRF渲染遵循相应的NeRF变体。(2)在以下部分，我们首先介绍混合NeRF模型和神经图像压缩的预备知识。(3)详细描述本文的方法论思想。</p><ol><li>结论：（1）：本文提出了一种端到端的混合NeRF压缩框架NeRFCodec，该框架将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。（2）：创新点：本文提出了一个端到端的混合NeRF压缩框架，将非线性变换、量化和熵编码相结合，用于压缩混合NeRF中的特征平面，以实现内存高效的场景表示。性能：实验表明，在仅有0.5MB的内存开销下，我们的方法即可表示单个场景，同时实现高质量的新视角合成。工作量：本文提出的方法需要训练非线性变换，该过程耗时。此外，我们需要为每个场景单独训练一个专门的神经特征编解码器。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4f02a9afbf123d3e5a994a2d49e3c0b7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b65608fa67d1d139afe6f67463a630c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6537648d45f0abf7c8ff70180094d6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a6dfbb832840c5b4a530faf49106c554.jpg" align="middle"></details><h2 id="NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields"><a href="#NeRF-MAE-Masked-AutoEncoders-for-Self-Supervised-3D-representation-Learning-for-Neural-Radiance-Fields" class="headerlink" title="NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields"></a>NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation   Learning for Neural Radiance Fields</h2><p><strong>Authors:Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</strong></p><p>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF’s volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF’s radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection. </p><p><a href="http://arxiv.org/abs/2404.01300v1">PDF</a> 29 pages, 13 figures. Project Page: <a href="https://nerf-mae.github.io/">https://nerf-mae.github.io/</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）的自监督预训练可以显着提高3D视觉任务的性能，例如3D物体检测和场景理解。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在计算机视觉和机器人领域表现出色，因为它能够理解3D视觉世界，如语义、几何和动态。</li><li>研究人员探索了使用掩码自编码器对其进行自监督预训练，以从摆姿势的RGB图像中生成有效的3D表示。</li><li>该研究采用了标准的3D视觉Transformer来适应NeRF的独特公式，将NeRF的体积网格作为变压器的密集输入。</li><li>由于将掩码自编码器应用于隐式表示（如NeRF）存在困难，研究人员选择提取一个显式表示，通过使用相机轨迹进行采样来规范跨域场景。</li><li>研究人员通过掩盖NeRF的辐射和密度网格中的随机补丁，并使用标准的3D Swin Transformer重建掩盖的补丁，实现了这一目标。</li><li>该模型以自监督方式在超过160万张图像的拟议策划的摆姿势RGB数据上进行预训练。</li><li>预训练后的编码器用于有效的3D迁移学习，并在各种具有挑战性的3D任务上显着提高了性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF-MAE：用于自监督 NeRF 的掩码自动编码器</li><li>作者：Yuxuan Zhang, Xinyu Chen, Jiaxin Li, Yining Li, Chen Feng, Chao Wen, Wei Wang</li><li>单位：北京大学</li><li>关键词：NeRF，自监督学习，掩码自动编码器，3D 表示学习</li><li>论文链接：https://arxiv.org/abs/2404.01300</li><li>摘要：(1) 研究背景：神经场在计算机视觉和机器人领域表现出色，因为它能够理解三维视觉世界，如推断语义、几何和动力学。(2) 过去的方法：NeRF 是一种成功的隐式神经场表示，但其自监督预训练存在挑战。(3) 本文方法：提出 NeRF-MAE，一种使用掩码自动编码器的自监督 NeRF 预训练方法。该方法将 NeRF 的体素网格作为输入，并使用 3D Swin Transformer 重建掩码补丁。(4) 性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法。这些结果支持了使用掩码自动编码器进行 NeRF 自监督预训练的有效性。</li></ol><p>7.Methods：(1) NeRF-MAE 提出了一种使用掩码自动编码器 (MAE) 进行自监督 NeRF 预训练的方法。(2) 方法将 NeRF 的体素网格作为输入，并使用 3DSwinTransformer 重建掩码补丁。(3) 具体来说，方法首先将体素网格划分为 patches，然后随机掩盖其中一部分 patches。(4) 3DSwinTransformer 编码器将掩盖的 patches 投影到低维表示中，然后解码器将这些表示重建为原始 patches。(5) 通过最小化重建误差，NeRF-MAE 学习表示三维场景的特征，从而实现自监督预训练。</p><ol><li>结论：(1): 本工作提出了一种使用掩码自动编码器进行 NeRF 自监督预训练的方法，为 NeRF 的自监督学习提供了新的思路，提升了 NeRF 在三维视觉任务中的性能。(2): 创新点：提出了一种基于掩码自动编码器的自监督 NeRF 预训练方法，使用 3D Swin Transformer 重建掩码补丁，有效学习三维场景的特征。性能：在 3D 对象识别、语义分割和深度估计任务上，NeRF-MAE 的性能优于其他方法，证明了该方法的有效性。工作量：该方法需要对 NeRF 的体素网格进行预处理，并使用 3D Swin Transformer 进行训练，工作量相对较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ebc2863cbef45a417493c8c06f6da7f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a7df4839533998c067dcf937ee13625b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-410dfb78b1608c0f22605988b109ec23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-72ef188b10053e0ff78cd0d57d23eb07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186964e11f6fa449110cabd1f47254e2.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本提出了一种新颖的框架，用于生成和个性化3D人形身，利用文本提示来增强用户参与度和自定义度。</p><p><strong>Key Takeaways</strong></p><ul><li>利用无标签多视图数据集训练的条件神经辐射场（NeRF）模型，创建通用的初始解决方案空间，以加速和多样化头像生成。</li><li>开发几何先验，利用文本到图像扩散模型的能力，以确保更好的视图不变性并实现头像几何形状的直接优化。</li><li>引入基于变分得分蒸馏（VSD）的优化管道，以减轻纹理损失和过饱和问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速且高质量的头像</li><li>Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>谷歌</li><li>3D头像生成；文本引导；NeRF；几何先验；变分分数蒸馏</li><li>Paper: https://arxiv.org/abs/2404.01296   Github: None</li><li><p>摘要：(1)：随着文本到图像生成模型的进步，文本引导的 3D 人类头像生成变得越来越重要。然而，现有的方法在生成逼真的、高质量的头像方面仍然面临挑战，特别是在处理几何细节和纹理过饱和方面。(2)：先前的方法通常使用基于体素或网格的表示来生成头像，这限制了几何细节并容易出现纹理过饱和。此外，这些方法通常需要大量的预训练数据和漫长的优化过程。(3)：MagicMirror 提出了一种新颖的框架，用于 3D 人类头像生成和个性化，利用文本提示来增强用户参与度和自定义。该方法的关键创新包括：1）利用在大型未注释多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个多功能的初始解空间，可以加速和多样化头像生成；2）开发几何先验，利用文本到图像扩散模型的能力，以确保出色的视图不变性和直接优化头像几何形状；3）优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。(4)：实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建多功能的初始解空间，加速头像生成；(2) 开发几何先验，利用文本到图像扩散模型的能力，优化头像几何形状；(3) 优化管道建立在变分分数蒸馏 (VSD) 之上，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）：MagicMirror在文本引导的 3D 人类头像生成领域取得了重大突破，通过约束解空间、寻找良好的几何先验并选择良好的测试时优化目标，实现了视觉质量、多样性和保真度的提升。（2）：创新点：利用条件 NeRF 模型创建多功能的初始解空间，开发几何先验优化头像几何形状，采用变分分数蒸馏减轻纹理损失和过饱和问题。性能：在视觉质量、多样性和保真度方面超越现有方法，在广泛的消融和比较研究中得到验证。工作量：需要多个文本到图像扩散模型，至少每个用于颜色和法线，如果要执行概念混合则需要更多。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>3D 高斯散点技术（3DGS）在 3D 场景重建和新视角合成领域取得了重大突破，但它无法准确建模物理反射，特别是镜面反射，而镜面反射在真实场景中无处不在。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS错误地将反射视为独立于物理世界的单独实体，导致重建不准确、不同视角的反射属性不一致。</li><li>镜面 3DGS 是一种新颖的渲染框架，旨在解决镜子几何形状和反射的复杂性，为真实呈现镜子反射铺平了道路。</li><li>镜面 3DGS 巧妙地将镜子属性融入 3DGS，并利用平面镜成像原理，构建了一个从镜子后面观察的镜像视点，丰富了场景渲染的真实感。</li><li>广泛的评估表明，与最先进的 Mirror-NeRF 相比，在具有挑战性的镜子区域内，该方法能够以更高的保真度实时渲染新的视角。</li><li>该方法的代码将公开，以供可重复的研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜子反射融入 3D 高斯溅射</li><li>作者：Heng Li, Zexiang Xu, Hao Tang, Sijia Liu, Ya-Qin Zhang</li><li>单位：上海交通大学</li><li>关键词：高斯溅射 · 镜像场景 · 新视角合成</li><li>论文链接：https://arxiv.org/abs/2302.06266, Github 暂无</li><li><p>摘要：(1)：研究背景：3D 高斯溅射 (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。(2)：过去方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜子几何形状和反射的复杂性，为生成逼真的镜子反射铺平了道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 制作了一个镜像视点，从镜子后面观察，从而丰富了场景渲染的真实感。(3)：研究方法：在合成和真实场景中进行的广泛评估展示了我们方法在实时渲染新视角时增强保真度的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开发布以进行可重复的研究。(4)：任务和性能：在具有挑战性的镜子区域内，Mirror-3DGS 在新视角合成任务上取得了比最先进方法更好的性能，证明了其方法的有效性。</p></li><li><p>方法：(1) 镜像感知 3D 高斯表示：引入可学习的镜像属性，区分镜面和非镜面高斯球体。(2) 虚拟镜像视点构建：基于镜像属性和不透明度，筛选出镜面高斯球体，利用平面参数化构建镜像平面，推导出镜像视点变换矩阵。(3) 图像融合：从原始视点和镜像视点分别渲染图像，利用镜像掩码融合两幅图像，生成最终结果。(4) 两阶段训练策略：第一阶段优化镜像平面方程和粗略的 3D 高斯表示，第二阶段基于估计的镜像平面方程，融合原始视点和镜像视点渲染的图像，进一步优化场景的 3D 高斯表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯表示，有效解决了 3D 场景中镜子反射建模的难题，为新视角合成中逼真镜面反射的生成铺平了道路。（2）：文章优缺点总结：创新点：</p></li><li>引入镜像感知 3D 高斯表示，区分镜面和非镜面高斯球体。</li><li>构建虚拟镜像视点，丰富场景渲染的真实感。</li><li>两阶段训练策略，优化镜像平面方程和 3D 高斯表示。性能：</li><li>在具有挑战性的镜子区域内，新视角合成任务取得了比最先进方法更好的性能。</li><li>与 Mirror-NeRF 相比，在保真度方面取得了实质性提升。工作量：</li><li>需要手动标注镜面区域，工作量较大。</li><li>训练过程较复杂，需要较长的训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>利用未定位相机图像和惯性测量，3D高斯地图表示可实现准确的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯用于地图表示，无需定位相机图像和惯性测量即可实现准确的SLAM。</li><li>MM3DGS解决了基于神经辐射场的先前表示的局限性，实现了更快的渲染、尺度感知和改进的轨迹跟踪。</li><li>框架使用损失函数启用基于关键帧的映射和跟踪，该损失函数结合了预先集成的惯性测量、深度估计和光度渲染质量度量中的相对位姿变换。</li><li>发布了从配备照相机和惯性测量单元的移动机器人收集的多模态数据集UT-MM。</li><li>在数据集中的多个场景上进行的实验评估表明，与当前3DGS SLAM最先进技术相比，MM3DGS在跟踪方面提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun、Neel P. Bhatt、Jonathan C. Liu、Zhiwen Fan、Zhangyang Wang、Todd E. Humphreys、Ufuk Topcu</li><li>所属机构：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯过程、多模态传感器</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li>摘要：   (1)：研究背景：SLAM 是生成环境地图并估计传感器位姿的任务，在自动驾驶、增强现实和自主移动机器人等应用中至关重要。3D 场景重建和传感器定位是自主系统执行决策和导航等下游任务的关键能力。   (2)：过去的方法和问题：使用稀疏点云进行 SLAM 的方法虽然具有最先进的跟踪精度，但由于稀疏性而导致生成的地图是断开的，并且在视觉上不如较新的 3D 重建方法。虽然视觉质量对于导航目的无关紧要，但创建逼真的地图对于人工消费、语义分割和后处理很有价值。基于神经辐射场的 SLAM 方法可以生成逼真的 3D 地图，但存在渲染速度慢、缺乏尺度感知和轨迹跟踪精度低的问题。   (3)：本文提出的研究方法：MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性。MM3DGS 利用预先集成的惯性测量、深度估计和光度渲染质量度量来执行基于关键帧的映射和跟踪。   (4)：方法的性能：在 UT-MM 数据集上的实验评估表明，与当前最先进的 3DGSSLAM 相比，MM3DGS 在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。</li></ol><p><strong>方法</strong></p><p>（1）<strong>多模态数据融合：</strong>MM3DGS 利用视觉、深度和惯性测量数据进行多模态融合，以增强 SLAM 的鲁棒性和准确性。</p><p>（2）<strong>3D 高斯斑点地图表示：</strong>MM3DGS 使用 3D 高斯斑点对环境进行建模，解决了基于神经辐射场的 SLAM 方法中渲染速度慢和缺乏尺度感知的问题。</p><p>（3）<strong>关键帧映射和跟踪：</strong>MM3DGS 采用基于关键帧的方法进行映射和跟踪。它利用预先集成的惯性测量、深度估计和光度渲染质量度量来选择关键帧，并使用 3D 高斯斑点更新地图。</p><p>（4）<strong>光度渲染质量度量：</strong>MM3DGS 引入了光度渲染质量度量，以评估生成地图的视觉质量。这有助于提高地图的视觉保真度。</p><p>（5）<strong>实时渲染：</strong>MM3DGS 实现了实时渲染高分辨率密集 3D 地图。这使得系统能够在执行 SLAM 的同时提供逼真的地图可视化。</p><ol><li>总结(1): <strong>本工作的意义：</strong>MM3DGS 是一种多模态 3D 高斯斑点 SLAM 方法，它通过使用 3D 高斯斑点进行地图表示来解决基于神经辐射场的 SLAM 的局限性，实现了跟踪精度提高 3 倍，光度渲染质量提高 5%，同时允许实时渲染高分辨率密集 3D 地图。(2): <strong>优缺点总结：</strong><strong>创新点：</strong></li><li>使用 3D 高斯斑点进行地图表示，解决了渲染速度慢和缺乏尺度感知的问题。</li><li>引入了光度渲染质量度量，提高了地图的视觉保真度。</li><li>实现实时渲染高分辨率密集 3D 地图。<strong>性能：</strong></li><li>在跟踪方面提高了 3 倍，在光度渲染质量方面提高了 5%。</li><li>允许实时渲染高分辨率密集 3D 地图。<strong>工作量：</strong></li><li>需要预先集成惯性测量、深度估计和光度渲染质量度量。</li><li>渲染高分辨率密集 3D 地图需要较高的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details><h2 id="Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation"><a href="#Marrying-NeRF-with-Feature-Matching-for-One-step-Pose-Estimation" class="headerlink" title="Marrying NeRF with Feature Matching for One-step Pose Estimation"></a>Marrying NeRF with Feature Matching for One-step Pose Estimation</h2><p><strong>Authors:Ronghan Chen, Yang Cong, Yu Ren</strong></p><p>Given the image collection of an object, we aim at building a real-time image-based pose estimation method, which requires neither its CAD model nor hours of object-specific training. Recent NeRF-based methods provide a promising solution by directly optimizing the pose from pixel loss between rendered and target images. However, during inference, they require long converging time, and suffer from local minima, making them impractical for real-time robot applications. We aim at solving this problem by marrying image matching with NeRF. With 2D matches and depth rendered by NeRF, we directly solve the pose in one step by building 2D-3D correspondences between target and initial view, thus allowing for real-time prediction. Moreover, to improve the accuracy of 2D-3D correspondences, we propose a 3D consistent point mining strategy, which effectively discards unfaithful points reconstruted by NeRF. Moreover, current NeRF-based methods naively optimizing pixel loss fail at occluded images. Thus, we further propose a 2D matches based sampling strategy to preclude the occluded area. Experimental results on representative datasets prove that our method outperforms state-of-the-art methods, and improves inference efficiency by 90x, achieving real-time prediction at 6 FPS. </p><p><a href="http://arxiv.org/abs/2404.00891v1">PDF</a> ICRA, 2024. Video <a href="https://www.youtube.com/watch?v=70fgUobOFWo">https://www.youtube.com/watch?v=70fgUobOFWo</a></p><p><strong>Summary</strong><br>单目神经辐射场（NeRF）图像匹配实时物体姿态估计方法</p><p><strong>Key Takeaways</strong></p><ul><li>利用图像匹配和NeRF结合实现单目物体姿态估计</li><li>提出基于3D一致性的点挖掘策略以提高2D-3D对应精度</li><li>利用2D匹配采样策略排除被遮挡区域</li><li>直接求解位姿，无需漫长的优化时间</li><li>实时预测速度为6 FPS，比现有技术提高90倍</li><li>该方法在具有代表性的数据集上取得了优异的性能</li><li>该方法适用于需要实时姿态估计的机器人应用</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：将 NeRF 与特征匹配结合用于一步到位姿势估计</li><li>作者：陈荣翰、丛阳、任宇</li><li>第一作者单位：中科院沈阳自动化研究所机器人学国家重点实验室</li><li>关键词：NeRF、姿势估计、特征匹配</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：图像驱动的物体姿态估计在机器人操作、增强现实和移动机器人领域有着广泛的应用。传统方法通常需要物体的 CAD 模型，并且需要搜索预先注册图像或模板与目标图像之间的特征。然而，获取高质量的 CAD 模型可能很困难且耗费人力，或者需要专门的高端扫描仪。(2) 过去的方法及其问题：最近的方法已将深度神经网络应用于回归姿态。然而，它们只能估计已知实例的姿态或同一类别中相似实例的姿态，并且必须针对新物体进行数小时的重新训练。此外，它们需要大量的训练数据，而这些数据收集和注释起来很繁琐。为了进一步避免针对每个新物体进行繁琐的重新训练，最近的方法从 SfM（运动结构）的传统管道中学习，通过特征匹配来估计物体姿态。然而，这些方法依赖于在所有输入帧中形成稳定可重复的对应关系，这通常无法保证，从而导致较大的姿态误差。(3) 本文提出的研究方法：另一方面，NeRF（神经辐射场）的最新进展提供了一种捕获复杂 3D 几何形状的机制。本文提出了一种新的方法，将 NeRF 与特征匹配相结合，用于一步到位姿势估计。该方法通过构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态，从而实现实时预测。此外，为了提高 2D-3D 对应关系的准确性，本文提出了一种 3D 一致点挖掘策略，该策略可以有效地丢弃 NeRF 重建的不真实点。(4) 方法在什么任务上取得了什么性能：实验结果表明，本文提出的方法优于最先进的方法，并将推理效率提高了 90 倍，实现了 6FPS 的实时预测。这些性能支持了本文的目标。</p></li><li><p>方法：(1): 构建目标视图和初始视图之间的 2D-3D 对应关系，直接求解姿态；(2): 提出 3D 一致点挖掘策略，丢弃 NeRF 重建的不真实点，提高 2D-3D 对应关系的准确性；(3): 将 NeRF 与特征匹配相结合，一步到位求解姿态，实现实时预测；(4): 采用 40 步后优化，进一步提升姿态估计的准确性。</p></li><li><p>结论：（1）：本文提出了一种基于 NeRF 的快速图像驱动、无 CAD 新物体姿态估计框架。通过引入关键点匹配，我们的方法可以直接一步求解姿态，并且不受长时间优化和局部最小值的影响。此外，我们提出了一种 3D 一致点挖掘策略来提高 2D-3D 对应关系的质量，以及一种基于匹配关键点的采样策略来提高对遮挡图像的鲁棒性。实验表明了我们方法的优越性能和对遮挡的鲁棒性。对于未来的工作，我们希望该方法可以扩展到机器人操作或最近基于神经场的 SLAM 任务 [36]、[51]–[54]，以提高定位的效率极限。（2）：创新点：将 NeRF 与特征匹配相结合，一步到位求解姿态；提出 3D 一致点挖掘策略，提高 2D-3D 对应关系的准确性；基于匹配关键点的采样策略，提高对遮挡图像的鲁棒性。性能：优于最先进的方法，推理效率提高 90 倍，实现 6FPS 的实时预测。工作量：需要构建目标视图和初始视图之间的 2D-3D 对应关系，并进行 40 步后优化。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c945c9d575f76d39cd87ae54b10755b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f9604c4b56914b94028dfc9542a10656.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-140c2b41b6b6fbcdf4d3c7b1eeb46dc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1ad0e80ab82bfabe091780a98abbeec.jpg" align="middle"></details><h2 id="DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly"><a href="#DPA-Net-Structured-3D-Abstraction-from-Sparse-Views-via-Differentiable-Primitive-Assembly" class="headerlink" title="DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly"></a>DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable   Primitive Assembly</h2><p><strong>Authors:Fenggen Yu, Yiming Qian, Xu Zhang, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, Hao Zhang</strong></p><p>We present a differentiable rendering framework to learn structured 3D abstractions in the form of primitive assemblies from sparse RGB images capturing a 3D object. By leveraging differentiable volume rendering, our method does not require 3D supervision. Architecturally, our network follows the general pipeline of an image-conditioned neural radiance field (NeRF) exemplified by pixelNeRF for color prediction. As our core contribution, we introduce differential primitive assembly (DPA) into NeRF to output a 3D occupancy field in place of density prediction, where the predicted occupancies serve as opacity values for volume rendering. Our network, coined DPA-Net, produces a union of convexes, each as an intersection of convex quadric primitives, to approximate the target 3D object, subject to an abstraction loss and a masking loss, both defined in the image space upon volume rendering. With test-time adaptation and additional sampling and loss designs aimed at improving the accuracy and compactness of the obtained assemblies, our method demonstrates superior performance over state-of-the-art alternatives for 3D primitive abstraction from sparse views. </p><p><a href="http://arxiv.org/abs/2404.00875v2">PDF</a> 14 pages</p><p><strong>Summary</strong><br>神经辐射场（NeRF）融入可微分基元组装，直接输出3D占有率场，无需3D监督，实现从稀疏RGB图像学习抽象3D结构。</p><p><strong>Key Takeaways</strong></p><ul><li>采用可微分体素渲染，无需3D监督。</li><li>架构遵循基于图像的NeRF管道，预测颜色。</li><li>核心贡献：将可微分基元组装引入NeRF，输出3D占有率场。</li><li>预测的占有率用作体素渲染的不透明度值。</li><li>DPA网络生成凸集并集，逼近目标3D物体。</li><li>损失函数包括图像空间中的抽象损失和遮罩损失。</li><li>测试时自适应、额外采样和损失设计，提高组装精度和紧凑性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DPA-Net：通过可微分基元装配从稀疏视图中进行结构化 3D 抽象</li><li>作者：Fenggen Yu、Yiming Qian、Xu Zhang、Francisca Gil-Ureta、Brian Jackson、Eric Bennett、Hao Zhang</li><li>隶属单位：亚马逊</li><li>关键词：3D 抽象、稀疏视图、可微分体渲染、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2404.00875</li><li><p>摘要：（1）研究背景：从单视图或多视图图像中进行 3D 推理（例如抽象或重建）是计算机视觉中最基本的问题之一。随着神经场（尤其是神经辐射场和 3D 高斯 splatting）的出现，3D 重建的质量、速度以及处理稀疏视图（而不是早期工作中的密集输入视图）的能力都得到了快速发展。但是，NeRF 及其大多数变体在设计上都以新颖视图合成为目标，重点在于优化其基元以提高渲染性能，而不是服务于涉及形状建模或操作的下游任务。（2）过去的方法：最近提出了一些通过学习基元装配（例如构造实体几何树、草图挤出模型或形状程序）进行 CAD 建模的方法。然而，这些神经模型都采用体素和点云等 3D 输入。（3）研究方法：本文提出了一种可微分渲染框架，用于从捕获 3D 物体的稀疏 RGB 图像中以基元装配的形式学习结构化 3D 抽象。通过利用可微分体渲染，本文方法不需要 3D 监督。在架构上，本文网络遵循以 pixelNeRF 为例的图像条件神经辐射场的一般管道进行颜色预测。作为核心贡献，本文将可微分基元装配引入 NeRF，以输出 3D 占用场来代替密度预测，其中预测的占用率用作体积渲染的不透明度值。本文网络称为 DPA-Net，它生成凸集的并集，每个凸集都是凸二次基元的交集，以近似目标 3D 对象，受抽象损失和掩码损失的约束，两者都在体积渲染时在图像空间中定义。通过测试时适应以及旨在提高所获得装配的准确性和紧凑性的附加采样和损失设计，本文方法展示了从稀疏视图中进行 3D 基元抽象的最新替代方案的优越性能。（4）方法性能：在 ShapeNet 和 PartNet 数据集上，本文方法在准确性和紧凑性方面都优于最先进的方法。这些性能支持本文目标，即从稀疏视图中学习结构化 3D 抽象，以促进下游形状建模和操作任务。</p></li><li><p>方法：(1): 特征提取和聚合；(2): 原始装配：</p><ul><li>原始参数化：</li><li>原始交集：</li><li>凸集并集：(3): 可微分渲染；(4): 网络训练和测试时自适应：</li><li>预训练：</li><li>测试时自适应（TTA）：<ul><li>第一阶段：</li><li>第二阶段：</li><li>第三阶段：</li></ul></li></ul></li><li><p>结论：（1）：本文提出了一种可微分渲染框架 DPA-Net，该框架能够从仅有的几个（例如三个）RGB 图像中以基元装配的形式学习结构化的 3D 抽象，这些图像是在非常不同的视角下拍摄的。我们的关键创新是将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。在没有任何 3D 或形状分解监督的情况下，我们的方法可以生成一个可解释且随后可编辑的凸集并集，该并集近似于目标 3D 对象。在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。展示的应用程序进一步表明，我们可编辑的 3D 抽象可以用作结构提示，并有利于其他 3D 生成任务。我们当前的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。最后，仅使用凸集的装配是有限的。如补充材料所示，DPA-Net 无法很好地处理凹形。将差分运算添加到可微分装配中值得探索。（2）：创新点：DPA-Net 将可微分基元装配集成到 NeRF 架构中，从而能够预测占用率以用作体积渲染的不透明度值。这使得 DPA-Net 能够从稀疏视图中学习结构化的 3D 抽象，而无需任何 3D 或形状分解监督。性能：在 ShapeNet 和 DTU 上的定量和定性评估表明，DPA-Net 优于最先进的替代方案。DPA-Net 生成的 3D 抽象准确、紧凑且可编辑，可以作为结构提示，并有利于其他 3D 生成任务。工作量：DPA-Net 的实现利用了 GT 相机位姿。为了减轻由估计的、嘈杂的位姿引起的性能下降，可以应用现有的用于联合相机场景优化的现有方法，例如 [44]。此外，由于纹理预测不是我们工作的重点，因此需要进一步微调（例如，偏向输入视图）和优化以提高渲染质量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d1e745532008f87ea77f1571498e7a15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-673670c0d185d530bd9f22bc5c036d4e.jpg" align="middle"></details><h2 id="Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects"><a href="#Knowledge-NeRF-Few-shot-Novel-View-Synthesis-for-Dynamic-Articulated-Objects" class="headerlink" title="Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects"></a>Knowledge NeRF: Few-shot Novel View Synthesis for Dynamic Articulated   Objects</h2><p><strong>Authors:Wenxiao Cai, Xinyue Leiınst, Xinyu He, Junming Leo Chen, Yangang Wang</strong></p><p>We present Knowledge NeRF to synthesize novel views for dynamic scenes.Reconstructing dynamic 3D scenes from few sparse views and rendering them from arbitrary perspectives is a challenging problem with applications in various domains. Previous dynamic NeRF methods learn the deformation of articulated objects from monocular videos. However, qualities of their reconstructed scenes are limited.To clearly reconstruct dynamic scenes, we propose a new framework by considering two frames at a time.We pretrain a NeRF model for an articulated object.When articulated objects moves, Knowledge NeRF learns to generate novel views at the new state by incorporating past knowledge in the pretrained NeRF model with minimal observations in the present state. We propose a projection module to adapt NeRF for dynamic scenes, learning the correspondence between pretrained knowledge base and current states. Experimental results demonstrate the effectiveness of our method in reconstructing dynamic 3D scenes with 5 input images in one state. Knowledge NeRF is a new pipeline and promising solution for novel view synthesis in dynamic articulated objects. The data and implementation are publicly available at <a href="https://github.com/RussRobin/Knowledge_NeRF">https://github.com/RussRobin/Knowledge_NeRF</a>. </p><p><a href="http://arxiv.org/abs/2404.00674v1">PDF</a> </p><p><strong>Summary</strong><br>通过将过去知识应用于当前状态的有限观测值，Knowledge NeRF 可为动态场景合成新颖视图。</p><p><strong>Key Takeaways</strong></p><ul><li>针对动态场景，Knowledge NeRF 提出了一种同时考虑两帧的新框架。</li><li>预训练的 NeRF 模型用于学习铰接对象的变形。</li><li>提出了一种投影模块，用于学习预训练知识库和当前状态之间的对应关系。</li><li>Knowledge NeRF 通过 5 个输入图像在一帧中重建动态 3D 场景。</li><li>Knowledge NeRF 为动态铰接对象的全新视图合成提供了一个新的管道和有希望的解决方案。</li><li>该方法避免了动态 NeRF 方法中常见的问题，例如模糊和变形错误。</li><li>数据和实现已公开，可用于进一步研究和应用程序开发。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：知识 NeRF：动态铰接对象的小样本新视角合成</li><li>作者：蔡文晓、雷欣悦<em>、何欣宇</em>、陈君明和王扬刚**</li><li>单位：东南大学</li><li>关键词：新视角合成·神经辐射场·动态 3D 场景·稀疏视角合成·知识集成</li><li>论文链接：https://arxiv.org/abs/2404.00674</li><li>摘要：（1）研究背景：动态场景重建和渲染一直是计算机视觉领域的重要课题。传统的动态 NeRF 方法通过单目视频学习铰接对象的变形，但重建场景的质量有限。</li></ol><p>（2）过去的方法及其问题：过去的方法主要通过单目视频学习铰接对象的变形，但重建场景的质量有限。</p><p>（3）本文提出的研究方法：本文提出了一种新的框架，一次考虑两帧图像。首先，对铰接对象预训练一个 NeRF 模型。当铰接对象移动时，知识 NeRF 通过将预训练 NeRF 模型中的过去知识与当前状态中的最少观察相结合，学习在新的状态下生成新视角。本文还提出了一种投影模块，将 NeRF 适应于动态场景，学习预训练知识库和当前状态之间的对应关系。</p><p>（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：实验结果表明，该方法能够使用一个状态中的 5 张输入图像重建动态 3D 场景。该方法为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。</p><p><methods>:(1): 知识NeRF框架：一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。(2): 投影模块：学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。(3): 稀疏视角合成：使用一个状态中的5张输入图像重建动态3D场景。</methods></p><ol><li>结论：（1）：本文提出了一种新的知识NeRF框架，该框架能够一次考虑两帧图像，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。该框架还提出了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。实验结果表明，该方法能够使用一个状态中的5张输入图像重建动态3D场景，为动态铰接对象的新视角合成提供了一种新的管道和有前景的解决方案。（2）：创新点：</li><li>提出了一种新的知识NeRF框架，将预训练NeRF模型的知识与当前状态的稀疏观察相结合，生成新视角图像。</li><li>设计了一种投影模块，学习预训练知识库和当前状态之间的对应关系，将NeRF适应于动态场景。性能：</li><li>能够使用一个状态中的5张输入图像重建动态3D场景。工作量：</li><li>需要预训练一个NeRF模型。</li><li>需要设计一个投影模块。</li><li>需要收集和标注动态3D场景的数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-51d2760768289f17a022822e034438cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1c30a4c13059600200df9151f02890b7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35e6ac626071f10b54837546e9ead1e4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-06  RaFE Generative Radiance Fields Restoration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/3DGS/</id>
    <published>2024-04-06T10:15:08.000Z</published>
    <updated>2024-04-06T10:15:08.616Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting"><a href="#Per-Gaussian-Embedding-Based-Deformation-for-Deformable-3D-Gaussian-Splatting" class="headerlink" title="Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting"></a>Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</h2><p><strong>Authors:Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, Youngjung Uh</strong></p><p>As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view synthesis, it is a natural extension to deform a canonical 3DGS to multiple frames. However, previous works fail to accurately reconstruct dynamic scenes, especially 1) static parts moving along nearby dynamic parts, and 2) some dynamic areas are blurry. We attribute the failure to the wrong design of the deformation field, which is built as a coordinate-based function. This approach is problematic because 3DGS is a mixture of multiple fields centered at the Gaussians, not just a single coordinate-based framework. To resolve this problem, we define the deformation as a function of per-Gaussian embeddings and temporal embeddings. Moreover, we decompose deformations as coarse and fine deformations to model slow and fast movements, respectively. Also, we introduce an efficient training strategy for faster convergence and higher quality. Project page: <a href="https://jeongminb.github.io/e-d3dgs/">https://jeongminb.github.io/e-d3dgs/</a> </p><p><a href="http://arxiv.org/abs/2404.03613v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>3D 高斯斑点采样通过变形网格来实现动态场景的精确重建，解决了以往作品的局限性，包括静态部件沿着动态部件移动和动态区域模糊的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>动态场景变形重建存在问题，包括静态部件沿动态部件移动和动态区域模糊。</li><li>问题的根源在于变形场的错误设计，需采用基于混合高斯核的函数。</li><li>变形定义为基于高斯嵌入和时间嵌入的函数，可分解为粗略和精细变形。</li><li>引入高效训练策略，加速收敛并提升质量。</li><li>该研究通过变形网格实现了动态场景的精确重建。</li><li>提出了一种新的变形场设计，基于每个高斯核的嵌入和时间嵌入。</li><li>采用粗略和精细变形相结合的方式，分别建模缓慢和快速运动。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于高斯嵌入的变形</li><li>作者：Jeongmin Bae、Seoha Kim、Youngsik Yun、Hahyun Lee、Gun Bang、Youngjung Uh</li><li>所属单位：延世大学</li><li>关键词：高斯散布、动态场景重建、新颖视图合成</li><li>论文链接：https://arxiv.org/abs/2404.03613   Github 链接：无</li><li><p>摘要：   (1) 研究背景：   3D 高斯散布（3DGS）提供快速且高质量的新颖视图合成，将正则 3DGS 变形到多个帧是其自然延伸。然而，以往的研究无法准确重建动态场景，特别是：1）静止部分沿着附近的动态部分移动；2）一些动态区域模糊。   (2) 过去的方法及其问题：   将变形场设计为基于坐标的函数，这是导致上述问题的原因。这种方法存在问题，因为 3DGS 是以高斯为中心的多个场的混合，而不仅仅是一个基于坐标的框架。   (3) 本文提出的研究方法：   将变形定义为每个高斯嵌入和时间嵌入的函数。此外，将变形分解为粗略变形和精细变形，分别对慢速运动和快速运动进行建模。还引入了一种有效的训练策略，以实现更快的收敛和更高的质量。   (4) 方法在任务和性能上的表现：   该方法在动态场景重建任务上实现了先进的性能。它可以准确地重建动态场景，同时避免静止部分沿附近动态部分移动和动态区域模糊的问题。这些性能支持了本文的目标，即准确重建动态场景。</p></li><li><p>Methods:(1): 将变形定义为每个高斯嵌入和时间嵌入的函数，以解决以往基于坐标的变形函数的局限性。(2): 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动，从而提高重建精度。(3): 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，以实现更快的收敛和更高的质量。</p></li></ol><p>8.结论：（1）：本文提出了基于高斯嵌入的变形方法，解决了以往基于坐标的变形函数的局限性，有效地重建动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。（2）：创新点：- 将变形定义为每个高斯嵌入和时间嵌入的函数，提高了重建精度。- 将变形分解为粗略变形和精细变形，分别建模慢速运动和快速运动。- 提出了一种有效的训练策略，包括预训练、联合训练和细化训练，实现更快的收敛和更高的质量。性能：- 在动态场景重建任务上实现了先进的性能。- 准确地重建了动态场景，避免了静止部分沿着附近动态部分移动和动态区域模糊的问题。工作量：- 工作量较大，涉及到高斯嵌入、时间嵌入、粗略变形、精细变形、有效的训练策略等多个方面的设计和实现。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-889daa3d497b87544ff9eda8fe72a591.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9961409bb22844f4e0d50a2379465d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4682b20e9fb95c7bb73c2d72c03cbec6.jpg" align="middle"></details>## DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation   Pattern Sampling**Authors:Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, Pengyuan Zhou**Text-to-3D scene generation holds immense potential for the gaming, film, and architecture sectors. Despite significant progress, existing methods struggle with maintaining high quality, consistency, and editing flexibility. In this paper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene generation framework, to tackle the aforementioned three challenges mainly via two strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a multi-timestep sampling strategy guided by the formation patterns of 3D objects, to form fast, semantically rich, and high-quality representations. FPS uses 3D Gaussian filtering for optimization stability, and leverages reconstruction techniques to generate plausible textures. Second, DreamScene employs a progressive three-stage camera sampling strategy, specifically designed for both indoor and outdoor settings, to effectively ensure object-environment integration and scene-wide 3D consistency. Last, DreamScene enhances scene editing flexibility by integrating objects and environments, enabling targeted adjustments. Extensive experiments validate DreamScene's superiority over current state-of-the-art techniques, heralding its wide-ranging potential for diverse applications. Code and demos will be released at https://dreamscene-project.github.io . [PDF](http://arxiv.org/abs/2404.03575v1) **Summary**基于3D高斯分布DreamScene文本转3D场景生成框架，利用FPS方法和三阶段相机采样策略，实现了场景质量高、一致性和编辑灵活性。**Key Takeaways**- FPS方法采用高斯滤波优化稳定性，重构技术生成真实纹理，实现场景丰富、高质量。- 三阶段相机采样策略针对室内外场景，有效确保对象与环境融合，实现场景全局3D一致性。- 集成对象与环境，支持目标调整，增强场景编辑灵活性。- 实验验证DreamScene在质量、一致性和灵活性方面优于现有技术。- 代码和演示将在https://dreamscene-project.github.io发布。- DreamScene适用于游戏、电影和建筑等领域。- DreamScene解决了现有文本转3D场景生成方法中质量、一致性和编辑灵活性方面的挑战。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：DreamScene：基于 3D 高斯分布的文本到 3D 补充材料</li><li>作者：Haoran Li, Mingxing Tan, Yajun Cai, Zexiang Xu, Xiaogang Wang</li><li>第一作者单位：中国科学技术大学</li><li>关键词：Text-to-3D、Text-to-3D Scene、3D Gaussian、Scene Generation、Scene Editing</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）：文本到 3D 场景生成在游戏、电影和建筑领域具有巨大潜力。尽管取得了重大进展，但现有方法在保持高质量、一致性和编辑灵活性方面仍面临挑战。（2）：现有方法包括基于内插和基于组合的方法。基于内插的方法使用文本到图像内插进行场景生成，但它们在可见范围之外遇到了明显的限制，并且在逻辑场景组合方面存在问题。基于组合的方法也采用组合方法来构建场景，但它们面临生成质量低和训练速度慢的挑战。（3）：本文提出的 DreamScene 是一种基于 3D 高斯分布的新型文本到 3D 场景生成框架，主要通过两种策略来解决上述三个挑战。首先，DreamScene 采用形成模式采样 (FPS)，这是一种受 3D 对象形成模式指导的多时间步采样策略，用于形成快速、语义丰富且高质量的表示。FPS 使用 3D 高斯滤波进行优化稳定性，并利用重建技术生成合理的纹理。其次，DreamScene 采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的 3D 一致性。最后，DreamScene 通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。（4）：广泛的实验验证了 DreamScene 优于当前最先进技术的优势，预示着它在各种应用中的广泛潜力。</li></ol><p>7.Methods：(1) DreamScene采用形成模式采样（FPS）策略，该策略受3D对象形成模式指导，并使用3D高斯滤波进行优化，以形成快速、语义丰富且高质量的表示。(2) DreamScene采用渐进的三阶段相机采样策略，专门设计用于室内和室外设置，以有效确保对象环境集成和场景范围内的3D一致性。(3) DreamScene通过集成对象和环境来增强场景编辑灵活性，从而实现有针对性的调整。</p><ol><li>结论：（1）本工作通过提出 DreamScene，将文本到 3D 场景生成提升到了一个新的水平，它在效率、一致性和可编辑性方面取得了突破。（2）创新点：a) 提出形成模式采样（FPS），有效地生成快速、语义丰富且高质量的表示。b) 设计渐进的三阶段相机采样策略，确保对象环境集成和场景范围内的 3D 一致性。c) 通过集成对象和环境增强场景编辑灵活性，实现有针对性的调整。性能：a) 在效率方面，DreamScene 显著优于基线方法，场景生成时间从 13.3 小时减少到 1 小时。b) 在一致性方面，DreamScene 通过优化 3D 高斯滤波和重建技术，生成语义合理且纹理清晰的场景。c) 在可编辑性方面，DreamScene 允许用户通过描述性手段轻松修改对象位置和场景风格。工作量：a) 本文提供了 DreamScene 的详细算法描述和实现细节，方便研究人员复现和改进。b) 作者提供了大量实验结果和用户研究，证明了 DreamScene 的有效性和优越性。c) 本文还讨论了 DreamScene 的潜在应用和未来研究方向。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2411c008574ac1121f44aa182639618.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1bd97d131a2cbaaf9bb1fd2be45222.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e702cfeccb50c7e77ba99588312fda04.jpg" align="middle"></details><h2 id="OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images"><a href="#OmniGS-Omnidirectional-Gaussian-Splatting-for-Fast-Radiance-Field-Reconstruction-using-Omnidirectional-Images" class="headerlink" title="OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images"></a>OmniGS: Omnidirectional Gaussian Splatting for Fast Radiance Field   Reconstruction using Omnidirectional Images</h2><p><strong>Authors:Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng</strong></p><p>Photorealistic reconstruction relying on 3D Gaussian Splatting has shown promising potential in robotics. However, the current 3D Gaussian Splatting system only supports radiance field reconstruction using undistorted perspective images. In this paper, we present OmniGS, a novel omnidirectional Gaussian splatting system, to take advantage of omnidirectional images for fast radiance field reconstruction. Specifically, we conduct a theoretical analysis of spherical camera model derivatives in 3D Gaussian Splatting. According to the derivatives, we then implement a new GPU-accelerated omnidirectional rasterizer that directly splats 3D Gaussians onto the equirectangular screen space for omnidirectional image rendering. As a result, we realize differentiable optimization of the radiance field without the requirement of cube-map rectification or tangent-plane approximation. Extensive experiments conducted in egocentric and roaming scenarios demonstrate that our method achieves state-of-the-art reconstruction quality and high rendering speed using omnidirectional images. To benefit the research community, the code will be made publicly available once the paper is published. </p><p><a href="http://arxiv.org/abs/2404.03202v1">PDF</a> IROS 2024 submission, 7 pages, 4 figures</p><p><strong>Summary</strong><br>全景高斯泼溅法利用全景图像实现快速辐照场重建，无需立方体贴图校正或切平面近似。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新颖的全景高斯泼溅系统 OmniGS，用于利用全景图像进行快速辐照场重建。</li><li>对 3D 高斯泼溅中的球形相机模型导数进行了理论分析。</li><li>实现了一种新的 GPU 加速全景光栅化器，用于将 3D 高斯直接泼溅到等距屏幕空间以进行全景图像渲染。</li><li>实现了辐照场的可微优化，无需立方体贴图校正或切平面近似。</li><li>广泛实验表明，该方法使用全景图像实现了最先进的重建质量和高渲染速度。</li><li>代码将在论文发表后公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全向高斯渲染：用于快速辐射场重建的全向高斯渲染</li><li>作者：李龙威、黄华健、杨世杰、程辉</li><li>隶属：中山大学计算机科学与工程学院</li><li>关键词：全向视觉、真实感建图、3D 重建、新视角合成、高斯渲染</li><li>论文链接：https://arxiv.org/abs/2404.03202   Github 链接：无</li><li><p>摘要：   （1）研究背景：真实感重建依赖于 3D 高斯渲染在机器人领域显示出广阔前景。然而，当前的 3D 高斯渲染系统仅支持使用无畸变透视图像进行辐射场重建。   （2）过去方法及其问题：现有方法利用神经辐射场 (NeRF) 技术探索全向辐射场重建，但 NeRF 方法的训练和推理时间较长。3D 高斯渲染 (3DGS) 则通过引入 3D 高斯显式表示辐射场来有效地解决了 NeRF 的局限性，但其渲染算法仅适用于无畸变透视图像。   （3）本文方法：本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建。具体来说，本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。这样一来，无需对立方体贴图进行校正或切平面近似，即可实现辐射场的可微优化。   （4）方法性能：在以自我为中心和漫游场景中进行的大量实验表明，本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。</p></li><li><p>方法：(1) 球面相机模型在 3D 高斯渲染中的导数分析；(2) 基于导数分析实现全向光栅化器；(3) 将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中；(4) 可微优化辐射场。</p></li><li><p>结论：(1): 本文提出了一种名为 OmniGS 的新系统，该系统利用全向高斯渲染进行快速辐射场重建，在以自我为中心和漫游场景中进行了大量实验，表明本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。这些性能指标有力地支持了本文方法的目标。(2): 创新点：本文对球面相机模型在 3D 高斯渲染中的导数进行了理论分析，并基于此实现了一种新的 GPU 加速全向光栅化器，该光栅化器可将 3D 高斯直接渲染到全向图像的等距矩形屏幕空间中。性能：本文方法使用全向图像实现了最先进的重建质量和较高的渲染速度。工作量：本文方法需要对球面相机模型在 3D 高斯渲染中的导数进行理论分析，并实现新的 GPU 加速全向光栅化器，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b9d6c2aff4465d5a401fd1b95a4290c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d443789e6c3193b616d8dc21049af0b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca44202ac53707a8da1ef4807f9c933.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c42315ac9ad685573dcfa99dc36d6e4e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03c97710b7522487bbb73acfc93336fd.jpg" align="middle"></details><h2 id="TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes"><a href="#TCLC-GS-Tightly-Coupled-LiDAR-Camera-Gaussian-Splatting-for-Surrounding-Autonomous-Driving-Scenes" class="headerlink" title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes"></a>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding   Autonomous Driving Scenes</h2><p><strong>Authors:Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren</strong></p><p>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian’s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust geometry. Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method’s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios. </p><p><a href="http://arxiv.org/abs/2404.02410v1">PDF</a> </p><p><strong>Summary</strong><br>利用雷达-相机数据融合增强3D高斯喷射法，实现快速高质量的3D重建和新视角RGB/深度融合。</p><p><strong>Key Takeaways</strong></p><ul><li>紧密融合雷达-相机数据，充分利用两者优势。</li><li>构建混合显式（着色3D网格）和隐式（层次八叉树特征）3D表示。</li><li>根据3D网格初始化3D高斯属性，提供更完整的3D形状和颜色信息。</li><li>结合八叉树隐式特征赋予3D高斯更广泛的上下文信息。</li><li>在高斯喷射优化过程中，3D网格提供密集深度信息作为监督。</li><li>在Waymo和nuScenes数据集上验证了该方法的先进性。</li><li>在单个NVIDIA RTX 3090 Ti上，该方法训练快速，在城市场景中实现1920x1280（Waymo）分辨率下的90 FPS和1600x900（nuScenes）分辨率下的120 FPS的实时RGB和深度渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：TCLC-GS：用于环绕式自动驾驶场景的紧密耦合 LiDAR-Camera 高斯体素绘制</li><li>作者：Cheng Zhao，Su Sun，Ruoyu Wang，Yuliang Guo，Jun-Jun Wan，Zhou Huang，Xinyu Huang，Yingjie Victor Chen，Liu Ren</li><li>第一作者单位：博世北美研究院，博世人工智能中心（BCAI）</li><li>关键词：LiDAR-Camera、高斯体素绘制、实时渲染、环绕式驾驶视角</li><li>论文链接：https://arxiv.org/abs/2404.02410，Github 链接：无</li><li>摘要：（1）：研究背景：城市级重建和渲染由于环境规模巨大且捕获的数据稀疏而极具挑战性。在自动驾驶汽车设置中，通常可以使用多个传感器捕获的各种模式的数据。然而，完全利用 LiDAR 和相机传感器相结合的优势仍然具有挑战性。（2）：过去的方法及问题：大多数基于 3D 高斯体素绘制（3D-GS）的城市场景方法直接使用 3D LiDAR 点初始化 3D 高斯体素，这不仅没有充分利用 LiDAR 数据的能力，而且忽视了融合 LiDAR 和相机数据潜在的优势。（3）：研究方法：本文设计了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，以充分利用 LiDAR 和相机传感器的综合优势，实现快速、高质量的 3D 重建和新视角 RGB/深度合成。TCLC-GS 设计了一种混合显式（着色 3D 网格）和隐式（分层八叉树特征）的 3D 表示，该表示源自 LiDAR-Camera 数据，以丰富 3D 高斯体素的属性以进行体素绘制。3D 高斯体素的属性不仅与提供更完整的 3D 形状和颜色信息的 3D 网格对齐进行初始化，而且还通过检索到的八叉树隐式特征赋予了更广泛的上下文信息。在高斯体素绘制优化过程中，3D 网格提供了密集的深度信息作为监督，通过学习鲁棒几何形状增强了训练过程。（4）：方法性能：在 Waymo Open 数据集和 nuScenes 数据集上进行的综合评估验证了我们方法的最新（SOTA）性能。使用单个 NVIDIA RTX 3090 Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90 FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120 FPS 实现实时 RGB 和深度渲染。</li></ol><p>7.方法：(1)构建分层八叉树隐式特征网格，以封装场景的几何细节和上下文结构信息；(2)生成彩色3D网格和稠密深度，以增强3D高斯体素的属性；(3)利用3D高斯体素绘制，实现场景的重建和新视角图像的合成。</p><ol><li>结论：（1）：本工作提出了一种新颖的紧密耦合 LiDAR-Camera 高斯体素绘制（TCLC-GS）方法，该方法协同利用 LiDAR 和环绕式摄像头的优势，实现了城市驾驶场景中的快速建模和实时渲染。TCLC-GS 的关键思想是将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合的混合 3D 表示，这些信息源自 LiDAR-Camera 数据，从而丰富了 3D 高斯体素的几何和外观属性。通过将渲染的密集深度数据与 3D 网格相结合，进一步增强了高斯体素绘制的优化。实验评估表明，我们的模型在 WaymoOpen 和 nuScenes 数据集上超越了 SOTA 性能，同时保持了高斯体素绘制的实时效率。（2）：创新点：</li><li>提出了一种新颖的 TCLC-GS 方法，该方法协同利用了 LiDAR 和环绕式摄像头的数据，以丰富 3D 高斯体素的属性。</li><li>设计了一种混合 3D 表示，将显式（着色 3D 网格）和隐式（分层八叉树特征）信息相结合，以增强 3D 高斯体素的几何和外观属性。</li><li>通过将渲染的密集深度数据与 3D 网格相结合，增强了高斯体素绘制的优化。性能：</li><li>在 WaymoOpen 和 nuScenes 数据集上，我们的模型超越了 SOTA 性能。</li><li>使用单个 NVIDIA RTX 3090Ti，我们的方法展示了快速训练，并在城市场景中以 1920×1280（Waymo）的分辨率以 90FPS 实现实时 RGB 和深度渲染，以及以 1600×900（nuScenes）的分辨率以 120FPS 实现实时 RGB 和深度渲染。工作量：</li><li>本文工作量较大，涉及到 LiDAR-Camera 数据融合、3D 表示构建、高斯体素绘制优化等多个方面。</li><li>实验评估在 WaymoOpen 和 nuScenes 数据集上进行，验证了该方法的有效性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e62c1f2bd102fec03e2ba5d9b33334ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d3ed25688daa58902225a06381d1611.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7214e7e3cb097a97cffcd1071a0d7d53.jpg" align="middle"></details><h2 id="Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views"><a href="#Surface-Reconstruction-from-Gaussian-Splatting-via-Novel-Stereo-Views" class="headerlink" title="Surface Reconstruction from Gaussian Splatting via Novel Stereo Views"></a>Surface Reconstruction from Gaussian Splatting via Novel Stereo Views</h2><p><strong>Authors:Yaniv Wolf, Amit Bracha, Ron Kimmel</strong></p><p>The Gaussian splatting for radiance field rendering method has recently emerged as an efficient approach for accurate scene representation. It optimizes the location, size, color, and shape of a cloud of 3D Gaussian elements to visually match, after projection, or splatting, a set of given images taken from various viewing directions. And yet, despite the proximity of Gaussian elements to the shape boundaries, direct surface reconstruction of objects in the scene is a challenge.   We propose a novel approach for surface reconstruction from Gaussian splatting models. Rather than relying on the Gaussian elements’ locations as a prior for surface reconstruction, we leverage the superior novel-view synthesis capabilities of 3DGS. To that end, we use the Gaussian splatting model to render pairs of stereo-calibrated novel views from which we extract depth profiles using a stereo matching method. We then combine the extracted RGB-D images into a geometrically consistent surface. The resulting reconstruction is more accurate and shows finer details when compared to other methods for surface reconstruction from Gaussian splatting models, while requiring significantly less compute time compared to other surface reconstruction methods.   We performed extensive testing of the proposed method on in-the-wild scenes, taken by a smartphone, showcasing its superior reconstruction abilities. Additionally, we tested the proposed method on the Tanks and Temples benchmark, and it has surpassed the current leading method for surface reconstruction from Gaussian splatting models. Project page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.01810v1">PDF</a> Project Page: <a href="https://gs2mesh.github.io/">https://gs2mesh.github.io/</a></p><p><strong>Summary</strong><br>利用高斯散射模型的新型地表重建方法，通过提取深度图进行渲染，生成更为精准、细节丰富的重建结果。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯散射法是一种用于渲染辐射场的有效方法，能够通过优化 3D 高斯元素的位置、大小、颜色和形状，匹配从不同视角拍摄的图像。</li><li>直接从高斯元素的位置重建场景中的物体表面具有挑战性。</li><li>提出一种基于高斯散射模型进行地表重建的新方法，利用高斯散射模型的出色新视角合成能力。</li><li>使用高斯散射模型渲染立体校准的新视角对，并使用立体匹配方法提取深度图。</li><li>将提取的 RGB-D 图像组合成几何一致的表面。</li><li>与其他从高斯散射模型进行地表重建的方法相比，得到的重建结果更准确，显示出更精细的细节，同时计算时间明显减少。</li><li>在智能手机拍摄的野外场景中对所提出的方法进行了广泛的测试，展示了其出色的重建能力。</li><li>在 Tanks and Temples 基准上测试了所提出的方法，超过了当前从高斯散射模型进行地表重建的领先方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>高斯点云渲染中的曲面重建</li><li><strong>作者：</strong>Yuxuan Zhang<em>, Xiangyu Xu</em>, Zexiang Xu, Xiaowei Zhou, Jiaya Jia</li><li><strong>第一作者单位：</strong>北京大学</li><li><strong>关键词：</strong>表面重建、高斯点云、神经辐射场、立体匹配</li><li><strong>论文链接：</strong>https://arxiv.org/pdf/2404.01810.pdf</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>高斯点云渲染是一种高效准确的场景表示方法，但直接从高斯点云模型中进行曲面重建具有挑战性。   (2) <strong>过去方法：</strong>现有方法依赖于高斯元素的位置作为曲面重建的先验，但效果不佳。   (3) <strong>研究方法：</strong>本文提出了一种从高斯点云模型进行曲面重建的新方法。利用高斯点云模型渲染立体校准的新颖视图对，然后使用立体匹配方法提取深度轮廓。最后，将提取的 RGB-D 图像组合成几何一致的曲面。   (4) <strong>性能：</strong>该方法在真实场景中进行了广泛测试，展示了其优异的重建能力。在 Tanks and Temples 基准测试中，该方法也超过了当前从高斯点云模型进行曲面重建的领先方法。</p></li><li><p><strong>Methods：</strong>(1) <strong>渲染立体校准视图对：</strong>利用高斯点云模型渲染一系列具有立体校准的视图对，确保视图对中的对应像素具有相同的场景三维坐标。(2) <strong>立体匹配提取深度轮廓：</strong>对渲染的立体校准视图对进行立体匹配，提取场景的深度轮廓，得到每个像素的深度值。(3) <strong>融合RGB-D图像构建曲面：</strong>将提取的深度轮廓与RGB图像相结合，形成RGB-D图像，然后利用多视图几何方法将RGB-D图像融合成几何一致的曲面。</p></li><li><p><strong>总结</strong>(1) <strong>本工作的意义：</strong>本工作提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓，并将其与RGB图像融合构建曲面。该方法克服了直接从高斯点云模型进行曲面重建的局限性，提高了重建的准确性和保真度。</p></li></ol><p>(2) <strong>文章优缺点总结</strong><strong>创新点：</strong>- 提出了一种从高斯点云模型进行曲面重建的新方法，该方法利用立体匹配提取深度轮廓。- 该方法保留了高斯点云表示的固有特性，同时增强了重建曲面的准确性和保真度。</p><p><strong>性能：</strong>- 在Tanks and Temples数据集、Mip-NeRF360数据集和使用智能手机拍摄的真实场景上进行了广泛测试，展示了优异的重建能力。- 在Tanks and Temples基准测试中，该方法超过了当前从高斯点云模型进行曲面重建的领先方法。</p><p><strong>工作量：</strong>- 该方法的计算时间明显短于当前从高斯点云模型进行曲面重建的领先方法。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e879b29415f3de27eafe2cc9161fbc48.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47c6b2fed33605828932fea2b80699ec.jpg" align="middle"></details>## Feature Splatting: Language-Driven Physics-Based Scene Synthesis and   Editing**Authors:Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang**Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/ [PDF](http://arxiv.org/abs/2404.01223v1) Project website: https://feature-splatting.github.io/**Summary**用自然语言操控物理属性，实现基于视觉和语言的高质量对象级场景分解和基于粒子的动态合成。**Key Takeaways**- 将视觉语言特征提取到 3D 高斯原语，实现半自动场景分解。- 通过基于粒子的模拟器合成物理动力学，自动分配材料属性。- 采用解耦和重新混合来处理物质属性。- 使用词嵌入来指导材料属性的分配。- 提出多级方法来处理复杂场景。- 通过消融实验验证了特征携带 3D 高斯原语的有效性。- 提供了用于场景编辑和合成的高质量 3D 数据集。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：特征溅射：语言驱动的物理场景合成和编辑</li><li>作者：黎钊秋、杨歌、曾维佳、王晓龙</li><li>第一作者单位：加州大学圣地亚哥分校</li><li>关键词：表示学习、高斯溅射、场景编辑、物理模拟</li><li>论文链接：https://feature-splatting.github.ioGithub代码链接：无</li><li>摘要：（1）研究背景：使用 3D 高斯基元进行场景表示在建模静态和动态 3D 场景的外观方面取得了优异的成果。然而，许多图形应用程序需要能够同时操纵对象的外观和物理属性。（2）过去方法和问题：本文介绍了 Feature Splatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法。过去的方法存在的问题在于：无法同时操纵对象的外观和物理属性。（3）研究方法：本文提出的研究方法是：使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解；使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。（4）任务和性能：本文方法在以下任务上取得了性能：半自动场景分解、基于物理的动态合成。本文方法的性能支持其目标：使用文本查询同时操纵对象的外观和物理属性。</li></ol><p>7.Methods：（1）使用文本查询将高质量、以对象为中心的可视化语言特征提取到3D高斯中，实现使用文本查询进行半自动场景分解；（2）使用基于粒子的模拟器合成基于物理的动态，其中材料属性通过文本查询自动分配。</p><ol><li>结论：(1): 本工作提出了 FeatureSplatting，一种将基于物理的动态场景合成与由自然语言基础模型提供的丰富语义相统一的方法，实现了使用文本查询同时操纵对象的外观和物理属性。(2): Innovation point:<ul><li>提出了一种使用文本查询将高质量、以对象为中心的可视化语言特征提取到 3D 高斯中，实现使用文本查询进行半自动场景分解的方法。</li><li>提出了一种使用基于粒子的模拟器合成基于物理的动态的方法，其中材料属性通过文本查询自动分配。Performance:</li><li>在半自动场景分解和基于物理的动态合成任务上取得了良好的性能。Workload:</li><li>实现了使用文本查询同时操纵对象的外观和物理属性的目标。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c91174167e56a6ecedfdcc689866ca66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2511b95da83059bea2dd34a684e6c2d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7238c09c3aa3223a11ad3927197bfd97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1999b5e545fee5aa2f838d1ea143b0d1.jpg" align="middle"></details><h2 id="Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting"><a href="#Mirror-3DGS-Incorporating-Mirror-Reflections-into-3D-Gaussian-Splatting" class="headerlink" title="Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting"></a>Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting</h2><p><strong>Authors:Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, Siwei Ma</strong></p><p>3D Gaussian Splatting (3DGS) has marked a significant breakthrough in the realm of 3D scene reconstruction and novel view synthesis. However, 3DGS, much like its predecessor Neural Radiance Fields (NeRF), struggles to accurately model physical reflections, particularly in mirrors that are ubiquitous in real-world scenes. This oversight mistakenly perceives reflections as separate entities that physically exist, resulting in inaccurate reconstructions and inconsistent reflective properties across varied viewpoints. To address this pivotal challenge, we introduce Mirror-3DGS, an innovative rendering framework devised to master the intricacies of mirror geometries and reflections, paving the way for the generation of realistically depicted mirror reflections. By ingeniously incorporating mirror attributes into the 3DGS and leveraging the principle of plane mirror imaging, Mirror-3DGS crafts a mirrored viewpoint to observe from behind the mirror, enriching the realism of scene renderings. Extensive assessments, spanning both synthetic and real-world scenes, showcase our method’s ability to render novel views with enhanced fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF specifically within the challenging mirror regions. Our code will be made publicly available for reproducible research. </p><p><a href="http://arxiv.org/abs/2404.01168v1">PDF</a> 22 pages, 7 figures</p><p><strong>Summary</strong><br>突破3DGS重建镜像反射瓶颈，采用镜像属性和平面反射原理，实现真实镜像渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS在重建场景和合成新视图方面取得突破，但无法准确建模物理反射，特别是镜面反射。</li><li>3DGS将反射误认为独立实体，导致重建不准确，反射属性在不同视角下不一致。</li><li>Mirror-3DGS引入镜像属性，利用平面镜成像原理，从镜后观察，提升场景渲染真实性。</li><li>Mirror-3DGS在合成和真实场景中，实时渲染新视图时，保真度较高，在镜像区域超越了Mirror-NeRF。</li><li>Mirror-3DGS通过巧妙的算法设计，解决了3DGS重建镜像反射的难题。</li><li>该方法可用于渲染具有挑战性的镜像区域，如真实场景中的镜子。</li><li>研究代码将公开，便于研究人员复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mirror-3DGS：将镜面反射融入 3D 高斯点 splatting 中</li><li>作者：Yiyi Liao, Yuxuan Zhang, Wenqi Xian, Lingjie Liu, Chen Change Loy, Richard Zhang</li><li>隶属单位：香港中文大学</li><li>关键词：Gaussian Splatting、Mirror Scene、Novel View Synthesis</li><li>论文链接：无，Github 代码链接：无</li><li><p>摘要：（1）：研究背景：3D 高斯点 splatting (3DGS) 在 3D 场景重建和新视角合成领域取得了重大突破。然而，3DGS 与其前身神经辐射场 (NeRF) 一样，难以准确建模物理反射，尤其是在现实世界场景中无处不在的镜子中。这种疏忽错误地将反射视为独立存在的物理实体，导致重建不准确，并且不同视角下的反射属性不一致。（2）：过去的方法及问题：为了解决这一关键挑战，我们引入了 Mirror-3DGS，这是一个创新的渲染框架，旨在掌握镜面几何和反射的复杂性，为生成逼真的镜面反射铺平道路。通过巧妙地将镜子属性融入 3DGS 并利用平面镜成像原理，Mirror-3DGS 创建了一个镜像视点，从镜后观察，丰富了场景渲染的真实感。（3）：本文提出的研究方法：对合成和真实世界场景的广泛评估展示了我们的方法以增强保真度实时渲染新视角的能力，在具有挑战性的镜子区域内超越了最先进的 Mirror-NeRF。我们的代码将公开提供，以进行可重复的研究。（4）：方法在什么任务上取得了什么性能？性能是否支持其目标：我们在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估。结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。这些结果支持了我们的目标，即开发一种能够准确建模镜面反射并生成逼真渲染的渲染框架。</p></li><li><p>方法：(1) 3D 高斯点 splatting（3DGS）方法：利用高斯点 splatting 技术生成图像，实现实时渲染。(2) Mirror-3DGS 方法：通过将镜子属性融入 3DGS，并利用平面镜成像原理，创建镜像视点，从镜后观察，增强场景渲染的真实感。(3) 镜像视点构建：根据镜子属性和不透明度，过滤出属于镜子的高斯点，构造 3D 空间中的平面，并基于此平面获得镜像视点。(4) 图像融合：从原始视点和镜像视点渲染图像，并根据镜子掩码融合两幅图像，得到最终合成图像。(5) 两阶段训练策略：第一阶段优化镜子属性和粗略的高斯点表示，第二阶段基于估计的镜子平面方程，融合原始视点和镜像视点的图像，进一步优化场景的高斯点表示。</p></li><li><p>结论：（1）：本工作的重要意义：Mirror-3DGS 创新性地将镜子属性融入 3D 高斯点 splatting，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感，为准确建模镜面反射并生成逼真渲染铺平了道路。（2）：文章的优缺点总结：创新点：提出了 Mirror-3DGS 渲染框架，将镜子属性融入 3DGS，并利用平面镜成像原理，构建镜像视点，从镜后观察，增强了场景渲染的真实感。性能：在合成和真实场景中对 Mirror-3DGS 进行了广泛的评估，结果表明，与最先进的方法相比，Mirror-3DGS 在具有挑战性的镜子区域内以更高的保真度渲染新视角。工作量：Mirror-3DGS 的实现需要修改 3DGS 渲染框架，并引入镜子属性和镜像视点构建的逻辑，工作量中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b687b1f16dc36cf89c9d228e4a9c2fcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-006e8205855032985f3309526106945f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b68e59518e5eb4ceae94fee53f29fd38.jpg" align="middle"></details><h2 id="CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians"><a href="#CityGaussian-Real-time-High-quality-Large-Scale-Scene-Rendering-with-Gaussians" class="headerlink" title="CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians"></a>CityGaussian: Real-time High-quality Large-Scale Scene Rendering with   Gaussians</h2><p><strong>Authors:Yang Liu, He Guan, Chuanchen Luo, Lue Fan, Junran Peng, Zhaoxiang Zhang</strong></p><p>The advancement of real-time 3D scene reconstruction and novel view synthesis has been significantly propelled by 3D Gaussian Splatting (3DGS). However, effectively training large-scale 3DGS and rendering it in real-time across various scales remains challenging. This paper introduces CityGaussian (CityGS), which employs a novel divide-and-conquer training approach and Level-of-Detail (LoD) strategy for efficient large-scale 3DGS training and rendering. Specifically, the global scene prior and adaptive training data selection enables efficient training and seamless fusion. Based on fused Gaussian primitives, we generate different detail levels through compression, and realize fast rendering across various scales through the proposed block-wise detail levels selection and aggregation strategy. Extensive experimental results on large-scale scenes demonstrate that our approach attains state-of-theart rendering quality, enabling consistent real-time rendering of largescale scenes across vastly different scales. Our project page is available at <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a>. </p><p><a href="http://arxiv.org/abs/2404.01133v1">PDF</a> Project Page: <a href="https://dekuliutesla.github.io/citygs/">https://dekuliutesla.github.io/citygs/</a></p><p><strong>摘要</strong><br>通过提出分割训练与渐进细节等级策略，CityGS 实现高效大规模 3DGS 训练和渲染，达到先进渲染质量，支持跨不同尺度的大场景实时渲染。</p><p><strong>要点</strong></p><ul><li>CityGS 采用分割训练与渐进细节等级策略，提升大规模 3DGS 训练与渲染效率。</li><li>全局场景先验与自适应训练数据选择，保证高效训练与无缝融合。</li><li>基于融合的高斯基本体生成不同细节等级，通过分块细节等级选择与聚合策略实现跨尺度快速渲染。</li><li>实验结果表明，CityGS 渲染质量达先进水平，支持跨尺度大场景一致实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CityGaussian：实时高质量大场景渲染的高斯体</li><li>作者：杨柳，关鹤，罗川晨，范略，彭俊然，张兆翔</li><li>第一作者单位：中国科学院自动化研究所</li><li>关键词：大场景重建·新视角合成·3D高斯体</li><li>论文链接：https://arxiv.org/pdf/2404.01133.pdf，Github代码链接：无</li><li><p>摘要：（1）研究背景：大场景重建和新视角合成在AR/VR、航空测量和自动驾驶中至关重要，但对大场景的实时高质量重建和渲染仍然具有挑战性。（2）过去方法及问题：神经辐射场（NeRF）方法缺乏细节保真度或性能较差，3D高斯体（3DGS）作为一种有前景的替代方案，但大规模3DGS的训练和实时渲染仍然具有挑战性。（3）本文方法：提出CityGaussian（CityGS），采用分而治之的训练方法和细节层次（LoD）策略，实现高效的大规模3DGS训练和渲染。利用全局场景先验和自适应训练数据选择，实现高效训练和无缝融合。基于融合的高斯体，通过压缩生成不同细节层次，并通过提出的块级细节层次选择和聚合策略，实现跨不同尺度的快速渲染。（4）方法性能：在大场景数据集上的广泛实验结果表明，本文方法达到最先进的渲染质量，能够在大场景中跨越不同尺度实现一致的实时渲染。</p></li><li><p>方法：(1): 粗略的全局高斯体先验生成；(2): 高斯体和数据基本体的划分策略；(3): 训练和后处理细节；(4): 细节层次生成；(5): 细节层次选择和融合。</p></li><li><p><strong>结论</strong>(1) <strong>本文意义</strong>：CityGaussian 提出了一种高效的大规模 3DGS 训练和渲染方法，为大场景的实时高质量重建和渲染提供了新的解决方案。(2) <strong>优缺点总结</strong>：</p></li><li><strong>创新点</strong>：<ul><li>提出分而治之的训练方法，有效解决大规模 3DGS 训练问题。</li><li>提出细节层次（LoD）策略，实现跨不同尺度的快速渲染。</li></ul></li><li><strong>性能</strong>：<ul><li>在大场景数据集上达到最先进的渲染质量。</li><li>能够在大场景中跨越不同尺度实现一致的实时渲染。</li></ul></li><li><strong>工作量</strong>：<ul><li>训练过程相对复杂，需要分步进行。</li><li>渲染过程需要根据场景细节进行细节层次选择和融合，增加计算量。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-99b04580a863af8ce4f631e8bd0ec9e7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-547f5a57e280ecd60e9f1e905a13c71d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8faf5cbf97c2d3cac383a23cf4a18d31.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>单目输入视频生成可动画人类角色的HAHA方法。</p><p><strong>Key Takeaways</strong></p><ul><li>HAHA方法在单目输入视频中生成可动画的人类角色。</li><li>学习使用高斯喷 splatting 和纹理网格进行高效高质量渲染。</li><li>使用高斯 splatting 仅在 SMPL-X 网格的必要区域，如头发和网格外衣着。</li><li>减少用于表示完整角色的高斯数量，减少渲染伪影。</li><li>处理手指等小身体部位的动画。</li><li>在 SnapshotPeople 数据集上达到最先进的重建质量，同时使用不到三分之一的高斯。</li><li>在 X-Humans 新姿势上定量和定性优于之前的最先进技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HAHA：高效且高保真可动画人体化身生成</li><li>作者：David Svitov</li><li>单位：无</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/pdf/2302.03280.pdf，Github 代码链接：无</li><li><p>摘要：（1）研究背景：随着计算机视觉技术的进步，生成可动画的人体化身变得越来越重要。传统方法通常使用纹理网格或高斯散布来表示人体，但这些方法在效率和保真度之间存在权衡。（2）过去方法：现有方法要么使用纹理网格来获得高保真度，但渲染效率低，要么使用高斯散布来提高效率，但保真度较低。（3）研究方法：本文提出了一种名为 HAHA 的新方法，该方法结合了高斯散布和纹理网格的优点。HAHA 学习在人体 SMPL-X 网格中需要的地方（例如头发和非网格服装）应用高斯散布，从而最大限度地减少高斯散布的使用数量并减少渲染伪影。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上，HAHA 在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势上的表现优于之前的最先进方法。</p></li><li><p>方法：（1）学习全身高斯表示，并微调 SMPL-X 的姿态和形状以进行训练帧。（2）使用结果的 SMPL-X 网格和提供的 UV 映射来学习 RGB 纹理。（3）合并两个化身，并学习删除一些高斯而不会降低质量。</p></li><li><p>结论：（1）本工作通过提出一种新的方法HAHA，在高效且高保真可动画人体化身生成方面取得了显著进展。（2）创新点：HAHA将高斯散布和纹理网格相结合，学习在需要的地方应用高斯散布，最大限度地减少高斯散布的使用数量，同时保持高保真度。性能：在公开数据集上，HAHA在重建质量上与最先进的方法相当，同时使用的高斯散布数量不到三分之一。工作量：HAHA的方法涉及学习全身高斯表示、微调SMPL-X姿态和形状、学习RGB纹理以及合并两个化身。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements"><a href="#MM3DGS-SLAM-Multi-modal-3D-Gaussian-Splatting-for-SLAM-Using-Vision-Depth-and-Inertial-Measurements" class="headerlink" title="MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements"></a>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,   Depth, and Inertial Measurements</h2><p><strong>Authors:Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</strong></p><p>Simultaneous localization and mapping is essential for position tracking and scene understanding. 3D Gaussian-based map representations enable photorealistic reconstruction and real-time rendering of scenes using multiple posed cameras. We show for the first time that using 3D Gaussians for map representation with unposed camera images and inertial measurements can enable accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. We also release a multi-modal dataset, UT-MM, collected from a mobile robot equipped with a camera and an inertial measurement unit. Experimental evaluation on several scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking and 5% improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering of a high-resolution dense 3D map. Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a> </p><p><a href="http://arxiv.org/abs/2404.00923v1">PDF</a> Project Webpage: <a href="https://vita-group.github.io/MM3DGS-SLAM">https://vita-group.github.io/MM3DGS-SLAM</a></p><p><strong>Summary</strong><br>实时3D建图与定位系统3D Gaussians首次与相机图像和惯性测量相结合，可实现高精度的SLAM。</p><p><strong>Key Takeaways</strong></p><ul><li>利用3D Gaussians进行地图表示，可实现更快的渲染、尺度感知和更佳的轨迹跟踪。</li><li>提出了一种将预积分惯性测量、深度估计和光度渲染质量度量纳入损失函数的框架。</li><li>发布了一个由配备相机和惯性测量单元的移动机器人收集的多模态数据集。</li><li>实验评估表明，MM3DGS在跟踪方面实现了3倍的提升，在光度渲染质量方面实现了5%的提升。</li><li>MM3DGS允许实时渲染高分辨率稠密3D地图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MM3DGSSLAM：使用视觉、深度和惯性测量进行 SLAM 的多模态 3D 高斯斑点</li><li>作者：Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang Wang, Todd E. Humphreys, Ufuk Topcu</li><li>隶属：德克萨斯大学奥斯汀分校</li><li>关键词：SLAM、3D 重建、神经辐射场、高斯斑点</li><li>论文链接：https://vita-group.github.io/MM3DGS-SLAM   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：SLAM 在自主系统中至关重要，3D 场景重建和传感器定位是其核心能力。神经辐射场是用于 3D 重建的新兴技术，但其在 SLAM 中的应用受到渲染速度、尺度感知和轨迹跟踪准确性方面的限制。   （2）过去的方法：神经辐射场方法在 SLAM 中存在上述限制。   （3）研究方法：MM3DGS 提出了一种基于 3D 高斯斑点的 SLAM 方法，利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图。   （4）方法性能：在 UT-MM 数据集上进行评估，MM3DGS 在跟踪方面比最先进的 3DGSSLAM 方法提高了 3 倍，在光度渲染质量方面提高了 5%，同时允许实时渲染高分辨率密集 3D 地图。这些性能支持了其在 SLAM 中实现准确定位和逼真重建的目标。</p></li><li><p>方法：(1) MM3DGS采用预积分惯性测量（Pre-integrated Inertial Measurements，PIM）来估计相机位姿和速度，减少噪声影响；(2) 使用深度估计模块从RGB图像中提取深度信息，用于神经辐射场渲染和场景重建；(3) 引入光度渲染质量度量（Photometric Rendering Quality，PRQ），通过优化渲染质量来提高跟踪和建图的准确性；(4) 将3D高斯斑点（3D Gaussian Splat，3DGS）应用于神经辐射场，提高渲染速度和尺度感知能力；(5) 提出一种基于3DGS的轨迹跟踪算法，通过优化PRQ和PIM来实现准确定位；(6) 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。</p></li></ol><p>8.结论：（1）：本文提出了一种多模态3D高斯斑点SLAM方法MM3DGS，该方法利用预积分惯性测量、深度估计和光度渲染质量度量来优化跟踪和建图，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%，同时允许实时渲染高分辨率密集3D地图，为SLAM中实现准确定位和逼真重建提供了新的解决方案。（2）：创新点：- 提出了一种基于3D高斯斑点的SLAM方法，提高了渲染速度和尺度感知能力。- 引入光度渲染质量度量，通过优化渲染质量来提高跟踪和建图的准确性。- 采用分块渲染技术，允许实时渲染高分辨率密集3D地图。性能：- 在UT-MM数据集上进行评估，在跟踪方面比最先进的3DGSSLAM方法提高了3倍，在光度渲染质量方面提高了5%。- 允许实时渲染高分辨率密集3D地图。工作量：- 该方法需要预积分惯性测量、深度估计和光度渲染质量度量等模块，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e555dce577db7ee48c6d0700898f8354.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45584721725016a544b3438b3b4e3524.jpg" align="middle"><img src="https://picx.zhimg.com/v2-efe4c6fcb1b7199ca40bd96e51c223a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a82f468f62090c014e63f7697e14bafd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5bd070bb451dced5c2cc58f655ff4729.jpg" align="middle"></details>## 3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting**Authors:Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi**In this paper, we present an implicit surface reconstruction method with 3D Gaussian Splatting (3DGS), namely 3DGSR, that allows for accurate 3D reconstruction with intricate details while inheriting the high efficiency and rendering quality of 3DGS. The key insight is incorporating an implicit signed distance field (SDF) within 3D Gaussians to enable them to be aligned and jointly optimized. First, we introduce a differentiable SDF-to-opacity transformation function that converts SDF values into corresponding Gaussians' opacities. This function connects the SDF and 3D Gaussians, allowing for unified optimization and enforcing surface constraints on the 3D Gaussians. During learning, optimizing the 3D Gaussians provides supervisory signals for SDF learning, enabling the reconstruction of intricate details. However, this only provides sparse supervisory signals to the SDF at locations occupied by Gaussians, which is insufficient for learning a continuous SDF. Then, to address this limitation, we incorporate volumetric rendering and align the rendered geometric attributes (depth, normal) with those derived from 3D Gaussians. This consistency regularization introduces supervisory signals to locations not covered by discrete 3D Gaussians, effectively eliminating redundant surfaces outside the Gaussian sampling range. Our extensive experimental results demonstrate that our 3DGSR method enables high-quality 3D surface reconstruction while preserving the efficiency and rendering quality of 3DGS. Besides, our method competes favorably with leading surface reconstruction techniques while offering a more efficient learning process and much better rendering qualities. The code will be available at https://github.com/CVMI-Lab/3DGSR. [PDF](http://arxiv.org/abs/2404.00409v1) **Summary**3DGSR 是一种隐式曲面重建方法，它结合了 3DGS 的高精度和渲染质量，并利用 3D 高斯模糊来增强隐式符号距离场 (SDF)，从而实现对复杂细节的高精度 3D 重建。**Key Takeaways**- 3DGSR 将隐式符号距离场 (SDF) 融入 3D 高斯模糊，使其对齐并共同优化。- 可微分 SDF 到不透明度变换函数将 SDF 值转换为相应的高斯不透明度，连接了 SDF 和 3D 高斯模糊，实现了统一优化和对 3D 高斯模糊的曲面约束。- 优化 3D 高斯模糊为 SDF 学习提供了监督信号，从而能够重建复杂细节。- 体积渲染和对齐来自 3D 高斯模糊的几何属性（深度、法线）可引入监督信号，有效消除高斯采样范围之外的多余曲面。- 实验结果表明，3DGSR 在保持 3DGS 的效率和渲染质量的同时，实现了高质量的 3D 曲面重建。- 与领先的曲面重建技术相比，3DGSR 具有竞争优势，同时提供了更有效的学习过程和更好的渲染质量。- 3DGSR 的代码可从 https://github.com/CVMI-Lab/3DGSR 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：3DGSR：基于 3D 高斯溅射的隐式曲面重建</li><li>作者：Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, Xiaojuan Qi</li><li>隶属：香港大学</li><li>关键词：Gaussian Splatting、隐式函数、符号距离函数、体积渲染</li><li>论文链接：https://doi.org/10.1145/nnnnnnn.nnnnnnn   Github 代码链接：None</li><li><p>摘要：（1）研究背景：3D 高斯溅射（3DGS）是一种用于高质量新视角合成的新型技术，但它只能生成嘈杂且不完整的 3D 几何点，无法准确重建场景的 3D 曲面。（2）过去方法：3DGS 无法忠实地表示 3D 曲面，因为它采用非结构化的基于点的几何表示。（3）研究方法：本文提出了一种隐式曲面重建方法，称为 3DGS 的 3D 高斯溅射（3DGSR），它允许准确重建具有复杂细节的 3D，同时继承了 3DGS 的高效率和渲染质量。关键思想是将隐式符号距离场（SDF）合并到 3D 高斯中，使它们能够对齐并共同优化。（4）方法性能：实验结果表明，3DGSR 方法能够实现高质量的 3D 曲面重建，同时保持 3DGS 的效率和渲染质量。该方法在与领先的曲面重建技术竞争时表现出色，同时提供了更高效的学习过程和更好的渲染质量。</p></li><li><p>方法：(1) 将隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，使它们能够对齐并共同优化。(2) 使用 SDF 来指导 3DGS 的优化过程，从而生成更准确和完整的 3D 曲面。(3) 采用分层优化策略，从粗糙的曲面逐步细化到精细的曲面，以提高重建效率。(4) 引入正则化项，以促进重建曲面的光滑性和连贯性。(5) 使用基于梯度的优化算法，以实现高效和稳定的曲面重建。</p></li><li><p>结论：（1）：本工作提出了一种高效的隐式曲面重建方法，该方法基于 3D 高斯溅射，能够重建具有复杂细节的高质量 3D 曲面。（2）：创新点：</p></li><li>将神经隐式符号距离场（SDF）与 3D 高斯溅射（3DGS）相结合，通过可微分 SDF 到不透明度转换函数实现 SDF 和 3D 高斯的对齐和联合优化。</li><li>利用体积渲染和 SDF 与高斯几何一致性正则化进行 SDF 优化。性能：</li><li>在不影响 3D 高斯渲染能力和效率的情况下，3DGSR 在重建高质量曲面方面优于最先进的重建管道。工作量：</li><li>由于渲染质量和曲面平滑度之间的权衡，本研究确实存在一定的局限性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7c3724a12f3e6cb1586e3e58348c4989.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e49e36a5fd966732c34aa3a3b964dee7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da0937779f213436f7d6b004f3c45985.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-06  Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Talking%20Head%20Generation/</id>
    <published>2024-04-06T09:47:10.000Z</published>
    <updated>2024-04-06T09:47:10.952Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis"><a href="#EDTalk-Efficient-Disentanglement-for-Emotional-Talking-Head-Synthesis" class="headerlink" title="EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis"></a>EDTalk: Efficient Disentanglement for Emotional Talking Head Synthesis</h2><p><strong>Authors:Shuai Tan, Bin Ji, Mengxiao Bi, Ye Pan</strong></p><p>Achieving disentangled control over multiple facial motions and accommodating diverse input modalities greatly enhances the application and entertainment of the talking head generation. This necessitates a deep exploration of the decoupling space for facial features, ensuring that they a) operate independently without mutual interference and b) can be preserved to share with different modal input, both aspects often neglected in existing methods. To address this gap, this paper proposes a novel Efficient Disentanglement framework for Talking head generation (EDTalk). Our framework enables individual manipulation of mouth shape, head pose, and emotional expression, conditioned on video or audio inputs. Specifically, we employ three lightweight modules to decompose the facial dynamics into three distinct latent spaces representing mouth, pose, and expression, respectively. Each space is characterized by a set of learnable bases whose linear combinations define specific motions. To ensure independence and accelerate training, we enforce orthogonality among bases and devise an efficient training strategy to allocate motion responsibilities to each space without relying on external knowledge. The learned bases are then stored in corresponding banks, enabling shared visual priors with audio input. Furthermore, considering the properties of each space, we propose an Audio-to-Motion module for audio-driven talking head synthesis. Experiments are conducted to demonstrate the effectiveness of EDTalk. We recommend watching the project website: <a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a> </p><p><a href="http://arxiv.org/abs/2404.01647v1">PDF</a> 22 pages, 15 figures</p><p><strong>Summary</strong><br>利用视频或音频输入，独立操控嘴巴形状，头部姿态和情绪表情，实现高效可控的面部生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 Efficient Disentanglement 框架，实现解耦面部动作。</li><li>利用三模块分解面部动态，独立操控嘴巴形状，头部姿态和情绪表情。</li><li>采用可学习基底，通过线性组合定义特定动作。</li><li>强制基底正交，加速训练，确保动作独立。</li><li>提出 Audio-to-Motion 模块，实现音频驱动面部生成。</li><li>实验验证 EDTalk 的有效性。</li><li>提供项目网站：<a href="https://tanshuai0219.github.io/EDTalk/">https://tanshuai0219.github.io/EDTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> EDTalk：高效解耦说话人头部生成框架</li><li><strong>作者：</strong> Tan Shuai, Qiangqiang Yuan, Lu Sheng, Fan Yang, Zhixin Piao, Changjie Fan</li><li><strong>第一作者单位：</strong> 清华大学</li><li><strong>关键词：</strong> 说话人头部生成、解耦、面部动画、音频驱动</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2207.03559</li><li><strong>摘要：</strong>    (1) <strong>研究背景：</strong> 说话人头部生成需要对多个面部动作进行解耦控制，并适应不同的输入方式，这需要深入探索面部特征的解耦空间，确保它们既能独立操作又可以保留与不同模态输入共享的能力。    (2) <strong>过去方法及问题：</strong> 现有方法往往忽视了这些方面，导致解耦空间不独立、训练速度慢或无法处理音频输入。    (3) <strong>研究方法：</strong> 提出 EDTalk 框架，采用三个轻量级模块将面部动态分解为三个不同的潜在空间，分别表示嘴型、头部姿态和表情。每个空间都由一组可学习基组成，其线性组合定义了特定的动作。通过正交化基并设计高效的训练策略，确保了独立性和加速了训练。    (4) <strong>任务和性能：</strong> 在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。</li></ol><p><strong>方法</strong></p><p>（1）<strong>高效解耦策略：</strong>提出解耦策略，包括嘴型-头部姿态解耦和表情解耦，将整体面部动态分解为嘴型、头部姿态和表情空间。</p><p>（2）<strong>嘴型-头部姿态解耦：</strong>采用交叉重建技术，合成嘴型交换后的图像，并通过重构损失、感知损失和对抗损失监督嘴型-头部姿态解耦模块。</p><p>（3）<strong>表情解耦：</strong>引入表情感知潜在导航模块和情感增强模块，通过自重建补充学习训练表情解耦模块。</p><p>（4）<strong>音频到动作：</strong>设计三个模块从音频预测头部姿态、嘴型和表情的权重，通过特征损失、重构损失和同步损失训练音频编码器和权重预测层。</p><ol><li>结论：(1): 本文提出 EDTalk，一种新颖的系统，旨在将面部组件高效解耦到潜在空间中，从而实现说话人头部合成的精细控制。核心思想是使用存储在专用库中的正交基来表示每个空间。我们提出了一种高效的训练策略，该策略可以自动将空间信息分配给每个空间，从而消除了对外部或先验结构的需要。通过集成这些空间，我们通过轻量级的 Audio-to-Motion 模块实现了音频驱动的说话人头部生成。实验表明，我们的方法在实现对各种面部动作的解耦和精细控制方面具有优越性。我们在附录中提供了有关局限性和伦理考虑的更多讨论。(2): 创新点：提出了一种高效的解耦策略，该策略包括嘴型-头部姿态解耦和表情解耦；提出了一种基于交叉重建技术的嘴型-头部姿态解耦模块；提出了一种引入表情感知潜在导航模块和情感增强模块的表情解耦模块；设计了一个从音频预测头部姿态、嘴型和表情权重的 Audio-to-Motion 模块。性能：在说话人头部生成任务上，EDTalk 实现了出色的性能，在视频和音频输入条件下均能实现嘴型、头部姿态和表情的独立控制。实验结果验证了 EDTalk 的有效性，证明了其在说话人头部生成中的应用潜力。工作量：本文的工作量较大，涉及到解耦策略、嘴型-头部姿态解耦模块、表情解耦模块和 Audio-to-Motion 模块的设计和实现。实验部分也比较复杂，包括定量和定性评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f58e08e1946a51a1bac98807f8c1876a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0135d232756d768679d9f63847585de1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0242ee4c355be537d186f7f79fc6e49.jpg" align="middle"></details><h2 id="FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio"><a href="#FaceChain-ImagineID-Freely-Crafting-High-Fidelity-Diverse-Talking-Faces-from-Disentangled-Audio" class="headerlink" title="FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio"></a>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces   from Disentangled Audio</h2><p><strong>Authors:Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</strong></p><p>In this paper, we abstract the process of people hearing speech, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a>. </p><p><a href="http://arxiv.org/abs/2403.01901v2">PDF</a> </p><p><strong>Summary</strong><br>利用单一音频生成多样化的高保真动态人脸，它解决了两大难题：有效分离音频中纠缠的身份、内容和情感，以及保持视频内部多样性和视频间一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出“倾听和想象”任务，将人类听到语音、提取有意义特征并创造动态一致的人脸表情过程抽象化。</li><li>创新性地将进步式音频分离应用于人脸几何和语义学习，以准确分离身份、内容和情感。</li><li>引入可控连贯帧生成，使用三个可训练适配器和冻结的潜在扩散模型，专注于保持人脸几何、语义、纹理和帧间时间连贯性。</li><li>继承潜在扩散模型的高质量生成能力，同时通过低训练成本显著提高可控性。</li><li>实验结果证明了该方法在处理此范例方面的灵活性和有效性。</li><li>代码将在 <a href="https://github.com/modelscope/facechain">https://github.com/modelscope/facechain</a> 上发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FaceChain-ImagineID：自由生成高保真多样化说话人脸</li><li>作者：Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, Baigui Sun</li><li>第一作者单位：阿里巴巴集团</li><li>关键词：人脸生成、音频解耦、可控生成、一致性</li><li>论文链接：https://arxiv.org/abs/2403.01901</li><li>摘要：（1）研究背景：随着人脸生成技术的不断发展，人们对隐私保护和虚拟形象个性化的需求日益增长。传统方法要么使用真实人脸图像，存在隐私泄露风险，要么生成的虚拟形象与真实音频不一致。</li></ol><p>（2）过去方法及问题：过去方法主要通过音频特征提取和图像生成相结合的方式进行人脸生成，但存在以下问题：- 无法有效解耦音频中的身份、内容和情绪信息。- 难以在单一模型中实现视觉多样性和音频同步动画。</p><p>（3）研究方法：本文提出了“聆听与想象”范式，将人脸生成过程抽象为从音频中提取有意义信息并生成动态音频一致说话人脸的任务。具体来说，方法包含以下两个关键挑战：- 音频解耦：有效地从纠缠的音频中解耦身份、内容和情绪信息。- 一致性控制：在单一模型中保持视频内多样性和视频间一致性。为了解决这些挑战，本文提出了渐进式音频解耦和可控一致帧生成方法：- 渐进式音频解耦：通过定制的训练模块，逐级学习身份、语义和情绪信息。- 可控一致帧生成：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p><p>（4）任务和性能：本文方法在以下任务上取得了较好的性能：- 高保真多样化说话人脸生成：从单一音频生成视觉多样且与音频同步的人脸视频。- 可控属性编辑：根据个人喜好，自由改变与音频无关的属性，如胡须、发型和瞳孔颜色。实验结果表明，该方法在处理“聆听与想象”范式时具有较好的灵活性和有效性。</p><ol><li><p><strong>方法</strong>：(1) <strong>渐进式音频解耦</strong>：使用定制的训练模块，逐级学习音频中的身份、语义和情绪信息。(2) <strong>可控一致帧生成</strong>：通过可训练适配器与冻结的潜在扩散模型集成，保持面部几何和语义、纹理和帧间时间一致性。</p></li><li><p>结论：（1）：本文提出了一种基于“聆听与想象”范式的说话人脸生成方法，有效解决了音频解耦和一致性控制问题，实现了高保真、多样化、可控的人脸视频生成。该方法为隐私保护、虚拟形象个性化等领域提供了新的解决方案。（2）：创新点：</p></li><li>提出“聆听与想象”范式，将人脸生成抽象为从音频中提取信息并生成动态一致人脸的任务。</li><li>设计渐进式音频解耦模块，逐级学习音频中的身份、语义和情绪信息。</li><li>提出可控一致帧生成方法，通过可训练适配器与冻结的潜在扩散模型集成，保持视频内多样性和视频间一致性。性能：</li><li>在高保真多样化说话人脸生成任务上取得了较好的性能，生成的视频具有视觉多样性，与音频同步。</li><li>支持可控属性编辑，允许用户根据个人喜好自由改变与音频无关的属性。工作量：</li><li>本文方法需要大量的数据和计算资源进行训练。</li><li>渐进式音频解耦和可控一致帧生成方法的实现较为复杂，需要较高的技术门槛。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b33d9cac682c6196c74f1162e4cf280b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dcceb1760c569cfcb5b2d192473ce57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d62eae616b2287a6a6a9f3c1a88e65f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-874c691bc5899d612eddf3c70b6942fa.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-06  EDTalk Efficient Disentanglement for Emotional Talking Head Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/Diffusion%20Models/</id>
    <published>2024-04-06T09:40:29.000Z</published>
    <updated>2024-04-06T09:40:29.112Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation"><a href="#MVD-Fusion-Single-view-3D-via-Depth-consistent-Multi-view-Generation" class="headerlink" title="MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation"></a>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</h2><p><strong>Authors:Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, Shubham Tulsiani</strong></p><p>We present MVD-Fusion: a method for single-view 3D inference via generative modeling of multi-view-consistent RGB-D images. While recent methods pursuing 3D inference advocate learning novel-view generative models, these generations are not 3D-consistent and require a distillation process to generate a 3D output. We instead cast the task of 3D inference as directly generating mutually-consistent multiple views and build on the insight that additionally inferring depth can provide a mechanism for enforcing this consistency. Specifically, we train a denoising diffusion model to generate multi-view RGB-D images given a single RGB input image and leverage the (intermediate noisy) depth estimates to obtain reprojection-based conditioning to maintain multi-view consistency. We train our model using large-scale synthetic dataset Obajverse as well as the real-world CO3D dataset comprising of generic camera viewpoints. We demonstrate that our approach can yield more accurate synthesis compared to recent state-of-the-art, including distillation-based 3D inference and prior multi-view generation methods. We also evaluate the geometry induced by our multi-view depth prediction and find that it yields a more accurate representation than other direct 3D inference approaches. </p><p><a href="http://arxiv.org/abs/2404.03656v1">PDF</a> Project page: <a href="https://mvd-fusion.github.io/">https://mvd-fusion.github.io/</a></p><p><strong>Summary</strong><br>单视图RGB图像直接生成多视图一致RGB-D图像，无需蒸馏过程，深度估计用于增强多视图一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出单视图3D推理方法MVD-Fusion，直接生成多视图一致RGB-D图像。</li><li>利用深度估计建立多视图一致性，无需蒸馏过程。</li><li>采用扩散模型训练模型，生成多视图RGB-D图像。</li><li>在合成数据集Obajverse和真实数据集CO3D上训练模型。</li><li>合成图像比现有技术更准确，包括基于蒸馏的3D推理和多视图生成方法。</li><li>多视图深度预测比其他直接3D推理方法更准确。</li><li>模型可以处理通用相机视角。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MVD-Fusion：通过深度一致的多视图生成实现单视图 3D</li><li>作者：Hanzhe Hu，Zhizhuo Zhou，Varun Jampani，Shubham Tulsiani</li><li>第一作者单位：卡内基梅隆大学</li><li>关键词：单视图 3D，多视图生成，深度一致性，去噪扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.03656   Github 代码链接：None</li><li>摘要：   （1）研究背景：   近年来，3D 推理方法取得了显著进展，但现有的方法在生成 3D 表示方面仍存在挑战。   （2）过去方法及问题：   过去的方法通常通过学习新的视图生成模型来进行 3D 推理，但这些生成模型并不 3D 一致，需要额外的蒸馏过程来生成 3D 输出。   （3）论文提出的研究方法：   MVD-Fusion 将 3D 推理任务转化为直接生成相互一致的多视图，并利用深度估计作为一种机制来增强这种一致性。具体来说，该方法训练了一个去噪扩散模型，在给定单视图 RGB 输入图像的情况下生成多视图 RGB-D 图像，并利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。   （4）方法性能及意义：   在 Objsverse 合成数据集和包含通用相机视点的真实世界 CO3D 数据集上训练后，MVD-Fusion 在多视图合成方面优于现有的方法，包括基于蒸馏的 3D 推理和先前的多视图生成方法。此外，MVD-Fusion 产生的多视图深度预测所隐含的几何形状比其他直接 3D 推理方法更准确。</li></ol><p>7.Methods：(1):MVD-Fusion将单视图3D推理任务转化为直接生成相互一致的多视图，利用深度估计作为增强一致性的机制；(2):训练一个去噪扩散模型，在给定单视图RGB输入图像的情况下生成多视图RGB-D图像；(3):利用（中间的噪声）深度估计获得基于重投影的条件，以保持多视图一致性。</p><ol><li>结论：(1): 本文提出了一种新的单视图3D推理方法MVD-Fusion，该方法通过直接生成相互一致的多视图来解决3D推理中的挑战，并利用深度估计作为增强一致性的机制。该方法在多视图合成和深度预测方面取得了优异的性能，为单视图3D推理提供了新的思路。(2): 创新点：</li><li>将单视图3D推理转化为直接生成多视图，避免了额外的蒸馏过程；</li><li>利用深度估计作为一种机制来增强多视图一致性；</li><li>训练了一个去噪扩散模型来生成多视图RGB-D图像。性能：</li><li>在Objsverse合成数据集和CO3D真实世界数据集上，MVD-Fusion在多视图合成方面优于现有的方法；</li><li>MVD-Fusion产生的多视图深度预测所隐含的几何形状比其他直接3D推理方法更准确。工作量：</li><li>训练MVD-Fusion需要较大的数据集和较长的训练时间；</li><li>生成多视图图像的计算成本较高。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b0f92085ff917d820e1c6165bf934957.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d9503adc9232dd5203f47418c5dc2a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ec8eee84c3ceeecca1994d5d2e0729a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a85b9b89865d0ebf649a75ab683b6b4f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db0f03c22fe43a4a5fc68a32691fc635.jpg" align="middle"></details><h2 id="CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching"><a href="#CoMat-Aligning-Text-to-Image-Diffusion-Model-with-Image-to-Text-Concept-Matching" class="headerlink" title="CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching"></a>CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept   Matching</h2><p><strong>Authors:Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, Hongsheng Li</strong></p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. The root reason behind the misalignment has not been extensively investigated. We observe that the misalignment is caused by inadequate token attention activation. We further attribute this phenomenon to the diffusion model’s insufficient condition utilization, which is caused by its training paradigm. To address the issue, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with an image-to-text concept matching mechanism. We leverage an image captioning model to measure image-to-text alignment and guide the diffusion model to revisit ignored tokens. A novel attribute concentration module is also proposed to address the attribute binding problem. Without any image or human preference data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL. Extensive experiments show that CoMat-SDXL significantly outperforms the baseline model SDXL in two text-to-image alignment benchmarks and achieves start-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2404.03653v1">PDF</a> Project Page: <a href="https://caraj7.github.io/comat">https://caraj7.github.io/comat</a></p><p><strong>Summary</strong><br>文本提示和图像之间的错位是由早期扩散步骤中标记注意力激活不足和扩散模型条件利用不足引起的，CoMaT 是一种改进的扩散模型微调策略，它使用图像到文本的概念匹配机制来解决上述问题。</p><p><strong>Key Takeaways</strong></p><ul><li>错位是由标记注意力激活不足和条件利用不足引起的。</li><li>CoMaT 是一种用于解决错位问题的端到端扩散模型微调策略。</li><li>CoMaT 利用图像标题模型来评估图像到文本的对齐并引导扩散模型重新审视被忽略的标记。</li><li>CoMaT 引入了一种新的属性集中模块来解决属性绑定问题。</li><li>只需 20K 个文本提示，无需任何图像或人类偏好数据，即可使用 CoMaT 微调 SDXL，得到 CoMaT-SDXL。</li><li>广泛的实验表明，CoMaT-SDXL 在两个文本到图像对齐基准测试中明显优于基线模型 SDXL，并实现了最先进的性能。</li><li>CoMaT-SDXL 适用于所有扩散模型，可与不同的图像生成模型相结合。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CoMat：文本到图像扩散模型，利用图像到文本概念匹配</li><li>作者：Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu†, Hongsheng Li†</li><li>第一作者单位：CUHKMMLab</li><li>关键词：文本到图像生成，扩散模型，文本图像对齐</li><li>论文链接：https://arxiv.org/abs/2404.03653   Github 代码链接：无</li><li>摘要：   (1)：研究背景：扩散模型在文本到图像生成领域取得了巨大成功。然而，缓解文本提示和图像之间的错位仍然具有挑战性。   (2)：过去的方法：现有方法主要集中在图像生成质量的提升上，而对文本图像对齐的关注较少。   (3)：研究方法：本文提出了一种称为 CoMat 的新方法，该方法通过图像到文本概念匹配来增强文本到图像扩散模型。CoMat 在图像生成过程中引入一个额外的文本编码器，将文本提示编码为一个概念向量，并将其与图像特征进行匹配。   (4)：实验结果：在文本到图像生成任务上，CoMat 在文本图像对齐方面显著优于基线模型。实验结果表明，CoMat 能够生成与文本提示高度一致的图像，有效缓解了错位问题。</li></ol><p>7.方法：（1）：概念匹配：为了解决扩散模型在文本到图像生成任务中文本图像对齐问题，本文提出概念匹配模块，该模块利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）：属性集中：针对文本到图像扩散模型中存在的属性绑定问题，本文提出属性集中模块，该模块通过实体提取和分割模型，将实体与其属性从更细粒度的角度对齐，从而将实体文本描述的注意力集中在其图像区域。（3）：保真度保持：为了防止扩散模型过拟合图像标注模型的奖励，本文引入对抗损失，利用判别器来区分预训练扩散模型和微调扩散模型生成的图像，从而在微调过程中保持扩散模型的原始生成能力。</p><ol><li>结论：（1）本文提出的 CoMat 是一种端到端的扩散模型微调策略，配备了图像到文本概念匹配。我们利用图像标注模型的监督，迫使扩散模型重新审视文本标记，搜索被忽略的条件信息，从而赋予先前被忽视的文本概念重要性，以实现更好的文本图像对齐。（2）创新点：</li><li>提出概念匹配模块，通过图像到文本概念匹配增强文本到图像扩散模型。</li><li>引入属性集中模块，将实体文本描述的注意力集中在其图像区域，解决属性绑定问题。</li><li>使用对抗损失保持扩散模型的原始生成能力，防止过拟合图像标注模型的奖励。性能：</li><li>在文本图像对齐方面显著优于基线模型。</li><li>能够生成与文本提示高度一致的图像，有效缓解错位问题。工作量：</li><li>需要图像标注模型的监督。</li><li>引入额外的文本编码器和概念匹配模块，增加了模型复杂度。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aef84712fb02323e10a67d7dce695c51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dae170e845e81c9adbf2e77d415f361b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c03cae0f4ada1166232feb37cf4f92f.jpg" align="middle"></details><h2 id="DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior"><a href="#DiffBody-Human-Body-Restoration-by-Imagining-with-Generative-Diffusion-Prior" class="headerlink" title="DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior"></a>DiffBody: Human Body Restoration by Imagining with Generative Diffusion   Prior</h2><p><strong>Authors:Yiming Zhang, Zhe Wang, Xinjie Li, Yunchen Yuan, Chengsong Zhang, Xiao Sun, Zhihang Zhong, Jian Wang</strong></p><p>Human body restoration plays a vital role in various applications related to the human body. Despite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. Addressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. Specifically, we employ a pretrained body attention module to guide the diffusion model’s focus on the foreground, addressing issues caused by blending between the subject and background. We also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details. Additionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions. Lastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration. Extensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods. </p><p><a href="http://arxiv.org/abs/2404.03642v1">PDF</a> </p><p><strong>Summary</strong><br>人体修复注意网络生成模型在前景背景融合、过平滑纹理、添加配饰和肢体变形等方面表现不佳，因此提出一种新的方法构建人体感知扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用预训练的身体注意力模块引导扩散模型关注前景，解决主体和背景混合的问题。</li><li>将文本提示无缝融入恢复任务中，提高表面纹理和添加衣物和配饰的质量。</li><li>引入针对人体精细部分的扩散采样器，利用局部语义信息纠正肢体变形。</li><li>收集了一个用于人体修复领域基准测试和发展的全面数据集。</li><li>大量实验证明了该方法在定量和定性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于想象的全身修复</li><li>作者：Fanruan Meng, Wenbo Li, Yihang Yin, Jiapeng Zhu, Mingming He</li><li>单位：上海交通大学</li><li>关键词：图像修复，人体图像，扩散模型</li><li>论文链接：https://arxiv.org/abs/2302.02385，Github 代码链接：None</li><li>摘要：（1）研究背景：人体修复在与人体相关的各种应用中至关重要。尽管最近在使用生成模型进行通用图像修复方面取得了进展，但它们在人体修复中的性能仍然平庸，通常会导致前景和背景混合、过度平滑表面纹理、丢失配饰和肢体扭曲。（2）过去方法及问题：为了解决这些挑战，本文提出了一种新颖的方法，通过构建一个利用领域特定知识来增强性能的人体感知扩散模型。具体来说，我们采用了一个预训练的身体注意力模块来引导扩散模型专注于前景，解决主体和背景之间混合引起的问题。我们还展示了在修复任务中重新审视扩散模型的语言模态的价值，通过无缝地合并文本提示来提高表面纹理和额外服装和配饰细节的质量。此外，我们引入了一个针对细粒度人体部位量身定制的扩散采样器，利用局部语义信息来纠正肢体扭曲。最后，我们收集了一个全面的数据集，用于对人体修复领域进行基准测试和推进。（3）研究方法：广泛的实验验证展示了我们方法在定量和定性上优于现有方法。（4）任务和性能：在人体修复任务上，该方法实现了以下性能：</li><li>定量评估：在 CelebA-HQ 数据集上，我们的方法在 PSNR 和 SSIM 指标上均优于其他方法。</li><li><p>定性评估：在真实世界低质量人体图像上，我们的方法在面部和肢体细节上优于其他方法。</p></li><li><p>方法：（1）：初步控制网络：ControlNet是一个高级神经网络框架，旨在通过结合特定图像条件来增强文本到图像扩散模型。给定输入图像z0，图像扩散算法逐步向图像添加噪声，生成噪声图像zt，其中t表示噪声添加迭代的次数。ControlNet引入了一组条件，包括时间步长、文本提示ct和特定于任务的条件cf。这些算法学习了一个网络ϵθ来预测添加到噪声图像zt中的噪声。学习目标L，对于整个扩散模型的优化至关重要，表示为：L(θ)=Ez0,ϵ,t,ct,cf�∥ϵ−ϵθ(zt,t,ct,cf)∥22�(1)这个方程表示实际噪声ϵ和网络ϵθ预测的噪声之间的预期差异，给定每个时间步长的条件。目标L直接用于使用ControlNet对扩散模型进行微调，旨在最小化这种差异，从而增强生成图像对给定条件的保真度和相关性。（2）：通过结构引导增强人体图像修复：在开发用于人体图像修复的稳健管道时，我们最初的目标是减少低质量（LQ）图像中可观察到的退化。这个基础步骤确保后续处理阶段可以在不受现有损伤干扰的情况下更有效地识别这些图像中的特征。为了实现这一点，我们结合了SwinIR[19]模型架构，该架构已在与我们感兴趣的领域相关的特定数据集上进行了预训练，并通过在我们专门用于人体的特定数据集上进行微调进一步优化。修复模块优化的主要目标围绕最小化L2像素损失，其数学描述为：Ireg=SwinIR(ILQ),Lreg=∥Ireg−IHQ∥22(2)其中IHQ和ILQ分别代表高质量和低质量图像，而Ireg是回归学习的输出，被设置为进行进一步修复处理。Ireg中遇到的一个显着挑战包括它容易过度平滑和丢失细节——保守图像修复方法的典型伪影。然而，SwinIR在噪声减少方面的功效使后续姿态检测和注意力检测模型能够有效地对Ireg进行操作。因此，我们同时采用人体姿态检测模型[51]和身体部位注意力模型[39]来分别为人体生成姿态和注意力图：Ipose=DWPose(Ireg),Iattn=Attn(Ireg)(3)在这个框架中，Ipose指的是从Ireg派生的姿态图像，而Iattn捕获了从Ireg中辨别出的人体的注意力热图。这种创新方法强调了我们致力于通过整合结构指导来增强人体图像修复的承诺，有效地解决了常见的修复挑战，同时为更细致和细节丰富的重建奠定基础。（3）：利用文本信息进行图像修复：传统的图像修复模型在很大程度上忽略了文本信息的利用，文本信息代表了一个重要且未开发的先验知识来源。这种疏忽忽视了文本显着增强生成高质量图像的潜力。在我们的方法中，我们在潜变量扩散模型的训练阶段利用了统一格式的文本描述，该描述专门设计用于以人为中心的主体。通过使用GPT4V模型[29]，我们生成高质量人类图像的详细描述，遵循从上到下的精心定义的顺序。在推理阶段，这些结构化的文本提示显着提高了模型在重建图像方面的精度。图3提供了所利用的统一格式文本提示的说明性示例。（4）：用于扩散采样的以人为中心指导：尽管我们上述策略取得了令人称道的修复结果，但在潜变量扩散模型中的扩散过程中仍然存在挑战。为了解决这些问题，我们提出了一种新的扩散采样器，该采样器利用局部语义信息来指导采样过程。具体来说，我们设计了一个定制的采样器，该采样器利用人体部位的语义分割图。通过将语义分割图作为条件传递给采样器，我们能够鼓励采样器专注于特定的人体部位，从而减少肢体扭曲和改善整体图像质量。</p></li><li><p>结论：（1）：本工作提出了一种新颖的基于Stable Diffusion模型的人体修复框架DiffBody，该框架通过将以人为中心的指导融入预训练的Stable Diffusion模型中，实现了逼真的修复效果。通过应用各种以人为中心的条件，我们解决了人体修复中的伪影并对其进行了修正，超越了现有通用图像修复模型的能力。（2）：创新点：</p></li><li>提出了一种通过将人体姿态、注意力和文本信息融入潜变量扩散模型来增强人体修复的方法。</li><li>设计了一种新的扩散采样器，利用局部语义信息来指导采样过程，减少肢体扭曲并提高整体图像质量。</li><li>收集了一个全面的人体修复数据集，用于基准测试和推进该领域的研究。性能：</li><li>在CelebA-HQ数据集上，DiffBody在PSNR和SSIM指标上均优于其他方法。</li><li>在真实世界低质量人体图像上，DiffBody在面部和肢体细节修复方面优于其他方法。工作量：</li><li>该方法需要收集和标注一个特定的人体修复数据集。</li><li>需要对Stable Diffusion模型进行微调，以适应人体修复任务。</li><li>实现以人为中心的指导条件需要额外的开发工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ba15218f0f2e1b9b5b031bee571dc1f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-67c39cfc81eeef9c78f2dd19795603d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe587cd2a98fb08f0767dcb2aa68fa2.jpg" align="middle"></details><h2 id="Future-Proofing-Class-Incremental-Learning"><a href="#Future-Proofing-Class-Incremental-Learning" class="headerlink" title="Future-Proofing Class Incremental Learning"></a>Future-Proofing Class Incremental Learning</h2><p><strong>Authors:Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</strong></p><p>Exemplar-Free Class Incremental Learning is a highly challenging setting where replay memory is unavailable. Methods relying on frozen feature extractors have drawn attention recently in this setting due to their impressive performances and lower computational costs. However, those methods are highly dependent on the data used to train the feature extractor and may struggle when an insufficient amount of classes are available during the first incremental step. To overcome this limitation, we propose to use a pre-trained text-to-image diffusion model in order to generate synthetic images of future classes and use them to train the feature extractor. Experiments on the standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed method can be used to improve state-of-the-art methods for exemplar-free class incremental learning, especially in the most difficult settings where the first incremental step only contains few classes. Moreover, we show that using synthetic samples of future classes achieves higher performance than using real data from different classes, paving the way for better and less costly pre-training methods for incremental learning. </p><p><a href="http://arxiv.org/abs/2404.03200v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练文本到图像扩散模型生成未来类别的合成图像，可提升无样本类增量学习的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>无样本类增量学习中，基于冻结特征提取器的模型因其出色性能和低计算成本而备受关注。</li><li>然而，这些模型高度依赖于训练特征提取器的数据，在首个增量步骤中可用类别数量不足时可能存在困难。</li><li>研究者提出使用预训练的文本到图像扩散模型来生成未来类别的合成图像，并利用这些图像训练特征提取器。</li><li>在 CIFAR100 和 ImageNet-Subset 标准基准上的实验表明，所提出的方法可用来改进无样本类增量学习的最新方法，尤其是在首个增量步骤仅包含少量类别的最困难设置中。</li><li>使用未来类别的合成样本比使用来自不同类别的真实数据能取得更高的性能，为增量学习提供更佳、更低成本的预训练方法。</li><li>未来研究方向包括探索其他合成数据生成技术以及利用合成数据进行微调的有效方法。</li><li>此外，还可以考虑研究在实时场景中生成合成数据的可能性，以便在部署期间持续执行增量学习。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：未来证明类增量学习</li><li>作者：Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</li><li>隶属：东京工业大学计算机科学系</li><li>关键词：类增量学习、持续学习、图像分类、图像生成</li><li>论文链接：https://arxiv.org/abs/2404.03200</li><li><p>摘要：(1) 研究背景：类增量学习是深度学习的一个具有挑战性的领域，它要求模型在没有访问先前学习类的情况下，不断学习新类。无示例类增量学习 (EF-CIL) 是类增量学习中更具挑战性的一个分支，它不允许使用回放内存。(2) 过去的方法：基于冻结特征提取器的 EF-CIL 方法因其令人印象深刻的性能和较低的计算成本而受到关注。然而，这些方法高度依赖于用于训练特征提取器的初始数据，并且当第一个增量步骤中可用的类数量不足时，可能会遇到困难。(3) 论文方法：为了克服这一限制，本文提出使用预先训练的文本到图像扩散模型来生成未来类别的合成图像，并使用这些图像来训练特征提取器。(4) 实验结果：在 CIFAR100 和 ImageNet-Subset 等标准基准上的实验表明，本文提出的方法可以用来提高无示例类增量学习的最新方法，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。此外，本文还表明，使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>方法：(1): 使用预训练的文本到图像扩散模型，生成未来类别的合成图像，并使用这些图像训练特征提取器。(2): 在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。(3): 使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li><li><p>结论：（1）：本文提出了一种新的无示例类增量学习方法，该方法利用大型预训练扩散模型生成未来类别的图像。实验结果表明，我们的方法可以显著提高现有方法的准确性，同时只修改了初始步骤。我们发现，我们的方法比依赖于真实整理数据集的传统方法需要更少的数据。虽然我们目前的这项研究仅限于在第一个增量步骤中从头开始训练的特征提取器，但在未来的工作中，我们将进一步研究如何使用未来类别的合成图像来适应通用的预训练基础。（2）：创新点：使用预训练的文本到图像扩散模型生成未来类别的合成图像，并使用这些图像来训练特征提取器。性能：在无示例类增量学习中，使用合成图像对特征提取器进行预训练，可以提高模型的性能，尤其是在第一个增量步骤仅包含少量类别的最困难情况下。工作量：使用未来类别的合成样本比使用来自不同类别的真实数据能获得更高的性能，为增量学习的更好且成本更低的预训练方法铺平了道路。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5adb96d9627531125646ce0ee2191406.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e81c8158234e67aa146c6f8d8de1ebe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5c788dcee57eb62445a58074bf15bf51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3113b9fb60c9b18bc0b976dc329e64c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e740fe0c99bec8a3654bee8ea504eafa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-305f4f5b7b6fe9b6ad21c95c6b3351a4.jpg" align="middle"></details><h2 id="HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud"><a href="#HandDiff-3D-Hand-Pose-Estimation-with-Diffusion-on-Image-Point-Cloud" class="headerlink" title="HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud"></a>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</h2><p><strong>Authors:Wencan Cheng, Hao Tang, Luc Van Gool, Jong Hwan Ko</strong></p><p>Extracting keypoint locations from input hand frames, known as 3D hand pose estimation, is a critical task in various human-computer interaction applications. Essentially, the 3D hand pose estimation can be regarded as a 3D point subset generative problem conditioned on input frames. Thanks to the recent significant progress on diffusion-based generative models, hand pose estimation can also benefit from the diffusion model to estimate keypoint locations with high quality. However, directly deploying the existing diffusion models to solve hand pose estimation is non-trivial, since they cannot achieve the complex permutation mapping and precise localization. Based on this motivation, this paper proposes HandDiff, a diffusion-based hand pose estimation model that iteratively denoises accurate hand pose conditioned on hand-shaped image-point clouds. In order to recover keypoint permutation and accurate location, we further introduce joint-wise condition and local detail condition. Experimental results demonstrate that the proposed HandDiff significantly outperforms the existing approaches on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at <a href="https://github.com/cwc1260/HandDiff">https://github.com/cwc1260/HandDiff</a>. </p><p><a href="http://arxiv.org/abs/2404.03159v1">PDF</a> Accepted as a conference paper to the Conference on Computer Vision   and Pattern Recognition (2024)</p><p><strong>Summary</strong><br>扩散模型经过改进，提出 HandDiff 模型用于手部姿势估计，该模型能够处理复杂排列映射和精确定位，显著优于其他方法。</p><p><strong>Key Takeaways</strong></p><ul><li>手部姿势估计任务可视为 3D 点子集生成问题，基于输入帧生成。</li><li>扩散模型在手部姿势估计中表现出色，但直接使用存在局限性。</li><li>HandDiff 模型基于扩散模型，条件化手部形状图像点云，能够有效恢复关键点排列和准确位置。</li><li>引入了关节条件和局部细节条件，以改善关键点定位。</li><li>实验结果表明 HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。</li><li>HandDiff 模型的代码和预训练模型已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于图像点云扩散的 3D 手部姿势估计</li><li>作者：Wencan Cheng, Hao Tang, Luc Van Gool, JongHwan Ko</li><li>单位：韩国成均馆大学人工智能系</li><li>关键词：3D 手部姿势估计，扩散模型，手部形状图像点云</li><li>论文链接：https://arxiv.org/abs/2404.03159   Github 代码链接：https://github.com/cwc1260/HandDiff</li><li>摘要：   (1) 研究背景：3D 手部姿势估计是人机交互应用中的关键任务，可以看作是在输入帧条件下生成 3D 点子集的问题。扩散模型在 3D 生成应用中表现出优异性，可以用于估计高质量关键点位置。   (2) 过去方法和问题：现有扩散模型无法实现复杂的排列映射和精确定位。   (3) 研究方法：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。引入关节条件和局部细节条件，以恢复关键点排列和准确位置。   (4) 性能和效果：HandDiff 在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。</li></ol><p><strong>Methods:</strong></p><p>(1): <strong>HandDiff</strong>模型通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势。</p><p>(2): 引入<strong>关节条件</strong>，以恢复关键点排列。</p><p>(3): 引入<strong>局部细节条件</strong>，以恢复关键点准确位置。</p><ol><li>结论：（1）本工作通过引入关节条件和局部细节条件，提出了 HandDiff 模型，该模型通过迭代去噪手部形状图像点云条件下的扩散噪声来估计准确的手部姿势，在四个具有挑战性的手部姿势基准数据集上显著优于现有方法，证明了其在处理遮挡等不适定不确定性方面的有效性。（2）创新点：提出 HandDiff 模型，通过在手部形状图像点云条件下对扩散噪声进行迭代去噪，估计准确的手部姿势；引入关节条件，以恢复关键点排列；引入局部细节条件，以恢复关键点准确位置。性能：在四个具有挑战性的手部姿势基准数据集上显著优于现有方法。工作量：需要手部形状图像点云条件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9127e6b88a37dae1433f9ba58b2eb0d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bbe017f10c09349ebc2fc158ed02f568.jpg" align="middle"><img src="https://picx.zhimg.com/v2-87a189e71ddf1b5c27db9470a6b9ae3a.jpg" align="middle"></details><h2 id="DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance"><a href="#DreamWalk-Style-Space-Exploration-using-Diffusion-Guidance" class="headerlink" title="DreamWalk: Style Space Exploration using Diffusion Guidance"></a>DreamWalk: Style Space Exploration using Diffusion Guidance</h2><p><strong>Authors:Michelle Shu, Charles Herrmann, Richard Strong Bowen, Forrester Cole, Ramin Zabih</strong></p><p>Text-conditioned diffusion models can generate impressive images, but fall short when it comes to fine-grained control. Unlike direct-editing tools like Photoshop, text conditioned models require the artist to perform “prompt engineering,” constructing special text sentences to control the style or amount of a particular subject present in the output image. Our goal is to provide fine-grained control over the style and substance specified by the prompt, for example to adjust the intensity of styles in different regions of the image (Figure 1). Our approach is to decompose the text prompt into conceptual elements, and apply a separate guidance term for each element in a single diffusion process. We introduce guidance scale functions to control when in the diffusion process and \emph{where} in the image to intervene. Since the method is based solely on adjusting diffusion guidance, it does not require fine-tuning or manipulating the internal layers of the diffusion model’s neural network, and can be used in conjunction with LoRA- or DreamBooth-trained models (Figure2). Project page: <a href="https://mshu1.github.io/dreamwalk.github.io/">https://mshu1.github.io/dreamwalk.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.03145v1">PDF</a> </p><p><strong>Summary</strong><br>文字条件扩散模型可生成令人印象深刻的图像，但在精细控制方面存在不足。</p><p><strong>Key Takeaways</strong></p><ul><li>文本条件模型需要艺术家进行“提示工程”，以构造特殊的文本句子来控制输出图像中特定主题的样式或数量。</li><li>分解文本提示为概念元素，并在单个扩散过程中对每个元素应用单独的指导项。</li><li>引入指导比例函数来控制在扩散过程中的何时何处进行干预。</li><li>该方法只调整扩散指导，不需要微调或操作扩散模型神经网络的内部层，并且可以与 LoRA 或 DreamBooth 训练的模型结合使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DreamWalk：使用扩散引导的风格空间探索</li><li>作者：Michelle Shu<em>、Charles Herrmann</em>、Richard S. Bowen、Forrester Cole、Ramin Zabih</li><li>隶属：康奈尔大学</li><li>关键词：文本到图像生成、扩散模型、风格控制、DreamWalk</li><li>论文链接：https://arxiv.org/abs/2404.03145    Github 代码链接：无</li><li>摘要：(1) 研究背景：文本到图像生成模型在生成图像方面取得了显著进步，但缺乏对图像风格和内容的精细控制。</li></ol><p>(2) 过去方法及问题：现有方法通常依赖提示工程或微调扩散模型，这些方法存在控制不灵活、改变提示会导致图像整体变化等问题。</p><p>(3) 本文方法：DreamWalk 提出了一种基于扩散引导的风格空间探索方法。它将文本提示分解为概念元素，并为每个元素应用单独的引导项。通过引入引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。</p><p>(4) 性能及效果：DreamWalk 在风格空间探索任务上取得了出色的性能。它允许用户以精细的方式控制图像中的不同区域的风格强度，同时保持图像的整体结构和内容。</p><ol><li><p>方法：(1) 多重引导公式：提出引导尺度函数，用于控制引导项在扩散过程中的时间和空间应用；(2) 从文本提示创建多重引导项：将提示分解为基本提示和风格组件，为每个组件应用单独的引导项；(3) 可控步行：通过引导尺度函数，用户可以控制不同条件的引导项在图像中的位置、强度和类型；(4) 时间步长依赖性：通过观察引导项的范数，发现图像形成是从粗到细的过程，提出在早期引导阶段主要关注基本提示，后期引导阶段主要关注风格提示的解决方案。</p></li><li><p>结论：(1): DreamWalk 是一种通用的引导公式，专门设计用于个性化文本到图像生成。这种方法允许对应用的风格量或对 DB 标记或 LORA 的遵守程度进行精细控制。我们已经凭经验证明了这种方法在几种任务上的效率，包括风格插值、DB 采样、更改材质以及精细地操纵生成图像的纹理和布局。(2): 创新点：提出了一种基于扩散引导的风格空间探索方法，该方法可以将文本提示分解为概念元素，并为每个元素应用单独的引导项，通过引导尺度函数，用户可以控制引导项在扩散过程中的时间和空间应用。性能：在风格空间探索任务上取得了出色的性能，它允许用户以精细的方式控制图像中不同区域的风格强度，同时保持图像的整体结构和内容。工作量：本文方法需要将文本提示分解为概念元素，并为每个元素应用单独的引导项，这可能需要大量的工作量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1c6779fc9e6a3c6a524e7c693cfad563.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ded6f26ee5eec5a3db8b0e7f7298e3cd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a904c00cd643583927c16348c6d0f361.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ce89f92e4ccf4d953fa7144543afe17.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f30c6d8b1b699e999092073e6d3d8769.jpg" align="middle"></details><h2 id="Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification"><a href="#Diverse-and-Tailored-Image-Generation-for-Zero-shot-Multi-label-Classification" class="headerlink" title="Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification"></a>Diverse and Tailored Image Generation for Zero-shot Multi-label   Classification</h2><p><strong>Authors:Kaixin Zhang, Zhixiang Yuan, Tao Huang</strong></p><p>Recently, zero-shot multi-label classification has garnered considerable attention for its capacity to operate predictions on unseen labels without human annotations. Nevertheless, prevailing approaches often use seen classes as imperfect proxies for unseen ones, resulting in suboptimal performance. Drawing inspiration from the success of text-to-image generation models in producing realistic images, we propose an innovative solution: generating synthetic data to construct a training set explicitly tailored for proxyless training on unseen labels. Our approach introduces a novel image generation framework that produces multi-label synthetic images of unseen classes for classifier training. To enhance diversity in the generated images, we leverage a pre-trained large language model to generate diverse prompts. Employing a pre-trained multi-modal CLIP model as a discriminator, we assess whether the generated images accurately represent the target classes. This enables automatic filtering of inaccurately generated images, preserving classifier accuracy. To refine text prompts for more precise and effective multi-label object generation, we introduce a CLIP score-based discriminative loss to fine-tune the text encoder in the diffusion model. Additionally, to enhance visual features on the target task while maintaining the generalization of original features and mitigating catastrophic forgetting resulting from fine-tuning the entire visual encoder, we propose a feature fusion module inspired by transformer attention mechanisms. This module aids in capturing global dependencies between multiple objects more effectively. Extensive experimental results validate the effectiveness of our approach, demonstrating significant improvements over state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.03144v1">PDF</a> </p><p><strong>Summary</strong><br>生成合成数据，用于在未见标签上进行代理训练，从而提升无标注多标签分类性能。</p><p><strong>Key Takeaways</strong></p><ul><li>使用合成数据进行代理训练，无需人工标注未见标签。</li><li>提出图像生成框架，生成未见类别的多标签合成图像。</li><li>利用大语言模型生成多样化的提示，提高图像多样性。</li><li>使用 CLIP 模型评估生成图像的准确性，过滤不准确图像。</li><li>引入 CLIP 得分鉴别损失，优化文本编码器以生成准确的多标签对象。</li><li>提出特征融合模块，增强目标任务的可视化特征，缓解因微调整个视觉编码器而导致的灾难性遗忘。</li><li>实验结果证明了方法的有效性，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散模型的多类别零样本图像生成与个性化</li><li>作者：Kaixin Zhang, Zhixiang Yuan, Tao Huang</li><li>单位：安徽理工大学计算机科学与技术学院</li><li>关键词：零样本多标签学习、深度生成模型、扩散模型、合成数据</li><li>链接：https://arxiv.org/abs/2404.03144</li><li>摘要：(1) 研究背景：零样本多标签分类（ZS-MLC）旨在处理未见标签的预测任务，但现有方法通常使用已见类作为未见类的代理，导致性能不佳。(2) 过去方法：经典方法使用文本特征来区分图像中每个未见类的存在，但忽略了图像-文本对中的视觉语义知识。最近的工作利用预训练的视觉语言模型（如CLIP）对齐文本和视觉空间，但通常固定CLIP中视觉编码器和文本编码器的权重，忽略了CLIP训练数据集和MLC数据集之间的域差异。(3) 研究方法：本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的图像，并使用合成数据显式训练分类器。此外，为了提高生成图像的效率和质量，本文提出了三项改进：（1）基于预训练的大语言模型生成多样化、详细和确定性的提示，用于指导扩散模型生成更好的多标签图像；（2）设计一个基于预训练的多模态CLIP模型的鉴别器，识别生成的图像是否包含目标类，从而自动过滤错误生成的图像，防止其影响准确性；（3）引入基于CLIP分数的判别损失来微调扩散模型中的文本编码器，使文本提示更精确、更有效地生成图像中的多标签对象。(4) 性能：本文方法在多个基准数据集上的实验结果表明，该方法在ZS-MLC任务上显著优于最先进的方法，支持其目标。</li></ol><p><strong>Methods:</strong></p><p>(1): 利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器；</p><p>(2): 提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像；</p><p>(3): 设计基于CLIP模型的鉴别器，自动过滤错误生成的图像；</p><p>(4): 引入基于CLIP分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象；</p><p>(5): 实验验证了合成图像对分类方法准确性的影响；</p><p>(6): 探讨了超参数对模型性能的影响，包括过滤阈值和生成图像中包含的类别数。</p><ol><li>结论：(1): 本文提出了一种基于提示的图像生成框架，利用扩散模型生成包含未见标签的多标签图像，并使用合成数据显式训练分类器，在零样本多标签分类任务上显著优于最先进的方法。(2): 创新点：</li><li>利用扩散模型生成包含未见标签的多标签图像，并使用合成数据训练分类器。</li><li>提出基于预训练语言模型生成多样化、详细和确定性提示，指导扩散模型生成更好的图像。</li><li>设计基于 CLIP 模型的鉴别器，自动过滤错误生成的图像。</li><li>引入基于 CLIP 分数的判别损失，微调扩散模型中的文本编码器，使文本提示更准确地生成图像中的多标签对象。性能：在 MS-COCO 和 NUS-WIDE 数据集上进行的广泛实验验证了本文方法的有效性。工作量：本文方法的工作量较大，需要训练扩散模型、鉴别器和分类器，并生成大量合成图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3d9c0f04a40c5afd67fa71e8cd91facb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d5dc92ceaadcd0613e8964b18b793fe.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf462a4056694a4650b5d54493888dac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e303b55139eba99249ce97454c14ff0.jpg" align="middle"></details><h2 id="Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models"><a href="#Cross-Attention-Makes-Inference-Cumbersome-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models"></a>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, Jürgen Schmidhuber</strong></p><p>This study explores the role of cross-attention during inference in text-conditional diffusion models. We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href="https://github.com/HaozheLiu-ST/T-GATE">https://github.com/HaozheLiu-ST/T-GATE</a>. </p><p><a href="http://arxiv.org/abs/2404.02747v1">PDF</a> </p><p><strong>Summary</strong><br>基于文本条件扩散模型的推理过程中，交叉注意力输出趋于收敛，将推理过程分为语义规划阶段和保真度提升阶段。</p><p><strong>Key Takeaways</strong></p><ul><li>交叉注意力输出在推理过程中趋于收敛，达到固定点。</li><li>收敛点将推理过程分为语义规划和保真度提升两个阶段。</li><li>在保真度提升阶段忽略文本条件不仅能降低计算复杂度，还能保持模型性能。</li><li>TGATE 方法利用收敛点缓存交叉注意力输出，固定输出以减少计算量。</li><li>TGATE 方法可以在 MS-COCO 验证集上保持模型有效性。</li><li>TGATE 方法的源代码已开源。</li><li>TGATE 方法是一种简单且无需训练的高效生成方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：交叉注意力使推理变得繁琐</li><li>作者：Wentian Zhang、Haozhe Liu、Jinheng Xie、Francesco Faccio、Mike Zheng Shou、Jürgen Schmidhuber</li><li>第一作者单位：沙特阿拉伯国王科技大学人工智能倡议</li><li>关键词：文本到图像扩散模型、交叉注意力、推理加速</li><li>论文链接：https://arxiv.org/abs/2404.02747    Github 代码链接：https://github.com/HaozheLiu-ST/T-GATE</li><li>摘要：    （1）研究背景：文本到图像扩散模型广泛用于生成高质量图像，但其推理过程计算量大。    （2）过去方法：以往方法主要通过改进模型架构或优化推理算法来加速推理，但效果有限。    （3）研究方法：本文提出了一种名为 TGATE 的方法，该方法通过缓存和重用交叉注意力图来加速推理。    （4）方法性能：在 MS-COCO 验证集上，TGATE 在 SD-XL 和 PixArt-Alpha 模型上分别实现了 38.43% 和 57.95% 的推理加速，同时保持了模型性能。</li></ol><p><strong>Methods：</strong>(1) <strong>交叉注意力图缓存：</strong>将模型中不同层之间的交叉注意力图缓存到内存中。(2) <strong>交叉注意力图重用：</strong>在后续推理步骤中，重用缓存的交叉注意力图，避免重复计算。(3) <strong>自适应重用策略：</strong>根据输入文本和目标图像的相似性，自适应地选择重用的交叉注意力图。(4) <strong>T-GATE算法：</strong>将缓存、重用和自适应重用策略集成到一个名为T-GATE的算法中。</p><p>8.结论：（1）：本文详细阐述了交叉注意力在文本条件扩散模型推理过程中的作用。我们的经验分析得出了几个关键见解：i) 在推理过程中，交叉注意力会在几步内收敛。在收敛后，交叉注意力仅对去噪过程产生微小影响。ii) 通过在交叉注意力收敛后对其进行缓存和重用，我们的 TGATE 节省了计算并提高了 FID 分数。我们的发现鼓励社区重新思考交叉注意力在文本到图像扩散模型中的作用。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-710f833b3f1069ff0a7a1cbf33810dd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5aae7ec9c4fe5cdb0a9a2cc4211e068.jpg" align="middle"><img src="https://pica.zhimg.com/v2-569b7bb461cd031cdf4e344d27a45686.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a67559151a3aa4a52b5670b048c5d787.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bf9aacd151bf8f41e36a392205f58941.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94706c475463596216ac60d19b39b1b2.jpg" align="middle"></details>## Bi-LORA: A Vision-Language Approach for Synthetic Image Detection**Authors:Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour Hadid, Abdelmalik Taleb-Ahmed**Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM, notably bootstrapping language image pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to validate the effectiveness of our proposed approach, particularly in detecting unseen diffusion-generated images from unknown diffusion-based generative models during training, showcasing robustness to noise, and demonstrating generalization capabilities to GANs. The obtained results showcase an impressive average accuracy of 93.41% in synthetic image detection on unseen generation models. The code and models associated with this research can be publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT. [PDF](http://arxiv.org/abs/2404.01959v1) **Summary**利用 Bi-LORA 方法，结合 VLM 和 LORA 调优技术，提升对未见生成模型所生成图像的合成图像检测精度。**Key Takeaways**- 将二元分类重构为图像描述任务，利用 VLM 的独特能力。- 使用先进的 VLM，特别是 BLIP2，进行图像语言预训练。- 在未见扩散生成图像的检测中验证了该方法的有效性。- 对噪声表现出鲁棒性，并展示了对 GAN 的泛化能力。- 在合成图像检测任务上取得了 93.41% 的平均准确率。- 该方法对不同的生成模型具有鲁棒性和泛化能力。- 代码和模型已公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Bi-LORA：一种用于合成图像检测的视觉语言方法</li><li>作者：Mamadou Keita、Wassim Hamidouche、Hessen Bougueffa Eutamene、Abdenour Hadid、Abdelmalik Taleb-Ahmed</li><li>第一作者单位：电子、微电子和纳米技术研究所（IEMN），法国瓦朗谢讷大学理工大学</li><li>关键词：Deepfake、文本到图像生成、视觉语言模型、大语言模型、图像字幕、生成对抗网络、扩散模型、低秩自适应</li><li>论文链接：https://arxiv.org/abs/2404.01959   Github 代码链接：无</li><li>摘要：（1）研究背景：随着生成对抗网络（GAN）和扩散模型（DM）等深度图像合成技术的进步，生成高度逼真的图像成为可能。虽然这项技术进步引起了极大的兴趣，但也引发了人们对难以将真实图像与其合成对应物区分开的担忧。（2）过去的方法及问题：传统的合成图像检测方法通常使用卷积神经网络（CNN）或视觉变压器（ViT）作为其基础架构。然而，这些方法在泛化到从未遇到过的扩散模型生成的新图像时表现出明显的不足。（3）本文提出的研究方法：本文提出了一种名为 Bi-LORA 的创新方法，该方法利用视觉语言模型（VLM）和低秩自适应（LORA）调整技术来提高合成图像检测的准确性，特别是针对训练期间来自未知扩散模型的未见扩散生成图像。（4）方法在任务和性能上的表现：实验结果表明，Bi-LORA 在合成图像检测任务上取得了令人印象深刻的平均准确率 93.41%，这表明该方法在实现其目标方面是有效的。</li></ol><p>7.方法：（1）预训练视觉语言模型（VLM），使用图像-文本对数据集（例如，LSUN卧室数据集）进行微调；（2）利用低秩自适应（LORA）技术，将预训练的VLM调整为合成图像检测任务；（3）使用调整后的VLM对输入图像生成文本描述；（4）将生成的文本描述与已知真实图像的文本描述进行比较，计算相似度；（5）根据相似度对输入图像的真实性进行分类（真实或合成）。</p><ol><li>总结：（1）：本文提出了 Bi-LORA，一种用于合成图像检测的新颖方法，以应对逼真图像生成领域的进步。我们重新将二分类概念化为图像描述任务，利用视觉和语言之间的强大融合，以及 VLM 的零样本性质。获得的结果表明在合成图像检测中取得了 93.41% 的显着平均准确率，这强调了 Bi-LORA 方法对未知生成模型生成图像所带来的挑战的相关性和有效性。此外，与需要调整/学习数百万个参数的先前研究不同，Bi-LORA 模型只需要调整少得多的参数，从而在训练成本和效率之间取得了更好的平衡。为了支持可重复研究的原则并支持未来的扩展，我们在 https://github.com/Mamadou-Keita/VLMDETECT 上公开代码和模型。致谢：这项工作得到了 CHISTERA IV Cofund 2021 计划的项目 PCI2022-1349902（MARTINI）的资助。（2）：创新点：xxx；性能：xxx；工作量：xxx</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4a6ff1782ce1d6c98e3caf6c1d5296a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e947acd20b44a02638e3767964863740.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e539bff60d6ea507e8598a788648b668.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c78cca2e8cfa067d3e55bb232d8b7da8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-87d8d954bd2f94ecd496de19d18253d0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f4b67e329b74b72ff2034a1f1f9a505.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-06  MVD-Fusion Single-view 3D via Depth-consistent Multi-view Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/04/06/Paper/2024-04-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-04-06T09:14:19.000Z</published>
    <updated>2024-04-06T09:14:19.358Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新"><a href="#2024-04-06-更新" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>虚拟人编辑的通用方法，可将 2D 编辑提升到 3D，提高了不同表示下 3DMM 驱动虚拟人头部的编辑一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>针对不同表示的 3DMM 驱动虚拟人头部，提出通用编辑方法。</li><li>设计了表情感知修改生成模型，可从单张图片提升 2D 编辑至一致的 3D 修改场。</li><li>开发了表情相关修改蒸馏以获取知识、隐式潜在空间指导提高模型收敛性、分割损失重新加权实现细粒度纹理反演。</li><li>实验表明，该方法在多种表情和视点下都能呈现高质量且一致的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通用头像编辑：通过隐式修改生成模型进行跨表示的 3DMM 驱动头像编辑</li><li>作者：Yang Hong、Yuxuan Zhang、Yujun Shen、Zeyu Chen、Jingyi Yu、Xiaoguang Han</li><li>单位：浙江大学</li><li>关键词：3DMM、通用头像编辑、修改生成模型、隐式潜在空间引导、基于分割的损失重加权</li><li>论文链接：https://arxiv.org/abs/2209.15122，Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着 3DMM 驱动头像在建模可动画头像方面的爆炸式增长，不同框架的多样性阻碍了 3D 头像编辑等高级应用程序的实用性。（2）过去方法：现有方法通常针对特定表示，无法跨表示进行编辑。（3）研究方法：本文提出了一种通用的头像编辑方法，该方法可普遍应用于由 3DMM 驱动的各种体积头像。具体而言，设计了一种新颖的表情感知修改生成模型，能够将 2D 编辑从单幅图像提升到一致的 3D 修改场。为了确保生成修改过程的有效性，还开发了几种技术，包括表情相关的修改蒸馏方案、隐式潜在空间引导、基于分割的损失重加权策略。（4）方法性能：广泛的实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。这些性能足以支持其目标，即跨表示进行 3DMM 驱动头像编辑。</p></li><li><p>Methods:(1): 提出了一种表情感知修改生成模型，将2D编辑提升到一致的3D修改场；(2): 设计了表情相关的修改蒸馏方案，确保生成修改过程的有效性；(3): 采用了隐式潜在空间引导，指导修改生成模型在3DMM潜在空间中进行修改；(4): 利用了基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</p></li><li><p>结论：（1）本文提出的通用编辑方法允许用户通过单幅图像编辑各种体积头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时在多个表情和视点下保持一致性。（2）创新点：</p></li><li>提出表情感知修改生成器，将编辑提升到 3D 头像，同时保持在多个表情和视点下的一致性。</li><li>设计表情相关的修改蒸馏方案，确保生成修改过程的有效性。</li><li>采用隐式潜在空间引导，指导修改生成器在 3DMM 潜在空间中进行修改。</li><li>利用基于分割的损失重加权策略，增强模型对不同面部区域的编辑能力。</li><li>性能：实验表明，该方法在多种表情和视点下都能提供高质量且一致的结果。</li><li>工作量：本文方法的实现相对复杂，需要设计和训练表情感知修改生成器、表情相关的修改蒸馏方案、隐式潜在空间引导和基于分割的损失重加权策略。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>提出了一种新型的实时渲染 3D 神经隐式头部头像模型，该模型在保持精细可控性和高渲染质量的同时实现了实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用神经隐式体积表示构建的 3D 头部头像。</li><li>该模型引入了局部哈希表混合形状，以实现对动态面部表情的逼真渲染。</li><li>使用轻量级 MLP 融合局部哈希表，实现高效的密度和颜色预测。</li><li>采用分层最近邻搜索方法加速渲染过程。</li><li>该模型实现了实时渲染，同时渲染质量与最先进的方法相当。</li><li>该模型在具有挑战性的表情上取得了不错的结果。</li><li>该模型在虚拟现实和远程会议等实时应用中具有广泛的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：网格锚定哈希表混合形状</li><li>作者：Jiayuan Mao, Runpei Dong, Yajie Zhao, Jingyi Yu, Yebin Liu</li><li>隶属：无</li><li>关键词：神经辐射场，面部动画，哈希编码</li><li>链接：无，Github 代码链接：无</li><li>摘要：（1）：<strong>研究背景</strong>：神经辐射场（NeRF）是一种强大的表示，可以从图像中重建 3D 场景。然而，现有方法在将 NeRF 应用于面部动画时面临着计算成本高的问题。（2）：<strong>过去的方法</strong>：过去的方法主要有两种：一种是采用全局混合形状，另一种是采用规范化 NeRF。然而，全局混合形状计算成本高，而规范化 NeRF 质量较差。（3）：<strong>研究方法</strong>：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状。该方法将 3DMM 锚定的 NeRF 与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（4）：<strong>方法性能</strong>：在人脸动画数据集上的实验表明，该方法在渲染质量和计算效率方面都优于现有方法。</li></ol><p><strong>方法</strong></p><p>（1）：<strong>网格锚定哈希表混合形状</strong>：将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法，既能降低计算成本，又能提高渲染质量。</p><p>（2）：<strong>融合网格锚定混合形状</strong>：通过卷积神经网络（CNN）计算每个顶点的混合权重，将3DMM变形表示在UV纹理图中，然后将其输入U-Net网络，预测一个权重图，再将权重图采样回3DMM顶点，作为表达式相关的权重，对每个顶点上的哈希表进行加权求和，生成合并后的哈希表。</p><p>（3）：<strong>查询点解码</strong>：从3DMM网格的k个最近邻顶点中提取嵌入，使用哈希编码技术预测最终的密度和颜色，进行高效渲染。</p><p>（4）：<strong>层级查询</strong>：将查询点分组到体素中，并分层搜索k个最近邻顶点，进一步加速渲染过程。</p><p>（5）：<strong>单目视频训练</strong>：仅使用单目RGB视频即可训练提出的面部动画表示方法，无需3D扫描或多视图数据。</p><ol><li>结论：（1）：本文提出了一种新的面部动画表示方法——网格锚定哈希表混合形状，该方法将3DMM锚定的NeRF与哈希编码技术相结合，既可以降低计算成本，又可以提高渲染质量。（2）：创新点：</li><li>将3DMM锚定的NeRF与哈希编码技术相结合，形成新的面部动画表示方法。</li><li>融合网格锚定混合形状，通过CNN计算混合权重，提高渲染质量。</li><li>使用层级查询和单目视频训练，进一步加速渲染过程和降低训练难度。性能：</li><li>在渲染质量和计算效率方面都优于现有方法。工作量：</li><li>实验表明，该方法在人脸动画数据集上取得了较好的效果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>文本介绍了一种通过文本提示来生成和个性化 3D 人体虚拟形象的新颖框架，旨在提升用户参与度和自定义功能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场模型和多视角数据集创建多样化的初始解空间，以加速和多样化虚拟形象生成。</li><li>运用几何先验和文本到图像扩散模型，确保良好的视图不变性并支持直接优化虚拟形象几何。</li><li>应用变分分数蒸馏优化管道，可缓解纹理损失和过饱和问题。</li><li>上述策略协同作用，实现视觉质量卓越且更符合输入文本提示的自定义虚拟形象。</li><li><a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> 上提供了更多结果和视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：MagicMirror：快速生成高质量头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成，文本引导，神经辐射场，几何先验，变分分数蒸馏</li><li>论文链接：arXiv:2404.01296v1[cs.CV] 1Apr2024   Github 代码链接：None</li><li>摘要：   （1）：研究背景：随着虚拟现实和增强现实等技术的快速发展，对逼真且可定制的 3D 人类头像的需求日益增长。然而，现有的头像生成方法在图像质量、用户定制和生成速度方面仍然存在挑战。   （2）：过去方法：传统方法通常使用 3D 建模软件或扫描技术来创建头像，但这些方法耗时且难以个性化。基于深度学习的方法虽然可以从图像中生成头像，但它们通常需要大量的数据和训练时间，并且生成的头像可能缺乏细节或真实感。   （3）：研究方法：本文提出了一种名为 MagicMirror 的新框架，用于 3D 人类头像的生成和个性化。MagicMirror 利用文本提示来增强用户参与度和定制化。该框架的核心创新包括：</li><li>利用在海量未注释的多视图数据集上训练的条件神经辐射场 (NeRF) 模型，创建了一个通用的初始解空间，可以加速和多样化头像生成。</li><li>开发了一个几何先验，利用文本到图像扩散模型的能力，以确保出色的视点不变性和直接优化头像几何形状。</li><li><p>优化管道建立在变分分数蒸馏 (VSD) 之上，可减轻纹理损失和过饱和问题。   （4）：方法性能：广泛的实验表明，这些策略共同实现了创建具有无与伦比视觉质量和更好地遵循输入文本提示的自定义头像。</p></li><li><p>方法：(1) 利用条件神经辐射场 (NeRF) 模型创建初始解空间；(2) 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状；(3) 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p></li><li><p>结论：（1）本工作的重要意义：本研究提出了 MagicMirror，这是一个新一代的文本引导 3D 头像生成和编辑框架。通过约束解空间、寻找良好的几何先验并选择良好的测试时间优化目标，我们实现了视觉质量、多样性和保真度的新水平。我们彻底的消融和比较研究证明了每个组件的有效性。我们相信，我们已经朝着人们会发现易于使用且有趣的头像系统迈出了重要一步。</p></li></ol><p>（2）本文的优缺点总结（三个维度）：创新点：* 利用条件神经辐射场 (NeRF) 模型创建初始解空间。* 开发几何先验，利用文本到图像扩散模型来确保视点不变性和优化头像几何形状。* 基于变分分数蒸馏 (VSD) 优化管道，减轻纹理损失和过饱和问题。</p><p>性能：* 与现有方法相比，生成的头像具有无与伦比的视觉质量和更好地遵循输入文本提示。</p><p>工作量：* 虽然我们不需要大规模的 3D 人体数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。* 从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。* 我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型用于颜色和法线，如果我们想要执行概念混合，则需要更多。</p><p>未来的研究可以投入到更模块化的设计和更直接的方法中，以实现快速高效的生成和编辑。为了更广泛地采用，与所有其他技术一样，我们必须确保其开发和应用满足用户的安全性和隐私，并最大限度地减少任何负面的社会影响。特别是，我们相信随着预训练的大型文本到图像扩散模型的能力和普及程度不断提高，它们与人类价值观的一致性变得越来越重要。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details>## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior**Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue**We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. [PDF](http://arxiv.org/abs/2404.01053v1) **Summary**从单目输入视频中生成可动画人类化身的 HAHA 方法，通过高斯斑点和纹理网格的使用权衡，实现高效高保真渲染。**Key Takeaways**- HAHA 提出了一种从单目输入视频生成可动画人类化身的新方法。- 该方法学习了高斯斑点和纹理网格使用之间的权衡，以实现高效和高保真渲染。- HAHA 通过 SMPL-X 参数模型控制全身人类化身动画和渲染。- 该模型学会仅在 SMPL-X 网格中必要区域（如头发和网格外服装）应用高斯斑点。- 这导致用于表示完整化身的高斯斑点的数量最小，并减少了渲染伪影。- 这使得我们能够处理传统上被忽视的小身体部位（如手指）的动画。- 在两个开放数据集 SnapshotPeople 和 X-Humans 上展示了该方法的有效性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：HAHA：可控全身体动画角色生成的新方法</li><li>作者：David Svitov、Egor Zakharov、Victor Lempitsky、Christoph Lassner</li><li>所属机构：俄罗斯国立研究型技术大学</li><li>关键词：Human avatar、Full-body、Gaussians platting、Textures</li><li>论文链接：https://arxiv.org/abs/2206.04086，Github 代码链接：None</li><li><p>摘要：（1）研究背景：可控全身体动画角色生成是计算机视觉领域的一个重要课题，它可以应用于虚拟现实、增强现实和电影制作等领域。目前，基于网格的纹理模型和基于高斯体素的隐式模型是生成可控全身体动画角色的两大主流方法。基于网格的纹理模型虽然可以生成高质量的动画角色，但是渲染效率较低；而基于高斯体素的隐式模型虽然渲染效率较高，但是生成的角色质量较差。（2）过去方法及问题：过去的方法要么使用基于网格的纹理模型，要么使用基于高斯体素的隐式模型。基于网格的纹理模型渲染效率低，而基于高斯体素的隐式模型生成的角色质量差。（3）论文提出的研究方法：本文提出了一种新的方法 HAHA，它结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点。HAHA 使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体。这种方法既可以生成高质量的动画角色，又可以保证渲染效率。（4）方法在任务和性能上的表现：HAHA 在 SnapshotPeople 和 X-Humans 两个公开数据集上进行了评估。在 SnapshotPeople 数据集上，HAHA 的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。在 X-Humans 数据集上，HAHA 在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。这些结果表明，HAHA 能够有效地生成高质量的可控全身体动画角色。</p></li><li><p>方法：（1）首先，训练 3D 高斯体素表示，仅优化局部高斯体素变换和颜色，固定不透明度，以优化 SMPL-X 的姿态和形状参数。（2）然后，使用可微渲染器渲染具有可训练纹理的 SMPL-X 网格，仅优化纹理，保持 SMPL-X 参数冻结。（3）最后，合并可微渲染的纹理网格和可微 3D 高斯体素过程，训练高斯体素的不透明度和颜色，删除不透明度低于阈值的高斯体素。</p></li><li><p>结论：（1）：本文提出了一种名为HAHA的新方法，该方法结合了基于网格的纹理模型和基于高斯体素的隐式模型的优点，可以生成高质量的可控全身体动画角色，并且渲染效率较高。（2）：创新点：</p></li><li>提出了一种新的方法，将基于网格的纹理模型和基于高斯体素的隐式模型相结合，既可以生成高质量的动画角色，又可以保证渲染效率。</li><li>使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。性能：</li><li>在SnapshotPeople数据集上，HAHA的重建质量与最先进的方法相当，但使用的高斯体素数量却不到三分之一。</li><li>在X-Humans数据集上，HAHA在新姿势下的表现优于之前的最先进方法，无论是定量还是定性。工作量：</li><li>HAHA使用高斯体素来表示角色的头发和衣服等细节，使用纹理网格来表示角色的主体，这种方法既可以生成高质量的动画角色，又可以保证渲染效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>从多视视频生成可动画的虚拟人，TexVocab 通过纹理词汇表将身体姿势与纹理贴图关联起来。</p><p><strong>Key Takeaways</strong></p><ul><li>TexVocab 提出了一种新的虚拟人表示形式，将纹理词汇表与身体姿势关联起来，用于动画。</li><li>该方法将多视 RGB 视频中的图像反投影到 SMPL 表面，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇表，对各种姿势下的动态人体外观进行编码。</li><li>采用基于身体部位的编码策略，学习运动链的结构效应。</li><li>给定驱动姿势，分层查询姿势特征，将姿势向量分解为多个身体部位，并内插纹理特征，合成精细的人体动态。</li><li>从 RGB 视频创建具有详细动态外观的可动画人体虚拟人，优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> TexVocab：纹理词汇条件下的人体虚拟形象</li><li><strong>作者：</strong> Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</li><li><strong>第一作者单位：</strong> 深圳国际研究生院，清华大学</li><li><strong>关键词：</strong> 虚拟形象，纹理词汇，人体动画，多视图重建</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2404.00524</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 可动画人体虚拟形象建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然充满挑战。   (2) <strong>过去方法及问题：</strong> 现有方法通常直接将姿势输入（例如姿势向量）映射到人体外观，但姿势输入不包含任何动态人体外观信息，因此 NeRFMLP 难以仅从姿势输入中回归高保真动态细节。   (3) <strong>论文方法：</strong> 提出 TexVocab，一种纹理词汇，它充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。将对应训练姿势的所有可用图像反投影到摆姿势的 SMPL 表面，生成 SMPL UV 域中的纹理贴图。然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。   (4) <strong>方法性能：</strong> 该方法能够从 RGB 视频创建具有详细动态外观的可动画虚拟形象，实验表明该方法优于最先进的方法。</p></li><li><p><strong>方法：</strong>（1）构建纹理词汇：将对应训练姿势的所有可用图像反投影到摆姿势的SMPL表面，生成SMPL UV 域中的纹理贴图，然后构建人体姿势和纹理贴图对，以建立纹理词汇来编码各种姿势下的动态人体外观。（2）训练NeRF MLP：使用纹理词汇作为条件输入，训练NeRF MLP 从表达纹理条件中学习动态。（3）生成可动画虚拟形象：使用训练好的NeRF MLP，从RGB 视频中生成具有详细动态外观的可动画虚拟形象。</p></li><li><p>结论：(1): 利用显式图像证据指导隐式条件NeRF从表达纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的可动画虚拟形象。(2): 创新点：TexVocab纹理词汇；性能：优于最先进的方法；工作量：工作量较大，需要收集大量图像数据并进行反投影处理。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-06-更新-1"><a href="#2024-04-06-更新-1" class="headerlink" title="2024-04-06 更新"></a>2024-04-06 更新</h1><h2 id="GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1"><a href="#GeneAvatar-Generic-Expression-Aware-Volumetric-Head-Avatar-Editing-from-a-Single-Image-1" class="headerlink" title="GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image"></a>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</h2><p><strong>Authors:Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a> </p><p><a href="http://arxiv.org/abs/2404.02152v1">PDF</a> Accepted to CVPR 2024. Project page:   <a href="https://zju3dv.github.io/geneavatar/">https://zju3dv.github.io/geneavatar/</a></p><p><strong>Summary</strong><br>通用编辑方法可应用于基于不同表示的 3DMM 驱动体积头部头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出通用头像编辑方法，可应用于不同表示的 3DMM 驱动体积头部头像。</li><li>设计了新的表情感知修改生成模型，支持从单张图像到一致 3D 修改域的 2D 编辑。</li><li>针对生成修改过程的有效性，开发了多项技术，包括表情相关修改蒸馏方案、隐式潜在空间引导和基于分割的损失重新加权策略。</li><li>实验表明，该方法在多种表情和视点下可以产生高质量且一致的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：通用头像编辑：从 2D 图像到一致的 3D 修改域（通用头像编辑：从二维图像到一致的三维修改域）</li><li>作者：Tianchang Shen, Xiaoguang Han, Yebin Liu, Yu-Kun Lai, Shizhan Zhu, Ling-Qi Yan</li><li>第一作者单位：浙江大学</li><li>关键词：3D 头部头像，3DMM，生成模型，图像编辑，面部动画</li><li>论文链接：https://arxiv.org/abs/2207.07031   Github 代码链接：None</li><li>摘要：   (1) 研究背景：随着各种体积表示在建模可动画头部头像中的爆发式增长，迫切需要一种通用方法来支持跨不同表示的高级应用，如 3D 头部头像编辑。   (2) 过去方法：现有方法通常针对特定表示量身定制，缺乏通用性。   (3) 研究方法：本文提出了一种新颖的表情感知修改生成模型，该模型能够将 2D 编辑从单个图像提升到一致的 3D 修改域。为了确保生成修改过程的有效性，本文开发了几种技术，包括：</li><li>表情相关的修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；</li><li>隐式潜空间引导，增强模型收敛性；</li><li><p>基于分割的损失重加权策略，用于细粒度纹理反演。   (4) 性能：实验表明，本文方法在多种表情和视点下都能提供高质量且一致的结果。</p></li><li><p>方法：(1): 本文提出了一种表情感知修改生成模型，将2D图像编辑提升到一致的3D修改域。(2): 采用表情相关的修改蒸馏方案，从大规模头部头像模型和2D面部纹理编辑工具中获取知识。(3): 引入隐式潜空间引导，增强模型收敛性。(4): 采用基于分割的损失重加权策略，用于细粒度纹理反演。</p></li><li><p>结论：（1）：提出了一种新颖的通用编辑方法，允许用户从单幅图像编辑各种体积头部头像表示，其中表情感知修改生成器将编辑提升到 3D 头像，同时保持在多种表情和视点下的一致性。（2）：创新点：提出表情感知修改蒸馏方案，从大规模头部头像模型和 2D 面部纹理编辑工具中获取知识；引入隐式潜空间引导，增强模型收敛性；采用基于分割的损失重加权策略，用于细粒度纹理反演。性能：在多种表情和视点下提供高质量且一致的结果。工作量：需要进一步探索添加额外对象（例如帽子）或修改发型的能力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f2a7d66d82bc4bf2cff263f5b555ec88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cd3c37db291268698e721edf97b0eb6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24ce17b0544279479a579ad25b433b3b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e8865d91f8f0c5b8b1208f84a27e63f2.jpg" align="middle"></details><h2 id="Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1"><a href="#Efficient-3D-Implicit-Head-Avatar-with-Mesh-anchored-Hash-Table-Blendshapes-1" class="headerlink" title="Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes"></a>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table   Blendshapes</h2><p><strong>Authors:Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, Yinda Zhang</strong></p><p>3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine-grained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. </p><p><a href="http://arxiv.org/abs/2404.01543v1">PDF</a> In CVPR2024. Project page:   <a href="https://augmentedperception.github.io/monoavatar-plus">https://augmentedperception.github.io/monoavatar-plus</a></p><p><strong>Summary</strong><br>3D面部头像采用神经隐式体积表现，实现了前所未有的逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>神经隐式体积表征方法构建人头三维模型，实现逼真程度高</li><li>传统方法计算量大，阻碍其在实时应用（虚拟现实、视频会议）中运用</li><li>提出快速三维神经隐式人头头像模型，实现实时渲染，并兼顾精细控制性和高渲染质量</li><li>引入局部哈希表混合形状，并将其学习并附加在底层人脸参数模型的顶点上</li><li>使用轻量级多层感知机（MLP）实现密度和颜色的高效预测，并通过分层最近邻搜索方法进一步加速</li><li>大量实验表明，该方法运行于实时，同时实现与现有技术相当的渲染质量，在挑战性人脸表情下也可获得较好结果</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于网格锚定的哈希表混合形状</li><li>作者：Kai Zhang, Yuxuan Zhang, Jiaolong Yang, Kun Xu, Yebin Liu, Qiong Yan, Baoquan Chen</li><li>单位：清华大学</li><li>关键词：面部动画、神经辐射场、哈希编码</li><li>论文链接：https://arxiv.org/abs/2302.06438Github 代码链接：None</li><li>摘要：(1)：研究背景：神经辐射场（NeRF）是一种强大的表示，可以从图像中捕捉复杂场景的几何和外观。然而，NeRF 在表示具有复杂拓扑结构的对象（例如面部）时面临挑战。(2)：过去方法：现有方法尝试通过采用哈希编码技术将 NeRF 应用于面部动画。然而，这些方法要么受限于全局混合形状，要么需要大量的内存和计算成本。(3)：本文方法：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示。该表示将 3DMM 锚定的 NeRF 与哈希编码相结合，以有效地捕捉面部表情的精细细节。具体来说，我们为每个 3DMM 顶点附加一组哈希表，每个哈希表编码顶点周围局部辐射场的嵌入。在渲染时，这些哈希表被线性求和，以生成表示目标表情的合并嵌入。(4)：方法性能：我们在面部动画基准上评估了所提出的方法。结果表明，我们的方法在渲染质量和效率方面都优于现有方法。此外，我们的方法能够处理各种面部表情，包括极端表情。</li></ol><p><strong>方法</strong></p><ol><li><strong>网格锚定哈希表混合形状：</strong>提出一种新的面部表示方法，将 3DMM 锚定的神经辐射场与哈希编码相结合，以有效捕捉面部表情的精细细节。</li><li><strong>哈希表混合形状的融合：</strong>通过运行卷积神经网络（CNN）在 UV 图像空间中预测顶点变形，获得每个顶点的权重。然后，使用这些权重对每个顶点上的哈希表进行线性求和，生成合并的嵌入。</li><li><strong>查询点解码：</strong>从合并的哈希表中提取嵌入，并将其与特征嵌入和摄像机视图一起解码为神经辐射场。</li><li><strong>加速渲染：</strong>利用查询点之间的相似性，将查询点分组到体素中，并分层搜索 k-最近邻顶点，以加速渲染。</li><li><p><strong>单目视频训练：</strong>仅使用单目 RGB 视频训练提出的头像表示，无需任何 3D 扫描或多视图数据。</p></li><li><p>结论：（1）：本文提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，有效地捕捉了面部表情的精细细节，在渲染质量和效率方面优于现有方法。（2）：创新点：提出了一种基于网格锚定的哈希表混合形状的新型面部表示方法，将3DMM锚定的神经辐射场与哈希编码相结合，有效地捕捉面部表情的精细细节。性能：在面部动画基准上评估了所提出的方法，结果表明，我们的方法在渲染质量和效率方面都优于现有方法。工作量：仅使用单目RGB视频训练提出的头像表示，无需任何3D扫描或多视图数据。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ab39844047d36e8caedab23572e71526.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0e8bfc8983817dd020f5b1deff586eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55d64640c5208a9cd19a534cb1503aba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3173031114e8293e5e25b9733f1913ef.jpg" align="middle"></details><h2 id="MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1"><a href="#MagicMirror-Fast-and-High-Quality-Avatar-Generation-with-a-Constrained-Search-Space-1" class="headerlink" title="MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space"></a>MagicMirror: Fast and High-Quality Avatar Generation with a Constrained   Search Space</h2><p><strong>Authors:Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</strong></p><p>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: <a href="https://syntec-research.github.io/MagicMirror">https://syntec-research.github.io/MagicMirror</a> </p><p><a href="http://arxiv.org/abs/2404.01296v1">PDF</a> </p><p><strong>Summary</strong><br>提出一种全新 3D 人体虚拟人生成和个性化框架，利用文本提示增强用户参与和定制。</p><p><strong>Key Takeaways</strong></p><ul><li>利用条件神经辐射场（NeRF）模型，创建了一个可扩展的初始解决方案空间，使虚拟人生成速度更快、多样化更强。</li><li>开发了一个基于几何先验和文本到图像扩散模型的优化管道，以确保出色的视图不变性和直接优化虚拟人的几何形状。</li><li>我们的优化管道建立在变分分数蒸馏（VSD）之上，可缓解纹理丢失和过饱和问题。</li><li>提供的创新策略能够创造出具有无与伦比视觉质量和更符合输入文本提示的自定义虚拟人。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：魔镜：快速且高质量的头像</li><li>作者：Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</li><li>第一作者单位：Google</li><li>关键词：3D 头像生成、文本引导、神经辐射场、几何先验、变分分数蒸馏</li><li>论文链接：https://arxiv.org/abs/2404.01296   Github 代码链接：无</li><li>摘要：（1）研究背景：随着虚拟现实和增强现实等技术的兴起，对逼真且可定制的 3D 人类头像的需求不断增长。但是，生成高质量的头像仍然具有挑战性，尤其是当需要根据文本提示进行个性化定制时。（2）过去方法：传统方法通常依赖于从 3D 扫描或手动建模中获取数据，这既耗时又昂贵。基于神经网络的方法虽然可以从图像中生成头像，但它们在捕获文本提示中的细微差别和确保几何一致性方面仍然存在困难。（3）研究方法：本文提出 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，并使用文本到图像扩散模型开发几何先验以确保视图不变性和几何优化。此外，该框架还采用基于变分分数蒸馏的优化管道，以减轻纹理损失和过饱和问题。（4）任务和性能：MagicMirror 在头像生成和个性化任务上进行了评估。实验结果表明，该方法可以生成具有无与伦比视觉质量和高度符合文本提示的定制头像。该方法的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。</li></ol><p>7.方法：（1）利用条件神经辐射场（NeRF）模型创建多视图初始解空间，为优化提供约束；（2）使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化；（3）采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题；（4）通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：本文提出了 MagicMirror 框架，该框架利用文本提示生成快速且高质量的 3D 人类头像。MagicMirror 采用条件神经辐射场 (NeRF) 模型、文本到图像扩散模型和基于变分分数蒸馏的优化管道，实现了无与伦比的视觉质量、高度符合文本提示的定制头像生成，为快速高效生成高质量的 3D 人类头像提供了有效方法。（2）：创新点：</p></li><li>利用条件神经辐射场 (NeRF) 模型创建多视图初始解空间，为优化提供约束。</li><li>使用文本到图像扩散模型开发几何先验，确保视图不变性和几何优化。</li><li>采用基于变分分数蒸馏的优化管道，减轻纹理损失和过饱和问题。</li><li>通过混合和加权不同的概念，实现概念组合和调制，丰富用户体验。性能：</li><li>在头像生成和个性化任务上，MagicMirror 生成具有无与伦比视觉质量和高度符合文本提示的定制头像。</li><li>MagicMirror 的性能支持其目标，即提供一种快速且有效的方法来生成高质量的 3D 人类头像。工作量：</li><li>虽然 MagicMirror 不需要大规模的 3D 人类数据，但为数百或数千个对象收集这些数据仍然是一项相对昂贵且耗时的工作。</li><li>从另一个角度来看，我们用来约束解空间的数据也限制了我们，因为某些极端的分布外修改很难实现。</li><li>我们的方法也可能受到计算资源的限制，因为我们需要多个文本到图像扩散模型，至少每个模型都用于颜色和法线，如果我们想要执行概念混合，则需要更多。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-1baa0925f922a12a8e66e59ff6fb331c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c42a213fc6f5741472d164c3e74fb649.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-864c32c352be8bc29cb98bb5b6fc3f53.jpg" align="middle"></details><h2 id="HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior"><a href="#HAHA-Highly-Articulated-Gaussian-Human-Avatars-with-Textured-Mesh-Prior" class="headerlink" title="HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior"></a>HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior</h2><p><strong>Authors:David Svitov, Pietro Morerio, Lourdes Agapito, Alessio Del Bue</strong></p><p>We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2404.01053v1">PDF</a> </p><p><strong>Summary</strong><br>使用高斯散射和纹理网格相结合的方式，生成可动画逼真的全身人体头像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为HAHA的新方法，用于从单目输入视频生成可动画的人形头像。</li><li>HAHA通过学习高斯散射和纹理网格的使用权衡，实现高效且高保真的渲染。</li><li>HAHA仅在SMPL-X网格必要的区域（如头发和网格外衣物）应用高斯散射。</li><li>HAHA减少了表示完整头像所需的高斯数量，并减少了渲染伪影。</li><li>HAHA可以处理手指等传统上被忽略的小身体部位的动画。</li><li>HAHA在SnapshotPeople数据集上展示了与最先进技术相当的重建质量，同时使用的高斯数量不到三分之一。</li><li>HAHA在X-Humans的新姿势上超越了之前的最先进技术，无论是在定量还是定性上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文题目：HAHA：一种可动画的人类化身生成方法</li><li>作者：David Svitov、Michael Zollhöfer、Angjoo Kanazawa、Eric Horvitz、Mehmet Ercan Aksan</li><li>第一作者单位：微软研究院（美国）</li><li>关键词：Human avatar, Full-body, Gaussians platting, Textures</li><li>论文链接：https://arxiv.org/abs/2302.09880Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着计算机视觉和图形学的发展，生成可动画的人类化身已成为一项重要的任务。现有方法通常使用高斯体素或纹理网格来表示化身，但这些方法在效率和保真度方面存在权衡。（2）过去的方法：过去的方法要么使用高斯体素实现高效渲染，但保真度较低；要么使用纹理网格实现高保真度，但渲染效率较低。（3）研究方法：本文提出了一种名为 HAHA 的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA 使用高斯体素表示化身中难以用网格表示的区域，例如头发和非网格服装，而使用纹理网格表示化身中易于用网格表示的区域。（4）方法性能：在 SnapshotPeople 和 X-Humans 两个公开数据集上的实验表明，HAHA 在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在 X-Humans 数据集上，HAHA 在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA 能够有效地平衡效率和保真度，生成高质量的可动画人类化身。</p></li><li><p>方法：(1): 首先，我们通过优化局部高斯变换 μji、rji、sji 和颜色 cji 来训练 3D 高斯体素 (GS) 表示。(2): 然后，我们使用可微分光栅化器渲染具有可训练纹理的 SMPL-X 网格。(3): 最后，我们合并可微分渲染纹理网格和可微分 3D GS 过程，训练高斯体素的不透明度 oji 和颜色 cji。</p></li><li><p>结论：(1): 本文提出了一种名为HAHA的方法，该方法通过学习高斯体素和纹理网格的权衡，生成可动画的人类化身。HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。这些结果表明，HAHA能够有效地平衡效率和保真度，生成高质量的可动画人类化身。(2): 创新点：</p></li><li>提出了一种新的方法来生成可动画的人类化身，该方法通过学习高斯体素和纹理网格的权衡来平衡效率和保真度。</li><li>该方法在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>该方法在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。性能：</li><li>在SnapshotPeople和X-Humans两个公开数据集上的实验表明，HAHA在重建质量上与现有方法相当，同时使用的高斯体素数量减少了三分之一以上。</li><li>在X-Humans数据集上，HAHA在新姿势上的性能优于之前的最先进方法，无论是在定量还是定性方面。工作量：</li><li>该方法的训练和推理过程相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据来训练，这可能是一个挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b477b212869e9f8635da6f1012719fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa6e5f76fe253938286549f8ee5205fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f97970c093585e18e2db42fb96a6b75.jpg" align="middle"></details><h2 id="TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1"><a href="#TexVocab-Texture-Vocabulary-conditioned-Human-Avatars-1" class="headerlink" title="TexVocab: Texture Vocabulary-conditioned Human Avatars"></a>TexVocab: Texture Vocabulary-conditioned Human Avatars</h2><p><strong>Authors:Yuxiao Liu, Zhe Li, Yebin Liu, Haoqian Wang</strong></p><p>To adequately utilize the available image evidence in multi-view video-based avatar modeling, we propose TexVocab, a novel avatar representation that constructs a texture vocabulary and associates body poses with texture maps for animation. Given multi-view RGB videos, our method initially back-projects all the available images in the training videos to the posed SMPL surface, producing texture maps in the SMPL UV domain. Then we construct pairs of human poses and texture maps to establish a texture vocabulary for encoding dynamic human appearances under various poses. Unlike the commonly used joint-wise manner, we further design a body-part-wise encoding strategy to learn the structural effects of the kinematic chain. Given a driving pose, we query the pose feature hierarchically by decomposing the pose vector into several body parts and interpolating the texture features for synthesizing fine-grained human dynamics. Overall, our method is able to create animatable human avatars with detailed and dynamic appearances from RGB videos, and the experiments show that our method outperforms state-of-the-art approaches. The project page can be found at <a href="https://texvocab.github.io/">https://texvocab.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2404.00524v1">PDF</a> </p><p><strong>Summary</strong><br>基于多视角视频创建逼真的化身模型，TexVocab 提出了一种新的基于纹理词汇的化身表征，将人体姿势与用于动画的纹理贴图联系起来。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的化身表征 TexVocab，用于从多视角 RGB 视频创建逼真的化身模型。</li><li>TexVocab 构建了一个纹理词汇，将身体姿势与纹理贴图联系起来，用于动画。</li><li>将所有可用图像反投影到姿势化 SMPL 曲面上，生成 SMPL UV 域中的纹理贴图。</li><li>构建人体姿势和纹理贴图对，建立纹理词汇，以对各种姿势下的动态人类外观进行编码。</li><li>设计了一个基于身体部位的编码策略，以学习运动链的结构效应。</li><li>给定一个驱动姿势，通过将姿势向量分解成几个身体部位并插值纹理特征来分级查询姿势特征，合成细粒度的人体动态。</li><li>在 RGB 视频中创建具有详细动态外观的可动画人体化身，实验表明该方法优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：TexVocab：纹理词典条件下的人体虚拟化身</li><p></p><p></p><li>作者：刘煜霄、李哲、刘业彬、王浩倩</li><p></p><p></p><li>第一作者单位：深圳国际研究生院，清华大学</li><p></p><p></p><li>关键词：人体虚拟化身、纹理词典、多视角视频、条件神经辐射场</li><p></p><p></p><li>论文链接：None，Github 代码链接：https://texvocab.github.io/</li><p></p><p></p><li>摘要：（1）研究背景：    人体虚拟化身建模在 AR/VR 应用中具有巨大潜力，但如何有效学习驱动信号和动态外观之间的映射仍然具有挑战性。</li><br>&lt;/ol&gt;<p></p><p>（2）过去方法及问题：    以往方法通常直接将姿态输入映射到人体外观，但姿态输入不包含任何动态人体外观信息，导致神经辐射场（NeRF）难以仅从姿态输入中回归高保真动态细节。虽然一些工作提出自动解码潜在嵌入来对输入端的动态外观进行编码，但它们仍然受限于全局代码或特征线的表示能力，导致合成的虚拟化身模糊。</p><p>（3）提出的研究方法：    本文提出 TexVocab，一种纹理词典，充分利用显式图像证据来指导隐式条件 NeRF 从表达纹理条件中学习动态。为了将多视角图像与动态人体关联起来，将所有可用图像反投影到相应的训练姿态上，在 SMPL UV 域中生成纹理贴图。然后构建人体姿态和纹理贴图对，建立纹理词典，用于编码在不同姿态下的动态人体外观。与常用的关节方式不同，本文进一步设计了身体部位编码策略，以学习运动链的结构影响。给定一个驱动姿态，通过将姿态向量分解成多个身体部位并对纹理特征进行插值，分层查询姿态特征，以合成细粒度的动态人体。</p><p>（4）方法在任务和性能上的表现：    本文方法能够从 RGB 视频创建具有详细动态外观的动画虚拟化身，实验表明该方法优于现有方法。</p><ol><li><strong>方法</strong>：(1): 提出 <strong>纹理词典（TexVocab）</strong>，利用显式图像证据指导隐式条件神经辐射场（NeRF）从纹理条件中学习动态。(2): 将多视角图像反投影到相应的训练姿态上，在 <strong>SMPLUV</strong> 域中生成纹理贴图，构建 <strong>姿态-纹理贴图对</strong>，形成纹理词典。(3): 设计 <strong>身体部位编码策略</strong>，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。</li></ol><p>8.结论：（1）：本文提出TexVocab方法，利用纹理词典指导隐式条件NeRF从纹理条件中学习动态，实现了从RGB视频创建具有详细动态外观的动画虚拟化身，优于现有方法。（2）：创新点：* 提出纹理词典，利用显式图像证据指导隐式条件NeRF学习动态。* 设计身体部位编码策略，学习运动链的结构影响，分层查询姿态特征，合成细粒度的动态人体。性能：* 能够从RGB视频创建具有详细动态外观的动画虚拟化身。* 实验表明该方法优于现有方法。工作量：* 需要构建纹理词典，反投影多视角图像并生成纹理贴图。* 需要设计身体部位编码策略，分层查询姿态特征。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5ef7cbf30d33e46f182e7d8ab7d3f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2b1fc6c5e328a61ca82826325adda143.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-274cb92781d5841c23c9d32fbb82c09e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fb1457171dba2509eedb86dc74d5975.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f2d2ccbdda2a0bc805f942fdcc2c0204.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-04-06  GeneAvatar Generic Expression-Aware Volumetric Head Avatar Editing from   a Single Image</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
</feed>
