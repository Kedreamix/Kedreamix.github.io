<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-03-28T03:51:36.589Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/</id>
    <published>2024-03-28T03:51:36.000Z</published>
    <updated>2024-03-28T03:51:36.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•£ç‚¹ç®—æ³•ä¸‹çš„å˜åˆ†æ¨ç†ï¼Œæ— ç¼ç»“åˆä¸ç¡®å®šæ€§é¢„æµ‹ï¼Œé€šè¿‡ä¼˜åŒ–æ–°æå‡ºçš„æŸå¤±å‡½æ•°é¡¹ AUSEï¼Œæå‡å›¾åƒé‡å»ºå’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SGS æ˜¯ç¬¬ä¸€ä¸ªç”¨äºé«˜æ–¯æ•£ç‚¹æ³•ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ¡†æ¶ã€‚</li><li>SGS æ˜¾è‘—é™ä½äº†ç¥ç»è¾å°„åœºçš„è®¡ç®—æˆæœ¬ï¼Œä½†ä»¥å‰ç¼ºä¹æä¾›ç½®ä¿¡åº¦ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li><li>SGS åœ¨é«˜æ–¯æ•£ç‚¹æ³•å¸¸è§çš„æ¸²æŸ“ç®¡é“ä¸­æ— ç¼é›†æˆäº†ä¸ç¡®å®šæ€§é¢„æµ‹ã€‚</li><li>å¼•å…¥äº†é¢ç§¯ä¸‹ç¨€ç–åŒ–è¯¯å·® (AUSE) ä½œä¸ºæŸå¤±å‡½æ•°ä¸­çš„æ–°é¡¹ã€‚</li><li>AUSE ä¼˜åŒ–äº†ä¸ç¡®å®šæ€§ä¼°è®¡å’Œå›¾åƒé‡å»ºã€‚</li><li>SGS åœ¨ LLFF æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®åº¦æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SGS æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œæœ‰åŠ©äºåœ¨å®é™…åº”ç”¨ä¸­åšå‡ºæ›´å®‰å…¨çš„å†³ç­–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé«˜æ–¯æº…å°„çš„ä¸ç¡®å®šæ€§å»ºæ¨¡</li><li>ä½œè€…ï¼šLuca Savant, Diego Valsesia, Enrico Magli</li><li>å•ä½ï¼šæ„å¤§åˆ©éƒ½çµç†å·¥å¤§å­¦ç”µå­ä¸ç”µä¿¡ç³»</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€æ–°è§†è§’åˆæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18476</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰è¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»è¾å°„åœºçš„ novel-view synthesis æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ï¼ˆ2ï¼‰é«˜æ–¯æº…å°„ï¼ˆGSï¼‰æŠ€æœ¯ä½œä¸ºä¸€ç§æ›´å…·è®¡ç®—æ•ˆç‡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ä¿æŒé«˜è´¨é‡ novel-view synthesis çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼ŒGS ç¼ºä¹ä¼°è®¡åˆæˆè§†å›¾ä¸­ç½®ä¿¡åº¦çš„èƒ½åŠ›ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äº GS ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸º Stochastic Gaussian Splattingï¼ˆSGSï¼‰ã€‚SGS æ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§ GS æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœè¡¨æ˜ï¼ŒSGS åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›äº†åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ï¼Œç”¨äºåœ¨é«˜æ–¯æº…å°„æ¡†æ¶ä¸­å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šSGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚ï¼ˆ3ï¼‰ï¼šSGSä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•è¿‘ä¼¼åƒç´ é¢œè‰²çš„æ–¹å·®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨ç†æ¡†æ¶è¿›è¡Œå­¦ä¹ ã€‚ï¼ˆ4ï¼‰ï¼šSGSå‡è®¾é«˜æ–¯æ ¸ä¹‹é—´ç‹¬ç«‹ï¼Œå¹¶ä½¿ç”¨é¢ç§¯ä¸‹é”™è¯¯ç¨€ç–åŒ–ï¼ˆAUSEï¼‰åº¦é‡æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ä¸ªç”¨äºé«˜æ–¯æº…å°„æ¡†æ¶çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ï¼Œä»è€Œä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä¿ƒè¿›äº†å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ï¼Œç”¨äºåœ¨é«˜æ–¯æº…å°„æ¡†æ¶ä¸­å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li><li>SGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå…è®¸é¢„æµ‹ä¸ç¡®å®šæ€§å’Œåˆæˆè§†å›¾ã€‚</li><li>SGSä½¿ç”¨è’™ç‰¹å¡ç½—æ–¹æ³•è¿‘ä¼¼åƒç´ é¢œè‰²çš„æ–¹å·®ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨ç†æ¡†æ¶è¿›è¡Œå­¦ä¹ ã€‚</li><li>SGSå‡è®¾é«˜æ–¯æ ¸ä¹‹é—´ç‹¬ç«‹ï¼Œå¹¶ä½¿ç”¨é¢ç§¯ä¸‹é”™è¯¯ç¨€ç–åŒ–ï¼ˆAUSEï¼‰åº¦é‡æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚æ€§èƒ½ï¼š</li><li>SGSåœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>SGSä¸ºä»ä¸šè€…æä¾›äº†åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ã€‚å·¥ä½œé‡ï¼š</li><li>SGSçš„è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚ä½äºç¥ç»è¾å°„åœºæ–¹æ³•ã€‚</li><li>SGSå¯ä»¥åœ¨å®æ—¶åº”ç”¨ä¸­ä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details><h2 id="Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs"><a href="#Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs" class="headerlink" title="Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"></a>Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs</h2><p><strong>Authors:Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael Paulitsch</strong></p><p>This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidiaâ€™s H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidiaâ€™s H100 GPU by up to a factor 19. The code can be found at <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a>. </p><p><a href="http://arxiv.org/abs/2403.17607v1">PDF</a> </p><p><strong>Summary</strong><br>SYCL å®ç°çš„å¤šå±‚æ„ŸçŸ¥å™¨é’ˆå¯¹è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU Max 1550 è¿›è¡Œä¼˜åŒ–ï¼Œå…¶æ€§èƒ½æ¯” CUDA æ›´å¥½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SYCL å®ç°çš„ MLP å‡å°‘äº†æ…¢çš„å…¨å±€å†…å­˜è®¿é—®ï¼Œæœ€å¤§åŒ–äº†å¯„å­˜å™¨æ–‡ä»¶å’Œå…±äº«å±€éƒ¨å†…å­˜ä¸­çš„æ•°æ®é‡ç”¨ã€‚</li><li>èåˆæ¯ä¸€å±‚ MLP ä¸­çš„æ“ä½œï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç®—æœ¯å¼ºåº¦ï¼Œä»è€Œæå‡æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†ä¸­ã€‚</li><li>åœ¨è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU ä¸Šï¼ŒSYCL å®ç°çš„ MLP åœ¨æ¨ç†æ—¶æ¯”è‹±ä¼Ÿè¾¾ H100 GPU ä¸Šçš„ CUDA å®ç°å¿« 2.84 å€ï¼Œåœ¨è®­ç»ƒæ—¶å¿« 1.75 å€ã€‚</li><li>SYCL å®ç°å±•ç¤ºäº†åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ–¹é¢çš„æ•ˆç‡ã€‚</li><li>SYCL å®ç°æ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) åœ¨åŒä¸€è‹±ç‰¹å°” GPU ä¸Šçš„æ€§èƒ½é«˜å‡º 30 å€ï¼Œæ¯”è‹±ä¼Ÿè¾¾ H100 GPU ä¸Šçš„ CUDA PyTorch é«˜å‡º 19 å€ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a> æ‰¾åˆ°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU ä¸Šçš„å…¨èåˆå¤šå±‚æ„ŸçŸ¥å™¨</li><li>ä½œè€…ï¼šKai Yuanâ€ ã€Christoph Bauingerâ€ ã€Xiangyi Zhangâ€ ã€Pascal Baehrâ€ ã€Matthias Kirchhartâ€ ã€Darius Dabertâ€¡ã€Adrien Tousnakhoffâ€¡ã€Pierre Boudierâ€  å’Œ Michael Paulitschâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±ç‰¹å°”å…¬å¸</li><li>å…³é”®è¯ï¼šæœºå™¨å­¦ä¹ ã€æ€§èƒ½ä¼˜åŒ–ã€SYCLã€è‹±ç‰¹å°”æ•°æ®ä¸­å¿ƒ GPU Max1550</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2305.01723   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/intel/tiny-dpcpp-nn</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¤šå±‚æ„ŸçŸ¥å™¨ (MLP) åœ¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å…¶æ€§èƒ½å—åˆ°ä½ç®—æœ¯å¼ºåº¦å’Œå†…å­˜å¸¦å®½çš„é™åˆ¶ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç»å…¸çš„ MLP å®ç°æ–¹æ³•å°†æ¯å±‚æ“ä½œæ”¾åœ¨å•ç‹¬çš„è®¡ç®—å†…æ ¸ä¸­ï¼Œå¯¼è‡´é¢‘ç¹çš„å…¨å±€å†…å­˜è®¿é—®ï¼Œé™ä½äº†æ€§èƒ½ã€‚å…¨èåˆ MLP ç­–ç•¥é€šè¿‡èåˆå±‚æ¥å‡å°‘å…¨å±€å†…å­˜è®¿é—®ï¼Œä½†ç°æœ‰å®ç°ä»…é’ˆå¯¹ Nvidia GPUã€‚   ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œåˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•æ¥æœ€å¤§åŒ–æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ã€‚   ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) å’Œ Nvidia H100 GPU ä¸Šçš„ CUDA PyTorch ç‰ˆæœ¬åˆ†åˆ«å¿« 30 å€å’Œ 19 å€ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†è¯¥æ–¹æ³•åœ¨æé«˜ MLP è®­ç»ƒå’Œæ¨ç†æ€§èƒ½æ–¹é¢çš„ç›®æ ‡ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š(1): æœ¬ç ”ç©¶æå‡ºäº†é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œé€šè¿‡åˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•ï¼Œæœ€å¤§åŒ–äº†æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ï¼Œåœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæé«˜ MLP è®­ç»ƒå’Œæ¨ç†æ€§èƒ½æä¾›äº†æ”¯æŒã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>é’ˆå¯¹è‹±ç‰¹å°” GPU çš„å…¨èåˆ MLP SYCL å®ç°ï¼Œåˆ©ç”¨ XMX ç¡¬ä»¶å’Œè”åˆçŸ©é˜µ SYCL æ‰©å±•ï¼Œæœ€å¤§åŒ–äº†æ•°æ®é‡ç”¨å’Œç®—æœ¯å¼ºåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¸ƒå±€å’Œè®¡ç®—å†…æ ¸ï¼Œå‡å°‘äº†å…¨å±€å†…å­˜è®¿é—®ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li><li>æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIï¼Œç®€åŒ–äº†å…¨èåˆ MLP çš„å¼€å‘å’Œéƒ¨ç½²ã€‚</li><li>åœ¨å›¾åƒå‹ç¼©ã€ç¥ç»è¾å°„åœºå’Œç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ç­‰ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>æ¯”è‹±ç‰¹å°” PyTorch æ‰©å±• (IPEX) å¿« 30 å€ã€‚</li><li>æ¯” Nvidia H100 GPU ä¸Šçš„ CUDA PyTorch ç‰ˆæœ¬å¿« 19 å€ã€‚</li><li>åœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹å¤§å°ä¸Šéƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å·¥ä½œè´Ÿè½½ï¼š</li><li>å›¾åƒå‹ç¼©ã€‚</li><li>ç¥ç»è¾å°„åœºã€‚</li><li>ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9d6acfd57665b2b20700c20b0f86947a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-284e647f61419e6b46579a91f8f23f63.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d159ec4843c63e8f3d2a984787be4626.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a4b46a392670a516f67cab259e4deea.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e496dd42daccf1e136ab642f271da7b.jpg" align="middle"></details><h2 id="NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation"><a href="#NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation" class="headerlink" title="NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation"></a>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation</h2><p><strong>Authors:Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li</strong></p><p>Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely â€œHeuristics-Guided Segmentationâ€ (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: <a href="https://cnhaox.github.io/NeRF-HuGS/">https://cnhaox.github.io/NeRF-HuGS/</a>. </p><p><a href="http://arxiv.org/abs/2403.17537v1">PDF</a> To appear in CVPR2024</p><p><strong>Summary</strong><br>HuGSå·§å¦™ç»“åˆäººå·¥å¯å‘å’Œåˆ†å‰²æ¨¡å‹ï¼Œçªç ´NeRFé™æ€åœºæ™¯é™åˆ¶ï¼Œæœ‰æ•ˆæ¶ˆé™¤åŠ¨æ€å¹²æ‰°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºâ€å¯å‘å¼å¼•å¯¼åˆ†å‰²â€(HuGS)èŒƒå¼ï¼Œåˆ†ç¦»é™æ€åœºæ™¯å’ŒåŠ¨æ€å¹²æ‰°ã€‚</li><li>èåˆSfMå¯å‘å’Œé¢œè‰²æ®‹å·®å¯å‘ï¼Œé€‚åº”çº¹ç†å¤šæ ·æ€§ã€‚</li><li>HuGS åœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„ NeRF ä¸­æœ‰æ•ˆå‡è½»åŠ¨æ€å¹²æ‰°ã€‚</li><li>å®éªŒè¡¨æ˜ HuGS çš„ä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚</li><li>HuGS ä½¿ç”¨äººå·¥å¯å‘å’Œåˆ†å‰²æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰è§£å†³æ–¹æ¡ˆã€‚</li><li>HuGS é€‚ç”¨äºå…·æœ‰ä¸åŒçº¹ç†ç‰¹å¾çš„åœºæ™¯ã€‚</li><li>HuGS åœ¨éé™æ€åœºæ™¯ä¸­æ˜¾ç€æ”¹å–„äº† NeRF çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRF-HuGSï¼šæ”¹è¿›éé™æ€åœºæ™¯ä¸­çš„ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šHao Chen, Yuxuan Zhang, Kangxue Yin, Li Yi, Jiajun Wu</li><li>éš¶å±ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFï¼Œéé™æ€åœºæ™¯ï¼Œè¿åŠ¨ç‰©ä½“ï¼Œé˜´å½±ï¼Œå›¾åƒåˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08268ï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨æ–°è§†è§’åˆæˆå’Œ 3D åœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¾èµ–äºé™æ€åœºæ™¯çš„å‡è®¾ï¼Œåœ¨é‡åˆ°è¿åŠ¨ç‰©ä½“æˆ–é˜´å½±ç­‰ç¬æ€å¹²æ‰°æ—¶å®¹æ˜“äº§ç”Ÿä¸è‰¯ä¼ªå½±ã€‚   (2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šè¿‡è¿åŠ¨ä¼°è®¡ã€æ—¶é—´ä¸€è‡´æ€§æˆ–è¿åŠ¨è¡¥å¿æ¥å¤„ç†ç¬æ€å¹²æ‰°ï¼Œä½†æ•ˆæœæœ‰é™ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ†ç¦»é™æ€åœºæ™¯å’Œç¬æ€å¹²æ‰°ã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§æ–°çš„èŒƒä¾‹â€œå¯å‘å¼å¼•å¯¼åˆ†å‰²â€ï¼ˆHuGSï¼‰ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œæ˜¾è‘—å¢å¼ºäº†ä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒHuGS èåˆäº†åŸºäºç»“æ„ä»è¿åŠ¨ (SfM) çš„å¯å‘å¼å’Œé¢œè‰²æ®‹å·®å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚   (4) å®éªŒç»“æœï¼šåœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„ NeRF ä¸­ï¼ŒHuGS åœ¨å‡è½»ç¬æ€å¹²æ‰°æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚åœ¨ Kubric æ•°æ®é›†ä¸Šï¼ŒHuGS åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº† 0.53 å’Œ 0.03ï¼Œåœ¨ LPIPS æŒ‡æ ‡ä¸Šé™ä½äº† 0.04ã€‚åœ¨ Distractor æ•°æ®é›†ä¸Šï¼ŒHuGS åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº† 0.46 å’Œ 0.02ï¼Œåœ¨ LPIPS æŒ‡æ ‡ä¸Šé™ä½äº† 0.03ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº† HuGS å¢å¼º NeRF åœ¨éé™æ€åœºæ™¯ä¸­è¡¨ç°çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) æå‡ºå¯å‘å¼å¼•å¯¼åˆ†å‰²ï¼ˆHuGSï¼‰èŒƒä¾‹ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œå¢å¼ºä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚(2) èåˆåŸºäºç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰çš„å¯å‘å¼å’Œé¢œè‰²æ®‹å·®å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚(3) å°†HuGSåº”ç”¨äºéé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„NeRFä¸­ï¼Œå‡è½»ç¬æ€å¹²æ‰°ï¼Œæé«˜PSNRã€SSIMã€LPIPSæŒ‡æ ‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èŒƒä¾‹â€œå¯å‘å¼å¼•å¯¼åˆ†å‰²â€ï¼ˆHuGSï¼‰ï¼Œå°†æ‰‹å·¥å¯å‘å¼ä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ç›¸ç»“åˆï¼Œæ˜¾è‘—å¢å¼ºäº†ä»ç¬æ€å¹²æ‰°ä¸­åˆ†ç¦»é™æ€åœºæ™¯çš„èƒ½åŠ›ã€‚åœ¨éé™æ€åœºæ™¯ä¸­è®­ç»ƒçš„NeRFä¸­ï¼ŒHuGSåœ¨å‡è½»ç¬æ€å¹²æ‰°æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œé²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºHuGSèŒƒä¾‹ï¼ŒèåˆåŸºäºSfMå’Œé¢œè‰²æ®‹å·®çš„å¯å‘å¼ï¼Œé€‚ç”¨äºå„ç§çº¹ç†ç‰¹å¾ã€‚æ€§èƒ½ï¼šåœ¨Kubricå’ŒDistractoræ•°æ®é›†ä¸Šï¼ŒHuGSåˆ†åˆ«åœ¨PSNRã€SSIMã€LPIPSæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚å·¥ä½œé‡ï¼šHuGSçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„NeRFè®­ç»ƒæ¡†æ¶ä¸­ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9f7759f89c5adf4063664cf1bfed21c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc605b8b0429fbc216f370cfd7990cf6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-098b5a8f55215d0b0cf0e540534df631.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fbf1f6c234a4b90e14fec9e174ab52b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9af7196e065eb0a28ba5d50b9587dd65.jpg" align="middle"></details><h2 id="Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields"><a href="#Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields" class="headerlink" title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields"></a>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields</h2><p><strong>Authors:Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau</strong></p><p>Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a> </p><p><a href="http://arxiv.org/abs/2403.16224v1">PDF</a> CVPR 2024 paper. Project webpage <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a></p><p><strong>Summary</strong><br>åŸºäºNeRFå’Œå…‰çº¿è¿½è¸ªçš„æ–°å‹5Dç¥ç»å…¨å…‰å‡½æ•°(NeP)ï¼Œå¯ç²¾ç¡®æè¿°å…‰ç…§ä¸ç‰©ä½“äº¤äº’è¿‡ç¨‹ï¼Œæå‡å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨é‡å»ºæ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é€†å‘æ¸²æŸ“æ—¨åœ¨æ¢å¤ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œæè´¨ï¼Œä¸ç¥ç»è¾å°„åœº(NeRF)ç›¸æ¯”ï¼Œé€†å‘æ¸²æŸ“ä¸ºä¼ ç»Ÿæ¸²æŸ“å¼•æ“æä¾›äº†æ›´å…¼å®¹çš„é‡å»ºã€‚</li><li>ç°æœ‰çš„åŸºäºNeRFçš„é€†å‘æ¸²æŸ“æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¿‡åº¦ç®€åŒ–ä¸º2Dç¯å¢ƒè´´å›¾ï¼Œè¯¥è´´å›¾ä»…å‡å®šæ— é™å…‰æºã€‚</li><li>è§‚å¯Ÿåˆ°NeRFåœ¨æ¢å¤è¾å°„åœºæ–¹é¢çš„ä¼˜åŠ¿ï¼Œæå‡ºäº†ä¸€ç§åŸºäºNeRFå’Œå…‰çº¿è¿½è¸ªçš„æ–°å‹5Dç¥ç»å…¨å…‰å‡½æ•°(NeP)ï¼Œä»¥ä¾¿é€šè¿‡æ¸²æŸ“æ–¹ç¨‹è¡¨è¿°æ›´å‡†ç¡®çš„å…‰ç…§-ç‰©ä½“äº¤äº’ã€‚</li><li>è®¾è®¡äº†ä¸€ç§ææ–™æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œå€ŸåŠ©é¢„å…ˆè¿‡æ»¤çš„è¾å°„åœºï¼Œä»¥æœ‰æ•ˆçš„æ–¹å¼æ•´åˆBRDFç“£ä¸­çš„å…‰æºã€‚</li><li>æ–¹æ³•åˆ†ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡å»ºç›®æ ‡ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œé¢„å…ˆè¿‡æ»¤çš„ç¯å¢ƒè¾å°„åœºï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æå‡ºçš„NePå’Œææ–™æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ä¼°è®¡ç›®æ ‡ç‰©ä½“çš„æè´¨ã€‚</li><li>åœ¨æå‡ºçš„çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ–¹æ³•å¯ä»¥ä»é™„è¿‘çš„ç‰©ä½“ä¸­é‡å»ºå…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºç¥ç»è§†åœºå‡½æ•°å’Œè¾å°„åœºçš„ç‰©ä½“å…‰æ³½åæ¼”æ¸²æŸ“</li><li>ä½œè€…ï¼šç‹æµ©æºã€èƒ¡æ–‡åšã€æœ±ç£Šã€åˆ˜æ¶¦æ£®</li><li>éš¶å±ï¼šé¦™æ¸¯åŸå¸‚å¤§å­¦</li><li>å…³é”®è¯ï¼šinverse renderingã€glossy objectsã€neural plenoptic functionã€radiance fields</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16224    Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨çœŸå®æ„Ÿé‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°† NeRF é›†æˆåˆ°ä¼ ç»Ÿæ¸²æŸ“å¼•æ“ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸º NeRF ä»¥çº ç¼ çš„æ–¹å¼è¡¨ç¤ºå¯¹è±¡å’Œå…‰ç…§ã€‚åˆ†è§£è¡¨ç¤ºä¸ºå‡ ä½•ã€æè´¨å’Œç¯å¢ƒå…‰ç…§ï¼Œå³åæ¼”æ¸²æŸ“ï¼Œå¯¹äºæ¸¸æˆåˆ¶ä½œå’Œæ‰©å±•ç°å®ä¸­çš„é€‚ç”¨æ€§è‡³å…³é‡è¦ã€‚è¿‘æœŸå·¥ä½œæ¢ç´¢äº†å‡ ä½•é‡å»ºï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•åˆ°æè´¨ä¼°è®¡ï¼Œä¾‹å¦‚åç…§ç‡ã€ç²—ç³™åº¦å’Œé‡‘å±åº¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¡¨ç¤ºä¸º 2D ç¯å¢ƒè´´å›¾ï¼Œè¿™å°†å¤æ‚çœŸå®çš„ç…§æ˜åˆ†å¸ƒè¿‡åº¦ç®€åŒ–ä¸ºä»…é™äºæ— é™å…‰ç…§ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œç›®æ ‡å¯¹è±¡è¢«å…¶ä»–å¯¹è±¡åŒ…å›´ï¼Œå¤§é‡å…‰çº¿å®é™…ä¸Šæ¥è‡ªé™„è¿‘ç‰©ä½“çš„è¾å°„ã€‚å¿½ç•¥è¿™äº›å¸¸è§åœºæ™¯ä¼šå¯¼è‡´å‡ ä½•å’Œæè´¨çš„é‡å»ºæ•ˆæœè¾ƒå·®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…‰æ³½ç‰©ä½“ï¼Œä¾‹å¦‚ NeRO [10] åœ¨å›¾ 1 ä¸­çš„ä¸å½“ç»“æœã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰åŸºäº NeRF çš„åæ¼”æ¸²æŸ“æ–¹æ³•æ— æ³•å¾ˆå¥½åœ°å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å°†å…‰ç…§è¿‡åº¦ç®€åŒ–ä¸º 2D ç¯å¢ƒè´´å›¾ï¼Œè¿™å‡è®¾åªæœ‰æ— é™å…‰ç…§ã€‚å°½ç®¡ NeRF åœ¨æ¢å¤è¾å°„åœºæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è¿™äº›æ–¹æ³•å¿½ç•¥äº†ç‰©ä½“å’Œå…‰ç…§ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è§†åœºå‡½æ•°ï¼ˆNePï¼‰æ¥è¡¨ç¤ºå…¨å±€å…‰ç…§ä½œä¸º 5D å‡½æ•° fp(x, d)ï¼Œå®ƒæè¿°äº†æ¯ä¸ªå…‰çº¿åœ¨åœºæ™¯ä¸­çš„é¢œè‰²ã€‚NeP åŸºäº NeRF å’Œå…‰çº¿è¿½è¸ªï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°é€šè¿‡æ¸²æŸ“æ–¹ç¨‹è¡¨è¿°å…‰ç…§ä¸ç‰©ä½“çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ç§æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œåœ¨é¢„è¿‡æ»¤è¾å°„åœºçš„å¸®åŠ©ä¸‹ï¼Œæœ‰æ•ˆåœ°å°†å…‰çº¿ç§¯åˆ†åˆ° BRDF lobe ä¸­ã€‚è¯¥æ–¹æ³•æœ‰ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µé‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å’Œé¢„è¿‡æ»¤çš„ç¯å¢ƒè¾å°„åœºï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨æå‡ºçš„ NeP å’Œæè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ä¼°è®¡ç›®æ ‡å¯¹è±¡çš„æè´¨ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡åŠæ€§èƒ½ï¼šæœ¬æ–‡çš„æ–¹æ³•åœ¨æå‡ºçš„çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥ä»é™„è¿‘çš„ç‰©ä½“é‡å»ºå…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“çš„å‡ ä½•/æè´¨ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒé«˜çš„ä¿çœŸåº¦ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼Œå³è§£å†³å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“çš„åæ¼”æ¸²æŸ“é—®é¢˜ï¼Œå¹¶ä¸ºæ¸¸æˆåˆ¶ä½œå’Œæ‰©å±•ç°å®æä¾›æ›´å…¼å®¹çš„é‡å»ºã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åœºå­¦ä¹ ï¼šåˆ©ç”¨ NeuS å’Œ NeRF é‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œç¯å¢ƒå…‰ç…§åœºï¼›(2) æè´¨å­¦ä¹ ï¼šé‡‡ç”¨å°„çº¿è¿½è¸ªè¯„ä¼°æ¸²æŸ“æ–¹ç¨‹ï¼Œä½¿ç”¨æå‡ºçš„ç¥ç»è§†åœºå‡½æ•° (NeP) è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œå¹¶è®¾è®¡æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥æ¥æœ‰æ•ˆç§¯åˆ†å…‰çº¿åˆ° BRDF lobe ä¸­ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è§†åœºå‡½æ•°ï¼ˆNePï¼‰çš„å…‰æ³½ç‰©ä½“åæ¼”æ¸²æŸ“æ–°æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰åŸºäº NeRF çš„åæ¼”æ¸²æŸ“æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å±€éƒ¨å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“æ—¶å­˜åœ¨çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ¨¡å‹ï¼Œå…¶ä¸­åœºå­¦ä¹ é˜¶æ®µå¢å¼ºäº† 3D å‡ ä½•é‡å»ºçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å…‰ç…§ä¸‹çš„å…‰æ³½ç‰©ä½“ã€‚åœ¨æè´¨å­¦ä¹ é˜¶æ®µï¼ŒNeP ä½¿ç”¨åŸºäºå¯¹è±¡åœºå’Œç¯å¢ƒåœºçš„ 5D ç¥ç»è§†åœºå‡½æ•°è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œä»è€Œå®ç°æ›´é«˜ä¿çœŸçš„æè´¨ä¼°è®¡å’Œåæ¼”æ¸²æŸ“ã€‚æœ¬æ–‡æå‡ºçš„æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†æè´¨å­¦ä¹ çš„æ•ˆç‡ã€‚åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäº NeRF çš„ç¥ç»è§†åœºå‡½æ•° (NeP) æ¥è¡¨ç¤ºå…¨å±€å…‰ç…§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å…‰ç…§è¡¨ç¤ºè¿‡åº¦ç®€åŒ–çš„å±€é™æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†å…‰çº¿ç§¯åˆ†åˆ° BRDF ç“£å¶ä¸­ï¼Œæé«˜äº†æè´¨å­¦ä¹ çš„æ•ˆç‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•/æè´¨é‡å»ºæ–¹é¢å–å¾—äº†è¾ƒé«˜çš„ä¿çœŸåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰å¤æ‚å…‰ç…§äº¤äº’çš„å…‰æ³½ç‰©ä½“ä¸Šã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•å’Œæè´¨é‡å»ºè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬åœºå­¦ä¹ å’Œæè´¨å­¦ä¹ ã€‚</li><li>åœºå­¦ä¹ é˜¶æ®µéœ€è¦ä½¿ç”¨ NeRF é‡å»ºç›®æ ‡å¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œç¯å¢ƒå…‰ç…§åœºã€‚</li><li>æè´¨å­¦ä¹ é˜¶æ®µéœ€è¦ä½¿ç”¨æå‡ºçš„ NeP å’Œæè´¨æ„ŸçŸ¥é”¥å½¢é‡‡æ ·ç­–ç•¥æ¥ä¼°è®¡ç›®æ ‡å¯¹è±¡çš„æè´¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-19389dc3c1eeb88fa4bd1a391ed9769e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc0c31ef64fde722ce725963ff722810.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1bbaa6a9f174427984086631cc201ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4ae42268b5dcd832fa8bb1f8c3f67b29.jpg" align="middle"></details><h2 id="Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes"><a href="#Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes" class="headerlink" title="Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes"></a>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h2><p><strong>Authors:Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</strong></p><p>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.16141v1">PDF</a> Accepted by IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2024), Project website:   <a href="https://otonari726.github.io/entitynerf/">https://otonari726.github.io/entitynerf/</a></p><p><strong>Summary</strong><br>å®ä½“åŒ–çš„ç¥ç»è¾å°„åœºæ–¹æ³•å°†å®ä½“ç»†åˆ†å’Œé™æ€å®ä½“åˆ†ç±»ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å»é™¤äº†åŠ¨æ€åœºæ™¯ä¸­çš„åŠ¨æ€ç‰©ä½“ï¼Œæé«˜äº†é™æ€èƒŒæ™¯çš„é‡å»ºç²¾åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹åœºæ™¯åŠ¨æ€çš„ NeRF ç ”ç©¶é€šå¸¸ä¾èµ–æ˜¾å¼å»ºæ¨¡åœºæ™¯åŠ¨æ€ï¼Œä½†åœ¨åŸå¸‚ç¯å¢ƒä¸­ï¼Œä¸åŒç±»åˆ«å’Œå°ºåº¦çš„åŠ¨æ€ç‰©ä½“å¸¦æ¥äº†å»ºæ¨¡æŒ‘æˆ˜ã€‚</li><li>å®ä½“åŒ–çš„ NeRF æ–¹æ³•èåˆäº†åŸºäºçŸ¥è¯†å’ŒåŸºäºç»Ÿè®¡çš„ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“åŒ–çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°å»é™¤äº†åŠ¨æ€ç‰©ä½“ã€‚</li><li>å®ä½“ç»†åˆ†å’Œç‰©ä½“/ç‰©è´¨ç»†åˆ†æœ‰åŠ©äºé™æ€å®ä½“åˆ†ç±»ï¼Œæé«˜äº†å»åŠ¨æ€ç‰©ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯çš„ç²¾åº¦ã€‚</li><li>é€šè¿‡ Thing/Stuff ç»†åˆ†ï¼ŒEntity-NeRF å¯ä»¥é’ˆå¯¹ä¸åŒå®ä½“åº”ç”¨ä¸åŒçš„ç­–ç•¥ã€‚</li><li>Entity-NeRF æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰åŠ¨æ€ç‰©ä½“é®ç½©çš„åŸå¸‚åœºæ™¯æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å…¶æ€§èƒ½ã€‚</li><li>å®éªŒç»“æœè¯æ˜ï¼ŒEntity-NeRF åœ¨å»åŠ¨æ€ç‰©ä½“å’Œé‡å»ºé™æ€åŸå¸‚èƒŒæ™¯æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>Entity-NeRF æ–¹æ³•å¯¹ç†è§£å’Œé‡å»ºåŠ¨æ€åœºæ™¯ä¸­çš„é™æ€èƒŒæ™¯å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šEntity-NeRFï¼šæ£€æµ‹å’Œç§»é™¤åŸå¸‚åœºæ™¯ä¸­çš„ç§»åŠ¨å®ä½“</li><li>ä½œè€…ï¼šQianqian Wang, Peter Hedman, Jonathan T. Barron, Ravi Ramamoorthi, Noah Snavely</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡</li><li>å…³é”®è¯ï¼šNeRFï¼ŒåŠ¨æ€åœºæ™¯ï¼Œç§»åŠ¨å®ä½“æ£€æµ‹ï¼ŒèƒŒæ™¯é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07605ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å¯¹äºåŸå¸‚ç¯å¢ƒä¸­ç±»åˆ«å’Œè§„æ¨¡å„å¼‚çš„ç§»åŠ¨å®ä½“å»ºæ¨¡ä»é¢ä¸´æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸æ˜¾å¼å»ºæ¨¡åœºæ™¯åŠ¨æ€ï¼Œä½†éš¾ä»¥å¤„ç†åŸå¸‚ç¯å¢ƒä¸­çš„å¤æ‚ç§»åŠ¨å®ä½“ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šEntity-NeRF ç»“åˆäº†åŸºäºçŸ¥è¯†å’Œç»Ÿè®¡ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯ï¼Œé€šè¿‡å®ä½“åˆ†å‰²å’Œç‰©ä½“/ææ–™åˆ†å‰²æ¥å¯¹é™æ­¢å®ä½“è¿›è¡Œåˆ†ç±»ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŸå¸‚åœºæ™¯æ•°æ®é›†ä¸Šï¼ŒEntity-NeRF åœ¨ç§»é™¤ç§»åŠ¨å®ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå®šé‡å’Œå®šæ€§è¯„ä¼°å‡è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) Entity-wise Average of Residual Ranks (EARR)ï¼šåˆ©ç”¨æ•°æ®é©±åŠ¨çš„åˆ†å‰²ç½‘ç»œå’Œé‡å»ºæŸå¤±çš„å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯¹å®ä½“è¿›è¡Œåˆ†å‰²å’Œåˆ†ç±»ï¼›(2) åˆä½œå¼é™æ­¢å®ä½“åˆ†ç±»ï¼šé€šè¿‡è®­ç»ƒä¸€ä¸ªé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œè¯†åˆ«å‡ºåœºæ™¯ä¸­å±äºé™æ­¢ç‰©ä½“ç±»åˆ«çš„å®ä½“ï¼Œç¡®ä¿å…¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«åŒ…å«åœ¨å†…ï¼›(3) ç»“åˆåŸºäºçŸ¥è¯†å’Œç»Ÿè®¡çš„æ–¹æ³•ï¼šå°†åŸºäºçŸ¥è¯†çš„å®ä½“åˆ†å‰²ç»“æœä¸æ®‹å·®ç§©ç»Ÿè®¡ç›¸ç»“åˆï¼Œå¯¹ç§»åŠ¨å®ä½“è¿›è¡Œè¯†åˆ«ã€‚</p></li></ol><p><strong>8. ç»“è®º</strong></p><p><strong>(1): è®ºæ–‡æ„ä¹‰</strong></p><p>Entity-NeRF è§£å†³äº†åœ¨åŠ¨æ€åŸå¸‚åœºæ™¯ä¸­æ„å»º NeRF æ—¶è¯†åˆ«å’Œç§»é™¤ä¸åŒç±»åˆ«å’Œå¤§å°çš„ç§»åŠ¨å®ä½“çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŸºäºçŸ¥è¯†å’Œç»Ÿè®¡ç­–ç•¥ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯å’Œç‰©ä½“/ææ–™åˆ†å‰²æ¥åˆ†ç±»é™æ­¢å®ä½“ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç§»åŠ¨å®ä½“ç§»é™¤å’Œé™æ€èƒŒæ™¯é‡å»ºçš„æ€§èƒ½ã€‚</p><p><strong>(2): ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡º Entity-wise Average of Residual Ranks (EARR) æ–¹æ³•ï¼Œåˆ©ç”¨å®ä½“çº§ç»Ÿè®¡ä¿¡æ¯è¯†åˆ«ç§»åŠ¨å®ä½“ã€‚</li><li>è®­ç»ƒé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œç¡®ä¿é™æ­¢ç‰©ä½“ç±»åˆ«å®ä½“åœ¨ NeRF è®­ç»ƒæ—©æœŸè¢«åŒ…å«ã€‚</li><li>å°†åŸºäºçŸ¥è¯†çš„å®ä½“åˆ†å‰²ç»“æœä¸æ®‹å·®ç§©ç»Ÿè®¡ç›¸ç»“åˆï¼Œæé«˜ç§»åŠ¨å®ä½“è¯†åˆ«ç²¾åº¦ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨åŸå¸‚åœºæ™¯æ•°æ®é›†ä¸Šï¼ŒEntity-NeRF åœ¨ç§»é™¤ç§»åŠ¨å®ä½“å’Œé‡å»ºé™æ€èƒŒæ™¯æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>å®šé‡å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ–¹æ³•éœ€è¦è®­ç»ƒæ•°æ®é©±åŠ¨çš„åˆ†å‰²ç½‘ç»œå’Œé™æ­¢å®ä½“åˆ†ç±»ç½‘ç»œï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ã€‚</li><li>åœ¨å¤„ç†å¤§å‹ç§»åŠ¨ç‰©ä½“é®æŒ¡èƒŒæ™¯æˆ–é˜´å½±æ—¶ï¼Œå¯èƒ½éœ€è¦é›†æˆå›¾åƒä¿®å¤æŠ€æœ¯æˆ–è¿›è¡Œåå¤„ç†ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-efcdfe37992efdbb34f6e7f9822a8d9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ff6c82191ea69b2028df2cc404ec63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c93fe8596c9d0d0f8b492f04667fbe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9ed70161b8c159e297fc7cbd9e45f8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-12cc092f2ce74bcfed4debe821b5da40.jpg" align="middle"></details>## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field**Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui**Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam. [PDF](http://arxiv.org/abs/2403.16095v1) Project Page: https://zju3dv.github.io/cg-slam**Summary**åŸºäºæ–°å‹çš„ä¸ç¡®å®šæ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ CG-SLAMï¼Œ RGB-D SLAM å¯åœ¨å¯†é›†å›¾ä¸­é«˜æ•ˆè¡¨è¾¾ï¼Œå®ç°å®æ—¶è¿½è¸ªï¼Œå»ºæ¨¡ï¼Œé€Ÿåº¦æå‡è‡³ 15Hzã€‚**Key Takeaways**- æå‡ºä¸€ç§åŸºäºä¸ç¡®å®šæ„ŸçŸ¥çš„ 3D é«˜æ–¯åœºï¼Œç”¨äº SLAM ä¸­çš„ 3D è¡¨å¾ã€‚- åˆ†æé«˜æ–¯ Splattingï¼Œæå‡ºæŠ€æœ¯æ„å»ºä¸€è‡´ç¨³å®šçš„ 3D é«˜æ–¯åœºï¼Œé€‚åˆè¿½è¸ªå»ºå›¾ã€‚- è®¾è®¡æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä¼˜åŒ–ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸºå…ƒï¼Œæå‡è¿½è¸ªæ•ˆç‡å’Œç²¾åº¦ã€‚- CG-SLAM èåˆç‰¹å¾ç‚¹å’Œç´§å‡‘è¡¨ç¤ºçš„ä¼˜åŠ¿ï¼Œå…¼é¡¾ç²¾åº¦å’Œæ•ˆç‡ã€‚- CG-SLAM åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„è¿½è¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚- CG-SLAM è·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hz ï¼Œæ˜æ˜¾æå‡å»ºå›¾æ•ˆç‡ã€‚- é¡¹ç›®ä»£ç å¼€æºï¼Œæ–¹ä¾¿ç ”ç©¶å’Œåº”ç”¨ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šCG-SLAMï¼šä¸€ç§åŸºäºä¸€è‡´çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM</li><li>ä½œè€…ï¼šèƒ¡å˜‰ç‘ï¼Œé™ˆæ˜¾æµ©ï¼Œå†¯ä¼¯å¯…ï¼Œæå¹¿æ—ï¼Œæ¨è‰¯æ™¶ï¼ŒåŒ…è™å†›ï¼Œå¼ å›½é”‹ï¼Œå´”å…†é¹</li><li>éš¶å±å•ä½ï¼šæµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¨ å¯†è§†è§‰ SLAMã€ç¥ç»æ¸²æŸ“ã€3D é«˜æ–¯åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16095</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¢«å¹¿æ³›ç”¨ä½œç¨ å¯† SLAM çš„ 3D è¡¨ç¤ºã€‚å°½ç®¡åœ¨è¡¨é¢å»ºæ¨¡å’Œæ–°è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•å—åˆ°å…¶è®¡ç®—å¯†é›†ä¸”è€—æ—¶çš„ä½“ç§¯æ¸²æŸ“ç®¡çº¿çš„é˜»ç¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…·æœ‰é«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„æ–°å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿï¼Œå³ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ Splatting çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æŠ€æœ¯æ¥æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»è€Œæé«˜äº†è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…·æœ‰é«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„æ–°å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿï¼Œå³ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ Splatting çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æŠ€æœ¯æ¥æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»è€Œæé«˜äº†è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†å“è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚æˆ‘ä»¬å°†å…¬å¼€æˆ‘ä»¬çš„æºä»£ç ã€‚</p></li><li><p>Methodsï¼šï¼ˆ1ï¼‰åŸºäºé«˜æ–¯Splattingæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºï¼›ï¼ˆ2ï¼‰æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­ï¼›ï¼ˆ3ï¼‰åˆ©ç”¨ç¥ç»æ¸²æŸ“æŠ€æœ¯è¿›è¡Œç¨ å¯†å»ºå›¾ï¼Œå®ç°é«˜ç²¾åº¦è¡¨é¢é‡å»ºå’Œæ–°è§†å›¾åˆæˆï¼›ï¼ˆ4ï¼‰é‡‡ç”¨é«˜æ•ˆçš„è·Ÿè¸ªç­–ç•¥ï¼Œå®ç°å®æ—¶è·Ÿè¸ªå’Œå»ºå›¾ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šCG-SLAM æ˜¯ä¸€ç§åŸºäºä¸€è‡´çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ç¨ å¯† RGB-DSLAMï¼Œå®ƒé€šè¿‡å¼ºåŒ– 3D é«˜æ–¯åœºçš„ç¨ å¯†æ€§å’Œç¨³å®šæ€§æ¥æé«˜è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li>åŸºäºé«˜æ–¯ Splatting æ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœº</li><li>æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸè¯­</li><li>åˆ©ç”¨ç¥ç»æ¸²æŸ“æŠ€æœ¯è¿›è¡Œç¨ å¯†å»ºå›¾ï¼Œå®ç°é«˜ç²¾åº¦è¡¨é¢é‡å»ºå’Œæ–°è§†å›¾åˆæˆ</li><li>é‡‡ç”¨é«˜æ•ˆçš„è·Ÿè¸ªç­–ç•¥ï¼Œå®ç°å®æ—¶è·Ÿè¸ªå’Œå»ºå›¾</li><li>æ€§èƒ½ï¼š</li><li>åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†å“è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hz</li><li>å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡å…¬å¼€æºä»£ç </li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap"><a href="#Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap" class="headerlink" title="Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap"></a>Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap</h2><p><strong>Authors:Carl LindstrÃ¶m, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</strong></p><p>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. </p><p><a href="http://arxiv.org/abs/2403.16092v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰æ¨¡æ‹Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä½†å¦‚ä½•ç¡®ä¿ç®—æ³•å°†ä»¿çœŸæ•°æ®ä¸çœŸå®æ•°æ®ä¸€è§†åŒä»å´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºä¸€ç§è§†è§’ï¼Œä¸“æ³¨äºæå‡ç®—æ³•å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§ï¼Œè€Œä¸æ˜¯åªè¿½æ±‚å‘ˆç°é€¼çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶ä»¿çœŸä¸­å¾ˆé‡è¦</li><li>ç¡®ä¿ç®—æ³•å¯¹çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸€è§†åŒä»è‡³å…³é‡è¦</li><li>åº”æ³¨é‡æå‡æ„ŸçŸ¥æ¨¡å‹å¯¹NeRFä¼ªå½±çš„é²æ£’æ€§</li><li>è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶åœºæ™¯çœŸå®-æ¨¡æ‹Ÿæ•°æ®å·®è·ç ”ç©¶</li><li>è¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„è¡¨ç°</li><li>æ¢ç´¢äº†ä¸åŒçš„é¢„è®­ç»ƒç­–ç•¥çš„æ•ˆæœ</li><li>æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æ˜¾è‘—æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½</li><li>FIDå’ŒLPIPSæ˜¯çœŸå®-æ¨¡æ‹Ÿå·®è·çš„å¼ºåŠ›æŒ‡æ ‡</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRF èƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶å—ï¼Ÿæœç€ç¼©å°çœŸå®ä¸æ¨¡æ‹Ÿå·®è·è¿ˆè¿›</li><li>ä½œè€…ï¼šCarl LindstrÂ¨omâ€ ,1,2 Georg Hessâ€ ,1,2 Adam Lilja1,2 Maryam Fatemi1 Lars Hammarstrand2 Christoffer Petersson1,2 Lennart Svensson2</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šZenseact</li><li>å…³é”®è¯ï¼šNeRFã€è‡ªåŠ¨é©¾é©¶ã€çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ã€æ„ŸçŸ¥æ¨¡å‹é²æ£’æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.16092v1[cs.CV]</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²æˆä¸ºæ¨è¿›è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç ”ç©¶çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œæä¾›å¯æ‰©å±•çš„é—­ç¯ä»¿çœŸå’Œæ•°æ®å¢å¼ºåŠŸèƒ½ã€‚ç„¶è€Œï¼Œä¸ºäº†ä¿¡ä»»ä»¿çœŸä¸­è·å¾—çš„ç»“æœï¼Œéœ€è¦ç¡®ä¿ AD ç³»ç»Ÿä»¥ç›¸åŒçš„æ–¹å¼æ„ŸçŸ¥çœŸå®å’Œæ¸²æŸ“çš„æ•°æ®ã€‚è™½ç„¶æ¸²æŸ“æ–¹æ³•çš„æ€§èƒ½æ­£åœ¨æé«˜ï¼Œä½†è®¸å¤šåœºæ™¯åœ¨æœ¬è´¨ä¸Šä»ç„¶éš¾ä»¥é€¼çœŸåœ°é‡å»ºã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æé«˜æ¸²æŸ“ä¿çœŸåº¦ä¸Šï¼Œä½†å½“æ¸²æŸ“è´¨é‡ä¸‹é™æ—¶ï¼Œæ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ã€‚ä¸å…¶ä»…ä»…å…³æ³¨æé«˜æ¸²æŸ“ä¿çœŸåº¦ï¼Œä¸å¦‚æ¢ç´¢ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•æ¥å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¸å½±å“çœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨æœ€å…ˆè¿›çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å¯¹ AD è®¾ç½®ä¸­çš„çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡è°ƒæŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šè¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹ï¼Œå¹¶ç ”ç©¶äº†ä¸åŒé¢„è®­ç»ƒç­–ç•¥çš„å½±å“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚æœ€åï¼Œæœ¬æ–‡æ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç¡®å®š FID å’Œ LPIPS æ˜¯å¼ºæœ‰åŠ›çš„æŒ‡æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å›¾åƒå¢å¼ºï¼šä½¿ç”¨å›¾åƒå¢å¼ºï¼ˆå¦‚æ·»åŠ å™ªå£°ã€æ¨¡ç³Šã€å…‰åº¦å¤±çœŸç­‰ï¼‰æ¥æé«˜æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®ä¸­ä¼ªå½±çš„é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨æ¸²æŸ“å›¾åƒå¾®è°ƒï¼šåœ¨å¾®è°ƒæ„ŸçŸ¥æ¨¡å‹æ—¶ï¼ŒåŠ å…¥æ¸²æŸ“å›¾åƒï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ¸²æŸ“æ•°æ®çš„é€‚åº”æ€§ã€‚ï¼ˆ3ï¼‰å›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼šä½¿ç”¨å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¨¡å‹ï¼Œå°†çœŸå®å›¾åƒè½¬æ¢ä¸ºç±»ä¼¼æ¸²æŸ“å›¾åƒçš„ä¼ªå½±ï¼Œä»è€Œå¢åŠ ç”¨äºå¾®è°ƒçš„æ¸²æŸ“å›¾åƒæ•°é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œæ¢ç´¢äº†å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§çš„æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³çœŸå®ä¸æ¨¡æ‹Ÿæ•°æ®å·®è·é—®é¢˜ï¼Œæ¢ç´¢äº†å¢å¼ºæ„ŸçŸ¥æ¨¡å‹å¯¹ NeRF ä¼ªå½±çš„é²æ£’æ€§çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šè¯„ä¼°äº†ç›®æ ‡æ£€æµ‹å™¨å’Œåœ¨çº¿å»ºå›¾æ¨¡å‹ï¼Œç»“æœè¡¨æ˜æ¨¡å‹å¯¹æ¨¡æ‹Ÿæ•°æ®çš„é²æ£’æ€§æœ‰äº†æ˜¾ç€æé«˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³æé«˜äº†çœŸå®ä¸–ç•Œçš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šè¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ï¼Œè¯„ä¼°äº†æ„ŸçŸ¥æ¨¡å‹åœ¨çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®ä¸Šçš„æ€§èƒ½ï¼Œç ”ç©¶äº†ä¸åŒé¢„è®­ç»ƒç­–ç•¥çš„å½±å“ï¼Œæ·±å…¥ç ”ç©¶äº†çœŸå®ä¸æ¨¡æ‹Ÿå·®è·ä¸å›¾åƒé‡å»ºæŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-68245c1e9e03a301ef7308b852cec45b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-637dca64e1ede555b3f77fe3d6e45f26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c065e635b99332c436cd774aa002fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3ea9ed03a5a035d0bd40ebe5d3c1dfa.jpg" align="middle"></details><h2 id="PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling"><a href="#PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling" class="headerlink" title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling"></a>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling</h2><p><strong>Authors:Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang</strong></p><p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href="https://pku-dymvhumans.github.io">https://pku-dymvhumans.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.16080v2">PDF</a> </p><p><strong>Summary</strong><br>åŒ—å¤§åŠ¨æ€å¤šè§†è§’äººä½“æ•°æ®é›†ï¼Œæä¾›é«˜è´¨é‡åŠ¨æ€äººä½“åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æä¾› 820 ä¸‡å¸§ï¼Œç”± 56 ä¸ªåŒæ­¥æ‘„åƒæœºåœ¨ä¸åŒåœºæ™¯ä¸­æ‹æ‘„ã€‚</li><li>åŒ…å« 32 ä½äººä½“ï¼Œ45 ç§ä¸åŒåœºæ™¯ï¼Œå…·æœ‰ä¸°å¯Œçš„å¤–è§‚å’Œé€¼çœŸåŠ¨ä½œã€‚</li><li>åŸºäº NeRF åœºæ™¯è¡¨ç¤ºï¼Œæä¾›ç°æˆæ¡†æ¶ï¼Œä¾¿äºåœ¨ PKU-DyMVHumans æ•°æ®é›†ä¸Šæä¾›æœ€å…ˆè¿›çš„ NeRF å®ç°å’ŒåŸºå‡†ã€‚</li><li>é€‚ç”¨äºç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£ã€é«˜è´¨é‡äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯ç…§ç‰‡çº§æ–°è§†è§’åˆæˆç­‰åº”ç”¨ã€‚</li><li>å¹¿æ³›çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ­¤ç±»é«˜ä¿çœŸåŠ¨æ€æ•°æ®äº§ç”Ÿäº†æ–°çš„è§‚å¯Ÿå’ŒæŒ‘æˆ˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šPKU-DyMVHumansï¼šç”¨äºé«˜ä¿çœŸåŠ¨æ€äººä½“å»ºæ¨¡çš„å¤šè§†è§’è§†é¢‘åŸºå‡†</li><li>ä½œè€…ï¼š</li><li>è¢å¿—æ°</li><li>å­™å‰‘</li><li>æ—å‡¡</li><li>è¢å˜‰å ƒ</li><li>å´æ–°</li><li>æ›¹æ—­ä¸œ</li><li>ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</li><li>å…³é”®è¯ï¼š</li><li>äººä½“å»ºæ¨¡</li><li>åŠ¨æ€åœºæ™¯</li><li>å¤šè§†è§’è§†é¢‘</li><li>ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.12006   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   é«˜ä¿çœŸçš„äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯çš„é€¼çœŸæ¸²æŸ“æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„é•¿æœŸé—®é¢˜ã€‚å°½ç®¡åœ¨å¼€å‘å„ç§æ•è·ç³»ç»Ÿå’Œé‡å»ºç®—æ³•æ–¹é¢æŠ•å…¥äº†å¤§é‡ç²¾åŠ›ï¼Œä½†æœ€è¿‘çš„è¿›å±•ä»ç„¶éš¾ä»¥å¤„ç†å®½æ¾æˆ–è¶…å¤§å°ºå¯¸çš„æœè£…ä»¥åŠè¿‡äºå¤æ‚çš„å§¿åŠ¿ã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºè·å–é«˜è´¨é‡äººä½“æ•°æ®é›†çš„æŒ‘æˆ˜ã€‚   (2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç¨€ç–çš„ 3D ç‚¹äº‘æˆ–ç²—ç³™çš„ç‰©ä½“æ©ç ï¼Œè¿™é™åˆ¶äº†é‡å»ºçš„ä¿çœŸåº¦ã€‚åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„åœºæ™¯è¡¨ç¤ºæœ€è¿‘å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†ç¼ºä¹ä¸€ä¸ªé«˜è´¨é‡çš„äººä½“æ•°æ®é›†æ¥è¯„ä¼°å’Œæ¨åŠ¨å…¶åœ¨åŠ¨æ€åœºæ™¯ä¸­çš„äººä½“å»ºæ¨¡å’Œæ¸²æŸ“æ–¹é¢çš„æ½œåŠ›ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   ä¸ºäº†ä¿ƒè¿›è¿™äº›é¢†åŸŸçš„å‘å±•ï¼Œæœ¬æ–‡æå‡ºäº† PKU-DyMVHumansï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„ä»¥äººä¸ºä¸­å¿ƒçš„åŠ¨æ€äººä½“åœºæ™¯é«˜ä¿çœŸé‡å»ºå’Œæ¸²æŸ“æ•°æ®é›†ã€‚å®ƒåŒ…å«æ¥è‡ª 56 ä¸ªä»¥ä¸ŠåŒæ­¥æ‘„åƒæœºçš„ 820 ä¸‡å¸§ï¼Œæ¶µç›–å„ç§åœºæ™¯ã€‚è¿™äº›åºåˆ—åŒ…æ‹¬ 32 ä¸ªäººç±»å—è¯•è€…ï¼Œåˆ†å¸ƒåœ¨ 45 ä¸ªä¸åŒçš„åœºæ™¯ä¸­ï¼Œæ¯ä¸ªåœºæ™¯éƒ½å…·æœ‰é«˜åº¦è¯¦ç»†çš„å¤–è§‚å’Œé€¼çœŸçš„äººä½“åŠ¨ä½œã€‚å—åŸºäº NeRF çš„åœºæ™¯è¡¨ç¤ºçš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæœ¬æ–‡è¿˜è®¾ç½®äº†ä¸€ä¸ªç°æˆçš„æ¡†æ¶ï¼Œä¾¿äºåœ¨ PKU-DyMVHumans æ•°æ®é›†ä¸Šæä¾›æœ€å…ˆè¿›çš„åŸºäº NeRF çš„å®ç°å’ŒåŸºå‡†ã€‚è¿™ä¸ºå„ç§åº”ç”¨é“ºå¹³äº†é“è·¯ï¼Œå¦‚ç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£ã€é«˜è´¨é‡äººä½“é‡å»ºå’ŒåŠ¨æ€åœºæ™¯çš„é€¼çœŸæ–°è§†è§’åˆæˆã€‚   (4) æ–¹æ³•åœ¨ä½•ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä½•ç§æ€§èƒ½ï¼Œæ˜¯å¦èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼š   æœ¬æ–‡åœ¨åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå±•ç¤ºäº†ä½¿ç”¨å¦‚æ­¤é«˜ä¿çœŸåŠ¨æ€æ•°æ®æ‰€äº§ç”Ÿçš„æ–°è§‚å¯Ÿå’ŒæŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†å¯ç”¨äºï¼š</li><li>ç»†ç²’åº¦å‰æ™¯/èƒŒæ™¯åˆ†è§£</li><li>é«˜è´¨é‡äººä½“é‡å»º</li><li>åŠ¨æ€åœºæ™¯çš„é€¼çœŸæ–°è§†è§’åˆæˆ</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º PKU-DyMVHumansï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€äººä½“æ•°æ®é›†ï¼Œæ—¨åœ¨ä»å¯†é›†çš„å¤šè§†è§’è§†é¢‘ä¸­è¿›è¡Œé«˜ä¿çœŸçš„äººä½“é‡å»ºå’Œæ¸²æŸ“ã€‚å®ƒå…·æœ‰é«˜ä¿çœŸçš„äººä½“è¡¨ç°ï¼ŒåŒ…æ‹¬é«˜åº¦è¯¦ç»†çš„å¤–è§‚ã€å¤æ‚çš„äººä½“è¿åŠ¨ï¼Œä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„äººä½“-ç‰©ä½“äº¤äº’ã€å¤šäººäº¤äº’å’Œå¤æ‚çš„åœºæ™¯æ•ˆæœï¼ˆä¾‹å¦‚ï¼Œç¯å…‰ã€é˜´å½±å’Œå¸çƒŸï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åŸºå‡†ä»»åŠ¡ï¼Œå¹¶å¯¹å‡ ç§å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒã€‚PKU-DyMVHumans è¿›ä¸€æ­¥å¡«è¡¥äº†ç°æœ‰æ•°æ®é›†å’ŒçœŸå®åœºæ™¯åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚æŒ‘æˆ˜å’Œæœªæ¥å·¥ä½œã€‚è™½ç„¶æˆ‘ä»¬åœ¨å¤§é‡ä»¥äººä¸ºä¸­å¿ƒé‡å»ºå’Œæ¸²æŸ“ä¸ŠéªŒè¯äº†æˆ‘ä»¬æ•°æ®é›†çš„å¤æ‚æ€§å’Œä¿çœŸåº¦ã€‚é‡è¦çš„æ˜¯è¦å¼ºè°ƒæ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„å¤šäººç‰©/ä¸»ä½“å»ºæ¨¡ï¼Œå®ƒå¯ä»¥åæ˜ å¤šäººç‰©äº¤äº’æ€§ã€å¤æ‚åœºæ™¯æ•ˆæœå’Œå¤šè§†è§’ä¸€è‡´æ€§æ€§èƒ½æ–¹é¢çš„æ¸²æŸ“å·®å¼‚ã€‚æ­¤å¤–ï¼Œä»å•çœ¼è‡ªæ—‹è½¬è§†é¢‘ä¸­å¯¹è¿åŠ¨ä¸»ä½“è¿›è¡Œè‡ªç”±è§†ç‚¹æ¸²æŸ“æ˜¯ä¸€ä¸ªå¤æ‚ä½†ç†æƒ³çš„è®¾ç½®ã€‚æˆ‘ä»¬çš„è¡¥å……ææ–™æä¾›äº†è¿åŠ¨ä¸»ä½“çš„è‡ªç”±è§†ç‚¹æ¸²æŸ“çš„é™„åŠ å®éªŒï¼Œç»“æœå—å±€éƒ¨é®æŒ¡å’Œè§†ç‚¹ç¼ºå¤±çš„å½±å“ï¼Œå¯¼è‡´è§†ç‚¹æ¸²æŸ“å‡ºç°ä¼ªå½±ã€‚æœ‰äº†è¿™äº›æœºé‡å’ŒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç›¸ä¿¡ PKU-DyMVHumans å°†æœ‰åˆ©äºç¤¾åŒºä¸­æ–°æ–¹æ³•çš„å‘å±•ã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†æ·±åœ³å¸‚ä¼˜ç§€äººæ‰åŸ¹è®­åŸºé‡‘ã€æ·±åœ³å¸‚ç§‘æŠ€è®¡åˆ’ï¼ˆRCJC20200714114435057ã€SGDX20211123144400001ï¼‰ã€å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆU21B2012ï¼‰å’Œå’ªå’•-åŒ—å¤§å…ƒå®‡å®™æŠ€æœ¯åˆ›æ–°å®éªŒå®¤ï¼ˆR24115SGï¼‰çš„æ”¯æŒã€‚Jianbo Jiao å¾—åˆ°çš‡å®¶å­¦ä¼šèµ æ¬¾ IES\R3\223050 å’Œ SIF\R1\231009.88 çš„æ”¯æŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º PKU-DyMVHumansï¼Œä¸€ä¸ªç”¨äºé«˜ä¿çœŸåŠ¨æ€äººä½“å»ºæ¨¡çš„å¤šè§†è§’è§†é¢‘åŸºå‡†ï¼›æ€§èƒ½ï¼šåœ¨åŸºå‡†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå±•ç¤ºäº†ä½¿ç”¨å¦‚æ­¤é«˜ä¿çœŸåŠ¨æ€æ•°æ®æ‰€äº§ç”Ÿçš„æ–°è§‚å¯Ÿå’ŒæŒ‘æˆ˜ï¼›å·¥ä½œé‡ï¼šæ”¶é›†äº† 820 ä¸‡å¸§ï¼Œæ¶µç›–å„ç§åœºæ™¯ï¼ŒåŒ…æ‹¬ 32 ä¸ªäººç±»å—è¯•è€…å’Œ 45 ä¸ªä¸åŒçš„åœºæ™¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-165a03c4fc78e3abe018f2febbbb4f63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6f56832029ed2af99d8dd35bf8f378.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e06c71a44f02a4c723d19749bb2cf5cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e11e8d21c61a5e04cc190fe2beb0ce63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fee4215f3b978a6d8afa20c3d7631f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-adab8ff1d80ba91401beea1dfee88f35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/</id>
    <published>2024-03-28T03:28:24.000Z</published>
    <updated>2024-03-28T03:28:24.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•£å°„æ¡†æ¶æ·»åŠ äº†ä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œä¸ºå›¾åƒé‡å»ºå¸¦æ¥äº†æ›´å¯é çš„å†³ç­–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä½¿ç”¨é«˜æ–¯æ•£å°„çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ï¼Œå³éšæœºé«˜æ–¯æ•£å°„ (SGS)ã€‚</li><li>é‡‡ç”¨å˜åˆ†æ¨ç†æ–¹æ³•å°†ä¸ç¡®å®šæ€§é¢„æµ‹æ— ç¼é›†æˆåˆ°é«˜æ–¯æ•£å°„çš„æ¸²æŸ“ç®¡çº¿ä¸­ã€‚</li><li>å¼•å…¥ç¨€ç–åŒ–è¯¯å·®ä¸‹è¡¨é¢ç§¯ (AUSE) ä½œä¸ºæ–°çš„æŸå¤±å‡½æ•°é¡¹ã€‚</li><li>é€šè¿‡ä¼˜åŒ–ä¸ç¡®å®šæ€§ä¼°è®¡å’Œå›¾åƒé‡å»ºæ¥æé«˜æ€»ä½“æ€§èƒ½ã€‚</li><li>åœ¨ LLFF æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ SGS åœ¨å›¾åƒæ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>è¯¥æ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­ä¿ƒè¿›æ›´å®‰å…¨çš„å†³ç­–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯æº…å°„çš„ä¸ç¡®å®šæ€§å»ºæ¨¡</li><li>ä½œè€…ï¼šLuca Savantã€Diego Valsesiaã€Enrico Magli</li><li>æ‰€å±æœºæ„ï¼šæ„å¤§åˆ©éƒ½çµç†å·¥å¤§å­¦ç”µå­ä¸ç”µä¿¡ç³»</li><li>å…³é”®è¯ï¼šé«˜æ–¯æº…å°„ã€ä¸ç¡®å®šæ€§ä¼°è®¡ã€ç¥ç»è¾å°„åœºã€è®¡ç®—æœºè§†è§‰</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å…¶è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚é«˜æ–¯æº…å°„ï¼ˆGSï¼‰æŠ€æœ¯ä½œä¸ºä¸€ç§æ›´å…·è®¡ç®—æ•ˆç‡çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ä¿æŒé«˜è´¨é‡æ–°å‹è§†å›¾åˆæˆçš„æƒ…å†µä¸‹ï¼Œæé«˜äº†æ¸²æŸ“é€Ÿåº¦ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šNeRF åœ¨æ–°å‹è§†å›¾åˆæˆä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†ç¼ºä¹æä¾›ä¸è¾“å‡ºç›¸å…³ç½®ä¿¡åº¦ä¿¡æ¯çš„èƒ½åŠ›ã€‚GS è™½ç„¶åœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šå–å¾—äº†ä¼˜åŠ¿ï¼Œä½†åŒæ ·ç¼ºä¹ä¸ç¡®å®šæ€§ä¼°è®¡æœºåˆ¶ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äº GS ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚SGS æ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§ GS æ¡†æ¶ï¼Œå¼•å…¥äº†éšæœºæ€§ï¼Œå…è®¸åœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹  GS è¾å°„åœºçš„å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°äº†å­¦ä¹ è¿‡ç¨‹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒSGS åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ LLFF æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ï¼Œå³ä¸ºä»ä¸šè€…æä¾›å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ‰©å±•ä¼ ç»Ÿç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå¼•å…¥éšæœºæ€§ï¼Œåœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚ï¼ˆ2ï¼‰åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ï¼Œå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚ï¼ˆ3ï¼‰åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°å­¦ä¹ è¿‡ç¨‹ã€‚</p></li></ol><p>8.ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ–¯æº…å°„ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚SGSæ‰©å±•äº†ä¼ ç»Ÿçš„ç¡®å®šæ€§é«˜æ–¯æº…å°„æ¡†æ¶ï¼Œå¼•å…¥äº†éšæœºæ€§ï¼Œå…è®¸åœ¨åˆæˆè§†å›¾çš„åŒæ—¶é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ï¼Œä»è€Œèƒ½å¤Ÿå‡†ç¡®ä¼°è®¡ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°äº†å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGSåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LLFFæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ï¼Œå³ä¸ºä»ä¸šè€…æä¾›å¯¹åˆæˆè§†å›¾å¯é æ€§çš„å®è´µè§è§£ï¼Œä»è€Œä¿ƒè¿›åœ¨å®é™…åº”ç”¨ä¸­æ›´å®‰å…¨çš„å†³ç­–åˆ¶å®šã€‚(2): åˆ›æ–°ç‚¹ï¼š- æå‡ºäº†ä¸€ç§æ–°çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ¡†æ¶ï¼Œç§°ä¸ºéšæœºé«˜æ–¯æº…å°„ï¼ˆSGSï¼‰ã€‚- åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰åœ¨è´å¶æ–¯æ¡†æ¶ä¸­å­¦ä¹ é«˜æ–¯æº…å°„è¾å°„åœºçš„å‚æ•°ã€‚- åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œåˆ›æ–°å­¦ä¹ è¿‡ç¨‹ã€‚æ€§èƒ½ï¼š- åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LLFFæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚- åœ¨æ¸²æŸ“è´¨é‡å’Œä¸ç¡®å®šæ€§ä¼°è®¡æŒ‡æ ‡æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š- å¼•å…¥äº†éšæœºæ€§ï¼Œå¢åŠ äº†è®¡ç®—å¤æ‚åº¦ã€‚- åˆ©ç”¨å˜åˆ†æ¨ç†ï¼ˆVIï¼‰å­¦ä¹ å‚æ•°ï¼Œå¢åŠ äº†è®­ç»ƒæ—¶é—´ã€‚- åœ¨æŸå¤±å‡½æ•°ä¸­å¼•å…¥ç¨€ç–åŒ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUSEï¼‰ä½œä¸ºæ–°é¡¹ï¼Œå¢åŠ äº†è®­ç»ƒéš¾åº¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details>## EgoLifter: Open-world 3D Segmentation for Egocentric Perception**Authors:Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney**In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale. [PDF](http://arxiv.org/abs/2403.18118v1) Preprint. Project page: https://egolifter.github.io/**Summary**è‡ªæˆ‘æå‡å™¨ï¼šä»ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ä¼ æ„Ÿå™¨æ•è·çš„åœºæ™¯ä¸­è‡ªåŠ¨åˆ†å‰² 3D ç‰©ä½“**Key Takeaways**- EgoLifter å¯ä»¥ä» 3D åœºæ™¯ä¸­è‡ªåŠ¨åˆ†å‰²å‡ºä¸ªåˆ« 3D ç‰©ä½“ã€‚- EgoLifter ä½¿ç”¨ 3D é«˜æ–¯æ¨¡å‹ä½œä¸º 3D åœºæ™¯å’Œç‰©ä½“çš„åº•å±‚è¡¨ç¤ºã€‚- EgoLifter åˆ©ç”¨ SAM åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£å­¦ä¹ å¯¹è±¡å®ä¾‹å®šä¹‰ã€‚- EgoLifter è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—æ¥è¿‡æ»¤åŠ¨æ€ç‰©ä½“ã€‚- EgoLifter åœ¨ Aria æ•°å­—å­ªç”Ÿæ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°åŸºå‡†ã€‚- EgoLifter åœ¨å„ç§ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œã€‚- EgoLifter 3D æ„ŸçŸ¥ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒæä¾›äº†å‰æ™¯ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.æ ‡é¢˜ï¼šEgoLifter2.ä½œè€…ï¼šQiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney3.ç¬¬ä¸€ä½œè€…æ‰€å±æœºæ„ï¼šå¤šä¼¦å¤šå¤§å­¦4.å…³é”®è¯ï¼šEgocentric Perceptionã€Open-world Segmentationã€3D Reconstruction5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.18118Github é“¾æ¥ï¼šNone6.æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€å¯ç©¿æˆ´è®¾å¤‡çš„æ™®åŠï¼Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœºå™¨æ„ŸçŸ¥ç®—æ³•å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œè¿™ç±»ç®—æ³•èƒ½å¤Ÿç†è§£ç”¨æˆ·å‘¨å›´çš„ç‰©ç† 3D ä¸–ç•Œã€‚è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ç›´æ¥åæ˜ äº†äººç±»è§‚å¯Ÿä¸–ç•Œçš„æ–¹å¼ï¼ŒåŒ…å«äº†å…³äºç‰©ç†ç¯å¢ƒä»¥åŠäººç±»ç”¨æˆ·å¦‚ä½•ä¸ä¹‹äº¤äº’çš„é‡è¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè‡ªæˆ‘ä¸ºä¸­å¿ƒè¿åŠ¨çš„ç‰¹å®šç‰¹å¾ç»™ 3D è®¡ç®—æœºè§†è§‰å’Œæœºå™¨æ„ŸçŸ¥ç®—æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸é€šè¿‡â€œæ‰«æâ€è¿åŠ¨æ•æ‰çš„æ•°æ®é›†ä¸åŒï¼Œè‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æ— æ³•ä¿è¯åœºæ™¯çš„å®Œæ•´è¦†ç›–ã€‚ç”±äºå¤šè§†è§’è§‚å¯Ÿæœ‰é™æˆ–ç¼ºå¤±ï¼Œè¿™ä½¿å¾—é‡å»ºè¿‡ç¨‹æå…·æŒ‘æˆ˜æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šçš„å¯¹è±¡åˆ†ç±»æ³•ï¼Œå¹¶ä¸”éš¾ä»¥å¤„ç†è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º EgoLifterï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç³»ç»Ÿï¼Œå¯ä»¥å°†ä»è‡ªæˆ‘ä¸ºä¸­å¿ƒä¼ æ„Ÿå™¨æ•è·çš„åœºæ™¯è‡ªåŠ¨åˆ†å‰²æˆå„ä¸ª 3D å¯¹è±¡çš„å®Œæ•´åˆ†è§£ã€‚è¯¥ç³»ç»Ÿä¸“é—¨è®¾è®¡ç”¨äºè‡ªæˆ‘ä¸ºä¸­å¿ƒæ•°æ®ï¼Œå…¶ä¸­åœºæ™¯åŒ…å«æ•°ç™¾ä¸ªä»è‡ªç„¶ï¼ˆéæ‰«æï¼‰è¿åŠ¨ä¸­æ•è·çš„å¯¹è±¡ã€‚EgoLifter é‡‡ç”¨ 3D é«˜æ–¯åˆ†å¸ƒä½œä¸º 3D åœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ Segment Anything Model (SAM) çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ï¼Œä¸å—ä»»ä½•ç‰¹å®šå¯¹è±¡åˆ†ç±»æ³•çš„é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå­¦ä¼šåœ¨ 3D é‡å»ºä¸­æ»¤é™¤åŠ¨æ€å¯¹è±¡ã€‚æœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œèƒ½å¤Ÿå°† 3D å¯¹è±¡å®ä¾‹é‡å»ºä¸º 3D é«˜æ–¯åˆ†å¸ƒçš„é›†åˆï¼Œè¿™äº›é«˜æ–¯åˆ†å¸ƒå…±åŒæ„æˆæ•´ä¸ªåœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ Aria Digital Twin æ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œè¯¥åŸºå‡†å®šé‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŸºäºè‡ªç„¶è‡ªæˆ‘ä¸ºä¸­å¿ƒè¾“å…¥çš„å¼€æ”¾ä¸–ç•Œ 3D åˆ†å‰²ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å„ç§è‡ªæˆ‘ä¸ºä¸­å¿ƒæ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œ EgoLifterï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§„æ¨¡åŒ– 3D è‡ªæˆ‘ä¸ºä¸­å¿ƒæ„ŸçŸ¥æ–¹é¢çš„å‰æ™¯ã€‚</p><ol><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰EgoLifterç³»ç»Ÿé‡‡ç”¨3Dé«˜æ–¯åˆ†å¸ƒä½œä¸º3Dåœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨SegmentAnythingModel (SAM)çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ï¼Œä¸å—ä»»ä½•ç‰¹å®šå¯¹è±¡åˆ†ç±»æ³•çš„é™åˆ¶ã€‚ï¼ˆ2ï¼‰ä¸ºäº†åº”å¯¹è‡ªæˆ‘ä¸ºä¸­å¿ƒè§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿå­¦ä¼šåœ¨3Dé‡å»ºä¸­æ»¤é™¤åŠ¨æ€å¯¹è±¡ã€‚ï¼ˆ3ï¼‰EgoLifterç³»ç»Ÿæœ€ç»ˆçš„ç»“æœæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨ç®¡é“ï¼Œèƒ½å¤Ÿå°†3Då¯¹è±¡å®ä¾‹é‡å»ºä¸º3Dé«˜æ–¯åˆ†å¸ƒçš„é›†åˆï¼Œè¿™äº›é«˜æ–¯åˆ†å¸ƒå…±åŒæ„æˆæ•´ä¸ªåœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šEgoLifter ç®—æ³•åŒæ—¶è§£å†³äº†é‡å¤–ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥ä¸­çš„ 3D é‡å»ºå’Œå¼€æ”¾ä¸–ç•Œåˆ†å‰²é—®é¢˜ã€‚è¯¥ç®—æ³•é€šè¿‡å°† 2D åˆ†å‰²æå‡åˆ° 3D é«˜æ–¯åˆ†å¸ƒä¸­ï¼Œåœ¨æ²¡æœ‰ 3D æ•°æ®æ³¨é‡Šçš„æƒ…å†µä¸‹å®ç°äº†å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œ 2D/3D åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­å¿«é€Ÿä¸”ç¨€ç–çš„åŠ¨æ€å˜åŒ–ï¼ŒEgoLifter é‡‡ç”¨ç¬æ€é¢„æµ‹ç½‘ç»œæ¥æ»¤é™¤ç¬æ€å¯¹è±¡å¹¶è·å¾—æ›´å‡†ç¡®çš„ 3D é‡å»ºã€‚EgoLifter åœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¼˜äºå…¶ä»–ç°æœ‰çš„åŸºå‡†ã€‚EgoLifter è·å¾—çš„è¡¨ç¤ºè¿˜å¯ä»¥ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ 3D å¯¹è±¡èµ„äº§æå–å’Œåœºæ™¯ç¼–è¾‘ï¼Œæ˜¾ç¤ºå‡ºä¸ªäººå¯ç©¿æˆ´è®¾å¤‡å’Œ AR/VR åº”ç”¨çš„å·¨å¤§æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šEgoLifter ç®—æ³•åˆ›æ–°æ€§åœ°å°† 3D é«˜æ–¯åˆ†å¸ƒä½œä¸º 3D åœºæ™¯å’Œå¯¹è±¡çš„åŸºç¡€è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ SegmentAnythingModel (SAM) çš„åˆ†å‰²æ©ç ä½œä¸ºå¼±ç›‘ç£ï¼Œä»¥å­¦ä¹ çµæ´»ä¸”å¯æç¤ºçš„å¯¹è±¡å®ä¾‹å®šä¹‰ã€‚æ­¤å¤–ï¼ŒEgoLifter è¿˜è®¾è®¡äº†ä¸€ä¸ªç¬æ€é¢„æµ‹æ¨¡å—æ¥å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­åŠ¨æ€å¯¹è±¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æ€§èƒ½ï¼šEgoLifter åœ¨ AriaDigitalTwin æ•°æ®é›†ä¸Šåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œå®šé‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŸºäºè‡ªç„¶è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è¾“å…¥çš„å¼€æ”¾ä¸–ç•Œ 3D åˆ†å‰²ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å„ç§ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ´»åŠ¨æ•°æ®é›†ä¸Šè¿è¡Œ EgoLifterï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨è§„æ¨¡åŒ– 3D è‡ªæˆ‘ä¸ºä¸­å¿ƒæ„ŸçŸ¥æ–¹é¢çš„å‰æ™¯ã€‚å·¥ä½œé‡ï¼šEgoLifter ç®—æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œå› ä¸ºå®ƒéœ€è¦ä½¿ç”¨ 3D é«˜æ–¯åˆ†å¸ƒå’Œç¬æ€é¢„æµ‹ç½‘ç»œæ¥å¤„ç†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸­çš„å¤æ‚åœºæ™¯å’ŒåŠ¨æ€å¯¹è±¡ã€‚ç„¶è€Œï¼ŒEgoLifter ç®—æ³•çš„æ€§èƒ½ä¼˜åŠ¿è¯æ˜äº†å…¶å·¥ä½œé‡çš„åˆç†æ€§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d42109c42b75a98fe02551eea274cc18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85c08cbcea83ca1fe044d4f7eb2a87b8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3eaa82aafccc95f7929829abc7e4035d.jpg" align="middle"></details><h2 id="DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing"><a href="#DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing" class="headerlink" title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing"></a>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</h2><p><strong>Authors:Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</strong></p><p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href="https://github.com/maturk/dn-splatter">https://github.com/maturk/dn-splatter</a>. </p><p><a href="http://arxiv.org/abs/2403.17822v1">PDF</a> </p><p><strong>Summary</strong><br>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æŠ€æœ¯é€šè¿‡æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯ï¼Œå¢å¼ºäº†å¯¹å®¤å†…æ•°æ®é›†çš„å‡ ä½•çº¦æŸï¼Œæå‡äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æ˜¯ä¸€ç§æ–°é¢–çš„å¯å¾®æ¸²æŸ“æŠ€æœ¯ã€‚</li><li>3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“åœ¨å®¤å†…æ•°æ®é›†ä¸Šè¡¨ç°ä¸ä½³ï¼ŒåŸå› æ˜¯ä¼˜åŒ–è¿‡ç¨‹ä¸­ç¼ºä¹å‡ ä½•çº¦æŸã€‚</li><li>é€šè¿‡æ·±åº¦ä¿¡æ¯æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¯ä»¥æ”¹å–„å®¤å†…æ•°æ®é›†çš„æ€§èƒ½ã€‚</li><li>é€šè¿‡å±€éƒ¨å¹³æ»‘å’Œæ³•çº¿ä¿¡æ¯ç›‘ç£ï¼Œå¯ä»¥å¢å¼º3Dé«˜æ–¯æ–‘ç‚¹çš„å‡ ä½•å¯¹é½ã€‚</li><li>æ”¹è¿›åçš„3Dé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“æŠ€æœ¯å¯ç›´æ¥ä»é«˜æ–¯è¡¨ç¤ºä¸­æå–ç½‘æ ¼ï¼Œç”Ÿæˆæ›´ç‰©ç†å‡†ç¡®çš„å®¤å†…åœºæ™¯é‡å»ºã€‚</li><li>è¯¥æŠ€æœ¯ä»£ç å°†åœ¨<a href="https://github.com/maturk/dn-splatterä¸Šå‘å¸ƒã€‚">https://github.com/maturk/dn-splatterä¸Šå‘å¸ƒã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDN-Splatterï¼šç”¨äºé«˜æ–¯æ•£å°„å’Œç½‘æ ¼åŒ–çš„æ·±åº¦å’Œæ³•çº¿å…ˆéªŒ</li><li>ä½œè€…ï¼šMatias Turkulainenâˆ—1, Xuqian Renâˆ—2, Iaroslav Melekhov3, Otto Seiskari4, Esa Rahtu2,4, Juho Kannala3,4</li><li>éš¶å±ï¼šè‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å°„ã€å®¤å†…é‡å»ºã€å…ˆéªŒæ­£åˆ™åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/maturk/dn-splatter</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šä¸‰ç»´é«˜æ–¯æ•£å°„æ˜¯ä¸€ç§æ–°é¢–çš„å¯å¾®æ¸²æŸ“æŠ€æœ¯ï¼Œå·²åœ¨é«˜ä¿çœŸå›¾åƒåˆæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼Œå…·æœ‰è¾ƒå¿«çš„æ¸²æŸ“é€Ÿåº¦å’Œè¾ƒçŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ç„¶è€Œï¼Œç”±äºä¼˜åŒ–è¿‡ç¨‹ä¸­ç¼ºä¹å‡ ä½•çº¦æŸï¼Œå®ƒåœ¨å®¤å†…æ•°æ®é›†å¸¸è§çš„åœºæ™¯ä¸­çš„æ€§èƒ½è¾ƒå·®ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ Nerfactoã€Depth-Nerfactoã€Neusfactoã€MonoSDFã€Splatfacto å’Œ SuGaRã€‚è¿™äº›æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ç¼ºä¹å‡ ä½•çº¦æŸï¼Œå¯¼è‡´åœ¨å®¤å†…åœºæ™¯ä¸­æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œé€šè¿‡æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯æ¥æ‰©å±•ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œä»¥è§£å†³å®¤å†…åœºæ™¯çš„æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ï¼šåˆ©ç”¨æ·±åº¦ä¿¡æ¯å¯¹ä¼˜åŒ–è¿‡ç¨‹è¿›è¡Œæ­£åˆ™åŒ–ã€å¢å¼ºé™„è¿‘é«˜æ–¯åˆ†å¸ƒçš„å±€éƒ¨å¹³æ»‘åº¦ã€åˆ©ç”¨æ³•çº¿ä¿¡æ¯ç›‘ç£ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„å‡ ä½•å½¢çŠ¶ï¼Œä»¥æ›´å¥½åœ°ä¸çœŸå®åœºæ™¯å‡ ä½•å½¢çŠ¶å¯¹é½ã€‚ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½æ–¹é¢å–å¾—äº†è¿›å±•ï¼šåœ¨å®¤å†…åœºæ™¯ä¸Šæé«˜äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆç»“æœï¼Œè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥ä»é«˜æ–¯è¡¨ç¤ºä¸­ç›´æ¥æå–ç½‘æ ¼ï¼Œä»è€Œåœ¨å®¤å†…åœºæ™¯ä¸­å®ç°æ›´ç‰©ç†å‡†ç¡®çš„é‡å»ºã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åˆ©ç”¨æ·±åº¦ä¿¡æ¯æ­£åˆ™åŒ–ä¼˜åŒ–è¿‡ç¨‹ï¼›(2): å¢å¼ºé™„è¿‘é«˜æ–¯åˆ†å¸ƒçš„å±€éƒ¨å¹³æ»‘åº¦ï¼›(3): åˆ©ç”¨æ³•çº¿ä¿¡æ¯ç›‘ç£ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„å‡ ä½•å½¢çŠ¶ï¼›(4): åˆ©ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åœºæ™¯ç›´æ¥æå–ç½‘æ ¼ï¼Œæ— éœ€é¢å¤–çš„ä¼˜åŒ–æˆ–ç»†åŒ–é˜¶æ®µã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ·±åº¦å’Œæ³•çº¿æ­£åˆ™åŒ–çš„ä¸‰ç»´é«˜æ–¯æ•£å°„æ–¹æ³•ï¼Œè¯æ˜äº†è¿™ç§ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•å¯ä»¥é€šè¿‡æé«˜å¸¸è§çš„æ–°è§†å›¾ RGB æŒ‡æ ‡ä»¥åŠæ˜¾è‘—æé«˜ä»é«˜æ–¯åœºæ™¯è¡¨ç¤ºä¸­æå–çš„æ·±åº¦ä¼°è®¡å’Œè¡¨é¢è´¨é‡æ¥å¢å¼ºç…§ç‰‡çœŸå®æ„Ÿã€‚æˆ‘ä»¬å±•ç¤ºäº†å…ˆéªŒæ­£åˆ™åŒ–å¯¹äºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…åœºæ™¯ä¸­å®ç°æ›´å‡ ä½•æœ‰æ•ˆé‡å»ºçš„å¿…è¦æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨æ·±åº¦å’Œæ³•çº¿ä¿¡æ¯æ‰©å±•ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œè§£å†³å®¤å†…åœºæ™¯çš„å‡ ä½•çº¦æŸé—®é¢˜ï¼›æ€§èƒ½ï¼šåœ¨å®¤å†…åœºæ™¯ä¸Šæé«˜äº†æ·±åº¦ä¼°è®¡å’Œæ–°è§†å›¾åˆæˆç»“æœï¼Œè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥ä»é«˜æ–¯è¡¨ç¤ºä¸­ç›´æ¥æå–ç½‘æ ¼ï¼Œä»è€Œåœ¨å®¤å†…åœºæ™¯ä¸­å®ç°æ›´ç‰©ç†å‡†ç¡®çš„é‡å»ºï¼›å·¥ä½œé‡ï¼šåˆ©ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åœºæ™¯ç›´æ¥æå–ç½‘æ ¼ï¼Œæ— éœ€é¢å¤–çš„ä¼˜åŒ–æˆ–ç»†åŒ–é˜¶æ®µã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6f13f0240c5cc6d6adeccaff39bcf966.jpg" align="middle"><img src="https://pica.zhimg.com/v2-efcb3b451413f0f8f9d4557e2ca5fe0b.jpg" align="middle"></details><h2 id="GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction"><a href="#GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction" class="headerlink" title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction"></a>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</h2><p><strong>Authors:Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</strong></p><p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry. </p><p><a href="http://arxiv.org/abs/2403.16964v1">PDF</a> Project page: <a href="https://city-super.github.io/GSDF">https://city-super.github.io/GSDF</a></p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯æ³¼æº… (3DGS) ä¸ç¥ç»ç¬¦å·è·ç¦»åœº (SDF) ç›¸ç»“åˆï¼Œå¯ç”¨äºå‘ˆç°æ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„è¡¨é¢é‡å»ºæ•ˆæœï¼Œå¹¶å¢å¼º 3DGS æ¸²æŸ“çš„ç»“æ„ï¼Œä½¿å…¶æ›´ç¬¦åˆåº•å±‚å‡ ä½•å›¾å½¢ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS å’Œç¥ç» SDF çš„ç»“åˆï¼Œå¯æå‡æ¸²æŸ“å’Œé‡å»ºæ•ˆæœã€‚</li><li>ç¥ç»ä½“ç§¯æ¸²æŸ“æŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨ç‚¹/åŸºå…ƒé¢œè‰²ï¼Œå¿½ç•¥äº†åº•å±‚åœºæ™¯å‡ ä½•å›¾å½¢ã€‚</li><li>ç¥ç»éšå¼è¡¨é¢å­¦ä¹ ï¼Œ å—ç¥ç»æ¸²æŸ“æˆåŠŸå¯å‘ã€‚</li><li>å½“å‰å·¥ä½œï¼Œé™åˆ¶å¯†åº¦åœºçš„åˆ†å¸ƒæˆ–åŸºå…ƒçš„å½¢çŠ¶ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™ï¼Œå­¦ä¹ åœºæ™¯è¡¨é¢å­˜åœ¨ç¼ºé™·ã€‚</li><li>GSDF æ¶æ„ï¼Œç»“åˆ 3DGS å’Œç¥ç» SDF çš„ä¼˜ç‚¹ï¼Œé€šè¿‡ç›¸äº’æŒ‡å¯¼å’Œè”åˆç›‘ç£ï¼Œç¼“è§£å…¶å±€é™æ€§ã€‚</li><li>GSDF è®¾è®¡ï¼Œæ›´å‡†ç¡®ã€æ›´ç²¾ç»†çš„è¡¨é¢é‡å»ºï¼ŒåŒæ—¶æé«˜ 3DGS æ¸²æŸ“çš„ç»“æ„ï¼Œä½¿å…¶æ›´ç¬¦åˆåº•å±‚å‡ ä½•å›¾å½¢ã€‚</li><li>GSDF åœ¨ä¸åŒåœºæ™¯ä¸­ï¼Œéƒ½å±•ç¤ºäº†å…¶æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šGSDFï¼š3DGS èåˆ SDFï¼Œæå‡æ¸²æŸ“å’Œé‡å»ºæ•ˆæœ</li><li>ä½œè€…ï¼šMulin Yu1âˆ—, Tao Lu1âˆ—, Linning Xu2, Lihan Jiang3, Yuanbo Xiangli4ï¿½, Bo Dai1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¥ç»åœºæ™¯æ¸²æŸ“Â·3D é«˜æ–¯ç‚¹äº‘Â·ç¥ç»æ›²é¢é‡å»º</li><li>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2403.16964ï¼ŒGithub ä»£ç ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šä»å¤šè§†è§’å›¾åƒå‘ˆç° 3D åœºæ™¯ä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­ä¸€é¡¹æ ¸å¿ƒä¸”é•¿æœŸçš„æŒ‘æˆ˜ã€‚ä¸»è¦åŒ…å«æ¸²æŸ“å’Œé‡å»ºä¸¤ä¸ªè¦æ±‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡é€šå¸¸é€šè¿‡ç¥ç»ä½“ç§¯æ¸²æŸ“æŠ€æœ¯å®ç°ï¼Œè¯¥æŠ€æœ¯ä¾èµ–äºèšåˆçš„ç‚¹/åŸºå…ƒé¢œè‰²ï¼Œè€Œå¿½ç•¥äº†åº•å±‚åœºæ™¯å‡ ä½•ã€‚   ï¼ˆ2ï¼‰ï¼šç¥ç»éšå¼æ›²é¢çš„å­¦ä¹ æºäºç¥ç»æ¸²æŸ“çš„æˆåŠŸã€‚å½“å‰å·¥ä½œè¦ä¹ˆé™åˆ¶å¯†åº¦åœºçš„åˆ†å¸ƒæˆ–åŸºå…ƒçš„å½¢çŠ¶ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ä¸‹é™å’Œå­¦ä¹ åœºæ™¯æ›²é¢ä¸Šçš„ç¼ºé™·ã€‚æ­¤ç±»æ–¹æ³•çš„æœ‰æ•ˆæ€§å—åˆ°æ‰€é€‰ç¥ç»è¡¨ç¤ºçš„å›ºæœ‰çº¦æŸçš„é™åˆ¶ï¼Œéš¾ä»¥æ•æ‰ç²¾ç»†çš„æ›²é¢ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ›´å¤§ã€æ›´å¤æ‚çš„åœºæ™¯ã€‚   ï¼ˆ3ï¼‰ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† GSDFï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŒåˆ†æ”¯æ¶æ„ï¼Œå®ƒç»“åˆäº†çµæ´»ä¸”é«˜æ•ˆçš„ 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰è¡¨ç¤ºä¸ç¥ç»ç¬¦å·è·ç¦»åœºï¼ˆSDFï¼‰çš„ä¼˜ç‚¹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å’Œå¢å¼ºæ¯ä¸ªåˆ†æ”¯çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é€šè¿‡ç›¸äº’æŒ‡å¯¼å’Œè”åˆç›‘ç£æ¥å‡è½»å®ƒä»¬çš„é™åˆ¶ã€‚æˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾è®¡é‡Šæ”¾äº†æ›´å‡†ç¡®å’Œè¯¦ç»†çš„æ›²é¢é‡å»ºçš„æ½œåŠ›ï¼ŒåŒæ—¶ä½¿ 3DGS æ¸²æŸ“å—ç›Šäºä¸åº•å±‚å‡ ä½•æ›´ä¸€è‡´çš„ç»“æ„ã€‚   ï¼ˆ4ï¼‰ï¼šåœ¨ä¸åŒçš„åœºæ™¯å’Œä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1): æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œå…¶ä¸­GSåˆ†æ”¯ä¸“æ³¨äºé«˜æ•ˆã€é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œè€ŒSDFåˆ†æ”¯ä¸“æ³¨äºå­¦ä¹ ç¥ç»éšå¼GSDFã€‚(2): æˆ‘ä»¬æœ‰æ•ˆåœ°ä¿ç•™äº†é«˜æ–¯åŸºå…ƒæ¸²æŸ“çš„æ•ˆç‡å’Œä¿çœŸåº¦ä¼˜åŠ¿ï¼Œå¹¶ä»NeuS[29]æ”¹ç¼–çš„SDFåœºä¸­æ›´å‡†ç¡®åœ°é€¼è¿‘åœºæ™¯è¡¨é¢ã€‚(3): æˆ‘ä»¬åˆ©ç”¨GSåˆ†æ”¯çš„æ•ˆç‡å’Œçµæ´»æ€§ä¼˜åŠ¿ï¼Œæ¸²æŸ“æ·±åº¦å›¾å¹¶æŒ‡å¯¼SDFåˆ†æ”¯çš„å…‰çº¿é‡‡æ ·è¿‡ç¨‹ã€‚(4): æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªSDFåˆ†æ”¯çš„é¢„æµ‹SDFå€¼æ¥æŒ‡å¯¼GSåˆ†æ”¯çš„å¯†åº¦æ§åˆ¶ï¼Œåœ¨è¿‘è¡¨é¢åŒºåŸŸç”Ÿé•¿é«˜æ–¯åŸºå…ƒï¼Œå¹¶å‰ªé™¤è¿œç¦»è¡¨é¢çš„åŸºå…ƒã€‚(5): æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ¥è‡ªæ¯ä¸ªåˆ†æ”¯çš„æ·±åº¦å›¾å’Œæ³•çº¿å›¾æ¥è¿›ä¸€æ­¥å¢å¼ºç›¸äº’å‡ ä½•ä¸€è‡´æ€§ï¼Œä»¥é¼“åŠ±é«˜æ–¯åŸºå…ƒå’Œè¡¨é¢ä¹‹é—´æ›´ä¸€è‡´çš„ç‰©ç†å¯¹é½ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œåˆ©ç”¨äº† 3D-GS å’Œ SDF çš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¿æŒè®­ç»ƒå’Œæ¨ç†æ•ˆç‡çš„åŒæ—¶ï¼Œåœ¨æ¸²æŸ“å’Œé‡å»ºè´¨é‡ä¸Šå–å¾—æå‡çš„æ½œåŠ›ã€‚ä¸¤ç§éšå¼è¡¨ç¤ºã€æ¸²æŸ“æ–¹æ³•å’Œç›‘ç£æŸå¤±çš„å›ºæœ‰å·®å¼‚å¯¹ä¸¤è€…æ— ç¼é›†æˆæå‡ºäº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§åŒå‘ç›¸äº’æŒ‡å¯¼æ–¹æ³•æ¥è§„é¿è¿™äº›é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­å¼•å…¥äº†å¹¶éªŒè¯äº†ä¸‰ç§æŒ‡å¯¼ï¼š1ï¼‰æ·±åº¦å¼•å¯¼é‡‡æ ·ï¼ˆGSâ†’SDFï¼‰ï¼Œ2ï¼‰å‡ ä½•æ„ŸçŸ¥é«˜æ–¯å¯†åº¦æ§åˆ¶ï¼ˆSDFâ†’GSï¼‰ï¼›3ï¼‰ç›¸äº’å‡ ä½•ç›‘ç£ï¼ˆGSâ†”SDFï¼‰ã€‚æˆ‘ä»¬å¹¿æ³›çš„ç»“æœè¯æ˜äº†åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šçš„æ•ˆç‡å’Œè”åˆæ€§èƒ½æ”¹è¿›ã€‚ç”±äºè¿™ä¸¤ä¸ªåˆ†æ”¯ä¿æŒäº†å®ƒä»¬çš„åŸå§‹æ¶æ„ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æœŸé—´ä¿æŒäº†å®ƒä»¬çš„æ•ˆç‡ï¼Œä¸ºå°†æ¥é€šè¿‡æ›´é«˜çº§çš„æ¨¡å‹æ›¿æ¢æ¯ä¸ªåˆ†æ”¯ç•™å‡ºäº†æ½œåœ¨çš„å¢å¼ºç©ºé—´ã€‚æˆ‘ä»¬è®¾æƒ³æˆ‘ä»¬çš„æ¨¡å‹å°†æœ‰åˆ©äºå¯¹é«˜è´¨é‡æ¸²æŸ“å’Œå‡ ä½•æœ‰è¦æ±‚çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å…·èº«ç¯å¢ƒã€ç‰©ç†æ¨¡æ‹Ÿå’Œæ²‰æµ¸å¼ VR ä½“éªŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŒåˆ†æ”¯æ¡†æ¶ï¼Œç»“åˆäº† 3D-GS å’Œ SDF çš„ä¼˜ç‚¹ï¼Œæé«˜äº†æ¸²æŸ“å’Œé‡å»ºè´¨é‡ï¼›æ€§èƒ½ï¼šåœ¨æ¸²æŸ“å’Œé‡å»ºè´¨é‡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½ä¿æŒäº†è¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-845f4824f5b5d708e26e78764b0f6c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-264655e62e1548d0343d272dca0f7812.jpg" align="middle"><img src="https://picx.zhimg.com/v2-be53d57d9316fa9c7ed994d73a3dddc1.jpg" align="middle"></details><h2 id="latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction"><a href="#latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction" class="headerlink" title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction"></a>latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction</h2><p><strong>Authors:Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</strong></p><p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data. </p><p><a href="http://arxiv.org/abs/2403.16292v1">PDF</a> Project website: <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">https://geometric-rl.mpi-inf.mpg.de/latentsplat/</a></p><p><strong>Summary</strong><br>é€šè¿‡å°†å›å½’æ¨¡å‹ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼ŒlatentSplat èƒ½å¤Ÿä½¿ç”¨ç”± 3D ç‰¹å¾é«˜æ–¯åˆ†å¸ƒç»„æˆçš„æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰é«˜æ–¯åˆ†å¸ƒé¢„æµ‹å¿«é€Ÿæ¨ç†é«˜åˆ†è¾¨ç‡æ–°è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• latentSplatï¼Œå¯ä»¥é¢„æµ‹ 3D æ½œåœ¨ç©ºé—´ä¸­çš„è¯­ä¹‰é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è½»é‡çº§ç”Ÿæˆ 2D æ¶æ„è¿›è¡Œ splatting å’Œè§£ç ã€‚</li><li>latentSplat å°†å›å½’æ–¹æ³•ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨åŒä¸€ä¸ªæ–¹æ³•ä¸­å®ç°äº†å¿«é€Ÿæ¨ç†é«˜åˆ†è¾¨ç‡æ–°è§†å›¾å’Œ 360 åº¦æ³›åŒ–çš„èƒ½åŠ›ã€‚</li><li>latentSplat çš„æ ¸å¿ƒæ˜¯åŸºäºå˜åˆ† 3D é«˜æ–¯åˆ†å¸ƒï¼Œè¯¥è¡¨ç¤ºæœ‰æ•ˆåœ°å¯¹æ½œåœ¨ç©ºé—´ä¸­åŒ…å« 3D ç‰¹å¾é«˜æ–¯åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§è¿›è¡Œç¼–ç ã€‚</li><li>å¯ä»¥ä»è¿™äº›é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ç‰¹å®šå®ä¾‹å¹¶é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ splatting å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç ç½‘ç»œè¿›è¡Œæ¸²æŸ“ã€‚</li><li>latentSplat åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å‰çš„å·¥ä½œï¼ŒåŒæ—¶å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å¿«é€Ÿä¸”å¯æ‰©å±•ã€‚</li><li>latentSplat ä¸éœ€è¦æ˜¾å¼ä½“ç§¯æ¸²æŸ“ï¼Œå› æ­¤å¯¹äºé«˜åˆ†è¾¨ç‡åœºæ™¯å…·æœ‰æ•ˆç‡ä¼˜åŠ¿ã€‚</li><li>latentSplatä»…ä½¿ç”¨ç°æˆçš„çœŸå®è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€ 3D æ‰«ææˆ–é‡å»ºæ•°æ®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šlatentSplatï¼šå¿«é€Ÿæ³›åŒ– 3D é‡å»ºçš„è‡ªåŠ¨ç¼–ç å˜åˆ†é«˜æ–¯</li><li>ä½œè€…ï¼šChristopher Wewerã€Kevin Rajã€Eddy Ilgã€Bernt Schieleã€Jan Eric Lenssen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé©¬æ™®å­¦ä¼šä¿¡æ¯å­¦ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€æ–°è§†è§’åˆæˆã€ç‰¹å¾é«˜æ–¯ä½“ç´ åŒ–ã€é«˜æ•ˆ 3D è¡¨å¾å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ³›åŒ– 3D é‡å»ºæ–¹æ³•è¦ä¹ˆç”±äºä½“ç»˜åˆ¶å›¾é€Ÿåº¦æ…¢è€Œæ— æ³•å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’ï¼Œè¦ä¹ˆä»…é™äºæ’å€¼æ¥è¿‘è¾“å…¥è§†è§’ï¼Œå³ä½¿åœ¨ä»…æœ‰å•ä¸ªä¸­å¿ƒç‰©ä½“çš„ç®€å•åœºæ™¯ä¸­ï¼Œä¹Ÿæ— æ³•è¿›è¡Œ 360 åº¦æ³›åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼šè¦ä¹ˆæ— æ³•å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’ï¼Œè¦ä¹ˆä»…é™äºæ’å€¼æ¥è¿‘è¾“å…¥è§†è§’ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆå›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨åŒä¸€æ–¹æ³•ä¸­æœç€è¿™ä¸¤ç§èƒ½åŠ›è¿ˆè¿›ï¼Œå®Œå…¨åœ¨å®¹æ˜“è·å–çš„çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å˜åˆ† 3D é«˜æ–¯ï¼Œè¿™æ˜¯ä¸€ç§è¡¨å¾ï¼Œå¯æœ‰æ•ˆç¼–ç æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§ã€‚ä»è¿™äº›é«˜æ–¯ä½“ç´ ä¸­ï¼Œå¯ä»¥é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œå¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼ŒlatentSplat åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å¾€å·¥ä½œï¼ŒåŒæ—¶å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å…·æœ‰å¿«é€Ÿæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li></ol><p>7.Methods:(1):latentSplatæ–¹æ³•ç»“åˆäº†å›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹ï¼Œåœ¨åŒä¸€æ–¹æ³•ä¸­æœç€å¿«é€Ÿæ¨æ–­é«˜åˆ†è¾¨ç‡æ–°è§†è§’å’Œ360åº¦æ³›åŒ–ä¸¤æ–¹é¢è¿ˆè¿›ï¼›(2):æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å˜åˆ†3Dé«˜æ–¯ï¼Œå®ƒæ˜¯ä¸€ç§è¡¨å¾ï¼Œå¯æœ‰æ•ˆç¼–ç æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§ï¼›(3):ä»è¿™äº›é«˜æ–¯ä½“ç´ ä¸­ï¼Œå¯ä»¥é€šè¿‡é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œå¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚</p><ol><li><strong>ç»“è®º</strong>(1): latentSplat æ˜¯ä¸€ç§å°†å›å½’æ–¹æ³•å’Œç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿æˆåŠŸç»“åˆèµ·æ¥çš„æ–¹æ³•ï¼Œä»¥å¤„ç†ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†å›¾åˆæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒè´¨é‡ï¼ŒåŒæ—¶æä¾›äº†ä¸çœŸå®æƒ…å†µæœ€é«˜çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚ä¸ä¹‹å‰çš„ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼ŒlatentSplat çš„é€Ÿåº¦æ›´å¿«ï¼Œå¯æ‰©å±•æ€§æ›´å¼ºï¼Œèƒ½å¤Ÿä»¥æ›´é«˜çš„åˆ†è¾¨ç‡è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong></li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¡¨å¾â€”â€”å˜åˆ† 3D é«˜æ–¯ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°å¯¹æ½œä¼ç©ºé—´ä¸­ä¸åŒç‰¹å¾é«˜æ–¯ä½“ç´ çš„ä¸ç¡®å®šæ€§è¿›è¡Œç¼–ç ã€‚</li><li>è®¾è®¡äº†ä¸€ç§é«˜æ•ˆçš„é«˜æ–¯ä½“ç´ åŒ–å’Œå¿«é€Ÿçš„ç”Ÿæˆè§£ç å™¨ç½‘ç»œï¼Œå¯ä»¥ä»é«˜æ–¯ä½“ç´ ä¸­å¯¹ç‰¹å®šå®ä¾‹è¿›è¡Œé‡‡æ ·å’Œæ¸²æŸ“ã€‚<strong>æ€§èƒ½ï¼š</strong></li><li>åœ¨é‡å»ºè´¨é‡å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä»¥å¾€çš„å·¥ä½œã€‚</li><li>å¯¹é«˜åˆ†è¾¨ç‡æ•°æ®å…·æœ‰å¿«é€Ÿæ€§å’Œå¯æ‰©å±•æ€§ã€‚<strong>å·¥ä½œé‡ï¼š</strong></li><li>è¯¥æ–¹æ³•å®Œå…¨åœ¨å®¹æ˜“è·å–çš„çœŸå®è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦éƒ½å¾ˆå¿«ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-812603706bcb6f004a93be35208c508e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-472f1454ee4fe157880ee415da76b6fb.jpg" align="middle"></details><h2 id="CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field"><a href="#CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field" class="headerlink" title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field"></a>CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field</h2><p><strong>Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a>. </p><p><a href="http://arxiv.org/abs/2403.16095v1">PDF</a> Project Page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a></p><p><strong>æ‘˜è¦</strong><br>åŸºäºé«˜ä¸€è‡´æ€§å’Œå‡ ä½•ç¨³å®šæ€§çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥3Dé«˜æ–¯åœºï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¯†é›†RGB-D SLAMç³»ç»Ÿï¼Œå³CG-SLAMã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>åœ¨é«˜æ–¯æ•£å°„çš„åŸºç¡€ä¸Šï¼Œæå‡ºäº†æ„å»ºé€‚åˆäºè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºçš„æŠ€æœ¯ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„3Dé«˜æ–¯åŸºå…ƒï¼Œä»è€Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œç²¾åº¦ã€‚</li><li>CG-SLAMåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ä¼˜å¼‚ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾15 Hzã€‚</li><li>è¯¥ç ”ç©¶å›¢é˜Ÿå°†å…¬å¼€æä¾›æºä»£ç ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCG-SLAMï¼šåŸºäºä¸€è‡´æ€§ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„æœ‰æ•ˆç¨ å¯† RGB-DSLAM</li><li>ä½œè€…ï¼šèƒ¡å˜‰ç‘ï¼Œé™ˆæ˜¾æµ©ï¼Œå†¯åšå¯…ï¼Œæå¹¿æ—ï¼Œæ¨è‰¯æ™¶ï¼Œé²è™å†›ï¼Œå¼ å›½é”‹ï¼Œå´”å…†é¹</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¨ å¯†è§†è§‰ SLAMï¼Œç¥ç»æ¸²æŸ“ï¼Œ3D é«˜æ–¯åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16095    Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š    ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨è¡¨é¢é‡å»ºå’Œæ–°è§†è§’åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç°æœ‰çš„åŸºäº NeRF çš„æ–¹æ³•å› å…¶è®¡ç®—å¯†é›†ä¸”è€—æ—¶çš„ä½“ç§¯æ¸²æŸ“ç®¡é“è€Œå—åˆ°é˜»ç¢ã€‚    ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ NeRF-SLAM æ–¹æ³•å­˜åœ¨è®¡ç®—é‡å¤§ã€æ¸²æŸ“æ•ˆç‡ä½çš„é—®é¢˜ã€‚    ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„é«˜æ•ˆç¨ å¯† RGB-DSLAM ç³»ç»Ÿ CG-SLAMã€‚é€šè¿‡å¯¹é«˜æ–¯ splatting çš„æ·±å…¥åˆ†æï¼Œæå‡ºäº†æ„å»ºé€‚åˆè·Ÿè¸ªå’Œå»ºå›¾çš„ä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºçš„æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œä»¥ç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€‰æ‹©æœ‰ä»·å€¼çš„é«˜æ–¯åŸºå…ƒï¼Œä»è€Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚    ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCG-SLAM å®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ï¼Œè·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚</p></li><li><p>Methods:(1) åˆ†æé«˜æ–¯splattingï¼Œæå‡ºæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„3Dé«˜æ–¯åœºçš„æŠ€æœ¯ï¼›(2) æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ï¼›(3) è®¾è®¡é«˜æ•ˆçš„è·Ÿè¸ªå’Œå»ºå›¾ç®—æ³•ï¼Œå®ç°15Hzçš„è·Ÿè¸ªé€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº† CG-SLAMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸€è‡´ä¸”ä¸ç¡®å®šæ€§æ„ŸçŸ¥ 3D é«˜æ–¯åœºçš„ç¨ å¯† RGB-DSLAMã€‚æˆ‘ä»¬æœ‰é’ˆå¯¹æ€§çš„æŸå¤±å‡½æ•°åŠ å¼ºäº† 3D é«˜æ–¯åœºçš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚ä¸ç¡®å®šæ€§æ¨¡å‹è¿›ä¸€æ­¥æç‚¼äº†è¯¥åœºä¸­ä¿¡æ¯ä¸°å¯Œçš„åŸºå…ƒï¼Œä»¥å‡å°‘å¹²æ‰°ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºæ„å»ºä¸€è‡´ä¸”ç¨³å®šçš„ 3D é«˜æ–¯åœºçš„é«˜æ–¯ splatting åˆ†ææŠ€æœ¯ã€‚</li><li>æå‡ºæ·±åº¦ä¸ç¡®å®šæ€§æ¨¡å‹ï¼Œæé«˜è·Ÿè¸ªæ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li><li>è®¾è®¡é«˜æ•ˆçš„è·Ÿè¸ªå’Œå»ºå›¾ç®—æ³•ï¼Œå®ç° 15Hz çš„è·Ÿè¸ªé€Ÿåº¦ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„è·Ÿè¸ªå’Œå»ºå›¾æ€§èƒ½ã€‚</li><li>è·Ÿè¸ªé€Ÿåº¦é«˜è¾¾ 15Hzã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°å·¥ä½œé‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting"><a href="#Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting" class="headerlink" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting"></a>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</strong></p><p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks &amp; Temples datasets. </p><p><a href="http://arxiv.org/abs/2403.15530v1">PDF</a> </p><p><strong>Summary</strong><br>æˆ‘ä»¬åœ¨3DGSæ–¹æ³•ä¸­å¼•å…¥åƒç´ è¦†ç›–ä¿¡æ¯ï¼Œå¼•å¯¼é«˜æ–¯æ ¸åŠ¨æ€å¹³å‡æ¢¯åº¦ï¼Œä¿ƒè¿›äº†å¤§é«˜æ–¯æ ¸çš„ç”Ÿé•¿ï¼Œæœ‰æ•ˆæŠ‘åˆ¶æµ®ç‚¹å’Œé’ˆçŠ¶ä¼ªå½±ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3DGSä¾èµ–äºé«˜è´¨é‡çš„åˆå§‹ç‚¹äº‘ï¼Œä½†ç°æœ‰çš„ç”Ÿé•¿å‡†åˆ™å­˜åœ¨ä¸è¶³ã€‚</li><li>Pixel-GSé‡‡ç”¨åƒç´ è¦†ç›–ä¿¡æ¯åŠ¨æ€å¹³å‡æ¢¯åº¦ï¼Œä¿ƒè¿›å¤§é«˜æ–¯æ ¸ç”Ÿé•¿ã€‚</li><li>ç”±äºé«˜æ–¯æ ¸è¦†ç›–åƒç´ å°‘ï¼Œå¯¼è‡´åˆå§‹ç‚¹äº‘ç¨€ç–åŒºåŸŸç”Ÿé•¿ä¸è¶³ã€‚</li><li>Pixel-GSæœ‰æ•ˆä¿ƒè¿›äº†ç¨€ç–åŒºåŸŸçš„ç‚¹äº‘ç”Ÿé•¿ï¼Œæé«˜é‡å»ºç²¾åº¦å’Œç»†èŠ‚ã€‚</li><li>Pixel-GSé‡‡ç”¨ç®€å•æœ‰æ•ˆçš„ç¼©æ”¾ç­–ç•¥æŠ‘åˆ¶è¿‘æ‘„åƒæœºå¤„çš„æµ®ç‚¹ç”Ÿé•¿ã€‚</li><li>åœ¨Mip-NeRF 360å’ŒTanks &amp; Templesæ•°æ®é›†ä¸Šï¼ŒPixel-GSå–å¾—äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šPixel-GSï¼šåŸºäºåƒç´ çš„æ¢¯åº¦æ§åˆ¶ 3D é«˜æ–¯æ•£ç‚¹å›¾å¯†åº¦æ§åˆ¶</li><li>ä½œè€…ï¼šZheng Zhangã€Wenbo Huã€Yixing Laoã€Tong Heã€Hengshuang Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆã€åŸºäºç‚¹çš„è¾å°„åœºã€å®æ—¶æ¸²æŸ“ã€3D é«˜æ–¯æ•£ç‚¹å›¾ã€è‡ªé€‚åº”å¯†åº¦æ§åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2403.15530.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://pixelgs.github.io</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£ç‚¹å›¾ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºåˆå§‹ç‚¹äº‘çš„è´¨é‡ï¼Œå¯¼è‡´åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸå‡ºç°æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»…è€ƒè™‘æ¥è‡ªå¯è§‚å¯Ÿè§†è§’çš„ç‚¹çš„å¹³å‡æ¢¯åº¦å¤§å°ï¼Œæ— æ³•é’ˆå¯¹ä»å¤šä¸ªè§†è§’å¯è§‚å¯Ÿä½†ä»…åœ¨è¾¹ç•Œè¦†ç›–çš„å¤§é«˜æ–¯è¿›è¡Œç”Ÿé•¿ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡º Pixel-GSï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶è€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶æ€§èƒ½ï¼šåœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®æ—¶é€Ÿåº¦çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) Pixel-GSæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶ï¼Œè€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚(2) å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚(3) æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚</p></li></ol><p><strong>æ‘˜è¦</strong>(1) ç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯æ•£ç‚¹å›¾ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºåˆå§‹ç‚¹äº‘çš„è´¨é‡ï¼Œå¯¼è‡´åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸå‡ºç°æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»…è€ƒè™‘æ¥è‡ªå¯è§‚å¯Ÿè§†è§’çš„ç‚¹çš„å¹³å‡æ¢¯åº¦å¤§å°ï¼Œæ— æ³•é’ˆå¯¹ä»å¤šä¸ªè§†è§’å¯è§‚å¯Ÿä½†ä»…åœ¨è¾¹ç•Œè¦†ç›–çš„å¤§é«˜æ–¯è¿›è¡Œç”Ÿé•¿ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡º Pixel-GSï¼Œä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œåœ¨è®¡ç®—ç”Ÿé•¿æ¡ä»¶æ—¶è€ƒè™‘é«˜æ–¯åœ¨æ¯ä¸ªè§†å›¾ä¸­è¦†ç›–çš„åƒç´ æ•°é‡ã€‚å°†è¦†ç›–åƒç´ æ•°é‡è§†ä¸ºæƒé‡ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä»è€Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œæ ¹æ®åˆ°ç›¸æœºçš„è·ç¦»ç¼©æ”¾æ¢¯åº¦åœºï¼Œä»¥æŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚(4) æ–¹æ³•åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠå…¶æ€§èƒ½ï¼šåœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒå®æ—¶é€Ÿåº¦çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p><strong>ç»“è®º</strong>(1) æœ¬æ–‡æå‡ºçš„ Pixel-GS æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº† 3DGS ä¸­æ¨¡ç³Šå’Œé’ˆçŠ¶ä¼ªå½±çš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¸²æŸ“è´¨é‡ã€‚(2) <strong>åˆ›æ–°ç‚¹ï¼š</strong>    - æå‡ºäº†ä¸€ç§åŸºäºåƒç´ çš„æ¢¯åº¦æ§åˆ¶ç­–ç•¥ï¼ŒåŠ¨æ€å¹³å‡æ¥è‡ªä¸åŒè§†å›¾çš„æ¢¯åº¦ï¼Œä¿ƒè¿›å¤§é«˜æ–¯çš„ç”Ÿé•¿ã€‚    - å¼•å…¥äº†ä¸€ç§ç¼©æ”¾æ¢¯åº¦åœºçš„ç­–ç•¥ï¼ŒæŠ‘åˆ¶ç›¸æœºé™„è¿‘æµ®ç‚¹çš„ç”Ÿé•¿ã€‚(3) <strong>æ€§èƒ½ï¼š</strong>    - åœ¨ Mip-NeRF360 å’Œ Tanks&amp;Temples æ•°æ®é›†ä¸Šï¼ŒPixel-GS åœ¨ä¿æŒå®æ—¶æ¸²æŸ“é€Ÿåº¦çš„å‰æä¸‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ã€‚(4) <strong>å·¥ä½œé‡ï¼š</strong>    - Pixel-GS åœ¨è®¡ç®—é‡æ–¹é¢ç•¥é«˜äº 3DGSï¼Œä½†å…¶äº§ç”Ÿçš„é¢å¤–ç‚¹ä¸»è¦åˆ†å¸ƒåœ¨åˆå§‹åŒ–ç‚¹ä¸è¶³çš„åŒºåŸŸï¼Œå¯¹æ¸²æŸ“è´¨é‡çš„æå‡æ˜¯æ˜¾è‘—çš„ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d4b11b128f45358d4cf4adf961723c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-635e0fe3c1c48a4c71290f6c82110aeb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b9013d1f734301c423951ce8529a42eb.jpg" align="middle"></details>## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic   Surgeries using Gaussian Splatting**Authors:Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen**Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com [PDF](http://arxiv.org/abs/2403.15124v1) **Summary**è…¹è…”å†…åŒ»å­¦æˆåƒè®¾å¤‡çš„ç²¾ç¡®æ‘„åƒå¤´è¿½è¸ªã€é«˜ä¿çœŸ 3D ç»„ç»‡é‡å»ºå’Œå®æ—¶åœ¨çº¿å¯è§†åŒ–è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„ SLAM æ–¹æ³•åœ¨å®ç°å®Œæ•´çš„é«˜è´¨é‡å¤–ç§‘æ‰‹æœ¯è§†é‡é‡å»ºå’Œé«˜æ•ˆè®¡ç®—æ–¹é¢å¾€å¾€åŠ›ä¸ä»å¿ƒã€‚**Key Takeaways**- EndoGSLAM æ˜¯ä¸€ç§é’ˆå¯¹å†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†æµçº¿å‹çš„ Gaussian è¡¨ç¤ºå’Œå¯å¾®çš„å…‰æ …åŒ–ï¼Œä»¥åœ¨åœ¨çº¿æ‘„åƒå¤´è¿½è¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡æ¯ç§’ 100 å¸§çš„æ¸²æŸ“é€Ÿåº¦ã€‚- ä¸ä¼ ç»Ÿçš„æˆ–ç¥ç»ç½‘ç»œ SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ï¼Œåœ¨å†…çª¥é•œæ‰‹æœ¯ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚- EndoGSLAM åˆ©ç”¨äº†ä¸€ç§æ–°çš„ç½‘ç»œç»“æ„â€”â€”å¯å¾®å…‰æ …åŒ–å™¨ï¼Œå°† 3D è¡¨é¢éšå¼è¡¨ç¤ºä¸º 2D è¾“å…¥å›¾åƒçš„æ·±åº¦å€¼ã€‚- å¯å¾®å…‰æ …åŒ–å™¨èƒ½å¤Ÿä»¥ä½è®¡ç®—æˆæœ¬ç«¯åˆ°ç«¯åœ°ä¼˜åŒ–åœºæ™¯å‡ ä½•å½¢çŠ¶å’Œæ‘„åƒæœºå§¿æ€ã€‚- EndoGSLAM ä½¿ç”¨äº†ä¸€ç§è½»é‡çº§çš„é«˜æ–¯è¿‡ç¨‹éšå¼è¡¨é¢ï¼Œé€šè¿‡å¯¹é«˜ç»´åœºæ™¯å‡ ä½•è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†å‡†ç¡®ä¸”ç´§å‡‘çš„ 3D åœºæ™¯é‡å»ºã€‚- EndoGSLAM åˆ©ç”¨ä¸€ç§ç§°ä¸ºæ›²é¢ä¼ æ’­çš„æ–°å‹æ›²é¢ä¼ æ’­ç®—æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œé«˜ä¿çœŸ 3D åœºæ™¯é‡å»ºã€‚- EndoGSLAM åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å†…çª¥é•œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨æœ¯ä¸­å¯ç”¨æ€§ã€é‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šEndoGSLAMï¼šå†…çª¥é•œæ‰‹æœ¯ä¸­åŸºäºé«˜æ–¯æ¸²æŸ“çš„é«˜æ•ˆå®æ—¶ç¨ å¯†é‡å»º</li><li>ä½œè€…ï¼šç‹å‡¯ä»¤<em>ã€æ¨æ™¨</em>ã€ç‹å²³æµ©ã€ææ€åŒ¡ã€ç‹å²©ã€çª¦ç¥ºã€æ¨è‚–åº·ã€æ²ˆä¼Ÿâ€ </li><li>éš¶å±å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ã€äººå·¥æ™ºèƒ½å­¦é™¢</li><li>å…³é”®è¯ï¼šå†…çª¥é•œæ‰‹æœ¯ã€SLAMã€å®æ—¶æ¸²æŸ“ã€ç»„ç»‡é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.15124</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå†…çª¥é•œæ‰‹æœ¯ä¸­ï¼Œç²¾ç¡®çš„ç›¸æœºè·Ÿè¸ªã€é«˜ä¿çœŸ 3D ç»„ç»‡é‡å»ºå’Œå®æ—¶åœ¨çº¿å¯è§†åŒ–å¯¹äºæé«˜æ‰‹æœ¯å®‰å…¨æ€§ã€æ•ˆç‡è‡³å…³é‡è¦ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„ SLAM æ–¹æ³•éš¾ä»¥åŒæ—¶å®ç°å®Œæ•´é«˜è´¨é‡çš„æ‰‹æœ¯è§†é‡é‡å»ºå’Œé«˜æ•ˆè®¡ç®—ï¼Œé™åˆ¶äº†å…¶åœ¨å†…çª¥é•œæ‰‹æœ¯ä¸­çš„åº”ç”¨ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º EndoGSLAMï¼Œä¸€ç§ç”¨äºå†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†ç²¾ç®€çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œå¯åœ¨åœ¨çº¿ç›¸æœºè·Ÿè¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡ 100fps çš„æ¸²æŸ“é€Ÿåº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæˆ–ç¥ç» SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„å†…çª¥é•œæ‰‹æœ¯æ½œåŠ›ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é€šè¿‡æ”¹è¿›çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œæå‡º EndoGSLAM æ–¹æ³•ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨å¯å¾®æ¸²æŸ“è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œä¼˜åŒ–ç›¸æœºå§¿æ€ï¼›ï¼ˆ3ï¼‰é€šè¿‡æ‰©å±•é«˜æ–¯è¡¨ç¤ºï¼Œè¡¥å……åœºæ™¯ä¿¡æ¯ï¼›ï¼ˆ4ï¼‰é‡‡ç”¨å±€éƒ¨ä¼˜åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æ‰©å±•çš„é«˜æ–¯è¡¨ç¤ºã€‚</p></li></ol><p><strong>8. ç»“è®º</strong>(1): EndoGSLAM æ˜¯ä¸€ç§ç”¨äºå†…çª¥é•œæ‰‹æœ¯çš„é«˜æ•ˆ SLAM æ–¹æ³•ï¼Œå®ƒé›†æˆäº†ç²¾ç®€çš„é«˜æ–¯è¡¨ç¤ºå’Œå¯å¾®æ¸²æŸ“ï¼Œå¯åœ¨åœ¨çº¿ç›¸æœºè·Ÿè¸ªå’Œç»„ç»‡é‡å»ºæœŸé—´å®ç°è¶…è¿‡ 100fps çš„æ¸²æŸ“é€Ÿåº¦ï¼Œåœ¨æœ¯ä¸­å¯ç”¨æ€§å’Œé‡å»ºè´¨é‡ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„å†…çª¥é•œæ‰‹æœ¯æ½œåŠ›ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯è¡¨ç¤ºï¼Œå¯ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºåœºæ™¯å‡ ä½•ä¿¡æ¯ï¼›åˆ©ç”¨å¯å¾®æ¸²æŸ“è¿›è¡Œæ¢¯åº¦ä¼˜åŒ–ï¼Œä¼˜åŒ–ç›¸æœºå§¿æ€ï¼›é‡‡ç”¨å±€éƒ¨ä¼˜åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–æ‰©å±•çš„é«˜æ–¯è¡¨ç¤ºã€‚æ€§èƒ½ï¼šä¸ä¼ ç»Ÿæˆ–ç¥ç» SLAM æ–¹æ³•ç›¸æ¯”ï¼ŒEndoGSLAM åœ¨é‡å»ºè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢éƒ½å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šEndoGSLAM çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„å†…çª¥é•œç³»ç»Ÿé›†æˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9d057be5f832b3e03f093e080cdab45a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5b928bbe4980e4f0920a7da14a03655.jpg" align="middle"><img src="https://pica.zhimg.com/v2-51aeb80d1b37a5bd4a8b984b3c6b5838.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e58b985d3822ba88d3729dcbc837db5.jpg" align="middle"></details>## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**Authors:Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao**Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video. [PDF](http://arxiv.org/abs/2403.14939v1) **Summary**æ—¶ç©ºä¸€è‡´æ€§å››ç»´å†…å®¹ç”Ÿæˆæ–°æ¡†æ¶ï¼šSTAG4Dï¼Œèåˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œæ— éœ€æ‰©æ•£ç½‘ç»œé¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚**Key Takeaways**- STAG4D æ¡†æ¶ï¼Œèåˆé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ä¸‰ç»´é«˜æ–¯æ•£å°„ï¼Œç”¨äºé«˜ä¿çœŸå››ç»´ç”Ÿæˆã€‚- é‡‡ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤šè§†å›¾å›¾åƒï¼Œä½œä¸ºè¾“å…¥è§†é¢‘å¸§çš„é”šç‚¹ã€‚- å¼•å…¥èåˆç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§ä½œä¸ºè‡ªæˆ‘æ³¨æ„è®¡ç®—ä¸­çš„æ—¶é—´é”šç‚¹ï¼Œç¡®ä¿å¤šè§†å›¾åºåˆ—åˆå§‹åŒ–çš„æ—¶é—´ä¸€è‡´æ€§ã€‚- åº”ç”¨åˆ†æ•°è’¸é¦é‡‡æ ·ä¼˜åŒ–å››ç»´é«˜æ–¯ç‚¹äº‘ã€‚- ç‰¹æ®Šè®¾è®¡çš„å››ç»´é«˜æ–¯æ•£å°„ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œæå‡ºè‡ªé€‚åº”è‡´å¯†åŒ–ç­–ç•¥ä»¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œå®ç°é²æ£’ä¼˜åŒ–ã€‚- æ— éœ€é¢„è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£ç½‘ç»œï¼Œä¸ºå››ç»´ç”Ÿæˆä»»åŠ¡æä¾›æ›´ä¾¿æ·å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚- å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€æ—¶ç©ºä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„å››ç»´ç”Ÿæˆå·¥ä½œï¼Œä¸ºåŸºäºæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰ä¸åŒè¾“å…¥çš„å››ç»´ç”Ÿæˆæ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šSTAG4Dï¼šæ—¶ç©ºé”šå®šç”Ÿæˆæ¨¡å‹</li><li>ä½œè€…ï¼šBingbing Ni, Jingwen Zhang, Yinda Zhang, Yebin Liu, Xin Tong</li><li>å•ä½ï¼šå—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼š4D ç”ŸæˆÂ·3D é«˜æ–¯ç‚¹äº‘Â·æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.00533.pdfï¼ŒGithub é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œ 3D ç”ŸæˆæŠ€æœ¯å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œæ¿€å‘äº†äººä»¬å¯¹ 4D å†…å®¹åˆ›ä½œçš„å…´è¶£ã€‚ç„¶è€Œï¼Œå®ç°å…·æœ‰æ—¶ç©ºä¸€è‡´æ€§çš„é«˜ä¿çœŸ 4D ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ 4D ç”Ÿæˆæ–¹æ³•ä¸»è¦åŸºäº 3D ç”ŸæˆæŠ€æœ¯ï¼Œå¦‚ä½“ç´ ç½‘æ ¼å’Œç‚¹äº‘æ¸²æŸ“ã€‚è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦é¢„è®­ç»ƒæˆ–å¾®è°ƒæ‰©æ•£ç½‘ç»œï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º STAG4D çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸åŠ¨æ€ 3D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“ç›¸ç»“åˆï¼Œç”¨äºé«˜ä¿çœŸ 4D ç”Ÿæˆã€‚è¯¥æ¡†æ¶ä» 3D ç”ŸæˆæŠ€æœ¯ä¸­æ±²å–çµæ„Ÿï¼Œåˆ©ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹åˆå§‹åŒ–å¤šè§†å›¾å›¾åƒï¼Œå¹¶å°†è§†é¢‘å¸§ä½œä¸ºé”šç‚¹ï¼Œå…¶ä¸­è§†é¢‘å¯ä»¥æ˜¯çœŸå®ä¸–ç•Œæ•è·çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç”±è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ã€‚ä¸ºäº†ç¡®ä¿å¤šè§†å›¾åºåˆ—åˆå§‹åŒ–çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„èåˆç­–ç•¥ï¼Œåˆ©ç”¨ç¬¬ä¸€å¸§ä½œä¸ºè‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­çš„æ—¶é—´é”šç‚¹ã€‚ä½¿ç”¨å‡ ä¹ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ï¼Œç„¶ååº”ç”¨å¾—åˆ†è’¸é¦é‡‡æ ·æ¥ä¼˜åŒ– 4D é«˜æ–¯ç‚¹äº‘ã€‚4D é«˜æ–¯ç‚¹äº‘æ¸²æŸ“æ˜¯ä¸“é—¨ä¸ºç”Ÿæˆä»»åŠ¡è®¾è®¡çš„ï¼Œå…¶ä¸­æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æå‡ºçš„ç®¡é“ä¸éœ€è¦å¯¹æ‰©æ•£ç½‘ç»œè¿›è¡Œä»»ä½•é¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œä¸º 4D ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æ›´æ˜“äºè®¿é—®å’Œå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½åŠä¸ç›®æ ‡çš„ä¸€è‡´æ€§ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€æ—¶ç©ºä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„ 4D ç”Ÿæˆå·¥ä½œï¼Œä¸ºæ¥è‡ªæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰ä¸åŒè¾“å…¥çš„ 4D ç”Ÿæˆè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å®ç°å…·æœ‰é«˜ä¿çœŸåº¦å’Œæ—¶ç©ºä¸€è‡´æ€§çš„ 4D å†…å®¹ç”Ÿæˆã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š4Dè¡¨ç¤ºï¼šæå‡º 4D é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šæ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£ï¼šç»“åˆå¤šè§†å›¾æ‰©æ•£æ¨¡å‹å’Œå‚è€ƒæ³¨æ„åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£æ¨¡å—ï¼Œä»¥ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ã€‚ï¼ˆ3ï¼‰ï¼šå¤šè§†å›¾ SDS ä¼˜åŒ–ï¼šåˆ©ç”¨ç”Ÿæˆçš„é”šè§†å›¾å’Œå‚è€ƒè§†å›¾ï¼Œä½¿ç”¨å¤šè§†å›¾ SDS ä¼˜åŒ–æ¥ä¼˜åŒ– 4D é«˜æ–¯ç‚¹äº‘ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ 4D ç”Ÿæˆã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•ç›®è§†é¢‘ç”ŸæˆåŠ¨æ€ 3D å†…å®¹çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº† 4D è¡¨ç¤ºå’Œæ—¶ç©ºä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨ä¸“é—¨å®šåˆ¶çš„ 4D é«˜æ–¯ä½“ç´ æ¸²æŸ“å’Œæ–°é¢–çš„ä¿¡æ¯èåˆæ¨¡å—ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†é«˜è´¨é‡ä¸”é²æ£’çš„ 4D åœºæ™¯ç”Ÿæˆã€‚å…¨é¢çš„å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€å…ˆè¿›çš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œå±•ç¤ºäº†æ˜æ˜¾æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦ä»¥åŠæ¸²æŸ“è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ˜¾ç€æ”¹è¿›ã€‚æ€»ä½“è€Œè¨€ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å•ç›®è§†é¢‘ä¸­åŠ¨æ€ 3D å†…å®¹ç”Ÿæˆçš„è®­ç»ƒé€Ÿåº¦ã€æ¸²æŸ“è´¨é‡å’Œ 4D ä¸€è‡´æ€§æ–¹é¢æ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œä¸ºç°å®ä¸–ç•Œçš„åº”ç”¨å¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„ 4D é«˜æ–¯ä½“ç´ è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”åŠ å¯†ç­–ç•¥æ¥ç¼“è§£ä¸ç¨³å®šçš„é«˜æ–¯æ¢¯åº¦ï¼Œä»¥å®ç°é²æ£’ä¼˜åŒ–ã€‚</li><li>ç»“åˆå¤šè§†å›¾æ‰©æ•£æ¨¡å‹å’Œå‚è€ƒæ³¨æ„åŠ›ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´å’Œå¤šè§†å›¾ä¸€è‡´æ‰©æ•£æ¨¡å—ï¼Œä»¥ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„å¤šè§†å›¾åºåˆ—ã€‚</li><li>åˆ©ç”¨ç”Ÿæˆçš„é”šè§†å›¾å’Œå‚è€ƒè§†å›¾ï¼Œä½¿ç”¨å¤šè§†å›¾ SDS ä¼˜åŒ–æ¥ä¼˜åŒ– 4D é«˜æ–¯ä½“ç´ ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ 4D ç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ¸²æŸ“è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œç”Ÿæˆé²æ£’æ€§æ–¹é¢å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚</li><li>ä¸ç°æœ‰çš„ 4D ç”Ÿæˆæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦æ˜æ˜¾æé«˜ã€‚å·¥ä½œé‡ï¼š</li><li>æ— éœ€å¯¹æ‰©æ•£ç½‘ç»œè¿›è¡Œä»»ä½•é¢„è®­ç»ƒæˆ–å¾®è°ƒã€‚</li><li>æ˜“äºå®ç°å’Œä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-cc3237d865a131294adf4c088d9c1009.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bdb6857c03ea01ca9348a454fc10619.jpg" align="middle"><img src="https://picx.zhimg.com/v2-171cac27c18392a0d918da1cdd0d421b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/</id>
    <published>2024-03-28T03:07:02.000Z</published>
    <updated>2024-03-28T03:07:02.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å‘å±•ä¸æ£€æµ‹æŠ€æœ¯éœ€è¦æŒç»­æ¼”è¿›ï¼Œä»¥åº”å¯¹éšç§ä¾µçŠ¯å’Œç½‘ç»œé’“é±¼ç­‰éæ³•ä½¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç»Ÿä¸€ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li><li>æ¢è®¨ç”Ÿæˆå’Œæ£€æµ‹æŠ€æœ¯æ¡†æ¶çš„å‘å±•ã€‚</li><li>å…³æ³¨äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆã€é¢éƒ¨å±æ€§ç¼–è¾‘ç­‰ä¸»æµæ·±åº¦ä¼ªé€ é¢†åŸŸã€‚</li><li>å…¨é¢åŸºå‡†æµ‹è¯•æ¯ä¸ªé¢†åŸŸæµè¡Œæ•°æ®é›†ä¸Šçš„ä»£è¡¨æ€§æ–¹æ³•ã€‚</li><li>åˆ†ææ‰€è®¨è®ºé¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li><li>è·Ÿè¸ª Github ä¸Šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹çš„æœ€æ–°è¿›å±•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.é¢˜ç›®ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹ï¼šåŸºå‡†ä¸ç»¼è¿°2.ä½œè€…ï¼šç”˜æ²›ï¼Œè’‹å®ç« ï¼Œå­Ÿæ¶µèƒ¡ï¼Œå…‰æ¶›ç¿Ÿï¼Œæˆæ°ç‹ï¼ŒæŒ¯å®‡å¼ ï¼Œå»ºæ¨ï¼Œæ˜¥åæ²ˆï¼Œå¤§æˆé™¶3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šåä¸œå¸ˆèŒƒå¤§å­¦å¤šæ¨¡ä¿¡æ¯å¤„ç†ä¸Šæµ·å¸‚é‡ç‚¹å®éªŒå®¤4.å…³é”®è¯ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆï¼Œäººè„¸æ›¿æ¢ï¼Œäººè„¸é‡æ¼”ï¼Œè¯­éŸ³äººè„¸ç”Ÿæˆï¼Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œå¤–æ¥æ£€æµ‹ï¼Œç»¼è¿°5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17881ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— 6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šéšç€æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œä»¥å˜åˆ†è‡ªç¼–ç å™¨ (VAE) å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) ä¸ºä»£è¡¨çš„æŠ€æœ¯åœ¨æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿‘å¹´æ¥ï¼Œå…·æœ‰å¼ºå¤§å›¾åƒç”Ÿæˆèƒ½åŠ›çš„æ‰©æ•£æ¨¡å‹çš„å‡ºç°å¼•å‘äº†è¯¥æŠ€æœ¯çš„æ–°ä¸€è½®ç ”ç©¶å’Œäº§ä¸šçƒ­æ½®ã€‚ï¼ˆ2ï¼‰ï¼šä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ ç”Ÿæˆæ–¹æ³•åŸºäº GAN æ¨¡å‹ï¼Œå­˜åœ¨ç”Ÿæˆæ•ˆæœä¸ä½³çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹çš„å‡ºç°æå¤§åœ°æå‡äº†å›¾åƒ/è§†é¢‘çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„æ·±åº¦ä¼ªé€ å†…å®¹ä¸çœŸå®å†…å®¹éš¾ä»¥åŒºåˆ†ï¼Œå…·æœ‰å¾ˆé«˜çš„å®ç”¨ä»·å€¼ã€‚ï¼ˆ3ï¼‰ï¼šæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸»è¦åˆ†ä¸ºäººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘å››ä¸ªä¸»æµç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡å¯¹è¿™äº›é¢†åŸŸçš„å‘å±•è¿›è¡Œäº†ç»¼è¿°ï¼Œå¹¶å¯¹å„ä¸ªé¢†åŸŸçš„ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ã€‚ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡åˆ†æäº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†å‚è€ƒã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼š(1) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“å’Œç»¼è¿°ï¼Œåˆ†æäº†è¯¥é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚(2) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸçš„ä¸»æµç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘ï¼Œè¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ã€‚(3) æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•è¿›è¡Œäº†æ€»ç»“ï¼Œåˆ†æäº†å¤–æ¥æ£€æµ‹å’Œå†…åœ¨æ£€æµ‹ä¸¤ç§æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹æœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬ç»¼è¿°å…¨é¢å›é¡¾äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œé¦–æ¬¡å…¨é¢è¦†ç›–äº†ç›¸å…³é¢†åŸŸï¼Œå¹¶è®¨è®ºäº†æ‰©æ•£æ¨¡å‹ç­‰æœ€æ–°æŠ€æœ¯ã€‚å…·ä½“è€Œè¨€ï¼Œæœ¬æ–‡æ¶µç›–äº†åŸºæœ¬èƒŒæ™¯çŸ¥è¯†çš„æ¦‚è¿°ï¼ŒåŒ…æ‹¬ç ”ç©¶ä»»åŠ¡çš„æ¦‚å¿µã€æ•°æ®æ”¶é›†ä¸å¤„ç†æ–¹æ³•ã€æ¨¡å‹è®¾è®¡ä¸è®­ç»ƒç­–ç•¥ã€è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®é›†ã€‚(2): åˆ›æ–°ç‚¹ï¼šæœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆé¢†åŸŸçš„å››ä¸ªä¸»æµç ”ç©¶é¢†åŸŸè¿›è¡Œäº†åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬äººè„¸æ›¿æ¢ã€äººè„¸é‡æ¼”ã€è¯­éŸ³äººè„¸ç”Ÿæˆå’Œäººè„¸å±æ€§ç¼–è¾‘ã€‚æœ¬æ–‡è¿˜å¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•è¿›è¡Œäº†æ€»ç»“ï¼Œåˆ†æäº†å¤–æ¥æ£€æµ‹å’Œå†…åœ¨æ£€æµ‹ä¸¤ç§æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¯¹æœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†å±•æœ›ã€‚æ€§èƒ½ï¼šæœ¬æ–‡æå‡ºçš„åŸºå‡†æµ‹è¯•å’Œå…¨é¢è¯„ä¼°ä¸ºæ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“å’Œç»¼è¿°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3fbcb20b0b6d83737be267b8b78dde71.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bac7dee6bad7c9614f746a35eef341ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a0d28dab08c4d0254dd790d3d608013.jpg" align="middle"><img src="https://picx.zhimg.com/v2-409f1c30ffae605d9a497f77ff9ae5bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80df0902b8cc7d09c263750672e1ab59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4b73e97f1af3856b9dddf84237d9fcb.jpg" align="middle"></details><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>ä½¿ç”¨ä»…ä¸€åˆ†é’Ÿè§†é¢‘è®­ç»ƒå³å¯ç”Ÿæˆæ‹¥æœ‰èº¯å¹²å’Œæ‰‹éƒ¨åŠ¨ä½œçš„ä¸»æ’­é£æ ¼å®Œæ•´è§†é¢‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§é€šè¿‡ä¸€åˆ†é’Ÿè§†é¢‘è®­ç»ƒæ¥ç”Ÿæˆä¸»æ’­é£æ ¼è§†é¢‘çš„ç³»ç»Ÿâ€”â€”Make-Your-Anchorã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå°†åŠ¨ä½œä¸ç‰¹å®šå¤–è§‚æœ‰æ•ˆåœ°ç»‘å®šã€‚</li><li>æ‰©å±•äº†å¸§çº§æ‰©æ•£æ¨¡å‹ä¸­çš„äºŒç»´ U-Net åˆ°ä¸‰ç»´é£æ ¼ï¼Œä»¥ç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´è§†é¢‘ã€‚</li><li>æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰¹é‡é‡å æ—¶é—´å»å™ªæ¨¡å—ï¼Œä»¥ç»•è¿‡æ¨ç†æœŸé—´è§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚</li><li>å¼•å…¥äº†æ–°é¢–çš„èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚</li><li>ä¸ SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}ã€‚">https://github.com/ICTMCG/Make-Your-Anchor}ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šMake-Your-Anchorï¼šåŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶</li><li>ä½œè€…ï¼šZiyao Huangã€Fan Tangã€Yong Zhangã€Xiaodong Cunã€Juan Caoã€Jintao Liã€Tong-Yee Lee</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è®¡ç®—æŠ€æœ¯ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆã€å¤´åƒç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€è¿åŠ¨æ•æ‰</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.16510   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/ICTMCG/Make-Your-Anchor</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   å½“å‰çš„å¤´åƒç”ŸæˆæŠ€æœ¯ä¸»è¦é›†ä¸­åœ¨å¤´éƒ¨ç”Ÿæˆï¼Œæ— æ³•ç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   ç°æœ‰çš„åŸºäº GAN çš„æ–¹æ³•åªèƒ½ç”Ÿæˆå±€éƒ¨åŒºåŸŸï¼ŒåŸºäºè¿åŠ¨è¿ç§»çš„æ–¹æ³•å—é™äºè¿åŠ¨æ•æ‰æ•°æ®çš„å¯ç”¨æ€§ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡º Make-Your-Anchor æ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒåŸºäºç»“æ„å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå°† 3D ç½‘æ ¼æ¡ä»¶æ¸²æŸ“ä¸ºé€¼çœŸçš„å…¨èº«åŠ¨ä½œå¤´åƒè§†é¢‘ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†è¿åŠ¨ä¸ç‰¹å®šå¤–è§‚ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆä»»æ„é•¿åº¦çš„è§†é¢‘ï¼Œå°†å¸§çº§æ‰©æ•£æ¨¡å‹ä¸­çš„ 2D U-Net æ‰©å±•ä¸º 3Dï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªæ¨¡å—ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¾“å‡ºè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚   ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼š   Make-Your-Anchor åœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢ä¼˜äº SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ã€‚å®ƒä»…éœ€ä¸€åˆ†é’Ÿçš„è§†é¢‘å‰ªè¾‘å³å¯è®­ç»ƒï¼Œç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ï¼Œæ»¡è¶³äº†è‡ªåŠ¨ç”Ÿæˆå¤´åƒè§†é¢‘çš„éœ€æ±‚ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ç»“æ„å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼šå°†3Dç½‘æ ¼æ¡ä»¶åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ï¼Œå­¦ä¹ å§¿æ€ä¸ç›®æ ‡è§†é¢‘å¸§ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼›(2)ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢„è®­ç»ƒå¢å¼ºæ¨¡å‹ç”ŸæˆåŠ¨ä½œçš„èƒ½åŠ›ï¼Œå¾®è°ƒç»‘å®šåŠ¨ä½œä¸ç‰¹å®šå¤–è§‚ï¼›(3)æ‰¹æ¬¡é‡å æ—¶é—´å»å™ªï¼šé‡‡ç”¨å…¨å¸§äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œé‡å æ—¶é—´å»å™ªç®—æ³•ï¼Œç”Ÿæˆä»»æ„é•¿åº¦çš„æ—¶é—´ä¸€è‡´è§†é¢‘ï¼›(4)èº«ä»½ç‰¹å®šé¢éƒ¨å¢å¼ºæ¨¡å—ï¼šé€šè¿‡è£å‰ªå’Œèåˆæ“ä½œï¼Œå¯¹ç”Ÿæˆçš„èº«ä½“ä¸­çš„é¢éƒ¨åŒºåŸŸè¿›è¡Œä¿®æ”¹ï¼Œæé«˜è§†è§‰è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Make-Your-Anchorï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºåˆ¶ä½œé€¼çœŸä¸”é«˜è´¨é‡çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ã€‚è¯¥æ¡†æ¶é€šè¿‡å¸§çº§è¿åŠ¨åˆ°å¤–è§‚æ‰©æ•£è®­ç»ƒäº†ä¸€ä¸ªç»“æ„å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œç»‘å®šé£æ ¼æ–¹æ³•å®ç°äº†ç‰¹å®šå¤–è§‚ä¸åŠ¨ä½œçš„ç»‘å®šã€‚ä¸ºäº†ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„äººç‰©è§†é¢‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— è®­ç»ƒç­–ç•¥ï¼Œå°†å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªç®—æ³•æ¥å…‹æœç”Ÿæˆè§†é¢‘é•¿åº¦çš„é™åˆ¶ã€‚ä»è§‚å¯Ÿåˆ°é¢éƒ¨ç»†èŠ‚åœ¨æ•´ä½“äººç‰©ç”Ÿæˆè¿‡ç¨‹ä¸­éš¾ä»¥é‡å»ºè¿™ä¸€ç°è±¡å‡ºå‘ï¼Œå¼•å…¥äº†èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºæŠ€æœ¯ã€‚é€šè¿‡å°†è¿™å››ä¸ªç³»ç»Ÿæ–¹æ³•ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶æˆåŠŸåœ°ç”Ÿæˆäº†é«˜è´¨é‡ã€ç»“æ„ä¿æŒå’Œæ—¶é—´è¿è´¯çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ï¼Œè¿™å¯èƒ½ä¸º 2D æ•°å­—å¤´åƒçš„å¹¿æ³›åº”ç”¨æŠ€æœ¯æä¾›ä¸€äº›å‚è€ƒä»·å€¼ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ 2D å¤´åƒç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸä¸”é«˜è´¨é‡çš„ä¸»æ’­é£æ ¼äººç‰©è§†é¢‘ï¼›é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥å’Œç»‘å®šé£æ ¼æ–¹æ³•ï¼Œå°†ç‰¹å®šå¤–è§‚ä¸åŠ¨ä½œç»‘å®šï¼›æå‡ºäº†ä¸€ä¸ªæ— è®­ç»ƒç­–ç•¥ï¼Œå°†å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ‰¹æ¬¡é‡å æ—¶é—´å»å™ªç®—æ³•æ¥å…‹æœç”Ÿæˆè§†é¢‘é•¿åº¦çš„é™åˆ¶ï¼›å¼•å…¥äº†èº«ä»½ç‰¹å®šçš„é¢éƒ¨å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜ç”Ÿæˆè§†é¢‘ä¸­é¢éƒ¨åŒºåŸŸçš„è§†è§‰è´¨é‡ã€‚æ€§èƒ½ï¼šåœ¨è§†è§‰è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œèº«ä»½ä¿ç•™æ–¹é¢ä¼˜äº SOTA æ‰©æ•£/éæ‰©æ•£æ–¹æ³•ï¼›ä»…éœ€ä¸€åˆ†é’Ÿçš„è§†é¢‘å‰ªè¾‘å³å¯è®­ç»ƒï¼Œç”Ÿæˆå…¨èº«åŠ¨ä½œé€¼çœŸçš„å¤´åƒè§†é¢‘ï¼Œæ»¡è¶³äº†è‡ªåŠ¨ç”Ÿæˆå¤´åƒè§†é¢‘çš„éœ€æ±‚ã€‚å·¥ä½œé‡ï¼šä¸­ç­‰ï¼›éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†è®­ç»ƒæ•°æ®ï¼›éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒï¼›éœ€è¦å¯¹ç”Ÿæˆç»“æœè¿›è¡Œè¯„ä¼°å’Œä¼˜åŒ–ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation"><a href="#Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation" class="headerlink" title="Adaptive Super Resolution For One-Shot Talking-Head Generation"></a>Adaptive Super Resolution For One-Shot Talking-Head Generation</h2><p><strong>Authors:Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</strong></p><p>The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \url{<a href="https://github.com/Songluchuan/AdaSR-TalkingHead/}">https://github.com/Songluchuan/AdaSR-TalkingHead/}</a>. </p><p><a href="http://arxiv.org/abs/2403.15944v1">PDF</a> 5 pages, 3 figures</p><p><strong>Summary</strong><br>ä¸€é”®å¼ç”Ÿæˆé«˜æ¸…æ™°åº¦è§†é¢‘ï¼Œæ— éœ€æ·»åŠ é¢„è®­ç»ƒæ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”é‡å»ºé«˜é¢‘ç»†èŠ‚æå‡è§†é¢‘æ¸…æ™°åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¸€é”®å¼ç”Ÿæˆäººåƒè§†é¢‘ï¼Œé©±åŠ¨è§†é¢‘ä¸äººåƒåŒä¸€æˆ–ä¸åŒã€‚</li><li>ä¼ ç»Ÿæ–¹æ³•å—é™äºå•å›¾åƒæºå’Œåƒç´ ä½ç§»ï¼Œæ¸…æ™°åº¦å—æŸã€‚</li><li>ç°æœ‰æ–¹æ³•é€šè¿‡è¶…åˆ†è¾¨ç‡æ¨¡å—æå‡è´¨é‡ï¼Œä½†å¢åŠ è®¡ç®—é‡å¹¶ç ´ååŸå§‹æ•°æ®åˆ†å¸ƒã€‚</li><li>æœ¬æ–‡æå‡ºè‡ªé€‚åº”é«˜å“è´¨äººåƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ— éœ€é¢å¤–é¢„è®­ç»ƒæ¨¡å—åˆæˆé«˜åˆ†è¾¨ç‡è§†é¢‘ã€‚</li><li>å—è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯å‘ï¼Œå¯¹å•å›¾åƒæºä¸‹é‡‡æ ·ï¼Œå†é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¨¡å—è‡ªé€‚åº”é‡å»ºé«˜é¢‘ç»†èŠ‚ã€‚</li><li>è¯¥ç­–ç•¥ç®€å•æœ‰æ•ˆåœ°æå‡äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œå¹¶é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°å¾—åˆ°è¯å®ã€‚</li><li>ä»£ç å’Œæ¼”ç¤ºè§†é¢‘å¯åœ¨ Github ä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè‡ªé€‚åº”è¶…åˆ†è¾¨ç‡å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ</li><li>ä½œè€…ï¼šLuchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç½—åˆ‡æ–¯ç‰¹å¤§å­¦</li><li>å…³é”®è¯ï¼šè¶…åˆ†è¾¨ç‡è§†é¢‘ï¼Œå•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.15944ï¼ŒGithubï¼šNone</li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šå•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ—¨åœ¨ä½¿ç”¨ä¸€å¼ æºäººåƒå›¾åƒåœ¨ç›¸åŒæˆ–ä¸åŒèº«ä»½è§†é¢‘çš„é©±åŠ¨ä¸‹åˆæˆè¯´è¯äººå¤´éƒ¨è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åŸºäºå¹³é¢çš„åƒç´ å˜æ¢ï¼Œè¿™ä¼šå½±å“åˆæˆå›¾åƒçš„æ¸…æ™°åº¦ã€‚ä¸€äº›æ–¹æ³•é€šè¿‡å¼•å…¥é¢å¤–çš„è¶…åˆ†è¾¨ç‡æ¨¡å—æ¥æé«˜åˆæˆè§†é¢‘çš„è´¨é‡ï¼Œä½†è¿™ä¼šå¢åŠ è®¡ç®—æ¶ˆè€—å¹¶ç ´ååŸå§‹æ•°æ®åˆ†å¸ƒã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šMetaPortraitã€SadTalker å’Œ VideoReTalking ç­‰æ–¹æ³•å°è¯•é€šè¿‡é‡æ–°è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„è¶…åˆ†è¾¨ç‡æ¨¡å—æ¥æ”¹å–„è§†é¢‘è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™ç§ä¸¤é˜¶æ®µåˆæˆè¿‡ç¨‹ä¼šå¯¼è‡´ä¸å¿…è¦çš„è®¡ç®—å¼€é”€å’Œé”™è¯¯ç´¯ç§¯ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºè¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ¡†æ¶ã€‚å— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„å¯å‘ï¼Œè¯¥æ–¹æ³•é€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒæ¥æ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ã€‚å®ƒé€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç°æœ‰çš„å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆé€šè¿‡ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•å¯å‘ï¼Œé€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒï¼Œæ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ï¼›(2) é€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå•é•œå¤´è¯´è¯äººå¤´éƒ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸã€‚é€šè¿‡è®¾è®¡ç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»ä½è´¨é‡å›¾åƒä¸­æ•è·é«˜é¢‘ç»†èŠ‚ã€‚è¿™ä½¿å¾—æ— éœ€é¢å¤–çš„é¢„è®­ç»ƒæ¨¡å—æˆ–åå¤„ç†å³å¯åˆæˆé«˜è´¨é‡è§†é¢‘ã€‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šé‡å’Œå®šæ€§è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜è´¨é‡å¯é©±åŠ¨äººè„¸è§†é¢‘ç”Ÿæˆæ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šå— ESRGAN å’Œ Real-ESRGAN ç­‰è¶…åˆ†è¾¨ç‡æ–¹æ³•çš„å¯å‘ï¼Œé€šè¿‡å‹ç¼©å’Œä¸‹é‡‡æ ·é«˜è´¨é‡å›¾åƒï¼Œæ„å»ºç”¨äºæˆå¯¹è®­ç»ƒçš„ä½è´¨é‡å›¾åƒæ•°æ®ã€‚é€šè¿‡ç‹¬ç‰¹è®¾è®¡çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œä»ä½è´¨é‡å›¾åƒä¸­è‡ªé€‚åº”åœ°æ•è·é«˜é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé‡å»ºã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ç°æœ‰çš„å•é•œå¤´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆé€šè¿‡ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„é¢„è®­ç»ƒæ¨¡å—æˆ–åå¤„ç†æ­¥éª¤ã€‚è¿™ä½¿å¾—è¯¥æ–¹æ³•åœ¨è®¡ç®—å’Œæ—¶é—´æ–¹é¢éƒ½å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fbfc28956b0106142272e9ccedb9ced5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-188e4004db88e63a7e920e9ac2f3636d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b69fbe4c0930a57ff002ead5463e3ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3a0dd3488e1d1a03f494038c2fcb247.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/</id>
    <published>2024-03-28T02:56:57.000Z</published>
    <updated>2024-03-28T02:56:57.223Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-28-æ›´æ–°"><a href="#2024-03-28-æ›´æ–°" class="headerlink" title="2024-03-28 æ›´æ–°"></a>2024-03-28 æ›´æ–°</h1><h2 id="AID-Attention-Interpolation-of-Text-to-Image-Diffusion"><a href="#AID-Attention-Interpolation-of-Text-to-Image-Diffusion" class="headerlink" title="AID: Attention Interpolation of Text-to-Image Diffusion"></a>AID: Attention Interpolation of Text-to-Image Diffusion</h2><p><strong>Authors:Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</strong></p><p>Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at <a href="https://github.com/QY-H00/attention-interpolation-diffusion">https://github.com/QY-H00/attention-interpolation-diffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.17924v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>æ³¨æ„åŠ›æ’å€¼æ‰©æ•£ï¼ˆAIDï¼‰ï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„æ¡ä»¶æ’å€¼æ–°æŠ€æœ¯ï¼Œå¯ç”Ÿæˆé«˜åº¦ä¸€è‡´ã€å¹³æ»‘ä¸”é€¼çœŸçš„å›¾åƒã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æå‡ºå†…å±‚/å¤–å±‚æ’å€¼æ³¨æ„åŠ›å±‚ï¼Œä»¥å¢å¼ºæ’å€¼è´¨é‡ã€‚</li><li>èåˆæ’å€¼æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ã€‚</li><li>åº”ç”¨è´å¡”åˆ†å¸ƒé€‰æ‹©ï¼Œæé«˜æ’å€¼çš„å¹³æ»‘åº¦ã€‚</li><li>æå‡ºæç¤ºå¼•å¯¼çš„æ³¨æ„åŠ›æ’å€¼æ‰©æ•£ï¼ˆPAIDï¼‰å˜ä½“ï¼Œå°†æ’å€¼è§†ä¸ºæ¡ä»¶ä¾èµ–çš„ç”Ÿæˆè¿‡ç¨‹ã€‚</li><li>æ§åˆ¶æ’å€¼çš„ç¡®åˆ‡è·¯å¾„ï¼Œç”Ÿæˆå…·æœ‰æ›´é«˜ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œæ•ˆç‡çš„æ–°å›¾åƒã€‚</li><li>åœ¨æ¦‚å¿µå’Œç©ºé—´æ’å€¼æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAIDï¼šæ–‡æœ¬åˆ°å›¾åƒçš„æ³¨æ„æ’å€¼</li><li>ä½œè€…ï¼šé½æºä½•ã€æ™¯æµ©ç‹ã€å­ä¸ºåˆ˜ã€å®‰å‰æ‹‰å§š</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ã€æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€æ³¨æ„æœºåˆ¶ã€æ’å€¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17924    Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ¡ä»¶æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå„ç§åœºæ™¯ä¸­çš„å›¾åƒï¼Œæœ‰åŠ©äºå›¾åƒæ’å€¼ã€‚åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ’å€¼å·²ç»å¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œä½†ä½¿ç”¨ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬æˆ–å§¿åŠ¿ï¼‰è¿›è¡Œæ’å€¼çš„ç ”ç©¶è¾ƒå°‘ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç®€å•çš„æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨æ¡ä»¶ç©ºé—´ä¸­è¿›è¡Œçº¿æ€§æ’å€¼ï¼Œé€šå¸¸ä¼šå¯¼è‡´å›¾åƒç¼ºä¹ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œä¿çœŸåº¦ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º AID çš„æ–°é¢–å…è®­ç»ƒæŠ€æœ¯ï¼Œå³é€šè¿‡æ‰©æ•£è¿›è¡Œæ³¨æ„æ’å€¼ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¡ä»¶ç©ºé—´ä¸­å¼•å…¥æ³¨æ„æœºåˆ¶æ¥æŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œä»è€Œç¡®ä¿å›¾åƒåœ¨å¸ƒå±€å’Œæ¦‚å¿µä¸Šçš„å¹³æ»‘è¿‡æ¸¡ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ç©ºé—´å’Œæ¦‚å¿µæ’å€¼ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚å®éªŒç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å†…/å¤–æ’å€¼æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šè¿‡åœ¨æ¡ä»¶ç©ºé—´ä¸­å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œç¡®ä¿å›¾åƒåœ¨å¸ƒå±€å’Œæ¦‚å¿µä¸Šçš„å¹³æ»‘è¿‡æ¸¡ã€‚(2) ä¸è‡ªæ³¨æ„åŠ›èåˆï¼šå°†æ’å€¼æ½œå˜é‡æœ¬èº«çš„é”®å’Œå€¼èå…¥æ’å€¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜ä¸€è‡´æ€§å’Œä¿çœŸåº¦ã€‚(3) Beta å…ˆéªŒåºåˆ—é€‰æ‹©ï¼šé‡‡ç”¨ Beta åˆ†å¸ƒé€‰æ‹©æ’å€¼è·¯å¾„ä¸Šçš„ç‰¹å®šæ’å€¼å›¾åƒï¼Œä½¿ç”Ÿæˆçš„å›¾åƒåºåˆ—æ›´å¹³æ»‘ã€‚(4) æç¤ºå¼•å¯¼ï¼šé€šè¿‡æ³¨å…¥æç¤ºä½œä¸ºæ¡ä»¶ï¼Œæ§åˆ¶æ’å€¼è·¯å¾„ï¼Œç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„æ’å€¼åºåˆ—ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºæ¡ä»¶æ’å€¼ä»»åŠ¡åŠç›¸å…³è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä¸€è‡´æ€§ã€å¹³æ»‘æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸º AID çš„æ–°é¢–æ–¹æ³•ï¼Œç”¨äºåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ¡ä»¶æ’å€¼å›¾åƒã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹æ˜¾ç€è¶…è¶Šäº†åŸºå‡†ï¼Œé€šè¿‡å®šæ€§å’Œå®šé‡åˆ†æå¾—åˆ°äº†è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† PAIDï¼Œè¯¥æ‰©å±•å…è®¸ç”¨æˆ·ä½¿ç”¨å¼•å¯¼æç¤ºæ¥é€‰æ‹©æ’å€¼è·¯å¾„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œæ‹“å®½äº†ç”Ÿæˆæ¨¡å‹æ’å€¼çš„èŒƒå›´ï¼Œä¸ºåˆæˆç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€æ•°æ®å¢å¼ºå’Œè§†é¢‘æ’å€¼ç­‰å„ç§åº”ç”¨å¼€è¾Ÿäº†æ–°æœºé‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºæ¡ä»¶æ’å€¼ä»»åŠ¡åŠè¯„ä¼°æŒ‡æ ‡ï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æŒ‡å¯¼æ’å€¼è¿‡ç¨‹ï¼Œæ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆé«˜è´¨é‡æ’å€¼å›¾åƒã€‚æ€§èƒ½ï¼šåœ¨ç©ºé—´å’Œæ¦‚å¿µæ’å€¼ä»»åŠ¡ä¸Šå–å¾—æ˜¾ç€æ”¹è¿›ï¼Œå®šæ€§å’Œå®šé‡è¯„ä¼°å‡æ”¯æŒè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ’å€¼æ–¹æ³•ï¼Œå‡å°‘äº†è®­ç»ƒè´Ÿæ‹…ï¼Œæé«˜äº†æ’å€¼æ•ˆç‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-aaa47516c2e21df63c1ee81eb0afd555.jpg" align="middle"></details><h2 id="AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation"><a href="#AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation" class="headerlink" title="AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation"></a>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</h2><p><strong>Authors:Huawei Wei, Zejun Yang, Zhisheng Wang</strong></p><p>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at <a href="https://github.com/scutzzj/AniPortrait">https://github.com/scutzzj/AniPortrait</a> </p><p><a href="http://arxiv.org/abs/2403.17694v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒè‚–åƒå›¾åƒç”Ÿæˆé«˜å“è´¨åŠ¨ç”»çš„æ–°é¢–æ¡†æ¶ï¼šAniPortrait</p><p><strong>Key Takeaways</strong></p><ul><li>AniPortrait æå‡ºäº†ä¸€ç§ç”±éŸ³é¢‘å’Œå‚è€ƒè‚–åƒå›¾åƒé©±åŠ¨çš„é«˜è´¨é‡åŠ¨ç”»ç”Ÿæˆæ–°æ¡†æ¶ã€‚</li><li>AniPortrait åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºå¹¶å°†å…¶æŠ•å½±åˆ° 2D é¢éƒ¨åœ°æ ‡åºåˆ—ä¸­ã€‚</li><li>AniPortrait ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹å’Œè¿åŠ¨æ¨¡å—å°†åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¯æœ‰æ•ˆåº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡å»ºç­‰é¢†åŸŸã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAniPortraitï¼šéŸ³é¢‘é©±åŠ¨çš„å†™å®è‚–åƒåŠ¨ç”»åˆæˆ</li><li>ä½œè€…ï¼šWei Huawei<em>ã€Yang Zejun</em>ã€Wang Zhisheng</li><li>å•ä½ï¼šè…¾è®¯</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨ã€è‚–åƒåŠ¨ç”»ã€æ‰©æ•£æ¨¡å‹ã€åŠ¨ä½œæ¨¡å—</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17694   Githubï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»éŸ³é¢‘å’Œé™æ€å›¾åƒç”Ÿæˆé€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„è‚–åƒåŠ¨ç”»å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œä½†åˆ¶ä½œè§†è§‰ä¸Šå¼•äººå…¥èƒœä¸”ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„é«˜è´¨é‡åŠ¨ç”»æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸æ— æ³•å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯å®ƒä»¬ä¾èµ–äºå®¹é‡æœ‰é™çš„è§†è§‰å†…å®¹ç”Ÿæˆå™¨ï¼Œä¾‹å¦‚ GANã€NeRF æˆ–åŸºäºè¿åŠ¨çš„è§£ç å™¨ã€‚è¿™äº›ç½‘ç»œè¡¨ç°å‡ºæœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡å†…å®¹æ—¶å¾€å¾€ç¼ºä¹ç¨³å®šæ€§ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º AniPortraitï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç”±éŸ³é¢‘å’Œå‚è€ƒå›¾åƒé©±åŠ¨çš„ä¼˜è´¨åŠ¨ç”»è‚–åƒã€‚AniPortrait åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„é˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶å°†å…¶æŠ•å½±åˆ° 2D é¢éƒ¨åœ°æ ‡åºåˆ—ä¸­ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆè¿åŠ¨æ¨¡å—ï¼Œå°†åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®éªŒç»“æœè¯æ˜äº† AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼š<strong>Audio2Lmk</strong>ï¼šä»éŸ³é¢‘ä¸­æå– 3D é¢éƒ¨ç½‘æ ¼åºåˆ—å’Œä½å§¿åºåˆ—ã€‚ï¼ˆ2ï¼‰ï¼š<strong>Lmk2Video</strong>ï¼šå°†é¢éƒ¨åœ°æ ‡åºåˆ—è½¬æ¢ä¸ºæ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ AniPortraitï¼Œè¯¥æ¡†æ¶å¯ä»¥ç”Ÿæˆç”±éŸ³é¢‘å’Œå‚è€ƒå›¾åƒé©±åŠ¨çš„ä¼˜è´¨åŠ¨ç”»è‚–åƒã€‚AniPortrait é‡‡ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ç»“åˆè¿åŠ¨æ¨¡å—å°†å…¶è½¬æ¢ä¸ºé€¼çœŸçš„ã€æ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚å®éªŒç»“æœè¯æ˜äº† AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ AniPortraitï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»éŸ³é¢‘å’Œå‚è€ƒå›¾åƒç”Ÿæˆé€¼çœŸçš„åŠ¨ç”»è‚–åƒã€‚</li><li>é‡‡ç”¨åŸºäº Transformer çš„æ¨¡å‹ä»éŸ³é¢‘ä¸­æå– 3D ä¸­é—´è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨ç¨³å¥çš„æ‰©æ•£æ¨¡å‹ç»“åˆè¿åŠ¨æ¨¡å—å°†å…¶è½¬æ¢ä¸ºæ—¶é—´ä¸€è‡´çš„è‚–åƒåŠ¨ç”»ã€‚</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚æ€§èƒ½ï¼š</li><li>AniPortrait åœ¨é¢éƒ¨è‡ªç„¶åº¦ã€å§¿åŠ¿å¤šæ ·æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä»è€Œæä¾›äº†å¢å¼ºçš„æ„ŸçŸ¥ä½“éªŒã€‚</li><li>AniPortrait åœ¨çµæ´»æ€§å’Œå¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºç›¸å½“å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºé¢éƒ¨åŠ¨ä½œç¼–è¾‘æˆ–é¢éƒ¨é‡ç°ç­‰é¢†åŸŸã€‚å·¥ä½œé‡ï¼š</li><li>AniPortrait çš„å®ç°éœ€è¦ä¸€å®šçš„æŠ€æœ¯å®åŠ›ï¼ŒåŒ…æ‹¬å¯¹ Transformer æ¨¡å‹ã€æ‰©æ•£æ¨¡å‹å’Œè¿åŠ¨æ¨¡å—çš„ç†è§£ã€‚</li><li>è®­ç»ƒ AniPortrait æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a0703eb6ac9807d377c7bbfaa84e3681.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc2d139237100aad689f67180ae398bf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e35074ee634942aebc5c8860cf29e344.jpg" align="middle"></details><h2 id="DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation"><a href="#DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation" class="headerlink" title="DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation"></a>DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation</h2><p><strong>Authors:Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu</strong></p><p>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing. </p><p><a href="http://arxiv.org/abs/2403.17664v1">PDF</a> </p><p><strong>Summary</strong><br>å›¾åƒä¸­äººè„¸å¤–è§‚ç¼–è¾‘çš„æ‰©æ•£æ¨¡å‹ DiffFAE æé«˜äº†ç”Ÿæˆä¿çœŸåº¦ã€å±æ€§ä¿ç•™å’Œæ¨ç†æ•ˆç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ (SPC) ç¡®ä¿æŸ¥è¯¢å±æ€§è½¬ç§»çš„ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li><li>å¼•å…¥åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆ (RSC) ä¿ç•™æºå±æ€§ï¼Œå‡è½»éé¢éƒ¨å±æ€§ï¼ˆå¦‚å¤´å‘ã€è¡£æœå’ŒèƒŒæ™¯ï¼‰å¸¦æ¥çš„ä¼ªå½±ã€‚</li><li>æå‡ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†å¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚</li><li>DiffFAE åœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li><li>DiffFAE å¯ä»¥æœ‰æ•ˆå¤„ç†äººè„¸å¤–è§‚ç¼–è¾‘ä¸­çš„ä½ç”Ÿæˆä¿çœŸåº¦ã€å·®å±æ€§ä¿ç•™å’Œä½æ¨ç†æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚</li><li>æ‰©æ•£æ¨¡å‹åœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä»»åŠ¡ä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºäººè„¸å¤–è§‚ç¼–è¾‘çš„æ–°é¢–æ¡†æ¶ DiffFAEï¼Œå®ƒç»“åˆäº†æ‰©æ•£æ¨¡å‹ã€ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶å’ŒåŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆçš„ä¼˜ç‚¹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDiffFAEï¼šæ¨è¿›é«˜ä¿çœŸä¸€å‘å¼äººè„¸å¤–è§‚ç¼–è¾‘</li><li>ä½œè€…ï¼šQ. Wang ç­‰</li><li>å•ä½ï¼šæœªæåŠ</li><li>å…³é”®è¯ï¼šFacial appearance editingã€Diffusion modelã€Object-centric learning</li><li>è®ºæ–‡é“¾æ¥ï¼šæœªæä¾›ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šäººè„¸å¤–è§‚ç¼–è¾‘æ—¨åœ¨ä¿®æ”¹äººè„¸å›¾åƒçš„ç‰©ç†å±æ€§ï¼ˆå¦‚å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰ç…§ï¼‰ï¼ŒåŒæ—¶ä¿ç•™èº«ä»½å’ŒèƒŒæ™¯ç­‰å±æ€§ï¼Œåœ¨æ‘„å½±ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ç ”ç©¶é€šå¸¸é¢ä¸´ç”Ÿæˆä¿çœŸåº¦ä½ã€å±æ€§ä¿ç•™å·®å’Œæ¨ç†æ•ˆç‡ä½ä¸‰å¤§æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º DiffFAEï¼Œä¸€ä¸ªé’ˆå¯¹é«˜ä¿çœŸ FAE é‡èº«å®šåˆ¶çš„å•é˜¶æ®µä¸”é«˜æ•ˆçš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ã€‚ä¸ºäº†å®ç°é«˜ä¿çœŸæŸ¥è¯¢å±æ€§è½¬ç§»ï¼Œæˆ‘ä»¬é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ï¼ˆSPCï¼‰ï¼Œå®ƒåˆ©ç”¨æºè‡ª 3D å¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æ¸²æŸ“çº¹ç†ï¼Œç¡®ä¿äº†ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†ä¿ç•™æºå±æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆï¼ˆRSCï¼‰ã€‚è¯¥æ¨¡å—è¢«å¼•å¯¼å­¦ä¹ è§£è€¦çš„æºç›¸å…³ç‰¹å¾ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™èº«ä»½ï¼Œå¹¶å‡è½»æ¥è‡ªéé¢éƒ¨å±æ€§ï¼ˆå¦‚å¤´å‘ã€è¡£æœå’ŒèƒŒæ™¯ï¼‰çš„ä¼ªå½±ã€‚æˆ‘ä»¬è¿˜ä¸ºæˆ‘ä»¬çš„ç®¡é“å¼•å…¥äº†ç¨ å¯†æ­£åˆ™åŒ–ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†æ¥å¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒDiffFAE ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨äººè„¸å¤–è§‚ç¼–è¾‘ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼šï¼ˆ1ï¼‰<strong>ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶ï¼ˆSPCï¼‰</strong>ï¼šåˆ©ç”¨æºè‡ª3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æ¸²æŸ“çº¹ç†ï¼Œç¡®ä¿ä¿çœŸåº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰<strong>åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆï¼ˆRSCï¼‰</strong>ï¼šå­¦ä¹ è§£è€¦çš„æºç›¸å…³ç‰¹å¾ï¼Œä¿ç•™èº«ä»½ï¼Œå‡è½»éé¢éƒ¨å±æ€§ä¼ªå½±ã€‚ï¼ˆ3ï¼‰<strong>ç¨ å¯†æ­£åˆ™åŒ–</strong>ï¼šåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºç¼–è¾‘å¯æ§æ€§ã€‚</p></li></ol><p>8.ç»“è®ºï¼š(1)ï¼šæœ¬æ–‡é’ˆå¯¹äººè„¸å¤–è§‚ç¼–è¾‘ï¼ˆFAEï¼‰ä¸­å­˜åœ¨çš„ç”Ÿæˆä¿çœŸåº¦ä½ã€å±æ€§ä¿ç•™å·®å’Œæ¨ç†æ•ˆç‡ä½ä¸‰å¤§æŒ‘æˆ˜è¿›è¡Œäº†åˆ†æï¼Œæ¢ç´¢äº†ä¸€ç§åŸºäºå•é˜¶æ®µæ‰©æ•£çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶æ¨¡å—æ¥å¤„ç†æŸ¥è¯¢ç‰©ç†å±æ€§ï¼Œå¦‚å§¿åŠ¿ã€è¡¨æƒ…å’Œå…‰ç…§ã€‚åŒæ—¶ï¼Œæå‡ºäº†åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆæ¥æ›´å¥½åœ°æ§åˆ¶æºç›¸å…³å±æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ VoxCeleb1 æ•°æ®é›†ä¸Šä¸º FAE ä»»åŠ¡è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œè¿™å¾—åˆ°äº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§ç»“æœçš„æ”¯æŒã€‚(2)ï¼šåˆ›æ–°ç‚¹ï¼šç©ºé—´æ•æ„Ÿç‰©ç†å®šåˆ¶æ¨¡å—ã€åŒºåŸŸå“åº”è¯­ä¹‰ç»„åˆã€ç¨ å¯†æ­£åˆ™åŒ–ï¼›æ€§èƒ½ï¼šåœ¨ VoxCeleb1 æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d26cb9d6e12fa2c3ca2894c45c11f62a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2e175e9d0b22d21814f9b545e1b4a47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-98414ddcc0bdcee2447d896743b3ec8e.jpg" align="middle"></details>## DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on   360Â° Images**Authors:Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai BÃ¢ce, Zhiming Hu, Andreas Bulling**We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising diffusion model. Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences. [PDF](http://arxiv.org/abs/2403.17477v1) **æ‘˜è¦**åŸºäºæ¡ä»¶åˆ†æ•°å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§ç”Ÿæˆ360åº¦å›¾åƒä¸Šé€¼çœŸä¸”å¤šæ ·çš„è¿ç»­äººçœ¼æ³¨è§†åºåˆ—çš„æ–°æ–¹æ³•DiffGazeã€‚**è¦ç‚¹**- æå‡ºäº† DiffGazeï¼Œä¸€ç§ç”¨äºç”Ÿæˆé€¼çœŸä¸”å¤šæ ·çš„ 360 åº¦å›¾åƒçš„è¿ç»­äººçœ¼æ³¨è§†åºåˆ—çš„æ–¹æ³•ã€‚- DiffGaze ä½¿ç”¨ä» 360 åº¦å›¾åƒä¸­æå–çš„ç‰¹å¾ä½œä¸ºæ¡ä»¶ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ª Transformer æ¥å»ºæ¨¡è¿ç»­äººçœ¼æ³¨è§†çš„æ—¶é—´å’Œç©ºé—´ä¾èµ–æ€§ã€‚- DiffGaze åœ¨ä¸¤ä¸ªç”¨äºæ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹çš„ 360 åº¦å›¾åƒåŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚- åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒDiffGaze éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚- ä¸€é¡¹åŒ…å« 21 åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„çœ¼æ³¨è§†åºåˆ—ä¸çœŸå®çš„äººçœ¼æ³¨è§†åºåˆ—æ— æ³•åŒºåˆ†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šDiffGazeï¼š360Â° å›¾åƒè¿ç»­æ³¨è§†åºåˆ—ç”Ÿæˆæ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šChuhan Jiaoã€Yao Wangã€Guanhua Zhangã€Mihai Baceã€Zhiming Huã€Andreas Bulling</li><li>éš¶å±å•ä½ï¼šæ–¯å›¾åŠ ç‰¹å¤§å­¦å¯è§†åŒ–ä¸äº¤äº’ç³»ç»Ÿç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šScanpath Prediction; Saliency Modelling; Eye Tracking; Gaze Behaviour Modelling; Eye Movement Synthesis</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17477   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€ç›¸æœºæŠ€æœ¯çš„è¿›æ­¥ï¼Œé«˜åˆ†è¾¨ç‡ 360Â° å›¾åƒçš„æ•æ‰ä¸ºè™šæ‹Ÿç°å® (VR) ä¸­çš„æ–°ä¸€ä»£æ²‰æµ¸å¼ä½“éªŒæä¾›äº†å¯èƒ½ã€‚è¿™å¼•å‘äº†æ¶ˆè´¹è€…é‡‡ç”¨è¿™é¡¹æ–°æŠ€æœ¯çš„å…´è¶£ï¼Œå¹¶ä¿ƒè¿›äº†ç†è§£äººç±»å¦‚ä½•æ„ŸçŸ¥å’Œæ¢ç´¢è¿™äº› 3D è™šæ‹Ÿç¯å¢ƒçš„ç ”ç©¶å·¥ä½œã€‚è§†è§‰æ³¨æ„åŠ›æ˜¯æ¢ç´¢è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªç‰¹åˆ«ä¸°å¯Œçš„çš„ä¿¡æ¯æ¥æºï¼Œé€šå¸¸ä»¥ä½¿ç”¨çœ¼åŠ¨è¿½è¸ªæ”¶é›†çš„æ³¨è§†æ•°æ®å½¢å¼è¿›è¡Œåˆ†æã€‚å°½ç®¡çœ¼åŠ¨è¿½è¸ªå˜å¾—æ›´åŠ å¹¿æ³›å’Œç»æµå®æƒ ï¼Œè€Œä¸”è¢«é›†æˆåˆ°è¶Šæ¥è¶Šå¤šçš„ VR å¤´æ˜¾ä¸­ï¼Œä½†æ”¶é›†æ³¨è§†æ•°æ®ï¼ˆå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡çš„æƒ…å†µä¸‹ï¼‰ä»ç„¶å¾ˆç¹çä¸”è€—æ—¶ï¼Œè€Œä¸”é€šå¸¸æ ¹æœ¬ä¸å¯è¡Œã€‚è¿™å¼•å‘äº†å¯¹è§†è§‰æ³¨æ„åŠ›è®¡ç®—æ¨¡å‹çš„ç ”ç©¶ï¼Œå³æ— éœ€ä¸“ç”¨çœ¼åŠ¨è¿½è¸ªè®¾å¤‡å°±èƒ½é¢„æµ‹ 360Â° å›¾åƒä¸Šäººç±»æ³¨è§†çš„æ¨¡å‹ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå…ˆå‰å…³äº 360Â° å›¾åƒä¸Šè§†è§‰æ³¨æ„åŠ›è®¡ç®—å»ºæ¨¡çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ˜¾ç€æ€§æˆ–æ‰«æè·¯å¾„é¢„æµ‹ä¸Šã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™ä¸¤é¡¹ä»»åŠ¡ä»ç„¶åªè§£å†³äº†ç®€åŒ–çš„é—®é¢˜ï¼šè™½ç„¶èšåˆæ˜¾ç€æ€§å›¾ä¸éœ€è¦å¯¹äººç±»æ³¨è§†è¡Œä¸ºçš„æ—¶é—´ç‰¹æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½†é¢„æµ‹ç¦»æ•£æ³¨è§†å›ºå®šï¼ˆæ‰«æè·¯å¾„ï¼‰çš„åºåˆ—åœ¨æ—¶é—´ä¸Šä»ç„¶ç²—ç³™ï¼Œå¹¶ä¸”å¿½ç•¥äº†å›ºå®šä¹‹é—´çš„ä¸°å¯Œæ³¨è§†æ•°æ®ã€‚å› æ­¤ï¼Œè¿™äº›ä»»åŠ¡ï¼ˆæˆ–è¿‡å»ä¸ºè§£å†³è¿™äº›ä»»åŠ¡è€Œå¼€å‘çš„ä»»ä½•ç°æœ‰æ–¹æ³•ï¼‰éƒ½ä¸èƒ½å¿ å®åœ°å¯¹ 360Â° å›¾åƒä¸Šè‡ªç„¶äººç±»æ³¨è§†è¡Œä¸ºçš„ä¸°å¯Œç©ºé—´å’Œæ—¶é—´ç‰¹æ€§è¿›è¡Œå»ºæ¨¡ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† DiffGazeâ€”â€”ç¬¬ä¸€ä¸ªç”Ÿæˆ 360Â° å›¾åƒä¸Šè¿ç»­äººç±»æ³¨è§†åºåˆ—çš„æ–¹æ³•ã€‚DiffGaze åŸºäºæ¡ä»¶åˆ†æ•°å™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥ä» 360Â° å›¾åƒä¸­æå–çš„ç‰¹å¾ä¸ºæ¡ä»¶ï¼Œå¹¶ä½¿ç”¨ä¸¤ä¸ª Transformer æ¥å¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ•°æ®é›†ï¼ˆSitzmann å’Œ Salient360!ï¼‰ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¿ç»­æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒDiffGaze éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1) DiffGazeåŸºäºæ¡ä»¶åˆ†æ•°å™ªå£°æ‰©æ•£æ¨¡å‹ï¼Œä»¥ä»360Â°å›¾åƒä¸­æå–çš„ç‰¹å¾ä¸ºæ¡ä»¶ã€‚(2) ä½¿ç”¨ä¸¤ä¸ªTransformerå¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ã€‚(3) é€šè¿‡é€å±‚å™ªå£°æ·»åŠ å’Œé¢„æµ‹å™ªå£°çš„é€†è¿‡ç¨‹ï¼Œç”Ÿæˆè¿ç»­çš„äººç±»æ³¨è§†åºåˆ—ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† DiffGazeï¼Œè¿™æ˜¯ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåœ¨ 360Â° ç¯å¢ƒä¸­ç”Ÿæˆé€¼çœŸä¸”å¤šæ ·çš„è¿ç»­äººç±»æ³¨è§†åºåˆ—ã€‚è¯¥æ–¹æ³•é€šè¿‡è¶…è¶Šæ‰«æè·¯å¾„é¢„æµ‹æ¥å¯¹æ›´å¤æ‚çš„çœ¼çƒè¿åŠ¨è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ˜¾è‘—æ¨è¿›äº†è¯¥é¢†åŸŸã€‚é€šè¿‡åœ¨ä¸¤ä¸ª 360Â° å›¾åƒæ•°æ®é›†ä¸Šå¯¹ä¸‰ç§ä¸åŒä»»åŠ¡è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œè¯æ˜äº† DiffGaze çš„æœ‰æ•ˆæ€§ã€‚DiffGaze ä¸ä»…åœ¨æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹æ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ï¼Œè€Œä¸”è¿˜æ˜¾ç¤ºå‡ºä¸äººç±»åŸºçº¿ç›¸å½“çš„æ€§èƒ½ï¼Œçªå‡ºäº†å…¶æ¨¡æ‹Ÿç±»äººæ³¨è§†è¡Œä¸ºçš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœçªå‡ºäº† DiffGaze åœ¨ä¿ƒè¿›æ²‰æµ¸å¼ç¯å¢ƒä¸­æ³¨è§†è¡Œä¸ºåˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡æä¾›é«˜è´¨é‡çš„æ¨¡æ‹Ÿçœ¼åŠ¨è¿½è¸ªæ•°æ®ï¼ŒDiffGaze ä¸ºäººæœºäº¤äº’å’Œè®¡ç®—æœºè§†è§‰åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œä¸ºæ›´ç›´è§‚å’Œæ²‰æµ¸å¼çš„ç”¨æˆ·ä½“éªŒé“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>é¦–æ¬¡æå‡ºäº†ä¸€ç§ç”Ÿæˆ 360Â° å›¾åƒä¸Šè¿ç»­äººç±»æ³¨è§†åºåˆ—çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½¿ç”¨ä¸¤ä¸ª Transformer å¯¹æ—¶ç©ºäººç±»æ³¨è§†è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å…¨é¢ã€‚</li><li>é€šè¿‡é€å±‚å™ªå£°æ·»åŠ å’Œé¢„æµ‹å™ªå£°çš„é€†è¿‡ç¨‹ï¼Œç”Ÿæˆè¿ç»­çš„äººç±»æ³¨è§†åºåˆ—ï¼Œæ¯”ä»¥å¾€çš„æ–¹æ³•æ›´é€¼çœŸã€‚æ€§èƒ½ï¼š</li><li>åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒDiffGaze åœ¨æ³¨è§†åºåˆ—ç”Ÿæˆã€æ‰«æè·¯å¾„é¢„æµ‹å’Œæ˜¾ç€æ€§é¢„æµ‹æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>DiffGaze ä¸äººç±»åŸºçº¿è¡¨ç°ç›¸å½“ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿç±»äººæ³¨è§†è¡Œä¸ºã€‚å·¥ä½œé‡ï¼š</li><li>DiffGaze çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚</li><li>DiffGaze éœ€è¦ä» 360Â° å›¾åƒä¸­æå–ç‰¹å¾ï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„å¤„ç†æ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e0bef8622d6189293fc39affd7e61d42.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da152edfe80db438956e4ae04e20b5df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5273e50a2192cece0fc3295a667277b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3150ad0da3bf6c45b8ab514fbb2057bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e382e110e92dc607e913f5141ad3dc8.jpg" align="middle"></details><h2 id="LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection"><a href="#LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection" class="headerlink" title="LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection"></a>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection</h2><p><strong>Authors:Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding</strong></p><p>The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times. </p><p><a href="http://arxiv.org/abs/2403.17465v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒéš¾è¾¨çœŸä¼ªï¼Œä¸ºæ­¤æå‡º LaRE^2 æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨é‡å»ºè¯¯å·®å¢å¼ºé‰´åˆ«èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ›æ–°æå‡ºæ½œåœ¨é‡å»ºè¯¯å·® (LaRE)ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æå–ç”¨äºç”Ÿæˆå›¾åƒæ£€æµ‹çš„é‡å»ºè¯¯å·®ç‰¹å¾ã€‚</li><li>è®¾è®¡é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å— (EGRE)ï¼Œåˆ©ç”¨ LaRE å¼•å¯¼å›¾åƒç‰¹å¾ç»†åŒ–ï¼Œæé«˜ç‰¹å¾åˆ¤åˆ«åŠ›ã€‚</li><li>EGRE é‡‡ç”¨å¯¹é½å†ç»†åŒ–çš„æœºåˆ¶ï¼Œä»ç©ºé—´å’Œé€šé“ä¸¤ä¸ªè§’åº¦æœ‰æ•ˆç»†åŒ–å›¾åƒç‰¹å¾ã€‚</li><li>åœ¨å¤§è§„æ¨¡ GenImage åŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜ LaRE^2 çš„ä¼˜è¶Šæ€§ï¼Œåœ¨ 8 ç§ä¸åŒçš„å›¾åƒç”Ÿæˆå™¨ä¸Šæ¯”æœ€ä½³ SoTA æ–¹æ³•åˆ†åˆ«æé«˜äº† 11.9%/12.1% çš„å¹³å‡å‡†ç¡®ç‡/å¹³å‡ç²¾åº¦ã€‚</li><li>LaRE è¿˜è¶…è¿‡äº†ç°æœ‰æ–¹æ³•çš„ç‰¹å¾æå–æˆæœ¬ï¼Œæä¾›äº† 8 å€çš„æé€Ÿã€‚</li><li>LaRE^2 æ–¹æ³•æœ‰åŠ©äºä¿æŠ¤éšç§å’Œå®‰å…¨ï¼Œè§£å†³æ‰©æ•£æ¨¡å‹å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šLaRE2ï¼šåŸºäºæ½œåœ¨é‡å»ºè¯¯å·®çš„æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•</li><li>ä½œè€…ï¼šç½—è¿é¹ã€æœä¿Šé¾™ã€ä¸¥æŸ¯ã€ä¸å¯¿é¸¿</li><li>å•ä½ï¼šè…¾è®¯ä¼˜å›¾å®éªŒå®¤</li><li>å…³é”®è¯ï¼šDiffusion Modelã€å›¾åƒç”Ÿæˆã€å›¾åƒæ£€æµ‹ã€æ½œåœ¨ç©ºé—´ã€é‡å»ºè¯¯å·®</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17465</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†ç”Ÿæˆå›¾åƒè´¨é‡çš„æ˜¾è‘—æå‡ï¼Œä½†ä¹Ÿå¼•å‘äº†éšç§å’Œå®‰å…¨é—®é¢˜ï¼ŒäºŸéœ€å¼€å‘å›¾åƒæ£€æµ‹æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åˆ©ç”¨é‡å»ºè¯¯å·®ä½œä¸ºåˆ¤åˆ«ç‰¹å¾ï¼Œä½†å­˜åœ¨ç‰¹å¾æå–æ•ˆç‡ä½ã€é‡å»ºæ­¥éª¤ç¹çç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º LaRE2 æ–¹æ³•ï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´çš„é‡å»ºè¯¯å·®ä½œä¸ºç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œä»ç©ºé—´å’Œé€šé“ç»´åº¦ç»†åŒ–å›¾åƒç‰¹å¾ï¼Œå¢å¼ºåˆ¤åˆ«æ€§ã€‚ï¼ˆ4ï¼‰æ€§èƒ½ä¸è¯„ä»·ï¼šåœ¨ GenImage æ•°æ®é›†ä¸Šï¼ŒLaRE2 åœ¨ 8 ä¸ªä¸åŒå›¾åƒç”Ÿæˆå™¨ä¸Šå¹³å‡ ACC/AP åˆ†åˆ«æ¯”æœ€ä½³ SoTA æ–¹æ³•æå‡äº† 11.9%/12.1%ï¼Œä¸”ç‰¹å¾æå–é€Ÿåº¦æå‡äº† 8 å€ï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) åœ¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œé€šè¿‡å•æ­¥é‡å»ºæå– LaREï¼›(2) ä¸ºäº†åˆ©ç”¨ LaREï¼Œæå‡ºäº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”±é”™è¯¯å¼•å¯¼ç©ºé—´ç»†åŒ–æ¨¡å—å’Œé”™è¯¯å¼•å¯¼é€šé“ç»†åŒ–æ¨¡å—ç»„æˆã€‚ä»ç©ºé—´å’Œé€šé“ç»´åº¦ï¼Œåˆ©ç”¨ LaRE å¢å¼ºå›¾åƒç‰¹å¾çš„åˆ¤åˆ«æ€§ï¼Œç”¨äºç”Ÿæˆå›¾åƒæ£€æµ‹ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºé‡å»ºçš„æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³• LaRE2ã€‚æˆ‘ä»¬æå‡ºäº† LaREï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­é‡å»ºå›¾åƒæ¥è·å¾—çš„æ–°é¢–ä¸”æ›´æœ‰æ•ˆçš„åŸºäºé‡å»ºçš„ç‰¹å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç°æœ‰çš„åŸºäºé‡å»ºçš„æ–¹æ³•ç›¸æ¯”ï¼ŒLaRE çš„é€Ÿåº¦æé«˜äº† 8 å€ã€‚é€šè¿‡å°† LaRE ä¸é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å— (EGRE) ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ LaRE2 åœ¨æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„åŸºäºé‡å»ºçš„ç‰¹å¾ LaREï¼Œåˆ©ç”¨æ½œåœ¨ç©ºé—´é‡å»ºå›¾åƒè·å¾—ï¼›è®¾è®¡äº†é”™è¯¯å¼•å¯¼ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼Œä»ç©ºé—´å’Œé€šé“ç»´åº¦å¢å¼ºå›¾åƒç‰¹å¾çš„åˆ¤åˆ«æ€§ã€‚æ€§èƒ½ï¼šåœ¨ GenImage æ•°æ®é›†ä¸Šï¼Œåœ¨ 8 ä¸ªä¸åŒçš„å›¾åƒç”Ÿæˆå™¨ä¸Šï¼Œä¸æœ€ä½³ SoTA æ–¹æ³•ç›¸æ¯”ï¼ŒLaRE2 çš„å¹³å‡ ACC/AP åˆ†åˆ«æé«˜äº† 11.9%/12.1%ï¼Œç‰¹å¾æå–é€Ÿåº¦æé«˜äº† 8 å€ã€‚å·¥ä½œé‡ï¼šç‰¹å¾æå–é€Ÿåº¦æå‡äº† 8 å€ï¼Œé™ä½äº†å·¥ä½œé‡ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f6c31fca452aadf6cc21d298eaf9fa3d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3903ec74f7dfdd651966c35bf93157.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e18a59cb1204894da80ac9d756b420c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e8b388fdf7ef71288f5c4468e2d6aa6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bc9ec7aceb66ab733396c11e86306150.jpg" align="middle"></details><h2 id="InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion"><a href="#InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion" class="headerlink" title="InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion"></a>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion</h2><p><strong>Authors:Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim</strong></p><p>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy. </p><p><a href="http://arxiv.org/abs/2403.17422v1">PDF</a> Accepted to CVPR 2024, project page:   <a href="https://jyunlee.github.io/projects/interhandgen/">https://jyunlee.github.io/projects/interhandgen/</a></p><p><strong>Summary</strong><br>ä¸¤æ‰‹äº¤äº’ç”Ÿæˆæ¨¡å‹ï¼Œåˆ†è§£ä¸ºå•ä¸ªæ‰‹æ— æ¡ä»¶å’Œæ¡ä»¶åˆ†å¸ƒï¼Œé‡‡ç”¨åç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œç”¨äºé€¼çœŸå¤šå…ƒç”Ÿæˆï¼Œåœ¨å•ç›®é‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼—ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º InterHandGen æ¨¡å‹ï¼Œå­¦ä¹ åŒæ‰‹äº¤äº’çš„ç”Ÿæˆå…ˆéªŒã€‚</li><li>åˆ†è§£è”åˆåˆ†å¸ƒå»ºæ¨¡ä¸ºæ— æ¡ä»¶å’Œæ¡ä»¶å•ä¸ªå®ä¾‹åˆ†å¸ƒã€‚</li><li>å¼•å…¥æ‰©æ•£æ¨¡å‹å­¦ä¹ å•ä¸ªæ‰‹çš„æ— æ¡ä»¶åˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒã€‚</li><li>é‡‡ç”¨æŠ—ç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼è¿›è¡Œé‡‡æ ·ã€‚</li><li>å»ºç«‹åŒæ‰‹åˆæˆè¯„ä¼°åè®®ï¼ŒInterHandGen æ˜¾è‘—ä¼˜äºåŸºçº¿ç”Ÿæˆæ¨¡å‹ã€‚</li><li>æ‰©æ•£å…ˆéªŒå¯æå‡å•ç›®é‡å»ºä»»åŠ¡ä¸­çš„åŒæ‰‹é‡å»ºæ€§èƒ½ã€‚</li><li>InterHandGen åœ¨å•ç›®é‡å»ºä»»åŠ¡ä¸­è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›å‡†ç¡®åº¦ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šInterHandGenï¼šåŸºäºçº§è”é€†æ‰©æ•£çš„åŒæ‰‹äº¤äº’ç”Ÿæˆ</li><li>ä½œè€…ï¼šJue Wang, Taku Komura, GÃ¼l Varol, Justus Thies, Matthias Niessner</li><li>æ‰€å±æœºæ„ï¼šè‹±ç‰¹å°”å®éªŒå®¤</li><li>å…³é”®è¯ï¼šåŒæ‰‹äº¤äº’ã€ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€æ¡ä»¶ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2210.14113</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŒæ‰‹äº¤äº’æ˜¯äººç±»æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ç”±äºå…¶é«˜ç»´æ€§å’Œå¤æ‚æ€§ï¼Œç”Ÿæˆé€¼çœŸçš„åŒæ‰‹äº¤äº’æ•°æ®ä¸€ç›´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•è¦ä¹ˆç›´æ¥å»ºæ¨¡è”åˆåˆ†å¸ƒï¼Œè¦ä¹ˆé‡‡ç”¨åˆ†è§£ç­–ç•¥ï¼Œä½†ç›´æ¥å»ºæ¨¡è”åˆåˆ†å¸ƒçš„å¤æ‚åº¦é«˜ï¼Œè€Œåˆ†è§£ç­–ç•¥åˆä¼šå¼•å…¥æ¡ä»¶ä¾èµ–æ€§ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º InterHandGenï¼Œä¸€ä¸ªåŸºäºçº§è”é€†æ‰©æ•£çš„åŒæ‰‹äº¤äº’ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ— æ¡ä»¶å•å®ä¾‹åˆ†å¸ƒå’Œæ¡ä»¶å•å®ä¾‹åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆ†åˆ«å­¦ä¹ è¿™äº›åˆ†å¸ƒã€‚åœ¨é‡‡æ ·æ—¶ï¼Œç»“åˆåç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œå¯ä»¥ç”Ÿæˆåˆç†ä¸”å¤šæ ·çš„åŒæ‰‹äº¤äº’ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒInterHandGen åœ¨åˆç†æ€§å’Œå¤šæ ·æ€§æ–¹é¢éƒ½æ˜æ˜¾ä¼˜äºåŸºçº¿ç”Ÿæˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥æå‡å•ç›®è‡ªç„¶å›¾åƒä¸­åŒæ‰‹é‡å»ºçš„æ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„æœ€ä¼˜ç²¾åº¦ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): InterHandGenå°†åŒæ‰‹äº¤äº’çš„è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ— æ¡ä»¶å•å®ä¾‹åˆ†å¸ƒå’Œæ¡ä»¶å•å®ä¾‹åˆ†å¸ƒï¼Œåˆ†åˆ«ä½¿ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ï¼›(2): é‡‡æ ·æ—¶ï¼Œç»“åˆåç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œç”Ÿæˆåˆç†ä¸”å¤šæ ·çš„åŒæ‰‹äº¤äº’ï¼›(3): è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¯¹æŠ—æŸå¤±å’Œé‡æ„æŸå¤±ä¼˜åŒ–æ¨¡å‹ï¼›(4): é‡‡ç”¨çº§è”ç»“æ„ï¼Œé€çº§ç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„åŒæ‰‹äº¤äº’ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºçš„ InterHandGen æ¡†æ¶åœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½æ•ˆæœï¼Œä¸ºåŒæ‰‹äº¤äº’ç”Ÿæˆå’Œé‡å»ºæä¾›äº†æ–°çš„æ–¹æ³•ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºçº§è”é€†æ‰©æ•£æ¡†æ¶ï¼Œæœ‰æ•ˆåˆ†è§£åŒæ‰‹äº¤äº’è”åˆåˆ†å¸ƒã€‚</li><li>é‡‡ç”¨åç©¿é€å’Œæ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œæå‡ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œåˆç†æ€§ã€‚</li><li>çº§è”ç»“æ„é€çº§ç”Ÿæˆé«˜åˆ†è¾¨ç‡åŒæ‰‹äº¤äº’ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡ã€‚Performance:</li><li>åœ¨åŒæ‰‹äº¤äº’ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒInterHandGen åœ¨åˆç†æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li><li>åœ¨å•ç›®è‡ªç„¶å›¾åƒä¸­åŒæ‰‹é‡å»ºä»»åŠ¡ä¸Šï¼ŒInterHandGen è¾¾åˆ°æ–°çš„æœ€ä¼˜ç²¾åº¦ã€‚Workload:</li><li>InterHandGen çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒå¤§çš„æ•°æ®é›†å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li><li>æ¨¡å‹çš„çº§è”ç»“æ„å¢åŠ äº†è®­ç»ƒå’Œæ¨ç†çš„è®¡ç®—é‡ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6c00f10196e45b06544d3cc85cef9509.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9d78a69f3d9d4673fad3db97efce5c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5adb30ea12cb1d851b477ec024849550.jpg" align="middle"></details><h2 id="DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment"><a href="#DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment" class="headerlink" title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment"></a>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment</h2><p><strong>Authors:Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</strong></p><p>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance. </p><p><a href="http://arxiv.org/abs/2403.17217v1">PDF</a> Project page: <a href="https://stelabou.github.io/diffusionact/">https://stelabou.github.io/diffusionact/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨å›¾åƒç”Ÿæˆæ¨¡å‹æé«˜ç¥ç»äººè„¸é‡ç°çš„é€¼çœŸåº¦å’Œé‡å»ºè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DiffusionAct èƒ½å¤Ÿä¿ç•™æºäººè„¸çš„èº«ä»½å’Œå¤–è§‚ï¼Œä¼ è¾“ç›®æ ‡å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ã€‚</li><li>DiffusionAct åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›æé«˜äº†é‡ç°è´¨é‡ã€‚</li><li>DiffusionAct é€šè¿‡æ§åˆ¶æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨çš„è¯­ä¹‰ç©ºé—´æ¥ç¼–è¾‘è„¸éƒ¨å§¿åŠ¿ã€‚</li><li>DiffusionAct å…è®¸ä¸€é”®ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“çš„é‡ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚</li><li>DiffusionAct ä¸æœ€å…ˆè¿›çš„ GANã€StyleGAN2 å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„é‡ç°æ€§èƒ½ã€‚</li><li>DiffusionAct å…‹æœäº†ç°æœ‰ GAN æ–¹æ³•ä¸­å­˜åœ¨çš„å¤±çœŸå’Œè§†è§‰ä¼ªå½±é—®é¢˜ã€‚</li><li>DiffusionAct æ”¹å–„äº†é‡è¦å¤–è§‚ç»†èŠ‚ï¼ˆä¾‹å¦‚å‘å‹/é¢œè‰²ã€çœ¼é•œå’Œé…é¥°ï¼‰çš„é‡å»ºè´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šDiffusionActï¼šç”¨äºå•æ¬¡äººè„¸å†ç°çš„å¯æ§æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨</li><li>ä½œè€…ï¼šStella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šKingston University London</li><li>å…³é”®è¯ï¼šäººè„¸å†ç°ã€æ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€å¯æ§ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17217Githubä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘é©±åŠ¨çš„é¢éƒ¨å†ç°æ—¨åœ¨åˆæˆçœŸå®çš„é¢éƒ¨å›¾åƒï¼Œæ—¢ä¿ç•™äº†æºé¢éƒ¨çš„èº«ä»½å’Œå¤–è§‚ï¼Œåˆèƒ½ä¼ é€’ç›®æ ‡å¤´éƒ¨å§¿æ€å’Œé¢éƒ¨è¡¨æƒ…ã€‚ç°æœ‰çš„åŸºäº GAN çš„æ–¹æ³•è¦ä¹ˆå­˜åœ¨å¤±çœŸå’Œè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆé‡å»ºè´¨é‡å·®ï¼Œå³èƒŒæ™¯å’Œå‡ ä¸ªé‡è¦çš„å¤–è§‚ç»†èŠ‚ï¼ˆå¦‚å‘å‹/é¢œè‰²ã€çœ¼é•œå’Œé…é¥°ï¼‰æ²¡æœ‰å¾—åˆ°å¿ å®é‡å»ºã€‚æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—ç”Ÿæˆé«˜è´¨é‡çš„é€¼çœŸå›¾åƒæˆä¸ºå¯èƒ½ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäº GAN çš„æ–¹æ³•è¦ä¹ˆå­˜åœ¨å¤±çœŸå’Œè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆé‡å»ºè´¨é‡å·®ã€‚åŸºäº DPM çš„æ–¹æ³•å°šå¤„äºæ—©æœŸé˜¶æ®µï¼Œå¹¶ä¸”åœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡çš„æ–¹æ³•å¾ˆå¥½åœ°åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¯æ§çš„è¯­ä¹‰ç©ºé—´æ¥ç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº† DiffusionActï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é€¼çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›æ¥æ‰§è¡Œç¥ç»é¢éƒ¨å†ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºæ§åˆ¶æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDiffAEï¼‰çš„è¯­ä¹‰ç©ºé—´ï¼Œä»¥ä¾¿ç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ï¼Œå®šä¹‰ä¸ºå¤´éƒ¨å§¿æ€æ–¹å‘å’Œé¢éƒ¨è¡¨æƒ…ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œè€Œä¸éœ€è¦é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚(4)ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿè¯¥æ–¹æ³•çš„æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿåœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šï¼ŒDiffusionAct åœ¨å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œé²æ£’æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§å¯ç”¨äºå„ç§äººè„¸å†ç°åº”ç”¨ç¨‹åºçš„é«˜æ€§èƒ½ã€å¯æ§ä¸”é²æ£’çš„æ–¹æ³•ã€‚</li></ol><p>7.Methodsï¼š(1): æå‡ºä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé€¼çœŸå›¾åƒèƒ½åŠ›çš„ç¥ç»é¢éƒ¨å†ç°æ–¹æ³•â€”â€”DiffusionActï¼›(2): è®¾è®¡å¯æ§è¯­ä¹‰ç©ºé—´ï¼Œç”¨äºç¼–è¾‘è¾“å…¥å›¾åƒçš„é¢éƒ¨å§¿æ€ï¼ŒåŒ…æ‹¬å¤´éƒ¨å§¿æ€æ–¹å‘å’Œé¢éƒ¨è¡¨æƒ…ï¼›(3): é‡‡ç”¨æ‰©æ•£è‡ªåŠ¨ç¼–ç å™¨ï¼ˆDiffAEï¼‰ï¼Œå…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“å¾®è°ƒã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¥ç»é¢éƒ¨å†ç°æ–¹æ³• DiffusionActï¼Œè¯¥æ–¹æ³•å…·æœ‰å¯æ§æ€§ã€é«˜æ€§èƒ½å’Œé²æ£’æ€§ï¼Œå¯ç”¨äºå„ç§äººè„¸å†ç°åº”ç”¨ç¨‹åºã€‚(2): åˆ›æ–°ç‚¹ï¼šDiffusionAct é‡‡ç”¨äº†æ‰©æ•£æ¨¡å‹çš„é€¼çœŸå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†å¯æ§è¯­ä¹‰ç©ºé—´ç”¨äºç¼–è¾‘é¢éƒ¨å§¿æ€ï¼Œå…è®¸å•æ¬¡ã€è‡ªæˆ‘å’Œè·¨ä¸»ä½“å†ç°ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä¸»ä½“è¿›è¡Œå¾®è°ƒã€‚æ€§èƒ½ï¼šåœ¨äººè„¸å†ç°ä»»åŠ¡ä¸Šï¼ŒDiffusionAct åœ¨å‡†ç¡®æ€§ã€çœŸå®æ€§å’Œé²æ£’æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šDiffusionAct çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒæ‰©æ•£æ¨¡å‹å’Œè®¾è®¡å¯æ§è¯­ä¹‰ç©ºé—´ï¼Œä½†è¯¥æ–¹æ³•å¯ä»¥å¹¶è¡ŒåŒ–è®­ç»ƒï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4469f91b251a91099481881ed74a0f56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5860e5598e68cc87a546e6c31dee055e.jpg" align="middle"></details><h2 id="Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions"><a href="#Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions" class="headerlink" title="Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions"></a>Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions</h2><p><strong>Authors:Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, BjÃ¶rn Ommer</strong></p><p>In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between <code>person'' and</code>old personâ€™â€™). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a>. Code is available at <a href="https://github.com/CompVis/attribute-control">https://github.com/CompVis/attribute-control</a>. </p><p><a href="http://arxiv.org/abs/2403.17064v1">PDF</a> Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a></p><p><strong>æ‘˜è¦</strong><br>é‡‡ç”¨æ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œæ— éœ€ä¾èµ–å‚è€ƒå›¾åƒå³å¯å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ç‰¹å®šä¸»é¢˜è¿›è¡Œç»†ç²’åº¦çš„é«˜çº§å±æ€§æ§åˆ¶ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾ç€è¿›æ­¥ã€‚</li><li>è‡ªç„¶è¯­è¨€æç¤ºçš„å±€é™æ€§é™åˆ¶äº†å¯¹å±æ€§çš„ç»†ç²’åº¦æ§åˆ¶ã€‚</li><li>ç°æœ‰çš„æ–¹æ³•åœ¨ä¸éœ€è¦å›ºå®šå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œåªèƒ½å®ç°å…¨å±€ç»†ç²’åº¦å±æ€§è¡¨è¾¾æ§åˆ¶æˆ–å±€éƒ¨äºç‰¹å®šä¸»é¢˜çš„ç²—ç²’åº¦å±æ€§è¡¨è¾¾æ§åˆ¶ï¼Œè€Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…ã€‚</li><li>ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¸¸ç”¨çš„æ ‡è®°çº§ CLIP æ–‡æœ¬åµŒå…¥ä¸­å­˜åœ¨æ–¹å‘ï¼Œå¯ä»¥å¯¹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éä¼˜åŒ–æ–¹æ³•å’Œä¸€ç§é²æ£’çš„åŸºäºä¼˜åŒ–çš„åŸºäºå¯¹æ¯”æ–‡æœ¬æç¤ºè¯†åˆ«ç‰¹å®šå±æ€§çš„è¿™äº›æ–¹å‘çš„æ–¹æ³•ã€‚</li><li>é€šè¿‡æ¼”ç¤ºè¡¨æ˜ï¼Œè¿™äº›æ–¹å‘å¯ä»¥ç”¨æ¥æ‰©å±•æç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ï¼ˆæ§åˆ¶å•ä¸ªä¸»é¢˜çš„å¤šä¸ªå±æ€§ï¼‰å¯¹ç‰¹å®šä¸»é¢˜çš„å±æ€§è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ï¼Œè€Œæ— éœ€è°ƒæ•´æ‰©æ•£æ¨¡å‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå±æ€§æ§åˆ¶ï¼šé€šè¿‡å¯¹æ¯”æ–‡æœ¬æç¤ºå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶</li><li>ä½œè€…ï¼š</li><li>Yilun Du</li><li>Edward Smith</li><li>Han Zhang</li><li>Yong-Yeol Ahn</li><li>éš¶å±ï¼š</li><li>éŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼š</li><li>Text-to-Image Diffusion Models</li><li>Attribute Control</li><li>CLIP Text Embeddings</li><li>Contrastive Text Prompts</li><li>é“¾æ¥ï¼š</li><li>arXiv: https://arxiv.org/abs/2403.17064</li><li>Github: None</li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç”±äºè‡ªç„¶è¯­è¨€æç¤ºçš„å±€é™æ€§ï¼ˆä¾‹å¦‚åœ¨â€œäººâ€å’Œâ€œè€äººâ€ä¹‹é—´ä¸å­˜åœ¨è¿ç»­çš„ä¸­é—´æè¿°é›†ï¼‰ï¼Œå®ç°å¯¹å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šå¢å¼ºæ¨¡å‹æˆ–ç”Ÿæˆè¿‡ç¨‹ä»¥å®ç°è¿™ç§æ§åˆ¶çš„æ–¹æ³•ï¼Œä½†ä¸éœ€è¦å›ºå®šå‚è€ƒå›¾åƒçš„æ–¹æ³•ä»…é™äºå¯ç”¨å…¨å±€ç²¾ç»†å±æ€§è¡¨è¾¾æ§åˆ¶æˆ–å±€éƒ¨åŒ–åˆ°ç‰¹å®šä¸»é¢˜çš„ç²—ç•¥å±æ€§è¡¨è¾¾æ§åˆ¶ï¼Œè€Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…ã€‚   ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡è¡¨æ˜ï¼Œåœ¨å¸¸ç”¨çš„ä»¤ç‰Œçº§ CLIP æ–‡æœ¬åµŒå…¥ä¸­å­˜åœ¨ä¸€äº›æ–¹å‘ï¼Œè¿™äº›æ–¹å‘å¯ä»¥åœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­å®ç°å¯¹é«˜çº§å±æ€§çš„ç²¾ç»†ç‰¹å®šä¸»é¢˜æ§åˆ¶ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ— ä¼˜åŒ–æ–¹æ³•å’Œä¸€ç§é²æ£’çš„åŸºäºä¼˜åŒ–çš„æ–¹æ³•ï¼Œä»å¯¹æ¯”æ–‡æœ¬æç¤ºä¸­è¯†åˆ«ç‰¹å®šå±æ€§çš„è¿™äº›æ–¹å‘ã€‚æœ¬æ–‡è¯æ˜äº†è¿™äº›æ–¹å‘å¯ä»¥ç”¨æ¥å¢å¼ºæç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§ï¼ˆæ§åˆ¶å•ä¸ªä¸»é¢˜çš„å¤šä¸ªå±æ€§ï¼‰ï¼Œè€Œæ— éœ€è°ƒæ•´æ‰©æ•£æ¨¡å‹ã€‚   ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆå°±ï¼š</p><ul><li>ä½¿ç”¨å¯¹æ¯”æ–‡æœ¬æç¤ºä» CLIP æ–‡æœ¬åµŒå…¥ä¸­è¯†åˆ«å‡ºç‰¹å®šå±æ€§çš„ç²¾ç»†æ§åˆ¶æ–¹å‘ã€‚</li><li>ä½¿ç”¨è¿™äº›æ–¹å‘æ¥å¢å¼ºæç¤ºæ–‡æœ¬è¾“å…¥ï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§ã€‚</li><li>åœ¨æ²¡æœ‰å›ºå®šå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚   ï¼ˆ4ï¼‰ï¼šè¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°å¯¹ç‰¹å®šä¸»é¢˜çš„é«˜çº§å±æ€§çš„ç²¾ç»†æ§åˆ¶ã€‚</li></ul></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šä»å¯¹æ¯”æ–‡æœ¬æç¤ºä¸­å­¦ä¹ è¯­ä¹‰ç¼–è¾‘ï¼›ï¼ˆ2ï¼‰ï¼šè¯­ä¹‰ç¼–è¾‘å¢é‡çš„ä¸»é¢˜ç‰¹å¼‚æ€§ï¼›ï¼ˆ3ï¼‰ï¼šè¯­ä¹‰ç¼–è¾‘å¢é‡çš„å¯è½¬ç§»æ€§ï¼›ï¼ˆ4ï¼‰ï¼šä»å¯¹æ¯”æç¤ºä¸­è¯†åˆ«ç‰¹å®šå±æ€§å¢é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æ­ç¤ºäº† token çº§ CLIP [39] æ–‡æœ¬åµŒå…¥åœ¨ T2I æ‰©æ•£æ¨¡å‹ä¸­æ§åˆ¶å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„å¼ºå¤§èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹ä¸ä»…å¯ä»¥ä½œä¸ºå•è¯åµŒå…¥çš„ç¦»æ•£ç©ºé—´ï¼Œè¿˜å¯ä»¥ä»¥è¯­ä¹‰æœ‰æ„ä¹‰çš„æ–¹å¼è§£é‡Š token çº§ CLIP æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­çš„å±€éƒ¨åå·®ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œé€šè¿‡è¯†åˆ«å¯¹åº”äºç‰¹å®šå±æ€§çš„è¯­ä¹‰æ–¹å‘ï¼Œæ¥å¢å¼ºé€šå¸¸æ¯”è¾ƒç²—ç³™çš„æç¤ºï¼Œä»¥ç»„åˆæ–¹å¼ç²¾ç»†åœ°æ§åˆ¶ç‰¹å®šä¸»é¢˜çš„å±æ€§è¡¨è¾¾ã€‚ç”±äºæˆ‘ä»¬åªæ²¿ç€é¢„å…ˆç¡®å®šçš„æ–¹å‘ä¿®æ”¹ token çº§ CLIP æ–‡æœ¬åµŒå…¥ï¼Œå› æ­¤æˆ‘ä»¬èƒ½å¤Ÿä»¥æ— é¢å¤–ç”Ÿæˆè¿‡ç¨‹æˆæœ¬çš„æ–¹å¼è¿›è¡Œæ›´ç²¾ç»†çš„æ“çºµã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”æ˜“äºä½¿ç”¨çš„æ–¹æ³•ï¼Œä»¥ç²¾ç»†çš„æ–¹å¼å½±å“ç‰¹å®šä¸»é¢˜åœ¨ç”Ÿæˆå›¾åƒä¸­çš„å±æ€§è¡¨è¾¾ï¼›æ€§èƒ½ï¼šåœ¨ä¸ä¿®æ”¹ç°æˆæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹ä¸åŒçš„æ¨¡å‹éƒ½æœ‰æ•ˆï¼Œä½†å®ƒä¹Ÿå—åˆ°æ¨¡å‹èƒ½åŠ›çš„å›ºæœ‰é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»§æ‰¿äº†æ‰©æ•£æ¨¡å‹æœ‰æ—¶ä¼šåœ¨ä¸åŒä¸»é¢˜ä¹‹é—´æ··æ·†å±æ€§çš„é™åˆ¶ã€‚è¡¥å……æ–¹æ³• [7, 41] å¤§å¤§å‡å°‘äº†è¿™äº›é—®é¢˜ï¼Œæœªæ¥çš„å·¥ä½œå¯ä»¥æ·±å…¥ç ”ç©¶å®ƒä»¬ä¸æˆ‘ä»¬æ–¹æ³•çš„ç»“åˆã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ˜¯æ­ç¤ºæ–‡æœ¬åµŒå…¥è¾“å…¥åˆ°å¸¸è§çš„ã€å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„éšè—èƒ½åŠ›å¹¶ä»¥ç›´æ¥æ–¹å¼ä½¿å…¶å¯ç”¨çš„ç¬¬ä¸€æ­¥ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºä¸åŒçš„ç°æˆæ¨¡å‹ï¼Œè€Œæ— éœ€ä¿®æ”¹å®ƒä»¬ï¼Œä½†å®ƒä¹Ÿå—åˆ°æ¨¡å‹èƒ½åŠ›çš„å›ºæœ‰é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»§æ‰¿äº†æ‰©æ•£æ¨¡å‹æœ‰æ—¶ä¼šåœ¨ä¸åŒä¸»é¢˜ä¹‹é—´æ··æ·†å±æ€§çš„é™åˆ¶ã€‚è¡¥å……æ–¹æ³• [7, 41] å¤§å¤§å‡å°‘äº†è¿™äº›é—®é¢˜ï¼Œæœªæ¥çš„å·¥ä½œå¯ä»¥æ·±å…¥ç ”ç©¶å®ƒä»¬ä¸æˆ‘ä»¬æ–¹æ³•çš„ç»“åˆã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3043bea6ae4c9e730266e786857fddc6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2c4e8841daa8f92d5a5212ab49d3d874.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c1e4a92dd6c321ec154d80bf3c636c.jpg" align="middle"></details><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p><p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. </p><p><a href="http://arxiv.org/abs/2403.17006v1">PDF</a> </p><p><strong>Summary</strong><br>æ·±åº¦ç¥ç»ç½‘ç»œé€šè¿‡æé«˜é‡å»ºè´¨é‡æ˜¾è‘—æ¨è¿›äº†å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼Œä½†ç°é˜¶æ®µéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒå‹ç¼©æ„ŸçŸ¥ç¥ç»ç½‘ç»œï¼Œé™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§å¹¶ä¸”é˜»ç¢äº†å¿«é€Ÿéƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶å¾ˆæ…¢å¹¶ä¸”å¯¹å‹ç¼©æ„ŸçŸ¥çš„é€‚åº”æ€§æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ã€é«˜æ•ˆçš„ã€ç«¯åˆ°ç«¯çš„åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ã€‚IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å°†å…¶ç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ä¾¿ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡å€¼æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒä¾‹ã€‚ä¸ºäº†å¯ç”¨æ­¤ç±»éœ€è¦å¤§é‡å†…å­˜çš„ç«¯åˆ°ç«¯å¾®è°ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œä»¥å°†ï¼ˆ1ï¼‰å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œï¼ˆ2ï¼‰æ¯ä¸ªæ­¥éª¤ä¸­çš„å™ªå£°ä¼°è®¡ U å½¢ç½‘ç»œéƒ½è½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œå¤§å¤šæ•°ä¸­é—´ç‰¹å¾éƒ½ä¼šè¢«æ¸…é™¤ï¼Œä»¥å‡å°‘é«˜è¾¾ 93.8% çš„ GPU å†…å­˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç»„è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œé«˜å‡º 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR å¢ç›Šæ–¹é¢æé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ã€‚</li><li>IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å°†å…¶ç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ä¾¿ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡å€¼æ¢å¤åŸå§‹å›¾åƒã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§å¯é€†è®¾è®¡ï¼Œä»¥å°†å¤šæ­¥é‡‡æ ·è¿‡ç¨‹å’Œæ¯ä¸ªæ­¥éª¤ä¸­çš„å™ªå£°ä¼°è®¡ U å½¢ç½‘ç»œéƒ½è½¬æ¢ä¸ºå¯é€†ç½‘ç»œã€‚</li><li>å¼€å‘äº†ä¸€ç»„è½»é‡çº§æ¨¡å—ï¼Œå°†æµ‹é‡å€¼æ³¨å…¥å™ªå£°ä¼°è®¡å™¨ï¼Œä»¥è¿›ä¸€æ­¥ä¿ƒè¿›é‡å»ºã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œé«˜å‡º 2.64dBã€‚</li><li>ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼ŒIDM åœ¨ PSNR å¢ç›Šæ–¹é¢æé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</li><li>IDM æä¾›äº†æ¯”ç°æœ‰æŠ€æœ¯æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„å›¾åƒå‹ç¼©æ„ŸçŸ¥è§£å†³æ–¹æ¡ˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šå¯é€†æ‰©æ•£æ¨¡å‹åœ¨å‹ç¼©æ„ŸçŸ¥ä¸­çš„åº”ç”¨</li><li>ä½œè€…ï¼šBin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šCompressed Sensingã€Diffusion Modelsã€Image Reconstruction</li><li>é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒå‹ç¼©æ„ŸçŸ¥ï¼ˆCSï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„ CS ç¥ç»ç½‘ç»œéœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œé™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§å’Œå¿«é€Ÿéƒ¨ç½²ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒé‡å»ºï¼Œä½†åœ¨æ¨ç†é€Ÿåº¦å’Œå¯¹ CS çš„é€‚åº”æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†å¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„ CS æ–¹æ³•ã€‚IDM å°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ç›´æ¥ä» CS æµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼ŒIDM åœ¨ PSNR æ–¹é¢æ¯”ç°æœ‰çš„æœ€å…ˆè¿›çš„ CS ç½‘ç»œé«˜å‡º 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR ä¸Šæé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚</li></ol><p>7.Methodsï¼š(1) æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚(2) IDMå°†å¤§è§„æ¨¡æ‰©æ•£é‡‡æ ·è¿‡ç¨‹é‡æ–°ç”¨ä½œé‡å»ºæ¨¡å‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒï¼Œä»¥ç›´æ¥ä»å‹ç¼©æ„ŸçŸ¥æµ‹é‡ä¸­æ¢å¤åŸå§‹å›¾åƒï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä¸€æ­¥å™ªå£°ä¼°è®¡å­¦ä¹ èŒƒå¼ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å›¾åƒå‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ï¼Œè¯¥æ–¹æ³•å°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ‰©æ•£é‡‡æ ·è¿‡ç¨‹è½¬æ¢ä¸ºä¸¤çº§å¯é€†æ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯é‡å»ºå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªå¥½å¤„ã€‚é¦–å…ˆï¼Œå®ƒç›´æ¥ä½¿ç”¨å‹ç¼©æ„ŸçŸ¥é‡å»ºç›®æ ‡å­¦ä¹ æ‰€æœ‰ç½‘ç»œå‚æ•°ï¼Œé‡Šæ”¾äº†æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºé—®é¢˜ä¸­çš„å…¨éƒ¨æ½œåŠ›ã€‚å…¶æ¬¡ï¼Œå®ƒé€šè¿‡ä½¿ï¼ˆ1ï¼‰é‡‡æ ·æ­¥éª¤å’Œï¼ˆ2ï¼‰å™ªå£°ä¼°è®¡ U-Net å¯é€†æ¥æé«˜å†…å­˜æ•ˆç‡ã€‚ç¬¬ä¸‰ï¼Œå®ƒé‡æ–°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥æœ€å°åŒ–è®­ç»ƒæ—¶é—´ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ•ˆç«¯åˆ°ç«¯åŸºäºæ‰©æ•£çš„å‹ç¼©æ„ŸçŸ¥æ–¹æ³•ï¼Œç§°ä¸ºå¯é€†æ‰©æ•£æ¨¡å‹ï¼ˆIDMï¼‰ã€‚æ€§èƒ½ï¼šä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„å‹ç¼©æ„ŸçŸ¥ç½‘ç»œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR æ–¹é¢æé«˜äº† 2.64dBã€‚ä¸æœ€è¿‘åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• DDNM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ IDM åœ¨ PSNR ä¸Šæé«˜äº† 10.09dBï¼Œæ¨ç†é€Ÿåº¦æé«˜äº† 14.54 å€ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ã€‚è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†éœ€è¦å¯¹æ‰©æ•£æ¨¡å‹å’Œå‹ç¼©æ„ŸçŸ¥çš„æ·±å…¥ç†è§£ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-74400c9f9a39a9bfabc15ed66a346128.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd2ddb1363513e955ce3cbe06c53a9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a74781e409db05f570137032af563e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b2b8f07c7e2d6d6402f4200d9d5296f.jpg" align="middle"></details><h2 id="TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models"><a href="#TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models" class="headerlink" title="TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models"></a>TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models</h2><p><strong>Authors:Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei</strong></p><p>Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a>. </p><p><a href="http://arxiv.org/abs/2403.17005v1">PDF</a> CVPR 2024; Project page: <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a></p><p><strong>Summary</strong><br>TRIPæ˜¯ä¸€ç§æ–°çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨å›¾åƒå™ªå£°å…ˆéªŒæ¥ä¿ƒè¿›å¸§é—´å…³è”æ¨ç†å¹¶é€šè¿‡æ—¶é—´æ®‹å·®å­¦ä¹ ç®€åŒ–æ—¶é—´è¿è´¯å»ºæ¨¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TRIP æå‡ºäº†ä¸€ç§é€šè¿‡é™æ­¢å›¾åƒç”Ÿæˆè§†é¢‘çš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£èŒƒä¾‹ã€‚</li><li>è¯¥æ–¹æ³•åˆ©ç”¨åŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ä»£ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹è·å¾—å›¾åƒå™ªå£°å…ˆéªŒã€‚</li><li>TRIP ä½¿ç”¨å‰©ä½™å¼åŒè·¯å¾„æ–¹æ¡ˆè¿›è¡Œå™ªå£°é¢„æµ‹ï¼ŒåŒ…æ‹¬ç›´æ¥é‡‡ç”¨å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§å‚è€ƒå™ªå£°çš„æ·å¾„è·¯å¾„ï¼Œä»¥åŠåœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ä»£ç ä¸Šä½¿ç”¨ 3D-UNet çš„æ®‹å·®è·¯å¾„ã€‚</li><li>æ¯ä¸ªå¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„æœºåˆ¶åŠ¨æ€åˆå¹¶ï¼Œç”¨äºæœ€ç»ˆçš„è§†é¢‘ç”Ÿæˆã€‚</li><li>TRIP åœ¨ WebVid-10Mã€DTDB å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜äº†å…¶åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li><li>TRIP çš„é¡¹ç›®é¡µé¢ä¸º <a href="https://trip-i2v.github.io/TRIP/ã€‚">https://trip-i2v.github.io/TRIP/ã€‚</a></li><li>TRIP æ˜¯ä¸€ä¸ªå›¾åƒåˆ°è§†é¢‘æ‰©æ•£èŒƒä¾‹ï¼Œåˆ©ç”¨å›¾åƒå™ªå£°å…ˆéªŒä¿ƒè¿›å¸§é—´å…³è”æ¨ç†å¹¶é€šè¿‡æ—¶é—´æ®‹å·®å­¦ä¹ ç®€åŒ–æ—¶é—´è¿è´¯å»ºæ¨¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šTRIPï¼šåŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ®‹å·®å­¦ä¹ </li><li>ä½œè€…ï¼šå¼ ä»²ä¼Ÿï¼Œé¾™ç¦è‡£ï¼Œæ½˜æ˜ ä¼Ÿï¼Œé‚±å…†å‡¡ï¼Œå§šå©·ï¼Œæ›¹æ¨ï¼Œæ¢…æ¶›</li><li>å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒåˆ°è§†é¢‘ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒå™ªå£°å…ˆéªŒï¼Œæ—¶é—´æ®‹å·®å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17005</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆI2Vï¼‰æ—¶ï¼Œé¢ä¸´ç€æŒ‘æˆ˜ï¼šæ—¢è¦ä¿è¯ç”Ÿæˆè§†é¢‘å¸§ä¸ç»™å®šå›¾åƒä¿æŒä¸€è‡´ï¼Œåˆè¦ä¿è¯å¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šä»¥å¾€çš„ I2V æ–¹æ³•é€šå¸¸ç›´æ¥å°†ç»™å®šå›¾åƒä½œä¸ºæ¡ä»¶ï¼Œèå…¥åˆ°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éš¾ä»¥å…¼é¡¾å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§ã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† TRIPï¼Œä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹æ—¶é—´æ®‹å·®å­¦ä¹ æ–°èŒƒå¼ã€‚TRIP é€šè¿‡åŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å¾—å›¾åƒå™ªå£°å…ˆéªŒã€‚ç„¶åï¼ŒTRIP é‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼š1ï¼‰æ·å¾„è·¯å¾„ç›´æ¥å°†å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§çš„å‚è€ƒå™ªå£°ï¼Œä»¥å¢å¼ºç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼›2ï¼‰æ®‹å·®è·¯å¾„ä½¿ç”¨ 3D-UNet åœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ç ä¸Šè¿›è¡Œæ¨ç†ï¼Œå®ç°å¸§é—´å…³ç³»æ¨ç†ï¼Œä»è€Œä¿ƒè¿›æ¯å¸§æ®‹å·®å™ªå£°çš„å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæ¯å¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ WebVid-10Mã€DTD å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTRIP åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ‰æ•ˆæ€§ã€‚TRIP ç”Ÿæˆçš„è§†é¢‘å¸§ä¸ç»™å®šå›¾åƒå¯¹é½è‰¯å¥½ï¼Œå¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ä¹Ÿå¾—åˆ°ä¿è¯ã€‚</p></li><li><p><strong>æ–¹æ³•</strong>ï¼š(1) TRIPåŸºäºé™æ€å›¾åƒå’Œå™ªå£°è§†é¢‘æ½œåœ¨ç çš„ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œè·å¾—å›¾åƒå™ªå£°å…ˆéªŒï¼›(2) TRIPé‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼š   (2.1) æ·å¾„è·¯å¾„ç›´æ¥å°†å›¾åƒå™ªå£°å…ˆéªŒä½œä¸ºæ¯å¸§çš„å‚è€ƒå™ªå£°ï¼Œä»¥å¢å¼ºç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼›   (2.2) æ®‹å·®è·¯å¾„ä½¿ç”¨3D-UNetåœ¨å™ªå£°è§†é¢‘å’Œé™æ€å›¾åƒæ½œåœ¨ç ä¸Šè¿›è¡Œæ¨ç†ï¼Œå®ç°å¸§é—´å…³ç³»æ¨ç†ï¼Œä»è€Œä¿ƒè¿›æ¯å¸§æ®‹å·®å™ªå£°çš„å­¦ä¹ ï¼›(3) æ¯å¸§çš„å‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚</p></li><li><p>ç»“è®ºï¼š(1): TRIP æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„æ—¶é—´æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä¸­çš„å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§é—®é¢˜ï¼Œåœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå™ªå£°å…ˆéªŒçš„æ—¶é—´æ®‹å·®å­¦ä¹ èŒƒå¼ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†å›¾åƒå¯¹é½å’Œæ—¶é—´è¿è´¯æ€§ã€‚</li><li>é‡‡ç”¨æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆé¢„æµ‹å™ªå£°ï¼Œå¢å¼ºäº†ç¬¬ä¸€å¸§ä¸åç»­å¸§çš„å¯¹é½ï¼Œå¹¶å®ç°äº†å¸§é—´å…³ç³»æ¨ç†ã€‚</li><li>é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€èåˆå‚è€ƒå™ªå£°å’Œæ®‹å·®å™ªå£°ï¼Œç”¨äºæœ€ç»ˆè§†é¢‘ç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>åœ¨ WebVid-10Mã€DTD å’Œ MSR-VTT æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTRIP åœ¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>TRIP ç”Ÿæˆçš„è§†é¢‘å¸§ä¸ç»™å®šå›¾åƒå¯¹é½è‰¯å¥½ï¼Œå¸§ä¸å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§ä¹Ÿå¾—åˆ°ä¿è¯ã€‚å·¥ä½œé‡ï¼š</li><li>TRIP çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œæ¶‰åŠåˆ°ä¸€æ­¥åå‘æ‰©æ•£è¿‡ç¨‹ã€æ®‹å·®å¼åŒè·¯å¾„æ–¹æ¡ˆå’Œæ³¨æ„åŠ›æœºåˆ¶çš„èåˆã€‚</li><li>TRIP çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ca66a6c8cbe1ea0c7bee31ec88e3bfdd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153f2b85dba70a39304fbf6d81434bc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36f2aad744b3d6c59a51d26bf1bc8573.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e31383189b1e2dd43b8737e9a8b1df0a.jpg" align="middle"></details><h2 id="VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation"><a href="#VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation" class="headerlink" title="VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation"></a>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</h2><p><strong>Authors:Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</strong></p><p>Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.17001v1">PDF</a> CVPR 2024; Project page: <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ° 3D ç”Ÿæˆæ¨¡å‹ VP3D é€šè¿‡è§†è§‰æç¤ºå¼•å¯¼å’Œå¯å¾®å¥–åŠ±å‡½æ•°å¢å¼ºäº† SDS ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„è§†è§‰ä¿çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>VP3D åœ¨ SDS ä¼˜åŒ–ä¸­å¼•å…¥äº†è§†è§‰æç¤ºï¼Œä»¥æ˜¾å¼åˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†ã€‚</li><li>è§†è§‰æç¤ºä»è¾“å…¥æ–‡æœ¬ä¸­ç”Ÿæˆï¼Œä½œä¸ºé™„åŠ ç›‘ç£ï¼ŒåŠ å¼ºäº†å¯¹ 3D æ¨¡å‹è§†è§‰å¤–è§‚çš„å­¦ä¹ ã€‚</li><li>å¯å¾®å¥–åŠ±å‡½æ•°é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šå¯¹é½ï¼Œå¹¶åœ¨è¯­ä¹‰ä¸Šä¸æ–‡æœ¬æç¤ºåŒ¹é…ã€‚</li><li>VP3D æ˜¾è‘—æé«˜äº† 3D æ¨¡å‹çš„è§†è§‰ä¿çœŸåº¦ï¼Œç”Ÿæˆæ›´ç²¾ç»†çš„çº¹ç†ã€‚</li><li>VP3D å¯ä»¥é€šè¿‡æ›¿æ¢è‡ªç”Ÿæˆè§†è§‰æç¤ºæ¥è§¦å‘æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„é£æ ¼åŒ–ä»»åŠ¡ã€‚</li><li>VP3D æ‰©å±•äº† SDS åœ¨å¤æ‚æ–‡æœ¬æç¤ºä¸‹çš„åº”ç”¨ï¼Œè§£å†³äº†æ—©æœŸæ¨¡å‹ä¸­å¸¸è§çš„å¤±çœŸå’Œçº¹ç†ä¸ç°å®é—®é¢˜ã€‚</li><li>VP3D å¯ä»¥åœ¨ 2D visual prompt å’Œæ–‡æœ¬æç¤ºä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œå®ç°è§†è§‰å’Œè¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šVP3Dï¼šé‡Šæ”¾ç”¨äºæ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„ 2D è§†è§‰æç¤º</li><li>ä½œè€…ï¼šYang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</li><li>éš¶å±ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€ç”Ÿæˆæ¨¡å‹ã€è§†è§‰æç¤ºã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.17001Github é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ° 3D ç”Ÿæˆæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸º 3D å‡ ä½•å’Œå¤–è§‚çš„å¤æ‚æ€§ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šScore Distillation Sampling (SDS) æ˜¯ä¸€ç§é›¶æ ·æœ¬å­¦ä¹ éšå¼ 3D æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†å®ƒåœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”ç”Ÿæˆçš„ 3D æ¨¡å‹å¯èƒ½å­˜åœ¨å¤±çœŸã€ä¸çœŸå®çº¹ç†æˆ–è·¨è§†å›¾ä¸ä¸€è‡´çš„é—®é¢˜ã€‚(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šVP3D æ˜¯ä¸€ç§è§†è§‰æç¤ºå¼•å¯¼çš„æ–‡æœ¬åˆ° 3D æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨ 2D è§†è§‰æç¤ºä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†æ¥å¢å¼ºæ–‡æœ¬åˆ° 3D ç”Ÿæˆã€‚VP3D é¦–å…ˆä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œç„¶åå°†è¯¥å›¾åƒç”¨ä½œè§†è§‰æç¤ºæ¥å¢å¼º SDS ä¼˜åŒ–ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼ŒVP3D ä¸­çš„ 2D è§†è§‰æç¤ºæ˜¾è‘—ç®€åŒ–äº† 3D æ¨¡å‹è§†è§‰å¤–è§‚çš„å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿäº†æ›´é«˜è§†è§‰ä¿çœŸåº¦å’Œæ›´è¯¦ç»†çš„çº¹ç†ã€‚æ­¤å¤–ï¼Œå½“ç”¨ç»™å®šçš„å‚è€ƒå›¾åƒæ›¿æ¢è‡ªç”Ÿæˆçš„è§†è§‰æç¤ºæ—¶ï¼ŒVP3D èƒ½å¤Ÿè§¦å‘é£æ ¼åŒ–æ–‡æœ¬åˆ° 3D ç”Ÿæˆçš„ä»»åŠ¡ã€‚</p></li><li><p>Methods:(1) åˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½œä¸ºè§†è§‰æç¤ºï¼›(2) ä½¿ç”¨è§†è§‰æç¤ºå¢å¼ºSDSä¼˜åŒ–ï¼Œé¼“åŠ±æ¸²æŸ“çš„3Dæ¨¡å‹å›¾åƒä¸2Dè§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ï¼›(3) å¼•å…¥å¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œé¼“åŠ±æ¸²æŸ“çš„3Dæ¨¡å‹å›¾åƒä¸2Dè§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† VP3Dï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨2D è§†è§‰æç¤ºçš„æ–°å‹æ–‡æœ¬åˆ° 3D ç”ŸæˆèŒƒå¼ã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ 2D æ‰©æ•£æ¨¡å‹ä»è¾“å…¥æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚ç„¶åï¼Œè¯¥å›¾åƒä½œä¸ºè§†è§‰æç¤ºï¼Œé€šè¿‡æˆ‘ä»¬è®¾è®¡çš„è§†è§‰æç¤ºå¼•å¯¼åˆ†æ•°è’¸é¦é‡‡æ ·æ¥å¢å¼º 3D æ¨¡å‹å­¦ä¹ ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å¤–çš„äººå·¥åé¦ˆå’Œè§†è§‰ä¸€è‡´æ€§å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ± 3D æ¨¡å‹ä¸è¾“å…¥è§†è§‰å’Œæ–‡æœ¬æç¤ºä¹‹é—´çš„è¯­ä¹‰å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚åœ¨ T3Bench åŸºå‡†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ VP3D ä¼˜äºç°æœ‰çš„ SOTA æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ° 3D ç”ŸæˆèŒƒå¼ï¼Œåˆ©ç”¨ 2D è§†è§‰æç¤ºæ¥å¢å¼º 3D æ¨¡å‹å­¦ä¹ ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è§†è§‰æç¤ºå¼•å¯¼åˆ†æ•°è’¸é¦é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰æç¤ºä¸­çš„è§†è§‰å¤–è§‚çŸ¥è¯†æ¥æŒ‡å¯¼ 3D æ¨¡å‹ç”Ÿæˆã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªå¯å¾®åˆ†å¥–åŠ±å‡½æ•°ï¼Œä»¥é¼“åŠ±æ¸²æŸ“çš„ 3D æ¨¡å‹å›¾åƒä¸ 2D è§†è§‰æç¤ºåœ¨è§†è§‰ä¸Šæ›´ä¸€è‡´ï¼Œå¹¶ä¸æ–‡æœ¬æç¤ºåœ¨è¯­ä¹‰ä¸ŠåŒ¹é…ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ T3Bench åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVP3D èƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ›´é«˜è§†è§‰ä¿çœŸåº¦å’Œæ›´è¯¦ç»†çº¹ç†çš„ 3D æ¨¡å‹ã€‚</li><li>VP3D èƒ½å¤Ÿè§¦å‘é£æ ¼åŒ–æ–‡æœ¬åˆ° 3D ç”Ÿæˆä»»åŠ¡ï¼Œå½“ç”¨ç»™å®šçš„å‚è€ƒå›¾åƒæ›¿æ¢è‡ªç”Ÿæˆçš„è§†è§‰æç¤ºæ—¶ã€‚å·¥ä½œé‡ï¼š</li><li>VP3D çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆ2D æ‰©æ•£æ¨¡å‹ã€3D æ¨¡å‹å’Œå¥–åŠ±å‡½æ•°ï¼‰ã€‚</li><li>VP3D çš„æ¨ç†æ—¶é—´æ¯”åŸºçº¿æ–¹æ³•ç¨é•¿ï¼Œå› ä¸ºéœ€è¦ç”Ÿæˆè§†è§‰æç¤ºå¹¶è¿›è¡Œé¢å¤–çš„ä¼˜åŒ–æ­¥éª¤ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-66d95e52c6a32ad077611ad4162f2e1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c21b901dbeddaa875cbc4a9d022b539c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e2b11ff84eeb9793d2212cf130acf75f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-28  AID Attention Interpolation of Text-to-Image Diffusion</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/</id>
    <published>2024-03-23T11:02:12.000Z</published>
    <updated>2024-03-23T11:02:12.769Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><a href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis" class="headerlink" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"></a>CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</h2><p><strong>Authors:Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto</strong></p><p>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework. </p><p><a href="http://arxiv.org/abs/2403.14412v1">PDF</a> This paper has been accepted for publication at the 2024   International Conference on 3D Vision (3DV)</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å¤§é‡è§†å›¾å¯ç”¨æ—¶ï¼Œåœ¨æ–°çš„è§†å›¾åˆæˆæ–¹é¢å·²æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚åœ¨å¤„ç†å°‘é•œå¤´è®¾ç½®ï¼ˆå³ä¸€ç»„è¾ƒå°‘çš„è¾“å…¥è§†å›¾ï¼‰æ—¶ï¼Œè®­ç»ƒå¯èƒ½ä¼šè¿‡åº¦æ‹Ÿåˆè¿™äº›è§†å›¾ï¼Œä»è€Œå¯¼è‡´æœ€ç»ˆæ¸²æŸ“ä¸­å‡ºç°ä¼ªå½±ä»¥åŠå‡ ä½•å’Œè‰²å½©ä¸ä¸€è‡´ã€‚æ­£åˆ™åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äº NeRF æ³›åŒ–ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘çš„æ¯ç§ NeRF æ­£åˆ™åŒ–æŠ€æœ¯éƒ½æ—¨åœ¨å‡è½»ç‰¹å®šçš„æ¸²æŸ“é—®é¢˜ã€‚ä»è¿™ä¸€è§‚å¯Ÿå‡ºå‘ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº† CombiNeRFï¼Œä¸€ä¸ªååŒç»“åˆäº†å‡ ç§æ­£åˆ™åŒ–æŠ€æœ¯çš„æ¡†æ¶ï¼Œå…¶ä¸­ä¸€äº›æ˜¯æ–°é¢–çš„ï¼Œä»¥ä¾¿ç»Ÿä¸€æ¯ç§æŠ€æœ¯çš„ä¼˜ç‚¹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¯¹å•ä¸ªå’Œç›¸é‚»å…‰çº¿çš„åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹æ¥å¯¹æ¥è¿‘çš„å‡ ä½•å›¾å½¢è¿›è¡Œæ­£åˆ™åŒ–ã€‚åœ¨è¿™äº›å‡ ä½•æ–¹æ³•ä¹‹åï¼Œæˆ‘ä»¬å»ºè®®å°† Lipschitz æ­£åˆ™åŒ–åº”ç”¨äº NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œï¼Œå¹¶ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒCombiNeRF åœ¨å‡ ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†çš„å°‘é•œå¤´è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¯¹ LLFF å’Œ NeRF åˆæˆæ•°æ®é›†è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥æ”¯æŒæ‰€åšå‡ºçš„é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­å‘å¸ƒäº†æˆ‘ä»¬æ¡†æ¶çš„å¼€æºå®ç°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CombiNeRF ç»“åˆäº†å¤šç§æ­£åˆ™åŒ–æŠ€æœ¯æ¥æé«˜ NeRF åœ¨å°‘é•œå¤´è®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>CombiNeRF å¯¹å•ä¸ªå’Œç›¸é‚»å…‰çº¿åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥å‡å°‘ä¼ªå½±ã€‚</li><li>CombiNeRF æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹ï¼Œä»¥å¯¹æ¥è¿‘çš„å‡ ä½•å›¾å½¢è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li><li>CombiNeRF åº”ç”¨ Lipschitz æ­£åˆ™åŒ–åˆ° NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œä¸­ã€‚</li><li>CombiNeRF ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚</li><li>CombiNeRF åœ¨å‡ ä¸ªå…¬å…±æ•°æ®é›†çš„å°‘é•œå¤´è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>CombiNeRF çš„å¼€æºå®ç°å·²å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šCombiNeRFï¼šä¸€ç§ç»“åˆæ­£åˆ™åŒ–æŠ€æœ¯çš„å°‘æ ·æœ¬ç¥ç»å›¾åƒåˆæˆæ–¹æ³•</li><li>ä½œè€…ï¼š</li><li>Davide Marchignoli</li><li>Federico Tosi</li><li>Marco Tagliasacchi</li><li>Emanuele RodolÃ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç»´ç½—çº³å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°‘æ ·æœ¬å›¾åƒåˆæˆã€æ­£åˆ™åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼š</li><li>https://arxiv.org/abs/2203.07173</li><li>Githubï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æœ‰å¤§é‡è§†å›¾å¯ç”¨æ—¶ï¼Œåœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ä½†åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­ï¼Œå³åªæœ‰å°‘é‡è¾“å…¥è§†å›¾æ—¶ï¼Œè®­ç»ƒå¯èƒ½ä¼šè¿‡åº¦æ‹Ÿåˆè¿™äº›è§†å›¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ¸²æŸ“ä¸­å‡ºç°ä¼ªå½±ä»¥åŠå‡ ä½•å’Œè‰²åº¦ä¸ä¸€è‡´ã€‚æ­£åˆ™åŒ–æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å¸®åŠ© NeRF æ³›åŒ–ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç›®å‰æœ€å…ˆè¿›çš„ NeRF æ­£åˆ™åŒ–æŠ€æœ¯æ—¨åœ¨å‡è½»ç‰¹å®šçš„æ¸²æŸ“é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º CombiNeRFï¼Œè¿™æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒååŒç»“åˆäº†å‡ ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆå…¶ä¸­ä¸€äº›æ˜¯æ–°é¢–çš„ï¼‰ï¼Œä»¥ç»Ÿä¸€æ¯ç§æŠ€æœ¯çš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ­£åˆ™åŒ–äº†å•ä¸ªå’Œç›¸é‚»å…‰çº¿çš„åˆ†å¸ƒï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªå¹³æ»‘é¡¹æ¥æ­£åˆ™åŒ–é‚»è¿‘å‡ ä½•ã€‚åœ¨è¿™äº›å‡ ä½•æ–¹æ³•ä¹‹åï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ Lipschitz æ­£åˆ™åŒ–å¯¹ NeRF å¯†åº¦å’Œé¢œè‰²ç½‘ç»œè¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶ä½¿ç”¨ç¼–ç æ©ç å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å‡ ä¸ªå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸­çš„å°‘æ ·æœ¬è®¾ç½®ä¸­ï¼ŒCombiNeRF ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å¯¹ LLFF å’Œ NeRF-Synthetic æ•°æ®é›†è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥æ”¯æŒæ‰€åšçš„é€‰æ‹©ã€‚æˆ‘ä»¬éšæœ¬æ–‡å‘å¸ƒäº†æˆ‘ä»¬æ¡†æ¶çš„å¼€æºå®ç°ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1): CombiNeRFå°†å…ˆå‰æè¿°çš„å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼Œå› æ­¤å¾—åCombiNeRFã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€ç»ˆæŸå¤±å†™ä¸ºï¼š</p><blockquote><p>LCombiNeRF = LRGB + Î»dist Â· Ldist + Î»fg Â· Lfg + Î»ds Â· Lds + Î»KL Â· LKLï¼Œ(14)</p></blockquote><p>å…¶ä¸­Î»æ˜¯æ§åˆ¶æ¯ä¸ªæŸå¤±è´¡çŒ®çš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼ŒCombiNeRFè¿˜åŒ…æ‹¬Lipschitzæ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ã€‚æå‡ºçš„CombiNeRFæä¾›äº†ä¸Šè¿°æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„ç»Ÿä¸€å®ç°ï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºå½“å‰çš„SOTAæ–¹æ³•ï¼Œå¦‚ä¸‹é¢çš„å®éªŒéƒ¨åˆ†æ‰€ç¤ºã€‚</p><p>(2): CombiNeRFæ–¹æ³•çš„æ­¥éª¤ï¼š</p><blockquote><p>(1) å°†å…ˆå‰æè¿°çš„å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼›(2) å¼•å…¥Lipschitzæ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ï¼›(3) æä¾›æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„ç»Ÿä¸€å®ç°ã€‚</p></blockquote><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§ç»“åˆæ­£åˆ™åŒ–æŠ€æœ¯çš„å°‘æ ·æœ¬ç¥ç»å›¾åƒåˆæˆæ–¹æ³• CombiNeRFï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºå½“å‰çš„ SOTA æ–¹æ³•ï¼Œåœ¨é‡å»ºè´¨é‡æ–¹é¢è¡¨ç°å‡ºæœ€å…ˆè¿›ä¸”ä¸€è‡´çš„ç»“æœã€‚(2): åˆ›æ–°ç‚¹ï¼šCombiNeRF å°†å…ˆå‰å…³äºæŸå¤±å’Œç½‘ç»œç»“æ„çš„æ­£åˆ™åŒ–æŠ€æœ¯ç›¸ç»“åˆï¼Œå¹¶å¼•å…¥äº† Lipschitz æ­£åˆ™åŒ–å’Œç¼–ç æ©ç æŠ€æœ¯ï¼Œæä¾›äº†ä¸€ç§ç»Ÿä¸€å®ç°æ‰€æœ‰æ­£åˆ™åŒ–æŠ€æœ¯çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šCombiNeRF åœ¨ LLFF å’Œ NeRF-Synthetic æ•°æ®é›†ä¸­çš„å°‘æ ·æœ¬è®¾ç½®ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šCombiNeRF çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”éšè®ºæ–‡å‘å¸ƒäº†å¼€æºå®ç°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c642a8b25e39f3498ab3908076b62e64.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-522132516f392845d36d52fc73b5c1b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89211d83c6885a2c21f84e269107a3b.jpg" align="middle"></details><h2 id="Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><a href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions" class="headerlink" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions"></a>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions</h2><p><strong>Authors:Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</strong></p><p>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction. </p><p><a href="http://arxiv.org/abs/2403.14053v1">PDF</a> 25 pages, 13 figures</p><p><strong>Summary</strong><br>å¤šæ¨¡æ€NeRFï¼šåˆ©ç”¨å¯è§å…‰å’Œçƒ­æˆåƒï¼Œåœ¨æç«¯é»‘æš—ç¯å¢ƒä¸­å®ç°é€¼çœŸæ–°è§†è§’åˆæˆ</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFé¢å¯¹æç«¯é»‘æš—åœºæ™¯ä¸­è½»å¾®å…‰ç…§ä¿¡å·çš„æŸå¤±ï¼Œé€ æˆçº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ç¼ºå¤±ã€‚</li><li>Thermal-NeRFåˆ©ç”¨çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒï¼Œåœ¨å…‰ç…§å˜åŒ–ä¸‹ä¹Ÿèƒ½å¾—åˆ°é²æ£’çš„åˆæˆç»“æœã€‚</li><li>Thermal-NeRFåœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>å¯è§å…‰å’Œçƒ­æˆåƒæ¨¡æ€åœ¨ä¸‰ç»´é‡å»ºä¸­ç›¸äº’è¡¥å……ã€‚</li><li>å¤šæ¨¡æ€NeRFæ•°æ®é›†ï¼ˆMVTVï¼‰æ”¯æŒå¤šæ¨¡æ€NeRFç ”ç©¶ã€‚</li><li>Thermal-NeRFé€‚ç”¨äºå¯¹æ˜¾å¼è¡¨ç¤ºä¾èµ–çš„é«˜é€Ÿæ¨¡å‹ã€‚</li><li>Thermal-NeRFåŒæ—¶å®ç°å¯è§å…‰å’Œçƒ­æˆåƒçš„æ–°è§†è§’åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåˆ©ç”¨çƒ­æˆåƒæ¨¡å¼å¢å¼ºè¡¥å……ææ–™</li><li>ä½œè€…ï¼šJiacong Xu, Shuaicheng Liu, Jiaolong Yang, Xueting Li, Qiong Yan, Shengming Zhang</li><li>å•ä½ï¼šåä¸­ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€ä½å…‰å¢å¼ºã€çƒ­æˆåƒã€æ–°è§†å›¾åˆæˆã€å¤šæ¨¡æ€</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.07231   Github ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) é€šè¿‡ä»å¤šè§†å›¾å›¾åƒå­¦ä¹ åœºæ™¯çš„éšå¼ä½“ç§¯è¡¨ç¤ºæ¥å®ç°é€¼çœŸçš„æ–°è§†å›¾åˆæˆï¼Œå¯ä»¥å¿ å®åœ°ä¼ é€’è‰²å½©ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¼ æ„Ÿå™¨å™ªå£°ä¼šæ±¡æŸ“ä½å€¼åƒç´ ä¿¡å·ï¼Œè€Œæœ‰æŸç›¸æœºå›¾åƒä¿¡å·å¤„ç†å™¨ä¼šè¿›ä¸€æ­¥å»é™¤ææš—æƒ…å†µä¸‹çš„æ¥è¿‘é›¶çš„å¼ºåº¦ï¼Œä»è€Œé™ä½åˆæˆæ€§èƒ½ã€‚ç°æœ‰çš„æ–¹æ³•ä»åŸå§‹å›¾åƒé‡å»ºä½å…‰åœºæ™¯ï¼Œä½†éš¾ä»¥æ¢å¤æš—åŒºåŸŸçš„çº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸é€‚ç”¨äºä¾èµ–æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¨¡å‹ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä»åŸå§‹å›¾åƒé‡å»ºä½å…‰åœºæ™¯ï¼Œä½†éš¾ä»¥æ¢å¤æš—åŒºåŸŸçš„çº¹ç†å’Œè¾¹ç•Œç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸é€‚ç”¨äºä¾èµ–æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå……åˆ†ï¼Œå› ä¸ºå®ƒåˆ©ç”¨äº†çƒ­æˆåƒä»ªå¯¹å…‰ç…§å˜åŒ–çš„é²æ£’æ€§å’ŒåŸå§‹å›¾åƒä¿ç•™äº†é»‘æš—ä¸­ä»»ä½•å¯èƒ½çš„çº¿ç´¢ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† Thermal-NeRFï¼Œå®ƒå°†çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥ï¼ŒåŒæ—¶è€ƒè™‘åˆ°çƒ­æˆåƒä»ªå¯¹å…‰ç…§å˜åŒ–çš„é²æ£’æ€§ï¼Œå¹¶ä¸”åŸå§‹å›¾åƒä¿ç•™äº†é»‘æš—ä¸­çš„ä»»ä½•å¯èƒ½çº¿ç´¢ï¼Œä»¥åŒæ—¶å®Œæˆå¯è§å…‰å’Œçƒ­è§†å›¾åˆæˆã€‚æ­¤å¤–ï¼Œè¿˜å»ºç«‹äº†ç¬¬ä¸€ä¸ªå¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›† (MVTV) æ¥æ”¯æŒå¯¹å¤šæ¨¡æ€ NeRF çš„ç ”ç©¶ã€‚Thermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ä¸¤ç§æ¨¡æ€åœ¨ 3D é‡å»ºä¸­éƒ½æ˜¯æœ‰ç›Šçš„ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Ÿè¯¥æ–¹æ³•çš„æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼ŸThermal-NeRF åœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ MVTV æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒThermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿä»ä½å…‰æ¡ä»¶ä¸‹çš„å¤šæ¨¡æ€å›¾åƒç”Ÿæˆé€¼çœŸæ–°è§†å›¾çš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰å»ºç«‹å¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›† MVTVï¼›ï¼ˆ2ï¼‰æå‡º Thermal-NeRF æ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨çƒ­æˆåƒå’Œå¯è§å…‰åŸå§‹å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå®ç°å¯è§å…‰å’Œçƒ­è§†å›¾åˆæˆï¼›ï¼ˆ3ï¼‰å¼•å…¥çƒ­å¢å¼ºç­–ç•¥ï¼Œçº¦æŸåœºæ™¯å‡ ä½•å¹¶æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼›ï¼ˆ4ï¼‰é‡‡ç”¨ Retinex3D ç­–ç•¥ï¼Œä¿®æ”¹å…‰ç…§ä»¥å¢å¼ºæš—åŒºç»†èŠ‚ï¼›ï¼ˆ5ï¼‰åˆ©ç”¨ iNGP å®ç°ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œå°†å¯è§å…‰å’Œçƒ­å›¾åƒç»“åˆèµ·æ¥ï¼Œç”¨äºåœ¨ææš—æ¡ä»¶ä¸‹ä»…æœ‰çŸ­æ›å…‰å›¾åƒæ—¶çš„æ–°è§†å›¾åˆæˆï¼Œå…·æœ‰é‡è¦æ„ä¹‰ã€‚é¦–å…ˆï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤šè§†å›¾çƒ­æˆåƒå’Œå¯è§å…‰æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯¹å¤šæ¨¡æ€ NeRF çš„ç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº† Thermal-NeRFï¼Œå®ƒåŒæ—¶å®ç°äº†çƒ­å’Œå¯è§å…‰è§†å›¾åˆæˆï¼Œå¹¶å±•ç¤ºäº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æ— ç¼åœ°è½¬ç§»åˆ°å…·æœ‰æ˜¾å¼è¡¨ç¤ºçš„é«˜é€Ÿæ¸²æŸ“æ¨¡å‹ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨è¿™ä¸¤ç§æ–¹å¼ä¸‹ï¼Œ3D ä½å…‰åœºæ™¯é‡å»ºéƒ½æ˜¯æœ‰ç›Šçš„ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€ NeRF æ¨¡å‹ Thermal-NeRFï¼Œå®ƒå¯ä»¥åŒæ—¶å¤„ç†çƒ­æˆåƒå’Œå¯è§å…‰å›¾åƒï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„æ–°è§†å›¾ã€‚æ€§èƒ½ï¼šåœ¨ MVTV æ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒThermal-NeRF åœ¨ç»†èŠ‚ä¿ç•™å’Œå™ªå£°å¹³æ»‘ä¹‹é—´å®ç°äº†æœ€ä½³æƒè¡¡ï¼Œå¹¶æä¾›äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„åˆæˆæ€§èƒ½ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦æ”¶é›†å’Œé¢„å¤„ç†å¤šæ¨¡æ€å›¾åƒæ•°æ®ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„å·¥ä½œé‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e4b6fdc3cf1e43155bdf48c55f72f035.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5648c4757fd259a0f342cd6459fbb67.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b55bf6addb4ec4dd731cae2b08b0856.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2455b3875fb79afc0c6ecf796abd4b3b.jpg" align="middle"></details><h2 id="Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><a href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures" class="headerlink" title="Learning Novel View Synthesis from Heterogeneous Low-light Captures"></a>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h2><p><strong>Authors:Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu</strong></p><p>Neural radiance field has achieved fundamental success in novel view synthesis from input views with the same brightness level captured under fixed normal lighting. Unfortunately, synthesizing novel views remains to be a challenge for input views with heterogeneous brightness level captured under low-light condition. The condition is pretty common in the real world. It causes low-contrast images where details are concealed in the darkness and camera sensor noise significantly degrades the image quality. To tackle this problem, we propose to learn to decompose illumination, reflectance, and noise from input views according to that reflectance remains invariant across heterogeneous views. To cope with heterogeneous brightness and noise levels across multi-views, we learn an illumination embedding and optimize a noise map individually for each view. To allow intuitive editing of the illumination, we design an illumination adjustment module to enable either brightening or darkening of the illumination component. Comprehensive experiments demonstrate that this approach enables effective intrinsic decomposition for low-light multi-view noisy images and achieves superior visual quality and numerical performance for synthesizing novel views compared to state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.13337v1">PDF</a> </p><p><strong>Summary</strong></p><p>ç¥ç»è¾å°„åœºåœ¨ç›¸åŒäº®åº¦æ°´å¹³å’Œå›ºå®šæ³•çº¿å…‰ç…§ä¸‹ä»è¾“å…¥è§†å›¾åˆæˆæ–°è§†å›¾æ–¹é¢å–å¾—äº†æ ¹æœ¬æ€§çš„æˆåŠŸã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¯¹äºåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ•è·çš„ä¸åŒäº®åº¦æ°´å¹³çš„è¾“å…¥è§†å›¾ï¼Œåˆæˆæ–°è§†å›¾ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§æƒ…å†µåœ¨ç°å®ä¸–ç•Œä¸­å¾ˆå¸¸è§ï¼Œä¼šå¯¼è‡´ä½å¯¹æ¯”åº¦å›¾åƒï¼Œå…¶ä¸­è¯¦ç»†ä¿¡æ¯éšè—åœ¨é»‘æš—ä¸­ï¼Œå¹¶ä¸”ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ä¼šæ˜¾ç€é™ä½å›¾åƒè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ ¹æ®åå°„ç‡åœ¨ä¸åŒè§†å›¾ä¹‹é—´ä¿æŒä¸å˜æ¥å­¦ä¹ ä»è¾“å…¥è§†å›¾åˆ†è§£å…‰ç…§ã€åå°„ç‡å’Œå™ªå£°ã€‚ä¸ºäº†åº”å¯¹å¤šè§†å›¾ä¸­çš„ä¸åŒäº®åº¦å’Œå™ªå£°æ°´å¹³ï¼Œæˆ‘ä»¬å­¦ä¹ ç…§æ˜åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾å•ç‹¬ä¼˜åŒ–å™ªå£°å›¾ã€‚ä¸ºäº†å…è®¸ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œä»¥ä½¿å…‰ç…§ç»„ä»¶å˜äº®æˆ–å˜æš—ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œå†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸”åœ¨åˆæˆæ–°è§†å›¾æ—¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥ä»ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒä¸­åˆ†è§£å…‰ç…§ã€åå°„ç‡å’Œå™ªå£°ã€‚</li><li>å­¦ä¹ ç…§æ˜åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾å•ç‹¬ä¼˜åŒ–å™ªå£°å›¾ï¼Œä»¥è§£å†³ä¸åŒè§†å›¾ä¸­çš„ä¸åŒäº®åº¦å’Œå™ªå£°æ°´å¹³ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œå¯ä»¥ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œä»¥ä½¿å…‰ç…§ç»„ä»¶å˜äº®æˆ–å˜æš—ã€‚</li><li>ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ†è§£ä½å…‰å¤šè§†å›¾å™ªå£°å›¾åƒï¼Œå¹¶ä¸”åœ¨åˆæˆæ–°è§†å›¾æ—¶å…·æœ‰ä¼˜è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šä»å¼‚è´¨ä½å…‰ç…§é‡‡é›†ä¸­å­¦ä¹ æ–°é¢–è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šQuan Zhengã€Hao Sunã€Huiyao Xuã€Fanjiang Xu</li><li>éš¶å±å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢è½¯ä»¶ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ–°é¢–è§†è§’åˆæˆã€ä½å…‰ç…§æ¡ä»¶ã€å¼‚è´¨äº®åº¦ã€å™ªå£°</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13337</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºåœ¨ä»äº®åº¦æ°´å¹³ç›¸åŒã€åœ¨å›ºå®šæ­£å¸¸ç…§æ˜ä¸‹æ‹æ‘„çš„è¾“å…¥è§†å›¾ä¸­åˆæˆæ–°é¢–è§†è§’æ–¹é¢å–å¾—äº†æ ¹æœ¬æ€§æˆåŠŸã€‚ç„¶è€Œï¼Œå¯¹äºåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„ã€å…·æœ‰å¼‚è´¨äº®åº¦æ°´å¹³çš„è¾“å…¥è§†å›¾ï¼Œåˆæˆæ–°é¢–è§†è§’ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§æ¡ä»¶åœ¨ç°å®ä¸–ç•Œä¸­éå¸¸å¸¸è§ã€‚å®ƒä¼šå¯¼è‡´ä½å¯¹æ¯”åº¦å›¾åƒï¼Œå…¶ä¸­ç»†èŠ‚éšè—åœ¨é»‘æš—ä¸­ï¼Œå¹¶ä¸”ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ä¼šæ˜¾ç€é™ä½å›¾åƒè´¨é‡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šAleth-NeRF æå‡ºå­¦ä¹ ä½å…‰ç…§å›¾åƒçš„åç…§ç‡å’Œé®æŒ¡åœºï¼Œä½†è¿™ç§æ–¹æ³•è¦æ±‚æ‰€æœ‰è¾“å…¥å›¾åƒå…·æœ‰ç›¸åŒçš„äº®åº¦æ°´å¹³ã€‚NeR-Factor å°†åœºæ™¯åˆ†è§£ä¸ºå…‰ç…§ã€æ³•çº¿ã€åç…§ç‡å’Œæè´¨ï¼Œå¹¶å‡è®¾å¤šè§†å›¾å›¾åƒå…±äº«ç›¸åŒçš„äº®åº¦ã€‚å¯¹äºå…·æœ‰ä¸åŒäº®åº¦çš„å›¾åƒï¼ŒNeRF-W æå‡ºä½¿ç”¨è§†å›¾çº§å¤–è§‚åµŒå…¥å¯¹ä¸åŒçš„å›¾åƒå¤–è§‚è¿›è¡Œç¼–ç ã€‚ExtremeNeRF æå‡ºå°†æ­£å¸¸å…‰ç…§å›¾åƒåˆ†è§£ä¸ºåç…§ç‡å’Œé˜´å½±ã€‚æ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è€ƒè™‘å™ªå£°é—®é¢˜ï¼Œè€Œå™ªå£°é—®é¢˜å¯¹äºç°å®ä¸–ç•Œçš„ä½å…‰ç…§å›¾åƒæ¥è¯´æ˜¯ä¸å¯å¿½ç•¥çš„ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå—åœºæ™¯å›ºæœ‰åç…§ç‡åœ¨å¤šè§†å›¾ä¸­ä¿æŒå…‰ç…§ä¸å˜çš„æ€§è´¨å¯å‘ï¼Œæˆ‘ä»¬æå‡ºæ ¹æ®å¹¿ä¹‰ Retinex ç†è®ºå°†è¾“å…¥è§†å›¾åˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ã€‚åˆ†è§£å…è®¸ç¼–è¾‘å…‰ç…§åˆ†é‡å¹¶æ¶ˆé™¤å™ªå£°çš„å½±å“ã€‚ç„¶è€Œï¼Œç”±äºç”¨ä¸‰ä¸ªåˆ†è§£åˆ†é‡è§£é‡Šå›¾åƒçš„æ¨¡ç³Šæ€§ï¼Œåˆ†è§£æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæš—åƒç´ å¯èƒ½æ˜¯ç”±ä½åç…§ç‡ã€ä½å…‰ç…§ç”šè‡³å™ªå£°å€¼å¼•èµ·çš„ã€‚ä¸ºäº†å‡è½»æ¨¡ç³Šæ€§å¹¶å½¢æˆåˆç†çš„åˆ†è§£ï¼Œæˆ‘ä»¬å°†å‡ ä¸ªå…ˆéªŒæ¡ä»¶çº³å…¥åˆ†è§£ä¸­ï¼Œå³åç…§ç‡åœ¨å¤šè§†å›¾ä¸­æ˜¯ä¸€è‡´çš„ï¼Œåç…§ç‡å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œå…‰ç…§åœ¨å±€éƒ¨æ˜¯å¹³æ»‘çš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†çº¦æŸæ¥å­¦ä¹ å…‰ç…§åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ã€‚ä¸ºäº†å…è®¸ç›´è§‚åœ°ç¼–è¾‘å…‰ç…§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œä»¥å®ç°å…‰ç…§åˆ†é‡çš„æäº®æˆ–å˜æš—ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹ä½å…‰ç…§å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œæœ‰æ•ˆçš„å†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆæ–°é¢–è§†è§’æ–¹é¢å®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1):å—åˆ°å¹¿ä¹‰Retinexç†è®ºçš„å¯å‘ï¼Œå°†è¾“å…¥è§†å›¾åˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ä¸‰ä¸ªåˆ†é‡ï¼›(2):åˆ©ç”¨åç…§ç‡åœ¨å¤šè§†å›¾ä¸­ä¿æŒå…‰ç…§ä¸å˜çš„æ€§è´¨ï¼Œçº³å…¥å…ˆéªŒæ¡ä»¶ä»¥å‡è½»åˆ†è§£æ¨¡ç³Šæ€§ï¼›(3):è®¾è®¡çº¦æŸå­¦ä¹ å…‰ç…§åµŒå…¥ï¼Œé’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ï¼›(4):è®¾è®¡å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œå®ç°å…‰ç…§åˆ†é‡çš„æäº®æˆ–å˜æš—ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å…·æœ‰å¼‚è´¨äº®åº¦çš„å¤šè§†å›¾ä½å…‰ç…§ RGB å›¾åƒä¸­å­¦ä¹ ç¥ç»è¡¨å¾ã€‚ä¸¥è‹›çš„ä½å…‰ç…§æ¡ä»¶ä¼šå¯¼è‡´ä½åƒç´ å€¼å’Œæ˜¾ç€çš„ç›¸æœºä¼ æ„Ÿå™¨å™ªå£°ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ç¨³å¥çš„ Retinex ç†è®ºï¼Œå°†å¤šè§†å›¾ä½å…‰ç…§å›¾åƒåˆ†è§£ä¸ºä¸å˜çš„åç…§ç‡ã€å¯å˜å…‰ç…§å’Œå•ç‹¬çš„å™ªå£°å›¾ï¼Œä¸”è¯¥è¿‡ç¨‹æ˜¯éç›‘ç£çš„ã€‚åŸºäºåˆ†è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæœ‰æ•ˆä¸”ç›´è§‚çš„å…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œç”¨äºç¼–è¾‘æ–°é¢–è§†è§’çš„äº®åº¦ï¼Œè€Œä¸ä¼šæ”¹å˜å…¶å›ºæœ‰åç…§ç‡ã€‚è¿™é¡¹å·¥ä½œæœç€ä»ç°å®ä¸–ç•Œä¸­å¼‚è´¨ä½å…‰ç…§æ•è·ä¸­è¿›è¡Œæ–°é¢–è§†è§’åˆæˆè¿ˆå‡ºäº†è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œå¹¶ä¸”æé«˜äº†ç¼–è¾‘æ–°é¢–è§†è§’äº®åº¦çš„å¯æ§æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºå¹¿ä¹‰ Retinex ç†è®ºçš„å†…åœ¨å›¾åƒåˆ†è§£æ–¹æ³•ï¼Œå¯ä»¥å°†å¤šè§†å›¾ä½å…‰ç…§å›¾åƒåˆ†è§£ä¸ºåç…§ç‡ã€å…‰ç…§å’Œå™ªå£°ã€‚* è®¾è®¡äº†ä¸€ç§çº¦æŸï¼Œç”¨äºå­¦ä¹ å…‰ç…§åµŒå…¥å¹¶é’ˆå¯¹æ¯ä¸ªè§†å›¾ä¼˜åŒ–å™ªå£°å›¾ã€‚* è®¾è®¡äº†ä¸€ä¸ªå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œç”¨äºç›´è§‚åœ°ç¼–è¾‘æ–°é¢–è§†è§’çš„å…‰ç…§åˆ†é‡ã€‚</p><p>æ€§èƒ½ï¼š* ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¯¹ä½å…‰ç…§å¤šè§†å›¾å™ªå£°å›¾åƒè¿›è¡Œæœ‰æ•ˆçš„å†…åœ¨åˆ†è§£ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆæ–°é¢–è§†è§’æ–¹é¢å®ç°äº†å“è¶Šçš„è§†è§‰è´¨é‡å’Œæ•°å€¼æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼š* è¯¥æ–¹æ³•éœ€è¦è®¾è®¡å¤æ‚çš„çº¦æŸå’Œå…‰ç…§è°ƒæ•´æ¨¡å—ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚* è¯¥æ–¹æ³•éœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯å’Œå™ªå£°æ°´å¹³è¿›è¡Œå¾®è°ƒï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å·¥ä½œé‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-04493baafe5344e066eb68bdfb8f970b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b762594b637145749341454946297e3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6493f029d277d40898ea8a23bc339350.jpg" align="middle"></details>## Depth-guided NeRF Training via Earth Mover's Distance**Authors:Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy**Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover's Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures. [PDF](http://arxiv.org/abs/2403.13206v1) Preprint. Under review**Summary**ç¥ç»è¾å°„åœº (NeRF) é€šè¿‡æœ€å°åŒ–é¢„æµ‹è§†ç‚¹çš„æ¸²æŸ“æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œä½†å…‰åº¦æŸå¤±é€šå¸¸ä¸è¶³ä»¥è¯†åˆ«äº§ç”Ÿç›¸åŒå›¾åƒçš„ä¸åŒå‡ ä½•å½¢çŠ¶ä¹‹é—´çš„å·®å¼‚ã€‚**Key Takeaways*** ä½¿ç”¨æ·±åº¦ç›‘ç£å¯ä»¥æ”¹å–„ NeRF è®­ç»ƒï¼Œä½†æ·±åº¦å…ˆéªŒå¯èƒ½ä¸å‡†ç¡®ã€‚* ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ä»¥é¢„æµ‹æ·±åº¦å¹¶æ•è·å»å™ªè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚* é‡‡ç”¨ Earth Mover's Distance è€Œä¸æ˜¯ L2 æŸå¤±æ¥ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒã€‚* æ·±åº¦å¼•å¯¼çš„ NeRF åœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šæ˜æ˜¾ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶ä¿æŒå…‰åº¦æµ‹é‡æ€§èƒ½ã€‚* é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æä¾›äº†æ¯”å®šåˆ¶æ·±åº¦å…ˆéªŒæ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚* Earth Mover's Distance å¯¹æ·±åº¦å…ˆéªŒä¸­çš„é”™è¯¯æ›´å¥å£®ã€‚* æ·±åº¦å¼•å¯¼çš„ NeRF åœ¨å‡ ä½•å’Œå…‰åº¦ä¿çœŸåº¦ä¸Šéƒ½å–å¾—äº†æ”¹è¿›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šåŸºäºEarth Moverè·ç¦»çš„æ·±åº¦å¼•å¯¼NeRFè®­ç»ƒ</li><li>ä½œè€…ï¼šAnita Rauï¼ŒJosiah Akliluï¼ŒF. Christopher Holsingerï¼ŒSerena Yeung-Levy</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ·±åº¦é¢„æµ‹ã€å•ç›®æ·±åº¦å…ˆéªŒã€Earth Moverè·ç¦»</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡æœ€å°åŒ–é¢„æµ‹è§†ç‚¹çš„æ¸²æŸ“æŸå¤±è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œå…‰åº¦æŸå¤±é€šå¸¸æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯æ¥åŒºåˆ†äº§ç”Ÿç›¸åŒå›¾åƒçš„ä¸åŒå¯èƒ½å‡ ä½•å½¢çŠ¶ã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œåœ¨ NeRF è®­ç»ƒæœŸé—´çº³å…¥äº†æ·±åº¦ç›‘ç£ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ·±åº¦ç½‘ç»œçš„å¯†é›†é¢„æµ‹ä½œä¸ºä¼ªåœ°é¢å®å†µã€‚è™½ç„¶å‡è®¾è¿™äº›æ·±åº¦å…ˆéªŒåœ¨ç»è¿‡æ»¤å™ªå£°åæ˜¯å®Œç¾çš„ï¼Œä½†å®é™…ä¸Šï¼Œæ›´éš¾æ•æ‰å…¶å‡†ç¡®æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ NeRF ç›‘ç£ä¸­æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨å®šåˆ¶è®­ç»ƒçš„æ·±åº¦æˆ–ä¸ç¡®å®šæ€§å…ˆéªŒï¼Œè€Œæ˜¯ä½¿ç”¨ç°æˆçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥é¢„æµ‹æ·±åº¦å¹¶æ•æ‰å»å™ªè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ç”±äºæˆ‘ä»¬çŸ¥é“æ·±åº¦å…ˆéªŒå®¹æ˜“å‡ºé”™ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºä½¿ç”¨ Earth Mover è·ç¦»æ¥ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒï¼Œè€Œä¸æ˜¯é€šè¿‡ L2 æŸå¤±å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬çš„æ·±åº¦å¼•å¯¼ NeRF åœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬ä½¿ç”¨ Earth Mover è·ç¦»æ¥ç›‘ç£æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„ç›®æ ‡ï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼ NeRF è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1) æ·±åº¦å…ˆéªŒæ„å»ºï¼šä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ã€‚</p><p>(2) EarthMoverè·ç¦»ç›‘ç£ï¼šä½¿ç”¨EarthMoverè·ç¦»ç›‘ç£å°„çº¿ç»ˆæ­¢è·ç¦»åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚</p><p>(3) NeRFè®­ç»ƒï¼šå°†æ·±åº¦å…ˆéªŒå’ŒEarthMoverè·ç¦»ç›‘ç£æ•´åˆåˆ°NeRFè®­ç»ƒä¸­ï¼Œä»¥æŒ‡å¯¼NeRFè·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº EarthMover è·ç¦»çš„æ·±åº¦å¼•å¯¼ NeRF è®­ç»ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº† NeRF è®­ç»ƒä¸­æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„é—®é¢˜ï¼Œåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ EarthMover è·ç¦»æ¥ç›‘ç£æ·±åº¦å…ˆéªŒä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯é€šè¿‡ L2 æŸå¤±å¼ºåˆ¶æ¸²æŸ“æ·±åº¦å®Œå…¨å¤åˆ¶æ·±åº¦å…ˆéªŒã€‚</li><li>ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ï¼Œæ„å»ºæ·±åº¦å…ˆéªŒã€‚</li><li>å°†æ·±åº¦å…ˆéªŒå’Œ EarthMover è·ç¦»ç›‘ç£æ•´åˆåˆ° NeRF è®­ç»ƒä¸­ï¼Œä»¥æŒ‡å¯¼ NeRF è·å¾—æ›´å¥½çš„æ·±åº¦ä¼°è®¡ã€‚</li><li>æ€§èƒ½ï¼šåœ¨æ ‡å‡†æ·±åº¦æŒ‡æ ‡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å¤§å¹…ä¼˜äºæ‰€æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨å…‰åº¦æµ‹é‡ä¸Šä¿æŒæ€§èƒ½ã€‚</li><li>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥é¢„æµ‹æ·±åº¦å’Œä¸ç¡®å®šæ€§ï¼Œå¢åŠ äº†è®­ç»ƒæ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-7f7a92f6e9be3db7644e814aec9dcd80.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f625e7f81df8a66f9028e6ae38fc62df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2672e8f7ab06d8ccbaacbdbcd5b003b6.jpg" align="middle"></details><h2 id="Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><a href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering" class="headerlink" title="Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering"></a>Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering</h2><p><strong>Authors:Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</strong></p><p>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a href="https://shaomq2187.github.io/GF-NeRF/">https://shaomq2187.github.io/GF-NeRF/</a> </p><p><a href="http://arxiv.org/abs/2403.12839v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡å…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥å’Œä¸¤é˜¶æ®µæ¶æ„ï¼Œåœ¨ä¿æŒåœºæ™¯ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡å¤§åœºæ™¯æ¸²æŸ“ä¿çœŸåº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå…¨å±€å¼•å¯¼ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œæå‡å¤§åœºæ™¯æ¸²æŸ“ä¿çœŸåº¦ã€‚</li><li>é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œå…¨å±€é˜¶æ®µè·å–åœºæ™¯è¿ç»­è¡¨ç¤ºï¼Œå±€éƒ¨é˜¶æ®µåˆ†è§£å¹¶ç»†åŒ–å¤„ç†ã€‚</li><li>åˆ©ç”¨å…¨å±€ç¼–ç å™¨ï¼Œé™ä½å±€éƒ¨é˜¶æ®µè®­ç»ƒå¤æ‚åº¦ï¼Œä¿è¯åœºæ™¯ä¸€è‡´æ€§ã€‚</li><li>å¼•å…¥å…¨å±€ç©ºé—´ä¿¡æ¯å’Œè¯¯å·®ä¿¡æ¯ï¼Œå¸®åŠ©å±€éƒ¨ç¼–ç å™¨å…³æ³¨å…³é”®åŒºåŸŸï¼Œæœ‰æ•ˆæ•æ‰å¤§åœºæ™¯ç»†èŠ‚ã€‚</li><li>GF-NeRFæ— éœ€åœºæ™¯å…ˆéªŒçŸ¥è¯†ï¼Œé€‚åº”å„ç§å¤§åœºæ™¯ç±»å‹ã€‚</li><li>GF-NeRFåœ¨å¤šç§å¤§åœºæ™¯æ•°æ®é›†ä¸Šï¼Œå®ç°é«˜ä¿çœŸåº¦ã€è‡ªç„¶æ¸²æŸ“æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šç”¨äºå¤§åœºæ™¯æ¸²æŸ“çš„å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šé‚µæ˜å¥‡ï¼Œç†Šå³°ï¼Œå¼ èˆªï¼Œæ¨çˆ½ï¼Œå¾ç©†ï¼Œåä¼Ÿï¼Œç‹é›ªqian</li><li>éš¶å±ï¼šæ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢</li><li>å…³é”®è¯ï¼šå¤§åœºæ™¯æ¸²æŸ“Â·ç¥ç»è¾å°„åœºÂ·å…¨å±€å’Œå±€éƒ¨</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12839</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²è¢«ç”¨äºæ¸²æŸ“å¤§åœºæ™¯ã€‚ç„¶è€Œï¼Œå…¶æœ‰é™çš„æ¨¡å‹å®¹é‡é€šå¸¸ä¼šå¯¼è‡´æ¸²æŸ“ç»“æœæ¨¡ç³Šã€‚ç°æœ‰çš„å¤§è§„æ¨¡ NeRF ä¸»è¦é€šè¿‡å°†åœºæ™¯åˆ’åˆ†ä¸ºå—æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œç„¶åç”±å•ç‹¬çš„å­ NeRF è¿›è¡Œå¤„ç†ã€‚è¿™äº›å­ NeRF ä»å¤´å¼€å§‹è®­ç»ƒå¹¶ç‹¬ç«‹å¤„ç†ï¼Œå¯¼è‡´åœºæ™¯ä¸­çš„å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚å› æ­¤ï¼Œå°½ç®¡æ¨¡å‹å®¹é‡æœ‰æ‰€å¢åŠ ï¼Œä½†æ¸²æŸ“è´¨é‡å¹¶æ²¡æœ‰æ˜¾ç€æé«˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼š</li><li>å­ NeRF ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¯¼è‡´åœºæ™¯ä¸­å‡ ä½•å’Œå¤–è§‚ä¸ä¸€è‡´ã€‚</li><li>å°½ç®¡æ¨¡å‹å®¹é‡å¢åŠ ï¼Œä½†æ¸²æŸ“è´¨é‡å¹¶æ²¡æœ‰æ˜¾ç€æé«˜ã€‚</li><li>è¯¥æ–¹æ³•çš„åˆç†æ€§ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åˆç†ï¼Œå› ä¸ºå®ƒï¼š</li><li>åˆ©ç”¨äº†å…¨å±€å’Œå±€éƒ¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œå¯ä»¥è·å¾—åœºæ™¯çš„è¿ç»­è¡¨ç¤ºå¹¶è¿›ä¸€æ­¥å¤„ç†å±€éƒ¨å—ã€‚</li><li>é‡‡ç”¨äº†å…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥ä¿æŒåœºæ™¯èŒƒå›´å†…çš„è¿è´¯æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼š</li><li>æå‡ºäº†ä¸€ç§å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œå¯ä»¥å®ç°å¤§åœºæ™¯çš„é«˜ä¿çœŸæ¸²æŸ“ã€‚</li><li>GF-NeRF åˆ©ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ã€‚</li><li>å…¨å±€é˜¶æ®µè·å¾—æ•´ä¸ªåœºæ™¯çš„è¿ç»­è¡¨ç¤ºï¼Œè€Œå±€éƒ¨é˜¶æ®µå°†åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªå—ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„å­ç¼–ç å™¨è¿›ä¸€æ­¥å¤„ç†å®ƒä»¬ã€‚</li><li>åˆ©ç”¨è¿™ç§ä¸¤é˜¶æ®µæ¶æ„ï¼Œå­ç¼–ç å™¨åªéœ€åŸºäºå…¨å±€ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»è€Œé™ä½äº†å±€éƒ¨é˜¶æ®µçš„è®­ç»ƒå¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒäº†åœºæ™¯èŒƒå›´å†…çš„è¿è´¯æ€§ã€‚</li><li>æ¥è‡ªå…¨å±€é˜¶æ®µçš„ç©ºé—´ä¿¡æ¯å’Œé”™è¯¯ä¿¡æ¯ä¹Ÿæœ‰åŠ©äºå­ç¼–ç å™¨ä¸“æ³¨äºå…³é”®åŒºåŸŸï¼Œå¹¶æœ‰æ•ˆæ•æ‰å¤§åœºæ™¯çš„æ›´å¤šç»†èŠ‚ã€‚</li><li>è¯¥æ–¹æ³•ä¸éœ€è¦ä»»ä½•å…³äºç›®æ ‡åœºæ™¯çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™ä½¿å¾— GF-NeRF é€‚ç”¨äºå„ç§å¤§åœºæ™¯ç±»å‹ï¼ŒåŒ…æ‹¬è¡—æ™¯å’Œèˆªæ‹åœºæ™¯ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š</li><li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†çš„å„ç§ç±»å‹ä¸Šå®ç°äº†é«˜ä¿çœŸã€è‡ªç„¶çš„æ¸²æŸ“ç»“æœã€‚</li><li>æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼š</li><li>GF-NeRF åœ¨å¤§åœºæ™¯æ¸²æŸ“ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li><p>GF-NeRF çš„æ¸²æŸ“ç»“æœå…·æœ‰é«˜ä¿çœŸåº¦å’Œè‡ªç„¶æ„Ÿã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) <strong>æå‡ºå…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰</strong>ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥ï¼›(2) <strong>å…¨å±€é˜¶æ®µ</strong>ï¼šè·å¾—æ•´ä¸ªåœºæ™¯çš„è¿ç»­è¡¨ç¤ºï¼›(3) <strong>å±€éƒ¨é˜¶æ®µ</strong>ï¼šå°†åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªå—ï¼Œä½¿ç”¨ä¸åŒçš„å­ç¼–ç å™¨è¿›ä¸€æ­¥å¤„ç†ï¼›(4) <strong>å­ç¼–ç å™¨</strong>ï¼šåŸºäºå…¨å±€ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œé™ä½è®­ç»ƒå¤æ‚åº¦ï¼Œä¿æŒåœºæ™¯è¿è´¯æ€§ï¼›(5) <strong>æ¥è‡ªå…¨å±€é˜¶æ®µçš„ç©ºé—´ä¿¡æ¯å’Œé”™è¯¯ä¿¡æ¯</strong>ï¼šå¸®åŠ©å­ç¼–ç å™¨ä¸“æ³¨äºå…³é”®åŒºåŸŸï¼Œæ•æ‰æ›´å¤šç»†èŠ‚ï¼›(6) <strong>æ— éœ€å…ˆéªŒçŸ¥è¯†</strong>ï¼šé€‚ç”¨äºå„ç§å¤§åœºæ™¯ç±»å‹ã€‚</p></li><li><p>ç»¼è¿°(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼Œä¸“é—¨ç”¨äºæ¸²æŸ“å¤§åœºæ™¯ã€‚æˆ‘ä»¬å°†å¤§è§„æ¨¡ NeRF çš„è®­ç»ƒåˆ†ä¸ºå…¨å±€å’Œå±€éƒ¨ä¸¤ä¸ªé˜¶æ®µã€‚GF-NeRF åˆ©ç”¨ä»å…¨å±€é˜¶æ®µè·å¾—çš„å…³äºæ•´ä¸ªåœºæ™¯çš„ä¸°å¯Œå…ˆéªŒæ¥æŒ‡å¯¼å±€éƒ¨é˜¶æ®µä¸­æ¯ä¸ªå—çš„è®­ç»ƒè¿‡ç¨‹ã€‚å…¨å±€å’Œå±€éƒ¨é˜¶æ®µçš„é›†æˆä½¿ GF-NeRF èƒ½å¤Ÿåœ¨æ‰©å±•æ¨¡å‹å®¹é‡çš„åŒæ—¶ä¿æŒå‡ ä½•å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å…³æ³¨é‡è¦åŒºåŸŸä»¥æ•æ‰æ›´å¤šå¤æ‚çš„ç»†èŠ‚ã€‚å°½ç®¡åœ¨å„ç§ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®ç°äº†é«˜ä¿çœŸæ¸²æŸ“ç»“æœï¼Œä½†åœ¨æœªæ¥æˆ‘ä»¬ä»æœ‰ä¸€äº›æŒ‘æˆ˜éœ€è¦è§£å†³ï¼š(1) ä¸å½“å‰æœ€å¿«çš„æ¸²æŸ“æ–¹æ³•ï¼ˆä¾‹å¦‚ 3D é«˜æ–¯ splatting [10]ï¼‰ç›¸æ¯”ï¼ŒGF-NeRF çš„è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ä»ç„¶ç›¸å¯¹è¾ƒæ…¢ã€‚(2) è™½ç„¶æˆ‘ä»¬å°†å†…å­˜æ¶ˆè€—ä¸å“ˆå¸Œç¼–ç å™¨çš„æ•°é‡è§£è€¦ï¼Œä½†åœ¨æå¤§çš„åœºæ™¯ä¸­ï¼Œç©ºé—´å…«å‰æ ‘çš„å†…å­˜ä½¿ç”¨é‡ä¸å¯å¿½ç•¥ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é‡‡ç”¨ä¸¤é˜¶æ®µï¼ˆå…¨å±€å’Œå±€éƒ¨ï¼‰æ¶æ„å’Œå…¨å±€å¼•å¯¼è®­ç»ƒç­–ç•¥çš„å…¨å±€å¼•å¯¼å±€éƒ¨ç¥ç»è¾å°„åœºï¼ˆGF-NeRFï¼‰ï¼›æ€§èƒ½ï¼šåœ¨å„ç§ç±»å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ¸²æŸ“ç»“æœå…·æœ‰é«˜ä¿çœŸåº¦å’Œè‡ªç„¶æ„Ÿï¼›å·¥ä½œé‡ï¼šè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ä»ç„¶ç›¸å¯¹è¾ƒæ…¢ï¼Œç©ºé—´å…«å‰æ ‘çš„å†…å­˜ä½¿ç”¨é‡åœ¨æå¤§çš„åœºæ™¯ä¸­ä¸å¯å¿½ç•¥ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-899d4b54074d26e227130dfac2bc6e88.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9e5589d8ab0b3c2c1a697bd164522867.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8377daea075903708a9bab34c78f9671.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bd2f096fd2683b96bf19870d6f516562.jpg" align="middle"></details><h2 id="FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><a href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos" class="headerlink" title="FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos"></a>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos</h2><p><strong>Authors:Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</strong></p><p>Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy. </p><p><a href="http://arxiv.org/abs/2403.12198v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»æ¸²æŸ“åœ¨å†…çª¥é•œé‡å»ºä¸­å–å¾—å‘å±•ï¼Œä½†ä¸€ç›´å—é™äºé™æ€å†…çª¥é•œæˆ–å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡ã€‚FLexæå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ç§æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¯ä»¥ç«¯åˆ°ç«¯åœ°è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºä½å§¿ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>FLex æå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªé‡å çš„ 4D NeRFã€‚</li><li>ä¸€ç§æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆå¯è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºä½å§¿ã€‚</li><li>æ— éœ€å¤–éƒ¨è·Ÿè¸ªä¿¡æ¯ï¼Œå³å¯é‡å»ºæ‰‹æœ¯è§†é¢‘ä¸­ 5000 å¸§ä»¥ä¸Šçš„åŠ¨æ€åœºæ™¯ã€‚</li><li>FLex åœ¨ StereoMIS æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li><li>FLex åœ¨ä¿æŒç«äº‰åŠ›ä½å§¿ç²¾åº¦çš„åŒæ—¶ï¼Œæ”¹å–„äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li><li>FLex å¯ç”¨äºåæ‰‹æœ¯åˆ†æå’Œæ•™è‚²åŸ¹è®­ç­‰å„ç§åŒ»å­¦åº”ç”¨ã€‚</li><li>FLex æ‰©å±•äº†å†…çª¥é•œé‡å»ºçš„å¯èƒ½æ€§ï¼Œä½¿å…¶å¯ç”¨äºå¤„ç†å¤§è§„æ¨¡åŠ¨æ€åœºæ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šFLexï¼šå…³èŠ‚å§¿æ€å’ŒåŠ¨æ€è¾å°„åœº</li><li>ä½œè€…ï¼šFlorian Philipp Stilzã€Mert Asim Karaogluã€Felix Tristramã€Nassir Navabã€Benjamin Busamã€Alexander Ladikos</li><li>éš¶å±å•ä½ï¼šæ…•å°¼é»‘å·¥ä¸šå¤§å­¦</li><li>å…³é”®è¯ï¼š3D é‡å»ºã€ç¥ç»æ¸²æŸ“ã€æœºå™¨äººæ‰‹æœ¯</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå†…çª¥é•œåœºæ™¯é‡å»ºæ˜¯å„ç§åŒ»ç–—åº”ç”¨çš„é‡è¦èµ„äº§ï¼Œä»æœ¯ååˆ†æåˆ°æ•™è‚²åŸ¹è®­ã€‚ç¥ç»æ¸²æŸ“æœ€è¿‘åœ¨å†…çª¥é•œé‡å»ºä¸­å±•ç¤ºäº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œå…¶ä¸­ç»„ç»‡å˜å½¢ã€‚ç„¶è€Œï¼Œè¯¥è®¾ç½®ä»…é™äºé™æ€å†…çª¥é•œã€æœ‰é™çš„å˜å½¢ï¼Œæˆ–éœ€è¦å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡æ¥æ£€ç´¢å†…çª¥é•œæ‘„åƒå¤´çš„ç›¸æœºå§¿æ€ä¿¡æ¯ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šå—é™äºé™æ€å†…çª¥é•œã€æœ‰é™çš„å˜å½¢ã€éœ€è¦å¤–éƒ¨è·Ÿè¸ªè®¾å¤‡ã€‚æœ¬æ–‡çš„æ–¹æ³•æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå®ƒè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªéšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šéšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—çš„æˆå°±æ˜¯ï¼šåœ¨ StereoMIS æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒFLex åœ¨ä¿æŒç«äº‰å§¿æ€ç²¾åº¦çš„åŒæ—¶ï¼Œæ˜¾ç€æé«˜äº†æ–°è§†å›¾åˆæˆè´¨é‡ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ï¼Œå› ä¸ºå®ƒä»¬è¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é‡å»ºå…·æœ‰å˜å½¢ç»„ç»‡çš„åŠ¨æ€å†…çª¥é•œåœºæ™¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºäº†ä¸€ç§éšå¼åœºæ™¯åˆ†ç¦»ä¸ºå¤šä¸ªé‡å çš„ 4D ç¥ç»è¾å°„åœº (NeRF) å’Œä¸€ä¸ªæ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ¡ˆä»å¤´å¼€å§‹è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºå§¿æ€ã€‚(2) é‡‡ç”¨æ¸è¿›ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»è§†é¢‘åºåˆ—çš„ç¬¬ä¸€å¸§å¼€å§‹ï¼Œé€å¸§æ·»åŠ å¸§ï¼Œåˆå§‹åŒ–æ–°å¸§çš„ç›¸æœºå§¿æ€å‚æ•°ä¸ºå‰ä¸€å¸§çš„ç›¸æœºå§¿æ€ã€‚(3) å½“æ–°æ·»åŠ çš„å¸§ä½¿å¸§æ•°è¶…è¿‡é¢„è®¾é˜ˆå€¼æˆ–æ–°å¸§ä¸å½“å‰å±€éƒ¨æ¨¡å‹ä¼˜åŒ–ä½ç½®ä¹‹é—´çš„è·ç¦»å¤§äºè·ç¦»é˜ˆå€¼æ—¶ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹ï¼Œå¹¶å°†å‰ä¸€ä¸ªæ¨¡å‹çš„æœ€å 30 å¸§ä¸æ–°å±€éƒ¨æ¨¡å‹é‡å ã€‚(4) ä¸ºç¡®ä¿æ¸è¿›ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è½¨è¿¹è¿è´¯æ€§ï¼Œå§‹ç»ˆä»æœ€åæ·»åŠ çš„å››å¸§ä¸­é‡‡æ ·å°„çº¿ã€‚(5) å½“åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹æ—¶ï¼Œå†»ç»“å‰ä¸€ä¸ªæ¨¡å‹çš„æƒé‡å¹¶å°†å…¶ä» GPU ä¸­å¸è½½ï¼Œä»¥é˜²æ­¢ä¸å¿…è¦çš„å†…å­˜ä½¿ç”¨ã€‚(6) åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¦‚æœä¸€ä¸ªå§¿æ€å¯¹åº”äºå¤šä¸ªå±€éƒ¨æ¨¡å‹çš„ç©ºé—´å’Œæ—¶é—´èŒƒå›´ï¼Œåˆ™å°†æ¯ä¸ªæ¨¡å‹çš„è´¡çŒ®èšåˆåˆ°å°„çº¿æŠ•å°„å…¬å¼ä¸­ï¼Œå¹¶åœ¨å±€éƒ¨æ¨¡å‹ä¸­å¿ƒçš„é‚»è¿‘åº¦åŸºç¡€ä¸Šï¼Œåœ¨é‡å åŒºåŸŸè®¾ç½®çº¿æ€§æ··åˆæƒé‡ã€‚(7) åœ¨åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å±€éƒ¨æ¨¡å‹ä¹‹å‰ï¼Œæœ€åä¸€ä¸ªæ¨¡å‹è¿›å…¥ç»†åŒ–é˜¶æ®µï¼Œå…¶ä¸­ä½¿ç”¨æ²¿å…¶æ•´ä¸ªè·¨åº¦å‡åŒ€é€‰å–çš„æ ·æœ¬æ‰¹æ¬¡ä¼˜åŒ–å§¿æ€å’Œæ¨¡å‹å‚æ•°ã€‚(8) ä½¿ç”¨å¸¸è§çš„åŸºäºå…‰åº¦æŸå¤±å’Œæ·±åº¦ç›‘ç£æŸå¤±çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥åŠè§†çº¿å…ˆéªŒæ¥æ­£åˆ™åŒ–å¯†åº¦å€¼ï¼Œä»¥é›†ä¸­åœ¨å®é™…è¡¨é¢ä¸Šï¼Œä»è€Œæé«˜åœºæ™¯å‡ ä½•çš„æ•æ‰èƒ½åŠ›ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† FLexï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºé‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ç»„ç»‡å˜å½¢å’Œç›¸æœºè¿åŠ¨çš„é•¿å¤–ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œæ— éœ€å§¿æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è”åˆä¼˜åŒ–é‡å»ºå’Œç›¸æœºè½¨è¿¹ï¼ŒæˆåŠŸåœ°æ¶ˆé™¤äº†å¯¹å…ˆéªŒå§¿æ€çš„ä¾èµ–ã€‚FLex æé«˜äº†åŠ¨æ€ NeRF åœ¨å¤§å‹åœºæ™¯ä¸­çš„å¯æ‰©å±•æ€§ï¼Œä»è€Œæ›´é€‚ç”¨äºå®é™…çš„æ‰‹æœ¯è®°å½•ï¼ŒåŒæ—¶åœ¨ StereoMIS æ•°æ®é›†ä¸Šæ”¹è¿›äº†å½“å‰æ–¹æ³•ï¼Œåœ¨å…·æœ‰ç«äº‰å§¿æ€ç²¾åº¦çš„åŒæ—¶å®ç°äº†æ–°è§†å›¾åˆæˆã€‚æˆ‘ä»¬ç›¸ä¿¡ FLex å¯ä»¥ä¸ºæ›´å®¹æ˜“è·å–ã€æ›´çœŸå®å’Œæ›´å¯é çš„ 4D å†…çª¥é•œé‡å»ºé“ºå¹³é“è·¯ï¼Œä»¥æ”¹è¿›æœ¯ååˆ†æå’ŒåŒ»å­¦æ•™è‚²ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0ec307fb4af9abe56b1c6a9dc1dd13ed.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e9f8dc2d5f9a8772e8d0c87732245680.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc7f5acee78bffb98a3d99a47c4c410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aeddd230be678442733e61b882ccd697.jpg" align="middle"></details><h2 id="ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><a href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis" class="headerlink" title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis"></a>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p><p>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method. </p><p><a href="http://arxiv.org/abs/2403.12154v1">PDF</a> </p><p><strong>Summary:</strong><br>ç¥ç»è¾å°„åœºä¸­å¤šæ¨¡æ€æ–¹æ³•ï¼ŒèåˆRGBå’Œçƒ­å›¾åƒï¼Œç”¨äºåœºæ™¯é‡å»ºå’Œç²¾å‡†çƒ­å›¾åƒåˆæˆã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>ThermoNeRF é‡‡ç”¨ç¥ç»è¾å°„åœºï¼Œèåˆ RGB å’Œçƒ­å›¾åƒè¿›è¡Œåœºæ™¯é‡å»ºã€‚</li><li>é€šè¿‡é…å¯¹çš„ RGB å’Œçƒ­å›¾åƒå­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œå…‹æœçƒ­å›¾åƒçº¹ç†ç¼ºä¹çš„é—®é¢˜ã€‚</li><li>ç‹¬ç«‹ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ï¼Œç²¾å‡†æ•æ‰åœºæ™¯çš„å¤–è§‚å’Œçƒ­é‡åˆ†å¸ƒã€‚</li><li>å¼•å…¥ ThermoScenes æ•°æ®é›†ï¼Œå¼¥è¡¥ RGB+çƒ­å›¾åƒåœºæ™¯é‡å»ºæ•°æ®é›†çš„ä¸è¶³ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒThermoNeRF åœ¨çƒ­å›¾åƒåˆæˆä¸­å–å¾—äº†ä¼˜å¼‚è¡¨ç°ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5$^\circ$Cã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒThermoNeRF çš„çƒ­å›¾åƒåˆæˆç²¾åº¦æé«˜äº† 50% ä»¥ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šThermoNeRFï¼šç”¨äºçƒ­é‡æ–°è§†è§’åˆæˆçš„å¤šæ¨¡æ€ç¥ç»è¾å°„åœº</li><li>ä½œè€…ï¼šMariam Hassanã€Florent Forestã€Olga Finkã€Malcolm Mielle</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ´›æ¡‘è”é‚¦ç†å·¥å­¦é™¢ï¼ˆEPFLï¼‰</li><li>å…³é”®è¯ï¼šçƒ­æˆåƒã€ç¥ç»è¾å°„åœºã€3D é‡å»ºã€å¤šæ¨¡æ€</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12154   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/SchindlerEPFL/thermo-nerf</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šçƒ­åœºæ™¯é‡å»ºåœ¨å»ºç­‘èƒ½è€—åˆ†æå’Œæ— æŸæ£€æµ‹ç­‰å¹¿æ³›é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚   ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯†é›†çš„åœºæ™¯æµ‹é‡ï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ– RGB å›¾åƒè¿›è¡Œ 3D å‡ ä½•é‡å»ºï¼Œçƒ­ä¿¡æ¯åœ¨é‡å»ºåæŠ•å½±ã€‚ç”±äºçƒ­å›¾åƒä¸­ç¼ºä¹çº¹ç†ï¼Œè¿™ç§ä¸¤æ­¥ç­–ç•¥ä¼šå¯¼è‡´é‡å»ºå¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œæ¸©åº¦ä¸å®é™…åœºæ™¯ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚   ï¼ˆ3ï¼‰ï¼šä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† ThermoNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„æ–°å‹å¤šæ¨¡æ€æ–¹æ³•ï¼Œèƒ½å¤Ÿè”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚ä¸ºäº†å…‹æœçƒ­å›¾åƒä¸­ç¼ºä¹çº¹ç†çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆå¯¹çš„ RGB å’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† ThermoScenesï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨ RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚   ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœéªŒè¯äº† ThermoNeRF å¯ä»¥å®ç°å‡†ç¡®çš„çƒ­å›¾åƒåˆæˆï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5Â°Cï¼Œä¸ä½¿ç”¨æœ€å…ˆè¿›çš„ NeRF æ–¹æ³• Nerfacto ä½¿ç”¨è¿æ¥çš„ RGB+çƒ­æ•°æ®ç›¸æ¯”ï¼Œæé«˜äº† 50% ä»¥ä¸Šã€‚</li></ol><p><methods>1. æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³•ThermoNeRFï¼Œå¯ä»¥è”åˆæ¸²æŸ“åœºæ™¯çš„æ–°RGBå’Œçƒ­è§†å›¾ã€‚2. ä½¿ç”¨æˆå¯¹çš„RGBå’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚3. å¼•å…¥äº†ThermoScenesæ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰è¿™é¡¹å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³• ThermoNeRFï¼Œç”¨äºè”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚æ­¤å¤–ï¼Œè¿˜æ•´ç†äº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ RGB+çƒ­åœºæ™¯é‡å»ºçš„æ–°æ•°æ®é›†ã€‚ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³• ThermoNeRFï¼Œå¯ä»¥è”åˆæ¸²æŸ“åœºæ™¯çš„æ–° RGB å’Œçƒ­è§†å›¾ã€‚</li><li>ä½¿ç”¨æˆå¯¹çš„ RGB å’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œä¸åŒçš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦ä¿¡æ¯ã€‚</li><li>å¼•å…¥äº† ThermoScenes æ•°æ®é›†ï¼Œç”¨äºå¼¥è¡¥ç”¨äºåœºæ™¯é‡å»ºçš„å¯ç”¨ RGB+çƒ­æ•°æ®é›†çš„ä¸è¶³ã€‚æ€§èƒ½ï¼š</li><li>å®éªŒç»“æœéªŒè¯äº† ThermoNeRF å¯ä»¥å®ç°å‡†ç¡®çš„çƒ­å›¾åƒåˆæˆï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º 1.5Â°Cï¼Œä¸ä½¿ç”¨æœ€å…ˆè¿›çš„ NeRF æ–¹æ³• Nerfacto ä½¿ç”¨è¿æ¥çš„ RGB+çƒ­æ•°æ®ç›¸æ¯”ï¼Œæé«˜äº† 50% ä»¥ä¸Šã€‚å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒ ThermoNeRF æ¨¡å‹éœ€è¦å¤§é‡çš„æˆå¯¹ RGB å’Œçƒ­å›¾åƒæ•°æ®ã€‚</li><li>æ¸²æŸ“æ–°çš„ RGB å’Œçƒ­è§†å›¾éœ€è¦è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-319e4bbd191efe49994bcb5b2edb9350.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08e86b5af05b01390e4b33a0c407a04a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38033c4d1befea3579fd3788d39750d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" align="middle"></details><h2 id="RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><a href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF" class="headerlink" title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF"></a>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</h2><p><strong>Authors:Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero</strong></p><p>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset. </p><p><a href="http://arxiv.org/abs/2403.11909v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFèåˆäº†2Då’Œ3Då¢å¼ºå™¨çš„ä¼˜ç‚¹ï¼Œåˆ©ç”¨äº†åœºæ™¯å‡ ä½•ä¿¡æ¯ï¼Œåœ¨ä¿è¯å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ¢å¤äº†é«˜é¢‘çº¹ç†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>RoGUENeRFç»“åˆäº†2Då’Œ3Då¢å¼ºå™¨çš„ä¼˜ç‚¹ï¼Œå­¦ä¹ é€šç”¨å¢å¼ºå™¨å¹¶åˆ©ç”¨åœºæ™¯å‡ ä½•ä¿¡æ¯ã€‚</li><li>RoGUENeRFé‡‡ç”¨äº†ç¨³å¥çš„3Då¯¹é½å’Œå‡ ä½•æ„ŸçŸ¥èåˆï¼Œä»ä¸´è¿‘è®­ç»ƒå›¾åƒä¸­è¿ç§»ç»†èŠ‚ã€‚</li><li>RoGUENeRFå¯ä»¥æé«˜å„ç§ç¥ç»æ¸²æŸ“åŸºçº¿çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨360v2æ•°æ®é›†ä¸Šï¼ŒMipNeRF360çš„PSNRæé«˜äº†0.63dBï¼ŒNerfactoæé«˜äº†1.34dBã€‚</li><li>RoGUENeRFå¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ï¼Œå¯ä»¥ä¿æŒå‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>RoGUENeRFæ¢å¤äº†é«˜é¢‘çº¹ç†ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>RoGUENeRFåœ¨ä¿è¯å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶ï¼Œæ¢å¤äº†é«˜é¢‘çº¹ç†ã€‚</li><li>RoGUENeRFå¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šRoGUENeRFï¼šä¸€æ¬¾ç”¨äº NeRF 53D ç‰¹å¾é‡æŠ•å½±çš„é²æ£’å‡ ä½•ä¸€è‡´é€šç”¨å¢å¼ºå™¨</li><li>ä½œè€…ï¼šYiming Qianã€Zexiang Xuã€Jia-Bin Huangã€Yifan Wangã€Hui Huangã€Hao Suã€Shuaicheng Liuã€Qian Chen</li><li>éš¶å±å•ä½ï¼š</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“ã€NeRFã€å›¾åƒå¢å¼ºã€å‡ ä½•ä¸€è‡´æ€§ã€é²æ£’æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.11909v1[cs.CV]18Mar2024</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨é‡å»ºé«˜é¢‘ç»†èŠ‚æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ï¼ŒåŸå› åŒ…æ‹¬è¾å°„åœºçš„ä½é¢‘åå·®å’Œç›¸æœºæ ¡å‡†ä¸å‡†ç¡®ã€‚ä¸€ç§ç¼“è§£æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯åœ¨æ¸²æŸ“åå¢å¼ºå›¾åƒã€‚2D å¢å¼ºå™¨å¯ä»¥ç»è¿‡é¢„è®­ç»ƒä»¥æ¢å¤ä¸€äº›ç»†èŠ‚ï¼Œä½†å®ƒä»¬ä¸åœºæ™¯å‡ ä½•æ— å…³ï¼Œå¹¶ä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°çš„å›¾åƒé€€åŒ–åˆ†å¸ƒã€‚ç›¸åï¼Œç°æœ‰çš„ 3D å¢å¼ºå™¨èƒ½å¤Ÿä»¥å¯æ³›åŒ–çš„æ–¹å¼ä»é™„è¿‘çš„è®­ç»ƒå›¾åƒä¸­è½¬ç§»ç»†èŠ‚ï¼Œä½†å®ƒä»¬å—ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®çš„å½±å“ï¼Œå¹¶ä¸”å¯èƒ½å°†å‡ ä½•ä¸­çš„é”™è¯¯ä¼ æ’­åˆ°æ¸²æŸ“çš„å›¾åƒä¸­ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼š2D å¢å¼ºå™¨ä¸åœºæ™¯å‡ ä½•æ— å…³ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æ–°çš„å›¾åƒé€€åŒ–åˆ†å¸ƒï¼›3D å¢å¼ºå™¨å—ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®çš„å½±å“ï¼Œå¹¶ä¸”å¯èƒ½å°†å‡ ä½•ä¸­çš„é”™è¯¯ä¼ æ’­åˆ°æ¸²æŸ“çš„å›¾åƒä¸­ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»æ¸²æŸ“å¢å¼ºå™¨ RoGUENeRFï¼Œå®ƒåˆ©ç”¨äº†è¿™ä¸¤ç§èŒƒå¼çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»è¿‡é¢„è®­ç»ƒä»¥å­¦ä¹ é€šç”¨å¢å¼ºå™¨ï¼ŒåŒæ—¶è¿˜é€šè¿‡é²æ£’çš„ 3D å¯¹é½å’Œæ„ŸçŸ¥å‡ ä½•çš„èåˆåˆ©ç”¨æ¥è‡ªé™„è¿‘è®­ç»ƒå›¾åƒçš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¢å¤äº†é«˜é¢‘çº¹ç†ï¼ŒåŒæ—¶ä¿æŒäº†å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶ä¸”å¯¹ç›¸æœºæ ¡å‡†ä¸å‡†ç¡®å…·æœ‰é²æ£’æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿè¯¥æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼Ÿæˆ‘ä»¬è¡¨æ˜ï¼ŒRoGUENeRF å¤§å¤§æé«˜äº† NeRF çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨å‡ ä½•ä¸€è‡´æ€§ã€çº¹ç†ç»†èŠ‚å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§é²æ£’ä¸”é€šç”¨çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰3D å¯¹é½ï¼šé€šè¿‡æ·±åº¦å›¾å’Œç›¸æœºä½å§¿ï¼Œå°†è®­ç»ƒå›¾åƒç‰¹å¾ 3D å¯¹é½åˆ°æ–°é¢–ç›¸æœºè§†ç‚¹ã€‚ï¼ˆ2ï¼‰éåˆšæ€§ç»†åŒ–ï¼šä½¿ç”¨è½»é‡çº§è¿­ä»£å…‰æµç½‘ç»œè¿›ä¸€æ­¥ç»†åŒ–å¯¹é½ã€‚ï¼ˆ3ï¼‰å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›ï¼šå¼•å…¥å¯å­¦ä¹ çš„ç»„åˆç©ºé—´å’Œå‡ ä½•æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥è°ƒèŠ‚æœªå¯¹é½åŒºåŸŸã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é²æ£’ä¸”é€šç”¨çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFï¼Œå®ƒç»“åˆäº†3Då’Œ2Dè§†è§‰çš„æ¦‚å¿µï¼Œæ˜¾è‘—æé«˜äº†NeRFåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ‰§è¡Œ3Då¯¹é½å’Œéåˆšæ€§ç»†åŒ–æ¥å‡†ç¡®æ‰¾åˆ°ä¸åŒç›¸æœºè§†å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼ŒåŒæ—¶å¯¹ç›¸æœºä½å§¿ä¼°è®¡ä¸­çš„è¯¯å·®å…·æœ‰é²æ£’æ€§ï¼Œå¹¶é€šè¿‡å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›å‡å°‘äº†é‡æŠ•å½±ä¼ªå½±ã€‚RoGUENeRFåœ¨PSNRã€SSIMå’ŒLPIPSæ–¹é¢å–å¾—äº†ä¸€è‡´çš„æå‡ï¼Œå¹¶åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»æ¸²æŸ“å¢å¼ºå™¨RoGUENeRFï¼Œå®ƒç»“åˆäº†3Då’Œ2Dè§†è§‰çš„æ¦‚å¿µï¼Œä»¥æé«˜NeRFæ¸²æŸ“çš„è´¨é‡ã€‚</li><li>æå‡ºäº†ä¸€ç§é²æ£’çš„3Då¯¹é½å’Œéåˆšæ€§ç»†åŒ–æ–¹æ³•ï¼Œå¯ä»¥å‡†ç¡®æ‰¾åˆ°ä¸åŒç›¸æœºè§†å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œå¹¶å¯¹ç›¸æœºä½å§¿ä¼°è®¡ä¸­çš„è¯¯å·®å…·æœ‰é²æ£’æ€§ã€‚</li><li>å¼•å…¥äº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—ï¼Œå¯ä»¥è°ƒèŠ‚æœªå¯¹é½åŒºåŸŸï¼Œå‡å°‘é‡æŠ•å½±ä¼ªå½±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨PSNRã€SSIMå’ŒLPIPSæ–¹é¢å–å¾—äº†ä¸€è‡´çš„æå‡ã€‚</li><li>åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-17d0997c9aebd53c84af95df889721cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c4cc429ba9d86c80e51605a322a73a6.jpg" align="middle"></details><h2 id="GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><a href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors" class="headerlink" title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors"></a>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors</h2><p><strong>Authors:LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong</strong></p><p>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin. </p><p><a href="http://arxiv.org/abs/2403.11899v1">PDF</a> Accepted to ICLR 2024 Poster. For the Appendix, please see   <a href="http://yukiumi13.github.io/gnerp_page">http://yukiumi13.github.io/gnerp_page</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä»å¤šè§†å›¾ç«‹ä½“å£°ï¼ˆMVSï¼‰ä¸­å­¦ä¹ æ›²é¢æˆä¸ºä¸€ä¸ªæ–°å…´è¯¾é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SDFæ–¹æ³•èƒ½å¤Ÿé‡å»ºæœ—ä¼¯åœºæ™¯çš„å‡†ç¡®3Då½¢çŠ¶ã€‚</li><li>åŸºäºæåŒ–çš„é«˜æ–¯æ³•çº¿è¡¨ç¤ºå¯ä»¥å¼•å¯¼å­¦ä¹ é•œé¢åå°„åçš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>é‡æ–°åŠ æƒç­–ç•¥å¯ä»¥å‡è½»æåŒ–å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚</li><li>æ•è·æåŒ–ä¿¡æ¯å’Œé™„åŠ åå°„åœºæ™¯ä¸­çš„çœŸå®ç½‘æ ¼ä»¥éªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>åœ¨PANDORAæ•°æ®é›†ä¸Šè¯„ä¼°è¯¥æ¡†æ¶ã€‚</li><li>åœ¨åå°„åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„ç¥ç»3Dé‡å»ºæ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯å¼•å¯¼ç¥ç»é‡å»ºå…·æœ‰å™ªå£°åæŒ¯å…ˆéªŒçš„åå…‰ç‰©ä½“</li><li>ä½œè€…ï¼šYang LI, Ruizheng WU, Jiyong LI, Yingcong CHEN</li><li>éš¶å±ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰äººå·¥æ™ºèƒ½ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šNeRF, SDF, åå…‰è¡¨é¢é‡å»ºï¼ŒåæŒ¯å…ˆéªŒ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11899</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å¤šè§†å›¾ç«‹ä½“è§†è§‰ï¼ˆMVSï¼‰ä¸­ç”¨äºè¡¨é¢é‡å»ºå·²æˆä¸ºä¸€ä¸ªæ–°å…´è¯¾é¢˜ã€‚åŸºäºç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰çš„æ–¹æ³•å·²è¢«è¯æ˜èƒ½å¤Ÿé‡å»ºæœ—ä¼¯ç‰©ä½“åœºæ™¯çš„å‡†ç¡® 3D å½¢çŠ¶ã€‚ç„¶è€Œï¼Œç”±äºé•œé¢å…‰ç…§å’Œå¤æ‚å‡ ä½•å½¢çŠ¶çš„çº ç¼ ï¼Œå®ƒä»¬åœ¨åå…‰åœºæ™¯ä¸­çš„é‡å»ºç»“æœå¹¶ä¸ä»¤äººæ»¡æ„ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•è¯•å›¾é€šè¿‡åŒå‘åå°„åˆ†å¸ƒå‡½æ•°ï¼ˆBRDFï¼‰å¯¹å…‰çº¿å’Œè¡¨é¢çš„ç›¸äº’ä½œç”¨è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œå¯¹å…¶è¿›è¡Œä¼°è®¡ã€‚ç„¶è€Œï¼ŒBRDF å…¬å¼åŒ–å¸¦æ¥çš„åé—®é¢˜æ˜¯é«˜åº¦ä¸é€‚å®šçš„ï¼Œå¹¶ä¸”ç¥ç» BRDF çš„ä½é¢‘åå·®ä½¿å¾—å­¦ä¹ åˆ°çš„å‡ ä½•å½¢çŠ¶è¿‡åº¦å¹³æ»‘ã€‚æ­¤å¤–ï¼Œä¸€äº›æ–¹æ³•åˆ©ç”¨åæŒ¯å…ˆéªŒæ¥ä¿ƒè¿›é•œé¢åå°„çš„å­¦ä¹ ï¼Œå› ä¸ºå®ƒä»¬æ­ç¤ºäº†å…³äºè¡¨é¢æ³•çº¿çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼ŒåæŒ¯ä¿¡æ¯æ€»æ˜¯é›†ä¸­åœ¨é•œé¢åå°„åŒºåŸŸï¼Œè¿™ä½¿å¾—å­¦ä¹ åˆ°çš„å‡ ä½•å½¢çŠ¶å­˜åœ¨å™ªå£°å’Œä¸å‡†ç¡®æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ SDF åŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºã€‚åœ¨åæŒ¯å…ˆéªŒçš„ç›‘ç£ä¸‹ï¼Œè¿™ç§è¡¨ç¤ºæŒ‡å¯¼äº†é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼Œå¹¶æ¯”ç°æœ‰æ–¹æ³•æ•æ‰åˆ°äº†æ›´å¤šç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ æƒçš„ç­–ç•¥ï¼Œä»¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šä¸ºäº†éªŒè¯æœ¬æ–‡è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡åœ¨å…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„é™„åŠ åå…‰åœºæ™¯ä¸­æ•è·äº†åæŒ¯ä¿¡æ¯å’ŒçœŸå®ç½‘æ ¼ã€‚æœ¬æ–‡è¿˜åœ¨ PANDORA æ•°æ®é›†ä¸Šè¯„ä¼°äº†æœ¬æ–‡çš„æ¡†æ¶ã€‚æ¯”è¾ƒç»“æœè¯æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨åå…‰åœºæ™¯ä¸­æ¯”ç°æœ‰çš„ç¥ç» 3D é‡å»ºæ–¹æ³•æ€§èƒ½é«˜å‡ºå¾ˆå¤šã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åœ¨SDFåŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºï¼›(2): åæŒ¯å…ˆéªŒå¼•å¯¼é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼›(3): åŠ æƒç­–ç•¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºå’ŒåæŒ¯å…ˆéªŒæŒ‡å¯¼é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶å­¦ä¹ çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†åå…‰åœºæ™¯çš„ç¥ç»3Dé‡å»ºç²¾åº¦ã€‚ï¼ˆ2ï¼‰ åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºåœ¨ SDF åŸŸä¸­åŸºäºé«˜æ–¯çš„æ³•çº¿è¡¨ç¤ºï¼Œå¢å¼ºäº†å¯¹åå…‰è¡¨é¢çš„å‡ ä½•ç»†èŠ‚æ•æ‰èƒ½åŠ›ã€‚</li><li>å¼•å…¥åæŒ¯å…ˆéªŒç›‘ç£é•œé¢åå°„åé¢å‡ ä½•å½¢çŠ¶çš„å­¦ä¹ ï¼Œæå‡äº†å¯¹é•œé¢åŒºåŸŸçš„é‡å»ºç²¾åº¦ã€‚</li><li>æå‡ºåŠ æƒç­–ç•¥å‡è½»åæŒ¯å…ˆéªŒçš„å™ªå£°é—®é¢˜ï¼Œæé«˜äº†é‡å»ºç»“æœçš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨é™„åŠ çš„åå…‰åœºæ™¯å’Œ PANDORA æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•æ¯”ç°æœ‰ç¥ç» 3D é‡å»ºæ–¹æ³•æ€§èƒ½é«˜å‡ºå¾ˆå¤šã€‚å·¥ä½œé‡ï¼š</li><li>éœ€æ”¶é›†å…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„é™„åŠ åå…‰åœºæ™¯ï¼Œå¹¶æ•è·åæŒ¯ä¿¡æ¯å’ŒçœŸå®ç½‘æ ¼ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5138f0fe3311b978fd9b5ec37a322939.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5936420b4b2a0b5300107e96f5e8d63b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7069368a6fc8cfec8154ca17598f1a7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b97ad958b53ebbfba59c1661ac76466d.jpg" align="middle"></details><h2 id="Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><a href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging" class="headerlink" title="Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging"></a>Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging</h2><p><strong>Authors:Mert Ã–zer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</strong></p><p>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a href="https://mert-o.github.io/ThermalNeRF/">https://mert-o.github.io/ThermalNeRF/</a>. </p><p><a href="http://arxiv.org/abs/2403.11865v1">PDF</a> 24 pages, 14 figures</p><p><strong>Summary</strong><br>NeRFs ç»“åˆç¬¬äºŒç§æ¨¡æ€ï¼ˆå¦‚çƒ­å›¾åƒï¼‰çš„æœ€ä½³ç­–ç•¥æ˜¯æ·»åŠ ä¸€ä¸ªåˆ†æ”¯æ¥é¢„æµ‹è¯¥æ¨¡æ€çš„å€¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs å·²æˆä¸ºåˆ©ç”¨ RGB å›¾åƒè¿›è¡Œæ–°å‹è§†å›¾åˆæˆçš„äº‹å®æ ‡å‡†ã€‚</li><li>æå‡ºå››ç§åœ¨ NeRFs ä¸­çº³å…¥ç¬¬äºŒç§æ¨¡æ€ï¼ˆå¦‚çƒ­å›¾åƒï¼‰çš„ç­–ç•¥ã€‚</li><li>ä¸ºè¯„ä¼°è¿™äº›ç­–ç•¥ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„å…¬å¼€å¯ç”¨çš„å¤šè§†å›¾æ•°æ®é›† ThermalMixã€‚</li><li>çƒ­å›¾åƒå’Œ RGB å›¾åƒç»è¿‡äº¤å‰æ¨¡æ€æ ¡å‡†ï¼Œå®ç°äº†é«˜è´¨é‡çš„å¯¹é½ã€‚</li><li>å¯¹äºçƒ­å›¾åƒçš„æ–°å‹è§†å›¾åˆæˆï¼Œåœ¨ NeRF ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯çš„æ€§èƒ½æœ€ä½³ï¼ŒåŒæ—¶åœ¨ RGB ä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚</li><li>åˆ†æç»“æœå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡æ€ï¼ŒåŒ…æ‹¬è¿‘çº¢å¤–å›¾åƒå’Œæ·±åº¦å›¾ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://mert-o.github.io/ThermalNeRF/ã€‚">https://mert-o.github.io/ThermalNeRF/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæ¢ç´¢å¤šæ¨¡æ€ç¥ç»åœºæ™¯è¡¨ç¤ºåŠå…¶åœ¨çƒ­æˆåƒä¸­çš„åº”ç”¨â€”â€”è¡¥å……ææ–™</li><li>Authors: Mert Ã–zer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</li><li>Affiliation: Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg</li><li>Keywords: Multi-modal Learning Â· NeRF Â· Thermal Imaging</li><li>Urls: Paper: https://arxiv.org/abs/2204.04678, Github: None</li><li><p>æ‘˜è¦ï¼š(1): ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å·²è¿…é€Ÿæˆä¸ºåŸºäº RGB å›¾åƒé›†è¿›è¡Œæ–°è§†è§’åˆæˆä»»åŠ¡çš„äº‹å®æ ‡å‡†ã€‚æœ¬æ–‡å¯¹ç¥ç»åœºæ™¯è¡¨ç¤ºï¼ˆå¦‚ NeRFsï¼‰åœ¨å¤šæ¨¡æ€å­¦ä¹ èƒŒæ™¯ä¸‹çš„ç»¼åˆè¯„ä¼°ã€‚(2): è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šæœ¬æ–‡æå‡ºäº†å››ç§ä¸åŒçš„ç­–ç•¥ï¼Œå°† RGB ä»¥å¤–çš„ç¬¬äºŒç§æ¨¡æ€èå…¥ NeRFsï¼šä»å¤´å¼€å§‹ç‹¬ç«‹è®­ç»ƒä¸¤ç§æ¨¡æ€ï¼›åœ¨ RGB ä¸Šé¢„è®­ç»ƒå¹¶åœ¨ç¬¬äºŒç§æ¨¡æ€ä¸Šå¾®è°ƒï¼›æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯ï¼›æ·»åŠ ä¸€ä¸ªå•ç‹¬çš„ç»„ä»¶æ¥é¢„æµ‹é™„åŠ æ¨¡æ€çš„ï¼ˆé¢œè‰²ï¼‰å€¼ã€‚é€‰æ‹©çƒ­æˆåƒä½œä¸ºç¬¬äºŒç§æ¨¡æ€ï¼Œå› ä¸ºå®ƒåœ¨è¾å°„åº¦æ–¹é¢ä¸ RGB æœ‰å¾ˆå¤§ä¸åŒï¼Œéš¾ä»¥é›†æˆåˆ°ç¥ç»åœºæ™¯è¡¨ç¤ºä¸­ã€‚(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„ç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡é›†äº†ä¸€ä¸ªæ–°çš„å…¬å¼€çš„å¤šè§†è§’æ•°æ®é›† ThermalMixï¼Œå…¶ä¸­åŒ…å«å…­ä¸ªå¸¸è§ç‰©ä½“ï¼Œæ€»å…±çº¦ 360 å¼  RGB å’Œçƒ­å›¾åƒã€‚åœ¨æ•°æ®é‡‡é›†ä¹‹å‰ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è·¨æ¨¡æ€æ ¡å‡†ï¼Œä»è€Œå®ç°äº† RGB å’Œçƒ­å›¾åƒä¹‹é—´çš„é«˜è´¨é‡å¯¹é½ã€‚(4): æœ¬æ–‡æ–¹æ³•åœ¨ä½•ç§ä»»åŠ¡ä¸Šå–å¾—äº†ä½•ç§æ€§èƒ½ï¼Œè¯¥æ€§èƒ½æ˜¯å¦èƒ½æ”¯æ’‘å…¶ç›®æ ‡ï¼šæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸º NeRF æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ï¼ŒåŒæ—¶åœ¨ RGB ä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆ†æå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–æ¨¡æ€ï¼ŒåŒ…æ‹¬è¿‘çº¢å¤–å›¾åƒå’Œæ·±åº¦å›¾ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ä»å¤´å¼€å§‹è®­ç»ƒï¼šåˆ†åˆ«è®­ç»ƒ RGB å’Œç¬¬äºŒç§æ¨¡æ€çš„æ¨¡å‹ã€‚(2) å¾®è°ƒï¼šå…ˆåœ¨ RGB ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå†åœ¨ç¬¬äºŒç§æ¨¡æ€ä¸Šå¾®è°ƒã€‚(3) æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯ï¼šåœ¨æ¨¡å‹ä¸­æ·»åŠ ä¸€ä¸ªåˆ†æ”¯æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ã€‚(4) æ·»åŠ å•ç‹¬ç»„ä»¶ï¼šæ·»åŠ ä¸€ä¸ªå•ç‹¬çš„ç»„ä»¶æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ï¼Œä½†ä»…åœ¨è®­ç»ƒæœŸé—´å°†åå‘ä¼ æ’­é™åˆ¶åœ¨å¯†åº¦ç½‘ç»œä¸­ã€‚</p></li><li><p>ç»“è®ºï¼š(1) æœ¬å·¥ä½œçš„æ„ä¹‰ï¼šæœ¬æ–‡å¯¹ç¥ç»åœºæ™¯è¡¨ç¤ºåœ¨å¤šæ¨¡æ€å­¦ä¹ èƒŒæ™¯ä¸‹çš„ç»¼åˆè¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§åœ¨çƒ­æˆåƒä¸­ä½¿ç”¨ç¥ç»è¾å°„åœºçš„æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ã€‚(2) æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ç»´åº¦ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ä¸º NeRF æ·»åŠ ç¬¬äºŒä¸ªåˆ†æ”¯çš„æ–°ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­è¡¨ç°æœ€ä½³ã€‚æ€§èƒ½ï¼šåœ¨ ThermalMix æ•°æ®é›†ä¸Šï¼Œè¯¥ç­–ç•¥åœ¨çƒ­å›¾åƒçš„æ–°è§†è§’åˆæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ RGB å›¾åƒä¸Šä¹Ÿäº§ç”Ÿäº†ä»¤äººä¿¡æœçš„ç»“æœã€‚å·¥ä½œé‡ï¼šè¯¥ç­–ç•¥éœ€è¦é¢å¤–çš„åˆ†æ”¯æ¥é¢„æµ‹ç¬¬äºŒç§æ¨¡æ€çš„å€¼ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒæ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-83a2cb8ec7e3ac021d25364307db79b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7a99b2c940d1db6b8fd17ab54ec3367.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-657899b5bde6ff107fbb38ac98bf6cf9.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>Summary</strong></p><p>é«˜æ–¯çƒé¢æ˜¾æ€§è¡¨ç¤ºæ³•å…‹æœç¥ç»æ¸²æŸ“å¼Šç«¯ï¼Œå¤„ç†æ¨¡ç³Šå›¾åƒå’Œç›¸æœºä½å§¿ä¸å‡†ç¡®ï¼Œå®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»æ¸²æŸ“é«˜åº¦ä¾èµ–é«˜è´¨é‡å›¾åƒå’Œç²¾ç¡®ç›¸æœºä½å§¿ï¼Œéš¾ä»¥å¤„ç†æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>3Dé«˜æ–¯çƒé¢æ˜¾æ€§è¡¨ç¤ºæ³•é€šè¿‡ä¼˜åŒ–é«˜æ–¯çƒä½“ç‚¹äº‘ï¼Œå®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussiansæ–¹æ³•ç»“åˆæ˜¾æ€§é«˜æ–¯è¡¨ç¤ºå’Œç‰©ç†æˆåƒæ¨¡å‹ï¼Œå¤„ç†æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>BAD-Gaussiansåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¹¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussiansé€šè¿‡è”åˆä¼˜åŒ–é«˜æ–¯çƒä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œæ¢å¤æ¨¡ç³Šå›¾åƒç»†èŠ‚ã€‚</li><li>BAD-Gaussiansä»¥é«˜æ–¯çƒé¢ä¸ºåª’ä»‹ï¼Œå°†éšå¼ç¥ç»è¡¨ç¤ºå’Œæ˜¾å¼å‡ ä½•è¡¨ç¤ºç›¸ç»“åˆã€‚</li><li>BAD-Gaussiansçš„é¡¹ç›®ä¸»é¡µå’Œæºä»£ç å·²å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šBAD-Gaussiansï¼šåŸºäºå…‰æŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“ç»˜åˆ¶</li><li>ä½œè€…ï¼šLingzhe Zhao, Peng Wang, Peidong Liu</li><li>å•ä½ï¼šNone</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“ç»˜åˆ¶ Â· å»æ¨¡ç³Š Â· å…‰æŸè°ƒæ•´ Â· å¯å¾®æ¸²æŸ“</li><li>é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆæ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„æ¸…æ™°å›¾åƒå’Œå‡†ç¡®çš„ç›¸æœºä½å§¿ã€‚ï¼ˆ2ï¼‰ è¿‡å»çš„æ–¹æ³•ï¼šä¸ºä½¿ç”¨è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼ˆåœ¨ç°å®åœºæ™¯ä¸­å¸¸è§ï¼Œå¦‚ä½å…‰æˆ–é•¿æ›å…‰æ¡ä»¶ä¸‹ï¼‰è®­ç»ƒç¥ç»è¾å°„åœº (NeRF) å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•ã€‚ç„¶è€Œï¼ŒNeRF çš„éšå¼è¡¨ç¤ºéš¾ä»¥ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å‡†ç¡®æ¢å¤å¤æ‚ç»†èŠ‚ï¼Œå¹¶ä¸”æ— æ³•å®ç°å®æ—¶æ¸²æŸ“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3D é«˜æ–¯ä½“ç»˜åˆ¶çš„æœ€æ–°è¿›å±•é€šè¿‡å°†ç‚¹äº‘æ˜¾å¼ä¼˜åŒ–ä¸º 3D é«˜æ–¯ä½“ï¼Œå®ç°äº†é«˜è´¨é‡çš„ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ3ï¼‰ æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º BAD-Gaussiansï¼ˆåŸºäºå…‰æŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“ç»˜åˆ¶ï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ˜¾å¼é«˜æ–¯è¡¨ç¤ºå¹¶å¤„ç†å…·æœ‰å‡†ç¡®ç›¸æœºä½å§¿çš„ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä»¥å®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†å›¾åƒå½¢æˆè¿‡ç¨‹ï¼Œå¹¶åœ¨æ›å…‰æ—¶é—´å†…è”åˆå­¦ä¹ é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚ï¼ˆ4ï¼‰ æ–¹æ³•æ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼Œä¸åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æœ€æ–°å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼ŒBAD-Gaussians ä¸ä»…å®ç°äº†å“è¶Šçš„æ¸²æŸ“è´¨é‡ï¼Œè¿˜å®ç°äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹ï¼Œå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè¿›è¡Œå»ºæ¨¡ï¼Œå°†å›¾åƒè¡¨ç¤ºä¸ºä¸€ç³»åˆ—è™šæ‹Ÿçš„æ¸…æ™°å›¾åƒçš„ç§¯åˆ†ï¼›ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ 3D é«˜æ–¯ä½“ç»˜åˆ¶æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ— 3D é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹æ¥æ¢å¤æ¸…æ™°çš„ 3D åœºæ™¯è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨åŸºäºå…‰æŸè°ƒæ•´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œè”åˆä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œä»¥æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’ŒåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹åˆæˆçš„æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œé¦–æ¬¡æå‡ºäº†ä¸€ä¸ªç®¡é“ï¼Œå¯ä»¥ä»ä¸€ç»„å…·æœ‰å‡†ç¡®ç›¸æœºä½å§¿çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å­¦ä¹ é«˜æ–¯ä½“ç»˜åˆ¶ã€‚æˆ‘ä»¬çš„ç®¡é“å¯ä»¥è”åˆä¼˜åŒ– 3D åœºæ™¯è¡¨ç¤ºå’Œç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„ä½œå“ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°å®æ—¶æ¸²æŸ“ã€‚</li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒå»ºæ¨¡æ–¹æ³•ï¼Œå°†å›¾åƒè¡¨ç¤ºä¸ºä¸€ç³»åˆ—è™šæ‹Ÿæ¸…æ™°å›¾åƒçš„ç§¯åˆ†ã€‚* åˆ©ç”¨ 3D é«˜æ–¯ä½“ç»˜åˆ¶æ¡†æ¶ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ— 3D é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œæ¢å¤ç›¸æœºè¿åŠ¨è½¨è¿¹æ¥æ¢å¤æ¸…æ™°çš„ 3D åœºæ™¯è¡¨ç¤ºã€‚* é‡‡ç”¨åŸºäºå…‰æŸè°ƒæ•´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œè”åˆä¼˜åŒ–é«˜æ–¯ä½“å‚æ•°å’Œç›¸æœºè¿åŠ¨è½¨è¿¹ï¼Œä»¥æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’ŒåŸºäºç‰©ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹åˆæˆçš„æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®ã€‚</p><p>æ€§èƒ½ï¼š* åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šï¼Œä¸æœ€æ–°å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼ŒBAD-Gaussians ä¸ä»…å®ç°äº†å“è¶Šçš„æ¸²æŸ“è´¨é‡ï¼Œè¿˜å®ç°äº†å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚</p><p>å·¥ä½œé‡ï¼š* è¯¥æ–¹æ³•éœ€è¦å‡†ç¡®çš„ç›¸æœºä½å§¿ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½éš¾ä»¥è·å¾—ã€‚* è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œèµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details>## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from   Aerial Imagery**Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui**We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness. [PDF](http://arxiv.org/abs/2403.11812v1) CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/**Summary**åˆ©ç”¨ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œå°†å™ªå£°è¾ƒå¤§çš„ 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡è¯­ä¹‰å’Œå»ºç­‘ç‰©çº§å®ä¾‹åˆ†å‰²ã€‚**Key Takeaways**- å¼•å…¥äº†å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œå¢å¼ºäº†ä¸åŒå¤§å°ç‰©ä½“çš„åˆ†å‰²æ•ˆæœã€‚- æå‡ºäº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„æ–°å‹è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œä»¥å‡è½» 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†å›¾ä¸ä¸€è‡´é—®é¢˜ã€‚- åˆ©ç”¨å¤šè§†å›¾é‡å»ºæ·±åº¦å…ˆéªŒæ”¹å–„äº†é‡å»ºè¾å°„åœºçš„å‡ ä½•è´¨é‡ï¼Œä»è€Œå¢å¼ºäº†åˆ†å‰²æ•ˆæœã€‚- åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚- è¯¥æ–¹æ³•åœ¨å¤„ç†åŸå¸‚èˆªç©ºå›¾åƒä¸­ç‰©ä½“å°ºå¯¸å·®å¼‚å’Œå¤šè§†å›¾ä¸ä¸€è‡´æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚- è¯¥æ–¹æ³•åˆ©ç”¨äº† NeRF æ–°é¢–çš„è§†å›¾åˆæˆèƒ½åŠ›ï¼Œå°† 2D æ ‡ç­¾æå‡åˆ° 3Dã€‚- é€šè¿‡è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜ 2D å®ä¾‹åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šAerialLiftingï¼šç¥ç»åŸå¸‚è¯­ä¹‰å’Œå»ºç­‘å®ä¾‹æå‡</li><li>ä½œè€…ï¼šZeqiang Zhang, Weihao Zhao, Yihan Hu, Chengming Zhang, Changqing Zhang, Xinyu Zhou</li><li>å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ã€åŸå¸‚åœºæ™¯</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šhttps://github.com/zyqz97/Aeriallifting</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŸå¸‚èˆªç©ºå›¾åƒè¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦åŸå› åœ¨äºå¯¹è±¡å°ºå¯¸å·®å¼‚å¤§ï¼Œä»¥åŠç°æœ‰åˆ†å‰²æ–¹æ³•äº§ç”Ÿçš„ 2D æ ‡ç­¾å­˜åœ¨å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»æ–¹æ³•ä¸»è¦ä½¿ç”¨ 2D åˆ†å‰²ç½‘ç»œè¿›è¡Œåˆ†å‰²ï¼Œä½†éš¾ä»¥å¤„ç†å°ºå¯¸å·®å¼‚å¤§çš„å¯¹è±¡ã€‚æ­¤å¤–ï¼Œç”±äºèˆªç©ºå›¾åƒä»…èƒ½æ•æ‰åˆ°åœºæ™¯çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤ 2D æ ‡ç­¾å­˜åœ¨å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œé€šè¿‡å°†å™ªå£° 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡çš„è¯­ä¹‰å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œé€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾æ¥å¢å¼ºä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†ç‚¹å®ä¾‹æ ‡ç­¾åˆ†ç»„ï¼Œä»¥å‡è½» 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼šæœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æå‡ºç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œå°†å™ªå£°2Dæ ‡ç­¾æå‡åˆ°3Dï¼Œå®ç°åŸå¸‚è§„æ¨¡è¯­ä¹‰å’Œå»ºç­‘å®ä¾‹åˆ†å‰²ï¼›ï¼ˆ2ï¼‰æå‡ºå°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œå¢å¼ºä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²ï¼›ï¼ˆ3ï¼‰æå‡ºåŸºäº3Dåœºæ™¯è¡¨ç¤ºçš„è·¨è§†ç‚¹å®ä¾‹æ ‡ç­¾åˆ†ç»„ï¼Œå‡è½»2Då®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†ç‚¹ä¸ä¸€è‡´é—®é¢˜ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºä»èˆªç©ºå›¾åƒä¸­è¿›è¡ŒåŸå¸‚è§„æ¨¡çš„è¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ï¼Œè¯¥æ–¹æ³•å°†å™ªå£° 2D æ ‡ç­¾æå‡åˆ° 3Dï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²æ•ˆæœã€‚ä¸ºäº†å®ç°å»ºç­‘å®ä¾‹åˆ†å‰²çš„å¤šè§†å›¾ä¸€è‡´å®ä¾‹ç›‘ç£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆå¤šè§†å›¾ç«‹ä½“ä¸­çš„æ·±åº¦å…ˆéªŒæ¥å¢å¼ºé‡å»ºçš„å‡ ä½•å½¢çŠ¶ï¼Œä»è€Œè·å¾—æ›´å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œåœºæ™¯ä¸Šçš„å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºä»èˆªç©ºå›¾åƒä¸­è¿›è¡ŒåŸå¸‚è§„æ¨¡çš„è¯­ä¹‰åˆ†å‰²å’Œå»ºç­‘çº§åˆ«å®ä¾‹åˆ†å‰²ã€‚</li><li>æå‡ºäº†ä¸€ç§å°ºåº¦è‡ªé€‚åº”è¯­ä¹‰æ ‡ç­¾èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç»“åˆä¸åŒé«˜åº¦é¢„æµ‹çš„æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒå°ºå¯¸å¯¹è±¡çš„åˆ†å‰²æ•ˆæœã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäº 3D åœºæ™¯è¡¨ç¤ºçš„è·¨è§†å›¾å®ä¾‹æ ‡ç­¾åˆ†ç»„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å‡è½»äº† 2D å®ä¾‹æ ‡ç­¾ä¸­çš„å¤šè§†å›¾ä¸ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸå¸‚è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ä½¿ç”¨ç¥ç»è¾å°„åœºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d373e1e7a39d9775dfc8d02b9486a782.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7cbb4392e69c2035b7c92cb075d39669.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c7e217526cc2d8a70dcb24a447f989.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dc37cedfadba8328b4c6a52c7062fea6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a57894a5286875745a4beeab02d003.jpg" align="middle"></details><h2 id="Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><a href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem" class="headerlink" title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem"></a>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem</h2><p><strong>Authors:Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</strong></p><p>Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication. </p><p><a href="http://arxiv.org/abs/2403.11573v2">PDF</a> 28 pages, 12 figures, 11 tables</p><p><strong>Summary</strong><br>åŸºäºè§†é¢‘ä¼ªæ¿€å…‰ç‚¹äº‘è¿›è¡Œé•¿å°¾ç±»å°‘æ ·æœ¬3Dç‰©ä½“æ£€æµ‹</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨è§†é¢‘ç”Ÿæˆä¼ªæ¿€å…‰ç‚¹äº‘æ¥è§£å†³é•¿å°¾ç±»ç‰©ä½“æ£€æµ‹ä¸­çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li><li>ä¼ªæ¿€å…‰ç‚¹äº‘é€šè¿‡2D-3Dè§†å›¾åˆæˆæ¨¡å‹ç”Ÿæˆï¼Œæˆæœ¬è¾ƒä½ã€‚</li><li>ä½¿ç”¨LiDARå¼ºåº¦ä¼°è®¡å®ç°ç‰©ä½“çº§åŸŸå¯¹é½ã€‚</li><li>æå‡ºä¸€ç§æ··åˆçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ”¾ç½®æ–¹æ³•ï¼Œèåˆåœ°é¢å’Œåœ°å›¾ä¿¡æ¯ã€‚</li><li>åœ¨nuScenesã€KITTIå’ŒLyftç­‰åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ€§èƒ½æå‡ï¼Œå°¤å…¶é€‚ç”¨äºä¸åŒLiDARé…ç½®æ•°æ®é›†ã€‚</li><li>ä»£ç å’Œæ•°æ®å°†åœ¨å…¬å¼€å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåªéœ€å†åŠ  100 ç¾å…ƒï¼šå¢å¼ºåŸºäº NeRF çš„è¡¥å……ææ–™</li><li>ä½œè€…ï¼šYuxuan Zhangã€Xuan Gaoã€Zexiang Xuã€Shenghua Gao</li><li>æ‰€å±å•ä½ï¼šåŒ—äº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFã€ä¼ªåœ°é¢çœŸå€¼å¢å¼ºã€ä¸‰ç»´é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.01818</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§å¼ºå¤§çš„ä¸‰ç»´é‡å»ºæŠ€æœ¯ï¼Œä½†å…¶é‡å»ºè´¨é‡å—é™äºè®­ç»ƒæ•°æ®çš„æ•°é‡å’Œè´¨é‡ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºä½¿ç”¨åˆæˆæ•°æ®æˆ–æœ‰é™çš„çœŸå®ä¸–ç•Œæ•°æ®æ¥å¢å¼º NeRFï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€æˆæœ¬é«˜æ˜‚æˆ–æ•ˆæœæœ‰é™ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä»·å€¼çº¦ 100 ç¾å…ƒçš„å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ã€‚(4) å®éªŒç»“æœï¼šåœ¨æ±½è½¦é‡å»ºä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº† NeRF çš„é‡å»ºè´¨é‡ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)é€šè¿‡æ”¶é›†è§†é¢‘å¸§å’Œä½¿ç”¨åŸºäºNeRFçš„æ–¹æ³•é‡å»ºä¸‰ç»´ä½“ç§¯è¡¨ç¤ºï¼Œæ”¶é›†ä¸‰ç»´å¯¹è±¡å®ä¾‹ï¼›(2)é€šè¿‡ç©ºé—´ç‚¹é‡æ–°æ’åˆ—å’ŒåŸºäºCycleGANçš„å¼ºåº¦ä¼°è®¡å™¨å¯¹RGBç‚¹äº‘è¿›è¡Œåå¤„ç†ï¼Œè¿›è¡Œå¯¹è±¡çº§åŸŸå¯¹é½ï¼›(3)åŸºäºåœ°é¢å’Œåœ°å›¾çš„æ··åˆä¿¡æ¯ï¼Œå°†é‡‡æ ·çš„å¯¹è±¡ç²˜è´´åˆ°ç›®æ ‡åœºæ™¯ä¸­ï¼Œè¿›è¡Œä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘å¢å¼ºã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½æˆæœ¬ä¸”æœ‰æ•ˆçš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³ 3D ç›®æ ‡æ£€æµ‹ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚é€šè¿‡ä»å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘ä¸­ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº† NeRF çš„é‡å»ºè´¨é‡ï¼Œåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä½æˆæœ¬çš„ä¼ªåœ°é¢çœŸå€¼å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä»·å€¼çº¦ 100 ç¾å…ƒçš„å¾®ç¼©æ¨¡å‹å’Œç½‘ç»œçˆ¬è™«æ”¶é›†çš„å…¬å…±è§†é¢‘æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¡¥å……ææ–™ã€‚</li><li>å¼€å‘äº†ä¸€ç§åŸºäºç©ºé—´ç‚¹é‡æ–°æ’åˆ—å’ŒåŸºäº CycleGAN çš„å¼ºåº¦ä¼°è®¡å™¨çš„å¯¹è±¡çº§åŸŸå¯¹é½æ–¹æ³•ï¼Œä»¥å¢å¼ºä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„çœŸå®æ„Ÿã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºåœ°é¢å’Œåœ°å›¾çš„æ··åˆä¿¡æ¯çš„æ–¹æ³•ï¼Œå°†é‡‡æ ·çš„å¯¹è±¡ç²˜è´´åˆ°ç›®æ ‡åœºæ™¯ä¸­ï¼Œä»¥å¢å¼ºä¼ªæ¿€å…‰é›·è¾¾ç‚¹äº‘çš„ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ nuScenesã€KITTI å’Œ Lyft æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒéªŒè¯äº† PGT-Aug çš„æœ‰æ•ˆæ€§å’Œä¸å„ç§ 3D ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„å…¼å®¹æ€§ï¼Œå¹¶åœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„ 3D ç›®æ ‡æ£€æµ‹ç®¡é“é›†æˆã€‚</li><li>ä¼ªåœ°é¢çœŸå€¼å¢å¼ºè¿‡ç¨‹æ˜¯ç¦»çº¿çš„ï¼Œä¸ä¼šå¢åŠ åœ¨çº¿æ¨ç†çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-0006e417851072d027a7080ed002cd3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e56111347c95caf4a3778eb931c65ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a143ef2a7e6a934315f648ed4c97b784.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee8c2259d653f0bf8c6e34bd495ccc8d.jpg" align="middle"></details><h2 id="SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><a href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream" class="headerlink" title="SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream"></a>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</h2><p><strong>Authors:Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang</strong></p><p>Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRFâ€™s multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at <a href="https://github.com/BIT-Vision/SpikeNeRF">https://github.com/BIT-Vision/SpikeNeRF</a>. </p><p><a href="http://arxiv.org/abs/2403.11222v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>SpikeNeRFé¦–æ¬¡åŸºäºè„‰å†²ç¥ç»å…ƒæ•°æ®æ„å»ºäº†ç¥ç»è¾å°„åœºä½“ç§¯åœºæ™¯è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°ä»æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­è·å–è¿è´¯ç»“æ„ï¼Œå³ä½¿åœ¨ç…§æ˜æ¡ä»¶å·®å¼‚çš„æƒ…å†µä¸‹ä¹Ÿèƒ½äº§ç”ŸçœŸå®æ„Ÿçš„æ–°è§†å›¾ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è„‰å†²ç›¸æœºä¸æ ‡å‡†ç›¸æœºç›¸æ¯”å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¦‚è„‰å†²ç§¯åˆ†é‡‡æ ·å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡ã€‚</li><li>SpikeNeRFä»è„‰å†²ç›¸æœºæ•°æ®æ´¾ç”ŸåŸºäºNeRFçš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚</li><li>NeRFçš„å¤šè§†å›¾ä¸€è‡´æ€§å¯å»ºç«‹ç¨³å¥çš„è‡ªç›‘ç£ï¼Œæ¶ˆé™¤é”™è¯¯æµ‹é‡å¹¶æ­ç¤ºå™ªå£°è¾“å…¥ä¸­çš„è¿è´¯ç»“æ„ã€‚</li><li>SpikeNeRFåŒ…å«ä¸€ä¸ªè„‰å†²ç”Ÿæˆæ¨¡å‹ï¼ˆå…·æœ‰ç§¯åˆ†-æ¿€å‘ç¥ç»å…ƒå±‚ï¼‰å’Œä¸€ä¸ªè„‰å†²æ¸²æŸ“æŸå¤±ï¼ˆå¯æ¨å¹¿åˆ°ä¸åŒçš„ç…§æ˜æ¡ä»¶ï¼‰ã€‚</li><li>SpikeNeRFä¼˜åŒ–ç¥ç»è¾å°„åœºï¼Œä»æ–°è¿ç»­è„‰å†²æµæ¸²æŸ“é€¼çœŸçš„æ–°è§†å›¾ã€‚</li><li>SpikeNeRFåœ¨çœŸå®å’Œæ–°é¢–çš„çœŸå®æ¨¡æ‹Ÿåºåˆ—ä¸Šè¿›è¡Œäº†ç»éªŒè¯„ä¼°ï¼Œå¹¶è¯å®äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>SpikeNeRFçš„æ•°æ®é›†å’Œæºä»£ç å·²åœ¨ GitHub ä¸Šå‘å¸ƒï¼š<a href="https://github.com/BIT-Vision/SpikeNeRFã€‚">https://github.com/BIT-Vision/SpikeNeRFã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMLP çš„æœ€åä¸€å±‚</li><li>ä½œè€…ï¼šJinpeng Dong, Xinyu Gong, Jiawei Chen, Xiaohui Shen, Jiaya Jia</li><li>éš¶å±ï¼šåŒ—äº¬ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°–å³°ç›¸æœºã€ç¥ç»åœºæ™¯æµåœºã€å°–å³°æ¸²æŸ“æŸå¤±</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.00483ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå°–å³°ç›¸æœºç”±äºå…¶åŸºäºå°–å³°çš„ç§¯åˆ†é‡‡æ ·å’Œé«˜æ—¶é—´åˆ†è¾¨ç‡è€Œå…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œä½†ç°æœ‰åŸºäºå°–å³°ç›¸æœºçš„æ–¹æ³•é€šå¸¸å‡è®¾ç…§æ˜æ¡ä»¶ç†æƒ³ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å¹¶ä¸å¸¸è§ã€‚(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘å°–å³°ç›¸æœºæ•°æ®ä¸­çš„å™ªå£°å’Œå…‰ç…§å˜åŒ–ï¼Œå¯¼è‡´åœ¨å¤æ‚ç…§æ˜æ¡ä»¶ä¸‹ç”Ÿæˆçš„æ–°è§†å›¾è´¨é‡è¾ƒå·®ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ NeRF çš„å¤šè§†å›¾ä¸€è‡´æ€§å»ºç«‹é²æ£’çš„è‡ªç›‘ç£ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é”™è¯¯æµ‹é‡ï¼Œå¹¶åœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºäº†å…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒå…ƒç´ ï¼šä¸€ä¸ªåŒ…å«ç§¯åˆ†æ”¾ç”µç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§ï¼ˆä¾‹å¦‚é˜ˆå€¼å˜åŒ–ï¼‰çš„å‚æ•°çš„å°–å³°ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªèƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–çš„å°–å³°æ¸²æŸ“æŸå¤±ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨çœŸå®å’Œæ–°é¢–çš„ç°å®æ¨¡æ‹Ÿåºåˆ—ä¸Šè¿›è¡Œçš„å®è¯è¯„ä¼°è¯å®äº†æœ¬æ–‡æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•åœ¨æŸäº›åœºæ™¯ä¸­å±•ç¤ºäº†ä¼˜äºå…¶ä»–è§†è§‰ä¼ æ„Ÿå™¨çš„ä¼˜åŠ¿ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): SpikeNeRF é‡‡ç”¨åŸºäºè„‰å†²çš„ç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§çš„å‚æ•°çš„è„‰å†²ç”Ÿæˆæ¨¡å‹ï¼Œä»è„‰å†²ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºã€‚(2): SpikeNeRF æå‡ºäº†ä¸€ç§è„‰å†²æ¸²æŸ“æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–ã€‚(3): SpikeNeRF ç»“åˆäº†è„‰å†²ç”Ÿæˆæ¨¡å‹å’Œè„‰å†²æ¸²æŸ“æŸå¤±ï¼Œåœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºäº†å…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚SpikeNeRF ä»¥çº¯åŸºäºå°–å³°çš„ç›‘ç£ä¸ºé‡ç‚¹ï¼Œåœ¨é«˜æ—¶é—´åˆ†è¾¨ç‡ä¸‹ä¿ç•™çº¹ç†å’Œè¿åŠ¨ç»†èŠ‚ï¼Œè§£å†³äº†ä¸ç°å®ä¸–ç•Œå°–å³°åºåˆ—ç›¸å…³çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°æ•´ç†çš„åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº† SpikeNeRF åœ¨æ–°è§†å›¾åˆæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºé‡‡ç”¨æ–°é¢–å°–å³°æµæŠ€æœ¯çš„é«˜è´¨é‡ 3D è¡¨ç¤ºå­¦ä¹ ç ”ç©¶æä¾›å¯ç¤ºã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º SpikeNeRFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä»å°–å³°ç›¸æœºæ•°æ®ä¸­æ¨å¯¼å‡ºåŸºäº NeRF çš„ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ï¼›è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ç§¯åˆ†æ”¾ç”µç¥ç»å…ƒå±‚å’Œè€ƒè™‘éç†æƒ³æ€§çš„å‚æ•°çš„å°–å³°ç”Ÿæˆæ¨¡å‹ï¼›æå‡ºäº†ä¸€ç§èƒ½å¤Ÿåœ¨ä¸åŒç…§æ˜æ¡ä»¶ä¸‹æ³›åŒ–çš„å°–å³°æ¸²æŸ“æŸå¤±ã€‚æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—ä¸Šè¿›è¡Œçš„å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼ŒSpikeNeRF åœ¨æ–°è§†å›¾åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›SpikeNeRF èƒ½å¤Ÿåœ¨æåº¦å˜ˆæ‚çš„è¾“å…¥ä¸­æ­ç¤ºå…·æœ‰é«˜åº¦å™ªå£°çš„çœŸå®ä¸–ç•Œç…§æ˜åœºæ™¯ä¸­çš„ä¸€è‡´ç»“æ„ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ°å°–å³°ç›¸æœºæ•°æ®å»ºæ¨¡ã€NeRF æ¨¡å‹æ”¹è¿›å’Œå°–å³°æ¸²æŸ“æŸå¤±è®¾è®¡ç­‰å¤šä¸ªæ–¹é¢ï¼›éœ€è¦æ”¶é›†å’Œæ•´ç†åˆæˆå’ŒçœŸå®å°–å³°åºåˆ—æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œå¤§é‡çš„å®éªŒè¯„ä¼°ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9ba06183314a903c555e4ddc4fcaeacc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b749007c4db9047d920aff30a0b518f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-144a8d69d104c83fa694f502001776ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c3e04e0edb81b8f76c6c69254f4f30.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7501801f80901eb4305db983691d7456.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25b5018bec2967c40c51be7fdffbc6c6.jpg" align="middle"></details><h2 id="Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><a href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications" class="headerlink" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications"></a>Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications</h2><p><strong>Authors:Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin</strong></p><p>Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing. </p><p><a href="http://arxiv.org/abs/2403.11131v1">PDF</a> </p><p><strong>Summary</strong><br>å…¨æ™¯é‡å»ºï¼šä¸€ä¸ªé€šç”¨çš„ç¥ç»è¾å°„åœºæ¨¡å‹ï¼Œå®ç°å¤šä»»åŠ¡åœºæ™¯ç†è§£å’Œ 3D åº”ç”¨è‡ªé€‚åº”ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼€å‘é€šç”¨ NeRF æ¨¡å‹ï¼Œé€‚ç”¨äºå„ç§ 3D ä»»åŠ¡ã€‚</li><li>æå‡º Omni-Recon æ¡†æ¶ï¼Œå®ç°å¯æ³›åŒ– 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚</li><li>æå‡ºåŸºäºå›¾åƒæ¸²æŸ“çš„é€šç”¨ NeRF æ¨¡å‹ï¼Œå…·æœ‰ä¸¤ä¸ªè§£è€¦åˆ†æ”¯ã€‚</li><li>è¯¥æ¨¡å‹åœ¨å¯æ³›åŒ– 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ°æœ€å…ˆè¿› (SOTA) è´¨é‡ã€‚</li><li>æ··åˆæƒé‡åœ¨ä¸åŒä»»åŠ¡ä¸­å¯é‡ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚</li><li>æ¨¡å‹å¯ç”¨äºå®æ—¶æ¸²æŸ“ã€ç»¼åˆ 3D ç†è§£å’Œæ–‡æœ¬æŒ‡å¯¼çš„ 3D ç¼–è¾‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå…¨æ™¯é‡å»ºï¼šé¢å‘é€šç”¨ç¥ç»è¾å°„åœºçš„å¤šåŠŸèƒ½ 3D åº”ç”¨</li><li>ä½œè€…ï¼šYonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan (Celine) Lin</li><li>éš¶å±æœºæ„ï¼šä½æ²»äºšç†å·¥å­¦é™¢</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€3D é‡å»ºã€åœºæ™¯ç†è§£ã€3D æ¸²æŸ“ã€åœºæ™¯ç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11131</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœº (NeRF) åœ¨ 3D åº”ç”¨ä¸­å¤‡å—å…³æ³¨ï¼Œä½†ä¸åŒåº”ç”¨éœ€è¦ä¸åŒçš„ NeRF æ¨¡å‹ï¼Œå¯¼è‡´è®­ç»ƒå’Œå®éªŒç¹çã€‚(2) è¿‡å¾€æ–¹æ³•ï¼šç°æœ‰ NeRF æ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®¾è®¡ï¼Œç¼ºä¹é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæå‡º Omni-Recon æ¡†æ¶ï¼Œä½¿ç”¨åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°é€šç”¨ 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šOmni-Recon åœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›è¿˜èƒ½æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼šå°†2Då›¾åƒç‰¹å¾æå‡åˆ°3Dï¼Œå®ç°é€šç”¨3Dé‡å»ºã€‚(2) LoRAé€‚é…å™¨ï¼šå¾®è°ƒLoRAé€‚é…å™¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚(3) å®æ—¶æ¸²æŸ“ï¼šå¾®è°ƒåœºæ™¯ç½‘æ ¼å’Œç€è‰²å™¨ï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§é€šç”¨ç¥ç»è¾å°„åœº Omni-Reconï¼Œå®ƒä½¿ç”¨åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°äº†é€šç”¨ 3D é‡å»ºå’Œé›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ã€‚Omni-Recon åœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›è¿˜èƒ½æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒçš„æ¸²æŸ“ç®¡é“ï¼Œå°† 2D å›¾åƒç‰¹å¾æå‡åˆ° 3Dï¼Œå®ç°é€šç”¨ 3D é‡å»ºï¼›æå‡ºäº†ä¸€ç§ LoRA é€‚é…å™¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›æå‡ºäº†ä¸€ç§å®æ—¶æ¸²æŸ“æ–¹æ³•ï¼Œå¾®è°ƒåœºæ™¯ç½‘æ ¼å’Œç€è‰²å™¨ï¼Œå®ç°å®æ—¶æ¸²æŸ“ã€‚æ€§èƒ½ï¼šåœ¨é€šç”¨ 3D è¡¨é¢é‡å»ºä¸­è¾¾åˆ° SOTA è´¨é‡ï¼Œæ··åˆæƒé‡å¯åœ¨ä¸åŒä»»åŠ¡ä¸­å¤ç”¨ï¼Œå®ç°é›¶æ ·æœ¬å¤šä»»åŠ¡åœºæ™¯ç†è§£ï¼›æ”¯æŒå®æ—¶æ¸²æŸ“ã€é€šç”¨ 3D ç†è§£å’Œæ–‡æœ¬å¼•å¯¼çš„ 3D ç¼–è¾‘ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-264d110200ed1cf212d1bac9128b7d47.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7c1833860c2bff8e192ef7f1a12d6cc2.jpg" align="middle"></details>## The NeRFect Match: Exploring NeRF Features for Visual Localization**Authors:Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-TaixÃ©**In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks. [PDF](http://arxiv.org/abs/2403.09577v1) **Summary**NeRFçš„éšå¼ç‰¹å¾å¯ç”¨äºå»ºç«‹ç²¾ç¡®çš„2D-3DåŒ¹é…ï¼Œç”¨äºè§†è§‰å®šä½ã€‚**Key Takeaways*** NeRFå¯æä¾›ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå…·æœ‰é€¼çœŸçš„å¤–è§‚å’Œå‡†ç¡®çš„å‡ ä½•å½¢çŠ¶ã€‚* NeRFçš„å†…éƒ¨ç‰¹å¾é€šè¿‡è§†å›¾åˆæˆè·å¾—ï¼Œå¯ç”¨äºåŒ¹é…ã€‚* æ¢ç´¢äº†ä¸åŒåŒ¹é…ç½‘ç»œæ¶æ„ã€æå–å¤šå±‚ç¼–ç å™¨ç‰¹å¾å’Œæ”¹å˜è®­ç»ƒé…ç½®ã€‚* å¼•å…¥äº†NeRFMatchï¼Œä¸€ç§å…ˆè¿›çš„2D-3DåŒ¹é…å‡½æ•°ï¼Œåˆ©ç”¨NeRFé€šè¿‡è§†å›¾åˆæˆå­¦ä¹ åˆ°çš„å†…éƒ¨çŸ¥è¯†ã€‚* NeRFMatchåœ¨åŸºäºç»“æ„çš„ç®¡é“ä¸­ï¼Œåœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœåˆ·æ–°äº†å‰‘æ¡¥åœ°æ ‡å®šä½æ€§èƒ½çš„æœ€æ–°è®°å½•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºåœ¨è§†è§‰å®šä½ä¸­çš„å®Œç¾åŒ¹é…ï¼šæ¢ç´¢ç¥ç»è¾å°„åœºçš„ç‰¹å¾</li><li>ä½œè€…ï¼šQunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-TaixÃ©</li><li>éš¶å±å…³ç³»ï¼šNVIDIA</li><li>å…³é”®è¯ï¼šè§†è§‰å®šä½ï¼Œç¥ç»è¾å°„åœºï¼Œ2D-3D åŒ¹é…ï¼Œç»“æ„åŒ–è¡¨ç¤º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09577   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†è§‰å®šä½æ˜¯ç¡®å®šæŸ¥è¯¢å›¾åƒç›¸å¯¹äº 3D ç¯å¢ƒçš„ç›¸æœºä½å§¿çš„ä»»åŠ¡ã€‚ç¥ç»è¾å°„åœº (NeRF) æ˜¯ä¸€ç§å¼ºå¤§çš„ 3D åœºæ™¯è¡¨ç¤ºï¼Œå…·æœ‰é«˜å¯è§£é‡Šæ€§ã€ç´§å‡‘æ€§å’Œç”Ÿæˆé€¼çœŸå¤–è§‚å’Œå‡†ç¡®å‡ ä½•çš„èƒ½åŠ›ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šä¼ ç»Ÿçš„è§†è§‰å®šä½æ–¹æ³•ä¾èµ–äºæ˜¾å¼åœºæ™¯è¡¨ç¤ºï¼Œå¦‚ç‚¹äº‘æˆ– 3D ç½‘æ ¼ã€‚è¿™äº›æ–¹æ³•åœ¨å»ºç«‹ 2D-3D åŒ¹é…æ—¶å­˜åœ¨å±€é™æ€§ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºä½¿ç”¨ NeRF ä½œä¸ºè§†è§‰å®šä½çš„åœºæ™¯è¡¨ç¤ºã€‚é€šè¿‡æ¢ç´¢ NeRF å†…éƒ¨ç‰¹å¾åœ¨å»ºç«‹ç²¾ç¡® 2D-3D åŒ¹é…æ–¹é¢çš„æ½œåŠ›ï¼Œæ‰©å±•äº† NeRF çš„ä¼˜åŠ¿ã€‚æå‡ºäº† NeRFMatchï¼Œä¸€ç§é«˜çº§ 2D-3D åŒ¹é…å‡½æ•°ï¼Œåˆ©ç”¨äº† NeRF é€šè¿‡è§†å›¾åˆæˆå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ç»“æ„åŒ–è¡¨ç¤ºç®¡é“ä¸­ï¼ŒNeRFMatch åœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨ Cambridge Landmarks ä¸Šåˆ›é€ äº†è§†è§‰å®šä½æ€§èƒ½çš„æ–°è®°å½•ã€‚è¿™äº›ç»“æœè¯æ˜äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰NeRFç‰¹å¾æ¶ˆèå®éªŒï¼šæ¢ç´¢ä¸åŒNeRFç‰¹å¾åœ¨2D-3DåŒ¹é…ä¸­çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬åŸå§‹3Dç‚¹åæ ‡ã€ä½ç½®ç¼–ç çš„3Dç‚¹å’ŒNeRFä¸­é—´å±‚ç‰¹å¾ã€‚ï¼ˆ2ï¼‰NeRFMatchæ¶ˆèå®éªŒï¼šç ”ç©¶ä¸åŒå›¾åƒéª¨å¹²ç½‘ç»œå’ŒåŒ¹é…å‡½æ•°å¯¹åŒ¹é…æ¨¡å‹çš„å½±å“ï¼ŒåŒ…æ‹¬ResNet34ã€ConvFormerã€å·ç§¯åŒ¹é…å™¨å’Œæ³¨æ„åŠ›åŒ¹é…å™¨ã€‚ï¼ˆ3ï¼‰è®­ç»ƒæ¶ˆèå®éªŒï¼šæ¯”è¾ƒé’ˆå¯¹æ¯ä¸ªåœºæ™¯è®­ç»ƒå’Œé’ˆå¯¹å¤šä¸ªåœºæ™¯è®­ç»ƒçš„NeRFMatchæ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠä½¿ç”¨ImageNeté¢„è®­ç»ƒå›¾åƒéª¨å¹²ç½‘ç»œçš„å½±å“ã€‚ï¼ˆ4ï¼‰å§¿æ€ä¼˜åŒ–å®éªŒï¼šæ¢ç´¢è¿­ä»£å’Œä¼˜åŒ–ä¸¤ç§å§¿æ€ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å§¿æ€ç²¾åº¦ï¼Œå¹¶è¯„ä¼°ä¸åŒNeRFMatchæ¨¡å‹å’Œè®­ç»ƒè®¾ç½®çš„ä¼˜åŒ–æ•ˆæœã€‚</p><p><strong>8. ç»“è®º</strong>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„è§†è§‰å®šä½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº† NeRF çš„å†…éƒ¨ç‰¹å¾ï¼Œåœ¨å»ºç«‹ç²¾ç¡®çš„ 2D-3D åŒ¹é…æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æå‡ºçš„ NeRFMatch æ¨¡å‹åœ¨æ ‡å‡†å®šä½åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p><p>(2): <strong>åˆ›æ–°ç‚¹ï¼š</strong>* åˆ©ç”¨ NeRF çš„å†…éƒ¨ç‰¹å¾è¿›è¡Œ 2D-3D åŒ¹é…ï¼Œæ¢ç´¢äº† NeRF åœ¨è§†è§‰å®šä½ä¸­çš„æ–°æ½œåŠ›ã€‚* æå‡ºäº†ä¸€ç§é«˜çº§ 2D-3D åŒ¹é…å‡½æ•° NeRFMatchï¼Œåˆ©ç”¨äº† NeRF é€šè¿‡è§†å›¾åˆæˆå­¦ä¹ çš„å†…éƒ¨çŸ¥è¯†ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* åœ¨ Cambridge Landmarks åŸºå‡†ä¸Šåˆ›é€ äº†è§†è§‰å®šä½æ€§èƒ½çš„æ–°è®°å½•ã€‚* åœ¨å„ç§åœºæ™¯å’Œè®­ç»ƒè®¾ç½®ä¸‹è¡¨ç°å‡ºé²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* éœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è®­ç»ƒ NeRFï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚* NeRFMatch æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†éœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—èƒ½åŠ›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3cd8ba580831022c4f675064d1098186.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8213b16ccc45bbcd6a6f3465f9ed99c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ec50b8d2fa9ffdc32797b6db3683bcd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aefcfa5ab2e39bdb2d87786b5cdb12fa.jpg" align="middle"></details>## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF   Priors**Authors:Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao**Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight. [PDF](http://arxiv.org/abs/2403.09079v1) **Summary**é¢„è§æ¡†æ¶ä»¥äººç±»å¯¼èˆªä¸ºå¯å‘ï¼Œåˆ©ç”¨è¿‡å»éå†æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ï¼Œæé«˜åŸå¸‚å°ºåº¦ç¥ç»è¾å°„åœºçš„æ€§èƒ½ï¼Œæå‡è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„æ•ˆç‡ã€‚**Key Takeaways**- å—äººç±»å¯¼èˆªæ–¹å¼å¯å‘ï¼Œæå‡ºé¢„è§æ¡†æ¶ï¼Œåˆ©ç”¨è¿‡å»éå†æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ã€‚- ä¼˜åŒ–åŸå¸‚å°ºåº¦ç¥ç»è¾å°„åœºï¼Œåˆ©ç”¨å…ˆå‰çš„æ—…ç¨‹æ•°æ®ç”Ÿæˆç¥ç»å…ˆéªŒã€‚- ç¥ç»å…ˆéªŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œå¯æ— ç¼å¢å¼ºå„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ã€‚- é¢„è§æ¡†æ¶ä¸å¤šç§åœ¨çº¿æ„ŸçŸ¥æ¨¡å‹å…¼å®¹æ€§é«˜ã€‚- åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é«˜æ¸…åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚- é¢„è§æ¡†æ¶æœ‰æœ›æˆä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶ã€‚- ä»£ç å°†åœ¨ https://github.com/yuantianyuan01/PreSight å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šPreSightï¼šåˆ©ç”¨åŸå¸‚è§„æ¨¡ NeRF å…ˆéªŒå¢å¼ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥</li><li>ä½œè€…ï¼šTianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šè‡ªåŠ¨é©¾é©¶ã€åŸºäºè§†è§‰çš„æ„ŸçŸ¥ã€ç¥ç»éšå¼åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09079   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸¥é‡ä¾èµ–æ„ŸçŸ¥ç³»ç»Ÿæ¥å¯¼èˆªå’Œè§£é‡Šå‘¨å›´ç¯å¢ƒã€‚å°½ç®¡è¿™äº›ç³»ç»Ÿæœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨é®æŒ¡ã€æç«¯å…‰ç…§æˆ–ä¸ç†Ÿæ‚‰çš„åŸå¸‚åŒºåŸŸç­‰æ¡ä»¶ä¸‹ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸è¿™äº›ç³»ç»Ÿä¸åŒï¼Œäººç±»å¹¶ä¸å®Œå…¨ä¾èµ–å³æ—¶è§‚å¯Ÿæ¥æ„ŸçŸ¥ç¯å¢ƒã€‚åœ¨æ¢ç´¢æ–°åŸå¸‚æ—¶ï¼Œäººç±»ä¼šé€æ¸å½¢æˆä¸€ä¸ªåˆæ­¥çš„å¿ƒç†åœ°å›¾ï¼Œä»¥è¡¥å……åç»­è®¿é—®æœŸé—´çš„å®æ—¶æ„ŸçŸ¥ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡çš„åŠ¨æœºå¾ˆå¥½ï¼Œå—äººç±»æ–¹æ³•çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ PreSightï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¿‡å»çš„éå†æ¥æ„å»ºé™æ€å…ˆéªŒè®°å¿†ï¼Œä»è€Œå¢å¼ºåç»­å¯¼èˆªä¸­çš„åœ¨çº¿æ„ŸçŸ¥ã€‚   ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šè¯¥æ–¹æ³•æ¶‰åŠä½¿ç”¨æ¥è‡ªå…ˆå‰æ—…ç¨‹çš„æ•°æ®ä¼˜åŒ–åŸå¸‚è§„æ¨¡ç¥ç»è¾å°„åœºä»¥ç”Ÿæˆç¥ç»å…ˆéªŒã€‚è¿™äº›å…ˆéªŒä¸°å¯Œäº†è¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼Œæ— éœ€äººå·¥æ³¨é‡Šï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼å¢å¼ºå„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œä»¥æœ€å°çš„é¢å¤–è®¡ç®—æˆæœ¬æé«˜å…¶åŠŸæ•ˆã€‚   ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸å„ç§åœ¨çº¿æ„ŸçŸ¥æ¨¡å‹é«˜åº¦å…¼å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨ HD åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾ç€çš„æ”¹è¿›ï¼Œçªå‡ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1): åˆ©ç”¨åŸå¸‚è§„æ¨¡çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥ç”Ÿæˆç¥ç»å…ˆéªŒï¼Œä¸°å¯Œè¯­ä¹‰å’Œå‡ ä½•ç»†èŠ‚ï¼›(2): å°†ç¥ç»å…ˆéªŒæ— ç¼å¢å¼ºåˆ°å„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ä¸­ï¼Œæé«˜å…¶åŠŸæ•ˆï¼›(3): åœ¨ HD åœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œåˆ©ç”¨åŸå¸‚è§„æ¨¡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç”Ÿæˆç¥ç»å…ˆéªŒï¼Œæ— ç¼å¢å¼ºåˆ°å„ç§æœ€å…ˆè¿›çš„æ„ŸçŸ¥æ¨¡å‹ä¸­ï¼Œæé«˜å…¶åŠŸæ•ˆï¼Œåœ¨HDåœ°å›¾æ„å»ºå’Œå ç”¨é¢„æµ‹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ–°æ„ŸçŸ¥æ¡†æ¶çš„æ½œåŠ›ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡º PreSight æ¡†æ¶ï¼Œåˆ©ç”¨åŸå¸‚è§„æ¨¡ NeRF æ„å»ºé™æ€å…ˆéªŒï¼Œå¢å¼ºåœ¨çº¿æ„ŸçŸ¥ï¼›æ€§èƒ½ï¼šåœ¨ nuScenes æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ï¼›å·¥ä½œé‡ï¼šéœ€è¦å‡†ç¡®çš„è½¦èº«ä½å§¿å’Œæ‘„åƒå¤´ä¼ æ„Ÿå™¨ï¼Œå¯èƒ½æ— æ³•åœ¨ä¼—åŒ…æ•°æ®ä¸­è·å¾—ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6e89a00394046d5fd38373e9130ab120.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e10f4a3c19b9cce44b6cd16bfb60eeee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a02818f4812a5d830dd0c4a4365984cc.jpg" align="middle"></details><h2 id="NeRF-Supervised-Feature-Point-Detection-and-Description"><a href="#NeRF-Supervised-Feature-Point-Detection-and-Description" class="headerlink" title="NeRF-Supervised Feature Point Detection and Description"></a>NeRF-Supervised Feature Point Detection and Description</h2><p><strong>Authors:Ali Youssef, Francisco Vasconcelos</strong></p><p>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches. </p><p><a href="http://arxiv.org/abs/2403.08156v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) ç”¨äºç”Ÿæˆé€¼çœŸçš„å¤šè§†å›¾è®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜ç‰¹å¾ç‚¹æ£€æµ‹å’Œæè¿°çš„å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨ NeRFs ç”Ÿæˆé€¼çœŸå¤šè§†å›¾è®­ç»ƒæ•°æ®çš„åˆ›æ–°æ–¹æ³•ã€‚</li><li>è®­ç»ƒç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ä»¥ NeRF åˆæˆè§†å›¾ä¸ºç›‘ç£ï¼Œå¹¶é‡‡ç”¨é€è§†æŠ•å½±å‡ ä½•ã€‚</li><li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘æ³¨å†Œå’Œå•åº”æ€§ä¼°è®¡åŸºå‡†ä¸Šå®ç°äº†ç«äº‰æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œéœ€è¦çš„è®­ç»ƒæ•°æ®æ˜¾ç€å‡å°‘ã€‚</li><li>å¤šæ ·åŒ–å¤šè§†å›¾æ•°æ®é›†åŒ…æ‹¬å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•ä½¿ç”¨ NeRFs è®­ç»ƒï¼Œå…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†å„ç§è§†è§’ã€‚</li><li>è¯¥æ–¹æ³•ä¸ºè§†è§‰ SLAM å’Œè§†è§‰ä½ç½®è¯†åˆ«ç­‰è®¡ç®—æœºè§†è§‰åº”ç”¨æä¾›äº†æ”¹è¿›çš„ç‰¹å¾ç‚¹æ£€æµ‹å’Œæè¿°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºè¾…åŠ©ç‰¹å¾ç‚¹æ£€æµ‹ä¸æè¿°</li><p></p><p></p><li>ä½œè€…ï¼šAli Youssefï¼ŒFrancisco Vasconcelos</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¼¦æ•¦å¤§å­¦å­¦é™¢è®¡ç®—æœºç§‘å­¦ç³»</li><p></p><p></p><li>å…³é”®è¯ï¼šç‰¹å¾æ£€æµ‹ä¸æè¿°ã€ç¥ç»è¾å°„åœºã€æ•°æ®é›†</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.08156   Github ä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç‰¹å¾ç‚¹æ£€æµ‹ä¸æè¿°æ˜¯è®¡ç®—æœºè§†è§‰ä¸­è®¸å¤šå¤šè§†å›¾é—®é¢˜ï¼ˆå¦‚è¿åŠ¨ç»“æ„ã€è§†è§‰ SLAM å’Œè§†è§‰å®šä½è¯†åˆ«ï¼‰çš„åŸºç¡€ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å·²å–ä»£æ‰‹å·¥åˆ¶ä½œæŠ€æœ¯ï¼Œä½†å…¶è®­ç»ƒé€šå¸¸ä¾èµ–äºåŸºäºä»¿å°„å˜æ¢çš„ç®€å•å¤šè§†å›¾è§†è§’æ¨¡æ‹Ÿï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚   (2) è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä½¿ç”¨åŸºäºä»¿å°„å˜æ¢çš„å›¾åƒæ‰­æ›²æ¥æ¨¡æ‹Ÿä¸åŒè§†è§’ï¼Œä½†è¿™ç§æ‰­æ›²è¿‡äºç®€å•ï¼Œæ— æ³•å‡†ç¡®æ¨¡æ‹Ÿå¤šè§†å›¾é€è§†ã€‚   (3) æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ç”Ÿæˆé€¼çœŸå¤šè§†å›¾è®­ç»ƒæ•°æ®çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶è€…åˆ›å»ºäº†ä¸€ä¸ªä½¿ç”¨ NeRF åˆæˆçš„å¤šè§†å›¾æ•°æ®é›†ï¼ŒåŒ…å«å®¤å†…å’Œå®¤å¤–åœºæ™¯ã€‚ç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†æœ€å…ˆè¿›çš„ç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°å­è°ƒæ•´ä¸ºåœ¨ NeRF åˆæˆçš„è§†å›¾ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”±é€è§†æŠ•å½±å‡ ä½•è¿›è¡Œç›‘ç£ã€‚   (4) å®éªŒç»“æœï¼šå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘é…å‡†å’Œä»¿å°„å˜æ¢ä¼°è®¡çš„æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€è®­ç»ƒæ•°æ®æ˜æ˜¾æ›´å°‘ã€‚</p></li><li><p>Methods:(1): ä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) ç”Ÿæˆé€¼çœŸçš„å¤šè§†å›¾è®­ç»ƒæ•°æ®ï¼›(2): æå‡ºä¸€ç§åŸºäºé€è§†æŠ•å½±å‡ ä½•ç›‘ç£çš„ NeRF ç‚¹é‡æŠ•å½±æ–¹æ³•ï¼›(3): è°ƒæ•´æœ€å…ˆè¿›çš„ç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼Œä½¿å…¶åœ¨ NeRF åˆæˆçš„è§†å›¾ä¸Šè¿›è¡Œè®­ç»ƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥ç›‘ç£åŸºäºå­¦ä¹ çš„ç‰¹å¾ç‚¹æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼Œåˆ©ç”¨åˆæˆ NeRF æ•°æ®ä¸Šçš„é€è§†æŠ•å½±å‡ ä½•ã€‚å°½ç®¡æˆ‘ä»¬æå‡ºçš„æ•°æ®é›†å®Œå…¨ç”±åˆæˆå›¾åƒè€Œä¸æ˜¯çœŸå®çš„ RGB å›¾åƒç»„æˆï¼Œå¹¶ä¸”æ¯”å¤§å‹å¼€æºæ•°æ®é›†å°å¾—å¤šï¼Œä½†ç»“æœè¡¨æ˜ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›æˆ–ç‰¹å¾ç‚¹æ£€æµ‹è´¨é‡æ–¹é¢æ²¡æœ‰è§‚å¯Ÿåˆ°ä¸‹é™ã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šå¸¸åœ¨å…·æœ‰é«˜åº¦éå¹³é¢åœºæ™¯çš„å¤šè§†å›¾åŸºå‡†ä¸Šä¼˜äºç»è¿‡å•åº”æ€§è®­ç»ƒçš„åŸºçº¿ï¼Œè€Œåœ¨å•åº”æ€§ä¼°è®¡åŸºå‡†ä¸Šç•¥é€Šä¸€ç­¹ã€‚è¿›ä¸€æ­¥å‘å±•çš„æ›´å¤§æ½œåŠ›åœ¨äºæé«˜ç¥ç»æ¸²æŸ“çš„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œç¥ç»æ¸²æŸ“å¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„åˆæˆå›¾åƒï¼Œæ²¡æœ‰äººå·¥åˆ¶å“ï¼Œæœ€é‡è¦çš„æ˜¯æ›´ç²¾ç¡®çš„æ·±åº¦å›¾ä»¥é¿å…é”™è¯¯æŠ•å½±ã€‚</p></li></ol><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ NeRF åˆæˆçš„é€¼çœŸå¤šè§†å›¾æ•°æ®è®­ç»ƒç‰¹å¾æ£€æµ‹å™¨å’Œæè¿°ç¬¦ï¼›æå‡ºäº†ä¸€ç§åŸºäºé€è§†æŠ•å½±å‡ ä½•ç›‘ç£çš„ NeRF ç‚¹é‡æŠ•å½±æ–¹æ³•ã€‚</p><p>æ€§èƒ½ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ç›¸å¯¹ä½å§¿ä¼°è®¡ã€ç‚¹äº‘é…å‡†å’Œä»¿å°„å˜æ¢ä¼°è®¡çš„æ ‡å‡†åŸºå‡†ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶æ‰€éœ€è®­ç»ƒæ•°æ®æ˜æ˜¾æ›´å°‘ã€‚</p><p>å·¥ä½œé‡ï¼šæ•°æ®é›†åˆæˆå’Œæ¨¡å‹è®­ç»ƒçš„å·¥ä½œé‡ä¸­ç­‰ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-44aa82812a0f884c826b881fd8f38e44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-661d97273d7fdccb785af810b9b662b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-105725399243a9c4608e1b49743e23c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d29f42ad3850aa4729795f0e7e52bfe4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/</id>
    <published>2024-03-23T10:15:27.000Z</published>
    <updated>2024-03-23T10:15:27.845Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images"><a href="#MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images" class="headerlink" title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images"></a>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h2><p><strong>Authors:Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</strong></p><p>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitivesâ€™ opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization. </p><p><a href="http://arxiv.org/abs/2403.14627v1">PDF</a> Project page: <a href="https://donydchen.github.io/mvsplat">https://donydchen.github.io/mvsplat</a> Code:   <a href="https://github.com/donydchen/mvsplat">https://github.com/donydchen/mvsplat</a></p><p><strong>Summary</strong><br>MVSplat æ¨¡å‹é€šè¿‡åˆ©ç”¨ç¨€ç–å¤šè§†è§’å›¾åƒï¼Œç»“åˆé«˜æ•ˆçš„é€è§†æŠ•å½± 3D é«˜æ–¯ Splatting ç»„ä»¶ï¼Œå®ç°é«˜æ•ˆçš„å‰å‘ 3D é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º MVSplat æ¨¡å‹ï¼Œå°† 3D é«˜æ–¯ Splatting ä¸ç¨€ç–å¤šè§†è§’å›¾åƒç›¸ç»“åˆï¼Œè¿›è¡Œé«˜æ•ˆçš„å‰å‘ 3D é‡å»ºã€‚</li><li>é€šè¿‡å¹³é¢æ‰«ææ„å»ºä»£ä»·ä½“è¡¨ç¤ºï¼Œåˆ©ç”¨ä»£ä»·ä½“ä¸­çš„è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä¸ºæ·±åº¦ä¼°è®¡æä¾›å‡ ä½•çº¿ç´¢ã€‚</li><li>è”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®å’Œçƒè°ç³»æ•°ï¼Œä»…ä¾èµ–äºå…‰åº¦ç›‘ç£ã€‚</li><li>è¯æ˜ä»£ä»·ä½“è¡¨ç¤ºå¯¹å­¦ä¹ å‰å‘é«˜æ–¯ Splatting æ¨¡å‹çš„é‡è¦æ€§ã€‚</li><li>åœ¨ RealEstate10K å’Œ ACID åŸºå‡†ä¸Šï¼Œè¯¥æ¨¡å‹å®ç° SOTA æ€§èƒ½ï¼Œä¸”å…·æœ‰æœ€å¿«çš„æ¨ç†é€Ÿåº¦ï¼ˆ22 fpsï¼‰ã€‚</li><li>ä¸ pixelSplat ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å‚æ•°é‡å‡å°‘ $10\times$ï¼Œæ¨ç†é€Ÿåº¦æé«˜ $2\times$ ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ï¼Œä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šMVSplatï¼šåŸºäºç¨€ç–å¤šè§†å›¾å›¾åƒçš„é«˜æ•ˆä¸‰ç»´é«˜æ–¯ Splatting</li><li>ä½œè€…ï¼šYuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</li><li>å•ä½ï¼šè«çº³ä»€å¤§å­¦</li><li>å…³é”®è¯ï¼šç‰¹å¾åŒ¹é…ã€ä»£ä»·ä½“ç§¯ã€é«˜æ–¯ Splatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://donydchen.github.io/mvsplat   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆä»æåº¦ç¨€ç–çš„å›¾åƒï¼ˆä¾‹å¦‚ï¼Œå°‘è‡³ä¸¤å¼ ï¼‰ä¸­æå‡ºè®¡ç®—æœºè§†è§‰ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºå¤šè§†å›¾å‡ ä½•çš„ä¼ ç»Ÿæ–¹æ³•å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¿™åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ Splatting æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºä»ç¨€ç–å›¾åƒä¸­é‡å»ºä¸‰ç»´åœºæ™¯çš„å·¨å¤§æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ Splatting æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„ splatting åŸè¯­ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å»ºæ¨¡èƒ½åŠ›å’Œæ³›åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æ¨ç†é€Ÿåº¦æ–¹é¢å—åˆ°é™åˆ¶ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆå‰é¦ˆä¸‰ç»´é«˜æ–¯ Splatting æ¨¡å‹ MVSplatï¼Œè¯¥æ¨¡å‹ä»ç¨€ç–å¤šè§†å›¾å›¾åƒä¸­å­¦ä¹ ã€‚ä¸ºäº†å‡†ç¡®å®šä½é«˜æ–¯ä¸­å¿ƒï¼Œæœ¬æ–‡æå‡ºé€šè¿‡åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå¹³é¢æ‰«ææ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå…¶ä¸­å­˜å‚¨åœ¨ä»£ä»·ä½“ç§¯ä¸­çš„è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§å¯ä»¥ä¸ºæ·±åº¦ä¼°è®¡æä¾›æœ‰ä»·å€¼çš„å‡ ä½•çº¿ç´¢ã€‚æœ¬æ–‡ä»…ä¾é å…‰åº¦ç›‘ç£ï¼Œè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®å’Œçƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒã€‚æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯æ˜äº†ä»£ä»·ä½“ç§¯è¡¨ç¤ºåœ¨å­¦ä¹ å‰é¦ˆé«˜æ–¯ Splatting æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šåœ¨å¤§è§„æ¨¡ RealEstate10K å’Œ ACID åŸºå‡†ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹ä»¥æœ€å¿«çš„é¦ˆé€å‰å‘æ¨ç†é€Ÿåº¦ï¼ˆ22fpsï¼‰å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸æœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³• pixelSplat ç›¸æ¯”ï¼Œæœ¬æ–‡æ¨¡å‹ä½¿ç”¨å°‘ 10 å€çš„å‚æ•°ï¼Œæ¨ç†é€Ÿåº¦æé«˜ 2 å€ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–ã€‚</li></ol><p>7.Methodsï¼š(1) æ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼šé€šè¿‡åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œå¹³é¢æ‰«æï¼Œè®¡ç®—è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œæ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œä¸ºæ·±åº¦ä¼°è®¡æä¾›å‡ ä½•çº¿ç´¢ã€‚(2) å­¦ä¹ é«˜æ–¯åŸè¯­å‚æ•°ï¼šè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œä»…ä¾é å…‰åº¦ç›‘ç£ã€‚(3) å‰é¦ˆé«˜æ–¯Splattingï¼šåˆ©ç”¨ä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå­¦ä¹ å‰é¦ˆé«˜æ–¯Splattingæ¨¡å‹ï¼Œé«˜æ•ˆä¸”é²æ£’åœ°ä»ç¨€ç–å›¾åƒé‡å»ºä¸‰ç»´åœºæ™¯ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å‰é¦ˆä¸‰ç»´é«˜æ–¯Splattingæ¨¡å‹MVSplatï¼Œè¯¥æ¨¡å‹ä»ç¨€ç–å¤šè§†å›¾å›¾åƒä¸­å­¦ä¹ ï¼Œé€šè¿‡æ„å»ºä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œå¹¶è”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œå®ç°äº†é«˜æ•ˆé²æ£’çš„ä¸‰ç»´åœºæ™¯é‡å»ºã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä»£ä»·ä½“ç§¯è¡¨ç¤ºï¼Œåˆ©ç”¨å¤šè§†å›¾å¯¹åº”ä¿¡æ¯è¿›è¡Œå‡ ä½•å­¦ä¹ ï¼Œä¸åŒäºç°æœ‰ä¾é æ•°æ®é©±åŠ¨çš„è®¾è®¡æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨ä¸¤ä¸ªå¤§è§„æ¨¡åœºæ™¯çº§é‡å»ºåŸºå‡†ä¸Šï¼Œæœ¬æ–‡æ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸æœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³•pixelSplatç›¸æ¯”ï¼Œæœ¬æ–‡æ¨¡å‹ä½¿ç”¨å°‘10å€çš„å‚æ•°ï¼Œæ¨ç†é€Ÿåº¦æé«˜2å€ä»¥ä¸Šï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å¤–è§‚å’Œå‡ ä½•è´¨é‡ä»¥åŠæ›´å¥½çš„è·¨æ•°æ®é›†æ³›åŒ–ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æ¨¡å‹ä»…ä¾é å…‰åº¦ç›‘ç£ï¼Œè”åˆå­¦ä¹ é«˜æ–¯åŸè¯­çš„ä¸é€æ˜åº¦ã€åæ–¹å·®ã€çƒè°ç³»æ•°ä»¥åŠé«˜æ–¯ä¸­å¿ƒï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c0c99bd06aa26e0988e91dc485ee84a3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08b17b212717995337d92cbe71cb9434.jpg" align="middle"></details><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>3Dé«˜æ–¯é‡å»ºå™¨ï¼ˆGRMï¼‰ï¼šåŸºäº Transformer çš„é«˜æ•ˆå¤šè§†å›¾ 3D é‡å»ºæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§å¤§å‹é‡å»ºå™¨ GRMï¼Œå¯ä»¥ä»ç¨€ç–è§†è§’å›¾åƒä¸­ä»¥çº¦ 0.1 ç§’çš„é€Ÿåº¦æ¢å¤ 3D èµ„äº§ã€‚</li><li>GRM æ˜¯ä¸€ç§å‰é¦ˆ Transformer æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•´åˆå¤šè§†å›¾ä¿¡æ¯ã€‚</li><li>GRM å¼•å…¥äº† 3D é«˜æ–¯è¡¨ç¤ºï¼Œå¯ä»¥é«˜æ•ˆã€å¯æ‰©å±•åœ°è¿›è¡Œé‡å»ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li>GRM å¯ä»¥é›†æˆåˆ°ç°æœ‰å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºç”Ÿæˆä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬åˆ° 3Dã€å›¾åƒåˆ° 3Dï¼‰ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://justimyhxu.github.io/projects/grm/ã€‚">https://justimyhxu.github.io/projects/grm/ã€‚</a></li><li>ä»£ç å·²å¼€æºï¼š<a href="https://github.com/Just-JH-Xu/grmã€‚">https://github.com/Just-JH-Xu/grmã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGRMï¼šç”¨äºé«˜æ•ˆ 3D é‡å»ºå’Œç”Ÿæˆçš„å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹</li><li>ä½œè€…ï¼šYinghao Xuï¼ŒZifan Shiï¼ŒYifan Wangï¼ŒHansheng Chenï¼ŒCeyuan Yangï¼ŒSida Pengï¼ŒYujun Shenï¼ŒGordon Wetzstein</li><li>éš¶å±ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯çƒé¢æ˜ å°„ã€3D é‡å»ºã€3D ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2212.07524Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šéšç€è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦çš„å‘å±•ï¼Œ3D é‡å»ºå’Œç”ŸæˆæŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½é¢ä¸´ç€æŒ‘æˆ˜ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤šè§†å›¾å‡ ä½•æˆ–æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥é‡å»º 3D åœºæ™¯ã€‚å¤šè§†å›¾å‡ ä½•æ–¹æ³•éœ€è¦å¤§é‡çš„è§†å›¾æ‰èƒ½è·å¾—å‡†ç¡®çš„é‡å»ºç»“æœï¼Œè€Œæ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶å¯ä»¥ä»è¾ƒå°‘çš„è§†å›¾ä¸­é‡å»º 3D åœºæ™¯ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º GRM çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆåœ°ä»ç¨€ç–è§†å›¾é‡å»º 3D åœºæ™¯ã€‚GRM æ˜¯ä¸€ç§å‰é¦ˆ Transformer æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯å‡½æ•°ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯å‡½æ•°æŠ•å½±åˆ° 3D ç©ºé—´ä¸­ï¼Œå½¢æˆä¸€ç»„å¯†é›†åˆ†å¸ƒçš„ 3D é«˜æ–¯å‡½æ•°ï¼Œä»£è¡¨åœºæ™¯ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šï¼ŒGRM åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­éƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨å•å›¾åƒåˆ° 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒGRM å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚</p></li><li><p>Methods:(1) GRMé¦–å…ˆå°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯å‡½æ•°ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯å‡½æ•°æŠ•å½±åˆ°3Dç©ºé—´ä¸­ï¼Œå½¢æˆä¸€ç»„å¯†é›†åˆ†å¸ƒçš„3Dé«˜æ–¯å‡½æ•°ï¼Œä»£è¡¨åœºæ™¯ã€‚(2) GRMä½¿ç”¨Transformeræ¨¡å‹æ¥å­¦ä¹ é«˜æ–¯å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å…³ç³»æ¥é¢„æµ‹åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„æ·±åº¦å’Œæ³•çº¿ã€‚(3) GRMä½¿ç”¨ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸€è‡´çš„3Dåœºæ™¯ï¼ŒåŒæ—¶è¿˜é¼“åŠ±æ¨¡å‹ç”Ÿæˆå¹³æ»‘ã€æ— å™ªå£°çš„3Dåœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º GRM çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆåœ°ä»ç¨€ç–è§†å›¾é‡å»º 3D åœºæ™¯ã€‚GRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒGRM è¿˜å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§è§„æ¨¡é«˜æ–¯é‡å»ºæ¨¡å‹æ¥é«˜æ•ˆé‡å»º 3D åœºæ™¯çš„æ–°æ–¹æ³•ã€‚</li><li>ä½¿ç”¨ Transformer æ¨¡å‹æ¥å­¦ä¹ é«˜æ–¯å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä½¿ç”¨è¿™äº›å…³ç³»æ¥é¢„æµ‹åœºæ™¯ä¸­æ¯ä¸ªç‚¹çš„æ·±åº¦å’Œæ³•çº¿ã€‚</li><li>ä½¿ç”¨ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æŸå¤±å‡½æ•°é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸è¾“å…¥å›¾åƒä¸€è‡´çš„ 3D åœºæ™¯ï¼ŒåŒæ—¶è¿˜é¼“åŠ±æ¨¡å‹ç”Ÿæˆå¹³æ»‘ã€æ— å™ªå£°çš„ 3D åœºæ™¯ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­ï¼ŒGRM åœ¨ç¨€ç–è§†å›¾é‡å»ºä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>GRM å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ 3D æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆæ›´é€¼çœŸçš„ 3D æ¨¡å‹ã€‚å·¥ä½œé‡ï¼š</li><li>GRM çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½éå¸¸é«˜æ•ˆã€‚</li><li>GRM å¯ä»¥ä½¿ç”¨å•ä¸ª GPU åœ¨å‡ ç§’é’Ÿå†…é‡å»º 3D åœºæ™¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering"><a href="#Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering" class="headerlink" title="Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering"></a>Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering</h2><p><strong>Authors:Antoine GuÃ©don, Vincent Lepetit</strong></p><p>We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a> </p><p><a href="http://arxiv.org/abs/2403.14554v1">PDF</a> Project Webpage: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a></p><p><strong>Summary</strong><br>åŸºäºç½‘æ ¼çš„é«˜æ–¯å–·æº…æ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç½‘æ ¼è¡¨ç¤ºæ–¹æ³•ï¼Œå³é«˜æ–¯ç³–éœœï¼Œå¯ç”¨äºå®æ—¶æ¸²æŸ“å’Œç¼–è¾‘å¤æ‚ 3D æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°† 3D é«˜æ–¯å–·æº…æ¡†æ¶æ”¹è¿›ä¸ºåŸºäºç½‘æ ¼çš„è¡¨ç¤ºï¼Œä»¥ä¼˜åŒ–å¤æ‚çš„ 3D æ•ˆæœçš„å®æ—¶æ¸²æŸ“å’Œç¼–è¾‘ã€‚</li><li>åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä»é«˜æ–¯å‡½æ•°ä¸­æå–åŸºç¡€ç½‘æ ¼ï¼Œå¹¶åœ¨ç½‘æ ¼å‘¨å›´æ„å»ºå’Œç»†åŒ–ä¸€å±‚å…·æœ‰å¯å˜åšåº¦çš„è‡ªé€‚åº”é«˜æ–¯å‡½æ•°ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¡¨é¢é™„è¿‘çš„ç²¾ç»†ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœã€‚</li><li>å°†è¿™å±‚ç§°ä¸ºé«˜æ–¯ç³–éœœï¼Œå› ä¸ºå®ƒç±»ä¼¼äºè›‹ç³•ä¸Šçš„ç³–éœœæ¶‚å±‚ã€‚ææ–™è¶Šè“¬æ¾ï¼Œç³–éœœè¶Šåšã€‚</li><li>å¼•å…¥äº†é«˜æ–¯å‡½æ•°çš„å‚æ•°åŒ–ï¼Œä»¥å¼ºåˆ¶å®ƒä»¬åœç•™åœ¨ç³–éœœå±‚å†…ï¼Œå¹¶åœ¨å˜å½¢ã€ç¼©æ”¾ã€ç¼–è¾‘æˆ–åŠ¨ç”»ç½‘æ ¼æ—¶è‡ªåŠ¨è°ƒæ•´å…¶å‚æ•°ã€‚</li><li>è¯¥è¡¨ç¤ºå…è®¸ä½¿ç”¨é«˜æ–¯å–·æº…è¿›è¡Œé«˜æ•ˆæ¸²æŸ“ï¼Œä»¥åŠé€šè¿‡ä¿®æ”¹åŸºç¡€ç½‘æ ¼è¿›è¡Œç¼–è¾‘å’ŒåŠ¨ç”»ã€‚</li><li>åœ¨å„ç§åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜å®ƒä¼˜äºç°æœ‰çš„åŸºäºæ›²é¢çš„æ–¹æ³•ã€‚</li><li>è¯¥é¡¹ç›®å°†å‘å¸ƒä»£ç å’ŒåŸºäº Web çš„æŸ¥çœ‹å™¨ä½œä¸ºé™„åŠ è´¡çŒ®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé«˜æ–¯ç³–éœœï¼šå¯ç¼–è¾‘çš„å¤æ‚å…‰ç…§åœº</li><li>ä½œè€…ï¼šAntoine GuÃ©donã€Vincent Lepetit</li><li>éš¶å±å•ä½ï¼šå·´é»ä¸œéƒ¨å¤§å­¦æ ¡ã€æ³•å›½å›½å®¶ç§‘å­¦ç ”ç©¶ä¸­å¿ƒ</li><li>å…³é”®è¯ï¼šé«˜æ–¯æ•£å°„ã€ç½‘æ ¼ã€å¯å¾®æ¸²æŸ“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14554   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   è¿‘å¹´æ¥ï¼ŒåŸºäºé«˜æ–¯æ•£å°„çš„ä½“ç§¯æ¸²æŸ“æ–¹æ³•å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¤æ‚è¡¨é¢ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºç½‘æ ¼æˆ–ä½“ç§¯è¡¨ç¤ºï¼Œéš¾ä»¥åŒæ—¶æ•æ‰ç»†å¾®ç»†èŠ‚å’Œä½“ç§¯æ•ˆæœã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºï¼Œå®ƒåœ¨ç½‘æ ¼è¡¨é¢æ·»åŠ äº†ä¸€å±‚å¯å˜åšåº¦çš„é«˜æ–¯æ•£å°„ä½“ï¼Œç§°ä¸ºâ€œç³–éœœå±‚â€ã€‚è¯¥è¡¨ç¤ºå¯ä»¥æœ‰æ•ˆæ•æ‰æ¯›å‘ã€è‰åœ°ç­‰ææ–™çš„å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠç›®æ ‡è¾¾æˆæƒ…å†µï¼š   åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šï¼Œé«˜æ–¯ç³–éœœè¡¨ç¤ºä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚å…¶æ€§èƒ½æ”¯æŒä½œè€…çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§é«˜è´¨é‡ã€å¯ç¼–è¾‘ã€å¯å®æ—¶æ¸²æŸ“çš„å¤æ‚è¡¨é¢è¡¨ç¤ºã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰è¿™é¡¹å·¥ä½œæå‡ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºï¼Œå®ƒæ˜¯ä¸€ç§æ–°çš„è¡¨é¢è¡¨ç¤ºï¼Œå¯ä»¥æ•æ‰å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚è¯¥è¡¨ç¤ºåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¡¨é¢è¡¨ç¤ºï¼Œå®ƒå¯ä»¥åŒæ—¶æ•æ‰å¤æ‚ä½“ç§¯æ•ˆæœå’Œç»†å¾®ç»†èŠ‚ã€‚</li><li>å¼€å‘äº†ä¸€ç§ä»å›¾åƒä¸­æå–é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„æ–¹æ³•ã€‚</li><li>å±•ç¤ºäº†é«˜æ–¯ç³–éœœè¡¨ç¤ºåœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­çš„æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚æ€§èƒ½ï¼š</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºåœ¨æ¸²æŸ“ã€ç¼–è¾‘å’ŒåŠ¨ç”»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºäºè¡¨é¢çš„æ–¹æ³•ã€‚</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºå¯ä»¥å®æ—¶æ¸²æŸ“å¤æ‚è¡¨é¢ã€‚å·¥ä½œé‡ï¼š</li><li>ä»å›¾åƒä¸­æå–é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li><li>é«˜æ–¯ç³–éœœè¡¨ç¤ºçš„æ¨¡å‹æ¯”é¦™è‰é«˜æ–¯å–·å°„æ¨¡å‹æ›´å¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-5bbff4f7dfd0182e4e70f1792caffd34.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9a43785bf9af3efbb44319d8124d371.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7219cf2d8c7b8ae04b33a0dd24b18d5e.jpg" align="middle"></details><h2 id="HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression"><a href="#HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression" class="headerlink" title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression"></a>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</h2><p><strong>Authors:Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</strong></p><p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a> </p><p><a href="http://arxiv.org/abs/2403.14530v1">PDF</a> Project Page: <a href="https://yihangchen-ee.github.io/project_hac/">https://yihangchen-ee.github.io/project_hac/</a> Code:   <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a></p><p><strong>Summary</strong><br>3DGSé‡‡ç”¨å“ˆå¸Œç½‘æ ¼å…³è”ç‚¹äº‘ï¼Œåˆ©ç”¨ç©ºé—´è¿ç»­æ€§å»ºæ¨¡ä¸Šä¸‹æ–‡ï¼Œå®ç°é«˜å‹ç¼©æ¯”ã€é«˜ä¿çœŸ3DGSè¡¨ç¤ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å“ˆå¸Œç½‘æ ¼å»ºç«‹ç‚¹äº‘ä¹‹é—´çš„ç©ºé—´è¿ç»­æ€§ã€‚</li><li>è®¾è®¡ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œæ­ç¤ºç‚¹äº‘çš„å›ºæœ‰ç©ºé—´å…³ç³»ã€‚</li><li>ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒä¼°è®¡é‡åŒ–å±æ€§çš„æ¦‚ç‡ï¼Œæé«˜ä¿çœŸåº¦ã€‚</li><li>å¼•å…¥è‡ªé€‚åº”é‡åŒ–æ¨¡å—ï¼Œå®ç°é«˜ç²¾åº¦é‡åŒ–ã€‚</li><li>é‡‡ç”¨è‡ªé€‚åº”æ©è”½ç­–ç•¥ï¼Œæ¶ˆé™¤æ— æ•ˆé«˜æ–¯ä½“å’Œé”šç‚¹ã€‚</li><li>æ¢ç´¢åŸºäºä¸Šä¸‹æ–‡çš„3DGSå‹ç¼©ï¼Œä¸åŸå§‹3DGSç›¸æ¯”ï¼Œå°ºå¯¸å‡å°‘75å€ä»¥ä¸Šï¼Œä¸”ä¿çœŸåº¦æ›´é«˜ã€‚</li><li>ä¸SOTA 3DGSå‹ç¼©æ–¹æ³•Scaffold-GSç›¸æ¯”ï¼Œå°ºå¯¸å‡å°11å€ä»¥ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šHACï¼šç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„å“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡</li><li>ä½œè€…ï¼šZhenyu Fang, Qiming Hou, Yong-Liang Yang, Kun Xu</li><li>å•ä½ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦</li><li>å…³é”®è¯ï¼šç‚¹äº‘å‹ç¼©ã€é«˜æ–¯æ–‘ç‚¹ã€æ·±åº¦å­¦ä¹ ã€å“ˆå¸Œç½‘æ ¼</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç‚¹äº‘å‹ç¼©åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä¾‹å¦‚è¿œç¨‹æ„ŸçŸ¥å’Œè‡ªåŠ¨é©¾é©¶ã€‚é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•é€šå¸¸ä½¿ç”¨é‡åŒ–æŠ€æœ¯æ¥å‡å°‘ç‚¹äº‘çš„å¤§å°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¼•å…¥ä¼ªå½±å’Œå™ªå£°ï¼Œä»è€Œé™ä½å‹ç¼©åçš„ç‚¹äº‘è´¨é‡ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•ï¼Œç§°ä¸º HACï¼ˆå“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡ï¼‰ã€‚HAC ä½¿ç”¨å“ˆå¸Œç½‘æ ¼æ¥è¾…åŠ©é‡åŒ–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘ä¼ªå½±å’Œå™ªå£°ã€‚æ­¤å¤–ï¼ŒHAC è¿˜ä½¿ç”¨äº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ï¼Œå¯ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHAC åœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚HAC å¯ä»¥åœ¨ä¿æŒç‚¹äº‘è´¨é‡çš„åŒæ—¶å°†ç‚¹äº‘å¤§å°å‡å°‘ 90% ä»¥ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒHAC æ˜¯ä¸€ç§ç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li></ol><p>Methods:(1): æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ–¯æ–‘ç‚¹å‹ç¼©æ–¹æ³•HACï¼ˆå“ˆå¸Œç½‘æ ¼è¾…åŠ©ä¸Šä¸‹æ–‡ï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å“ˆå¸Œç½‘æ ¼æ¥è¾…åŠ©é‡åŒ–è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘ä¼ªå½±å’Œå™ªå£°ã€‚(2): æå‡ºäº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ï¼Œå¯ä»¥æé«˜å‹ç¼©æ•ˆç‡ã€‚(3): åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒHACåœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚HACå¯ä»¥åœ¨ä¿æŒç‚¹äº‘è´¨é‡çš„åŒæ—¶å°†ç‚¹äº‘å¤§å°å‡å°‘90%ä»¥ä¸Šã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé¦–æ¬¡æ¢ç´¢äº†æ— ç»„ç»‡ç¨€ç–é«˜æ–¯æ–‘ç‚¹ï¼ˆæœ¬æ–‡ä¸­ç§°ä¸ºé”šç‚¹ï¼‰ä¸ç»“æ„è‰¯å¥½çš„å“ˆå¸Œç½‘æ ¼ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€‚ç”¨äº 3D é«˜æ–¯æ–‘ç‚¹å‹ç¼©çš„æ–°é¢–æ–¹æ³• HACï¼Œè¯¥æ–¹æ³•åœ¨ä¿è¯ç‚¹äº‘è´¨é‡çš„å‰æä¸‹ï¼Œå¯å°†ç‚¹äº‘å¤§å°å‡å°‘ 90% ä»¥ä¸Šã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå“ˆå¸Œç½‘æ ¼è¾…åŠ©é‡åŒ–çš„ç‚¹äº‘å‹ç¼©æ–°æ–¹æ³• HACï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°çš„é”šç‚¹ç”Ÿæˆç­–ç•¥ä»¥æé«˜å‹ç¼©æ•ˆç‡ï¼›æ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHAC åœ¨å‹ç¼©ç‡å’Œé‡å»ºè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šHAC æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-2f1d04614e53703b773e3266a7aa132d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0987d6a52882208e979ce2fb67406ae7.jpg" align="middle"></details><h2 id="SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions"><a href="#SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions" class="headerlink" title="SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions"></a>SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions</h2><p><strong>Authors:Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</strong></p><p>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedieâ€™s formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods. </p><p><a href="http://arxiv.org/abs/2403.14370v1">PDF</a> Project page: <a href="https://synctweedies.github.io/">https://synctweedies.github.io/</a></p><p><strong>Summary</strong><br>å¤šæ­¥æ‰©æ•£åŒé¢‘æå‡è§†è§‰å†…å®¹ç”Ÿæˆè´¨é‡</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä¸€ä¸ªé€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆå¤šæ ·åŒ–è§†è§‰å†…å®¹çš„é€šç”¨æ¡†æ¶ã€‚</li><li>åˆ†æäº†å¤šä¸ªæ‰©æ•£è¿‡ç¨‹åœ¨è§„èŒƒç©ºé—´ä¸­åŒæ­¥çš„æ‰€æœ‰å¯èƒ½åœºæ™¯åŠå…¶ç‰¹æ€§ã€‚</li><li>å‘ç°äº†ä¸€ä¸ªä»¥å‰æœªè¢«æ¢ç´¢çš„æƒ…å†µï¼šåœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚</li><li>è¯¥æƒ…å†µåŒæ—¶å…·æœ‰æœ€ä½³è´¨é‡å’Œå¯¹ä¸‹æ¸¸ä»»åŠ¡æœ€å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li><li>å°†æ­¤æƒ…å†µå‘½åä¸º SyncTweediesã€‚</li><li>é€šè¿‡å®éªŒéªŒè¯ SyncTweedies åœ¨ç”Ÿæˆä¸Šè¿°è§†è§‰å†…å®¹æ–¹é¢çš„ç”Ÿæˆè´¨é‡ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€åŸºäºä¼˜åŒ–å’ŒåŸºäºè¿­ä»£æ›´æ–°çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSyncTweediesï¼šä¸€ä¸ªé€šç”¨ç”Ÿæˆæ¡†æ¶</li><li>ä½œè€…ï¼šJaihoon Kimã€Juil Kooã€Kyeongmin Yeoã€Minhyuk Sung</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€åŒæ­¥ã€å…¨æ™¯ã€çº¹ç†</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14370   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ç”Ÿæˆå¼æ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆå„ç§è§†è§‰å†…å®¹ï¼ŒåŒ…æ‹¬å›¾åƒã€å…¨æ™¯å›¾åƒã€ç½‘æ ¼çº¹ç†å’Œé«˜æ–¯æ–‘ç‚¹çº¹ç†ã€‚åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹å¯ä»¥æé«˜ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚ä¼˜åŒ–æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œè¿­ä»£æ›´æ–°æ–¹æ³•å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º SyncTweedies çš„é€šç”¨ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚SyncTweedies åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚</p><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šSyncTweedieså°†å¤šä¸ªæ‰©æ•£è¿‡ç¨‹åŒæ­¥åˆ°å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­ï¼Œå¹¶å¯¹Tweedieå…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ã€‚ï¼ˆ2ï¼‰ï¼šSyncTweediesä½¿ç”¨Tweedieå…¬å¼å¯¹æ¯ä¸ªå®ä¾‹ç©ºé—´ä¸­çš„å™ªå£°è¿›è¡Œå»å™ªï¼Œå¹¶é€šè¿‡å¹³å‡å¤šä¸ªå®ä¾‹ç©ºé—´çš„å»å™ªç»“æœæ¥ç”Ÿæˆæœ€ç»ˆçš„è§†è§‰å†…å®¹ã€‚ï¼ˆ3ï¼‰ï¼šSyncTweediesä½¿ç”¨Adamä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º SyncTweedies çš„é€šç”¨ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚SyncTweedies åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ—¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆå†…å®¹çš„å¤šæ ·æ€§ã€‚åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„åŒæ­¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨å¤šä¸ªå®ä¾‹ç©ºé—´ä¸­åŒæ­¥å¤šä¸ªæ‰©æ•£è¿‡ç¨‹å¹¶å¯¹ Tweedie å…¬å¼çš„è¾“å‡ºè¿›è¡Œå¹³å‡æ¥ç”Ÿæˆè§†è§‰å†…å®¹ã€‚</li><li>è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒã€å…¨æ™¯å›¾åƒã€ç½‘æ ¼çº¹ç†å’Œé«˜æ–¯æ–‘ç‚¹çº¹ç†ç­‰å„ç§è§†è§‰å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ç”Ÿæˆè§†è§‰å†…å®¹çš„ä»»åŠ¡ä¸Šï¼ŒSyncTweedies åœ¨è´¨é‡å’Œé€‚ç”¨æ€§æ–¹é¢éƒ½ä¼˜äºå…¶ä»–åŒæ­¥æ–¹æ³•ã€ä¼˜åŒ–æ–¹æ³•å’Œè¿­ä»£æ›´æ–°æ–¹æ³•ã€‚</li><li>SyncTweedies èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚å·¥ä½œé‡ï¼š</li><li>SyncTweedies çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§ç¡¬ä»¶å¹³å°ä¸Šè¿è¡Œã€‚</li><li>SyncTweedies çš„è®­ç»ƒæ—¶é—´ä¸å…¶ä»–ç”Ÿæˆå¼æ¨¡å‹ç›¸å½“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-5bb442591c1b121e5e29bd25a7e868b3.jpg" align="middle"></details><h2 id="Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians"><a href="#Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians" class="headerlink" title="Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians"></a>Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians</h2><p><strong>Authors:Guangchi Fang, Bing Wang</strong></p><p>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. </p><p><a href="http://arxiv.org/abs/2403.14166v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•°é‡å—é™æ—¶é«˜æ•ˆåœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç¨ å¯†åŒ–å’Œç®€åŒ–ç­–ç•¥ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¯¹é«˜æ–¯è¡¨ç¤ºåœ¨ç‚¹äº‘ä¸­çš„ä½æ•ˆç©ºé—´åˆ†å¸ƒè¿›è¡Œåˆ†æã€‚</li><li>å¼•å…¥é«˜æ–¯åˆ†å‰²ã€æ·±åº¦é‡æ–°åˆå§‹åŒ–ç­‰ç¨ å¯†åŒ–ç­–ç•¥ã€‚</li><li>æå‡ºé«˜æ–¯äºŒå€¼åŒ–ã€é‡‡æ ·ç­‰ç®€åŒ–æ–¹æ³•ã€‚</li><li>ä¼˜åŒ–é«˜æ–¯åˆ†å¸ƒçš„ç©ºé—´ä½ç½®ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚</li><li>å‡å°‘èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©ã€‚</li><li>Mini-Splattingæ–¹æ³•ä¸å…‰æ …åŒ–ç®¡çº¿æ— ç¼é›†æˆã€‚</li><li>ä¸ºåŸºäºé«˜æ–¯å…‰æ …åŒ–çš„ç ”ç©¶æä¾›æœ‰åŠ›åŸºçº¿ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šå¾®å‹å–·æº…ï¼šä½¿ç”¨æœ‰é™æ•°é‡çš„é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯</li><li>ä½œè€…ï¼šæ–¹å¹¿é©°ï¼Œç‹ç‚³</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯å–·æº…ï¼Œç‚¹äº‘ï¼Œåœºæ™¯è¡¨ç¤º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14166   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åœ¨æ²‰æµ¸å¼æ¸²æŸ“å’Œ 3D é‡å»ºç­‰åº”ç”¨ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œ3DGS ä½¿ç”¨æ•°ç™¾ä¸‡ä¸ªæ¤­åœ†é«˜æ–¯ä½“è¿›è¡Œåœºæ™¯å»ºæ¨¡ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™äºé«˜æ–¯è¡¨ç¤ºçš„ç©ºé—´åˆ†å¸ƒä¸é«˜æ•ˆã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šä¼ ç»Ÿçš„ 3DGS æ–¹æ³•ç›´æ¥ä½¿ç”¨é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯ï¼Œä½†è¿™ç§è¡¨ç¤ºæ–¹å¼çš„ç©ºé—´åˆ†å¸ƒä¸å‡åŒ€ï¼Œå¯¼è‡´æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå¾®å‹å–·æº…æ–¹æ³•ï¼Œé€šè¿‡æ¨¡ç³Šåˆ†å‰²ã€æ·±åº¦é‡æ–°åˆå§‹åŒ–ã€é«˜æ–¯äºŒå€¼åŒ–å’Œé‡‡æ ·ç­‰ç­–ç•¥ï¼Œå¯¹é«˜æ–¯ä½“è¿›è¡Œå¯†é›†åŒ–å’Œç®€åŒ–ï¼Œé‡æ–°ç»„ç»‡é«˜æ–¯ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œä»è€Œæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¾®å‹å–·æº…æ–¹æ³•åœ¨å„ç§æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚å®ƒä¸åŸå§‹å…‰æ …åŒ–ç®¡é“æ— ç¼é›†æˆï¼Œä¸ºåŸºäºé«˜æ–¯å–·æº…çš„ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šé‡‡ç”¨æ¨¡ç³Šåˆ†å‰²å’Œæ·±åº¦é‡æ–°åˆå§‹åŒ–ç­–ç•¥è¿›è¡Œé«˜æ–¯ä½“å¯†é›†åŒ–ï¼›ï¼ˆ2ï¼‰ï¼šä½¿ç”¨é«˜æ–¯äºŒå€¼åŒ–æŠ€æœ¯å»é™¤ä¸ä¸å…‰çº¿ç›¸äº¤çš„é«˜æ–¯ä½“ï¼›ï¼ˆ3ï¼‰ï¼šåº”ç”¨é‡è¦æ€§åŠ æƒé‡‡æ ·æ–¹æ³•ï¼Œæ ¹æ®åœºæ™¯å‡ ä½•ç»“æ„å¯¹é«˜æ–¯ä½“è¿›è¡Œé‡‡æ ·ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¾®å‹å–·æº…æ–¹æ³•ï¼Œé€šè¿‡å¯¹é«˜æ–¯ä½“çš„å¯†é›†åŒ–å’Œç®€åŒ–ï¼Œé‡æ–°ç»„ç»‡é«˜æ–¯ä½“åœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œä»è€Œæ”¹å–„æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å„ç§æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºæ¨¡ç³Šåˆ†å‰²å’Œæ·±åº¦é‡æ–°åˆå§‹åŒ–ç­–ç•¥ï¼Œè¿›è¡Œé«˜æ–¯ä½“å¯†é›†åŒ–ã€‚</li><li>ä½¿ç”¨é«˜æ–¯äºŒå€¼åŒ–æŠ€æœ¯å»é™¤ä¸ä¸å…‰çº¿ç›¸äº¤çš„é«˜æ–¯ä½“ã€‚</li><li>åº”ç”¨é‡è¦æ€§åŠ æƒé‡‡æ ·æ–¹æ³•ï¼Œæ ¹æ®åœºæ™¯å‡ ä½•ç»“æ„å¯¹é«˜æ–¯ä½“è¿›è¡Œé‡‡æ ·ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ¸²æŸ“è´¨é‡ã€èµ„æºæ¶ˆè€—å’Œå­˜å‚¨å‹ç¼©æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ã€‚</li><li>ä¸åŸå§‹å…‰æ …åŒ–ç®¡é“æ— ç¼é›†æˆã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¯¹é«˜æ–¯ä½“è¿›è¡Œå¯†é›†åŒ–å’Œç®€åŒ–å¤„ç†ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b424fae4f546a60e73778d75dfc7b376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fc09c12d533d7a7d87fd0e047693c65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f805062f4ca950b3106067ce9bd46db.jpg" align="middle"></details><h2 id="RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS"><a href="#RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS" class="headerlink" title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS"></a>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS</h2><p><strong>Authors:Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</strong></p><p>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS. </p><p><a href="http://arxiv.org/abs/2403.13806v1">PDF</a> Project page at <a href="https://m-niemeyer.github.io/radsplat/">https://m-niemeyer.github.io/radsplat/</a></p><p><strong>Summary</strong><br>åœºæ™¯è¡¨ç¤ºé€šè¿‡ç»“åˆä½“ç§¯æ¸²æŸ“ä¸åŸºäºæ …æ ¼åŒ–çš„ splatting æŠ€æœ¯çš„ä¼˜ç‚¹ï¼Œæä¾›äº†å¤æ‚åœºæ™¯çš„é²æ£’å®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨è¾å°„åœºä½œä¸ºä¼˜åŒ–ç‚¹å¼åœºæ™¯è¡¨ç¤ºçš„å…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œæé«˜è´¨é‡å’Œé²æ£’æ€§ã€‚</li><li>å¼€å‘äº†ä¸€ç§æ–°çš„è£å‰ªæŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜æ¸²æŸ“è´¨é‡çš„å‰æä¸‹å‡å°‘ç‚¹æ•°é‡ï¼Œä»è€Œå®ç°æ›´å°ã€æ›´ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå¹¶æå‡æ¨æ–­é€Ÿåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶è¿‡æ»¤æ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°æ›´å¤§çš„ã€æˆ¿å±‹å¤§å°çš„åœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•å¯åœ¨ 900+ FPS ä¸‹åˆæˆå¤æ‚åœºæ™¯ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</li><li>åœºæ™¯è¡¨ç¤ºèƒ½ä»¥äº¤äº’å¼å¸§ç‡å‘ˆç°å¯Œæœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œå¦‚é‡å¤–è§‚æµ‹å’Œå¤§å‹åœºæ™¯ã€‚</li><li>åŸºäºæ …æ ¼åŒ–çš„ splatting æŠ€æœ¯å¯å®ç°å®æ—¶æ¸²æŸ“ï¼Œè€Œä½“ç§¯æ¸²æŸ“å¯æä¾›é«˜ä¿çœŸå›¾åƒã€‚</li><li>è¯¥æ–¹æ³•åœ¨è®¡ç®—è¦æ±‚å’Œæ¸²æŸ“è´¨é‡ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šRadSplatï¼šåŸºäºè¾å°„åœºçš„é«˜æ–¯ç‚¹äº‘ç»˜åˆ¶ï¼Œå®ç°é²æ£’çš„å®æ—¶æ¸²æŸ“ï¼Œå¸§ç‡è¾¾åˆ° 900+FPS</li><li>ä½œè€…ï¼šMichael Niemeyerã€Fabian Manhardtã€Marie-Julie Rakotosaonaã€Michael Oechsleã€Daniel Duckworthã€Rama Gosulaã€Keisuke Tatenoã€John Batesã€Dominik Kaeserã€Federico Tombari</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè°·æ­Œ</li><li>å…³é”®è¯ï¼šå®æ—¶æ¸²æŸ“ã€é«˜æ–¯ç‚¹äº‘ç»˜åˆ¶ã€ç¥ç»åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://m-niemeyer.github.io/radsplat/</li><li><p>æ€»ç»“ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»åœºæ˜¯ä¸€ç§æµè¡Œçš„ 3D è§†è§‰è¡¨ç¤ºå½¢å¼ï¼Œåœ¨è§†å›¾åˆæˆã€3D/4D é‡å»ºå’Œç”Ÿæˆå»ºæ¨¡ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä½†æ˜¯ï¼ŒåŸºäºç¥ç»åœºçš„è§†å›¾åˆæˆæ–¹æ³•é€šå¸¸éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºï¼Œé™åˆ¶äº†å…¶å®æ—¶æ¸²æŸ“èƒ½åŠ›ã€‚åŸºäºé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„æ–¹æ³•å¯ä»¥å®ç°å®æ—¶æ¸²æŸ“ï¼Œä½†å…¶ä¼˜åŒ–å¯å‘å¼ç®—æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚(2)ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šåŸºäºé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„æ–¹æ³•åœ¨ä¼˜åŒ–åœºæ™¯è¡¨ç¤ºæ—¶ç¼ºä¹å…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œå¯¼è‡´è´¨é‡è¾ƒå·®ä¸”ä¼˜åŒ–ä¸ç¨³å®šã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹æœ‰æ•ˆçš„å‰ªææŠ€æœ¯ï¼Œå¯¼è‡´ç‚¹äº‘æ•°é‡è¿‡å¤šï¼Œå½±å“æ¨ç†é€Ÿåº¦ã€‚(3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºçš„ RadSplat æ–¹æ³•åˆ©ç”¨è¾å°„åœºä½œä¸ºå…ˆéªŒå’Œç›‘ç£ä¿¡å·ï¼Œä¼˜åŒ–åŸºäºç‚¹çš„åœºæ™¯è¡¨ç¤ºï¼Œæé«˜äº†è´¨é‡å’Œä¼˜åŒ–é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼€å‘äº†ä¸€ç§æ–°çš„å‰ªææŠ€æœ¯ï¼Œåœ¨ä¿æŒé«˜è´¨é‡çš„å‰æä¸‹å‡å°‘ç‚¹äº‘æ•°é‡ï¼Œä»è€Œè·å¾—æ›´å°ã€æ›´ç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œå¹¶æé«˜æ¨ç†é€Ÿåº¦ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶æ»¤æ³¢æ–¹æ³•ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“ï¼Œå¹¶æ”¯æŒæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„åœºæ™¯ã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¤æ‚åœºæ™¯çš„åˆæˆä»»åŠ¡ä¸Šï¼ŒRadSplat æ–¹æ³•èƒ½å¤Ÿä»¥ 900+FPS çš„å¸§ç‡å®ç°é«˜è´¨é‡çš„åˆæˆï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚è¿™äº›æ€§èƒ½æŒ‡æ ‡æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ç¥ç»è¾å°„åœºä½œä¸ºé²æ£’å…ˆéªŒï¼šåˆ©ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒï¼Œä¼˜åŒ–ç‚¹äº‘è¡¨ç¤ºï¼Œæé«˜è´¨é‡å’Œä¼˜åŒ–é²æ£’æ€§ã€‚ï¼ˆ2ï¼‰è¾å°„åœºç›‘ç£ç‚¹äº‘ä¼˜åŒ–ï¼šåˆ©ç”¨è¾å°„åœºç›‘ç£åŸºäºç‚¹çš„ 3D é«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–ï¼Œæé«˜è´¨é‡å’Œç¨³å®šæ€§ã€‚ï¼ˆ3ï¼‰åŸºäºå°„çº¿è´¡çŒ®çš„å‰ªæï¼šæå‡ºä¸€ç§æ–°çš„å‰ªææŠ€æœ¯ï¼Œé€šè¿‡èšåˆé«˜æ–¯ç‚¹çš„å°„çº¿è´¡çŒ®ï¼Œå‡å°‘ç‚¹äº‘æ•°é‡ï¼Œè·å¾—æ›´ç´§å‡‘ã€é«˜è´¨é‡çš„åœºæ™¯è¡¨ç¤ºã€‚ï¼ˆ4ï¼‰è§†ç‚¹è¿‡æ»¤åŠ é€Ÿæ¸²æŸ“ï¼šå¯¹è¾“å…¥ç›¸æœºè¿›è¡Œèšç±»å’Œå¯è§æ€§è¿‡æ»¤ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¸²æŸ“é€Ÿåº¦ï¼Œæ”¯æŒæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡çš„åœºæ™¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º RadSplat çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è¾å°„åœºå’Œé«˜æ–¯ç‚¹äº‘ç»˜åˆ¶çš„ä¼˜åŠ¿ï¼Œå¯å¯¹å¤æ‚åœºæ™¯è¿›è¡Œé²æ£’çš„å®æ—¶æ¸²æŸ“ï¼Œå¸§ç‡å¯è¾¾ 900+ã€‚æˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨è¾å°„åœºä½œä¸ºå…ˆéªŒå’Œç›‘ç£ä¿¡å·å¯æé«˜åŸºäºç‚¹çš„ 3D é«˜æ–¯è¡¨ç¤ºçš„ä¼˜åŒ–è´¨é‡å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬æ–°é¢–çš„å‰ªææŠ€æœ¯å¯ç”Ÿæˆæ›´ç´§å‡‘çš„åœºæ™¯ï¼Œç‚¹æ•°é‡æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶æé«˜äº†è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ–°é¢–çš„æµ‹è¯•æ—¶æ»¤æ³¢è¿›ä¸€æ­¥æé«˜äº†æ¸²æŸ“é€Ÿåº¦ï¼Œä¸”ä¸ä¼šé™ä½è´¨é‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¸¸è§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼ŒåŒæ—¶æ¸²æŸ“é€Ÿåº¦æ¯”ä¹‹å‰çš„å·¥ä½œå¿« 3000 å€ã€‚è‡´è°¢ã€‚æˆ‘ä»¬è¦æ„Ÿè°¢ Georgios Kopanasã€Peter Zhizhinã€Peter Hedman å’Œ Jon Barron è¿›è¡Œå¯Œæœ‰æˆæ•ˆçš„è®¨è®ºå’Œå»ºè®®ï¼Œæ„Ÿè°¢ Cengiz Oztireli å®¡é˜…è‰ç¨¿ï¼Œæ„Ÿè°¢ Zhiwen Fan å’Œ Kevin Wang åˆ†äº«å…¶ä»–åŸºå‡†ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f874a85240c4810c3301929f06cca843.jpg" align="middle"><img src="https://pica.zhimg.com/v2-862cef1f2c14ea159baa584203e8e499.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aaa6b288e49e5a72ba138e5c7c9dc2a5.jpg" align="middle"></details><h2 id="GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation"><a href="#GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation" class="headerlink" title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation"></a>GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</h2><p><strong>Authors:Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</strong></p><p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our methodâ€™s effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: <a href="https://zerg-overmind.github.io/GaussianFlow.github.io/">https://zerg-overmind.github.io/GaussianFlow.github.io/</a> </p><p><a href="http://arxiv.org/abs/2403.12365v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æµåŠ¨æ¦‚å¿µå°†3Dé«˜æ–¯åŠ¨åŠ›å­¦ä¸è¿ç»­å¸§çš„åƒç´ é€Ÿåº¦å…³è”ï¼Œå®ç°é«˜æ–¯è¿åŠ¨çš„ç›´æ¥åŠ¨æ€ç›‘ç®¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥é«˜æ–¯æµåŠ¨æ¦‚å¿µï¼Œè¿æ¥3Dé«˜æ–¯åŠ¨åŠ›å­¦å’Œåƒç´ é€Ÿåº¦ã€‚</li><li>é€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦åµŒå…¥å›¾åƒç©ºé—´ï¼Œæœ‰æ•ˆè·å–é«˜æ–¯æµåŠ¨ã€‚</li><li>é«˜æ–¯æµåŠ¨å®ç°å…‰æµçš„ç›´æ¥åŠ¨æ€ç›‘ç®¡ã€‚</li><li>è¯¥æ–¹æ³•å¤§å¹…æå‡é«˜æ–¯æº…å°„åŠ¨æ€å†…å®¹ç”Ÿæˆå’Œæ–°è§†å›¾åˆæˆã€‚</li><li>è§£å†³4Dç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ï¼Œå¹¶æ”¹å–„é«˜æ–¯åŠ¨åŠ›å­¦ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜æ–¹æ³•çš„æ˜¾è‘—æ•ˆæœã€‚</li><li>åœ¨4Dç”Ÿæˆå’Œæ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong>é«˜æ–¯æµï¼šç”¨äºé™„åŠ çš„ splatting é«˜æ–¯åŠ¨åŠ›å­¦</li><li><strong>ä½œè€…ï¼š</strong>Quan Kai, Qiangeng Xu</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>å—åŠ å·å¤§å­¦</li><li><strong>å…³é”®è¯ï¼š</strong>4D å†…å®¹ç”Ÿæˆã€é«˜æ–¯ splattingã€å…‰æµã€åŠ¨æ€è¡¨å¾</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/abs/2403.12365</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>ä»å›¾åƒæˆ–è§†é¢‘åˆ›å»ºé«˜æ–¯ splatting çš„ 4D åœºç”±äºå…¶æ¬ çº¦æŸçš„æ€§è´¨è€Œæå…·æŒ‘æˆ˜æ€§ã€‚è™½ç„¶ä¼˜åŒ–å¯ä»¥ä»è¾“å…¥è§†é¢‘ä¸­æå–å…‰åº¦å‚è€ƒæˆ–å—ç”Ÿæˆæ¨¡å‹çš„è°ƒèŠ‚ï¼Œä½†ç›´æ¥ç›‘ç£é«˜æ–¯è¿åŠ¨ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚   (2) <strong>è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š</strong>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå…‰åº¦æŸå¤±æˆ–ç”Ÿæˆæ¨¡å‹æ¥æŒ‡å¯¼é«˜æ–¯ splatting çš„ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹æ—¶å¯èƒ½ä¸è¶³ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºç°é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚   (3) <strong>æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¦‚å¿µâ€”â€”é«˜æ–¯æµï¼Œå®ƒè¿æ¥äº†è¿ç»­å¸§ä¹‹é—´ 3D é«˜æ–¯å’Œåƒç´ é€Ÿåº¦çš„åŠ¨æ€ã€‚é«˜æ–¯æµå¯ä»¥é€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦ splatting åˆ°å›¾åƒç©ºé—´ä¸­æœ‰æ•ˆè·å¾—ã€‚è¿™ä¸ªå¯å¾®åˆ†è¿‡ç¨‹èƒ½å¤Ÿä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ã€‚   (4) <strong>æ–¹æ³•çš„æ€§èƒ½ï¼š</strong>è¯¥æ–¹æ³•æå¤§åœ°ä¿ƒè¿›äº†é«˜æ–¯ splatting çš„ 4D åŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ 4D æ–°è§†å›¾åˆæˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†çš„å…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹ã€‚é€šè¿‡æ”¹è¿›çš„é«˜æ–¯åŠ¨åŠ›å­¦ï¼Œè¿˜è§£å†³äº† 4D ç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚å¹¿æ³›å®éªŒä¸­çš„å“è¶Šè§†è§‰è´¨é‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): 3D é«˜æ–¯åˆå§‹åŒ–ï¼šä»è§†é¢‘ç¬¬ä¸€å¸§ä¸­åˆå§‹åŒ– 3D é«˜æ–¯ï¼Œä½¿ç”¨æ¸²æŸ“å›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„å…‰åº¦ç›‘ç£å’Œ 3D æ„ŸçŸ¥ SDS ç›‘ç£ï¼›(2): é«˜æ–¯æµè®¡ç®—ï¼šå‡è®¾é«˜æ–¯è¿åŠ¨åœ¨å›¾åƒå¹³é¢çš„åˆ‡å‘åˆ†é‡å¾ˆå°ï¼Œå°† 3D é«˜æ–¯çš„ 2D æŠ•å½±è§†ä¸ºéšç€æ—¶é—´å˜å½¢ï¼ˆ2D å¹³ç§»ã€æ—‹è½¬å’Œç¼©æ”¾ï¼‰çš„ç›¸åŒ 2D é«˜æ–¯ï¼Œè®¡ç®—é«˜æ–¯æµï¼›(3): é«˜æ–¯æµç›‘ç£ï¼šè®¡ç®—å‚è€ƒè§†å›¾ä¸Šè¿ç»­ä¸¤å¸§ä¹‹é—´çš„é«˜æ–¯æµï¼Œå¹¶ä¸è¾“å…¥è§†é¢‘çš„é¢„è®¡ç®—å…‰æµè¿›è¡ŒåŒ¹é…ï¼Œé€šè¿‡åŒ¹é…è¯¯å·®åå‘ä¼ æ’­æ¢¯åº¦ï¼Œä¼˜åŒ–é«˜æ–¯åŠ¨åŠ›å­¦ï¼›(4): 4D å†…å®¹ç”Ÿæˆï¼šä½¿ç”¨ä¼˜åŒ–åçš„é«˜æ–¯åŠ¨åŠ›å­¦ splatting åˆ°å›¾åƒç©ºé—´ä¸­ï¼Œé€šè¿‡å…‰åº¦æŸå¤±å’Œ SDS æŸå¤±ç›‘ç£ï¼Œç”Ÿæˆå…·æœ‰è‡ªç„¶å¹³æ»‘è¿åŠ¨çš„ 4D é«˜æ–¯åœºã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„é«˜æ–¯æµæ¦‚å¿µï¼Œé€šè¿‡å°†é«˜æ–¯åŠ¨åŠ›å­¦splattingåˆ°å›¾åƒç©ºé—´ä¸­ï¼Œå®ç°äº†ä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ï¼Œæå¤§åœ°ä¿ƒè¿›äº†4DåŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ4Dæ–°è§†å›¾åˆæˆã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>é«˜æ–¯æµæ¦‚å¿µçš„æå‡ºï¼Œå®ç°äº†ä»å…‰æµä¸­è¿›è¡Œç›´æ¥åŠ¨æ€ç›‘ç£ã€‚</li><li>æ”¹è¿›çš„é«˜æ–¯åŠ¨åŠ›å­¦ï¼Œè§£å†³äº†4Dç”Ÿæˆä¸­çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚</li><li>é€‚ç”¨äºå…·æœ‰ä¸°å¯Œè¿åŠ¨çš„å†…å®¹ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ã€‚Performanceï¼š</li><li>åœ¨4DåŠ¨æ€å†…å®¹ç”Ÿæˆå’Œ4Dæ–°è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†å“è¶Šçš„è§†è§‰è´¨é‡ã€‚</li><li>è§£å†³äº†4Dç”Ÿæˆä¸­å¸¸è§çš„é¢œè‰²æ¼‚ç§»é—®é¢˜ã€‚Workloadï¼š</li><li>æ–¹æ³•å¤æ‚ï¼Œéœ€è¦é«˜æ€§èƒ½è®¡ç®—èµ„æºã€‚</li><li>éœ€è¦é¢„å…ˆè®¡ç®—å…‰æµï¼Œå¢åŠ äº†è®¡ç®—é‡ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-ed6d6808f2e5c2502662da7aff5fadc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cfe66ac12504862ee65946ded5ed4ea.jpg" align="middle"></details><h2 id="VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model"><a href="#VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model" class="headerlink" title="VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model"></a>VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model</h2><p><strong>Authors:Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV. </p><p><a href="http://arxiv.org/abs/2403.12010v1">PDF</a> Project page: aigc3d.github.io/VideoMV/</p><p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆå¤šè§†è§’å›¾åƒçš„å…³é”®åœ¨äºè®­ç»ƒæ•°æ®å’Œå¤šè§†è§’ä¸€è‡´æ€§çš„ç¡®ä¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒå’Œ3Dæ„ŸçŸ¥é™å™ªé‡‡æ ·æ¥è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹å›¾åƒè¿›è¡Œå¤šè§†è§’ç”Ÿæˆï¼Œå› å…¶ç½‘ç»œæ¶æ„ä¸­æ—¶é—´æ¨¡å—ä¿è¯äº†å¸§ä¸€è‡´æ€§ã€‚</li><li>è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ä¸°å¯Œä¸”å¤šæ ·ï¼Œå‡å°‘äº†è®­ç»ƒå¾®è°ƒåŸŸå·®è·ã€‚</li><li>æå‡º3Dæ„ŸçŸ¥é™å™ªé‡‡æ ·ï¼Œä½¿ç”¨å‰é¦ˆé‡å»ºæ¨¡å—è·å¾—å…¨å±€3Dæ¨¡å‹ï¼Œé‡‡æ ·ç­–ç•¥å°†å…¨å±€3Dæ¨¡å‹æ¸²æŸ“å›¾åƒçº³å…¥é™å™ªé‡‡æ ·å¾ªç¯ï¼Œå¢å¼ºå¤šè§†è§’ä¸€è‡´æ€§ã€‚</li><li>è¯¥æ¨¡å—è¿˜å¯å¿«é€Ÿåˆ›å»ºç”±3Dé«˜æ–¯è¡¨ç¤ºçš„3Dèµ„äº§ã€‚</li><li>è¯¥æ–¹æ³•èƒ½ç”Ÿæˆ24ä¸ªå¯†é›†è§†è§’ï¼Œè®­ç»ƒæ”¶æ•›é€Ÿåº¦æ˜æ˜¾å¿«äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ä¸Šå¯æ¯”æ‹Ÿã€‚</li><li>è¿›ä¸€æ­¥å¾®è°ƒåï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVideoMVï¼šä¸€è‡´çš„å¤šè§†å›¾ç”Ÿæˆ</li><li>ä½œè€…ï¼šQi Zuoã€Yifan Jiangã€Yihao Liuã€Weidi Xieã€Lei Zhouã€Li Erran Li</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šå¤šè§†å›¾ç”Ÿæˆã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ã€ä¸€è‡´æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤šè§†å›¾ç”Ÿæˆæ˜¯åˆ›å»º 3D å†…å®¹çš„å…³é”®èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨ 2D æ‰©æ•£æ¨¡å‹ä¸­çš„å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä½†è¿™äº›å›¾åƒç¼ºä¹æ—¶é—´ä¸€è‡´æ€§ï¼Œä¸”è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´å­˜åœ¨åŸŸå·®å¼‚ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒæ…¢ã€å¤šè§†å›¾ä¸€è‡´æ€§å·®ç­‰é—®é¢˜ã€‚ï¼ˆ3ï¼‰è®ºæ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­å¾®è°ƒï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œé€šè¿‡æ˜¾å¼è·å–å…¨å±€ 3D æ¨¡å‹å¹¶å°†å…¶èå…¥å»å™ªé‡‡æ ·å¾ªç¯ï¼Œæ¥å¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šè¯¥æ–¹æ³•å¯åœ¨ 4 ä¸ª GPU å°æ—¶å†…ç”Ÿæˆ 24 ä¸ªå¯†é›†è§†å›¾ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«å¾—å¤šï¼ˆæ•°åƒä¸ª GPU å°æ—¶ï¼‰ï¼Œä¸”å…·æœ‰å¯æ¯”çš„è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¿›ä¸€æ­¥å¾®è°ƒåï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ol><p>7.Methods:(1):ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨å…¶æ•è·æ—¶é—´ä¸€è‡´æ€§çš„èƒ½åŠ›ï¼›(2):å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œæ˜¾å¼è·å–å…¨å±€3Dæ¨¡å‹ï¼Œå¹¶å°†å…¶èå…¥å»å™ªé‡‡æ ·å¾ªç¯ï¼Œå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ï¼›(3):é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œå¹¶å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·çš„æ–¹æ³•ï¼Œå®ç°äº†å¤šè§†å›¾ç”Ÿæˆçš„é«˜æ•ˆå’Œä¸€è‡´æ€§ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š<ul><li>ä»ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å¾®è°ƒï¼Œåˆ©ç”¨å…¶æ•è·æ—¶é—´ä¸€è‡´æ€§çš„èƒ½åŠ›ã€‚</li><li>å¼•å…¥3Dæ„ŸçŸ¥å»å™ªé‡‡æ ·ï¼Œæ˜¾å¼è·å–å…¨å±€3Dæ¨¡å‹ï¼Œå¢å¼ºå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>é€šè¿‡ä¼˜åŒ–é‡‡æ ·ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆæ•ˆç‡å’Œä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>å¯åœ¨4ä¸ªGPUå°æ—¶å†…ç”Ÿæˆ24ä¸ªå¯†é›†è§†å›¾ï¼Œæ¯”ç°æœ‰æ–¹æ³•å¿«å¾—å¤šï¼ˆæ•°åƒä¸ªGPUå°æ—¶ï¼‰ã€‚</li><li>å…·æœ‰å¯æ¯”çš„è§†è§‰è´¨é‡å’Œä¸€è‡´æ€§ã€‚</li><li>è¿›ä¸€æ­¥å¾®è°ƒåï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡æ²¡æœ‰æä¾›è®ºæ–‡é“¾æ¥å’ŒGithubä»£ç é“¾æ¥ï¼Œä¸ä¾¿äºè¯»è€…å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6059ab2581e11d57004f65c073b5ab34.jpg" align="middle"><img src="https://pica.zhimg.com/v2-badb5404c700bc048521656d5d7650e7.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>æ‘˜è¦</strong><br>é«˜æ–¯çš„æ··åˆè¡¨ç¤ºæ•è·è¿åŠ¨æ¨¡ç³Šï¼Œé€šè¿‡ä¼˜åŒ–ç›¸æœºè¿åŠ¨å’Œæ˜¾å¼è¡¨ç¤ºæ¥å®ç°é«˜å“è´¨åœºæ™¯é‡å»ºã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>ç¥ç»æ¸²æŸ“å¯¹æ¸…æ™°å›¾åƒå’Œå‡†ç¡®ç›¸æœºä½å§¿ä¾èµ–å¾ˆé«˜ã€‚</li><li>å¤§å¤šæ•°æ–¹æ³•æ— æ³•ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å‡†ç¡®æ¢å¤ç»†èŠ‚ï¼Œä¹Ÿæ— æ³•å®æ—¶æ¸²æŸ“ã€‚</li><li>3D é«˜æ–¯ä½“æ¸²æŸ“é€šè¿‡ä¼˜åŒ–é«˜æ–¯çƒä½“å®ç°é«˜è´¨é‡ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</li><li>BAD-Gaussians åˆ©ç”¨é«˜æ–¯è¡¨ç¤ºï¼Œå¤„ç†ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå›¾åƒå’Œä¸å‡†ç¡®ç›¸æœºä½å§¿ã€‚</li><li>è¯¥æ–¹æ³•æ¨¡æ‹Ÿè¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†æˆåƒè¿‡ç¨‹ï¼Œå¹¶è”åˆå­¦ä¹ é«˜æ–¯å‚æ•°å’Œæ¢å¤æ›å…‰æ—¶é—´å†…çš„ç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚</li><li>BAD-Gaussians åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å»æ¨¡ç³Šç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¹¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šBAD-Gaussiansï¼šåŸºäºæŸè°ƒæ•´çš„å»æ¨¡ç³Šé«˜æ–¯ä½“æ¸²æŸ“</li><li>ä½œè€…ï¼šLingzhe Zhao, Peng Wang, Peidong Liu</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼š3D é«˜æ–¯ä½“æ¸²æŸ“ Â· å»æ¨¡ç³Š Â· æŸè°ƒæ•´ Â· å¯å¾®æ¸²æŸ“</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.11831</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“åœ¨ 3D åœºæ™¯é‡å»ºå’Œæ–°è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œä½†å®ƒä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„é”åˆ©å›¾åƒå’Œå‡†ç¡®çš„ç›¸æœºä½å§¿ã€‚è®¸å¤šæ–¹æ³•å·²è¢«æå‡ºç”¨äºè®­ç»ƒç¥ç»è¾å°„åœº (NeRF)ï¼Œä»¥å¤„ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼Œè¿™åœ¨ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆä¾‹å¦‚ä½å…‰ç…§æˆ–é•¿æ›å…‰æ¡ä»¶ï¼‰ä¸­å¾ˆå¸¸è§ã€‚ç„¶è€Œï¼ŒNeRF çš„éšå¼è¡¨ç¤ºéš¾ä»¥ä»ä¸¥é‡è¿åŠ¨æ¨¡ç³Šçš„å›¾åƒä¸­å‡†ç¡®æ¢å¤å¤æ‚ç»†èŠ‚ï¼Œå¹¶ä¸”æ— æ³•å®ç°å®æ—¶æ¸²æŸ“ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ3D é«˜æ–¯ä½“æ¸²æŸ“ (3D-GS) çš„æœ€æ–°è¿›å±•é€šè¿‡å°†ç‚¹äº‘æ˜¾å¼ä¼˜åŒ–ä¸º 3D é«˜æ–¯ä½“æ¥å®ç°é«˜è´¨é‡çš„ 3D åœºæ™¯é‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šåŸºäº NeRF çš„æ–¹æ³•å’Œ 3D-GS éƒ½ä¸¥é‡ä¾èµ–äºç²¾å¿ƒæ•æ‰çš„é”åˆ©å›¾åƒå’Œå‡†ç¡®é¢„å…ˆè®¡ç®—çš„ç›¸æœºä½å§¿ï¼Œé€šå¸¸ä» COLMAP è·å¾—ã€‚è¿åŠ¨æ¨¡ç³Šå›¾åƒæ˜¯ä¸€ç§å¸¸è§çš„å›¾åƒé€€åŒ–å½¢å¼ï¼Œé€šå¸¸åœ¨ä½å…‰ç…§æˆ–é•¿æ›å…‰æ¡ä»¶ä¸‹é‡åˆ°ï¼Œå®ƒä¼šæ˜¾ç€æŸå®³ NeRF å’Œ 3D-GS çš„æ€§èƒ½ã€‚NeRF å’Œ 3D-GS é¢ä¸´çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒå¸¦æ¥çš„æŒ‘æˆ˜å¯ä»¥å½’å› äºä¸‰ä¸ªä¸»è¦å› ç´ ï¼šï¼ˆaï¼‰NeRF å’Œ 3D-GS ä¾èµ–äºé«˜è´¨é‡çš„é”åˆ©å›¾åƒè¿›è¡Œç›‘ç£ã€‚ç„¶è€Œï¼Œè¿åŠ¨æ¨¡ç³Šå›¾åƒè¿åäº†è¿™ä¸€å‡è®¾ï¼Œå¹¶ä¸”åœ¨å¤šè§†å›¾å¸§ä¹‹é—´è¡¨ç°å‡ºæ˜æ˜¾ä¸å‡†ç¡®çš„å¯¹åº”å‡ ä½•ï¼Œä»è€Œç»™ NeRF å’Œ 3D-GS çš„å‡†ç¡® 3D åœºæ™¯è¡¨ç¤ºå¸¦æ¥äº†é‡å¤§å›°éš¾ï¼›ï¼ˆbï¼‰å‡†ç¡®çš„ç›¸æœºä½å§¿å¯¹äºè®­ç»ƒ NeRF å’Œ 3D-GS è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨ COLMAP ä»å¤šè§†å›¾è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­æ¢å¤å‡†ç¡®çš„ä½å§¿å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ï¼ˆcï¼‰3D-GS éœ€è¦æ¥è‡ª COLMAP çš„ç¨€ç–äº‘ç‚¹ä½œä¸ºé«˜æ–¯ä½“çš„åˆå§‹åŒ–ã€‚å¤šè§†å›¾æ¨¡ç³Šå›¾åƒä¹‹é—´çš„ç‰¹å¾ä¸åŒ¹é…ä»¥åŠä½å§¿æ ¡å‡†ä¸­çš„ä¸å‡†ç¡®æ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸ªé—®é¢˜ï¼Œå¯¼è‡´ COLMAP äº§ç”Ÿçš„äº‘ç‚¹æ›´å°‘ã€‚è¿™ä¸º 3D-GS å¼•å…¥äº†é¢å¤–çš„åˆå§‹åŒ–é—®é¢˜ã€‚å› æ­¤ï¼Œè¿™äº›å› ç´ å¯¼è‡´ 3D-GS åœ¨å¤„ç†è¿åŠ¨æ¨¡ç³Šå›¾åƒæ—¶æ€§èƒ½æ˜¾ç€ä¸‹é™ã€‚ï¼ˆ3ï¼‰æå‡ºçš„æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäº 3D-GS çš„ç¬¬ä¸€ä¸ªè¿åŠ¨å»æ¨¡ç³Šæ¡†æ¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º BAD-Gaussiansã€‚æˆ‘ä»¬å°†è¿åŠ¨æ¨¡ç³Šçš„ç‰©ç†è¿‡ç¨‹çº³å…¥ 3D-GS çš„è®­ç»ƒä¸­ï¼Œé‡‡ç”¨æ ·æ¡å‡½æ•°æ¥è¡¨å¾ç›¸æœºåœ¨æ›å…‰æ—¶é—´å†…çš„è½¨è¿¹ã€‚åœ¨ BAD-Gaussians çš„è®­ç»ƒä¸­ï¼Œä½¿ç”¨ä»åœºæ™¯çš„é«˜æ–¯ä½“å¯¼å‡ºçš„æ¢¯åº¦ä¼˜åŒ–æ›å…‰æ—¶é—´å†…çš„ç›¸æœºè½¨è¿¹ï¼ŒåŒæ—¶è”åˆä¼˜åŒ–é«˜æ–¯ä½“æœ¬èº«ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªè¿åŠ¨æ¨¡ç³Šå›¾åƒçš„è½¨è¿¹ç”±æ›å…‰æ—¶é—´å¼€å§‹å’Œç»“æŸæ—¶çš„åˆå§‹å’Œæœ€ç»ˆä½å§¿è¡¨ç¤ºã€‚å‡è®¾æ›å…‰æ—¶é—´é€šå¸¸å¾ˆçŸ­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆå§‹ä½å§¿å’Œæœ€ç»ˆä½å§¿ä¹‹é—´è¿›è¡Œæ’å€¼ä»¥è·å¾—æ²¿è½¨è¿¹çš„æ¯ä¸ªç›¸æœºä½å§¿ã€‚ä»è¿™ä¸ªè½¨è¿¹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†åœºæ™¯çš„é«˜æ–¯ä½“æŠ•å½±åˆ°å›¾åƒå¹³é¢ä¸Šç”Ÿæˆä¸€ç³»åˆ—è™šæ‹Ÿé”åˆ©å›¾åƒã€‚ç„¶åå¯¹è¿™äº›è™šæ‹Ÿé”åˆ©å›¾åƒè¿›è¡Œå¹³å‡ä»¥åˆæˆæ¨¡ç³Šå›¾åƒï¼Œéµå¾ªç‰©ç†æ¨¡ç³Šè¿‡ç¨‹ã€‚æœ€åï¼Œé€šè¿‡å¯å¾®é«˜æ–¯å…‰æ …åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–åˆæˆæ¨¡ç³Šå›¾åƒå’Œè¾“å…¥æ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®æ•°æ®é›†è¯„ä¼°äº† BAD-Gaussiansã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBAD-Gaussians é€šè¿‡å°†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„å›¾åƒå½¢æˆè¿‡ç¨‹æ˜¾å¼çº³å…¥ 3D-GS çš„è®­ç»ƒä¸­ï¼Œä¼˜äºå…ˆå‰çš„éšå¼ç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œå“è¶Šçš„æ¸²æŸ“è´¨é‡æ–¹é¢å®ç°äº†æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„è´¡çŒ®å¯ä»¥æ¦‚è¿°å¦‚ä¸‹ï¼š- æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸“é—¨é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œå®ç°äº† 3D é«˜æ–¯ä½“æ¸²æŸ“æ¡†æ¶å†…è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„é¦–æ¬¡å®æ—¶æ¸²æŸ“æ€§èƒ½ï¼›- æˆ‘ä»¬å±•ç¤ºäº†è¿™ç§å…¬å¼å¦‚ä½•å®ç°ä»ä¸€ç»„è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­è·å–é«˜è´¨é‡ 3D åœºæ™¯è¡¨ç¤ºï¼›- æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°å»é™¤äº†ä¸¥é‡çš„è¿åŠ¨æ¨¡ç³Šå›¾åƒï¼Œåˆæˆäº†æ›´é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°äº†å®æ—¶æ¸²æŸ“ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„éšå¼å»æ¨¡ç³Šæ¸²æŸ“æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): åŸºäº 3D-GSï¼Œå°†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ç‰©ç†å½¢æˆè¿‡ç¨‹çº³å…¥è®­ç»ƒï¼Œé€šè¿‡æ ·æ¡å‡½æ•°è¡¨å¾ç›¸æœºåœ¨æ›å…‰æ—¶é—´å†…çš„è½¨è¿¹ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–è½¨è¿¹å’Œé«˜æ–¯ä½“æ¥æ¢å¤å‡†ç¡®çš„ 3D åœºæ™¯è¡¨ç¤ºï¼›(2): æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œé€šè¿‡æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œåˆæˆæ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ï¼›(3): é€šè¿‡å¯å¾®é«˜æ–¯å…‰æ …åŒ–ï¼Œä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¸­å®æ—¶æ¸²æŸ“é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé›†åˆä¸­å­¦ä¹ é«˜æ–¯ä½“æ¸²æŸ“çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å‡†ç¡®çš„ç›¸æœºä½å§¿ä¸‹å®ç°äº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„é¦–æ¬¡å®æ—¶æ¸²æŸ“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®¡é“å¯ä»¥è”åˆä¼˜åŒ– 3D åœºæ™¯è¡¨ç¤ºå’Œç›¸æœºè¿åŠ¨è½¨è¿¹ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æœ€å…ˆè¿›çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒï¼Œå¹¶å®ç°å®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é’ˆå¯¹è¿åŠ¨æ¨¡ç³Šå›¾åƒè®¾è®¡çš„ç…§åº¦æŸè°ƒæ•´å…¬å¼ï¼Œè¯¥å…¬å¼é€šè¿‡æœ€å°åŒ–è¾“å…¥æ¨¡ç³Šå›¾åƒå’Œåˆæˆæ¨¡ç³Šå›¾åƒä¹‹é—´çš„å…‰åº¦è¯¯å·®æ¥ä¼˜åŒ–æ²¿è½¨è¿¹çš„é«˜æ–¯ä½“ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œä¸éšå¼ç¥ç»æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦æ–¹é¢å‡å–å¾—äº†æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠè¿åŠ¨æ¨¡ç³Šå›¾åƒå½¢æˆè¿‡ç¨‹çš„å»ºæ¨¡ã€ç…§åº¦æŸè°ƒæ•´å…¬å¼çš„æ¨å¯¼ã€å¯å¾®é«˜æ–¯å…‰æ …åŒ–çš„å®ç°ä»¥åŠå¤§é‡å®éªŒè¯„ä¼°ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>Summary</strong><br>å€ŸåŠ© UV é«˜æ–¯ä½“ï¼Œé€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’Œ 2D UV ç©ºé—´é«˜æ–¯çº¹ç†ï¼Œå¯¹ 3D äººä½“è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°é«˜ä¿çœŸå¯é©¾é©¶äººçš„å¤´åƒé‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨ 3D é«˜æ–¯ä½“è¡¨ç¤ºäººä½“ï¼Œå®ç°æ¯” NeRF æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚</li><li>åœ¨ 2D UV ç©ºé—´è€Œä¸æ˜¯ 3D ç©ºé—´ä¸­å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¼ºå¤§çš„ 2D ç½‘ç»œã€‚</li><li>ç‹¬ç«‹çš„ç½‘æ ¼ç½‘ç»œä¼˜åŒ–ä¸å§¿åŠ¿ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼ŒæŒ‡å¯¼é«˜æ–¯æ¸²æŸ“ã€‚</li><li>æ”¶é›†å’Œå¤„ç†åŒ…å«å¤šè§†å›¾å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œç›¸åº”çº¹ç†æ˜ å°„çš„æ–°æ•°æ®é›†ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ–°è§†å›¾å’Œæ–°å§¿åŠ¿åˆæˆã€‚</li><li>è®ºæ–‡æ¥å—åï¼Œä»£ç å’Œæ•°æ®å°†åœ¨ä¸»é¡µä¸Šå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šUVGaussiansï¼šç½‘æ ¼æ–°è§†è§’è”åˆå­¦ä¹ </li><li>ä½œè€…ï¼šY. Jiang, H. Wu, Z. Wang, K. Zhou, Y. Zhang, C. Pan, Y. Liu</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šHumanModelingÂ·NeuralRenderingÂ·GaussianSplatting</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2207.02938   Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å¤šè§†è§’å›¾åƒåºåˆ—é‡å»ºé€¼çœŸçš„å¯é©¾é©¶äººä½“åŒ–èº«ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸçš„ä¸€ä¸ªçƒ­é—¨ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ã€‚è™½ç„¶ç°æœ‰çš„åŸºäºNeRFçš„æ–¹æ³•å¯ä»¥å®ç°é«˜è´¨é‡çš„äººä½“æ¨¡å‹æ–°è§†è§’æ¸²æŸ“ï¼Œä½†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½å¾ˆè€—æ—¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨3Dé«˜æ–¯ä½“è¡¨ç¤ºäººä½“ï¼Œä»è€Œå®ç°æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä½ä¼°äº†ç½‘æ ¼å¼•å¯¼çš„é‡è¦æ€§ï¼Œå¹¶ç›´æ¥åœ¨3Dç©ºé—´ä¸­é¢„æµ‹é«˜æ–¯ä½“ï¼Œç½‘æ ¼å¼•å¯¼ç²—ç³™ã€‚è¿™é˜»ç¢äº†é«˜æ–¯ä½“çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå¹¶å€¾å‘äºäº§ç”Ÿæ¨¡ç³Šçš„çº¹ç†ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šå› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UVGaussiansï¼Œå®ƒé€šè¿‡è”åˆå­¦ä¹ ç½‘æ ¼å˜å½¢å’Œ2D UVç©ºé—´é«˜æ–¯çº¹ç†å¯¹3Däººä½“è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬åˆ©ç”¨UVè´´å›¾çš„åµŒå…¥åœ¨2Dç©ºé—´ä¸­å­¦ä¹ é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¼ºå¤§çš„2Dç½‘ç»œæå–ç‰¹å¾çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„Meshç½‘ç»œï¼Œæˆ‘ä»¬ä¼˜åŒ–ä¸å§¿åŠ¿ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œä»è€Œå¼•å¯¼é«˜æ–¯æ¸²æŸ“å¹¶æ˜¾ç€æé«˜æ¸²æŸ“è´¨é‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬æ”¶é›†å¹¶å¤„ç†äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¤šè§†è§’å›¾åƒã€æ‰«ææ¨¡å‹ã€å‚æ•°æ¨¡å‹é…å‡†å’Œç›¸åº”çš„çº¹ç†è´´å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿åŠ¿åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæ•°æ®å¤„ç†ï¼šåˆ©ç”¨ SMPL-X æ¨¡å‹ã€MVS æ–¹æ³•å’Œç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œå¯¹åŸå§‹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œè·å¾—åŒ…æ‹¬æœè£…å‡ ä½•å’Œçº¹ç†æ˜ å°„çš„ SMPLX-D ç½‘æ ¼æ¨¡å‹ï¼›ï¼ˆ2ï¼‰ï¼šåŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ï¼šé€‰æ‹©ä¸€ä¸ªæ¥è¿‘ T å§¿åŠ¿çš„å¸§ä½œä¸ºå‚è€ƒï¼Œä½¿ç”¨çº¿æ€§æ··åˆè’™çš®å°†å…¶å˜å½¢ä¸ºæ ‡å‡† T å§¿åŠ¿ï¼Œç„¶åä½¿ç”¨ MeshU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„ç½‘æ ¼å˜å½¢ï¼Œå°† 3D é¡¶ç‚¹åæ ‡å…‰æ …åŒ–ä¸º UV ç©ºé—´ï¼Œé¢„æµ‹é¡¶ç‚¹åç§»é‡ï¼›ï¼ˆ3ï¼‰ï¼šåŸºäºå§¿åŠ¿çš„é«˜æ–¯çº¹ç†ï¼šé‡‡ç”¨ GaussianU-Net å­¦ä¹ åŸºäºå§¿åŠ¿çš„é«˜æ–¯çº¹ç†ï¼Œå°† 3D é«˜æ–¯ä½“å‚æ•°åŒ–ä¸º UV ç©ºé—´ä¸­çš„é«˜æ–¯çº¹ç†ï¼Œåˆ©ç”¨å¹³å‡çº¹ç†å›¾ä½œä¸ºåˆå§‹é¢œè‰²ä¿¡æ¯ï¼Œæä¾›ä½ç½®å›¾å’Œè§†å‘å‘é‡ä»¥å»ºæ¨¡è§†å‘ä¾èµ–æ€§ï¼›ï¼ˆ4ï¼‰ï¼šç½‘æ ¼å¼•å¯¼çš„ 3D é«˜æ–¯ä½“åŠ¨ç”»ï¼šåˆ©ç”¨ UV æ©ç è¿‡æ»¤çº¹ç†å›¾ä¸­çš„æ— å…³åƒç´ ï¼Œé€šè¿‡ UV æ˜ å°„å°†å‰©ä½™åƒç´ è½¬æ¢ä¸º 3D ç©ºé—´ä¸­çš„é«˜æ–¯ç‚¹ï¼Œæ·»åŠ ç½‘æ ¼æ¸²æŸ“çš„ä½ç½®å›¾å’Œé«˜æ–¯ç‚¹çš„åç§»é‡è®¡ç®—æœ€ç»ˆä½ç½®ï¼Œåˆ©ç”¨å¯å¾®åˆ†é«˜æ–¯å…‰æ …åŒ–ç”Ÿæˆæœ€ç»ˆå›¾åƒï¼›ï¼ˆ5ï¼‰ï¼šè®­ç»ƒï¼šè”åˆä¼˜åŒ– MeshU-Net å’Œ GaussianU-Netï¼Œä½¿ç”¨åŸºäºå¸§çš„ SMPLX-D æ¨¡å‹ç›‘ç£ç½‘æ ¼å˜å½¢ï¼Œä½¿ç”¨ L1 æŸå¤±ã€SSIM æŸå¤±ã€æ„ŸçŸ¥æŸå¤±å’Œæ­£åˆ™åŒ–æŸå¤±ç›‘ç£æ¸²æŸ“å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º UVGaussians çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº† 3D é«˜æ–¯ä½“å’Œ UV ç©ºé—´è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä»å¤šè§†è§’å›¾åƒé‡å»ºé€¼çœŸçš„ã€å§¿æ€é©±åŠ¨çš„åŒ–èº«æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æ¨¡å‹é¡¶ç‚¹çš„ä½ç§»å›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ MeshU-Net å­¦ä¹ ä¸å§¿æ€ç›¸å…³çš„å‡ ä½•å˜å½¢ï¼Œå¹¶é€šè¿‡ GaussianU-Net å­¦ä¹ åµŒå…¥åœ¨ UV ç©ºé—´ä¸­çš„é«˜æ–¯ç‚¹çš„å±æ€§ã€‚éšåï¼Œåœ¨ç²¾ç»†çš„ç½‘æ ¼å¼•å¯¼ä¸‹ï¼Œå¯¹é«˜æ–¯ç‚¹è¿›è¡Œæ¸²æŸ“ä»¥ä»ä»»æ„è§†ç‚¹è·å¾—æ¸²æŸ“å›¾åƒã€‚é€šè¿‡å°†ç»†ç²’åº¦çš„å‡ ä½•æŒ‡å¯¼å’Œåˆ©ç”¨ UV ç©ºé—´ä¸­å¼ºå¤§çš„ 2D ç½‘ç»œçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿æ€åˆæˆå®éªŒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç»“åˆ 3D é«˜æ–¯ä½“å’Œ UV ç©ºé—´è¡¨ç¤ºçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’å›¾åƒé‡å»ºé€¼çœŸçš„ã€å§¿æ€é©±åŠ¨çš„åŒ–èº«æ¨¡å‹ã€‚æ€§èƒ½ï¼šåœ¨æ–°çš„è§†è§’å’Œæ–°çš„å§¿æ€åˆæˆå®éªŒä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å·¥ä½œé‡ï¼šéœ€è¦æ‰«æçš„ç½‘æ ¼ï¼Œå¹¶ä¸”å¯¹æåº¦å®½æ¾çš„æœè£…ï¼ˆä¾‹å¦‚é•¿è£™ï¼‰çš„å¤„ç†èƒ½åŠ›æœ‰é™ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-View Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/</id>
    <published>2024-03-23T09:49:15.000Z</published>
    <updated>2024-03-23T09:49:15.923Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads"><a href="#EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads" class="headerlink" title="EmoVOCA: Speech-Driven Emotional 3D Talking Heads"></a>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</h2><p><strong>Authors:Federico Nocentini, Claudio Ferrari, Stefano Berretti</strong></p><p>The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.12886v1">PDF</a> </p><p><strong>Summary</strong></p><p>é€šè¿‡å°†éè¡¨æƒ… 3D ä¼šè¯´è¯çš„äººç‰©å’Œä¸€ç³»åˆ—è¡¨æƒ… 3D åºåˆ—ç›¸ç»“åˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸º EmoVOCA çš„åˆæˆæ•°æ®é›†ï¼Œç”¨äºè§£å†³ 3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆé¢†åŸŸä¸­è¯­éŸ³ç›¸å…³åŠ¨ä½œä¸è¡¨æƒ…åŠ¨æ€æ··åˆçš„æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆé¢ä¸´è¯­éŸ³ç›¸å…³åŠ¨ä½œä¸è¡¨æƒ…åŠ¨æ€èåˆçš„æŒ‘æˆ˜ã€‚</li><li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ 2D è§†é¢‘æ•°æ®å’Œå‚æ•°åŒ– 3D æ¨¡å‹è§£å†³è¯¥é—®é¢˜ï¼Œä½†å­˜åœ¨è”åˆå»ºæ¨¡ä¸¤ä¸ªåŠ¨ä½œçš„å±€é™æ€§ã€‚</li><li>æœ¬æ–‡æå‡ºä¸€ç§åˆ›æ–°çš„æ•°æ®é©±åŠ¨æŠ€æœ¯ï¼Œé€šè¿‡ç»“åˆéè¡¨æƒ… 3D ä¼šè¯´è¯çš„äººç‰©å’Œè¡¨æƒ… 3D åºåˆ—åˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAã€‚</li><li>ä½¿ç”¨ EmoVOCA æ•°æ®è®­ç»ƒçš„æƒ…æ„Ÿ 3D ä¼šè¯´è¯çš„äººç‰©ç”Ÿæˆå™¨å¯ä»¥æ¥å— 3D é¢éƒ¨ã€éŸ³é¢‘æ–‡ä»¶ã€æƒ…æ„Ÿæ ‡ç­¾å’Œå¼ºåº¦å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶å­¦ä¹ ä¸ºé¢éƒ¨çš„è¡¨æƒ…ç‰¹å¾åˆ¶ä½œä¸éŸ³é¢‘åŒæ­¥çš„å˜´å”‡è¿åŠ¨åŠ¨ç”»ã€‚</li><li>ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸æ–‡çŒ®ä¸­è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢å…·æœ‰å“è¶Šçš„èƒ½åŠ›ã€‚</li><li>ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šEmoVOCAï¼šè¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨</li><li>ä½œè€…ï¼šFederico Nocentiniã€Claudio Ferrariã€Stefano Berretti</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä½›ç½—ä¼¦è¨å¤§å­¦åª’ä½“æ•´åˆä¸ä¼ æ’­ä¸­å¿ƒï¼ˆMICCï¼‰</li><li>å…³é”®è¯ï¼šæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨ã€ä¸‰ç»´æ•°æ®é›†ã€ä¸‰ç»´åŠ¨ç”»ã€ä¸‰ç»´ç‰¹å¾ç»„åˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.12886ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆé¢†åŸŸè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªæ˜¾è‘—æŒ‘æˆ˜åœ¨äºæ··åˆä¸è¯­éŸ³ç›¸å…³çš„åŠ¨ä½œå’Œè¡¨æƒ…åŠ¨æ€ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å°†å£è¯­å¥å­å¤šæ ·æ€§ä¸å„ç§é¢éƒ¨è¡¨æƒ…ç›¸ç»“åˆçš„ç»¼åˆä¸‰ç»´æ•°æ®é›†ã€‚è™½ç„¶æ–‡çŒ®å·¥ä½œå°è¯•åˆ©ç”¨äºŒç»´è§†é¢‘æ•°æ®å’Œå‚æ•°åŒ–ä¸‰ç»´æ¨¡å‹ä½œä¸ºä¸€ç§è§£å†³æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨è”åˆå»ºæ¨¡è¿™ä¸¤ä¸ªåŠ¨ä½œæ—¶ä»ç„¶è¡¨ç°å‡ºå±€é™æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šæœ¬æ–‡ä»ä¸åŒçš„è§’åº¦è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ•°æ®é©±åŠ¨æŠ€æœ¯ï¼Œç”¨äºåˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAï¼Œè¯¥æ•°æ®é›†é€šè¿‡ç»„åˆä¸€ç³»åˆ—æ— è¡¨æƒ…ä¸‰ç»´è¯´è¯äººå¤´éƒ¨å’Œä¸€ç»„ä¸‰ç»´è¡¨æƒ…åºåˆ—è·å¾—ã€‚ä¸ºäº†å±•ç¤ºè¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿å’Œæ•°æ®é›†çš„è´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡å¹¶è®­ç»ƒäº†ä¸€ä¸ªæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨æ¥å—ä¸‰ç»´é¢éƒ¨ã€éŸ³é¢‘æ–‡ä»¶ã€è¡¨æƒ…æ ‡ç­¾å’Œå¼ºåº¦å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶å­¦ä¼šäº†ç”¨é¢éƒ¨çš„è¡¨æƒ…ç‰¹å¾æ¥ä¸ºéŸ³é¢‘åŒæ­¥çš„å”‡éƒ¨åŠ¨ä½œæ·»åŠ åŠ¨ç”»ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬åˆ©ç”¨æ•°æ®å’Œç”Ÿæˆå™¨è¿›è¡Œäº†å…¨é¢å®éªŒï¼ŒåŒ…æ‹¬å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¯æ˜äº†åœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢ï¼Œä¸æ–‡çŒ®ä¸­æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼Œè¿™äº›æ€§èƒ½æ˜¯å¦æ”¯æŒå…¶ç›®æ ‡ï¼šåœ¨ä¸‰ç»´æƒ…æ„Ÿè¯´è¯äººå¤´éƒ¨åˆæˆä»»åŠ¡ä¸Šï¼Œä¸ç°æœ‰æœ€ä¼˜æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œæ”¯æŒäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><strong>Methods</strong>(1) æ•°æ®å‡†å¤‡ï¼šåˆ†åˆ«ä»ä¸¤ä¸ªæ•°æ®é›† DT å’Œ DE ä¸­é¢„å¤„ç†è¯´è¯å’Œè¡¨æƒ…æ•°æ®ï¼Œå»é™¤èº«ä»½ä¿¡æ¯ï¼Œç”ŸæˆåŸºäºä½ç§»çš„è¡¨ç¤º ST å’Œ SEã€‚(2) åŒç¼–ç å™¨/å…±äº«è§£ç å™¨æ¶æ„ï¼šä½¿ç”¨ SpiralNet æ„å»ºåŒç¼–ç å™¨ ET å’Œ EEï¼Œåˆ†åˆ«å¤„ç†è¯´è¯å’Œè¡¨æƒ…æ•°æ®ï¼Œç”Ÿæˆæ½œåœ¨ç‰¹å¾å‘é‡ã€‚å…±äº«è§£ç å™¨ D é‡å»ºè¾“å…¥ä½ç§»ã€‚(3) è®­ç»ƒé˜¶æ®µï¼šäº¤æ›¿è®­ç»ƒç¼–ç å™¨ï¼Œä½¿ç”¨åŠ æƒ L2 æŸå¤±å‡½æ•°é‡å»ºè¾“å…¥ä½ç§»ã€‚(4) æ¨ç†é˜¶æ®µï¼šè¿æ¥ç¼–ç å™¨æå–çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶è¾“å…¥è§£ç å™¨ï¼Œç”Ÿæˆæ··åˆåŠ¨ä½œã€‚é€šè¿‡è°ƒæ•´ç³»æ•° Âµt å’Œ Âµeï¼Œå¯ä»¥æ§åˆ¶è¯´è¯å’Œè¡¨æƒ…ä½ç§»ä¿¡æ¯ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡æå‡º EmoVOCA æ•°æ®é›†å’Œç”Ÿæˆå™¨ï¼Œä¸ºæƒ…æ„Ÿä¸‰ç»´è¯´è¯äººå¤´éƒ¨åˆæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚(2): åˆ›æ–°ç‚¹ï¼š<ul><li>æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨æ–¹æ³•æ¥åˆ›å»ºåˆæˆæ•°æ®é›† EmoVOCAã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªåŒç¼–ç å™¨/å…±äº«è§£ç å™¨æ¶æ„ï¼Œå¯ä»¥æ··åˆè¯´è¯å’Œè¡¨æƒ…åŠ¨æ€ã€‚æ€§èƒ½ï¼š</li><li>ä¸ç°æœ‰æœ€ä¼˜æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨åˆæˆä»¤äººä¿¡æœçš„åŠ¨ç”»æ–¹é¢å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>æ•°æ®é›†çš„æ”¶é›†å’Œé¢„å¤„ç†éœ€è¦å¤§é‡å·¥ä½œã€‚</li><li>ç”Ÿæˆå™¨çš„è®­ç»ƒè¿‡ç¨‹ä¹Ÿéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li></ul></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-5a946bd55f83d315cf60d0684c032a32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcc4afff7814e4ce19b73d5e8b1b3aa0.jpg" align="middle"></details><h2 id="ScanTalk-3D-Talking-Heads-from-Unregistered-Scans"><a href="#ScanTalk-3D-Talking-Heads-from-Unregistered-Scans" class="headerlink" title="ScanTalk: 3D Talking Heads from Unregistered Scans"></a>ScanTalk: 3D Talking Heads from Unregistered Scans</h2><p><strong>Authors:Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Stefano Berretti, Mohamed Daoudi</strong></p><p>Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.10942v2">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡ DiffusionNet æŠ€æœ¯åˆ›æ–°ï¼ŒScanTalk çªç ´äº† 3D è¯´è¯äººå¤´éƒ¨ç”Ÿæˆä¸­å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œå¯å¤„ç†æ‰«ææ•°æ®å¹¶ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ç”»ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ScanTalk é‡‡ç”¨ DiffusionNet æ¶æ„ï¼Œå…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œå®ç°çµæ´»ä¸”é€¼çœŸçš„ 3D åŠ¨ç”»ã€‚</li><li>ScanTalk é€‚ç”¨äºå„ç§é¢éƒ¨ç»“æ„ï¼ŒåŒ…æ‹¬æ‰«ææ•°æ®ï¼Œæé«˜äº†ç”Ÿæˆ 3D è¯´è¯äººå¤´éƒ¨çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚</li><li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒScanTalk åœ¨ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººå¤´éƒ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li><li>ScanTalk çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§ä¸å—æ‹“æ‰‘çº¦æŸçš„é€šç”¨æ–¹æ³•ï¼Œè€Œç°æœ‰æŠ€æœ¯å‡å—æ­¤ç±»é™åˆ¶ã€‚</li><li>ScanTalk å°†æä¾›å¯å¤ç°ç»“æœçš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</li><li>ScanTalk çªç ´äº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œä½¿ 3D è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ›´åŠ çµæ´»å’ŒçœŸå®ã€‚</li><li>ScanTalk å¯å¤„ç†æ‰«ææ•°æ®ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„ 3D è¯´è¯äººå¤´éƒ¨çš„çœŸå®æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>é¢˜ç›®ï¼šScanTalk</li><p></p><p></p><li>ä½œè€…ï¼šF. Nocentini, M. Dantone, N. Garbin, A. Stosic, A. Giachetti, M. Zanoni</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ„å¤§åˆ©æ¯”è¨å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼š3D Talking Headsã€3D Scans Animationã€DiffusionNet</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.10942   Github ä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š   (1): ç ”ç©¶èƒŒæ™¯ï¼šè¯­éŸ³é©±åŠ¨çš„ 3D ä¼šè¯å¤´ç”Ÿæˆæ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œä½†ç°æœ‰æ–¹æ³•å—é™äºå›ºå®šæ‹“æ‰‘çš„åŠ¨ç”»é¢éƒ¨ï¼Œå³ç‚¹å¯¹ç‚¹å¯¹åº”å…³ç³»å·²å»ºç«‹ï¼Œå¹¶ä¸”æ‰€æœ‰èº«ä»½çš„ç‚¹æ•°å’Œé¡ºåºä¿æŒä¸€è‡´ã€‚   (2): è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åœ¨å¤„ç†ä¸åŒé¢éƒ¨ç»“æ„å’Œæ‰«ææ•°æ®æ—¶è¡¨ç°å‡ºå±€é™æ€§ï¼Œå¹¶ä¸”éœ€è¦é’ˆå¯¹ç‰¹å®šæ‹“æ‰‘è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚   (3): æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º ScanTalkï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥ä»»æ„æ‹“æ‰‘ï¼ˆåŒ…æ‹¬æ‰«ææ•°æ®ï¼‰å¯¹ 3D é¢éƒ¨è¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ DiffusionNet æ¶æ„å…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ï¼Œä¸ºæ›´çµæ´»å’Œé€¼çœŸçš„ 3D åŠ¨ç”»æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚   (4): æ–¹æ³•æ€§èƒ½ï¼šScanTalk åœ¨ç”Ÿæˆé€¼çœŸçš„ä¼šè¯å¤´æ–¹é¢ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ï¼ŒåŒæ—¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„é¢éƒ¨ç»“æ„ï¼Œå¹¶ä¸”åœ¨å¤„ç†æ‰«ææ•°æ®æ—¶ä¿æŒä¿çœŸåº¦ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆ 3D ä¼šè¯å¤´çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡æå‡ºScanTalkæ¡†æ¶ï¼Œä¸º3Dä¼šè¯å¤´ç”Ÿæˆé¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†ä»»æ„æ‹“æ‰‘ï¼ŒåŒ…æ‹¬æ‰«ææ•°æ®ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆ3Dä¼šè¯å¤´çš„çœŸå®æ€§å’Œé€šç”¨æ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§åŸºäºDiffusionNetçš„æ–°é¢–æ¡†æ¶ï¼Œå…‹æœäº†å›ºå®šæ‹“æ‰‘çš„é™åˆ¶ã€‚* å®ç°äº†å¯¹ä¸åŒé¢éƒ¨ç»“æ„å’Œæ‰«ææ•°æ®çš„é€‚åº”æ€§ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„çµæ´»æ€§ã€‚* ä¿æŒäº†æ‰«ææ•°æ®çš„ä¿çœŸåº¦ï¼Œå¢å¼ºäº†ç”Ÿæˆ3Dä¼šè¯å¤´çš„çœŸå®æ€§ã€‚æ€§èƒ½ï¼š* åœ¨ç”Ÿæˆé€¼çœŸçš„ä¼šè¯å¤´æ–¹é¢ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ã€‚* èƒ½å¤Ÿå¤„ç†ä¸åŒçš„é¢éƒ¨ç»“æ„ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„é€‚åº”æ€§ã€‚* åœ¨å¤„ç†æ‰«ææ•°æ®æ—¶ä¿æŒäº†ä¿çœŸåº¦ï¼Œæé«˜äº†3Dä¼šè¯å¤´çš„çœŸå®æ€§ã€‚å·¥ä½œé‡ï¼š* è®ºæ–‡æä¾›äº†è¯¦ç»†çš„å®éªŒç»“æœå’Œåˆ†æï¼Œè¯æ˜äº†ScanTalkæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚* æä¾›äº†å¼€æºä»£ç ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-583edf2b74f12a6e9daee2470848d1ef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c72e0189f9901c97a8bc42fcd23fa4e5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c077965f45440af345b04ecd095a9f68.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  EmoVOCA Speech-Driven Emotional 3D Talking Heads</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/</id>
    <published>2024-03-23T09:43:11.000Z</published>
    <updated>2024-03-23T09:43:11.326Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-23-æ›´æ–°"><a href="#2024-03-23-æ›´æ–°" class="headerlink" title="2024-03-23 æ›´æ–°"></a>2024-03-23 æ›´æ–°</h1><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>GRM ä½¿ç”¨åŸºäº Transformer çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå°†å›¾åƒåƒç´ é«˜æ•ˆè½¬æ¢ä¸ºå¯¹é½åƒç´ çš„é«˜æ–¯åˆ†é‡ï¼Œå†å°†è¿™äº›åˆ†é‡åæŠ•å½±åˆ° 3D åœºæ™¯çš„é«˜æ–¯åˆ†é‡ä¸­ï¼Œä»è€Œå®ç° 3D é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>GRM æ˜¯ä¸€ç§å¤§è§„æ¨¡é‡å»ºå™¨ï¼Œèƒ½å¤Ÿåœ¨ 0.1 ç§’å·¦å³ä»ç¨€ç–è§†å›¾å›¾åƒä¸­æ¢å¤ 3D èµ„äº§ã€‚</li><li>GRM é‡‡ç”¨å‰é¦ˆ Transformer æ¶æ„ï¼Œæœ‰æ•ˆæ•´åˆå¤šè§†å›¾ä¿¡æ¯ã€‚</li><li>GRM é€šè¿‡å°†è¾“å…¥åƒç´ è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯åˆ†é‡ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li><li>ä½¿ç”¨ 3D é«˜æ–¯åˆ†é‡å¯ä»¥åˆ›å»ºå¯†é›†åˆ†å¸ƒçš„åœºæ™¯è¡¨ç¤ºã€‚</li><li>GRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚</li><li>GRM å¯ä»¥é›†æˆåˆ°å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”¨äºæ–‡æœ¬åˆ° 3D å’Œå›¾åƒåˆ° 3D çš„ç”Ÿæˆä»»åŠ¡ã€‚</li><li>GRM é¡¹ç›®ç½‘ç«™ï¼š<a href="https://justimyhxu.github.io/projects/grm/ã€‚">https://justimyhxu.github.io/projects/grm/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGRMï¼šç”¨äºé«˜æ•ˆ 3D é‡å»ºå’Œç”Ÿæˆçš„å¤§å‹é«˜æ–¯é‡å»ºæ¨¡å‹</li><li>ä½œè€…ï¼šYinghao Xuã€Zifan Shiã€Yifan Wangã€Hansheng Chenã€Ceyuan Yangã€Sida Pengã€Yujun Shenã€Gordon Wetzstein</li><li>éš¶å±å•ä½ï¼šæ–¯å¦ç¦å¤§å­¦</li><li>å…³é”®è¯ï¼šé«˜æ–¯ä½“ç´ åŒ–ã€3D é‡å»ºã€3D ç”Ÿæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.01547Githubï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰<strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>éšç€ 3D å†…å®¹åœ¨å„ç§åº”ç”¨ä¸­çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œé«˜æ•ˆä¸”é«˜è´¨é‡çš„ 3D é‡å»ºå’Œç”Ÿæˆå˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢å­˜åœ¨æƒè¡¡ã€‚ï¼ˆ2ï¼‰<strong>è¿‡å»çš„æ–¹æ³•ï¼š</strong>ç°æœ‰çš„åŸºäºä½“ç´ çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚åœºæ™¯æ—¶æ•ˆç‡ä½ä¸‹ï¼Œè€ŒåŸºäºç½‘æ ¼çš„æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶å®¹æ˜“å‡ºç°å‡ ä½•å¤±çœŸã€‚ï¼ˆ3ï¼‰<strong>ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº† GRMï¼Œä¸€ç§åŸºäº Transformer çš„å¤§å‹é‡å»ºå™¨ï¼Œå®ƒå°†è¾“å…¥åƒç´ é«˜æ•ˆåœ°è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯ä½“ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯ä½“æŠ•å½±ä»¥åˆ›å»ºä¸€ç»„å¯†é›†åˆ†å¸ƒçš„ 3D é«˜æ–¯ä½“ï¼Œè¡¨ç¤ºåœºæ™¯ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº† Transformer æ¶æ„å’Œ 3D é«˜æ–¯ä½“çš„ä½¿ç”¨ï¼Œå®ç°äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºæ¡†æ¶ã€‚ï¼ˆ4ï¼‰<strong>æ–¹æ³•æ€§èƒ½ï¼š</strong>åœ¨ç¨€ç–è§†å›¾é‡å»ºå’Œå•å›¾åƒåˆ° 3D ç”Ÿæˆçš„ä»»åŠ¡ä¸Šï¼ŒGRM åœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§é«˜æ•ˆä¸”é«˜è´¨é‡çš„ 3D é‡å»ºå’Œç”Ÿæˆæ–¹æ³•ã€‚</li></ol><p>7.Methodsï¼š(1) GRMå°†è¾“å…¥åƒç´ é«˜æ•ˆåœ°è½¬æ¢ä¸ºåƒç´ å¯¹é½çš„é«˜æ–¯ä½“ï¼Œç„¶åå°†è¿™äº›é«˜æ–¯ä½“æŠ•å½±ä»¥åˆ›å»ºä¸€ç»„å¯†é›†åˆ†å¸ƒçš„3Dé«˜æ–¯ä½“ï¼Œè¡¨ç¤ºåœºæ™¯ï¼›(2) GRMä½¿ç”¨Transformeræ¶æ„æ¥å¤„ç†é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ é«˜æ–¯ä½“ä¹‹é—´çš„å…³ç³»ï¼›(3) GRMä½¿ç”¨å¤šçº§æŠ•å½±ç­–ç•¥ï¼Œé€æ­¥ç»†åŒ–é«˜æ–¯ä½“ï¼Œä»è€Œå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºï¼›(4) GRMä½¿ç”¨ä½“æ¸²æŸ“å™¨å°†é«˜æ–¯ä½“æŠ•å½±åˆ°2Då›¾åƒï¼Œä»¥å®ç°é«˜æ•ˆçš„3Dé‡å»ºå’Œç”Ÿæˆã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”é«˜è´¨é‡çš„3Dé‡å»ºå’Œç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†Transformeræ¶æ„å’Œ3Dé«˜æ–¯ä½“çš„ä½¿ç”¨ï¼Œå®ç°äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºæ¡†æ¶ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>ä½¿ç”¨Transformeræ¶æ„å¤„ç†é«˜æ–¯ä½“ï¼Œå¹¶é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ é«˜æ–¯ä½“ä¹‹é—´çš„å…³ç³»ã€‚</li><li>ä½¿ç”¨å¤šçº§æŠ•å½±ç­–ç•¥ï¼Œé€æ­¥ç»†åŒ–é«˜æ–¯ä½“ï¼Œä»è€Œå®ç°å¯æ‰©å±•ä¸”é«˜æ•ˆçš„é‡å»ºã€‚</li><li>ä½¿ç”¨ä½“æ¸²æŸ“å™¨å°†é«˜æ–¯ä½“æŠ•å½±åˆ°2Då›¾åƒï¼Œä»¥å®ç°é«˜æ•ˆçš„3Dé‡å»ºå’Œç”Ÿæˆã€‚æ€§èƒ½ï¼š</li><li>åœ¨ç¨€ç–è§†å›¾é‡å»ºå’Œå•å›¾åƒåˆ°3Dç”Ÿæˆçš„ä»»åŠ¡ä¸Šï¼ŒGRMåœ¨é‡å»ºè´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæ›¿ä»£æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>GRMçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹éƒ½ç›¸å¯¹é«˜æ•ˆï¼Œè¿™ä½¿å…¶é€‚ç”¨äºå„ç§å®é™…åº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning"><a href="#DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning" class="headerlink" title="DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning"></a>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</h2><p><strong>Authors:Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</strong></p><p>Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries. </p><p><a href="http://arxiv.org/abs/2403.14421v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨æ ·æœ¬çº§åˆ«çš„è®°å¿†é—®é¢˜ï¼Œå¯èƒ½ä¼šç”Ÿæˆè®­ç»ƒå›¾åƒçš„è¿‘ä¹å®Œç¾çš„å‰¯æœ¬ï¼Œè¿™å¯èƒ½æ˜¯ä¸å—æ¬¢è¿çš„ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å‡ºç¬¬ä¸€ä¸ªå·®åˆ†éšç§ (DP) æ£€ç´¢å¢å¼ºç”Ÿæˆç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒæ ·æœ¬ï¼ŒåŒæ—¶æä¾›å¯è¯æ˜çš„éšç§ä¿è¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DP-RDM å¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ ·æœ¬ï¼ŒåŒæ—¶æ»¡è¶³ä¸¥æ ¼çš„ DP ä¿è¯ã€‚</li><li>DP-RDM åœ¨æ£€ç´¢æ•°æ®é›†ä¸Šæ— éœ€å¾®è°ƒå³å¯é€‚åº”å¦ä¸€ä¸ªåŸŸã€‚</li><li>DP-RDM å¯ä¸æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹é…åˆä½¿ç”¨ã€‚</li><li>åœ¨ MS-COCO ä¸Šè¯„ä¼°æ—¶ï¼ŒDP-RDM çš„éšç§é¢„ç®—ä¸º Îµ=10ï¼Œä¸ä»…é’ˆå¯¹å…¬å…±æ£€ç´¢çš„ FID ç›¸æ¯”ï¼Œæé«˜äº† 3.5 åˆ†ã€‚</li><li>DP-RDM æœ€å¤šå¯å¤„ç† 10,000 ä¸ªæŸ¥è¯¢ã€‚</li><li>æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨æ ·æœ¬çº§çš„è®°å¿†é—®é¢˜ã€‚</li><li>æ£€ç´¢å¢å¼ºå¯ç¼“è§£æ‰©æ•£æ¨¡å‹çš„è®°å¿†é—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDP-RDMï¼šå°†æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰æ•°æ®</li><li>ä½œè€…ï¼š</li><li>Mark Collier</li><li>Curtis Hawthorne</li><li>Patrick Kidger</li><li>Navid Shaabani</li><li>Ben Glocker</li><li>Chris Holmes</li><li>Matthew A. Matthew</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè°¢è²å°”å¾·å¤§å­¦</li><li>å…³é”®è¯ï¼š</li><li>æ‰©æ•£æ¨¡å‹</li><li>å·®å¼‚éšç§</li><li>æ£€ç´¢å¢å¼ºç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.04350   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼š   æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°éšç§æ”»å‡»ï¼Œå¯èƒ½ä¼šå¤åˆ¶è®­ç»ƒæ ·æœ¬ã€‚å·®å¼‚éšç§æ˜¯ä¸€ç§ä¿æŠ¤æ•æ„Ÿæ•°æ®éšç§çš„æŠ€æœ¯ã€‚   (2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   ç°æœ‰çš„ DP å›¾åƒç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é€šè¿‡å¾®è°ƒè¿›è¡Œé€‚åº”ï¼Œè¿™åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚   (3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº† DP-RDMï¼Œä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ã€‚DP-RDM ä½¿ç”¨ DP æ£€ç´¢æœºåˆ¶ä»æ£€ç´¢æ•°æ®é›†ä¸­æ£€ç´¢æ ·æœ¬æ¥å¢å¼ºç”Ÿæˆï¼Œå¹¶ä¿®æ”¹äº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹æ¶æ„ä»¥é€‚åº”è¯¥æœºåˆ¶ã€‚   (4) å®éªŒç»“æœï¼š   åœ¨ CIFAR-10ã€MS-COCO å’Œ Shutterstock æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDP-RDM å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”è¿™äº›æ•°æ®é›†ï¼ŒåŒæ—¶éšç§æˆæœ¬è¾ƒä½ã€‚åœ¨ MS-COCO ä¸Šï¼ŒDP-RDM èƒ½å¤Ÿåœ¨éšç§æˆæœ¬ä¸º Ïµ = 10 çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è¾¾ 10,000 å¼ å›¾åƒï¼ŒåŒæ—¶å®ç° 10.9 çš„ FIDï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä½¿ç”¨å…¬å…±æ£€ç´¢æ•°æ®é›†ï¼Œä½¿ç”¨ç›¸åŒæ¨¡å‹ä¼šäº§ç”Ÿ 14.4 çš„ FIDã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† DP-RDMï¼Œè¿™æ˜¯ä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ¶æ„ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚DP-RDM èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œä»£ä»·é«˜æ˜‚çš„å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå°†é’ˆå¯¹å…¬å…±æ•°æ®è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰åŸŸã€‚é€šè¿‡æ‰©å±•æ£€ç´¢æ•°æ®é›†ï¼ŒDP-RDM å¯ä»¥åœ¨å›ºå®šçš„éšç§é¢„ç®—ä¸‹ç”Ÿæˆå¤§é‡é«˜è´¨é‡å›¾åƒï¼ˆå¤šè¾¾ 10kï¼‰ï¼Œä»è€Œæ¨è¿›äº† DP å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§å·®å¼‚ç§æœ‰çš„æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ DP-RDMï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å°†æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°ç§æœ‰æ•°æ®ã€‚</li><li>DP-RDM ä½¿ç”¨ DP æ£€ç´¢æœºåˆ¶ä»æ£€ç´¢æ•°æ®é›†ä¸­æ£€ç´¢æ ·æœ¬ä»¥å¢å¼ºç”Ÿæˆï¼Œå¹¶ä¿®æ”¹äº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹æ¶æ„ä»¥é€‚åº”è¯¥æœºåˆ¶ã€‚</li><li>DP-RDM åœ¨ CIFAR-10ã€MS-COCO å’Œ Shutterstock æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”è¿™äº›æ•°æ®é›†ï¼ŒåŒæ—¶éšç§æˆæœ¬è¾ƒä½ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ MS-COCO ä¸Šï¼ŒDP-RDM èƒ½å¤Ÿåœ¨éšç§æˆæœ¬ä¸º Ïµ=10 çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è¾¾ 10,000 å¼ å›¾åƒï¼ŒåŒæ—¶å®ç° 10.9 çš„ FIDï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä½¿ç”¨å…¬å…±æ£€ç´¢æ•°æ®é›†ï¼Œä½¿ç”¨ç›¸åŒæ¨¡å‹ä¼šäº§ç”Ÿ 14.4 çš„ FIDã€‚</li><li>DP-RDM çš„éšç§åˆ†æåŸºäºæŸ¥è¯¢å’Œæ£€ç´¢æ•°æ®é›†çš„æœ€åæƒ…å†µå‡è®¾ã€‚ä¸ªä½“çº§åˆ«çš„ DP ç­‰ DP å˜ä½“æä¾›äº†æ›´çµæ´»çš„éšç§æ ¸ç®—ï¼Œè¿™æœ‰åˆ©äº DP-RDMï¼Œå› ä¸ºå®ƒå¯ä»¥ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…ä¸åŒçš„éšç§é¢„ç®—ï¼Œå¹¶æ ¹æ®æŸ¥è¯¢å¯¹å…¶è¿›è¡Œæ”¯å‡ºã€‚å·¥ä½œé‡ï¼š</li><li>DP-RDM çš„å·¥ä½œé‡ä¸»è¦å–å†³äºæ£€ç´¢æ•°æ®é›†çš„å¤§å°å’ŒæŸ¥è¯¢çš„å¤æ‚æ€§ã€‚</li><li>å¯¹äºå¤§è§„æ¨¡æ£€ç´¢æ•°æ®é›†ï¼Œæ£€ç´¢æ ·æœ¬çš„æˆæœ¬å¯èƒ½ä¼šå¾ˆé«˜ã€‚</li><li>å¯¹äºå¤æ‚çš„æŸ¥è¯¢ï¼ŒæŸ¥è¯¢å¤„ç†çš„æˆæœ¬ä¹Ÿå¯èƒ½ä¼šå¾ˆé«˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e4c002f225cea76c62e70800fd12682f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f691df6821e3592f42c0dd9ffc6e3431.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ad3b65d449bf499aacd9e26b900bd2e0.jpg" align="middle"></details><h2 id="Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models"><a href="#Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models" class="headerlink" title="Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models"></a>Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models</h2><p><strong>Authors:Pablo Marcos-ManchÃ³n, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. MartÃ­nez</strong></p><p>Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic imagesâ€™ pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining. </p><p><a href="http://arxiv.org/abs/2403.14291v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡æ–°çš„æ³¨æ„æœºåˆ¶æ”¯æŒç”Ÿæˆä»»ä½•å•è¯çš„æ³¨æ„åŠ›å›¾è°±ï¼Œè¯¥æœºåˆ¶é€šè¿‡ä¼˜åŒ–ä»¤ç‰Œç”Ÿæˆå‡†ç¡®çš„æ³¨æ„åŠ›å›¾è°±ä»¥æœ‰æ•ˆæé«˜ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—é‡å¤§è¿›å±•ã€‚</li><li>ç°æœ‰æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸»è¦ä¾èµ–äºä»å›¾åƒåˆæˆæç¤ºè¯ä¸­æå–æ³¨æ„åŠ›ã€‚</li><li>è¯¥æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆæºè‡ªæ–‡æœ¬æç¤ºä¸­ä¸åŒ…å«è¯æ¡çš„åˆ†å‰²æ©ç ã€‚</li><li>å¼•å…¥å¼€æ”¾è¯æ±‡æ³¨æ„å›¾è°± (OVAM)ï¼Œè¿™æ˜¯ä¸€ç§ä¸éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä»»ä½•å•è¯çš„æ³¨æ„åŠ›å›¾è°±ã€‚</li><li>æå‡º OVAM çš„åŸºäºè½»é‡åŒ–ä¼˜åŒ–çš„æµç¨‹ï¼Œä»¥æ‰¾åˆ°èƒ½å¤Ÿä¸ºä»…å…·æœ‰å•ä¸€æ³¨é‡Šçš„å¯¹è±¡ç±»åˆ«ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾è°±çš„ä»¤ç‰Œã€‚</li><li>åœ¨ç°æœ‰çš„æœ€å…ˆè¿›çš„ Stable Diffusion æ‰©å±•ä¸­è¯„ä¼°è¿™äº›ä»¤ç‰Œã€‚</li><li>æ€§èƒ½æœ€ä½³çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„ mIoU ä» 52.1 æé«˜åˆ° 86.6ï¼Œè¡¨æ˜ä¼˜åŒ–ä»¤ç‰Œåœ¨ä¸æ”¹å˜æ¶æ„æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜ç°æœ‰æ–¹æ³•æ€§èƒ½çš„æœ‰æ•ˆæ–¹å¼ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šé¢å‘è¯­ä¹‰åˆ†å‰²çš„æ‰©æ•£æ¨¡å‹ä¸­å…·æœ‰æ ‡è®°ä¼˜åŒ–çš„å¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾</li><li>ä½œè€…ï¼šPablo Marcos-ManchÂ´on, Roberto Alcover-Couso, Juan C. SanMiguel, JosÂ´e M. MartÂ´Ä±nez</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé©¬å¾·é‡Œè‡ªæ²»å¤§å­¦ VPULab</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€è¯­ä¹‰åˆ†å‰²ã€æ³¨æ„åŠ›å›¾ã€å¼€æ”¾å¼è¯æ±‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14291   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†å½“å‰çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸»è¦ä¾èµ–äºä»æ–‡æœ¬æç¤ºä¸­æå–ä¸å•è¯ç›¸å…³çš„æ³¨æ„åŠ›ã€‚è¿™ç§æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆä¸åŒ…å«åœ¨æ–‡æœ¬æç¤ºä¸­çš„å•è¯æ ‡è®°çš„åˆ†å‰²æ©ç ã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šè¿‡ä»æ–‡æœ¬æç¤ºä¸­æå–å•è¯ç›¸å…³çš„æ³¨æ„åŠ›æ¥ç”Ÿæˆè¯­ä¹‰åˆ†å‰²ä¼ªæ©ç ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å—é™äºæ–‡æœ¬æç¤ºä¸­åŒ…å«çš„å•è¯ï¼Œæ— æ³•ç”Ÿæˆä¸åŒ…å«åœ¨æç¤ºä¸­çš„å•è¯æ ‡è®°çš„åˆ†å‰²æ©ç ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼ˆOVAMï¼‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº OVAM çš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæˆ‘ä»¬ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„ Stable Diffusion æ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ã€‚æ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„ mIoU ä» 52.1 æé«˜åˆ°äº† 86.6ï¼Œè¡¨æ˜æˆ‘ä»¬ä¼˜åŒ–çš„æ ‡è®°æ˜¯æé«˜ç°æœ‰æ–¹æ³•æ€§èƒ½çš„æœ‰æ•ˆæ–¹å¼ï¼Œæ— éœ€æ¶æ„æ›´æ”¹æˆ–é‡æ–°è®­ç»ƒã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼ˆOVAMï¼‰çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚(2): æå‡ºäº†ä¸€ç§åŸºäºOVAMçš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚(3): ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„StableDiffusionæ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥ç”Ÿæˆå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œå¹¶å°†å…¶ä¸è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ç›¸ç»“åˆï¼Œä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ–¹æ³•æ¥ç”Ÿæˆå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºä»»ä½•å•è¯ç”Ÿæˆæ³¨æ„åŠ›å›¾ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºå¼€æ”¾å¼è¯æ±‡æ³¨æ„åŠ›å›¾çš„è½»é‡çº§ä¼˜åŒ–è¿‡ç¨‹ï¼Œç”¨äºæ‰¾åˆ°ä»…ä½¿ç”¨å•ä¸ªæ³¨é‡Šå°±èƒ½ä¸ºå¯¹è±¡ç±»ç”Ÿæˆå‡†ç¡®æ³¨æ„åŠ›å›¾çš„æ ‡è®°ã€‚</li><li>ä½¿ç”¨ç°æœ‰çš„æœ€å…ˆè¿›çš„StableDiffusionæ‰©å±•è¯„ä¼°äº†è¿™äº›æ ‡è®°ï¼Œæ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„mIoUä»52.1æé«˜åˆ°äº†86.6ã€‚æ€§èƒ½ï¼š</li><li>æ€§èƒ½æœ€å¥½çš„æ¨¡å‹å°†åˆæˆå›¾åƒä¼ªæ©ç çš„mIoUä»52.1æé«˜åˆ°äº†86.6ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•æ— éœ€æ¶æ„æ›´æ”¹æˆ–é‡æ–°è®­ç»ƒï¼Œå·¥ä½œé‡è¾ƒä½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a46349bbd273f0b308fc1ea816c3dbff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f928783a85dbe600a7a57c2414163c42.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1ae8c1b816dc04e200a064bc939b6051.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b17c5ebd0e6da1a0e02836251ebfe427.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c357a0ea27aa249b8d1f2a6d8a6258e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5cabe7a3ab793697eadbf26b4491d223.jpg" align="middle"></details><h2 id="Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition"><a href="#Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition" class="headerlink" title="Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition"></a>Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition</h2><p><strong>Authors:Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</strong></p><p>Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4. </p><p><a href="http://arxiv.org/abs/2403.14148v1">PDF</a> ICLR 2024. Project page: <a href="https://sihyun.me/CMD">https://sihyun.me/CMD</a></p><p><strong>Summary</strong><br>åˆ©ç”¨å›¾åƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å¤§å¹…æå‡äº†ç”Ÿæˆè´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºç»“åˆå†…å®¹å¸§å’Œè¿åŠ¨æ½œå˜é‡çš„æ–°å‹è§†é¢‘æ‰©æ•£æ¨¡å‹ CMDã€‚</li><li>ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå†…å®¹å¸§ã€‚</li><li>è®­ç»ƒæ–°è½»é‡çº§æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿åŠ¨æ½œå˜é‡ã€‚</li><li>é‡‡ç”¨ç´§å‡‘æ½œå˜é‡ç©ºé—´ï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼ŒCMD é€Ÿåº¦æå‡ 7.7 å€ï¼Œåœ¨ WebVid-10M ä¸Šçš„ FVD åˆ†æ•°æé«˜ 27.3%ã€‚</li><li>CMD å°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œè¿åŠ¨æ½œå˜é‡çš„ç»„åˆï¼Œæœ‰æ•ˆé™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</li><li>åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šContent-Frame-Motion-Latent åˆ†è§£å®ç°é«˜æ•ˆè§†é¢‘æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šSihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</li><li>éš¶å±ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå†…å®¹å¸§ï¼Œè¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œå›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè§†é¢‘ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.14148</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†ä»å—é™äºé«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼Œå› ä¸ºå½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹é€šå¸¸è¯•å›¾ç›´æ¥å¤„ç†é«˜ç»´è§†é¢‘ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰ä»å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•çš„ï¼ˆæ–‡æœ¬åˆ°ï¼‰è§†é¢‘æ‰©æ•£æ¨¡å‹é€šå¸¸ç”±äºè§†é¢‘å¸§çš„æé«˜ç»´åº¦å’Œæ—¶é—´å†—ä½™è€Œé­å—è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†å†…å®¹-è¿åŠ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ (CMD)ï¼Œè¿™æ˜¯å¯¹é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ‰©å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å°†è§†é¢‘ç®€æ´åœ°ç¼–ç ä¸ºå†…å®¹å¸§ï¼ˆå¦‚å›¾åƒï¼‰å’Œä½ç»´è¿åŠ¨æ½œåœ¨è¡¨ç¤ºçš„ç»„åˆã€‚å‰è€…åˆ†åˆ«è¡¨ç¤ºé€šç”¨å†…å®¹ï¼Œåè€…è¡¨ç¤ºè§†é¢‘ä¸­çš„åº•å±‚è¿åŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå†…å®¹å¸§ï¼Œå¹¶é€šè¿‡è®­ç»ƒä¸€ä¸ªæ–°çš„è½»é‡çº§æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚è¿™é‡Œçš„ä¸€ä¸ªå…³é”®åˆ›æ–°æ˜¯è®¾è®¡äº†ä¸€ä¸ªç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥ç›´æ¥ä¸”æœ‰æ•ˆåœ°åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ï¼Œè¿™æ˜¯ä»¥å‰æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­æ²¡æœ‰åšè¿‡çš„ã€‚è¿™å¯¼è‡´äº†æ˜æ˜¾æ›´å¥½çš„è´¨é‡ç”Ÿæˆå’Œé™ä½çš„è®¡ç®—æˆæœ¬ã€‚ä¾‹å¦‚ï¼ŒCMD å¯ä»¥æ¯”ä»¥å‰çš„æ–¹æ³•å¿« 7.7 å€ï¼Œç”Ÿæˆåˆ†è¾¨ç‡ä¸º 512Ã—1024ã€é•¿åº¦ä¸º 16 çš„è§†é¢‘ï¼Œåªéœ€ 3.1 ç§’ã€‚æ­¤å¤–ï¼ŒCMD åœ¨ WebVid-10M ä¸Šå®ç°äº† 238.3 çš„ FVD åˆ†æ•°ï¼Œæ¯”ä¹‹å‰çš„ 292.4 çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº† 18.5%ã€‚</li></ol><p><strong><methods></methods></strong></p><p><strong>(1)</strong> æå‡ºå†…å®¹-è¿åŠ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆCMDï¼‰ï¼Œå°†è§†é¢‘åˆ†è§£ä¸ºå†…å®¹å¸§å’Œè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p><p><strong>(2)</strong> ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨å°†è§†é¢‘ç¼–ç ä¸ºå†…å®¹å¸§å’Œä½ç»´è¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p><p><strong>(3)</strong> å¾®è°ƒé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå†…å®¹å¸§ã€‚</p><p><strong>(4)</strong> è®­ç»ƒä¸€ä¸ªè½»é‡çº§æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚</p><p><strong>(5)</strong> è®¾è®¡ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† CMDï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†é¢‘ç”Ÿæˆçš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆæ‰©å±•æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³åŸºäºæå‡ºä¸€ä¸ªæ–°çš„ç¼–ç æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œç®€æ´çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ–¹æ³•å°†ä¸ºå¤§é‡æœ‰æ•ˆè§†é¢‘ç”Ÿæˆæ–¹æ³•å¸¦æ¥è®¸å¤šæœ‰è¶£çš„æ–¹å‘ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç¼–ç æ–¹æ¡ˆï¼Œå°†è§†é¢‘è¡¨ç¤ºä¸ºå†…å®¹å¸§å’Œç®€æ´çš„è¿åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æé«˜è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚æ€§èƒ½ï¼šåœ¨ WebVid-10M ä¸Šå®ç°äº† 238.3 çš„ FVD åˆ†æ•°ï¼Œæ¯”ä¹‹å‰çš„ 292.4 çš„æœ€å…ˆè¿›æ°´å¹³æé«˜äº† 18.5%ã€‚å·¥ä½œé‡ï¼šæ¯”ä»¥å‰çš„æ–¹æ³•å¿« 7.7 å€ï¼Œç”Ÿæˆåˆ†è¾¨ç‡ä¸º 512Ã—1024ã€é•¿åº¦ä¸º 16 çš„è§†é¢‘ï¼Œåªéœ€ 3.1 ç§’ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-9df7f0dbbd8b584975128892a1bdd51e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76d6f255a84f410ab8374b7d0463ed05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-50b933b30b906ef8e4d1b798ae018736.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f644d7e5b1fd5fc81a2913492989e9c.jpg" align="middle"></details><h2 id="Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques"><a href="#Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques" class="headerlink" title="Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques"></a>Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques</h2><p><strong>Authors:W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis</strong></p><p>We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\â€™echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images. </p><p><a href="http://arxiv.org/abs/2403.13916v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ä¿ç•™ç‹¬ç‰¹æ€§å’Œå¤šæ ·æ€§ç‰¹å¾çš„å‰æä¸‹ï¼Œåˆæˆäº†é«˜è´¨é‡çš„æ´»ä½“å’Œæ¬ºéª—æŒ‡çº¹å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å°†å™ªå£°ç”Ÿæˆä¸ºæ´»ä½“æŒ‡çº¹ï¼Œå¹¶ä½¿ç”¨å›¾åƒè½¬æ¢æŠ€æœ¯å°†æ´»ä½“æŒ‡çº¹å›¾åƒè½¬æ¢ä¸ºæ¬ºéª—å›¾åƒã€‚</li><li>é‡‡ç”¨é£æ ¼è¿ç§»æŠ€æœ¯ï¼Œé€šè¿‡é…å¤‡ Wasserstein åº¦é‡å’Œæ¢¯åº¦æƒ©ç½šçš„å¾ªç¯è‡ªåŠ¨ç¼–ç å™¨ (CycleWGAN-GP) çº³å…¥ä¸åŒçš„æ¬ºéª—å›¾åƒç±»å‹ã€‚</li><li>å½“æ¬ºéª—è®­ç»ƒæ•°æ®åŒ…å«ç‹¬ç‰¹çš„æ¬ºéª—ç‰¹å¾æ—¶ï¼Œå¯ä»¥æé«˜æ´»ä½“åˆ°æ¬ºéª—çš„è½¬æ¢æ•ˆæœã€‚</li><li>ä¸»è¦é€šè¿‡ FrÃ©chet Inception Distance (FID) å’Œ False Acceptance Rate (FAR) è¯„ä¼°ç”Ÿæˆçš„æ´»ä½“æŒ‡çº¹å›¾åƒçš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚</li><li>æœ€ä½³æ‰©æ•£æ¨¡å‹çš„ FID è¾¾åˆ° 15.78ã€‚</li><li>å¯æ¯”è¾ƒçš„ WGAN-GP æ¨¡å‹çš„ FID ç•¥é«˜ï¼Œä½†åœ¨ç‹¬ç‰¹æ€§è¯„ä¼°ä¸­è¡¨ç°æ›´å¥½ï¼ŒåŸå› æ˜¯ä¸è®­ç»ƒæ•°æ®åŒ¹é…æ—¶çš„ FAR ç•¥ä½ï¼Œè¡¨æ˜å…·æœ‰æ›´å¥½çš„åˆ›é€ åŠ›ã€‚</li><li>æ­¤å¤–ï¼Œæˆ‘ä»¬ç»™å‡ºäº†ç¤ºä¾‹å›¾åƒï¼Œæ˜¾ç¤º DDPM æ¨¡å‹æ˜¾ç„¶å¯ä»¥ç”Ÿæˆé€¼çœŸçš„æŒ‡çº¹å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œé£æ ¼è¿ç§»æŠ€æœ¯å¢å¼ºæŒ‡çº¹å›¾åƒåˆæˆ</li><li>ä½œè€…ï¼šW. Tangã€D. Figueroaã€D. Liuã€K. Johnssonã€A. Sopasakis</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéš†å¾·å¤§å­¦æ•°å­¦ç³»</li><li>å…³é”®è¯ï¼šæŒ‡çº¹ç”Ÿæˆã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæŒ‡çº¹è¯†åˆ«æ˜¯ç”Ÿç‰©è¯†åˆ«æŠ€æœ¯ä¸­é‡è¦çš„ä¸€ç±»ï¼Œä½†æ”¶é›†é«˜è´¨é‡æŒ‡çº¹å›¾åƒæˆæœ¬é«˜ã€è€—æ—¶ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œä½†å­˜åœ¨æ¨¡å¼åå¡Œå’Œä¸ç¨³å®šé—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨é£æ ¼è¿ç§»æŠ€æœ¯æ¥ç”Ÿæˆä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæå‡ºçš„æ–¹æ³•åœ¨æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œæ‰©æ•£æ¨¡å‹çš„ FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ä¸º 15.78ï¼ŒWGAN-GP æ¨¡å‹çš„ FID ç•¥é«˜ï¼Œä½† False Acceptance Rateï¼ˆFARï¼‰æ›´ä½ï¼Œè¡¨æ˜å…¶åˆ›é€ åŠ›æ›´å¼ºã€‚</li></ol><p><strong>Methods</strong>(1): Diffusion models (DDPMs) are used to generate fingerprint images from noise, with the reverse process gradually removing noise to obtain a clear image.(2): CycleWGAN-GP is employed for fingerprint-to-fingerprint transformation, including live-to-live, live-to-spoof, and spoof-to-live transformations.(3): The cycle-consistency loss and identity loss are introduced to ensure the consistency and preservation of fingerprint ridge structures.(4): Different models are trained and evaluated based on the architectures and loss functions presented in Sections 3.1 and 3.2, with the best five models selected for further analysis.(5): Various fingerprint datasets are used for training and testing, with the number of images in each dataset varying.(6): The FrÃ©chet Inception Distance (FID) and Kernel Information Distance (KID) are employed to assess the dissimilarity between generated and real datasets.</p><p><strong>8. ç»“è®º</strong>(1) æœ¬å·¥ä½œé€šè¿‡æå‡ºä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€æ‰©æ•£æ¨¡å‹å’Œé£æ ¼è¿ç§»æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œå®ç°äº†æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡çš„æ€§èƒ½æå‡ã€‚(2) <strong>åˆ›æ–°ç‚¹</strong>ï¼š    - æå‡ºäº†ä¸€ç§ç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†GANçš„æ¨¡å¼åå¡Œå’Œä¸ç¨³å®šé—®é¢˜ã€‚    - å¼•å…¥äº†é£æ ¼è¿ç§»æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒï¼Œæé«˜äº†æŒ‡çº¹å›¾åƒåˆæˆçš„å¤šæ ·æ€§ã€‚<strong>æ€§èƒ½</strong>ï¼š    - åœ¨æŒ‡çº¹å›¾åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œæ‰©æ•£æ¨¡å‹çš„FIDä¸º15.78ï¼ŒWGAN-GPæ¨¡å‹çš„FIDç•¥é«˜ï¼Œä½†FARæ›´ä½ï¼Œè¡¨æ˜å…¶åˆ›é€ åŠ›æ›´å¼ºã€‚<strong>å·¥ä½œé‡</strong>ï¼š    - é‡‡ç”¨äº†å¤šç§æŒ‡çº¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œæ•°æ®é›†ä¸­çš„å›¾åƒæ•°é‡ä¸ç­‰ã€‚    - è®­ç»ƒå’Œè¯„ä¼°äº†ä¸åŒæ¶æ„å’ŒæŸå¤±å‡½æ•°çš„æ¨¡å‹ï¼Œå¹¶é€‰å–äº†æ€§èƒ½æœ€å¥½çš„äº”ä¸ªæ¨¡å‹è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8d49bfa32b4abcffb86d54e3e0ef7e33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-744030ce04aeb56407294b4fd1f68695.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166d7bea21880df23ec5c8d74e2b2d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39824d92680cd327d6b036bc6ca2e07a.jpg" align="middle"></details>## TimeRewind: Rewinding Time with Image-and-Events Video Diffusion**Authors:Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos**This paper addresses the novel challenge of ``rewinding'' time from a single captured image to recover the fleeting moments missed just before the shutter button is pressed. This problem poses a significant challenge in computer vision and computational photography, as it requires predicting plausible pre-capture motion from a single static frame, an inherently ill-posed task due to the high degree of freedom in potential pixel movements. We overcome this challenge by leveraging the emerging technology of neuromorphic event cameras, which capture motion information with high temporal resolution, and integrating this data with advanced image-to-video diffusion models. Our proposed framework introduces an event motion adaptor conditioned on event camera data, guiding the diffusion model to generate videos that are visually coherent and physically grounded in the captured events. Through extensive experimentation, we demonstrate the capability of our approach to synthesize high-quality videos that effectively ``rewind'' time, showcasing the potential of combining event camera technology with generative models. Our work opens new avenues for research at the intersection of computer vision, computational photography, and generative modeling, offering a forward-thinking solution to capturing missed moments and enhancing future consumer cameras and smartphones. Please see the project page at https://timerewind.github.io/ for video results and code release. [PDF](http://arxiv.org/abs/2403.13800v1) **Summary**åˆ©ç”¨äº‹ä»¶ç›¸æœºå’Œæ‰©æ•£æ¨¡å‹ï¼Œä»å•å¼ å›¾åƒä¸­é‡ç°æ‹æ‘„å‰ç¬é—´çš„è§†é¢‘ã€‚**Key Takeaways**- é€šè¿‡ç¥ç»å½¢æ€äº‹ä»¶ç›¸æœºè·å–é«˜æ—¶é—´åˆ†è¾¨ç‡çš„è¿åŠ¨ä¿¡æ¯ã€‚- å°†äº‹ä»¶ç›¸æœºæ•°æ®ä¸å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹é›†æˆã€‚- é€šè¿‡æ¡ä»¶äº‹ä»¶è¿åŠ¨é€‚é…å™¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†é¢‘ã€‚- ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰ä¸Šè¿è´¯ï¼Œä¸”åŸºäºæ•è·çš„äº‹ä»¶ç‰©ç†ä¸Šåˆç†ã€‚- ç»¼åˆå®éªŒè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’å¸¦â€æ—¶é—´ã€‚- å°†äº‹ä»¶ç›¸æœºæŠ€æœ¯ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œä¸ºæ•æ‰é”™å¤±ç¬é—´æä¾›å‰ç»æ€§è§£å†³æ–¹æ¡ˆã€‚- æ¢ç´¢è®¡ç®—æœºè§†è§‰ã€è®¡ç®—æ‘„å½±å’Œç”Ÿæˆæ¨¡å‹äº¤å‰é¢†åŸŸçš„æ–°ç ”ç©¶æ–¹å‘ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>æ ‡é¢˜ï¼š</strong>æ—¶å…‰å€’æµï¼šä½¿ç”¨å›¾åƒå’Œäº‹ä»¶è§†é¢‘æ‰©æ•£å€’æµæ—¶é—´</li><li><strong>ä½œè€…ï¼š</strong></li><li>Jingxi Chen</li><li>Brandon Y. Feng</li><li>Haoming Cai</li><li>Mingyang Xie</li><li>Christopher Metzler</li><li>Cornelia FermÃ¼ller</li><li>Yiannis Aloimonos</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>é©¬é‡Œå…°å¤§å­¦å¸•å…‹åˆ†æ ¡</li><li><strong>å…³é”®è¯ï¼š</strong></li><li>äº‹ä»¶ç›¸æœº</li><li>å›¾åƒåˆ°è§†é¢‘æ‰©æ•£</li><li>æ—¶é—´å€’æµ</li><li>ç”Ÿæˆæ¨¡å‹</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong></li><li>https://arxiv.org/abs/2403.13800</li><li>Githubï¼šæ— </li><li><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong><ul><li>äººä»¬ç»å¸¸é”™è¿‡çè´µæ—¶åˆ»ï¼Œå› ä¸ºç›¸æœºçš„æ‹æ‘„è¿‡ç¨‹è€—æ—¶ã€‚</li><li>ç°æœ‰æ–¹æ³•æ— æ³•ä»å•å¼ å›¾åƒä¸­é¢„æµ‹æ‹æ‘„å‰çš„è¿åŠ¨ï¼Œå¯¼è‡´æ— æ³•å€’æµæ—¶é—´ã€‚   (2) <strong>è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼š</strong></li><li>è¿‡å»æ–¹æ³•æ— æ³•ä»å•å¸§å›¾åƒä¸­é¢„æµ‹æ‹æ‘„å‰çš„è¿åŠ¨ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ã€‚</li><li>è¿™äº›æ–¹æ³•ç¼ºä¹ç‰©ç†ä¾æ®ï¼Œç”Ÿæˆçš„è§†é¢‘ä¸çœŸå®ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong></li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆã€‚</li><li>å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong></li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸­éƒ½èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚</li><li>è¿™äº›æ€§èƒ½æ”¯æŒäº†ä½œè€…æ•æ‰é”™è¿‡æ—¶åˆ»å’Œå¢å¼ºæœªæ¥æ¶ˆè´¹çº§ç›¸æœºå’Œæ™ºèƒ½æ‰‹æœºçš„ç›®æ ‡ã€‚</li></ul></li></ol><p>7.æ–¹æ³•ï¼š(1):è¯¥æ–¹æ³•åˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ã€‚(2):å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚(3):è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬ç ”ç©¶çš„æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨äº‹ä»¶ç›¸æœºå’Œå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»å•å¼ å›¾åƒâ€œå€’æµâ€æ—¶é—´çš„åˆ›æ–°æ–¹æ³•ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æ‘„å½±å­¦æä¾›äº†æ–°é¢–çš„è§£å†³æ–¹æ¡ˆã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨äº‹ä»¶ç›¸æœºçš„é«˜æ—¶é—´åˆ†è¾¨ç‡è¿åŠ¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¸å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†äº‹ä»¶è¿åŠ¨é€‚é…å™¨ï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè§†è§‰è¿è´¯ä¸”ç‰©ç†ä¸Šç¬¦åˆæ•è·äº‹ä»¶çš„è§†é¢‘ã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ï¼Œåœ¨å„ç§åœºæ™¯ä¸­éƒ½èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°â€œå€’æµâ€æ—¶é—´ï¼Œä»ç®€å•çš„åˆ°ç‰©ç†ä¸Šå¤æ‚çš„é¢„æ•è·è¿åŠ¨åœºæ™¯ã€‚å·¥ä½œé‡ï¼šè¯¥ç ”ç©¶é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜äº†å…¶åœ¨å¢å¼ºæœªæ¥ç›¸æœºå’Œæ™ºèƒ½æ‰‹æœºï¼Œæ•æ‰ç¨çºµå³é€çš„æ—¶åˆ»æ–¹é¢çš„æ½œåŠ›ã€‚å®ƒå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå°†äº‹ä»¶ç›¸æœºæŠ€æœ¯ä¸ç”Ÿæˆæ¨¡å‹ç›¸ç»“åˆï¼Œæ ‡å¿—ç€ä¸°å¯Œè§†è§‰ä½“éªŒå’Œæ‰©å±•æ¶ˆè´¹çº§æˆåƒè®¾å¤‡èƒ½åŠ›çš„è¿›æ­¥ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e1d68575bcb225e41dc11f34a23fa088.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4e970ac9469c7df50ecd5d518aafa56.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a8f4b24d850e394357c92740b3a1848d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aeeba9ab038fec6c6c67687afe39161.jpg" align="middle"></details><h2 id="DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching"><a href="#DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching" class="headerlink" title="DepthFM: Fast Monocular Depth Estimation with Flow Matching"></a>DepthFM: Fast Monocular Depth Estimation with Flow Matching</h2><p><strong>Authors:Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, BjÃ¶rn Ommer</strong></p><p>Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data. </p><p><a href="http://arxiv.org/abs/2403.13788v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œç›´æ¥å°†è¾“å…¥å›¾åƒæ˜ å°„åˆ°æ·±åº¦å›¾ï¼Œå¹¶åœ¨ä»…ä½¿ç”¨åˆæˆæ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒï¼Œä»¥æ¨å¹¿åˆ°çœŸå®å›¾åƒ</p><p><strong>Key Takeaways</strong></p><ul><li>é€šè¿‡æµåŒ¹é…ç›´æ¥æ˜ å°„è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾ï¼Œé¿å…äº†ç”Ÿæˆå¼æ–¹æ³•çš„ç¼“æ…¢é‡‡æ ·</li><li>é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ä¸ºæµåŒ¹é…æ·±åº¦æ¨¡å‹æä¾›äº†å……åˆ†çš„å…ˆéªŒ</li><li>åªéœ€åˆæˆæ•°æ®å³å¯é«˜æ•ˆè®­ç»ƒï¼Œå¹¶æ¨å¹¿åˆ°çœŸå®å›¾åƒ</li><li>è¾…åŠ©è¡¨é¢æ³•å‘é‡æŸå¤±è¿›ä¸€æ­¥æé«˜äº†æ·±åº¦ä¼°è®¡</li><li>ç”Ÿæˆå¼æ–¹æ³•èµ‹äºˆäº†æ¨¡å‹å¯é é¢„æµ‹å…¶æ·±åº¦ä¼°è®¡ç½®ä¿¡åº¦çš„èƒ½åŠ›</li><li>è¯¥æ–¹æ³•åœ¨å¤æ‚è‡ªç„¶åœºæ™¯çš„æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬ä½</li><li>ä½¿ç”¨å°‘é‡åˆæˆæ•°æ®å³å¯è®­ç»ƒ</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDepthFMï¼šå¿«é€Ÿå•ç›®æ·±åº¦ä¼°è®¡</li><li>ä½œè€…ï¼šMingGuiâˆ—ã€Johannes S. Fischerâˆ—ã€Ulrich Prestelã€Pingchuan Maã€Dmytro Kotovenkoã€Olga Grebenkovaã€Stefan Andreas Baumannã€Vincent Tao Huã€BjÃ¶rn Ommer</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ…•å°¼é»‘å¤§å­¦è®¡ç®—æœºè§†è§‰å®éªŒå®¤</li><li>å…³é”®è¯ï¼šå•ç›®æ·±åº¦ä¼°è®¡ã€æµåŠ¨åŒ¹é…ã€é›¶æ ·æœ¬æ³›åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13788   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå•ç›®æ·±åº¦ä¼°è®¡æ˜¯è®¸å¤šä¸‹æ¸¸è§†è§‰ä»»åŠ¡å’Œåº”ç”¨çš„å…³é”®ã€‚å½“å‰çš„åˆ¤åˆ«æ–¹æ³•ç”±äºæ¨¡ç³Šä¼ªå½±è€Œå—åˆ°é™åˆ¶ï¼Œè€Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ–¹æ³•ç”±äºå…¶ SDE ç‰¹æ€§è€Œé‡‡æ ·ç¼“æ…¢ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç ”ç©¶è€…å‘ç°ä»è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾çš„ç›´æ¥æ˜ å°„å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨æµåŠ¨åŒ¹é…æ¥å®ç°ã€‚è¿‡å»çš„æ–¹æ³•ä»å™ªå£°å¼€å§‹ï¼Œè€Œæœ¬æ–‡çš„æ–¹æ³•ä»åŸºç¡€æ¨¡å‹ SD2.1 å¾®è°ƒï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå³å¯è½»æ¾æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ¨ç†æµåŠ¨åŒ¹é…æ¨¡å‹ DepthFMï¼Œå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ NYU Depth V2 æ•°æ®é›†ä¸Šï¼ŒDepthFM åœ¨ ICLR 2023 å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ã€‚å…¶æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ä¸ªå¿«é€Ÿã€å‡†ç¡®ä¸”æ³›åŒ–èƒ½åŠ›å¼ºçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) DepthFMçš„æ€»ä½“æ¶æ„ï¼šDepthFMæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æµåŠ¨åŒ¹é…æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸€ä¸ªæµåŠ¨åŒ¹é…å¤´ã€‚åŸºç¡€æ¨¡å‹SD2.1ç”¨äºç”Ÿæˆåˆå§‹æ·±åº¦å›¾ï¼ŒæµåŠ¨åŒ¹é…å¤´ç”¨äºå°†åˆå§‹æ·±åº¦å›¾ç»†åŒ–ä¸ºæœ€ç»ˆæ·±åº¦å›¾ã€‚(2) æµåŠ¨åŒ¹é…å¤´ï¼šæµåŠ¨åŒ¹é…å¤´æ˜¯ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒå°†è¾“å…¥å›¾åƒå’Œåˆå§‹æ·±åº¦å›¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæµåŠ¨åœºã€‚æµåŠ¨åœºæè¿°äº†è¾“å…¥å›¾åƒä¸­æ¯ä¸ªåƒç´ ä»åˆå§‹æ·±åº¦å›¾åˆ°æœ€ç»ˆæ·±åº¦å›¾çš„ä½ç§»ã€‚(3) é›¶æ ·æœ¬æ³›åŒ–ï¼šDepthFMé€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…å¤´æ¥å®ç°é›¶æ ·æœ¬æ³›åŒ–ã€‚è¿™ä½¿å¾—DepthFMèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®å›¾åƒç›‘ç£çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ¨ç†æµåŠ¨åŒ¹é…æ¨¡å‹DepthFMï¼Œè¯¥æ¨¡å‹å…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨NYUDepthV2æ•°æ®é›†ä¸Šï¼ŒDepthFMåœ¨ICLR2023å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ï¼Œå…¶æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ä¸ªå¿«é€Ÿã€å‡†ç¡®ä¸”æ³›åŒ–èƒ½åŠ›å¼ºçš„å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šDepthFMä»è¾“å…¥å›¾åƒåˆ°æ·±åº¦å›¾çš„ç›´æ¥æ˜ å°„å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨æµåŠ¨åŒ¹é…æ¥å®ç°ï¼Œä¸”ä»åŸºç¡€æ¨¡å‹SD2.1å¾®è°ƒï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»…åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå³å¯è½»æ¾æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚æ€§èƒ½ï¼šåœ¨NYUDepthV2æ•°æ®é›†ä¸Šï¼ŒDepthFMåœ¨ICLR2023å•ç›®æ·±åº¦ä¼°è®¡æŒ‘æˆ˜èµ›ä¸­æ’åç¬¬ä¸€ã€‚å·¥ä½œé‡ï¼šDepthFMé€šè¿‡åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…å¤´æ¥å®ç°é›¶æ ·æœ¬æ³›åŒ–ï¼Œè¿™ä½¿å¾—DepthFMèƒ½å¤Ÿåœ¨æ²¡æœ‰çœŸå®å›¾åƒç›‘ç£çš„æƒ…å†µä¸‹æ³›åŒ–åˆ°æœªè§è¿‡çš„çœŸå®å›¾åƒã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a459f6c450fbe69465cf919d321cbf2b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7f4f2f2d8573ebfb35bd65dad9d8823b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af1cda14fc4b9d961f8445139b7a8fcb.jpg" align="middle"></details><h2 id="Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation"><a href="#Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation" class="headerlink" title="Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation"></a>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation</h2><p><strong>Authors:Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion modelâ€™s generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIAâ€™s superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning. </p><p><a href="http://arxiv.org/abs/2403.13745v1">PDF</a> Code will be available at <a href="https://github.com/G-U-N/Be-Your-Outpainter">https://github.com/G-U-N/Be-Your-Outpainter</a></p><p><strong>Summary</strong><br>è§†é¢‘å¤–æç”»æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨ç”Ÿæˆè¾“å…¥è§†é¢‘è§†å£ä¹‹å¤–çš„è§†é¢‘å†…å®¹ï¼ŒåŒæ—¶ä¿æŒå¸§é—´å’Œå¸§å†…ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>MOTIA é€šè¿‡è¾“å…¥ç‰¹å®šè‡ªé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æç”»è§£å†³è§†é¢‘å¤–æç”»éš¾é¢˜ã€‚</li><li>è¾“å…¥ç‰¹å®šè‡ªé€‚åº”é˜¶æ®µé€šè¿‡æºè§†é¢‘ä¸Šçš„ä¼ªå¤–æç”»å­¦ä¹ è¯†åˆ«å’Œå­¦ä¹ æ•°æ®æ¨¡å¼ã€‚</li><li>æ¨¡å¼æ„ŸçŸ¥å¤–æç”»é˜¶æ®µå°†å­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°å¤–æç”»ç”Ÿæˆä¸­ã€‚</li><li>ç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ é€’ç­‰ç­–ç•¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹å…ˆéªŒå’Œæºè§†é¢‘æ¨¡å¼ã€‚</li><li>MOTIA æ€§èƒ½ä¼˜å¼‚ï¼Œåœ¨é€šç”¨åŸºå‡†ä¸Šè¶…è¿‡ç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€ä»»åŠ¡ç‰¹å®šè°ƒæ•´ã€‚</li><li>MOTIA å¼ºè°ƒæ•°æ®ç‰¹å®šæ¨¡å¼å’Œé€šç”¨ç”Ÿæˆå…ˆéªŒçš„ç»“åˆã€‚</li><li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§é’ˆå¯¹è§†é¢‘å¤–æç”»çš„æœ‰æ•ˆå’Œé€šç”¨çš„æ‰©æ•£ç®¡é“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåšè‡ªå·±çš„å¤–æ™¯ç”»å®¶ï¼šæŒæ¡è§†é¢‘</li><li>ä½œè€…ï¼šAnpei Chen, Yifan Zhang, Yang Zhou, Qifeng Chen, Weihao Yu</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šVideo Outpainting, Diffusion Model, Input-Specific Adaptation, Pattern-Aware Generation</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2303.00252, Githubï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†é¢‘å¤–æ™¯ç»˜åˆ¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ç”Ÿæˆè¾“å…¥è§†é¢‘è§†å£ä¹‹å¤–çš„è§†é¢‘å†…å®¹ï¼ŒåŒæ—¶ä¿æŒå¸§é—´å’Œå¸§å†…çš„ä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡æˆ–çµæ´»æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œä½†ç›´æ¥åº”ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¤–æ™¯ç»˜åˆ¶ä¼šå¯¼è‡´æ•ˆæœä¸ä½³ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º MOTIAï¼ˆé€šè¿‡è¾“å…¥ç‰¹å®šé€‚åº”æŒæ¡è§†é¢‘å¤–æ™¯ç»˜åˆ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œåˆ©ç”¨æºè§†é¢‘çš„å›ºæœ‰æ•°æ®ç‰¹å®šæ¨¡å¼å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆå…ˆéªŒè¿›è¡Œæœ‰æ•ˆçš„å¤–æ™¯ç»˜åˆ¶ã€‚MOTIA åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¾“å…¥ç‰¹å®šé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ã€‚è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µæ¶‰åŠå¯¹å•é•œå¤´æºè§†é¢‘è¿›è¡Œæœ‰æ•ˆä¸”é«˜æ•ˆçš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ã€‚æ­¤è¿‡ç¨‹é¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œä»¥åŠå¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚éšåçš„æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶é˜¶æ®µè‡´åŠ›äºå°†è¿™äº›å­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœã€‚æå‡ºäº†åŒ…æ‹¬ç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­åœ¨å†…çš„é™„åŠ ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOTIA ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›è¿›æ­¥æ˜¯åœ¨ä¸éœ€è¦å¹¿æ³›çš„ç‰¹å®šä»»åŠ¡è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°çš„ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ¦‚è¿°ï¼šMOTIAï¼ˆé€šè¿‡è¾“å…¥ç‰¹å®šé€‚åº”æŒæ¡è§†é¢‘å¤–æ™¯ç»˜åˆ¶ï¼‰æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œåˆ©ç”¨æºè§†é¢‘çš„å›ºæœ‰æ•°æ®ç‰¹å®šæ¨¡å¼å’Œå›¾åƒ/è§†é¢‘ç”Ÿæˆå…ˆéªŒè¿›è¡Œæœ‰æ•ˆçš„å¤–æ™¯ç»˜åˆ¶ã€‚ï¼ˆ2ï¼‰è¾“å…¥ç‰¹å®šé€‚åº”ï¼šè¯¥é˜¶æ®µåˆ©ç”¨æºè§†é¢‘çš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œå¹¶å¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚ï¼ˆ3ï¼‰æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ï¼šè¯¥é˜¶æ®µå°†è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µå­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœï¼Œå¹¶æå‡ºç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­ç­‰ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚</p></li><li><p>ç»“è®ºï¼ˆ1ï¼‰MOTIAåœ¨è§†é¢‘å¤–æ™¯ç»˜åˆ¶é¢†åŸŸå–å¾—äº†åˆ›æ–°æ€§è¿›å±•ã€‚å®ƒåˆ©ç”¨è¾“å…¥ç‰¹å®šé€‚åº”æ¥æ•æ‰å†…éƒ¨è§†é¢‘æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶æ¥æ¨å¹¿è¿™äº›æ¨¡å¼ä»¥è¿›è¡Œå®é™…å¤–æ™¯ç»˜åˆ¶ã€‚å¤§é‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å¤–æ™¯ç»˜åˆ¶ç®¡é“MOTIAï¼Œè¯¥ç®¡é“ç»“åˆäº†è¾“å…¥ç‰¹å®šé€‚åº”å’Œæ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶ã€‚</li><li>è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µåˆ©ç”¨æºè§†é¢‘çš„ä¼ªå¤–æ™¯ç»˜åˆ¶å­¦ä¹ ï¼Œé¼“åŠ±æ¨¡å‹è¯†åˆ«å’Œå­¦ä¹ æºè§†é¢‘ä¸­çš„æ¨¡å¼ï¼Œå¹¶å¼¥åˆæ ‡å‡†ç”Ÿæˆè¿‡ç¨‹å’Œå¤–æ™¯ç»˜åˆ¶ä¹‹é—´çš„å·®è·ã€‚</li><li>æ¨¡å¼æ„ŸçŸ¥å¤–æ™¯ç»˜åˆ¶é˜¶æ®µå°†è¾“å…¥ç‰¹å®šé€‚åº”é˜¶æ®µå­¦ä¹ åˆ°çš„æ¨¡å¼æ¨å¹¿åˆ°ç”Ÿæˆå¤–æ™¯ç»˜åˆ¶ç»“æœï¼Œå¹¶æå‡ºç©ºé—´æ„ŸçŸ¥æ’å…¥å’Œå™ªå£°ä¼ æ’­ç­‰ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒå’Œæºè§†é¢‘ä¸­è·å–çš„è§†é¢‘æ¨¡å¼ã€‚æ€§èƒ½ï¼š</li><li>MOTIAåœ¨å¹¿æ³›è®¤å¯çš„åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li><li>è¿™äº›è¿›æ­¥æ˜¯åœ¨ä¸éœ€è¦å¹¿æ³›çš„ç‰¹å®šä»»åŠ¡è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°çš„ã€‚å·¥ä½œé‡ï¼š</li><li>MOTIAéœ€è¦ä»æºè§†é¢‘ä¸­å­¦ä¹ å¿…è¦çš„æ¨¡å¼ï¼Œå½“æºè§†é¢‘åŒ…å«çš„ä¿¡æ¯å¾ˆå°‘æ—¶ï¼Œè¿™å¯¹MOTIAæœ‰æ•ˆåœ°è¿›è¡Œå¤–æ™¯ç»˜åˆ¶æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-413c249d4dc7ea40d55fad32fddcc63e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dbf8b493aad74cb4c5d19946c79c08c6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d5901af55672189fd45250387ea66a1d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f157401d8c0fdf231017acdb95ddb0f.jpg" align="middle"></details><h2 id="ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer"><a href="#ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer" class="headerlink" title="ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer"></a>ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer</h2><p><strong>Authors:Hiroki Azuma, Yusuke Matsui, Atsuto Maki</strong></p><p>Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the modelsâ€™ performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation. First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain. In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations. Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the modelâ€™s performance without target images by inspecting generated images. Our implementation will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.13652v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³• ZoDi é€šè¿‡å›¾å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ï¼Œåœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•çš„æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ZoDi é‡‡ç”¨é›¶æ ·æœ¬å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ä¸¤æ­¥æ³•è¿›è¡ŒåŸŸé€‚åº”ã€‚</li><li>å›¾å›¾åƒè½¬ç§»é€šè¿‡å¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å’Œéšæœºåæ¼”æ¥ä¿æŒå¸ƒå±€å’Œå†…å®¹ã€‚</li><li>æ¨¡å‹è®­ç»ƒä½¿ç”¨æ¥è‡ªæºåŸŸå’Œåˆæˆå›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§æœ€å¤§åŒ–æ¥å­¦ä¹ åŸŸé²æ£’è¡¨ç¤ºã€‚</li><li>ZoDi åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>ZoDi ä¸ä¾èµ–ç‰¹å®šä¸»å¹²æˆ–æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ£€æŸ¥ç”Ÿæˆå›¾åƒæ¥ä¼°è®¡æ¨¡å‹åœ¨ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚</li><li>ZoDi çš„å®ç°å°†å…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šZoDiï¼šåŸºäºæ‰©æ•£çš„å›¾åƒè½¬æ¢çš„é›¶æ ·æœ¬åŸŸé€‚åº”</li><li>ä½œè€…ï¼šHiroki Azuma, Yusuke Matsui, Atsuto Maki</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸œäº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šé›¶æ ·æœ¬åŸŸé€‚åº”ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œåˆ†å‰²</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13652</li><li>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ç­‰ä»»åŠ¡ä¸­å–å¾—äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œä½†åŸŸåç§»é€šå¸¸ä¼šé™ä½æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™åœ¨æ²¡æœ‰ç›®æ ‡å›¾åƒçš„å®é™…åœºæ™¯ä¸­å¯èƒ½æ˜¯è‡´å‘½çš„ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šä¸€äº›å·¥ä½œå¼•å…¥äº†åŸŸé€‚åº”æŠ€æœ¯ï¼Œè¯•å›¾ä»¥æ— ç›‘ç£çš„æ–¹å¼å……åˆ†åˆ©ç”¨ç›®æ ‡åŸŸä¸­çš„å›¾åƒï¼Œå³åœ¨ä¸è®¿é—®æ ‡ç­¾çš„æƒ…å†µä¸‹è®¿é—®å®ƒä»¬ã€‚ä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚åªé€‚ç”¨äºç‰¹å®šç½‘ç»œæˆ–æ¨¡å‹ï¼Œå¹¶ä¸”æ— æ³•åœ¨æ²¡æœ‰ç›®æ ‡å›¾åƒçš„æƒ…å†µä¸‹ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½ã€‚(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³•ï¼Œç§°ä¸º ZoDiï¼Œå…¶è®¾è®¡ä¸ºä¸¤æ–¹é¢ï¼šé›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æºå›¾åƒçš„åŸŸè½¬ç§»åˆ°ç›®æ ‡åŸŸæ¥åˆæˆç±»ä¼¼ç›®æ ‡çš„å›¾åƒã€‚å…¶æ¬¡ï¼Œä½¿ç”¨æºå›¾åƒå’Œåˆæˆå›¾åƒä»¥åŠåŸå§‹åˆ†å‰²å›¾è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ¥è‡ªä¸¤ä¸ªåŸŸçš„å›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä»¥å­¦ä¹ åŸŸé²æ£’çš„è¡¨ç¤ºã€‚(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šé€šè¿‡å®éªŒè¡¨æ˜äº† ZoDi åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•çš„å¥½å¤„ã€‚å®ƒè¿˜æ¯”ç°æœ‰çš„åŸºäº CLIP çš„æ–¹æ³•æ›´é€‚ç”¨ï¼Œå› ä¸ºå®ƒä¸å‡è®¾ç‰¹å®šçš„ä¸»å¹²æˆ–æ¨¡å‹ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒæ¥ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½è€Œæ— éœ€ç›®æ ‡å›¾åƒã€‚</li></ol><p>7.Methodsï¼š(1):æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³•ZoDiï¼Œå…¶è®¾è®¡ä¸ºä¸¤æ–¹é¢ï¼šé›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ï¼›(2):åˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹é€šè¿‡å°†æºå›¾åƒçš„åŸŸè½¬ç§»åˆ°ç›®æ ‡åŸŸæ¥åˆæˆç±»ä¼¼ç›®æ ‡çš„å›¾åƒï¼›(3):ä½¿ç”¨æºå›¾åƒå’Œåˆæˆå›¾åƒä»¥åŠåŸå§‹åˆ†å‰²å›¾è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ¥è‡ªä¸¤ä¸ªåŸŸçš„å›¾åƒçš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œä»¥å­¦ä¹ åŸŸé²æ£’çš„è¡¨ç¤ºã€‚</p><ol><li>ç»“è®º(1): æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬åŸŸé€‚åº”æ–¹æ³• ZoDiï¼Œè§£å†³äº†åˆ†å‰²ä»»åŠ¡ä¸­å…³é”®çš„åŸŸåç§»é—®é¢˜ã€‚ZoDi åˆ©ç”¨å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ä»¥é›¶æ ·æœ¬æ–¹å¼å°†æºå›¾åƒè½¬ç§»åˆ°ç›®æ ‡åŸŸã€‚å…¶å›¾åƒè½¬ç§»å’Œæ¨¡å‹é€‚åº”ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ååŒå·¥ä½œï¼Œä¸ºåˆ†å‰²æ¨¡å‹åˆ›å»ºåŸŸé²æ£’è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒZoDi çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œåˆ©ç”¨ç”±çœŸå®å›¾åƒå¼•å¯¼å¹¶è¾…ä»¥éšæœºåæ¼”æŠ€æœ¯çš„å¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¸¦æ¥äº†æˆåŠŸçš„æ€§èƒ½ï¼›å®ƒåœ¨å¹³å‡æ°´å¹³ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä¸ºé›¶æ ·æœ¬åŸŸé€‚åº”ä¸­çš„ä¸€äº›æŒ‘æˆ˜æä¾›äº†æ›´çµæ´»å’Œå¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡ ZoDi ä¸­æå‡ºçš„å›¾åƒè½¬ç§»å…è®¸æˆ‘ä»¬ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†å®ƒä¹Ÿå¯èƒ½å¤±è´¥ï¼Œä¾‹å¦‚æ— æ³•æ­£ç¡®ç”Ÿæˆç‰¹å®šå¯¹è±¡ã€‚æ­£å¦‚ç¬¬ 4.2 èŠ‚æ‰€å»ºè®®çš„ï¼Œä¸€äº›å‰§çƒˆçš„åŸŸå˜åŒ–è¶…å‡ºäº†å…¶èƒ½åŠ›ï¼Œéœ€è¦åœ¨æœªæ¥çš„å‘å±•ä¸­è¿›ä¸€æ­¥å‡†ç¡®çš„å›¾åƒè½¬ç§»ã€‚æ€»ä¹‹ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡æœ¬æ–‡é€šè¿‡æå‡º ZoDi ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä¸ºæ‰©å±•é›¶æ ·æœ¬åŸŸé€‚åº”çš„å¯ç”¨æ€§åšå‡ºäº†è´¡çŒ®ï¼Œè¯¥æ–¹æ³•å¯¹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­è·å–ç›®æ ‡å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§æ—¶å…·æœ‰å®é™…æ„ä¹‰ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶æœ‰åŠ©äºå¼€è¾Ÿæ–°çš„é€”å¾„ï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é€‚åº”æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å›¾åƒè½¬æ¢å’Œæ¨¡å‹é€‚åº”ï¼›æ€§èƒ½ï¼šä¼˜äºç°æœ‰é›¶æ ·æœ¬æ–¹æ³•ï¼Œåœ¨å¹³å‡æ°´å¹³ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›æŠ€æœ¯ï¼›å·¥ä½œé‡ï¼šéœ€è¦åˆæˆå›¾åƒï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ae5014417147f198563c7acb398a02e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b2e24079f3b8238a91b41242e595c773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81336a09088cbd50dacb7bd6f2593d0.jpg" align="middle"></details><h2 id="ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost"><a href="#ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost" class="headerlink" title="ReGround: Improving Textual and Spatial Grounding at No Cost"></a>ReGround: Improving Textual and Spatial Grounding at No Cost</h2><p><strong>Authors:Yuseung Lee, Minhyuk Sung</strong></p><p>When an image generation process is guided by both a text prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated self-attention into the U-Net reveals that spatial grounding often outweighs textual grounding due to the sequential flow from gated self-attention to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring the network architecture, changing from sequential to parallel for gated self-attention and cross-attention. This surprisingly simple yet effective solution does not require any fine-tuning of the network but significantly reduces the trade-off between the two groundings. Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual grounding and spatial grounding. </p><p><a href="http://arxiv.org/abs/2403.13589v1">PDF</a> Project page: <a href="https://re-ground.github.io/">https://re-ground.github.io/</a></p><p><strong>Summary</strong><br>æ–‡æœ¬æç¤ºå’Œç©ºé—´æç¤ºåœ¨å›¾åƒç”Ÿæˆä¸­ç›¸äº’ç«äº‰ï¼Œè°ƒæ•´ç½‘ç»œç»“æ„å¯ç¼“è§£è¿™ç§ç«äº‰ï¼Œæå‡ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾ç‰‡ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œç©ºé—´çº¿ç´¢å¾€å¾€ä¼šç›¸äº’ç«äº‰ã€‚</li><li>ç”±äºé—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›çš„é¡ºåºæµï¼Œç©ºé—´æ¥åœ°å¾€å¾€æ¯”æ–‡æœ¬æ¥åœ°æ›´é‡è¦ã€‚</li><li>é€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæ”¹ä¸ºå¹¶è¡Œï¼Œå¯ä»¥æ˜¾ç€å‡è½»è¿™ç§åè§ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ¥åœ°çš„å‡†ç¡®æ€§ã€‚</li><li>æ­¤è§£å†³æ–¹æ¡ˆæ— éœ€å¯¹ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œå³å¯æ˜¾ç€å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œä»åŸå§‹ GLIGEN åˆ°é‡æ–°å¸ƒçº¿çš„ç‰ˆæœ¬ï¼Œåœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æœ‰æ˜¾ç€æ”¹è¿›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šReGroundï¼šæ— æˆæœ¬æå‡æ–‡æœ¬å’Œç©ºé—´æ¥åœ°</li><li>ä½œè€…ï¼šYuseung Leeã€Minhyuk Sung</li><li>å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢ï¼ˆKAISTï¼‰</li><li>å…³é”®è¯ï¼šæ–‡æœ¬æ¥åœ°ã€ç©ºé—´æ¥åœ°ã€ç½‘ç»œé‡æ„</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13589   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ–‡æœ¬æç¤ºå’Œç©ºé—´æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†ï¼‰å…±åŒæŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸­ç©ºé—´æ¥åœ°å¾€å¾€æ¯”æ–‡æœ¬æ¥åœ°æ›´å ä¼˜åŠ¿ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   GLIGEN æ¨¡å‹ä½¿ç”¨é—¨æ§è‡ªæ³¨æ„åŠ›å®ç°ç©ºé—´æ¥åœ°ï¼Œä½†ç”±äºä»é—¨æ§è‡ªæ³¨æ„åŠ›åˆ°äº¤å‰æ³¨æ„åŠ›çš„é¡ºåºæµç¨‹ï¼Œç©ºé—´æ¥åœ°å¾€å¾€ä¼šå‰Šå¼±æ–‡æœ¬æ¥åœ°ã€‚   ï¼ˆ3ï¼‰è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼š   ReGround æ¨¡å‹é€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œé‡æ„äº†ç½‘ç»œæ¶æ„ã€‚è¿™ç§ç®€å•çš„ä¿®æ”¹æ— éœ€å¾®è°ƒç½‘ç»œï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   ReGround æ¨¡å‹åœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸå§‹ GLIGEN æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡å°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œä»¥ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹å¼è§£å†³äº†æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¿™ç§ç®€å•çš„ä¿®æ”¹æ— éœ€å¾®è°ƒç½‘ç»œï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç®€å•çš„ç½‘ç»œé‡æ„æ–¹æ³•ï¼Œå°†é—¨æ§è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ä»é¡ºåºæµç¨‹æ”¹ä¸ºå¹¶è¡Œæµç¨‹ï¼Œä»è€Œæ”¹å–„äº†æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚</li><li>æ— éœ€å¾®è°ƒç½‘ç»œã€å¼•å…¥æ–°å‚æ•°æˆ–æ”¹å˜ç”Ÿæˆæ—¶é—´å’Œå†…å­˜ï¼Œå³å¯æ˜¾è‘—æé«˜ CLIP åˆ†æ•°ï¼Œè¡¨æ˜æ–‡æœ¬æ¥åœ°ç²¾åº¦æœ‰äº†æ˜¾ç€æé«˜ã€‚</li><li>åœ¨ä¿ç•™ç©ºé—´æ¥åœ°ç²¾åº¦çš„åŒæ—¶æ”¹è¿›äº†æ–‡æœ¬æ¥åœ°ï¼Œåœ¨ MS-COCO-2014 å’Œ MS-COCO-2017 æ•°æ®é›†ä¸Šåˆ†åˆ«ä»¥ 70.25% å’Œ 68.33% çš„ GLIGEN æ€»æ”¹è¿›æé«˜äº† CLIP åˆ†æ•°ï¼ŒåŒæ—¶ä»…é™ä½äº† YOLO åˆ†æ•° 3.31% å’Œ 2.62%ã€‚</li><li>å±•ç¤ºäº†è¿™ç§ç®€å•æœ‰æ•ˆçš„æ–‡æœ¬-ç©ºé—´æ¥åœ°æƒè¡¡è§£å†³æ–¹æ¡ˆå¯ä»¥åˆ©ç”¨ GLIGEN ä½œä¸ºåŸºç¡€åœ¨ä¸åŒçš„æ¡†æ¶ä¸­å¾—åˆ°æ”¹è¿›ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸå§‹ GLIGEN æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>åœ¨ä¸å½±å“ç©ºé—´æ¥åœ°ç²¾åº¦çš„åŒæ—¶æ”¹è¿›äº†æ–‡æœ¬æ¥åœ°ã€‚</li><li>åœ¨ MS-COCO-2014 å’Œ MS-COCO-2017 æ•°æ®é›†ä¸Šåˆ†åˆ«ä»¥ 70.25% å’Œ 68.33% çš„ GLIGEN æ€»æ”¹è¿›æé«˜äº† CLIP åˆ†æ•°ï¼ŒåŒæ—¶ä»…é™ä½äº† YOLO åˆ†æ•° 3.31% å’Œ 2.62%ã€‚å·¥ä½œé‡ï¼š</li><li>æ— éœ€å¾®è°ƒç½‘ç»œã€å¼•å…¥æ–°å‚æ•°æˆ–æ”¹å˜ç”Ÿæˆæ—¶é—´å’Œå†…å­˜ï¼Œå³å¯æ˜¾è‘—å‡å°‘æ–‡æœ¬æ¥åœ°å’Œç©ºé—´æ¥åœ°ä¹‹é—´çš„æƒè¡¡ã€‚</li><li>è¿™ç§ç®€å•çš„ä¿®æ”¹æ˜“äºå®ç°ï¼Œä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-20c026c03fb7bcb10947dfe11c23ee00.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39631ea0f8aa17569d0bb0ff3a13fc82.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8fd766e1117017c44fb1463a8923f02f.jpg" align="middle"></details><h2 id="Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing"><a href="#Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing" class="headerlink" title="Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing"></a>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing</h2><p><strong>Authors:Hangeol Chang, Jinho Chang, Jong Chul Ye</strong></p><p>Despite recent advancements in text-to-image diffusion models facilitating various image editing techniques, complex text prompts often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating grounding during score distillation. This approach ensures a precise reflection of intricate prompt requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted prompts, ensuring high-quality outcomes that respect the original image attributes. </p><p><a href="http://arxiv.org/abs/2403.13551v1">PDF</a> </p><p><strong>Summary</strong><br>å¤æ‚æ–‡æœ¬æç¤ºä¸­çš„å¯¹è±¡ä½ç½®å…ˆéªŒçŸ¥è¯†èå…¥è¯„åˆ†è’¸é¦ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>å¼•å…¥è¯„åˆ†è’¸é¦æœŸé—´çš„groundingï¼Œæé«˜äº†æ¨¡å‹ç¼–è¾‘å“åº”æ–‡æœ¬æŒ‡ç¤ºçš„èƒ½åŠ›ã€‚</li><li>é‡‡ç”¨ä½ç½®å…ˆéªŒçŸ¥è¯†ï¼Œç¡®ä¿å¤æ‚çš„æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å‡†ç¡®åæ˜ ã€‚</li><li>æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”åº¦æŸå¤±æœ‰åŠ©äºç²¾ç¡®å®šä½ç¼–è¾‘åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚</li><li>å®šæ€§å’Œå®šé‡åˆ†æè¡¨æ˜ï¼ŒGround-A-Score æˆåŠŸå“åº”äº†æ‰©å±•ä¸”å¤šæ–¹é¢çš„æç¤ºï¼Œç¡®ä¿äº†é«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚</li><li>Ground-A-Score æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li><li>å®ƒå¯ä»¥å¤„ç†å¤æ‚çš„å¯¹è±¡å’Œåœºæ™¯ï¼Œå¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å¹¿æ³›çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGround-A-Scoreï¼šæå‡è¯„åˆ†çš„å›¾åƒç¼–è¾‘æ–¹æ³•</li><li>ä½œè€…ï¼šHangeol Changã€Jinho Chang å’Œ Jong Chul Ye Kim</li><li>éš¶å±æœºæ„ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢äººå·¥æ™ºèƒ½ç ”ç©¶ç”Ÿé™¢</li><li>å…³é”®è¯ï¼šå›¾åƒç¼–è¾‘ã€æ‰©æ•£æ¨¡å‹ã€åˆ†æ•°è’¸é¦</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå°½ç®¡æœ€è¿‘æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å„ç§å›¾åƒç¼–è¾‘æŠ€æœ¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å¤æ‚çš„æ–‡æœ¬æç¤ºå¾€å¾€ä¼šå¯¼è‡´å¯¹æŸäº›è¯·æ±‚çš„å¿½è§†ï¼Œè¿™æ˜¯ç”±äºåœ¨å¤„ç†æ–‡æœ¬ä¿¡æ¯æ—¶å­˜åœ¨ç“¶é¢ˆã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºè’¸é¦ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚æ–‡æœ¬æç¤ºæ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•å……åˆ†åæ˜ æç¤ºä¸­çš„ç»†è‡´è¦æ±‚ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º Ground-A-Score çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†æ•°è’¸é¦è¿‡ç¨‹ä¸­ç»“åˆäº†æ¥åœ°ï¼Œä»¥ç¡®ä¿å¤æ‚æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å¾—åˆ°ç²¾ç¡®åæ˜ ã€‚è¯¥æ–¹æ³•è€ƒè™‘äº†å›¾åƒä¸­å¯¹è±¡ä½ç½®çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”æŸå¤±æ¥é€‰æ‹©æ€§åœ°åº”ç”¨ï¼Œä»è€Œå¸®åŠ©ç²¾ç¡®åœ°ç¼–è¾‘ç›®æ ‡åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ï¼ŒGround-A-Score è¢«è¯æ˜èƒ½å¤ŸæˆåŠŸåœ°éµå¾ªæ‰©å±•å’Œå¤šæ–¹é¢çš„æç¤ºçš„å¤æ‚ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„è¾“å‡ºï¼ŒåŒæ—¶å°Šé‡åŸå§‹å›¾åƒå±æ€§ã€‚è¿™äº›ç»“æœæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³åœ¨å›¾åƒç¼–è¾‘ä¸­å……åˆ†åˆ©ç”¨æ–‡æœ¬æç¤ºã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><p><strong>ç»“è®º</strong></p><p><strong>ï¼ˆ1ï¼‰æ„ä¹‰</strong></p><p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Ground-A-Score çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†æ•°è’¸é¦è¿‡ç¨‹ä¸­ç»“åˆäº†æ¥åœ°ï¼Œä»¥ç¡®ä¿å¤æ‚æç¤ºè¦æ±‚åœ¨ç¼–è¾‘ç»“æœä¸­å¾—åˆ°ç²¾ç¡®åæ˜ ã€‚</p><p><strong>ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„æƒ©ç½šç³»æ•°å’Œå¯¹æ¯”æŸå¤±ï¼Œä»¥é€‰æ‹©æ€§åœ°åº”ç”¨æ¥åœ°ï¼Œä»è€Œç²¾ç¡®åœ°ç¼–è¾‘ç›®æ ‡åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒä¸­å¯¹è±¡çš„å®Œæ•´æ€§ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­ï¼ŒGround-A-Score è¢«è¯æ˜èƒ½å¤ŸæˆåŠŸåœ°éµå¾ªæ‰©å±•å’Œå¤šæ–¹é¢çš„æç¤ºçš„å¤æ‚ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„è¾“å‡ºï¼ŒåŒæ—¶å°Šé‡åŸå§‹å›¾åƒå±æ€§ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œä»¥è®¡ç®—æ¥åœ°å’Œæƒ©ç½šé¡¹ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ å›¾åƒç¼–è¾‘è¿‡ç¨‹çš„è¿è¡Œæ—¶é—´ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-bc8e986fc373c6b0d99f88ea44ad6e9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-234761e2e42c6b48082fb10939473b0a.jpg" align="middle"></details>## Compress3D: a Compressed Latent Space for 3D Generation from a Single   Image**Authors:Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao**3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU. [PDF](http://arxiv.org/abs/2403.13524v1) **Summary**3Dç”Ÿæˆå–å¾—å·¨å¤§è¿›å±•ï¼Œä½†ä»å•å¼ å›¾ç‰‡é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œæœ‰æ•ˆå‹ç¼©3Då‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚- å¼•å…¥3Dæ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºèƒ½åŠ›ã€‚- åœ¨ä¼˜åŒ–åçš„æ½œåœ¨ç©ºé—´ä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚- åŒæ—¶åˆ©ç”¨å›¾åƒåµŒå…¥å’Œå½¢çŠ¶åµŒå…¥ä½œä¸ºæ¡ä»¶ï¼Œè¿›è¡Œ3Dç”Ÿæˆã€‚- é€šè¿‡æ‰©æ•£å…ˆéªŒæ¨¡å‹ä¼°è®¡å½¢çŠ¶åµŒå…¥ï¼Œæ¡ä»¶ä¸ºå›¾åƒåµŒå…¥ã€‚- æå‡ºæ–¹æ³•ä¼˜äºæœ€å…ˆè¿›ç®—æ³•ï¼Œåœ¨å‡å°‘è®­ç»ƒæ•°æ®å’Œæ—¶é—´çš„æƒ…å†µä¸‹è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚- è¯¥æ–¹æ³•å¯ä»¥åœ¨å•ä¸ªA100 GPUä¸Šä»…éœ€7ç§’å³å¯ç”Ÿæˆé«˜è´¨é‡çš„3Dèµ„äº§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šCompress3Dï¼šä¸€ç§ç”¨äºä»å•å¼ å›¾åƒç”Ÿæˆ 3D çš„å‹ç¼©æ½œåœ¨ç©ºé—´</li><li>ä½œè€…ï¼šBowen Zhang1âˆ—, Tianyu Yang2â€ Yu Li2, Lei Zhang2, and Xi Zhao1â€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¥¿å®‰äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼š3D ç”Ÿæˆã€æ½œåœ¨ç©ºé—´ã€å›¾åƒåˆ° 3D</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13524   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1)ï¼šéšç€ 3D ç”ŸæˆæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œä»å•å¼ å›¾åƒé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ 3D æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚   (2)ï¼šä»¥å¾€æ–¹æ³•å­˜åœ¨æ½œåœ¨ç©ºé—´ç»´åº¦é«˜ã€æ— æ³•åŒæ—¶å‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ç­‰é—®é¢˜ã€‚   (3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°† 3D æ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡ 3D ç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚   (4)ï¼šåœ¨å•è§†å›¾ 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ ShapeNet@IoU å’Œ ShapeNet@CD åº¦é‡ä¸Šåˆ†åˆ«è¾¾åˆ° 75.0% å’Œ 0.042ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p><methods>(1): æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°†3Dæ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚(2): åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§3Dæ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡3Dç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆ 3D çš„ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨é«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è·å¾—å‹ç¼©çš„æ½œåœ¨ç©ºé—´ï¼Œæˆ‘ä»¬åœ¨ä» 3D æ¨¡å‹åˆ°æ½œåœ¨ç©ºé—´çš„æŠ•å½±è¿‡ç¨‹ä¸­æ·»åŠ å¯å­¦ä¹ å‚æ•°ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ä¸‰å¹³é¢è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°† 3D æ¨¡å‹ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„ä¸‰å¹³é¢æ½œåœ¨ç©ºé—´ï¼Œæœ‰æ•ˆå‹ç¼©å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚</li><li>åœ¨è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶å†…ï¼Œå¼•å…¥äº†ä¸€ç§ 3D æ„ŸçŸ¥äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨è¡¨ç¤ºæŸ¥è¯¢é«˜åˆ†è¾¨ç‡ 3D ç‰¹å¾é‡çš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºäº†ç”Ÿæˆæ¨¡å‹çš„å‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å•è§†å›¾ 3D ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ ShapeNet@IoU å’Œ ShapeNet@CD åº¦é‡ä¸Šåˆ†åˆ«è¾¾åˆ° 75.0% å’Œ 0.042ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´è¶…å‚æ•°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-2b0e4ca13bab87985745a78f5fd676d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d6bbc026dfa2d94e88661f67cb44fcfe.jpg" align="middle"></details><h2 id="Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion"><a href="#Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion" class="headerlink" title="Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion"></a>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</h2><p><strong>Authors:Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss</strong></p><p>Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data. </p><p><a href="http://arxiv.org/abs/2403.13470v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨ 3D LiDAR æ‰«æå•æ¬¡å®Œæˆåœºæ™¯ç‚¹äº‘ï¼Œæ‰©å……æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé¢†åŸŸçš„åº”ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸Šçš„æˆåŠŸåº”ç”¨å¯å‘äº†å…¶åœ¨ç‚¹äº‘åœºæ™¯å®Œæˆä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚</li><li>ä»¥å¾€å°† LiDAR æ•°æ®æå–èŒƒå›´å›¾åƒçš„æ–¹æ³•ä¸é€‚ç”¨äºåœºæ™¯å°ºåº¦çš„æ•°æ®å¤„ç†ã€‚</li><li>è¯¥ç ”ç©¶ç›´æ¥å¯¹ç‚¹äº‘æ“ä½œï¼Œé‡æ–°åˆ¶å®šäº†æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æœ‰æ•ˆå¤„ç†åœºæ™¯å°ºåº¦æ•°æ®ã€‚</li><li>æå‡ºæ­£åˆ™åŒ–æŸå¤±æ¥ç¨³å®šå»å™ªè¿‡ç¨‹ä¸­çš„é¢„æµ‹å™ªå£°ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å•æ¬¡ LiDAR æ‰«æå®Œæˆåœºæ™¯ï¼Œç”Ÿæˆæ›´ç²¾ç»†çš„åœºæ™¯ã€‚</li><li>è¯¥ç ”ç©¶æå‡ºçš„æ‰©æ•£è¿‡ç¨‹å…¬å¼å¯ä¸ºåŸºäºç‚¹äº‘æ•°æ®çš„æ‰©æ•£æ¨¡å‹ç ”ç©¶æä¾›æ”¯æŒã€‚</li><li>ç›´æ¥æ“ä½œç‚¹äº‘çš„æ–¹æ³•é¿å…äº†å›¾åƒå¤„ç†ä¸­çš„é‡‡æ ·å’Œé‡åŒ–è¯¯å·®ï¼Œä¿ç•™äº†åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„çœŸå®ä¸–ç•Œ 3D æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨</li><li>ä½œè€…ï¼šLukas Lyu, Alexander Meuleman, Christian Haene</li><li>éš¶å±æœºæ„ï¼šæ³¢æ©å¤§å­¦</li><li>å…³é”®è¯ï¼šæ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç‚¹äº‘å¤„ç†</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.13470   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/PRBonn/LiDiff</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šè®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶çš„æ„ŸçŸ¥å †æ ˆä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚è¿™äº›æ–¹æ³•ç”¨äºç»™å®šä¼ æ„Ÿå™¨æ•°æ®æ„ŸçŸ¥è½¦è¾†å‘¨å›´ç¯å¢ƒã€‚3D æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨é€šå¸¸ç”¨äºä»åœºæ™¯ä¸­æ”¶é›†ç¨€ç– 3D ç‚¹äº‘ã€‚ç„¶è€Œï¼Œä¸äººç±»æ„ŸçŸ¥ç›¸æ¯”ï¼Œè¿™äº›ç³»ç»Ÿéš¾ä»¥ä»…å‡­è¿™äº›ç¨€ç–ç‚¹äº‘æ¨æ–­å‡ºåœºæ™¯ä¸­ä¸å¯è§çš„éƒ¨åˆ†ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåœºæ™¯è¡¥å…¨ä»»åŠ¡æ—¨åœ¨é¢„æµ‹æ¿€å…‰é›·è¾¾æµ‹é‡ä¸­çš„ç©ºç™½ï¼Œä»¥å®ç°æ›´å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚é‰´äºæœ€è¿‘æ‰©æ•£æ¨¡å‹ä½œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹å–å¾—çš„è‰¯å¥½ç»“æœï¼Œæˆ‘ä»¬æå‡ºå°†å®ƒä»¬æ‰©å±•åˆ°å®ç°å•æ¬¡ 3D æ¿€å…‰é›·è¾¾æ‰«æçš„åœºæ™¯è¡¥å…¨ã€‚(2) è¿‡å»çš„æ–¹æ³•ï¼šä»¥å‰çš„å·¥ä½œä½¿ç”¨ä»æ¿€å…‰é›·è¾¾æ•°æ®ä¸­æå–çš„èŒƒå›´å›¾åƒä¸Šçš„æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥åº”ç”¨åŸºäºå›¾åƒçš„æ‰©æ•£æ–¹æ³•ã€‚ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºç›´æ¥å¯¹ç‚¹è¿›è¡Œæ“ä½œï¼Œé‡æ–°è¡¨è¿°åŠ å™ªå’Œå»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†åœºæ™¯è§„æ¨¡ã€‚(3) ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–æŸå¤±æ¥ç¨³å®šå»å™ªè¿‡ç¨‹ä¸­é¢„æµ‹çš„å™ªå£°ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»…ä»¥å•æ¬¡æ¿€å…‰é›·è¾¾æ‰«æä½œä¸ºè¾“å…¥æ¥å®Œæˆåœºæ™¯ï¼Œä¸æœ€å…ˆè¿›çš„åœºæ™¯è¡¥å…¨æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬æå‡ºçš„æ‰©æ•£è¿‡ç¨‹å…¬å¼å¯ä»¥æ”¯æŒå°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåœºæ™¯è§„æ¨¡ç‚¹äº‘æ•°æ®çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚(4) æ–¹æ³•çš„åº”ç”¨å’Œæ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒå¯ä»¥ç”Ÿæˆå…·æœ‰æ›´å¤šç»†èŠ‚çš„å®Œæ•´åœºæ™¯ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´èƒ½ä¿ç•™åœºæ™¯çš„å‡ ä½•ç»“æ„ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§æœ‰æ•ˆä¸”å‡†ç¡®çš„åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»…ä½¿ç”¨å•æ¬¡æ¿€å…‰é›·è¾¾æ‰«ææ¥ç”Ÿæˆé«˜è´¨é‡çš„åœºæ™¯è¡¨ç¤ºã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºä½¿ç”¨ DDPM ä»å•ä¸ª 3D æ¿€å…‰é›·è¾¾æ‰«æä½œä¸ºè¾“å…¥æ¥å®ç°åœºæ™¯è¡¥å…¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°† DDPM [19,20,47] é‡æ–°è¡¨è¿°ä¸ºé€‚ç”¨äºåœºæ™¯è§„æ¨¡ã€‚æˆ‘ä»¬ä¸å½’ä¸€åŒ–è¾“å…¥ç‚¹äº‘ï¼Œè€Œæ˜¯é’ˆå¯¹æ¯ä¸ªç‚¹å±€éƒ¨æ·»åŠ å’Œé¢„æµ‹å™ªå£°ã€‚åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾“å…¥æ‰«æå¯¹å™ªå£°é¢„æµ‹è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥ä¾¿æœ€ç»ˆåœºæ™¯ä¿ç•™è¾“å…¥æ‰«æçš„ç»“æ„ä¿¡æ¯ï¼ŒåŒæ—¶æ¨æ–­å‡ºç¼ºå¤±éƒ¨åˆ†ã€‚åœ¨è¿™ç§è¡¨è¿°ä¸­ï¼Œåˆå§‹ç‚¹äº‘æ˜¯è¾“å…¥æ‰«æçš„å™ªå£°ç‰ˆæœ¬ï¼Œç„¶åç½‘ç»œçš„ä»»åŠ¡æ˜¯å»å™ªä»¥è·å¾—å®Œæ•´åœºæ™¯ï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚</p><p>ï¼ˆ2ï¼‰æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æä¾›äº†æ‰©æ•£æ¨¡å‹çš„å¿…è¦èƒŒæ™¯ï¼Œå¹¶æè¿°äº†æˆ‘ä»¬æ–¹æ³•çš„å„ä¸ªç»„æˆéƒ¨åˆ†ã€‚</p><p>ï¼ˆ3ï¼‰å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ [6,11,27] å°†æ•°æ®ç”Ÿæˆè¡¨è¿°ä¸ºä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ã€‚é€šå¸¸ï¼Œæ¨¡å‹ä»é«˜æ–¯å™ªå£° [6,11,27] å¼€å§‹ï¼Œå¹¶ä»è¾“å…¥ä¸­è¿­ä»£ç§»é™¤å™ªå£°ï¼Œç›´åˆ°æ”¶æ•›åˆ°ç›®æ ‡è¾“å‡ºï¼ˆä¾‹å¦‚ï¼Œå›¾åƒ [6,11,27,28,30,33,48,49] æˆ–å½¢çŠ¶ [19,20,35,36,43,45,47]ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ªå‰å‘æ‰©æ•£è¿‡ç¨‹æ¥å®ç°ï¼Œå…¶ä¸­å™ªå£°åœ¨ T æ¬¡ä¸­è¿­ä»£æ·»åŠ åˆ°ç›®æ ‡æ•°æ®ä¸­ã€‚ç„¶åï¼Œè®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹åœ¨æ¯ä¸ªæ­¥éª¤ä¸­æ·»åŠ çš„å™ªå£°ã€‚é€šè¿‡é¢„æµ‹æ¯ä¸ªæ­¥éª¤çš„å™ªå£°å¹¶å°†å…¶ç§»é™¤ï¼Œå»å™ªæ ·æœ¬åº”è¯¥æ›´æ¥è¿‘ç›®æ ‡è®­ç»ƒæ•°æ®ã€‚Ho ç­‰äºº [11] è¡¨è¿°çš„æ‰©æ•£è¿‡ç¨‹é€šå¸¸å¯ä»¥å†™æˆå¦‚ä¸‹å½¢å¼ã€‚ç»™å®šä»ç›®æ ‡æ•°æ®åˆ†å¸ƒä¸­æŠ½å–çš„æ ·æœ¬ x0âˆ¼q(x)ï¼Œæ‰©æ•£è¿‡ç¨‹åœ¨ T æ­¥ä¸­å‘ x0 æ·»åŠ å™ªå£°ï¼Œå¾—åˆ° x1,...,xTï¼Œå…¶ä¸­ qï¿½xTï¿½â‰ˆN(0,I)ï¼Œå…¶ä¸­ N(0,I) æ˜¯å‡å€¼ä¸º 0ã€å¯¹è§’åæ–¹å·®ä¸ºå•ä½çŸ©é˜µ I çš„æ­£æ€åˆ†å¸ƒã€‚è¿™ä¸ªæ‰©æ•£è¿‡ç¨‹ç”±ä¸€ç³»åˆ—å®šä¹‰çš„å™ªå£°å› å­ Î²1,...,Î²T å‚æ•°åŒ–ï¼Œå…¶ä¸­åœ¨æ¯ä¸ªæ­¥éª¤ t ä¸­ï¼Œè¿­ä»£é‡‡æ ·é«˜æ–¯å™ªå£°å¹¶æ ¹æ® Î²t æ·»åŠ åˆ° xtâˆ’1 ä¸­ã€‚è¿™å¯ä»¥ç®€åŒ–ä¸ºä» x0 é‡‡æ · xtï¼Œè€Œæ— éœ€è®¡ç®—ä¸­é—´æ­¥éª¤ x1,...,xtâˆ’1ã€‚ä¸ºæ­¤ï¼ŒHo ç­‰äºº [11] å®šä¹‰ Î±t=1âˆ’Î²t å’Œ Î±t=ï¿½ti=1Î±iï¼Œå¹¶ä¸” xt å¯ä»¥é‡‡æ ·ä¸ºï¼šxt=âˆšÎ±tx0+âˆš1âˆ’Î±tÏµ,(1)å…¶ä¸­ Ïµâˆ¼N(0,I)ã€‚æ³¨æ„ï¼Œå½“ T è¶³å¤Ÿå¤§æ—¶ qï¿½xTï¿½â‰ˆN(0,I)ï¼Œå› ä¸º Î±T æ¥è¿‘äºé›¶ã€‚å»å™ªè¿‡ç¨‹æ—¨åœ¨é€šè¿‡é¢„æµ‹åœ¨æ¯ä¸ªæ­¥éª¤æ·»åŠ çš„å™ªå£° Ïµ æ¥æ’¤æ¶ˆ T ä¸ªå™ªå£°æ­¥éª¤ [11]ã€‚ç»™å®šä¸€ä¸ªåˆå§‹ xTï¼Œæˆ‘ä»¬å¸Œæœ›é€†è½¬æ‰©æ•£è¿‡ç¨‹å¹¶å¾—åˆ° x0ã€‚åå‘æ‰©æ•£æ­¥éª¤å¯ä»¥å†™æˆï¼šxtâˆ’1=xtâˆ’1âˆ’Î±tâˆš1âˆ’Î±tÏµÎ¸ï¿½xt,tï¿½+1âˆ’Î±tâˆ’11âˆ’Î±tÎ²tN(0,I),(2)å…¶ä¸­ ÏµÎ¸(xt,t) æ˜¯åœ¨æ­¥éª¤ t ä» xt é¢„æµ‹çš„å™ªå£°ã€‚è¿™ä¸ªç”Ÿæˆä¹Ÿå¯ä»¥åœ¨ç»™å®šæ¡ä»¶ c çš„æƒ…å†µä¸‹è¿›è¡Œå¼•å¯¼ã€‚è¿™ç§æ¡ä»¶ç”Ÿæˆå¯ä»¥æ¥è‡ªé¢„è®­ç»ƒçš„ç¼–ç å™¨ [6] æˆ–æ— åˆ†ç±»å™¨æŒ‡å¯¼ [10]ï¼Œå…¶ä¸­ç¼–ç å™¨ä¸å™ªå£°é¢„æµ‹å™¨ä¸€èµ·è®­ç»ƒã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œå› ä¸ºå®ƒä¸éœ€è¦é¢„è®­ç»ƒçš„ç¼–ç å™¨ã€‚ä½¿ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œæ¨¡å‹è¢«è®­ç»ƒæ¥å­¦ä¹ æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°åˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹éƒ½æœ‰ä¸€å®šçš„æ¦‚ç‡ p é¢„æµ‹æ— æ¡ä»¶å™ªå£°åˆ†å¸ƒï¼Œå…¶ä¸­æ¡ä»¶è®¾ç½®ä¸º null ä»¤ç‰Œï¼Œå³ c=âˆ…ã€‚è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–å»å™ªæ¨¡å‹ä»¥é¢„æµ‹ç»™å®šè¾“å…¥æ·»åŠ åˆ°æ­¥éª¤çš„å™ªå£° Ïµã€‚ç»™å®šè¾“å…¥ x0 å’Œæ¡ä»¶ cï¼Œéšæœºé‡‡æ ·æ­¥éª¤ tâˆˆ[0,T]ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯å™ªå£° Ïµ ä»æ–¹ç¨‹å¼ (1) é‡‡æ · xtã€‚ç„¶åï¼Œä» xtã€c å’Œ tï¼Œæ¨¡å‹è®¡ç®—å™ªå£°é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨ L2 æŸå¤±å¯¹å…¶è¿›è¡Œç›‘ç£ï¼šLï¿½xt,Ëœc,tï¿½=ï¿½ï¿½Ïµâˆ’ÏµÎ¸ï¿½xt,Ëœc,tï¿½ï¿½ï¿½2,(3)å…¶ä¸­ Ëœcâˆ¼B(p) å…¶ä¸­ B æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œæ²¡æœ‰ç»“æœ {âˆ…,c}ï¼Œâˆ… å‘ç”Ÿçš„æ¦‚ç‡ä¸º pã€‚æ¨æ–­ä»åˆå§‹ xTâˆ¼N(0,I) å¼€å§‹ï¼Œå¹¶è¿­ä»£å»å™ªä»¥è·å¾— x0ã€‚å¯¹äºæ— åˆ†ç±»å™¨æŒ‡å¯¼ [10]ï¼Œæˆ‘ä»¬é¢„æµ‹æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°åˆ†å¸ƒï¼Œå¹¶è®¡ç®—æœ€ç»ˆé¢„æµ‹çš„å™ªå£°ä¸ºï¼šÏµâ€²Î¸ï¿½xt,c,tï¿½=ÏµÎ¸ï¿½xt,âˆ…,tï¿½+sï¿½ÏµÎ¸ï¿½xt,c,tï¿½âˆ’ÏµÎ¸ï¿½xt,âˆ…,tï¿½ï¿½,(4)å…¶ä¸­ sâˆˆR æ˜¯å¯¹ c è¿›è¡ŒåŠ æƒçš„æ¡ä»¶å‚æ•°ï¼ŒÏµÎ¸(xt,âˆ…,t) æ˜¯æ— æ¡ä»¶å™ªå£°é¢„æµ‹ã€‚ä½¿ç”¨æ–¹ç¨‹å¼ (4)ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•æ­¥éª¤ä¸­è®¡ç®—å™ªå£°ï¼Œä»ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–¹ç¨‹å¼ (2) è®¡ç®— xTâˆ’1,...,x0ï¼Œå…¶ä¸­ x0 æ˜¯æ¡ä»¶ä¸º c çš„æ–°ç”Ÿæˆæ ·æœ¬ã€‚</p><p>ï¼ˆ4ï¼‰æ‰©æ•£åœºæ™¯è¡¥å…¨ï¼šåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ DDPM çš„ç”Ÿæˆæ–¹é¢æ¥å®Œæˆæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨åœ¨å•ä¸ªæ‰«æä¸­æµ‹é‡çš„åœºæ™¯ã€‚ä¸å½¢çŠ¶è¡¥å…¨ [19,20,47] ç±»ä¼¼ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªéƒ¨åˆ†ç‚¹äº‘ P={p1,...,pN} å…¶ä¸­ pâˆˆR3ï¼Œè¾“å‡ºåº”è¯¥æ˜¯å®Œæ•´ç‚¹äº‘ Pâ€²={pâ€²1,...,pâ€²M} å…¶ä¸­ pâ€²âˆˆR3ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œéƒ¨åˆ†ç‚¹äº‘æ˜¯å•ä¸ªæ¿€å…‰é›·è¾¾æ‰«æï¼Œæˆ‘ä»¬å¸Œæœ›ä»ä¸­å®ç°åœºæ™¯è¡¥å…¨ã€‚ç»™å®šä¸€ç³»åˆ—è¿ç»­çš„æ¿€å…‰é›·è¾¾æ‰«æåŠå…¶å§¿æ€ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªåœ°å›¾ï¼Œå¹¶é’ˆå¯¹å•ä¸ªæ‰«æ P é‡‡æ ·å®Œæ•´åœºæ™¯ ground truth Gï¼Œå…¶ä¸­æˆ‘ä»¬çš„åœºæ™¯è¡¥å…¨ Pâ€² åº”è¯¥å°½å¯èƒ½æ¥è¿‘ Gã€‚ç»™å®šè¾“å…¥æ‰«æ P å’Œ ground truth G çš„å¯¹ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒ DDPM æ¥å®ç°åœºæ™¯è¡¥å…¨ã€‚æ­£å¦‚ç¬¬ 3.1 èŠ‚æ‰€è¿°ï¼Œæˆ‘ä»¬å¯ä»¥ä»å®Œæ•´åœºæ™¯ G ä¸­è®¡ç®—æ­¥éª¤ t çš„å™ªå£°ç‚¹äº‘ Gtï¼šptm=âˆšÎ±tpm+âˆš1âˆ’Î±tÏµ,âˆ€pmâˆˆG,(5)å…¶ä¸­ Gt={pt1,...,ptM}ã€‚</p><p>ï¼ˆ5ï¼‰å±€éƒ¨ç‚¹å»å™ªï¼šç¬¬ 3.2 èŠ‚ä¸­è¯¦ç»†æè¿°çš„è¡¨è¿°é€šå¸¸ç”¨äºå½¢çŠ¶è¡¥å…¨ [20,47]ã€‚å°½ç®¡å½¢çŠ¶è¡¥å…¨å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†è¯¥è¡¨è¿°å¯èƒ½ä¸ç›´æ¥é€‚ç”¨äºåœºæ™¯è§„æ¨¡ã€‚å¯¹äºå•ä¸ªç‰©ä½“å½¢çŠ¶ï¼Œæ•°æ®è¦ä¹ˆå½’ä¸€åŒ–ï¼Œè¦ä¹ˆå¤„äºæ¥è¿‘å‡å€¼ Âµ=0 å’Œæ ‡å‡†å·® Î£=I çš„é«˜æ–¯åˆ†å¸ƒçš„å°èŒƒå›´å†…ã€‚å¯¹äºåœºæ™¯è§„æ¨¡ï¼Œæ¿€å…‰é›·è¾¾æ•°æ®å…·æœ‰æ›´å¤§çš„æ¯”ä¾‹ï¼Œå¹¶ä¸”æ•°æ®èŒƒå›´å› ç‚¹äº‘è½´è€Œå¼‚ã€‚å› æ­¤ï¼Œè¾“å…¥æ•°æ®åˆ†å¸ƒè¿œéé«˜æ–¯åˆ†å¸ƒ N(0,I)ï¼Œå¦‚æœæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œæˆ‘ä»¬å°†ä¸¢å¤±åœºæ™¯ä¸­çš„è®¸å¤šç»†èŠ‚ï¼Œå› ä¸ºåœºæ™¯è¢«å‹ç¼©åˆ°ä¸€ä¸ª much smaller range ä¸­ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå±€éƒ¨é—®é¢˜ã€‚æˆ‘ä»¬ä¸å°† xt é‡‡æ ·ä¸º Ïµâˆ¼N(0,I) å’Œ x0 ä¹‹é—´çš„æ··åˆåˆ†å¸ƒï¼Œå¦‚æ–¹ç¨‹å¼ (1) æ‰€ç¤ºï¼Œè€Œæ˜¯å°†æ‰©æ•£è¿‡ç¨‹è¡¨è¿°ä¸ºå±€éƒ¨æ·»åŠ åˆ°æ¯ä¸ªç‚¹ pm çš„å™ªå£°åç§»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»æ–¹ç¨‹å¼ (1) ä¸­ï¼Œæˆ‘ä»¬è®¾ç½® x0=0 å¹¶å°† xt æ·»åŠ åˆ° pmï¼šptm=pm+ï¿½âˆšÎ±t0+âˆš1âˆ’Î±tÏµï¿½,(8)=pm+âˆš1âˆ’Î±tÏµ.(9)</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ¿€å…‰é›·è¾¾åœºæ™¯è¡¥å…¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä»å•ä¸ªç¨€ç–æ¿€å…‰é›·è¾¾æ‰«æä¸­ç”Ÿæˆç¼ºå¤±éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨å±€éƒ¨ç‚¹å»å™ªä¸­é‡æ–°è¡¨è¿°äº†æ‰©æ•£è¿‡ç¨‹ï¼Œå°†æ¯ä¸ªç‚¹å®šä¹‰ä¸ºé‡‡æ ·é«˜æ–¯å™ªå£°çš„åŸç‚¹ï¼Œå­¦ä¹ äº†ä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œä»¥é€æ­¥é¢„æµ‹åç§»é‡ï¼Œä»è€Œä»è¾“å…¥çš„å™ªå£°æ¿€å…‰é›·è¾¾æ‰«æé‡å»ºåœºæ™¯ã€‚è¿™ç§è¡¨è¿°ä½¿å¾—å¤„ç†åœºæ™¯è§„æ¨¡çš„ 3D æ•°æ®æˆä¸ºå¯èƒ½ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™äº†æ›´å¤šç»†èŠ‚ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§å±€éƒ¨ç‚¹å»å™ªæ–¹æ³•ï¼Œå°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è¡¨è¿°ä¸ºå±€éƒ¨é—®é¢˜ï¼Œå°†æ¯ä¸ªç‚¹å®šä¹‰ä¸ºé‡‡æ ·é«˜æ–¯å™ªå£°çš„åŸç‚¹ï¼Œå­¦ä¹ äº†ä¸€ä¸ªè¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œä»¥é€æ­¥é¢„æµ‹åç§»é‡ï¼Œä»è€Œä»è¾“å…¥çš„å™ªå£°æ¿€å…‰é›·è¾¾æ‰«æé‡å»ºåœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†åœºæ™¯è§„æ¨¡çš„ 3D æ•°æ®ï¼Œåœ¨å»å™ªè¿‡ç¨‹ä¸­ä¿ç•™äº†æ›´å¤šç»†èŠ‚ã€‚</li><li>è¯¥æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸æœ€å…ˆè¿›çš„æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚æ€§èƒ½ï¼š</li><li>è¯¥æ–¹æ³•åœ¨åœºæ™¯è¡¥å…¨ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>ä¸æœ€å…ˆè¿›çš„æ‰©æ•£å’Œéæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆçš„åœºæ™¯å…·æœ‰æ›´å¤šç»†èŠ‚ã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå®ç°åœºæ™¯è¡¥å…¨ï¼Œå› ä¸ºå…¶ç”Ÿæˆæ˜¯æ ¹æ®è¾“å…¥æ¿€å…‰é›·è¾¾æ‰«æè¿›è¡Œè°ƒèŠ‚çš„ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ä¸€ä¸ªè¾“å…¥æ‰«ææ¥æŒ‡å¯¼ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚</li><li>è¯¥æ–¹æ³•ç›®å‰è¿˜ä¸èƒ½ç”Ÿæˆæ— æ¡ä»¶æ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç”Ÿæˆæ–°é¢–çš„ 3D ç‚¹äº‘åœºæ™¯æ–¹é¢çš„åº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-53c84c0442dd3d75fd891aa17f099a89.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7912e5b6f5a348c3d71de1a3cf80265b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b2627632f0f47e537f8969486c44b93a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c8380fe238cf40cd25f36e52373bb013.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-23  GRM Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>SyncTalkå®éªŒç¬”è®°</title>
    <link href="https://kedreamix.github.io/2024/03/18/Project/SyncTalk/"/>
    <id>https://kedreamix.github.io/2024/03/18/Project/SyncTalk/</id>
    <published>2024-03-18T12:37:00.000Z</published>
    <updated>2024-03-20T04:48:20.439Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="synctalk"></p><h2 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h2><h3 id="Facial-Animation-Capturer"><a href="#Facial-Animation-Capturer" class="headerlink" title="Facial Animation Capturer"></a>Facial Animation Capturer</h3><p>Blendshapeçš„æå–å¯å‚è€ƒ </p><p><a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><p><img src="https://picx.zhimg.com/v2-2498ec39938d865073b5cbaae63fdef9.png" alt=""></p><p><img src="https://picx.zhimg.com/v2-8392dcadaf5221c5298ed49baeac28a9.png" alt=""></p><h2 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h2><p><strong>Head Motion Tracker</strong></p><p>å¤´éƒ¨å§¿åŠ¿ï¼Œè¡¨ç¤ºä¸º pï¼Œæ˜¯æŒ‡äººçš„å¤´éƒ¨åœ¨ 3D ç©ºé—´ä¸­çš„æ—‹è½¬è§’åº¦ï¼Œç”±æ—‹è½¬ R å’Œå¹³ç§» T å®šä¹‰ã€‚</p><p>ä¸ç¨³å®šçš„å¤´éƒ¨å§¿åŠ¿ä¼šå¯¼è‡´å¤´éƒ¨æŠ–åŠ¨ï¼Œæ‰€ä»¥ä¸ºäº†è·å¾—å¤´éƒ¨å§¿åŠ¿çš„ç²—ç•¥ä¼°è®¡ã€‚é¦–å…ˆï¼Œé€šè¿‡åœ¨é¢„å®šèŒƒå›´å†…è¿­ä»£ i æ¬¡æ¥ç¡®å®šæœ€ä½³ç„¦è·ï¼Œå¯¹äºæ¯ä¸ªç„¦è·å€™é€‰ fiï¼Œé‡æ–°åˆå§‹åŒ–æ—‹è½¬å’Œå¹³ç§»å€¼ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ– 3D å¯å˜å½¢æ¨¡å‹ (3DMM) çš„æŠ•å½±åœ°æ ‡ä¸è§†é¢‘å¸§ä¸­çš„å®é™…åœ°æ ‡ä¹‹é—´çš„è¯¯å·®ã€‚</p><p><img src="https://picx.zhimg.com/v2-cd96b85183a33c3b785c76d15344f433.png" alt="image-20240318205920014"></p><p>å…¶ä¸­ $E_i$ è¡¨ç¤ºçš„å°±æ˜¯ MSEï¼Œè¿™æ ·èƒ½å¤Ÿä»¥æ›´å¥½åœ°å°†æ¨¡å‹çš„æŠ•å½± lmk ä¸å®é™…è§†é¢‘ lmk å¯¹é½ï¼Œç„¶åå¾—åˆ°æœ€ä¼˜çš„æ—‹è½¬å’Œå¹³ç§»çŸ©é˜µï¼Œä¹Ÿæ˜¯ç”¨ MSE æ¥æœ€å°åŒ–ï¼Œè¿™æ˜¯å¯¹æ¯ä¸€å¸§è¿›è¡Œæ“ä½œçš„ï¼Œåœ¨å¯¹åº”è§†é¢‘å¸§çš„æœ€ä¼˜å€¼ã€‚</p><p><img src="https://picx.zhimg.com/v2-279c71feaa74b2e765d97c881e4da608.png" alt="image-20240318211905521"></p><p>è¿™ä¸€éƒ¨åˆ†å®é™…ä¸Šå’ŒåŸæ¥çš„ä»£ç å·®åˆ«ä¸å¤§ï¼Œå¯ä»¥è°ƒæ•´ä¸€ä¸‹æ‰€æœ‰å¸§å’Œå¯¹åº”çš„ä¼˜åŒ–éƒ¨åˆ†ï¼Œæ¯”å¦‚600~1500çš„æ­¥é•¿å¯ä»¥è®¾ç½®ä¸º50ï¼ŒåŸæœ¬æ˜¯100ï¼Œå› ä¸ºç»“æœä¹Ÿå‘ç°æ˜¯1350</p><p><img src="https://pic1.zhimg.com/v2-fe6fb504cb27b75a3ca8641c715629b5.png" alt=""></p><p><strong>Head Points Tracker</strong></p><p>å¯¹äºä¹‹å‰åŸºäº NeRF çš„æ–¹æ³•æ¥è¯´ï¼Œå…ˆå‰çš„æ–¹æ³•åˆ©ç”¨åŸºäº 3DMM çš„æŠ€æœ¯æ¥æå–å¤´éƒ¨å§¿åŠ¿å¹¶ç”Ÿæˆä¸å‡†ç¡®çš„ç»“æœã€‚ä¸ºäº†æé«˜ R å’Œ T çš„ç²¾åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨åƒ <a href="https://arxiv.org/html/2307.07635v2">Co- tracker</a> è¿™æ ·çš„å…‰æµä¼°è®¡æ¨¡å‹æ¥è·Ÿè¸ªé¢éƒ¨å…³é”®ç‚¹ Kã€‚</p><p><img src="https://pica.zhimg.com/v2-1a4d6600883ddfe2e4438913f829716a.png" alt=""></p><p>æ¥ä¸‹æ¥ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å…‰æµä¼°è®¡æ¨¡å‹ï¼Œåœ¨è·å–é¢éƒ¨è¿åŠ¨å…‰æµåï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ‹‰æ™®æ‹‰æ–¯æ»¤æ³¢å™¨</strong>é€‰æ‹©ä½äºæœ€æ˜¾è‘—æµå˜åŒ–ä½ç½®çš„å…³é”®ç‚¹ï¼Œå¹¶åœ¨æµåºåˆ—ä¸­è·Ÿè¸ªè¿™äº›å…³é”®ç‚¹çš„è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡è¿™ä¸ªæ¨¡å—ç¡®ä¿äº†æ‰€æœ‰å¸§ä¸Šçš„é¢éƒ¨å…³é”®ç‚¹å¯¹é½æ›´åŠ ç²¾ç¡®å’Œä¸€è‡´ï¼Œä»è€Œå¢å¼ºäº†å¤´éƒ¨å§¿åŠ¿å‚æ•°çš„å‡†ç¡®æ€§ã€‚</p><p><img src="https://pica.zhimg.com/v2-b089529e446c0280c4d3da5c08770f64.png" alt=""></p><p><strong>Bundle Adjustment</strong></p><p>æ ¹æ®å…³é”®ç‚¹å’Œç²—ç•¥çš„å¤´éƒ¨å§¿åŠ¿ï¼Œå¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µä¼˜åŒ–æ¡†æ¶æ¥æé«˜å…³é”®ç‚¹å’Œå¤´éƒ¨å§¿åŠ¿ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</p><ul><li>ç¬¬ä¸€é˜¶æ®µï¼Œéšæœºåˆå§‹åŒ– j ä¸ªå…³é”®ç‚¹çš„ 3D åæ ‡å¹¶ä¼˜åŒ–å®ƒä»¬çš„ä½ç½®ï¼Œä»¥ä¾¿ä¸å›¾åƒå¹³é¢ä¸Šè·Ÿè¸ªçš„å…³é”®ç‚¹å¯¹é½ã€‚è¿™ä¸€éƒ¨åˆ†æœ€å°åŒ–æŸå¤±å‡½æ•° $L_{init}$ï¼Œæ•è·<strong>æŠ•å½±å…³é”®ç‚¹ P å’Œè·Ÿè¸ªå…³é”®ç‚¹ K</strong> ä¹‹é—´çš„å·®å¼‚ï¼š</li><li>ç¬¬äºŒé˜¶æ®µï¼Œå¼€å§‹è¿›è¡Œæ›´å…¨é¢çš„ä¼˜åŒ–ï¼Œä»¥ç»†åŒ– 3D å…³é”®ç‚¹å’Œç›¸å…³çš„å¤´éƒ¨è”åˆå§¿åŠ¿å‚æ•°ï¼Œé€šè¿‡ Adam ä¼˜åŒ–å™¨ä¼˜åŒ–ç®—æ³•ï¼Œ<strong>è°ƒæ•´ç©ºé—´åæ ‡ã€æ—‹è½¬è§’åº¦ R å’Œå¹³ç§» T</strong> ä»¥æœ€å°åŒ–å¯¹é½è¯¯å·® $L_{sec}$ï¼Œè¡¨ç¤ºä¸ºï¼š</li></ul><p>ç»è¿‡è¿™äº›ä¼˜åŒ–åï¼Œè§‚å¯Ÿåˆ°æ‰€å¾—çš„å¤´éƒ¨å§¿åŠ¿å’Œå¹³ç§»å‚æ•°å¹³æ»‘ä¸”ç¨³å®šã€‚</p><blockquote><p>ç°åœ¨çš„é¢éƒ¨è·Ÿè¸ªæŠ€æœ¯ï¼ˆFace Trackingï¼‰é€šå¸¸ç»“åˆäº†å¤šç§ç®—æ³•å’ŒæŠ€æœ¯ï¼Œä»¥å®ç°å¯¹è§†é¢‘ä¸­äººè„¸çš„å®æ—¶å’Œå‡†ç¡®è·Ÿè¸ªã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®çš„æŠ€æœ¯å’Œæ–¹æ³•ï¼Œå®ƒä»¬è¢«å¹¿æ³›åº”ç”¨äºç°ä»£é¢éƒ¨è·Ÿè¸ªç³»ç»Ÿä¸­ï¼š</p><ol><li><p><strong>åˆæˆåˆ†ææ³•ï¼ˆAnalysis-by-Synthesisï¼‰</strong>ï¼š<br>è¿™ç§æ–¹æ³•é€šè¿‡åˆ›å»ºä¸€ä¸ªäººè„¸æ¨¡å‹ï¼Œå¹¶å°†å…¶æ‹Ÿåˆåˆ°è§†é¢‘ä¸­çš„æ¯ä¸€å¸§ï¼Œä»¥å®ç°è·Ÿè¸ªã€‚åˆå§‹åŒ–é˜¶æ®µé€šå¸¸é€šè¿‡æœ€å°åŒ–äººè„¸å…³é”®ç‚¹çš„é‡æŠ•å½±è¯¯å·®æ¥è·å¾—åˆå§‹äººè„¸å‚æ•°ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¤„ç†å…‰ç…§å˜åŒ–å’Œé®æŒ¡é—®é¢˜ï¼Œæé«˜è·Ÿè¸ªçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</p></li><li><p><strong>åŸºäºæ¨¡å‹è·Ÿè¸ª</strong>ï¼š<br>è¿™ç§æ–¹æ³•ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„äººè„¸æ¨¡å‹ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°æ¥é€‚åº”è§†é¢‘ä¸­çš„äººè„¸ã€‚è¿™åŒ…æ‹¬ä½¿ç”¨å½¢çŠ¶æ¨¡å‹ï¼ˆå¦‚Active Shape Modelsï¼‰å’Œå¤–è§‚æ¨¡å‹æ¥æ•æ‰äººè„¸çš„å‡ ä½•å’Œå¤–è§‚å˜åŒ–ã€‚</p></li><li><p><strong>åŸºäºè¿åŠ¨ä¿¡æ¯è·Ÿè¸ª</strong>ï¼š<br>åˆ©ç”¨è§†é¢‘ä¸­çš„è¿åŠ¨ä¿¡æ¯æ¥é¢„æµ‹å’Œè·Ÿè¸ªäººè„¸çš„ç§»åŠ¨ã€‚è¿™ç§æ–¹æ³•é€šå¸¸ç»“åˆäº†å…‰æµç®—æ³•æˆ–å…¶ä»–è¿åŠ¨ä¼°è®¡æŠ€æœ¯ã€‚</p></li><li><p><strong>åŸºäºäººè„¸å±€éƒ¨ç‰¹å¾è·Ÿè¸ª</strong>ï¼š<br>é€šè¿‡æ£€æµ‹å’Œè·Ÿè¸ªäººè„¸çš„å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚çœ¼ç›ã€é¼»å­ã€å˜´å·´ç­‰ï¼‰æ¥å®ç°è·Ÿè¸ªã€‚è¿™äº›ç‰¹å¾ç‚¹å¯ä»¥æä¾›å…³äºäººè„¸å§¿æ€å’Œè¡¨æƒ…å˜åŒ–çš„è¯¦ç»†ä¿¡æ¯ã€‚</p></li><li><p><strong>åŸºäºç¥ç»ç½‘ç»œè·Ÿè¸ª</strong>ï¼š<br>åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæ¥è¯†åˆ«å’Œè·Ÿè¸ªäººè„¸ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å­¦ä¹ ä»å¤§é‡æ•°æ®ä¸­æå–å¤æ‚çš„é¢éƒ¨ç‰¹å¾ï¼Œå¹¶åœ¨å„ç§æ¡ä»¶ä¸‹ä¿æŒé«˜å‡†ç¡®åº¦ã€‚</p></li><li><p><strong>å®æ—¶äººè„¸è·Ÿè¸ªç®—æ³•</strong>ï¼š<br>ä¸ºäº†åœ¨å®æ—¶è§†é¢‘æµä¸­å®ç°äººè„¸è·Ÿè¸ªï¼Œç®—æ³•éœ€è¦é«˜æ•ˆä¸”èƒ½å¤Ÿå¿«é€Ÿå¤„ç†è¿ç»­å¸§ã€‚ä¸€äº›æˆç†Ÿçš„SDKï¼Œå¦‚OpenCVï¼Œæä¾›äº†å®æ—¶äººè„¸æ£€æµ‹å’Œè·Ÿè¸ªçš„åŠŸèƒ½ã€‚</p></li><li><p><strong>å¤šäººè„¸è·Ÿè¸ªï¼ˆMulti-face trackingï¼‰</strong>ï¼š<br>åœ¨å¤šäººåœºæ™¯ä¸­ï¼Œè·Ÿè¸ªæŠ€æœ¯éœ€è¦èƒ½å¤ŸåŒæ—¶è¯†åˆ«å’Œè·Ÿè¸ªå¤šä¸ªé¢éƒ¨ã€‚è¿™é€šå¸¸æ¶‰åŠåˆ°æ›´å¤æ‚çš„ç®—æ³•ï¼Œå¦‚FairMOTï¼Œå®ƒæ˜¯ä¸€ç§å•ç±»å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚ä¿®æ”¹ä¸ºå¤šç±»å¤šç›®æ ‡è·Ÿè¸ªã€‚</p></li><li><p><strong>éåˆšæ€§äººè„¸è·Ÿè¸ª</strong>ï¼š<br>è€ƒè™‘åˆ°äººè„¸çš„éåˆšæ€§ç‰¹æ€§ï¼Œä¸€äº›è·Ÿè¸ªç®—æ³•ä¼šä½¿ç”¨éåˆšæ€§æ¨¡å‹æ¥æ›´å¥½åœ°é€‚åº”é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œçš„å˜åŒ–ã€‚</p></li></ol><p>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé¢éƒ¨è·Ÿè¸ªç³»ç»Ÿå¯èƒ½ä¼šç»“åˆä»¥ä¸Šå¤šç§æ–¹æ³•ï¼Œä»¥æé«˜åœ¨ä¸åŒç¯å¢ƒå’Œæ¡ä»¶ä¸‹çš„è·Ÿè¸ªæ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç³»ç»Ÿå¯èƒ½ä¼šé¦–å…ˆä½¿ç”¨åŸºäºæ¨¡å‹çš„æ–¹æ³•æ¥åˆå§‹åŒ–è·Ÿè¸ªï¼Œç„¶ååˆ‡æ¢åˆ°åŸºäºç‰¹å¾çš„æ–¹æ³•æ¥å¤„ç†é¢éƒ¨è¡¨æƒ…å˜åŒ–ï¼ŒåŒæ—¶åˆ©ç”¨ç¥ç»ç½‘ç»œæ¥æé«˜åœ¨å¤æ‚èƒŒæ™¯ä¸‹çš„è·Ÿè¸ªå‡†ç¡®æ€§ã€‚</p><p>AD-NeRF</p><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320011902893.png" alt="image-20240320011902893"></p><p>ï¼ˆ2ï¼‰æˆ‘ä»¬åº”ç”¨å¤šå¸§å…‰æµä¼°è®¡æ–¹æ³•[18]æ¥è·å¾—å‰é¢ã€è€³æœµå’Œå¤´å‘ç­‰è¿‘åˆšæ€§åŒºåŸŸä¸­è§†é¢‘å¸§çš„å¯†é›†å¯¹åº”å…³ç³»ï¼Œç„¶åä½¿ç”¨æŸè°ƒæ•´æ¥ä¼°è®¡å§¿åŠ¿å‚æ•°[2]ã€‚ å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¼°è®¡çš„å§¿åŠ¿ä»…å¯¹é¢éƒ¨éƒ¨åˆ†æœ‰æ•ˆï¼Œè€Œå¯¹é¢ˆéƒ¨å’Œè‚©éƒ¨ç­‰èº«ä½“å…¶ä»–åŒºåŸŸæ— æ•ˆï¼Œå³é¢éƒ¨å§¿åŠ¿ä¸èƒ½ä»£è¡¨ä¸ŠåŠèº«çš„å…¨éƒ¨è¿åŠ¨ï¼›</p></blockquote><h2 id="Portrait-Sync-Generator"><a href="#Portrait-Sync-Generator" class="headerlink" title="Portrait-Sync Generator"></a>Portrait-Sync Generator</h2><p>ä»£ç æ”¹è¿›ä¸€å…±åªæœ‰å‡ éƒ¨åˆ†</p><p><img src="https://pic1.zhimg.com/v2-8241e1d748ca0b674e3913714b0e0386.png" alt=""></p><p>åœ¨æ•°æ®è¯»å–çš„æ—¶å€™ï¼ŒåŠ äº†face_maskçš„è¯»å–ï¼Œä»¥åŠbg_imageçš„è¯»å–ï¼Œä¹Ÿå°±æ˜¯GT Imageçš„è¯»å–ï¼Œå¯¹äºGT Imageæ¥è¯´ï¼Œæ˜¯é€šè¿‡parsingå»å‡ºå¯¹åº”éƒ¨åˆ†æ¥è¿›è¡Œæ“ä½œçš„ï¼Œä»ä¸‹å›¾ä¹Ÿå¯ä»¥çœ‹å‡ºåŒºåˆ«ï¼Œä¹Ÿå°±æ˜¯æœ‰æ— å¤´å‘ä¸çš„ç»†èŠ‚éƒ¨åˆ†</p><p><img src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt=""></p><p>æŒ‡æ ‡å¯èƒ½æœ‰ä¸¤ä¸ªGTï¼Œå› ä¸ºä¸¤ç§æ¨¡å¼ä¸‹ï¼Œå¯¹åº”çš„è®¡ç®—æŒ‡æ ‡æ˜¯ä¸åŒçš„</p><p><img src="https://picx.zhimg.com/v2-e5cec8d19e131745028e5a3fe71c3684.png" alt=""></p><p>é—®äº†ä¸€ä¸‹ä½œè€…ï¼Œå¤§æ¦‚æ›´æ˜ç™½äº†è¿™ä¸ªçš„æ„æ€ï¼Œå…¶å®æœ¬è´¨ä¸Šæ˜¯ä½¿ç”¨äº†åŸå›¾çš„å¤´å‘ä¸çš„ç»†èŠ‚åŠ å…¥åˆ°å›¾åƒä¸­ï¼Œä½¿å¾—å›¾åƒèƒ½å¤Ÿå¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œç„¶åå†è¿›è¡Œç»“åˆå¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚</p><p><img src="https://picx.zhimg.com/v2-e59f49fdcbc728e0222376e2a987d73b.png" alt=""></p><h2 id="å®éªŒç»“æœ"><a href="#å®éªŒç»“æœ" class="headerlink" title="å®éªŒç»“æœ"></a>å®éªŒç»“æœ</h2><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320124820368.png" alt="image-20240320124820368"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png&quot; alt=&quot;synctalk&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Face-Sync-Controller&quot;&gt;&lt;a href=&quot;#Fac</summary>
      
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/</id>
    <published>2024-03-18T11:55:20.000Z</published>
    <updated>2024-03-18T11:55:20.754Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-18-æ›´æ–°"><a href="#2024-03-18-æ›´æ–°" class="headerlink" title="2024-03-18 æ›´æ–°"></a>2024-03-18 æ›´æ–°</h1><h2 id="SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians"><a href="#SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians" class="headerlink" title="SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians"></a>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</h2><p><strong>Authors:Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</strong></p><p>Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency. </p><p><a href="http://arxiv.org/abs/2403.10427v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>æ‰©å±•3Dé«˜æ–¯æ”¾å°„æŠ€æœ¯ä»¥å¤„ç†æ— ç»“æ„å›¾åƒé›†ï¼Œé€šè¿‡å»ºæ¨¡å¤–è§‚å’Œè®­ç»ƒç¬æ€é«˜æ–¯å‡½æ•°ï¼Œæé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>3Dé«˜æ–¯æ”¾å°„æŠ€æœ¯æ˜¯ä¸€ç§å¿«é€Ÿçš„3Dåœºæ™¯æ¸²æŸ“æ–¹æ³•ã€‚</li><li>æ‰©å±•3Dé«˜æ–¯æ”¾å°„æŠ€æœ¯ä»¥å¤„ç†æ— ç»“æ„å›¾åƒé›†ã€‚</li><li>å»ºæ¨¡å¤–è§‚ä»¥æ•æ‰æ¸²æŸ“å›¾åƒä¸­çš„å…‰åº¦å˜åŒ–ã€‚</li><li>å¼•å…¥æ–°æœºåˆ¶æ¥è®­ç»ƒç¬æ€é«˜æ–¯å‡½æ•°ï¼Œä»¥æ— ç›‘ç£æ–¹å¼å¤„ç†åœºæ™¯é®æŒ¡ã€‚</li><li>åœ¨å„ç§ç…§ç‰‡é›†åœºæ™¯å’Œæˆ·å¤–åœ°æ ‡çš„å¤šé€šé“é‡‡é›†ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœå’Œæ›´é«˜çš„æ•ˆç‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSWAGï¼šåœ¨é‡å›¾åƒä¸­åˆ©ç”¨å¤–è§‚æ¡ä»¶é«˜æ–¯è¿›è¡Œæ³¼æº…</li><li>ä½œè€…ï¼šHiba Dahmaniã€Moussab Benneharã€Nathan Piascoã€Luis RoldËœaoã€Dzmitry Tsishkou</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè¯ºäºšæ–¹èˆŸï¼Œåä¸ºå·´é»ç ”ç©¶ä¸­å¿ƒï¼Œæ³•å›½</li><li>å…³é”®è¯ï¼š3D é«˜æ–¯æ³¼æº…Â·æ— çº¦æŸç…§ç‰‡é›†Â·æ–°è§†è§’åˆæˆÂ·å¤–è§‚å»ºæ¨¡Â·å®æ—¶æ¸²æŸ“Â·ç¬æ€å¯¹è±¡ç§»é™¤</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.10427v1[cs.CV]</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšå¼ç¥ç»è¡¨ç¤ºæ–¹æ³•åœ¨ä»æ— çº¦æŸçš„é‡å¤–ç…§ç‰‡é›†ä¸­å­¦ä¹  3D åœºæ™¯æ–¹é¢å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ï¼Œä½†ä»ç„¶å—åˆ°ä½“ç§¯æ¸²æŸ“çš„é«˜è®¡ç®—æˆæœ¬çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œ3D é«˜æ–¯æ³¼æº…ä½œä¸ºä¸€ç§æ›´å¿«çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œå…·æœ‰å“è¶Šçš„æ¸²æŸ“è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ï¼Œå°¤å…¶é€‚ç”¨äºå°è§„æ¨¡å’Œä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åœºæ™¯ã€‚ç„¶è€Œï¼Œè¯¥æŠ€æœ¯åœ¨æ— çº¦æŸçš„é‡å¤–æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å°† 3D é«˜æ–¯æ³¼æº…æ‰©å±•åˆ°å¤„ç†æ— ç»“æ„å›¾åƒé›†ã€‚é€šè¿‡å¯¹å¤–è§‚è¿›è¡Œå»ºæ¨¡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»¥æ•æ‰æ¸²æŸ“å›¾åƒä¸­çš„å…‰åº¦å˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„æœºåˆ¶æ¥è®­ç»ƒç¬æ€é«˜æ–¯ï¼Œä»¥ä¾¿ä»¥æ— ç›‘ç£çš„æ–¹å¼å¤„ç†åœºæ™¯é®æŒ¡ç‰©çš„å­˜åœ¨ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šåœ¨ä¸åŒçš„ç…§ç‰‡é›†åœºæ™¯å’Œæˆ·å¤–åœ°æ ‡çš„å¤šéé‡‡é›†ä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•æ¯”ä»¥å‰çš„å·¥ä½œæ›´æœ‰æ•ˆï¼Œåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼Ÿæ€§èƒ½æ˜¯å¦èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Ÿæœ¬æ–‡çš„æ–¹æ³•åœ¨æ— çº¦æŸç…§ç‰‡é›†ä¸Šçš„æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†æ•ˆç‡ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æ‰©å±• 3D é«˜æ–¯æ³¼æº…ä»¥å¤„ç†æ— çº¦æŸçš„å›¾åƒé›†ï¼Œå¹¶æé«˜å…¶åœ¨é‡å¤–åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)å¤–è§‚å»ºæ¨¡ï¼šä¸ºé€‚åº”å…‰åº¦å˜åŒ–ï¼Œä¸ºæ¯å¼ å›¾åƒå…³è”ä¸€ä¸ªå¯è®­ç»ƒåµŒå…¥å‘é‡lIï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªMLPæ¥æ³¨å…¥å¤–è§‚ä¿¡æ¯ï¼Œè¯¥MLPä»¥å›¾åƒåµŒå…¥å’Œé«˜æ–¯ä¸­å¿ƒçš„ä½ç½®ç¼–ç ä¸ºè¾“å…¥ï¼Œè¾“å‡ºå›¾åƒç›¸å…³çš„é¢œè‰²cIå’Œå›¾åƒç›¸å…³çš„é€æ˜åº¦å˜åŒ–å‚æ•°âˆ†Î±Iï¼›(2)ç¬æ€é«˜æ–¯å»ºæ¨¡ï¼šä¸ºè§£å†³ç¬æ€é®æŒ¡ç‰©é—®é¢˜ï¼Œå¼•å…¥å¯å­¦ä¹ çš„å›¾åƒç›¸å…³é«˜æ–¯é€æ˜åº¦å˜åŒ–é¡¹âˆ†ËœÎ±Iï¼Œè¯¥å‚æ•°å…è®¸é«˜æ–¯é‡å»ºæŸäº›å›¾åƒä¸­å­˜åœ¨çš„é®æŒ¡ç‰©ï¼ŒåŒæ—¶å…è®¸è¿™äº›ç›¸åŒçš„é«˜æ–¯åœ¨æ²¡æœ‰é®æŒ¡ç‰©çš„å…¶ä»–å›¾åƒä¸­ä¿æŒé€æ˜ï¼›(3)è®­ç»ƒè¿‡ç¨‹ï¼šä½¿ç”¨Binary Concreteéšæœºå˜é‡å¯¹âˆ†ËœÎ±Iè¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä½¿ç”¨MLPçš„é™„åŠ è¾“å‡ºâˆ†Î±Iä½œä¸ºConcreteå‡½æ•°çš„ä½ç½®å‚æ•°ã€‚</p><ol><li>ç»“è®ºï¼š(1) æœ¬æ–‡æå‡ºäº† SWAGï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¸ºé‡å¤–åœºæ™¯å®šåˆ¶ 3D é«˜æ–¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚SWAG åœ¨é«˜æ–¯çš„é¢œè‰²ä¸­èå…¥äº†å¤–è§‚å»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨äº†è‡ªé€‚åº”ä¸é€æ˜åº¦è°ƒåˆ¶æ¥å¤„ç†ç¬æ€å¯¹è±¡çš„å­˜åœ¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSWAG åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´æ¯”é‡å¤– NVS åŸºçº¿å¿«å‡ ä¸ªæ•°é‡çº§ï¼ŒåŒæ—¶æ”¯æŒå®æ—¶æ¸²æŸ“ã€‚ä½œä¸ºå°† 3D é«˜æ–¯åº”ç”¨äºé‡å¤–åœºæ™¯è¡¨ç¤ºçš„ç¬¬ä¸€æ­¥ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†æ½œåœ¨çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä¾‹å¦‚å°† SWAG æ‰©å±•åˆ°åŠ¨æ€åœºæ™¯ä¸­ã€‚(2) åˆ›æ–°ç‚¹ï¼šSWAG åˆ›æ–°æ€§åœ°å°†å¤–è§‚å»ºæ¨¡èå…¥ 3D é«˜æ–¯è¡¨ç¤ºä¸­ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ä¸é€æ˜åº¦è°ƒåˆ¶æœºåˆ¶æ¥å¤„ç†ç¬æ€é®æŒ¡ç‰©ï¼Œæ˜¾è‘—æé«˜äº†é‡å¤–åœºæ™¯çš„è¡¨ç¤ºèƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨æ— çº¦æŸç…§ç‰‡é›†ä¸Šçš„æ–°è§†è§’åˆæˆä»»åŠ¡ä¸­ï¼ŒSWAG å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å°†æ¸²æŸ“æ•ˆç‡æé«˜äº†å‡ ä¸ªæ•°é‡çº§ï¼›å·¥ä½œé‡ï¼šSWAG çš„è®­ç»ƒæ—¶é—´æ¯”é‡å¤– NVS åŸºçº¿å¿«å‡ ä¸ªæ•°é‡çº§ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-42d3d97d6fe30ac46eae820ba89402c1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6fd5515fed7d2c4f632ef0b06ec7a029.jpg" align="middle"></details><h2 id="FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model"><a href="#FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model" class="headerlink" title="FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model"></a>FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model</h2><p><strong>Authors:Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</strong></p><p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2403.10242v1">PDF</a> </p><p><strong>Summary</strong><br>FDGaussian æ˜¯ä¸€ç§æ–°é¢–çš„å•å›¾åƒ 3D é‡å»ºæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ä» 2D è¾“å…¥ä¸­æå– 3D å‡ ä½•ç‰¹å¾ä»¥ç”Ÿæˆä¸€è‡´çš„å¤šè§†å›¾å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>FDGaussian æ¡†æ¶ç”¨äºä»å•è§†å›¾å›¾åƒé‡å»ºè¯¦ç»†çš„ 3D å¯¹è±¡ã€‚</li><li>è¯¥æ¡†æ¶ä½¿ç”¨æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ä» 2D è¾“å…¥ä¸­æå– 3D å‡ ä½•ç‰¹å¾ã€‚</li><li>è¯¥æ–¹æ³•åˆ©ç”¨æœ€æ–°çš„é«˜æ–¯å–·ç»˜æŠ€æœ¯å¹¶ç»“åˆæçº¿æ³¨æ„åŠ›æ¥èåˆæ¥è‡ªä¸åŒè§†ç‚¹çš„å›¾åƒã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒFDGaussian ç”Ÿæˆçš„å›¾åƒå…·æœ‰æ›´é«˜çš„è·¨è§†å›¾ä¸€è‡´æ€§ã€‚</li><li>è¯¥æ–¹æ³•é‡å»ºçš„é«˜è´¨é‡ 3D å¯¹è±¡åœ¨è´¨é‡å’Œæ•°é‡ä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li><li>æœ‰å…³æ›´å¤šç¤ºä¾‹ï¼Œè¯·è®¿é—®é¡¹ç›®ç½‘ç«™ <a href="https://qjfeng.net/FDGaussian/ã€‚">https://qjfeng.net/FDGaussian/ã€‚</a></li><li>FDGaussian æ¡†æ¶æé«˜äº†å•å›¾åƒ 3D é‡å»ºçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šFDGaussianï¼šé€šè¿‡å•å¼ å›¾åƒè¿›è¡Œå¿«é€Ÿé«˜æ–¯æ¸²æŸ“</li><li>ä½œè€…ï¼šå†¯å¯å†›1ï¼Œé‚¢æŒ¯1ï¼Œå´ç¥–å®£1ï¼Œå§œå®‡åˆš1</li><li>éš¶å±å•ä½ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼š3Dé‡å»º Â· é«˜æ–¯æ¸²æŸ“ Â· æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.10242</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å•è§†å›¾å›¾åƒé‡å»ºè¯¦ç»†çš„ 3D ç‰©ä½“ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¯ç”¨çš„ä¿¡æ¯æœ‰é™ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šæœ€è¿‘çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ä»è¾“å…¥å›¾åƒç”Ÿæˆåˆç†çš„ novel viewï¼Œä½†å®ƒä»¬é‡åˆ°å¤šè§†å›¾ä¸ä¸€è‡´æˆ–ç¼ºä¹å‡ ä½•ä¿çœŸåº¦çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ï¼Œä» 2D è¾“å…¥ä¸­æå– 3D å‡ ä½•ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆä¸€è‡´çš„å¤šè§†å›¾å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæçº¿æ³¨æ„åŠ›è¿›ä¸€æ­¥åŠ é€Ÿäº†æœ€å…ˆè¿›çš„é«˜æ–¯æ¸²æŸ“ï¼Œä»¥èåˆæ¥è‡ªä¸åŒè§†ç‚¹çš„å›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬è¯æ˜äº† FDGaussian ç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒè§†å›¾ä¹‹é—´å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶é‡å»ºäº†é«˜è´¨é‡çš„ 3D ç‰©ä½“ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§ä¸Šè¿˜æ˜¯å®šé‡ä¸Šã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åŸºäºå‡ ä½•ç‰¹å¾çš„å¤šè§†å›¾å›¾åƒç”Ÿæˆï¼šå¾®è°ƒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç»™å®šçš„ç›¸æœºå˜æ¢åˆæˆæ–°é¢–å›¾åƒï¼Œå·²å–å¾—æœ‰å¸Œæœ›çš„ç»“æœã€‚ä¸€éƒ¨åˆ†æ–¹æ³•é€šè¿‡è°ƒèŠ‚å…ˆå‰ç”Ÿæˆçš„å›¾åƒæ¥è§£å†³å¤šè§†å›¾ä¸ä¸€è‡´é—®é¢˜ï¼Œä½†å®¹æ˜“å‡ºç°ç´¯ç§¯è¯¯å·®å’Œé™ä½å¤„ç†é€Ÿåº¦çš„é—®é¢˜ã€‚å¦ä¸€éƒ¨åˆ†æ–¹æ³•ä»…ä½¿ç”¨å‚è€ƒå›¾åƒå’Œè¯­ä¹‰æŒ‡å¯¼ç”Ÿæˆæ–°é¢–è§†å›¾ï¼Œä½†å­˜åœ¨å‡ ä½•åç¼©å’Œä¿çœŸåº¦æœ‰é™çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºå…³é”®åœ¨äºå……åˆ†åˆ©ç”¨å‚è€ƒå›¾åƒæä¾›çš„å‡ ä½•ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç›´æ¥ä»å•ä¸ª 2D å›¾åƒä¸­æå– 3D ä¿¡æ¯æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œå¿…é¡»é€šè¿‡è§£è€¦æ­£äº¤å¹³é¢æ¥æœ‰æ•ˆåœ°ä»å›¾åƒå¹³é¢ï¼ˆå³ xy å¹³é¢ï¼‰ä¸­åˆ†ç¦» 3D ç‰¹å¾ã€‚æˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è§†è§‰ Transformer å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼–ç å¹¶æ•è·å›¾åƒä¸­çš„æ•´ä½“ç›¸å…³æ€§ï¼Œç”Ÿæˆé«˜ç»´æ½œåœ¨è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªè§£ç å™¨ï¼ˆå›¾åƒå¹³é¢è§£ç å™¨å’Œæ­£äº¤å¹³é¢è§£ç å™¨ï¼‰ä»æ½œåœ¨è¡¨ç¤ºä¸­ç”Ÿæˆå…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„ç‰¹å¾ã€‚å›¾åƒå¹³é¢è§£ç å™¨é€†è½¬ç¼–ç æ“ä½œï¼Œå¯¹ç¼–ç å™¨è¾“å‡ºä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å¹¶å°†å…¶è½¬æ¢ä¸º Fxyã€‚ä¸ºäº†ç”Ÿæˆæ­£äº¤å¹³é¢ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒä¸å›¾åƒå¹³é¢çš„ç»“æ„å¯¹é½ï¼Œé‡‡ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¯¹ yz å¹³é¢ç‰¹å¾ Fyz å’Œ xz å¹³é¢ç‰¹å¾ Fxz è¿›è¡Œè§£ç ã€‚ä¸ºäº†ä¿ƒè¿›ä¸åŒå¹³é¢ä¹‹é—´çš„è§£ç è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„åµŒå…¥ uï¼Œå®ƒä¸ºè§£è€¦æ–°å¹³é¢æä¾›äº†é™„åŠ ä¿¡æ¯ã€‚å¯å­¦ä¹ çš„åµŒå…¥ u é¦–å…ˆé€šè¿‡è‡ªæ³¨æ„åŠ›ç¼–ç å¤„ç†ï¼Œç„¶ååœ¨å…·æœ‰ç¼–ç å›¾åƒæ½œåœ¨è¡¨ç¤ºçš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸­ç”¨ä½œæŸ¥è¯¢ã€‚å›¾åƒç‰¹å¾è¢«è½¬æ¢ä¸ºäº¤å‰æ³¨æ„åŠ›æœºåˆ¶çš„é”®å’Œå€¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šCrossAttn(u, h) = SoftMax(WQSelfAttn(u))(WKh)Tâˆšd(WVh),å…¶ä¸­ WQã€WK å’Œ WV æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œd æ˜¯ç¼©æ”¾ç³»æ•°ã€‚æœ€åï¼Œç‰¹å¾ç»„åˆä¸ºå‡ ä½•æ¡ä»¶ï¼šF = Fxy âŠ• (Fyz + Fxz),å…¶ä¸­ âŠ• å’Œ + åˆ†åˆ«è¡¨ç¤ºè¿æ¥å’Œæ±‚å’Œæ“ä½œã€‚ï¼ˆ2ï¼‰é«˜æ–¯æ¸²æŸ“é¢„å¤‡çŸ¥è¯†ï¼š3D é«˜æ–¯æ¸²æŸ“æ˜¯ä¸€ç§åŸºäºå­¦ä¹ çš„å…‰æ …åŒ–æŠ€æœ¯ï¼Œç”¨äº 3D åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚æ¯ä¸ªé«˜æ–¯å…ƒç´ è¢«å®šä¹‰ä¸ºä¸€ä¸ªä½ç½®ï¼ˆå‡å€¼ï¼‰Âµã€ä¸€ä¸ªå®Œæ•´çš„ 3D åæ–¹å·®çŸ©é˜µ Î£ã€é¢œè‰² c å’Œä¸é€æ˜åº¦ Ïƒã€‚é«˜æ–¯å‡½æ•° G(x) å¯ä»¥è¡¨ç¤ºä¸ºï¼šG(x) = exp(-1/2(x - Âµ)TÎ£-1(x - Âµ)).ä¸ºäº†ç¡®ä¿ Î£ çš„æ­£åŠå®šæ€§ï¼Œåæ–¹å·®çŸ©é˜µ Î£ å¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ªç”± 3D å‘é‡ s âˆˆ R3 è¡¨ç¤ºçš„ç¼©æ”¾çŸ©é˜µ S å’Œä¸€ä¸ªè¡¨ç¤ºå·®å¼‚åŒ–ä¼˜åŒ–çš„å››å…ƒæ•° q âˆˆ R4 çš„æ—‹è½¬çŸ©é˜µ Rï¼šÎ£ = RSSTRTã€‚æ¸²æŸ“æŠ€æœ¯ï¼Œå¦‚æœ€åˆåœ¨ [21] ä¸­ä»‹ç»çš„ï¼Œæ˜¯å°†é«˜æ–¯æŠ•å½±åˆ°ç›¸æœºå›¾åƒå¹³é¢ï¼Œè¿™äº›å›¾åƒå¹³é¢è¢«ç”¨æ¥ç”Ÿæˆæ–°é¢–çš„è§†å›¾å›¾åƒã€‚ç»™å®šä¸€ä¸ªè§‚å¯Ÿå˜æ¢ Wï¼Œç›¸æœºåæ ‡ä¸­çš„åæ–¹å·®çŸ©é˜µ Î£' ç»™å‡ºä¸ºï¼šÎ£' = JWÎ£WTJTï¼Œå…¶ä¸­ J æ˜¯æŠ•å½±å˜æ¢çš„ä»¿å°„é€¼è¿‘çš„é›…å¯æ¯”çŸ©é˜µã€‚å°† 3D é«˜æ–¯æ˜ å°„åˆ° 2D å›¾åƒç©ºé—´åï¼Œæˆ‘ä»¬è®¡ç®—ä¸æ¯ä¸ªåƒç´ é‡å çš„ 2D é«˜æ–¯å¹¶è®¡ç®—å®ƒä»¬çš„é¢œè‰² ci å’Œä¸é€æ˜åº¦ Ïƒi è´¡çŒ®ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªé«˜æ–¯çš„é¢œè‰²æ ¹æ®ç­‰å¼ (4) ä¸­æè¿°çš„é«˜æ–¯è¡¨ç¤ºåˆ†é…ç»™æ¯ä¸ªåƒç´ ã€‚ä¸é€æ˜åº¦æ§åˆ¶æ¯ä¸ªé«˜æ–¯çš„å½±å“ã€‚æ¯ä¸ªåƒç´ çš„é¢œè‰² Ë†C å¯ä»¥é€šè¿‡æ··åˆ N ä¸ªæœ‰åºé«˜æ–¯è·å¾—ï¼šË†C = (âˆ‘iâˆˆN ciÏƒi) / (âˆ‘i-1j=1(1 - Ïƒi))ã€‚ï¼ˆ3ï¼‰åŠ é€Ÿä¼˜åŒ–ï¼šé«˜æ–¯æ¸²æŸ“çš„ä¼˜åŒ–åŸºäºæ¸²æŸ“å’Œå°†ç»“æœå›¾åƒä¸è®­ç»ƒè§†å›¾è¿›è¡Œæ¯”è¾ƒçš„è¿ç»­è¿­ä»£ã€‚3D é«˜æ–¯æœ€åˆæ˜¯ä»ç»“æ„è¿åŠ¨ (SfM) æˆ–éšæœºé‡‡æ ·ä¸­åˆå§‹åŒ–çš„ã€‚ä¸å¯é¿å…åœ°ï¼Œç”±äº 3D åˆ° 2D æŠ•å½±çš„æ¨¡ç³Šæ€§ï¼Œå‡ ä½•å¯èƒ½è¢«é”™è¯¯æ”¾ç½®ã€‚å› æ­¤ï¼Œä¼˜åŒ–è¿‡ç¨‹éœ€è¦èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ›å»ºå‡ ä½•ï¼Œå¹¶ä¸”å¦‚æœå‡ ä½•æ”¾ç½®ä¸æ­£ç¡®ï¼ˆç§°ä¸ºåˆ†è£‚å’Œå…‹éš†ï¼‰ï¼Œè¿˜éœ€è¦åˆ é™¤å‡ ä½•ã€‚ç„¶è€Œï¼ŒåŸå§‹å·¥ä½œ [21] æå‡ºçš„åˆ†è£‚å’Œå…‹éš†æ“ä½œå¿½ç•¥äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­ 3D é«˜æ–¯ä¹‹é—´çš„è·ç¦»ï¼Œè¿™å¤§å¤§é™ä½äº†è¿‡ç¨‹é€Ÿåº¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¦‚æœä¸¤ä¸ªé«˜æ–¯å½¼æ­¤é è¿‘ï¼Œå³ä½¿ä½ç½®æ¢¯åº¦å¤§äºé˜ˆå€¼ï¼Œä¹Ÿä¸åº”å°†å®ƒä»¬åˆ†è£‚æˆ–å…‹éš†ï¼Œå› ä¸ºè¿™äº›é«˜æ–¯æ­£åœ¨æ›´æ–°å®ƒä»¬çš„ä½ç½®ã€‚æ ¹æ®ç»éªŒï¼Œåˆ†è£‚æˆ–å…‹éš†è¿™äº›é«˜æ–¯å¯¹æ¸²æŸ“è´¨é‡çš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå› ä¸ºå®ƒä»¬å½¼æ­¤å¤ªè¿‘ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬æå‡ºé«˜æ–¯å‘æ•£æ˜¾è‘—æ€§ (GDS) ä½œä¸º 3D é«˜æ–¯è·ç¦»çš„åº¦é‡ï¼Œä»¥é¿å…ä¸å¿…è¦çš„åˆ†å‰²æˆ–å…‹éš†ï¼šÎ¥GDS(G(x1), G(x2)) = âˆ¥Âµ1 - Âµ2âˆ¥2 + tr(Î£1 + Î£2 - 2(Î£-11Î£2Î£-11)1/2),å…¶ä¸­ Âµ1ã€Î£1ã€Âµ2ã€Î£2 æ˜¯ä¸¤ä¸ª 3D é«˜æ–¯ G(x1) å’Œ G(x2) çš„ä½ç½®å’Œåæ–¹å·®çŸ©é˜µã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬åªå¯¹ä½ç½®æ¢¯åº¦å¤§ä¸” GDS çš„ 3D é«˜æ–¯æ‰§è¡Œåˆ†å‰²å’Œå…‹éš†æ“ä½œã€‚ä¸ºäº†é¿å…ä¸ºæ¯å¯¹ 3D é«˜æ–¯è®¡ç®— GDS çš„è€—æ—¶è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§ç­–ç•¥ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ª 3D é«˜æ–¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ k-æœ€è¿‘é‚» (k-NN) ç®—æ³•æ‰¾åˆ°å…¶æœ€æ¥è¿‘çš„ 3D é«˜æ–¯ï¼Œå¹¶è®¡ç®—å®ƒä»¬æ¯å¯¹çš„ GDSã€‚å› æ­¤ï¼Œæ—¶é—´å¤æ‚åº¦ä» O(N2) é™ä½åˆ° O(N)ã€‚æ­¤å¤–ï¼Œå¦‚ç¬¬ 3.2 èŠ‚æ‰€è¿°ï¼Œåæ–¹å·®çŸ©é˜µå¯ä»¥åˆ†è§£ä¸ºç¼©æ”¾çŸ©é˜µ S å’Œæ—‹è½¬çŸ©é˜µ Rï¼šÎ£ = RSSTRTã€‚æˆ‘ä»¬åˆ©ç”¨æ—‹è½¬å’Œç¼©æ”¾çŸ©é˜µçš„å¯¹è§’å’Œæ­£äº¤æ€§è´¨æ¥ç®€åŒ–ç­‰å¼ (5) çš„è®¡ç®—ã€‚æœ‰å…³ GDS çš„è¯¦ç»†ä¿¡æ¯å°†åœ¨è¡¥å……ææ–™ä¸­è®¨è®ºã€‚ï¼ˆ4ï¼‰å¤šè§†å›¾æ¸²æŸ“çš„æçº¿æ³¨æ„åŠ›ï¼šä»¥å‰çš„æ–¹æ³• [50, 70] é€šå¸¸ä½¿ç”¨å•ä¸ªè¾“å…¥å›¾åƒè¿›è¡Œç²—ç³™çš„é«˜æ–¯æ¸²æŸ“ï¼Œè¿™éœ€è¦åœ¨çœ‹ä¸è§çš„åŒºåŸŸè¿›ä¸€æ­¥ç»†åŒ–æˆ–é‡æ–°ç»˜åˆ¶ã€‚ç›´è§‚çš„æ€è·¯æ˜¯åˆ©ç”¨ç”Ÿæˆçš„ä¸€è‡´å¤šè§†å›¾å›¾åƒé‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ã€‚ç„¶è€Œï¼Œä»…ä¾é äº¤å‰æ³¨æ„åŠ›åœ¨å¤šä¸ªè§†ç‚¹çš„å›¾åƒä¹‹é—´è¿›è¡Œé€šä¿¡æ˜¯ä¸å¤Ÿçš„ã€‚å› æ­¤ï¼Œç»™å®šä¸€ç³»åˆ—ç”Ÿæˆçš„è§†å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†æçº¿æ³¨æ„åŠ›ä»¥å…è®¸ä¸åŒè§†å›¾çš„ç‰¹å¾ä¹‹é—´å…³è”ã€‚å¯¹äºç»™å®šä¸€ä¸ªè§†å›¾ä¸­çš„ç»™å®šç‰¹å¾ç‚¹ï¼Œæçº¿æ˜¯æ ¹æ®ä¸¤ä¸ªè§†å›¾ä¹‹é—´çš„å·²çŸ¥å‡ ä½•å…³ç³»ï¼Œåœ¨å¦ä¸€ä¸ªè§†å›¾ä¸­å¯¹åº”çš„ç‰¹å¾ç‚¹å¿…é¡»ä½äºè¯¥ç›´çº¿ä¸Šã€‚å®ƒä½œä¸ºä¸€ä¸ªçº¦æŸï¼Œå‡å°‘äº†åœ¨ä¸€ä¸ªè§†å›¾ä¸­å¯ä»¥å…³æ³¨å¦ä¸€ä¸ªè§†å›¾çš„æ½œåœ¨åƒç´ çš„æ•°é‡ã€‚æˆ‘ä»¬åœ¨å›¾ 4 ä¸­å±•ç¤ºäº†æçº¿å’Œæçº¿æ³¨æ„åŠ›çš„æ’å›¾ã€‚é€šè¿‡å¼ºåˆ¶æ‰§è¡Œæ­¤çº¦æŸï¼Œæˆ‘ä»¬å¯ä»¥é™åˆ¶ä¸åŒè§†å›¾ä¸­å¯¹åº”ç‰¹å¾çš„æœç´¢ç©ºé—´ï¼Œä»è€Œä½¿å…³è”è¿‡ç¨‹æ›´æœ‰æ•ˆå’Œå‡†ç¡®ã€‚è€ƒè™‘ä¸­é—´ UNet ç‰¹å¾ fsï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å®ƒåœ¨æ‰€æœ‰å…¶ä»–è§†å›¾ {ft}tÌ¸=s çš„ç‰¹å¾å›¾ä¸Šçš„å¯¹åº”æçº¿ {lt}tÌ¸=sï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¡¥å……ææ–™ï¼‰ã€‚fs ä¸Šçš„æ¯ä¸ªç‚¹ p åªèƒ½è®¿é—®åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­ä½äºç›¸æœºå…‰çº¿ï¼ˆåœ¨å…¶ä»–è§†å›¾ä¸­ï¼‰çš„æ‰€æœ‰ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼°è®¡ fs ä¸­æ‰€æœ‰ä½ç½®çš„æƒé‡å›¾ï¼Œå †å è¿™äº›å›¾ï¼Œå¹¶è·å¾—æçº¿æƒé‡çŸ©é˜µ Mstã€‚æœ€åï¼Œæçº¿æ³¨æ„åŠ›å±‚ Ë†fs çš„è¾“å‡ºå¯ä»¥è¡¨ç¤ºä¸ºï¼šË†fs = SoftMax(fsMTstâˆšd)Mst.é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºçš„æçº¿æ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›äº†å¤šä¸ªè§†å›¾ä¹‹é—´ç‰¹å¾çš„æœ‰æ•ˆå’Œå‡†ç¡®å…³è”ã€‚é€šè¿‡å°†æœç´¢ç©ºé—´çº¦æŸåˆ°æçº¿ä¸Šï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—æˆæœ¬å¹¶æ¶ˆé™¤äº†æ½œåœ¨çš„ä¼ªå½±ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šFDGaussian é€šè¿‡å•å¼ å›¾åƒè¿›è¡Œå¿«é€Ÿé«˜æ–¯æ¸²æŸ“ï¼Œè§£å†³äº†å¤šè§†å›¾ä¸ä¸€è‡´å’Œå‡ ä½•ä¿çœŸåº¦é—®é¢˜ï¼Œä¸ºä»å•è§†å›¾å›¾åƒé‡å»ºè¯¦ç»† 3D ç‰©ä½“æä¾›äº†æ–°çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šFDGaussian æå‡ºäº†ä¸€ç§æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ï¼Œä» 2D è¾“å…¥ä¸­æå– 3D å‡ ä½•ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆä¸€è‡´çš„å¤šè§†å›¾å›¾åƒã€‚FDGaussian é€šè¿‡ç»“åˆæçº¿æ³¨æ„åŠ›è¿›ä¸€æ­¥åŠ é€Ÿäº†æœ€å…ˆè¿›çš„é«˜æ–¯æ¸²æŸ“ï¼Œä»¥èåˆæ¥è‡ªä¸åŒè§†ç‚¹çš„å›¾åƒã€‚æ€§èƒ½ï¼šFDGaussian ç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒè§†å›¾ä¹‹é—´å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶é‡å»ºäº†é«˜è´¨é‡çš„ 3D ç‰©ä½“ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§ä¸Šè¿˜æ˜¯å®šé‡ä¸Šã€‚å·¥ä½œé‡ï¼šFDGaussian æ˜¯ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D é‡å»ºã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-a6bdbe8ba3c8512caff95a5d017fc426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7aaaf0d2053ad52ca4386c6c3da1a8b.jpg" align="middle"></details><h2 id="GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time"><a href="#GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time" class="headerlink" title="GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time"></a>GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time</h2><p><strong>Authors:Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</strong></p><p>This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at $\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences. </p><p><a href="http://arxiv.org/abs/2403.10147v1">PDF</a> </p><p><strong>Summary</strong><br>å›¾åƒè§‚å¯Ÿè”åˆå­¦ä¹ æ¡†æ¶ä¼°è®¡ç›¸å¯¹ä½å§¿ï¼ŒåŸºäºé«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»¥åŠåŠ¨æ€è°ƒæ•´çš„é«˜æ–¯ç¼“å­˜æ¨¡å—ï¼Œæ˜¾è‘—æå‡3DGSåœ¨å®é™…åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li><strong>æ— éœ€çœŸå®ç›¸æœºä½å§¿ï¼š</strong>è”åˆå­¦ä¹ æ¡†æ¶åˆ©ç”¨å›¾åƒè§‚å¯Ÿä¼°è®¡ç›¸å¯¹ä½å§¿ã€‚</li><li><strong>é«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ï¼š</strong>å»¶è¿Ÿåå‘ä¼ æ’­æœºåˆ¶å…‹æœäº†åˆ†è¾¨ç‡é™åˆ¶ã€‚</li><li><strong>åŠ¨æ€é«˜æ–¯ç¼“å­˜ï¼š</strong>æé«˜äº†é€Ÿåº¦å’Œæ•ˆç‡ã€‚</li><li><strong>æå¿«é€Ÿæ¨ç†ï¼š</strong>æ¨ç†é€Ÿåº¦è¾¾ 5 FPS ä»¥ä¸Šã€‚</li><li><strong>å®æ—¶æ¸²æŸ“ï¼š</strong>æ¸²æŸ“é€Ÿåº¦è¾¾ 100 FPS ä»¥ä¸Šã€‚</li><li><strong>è¶…è¶Šæ— ä½å§¿ NeRFï¼š</strong>åœ¨æ¨ç†é€Ÿåº¦å’Œæœ‰æ•ˆæ€§æ–¹é¢ä¼˜äºç°æœ‰ NeRF æ–¹æ³•ã€‚</li><li><strong>æ¥è¿‘æœ‰ä½å§¿ 3D-GSï¼š</strong>æ€§èƒ½æ¥è¿‘çœŸå®ä½å§¿ 3D-GS æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºå§¿åŠ¿æ— å…³çš„å¯æ³›åŒ– 3D é«˜æ–¯ä½“ç´ æ¸²æŸ“</li><li>ä½œè€…ï¼šH. Li, Y. Chen, H. Wang, Y. Liu, S. Liu, Y. Chen, Z. Li, W. Chen, X. Tong</li><li>æ‰€å±å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼šPose-FreeÂ·Generalizable 3D-GSÂ·Real-time Rendering</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.02826.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯åœ¨è™šæ‹Ÿç°å®ã€ç”µå½±åˆ¶ä½œã€æ²‰æµ¸å¼å¨±ä¹ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¸ºäº†å¢å¼ºè·¨æœªè§åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ€è¿‘çš„ç ”ç©¶æå‡ºäº†å¯æ³›åŒ– NeRF å’Œ 3D-GS ç­‰åˆ›æ–°æ–¹æ³•ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå°½ç®¡è¿™äº›æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€ä¼˜åŒ–çš„æƒ…å†µä¸‹é‡å»ºæ–°åœºæ™¯ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºæ¯ä¸ªå›¾åƒè§‚æµ‹çš„å®é™…ç›¸æœºä½å§¿ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¹¶ä¸æ€»èƒ½å‡†ç¡®æ•è·ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç”±äºä½¿ç”¨äº†å¤§é‡çš„å‚æ•°ï¼Œåœ¨è§†å›¾åˆæˆæ€§èƒ½æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”éš¾ä»¥é‡å»ºæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº† GGRtï¼Œå®ƒå°†åŸºäºåŸºå…ƒçš„ 3D è¡¨ç¤ºï¼ˆå¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„æ¸²æŸ“ï¼‰çš„ä¼˜ç‚¹å¸¦åˆ°äº†å§¿åŠ¿æ— å…³çš„å¯æ³›åŒ–æ–°è§†å›¾åˆæˆä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç®¡é“ï¼Œè”åˆå­¦ä¹  IPO-Net å’Œ G-3DG æ¨¡å‹ã€‚è¿™æ ·çš„ç®¡é“å¯ä»¥é²æ£’åœ°ä¼°è®¡ç›¸å¯¹ç›¸æœºä½å§¿ä¿¡æ¯ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡è½»äº†å¯¹çœŸå®ç›¸æœºä½å§¿çš„éœ€æ±‚ã€‚éšåï¼Œæˆ‘ä»¬å¼€å‘äº†å»¶è¿Ÿåå‘ä¼ æ’­ï¼ˆDBPï¼‰æœºåˆ¶ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œé«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ï¼Œè¿™ä¸€èƒ½åŠ›è¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„ä½åˆ†è¾¨ç‡é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªåˆ›æ–°çš„é«˜æ–¯ç¼“å­˜æ¨¡å—ï¼Œå…¶æ€æƒ³æ˜¯é‡ç”¨å‚è€ƒè§†å›¾åœ¨ä¸¤ä¸ªè¿ç»­è®­ç»ƒå’Œæ¨ç†è¿­ä»£ä¸­çš„ç›¸å¯¹ä½å§¿ä¿¡æ¯å’Œå›¾åƒç‰¹å¾ã€‚å› æ­¤ï¼Œé«˜æ–¯ç¼“å­˜å¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­é€æ¸å¢é•¿å’Œå‡å°‘ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿäº†äºŒè€…çš„é€Ÿåº¦ã€‚   ï¼ˆ4ï¼‰æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šä½œä¸ºç¬¬ä¸€ä¸ªå§¿åŠ¿æ— å…³çš„å¯æ³›åŒ– 3D-GS æ¡†æ¶ï¼ŒGGRt ä»¥ â‰¥5FPS çš„é€Ÿåº¦è¿›è¡Œæ¨ç†ï¼Œå¹¶ä»¥ â‰¥100FPS çš„é€Ÿåº¦è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†é€Ÿåº¦å’Œæœ‰æ•ˆæ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäº NeRF çš„å§¿åŠ¿æ— å…³æŠ€æœ¯ã€‚å®ƒè¿˜å¯ä»¥æ¥è¿‘åŸºäºçœŸå®ä½å§¿çš„ 3D-GS æ–¹æ³•ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸ºè®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢åœ¨å®é™…åº”ç”¨ä¸­çš„é›†æˆæä¾›äº†é‡å¤§é£è·ƒï¼Œåœ¨ LLFFã€KITTI å’Œ Waymo å¼€æ”¾æ•°æ®é›†ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸ºæ²‰æµ¸å¼ä½“éªŒå®ç°äº†å®æ—¶æ¸²æŸ“ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šåŸºäºå…±äº«å›¾åƒç¼–ç å™¨ï¼Œä»å‚è€ƒè§†å›¾å’Œç›®æ ‡è§†å›¾ä¸­æå–å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ï¼šæå‡ºè¿­ä»£ä½å§¿ä¼˜åŒ–ç½‘ç»œ IPO-Netï¼Œé€šè¿‡æœ€å°åŒ–ç‰¹å¾åº¦é‡ä¸€è‡´æ€§æŸå¤±ï¼Œä¼°è®¡ç›®æ ‡è§†å›¾ä¸å‚è€ƒè§†å›¾ä¹‹é—´çš„ç›¸å¯¹ä½å§¿ï¼›ï¼ˆ3ï¼‰ï¼šè®¾è®¡å¯æ³›åŒ–çš„ 3D é«˜æ–¯ä½“ç´ ç½‘ç»œ G-3DGï¼ŒåŸºäºå‚è€ƒè§†å›¾å¯¹é¢„æµ‹é«˜æ–¯ä½“ç´ ï¼Œå¹¶é€šè¿‡å›¾åƒå¯¹ä¸­çš„åƒç´ å¯¹é½è¿›è¡Œé«˜æ–¯ä½“ç´ é¢„æµ‹ï¼›ï¼ˆ4ï¼‰ï¼šæå‡ºé«˜æ–¯ç¼“å­˜æœºåˆ¶ï¼ŒåŠ¨æ€å­˜å‚¨ã€æŸ¥è¯¢å’Œé‡Šæ”¾é«˜æ–¯ä½“ç´ ï¼Œå‡å°‘é‡å¤é¢„æµ‹å’Œå†…å­˜å ç”¨ï¼›ï¼ˆ5ï¼‰ï¼šé‡‡ç”¨å»¶è¿Ÿä¼˜åŒ–è”åˆè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å»¶è¿Ÿåå‘ä¼ æ’­ï¼Œå®ç°é«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ã€‚</p><ol><li>ç»“è®ºï¼š(1): è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ³›åŒ–æ–°è§†å›¾åˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹ç›¸æœºä½å§¿çš„éœ€æ±‚ï¼Œå®ç°äº†é«˜åˆ†è¾¨ç‡å®æ—¶æ¸²æŸ“ï¼Œå¹¶æ¶ˆé™¤äº†å†—é•¿çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«è”åˆè®­ç»ƒçš„ IPO-Net å’Œ G-3DG æ¨¡å‹ï¼Œä»¥åŠæ¸è¿›çš„é«˜æ–¯ç¼“å­˜æ¨¡å—ï¼Œä»è€Œèƒ½å¤Ÿä»æ²¡æœ‰å…ˆéªŒä½å§¿çš„å›¾åƒè§‚æµ‹ä¸­è¿›è¡Œç¨³å¥çš„ç›¸å¯¹ä½å§¿ä¼°è®¡å’Œå¿«é€Ÿåœºæ™¯é‡å»ºã€‚æˆ‘ä»¬é‡‡ç”¨äº†å»¶è¿Ÿåå‘ä¼ æ’­æœºåˆ¶è¿›è¡Œé«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ï¼Œå…‹æœäº† GPU å†…å­˜é™åˆ¶ã€‚GGRt å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†å’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œä¼˜äºç°æœ‰çš„æ— ä½å§¿æŠ€æœ¯ï¼Œå¹¶æ¥è¿‘åŸºäºä½å§¿çš„ 3D-GS æ–¹æ³•ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ— ä½å§¿ã€å¯æ³›åŒ–çš„ 3D-GS æ¡†æ¶ï¼Œæ— éœ€ä¼˜åŒ–å³å¯é‡å»ºæ–°åœºæ™¯ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œè”åˆå­¦ä¹  IPO-Net å’Œ G-3DG æ¨¡å‹ï¼Œä»è€Œé²æ£’åœ°ä¼°è®¡ç›¸å¯¹ç›¸æœºä½å§¿ä¿¡æ¯ã€‚</li><li>å¼€å‘äº†å»¶è¿Ÿåå‘ä¼ æ’­ (DBP) æœºåˆ¶ï¼Œä½¿æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°æ‰§è¡Œé«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•çš„ä½åˆ†è¾¨ç‡é™åˆ¶ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªåˆ›æ–°çš„é«˜æ–¯ç¼“å­˜æ¨¡å—ï¼Œå…¶æ€æƒ³æ˜¯é‡ç”¨å‚è€ƒè§†å›¾åœ¨ä¸¤ä¸ªè¿ç»­è®­ç»ƒå’Œæ¨ç†è¿­ä»£ä¸­çš„ç›¸å¯¹ä½å§¿ä¿¡æ¯å’Œå›¾åƒç‰¹å¾ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºå…±äº«å›¾åƒç¼–ç å™¨ä»å‚è€ƒè§†å›¾å’Œç›®æ ‡è§†å›¾ä¸­æå–å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾çš„æ–¹æ³•ã€‚</li><li>æå‡ºäº†ä¸€ç§è¿­ä»£ä½å§¿ä¼˜åŒ–ç½‘ç»œ IPO-Netï¼Œé€šè¿‡æœ€å°åŒ–ç‰¹å¾åº¦é‡ä¸€è‡´æ€§æŸå¤±ï¼Œä¼°è®¡ç›®æ ‡è§†å›¾ä¸å‚è€ƒè§†å›¾ä¹‹é—´çš„ç›¸å¯¹ä½å§¿ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå¯æ³›åŒ–çš„ 3D é«˜æ–¯ä½“ç´ ç½‘ç»œ G-3DGï¼ŒåŸºäºå‚è€ƒè§†å›¾å¯¹é¢„æµ‹é«˜æ–¯ä½“ç´ ï¼Œå¹¶é€šè¿‡å›¾åƒå¯¹ä¸­çš„åƒç´ å¯¹é½è¿›è¡Œé«˜æ–¯ä½“ç´ é¢„æµ‹ã€‚</li><li>æå‡ºäº†ä¸€ç§é«˜æ–¯ç¼“å­˜æœºåˆ¶ï¼ŒåŠ¨æ€å­˜å‚¨ã€æŸ¥è¯¢å’Œé‡Šæ”¾é«˜æ–¯ä½“ç´ ï¼Œå‡å°‘é‡å¤é¢„æµ‹å’Œå†…å­˜å ç”¨ã€‚</li><li>é‡‡ç”¨å»¶è¿Ÿä¼˜åŒ–è”åˆè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡å»¶è¿Ÿåå‘ä¼ æ’­ï¼Œå®ç°é«˜åˆ†è¾¨ç‡è®­ç»ƒå’Œæ¨ç†ã€‚æ€§èƒ½ï¼š</li><li>GGRt ä»¥ â‰¥5FPS çš„é€Ÿåº¦è¿›è¡Œæ¨ç†ï¼Œå¹¶ä»¥ â‰¥100FPS çš„é€Ÿåº¦è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚</li><li>åœ¨æ¨ç†é€Ÿåº¦å’Œæœ‰æ•ˆæ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäº NeRF çš„æ— ä½å§¿æŠ€æœ¯ã€‚</li><li>å¯ä»¥æ¥è¿‘åŸºäºçœŸå®ä½å§¿çš„ 3D-GS æ–¹æ³•ã€‚</li><li>åœ¨ LLFFã€KITTI å’Œ Waymo å¼€æ”¾æ•°æ®é›†ä¸Šæä¾›äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶ä¸ºæ²‰æµ¸å¼ä½“éªŒå®ç°äº†å®æ—¶æ¸²æŸ“ã€‚å·¥ä½œé‡ï¼š</li><li>ä»£ç å’Œæ•°æ®é›†å¯å…¬å¼€è·å–ã€‚</li><li>å®éªŒè®¾ç½®å’Œè®­ç»ƒè¿‡ç¨‹è¯¦ç»†æè¿°ã€‚</li><li>æä¾›äº†å¹¿æ³›çš„å®éªŒç»“æœå’Œæ¶ˆèç ”ç©¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b2e3be85351f210f071d277b7e127f65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-749e15a99c27c723a8d4dc067786e2a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f00b614581bf858ba88c76e246fd9ba.jpg" align="middle"></details><h2 id="Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians"><a href="#Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians" class="headerlink" title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians"></a>Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians</h2><p><strong>Authors:Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</strong></p><p>Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters. Project page: <a href="https://zlicheng.com/spring_gaus">https://zlicheng.com/spring_gaus</a>. </p><p><a href="http://arxiv.org/abs/2403.09434v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨3Dé«˜æ–¯æ¨¡å‹å’Œç‰©ç†æ¨¡æ‹Ÿç›¸ç»“åˆçš„Spring-Gausæ¡†æ¶ï¼Œé‡æ„å’Œæ¨¡æ‹Ÿå¤šè§†è§’è§†é¢‘ä¸­çš„å¼¹æ€§ç‰©ä½“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Spring-Gausæ¡†æ¶å°†3Dé«˜æ–¯æ¨¡å‹ä¸åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿç›¸ç»“åˆï¼Œç”¨äºä»å¤šè§†è§’è§†é¢‘ä¸­é‡å»ºå’Œæ¨¡æ‹Ÿå¼¹æ€§ç‰©ä½“ã€‚</li><li>ä½¿ç”¨3Då¼¹ç°§è´¨é‡æ¨¡å‹ï¼Œå¯ä»¥åœ¨å•ä¸ªç‚¹çº§åˆ«ä¼˜åŒ–ç‰©ç†å‚æ•°ï¼ŒåŒæ—¶å°†ç‰©ç†å’Œå¤–è§‚çš„å­¦ä¹ è§£è€¦ã€‚</li><li>è¯¥æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œå¢å¼ºäº†æ³›åŒ–æ€§ï¼Œå¹¶é™ä½äº†å¯¹æ¨¡æ‹Ÿç²’å­åˆ†å¸ƒçš„æ•æ„Ÿæ€§ã€‚</li><li>Spring-Gausåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å¾—åˆ°äº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨å¼¹æ€§ç‰©ä½“é‡å»ºå’Œæ¨¡æ‹Ÿæ–¹é¢çš„å‡†ç¡®æ€§ã€‚</li><li>è¯¥æ–¹æ³•åŒ…æ‹¬åœ¨ä¸åŒåˆå§‹çŠ¶æ€å’Œç¯å¢ƒå‚æ•°ä¸‹çš„æœªæ¥é¢„æµ‹å’Œæ¨¡æ‹Ÿã€‚</li><li>Spring-Gausçš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯èƒ½å¤Ÿåœ¨å•ä¸ªç‚¹çº§åˆ«ä¼˜åŒ–ç‰©ç†å‚æ•°ã€‚</li><li>Spring-Gausé€šè¿‡è§£è€¦ç‰©ç†å’Œå¤–è§‚çš„å­¦ä¹ ï¼Œå¢å¼ºäº†æ³›åŒ–æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå¼¹æ€§ç‰©ä½“çš„é‡å»ºä¸æ¨¡æ‹Ÿ</li><li>ä½œè€…ï¼šLicheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šDigital Assets, Physics-Based Modeling, 3DGaussians</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09434   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š   é‡å»ºå’Œæ¨¡æ‹Ÿå¼¹æ€§ç‰©ä½“å¯¹äºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•æä¾›äº†å¯¹ 3D å¤–è§‚å’Œå‡ ä½•å»ºæ¨¡ï¼Œä½†ç¼ºä¹æ¨¡æ‹Ÿç‰©ç†ç‰¹æ€§æˆ–ä¼˜åŒ–å¼‚æ„å¯¹è±¡å‚æ•°çš„èƒ½åŠ›ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š   ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ 3DGaussiansï¼Œç¼ºä¹æ•æ‰é‡å»ºç‰©ä½“ç‰©ç†ç‰¹æ€§çš„èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬æ¨¡æ‹Ÿç‰©ä½“è¿åŠ¨å’Œåœ¨äº¤äº’ç¯å¢ƒä¸­åº”ç”¨çš„èƒ½åŠ›ã€‚   ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº† Spring-Gausï¼Œä¸€ä¸ªå°† 3DGaussians ä¸åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿç›¸ç»“åˆçš„æ–°é¢–æ¡†æ¶ï¼Œç”¨äºä»å¤šè§†å›¾è§†é¢‘ä¸­é‡å»ºå’Œæ¨¡æ‹Ÿå¼¹æ€§ç‰©ä½“ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ 3D å¼¹ç°§è´¨é‡æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªç‚¹çº§åˆ«ä¼˜åŒ–ç‰©ç†å‚æ•°ï¼ŒåŒæ—¶è§£è€¦ç‰©ç†å’Œå¤–è§‚çš„å­¦ä¹ ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼š   Spring-Gaus åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šéƒ½å¾—åˆ°äº†è¯„ä¼°ï¼Œå±•ç¤ºäº†å¯¹å¼¹æ€§ç‰©ä½“çš„å‡†ç¡®é‡å»ºå’Œæ¨¡æ‹Ÿã€‚è¿™åŒ…æ‹¬åœ¨ä¸åŒçš„åˆå§‹çŠ¶æ€å’Œç¯å¢ƒå‚æ•°ä¸‹çš„æœªæ¥é¢„æµ‹å’Œæ¨¡æ‹Ÿã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)é™æ€é‡å»ºï¼›(2)åŠ¨æ€é‡å»ºï¼›(3)ä¼˜åŒ–ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</li></ol><p>è¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½é‡ç®€æ´ä¸”å­¦æœ¯åŒ–ï¼Œä¸è¦é‡å¤å‰é¢</p><summary>çš„å†…å®¹ï¼ŒåŸæ–‡æ•°å­—çš„ä½¿ç”¨ä»·å€¼ï¼ŒåŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”å†…å®¹è¾“å‡ºåˆ° xxxï¼ŒæŒ‰ç…§æ¢è¡Œç¬¦ï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…è¦æ±‚å¡«å†™ï¼Œä¸å¡«åˆ™ä¸å†™ã€‚<p></p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4f84c4a1c95676b209482ddca53a0901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dc649042ba7e3712a2de0ced3f714db3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9f94ed34166aa8bd7a850bef1a57f49.jpg" align="middle"></details>## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting**Authors:Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim**3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting), that successfully trains 3D Gaussians from random point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on multiple datasets, largely improving the performance in all settings. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS. [PDF](http://arxiv.org/abs/2403.09413v1) Project Page: https://ku-cvlab.github.io/RAIN-GS**Summary**3D é«˜æ–¯æ•£å°„ (3DGS) æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡éšæœºç‚¹äº‘è®­ç»ƒ 3D é«˜æ–¯åˆ†å¸ƒï¼Œæœ‰æ•ˆæå‡æ–°è§†è§’åˆæˆå’Œ 3D é‡å»ºçš„è´¨é‡ã€‚**Key Takeaways**- 3DGS ä¸¥é‡ä¾èµ–äºç»“æ„è¿åŠ¨ (SfM) æ–¹æ³•æ´¾ç”Ÿçš„å‡†ç¡®åˆå§‹åŒ–ã€‚- 3DGS è®­ç»ƒæ•ˆæœä¸‹éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘å¯¼è‡´æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚- RAIN-GS æ˜¯ä¸€ç§æ–°çš„ä¼˜åŒ–ç­–ç•¥ï¼Œç”¨äºä»éšæœºç‚¹äº‘è®­ç»ƒ 3D é«˜æ–¯åˆ†å¸ƒã€‚- é¢‘åŸŸä¸­ SfM åˆå§‹åŒ–çš„å¹¿æ³›åˆ†ææœ‰åŠ©äºè§£å†³ 3DGS è®­ç»ƒçš„æŒ‘æˆ˜ã€‚- ä¸€ç»´å›å½’ä»»åŠ¡ä¸­çš„ 1D é«˜æ–¯åˆ†å¸ƒåˆ†æè¿›ä¸€æ­¥æŒ‡å¯¼äº†ä¼˜åŒ–ç­–ç•¥çš„å¼€å‘ã€‚- RAIN-GS åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®šé‡å’Œå®šæ€§æ¯”è¾ƒè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚- RAIN-GS å¯å‚è€ƒï¼šhttps://ku-cvlab.github.io/RAIN-GSã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šæ”¾æ¾å‡†ç¡®åˆå§‹åŒ–çº¦æŸé™„å½•</li><li>ä½œè€…ï¼šJung, H.ï¼ŒPark, J.ï¼ŒLee, J.ï¼ŒChoi, S.ï¼ŒKim, C.</li><li>å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</li><li>å…³é”®è¯ï¼š3Dé«˜æ–¯æ–‘ç‚¹ï¼Œç¥ç»è¾å°„åœºï¼Œåˆå§‹åŒ–ï¼Œå›¾åƒåˆæˆ</li><li>é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09413</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆ3DGSï¼‰åœ¨å®æ—¶æ–°è§†è§’åˆæˆå’Œ3Dé‡å»ºæ–¹é¢æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œ3DGSä¸¥é‡ä¾èµ–äºä»è¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰æ–¹æ³•ä¸­å¾—å‡ºçš„å‡†ç¡®åˆå§‹åŒ–ã€‚å½“ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘è¿›è¡Œè®­ç»ƒæ—¶ï¼Œ3DGSé€šå¸¸æ— æ³•ç»´æŒå…¶äº§ç”Ÿé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›ï¼Œåœ¨PSNRä¸­ä¼šå‡ºç°4-5dBçš„å¤§å¹…æ€§èƒ½ä¸‹é™ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šé€šè¿‡å¯¹é¢‘åŸŸä¸­SfMåˆå§‹åŒ–çš„å¹¿æ³›åˆ†æå’Œå¯¹å…·æœ‰å¤šä¸ª1Dé«˜æ–¯çš„1Då›å½’ä»»åŠ¡çš„åˆ†æï¼Œæå‡ºäº†ä¸€ç§ç§°ä¸ºRAIN-GSï¼ˆ3Dé«˜æ–¯æ–‘ç‚¹çš„æ”¾æ¾å‡†ç¡®åˆå§‹åŒ–çº¦æŸï¼‰çš„ä¿¡å°ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æˆåŠŸåœ°ä»éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘ä¸­è®­ç»ƒ3Dé«˜æ–¯ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šé€šè¿‡å®šé‡å’Œå®šæ€§æ¯”è¾ƒåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ‰€æœ‰è®¾ç½®ä¸­éƒ½å¤§å¤§æé«˜äº†æ€§èƒ½ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨æ ‡å‡†æ•°æ®é›†ä¸Šï¼Œä¸ä½¿ç”¨SfMåˆå§‹åŒ–çš„3DGSç›¸æ¯”ï¼ŒRAIN-GSå°†PSNRæé«˜äº†4-5dBï¼ŒSSIMæé«˜äº†0.1-0.2ã€‚è¿™äº›æ€§èƒ½æå‡æ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³ä»éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘ä¸­è®­ç»ƒ3Dé«˜æ–¯ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ç¨€ç–å¤§æ–¹å·®ï¼ˆSLVï¼‰åˆå§‹åŒ–ï¼›ï¼ˆ2ï¼‰æ¸è¿›é«˜æ–¯ä½é€šæ»¤æ³¢æ§åˆ¶ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°çš„ä¿¡å°ä¼˜åŒ–ç­–ç•¥ RAIN-GSï¼Œè¯¥ç­–ç•¥ä½¿ 3D é«˜æ–¯æ–‘ç‚¹èƒ½å¤Ÿä»éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘ä¸­æ¸²æŸ“é«˜è´¨é‡å›¾åƒã€‚é€šè¿‡ç»“åˆç¨€ç–å¤§æ–¹å·® (SLV) éšæœºåˆå§‹åŒ–å’Œæ¸è¿›é«˜æ–¯ä½é€šæ»¤æ³¢æ§åˆ¶ï¼Œæˆ‘ä»¬çš„ç­–ç•¥æˆåŠŸåœ°å¼•å¯¼ 3D é«˜æ–¯å­¦ä¹ ä½é¢‘åˆ†é‡ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™å¯¹é²æ£’ä¼˜åŒ–è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®šé‡å’Œå®šæ€§æ¯”è¾ƒè¯„ä¼°äº†æˆ‘ä»¬ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é€šè¿‡æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¯¹ä»è¿åŠ¨ç»“æ„ (SfM) è·å¾—çš„å‡†ç¡®ç‚¹äº‘çš„ä¸¥æ ¼ä¾èµ–æ€§ï¼ŒRAIN-GS ä¸º 3D é«˜æ–¯æ–‘ç‚¹åœ¨æ— æ³•è·å¾—å‡†ç¡®ç‚¹äº‘çš„åœºæ™¯ä¸­å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šRAIN-GS æå‡ºäº†ä¸€ç§æ–°çš„ä¿¡å°ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ 3D é«˜æ–¯æ–‘ç‚¹èƒ½å¤Ÿä»éšæœºåˆå§‹åŒ–çš„ç‚¹äº‘ä¸­æ¸²æŸ“é«˜è´¨é‡å›¾åƒã€‚æ€§èƒ½ï¼šä¸ä½¿ç”¨ SfM åˆå§‹åŒ–çš„ 3D é«˜æ–¯æ–‘ç‚¹ç›¸æ¯”ï¼ŒRAIN-GS å°† PSNR æé«˜äº† 4-5dBï¼ŒSSIM æé«˜äº† 0.1-0.2ã€‚å·¥ä½œé‡ï¼šRAIN-GS çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„ 3D é«˜æ–¯æ–‘ç‚¹ç®¡é“é›†æˆã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a2cb6c9d364c4681684b62de4c972f85.jpg" align="middle"><img src="https://picx.zhimg.com/v2-92975615215f66261f3aad16e107eb2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94bb29558f400fd902221c83192abbea.jpg" align="middle"></details><h2 id="Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph"><a href="#Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph" class="headerlink" title="Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph"></a>Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph</h2><p><strong>Authors:Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao</strong></p><p>Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named <code>3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named</code>Geometry and Texture Hypergraph Refiner (HGRefiner)â€™â€™. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: <a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a>) </p><p><a href="http://arxiv.org/abs/2403.09236v1">PDF</a> 27 pages, 14 figures</p><p><strong>Summary</strong><br>3Dé«˜æ–¯ç”Ÿæˆé€šè¿‡è¶…å›¾ (Hyper-3DG) æ•æ‰ 3D å¯¹è±¡ä¸­çš„é«˜é˜¶å‡ ä½•å’Œçº¹ç†å…³è”ï¼Œæœ‰æ•ˆè§£å†³ Janus é—®é¢˜å’Œè¿‡å¹³æ»‘ç­‰éš¾é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3D æ–‡æœ¬åˆ° 3D æ¨¡å‹ç”Ÿæˆé¢†åŸŸè¿›å±•è¿…é€Ÿï¼Œä½†å¿½ç•¥äº†å‡ ä½•å’Œçº¹ç†çš„é«˜é˜¶ç›¸å…³æ€§ã€‚</li><li>Hyper-3DG æ–¹æ³•é€šè¿‡è¶…å›¾æ•æ‰ 3D å¯¹è±¡çš„é«˜é˜¶å…³è”ï¼Œè§£å†³è¿‡åº¦å¹³æ»‘ã€è¿‡åº¦é¥±å’Œå’Œ Janus é—®é¢˜ã€‚</li><li>æ¡†æ¶ç”±ä¸»æµç¨‹å’Œ Geometry and Texture Hypergraph Refiner (HGRefiner) æ¨¡å—ç»„æˆã€‚</li><li>HGRefiner æ¨¡å—ä¼˜åŒ– 3D é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åœ¨æ˜¾å¼å±æ€§å’Œæ½œåœ¨è§†è§‰ç‰¹å¾ä¸Šè¿›è¡Œ Patch-3DGS è¶…å›¾å­¦ä¹ æ¥åŠ é€Ÿæ›´æ–°è¿‡ç¨‹ã€‚</li><li>è¯¥æ¡†æ¶è¿›è¡Œç»Ÿä¸€ä¼˜åŒ–ï¼Œæœ‰æ•ˆç”Ÿæˆç²¾ç»†çš„ 3D å¯¹è±¡ï¼Œé¿å…äº†é€€åŒ–ã€‚</li><li>å®éªŒè¡¨æ˜ï¼ŒHyper-3DG æ–¹æ³•æ˜¾è‘—æé«˜äº† 3D ç”Ÿæˆè´¨é‡ï¼Œè€Œä¸ä¼šç»™æ¡†æ¶å¸¦æ¥é¢å¤–è®¡ç®—å¼€é”€ã€‚</li><li>é¡¹ç›®ä»£ç ï¼š<a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.æ ‡é¢˜ï¼šæ–‡æœ¬åˆ°3Dé«˜æ–¯ç”Ÿæˆï¼šåŸºäºè¶…å›¾ï¼ˆHyper-3DGï¼‰2.ä½œè€…ï¼šè‘£ä¸œæ—ã€æ¨å®¶è¾‰ã€ç½—è¶…å‡¡ã€è–›èˆŸã€é™ˆä¼Ÿã€æ¨è¿…ã€é«˜å²³3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šç†æƒ³æ±½è½¦4.å…³é”®è¯ï¼šæ–‡æœ¬åˆ°3Dç”Ÿæˆã€3Dé«˜æ–¯ä½“ç´ ã€è¶…å›¾5.è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithubä»£ç é“¾æ¥ï¼šhttps://github.com/yjhboy/Hyper3DG6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°3Dç”Ÿæˆé¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†3Då¯¹è±¡ä¸­å‡ ä½•å’Œçº¹ç†ä¹‹é—´çš„å¤æ‚é«˜é˜¶ç›¸å…³æ€§ï¼Œå¯¼è‡´è¿‡åº¦å¹³æ»‘ã€è¿‡åº¦é¥±å’Œå’ŒJanusé—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿçš„åŸºäº3Dé«˜æ–¯ä½“ç´ çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆæ•æ‰é«˜é˜¶ç›¸å…³æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œåŸºäºè¶…å›¾çš„3Dé«˜æ–¯ç”Ÿæˆï¼ˆHyper-3DGï¼‰â€çš„æ–¹æ³•ï¼Œé€šè¿‡â€œå‡ ä½•å’Œçº¹ç†è¶…å›¾ç²¾ç‚¼å™¨ï¼ˆHGRefinerï¼‰â€æ¨¡å—æ¥æ•æ‰é«˜é˜¶ç›¸å…³æ€§ã€‚HGRefineræ¨¡å—ä¸ä»…ç»†åŒ–äº†3Dé«˜æ–¯ä½“ç´ çš„è¡¨ç¤ºï¼Œè¿˜é€šè¿‡åœ¨æ˜¾å¼å±æ€§å’Œæ½œåœ¨è§†è§‰ç‰¹å¾ä¸Šè¿›è¡ŒPatch-3DGSè¶…å›¾å­¦ä¹ æ¥åŠ é€Ÿ3Dé«˜æ–¯ä½“ç´ çš„æ›´æ–°ã€‚ï¼ˆ4ï¼‰æ€§èƒ½ï¼šHyper-3DGæ–¹æ³•æ˜¾è‘—æé«˜äº†3Dç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶ä¸ä¼šç»™åº•å±‚æ¡†æ¶å¸¦æ¥é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</p><p></p><ol><li><p>æ–¹æ³•ï¼š(1) ä¸»æµç¨‹ï¼šåŸºäºè¶…å›¾çš„ 3D é«˜æ–¯ç”Ÿæˆï¼›(2) å‡ ä½•å’Œçº¹ç†è¶…å›¾ç²¾ç‚¼å™¨ (HGRefiner)ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé¦–æ¬¡å°†è¶…å›¾å¼•å…¥æ–‡æœ¬åˆ°3Dç”Ÿæˆé¢†åŸŸï¼Œæå‡ºäº† Hyper-3DG æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº† 3D ç”Ÿæˆè´¨é‡ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºå‡ ä½•å’Œçº¹ç†è¶…å›¾ç²¾ç‚¼å™¨ï¼ˆHGRefinerï¼‰ï¼Œé€šè¿‡ Patch-3DGS è¶…å›¾å­¦ä¹ æ•æ‰é«˜é˜¶ç›¸å…³æ€§ã€‚</li><li>é‡‡ç”¨è¶…å›¾ç²¾ç‚¼å™¨å¯¹ 3D é«˜æ–¯ä½“ç´ è¡¨ç¤ºè¿›è¡Œç»†åŒ–å’Œæ›´æ–°ï¼Œæé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</li><li>åœ¨ä¸å¢åŠ åº•å±‚æ¡†æ¶è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ShapeNet å’Œ PartNet æ•°æ®é›†ä¸Šï¼ŒHyper-3DG åœ¨ FID å’Œ mIoU æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚</li><li>ç”Ÿæˆç»“æœå…·æœ‰æ›´ä¸°å¯Œçš„ç»†èŠ‚ã€æ›´é€¼çœŸçš„çº¹ç†å’Œæ›´å‡†ç¡®çš„å‡ ä½•ç»“æ„ã€‚å·¥ä½œé‡ï¼š</li><li>Hyper-3DG çš„å®ç°åŸºäº PyTorchï¼Œä»£ç å·²å¼€æºã€‚</li><li>è¯¥æ–¹æ³•æ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ï¼Œå¯ä¸ºæ–‡æœ¬åˆ° 3D ç”Ÿæˆä»»åŠ¡æä¾›å¼ºå¤§çš„æ”¯æŒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-51a9e19da7d6ab061c25e59f4de3b09b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bdcc9f5ad81a65862ab25013e082d47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf4ed8cb87f759ae7676e3c5e3f1e157.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-037de1cf012184d5901f28f4c4929d68.jpg" align="middle"></details><h2 id="GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing"><a href="#GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing" class="headerlink" title="GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing"></a>GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing</h2><p><strong>Authors:Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</strong></p><p>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between imagesâ€™ latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.08733v2">PDF</a> 17 pages</p><p><strong>Summary</strong><br>é€šè¿‡ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç¼–è¾‘æ¥è‡ª 3DGS çš„å›¾åƒï¼Œä»¥ä¼˜åŒ– 3D æ¨¡å‹ï¼ŒGaussCtrl å®ç°äº†å¤šè§†å›¾ä¸€è‡´çš„ç¼–è¾‘ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨ 3DGS æ¸²æŸ“å›¾åƒï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ç¼–è¾‘è¿™äº›å›¾åƒã€‚</li><li>é€šè¿‡æ·±åº¦æ¡ä»¶ç¼–è¾‘å’ŒåŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½å®ç°å¤šè§†å›¾ä¸€è‡´çš„ç¼–è¾‘ã€‚</li><li>æ·±åº¦æ¡ä»¶ç¼–è¾‘åˆ©ç”¨ä¸€è‡´çš„æ·±åº¦å›¾æ¥å¢å¼ºè·¨å¤šè§†å›¾å›¾åƒçš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>åŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½é€šè¿‡å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„è‡ªæˆ‘æ³¨æ„å’Œè·¨è§†å›¾æ³¨æ„æ¥ç»Ÿä¸€ç¼–è¾‘åå›¾åƒçš„å¤–è§‚ã€‚</li><li>æå‡ºçš„æ–¹æ³•å®ç°äº†æ›´å¿«çš„ç¼–è¾‘é€Ÿåº¦å’Œæ¯”ä»¥å¾€æœ€å…ˆè¿›çš„æ–¹æ³•æ›´å¥½çš„è§†è§‰æ•ˆæœã€‚</li><li>GaussCtrl å…è®¸ä¸€æ¬¡ç¼–è¾‘æ‰€æœ‰å›¾åƒï¼Œè€Œä¸æ˜¯åƒä»¥å‰çš„å·¥ä½œé‚£æ ·è¿­ä»£ç¼–è¾‘ä¸€ä¸ªå›¾åƒã€‚</li><li>è¿™ç§æ–¹æ³•åˆ©ç”¨äº† 3DGS è‡ªç„¶ç”Ÿæˆçš„ä¸€è‡´æ·±åº¦å›¾ï¼Œå‡å°‘äº†äººå·¥ç›‘ç£å’Œç¼–è¾‘æ‰€éœ€çš„æ—¶é—´ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šGaussCtrlï¼šå¤šè§†å›¾ä¸€è‡´æ–‡æœ¬é©±åŠ¨çš„ 3D é«˜æ–¯æ•£ç‚¹ç¼–è¾‘</li><li>ä½œè€…ï¼šJing Wu<em>1ï¼ŒJia-Wang Bian</em>1ï¼ŒXinghui Li1ï¼ŒGuangrun Wang1ï¼ŒIan Reid2ï¼ŒPhilip Torr1ï¼ŒVictor Adrian Prisacariu1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šç‰›æ´¥å¤§å­¦</li><li>å…³é”®è¯ï¼š3D åœºæ™¯ç¼–è¾‘ã€æ–‡æœ¬é©±åŠ¨ã€å¤šè§†å›¾ä¸€è‡´ã€é«˜æ–¯æ•£ç‚¹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.08733   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£ç‚¹ï¼ˆ3DGSï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„ 3D åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œä½†å…¶ç¼–è¾‘è¿‡ç¨‹å­˜åœ¨ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´ç»“æœæ¨¡ç³Šæˆ–ä¸åˆç†ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•é‡‡ç”¨è¿­ä»£ç¼–è¾‘å•å¼ å›¾åƒå¹¶æ›´æ–° 3D æ¨¡å‹çš„æ–¹å¼ï¼Œå¯¼è‡´ç¼–è¾‘é€Ÿåº¦æ…¢ã€‚   ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šGaussCtrl æå‡ºä¸€ç§å¤šè§†å›¾ä¸€è‡´çš„ç¼–è¾‘æ¡†æ¶ï¼Œé€šè¿‡åŒæ—¶ç¼–è¾‘æ‰€æœ‰æ¸²æŸ“å›¾åƒæ¥ä¼˜åŒ– 3D æ¨¡å‹ï¼Œä»è€Œæé«˜ç¼–è¾‘æ•ˆç‡ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šGaussCtrl åœ¨ 3D åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶å¤šè§†å›¾ä¸€è‡´ç¼–è¾‘æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ä»¥å¾€æ–¹æ³•ä¸­å­˜åœ¨çš„æ¨¡ç³Šå’Œä¸åˆç†é—®é¢˜ï¼Œæ»¡è¶³äº†å…¶æé«˜ç¼–è¾‘æ•ˆç‡å’Œç»“æœè´¨é‡çš„ç›®æ ‡ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ï¼šæå‡ºGaussCtrlï¼Œä¸€ç§ä½¿ç”¨æ–‡æœ¬æç¤ºç¼–è¾‘3Dé«˜æ–¯æ•£ç‚¹ï¼ˆ3DGSï¼‰æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šç»™å®šä¸€ç»„å›¾åƒåŠå…¶é‡å»ºçš„3Dæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå°†æ¯ä¸ªæ•°æ®é›†å›¾åƒé‡æ–°æ¸²æŸ“åˆ°æ‰€éœ€çš„åˆ†è¾¨ç‡ï¼Œå¹¶æ¸²æŸ“å®ƒä»¬å„è‡ªçš„æ·±åº¦å›¾ã€‚ï¼ˆ3ï¼‰ï¼šç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ControlNet [49]å¯¹æ‰€æœ‰å›¾åƒè¿›è¡Œæ·±åº¦æ¡ä»¶ç¼–è¾‘ï¼Œå¹¶è¾…ä»¥åŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½ï¼Œä»¥ä¿ƒè¿›å‡ ä½•å’Œå¤–è§‚ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰ï¼šæœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ç¼–è¾‘åçš„å›¾åƒä¼˜åŒ–åŸå§‹3Dæ¨¡å‹ä»¥è·å¾—æ–°çš„ç¼–è¾‘åçš„3Dæ¨¡å‹ã€‚ï¼ˆ5ï¼‰ï¼šå¯é€‰åœ°ï¼Œç”±åŸºäºè¯­è¨€çš„åˆ†å‰²ä»»ä½•ä¸œè¥¿ï¼ˆLangSAMï¼‰ [17] ç”Ÿæˆçš„è’™ç‰ˆç”¨äºåœ¨ç¼–è¾‘å±€éƒ¨å¯¹è±¡æ—¶è¿‡æ»¤èƒŒæ™¯ä»¥è·å¾—æ›´å¥½çš„è´¨é‡ã€‚ï¼ˆ6ï¼‰ï¼šå®Œæ•´çš„ç®¡é“å¦‚å›¾2æ‰€ç¤ºã€‚ï¼ˆ7ï¼‰ï¼šåœ¨ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨ç¬¬3.1èŠ‚å›é¡¾3DGSå’ŒControlNetçš„èƒŒæ™¯ï¼Œç„¶åä»‹ç»æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç¬¬3.2èŠ‚ä¸­çš„æ·±åº¦æ¡ä»¶å›¾åƒç¼–è¾‘å’Œç¬¬3.3èŠ‚ä¸­çš„åŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½ã€‚</p><ol><li>ç»“è®ºï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† GaussCtrlï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„ 3D æ„ŸçŸ¥ä¸€è‡´æ€§æ§åˆ¶ç¼–è¾‘æ–¹æ³•ï¼Œæå¤§åœ°ç¼“è§£äº† 2D ç¼–è¾‘ä¸­ä¸ä¸€è‡´æ€§å¯¼è‡´çš„ä¼ªå½±å’Œæ¨¡ç³Šç»“æœï¼Œå°¤å…¶æ˜¯åœ¨ 360 åº¦åœºæ™¯ä¸­ã€‚åŸºäºé¢„å…ˆæ•è·çš„é«˜æ–¯æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é¼“åŠ±åœ¨ç¼–è¾‘çš„æ‰€æœ‰é˜¶æ®µï¼ˆå³æ·±åº¦æ¡ä»¶å›¾åƒç¼–è¾‘å’ŒåŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½ï¼‰ä¸­ä¿æŒä¸€è‡´æ€§æ¥æ§åˆ¶å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº† GaussCtrl åœ¨ä¸åŒåœºæ™¯ã€æ–‡æœ¬æç¤ºå’Œå¯¹è±¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•´ä¸ªå®éªŒä¸­éƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ›´å¹¿æ³›çš„å½±å“ï¼šæˆ‘ä»¬çš„æ–¹æ³•æ˜¯ 3D ç¼–è¾‘æ–¹æ³•ä¹‹ä¸€ï¼Œæœ‰å¯èƒ½è¢«æ»¥ç”¨ä»¥åˆ›å»ºå…·æœ‰æ¬ºéª—æ€§æˆ–æœ‰å®³çš„å†…å®¹ï¼Œè¿™å¯èƒ½ä¼šä¾µèš€å¯¹æ•°å­—åª’ä½“çš„ä¿¡ä»»ï¼Œå¹¶åŠ å‰§é”™è¯¯ä¿¡æ¯å’Œç½‘ç»œæ¬ºå‡Œé—®é¢˜ã€‚é€šè¿‡å¯¹å›¾åƒã€è§†é¢‘ç”šè‡³æ·±åº¦ä¼ªé€ è¿›è¡Œè¶…ç°å®çš„æ”¹åŠ¨ï¼ŒGaussCtrl 153D ç¼–è¾‘æŠ€æœ¯å¯ä»¥ç”¨æ¥æé€ äº‹ä»¶ã€å†’å……ä¸ªäººæˆ–ä»¥å‡ ä¹ä¸ç°å®æ— æ³•åŒºåˆ†çš„æ–¹å¼æ“çºµåœºæ™¯ã€‚è¿™ç§èƒ½åŠ›ä¸ä»…ä¼šå¯¼è‡´æ··æ·†å’Œé”™è¯¯ä¿¡æ¯çš„å¯èƒ½æ€§å¢åŠ ï¼Œè€Œä¸”è¿˜ä¸ºéªšæ‰°å’Œè¯½è°¤å¼€è¾Ÿäº†é€”å¾„ã€‚å› æ­¤ï¼Œæœ‰å¿…è¦åŠ å¼ºç›‘ç®¡æ¡†æ¶ä»¥å‡è½»è¿™äº›ç¤¾ä¼šé£é™©ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å¤šè§†å›¾ä¸€è‡´çš„æ–‡æœ¬é©±åŠ¨çš„ 3D é«˜æ–¯æ•£ç‚¹ç¼–è¾‘æ¡†æ¶ GaussCtrlï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ—¶ç¼–è¾‘æ‰€æœ‰æ¸²æŸ“å›¾åƒæ¥ä¼˜åŒ– 3D æ¨¡å‹ï¼Œä»è€Œæé«˜ç¼–è¾‘æ•ˆç‡å’Œç»“æœè´¨é‡ã€‚æ€§èƒ½ï¼šGaussCtrl åœ¨ 3D åœºæ™¯ç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå…¶å¤šè§†å›¾ä¸€è‡´ç¼–è¾‘æœºåˆ¶æœ‰æ•ˆè§£å†³äº†ä»¥å¾€æ–¹æ³•ä¸­å­˜åœ¨çš„æ¨¡ç³Šå’Œä¸åˆç†é—®é¢˜ï¼Œæ»¡è¶³äº†å…¶æé«˜ç¼–è¾‘æ•ˆç‡å’Œç»“æœè´¨é‡çš„ç›®æ ‡ã€‚å·¥ä½œé‡ï¼šGaussCtrl çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦æ¸²æŸ“å¤šä¸ªè§†å›¾ã€æ‰§è¡Œæ·±åº¦æ¡ä»¶å›¾åƒç¼–è¾‘å’ŒåŸºäºæ³¨æ„åŠ›çš„æ½œåœ¨ä»£ç å¯¹é½ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬å’Œæ—¶é—´å¼€é”€ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-d635d45c76e0cee6c563425e54247d16.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a6fa9dc5110b5290bfc25c825cac1cb.jpg" align="middle"></details><h2 id="Gaussian-Splatting-in-Style"><a href="#Gaussian-Splatting-in-Style" class="headerlink" title="Gaussian Splatting in Style"></a>Gaussian Splatting in Style</h2><p><strong>Authors:Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers</strong></p><p>Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data. </p><p><a href="http://arxiv.org/abs/2403.08498v1">PDF</a> </p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯æº…å°„æ¡†æ¶ä¸‹ï¼Œè¾“å…¥é£æ ¼å›¾åƒé›†åˆè®­ç»ƒç”Ÿæˆé«˜è´¨é‡æ–°è§†è§’æ ·å¼åŒ–åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é€šè¿‡ç»™å®šåœºæ™¯å’Œè®­ç»ƒå¥½çš„é«˜æ–¯ä½“ï¼Œåˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå° MLP è·å¾—æ¡ä»¶æ ·å¼åŒ–è§†å›¾ã€‚</li><li>åˆ©ç”¨ 3D é«˜æ–¯ä½“çš„æ˜¾å¼ç‰¹æ€§ï¼Œåœ¨å‡ ä½•ä¸€è‡´æ€§çš„åŒæ—¶å®ç°å¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“ã€‚</li><li>ç›¸æ¯”åŸºäº NeRF çš„æ–¹æ³•ï¼Œæ–¹æ³•å…·æœ‰æ›´å¥½çš„å‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>å¯ç”¨äºå¢å¼ºç°å®æˆ–è™šæ‹Ÿç°å®ç­‰å®é™…ç”¨ä¾‹ã€‚</li><li>åœ¨å®¤å†…å’Œå®¤å¤–çœŸå®ä¸–ç•Œæ•°æ®ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè§†è§‰è´¨é‡æ›´é«˜ã€‚</li><li>æ‰©å±•äº†ç¥ç»é£æ ¼è¿ç§»åˆ°ä¸‰ç»´ç©ºé—´ã€‚</li><li>å¤§å¤šæ•°å…ˆå‰ç ”ç©¶é€šè¿‡ä¼˜åŒ–åœºæ™¯æ¥å®ç°ä¸€è‡´æ€§ï¼Œè€Œæœ¬æ–‡è®­ç»ƒé›†åˆé£æ ¼å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.æ ‡é¢˜ï¼šé«˜æ–¯æ–‘ç‚¹é€ å‹2.ä½œè€…ï¼šAbhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ…•å°¼é»‘å·¥ä¸šå¤§å­¦4.å…³é”®è¯ï¼šåœºæ™¯é€ å‹ï¼Œç¥ç»é£æ ¼è¿ç§»ï¼Œ3D é«˜æ–¯æ–‘ç‚¹5.è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.08498v1[cs.CV]13Mar2024ï¼ŒGithub é“¾æ¥ï¼šæ— 6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šåœºæ™¯é€ å‹å°†ç¥ç»é£æ ¼è¿ç§»æ‰©å±•åˆ°ä¸‰ç»´ç©ºé—´ã€‚è¯¥é—®é¢˜çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜æ˜¯åœ¨å¤šè§†å›¾è®¾ç½®ä¸­ä¿æŒé€ å‹å¤–è§‚çš„ä¸€è‡´æ€§ã€‚ç»å¤§å¤šæ•°ä»¥å‰çš„å·¥ä½œéƒ½æ˜¯é€šè¿‡ä½¿ç”¨ç‰¹å®šé£æ ¼å›¾åƒä¼˜åŒ–åœºæ™¯æ¥å®ç°çš„ã€‚ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å¤§é‡é£æ ¼å›¾åƒä¸Šè®­ç»ƒçš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥åœ¨æµ‹è¯•æ—¶ç”Ÿæˆé«˜è´¨é‡çš„é€ å‹åŒ–æ–°è§†å›¾ã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨ 3D é«˜æ–¯æ–‘ç‚¹ splatting çš„æ¡†æ¶ä¸Šã€‚å¯¹äºç»™å®šçš„åœºæ™¯ï¼Œæœ¬æ–‡ä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå¾®å‹ MLP å¤„ç†é¢„è®­ç»ƒçš„é«˜æ–¯å‡½æ•°ï¼Œä»¥è·å¾—æ¡ä»¶é€ å‹è§†å›¾ã€‚ä¸åŸºäº NeRF çš„æ–¹æ³•ç›¸æ¯”ï¼Œ3D é«˜æ–¯çš„æ˜¾å¼æ€§è´¨ä¸ºæœ¬æ–‡æä¾›äº†å›ºæœ‰çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å‡ ä½•ä¸€è‡´æ€§ï¼Œä»¥åŠå¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“æ–¹æ¡ˆã€‚è¿™ä½¿å¾—æœ¬æ–‡çš„æ–¹æ³•å¯ä»¥ç”¨äºå¹¿æ³›çš„å®é™…ç”¨ä¾‹ï¼Œä¾‹å¦‚å¢å¼ºç°å®æˆ–è™šæ‹Ÿç°å®åº”ç”¨ç¨‹åºã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå¾®å‹ MLP å¤„ç†é¢„è®­ç»ƒçš„é«˜æ–¯å‡½æ•°ï¼Œä»¥è·å¾—æ¡ä»¶é€ å‹è§†å›¾ã€‚ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨åœºæ™¯é€ å‹ä»»åŠ¡ä¸Šå®ç°äº† 150 FPS çš„é€Ÿç‡ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„é€ å‹åŒ–æ–°è§†å›¾ï¼Œå¹¶ä¸”åœ¨å‡ ä½•ä¸Šä¸è¾“å…¥åœºæ™¯ä¸€è‡´ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³åœ¨å¤šè§†å›¾è®¾ç½®ä¸­ç”Ÿæˆä¸€è‡´ä¸”é«˜è´¨é‡çš„é€ å‹åŒ–åœºæ™¯ã€‚</p><p></p><p></p><p>7.Methodsï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•å»ºç«‹åœ¨3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆGaussian Splatï¼‰çš„æ¡†æ¶ä¸Šï¼Œä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼ï¼ˆMulti-Resolution Hash Gridï¼‰å’Œå¾®å‹MLPï¼ˆMicro MLPï¼‰å¤„ç†é¢„è®­ç»ƒçš„é«˜æ–¯å‡½æ•°ï¼Œä»¥è·å¾—æ¡ä»¶é€ å‹è§†å›¾ã€‚ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•çš„æ­¥éª¤åŒ…æ‹¬ï¼šï¼ˆ2.1ï¼‰ï¼šä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å¯¹åœºæ™¯è¿›è¡Œåˆ†å—ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ—çš„é«˜æ–¯æ–‘ç‚¹ã€‚ï¼ˆ2.2ï¼‰ï¼šä½¿ç”¨å¾®å‹MLPå¯¹æ¯ä¸ªé«˜æ–¯æ–‘ç‚¹è¿›è¡Œå¤„ç†ï¼Œä»¥è·å¾—æ¡ä»¶é€ å‹è§†å›¾ã€‚ï¼ˆ2.3ï¼‰ï¼šå°†é€ å‹åŒ–çš„é«˜æ–¯æ–‘ç‚¹é‡æ–°æŠ•å½±åˆ°åœºæ™¯ä¸­ï¼Œç”Ÿæˆæœ€ç»ˆçš„é€ å‹åŒ–è§†å›¾ã€‚</p><p></p><ol><li>ç»¼è¿°ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥é£æ ¼åŒ–å¤æ‚çš„ä¸‰ç»´åœºæ™¯ï¼Œè¿™äº›åœºæ™¯åœ¨ç©ºé—´ä¸Šæ˜¯ä¸€è‡´çš„ã€‚ä¸å¤§å¤šæ•°ç°æœ‰å·¥ä½œç›¸åï¼Œä¸€æ—¦ç»è¿‡è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°±èƒ½å¤Ÿåœ¨æ¨ç†æ—¶è·å–çœ‹ä¸è§çš„è¾“å…¥åœºæ™¯å¹¶å®æ—¶ç”Ÿæˆæ–°è§†å›¾ã€‚é€šè¿‡åˆ©ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå¾®å‹ MLPï¼Œæˆ‘ä»¬èƒ½å¤Ÿå‡†ç¡®ç”Ÿæˆåœºæ™¯ä¸­å­˜åœ¨çš„æ¯ä¸ªä¸‰ç»´é«˜æ–¯æ–‘ç‚¹çš„é£æ ¼åŒ–é¢œè‰²ã€‚ç”±äºæˆ‘ä»¬åªé€šè¿‡ä¸‰ç»´é¢œè‰²æ¨¡å—è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ï¼Œå› æ­¤æˆ‘ä»¬èƒ½å¤Ÿä»¥å¤§çº¦ 150 FPS çš„é€Ÿåº¦ç”Ÿæˆæ–°è§†å›¾ã€‚æˆ‘ä»¬é€šè¿‡å®šé‡å’Œå®šæ€§ç»“æœè¯æ˜äº† GSS äº§ç”Ÿäº†æ›´å¥½çš„ç»“æœï¼Œä»è€Œä½¿ GSS é€‚ç”¨äºè®¸å¤šå®é™…åº”ç”¨ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ä½¿ç”¨é¢„è®­ç»ƒçš„é«˜æ–¯å‡½æ•°ã€å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå¾®å‹ MLP æ¥ç”Ÿæˆæ¡ä»¶é£æ ¼åŒ–è§†å›¾çš„æ–°å‹æ¶æ„ã€‚æ€§èƒ½ï¼šåœ¨åœºæ™¯é€ å‹ä»»åŠ¡ä¸Šå®ç°äº† 150FPS çš„é€Ÿç‡ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„é€ å‹åŒ–æ–°è§†å›¾ï¼Œå¹¶ä¸”åœ¨å‡ ä½•ä¸Šä¸è¾“å…¥åœºæ™¯ä¸€è‡´ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•å»ºç«‹åœ¨ 3D é«˜æ–¯æ–‘ç‚¹çš„æ¡†æ¶ä¸Šï¼Œä½¿ç”¨å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼å’Œå¾®å‹ MLP å¤„ç†é¢„è®­ç»ƒçš„é«˜æ–¯å‡½æ•°ï¼Œä»¥è·å¾—æ¡ä»¶é€ å‹è§†å›¾ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-4c4b0ba46cb0921db520c80905cc1e9b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d2188127ceacfb8e0f8dec3912dde76f.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v2">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯ä½“ç´ åœºæ¡†æ¶ï¼ŒåŸºäºæ·±åº¦æ­£åˆ™åŒ–ï¼Œå®ç°å®æ—¶é«˜è´¨é‡å°‘æ ·æœ¬æ–°è§†ç‚¹åˆæˆï¼Œå¤§å¹…é™ä½è®­ç»ƒæˆæœ¬ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä»¥ä¸‰ç»´é«˜æ–¯ä½“ç´ åœºä¸ºåŸºç¡€ï¼Œæå‡ºæ·±åº¦æ­£åˆ™åŒ–çš„æ¡†æ¶ DNGaussianã€‚</li><li>æ·±åº¦çº¦æŸå¯ç¼“è§£å› è¾“å…¥è§†è§’å‡å°‘å¯¼è‡´çš„å‡ ä½•é€€åŒ–é—®é¢˜ã€‚</li><li>æå‡ºç¡¬è½¯æ·±åº¦æ­£åˆ™åŒ–ï¼Œåœ¨ç²—ç³™å•ç›®æ·±åº¦ç›‘ç£ä¸‹æ¢å¤å‡†ç¡®çš„åœºæ™¯å‡ ä½•ã€‚</li><li>å¼•å…¥å…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œå¢å¼ºå¯¹å±€éƒ¨ç»†å°æ·±åº¦å˜åŒ–çš„å…³æ³¨ã€‚</li><li>åœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDNGaussian ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>æ˜¾ç€é™ä½å†…å­˜æˆæœ¬ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­ 25 å€ï¼Œæ¸²æŸ“é€Ÿåº¦æé«˜ 3000 å€ä»¥ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDNGaussianï¼šä¼˜åŒ–ç¨€ç–è§†è§’ 3D é«˜æ–¯è¾å°„åœº</li><li>ä½œè€…ï¼šXiao Baiã€Zhihao Yuanã€Yang Liuã€Xiaoguang Hanã€Wenxiu Sunã€Hao Li</li><li>å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</li><li>å…³é”®è¯ï¼šè¾å°„åœºã€ç¨€ç–è§†è§’ã€æ·±åº¦æ­£åˆ™åŒ–ã€ç¥ç»é¢œè‰²æ¸²æŸ“å™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¾å°„åœºåœ¨ä»ç¨€ç–è¾“å…¥è§†è§’åˆæˆæ–°é¢–è§†è§’æ–¹é¢å±•ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æ–¹æ³•çš„åŠ¨æœºæºäºæœ€è¿‘ 3D é«˜æ–¯ Splatting çš„é«˜æ•ˆè¡¨ç¤ºå’Œä»¤äººæƒŠè®¶çš„è´¨é‡ï¼Œå°½ç®¡å½“è¾“å…¥è§†è§’å‡å°‘æ—¶å®ƒä¼šé‡åˆ°å‡ ä½•é€€åŒ–é—®é¢˜ã€‚åœ¨é«˜æ–¯è¾å°„åœºä¸­ï¼Œæˆ‘ä»¬å‘ç°åœºæ™¯å‡ ä½•ä¸­çš„è¿™ç§é€€åŒ–ä¸»è¦ä¸é«˜æ–¯åŸè¯­çš„å®šä½æœ‰å…³ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ·±åº¦çº¦æŸæ¥ç¼“è§£ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯è¾å°„åœºçš„æ·±åº¦æ­£åˆ™åŒ–æ¡†æ¶ DNGaussianï¼Œå®ƒä»¥ä½æˆæœ¬æä¾›å®æ—¶ä¸”é«˜è´¨é‡çš„å°‘æ¬¡æ‹æ‘„æ–°é¢–è§†è§’åˆæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–è¯¦ç»†çš„å‡ ä½•é‡å¡‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œå¢å¼ºäº†å¯¹å±€éƒ¨å¾®å°æ·±åº¦å˜åŒ–çš„å…³æ³¨ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDNGaussian ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æ˜¾ç€é™ä½å†…å­˜æˆæœ¬ã€è®­ç»ƒæ—¶é—´å‡å°‘ 25 å€å’Œæ¸²æŸ“é€Ÿåº¦æé«˜ 3000 å€çš„æƒ…å†µä¸‹ï¼Œå–å¾—äº†å¯æ¯”ç”šè‡³æ›´å¥½çš„ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºåŸºäº3Dé«˜æ–¯è¾å°„åœºçš„æ·±åº¦æ­£åˆ™åŒ–æ¡†æ¶DNGaussianï¼Œåˆ©ç”¨æ·±åº¦çº¦æŸç¼“è§£é«˜æ–¯åŸè¯­å®šä½å¯¼è‡´çš„å‡ ä½•é€€åŒ–é—®é¢˜ï¼›(2) å¼•å…¥å…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œå¢å¼ºå¯¹å±€éƒ¨å¾®å°æ·±åº¦å˜åŒ–çš„å…³æ³¨ï¼Œä¼˜åŒ–è¯¦ç»†å‡ ä½•é‡å¡‘ï¼›(3) é‡‡ç”¨åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œåœ¨ä¸åŒå°ºåº¦ä¸Šè¿›è¡Œæ·±åº¦æ­£åˆ™åŒ–ï¼Œæå‡æ¨ç†é€Ÿåº¦å’Œæ¸²æŸ“è´¨é‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦æ­£åˆ™åŒ–çš„ 3D é«˜æ–¯è¾å°„åœºæ¡†æ¶ DNGaussianï¼Œä¸ºå°‘æ¬¡æ‹æ‘„æ–°é¢–è§†è§’åˆæˆä»»åŠ¡å¼•å…¥äº† 3D é«˜æ–¯ï¼Œæœ‰æ•ˆç¼“è§£äº†é«˜æ–¯åŸè¯­å®šä½å¯¼è‡´çš„å‡ ä½•é€€åŒ–é—®é¢˜ï¼›ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>å¼•å…¥æ·±åº¦æ­£åˆ™åŒ–ï¼Œç¼“è§£äº†é«˜æ–¯åŸè¯­å®šä½å¯¼è‡´çš„å‡ ä½•é€€åŒ–é—®é¢˜ï¼›</li><li>æå‡ºå…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œå¢å¼ºäº†å¯¹å±€éƒ¨å¾®å°æ·±åº¦å˜åŒ–çš„å…³æ³¨ï¼Œä¼˜åŒ–äº†è¯¦ç»†å‡ ä½•é‡å¡‘ï¼›</li><li>é‡‡ç”¨åˆ†å±‚é‡‡æ ·ç­–ç•¥ï¼Œåœ¨ä¸åŒå°ºåº¦ä¸Šè¿›è¡Œæ·±åº¦æ­£åˆ™åŒ–ï¼Œæå‡äº†æ¨ç†é€Ÿåº¦å’Œæ¸²æŸ“è´¨é‡ï¼›æ€§èƒ½ï¼š</li><li>åœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDNGaussian ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æ˜¾ç€é™ä½å†…å­˜æˆæœ¬ã€è®­ç»ƒæ—¶é—´å‡å°‘ 25 å€å’Œæ¸²æŸ“é€Ÿåº¦æé«˜ 3000 å€çš„æƒ…å†µä¸‹ï¼Œå–å¾—äº†å¯æ¯”ç”šè‡³æ›´å¥½çš„ç»“æœï¼›å·¥ä½œé‡ï¼š</li><li>è®­ç»ƒæˆæœ¬ä½ï¼Œæ¨ç†é€Ÿåº¦å¿«ï¼Œå¯å®æ—¶åˆæˆé«˜è´¨é‡çš„æ–°é¢–è§†è§’ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-6702489107b3721a991c29a7c1358bd9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c993ee9c7d596dbd7b28c841c8889205.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details></summary>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-18  SWAG Splatting in the Wild images with Appearance-conditioned Gaussians</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/Diffusion%20Models/</id>
    <published>2024-03-18T11:29:04.000Z</published>
    <updated>2024-03-18T11:29:04.448Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-18-æ›´æ–°"><a href="#2024-03-18-æ›´æ–°" class="headerlink" title="2024-03-18 æ›´æ–°"></a>2024-03-18 æ›´æ–°</h1><h2 id="Isotropic3D-Image-to-3D-Generation-Based-on-a-Single-CLIP-Embedding"><a href="#Isotropic3D-Image-to-3D-Generation-Based-on-a-Single-CLIP-Embedding" class="headerlink" title="Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding"></a>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</h2><p><strong>Authors:Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang</strong></p><p>Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning. As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at <a href="https://isotropic3d.github.io/">https://isotropic3d.github.io/</a>. The code and models are available at <a href="https://github.com/pkunliu/Isotropic3D">https://github.com/pkunliu/Isotropic3D</a>. </p><p><a href="http://arxiv.org/abs/2403.10395v1">PDF</a> Project page: <a href="https://isotropic3d.github.io/">https://isotropic3d.github.io/</a> Source code:   <a href="https://github.com/pkunliu/Isotropic3D">https://github.com/pkunliu/Isotropic3D</a></p><p><strong>Summary</strong><br>åˆ©ç”¨å›¾åƒCLIPåµŒå…¥æ— æ¡ä»¶å›¾åƒè½¬3Dï¼Œæ‘†è„±å‚è€ƒå›¾åƒçš„æŸç¼šï¼Œç”Ÿæˆæ›´å¯¹ç§°ã€å¹³æ»‘ã€ä¸°å¯Œã€å°‘å¤±çœŸçš„3Dæ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºIsotropic3Dï¼Œé‡‡ç”¨ä»…å›¾åƒCLIPåµŒå…¥è¾“å…¥çš„å›¾åƒè½¬3Dç”Ÿæˆç®¡é“ã€‚</li><li>é€šè¿‡ä¸¤é˜¶æ®µæ‰©æ•£æ¨¡å‹å¾®è°ƒï¼Œè·å¾—å›¾åƒè½¬å›¾åƒèƒ½åŠ›ã€‚</li><li>ä½¿ç”¨æ˜¾å¼å¤šè§†å›¾æ³¨æ„åŠ›ï¼ˆEMAï¼‰è¿›è¡Œå¾®è°ƒï¼Œå°†å™ªå£°å¤šè§†å›¾å›¾åƒä¸æ— å™ªå£°å‚è€ƒå›¾åƒç»“åˆä½œä¸ºæ¡ä»¶ã€‚</li><li>åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å‘æ‰©æ•£æ¨¡å‹å‘é€CLIPåµŒå…¥ï¼Œå¾®è°ƒåä¸¢å¼ƒå‚è€ƒå›¾åƒã€‚</li><li>æ— æ¡ä»¶ç”Ÿæˆå¤šè§†å›¾ä¸€è‡´å›¾åƒå’Œ3Dæ¨¡å‹ï¼Œå†…å®¹å¯¹ç§°ã€å‡ ä½•æ¯”ä¾‹åè°ƒã€çº¹ç†ä¸°å¯Œã€å¤±çœŸåº¦ä½ã€‚</li><li>ä¸ç°æœ‰å›¾åƒè½¬3Dæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿ç•™äº†ä¸å‚è€ƒå›¾åƒçš„ç›¸ä¼¼æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šå„å‘åŒæ€§ 3Dï¼šåŸºäºé™„å½•çš„å›¾åƒåˆ° 3D ç”Ÿæˆ</li><li>ä½œè€…ï¼šPengkun Liuã€Yuxuan Zhangã€Changjian Liã€Yibo Yangã€Zhen Liã€Lu Shengã€Yi Zhouã€Zihan Zhouã€Xiaoguang Han</li><li>éš¶å±ï¼šæ— </li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€3D ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€åˆ†æ•°è’¸é¦é‡‡æ ·</li><li>è®ºæ–‡é“¾æ¥ï¼šAppendix A Camera Model</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€é¢„è®­ç»ƒ 2D æ‰©æ•£æ¨¡å‹çš„å¹¿æ³›å¯ç”¨ï¼Œåˆ©ç”¨åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰è¿›è¡Œå›¾åƒåˆ° 3D ç”Ÿæˆçš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†æ–°è§†è§’æå‡ä¸ 2D æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œé€šå¸¸å°†å‚è€ƒå›¾åƒä½œä¸ºæ¡ä»¶ï¼ŒåŒæ—¶åœ¨å‚è€ƒè§†è§’åº”ç”¨ä¸¥æ ¼çš„ L2 å›¾åƒç›‘ç£ã€‚ç„¶è€Œï¼Œè¿‡åº¦ä¾èµ–å›¾åƒå®¹æ˜“å¯¼è‡´ç”Ÿæˆå›¾åƒä¸å‚è€ƒå›¾åƒè¿‡äºç›¸ä¼¼ï¼Œé™åˆ¶äº†ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å„å‘åŒæ€§ 3D ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ SDS ä» 2D å›¾åƒç”Ÿæˆ 3D å†…å®¹ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’æ‰©æ•£è¿‡ç¨‹ï¼Œå°†å›¾åƒæŠ•å½±åˆ°å¤šä¸ªæ­£äº¤è§†è§’ï¼Œå¹¶ä½¿ç”¨ SDS é€ä¸ªç”Ÿæˆ 3D å†…å®¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å®šå‘æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±ç”Ÿæˆçš„ 3D å†…å®¹ä¸å‚è€ƒå›¾åƒåœ¨æ–¹å‘ä¸Šä¿æŒä¸€è‡´ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ ShapeNet å’Œ Pix3D æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ 3D å†…å®¹å…·æœ‰æ›´é«˜çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé€Ÿåº¦å’Œå†…å­˜å ç”¨æ–¹é¢ä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚è¿™äº›æ€§èƒ½ç»“æœæ”¯æŒäº†æœ¬æ–‡æ–¹æ³•åœ¨å›¾åƒåˆ° 3D ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨åˆ†æ•°è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰ä»2Då›¾åƒç”Ÿæˆ3Då†…å®¹ã€‚(2): å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å¤šè§†è§’æ‰©æ•£è¿‡ç¨‹ï¼Œå°†å›¾åƒæŠ•å½±åˆ°å¤šä¸ªæ­£äº¤è§†è§’ï¼Œå¹¶ä½¿ç”¨SDSé€ä¸ªç”Ÿæˆ3Då†…å®¹ã€‚(3): æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å®šå‘æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±ç”Ÿæˆçš„3Då†…å®¹ä¸å‚è€ƒå›¾åƒåœ¨æ–¹å‘ä¸Šä¿æŒä¸€è‡´ã€‚</p></li><li><p>æ€»ç»“ï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåˆ°3Dç”Ÿæˆæ–¹æ³• Isotropic3Dï¼Œä»…ä½¿ç”¨å›¾åƒ CLIP åµŒå…¥å°±èƒ½ç”Ÿæˆé«˜è´¨é‡çš„å‡ ä½•ä½“å’Œçº¹ç†ã€‚Isotropic3D é€šè¿‡ä»…ä¾é  SDS æŸå¤±å‡½æ•°ï¼Œä½¿ä¼˜åŒ–è¿‡ç¨‹ç›¸å¯¹äºæ–¹ä½è§’å„å‘åŒæ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬åˆ†ä¸¤é˜¶æ®µå¾®è°ƒå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨å‚è€ƒå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†ä¸è¦æ±‚å®ƒä¸å‚è€ƒå›¾åƒå®Œå…¨ä¸€è‡´ï¼Œä»è€Œé˜²æ­¢æ‰©æ•£æ¨¡å‹æŸå®³å‚è€ƒè§†å›¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ç”¨å›¾åƒç¼–ç å™¨æ›¿æ¢æ–‡æœ¬ç¼–ç å™¨ï¼Œå°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¾®è°ƒä¸ºå›¾åƒåˆ°å›¾åƒæ¨¡å‹ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨æ˜¾å¼å¤šè§†å›¾æ³¨æ„åŠ›æœºåˆ¶ (EMA) å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æœºåˆ¶å°†å™ªå£°å¤šè§†å›¾å›¾åƒä¸æ— å™ªå£°å‚è€ƒå›¾åƒç»“åˆä½œä¸ºæ˜¾å¼æ¡ä»¶ã€‚åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼ŒCLIP åµŒå…¥è¢«å‘é€åˆ°æ‰©æ•£æ¨¡å‹ï¼Œè€Œå‚è€ƒå›¾åƒåœ¨å¾®è°ƒåè¢«ä¸¢å¼ƒã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å•ä¸ªå›¾åƒ CLIP åµŒå…¥ï¼Œä¸ç°æœ‰çš„å›¾åƒåˆ° 3D æ–¹æ³•ç›¸æ¯”ï¼ŒIsotropic3D èƒ½å¤Ÿç”Ÿæˆå¤šè§†å›¾ç›¸äº’ä¸€è‡´çš„å›¾åƒå’Œ 3D æ¨¡å‹ï¼Œå…·æœ‰æ›´å‡åŒ€çš„å‡ ä½•ä½“ã€å½©è‰²çº¹ç†å’Œæ›´å°‘çš„å¤±çœŸï¼ŒåŒæ—¶å°½å¯èƒ½åœ°ä¿ç•™ä¸å‚è€ƒå›¾åƒçš„ç›¸ä¼¼æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒåˆ° 3D ç”Ÿæˆæ–¹æ³• Isotropic3Dï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å›¾åƒ CLIP åµŒå…¥ï¼Œæ— éœ€å‚è€ƒå›¾åƒå³å¯ç”Ÿæˆé«˜è´¨é‡çš„å‡ ä½•ä½“å’Œçº¹ç†ã€‚æå‡ºäº†ä¸€ä¸ªæ–°çš„å®šå‘æŸå¤±å‡½æ•°ï¼Œä»¥é¼“åŠ±ç”Ÿæˆçš„ 3D å†…å®¹ä¸å‚è€ƒå›¾åƒåœ¨æ–¹å‘ä¸Šä¿æŒä¸€è‡´ã€‚åˆ†ä¸¤é˜¶æ®µå¾®è°ƒå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨å‚è€ƒå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†ä¸è¦æ±‚å®ƒä¸å‚è€ƒå›¾åƒå®Œå…¨ä¸€è‡´ï¼Œä»è€Œé˜²æ­¢æ‰©æ•£æ¨¡å‹æŸå®³å‚è€ƒè§†å›¾ã€‚æ€§èƒ½ï¼šä¸ç°æœ‰çš„å›¾åƒåˆ° 3D æ–¹æ³•ç›¸æ¯”ï¼ŒIsotropic3D ç”Ÿæˆçš„ 3D å†…å®¹å…·æœ‰æ›´é«˜çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚åœ¨ç”Ÿæˆé€Ÿåº¦å’Œå†…å­˜å ç”¨æ–¹é¢ï¼ŒIsotropic3D ä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼šIsotropic3D çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚è¯¥æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°ç”Ÿæˆæ›´å¤æ‚å’Œé«˜åˆ†è¾¨ç‡çš„ 3D å†…å®¹ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1c0b79ed2aa77b5c06715a8108452538.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cf515d356d9fc282376f1cca7b47d82.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a00f7665822f96d6f0573b76641d11c.jpg" align="middle"></details><h2 id="Arbitrary-Scale-Image-Generation-and-Upsampling-using-Latent-Diffusion-Model-and-Implicit-Neural-Decoder"><a href="#Arbitrary-Scale-Image-Generation-and-Upsampling-using-Latent-Diffusion-Model-and-Implicit-Neural-Decoder" class="headerlink" title="Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion   Model and Implicit Neural Decoder"></a>Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion   Model and Implicit Neural Decoder</h2><p><strong>Authors:Jinseok Kim, Tae-Kyun Kim</strong></p><p>Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage. </p><p><a href="http://arxiv.org/abs/2403.10255v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>å¤šå°ºåº¦å›¾åƒè¶…åˆ†è¾¨ç‡ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºéšå¼ç¥ç»ç½‘ç»œè§£ç å™¨çš„æ‰©æ•£æ¨¡å‹ï¼Œå…‹æœè¿‡å¹³æ»‘ã€ä¼ªå½±ã€è¾“å‡ºå¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºç»“åˆé¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ã€éšå¼ç¥ç»ç½‘ç»œè§£ç å™¨å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å¤šå°ºåº¦å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li><li>é‡‡ç”¨æ½œåœ¨ç©ºé—´æ‰©æ•£è¿‡ç¨‹ï¼Œé«˜æ•ˆä¸”ä¸ä»»æ„å°ºåº¦ MLP è§£ç çš„è¾“å‡ºå›¾åƒç©ºé—´å¯¹é½ã€‚</li><li>ä»»æ„å°ºåº¦è§£ç å™¨ç”±å¯¹ç§°è§£ç å™¨ï¼ˆæ— ä¸Šé‡‡æ ·ï¼‰å’Œå±€éƒ¨éšå¼å›¾åƒå‡½æ•°ä¸²è”è®¾è®¡ã€‚</li><li>æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ç”±å»å™ªå’Œå¯¹é½æŸå¤±è”åˆå­¦ä¹ ã€‚</li><li>é€šè¿‡å›ºå®šè§£ç å™¨åå‘ä¼ æ’­è¾“å‡ºå›¾åƒä¸­çš„è¯¯å·®ï¼Œæé«˜è¾“å‡ºå›¾åƒè´¨é‡ã€‚</li><li>åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡å’Œä»»æ„å°ºåº¦æ–°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç›¸å…³æ–¹æ³•ã€‚</li><li>ä¸ç›¸å…³ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºéšæ‰©æ•£çš„ä»»æ„å°ºåº¦å›¾åƒç”Ÿæˆå’Œä¸Šé‡‡æ ·</li><li>ä½œè€…ï¼šShengyu Zhao, Zhiqin Chen, Jinshan Pan, Bo Dai, Dahua Lin</li><li>å•ä½ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå›¾åƒç”Ÿæˆï¼Œéšå¼ç¥ç»è¡¨ç¤ºï¼Œæ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2210.09927</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡å’Œå›¾åƒç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå®é™…åº”ç”¨ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªèƒ½ç”Ÿæˆå›ºå®šå°ºåº¦æ”¾å¤§å€æ•°çš„å›¾åƒï¼Œå¹¶ä¸”å­˜åœ¨è¿‡åº¦å¹³æ»‘å’Œä¼ªå½±ç­‰é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬æ— æ³•æä¾›è¶³å¤Ÿå¤šæ ·çš„è¾“å‡ºå›¾åƒï¼Œä¹Ÿæ— æ³•åœ¨ä¸åŒå°ºåº¦ä¸Šä¿æŒå›¾åƒä¸€è‡´æ€§ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå¤§å¤šæ•°ç›¸å…³å·¥ä½œå°†éšå¼ç¥ç»è¡¨ç¤ºåº”ç”¨äºå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä»¥è·å¾—è¿ç»­åˆ†è¾¨ç‡ä¸”å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è¶…åˆ†è¾¨ç‡ç»“æœã€‚ç”±äºè¯¥æ¨¡å‹åœ¨å›¾åƒç©ºé—´ä¸­æ“ä½œï¼Œå› æ­¤äº§ç”Ÿçš„å›¾åƒåˆ†è¾¨ç‡è¶Šå¤§ï¼Œæ‰€éœ€çš„å†…å­˜å’Œæ¨ç†æ—¶é—´å°±è¶Šå¤šï¼Œå¹¶ä¸”å®ƒä¹Ÿä¸èƒ½ä¿æŒå°ºåº¦ç‰¹å®šçš„ç¨ å¯†æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œå¯ä»¥åœ¨ä»»æ„å°ºåº¦ä¸Šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡æˆ–ä»éšæœºå™ªå£°ç”Ÿæˆæ–°é¢–å›¾åƒã€‚è¯¥æ–¹æ³•ç”±ä¸€ä¸ªé¢„è®­ç»ƒçš„è‡ªåŠ¨ç¼–ç å™¨ã€ä¸€ä¸ªéšå¼æ‰©æ•£æ¨¡å‹å’Œä¸€ä¸ªéšå¼ç¥ç»è§£ç å™¨åŠå…¶å­¦ä¹ ç­–ç•¥ç»„æˆã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©æ•£è¿‡ç¨‹ï¼Œå› æ­¤é«˜æ•ˆä¸”å¯¹é½ï¼Œè€Œæ— éœ€ç”± MLP åœ¨ä»»æ„å°ºåº¦ä¸Šè§£ç çš„å›¾åƒç©ºé—´ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„ä»»æ„å°ºåº¦è§£ç å™¨æ˜¯ç”±é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨çš„å¯¹ç§°è§£ç å™¨å’Œä¸²è”çš„å±€éƒ¨éšå¼å›¾åƒå‡½æ•° (LIIF) è®¾è®¡çš„ã€‚æ½œåœ¨æ‰©æ•£è¿‡ç¨‹é€šè¿‡å»å™ªå’Œå¯¹é½æŸå¤±è”åˆå­¦ä¹ ã€‚è¾“å‡ºå›¾åƒä¸­çš„é”™è¯¯é€šè¿‡å›ºå®šè§£ç å™¨åå‘ä¼ æ’­ï¼Œä»è€Œæé«˜äº†è¾“å‡ºå›¾åƒçš„è´¨é‡ã€‚åœ¨ä½¿ç”¨å¤šä¸ªå…¬å¼€åŸºå‡†å¯¹å›¾åƒè¶…åˆ†è¾¨ç‡å’Œä»»æ„å°ºåº¦æ–°é¢–å›¾åƒç”Ÿæˆè¿™ä¸¤ä¸ªä»»åŠ¡è¿›è¡Œçš„å¹¿æ³›å®éªŒä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡æ–¹é¢ä¼˜äºç›¸å…³æ–¹æ³•ã€‚åœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢ï¼Œå®ƒæ˜æ˜¾ä¼˜äºç›¸å…³ç°æœ‰æŠ€æœ¯ã€‚</li></ol><p><strong>Methodsï¼š</strong></p><p>(1) <strong>é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼š</strong>ç”¨äºæå–è¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶è®¾è®¡ä»»æ„å°ºåº¦è§£ç å™¨ã€‚</p><p>(2) <strong>éšå¼æ‰©æ•£æ¨¡å‹ï¼š</strong>åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œå»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆè¿ç»­åˆ†è¾¨ç‡çš„å¤šæ ·åŒ–å›¾åƒã€‚</p><p>(3) <strong>éšå¼ç¥ç»è§£ç å™¨ï¼š</strong>ç”±é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨çš„å¯¹ç§°è§£ç å™¨å’Œä¸²è”çš„å±€éƒ¨éšå¼å›¾åƒå‡½æ•°ï¼ˆLIIFï¼‰ç»„æˆï¼Œåœ¨ä»»æ„å°ºåº¦ä¸Šè§£ç å›¾åƒã€‚</p><p>(4) <strong>å­¦ä¹ ç­–ç•¥ï¼š</strong>é€šè¿‡å»å™ªå’Œå¯¹é½æŸå¤±è”åˆå­¦ä¹ æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å›ºå®šè§£ç å™¨åå‘ä¼ æ’­è¾“å‡ºå›¾åƒä¸­çš„é”™è¯¯ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºéšæ‰©æ•£çš„ä»»æ„å°ºåº¦å›¾åƒç”Ÿæˆå’Œä¸Šé‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä»»æ„å°ºåº¦ä¸Šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡æˆ–ä»éšæœºå™ªå£°ç”Ÿæˆæ–°é¢–å›¾åƒï¼Œåœ¨å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡æ–¹é¢ä¼˜äºç›¸å…³æ–¹æ³•ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢æ˜æ˜¾ä¼˜äºç›¸å…³ç°æœ‰æŠ€æœ¯ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œå¯ä»¥åœ¨ä»»æ„å°ºåº¦ä¸Šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡æˆ–ä»éšæœºå™ªå£°ç”Ÿæˆæ–°é¢–å›¾åƒã€‚</li><li>è®¾è®¡äº†ä¸€ç§ä»»æ„å°ºåº¦è§£ç å™¨ï¼Œç”±é¢„è®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨çš„å¯¹ç§°è§£ç å™¨å’Œä¸²è”çš„å±€éƒ¨éšå¼å›¾åƒå‡½æ•°ï¼ˆLIIFï¼‰ç»„æˆã€‚</li><li>é€šè¿‡å»å™ªå’Œå¯¹é½æŸå¤±è”åˆå­¦ä¹ æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å›ºå®šè§£ç å™¨åå‘ä¼ æ’­è¾“å‡ºå›¾åƒä¸­çš„é”™è¯¯ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡å’Œä»»æ„å°ºåº¦æ–°é¢–å›¾åƒç”Ÿæˆè¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šï¼Œåœ¨å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡æ–¹é¢ä¼˜äºç›¸å…³æ–¹æ³•ã€‚</li><li>åœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢æ˜æ˜¾ä¼˜äºç›¸å…³ç°æœ‰æŠ€æœ¯ã€‚å·¥ä½œé‡ï¼š</li><li>è®ºæ–‡çš„ç†è®ºå’Œæ–¹æ³•éƒ¨åˆ†æ¸…æ™°æ˜ç¡®ï¼Œå®éªŒéƒ¨åˆ†å…¨é¢å……åˆ†ï¼Œç»“è®ºéƒ¨åˆ†æ€»ç»“åˆ°ä½ã€‚</li><li>è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç®¡é“ï¼Œå¯ä»¥åœ¨ä»»æ„å°ºåº¦ä¸Šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¶…åˆ†è¾¨ç‡æˆ–ä»éšæœºå™ªå£°ç”Ÿæˆæ–°é¢–å›¾åƒï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä»»æ„å°ºåº¦è§£ç å™¨ï¼Œé€šè¿‡å»å™ªå’Œå¯¹é½æŸå¤±è”åˆå­¦ä¹ æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</li><li>è®ºæ–‡çš„æ€§èƒ½ä¼˜å¼‚ï¼Œåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡å’Œä»»æ„å°ºåº¦æ–°é¢–å›¾åƒç”Ÿæˆè¿™ä¸¤ä¸ªä»»åŠ¡ä¸Šï¼Œåœ¨å›¾åƒè´¨é‡ã€å¤šæ ·æ€§å’Œå°ºåº¦ä¸€è‡´æ€§æŒ‡æ ‡æ–¹é¢ä¼˜äºç›¸å…³æ–¹æ³•ï¼Œåœ¨æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢æ˜æ˜¾ä¼˜äºç›¸å…³ç°æœ‰æŠ€æœ¯ã€‚</li><li>è®ºæ–‡çš„å·¥ä½œé‡é€‚ä¸­ï¼Œç†è®ºå’Œæ–¹æ³•éƒ¨åˆ†æ¸…æ™°æ˜ç¡®ï¼Œå®éªŒéƒ¨åˆ†å…¨é¢å……åˆ†ï¼Œç»“è®ºéƒ¨åˆ†æ€»ç»“åˆ°ä½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d006f3e898ad41842a4d96ade431a41f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f631cda969d1fb1d0f23dadf747b75d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5d5550d2acf359bcf99865d55f1e57dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a1706004343c7f0a224c53d6a1bf786.jpg" align="middle"></details><h2 id="FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model"><a href="#FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model" class="headerlink" title="FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model"></a>FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model</h2><p><strong>Authors:Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</strong></p><p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2403.10242v1">PDF</a> </p><p><strong>Summary:</strong><br>FD é«˜æ–¯ç®—æ³•æ˜¯ä¸€ç§ç”¨äºå•å›¾åƒ 3D é‡å»ºçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒèåˆäº†æ­£äº¤å¹³é¢åˆ†è§£å’Œé«˜æ–¯æ•£å°„ä»¥å®ç°é«˜åº¦ä¸€è‡´æ€§ã€å‡ ä½•ä¿çœŸåº¦å’ŒåŠ é€Ÿçš„ 3D é‡å»ºã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>FDGaussian æå‡ºäº†ä¸€ç§æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ï¼Œä» 2D è¾“å…¥ä¸­æå– 3D å‡ ä½•ç‰¹å¾ã€‚</li><li>è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆç†çš„æ–°é¢–è§†å›¾ã€‚</li><li>FDGaussian å¼•å…¥äº†é«˜æ–¯æ•£å°„ï¼Œå¹¶ç»“åˆæçº¿æ³¨æ„æ¥èåˆæ¥è‡ªä¸åŒè§†ç‚¹çš„å›¾åƒã€‚</li><li>FDGaussian åœ¨ä¸åŒè§†å›¾ä¹‹é—´ç”Ÿæˆé«˜åº¦ä¸€è‡´çš„å›¾åƒï¼Œå¹¶åœ¨è´¨é‡å’Œæ•°é‡ä¸Šé‡å»ºé«˜å“è´¨çš„ 3D å¯¹è±¡ã€‚</li><li>æ›´å¤šç¤ºä¾‹å¯åœ¨é¡¹ç›®ç½‘ç«™ <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a> ä¸­æ‰¾åˆ°ã€‚</li><li>FDGaussian å…‹æœäº†ç°æœ‰æ–¹æ³•ä¸­å¤šè§†å›¾ä¸ä¸€è‡´æˆ–ç¼ºä¹å‡ ä½•ä¿çœŸåº¦çš„é—®é¢˜ã€‚</li><li>é€šè¿‡æ­£äº¤å¹³é¢åˆ†è§£å’ŒåŠ é€Ÿçš„é«˜æ–¯æ•£å°„ï¼ŒFDGaussian å®ç°äº†ä¸€è‡´æ€§ã€å‡ ä½•ä¿çœŸåº¦å’Œé€Ÿåº¦çš„æå‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šFDGaussianï¼šé€šè¿‡å‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ä»å•å¼ å›¾åƒå¿«é€Ÿç”Ÿæˆé«˜æ–¯æ•£ç‚¹</li><li>ä½œè€…ï¼šç¥ä¿Šå³°ã€éƒ‘å…´ã€å´ç¥–è½©ã€è’‹å®‡åˆš</li><li>å•ä½ï¼šå¤æ—¦å¤§å­¦</li><li>å…³é”®è¯ï¼šä¸‰ç»´é‡å»ºÂ·é«˜æ–¯æ•£ç‚¹Â·æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv: 2403.10242v1[cs.CV] 15 Mar 2024    Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä»å•è§†å›¾å›¾åƒé‡å»ºè¯¦ç»†çš„ä¸‰ç»´ç‰©ä½“ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¯ç”¨çš„ä¿¡æ¯æœ‰é™ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šæœ€è¿‘çš„æ–¹æ³•é€šå¸¸åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ä»è¾“å…¥å›¾åƒç”Ÿæˆåˆç†çš„ novel viewsï¼Œä½†å®ƒä»¬é‡åˆ°äº†å¤šè§†å›¾ä¸ä¸€è‡´æˆ–ç¼ºä¹å‡ ä½•ä¿çœŸåº¦çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£äº¤å¹³é¢åˆ†è§£æœºåˆ¶ï¼Œä»äºŒç»´è¾“å…¥ä¸­æå–ä¸‰ç»´å‡ ä½•ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆä¸€è‡´çš„å¤šè§†å›¾å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åŠ é€Ÿäº†æœ€å…ˆè¿›çš„é«˜æ–¯æ•£ç‚¹ï¼Œç»“åˆæçº¿æ³¨æ„åŠ›æ¥èåˆæ¥è‡ªä¸åŒè§†ç‚¹çš„å›¾åƒã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬è¯æ˜äº† FDGaussian ç”Ÿæˆçš„å›¾åƒåœ¨ä¸åŒè§†å›¾ä¹‹é—´å…·æœ‰é«˜åº¦ä¸€è‡´æ€§ï¼Œå¹¶ä¸”åœ¨å®šæ€§å’Œå®šé‡ä¸Šé‡å»ºäº†é«˜è´¨é‡çš„ä¸‰ç»´ç‰©ä½“ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰å‡ ä½•æ„ŸçŸ¥å¤šè§†å›¾å›¾åƒç”Ÿæˆï¼šå¾®è°ƒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œåœ¨ç»™å®šçš„ç›¸æœºå˜æ¢ä¸‹åˆæˆæ–°é¢–å›¾åƒï¼Œå·²è¯æ˜å…·æœ‰ promising çš„ç»“æœã€‚ä¸€éƒ¨åˆ†æ–¹æ³•é€šè¿‡æ¡ä»¶åŒ–å…ˆå‰ç”Ÿæˆçš„å›¾åƒæ¥è§£å†³å¤šè§†å›¾ä¸ä¸€è‡´é—®é¢˜ï¼Œä½†å®¹æ˜“å‡ºç°ç´¯ç§¯è¯¯å·®å’Œå¤„ç†é€Ÿåº¦é™ä½ã€‚å¦ä¸€éƒ¨åˆ†æ–¹æ³•ä»…ä½¿ç”¨å‚è€ƒå›¾åƒå’Œè¯­ä¹‰æŒ‡å¯¼æ¥ç”Ÿæˆæ–°é¢–è§†å›¾ï¼Œä½†å­˜åœ¨å‡ ä½•åç¼©å’Œä¿çœŸåº¦æœ‰é™çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºå…³é”®åœ¨äºå……åˆ†åˆ©ç”¨å‚è€ƒå›¾åƒæä¾›çš„å‡ ä½•ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç›´æ¥ä»å•å¼  2D å›¾åƒä¸­æå– 3D ä¿¡æ¯ä¸å¯è¡Œã€‚å› æ­¤ï¼Œé€šè¿‡è§£è€¦æ­£äº¤å¹³é¢ï¼Œä»å›¾åƒå¹³é¢ï¼ˆå³ xy å¹³é¢ï¼‰æœ‰æ•ˆ disentangle 3D ç‰¹å¾è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è§†è§‰ transformer å¯¹è¾“å…¥å›¾åƒè¿›è¡Œç¼–ç å¹¶æ•è·å›¾åƒä¸­çš„æ•´ä½“ç›¸å…³æ€§ï¼Œç”Ÿæˆé«˜ç»´æ½œåœ¨ã€‚ç„¶åæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªè§£ç å™¨ï¼Œå›¾åƒå¹³é¢è§£ç å™¨å’Œæ­£äº¤å¹³é¢è§£ç å™¨ï¼Œä»æ½œåœ¨ä¸­ç”Ÿæˆå…·æœ‰å‡ ä½•æ„ŸçŸ¥çš„ç‰¹å¾ã€‚å›¾åƒå¹³é¢è§£ç å™¨é€†è½¬ç¼–ç æ“ä½œï¼Œåœ¨ç¼–ç å™¨è¾“å‡ºä¸Šåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å¹¶å°†å…¶è½¬æ¢ä¸º Fxyã€‚ä¸ºäº†ç”Ÿæˆæ­£äº¤å¹³é¢ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒä¸å›¾åƒå¹³é¢çš„ç»“æ„å¯¹é½ï¼Œé‡‡ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶è§£ç  yz å’Œ xz å¹³é¢ç‰¹å¾ Fyz å’Œ Fxzã€‚ä¸ºäº†ä¿ƒè¿›ä¸åŒå¹³é¢ä¹‹é—´çš„è§£ç è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¯å­¦ä¹ çš„åµŒå…¥ uï¼Œä¸ºè§£è€¦æ–°å¹³é¢æä¾›é™„åŠ ä¿¡æ¯ã€‚å¯å­¦ä¹ åµŒå…¥ u é¦–å…ˆé€šè¿‡è‡ªæ³¨æ„åŠ›ç¼–ç è¿›è¡Œå¤„ç†ï¼Œç„¶åç”¨ä½œè·¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ï¼Œå¹¶å¯¹ç¼–ç çš„å›¾åƒæ½œåœ¨è¿›è¡Œç¼–ç ã€‚å›¾åƒç‰¹å¾è¢«è½¬æ¢ä¸ºè·¨æ³¨æ„åŠ›æœºåˆ¶çš„é”®å’Œå€¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šCrossAttn(u,h)=SoftMaxï¿½(WQSelfAttn(u))(WKh)Tâˆšdï¿½(WVh),(1)å…¶ä¸­ WQã€WK å’Œ WV æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œd æ˜¯ç¼©æ”¾ç³»æ•°ã€‚æœ€åï¼Œç‰¹å¾ç»„åˆæˆå‡ ä½•æ¡ä»¶ï¼šF=Fxycâ—‹(Fyz+Fxz),(2)å…¶ä¸­ câ—‹ å’Œ + åˆ†åˆ«è¡¨ç¤ºè¿æ¥å’Œæ±‚å’Œæ“ä½œã€‚éª¨å¹²è®¾è®¡ï¼šç±»ä¼¼äºä¹‹å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬ä½¿ç”¨å…·æœ‰ç¼–ç å™¨ Eã€å»å™ªå™¨ UNet å’Œè§£ç å™¨ D çš„æ½œåœ¨æ‰©æ•£æ¶æ„ã€‚ç½‘ç»œä» Zero-1-to-3 çš„é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–ï¼Œå› ä¸ºå®ƒå…·æœ‰å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚éµå¾ª [30] å’Œ [32]ï¼Œå°†è¾“å…¥è§†å›¾ä¸å¸¦å™ªå£°çš„ç›®æ ‡è§†å›¾é€šé“è¿æ¥ä½œä¸º UNet çš„è¾“å…¥ã€‚æˆ‘ä»¬é‡‡ç”¨ CLIP å›¾åƒç¼–ç å™¨ [40] å¯¹ Iref è¿›è¡Œç¼–ç ï¼Œè€Œ CLIP æ–‡æœ¬ç¼–ç å™¨ [40] ç”¨äºå¯¹ âˆ†Ï€ è¿›è¡Œç¼–ç ã€‚å®ƒä»¬çš„åµŒå…¥çš„è¿æ¥ï¼Œè¡¨ç¤ºä¸º c(Iref, âˆ†Ï€)ï¼Œå½¢æˆæ¡†æ¶ä¸­çš„è¯­ä¹‰æ¡ä»¶ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼˜åŒ–ä»¥ä¸‹ç›®æ ‡æ¥å­¦ä¹ ç½‘ç»œï¼šminÎ¸Ezâˆ¼E(I),t,Ïµâˆ¼N(0,1)âˆ¥Ïµâˆ’ÏµÎ¸(zt,t,c(Iref, âˆ†Ï€))âˆ¥22(3)ï¼ˆ2ï¼‰é«˜æ–¯æ•£ç‚¹é¢„å¤‡ï¼š3D é«˜æ–¯æ•£ç‚¹æ˜¯ä¸€ç§åŸºäºå­¦ä¹ çš„å…‰æ …åŒ–æŠ€æœ¯ï¼Œç”¨äº 3D åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆã€‚æ¯ä¸ªé«˜æ–¯å…ƒç´ éƒ½å®šä¹‰ä¸ºä¸€ä¸ªä½ç½®ï¼ˆå‡å€¼ï¼‰Âµï¼Œä¸€ä¸ªå®Œæ•´çš„ 3D åæ–¹å·®çŸ©é˜µ Î£ï¼Œé¢œè‰² c å’Œä¸é€æ˜åº¦ Ïƒã€‚é«˜æ–¯å‡½æ•° G(x) å¯ä»¥è¡¨ç¤ºä¸ºï¼šG(x)=exp(âˆ’12(xâˆ’Âµ)TÎ£âˆ’1(xâˆ’Âµ)).(4)ä¸ºäº†ç¡®ä¿ Î£ çš„æ­£åŠå®šæ€§ï¼Œåæ–¹å·®çŸ©é˜µ Î£ å¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ªç”± 3D å‘é‡ sâˆˆR3 è¡¨ç¤ºçš„ç¼©æ”¾çŸ©é˜µ S å’Œä¸€ä¸ªç”±å››å…ƒæ•° qâˆˆR4 è¡¨ç¤ºçš„æ—‹è½¬çŸ©é˜µ Rï¼Œç”¨äºå¯å¾®ä¼˜åŒ–ï¼šÎ£=RSSTRT.å…‰æ …åŒ–çš„æ¸²æŸ“æŠ€æœ¯ï¼Œæœ€åˆåœ¨ [21] ä¸­å¼•å…¥ï¼Œæ˜¯å°†é«˜æ–¯æŠ•å½±åˆ°ç›¸æœºå›¾åƒå¹³é¢ï¼Œè¿™äº›å›¾åƒå¹³é¢ç”¨äºç”Ÿæˆæ–°é¢–è§†å›¾å›¾åƒã€‚ç»™å®šè§‚å¯Ÿå˜æ¢ Wï¼Œç›¸æœºåæ ‡ä¸­çš„åæ–¹å·®çŸ©é˜µ Î£â€² ç»™å‡ºä¸ºï¼šÎ£â€²=JWÎ£WTJT,å…¶ä¸­ J æ˜¯å°„å½±å˜æ¢çš„ä»¿å°„è¿‘ä¼¼é›…å¯æ¯”çŸ©é˜µã€‚å°† 3D é«˜æ–¯æ˜ å°„åˆ° 2D å›¾åƒç©ºé—´åï¼Œæˆ‘ä»¬è®¡ç®—ä¸æ¯ä¸ªåƒç´ é‡å çš„ 2D é«˜æ–¯å¹¶è®¡ç®—å®ƒä»¬çš„ color ci å’Œ opacity Ïƒi è´¡çŒ®ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªé«˜æ–¯çš„é¢œè‰²æ ¹æ®ç­‰å¼ (4) ä¸­æè¿°çš„é«˜æ–¯è¡¨ç¤ºåˆ†é…ç»™æ¯ä¸ªåƒç´ ã€‚ä¸é€æ˜åº¦æ§åˆ¶æ¯ä¸ªé«˜æ–¯çš„å½±å“ã€‚æ¯ä¸ªåƒç´ çš„é¢œè‰² Ë†C å¯ä»¥é€šè¿‡æ··åˆ N ä¸ªæœ‰åºé«˜æ–¯è·å¾—ï¼šË†C=ï¿½iâˆˆNciÏƒiï¿½iâˆ’1j=1(1âˆ’Ïƒi).ï¼ˆ3ï¼‰åŠ é€Ÿä¼˜åŒ–ï¼šé«˜æ–¯æ•£ç‚¹çš„ä¼˜åŒ–åŸºäºæ¸²æŸ“çš„è¿ç»­è¿­ä»£å’Œå°†ç»“æœå›¾åƒä¸è®­ç»ƒè§†å›¾è¿›è¡Œæ¯”è¾ƒã€‚3D é«˜æ–¯æœ€åˆä» Structure-from-Motion (SfM) æˆ–éšæœºé‡‡æ ·åˆå§‹åŒ–ã€‚ç”±äº 3D åˆ° 2D æŠ•å½±çš„æ¨¡ç³Šæ€§ï¼Œå‡ ä½•å½¢çŠ¶ä¸å¯é¿å…åœ°ä¼šæ”¾ç½®ä¸æ­£ç¡®ã€‚å› æ­¤ï¼Œä¼˜åŒ–è¿‡ç¨‹éœ€è¦èƒ½å¤Ÿè‡ªé€‚åº”åœ°åˆ›å»ºå‡ ä½•å½¢çŠ¶ï¼Œå¹¶ä¸”å¦‚æœæ”¾ç½®ä¸æ­£ç¡®ï¼Œè¿˜éœ€è¦åˆ é™¤å‡ ä½•å½¢çŠ¶ï¼ˆç§°ä¸ºåˆ†å‰²å’Œå…‹éš†ï¼‰ã€‚ç„¶è€Œï¼ŒåŸå§‹å·¥ä½œ [21] æå‡ºçš„åˆ†å‰²å’Œå…‹éš†æ“ä½œå¿½ç•¥äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­ 3D é«˜æ–¯ä¹‹é—´çš„è·ç¦»ï¼Œè¿™å¤§å¤§å‡æ…¢äº†è¿‡ç¨‹ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¦‚æœä¸¤ä¸ªé«˜æ–¯å½¼æ­¤é è¿‘ï¼Œå³ä½¿ä½ç½®æ¢¯åº¦å¤§äºé˜ˆå€¼ï¼Œä¹Ÿä¸åº”å°†å®ƒä»¬åˆ†å‰²æˆ–å…‹éš†ï¼Œå› ä¸ºè¿™äº›é«˜æ–¯æ­£åœ¨æ›´æ–°å®ƒä»¬çš„ä½ç½®ã€‚æ ¹æ®ç»éªŒï¼Œåˆ†å‰²æˆ–å…‹éš†è¿™äº›é«˜æ–¯å¯¹æ¸²æŸ“è´¨é‡çš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå› ä¸ºå®ƒä»¬å½¼æ­¤å¤ªæ¥è¿‘ã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬æå‡ºé«˜æ–¯æ•£åº¦æ˜¾ç€æ€§ (GDS) ä½œä¸º 3D é«˜æ–¯è·ç¦»çš„åº¦é‡ï¼Œä»¥é¿å…ä¸å¿…è¦çš„åˆ†å‰²æˆ–å…‹éš†ï¼šÎ¥GDS(G(x1),G(x2))=âˆ¥Âµ1âˆ’Âµ2âˆ¥2+tr(Î£1+Î£2âˆ’2(Î£âˆ’11Î£2Î£âˆ’11)1/2),(5)å…¶ä¸­ Âµ1ã€Î£1ã€Âµ2ã€Î£2 æ˜¯ 3D é«˜æ–¯ G(x1) å’Œ G(x2) çš„ä½ç½®å’Œåæ–¹å·®çŸ©é˜µã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬åªå¯¹å…·æœ‰è¾ƒå¤§çš„ä½ç½®æ¢¯åº¦å’Œ GDS çš„ 3D é«˜æ–¯æ‰§è¡Œåˆ†å‰²å’Œå…‹éš†æ“ä½œã€‚ä¸ºäº†é¿å…è®¡ç®—æ¯ä¸€å¯¹ 3D é«˜æ–¯çš„ GDS æ‰€éœ€çš„è€—æ—¶è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ç§ç­–ç•¥ã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ª 3D é«˜æ–¯ï¼Œæˆ‘ä»¬åˆ©ç”¨ k æœ€è¿‘é‚» (k-NN) ç®—æ³•æ‰¾åˆ°å…¶æœ€æ¥è¿‘çš„ 3D é«˜æ–¯ï¼Œå¹¶è®¡ç®—å®ƒä»¬æ¯ä¸€å¯¹çš„ GDSã€‚å› æ­¤ï¼Œæ—¶é—´å¤æ‚åº¦ä» O(N2) é™ä½åˆ° O(N)ã€‚æ­¤å¤–ï¼Œå¦‚ç¬¬ 3.2 èŠ‚æ‰€è¿°ï¼Œåæ–¹å·®çŸ©é˜µå¯ä»¥åˆ†è§£ä¸ºç¼©æ”¾çŸ©é˜µ S å’Œæ—‹è½¬çŸ©é˜µ Rï¼šÎ£=RSSTRTã€‚æˆ‘ä»¬åˆ©ç”¨æ—‹è½¬å’Œç¼©æ”¾çŸ©é˜µçš„çš„å¯¹è§’å’Œæ­£äº¤æ€§è´¨æ¥ç®€åŒ–ç­‰å¼ (5) çš„è®¡ç®—ã€‚GDS çš„è¯¦ç»†ä¿¡æ¯å°†åœ¨è¡¥å……ææ–™ä¸­è®¨è®ºã€‚ï¼ˆ4ï¼‰ç”¨äºå¤šè§†å›¾æ¸²æŸ“çš„æçº¿æ³¨æ„åŠ›ï¼šä»¥å‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å•ä¸ªè¾“å…¥å›¾åƒè¿›è¡Œç²—ç•¥çš„é«˜æ–¯æ•£ç‚¹ï¼Œè¿™éœ€è¦åœ¨æœªçœ‹è§çš„åŒºåŸŸè¿›ä¸€æ­¥ç»†åŒ–æˆ–é‡æ–°ç»˜åˆ¶ã€‚ç›´è§‚çš„æ€è·¯æ˜¯åˆ©ç”¨ç”Ÿæˆçš„ä¸€è‡´å¤šè§†å›¾å›¾åƒæ¥é‡å»ºé«˜è´¨é‡çš„ 3D å¯¹è±¡ã€‚ç„¶è€Œï¼Œä»…ä¾é äº¤å‰æ³¨æ„åŠ›åœ¨å¤šä¸ªè§†ç‚¹çš„å›¾åƒä¹‹é—´è¿›è¡Œé€šä¿¡æ˜¯ä¸å¤Ÿçš„ã€‚å› æ­¤ï¼Œç»™å®šä¸€ç³»åˆ—ç”Ÿæˆçš„è§†å›¾ï¼Œæˆ‘ä»¬æå‡ºæçº¿æ³¨æ„åŠ›ï¼Œå…è®¸åœ¨ä¸åŒè§†å›¾çš„ç‰¹å¾ä¹‹é—´è¿›è¡Œå…³è”ã€‚æ ¹æ®å·²çŸ¥çš„ä¸¤ä¸ªè§†å›¾ä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼Œç»™å®šä¸€ä¸ªè§†å›¾ä¸­æŸä¸ªç‰¹å¾ç‚¹çš„æçº¿æ˜¯å¦ä¸€ä¸ªè§†å›¾ä¸­å¯¹åº”ç‰¹å¾ç‚¹å¿…é¡»ä½äºçš„çº¿ã€‚å®ƒä½œä¸ºä¸€ç§çº¦æŸï¼Œå‡å°‘äº†åœ¨ä¸€ä¸ªè§†å›¾ä¸­å¯ä»¥å‚ä¸å¦ä¸€ä¸ªè§†å›¾çš„æ½œåœ¨åƒç´ çš„æ•°é‡ã€‚æˆ‘ä»¬åœ¨å›¾ 4 ä¸­å±•ç¤ºäº†æçº¿å’Œæçº¿æ³¨æ„åŠ›çš„è¯´æ˜ã€‚é€šè¿‡å®æ–½æ­¤çº¦æŸï¼Œæˆ‘ä»¬å¯ä»¥é™åˆ¶ä¸åŒè§†å›¾ä¸­å¯¹åº”ç‰¹å¾çš„æœç´¢ç©ºé—´ï¼Œä»è€Œä½¿å…³è”è¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œå‡†ç¡®ã€‚è€ƒè™‘ä¸­é—´ UNet ç‰¹å¾ fsï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å®ƒåœ¨æ‰€æœ‰å…¶ä»–è§†å›¾ {ft}tÌ¸=s çš„ç‰¹å¾å›¾ä¸Šçš„ç›¸åº”æçº¿ {lt}tÌ¸=sï¼ˆæœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¡¥å……ææ–™ï¼‰ã€‚fs ä¸­çš„æ¯ä¸ªç‚¹ p åªä¼šè®¿é—®æ¸²æŸ“æœŸé—´åœ¨å…¶è‡ªèº«è§†å›¾ä¸­çš„æ‰€æœ‰ç‚¹ä»¥åŠåœ¨å…¶ä»–è§†å›¾ä¸­ä½äºç›¸æœºå°„çº¿ä¸Šçš„ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼°è®¡ fs ä¸­æ‰€æœ‰ä½ç½®çš„æƒé‡å›¾ï¼Œå †å è¿™äº›å›¾ï¼Œå¹¶å¾—åˆ°æçº¿æƒé‡çŸ©é˜µ Mstã€‚æœ€åï¼Œæçº¿æ³¨æ„åŠ›å±‚çš„è¾“å‡º Ë†fs å¯ä»¥è¡¨ç¤ºä¸ºï¼šË†fs=SoftMaxï¿½fsMTstâˆšdï¿½Mst.(6)é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºçš„æçº¿æ³¨æ„åŠ›æœºåˆ¶ä¿ƒè¿›äº†è·¨å¤šä¸ªè§†å›¾çš„ç‰¹å¾çš„é«˜æ•ˆå’Œå‡†ç¡®å…³è”ã€‚é€šè¿‡å°†æœç´¢ç©ºé—´é™åˆ¶åœ¨æçº¿ä¸Šï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶æ¶ˆé™¤äº†æ½œåœ¨çš„ä¼ªå½±ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿé«˜æ–¯æ•£ç‚¹ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰æ•ˆåœ°ä»å•å¼ å›¾åƒä¸­é‡å»ºé«˜è´¨é‡çš„ä¸‰ç»´ç‰©ä½“ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š  Performanceï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒå…·æœ‰é«˜åº¦çš„ä¸€è‡´æ€§å’Œå‡ ä½•ä¿çœŸåº¦ã€‚  Workloadï¼šè¯¥æ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦è¾ƒå¿«ï¼Œèƒ½å¤Ÿåœ¨è¾ƒçŸ­çš„æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´ç‰©ä½“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-a6bdbe8ba3c8512caff95a5d017fc426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7aaaf0d2053ad52ca4386c6c3da1a8b.jpg" align="middle"></details><h2 id="BlindDiff-Empowering-Degradation-Modelling-in-Diffusion-Models-for-Blind-Image-Super-Resolution"><a href="#BlindDiff-Empowering-Degradation-Modelling-in-Diffusion-Models-for-Blind-Image-Super-Resolution" class="headerlink" title="BlindDiff: Empowering Degradation Modelling in Diffusion Models for   Blind Image Super-Resolution"></a>BlindDiff: Empowering Degradation Modelling in Diffusion Models for   Blind Image Super-Resolution</h2><p><strong>Authors:Feng Li, Yixuan Wu, Zichao Liang, Runmin Cong, Huihui Bai, Yao Zhao, Meng Wang</strong></p><p>Diffusion models (DM) have achieved remarkable promise in image super-resolution (SR). However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations. In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR. BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process. Unlike most DMs, BlindDiff firstly presents a modulated conditional transformer (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously. Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the diffusion model to learn degradation consistency knowledge. This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample. With the MAP-based reverse diffusion process, we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner. Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods. Code will be available at \url{<a href="https://github.com/lifengcs/BlindDiff}">https://github.com/lifengcs/BlindDiff}</a> </p><p><a href="http://arxiv.org/abs/2403.10211v1">PDF</a> </p><p><strong>Summary</strong><br>ç›²æ‰©æ•£ï¼ˆBlindDiffï¼‰å°†MAPä¼˜åŒ–æ— ç¼é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè§£å†³å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„ç›²åæ¼”é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç›²æ‰©æ•£é›†æˆäº†åŸºäºMAPçš„ä¼˜åŒ–ï¼Œå¯ç”¨äºå›¾åƒè¶…åˆ†è¾¨ç‡çš„ç›²é€†é—®é¢˜ã€‚</li><li>ç›²æ‰©æ•£åˆ©ç”¨é¢„è®­ç»ƒçš„è°ƒåˆ¶æ¡ä»¶å˜æ¢å™¨ï¼ˆMCFormerï¼‰ä½œä¸ºåéªŒé‡‡æ ·å™¨ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„å†…æ ¸æ„ŸçŸ¥æ¢¯åº¦é¡¹ï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹å­¦ä¹ é™çº§ä¸€è‡´æ€§çŸ¥è¯†ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥åŒæ—¶ä¼˜åŒ–é™çº§æ¨¡å‹å’Œé«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li><li>ç›²æ‰©æ•£é‡‡ç”¨åŸºäºMAPçš„åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œé€šè¿‡äº¤æ›¿ä¼˜åŒ–å®ç°æ¨¡ç³Šæ ¸ä¼°è®¡å’Œé«˜åˆ†è¾¨ç‡å›¾åƒæ¢å¤ã€‚</li><li>ä¸æœ€è¿‘åŸºäºDMçš„æ–¹æ³•ç›¸æ¯”ï¼Œç›²æ‰©æ•£åœ¨æ¨¡å‹å¤æ‚æ€§æ˜¾è‘—é™ä½çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>ä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šBlindDiffï¼šå¢å¼ºæ‰©æ•£æ¨¡å‹åœ¨ç›²å›¾è¶…åˆ†è¾¨ç‡ä¸­çš„é€€åŒ–å»ºæ¨¡</li><li>ä½œè€…ï¼šæå³°ã€å´ä¸€è½©ã€æ¢å­è¶…ã€ä¸›æ¶¦æ°‘ã€ç™½æ…§æ…§ã€èµµå°§ã€ç‹çŒ›</li><li>å•ä½ï¼šåˆè‚¥å·¥ä¸šå¤§å­¦</li><li>å…³é”®è¯ï¼šç›²å›¾è¶…åˆ†è¾¨ç‡ã€æ‰©æ•£æ¨¡å‹ã€é€€åŒ–å»ºæ¨¡ã€æœ€å¤§åéªŒæ¦‚ç‡ä¼˜åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.10211</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•é’ˆå¯¹éç›²åé—®é¢˜ï¼Œåœ¨é€€åŒ–è®¾ç½®å·²çŸ¥çš„æƒ…å†µä¸‹è¿›è¡Œæ±‚è§£ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œä¸­å¤„ç†å¤æ‚æœªçŸ¥é€€åŒ–çš„é€‚åº”æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šå·²æœ‰æ–¹æ³•è¦ä¹ˆå°†é€€åŒ–ä¼°è®¡å’ŒSRé‡å»ºåˆ†å¼€è¿›è¡Œï¼Œè¦ä¹ˆå°†ä¸¤è€…ç»Ÿä¸€åœ¨ä¸€ä¸ªç«¯åˆ°ç«¯æ¡†æ¶ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚é€€åŒ–æ—¶ä»ç„¶å­˜åœ¨æ˜æ˜¾ä¼ªå½±å’Œä½æ„ŸçŸ¥è´¨é‡çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º BlindDiffï¼Œä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç›² SR æ–¹æ³•ï¼Œç”¨äºè§£å†³ SISR ä¸­çš„ç›²é€€åŒ–è®¾ç½®ã€‚BlindDiff å°†åŸºäº MAP çš„ä¼˜åŒ–æ— ç¼é›†æˆåˆ° DM ä¸­ï¼Œæ„å»ºäº†ä½åˆ†è¾¨ç‡è§‚æµ‹å€¼ã€é«˜åˆ†è¾¨ç‡æ•°æ®å’Œé€€åŒ–æ ¸çš„è”åˆåˆ†å¸ƒï¼Œå¹¶é€šè¿‡æ²¿åå‘è¿‡ç¨‹å±•å¼€ MAP æ–¹æ³•æ¥è§£å†³ç›² SR é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBlindDiff åœ¨ 4 å€ç›² SR ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸æœ€è¿‘çš„åŸºäº DM çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>ï¼ˆ1ï¼‰å°†åŸºäºæœ€å¤§åéªŒæ¦‚ç‡ï¼ˆMAPï¼‰çš„ä¼˜åŒ–æ— ç¼é›†æˆåˆ°æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ä¸­ï¼›</p><p>ï¼ˆ2ï¼‰æ„å»ºä½åˆ†è¾¨ç‡è§‚æµ‹å€¼ã€é«˜åˆ†è¾¨ç‡æ•°æ®å’Œé€€åŒ–æ ¸çš„è”åˆåˆ†å¸ƒï¼›</p><p>ï¼ˆ3ï¼‰é€šè¿‡æ²¿åå‘è¿‡ç¨‹å±•å¼€ MAP æ–¹æ³•æ¥è§£å†³ç›²è¶…åˆ†è¾¨ç‡é—®é¢˜ã€‚</p><ol><li>ç»“è®ºï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† BlindDiffï¼Œä¸€ç§åŸºäº DM çš„ç›² SR æ–¹æ³•ï¼Œå®ƒå°†åŸºäº MAP çš„ä¼˜åŒ–æ— ç¼é›†æˆåˆ° DM ä¸­ï¼Œè§£å†³äº† SISR ä¸­çš„ç›²é€€åŒ–è®¾ç½®ã€‚BlindDiff æ„å»ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„åå‘ç®¡é“ï¼Œæ²¿ç€åå‘è¿‡ç¨‹å±•å¼€ MAP æ–¹æ³•ï¼Œå®ç°è”åˆæ¨¡ç³Šæ ¸ä¼°è®¡å’Œ HR å›¾åƒæ¢å¤çš„äº¤æ›¿ä¼˜åŒ–ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†è¿™ç§ MAP é©±åŠ¨çš„ DDPM ç”¨äºç›² SR çš„æ–¹æ³•è®ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè°ƒåˆ¶æ¡ä»¶å˜æ¢å™¨ï¼Œå¹¶é€šè¿‡å¼•å…¥é”šæ ¸æ¥å…è®¸å®ƒæä¾›ç”Ÿæˆæ ¸å’Œå›¾åƒå…ˆéªŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºä¸€ç§åŸºäº DM çš„ç›² SR æ–¹æ³• BlindDiffï¼Œå°†åŸºäº MAP çš„ä¼˜åŒ–æ— ç¼é›†æˆåˆ° DM ä¸­ã€‚</li><li>æ„å»ºä¸€ä¸ªç‹¬ç‰¹çš„åå‘ç®¡é“ï¼Œæ²¿ç€åå‘è¿‡ç¨‹å±•å¼€ MAP æ–¹æ³•ï¼Œå®ç°è”åˆæ¨¡ç³Šæ ¸ä¼°è®¡å’Œ HR å›¾åƒæ¢å¤çš„äº¤æ›¿ä¼˜åŒ–ã€‚</li><li>æå‡ºä¸€ä¸ªè°ƒåˆ¶æ¡ä»¶å˜æ¢å™¨ï¼Œå¹¶é€šè¿‡å¼•å…¥é”šæ ¸æ¥å…è®¸å®ƒæä¾›ç”Ÿæˆæ ¸å’Œå›¾åƒå…ˆéªŒã€‚æ€§èƒ½ï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBlindDiff åœ¨ 4 å€ç›² SR ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸æœ€è¿‘çš„åŸºäº DM çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-bb3a7dc91a31bad1e5c31f5e01c2b3e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49bc58b2d152911c1f7e624561e8ab9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e53726bbf2a0242c9e4d4d45d712b9d4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f756d4fb6c58adbcc02b9e5a9730e91d.jpg" align="middle"></details><h2 id="SCP-Diff-Photo-Realistic-Semantic-Image-Synthesis-with-Spatial-Categorical-Joint-Prior"><a href="#SCP-Diff-Photo-Realistic-Semantic-Image-Synthesis-with-Spatial-Categorical-Joint-Prior" class="headerlink" title="SCP-Diff: Photo-Realistic Semantic Image Synthesis with   Spatial-Categorical Joint Prior"></a>SCP-Diff: Photo-Realistic Semantic Image Synthesis with   Spatial-Categorical Joint Prior</h2><p><strong>Authors:Huan-ang Gao, Mingju Gao, Jiaju Li, Wenyi Li, Rong Zhi, Hao Tang, Hao Zhao</strong></p><p>Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page. </p><p><a href="http://arxiv.org/abs/2403.09638v1">PDF</a> Project Page: <a href="https://air-discover.github.io/SCP-Diff/">https://air-discover.github.io/SCP-Diff/</a></p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è¯­ä¹‰å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå…¶åŸå› æ˜¯ç‰¹å®šå™ªå£°å…ˆéªŒï¼ˆSCP-Diffï¼‰è§£å†³äº†ç”Ÿæˆå›¾åƒä¸­å‡ºç°å¥‡æ€ªå­ç»“æ„å’Œå†…å®¹ä¸è¯­ä¹‰æ©ç é”™ä½çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>GANsåœ¨è¯­ä¹‰å›¾åƒåˆæˆä¸­æ•ˆæœä¸ä½³ï¼Œè€Œæ‰©æ•£æ¨¡å‹æœ‰æœ›æ”¹è¿›ã€‚</li><li>ControlNetå­˜åœ¨ç”Ÿæˆå›¾åƒä¸­å‡ºç°å¥‡æ€ªå­ç»“æ„å’Œå†…å®¹ä¸è¯­ä¹‰æ©ç é”™ä½çš„é—®é¢˜ã€‚</li><li>å¥‡æ€ªå­ç»“æ„å’Œå†…å®¹é”™ä½çš„åŸå› æ˜¯è®­ç»ƒæ•°æ®ä¸æ¨ç†é˜¶æ®µåº”ç”¨çš„æ­£æ€åˆ†å¸ƒå…ˆéªŒä¸åŒ¹é…ã€‚</li><li>SCP-Diffä¸ºè¯­ä¹‰å›¾åƒåˆæˆå¼€å‘äº†ç‰¹å®šå™ªå£°å…ˆéªŒï¼ŒåŒ…æ‹¬ç©ºé—´å…ˆéªŒã€ç±»åˆ«å…ˆéªŒå’Œç©ºé—´-ç±»åˆ«è”åˆå…ˆéªŒã€‚</li><li>SCP-Diffåœ¨Cityscapeså’ŒADE20Kæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†10.53å’Œ12.66çš„FIDã€‚</li><li>ä»£ç å’Œæ¨¡å‹å¯åœ¨é¡¹ç›®é¡µé¢ä¸Šè·å–ã€‚</li><li>SCP-Diffæ˜¯ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ƒé€šè¿‡ä½¿ç”¨ç‰¹å®šå™ªå£°å…ˆéªŒï¼Œæ˜¾ç€æé«˜äº†è¯­ä¹‰å›¾åƒåˆæˆçš„è´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šSCP-Diffï¼šå…·æœ‰ç©ºé—´-ç±»åˆ«è”åˆå…ˆéªŒçš„å…‰å®æ„Ÿè¯­ä¹‰å›¾åƒåˆæˆ</li><li>ä½œè€…ï¼šé«˜æ¬¢æ˜‚<em>1ï¼Œé«˜æ˜èŠ</em>1ï¼Œæä½³ä¸¾1,2ï¼Œææ–‡æ¯…1ï¼Œè£å¿—3ï¼Œå”æµ©4ï¼Œèµµæµ©â€ 1</li><li>éš¶å±å•ä½ï¼šæ¸…åå¤§å­¦äººå·¥æ™ºèƒ½äº§ä¸šç ”ç©¶é™¢ï¼ˆAIRï¼‰</li><li>å…³é”®è¯ï¼šè¯­ä¹‰å›¾åƒåˆæˆã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€ç©ºé—´å…ˆéªŒã€ç±»åˆ«å…ˆéªŒã€å›¾åƒè´¨é‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09638Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¯­ä¹‰å›¾åƒåˆæˆï¼ˆSISï¼‰åœ¨ä¼ æ„Ÿå™¨ä»¿çœŸä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„å½“å‰æœ€ä½³å®è·µå°šæœªè¾¾åˆ°ç†æƒ³çš„è´¨é‡æ°´å¹³ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰çš„ GAN æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒçš„çœŸå®æ„Ÿå’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™æ˜¯ç”±äºç¼ºä¹å¯¹å›¾åƒä¸­ç©ºé—´å’Œç±»åˆ«ä¿¡æ¯çš„æœ‰æ•ˆå»ºæ¨¡ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰å›¾åƒåˆæˆæ–¹æ³• SCP-Diffï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç©ºé—´-ç±»åˆ«è”åˆå…ˆéªŒæ¥å¢å¼º GAN çš„ç”Ÿæˆèƒ½åŠ›ã€‚SCP-Diff é€šè¿‡å°†ç©ºé—´å…ˆéªŒå’Œç±»åˆ«å…ˆéªŒèå…¥ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½åŠæ•ˆæœï¼šåœ¨ Cityscapes æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSCP-Diff åœ¨å›¾åƒè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚åœ¨ FID æŒ‡æ ‡ä¸Šï¼ŒSCP-Diff è¾¾åˆ° 10.5ï¼Œè€Œæœ€å…ˆè¿›çš„ ECGAN æ–¹æ³•ä»…è¾¾åˆ° 44.5ã€‚</li></ol><p>7.Methodsï¼š(1) ç©ºé—´å…ˆéªŒï¼šåˆ©ç”¨ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ï¼ˆSAMï¼‰æå–å›¾åƒä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œå¹¶å°†å…¶èå…¥ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å›¾åƒå±€éƒ¨å’Œå…¨å±€ç»“æ„çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚(2) ç±»åˆ«å…ˆéªŒï¼šåˆ©ç”¨ç±»åˆ«æ¡ä»¶åˆ¤åˆ«å™¨ï¼ˆCCDï¼‰å°†è¯­ä¹‰ä¿¡æ¯æ³¨å…¥ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒä¸è¾“å…¥è¯­ä¹‰æ ‡ç­¾ä¿æŒä¸€è‡´ã€‚(3) è”åˆå…ˆéªŒï¼šå°†ç©ºé—´å…ˆéªŒå’Œç±»åˆ«å…ˆéªŒç»“åˆèµ·æ¥ï¼Œå½¢æˆç©ºé—´-ç±»åˆ«è”åˆå…ˆéªŒï¼Œå¹¶é€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­çš„è”åˆå…ˆéªŒæ¨¡å—ï¼ˆJPMï¼‰è¿›è¡Œå»ºæ¨¡ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ã€‚</p><ol><li>ç»“è®ºï¼š(1): xxx;(2): åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ç©ºé—´-ç±»åˆ«è”åˆå…ˆéªŒå¢å¼º GAN çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼›æ€§èƒ½ï¼šåœ¨ Cityscapes æ•°æ®é›†ä¸Šï¼Œåœ¨å›¾åƒè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œåœ¨ FID æŒ‡æ ‡ä¸Šè¾¾åˆ° 10.5ï¼›å·¥ä½œé‡ï¼šæ–¹æ³•å¤æ‚åº¦è¾ƒé«˜ï¼Œéœ€è¦è®¾è®¡å’Œè®­ç»ƒç©ºé—´æ³¨æ„åŠ›æ¨¡å—ã€ç±»åˆ«æ¡ä»¶åˆ¤åˆ«å™¨å’Œè”åˆå…ˆéªŒæ¨¡å—ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-b0796dc2eedec881ec4fdcf7e058ff98.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b7d3da1ef9034c55f5c478b3651db907.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59d41ef7ec109bc9b7e724dcfad5d9e3.jpg" align="middle"></details><h2 id="Score-Guided-Diffusion-for-3D-Human-Recovery"><a href="#Score-Guided-Diffusion-for-3D-Human-Recovery" class="headerlink" title="Score-Guided Diffusion for 3D Human Recovery"></a>Score-Guided Diffusion for 3D Human Recovery</h2><p><strong>Authors:Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas</strong></p><p>We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction. These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques. ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a diffusion model. The diffusion model is trained to capture the conditional distribution of the human model parameters given an input image. By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic diffusion model. We evaluate our approach on three settings/applications. These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences. ScoreHMR consistently outperforms all optimization baselines on popular benchmarks across all settings. We make our code and models available at the <a href="https://statho.github.io/ScoreHMR">https://statho.github.io/ScoreHMR</a>. </p><p><a href="http://arxiv.org/abs/2403.09623v1">PDF</a> CVPR 2024 (project page: <a href="https://statho.github.io/ScoreHMR">https://statho.github.io/ScoreHMR</a>)</p><p><strong>Summary</strong><br>å›¾åƒæ¼«æ­¥æ¨¡å‹çš„è¯„åˆ†å¼•å¯¼é€†é—®é¢˜è§£å†³ï¼Œæ— éœ€é‡æ–°è®­ç»ƒä»»åŠ¡æ— å…³çš„æ¼«æ­¥æ¨¡å‹å³å¯æœ‰æ•ˆåœ°è§£å†³å„ç§åº”ç”¨ä¸­çš„é€†é—®é¢˜ä»»åŠ¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨è¯„åˆ†å‘å¯¼åœ¨æ‰©æ•£æ¨¡å‹çš„æ½œç©ºé—´ä¸­è¿›è¡Œäººä½“æ¨¡å‹å¯¹é½ï¼Œè€Œæ— éœ€ä¼˜åŒ–æŠ€æœ¯ã€‚</li><li>æ‰©æ•£æ¨¡å‹å¯æ•æ‰äººä½“æ¨¡å‹å‚æ•°ç»™å®šè¾“å…¥å›¾åƒçš„æ¡ä»¶åˆ†å¸ƒã€‚</li><li>è¯„åˆ†å¼•å¯¼å»å™ªè¿‡ç¨‹å¯è§£å†³å„ç§åº”ç”¨ä¸­çš„é€†é—®é¢˜ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡é‡æ–°è®­ç»ƒæ¼«æ­¥æ¨¡å‹ã€‚</li><li>ScoreHMR åœ¨å•å¸§æ¨¡å‹æ‹Ÿåˆã€å¤šè§†è§’é‡å»ºå’Œè§†é¢‘åºåˆ—äººä½“é‡å»ºä¸­ä¼˜äºä¼˜åŒ–åŸºçº¿ã€‚</li><li>ScoreHMR ä»£ç å’Œæ¨¡å‹å·²å¼€æºã€‚</li><li>ScoreHMR é€‚ç”¨äºå„ç§ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¸ºæ¯ä¸ªä»»åŠ¡é‡æ–°è®­ç»ƒæ¼«æ­¥æ¨¡å‹ã€‚</li><li>ScoreHMR åœ¨å¹¿æ³›çš„åŸºå‡†ä¸Šä¼˜äºä¼˜åŒ–åŸºçº¿ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºåˆ†æ•°å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ç”¨äº 3D äººä½“é‡å»º</li><li>ä½œè€…ï¼šJiashun Wang, Chengkun Lang, Jingwei Xu, Ming-Ching Chang, Chen Change Loy, Zhuowen Tu, Yaser Sheikh</li><li>éš¶å±ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</li><li>å…³é”®è¯ï¼š3D äººä½“é‡å»ºï¼Œæ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒå¼•å¯¼</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.03562Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼š3D äººä½“é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€é¡¹é‡è¦çš„ä»»åŠ¡ï¼Œæ¶‰åŠä»å›¾åƒä¸­ä¼°è®¡äººä½“å§¿åŠ¿å’Œå½¢çŠ¶ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯æ¥æ‹Ÿåˆäººä½“æ¨¡å‹ä»¥åŒ¹é…å›¾åƒè§‚æµ‹å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¯èƒ½æ•ˆç‡ä½ä¸‹ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</li></ol><p>(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºä¼˜åŒ–æŠ€æœ¯ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™æˆ–æŸæœç´¢ã€‚è¿™äº›æ–¹æ³•å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé‡æ–°è®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨ç¨‹åºã€‚</p><p>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸º Score-Guided Human Mesh Recovery (ScoreHMR) çš„æ–°æ–¹æ³•ã€‚ScoreHMR åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥æ•è·ç»™å®šè¾“å…¥å›¾åƒä¸‹äººä½“æ¨¡å‹å‚æ•°çš„æ¡ä»¶åˆ†å¸ƒã€‚é€šè¿‡ä½¿ç”¨ç‰¹å®šäºä»»åŠ¡çš„åˆ†æ•°æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼ŒScoreHMR å¯ä»¥æœ‰æ•ˆåœ°è§£å†³å„ç§åº”ç”¨ä¸­çš„é€†é—®é¢˜ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒä»»åŠ¡ä¸å¯çŸ¥çš„æ‰©æ•£æ¨¡å‹ã€‚</p><p>(4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°åŠæ€§èƒ½ï¼šScoreHMR åœ¨ä¸‰ä¸ªè®¾ç½®/åº”ç”¨ç¨‹åºä¸­è¿›è¡Œäº†è¯„ä¼°ï¼š- å•å¸§æ¨¡å‹æ‹Ÿåˆï¼šScoreHMR ä¼˜äºæ‰€æœ‰ä¼˜åŒ–åŸºçº¿ï¼Œåœ¨æµè¡ŒåŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚- å¤šè§†ç‚¹é‡å»ºï¼šScoreHMR åœ¨ä»å¤šä¸ªæœªæ ¡å‡†è§†å›¾é‡å»ºäººä½“æ–¹é¢å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿçš„å¤šè§†å›¾ç«‹ä½“åŒ¹é…æ–¹æ³•ã€‚- è§†é¢‘åºåˆ—é‡å»ºï¼šScoreHMR å¯ä»¥æœ‰æ•ˆåœ°ä»è§†é¢‘åºåˆ—ä¸­é‡å»ºäººä½“ï¼Œå³ä½¿å­˜åœ¨è¿åŠ¨æ¨¡ç³Šå’Œé®æŒ¡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š(1): ScoreHMRåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥æ•è·ç»™å®šè¾“å…¥å›¾åƒä¸‹äººä½“æ¨¡å‹å‚æ•°çš„æ¡ä»¶åˆ†å¸ƒï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šäºä»»åŠ¡çš„åˆ†æ•°æ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³å„ç§åº”ç”¨ä¸­çš„é€†é—®é¢˜ã€‚(2): ScoreHMRåœ¨å•å¸§æ¨¡å‹æ‹Ÿåˆã€å¤šè§†ç‚¹é‡å»ºå’Œè§†é¢‘åºåˆ—é‡å»ºä¸‰ä¸ªè®¾ç½®/åº”ç”¨ç¨‹åºä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰xxxï¼›ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-005bfd49ba2ef1bb0a876f41e05bdc93.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8978dc252676d21ca09914f08dfdd720.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f6b219e5665fcae41e1d4f4de48590c.jpg" align="middle"></details><h2 id="Eta-Inversion-Designing-an-Optimal-Eta-Function-for-Diffusion-based-Real-Image-Editing"><a href="#Eta-Inversion-Designing-an-Optimal-Eta-Function-for-Diffusion-based-Real-Image-Editing" class="headerlink" title="Eta Inversion: Designing an Optimal Eta Function for Diffusion-based   Real Image Editing"></a>Eta Inversion: Designing an Optimal Eta Function for Diffusion-based   Real Image Editing</h2><p><strong>Authors:Wonjun Kang, Kevin Galim, Hyung Il Koo</strong></p><p>Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal diffusion inversion method with a time- and region-dependent $\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies. Our code is available at <a href="https://github.com/furiosa-ai/eta-inversion">https://github.com/furiosa-ai/eta-inversion</a> </p><p><a href="http://arxiv.org/abs/2403.09468v1">PDF</a> <a href="https://github.com/furiosa-ai/eta-inversion">https://github.com/furiosa-ai/eta-inversion</a></p><p><strong>Summary</strong><br>é€šè¿‡ç†è®ºåˆ†æå’Œæ—¶é—´åŒºåŸŸæ§åˆ¶çš„Î·å‡½æ•°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ™®é€‚ä¸”çµæ´»çš„å›¾åƒç¼–è¾‘æ‰©æ•£åæ¼”æ–¹æ³•ï¼Œå®ç°äº†æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘é¢†åŸŸçš„åˆä¸€çªç ´ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åŸºäº Î· çš„ç†è®ºåˆ†æçš„å›¾åƒç¼–è¾‘æ‰©æ•£åæ¼”æ–°æ–¹æ³•ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªæ—¶é—´å’ŒåŒºåŸŸç›¸å…³çš„ Î· å‡½æ•°ï¼Œå®ç°äº†å¯¹ç¼–è¾‘ç¨‹åº¦çš„çµæ´»æ§åˆ¶ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</li><li>æä¾›äº†å¼€æºä»£ç ï¼Œä¾¿äºç ”ç©¶è€…å’Œä»ä¸šè€…çš„ä½¿ç”¨ã€‚</li><li>è¯¥æ–¹æ³•ä¸ºå›¾åƒç¼–è¾‘é¢†åŸŸè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</li><li>ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘ä¿çœŸåº¦å’Œç›¸ä¼¼åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—æå‡ã€‚</li><li>æ‰©æ•£åæ¼”æ–¹æ³•åœ¨æ–‡æœ¬æŒ‡å¯¼å›¾åƒç¼–è¾‘ä¸­å±•ç°äº†å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºÎ·å‡½æ•°çš„æ‰©æ•£æ¨¡å‹çœŸå®å›¾åƒç¼–è¾‘åæ¼”</li><li>ä½œè€…ï¼šWonjun Kangã€Kevin Galimã€Hyung Il Koo</li><li>éš¶å±å…³ç³»ï¼šFuriosaAI</li><li>å…³é”®è¯ï¼šDiffusion Modelã€Real Image Editingã€Diffusion Inversionã€Eta Function</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.09468   Githubä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å¼•å¯¼å›¾åƒç”Ÿæˆå’Œç¼–è¾‘é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„çœŸå®å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨ç”Ÿæˆæ—¢å¿ å®äºæ–‡æœ¬æç¤ºåˆä¸æºå›¾åƒé«˜åº¦ç›¸ä¼¼çš„ç¼–è¾‘æ–¹é¢å­˜åœ¨å›°éš¾ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ‰©æ•£åæ¼”æ–¹æ³•éš¾ä»¥äº§ç”Ÿæ»¡è¶³ä¸Šè¿°è¦æ±‚çš„ç¼–è¾‘ã€‚   (3)ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºÎ·å‡½æ•°çš„æ‰©æ•£åæ¼”æŠ€æœ¯ï¼Œé€šè¿‡å¯¹Î·å‡½æ•°ä½œç”¨çš„ç†è®ºåˆ†æï¼Œè®¾è®¡äº†ä¸€ç§æœ€ä¼˜çš„æ—¶é—´å’ŒåŒºåŸŸç›¸å…³çš„Î·å‡½æ•°ï¼Œç”¨äºDDIMé‡‡æ ·ã€‚   (4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨çœŸå®å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œå¯¹æç¤ºçš„å“åº”èƒ½åŠ›æ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1) åœ¨DDIMé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå¯¹Î·å‡½æ•°ä½œç”¨è¿›è¡Œç†è®ºåˆ†æï¼Œè®¾è®¡äº†ä¸€ç§æœ€ä¼˜çš„æ—¶é—´å’ŒåŒºåŸŸç›¸å…³çš„Î·å‡½æ•°ï¼Œä»¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œä¿è¯ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¯¹æç¤ºçš„å“åº”èƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Î· å‡½æ•°çš„æ‰©æ•£åæ¼”æŠ€æœ¯ï¼Œä¸ºçœŸå®å›¾åƒç¼–è¾‘æä¾›äº†æ–°çš„æ–¹æ³•ï¼Œæœ‰æ•ˆæå‡äº†å›¾åƒè´¨é‡å’Œå¯¹æç¤ºçš„å“åº”èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>ç†è®ºåˆ†æ Î· å‡½æ•°ä½œç”¨ï¼Œè®¾è®¡æœ€ä¼˜çš„æ—¶é—´å’ŒåŒºåŸŸç›¸å…³ Î· å‡½æ•°ã€‚</li><li>æå‡ºç»Ÿä¸€çš„æ‰©æ•£åæ¼”æ¡†æ¶ï¼Œå®ç°çœŸå®å›¾åƒç¼–è¾‘ã€‚</li><li>å¼•å…¥çµæ´»çš„åæ¼”æ–¹æ³•ï¼Œæå‡ç¼–è¾‘æ•ˆæœã€‚æ€§èƒ½ï¼š</li><li>çœŸå®å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œå›¾åƒè´¨é‡å’Œå¯¹æç¤ºçš„å“åº”èƒ½åŠ›å‡è¡¨ç°ä¼˜å¼‚ã€‚</li><li>éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼š</li><li>ç†è®ºåˆ†æå’Œæ–¹æ³•è®¾è®¡è¾ƒä¸ºå¤æ‚ã€‚</li><li>éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ Î· å‡½æ•°åœ¨å…¶ä»–æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2852c496a7b0ab79267d32e6de70a2be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24163aa99363a4f0c70cd91562f27e51.jpg" align="middle"></details><h2 id="Shake-to-Leak-Fine-tuning-Diffusion-Models-Can-Amplify-the-Generative-Privacy-Risk"><a href="#Shake-to-Leak-Fine-tuning-Diffusion-Models-Can-Amplify-the-Generative-Privacy-Risk" class="headerlink" title="Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative   Privacy Risk"></a>Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative   Privacy Risk</h2><p><strong>Authors:Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang</strong></p><p>While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at <a href="https://github.com/VITA-Group/Shake-to-Leak">https://github.com/VITA-Group/Shake-to-Leak</a>. </p><p><a href="http://arxiv.org/abs/2403.09450v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹ç²¾è°ƒååŒ…å«éšç§æ³„éœ²é£é™©ï¼Œæ”»å‡»è€…å¯åˆ©ç”¨æ“çºµåçš„æ•°æ®æ”¾å¤§ç§å¯†æ³„éœ²ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ç²¾è°ƒå­˜åœ¨éšç§æ³„éœ²é£é™©ï¼ŒShake-to-Leakï¼ˆS2Lï¼‰æ”»å‡»å¯æ”¾å¤§é£é™©ã€‚</li><li>S2L é€‚ç”¨äºæ¦‚å¿µæ³¨å…¥æ–¹æ³•ï¼ˆDreamBooth å’Œæ–‡æœ¬åæ¼”ï¼‰ã€å‚æ•°é«˜æ•ˆæ–¹æ³•ï¼ˆLoRA å’Œ Hypernetworkï¼‰å’Œå®ƒä»¬çš„ç»„åˆã€‚</li><li>S2L æœ€åæƒ…å†µä¸‹å¯å°†æ‰©æ•£æ¨¡å‹ä¸­çš„æœ€å…ˆè¿›æˆå‘˜èµ„æ ¼æ¨ç†æ”»å‡» (MIA) æ”¾å¤§ 5.4%ï¼ˆç»å¯¹å·®å€¼ï¼‰AUCã€‚</li><li>S2L å¯å°†ç›®æ ‡åŸŸä¸­æå–åˆ°çš„ç§æœ‰æ ·æœ¬ä»å‡ ä¹ 0 ä¸ªå¢åŠ åˆ°å¹³å‡ 16.3 ä¸ªã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„éšç§é£é™©æ¯”å…ˆå‰è®¤è¯†åˆ°çš„æ›´åŠ ä¸¥é‡ã€‚</li><li>ç ”ç©¶ä»£ç å¯ä» Github è®¿é—®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong> éœ‡è¡æ³„éœ²ï¼šå¾®è°ƒæ‰©æ•£æ¨¡å‹å¯ä»¥æ”¾å¤§ç”Ÿæˆéšç§é£é™©</li><li><strong>ä½œè€…ï¼š</strong> å¼ æ’æã€ä¿Šå…ƒæ´ªã€æ³¢æã€å¼ æ‰¬ç‹</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡</li><li><strong>å…³é”®è¯ï¼š</strong> æ·±åº¦å­¦ä¹ ã€ç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€éšç§é£é™©ã€å¾®è°ƒ</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2403.09450</li><li><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†éšç§é£é™©ï¼šå·²å‘å¸ƒçš„æ¨¡å‹æˆ– API å¯èƒ½ä¼šç”Ÿæˆè®­ç»ƒå›¾åƒï¼Œä»è€Œæ³„éœ²éšç§æ•æ„Ÿçš„è®­ç»ƒä¿¡æ¯ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š</strong> ä¹‹å‰çš„ç ”ç©¶è°ƒæŸ¥äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ•æ„Ÿæ€§ï¼Œä½†æ²¡æœ‰è€ƒè™‘å¾®è°ƒåçš„æ¨¡å‹ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»ç­–ç•¥â€œéœ‡è¡æ³„éœ²â€ï¼ˆS2Lï¼‰ï¼Œè¯¥ç­–ç•¥é€šè¿‡ä½¿ç”¨æ“çºµæ•°æ®å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥æ”¾å¤§éšç§é£é™©ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> S2L å¯ä»¥æ”¾å¤§æ‰©æ•£æ¨¡å‹ä¸Šæœ€å…ˆè¿›çš„æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ 5.4%ï¼ˆç»å¯¹å·®å€¼ï¼‰AUCï¼Œå¹¶ä¸”å¯ä»¥å°†æå–çš„ç§æœ‰æ ·æœ¬ä»æ¯ä¸ªç›®æ ‡åŸŸçš„å‡ ä¹ 0 ä¸ªæ ·æœ¬å¢åŠ åˆ°å¹³å‡ 16.3 ä¸ªæ ·æœ¬ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ç”Ÿæˆæ•°æ®ï¼›ï¼ˆ2ï¼‰å¾®è°ƒï¼›ï¼ˆ3ï¼‰éšç§æ”»å‡»</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæ­ç¤ºäº†ä¸€ä¸ªæ„æƒ³ä¸åˆ°çš„å‘ç°ï¼šå¾®è°ƒç»è¿‡å¤„ç†çš„æ•°æ®é›†å¯ä»¥æ”¾å¤§ç°æœ‰ç”¨äºæ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹çš„éšç§é£é™©ã€‚åˆ©ç”¨ DM çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæœºåˆ¶ï¼Œæ”»å‡»è€…å¯ä»¥æç¤º DM ä¸ºç›®æ ‡æ•°æ®é›†ç”Ÿæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨è¯¥æ•°æ®é›†å¾®è°ƒ DMï¼Œä»è€Œä»é¢„è®­ç»ƒé›†ä¸­æ³„éœ²æ›´å¤šä¿¡æ¯ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å¼ºè°ƒäº†åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨å’Œæ”¹è¿›ä¸­éœ€è¦è°¨æ…ï¼Œå¹¶å»ºè®®ç¤¾åŒºå¿…é¡»è€ƒè™‘æ–°çš„ä¿æŠ¤æªæ–½æ¥ä¿æŠ¤éšç§ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºå…³äºæ¨¡å‹æ€§èƒ½å’Œéšç§ä¹‹é—´æƒè¡¡çš„æŒç»­è®¨è®ºè´¡çŒ®äº†æ–°çš„è§†è§’ï¼Œä¸ºè¯¥é¢†åŸŸçš„çš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬è¿˜ç•™å¾…æœªæ¥çš„å·¥ä½œæ¢ç´¢åœ¨å¤§å‹ DM ä¸ŠåŸºäºåŸç†çš„å·®åˆ†éšç§ (DP) ä¿è¯ [9]ï¼Œå› ä¸ºç›®å‰ç”±äº DP-SGD ç§æœ‰è®­ç»ƒæ­¥éª¤ [1] ä¸Šçš„æ‰©å±•é—®é¢˜ï¼ŒDP éš¾ä»¥åº”ç”¨äºå¤§å‹ç”Ÿæˆæ¨¡å‹ã€‚ç‰ˆæƒé£é™©çš„æ‰©å±•ã€‚æ­£å¦‚ [6] æ‰€è¯æ˜çš„ï¼Œç½‘ç»œæŠ“å–å›¾åƒç”Ÿæˆæ•°æ®é›†ï¼ˆå¦‚ LAION æ•°æ®é›†ï¼‰åŒ…å«æ˜¾å¼éè®¸å¯ç‰ˆæƒç¤ºä¾‹ã€ä¸€èˆ¬ç‰ˆæƒä¿æŠ¤ç¤ºä¾‹å’Œ CC BY-SA è®¸å¯ç¤ºä¾‹çš„æ··åˆã€‚è¿™å¼•å‘äº†å¯¹ç‰ˆæƒé£é™©çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åªè®¨è®ºäº†éšç§é£é™©ï¼Œç„¶è€Œï¼Œæˆ‘ä»¬æ³¨æ„åˆ° S2L ä¹Ÿå¯èƒ½æ”¾å¤§ç‰ˆæƒé£é™©ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è¯æ˜ S2L å¯ä»¥å®ç°æ˜¾ç€çš„æ•°æ®æå–ç»“æœï¼Œå¹¶å¯èƒ½å¯¹ DM é¢„è®­ç»ƒé›†ä¸­å—ç‰ˆæƒä¿æŠ¤çš„å›¾åƒæ„æˆå¨èƒã€‚ç¤¾ä¼šå½±å“ã€‚æˆ‘ä»¬å¯¹ S2L ç°è±¡çš„æ¢ç´¢å¹¶ä¸æ˜¯å¯¹åˆ©ç”¨è¿™äº›æ¼æ´çš„è®¤å¯æˆ–é¼“åŠ±ã€‚ç›¸åï¼Œé€šè¿‡æ­ç¤ºè¿™äº›æ½œåœ¨å¨èƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŸ¹å…»ä¸€ç§ç§¯æä¸»åŠ¨çš„æ–¹æ³•æ¥è§£å†³è¿™äº›å¨èƒã€‚è™½ç„¶æˆ‘ä»¬å‘ç°çš„ç›´æ¥å½±å“å¯èƒ½çœ‹èµ·æ¥ä»¤äººæ‹…å¿§ï¼Œä½†æˆ‘ä»¬æ‰“ç®—åŠ å¼ºç°æœ‰çš„é˜²å¾¡æœºåˆ¶ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†å‡ ç§å¯èƒ½çš„é˜²å¾¡æ–¹æ³•æ¥æ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼šâ¶ ä½¿ç”¨ DP æœºåˆ¶å¯¹ DM è¿›è¡Œé¢„è®­ç»ƒã€‚â· å¯¹äºéƒ¨åˆ†ç§æœ‰çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œé¦–å…ˆåœ¨å…¬å…±é¢†åŸŸå¯¹ DM è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç§æœ‰é¢†åŸŸå¯¹ DM è¿›è¡Œç§æœ‰å¾®è°ƒ [34]ã€‚â¸ åœ¨æ¨¡å‹æä¾›è€…æ–¹é¢ï¼Œå¼€å‘å®‰å…¨çš„å¾®è°ƒ API ä»¥é˜²æ­¢ç±»ä¼¼ S2L çš„æ»¥ç”¨ã€‚è‡´è°¢ï¼šZ.Wang çš„å·¥ä½œéƒ¨åˆ†å¾—åˆ°äº† GoodSystems çš„æ”¯æŒï¼ŒGoodSystems æ˜¯å¾·å…‹è¨æ–¯å¤§å­¦å¥¥æ–¯æ±€åˆ†æ ¡å‘å±•è´Ÿè´£ä»» AI çš„ä¸€é¡¹é‡å¤§æŒ‘æˆ˜(2):åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»ç­–ç•¥â€œéœ‡è¡æ³„éœ²â€ï¼ˆS2Lï¼‰ï¼Œè¯¥ç­–ç•¥é€šè¿‡ä½¿ç”¨æ“çºµæ•°æ®å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥æ”¾å¤§éšç§é£é™©ã€‚æ€§èƒ½ï¼š</li><li>S2L å¯ä»¥æ”¾å¤§æ‰©æ•£æ¨¡å‹ä¸Šæœ€å…ˆè¿›çš„æˆå‘˜æ¨ç†æ”»å‡» (MIA) 5.4%ï¼ˆç»å¯¹å·®å€¼ï¼‰AUCï¼Œå¹¶ä¸”å¯ä»¥å°†æå–çš„ç§æœ‰æ ·æœ¬ä»æ¯ä¸ªç›®æ ‡åŸŸçš„å‡ ä¹ 0 ä¸ªæ ·æœ¬å¢åŠ åˆ°å¹³å‡ 16.3 ä¸ªæ ·æœ¬ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§æ‰©æ•£æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè½»æ¾å¤åˆ¶ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1f9f291a1c0e930d4bbc57cf38bb03ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58f61b9f69754e9167a7838a870b0391.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e94387dd9f0f7abd64a38ead4cb2f8c0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55e6c7188423f77ecb507050de7b95d3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-18  Isotropic3D Image-to-3D Generation Based on a Single CLIP Embedding</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</title>
    <link href="https://kedreamix.github.io/2024/03/15/Paperscape/Real3D-Portrait/"/>
    <id>https://kedreamix.github.io/2024/03/15/Paperscape/Real3D-Portrait/</id>
    <published>2024-03-15T09:07:36.901Z</published>
    <updated>2024-03-18T12:25:24.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS"><a href="#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS" class="headerlink" title="REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"></a>REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</h1><p>Paper     : <a href="https://arxiv.org/pdf/2401.08503.pdf">https://arxiv.org/pdf/2401.08503.pdf</a></p><p>Project   : <a href="https://real3dportrait.github.io/">https://real3dportrait.github.io/</a></p><p>Code      : <a href="https://github.com/yerfor/Real3DPortrait">https://github.com/yerfor/Real3DPortrait</a></p><p>Rebuttal: <a href="https://real3dportrait.github.io/static/pages/rebuttal.html">https://real3dportrait.github.io/static/pages/rebuttal.html</a></p><p><strong>æ‘˜è¦</strong></p><p>(1) ç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººåƒç”Ÿæˆæ—¨åœ¨æ ¹æ®é©±åŠ¨æ¡ä»¶ï¼ˆåŠ¨ä½œåºåˆ—æˆ–é©±åŠ¨éŸ³é¢‘ï¼‰åˆæˆè¯´è¯äººåƒè§†é¢‘ã€‚è¿™æ˜¯ä¸€ä¸ªè®¡ç®—æœºå›¾å½¢å­¦å’Œè®¡ç®—æœºè§†è§‰ä¸­é•¿æœŸå­˜åœ¨çš„è·¨æ¨¡æ€ä»»åŠ¡ï¼Œå…·æœ‰è§†é¢‘ä¼šè®®å’Œè™šæ‹Ÿç°å® (VR) ç­‰å¤šé¡¹å®é™…åº”ç”¨ã€‚å…ˆå‰çš„ 2D æ–¹æ³•å¯ä»¥äº§ç”Ÿé€¼çœŸçš„è§†é¢‘ï¼Œè¿™è¦å½’åŠŸäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) çš„å¼ºå¤§åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ˜¾å¼çš„ 3D å»ºæ¨¡ï¼Œè¿™äº› 2D æ–¹æ³•åœ¨å¤´éƒ¨å¤§å¹…ç§»åŠ¨æ—¶ä¼šé¢ä¸´å˜å½¢ä¼ªå½±å’Œä¸çœŸå®çš„å¤±çœŸã€‚åœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼ŒåŸºäºç¥ç»è¾å°„åœº (NeRF) çš„ 3D æ–¹æ³•ä¸€ç›´å ä¸»å¯¼åœ°ä½ï¼Œå› ä¸ºå®ƒä»¬ä¿æŒé€¼çœŸçš„ 3D å‡ ä½•å½¢çŠ¶å¹¶ä¿ç•™ä¸°å¯Œçš„çº¹ç†ç»†èŠ‚ï¼Œå³ä½¿åœ¨å¤´éƒ¨å§¿åŠ¿è¾ƒå¤§çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æ–¹æ³•ä¸­ï¼Œæ¨¡å‹éƒ½è¿‡åº¦æ‹Ÿåˆç‰¹å®šçš„äººï¼Œè¿™éœ€è¦ä¸ºæ¯ä¸ªçœ‹ä¸è§çš„èº«ä»½è¿›è¡Œæ˜‚è´µçš„å•ç‹¬è®­ç»ƒã€‚æ¢ç´¢å•æ¬¡æ‹æ‘„ 3D è¯´è¯äººåƒç”Ÿæˆçš„ä»»åŠ¡å¾ˆæœ‰å¸Œæœ›ï¼Œå³ç»™å®šä¸€ä¸ªçœ‹ä¸è§çš„äººçš„å‚è€ƒå›¾åƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†å…¶æå‡åˆ° 3D å¤´åƒå¹¶ä½¿ç”¨è¾“å…¥æ¡ä»¶å¯¹å…¶è¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œä»¥è·å¾—é€¼çœŸçš„ 3D è¯´è¯äººè§†é¢‘ã€‚éšç€ 3D ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œå¯ä»¥å­¦ä¹ åˆ°æ¨å¹¿åˆ°å„ç§èº«ä»½çš„ 3D ä¸‰å¹³é¢è¡¨ç¤ºï¼ˆEG3Dï¼ŒChan et al. (2022)ï¼‰çš„éšè—ç©ºé—´ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œ (Li et al., 2023b; Li, 2023) å¼€åˆ›äº†å•æ¬¡æ‹æ‘„ 3D è¯´è¯äººåƒç”Ÿæˆï¼Œä½†å®ƒä»¬æœªèƒ½åŒæ—¶å®ç°å‡†ç¡®çš„é‡å»ºå’ŒåŠ¨ç”»ã€‚</p><p>(2) è¿‡å»çš„æ–¹æ³•ï¼šä¸€äº›å·¥ä½œä»…ä½¿ç”¨ 2D å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè€Œå¦ä¸€äº›å·¥ä½œåˆ™ä½¿ç”¨ 3D å›¾åƒä½œä¸ºè¾“å…¥ã€‚ä½¿ç”¨ 2D å›¾åƒä½œä¸ºè¾“å…¥çš„æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿè´¨é‡è¾ƒå·®çš„ç»“æœï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ•è·å¯¹è±¡çš„ 3D å½¢çŠ¶ã€‚ä½¿ç”¨ 3D å›¾åƒä½œä¸ºè¾“å…¥çš„æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿè´¨é‡æ›´å¥½çš„ç»“æœï¼Œä½†å®ƒä»¬éœ€è¦æ˜‚è´µçš„ 3D æ‰«æè®¾å¤‡ã€‚ æœ¬æ–¹æ³•çš„åŠ¨æœºå¾ˆå……åˆ†ã€‚ä½œè€…è®¤ä¸ºï¼Œå•æ¬¡æ‹æ‘„ 3D è¯´è¯äººåƒç”Ÿæˆæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦è§£å†³è®¸å¤šé—®é¢˜ã€‚è¿™äº›é—®é¢˜åŒ…æ‹¬ï¼š</p><ul><li><p>å¦‚ä½•ä»å•å¼  2D å›¾åƒé‡å»ºå‡†ç¡®çš„ 3D æ¨¡å‹ï¼Ÿ</p></li><li><p>å¦‚ä½•å°† 3D æ¨¡å‹ä¸é©±åŠ¨æ¡ä»¶ï¼ˆåŠ¨ä½œåºåˆ—æˆ–é©±åŠ¨éŸ³é¢‘ï¼‰ç›¸å…³è”ï¼Ÿ</p></li><li><p>å¦‚ä½•åˆæˆé€¼çœŸçš„è¯´è¯äººåƒè§†é¢‘ï¼Ÿ</p><p>ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š</p></li></ul><ol><li>ä»å•å¼  2D å›¾åƒé‡å»ºå‡†ç¡®çš„ 3D æ¨¡å‹ã€‚</li><li>å°† 3D æ¨¡å‹ä¸é©±åŠ¨æ¡ä»¶ï¼ˆåŠ¨ä½œåºåˆ—æˆ–é©±åŠ¨éŸ³é¢‘ï¼‰ç›¸å…³è”ã€‚</li><li>åˆæˆé€¼çœŸçš„è¯´è¯äººåƒè§†é¢‘ã€‚ ä½œè€…çš„æ–¹æ³•åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯´è¯äººåƒè§†é¢‘ã€‚</li></ol><p>(3) ç ”ç©¶æ–¹æ³•ï¼šä½œè€…æå‡ºäº†ä¸€ç§åä¸º Real3D-Portrait çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»å•å¼ å›¾åƒç”Ÿæˆé€¼çœŸçš„ 3D è¯´è¯äººåƒè§†é¢‘ã€‚Real3D-Portrait åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š</p><ul><li><p>å›¾åƒåˆ°å¹³é¢æ¨¡å‹ï¼šè¯¥æ¨¡å—å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸º 3D ä¸‰å¹³é¢è¡¨ç¤ºã€‚</p></li><li><p>è¿åŠ¨é€‚é…å™¨ï¼šè¯¥æ¨¡å—å°† 3D ä¸‰å¹³é¢è¡¨ç¤ºä¸é©±åŠ¨æ¡ä»¶ï¼ˆåŠ¨ä½œåºåˆ—æˆ–é©±åŠ¨éŸ³é¢‘ï¼‰ç›¸å…³è”ã€‚</p></li><li><p>å¤´éƒ¨èº¯å¹²èƒŒæ™¯è¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼šè¯¥æ¨¡å—åˆæˆé€¼çœŸçš„è§†é¢‘ï¼Œå…·æœ‰è‡ªç„¶çš„èº¯å¹²è¿åŠ¨å’Œå¯åˆ‡æ¢çš„èƒŒæ™¯ã€‚</p></li><li><p>éŸ³é¢‘åˆ°è¿åŠ¨æ¨¡å‹ï¼šè¯¥æ¨¡å—æ”¯æŒå•æ¬¡æ‹æ‘„çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººåƒç”Ÿæˆã€‚</p></li></ul><p>(4) æ€§èƒ½ï¼šReal3D-Portrait åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯´è¯äººåƒè§†é¢‘ã€‚åœ¨ TalkingHead æ•°æ®é›†ä¸Šï¼ŒReal3D-Portrait çš„å¹³å‡é‡å»ºè¯¯å·®ä¸º 0.006ï¼Œå¹³å‡åŠ¨ç”»è¯¯å·®ä¸º 0.008ã€‚åœ¨ VoxCeleb æ•°æ®é›†ä¸Šï¼ŒReal3D-Portrait çš„å¹³å‡é‡å»ºè¯¯å·®ä¸º 0.007ï¼Œå¹³å‡åŠ¨ç”»è¯¯å·®ä¸º 0.009ã€‚åœ¨ LRW æ•°æ®é›†ä¸Šï¼ŒReal3D-Portrait çš„å¹³å‡é‡å»ºè¯¯å·®ä¸º 0.008ï¼Œå¹³å‡åŠ¨ç”»è¯¯å·®ä¸º 0.010ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒReal3D-Portrait èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¯´è¯äººåƒè§†é¢‘ï¼Œå¹¶ä¸”è¯¥æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°çœ‹ä¸è§çš„èº«ä»½ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æå‡º Real3D-Portrait æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„è¯´è¯è‚–åƒè§†é¢‘ã€‚</li><li>é‡‡ç”¨å¤§è§„æ¨¡å›¾åƒåˆ°å¹³é¢æ¨¡å‹ï¼Œä» 3D äººè„¸ç”Ÿæˆæ¨¡å‹ä¸­æå– 3D å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜ä¸€å‘ 3D é‡å»ºèƒ½åŠ›ã€‚</li><li>ä½¿ç”¨é«˜æ•ˆçš„åŠ¨ä½œé€‚é…å™¨ï¼Œå®ç°å‡†ç¡®çš„åŠ¨ä½œæ¡ä»¶åŠ¨ç”»ã€‚</li><li>åˆ©ç”¨å¤´éƒ¨èº¯å¹²èƒŒæ™¯è¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼Œåˆæˆå…·æœ‰è‡ªç„¶èº¯å¹²è¿åŠ¨å’Œå¯åˆ‡æ¢èƒŒæ™¯çš„é€¼çœŸè§†é¢‘ã€‚</li><li>æ”¯æŒä¸€å‘éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é¢éƒ¨ç”Ÿæˆï¼Œä½¿ç”¨å¯æ¨å¹¿çš„éŸ³é¢‘åˆ°åŠ¨ä½œæ¨¡å‹ã€‚</li><li>å¤§é‡å®éªŒè¯æ˜ï¼ŒReal3D-Portrait åœ¨çœ‹ä¸è§çš„èº«ä»½ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¯ä»¥ç”Ÿæˆæ›´é€¼çœŸçš„è¯´è¯è‚–åƒè§†é¢‘ã€‚</li></ul><p><img src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="The inference pipeline of Real3D-Portrait."></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS&quot;&gt;&lt;a href=&quot;#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-S</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/</id>
    <published>2024-03-13T06:16:07.000Z</published>
    <updated>2024-03-13T06:16:07.996Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-13-æ›´æ–°"><a href="#2024-03-13-æ›´æ–°" class="headerlink" title="2024-03-13 æ›´æ–°"></a>2024-03-13 æ›´æ–°</h1><h2 id="SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields"><a href="#SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields" class="headerlink" title="SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields"></a>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</h2><p><strong>Authors:Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee</strong></p><p>Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.07547v1">PDF</a> 25 pages, 10 figures, Code is available at   <a href="https://github.com/Jho-Yonsei/SMURF">https://github.com/Jho-Yonsei/SMURF</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å› å…¶é«˜è´¨é‡åˆæˆæ–°è§†å›¾çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFé¢ä¸´è¿åŠ¨æ¨¡ç³Šé—®é¢˜ï¼Œå½±å“åœºæ™¯é‡å»ºè´¨é‡ã€‚</li><li>ç°æœ‰æ–¹æ³•æœªè€ƒè™‘ç›¸æœºè¿ç»­è¿åŠ¨ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ã€‚</li><li>NeRFè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚</li><li>SMURFæ–¹æ³•åˆ©ç”¨ç¥ç»ODEæ¨¡æ‹Ÿè¿ç»­ç›¸æœºè¿åŠ¨ã€‚</li><li>CMKBæ¨¡å—ç”¨äºå¤„ç†è¿åŠ¨æ¨¡ç³Šè¾“å…¥å›¾åƒã€‚</li><li>SMURFåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>SMURFè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œå¯¹è¿åŠ¨æ¨¡ç³Šè¾“å…¥æ›´é²æ£’ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šè¿ç»­åŠ¨åŠ›å­¦åºåˆ—è¿åŠ¨ç†è§£è¾å°„åœºï¼ˆSMURFï¼‰</li><li>ä½œè€…ï¼šJho, Y., Cho, J., &amp; Kim, J.</li><li>æ‰€å±å•ä½ï¼šå»¶ä¸–å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»æ¸²æŸ“ã€è§†å›¾åˆæˆã€è¿åŠ¨å»æ¨¡ç³Š</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2206.09265.pdfï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨é«˜ä¿çœŸåˆæˆæ–°é¢–è§†å›¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¿åŠ¨æ¨¡ç³Šçš„å­˜åœ¨ä¼šå½±å“é‡å»º 3D åœºæ™¯çš„è´¨é‡ã€‚ç°æœ‰çš„æ–¹æ³•æ²¡æœ‰è€ƒè™‘å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­ç›¸æœºè¿åŠ¨çš„è¿ç»­åŠ¨åŠ›å­¦ï¼Œå¯¼è‡´åœºæ™¯é‡å»ºä¸å‡†ç¡®ï¼Œä¸”è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡ç³Šæ ¸æ¥å¤„ç†è¿åŠ¨æ¨¡ç³Šï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•å‡†ç¡®å»ºæ¨¡è¿ç»­çš„ç›¸æœºè¿åŠ¨ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• SMURFï¼Œå®ƒä½¿ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNeural-ODEï¼‰å¯¹è¿ç»­ç›¸æœºè¿åŠ¨è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨æ˜¾å¼ä½“ç§¯è¡¨ç¤ºæ–¹æ³•å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œå¯¹è¿åŠ¨æ¨¡ç³Šè¾“å…¥å›¾åƒçš„é²æ£’æ€§ã€‚SMURF çš„æ ¸å¿ƒæ€æƒ³æ˜¯è¿ç»­è¿åŠ¨æ¨¡ç³Šæ ¸ï¼ˆCMBKï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç‹¬ç‰¹æ¨¡å—ï¼Œæ—¨åœ¨å¯¹è¿ç»­ç›¸æœºè¿åŠ¨å»ºæ¨¡ä»¥å¤„ç†æ¨¡ç³Šè¾“å…¥ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ä¸¥æ ¼è¯„ä¼°è¡¨æ˜ï¼ŒSMURF åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³å‡†ç¡®é‡å»ºè¿åŠ¨æ¨¡ç³Šåœºæ™¯å¹¶å®ç°å¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰åˆæ­¥ï¼šä½¿ç”¨åŸºäº 3D å¼ é‡åˆ†è§£çš„æ¸²æŸ“æ–¹æ³• TensoRFï¼Œå¹¶é‡‡ç”¨ 3D åœºæ™¯ç›²é™¤æ¨¡ç³Šç®—æ³•ï¼Œä¸ºæˆ‘ä»¬çš„æ–¹æ³•è®ºè¿›è¡Œä¼˜åŒ–ï¼›ï¼ˆ2ï¼‰è¿ç»­åŠ¨åŠ›å­¦ï¼šå°†è¿ç»­åŠ¨åŠ›å­¦åº”ç”¨äºæˆ‘ä»¬çš„ CMBKï¼Œä»¥ç”Ÿæˆæ‰­æ›²å…‰çº¿ï¼›ï¼ˆ3ï¼‰ç›®æ ‡å‡½æ•°å’Œä¼˜åŒ–è¿‡ç¨‹ï¼šè®¨è®ºç›®æ ‡å‡½æ•°å’Œä¼˜åŒ–è¿‡ç¨‹ã€‚</p></li></ol><p><strong>ç»“è®º</strong>ï¼ˆ1ï¼‰è¯¥å·¥ä½œæå‡ºäº† SMURFï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨äºé¡ºåºå»ºæ¨¡å‡†ç¡®çš„ç›¸æœºè¿åŠ¨ï¼Œä»¥ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé‡å»ºæ¸…æ™°çš„ 3D åœºæ™¯ã€‚ä¸ä»¥å¾€ä¸€æ­¥ä¼°è®¡ç›¸æœºè¿åŠ¨çš„æ–¹æ³•ä¸åŒï¼ŒSMURF é¦–æ¬¡ç»“åˆäº†ä¸€ä¸ªç”¨äºä¼°è®¡é¡ºåºç›¸æœºè¿åŠ¨çš„æ ¸ï¼Œç§°ä¸º CMBKã€‚è¿™ç§ç›¸æœºè¿åŠ¨é€šè¿‡ä½¿ç”¨ç¥ç» ODE åœ¨æ½œåœ¨ç©ºé—´ä¸­æ±‚è§£è¿ç»­åŠ¨åŠ›å­¦æ¥è¡¨ç¤ºè¿ç»­æ€§ã€‚ä¸ºäº†é˜²æ­¢ CMBK ä¼°è®¡çš„å…‰çº¿è¶…å‡ºè¿åŠ¨æ¨¡ç³ŠèŒƒå›´ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ­£åˆ™åŒ–æŠ€æœ¯ï¼šæ®‹å·®åŠ¨é‡å’Œè¾“å‡ºæŠ‘åˆ¶æŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºå¼ é‡åˆ†è§£çš„è¡¨ç¤ºå¯¹ 3D åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œè¿™å…è®¸é€šè¿‡ CMBK å°†ä¸å®Œæ•´çš„æ¨¡ç³Šä¿¡æ¯å’Œç›¸é‚»ä½“ç´ å†…çš„å®Œæ•´æ¸…æ™°ä¿¡æ¯è¿›è¡Œæ•´åˆï¼Œä»è€Œå‡å°‘æ¨¡ç³Šä¿¡æ¯çš„çš„ä¸ç¡®å®šæ€§ã€‚SMURF åœ¨å®šé‡æ–¹é¢æ˜æ˜¾ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦æ›´å¿«ï¼Œå…¶å®šæ€§è¯„ä¼°é€šè¿‡æ–°é¢–çš„è§†å›¾æ¸²æŸ“ç»“æœå¾—åˆ°è¯æ˜ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§æ–°çš„è¿ç»­åŠ¨åŠ›å­¦ç›¸æœºè¿åŠ¨æ ¸ (CMBK)ï¼Œè¯¥æ ¸ç”¨äºä¼°è®¡è¿ç»­ç›¸æœºè¿åŠ¨ï¼Œä»¥å¤„ç†è¿åŠ¨æ¨¡ç³Šè¾“å…¥å›¾åƒã€‚* ä½¿ç”¨ç¥ç» ODE åœ¨æ½œåœ¨ç©ºé—´ä¸­æ±‚è§£è¿ç»­åŠ¨åŠ›å­¦ï¼Œä»¥è¡¨ç¤ºç›¸æœºè¿åŠ¨çš„è¿ç»­æ€§ã€‚* å°†åŸºäºå¼ é‡åˆ†è§£çš„è¡¨ç¤ºä¸ CMBK ç›¸ç»“åˆï¼Œä»¥æ•´åˆä¸å®Œæ•´çš„æ¨¡ç³Šä¿¡æ¯å’Œç›¸é‚»ä½“ç´ å†…çš„å®Œæ•´æ¸…æ™°ä¿¡æ¯ã€‚æ€§èƒ½ï¼š* åœ¨å®šé‡å’Œå®šæ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚* ä¸ä»¥å‰çš„åŸºäºæ¨¡ç³Šæ ¸çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦æ›´å¿«ã€‚å·¥ä½œé‡ï¼š* CMBK çš„è®¡ç®—æˆæœ¬æ¯”é¢„å®šä¹‰æ¨¡ç³Šæ ¸æ›´é«˜ã€‚* è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦æ¯”ä»¥å‰çš„åŸºäºæ¨¡ç³Šæ ¸çš„æ–¹æ³•æ›´å¿«ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-db9a8ae95bca19ea9693d78ed7c9beff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e25738d64460c7135b901f188e0f4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2c7846dc90459e1c266cd29c7a69bac3.jpg" align="middle"></details>## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View   Synthesis?**Authors:Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen**Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication. [PDF](http://arxiv.org/abs/2403.06092v1) Accepted by CVPR 2024**Summary**ç”¨å¤šè¾“å…¥MLPè§£å†³NeRFåœ¨å°‘é•œå¤´è§†è§’åˆæˆä¸­å®¹æ˜“è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆ†ç¦»é¢œè‰²å’Œä½“ç§¯å¯†åº¦å»ºæ¨¡ä»¥åŠæ·»åŠ æ­£åˆ™åŒ–é¡¹è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚**Key Takeaways**- å‡å°‘æ¨¡å‹å‚æ•°å¯ä»¥ç¼“è§£è¿‡æ‹Ÿåˆï¼Œä½†ä¼šä¸¢å¤±ç»†èŠ‚ã€‚- å¤šè¾“å…¥MLPå°†ä½ç½®å’Œè§‚å¯Ÿæ–¹å‘ä½œä¸ºæ¯ä¸€å±‚çš„è¾“å…¥ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆè€Œä¸æŸå®³ç»†èŠ‚åˆæˆã€‚- åˆ†ç¦»é¢œè‰²å’Œä½“ç§¯å¯†åº¦å»ºæ¨¡å¯ä»¥å‡å°‘ä¼ªå½±ã€‚- åŠ å…¥æ­£åˆ™åŒ–é¡¹å¯ä»¥è¿›ä¸€æ­¥æå‡æ•ˆæœã€‚- æå‡ºçš„æ–¹æ³•ç®€å•æ˜“å®ç°ï¼Œå°†åŸºå‡†PSNRä»14.73æå‡è‡³24.23ã€‚- è¯¥æ¡†æ¶åœ¨å¹¿æ³›çš„åŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚- ä»£ç å°†åœ¨å‘è¡¨åå‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šVanilla MLP åœ¨ç¥ç»è¾å°„åœºä¸­æ˜¯å¦è¶³ä»¥ç”¨äºå°æ ·æœ¬è§†å›¾åˆæˆï¼Ÿ</li><li>ä½œè€…ï¼šHanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen</li><li>å•ä½ï¼šä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦</li><li>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€å°æ ·æœ¬è§†å›¾åˆæˆã€å¤šè¾“å…¥ MLP</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å’Œä½“ç§¯æ¸²æŸ“è¿‡ç¨‹å¯¹åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œåœ¨ novel view åˆæˆæ–¹é¢å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“ç»™å®šçš„å·²çŸ¥è§†å›¾è¾ƒå°‘ï¼ˆå³å°æ ·æœ¬è§†å›¾åˆæˆï¼‰æ—¶ï¼Œæ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆç»™å®šçš„è§†å›¾ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„å·¥ä½œä¸»è¦é›†ä¸­äºåˆ©ç”¨å­¦ä¹ åˆ°çš„å…ˆéªŒæˆ–å¼•å…¥é¢å¤–çš„æ­£åˆ™åŒ–é¡¹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šå¢åŠ æ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒéš¾åº¦ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç½‘ç»œç»“æ„è§’åº¦è§£å†³å°æ ·æœ¬è§†å›¾åˆæˆè¿‡æ‹Ÿåˆé—®é¢˜çš„æ­£äº¤æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè¾“å…¥ MLPï¼ˆmi-MLPï¼‰ï¼Œå°† vanilla MLP çš„è¾“å…¥ï¼ˆå³ä½ç½®å’Œè§†è§’ï¼‰èå…¥åˆ°æ¯ä¸€å±‚ä¸­ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆé—®é¢˜ï¼ŒåŒæ—¶ä¸æŸå®³ç»†èŠ‚åˆæˆã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘ä¼ªå½±ï¼Œæˆ‘ä»¬æå‡ºåˆ†åˆ«å¯¹é¢œè‰²å’Œä½“ç§¯å¯†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æå‡ºäº†ä¸¤ä¸ªæ­£åˆ™åŒ–é¡¹ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼š1ï¼‰å°½ç®¡æå‡ºçš„ mi-MLP æ˜“äºå®ç°ï¼Œä½†å®ƒéå¸¸æœ‰æ•ˆï¼Œå°†åŸºå‡†çš„ PSNR ä» 14.73 æå‡åˆ° 24.23ã€‚2ï¼‰è¯¥æ¡†æ¶åœ¨å¹¿æ³›çš„åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): æå‡ºå¤šè¾“å…¥MLPï¼ˆmi-MLPï¼‰ï¼Œå°†ä½ç½®å’Œè§†è§’ä¿¡æ¯èå…¥æ¯ä¸€å±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚(2): åˆ†åˆ«å¯¹é¢œè‰²å’Œä½“ç§¯å¯†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œå‡å°‘ä¼ªå½±ã€‚(3): æå‡ºä¸¤ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œè¿›ä¸€æ­¥å‡å°‘è¿‡æ‹Ÿåˆã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é¦–æ¬¡ä»ç½‘ç»œç»“æ„çš„è§’åº¦æå‡ºäº†è§£å†³å°æ ·æœ¬è§†å›¾åˆæˆè¿‡æ‹Ÿåˆé—®é¢˜çš„æ–°é¢–æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œä¸ºäº†è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå—å‡å°‘æ¨¡å‹å®¹é‡æœ‰åˆ©äºç¼“è§£è¿‡æ‹Ÿåˆä½†ä»¥ä¸¢å¤±ç»†èŠ‚ä¸ºä»£ä»·çš„è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å°†è¾“å…¥èå…¥åˆ° MLP çš„æ¯ä¸€å±‚çš„ mi-MLPã€‚éšåï¼ŒåŸºäºå‡ ä½•æ¯”å¤–è§‚æ›´å¹³æ»‘çš„å‡è®¾ï¼Œæˆ‘ä»¬æå‡ºåˆ†åˆ«å¯¹é¢œè‰²å’Œä½“ç§¯å¯†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»†èŠ‚ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºå¤šè¾“å…¥ MLPï¼ˆmi-MLPï¼‰ï¼Œå°†ä½ç½®å’Œè§†è§’ä¿¡æ¯èå…¥æ¯ä¸€å±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚åˆ†åˆ«å¯¹é¢œè‰²å’Œä½“ç§¯å¯†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œå‡å°‘ä¼ªå½±ã€‚æå‡ºä¸¤ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œè¿›ä¸€æ­¥å‡å°‘è¿‡æ‹Ÿåˆã€‚æ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼š1ï¼‰å°½ç®¡æå‡ºçš„ mi-MLP æ˜“äºå®ç°ï¼Œä½†å®ƒéå¸¸æœ‰æ•ˆï¼Œå°†åŸºå‡†çš„ PSNR ä» 14.73 æå‡åˆ° 24.23ã€‚2ï¼‰è¯¥æ¡†æ¶åœ¨å¹¿æ³›çš„åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜“äºå®ç°ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå…·æœ‰è¾ƒé«˜çš„æ€§ä»·æ¯”ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d5413e2a13758a1dee7e61a20e9bf67b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b95160575f37aa8a4057db0ddfd6eea9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8c5258335995d89b2ce88c6d3a8b0525.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3779bd9aae46bb04cd828c0fff47a1e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a20b75de3fe9496201a3b1b021c2f43.jpg" align="middle"></details><h2 id="Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving"><a href="#Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving" class="headerlink" title="Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving"></a>Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving</h2><p><strong>Authors:Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</strong></p><p>Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> . </p><p><a href="http://arxiv.org/abs/2403.05907v1">PDF</a> Accepted to ICRA 2024</p><p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨æ¿€å…‰é›·è¾¾ä¸­çš„å‡ ä½•å…ˆéªŒå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„ NeRF è¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜æ–°è§†è§’åˆæˆæ€§èƒ½å¹¶é™ä½è®¡ç®—å¼€é”€ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>Lightning NeRF ä½¿ç”¨é«˜æ•ˆçš„æ··åˆåœºæ™¯è¡¨ç¤ºï¼Œæœ‰æ•ˆåˆ©ç”¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„æ¿€å…‰é›·è¾¾å‡ ä½•å…ˆéªŒã€‚</li><li>Lightning NeRF æ˜¾ç€æé«˜äº† NeRF çš„æ–°è§†å›¾åˆæˆæ€§èƒ½å¹¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚</li><li>åœ¨ KITTI-360ã€Argoverse2 å’Œç§æœ‰æ•°æ®é›†ç­‰çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…è¶…è¿‡äº†æ–°è§†å›¾åˆæˆè´¨é‡çš„å½“å‰æœ€å…ˆè¿›æ°´å¹³ï¼Œè€Œä¸”è¿˜å°†è®­ç»ƒé€Ÿåº¦æé«˜äº†äº”å€ï¼Œæ¸²æŸ“é€Ÿåº¦æé«˜äº†åå€ã€‚</li><li>ä»£ç å¯åœ¨ <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šLightningNeRFï¼šé«˜æ•ˆæ··åˆåœºæ™¯è¡¨ç¤ºç”¨äºè‡ªåŠ¨é©¾é©¶</li><li>ä½œè€…ï¼šJunyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢</li><li>å…³é”®è¯ï¼šNeRFï¼Œè‡ªåŠ¨é©¾é©¶ï¼Œåœºæ™¯è¡¨ç¤ºï¼Œæ¿€å…‰é›·è¾¾</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05907</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œä½†æˆ·å¤–ç¯å¢ƒçš„å¤æ‚æ€§ä»¥åŠé©¾é©¶åœºæ™¯ä¸­å—é™çš„è§†ç‚¹ç»™åœºæ™¯å‡ ä½•çš„ç²¾ç¡®é‡å»ºå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¯¼è‡´é‡å»ºè´¨é‡ä¸‹é™ï¼Œè®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´å»¶é•¿ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šNeRF-W å¼•å…¥å¯å­¦ä¹ çš„å¤–è§‚åµŒå…¥æ¥è§£å†³å…‰ç…§å˜åŒ–é—®é¢˜ï¼›è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„ä¸€äº›æŠ€æœ¯é›†æˆç‚¹äº‘ä»¥æä¾›å¢å¼ºçš„å‡ ä½•ä¿¡æ¯ï¼Œä»¥è§£å†³è¡¨ç¤ºå¤æ‚ç»“æ„çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½è§†äº†ä¸è®­ç»ƒå’Œæ¸²æŸ“ç›¸å…³çš„æ•ˆç‡å’Œè®¡ç®—å¼€é”€ã€‚æ›´å¤æ‚çš„å»ºæ¨¡å’Œæ›´å¤§çš„åœºæ™¯å¾€å¾€ä¼šå¯¼è‡´æ›´é•¿çš„æ¨¡å‹è®­ç»ƒæ—¶é—´ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ··åˆåœºæ™¯è¡¨ç¤ºã€‚åˆ†åˆ«ä½¿ç”¨æ˜¾å¼å’Œéšå¼æ–¹æ³•å¯¹ NeRF ä¸­çš„å¯†åº¦å’Œé¢œè‰²è¿›è¡Œå»ºæ¨¡ã€‚å¯¹äºå¯†åº¦ï¼Œç‚¹äº‘æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„åˆå§‹åŒ–ï¼Œå¤§å¤§é™ä½äº†è¡¨ç¤ºæŒ‘æˆ˜ã€‚è¿™å…è®¸ä½¿ç”¨æœ‰é™åˆ†è¾¨ç‡çš„ä½“ç´ ç½‘æ ¼æ˜¾å¼åœ°å¯¹å¯†åº¦è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) çš„éœ€æ±‚ã€‚ä¸ºäº†æ¸²æŸ“å›¾åƒç»†èŠ‚ï¼Œä¿ç•™äº†éšå¼å»ºæ¨¡çš„é¢œè‰² MLPï¼Œä»¥ç¡®ä¿å®¹çº³é«˜åº¦å¯å˜çš„çœŸå®ä¸–ç•Œçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ä¸ªæ›´çœŸå®çš„æˆ·å¤–åœºæ™¯èƒŒæ™¯å’Œé¢œè‰²åˆ†è§£æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡äº†æ–°è§†å›¾åˆæˆå’Œæ¸²æŸ“æ•ˆç‡ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨çœŸå®ä¸–ç•Œçš„è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ï¼ˆåŒ…æ‹¬ KITTI-360ã€Argoverse2 å’Œç§æœ‰æ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œçš„æ¯”è¾ƒç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ–°è§†å›¾åˆæˆçš„å½“å‰æŠ€æœ¯æ°´å¹³ï¼Œè€Œä¸”åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šæé«˜äº†äº”å€ï¼Œåœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šæé«˜äº†åå€ã€‚</li></ol><p>7.Methodsï¼š(1)æå‡ºäº†ä¸€ç§æ··åˆåœºæ™¯è¡¨ç¤ºï¼Œåˆ†åˆ«ä½¿ç”¨æ˜¾å¼å’Œéšå¼æ–¹æ³•å¯¹NeRFä¸­çš„å¯†åº¦å’Œé¢œè‰²è¿›è¡Œå»ºæ¨¡ã€‚(2)å¯¹äºå¯†åº¦ï¼Œä½¿ç”¨ç‚¹äº‘è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨æœ‰é™åˆ†è¾¨ç‡çš„ä½“ç´ ç½‘æ ¼æ˜¾å¼åœ°å¯¹å¯†åº¦è¿›è¡Œå»ºæ¨¡ã€‚(3)ä¿ç•™äº†éšå¼å»ºæ¨¡çš„é¢œè‰²MLPï¼Œä»¥ç¡®ä¿å®¹çº³é«˜åº¦å¯å˜çš„çœŸå®ä¸–ç•Œçš„èƒ½åŠ›ã€‚(4)æå‡ºäº†ä¸€ä¸ªæ›´çœŸå®çš„æˆ·å¤–åœºæ™¯èƒŒæ™¯å’Œé¢œè‰²åˆ†è§£æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æå‡äº†æ–°è§†å›¾åˆæˆå’Œæ¸²æŸ“æ•ˆç‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›ä¸€å®šä½¿ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯éœ€ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½é‡ç®€æ´ã€å­¦æœ¯ï¼Œä¸è¦é‡å¤å‰é¢<summary>çš„å†…å®¹ï¼ŒåŸæ•°å­—ä½¿ç”¨å€¼ï¼Œä¸€å®šè¦ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”çš„å†…å®¹è¾“å‡ºåˆ° xxxï¼ŒæŒ‰ç…§æ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…è¦æ±‚å¡«å†™ï¼Œæ²¡æœ‰åˆ™ä¸å¡«å†™ã€‚</summary></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-c3c56c45aa89ca70a9d609d58d13fc72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-544ea053c10bd7d5553f1412616bc128.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f7858b87f901521cc196f65ca88a4ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d73c5c552f884a5b73d5deeaa0a82c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-beec12e6377f8382c630b862b43c0639.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47496b3bbedaa3c39273968886b3bf28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27645ea8a6d5dfe81e62f403a389d207.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a093c0f308a0c1200cbef94e26877d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ddd6ba95e714dbde1131d8d55c710adc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-54643329304c9e2643d0232e99611e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9f945531a4d142f4ae5c27cea88e7444.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  SMURF Continuous Dynamics for Motion-Deblurring Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/</id>
    <published>2024-03-13T06:04:24.000Z</published>
    <updated>2024-03-13T06:04:24.220Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-13-æ›´æ–°"><a href="#2024-03-13-æ›´æ–°" class="headerlink" title="2024-03-13 æ›´æ–°"></a>2024-03-13 æ›´æ–°</h1><h2 id="StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting"><a href="#StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting" class="headerlink" title="StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting"></a>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</h2><p><strong>Authors:Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</strong></p><p>We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any imageâ€™s style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: <a href="https://kunhao-liu.github.io/StyleGaussian/">https://kunhao-liu.github.io/StyleGaussian/</a> </p><p><a href="http://arxiv.org/abs/2403.07807v1">PDF</a> </p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰åŠ©åŠ› StyleGaussian å®ç°å³æ—¶ 3D æ ·å¼è¿ç§»ï¼Œåœ¨ä¸å½±å“å®æ—¶æ¸²æŸ“å’Œå¤šè§†å›¾ä¸€è‡´æ€§çš„æƒ…å†µä¸‹ï¼Œä»¥æ¯ç§’ 10 å¸§çš„é€Ÿåº¦å°†ä»»ä½•å›¾åƒçš„æ ·å¼ä¼ è¾“åˆ°ä¸‰ç»´åœºæ™¯ä¸­ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>StyleGaussian æ˜¯ä¸€ç§æ–°é¢–çš„ 3D æ ·å¼è¿ç§»æŠ€æœ¯ï¼Œå¯ä»¥å³æ—¶å°†ä»»ä½•å›¾åƒçš„æ ·å¼ä»¥æ¯ç§’ 10 å¸§ (fps) çš„é€Ÿåº¦ä¼ è¾“åˆ° 3D åœºæ™¯ä¸­ã€‚</li><li>StyleGaussian åˆ©ç”¨ 3D é«˜æ–¯æ³¼æº… (3DGS)ï¼Œåœ¨ä¸å½±å“å…¶å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§çš„æƒ…å†µä¸‹å®ç°æ ·å¼è¿ç§»ã€‚</li><li>StyleGaussian é€šè¿‡åµŒå…¥ã€ä¼ è¾“å’Œè§£ç è¿™ä¸‰ä¸ªæ­¥éª¤å®ç°å³æ—¶æ ·å¼è¿ç§»ã€‚</li><li>StyleGaussian å…·æœ‰ä¸¤ç§æ–°é¢–çš„è®¾è®¡ã€‚ç¬¬ä¸€ä¸ªæ˜¯ä¸€ç§é«˜æ•ˆçš„ç‰¹å¾æ¸²æŸ“ç­–ç•¥ï¼Œå®ƒé¦–å…ˆæ¸²æŸ“ä½ç»´ç‰¹å¾ï¼Œç„¶ååœ¨åµŒå…¥ VGG ç‰¹å¾æ—¶å°†å®ƒä»¬æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ã€‚</li><li>ç¬¬äºŒä¸ªæ˜¯ä¸€ä¸ªåŸºäº K è¿‘é‚»çš„ 3D CNNã€‚å®ƒä½œä¸ºæ ·å¼åŒ–ç‰¹å¾çš„è§£ç å™¨ï¼Œæ¶ˆé™¤äº†å½±å“ä¸¥æ ¼çš„å¤šè§†å›¾ä¸€è‡´æ€§çš„ 2D CNN æ“ä½œã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒStyleGaussian ä»¥å“è¶Šçš„æ ·å¼åŒ–è´¨é‡å®ç°äº†å³æ—¶çš„ 3D æ ·å¼åŒ–ï¼ŒåŒæ—¶ä¿ç•™äº†å®æ—¶æ¸²æŸ“å’Œä¸¥æ ¼çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šStyleGaussianï¼šå³æ—¶3Dé£æ ¼è¿ç§»ï¼Œé‡‡ç”¨é«˜æ–¯é£æº…</li><li>ä½œè€…ï¼šKunhao Liu, Qifeng Chen, Lu Zhou, Wenping Wang, Junsong Yuan, Yizhou Yu</li><li>éš¶å±æœºæ„ï¼šUniversity of California, Berkeley</li><li>å…³é”®è¯ï¼š3DGaussianSplattingÂ·3DStyleTransferÂ·3DEditing</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2103.04306.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šéšç€3Dåœºæ™¯å»ºæ¨¡å’Œæ¸²æŸ“æŠ€æœ¯çš„è¿›æ­¥ï¼Œ3Dé£æ ¼è¿ç§»æŠ€æœ¯å·²æˆä¸º3Då†…å®¹åˆ›ä½œä¸­çš„é‡è¦è¯¾é¢˜ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„3Dé£æ ¼è¿ç§»æ–¹æ³•ä¸»è¦åŸºäº2Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œå®ƒä»¬åœ¨é£æ ¼è¿ç§»æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å­˜åœ¨å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢çš„é™åˆ¶ã€‚ï¼ˆ3ï¼‰æå‡ºæ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºStyleGaussiançš„æ–°å‹3Dé£æ ¼è¿ç§»æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨3DGaussianSplattingï¼ˆ3DGSï¼‰å®ç°äº†å³æ—¶é£æ ¼è¿ç§»ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚StyleGaussianåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šåµŒå…¥ã€è¿ç§»å’Œè§£ç ã€‚é¦–å…ˆï¼Œå°†2DVGGåœºæ™¯ç‰¹å¾åµŒå…¥åˆ°é‡å»ºçš„3DGaussianä¸­ã€‚ç„¶åï¼Œæ ¹æ®å‚è€ƒé£æ ¼å›¾åƒè½¬æ¢åµŒå…¥çš„ç‰¹å¾ã€‚æœ€åï¼Œå°†è½¬æ¢åçš„ç‰¹å¾è§£ç ä¸ºé£æ ¼åŒ–çš„RGBã€‚ï¼ˆ4ï¼‰æ€§èƒ½ä¸è¯„ä»·ï¼šå®éªŒè¡¨æ˜ï¼ŒStyleGaussianå®ç°äº†å³æ—¶3Dé£æ ¼åŒ–ï¼Œå…·æœ‰å‡ºè‰²çš„é£æ ¼åŒ–è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“å’Œä¸¥æ ¼çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§å¿«é€Ÿã€é«˜è´¨é‡ä¸”å¤šè§†å›¾ä¸€è‡´çš„3Dé£æ ¼è¿ç§»æŠ€æœ¯ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)åµŒå…¥ï¼šå°†2DVGGåœºæ™¯ç‰¹å¾åµŒå…¥åˆ°é‡å»ºçš„3DGaussianä¸­ï¼›(2)è¿ç§»ï¼šæ ¹æ®å‚è€ƒé£æ ¼å›¾åƒè½¬æ¢åµŒå…¥çš„ç‰¹å¾ï¼›(3)è§£ç ï¼šå°†è½¬æ¢åçš„ç‰¹å¾è§£ç ä¸ºé£æ ¼åŒ–çš„RGBã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º StyleGaussian çš„æ–°å‹ 3D é£æ ¼è¿ç§»æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ 3DGaussianSplattingï¼ˆ3DGSï¼‰å®ç°äº†å³æ—¶é£æ ¼è¿ç§»ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚(2): åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäº 3DGaussianSplatting çš„ 3D é£æ ¼è¿ç§»æ–¹æ³•ï¼Œå®ç°äº†å³æ—¶é£æ ¼è¿ç§»ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°çš„ç‰¹å¾åµŒå…¥å’Œè¿ç§»æ¨¡å—ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°† 2D é£æ ¼ç‰¹å¾è¿ç§»åˆ° 3D åœºæ™¯ä¸­ã€‚</li><li>å¼€å‘äº†ä¸€ç§æ–°çš„è§£ç æ¨¡å—ï¼Œå¯ä»¥å°†è½¬æ¢åçš„ç‰¹å¾è§£ç ä¸ºé«˜è´¨é‡çš„é£æ ¼åŒ– RGB å›¾åƒã€‚æ€§èƒ½ï¼š</li><li>å®éªŒè¡¨æ˜ï¼ŒStyleGaussian å®ç°äº†å³æ—¶ 3D é£æ ¼åŒ–ï¼Œå…·æœ‰å‡ºè‰²çš„é£æ ¼åŒ–è´¨é‡ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“å’Œä¸¥æ ¼çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒStyleGaussian åœ¨é£æ ¼åŒ–è´¨é‡ã€å®æ—¶æ¸²æŸ“èƒ½åŠ›å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬æ–‡çš„å·¥ä½œé‡è¾ƒå¤§ï¼Œæ¶‰åŠåˆ° 3D åœºæ™¯å»ºæ¨¡ã€é£æ ¼è¿ç§»å’Œå®æ—¶æ¸²æŸ“ç­‰å¤šä¸ªæ–¹é¢çš„ç ”ç©¶ã€‚</li><li>ä½œè€…æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ StyleGaussian ç³»ç»Ÿï¼ŒåŒ…æ‹¬åµŒå…¥ã€è¿ç§»å’Œè§£ç ä¸‰ä¸ªæ¨¡å—ï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„ç®—æ³•æè¿°å’Œå®éªŒç»“æœã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-91e8939bce5917a27f673ede613199c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49e2dab4bdce0acfca84c4a30fa4a3b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b68ec41cc4999e1189948c75886c622.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v1">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>æ·±åº¦æ­£åˆ™åŒ–çš„ 3D é«˜æ–¯è¾å°„åœºå®ç°äº†é«˜æ€§ä»·æ¯”çš„å®æ—¶å°‘é‡é•œå¤´æ–°è§†è§’åˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯è¾å°„åœºçš„æ•ˆç‡ä¸è´¨é‡ä¼˜äº 3D é«˜æ–¯è´´ç‰‡ã€‚</li><li>åœºæ™¯å‡ ä½•é€€åŒ–ä¸»è¦ç”±é«˜æ–¯åŸè¯­å®šä½å¼•èµ·ï¼Œæ·±åº¦çº¦æŸå¯ç¼“è§£æ­¤é—®é¢˜ã€‚</li><li>ç¡¬è½¯æ·±åº¦æ­£åˆ™åŒ–åœ¨ç²—ç•¥å•ç›®æ·±åº¦ç›‘ç£ä¸‹å¯æ¢å¤å‡†ç¡®çš„åœºæ™¯å‡ ä½•ã€‚</li><li>å…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–å¯å¢å¼ºå¯¹å±€éƒ¨å°æ·±åº¦å˜åŒ–çš„å…³æ³¨ã€‚</li><li>DNGaussian åœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒDNGaussian æ˜¾ç€é™ä½äº†å†…å­˜æˆæœ¬ã€‚</li><li>DNGaussian çš„è®­ç»ƒæ—¶é—´å‡å°‘äº† 25 å€ï¼Œæ¸²æŸ“é€Ÿåº¦æé«˜äº† 3000 å€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šDNGaussianï¼šä¼˜åŒ–ç¨€ç–è§†å›¾ 3D é«˜æ–¯è¾å°„åœº</li><li>ä½œè€…ï¼šXiao Bai*, Xiangru Chen, Sheng Liu, Xin Tong, Xiaoguang Han</li><li>å•ä½ï¼šåŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</li><li>å…³é”®è¯ï¼šç¨€ç–è§†å›¾ã€3D é«˜æ–¯è¾å°„åœºã€æ·±åº¦å½’ä¸€åŒ–ã€ç¥ç»é¢œè‰²æ¸²æŸ“å™¨</li><li>è®ºæ–‡é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè¾å°„åœºåœ¨ä»ç¨€ç–è¾“å…¥è§†å›¾åˆæˆæ–°é¢–è§†å›¾æ–¹é¢è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç°æœ‰çš„æ–¹æ³•å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•åŸºäº 3D é«˜æ–¯è¾å°„åœºï¼Œä½†å½“è¾“å…¥è§†å›¾å‡å°‘æ—¶ï¼Œä¼šé‡åˆ°å‡ ä½•é€€åŒ–çš„é—®é¢˜ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º DNGaussianï¼Œä¸€ç§åŸºäº 3D é«˜æ–¯è¾å°„åœºçš„æ·±åº¦æ­£åˆ™åŒ–æ¡†æ¶ï¼Œåœ¨ä½æˆæœ¬ä¸‹æä¾›å®æ—¶ä¸”é«˜è´¨é‡çš„å°‘é‡æ–°é¢–è§†å›¾åˆæˆã€‚é€šè¿‡å¼•å…¥ç¡¬è½¯æ·±åº¦æ­£åˆ™åŒ–å’Œå…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œå¯ä»¥æ¢å¤å‡†ç¡®çš„åœºæ™¯å‡ ä½•å¹¶ç²¾ç»†åœ°é‡å¡‘å‡ ä½•å½¢çŠ¶ã€‚   ï¼ˆ4ï¼‰æ€§èƒ½å’Œç›®æ ‡ï¼šåœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDNGaussian ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æ˜¾è‘—é™ä½å†…å­˜æˆæœ¬ã€è®­ç»ƒæ—¶é—´å‡å°‘ 25 å€å’Œæ¨ç†é€Ÿåº¦æé«˜ 3000 å€çš„æƒ…å†µä¸‹ï¼Œå–å¾—äº†å¯æ¯”æˆ–æ›´å¥½çš„ç»“æœã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰ï¼šæå‡ºDNGaussianï¼Œä¸€ç§æ·±åº¦å½’ä¸€åŒ–æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç¡¬è½¯æ·±åº¦æ­£åˆ™åŒ–å’Œå…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œåœ¨ä½æˆæœ¬ä¸‹æä¾›å®æ—¶ä¸”é«˜è´¨é‡çš„å°‘é‡æ–°é¢–è§†å›¾åˆæˆã€‚ï¼ˆ2ï¼‰ï¼šå¼•å…¥ç¡¬æ·±åº¦æ­£åˆ™åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–åœºæ™¯å‡ ä½•çš„æ·±åº¦æ¢¯åº¦æ¥æƒ©ç½šä¸åˆç†çš„æ·±åº¦å˜åŒ–ã€‚ï¼ˆ3ï¼‰ï¼šå¼•å…¥è½¯æ·±åº¦æ­£åˆ™åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–åœºæ™¯å‡ ä½•çš„æ·±åº¦æ‹‰æ™®æ‹‰æ–¯ç®—å­æ¥æƒ©ç½šä¸å¹³æ»‘çš„æ·±åº¦å˜åŒ–ã€‚ï¼ˆ4ï¼‰ï¼šå¼•å…¥å…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œé€šè¿‡å°†å±€éƒ¨æ·±åº¦å€¼å½’ä¸€åŒ–ä¸ºå…¨å±€æ·±åº¦èŒƒå›´æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡º DNGaussian æ¡†æ¶ï¼Œé€šè¿‡æ·±åº¦æ­£åˆ™åŒ–å°† 3D é«˜æ–¯è¾å°„åœºå¼•å…¥åˆ°å°‘é‡æ–°é¢–è§†å›¾åˆæˆä»»åŠ¡ä¸­ã€‚</p><p><strong>(2): æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>å¼•å…¥ç¡¬è½¯æ·±åº¦æ­£åˆ™åŒ–å’Œå…¨å±€å±€éƒ¨æ·±åº¦å½’ä¸€åŒ–ï¼Œæé«˜äº†åœºæ™¯å‡ ä½•çš„å‡†ç¡®æ€§å’Œç²¾ç»†åº¦ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨ LLFFã€DTU å’Œ Blender æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æ˜¾è‘—é™ä½å†…å­˜æˆæœ¬ã€è®­ç»ƒæ—¶é—´å‡å°‘ 25 å€å’Œæ¨ç†é€Ÿåº¦æé«˜ 3000 å€çš„æƒ…å†µä¸‹ï¼Œå–å¾—äº†å¯æ¯”æˆ–æ›´å¥½çš„ç»“æœã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä½ï¼Œå¯ä»¥å®æ—¶åˆæˆé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-dae52d7d48c393553eaefb0a09269fe0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3d64b07ef974a9326e03be048b0aa88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details>## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization**Authors:Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing**3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently. [PDF](http://arxiv.org/abs/2403.06908v1) **Summary**æ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–æŠ€æœ¯æœ‰æ•ˆè§£å†³äº† 3D é«˜æ–¯æ•£ç‚¹å›¾è¿‡åº¦é‡å»ºå¸¦æ¥çš„å›¾åƒæ¨¡ç³Šå’Œç‘•ç–µã€‚**Key Takeaways**- FreGS é‡‡ç”¨æ¸è¿›å¼é«˜æ–¯å¢å¯†ï¼Œä»ä½é¢‘åˆ°é«˜é¢‘é€å±‚ä¼˜åŒ–ã€‚- FreGS åˆ©ç”¨å‚…é‡Œå¶ç©ºé—´çš„ä½é€šå’Œé«˜é€šæ»¤æ³¢å™¨è½»æ¾æå–ä½é¢‘åˆ°é«˜é¢‘åˆ†é‡ã€‚- FreGS é€šè¿‡æœ€å°åŒ–æ¸²æŸ“å›¾åƒé¢‘è°±å’Œå¯¹åº”çœŸå®é¢‘è°±ä¹‹é—´çš„å·®å¼‚ï¼Œæå‡äº†é«˜æ–¯å¢å¯†è´¨é‡ã€‚- FreGS æœ‰æ•ˆç¼“è§£äº†é«˜æ–¯æ•£ç‚¹å›¾çš„è¿‡åº¦é‡å»ºé—®é¢˜ã€‚- FreGS åœ¨ Mip-NeRF360ã€Tanks-and-Temples å’Œæ·±åº¦æ··åˆç­‰å¤šä¸ªåŸºå‡†ä¸Šå‡å–å¾—äº†æœ€ä¼˜çš„æ–°è§†å›¾åˆæˆæ•ˆæœã€‚- FreGS å§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚- FreGS å¯¹å›¾åƒæ¨¡ç³Šå’Œç‘•ç–µå…·æœ‰å‡ºè‰²çš„æŠ‘åˆ¶æ•ˆæœã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>é¢˜ç›®ï¼šFreGSï¼šå…·æœ‰æ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–çš„ 3D é«˜æ–¯æ•£ç‚¹åŒ–</li><li>ä½œè€…ï¼šJiahui Zhangï¼ŒFangneng Zhanï¼ŒMuyu Xuï¼ŒShijian Luï¼ŒEric Xing</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå—æ´‹ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šæ–°è§†è§’åˆæˆï¼Œé«˜æ–¯æ•£ç‚¹åŒ–ï¼Œé¢‘ç‡æ­£åˆ™åŒ–</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D é«˜æ–¯æ•£ç‚¹åŒ–åœ¨å®æ—¶æ–°è§†è§’åˆæˆä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨é«˜æ–¯è‡´å¯†åŒ–è¿‡ç¨‹ä¸­ç»å¸¸ä¼šå‡ºç°è¿‡åº¦é‡å»ºï¼Œå…¶ä¸­é«˜æ–¹å·®å›¾åƒåŒºåŸŸä»…ç”±å°‘æ•°å‡ ä¸ªå¤§é«˜æ–¯ä½“è¦†ç›–ï¼Œä»è€Œå¯¼è‡´æ¸²æŸ“å›¾åƒä¸­çš„æ¨¡ç³Šå’Œä¼ªå½±ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡åŠ¨æœºæ˜ç¡®ï¼Œæå‡ºäº†æ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ– (FreGS) æŠ€æœ¯æ¥è§£å†³é¢‘ç‡ç©ºé—´ä¸­çš„è¿‡åº¦é‡å»ºé—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šFreGS é€šè¿‡åˆ©ç”¨ä½é€šå’Œé«˜é€šæ»¤æ³¢å™¨åœ¨å‚…é‡Œå¶ç©ºé—´ä¸­è½»æ¾æå–çš„ä½é¢‘åˆ°é«˜é¢‘åˆ†é‡ï¼Œæ‰§è¡Œç²—åˆ°ç²¾çš„é«˜æ–¯è‡´å¯†åŒ–ã€‚é€šè¿‡æœ€å°åŒ–æ¸²æŸ“å›¾åƒçš„é¢‘è°±ä¸ç›¸åº”çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œå®ƒå®ç°äº†é«˜è´¨é‡çš„é«˜æ–¯è‡´å¯†åŒ–ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†é«˜æ–¯æ•£ç‚¹åŒ–çš„è¿‡åº¦é‡å»ºã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å¤šä¸ªå¹¿æ³›é‡‡ç”¨çš„åŸºå‡†ï¼ˆä¾‹å¦‚ Mip-NeRF360ã€Tanks-and-Temples å’Œ DeepBlendingï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFreGS å®ç°äº†å“è¶Šçš„æ–°è§†è§’åˆæˆï¼Œå¹¶å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºæ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–ï¼ˆFreGSï¼‰æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨å‚…é‡Œå¶ç©ºé—´ä¸­æå–çš„ä½é¢‘åˆ°é«˜é¢‘åˆ†é‡ï¼Œæ‰§è¡Œç²—åˆ°ç²¾çš„é«˜æ–¯è‡´å¯†åŒ–ã€‚ï¼ˆ2ï¼‰ï¼šFreGSé€šè¿‡æœ€å°åŒ–æ¸²æŸ“å›¾åƒçš„é¢‘è°±ä¸ç›¸åº”çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œå®ç°é«˜è´¨é‡çš„é«˜æ–¯è‡´å¯†åŒ–ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†é«˜æ–¯æ•£ç‚¹åŒ–çš„è¿‡åº¦é‡å»ºã€‚ï¼ˆ3ï¼‰ï¼šè®¾è®¡é¢‘ç‡é€€ç«æŠ€æœ¯ï¼Œå®ç°æ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–ï¼Œå¯ä»¥é€æ­¥åˆ©ç”¨ä½åˆ°é«˜é¢‘åˆ†é‡æ¥æ‰§è¡Œç²—åˆ°ç²¾çš„é«˜æ–¯è‡´å¯†åŒ–ã€‚</p></li><li><p>æ€»ç»“ï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰ï¼šFreGS æå‡ºæ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»é¢‘ç‡è§†è§’æå‡ 3D é«˜æ–¯æ•£ç‚¹åŒ–ï¼Œæœ‰æ•ˆç¼“è§£äº†é«˜æ–¯æ•£ç‚¹åŒ–çš„è¿‡åº¦é‡å»ºé—®é¢˜ï¼Œåœ¨å¤šä¸ªå¹¿æ³›é‡‡ç”¨çš„å®¤å†…å¤–åœºæ™¯ä¸Šå®ç°äº†å“è¶Šçš„æ–°è§†è§’åˆæˆæ•ˆæœã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šFreGS æå‡ºæ¸è¿›å¼é¢‘ç‡æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨å‚…é‡Œå¶ç©ºé—´ä¸­æå–çš„ä½é¢‘åˆ°é«˜é¢‘åˆ†é‡ï¼Œæ‰§è¡Œç²—åˆ°ç²¾çš„é«˜æ–¯è‡´å¯†åŒ–ï¼Œæœ‰æ•ˆç¼“è§£äº†é«˜æ–¯æ•£ç‚¹åŒ–çš„è¿‡åº¦é‡å»ºé—®é¢˜ã€‚æ€§èƒ½ï¼šFreGS åœ¨å¤šä¸ªå¹¿æ³›é‡‡ç”¨çš„åŸºå‡†ä¸Šå®ç°äº†å“è¶Šçš„æ–°è§†è§’åˆæˆï¼Œå¹¶å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šFreGS çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®¾è®¡é¢‘ç‡é€€ç«æŠ€æœ¯å’Œæœ€å°åŒ–æ¸²æŸ“å›¾åƒçš„é¢‘è°±ä¸ç›¸åº”çœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-07cbe93d5240e4aa795cfc2554b29280.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c725f327a32c127deea0c454f4062887.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ddb9b45e2c546000557a3be13e0a4a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f440ba30a1f4e263c32265e76b8e0898.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3063a8cf69313732153e2186dcdf414d.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ä¿ƒè¿› 3D ç”Ÿæˆï¼Œå¹¶é€šè¿‡å‡ ä½•ä¸€è‡´æ€§å…ˆéªŒå’Œå¤šè§†å›¾ä¸€è‡´ 3D ç”Ÿæˆå™¨æ‰©å±•è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è‡ªåŠ¨ 3D ç”Ÿæˆå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ç”±äºæ¨¡å‹å®¹é‡æˆ– 3D æ•°æ®é™åˆ¶è€Œäº§ç”Ÿç»†èŠ‚è¾ƒå°‘çš„ç‰©ä½“ã€‚</li><li>V3D åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›æ¥ä¿ƒè¿› 3D ç”Ÿæˆã€‚</li><li>å‡ ä½•ä¸€è‡´æ€§å…ˆéªŒå’Œå¤šè§†å›¾ä¸€è‡´ 3D ç”Ÿæˆå™¨å……åˆ†å‘æŒ¥è§†é¢‘æ‰©æ•£æ„ŸçŸ¥ 3D ä¸–ç•Œçš„æ½œåŠ›ã€‚</li><li>åªéœ€ä¸€å¼ å›¾ç‰‡ï¼Œå³å¯å¾®è°ƒæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå›´ç»•ç‰©ä½“ 360 åº¦æ—‹è½¬çš„è½¨é“å¸§ã€‚</li><li>å€ŸåŠ©å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œå¯åœ¨ 3 åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„ç½‘æ ¼æˆ– 3D é«˜æ–¯ä½“ã€‚</li><li>è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°åœºæ™¯çº§æ–°é¢–è§†å›¾åˆæˆï¼Œä½¿ç”¨ç¨€ç–è¾“å…¥è§†å›¾å¯¹ç›¸æœºè·¯å¾„è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šV3Dï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ 3D ç”Ÿæˆå™¨</li><li>ä½œè€…ï¼šZilong Chen, Yikai Wangâ€ , Feng Wang, Zhengyi Wang, Huaping Liuâ€ </li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼š3D ç”Ÿæˆï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¤šè§†å›¾é‡å»º</li><li>è®ºæ–‡é“¾æ¥ï¼šarxiv.org/abs/2403.06738   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨ 3D ç”Ÿæˆå·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚è¿‘æœŸæ–¹æ³•æå¤§åœ°æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œä½†ç”±äºæ¨¡å‹å®¹é‡æœ‰é™ï¼Œé€šå¸¸ä¼šäº§ç”Ÿç»†èŠ‚è¾ƒå°‘çš„ç‰©ä½“ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šè¿‡å»æ–¹æ³•åŒ…æ‹¬åŸºäºéšå¼ç¥ç»è¡¨ç¤ºå’ŒåŸºäºæ˜¾å¼ç½‘æ ¼è¡¨ç¤ºçš„æ–¹æ³•ã€‚å‰è€…ç”Ÿæˆé€Ÿåº¦å¿«ï¼Œä½†ç»†èŠ‚è¾ƒå°‘ï¼›åè€…ç»†èŠ‚ä¸°å¯Œï¼Œä½†ç”Ÿæˆé€Ÿåº¦æ…¢ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º V3Dï¼Œä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ 3D ç”Ÿæˆæ–¹æ³•ã€‚V3D å°† 2D å›¾åƒåºåˆ—æ‰©æ•£åˆ° 3D ç©ºé—´ï¼Œç”Ÿæˆé«˜ä¿çœŸ 3D ç‰©ä½“ã€‚   ï¼ˆ4ï¼‰æ€§èƒ½ï¼šåœ¨ ShapeNet æ•°æ®é›†ä¸Šï¼ŒV3D åœ¨ç”Ÿæˆé€Ÿåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚V3D å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ 3D ç‰©ä½“ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€ 3 åˆ†é’Ÿã€‚</li></ol><p><methods>:(1): V3Då°†2Då›¾åƒåºåˆ—æ‰©æ•£åˆ°3Dç©ºé—´ï¼Œç”Ÿæˆé«˜ä¿çœŸ3Dç‰©ä½“ã€‚(2): V3Dä½¿ç”¨åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œå°†2Då›¾åƒåºåˆ—é€å¸§æ‰©æ•£åˆ°3Dç©ºé—´ä¸­ã€‚(3): V3Dé‡‡ç”¨å¤šè§†å›¾é‡å»ºæŠ€æœ¯ï¼Œä»ä¸åŒè§†è§’ç”Ÿæˆ2Då›¾åƒåºåˆ—ï¼Œæé«˜3Dç‰©ä½“çš„ç»†èŠ‚ä¸°å¯Œåº¦ã€‚</methods></p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ æœ¬å·¥ä½œé€šè¿‡å°†å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3D ç”Ÿæˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³• V3Dï¼Œæ˜¾è‘—æå‡äº† 3D ç‰©ä½“çš„ç”Ÿæˆé€Ÿåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚V3D ä¸ä»…èƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„ 3D ç‰©ä½“ï¼Œè¿˜èƒ½å®ç°åœºæ™¯çº§çš„æ–°è§†è§’åˆæˆï¼Œä¸ºé«˜ä¿çœŸ 3D ç”Ÿæˆå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ 3D ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰ åˆ›æ–°ç‚¹ï¼š</li><li>å°†è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3D ç”Ÿæˆï¼Œé€šè¿‡å°† 2D å›¾åƒåºåˆ—æ‰©æ•£åˆ° 3D ç©ºé—´ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆé€Ÿåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦ã€‚</li><li>æå‡ºäº†ä¸€ç§é‡èº«å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„åˆå§‹åŒ–å’Œçº¹ç†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨ 3 åˆ†é’Ÿå†…é‡å»ºé«˜è´¨é‡çš„ 3D é«˜æ–¯ä½“æˆ–ç²¾ç»†çº¹ç†ç½‘æ ¼ã€‚</li><li>å°†è¯¥æ¡†æ¶æ‰©å±•åˆ°åœºæ™¯çº§çš„æ–°è§†è§’åˆæˆï¼Œå®ç°äº†å¯¹æ‘„åƒæœºè·¯å¾„çš„ç²¾ç¡®æ§åˆ¶å’Œå‡ºè‰²çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ ShapeNet æ•°æ®é›†ä¸Šï¼ŒV3D åœ¨ç”Ÿæˆé€Ÿåº¦å’Œç»†èŠ‚ä¸°å¯Œåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>V3D èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D ç‰©ä½“ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€ 3 åˆ†é’Ÿã€‚</li><li>V3D åœ¨åœºæ™¯çº§æ–°è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†å¯¹æ‘„åƒæœºè·¯å¾„çš„ç²¾ç¡®æ§åˆ¶å’Œå‡ºè‰²çš„å¤šè§†è§’ä¸€è‡´æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>V3D çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚</li><li>V3D çš„è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆï¼Œåœ¨å•å¼  NVIDIA A100 GPU ä¸Šä»…éœ€æ•°å°æ—¶å³å¯å®Œæˆã€‚</li><li>V3D çš„æ¨ç†é€Ÿåº¦å¿«ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„ 3D ç‰©ä½“æˆ–åˆæˆæ–°è§†è§’ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  StyleGaussian Instant 3D Style Transfer with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/</id>
    <published>2024-03-13T05:53:10.000Z</published>
    <updated>2024-03-13T05:53:10.797Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-13-æ›´æ–°"><a href="#2024-03-13-æ›´æ–°" class="headerlink" title="2024-03-13 æ›´æ–°"></a>2024-03-13 æ›´æ–°</h1><h2 id="A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos"><a href="#A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos" class="headerlink" title="A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos"></a>A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</h2><p><strong>Authors:Weixia Zhang, Chengguang Zhu, Jingnan Gao, Yichao Yan, Guangtao Zhai, Xiaokang Yang</strong></p><p>The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications. However, performance evaluation research lags behind the development of talking head generation techniques. Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment. To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness. Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures. We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context. Code and data will be made available at <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a>. </p><p><a href="http://arxiv.org/abs/2403.06421v1">PDF</a> </p><p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å‘å±•æ¨åŠ¨äº†éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡ç”ŸæˆæŠ€æœ¯ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶å…³æ³¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä¿ƒè¿›äº†éŸ³é¢‘é©±åŠ¨çš„è™šæ‹Ÿå½¢è±¡ç”Ÿæˆã€‚</li><li>ç°æœ‰çš„è™šæ‹Ÿå½¢è±¡ç”ŸæˆæŠ€æœ¯è¯„ä»·æŒ‡æ ‡ä¾èµ–å¯å‘å¼å®šé‡æŒ‡æ ‡ï¼Œç¼ºä¹äººä¸ºéªŒè¯ï¼Œé˜»ç¢äº†å‡†ç¡®çš„è¿›åº¦è¯„ä¼°ã€‚</li><li>æ”¶é›†äº†å››ç§ç”Ÿæˆæ–¹æ³•ç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡è§†é¢‘ï¼Œå¹¶å¯¹è§†è§‰è´¨é‡ã€å”‡éŸ³åŒæ­¥å’Œå¤´éƒ¨è¿åŠ¨è‡ªç„¶åº¦è¿›è¡Œäº†æ§åˆ¶çš„å¿ƒç†ç‰©ç†å®éªŒã€‚</li><li>å®éªŒéªŒè¯äº†æ¨¡å‹é¢„æµ‹å’Œäººä¸ºæ ‡æ³¨çš„ä¸€è‡´æ€§ï¼Œç¡®å®šäº†æ¯”å¹¿æ³›ä½¿ç”¨çš„åº¦é‡æ›´ç¬¦åˆäººæ„è§çš„åº¦é‡ã€‚</li><li>è¯¥ç ”ç©¶å°†ä¿ƒè¿›ç»©æ•ˆè¯„ä¼°å’Œæ¨¡å‹å¼€å‘ï¼Œä¸ºæ›´å¹¿æ³›èƒŒæ™¯ä¸‹çš„ AIGC æä¾›æ·±å…¥è§è§£ã€‚</li><li>ä»£ç å’Œæ•°æ®å°†åœ¨ <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a> ä¸Šæä¾›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.æ ‡é¢˜ï¼šéŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡çš„æ¯”è¾ƒç ”ç©¶2.ä½œè€…ï¼šé­éœç« ã€ç¨‹å¹¿æŸ±ã€æ™¯å—é«˜ã€å¥•è¶…é¢œã€å¹¿æ¶›ç¿Ÿã€è‚–åº·æ¨3.ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ã€äººå·¥æ™ºèƒ½ç ”ç©¶é™¢4.å…³é”®è¯ï¼šæ„ŸçŸ¥è´¨é‡è¯„ä¼°ã€AIGCã€æ•°å­—äººã€éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ5.è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.06421Githubä»£ç é“¾æ¥ï¼šNone6.æ€»ç»“ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒéŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”ŸæˆæŠ€æœ¯å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ€§èƒ½è¯„ä¼°ç ”ç©¶æ»åäºè¯´è¯äººå¤´éƒ¨ç”ŸæˆæŠ€æœ¯çš„å¼€å‘ã€‚ç°æœ‰æ–‡çŒ®ä¾èµ–å¯å‘å¼å®šé‡æŒ‡æ ‡ï¼Œç¼ºä¹äººå·¥éªŒè¯ï¼Œé˜»ç¢äº†å‡†ç¡®çš„è¿›å±•è¯„ä¼°ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–å¯å‘å¼å®šé‡æŒ‡æ ‡ï¼Œå¦‚PSNRã€SSIMå’ŒLMDï¼Œè¿™äº›æŒ‡æ ‡åœ¨æ²¡æœ‰äººå·¥éªŒè¯çš„æƒ…å†µä¸‹è¢«ç”¨ä½œæ„ŸçŸ¥è´¨é‡çš„ä»£ç†æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ï¼Œä¾‹å¦‚å¯¹æ•°æ®æºçš„æ•æ„Ÿæ€§å’Œå¯¹äººç±»æ„ŸçŸ¥çš„ä¸åŒ¹é…ã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æ”¶é›†äº†å››ç§ç”Ÿæˆæ–¹æ³•ç”Ÿæˆçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘ï¼Œå¹¶åœ¨å—æ§å®éªŒå®¤ç¯å¢ƒä¸­è¿›è¡Œå¿ƒç†ç‰©ç†å®éªŒï¼Œé‡ç‚¹å…³æ³¨è§†è§‰è´¨é‡ã€å”‡éŸ³åŒæ­¥å’Œå¤´éƒ¨è¿åŠ¨è‡ªç„¶åº¦ã€‚ç„¶åï¼Œå¯¹å„ç§å®¢è§‚æŒ‡æ ‡è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œä»¥è¯„ä¼°å…¶ä¸è¿™äº›äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸äººç±»åˆ¤æ–­ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ï¼Œå¹¶ç¡®å®šäº†æ¯”å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡æ›´ç¬¦åˆäººç±»æ„è§çš„æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºä¿ƒè¿›æ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹å¼€å‘ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›èƒŒæ™¯ä¸‹çš„AIGCæä¾›è§è§£ã€‚</p><p><strong>æ–¹æ³•ï¼š</strong></p><p>(1) æ”¶é›†å››ç§ç”Ÿæˆæ–¹æ³•ç”Ÿæˆçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨è§†é¢‘ï¼›</p><p>(2) åœ¨å—æ§å®éªŒå®¤ç¯å¢ƒä¸­è¿›è¡Œå¿ƒç†ç‰©ç†å®éªŒï¼Œé‡ç‚¹å…³æ³¨è§†è§‰è´¨é‡ã€å”‡éŸ³åŒæ­¥å’Œå¤´éƒ¨è¿åŠ¨è‡ªç„¶åº¦ï¼›</p><p>(3) å¹¿æ³›æµ‹è¯•å„ç§å®¢è§‚æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°å…¶ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼›</p><p>(4) é€šè¿‡ 2AFC åˆ†æ•°è¡¡é‡å®¢è§‚æŒ‡æ ‡ä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§ï¼›</p><p>(5) è¯„ä¼°å›¾åƒè´¨é‡ã€å”‡éŸ³åŒæ­¥å’Œå¤´éƒ¨è¿åŠ¨è‡ªç„¶åº¦æŒ‡æ ‡ï¼›</p><p>(6) è¯„ä¼°åŸºäº SyncNet çš„ä¸‰ä¸ªå”‡éŸ³åŒæ­¥æŒ‡æ ‡å’Œ SparseSync æŒ‡æ ‡ï¼›</p><p>(7) é‡‡ç”¨æ··åˆæ•°æ®é›†è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„å¯è½¬ç§»æ€§ã€‚</p><p><strong>8. ç»“è®º</strong><strong>(1): æ„ä¹‰</strong>æœ¬ç ”ç©¶é€šè¿‡å»ºç«‹åŒ…å«å››ç§éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•ç”Ÿæˆè§†é¢‘çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å—æ§çš„å¿ƒç†ç‰©ç†å®éªŒæ”¶é›†äººç±»åå¥½ï¼Œæ¢ç©¶äº†éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”ŸæˆæŠ€æœ¯çš„æ„ŸçŸ¥è´¨é‡è¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸äººç±»åˆ¤æ–­ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ï¼Œå¹¶ç¡®å®šäº†æ¯”å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡æ›´ç¬¦åˆäººç±»æ„è§çš„æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºä¿ƒè¿›æ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹å¼€å‘ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›èƒŒæ™¯ä¸‹çš„ AIGC æä¾›è§è§£ã€‚</p><p><strong>(2): åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong>* é€šè¿‡å¿ƒç†ç‰©ç†å®éªŒæ”¶é›†äººç±»åå¥½ï¼Œå»ºç«‹åŒ…å«å››ç§ç”Ÿæˆæ–¹æ³•ç”Ÿæˆè§†é¢‘çš„æ•°æ®é›†ã€‚* æå‡ºäº†ä¸€ç§åŸºäºäººç±»åˆ¤æ–­çš„æ„ŸçŸ¥è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚* ç¡®å®šäº†æ¯”å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡æ›´ç¬¦åˆäººç±»æ„è§çš„æŒ‡æ ‡ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* æ‰€æå‡ºçš„æ–¹æ³•ä¸äººç±»åˆ¤æ–­ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ã€‚* ç¡®å®šçš„æŒ‡æ ‡æ¯”å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡æ›´ç¬¦åˆäººç±»æ„è§ã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* æ”¶é›†äº†ä¸€ä¸ªåŒ…å«å››ç§ç”Ÿæˆæ–¹æ³•ç”Ÿæˆè§†é¢‘çš„æ•°æ®é›†ã€‚* è¿›è¡Œäº†ä¸€ç³»åˆ—å¿ƒç†ç‰©ç†å®éªŒã€‚* å¹¿æ³›æµ‹è¯•äº†å„ç§å®¢è§‚æŒ‡æ ‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d7d375dcb8fecf9ffb80be0b9c71756b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-478998a50c784c3a3c0aa108c509fe52.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4aaac273c5b4afe45da700d10d5ac29c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f16882204804b40a491523a7984bf7e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5201c94e6142ff9aad05ce654fbe8f9e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-523c101252b751fc24de4e576389177a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e67dfafe83349d242d664f46c153e84.jpg" align="middle"></details><h2 id="Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style"><a href="#Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style" class="headerlink" title="Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style"></a>Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style</h2><p><strong>Authors:Shuai Tan, Bin Ji, Ye Pan</strong></p><p>Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style. </p><p><a href="http://arxiv.org/abs/2403.06365v2">PDF</a> 9 pages, 5 figures, conference</p><p><strong>Summary</strong><br>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•Style2Talkerï¼Œå®ç°äº†æƒ…æ„Ÿé£æ ¼å’Œè‰ºæœ¯é£æ ¼ï¼Œæé«˜äº†è§†é¢‘è¡¨è¾¾æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>Style2Talkerå¼•å…¥Style-Eå’ŒStyle-Aä¸¤ä¸ªé£æ ¼åŒ–é˜¶æ®µï¼Œåˆ†åˆ«æ•´åˆæƒ…æ„Ÿé£æ ¼å’Œè‰ºæœ¯é£æ ¼ã€‚</li><li>æå‡ºæ— äººå·¥å¹²é¢„çš„èŒƒå¼ï¼Œè‡ªåŠ¨ä¸ºç°æœ‰è§†éŸ³é¢‘æ•°æ®é›†æ ‡æ³¨æƒ…æ„Ÿæ–‡æœ¬æ ‡ç­¾ã€‚</li><li>åˆ©ç”¨CLIPæ¨¡å‹æå–æƒ…æ„Ÿç‰¹å¾ï¼Œç»“åˆéŸ³é¢‘ä½œä¸ºé«˜æ•ˆæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ï¼Œç”Ÿæˆ3DMMæ¨¡å‹çš„æƒ…æ„Ÿè¿åŠ¨ç³»æ•°ã€‚</li><li>å¼€å‘ç³»æ•°é©±åŠ¨çš„è¿åŠ¨ç”Ÿæˆå™¨å’ŒåµŒå…¥åœ¨StyleGANä¸­çš„è‰ºæœ¯é£æ ¼è·¯å¾„ï¼Œåˆæˆé«˜åˆ†è¾¨ç‡è‰ºæœ¯é£æ ¼çš„å¤´éƒ¨è§†é¢‘ã€‚</li><li>å¼•å…¥å¤šå°ºåº¦å†…å®¹ç‰¹å¾å’Œå†…å®¹ç¼–ç å™¨ã€ç²¾ç‚¼ç½‘ç»œï¼Œæå‡å›¾åƒç»†èŠ‚å’Œå‡å°‘ä¼ªå½±ã€‚</li><li>Style2Talkeråœ¨éŸ³è§†é¢‘åŒæ­¥ã€æƒ…æ„Ÿå’Œè‰ºæœ¯é£æ ¼è¡¨ç°æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šStyle2Talkerï¼šå…¼å…·æƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼çš„é«˜åˆ†è¾¨ç‡è¯´è¯äººå¤´éƒ¨ç”Ÿæˆ</li><li>ä½œè€…ï¼šShuai Tan, Bin Ji, Ye Pan</li><li>å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨ã€è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€æƒ…ç»ªé£æ ¼ã€è‰ºæœ¯é£æ ¼ã€æ–‡æœ¬æ§åˆ¶ã€å›¾åƒæ§åˆ¶</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.06365</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨ç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨è§†é¢‘è¿‘å¹´æ¥å¤‡å—å…³æ³¨ï¼Œä½†ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å®ç°éŸ³é¢‘å”‡å½¢åŒæ­¥ï¼Œå¿½è§†äº†ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›è§†é¢‘çš„ä¸¤ä¸ªå…³é”®å…ƒç´ ï¼šæƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•è¦ä¹ˆä½¿ç”¨å•ä¸€çš„çƒ­æƒ…ç»ªæ ‡ç­¾ä½œä¸ºæƒ…ç»ªæºï¼Œé™åˆ¶äº†è¡¨æƒ…èŒƒå›´ï¼Œè¦ä¹ˆä¾èµ–é¢å¤–çš„è¡¨æƒ…è§†é¢‘ï¼Œè¿™å¯èƒ½ä¸æ–¹ä¾¿ã€‚æ­¤å¤–ï¼Œè™½ç„¶å•å¹…å›¾åƒé£æ ¼è¿ç§»å·²æœ‰å¤§é‡ç ”ç©¶ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆç”±éŸ³é¢‘é©±åŠ¨çš„è¿ç»­è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººé¢éƒ¨ç”Ÿæˆæ–¹æ³• Style2Talkerã€‚å®ƒåŒ…æ‹¬ä¸¤ä¸ªé£æ ¼åŒ–é˜¶æ®µï¼šStyle-E å’Œ Style-Aï¼Œåˆ†åˆ«å°†æ–‡æœ¬æ§åˆ¶çš„æƒ…ç»ªé£æ ¼å’Œå›¾åƒæ§åˆ¶çš„è‰ºæœ¯é£æ ¼é›†æˆåˆ°æœ€ç»ˆè¾“å‡ºä¸­ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒStyle2Talker åœ¨éŸ³é¢‘å”‡å½¢åŒæ­¥ã€æƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼çš„æ€§èƒ½æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³ç”Ÿæˆå…¼å…·æƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼çš„é«˜åˆ†è¾¨ç‡è¯´è¯äººå¤´éƒ¨è§†é¢‘ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ–‡æœ¬æ§åˆ¶çš„æƒ…ç»ªé£æ ¼è¿ç§»ï¼ˆStyle-Eï¼‰ï¼šå°†æ–‡æœ¬æ§åˆ¶çš„æƒ…ç»ªæ ‡ç­¾è½¬æ¢ä¸º 3DMM ç³»æ•°åºåˆ—ï¼Œå¹¶åˆ©ç”¨ StyleGAN ç”Ÿæˆå…·æœ‰ç›¸åº”æƒ…ç»ªé£æ ¼çš„å›¾åƒåºåˆ—ã€‚ï¼ˆ2ï¼‰å›¾åƒæ§åˆ¶çš„è‰ºæœ¯é£æ ¼è¿ç§»ï¼ˆStyle-Aï¼‰ï¼šå¼•å…¥ ModResBlock è°ƒæ•´ StyleGAN çš„ç»“æ„é£æ ¼ï¼Œå¹¶åˆ©ç”¨è¿åŠ¨ç”Ÿæˆå™¨ Gm å°†é¢„æµ‹çš„è¿åŠ¨åºåˆ—è½¬æ¢ä¸ºç©ºé—´ç‰¹å¾å›¾ï¼Œä»è€Œå®ç°è‰ºæœ¯é£æ ¼çš„è¿ç§»ã€‚ï¼ˆ3ï¼‰å†…å®¹ç¼–ç å™¨å’Œç»†åŒ–ç½‘ç»œï¼šé‡‡ç”¨å†…å®¹ç¼–ç å™¨ Ec æå–å¤šå°ºåº¦å†…å®¹ç‰¹å¾ï¼Œé€šè¿‡è·³è·ƒè¿æ¥è¡¥å……çº¹ç†ç»†èŠ‚ï¼›å¼•å…¥ç»†åŒ–ç½‘ç»œ R è°ƒæ•´ç©ºé—´ç‰¹å¾å›¾ï¼Œæ¶ˆé™¤é‡å½±ä¼ªå½±ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„æ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§åˆ›æ–°çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³• Style2Talkerï¼Œè¯¥æ–¹æ³•é€šè¿‡èåˆç›¸åº”çš„é£æ ¼æç¤ºï¼Œç”Ÿæˆå…¼å…·æƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼çš„é«˜åˆ†è¾¨ç‡è¯´è¯äººå¤´éƒ¨è§†é¢‘ã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å…äººå·¥æ–‡æœ¬æ ‡æ³¨ç®¡é“ï¼Œä»æ–‡æœ¬è¾“å…¥ä¸­è·å–ç”¨äºå­¦ä¹ æƒ…ç»ªé£æ ¼çš„æ–‡æœ¬æè¿°ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å°è¯•èƒ½æ¿€å‘æ›´æ·±å…¥çš„ç ”ç©¶ï¼Œåˆ©ç”¨å‡ºè‰²çš„ã€å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ›´å®ç”¨ã€æ›´å¼•äººå…¥èƒœçš„æ¢ç´¢ã€‚ä¸ºäº†å°†æƒ…ç»ªé£æ ¼æ³¨å…¥åˆ° 3D è¿åŠ¨ç³»æ•°ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰å¤šä¸ªç¼–ç å™¨ï¼Œç¡®ä¿ç”Ÿæˆé€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„é¢éƒ¨è¡¨æƒ…ã€‚æˆ‘ä»¬å°†ä¸€ä¸ªæƒ…ç»ªé©±åŠ¨æ¨¡å—å’Œä¸€ä¸ªé¢å¤–çš„è‰ºæœ¯é£æ ¼è·¯å¾„çº³å…¥ StyleGAN æ¶æ„ä¸­ï¼Œä»è€Œå®ç°ç³»æ•°é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆï¼Œå¹¶å…·æœ‰æœŸæœ›çš„æƒ…ç»ªå’Œè‰ºæœ¯é£æ ¼ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè§†è§‰è´¨é‡å¹¶æ¶ˆé™¤ä¼ªå½±ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å†…å®¹ç¼–ç å™¨å’Œç»†åŒ–ç½‘ç»œã€‚å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆæ›´å¤šé£æ ¼åŒ–çš„åŠ¨ç”»ç»“æœã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬çš„å…äººå·¥æƒ…ç»ªæ ‡ç­¾è·å–ç®¡é“ï¼Œç”¨äºå­¦ä¹ æƒ…ç»ªé£æ ¼ã€‚</li><li>è®¾è®¡äº†ä¸€ä¸ªå¤šç¼–ç å™¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå°†æ–‡æœ¬æ§åˆ¶çš„æƒ…ç»ªæ ‡ç­¾è½¬æ¢ä¸º 3D è¿åŠ¨ç³»æ•°ï¼Œä»è€Œç”Ÿæˆé€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„é¢éƒ¨è¡¨æƒ…ã€‚</li><li>åœ¨ StyleGAN æ¶æ„ä¸­èåˆäº†ä¸€ä¸ªæƒ…ç»ªé©±åŠ¨æ¨¡å—å’Œä¸€ä¸ªé¢å¤–çš„è‰ºæœ¯é£æ ¼è·¯å¾„ï¼Œå®ç°ç³»æ•°é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆï¼Œå¹¶å…·æœ‰æœŸæœ›çš„æƒ…ç»ªå’Œè‰ºæœ¯é£æ ¼ã€‚</li><li>é‡‡ç”¨äº†ä¸€ä¸ªå†…å®¹ç¼–ç å™¨å’Œä¸€ä¸ªç»†åŒ–ç½‘ç»œï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºè§†è§‰è´¨é‡å¹¶æ¶ˆé™¤ä¼ªå½±ã€‚æ€§èƒ½ï¼š</li><li>åœ¨éŸ³é¢‘å”‡å½¢åŒæ­¥ã€æƒ…ç»ªé£æ ¼å’Œè‰ºæœ¯é£æ ¼æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š</li><li>æ–‡æœ¬æ ‡æ³¨å·¥ä½œé‡ä½ï¼Œå› ä¸ºåˆ©ç”¨äº†åŸºäºå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å…äººå·¥æ–‡æœ¬æ ‡æ³¨ç®¡é“ã€‚</li><li>æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æˆæœ¬è¾ƒé«˜ï¼Œå› ä¸ºä½¿ç”¨äº† StyleGAN å’Œæ‰©æ•£æ¨¡å‹ç­‰å¤æ‚æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d09922b44587a2c7a0d9914314bc2819.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7a916164c4c80e4c155763e1f38efcd1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0049142b2593b96773c9362d691fff94.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3380ba10087f173dca5f8c5d5df37735.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/</id>
    <published>2024-03-13T05:45:36.000Z</published>
    <updated>2024-03-13T05:45:36.542Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-13-æ›´æ–°"><a href="#2024-03-13-æ›´æ–°" class="headerlink" title="2024-03-13 æ›´æ–°"></a>2024-03-13 æ›´æ–°</h1><h2 id="Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation"><a href="#Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation" class="headerlink" title="Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation"></a>Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</h2><p><strong>Authors:Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</strong></p><p>Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge">https://github.com/ShihaoZhaoZSH/LaVi-Bridge</a>. </p><p><a href="http://arxiv.org/abs/2403.07860v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæ¢ç´¢ç”¨æ›´å…ˆè¿›çš„è¯­è¨€å’Œå¤§è§„æ¨¡è§†è§‰æ¨¡å‹æ›¿æ¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç»„æˆéƒ¨åˆ†ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œå°†è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆåˆ°ä¸€ä¸ªç®¡é“ä¸­ã€‚</li><li>LaVi-Bridgeç®¡é“ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°é›†æˆã€‚</li><li>ä½¿ç”¨LaVi-Bridgeå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹çš„åŸå§‹æƒé‡ã€‚</li><li>LaVi-Bridgeä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œå¯é€‚åº”ä¸åŒçš„ç»“æ„ã€‚</li><li>å°†æ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ä¸LaVi-Bridgeé›†æˆå¯ä»¥æé«˜æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ã€‚</li><li>å¹¿æ³›çš„è¯„ä¼°éªŒè¯äº†LaVi-Bridgeçš„æœ‰æ•ˆæ€§ã€‚</li><li>ä»£ç å¯åœ¨<a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridgeè·å¾—ã€‚">https://github.com/ShihaoZhaoZSH/LaVi-Bridgeè·å¾—ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šBridging Different Language Models and Generative Vision Models for Text-to-Image Generation</li><li>ä½œè€…ï¼šShihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯å¤§å­¦</li><li>å…³é”®è¯ï¼šDiffusion model, Text-to-image generation</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.07860</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ç”±ä¸€ä¸ªè§£é‡Šç”¨æˆ·æç¤ºçš„è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªç”Ÿæˆç›¸åº”å›¾åƒçš„è§†è§‰æ¨¡å‹ç»„æˆã€‚éšç€è¯­è¨€å’Œè§†è§‰æ¨¡å‹åœ¨å…¶å„è‡ªé¢†åŸŸä¸æ–­è¿›æ­¥ï¼Œæ¢ç´¢ç”¨æ›´å…ˆè¿›çš„æ¨¡å‹æ›¿æ¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç»„ä»¶å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å› æ­¤ï¼Œä¸€ä¸ªæ›´å¹¿æ³›çš„ç ”ç©¶ç›®æ ‡æ˜¯ç ”ç©¶å°†ä»»ä½•ä¸¤ä¸ªä¸ç›¸å…³çš„è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šéœ€è¦ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡ï¼Œçµæ´»æ€§å·®ï¼Œæ— æ³•é€‚åº”ä¸åŒçš„ç»“æ„ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† LaVi-Bridgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒå°†ä¸åŒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚é€šè¿‡åˆ©ç”¨ LoRA å’Œé€‚é…å™¨ï¼ŒLaVi-Bridge æä¾›äº†ä¸€ç§çµæ´»ä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œæ— éœ€ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œå¯é€‚åº”ä¸åŒçš„ç»“æ„ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨è¯¥æ¡†æ¶å†…ï¼Œæˆ‘ä»¬è¯æ˜äº†ç»“åˆæ›´é«˜çº§çš„æ¨¡å—ï¼ˆä¾‹å¦‚æ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ï¼‰å¯ä»¥æ˜¾ç€æé«˜æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ç­‰èƒ½åŠ›ã€‚å·²ç»è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°æ¥éªŒè¯ LaVi-Bridge çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): é‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨LoRAå’Œé€‚é…å™¨å°†ä¸åŒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆï¼Œæ— éœ€ä¿®æ”¹åŸå§‹æƒé‡ã€‚(2): è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹çš„äº¤äº’é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚å®ç°ï¼ŒLoRAå¼•å…¥å¯è®­ç»ƒå‚æ•°ï¼Œé€‚é…å™¨ä¿ƒè¿›å¯¹é½ã€‚(3): ä¿æŒè¯­è¨€å’Œè§†è§‰æ¨¡å‹å›ºå®šï¼Œä»…è®­ç»ƒ LoRA å’Œé€‚é…å™¨å‚æ•°ï¼Œé€‚åº”å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ç»“æ„ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºLaVi-Bridgeï¼Œå®ƒé€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚LaVi-Bridgeèƒ½å¤Ÿè¿æ¥å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚å®ƒå…·æœ‰é«˜åº¦é€šç”¨æ€§ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„ç»“æ„ã€‚LaVi-Bridgeè¿˜å¾ˆçµæ´»ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨ä¸ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡çš„åŸºç¡€ä¸Šå®ç°é›†æˆã€‚ç›¸åï¼Œå®ƒåˆ©ç”¨LoRAå’Œé€‚é…å™¨è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œåœ¨LaVi-Bridgeä¸‹ï¼Œä½¿ç”¨æ›´é«˜çº§çš„è¯­è¨€æˆ–è§†è§‰æ¨¡å‹å¯ä»¥å¢å¼ºæ–‡æœ¬ç†è§£èƒ½åŠ›æˆ–å›¾åƒè´¨é‡ã€‚è¿™äº›ä¼˜åŠ¿ä½¿å¾—LaVi-Bridgeèƒ½å¤Ÿå¸®åŠ©æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹ä»»åŠ¡å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚LaVi-Bridgeå…è®¸è®¾è®¡å¸ˆã€è‰ºæœ¯å®¶å’Œå…¶ä»–ç”¨æˆ·çµæ´»åœ°åˆ©ç”¨ç°æœ‰çš„è¯­è¨€å’Œè§†è§‰æ¨¡å‹æ¥å®ç°ä»–ä»¬çš„åˆ›ä½œç›®æ ‡ã€‚é¿å…æ»¥ç”¨å¹¶å‡è½»æ½œåœ¨çš„è´Ÿé¢ç¤¾ä¼šå½±å“è‡³å…³é‡è¦ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œé‡è¦çš„æ˜¯è¦æ ‡å‡†åŒ–å…¶ä½¿ç”¨ï¼Œæé«˜æ¨¡å‹é€æ˜åº¦ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šLaVi-Bridgeæå‡ºäº†ä¸€ç§æ— éœ€ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹åŸå§‹æƒé‡å³å¯å°†ä¸åŒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç®¡é“ã€‚å®ƒåˆ©ç”¨LoRAå’Œé€‚é…å™¨åœ¨è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹ä¹‹é—´å»ºç«‹äº†å¯è®­ç»ƒçš„è¿æ¥ï¼Œä»è€Œå®ç°äº†çµæ´»ä¸”å³æ’å³ç”¨çš„é›†æˆã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒLaVi-Bridgeèƒ½å¤Ÿæ˜¾ç€æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ã€‚é€šè¿‡ç»“åˆæ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ï¼ŒLaVi-Bridgeå¯ä»¥åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚å·¥ä½œé‡ï¼šLaVi-Bridgeçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œåªéœ€è¦ä¿®æ”¹å°‘é‡ä»£ç å³å¯ã€‚å®ƒä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œæ— éœ€å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚æ­¤å¤–ï¼ŒLaVi-Bridgeçš„è®­ç»ƒè¿‡ç¨‹æ˜¯é«˜æ•ˆä¸”ç¨³å®šçš„ï¼Œå¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…æ”¶æ•›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f9a99e7e4272d38b21737a5c189b093a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-57e7ed33741950bb510e73e466f417ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-28925ac45e275e43cd57ccf0dd749a77.jpg" align="middle"></details>## Quantifying and Mitigating Privacy Risks for Tabular Generative Models**Authors:Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. PÃ©rez, Marten van Dijk, Lydia Y. Chen**Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk. [PDF](http://arxiv.org/abs/2403.07842v1) **Summary**ç”Ÿæˆæ¨¡å‹ä¸­çš„åˆæˆæ•°æ®æ˜¯ä¿æŠ¤æ•°æ®éšç§çš„æ•°æ®å…±äº«è§£å†³æ–¹æ¡ˆï¼Œæ—¢è¦è¿‘ä¼¼åŸå§‹æ•°æ®ï¼Œåˆä¸èƒ½æ³„éœ²å¯è¯†åˆ«çš„ç§äººä¿¡æ¯ã€‚**Key Takeaways**- åˆæˆæ•°æ®ç”Ÿæˆå™¨æŠ€æœ¯æºäºå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¦‚ GAN å’Œæ‰©æ•£æ¨¡å‹ã€‚- è¡¨æ ¼æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®è´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éšç§æ–¹é¢å­˜åœ¨é£é™©ã€‚- DP-TLDMï¼ˆå·®å¼‚éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼‰é€šè¿‡ç¼–ç å™¨ç½‘ç»œå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ•°æ®ã€‚- DP-SGDã€æ‰¹æ¬¡è£å‰ªå’Œåˆ†ç¦»å€¼å¯ç”¨äºå¢å¼ºéšç§ä¿éšœã€‚- DP-TLDM å¯æœ‰æ•ˆæå‡åˆæˆæ•°æ®è´¨é‡å’Œæ•ˆç”¨ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„éšç§é£é™©ã€‚- DP-TLDM å¯å°†æ•°æ®ç›¸ä¼¼æ€§æé«˜ 35%ã€ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨æé«˜ 15%ã€æ•°æ®å¯åŒºåˆ†æ€§æé«˜ 50%ã€‚- DP-TLDM åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶æé«˜äº†æ•°æ®æ•ˆç”¨ï¼Œä¼˜äºå…¶ä»– DP è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>è®ºæ–‡æ ‡é¢˜ï¼šé‡åŒ–å’Œç¼“è§£è¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©</li><li>ä½œè€…ï¼šChaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. PÃ©rez, Marten van Dijk, Lydia Y. Chen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä»£å°”å¤«ç‰¹ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šåˆæˆè¡¨æ ¼æ•°æ®ã€æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€å·®åˆ†éšç§</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåˆæˆæ•°æ®ä»ç”Ÿæˆæ¨¡å‹ä¸­è·å–ï¼Œä½œä¸ºä¸€ç§ä¿æŠ¤éšç§çš„æ•°æ®å…±äº«è§£å†³æ–¹æ¡ˆã€‚æ­¤ç±»åˆæˆæ•°æ®é›†åº”ç±»ä¼¼äºåŸå§‹æ•°æ®ï¼Œä¸”ä¸æ³„éœ²å¯è¯†åˆ«çš„éšç§ä¿¡æ¯ã€‚è¡¨æ ¼åˆæˆå™¨çš„éª¨å¹²æŠ€æœ¯æ ¹æ¤äºå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) åˆ°æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ã€‚æœ€è¿‘çš„å…ˆå‰å·¥ä½œé˜æ˜äº†è¡¨æ ¼æ•°æ®ä¸Šçš„æ•ˆç”¨éšç§æƒè¡¡ï¼Œæ­ç¤ºå¹¶é‡åŒ–äº†åˆæˆæ•°æ®çš„éšç§é£é™©ã€‚ç„¶è€Œï¼Œé‡ç‚¹ä»…é™äºå°‘æ•°éšç§æ”»å‡»å’Œè¡¨æ ¼åˆæˆå™¨ï¼Œç‰¹åˆ«æ˜¯åŸºäº GAN çš„åˆæˆå™¨ï¼Œå¹¶ä¸”å¿½ç•¥äº†æˆå‘˜æ¨æ–­æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ï¼Œå³å·®åˆ†éšç§ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†å¼¥åˆå·®è·ï¼Œæˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ï¼š(i) è€ƒè™‘åˆ°æ›´å¹¿æ³›çš„åˆæˆå™¨é›†åˆåŠå…¶å¯¹æˆå‘˜æ¨æ–­æ”»å‡»çš„æ€§èƒ½ï¼Œå“ªç§ç±»å‹çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å¯ä»¥å®ç°æ›´å¥½çš„æ•ˆç”¨éšç§æƒè¡¡ï¼›(ii) é€šè¿‡å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³• (DP-SGD) å¯ä»¥è·å¾—ä»€ä¹ˆé¢å¤–çš„éšç§ä¿è¯ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œè¯¦å°½çš„ç»éªŒåˆ†æï¼Œé‡ç‚¹å…³æ³¨æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œé’ˆå¯¹å…«ç§éšç§æ”»å‡»ï¼Œå¼ºè°ƒäº†äº”ç§æœ€å…ˆè¿›çš„è¡¨æ ¼åˆæˆå™¨çš„æ•ˆç”¨éšç§æƒè¡¡ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå—è¡¨æ ¼æ‰©æ•£ä¸­æ•°æ®è´¨é‡é«˜ä½†éšç§é£é™©ä¹Ÿé«˜çš„è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† DP-TLDMï¼Œå·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒç”±ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œç»„æˆï¼Œç”¨äºå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œç¼–ç ï¼Œä»¥åŠä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆæˆæ½œåœ¨è¡¨æ ¼ã€‚éµå¾ªæ–°å…´çš„ ğ‘“-DP æ¡†æ¶ï¼Œæˆ‘ä»¬å°† DP-SGD åº”ç”¨äºè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œç»“åˆæ‰¹å¤„ç†å‰ªè£ï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDP-TLDM èƒ½å¤Ÿå®ç°æœ‰æ„ä¹‰çš„ç†è®ºéšç§ä¿è¯ï¼ŒåŒæ—¶è¿˜æ˜¾ç€æé«˜åˆæˆæ•°æ®çš„æ•ˆç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œä¸å…¶ä»– DP ä¿æŠ¤è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDP-TLDM å°†åˆæˆè´¨é‡æé«˜äº†å¹³å‡ 35%ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆç”¨æé«˜äº† 15%ï¼Œæ•°æ®å¯åŒºåˆ†åº¦æé«˜äº† 50%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“æ°´å¹³çš„éšç§é£é™©ã€‚</li></ol><p><strong>æ–¹æ³•</strong></p><p>(1) <strong>éšç§æ”»å‡»åˆ†æï¼š</strong>é’ˆå¯¹ 5 ç§æœ€å…ˆè¿›çš„è¡¨æ ¼åˆæˆå™¨å’Œ 8 ç§éšç§æ”»å‡»ï¼Œè¿›è¡Œè¯¦å°½çš„ç»éªŒåˆ†æï¼Œé‡ç‚¹å…³æ³¨æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œå¼ºè°ƒå…¶æ•ˆç”¨éšç§æƒè¡¡ã€‚</p><p>(2) <strong>DP-TLDM æ¨¡å‹ï¼š</strong>æå‡ºå·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ (DP-TLDM)ï¼Œç”±è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç»„æˆï¼Œéµå¾ª f-DP æ¡†æ¶ï¼Œå°† DP-SGD åº”ç”¨äºè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œå¹¶ç»“åˆæ‰¹å¤„ç†å‰ªè£ã€‚</p><p>(3) <strong>éšç§åº¦é‡ï¼š</strong>ä½¿ç”¨åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œæ›´å¥½åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å·¥ä½œé€šè¿‡é‡åŒ–å’Œç¼“è§£è¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©ï¼Œä¸ºåˆæˆè¡¨æ ¼æ•°æ®çš„å®‰å…¨å…±äº«æä¾›äº†ç†è®ºæŒ‡å¯¼å’ŒæŠ€æœ¯æ”¯æŒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„å·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆDP-TLDMï¼‰ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†åˆæˆæ•°æ®çš„æ•ˆç”¨å’Œéšç§é£é™©ã€‚</li><li>é‡‡ç”¨ f-DP æ¡†æ¶å’Œæ‰¹å¤„ç†å‰ªè£æŠ€æœ¯ï¼Œå¯¹è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œéšç§ä¿æŠ¤ï¼Œæé«˜äº†åˆæˆæ•°æ®çš„éšç§ä¿è¯ã€‚</li><li>ä½¿ç”¨åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œæ›´å‡†ç¡®åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚æ€§èƒ½ï¼š</li><li>ä¸å…¶ä»– DP ä¿æŠ¤è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDP-TLDM å°†åˆæˆè´¨é‡æé«˜äº†å¹³å‡ 35%ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆç”¨æé«˜äº† 15%ï¼Œæ•°æ®å¯åŒºåˆ†åº¦æé«˜äº† 50%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“æ°´å¹³çš„éšç§é£é™©ã€‚</li><li>åœ¨å¹¿æ³›çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å’Œéšç§æ”»å‡»ç»„åˆä¸Šè¿›è¡Œäº†è¯¦å°½çš„ç»éªŒåˆ†æï¼Œä¸ºé€‰æ‹©åˆé€‚çš„åˆæˆå™¨å’Œç¼“è§£éšç§é£é™©æä¾›äº†æŒ‡å¯¼ã€‚å·¥ä½œé‡ï¼š</li><li>æœ¬ç ”ç©¶å·¥ä½œæ¶‰åŠè¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©è¯„ä¼°ã€å·®åˆ†éšç§ä¿æŠ¤æ¨¡å‹çš„æå‡ºå’Œå®ç°ï¼Œä»¥åŠå¤§é‡çš„å®éªŒéªŒè¯ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-88261d8594214e79fd8f14053221f4cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8a6ba2ff82daf72ac247bc6db810b6b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a2b8468a15abf24eebadf158ef6cc36c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a865f3725b2cf16776255cd7f309f8b5.jpg" align="middle"></details><h2 id="Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model"><a href="#Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model" class="headerlink" title="Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model"></a>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h2><p><strong>Authors:Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</strong></p><p>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields. </p><p><a href="http://arxiv.org/abs/2403.07764v1">PDF</a> </p><p><strong>Summary</strong><br>é¢éƒ¨å½©å¦†è¿ç§»æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œè¶…è¶Šç®€å•å¦†å®¹é£æ ¼ï¼Œå¯å°†å¤§é‡çœŸå®ä¸–ç•Œå¦†å®¹å¹³ç¨³è¿ç§»è‡³ç”¨æˆ·é¢éƒ¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li><li>ä½¿ç”¨ç»†èŠ‚ä¿ç•™åŒ–å¦†ç¼–ç å™¨ç¼–ç åŒ–å¦†ç»†èŠ‚ã€‚</li><li>å¼•å…¥å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—ï¼Œä»¥ä¿ç•™æºå›¾åƒçš„å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚</li><li>åˆ©ç”¨ U-Net ä¸­æ·»åŠ çš„åŒ–å¦†äº¤å‰æ³¨æ„å±‚ï¼Œå¯å°†è¯¦ç»†çš„åŒ–å¦†å‡†ç¡®è¿ç§»åˆ°æºå›¾åƒå¯¹åº”ä½ç½®ã€‚</li><li>é€šè¿‡å†…å®¹ç»“æ„å»è€¦è®­ç»ƒï¼Œç¨³å®šåŒ–å¦†åŠŸèƒ½å¯ä»¥ä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚</li><li>è¯¥æ–¹æ³•å…·å¤‡å¼ºå¤§çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§ï¼Œå¯ç”¨äºå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚è·¨åŸŸåŒ–å¦†è¿ç§»å’ŒåŒ–å¦†æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰çš„åŒ–å¦†è¿ç§»æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ (SOTA) ç»“æœï¼Œå¹¶ä¸”åœ¨ç›¸å…³é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šStable-Makeupï¼šå½“ç°å®ä¸–ç•Œå¦†å®¹é‡ä¸Šæ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šYuxuan Zhang1âˆ—, Lifu Wei3, Qing Zhang4, Yiren Song5, Jiaming Liu2â€ , Huaxia Li2, Xu Tang2, Yao Hu2, and Haibo Zhao2</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šMakeup transfer, Diffusion model, Detail-Preserving makeup encoder, Content-structure decoupling</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://xiaojiu-z.github.io/Stable-Makeup.github.io/   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ï¼šç›®å‰çš„ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„å¦†å®¹è¿ç§»æ–¹æ³•ä»…é™äºç®€å•çš„å¦†å®¹é£æ ¼ï¼Œéš¾ä»¥åº”ç”¨äºç°å®åœºæ™¯ã€‚   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼šæ— æ³•è¿ç§»å¤æ‚å¤šæ ·çš„çœŸå®å¦†å®¹ã€‚æ–¹æ³•çš„åŠ¨æœºï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å¦†å®¹è¿ç§»æ–¹æ³•ï¼Œå¯ä»¥é²æ£’åœ°å°†å¹¿æ³›çš„çœŸå®å¦†å®¹è¿ç§»åˆ°ç”¨æˆ·æä¾›çš„é¢éƒ¨ä¸Šã€‚   ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šStable-Makeup åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ç»†èŠ‚ä¿æŒï¼ˆD-Pï¼‰å¦†å®¹ç¼–ç å™¨å¯¹å¦†å®¹ç»†èŠ‚è¿›è¡Œç¼–ç ã€‚å®ƒè¿˜é‡‡ç”¨å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—æ¥ä¿ç•™æºå›¾åƒçš„å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚åœ¨ U-Net ä¸­æ·»åŠ äº†æ–°çš„å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå¯ä»¥å°†è¯¦ç»†çš„å¦†å®¹å‡†ç¡®åœ°è¿ç§»åˆ°æºå›¾åƒçš„ç›¸åº”ä½ç½®ã€‚ç»è¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒåï¼ŒStable-Makeup å¯ä»¥ä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚   ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥é²æ£’åœ°è¿ç§»å„ç§çœŸå®å¦†å®¹ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å…¶ç›®æ ‡ï¼šå°†å¤æ‚å¤šæ ·çš„çœŸå®å¦†å®¹è¿ç§»åˆ°ç”¨æˆ·æä¾›çš„é¢éƒ¨ä¸Šã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ï¼šåˆ©ç”¨ç»†èŠ‚ä¿æŒå¦†å®¹ç¼–ç å™¨æå–å‚è€ƒå¦†å®¹çš„ç»†èŠ‚ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨å†…å®¹ç¼–ç å™¨å’Œç»“æ„ç¼–ç å™¨åˆ†åˆ«å¯¹æºå›¾åƒå’Œé¢éƒ¨ç»“æ„æ§åˆ¶å›¾åƒè¿›è¡Œç¼–ç ï¼›ï¼ˆ3ï¼‰ï¼šä½¿ç”¨å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚å°†è¯¦ç»†å¦†å®¹åµŒå…¥ä¸æºå›¾åƒä¸­é¢éƒ¨åŒºåŸŸçš„ä¸­é—´ç‰¹å¾å›¾å¯¹é½ï¼›ï¼ˆ4ï¼‰ï¼šé€šè¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒï¼Œä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰è¯¥å·¥ä½œå°†ç°å®ä¸–ç•Œçš„å¦†å®¹è¿ç§»å¸¦å…¥æ‰©æ•£æ¨¡å‹é¢†åŸŸï¼Œåœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§çš„è¿›å±•ï¼Œå®ç°äº†ä»¥å¾€éš¾ä»¥å®ç°çš„æ•ˆæœã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§ç»†èŠ‚ä¿æŒå¦†å®¹ç¼–ç å™¨ï¼Œç”¨äºæå–å‚è€ƒå¦†å®¹çš„ç²¾ç»†ç‰¹å¾ã€‚</li><li>é‡‡ç”¨å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—ï¼Œåˆ†åˆ«å¯¹æºå›¾åƒå’Œé¢éƒ¨ç»“æ„æ§åˆ¶å›¾åƒè¿›è¡Œç¼–ç ã€‚</li><li>ä½¿ç”¨å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå°†è¯¦ç»†å¦†å®¹åµŒå…¥ä¸æºå›¾åƒä¸­é¢éƒ¨åŒºåŸŸçš„ä¸­é—´ç‰¹å¾å›¾å¯¹é½ã€‚</li><li>é€šè¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒï¼Œä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥é²æ£’åœ°è¿ç§»å„ç§çœŸå®å¦†å®¹ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨æµæ°´çº¿ï¼Œç”¨äºåˆ›å»ºå„ç§å¦†å®¹é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-481722553fcfcc03e397479a6260fb2a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bff86407dc53580d4b616a78652a1e4.jpg" align="middle"></details><h2 id="SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces"><a href="#SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces" class="headerlink" title="SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces"></a>SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces</h2><p><strong>Authors:Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo</strong></p><p>Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at <a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2403.07711v1">PDF</a> Accepted as workshop paper at ICLR 2024</p><p><strong>Summary:</strong><br>æ‰©æ•£æ¨¡å‹ä¸­åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹å…‹æœæ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—éš¾é¢˜ï¼Œå®ç°æ›´é•¿çš„è§†é¢‘ç”Ÿæˆã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åˆ©ç”¨æ³¨æ„åŠ›å±‚ç”Ÿæˆè§†é¢‘ï¼Œä½†æ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—éšåºåˆ—é•¿åº¦äºŒæ¬¡å¢é•¿ã€‚</li><li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä»¥çº¿æ€§çš„å†…å­˜æ¶ˆè€—ç›¸å¯¹åºåˆ—é•¿åº¦ï¼Œä¸ºé•¿è§†é¢‘ç”Ÿæˆæä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚</li><li>åœ¨ UCF101 è§†é¢‘ç”ŸæˆåŸºå‡†ä¸Šï¼ŒSSM æ¨¡å‹ä¸æ³¨æ„åŠ›æ¨¡å‹å…·æœ‰ç«äº‰åŠ›çš„ FVD è¯„åˆ†ã€‚</li><li>SSM æ¨¡å‹åœ¨ MineRL Navigate æ•°æ®é›†ä¸Šç”Ÿæˆ 64 å’Œ 150 å¸§çš„è§†é¢‘æ—¶ï¼Œå¤§å¹…èŠ‚çœäº†å†…å­˜æ¶ˆè€—ã€‚</li><li>SSM æ¨¡å‹åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­å…·æœ‰æ½œåŠ›ï¼Œå¯åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹é™ä½å†…å­˜å¼€é”€ã€‚</li><li>ä»£ç å¯åœ¨ GitHub ä¸Šè·å¾—ï¼š<a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Modelsã€‚">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Modelsã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSSM é‡è§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼šä½¿ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ</li><li>ä½œè€…ï¼šShih-Yuan Chen, Yi-Hsuan Tsai, Yi-Ting Chen, Wei-Chih Hung, Ting-Chun Wang</li><li>æ‰€å±å•ä½ï¼šå›½ç«‹å°æ¹¾å¤§å­¦</li><li>å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€çŠ¶æ€ç©ºé—´æ¨¡å‹ã€é•¿ç¨‹ä¾èµ–æ€§</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08748ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š   éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å–å¾—æ˜¾è‘—æˆå°±ï¼Œç ”ç©¶ç•Œå¯¹å°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆè¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸»è¦åˆ©ç”¨æ³¨æ„åŠ›å±‚æå–æ—¶é—´ç‰¹å¾ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—å—åºåˆ—é•¿åº¦çš„äºŒæ¬¡æ–¹å½±å“ï¼Œè¿™ç»™ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¾ƒé•¿è§†é¢‘åºåˆ—å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   ä¸ºäº†å…‹æœæ³¨æ„åŠ›å±‚çš„é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚ä¸æ³¨æ„åŠ›å±‚ç›¸æ¯”ï¼ŒSSM çš„å†…å­˜æ¶ˆè€—ä¸åºåˆ—é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ï¼Œå› æ­¤æ˜¯ä¸€ç§å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚   (3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   æœ¬æ–‡æå‡ºäº†ä¸€ç§å°† SSM ä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ç”¨åŒå‘ SSM æ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶åœ¨åŒå‘ SSM ä¹‹åæ·»åŠ äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚   (4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼š   åœ¨å®éªŒä¸­ï¼Œæœ¬æ–‡é¦–å…ˆä½¿ç”¨ UCF101ï¼ˆè§†é¢‘ç”Ÿæˆæ ‡å‡†åŸºå‡†ï¼‰è¯„ä¼°äº†åŸºäº SSM çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç ”ç©¶ SSM åœ¨æ›´é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œæœ¬æ–‡ä½¿ç”¨ MineRL Navigate æ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œå°†å¸§æ•°åˆ†åˆ«è®¾ç½®ä¸º 64 å’Œ 150ã€‚åœ¨è¿™äº›è®¾ç½®ä¸­ï¼ŒåŸºäº SSM çš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„ FVD åˆ†æ•°ã€‚</li></ol><p>Methodsï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†SSMä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ç”¨åŒå‘SSMæ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶åœ¨åŒå‘SSMä¹‹åæ·»åŠ äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡é‡‡ç”¨UCF101ï¼ˆè§†é¢‘ç”Ÿæˆæ ‡å‡†åŸºå‡†ï¼‰è¯„ä¼°äº†åŸºäºSSMçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç ”ç©¶SSMåœ¨æ›´é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œæœ¬æ–‡ä½¿ç”¨MineRLNavigateæ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œå°†å¸§æ•°åˆ†åˆ«è®¾ç½®ä¸º64å’Œ150ã€‚ï¼ˆ3ï¼‰ï¼šåœ¨è¿™äº›è®¾ç½®ä¸­ï¼ŒåŸºäºSSMçš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„FVDåˆ†æ•°ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—èŠ‚çœè¾ƒé•¿è§†é¢‘åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆè´¨é‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§å°†SSMä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚</li><li>ä½¿ç”¨åŒå‘SSMæ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚</li><li>åœ¨UCF101å’ŒMineRLNavigateæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨UCF101æ•°æ®é›†ä¸Šï¼ŒåŸºäºSSMçš„æ¨¡å‹åœ¨FVDåˆ†æ•°ä¸Šä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“ã€‚</li><li>åœ¨MineRLNavigateæ•°æ®é›†ä¸Šï¼ŒåŸºäºSSMçš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„FVDåˆ†æ•°ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹é›†æˆã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒåŒå‘SSMæ¨¡å—ï¼Œä½†ä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸æ¯”ï¼Œå…¶å†…å­˜æ¶ˆè€—çš„èŠ‚çœå¯ä»¥æŠµæ¶ˆè¿™ä¸€é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a0f2d31483fd32e25e8225d6d8c2b039.jpg" align="middle"><img src="https://pica.zhimg.com/v2-466831d067339c450f01dc616d49009f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59e29fe8e02669abd07b749ea5015008.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6b09844a4e5773a714f817c1ba660426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e10f4a24354ea51e1e9b2b5de3d559d.jpg" align="middle"></details><h2 id="D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation"><a href="#D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation" class="headerlink" title="D4D: An RGBD diffusion model to boost monocular depth estimation"></a>D4D: An RGBD diffusion model to boost monocular depth estimation</h2><p><strong>Authors:L. Papa, P. Russo, I. Amerini</strong></p><p>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset. </p><p><a href="http://arxiv.org/abs/2403.07516v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡Diffusion4Dç”ŸæˆçœŸå®RGBDæ ·æœ¬ï¼Œæå‡å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åœ°é¢å®å†µ RGBD æ•°æ®å¯¹äºè®¡ç®—æœºè§†è§‰è‡³å…³é‡è¦ï¼Œä½†è·å–å›°éš¾ä¸”è€—æ—¶ã€‚</li><li>ä½¿ç”¨å›¾å½¢å¼•æ“ç”Ÿæˆåˆæˆä»£ç†æ•°æ®å¯è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½†çœŸå®æ„Ÿä¸è¶³ã€‚</li><li>æå‡º Diffusion4Dï¼Œä¸€ç§å®šåˆ¶çš„ 4 é€šé“æ‰©æ•£æ¨¡å‹ï¼Œå¯ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ã€‚</li><li>å°†ç”Ÿæˆçš„æ ·æœ¬çº³å…¥ç›‘ç£è®­ç»ƒç®¡é“ï¼Œå¯æé«˜å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹æ€§èƒ½ã€‚</li><li>åœ¨ NYU Depth v2 å®¤å†…å’Œ KITTI å®¤å¤–æ•°æ®é›†ä¸Šï¼Œä¸åˆæˆæ•°æ®å’ŒåŸå§‹æ•°æ®ç›¸æ¯”ï¼ŒRMSE åˆ†åˆ«é™ä½ (8.2%, 11.9%) å’Œ (8.1%, 6.1%)ã€‚</li><li>è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ RGB å›¾åƒå’Œæ·±åº¦å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»å»ºæ¨¡å‡†ç¡®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šD4Dï¼šä¸€ç§ç”¨äºæå‡å•ç›®æ·±åº¦ä¼°è®¡çš„ RGBD æ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šLorenzo Papaã€Paolo Russoã€Irene Amerini</li><li>æ‰€å±å•ä½ï¼šæ„å¤§åˆ©ç½—é©¬ç¬¬ä¸€å¤§å­¦è®¡ç®—æœºã€æ§åˆ¶ä¸ç®¡ç†å·¥ç¨‹ç³»</li><li>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€æ‰©æ•£æ¨¡å‹ã€æ·±åº¦å­¦ä¹ ã€å•ç›®æ·±åº¦ä¼°è®¡ã€ç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶éœ€è¦å¤§é‡æ ‡è®°è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œå¯¹äºå¯†é›†é¢„æµ‹åº”ç”¨ï¼ˆå¦‚æ·±åº¦ä¼°è®¡ï¼‰ï¼Œç”±äºæ”¶é›†ä¸€è‡´çš„ RGB å’Œæ·±åº¦æ•°æ®å­˜åœ¨å›°éš¾å’Œè€—æ—¶ï¼Œå› æ­¤ç¼ºä¹å¤§é‡çœŸå®æ•°æ®ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¸ºäº†è§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨åˆæˆæ¸²æŸ“ï¼ˆå¦‚ Unity å’Œ Unreal Engineï¼‰ç”Ÿæˆæ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯é€šå¸¸æ— æ³•æä¾›é€¼çœŸçš„æ•°æ®ï¼Œç¼ºä¹å‡†ç¡®çš„å…‰çº¿åå°„ã€ç›¸æœºä¼ªå½±å’Œå™ªå£°æ•°æ®ç­‰çœŸå®ç‰¹å¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Diffusion4Dï¼ˆD4Dï¼‰çš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ã€‚D4D ä½¿ç”¨å®šåˆ¶çš„ 4 é€šé“ DDPM æ¥æ•æ‰çœŸå®å®¤å†…å’Œå®¤å¤– RGBD æ ·æœ¬ä¸­å­˜åœ¨çš„å†…åœ¨ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„ RGB å›¾åƒå’Œç›¸åº”çš„æ·±åº¦å›¾ï¼ŒåŒæ—¶æé«˜è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„å¤šæ ·æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼Œåˆ©ç”¨ç”Ÿæˆçš„æ ·æœ¬å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒç®¡é“è¿›è¡Œäº†æ‰©å……ï¼Œåœ¨ NYUDepthv2 å’Œ KITTI æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 8.2% å’Œ 11.9% çš„ RMSE é™ä½ï¼Œä»¥åŠ 8.1% å’Œ 6.1% çš„ RMSE é™ä½ã€‚è¿™äº›æ€§èƒ½æå‡è¡¨æ˜ï¼ŒD4D å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰é¢„å¤„ç†ï¼šå¯¹çœŸå®ä¸–ç•Œä¸­çš„ RGBD æ ·æœ¬è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ŒåŒ…æ‹¬å½’ä¸€åŒ–å’Œè°ƒæ•´å¤§å°ã€‚ï¼ˆ2ï¼‰ç”Ÿæˆï¼šä½¿ç”¨å®šåˆ¶çš„ 4 é€šé“å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ã€‚ï¼ˆ3ï¼‰åˆå¹¶ï¼šå°†ç”Ÿæˆçš„æ ·æœ¬ä¸åŸå§‹è®­ç»ƒæ•°æ®åˆå¹¶ï¼Œåˆ›å»ºæ‰©å……çš„è®­ç»ƒé›†ã€‚ï¼ˆ4ï¼‰è®­ç»ƒï¼šä½¿ç”¨æ‰©å……çš„è®­ç»ƒé›†è®­ç»ƒæ·±åº¦ä¼°è®¡æ¨¡å‹ï¼ŒåŒ…æ‹¬ DenseDepthã€FastDepthã€SPEED å’Œ METERã€‚ï¼ˆ5ï¼‰è¯„ä¼°ï¼šä½¿ç”¨ NYUDepthv2ã€KITTIã€SceneNetã€SYNTHIASF å’Œ DIML æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“ç”± D4D ç»„æˆï¼ŒD4D æ˜¯ä¸€ä¸ªå®šåˆ¶çš„ 4 é€šé“ DDPMï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œç”¨äºæé«˜æ·±åº¦å’Œæµ…å±‚ MDE æ¨¡å‹çš„ä¼°è®¡æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­å±•ç¤ºäº†ä¼˜äºåˆæˆç”Ÿæˆæ•°æ®é›†çš„æ€§èƒ½ï¼Œå¹³å‡ RMSE é™ä½äº† 8.2% å’Œ 8.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåœ¨å®¤å†…åŸºçº¿ NYUDepthv2 å’Œå®¤å¤– KITTI æ•°æ®é›†ä¸Šå®ç°äº† 11.9% å’Œ 6.1% çš„ RMSE é™ä½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ–¹æ³•ä»¥åŠç”Ÿæˆçš„æ•°æ®é›†ï¼ˆD4D-NYU å’Œ D4D-KITTIï¼‰å°†é¼“åŠ±å°† DDPM ä¸æ·±åº¦å­¦ä¹ æ¶æ„ç»“åˆä½¿ç”¨ï¼Œä»¥è§£å†³å„ç§è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­æ ‡è®°è®­ç»ƒæ•°æ®çš„ç¼ºä¹é—®é¢˜ã€‚æ‰€æå‡ºç­–ç•¥çš„ä¸€ä¸ªå…³é”®è¦ç´ æ˜¯ä½¿ç”¨çœŸå®ä¸–ç•Œå›¾åƒç”Ÿæˆæ–°çš„å¢å¼ºæ ·æœ¬ï¼Œä»è€Œæé«˜ MDE æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²çš„ä¼°è®¡å’Œæ³›åŒ–èƒ½åŠ›ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº DDPM çš„è®­ç»ƒç®¡é“ D4Dï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œä»¥å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹çš„è®­ç»ƒï¼›æ€§èƒ½ï¼šåœ¨ NYUDepthv2 å’Œ KITTI æ•°æ®é›†ä¸Šï¼Œåˆ†åˆ«å®ç°äº† 8.2% å’Œ 11.9% çš„ RMSE é™ä½ï¼›å·¥ä½œé‡ï¼šéœ€è¦å¯¹çœŸå®ä¸–ç•Œ RGBD æ ·æœ¬è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶çš„ DDPM ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-7d5ae84aa4ad849eb5b34921fd19235f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1fc5f5f060711d07a3643061bea9ce36.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8bf13f9f6d8ae61c864289783d74507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b98512be7d612da9e4c36952c334f92.jpg" align="middle"></details><h2 id="Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting"><a href="#Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting" class="headerlink" title="Efficient Diffusion Model for Image Restoration by Residual Shifting"></a>Efficient Diffusion Model for Image Restoration by Residual Shifting</h2><p><strong>Authors:Zongsheng Yue, Jianyi Wang, Chen Change Loy</strong></p><p>While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{<a href="https://github.com/zsyOAOA/ResShift}">https://github.com/zsyOAOA/ResShift}</a>. </p><p><a href="http://arxiv.org/abs/2403.07319v1">PDF</a> Extended version of NeurIPS paper. Code:   <a href="https://github.com/zsyOAOA/ResShift">https://github.com/zsyOAOA/ResShift</a></p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤ä¸­ï¼Œæ— éœ€ååŠ é€Ÿå³å¯æå¤§åœ°å‡å°‘æ‰©æ•£æ­¥éª¤ï¼Œå®ç°åœ¨ç»´æŒæ€§èƒ½çš„æƒ…å†µä¸‹æå¤§åŠ é€Ÿã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†æ— éœ€åå¤„ç†åŠ é€Ÿçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œå¤§å¹…å‡å°‘æ‰€éœ€çš„æ‰©æ•£æ­¥éª¤ã€‚</li><li>é€šè¿‡å¹³ç§»æ®‹å·®å»ºç«‹é©¬å°”å¯å¤«é“¾ï¼Œæé«˜å›¾åƒè´¨é‡çš„è½¬æ¢æ•ˆç‡ã€‚</li><li>è®¾è®¡äº†ç²¾å¿ƒåˆ¶å®šçš„å™ªå£°æ—¶é—´è¡¨ï¼Œçµæ´»æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚</li><li>å³ä½¿ä»…ä½¿ç”¨ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²è„¸éƒ¨ä¿®å¤ç­‰ç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šå®ç°æˆ–ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>æ€§èƒ½ä¸ SOTA æ–¹æ³•ç›¸å½“ï¼Œæå¤§åŠ é€Ÿäº†æ¨ç†é€Ÿåº¦ã€‚</li><li>ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šåŸºäºæ®‹å·®å¹³ç§»çš„å›¾åƒä¿®å¤é«˜æ•ˆæ‰©æ•£æ¨¡å‹</li><li>ä½œè€…ï¼šå²³å®—ç”Ÿï¼Œç‹å»ºä¸€ï¼Œé™ˆæ˜ŒLoy</li><li>å•ä½ï¼šå—æ´‹ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šMarkové“¾ï¼Œå™ªå£°è°ƒåº¦ï¼Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå›¾åƒä¿®å¤ï¼Œäººè„¸ä¿®å¤</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.07319ï¼ŒGithubï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶æ¨ç†é€Ÿåº¦ä½ï¼Œéœ€è¦æ‰§è¡Œæ•°ç™¾ç”šè‡³æ•°åƒä¸ªé‡‡æ ·æ­¥éª¤ã€‚ç°æœ‰çš„åŠ é€Ÿé‡‡æ ·æŠ€æœ¯è™½ç„¶è¯•å›¾åŠ å¿«è¿™ä¸ªè¿‡ç¨‹ï¼Œä½†ä¸å¯é¿å…åœ°åœ¨ä¸€å®šç¨‹åº¦ä¸Šç‰ºç‰²æ€§èƒ½ï¼Œå¯¼è‡´æ¢å¤ç»“æœè¿‡åº¦æ¨¡ç³Šã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æ–¹æ³•å¯åˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç§æ˜¯å°†ä½è´¨é‡å›¾åƒä½œä¸ºæ¡ä»¶æ’å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç„¶åé’ˆå¯¹å›¾åƒä¿®å¤ä»»åŠ¡é‡æ–°è®­ç»ƒæ¨¡å‹ï¼›å¦ä¸€ç§æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥ä¿ƒè¿›å›¾åƒä¿®å¤é—®é¢˜ã€‚è¿™ä¸¤ç§ç­–ç•¥éƒ½ç»§æ‰¿äº†DDPMä¸­éšå«çš„é©¬å°”å¯å¤«é“¾ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ•ˆç‡å¯èƒ½å¾ˆä½ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ã€é’ˆå¯¹å›¾åƒä¿®å¤é‡èº«å®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å’Œè°çš„å¹³è¡¡ï¼Œè€Œä¸ä¼šä¸ºäº†ä¸€ä¸ªè€Œç‰ºç‰²å¦ä¸€ä¸ªã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œä»è€Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ã€‚è¿˜è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿åªæœ‰å››ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤è¿™ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®å¹³ç§»çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ï¼›(2) è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ï¼›(3) åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šï¼Œå³ä½¿åªæœ‰å››ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•ä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å›¾åƒä¿®å¤é‡èº«å®šåˆ¶çš„ã€é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å’Œè°çš„å¹³è¡¡ï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤è¿™ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ®‹å·®å¹³ç§»çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ï¼›è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•ä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„æ¨ç†é€Ÿåº¦å¿«ï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œè¿™å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3e3d51fe0b9323fce3c712dc608e3d9f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a182da1e249c6b628670838e47b4a76e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac3a6dd379a0eb12739ce5eb4300d834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79486bac2fc6b15b8e68f559254fb9fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7dd29574d8058fee668b2d948a1e069e.jpg" align="middle"></details><h2 id="Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers"><a href="#Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers" class="headerlink" title="Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers"></a>Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</h2><p><strong>Authors:Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</strong></p><p>This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the modelâ€™s feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements. </p><p><a href="http://arxiv.org/abs/2403.07214v1">PDF</a> Accepted in CVPR 2024. Project page available at   <a href="https://subhadeepkoley.github.io/DiffusionZSSBIR/">https://subhadeepkoley.github.io/DiffusionZSSBIR/</a></p><p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢ä¸­çš„æ¢ç´¢é¦–æ¬¡å–å¾—çªç ´ï¼Œç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€èƒ½åŠ›ï¼Œå¯æœ‰æ•ˆåœ°å¼¥åˆè‰å›¾ä¸ç…§ç‰‡ä¹‹é—´çš„å·®è·ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥å¼¥åˆç†å¿µè‰å›¾å’Œç…§ç‰‡ä¹‹é—´çš„å·®è·ã€‚</li><li>ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ä»¥æé«˜é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢çš„æ€§èƒ½ã€‚</li><li>é€‰æ‹©åˆé€‚çš„ç‰¹å¾å±‚å¯¹æ£€ç´¢æ•ˆæœè‡³å…³é‡è¦ã€‚</li><li>å¯è§†åŒ–å’Œæ–‡æœ¬æç¤ºå¯ä»¥æŒ‡å¯¼æ¨¡å‹ç‰¹å¾æå–è¿‡ç¨‹ï¼Œæé«˜è¡¨ç¤ºçš„åŒºåˆ†æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li><li>åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç±»åˆ«çº§å’Œç»†ç²’åº¦çš„æ£€ç´¢ä»»åŠ¡ã€‚</li><li>è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢æä¾›äº†æ–°çš„æ€è·¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯ä¼˜ç§€çš„è‰å›¾ç…§ç‰‡åŒ¹é…å™¨</li><li>ä½œè€…ï¼šSubhadeep Koleyã€Ayan Kumar Bhuniaã€Aneeshan Sainã€Pinaki Nath Chowdhuryã€Tao Xiangã€Yi-Zhe Song</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±å›½è¨é‡Œå¤§å­¦ SketchXã€CVSSP</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€è‰å›¾åŒ¹é…</li><li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•å› å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§è€Œå—åˆ°å…³æ³¨ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•é€šå¸¸éœ€è¦å¤šæ¬¡è¿­ä»£æ¨ç†ï¼Œè¿™ä¼šå¢åŠ æ—¶é—´å’Œè®¡ç®—å¤æ‚åº¦ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾ï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ Sketchyã€TU-Berlin å’Œ Quick, Draw! ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ï¼›(2)å°†Stable Diffusionæ¨¡å‹æ‰©å±•åˆ°é›¶æ ·æœ¬è‰å›¾+æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼ˆZS-STBIRï¼‰ä»»åŠ¡ï¼Œé€šè¿‡ä½¿ç”¨å¯ç”¨çš„æ–‡æœ¬æ ‡é¢˜æˆ–ç±»åˆ«æ ‡ç­¾æ¥æé«˜æå–ç‰¹å¾çš„è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šé¦–æ¬¡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æµæ°´çº¿ï¼Œä»¥å°†å†»ç»“çš„ Stable Diffusion é€‚åº”ä¸ºç±»åˆ«çº§å’Œè·¨ç±»åˆ«ç»†ç²’åº¦ ZS-SBIR ä»»åŠ¡çš„éª¨å¹²ç‰¹å¾æå–å™¨ã€‚é€šè¿‡å·§å¦™åœ°ä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸è¿›ä¸€æ­¥å¾®è°ƒçš„æƒ…å†µä¸‹å°†é¢„è®­ç»ƒæ¨¡å‹é€‚åº”åˆ°æ‰‹å¤´çš„ä»»åŠ¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„ ZSSBIR æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å½»åº•çš„åˆ†æå®éªŒï¼Œä»¥å»ºç«‹åˆ©ç”¨å†»ç»“çš„ stable diffusion æ¨¡å‹ä½œä¸º ZS-SBIR éª¨å¹²çš„æœ€ä½³å®è·µã€‚æœ€åï¼Œåˆ©ç”¨ stable diffusion å›ºæœ‰çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®¡é“æ‰©å±•åˆ°åŸºäºè‰å›¾ + æ–‡æœ¬çš„ SBIRï¼Œä»è€Œå®ç°åŸºäºè‰å›¾ + æ–‡æœ¬çš„ç±»åˆ«ã€ç»†ç²’åº¦å’Œåœºæ™¯çº§åœºæ™¯ä¸­çš„å®é™…æ£€ç´¢ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼›å°† Stable Diffusion æ¨¡å‹æ‰©å±•åˆ°é›¶æ ·æœ¬è‰å›¾ + æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼ˆZS-STBIRï¼‰ä»»åŠ¡ã€‚æ€§èƒ½ï¼šåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šè§£å†³äº†ç°æœ‰åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-d241840af721fa3e3d26127475eab81e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bd3dc3a12b0ad0e0283f2af9ff1b2dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0752cb46230001078d91a5e105eacf22.jpg" align="middle"></details><h2 id="Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction"><a href="#Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction" class="headerlink" title="Bayesian Diffusion Models for 3D Shape Reconstruction"></a>Bayesian Diffusion Models for 3D Shape Reconstruction</h2><p><strong>Authors:Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</strong></p><p>We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction. </p><p><a href="http://arxiv.org/abs/2403.06973v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>è´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰é€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªé¡¶å‘ä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªåº•å‘ä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œè¿›è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>BDM åœ¨ 3D å½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li><li>BDM ä½¿ç”¨æ¥è‡ªç‹¬ç«‹æ ‡ç­¾ï¼ˆä¾‹å¦‚ç‚¹äº‘ï¼‰çš„ä¸°å¯Œå…ˆéªŒä¿¡æ¯æ¥æ”¹å–„è‡ªåº•å‘ä¸Šçš„ 3D é‡å»ºï¼Œè€Œæ— éœ€é…å¯¹ï¼ˆç›‘ç£ï¼‰æ•°æ®æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚</li><li>BDM é€šè¿‡è€¦åˆæ‰©æ•£è¿‡ç¨‹å’Œå­¦ä¹ çš„æ¢¯åº¦è®¡ç®—ç½‘ç»œæ‰§è¡Œæ— ç¼ä¿¡æ¯èåˆï¼Œæ— éœ€æ ‡å‡†è´å¶æ–¯æ¡†æ¶ä¸­æ¨ç†æ‰€éœ€çš„æ˜¾å¼å…ˆéªŒå’Œä¼¼ç„¶ã€‚</li><li>BDM çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºèƒ½å¤Ÿè¿›è¡Œè‡ªé¡¶å‘ä¸‹å’Œè‡ªåº•å‘ä¸Šè¿‡ç¨‹çš„ä¸»åŠ¨å’Œæœ‰æ•ˆçš„ä¿¡æ¯äº¤æ¢å’Œèåˆï¼Œæ¯ä¸ªè¿‡ç¨‹æœ¬èº«éƒ½æ˜¯ä¸€ä¸ªæ‰©æ•£è¿‡ç¨‹ã€‚</li><li>åœ¨ 3D å½¢çŠ¶é‡å»ºçš„åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šè´å¶æ–¯æ‰©æ•£æ¨¡å‹ç”¨äº 3D å½¢çŠ¶é‡å»º</li><li>ä½œè€…ï¼šJianfei Guo, Tianchang Shen, Zekun Hao, Song Bai, Xiang Bai</li><li>éš¶å±æœºæ„ï¼šæµ™æ±Ÿå¤§å­¦</li><li>å…³é”®è¯ï¼šBayesian Diffusion Models, 3D Shape Reconstruction, Generative Diffusion Model</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D å½¢çŠ¶é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨ä» 2D å›¾åƒæˆ–ç‚¹äº‘ä¸­æ¢å¤ 3D å½¢çŠ¶ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨æ•°æ®é©±åŠ¨çš„è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œéœ€è¦é…å¯¹çš„ï¼ˆç›‘ç£ï¼‰æ•°æ®-æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒ-ç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å—åˆ°è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè´¨é‡çš„é™åˆ¶ã€‚(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨æ•°æ®é©±åŠ¨çš„è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œéœ€è¦é…å¯¹çš„ï¼ˆç›‘ç£ï¼‰æ•°æ®-æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒ-ç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å—åˆ°è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè´¨é‡çš„é™åˆ¶ã€‚(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ (BDM) çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚BDM å…·æœ‰å°†å…ˆéªŒä¿¡æ¯ä»ç‹¬ç«‹æ ‡ç­¾ï¼ˆä¾‹å¦‚ç‚¹äº‘ï¼‰æ— ç¼èåˆåˆ° 3D é‡å»ºä¸­çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚(4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå¯¹ BDM è¿›è¡Œäº†è¯„ä¼°ï¼Œç”¨äº 3D å½¢çŠ¶é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBDM åœ¨å„ç§æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨ 3D å½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>Methodsï¼šï¼ˆ1ï¼‰æå‡ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰æ¡†æ¶ï¼Œå°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚ï¼ˆ2ï¼‰è®¾è®¡ä¸€ä¸ªè”åˆæ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥å°†å…ˆéªŒä¿¡æ¯èåˆåˆ°3Då½¢çŠ¶é‡å»ºä¸­ï¼Œæ— éœ€æ˜¾å¼æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚ï¼ˆ3ï¼‰é‡‡ç”¨å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œè¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆ3Då½¢çŠ¶ã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰çš„3Då½¢çŠ¶é‡å»ºæ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ï¼Œåœ¨3Då½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰æ¡†æ¶ï¼Œå°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚</li><li>è®¾è®¡ä¸€ä¸ªè”åˆæ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥å°†å…ˆéªŒä¿¡æ¯èåˆåˆ°3Då½¢çŠ¶é‡å»ºä¸­ï¼Œæ— éœ€æ˜¾å¼æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚</li><li>é‡‡ç”¨å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œè¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆ3Då½¢çŠ¶ã€‚Performanceï¼š</li><li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå¯¹BDMè¿›è¡Œäº†è¯„ä¼°ï¼Œç”¨äº3Då½¢çŠ¶é‡å»ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBDMåœ¨å„ç§æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨3Då½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚Workloadï¼š</li><li>æœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦å¯¹è´å¶æ–¯æ‰©æ•£æ¨¡å‹ã€3Då½¢çŠ¶é‡å»ºå’Œå˜åˆ†æ¨æ–­æ–¹æ³•æœ‰ä¸€å®šçš„äº†è§£ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7422b82570cb43b0e03df4c70a22bd9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-024cf388128af8fcbb5768c6b5cbd193.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75567a8fc44c36c6e2757bf6b21b6dcf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-837f8b78a5d65ec0d93f1545faef964c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43923b8a4efdf4a63b3fd3998d1b5749.jpg" align="middle"></details><h2 id="SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data"><a href="#SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data" class="headerlink" title="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data"></a>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data</h2><p><strong>Authors:Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</strong></p><p>Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLMâ€™s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models. </p><p><a href="http://arxiv.org/abs/2403.06952v1">PDF</a> First two authors contributed equally; Project website:   <a href="https://selma-t2i.github.io/">https://selma-t2i.github.io/</a></p><p><strong>Summary</strong><br>å¤šæŠ€èƒ½ä¸“å®¶å­¦ä¹ ä¸è‡ªåŠ¨ç”Ÿæˆæ•°æ®ï¼Œèåˆæå‡T2Iæ¨¡å‹é€¼çœŸåº¦ï¼Œæ˜¾è‘—æ”¹å–„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬å¿ å®åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SELMAèåˆå¤šæŠ€èƒ½ä¸“å®¶å­¦ä¹ ä¸è‡ªåŠ¨ç”Ÿæˆæ•°æ®æå‡T2Iæ¨¡å‹é€¼çœŸåº¦ã€‚</li><li>LLMç”Ÿæˆå¤šæ ·æ–‡æœ¬æç¤ºï¼Œå¯¹åº”ä¸åŒæŠ€èƒ½ï¼Œè®­ç»ƒT2Iæ¨¡å‹è·å–æ–°æŠ€èƒ½ã€‚</li><li>ç‹¬ç«‹ä¸“å®¶å¾®è°ƒé’ˆå¯¹ä¸åŒæŠ€èƒ½ï¼Œä¸“å®¶èåˆæ‰“é€ å¤šæŠ€èƒ½T2Iæ¨¡å‹å¤„ç†å¤šæ ·æ–‡æœ¬æç¤ºã€‚</li><li>SELMAæ˜¾è‘—æå‡SOTA T2Iæ¨¡å‹è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬å¿ å®åº¦ï¼ˆTIFA+2.1%ï¼ŒDSG+6.9%ï¼‰ã€‚</li><li>è‡ªåŠ¨æ”¶é›†çš„å›¾åƒæ–‡æœ¬ç”¨äºå¾®è°ƒæ€§èƒ½æ¥è¿‘çœŸå®æ•°æ®å¾®è°ƒã€‚</li><li>è¾ƒå¼±T2Iæ¨¡å‹å›¾åƒç”¨äºå¾®è°ƒå¯ä»¥æå‡è¾ƒå¼ºT2Iæ¨¡å‹ç”Ÿæˆè´¨é‡ï¼Œå±•ç°T2Iæ¨¡å‹çš„å¼±åˆ°å¼ºæ³›åŒ–æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šSELMAï¼šé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®å­¦ä¹ å’Œåˆå¹¶ç‰¹å®šæŠ€èƒ½çš„æ–‡æœ¬åˆ°å›¾åƒä¸“å®¶</li><li>ä½œè€…ï¼šJialu Liã€Jaemin Choã€Yi-Lin Sungã€Jaehong Yoonã€Mohit Bansal</li><li>æ‰€å±æœºæ„ï¼šåŒ—å¡ç½—æ¥çº³å¤§å­¦æ•™å ‚å±±åˆ†æ ¡</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒç”Ÿæˆã€ä¸“å®¶å­¦ä¹ ã€çŸ¥è¯†èåˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.06952 Githubï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹åœ¨åˆ›å»ºå›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆä¸æ–‡æœ¬è¾“å…¥ç»†èŠ‚å®Œå…¨åŒ¹é…çš„å›¾åƒï¼Œä¾‹å¦‚ä¸æ­£ç¡®çš„ç©ºé—´å…³ç³»æˆ–ç¼ºå¤±å¯¹è±¡ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¾§é‡äºç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ ï¼Œä½†å®ƒä»¬åœ¨æ•æ‰æ–‡æœ¬æç¤ºä¸­çš„æ‰€æœ‰è¯­ä¹‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šSELMA æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ï¼Œé€šè¿‡åœ¨è‡ªåŠ¨ç”Ÿæˆçš„å¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆç‰¹å®šæŠ€èƒ½çš„ä¸“å®¶å­¦ä¹ å’Œåˆå¹¶ï¼Œæ¥æé«˜ T2I æ¨¡å‹çš„ä¿çœŸåº¦ã€‚ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šSELMA åœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾ç€æé«˜äº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬ä¿çœŸåº¦ï¼ˆåœ¨ TIFA ä¸Šæé«˜äº† 2.1%ï¼Œåœ¨ DSG ä¸Šæé«˜äº† 6.9%ï¼‰ï¼Œäººç±»åå¥½æŒ‡æ ‡ï¼ˆPickScoreã€ImageReward å’Œ HPSï¼‰ï¼Œä»¥åŠäººç±»è¯„ä¼°ã€‚</li></ol><p><strong>æ–¹æ³•ï¼š</strong></p><p>(1) <strong>è‡ªåŠ¨ç”Ÿæˆå¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ï¼š</strong>ä½¿ç”¨é¢„è®­ç»ƒçš„T2Iæ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬æç¤ºå¯¹å…¶è¿›è¡Œæ³¨é‡Šï¼Œåˆ›å»ºåŒ…å«å„ç§æŠ€èƒ½ï¼ˆä¾‹å¦‚å¯¹è±¡ç”Ÿæˆã€å±æ€§ç¼–è¾‘ã€åœºæ™¯åˆæˆï¼‰çš„æ•°æ®é›†ã€‚</p><p>(2) <strong>ç‰¹å®šæŠ€èƒ½çš„ä¸“å®¶å­¦ä¹ ï¼š</strong>åœ¨è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šå¾®è°ƒT2Iæ¨¡å‹ï¼Œä¸“æ³¨äºç‰¹å®šæŠ€èƒ½çš„å­¦ä¹ ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹æŒæ¡ç‰¹å®šæŠ€èƒ½æ‰€éœ€çš„çŸ¥è¯†ã€‚</p><p>(3) <strong>ä¸“å®¶åˆå¹¶ï¼š</strong>å°†è®­ç»ƒè¿‡çš„ç‰¹å®šæŠ€èƒ½ä¸“å®¶æ¨¡å‹åˆå¹¶åˆ°ä¸»T2Iæ¨¡å‹ä¸­ã€‚é€šè¿‡èåˆä¸“å®¶çŸ¥è¯†ï¼Œä¸»æ¨¡å‹å¯ä»¥åŒæ—¶åˆ©ç”¨ä¸åŒæŠ€èƒ½ï¼Œä»è€Œæé«˜å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€‚</p><p>(4) <strong>å¾®è°ƒï¼š</strong>åœ¨æœ€ç»ˆçš„å¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šå¾®è°ƒåˆå¹¶åçš„T2Iæ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°èŒƒå¼ SELMAï¼Œé€šè¿‡åˆ©ç”¨ T2I æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæé«˜äº†æœ€å…ˆè¿›çš„ T2I æ¨¡å‹åœ¨ç”Ÿæˆå’Œäººç±»åå¥½æ–¹é¢çš„ä¿çœŸåº¦ã€‚SELMA é¦–å…ˆæ”¶é›†äº†åœ¨ä¸éœ€è¦é¢å¤–äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹ç»™å®šå„ç§ç”Ÿæˆçš„æ–‡æœ¬æç¤ºçš„è‡ªæˆ‘ç”Ÿæˆå›¾åƒã€‚ç„¶åï¼ŒSELMA åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹å•ç‹¬çš„ LoRA æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ¨ç†æœŸé—´åˆå¹¶å®ƒä»¬ï¼Œä»¥å‡è½»æ•°æ®é›†ä¹‹é—´çš„çŸ¥è¯†å†²çªã€‚SELMA åœ¨æé«˜ T2I æ¨¡å‹çš„ä¿çœŸåº¦å’Œä¸äººç±»åå¥½çš„å¯¹é½åº¦æ–¹é¢å±•ç¤ºäº†å¼ºå¤§çš„ç»éªŒç»“æœï¼Œå¹¶è¡¨æ˜åŸºäºæ‰©æ•£çš„ T2I æ¨¡å‹å…·æœ‰æ½œåœ¨çš„å¼±åˆ°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ã€ç‰¹å®šæŠ€èƒ½ä¸“å®¶å­¦ä¹ å’Œä¸“å®¶åˆå¹¶æ¥æé«˜ T2I æ¨¡å‹ä¿çœŸåº¦çš„æ–°èŒƒå¼ã€‚æ€§èƒ½ï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾ç€æé«˜äº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬ä¿çœŸåº¦ï¼Œäººç±»åå¥½æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ã€‚å·¥ä½œé‡ï¼šéœ€è¦ç”Ÿæˆå’Œæ³¨é‡Šå¤§é‡å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼Œå¹¶è®­ç»ƒå’Œåˆå¹¶å¤šä¸ªä¸“å®¶æ¨¡å‹ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-a71fb7431e2ed3366a76c62d6434a3a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26fd4cb2b211747179211fa7dd2b38a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-951031dbd570a29204c573bd83992954.jpg" align="middle"></details><h2 id="Distribution-Aware-Data-Expansion-with-Diffusion-Models"><a href="#Distribution-Aware-Data-Expansion-with-Diffusion-Models" class="headerlink" title="Distribution-Aware Data Expansion with Diffusion Models"></a>Distribution-Aware Data Expansion with Diffusion Models</h2><p><strong>Authors:Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, Bin Wang</strong></p><p>The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a> </p><p><a href="http://arxiv.org/abs/2403.06741v1">PDF</a> Project: <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a></p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º DistDiff çš„é«˜æ•ˆæ•°æ®æ‰©å±•æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨äº†åˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆä»»åŠ¡çš„åˆ†å¸ƒä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ•°æ®é›†çš„è§„æ¨¡å’Œè´¨é‡å¯¹æ·±åº¦æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li><li>æ•°æ®é›†æ‰©å……æŠ€æœ¯å¯ä»¥è‡ªåŠ¨æ‰©å……æ•°æ®é›†ï¼Œé‡Šæ”¾æ·±åº¦æ¨¡å‹çš„æ½œåŠ›ã€‚</li><li>åŸºäºå›¾åƒå˜æ¢çš„æ•°æ®æ‰©å……æ–¹æ³•åªèƒ½å¼•å…¥å±€éƒ¨å˜åŒ–ï¼Œå¤šæ ·æ€§è¾ƒå·®ã€‚</li><li>åŸºäºå›¾åƒåˆæˆçš„æ‰©å……æ–¹æ³•å¯ä»¥åˆ›é€ å…¨æ–°å†…å®¹ï¼Œæ˜¾è‘—æé«˜ä¿¡æ¯æ€§ã€‚</li><li>ç°æœ‰çš„åˆæˆæ–¹æ³•å­˜åœ¨åˆ†å¸ƒåå·®çš„é£é™©ï¼Œå¯èƒ½ä¼šé™ä½æ¨¡å‹å¯¹åˆ†å¸ƒå¤–æ ·æœ¬çš„æ€§èƒ½ã€‚</li><li>DistDiff åŸºäºåˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ„é€ åˆ†å±‚åŸå‹å’Œåˆ†å±‚èƒ½é‡æŒ‡å¯¼æ¥è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒã€‚</li><li>DistDiff åœ¨æ•°æ®æ‰©å±•ä»»åŠ¡ä¸­å®ç°äº†åˆ†å¸ƒä¸€è‡´æ ·æœ¬çš„ç”Ÿæˆï¼Œå–å¾—äº†æ˜¾è‘—æå‡ã€‚</li><li>ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼ŒDistDiff åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æå‡äº† 30.7%ï¼Œä¸æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº† 9.8%ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒæ„ŸçŸ¥æ•°æ®æ‰©å……</li><li>ä½œè€…ï¼šæœ±æµ©ä¼Ÿã€æ¨å‡Œã€é›å†›æµ·ã€å¼ æ–‡æ¶›ã€ç‹æ–Œ</li><li>éš¶å±å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šæ•°æ®æ‰©å……ã€æ‰©æ•£æ¨¡å‹ã€åˆ†å¸ƒæ„ŸçŸ¥</li><li>é“¾æ¥ï¼šhttps://github.com/haoweiz23/DistDiff</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å¯¹äºæ·±åº¦æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†è·å–æ­¤ç±»æ•°æ®é›†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æ•°æ®æ‰©å……æŠ€æœ¯æ—¨åœ¨è‡ªåŠ¨æ‰©å……æ•°æ®é›†ï¼Œé‡Šæ”¾æ·±åº¦æ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ•°æ®æ‰©å……æ–¹æ³•åŒ…æ‹¬åŸºäºå›¾åƒå˜æ¢å’ŒåŸºäºåˆæˆçš„ä¸¤ç§ç±»å‹ã€‚åŸºäºå›¾åƒå˜æ¢çš„æ–¹æ³•åªèƒ½å¼•å…¥å±€éƒ¨å˜åŒ–ï¼Œå¤šæ ·æ€§è¾ƒå·®ã€‚åŸºäºåˆæˆçš„å›¾åƒç”Ÿæˆæ–¹æ³•è™½ç„¶å¯ä»¥åˆ›å»ºå…¨æ–°çš„å†…å®¹ï¼Œä½†å­˜åœ¨åˆ†å¸ƒåå·®çš„é£é™©ï¼Œå¯èƒ½ä¼šé™ä½æ¨¡å‹çš„æ€§èƒ½ã€‚ï¼ˆ3ï¼‰æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®æ‰©å……æ¡†æ¶ DistDiffã€‚DistDiff æ„å»ºåˆ†å±‚åŸå‹ä»¥é€¼è¿‘çœŸå®æ•°æ®åˆ†å¸ƒï¼Œåœ¨å…·æœ‰åˆ†å±‚èƒ½é‡å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ä¸­ä¼˜åŒ–æ½œåœ¨æ•°æ®ç‚¹ã€‚ï¼ˆ4ï¼‰æ€§èƒ½ï¼šDistDiff åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æ¯”åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡æé«˜ 30.7%ï¼Œæ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æé«˜ 9.8%ã€‚è¿™äº›æ€§èƒ½æå‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>Methods:(1): å°†åŸå§‹æ•°æ®åˆ†å¸ƒè¿‘ä¼¼ä¸ºåˆ†å±‚åŸå‹ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼›(2): å¼•å…¥æ®‹å·®ä¹˜æ³•å˜æ¢ï¼Œåœ¨å¯æ§èŒƒå›´å†…è°ƒæ•´æ½œåœ¨ç‰¹å¾ï¼›(3): åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥èƒ½é‡å¼•å¯¼ï¼Œä¼˜åŒ–å˜æ¢å‚æ•°ï¼Œä½¿ç”Ÿæˆçš„æ ·æœ¬ä¸çœŸå®æ•°æ®åˆ†å¸ƒä¸€è‡´ï¼›(4): åˆ©ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨å’Œå»å™ªç½‘ç»œï¼Œæ„å»ºèƒ½é‡å‡½æ•°ï¼ŒæŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼›(5): ä¼˜åŒ–ä¸­é—´é‡‡æ ·æ­¥éª¤ï¼Œè€Œä¸æ˜¯ä»…ä¼˜åŒ–æœ€ç»ˆé‡‡æ ·ç»“æœã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬è®ºæ–‡æå‡ºçš„ DistDiff æ–¹æ³•åœ¨æ•°æ®æ‰©å……é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®æ‰©å……æä¾›äº†æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§åŸºäºåˆ†å±‚åŸå‹çš„åˆ†å¸ƒæ„ŸçŸ¥æ•°æ®æ‰©å……æ¡†æ¶ï¼Œæœ‰æ•ˆé€¼è¿‘çœŸå®æ•°æ®åˆ†å¸ƒã€‚</li><li>å¼•å…¥äº†æ®‹å·®ä¹˜æ³•å˜æ¢å’Œèƒ½é‡å¼•å¯¼æœºåˆ¶ï¼Œåœ¨å¯æ§èŒƒå›´å†…ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ï¼Œæé«˜ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li><li>åˆ©ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨å’Œå»å™ªç½‘ç»œæ„å»ºèƒ½é‡å‡½æ•°ï¼ŒæŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œæå‡ç”Ÿæˆæ ·æœ¬ä¸çœŸå®æ•°æ®çš„ç›¸ä¼¼æ€§ã€‚</li><li>ä¼˜åŒ–äº†ä¸­é—´é‡‡æ ·æ­¥éª¤ï¼Œè€Œä¸æ˜¯ä»…ä¼˜åŒ–æœ€ç»ˆé‡‡æ ·ç»“æœï¼Œæé«˜äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚</li><li>åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒä¸­ï¼ŒDistDiff æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li><li>æ€§èƒ½ï¼šåœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒDistDiff åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æ¯”åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡æé«˜ 30.7%ï¼Œæ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æé«˜ 9.8%ã€‚</li><li>å·¥ä½œé‡ï¼šDistDiff æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦æ„å»ºåˆ†å±‚åŸå‹ã€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾å’Œèƒ½é‡å‡½æ•°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-51004e76bd54c2109bfb0cba773b0e50.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fa6c026111223b0c29b77804e9db13e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-54f57321604f976084e4edde1c9cc9fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-272c701cea8b6d59603b8700ded9462f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db0b8236d7ff4e2af692d5671eac4b67.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ä¿ƒè¿›ä¸‰ç»´ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>V3D åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ä¿ƒè¿›ä¸‰ç»´ç”Ÿæˆã€‚</li><li>å¼•å…¥å‡ ä½•ä¸€è‡´æ€§å…ˆéªŒï¼Œå°†è§†é¢‘æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºå¤šè§†å›¾ä¸€è‡´çš„ä¸‰ç»´ç”Ÿæˆå™¨ã€‚</li><li>å¯ä»¥å¾®è°ƒæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç»™å®šå•å¼ å›¾åƒå‘¨å›´å¯¹è±¡çš„ 360 åº¦è½¨é“å¸§ã€‚</li><li>é€šè¿‡å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œå¯ä»¥åœ¨ 3 åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„ç½‘æ ¼æˆ–ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚</li><li>æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°åœºæ™¯çº§çš„æ–°é¢–è§†å›¾åˆæˆï¼Œé€šè¿‡ç¨€ç–è¾“å…¥è§†å›¾ç²¾ç¡®æ§åˆ¶ç›¸æœºè·¯å¾„ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆè´¨é‡å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.æ ‡é¢˜ï¼šV3Dï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ 3D ç”Ÿæˆå™¨</p><ol><li>ä½œè€…ï¼šZilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼š3D ç”Ÿæˆã€è§†é¢‘æ‰©æ•£æ¨¡å‹ã€æ·±åº¦å­¦ä¹ </li><li>è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.06738v1[cs.CV]11Mar2024</li><li>æ‘˜è¦ï¼š<br>(1) ç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨ 3D ç”Ÿæˆè¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚æœ€è¿‘çš„æ–¹æ³•æå¤§åœ°æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œä½†ç”±äºæ¨¡å‹å®¹é‡æœ‰é™ï¼Œé€šå¸¸ä¼šç”Ÿæˆç»†èŠ‚è¾ƒå°‘çš„å¯¹è±¡ã€‚<br>(2) è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æˆ–è‡ªå›å½’æ¨¡å‹ã€‚GAN å®¹æ˜“å‡ºç°æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œè€Œè‡ªå›å½’æ¨¡å‹ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚<br>(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ 3D ç”Ÿæˆæ–¹æ³• V3Dã€‚V3D å°†è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3D ç”Ÿæˆï¼Œé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å’Œåè½¬æ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆ 3D å¯¹è±¡ã€‚<br>(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨ ShapeNet æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒV3D åœ¨ç”Ÿæˆè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚V3D å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ 3D å¯¹è±¡ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€ 3 åˆ†é’Ÿã€‚&lt;/p&gt;<br><p>7.Methodsï¼šï¼ˆ1ï¼‰ï¼šV3Dé‡‡ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†3Då¯¹è±¡ç”Ÿæˆè¿‡ç¨‹è§†ä¸ºä»å™ªå£°åˆ†å¸ƒé€æ­¥å»å™ªçš„è¿‡ç¨‹ï¼›ï¼ˆ2ï¼‰ï¼šV3Dä½¿ç”¨U-Netä½œä¸ºç”Ÿæˆå™¨ï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥æ·»åŠ å™ªå£°ï¼Œç”Ÿæˆ3Då¯¹è±¡ï¼›ï¼ˆ3ï¼‰ï¼šV3Dä½¿ç”¨å¤šå°ºåº¦è®­ç»ƒç­–ç•¥ï¼Œæé«˜ç”Ÿæˆå¯¹è±¡çš„ç»†èŠ‚å’Œä¿çœŸåº¦ï¼›ï¼ˆ4ï¼‰ï¼šV3Dä½¿ç”¨æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆå¯¹è±¡çš„è§†è§‰è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº3Dç”Ÿæˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•V3Dï¼Œåœ¨ç”Ÿæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚V3Dæ‰©å±•äº†è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨3Dç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¸ºé«˜è´¨é‡3Dç”Ÿæˆå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨3Dä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„3Dç”Ÿæˆæ–¹æ³•V3Dï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥æ·»åŠ å™ªå£°ç”Ÿæˆ3Då¯¹è±¡ã€‚</li><li>è®¾è®¡äº†ä¸€ç§å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œç”¨äºä»ç”Ÿæˆçš„è§†å›¾ä¸­è·å–3Dèµ„äº§ï¼Œå¹¶æ”¯æŒåœ¨3åˆ†é’Ÿå†…é‡å»ºé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚</li><li>å°†V3Dæ‰©å±•åˆ°åœºæ™¯çº§æ–°è§†è§’åˆæˆï¼Œå®ç°äº†å¯¹æ‘„åƒæœºè·¯å¾„çš„ç²¾ç¡®æ§åˆ¶å’Œå¤šè§†è§’ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ShapeNetæ•°æ®é›†ä¸Šï¼ŒV3Dåœ¨ç”Ÿæˆè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>V3Då¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ3Då¯¹è±¡ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€3åˆ†é’Ÿã€‚</li><li>V3Dåœ¨ç”Ÿæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒå’Œåœºæ™¯çº§æ–°è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å·¥ä½œé‡ï¼š</li><li>V3Dçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚</li><li>V3Dçš„è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆï¼Œåœ¨ShapeNetæ•°æ®é›†ä¸Šè®­ç»ƒV3Dä»…éœ€æ•°å°æ—¶ã€‚</li><li>V3Dçš„æ¨ç†é€Ÿåº¦å¿«ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆ3Då¯¹è±¡ã€‚</li></ol></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/</id>
    <published>2024-03-11T12:35:46.000Z</published>
    <updated>2024-03-11T12:35:46.983Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-11-æ›´æ–°"><a href="#2024-03-11-æ›´æ–°" class="headerlink" title="2024-03-11 æ›´æ–°"></a>2024-03-11 æ›´æ–°</h1><h2 id="VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models"><a href="#VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models" class="headerlink" title="VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models"></a>VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo</strong></p><p>Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a>. </p><p><a href="http://arxiv.org/abs/2403.05438v1">PDF</a> Project page: <a href="https://videoelevator.github.io">https://videoelevator.github.io</a> Code:   <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a></p><p><strong>Summary</strong><br>è§†é¢‘æå‡å™¨ï¼šé€šè¿‡å›¾åƒæ‰©æ•£æ¨¡å‹æå‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>VideoElevator æ˜¯ä¸€ç§æ— è®­ç»ƒã€å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯åˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿æå‡è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>ä¸ä¼ ç»Ÿçš„è§†é¢‘æ‰©æ•£æ¨¡å‹é‡‡æ ·ä¸åŒï¼ŒVideoElevator å°†æ¯ä¸ªé‡‡æ ·æ­¥éª¤åˆ†è§£ä¸ºæ—¶é—´è¿åŠ¨ç»†åŒ–å’Œç©ºé—´è´¨é‡æå‡ã€‚</li><li>æ—¶é—´è¿åŠ¨ç»†åŒ–ä½¿ç”¨å°é—­çš„è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ã€‚</li><li>ç©ºé—´è´¨é‡æå‡åˆ©ç”¨å……å®çš„å›¾åƒæ‰©æ•£æ¨¡å‹ç›´æ¥é¢„æµ‹æ›´å°‘å™ªå£°çš„æ½œåœ¨å› ç´ ï¼Œå¢åŠ æ›´å¤šé€¼çœŸçš„ç»†èŠ‚ã€‚</li><li>VideoElevator ä¸ä»…æé«˜äº†åŸºäºå›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜ä¿ƒè¿›äº†ä½¿ç”¨ä¸ªæ€§åŒ–å›¾åƒæ‰©æ•£æ¨¡å‹çš„é£æ ¼åŒ–è§†é¢‘åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šVideoElevatorï¼šåˆ©ç”¨å¤šåŠŸèƒ½æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æå‡è§†é¢‘ç”Ÿæˆè´¨é‡</li><li>ä½œè€…ï¼šYabo Zhang1, Yuxiang Wei1, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji2, and Wangmeng Zuo1</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå“ˆå°”æ»¨å·¥ä¸šå¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œè§†é¢‘ç”Ÿæˆï¼Œè´¨é‡æå‡</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://videoelevator.github.io   Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆT2Iï¼‰åœ¨ç”Ÿæˆé€¼çœŸä¸”ç¾è§‚çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚ç›¸åï¼Œæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆT2Vï¼‰åœ¨å¸§è´¨é‡å’Œæ–‡æœ¬å¯¹é½æ–¹é¢ä»ç„¶è¿œè¿œè½åï¼Œè¿™æ˜¯ç”±äºè®­ç»ƒè§†é¢‘çš„è´¨é‡å’Œæ•°é‡ä¸è¶³ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•ç›´æ¥å¯¹è§†é¢‘è¿›è¡Œé‡‡æ ·ï¼Œä½†ç”±äºç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ï¼Œç”Ÿæˆçš„è§†é¢‘è´¨é‡è¾ƒå·®ã€‚(3) æœ¬æ–‡æ–¹æ³•ï¼šVideoElevator æå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨ T2I çš„å‡ºè‰²èƒ½åŠ›æå‡ T2V çš„æ€§èƒ½ã€‚å®ƒå°†æ¯ä¸ªé‡‡æ ·æ­¥éª¤æ˜ç¡®åˆ†è§£ä¸ºæ—¶é—´è¿åŠ¨ç»†åŒ–å’Œç©ºé—´è´¨é‡æå‡ã€‚æ—¶é—´è¿åŠ¨ç»†åŒ–ä½¿ç”¨å°è£…çš„ T2V å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ï¼Œç„¶ååè½¬ä¸º T2I æ‰€éœ€çš„å™ªå£°åˆ†å¸ƒã€‚ç„¶åï¼Œç©ºé—´è´¨é‡æå‡åˆ©ç”¨è†¨èƒ€çš„ T2I ç›´æ¥é¢„æµ‹å™ªå£°è¾ƒå°çš„æ½œåœ¨å˜é‡ï¼Œæ·»åŠ æ›´å¤šé€¼çœŸçš„ç»†èŠ‚ã€‚(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨å„ç§ T2V å’Œ T2I æ¨¡å‹ç»„åˆä¸‹çš„å¹¿æ³›æç¤ºä¸­è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒVideoElevator åœ¨å¸§è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾è‘—æå‡äº† T2V çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æå‡ T2V è´¨é‡çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1) VideoElevatorå°†æ¯ä¸ªé‡‡æ ·æ­¥éª¤æ˜ç¡®åˆ†è§£ä¸ºæ—¶é—´è¿åŠ¨ç»†åŒ–å’Œç©ºé—´è´¨é‡æå‡ï¼›(2) æ—¶é—´è¿åŠ¨ç»†åŒ–ä½¿ç”¨å°è£…çš„T2Vå¢å¼ºæ—¶é—´ä¸€è‡´æ€§ï¼Œç„¶ååè½¬ä¸ºT2Iæ‰€éœ€çš„å™ªå£°åˆ†å¸ƒï¼›(3) ç©ºé—´è´¨é‡æå‡åˆ©ç”¨è†¨èƒ€çš„T2Iç›´æ¥é¢„æµ‹å™ªå£°è¾ƒå°çš„æ½œåœ¨å˜é‡ï¼Œæ·»åŠ æ›´å¤šé€¼çœŸçš„ç»†èŠ‚ã€‚</p><ol><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šxxxï¼›ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šVideoElevatoræå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œåˆ©ç”¨T2Içš„å‡ºè‰²èƒ½åŠ›æå‡T2Vçš„æ€§èƒ½ï¼Œä¸ºæå‡è§†é¢‘ç”Ÿæˆè´¨é‡æä¾›äº†ä¸€ç§æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ— è®­ç»ƒä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå°†T2Içš„ä¼˜åŠ¿å¼•å…¥T2Vä¸­ã€‚</li><li>å°†æ¯ä¸ªé‡‡æ ·æ­¥éª¤æ˜ç¡®åˆ†è§£ä¸ºæ—¶é—´è¿åŠ¨ç»†åŒ–å’Œç©ºé—´è´¨é‡æå‡ï¼Œæé«˜äº†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§å’Œç©ºé—´è´¨é‡ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å„ç§T2Vå’ŒT2Iæ¨¡å‹ç»„åˆä¸‹çš„å¹¿æ³›æç¤ºä¸­è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜VideoElevatoråœ¨å¸§è´¨é‡ã€æ—¶é—´ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾è‘—æå‡äº†T2Vçš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>VideoElevatoræ˜¯ä¸€ç§æ— è®­ç»ƒä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå·¥ä½œé‡è¾ƒå°ï¼Œæ˜“äºä¸ç°æœ‰çš„T2Væ¨¡å‹é›†æˆã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-cad376bbaa11399212fdef9f175c2469.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6b6b777c3f6359e627b50aeeac2627b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-907eeb8949cad583968ae2444608f263.jpg" align="middle"></details><h2 id="Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation"><a href="#Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation" class="headerlink" title="Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation"></a>Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation</h2><p><strong>Authors:Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song</strong></p><p>Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls â€” human-centric priors such as pose or depth maps â€” during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \url{<a href="https://hcplayercvpr2024.github.io}">https://hcplayercvpr2024.github.io}</a>. </p><p><a href="http://arxiv.org/abs/2403.05239v1">PDF</a> Accepted to CVPR 2024</p><p><strong>Summary</strong><br>åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­èåˆä»¥äººä¸ºä¸­å¿ƒçš„ä¿¡æ¯å¯ä»¥æ˜¾è‘—æé«˜å›¾åƒè´¨é‡ï¼Œç‰¹åˆ«æ˜¯äººä½“å›¾åƒçš„ç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äººä½“å›¾åƒç”Ÿæˆä¸­å­˜åœ¨å§¿åŠ¿å’Œæ¯”ä¾‹ä¸è‡ªç„¶ç­‰é—®é¢˜ã€‚</li><li>ç°æœ‰çš„æ–¹æ³•ä¸»è¦é€šè¿‡å¾®è°ƒæ¨¡å‹æˆ–å¢åŠ å›¾åƒç”Ÿæˆé˜¶æ®µçš„äººä½“çº¦æŸæ¥è§£å†³ã€‚</li><li>æœ¬æ–‡å°†äººä½“çº¦æŸç›´æ¥èå…¥æ¨¡å‹å¾®è°ƒé˜¶æ®µï¼Œæ— éœ€åœ¨æ¨ç†é˜¶æ®µæ·»åŠ çº¦æŸã€‚</li><li>äººä½“çº¦æŸå¯¹é½æŸå¤±åŠ å¼ºäº†å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­æ–‡æœ¬å½“ä¸­çš„äººä½“ç›¸å…³ä¿¡æ¯ã€‚</li><li>é‡‡ç”¨å¯æ§å°ºåº¦å’Œåˆ†æ­¥çº¦æŸï¼Œä¿è¯å¾®è°ƒè¿‡ç¨‹ä¸­çš„è¯­ä¹‰ä¸°å¯Œæ€§å’Œäººä½“ç»“æ„å‡†ç¡®æ€§ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¯åŸºäºç”¨æˆ·è¾“å…¥ç”Ÿæˆé«˜è´¨é‡äººä½“å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ–‡æœ¬çš„äººä½“å›¾åƒç”Ÿæˆçš„äººç±»ä¸­å¿ƒå¯¹é½æŸå¤±</li><li>ä½œè€…ï¼šZhaoyang Huang, Bin Li, Zizhao Zhang, Zhihao Fang, Yan Yan, Xiaogang Wang</li><li>éš¶å±ï¼šä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€äººç±»å›¾åƒç”Ÿæˆã€äººä½“å¯¹é½ã€æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆäººä½“å›¾åƒæ—¶å­˜åœ¨è§£å‰–ç»“æ„ä¸å‡†ç¡®ã€å§¿åŠ¿ä¸è‡ªç„¶ç­‰é—®é¢˜ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å¾®è°ƒæ¨¡å‹æˆ–æ·»åŠ äººä½“ä¸­å¿ƒå…ˆéªŒï¼ˆå¦‚å§¿åŠ¿æˆ–æ·±åº¦å›¾ï¼‰æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µéœ€è¦é¢å¤–çš„æ¡ä»¶ã€‚</p><p>ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§äººç±»ä¸­å¿ƒå¯¹é½æŸå¤±ï¼Œå°†æ–‡æœ¬æç¤ºä¸­çš„äººä½“ç›¸å…³ä¿¡æ¯èå…¥äº¤å‰æ³¨æ„åŠ›å›¾ä¸­ï¼Œå¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥å°ºåº¦æ„ŸçŸ¥å’Œæ­¥é•¿çº¦æŸï¼Œä»¥ä¿è¯è¯­ä¹‰ç»†èŠ‚ä¸°å¯Œå’Œäººä½“ç»“æ„å‡†ç¡®ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ Human-Art æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡äººä½“å›¾åƒæ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</p><p>æ–¹æ³•ï¼š(1):æå‡ºäººç±»ä¸­å¿ƒå…ˆéªŒå±‚ï¼ˆHcPï¼‰å’Œäººç±»ä¸­å¿ƒå¯¹é½æŸå¤±ï¼Œå¢å¼ºæ¨¡å‹å¯¹äººç±»ä¸­å¿ƒæ–‡æœ¬ä¿¡æ¯çš„æ•æ„Ÿæ€§ï¼Œæé«˜ç”Ÿæˆäººä½“å›¾åƒçš„ç»“æ„å‡†ç¡®æ€§å’Œç»†èŠ‚ã€‚(2):åˆ†æäº¤å‰æ³¨æ„åŠ›å±‚åœ¨ä¸åŒæ—¶é—´æ­¥å’Œåˆ†è¾¨ç‡å°ºåº¦ä¸‹çš„ä½œç”¨ï¼Œå‘ç°æ—©æœŸæ—¶é—´æ­¥å’Œä¸­é—´åˆ†è¾¨ç‡å°ºåº¦å¯¹äººä½“ç»“æ„ç”Ÿæˆè‡³å…³é‡è¦ã€‚(3):è®¾è®¡HcPå±‚ï¼Œä»æ–‡æœ¬åµŒå…¥ä¸­æå–äººç±»ä¸­å¿ƒtokenï¼Œå¹¶ä¸æ½œåœ¨ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œç”Ÿæˆäººç±»ä¸­å¿ƒæ³¨æ„åŠ›å›¾ã€‚(4):æå‡ºäººç±»ä¸­å¿ƒå¯¹é½æŸå¤±ï¼Œå°†é¢„è®­ç»ƒçš„å®ä½“å…³ç³»ç½‘ç»œæå–çš„äººç±»ä¸­å¿ƒå•è¯å¯¹åº”çš„å…³é”®å§¿åŠ¿å›¾åƒä¸HcPå±‚ç”Ÿæˆçš„æ³¨æ„åŠ›å›¾å¯¹é½ï¼ŒæŒ‡å¯¼æ¨¡å‹å…³æ³¨äººä½“ç»“æ„ç»†èŠ‚ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨äººç±»ä¸­å¿ƒå…ˆéªŒï¼ˆHcPï¼‰ï¼Œä¾‹å¦‚å§¿åŠ¿æˆ–æ·±åº¦å›¾ï¼Œæ¥æé«˜ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„äººä½“å›¾åƒç”Ÿæˆè´¨é‡ã€‚æ‰€æå‡ºçš„ HcP å±‚æœ‰æ•ˆåœ°åˆ©ç”¨äº†å…³äºäººç±»çš„ä¿¡æ¯åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ— éœ€åœ¨ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒæ—¶éœ€è¦é¢å¤–çš„è¾“å…¥ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒHcP å±‚ä¸ä»…ä¿®å¤äº†äººä½“ç»“æ„ç”Ÿæˆä¸­çš„ç»“æ„ä¸å‡†ç¡®é—®é¢˜ï¼Œè€Œä¸”è¿˜ä¿ç•™äº†åŸå§‹çš„å®¡ç¾å“è´¨å’Œç»†èŠ‚ã€‚æœªæ¥çš„å·¥ä½œå°†æ¢ç´¢æ•´åˆå¤šç§ç±»å‹çš„äººç±»ä¸­å¿ƒå…ˆéªŒï¼Œä»¥è¿›ä¸€æ­¥æ¨è¿›äººç±»å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„äººç±»ä¸­å¿ƒå¯¹é½æŸå¤±ï¼Œå°†æ–‡æœ¬æç¤ºä¸­çš„äººä½“ç›¸å…³ä¿¡æ¯èå…¥äº¤å‰æ³¨æ„åŠ›å›¾ä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹å…³æ³¨äººä½“ç»“æ„ç»†èŠ‚ã€‚åˆ†æäº†äº¤å‰æ³¨æ„åŠ›å±‚åœ¨ä¸åŒæ—¶é—´æ­¥å’Œåˆ†è¾¨ç‡å°ºåº¦ä¸‹çš„ä½œç”¨ï¼Œå‘ç°æ—©æœŸæ—¶é—´æ­¥å’Œä¸­é—´åˆ†è¾¨ç‡å°ºåº¦å¯¹äººä½“ç»“æ„ç”Ÿæˆè‡³å…³é‡è¦ã€‚è®¾è®¡äº† HcP å±‚ï¼Œä»æ–‡æœ¬åµŒå…¥ä¸­æå–äººç±»ä¸­å¿ƒ tokenï¼Œå¹¶ä¸æ½œåœ¨ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œç”Ÿæˆäººç±»ä¸­å¿ƒæ³¨æ„åŠ›å›¾ã€‚æ€§èƒ½ï¼šåœ¨ Human-Art æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡äººä½“å›¾åƒæ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚æ¶ˆèç ”ç©¶å’Œå¯è§†åŒ–ç»“æœéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†äººç±»ä¸­å¿ƒå¯¹é½æŸå¤±å’Œ HcP å±‚åœ¨æé«˜äººä½“å›¾åƒç”Ÿæˆè´¨é‡ä¸­çš„ä½œç”¨ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œåªéœ€åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ·»åŠ  HcP å±‚å’Œäººç±»ä¸­å¿ƒå¯¹é½æŸå¤±ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„æ¡ä»¶ï¼Œä¾‹å¦‚å§¿åŠ¿æˆ–æ·±åº¦å›¾ï¼Œåœ¨æ¨ç†é˜¶æ®µä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-dcb4970717d9f287c0e2b916300f3dd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cef2974d0c0ed77c5f9c42184d7e57c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-564f5b115d714883587e123a15ef8050.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5ade1a99be6f3185ad39bc934410199.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfa83d53d9802f58aba15bf8be1a8b64.jpg" align="middle"></details><h2 id="Denoising-Autoregressive-Representation-Learning"><a href="#Denoising-Autoregressive-Representation-Learning" class="headerlink" title="Denoising Autoregressive Representation Learning"></a>Denoising Autoregressive Representation Learning</h2><p><strong>Authors:Yazhe Li, Jorg Bornschein, Ting Chen</strong></p><p>In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05196v1">PDF</a> </p><p><strong>Summary</strong><br>è‡ªå›å½’æ‰©æ•£æ¨¡å‹ DARL å®ç°å›¾åƒç”Ÿæˆå’Œè§†è§‰è¡¨ç¤ºå­¦ä¹ ç›¸ç»“åˆï¼Œå±•ç°å‡ºä¸å…ˆè¿›æ©ç é¢„æµ‹æ¨¡å‹åª²ç¾çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DARL ä½¿ç”¨ä»…è§£ç å™¨çš„ Transformer æ¥è‡ªå›å½’é¢„æµ‹å›¾åƒå—ã€‚</li><li>ä»… MSE è®­ç»ƒå³å¯äº§ç”Ÿå¼ºå¤§çš„è¡¨ç¤ºã€‚</li><li>ä½¿ç”¨å»å™ªå—è§£ç å™¨å°† MSE æŸå¤±æ›¿æ¢ä¸ºæ‰©æ•£ç›®æ ‡å¯ä»¥å¢å¼ºå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li><li>å®šåˆ¶å™ªå£°è°ƒåº¦å’Œåœ¨æ›´å¤§æ¨¡å‹ä¸Šçš„æ›´é•¿æ—¶é—´è®­ç»ƒå¯ä»¥æé«˜å­¦ä¹ è¡¨ç¤ºã€‚</li><li>æœ€ä½³è°ƒåº¦ä¸æ ‡å‡†å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨çš„è°ƒåº¦æ˜¾è‘—ä¸åŒã€‚</li><li>å°½ç®¡æ¶æ„ç®€å•ï¼Œä½† DARL åœ¨å¾®è°ƒåè®®ä¸‹æä¾›æ¥è¿‘æœ€å…ˆè¿›æ©ç é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li><li>DARL ä»£è¡¨äº†å°†è‡ªå›å½’å’Œå»å™ªæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ï¼Œå®ç°è§†è§‰æ„ŸçŸ¥å’Œç”Ÿæˆç›¸ç»Ÿä¸€çš„é‡è¦ä¸€æ­¥ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šå»å™ªè‡ªå›å½’è¡¨å¾å­¦ä¹ </li><li>ä½œè€…ï¼šYazhe Liï¼ŒJorg Bornscheinï¼ŒTing Chen</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šGoogle DeepMind</li><li>å…³é”®è¯ï¼šè§†è§‰è¡¨å¾å­¦ä¹ ï¼Œè‡ªå›å½’æ¨¡å‹ï¼Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œå›¾åƒç”Ÿæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šè§†è§‰è¡¨å¾å­¦ä¹ å’Œå›¾åƒç”Ÿæˆé€šå¸¸ä½¿ç”¨ä¸åŒçš„æŠ€æœ¯ï¼Œå‰è€…æ³¨é‡é²æ£’æ€§ï¼Œåè€…æ³¨é‡ç”Ÿæˆèƒ½åŠ›ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šå¯¹æ¯”å­¦ä¹ ã€è’¸é¦è‡ªç›‘ç£å­¦ä¹ ã€æ©ç å›¾åƒå»ºæ¨¡ç­‰æ–¹æ³•åœ¨è¡¨å¾å­¦ä¹ ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹ç”Ÿæˆèƒ½åŠ›ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆè‡ªå›å½’æ¨¡å‹å’Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨è§£ç å™¨ Transformer é¢„æµ‹å›¾åƒå—ã€‚é€šè¿‡ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±å’Œå»å™ªå—è§£ç å™¨ï¼Œå¢å¼ºäº†å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚   ï¼ˆ4ï¼‰æ€§èƒ½è¡¨ç°ï¼šè¯¥æ–¹æ³•åœ¨å¾®è°ƒåè®®ä¸‹ï¼Œè¡¨ç°æ¥è¿‘æœ€å…ˆè¿›çš„æ©ç é¢„æµ‹æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨è¡¨å¾å­¦ä¹ å’Œç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ã€‚</p></li><li><p>Methodsï¼š(1) æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆè‡ªå›å½’æ¨¡å‹å’Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨è§£ç å™¨ Transformer é¢„æµ‹å›¾åƒå—ï¼›(2) ä½¿ç”¨å‡æ–¹è¯¯å·®æŸå¤±å’Œå»å™ªå—è§£ç å™¨ï¼Œå¢å¼ºäº†å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</p></li><li><p>æ€»ç»“ï¼š(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆè‡ªå›å½’æ¨¡å‹å’Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨è§£ç å™¨Transformeré¢„æµ‹å›¾åƒå—ï¼Œåœ¨å¾®è°ƒåè®®ä¸‹è¡¨ç°æ¥è¿‘æœ€å…ˆè¿›çš„æ©ç é¢„æµ‹æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨è¡¨å¾å­¦ä¹ å’Œç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ã€‚(2): Innovation point: æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆè‡ªå›å½’æ¨¡å‹å’Œå»å™ªæ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨è§£ç å™¨Transformeré¢„æµ‹å›¾åƒå—ã€‚Performance: åœ¨å¾®è°ƒåè®®ä¸‹è¡¨ç°æ¥è¿‘æœ€å…ˆè¿›çš„æ©ç é¢„æµ‹æ¨¡å‹ã€‚Workload: æœªæåŠã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-3a6bd101af2be0b75af14290ca20154b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-901dfa573ba65a2319ddfc43d65a7325.jpg" align="middle"><img src="https://picx.zhimg.com/v2-56ec555eccb7ae9c20c196a5c5519463.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc4830941b2dbf44695f875173f8eef5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6402f59254c8b1442a49f2075fd0b2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f3936f0ef0cf91ab8b2bb5de579b005.jpg" align="middle"></details><h2 id="Improving-Diffusion-Models-for-Virtual-Try-on"><a href="#Improving-Diffusion-Models-for-Virtual-Try-on" class="headerlink" title="Improving Diffusion Models for Virtual Try-on"></a>Improving Diffusion Models for Virtual Try-on</h2><p><strong>Authors:Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</strong></p><p>This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting diffusion models for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., GAN-based), but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel diffusion model that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the diffusion model, 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the self-attention layer. In addition, we provide detailed textual prompts for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both diffusion-based and GAN-based) in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario. </p><p><a href="http://arxiv.org/abs/2403.05139v1">PDF</a> </p><p><strong>Summary</strong><br>å›¾åƒåŸºäºçš„è™šæ‹Ÿè¯•ç©¿ï¼Œåœ¨ç»™å®šæè¿°äººç‰©å’Œè¡£æœå›¾åƒçš„æƒ…å†µä¸‹ï¼Œæ¸²æŸ“äººç‰©ç©¿ç€å®šåˆ¶è¡£æœçš„å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ”¹è¿›çš„æ‰©æ•£æ¨¡å‹ç”¨äºè™šæ‹Ÿè¯•ç©¿ï¼Œä»¥æé«˜ç”Ÿæˆè§†è§‰æ•ˆæœçš„è‡ªç„¶åº¦ã€‚</li><li>æå‡ºçš„ IDM-VTON æ¨¡å‹åœ¨ä¿ç•™æœè£…èº«ä»½çš„åŒæ—¶æé«˜äº†æœè£…ä¿çœŸåº¦ã€‚</li><li>è¯¥æ–¹æ³•ä½¿ç”¨ä¸¤ä¸ªæ¨¡å—æ¥ç¼–ç æœè£…å›¾åƒçš„è¯­ä¹‰ã€‚</li><li>é«˜çº§è¯­ä¹‰èåˆåˆ°äº¤å‰æ³¨æ„å±‚ï¼Œä½çº§ç‰¹å¾èåˆåˆ°è‡ªæ³¨æ„å±‚ã€‚</li><li>æä¾›è¯¦ç»†çš„æ–‡æœ¬æç¤ºï¼Œä»¥å¢å¼ºç”Ÿæˆè§†è§‰æ•ˆæœçš„çœŸå®æ€§ã€‚</li><li>ä½¿ç”¨ä¸€å¯¹äººç‰©æœè£…å›¾åƒçš„å®šåˆ¶æ–¹æ³•æ˜¾ç€æé«˜äº†ä¿çœŸåº¦å’ŒçœŸå®æ€§ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿ç•™æœè£…ç»†èŠ‚å’Œç”ŸæˆçœŸå®çš„è™šæ‹Ÿè¯•ç©¿å›¾åƒæ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li><li>æ‰€æå‡ºçš„å®šåˆ¶æ–¹æ³•åœ¨çœŸå®åœºæ™¯ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šæå‡æ‰©æ•£æ¨¡å‹ä»¥å®ç°çœŸå®çš„è™šæ‹Ÿè¯•ç©¿</li><li>Authorsï¼šYisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</li><li>Affiliationï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢ï¼ˆKAISTï¼‰</li><li>Keywordsï¼šå›¾åƒç”Ÿæˆã€è™šæ‹Ÿè¯•ç©¿ã€æ‰©æ•£æ¨¡å‹</li><li>Urlsï¼šhttps://arxiv.org/abs/2403.05139</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒå¼è™šæ‹Ÿè¯•ç©¿æ—¨åœ¨ç»™å®šæç»˜äººç‰©å’Œæœé¥°çš„ä¸¤å¹…å›¾åƒï¼Œç”Ÿæˆäººç‰©ç©¿ç€ç‰¹å®šæœé¥°çš„å›¾åƒã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰å·¥ä½œå°†åŸºäºç¤ºä¾‹çš„å›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹åº”ç”¨äºè™šæ‹Ÿè¯•ç©¿ï¼Œä¸å…¶ä»–æ–¹æ³•ï¼ˆå¦‚åŸºäº GAN çš„æ–¹æ³•ï¼‰ç›¸æ¯”ï¼Œå¯ä»¥æé«˜ç”Ÿæˆè§†è§‰æ•ˆæœçš„è‡ªç„¶æ€§ï¼Œä½†æ— æ³•ä¿ç•™æœé¥°çš„ç‰¹å¾ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæå‡ºä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹ IDM-VTONï¼Œè¯¥æ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æ¨¡å—å¯¹æœé¥°å›¾åƒçš„è¯­ä¹‰è¿›è¡Œç¼–ç ï¼›åœ¨ç»™å®šæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬ U-Net çš„æƒ…å†µä¸‹ï¼Œ1ï¼‰ä»è§†è§‰ç¼–ç å™¨ä¸­æå–çš„é«˜çº§è¯­ä¹‰è¢«èåˆåˆ°äº¤å‰æ³¨æ„å±‚ï¼Œç„¶å 2ï¼‰ä»å¹¶è¡Œ U-Net ä¸­æå–çš„ä½çº§ç‰¹å¾è¢«èåˆåˆ°è‡ªæ³¨æ„å±‚ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼ŒIDM-VTON åœ¨å›¾åƒè´¨é‡å’Œæœé¥°ä¿çœŸåº¦æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›ç»“æœæ”¯æŒäº†è¯¥æ–¹æ³•çš„ç›®æ ‡ï¼Œå³ç”ŸæˆçœŸå®ã€ä¿çœŸä¸”å¯å®šåˆ¶çš„è™šæ‹Ÿè¯•ç©¿å›¾åƒã€‚</p></li><li><p>Methods:(1): IDM-VTONé‡‡ç”¨åŸºäºç¤ºä¾‹çš„å›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªä¸åŒçš„æ¨¡å—å¯¹æœé¥°å›¾åƒçš„è¯­ä¹‰è¿›è¡Œç¼–ç ã€‚(2): è§†è§‰ç¼–ç å™¨æå–æœé¥°å›¾åƒçš„é«˜çº§è¯­ä¹‰ï¼Œå¹¶å°†å…¶èåˆåˆ°äº¤å‰æ³¨æ„å±‚ä¸­ã€‚(3): å¹¶è¡ŒU-Netæå–æœé¥°å›¾åƒçš„ä½çº§ç‰¹å¾ï¼Œå¹¶å°†å…¶èåˆåˆ°è‡ªæ³¨æ„å±‚ä¸­ã€‚(4): åœ¨ç»™å®šæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬U-Netçš„æƒ…å†µä¸‹ï¼Œèåˆåçš„é«˜çº§è¯­ä¹‰å’Œä½çº§ç‰¹å¾è¢«ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œæ„ä¹‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹ IDM-VTONï¼Œç”¨äºçœŸå®è™šæ‹Ÿè¯•ç©¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åœºæ™¯ä¸­ã€‚æˆ‘ä»¬ç»“åˆäº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å—å¯¹æœé¥°å›¾åƒè¿›è¡Œç¼–ç ï¼Œå³è§†è§‰ç¼–ç å™¨å’Œå¹¶è¡Œ U-Netï¼Œå®ƒä»¬åˆ†åˆ«æœ‰æ•ˆåœ°å¯¹åŸºæœ¬ U-Net ç¼–ç é«˜çº§è¯­ä¹‰å’Œä½çº§ç‰¹å¾ã€‚ä¸ºäº†åœ¨å®é™…åœºæ™¯ä¸­æ”¹è¿›è™šæ‹Ÿè¯•ç©¿ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¾®è°ƒç»™å®šä¸€å¯¹æœé¥°-äººç‰©å›¾åƒçš„ U-Net è§£ç å™¨å±‚æ¥å®šåˆ¶æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨äº†æœé¥°çš„è¯¦ç»†è‡ªç„¶è¯­è¨€æè¿°ï¼Œè¿™æœ‰åŠ©äºç”ŸæˆçœŸå®çš„è™šæ‹Ÿè¯•ç©¿å›¾åƒã€‚åœ¨å„ç§æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿ç•™æœé¥°ç»†èŠ‚å’Œç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢ä¼˜äºå…ˆå‰çš„ç ”ç©¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®é™…åœºæ™¯ä¸­è¿›è¡Œè™šæ‹Ÿè¯•ç©¿çš„æ½œåŠ›ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæå‡ºäº† IDM-VTONï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºçœŸå®è™šæ‹Ÿè¯•ç©¿çš„æ‰©æ•£æ¨¡å‹çš„æ–°è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åœºæ™¯ä¸­ã€‚ç»“åˆäº†ä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å—å¯¹æœé¥°å›¾åƒè¿›è¡Œç¼–ç ï¼Œå³è§†è§‰ç¼–ç å™¨å’Œå¹¶è¡Œ U-Netï¼Œå®ƒä»¬åˆ†åˆ«æœ‰æ•ˆåœ°å¯¹åŸºæœ¬ U-Net ç¼–ç é«˜çº§è¯­ä¹‰å’Œä½çº§ç‰¹å¾ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè´¨é‡å’Œæœé¥°ä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šä¸åŸºäº GAN çš„æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºç¤ºä¾‹çš„å›¾åƒä¿®å¤æ‰©æ•£æ¨¡å‹é€šå¸¸å…·æœ‰æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d38c4cb395c666b5e4fd3e52269fff3f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d67b069f37d9810aa657e9e7dd415a5a.jpg" align="middle"></details><h2 id="ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment"><a href="#ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment" class="headerlink" title="ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment"></a>ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment</h2><p><strong>Authors:Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu</strong></p><p>Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient Large Language Model Adapter, termed ELLA, which equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment without training of either U-Net or LLM. To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from LLM. Our approach adapts semantic features at different stages of the denoising process, assisting diffusion models in interpreting lengthy and intricate prompts over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their prompt-following capabilities. To assess text-to-image models in dense prompt following, we introduce Dense Prompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K dense prompts. Extensive experiments demonstrate the superiority of ELLA in dense prompt following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships. </p><p><a href="http://arxiv.org/abs/2403.05135v1">PDF</a> Project Page: <a href="https://ella-diffusion.github.io/">https://ella-diffusion.github.io/</a></p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åŠ å…¥è¯­è¨€å¤§æ¨¡å‹å¢å¼ºå™¨ ELLAï¼Œå¤§å¹…æå‡ä¸°å¯Œæç¤ºç†è§£èƒ½åŠ›ï¼Œæ— éœ€è®­ç»ƒ U å½¢ç½‘ç»œæˆ–è¯­è¨€å¤§æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ELLA è¯­è¨€å¤§æ¨¡å‹å¢å¼ºå™¨é€šè¿‡æ— ç¼è¿æ¥ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼Œæ— éœ€è®­ç»ƒ U å½¢ç½‘ç»œæˆ–è¯­è¨€å¤§æ¨¡å‹ã€‚</li><li>æå‡ºæ—¶é—´æ„ŸçŸ¥è¯­ä¹‰è¿æ¥å™¨ (TSC)ï¼ŒåŠ¨æ€ä»è¯­è¨€å¤§æ¨¡å‹ä¸­æå–ä¸æ—¶é—´æ­¥é•¿ç›¸å…³çš„æ¡ä»¶ã€‚</li><li>åœ¨å»å™ªè¿‡ç¨‹çš„ä¸åŒé˜¶æ®µï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´è¯­ä¹‰ç‰¹å¾ï¼Œå¸®åŠ©æ‰©æ•£æ¨¡å‹éšç€é‡‡æ ·æ—¶é—´æ­¥é•¿è§£é‡Šå†—é•¿å¤æ‚æç¤ºã€‚</li><li>ELLA å¯ä»¥è½»æ¾ä¸ç¤¾åŒºæ¨¡å‹å’Œå·¥å…·é›†æˆï¼Œæå‡å…¶æç¤ºéµå¾ªèƒ½åŠ›ã€‚</li><li>å¼•å…¥å¯†é›†æç¤ºå›¾åŸºå‡† (DPG-Bench)ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¯†é›†æç¤ºéµå¾ªæ–¹é¢çš„è¡¨ç°ã€‚</li><li>å¹¿æ³›å®éªŒéªŒè¯äº† ELLA åœ¨å¯†é›†æç¤ºéµå¾ªæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤šç§å±æ€§å’Œå…³ç³»çš„å¤šå¯¹è±¡ç»„åˆä¸­ã€‚</li><li>ELLA åœ¨ä¿æŒç”Ÿæˆå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæå‡äº†å®šé‡å’Œå®šæ€§è¯„ä¼°çš„æ–‡æœ¬å¯¹é½åˆ†æ•°ã€‚</li><li>ELLA å°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸è¯­è¨€å¤§æ¨¡å‹ç›¸ç»“åˆï¼Œæ¢ç´¢äº†æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆä¹‹é—´çš„æ½œåœ¨è”ç³»ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>è®ºæ–‡æ ‡é¢˜ï¼šELLAï¼šä½¿ç”¨ LLM ä¸ºæ‰©æ•£æ¨¡å‹èµ‹èƒ½ä»¥å¢å¼ºè¯­ä¹‰å¯¹é½</li><li>ä½œè€…ï¼šèƒ¡é”¡å¨ã€ç‹ç‘ã€æ–¹ä¸€æ™“ã€ä»˜æ–Œã€ç¨‹åŸ¹ã€äºé’¢</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè…¾è®¯</li><li>å…³é”®è¯ï¼šæ‰©æ•£æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹ã€æ–‡æœ¬-å›¾åƒå¯¹é½</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://ella-diffusion.github.ioï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å¹¿æ³›ä½¿ç”¨çš„æ¨¡å‹ä»ç„¶ä½¿ç”¨ CLIP ä½œä¸ºå…¶æ–‡æœ¬ç¼–ç å™¨ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç†è§£åŒ…å«å¤šä¸ªå¯¹è±¡ã€è¯¦ç»†å±æ€§ã€å¤æ‚å…³ç³»ã€é•¿æ–‡æœ¬å¯¹é½ç­‰å†…å®¹çš„å¯†é›†æç¤ºçš„èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰å·²æœ‰æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº† ELLAï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹é€‚é…å™¨ï¼Œå®ƒä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é…å¤‡äº†å¼ºå¤§çš„å¤§è¯­è¨€æ¨¡å‹ (LLM)ï¼Œä»¥å¢å¼ºæ–‡æœ¬å¯¹é½ï¼Œè€Œæ— éœ€è®­ç»ƒ U-Net æˆ– LLMã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†æ— ç¼æ¡¥æ¥ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸€ç³»åˆ—è¯­ä¹‰å¯¹é½è¿æ¥å™¨è®¾è®¡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¨¡å—ï¼Œå³ TimeStep-Aware è¯­ä¹‰è¿æ¥å™¨ (TSC)ï¼Œå®ƒåŠ¨æ€åœ°ä» LLM ä¸­æå–ä¸æ—¶é—´æ­¥é•¿ç›¸å…³çš„æ¡ä»¶ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å¯†é›†æç¤ºè·Ÿéšä»»åŠ¡ä¸­å±•ç¤ºäº†ä¼˜äºæœ€å…ˆè¿›æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸åŒå±æ€§å’Œå…³ç³»çš„å¤šä¸ªå¯¹è±¡ç»„åˆä¸­ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šè®¾è®¡ELLAæ¶æ„ï¼Œåˆ©ç”¨LLMçš„è¯­è¨€ç†è§£èƒ½åŠ›å’Œæ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆæ½œåŠ›ï¼Œé‡‡ç”¨TimeStep-Awareè¯­ä¹‰è¿æ¥å™¨ï¼ˆTSCï¼‰æ— ç¼è¿æ¥ä¸¤ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼›ï¼ˆ2ï¼‰ï¼šæ„å»ºæ•°æ®é›†ï¼Œé‡‡ç”¨CogVLMè‡ªåŠ¨ç”Ÿæˆå›¾åƒæè¿°ï¼Œæé«˜å›¾åƒä¸æ–‡æœ¬çš„ç›¸å…³æ€§å’Œè¯­ä¹‰ä¿¡æ¯çš„å¯†åº¦ï¼›ï¼ˆ3ï¼‰ï¼šæ„å»ºåŸºå‡†æµ‹è¯•ï¼Œæå‡ºå¯†é›†æç¤ºå›¾è°±åŸºå‡†ï¼ˆDPG-Benchï¼‰ï¼Œæä¾›æ›´é•¿ã€æ›´å…·ä¿¡æ¯é‡çš„æç¤ºï¼Œå…¨é¢è¯„ä¼°ç”Ÿæˆæ¨¡å‹éµå¾ªå¯†é›†æç¤ºçš„èƒ½åŠ›ã€‚</p><p>8.ç»“è®ºï¼š(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹é€‚é…å™¨ELLAï¼Œè¯¥é€‚é…å™¨é€šè¿‡TimeStep-Awareè¯­ä¹‰è¿æ¥å™¨å°†å¤§è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹æ— ç¼è¿æ¥ï¼Œå¢å¼ºäº†æ–‡æœ¬å¯¹é½ï¼Œåœ¨å¯†é›†æç¤ºè·Ÿéšä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š* æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰å¯¹é½è¿æ¥å™¨TSCï¼ŒåŠ¨æ€åœ°ä»å¤§è¯­è¨€æ¨¡å‹ä¸­æå–ä¸æ—¶é—´æ­¥é•¿ç›¸å…³çš„æ¡ä»¶ï¼Œå¢å¼ºäº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚* æ„å»ºäº†å¯†é›†æç¤ºå›¾è°±åŸºå‡†DPG-Benchï¼Œæä¾›æ›´é•¿ã€æ›´å…·ä¿¡æ¯é‡çš„æç¤ºï¼Œå…¨é¢è¯„ä¼°ç”Ÿæˆæ¨¡å‹éµå¾ªå¯†é›†æç¤ºçš„èƒ½åŠ›ã€‚* é‡‡ç”¨CogVLMè‡ªåŠ¨ç”Ÿæˆå›¾åƒæè¿°ï¼Œæé«˜å›¾åƒä¸æ–‡æœ¬çš„ç›¸å…³æ€§å’Œè¯­ä¹‰ä¿¡æ¯çš„å¯†åº¦ã€‚æ€§èƒ½ï¼š* åœ¨å¯†é›†æç¤ºè·Ÿéšä»»åŠ¡ä¸­ï¼ŒELLAåœ¨ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰å¯¹é½å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å·¥ä½œé‡ï¼š* ELLAçš„è®­ç»ƒå’Œéƒ¨ç½²ç›¸å¯¹é«˜æ•ˆï¼Œä¸éœ€è¦è®­ç»ƒU-Netæˆ–å¤§è¯­è¨€æ¨¡å‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-4cf50b2bd0a34d7b9b26b53c13b5a923.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc587ddf93c75ebf159a0c6b73925633.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a0b7496441cb8c23d5d6a09243c13c67.jpg" align="middle"></details>## CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion**Authors:Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang**Recent advancements in text-to-image generative systems have been largely driven by diffusion models. However, single-stage text-to-image diffusion models still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of text-to-image diffusion. CogView3 is the first model implementing relay diffusion in the realm of text-to-image generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive text-to-image outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source text-to-image diffusion model, by 77.0\% in human evaluations, all while requiring only about 1/2 of the inference time. The distilled variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL. [PDF](http://arxiv.org/abs/2403.05121v1) **Summary**CogView3ï¼Œä¸€ä¸ªçº§è”æ¡†æ¶ï¼Œå¼•å…¥æ¥åŠ›æ‰©æ•£ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡ï¼Œæé«˜æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚**Key Takeaways**- CogView3æå‡ºçº§è”æ¡†æ¶ï¼Œä½¿ç”¨æ¥åŠ›æ‰©æ•£ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒã€‚- æ¥åŠ›æ‰©æ•£åˆ†æ­¥ç”Ÿæˆå›¾åƒï¼Œä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡ï¼Œé™ä½è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚- CogView3è¶…è¶ŠSDXLï¼Œäººç±»è¯„ä¼°å¾—åˆ†é«˜å‡º77.0%ï¼Œæ¨ç†æ—¶é—´å‡å°‘ä¸€åŠã€‚- CogView3çš„ç²¾ç®€ç‰ˆæ€§èƒ½ç›¸å½“ï¼Œæ¨ç†æ—¶é—´ä»…ä¸ºSDXLçš„ååˆ†ä¹‹ä¸€ã€‚- CogView3æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚- CogView3 å¼•å…¥äº†æ¥åŠ›æ‰©æ•£çš„æ¦‚å¿µï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å®ç°äº†åˆ†è¾¨ç‡çš„æ¸è¿›æå‡ã€‚- çº§è”æ¡†æ¶å’Œæ¥åŠ›æ‰©æ•£çš„ç»“åˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡å›¾åƒè´¨é‡å’Œè®¡ç®—æˆæœ¬ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šCogView3ï¼šæ›´ç²¾ç»†ã€æ›´å¿«é€Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ</li><li>ä½œè€…ï¼šWendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”ŸæˆÂ·æ‰©æ•£æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05121</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç³»ç»Ÿçš„ä¸»æµæ¡†æ¶ã€‚ç„¶è€Œï¼Œå•é˜¶æ®µæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œå›¾åƒç»†èŠ‚ç²¾ç»†åŒ–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚(2) è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å¤§å¤šåœ¨é«˜å›¾åƒåˆ†è¾¨ç‡ä¸‹è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œè¿™å¯¼è‡´è®¡ç®—æˆæœ¬é«˜ã€å›¾åƒç»†èŠ‚ä¸å¤Ÿç²¾ç»†ã€‚(3) æå‡ºæ–¹æ³•ï¼šæœ¬æ–‡æå‡º CogView3ï¼Œä¸€ä¸ªåˆ›æ–°çš„çº§è”æ¡†æ¶ï¼Œé€šè¿‡ä¸­ç»§æ‰©æ•£æ¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£çš„æ€§èƒ½ã€‚CogView3 æ˜¯ç¬¬ä¸€ä¸ªåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå®ç°ä¸­ç»§æ‰©æ•£çš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡é¦–å…ˆåˆ›å»ºä½åˆ†è¾¨ç‡å›¾åƒï¼Œç„¶ååº”ç”¨åŸºäºä¸­ç»§çš„è¶…åˆ†è¾¨ç‡æ¥æ‰§è¡Œä»»åŠ¡ã€‚(4) å®éªŒç»“æœï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCogView3 åœ¨äººç±»è¯„ä¼°ä¸­æ¯”å½“å‰æœ€å…ˆè¿›çš„å¼€æºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ SDXL é«˜å‡º 77.0%ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´ä»…ä¸ºå…¶ä¸€åŠå·¦å³ã€‚CogView3 çš„è’¸é¦å˜ä½“åœ¨æ¨ç†æ—¶é—´ä»…ä¸º SDXL çš„ 1/10 çš„æƒ…å†µä¸‹å®ç°äº†å¯æ¯”çš„æ€§èƒ½ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰æ–‡æœ¬é¢„å¤„ç†å›¾åƒé‡è¿°ï¼šåˆ©ç”¨ GPT-4V è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®é›†å›¾åƒçš„é‡è¿°æ–‡æœ¬ï¼Œå¹¶å¾®è°ƒ CogVLM-17B ä»¥è·å¾—é‡è¿°æ¨¡å‹ï¼›ï¼ˆ2ï¼‰æç¤ºæ‰©å±•ï¼šåˆ©ç”¨è¯­è¨€æ¨¡å‹å°†ç”¨æˆ·æç¤ºæ‰©å±•ä¸ºæ›´å…¨é¢çš„æè¿°ï¼Œä»¥å‡å°‘è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„ä¸ä¸€è‡´ï¼›ï¼ˆ3ï¼‰æ¨¡å‹æ„å»ºï¼šCogView3 é‡‡ç”¨ 3 çº§ UNet æ¶æ„çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„ T5-XXL ç¼–ç å™¨ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨ï¼›ï¼ˆ4ï¼‰è®­ç»ƒç®¡é“ï¼šä½¿ç”¨ Laion-2B æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨æ¸è¿›è®­ç»ƒç­–ç•¥ä»¥é™ä½è®­ç»ƒæˆæœ¬ï¼›ï¼ˆ5ï¼‰ä¸­ç»§è¶…åˆ†è¾¨ç‡ï¼šåœ¨æ½œåœ¨ç©ºé—´ä¸­å®ç°ä¸­ç»§è¶…åˆ†è¾¨ç‡ï¼Œä½¿ç”¨çº¿æ€§å˜æ¢ä»£æ›¿åŸå§‹çš„å±€éƒ¨æ¨¡ç³Šï¼›ï¼ˆ6ï¼‰é‡‡æ ·å™¨æ„å»ºï¼šè®¾è®¡äº†ä¸ä¸­ç»§è¶…åˆ†è¾¨ç‡ç›¸ä¸€è‡´çš„é‡‡æ ·å™¨ï¼Œå¹¶ä½¿ç”¨ DDIM èŒƒå¼è¿›è¡Œé‡‡æ ·ï¼›ï¼ˆ7ï¼‰ä¸­ç»§æ‰©æ•£çš„è’¸é¦ï¼šå°†æ¸è¿›è’¸é¦æ–¹æ³•ä¸ä¸­ç»§æ‰©æ•£æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥è·å¾— CogView3 çš„è’¸é¦ç‰ˆæœ¬ã€‚</p></li><li><p>ç»“è®º(1): æœ¬å·¥ä½œæå‡ºäº† CogView3ï¼Œè¿™æ˜¯ç»§ç”µæ‰©æ•£æ¡†æ¶ä¸­ç¬¬ä¸€ä¸ªæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç³»ç»Ÿã€‚CogView3 ä»¥æå¤§é™ä½çš„æ¨ç†æˆæœ¬å®ç°äº†ä¼˜è‰¯çš„ç”Ÿæˆè´¨é‡ï¼Œè¿™ä¸»è¦å½’åŠŸäºä¸­ç»§ç®¡é“ã€‚é€šè¿‡è¿­ä»£å®ç° CogView3 çš„è¶…åˆ†è¾¨ç‡é˜¶æ®µï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°æé«˜åˆ†è¾¨ç‡ï¼ˆå¦‚ 2048Ã—2048ï¼‰çš„é«˜è´¨é‡å›¾åƒã€‚åŒæ—¶ï¼Œéšç€æ•°æ®é‡æ–°æè¿°å’Œæç¤ºæ‰©å±•è¢«çº³å…¥æ¨¡å‹ç®¡é“ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„å¼€æºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒCogView3 åœ¨æç¤ºç†è§£å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº† CogView3 çš„è’¸é¦ï¼Œå¹¶å±•ç¤ºäº†å…¶å½’åŠŸäºç»§ç”µæ‰©æ•£æ¡†æ¶çš„ç®€å•æ€§å’Œèƒ½åŠ›ã€‚åˆ©ç”¨æ¸è¿›è’¸é¦èŒƒä¾‹ï¼ŒCogView3 çš„è’¸é¦å˜ä½“å¤§å¹…å‡å°‘äº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä»ä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„çº§è”æ¡†æ¶ CogView3ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸­ç»§æ‰©æ•£å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£çš„æ€§èƒ½ã€‚</li><li>è®¾è®¡äº†ä¸€ç§ä¸­ç»§è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œè¶…åˆ†è¾¨ç‡ï¼Œå¹¶ä½¿ç”¨çº¿æ€§å˜æ¢ä»£æ›¿åŸå§‹çš„å±€éƒ¨æ¨¡ç³Šã€‚</li><li>æ¢ç´¢äº†æ•°æ®é‡æ–°æè¿°å’Œæç¤ºæ‰©å±•ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æç¤ºçš„ç†è§£å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚æ€§èƒ½ï¼š</li><li>åœ¨äººç±»è¯„ä¼°ä¸­ï¼ŒCogView3 æ¯”å½“å‰æœ€å…ˆè¿›çš„å¼€æºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ SDXL é«˜å‡º 77.0%ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´ä»…ä¸ºå…¶ä¸€åŠå·¦å³ã€‚</li><li>CogView3 çš„è’¸é¦å˜ä½“åœ¨æ¨ç†æ—¶é—´ä»…ä¸º SDXL çš„ 1/10 çš„æƒ…å†µä¸‹å®ç°äº†å¯æ¯”çš„æ€§èƒ½ã€‚</li><li>CogView3 èƒ½å¤Ÿç”Ÿæˆæé«˜åˆ†è¾¨ç‡ï¼ˆå¦‚ 2048Ã—2048ï¼‰çš„é«˜è´¨é‡å›¾åƒã€‚å·¥ä½œé‡ï¼š</li><li>CogView3 çš„è®­ç»ƒç®¡é“ç›¸å¯¹ç®€å•ï¼Œé‡‡ç”¨æ¸è¿›è®­ç»ƒç­–ç•¥ä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚</li><li>CogView3 çš„è’¸é¦å˜ä½“è¿›ä¸€æ­¥é™ä½äº†æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„æ€§èƒ½ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-39c07129df4e18479bf6f2000e3bd45b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3130242f65670e2f9a99c29710ffccef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a4b8e0b9de2b5980d7c1d4c49daded3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7d124475c2a36f974604208e23b856.jpg" align="middle"></details><h2 id="Face2Diffusion-for-Fast-and-Editable-Face-Personalization"><a href="#Face2Diffusion-for-Fast-and-Editable-Face-Personalization" class="headerlink" title="Face2Diffusion for Fast and Editable Face Personalization"></a>Face2Diffusion for Fast and Editable Face Personalization</h2><p><strong>Authors:Kaede Shiohara, Toshihiko Yamasaki</strong></p><p>Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.05094v1">PDF</a> CVPR2024. Code: <a href="https://github.com/mapooon/Face2Diffusion">https://github.com/mapooon/Face2Diffusion</a>, Webpage:   <a href="https://mapooon.github.io/Face2DiffusionPage/">https://mapooon.github.io/Face2DiffusionPage/</a></p><p><strong>Summary</strong><br>äººè„¸ä¸ªæ€§åŒ–é€šè¿‡æ¤å…¥ä»å›¾ç‰‡è·å–çš„äººè„¸æ¥å®ç°é¢„å…ˆè®­ç»ƒçš„æ–‡è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä»è®­ç»ƒç®¡é“ä¸­å»é™¤ä¸äººè„¸æ— å…³çš„ä¿¡æ¯æœ‰åŠ©äºæå‡ç¼–è¾‘èƒ½åŠ›ã€‚</li><li>å¤šå°ºåº¦äººè„¸ç¼–ç å™¨æä¾›äº†æ¸…æ™°åˆ†ç¦»çš„äººè„¸ç‰¹å¾ã€‚</li><li>è¡¨æƒ…æŒ‡å¯¼å°†äººè„¸è¡¨æƒ…ä¸äººè„¸èº«ä»½è¿›è¡Œåˆ†ç¦»ã€‚</li><li>ç±»åˆ«å¼•å¯¼å»å™ªæ­£åˆ™åŒ–å¢å¼ºæ¨¡å‹å¯¹äººè„¸å»å™ªçš„å­¦ä¹ ã€‚</li><li>è·¨æ•°æ®é›†å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æå‡äº†èº«ä»½ä¿çœŸåº¦ä¸æ–‡æœ¬ä¿çœŸåº¦ä¹‹é—´çš„å¹³è¡¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šFace2Diffusionï¼šå¿«é€Ÿä¸”å¯ç¼–è¾‘çš„äººè„¸ä¸ªæ€§åŒ–</li><li>ä½œè€…ï¼šKaede Shiohara, Toshihiko Yamasaki</li><li>å•ä½ï¼šä¸œäº¬å¤§å­¦</li><li>å…³é”®è¯ï¼šFace personalization, Text-to-image diffusion model, Identity preservation, Editability</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05094</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†ç‰¹å®šäººè„¸æ’å…¥é¢„è®­ç»ƒæ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ—¢è¦ä¿æŒèº«ä»½ç›¸ä¼¼æ€§ï¼Œåˆè¦ä¿è¯å¯ç¼–è¾‘æ€§ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å®¹æ˜“è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ ·æœ¬ï¼Œå¯¼è‡´èº«ä»½ç›¸ä¼¼æ€§å’Œå¯ç¼–è¾‘æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šFace2Diffusionï¼ˆF2Dï¼‰é€šè¿‡ä»è®­ç»ƒç®¡é“ä¸­å»é™¤ä¸èº«ä»½æ— å…³çš„ä¿¡æ¯æ¥è§£å†³è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œæé«˜ç¼–ç äººè„¸çš„å¯ç¼–è¾‘æ€§ã€‚F2DåŒ…å«ä¸‰ä¸ªæ–°é¢–çš„ç»„ä»¶ï¼šå¤šå°ºåº¦èº«ä»½ç¼–ç å™¨ã€è¡¨æƒ…å¼•å¯¼å™¨å’Œç±»åˆ«å¼•å¯¼å»å™ªæ­£åˆ™åŒ–ã€‚ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ FaceForensics++ æ•°æ®é›†å’Œå„ç§æç¤ºä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒF2D åœ¨èº«ä»½å’Œæ–‡æœ¬ä¿çœŸåº¦ä¹‹é—´çš„æƒè¡¡æ–¹é¢æ˜æ˜¾ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å¤šå°ºåº¦èº«ä»½ç¼–ç å™¨ï¼šä»äººè„¸å›¾åƒä¸­æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œä¿ç•™èº«ä»½ä¿¡æ¯ï¼Œé™ä½è¿‡åº¦æ‹Ÿåˆé£é™©ã€‚(2) è¡¨æƒ…å¼•å¯¼å™¨ï¼šæŒ‡å¯¼æ‰©æ•£æ¨¡å‹å…³æ³¨äººè„¸è¡¨æƒ…çš„ç¼–è¾‘ï¼Œæé«˜å¯ç¼–è¾‘æ€§ã€‚(3) ç±»åˆ«å¼•å¯¼å»å™ªæ­£åˆ™åŒ–ï¼šå¼•å…¥ç±»åˆ«ä¿¡æ¯ï¼Œé˜²æ­¢æ¨¡å‹ä»æ— å…³å™ªå£°ä¸­å­¦ä¹ ï¼Œæé«˜èº«ä»½ä¿çœŸåº¦ã€‚</p></li></ol><p><strong>8. ç»“è®º</strong></p><p><strong>(1): æ­¤é¡¹å·¥ä½œçš„æ„ä¹‰</strong></p><p>Face2Diffusion æå‡ºäº†ä¸€ç§å¯ç¼–è¾‘çš„äººè„¸ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œé€šè¿‡è§£å†³è¿‡åº¦æ‹Ÿåˆé—®é¢˜ï¼Œæé«˜äº†ç”Ÿæˆäººè„¸çš„å¯ç¼–è¾‘æ€§ï¼Œåœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬ä¿çœŸåº¦ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚</p><p><strong>(2): æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆä¸‰ä¸ªç»´åº¦ï¼šåˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ï¼‰</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>å¤šå°ºåº¦èº«ä»½ç¼–ç å™¨ï¼šæå–å¤šå°ºåº¦ç‰¹å¾ï¼Œé™ä½è¿‡åº¦æ‹Ÿåˆé£é™©ã€‚</li><li>è¡¨æƒ…å¼•å¯¼å™¨ï¼šæŒ‡å¯¼æ‰©æ•£æ¨¡å‹å…³æ³¨äººè„¸è¡¨æƒ…çš„ç¼–è¾‘ã€‚</li><li>ç±»åˆ«å¼•å¯¼å»å™ªæ­£åˆ™åŒ–ï¼šé˜²æ­¢æ¨¡å‹ä»æ— å…³å™ªå£°ä¸­å­¦ä¹ ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬ä¿çœŸåº¦ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚</li><li>åœ¨å„ç§æç¤ºå’Œäººè„¸æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è®­ç»ƒè¿‡ç¨‹ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¤šå°ºåº¦ç‰¹å¾æå–å’Œæ­£åˆ™åŒ–ç­–ç•¥ã€‚</li><li>ç”Ÿæˆå•ä¸ªå›¾åƒæ‰€éœ€çš„æ—¶é—´ä¸å…¶ä»–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç±»ä¼¼ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-a4d3199be75c4ed763ad12e5fd6fd186.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-073fc885846ed7841fbefca59dc75bb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2c9194bd5afd5f761cca65c865fe0fb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b6ba7d02ff97010b563089ea86c62c6b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bdf1923916c837b5df8251aa84ce58b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5d8555605f33ef6be1a8b7ab0be10cc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-302c4a1ee77cbdfd8dba69c7d6a94497.jpg" align="middle"></details>## Spectrum Translation for Refinement of Image Generation (STIG) Based on   Contrastive Learning and Spectral Filter Profile**Authors:Seokjun Lee, Seung-Won Jung, Hyunseok Seo**Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG. [PDF](http://arxiv.org/abs/2403.05093v1) Accepted to AAAI 2024**Summary**ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ä¸­é¢‘åŸŸå·®å¼‚é—®é¢˜ï¼Œå¯é€šè¿‡é¢‘è°±å¯¹æ¯”å­¦ä¹ ä¸‹çš„å›¾åƒç”Ÿæˆè°±è½¬æ¢æ¡†æ¶ï¼ˆSTIGï¼‰æœ‰æ•ˆè§£å†³ã€‚**Key Takeaways*** æå‡ºSTIGæ¡†æ¶å‡è½»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹å›¾åƒé¢‘åŸŸå·®å¼‚ã€‚* STIGåŸºäºå›¾åƒåˆ°å›¾åƒè½¬æ¢å’Œå¯¹ç…§å­¦ä¹ ï¼Œä¼˜åŒ–ç”Ÿæˆå›¾åƒé¢‘è°±ã€‚* STIGåœ¨å…«ä¸ªä¼ªé€ å›¾åƒæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œæ˜¾ç€é™ä½FIDå’Œå…‰è°±çš„å¯¹æ•°é¢‘ç‡è·ç¦»ã€‚* STIGé€šè¿‡å‡å°å…‰è°±å¼‚å¸¸æé«˜å›¾åƒè´¨é‡ã€‚* ç»è¿‡STIGå¤„ç†çš„ä¼ªé€ å›¾åƒä¼šè¿·æƒ‘åŸºäºé¢‘ç‡çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨ã€‚* STIGä½¿ç”¨é¢‘è°±è½¬æ¢æœ‰æ•ˆè§£å†³ç”Ÿæˆæ¨¡å‹ä¸­é¢‘åŸŸå·®å¼‚é—®é¢˜ã€‚* STIGæå‡å›¾åƒç”Ÿæˆè´¨é‡ï¼Œå¢å¼ºå¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„é²æ£’æ€§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šå›¾åƒç”Ÿæˆç²¾ç‚¼çš„å…‰è°±è½¬æ¢ï¼ˆSTIGï¼‰</li><li>ä½œè€…ï¼šSeokjun Leeã€Seung-Won Jungã€Hyunseok Seo</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šéŸ©å›½ç§‘å­¦æŠ€æœ¯ç ”ç©¶é™¢ç”Ÿç‰©åŒ»å­¦ç ”ç©¶éƒ¨</li><li>å…³é”®è¯ï¼šå›¾åƒç”Ÿæˆã€å…‰è°±è½¬æ¢ã€å¯¹æ¯”å­¦ä¹ ã€é¢‘è°±æ»¤æ³¢å™¨è½®å»“</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç›®å‰ï¼Œå›¾åƒç”Ÿæˆå’Œåˆæˆåœ¨ç”Ÿæˆæ¨¡å‹çš„å¸®åŠ©ä¸‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å°½ç®¡ç”Ÿæˆç»“æœé€¼çœŸï¼Œä½†åœ¨é¢‘åŸŸä¸­ä»ç„¶å­˜åœ¨å›ºæœ‰çš„å·®å¼‚ã€‚è¿™ç§é¢‘è°±å·®å¼‚ä¸ä»…å‡ºç°åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸­ï¼Œè¿˜å‡ºç°åœ¨æ‰©æ•£æ¨¡å‹ä¸­ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€çš„ç ”ç©¶æå‡ºäº†é€šè¿‡ä¿®æ”¹ç”Ÿæˆç½‘ç»œæ¶æ„æˆ–ç›®æ ‡å‡½æ•°æ¥å¼¥è¡¥é¢‘åŸŸå·®å¼‚çš„æ–¹æ³•ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„å…‰è°±è½¬æ¢æ¡†æ¶ï¼ˆSTIGï¼‰ï¼Œç”¨äºæœ‰æ•ˆå‡è½»ç”Ÿæˆå›¾åƒé¢‘åŸŸä¸­çš„å·®å¼‚ï¼Œä»¥æé«˜ GAN å’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ•°å­—ä¿¡å·å¤„ç†ä¸­å›¾åƒåˆ°å›¾åƒè½¬æ¢å’Œå¯¹æ¯”å­¦ä¹ çš„æ¦‚å¿µæ¥ä¼˜åŒ–ç”Ÿæˆå›¾åƒçš„å…‰è°±ã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨å…«ä¸ªå‡å›¾åƒæ•°æ®é›†å’Œå„ç§å‰æ²¿æ¨¡å‹ä¸Šè¯„ä¼°äº† STIG çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒSTIG ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ï¼Œåœ¨ FID å’Œå…‰è°±å¯¹æ•°é¢‘ç‡è·ç¦»æ–¹é¢æœ‰æ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼ŒSTIG é€šè¿‡å‡å°‘å…‰è°±å¼‚å¸¸æ¥æé«˜å›¾åƒè´¨é‡ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œå½“ STIG å¤„ç†è™šå‡å…‰è°±æ—¶ï¼ŒåŸºäºé¢‘ç‡çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨æ›´å®¹æ˜“æ··æ·†ã€‚</li></ol><p>7.Methodsï¼šï¼ˆ1ï¼‰STIGæ¡†æ¶æ¦‚è¿°ï¼šSTIGæ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šå›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œï¼ˆI2Iï¼‰ã€å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°å’Œé¢‘è°±æ»¤æ³¢å™¨è½®å»“ï¼ˆSFPï¼‰ã€‚ï¼ˆ2ï¼‰å›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œï¼ˆI2Iï¼‰ï¼šI2Iç½‘ç»œé‡‡ç”¨U-Netæ¶æ„ï¼Œç”¨äºå°†ç”Ÿæˆå›¾åƒä»æºé¢‘åŸŸè½¬æ¢åˆ°ç›®æ ‡é¢‘åŸŸã€‚ï¼ˆ3ï¼‰å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°ï¼šå¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°åŸºäºå›¾åƒå¯¹çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œé€šè¿‡æœ€å¤§åŒ–ç›¸ä¼¼å›¾åƒçš„ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸åŒå›¾åƒçš„ç‰¹å¾è¡¨ç¤ºä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ¥ä¼˜åŒ–I2Iç½‘ç»œã€‚ï¼ˆ4ï¼‰é¢‘è°±æ»¤æ³¢å™¨è½®å»“ï¼ˆSFPï¼‰ï¼šSFPæ˜¯ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„é¢‘è°±æ»¤æ³¢å™¨é›†åˆï¼Œç”¨äºæŒ‡å¯¼I2Iç½‘ç»œå­¦ä¹ ç›®æ ‡é¢‘åŸŸçš„ç‰¹å¾åˆ†å¸ƒã€‚ï¼ˆ5ï¼‰STIGè®­ç»ƒè¿‡ç¨‹ï¼šSTIGæ¡†æ¶çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œå¾®è°ƒé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒI2Iç½‘ç»œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°å’ŒSFPè¿›è¡Œè®­ç»ƒã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼ŒI2Iç½‘ç»œä½¿ç”¨ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å¯¹æŠ—æ€§æŸå¤±å‡½æ•°è¿›è¡Œå¾®è°ƒã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† STIG æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç›´æ¥æ“ä½œç”Ÿæˆå›¾åƒçš„é¢‘ç‡åˆ†é‡ï¼Œåœ¨é¢‘åŸŸä¸­å‡å°‘ç”Ÿæˆå›¾åƒçš„å…‰è°±å·®å¼‚ï¼Œä»è€Œæé«˜ç”Ÿæˆæ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šSTIG æ¡†æ¶åœ¨é¢‘åŸŸä¸­ç›´æ¥æ“ä½œç”Ÿæˆå›¾åƒï¼Œä»¥å‡å°‘ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒä¹‹é—´çš„å…‰è°±å·®å¼‚ã€‚STIG æ¡†æ¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°å’Œé¢‘è°±æ»¤æ³¢å™¨è½®å»“ï¼Œä¼˜åŒ–å›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ã€‚STIG æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°æé«˜ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½ã€‚æ€§èƒ½ï¼šSTIG æ¡†æ¶åœ¨å…«ä¸ªå‡å›¾åƒåŸºå‡†ä¸Šå‡ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ï¼Œåœ¨ FID å’Œå…‰è°±å¯¹æ•°é¢‘ç‡è·ç¦»æ–¹é¢æœ‰æ˜¾è‘—ä¸‹é™ã€‚STIG æ¡†æ¶é€šè¿‡å‡å°‘å…‰è°±å¼‚å¸¸æ¥æé«˜å›¾åƒè´¨é‡ã€‚STIG æ¡†æ¶å¤„ç†è™šå‡å…‰è°±æ—¶ï¼ŒåŸºäºé¢‘ç‡çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨æ›´å®¹æ˜“æ··æ·†ã€‚å·¥ä½œé‡ï¼šSTIG æ¡†æ¶çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬é¢„è®­ç»ƒé˜¶æ®µå’Œå¾®è°ƒé˜¶æ®µã€‚é¢„è®­ç»ƒé˜¶æ®µéœ€è¦å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°å’Œé¢‘è°±æ»¤æ³¢å™¨è½®å»“è¿›è¡Œè®­ç»ƒã€‚å¾®è°ƒé˜¶æ®µéœ€è¦å¯¹å›¾åƒåˆ°å›¾åƒè½¬æ¢ç½‘ç»œä½¿ç”¨ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å¯¹æŠ—æ€§æŸå¤±å‡½æ•°è¿›è¡Œå¾®è°ƒã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-59b9082b16c536f6e3dc82d3eedb0929.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc9ad99c3613618bd289ca6d732974f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed21a9f11c14097979acb60a01fc0faa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee45629a830dd20e3e691c354e6c5761.jpg" align="middle"><img src="https://picx.zhimg.com/v2-82896ffced53d8bc120b544471040628.jpg" align="middle"></details><h2 id="Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport"><a href="#Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport" class="headerlink" title="Improving Diffusion-Based Generative Models via Approximated Optimal   Transport"></a>Improving Diffusion-Based Generative Models via Approximated Optimal   Transport</h2><p><strong>Authors:Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon</strong></p><p>We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for diffusion-based generative models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of diffusion models to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing the performance of diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05069v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é€šè¿‡è¿‘ä¼¼æœ€ä¼˜ä¼ è¾“ï¼ˆAOTï¼‰æŠ€æœ¯æå‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ•ˆæœï¼Œé™ä½é‡‡æ ·è¯¯å·®ï¼Œæå‡å›¾åƒè´¨é‡ã€‚</p><p><strong>è¦ç‚¹</strong></p><ul><li>æå‡ºè¿‘ä¼¼æœ€ä¼˜ä¼ è¾“ï¼ˆAOTï¼‰æŠ€æœ¯ï¼Œæ”¹è¿›æ‰©æ•£æ¨¡å‹è®­ç»ƒã€‚</li><li>AOT æŠ€æœ¯å°†æœ€ä¼˜ä¼ è¾“æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼Œæå‡å»å™ªè¾“å‡ºå‡†ç¡®æ€§ã€‚</li><li>ä¼˜åŒ–åçš„æ‰©æ•£è½¨è¿¹æ›²ç‡é™ä½ï¼Œé‡‡æ ·æˆªæ–­è¯¯å·®å‡å°ã€‚</li><li>é‡‡ç”¨ AOT è®­ç»ƒï¼Œå›¾åƒè´¨é‡æå‡ï¼Œé‡‡æ ·æ­¥éª¤å‡å°‘ã€‚</li><li>æ— æ¡ä»¶ç”Ÿæˆä¸­ï¼Œ27 æ¬¡è¯ºç¦å…‹åºåˆ—ï¼ˆNFEï¼‰ï¼ŒFID å¾—åˆ†è¾¾åˆ° 1.88ï¼›29 æ¬¡ NFEï¼ŒFID å¾—åˆ†è¾¾åˆ° 1.73ã€‚</li><li>æ¡ä»¶ç”Ÿæˆä¸­ï¼Œ29 æ¬¡ NFEï¼ŒFID å¾—åˆ†è¾¾åˆ° 1.68ï¼›æŒ‡å¯¼åˆ¤åˆ«å™¨è®­ç»ƒï¼ŒFID å¾—åˆ†è¾¾åˆ° 1.58ã€‚</li><li>AOT æŠ€æœ¯æœ‰æ•ˆæå‡äº†æ‰©æ•£æ¨¡å‹æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šé€šè¿‡è¿‘ä¼¼æœ€ä¼˜ä¼ è¾“æ”¹è¿›åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹</li><li>ä½œè€…ï¼šDaegyu Kimã€Jooyoung Choiã€Chaehun Shinã€Uiwon Hwangã€Sungroh Yoon</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦–å°”å›½ç«‹å¤§å­¦æ•°æ®ç§‘å­¦ä¸äººå·¥æ™ºèƒ½å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç”Ÿæˆæ¨¡å‹ã€æ‰©æ•£æ¨¡å‹ã€æœ€ä¼˜ä¼ è¾“</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05069   Github ä»£ç é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§é€šè¿‡é€æ¸å»å™ªæ¥åˆæˆå›¾åƒçš„ç”Ÿæˆæ¨¡å‹ã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ ODE è½¨è¿¹æ›²ç‡é«˜çš„é—®é¢˜ï¼Œè¿™ä¼šå½±å“å›¾åƒè´¨é‡å’Œé‡‡æ ·æ•ˆç‡ã€‚</li></ol><p>ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šFlowMatching ç­‰æ–¹æ³•æå‡ºäº†ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æ¥è§£å†³æ›²ç‡é—®é¢˜ï¼Œä½†ç”±äºæ‰©æ•£æ¨¡å‹çš„ç»“æ„ï¼Œç›´æ¥åº”ç”¨è¿™äº›æ–¹æ³•å­˜åœ¨è®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è¿‘ä¼¼æœ€ä¼˜ä¼ è¾“ï¼ˆAOTï¼‰è®­ç»ƒæŠ€æœ¯ï¼Œå°†æœ€ä¼˜ä¼ è¾“è¿‘ä¼¼å¹¶æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»è€Œé™ä½ ODE è½¨è¿¹çš„æ›²ç‡å’Œæˆªæ–­è¯¯å·®ã€‚</p><p>ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°åŠæ€§èƒ½ï¼šåœ¨ CIFAR-10 å›¾åƒæ— æ¡ä»¶å’Œæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œä¸åŸºçº¿ç ”ç©¶å’Œ EDM ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œ NFEï¼ˆå‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼‰æ–¹é¢å‡å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æ— æ¡ä»¶ç”Ÿæˆä¸­ï¼Œæœ¬æ–‡æ–¹æ³•ä»¥ 27 NFE å®ç°äº† 1.88 çš„ FID å¾—åˆ†ï¼Œä»¥ 29 NFE å®ç°äº† 1.73 çš„ FID å¾—åˆ†ï¼›åœ¨æ¡ä»¶ç”Ÿæˆä¸­ï¼Œä»¥ 29 NFE å®ç°äº† 1.68 çš„ FID å¾—åˆ†ï¼Œä»¥ 29 NFE å®ç°äº† 1.58 çš„ FID å¾—åˆ†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒAOT æŠ€æœ¯å¯ä»¥æœ‰æ•ˆæå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å·¥ä½œé€šè¿‡æå‡ºè¿‘ä¼¼æœ€ä¼˜ä¼ è¾“ï¼ˆAOTï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆé™ä½äº†æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„æ›²ç‡å’Œæˆªæ–­è¯¯å·®ï¼Œä»è€Œæå‡äº†å›¾åƒç”Ÿæˆè´¨é‡å’Œé‡‡æ ·æ•ˆç‡ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºè¿‘ä¼¼æœ€ä¼˜ä¼ è¾“ï¼ˆAOTï¼‰æŠ€æœ¯ï¼Œå°†æœ€ä¼˜ä¼ è¾“è¿‘ä¼¼å¹¶æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé™ä½äº† ODE è½¨è¿¹çš„æ›²ç‡å’Œæˆªæ–­è¯¯å·®ã€‚</li><li>å°† AOT æŠ€æœ¯æˆåŠŸé›†æˆåˆ° Discriminator Guidanceï¼ˆDGï¼‰æ¡†æ¶ä¸­ï¼Œå±•ç¤ºäº†å…¶åœ¨æ›´å¹¿æ³›åº”ç”¨ä¸­çš„å¤šåŠŸèƒ½æ€§å’Œæ½œåŠ›ã€‚æ€§èƒ½ï¼š</li><li>åœ¨ CIFAR-10 å›¾åƒæ— æ¡ä»¶å’Œæ¡ä»¶ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œä¸åŸºçº¿ç ”ç©¶å’Œ EDM ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œ NFEï¼ˆå‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼‰æ–¹é¢å‡å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li><li>åœ¨æ— æ¡ä»¶ç”Ÿæˆä¸­ï¼Œæœ¬æ–‡æ–¹æ³•ä»¥ 27NFE å®ç°äº† 1.88 çš„ FID å¾—åˆ†ï¼Œä»¥ 29NFE å®ç°äº† 1.73 çš„ FID å¾—åˆ†ï¼›åœ¨æ¡ä»¶ç”Ÿæˆä¸­ï¼Œä»¥ 29NFE å®ç°äº† 1.68 çš„ FID å¾—åˆ†ï¼Œä»¥ 29NFE å®ç°äº† 1.58 çš„ FID å¾—åˆ†ã€‚å·¥ä½œé‡ï¼š</li><li>ä¸ EDM ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨è®­ç»ƒæˆæœ¬ä¸Šç•¥æœ‰å¢åŠ ï¼ˆ2% åˆ° 15%ï¼‰ã€‚</li><li>æœ¬æ–¹æ³•éœ€è¦ç®—æ³•æ”¹è¿›ï¼Œä»¥æ‰©å±•å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ç”Ÿæˆï¼ˆä¾‹å¦‚æ–‡æœ¬æŒ‡å¯¼ç”Ÿæˆï¼‰ä¸­çš„é€‚ç”¨æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8b3484bb01610ca257b110266a789659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a1905f26c3dd85ac5906dbc02f95a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-06436ae944972e738965038412bab51a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-092c4ba972936da93fe5ca9a1e0c861e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3004bc0c97615bac6076ff6a3cd11e53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63b88ad2349cca16dbda28634bc2b6d1.jpg" align="middle"></details><h2 id="XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution"><a href="#XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution" class="headerlink" title="XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution"></a>XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</h2><p><strong>Authors:Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</strong></p><p>Diffusion-based methods, endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a \textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the diffusion model, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a \textit{Semantic-Fusion Attention} is raised. To distill semantic-preserved information instead of undesired degradations, a \textit{Degradation-Free Constraint} is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes will be released at \url{<a href="https://github.com/qyp2000/XPSR}">https://github.com/qyp2000/XPSR}</a>. </p><p><a href="http://arxiv.org/abs/2403.05049v1">PDF</a> 19 pages, 7 figures</p><p><strong>Summary</strong><br>åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œè¯­ä¹‰èåˆç­–ç•¥ï¼Œæå‡ºä¸€ç§å›¾åƒè¶…åˆ†è¾¨ç‡æ¡†æ¶XPSRï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸå’Œé€¼çœŸçš„å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¼å…ˆéªŒæå‡å›¾åƒè¶…åˆ†è¾¨ç‡æ€§èƒ½ã€‚</li><li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æä¾›ç²¾ç¡®è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>è¯­ä¹‰èåˆæ³¨æ„åŠ›ä¿ƒè¿›è·¨æ¨¡æ€å…ˆéªŒèåˆã€‚</li><li>æ— é€€åŒ–çº¦æŸæå–è¯­ä¹‰å†…å®¹ï¼Œè€Œéé€€åŒ–ä¿¡æ¯ã€‚</li><li>XPSRåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šç”Ÿæˆé«˜è´¨é‡è¶…åˆ†è¾¨ç‡å›¾åƒã€‚</li><li>XPSRä»£ç å°†äº<a href="https://github.com/qyp2000/XPSRå‘å¸ƒã€‚">https://github.com/qyp2000/XPSRå‘å¸ƒã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šXPSRï¼šç”¨äºåŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡çš„è·¨æ¨¡æ€å…ˆéªŒ</li><li>ä½œè€…ï¼šæ›²äº‘é¹ã€è¢å¤ã€èµµå‡¯ã€è°¢å¯ä¹‹ã€éƒé‡‘åã€å­™æ˜ã€å‘¨è¶…</li><li>å•ä½ï¼šæ¸…åå¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤ã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05049Githubä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆISRï¼‰æ–¹æ³•å› å…¶å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒè€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒé€šå¸¸ä¼šé­å—ä¸¥é‡çš„é€€åŒ–ï¼Œå› æ­¤å¯¹äºISRæ¨¡å‹æ¥è¯´ï¼Œæ„ŸçŸ¥è¯­ä¹‰å’Œé€€åŒ–ä¿¡æ¯å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯¼è‡´æ¢å¤çš„å›¾åƒå†…å®¹ä¸æ­£ç¡®æˆ–å‡ºç°ä¸çœŸå®çš„ä¼ªå½±ã€‚ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡çš„åŠ¨æœºæ˜¯è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ä»¥å¾€æ–¹æ³•ä¸»è¦ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰è¿›è¡Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œä½†GANåœ¨ç”Ÿæˆé€¼çœŸçº¹ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”å­˜åœ¨åˆæˆè®­ç»ƒæ•°æ®å’ŒçœŸå®ä¸–ç•Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„åŸŸå·®è·é—®é¢˜ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè·¨æ¨¡æ€å…ˆéªŒè¶…åˆ†è¾¨ç‡ï¼ˆXPSRï¼‰æ¡†æ¶ã€‚åœ¨XPSRä¸­ï¼Œåˆ©ç”¨å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸ºæ‰©æ•£æ¨¡å‹è·å–å‡†ç¡®å’Œå…¨é¢çš„è¯­ä¹‰æ¡ä»¶ã€‚ä¸ºäº†ä¿ƒè¿›è·¨æ¨¡æ€å…ˆéªŒçš„æ›´å¥½èåˆï¼Œæå‡ºäº†ä¸€ç§è¯­ä¹‰èåˆæ³¨æ„åŠ›æœºåˆ¶ã€‚ä¸ºäº†æå–è¯­ä¹‰ä¿ç•™çš„ä¿¡æ¯è€Œä¸æ˜¯ä¸éœ€è¦çš„é€€åŒ–ï¼Œåœ¨LRåŠå…¶é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å¯¹åº”å›¾åƒä¹‹é—´é™„åŠ äº†ä¸€ä¸ªæ— é€€åŒ–çº¦æŸã€‚ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šå®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒXPSRèƒ½å¤Ÿè·¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ç”Ÿæˆé«˜ä¿çœŸå’Œé«˜é€¼çœŸçš„å›¾åƒã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆè§£å†³å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­çš„æŒ‘æˆ˜ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) é‡‡ç”¨å¤§è¯­è¨€æ¨¡å‹ LLaVA è·å–å›¾åƒçš„è¯­ä¹‰å…ˆéªŒï¼ŒåŒ…æ‹¬é«˜å±‚è¯­ä¹‰å’Œä½å±‚è¯­ä¹‰ï¼›(2) ä½¿ç”¨è¯­ä¹‰èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­ä¹‰å…ˆéªŒä¸ T2I æ¨¡å‹ç”Ÿæˆçš„å…ˆéªŒæœ‰æ•ˆèåˆï¼›(3) æ·»åŠ æ— é€€åŒ–çº¦æŸï¼Œä» LR å›¾åƒä¸­æå–è¯­ä¹‰ä¿ç•™ä½†ä¸é€€åŒ–æ— å…³çš„ä¿¡æ¯ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºçš„ XPSR æ¡†æ¶è§£å†³äº†åŸºäºæ‰©æ•£çš„å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹åœ¨å‡†ç¡®æ¢å¤è¯­ä¹‰ç»†èŠ‚æ–¹é¢çš„éš¾é¢˜ï¼Œä¸ºå›¾åƒè¶…åˆ†è¾¨ç‡é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰ä¼˜ç¼ºç‚¹æ€»ç»“ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºè·¨æ¨¡æ€å…ˆéªŒæ¦‚å¿µï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸ºæ‰©æ•£æ¨¡å‹æä¾›å‡†ç¡®å…¨é¢çš„è¯­ä¹‰æ¡ä»¶ã€‚</li><li>è®¾è®¡è¯­ä¹‰èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆèåˆè¯­ä¹‰å…ˆéªŒå’Œ T2I æ¨¡å‹ç”Ÿæˆçš„å…ˆéªŒã€‚</li><li>å¼•å…¥æ— é€€åŒ–çº¦æŸï¼Œä»ä½åˆ†è¾¨ç‡å›¾åƒä¸­æå–è¯­ä¹‰ä¿ç•™ä½†ä¸é€€åŒ–æ— å…³çš„ä¿¡æ¯ã€‚æ€§èƒ½ï¼š</li><li>å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒXPSR èƒ½å¤Ÿè·¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ç”Ÿæˆé«˜ä¿çœŸå’Œé«˜é€¼çœŸçš„å›¾åƒã€‚</li><li>ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒXPSR åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼š</li><li>XPSR æ¡†æ¶çš„å®ç°éœ€è¦ä¸€å®šçš„å·¥ä½œé‡ï¼ŒåŒ…æ‹¬è®­ç»ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ã€‚</li><li>æ­¤å¤–ï¼Œè¯­ä¹‰èåˆæ³¨æ„åŠ›æœºåˆ¶å’Œæ— é€€åŒ–çº¦æŸçš„å®ç°ä¹Ÿéœ€è¦é¢å¤–çš„å¼€å‘å·¥ä½œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7216c617badf932e3f8d18daf0977b1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c7194197140a421dc8eb74d3c744901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca3923ee7424c689775b0bb281aa1184.jpg" align="middle"></details><h2 id="DiffClass-Diffusion-Based-Class-Incremental-Learning"><a href="#DiffClass-Diffusion-Based-Class-Incremental-Learning" class="headerlink" title="DiffClass: Diffusion-Based Class Incremental Learning"></a>DiffClass: Diffusion-Based Class Incremental Learning</h2><p><strong>Authors:Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, Yanzhi WAng</strong></p><p>Class Incremental Learning (CIL) is challenging due to catastrophic forgetting. On top of that, Exemplar-free Class Incremental Learning is even more challenging due to forbidden access to previous task data. Recent exemplar-free CIL methods attempt to mitigate catastrophic forgetting by synthesizing previous task data. However, they fail to overcome the catastrophic forgetting due to the inability to deal with the significant domain gap between real and synthetic data. To overcome these issues, we propose a novel exemplar-free CIL method. Our method adopts multi-distribution matching (MDM) diffusion models to unify quality and bridge domain gaps among all domains of training data. Moreover, our approach integrates selective synthetic image augmentation (SSIA) to expand the distribution of the training data, thereby improving the modelâ€™s plasticity and reinforcing the performance of our methodâ€™s ultimate component, multi-domain adaptation (MDA). With the proposed integrations, our method then reformulates exemplar-free CIL into a multi-domain adaptation problem to implicitly address the domain gap problem to enhance model stability during incremental training. Extensive experiments on benchmark class incremental datasets and settings demonstrate that our method excels previous exemplar-free CIL methods and achieves state-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2403.05016v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>å¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹åœ¨æ— ä¾‹å¯å¾ªçš„ç±»å¢é‡å­¦ä¹ ä¸­è§£å†³ç¾éš¾æ€§é—å¿˜å’Œé¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œé€šè¿‡å¤šåŸŸé€‚åº”éšå¼è§£å†³é¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç±»å¢é‡å­¦ä¹ é¢ä¸´ç¾éš¾æ€§é—å¿˜å’Œæ— ä¾‹å¯å¾ªçš„æŒ‘æˆ˜ã€‚</li><li>æ— ä¾‹å¯å¾ªçš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•é€šè¿‡åˆæˆå…ˆå‰ä»»åŠ¡æ•°æ®æ¥å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li><li>æ­¤ç±»æ–¹æ³•ç”±äºæ— æ³•å¤„ç†çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´çš„æ˜¾ç€é¢†åŸŸå·®å¼‚è€Œæ— æ³•å…‹æœç¾éš¾æ€§é—å¿˜ã€‚</li><li>æå‡ºä¸€ç§æ–°çš„æ— ä¾‹å¯å¾ªçš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œé‡‡ç”¨å¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹ç»Ÿä¸€è´¨é‡å’Œå¼¥åˆæ‰€æœ‰è®­ç»ƒæ•°æ®åŸŸä¹‹é—´çš„é¢†åŸŸå·®å¼‚ã€‚</li><li>è¯¥æ–¹æ³•é›†æˆäº†é€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼ºï¼Œä»¥æ‰©å±•è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒã€‚</li><li>è¿™ç§æ–¹æ³•å°†æ— ä¾‹å¯å¾ªçš„ç±»å¢é‡å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºå¤šåŸŸé€‚åº”é—®é¢˜ï¼Œä»¥éšå¼è§£å†³é¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œæé«˜æ¨¡å‹åœ¨å¢é‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…ˆå‰çš„æ— ä¾‹å¯å¾ªçš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£çš„ç±»å¢é‡å­¦ä¹ </li><li>ä½œè€…ï¼šå­Ÿå­èªï¼Œå¼ æ°ï¼Œæ¨æ˜Œè¿ªï¼Œè©¹æ”¿ï¼Œèµµæ™®ï¼Œç‹å»¶ä¹‹</li><li>ä¸œåŒ—å¤§å­¦</li><li>ClassIncrementalLearningï¼ŒExemplarFreeï¼ŒDiffusionModel</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.05016   Githubä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç±»å¢é‡å­¦ä¹ ï¼ˆCILï¼‰å› ç¾éš¾æ€§é—å¿˜è€Œæå…·æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç”±äºæ— æ³•è®¿é—®å…ˆå‰ä»»åŠ¡çš„æ•°æ®ï¼Œæ— ç¤ºä¾‹ CIL æ›´æ˜¯éš¾ä¸ŠåŠ éš¾ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šæœ€è¿‘çš„æ— ç¤ºä¾‹ CIL æ–¹æ³•å°è¯•é€šè¿‡åˆæˆå…ˆå‰ä»»åŠ¡æ•°æ®æ¥ç¼“è§£ç¾éš¾æ€§é—å¿˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç”±äºæ— æ³•å¤„ç†çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®ä¹‹é—´çš„å·¨å¤§åŸŸå·®è·è€Œæ— æ³•å…‹æœç¾éš¾æ€§é—å¿˜ã€‚ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— ç¤ºä¾‹ CIL æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šåˆ†å¸ƒåŒ¹é… (MDM) æ‰©æ•£æ¨¡å‹æ¥å¯¹é½åˆæˆæ•°æ®çš„è´¨é‡ï¼Œå¹¶å¼¥åˆè®­ç»ƒæ•°æ®æ‰€æœ‰åŸŸä¹‹é—´çš„åŸŸå·®è·ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„æ–¹æ³•é›†æˆäº†é€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼º (SSIA) æ¥æ‰©å±•è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„å¯å¡‘æ€§å¹¶å¢å¼ºå¤šåŸŸè‡ªé€‚åº” (MDA) æŠ€æœ¯çš„æ€§èƒ½ã€‚é€šè¿‡æå‡ºçš„é›†æˆï¼Œæœ¬æ–‡çš„æ–¹æ³•å°†æ— ç¤ºä¾‹ CIL é‡æ–°è¡¨è¿°ä¸ºå¤šåŸŸè‡ªé€‚åº”é—®é¢˜ï¼Œä»¥éšå¼è§£å†³åŸŸå·®è·é—®é¢˜å¹¶å¢å¼ºæ¨¡å‹åœ¨å¢é‡è®­ç»ƒæœŸé—´çš„ç¨³å®šæ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨åŸºå‡† CIL æ•°æ®é›†å’Œè®¾ç½®ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„æ— ç¤ºä¾‹ CIL æ–¹æ³•ï¼Œå…·æœ‰éè¾¹é™…æ”¹è¿›ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) å¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹ç²¾è°ƒï¼šä½¿ç”¨ LoRA ç²¾è°ƒå¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹ï¼Œå¯¹é½åˆæˆæ•°æ®çš„è´¨é‡ï¼Œç¼©å°è®­ç»ƒæ•°æ®æ‰€æœ‰åŸŸä¹‹é—´çš„åŸŸå·®è·ã€‚(2) é€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼ºï¼šé€šè¿‡é€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼ºæ‰©å±•è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹çš„å¯å¡‘æ€§ï¼Œå¢å¼ºå¤šåŸŸè‡ªé€‚åº”æŠ€æœ¯çš„æ€§èƒ½ã€‚(3) å¤šåŸŸè‡ªé€‚åº”ï¼šé‡‡ç”¨å¤šåŸŸè‡ªé€‚åº”è®­ç»ƒæ–¹æ³•ï¼Œå°†æ— ç¤ºä¾‹ CIL é‡æ–°è¡¨è¿°ä¸ºå¤šåŸŸè‡ªé€‚åº”é—®é¢˜ï¼Œéšå¼è§£å†³åŸŸå·®è·é—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹åœ¨å¢é‡è®­ç»ƒæœŸé—´çš„ç¨³å®šæ€§ã€‚</p></li></ol><p>8.ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°é¢–æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹å’Œé€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼ºæœ‰æ•ˆè§£å†³äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶é€šè¿‡å¤šåŸŸè‡ªé€‚åº”æŠ€æœ¯å¢å¼ºäº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œå¯å¡‘æ€§ï¼Œåœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š* åŸºäºå¤šåˆ†å¸ƒåŒ¹é…æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾å¼å¼¥åˆåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„åŸŸå·®è·ã€‚* é‡‡ç”¨é€‰æ‹©æ€§åˆæˆå›¾åƒå¢å¼ºï¼Œæ‰©å±•è®­ç»ƒæ•°æ®åˆ†å¸ƒï¼Œæé«˜æ¨¡å‹çš„å¯å¡‘æ€§ã€‚* å°†æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ é‡æ–°è¡¨è¿°ä¸ºå¤šåŸŸè‡ªé€‚åº”é—®é¢˜ï¼Œéšå¼è§£å†³åŸŸå·®è·é—®é¢˜ï¼Œå¢å¼ºæ¨¡å‹åœ¨å¢é‡è®­ç»ƒæœŸé—´çš„ç¨³å®šæ€§ã€‚æ€§èƒ½ï¼š* åœ¨ CIFAR100 å’Œ ImageNet100 åŸºå‡†æ•°æ®é›†ä¸Šï¼Œåœ¨å„ç§æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ è®¾ç½®ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚* æ¶ˆèç ”ç©¶è¯æ˜äº†æœ¬æ–‡æ–¹æ³•ä¸­æ¯ä¸ªç»„ä»¶åœ¨æ— ç¤ºä¾‹ç±»å¢é‡å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚å·¥ä½œé‡ï¼š* æ¯ä¸ªå¢é‡ä»»åŠ¡çš„è®­ç»ƒæ—¶é—´ç›¸å¯¹è¾ƒé•¿ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ LoRA å¾®è°ƒç”Ÿæˆæ¨¡å‹çš„æ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3071368b15837785fc8226279a7a69f4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-022f350905045d5945b926c68a304727.jpg" align="middle"><img src="https://picx.zhimg.com/v2-191fbfc51055a8bc7b2acc064efa3416.jpg" align="middle"><img src="https://picx.zhimg.com/v2-70ca4a001e09124d997a32d6f30da7f0.jpg" align="middle"></details>## StereoDiffusion: Training-Free Stereo Image Generation Using Latent   Diffusion Models**Authors:Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand Bigdeli**The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations. [PDF](http://arxiv.org/abs/2403.04965v1) **Summary**ç«‹ä½“æ‰©æ•£ï¼šæ— è®­ç»ƒã€ç®€å•æ˜“ç”¨ï¼Œæ— ç¼é›†æˆåŸæœ‰ Stable Diffusion æ¨¡å‹ï¼Œç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ã€‚**Key Takeaways**- StereoDiffusion æ— éœ€è®­ç»ƒï¼Œä½¿ç”¨æ–¹ä¾¿ã€‚- ä¸åŸå§‹ Stable Diffusion æ¨¡å‹æ— ç¼é›†æˆã€‚- ç”Ÿæˆç«‹ä½“å›¾åƒå¯¹æ—¶æ— éœ€å¾®è°ƒæ¨¡å‹æƒé‡æˆ–å›¾åƒåå¤„ç†ã€‚- åˆ©ç”¨åŸå§‹è¾“å…¥ç”Ÿæˆå·¦å›¾åƒå¹¶ä¼°è®¡å…¶è§†å·®å›¾ã€‚- ä½¿ç”¨ç«‹ä½“åƒç´ ä½ç§»æ“ä½œç”Ÿæˆå³å›¾åƒçš„æ½œå˜é‡ã€‚- ä½¿ç”¨å¯¹ç§°åƒç´ ä½ç§»æ©ç å»å™ªå’Œè‡ªæ³¨æ„åŠ›å±‚ä¿®æ”¹æ–¹æ³•ã€‚- ä¿æŒç«‹ä½“ç”Ÿæˆè¿‡ç¨‹ä¸­å›¾åƒè´¨é‡çš„é«˜æ ‡å‡†ã€‚- åœ¨å„ç§å®šé‡è¯„ä¼°ä¸­å–å¾—æœ€å…ˆè¿›çš„åˆ†æ•°ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>æ ‡é¢˜ï¼šç«‹ä½“æ‰©æ•£ï¼šåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ— è®­ç»ƒç«‹ä½“å›¾åƒç”Ÿæˆ</li><li>ä½œè€…ï¼šLezhong Wangã€Jeppe Revall Frisvadã€Mark Bo Jensenã€Siavash Arjomand Bigdeli</li><li>éš¶å±å•ä½ï¼šä¸¹éº¦æŠ€æœ¯å¤§å­¦åº”ç”¨æ•°å­¦ä¸è®¡ç®—æœºç§‘å­¦ç³»</li><li>å…³é”®è¯ï¼šXRã€æ·±åº¦å›¾åƒ/è§†é¢‘åˆæˆã€å›¾åƒç¼–è¾‘ã€äººå·¥æ™ºèƒ½ã€ä¿®å¤ã€Stable Diffusion</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.04965   Github ä»£ç é“¾æ¥ï¼šæ— </li><li><p>æ‘˜è¦ï¼š   (1)ï¼šéšç€åˆ¶é€ å•†æ¨å‡ºæ›´å¤š XR è®¾å¤‡ï¼Œå¯¹ç«‹ä½“å›¾åƒçš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚ä¸ºäº†æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç«‹ä½“æ‰©æ•£ï¼Œè¿™æ˜¯ä¸€ç§ä¸ä¼ ç»Ÿä¿®å¤ç®¡é“ä¸åŒã€æ— éœ€è®­ç»ƒã€ä½¿ç”¨æå…¶ç®€å•ä¸”å¯ä¸åŸå§‹ Stable Diffusion æ¨¡å‹æ— ç¼é›†æˆçš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†æ½œåœ¨å˜é‡ï¼Œæä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è½»é‡çº§åŠŸèƒ½ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ï¼Œè€Œæ— éœ€å¾®è°ƒæ¨¡å‹æƒé‡æˆ–å¯¹å›¾åƒè¿›è¡Œä»»ä½•åå¤„ç†ã€‚æˆ‘ä»¬ä½¿ç”¨åŸå§‹è¾“å…¥ç”Ÿæˆå·¦ä¾§å›¾åƒå¹¶ä¼°è®¡å…¶è§†å·®å›¾ï¼Œé€šè¿‡ç«‹ä½“åƒç´ ä½ç§»æ“ä½œç”Ÿæˆå³ä¾§å›¾åƒçš„æ½œåœ¨å‘é‡ï¼Œå¹¶è¾…ä»¥å¯¹ç§°åƒç´ ä½ç§»æ©ç å»å™ªå’Œè‡ªæ³¨æ„åŠ›å±‚ä¿®æ”¹æ–¹æ³•ï¼Œå°†å³ä¾§å›¾åƒä¸å·¦ä¾§å›¾åƒå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ•´ä¸ªç«‹ä½“ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒäº†è¾ƒé«˜çš„å›¾åƒè´¨é‡æ ‡å‡†ï¼Œåœ¨å„ç§å®šé‡è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„å¾—åˆ†ã€‚   (2)ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå›¾åƒä¿®å¤ç®¡é“ï¼Œè¯¥ç®¡é“éœ€è¦é¢å¤–çš„æ¨¡å‹è¿›è¡Œåå¤„ç†ä»¥ç”Ÿæˆç«‹ä½“å›¾åƒã€‚è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¯¹æ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”ç”Ÿæˆè¿‡ç¨‹å¤æ‚ä¸”è€—æ—¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¿®æ”¹ Stable Diffusion æ¨¡å‹çš„æ½œåœ¨å˜é‡æ¥ç›´æ¥ç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹æˆ–åå¤„ç†ã€‚è¿™ç§æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„ç«‹ä½“å›¾åƒã€‚   (3)ï¼šæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„ç«‹ä½“å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå®ƒä¿®æ”¹äº† Stable Diffusion æ¨¡å‹çš„æ½œåœ¨å˜é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸå§‹è¾“å…¥ç”Ÿæˆå·¦ä¾§å›¾åƒå¹¶ä¼°è®¡å…¶è§†å·®å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ç«‹ä½“åƒç´ ä½ç§»æ“ä½œç”Ÿæˆå³ä¾§å›¾åƒçš„æ½œåœ¨å‘é‡ã€‚ä¸ºäº†å¯¹é½å³ä¾§å›¾åƒå’Œå·¦ä¾§å›¾åƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¯¹ç§°åƒç´ ä½ç§»æ©ç å»å™ªå’Œè‡ªæ³¨æ„åŠ›å±‚ä¿®æ”¹æ–¹æ³•ã€‚   (4)ï¼šæˆ‘ä»¬åœ¨ç«‹ä½“å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§å®šé‡è¯„ä¼°ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„å¾—åˆ†ï¼ŒåŒ…æ‹¬ PSNRã€SSIM å’Œ LPIPSã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç«‹ä½“å›¾åƒï¼Œå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ”¯æŒæˆ‘ä»¬çš„ç›®æ ‡ï¼Œå³å¿«é€Ÿç”Ÿæˆæ— éœ€è®­ç»ƒçš„ç«‹ä½“å›¾åƒå¯¹ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ä½¿ç”¨åŸå§‹è¾“å…¥ç”Ÿæˆå·¦ä¾§å›¾åƒå¹¶ä¼°è®¡å…¶è§†å·®å›¾ï¼›ï¼ˆ2ï¼‰é€šè¿‡ç«‹ä½“åƒç´ ä½ç§»æ“ä½œç”Ÿæˆå³ä¾§å›¾åƒçš„æ½œåœ¨å‘é‡ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨å¯¹ç§°åƒç´ ä½ç§»æ©ç å»å™ªå’Œè‡ªæ³¨æ„åŠ›å±‚ä¿®æ”¹æ–¹æ³•å¯¹é½å³ä¾§å›¾åƒå’Œå·¦ä¾§å›¾åƒã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šç«‹ä½“æ‰©æ•£ï¼šåŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ— è®­ç»ƒç«‹ä½“å›¾åƒç”Ÿæˆï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§é€šè¿‡ä¿®æ”¹æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨å˜é‡æ¥ç”Ÿæˆç«‹ä½“å›¾åƒå¯¹çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€é¢å¤–çš„æ¨¡å‹æˆ–åå¤„ç†ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„ç«‹ä½“å›¾åƒã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æ— éœ€è®­ç»ƒï¼šè¯¥æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒï¼Œç›´æ¥ç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ï¼Œç®€åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚</li><li>ç«¯åˆ°ç«¯ï¼šè¯¥æ–¹æ³•ä¿®æ”¹æ½œåœ¨å˜é‡ï¼Œæä¾›äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è½»é‡çº§åŠŸèƒ½ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ã€‚</li><li>ä¸åŸå§‹StableDiffusionæ¨¡å‹æ— ç¼é›†æˆï¼šè¯¥æ–¹æ³•å¯ä»¥ä¸åŸå§‹StableDiffusionæ¨¡å‹æ— ç¼é›†æˆï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚æ€§èƒ½ï¼š</li><li>å®šé‡è¯„ä¼°ï¼šè¯¥æ–¹æ³•åœ¨KITTIå’ŒMiddleburyæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„å¾—åˆ†ï¼Œè¡¨æ˜å…¶å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç«‹ä½“å›¾åƒã€‚å·¥ä½œé‡ï¼š</li><li>è®¡ç®—æˆæœ¬ï¼šè¯¥æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒä½ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆç«‹ä½“å›¾åƒå¯¹ã€‚</li><li>å†…å­˜å ç”¨ï¼šè¯¥æ–¹æ³•çš„å†…å­˜å ç”¨è¾ƒå°ï¼Œå¯ä»¥åœ¨å„ç§è®¾å¤‡ä¸Šè¿è¡Œã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-2042e22706397759569cb6c0ac2c19fc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a050df593611d8551bcd2b7e676c281.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4970b55916ca916d6716d8304932590e.jpg" align="middle"></details><h2 id="AFreeCA-Annotation-Free-Counting-for-All"><a href="#AFreeCA-Annotation-Free-Counting-for-All" class="headerlink" title="AFreeCA: Annotation-Free Counting for All"></a>AFreeCA: Annotation-Free Counting for All</h2><p><strong>Authors:Adriano Dâ€™Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</strong></p><p>Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable \textit{sorting} signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms other unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code to be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.04943v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ (LDM) è‡ªåŠ¨ç”Ÿæˆåˆ†ç±»æ•°æ®ï¼Œç„¶åé€šè¿‡æ— ç›‘ç£å­¦ä¹ å’Œå¯†åº¦åˆ†ç±»æŒ‡å¯¼æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°ç±»åˆ«æ— å…³çš„æ— ç›‘ç£å¯¹è±¡è®¡æ•°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>LDMs èƒ½å¤Ÿæä¾›å›¾åƒæ·»åŠ å’Œåˆ é™¤å¯¹è±¡çš„å¯é åˆ†ç±»ä¿¡å·ã€‚</li><li>åˆ©ç”¨ LDM ç”Ÿæˆçš„åˆ†ç±»æ•°æ®ï¼Œå¯ä»¥æ— ç›‘ç£åœ°å­¦ä¹ ä¸å¯¹è±¡ç›¸å…³çš„ç‰¹å¾ã€‚</li><li>é€šè¿‡è®¡æ•°æ•°æ®å¯¹ç‰¹å¾è¿›è¡Œç²¾ç‚¼å’Œé”šå®šã€‚</li><li>å¯†åº¦åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹æ³•å¯å°†å›¾åƒåˆ’åˆ†ä¸ºåŒ…å«å¯è¢«å¯é è®¡æ•°çš„å¯¹è±¡çš„åŒºåŸŸã€‚</li><li>è¯¥æ–¹æ³•å¯ç”Ÿæˆä»»ä½•ç±»å‹å¯¹è±¡çš„è®¡æ•°æ•°æ®ï¼Œå¹¶èƒ½ä»¥æ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œè®¡æ•°ã€‚</li><li>ç›¸å¯¹äºå…¶ä»–æ— ç›‘ç£å’Œå°‘æ ·æœ¬æ›¿ä»£æ–¹æ³•å…·æœ‰è¾ƒå¥½çš„æ€§èƒ½ã€‚</li><li>æ— éœ€ç‰¹å®šå¯¹è±¡ç±»åˆ«å³å¯ç”Ÿæˆè®¡æ•°æ•°æ®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ— æ ‡æ³¨è®¡æ•°ï¼šå¯†åº¦åˆ†ç±»å™¨å¼•å¯¼åˆ†åŒº</li><li>ä½œè€…ï¼šLu Qi, Minghao Chen, Junwei Han, Yu Liu, Xiang Bai, Xiaogang Wang</li><li>å•ä½ï¼šæ— </li><li>å…³é”®è¯ï¼šObjectCountingÂ·SyntheticDataÂ·Annotation-Free</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06673   Github é“¾æ¥ï¼šæ— </li><li>æ‘˜è¦ï¼š   (1) ç ”ç©¶èƒŒæ™¯ï¼šç›®æ ‡è®¡æ•°æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†ç½‘ç»œé’ˆå¯¹ç‰¹å®šç±»åˆ«ï¼ˆå¦‚äººæˆ–ä¼é¹…ï¼‰è®¡æ•°ç›®æ ‡çš„é€šç”¨æ€§ï¼Œå¹¶ä¸”å¯¹ä¸åŒç±»åˆ«ç›®æ ‡çš„è®¡æ•°ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚   (2) è¿‡å»æ–¹æ³•ï¼šæ— ç›‘ç£ã€å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æ–¹æ³•æ—¨åœ¨ä½¿ç”¨åŒ…å«ä¸åŒç±»åˆ«çš„å¤§å‹äººå·¥æ ‡æ³¨æ•°æ®é›†æ¥åˆ›å»ºé€‚ç”¨äºä»»ä½•ç±»åˆ«çš„é€šç”¨è®¡æ•°ç½‘ç»œã€‚å°‘æ ·æœ¬æ–¹æ³•ä¾èµ–äºä»ç›®æ ‡å›¾åƒä¸­é‡‡æ ·çš„æ ·æœ¬ä¾‹æ¥å®šä¹‰ç›®æ ‡ç±»åˆ«ï¼Œè€Œé›¶æ ·æœ¬æ–¹æ³•ä½¿ç”¨æ–‡æœ¬æç¤ºã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºå¹¿æ³›çš„æ ‡æ³¨æ•°æ®é›†ï¼Œä½†   (3) æœ¬æ–‡æ–¹æ³•ï¼šåˆ©ç”¨äº†æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚LDM éš¾ä»¥ä»…åŸºäºæ–‡æœ¬æç¤ºåˆ›å»ºå…·æœ‰ç²¾ç¡®æ•°é‡ç›®æ ‡çš„å›¾åƒï¼Œä½†å¯ä»¥é€šè¿‡æ·»åŠ å’Œç§»é™¤å›¾åƒä¸­çš„ç›®æ ‡æ¥æä¾›å¯é çš„æ’åºä¿¡å·ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œæœ¬æ–‡é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ— ç›‘ç£æ’åºæ–¹æ³•æ¥å­¦ä¹ ç›®æ ‡ç›¸å…³ç‰¹å¾ï¼Œéšåä½¿ç”¨ LDM ç”Ÿæˆçš„è®¡æ•°æ•°æ®å¯¹è¿™äº›ç‰¹å¾è¿›è¡Œç²¾ç‚¼å’Œé”šå®šä»¥ç”¨äºè®¡æ•°ç›®çš„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å¯†åº¦åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºåŒ…å«å¯è¢«å¯é è®¡æ•°çš„ç›®æ ‡çš„å—ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¯ä»¥ä¸ºä»»ä½•ç±»å‹çš„ç›®æ ‡ç”Ÿæˆè®¡æ•°æ•°æ®å¹¶ä»¥æ— ç›‘ç£çš„æ–¹å¼å¯¹å…¶è¿›è¡Œè®¡æ•°ã€‚   (4) æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•ä¼˜äºå…¶ä»–æ— ç›‘ç£å’Œå°‘æ ·æœ¬æ›¿ä»£æ–¹æ³•ï¼Œå¹¶ä¸”ä¸å—ç‰¹å®šç›®æ ‡ç±»åˆ«çš„é™åˆ¶ï¼Œè¿™äº›ç±»åˆ«æœ‰å¯ç”¨çš„è®¡æ•°æ•°æ®ã€‚</li></ol><p>7.æ–¹æ³•ï¼š(1)ç”Ÿæˆåˆæˆæ’åºæ•°æ®ï¼Œé€šè¿‡æ·»åŠ å’Œç§»é™¤å›¾åƒä¸­çš„ç›®æ ‡ï¼Œä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¯¹å›¾åƒè¿›è¡Œæ’åºï¼›(2)é¢„è®­ç»ƒæ’åºç½‘ç»œï¼Œä½¿ç”¨æ’åºæŸå¤±å’Œå…³ç³»æŸå¤±ï¼Œå¯¹å›¾åƒç‰¹å¾è¿›è¡Œæ’åºï¼›(3)ä»åˆæˆæ•°æ®å­¦ä¹ è®¡æ•°ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ’åºç½‘ç»œï¼Œé€šè¿‡å¾®è°ƒçº¿æ€§å±‚ï¼Œå°†ç‰¹å¾é”šå®šåˆ°å®é™…è®¡æ•°å€¼ï¼›(4)äººç¾¤å¯†åº¦åˆ†ç±»ï¼Œä½¿ç”¨ Stable Diffusion ç”Ÿæˆåˆæˆæ•°æ®ï¼Œå¯¹äººç¾¤å¯†åº¦è¿›è¡Œåˆ†ç±»ï¼›(5)å¯†åº¦åˆ†ç±»å™¨å¼•å¯¼åˆ†åŒºï¼ˆDCGPï¼‰ï¼Œæ ¹æ®ä¼°è®¡çš„å¯†åº¦å¯¹å›¾åƒè¿›è¡Œåˆ†åŒºï¼Œå°†å›¾åƒå¤„ç†ä¸ºæ›´å°çš„è¡¥ä¸ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„ç›®æ ‡è®¡æ•°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ–‡æœ¬åˆ°å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ç”Ÿæˆçš„åˆæˆæ•°æ®ã€‚è¯¥æ–¹æ³•é€šè¿‡æ’åºå’Œé”šå®šå­¦ä¹ ç›®æ ‡ç›¸å…³ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¯†åº¦åˆ†ç±»å™¨å¼•å¯¼åˆ†åŒºï¼ˆDCGPï¼‰å°†å›¾åƒåˆ’åˆ†ä¸ºåŒ…å«å¯è¢«å¯é è®¡æ•°çš„ç›®æ ‡çš„å—ã€‚è¯¥æ–¹æ³•ä¸å—ç‰¹å®šç›®æ ‡ç±»åˆ«çš„é™åˆ¶ï¼Œå¹¶ä¸”ä¼˜äºå…¶ä»–æ— ç›‘ç£å’Œå°‘æ ·æœ¬æ›¿ä»£æ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>åˆ©ç”¨LDMç”Ÿæˆåˆæˆæ’åºæ•°æ®å’Œè®¡æ•°æ•°æ®ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li><li>æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ’åºæ–¹æ³•ï¼Œå­¦ä¹ ç›®æ ‡ç›¸å…³ç‰¹å¾ã€‚</li><li>æå‡ºäº†ä¸€ç§å¯†åº¦åˆ†ç±»å™¨å¼•å¯¼åˆ†åŒºï¼ˆDCGPï¼‰æ–¹æ³•ï¼Œå°†å›¾åƒåˆ’åˆ†ä¸ºåŒ…å«å¯è¢«å¯é è®¡æ•°çš„ç›®æ ‡çš„å—ã€‚æ€§èƒ½ï¼š</li><li>åœ¨PASCAL VOCã€COCOå’ŒCityscapesæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–æ— ç›‘ç£å’Œå°‘æ ·æœ¬æ›¿ä»£æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•ä¸å—ç‰¹å®šç›®æ ‡ç±»åˆ«çš„é™åˆ¶ï¼Œå¯ä»¥ä¸ºä»»ä½•ç±»å‹çš„ç›®æ ‡ç”Ÿæˆè®¡æ•°æ•°æ®å¹¶ä»¥æ— ç›‘ç£çš„æ–¹å¼å¯¹å…¶è¿›è¡Œè®¡æ•°ã€‚å·¥ä½œé‡ï¼š</li><li>è¯¥æ–¹æ³•éœ€è¦ç”Ÿæˆåˆæˆæ’åºæ•°æ®å’Œè®¡æ•°æ•°æ®ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li><li>è¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ’åºç½‘ç»œå’Œå¾®è°ƒçº¿æ€§å±‚ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0bdfaf4b65221e3f6287dfe2ed850459.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7e6e2c7b151a6f679f9aa91c763c21aa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25087217d0ca2a3290d33e79013e2984.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04536de3c0849a068b94d559fbfb1068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7e2084d668b0f9c9e859eecaa8550c.jpg" align="middle"></details><h2 id="An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control"><a href="#An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control" class="headerlink" title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control"></a>An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control</h2><p><strong>Authors:Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</strong></p><p>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.04880v1">PDF</a> </p><p><strong>Summary</strong><br>æ–‡æœ¬æç¤ºç¼–è¾‘å®ç°äº†å›¾åƒç¼–è¾‘ï¼Œä½†ç”±äºæ‰©æ•£æ¨¡å‹çš„é¢„è®­ç»ƒæ–¹å¼ï¼Œç›´æ¥ç¼–è¾‘æç¤ºä¸­çš„æ–‡å­—ä¼šå¯¼è‡´ç”Ÿæˆå®Œå…¨ä¸åŒçš„å›¾åƒï¼Œè¿èƒŒäº†å›¾åƒç¼–è¾‘çš„è¦æ±‚ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºæ–‡æœ¬æç¤ºç¼–è¾‘æ–¹æ³• D-Editã€‚</li><li>å°†å›¾åƒæç¤ºäº¤äº’åˆ†è§£ä¸ºå¤šä¸ªé¡¹ç›®æç¤ºäº¤äº’ï¼Œæ¯ä¸ªé¡¹ç›®é“¾æ¥åˆ°ä¸€ä¸ªç‰¹æ®Šå­¦ä¹ æç¤ºã€‚</li><li>é‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–æ„å»ºé¡¹ç›®æç¤ºå…³è”ã€‚</li><li>å¯è¿›è¡Œå¤šç§å›¾åƒç¼–è¾‘ï¼ŒåŒ…æ‹¬åŸºäºå›¾åƒã€åŸºäºæ–‡æœ¬ã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ã€‚</li><li>å¯ä»¥åœ¨å•ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­å®ç°æ‰€æœ‰ç±»å‹çš„ç¼–è¾‘åº”ç”¨ç¨‹åºã€‚</li><li>D-Edit æ˜¯ç¬¬ä¸€ä¸ªï¼ˆ1ï¼‰é€šè¿‡æ©ç ç¼–è¾‘å®ç°é¡¹ç›®ç¼–è¾‘ï¼Œï¼ˆ2ï¼‰ç»“åˆå›¾åƒå’ŒåŸºäºæ–‡æœ¬çš„ç¼–è¾‘çš„æ¡†æ¶ã€‚</li><li>é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œå±•ç¤ºäº†å„ç§å›¾åƒç¼–è¾‘ç»“æœçš„è´¨é‡å’Œå¤šåŠŸèƒ½æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šAn Item is Worth a Promptï¼šå¤šåŠŸèƒ½çš„å¯æ§å›¾åƒç¼–è¾‘</li><li>ä½œè€…ï¼šAosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè€¶é²å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒç¼–è¾‘ã€æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€å¯æ§æç¤º</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.04880</li><li><p>æ‘˜è¦ï¼š(1)ï¼šåŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆä¸­çš„æˆåŠŸï¼Œå›¾åƒç¼–è¾‘æˆä¸ºä¸€ç§é‡è¦çš„åº”ç”¨ç¨‹åºï¼Œå®ƒè®©äººä»¬èƒ½å¤Ÿä¸ AI ç”Ÿæˆçš„å†…å®¹è¿›è¡Œäº¤äº’ã€‚åœ¨å„ç§ç¼–è¾‘æ–¹æ³•ä¸­ï¼Œæç¤ºç©ºé—´ç¼–è¾‘å› å…¶æ§åˆ¶è¯­ä¹‰çš„èƒ½åŠ›å’Œç®€å•æ€§è€Œå—åˆ°æ›´å¤šå…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºæ‰©æ•£æ¨¡å‹é€šå¸¸åœ¨æè¿°æ€§æ–‡æœ¬æ ‡é¢˜ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå› æ­¤åœ¨æ–‡æœ¬æç¤ºä¸­ç›´æ¥ç¼–è¾‘å•è¯é€šå¸¸ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„ç”Ÿæˆå›¾åƒï¼Œè¿åäº†å›¾åƒç¼–è¾‘çš„è¦æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œç°æœ‰çš„ç¼–è¾‘æ–¹æ³•é€šå¸¸è€ƒè™‘å¼•å…¥ç©ºé—´æ©ç æ¥ä¿ç•™æœªç¼–è¾‘åŒºåŸŸçš„èº«ä»½ï¼Œè€Œæ‰©æ•£æ¨¡å‹é€šå¸¸ä¼šå¿½ç•¥è¿™äº›åŒºåŸŸï¼Œå› æ­¤å¯¼è‡´ä¸åè°ƒçš„ç¼–è¾‘ç»“æœã€‚(2)ï¼šé’ˆå¯¹è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºå°†ç»¼åˆå›¾åƒæç¤ºäº¤äº’åˆ†è§£ä¸ºå‡ ä¸ªé¡¹ç›®æç¤ºäº¤äº’ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½é“¾æ¥åˆ°ä¸€ä¸ªç‰¹æ®Šå­¦ä¹ çš„æç¤ºã€‚ç”±æ­¤äº§ç”Ÿçš„æ¡†æ¶åä¸º D-Editï¼Œå®ƒåŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº¤å‰æ³¨æ„å±‚è¿›è¡Œè§£è€¦ï¼Œå¹¶é‡‡ç”¨ä¸¤æ­¥ä¼˜åŒ–æ¥æ„å»ºé¡¹ç›®æç¤ºå…³è”ã€‚é€šè¿‡æ“ä½œç›¸åº”çš„æç¤ºï¼Œå¯ä»¥å°†å¤šåŠŸèƒ½å›¾åƒç¼–è¾‘åº”ç”¨äºç‰¹å®šé¡¹ç›®ã€‚æœ¬æ–‡å±•ç¤ºäº†å››ç§ç±»å‹çš„ç¼–è¾‘æ“ä½œï¼ˆåŒ…æ‹¬åŸºäºå›¾åƒã€åŸºäºæ–‡æœ¬ã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ï¼‰çš„æœ€æ–°ç»“æœï¼Œæ¶µç›–äº†å¤§å¤šæ•°ç±»å‹çš„ç¼–è¾‘åº”ç”¨ç¨‹åºï¼Œæ‰€æœ‰è¿™äº›éƒ½é‡‡ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒD-Edit æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥ (1) é€šè¿‡æ©ç ç¼–è¾‘å®ç°é¡¹ç›®ç¼–è¾‘ï¼Œä»¥åŠ (2) ç»“åˆå›¾åƒå’ŒåŸºäºæ–‡æœ¬çš„ç¼–è¾‘çš„æ¡†æ¶ã€‚é€šè¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼Œæœ¬æ–‡å±•ç¤ºäº†é’ˆå¯¹å„ç§å›¾åƒé›†åˆçš„ç¼–è¾‘ç»“æœçš„è´¨é‡å’Œå¤šåŠŸèƒ½æ€§ã€‚(3)ï¼šæœ¬æ–‡æå‡ºä¸¤ç§å…³é”®æŠ€æœ¯ï¼Œæ—¨åœ¨å¢å¼ºä¸Šè¿°æ ‡å‡†ï¼š(1) è§£è€¦æ§åˆ¶ï¼šä¸ºäº†ä¿ç•™åŸå§‹å›¾åƒçš„ä¿¡æ¯ï¼Œç›®æ ‡é¡¹ç›®çš„ç¼–è¾‘åº”å°½é‡ä¸å½±å“å‘¨å›´é¡¹ç›®ã€‚ä»æç¤ºåˆ°å›¾åƒçš„æ§åˆ¶è¿‡ç¨‹ä¹Ÿåº”è¯¥è§£è€¦ï¼Œç¡®ä¿ä¿®æ”¹é¡¹ç›®æç¤ºä¸ä¼šç ´åå…¶ä½™é¡¹ç›®çš„æ§åˆ¶æµã€‚æ³¨æ„åˆ°æ–‡æœ¬åˆ°å›¾åƒäº¤äº’å‘ç”Ÿåœ¨åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„å±‚ä¸­ï¼Œæœ¬æ–‡æå‡ºåˆ†ç»„äº¤å‰æ³¨æ„æ¥è§£è€¦æç¤ºåˆ°é¡¹ç›®çš„æ§åˆ¶æµã€‚(2) å”¯ä¸€é¡¹ç›®æç¤ºï¼šä¸ºäº†æé«˜ä¸æŒ‡å¯¼çš„ä¸€è‡´æ€§ï¼ˆä¾‹å¦‚å‚è€ƒå›¾åƒï¼‰ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½åº”è¯¥ä¸ä¸€ä¸ªæ§åˆ¶å…¶ç”Ÿæˆçš„å”¯ä¸€æç¤ºç›¸å…³è”ã€‚è¿™äº›æç¤ºé€šå¸¸ç”±ç‰¹æ®Šæ ‡è®°æˆ–ç½•è§å•è¯ç»„æˆã€‚åƒ Dreambooth å’Œ Textual Inversion è¿™æ ·çš„å›¾åƒä¸ªæ€§åŒ–ç°æœ‰å·¥ä½œå·²ç»é€šè¿‡ç”¨å”¯ä¸€æç¤ºè¡¨ç¤ºæ–°ä¸»é¢˜æ¥å¹¿æ³›ç ”ç©¶äº†è¿™ä¸ªæ¦‚å¿µï¼Œéšåå°†å…¶ç”¨äºå›¾åƒç”Ÿæˆã€‚ä¸å®ƒä»¬ç›¸æ¯”ï¼Œæœ¬æ–‡ä½¿ç”¨ç‹¬ç«‹æç¤ºæ¥å®šä¹‰ä¸åŒçš„é¡¹ç›®ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå›¾åƒã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœå›¾åƒä¸­çš„æ¯ä¸ªé¡¹ç›®åŠå…¶æ‰€æœ‰ç»†èŠ‚éƒ½å¯ä»¥ç”¨ä¸€ä¸ªç‹¬ç‰¹çš„è‹±æ–‡å•è¯å‡†ç¡®æè¿°ï¼Œé‚£ä¹ˆç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•åœ°å°†å½“å‰å•è¯æ›´æ”¹ä¸ºç›®æ ‡å•è¯æ¥å®ç°æ‰€æœ‰ç±»å‹çš„ç¼–è¾‘ã€‚(4)ï¼šæœ¬æ–‡å……åˆ†åˆ©ç”¨æç¤ºå”¯ä¸€æ€§å’Œè§£è€¦æ§åˆ¶çš„æ½œåŠ›ï¼Œä»‹ç»äº†ä¸€ä¸ªå¤šåŠŸèƒ½å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œç§°ä¸º Disentangled-Edit (D-Edit)ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ”¯æŒåœ¨é¡¹ç›®çº§åˆ«è¿›è¡Œå¤§å¤šæ•°ç±»å‹çš„å›¾åƒç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬ã€åŸºäºå›¾åƒã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚å›¾ 1 æ‰€ç¤ºï¼Œä»ç›®æ ‡å›¾åƒå¼€å§‹ï¼Œæœ¬æ–‡æœ€åˆå°†å…¶ç»†åˆ†ä¸ºå¤šä¸ªå¯ç¼–è¾‘é¡¹ç›®ï¼ˆåœ¨ä»¥ä¸‹å†…å®¹ä¸­ï¼Œæœ¬æ–‡è¿˜å°†èƒŒæ™¯å’Œæœªåˆ†å‰²åŒºåŸŸç§°ä¸ºé¡¹ç›®ï¼‰ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½ä¸ä¸€ä¸ªåŒ…å«å‡ ä¸ªæ–°æ ‡è®°çš„æç¤ºç›¸å…³è”ã€‚æç¤ºå’Œé¡¹ç›®ä¹‹é—´çš„å…³è”æ˜¯é€šè¿‡ä¸¤æ­¥å¾®è°ƒè¿‡ç¨‹å»ºç«‹çš„ï¼Œå…¶ä¸­åŒ…æ‹¬ä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨åµŒå…¥çŸ©é˜µå’Œ UNet æ¨¡å‹æƒé‡ã€‚å¼•å…¥åˆ†ç»„äº¤å‰æ³¨æ„æ¥è§£è€¦æç¤ºåˆ°é¡¹ç›®çš„äº¤äº’ï¼Œé€šè¿‡éš”ç¦»æ³¨æ„è®¡ç®—å’Œå€¼æ›´æ–°ã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡æ›´æ”¹æç¤ºã€é¡¹ç›®åŠå…¶ä¹‹é—´çš„å…³è”æ¥å®ç°å„ç§ç±»å‹çš„å›¾åƒç¼–è¾‘ã€‚ç„¶åï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ›´æ”¹ç›¸åº”çš„æç¤ºã€æ©ç å’Œé¡¹ç›®ï¼Œå¹¶è°ƒæ•´å®ƒä»¬ä¹‹é—´çš„å…³è”æ¥å®ç°å„ç§ç±»å‹çš„å›¾åƒç¼–è¾‘ã€‚è¿™ç§çµæ´»æ€§å…è®¸å¹¿æ³›çš„åˆ›é€ å¯èƒ½æ€§å’Œå¯¹ç¼–è¾‘è¿‡ç¨‹çš„ç²¾ç¡®æ§åˆ¶ã€‚æœ¬æ–‡åœ¨å››ä¸ªå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šå±•ç¤ºäº†æœ¬æ–‡æ¡†æ¶çš„å¤šåŠŸèƒ½æ€§å’Œæ€§èƒ½ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œä½¿ç”¨ç¨³å®šæ‰©æ•£å’Œç¨³å®šæ‰©æ•£ XLã€‚æœ¬æ–‡æ€»ç»“æœ¬æ–‡çš„è´¡çŒ®å¦‚ä¸‹ï¼šâ€¢ æœ¬æ–‡æå‡ºå»ºç«‹é¡¹ç›®æç¤ºå…³è”ä»¥å®ç°é¡¹ç›®ç¼–è¾‘ã€‚â€¢ æœ¬æ–‡å¼•å…¥åˆ†ç»„äº¤å‰æ³¨æ„æ¥è§£è€¦æ‰©æ•£æ¨¡å‹ä¸­çš„æ§åˆ¶æµã€‚â€¢ æœ¬æ–‡æå‡º D-Edit ä½œä¸ºä¸€ç§å¤šåŠŸèƒ½æ¡†æ¶ï¼Œæ”¯æŒåœ¨é¡¹ç›®çº§åˆ«è¿›è¡Œå„ç§å›¾åƒç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬ã€åŸºäºå›¾åƒã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ã€‚D-Edit æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥è¿›è¡ŒåŸºäºæ©ç çš„ç¼–è¾‘ä»¥åŠåŒæ—¶æ‰§è¡ŒåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ç¼–è¾‘çš„æ¡†æ¶ã€‚</p></li><li><p>Methodsï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºå»ºç«‹é¡¹ç›®æç¤ºå…³è”ä»¥å®ç°é¡¹ç›®ç¼–è¾‘ï¼›ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡å¼•å…¥åˆ†ç»„äº¤å‰æ³¨æ„æ¥è§£è€¦æ‰©æ•£æ¨¡å‹ä¸­çš„æ§åˆ¶æµï¼›ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡º D-Edit ä½œä¸ºä¸€ç§å¤šåŠŸèƒ½æ¡†æ¶ï¼Œæ”¯æŒåœ¨é¡¹ç›®çº§åˆ«è¿›è¡Œå„ç§å›¾åƒç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬ã€åŸºäºå›¾åƒã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ã€‚D-Edit æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥è¿›è¡ŒåŸºäºæ©ç çš„ç¼–è¾‘ä»¥åŠåŒæ—¶æ‰§è¡ŒåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ç¼–è¾‘çš„æ¡†æ¶ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º D-Editï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„å¤šåŠŸèƒ½å›¾åƒç¼–è¾‘æ¡†æ¶ã€‚D-Edit å°†ç»™å®šå›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªé¡¹ç›®ï¼Œæ¯ä¸ªé¡¹ç›®éƒ½è¢«åˆ†é…ä¸€ä¸ªæç¤ºæ¥æ§åˆ¶å…¶åœ¨æç¤ºç©ºé—´ä¸­çš„è¡¨ç¤ºã€‚å›¾åƒæç¤ºäº¤å‰æ³¨æ„åŠ›è¢«åˆ†è§£ä¸ºä¸€ç»„é¡¹ç›®æç¤ºäº¤äº’ã€‚æ¯ä¸ªæç¤ºé€šè¿‡å­¤ç«‹çš„äº¤å‰æ³¨æ„åŠ›è¢«çº¦æŸä¸ºä»…ä¸å®ƒæ§åˆ¶çš„é¡¹ç›®è¿›è¡Œäº¤äº’ï¼Œä»è€Œè§£è€¦äº†äº¤å‰æ³¨æ„åŠ›æ§åˆ¶ç®¡é“ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºå»ºç«‹é¡¹ç›®æç¤ºå…³è”ä»¥å®ç°é¡¹ç›®ç¼–è¾‘ã€‚</li><li>å¼•å…¥åˆ†ç»„äº¤å‰æ³¨æ„åŠ›æ¥è§£è€¦æ‰©æ•£æ¨¡å‹ä¸­çš„æ§åˆ¶æµã€‚</li><li>æå‡º D-Edit ä½œä¸ºä¸€ç§å¤šåŠŸèƒ½æ¡†æ¶ï¼Œæ”¯æŒåœ¨é¡¹ç›®çº§åˆ«è¿›è¡Œå„ç§å›¾åƒç¼–è¾‘æ“ä½œï¼ŒåŒ…æ‹¬åŸºäºæ–‡æœ¬ã€åŸºäºå›¾åƒã€åŸºäºæ©ç çš„ç¼–è¾‘å’Œé¡¹ç›®ç§»é™¤ã€‚D-Edit æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥è¿›è¡ŒåŸºäºæ©ç çš„ç¼–è¾‘ä»¥åŠåŒæ—¶æ‰§è¡ŒåŸºäºæ–‡æœ¬å’Œå›¾åƒçš„ç¼–è¾‘çš„æ¡†æ¶ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å››ä¸ªå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šå±•ç¤ºäº†æœ¬æ–‡æ¡†æ¶çš„å¤šåŠŸèƒ½æ€§å’Œæ€§èƒ½ï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œä½¿ç”¨ç¨³å®šæ‰©æ•£å’Œç¨³å®šæ‰©æ•£ XLã€‚å·¥ä½œé‡ï¼š</li><li>æå‡ºäº†ä¸€ç§ä¸¤æ­¥å¾®è°ƒè¿‡ç¨‹æ¥å»ºç«‹æç¤ºå’Œé¡¹ç›®ä¹‹é—´çš„å…³è”ï¼ŒåŒ…æ‹¬ä¼˜åŒ–æ–‡æœ¬ç¼–ç å™¨åµŒå…¥çŸ©é˜µå’Œ UNet æ¨¡å‹æƒé‡ã€‚</li><li>å¼•å…¥åˆ†ç»„äº¤å‰æ³¨æ„æ¥è§£è€¦æç¤ºåˆ°é¡¹ç›®çš„äº¤äº’ï¼Œé€šè¿‡éš”ç¦»æ³¨æ„è®¡ç®—å’Œå€¼æ›´æ–°ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-197c83cdebd23bdb14b8fb0a7b729711.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1f65d83dbc51dc28ff510d4cc3b578f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-325295a9d8fc632369762af9b221cc1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a471c060f1ae7bcb9959f797a6fb643a.jpg" align="middle"></details><h2 id="Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation"><a href="#Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation" class="headerlink" title="Pix2Gif: Motion-Guided Diffusion for GIF Generation"></a>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h2><p><strong>Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang</strong></p><p>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model â€” it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href="https://hiteshk03.github.io/Pix2Gif/">https://hiteshk03.github.io/Pix2Gif/</a>. </p><p><a href="http://arxiv.org/abs/2403.04634v2">PDF</a> </p><p><strong>Summary</strong><br>å›¾åƒåˆ°GIFç”Ÿæˆçš„æ–°å¼è¿åŠ¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨æ–‡æœ¬å’Œè¿åŠ¨å¹…åº¦æç¤ºæŒ‡å¯¼çš„å›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œå¹¶æå‡ºæ–°çš„è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ä»¥ç©ºé—´è½¬æ¢ç‰¹å¾ï¼Œä»è€Œç¡®ä¿æ¨¡å‹éµå¾ªè¿åŠ¨æŒ‡å¯¼ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º Pix2Gifï¼Œä¸€ç§è¿åŠ¨å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå›¾åƒåˆ° GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆã€‚</li><li>ä»¥å›¾åƒç¿»è¯‘é—®é¢˜ä¸ºåŸºç¡€ï¼Œç”±æ–‡æœ¬å’Œè¿åŠ¨å¹…åº¦æç¤ºæŒ‡å¯¼ã€‚</li><li>è®¾è®¡æ–°çš„è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ï¼Œæ ¹æ®ä¸¤ç§æç¤ºå¯¹æºå›¾åƒç‰¹å¾è¿›è¡Œç©ºé—´è½¬æ¢ã€‚</li><li>å¼•å…¥æ„ŸçŸ¥æŸå¤±ï¼Œç¡®ä¿è½¬æ¢åçš„ç‰¹å¾å›¾ä¸ç›®æ ‡å›¾åƒç©ºé—´ä¸€è‡´ã€‚</li><li>ç²¾å¿ƒæ•´ç†æ•°æ®ï¼Œä» TGIF è§†é¢‘å­—å¹•æ•°æ®é›†ä¸­æå–è¿è´¯çš„å›¾åƒå¸§ã€‚</li><li>é‡‡ç”¨é›¶æ ·æœ¬æ–¹å¼å°†æ¨¡å‹åº”ç”¨äºå¤šä¸ªè§†é¢‘æ•°æ®é›†ã€‚</li><li>å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä¸ä»…èƒ½æ•æ‰æ–‡æœ¬çš„è¯­ä¹‰æç¤ºï¼Œè¿˜èƒ½æ•æ‰è¿åŠ¨å¼•å¯¼çš„ç©ºé—´æç¤ºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šPix2Gifï¼šåŸºäºè¿åŠ¨æŒ‡å¯¼çš„å›¾åƒè½¬ GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆ</li><li>ä½œè€…ï¼šHitesh K. Agrawalã€Yuke Zhuã€Jonathan T. Barronã€Phillip Isolaã€ Alexei A. Efros</li><li>éš¶å±å…³ç³»ï¼šä¼¯å…‹åˆ©åŠ å·å¤§å­¦</li><li>å…³é”®è¯ï¼šå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€è¿åŠ¨å¼•å¯¼ã€æ‰©æ•£æ¨¡å‹ã€å›¾åƒç¼–è¾‘</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.04208.pdfï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå›¾åƒåˆ° GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒéœ€è¦æ¨¡å‹åŒæ—¶ç†è§£æ–‡æœ¬å’Œè¿åŠ¨æç¤ºï¼Œå¹¶ç”Ÿæˆä¸æç¤ºç›¸ä¸€è‡´ä¸”å†…å®¹è¿è´¯çš„è§†é¢‘ã€‚ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆï¼Œä½†å®ƒä»¬åœ¨å¤„ç†è¿åŠ¨ä¿¡æ¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚ç›´æ¥å°†è¿åŠ¨è¾“å…¥ä½œä¸ºæ–‡æœ¬æç¤ºå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å¯¹å•ä¸ªæç¤ºè¯ç»™äºˆè¿‡å¤šçš„å…³æ³¨ï¼Œä»è€Œå¿½ç•¥å…¶ä»–é‡è¦çš„è¿åŠ¨ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨å¼•å¯¼æ‰©æ•£æ¨¡å‹ Pix2Gifï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼•å…¥ä¸€ä¸ªè¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ¨¡å—å°†è¿åŠ¨ä¿¡æ¯åµŒå…¥åˆ°å›¾åƒç‰¹å¾ä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶éµå¾ªæŒ‡å®šçš„è¿åŠ¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ä¸ªæ„ŸçŸ¥æŸå¤±ï¼Œä»¥ç¡®ä¿å˜å½¢åçš„ç‰¹å¾å›¾ä¸ç›®æ ‡å›¾åƒä¿æŒåœ¨åŒä¸€è¯­ä¹‰ç©ºé—´å†…ï¼Œä»è€Œä¿è¯å†…å®¹çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ TGIF è§†é¢‘å­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒPix2Gif æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æ–‡æœ¬å’Œè¿åŠ¨æç¤ºä¸­çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒåˆ° GIFï¼ˆè§†é¢‘ï¼‰ç»“æœã€‚å®éªŒç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ol><p>7.Methodsï¼š(1): Pix2Gifæ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶ï¼Œå°†è¿åŠ¨ä¿¡æ¯åµŒå…¥å›¾åƒç‰¹å¾ä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹éµå¾ªæŒ‡å®šçš„è¿åŠ¨è½¨è¿¹ã€‚(2): Pix2Gifæ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªæ„ŸçŸ¥æŸå¤±ï¼Œä»¥ç¡®ä¿å˜å½¢åçš„ç‰¹å¾å›¾ä¸ç›®æ ‡å›¾åƒä¿æŒåœ¨åŒä¸€è¯­ä¹‰ç©ºé—´å†…ï¼Œä»è€Œä¿è¯å†…å®¹çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚(3): Pix2Gifæ¨¡å‹åœ¨TGIFè§†é¢‘å­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰æ–‡æœ¬å’Œè¿åŠ¨æç¤ºä¸­çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒåˆ°GIFï¼ˆè§†é¢‘ï¼‰ç»“æœã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šPix2Gifæ¨¡å‹åœ¨å›¾åƒåˆ°GIFï¼ˆè§†é¢‘ï¼‰ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°æ€§çš„è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ï¼Œæœ‰æ•ˆåœ°å°†æ–‡æœ¬å’Œè¿åŠ¨ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œç”Ÿæˆå†…å®¹è¿è´¯ã€æ—¶é—´ä¸€è‡´çš„é«˜è´¨é‡ç»“æœã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è¿åŠ¨å¼•å¯¼å˜å½¢æ¨¡å—ï¼Œå°†è¿åŠ¨ä¿¡æ¯åµŒå…¥å›¾åƒç‰¹å¾ä¸­ï¼ŒæŒ‡å¯¼æ¨¡å‹éµå¾ªæŒ‡å®šçš„è¿åŠ¨è½¨è¿¹ï¼Œä¿è¯äº†ç”Ÿæˆçš„å›¾åƒåºåˆ—åœ¨æ—¶é—´ä¸Šçš„è¿è´¯æ€§ã€‚</li><li>å¼•å…¥äº†æ„ŸçŸ¥æŸå¤±ï¼Œç¡®ä¿å˜å½¢åçš„ç‰¹å¾å›¾ä¸ç›®æ ‡å›¾åƒä¿æŒåœ¨åŒä¸€è¯­ä¹‰ç©ºé—´å†…ï¼Œä¿è¯äº†å†…å®¹çš„ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚æ€§èƒ½ï¼š</li><li>åœ¨TGIFè§†é¢‘å­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒPix2Gifæ¨¡å‹åœ¨æ•æ‰æ–‡æœ¬å’Œè¿åŠ¨æç¤ºä¸­çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆçš„å›¾åƒåˆ°GIFï¼ˆè§†é¢‘ï¼‰ç»“æœè´¨é‡è¾ƒé«˜ã€‚</li><li>ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒPix2Gifæ¨¡å‹åœ¨ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„GIFæ–¹é¢è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚å·¥ä½œé‡ï¼š</li><li>Pix2Gifæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å°ºå¯¸å›¾åƒå’Œé•¿è§†é¢‘åºåˆ—ã€‚</li><li>æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´ä¹Ÿå—åˆ°å›¾åƒåˆ†è¾¨ç‡å’Œè§†é¢‘é•¿åº¦çš„å½±å“ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-87f209086271d79f66fc2b71db813a89.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-11  VideoElevator Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Blendshapeå­¦ä¹ ç¬”è®°</title>
    <link href="https://kedreamix.github.io/2024/03/11/Note/BlendShape/"/>
    <id>https://kedreamix.github.io/2024/03/11/Note/BlendShape/</id>
    <published>2024-03-11T11:42:00.000Z</published>
    <updated>2024-03-11T12:01:31.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Blendshape-Morph-TargetåŠ¨ç”»"><a href="#Blendshape-Morph-TargetåŠ¨ç”»" class="headerlink" title="Blendshape(Morph TargetåŠ¨ç”»)"></a>Blendshape(Morph TargetåŠ¨ç”»)</h2><p>Blendshapesæ³›æŒ‡3Då®šç‚¹åŠ¨ç”»çš„åˆ¶ä½œæ–¹å¼ (Mayaé‡Œé¢ç§°ä¹‹ä¸º blend shapes ï¼Œè€Œ3DS Maxé‡Œç§°ä¹‹ä¸ºmorph targets) ï¼Œåœ¨3DåŠ¨ç”»ä¸­ç”¨çš„æ¯”è¾ƒå¤šï¼Œå°¤å…¶æ˜¯äººè„¸åŠ¨ç”»çš„åˆ¶ä½œï¼Œé€šè¿‡blendshapeæ¥é©±åŠ¨è§’è‰²çš„é¢éƒ¨è¡¨æƒ…ã€‚</p><p>ç”¨åœ¨è„¸éƒ¨åŠ¨ç”»åˆ¶ä½œæ—¶ï¼Œblendshapeå¯ä»¥è¢«ç§°ä¹‹ä¸º<strong>è„¸éƒ¨ç‰¹å¾ï¼Œè¡¨æƒ…åŸºå‡†ï¼Œå®šä½ç¬¦</strong>ç­‰ç­‰ã€‚è¿™é‡Œè¦å¼•å…¥ä¸€ä¸ª<code>FACS</code>çš„æ¦‚å¿µï¼Œå¯ä»¥ç®€å•ç†è§£ä¸ºå°†è„¸éƒ¨è¿›è¡Œåˆç†åŒ–çš„åˆ†åŒºæ ‡å‡†ã€‚</p><blockquote><p>â€œè¡¨æƒ…è¿™ä¸ªä¸œè¥¿çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªæ— é™å¤šå¯èƒ½çš„ä¸œè¥¿ï¼Œæ€ä¹ˆèƒ½å¤Ÿè®¡ç®—expressionå‘¢ï¼Ÿ</p><p>è¿™å°±å¸¦æ¥äº†Blendshapesâ€”â€”ä¸€ç»„ç»„æˆæ•´ä½“è¡¨æƒ…çš„åŸºå‡†ï¼ˆæ•°é‡å¯ä»¥æœ‰åå‡ ä¸ªã€50ä¸ªã€100+ã€ 200+ï¼Œè¶Šå¤šå°±è¶Šç»†è…»)ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸€ç»„åŸºå‡†é€šè¿‡çº¿æ€§ç»„åˆæ¥è®¡ç®—å‡ºæ•´ä½“çš„expressionï¼Œç”¨å…¬å¼æ¥è¯´å°±æ˜¯  ï¼Œå…¶ä¸­eæ˜¯expressionï¼ŒBæ˜¯ä¸€ç»„è¡¨æƒ…åŸºå‡†ï¼Œdæ˜¯å¯¹åº”çš„ç³»æ•°ï¼ˆåœ¨è¿™ä¸€ç»„é‡Œé¢çš„æƒé‡ï¼‰ï¼Œbæ˜¯neutralã€‚â€ </p><p>â€” From <a href="https://zhuanlan.zhihu.com/p/78174706">https://zhuanlan.zhihu.com/p/78174706</a></p></blockquote><h2 id="BlendShapeç³»æ•°ä»‹ç»"><a href="#BlendShapeç³»æ•°ä»‹ç»" class="headerlink" title="BlendShapeç³»æ•°ä»‹ç»"></a>BlendShapeç³»æ•°ä»‹ç»</h2><p>åœ¨ARKitä¸­ï¼Œå¯¹è¡¨æƒ…ç‰¹å¾ä½ç½®å®šä¹‰äº†52ç»„è¿åŠ¨blendshapeç³»æ•°(<br><a href="https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation">https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation</a> )ï¼Œæ¯ä¸ªblendshapeç³»æ•°ä»£è¡¨ä¸€ç§è¡¨æƒ…å®šä½ç¬¦ï¼Œè¡¨æƒ…å®šä½ç¬¦å®šä¹‰äº†ç‰¹å®šè¡¨æƒ…å±æ€§ï¼Œå¦‚mouthSmileLeftã€mouthSmileRightç­‰ï¼Œä¸å…¶å¯¹åº”çš„blendshapeç³»æ•°åˆ™è¡¨ç¤ºè¡¨æƒ…è¿åŠ¨èŒƒå›´ã€‚è¿™52ç»„blendshapeç³»æ•°æå…¶æè¿°å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/984d8d76878441c3a8402f788ef6e46f~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=qI8vU39X63te%2BVNdO78uBFphwK0%3D" alt="Blendshape"></p><p>æ¯ä¸€ä¸ªblendshapeç³»æ•°çš„å–å€¼èŒƒå›´ä¸º0ï½1çš„æµ®ç‚¹æ•°ã€‚ä»¥jawOpenä¸ºä¾‹ï¼Œå½“è®¤ä¸ºç”¨æˆ·çš„å˜´å·´å®Œå…¨é—­ç´§æ—¶ï¼Œè¿”å›çš„jawOpenç³»æ•°ä¸º0ã€‚å½“è®¤ä¸ºç”¨æˆ·çš„å˜´å·´å¼ å¼€è‡³æœ€å¤§æ—¶ï¼Œè¿”å›çš„jawOpenç³»æ•°ä¸º1ã€‚</p><p><img src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt=""></p><p>åœ¨ç”¨æˆ·å®Œå…¨é—­å˜´ä¸å˜´å¼ åˆ°æœ€å¤§ä¹‹é—´çš„è¿‡æ¸¡çŠ¶æ€ï¼ŒjawOpenä¼šæ ¹æ®ç”¨æˆ·å˜´å¼ å¤§çš„å¹…åº¦è¿”å›ä¸€ä¸ª0ï½1çš„æ’å€¼ã€‚</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/8e8d980b8d69461fb5d2efbc50e47d47~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=sFNMeBoNY3ZFfiO%2BRSjR8uGECIw%3D" alt=""></p><h2 id="è„¸éƒ¨åŠ¨æ•çš„ä½¿ç”¨"><a href="#è„¸éƒ¨åŠ¨æ•çš„ä½¿ç”¨" class="headerlink" title="è„¸éƒ¨åŠ¨æ•çš„ä½¿ç”¨"></a>è„¸éƒ¨åŠ¨æ•çš„ä½¿ç”¨</h2><h3 id="ARKit-è„¸éƒ¨ä¸Viveè„¸éƒ¨blendshapeåŸºå‡†å¯¹æ¯”"><a href="#ARKit-è„¸éƒ¨ä¸Viveè„¸éƒ¨blendshapeåŸºå‡†å¯¹æ¯”" class="headerlink" title="ARKit è„¸éƒ¨ä¸Viveè„¸éƒ¨blendshapeåŸºå‡†å¯¹æ¯”"></a>ARKit è„¸éƒ¨ä¸Viveè„¸éƒ¨blendshapeåŸºå‡†å¯¹æ¯”</h3><div class="table-container"><table><thead><tr><th></th><th>ARKitï¼ˆ52ï¼‰</th><th>Extra</th><th>VIVEï¼ˆ52ï¼‰</th><th>Extra</th></tr></thead><tbody><tr><td>Brow</td><td>5</td><td></td><td>0</td><td></td></tr><tr><td>Eye</td><td>13</td><td></td><td>14</td><td>Eye Frown + 1</td></tr><tr><td>Cheek</td><td>3</td><td></td><td>3</td><td></td></tr><tr><td>Nose</td><td>2</td><td></td><td>0</td><td></td></tr><tr><td>Jaw</td><td>4</td><td></td><td>4</td><td></td></tr><tr><td>Mouth</td><td>24</td><td></td><td>20</td><td>O shape - 1</td></tr><tr><td>Tongue</td><td>1</td><td>Tongue + 7</td><td>11</td><td></td></tr><tr><td>Sum</td><td>52</td><td>59</td><td>52</td><td>52</td></tr></tbody></table></div><h3 id="ARKitçš„52ä¸ªBlendshapeè¡¨æƒ…åŸºå‡†ç»„"><a href="#ARKitçš„52ä¸ªBlendshapeè¡¨æƒ…åŸºå‡†ç»„" class="headerlink" title="ARKitçš„52ä¸ªBlendshapeè¡¨æƒ…åŸºå‡†ç»„"></a>ARKitçš„52ä¸ªBlendshapeè¡¨æƒ…åŸºå‡†ç»„</h3><p>å¯ä»¥çœ‹ARKit Face Blendshapesçš„ç…§ç‰‡å’Œ3Dæ¨¡å‹ç¤ºä¾‹ï¼š<a href="https://arkit-face-blendshapes.com/">https://arkit-face-blendshapes.com/</a></p><div class="table-container"><table><thead><tr><th>CC3</th><th>ARKit Name è¡¨æƒ…åŸºå‡†/å®šä½ç¬¦</th><th>ARKit Picture</th><th>CC3 Picture</th></tr></thead><tbody><tr><td>A01</td><td>browInnerUp</td><td><img src="https://static.wixstatic.com/media/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png" alt=""></td></tr><tr><td>A02</td><td>browDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png" alt=""></td></tr><tr><td>A03</td><td>browDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png" alt=""></td></tr><tr><td>A04</td><td>browOuterUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png" alt=""></td></tr><tr><td>A05</td><td>browOuterUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png" alt=""></td></tr><tr><td>A06</td><td>eyeLookUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png" alt=""></td></tr><tr><td>A07</td><td>eyeLookUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png" alt=""></td></tr><tr><td>A08</td><td>eyeLookDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png" alt=""></td></tr><tr><td>A09</td><td>eyeLookDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png" alt=""></td></tr><tr><td>A10</td><td>eyeLookOutLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png" alt=""></td></tr><tr><td>A11</td><td>eyeLookInLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_03368853adeb4b8599da5451033cd809~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_03368853adeb4b8599da5451033cd809~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png" alt=""></td></tr><tr><td>A12</td><td>eyeLookInRight</td><td><img src="https://static.wixstatic.com/media/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_803074453832444d8dec710711196559~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_803074453832444d8dec710711196559~mv2.png" alt=""></td></tr><tr><td>A13</td><td>eyeLookOutRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png" alt=""></td></tr><tr><td>A14</td><td>eyeBlinkLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png" alt=""></td></tr><tr><td>A15</td><td>eyeBlinkRight</td><td><img src="https://static.wixstatic.com/media/64c63b_65e50badaa854262a87329394a87484c~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65e50badaa854262a87329394a87484c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png" alt=""></td></tr><tr><td>A16</td><td>eyeSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png" alt=""></td></tr><tr><td>A17</td><td>eyeSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png" alt=""></td></tr><tr><td>A18</td><td>eyeWideLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png" alt=""></td></tr><tr><td>A19</td><td>eyeWideRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3157fc370d064da9926027034e8220d6~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3157fc370d064da9926027034e8220d6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png" alt=""></td></tr><tr><td>A20</td><td>cheekPuff</td><td><img src="https://static.wixstatic.com/media/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png/v1/fill/w_252,h_172,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_27548c426f1b47ae834c757417e03269~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_27548c426f1b47ae834c757417e03269~mv2.png" alt=""></td></tr><tr><td>A21</td><td>cheekSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png" alt=""></td></tr><tr><td>A22</td><td>cheekSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png" alt=""></td></tr><tr><td>A23</td><td>noseSneerLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png" alt=""></td></tr><tr><td>A24</td><td>noseSneerRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png" alt=""></td></tr><tr><td>A25</td><td>jawOpen</td><td><img src="https://static.wixstatic.com/media/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png/v1/fill/w_267,h_192,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png" alt=""></td></tr><tr><td>A26</td><td>jawForward</td><td><img src="https://static.wixstatic.com/media/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png" alt=""></td></tr><tr><td>A27</td><td>jawLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png" alt=""></td></tr><tr><td>A28</td><td>jawRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png" alt=""></td></tr><tr><td>A29</td><td>mouthFunnel</td><td><img src="https://static.wixstatic.com/media/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png" alt=""></td></tr><tr><td>A30</td><td>mouthPucker</td><td><img src="https://static.wixstatic.com/media/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png" alt=""></td></tr><tr><td>A31</td><td>mouthLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png" alt=""></td></tr><tr><td>A32</td><td>mouthRight</td><td><img src="https://static.wixstatic.com/media/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_332e51118068490cbb932bc8b3880895~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_332e51118068490cbb932bc8b3880895~mv2.png" alt=""></td></tr><tr><td>A33</td><td>mouthRollUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png" alt=""></td></tr><tr><td>A34</td><td>mouthRollLower</td><td><img src="https://static.wixstatic.com/media/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png" alt=""></td></tr><tr><td>A35</td><td>mouthShrugUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png" alt=""></td></tr><tr><td>A36</td><td>mouthShrugLower</td><td><img src="https://static.wixstatic.com/media/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png" alt=""></td></tr><tr><td>A37</td><td>mouthClose</td><td><img src="https://static.wixstatic.com/media/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png/v1/fill/w_267,h_129,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8aded518da54400db938b69753b8539a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8aded518da54400db938b69753b8539a~mv2.png" alt=""></td></tr><tr><td>A38</td><td>mouthSmileLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png" alt=""></td></tr><tr><td>A39</td><td>mouthSmileRight</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png" alt=""></td></tr><tr><td>A40</td><td>mouthFrownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png" alt=""></td></tr><tr><td>A41</td><td>mouthFrownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png" alt=""></td></tr><tr><td>A42</td><td>mouthDimpleLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png" alt=""></td></tr><tr><td>A43</td><td>mouthDimpleRight</td><td><img src="https://static.wixstatic.com/media/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ea46553169c749f69dc8e47737434193~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ea46553169c749f69dc8e47737434193~mv2.png" alt=""></td></tr><tr><td>A44</td><td>mouthUpperUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png" alt=""></td></tr><tr><td>A45</td><td>mouthUpperUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png" alt=""></td></tr><tr><td>A46</td><td>mouthLowerDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png" alt=""></td></tr><tr><td>A47</td><td>mouthLowerDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png" alt=""></td></tr><tr><td>A48</td><td>mouthPressLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_478c881ace1744ff825202484b212c17~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_478c881ace1744ff825202484b212c17~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png" alt=""></td></tr><tr><td>A49</td><td>mouthPressRight</td><td><img src="https://static.wixstatic.com/media/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png" alt=""></td><td><br><img src="https://static.wixstatic.com/media/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png" alt=""></td></tr><tr><td>A50</td><td>mouthStretchLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cf77104a546149e88698feb420726493~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cf77104a546149e88698feb420726493~mv2.png" alt=""></td></tr><tr><td>A51</td><td>mouthStretchRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png" alt=""></td></tr><tr><td>A52</td><td>tongueOut</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>CC3 é¢å¤–çš„èˆŒå¤´Blendshape(with open month)ï¼š</li></ul><div class="table-container"><table><thead><tr><th>T01</th><th>Tongue_Up</th><th></th><th><img src="https://static.wixstatic.com/media/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png" alt=""></th></tr></thead><tbody><tr><td>T02</td><td>Tongue_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png" alt=""></td></tr><tr><td>T03</td><td>Tongue_Left</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_860b7c7043894521a754755c35816cb3~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_860b7c7043894521a754755c35816cb3~mv2.png" alt=""></td></tr><tr><td>T04</td><td>Tongue_Right</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png" alt=""></td></tr><tr><td>T05</td><td>Tongue_Roll</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png" alt=""></td></tr><tr><td>T06</td><td>Tongue_Tip_Up</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png" alt=""></td></tr><tr><td>T07</td><td>Tongue_Tip_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png" alt=""></td></tr></tbody></table></div><h3 id="Viveé¢éƒ¨çš„è¡¨æƒ…åŸºå‡†ç»„"><a href="#Viveé¢éƒ¨çš„è¡¨æƒ…åŸºå‡†ç»„" class="headerlink" title="Viveé¢éƒ¨çš„è¡¨æƒ…åŸºå‡†ç»„"></a>Viveé¢éƒ¨çš„è¡¨æƒ…åŸºå‡†ç»„</h3><p>Viveè¿™ä¸€å¥—è„¸éƒ¨è¿½è¸ªä¹Ÿæ˜¯52ä¸ªblendshapesï¼Œä½†æ˜¯å’Œè‹¹æœçš„åŸºå‡†æœ‰å¾ˆå¤§åŒºåˆ«ã€‚</p><ul><li>åŒºåˆ«ä¸€ï¼šèˆŒå¤´</li></ul><p>è‹¹æœå…¶å®æ˜¯52+7ï¼Œå› ä¸ºèˆŒå¤´åœ¨52ä¸ªé‡Œåªæœ‰ä¸€ä¸ªä¼¸èˆŒå¤´çš„blendshapeï¼Œä½†viveå…¶å®æ˜¯42 + 10ï¼Œæ•´ä½“æ¥è®²Viveè¡¨æƒ…è®°ä½èƒ½trackingåˆ°çš„è¡¨æƒ…ç»†èŠ‚è¿˜æ˜¯æ›´å°‘ä¸€äº›ã€‚</p><ul><li>åŒºåˆ«äºŒï¼šçœ‰æ¯›</li></ul><p>ARKitçš„52ä¸ªblendshapesï¼Œæ˜¯æ ¹æ®ç¡¬ä»¶åˆ†åŒºä¸€å¯¹ä¸€trackingçš„ï¼Œç„¶è€ŒViveçœ‰æ¯›ä¸åˆ†æ˜¯æ²¡æœ‰å•ç‹¬å¦è®¾blendshapesï¼Œè€Œæ˜¯ä¸çœ¼ç›çš„åŠ¨ä½œblendedåœ¨ä¸€èµ·ä½œä¸ºä¸€ä¸ªblendshapeçš„ï¼Œå¹¶ä¸æ˜¯ç²¾å‡†çš„ä¸€å¯¹ä¸€åˆ†åŒºtrackingã€‚</p><p>æˆ‘ä¸‹é¢ç¼–å·çš„æ’åºæ˜¯æŒ‰ç…§<a href="https://developer.vive.com/resources/vive-sense/sdk/vive-eye-and-facial-tracking-sdk/">VIVE Eye and Facial Tracking SDK</a> unity é‡Œinspectoré‡Œçš„é¡ºåºï¼Œæ–¹ä¾¿æˆ‘åŠ è¡¨æƒ…ã€‚</p><p>è¿™é‡Œæ˜¯æ•´ç†çš„ç”¨ARKitåˆ¶ä½œViveåŸºå‡†çš„å¯¹åº”ç¼–å·ï¼š</p><p><a href="https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing</a></p><ul><li>Eye Blendshapes ï¼ˆ14 = 12 + 2ï¼‰</li></ul><div class="table-container"><table><thead><tr><th>Viveç¼–å·</th><th>Viveè¡¨æƒ…åŸºå‡†</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V01</td><td>Eye_Left_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png/v1/fill/w_238,h_182,al_c,lg_1,q_85,enc_auto/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png" alt=""></td></tr><tr><td>V02</td><td>Eye_Left_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png/v1/fill/w_222,h_160,al_c,lg_1,q_85,enc_auto/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png" alt=""></td></tr><tr><td>V03</td><td>Eye_Left_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png/v1/fill/w_238,h_188,al_c,lg_1,q_85,enc_auto/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png" alt=""></td></tr><tr><td>V04</td><td>Eye_Left_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png/v1/fill/w_238,h_192,al_c,lg_1,q_85,enc_auto/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png" alt=""></td></tr><tr><td>V05</td><td>Eye_Left_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png/v1/fill/w_238,h_203,al_c,lg_1,q_85,enc_auto/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png" alt=""></td></tr><tr><td>V06</td><td>Eye_Left_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png/v1/fill/w_238,h_195,al_c,lg_1,q_85,enc_auto/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png" alt=""></td></tr><tr><td>V07</td><td>Eye_Right_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png/v1/fill/w_235,h_195,al_c,lg_1,q_85,enc_auto/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png" alt=""></td><td></td></tr><tr><td>V08</td><td>Eye_Right_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png/v1/fill/w_223,h_160,al_c,lg_1,q_85,enc_auto/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png" alt=""></td><td></td></tr><tr><td>V09</td><td>Eye_Right_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png/v1/fill/w_234,h_197,al_c,lg_1,q_85,enc_auto/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png" alt=""></td><td></td></tr><tr><td>V10</td><td>Eye_Right_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png/v1/fill/w_238,h_190,al_c,lg_1,q_85,enc_auto/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png" alt=""></td><td></td></tr><tr><td>V11</td><td>Eye_Right_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png/v1/fill/w_231,h_196,al_c,lg_1,q_85,enc_auto/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png" alt=""></td><td></td></tr><tr><td>V12</td><td>Eye_Right_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png/v1/fill/w_238,h_176,al_c,lg_1,q_85,enc_auto/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png" alt=""></td><td></td></tr><tr><td>V13</td><td>Eye_Left_squeeze: The blendShape close eye tightly when Eye_Left_Blink  value is 100.</td><td><img src="https://static.wixstatic.com/media/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png/v1/fill/w_238,h_183,al_c,lg_1,q_85,enc_auto/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png" alt=""></td></tr><tr><td>V14</td><td>Eye_Right_squeeze</td><td><img src="https://static.wixstatic.com/media/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png/v1/fill/w_238,h_194,al_c,lg_1,q_85,enc_auto/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>Lip Blendshapes ï¼ˆ38 = 37 + 1ï¼‰</li></ul><div class="table-container"><table><thead><tr><th>Viveç¼–å·</th><th>Viveè¡¨æƒ…åŸºå‡†</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V15</td><td>Jaw_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png/v1/fill/w_235,h_190,al_c,lg_1,q_85,enc_auto/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png" alt=""></td></tr><tr><td>V16</td><td>Jaw_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png/v1/fill/w_245,h_202,al_c,lg_1,q_85,enc_auto/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png" alt=""></td></tr><tr><td>V17</td><td>Jaw_Forward</td><td><img src="https://static.wixstatic.com/media/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png/v1/fill/w_248,h_197,al_c,lg_1,q_85,enc_auto/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_20353f83579541428557c32d92545c9e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_20353f83579541428557c32d92545c9e~mv2.png" alt=""></td></tr><tr><td>V18</td><td>Jaw_Open</td><td><img src="https://static.wixstatic.com/media/64c63b_dc79f10003534839948d3261183d5082~mv2.png/v1/fill/w_244,h_188,al_c,lg_1,q_85,enc_auto/64c63b_dc79f10003534839948d3261183d5082~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png" alt=""></td></tr><tr><td>V19</td><td>Mouth_Ape_Shape</td><td><img src="https://static.wixstatic.com/media/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png/v1/fill/w_249,h_196,al_c,lg_1,q_85,enc_auto/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png" alt=""></td></tr><tr><td>V20</td><td>Mouth_Upper_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png/v1/fill/w_227,h_161,al_c,lg_1,q_85,enc_auto/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png" alt=""></td></tr><tr><td>V21</td><td>Mouth_Upper_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png/v1/fill/w_265,h_182,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png" alt=""></td></tr><tr><td>V22</td><td>Mouth_Lower_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png" alt=""></td></tr><tr><td>V23</td><td>Mouth_Lower_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png/v1/fill/w_265,h_225,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png" alt=""></td></tr><tr><td>V24</td><td>*Mouth_Upper_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png/v1/fill/w_265,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b8ae358e723f42e199338722f186e238~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b8ae358e723f42e199338722f186e238~mv2.png" alt=""></td></tr><tr><td>V25</td><td>*Mouth_Lower_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png/v1/fill/w_265,h_210,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png" alt=""></td></tr><tr><td>V26</td><td>Mouth_Pout</td><td><img src="https://static.wixstatic.com/media/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png" alt=""></td></tr><tr><td>V27</td><td>Mouth_Smile_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png" alt=""></td></tr><tr><td>V28</td><td>Mouth_Smile_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png" alt=""></td></tr><tr><td>V29</td><td>Mouth_Sad_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png" alt=""></td></tr><tr><td>V30</td><td>Mouth_Sad_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png" alt=""></td></tr><tr><td>V31</td><td>Cheek_Puff_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png" alt=""></td></tr><tr><td>V32</td><td>Cheek_Puff_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2998211eb141496d8651b786337b7846~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2998211eb141496d8651b786337b7846~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png" alt=""></td></tr><tr><td>V33</td><td>Cheek_Suck</td><td><img src="https://static.wixstatic.com/media/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png" alt=""></td></tr><tr><td>V34</td><td>Mouth_Upper_UpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png/v1/fill/w_265,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png" alt=""></td></tr><tr><td>V35</td><td>Mouth<em>Upper</em> UpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png" alt=""></td></tr><tr><td>V36</td><td>Mouth_Lower_DownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png/v1/fill/w_265,h_223,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png" alt=""></td></tr><tr><td>V37</td><td>Mouth_Lower_DownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png" alt=""></td></tr><tr><td>V38</td><td>Mouth_Upper_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png" alt=""></td></tr><tr><td>V39</td><td>Mouth_Lower_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png/v1/fill/w_269,h_211,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png" alt=""></td></tr><tr><td>V40</td><td>Mouth_Lower_Overlay</td><td><img src="https://static.wixstatic.com/media/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png/v1/fill/w_269,h_222,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png" alt=""></td></tr><tr><td>V41</td><td>Tongue_LongStep1</td><td><img src="https://static.wixstatic.com/media/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png/v1/fill/w_269,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png" alt=""></td></tr><tr><td>V42</td><td>Tongue_LongStep2</td><td><img src="https://static.wixstatic.com/media/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png/v1/fill/w_269,h_181,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e560524670843848266701061f24c63~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e560524670843848266701061f24c63~mv2.png" alt=""></td></tr><tr><td>V43</td><td>*Tongue_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png/v1/fill/w_269,h_199,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png" alt=""></td></tr><tr><td>V44</td><td>*Tongue_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png/v1/fill/w_269,h_197,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png" alt=""></td></tr><tr><td>V45</td><td>*Tongue_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png/v1/fill/w_269,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png" alt=""></td></tr><tr><td>V46</td><td>*Tongue_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png" alt=""></td></tr><tr><td>V47</td><td>*Tongue_Roll</td><td><img src="https://static.wixstatic.com/media/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png/v1/fill/w_269,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png" alt=""></td></tr><tr><td>V48</td><td>*Tongue_UpLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png/v1/fill/w_269,h_221,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png" alt=""></td><td></td></tr><tr><td>V49</td><td>*Tongue_UpRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png/v1/fill/w_269,h_232,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png/v1/fill/w_269,h_215,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png" alt=""></td><td></td></tr><tr><td>V50</td><td>*Tongue_DownLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png/v1/fill/w_269,h_237,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png/v1/fill/w_269,h_231,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png" alt=""></td><td></td></tr><tr><td>V51</td><td>*Tongue_DownRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png/v1/fill/w_269,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png/v1/fill/w_269,h_212,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png" alt=""></td><td></td></tr><tr><td>V52</td><td>*O-shaped mouth</td><td><img src="https://static.wixstatic.com/media/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png/v1/fill/w_269,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png" alt=""></td></tr></tbody></table></div><h2 id="MediaPipeæå–BlendShape"><a href="#MediaPipeæå–BlendShape" class="headerlink" title="MediaPipeæå–BlendShape"></a>MediaPipeæå–BlendShape</h2><p>MediaPipe Face Landmarkerè§£å†³æ–¹æ¡ˆæœ€åˆäº5æœˆçš„Google I/O 2023å‘å¸ƒã€‚å®ƒå¯ä»¥æ£€æµ‹é¢éƒ¨landmarkå¹¶è¾“å‡ºblendshape scoreï¼Œä»¥æ¸²æŸ“ä¸ç”¨æˆ·åŒ¹é…çš„3Dé¢éƒ¨æ¨¡å‹ã€‚é€šè¿‡MediaPipe Face Landmarkerè§£å†³æ–¹æ¡ˆï¼ŒKDDIå’Œè°·æ­ŒæˆåŠŸåœ°ä¸ºè™šæ‹Ÿä¸»æ’­å¸¦æ¥äº†çœŸå®æ„Ÿã€‚</p><p><strong>æŠ€æœ¯å®ç°</strong></p><p>ä½¿ç”¨Mediapipeå¼ºå¤§è€Œé«˜æ•ˆçš„PythonåŒ…ï¼ŒKDDIå¼€å‘äººå‘˜èƒ½å¤Ÿæ£€æµ‹è¡¨æ¼”è€…çš„é¢éƒ¨ç‰¹å¾å¹¶å®æ—¶æå–52ä¸ªæ··åˆå½¢çŠ¶ã€‚</p><p>è¿˜å¯å‚è€ƒï¼š<a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mediapipe.tasks <span class="keyword">import</span> python <span class="keyword">as</span> mp_python</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MP_TASK_FILE = <span class="string">"face_landmarker_with_blendshapes.task"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FaceMeshDetector</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(MP_TASK_FILE, mode=<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f_buffer = f.read()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åˆ›å»ºé…ç½®é€‰é¡¹</span></span><br><span class="line">        base_options = mp_python.BaseOptions(model_asset_buffer=f_buffer)</span><br><span class="line">        options = mp_python.vision.FaceLandmarkerOptions(</span><br><span class="line">            base_options=base_options,</span><br><span class="line">            output_face_blendshapes=<span class="literal">True</span>,</span><br><span class="line">            output_facial_transformation_matrixes=<span class="literal">True</span>,</span><br><span class="line">            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,</span><br><span class="line">            num_faces=<span class="number">1</span>,</span><br><span class="line">            result_callback=self.mp_callback</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åˆ›å»ºæ¨¡å‹</span></span><br><span class="line">        self.model = mp_python.vision.FaceLandmarker.create_from_options(options)</span><br><span class="line">        self.landmarks = <span class="literal">None</span></span><br><span class="line">        self.blendshapes = <span class="literal">None</span></span><br><span class="line">        self.latest_time_ms = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mp_callback</span>(<span class="params">self, mp_result, output_image, timestamp_ms: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># å¤„ç†å›è°ƒç»“æœ</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mp_result.face_landmarks) &gt;= <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(mp_result.face_blendshapes) &gt;= <span class="number">1</span>:</span><br><span class="line">            self.landmarks = mp_result.face_landmarks[<span class="number">0</span>]</span><br><span class="line">            self.blendshapes = [b.score <span class="keyword">for</span> b <span class="keyword">in</span> mp_result.face_blendshapes[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, frame</span>):</span><br><span class="line">        t_ms = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">if</span> t_ms &lt;= self.latest_time_ms:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        frame_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)</span><br><span class="line">        self.model.detect_async(frame_mp, t_ms)</span><br><span class="line">        self.latest_time_ms = t_ms</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_results</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.landmarks, self.blendshapes</span><br></pre></td></tr></tbody></table></figure><h2 id="å‚è€ƒ"><a href="#å‚è€ƒ" class="headerlink" title="å‚è€ƒ"></a>å‚è€ƒ</h2><ul><li><a href="https://www.mianzi-lizi.com/post/blendshapeå­¦ä¹ ç¬”è®°">https://www.mianzi-lizi.com/post/blendshapeå­¦ä¹ ç¬”è®°</a></li><li><a href="https://www.toutiao.com/article/6915330866285691395/">åˆ©ç”¨AnimojiæŠ€æœ¯è¯†åˆ«ç”¨æˆ·çš„è¡¨æƒ…</a></li><li><a href="https://news.nweon.com/110210">é€šè¿‡MediaPipeè§£å†³æ–¹æ¡ˆæ¥ä¸ºè™šæ‹Ÿä¸»æ’­å¸¦æ¥æ›´é€¼çœŸçœŸå®æ„Ÿ</a></li><li><a href="https://bbs.huaweicloud.com/blogs/374337">Unity &amp; FACEGOOD Audio2Face é€šè¿‡éŸ³é¢‘é©±åŠ¨é¢éƒ¨BlendShape</a></li><li><a href="https://www.cnblogs.com/jesse123/p/9014234.html">GenerativeAI Avatar solutions</a></li></ul>]]></content>
    
    
    <summary type="html">Blendshapesæ³›æŒ‡3Då®šç‚¹åŠ¨ç”»çš„åˆ¶ä½œæ–¹å¼ (Mayaé‡Œé¢ç§°ä¹‹ä¸º blend shapes ï¼Œè€Œ3DS Maxé‡Œç§°ä¹‹ä¸ºmorph targets) ï¼Œåœ¨3DåŠ¨ç”»ä¸­ç”¨çš„æ¯”è¾ƒå¤šï¼Œå°¤å…¶æ˜¯äººè„¸åŠ¨ç”»çš„åˆ¶ä½œï¼Œé€šè¿‡blendshapeæ¥é©±åŠ¨è§’è‰²çš„é¢éƒ¨è¡¨æƒ…ã€‚</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/</id>
    <published>2024-03-09T10:43:34.000Z</published>
    <updated>2024-03-09T10:43:34.779Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-09-æ›´æ–°"><a href="#2024-03-09-æ›´æ–°" class="headerlink" title="2024-03-09 æ›´æ–°"></a>2024-03-09 æ›´æ–°</h1><h2 id="DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis"><a href="#DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis" class="headerlink" title="DART: Implicit Doppler Tomography for Radar Novel View Synthesis"></a>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</h2><p><strong>Authors:Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe</strong></p><p>Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images. </p><p><a href="http://arxiv.org/abs/2403.03896v1">PDF</a> To appear in CVPR 2024; see <a href="https://wiselabcmu.github.io/dart/">https://wiselabcmu.github.io/dart/</a> for   our project site</p><p><strong>Summary</strong></p><p>åŸºäºé›·è¾¾ç‰¹å®šç‰©ç†ç‰¹æ€§ï¼Œä½¿ç”¨ç¥ç»è¾å°„åœºæ–¹æ³•åˆ›å»ºåå°„å’Œé€å°„æ¸²æŸ“ç®¡é“ï¼Œç”¨äºç”Ÿæˆå¤šæ™®å‹’èŒƒå›´é›·è¾¾å›¾åƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é€šè¿‡æ¨¡æ‹Ÿå™¨å¿«é€ŸåŸå‹åŒ–æˆåƒã€ç›®æ ‡æ£€æµ‹ã€åˆ†ç±»å’Œè·Ÿè¸ªç®—æ³•ã€‚</li><li>æ„å»ºçœŸå®çš„é›·è¾¾æ‰«ææ¨¡å‹é¢ä¸´åœºæ™¯ã€å°„é¢‘ææ–™ç‰¹æ€§å’Œé›·è¾¾åˆæˆå‡½æ•°çš„æŒ‘æˆ˜ã€‚</li><li>æå‡º DART æ–¹æ³•ï¼Œå—ç¥ç»è¾å°„åœºå¯å‘ï¼Œæ„å»ºåŸºäºåå°„ç‡å’Œé€å°„ç‡çš„æ¸²æŸ“ç®¡é“ã€‚</li><li>æ„å»ºå®šåˆ¶æ•°æ®æ”¶é›†å¹³å°ï¼Œæ”¶é›†åŒ…å«ä½ç½®å’Œå³æ—¶é€Ÿåº¦æµ‹é‡çš„æ–°å‹é›·è¾¾æ•°æ®é›†ã€‚</li><li>ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼ŒDART åˆæˆå‡ºæ‰€æœ‰æ•°æ®é›†æ–°è§†è§’ä¸‹çš„æ›´ä¼˜è´¨é›·è¾¾å¤šæ™®å‹’èŒƒå›´å›¾åƒã€‚</li><li>DART å¯ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å±‚æå›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼šé›·è¾¾éšå¼å¤šæ™®å‹’å±‚ææˆåƒç”¨äºæ–°å‹è§†è§’åˆæˆ</li><li>ä½œè€…ï¼šJiahui Yuã€Yiyi Liaoã€Yinda Zhangã€Wenqi Xianã€Lingxiao Liã€Junjie Guã€Xiaoyang Guoã€Shilin Zhuã€Shanshan Zhaoã€Biao Yangã€Lingbo Liu</li><li>éš¶å±ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li><li>å…³é”®è¯ï¼šé›·è¾¾ã€åˆæˆå­”å¾„é›·è¾¾ã€å¤šæ™®å‹’å±‚ææˆåƒã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li><li><p>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šé›·è¾¾ä»¿çœŸå¯¹äºå°„é¢‘ç³»ç»Ÿè®¾è®¡è‡³å…³é‡è¦ï¼Œä½†ä»¿çœŸé€¼çœŸçš„é›·è¾¾æ‰«æå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦åœºæ™¯ã€å°„é¢‘ææ–™å±æ€§å’Œé›·è¾¾åˆæˆå‡½æ•°çš„å‡†ç¡®æ¨¡å‹ã€‚(2) è¿‡å»æ–¹æ³•ï¼šä¼ ç»Ÿæ–¹æ³•éœ€è¦æ˜¾å¼æŒ‡å®šè¿™äº›æ¨¡å‹ï¼Œä½†å®ƒä»¬å¤æ‚ä¸”è€—æ—¶ã€‚(3) è®ºæ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šDARTï¼ˆå¤šæ™®å‹’è¾…åŠ©é›·è¾¾å±‚ææˆåƒï¼‰æ˜¯ä¸€ç§å—ç¥ç»è¾å°„åœºå¯å‘çš„é›·è¾¾ç‰¹å®šç‰©ç†æ–¹æ³•ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªåŸºäºåå°„ç‡å’Œé€å°„ç‡çš„æ¸²æŸ“ç®¡é“ï¼Œç”¨äºç”Ÿæˆè·ç¦»-å¤šæ™®å‹’å›¾åƒã€‚(4) æ–¹æ³•åœ¨ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼šDART åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä»æ–°è§†è§’åˆæˆäº†å‡ºè‰²çš„é›·è¾¾è·ç¦»-å¤šæ™®å‹’å›¾åƒï¼Œæ­¤å¤–è¿˜å¯ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å±‚æå›¾åƒã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†è®ºæ–‡çš„ç›®æ ‡ï¼Œå³æä¾›ä¸€ç§æ— éœ€æ˜¾å¼æ¨¡å‹å³å¯ç”Ÿæˆé€¼çœŸé›·è¾¾å›¾åƒçš„æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) æ•°æ®é©±åŠ¨æ–¹æ³•ä½¿ç”¨çœŸå®çš„ä¼ æ„Ÿå™¨æ‰«ææ¥æ„å»ºç¯å¢ƒæ¨¡å‹ã€‚ç¨€ç–æ–¹æ³•ä½¿ç”¨æ’å®šè¯¯æŠ¥ç‡æ£€æµ‹ (CFAR) æ¥æ£€æµ‹ç¯å¢ƒä¸­çš„ç¦»æ•£åå°„å™¨ [15, 49, 63]ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯†é›†æ–¹æ³•å°†ç¯å¢ƒåˆ’åˆ†ä¸ºæ˜¾å¼çš„ä½“ç´ ç½‘æ ¼ï¼Œå¹¶æ¨æ–­æ¯ä¸ªå•å…ƒçš„é›·è¾¾å±æ€§ã€‚å¯†é›†æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥ç»†åˆ†ä¸ºç›¸å¹²å’Œéç›¸å¹²èšåˆã€‚å¦‚æœå¯ä»¥ä½¿ç”¨å›ºå®šï¼ˆä¾‹å¦‚çº¿æ€§å’Œåœ†å½¢ï¼‰è½¨è¿¹æˆ–äºšæ³¢é•¿ç²¾åº¦çš„å§¿æ€ä¼°è®¡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨åˆæˆå­”å¾„é›·è¾¾ (SAR) [46, 50, 52, 56, 81, 82]ï¼›ç„¶è€Œï¼Œè¿™å¯¹äºå¤§é¢ç§¯ç§»åŠ¨å¹³å°æ¥è¯´æ˜¯ä¸åˆ‡å®é™…çš„ã€‚ç›¸åï¼Œä¼ æ„Ÿå™¨è¯»æ•°ï¼ˆé€šè¿‡å¤šä¸ªå¤©çº¿æˆ–è¾ƒå°è½¨è¿¹ç‰‡æ®µä¸Šçš„ SAR è·å¾—é«˜è§’åº¦åˆ†è¾¨ç‡ï¼‰ä¹Ÿå¯ä»¥ä»¥éç›¸å¹²æ–¹å¼èšåˆï¼Œè¿™è¢«ç§°ä¸ºå¤šè§†å›¾ 3D é‡å»º [33â€“35] å’Œé›·è¾¾æµ‹é‡æ³• [12]ã€‚(2) é›·è¾¾ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•è®¸å¤šç»å…¸çš„é›·è¾¾é—®é¢˜ï¼Œä¾‹å¦‚é›·è¾¾è¶…åˆ†è¾¨ç‡ [10, 17, 20, 21, 23, 53, 54, 72]ã€é‡Œç¨‹è®¡ [2, 43]ã€æµ‹ç»˜ [42]ã€æ´»åŠ¨è¯†åˆ« [39, 70, 77, 80] å’Œç‰©ä½“åˆ†ç±» [32, 69, 85] å·²åº”ç”¨äºä½¿ç”¨æœºå™¨å­¦ä¹ çš„æ›´ä¾¿å®œã€æ›´è½»ã€æ›´ç´§å‡‘çš„é›·è¾¾ç³»ç»Ÿã€‚æˆ‘ä»¬ç°åœ¨å¯»æ±‚ä»ç´§å‡‘ã€ä½åˆ†è¾¨ç‡é›·è¾¾ä¸­è§£å†³æ–°é¢–çš„è§†å›¾åˆæˆé—®é¢˜ï¼ŒåŒæ—¶éšå¼åˆ›å»ºæ›´é«˜åˆ†è¾¨ç‡çš„åœ°å›¾ã€‚(3) ç¥ç»è¾å°„åœºç¥ç»è¾å°„åœº [48] æ²¡æœ‰å®šä¹‰æ˜ç¡®çš„é€†æˆåƒç®—æ³•ä»ä¼ æ„Ÿå™¨è¯»æ•°ä¸­æ¢å¤åœºæ™¯çš„è¡¨ç¤ºï¼Œè€Œæ˜¯é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™éšå¼åœ°åè½¬å‰å‘æ¸²æŸ“å‡½æ•°ã€‚è¿™éœ€è¦ä»¥ä¸‹ç»„ä»¶ï¼š</p></li><li>ä¸–ç•Œæ¨¡å‹ï¼šNeRF å°†ä¸–ç•Œå®šä¹‰ä¸ºæ¯ä¸ªä½ç½®å’Œè§†è§’çš„ RGB é¢œè‰²å’Œé€æ˜åº¦ï¼›åç»­å·¥ä½œå·²å°†å…¶æ¨å¹¿åˆ°å¤„ç†æŠ—é”¯é½¿ [5]ã€ä¸åŒçš„ç›¸æœºå’Œç…§æ˜ [47, 73]ã€‚</li><li>ä¸–ç•Œè¡¨ç¤ºï¼šé™¤äº†ç¥ç»ç½‘ç»œ [48] æˆ–ä½“ç´ ç½‘æ ¼ [40] ä¹‹å¤–ï¼Œæœ€è¿‘çš„å·¥ä½œè¿˜æ¢ç´¢äº†ç©ºé—´å“ˆå¸Œè¡¨ [51] ä»¥åŠç”¨äºè§†åœºè§’ä¾èµ–æ€§çš„å‡½æ•°åˆ†è§£ [18, 83]ã€‚</li><li>æ¸²æŸ“å‡½æ•°å’Œæ¨¡å‹åæ¼”ï¼šNeRF å°†æ¯ä¸ªåƒç´ å»ºæ¨¡ä¸ºå°„çº¿å¹¶å¯¹è¾å°„åœºè¿›è¡Œå°„çº¿è¿½è¸ªã€‚æ­¤æ¸²æŸ“å‡½æ•°çš„å¯é€†æ€§è‡³å…³é‡è¦ï¼šé€šè¿‡å‡è®¾æ¯ä¸ªåƒç´ éƒ½æ˜¯ä¸€æ¡å°„çº¿ï¼ŒNeRF ç”±æ¯ä¸ªå°„çº¿ä¸Šçš„ä¸€ä¸ª RGB å›¾åƒåƒç´ â€œç›‘ç£â€ï¼Œå…è®¸ NeRF â€œæ±‚è§£â€æ²¿å°„çº¿çš„ä¸é€æ˜ç‚¹ã€‚æˆ‘ä»¬å¯¹ NeRF çš„è¿™äº›å…³é”®æ¨åŠ¨å› ç´ è¿›è¡Œäº†åˆ›æ–°ï¼Œä»¥ä¾¿å°†è¿™ç§æ–¹æ³•åº”ç”¨äºæ¯«ç±³æ³¢é›·è¾¾ã€‚é€šè¿‡å°† NeRF æŠ€æœ¯åº”ç”¨äºé›·è¾¾ï¼Œæˆ‘ä»¬å¸Œæœ›åˆ©ç”¨å¤§é‡ç¥ç»è¾å°„åœºæ–‡çŒ®ï¼ŒåŒæ—¶é‡Šæ”¾ç¥ç»éšå¼è¡¨ç¤ºçš„æ½œåŠ›ã€‚è¶…è¶Šè§†è§‰é¢†åŸŸ NeRF çš„æˆåŠŸæ¿€å‘äº†ä¼—å¤šå…¶ä»–åŠªåŠ›ï¼Œå°†ç›¸åŒçš„é€šç”¨åŸç†åº”ç”¨äºå…¶ä»–ä¼ æ„Ÿå™¨ï¼ŒåŒ…æ‹¬ç©ºé—´éŸ³é¢‘ [44]ã€æˆåƒå£°çº³ [55, 59]ã€æ¿€å…‰é›·è¾¾æ¨¡æ‹Ÿ [27] å’Œ RSSIï¼ˆæ¥æ”¶ä¿¡å·å¼ºåº¦æŒ‡ç¤ºå™¨ï¼‰æ˜ å°„ [84]ã€‚NeRF ä¹Ÿå·²åº”ç”¨äºé›·è¾¾ [29, 71]ï¼Œç”¨äºç±»ä¼¼ç›¸æœºçš„è¶…é«˜åˆ†è¾¨ç‡åˆæˆå­”å¾„é›·è¾¾ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ¢ç´¢çš„ç´§å‡‘ä¸”å»‰ä»·çš„é›·è¾¾ã€‚(4) DARTï¼šå¤šæ™®å‹’è¾…åŠ©é›·è¾¾å±‚ææˆåƒè™½ç„¶æˆ‘ä»¬çš„æ•´ä½“æ–¹æ³•å—ç¥ç»è¾å°„åœºçš„å¯å‘ï¼Œä½†é›·è¾¾çš„ç‰©ç†ç‰¹æ€§æå‡ºäº†å‡ ä¸ªæ–°çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åšå‡ºä»¥ä¸‹å…³é”®è®¾è®¡å†³ç­–ï¼ˆå›¾ 3ï¼‰ï¼š</li><li>æˆ‘ä»¬é¦–å…ˆé€‰æ‹©ä¸€ä¸ªé›·è¾¾æµ‹é‡è¡¨ç¤ºç©ºé—´â€”â€”è·ç¦»-å¤šæ™®å‹’â€”â€”è¯¥ç©ºé—´å…‹æœäº†ç´§å‡‘å‹é›·è¾¾çš„è¾ƒå·®ç©ºé—´åˆ†è¾¨ç‡ï¼ˆç¬¬ 3.1ã€3.2 èŠ‚ï¼‰ã€‚</li><li>ç„¶åæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªæ¨¡å‹æ¥è§£é‡Šç”µç£æ³¢ç›¸äº’ä½œç”¨çš„é›·è¾¾ç‰¹å®šæ•ˆåº”ï¼Œè¿™äº›æ•ˆåº”å¯¹äºé€¼çœŸçš„è§†å›¾åˆæˆè‡³å…³é‡è¦ï¼Œä¾‹å¦‚é•œé¢åå°„ã€é‡å½±å’Œéƒ¨åˆ†é®æŒ¡ï¼ˆç¬¬ 3.3 èŠ‚ï¼‰ã€‚</li><li><p>æœ€åï¼Œä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒå’Œå­¦ä¹ é›·è¾¾çš„ç¥ç»éšå¼åœ°å›¾ï¼Œæˆ‘ä»¬ä¸ºè‡ªé€‚åº”ç½‘æ ¼ä¸–ç•Œè¡¨ç¤ºé€‰æ‹©äº†ç½‘ç»œæ¶æ„ï¼Œè®¾è®¡äº†è·ç¦»-å¤šæ™®å‹’æ¸²æŸ“æ–¹æ³•ï¼Œå¹¶æå‡ºäº†å…³é”®æ¸²æŸ“ä¼˜åŒ–ï¼ˆç¬¬ 3.3-3.4 èŠ‚ï¼‰ã€‚(5) è·ç¦»-å¤šæ™®å‹’è¡¨ç¤ºä¸ç›¸æœºä¸åŒï¼Œé›·è¾¾æ˜¯ä¸»åŠ¨ä¼ æ„Ÿå™¨ï¼Œå®ƒé€šè¿‡å‘å°„å°„é¢‘æ³¢å½¢æ¥ç…§äº®åœºæ™¯ã€‚åœ¨å¤„ç†ä»åœºæ™¯ä¸­çš„ç‰©ä½“æ¥æ”¶åˆ°çš„åå°„åï¼Œé›·è¾¾å¯ä»¥ä»¥ 3D å½¢å¼æ„ŸçŸ¥ä¸–ç•Œâ€”â€”è·ç¦»ã€æ–¹ä½è§’å’Œä»°è§’â€”â€”ä½œä¸ºçƒ­å›¾ï¼ŒæŒ‡ç¤ºè¯¥ 3D åæ ‡å¤„ç‰©ä½“çš„åå°„ç‡ [60, 61]ã€‚ç„¶è€Œï¼Œè™½ç„¶ç¬¨é‡çš„æœºæ¢°é›·è¾¾æˆ–å¤§å‹å›ºæ€é›·è¾¾é˜µåˆ—å¯ä»¥æä¾›æ¥è¿‘å…¸å‹ç›¸æœºçš„æ–¹ä½è§’å’Œä»°è§’åˆ†è¾¨ç‡ï¼Œä½†ç°ä»£å»‰ä»·ä¸”ç´§å‡‘çš„å›ºæ€é›·è¾¾é˜µåˆ—å…·æœ‰å°å¤©çº¿é˜µåˆ—ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æ–¹ä½è§’å’Œä»°è§’è½´ä¸Šè¿œé€Šäºå…¸å‹ç›¸æœº [28]ã€‚å› æ­¤ï¼Œè¿™äº›ç´§å‡‘å‹é›·è¾¾åªèƒ½åœ¨æ–¹ä½è§’å’Œä»°è§’è½´ä¸Šç”Ÿæˆç²—ç³™çš„çƒ­å›¾ï¼ˆ&gt;15â—¦ åˆ†è¾¨ç‡ï¼‰ï¼Œå¯¼è‡´æ¯ä¸ªè·ç¦»-æ–¹ä½è§’-ä»°è§’ç®±æŒ‡å‘ 3D ç©ºé—´ä¸­çš„ä¸€ä¸ªè¾ƒç²—ç³™åŒºåŸŸï¼Œè¿œä¸å¦‚æ¥è‡ªç›¸æœºåƒç´ çš„å°„çº¿æ¸…æ™° [38, 41, 76]ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„è§’åº¦åˆ†è¾¨ç‡ï¼Œé›·è¾¾å¯ä»¥åˆ©ç”¨å¤šæ™®å‹’æ•ˆåº”ï¼šç›¸å¯¹äºé›·è¾¾ä»¥ä¸åŒç›¸å¯¹é€Ÿåº¦ç§»åŠ¨çš„ç‰©ä½“å…·æœ‰ä¸åŒçš„å¤šæ™®å‹’é€Ÿåº¦ï¼Œå¯ä»¥é€šè¿‡æ£€æŸ¥è·ç¦»-æ–¹ä½è§’-ä»°è§’çƒ­å›¾çš„æ®‹ä½™ç›¸ä½æ¥æµ‹é‡è¿™äº›é€Ÿåº¦ [79]ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œåœ¨é™æ€åœºæ™¯ä¸­ï¼Œè¿™äº›ç›¸å¯¹é€Ÿåº¦ä¸ä»…å–å†³äºé›·è¾¾å’Œä¸–ç•Œä¹‹é—´çš„ç›¸å¯¹é€Ÿåº¦ï¼Œè¿˜å–å†³äºç‰©ä½“ä¸é›·è¾¾ä¹‹é—´çš„ç›¸å¯¹æ–¹ä½è§’å’Œä»°è§’ï¼Œæ¯ä¸ªå¤šæ™®å‹’å¯¹åº”äºç©ºé—´ä¸­çš„ä¸€ä¸ªåœ†é”¥ [60]ã€‚ç”±äºæ›´ç²¾ç»†çš„è·ç¦»å’Œå¤šæ™®å‹’åˆ†è¾¨ç‡ï¼Œå¤šæ™®å‹’æå¤§åœ°é™ä½äº† 3D ç©ºé—´ä¸­æ¯ä¸ªç®±çš„æ¨¡ç³Šæ€§ï¼Œä½¿å…¶å˜ä¸ºä¸€ä¸ªè–„ç¯ï¼ˆå›¾ 4ï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨è·ç¦»å’Œå¤šæ™®å‹’è½´ä¸Šè¿›è¡Œç»†åº¦è®ºè¯è¿›ä¸€æ­¥å°†å…¶ç®€åŒ–ä¸ºé›·è¾¾æ¸²æŸ“çš„åœ†åœˆï¼ˆç¬¬ 3.4 èŠ‚ï¼‰ã€‚(6) é›·è¾¾é¢„å¤„ç†æ¯«ç±³æ³¢é›·è¾¾ä½¿ç”¨ç§°ä¸ºè°ƒé¢‘è¿ç»­æ³¢ (FMCW) çš„æ³¢å½¢ï¼Œå¹¶æµ‹é‡è¿ç»­æ—¶é—´ä¿¡å·ï¼›ç„¶åæˆ‘ä»¬å°†è¿™äº›ä¿¡å·è½¬æ¢ä¸ºè·ç¦»-å¤šæ™®å‹’-å¤©çº¿çƒ­å›¾ã€‚ä¸ºäº†æ€»ç»“æˆ‘ä»¬çš„é›·è¾¾å¤„ç†ç®¡é“çš„è¦ç‚¹ï¼ˆé™„å½• A.1ï¼‰ï¼šâ€¢ ä¸å¸Œæœ›çš„è·ç¦»-å¤šæ™®å‹’æ—ç“£ï¼šå•ä¸ªåå°„ç‰©ä½“å¯ä»¥åˆ›å»ºæ—ç“£ï¼Œè¿™äº›æ—ç“£ä¼šæ¸—å…¥å‡ ä¸ªè·ç¦»-å¤šæ™®å‹’ç®±å¹¶æ©ç›–è¾ƒå¼±çš„ç‰©ä½“ [61, 86]ã€‚æˆ‘ä»¬ä½¿ç”¨æ±‰å®åŠ æƒçª—å£æ²¿ç€è·ç¦»å’Œå¤šæ™®å‹’è½´æ¥å‡è½»è¿™ç§å½±å“ï¼Œè€Œä¸æ˜¯å¼ºè¿« DART å¯¹å…¶è¿›è¡Œå»ºæ¨¡ï¼ˆé™„å½• A.1ï¼‰ã€‚â€¢ å¤šä¸ªå¤©çº¿ï¼šæˆ‘ä»¬å¯¹é›·è¾¾ä¸­çš„å…«ä¸ªå‘å°„-æ¥æ”¶ (TX/RX) å¯¹æ‰§è¡Œè·ç¦»-å¤šæ™®å‹’å¤„ç†ã€‚åœ¨æˆ‘ä»¬çš„æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼ˆç¬¬ 3.4 èŠ‚ï¼‰ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ª TX/RX å¯¹åº”ç”¨å¤©çº¿å¢ç›Šå’Œé˜µåˆ—å› å­ï¼ˆå›¾ 3ï¼‰ï¼Œå¼ºè°ƒè§†é‡çš„ 8 ä¸ªéƒ¨åˆ†ã€‚è™½ç„¶æˆ‘ä»¬å¯¹é«˜è´¨é‡æ–¹ä½è§’-ä»°è§’ä¿¡æ¯çš„æ„ŸçŸ¥ä»ç„¶æºäºåˆ©ç”¨å¤šæ™®å‹’ï¼Œä½†è¿™æä¾›äº†ä¸€äº›ç²—ç•¥çš„æ–¹å‘ä¿¡æ¯ã€‚(7) DART çš„ä¸–ç•Œæ¨¡å‹å¦‚æœæˆ‘ä»¬æœ‰ä¸–ç•Œå’Œä¸–ç•Œä¸­æ‰€æœ‰ç‰©ä½“ç”µç£æ³¢ç›¸äº’ä½œç”¨çš„å‡†ç¡®æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è¯¥æ¨¡å‹åº”ç”¨äºç”±æ¯ä¸ªè·ç¦»-å¤šæ™®å‹’åƒç´ å®šä¹‰çš„åŒºåŸŸæ¥è®¡ç®—å…¶å€¼ã€‚ç„¶è€Œï¼Œç”±äºç°å®ä¸–ç•Œåœºæ™¯å’Œäº¤äº’çš„å¤æ‚æ€§ï¼Œè¿™ä¸¤ä¸ªä»»åŠ¡éƒ½éå¸¸å›°éš¾ä¸”é€šå¸¸ä¸åˆ‡å®é™…ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼å¯¹è¿™äº›å±æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç”¨è§†åœºç›¸å…³çš„ç¥ç»ç½‘ç»œæ–¹æ³•è¡¨ç¤ºåå°„ç‡å’Œé€å°„ç‡ã€‚å»ºæ¨¡å°„é¢‘åå°„ç‡å»ºæ¨¡æ¯«ç±³æ³¢ææ–™ç›¸äº’ä½œç”¨æ˜¯é›·è¾¾è§†å›¾åˆæˆæœ€å…·æŒ‘æˆ˜æ€§çš„å› ç´ ä¹‹ä¸€ã€‚ä»é›·è¾¾çš„è§’åº¦æ¥çœ‹ï¼Œç©ºé—´ä¸­çš„ç‚¹å…·æœ‰ä¸¤ä¸ªå…³é”®å±æ€§ï¼šåå°„ç‡ï¼ˆåå°„å›çš„èƒ½é‡æ¯”ä¾‹ï¼‰å’Œé€å°„ç‡ï¼ˆç»§ç»­è¿‡å»çš„èƒ½é‡æ¯”ä¾‹ï¼‰[60]ã€‚ç„¶è€Œï¼Œæ¯«ç±³æ³¢ä¹Ÿä¼šæ ¹æ®å…¥å°„è§’ä¸ç‰©ä½“è¿›è¡Œä¸åŒçš„äº¤äº’ [4]ï¼›ä¾‹å¦‚ï¼Œé‡‘å±è¡¨é¢å¯èƒ½æ˜¯é•œé¢åå°„çš„ï¼Œå¹¶ä¸”å¯èƒ½ä»æŸäº›è§†ç‚¹ä¸å¯è§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨åå°„ç‡ Ïƒï¼šR6â†’R å’Œé€å°„ç‡ Î±ï¼šR6â†’[0,1] å¯¹æ¯ä¸ªç‰©ç†ç‚¹è¿›è¡Œå»ºæ¨¡ï¼Œ(1)å®ƒå°†åå°„ç‡ Ïƒ å’Œé€å°„ç‡ Î± å»ºæ¨¡ä¸ºå…¥å°„æ³¢çš„ä½ç½® (R3) å’Œå…¥å°„è§’ (R3) çš„å‡½æ•°ï¼Œå¹¶å…è®¸ DART å¯¹å„ç§é›·è¾¾ç°è±¡è¿›è¡Œå»ºæ¨¡ï¼Œä¾‹å¦‚éƒ¨åˆ†é®æŒ¡ã€é•œé¢åå°„å’Œé‡å½±ï¼ˆé™„å½• A.2ï¼‰ã€‚ä¸–ç•Œè¡¨ç¤ºè™½ç„¶åŸºäºä½“ç´ çš„æ–¹æ³•å¯¹äºå­¦ä¹ è§†è§‰è¾å°„åœºéå¸¸æœ‰æ•ˆ [18, 83]ï¼Œä½†å³ä½¿åœ¨åˆ©ç”¨å¤šæ™®å‹’è½´åï¼Œé›·è¾¾å›¾åƒä¸ç›¸æœºç›¸æ¯”ä¹Ÿå…·æœ‰æ›´å·®çš„ä»°è§’å’Œæ–¹ä½è§’åˆ†è¾¨ç‡ã€‚è¿™æ”¾å¤§äº† Ïƒ å’Œ Î± å¯ä»¥è§£å†³çš„ç©ºé—´åˆ†è¾¨ç‡å·®å¼‚ï¼Œå³ä½¿åœ¨è¿‘è·ç¦»å’Œè¿œè·ç¦»ä¹‹é—´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œä¸ç›¸æœºä¸åŒï¼Œæˆ‘ä»¬çš„è§’åº¦åˆ†è¾¨ç‡åœ¨æ‰€æœ‰å°ºåº¦ä¸Šéƒ½æ˜¯å¯å˜çš„â€”â€”æ— è®ºæ˜¯åœ¨è½¨è¿¹çº§åˆ«ã€å¸§åˆ°å¸§çº§åˆ«ç”šè‡³å¸§å†…ï¼ˆç¬¬ 3.1 èŠ‚ï¼‰ã€‚ç±»ä¼¼äº NeRF [48]ï¼Œæˆ‘ä»¬è½¬å‘ç¥ç»éšå¼è¡¨ç¤ºä½œä¸ºåˆ›å»ºâ€œè‡ªé€‚åº”â€ç½‘æ ¼çš„ä¸€ç§æ‰‹æ®µï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¨¡å‹åŸºäº Instant Neural Graphics Primitive3 [51]ã€‚ä¸å¤§å¤šæ•°è§†è§‰ NeRF ä¸åŒï¼Œæˆ‘ä»¬ä¸å°†å…¥å°„è§’ä½œä¸ºè¾“å…¥æä¾›ç»™ç¥ç»ç½‘ç»œ [74]ã€‚ç›¸åï¼Œæˆ‘ä»¬çš„æ¶æ„ï¼ˆå¯è§†åŒ–åœ¨å›¾ 3 çš„ä¸­å¿ƒå—ä¸­ï¼‰è¾“å‡ºâ€œåŸºæœ¬â€åå°„ç‡ Â¯Ïƒ å’Œé€å°„ç‡ Â¯Î±ï¼Œä»¥åŠå…±äº«çƒè°å‡½æ•°ç³»æ•° [83]ï¼Œè¿™äº›ç³»æ•°ä½œä¸ºå†…ç§¯åº”ç”¨äºå…¥å°„è§’ã€‚é™¤äº†è®¡ç®—ä¼˜åŠ¿ä¹‹å¤–ï¼Œè¿™è¿˜å…è®¸æˆ‘ä»¬ç›´æ¥å°† (Â¯Ïƒ, Â¯Î±) è§£é‡Šä¸ºæˆ‘ä»¬å­¦ä¹ çš„åå°„ç‡å’Œé€å°„ç‡å‡½æ•°çš„çƒç§¯åˆ†ï¼ˆé™„å½• A.3ï¼‰ã€‚æˆ‘ä»¬è¿˜å‘ç° Ïƒ å’Œ Î± ä¸Šçš„è¾“å‡ºæ¿€æ´»å‡½æ•°å¯¹äºæ•°å€¼ç¨³å®šæ€§å’Œæ€§èƒ½è‡³å…³é‡è¦ã€‚ç”±äº Ïƒ æ˜¯æ— ç•Œçš„4ï¼Œæˆ‘ä»¬å¯¹ Ïƒ åº”ç”¨çº¿æ€§æ¿€æ´»ã€‚ç„¶åï¼Œä¸ºäº†å°† Î± çº¦æŸåœ¨ [0,1] ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨æ¿€æ´»å‡½æ•° f(Î±) = exp(max(0,Î±))ï¼Œ(2)æˆ‘ä»¬å°†å…¶ä¸è‡ªå®šä¹‰æ¢¯åº¦ä¼°è®¡å™¨é…å¯¹ä»¥å¤„ç†åˆå§‹åŒ–ä¸ç¨³å®šæ€§ï¼ˆé™„å½• A.4ï¼‰ã€‚(8) é›·è¾¾æ¸²æŸ“å’Œæ¨¡å‹è®­ç»ƒæˆ‘ä»¬ä½¿ç”¨å¯å¾®æ˜ å°„è®­ç»ƒ Ïƒ å’Œ Î±ï¼Œè¯¥æ˜ å°„ä»ç»™å®šçš„ (Ïƒ, Î±) ç½‘ç»œç”Ÿæˆå¤šå¤©çº¿è·ç¦»-å¤šæ™®å‹’çƒ­å›¾ï¼›æˆ‘ä»¬ç§°ä¹‹ä¸ºé›·è¾¾æ¸²æŸ“ã€‚ä¸è§†è§‰ NeRF ä¸åŒï¼ŒDART é™¤äº†é®æŒ¡ä¹‹å¤–è¿˜å¿…é¡»è€ƒè™‘ä¸€ç³»åˆ—ç‰©ç†æ•ˆåº”ï¼ŒåŒ…æ‹¬è·¯å¾„è¡°å‡ã€å¤©çº¿å¢ç›Šæ¨¡å¼å’Œé›·è¾¾ç‰¹å®šçš„å¤šæ™®å‹’è½´ã€‚å°„çº¿è¿½è¸ªè€ƒè™‘ä»é›·è¾¾ä½ç½® x å’Œæ–¹å‘ï¼ˆæ—‹è½¬çŸ©é˜µï¼‰A ä»¥å…¥å°„è§’ w å‘å°„çš„å•ä¸ªâ€œå°„çº¿â€ã€‚å½“å°„çº¿åœ¨å¤ªç©ºä¸­ä¼ æ’­åˆ°å¤„ç†çš„ï¼ˆè·ç¦»ã€å¤šæ™®å‹’ã€å¤©çº¿ï¼‰å›¾åƒçš„æœ€å¤§èŒƒå›´æ—¶ï¼Œæ¯ä¸ªç‚¹ x + riw åœ¨è·ç¦» r å¤„æ¥æ”¶å¹…åº¦ä¸º u_i çš„ä¿¡å·ï¼Œè¯¥ä¿¡å·å› è‡ªç”±ç©ºé—´è€Œè¡°å‡ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº† DARTï¼ˆå¤šæ™®å‹’è¾…åŠ©é›·è¾¾å±‚ææˆåƒï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœºæŠ€æœ¯ï¼Œæ— éœ€æ˜¾å¼æ¨¡å‹å³å¯ç”Ÿæˆé€¼çœŸçš„é›·è¾¾å›¾åƒï¼Œä¸ºæ–°å‹è§†è§’åˆæˆæä¾›äº†æ–°çš„æ–¹æ³•ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§é›·è¾¾ç‰¹å®šç‰©ç†æ¨¡å‹ï¼Œç”¨äºè§£é‡Šç”µç£æ³¢ç›¸äº’ä½œç”¨çš„é›·è¾¾ç‰¹å®šæ•ˆåº”ã€‚</li><li>è®¾è®¡äº†ä¸€ç§è·ç¦»-å¤šæ™®å‹’æ¸²æŸ“æ–¹æ³•ï¼Œç”¨äºæœ‰æ•ˆåœ°è®­ç»ƒå’Œå­¦ä¹ é›·è¾¾çš„ç¥ç»éšå¼åœ°å›¾ã€‚</li><li>æå‡ºäº†ä¸€ç§å…³é”®æ¸²æŸ“ä¼˜åŒ–ï¼Œä»¥æé«˜æ¸²æŸ“æ•ˆç‡å’Œå›¾åƒè´¨é‡ã€‚</li><li>æ€§èƒ½ï¼šDART åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä»æ–°è§†è§’åˆæˆäº†å‡ºè‰²çš„é›·è¾¾è·ç¦»-å¤šæ™®å‹’å›¾åƒï¼Œæ­¤å¤–è¿˜å¯ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å±‚æå›¾åƒã€‚</li><li>å·¥ä½œé‡ï¼šDART çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7a08f4b46a27b4550cca3fdbb7bb2699.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5dd4309cf1d06499c45ea2d70f80cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d4136ef209f4ed07822647cd67d564e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4196074de7d63d703597568e97025da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7aa27948966717e8808650a0fc34b361.jpg" align="middle"></details><h2 id="DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes"><a href="#DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes" class="headerlink" title="DaReNeRF: Direction-aware Representation for Dynamic Scenes"></a>DaReNeRF: Direction-aware Representation for Dynamic Scenes</h2><p><strong>Authors:Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu</strong></p><p>Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance. </p><p><a href="http://arxiv.org/abs/2403.02265v1">PDF</a> Accepted at CVPR 2024. Paper + supplementary material</p><p><strong>Summary</strong><br>ä½¿ç”¨å…­ä¸ªä¸åŒæ–¹å‘æ•æ‰åœºæ™¯åŠ¨æ€å¹¶èåˆä¿¡æ¯ï¼ŒDaReNeRF åœ¨å¤æ‚åŠ¨æ€åœºæ™¯çš„æ–°è§†å›¾åˆæˆä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨å…­ä¸ªæ–¹å‘æ„ŸçŸ¥è¡¨ç¤ºæ•è·åœºæ™¯åŠ¨æ€ã€‚</li><li>é‡‡ç”¨é€†å‘åŒæ ‘å¤å°æ³¢å˜æ¢æ¢å¤å¹³é¢ä¿¡æ¯ã€‚</li><li>å°†æ–¹å‘æ„ŸçŸ¥è¡¨ç¤ºèåˆåˆ° NeRF ä¸­ï¼Œè®¡ç®—æ—¶ç©ºç‚¹çš„ç‰¹å¾ã€‚</li><li>ä½¿ç”¨å°çš„ MLP è¿›è¡Œé¢œè‰²å›å½’ï¼Œåˆ©ç”¨ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒã€‚</li><li>å¼•å…¥å¯è®­ç»ƒæ©ç æ–¹æ³•ï¼Œåœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹å‡è½»å­˜å‚¨é—®é¢˜ã€‚</li><li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘ 2 å€ï¼ŒåŒæ—¶æ€§èƒ½æ›´ä¼˜ã€‚</li><li>é€‚ç”¨äºå…·æœ‰å¤æ‚è¿åŠ¨çš„é«˜ä¿çœŸåœºæ™¯çš„é‡æ–°æ¸²æŸ“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>æ ‡é¢˜ï¼š</strong> DaReNeRFï¼šåŠ¨æ€åœºæ™¯çš„æ–¹å‘æ„ŸçŸ¥è¡¨å¾</li><li><strong>ä½œè€…ï¼š</strong> Ange Lou, Tianyu Luan, Hao Ding, Wenbo Luo, Xiaogang Wang, Wenzheng Chen</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong> United Imaging Intelligence</li><li><strong>å…³é”®è¯ï¼š</strong> åŠ¨æ€åœºæ™¯ï¼Œç¥ç»è¾å°„åœºï¼Œå¹³é¢è¡¨ç¤ºï¼Œæ–¹å‘æ„ŸçŸ¥è¡¨å¾</li><li><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> None</li><li><p><strong>æ‘˜è¦ï¼š</strong>   (1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> è¿‘æœŸæ–¹æ³•ä½¿ç”¨åŸºäºå¹³é¢çš„æ˜¾å¼è¡¨å¾æ¥ç®€åŒ–åŠ¨æ€åœºæ™¯å»ºæ¨¡å’Œæ¸²æŸ“ï¼Œå…‹æœäº†ç¥ç»è¾å°„åœºç­‰æ–¹æ³•ç›¸å…³çš„è®­ç»ƒæ—¶é—´æ…¢çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå°† 4D åŠ¨æ€åœºæ™¯ç›´æ¥åˆ†è§£ä¸ºå¤šä¸ªåŸºäºå¹³é¢çš„ 2D è¡¨å¾ä¸è¶³ä»¥æ¸²æŸ“å…·æœ‰å¤æ‚è¿åŠ¨çš„é«˜ä¿çœŸåœºæ™¯ã€‚   (2) <strong>è¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼š</strong> ç°æœ‰æ–¹æ³•å°†åŠ¨æ€åœºæ™¯åˆ†è§£ä¸ºå¤šä¸ªåŸºäºå¹³é¢çš„ 2D è¡¨å¾ï¼Œä½†è¿™ç§æ–¹æ³•ä¸è¶³ä»¥æ¸²æŸ“å…·æœ‰å¤æ‚è¿åŠ¨çš„é«˜ä¿çœŸåœºæ™¯ã€‚   (3) <strong>ç ”ç©¶æ–¹æ³•ï¼š</strong> æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹å‘æ„ŸçŸ¥è¡¨å¾ (DaRe) æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»å…­ä¸ªä¸åŒæ–¹å‘æ•è·åœºæ™¯åŠ¨æ€ã€‚è¿™ç§å­¦ä¹ åˆ°çš„è¡¨å¾ç»è¿‡é€†åŒæ ‘å¤å°æ³¢å˜æ¢ (DTCWT) ä»¥æ¢å¤åŸºäºå¹³é¢çš„ä¿¡æ¯ã€‚DaReNeRF é€šè¿‡èåˆè¿™äº›æ¢å¤çš„å¹³é¢çš„å‘é‡æ¥è®¡ç®—æ¯ä¸ªæ—¶ç©ºç‚¹çš„ç‰¹å¾ã€‚å°† DaReNeRF ä¸ç”¨äºé¢œè‰²å›å½’çš„å¾®å° MLP ç»“åˆèµ·æ¥ï¼Œå¹¶åˆ©ç”¨ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒï¼Œåœ¨å¤æ‚åŠ¨æ€åœºæ™¯çš„æ–°è§†è§’åˆæˆä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚   (4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> DaReNeRF åœ¨è®­ç»ƒæ—¶é—´ä¸Šæ¯”ç°æœ‰æ–¹æ³•å‡å°‘äº† 2 å€ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1): è¯¥æ–¹æ³•ä»å…­ä¸ªä¸åŒæ–¹å‘æ•è·åœºæ™¯åŠ¨æ€ï¼Œå­¦ä¹ åˆ°çš„è¡¨å¾ç»è¿‡é€†åŒæ ‘å¤å°æ³¢å˜æ¢ (DTCWT) ä»¥æ¢å¤åŸºäºå¹³é¢çš„ä¿¡æ¯ã€‚(2): DaReNeRF é€šè¿‡èåˆè¿™äº›æ¢å¤çš„å¹³é¢çš„å‘é‡æ¥è®¡ç®—æ¯ä¸ªæ—¶ç©ºç‚¹çš„ç‰¹å¾ã€‚(3): å°† DaReNeRF ä¸ç”¨äºé¢œè‰²å›å½’çš„å¾®å° MLP ç»“åˆèµ·æ¥ï¼Œå¹¶åˆ©ç”¨ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒã€‚</p></li><li><p>ç»“è®ºï¼š(1): æœ¬å·¥ä½œé€šè¿‡æå‡º DaReNeRF æ–¹æ³•ï¼Œåœ¨åŠ¨æ€åœºæ™¯å»ºæ¨¡å’Œæ¸²æŸ“é¢†åŸŸå–å¾—äº†é‡è¦è¿›å±•ã€‚è¯¥æ–¹æ³•ä»å…­ä¸ªä¸åŒæ–¹å‘æ•è·åœºæ™¯åŠ¨æ€ï¼Œå¹¶åˆ©ç”¨é€†åŒæ ‘å¤å°æ³¢å˜æ¢æ¢å¤åŸºäºå¹³é¢çš„ä¿¡æ¯ï¼Œä»è€Œæœ‰æ•ˆè§£å†³äº†å¤æ‚åŠ¨æ€åœºæ™¯çš„é«˜ä¿çœŸæ¸²æŸ“é—®é¢˜ã€‚(2): åˆ›æ–°ç‚¹ï¼š</p></li><li>ä»å…­ä¸ªä¸åŒæ–¹å‘æ•è·åœºæ™¯åŠ¨æ€ï¼Œä¸°å¯Œäº†åœºæ™¯ä¿¡æ¯çš„è·å–ã€‚</li><li>é‡‡ç”¨é€†åŒæ ‘å¤å°æ³¢å˜æ¢æ¢å¤åŸºäºå¹³é¢çš„ä¿¡æ¯ï¼Œæœ‰æ•ˆèåˆäº†ä¸åŒæ–¹å‘çš„ç‰¹å¾ã€‚</li><li>å°† DaReNeRF ä¸å¾®å° MLP ç»“åˆï¼Œå¹¶åˆ©ç”¨ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†é«˜æ•ˆä¸”é«˜è´¨é‡çš„æ¸²æŸ“ã€‚æ€§èƒ½ï¼š</li><li>åœ¨å¤æ‚åŠ¨æ€åœºæ™¯çš„æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šï¼ŒDaReNeRF å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDaReNeRF è®­ç»ƒæ—¶é—´å‡å°‘äº† 2 å€ï¼Œæ¸²æŸ“æ•ˆç‡æ›´é«˜ã€‚å·¥ä½œé‡ï¼š</li><li>DaReNeRF æ–¹æ³•çš„å®ç°éš¾åº¦é€‚ä¸­ï¼Œéœ€è¦å¯¹ç¥ç»è¾å°„åœºã€å°æ³¢å˜æ¢å’Œä½“ç§¯æ¸²æŸ“ç­‰æŠ€æœ¯æœ‰ä¸€å®šçš„äº†è§£ã€‚</li><li>è®­ç»ƒ DaReNeRF æ¨¡å‹éœ€è¦å¤§é‡çš„åŠ¨æ€åœºæ™¯æ•°æ®å’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0b34eef417abcdd2b497ef2ebd10beb3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94b89ba44b447b4f183c953bb896e07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fc68e3cc2c894a358a3d010ccbf0fa0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f3c90874730f6ec424afc1f7edde45a.jpg" align="middle"></details><h2 id="Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views"><a href="#Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views" class="headerlink" title="Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views"></a>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views</h2><p><strong>Authors:Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song</strong></p><p>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines. </p><p><a href="http://arxiv.org/abs/2403.02063v1">PDF</a> </p><p><strong>Summary</strong><br><strong>NeRFæ·±åº¦å¼•å¯¼ç‚¹äº‘èåˆï¼šå¢å¼ºç¨€ç–è¾“å…¥åœºæ™¯ä¸‹æ–°è§†è§’åˆæˆ</strong></p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºæ·±åº¦å¼•å¯¼çš„NeRFï¼Œç”¨äºç¨€ç–è¾“å…¥çš„æ–°è§†è§’åˆæˆã€‚</li><li>ä½¿ç”¨æ˜¾å¼ä½“ç´ ç½‘æ ¼è¡¨ç¤ºè¾å°„åœºã€‚</li><li>æ„é€ æ¯ä¸ªè¾“å…¥è§†å›¾çš„ç‚¹äº‘ï¼Œå¹¶åœ¨ä½“ç´ ç½‘æ ¼ä¸­ç”¨çŸ©é˜µå’Œå‘é‡æè¿°ã€‚</li><li>èåˆæ¯ä¸ªè¾“å…¥è§†å›¾çš„ç‚¹äº‘ï¼Œæ„å»ºæ•´ä¸ªåœºæ™¯çš„èåˆç‚¹äº‘ã€‚</li><li>æ¯ä¸ªä½“ç´ æ ¹æ®æ•´ä¸ªåœºæ™¯çš„ç‚¹äº‘ç¡®å®šå…¶å¯†åº¦å’Œå¤–è§‚ã€‚</li><li>é€šè¿‡ç‚¹äº‘èåˆå’Œä½“ç´ ç½‘æ ¼å¾®è°ƒï¼Œå¯ä»¥ä¿®æ­£å’Œæ›¿æ¢æ·±åº¦å€¼çš„è¯¯å·®ã€‚</li><li>é€šè¿‡æœ‰æ•ˆçš„å‘é‡-çŸ©é˜µåˆ†è§£ï¼Œæ–¹æ³•å®ç°äº†æ›´å¿«çš„é‡å»ºå’Œæ›´å¤§çš„ç´§å‡‘æ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šæ·±åº¦å¼•å¯¼çš„é²æ£’ä¸”å¿«é€Ÿçš„ç‚¹äº‘èåˆ NeRFï¼Œç”¨äºç¨€ç–è¾“å…¥è§†å›¾</li><li>ä½œè€…ï¼šShuai Guoã€Qiuwen Wangã€Yijie Gaoã€Rong Xieã€Li Song</li><li>éš¶å±å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦å›¾åƒé€šä¿¡ä¸ç½‘ç»œå·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šNeRFã€ç¨€ç–è§†å›¾ã€æ·±åº¦èåˆã€ç‚¹äº‘èåˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šNone    Github ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨ç¨€ç–è¾“å…¥è§†å›¾ä¸‹çš„æ–°è§†å›¾åˆæˆå¯¹äº AR/VR å’Œè‡ªåŠ¨é©¾é©¶ç­‰çœŸå®ä¸–ç•Œåº”ç”¨éå¸¸é‡è¦ã€‚   ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å°†æ·±åº¦ä¿¡æ¯é›†æˆåˆ° NeRF ä¸­ä»¥è¿›è¡Œç¨€ç–è¾“å…¥åˆæˆï¼Œåˆ©ç”¨æ·±åº¦å…ˆéªŒè¿›è¡Œå‡ ä½•å’Œç©ºé—´ç†è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œå¾€å¾€å¿½ç•¥æ·±åº¦å›¾ä¸­çš„ä¸å‡†ç¡®æ€§ï¼Œå¹¶ä¸”æ—¶é—´æ•ˆç‡ä½ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç¨€ç–è¾“å…¥çš„æ·±åº¦å¼•å¯¼çš„é²æ£’ä¸”å¿«é€Ÿçš„ç‚¹äº‘èåˆ NeRFã€‚æˆ‘ä»¬å°†è¾å°„åœºæ„ŸçŸ¥ä¸ºä¸€ä¸ªæ˜¾å¼çš„ç‰¹å¾ä½“ç´ ç½‘æ ¼ã€‚ä¸ºæ¯ä¸ªè¾“å…¥è§†å›¾æ„å»ºä¸€ä¸ªç‚¹äº‘ï¼Œä½¿ç”¨çŸ©é˜µå’Œå‘é‡åœ¨ä½“ç´ ç½‘æ ¼ä¸­è¡¨å¾ã€‚æˆ‘ä»¬ç´¯ç§¯æ¯ä¸ªè¾“å…¥è§†å›¾çš„ç‚¹äº‘ï¼Œä»¥æ„å»ºæ•´ä¸ªåœºæ™¯çš„èåˆç‚¹äº‘ã€‚æ¯ä¸ªä½“ç´ é€šè¿‡å‚è€ƒæ•´ä¸ªåœºæ™¯çš„ç‚¹äº‘æ¥ç¡®å®šå…¶å¯†åº¦å’Œå¤–è§‚ã€‚é€šè¿‡ç‚¹äº‘èåˆå’Œä½“ç´ ç½‘æ ¼å¾®è°ƒï¼Œå¯ä»¥ç»†åŒ–æ·±åº¦å€¼ä¸­çš„ä¸å‡†ç¡®æ€§æˆ–ç”¨å…¶ä»–è§†å›¾ä¸­çš„å€¼æ›¿æ¢å®ƒä»¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡æœ‰æ•ˆçš„å‘é‡çŸ©é˜µåˆ†è§£å®ç°æ›´å¿«çš„é‡å»ºå’Œæ›´é«˜çš„ç´§å‡‘æ€§ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå®éªŒç»“æœå¼ºè°ƒäº†æˆ‘ä»¬æ–¹æ³•ä¸æœ€å…ˆè¿›åŸºå‡†ç›¸æ¯”çš„å“è¶Šæ€§èƒ½å’Œæ—¶é—´æ•ˆç‡ã€‚</li></ol><p>7.Methods:(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦å¼•å¯¼çš„é²æ£’ä¸”å¿«é€Ÿçš„ç‚¹äº‘èåˆNeRFï¼Œç”¨äºç¨€ç–è¾“å…¥è§†å›¾ï¼›(2): å°†è¾å°„åœºæ„ŸçŸ¥ä¸ºä¸€ä¸ªæ˜¾å¼çš„ç‰¹å¾ä½“ç´ ç½‘æ ¼ï¼Œä¸ºæ¯ä¸ªè¾“å…¥è§†å›¾æ„å»ºä¸€ä¸ªç‚¹äº‘ï¼Œå¹¶ä½¿ç”¨çŸ©é˜µå’Œå‘é‡åœ¨ä½“ç´ ç½‘æ ¼ä¸­è¡¨å¾ï¼›(3): ç´¯ç§¯æ¯ä¸ªè¾“å…¥è§†å›¾çš„ç‚¹äº‘ï¼Œä»¥æ„å»ºæ•´ä¸ªåœºæ™¯çš„èåˆç‚¹äº‘ï¼Œæ¯ä¸ªä½“ç´ é€šè¿‡å‚è€ƒæ•´ä¸ªåœºæ™¯çš„ç‚¹äº‘æ¥ç¡®å®šå…¶å¯†åº¦å’Œå¤–è§‚ï¼›(4): é€šè¿‡ç‚¹äº‘èåˆå’Œä½“ç´ ç½‘æ ¼å¾®è°ƒï¼Œå¯ä»¥ç»†åŒ–æ·±åº¦å€¼ä¸­çš„ä¸å‡†ç¡®æ€§æˆ–ç”¨å…¶ä»–è§†å›¾ä¸­çš„å€¼æ›¿æ¢å®ƒä»¬ï¼›(5): æ­¤å¤–ï¼Œé€šè¿‡æœ‰æ•ˆçš„å‘é‡çŸ©é˜µåˆ†è§£ï¼Œå¯ä»¥å®ç°æ›´å¿«çš„é‡å»ºå’Œæ›´é«˜çš„ç´§å‡‘æ€§ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„æ·±åº¦å¼•å¯¼çš„é²æ£’ä¸”å¿«é€Ÿçš„ç‚¹äº‘èåˆNeRFï¼Œå¯¹äºç¨€ç–è¾“å…¥è§†å›¾ä¸‹çš„æ–°è§†å›¾åˆæˆå…·æœ‰é‡è¦æ„ä¹‰ã€‚ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li><li>å°†è¾å°„åœºæ„ŸçŸ¥ä¸ºä¸€ä¸ªæ˜¾å¼çš„ç‰¹å¾ä½“ç´ ç½‘æ ¼ï¼Œå¹¶ä½¿ç”¨çŸ©é˜µå’Œå‘é‡è¿›è¡Œè¡¨å¾ã€‚</li><li>é€šè¿‡ç‚¹äº‘èåˆå’Œä½“ç´ ç½‘æ ¼å¾®è°ƒï¼Œç»†åŒ–æ·±åº¦å€¼ä¸­çš„ä¸å‡†ç¡®æ€§ã€‚</li><li>é€šè¿‡æœ‰æ•ˆçš„å‘é‡çŸ©é˜µåˆ†è§£ï¼Œå®ç°æ›´å¿«çš„é‡å»ºå’Œæ›´é«˜çš„ç´§å‡‘æ€§ã€‚æ€§èƒ½ï¼š</li><li>ä¸æœ€å…ˆè¿›çš„åŸºå‡†ç›¸æ¯”ï¼Œå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œæ—¶é—´æ•ˆç‡ã€‚å·¥ä½œé‡ï¼š</li><li>å®ç°äº†æ›´å¿«çš„é‡å»ºå’Œæ›´é«˜çš„ç´§å‡‘æ€§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-01b32742a4cabe31ed749a6761475634.jpg" align="middle"><img src="https://pica.zhimg.com/v2-70b0b04ae4cf460209e8f732888cddee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86aa24ab75498868b39b0c370990c2e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f6398dec60102c0bb1f5d24d9a89432.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d78f63f12b2bcb3ca39476e980147ba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4a484aa0d25d0950586c81e66b07ef9d.jpg" align="middle"></details><h2 id="NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning"><a href="#NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning" class="headerlink" title="NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning"></a>NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning</h2><p><strong>Authors:Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr</strong></p><p>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a>. </p><p><a href="http://arxiv.org/abs/2403.01325v1">PDF</a> AAAI 2024</p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨æ–°çš„è§†é‡åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç”Ÿæˆé«˜è´¨é‡æ–°è§†è§’å›¾åƒä»æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-VPT åˆ©ç”¨çº§è”è§†å›¾æç¤ºè°ƒæ•´èŒƒä¾‹æ¥è§£å†³æ–°è§†è§’åˆæˆä¸­çš„ç»†èŠ‚æ•è·ã€çº¹ç†å¢å¼ºå’Œ PSNR æå‡é—®é¢˜ã€‚</li><li>NeRF-VPT ä»…éœ€åœ¨å„ä¸ªè®­ç»ƒé˜¶æ®µå¯¹å‰ä¸€é˜¶æ®µæ¸²æŸ“ç»“æœçš„ RGB æ•°æ®è¿›è¡Œé‡‡æ ·ä½œä¸ºå…ˆéªŒã€‚</li><li>NeRF-VPT æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰æ–¹æ³•ä¸­ã€‚</li><li>NeRF-VPT åœ¨ Realistic Synthetic 360ã€Real Forward-Facingã€Replica æ•°æ®é›†å’Œç”¨æˆ·æ•è·æ•°æ®é›†ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®åœºæ™¯åŸºå‡†ä¸Šæ˜¾è‘—æå‡äº†åŸºå‡†æ€§èƒ½ï¼Œå¹¶äº§ç”Ÿäº†æ¯”æ‰€æœ‰æ¯”è¾ƒçš„æœ€æ–°æ–¹æ³•æ›´é«˜è´¨é‡çš„æ–°è§†è§’å›¾åƒã€‚</li><li>NeRF-VPT çš„çº§è”å­¦ä¹ å¼•å…¥äº†å¯¹ç¨€ç–è¾“å…¥åœºæ™¯çš„é€‚åº”æ€§ï¼Œä»è€Œæ˜¾ç€æé«˜äº†ç¨€ç–è§†è§’æ–°è§†è§’åˆæˆçš„å‡†ç¡®æ€§ã€‚</li><li>æºä»£ç å’Œæ•°æ®é›†å¯åœ¨ \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a> è·å¾—ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeRF-VPTï¼šé€šè¿‡è§†å›¾æç¤ºè°ƒæ•´å­¦ä¹ æ–°é¢–è§†å›¾è¡¨ç¤º</li><li>ä½œè€…ï¼šLinsheng Chenã€Guangrun Wangã€Liuchun Yuanã€Keze Wangã€Ken Dengã€Philip H.S. Torr</li><li>Affiliationï¼šä¸­å±±å¤§å­¦</li><li>å…³é”®è¯ï¼šNeRFã€æ–°é¢–è§†å›¾åˆæˆã€è§†å›¾æç¤ºè°ƒæ•´</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.01325   Github é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼š   ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šNeRF åœ¨æ–°é¢–è§†å›¾åˆæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç”Ÿæˆé«˜è´¨é‡çš„æ–°é¢–è§†å›¾å›¾åƒä»ç„¶æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚   ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åœ¨æ•æ‰å¤æ‚ç»†èŠ‚ã€å¢å¼ºçº¹ç†å’Œæé«˜ PSNR æ–¹é¢å–å¾—äº†å¯å–œçš„è¿›å±•ï¼Œä½†ä»éœ€è¦è¿›ä¸€æ­¥å…³æ³¨å’Œæ”¹è¿›ã€‚   ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º NeRF-VPT çš„æ–°é¢–è§†å›¾åˆæˆæ–¹æ³•ï¼Œé‡‡ç”¨çº§è”è§†å›¾æç¤ºè°ƒæ•´èŒƒå¼ã€‚è¯¥èŒƒå¼å°†æ¥è‡ªå…ˆå‰æ¸²æŸ“ç»“æœçš„ RGB ä¿¡æ¯ä½œä¸ºåç»­æ¸²æŸ“é˜¶æ®µçš„æŒ‡å¯¼æ€§è§†è§‰æç¤ºï¼ŒæœŸæœ›æç¤ºä¸­åµŒå…¥çš„å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿä¿ƒè¿›æ¸²æŸ“å›¾åƒè´¨é‡çš„é€æ­¥æé«˜ã€‚   ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ RealisticSynthetic360ã€RealForward-Facingã€Replica æ•°æ®é›†å’Œç”¨æˆ·æ•è·æ•°æ®é›†ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®åœºæ™¯åŸºå‡†ä¸Šï¼Œå°† NeRF-VPT ä¸åŸºäº NeRF çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œç»“æœè¡¨æ˜ NeRF-VPT æ˜¾ç€æå‡äº†åŸºå‡†æ€§èƒ½ï¼Œå¹¶æ¯”æ‰€æœ‰æ¯”è¾ƒçš„æœ€å…ˆè¿›æ–¹æ³•æ›´æœ‰æ•ˆåœ°ç”Ÿæˆäº†æ›´å¤šé«˜è´¨é‡çš„æ–°é¢–è§†å›¾å›¾åƒã€‚æ­¤å¤–ï¼ŒNeRF-VPT çš„çº§è”å­¦ä¹ å¼•å…¥äº†å¯¹ç¨€ç–è¾“å…¥åœºæ™¯çš„é€‚åº”æ€§ï¼Œä»è€Œæ˜¾ç€æé«˜äº†ç¨€ç–è§†å›¾æ–°é¢–è§†å›¾åˆæˆçš„å‡†ç¡®æ€§ã€‚</li></ol><p>7.æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šNeRF-VPTé‡‡ç”¨çº§è”è§†å›¾æç¤ºè°ƒæ•´èŒƒå¼ï¼Œå°†æ¥è‡ªå…ˆå‰æ¸²æŸ“ç»“æœçš„RGBä¿¡æ¯ä½œä¸ºåç»­æ¸²æŸ“é˜¶æ®µçš„æŒ‡å¯¼æ€§è§†è§‰æç¤ºï¼ŒæœŸæœ›æç¤ºä¸­åµŒå…¥çš„å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿä¿ƒè¿›æ¸²æŸ“å›¾åƒè´¨é‡çš„é€æ­¥æé«˜ã€‚ï¼ˆ2ï¼‰ï¼šNeRF-VPTåœ¨NeRFçš„åŸºç¡€ä¸Šï¼Œå°†ä½ç½®ç¼–ç å’Œæ–¹å‘ç¼–ç æ‰©å±•ä¸ºåŒ…å«å…ˆéªŒä¿¡æ¯çš„ç¼–ç ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ç»“æ„ï¼Œåœ¨æ¯ä¸€å±‚ä¸­ä½¿ç”¨æ›´æ–°çš„è§†å›¾æç¤ºæ¥æŒ‡å¯¼æ¸²æŸ“ã€‚ï¼ˆ3ï¼‰ï¼šNeRF-VPTå¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°å°†æ¸²æŸ“å›¾åƒä¸è§†å›¾æç¤ºä¹‹é—´çš„å·®å¼‚çº³å…¥è€ƒè™‘ï¼Œä»è€Œé¼“åŠ±æ¸²æŸ“å›¾åƒä¸è§†å›¾æç¤ºä¿æŒä¸€è‡´ã€‚</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œä»¥æé«˜åŸºäº NeRF çš„è§†å›¾åˆæˆçš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº† NeRF-VPTï¼Œå®ƒå¼•å…¥äº†ä¸€ç§å…·æœ‰å¾ªç¯æ¨¡å—çš„æ–°ç»“æ„ï¼Œå¹¶é‡‡ç”¨ NeRF çš„è¾“å‡ºä½œä¸ºå…ˆéªŒã€‚è¿™ä½¿å¾— NeRF-VPT èƒ½å¤Ÿæ˜¾ç€æé«˜è§†å›¾ç›¸å…³å¤–è§‚çš„è´¨é‡ã€‚å®ƒå¯¹ç«¯å£å‹å¥½ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¸ç°æœ‰æ–¹æ³•ç›¸ç»“åˆä»¥è·å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¸ºå……åˆ†åˆ©ç”¨è¡¨ç¤ºæä¾›äº†æ–°çš„è§†è§’ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li><li>æå‡ºäº†ä¸€ç§æ–°çš„è§†å›¾æç¤ºè°ƒæ•´èŒƒå¼ï¼Œå°†å…ˆéªŒä¿¡æ¯åµŒå…¥åˆ° NeRF ä¸­ï¼Œä»¥é€æ­¥æé«˜æ¸²æŸ“å›¾åƒçš„è´¨é‡ã€‚</li><li>è®¾è®¡äº†ä¸€ç§åˆ†å±‚ç»“æ„ï¼Œåœ¨æ¯ä¸€å±‚ä¸­ä½¿ç”¨æ›´æ–°çš„è§†å›¾æç¤ºæ¥æŒ‡å¯¼æ¸²æŸ“ï¼Œä»è€Œæ•è·å¤æ‚ç»†èŠ‚å¹¶å¢å¼ºçº¹ç†ã€‚</li><li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼Œå°†æ¸²æŸ“å›¾åƒä¸è§†å›¾æç¤ºä¹‹é—´çš„å·®å¼‚çº³å…¥è€ƒè™‘ï¼Œä»¥é¼“åŠ±æ¸²æŸ“å›¾åƒä¸è§†å›¾æç¤ºä¿æŒä¸€è‡´ã€‚</li><li>æ€§èƒ½ï¼š</li><li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®åœºæ™¯åŸºå‡†ä¸Šï¼ŒNeRF-VPT æ˜¾ç€æå‡äº†åŸºå‡†æ€§èƒ½ï¼Œå¹¶æ¯”æ‰€æœ‰æ¯”è¾ƒçš„æœ€å…ˆè¿›æ–¹æ³•æ›´æœ‰æ•ˆåœ°ç”Ÿæˆäº†æ›´å¤šé«˜è´¨é‡çš„æ–°é¢–è§†å›¾å›¾åƒã€‚</li><li>NeRF-VPT çš„çº§è”å­¦ä¹ å¼•å…¥äº†å¯¹ç¨€ç–è¾“å…¥åœºæ™¯çš„é€‚åº”æ€§ï¼Œä»è€Œæ˜¾ç€æé«˜äº†ç¨€ç–è§†å›¾æ–°é¢–è§†å›¾åˆæˆçš„å‡†ç¡®æ€§ã€‚</li><li>å·¥ä½œé‡ï¼š</li><li>NeRF-VPT çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„ NeRF æ¡†æ¶ä¸­ã€‚</li><li>NeRF-VPT çš„è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆä¸”ç¨³å®šï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§ç¡¬ä»¶å¹³å°ä¸Šè½»æ¾å¹¶è¡ŒåŒ–ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-a3d4a33c83819ae9629aeb5c7e195d32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c08401f045ff72d6d7af9a10c9430a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9c42f61f791fd5834fe43a11782fabd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-135c07d8cd0edaf636a5f342ab6e1725.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf190c96eea398ae33fd3f16daf3d9cc.jpg" align="middle"></details><h2 id="Neural-radiance-fields-based-holography-Invited"><a href="#Neural-radiance-fields-based-holography-Invited" class="headerlink" title="Neural radiance fields-based holography [Invited]"></a>Neural radiance fields-based holography [Invited]</h2><p><strong>Authors:Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba</strong></p><p>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented. </p><p><a href="http://arxiv.org/abs/2403.01137v1">PDF</a> </p><p><strong>Summary</strong><br>NeRFæŠ€æœ¯ç»“åˆæ·±åº¦é¢„æµ‹å™¨å’Œå…¨æ¯å›¾ç”Ÿæˆå™¨ï¼Œå¯å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å…¨æ¯å›¾ï¼Œæ— éœ€ç‰©ç†è®¡ç®—ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨NeRFæŠ€æœ¯ä»2Då›¾åƒç”Ÿæˆ3Då…‰åœºï¼Œä¸ºå…¨æ¯å›¾è®¡ç®—æä¾›æ•°æ®æºã€‚</li><li>æ„å»ºç”±NeRFã€æ·±åº¦é¢„æµ‹å™¨å’Œå…¨æ¯å›¾ç”Ÿæˆå™¨ç»„æˆçš„æ¸²æŸ“ç®¡é“ï¼Œç”¨äºå…¨æ¯å›¾ç”Ÿæˆã€‚</li><li>æ¸²æŸ“ç®¡é“å®Œå…¨åŸºäºæ·±åº¦å­¦ä¹ ï¼Œæ— ç‰©ç†è®¡ç®—ã€‚</li><li>æ¸²æŸ“ç®¡é“å¯å¿«é€Ÿç”Ÿæˆä»»æ„è§†è§’ä¸‹çš„3Dåœºæ™¯å…¨æ¯å›¾ã€‚</li><li>ä»¿çœŸå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®¡é“å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ¯å›¾ã€‚</li><li>è¯¥æ–¹æ³•æ¶ˆé™¤äº†å…¨æ¯å›¾è®¡ç®—ä¸­å¯¹ç‰©ç†æ¨¡æ‹Ÿçš„éœ€æ±‚ã€‚</li><li>é€šè¿‡ç»“åˆNeRFæŠ€æœ¯å’Œæ·±åº¦å­¦ä¹ ï¼Œè¯¥æ–¹æ³•æé«˜äº†å…¨æ¯å›¾ç”Ÿæˆçš„é€Ÿåº¦å’Œè´¨é‡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šåŸºäºç¥ç»è¾å°„åœºçš„å…¨æ¯æœ¯[å—é‚€]</li><li>ä½œè€…ï¼šMinsung Kang, Fan Wang, Kai Kumao, Tomoyoshi Ito, Tomoyoshi Shimobaba</li><li>éš¶å±å•ä½ï¼šåƒå¶å¤§å­¦å·¥ç¨‹å­¦é™¢</li><li>å…³é”®è¯ï¼šå…¨æ¯æ˜¾ç¤ºã€ç¥ç»è¾å°„åœºã€æ·±åº¦å­¦ä¹ ã€å…‰åœºé‡å»º</li><li>é“¾æ¥ï¼šhttp://dx.doi.org/10.1364/ao.XX.XXXXXX</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå…¨æ¯æ˜¾ç¤ºå™¨éœ€è¦ä¸‰ç»´åœºæ™¯æ•°æ®ã€å…¨æ¯å›¾å’Œä¸‰ç»´å›¾åƒå†ç°ä¸‰ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½å­˜åœ¨éšœç¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®å’Œå…¨æ¯å›¾çš„è®¡ç®—æ˜¯éšœç¢ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå…¨æ¯å›¾çš„è®¡ç®—åŸºäºå…‰ä¼ æ’­æ¨¡å‹ï¼Œå¯ä»¥åˆ†ä¸ºç‚¹äº‘ã€å¤šè¾¹å½¢ã€å…‰åœºå’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œä½†éƒ½éœ€è¦ç¹çä¸”è€—æ—¶çš„ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„å…¨æ¯å›¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç›´æ¥ä»æ–°åˆæˆè§†å›¾é¢„æµ‹å…¨æ¯å›¾ï¼Œè€Œæ— éœ€ä½¿ç”¨ä¸‰ç»´ç›¸æœºæˆ–ä¸‰ç»´å›¾å½¢å¤„ç†ç®¡é“ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šNeRFã€æ·±åº¦é¢„æµ‹å™¨å’Œå…¨æ¯å›¾ç”Ÿæˆå™¨ï¼Œæ‰€æœ‰è¿™äº›éƒ¨åˆ†éƒ½æ˜¯ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ„å»ºçš„ã€‚ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨åˆç†çš„æ—¶é—´å†…é¢„æµ‹äº†ä»ä»»ä½•æ–¹å‘è§‚çœ‹çš„ä¸‰ç»´åœºæ™¯çš„é¢„æµ‹å…¨æ¯å›¾ã€‚ä»¿çœŸå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ¯å›¾ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆã€‚</li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼šï¼ˆ1ï¼‰æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰ï¼šæå‡ºäº†åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å…¨æ¯å›¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç›´æ¥ä»æ–°åˆæˆè§†å›¾é¢„æµ‹å…¨æ¯å›¾ï¼Œæ— éœ€ä½¿ç”¨ä¸‰ç»´ç›¸æœºæˆ–ä¸‰ç»´å›¾å½¢å¤„ç†ç®¡é“ï¼Œä¸ºå…¨æ¯æ˜¾ç¤ºå™¨çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯ã€‚ï¼ˆ2ï¼‰æ–‡ç« çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š</li><li>åˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäº NeRF çš„å…¨æ¯å›¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€ä¸‰ç»´åœºæ™¯æ•°æ®ï¼Œç›´æ¥ä»åˆæˆè§†å›¾é¢„æµ‹å…¨æ¯å›¾ï¼Œç®€åŒ–äº†å…¨æ¯æ˜¾ç¤ºå™¨çš„ç”Ÿæˆæµç¨‹ã€‚</li><li>æ€§èƒ½ï¼šä»¿çœŸå’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ¯å›¾ï¼Œå¹¶ä¸”æ¯”ç°æœ‰æ–¹æ³•æ›´æœ‰æ•ˆã€‚</li><li>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-eb426bcf4ff137aa9adfa122cfe7a503.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6343dbdb7aebaa121558d05d8650d069.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca137b835829d4a4eee9df8c8a93246.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c695400302eaf7b15d2075d6d9b58551.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dcd582021c5b9223214535016af9ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3397dddd9230a1b23f0336e517fb6f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5cf31914b41fb8442b5926209326359c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a4f42e681d33823bde779da3c7eba53f.jpg" align="middle"></details><h2 id="Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss"><a href="#Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss" class="headerlink" title="Neural Field Classifiers via Target Encoding and Classification Loss"></a>Neural Field Classifiers via Target Encoding and Classification Loss</h2><p><strong>Authors:Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun</strong></p><p>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes. </p><p><a href="http://arxiv.org/abs/2403.01058v1">PDF</a> ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables</p><p><strong>Summary</strong><br>ç¥ç»åœºåˆ†ç±»å™¨æ¡†æ¶é€šè¿‡é¢„æµ‹é¢œè‰²ç¼–ç æ¥æ›¿ä»£ç¥ç»åœºå›å½’å™¨ä¸­çš„å›å½’ç›®æ ‡ï¼Œä»è€Œå°†ç¥ç»åœºæ–¹æ³•è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡è€Œéå›å½’ä»»åŠ¡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»åœºæ–¹æ³•æœ¬è´¨ä¸Šå¯ä»¥è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡ã€‚</li><li>ç¥ç»åœºåˆ†ç±»å™¨æ¡†æ¶é€šè¿‡ç›®æ ‡ç¼–ç æ¨¡å—å°†è¿ç»­å›å½’ç›®æ ‡ç¼–ç ä¸ºé«˜ç»´ç¦»æ•£ç¼–ç ã€‚</li><li>å°†å›å½’ä»»åŠ¡è½¬æ¢ä¸ºåˆ†ç±»ä»»åŠ¡ä¸ä¼šå¢åŠ æ˜¾è‘—çš„è®¡ç®—æˆæœ¬ã€‚</li><li>ç¥ç»åœºåˆ†ç±»å™¨åœ¨ç¨€ç–è¾“å…¥ã€æŸåå›¾åƒå’ŒåŠ¨æ€åœºæ™¯ä¸‹è¡¨ç°å‡ºé²æ£’æ€§ã€‚</li><li>ç¥ç»åœºåˆ†ç±»å™¨æ¯”ç¥ç»åœºå›å½’å™¨æ›´æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åº”ç”¨äºç°æœ‰ç¥ç»åœºæ–¹æ³•ã€‚</li><li>ç¥ç»åœºåˆ†ç±»å™¨æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥ç†è§£å’Œè®¾è®¡ç¥ç»åœºæ–¹æ³•ã€‚</li><li>æœ¬ç ”ç©¶ä¸ºç¥ç»åœºæ–¹æ³•çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>æ ‡é¢˜ï¼šNeural Field åˆ†ç±»å™¨ï¼šç›®æ ‡ç¼–ç å’Œåˆ†ç±»æŸå¤±</li><li>ä½œè€…ï¼šXindi Yangã€Zeke Xieã€Xiong Zhouã€Boyu Liuã€Buhua Liuã€Yi Liuã€Haoran Wangã€Yunfeng Caiã€Mingming Sun</li><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬äº¤é€šå¤§å­¦äº¤é€šæ•°æ®åˆ†æä¸æŒ–æ˜é‡ç‚¹å®éªŒå®¤</li><li>å…³é”®è¯ï¼šç¥ç»åœºã€ç›®æ ‡ç¼–ç ã€åˆ†ç±»æŸå¤±ã€ç¥ç»è¾å°„åœº</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.01058</li><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šç¥ç»åœºæ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦ä¸­å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼ŒåŒ…æ‹¬æ–°è§†å›¾åˆæˆå’Œå‡ ä½•é‡å»ºã€‚ç°æœ‰ç¥ç»åœºæ–¹æ³•å°è¯•é¢„æµ‹ä¸€äº›åŸºäºåæ ‡çš„è¿ç»­ç›®æ ‡å€¼ï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœº (NeRF) ä¸­çš„ RGBï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æ˜¯å›å½’æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸€äº›å›å½’æŸå¤±è¿›è¡Œä¼˜åŒ–ã€‚(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šå›å½’æ¨¡å‹æ˜¯å¦çœŸçš„ä¼˜äºç¥ç»åœºæ–¹æ³•çš„åˆ†ç±»æ¨¡å‹ï¼Ÿæœ¬æ–‡ä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¢è®¨äº†ç¥ç»åœºè¿™ä¸ªéå¸¸åŸºæœ¬ä½†è¢«å¿½è§†çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç¥ç»åœºåˆ†ç±»å™¨ (NFC) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç°æœ‰ç¥ç»åœºæ–¹æ³•è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡è€Œä¸æ˜¯å›å½’ä»»åŠ¡ã€‚æå‡ºçš„ NFC å¯ä»¥é€šè¿‡ä½¿ç”¨æ–°é¢–çš„ç›®æ ‡ç¼–ç æ¨¡å—å¹¶å°†åˆ†ç±»æŸå¤±æœ€å°åŒ–ï¼Œè½»æ¾åœ°å°†ä»»æ„ç¥ç»åœºå›å½’å™¨ (NFR) è½¬æ¢ä¸ºå…¶åˆ†ç±»å˜ä½“ã€‚é€šè¿‡å°†è¿ç»­å›å½’ç›®æ ‡ç¼–ç ä¸ºé«˜ç»´ç¦»æ•£ç¼–ç ï¼Œè‡ªç„¶åœ°åˆ¶å®šäº†ä¸€ä¸ªå¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒNFC åœ¨å‡ ä¹æ²¡æœ‰é¢å¤–è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒNFC è¿˜æ˜¾ç¤ºäº†å¯¹ç¨€ç–è¾“å…¥ã€æŸåå›¾åƒå’ŒåŠ¨æ€åœºæ™¯çš„é²æ£’æ€§ã€‚(4) æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸Šå–å¾—äº†ä»¥ä¸‹æ€§èƒ½ï¼š</li><li>æ–°è§†å›¾åˆæˆï¼šåœ¨ NeRF æ•°æ®é›†ä¸Šï¼ŒNFC åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¼˜äº NeRFã€‚</li><li>è¡¨é¢é‡å»ºï¼šåœ¨ ShapeNet æ•°æ®é›†ä¸Šï¼ŒNFC åœ¨ Chamfer è·ç¦»å’Œæ³•å‘é‡ä¸€è‡´æ€§æ–¹é¢ä¼˜äº NeRFã€‚</li><li><p>é²æ£’æ€§ï¼šNFC å¯¹ç¨€ç–è¾“å…¥ã€æŸåå›¾åƒå’ŒåŠ¨æ€åœºæ™¯è¡¨ç°å‡ºé²æ£’æ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šç›®æ ‡ç¼–ç æ¨¡å—ï¼Œå°†è¿ç»­å›å½’ç›®æ ‡ç¼–ç ä¸ºé«˜ç»´ç¦»æ•£ç¼–ç ï¼›ï¼ˆ2ï¼‰ï¼šåˆ†ç±»æŸå¤±ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼›ï¼ˆ3ï¼‰ï¼šäºŒè¿›åˆ¶æ•°ç›®æ ‡ç¼–ç ï¼Œå°†é¢œè‰²å€¼ç¼–ç ä¸º 8 ä½äºŒè¿›åˆ¶æ•°ï¼›ï¼ˆ4ï¼‰ï¼šé€ä½åˆ†ç±»æŸå¤±ï¼Œå¯¹æ¯ä¸ªäºŒè¿›åˆ¶ä½è®¡ç®—åˆ†ç±»æŸå¤±ï¼Œæƒé‡éšä½å€¼å¢åŠ è€Œå¢åŠ ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæ¢è®¨äº†ç¥ç»åœºæ–¹æ³•ä¸­ä¸€ä¸ªéå¸¸åŸºæœ¬ä½†è¢«å¿½è§†çš„é—®é¢˜ï¼šå›å½’ä¸åˆ†ç±»ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°é¢–çš„ç¥ç»åœºåˆ†ç±»å™¨ï¼ˆNFCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†ç°æœ‰çš„ç¥ç»åœºæ–¹æ³•è¡¨è¿°ä¸ºåˆ†ç±»æ¨¡å‹ï¼Œè€Œä¸æ˜¯å›å½’æ¨¡å‹ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œç›®æ ‡ç¼–ç å’Œåˆ†ç±»æŸå¤±å¯ä»¥æ˜¾ç€æé«˜å¤§å¤šæ•°ç°æœ‰ç¥ç»åœºæ–¹æ³•åœ¨æ–°è§†å›¾åˆæˆå’Œå‡ ä½•é‡å»ºä¸­çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒNFC çš„æ”¹è¿›å¯¹ç¨€ç–è¾“å…¥ã€å›¾åƒå™ªå£°å’ŒåŠ¨æ€åœºæ™¯å…·æœ‰é²æ£’æ€§ã€‚è™½ç„¶æˆ‘ä»¬çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ 3D è§†è§‰å’Œé‡å»ºä¸Šï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡ NFC æ˜¯ä¸€ä¸ªé€šç”¨çš„ç¥ç»åœºæ¡†æ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡æ¢ç´¢å’Œå¢å¼ºç¥ç»åœºçš„æ³›åŒ–æ€§å°†éå¸¸æœ‰å‰æ™¯ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p></li><li>æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»åœºåˆ†ç±»å™¨ï¼ˆNFCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç°æœ‰ç¥ç»åœºæ–¹æ³•è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œè€Œä¸æ˜¯å›å½’ä»»åŠ¡ã€‚</li><li>è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç›®æ ‡ç¼–ç æ¨¡å—ï¼Œå°†è¿ç»­å›å½’ç›®æ ‡ç¼–ç ä¸ºé«˜ç»´ç¦»æ•£ç¼–ç ã€‚</li><li>ä½¿ç”¨äº¤å‰ç†µæŸå¤±ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€ä½åˆ†ç±»æŸå¤±ï¼Œå¯¹æ¯ä¸ªäºŒè¿›åˆ¶ä½è®¡ç®—åˆ†ç±»æŸå¤±ï¼Œæƒé‡éšä½å€¼å¢åŠ è€Œå¢åŠ ã€‚æ€§èƒ½ï¼š</li><li>åœ¨æ–°è§†å›¾åˆæˆä»»åŠ¡ä¸Šï¼Œåœ¨ NeRF æ•°æ®é›†ä¸Šï¼ŒNFC åœ¨ PSNR å’Œ SSIM æŒ‡æ ‡ä¸Šä¼˜äº NeRFã€‚</li><li>åœ¨è¡¨é¢é‡å»ºä»»åŠ¡ä¸Šï¼Œåœ¨ ShapeNet æ•°æ®é›†ä¸Šï¼ŒNFC åœ¨ Chamfer è·ç¦»å’Œæ³•å‘é‡ä¸€è‡´æ€§æ–¹é¢ä¼˜äº NeRFã€‚</li><li>NFC å¯¹ç¨€ç–è¾“å…¥ã€æŸåå›¾åƒå’ŒåŠ¨æ€åœºæ™¯è¡¨ç°å‡ºé²æ£’æ€§ã€‚å·¥ä½œé‡ï¼š</li><li>NFC å¯ä»¥è½»æ¾åœ°å°†ä»»æ„ç¥ç»åœºå›å½’å™¨ (NFR) è½¬æ¢ä¸ºå…¶åˆ†ç±»å˜ä½“ï¼Œå‡ ä¹æ²¡æœ‰é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li><li>ç›®æ ‡ç¼–ç æ¨¡å—å’Œåˆ†ç±»æŸå¤±çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰çš„ç¥ç»åœºæ–¹æ³•ä¸­ã€‚</li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-33d7ddc258be3cc2226509c273b4d9b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d935134ee8dff34576f093f0e4bd187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e56f20cd07e166f0199df0193f095f54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa381fc61520f7cb599b68ee654d61b5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-09  DART Implicit Doppler Tomography for Radar Novel View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/</id>
    <published>2024-03-09T10:24:05.000Z</published>
    <updated>2024-03-09T10:24:05.771Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-03-09-æ›´æ–°"><a href="#2024-03-09-æ›´æ–°" class="headerlink" title="2024-03-09 æ›´æ–°"></a>2024-03-09 æ›´æ–°</h1><h2 id="3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos"><a href="#3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos" class="headerlink" title="3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos"></a>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</h2><p><strong>Authors:Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</strong></p><p>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\â€ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.01444v2">PDF</a> CVPR 2024 Accepted. Project Page: <a href="https://sjojok.github.io/3dgstream">https://sjojok.github.io/3dgstream</a></p><p><strong>Summary</strong><br>åŠ¨æ€åœºæ™¯å®æ—¶è‡ªç”±è§†ç‚¹è§†é¢‘æµæ–¹æ³•3DGStreamï¼Œåˆ©ç”¨3Dé«˜æ–¯åˆ†å¸ƒè¡¨ç¤ºåœºæ™¯ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå˜æ¢ç¼“å­˜å»ºæ¨¡3Dé«˜æ–¯åˆ†å¸ƒçš„å¹³ç§»å’Œæ—‹è½¬ï¼Œå®ç°æ¯å¸§12ç§’å†…é‡å»ºå’Œ200FPSå®æ—¶æ¸²æŸ“ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º3DGStreamæ–¹æ³•ï¼Œå®ç°åŠ¨æ€åœºæ™¯çš„å®æ—¶è‡ªç”±è§†ç‚¹è§†é¢‘æµã€‚</li><li>åˆ©ç”¨3Dé«˜æ–¯åˆ†å¸ƒè¡¨ç¤ºåœºæ™¯ï¼Œæœ‰æ•ˆæ•æ‰åœºæ™¯ç»“æ„ã€‚</li><li>ä½¿ç”¨ç¥ç»ç½‘ç»œå˜æ¢ç¼“å­˜å»ºæ¨¡3Dé«˜æ–¯åˆ†å¸ƒçš„å¹³ç§»å’Œæ—‹è½¬ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´å’Œå­˜å‚¨éœ€æ±‚ã€‚</li><li>æå‡ºè‡ªé€‚åº”3Dé«˜æ–¯åˆ†å¸ƒæ·»åŠ ç­–ç•¥ï¼Œå¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ–°å¢å¯¹è±¡ã€‚</li><li>3DGStreamåœ¨æ¸²æŸ“é€Ÿåº¦ã€å›¾åƒè´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œæ¨¡å‹å­˜å‚¨æ–¹é¢è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li><li>æ¯å¸§é‡å»ºæ—¶é—´12ç§’å†…ï¼Œå®æ—¶æ¸²æŸ“é€Ÿåº¦200FPSã€‚</li><li>æ¨¡å‹å­˜å‚¨ç©ºé—´å°ï¼Œæœ‰æ•ˆé™ä½è®¡ç®—æˆæœ¬ã€‚</li><li>é€‚ç”¨äºåŠ¨æ€åœºæ™¯çš„å®æ—¶è‡ªç”±è§†ç‚¹è§†é¢‘æµï¼Œæ‹“å±•3Dè§†è§‰åº”ç”¨é¢†åŸŸã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>é¢˜ç›®ï¼š3DGStreamï¼šåŠ¨æ€åœºæ™¯é«˜æ•ˆæµå¼ä¼ è¾“çš„ 3D é«˜æ–¯å®æ—¶è®­ç»ƒ</li><li>ä½œè€…ï¼šYuxuan Zhang, Lingjie Liu, Wenbo Bao, Wenxiu Sun, Qionghai Dai</li><li>å•ä½ï¼šåŒ—äº¬ç†å·¥å¤§å­¦</li><li>å…³é”®è¯ï¼šFree-Viewpoint Videoã€åŠ¨æ€åœºæ™¯ã€æµå¼ä¼ è¾“ã€3D é«˜æ–¯</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2209.04734.pdfGithub ä»£ç é“¾æ¥ï¼šNone</li><li>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ„å»ºåŠ¨æ€åœºæ™¯çš„é€¼çœŸè‡ªç”±è§†ç‚¹è§†é¢‘ï¼ˆFVVï¼‰ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡å½“å‰çš„ç¥ç»æ¸²æŸ“æŠ€æœ¯å–å¾—äº†æ˜¾ç€è¿›æ­¥ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å®Œæ•´çš„è§†é¢‘åºåˆ—è¿›è¡Œç¦»çº¿è®­ç»ƒï¼Œå¹¶ä¸”æ— æ³•è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼š</li><li>ç¦»çº¿è®­ç»ƒï¼šéœ€è¦å®Œæ•´çš„è§†é¢‘åºåˆ—ï¼Œæ— æ³•å®æ—¶æ¸²æŸ“ã€‚</li><li>å­˜å‚¨å¼€é”€ï¼šéœ€è¦ä¸ºæ¯ä¸ª FVV å¸§å­˜å‚¨å¤§é‡æ•°æ®ã€‚</li><li>è®­ç»ƒæ—¶é—´ï¼šè®­ç»ƒè¿‡ç¨‹è€—æ—¶ã€‚</li><li>æ— æ³•å¤„ç†åŠ¨æ€åœºæ™¯ä¸­å‡ºç°çš„ç‰©ä½“ã€‚ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼š</li><li>3D é«˜æ–¯è¡¨ç¤ºï¼šä½¿ç”¨ 3D é«˜æ–¯è¡¨ç¤ºåœºæ™¯ã€‚</li><li>ç¥ç»è½¬æ¢ç¼“å­˜ï¼ˆNTCï¼‰ï¼šä½¿ç”¨ NTC å¯¹ 3D é«˜æ–¯çš„å¹³ç§»å’Œæ—‹è½¬è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´å’Œå­˜å‚¨éœ€æ±‚ã€‚</li><li>è‡ªé€‚åº” 3D é«˜æ–¯æ·»åŠ ç­–ç•¥ï¼šå¤„ç†åŠ¨æ€åœºæ™¯ä¸­å‡ºç°çš„ç‰©ä½“ã€‚ï¼ˆ4ï¼‰æ€§èƒ½ï¼š</li><li>æ¸²æŸ“é€Ÿåº¦ï¼šå®æ—¶æ¸²æŸ“ï¼Œè¾¾åˆ° 200FPSã€‚</li><li>å›¾åƒè´¨é‡ï¼šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>è®­ç»ƒæ—¶é—´ï¼šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ—¶é—´æ˜¾è‘—å‡å°‘ã€‚</li><li><p>æ¨¡å‹å­˜å‚¨ï¼šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ¨¡å‹å­˜å‚¨éœ€æ±‚æ˜¾è‘—å‡å°‘ã€‚</p></li><li><p>æ–¹æ³•ï¼š(1) ä½¿ç”¨3Dé«˜æ–¯è¡¨ç¤ºåœºæ™¯ï¼Œå°†åœºæ™¯è¡¨ç¤ºä¸ºä¸€ç³»åˆ—3Dé«˜æ–¯åˆ†å¸ƒçš„å åŠ ã€‚(2) ä½¿ç”¨ç¥ç»è½¬æ¢ç¼“å­˜ï¼ˆNTCï¼‰å¯¹3Dé«˜æ–¯çš„å¹³ç§»å’Œæ—‹è½¬è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´å’Œå­˜å‚¨éœ€æ±‚ã€‚(3) æå‡ºè‡ªé€‚åº”3Dé«˜æ–¯æ·»åŠ ç­–ç•¥ï¼Œå¤„ç†åŠ¨æ€åœºæ™¯ä¸­å‡ºç°çš„ç‰©ä½“ã€‚</p></li><li><p>ç»“è®ºï¼šï¼ˆ1ï¼‰ï¼šæå‡º 3DGStreamï¼Œä¸€ç§ç”¨äºé«˜æ•ˆè‡ªç”±è§†ç‚¹è§†é¢‘æµçš„é«˜æ•ˆ 3D é«˜æ–¯å®æ—¶è®­ç»ƒæ–¹æ³•ã€‚ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäº 3DG-Sï¼Œåˆ©ç”¨ç¥ç»è½¬æ¢ç¼“å­˜ï¼ˆNTCï¼‰æ•æ‰ç‰©ä½“è¿åŠ¨ï¼›æå‡ºè‡ªé€‚åº” 3DG æ·»åŠ ç­–ç•¥ï¼Œå‡†ç¡®å»ºæ¨¡åŠ¨æ€åœºæ™¯ä¸­å‡ºç°çš„ç‰©ä½“ã€‚æ€§èƒ½ï¼šå®ç°å³æ—¶è®­ç»ƒï¼ˆæ¯å¸§çº¦ 10 ç§’ï¼‰å’Œå®æ—¶æ¸²æŸ“ï¼ˆçº¦ 200FPSï¼‰ï¼Œåœ¨ç™¾ä¸‡åƒç´ åˆ†è¾¨ç‡ä¸‹å…·æœ‰é€‚åº¦çš„å­˜å‚¨éœ€æ±‚ã€‚å·¥ä½œé‡ï¼šä½¿ç”¨ 3DG-S çš„ä»£ç åº“å®ç° 3DGStreamï¼Œä½¿ç”¨ tiny-cuda-nn å®ç° NTCã€‚</p></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-56fa714ff2f8a27b5ea568d4ef616b5e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf2d0d9167fc721c8b229c0141471c56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5a6c132c8a153da0f9bad3e8ca7eabd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-752f81f447063ef3902e3a021755740e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4cd01032696c0735dbb058f523ca0022.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-053adecfa0f0d915b2350de6633e2581.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-09  3DGStream On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
