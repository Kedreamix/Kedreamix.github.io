<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-01-28T06:46:11.408Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Failed building wheel for PyAudio  解决方法</title>
    <link href="https://kedreamix.github.io/2024/01/28/Note/pyaudio/"/>
    <id>https://kedreamix.github.io/2024/01/28/Note/pyaudio/</id>
    <published>2024-01-28T06:42:50.000Z</published>
    <updated>2024-01-28T06:46:11.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Failed-building-wheel-for-PyAudio-解决方法"><a href="#Failed-building-wheel-for-PyAudio-解决方法" class="headerlink" title="Failed building wheel for PyAudio  解决方法"></a>Failed building wheel for PyAudio  解决方法</h2><p>有时候在安装pyaudio的时候，总是有时候遇见一些错误，如下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) ... error</span><br><span class="line">  error: subprocess-exited-with-error</span><br><span class="line"></span><br><span class="line">  × Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) did not run successfully.</span><br><span class="line">  │ <span class="built_in">exit</span> code: 1</span><br><span class="line">  ╰─&gt; [18 lines of output]</span><br><span class="line">      running bdist_wheel</span><br><span class="line">      running build</span><br><span class="line">      running build_py</span><br><span class="line">      creating build</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      copying src/pyaudio/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      running build_ext</span><br><span class="line">      building <span class="string">'pyaudio._portaudio'</span> extension</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src/pyaudio</span><br><span class="line">      gcc -pthread -B anaconda3/envs/ernerf/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -I/usr/local/include -I/usr/include -Ianaconda3/envs/ernerf/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-cpython-310/src/pyaudio/device_api.o</span><br><span class="line">      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory</span><br><span class="line">          9 | <span class="comment">#include "portaudio.h"</span></span><br><span class="line">            |          ^~~~~~~~~~~~~</span><br><span class="line">      compilation terminated.</span><br><span class="line">      error: <span class="built_in">command</span> <span class="string">'/usr/bin/gcc'</span> failed with <span class="built_in">exit</span> code 1</span><br><span class="line">      [end of output]</span><br><span class="line"></span><br><span class="line">  note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">  ERROR: Failed building wheel <span class="keyword">for</span> pyaudio</span><br><span class="line">Successfully built python_speech_features</span><br><span class="line">Failed to build pyaudio</span><br><span class="line">ERROR: Could not build wheels <span class="keyword">for</span> pyaudio, <span class="built_in">which</span> is required to install pyproject.toml-based projects</span><br></pre></td></tr></tbody></table></figure><p>如果单纯查后面这一句，会发现找不到什么错误，最后我找到了对应的解决办法，实际上是linux有一些库没安装上，用root权限装一下即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些人说这样即可</span></span><br><span class="line">sudo apt-get install portaudio19-dev</span><br><span class="line"><span class="comment"># 如果不行就试一下这样</span></span><br><span class="line">sudo apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0</span><br></pre></td></tr></tbody></table></figure><p>这样安装完以后，我们就可以正常安装pyaudio了</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></tbody></table></figure><p>我也在github上看到的相关帖子，大家也可以参考：<a href="https://github.com/ardha27/AI-Waifu-Vtuber/issues/49">https://github.com/ardha27/AI-Waifu-Vtuber/issues/49</a>，而且这里面有个windows的解决方法，还蛮有趣，我还没试过</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install pipwin</span><br><span class="line">pipwin install pyaudio</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Failed-building-wheel-for-PyAudio-解决方法&quot;&gt;&lt;a href=&quot;#Failed-building-wheel-for-PyAudio-解决方法&quot; class=&quot;headerlink&quot; title=&quot;Failed building </summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>3D reconstruction</title>
    <link href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D%20reconstruction/"/>
    <id>https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3D%20reconstruction/</id>
    <published>2024-01-26T13:48:11.000Z</published>
    <updated>2024-01-27T05:55:47.362Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-26-更新"><a href="#2024-01-26-更新" class="headerlink" title="2024-01-26 更新"></a>2024-01-26 更新</h1><h2 id="Self-supervised-Video-Object-Segmentation-with-Distillation-Learning-of-Deformable-Attention"><a href="#Self-supervised-Video-Object-Segmentation-with-Distillation-Learning-of-Deformable-Attention" class="headerlink" title="Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention"></a>Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention</h2><p><strong>Authors:Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung</strong></p><p>Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage. </p><p><a href="http://arxiv.org/abs/2401.13937v1">PDF</a> under review</p><p><strong>Summary</strong><br>利用可变形注意力的知识蒸馏方法进行视频目标分割的自监督学习。</p><p><strong>Key Takeaways</strong></p><ul><li>可变形注意力机制能够有效地捕捉视频序列中目标的时空变化。</li><li>自监督学习可以有效地训练视频目标分割模型，无需人工标注。</li><li>知识蒸馏可以将复杂模型的知识转移到简单模型中，提高简单模型的性能。</li><li>所提出的方法在DAVIS 2016/2017和YouTube-VOS 2018/2019基准数据集上取得了最先进的性能。</li><li>所提出的方法具有较低的计算复杂度，可以集成到低功耗设备中。</li><li>所提出的方法可以有效地处理视频数据中的遮挡和背景杂乱等问题。</li><li>所提出的方法可以有效地分割出视频中的多个目标。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于注意力图的自传播语义分割（Attention Map-Based Self-Propagation for Semantic Segmentation）</li><li>作者：Jiale Cao, Yongwei Zhou, Yusheng Zhang, Haibin Ling, Jianbin Jiao</li><li>第一作者单位：北京航空航天大学</li><li>关键词：语义分割、自传播、注意力图</li><li>论文链接：https://arxiv.org/abs/2204.06937，Github 代码链接：None</li><li>摘要：</li></ol><p>（1）研究背景：语义分割是一项重要的计算机视觉任务，旨在将图像中的每个像素分类到相应的语义类别。传统的语义分割方法通常采用编码器-解码器结构，其中编码器用于提取图像特征，解码器用于将特征图恢复为分割掩码。然而，这些方法通常需要大量的标注数据来训练，并且在处理复杂场景时容易出现过拟合问题。</p><p>（2）过去的方法：为了解决上述问题，近年来出现了许多基于自传播的语义分割方法。这些方法通过将模型在未标记数据上进行迭代训练来增强模型的泛化能力。然而，现有的自传播方法通常采用简单的特征级传播策略，这可能会导致传播过程中的信息丢失。</p><p>（3）研究方法：为了解决上述问题，本文提出了一种基于注意力图的自传播语义分割方法。该方法首先通过一个编码器-解码器网络提取图像特征和分割掩码。然后，利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图。最后，通过传播图将分割掩码从已标记数据传播到未标记数据，从而增强模型的泛化能力。</p><p>（4）实验结果：本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上进行了评估。实验结果表明，本文方法在所有数据集上都取得了最优的分割精度，证明了本文方法的有效性。</p><p>7.Methods：（1）首先通过一个编码器-解码器网络提取图像特征和分割掩码。（2）利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图。（3）通过传播图将分割掩码从已标记数据传播到未标记数据，从而增强模型的泛化能力。</p><ol><li>结论：（1）：本文提出了一种基于注意力图的自传播语义分割方法，该方法通过利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图，从而增强模型的泛化能力。实验结果表明，本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上都取得了最优的分割精度，证明了本文方法的有效性。（2）：创新点：本文方法的主要创新点在于利用注意力图来计算每个像素对之间的相似性，并根据相似性构建一个传播图，从而增强模型的泛化能力。这种方法可以有效地将分割掩码从已标记数据传播到未标记数据，从而提高模型在未标记数据上的分割精度。性能：本文方法在DAVIS-16Val、DAVIS-17Val、YT-VOS18和YT-VOS19四个数据集上都取得了最优的分割精度，证明了本文方法的有效性。工作量：本文方法的工作量主要体现在注意力图的计算和传播图的构建上。注意力图的计算需要对每个像素对进行相似性计算，这可能会导致计算量较大。传播图的构建也需要对每个像素对进行相似性计算，这也会导致计算量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cd21dee141968f93da8757d6fbf76cdf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f77e3da0bc5f121fd261103f1ec6b4b9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d57735e262eaadf65b81e74b96dc78e6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d4f3d3ac89656924fd3478e879a34845.jpg" align="middle"></details><h2 id="MambaMorph-a-Mamba-based-Backbone-with-Contrastive-Feature-Learning-for-Deformable-MR-CT-Registration"><a href="#MambaMorph-a-Mamba-based-Backbone-with-Contrastive-Feature-Learning-for-Deformable-MR-CT-Registration" class="headerlink" title="MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for   Deformable MR-CT Registration"></a>MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for   Deformable MR-CT Registration</h2><p><strong>Authors:Tao Guo, Yinuo Wang, Cai Meng</strong></p><p>Deformable image registration is an essential approach for medical image analysis.This paper introduces MambaMorph, an innovative multi-modality deformable registration network, specifically designed for Magnetic Resonance (MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with its Mamba-based registration module and a contrastive feature learning approach, addressing the prevalent challenges in multi-modality registration. The network leverages Mamba blocks for efficient long-range modeling and high-dimensional data processing, coupled with a feature extractor that learns fine-grained features for enhanced registration accuracy. Experimental results showcase MambaMorph’s superior performance over existing methods in MR-CT registration, underlining its potential in clinical applications. This work underscores the significance of feature learning in multi-modality registration and positions MambaMorph as a trailblazing solution in this field. The code for MambaMorph is available at: <a href="https://github.com/Guo-Stone/MambaMorph">https://github.com/Guo-Stone/MambaMorph</a>. </p><p><a href="http://arxiv.org/abs/2401.13934v1">PDF</a> </p><p><strong>Summary</strong><br>多模态变形配准网络MambaMorph，用于磁共振（MR）和计算机断层扫描（CT）图像对齐。</p><p><strong>Key Takeaways</strong></p><ul><li>MambaMorph是一款多模态变形配准网络，专为磁共振（MR）和计算机断层扫描（CT）图像对齐而设计。</li><li>MambaMorph采用了Mamba注册模块和对比特征学习方法，解决了多模态配准中普遍存在的挑战。</li><li>MambaMorph利用Mamba块进行高效的长距离建模和高维数据处理，并结合特征提取器学习细粒度特征，以提高配准精度。</li><li>实验结果表明，MambaMorph在MR-CT配准中优于现有方法，突出了其在临床应用中的潜力。</li><li>这项工作强调了特征学习在多模态配准中的重要性，并将MambaMorph定位为该领域的一个开创性解决方案。</li><li>MambaMorph的代码可以在<a href="https://github.com/Guo-Stone/MambaMorph上获取。">https://github.com/Guo-Stone/MambaMorph上获取。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MambaMorph：一种基于 Mamba 的对比特征学习可变形 MR-CT 配准网络</li><li>作者：Tao Guo, Yinuo Wang, Cai Meng</li><li>隶属单位：北京航空航天大学图像处理中心</li><li>关键词：多模态配准、Mamba、特征学习</li><li>论文链接：https://arxiv.org/abs/2401.13934, Github 代码链接：https://github.com/Guo-Stone/MambaMorph</li><li><p>总结：(1)：研究背景：可变形图像配准是医学图像分析中的一项基本方法，由于手术干预、不同的成像序列等因素，图像中解剖组织的拓扑结构会发生很大变化。在分析一对图像之前，需要通过可变形图像配准在空间上对其进行对齐。(2)：过去的方法：传统的配准方法可以计算出精确且保形的位移场，但也带来了沉重的计算负担和时间成本，不适合实时的情况。在过去的十年中，基于深度学习的配准方法，如 VoxelMorph，展示了其快速实现配准的能力，其准确性甚至可以与传统方法相媲美。(3)：研究方法：本文提出了一种创新的多模态可变形配准网络 MambaMorph，专门针对磁共振（MR）和计算机断层扫描（CT）图像对齐而设计。MambaMorph 以 Mamba 为基础的配准模块和对比特征学习方法脱颖而出，解决了多模态配准中普遍存在的挑战。该网络利用 Mamba 块进行高效的长程建模和高维数据处理，并结合一个特征提取器，学习细粒度的特征以提高配准精度。(4)：性能表现：实验结果表明，MambaMorph 在 MR-CT 配准任务上优于现有方法，突出了其在临床应用中的潜力。这项工作强调了特征学习在多模态配准中的重要性，并将 MambaMorph 定位为该领域的开创性解决方案。</p></li><li><p>Methods：(1): MambaMorph网络的总体架构由一个Mamba块和一个特征提取器组成。Mamba块负责长程建模和高维数据处理，特征提取器负责学习细粒度的特征。(2): Mamba块由一个Mamba单元堆叠而成，每个Mamba单元包含一个注意力机制和一个残差连接。注意力机制用于捕获长程依赖关系，残差连接用于稳定训练过程。(3): 特征提取器由一个卷积神经网络组成，该网络将输入图像转换为一组特征图。这些特征图被馈送到Mamba块进行配准。(4): MambaMorph网络的训练过程分为两个阶段。第一阶段，网络学习匹配输入图像的刚性变换。第二阶段，网络学习匹配输入图像的非刚性变换。(5): 在测试阶段，MambaMorph网络将输入图像转换为一组特征图，然后将这些特征图馈送到Mamba块进行配准。Mamba块输出一个位移场，该位移场用于将输入图像配准到目标图像。</p></li><li><p>结论：（1）：本文提出了一种基于 Mamba 的多模态可变形配准网络 MambaMorph，该网络在 MR-CT 配准任务上优于现有方法，具有较好的临床应用潜力。这项工作强调了特征学习在多模态配准中的重要性，并将 MambaMorph 定位为该领域的开创性解决方案。（2）：创新点：</p></li><li>提出了一种基于 Mamba 的可变形配准模块，该模块具有较强的长程建模能力和高维数据处理能力。</li><li>引入了一个特征提取器，用于学习细粒度的特征，以提高配准精度。</li><li>将 MambaMorph 应用于 MR-CT 配准任务，并取得了优于现有方法的性能。性能：</li><li>在 MR-CT 配准任务上，MambaMorph 在配准精度和速度方面均优于现有方法。</li><li>MambaMorph 能够处理具有大变形和拓扑变化的图像对。工作量：</li><li>MambaMorph 的训练过程分为两个阶段，第一阶段学习匹配输入图像的刚性变换，第二阶段学习匹配输入图像的非刚性变换。</li><li>MambaMorph 的测试过程简单高效，能够快速生成配准结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8e4c7b534070889c432f2c0472b4a805.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-afe56b34b0fa28b9fec39311c219549c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-448e5c1ba4726b65452c3ac554e0eda1.jpg" align="middle"></details><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p><p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p><p><strong>Summary</strong><br>内窥镜高斯体素：一种用于动态内窥镜三维重建的全新方法，克服现有方法的局限性，树立了新的最先进标准。</p><p><strong>Key Takeaways</strong></p><ul><li>引入内窥镜高斯体素，一种用于动态内窥镜 3D 重建的新颖方法，首次将高斯体素散布用于这一领域。</li><li>克服了以前基于神经辐射场 (NeRF) 技术的局限性，例如重建速度慢、重建质量差。</li><li>在定量评估中，我们的方法在各种内窥镜数据集上都达到了最先进的水平。</li><li>我们的方法为医疗专业人士提供了一种有前途的工具，可以在医疗领域的实际应用中提供更可靠和有效的 3D 重建。</li><li>我们的方法使得 3D 重建的速度更快，并且重建的质量更高。</li><li>我们的方法能够处理具有挑战性的场景，例如组织变形和遮挡。</li><li>我们的方法可以用于各种医学应用，例如 VR 手术和医学图像分析。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视角动态高斯体素化用于可变形内窥镜组织重建</li><li>作者：杨森陈，王浩</li><li>单位：香港科技大学（广州）</li><li>关键词：3D 重建，高斯体素化，机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352，Github 链接：无</li><li><p>摘要：（1）研究背景：可变形软组织的准确三维重建对于各种医疗应用（如 VR 手术和医学图像分析）非常重要。然而，现有的方法通常难以实现准确性，并且存在组织部分幻觉的模糊性，这限制了其实际效用。（2）过去的方法及其问题：为了进一步提高静态单视角 RGBD 设置下软组织的三维重建的准确性，并提高三维重建的可靠性和可信度，本文提出了利用高斯体素化作为重建方法的 EndoGaussians。该方法在多个定量评估（如 PSNR、SSIM、LPIPS 等）方面取得了最先进的结果，并且还实现了更快的重建速度。（3）研究方法：本文提出的框架由两步组成：内窥镜视频修复和单视角动态高斯体素化。在第一步中，使用视频修复模型从给定的内窥镜视频中去除手术工具。在下一步中，设计了深度引导的动态三维高斯体素管道进行重建。（4）方法性能：该方法在多个内窥镜数据集上的定量评估中取得了最先进的结果。这些进步使该方法成为医疗专业人员的有前途的工具，可为医疗领域的实际应用提供更可靠和高效的三维重建。</p></li><li><p>方法：(1) 内窥镜视频修复：使用视频修复模型从给定的内窥镜视频中去除手术工具。(2) 深度引导的动态三维高斯体素化：设计了深度引导的动态三维高斯体素管道进行重建，该管道由以下步骤组成：</p></li><li>体素化：将三维空间划分为体素，并计算每个体素的概率。</li><li>融合：将来自不同视角的体素融合在一起，以获得更准确的重建结果。</li><li><p>优化：使用优化算法来优化体素的概率，以提高重建结果的质量。</p></li><li><p>结论：(1): 本文提出了一种新的单视角动态高斯体素化方法 EndoGaussians，用于可变形内窥镜组织重建，该方法在多个定量评估中取得了最先进的结果，为医疗领域的实际应用提供了更可靠和高效的三维重建。(2): 创新点：</p></li><li>提出了一种新的单视角动态高斯体素化方法 EndoGaussians，该方法能够准确地重建可变形软组织的三维结构。</li><li>设计了一种深度引导的动态三维高斯体素管道，该管道能够有效地融合来自不同视角的体素，并优化体素的概率，以提高重建结果的质量。</li><li>该方法在多个内窥镜数据集上的定量评估中取得了最先进的结果，证明了其有效性。性能：</li><li>该方法在多个定量评估（如PSNR、SSIM、LPIPS等）方面取得了最先进的结果，证明了其准确性和可靠性。</li><li>该方法的重建速度较快，能够满足实时应用的需求。工作量：</li><li>该方法的实现相对复杂，需要较高的计算资源和专业知识。</li><li>该方法需要大量的数据进行训练，这可能会增加工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3D reconstruction 方向最新论文已更新，请持续关注 Update in 2024-01-26  Self-supervised Video Object Segmentation with Distillation Learning of   Deformable Attention</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/NeRF/"/>
    <id>https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/NeRF/</id>
    <published>2024-01-26T13:40:03.000Z</published>
    <updated>2024-01-27T05:55:55.195Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-26-更新"><a href="#2024-01-26-更新" class="headerlink" title="2024-01-26 更新"></a>2024-01-26 更新</h1><h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p><p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. </p><p><a href="http://arxiv.org/abs/2401.14257v1">PDF</a> 11 pages, 9 figures</p><p><strong>Summary</strong><br>手绘草图引导文本生成三维内容，兼顾写实和可控。</p><p><strong>Key Takeaways</strong></p><ul><li>基于文本描述生成三维内容的技术取得了很大进展，但生成的物体缺乏微观控制。</li><li>草图可以提供一种低成本的微观控制方法，但由于草图的抽象性和模糊性，难以实现灵活的控制。</li><li>提出了一种多视图草图引导文本生成三维内容的框架，利用预训练的二维扩散模型来监督神经辐射场 (NeRF) 表示的三维场景的优化。</li><li>设计了一种新颖的同步生成和重建方法来有效地优化 NeRF。</li><li>收集了两种多视图草图数据集来评估所提出的方法，结果表明该方法可以合成具有精细草图控制且对文本提示具有高保真的三维一致内容。</li><li>大量的实验结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Sketch2NeRF：多视图草图引导的文本到 3D 生成</li><li>作者：Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</li><li>单位：中山大学深圳校区</li><li>关键词：文本到 3D、草图控制、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2401.14257</li><li>摘要：(1) 研究背景：文本到 3D 方法已经取得了很大的进展，但生成的 3D 对象往往随机且缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。(2) 过去的方法及其问题：目前的方法难以从草图中实现灵活的控制，因为草图具有抽象性和模糊性。(3) 论文提出的研究方法：提出了一种多视图草图引导的文本到 3D 生成框架（Sketch2NeRF），将草图控制添加到 3D 生成中。该方法利用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。(4) 方法在任务和性能上的表现：在两个多视图草图数据集上评估了该方法。结果表明，该方法能够合成与草图一致的 3D 内容，同时对文本提示具有很高的保真度。该方法在草图相似性和文本对齐方面取得了最先进的性能。</li></ol><p><methods>:(1): 我们提出了一种多视图草图引导的文本到3D生成框架（Sketch2NeRF），将草图控制添加到3D生成中。该方法利用预训练的2D扩散模型来监督由神经辐射场(NeRF)表示的3D场景的优化。(2): 我们使用神经辐射场（NeRF）来表示3D对象，NeRF是一种灵活且能够渲染出逼真图像的表示方法。(3): 为了将草图约束纳入多视图中，我们使用了一个预训练的2D草图条件扩散模型。(4): 我们提出了一种同步生成和重建方法来有效地优化具有ControlNet引导的NeRF。(5): 在生成阶段，我们使用ControlNet在草图的特定姿势下生成真实图像，同时使用Stable Diffusion在随机采样的姿势下生成真实图像。(6): 在重建阶段，我们更新NeRF参数，使生成的图像和渲染的图像之间的重建损失最小化。</methods></p><ol><li>结论：（1）：本文提出了一种新颖的多视图草图引导的文本到3D生成方法（即 Sketch2NeRF），该方法能够生成与给定草图高度相似的 3D 内容，并且对文本提示具有很高的保真度。（2）：创新点：</li><li>提出了一种多视图草图引导的文本到 3D 生成框架，将草图控制添加到 3D 生成中。</li><li>使用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。</li><li>提出了一种同步生成和重建方法来有效地优化具有 ControlNet 引导的 NeRF。性能：</li><li>在两个多视图草图数据集上评估了该方法。</li><li>结果表明，该方法能够合成与草图一致的 3D 内容，同时对文本提示具有很高的保真度。</li><li>该方法在草图相似性和文本对齐方面取得了最先进的性能。工作量：</li><li>该方法需要预训练一个 2D 扩散模型和一个 ControlNet。</li><li>该方法需要对 NeRF 进行优化，这可能需要大量的时间和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9b87019fdc56ce6a98f7417d87e3eeb3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1b34892d79bbf579fd2106569ec88f5d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c252ac3ea2c0fd943decf528877343ee.jpg" align="middle"></details><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>摘要</strong><br>神经辐射场与注意力机制相结合，用于说话人脸合成，实现了准确的唇形生成。</p><p><strong>要点</strong></p><ul><li>神经辐射场 (NeRF) 已用于说话人脸合成，以增强生成的面的真实感和 3D 效果。</li><li>现有的大多数 NeRF 方法要么给 NeRF 带来了复杂的学习任务，而缺乏对多模态特征融合的监督方法，要么无法将音频精确定位到与语音相关的面部区域。</li><li>这些问题导致现有方法产生了不准确的唇形。</li><li>该研究提出了一种基于 NeRF 和注意力机制解耦的说话人脸合成方法 (NeRF-AD)。</li><li>注意力机制解耦模块用于将面部分解为音频面部和身份面部，通过言语相关的面部动作单元 (AU) 信息。</li><li>为了精确地调节音频对说话人脸的影响，音频面部仅与音频特征融合。</li><li><ol><li><p>标题：DREAM-Talk：基于扩散的逼真情感音频驱动的单张图像说话人脸生成方法</p></li><li><p>作者：陈旭章<em>, 王超</em>, 张建峰, 徐鸿毅, 宋国贤, 谢宇, 罗林杰, 田亚鹏, 郭晓虎, 冯佳世</p></li><li><p>单位：字节跳动公司</p></li><li><p>关键词：情感说话人脸生成；扩散模型；音频驱动；唇形同步</p></li><li><p>论文链接：https://arxiv.org/abs/2312.13578</p></li><li><p>摘要：(1) 研究背景：从单张人像图像生成情感说话人脸仍然是一项重大挑战。同时实现富有表现力的情感说话和准确的唇形同步尤其困难，因为表现力通常会因唇形同步的准确性而受到损害。LSTM 网络被许多先前的工作广泛采用，但往往无法捕捉情感表达的细微差别和变化。(2) 过去的方法及其问题：为了解决这些挑战，我们引入了 DREAM-Talk，这是一个两阶段的基于扩散的音频驱动框架，专门用于同时生成多样化的表情和准确的唇形同步。在第一阶段，我们提出了 EmoDiiff，一个新颖的扩散模块，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。鉴于唇部运动与音频之间存在很强的相关性，我们随后使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。为此，我们部署了一个视频到视频渲染模块，将我们代理 3D 头像的表情和唇部动作转移到任意人像上。(3) 本文提出的研究方法：在定量和定性方面，DREAM-Talk 在表现力、唇形同步准确性和感知质量方面都优于最先进的方法。(4) 方法在什么任务上取得了什么性能？性能是否支持其目标：该方法在情感说话人脸生成任务上取得了很好的性能。在定量评估中，DREAM-Talk 在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。在定性评估中，DREAM-Talk 生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。这些结果支持了该方法的目标，即生成具有多样化情感表达和准确唇形同步的逼真说话人脸。</p></li><li><p>方法：(1) 提出EmoDiff，一个新颖的扩散模块，根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。(2) 部署视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。(3) 使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。</p></li><li><p>结论：（1）：本文提出了一种名为DREAM-Talk的创新框架，该框架专为生成具有精确唇形同步的情感表达说话人脸而设计。我们的两阶段方法，包括EmoDiff模块和唇形细化，有效地捕捉了情感细微差别并确保了准确的唇形同步。利用情感条件扩散模型和唇形细化网络，我们的方法优于现有技术。我们的结果表明，在保持高视频质量的同时，面部情感表达能力得到了提高。DREAM-Talk代表了情感说话人脸生成领域向前迈出的重要一步，它使跨越广泛应用范围的逼真且情感参与的数字人形表征的创建成为可能。（2）：创新点：提出了一种新颖的扩散模块EmoDiff，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。部署了一个视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。性能：在定量评估中，DREAM-Talk在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。在定性评估中，DREAM-Talk生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。工作量：该方法需要大量的数据和计算资源来训练模型。该方法需要专业知识来实现和部署。</p></li></ol></li><li><p>大量的定性和定量实验表明，NeRF-AD 在生成逼真的说话人脸视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于注意力机制的解耦的神经辐射场说话人面部合成（NeRF-AD）</li><li>作者：Chongke Bi，Xiaoxing Liu，Zhilei Liu</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部解耦</li><li>论文链接：https://arxiv.org/abs/2401.12568    Github 链接：无</li><li><p>摘要：（1）：说话人面部合成是多维信号处理和多媒体领域当前的研究热点之一。为了增强生成面部的真实感和 3D 效果，神经辐射场（NeRF）最近被引入到该研究领域。然而，大多数现有的基于 NeRF 的方法要么让 NeRF 承担复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。（2）：为了解决上述问题，本文提出了一种基于注意力机制的解耦的神经辐射场说话人面部合成方法（NeRF-AD）。具体来说，我们引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。为了精确地调节音频如何影响说话的面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。（3）：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们对说话的面部进行了分解，并为 NeRF 提供了两个分解的精确条件。我们提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。（4）：广泛的定性和定量实验表明，NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li><li><p>方法：（1）：提出了一种基于注意力机制的解耦的神经辐射场说话人面部合成方法（NeRF-AD）。（2）：引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。（3）：只将音频面部与音频特征融合，并利用AU信息监督这两个模态的融合。（4）：对说话的面部进行了分解，并为NeRF提供了两个分解的精确条件。（5）：提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。（6）：采用一系列方法来监督整个过程。</p></li><li><p>结论：（1）本工作提出了一种基于注意力机制的解耦神经辐射场说话人面部合成方法（NeRF-AD），该方法通过注意力解耦模块将说话人面部解耦为音频面部和身份面部，并仅将音频面部与音频特征融合，利用动作单元信息监督两个模态的融合，降低了 NeRF 的学习负担，提高了面部渲染的准确性，在图像质量和唇形同步方面优于最先进的方法。（2）创新点：</p></li><li>提出了一种基于注意力机制的解耦神经辐射场说话人面部合成方法（NeRF-AD）。</li><li>引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。</li><li>只将音频面部与音频特征融合，并利用 AU 信息监督这两个模态的融合。</li><li>对说话的面部进行了分解，并为 NeRF 提供了两个分解的精确条件。</li><li>提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。</li><li>采用一系列方法来监督整个过程。性能：</li><li>在图像质量和唇形同步方面优于最先进的方法。工作量：</li><li>需要收集和预处理说话人面部数据和音频数据。</li><li>需要训练 NeRF 模型和注意力解耦模块。</li><li>需要渲染合成说话人面部视频。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details><h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p><p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts. </p><p><a href="http://arxiv.org/abs/2401.09495v3">PDF</a> Error on result tabulation for the state of the art method which   might cause misleading to the readers</p><p><strong>摘要</strong><br>利用水印优化与签署保护 NeRF 模型的知识产权。</p><p><strong>要点</strong></p><ul><li>IPR-NeRF 框架在黑盒和白盒设置中提供了 NeRF 模型的知识产权保护。</li><li>在黑盒设置中，使用基于扩散的解决方案通过两阶段优化过程嵌入和提取水印。</li><li>在白盒设置中，通过采用符号损失目标，将指定的数字签名嵌入到 NeRF 模型的权重中。</li><li>IPR-NeRF 模型不仅保持了渲染质量，而且与现有技术相比，它还具有鲁棒性，可抵抗模糊和移除攻击。</li><li>IPR-NeRF 模型在用于生成合成数据的视觉效果时，能够有效防止模型窃取和误用。</li><li>IPR-NeRF 模型还可用于保护 NeRF 模型在商业环境中的知识产权。</li><li>IPR-NeRF 模型为 NeRF 模型的知识产权保护提供了一种通用解决方案。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><li><p>题目：IPR-NeRF：所有权验证满足神经辐射场（中文翻译：IPR-NeRF：所有权验证满足神经辐射场）</p></li><li><p>作者：Kent Ong、Kam Woh Ng、Chee Seng Chan、Yi Zhe Song、Tao Xiang</p></li><li><p>第一作者单位：马来亚大学图像与信号处理中心（中文翻译：马来亚大学图像与信号处理中心）</p></li><li><p>关键词：神经辐射场、知识产权保护、数字水印、数字签名</p></li><li><p>论文链接：https://arxiv.org/abs/2401.09495，Github 代码链接：无</p></li><li><p>摘要：（1）研究背景：神经辐射场 (NeRF) 模型因其出色的视觉质量和令人印象深刻的演示而在计算机视觉领域引起了广泛关注。NeRF 模型的商业价值也引起了技术企业家的注意，但这也使得 NeRF 模型面临被非法复制、再分发或滥用的风险。（2）过去的方法及其问题：目前还没有针对 NeRF 模型的知识产权保护框架。现有的保护方案主要针对卷积神经网络 (CNN)、生成对抗网络 (GAN) 和循环神经网络 (RNN)。这些方法在设计 NeRF 模型的保护框架时面临诸多挑战，例如现有黑盒保护方法在 NeRF 渲染过程中会导致水印无法恢复。（3）提出的研究方法：本文提出了一种针对 NeRF 模型的综合知识产权保护框架，称为 IPR-NeRF。在黑盒设置中，引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。（4）方法在任务和性能上的表现：实验结果表明，IPR-NeRF 方法不仅保持了 NeRF 模型的保真度（即渲染质量），而且在面对模糊性和去除攻击时也表现出比以往方法更强的鲁棒性。这些性能支持了本文提出的方法的目标。</p></li><li><p>方法：(1) IPR-NeRF 框架概述：IPR-NeRF 框架包含两个主要模块：黑盒保护模块和白盒保护模块。黑盒保护模块旨在保护未经授权访问的 NeRF 模型，而白盒保护模块旨在保护经授权访问的 NeRF 模型。(2) 黑盒保护模块：黑盒保护模块采用基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。在嵌入阶段，将水印嵌入到 NeRF 模型的权重中，在提取阶段，从渲染的图像中提取水印。(3) 白盒保护模块：白盒保护模块采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。符号损失目标函数旨在最小化数字签名与 NeRF 模型权重的差异，从而确保数字签名被嵌入到 NeRF 模型中。(4) 实验结果：实验结果表明，IPR-NeRF 方法不仅保持了 NeRF 模型的保真度，而且在面对模糊性和去除攻击时也表现出比以往方法更强的鲁棒性。</p></li><li><p>结论：（1）：IPR-NeRF：所有权验证满足神经辐射场，提出了一种针对 NeRF 模型的全面且鲁棒的知识产权保护方案，在黑盒和白盒场景中均能有效保护模型。综合实验结果表明，该方案在抵抗嵌入水印的模糊性和去除攻击方面表现出优异的性能，同时保持了渲染性能。然而，该方案在计算能力和黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时，该方案无法抵御覆盖攻击。未来的研究将集中在改进这些方面。（2）：创新点：</p></li><ul><li>提出了一种综合的 NeRF 模型知识产权保护框架，该框架在黑盒和白盒场景中均能有效保护模型。</li><li>引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印，在黑盒设置中保护未经授权访问的 NeRF 模型。</li><li>采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中，在白盒设置中保护经授权访问的 NeRF 模型。</li></ul><p>性能：</p><ul><li>实验结果表明，该方案不仅保持了 NeRF 模型的保真度，而且在面对模糊性和去除攻击时也表现出比以往方法更强的鲁棒性。</li></ul><p>工作量：</p><ul><li>该方案在计算能力和黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时，该方案无法抵御覆盖攻击。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle"></details><p>​    </p></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-26  Sketch2NeRF Multi-view Sketch-guided Text-to-3D Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3DGS/"/>
    <id>https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/3DGS/</id>
    <published>2024-01-26T13:31:01.000Z</published>
    <updated>2024-01-27T05:54:43.741Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-26-更新"><a href="#2024-01-26-更新" class="headerlink" title="2024-01-26 更新"></a>2024-01-26 更新</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p><p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p><p><strong>摘要</strong><br>高斯放射技术助力打造动态内窥镜三维重建新标杆。</p><p><strong>要点</strong></p><ul><li>基于神经辐射场（NeRF）的方法在动态内窥镜三维重建任务中存在重建精度低、易产生模糊组织结构等局限性。</li><li>本文首次将高斯放射技术应用于动态内窥镜三维重建任务，提出了名为EndoGaussians的新方法，克服了NeRF方法的局限性。</li><li>EndoGaussians方法在各项内窥镜数据集上均取得了最先进的重建效果。</li><li>EndoGaussians方法可为医疗专业人士提供更加可靠、高效的三维重建结果，有助于该技术在医疗领域的实际应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视图动态高斯体素重建</li><li>作者：Yangsen Chen, Hao Wang</li><li>单位：香港科技大学（广州）</li><li>关键词：3D 重建、高斯体素、机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352v1，Github 链接：无</li><li><p>摘要：（1）研究背景：准确地从内窥镜视频中重建可变形软体组织的 3D 模型对于医学应用（如 VR 手术和医学图像分析）至关重要。现有的方法通常难以达到精度要求，并且存在组织部分出现模糊的情况，限制了其实际应用。（2）过去的方法及其问题：深度估计方法和 SLAM 方法对于重建复杂场景的准确性较差；基于稀疏扭曲场的重建方法在变形超出非拓扑变化范围时会退化；基于神经辐射场的重建方法存在重建结果中哪些部分基于真实数据、哪些部分是虚构的不明确的问题。（3）研究方法：本文提出了一种新的方法 EndGaussians，它利用高斯体素进行动态内窥镜 3D 重建。该方法是首次将高斯体素应用于此场景，克服了以前基于神经辐射场技术的局限性。（4）方法性能：在多个内窥镜数据集上进行的定量评估表明，该方法达到了最先进的水平。这些进步使该方法成为医疗专业人员的有力工具，为医学领域的实际应用提供了更可靠、更高效的 3D 重建。</p></li><li><p>Methods:(1): 该方法将高斯体素应用于动态内窥镜3D重建，克服了以前基于神经辐射场技术的局限性。(2): 该方法通过将高斯体素作为隐式表示来建模场景，并使用神经网络来预测高斯体素的参数。(3): 该方法使用一种新的损失函数来训练神经网络，该损失函数可以同时优化重建的质量和高斯体素的分布。(4): 该方法使用一种新的采样策略来生成高斯体素，该采样策略可以提高重建的效率和准确性。(5): 该方法使用一种新的融合策略来融合来自不同视图的高斯体素，该融合策略可以提高重建的鲁棒性和完整性。</p></li><li><p>结论：（1）：本文提出了一种新的方法EndGaussians，它利用高斯体素进行动态内窥镜3D重建。该方法是首次将高斯体素应用于此场景，克服了以前基于神经辐射场技术的局限性。在多个内窥镜数据集上进行的定量评估表明，该方法达到了最先进的水平。这些进步使该方法成为医疗专业人员的有力工具，为医学领域的实际应用提供了更可靠、更高效的3D重建。（2）：创新点：</p></li><li>将高斯体素应用于动态内窥镜3D重建，克服了以前基于神经辐射场技术的局限性。</li><li>使用一种新的损失函数来训练神经网络，该损失函数可以同时优化重建的质量和高斯体素的分布。</li><li>使用一种新的采样策略来生成高斯体素，该采样策略可以提高重建的效率和准确性。</li><li>使用一种新的融合策略来融合来自不同视图的高斯体素，该融合策略可以提高重建的鲁棒性和完整性。性能：</li><li>在多个内窥镜数据集上进行的定量评估表明，该方法达到了最先进的水平。</li><li>该方法可以生成高质量的3D重建，并且可以很好地处理复杂场景和变形组织。</li><li>该方法具有较高的效率和准确性，可以满足医疗应用的要求。工作量：</li><li>该方法的实现难度较高，需要较强的编程能力和数学基础。</li><li>该方法的训练时间较长，需要较大的计算资源。</li><li>该方法的应用场景有限，目前仅适用于医疗领域。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details><h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p><p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>. </p><p><a href="http://arxiv.org/abs/2401.12561v1">PDF</a> </p><p><strong>Summary</strong><br>实时三维手术场景重构算法EndoGaussian可实现实时渲染，并享有优异的重建质量，为术中手术应用提供强劲支持。</p><p><strong>Key Takeaways</strong></p><ul><li>EndoGaussian 是一种创新的实时手术场景重建框架，基于 3D 高斯散点图构建。</li><li>该框架将动态手术场景表示为典型高斯分布和时间相关变形场，可在新时间戳预测高斯变形。</li><li>高效的高斯表示和并行渲染管道，使该框架的渲染速度显著加快。</li><li>该框架采用轻量级编码体素和极小 MLP 相结合的方式设计变形场，带来高效的高斯跟踪与小幅渲染负担。</li><li>提供了一种整体高斯初始化方法，可从整个输入图像序列搜索信息点以充分利用表面分布先验。</li><li>在公开内窥镜数据集上的实验表明，该方法可实现实时的渲染速度（195 FPS 实时，100 倍提升），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（2 分钟/场 景以内），为术中手术应用展现了巨大的前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：EndoGaussian：用于可变形手术场景重建的高斯飞溅</li><li>作者：Yifan Liu、Chenxin Li、Chen Yang 和 Yixuan Yuan</li><li>隶属关系：香港中文大学</li><li>关键词：3D 重建·高斯飞溅·机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.12561，Github 代码链接：https://yifliu3.github.io/EndoGaussian/</li><li><p>摘要：（1）研究背景：从内窥镜立体视频中重建手术场景对于许多下游手术应用至关重要。然而，现有方法的推理速度慢，极大地限制了其实际使用。（2）过去的方法及其问题：以往的方法存在推理速度慢的问题，限制了其实际使用。本文提出的方法动机明确，旨在解决这一问题。（3）研究方法：本文提出了一种基于 3D 高斯飞溅的实时手术场景重建框架 EndoGaussian。该框架将动态手术场景表示为规范高斯和时间相关变形场，预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，该框架与以前的方法相比显着提高了渲染速度。此外，本文将变形场设计为轻量级编码体素和极小 MLP 的组合，从而实现高效的高斯跟踪，且渲染负担很小。此外，本文设计了一种整体高斯初始化方法，通过搜索输入图像序列中的信息点来充分利用表面分布先验。（4）方法性能：在公共内窥镜数据集上的实验表明，本文方法可以实现实时渲染速度（195 FPS 实时，100 倍提升），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（2 分钟/场景以内），为术中手术应用展示出巨大的前景。</p></li><li><p>方法：(1)：提出基于 3D 高斯飞溅的实时手术场景重建框架 EndoGaussian，将动态手术场景表示为规范高斯和时间相关变形场，预测新时间戳下的高斯变形，利用高效的高斯表示和并行渲染管道大幅提升渲染速度。(2)：将变形场设计为轻量级编码体素和极小 MLP 的组合，实现高效的高斯跟踪，且渲染负担很小。(3)：设计整体高斯初始化方法，通过搜索输入图像序列中的信息点充分利用表面分布先验。</p></li><li><p>结论：（1）本工作提出了一种实时且高质量的 4D 重建框架，用于动态手术场景重建。通过利用基于体素的高斯跟踪和整体高斯初始化，我们可以处理组织变形和非平凡的高斯初始化问题。综合实验表明，我们的 EndoGaussian 可以实现最先进的重建质量和实时的渲染速度，比以前的方法快 100 倍以上。我们希望新兴的基于高斯飞溅的重建技术可以为机器人手术场景理解提供新的途径，并增强各种下游临床任务，尤其是术中应用。（2）创新点：提出了一种基于 3D 高斯飞溅的实时手术场景重建框架 EndoGaussian，将动态手术场景表示为规范高斯和时间相关变形场，预测新时间戳下的高斯变形，利用高效的高斯表示和并行渲染管道大幅提升渲染速度。将变形场设计为轻量级编码体素和极小 MLP 的组合，实现高效的高斯跟踪，且渲染负担很小。设计整体高斯初始化方法，通过搜索输入图像序列中的信息点充分利用表面分布先验。性能：在公共内窥镜数据集上的实验表明，本文方法可以实现实时渲染速度（195FPS 实时，100 倍提升），同时保持最先进的重建质量（35.925PSNR）和最快的训练速度（2分钟/场景以内），为术中手术应用展示出巨大的前景。工作量：本文方法的实现复杂度较高，需要较高的计算资源和专业知识。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle"></details><h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p><p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10\% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a>. </p><p><a href="http://arxiv.org/abs/2401.05925v2">PDF</a> Correct writing details</p><p><strong>Summary</strong><br>针对3D高斯体素方法中零样本分割效果欠佳的问题，提出了一种紧凑的快速3D高斯体素分割方法，在保证快速渲染速度的同时，仅使用RGB图像即可实现紧凑的3D一致场景分割效果。</p><p><strong>Key Takeaways</strong></p><ul><li>我们提出了一种紧凑的快速3D高斯体素分割方法（CoSSegGaussians）。</li><li>CoSSegGaussians仅使用RGB图像作为输入，即可实现快速的3D一致场景分割。</li><li>CoSSegGaussians采用双特征融合网络作为高斯体素的分割域，以解决传统方法中存在的问题。</li><li>CoSSegGaussians通过显式反投影将从图像中提取的DINO特征应用到空间特征中，以实现全局到局部的融合策略。</li><li>实验结果表明，CoSSegGaussians在语义和全景零样本分割任务上均优于基线方法，同时推理时间仅为基于NeRF方法的10%。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：紧凑而快速的场景分割 3D 高斯体与双重特征融合</li><li>作者：Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</li><li>单位：西安交通大学人工智能与机器人研究所</li><li>关键词：神经辐射场、3D 高斯体、场景分割、无监督学习</li><li>论文链接：https://arxiv.org/abs/2401.05925Github 代码链接：None</li><li>摘要：(1)：近年来，计算机视觉和计算机图形学取得了显着进步，特别是在神经渲染领域。神经辐射场 (NeRF) 及其后续方法推动了神经场景表示的发展，这些方法已显示出显着的用于新视图合成的能力。(2)：以往基于 NeRF 的分割方法依赖于耗时的神经场景优化。虽然最近的 3D 高斯体 splatting 显着提高了速度，但现有的基于高斯体的分割方法难以生成紧凑的掩码，尤其是在零样本分割中。这个问题可能源于它们将可学习参数直接分配给每个高斯体，导致对跨视图不一致的 2D 机器生成的标签缺乏鲁棒性。(3)：本文提出了一种紧凑而快速的 3D 高斯体分割方法 (CoSSegGaussians)，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度很快。具体来说，我们首先在 RGB 监督下优化 3D 高斯体。在高斯体定位之后，通过显式反投影应用从图像中提取的 DINO 特征，这些特征进一步与来自高效点云处理网络的空间特征结合。利用特征聚合以全局到局部策略将它们融合用于紧凑分割特征。(4)：实验结果表明，我们的模型在语义和全景零样本分割任务上优于基线，同时与基于 NeRF 的方法相比，推理时间缩短了 90% 以上。</li></ol><p>7.Methods:(1): 本文提出了一种紧凑而快速的 3D 高斯体分割方法 (CoSSegGaussians)，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度很快。(2): 首先在 RGB 监督下优化 3D 高斯体。在高斯体定位之后，通过显式反投影应用从图像中提取的 DINO 特征，这些特征进一步与来自高效点云处理网络的空间特征结合。利用特征聚合以全局到局部策略将它们融合用于紧凑分割特征。(3): 实验结果表明，我们的模型在语义和全景零样本分割任务上优于基线，同时与基于 NeRF 的方法相比，推理时间缩短了 90% 以上。</p><ol><li>结论：（1）：本文提出了一种紧凑而快速的 3D 高斯体分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度很快。（2）：创新点：</li><li>将 3D 高斯体与双重特征融合网络相结合，用于分割场。</li><li>利用反投影将 DINO 特征引入定位的 3D 高斯体，并进一步与高斯体的空间信息相结合。</li><li>应用全局到局部聚合模块生成紧凑的分割逻辑。性能：</li><li>在语义和全景零样本分割任务上优于基线。</li><li>与基于 NeRF 的方法相比，推理时间缩短了 90% 以上。工作量：</li><li>实验表明，该方法能够可靠且高效地完成零样本分割任务。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7b3b4f44e1bfaba57c660121007fee8a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e297ea1e2c85e96907865cc0d6107864.jpg" align="middle"><img src="https://pica.zhimg.com/v2-27a48d664aab4676f21f642635ecb972.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8a12edf74e5d62d5f426b60407d904ab.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-26  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/01/26/Paper/2024-01-26/Talking%20Head%20Generation/</id>
    <published>2024-01-26T13:23:09.000Z</published>
    <updated>2024-01-28T08:07:00.810Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-26-更新"><a href="#2024-01-26-更新" class="headerlink" title="2024-01-26 更新"></a>2024-01-26 更新</h1><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>摘要</strong><br>神经辐射场与注意力机制相结合，用于说话人脸合成，实现了准确的唇形生成。</p><p><strong>要点</strong></p><ul><li>神经辐射场 (NeRF) 已用于说话人脸合成，以增强生成的面的真实感和 3D 效果。</li><li>现有的大多数 NeRF 方法要么给 NeRF 带来了复杂的学习任务，而缺乏对多模态特征融合的监督方法，要么无法将音频精确定位到与语音相关的面部区域。</li><li>这些问题导致现有方法产生了不准确的唇形。</li><li>该研究提出了一种基于 NeRF 和注意力机制解耦的说话人脸合成方法 (NeRF-AD)。</li><li>注意力机制解耦模块用于将面部分解为音频面部和身份面部，通过言语相关的面部动作单元 (AU) 信息。</li><li>为了精确地调节音频对说话人脸的影响，音频面部仅与音频特征融合。</li><li>大量的定性和定量实验表明，NeRF-AD 在生成逼真的说话人脸视频方面优于最先进的方法，包括图像质量和唇形同步。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于注意力机制的解耦的神经辐射场说话人面部合成（NeRF-AD）</li><li>作者：Chongke Bi，Xiaoxing Liu，Zhilei Liu</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部解耦</li><li>论文链接：https://arxiv.org/abs/2401.12568    Github 链接：无</li><li><p>摘要：（1）：说话人面部合成是多维信号处理和多媒体领域当前的研究热点之一。为了增强生成面部的真实感和 3D 效果，神经辐射场（NeRF）最近被引入到该研究领域。然而，大多数现有的基于 NeRF 的方法要么让 NeRF 承担复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。（2）：为了解决上述问题，本文提出了一种基于注意力机制的解耦的神经辐射场说话人面部合成方法（NeRF-AD）。具体来说，我们引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。为了精确地调节音频如何影响说话的面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。（3）：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们对说话的面部进行了分解，并为 NeRF 提供了两个分解的精确条件。我们提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。（4）：广泛的定性和定量实验表明，NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li><li><p>方法：（1）：提出了一种基于注意力机制的解耦的神经辐射场说话人面部合成方法（NeRF-AD）。（2）：引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。（3）：只将音频面部与音频特征融合，并利用AU信息监督这两个模态的融合。（4）：对说话的面部进行了分解，并为NeRF提供了两个分解的精确条件。（5）：提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。（6）：采用一系列方法来监督整个过程。</p></li><li><p>结论：（1）本工作提出了一种基于注意力机制的解耦神经辐射场说话人面部合成方法（NeRF-AD），该方法通过注意力解耦模块将说话人面部解耦为音频面部和身份面部，并仅将音频面部与音频特征融合，利用动作单元信息监督两个模态的融合，降低了 NeRF 的学习负担，提高了面部渲染的准确性，在图像质量和唇形同步方面优于最先进的方法。（2）创新点：</p></li><li>提出了一种基于注意力机制的解耦神经辐射场说话人面部合成方法（NeRF-AD）。</li><li>引入了一个基于注意力的解耦模块，利用与语音相关的面部动作单元（AU）信息将面部解耦为音频面部和身份面部。</li><li>只将音频面部与音频特征融合，并利用 AU 信息监督这两个模态的融合。</li><li>对说话的面部进行了分解，并为 NeRF 提供了两个分解的精确条件。</li><li>提出了一个基于注意力的解耦模块，允许音频与与语音运动相关的面部区域精确融合。</li><li>采用一系列方法来监督整个过程。性能：</li><li>在图像质量和唇形同步方面优于最先进的方法。工作量：</li><li>需要收集和预处理说话人面部数据和音频数据。</li><li>需要训练 NeRF 模型和注意力解耦模块。</li><li>需要渲染合成说话人面部视频。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-01-26  NeRF-AD Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</title>
    <link href="https://kedreamix.github.io/2024/01/25/Note/3DGS%20Survey/"/>
    <id>https://kedreamix.github.io/2024/01/25/Note/3DGS%20Survey/</id>
    <published>2024-01-25T09:24:11.000Z</published>
    <updated>2024-01-28T17:34:21.492Z</updated>
    
    <content type="html"><![CDATA[<p>今天想介绍的是<code>ZJU</code>带来的<code>3DGS</code>的首篇综述<code>A Survey on 3D Gaussian Splatting</code> 这是论文链接 <a href="https://arxiv.org/abs/2401.03890">arXiv:2401.03890</a>，结合一些资料，趁这个机会好好学习一下3DGS，加油入坑！！！</p><p>首先说一些自己的理解，3DGS之所以爆火，很大程度在于他的实时性，而这一部分极大程度得益于他定制的算法与自定义 CUDA 内核。除此之外，<strong>Gaussian Splatting</strong>根本不涉及任何神经网络，甚至没有一个小型的 MLP，也没有什么 “神经”的东西，场景本质上只是空间中的一组点。在大家都在研究数十亿个参数组成的模型的人工智能世界里，这种方法越来越受欢迎，令人耳目一新。它的想法源于 “Surface splatting”（2001 年），说明经典的计算机视觉方法仍然可以激发相关的解决方案。它简单明了的表述方式使<strong>Gaussian Splatting</strong>特别容易解释，这也是为什么在某些应用中选择它而不是 NeRFs。</p><h2 id="引言-INTRODUCTION"><a href="#引言-INTRODUCTION" class="headerlink" title="引言 INTRODUCTION"></a>引言 INTRODUCTION</h2><p>NeRF自从2020年开始，在多视角合成中做出来巨大的贡献，他利用神经网络，实现了空间坐标到颜色和密度的映射的，然NeRF的方法是计算密集型的，通常需要大量的训练时间和大量的渲染资源，特别是高分辨率的输出。</p><p><img src="https://pic1.zhimg.com/80/v2-c828848317a156fc6dd17c9a5310dd03.png" alt="NeRF"></p><p>针对这些问题，3DGS出现了，3DGS 采用显式表示和高度并行的工作流程，有利于更高效的计算和渲染，其创新在于其独特地融合了可微分管道和基于点的渲染技术的优点，通过用可学习的 3D 高斯函数表示场景，保留了连续体积辐射场的理想特性，这对于高质量图像合成至关重要，同时避免了与空白空间渲染相关的计算开销，这是传统 NeRF 方法的常见缺点，而3DGS很好的解决了这个问题，在不影响视觉质量的情况下达到了实时渲染。</p><p>论文中也发现，自3DGS出现以来，2023年有很多的论文在arXiv中挂出来，所以基于此也写了这样一个综述，同时促进3DGS领域的进一步研究和创新</p><p><img src="https://picx.zhimg.com/80/v2-167cd8779af5c5550c15156e2b9b52c0.png" alt="The number of papers on 3DGS is increasing every month."></p><p>以下是论文架构的图，论文的大概架构如下所示，可以看到这篇综述撰写的一个逻辑，还是非常好的，接下来，我会顺着这个架构进行解读论文来学习</p><ul><li>第2部分：主要是一些问题描述和相关研究领域的一些简要的背景</li><li>第3部分：介绍3DGS，包括3DGS的多视角的合成和3DGS的优化</li><li>第4部分：3DGS 产生重大影响的各种应用领域和任务，展示了其多功能性</li><li>第5部分：对3DGS进行了一些比较和分析</li><li>第6、7部分：对一些未来的开放性工作进行总结和调查</li></ul><p><img src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="Structure of the overall review."></p><h2 id="背景-BACKGROUND"><a href="#背景-BACKGROUND" class="headerlink" title="背景 BACKGROUND"></a>背景 BACKGROUND</h2><p>背景主要分两部分讲解</p><ul><li>辐射场的概念：隐式和显式</li><li>有关辐射场的场景重建、渲染等领域相关介绍</li></ul><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="辐射场"><a href="#辐射场" class="headerlink" title="辐射场"></a>辐射场</h4><p>辐射场是实际上是对三维空间中光分布的表示，它捕捉了光与环境中的表面和材质相互作用的方式。从数学上来说，辐射场可被描述为一个函数$L:\mathbb{R}^5\to\mathbb{R}^+$, 其中$L(x,y,z,\theta,\psi)$将点$(x,y,z)$和球坐标下的方向$(\theta,\phi)$映射为非负的辐射值。辐射场有显示表达和隐式表达，可用于场景表示和渲染。</p><h4 id="隐式辐射场"><a href="#隐式辐射场" class="headerlink" title="隐式辐射场"></a>隐式辐射场</h4><p>隐式辐射场是辐射场中的一种，在表示场景中的光分布时，不需显式定义场景的集合形状。这里面最常见的就是NeRF，使用神经网络来学习连续的体积表示。在NeRF中，使用MLP 网络用于将一组空间坐标 $(x, y, z)$ 和观察方向 $(\theta,\phi)$ 映射到颜色和密度值。任何点处的辐射不是显式存储的，而是通过查询神经网络实时计算得出。因此，该函数可以写成：</p><script type="math/tex; mode=display">L_\text{implicit}(x,y,z,\theta,\phi)=\text{NeuralNetwork}(x,y,z,\theta,\phi)</script><p>这种方式的好处是构建了一个可微且紧凑的复杂场景，但是由于我们总是需要对光线进行采样和体渲染的计算，会导致计算负载比较高。</p><h4 id="显式辐射场"><a href="#显式辐射场" class="headerlink" title="显式辐射场"></a>显式辐射场</h4><p>与隐式不同的是，显示是直接表示光在离散空间结构中的分布，比如体素网格或点云。该结构中的每个元素都存储了其在空间中相应位置的辐射信息，而不是像NeRF一样去执行查询的操作，所以他会更直接也更快的得到每个值，但是同时也需要更大内存使用和导致较低的分辨率。通常我们可以表示为：</p><script type="math/tex; mode=display">L_\text{explicit}{ ( x , y , z , \theta , \phi ) }=\text{DataStructure}[(x,y,z)]\cdot f(\theta,\phi)</script><p>其中，<code>DataStructure</code>可以是网格或点云，而$f(θ, ϕ)$是一个根据观察视线方向修改辐射的函数。</p><h4 id="3D-Gaussian-Splatting-（两全其美）"><a href="#3D-Gaussian-Splatting-（两全其美）" class="headerlink" title="3D Gaussian Splatting （两全其美）"></a>3D Gaussian Splatting （两全其美）</h4><p>3DGS通过利用3D 高斯函数作为其表示形式，充分利用了显示辐射场和隐式辐射场的优势。这些高斯函数被优化用于准确表示场景，结合了基于神经网络的优化和显式结构化数据存储的优点。这种混合方法能进行高质量渲染，同时具有更快的训练和实时性能，3D高斯表达可表示为：</p><script type="math/tex; mode=display">L_{\mathrm{3DGS}}(x,y,z,\theta,\phi)=\sum_{i}G(x,y,z,\mu_{i},\Sigma_{i})\cdot c_{i}(\theta,\phi)</script><p>其中 $G$ 是具有平均值 $μ_i$ 和协方差 $Σ_i$ 的高斯函数，$c$ 表示与视图相关的颜色。</p><h4 id="显式与隐式的理解"><a href="#显式与隐式的理解" class="headerlink" title="显式与隐式的理解"></a>显式与隐式的理解</h4><p>这里放一张理解显示隐式图像的图片，我还是觉得相当不错的</p><p><img src="https://pic1.zhimg.com/80/v2-e79d0183806753d34863598e544a0517.jpeg" alt="显式隐式表达"></p><h3 id="背景和术语"><a href="#背景和术语" class="headerlink" title="背景和术语"></a>背景和术语</h3><p>许多技术和研究学科与 <code>3DGS</code> 有着密切的关系，以下各节将对此进行简要介绍。</p><h4 id="场景重建与渲染"><a href="#场景重建与渲染" class="headerlink" title="场景重建与渲染"></a>场景重建与渲染</h4><p><strong>场景重建</strong>：从一组图像集合或其它数据建立场景的三维模型。</p><p><strong>渲染</strong>：将计算机可读取的信息（如场景中的3D物体）转化为图像。<br>早期技术基于光场生成逼真的图像，运动结构（SfM）与多视图立体匹配（MVS）算法通过从图像序列估计3D结构来增强光场。</p><h4 id="神经渲染和辐射场"><a href="#神经渲染和辐射场" class="headerlink" title="神经渲染和辐射场"></a>神经渲染和辐射场</h4><p><strong>神经渲染</strong>：将深度学习与传统图形技术结合生成逼真的图像。早期方法使用CNN估计混合权重或纹理空间解决方案。</p><p><strong>辐射场</strong>：一种函数表达，描述从各方向穿过空间各点的光的量。NeRF使用神经网络建模辐射场。</p><h4 id="体积表示和光线行进"><a href="#体积表示和光线行进" class="headerlink" title="体积表示和光线行进"></a>体积表示和光线行进</h4><p><strong>体积表达</strong>：不仅将物体和场景建模为表面，还将其其建模为充满材料或空白空间的体积。这样可以对如雾、烟或半透明材料进行更精确的渲染。</p><p><strong>光线行进</strong>：是体积表达渲染图像的技术，通过增量跟踪穿过“体”的光线来渲染图像。NeRF引入重要性采样和位置编码增强合成图像的质量，虽然能得到高质量的图像，但这一方法计算量大。</p><h4 id="基于点的渲染"><a href="#基于点的渲染" class="headerlink" title="基于点的渲染"></a>基于点的渲染</h4><p>基于点的渲染是一种使用点而非传统多边形来可视化3D场景的技术。该方法特别适用于渲染复杂、非结构化或稀疏的几何数据。点可以通过添加额外属性，如可学习的神经描述符来进行增强，并且可以高效地进行渲染，但这种方法可能会出现渲染中的空洞或混叠效应等问题。3DGS通过使用各向异性高斯进行更连贯的场景表达。</p><h2 id="用于显式辐射场的3DGS"><a href="#用于显式辐射场的3DGS" class="headerlink" title="用于显式辐射场的3DGS"></a>用于显式辐射场的3DGS</h2><p>3DGS能够实时渲染高分辨率的图像，并且不需要神经网络，是一个突破。</p><p>这一块主要围绕两块进行讲解</p><ul><li>3DGS的前向过程</li><li>3DGS的优化过程</li></ul><h3 id="学习3D高斯函数进行新视角合成"><a href="#学习3D高斯函数进行新视角合成" class="headerlink" title="学习3D高斯函数进行新视角合成"></a>学习3D高斯函数进行新视角合成</h3><p>假如现在有一个场景，目的是生成特定视角下的相机图像。NeRF对每一个像素使用光线行进和采样点，影响其实时性；而3DGS将3D高斯投影到图像平面，称为“泼溅”，如下图所示。然后对高斯进行排序并计算每个像素的值。NeRF和3DGS的渲染可视为互逆关系。</p><p><img src="https://pic1.zhimg.com/80/v2-9d5fff5c2390526cd03e5a14fd13f4fe.png" alt="3DGS的Splatting 泼溅"></p><p>这里面有个点很有意思，为什么说是互逆关系，我参考了知乎的一篇文章<a href="https://zhuanlan.zhihu.com/p/666465701">3D Gaussian Splatting中的数学推导</a>的说明，我觉得这个说的还不错。</p><blockquote><p> 首先，我们回忆一下体渲染的这个事情。假设读者跟我一样是从NeRF才接触体渲染的，那么回顾一下NeRF中，沿着一个像素，发出一条射线，然后这条射线“射向体数据”（在NeRF里就是沿着光线进行采样，然后查询采样点的属性）的过程。这个过程可以归结为一种<code>backward mapping</code>。</p><p> 所以很自然的，会有一种<code>forward mapping</code>的办法。形式上，就是将整个“体数据”投影到此时位姿所对应的图像平面。这种办法的前提就不能是用NeRF那种隐式表达了，需要一些显式的表达才能支持这样直接的投影。例如以三个顶点长成的三角面基元（primitive），然后将这些许多的三角面直接投影到成像平面上，判断哪些像素是什么颜色，当有多个三角形投影时，根据他们的“深度”来判断前后顺序，然后进行熟悉的alpha compositing。当然也会有其他基元，例如小的平面表示等等。</p><p> 无论是<code>backward mapping</code>还是<code>forward mapping</code>，这个过程都涉及到将连续的表示变成离散的。在<code>backward mapping</code>里，是对场进行采样；在<code>forward mapping</code>里，是需要直接生成出基元，这也是一种连续化为离散。为了理解在这个过程中，高斯分布为什么重要，我们需要牵扯到信号与系统中的概念。与混过数字信号处理考试不同的是，我们要清楚此时引入信号与系统里的工具的目的是什么。回想刚才三角面基元的情景，在实际情境中，我们其实都接触不到“连续”的表达，比如三角面，我们只会记录它的三个顶点。当投影完成后，我们只能做一些有限的操作来阻止“锯齿”，例如对结果进行一个模糊操作，这些操作一般都是局部的。我们这样做的目的，本质是“希望用离散的表达来重建原来的信号，进一步在重建好的信号上进行“resampling”。如果我们对处理后的结果，视觉上看起来没什么混叠或者锯齿上的问题，那就说明我们“resampling”是成功的。</p></blockquote><p>从下图也可以看到NeRF和Gaussian在概念上的区别，左边是NeRF沿着光线查询连续 MLP，右边是Gaussian一组与给定光线相关的离散的高斯分布</p><p><img src="https://picx.zhimg.com/80/v2-08473faff1a084b3de92e2a86f69f0fd.png" alt=""></p><p><img src="https://picx.zhimg.com/80/v2-37166011e5e81d299598141028acff42.png" alt="difference between NeRF and Gaussian Splatting"></p><p>首先简单介绍一下，3DGS是如何表示真实场景的，前面也有提过，在<strong>Gaussian Splatting</strong>中，3D世界用一组3D点表示，实际上是数百万个，大致在0.5到5百万之间。每个点是一个3D高斯，具有其独特的参数，这些参数是为每个场景拟合的，以便该场景的渲染与已知数据集图像紧密匹配，接下来就介绍他的属性。</p><p><img src="https://pica.zhimg.com/80/v2-f440b37ac00a08977b2b6e5514ffec1f.png" alt="Representing a 3D world"></p><ul><li><p><strong>3D高斯的属性</strong>： 一个3D高斯主要包括，中心（位置）$x,y,z$的均值$μ$、不透明度 $α$、3D 协方差矩阵 $Σ$ 和颜色 $c$（一般是RGB或者是球谐（SH）系数）。 其中$c$与视角有关，$c$ 由球谐函数表示。所有属性均可学习，都可以通过反向传播来学习和优化。</p></li><li><p><strong>视域剔除</strong>：给定特定的相机姿态，该步骤会判断哪些高斯位于相机的视锥外，并在后续步骤中剔除之，以节省计算。</p></li><li><p><strong>Splatting泼溅</strong>：实际上只是3D高斯（椭圆体）投影到2D图像空间（椭圆）中进行渲染。给定视图变换 $W$ 和3D协方差矩阵$\Sigma$，我们可以使用使用以下公式计算投影 2D 协方差矩阵 $\Sigma^{\prime}$</p><script type="math/tex; mode=display">\Sigma^{\prime}=JW\Sigma W^\top J^\top</script><p>其中 $J$ 为投影变换中仿射近似的雅可比矩阵。</p></li><li><p><strong>像素渲染</strong>：如果不考虑并行，采用最简单的方式：给定像素 $x$ 的位置，与其到所有重叠高斯函数的距离，即这些高斯函数的深度。这些可以通过观察变换 $W$ 计算出来，形成高斯函数的排序列表$N$。然后进行alpha混合，计算该像素的最终颜色：</p><script type="math/tex; mode=display">C=\sum_{i\in\mathcal{N}}c_i\alpha_i^{\prime}\prod_{j=1}^{i-1}\left(1-\alpha_j^{\prime}\right.)</script><p>其中 $c_i$ 是学习到的颜色，最终的不透明度 $\alpha_i^{\prime}$ 是学习的不透明度 $\alpha_i$ 与高斯的乘积:</p><script type="math/tex; mode=display">\alpha_i'=\alpha_i\times\exp\left(-\frac12(x'-\mu_i')^\top\Sigma_i'^{-1}(x'-\mu_i')\right)</script></li></ul><p>  其中 $x’$ 和 $μ’_i$ 是投影空间中的坐标，同时我也找了个gif来可视化了一下Gaussian Splatting对位置p的影响：</p><p>  <img src="/img/3dgs.gif" alt="3DGS"></p><p>  如果仔细看的话，我们会发现，实际上这个公式和<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">多变量正态分布的概率密度函数</a>十分相像，是忽略了带有协方差行列式的标准化项，而是用不透明度来加权。</p><script type="math/tex; mode=display">  (2\pi)^{-k/2}\det(\boldsymbol{\Sigma})^{-1/2}\exp\biggl(-\frac12(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\biggr)</script><p>  不过如果考虑并行的话加快速度，这种列表排序实际上很难并行化，所以很有可能这个渲染程度比NeRF还慢。为了实现实时渲染，3DGS也做了一个tradeoff，3DGS做出了一些让步来适应<strong>并行计算</strong>。</p><p>  <img src="https://picx.zhimg.com/80/v2-7cea6c4b183982cd921c0456d1f689b7.png" alt="Tiles(Patches)"></p><ul><li><p><strong>Tiles (Patches)</strong>：为避免逐像素计算出现的成本，3DGS改为<strong>patch</strong>级别的渲染。具体来说，首先将图像分割为多个不重叠的patch，称为<code>tile</code>，每个图块包含 16×16 像素，如下图所示。3DGS然后确定<code>tile</code>与投影高斯的相交情况，由于投影高斯可能会与多个<code>tile</code>相交，需要进行复制，并为每个复制体分配相关tile的标识符（如<code>tile</code>的ID）。(不用判断每个像素与高斯的距离，而是判断tile就简单多了)</p><p><img src="https://picx.zhimg.com/80/v2-c81242a6677621910801fcec4c0adbee.png" alt=""></p><p>从下图可以看到排序的结果，在排序中，高位是tile的ID，低位就是深度，一起进行排序，下面的图是AI葵视频的结果，还是很好理解的</p><p><img src="https://pic1.zhimg.com/80/v2-5c74958d484c1d2588c20c8c30b58411.png" alt="3DGS排序"></p><p><img src="https://picx.zhimg.com/80/v2-3d6e3aec3a86c1d94354458830dbf17f.png" alt="3DGS排序例子(AI葵)"></p></li><li><p><strong>并行渲染</strong>：复制后，3DGS（对应字节的无序列表）结合包含了相关的tile ID（对应字节的高位）和深度信息（对应字节的低位），如上图所示。由于每一块和每一像素的计算是独立的，所以可以基于CUDA编程的块和线程来实现并行计算，同时有利于访问公共共享内存并保持统一的读取顺序。排序后的列表可直接用于渲染（alpha混合），如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-6393ea51f715d0d0baa880cd1890a549.png" alt="并行渲染"></p><p>总的来说，3DGS在前向过程中做出了一些近似计算，以提高计算效率并保留图像合成的高质量。</p></li></ul><h3 id="3DGS的优化"><a href="#3DGS的优化" class="headerlink" title="3DGS的优化"></a>3DGS的优化</h3><p>学习到这里，我们可能会有一个问题，怎么可能在空间中的一堆圆球中得到一个像样的图像的，确实是这样，如果没有进行优化，在渲染的时候就会出现很多伪影，从下图你可以看到。</p><p><img src="https://pic1.zhimg.com/80/v2-7ad69d962fb9a18d84747130af62fe15.png" alt="An example of renders of an under-optimized scene"></p><p>3DGS的核心是<strong>3D高斯集合的优化过程</strong>。一方面需要通过可微渲染来使高斯符合场景纹理，另一方面表达场景需要的高斯数量是未知的。这分别对应参数优化与密度控制两步，这两步在优化过程中交替进行。优化过程中，需要手动设置很多超参数。</p><h4 id="参数优化-Parameter-Optimization"><a href="#参数优化-Parameter-Optimization" class="headerlink" title="参数优化 Parameter Optimization"></a>参数优化 Parameter Optimization</h4><ul><li><p><strong>损失函数</strong>：图像合成后，计算渲染图像与真实图像的差异作为损失：</p><script type="math/tex; mode=display">\mathcal{L}=(1-\lambda)\mathcal{L}_1+\lambda\mathcal{L}_{D-SSIM}</script><p>其中 $λ$ 是权重因子。与 NeRF 的损失函数略有不同，由于光线行进成本高昂，NeRF 通常在像素级别而不是图像级别进行计算，而3DGS是图像级别的。</p></li><li><p><strong>参数更新</strong>：3D高斯的多数参数可通过反向传播直接更新，但对于协方差矩阵 $\Sigma$来说，需要半正定矩阵（这里面是一个定义，应该是多元正态分布的协方差矩阵是一个半正定矩阵），直接优化可能会产生非半正定矩阵，而只有半正定矩阵才有物理意义。因此，改为优化四元数$q$和3D向量$s$。将协方差矩阵分解：</p><script type="math/tex; mode=display">\Sigma=RSS^\top R^\top</script><p>其中$R$与$S$分别由$q$和$s$推导得到的旋转和缩放矩阵。</p><ul><li>$S$是一个对角缩放矩阵，含有3个参数</li><li>$R$是一个3x3的旋转矩阵，通过旋转四元数来表示</li></ul><p>对于不透明度$α$, 其计算图较为复杂：$(q,s)\to\Sigma\to\Sigma^{\prime}\to\alpha$。为避免自动微分的计算消耗，3DGS还推导了$q$与$s$的梯度，在优化过程中直接计算之。</p></li></ul><h4 id="密度控制-Density-Control"><a href="#密度控制-Density-Control" class="headerlink" title="密度控制 Density Control"></a>密度控制 Density Control</h4><ul><li><strong>初始化</strong>：3DGS建议从SfM产生的稀疏点云初始化或随机初始化高斯，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a> 库来完成这一步。。然后进行点的密集化和剪枝以控制3D高斯的密度。当由于某种原因无法获得点云时，可以使用随机初始化来代替，但可能会降低最终的重建质量。</li></ul><p><img src="https://picx.zhimg.com/80/v2-0d67e5748993593a04ed46f7519e972e_720w.png" alt="A sparse 3D point cloud produced by SfM, means initialization"></p><ul><li><p><strong>点密集化</strong>：在点密集化阶段，3DGS自适应地增加高斯的密度，以更好地捕捉场景的细节。该过程特别关注缺失几何特征或高斯过于分散的区域。密集化在一定数量的迭代后执行，比如100个迭代，针对在视图空间中具有较大位置梯度（即超过特定阈值）的高斯。其包括在未充分重建的区域克隆小高斯或在过度重建的区域分裂大高斯。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。这一步旨在在3D空间中寻求高斯的最佳分布和表示，增强重建的整体质量。</p><p>这一部分的意义是什么呢，因为SGD只能对现有点进行调整，但是在完全没有点或点太多的区域，很难找到好的参数，所以这就是点密集化的作用。</p></li><li><p><strong>点的剪枝</strong>：点的剪枝阶段移除冗余或影响较小的高斯，可以在某种程度上看作是一种正则化过程。一般消除几乎是透明的高斯（α低于指定阈值）和在世界空间或视图空间中过大的高斯。此外，为防止输入相机附近的高斯密度不合理地增加，这些高斯会在固定次数的迭代后将$\alpha$设置为接近0的值。该步骤在保证高斯的精度和有效性的情况下，能节约计算资源。</p></li></ul><p><img src="https://picx.zhimg.com/80/v2-58c80507588563289c26e2ea4066ad81.png" alt="Adaptive Gaussian densification scheme."></p><h3 id="用SH系数来表示颜色"><a href="#用SH系数来表示颜色" class="headerlink" title="用SH系数来表示颜色"></a>用SH系数来表示颜色</h3><p>在计算机图形学中，用球谐函数（Spherical Harmonics，简称SH）表示视角相关的颜色起着重要作用，最初是在Plenoxels中提出的。他能表示非兰伯特效应，比如金属表面的高光反射。不过这样也不是一定的，实际上也可以使用3个RGB值表示颜色，然后使用Gaussian Splatting。</p><p> 图形学全局环境光照技术与球谐函数息息相关，我们的环境光来源四面八方，可以理解为一个球面函数，当模拟漫反射环境光，我们用一张环境贴图进行采样，对每一个点进行半球采样出在这个像素上的颜色，<strong>球谐光照</strong>简单来说就是用几个系数存取了整张环境贴图包围在球上<strong>法线方向</strong>所对应的的颜色信息。在渲染过程中传入球谐系数。在模型上根据对应的法线信息，从球谐函数中获取对应的颜色信息。</p><p>球谐函数是定义在球面上的特殊函数，换句话说，可以对球面上的任意点计算这样一个函数并得到一个值。</p><p>这里我们简单理解一下，SH，球谐函数，归根到底只是一组基函数，至于这组基函数是怎么来的，不管他。简单点来说，每一个函数都可以由多个基函数组合起来，如果我们有很多基函数，我们可以通过对应的权重系数复原出原来的函数，不过本质上还是一个有损压缩，不一定那么准确，不过如果基函数越多，复原的函数越准确，但是计算量也变大了。</p><p>在球面基函数中，最多的就是球谐函数了。球谐函数有很多很好的性质，比如正交性，旋转不变性（这边就不介绍了）。正交性说明每个基函数都是独立的，每个基函数都不能用别的基函数加权得到。当SH的系数用的越多，那么表达能力就越强，跟原始的函数就越接近。（如果更详细的了解可以看看一些原理，我主要是宏观的了解SH是什么，简单理解就是他是一种颜色的表示）</p><p><img src="https://pic1.zhimg.com/80/v2-9e660f32e92e1897aa986b0ab2ce073e.png" alt=""></p><p>当用来描述不同方向光照的SH基函数，我们一般用二阶或者三阶，比如下面的例子就是3阶的</p><p><img src="https://picx.zhimg.com/80/v2-f6bfb715b846bf13c95013ca96c1d51d.png" alt=""></p><p>下面展示的是一个$l=2$和3阶的球谐函数，一共包括9个学习系数，我们可以根据点的视角得到相关颜色，可以看到最后是red红色分量。</p><p><img src="https://pica.zhimg.com/80/v2-8241e4f7092a89a158df31b8cde94d33.png" alt="得到l=2和9个学习系数的点的视角相关颜色（红色分量）的过程"></p><h3 id="3DGS-流程"><a href="#3DGS-流程" class="headerlink" title="3DGS 流程"></a>3DGS 流程</h3><p>最后根据论文的图来总结一下3DGS的流程</p><p><img src="https://pic1.zhimg.com/80/v2-fda180df51e9171e3e147f5b40e520b9.png" alt="3DGS 流程"></p><ol><li><p><strong>Structure from Motion</strong>：使用SfM从一组图像中估计出点云，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a>  库操作</p><p><img src="https://picx.zhimg.com/80/v2-961548f1a56fb5bc81bc8b349472d8ab.png" alt="Structure from Motion"></p></li></ol><ol><li><p><strong>Convert to Gaussians</strong>：将每个点建模成一个 3D 高斯图像。从 SfM 数据中，我们能推断出每个高斯图像的位置和颜色。但如果是要得到更高质量的表征的话，还需要对每个高斯函数进行训练，以推断出更精细的位置和颜色，并推断出协方差和透明度。</p></li><li><p><strong>Training</strong>：与神经网络类似，我们使用随机梯度下降法进行训练，但这里没有神经网络的层的概念 (都是 3D 高斯函数)。</p><p>训练步骤如下:</p><ol><li>用当前所有可微高斯函数渲染出图像</li><li>根据渲染图像和真实图像之间的差异计算损失</li><li>根据损失调整每个高斯图像的参数</li><li>根据情况对当前相关高斯图像进行点的密度控制</li></ol><p>步骤 1-3 比较简单，下面我们稍微解释一下第 4 步的工作:</p><ul><li>如果某高斯图像的梯度很大 (即它错得比较离谱)，则对其进行分裂或克隆<ul><li>如果该高斯图像很小，则克隆它</li><li>如果该高斯图像很大，则将其分裂</li></ul></li><li>如果该高斯图像的 alpha 太低，则将其删除</li></ul><p>这么做能帮助高斯图像更好地拟合精细的细节，同时修剪掉不必要的高斯图像。</p></li><li><p><strong>Differentiable Gaussian Rasterization</strong>：3D Gaussian Splatting实际上是一种光栅化的方法，将数据成像到屏幕上，与其他方法相比，他有两个特点</p><ol><li>快</li><li>可微</li></ol><p>主要步骤如下：</p><ol><li>针对给定相机视角，把每个 3D 高斯投影到 2D。</li><li>按深度对高斯进行排序。</li><li>对每个像素，从前到后计算每个高斯在该像素点的值，并将所有值混合以得到最终像素值。</li></ol></li></ol><h3 id="3DGS-Limitations"><a href="#3DGS-Limitations" class="headerlink" title="3DGS Limitations"></a>3DGS Limitations</h3><p><strong>优点</strong></p><ol><li>高品质、逼真的场景</li><li>快速、实时的渲染</li><li>更快的训练速度</li></ol><p><strong>缺点</strong></p><ol><li>防止模型优化中的“破碎”的高斯：点太大、太长、冗余等</li><li>更高的显存使用率 (4GB 用于显示，12GB 用于训练)</li><li>更大的磁盘占用 (每场景 1GB+)</li><li>与现有渲染管线不兼容</li><li><del>只能重建静态场景（但是好像现在动态的Gaussian也出来了，所以这个不算缺点了）</del></li></ol><h2 id="应用领域和任务-APPLICATION-AREAS-AND-TASKS"><a href="#应用领域和任务-APPLICATION-AREAS-AND-TASKS" class="headerlink" title="应用领域和任务 APPLICATION AREAS AND TASKS"></a>应用领域和任务 APPLICATION AREAS AND TASKS</h2><h3 id="同时定位和建图（SLAM）"><a href="#同时定位和建图（SLAM）" class="headerlink" title="同时定位和建图（SLAM）"></a>同时定位和建图（SLAM）</h3><p>SLAM需要让设备实时理解自身位置并同时为环境建图，因此计算量大的表达技术难以应用。</p><p>传统SLAM使用点/surfel云或体素网格表达环境。3DGS的优势在于高效性（自适应控制高斯密度）、精确性（各向异性高斯能建模环境细节）、适应性（能用于各种尺度和复杂度的环境）。</p><h3 id="动态场景建模"><a href="#动态场景建模" class="headerlink" title="动态场景建模"></a>动态场景建模</h3><p>动态场景建模需要捕捉和表达场景随时间变化的的3D结构和外观。需要建立能精确反映场景中物体几何、运动和视觉方面的数字模型。4D高斯泼溅通过扩展3D高斯溅射的概念，引入时间维度，使得可以表达和渲染动态场景。现在也有一些方法在研究在动态场景中的一些编辑的功能，与3DGS进行交互。</p><h3 id="AI生成内容（AIGC）"><a href="#AI生成内容（AIGC）" class="headerlink" title="AI生成内容（AIGC）"></a>AI生成内容（AIGC）</h3><p>AIGC是人工智能自动创建或极大修改的数字内容，可以模仿、扩展或增强人类生成的内容。</p><p>3DGS的显式特性、实时渲染能力和可编辑水平使其与AIGC高度相关。例如，有方法使用3DGS与生成模型、化身或场景编辑结合，如3DGS-Avatar。</p><h3 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h3><p>自动驾驶的目标是在无人干涉的情况下导航并操作车辆，其主要目标是安全而高效地感知环境、做出决策和操作执行器。</p><p>其中，感知和理解环境需要实时重建驾驶场景，精确识别静态和动态物体，并理解其相互关系和运动。动态驾驶场景中，场景还会随时间连续变化。3DGS可以通过混合数据点（如激光雷达点）将场景重建为连贯表达，有利于处理数据点变化的密度，以及静态背景和动态物体的精确重建。</p><h2 id="性能比较-PERFORMANCE-COMPARISON"><a href="#性能比较-PERFORMANCE-COMPARISON" class="headerlink" title="性能比较 PERFORMANCE COMPARISON"></a>性能比较 PERFORMANCE COMPARISON</h2><p>在这一部分，针对3FGS在上述的领域上的一些性能评估。</p><h3 id="性能基准：定位"><a href="#性能基准：定位" class="headerlink" title="性能基准：定位"></a>性能基准：定位</h3><ul><li><p>数据集：Replica。</p></li><li><p>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</p></li><li>评估指标：均方根误差（RMSE）、绝对轨迹误差（ATE），测量传感器运动轨迹上真实位置与估计位置欧式距离的均方根。</li><li>结果：基于3D高斯的SLAM方法能超过基于NeRF的密集视觉SLAM。</li></ul><p><img src="https://picx.zhimg.com/80/v2-3277500ac5a850accdd2891db0595ae6.png" alt=""></p><h3 id="性能基准：静态场景渲染"><a href="#性能基准：静态场景渲染" class="headerlink" title="性能基准：静态场景渲染"></a>性能基准：静态场景渲染</h3><ul><li>数据集：Replica。</li><li>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</li><li>评估指标：峰值信噪比(PSNR)、结构相似性(SSIM)、学习的感知图像patch相似性(LPIPS),衡量RGB渲染性能。</li><li>结果：基于3D高斯的方法能超过基于<strong>NeRF</strong>的方法。</li></ul><p><img src="https://picx.zhimg.com/80/v2-94d0eea6ab0c03f32c82802f00a8102d.png" alt=""></p><h3 id="性能基准：动态场景渲染"><a href="#性能基准：动态场景渲染" class="headerlink" title="性能基准：动态场景渲染"></a>性能基准：动态场景渲染</h3><ul><li>数据集：D-NeRF。</li><li>基准算法：CoGS、4D-GS、GauFRe、4DGS。</li><li>评估指标：PSNR、SSIM、LPIPS, 用于衡量RGB渲染性能。</li><li>结果：3DGS能大幅超过基于NeRF的SOTA。但静态版本的3DGS对动态场景的重建是失败的。</li></ul><p><img src="https://picx.zhimg.com/80/v2-288ca353912cbbd221a111ee553ab607_720w.png" alt=""></p><h3 id="性能基准：驾驶场景渲染"><a href="#性能基准：驾驶场景渲染" class="headerlink" title="性能基准：驾驶场景渲染"></a>性能基准：驾驶场景渲染</h3><ul><li>数据集：nuScences。</li><li>基准算法：DrivingGaussian。</li><li>评估指标：PSNR、SSIM、LPIPS*（LPIPS× 1000）, 用于衡量RGB渲染性能。</li><li>结果：3DGS方法能大幅超过基于NeRF的方法。</li></ul><p><img src="https://pic1.zhimg.com/80/v2-8b7bb910fbb86e3d1b23fc062260dc5d_720w.png" alt=""></p><h3 id="性能基准：数字虚拟人"><a href="#性能基准：数字虚拟人" class="headerlink" title="性能基准：数字虚拟人"></a>性能基准：数字虚拟人</h3><p>该任务的目标是从给定的多视角视频渲染人体化身模型。</p><ul><li>数据集：ZJU-MoCap。</li><li>基准算法：GART、Human101、HUGS、3DGS-Avatar。</li><li>评估指标：PSNR、SSIM、LPIPS* (LPIPS×1000) ,用于衡量RGB渲染性能$_{9}$</li><li>结果：基于3DGS的方法能在渲染质量和速度上均有优势。</li></ul><p><img src="https://pica.zhimg.com/80/v2-d916a05d315e576e3cef738aa2306226.png" alt=""></p><h2 id="未来研究方向-FUTURE-RESEARCH-DIRECTIONS"><a href="#未来研究方向-FUTURE-RESEARCH-DIRECTIONS" class="headerlink" title="未来研究方向 FUTURE RESEARCH DIRECTIONS"></a>未来研究方向 FUTURE RESEARCH DIRECTIONS</h2><ul><li><strong>数据高效的3DGS解决方案</strong>：从少样本中进行新视图生成和场景重建很重要。目前的方法有探究引入深度信息、密集概率分布、像素到高斯的映射来促进该能力，实际上就是引入更多的信息。。此外，在观测不足的区域，3DGS会产生伪影，可尝试在这些区域进行数据插值或积分。</li><li><strong>存储高效的3DGS解决方案</strong>：3DGS的可扩展性较差，在大尺度环境中需要大量的存储。需要优化训练阶段和模型的存储利用，而对于NeRF来说只需要存储学习到的MLP参数。可以探索更多高效的数据结构和先进的压缩技术，如Light-Gaussian等</li><li><strong>先进的渲染算法</strong>：目前3DGS的渲染算法较为简单直接，可见性算法会导致高斯深度/混合顺序的剧烈切换，需要实施更先进的渲染算法，更好模拟光与材料属性的复杂相互作用。可结合传统计算机图形学的方法。此外，还可探索逆渲染。</li><li><strong>优化与正则化</strong>： 各向异性高斯虽然有利于表示复杂几何体，但可能产生不希望的视觉伪影。例如，特别是在具有视角依赖外观的区域，大的3D高斯可能导致弹出伪影，突然出现或消失的视觉元素打破了沉浸感。使用正则化可以增加收敛速度，平滑视觉噪声或提高图像质量。此外，3DGS中大量的超参数也会影响3DGS的泛化性。在3DGS的规则化和优化方面存在相当大的探索潜力。</li><li><strong>3D高斯在网格重建中的应用</strong>：可探索3DGS在网格重建中的潜力，从而缩小体积渲染和传统基于表面的方法的差距，以便提出新的渲染技巧和应用。</li><li><strong>赋予3DGS更多可能性</strong>： 尽管3DGS具有显著潜力，但3DGS的全范围应用仍然未被充分挖掘。一个有前景的探索方向是用额外的属性增强3D高斯，例如为特定应用定制的语言和物理属性。此外，最近的研究开始揭示3DGS在多个领域的能力，例如相机姿态估计、捕捉手对象互动和不确定性量化。这些初步发现突出了跨学科学者进一步探索3DGS的重要机会。</li></ul><h2 id="参考文献-REFERENCES"><a href="#参考文献-REFERENCES" class="headerlink" title="参考文献 REFERENCES"></a>参考文献 REFERENCES</h2><ol><li>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. (2023). <a href="https://arxiv.org/abs/2308.04079">3D Gaussian Splatting for Real-Time Radiance Field Rendering.</a> SIGGRAPH 2023.</li><li>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</a> ECCV 2020.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf">Surface Splatting.</a> SIGGRAPH 2001</li><li>Luiten, J., Kopanas, G., Leibe, B., &amp; Ramanan, D. (2023). <a href="https://arxiv.org/abs/2308.09713">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.</a> International Conference on 3D Vision.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf">EWA Volume Splatting.</a> IEEE Visualization 2001.</li><li>Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A. (2023). <a href="https://arxiv.org/abs/2112.05131">Plenoxels: Radiance Fields without Neural Networks.</a> CVPR 2022.</li><li><a href="https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">A Comprehensive Overview of Gaussian Splatting</a></li><li><a href="https://github.com/huggingface/blog/blob/main/gaussian-splatting.md">Introduction to 3D Gaussian Splatting</a></li><li><a href="https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum">Sample Representation</a></li><li><a href="https://zhuanlan.zhihu.com/p/664725693">《3D Gaussian Splatting for Real-Time Radiance Field Rendering》3D高斯的理论理解</a></li><li><a href="https://blog.csdn.net/weixin_45657478/article/details/135603696">【论文笔记】A Survey on 3D Gaussian Splatting</a></li></ol>]]></content>
    
    
    <summary type="html">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Arxiv学术论文查询接口详解</title>
    <link href="https://kedreamix.github.io/2024/01/24/Note/arXiv/"/>
    <id>https://kedreamix.github.io/2024/01/24/Note/arXiv/</id>
    <published>2024-01-24T05:40:00.000Z</published>
    <updated>2024-01-24T05:44:25.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Arxiv学术论文查询接口详解-转载"><a href="#Arxiv学术论文查询接口详解-转载" class="headerlink" title="Arxiv学术论文查询接口详解 转载"></a>Arxiv学术论文查询接口详解 转载</h1><blockquote><p>这篇博客主要转载自：<a href="https://hiyoungai.com/posts/arxiv%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/">Arxiv学术论文查询接口详解</a>，我觉得写的很好，所以我也不重新整理这一部分的API接口了。我后续使用这一部分的API接口来进行爬取得到最新的文章，还是非常方便的，所以也同时推荐给大家，能最快follow新文章</p></blockquote><p>Arxiv API 允许以编程方式获取 <a href="https://arxiv.org/">https://arxiv.org</a> 上的论文。API 的基本结构为：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/{method_name}?{parameters}</span><br></pre></td></tr></tbody></table></figure><h2 id="查询接口"><a href="#查询接口" class="headerlink" title="查询接口"></a>查询接口</h2><p>查询接口的的 method_name 为 query，下面是查询方法的参数，参数之间以 <em>&amp;</em> 分隔。</p><div class="table-container"><table><thead><tr><th style="text-align:center">parameters</th><th style="text-align:center">type</th><th style="text-align:center">defaults</th><th style="text-align:center">required</th></tr></thead><tbody><tr><td style="text-align:center">search_query</td><td style="text-align:center">string</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">id_list</td><td style="text-align:center">comma-delimited string（以 ‘，’ 分隔的字符串）</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">start</td><td style="text-align:center">int</td><td style="text-align:center">0</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">max_results</td><td style="text-align:center">int</td><td style="text-align:center">10</td><td style="text-align:center">No</td></tr></tbody></table></div><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>如果 API 只包含 search_query（不包含 id_list），那么返回与 search_query 内容匹配的结果。</li><li>如果 API 只包含 id_list（不包含 search_query），那么返回 id_list 中每一项的结果。</li><li>如果 API 中包含了 search_query 和 id_list，那么返回在 id_list 中，并且与 search_query 匹配的文章。</li></ul><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>通常情况下，一个查询可能有成百上千个返回结果。有时候我们不希望一次性查询到这么多数量，那么可以使用 <em>start</em> 和 <em>max_results</em> 两个字段来进行分页查询。</p><ul><li>start 是查询的起始索引，以 0 为第一个。</li><li>max_results 是查询返回的集合数。</li></ul><p>下面来举例说明一下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=10 (1)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=10&amp;max_results=10 (2)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=20&amp;max_results=10 (3)</span><br></pre></td></tr></tbody></table></figure><p>查询结果分别为：</p><ol><li>0 - 9</li><li>10 - 19</li><li>20 - 29</li></ol><p>需要注意的是，由于 API 的限制，在多次调用 API 的情况下，建议每次调用的时间间隔为 3 秒。每次调用返回的最大数量为 2000 个。arXiv的硬限制约为 50,000 条记录； 对于与 50,000 多个原稿匹配的查询，无法接收全部结果. 解决这个问题的最简单的解决方案是将中断查询成小块，例如使用的时间片，与一系列日期的<code>submittedDate</code>或<code>lastUpdatedDate</code> 。</p><h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><p>对查询的结果进行排序有两个选项：<em>sortBy</em> 和 <em>sortOrder</em>。</p><ul><li>sortBy 的值有：relevance，lastUpdatedDate 和 submittedDate。</li><li>sortOrder 的值有：ascending 和 descending。</li></ul><p>示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=ti:%22electron%20thermal%20conductivity%22&amp;sortBy=lastUpdatedDate&amp;sortOrder=ascending</span><br></pre></td></tr></tbody></table></figure><h2 id="结果响应"><a href="#结果响应" class="headerlink" title="结果响应"></a>结果响应</h2><p>API 的 Response 内容中是以 <em>Atom 1.0</em> 为主体的，<em>Atom</em> 是 XML 的一种语法。下面分别来说明各个标签的含义。</p><h3 id="Feed-Metadata"><a href="#Feed-Metadata" class="headerlink" title="Feed Metadata"></a>Feed Metadata</h3><p>每个 Response 都会包含的内容：</p><ol><li>版本和命名空间</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Title：feed 的标题，通常为查询 URL 的字符串。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    ArXiv Query:  search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：查询的唯一标识（注意不是查询的每个文章的 id），保证每个查询 id 是唯一的。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link：查询 URL 的规范化。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Updated：提供了 feed 内容最后一次更新的时间。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-08T00:00:00-04:00&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Opensearch：扩展元素，包含了查询的返回数量以及分页信息等。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1000</span><br><span class="line">&lt;/opensearch:totalResults&gt;</span><br><span class="line">&lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   0</span><br><span class="line">&lt;/opensearch:startIndex&gt;</span><br><span class="line">&lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1</span><br><span class="line">&lt;/opensearch:itemsPerPage&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Entry-Metadata"><a href="#Entry-Metadata" class="headerlink" title="Entry Metadata"></a>Entry Metadata</h3><p>正常情况下，Response 返回结果中的 <em>feed</em> 标签会包含 0 个或者多个 <em>entry</em> 标签。每个 entry 表示一个查询的返回文章，下面分别说一下 entry 中的各个元素。</p><ol><li>Title：返回文章的标题</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-Electron Production at High Transverse Momenta <span class="keyword">in</span> ep Collisions at HERA</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：文章的 URL ，可以认为是文章的绝对路径。最后一个字段是文章的唯一标识符。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/abs/hep-ex/0307015</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Published/Updated：文章的发布日期和更新日期。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;published xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-02-27T16:02:02-05:00</span><br><span class="line">&lt;/published&gt;</span><br><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-06-25T17:09:59-04:00</span><br><span class="line">&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Summary：文章的摘要。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-electron production is studied at high electron transverse momentum</span><br><span class="line">    <span class="keyword">in</span> positron- and electron-proton collisions using the H1 detector at HERA.</span><br><span class="line">    The data correspond to an integrated luminosity of 115 pb-1. Di-electron</span><br><span class="line">    and tri-electron event yields are measured. Cross sections are derived <span class="keyword">in</span></span><br><span class="line">    a restricted phase space region dominated by photon-photon collisions. In</span><br><span class="line">    general good agreement is found with the Standard Model predictions.</span><br><span class="line">    However, <span class="keyword">for</span> electron pair invariant masses above 100 GeV, three</span><br><span class="line">    di-electron events and three tri-electron events are observed, compared to</span><br><span class="line">    Standard Model expectations of 0.30 \pm 0.04 and 0.23 \pm 0.04,</span><br><span class="line">    respectively.</span><br><span class="line">&lt;/summary&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Author：文章的作者，包含一个或者多个 name 标签，分别表示多个作者。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;H1 Collaboration&lt;/name&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Category：文章的分类。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.AI"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"I.2.6"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link，对于每个文章，最多有三个 link 元素，通过 ref 和 title 来区别，下面的表格表示 ref 和 title 的内容：</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">rel</th><th style="text-align:center">title</th><th style="text-align:center">refers to</th><th style="text-align:center">always present</th></tr></thead><tbody><tr><td style="text-align:center">alternate</td><td style="text-align:center">-</td><td style="text-align:center">abstract page</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">pdf</td><td style="text-align:center">pdf</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">doi</td><td style="text-align:center">resolved doi</td><td style="text-align:center">no</td></tr></tbody></table></div><p>例子：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/abs/hep-ex/0307015v1"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"pdf"</span> href=<span class="string">"http://arxiv.org/pdf/hep-ex/0307015v1"</span> rel=<span class="string">"related"</span> <span class="built_in">type</span>=<span class="string">"application/pdf"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"doi"</span> href=<span class="string">"http://dx.doi.org/10.1529/biophysj.104.047340"</span> rel=<span class="string">"related"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:primary_category：主要分类的扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:primary_category xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:comment：评论扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:comment xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   23 pages, 8 figures and 4 tables</span><br><span class="line">&lt;/arxiv:comment&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:affiliation：作者从属关系。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;author&gt;</span><br><span class="line">   &lt;name&gt;G. G. Kacprzak&lt;/name&gt;</span><br><span class="line">   &lt;arxiv:affiliation xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;NMSU&lt;/arxiv:affiliation&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:journal_ref：期刊说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:journal_ref xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   Eur.Phys.J. C31 (2003) 17-29</span><br><span class="line">&lt;/arxiv:journal_ref&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:doi：doi 说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:doi xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   10.1529/biophysj.104.047340</span><br><span class="line">&lt;/arxiv:doi&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>返回错误，如果请求的响应出现错误，会返回一个详细的错误信息。例如下面是一个错误 id 的信息：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">  &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=&amp;amp;id_list=1234.12345"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br><span class="line">  &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;ArXiv Query: search_query=&amp;amp;id_list=1234.12345&lt;/title&gt;</span><br><span class="line">  &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/kvuntZ8c9a4Eq5CF7KY03nMug+Q&lt;/id&gt;</span><br><span class="line">  &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line">  &lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:totalResults&gt;</span><br><span class="line">  &lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;0&lt;/opensearch:startIndex&gt;</span><br><span class="line"></span><br><span class="line">  &lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:itemsPerPage&gt;</span><br><span class="line">  &lt;entry xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/errors<span class="comment">#incorrect_id_format_for_1234.12345&lt;/id&gt;</span></span><br><span class="line">    &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;Error&lt;/title&gt;</span><br><span class="line">    &lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;incorrect <span class="built_in">id</span> format <span class="keyword">for</span> 1234.12345&lt;/summary&gt;</span><br><span class="line">    &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line"></span><br><span class="line">    &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/errors#incorrect_id_format_for_1234.12345"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">    &lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;arXiv api core&lt;/name&gt;</span><br><span class="line">    &lt;/author&gt;</span><br><span class="line">  &lt;/entry&gt;</span><br><span class="line">&lt;/feed&gt;</span><br></pre></td></tr></tbody></table></figure><p>下面提供了一些常见的错误：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Sample query 示例查询</strong></th><th style="text-align:center"><strong>Error Explanation 错误解释</strong></th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=not_an_int">http://export.arxiv.org/api/query?start=not_an_int</a></td><td style="text-align:center"><code>start</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=-1">http://export.arxiv.org/api/query?start=-1</a></td><td style="text-align:center"><code>start</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=not_an_int">http://export.arxiv.org/api/query?max_results=not_an_int</a></td><td style="text-align:center"><code>max_results</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=-1">http://export.arxiv.org/api/query?max_results=-1</a></td><td style="text-align:center"><code>max_results</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=1234.1234">http://export.arxiv.org/api/query?id_list=1234.1234</a></td><td style="text-align:center">malformed id</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=cond—mat/0709123">http://export.arxiv.org/api/query?id_list=cond—mat/0709123</a></td><td style="text-align:center">malformed id</td></tr></tbody></table></div><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>python2.7 上的简单请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">url = <span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span></span><br><span class="line">data = urllib.urlopen(url).read()</span><br><span class="line"><span class="built_in">print</span> data</span><br></pre></td></tr></tbody></table></figure><p>python3 上的请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> libreq</span><br><span class="line"><span class="keyword">with</span> libreq.urlopen(<span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span>) <span class="keyword">as</span> url:</span><br><span class="line">    r = url.read()</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br></pre></td></tr></tbody></table></figure><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="查询的详细结构"><a href="#查询的详细结构" class="headerlink" title="查询的详细结构"></a>查询的详细结构</h3><p>在 arXiv 搜索引擎中，每篇文章都被划分为许多可以单独搜索的字段。 例如，可以搜索一篇文章的标题，以及作者列表、摘要、评论和期刊参考文献。 要搜索其中一个字段，只需在搜索词前加上字段前缀和冒号即可。 例如：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro</span><br></pre></td></tr></tbody></table></figure><p>下面的表格显示所有字段的前缀：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>prefix</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center">ti</td><td style="text-align:center">Title</td></tr><tr><td style="text-align:center">au</td><td style="text-align:center">Author</td></tr><tr><td style="text-align:center">abs</td><td style="text-align:center">Abstract</td></tr><tr><td style="text-align:center">co</td><td style="text-align:center">Comment</td></tr><tr><td style="text-align:center">jr</td><td style="text-align:center">Journal Reference</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">Subject Category</td></tr><tr><td style="text-align:center">rn</td><td style="text-align:center">Report Number</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">Id (use <code>id_list</code> instead)</td></tr><tr><td style="text-align:center">all</td><td style="text-align:center">All of the above</td></tr></tbody></table></div><p>并且查询也支持布尔运算，假设我们希望找到作者 Adrian DelMaestro 的所有文章，其标题中也包含单词 checkerboard。 我们可以使用 AND 操作符构造下面的查询：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro+AND+ti:checkerboard</span><br></pre></td></tr></tbody></table></figure><p>下面是三种可能的布尔值：</p><ul><li>AND</li><li>OR</li><li>ANDNOT</li></ul><p>下面是特殊符号的含义以及转义字符：</p><div class="table-container"><table><thead><tr><th style="text-align:left">symbol</th><th style="text-align:left">encoding</th><th style="text-align:left">explanation</th></tr></thead><tbody><tr><td style="text-align:left">( )</td><td style="text-align:left">%28 %29</td><td style="text-align:left">用于为布尔运算符优先级对布尔表达式进行分组</td></tr><tr><td style="text-align:left">“ “</td><td style="text-align:left">%22 %22</td><td style="text-align:left">用于将多个单词组合成短语以搜索特定字段</td></tr><tr><td style="text-align:left">空格</td><td style="text-align:left">+</td><td style="text-align:left">用于扩展<code>search_query</code> 包含多个字段</td></tr></tbody></table></div><h3 id="返回的详细结构"><a href="#返回的详细结构" class="headerlink" title="返回的详细结构"></a>返回的详细结构</h3><p>下表列出了返回的 Atom 结果的每个元素:</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>element</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>feed elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">包含规范化查询字符串的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">分配给此查询的唯一 id</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">最后一次更新此查询的搜索结果。 设置为当天的午夜</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">通过 GET 请求检索此提要的 url</td></tr><tr><td style="text-align:center">opensearch:totalResults</td><td style="text-align:center">此查询的搜索结果总数</td></tr><tr><td style="text-align:center">opensearch:startIndex</td><td style="text-align:center">总结果列表中第一个返回结果的基于0的索引</td></tr><tr><td style="text-align:center">opensearch:itemsPerPage</td><td style="text-align:center">每页返回的结果数</td></tr><tr><td style="text-align:center"><strong>entry elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">文章的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">文章的网址<code>http://arxiv.org/abs/id</code></td></tr><tr><td style="text-align:center">published</td><td style="text-align:center">文章的发布日期</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">文章的更新日期，如果为 v1 版本，那么与发布日期相同</td></tr><tr><td style="text-align:center">summary</td><td style="text-align:center">文章摘要</td></tr><tr><td style="text-align:center">author</td><td style="text-align:center">每个作者有一个子元素 name，包含了作者的名字</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">可以给定与这篇文章关联的 3 个网址</td></tr><tr><td style="text-align:center">category</td><td style="text-align:center">文章分类</td></tr><tr><td style="text-align:center">arxiv:primary_category</td><td style="text-align:center">主要的 arXiv 分类</td></tr><tr><td style="text-align:center">arxiv:comment</td><td style="text-align:center">作者对此发表的评论</td></tr><tr><td style="text-align:center">arxiv:affiliation</td><td style="text-align:center">作者的从属关系</td></tr><tr><td style="text-align:center">arxiv:journal_ref</td><td style="text-align:center">参考文献</td></tr><tr><td style="text-align:center">arxiv:doi</td><td style="text-align:center">已解析的 DOI 的 url，指向外部资源</td></tr></tbody></table></div><h3 id="学科的分类"><a href="#学科的分类" class="headerlink" title="学科的分类"></a>学科的分类</h3><p>下面是学科分类字段以及对应的翻译（软件脚本自动翻译，如不对请勿喷）：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">学科（英文）</th><th style="text-align:center">学科（中文）</th></tr></thead><tbody><tr><td style="text-align:center">astro-ph</td><td style="text-align:center">Astrophysics</td><td style="text-align:center">天体物理</td></tr><tr><td style="text-align:center">astro-ph.CO</td><td style="text-align:center">Cosmology and Nongalactic Astrophysics</td><td style="text-align:center">宇宙学与非规则天体物理学</td></tr><tr><td style="text-align:center">astro-ph.EP</td><td style="text-align:center">Earth and Planetary Astrophysics</td><td style="text-align:center">地球与行星天体物理学</td></tr><tr><td style="text-align:center">astro-ph.GA</td><td style="text-align:center">Astrophysics of Galaxies</td><td style="text-align:center">星系的天体物理学</td></tr><tr><td style="text-align:center">astro-ph.HE</td><td style="text-align:center">High Energy Astrophysical Phenomena</td><td style="text-align:center">高能天体物理现象</td></tr><tr><td style="text-align:center">astro-ph.IM</td><td style="text-align:center">Instrumentation and Methods for Astrophysics</td><td style="text-align:center">天体物理学的仪器和方法</td></tr><tr><td style="text-align:center">astro-ph.SR</td><td style="text-align:center">Solar and Stellar Astrophysics</td><td style="text-align:center">太阳与恒星天体物理学</td></tr><tr><td style="text-align:center">cond-mat.dis-nn</td><td style="text-align:center">Disordered Systems and Neural Networks</td><td style="text-align:center">无序系统与神经网络</td></tr><tr><td style="text-align:center">cond-mat.mes-hall</td><td style="text-align:center">Mesoscale and Nanoscale Physics</td><td style="text-align:center">中尺度和纳米尺度物理学</td></tr><tr><td style="text-align:center">cond-mat.mtrl-sci</td><td style="text-align:center">Materials Science</td><td style="text-align:center">材料科学</td></tr><tr><td style="text-align:center">cond-mat.other</td><td style="text-align:center">Other Condensed Matter</td><td style="text-align:center">其他凝聚态</td></tr><tr><td style="text-align:center">cond-mat.quant-gas</td><td style="text-align:center">Quantum Gases</td><td style="text-align:center">量子气体</td></tr><tr><td style="text-align:center">cond-mat.soft</td><td style="text-align:center">Soft Condensed Matter</td><td style="text-align:center">软凝聚物</td></tr><tr><td style="text-align:center">cond-mat.stat-mech</td><td style="text-align:center">Statistical Mechanics</td><td style="text-align:center">统计力学</td></tr><tr><td style="text-align:center">cond-mat.str-el</td><td style="text-align:center">Strongly Correlated Electrons</td><td style="text-align:center">强关联电子</td></tr><tr><td style="text-align:center">cond-mat.supr-con</td><td style="text-align:center">Superconductivity</td><td style="text-align:center">超导现象</td></tr><tr><td style="text-align:center">cs.AI</td><td style="text-align:center">Artificial Intelligence</td><td style="text-align:center">人工智能</td></tr><tr><td style="text-align:center">cs.AR</td><td style="text-align:center">Hardware Architecture</td><td style="text-align:center">硬件架构</td></tr><tr><td style="text-align:center">cs.CC</td><td style="text-align:center">Computational Complexity</td><td style="text-align:center">计算复杂性</td></tr><tr><td style="text-align:center">cs.CE</td><td style="text-align:center">Computational Engineering, Finance, and Science</td><td style="text-align:center">计算工程，金融和科学</td></tr><tr><td style="text-align:center">cs.CG</td><td style="text-align:center">Computational Geometry</td><td style="text-align:center">计算几何</td></tr><tr><td style="text-align:center">cs.CL</td><td style="text-align:center">Computation and Language</td><td style="text-align:center">计算与语言</td></tr><tr><td style="text-align:center">cs.CR</td><td style="text-align:center">Cryptography and Security</td><td style="text-align:center">密码学与保安</td></tr><tr><td style="text-align:center">cs.CV</td><td style="text-align:center">Computer Vision and Pattern Recognition</td><td style="text-align:center">计算机视觉与模式识别</td></tr><tr><td style="text-align:center">CY</td><td style="text-align:center">Computers and Society</td><td style="text-align:center">电脑与社会</td></tr><tr><td style="text-align:center">cs.DB</td><td style="text-align:center">Databases</td><td style="text-align:center">数据库</td></tr><tr><td style="text-align:center">cs.DC</td><td style="text-align:center">Distributed, Parallel, and Cluster Computing</td><td style="text-align:center">分布式、并行和集群计算</td></tr><tr><td style="text-align:center">cs.DL</td><td style="text-align:center">Digital Libraries</td><td style="text-align:center">数字仓库</td></tr><tr><td style="text-align:center">cs.DM</td><td style="text-align:center">Discrete Mathematics</td><td style="text-align:center">离散数学</td></tr><tr><td style="text-align:center">cs.DS</td><td style="text-align:center">Data Structures and Algorithms</td><td style="text-align:center">数据结构和算法</td></tr><tr><td style="text-align:center">cs.ET</td><td style="text-align:center">Emerging Technologies</td><td style="text-align:center">新兴科技</td></tr><tr><td style="text-align:center">cs.FL</td><td style="text-align:center">Formal Languages and Automata Theory</td><td style="text-align:center">形式语言与自动机理论</td></tr><tr><td style="text-align:center">cs.GL</td><td style="text-align:center">General Literature</td><td style="text-align:center">一般文学</td></tr><tr><td style="text-align:center">cs.GR</td><td style="text-align:center">Graphics</td><td style="text-align:center">图形</td></tr><tr><td style="text-align:center">cs.GT</td><td style="text-align:center">Computer Science and Game Theory</td><td style="text-align:center">计算机科学与博弈论</td></tr><tr><td style="text-align:center">cs.HC</td><td style="text-align:center">Human-Computer Interaction</td><td style="text-align:center">人机交互</td></tr><tr><td style="text-align:center">cs.IR</td><td style="text-align:center">Information Retrieval</td><td style="text-align:center">信息检索</td></tr><tr><td style="text-align:center">cs.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">cs.LG</td><td style="text-align:center">Learning</td><td style="text-align:center">学习</td></tr><tr><td style="text-align:center">cs.LO</td><td style="text-align:center">Logic in Computer Science</td><td style="text-align:center">计算机科学中的逻辑</td></tr><tr><td style="text-align:center">cs.MA</td><td style="text-align:center">Multiagent Systems</td><td style="text-align:center">多代理系统</td></tr><tr><td style="text-align:center">cs.MM</td><td style="text-align:center">Multimedia</td><td style="text-align:center">多媒体</td></tr><tr><td style="text-align:center">cs.MS</td><td style="text-align:center">Mathematical Software</td><td style="text-align:center">数学软件</td></tr><tr><td style="text-align:center">cs.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">cs.NE</td><td style="text-align:center">Neural and Evolutionary Computing</td><td style="text-align:center">神经和进化计算</td></tr><tr><td style="text-align:center">cs.NI</td><td style="text-align:center">Networking and Internet Architecture</td><td style="text-align:center">网络与互联网架构</td></tr><tr><td style="text-align:center">cs.OH</td><td style="text-align:center">Other Computer Science</td><td style="text-align:center">其他计算机科学</td></tr><tr><td style="text-align:center">cs.OS</td><td style="text-align:center">Operating Systems</td><td style="text-align:center">操作系统</td></tr><tr><td style="text-align:center">cs.PF</td><td style="text-align:center">Performance</td><td style="text-align:center">性能</td></tr><tr><td style="text-align:center">cs.PL</td><td style="text-align:center">Programming Languages</td><td style="text-align:center">编程语言</td></tr><tr><td style="text-align:center">cs.RO</td><td style="text-align:center">Robotics</td><td style="text-align:center">机器人技术</td></tr><tr><td style="text-align:center">cs.SC</td><td style="text-align:center">Symbolic Computation</td><td style="text-align:center">符号计算</td></tr><tr><td style="text-align:center">cs.SD</td><td style="text-align:center">Sound</td><td style="text-align:center">声音</td></tr><tr><td style="text-align:center">cs.SE</td><td style="text-align:center">Software Engineering</td><td style="text-align:center">软件工程</td></tr><tr><td style="text-align:center">cs.SI</td><td style="text-align:center">Social and Information Networks</td><td style="text-align:center">社会和信息网络</td></tr><tr><td style="text-align:center">cs.SY</td><td style="text-align:center">Systems and Control</td><td style="text-align:center">系统及控制</td></tr><tr><td style="text-align:center">econ.EM</td><td style="text-align:center">Econometrics</td><td style="text-align:center">计量经济学</td></tr><tr><td style="text-align:center">eess.AS</td><td style="text-align:center">Audio and Speech Processing</td><td style="text-align:center">音频及语音处理</td></tr><tr><td style="text-align:center">eess.IV</td><td style="text-align:center">Image and Video Processing</td><td style="text-align:center">图像和视频处理</td></tr><tr><td style="text-align:center">eess.SP</td><td style="text-align:center">Signal Processing</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">gr-qc</td><td style="text-align:center">General Relativity and Quantum Cosmology</td><td style="text-align:center">广义相对论和量子宇宙学</td></tr><tr><td style="text-align:center">hep-ex</td><td style="text-align:center">High Energy Physics - Experiment</td><td style="text-align:center">高能物理实验</td></tr><tr><td style="text-align:center">hep-lat</td><td style="text-align:center">High Energy Physics - Lattice</td><td style="text-align:center">高能物理-晶格</td></tr><tr><td style="text-align:center">hep-ph</td><td style="text-align:center">High Energy Physics - Phenomenology</td><td style="text-align:center">高能物理-现象学</td></tr><tr><td style="text-align:center">hep-th</td><td style="text-align:center">High Energy Physics - Theory</td><td style="text-align:center">高能物理理论</td></tr><tr><td style="text-align:center">math.AC</td><td style="text-align:center">Commutative Algebra</td><td style="text-align:center">交换代数</td></tr><tr><td style="text-align:center">math.AG</td><td style="text-align:center">Algebraic Geometry</td><td style="text-align:center">代数几何</td></tr><tr><td style="text-align:center">math.AP</td><td style="text-align:center">Analysis of PDEs</td><td style="text-align:center">偏微分方程分析</td></tr><tr><td style="text-align:center">math.AT</td><td style="text-align:center">Algebraic Topology</td><td style="text-align:center">代数拓扑</td></tr><tr><td style="text-align:center">math.CA</td><td style="text-align:center">Classical Analysis and ODEs</td><td style="text-align:center">传统分析和微分方程</td></tr><tr><td style="text-align:center">math.CO</td><td style="text-align:center">Combinatorics</td><td style="text-align:center">组合数学</td></tr><tr><td style="text-align:center">math.CT</td><td style="text-align:center">Category Theory</td><td style="text-align:center">范畴理论</td></tr><tr><td style="text-align:center">math.CV</td><td style="text-align:center">Complex Variables</td><td style="text-align:center">复杂变量</td></tr><tr><td style="text-align:center">math.DG</td><td style="text-align:center">Differential Geometry</td><td style="text-align:center">微分几何</td></tr><tr><td style="text-align:center">math.DS</td><td style="text-align:center">Dynamical Systems</td><td style="text-align:center">动力系统</td></tr><tr><td style="text-align:center">math.FA</td><td style="text-align:center">Functional Analysis</td><td style="text-align:center">功能分析</td></tr><tr><td style="text-align:center">math.GM</td><td style="text-align:center">General Mathematics</td><td style="text-align:center">普通数学</td></tr><tr><td style="text-align:center">math.GN</td><td style="text-align:center">General Topology</td><td style="text-align:center">点集拓扑学</td></tr><tr><td style="text-align:center">math.GR</td><td style="text-align:center">Group Theory</td><td style="text-align:center">群论</td></tr><tr><td style="text-align:center">math.GT</td><td style="text-align:center">Geometric Topology</td><td style="text-align:center">几何拓扑学</td></tr><tr><td style="text-align:center">math.HO</td><td style="text-align:center">History and Overview</td><td style="text-align:center">历史和概述</td></tr><tr><td style="text-align:center">math.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">math.KT</td><td style="text-align:center">K-Theory and Homology</td><td style="text-align:center">K 理论与同调</td></tr><tr><td style="text-align:center">math.LO</td><td style="text-align:center">Logic</td><td style="text-align:center">逻辑</td></tr><tr><td style="text-align:center">math.MG</td><td style="text-align:center">Metric Geometry</td><td style="text-align:center">度量几何学</td></tr><tr><td style="text-align:center">math.MP</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">math.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">math.NT</td><td style="text-align:center">Number Theory</td><td style="text-align:center">数论</td></tr><tr><td style="text-align:center">math.OA</td><td style="text-align:center">Operator Algebras</td><td style="text-align:center">算子代数</td></tr><tr><td style="text-align:center">math.OC</td><td style="text-align:center">Optimization and Control</td><td style="text-align:center">优化和控制</td></tr><tr><td style="text-align:center">math.PR</td><td style="text-align:center">Probability</td><td style="text-align:center">概率</td></tr><tr><td style="text-align:center">math.QA</td><td style="text-align:center">Quantum Algebra</td><td style="text-align:center">量子代数</td></tr><tr><td style="text-align:center">math.RA</td><td style="text-align:center">Rings and Algebras</td><td style="text-align:center">环与代数</td></tr><tr><td style="text-align:center">math.RT</td><td style="text-align:center">Representation Theory</td><td style="text-align:center">表示论</td></tr><tr><td style="text-align:center">math.SG</td><td style="text-align:center">Symplectic Geometry</td><td style="text-align:center">辛几何</td></tr><tr><td style="text-align:center">math.SP</td><td style="text-align:center">Spectral Theory</td><td style="text-align:center">光谱理论</td></tr><tr><td style="text-align:center">math.ST</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr><tr><td style="text-align:center">math-ph</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">nlin.AO</td><td style="text-align:center">Adaptation and Self-Organizing Systems</td><td style="text-align:center">适应与自组织系统</td></tr><tr><td style="text-align:center">nlin.CD</td><td style="text-align:center">Chaotic Dynamics</td><td style="text-align:center">混沌动力学</td></tr><tr><td style="text-align:center">nlin.CG</td><td style="text-align:center">Cellular Automata and Lattice Gases</td><td style="text-align:center">元胞自动机与格子气体</td></tr><tr><td style="text-align:center">nlin.PS</td><td style="text-align:center">Pattern Formation and Solitons</td><td style="text-align:center">模式形成与孤子</td></tr><tr><td style="text-align:center">nlin.SI</td><td style="text-align:center">Exactly Solvable and Integrable Systems</td><td style="text-align:center">严格可解可积系统</td></tr><tr><td style="text-align:center">nucl-ex</td><td style="text-align:center">Nuclear Experiment</td><td style="text-align:center">核试验</td></tr><tr><td style="text-align:center">nucl-th</td><td style="text-align:center">Nuclear Theory</td><td style="text-align:center">核理论</td></tr><tr><td style="text-align:center">physics.acc-ph</td><td style="text-align:center">Accelerator Physics</td><td style="text-align:center">加速器物理学</td></tr><tr><td style="text-align:center">physics.ao-ph</td><td style="text-align:center">Atmospheric and Oceanic Physics</td><td style="text-align:center">大气和海洋物理学</td></tr><tr><td style="text-align:center">physics.app-ph</td><td style="text-align:center">Applied Physics</td><td style="text-align:center">应用物理学</td></tr><tr><td style="text-align:center">physics.atm-clus</td><td style="text-align:center">Atomic and Molecular Clusters</td><td style="text-align:center">原子和分子团簇</td></tr><tr><td style="text-align:center">physics.atom-ph</td><td style="text-align:center">Atomic Physics</td><td style="text-align:center">原子物理学</td></tr><tr><td style="text-align:center">physics.bio-ph</td><td style="text-align:center">Biological Physics</td><td style="text-align:center">生物物理学</td></tr><tr><td style="text-align:center">physics.chem-ph</td><td style="text-align:center">Chemical Physics</td><td style="text-align:center">化学物理</td></tr><tr><td style="text-align:center">physics.class-ph</td><td style="text-align:center">Classical Physics</td><td style="text-align:center">经典物理学</td></tr><tr><td style="text-align:center">physics.comp-ph</td><td style="text-align:center">Computational Physics</td><td style="text-align:center">计算物理学</td></tr><tr><td style="text-align:center">physics.data-an</td><td style="text-align:center">Data Analysis, Statistics and Probability</td><td style="text-align:center">数据分析、统计和概率</td></tr><tr><td style="text-align:center">physics.ed-ph</td><td style="text-align:center">Physics Education</td><td style="text-align:center">物理教育</td></tr><tr><td style="text-align:center">physics.flu-dyn</td><td style="text-align:center">Fluid Dynamics</td><td style="text-align:center">流体动力学</td></tr><tr><td style="text-align:center">physics.gen-ph</td><td style="text-align:center">General Physics</td><td style="text-align:center">普通物理</td></tr><tr><td style="text-align:center">physics.geo-ph</td><td style="text-align:center">Geophysics</td><td style="text-align:center">地球物理学</td></tr><tr><td style="text-align:center">physics.hist-ph</td><td style="text-align:center">History and Philosophy of Physics</td><td style="text-align:center">物理学的历史与哲学</td></tr><tr><td style="text-align:center">physics.ins-det</td><td style="text-align:center">Instrumentation and Detectors</td><td style="text-align:center">仪器和探测器</td></tr><tr><td style="text-align:center">physics.med-ph</td><td style="text-align:center">Medical Physics</td><td style="text-align:center">医学物理学</td></tr><tr><td style="text-align:center">physics.optics</td><td style="text-align:center">Optics</td><td style="text-align:center">光学</td></tr><tr><td style="text-align:center">physics.plasm-ph</td><td style="text-align:center">Plasma Physics</td><td style="text-align:center">等离子体物理</td></tr><tr><td style="text-align:center">physics.pop-ph</td><td style="text-align:center">Popular Physics</td><td style="text-align:center">大众物理</td></tr><tr><td style="text-align:center">physics.soc-ph</td><td style="text-align:center">Physics and Society</td><td style="text-align:center">物理学与社会</td></tr><tr><td style="text-align:center">physics.space-ph</td><td style="text-align:center">Space Physics</td><td style="text-align:center">空间物理学</td></tr><tr><td style="text-align:center">q-bio.BM</td><td style="text-align:center">Biomolecules</td><td style="text-align:center">生物分子</td></tr><tr><td style="text-align:center">q-bio.CB</td><td style="text-align:center">Cell Behavior</td><td style="text-align:center">细胞行为</td></tr><tr><td style="text-align:center">q-bio.GN</td><td style="text-align:center">Genomics</td><td style="text-align:center">基因组学</td></tr><tr><td style="text-align:center">q-bio.MN</td><td style="text-align:center">Molecular Networks</td><td style="text-align:center">分子网络</td></tr><tr><td style="text-align:center">q-bio.NC</td><td style="text-align:center">Neurons and Cognition</td><td style="text-align:center">神经元与认知</td></tr><tr><td style="text-align:center">q-bio.OT</td><td style="text-align:center">Other Quantitative Biology</td><td style="text-align:center">其他定量生物学</td></tr><tr><td style="text-align:center">q-bio.PE</td><td style="text-align:center">Populations and Evolution</td><td style="text-align:center">种群与进化</td></tr><tr><td style="text-align:center">q-bio.QM</td><td style="text-align:center">Quantitative Methods</td><td style="text-align:center">定量方法</td></tr><tr><td style="text-align:center">q-bio.SC</td><td style="text-align:center">Subcellular Processes</td><td style="text-align:center">亚细胞突起</td></tr><tr><td style="text-align:center">q-bio.TO</td><td style="text-align:center">Tissues and Organs</td><td style="text-align:center">组织和器官</td></tr><tr><td style="text-align:center">q-fin.CP</td><td style="text-align:center">Computational Finance</td><td style="text-align:center">金融工程</td></tr><tr><td style="text-align:center">q-fin.EC</td><td style="text-align:center">Economics</td><td style="text-align:center">经济学</td></tr><tr><td style="text-align:center">q-fin.GN</td><td style="text-align:center">General Finance</td><td style="text-align:center">财务概述</td></tr><tr><td style="text-align:center">q-fin.MF</td><td style="text-align:center">Mathematical Finance</td><td style="text-align:center">数学金融</td></tr><tr><td style="text-align:center">q-fin.PM</td><td style="text-align:center">Portfolio Management</td><td style="text-align:center">投资组合管理</td></tr><tr><td style="text-align:center">q-fin.PR</td><td style="text-align:center">Pricing of Securities</td><td style="text-align:center">证券定价</td></tr><tr><td style="text-align:center">q-fin.RM</td><td style="text-align:center">Risk Management</td><td style="text-align:center">风险管理</td></tr><tr><td style="text-align:center">q-fin.ST</td><td style="text-align:center">Statistical Finance</td><td style="text-align:center">金融统计</td></tr><tr><td style="text-align:center">q-fin.TR</td><td style="text-align:center">Trading and Market Microstructure</td><td style="text-align:center">交易与市场微观结构</td></tr><tr><td style="text-align:center">quant-ph</td><td style="text-align:center">Quantum Physics</td><td style="text-align:center">量子物理学</td></tr><tr><td style="text-align:center">stat.AP</td><td style="text-align:center">Applications</td><td style="text-align:center">应用</td></tr><tr><td style="text-align:center">stat.CO</td><td style="text-align:center">Computation</td><td style="text-align:center">计算</td></tr><tr><td style="text-align:center">stat.ME</td><td style="text-align:center">Methodology</td><td style="text-align:center">方法论</td></tr><tr><td style="text-align:center">stat.ML</td><td style="text-align:center">Machine Learning</td><td style="text-align:center">机器学习</td></tr><tr><td style="text-align:center">stat.OT</td><td style="text-align:center">Other Statistics</td><td style="text-align:center">其他统计学</td></tr><tr><td style="text-align:center">stat.TH</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Arxiv学术论文查询接口详解-转载&quot;&gt;&lt;a href=&quot;#Arxiv学术论文查询接口详解-转载&quot; class=&quot;headerlink&quot; title=&quot;Arxiv学术论文查询接口详解 转载&quot;&gt;&lt;/a&gt;Arxiv学术论文查询接口详解 转载&lt;/h1&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Python" scheme="https://kedreamix.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>3D reconstruction</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3D%20reconstruction/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3D%20reconstruction/</id>
    <published>2024-01-24T03:06:02.000Z</published>
    <updated>2024-01-24T08:12:35.455Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong></p><p>我们将 EndoGS 提出了一种基于高斯斑点的可变形内镜组织重建方法。</p><p><strong>要点</strong></p><ul><li>EndoGS通过采用高斯斑点来实现可变形内镜组织的 3D 重建。</li><li>EndoGS 引入了变形场来处理动态场景，并通过深度引导监督来优化具有单个视点的 3D 目标。</li><li>EndoGS 利用时空权重掩码来减轻工具遮挡。</li><li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内镜组织。</li><li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 可实现优异的渲染质量。</li><li>EndoGS 的代码可在 <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：高斯散点法可变形内窥镜组织重建</p></li><li><p>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</p></li><li><p>第一作者单位：香港大学</p></li><li><p>关键词：高斯散点法 · 机器人手术 · 三维重建</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11535，Github 链接：https://github.com/HKU-MedAI/EndoGS</p></li><li><p>摘要：（1）研究背景：可变形组织的三维重建是机器人手术研究的关键领域，但现有方法往往存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。（2）过去的方法：早期尝试采用深度估计来实现内窥镜重建，但这些方法在处理非刚性变形和遮挡方面存在困难。[9, 12] 提出结合工具遮挡、立体深度估计和稀疏翘曲场的框架，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。神经辐射场 (NeRFs) 在三维重建方面取得了巨大成功，但它们在处理动态场景和遮挡方面也存在局限性。（3）研究方法：本文提出了一种称为 EndoGS 的方法，将高斯散点法应用于可变形内窥镜组织重建。EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。（4）方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 在渲染质量方面优于其他方法。这表明 EndoGS 可以为下游任务（如手术增强现实、教育和机器人学习）提供高质量的可变形组织重建。</p></li><li><p>方法：(1): 我们提出了一种称为 EndoGS 的方法，它将高斯散点法应用于可变形内窥镜组织重建。(2): EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。(3): 我们使用六个正交特征平面对空间和时间信息进行编码，并使用单个 MLP 来更新高斯属性，以获得变形的位置、比例因子、旋转因子、球谐系数和不透明度。(4): 我们结合工具掩码和深度图来训练 EndoGS，以处理工具遮挡和提高重建质量。</p></li><li><p>结论：</p></li></ol><p>（1）意义：本文提出了一种基于高斯散点法进行可变形内窥镜组织重建的方法，该方法能够从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</p><p>（2）优缺点：</p><p>创新点：</p><ul><li>将高斯散点法应用于可变形内窥镜组织重建。</li><li>结合变形场、深度引导监督和时空权重掩码，以处理工具遮挡和提高重建质量。</li></ul><p>性能：</p><ul><li>在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</li></ul><p>工作量：</p><ul><li>需要收集和标记大量的数据。</li><li>需要设计和训练复杂的模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details>​    ## SHINOBI: Shape and Illumination using Neural Object Decomposition via   BRDF Optimization In-the-wild**Authors:Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani**We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: https://shinobi.aengelhardt.com Video: https://www.youtube.com/watch?v=iFENQ6AcYd8&amp;feature=youtu.be [PDF](http://arxiv.org/abs/2401.10171v1) **摘要**多尺度哈希编码的隐式形状表示使更快速、更鲁棒的形状重建成为可能，并通过联合相机对齐优化实现了对现有技术的超越。**要点**- SHINOBI 是一种端到端框架，用于从以不同光线、姿势和背景拍摄的对象图像中重建形状、材质和照明。- SHINOBI 使用基于多尺度哈希编码的隐式形状表示，能够实现更快速、更鲁棒的形状重建。- SHINOBI 还可以编辑照明和对象反射率（即材质），并与对象的形状一起优化双向反射分布函数（BRDF）和照明。- SHINOBI 适用于各种各样的对象图像集合，可以为增强现实/虚拟现实、电影、游戏等多种用例生成可重新照明的 3D 资源。- SHINOBI 可以从不限数量的图像中重建形状、材料和照明。- SHINOBI 联合优化 BRDF 和照明以及对象的形状。- SHINOBI 在各种各样的对象图像集合上进行评估，并与最先进的方法进行了比较。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：SHINOBI：基于 BRDF 优化进行形状和光照分解的神经物体分解</p></li><li><p>作者：Andreas Engelhardt、Amit Raj、Mark Boss、Yunzhi Zhang、Abhishek Kar、Yuanzhen Li、Deqing Sun、Ricardo Martin Brualla、Jonathan T. Barron、Hendrik P. A. Lensch、Varun Jampani</p></li><li><p>第一作者单位：德国图宾根大学</p></li><li><p>关键词：计算机视觉、图形学、神经渲染、形状重建、材质估计、光照估计</p></li><li><p>论文链接：https://arxiv.org/abs/2401.10171，Github 代码链接：无</p></li><li><p>摘要：(1)：背景：逆向渲染基于无约束图像集合的对象是一个长期存在的挑战，需要对形状、光照和姿势进行联合优化。(2)：过去的方法：现有方法通常使用显式形状表示，这在处理具有复杂几何形状的对象时存在局限性。此外，它们通常需要大量的手动调整和用户交互。(3)：方法：本文提出了一种新的框架 SHINOBI，它使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。同时，该框架还联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。(4)：性能：SHINOBI 在多个数据集上的实验结果表明，它在形状重建、材质估计和光照估计方面都优于现有方法。该方法可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。</p></li><li><p>方法：(1) <strong>隐式形状表示</strong>：SHINOBI使用隐式形状表示来表示对象，这是一种无界表示，可以轻松处理具有复杂几何形状的对象。(2) <strong>多分辨率哈希编码</strong>：SHINOBI使用多分辨率哈希编码来编码隐式形状表示，这可以提高形状重建的速度和鲁棒性。(3) <strong>联合优化BRDF和光照</strong>：SHINOBI联合优化BRDF和光照，以实现对光照和对象反射率的编辑。(4) <strong>可照明3D资产生成</strong>：SHINOBI可以生成逼真的可照明3D资产，可用于AR/VR、电影、游戏等多种应用场景。</p></li><li><p>结论：（1）：本工作提出了一种新的框架 SHINOBI，该框架可以从未经摆放的野外图像集中估计物体的形状、姿态和光照。我们新颖的混合哈希网格编码能够使用多分辨率哈希网格更容易地优化相机姿态。此外，我们选择的相机参数化以及逐视图重要性权重和基于补丁的对齐损失允许更好地将图像与 3D 对齐，从而在具有高频细节的情况下更好地重建。虽然 SHINOBI 能够从任何类别的物体中恢复几何形状，但其性能在细长/透明结构上受到限制，并且无法在极端光照变化下恢复高频细节，我们将其留作未来工作的探索。（2）：创新点：</p></li></ol><ul><li>使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。</li><li>联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。</li><li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。性能：</li><li>在多个数据集上的实验结果表明，SHINOBI 在形状重建、材质估计和光照估计方面都优于现有方法。</li><li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。工作量：</li><li>该方法需要大量的数据和计算资源。</li><li>需要手动调整和用户交互。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6fa4b83c9b053b9d6092eca8188f4657.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b1a74a4a26394cb29cc941c3540acec6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1cad55e2949324c67746914275a9a371.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a4ea31f8709c7ac961337f4d618a8737.jpg" align="middle"></details>​    ## GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting**Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang**In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. [PDF](http://arxiv.org/abs/2401.09720v1) **Summary**高斯人体：基于 3D 高斯散布的动态穿衣人体重建方法。**Key Takeaways**- 我们提出了一种名为 GaussianBody 的新型穿衣人体重建方法，该方法基于 3D 高斯散布。- 与昂贵的神经辐照度模型相比，3D 高斯散布在训练时间和渲染质量方面最近表现出出色的性能。- 将静态 3D 高斯散布模型应用于动态人体重建问题并非易事，原因是复杂的非刚性变形和丰富的布料细节。- 为了应对这些挑战，我们的方法考虑了显式姿势引导变形，以在规范空间和观察空间中关联动态高斯分布，引入具有正则化变换的基于物理的先验有助于减轻两个空间之间的歧义。- 在训练过程中，我们进一步提出了一种姿势优化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一个具有尺度的分割机制来增强回归点云的密度。- 实验验证表明，我们的方法可以实现动态穿着人体的高质量细节，以及显式几何重建的最先进的照片级新视角渲染结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：高斯体：基于 3D 高斯散点的衣着人体重建</li><li>作者：孟天力、姚圣翔、谢志峰、陈可宇、姜玉刚</li><li>第一作者单位：上海大学</li><li>关键词：衣着人体重建、3D 高斯散点、动态捕捉、几何重建</li><li>论文链接：https://arxiv.org/abs/2401.09720</li><li>摘要：（1）研究背景：高保真衣着人体模型的创建在虚拟现实、远程临场和电影制作等领域具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工工作，这使得它们耗时且昂贵，从而限制了新手用户的可扩展性。最近，人们越来越关注从单个 RGB 图像或单目视频中自动重建衣着人体模型。（2）过去方法及问题：基于网格的方法最初被引入，通过回归参数化模型（如 SCAPE、SMPL、SMPL-X 和 STAR）来恢复人体形状。虽然它们可以实现快速且鲁棒的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。在这种情况下，添加顶点偏移成为一种增强解决方案。然而，它的表示能力仍然有限。（3）研究方法：本文提出了一种基于 3D 高斯散点的新型衣着人体重建方法，称为高斯体。与昂贵的神经辐射体模型相比，3D 高斯散点最近在训练时间和渲染质量方面表现出优异的性能。然而，由于复杂的非刚性变形和丰富的服装细节，将静态 3D 高斯散点模型应用于动态人体重建问题并非易事。为了应对这些挑战，本文的方法考虑了在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，引入了具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。在训练过程中，本文还提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。（4）方法性能：实验验证了本文方法可以实现动态衣着人体的高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody，该方法可以从单目视频中重建动态衣着人体模型。GaussianBody通过在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，以及引入具有正则化变换的基于物理的先验，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。实验表明，GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。（2）：创新点：</li></ol><ul><li>提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody。</li><li>在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。</li><li>引入具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。</li><li>提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。</li></ul><p>性能：</p><ul><li>GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li><li>GaussianBody在图像质量指标方面与基线和其他方法相当，证明了其竞争性能、相对较快的训练速度，以及使用更高分辨率图像进行训练的能力。</li></ul><p>工作量：</p><ul><li>GaussianBody需要收集和标注大量的数据。</li><li>GaussianBody的训练过程相对复杂，需要较大的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7931aa02d87b1007c7f5cdde77107e5c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3df005c3ea738aba56feb680b23b73d2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details>​    ## PPSURF: Combining Patches and Point Convolutions for Detailed Surface   Reconstruction**Authors:Philipp Erler, Lizeth Fuentes, Pedro Hermosilla, Paul Guerrero, Renato Pajarola Michael Wimmer**3D surface reconstruction from point clouds is a key step in areas such as content creation, archaeology, digital cultural heritage, and engineering. Current approaches either try to optimize a non-data-driven surface representation to fit the points, or learn a data-driven prior over the distribution of commonly occurring surfaces and how they correlate with potentially noisy point clouds. Data-driven methods enable robust handling of noise and typically either focus on a global or a local prior, which trade-off between robustness to noise on the global end and surface detail preservation on the local end. We propose PPSurf as a method that combines a global prior based on point convolutions and a local prior based on processing local point cloud patches. We show that this approach is robust to noise while recovering surface details more accurately than the current state-of-the-art.   Our source code, pre-trained model and dataset are available at: https://github.com/cg-tuwien/ppsurf [PDF](http://arxiv.org/abs/2401.08518v1) Published in Computer Graphics Forum (Jan 2024):   https://onlinelibrary.wiley.com/doi/10.1111/cgf.15000**Summary**云点重建结合局部特征与全局信息，提升表面重建的抗噪性和细节保存能力。**Key Takeaways**- 表面重建是内容创建、考古学、数字文化遗产和工程等领域的关键步骤。- 当前方法要么尝试优化非数据驱动的曲面表示以拟合点，要么学习数据驱动的先验分布，以及它们如何与潜在的噪声点云相关。- 数据驱动的方法能够对噪声进行鲁棒处理，通常专注于全局或局部先验，在全局末端的抗噪性和局部末端的表面细节保留之间进行权衡。- PPSurf 是一种将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合的方法。- PPSurf 在恢复表面细节方面比当前最先进的方法更准确。- PPSurf 的源代码、预训练模型和数据集可在 https://github.com/cg-tuwien/ppsurf 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：PPSURF：结合局部块和点卷积进行详细表面重建</p></li><li><p>作者：P. Erler、L. Fuentes-Perez、P. Hermosilla、P. Guerrero、R. Pajarola、M. Wimmer</p></li><li><p>第一作者单位：维也纳工业大学</p></li><li><p>关键词：点云、表面重建、数据驱动、局部块、点卷积</p></li><li><p>论文链接：https://onlinelibrary.wiley.com/doi/10.1111/cgf.150001Github 代码链接：https://github.com/cg-tuwien/ppsurf</p></li><li><p>摘要：（1）：3D 表面重建是内容创作、考古学、数字文化遗产和工程等领域的关键步骤。目前的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。（2）：过去的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，但通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。（3）：本文提出了一种名为 PPSURF 的方法，该方法结合了基于点卷积的全局先验和基于处理局部点云块的局部先验。（4）：在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</p></li><li><p>方法：(1)：PPSURF方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。(2)：全局分支使用POCO网络，该网络由点卷积模块和插值模块组成，点卷积模块计算每个稀疏点的特征向量，插值模块使用基于注意力的权重对特征向量进行插值以获得全局特征向量。(3)：局部分支使用PointNet网络，该网络经过修改，采用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。(4)：全局特征向量和局部特征向量通过MLP组合，以计算查询点处的占用概率。</p></li><li><p>结论：（1）：本文提出了一种名为 PPSURF 的方法，该方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。（2）：创新点：</p></li></ol><ul><li>将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。</li><li>使用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。</li><li>在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</li></ul><p>性能：</p><ul><li>在 ABCvar-noise 测试集上，PPSURF 在 Chamfer 距离、F1 分数和法线误差方面均优于当前最先进的方法。</li><li>在 PatchSize 消融研究中，PPSURF100NN 和 PPSURF200NN 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li><li>在 Miscellanous 消融研究中，PPSURFSymMax 在 Chamfer 距离方面优于 PPSURFFull，但在 F1 分数和法线误差方面略差。PPSURFQPoints 在 F1 分数和法线误差方面优于 PPSURFFull，但在 Chamfer 距离方面略差。PPSURFMergeCat 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li></ul><p>工作量：</p><ul><li>PPSURF 的实现相对简单，易于理解和使用。</li><li>PPSURF 的训练时间和推理时间与当前最先进的方法相当。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-deb0c891a44eda3abc848abfcb3b5052.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b71a526973638d7a94c6cac1ef5d0c4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78b2aade816834cf694d7949cbee5c89.jpg" align="middle"></details>​    ​    ## InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes**Authors:Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari**We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf. [PDF](http://arxiv.org/abs/2401.05335v1) **摘要**神经辐射场三维重建中，通过用户提供的文本描述和参考视点中的二维边界框，实现新的物体生成。**要点**- InseRF 是一种三维场景神经辐射场中生成式物体插入的新颖方法，支持生成新的物体并将其插入三维场景中。- InseRF 使用文本到图像扩散模型的先验知识，可以有效地编辑三维场景的样式和外观，或移除现有物体。- InseRF 将三维物体插入操作转化为参考视图中的二维物体插入，并使用单视图物体重建方法将二维编辑提升到三维。- InseRF 利用单目深度估计方法的先验知识，将重建的物体插入场景中。- InseRF 在多个三维场景上进行了评估，结果表明其优于现有方法。- InseRF 能够生成可控和三维一致的物体，且不需要显式的三维信息作为输入。- InseRF 项目主页：https://mohamad-shahbazi.github.io/inserf。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：无精修 URL：</p></li><li><p>作者：Mohamad Shahbazi<em>, Anh-Huy Phan</em>, Jia-Bin Huang, Yi-Ling Qiao, Hao Tang, Matthew Fisher, Angela Dai</p></li><li><p>第一作者单位：苏黎世联邦理工学院（ETH Zurich）</p></li><li><p>关键词：神经辐射场、对象插入、文本到图像扩散模型</p></li><li><p>论文链接：无，Github 链接：无</p></li><li><p>摘要：（1）研究背景：近年来，3D 场景编辑方法取得了重大进展，这主要归功于在 3D 生成建模中使用文本到图像扩散模型的强先验。现有方法主要通过改变样式和外观或移除现有对象来编辑 3D 场景。然而，生成新对象对于这些方法来说仍然是一个挑战。（2）过去的方法及其问题：现有方法的问题在于，它们无法生成新的对象，并且需要显式的 3D 信息作为输入。（3）研究方法：为了解决上述问题，本文提出了一种新的方法 InseRF，它可以根据用户提供的文本描述和参考视点中的 2D 边界框在 NeRF 重建的 3D 场景中生成新对象。InseRF 将 2D 编辑提升到 3D，使用单视图对象重建方法重建对象，然后在单目深度估计方法的指导下将重建的对象插入场景中。（4）方法性能：实验表明，InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法。InseRF 能够以可控且 3D 一致的方式插入对象，而无需显式 3D 信息作为输入。</p></li><li><p>方法：(1) 输入：InseRF 的输入是一个 NeRF 重建的 3D 场景、一个文本描述和一个参考视点中的 2D 边界框。(2) 对象重建：InseRF 使用单视图对象重建方法重建对象。(3) 对象插入：InseRF 使用单目深度估计方法估计对象的深度，然后将重建的对象插入场景中。</p></li><li><p>结论：（1）：xxx；（2）：创新点：InseRF 提出了一种新颖的方法，可以根据文本描述和 2D 边界框在 NeRF 重建的 3D 场景中生成新对象，无需显式 3D 信息作为输入。性能：InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法，能够以可控且 3D 一致的方式插入对象。工作量：InseRF 的实现相对复杂，需要结合单视图对象重建方法和单目深度估计方法，工作量较大。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9f528d642820c297073b934ba1373992.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8499c6fe071df44b8e34eafc6e0a99e2.jpg" align="middle"></details><br>​    <p></p><h2 id="Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging"><a href="#Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging" class="headerlink" title="Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging"></a>Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging</h2><p><strong>Authors:Jianyang Shi, Bowen Zhang, Amartansh Dubey, Ross Murch, Liwen Jing</strong></p><p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance. </p><p><a href="http://arxiv.org/abs/2401.04317v1">PDF</a> </p><p><strong>Summary</strong><br>室内成像通过Wi-Fi信号，将测量到的Wi-Fi功率转换为高分辨率图像。</p><p><strong>Key Takeaways</strong></p><ul><li>Wi-Fi室内成像被视为多模态图像生成任务。</li><li>Wi-Fi-GEN网络的形状重建精度是基于物理模型的反演方法的275%。</li><li>Fréchet起始距离评分降低了82%。</li><li>发布了首个大型数据集，包含80,000对Wi-Fi信号和成像目标。</li><li>Wi-Fi-GEN网络将基于模型的方法的挑战吸收为生成性AI网络的大量参数。</li><li>网络设计用于最适合测量的Wi-Fi信号和所需的成像输出。</li><li>为了可复制性，我们将发布数据和代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：视觉重构：WiFi 室内成像的 AI 赋能突破</p></li><li><p>作者：Jianyang Shi、Bowen Zhang、Amartansh Dubey、Ross Murch、Liwen Jing</p></li><li><p>隶属单位：深圳先进技术研究院信息与智能学院</p></li><li><p>关键词：WiFi成像、生成式人工智能、逆散射问题</p></li><li><p>论文链接：https://arxiv.org/abs/2401.04317Github 代码链接：无</p></li><li><p>摘要：(1)：研究背景：室内成像是机器人和物联网的一项关键任务。WiFi 作为一种无处不在的信号，是执行被动成像并将最新信息同步到所有连接设备的理想选择。(2)：过去的方法及其问题：现有研究探索了全波（即测量 WiFi 相位和功率）和无相测量来解决室内成像问题。然而，全波方法的相位测量在高频时非常棘手且昂贵，这使得它在实践中不切实际。最近的研究证明了通过无相 WiFi 信号进行室内成像的可行性。(3)：提出的研究方法：本文将 WiFi 室内成像视为多模态图像生成任务，将测量的 WiFi 功率转换为高分辨率室内图像。提出的 WiFi-GEN 网络实现了比基于物理模型的反演方法高出 275% 的重建精度。此外，Fréchet Inception Distance 得分显着降低了 82%。为了检查模型对这项任务的有效性，发布了第一个包含 80,000 对 WiFi 信号和成像目标的大规模数据集。该模型吸收了基于模型的方法的挑战，包括我们生成式 AI 网络中非线性和不确定性的大量参数。该网络还旨在最适合测量的 WiFi 信号和期望的成像输出。(4)：方法的性能：该方法在室内成像任务上取得了良好的性能。在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。这些结果表明，该方法能够有效地从 WiFi 信号中生成高分辨率的室内图像。</p></li><li><p>方法：(1)：WiFi室内成像问题被表述为多模态图像生成任务，利用生成式人工智能网络将测量的WiFi功率转换为高分辨率室内图像。(2)：WiFi-GEN网络由三个部分组成：WiFi信号编码器、控制信号网络和WiFi生成器。WiFi信号编码器将WiFi信号嵌入到潜在空间矩阵中；控制信号网络从潜在空间中提取WiFi信号相关特征并将其转换为多级特征向量；WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。(3)：WiFi信号编码器是一个三层全连接网络，将19×20的信号展平为380维向量并输入编码器。(4)：控制信号网络由下采样模块和上采样模块组成。下采样模块由多个残差层组成，用于减少特征图的空间尺寸；上采样模块利用卷积层和双线性插值来恢复特征图的空间分辨率。(5)：WiFi生成器采用了一种利用从潜在空间中提取的多尺度特征来生成最终图像的方法。WiFi生成器由多个信号块组成，每个信号块在生成过程中发挥着至关重要的作用。(6)：WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。</p></li><li><p>结论：（1）： 本文将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像，取得了良好的性能。（2）： 创新点：提出一种基于生成式人工智能的 WiFi 室内成像方法，将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像。性能：在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。工作量：该方法需要大量的数据集和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-86612c0b8ef65da0afea3c89e2c33374.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dc98bdad49ef910c1452e4c81ae51370.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38ec24519ebd731699e5acd71cf669b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de86c8629916709cd814c7c39e2990a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ba54f2b1b3eaec39559af94bdc51539d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2408b66da95959934b23ca823827183.jpg" align="middle"><img src="https://picx.zhimg.com/v2-303fefd6ac513f1e77ee4837d8d3a0bd.jpg" align="middle"></details><br>​    <p></p><h2 id="RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction"><a href="#RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction" class="headerlink" title="RHOBIN Challenge: Reconstruction of Human Object Interaction"></a>RHOBIN Challenge: Reconstruction of Human Object Interaction</h2><p><strong>Authors:Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</strong></p><p>Modeling the interaction between humans and objects has been an emerging research direction in recent years. Capturing human-object interaction is however a very challenging task due to heavy occlusion and complex dynamics, which requires understanding not only 3D human pose, and object pose but also the interaction between them. Reconstruction of 3D humans and objects has been two separate research fields in computer vision for a long time. We hence proposed the first RHOBIN challenge: reconstruction of human-object interactions in conjunction with the RHOBIN workshop. It was aimed at bringing the research communities of human and object reconstruction as well as interaction modeling together to discuss techniques and exchange ideas. Our challenge consists of three tracks of 3D reconstruction from monocular RGB images with a focus on dealing with challenging interaction scenarios. Our challenge attracted more than 100 participants with more than 300 submissions, indicating the broad interest in the research communities. This paper describes the settings of our challenge and discusses the winning methods of each track in more detail. We observe that the human reconstruction task is becoming mature even under heavy occlusion settings while object pose estimation and joint reconstruction remain challenging tasks. With the growing interest in interaction modeling, we hope this report can provide useful insights and foster future research in this direction. Our workshop website can be found at \href{<a href="https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}">https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}</a>. </p><p><a href="http://arxiv.org/abs/2401.04143v1">PDF</a> 14 pages, 5 tables, 7 figure. Technical report of the CVPR’23   workshop: RHOBIN challenge (<a href="https://rhobin-challenge.github.io/">https://rhobin-challenge.github.io/</a>)</p><p><strong>Summary</strong><br>首次人类与物体交互重建挑战赛（RHOBIN）及研讨会促成学术交流，吸引百余名参赛者，探讨单目RGB图像中的3D重建。</p><p><strong>Key Takeaways</strong></p><ul><li>首次人类与物体交互重建挑战赛（RHOBIN）于2020年举行，旨在促进人体、物体重建与交互建模研究社群的交流与合作。</li><li>挑战赛包含3个赛道，均以单目RGB图像作为输入，聚焦于解决富有挑战性的交互场景。</li><li>挑战赛吸引了来自世界各地的研究人员，收到300多篇投稿。</li><li>虽然人体重建在遮挡严重的情况下亦表现良好，但物体姿态估计与联合重建仍然具有挑战性。</li><li>挑战赛获奖方法在人体姿态估计、物体姿态估计、交互建模等方面取得优异成绩，表现出强大的鲁棒性和准确性。</li><li>RHOBIN挑战赛促进了研究社群的合作与交流，推动了人体、物体交互重建与建模研究的发展。</li><li>挑战赛获奖方法对后续人体、物体交互重建与建模研究有重要参考价值。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：RHOBIN 挑战：重建人与物体交互</p></li><li><p>作者：Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</p></li><li><p>第一作者单位：图宾根大学</p></li><li><p>关键词：人与物体交互、三维重建、单目 RGB 图像、BEHAVE 数据集</p></li><li><p>论文链接：https://arxiv.org/abs/2401.04143</p></li><li><p>摘要：(1) 研究背景：人与物体交互建模近年来成为一个新兴的研究方向。然而，由于严重的遮挡和复杂的动态，捕捉人与物体交互是一个非常具有挑战性的任务，这需要理解不仅是三维人体姿势和物体姿势，还有它们之间的交互。三维人体和物体的重建长期以来一直是计算机视觉中的两个独立的研究领域。(2) 过去的方法及问题：过去的方法通常将人与物体交互建模分为两个独立的任务：人体重建和物体姿态估计。然而，这些方法存在一些问题，例如，它们通常需要多个摄像头或深度传感器，并且它们对遮挡和动态变化非常敏感。(3) 本文提出的研究方法：为了解决上述问题，本文提出了一个新的挑战：RHOBIN 挑战：重建人与物体交互。该挑战赛旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。(4) 实验结果及性能：该挑战赛吸引了 100 多名参与者和 300 多份提交。所有表现最佳的团队都获得了优于先前最先进方法的结果。经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。随着对交互建模的兴趣日益浓厚，我们希望这份报告能够提供有用的见解，并促进该方向的未来研究。</p></li><li><p>方法：（1）RHOBIN挑战赛：该挑战赛由三个赛道组成，所有赛道都使用单目RGB图像作为输入，并输出三维人体或/和交互的三维物体。（2）数据增强和模型集成：应用数据增强和模型集成来提高性能非常重要。（3）联合重建：联合重建仍然是一个具有挑战性的任务。</p></li><li><p>结论：（1）：本文提出了 RHOBIN 挑战赛，旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。（2）：创新点：本文提出了一个新的挑战：RHOBIN 挑战赛：重建人与物体交互，该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。性能：经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。工作量：该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-60dae88b48b9b3b3fe863b3d312f44c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af42a68cb719096e3898c7bb71fe22b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53fd3a763ee32776a87f4b1ae0da73e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3cda82200fcfbcaad43b075c54ef0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27080102702f4941d7f14a6d3d4305f4.jpg" align="middle"></details><br>​    <p></p><h2 id="Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images"><a href="#Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images" class="headerlink" title="Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images"></a>Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images</h2><p><strong>Authors:Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia</strong></p><p>Multi-view surface reconstruction is an ill-posed, inverse problem in 3D vision research. It involves modeling the geometry and appearance with appropriate surface representations. Most of the existing methods rely either on explicit meshes, using surface rendering of meshes for reconstruction, or on implicit field functions, using volume rendering of the fields for reconstruction. The two types of representations in fact have their respective merits. In this work, we propose a new hybrid representation, termed Sur2f, aiming to better benefit from both representations in a complementary manner. Technically, we learn two parallel streams of an implicit signed distance field and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the implicit signed distance function (SDF) and surface rendering of the surrogate mesh with a shared, neural shader; the unified shading promotes their convergence to the same, underlying surface. We synchronize learning of the surrogate mesh by driving its deformation with functions induced from the implicit SDF. In addition, the synchronized surrogate mesh enables surface-guided volume sampling, which greatly improves the sampling efficiency per ray in volume rendering. We conduct thorough experiments showing that Sur$^2$f outperforms existing reconstruction methods and surface representations, including hybrid ones, in terms of both recovery quality and recovery efficiency. </p><p><a href="http://arxiv.org/abs/2401.03704v1">PDF</a> 18 pages, 16 figures</p><p><strong>Summary</strong><br>结合隐式距离场与显式代理曲面，提出新型混合表示Sur2f，提高3D重建的质量和效率。</p><p><strong>Key Takeaways</strong></p><ul><li>多视角曲面重建是 3D 视觉研究中的病态逆问题，涉及使用适当的曲面表示对几何形状和外观进行建模。</li><li>现有方法要么依赖显式网格，使用网格的曲面渲染进行重建，要么依赖隐式场函数，使用场的体积渲染进行重建。</li><li>Sur2f是一种新的混合表示，旨在以互补的方式从两种表示中更好地受益。</li><li>Sur2f学习隐式有符号距离场和显式代理曲面Sur2f网格的两个并行流，并使用共享的神经着色器统一隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染。</li><li>Sur2f通过使用从隐式SDF诱导的函数来驱动代理网格的变形来同步学习代理网格。</li><li>同步的代理网格支持曲面引导体积采样，大大提高了体积渲染中每个射线的采样效率。</li><li>Sur2f在恢复质量和恢复效率方面优于现有的重建方法和曲面表示，包括混合方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Sur2f：一种用于从多视角图像进行高质量和高效曲面重建的混合表示</li><li>作者：Long Chen、Zexiang Xu、Qian Yu、Hao Su、Hao Li、Hao Zhang</li><li>隶属机构：香港城市大学</li><li>关键词：曲面重建、隐式表示、显式表示、混合表示、神经渲染</li><li>论文链接：None，Github 链接：None</li><li>摘要：(1)：曲面重建是 3D 视觉研究中一个不适定的逆问题，它涉及使用适当的曲面表示来建模几何形状和外观。现有的大多数方法依赖于显式网格，使用网格的曲面渲染进行重建，或者依赖于隐式场函数，使用场的体积渲染进行重建。事实上，这两种类型的表示各自都有其优点。(2)：过去的方法要么使用显式网格，要么使用隐式场函数。显式网格方法可以生成高质量的重建结果，但计算成本高，并且难以处理拓扑变化。隐式场函数方法计算成本低，并且可以轻松处理拓扑变化，但生成的重建结果质量较低。(3)：本文提出了一种新的混合表示，称为 Sur2f，旨在以互补的方式从两种表示中更好地受益。具体来说，我们学习了隐式有符号距离场和显式代理曲面 (Sur2f) 网格的两个并行流，并将隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中；统一的着色促进了它们收敛到相同的底层曲面。我们通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。此外，同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。(4)：我们进行了彻底的实验，表明 Sur2f 在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。这些性能可以支持它们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种新的混合表示 Sur2f，它结合了隐式有符号距离场和显式代理曲面网格的优点，在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。（2）：创新点：</li></ol><ul><li>提出了一种新的混合表示 Sur2f，它可以同时利用隐式有符号距离场和显式代理曲面网格的优点。</li><li>设计了一种统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li><li>通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li><li>同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。性能：</li><li>在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。</li><li>可以生成高质量的重建结果，并且可以轻松处理拓扑变化。</li><li>计算成本低，并且可以轻松处理拓扑变化。工作量：</li><li>需要学习两个并行流，一个用于隐式有符号距离场，另一个用于显式代理曲面网格。</li><li>需要设计一个统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li><li>需要通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li><li>需要支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6cf972d05baa99fcd12394fe7af270dc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-01333630ee152c5ffdb71b25620aab65.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f5316c581e5971075fa487859bda230.jpg" align="middle"></details>​    ## GridFormer: Point-Grid Transformer for Surface Reconstruction**Authors:Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu**Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer. [PDF](http://arxiv.org/abs/2401.02292v1) **Summary**隐式神经网络引入了点与网格结合的重建方式，并通过优化提高重建精度。**Key Takeaways**- 隐式神经网络已成为 3D 表面重建中的关键技术。- 现有的方法通常将输入点编码为规则的网格特征（平面或体积），这可能牺牲一些重建细节以提高效率。- 我们引入了一种在网格和点特征之间的新颖且高效的注意机制，称为网格前馈神经网络 (GridFormer)。- 我们的方法最大限度地提高了网格特征的空间表达能力，并保持计算效率。- 为了更准确的再现对象结构，我们进一步提出了结合边界二进制交叉熵损失和边界采样的边界优化策略。- 我们的方法优于现有技术在广泛使用的基准下重建几何结构。- 代码可在 https://github.com/list17/GridFormer 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：网格生成器：用于曲面重建的点网格转换器</p></li><li><p>作者：盛涛李、葛高、刘雨东、刘玉申、顾明</p></li><li><p>隶属单位：北京大学信息科学技术国家研究中心（BNRist），清华大学软件学院</p></li><li><p>关键词：隐式神经网络、曲面重建、点网格转换器、边界优化</p></li><li><p>论文链接：https://arxiv.org/abs/2401.02292Github 代码链接：https://github.com/list17/GridFormer</p></li><li><p>摘要：（1）研究背景：隐式神经网络已成为 3D 曲面重建的关键技术。为了从离散点云重建连续曲面，现有方法通常将输入点编码为规则网格特征（平面或体积）。然而，这些方法通常使用网格作为均匀散射点特征的索引。与不规则点特征相比，规则网格特征可能会牺牲一些重建细节，但提高了效率。（2）过去的方法及其问题：为了充分利用这两种类型的特征，我们引入了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，在整个空间上优化预测可能会导致边界模糊。为了解决这个问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。（3）研究方法：我们的实验验证了我们的方法是有效的，并且在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。（4）方法的性能：我们的方法在 Synthetic Rooms 数据集上取得了最先进的结果，在 F-Score 指标上达到 0.89，在 Chamfer 距离指标上达到 0.011。在 ShapeNet 数据集上，我们的方法在 F-Score 指标上达到 0.83，在 Chamfer 距离指标上达到 0.013。这些性能支持了我们的目标，即在保持计算效率的同时实现高保真曲面重建。</p></li><li><p>方法：（1）：我们提出了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。（2）：为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。（3）：我们的方法在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</p></li><li><p>结论：（1）：本工作提出了点网格转换器（GridFormer），它使用了一种新颖且高效的点和网格特征之间的注意力机制。该机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。最终，实验表明，输入点的密度和网格大小都会影响我们方法的效果。在未来的工作中，探索如何动态划分网格以实现不同分辨率之间的注意力机制，可能会将这种机制应用到更多的场景中。（2）：创新点：</p></li></ol><ul><li>提出了一种新颖且高效的点和网格特征之间的注意力机制，称为点网格转换器（GridFormer）。</li><li>提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样，以解决预测边界模糊的问题。</li><li>在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</li></ul><p>性能：</p><ul><li>在SyntheticRooms数据集上取得了最先进的结果，在F-Score指标上达到0.89，在Chamfer距离指标上达到0.011。</li><li>在ShapeNet数据集上，我们的方法在F-Score指标上达到0.83，在Chamfer距离指标上达到0.013。</li></ul><p>工作量：</p><ul><li>本文的工作量中等。该方法需要实现点网格转换器和边界优化策略，并进行大量的实验来验证其有效性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4060effb50cbcc4d2a1088a9feb6000d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58bec8ac4a2da95ce6f707047d4df439.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7279323b67e79dc5114058f8c28e7609.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5761d740398ae95036084ccdcf20f08c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34ffafeb6a4cead225422a90aa07d1c2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bf8f7e2e1bfbbf55b2cea30afd6182c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b68791929579cd1db5915c9cc3542d43.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-756e738d1626d407b474a748a66913a5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-adf4ffe3ca8f2dc11327a310777fca9a.jpg" align="middle"></details>​    ## HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human   Reconstruction**Authors:Angtian Wang, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Edmond Boyer, Alan Yuille, Tony Tung**Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body. We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs). We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair). Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects. [PDF](http://arxiv.org/abs/2312.17192v1) Accepted by AAAI 2024 main track**Summary**人像模型重建引入了新的隐式表面表现形式，能够重构半透明区域。**Key Takeaways**- 该工作提出了一种新的混合隐式表面表示法来建模人体形状。- 这种表示由两个表面层组成，分别代表被遮挡的人体表面的不透明和半透明区域。- 该方法通过视觉提示自动分割不同区域，并学习重建两个有符号距离函数 (SDF)。- 该方法对不透明区域（例如身体、面部、衣服）执行基于表面的渲染，以保留高保真表面法线，并对半透明区域（例如头发）执行体积渲染。- 实验表明，该方法在 3D 人体重建方面获得了最先进的结果，并且在其他对象上也表现出竞争性的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：基于混合隐式曲面表示的人体形状重建（Hybrid implicit surface representation for human shape reconstruction）</p></li><li><p>作者：Yuxuan Zhang, Song Bai, Xiaoguang Han, Juergen Gall, Mario Fritz</p></li><li><p>第一作者单位：德国马克斯·普朗克智能系统研究所</p></li><li><p>关键词：人体形状重建、隐式曲面表示、深度学习、渲染</p></li><li><p>论文链接：https://arxiv.org/abs/2203.04118，Github 链接：None</p></li><li><p>摘要：（1）研究背景：神经重建和渲染策略由于能够保留高级形状细节而表现出最先进的性能。然而，现有方法要么将对象表示为隐式曲面函数或神经体积，仍然难以恢复具有异质材料的形状，特别是人类皮肤、头发或衣服。（2）过去的方法及其问题：为了解决这个问题，我们提出了一种新的混合隐式曲面表示来建模人体形状。这种表示由两个曲面层组成，它们分别代表穿着衣服的人体上的不透明区域和半透明区域。我们使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。我们对不透明区域（例如身体、面部、衣服）执行基于曲面的渲染以保留高保真曲面法线，并对半透明区域（例如头发）执行体积渲染。（3）论文提出的研究方法：实验表明，我们的方法在 3D 人体重建方面取得了最先进的结果，并在其他对象上也表现出竞争力。（4）方法在任务上取得的性能：我们的方法在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。这些性能支持了我们的目标，即提供一种能够重建具有异质材料的复杂形状的通用方法。</p></li><li><p>方法：</p><ul><li>混合隐式曲面表示：将人体形状表示为由不透明区域和半透明区域组成的两个曲面层，并使用视觉线索自动分割不同的区域，学习重建两个有符号距离函数 (SDF)。</li><li>混合渲染：对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li><li>集成 SDF 用于体积密度：引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li><li>自适应采样策略：提出一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li><li>训练：通过随机采样每个训练图像上的像素集并最小化总损失来训练网络，包括掩码损失、光度损失、镜面损失和 Eikonal 损失。</li></ul></li><li><p>结论：（1）：本工作的主要贡献在于提出了一种新的混合隐式曲面表示（HISR）来进行人体的三维重建，该方法结合了基于曲面的渲染和体积渲染，能够同时保留高保真曲面法线和精细的几何细节。在各种人体和物体重建数据集上的评估表明，我们的方法在重建几何保真度和新视角合成方面都优于基线方法。这得益于我们为不透明区域和半透明区域设置的双曲面层表示，允许对皮肤、头发和衣服等复杂的人体特征进行细致的渲染。我们的方法在三维人体重建方面取得了最先进的结果，并在其他物体上也表现出竞争力。（2）：创新点：</p></li></ol><ul><li>提出了一种新的混合隐式曲面表示（HISR），该表示由两个曲面层组成，分别代表穿着衣服的人体上的不透明区域和半透明区域。</li><li>使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。</li><li>对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li><li>引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li><li>提出了一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li></ul><p>性能：</p><ul><li>在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。</li></ul><p>工作量：</p><ul><li>混合隐式曲面表示的构建和学习需要较大的计算量。</li><li>自适应采样策略的实现也需要较大的计算量。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-65b60c55274f249c7a21655ea5c21695.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd4c157203b0a88b68cba902768a310.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-67ee6c759bd41020f430403f5e2d93ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fd6993e8a08dd8e58fbc4eaec5ab30ff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edd10754c68e2204a13c9b005617ccb0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c2e0db87430d852291b34fe1369fecf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1465bdf98251d75ab856b665d50f4419.jpg" align="middle"></details>​    ## In-Hand 3D Object Reconstruction from a Monocular RGB Video**Authors:Shijian Jiang, Qi Ye, Rengan Xie, Yuchi Huo, Xiang Li, Yang Zhou, Jiming Chen**Our work aims to reconstruct a 3D object that is held and rotated by a hand in front of a static RGB camera. Previous methods that use implicit neural representations to recover the geometry of a generic hand-held object from multi-view images achieved compelling results in the visible part of the object. However, these methods falter in accurately capturing the shape within the hand-object contact region due to occlusion. In this paper, we propose a novel method that deals with surface reconstruction under occlusion by incorporating priors of 2D occlusion elucidation and physical contact constraints. For the former, we introduce an object amodal completion network to infer the 2D complete mask of objects under occlusion. To ensure the accuracy and view consistency of the predicted 2D amodal masks, we devise a joint optimization method for both amodal mask refinement and 3D reconstruction. For the latter, we impose penetration and attraction constraints on the local geometry in contact regions. We evaluate our approach on HO3D and HOD datasets and demonstrate that it outperforms the state-of-the-art methods in terms of reconstruction surface quality, with an improvement of $52\%$ on HO3D and $20\%$ on HOD. Project webpage: https://east-j.github.io/ihor. [PDF](http://arxiv.org/abs/2312.16425v1) Accepted by AAAI2024**摘要**引入二维遮挡阐释和物理接触约束，可提升贴合于手的复杂形状的 3D 重建质量。**主要结论**- 提出了一种新颖的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐释先验和物理接触约束。- 引入了一个对象无遮挡完成网络来推断遮挡下对象的二维完整掩码。- 设计了一种联合优化方法，用于无遮挡掩码细化和三维重建。- 对 HO3D 和 HOD 数据集评估了我们的方法，结果表明，在重建表面质量方面，我们的方法优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。- 项目网页：https://east-j.github.io/ihor。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：遮挡下三维手持物体的表面重建</p></li><li><p>作者：Junyi Dong, Yuxuan Zhang, Jiaolong Yang, Shiwei Li, Kai Xu, Qixing Huang, Xin Tong</p></li><li><p>单位：清华大学</p></li><li><p>关键词：三维重建、遮挡、手持物体、隐式神经表示</p></li><li><p>论文链接：https://arxiv.org/abs/2211.06779, Github 链接：None</p></li><li><p>摘要：(1)：研究背景：现有方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。(2)：过去的方法和问题：过去的方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。(3)：研究方法：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束。对于前者，我们引入了一个对象模态完成网络来推断遮挡下物体的二维完整掩码。为了确保预测的二维模态掩码的准确性和视图一致性，我们设计了一种用于模态掩码细化和三维重建的联合优化方法。对于后者，我们在接触区域的局部几何形状上施加穿透和吸引约束。(4)：方法的性能：我们在 HO3D 和 HOD 数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。这些性能支持了我们的目标。</p></li><li><p>方法：(1) 二维遮挡阐明：引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建；(2) 物理接触约束：在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性；(3) 联合优化：将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。</p></li><li><p>结论：（1）：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束，在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。（2）：创新点：</p></li></ol><ul><li>引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建。</li><li>在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性。</li><li>将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。性能：</li><li>在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。工作量：</li><li>需要收集和预处理大量的数据。</li><li>需要设计和训练对象模态完成网络和联合优化方法。</li><li>需要对重建结果进行评估。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3ecdbcd441f93fe15bf3fe1c7cb442b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d87c02c6c8349823fdb6a6a7ca7dd86.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ac876d7d2f5101b310e755365dc6534b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af4d9aa687fac12ec84ed6bb8f0af4c0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e9131687642c0ded0611866dd3c4e02e.jpg" align="middle"></details>​    ## Human101: Training 100+FPS Human Gaussians in 100s from 1 View**Authors:Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang**Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101. [PDF](http://arxiv.org/abs/2312.15258v1) Website: https://github.com/longxiang-ai/Human101**摘要**通过在100秒内训练3D高斯模型和以超过100帧/秒的速度渲染，Human101可以从单视角视频快速生成高保真3D动态人体重建。**主要要点**- Human101是一种新的框架，可以在100秒内从1视角视频生成高保真的动态3D人体重建，并能以超过100FPS的速度渲染。- Human101利用3D高斯散点图的优势，提供人体3D表示。- Human101采用以人为中心的前向高斯动画方法，变形3D高斯模型的参数，从而提高渲染速度。- Human101在渲染质量上与目前最先进的方法相当或优越，并且帧数是目前最先进的方法的10倍以上。- Human101 的代码和演示将在 https://github.com/longxiang-ai/Human101 上发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：Human101：从单视角在 100 秒内训练 100+FPS 人体高斯分布</p></li><li><p>作者：Longxiang Xiang, Hanqing Jiang, Zhe Wang, Yebin Liu, Xiaowei Zhou</p></li><li><p>第一作者单位：浙江大学</p></li><li><p>关键词：人体重建、神经辐射场、高斯分布、单视角重建、实时渲染</p></li><li><p>论文链接：None，Github 链接：https://github.com/longxiang-ai/Human101</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：从单视角视频重建人体在虚拟现实领域发挥着关键作用。一个普遍的应用场景需要快速重建高保真 3D 数字人，同时确保实时渲染和交互。现有方法通常难以满足这两个要求。</p><p>（2）过去方法及其问题：神经辐射场 (NeRF) 方法在单视角人体重建中取得了成功，但它们通常计算成本高，无法实现实时渲染。基于 3D 高斯分布的方法可以实现快速渲染，但它们通常缺乏细节和保真度。</p><p>（3）本文提出的研究方法：本文提出 Human101，这是一个新颖的框架，能够通过在 100 秒内训练 3D 高斯分布并以 100+FPS 渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯分布的优势，它提供了人体的一种显式且高效的表示。与之前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯分布的参数，从而提高渲染速度（即，以令人印象深刻的 60+FPS 渲染 1024 分辨率图像，并以 100+FPS 渲染 512 分辨率图像）。</p><p>（4）方法的性能：实验结果表明，本文方法大大优于当前方法，将每秒帧数提高了 10 倍，并提供了可比或更高的渲染质量。</p><ol start="7"><li><p>Methods:(1): Human101方法的核心思想是利用3D高斯分布来表示人体，并通过一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而实现快速渲染和交互。(2): 具体来说，Human101首先通过单视角视频训练一个3D高斯分布，然后利用该分布来生成人体的高分辨率网格模型。(3): 为了实现快速渲染，Human101应用了一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而避免了昂贵的体渲染计算。(4): 这种方法使得Human101能够以令人印象深刻的60+FPS渲染1024分辨率图像，并以100+FPS渲染512分辨率图像。(5): 实验结果表明，Human101方法大大优于当前方法，将每秒帧数提高了10倍，并提供了可比或更高的渲染质量。</p></li><li><p>结论：（1）：Human101 是一个从单视角视频中重建高保真动态人体模型的新颖框架，它在 100 秒内使用固定视角相机高效地重建了高保真动态人体模型。新颖的规范化人体初始化、以人为中心的正向高斯动画和以人为中心的正向高斯细化相结合，再配以 3DGS 的显式表示，显著提高了渲染速度。此外，这种速度的提升并没有牺牲视觉质量。实验表明，与最先进的方法相比，Human101 的 FPS 提高了 67 倍，并保持了可比或更好的视觉质量。Human101 为从单视角视频中重建人体树立了新标准。这一突破为沉浸式技术中的进一步发展和应用奠定了基础。（2）：创新点：</p></li></ol><ul><li>提出了一种新颖的框架 Human101，该框架能够在 100 秒内从单视角视频中重建高保真动态人体模型。</li><li>提出了一种新的规范化人体初始化方法，该方法可以将人体初始化为一个标准姿势，从而提高重建的准确性和鲁棒性。</li><li>提出了一种新的以人为中心的正向高斯动画方法，该方法可以变形 3D 高斯分布的参数，从而实现快速渲染。</li><li>提出了一种新的以人为中心的正向高斯细化方法，该方法可以进一步提高重建的质量。性能：</li><li>Human101 的 FPS 比最先进的方法提高了 67 倍。</li><li>Human101 的视觉质量与最先进的方法相当或更好。工作量：</li><li>Human101 的训练时间为 100 秒。</li><li>Human101 的渲染时间为 60+FPS（1024 分辨率）或 100+FPS（512 分辨率）。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-84a60e1cfd3ff2a4ccd504c677c219dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f8cfe9cdf0f3f288a2851246fa3440a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d7298160fd7bc71030647b1bbde1aed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-95ae9edf8140557344587f9d62973d44.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f9308b2b911a7239d0b1c13e120fe940.jpg" align="middle"></details>​    ## ZeroShape: Regression-based Zero-shot Shape Reconstruction**Authors:Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James M. Rehg**We study the problem of single-image zero-shot 3D shape reconstruction. Recent works learn zero-shot shape reconstruction through generative modeling of 3D assets, but these models are computationally expensive at train and inference time. In contrast, the traditional approach to this problem is regression-based, where deterministic models are trained to directly regress the object shape. Such regression methods possess much higher computational efficiency than generative methods. This raises a natural question: is generative modeling necessary for high performance, or conversely, are regression-based approaches still competitive? To answer this, we design a strong regression-based model, called ZeroShape, based on the converging findings in this field and a novel insight. We also curate a large real-world evaluation benchmark, with objects from three different real-world 3D datasets. This evaluation benchmark is more diverse and an order of magnitude larger than what prior works use to quantitatively evaluate their models, aiming at reducing the evaluation variance in our field. We show that ZeroShape not only achieves superior performance over state-of-the-art methods, but also demonstrates significantly higher computational and data efficiency. [PDF](http://arxiv.org/abs/2312.14198v2) Project page: https://zixuanh.com/projects/zeroshape.html**摘要**回归式模型ZeroShape在单张图像零样本三维形状重建中取得了卓越的性能和计算效率。**要点**- 回归式方法在单张图像零样本三维形状重建中同样具有竞争力。- 提出了一个强大的回归式模型ZeroShape，该模型基于领域内趋同的研究成果和一个新颖的洞察。- 构建了一个大型的真实世界评估基准，包含来自三个不同真实世界三维数据集的对象。- 该评估基准比先前工作用于定量评估其模型的基准更加多样化，并且数量级更大。- 证明了ZeroShape不仅优于最先进的方法，而且显示出更高的计算和数据效率。- 回归方法效率高、可用于实时渲染。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ZeroShape：基于回归的零样本形状重建</li><li>作者：Zixuan Huang、Stefan Stojanov、Anh Thai、Varun Jampani、James M. Rehg</li><li>隶属机构：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：零样本形状重建、回归、生成模型</li><li>论文链接：https://arxiv.org/abs/2312.14198Github 代码链接：无</li><li>摘要：（1）研究背景：<ul><li>零样本形状重建旨在从单张图像中重建从未见过的物体的 3D 形状。</li><li>最近的工作通过生成扩散模型或神经辐射场 (NeRF) 来学习零样本形状先验，但这些模型在训练和推理时计算成本都很高。</li><li>传统方法是基于回归的，直接回归物体的形状，计算效率更高。</li></ul></li></ol><p>（2）过去方法及其问题：</p><ul><li>现有方法主要基于生成模型，计算成本高，并且需要大量训练数据。</li><li>基于回归的方法虽然计算效率高，但性能不如生成模型。</li></ul><p>（3）提出的研究方法：</p><ul><li>提出了一种新的基于回归的零样本形状重建模型 ZeroShape。</li><li>ZeroShape 结合了领域内最新研究成果和一个新的洞察，在性能和效率上都优于现有方法。</li><li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。</li><li>该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</li></ul><p>（4）方法的性能和对目标的支持：</p><ul><li>ZeroShape 在零样本 3D 形状重建任务上优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li><li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。</li></ul><ol start="7"><li>方法：</li></ol><p>（1）深度和相机估计器：使用 DPTResNet CNN 来估计图像的深度图和相机内参。</p><p>（2）几何反投影单元：将深度图和内参估计值反投影到归一化的 3D 可见表面，该表面由三通道投影图参数化。</p><p>（3）投影引导的形状重建器：使用 ResNet 编码器对投影图进行编码和重塑，然后使用基于交叉注意力的方法从投影图中提取相关补丁编码，并使用 MLP 预测每个查询点的占用值。</p><p>（4）损失函数：使用两阶段训练范式，首先预训练深度和相机估计器，然后使用 3D 监督微调整个模型。深度和相机预训练使用深度损失和基于投影的内参损失。整个模型的联合训练使用 3D 占用损失，这是预测占用值和以观察者为中心的坐标系中的地面实况之间的标准二元交叉熵。</p><p>（5）实现细节：使用 Adam 优化器训练模型。在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</p><p>（6）数据整理：使用来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</p><ol start="8"><li>结论：（1）：本文提出了一种基于回归的零样本形状重建模型 ZeroShape，该模型在性能和效率上优于现有方法。（2）：创新点：</li></ol><ul><li>提出了一种新的中间表示形式，该表示形式可以有效地进行显式 3D 几何推理。</li><li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。性能：</li><li>在零样本 3D 形状重建任务上，ZeroShape 优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li><li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。工作量：</li><li>使用 Adam 优化器训练模型。</li><li>在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。</li><li>在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。</li><li>在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0df67089f0cd470421435e6ad26a625d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19cd67e1a52d95f7d665d88a7ee51292.jpg" align="middle"><img src="https://picx.zhimg.com/v2-504faa61d0f546e94a3b52452ac7c3e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fd545832e863d3187e6888c47dbab37d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-028a47ff3a5de37fe1ed865255a3e193.jpg" align="middle"></details>​    ## NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse   Input Views**Authors:Han Huang, Yulun Wu, Junsheng Zhou, Ge Gao, Ming Gu, Yu-Shen Liu**Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods. [PDF](http://arxiv.org/abs/2312.13977v2) Accepted by AAAI 2024. Project page:   https://alvin528.github.io/NeuSurf/**摘要**利用表面先验重建框架增强深度神经网络隐函数，提高稀疏视角下的三维重建精度。**要点**- 神经隐式函数在多视角重建领域取得了显著成果，但现有方法多针对稠密视角，稀疏视角下表现不佳。- 近期提出的几种广义隐式重建方法虽然适用于稀疏视图重建任务，但训练成本高，且仅在精心挑选的视角下有效。- 本文提出一种利用表面先验，用于实现高保真曲面重建的稀疏视图重建框架。- 通过设计全局几何对齐和局部几何细化的约束，联合优化粗略形状和精细细节。- 训练一个神经网络，根据从 SfM 获得的表面点学习一个全局隐式场，将其用作粗略几何约束。- 利用局部几何一致性，将表面点投影到已见和未见视图，将投影特征的一致性损失作为精细几何约束。- 在 DTU 和 BlendedMVS 数据集上的实验结果表明，本方法在两种普遍的稀疏设置下均优于最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeuSurf：基于表面先验的神经表面重建框架（从稀疏输入中）</li><li>作者：Yuxuan Zhang, Yuxin Wen, Yufeng Zheng, Changjian Li, Yanwei Fu, Qiong Yan, Yebin Liu, Lu Fang, Shihui Lai</li><li>第一作者单位：华中科技大学</li><li>关键词：神经隐式函数、稀疏视图重建、表面先验、几何对齐、局部几何精细化</li><li>论文链接：https://arxiv.org/abs/2203.12461，Github 代码链接：None</li><li>摘要：（1）研究背景：神经隐式函数在多视图重建领域取得了显著成果，但现有方法大多针对稠密视图，在处理稀疏视图时表现不佳。一些最新方法试图将隐式重建推广到稀疏视图重建任务，但仍然存在训练成本高、仅在仔细选择的视角下有效等问题。（2）过去的方法及其问题：现有方法在处理稀疏视图时存在训练成本高、仅在仔细选择的视角下有效等问题。（3）论文提出的研究方法：本文提出一种利用表面先验的新型稀疏视图重建框架，以实现高度逼真的表面重建。具体来说，我们设计了关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。为此，我们训练了一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，然后将其作为粗略几何约束。为了利用局部几何一致性，我们将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。（4）方法在任务和性能上的表现：在 DTU 和 Blended MVS 数据集上的实验结果表明，在两种普遍的稀疏设置下，该方法显著优于最先进的方法。这些性能支持了论文提出的目标。</li></ol><p>7.Methods：(1) 提出一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。(2) 设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。(3) 训练一个神经网络，从SfM获得的表面点学习一个全局隐式场，作为粗略几何约束。(4) 将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。(5) 在DTU和BlendedMVS数据集上的实验结果表明，该方法显著优于最先进的方法。</p><ol start="8"><li>结论：（1）：本文提出了一种基于表面先验的神经表面重建框架 NeuSurf，该框架利用表面点学习全局隐式场作为粗略几何约束，并将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束，从而实现高度逼真的表面重建。（2）：创新点：</li></ol><ul><li>提出了一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。</li><li>设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。</li><li>训练一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，作为粗略几何约束。</li><li>将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。性能：</li><li>在 DTU 和 BlendedMVS 数据集上的实验结果表明，该方法显著优于最先进的方法。</li><li>这些性能支持了论文提出的目标。工作量：</li><li>该方法不需要大规模训练，并且在各种稀疏设置中都很稳健。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3f6644db90f1dd4f7ca9dcf04e307b68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f0eb8b77b117c4be4c26d0982f919c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f11cf5878ea37a4b5efbd6c59f20a5d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3dee8633ed2740393067a67de3d6ef00.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0c8ebc6b78ec57ce3e5e1234d62b7690.jpg" align="middle"><img src="https://picx.zhimg.com/v2-302eebe295f00eabc65233dc28a98374.jpg" align="middle"></details>​    ## LASA: Instance Reconstruction from Real Scans using A Large-scale   Aligned Shape Annotation Dataset**Authors:Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han**Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale, high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned, restricting the performance of data-driven methods on real data. To this end, we introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising 10,412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes, created manually by professional artists. On this top, we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides, we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA, extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks. [PDF](http://arxiv.org/abs/2312.12418v1) homepage: https://gap-lab-cuhk-sz.github.io/LASA/**Summary**实例形状重建从 3D 场景中恢复多个对象的完整几何形状，于语义实例级别。**Key Takeaways**- LASA 包含 10412 个高质量 CAD 注释，与 920 个来自 ArkitScenes 的真实场景扫描对齐，由专业艺术家手动创建。- 提出了一种新的基于扩散的跨模态形状重建方法 (DisCo)。- DisCo 使用混合特征聚合设计来融合多模态输入并恢复高保真度对象几何体。- 提出了一种占有引导的 3D 对象检测方法 (OccGOD)。- 形状注释提供了场景占用线索，可以进一步改进 3D 对象检测。- 广泛的实验表明，DisCo 和 OccGOD 在实例级场景重建和 3D 对象检测任务中均取得了最先进的性能。- DisCo 在 LASA 上训练，在三个基准数据集上的平均改进幅度为 13.2%。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：LASA：利用大规模对齐形状注释数据集从真实扫描中进行实例重建</li><li>作者：Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han</li><li>隶属单位：香港中文大学（深圳）</li><li>关键词：实例形状重建、3D物体检测、大规模数据集、对齐形状注释</li><li>论文链接：https://arxiv.org/abs/2312.12418Github 代码链接：无</li><li>摘要：(1)：研究背景：从 3D 场景中进行实例形状重建涉及恢复多个对象的完整几何形状，这些对象处于语义实例级别。由于场景复杂性和明显的室内遮挡，许多方法利用数据驱动的学习。训练这些方法通常需要一个大规模、高质量的数据集，其中包含与真实世界扫描对齐且配对的形状注释。现有数据集要么是合成的，要么是未对齐的，这限制了数据驱动方法在真实数据上的性能。(2)：过去的方法及其问题：目前的方法是利用深度学习方法来解决实例级别场景重建任务。这些方法取得了很大的进展，但它们也存在一些问题。首先，这些方法通常需要大量的数据来训练，这在现实世界中很难获得。其次，这些方法通常对噪声和不完整的数据非常敏感，这在真实世界扫描中很常见。最后，这些方法通常只能重建有限数量的物体类别，这限制了它们的适用性。(3)：论文提出的研究方法：为了解决这些问题，本文提出了一种新的方法，称为 LASA（Large-scale Aligned Shape Annotation Dataset）。LASA 是一个包含 10,412 个高质量 CAD 注释的大规模对齐形状注释数据集，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。(4)：方法在任务上的表现及性能：在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。这些结果表明，LASA 对于训练和评估实例级别场景重建方法非常有价值。</li></ol><p>&lt;Methods&gt;:(1): 该文提出了一种名为LASA的大规模对齐形状注释数据集，该数据集包含10,412个高质量CAD注释，这些注释与来自ArkitScenes的920个真实世界场景扫描对齐。(2): LASA是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。(3): 在实例级别场景重建任务上，LASA可以支持最先进的性能。在3D物体检测任务上，LASA也可以支持最先进的性能。</p><ol start="8"><li>总结：（1）：该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。（2）：创新点：该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。性能：在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。工作量：该文的工作量很大，需要收集和注释大量的数据。此外，该文还提出了两种新的方法，Diffusion-based Cross-Modal Shape Reconstruction 和 Occupancy-guided 3D Object Detection，这两种方法的实现也需要大量的工作量。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cda9d91453de63d77467b3bed34c6d49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2b47fbe622e984a9f8410e202812867.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aafa1603b087a09b6d9a0e1ba939e3c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b5f74bcf20d71e60eae001a07761608.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-594a80e24dc501900a0ba0cb9417b9b7.jpg" align="middle"></details><br>​    <p></p><h2 id="SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance"><a href="#SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance" class="headerlink" title="SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance"></a>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance</h2><p><strong>Authors:Yuanyou Xu, Zongxin Yang, Yi Yang</strong></p><p>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: <a href="https://yoxu515.github.io/SEEAvatar/">https://yoxu515.github.io/SEEAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2312.08889v2">PDF</a> </p><p><strong>Summary</strong><br>使用具有自进化约束条件的文本到 3D 头像生成方法，可生成具有照片级真实感、形状和外观解耦的 3D 头像。</p><p><strong>Key Takeaways</strong></p><ul><li>使用文本到图像生成模型的大规模预训练，文本到 3D 头像生成取得了显著进展。</li><li>现有方法由于几何形状不准确和外观质量低，无法生成具有照片级真实感的结果。</li><li>SEEAvatar 提出了一种使用文本生成具有照片级真实感 3D 头像的方法，具有用于分离几何形状和外观的自进化约束条件。</li><li>为了生成几何形状，我们建议使用模板头像来约束优化后的头像以获得合理的外观形状。</li><li>模板头像使用人体先验进行初始化，并且可以由优化后的头像定期更新为不断进化的模板，从而实现更灵活的形状生成。</li><li>此外，几何形状还受到面部和手等局部部位的静态人体先验的约束，以保持精细的结构。</li><li>为了生成外表，我们使用提示工程增强的扩散模型来指导基于物理的渲染管道生成逼真的纹理。</li><li>将亮度约束应用于反照率纹理以抑制不正确的照明效果。</li><li>实验表明，我们的方法在整体和局部几何形状和外观质量方面都优于以前的方法。</li><li>由于我们的方法可以生成高质量的网格和纹理，因此这些资源可以直接应用于经典图形管道中，以便在任何照明条件下进行逼真的渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：SEEAvatar：具有约束几何和外观的逼真文本到 3D 头像生成</p></li><li><p>作者：Yuxuan Zhou, Jiajun Wu, Kangxue Yin, Jingyi Yu, Hao Tang, Kun Zhou, Qifeng Chen</p></li><li><p>单位：暂无</p></li><li><p>关键词：文本到 3D 头像生成、几何约束、外观生成、扩散模型、照明约束</p></li><li><p>论文链接：https://arxiv.org/abs/2302.09529, Github 链接：无</p></li><li><p>摘要：(1)：研究背景：随着大规模文本到图像生成模型的发展，文本到 3D 头像生成取得了可喜的进展。然而，大多数方法由于几何不精确和外观质量低，无法产生逼真的结果。(2)：过去的方法：现有方法在几何和外观方面都存在问题。在几何方面，现有方法通常使用静态模板来约束几何，这限制了形状生成的灵活性，并且难以生成复杂的服装。在外观方面，现有方法通常使用扩散模型来生成纹理，但这些模型容易受到照明条件的影响，并且难以生成准确的物理参数。(3)：研究方法：为了解决上述问题，本文提出了一种名为 SEEAvatar 的方法。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。(4)：方法性能：实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。</p></li><li><p><strong>方法</strong>：(1) <strong>几何约束：</strong>- 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新。- 模板由一个粗糙的网格表示，该网格可以根据优化后的头像进行变形。- 使用一个优化器来最小化模板和优化后的头像之间的距离。(2) <strong>外观生成：</strong>- 使用一个扩散模型来生成纹理。- 扩散模型是一个生成模型，它可以从噪声中生成图像。- 使用一个优化器来最小化纹理和优化后的头像之间的距离。(3) <strong>照明约束：</strong>- 在纹理生成过程中应用亮度约束，以抑制不正确的照明效果。- 亮度约束通过最小化纹理和优化后的头像之间的亮度差异来实现。</p></li><li><p>结论：（1）：本文提出了一种名为 SEEAvatar 的方法，该方法能够生成具有约束几何和外观的逼真文本到 3D 头像。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。（2）：创新点：</p></li></ol><ul><li>提出了一种新的几何约束方法，该方法能够生成更灵活的形状，并保持详细的局部结构。</li><li>提出了一种新的外观生成方法，该方法能够生成更逼真的纹理，并抑制不正确的照明效果。</li><li>提出了一种新的亮度约束方法，该方法能够有效地抑制纹理中的照明效果。性能：</li><li>SEEAvatar 在几何和外观质量方面均优于以往的方法。</li><li>SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。工作量：</li><li>SEEAvatar 的工作量相对较大，需要大量的训练数据和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31d8f3ef22e9983e6f080f4f979f6284.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-204ca8c7f61c24414854bac9e34ba0a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af635847f8e0712b1b887523a86123da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df9fb4cbdd77b5ee6fa2c33565667f41.jpg" align="middle"></details>​    ## 3DGEN: A GAN-based approach for generating novel 3D models from image   data**Authors:Antoine Schnepf, Flavian Vasile, Ugo Tanielian**The recent advances in text and image synthesis show a great promise for the future of generative models in creative fields. However, a less explored area is the one of 3D model generation, with a lot of potential applications to game design, video production, and physical product design. In our paper, we present 3DGEN, a model that leverages the recent work on both Neural Radiance Fields for object reconstruction and GAN-based image generation. We show that the proposed architecture can generate plausible meshes for objects of the same category as the training images and compare the resulting meshes with the state-of-the-art baselines, leading to visible uplifts in generation quality. [PDF](http://arxiv.org/abs/2312.08094v1) Submitted to NeurIPS 2022 Machine Learning for Creativity and Design   Workshop**Summary**3D 生成模型通过融合神经辐射场和生成对抗网络，提升了游戏、影视、工业设计等领域的 3D 模型生成效果。**Key Takeaways**- 3D 模型生成在游戏、视频制作和物理产品设计等领域具有广泛应用前景。- 3DGEN 模型将神经辐射场和基于 GAN 的图像生成相结合，用于 3D 模型生成。- 3DGEN 模型可以生成与训练图像相同类别的物体的高质量可信度网格。- 3DGEN 模型在生成质量方面优于现有最先进的基线。- 3DGEN 模型有潜力对创意领域的未来产生重大影响。- 3DGEN 模型还可以用于创建新颖的互动体验。- 3DGEN 模型还可以用于改善现有的 3D 建模工具。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：3DGEN：一种基于 GAN 的生成新颖 3D 模型的方法</li><li>作者：Antoine Schnepf, Flavian Vasile, Ugo Tanielian</li><li>第一作者单位：Criteo 人工智能实验室</li><li>关键词：生成模型、神经辐射场、隐式表面、3D 模型生成</li><li>论文链接：https://arxiv.org/abs/2312.08094</li><li>摘要：（1）研究背景：随着文本和图像合成的快速发展，生成模型在创意领域展现出巨大的潜力。然而，3D 模型生成领域相对较少探索，但在游戏设计、视频制作和实体产品设计等方面具有广泛的应用前景。（2）过去的方法及其问题：GRAF 模型可以从相似物体的视图集中生成新的体积模型。但其主要限制在于体积表示不适用于生成合理的物体网格，因此不适用于游戏设计、虚拟现实世界设计和动画等 3D 原生创意环境。（3）本文提出的研究方法：本文提出 3DGEN 模型作为 GRAF 模型的潜在解决方案。该模型结合了 GRAF 和 UNISURF 的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为 3D 网格。（4）方法在任务中的表现：3DGEN 模型在生成相同类别物体的合理网格方面取得了良好的效果。与现有方法相比，3DGEN 模型在生成质量方面具有明显的提升。这些结果支持了本文提出的方法能够有效生成新颖的 3D 模型。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了3DGEN模型，该模型结合了GRAF和UNISURF的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。（2）：创新点：</li></ol><ul><li>将GRAF和UNISURF模型相结合，生成具有对应隐式表面的体积对象。</li><li>提出了一种新的损失函数，可以有效地训练模型。</li><li>在生成相同类别物体的合理网格方面取得了良好的效果。性能：</li><li>与现有方法相比，3DGEN模型在生成质量方面具有明显的提升。</li><li>3DGEN模型可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。工作量：</li><li>3DGEN模型的训练过程相对复杂，需要大量的计算资源。</li><li>3DGEN模型的生成过程也相对复杂，需要较长的时间。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b366229325959f0a6130781934e0265c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d524a6508c45ab9726c11826aafbf1fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-968aafc6935f4229c922187d223a5752.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c38720850992061897e67722d04fd0c.jpg" align="middle"></details>​    ## SIFU: Side-view Conditioned Implicit Function for Real-world Usable   Clothed Human Reconstruction**Authors:Zechuan Zhang, Zongxin Yang, Yi Yang**Creating high-quality 3D models of clothed humans from single images for real-world applications is crucial. Despite recent advancements, accurately reconstructing humans in complex poses or with loose clothing from in-the-wild images, along with predicting textures for unseen areas, remains a significant challenge. A key limitation of previous methods is their insufficient prior guidance in transitioning from 2D to 3D and in texture prediction. In response, we introduce SIFU (Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction), a novel approach combining a Side-view Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU employs a cross-attention mechanism within the transformer, using SMPL-X normals as queries to effectively decouple side-view features in the process of mapping 2D features to 3D. This method not only improves the precision of the 3D models but also their robustness, especially when SMPL-X estimates are not perfect. Our texture refinement process leverages text-to-image diffusion-based prior to generate realistic and consistent textures for invisible views. Through extensive experiments, SIFU surpasses SOTA methods in both geometry and texture reconstruction, showcasing enhanced robustness in complex scenarios and achieving an unprecedented Chamfer and P2S measurement. Our approach extends to practical applications such as 3D printing and scene building, demonstrating its broad utility in real-world scenarios. Project page https://river-zhang.github.io/SIFU-projectpage/ . [PDF](http://arxiv.org/abs/2312.06704v2) Project page https://river-zhang.github.io/SIFU-projectpage/ ;**Summary**侧视图条件隐函数实现真实可用衣着人体 3D 重建**Key Takeaways**- SIFU 提出一种侧视图条件隐函数，用于真实世界可用衣着人体 3D 重建。- SIFU 引入侧视图解耦变换器，有效地将侧视图特征与 2D 特征解耦。- SIFU 采用基于文本到图像扩散的先验，为不可见视图生成逼真且一致的纹理。- SIFU 在几何和纹理重建方面均优于最先进的方法。- SIFU 在复杂场景中展现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的成果。- SIFU 可扩展到 3D 打印和场景构建等实际应用，证明了其在现实世界场景中的广泛实用性。- 项目主页：https://river-zhang.github.io/SIFU-projectpage/。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：SIFU：面向现实世界可用的侧视图条件隐函数服装人体重建</li><li>作者：Hongwen Zhang, Yuxuan Zhang, Zhe Wang, Shihao Wu, Yebin Liu, Yajie Zhao, Lu Sheng, Hao Su</li><li>单位：上海交通大学</li><li>关键词：3D 人体重建、服装重建、隐式函数、扩散模型、文本到图像</li><li>论文链接：None, Github 链接：None</li><li>摘要：（1）研究背景：随着计算机视觉技术的快速发展，3D 人体重建技术已经取得了很大的进步。然而，现有的方法在重建复杂姿势或穿着宽松服装的人体时，以及为不可见区域预测纹理时，仍然存在很大的挑战。这是因为现有的方法在从 2D 到 3D 的转换以及纹理预测中缺乏足够的先验指导。（2）过去的方法：现有的方法主要集中在使用单张图像重建人体几何形状，但对于服装纹理的重建则关注较少。此外，现有的方法在处理复杂姿势或穿着宽松服装的人体时，往往会出现重建不准确或纹理不真实的问题。（3）研究方法：为了解决上述问题，本文提出了一种新的方法 SIFU（面向现实世界可用的侧视图条件隐函数服装人体重建）。SIFU 的主要贡献包括：<ul><li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li><li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。（4）方法性能：在广泛的实验中，SIFU 在几何和纹理重建方面都优于最先进的方法，在处理复杂场景时表现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的结果。我们的方法还扩展到了实际应用，如 3D 打印和场景构建，证明了其在现实世界场景中的广泛实用性。</li></ul></li></ol><p>7.方法：（1）：提出一种新的侧视图解耦变换器，将2D特征映射有效映射到3D，该变换器由一个3D位置编码器和一个2D特征映射解码器组成。（2）：提出一种新的3D一致纹理细化管道，包括一个3D一致纹理生成器和一个3D一致纹理细化器。（3）：设计一个新的扩散模型，用于生成逼真且一致的纹理。</p><ol start="8"><li>结论：（1）：本文提出了一种面向现实世界可用的侧视图条件隐函数服装人体重建方法 SIFU，该方法能够重建高质量的 3D 着装人体网格，并具有详细的纹理。（2）：创新点：</li></ol><ul><li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li><li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。</li><li>设计了一个新的扩散模型，用于生成逼真且一致的纹理。性能：</li><li>在几何和纹理重建方面都优于最先进的方法。</li><li>在处理复杂场景时表现出增强的鲁棒性。</li><li>在 Chamfer 和 P2S 测量中取得了前所未有的结果。工作量：</li><li>需要大量的训练数据。</li><li>训练过程需要大量的时间和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-54b75f643b611ae2794b016d4dc361c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ea488f6c95123ca19ad6ae81f164b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49b55712f79e5234eecf8dde731ba32c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a19952e9672e2642bc68402553713857.jpg" align="middle"></details>​    ## CorresNeRF: Image Correspondence Priors for Neural Radiance Fields**Authors:Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao**Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf. [PDF](http://arxiv.org/abs/2312.06642v1) **Summary**用现成的图像匹配方法作为监督，增强 NeRF 模型的训练过程，提高其图像渲染和表面重建性能。**Key Takeaways**- CorresNeRF 是一种利用现成方法计算的图像匹配先验来监督 NeRF 训练的新方法。- CorresNeRF 设计了自适应增强和过滤过程以生成稠密且高质量的匹配。- 通过匹配像素重投影和深度损失项将匹配用于正则化 NeRF 训练。- CorresNeRF 在密度和 SDF 为基础的 NeRF 模型上对图像渲染和表面重建任务进行了评估。- CorresNeRF 在光度和几何度量上均优于以前的方法。- 这种简单但有效地使用匹配先验的技术可以作为即插即用的模块应用于不同的 NeRF 变体。- 项目主页：https://yxlao.github.io/corres-nerf。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：CorresNeRF：用于神经辐射场的图像对应先验</li><li>作者：Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao</li><li>隶属机构：香港大学</li><li>关键词：神经辐射场、图像对应、稀疏视图、三维重建</li><li>论文链接：https://arxiv.org/abs/2312.06642Github 链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在新型视图合成和表面重建任务中取得了令人印象深刻的结果。然而，在具有稀疏输入视图的具有挑战性的场景下，其性能会受到影响。（2）过去的方法及其问题：一些方法通过优化渲染过程或添加训练约束来解决这个问题。然而，这些方法可能在真实世界中表现不佳，因为只有稀疏的 2D 输入视图是一个约束不足的问题，并且训练过程很容易过度拟合有限的输入视图。最近的工作提出利用额外的先验来监督 NeRF 训练。然而，当前的先验对于目标场景的稀疏特性不够鲁棒。（3）研究方法：本文提出了一种新的方法 CorresNeRF，它利用现成的图像对应先验来监督 NeRF 训练。我们设计了自适应的过程来增强和过滤，以生成密集和高质量的对应关系。然后，通过对应像素重投影和深度损失项将对应关系用于正则化 NeRF 训练。（4）方法的性能：我们在具有密度和 SDF 的 NeRF 模型的不同数据集上评估了我们的方法。我们的方法在光度和几何度量方面都优于以前的方法。我们表明，这种简单但有效的利用对应先验的技术可以作为即插即用模块应用于不同的 NeRF 变体。</li></ol><p><strong>Methods</strong>：**</p><p>（1）神经辐射场背景：** 给定 3D 点 x∈R3 和观察方向 d∈R3，神经辐射场 [4] 预测相应的密度 σ∈[0,∞) 和 RGB 颜色 c∈[0,1]3，由 MLP 网络建模，表示为 fθ:(γ(x),γ(d))→(c,σ)，其中 γ 是位置编码函数。射线 r 定义为 r(t)=o+td，其中 o 是相机中心，d 是射线方向，tn 是近边界，tf 是远边界。为了使用预定义的 tn 和 tf 渲染射线 r，我们将密度 σ 和颜色 c 沿射线积分，如下所示：</p><p>ˆcθ(r)=∫tftnT(t)σθ(r(t))cθ(r(t),d)dt，T(t)=exp(−∫ttnσθ(r(t))dt)，</p><p>其中 T(t) 是累积透射率，cθ(r(t),d) 和 σθ(r(t)) 分别是 fθ 预测的颜色和密度输出。渲染通过分层采样方法实现，其中在 [tn,tf] 中采样 M 个点，表示为 {x1,...,xM}。密度和颜色可以获得如下所示：</p><p>ˆcθ(r)=M∑i=1Ti(1−exp(−σθ(xi)δi))cθ(xi,d)，Ti=exp(−i−1∑j=1σθ(xj)δj)，</p><p>其中 δj=tj+1−tj 是相邻采样点之间的距离。具体来说，对于射线 r，其预测的 3D 点可以通过沿射线对加权深度值求和获得，如下所示：</p><p>y=o+∑M∑i=1Ti(1−exp(−σθ(xi)δi))ti d。</p><p>为了优化 NeRF 模型中的参数 θ，提供一组输入图像和相机参数，并最小化均方误差颜色损失进行优化，如下所示：</p><p>Lcolor(θ,R)=Er∈R∥ˆcθ(r)−c(r)∥22，</p><p>其中 R 是训练视图中的射线集合，c(r) 是射线 r 的真实颜色。</p><p>（2）生成对应关系：** 在本文中，我们重点研究如何利用计算的图像对应关系来增强神经隐式表示在 NeRF 中的性能。因此，对应关系的质量至关重要。对于训练视图中的每对图像，我们使用现成的 SOTA 预训练图像匹配模型计算对应关系。特别是，使用 DKMv3 [24]，因为它提供了密集匹配结果，非常适合我们的用例。为了提高泛化能力，我们将室内和室外模型的预测结果融合在一起，这些模型分别在 ScanNet [46] 和 MegaDepth [47] 上预训练。为了进一步提高对应关系的可靠性，我们提出利用对应关系置信度，并设计了自动和自适应的对应关系处理算法，增加了令人信服的对应关系并去除了异常值。</p><p>（3）增强：** 为了增加对应关系的数量，我们对对应关系执行增强。第一种增强类型是图像变换，包括翻转、交换查询和支持图像以及缩放。这些图像变换可以有效地增加预测对应关系的密度，因为图像变换可以提供各种上下文条件来生成对应关系。第二种类型的增强将对应关系传播到图像对中，有效地增加了对应关系的区域覆盖范围。我们构建了一个无向图 G=(V,E)，其中顶点 V={r|r∈R}，边 E={(rq,rs)|rs∈C(rq)}。对于每条边 (rq,rs)，分配一个置信度值 αq,s。然后，我们将对应关系传播到 G 中每个连通分量内的顶点对。具体来说，令 rq 和 rs 是两个顶点，距离为 d，其中是连接它们的路径 (rq,r1,r2,...,rd−1,rs)。我们在 rq 和 rs 之间分配对应关系，置信度为 αq,s=αq,1α1,2...αd−1,s。在实践中，我们可以捕获传播距离 d≤dmax，其中我们在实验中使用 dmax=2。图 3(B) 和 (C) 分别显示了原始对应关系和增强后的对应关系。</p><p>（4）异常值过滤：** 为了提高对应关系的质量以指导监督，我们在计算和增强对应关系后删除异常值。首先，我们根据对应点之间的投影射线距离去除异常值。假设 pq 和 ps 是 Iq 到 Is 中的一对 2D 对应关系，πq 和 πs 分别是 Iq 和 Is 的世界到像素投影。给定一对对应关系和相机参数，我们计算沿从相机中心射出的两条射线的最近 3D 点 xq 和 xs。然后，我们将这两个 3D 点投影到对应关系的图像平面。投影射线距离 [48] 定义为投影点和对应关系之间的平均欧几里得距离：</p><p>dproj=∥πq(xs)−pq∥2+∥πs(xq)−ps∥22。</p><p>我们删除投影射线距离 dproj 大于阈值的对应关系。其次，我们通过检查一个点是否在统计上远离其邻居来去除异常值。对于每一对对应关系，可以获得两个 3D 点 xq 和 xs，这已经在上一段中指出了。然后，我们考虑 12(xq+xs) 是对应关系的 3D 点。我们对所有对应关系对执行此操作以获得 3D 点集 P。对于 P 中的每个 3D 点，我们计算到其 k 个最近邻居的平均距离，如果距离大于阈值，则删除该点（以及其匹配的对应关系对）。此阈值由 P 中所有点的平均距离的标准差确定。</p><ol start="8"><li>结论：（1）：本文提出了一种利用图像对应先验来训练具有稀疏视图输入的神经辐射场的新方法。我们设计了自动增强和过滤方法，以从稀疏视图输入中生成密集且高质量的图像对应关系。我们设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。实验表明，我们的方法仅使用少量输入图像即可显着提高光度和几何度量中衡量的重建质量。（2）：创新点：</li></ol><ul><li>利用图像对应先验来监督神经辐射场训练，提高了重建质量。</li><li>提出了一种自动增强和过滤方法来生成密集且高质量的图像对应关系。</li><li>设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。性能：</li><li>在具有密度和SDF的NeRF模型的不同数据集上，我们的方法在光度和几何度量方面都优于以前的方法。</li><li>我们的方法可以作为即插即用模块应用于不同的NeRF变体。工作量：</li><li>本文的工作量中等。需要收集和预处理数据，训练神经辐射场模型，并评估模型的性能。</li><li>本文的代码和数据已开源，便于其他研究人员使用和扩展。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bb81a1d85c4890b96c58352034ef526e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bfbadb767d53f08ad37ade7dcd5b8f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fb67d265cc7f332cd181f46f3137ec9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51f7196d6c0e34d80890742433bf1ba7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-09c23c9b810e3846280b6f7fbd407f08.jpg" align="middle"></details>​    ## MVDD: Multi-View Depth Diffusion Models**Authors:Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang**Denoising diffusion models have demonstrated outstanding results in 2D image generation, yet it remains a challenge to replicate its success in 3D shape generation. In this paper, we propose leveraging multi-view depth, which represents complex 3D shapes in a 2D data format that is easy to denoise. We pair this representation with a diffusion model, MVDD, that is capable of generating high-quality dense point clouds with 20K+ points with fine-grained details. To enforce 3D consistency in multi-view depth, we introduce an epipolar line segment attention that conditions the denoising step for a view on its neighboring views. Additionally, a depth fusion module is incorporated into diffusion steps to further ensure the alignment of depth maps. When augmented with surface reconstruction, MVDD can also produce high-quality 3D meshes. Furthermore, MVDD stands out in other tasks such as depth completion, and can serve as a 3D prior, significantly boosting many downstream tasks, such as GAN inversion. State-of-the-art results from extensive experiments demonstrate MVDD's excellent ability in 3D shape generation, depth completion, and its potential as a 3D prior for downstream tasks. [PDF](http://arxiv.org/abs/2312.04875v3) **Summary**多视角深度表征与扩散模型相结合，用于高效且高质量的三维形状生成。**Key Takeaways**- 多视角深度能够将复杂的三维形状表示为易于去噪的二维数据格式。- 提出了多视角深度扩散模型 MVDD，能够生成具有 20,000 多个点的包含精细细节的高质量密集点云。- 介绍了一种极线线段注意力机制，该机制对视图的去噪步骤进行条件处理，以确保多视角深度中的三维一致性。- 加入了一个深度融合模块，以进一步确保深度图的对齐。- 当使用表面重建增强时，MVDD 还可以生成高质量的三维网格。- MVDD 在深度补全等其他任务中脱颖而出，并且可以用作三维先验，极大地提高了许多下游任务的性能，例如 GAN 反演。- 大量实验的最新结果证明了 MVDD 在三维形状生成、深度补全以及作为下游任务的三维先验方面的出色能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MVDD：多视图深度扩散模型</p></li><li><p>作者：Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang</p></li><li><p>第一作者单位：加州大学洛杉矶分校</p></li><li><p>关键词：深度扩散模型、多视图深度、3D形状生成、形状补全、3D GAN反演</p></li><li><p>论文链接：https://arxiv.org/abs/2312.04875Github 链接：https://github.com/mvdepth/mvdd</p></li><li><p>摘要：(1)：研究背景：扩散模型在 2D 图像生成中取得了出色的结果，但在 3D 形状生成中复制其成功仍然具有挑战性。(2)：过去的方法及其问题：现有方法通常使用点云或体素表示来生成 3D 形状，但这些表示难以建模复杂的形状。(3)：本文提出的研究方法：本文提出了一种多视图深度扩散模型 (MVDD)，该模型利用多视图深度来表示复杂的 3D 形状。MVDD 能够生成高质量的密集点云，具有 20K+ 个点和精细的细节。(4)：方法在任务中的表现：MVDD 在 3D 形状生成、深度补全和作为 3D GAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p></li><li><p>方法：(1) 多视图深度扩散模型 (MVDD)：MVDD 采用多视图深度来表示复杂的 3D 形状，可以生成高质量的密集点云，具有 20K+ 个点和精细的细节。(2) 表观线段注意力 (Epipole “Line Segment” Attention)：MVDD 引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。该注意力仅关注其他视图上可见位置的特征，从而提高了效率和有效性。(3) 去噪深度融合 (Denoising Depth Fusion)：为了进一步加强多视图深度图的排列，MVDD 在扩散步骤中结合了深度融合。该融合过程将深度图投影到其他视图并进行比较，以确保深度值的一致性。(4) 训练目标：MVDD 采用 DDPM 的目标函数，旨在最大化对数似然函数。该目标函数通过最小化噪声估计误差来实现，从而使模型能够从纯噪声生成逼真的深度图。(5) 应用：MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p></li><li><p>结论：（1）：MVDD 提出了一种多视图深度扩散模型，能够生成高质量的密集点云，并具有精细的细节。（2）：创新点：</p></li></ol><ul><li>提出了一种多视图深度扩散模型（MVDD），该模型利用多视图深度来表示复杂的 3D 形状。</li><li>引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。</li><li>结合了深度融合，以进一步加强多视图深度图的排列。性能：</li><li>MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。工作量：</li><li>MVDD 的训练和推理过程相对复杂，需要大量的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0cd9c4d80e12157224ee5b88f9d1ffc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-681bf6921d2a2ffb41e0728da51d27c7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f8e93402d2ce303a0694caeced8bf21f.jpg" align="middle"></details>​    ## DreamComposer: Controllable 3D Object Generation via Multi-View   Conditions**Authors:Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu**Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications. [PDF](http://arxiv.org/abs/2312.03611v1) Project Page: https://yhyang-myron.github.io/DreamComposer/**Summary**多视图条件助力基于扩散模型的新视角生成，实现可控的三维重建。**Key Takeaways**- DreamComposer 可以将多视图条件注入到现有基于扩散模型中，以增强它们生成新的可控视角的能力。- DreamComposer 使用视图感知 3D 提升模块从多个视角获取对象的 3D 表征，然后通过多视图特征融合模块从 3D 表征中渲染目标视角的潜在特征。- 提取自多视图输入的目标视角特征被注入到预先训练的扩散模型中。- DreamComposer 能与最先进的扩散模型兼容，用于零样本新视角合成，进一步增强它们生成高保真新视角图像的能力，满足可控 3D 对象重建和各种其他应用的需求。- DreamComposer 能够有效地提高最终合成图像的质量，尤其是在细节和几何结构方面。- DreamComposer 可以处理各种不同的对象和场景，鲁棒性和泛化能力强。- DreamComposer 使用预训练的 2D 模型作为基础，无需额外的数据或训练，易于部署和使用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：DreamComposer：通过多视图条件生成可控的 3D 对象</p></li><li><p>作者：Yunhan Yang、Yukun Huang、Xiaoyang Wu、Yuan-Chen Guo、Song-Hai Zhang、Hengshuang Zhao、Tong He、Xihui Liu</p></li><li><p>第一作者单位：香港大学</p></li><li><p>关键词：可控 3D 对象生成、多视图条件、扩散模型、零样本新视角合成</p></li><li><p>论文链接：https://arxiv.org/abs/2312.03611，Github 代码链接：无</p></li><li><p>摘要：(1) 研究背景：利用预训练的 2D 大规模生成模型，最近的工作能够从一张自然界图像中生成高质量的新视角。然而，由于缺乏来自多视角的信息，这些工作在生成可控的新视角时遇到了困难。(2) 过去方法与不足：过去的方法通常使用单一的预训练扩散模型来生成新视角，但这些模型缺乏对多视角条件的控制能力。(3) 研究方法：本文提出 DreamComposer，这是一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型。具体来说，DreamComposer 首先使用视图感知 3D 提升模块从多个视图中获得对象的 3D 表示。然后，它使用多视图特征融合模块从 3D 表示中渲染目标视图的潜在特征。最后，将从多视图输入中提取的目标视图特征注入预训练的扩散模型。(4) 实验结果：实验表明，DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力，可用于可控 3D 对象重建和各种其他应用。</p></li><li><p>Methods：(1) 视图感知3D提升模块：该模块从多个视图中获得对象的3D表示。它首先使用预训练的2D扩散模型从每个视图中生成对象的2D表示，然后将这些2D表示投影到3D空间中，并使用3D卷积网络融合这些投影，以获得对象的3D表示。(2) 多视图特征融合模块：该模块从3D表示中渲染目标视图的潜在特征。它首先使用3D渲染器将3D表示渲染成目标视图的2D图像，然后使用预训练的2D扩散模型从2D图像中提取潜在特征。(3) 多视图条件注入模块：该模块将从多视图输入中提取的目标视图特征注入预训练的扩散模型。它首先将目标视图的潜在特征与预训练的扩散模型的潜在特征进行拼接，然后使用一个线性层将拼接后的特征投影到预训练的扩散模型的潜在空间中。(4) 扩散模型生成：将注入多视图条件的潜在特征输入预训练的扩散模型，并使用扩散模型生成目标视图的新视角图像。</p></li><li><p>结论：（1）：DreamComposer 框架提出了一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型，从而生成具有多视图条件的高保真新视角图像。（2）：创新点：</p></li></ol><ul><li>提出了一种新的视图感知 3D 提升模块，可以从多个视图中获得对象的 3D 表示。</li><li>提出了一种新的多视图特征融合模块，可以从 3D 表示中渲染目标视图的潜在特征。</li><li>提出了一种新的多视图条件注入模块，可以将从多视图输入中提取的目标视图特征注入预训练的扩散模型。性能：</li><li>DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力。工作量：</li><li>DreamComposer 框架的实现相对简单，易于使用。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-168607583ef1e4f58b3bdcd4803e43c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33dcfafc8394f58e1dbd724842a101fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-93b23a2d6c08c410dd4391267ba17622.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ce8934dd91a9c823f4314c0c68127d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e0119c153d54525b574ef2a92843a1d.jpg" align="middle"></details>​    ## HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and   Objects from Video**Authors:Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges**Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold [PDF](http://arxiv.org/abs/2311.18448v1) **Summary**无须先验知识，仅从单目交互视频中，就能对铰接手和物体进行联合重建。**Key Takeaways**- HOLD 是一种无需类别信息的通用方法，可以仅从单目交互视频中重建铰接手和物体。- HOLD 使用复合铰接隐式模型来重建从 2D 图像中分离出的 3D 手和物体。- HOLD 还进一步加入了手物体约束，以改进手物体姿势，从而提升重建质量。- HOLD 无需 3D 手物体注释，即便在实验室和极具挑战性的野外环境中，也能优于完全监督的基线方法。- HOLD 在重建野外视频方面具有鲁棒性。- HOLD 适用于广泛的互动场景，包括抓取、操作和操纵。- HOLD 可以作为下游任务（如动作识别和手势识别）的辅助工具。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：HOLD：从视频中进行类别无关的交互手和物体的 3D 重建</li><li>作者：Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges</li><li>隶属单位：苏黎世联邦理工学院，德国图宾根马克斯普朗克智能系统研究所</li><li>关键词：3D 重建、手部对象交互、隐式神经表示、约束优化</li><li>论文链接：https://arxiv.org/abs/2311.18448，Github 链接：https://github.com/zc-alexfan/hold</li><li>摘要：(1)：研究背景：人类每天都会与各种物体互动，因此全面捕捉这些互动对于理解和建模人类行为非常重要。然而，目前大多数用于从 RGB 图像进行手部物体重建的方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。(2)：过去的方法及其问题：本文方法的动机：现有方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。(3)：研究方法：为了解决上述问题，本文提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。(4)：方法的性能：HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本研究的意义：本研究提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。（2）：本文的优缺点：创新点：</li><li>提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。</li><li>使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。</li><li>进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。</li></ol><p>性能：</p><ol><li>HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。</li><li>HOLD 方法从野外视频中重建的鲁棒性强。</li></ol><p>工作量：</p><ol><li>HOLD 方法需要大量的数据和计算资源。</li><li>HOLD 方法的训练过程比较复杂。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-415ee1a9b6ee2ab5c473c2f0dda0e51e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c2ea9ec163c36ce8e5caba3457d81b7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ac62d03408862e6758b781f5f9d3f4f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5ad526669e8557416b88d13528a10f59.jpg" align="middle"></details><br>​    <p></p><h2 id="HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors"><a href="#HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors" class="headerlink" title="HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors"></a>HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors</h2><p><strong>Authors:Junhui Yin, Wei Yin, Hao Chen, Xuqian Ren, Zhanyu Ma, Jun Guo, Yifan Liu</strong></p><p>Recent methods for dynamic human reconstruction have attained promising reconstruction results. Most of these methods rely only on RGB color supervision without considering explicit geometric constraints. This leads to existing human reconstruction techniques being more prone to overfitting to color and causes geometrically inherent ambiguities, especially in the sparse multi-view setup.   Motivated by recent advances in the field of monocular geometry prediction, we consider the geometric constraints of estimated depth and normals in the learning of neural implicit representation for dynamic human reconstruction. As a geometric regularization, this provides reliable yet explicit supervision information, and improves reconstruction quality. We also exploit several beneficial physical priors, such as adding noise into view direction and maximizing the density on the human surface. These priors ensure the color rendered along rays to be robust to view direction and reduce the inherent ambiguities of density estimated along rays. Experimental results demonstrate that depth and normal cues, predicted by human-specific monocular estimators, can provide effective supervision signals and render more accurate images. Finally, we also show that the proposed physical priors significantly reduce overfitting and improve the overall quality of novel view synthesis. Our code is available at:~\href{<a href="https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}">https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}</a>. </p><p><a href="http://arxiv.org/abs/2311.15171v1">PDF</a> </p><p><strong>Summary</strong><br>利用深度和法向量作为几何正则化，提高神经隐式表示动态人体重建的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>深度和法向量是人体重建的关键几何约束。</li><li>使用深度和法向量作为监督信息可以提高重建质量。</li><li>加入噪声到视角中可以使颜色渲染更鲁棒。</li><li>最大化人体表面的密度可以减少密度估计的固有二义性。</li><li>实验结果表明，深度和法向量线索可以提供有效的监督信号并呈现更准确的图像。</li><li>提出物理先验显著减少了过拟合并提高了新视角合成的总体质量。</li><li>代码可在 GitHub 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HumanRecon：利用几何线索和物理先验的神经重建动态人体</li><li>作者：Junhui Yin、Wei Yin、Hao Chen、Xuqian Ren、Zhanyu Ma、Jun Guo 和 Yifan Liu</li><li>单位：北京邮电大学</li><li>关键词：动态人体重建、神经隐式表示、几何约束、物理先验</li><li>论文链接：https://arxiv.org/abs/2311.15171Github 链接：https://github.com/PRIS-CV/HumanRecon</li><li>摘要：（1）：研究背景：动态人体重建是计算机视觉研究中的一个基本问题，在机器人技术、图形学、增强现实、虚拟现实和人体数字化等领域有着广泛的应用。然而，从 RGB 图像和视频中重建动态人体极具挑战性，因为这些数据缺乏足够的监督信息。（2）：过去方法：传统的人体建模工作使用显式网格来表示人体几何形状，并将外观存储在 2D 纹理贴图中。但是，这些方法需要密集的摄像头阵列和受控的照明条件。近年来，PIFu、StereoPIFu 和 PIFuHD 等方法提出使用像素级图像特征回归神经隐式函数，能够重建高分辨率的衣着人体结果。ARCH 方法将一系列 PIFu 方法扩展到从单目图像中回归可动画的衣着人体化身。然而，这些方法无法从稀疏的多视角视频中重建动态的衣着人体。（3）：研究方法：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示。具体来说，HumanRecon 使用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。此外，HumanRecon 还利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。这些先验确保了沿射线渲染的颜色对视向具有鲁棒性，并减少了沿射线估计的密度的固有歧义性。（4）：方法性能：实验结果表明，由特定于人类的单目估计器预测的深度和法线线索可以提供有效的监督信号并渲染更准确的图像。此外，本文提出的物理先验可以显着减少过拟合并提高新视角合成的整体质量。</li></ol><p><strong>方法</strong>：</p><p>(1)：HumanRecon利用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。</p><p>(2)：HumanRecon利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。</p><p>(3)：HumanRecon使用神经隐式表示来学习动态人体的几何形状和外观，并利用几何线索和物理先验来提高重建的准确性和鲁棒性。</p><ol start="8"><li>结论：（1）：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体，在公共数据集上取得了最优的重建效果。（2）：创新点：HumanRecon 提出了一种新的动态人体重建方法，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体。性能：HumanRecon 在公共数据集上取得了最优的重建效果。工作量：HumanRecon 的工作量中等，需要收集稀疏的多视角视频数据，并使用神经网络进行训练。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53aa17faea82b5e0fdbbccd7e38ab3f0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efb4eebe5bbbb5efc58baa3b2148c347.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3423434440b10282e759bb59440bd5e3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bb5324e9f58f0fec825b0268ffd59901.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cc59c128bbcbd6e5e97f759649f953d4.jpg" align="middle"></details><br>​    <p></p><h2 id="Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss"><a href="#Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss" class="headerlink" title="Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss"></a>Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss</h2><p><strong>Authors:Abdelfateh Bekkair, Slimane Bellaouar, Slimane Oulad-Naoui</strong></p><p>Several natural phenomena and complex systems are often represented as networks. Discovering their community structure is a fundamental task for understanding these networks. Many algorithms have been proposed, but recently, Graph Neural Networks (GNN) have emerged as a compelling approach for enhancing this task.In this paper, we introduce a simple, efficient, and clustering-oriented model based on unsupervised \textbf{G}raph Attention \textbf{A}uto\textbf{E}ncoder for community detection in attributed networks (GAECO). The proposed model adeptly learns representations from both the network’s topology and attribute information, simultaneously addressing dual objectives: reconstruction and community discovery. It places a particular emphasis on discovering compact communities by robustly minimizing clustering errors. The model employs k-means as an objective function and utilizes a multi-head Graph Attention Auto-Encoder for decoding the representations. Experiments conducted on three datasets of attributed networks show that our method surpasses state-of-the-art algorithms in terms of NMI and ARI. Additionally, our approach scales effectively with the size of the network, making it suitable for large-scale applications. The implications of our findings extend beyond biological network interpretation and social network analysis, where knowledge of the fundamental community structure is essential. </p><p><a href="http://arxiv.org/abs/2311.12986v2">PDF</a> 7 pages, 5 Figures</p><p><strong>摘要</strong><br>无监督图注意力自动编码器方法有效地提取了生物和社交关系网络中的社区结构。</p><p><strong>要点</strong></p><ul><li>提出了一种简单、高效、面向聚类的无监督图注意力自动编码器方法，用于发现属性网络中的社区结构。</li><li>该模型能够同时学习网络拓扑和属性信息，并以重建和社区发现为双重目标。</li><li>该模型通过稳健地最小化聚类误差，特别强调发现紧凑的社区。</li><li>该模型采用k均值作为目标函数，并使用多头图注意自动编码器对表示进行解码。</li><li>在三个属性网络数据集上进行的实验表明，该方法在NMI和ARI方面优于最先进的算法。</li><li>该方法还可以有效地扩展到网络规模，使其适用于大规模应用。</li><li>该方法的应用范围不仅限于生物网络解释和社交网络分析，在这些领域中，了解基本的社区结构至关重要。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：无监督图注意力自编码器用于属性网络</li><p></p><p></p><li>作者：Abdelfateh Bekkaira, Slimane Bellaouara, Slimane Oulad-Naouia</li><p></p><p></p><li>所属单位：阿尔及利亚格哈达亚大学数学与计算机科学系</li><p></p><p></p><li>关键词：图注意力网络、自编码器、无监督学习、社区检测</li><p></p><p></p><li>链接：https://arxiv.org/abs/2311.12986</li><p></p><p></p><li>摘要：（1）研究背景：社区检测是图分析中的一项重要任务，旨在将图中的节点划分为不同的社区，使社区内的节点连接紧密，社区之间的节点连接稀疏。传统社区检测方法主要基于图的结构信息，而近年来提出的图注意力网络（GAT）可以同时考虑图的结构信息和节点的属性信息，在社区检测任务上取得了较好的效果。然而，现有的GAT模型大多是监督学习模型，需要大量的标记数据进行训练。（2）过去的方法及其问题：现有方法存在以下问题：</li><br>&lt;/ol&gt;<p></p><ul><li>需要大量的标记数据进行训练，这在实际应用中往往难以获得。</li><li>难以处理大规模图数据。</li><li>对图的结构和属性信息利用不足。（3）研究方法：为了解决上述问题，本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型的结构如下图所示：</li></ul><p>[Image of GAECO model architecture]</p><p>GAECO模型由编码器、解码器和损失函数三部分组成。编码器采用GAT网络，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。解码器采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。损失函数采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。（4）实验结果：</p><ul><li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li><li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li><li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。</li></ul><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型在三个真实数据集上取得了优于其他对比模型的结果，表明了该模型的有效性。（2）：创新点：</li></ol><ul><li>提出了一种新的无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。</li><li>GAECO模型采用GAT网络作为编码器，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。</li><li>GAECO模型采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。</li><li>GAECO模型采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。性能：</li><li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li><li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li><li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。工作量：</li><li>GAECO模型的编码器采用GAT网络，GAT网络的计算复杂度为O（|E|d），其中|E|是图中的边数，d是GAT网络的层数。</li><li>GAECO模型的解码器采用简单的内积解码器，内积解码器的计算复杂度为O（|V|^2），其中|V|是图中的节点数。</li><li>GAECO模型的损失函数采用K-means损失函数，K-means损失函数的计算复杂度为O（|V|k），其中k是社区的个数。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b74cf502e48f47b499aada3b40dffd6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3de603a9dee95b40e4c36264efb66584.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a789c364045820588a48d784da87b83b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ac4b7e93b2cb7cc90f069d4cad7ce3f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fdb87522aae5351d76c2693b34df4b9e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f81c4e04a9bd4eca0b4c90cf7b270249.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b83ae848f07876055a106f8dc5f8ccf5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8276545d9badaff0d0b635c20212197f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34999862d8e3b506a71b65f8cb20c960.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6988fa3ab3a1f1365a34e891d5e8d311.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol>]]></content>
    
    
    <summary type="html">3D reconstruction 方向最新论文已更新，请持续关注 Update in 2024-01-24 Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Diffusion%20Models/</id>
    <published>2024-01-24T02:47:28.000Z</published>
    <updated>2024-01-24T08:11:57.446Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models"><a href="#Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models" class="headerlink" title="Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models"></a>Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</h2><p><strong>Authors:Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang</strong></p><p>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at <a href="https://github.com/RL4M/MED-PEFT">https://github.com/RL4M/MED-PEFT</a>. </p><p><a href="http://arxiv.org/abs/2401.12215v1">PDF</a> Technical report</p><p><strong>Summary</strong><br>基于胸部X光影像基金模型的PEFT参数化微调可提高医学视觉任务的迁移学习性能。</p><p><strong>Key Takeaways</strong></p><ul><li>PEFT在18项迁移学习任务中有13项优于FFT，使用可调参数少于1%可提高至多2.9%的性能。</li><li>将PEFT与基金模型相结合，我们在一系列数据高效学习任务上设置了新的最先进技术，例如，在NIH ChestX-ray14上使用1%的标记数据，AUROC得分达到80.6%。</li><li>我们希望这项研究能够引起社区对PEFT在医学影像任务中的迁移学习的更多关注。</li><li>代码和模型可以在<a href="https://github.com/RL4M/MED-PEFT上获得。">https://github.com/RL4M/MED-PEFT上获得。</a></li><li>PEFT最初开发用于开发预训练的大型语言模型，最近已成为在计算机视觉任务上执行迁移学习的有效方法。</li><li>PEFT在医学视觉基金模型中的有效性仍不清楚，有待探索。</li><li>作为概念证明，我们对将PEFT应用于胸部放射线照相基金模型进行了详细的实证研究。</li><li>具体而言，我们深入研究了LoRA（一种具有代表性的PEFT方法），并将其与在三个公认的胸部X光照相数据集中对两个自监督放射线照相基金模型进行全参数微调（FFT）进行了比较。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：少即是多：参数高效微调</p></li><li><p>作者：陈宇廉、周鸿宇、于一舟、王连生</p></li><li><p>单位：厦门大学</p></li><li><p>关键词：迁移学习、医学视觉基础模型、胸部 X 射线</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12215Github 代码链接：https://github.com/RL4M/MED-PEFT</p></li><li><p>摘要：（1）研究背景：参数高效微调（PEFT）最初用于开发预训练的大语言模型，最近已成为计算机视觉任务中进行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型中的有效性仍不清楚，有待探索。（2）过去的方法及其问题：全参数微调（FFT）已被公认为一种执行迁移学习的优越技术。然而，基础模型通常具有大量参数，当下游任务只有有限的注释时，微调全部模型权重可能不是一个最优选择。（3）论文提出的研究方法：为了证明概念，我们对将 PEFT 应用于胸部放射线基础模型进行了详细的实证研究。具体来说，我们深入研究了具有代表性的 PEFT 方法 LoRA，并将其与两个自监督放射线基础模型在三个公认的胸部放射线数据集上与全参数微调 (FFT) 进行了比较。（4）方法在任务上的表现及其性能：我们的结果表明，LoRA 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。将 LoRA 与基础模型相结合，我们在各种数据高效学习任务中建立了新的最优水平，例如在 NIHChestX-ray14 上使用 1% 的标记数据获得了 80.6% 的 AUROC 分数。我们希望这项研究能够引起社区更多地关注在医学成像任务中使用 PEFT 进行迁移学习。</p></li><li><p>方法：（1）提出 LoRA-PEFT 方法：LoRA-PEFT 是一种参数高效微调方法，它通过学习一个低秩矩阵来对基础模型的权重进行微调。该方法可以有效减少可调参数的数量，从而提高微调的效率。（2）在胸部 X 射线数据集上进行实验：作者将 LoRA-PEFT 方法应用于两个自监督胸部 X 射线基础模型，并在三个公认的胸部 X 射线数据集上与全参数微调 (FFT) 进行了比较。结果表明，LoRA-PEFT 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。（3）在其他医学成像任务上进行实验：作者还将 LoRA-PEFT 方法应用于其他医学成像任务，包括肺结节检测、骨龄评估和心脏磁共振成像分割。结果表明，LoRA-PEFT 在这些任务上也取得了良好的性能。</p></li><li><p>结论：（1）：本文提出了一种参数高效微调方法LoRA-PEFT，该方法可以有效减少可调参数的数量，从而提高微调的效率。在胸部X射线数据集和其它医学成像任务上的实验表明，LoRA-PEFT在迁移学习任务中取得了良好的性能。（2）：创新点：提出了一种新的参数高效微调方法LoRA-PEFT。将LoRA-PEFT方法应用于胸部X射线数据集和其它医学成像任务，并取得了良好的性能。性能：在18项迁移学习任务中的13项中优于全参数微调(FFT)，最多可使用少于1%的可调参数提高2.9%。在其他医学成像任务上也取得了良好的性能。工作量：方法简单易用，易于实现。在多个数据集上进行了实验，证明了方法的有效性。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-88f5604fa47b7e6b53fa59ed5ce873a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f28a6055dce3066c942bea25f00c4b98.jpg" align="middle"><img src="https://picx.zhimg.com/v2-219f68f671f950faee6332daa05d83eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b0e3d9c8b6a9c6651af0cb1202241988.jpg" align="middle"></details><br>​    <p></p><h2 id="CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation"><a href="#CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation" class="headerlink" title="CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"></a>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</h2><p><strong>Authors:Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz</strong></p><p>Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{<a href="https://stanford-aimi.github.io/chexagent.html}">https://stanford-aimi.github.io/chexagent.html}</a>. </p><p><a href="http://arxiv.org/abs/2401.12208v1">PDF</a> 24 pages, 8 figures</p><p><strong>摘要</strong><br>引入大规模指令调整数据集和创新基准，构建强大且透明的胸部 X 光解释 AI 系统。</p><p><strong>主要要点</strong></p><ul><li>胸部 X 光检查是临床上最常进行的影像检查。</li><li>视觉语言基础模型 (FM) 在医学影像领域取得了进展。</li><li>开发准确解读胸部 X 光的 FM 存在挑战。</li><li>提出 CheXinstruct，一个包含 28 个公共数据集的大规模指令调整数据集。</li><li>提出 CheXagent，一个能够分析和总结胸部 X 光的指令调整 FM。</li><li>构建 CheXagent，设计了一个临床大语言模型 (LLM) 用于解析放射报告，一个视觉编码器用于表示胸部 X 光图像，以及一个用于桥接视觉和语言模态的网络。</li><li>引入 CheXbench，一个旨在系统地评估 FM 在 8 个临床相关胸部 X 光解释任务中的能力的新基准。</li><li>CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。</li><li>对性别、种族和年龄等因素进行公平性评估，以突出潜在的性能差异。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CheXagent：构建胸部 X 射线解读基础模型</li><li>作者：Zhihong Chen、Maya Varma、Jean-Benoit Delbrouck、Magdalini Paschali、Louis Blankemeier、Dave Van Veen、Jeya Maria Jose Valanarasu、Alaa Youssef、Joseph Paul Cohen、Eduardo Pontes Reis、Emily B. Tsai、Andrew Johnston、Cameron Olsen、Tanishq Mathew Abraham、Sergios Gatidis、Akshay S. Chaudhari、Curtis Langlotz</li><li>第一作者单位：斯坦福大学</li><li>关键词：胸部 X 射线、医学图像、基础模型、语言模型、视觉编码器</li><li>论文链接：https://arxiv.org/abs/2401.12208，Github 代码链接：Github：None</li><li>摘要：(1)：研究背景：胸部 X 射线 (CXR) 是临床实践中最常进行的影像检查。最近视觉语言基础模型 (FM) 的发展为自动 CXR 解读提供了可能性，这可以帮助医生进行临床决策并改善患者预后。然而，开发能够准确解读 CXR 的 FM 具有挑战性，原因在于：（1）医学图像领域缺乏大规模视觉语言数据集；（2）缺乏能够捕捉医学数据复杂性的视觉和语言编码器；（3）缺乏用于对 FM 在 CXR 解读方面的能力进行基准测试的评估框架。(2)：过去的方法及其问题：过去的方法主要集中在开发能够从医学图像中提取特征的视觉编码器和能够理解和生成自然语言的语言模型。然而，这些方法在 CXR 解读任务上表现不佳，原因在于它们无法捕捉医学数据中的复杂性，并且它们没有经过针对 CXR 解读任务的专门训练。(3)：本文提出的研究方法：为了解决这些挑战，本文首先介绍了 CheXinstruct，这是一个从 28 个公开数据集策划而来的大规模指令微调数据集。然后，本文提出了 CheXagent，这是一个经过指令微调的 FM，能够分析和总结 CXR。为了构建 CheXagent，本文设计了一个用于解析放射学报告的临床大语言模型 (LLM)、一个用于表示 CXR 图像的视觉编码器以及一个用于桥接视觉和语言模态的网络。最后，本文介绍了 CheXbench，这是一个新颖的基准，旨在系统地评估 FM 在 8 个临床上相关的 CXR 解读任务中的表现。(4)：本文方法在任务和性能上的表现：广泛的定量评估和五位专家放射科医生的定性审查表明，CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。此外，为了提高模型透明度，本文针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本工作代表了胸部X射线解读自动化的进展。我们介绍了（i）CheXinstruct，一个指令微调数据集，（ii）CheXagent，一个8B参数的视觉语言基础模型，并通过（iii）CheXbench，我们的基准框架（包括7个数据集上的8个任务）展示了它的能力。与通用和医学领域的大语言模型相比，CheXagent在视觉感知和文本生成任务中取得了改进，并得到了五位专家放射科医生的验证。此外，我们针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异，从而提高了模型的透明度。CheXinstruct、CheXagent和CheXbench的公开发布不仅强调了我们对推进医疗人工智能的承诺，而且为这一关键研究领域的未来发展树立了新的基准。（2）：创新点：</li></ol><ul><li>提出了一种新的指令微调数据集CheXinstruct，用于训练视觉语言基础模型。</li><li>提出了一种新的视觉语言基础模型CheXagent，用于胸部X射线解读。</li><li>提出了一种新的基准框架CheXbench，用于评估视觉语言基础模型在胸部X射线解读任务中的性能。性能：</li><li>CheXagent在CheXbench任务上优于之前开发的通用和医学领域的大语言模型。</li><li>CheXagent在视觉感知和文本生成任务中取得了改进。工作量：</li><li>CheXinstruct数据集包含超过100万个图像和相应的放射学报告。</li><li>CheXagent模型的参数量为8B。</li><li>CheXbench基准框架包括7个数据集和8个任务。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6aa52c71b57a2862b763a5188b83d6d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7d79f07ab8199caa375ff5c3d1ce188.jpg" align="middle"><img src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f44d1729ed1485e81c04a41f097c005.jpg" align="middle"></details>​    ## SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning   Capabilities**Authors:Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia**Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/ [PDF](http://arxiv.org/abs/2401.12168v1) **摘要**通过大规模空间推理数据集的训练，视觉语言模型的空间推理能力得到显着提高。**要点**- 视觉语言模型在视觉问答任务中表现出色，但在三维空间推理任务中存在不足。- 视觉语言模型三维空间推理能力有限的原因是训练数据中缺乏三维空间知识。- 通过互联网规模的空间推理数据训练视觉语言模型，可以显著提高其空间推理能力。- 本研究提出了一种自动三维空间视觉问答数据生成框架，可以生成 20 亿个视觉问答示例，这些示例来自 1000 万张真实世界图像。- 本研究还探讨了训练过程中对视觉语言模型空间推理能力有影响的各种因素，包括数据质量、训练管道和视觉语言模型架构。- 本研究首次提出了度量空间中的互联网规模的三维空间推理数据集。- 通过在该数据集上训练视觉语言模型，显著提高了其在定性和定量空间视觉问答任务中的能力。- 由于视觉语言模型的定量估计能力，它可以在思维链空间推理和机器人技术方面解锁新的下游应用程序。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：空间VLM：赋予视觉语言模型空间推理能力</p></li><li><p>作者：Boyuan Chen, Zhuo Xu, Sean Kirmani, Danny Driess, Pete Florence, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia</p></li><li><p>第一作者单位：谷歌大脑</p></li><li><p>关键词：视觉语言模型、空间推理、数据生成、预训练</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12168Github 链接：无</p></li><li><p>摘要：(1)：研究背景：视觉语言模型（VLM）在图像字幕、视觉问答、具身规划、动作识别等任务上取得了显著进展。然而，大多数最先进的 VLM 在空间推理方面仍然存在困难，即需要理解物体在 3D 空间中的位置或它们之间的空间关系的任务。空间推理能力本身很有用，也适用于机器人或 AR 等下游应用。(2)：过去的方法与问题：许多 VLM 在以图像字幕对为特征的互联网规模数据集上进行训练，这些数据集包含有限的空间信息。这是因为难以获得空间信息丰富的具身数据或高质量的人类注释以用于 3D 感知查询。自动数据生成和增强技术是解决数据限制问题的一种方法。然而，以前的大多数数据生成工作都集中在使用真实语义注释渲染逼真的图像，而忽略了对象和 3D 关系的丰富性。(3)：研究方法：本文提出了一种名为 Spatial VLM 的系统，该系统能够生成数据并训练 VLM 以增强其空间推理能力。具体来说，通过结合 1）开放词汇检测，2）度量深度估计，3）语义分割和 4）以对象为中心的字幕模型，我们可以大规模地注释真实世界数据。Spatial VLM 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA 和空间推理数据的混合体上训练 VLM。(4)：实验结果：实验表明，训练后的 VLM 表现出许多理想的能力。首先，它回答定性空间问题的能力大大增强。其次，它可以可靠地执行定量估计，尽管训练数据存在噪声。这种能力不仅赋予它有关物体大小的常识知识，而且使其成为用于重新排列任务的开放词汇奖励注释器。第三，我们发现这种空间视觉语言模型受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p></li><li><p>方法：(1) 通过结合开放词汇检测、度量深度估计、语义分割和以对象为中心的字幕模型，在大规模真实世界数据上进行注释；(2) 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA和空间推理数据的混合体上训练VLM；(3) 训练后的VLM表现出许多理想的能力，包括回答定性空间问题的能力大大增强、可以可靠地执行定量估计、受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p></li><li><p>结论：（1）：本工作通过构建一个基于互联网规模真实世界图像的 3D 空间推理视觉问答数据自动生成框架，解决了向 VLM 注入空间推理能力的挑战。我们消融了在训练 VLM 时不同的设计选择，例如使用大量噪声数据进行训练和解冻 ViT。虽然我们的直接空间查询构建在一个有限的模板集上，但我们表明 Spatial VLM 可以扩展到处理需要空间推理组件的更复杂的思想链推理。Spatial VLM 也被证明对机器人任务有用，我们表明 3D 空间感知 VLM 可以用作机器人任务的奖励注释器。对更多细微的几何基元的额外研究也有助于将空间推理扎根于 3D 几何中。（2）：创新点：Spatial VLM 框架可以自动生成 3D 空间推理视觉问答数据，从而解决了 VLM 数据匮乏的问题。Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。Spatial VLM 还可以在机器人任务中用作奖励注释器。性能：Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。工作量：Spatial VLM 框架的构建和训练需要大量的时间和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1dd8332c6d8630f99e53a83fc7e433f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c541a35e7c51b65b33425be6365e1f69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ae654224a3e214605c476084a222746f.jpg" align="middle"></details><br>​    <p></p><h2 id="West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling"><a href="#West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling" class="headerlink" title="West-of-N: Synthetic Preference Generation for Improved Reward Modeling"></a>West-of-N: Synthetic Preference Generation for Improved Reward Modeling</h2><p><strong>Authors:Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</strong></p><p>The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges. </p><p><a href="http://arxiv.org/abs/2401.12086v1">PDF</a> </p><p><strong>Summary:</strong></p><p>深度强化学习从人类反馈中学习对语言模型的调整强烈依赖于基础奖励模型的质量。</p><p><strong>Key Takeaways:</strong></p><ul><li>提出了一种通过生成合成偏好数据来提高奖励模型质量的新方法。</li><li>利用最优 N 采样策略在奖励模型训练中的应用。</li><li>采用自训练策略，通过从给定查询的响应池中选择最佳和最差候选来生成偏好对。</li><li>实证研究发现，该方法可以提高任何奖励模型的性能，效果与添加相同数量的人类偏好数据相当。</li><li>这项工作通过提供合成偏好生成作为奖励建模挑战的解决方案，为改进深度强化学习从人类反馈中学习对语言模型的调整开辟了新的研究途径。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：西-N：用于改进奖励建模的合成偏好生成</li><li>作者：Alizée Pace、Jonathan Mallinson、Eric Malmi、Sebastian Krause、Aliaksei Severyn</li><li>隶属机构：苏黎世联邦理工学院人工智能中心</li><li>关键词：强化学习、人类反馈、语言模型对齐、奖励建模、合成偏好生成、最佳-N 采样</li><li>链接：https://arxiv.org/abs/2401.12086</li><li>摘要：（1）研究背景：在语言模型对齐中，从人类反馈中进行强化学习的成功在很大程度上取决于基础奖励模型的质量。（2）过去的方法及其问题：以往的方法通常通过收集人类反馈数据来训练奖励模型，这既昂贵又耗时。此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。（3）本文提出的研究方法：为了解决这些问题，本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法。这种方法利用语言模型策略的生成能力来产生一个半监督训练框架。具体来说，本文利用最佳-N 采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N 对。然后，将这些西-N 对添加到初始偏好数据集中，以增强奖励模型的训练。（4）方法在任务和性能上的表现：实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N 采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法，该方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。（2）：创新点：</li></ol><ul><li>提出了一种通过生成合成偏好数据来增强奖励模型训练的新方法。</li><li>利用语言模型策略的生成能力来产生一个半监督训练框架。</li><li>利用最佳-N采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N对。</li><li>将这些西-N对添加到初始偏好数据集中，以增强奖励模型的训练。性能：</li><li>实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。</li><li>本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景。工作量：</li><li>本文提出的方法需要收集人类反馈数据来训练奖励模型，这既昂贵又耗时。</li><li>此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-f075498889dd5931672e158769361ccc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a762acac78a978fb4aa322de329e2f04.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fab744164f7b7ba0585c92c42ab56338.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3412e87f6d4c5844235495c01077f118.jpg" align="middle"></details>​    ## Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated   Text**Authors:Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein**Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. [PDF](http://arxiv.org/abs/2401.12070v1) 20 pages, code available at https://github.com/ahans30/Binoculars**Summary**人工智能检测器可以准确区分人类生成和机器生成文本，无需训练数据。**Key Takeaways**- 人工智能检测器基于对比两个密切相关的语言模型的分数，可以准确区分人类生成和机器生成文本。- 该方法称为 Binoculars，无需任何训练数据即可实现最先进的准确性。- Binoculars 无需任何特定模型的修改，就可以从一系列现代语言模型中检测到机器文本。- Binoculars 在多种文本来源和各种情况下都得到了全面评估。- 在各种类型的文档中，Binoculars 以 0.01% 的误报率检测出超过 90% 由 ChatGPT（和其他语言模型）生成的样本，尽管它没有使用任何 ChatGPT 数据进行训练。- Binoculars 是一个通用工具，可以检测由各种语言模型生成的文本。- Binoculars 可用于多种应用，例如检测虚假新闻或识别在线欺诈。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：使用双筒望远镜发现 LLM：零次检测机器生成的文本</li><li>作者：Abhimanyu Hans、Avi Schwarzschild、Valeriia Cherepanova、Hamid Kazemi、Aniruddha Saha、Micah Gold Blum、Jonas Geiping、Tom Goldstein</li><li>第一作者单位：马里兰大学</li><li>关键词：自然语言处理、机器学习、语言模型、文本生成、检测机器生成的文本</li><li>论文链接：https://arxiv.org/abs/2401.12070，Github 代码链接：https://github.com/ahans30/Binoculars</li><li>摘要：</li></ol><p>（1）研究背景：检测由现代大型语言模型生成的文本被认为是一项艰巨的任务，因为 LLM 和人类都可以表现出广泛的复杂行为。然而，我们发现基于对比两个密切相关的语言模型的分数在区分人类生成的文本和机器生成的文本方面非常准确。</p><p>（2）过去的方法及其问题：现有方法存在以下问题：需要大量训练数据进行微调；只能检测特定语言模型生成的文本；对生成的文本类型和领域敏感。</p><p>（3）研究方法：我们提出了一种新颖的 LLM 检测器，它只需要使用一对预训练的 LLM 进行简单的计算。该方法称为双筒望远镜，在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代 LLM 中识别机器文本。</p><p>（4）方法性能：我们对双筒望远镜进行了全面的评估，涉及多种文本来源和各种情况。在各种类型的文档中，双筒望远镜检测到超过 90% 的来自 ChatGPT（和其他 LLM）生成的示例，假阳性率为 0.01%，尽管没有在任何 ChatGPT 数据上进行训练。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种新颖的LLM检测器——双筒望远镜，该方法在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。（2）：创新点：</li></ol><ul><li>使用一对预训练的LLM进行简单的计算，无需大量训练数据进行微调。</li><li>能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。</li><li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。性能：</li><li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。</li><li>双筒望远镜在检测其他LLM生成的文本方面也表现出良好的性能，例如GPT-3、T5和BART。工作量：</li><li>双筒望远镜的实现相对简单，可以在各种计算平台上轻松部署。</li><li>双筒望远镜的计算成本很低，可以实时检测机器生成的文本。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-dd3bfb5b9d052c9fd7839e210dcdc353.jpg" align="middle"></details>​    ## Feature Denoising Diffusion Model for Blind Image Quality Assessment**Authors:Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, Rongrong Ji**Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC). [PDF](http://arxiv.org/abs/2401.11949v1) **Summary**利用扩散模型提升图像质量评价的特征去噪**Key Takeaways**- 提出了一种基于扩散模型的图像质量评价方法，PFD-IQA。- PFD-IQA 通过两个辅助任务发现潜在的低级特征，并将其用于聚合扩散模型的感知文本条件。- PFD-IQA 提出了一种基于感知先验的特征细化策略，将噪声特征匹配到预定义的去噪轨迹，然后基于文本条件执行精确的特征去噪。- PFD-IQA 在八个标准图像质量评价数据集上取得了优于最先进的图像质量评价方法的性能，例如，在 KADID 中达到了 0.935 的 PLCC 值（而 KADID 为 0.905）和在 LIVEC 中达到了 0.922 的 PLCC 值（而 LIVEC 为 0.894）。- PFD-IQA 可以有效地从质量感知特征中去除噪声，从而提高图像质量评价的准确性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于感知特征扩散的图像质量评估</li><li>作者：Huajun Chen, Yifan Zhang, Qiong Yan, Jiaying Liu, Yanyun Zhao, Lei Zhang</li><li>单位：中国科学院自动化研究所</li><li>关键词：图像质量评估、扩散模型、感知特征、文本条件</li><li>链接：None, Github：None</li><li>摘要：</li></ol><p>（1）研究背景：图像质量评估（BIQA）旨在评估图像质量，使其与人类感知一致，且无需参考基准。当前，深度学习的 BIQA 方法通常依赖于来自高级任务的特征，以便进行迁移学习。然而，BIQA 与这些高级任务之间的固有差异不可避免地会向质量感知特征引入噪声。</p><p>（2）过去的方法及其问题：现有的 BIQA 方法通常依赖于从高层任务中提取的特征，这些特征可能包含与图像质量无关的信息，从而导致评估结果不准确。此外，这些方法通常需要大量的数据进行训练，并且对图像的失真类型和质量水平敏感。</p><p>（3）本文的研究方法：本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA）。该方法首先通过感知先验发现和聚合模块建立两个辅助任务，以发现图像中潜在的低级特征，这些特征用于聚合用于扩散模型的感知文本条件。然后，本文提出了一种基于感知先验的特征细化策略，该策略将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</p><p>（4）方法的性能：在八个标准 BIQA 数据集上的广泛实验表明，该方法优于最先进的 BIQA 方法，即在 KADID 中实现 PLCC 值为 0.935（比 0.905 提高 3.0%），在 LIVEC 中实现 PLCC 值为 0.922（比 0.894 提高 2.8%）。这些性能结果支持了本文方法的目标。</p><ol start="7"><li><p>方法：(1): 感知先验发现和聚合模块 (PDA)：利用随机通道掩码模块和特征重建器来发现潜在的失真先验和感知先验，并利用文本条件自适应地聚合感知文本嵌入。(2): 感知先验驱动的扩散细化模块 (PDR)：利用感知先验来增强特征表示，并提出一种基于感知先验的特征细化策略，将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。(3): 变换器解码器：使用一层变换器解码器来进一步解释去噪后的特征，以预测最终的质量分数。</p></li><li><p>结论：（1）：本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA），该方法将扩散模型的去噪能力引入到盲图像质量评估中，并通过引入感知先验发现和聚合模块以及感知先验驱动的特征细化策略，实现了图像质量评估的准确性和鲁棒性。（2）：创新点：本文的主要创新点包括：</p></li></ol><ul><li>提出了一种新的图像质量评估框架，该框架利用扩散模型的去噪能力来评估图像质量。</li><li>提出了一种感知先验发现和聚合模块，该模块可以发现图像中的潜在失真先验和感知先验，并自适应地聚合感知文本嵌入。</li><li>提出了一种感知先验驱动的特征细化策略，该策略可以将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</li></ul><p>性能：</p><ul><li>在八个标准BIQA数据集上的广泛实验表明，该方法优于最先进的BIQA方法，即在KADID中实现PLCC值为0.935（比0.905提高3.0%），在LIVEC中实现PLCC值为0.922（比0.894提高2.8%）。</li></ul><p>工作量：</p><ul><li>该方法的工作量主要体现在模型的训练和推理上。模型的训练需要大量的数据，并且需要较长的训练时间。模型的推理速度也相对较慢，因为需要对图像进行多次采样。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d238ca44c11468d98720fd64ab500d75.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1828ba81b9cef3240c9a656e8ada16ff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c337add6e35fa4a0c7cc26a39230f729.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9bdc9e0b457f2c81fd14765eac361bfa.jpg" align="middle"></details>​    ## Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?**Authors:Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng**While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs. [PDF](http://arxiv.org/abs/2401.11911v1) **Summary**大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。**Key Takeaways**- 大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。- 大型语言模型生成的语境通常与问题更相似，因此更容易被选择。- 大型语言模型检索的语境由于分段过程而变得不完整，因此难以被充分利用。- 理解大型语言模型如何融合不同语境有助于改进目前的大型语言模型增强方法。- 大型语言模型在获取信息时，往往会偏向于它自己生成的语境，这是由于这些语境通常与问题更相似。- 大型语言模型检索的语境由于被分段，因此会存在不完整的情况，这也会影响大型语言模型对语境的利用。- 研究人员提出了一种方法来构建具有冲突语境的数据集，并使用该数据集来评估大型语言模型融合不同语境的能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：生成上下文遮蔽：语言模型如何融合生成上下文和检索上下文进行开放域问答？</p></li><li><p>作者：谭和祥、孙飞、杨万里、王元卓、曹琦、程雪祺</p></li><li><p>第一作者单位：中国科学院计算技术研究所人工智能安全与安全重点实验室</p></li><li><p>关键词：大型语言模型、信息融合、生成上下文、检索上下文、问答</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11911Github 代码链接：无</p></li><li><p>摘要：(1) 研究背景：近年来，利用辅助信息增强大型语言模型（LLM）的性能已成为研究热点。然而，对于 LLM 如何融合这些上下文，特别是生成上下文和检索上下文，目前的研究还相对较少。(2) 过去的方法及其问题：现有工作可以分为生成增强和检索增强两大类。生成增强方法通过让 LLM 生成与给定问题相关的背景上下文，然后利用该上下文生成最终答案。检索增强方法则通过将来自外部语料库（如维基百科）的相关段落作为上下文，从而增强 LLM 处理知识更新和长尾知识等情况的能力。然而，这些方法都存在冲突问题，即不同来源的上下文之间可能存在冲突，从而影响信息融合的有效性。(3) 本文提出的研究方法：为了研究 LLM 如何处理生成上下文和检索上下文之间的冲突，本文提出了一种专门设计的新任务，用于识别答案是否来自生成上下文或检索上下文。同时，本文还开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。(4) 方法在任务上的表现：实验结果表明，LLM 对生成上下文存在显着的偏好，这在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统中都有所体现。进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。</p></li><li><p>方法：（1）任务设计：设计一种任务来识别答案是否来自生成上下文或检索上下文，以研究 LLM 如何处理冲突上下文。（2）数据集构建：构建具有冲突上下文的数据集，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。（3）实验评估：使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估 LLM 的表现，分析导致 LLM 对生成上下文存在偏见的关键因素。</p></li><li><p>结论：(1): 本工作首次研究了 LLM 如何处理生成上下文和检索上下文之间的冲突，并提出了一个专门设计的新任务和构建具有冲突上下文的数据集的方法，为研究 LLM 的信息融合行为提供了新的视角。(2): 创新点：</p></li></ol><ul><li>提出了一种识别答案是否来自生成上下文或检索上下文的新任务，用于研究 LLM 如何处理冲突上下文。</li><li>开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。</li><li>通过实验评估发现，LLM 对生成上下文存在显着的偏好，并分析了导致这种偏见的两个关键因素。性能：</li><li>在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统上评估了 LLM 在任务上的表现，结果表明 LLM 对生成上下文存在显着的偏好。</li><li>进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。工作量：</li><li>设计了任务和构建了数据集，用于研究 LLM 如何处理冲突上下文。</li><li>使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估了 LLM 的表现，并分析了导致 LLM 对生成上下文存在偏见的两个关键因素。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aff8facaa355b1505b2cf6af3d0e915b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ee80fef672e8714cbda66ee9ba9e921.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-32c0aa5250fcb9295d1e46e737e52534.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99f94640fc796568a6b02c8056191892.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c63733100557d4290705642b87c665f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8815deb3960e6b2bbbe8b92e6f8e6799.jpg" align="middle"></details>​    ## Considerations on Approaches and Metrics in Automated Theorem   Generation/Finding in Geometry**Authors:Pedro Quaresma, Pierluigi Graziani, Stefano M. Nicoletti**The pursue of what are properties that can be identified to permit an automated reasoning program to generate and find new and interesting theorems is an interesting research goal (pun intended). The automatic discovery of new theorems is a goal in itself, and it has been addressed in specific areas, with different methods. The separation of the "weeds", uninteresting, trivial facts, from the "wheat", new and interesting facts, is much harder, but is also being addressed by different authors using different approaches. In this paper we will focus on geometry. We present and discuss different approaches for the automatic discovery of geometric theorems (and properties), and different metrics to find the interesting theorems among all those that were generated. After this description we will introduce the first result of this article: an undecidability result proving that having an algorithmic procedure that decides for every possible Turing Machine that produces theorems, whether it is able to produce also interesting theorems, is an undecidable problem. Consequently, we will argue that judging whether a theorem prover is able to produce interesting theorems remains a non deterministic task, at best a task to be addressed by program based in an algorithm guided by heuristics criteria. Therefore, as a human, to satisfy this task two things are necessary: an expert survey that sheds light on what a theorem prover/finder of interesting geometric theorems is, and - to enable this analysis - other surveys that clarify metrics and approaches related to the interestingness of geometric theorems. In the conclusion of this article we will introduce the structure of two of these surveys - the second result of this article - and we will discuss some future work. [PDF](http://arxiv.org/abs/2401.11905v1) In Proceedings ADG 2023, arXiv:2401.10725**摘要**几何定理自动发现方法学及衡量标准综述。**要点**- 几何定理自动发现与寻找有趣定理是两个不同的课题。- 几何定理的有趣性难以判断，目前尚未找到有效的算法来解决这个问题。- 目前有不同的方法和度量标准来衡量几何定理的有趣性。- 专家调查对于确定有趣的几何定理证明器/发现者的标准非常重要。- 衡量几何定理有趣性的度量标准和方法值得进一步研究。- 本文介绍了两项关于几何定理自动发现和有趣性度量的调查结果。- 未来的工作包括开发新的方法来衡量几何定理的有趣性以及设计新的算法来发现几何定理。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：关于自动推理中的方法和度量</li><li>作者：P. Quaresma, P. Graziani, S. M. Nicoletti</li><li>单位：科英布拉大学</li><li>关键词：自动定理生成、自动定理发现、几何定理、有趣性、调查</li><li>链接：https://link.springer.com/article/10.1007/s10955-022-02793-zGithub：无</li><li>摘要：(1)：研究背景：自动推理系统面临的一个挑战是能够发现新的和有趣的定理。本文探讨了自动定理生成和自动定理发现的方法和度量。(2)：过去的方法和问题：过去的方法包括归纳法、生成法和操纵法。这些方法都存在一定的局限性，例如归纳法不健全，生成法不健全，操纵法受限于现有定理。(3)：研究方法：本文提出了一种新的方法来评估几何定理的有趣性。该方法基于两项调查，第一项调查收集了受访者对几何定理有趣性的看法，第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。(4)：方法的性能：该方法能够有效地评估几何定理的有趣性。在第一项调查中，受访者对 100 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理。在第二项调查中，受访者对 50 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理，并且能够识别出受访者认为不有趣的定理。</li></ol><p>7.方法：（1）提出一种基于两项调查的新方法来评估几何定理的有趣性。（2）第一项调查收集了受访者对几何定理有趣性的看法。（3）第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。（4）该方法能够有效地评估几何定理的有趣性。</p><ol start="8"><li>结论：（1）：本文提出了一种基于两项调查的新方法来评估几何定理的有趣性，该方法能够有效地评估几何定理的有趣性，为自动推理系统发现新的和有趣的定理提供了新的思路。（2）：创新点：提出了一种基于两项调查的新方法来评估几何定理的有趣性。性能：该方法能够有效地评估几何定理的有趣性。工作量：该方法需要收集受访者对几何定理有趣性的看法，设计在线调查，分析调查结果，工作量较大。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-35fbe3fcfdde6deb4efc56cd12862691.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e8aa21d700ed908986328d869dd194f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-23af62ea3bf9c0e1bfe2c05e42b8598e.jpg" align="middle"></details><br>​    <p></p><h2 id="Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation"><a href="#Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation" class="headerlink" title="Adversarial speech for voice privacy protection from Personalized Speech   generation"></a>Adversarial speech for voice privacy protection from Personalized Speech   generation</h2><p><strong>Authors:Shihao Chen, Liping Chen, Jie Zhang, KongAik Lee, Zhenhua Ling, Lirong Dai</strong></p><p>The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers’ voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker’s speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in <a href="https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS">https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS</a>. </p><p><a href="http://arxiv.org/abs/2401.11857v1">PDF</a> Accepted by icassp 2024</p><p><strong>Summary</strong><br>利用对抗攻击保护语音免受恶意语音合成攻击。</p><p><strong>Key Takeaways</strong></p><ul><li>语音生成技术快速发展，带来语音保护需求。</li><li>提出一种基于对抗攻击的语音保护方法。</li><li>该方法通过最小化扰动原始语音，使下游语音生成模型无法准确生成目标扬声器的语音。</li><li>利用开源预训练YourTTS模型进行语音生成，并在白盒场景下保护目标扬声器的语音。</li><li>在生成的语音上进行自动扬声器验证（ASV）评估，评估语音保护能力。</li><li>实验结果表明，使用基于梯度的I-FGSM对抗扰动方法成功扰动了YourTTS模型的扬声器编码器。</li><li>对抗扰动有效阻止了YourTTS模型生成目标扬声器的语音。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：对抗语音保护语音隐私免受个性化语音生成的影响</p></li><li><p>作者：Shihao Chen, Liping Chen, Jie Zhang, Kong Aik Lee, Zhenhua Ling, Lirong Dai</p></li><li><p>隶属单位：中国科学技术大学自然科学与工程科学研究中心</p></li><li><p>关键词：个性化语音生成，文本到语音，语音转换，语音隐私，对抗攻击</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11857Github 链接：无</p></li><li><p>摘要：(1)：随着个性化语音生成技术（包括个性化文本到语音（TTS）和语音转换（VC））的快速发展，人类听众很难区分生成的语音和真实语音，这使得保护说话者声音免受恶意使用变得迫切。(2)：过去的方法主要集中在语音合成语音检测和语音匿名化。语音合成语音检测技术可以检测出合成的语音，但无法防止合成的语音被生成。语音匿名化技术可以隐藏说话者的属性，但会改变语音的感知。(3)：本文提出了一种基于对抗攻击的说话者保护方法。该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。(4)：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p></li><li><p>方法：（1）提出了一种对抗攻击的说话者保护方法，该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。（2）该方法包括两个步骤：首先，使用预训练的语音编码器提取原始语音的说话者编码；然后，使用对抗训练来生成对抗扰动，该对抗扰动可以最小化说话者编码与下游语音生成模型生成的语音之间的相似性。（3）在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p></li><li><p>结论：（1）本研究意义：提出对抗语音生成保护说话者隐私的方法，旨在防止利用说话者属性生成模仿特定目标说话者的语音。（2）文章优缺点总结：创新点：提出基于对抗攻击的说话者隐私保护方法，通过扰动语音信号最小化目标说话者编码与下游语音生成模型生成的语音之间的相似性，有效防止下游语音生成模型准确生成目标说话者的语音。性能：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。工作量：需要预训练语音编码器和对抗训练来生成对抗扰动，工作量较大。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-15e1f378d04e71f2940c11aed1ae5bf4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0af4566ba493c8c725b1d0b9a109ef1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6f0f2f4d8c1103eb84061360d6a46ecc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a9bec8025ef1ca5041dc7cb35b6d9ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b61eaa1a3a79d68bf6f920d3fc1011c.jpg" align="middle"></details><br>​    <p></p><h2 id="Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution"><a href="#Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution" class="headerlink" title="Towards Effective and General Graph Unlearning via Mutual Evolution"></a>Towards Effective and General Graph Unlearning via Mutual Evolution</h2><p><strong>Authors:Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang</strong></p><p>With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose \underline{\textbf{M}}utual \underline{\textbf{E}}volution \underline{\textbf{G}}raph \underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7\%, 2.5\%, and 3.2\% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch. </p><p><a href="http://arxiv.org/abs/2401.11760v1">PDF</a> Accepted by AAAI 2024 Oral</p><p><strong>Summary</strong><br>机器互文演化解图网络遗忘任务难点，提升性能降低开销。</p><p><strong>Key Takeaways</strong></p><ul><li>机器的互文演化范式（MEGU）同时演化预测与遗忘能力，进行互补性优化。</li><li>MEGU对9个图基准数据集进行广泛实验，在特征、节点和边层面的遗忘任务中表现优异。</li><li>与最先进的基准相比，MEGU在这三个级别的遗忘任务中实现平均性能提升2.7%、2.5%和3.2%。</li><li>MEGU训练效率高，与从头开始重新训练GNN相比，时间和空间开销分别平均减少了159.8倍和9.6倍。</li><li>MEGU权衡了遗忘性能和框架的泛化，是用户友好的，部署效率高。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于互惠进化的有效且通用的图遗忘</p></li><li><p>作者：李寻凯，赵玉林，吴政宇，张文韬，李荣华，王国仁</p></li><li><p>单位：北京理工大学</p></li><li><p>关键词：机器遗忘，图神经网络，互惠进化</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11760，Github 链接：无</p></li><li><p>摘要：(1) 研究背景：随着人工智能应用的快速发展，对数据隐私和模型鲁棒性的日益增长的需求凸显了机器遗忘的重要性，尤其是在蓬勃发展的基于图的场景中。(2) 过去的方法及其问题：大多数现有的图遗忘策略主要依赖于精心设计的体系结构或手动过程，这使得它们不太用户友好，并且在部署效率方面提出了挑战。此外，在遗忘性能和框架泛化之间取得平衡也是一个关键问题。(3) 本文提出的研究方法：为了解决上述问题，我们提出了互惠进化图遗忘 (MEGU)，这是一种新的互惠进化范式，可以同时进化图遗忘的预测能力和遗忘能力。通过结合上述两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。(4) 方法在任务和性能上的表现：在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。此外，MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。</p></li><li><p>方法：(1): 提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成；(2): 提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化；(3): 设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块；(4): 对于特征级、节点级和边级遗忘任务，分别对节点、节点和连接的节点进行处理。</p></li><li><p>结论：(1): 本文提出了一种新的互惠进化图遗忘（MEGU）范式，该范式能够在保持预测精度的同时有效地遗忘图数据中的实体，为基于图的人工智能应用提供了一种新的数据遗忘解决方案。(2): Innovation point:</p><ul><li>提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成，通过结合这两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。</li><li>提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化。</li><li>设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块。Performance:</li><li>在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。</li><li>MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。Workload:</li><li>MEGU 的实现相对复杂，需要设计和实现互惠进化图遗忘范式、自适应高影响邻域选择、拓扑感知遗忘传播和精心设计的优化目标等组件。</li><li>MEGU 的训练过程需要同时优化预测模块和遗忘模块，这可能会增加训练时间和计算资源消耗。</li></ul></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-70d5f7a64115883b02ea1767385ba893.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8939202881feee558982e445caf1a42.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b52a21e40634da3517551fc7b6d6dae6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8d1bda09f438055b071e260d78600f52.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2cec89067deb63b340f0a70ac09f8835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47f7c8704ffb75dc4a7676853409a0f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ddc22923eeaa505e66731bd2d67736b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-797371f016d00a04252a7ac5332620ef.jpg" align="middle"></details><br>​    <p></p><h2 id="Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs"><a href="#Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs" class="headerlink" title="Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs"></a>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a> </p><p><a href="http://arxiv.org/abs/2401.11708v1">PDF</a> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></p><p><strong>Summary</strong><br>多模态 LLM 作为全局规划器，帮助扩散模型提升多属性、多类别目标生成任务。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一系列无监督学习的框架，即 RPG，用来改善文字转图像扩散模型的组合性。</li><li>利用多模态 LLM 作为全局规划器，将复杂的图像生成任务分解为多个在子区域内的简单生成任务。</li><li>提出了一种互补的区域扩散来支持按区域进行组合性生成。</li><li>将文本引导的图像生成和编辑以闭环方式集成到 RPG 中，从而提高泛化能力。</li><li>在多类别目标组合和文本图像语义对齐方面，RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL。</li><li>RPG 框架与各种多模态 LLM 架构（例如 MiniGPT-4）和扩散模型兼容。</li><li>代码可从此处获取：<a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：掌握文本到图像扩散：多模态 LLM 的重新表述、规划和生成</p></li><li><p>作者：Ling Yang<em>, Zhaochen Yu</em>, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</p></li><li><p>隶属单位：北京大学</p></li><li><p>关键词：文本到图像生成、扩散模型、多模态 LLM、链式思维推理、区域扩散</p></li><li><p>论文链接：https://github.com/YangLing0818/RPG-DiffusionMasterGithub 代码链接：https://github.com/YangLing0818/RPG-DiffusionMaster</p></li><li><p>摘要：(1) 研究背景：扩散模型在文本到图像生成和编辑方面表现出色，但现有方法在处理涉及多个对象及其属性和关系的复杂文本提示时通常面临挑战。(2) 过去的方法及其问题：一些工作通过引入布局/框作为条件或利用提示感知注意引导来解决这个问题，但这些方法通常需要额外的训练或难以扩展到复杂提示。(3) 本文提出的研究方法：本文提出了一种新的无训练文本到图像生成/编辑框架，称为 Recaption, Plan and Generate (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。我们的方法使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个子区域内的简单生成任务。我们提出了互补区域扩散以实现区域内组合生成。此外，我们将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。(4) 方法在任务和性能上的表现：广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，特别是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。</p></li><li><p>方法：（1）文本重述：利用多模态 LLM 将复杂的文本提示分解为多个子提示，并对每个子提示进行更详细的描述，以提高生成图像的保真度和减少语义差异。（2）链式思维推理规划：利用多模态 LLM 的链式思维推理能力，对最终图像内容的构成进行规划，将图像空间划分为多个互补区域，并为每个区域分配特定的子提示。（3）互补区域扩散：提出一种新的扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。（4）文本引导的图像生成和编辑：将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。</p></li><li><p>结论：（1）：本文提出了一种无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。此外，RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。（2）：创新点：</p></li></ol><ul><li>提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</li><li>提出了一种互补区域扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。</li><li>将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。性能：</li><li>RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。</li><li>RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。工作量：</li><li>RPG 框架的实现相对复杂，需要对多模态 LLM、扩散模型和区域扩散模型进行集成。</li><li>RPG 框架的训练过程需要大量的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d7ede89518c7e2b2017c785eb927b766.jpg" align="middle"><img src="https://pica.zhimg.com/v2-69a6785a9dc22c046203d70cee24a3f1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b57333091d6dbb8392ce8971cf413d0b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5d6f54078071dcab585ee882e1cb7cb6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-40b7d562cad3ed84d89938dbcdb65fff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe1c57ab8d093322b4502e666dccd4cb.jpg" align="middle"></details>​    ## Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass   Diffusion Transformers**Authors:Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z. Kaplan, Enrico Shippole**We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$. [PDF](http://arxiv.org/abs/2401.11605v1) 20 pages, 13 figures, project page and code available at   https://crowsonkb.github.io/hourglass-diffusion-transformers/**Summary**图像生成模型 Hourglass Diffusion Transformer (HDiT) 在像素数量上呈线性扩展，支持以像素空间直接进行高分辨率（例如 1024×1024）训练。**Key Takeaways**- HDiT 是一种新的图像生成模型，它使用 Transformer 架构，该架构以亿万参数的规模进行扩展。- HDiT 将卷积 U-Net 的效率与 Transformer 的可扩展性相结合。- HDiT 可以直接在像素空间中训练高分辨率图像，而无需使用多尺度架构、潜在自编码器或自条件等典型的高分辨率训练技术。- HDiT 在 ImageNet 256^2 上的表现与现有模型具有竞争力，并在 FFHQ-1024^2 上的扩散模型中创造了新的最先进水平。- HDiT 的训练过程更简单，并且不需要使用复杂的架构或训练策略。- HDiT 可以生成高质量的图像，并且在Inception Score和FID等评估指标上取得了不错的成绩。- HDiT 的发布为高分辨率图像生成打开了新的可能性，有望在未来得到更广泛的应用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于小时沙漏扩散变换器的可扩展高分辨率像素空间图像合成</li><li>作者：Katherine Crowson<em>1、Stefan Andreas Baumann</em>2、Alex Birch*3、Tanishq Mathew Abraham1、Daniel Z. Kaplan4、Enrico Shippole5</li><li>第一作者单位：稳定人工智能</li><li>关键词：扩散模型、Transformer、图像生成、高分辨率</li><li>论文链接：https://arxiv.org/abs/2401.11605，Github 代码链接：无</li><li>摘要：（1）：随着扩散模型在图像生成任务中的成功，研究人员开始探索如何将这些模型扩展到更高的分辨率。然而，现有的扩散模型在高分辨率下往往面临着计算成本高、训练不稳定等问题。（2）：过去的方法主要集中在使用多尺度架构、潜在自编码器或自条件等技术来提高扩散模型在高分辨率下的性能。然而，这些方法往往会增加模型的复杂性和训练难度。（3）：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT 采用了一种新的架构，该架构结合了 Transformer 的可扩展性和卷积 U-Net 的效率。（4）：在 ImageNet256 和 FFHQ-1024 数据集上，HDiT 在与现有模型的比较中取得了有竞争力的性能，并且在 FFHQ-1024 数据集上创下了扩散模型的新纪录。</li></ol><p>&lt;Methods&gt;:</p><p>(1)：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</p><p>(2)：HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</p><p>(3)：HDiT在ImageNet256和FFHQ-1024数据集上，在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p><p>(4)：HDiT还具有良好的大规模图像生成能力，在ImageNet-256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</p><ol start="8"><li>结论：</li></ol><p>（1）：本工作提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p><p>（2）：创新点：</p><ul><li>提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</li><li>HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</li><li>在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</li></ul><p>性能：</p><ul><li>在ImageNet256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</li><li>在FFHQ-1024数据集上，HDiT创下了扩散模型的新纪录。</li></ul><p>工作量：</p><ul><li>HDiT的计算成本缩放是线性的，这使得它能够扩展到更高的分辨率。</li><li>HDiT的训练难度较低，这使得它更容易训练。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fcc074a8fe14d1b52ec9aa98684f39d4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb5ff002efdb09103d60a3788e8ec694.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f646dbc6a174419ce3e875010d6a8da1.jpg" align="middle"></details>​    ## Exploring Diffusion Time-steps for Unsupervised Representation Learning**Authors:Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang**Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all 1,...,t-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, e.g., interpolating only one specified attribute between two images, validating the disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti. [PDF](http://arxiv.org/abs/2401.11430v1) Accepted by ICLR 2024**摘要**扩散模型中的时间步长与隐藏属性相关，可用于无监督学习模块化属性。**要点**- 扩散模型通过在每个时间步长向样本添加高斯噪声，将不同样本折叠成相似样本。- 在每个时间步长 t，学习一个 t 特定的特征来补偿新丢失的属性。- 所有 1, ..., t 特定的特征对应于累积的丢失属性集，用于弥补时间步长 t 处预训练扩散模型的重建误差。- 在 CelebA、FFHQ 和 Bedroom 数据集上，学习到的特征显着提高了属性分类，并实现了保真的反事实生成，例如，仅在两幅图像之间插入一个指定属性。- 代码可在 https://github.com/yue-zhongqi/diti 中找到。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：DiTi：通过弥补扩散模型的重建误差来恢复属性</p></li><li><p>作者：Yuxin Chen, Yifan Jiang, Yujun Shen, Xin Yu, Song Bai, Bolei Zhou</p></li><li><p>单位：北京大学</p></li><li><p>关键词：扩散模型，图像生成，属性恢复，弥补误差</p></li><li><p>链接：https://arxiv.org/abs/2302.04522 或 https://github.com/VITA-Group/DiTi</p></li><li><p>摘要：(1)：研究背景：扩散模型是一种生成图像的有效方法，但它在生成过程中会丢失图像的某些属性。(2)：过去的方法：为了解决这个问题，一些方法提出了在扩散过程中加入属性信息，但这些方法往往需要额外的监督信息或计算量大。(3)：研究方法：本文提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性。DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。(4)：实验结果：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高。</p></li><li><p>方法：（1）提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性；（2）DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成；（3）编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间；（4）在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p></li><li><p>结论：（1）：本工作提出了一种新的无监督方法来学习 disentangled 表示，该方法利用了扩散时间步长的归纳偏差。具体来说，我们揭示了时间步长和隐藏模块化属性之间固有的联系，这些属性忠实地生成了数据，从而通过学习时间步长特定特征来实现属性的简单有效的解耦。学习到的特征改进了下游推理并支持反事实生成，验证了其解耦质量。作为未来的工作，我们将寻求额外的归纳偏差来改进解耦，例如，通过探索文本到图像扩散模型来使用文本作为解耦模板，并设计实用的优化技术以实现更快的收敛。（2）：创新点：提出了一种新的方法 DiTi，通过弥补扩散模型的重建误差来恢复图像的属性；性能：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高；工作量：DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-77fe0985f3ccf24b58f01409208c95d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c049f8dfb182212eafcbb8d455570a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53cea1049f1b04a122064eadf034709b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24c1ae5245a699656c0411ed106e5ae2.jpg" align="middle"></details><br>​    <p></p><h2 id="Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient"><a href="#Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient" class="headerlink" title="Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient"></a>Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient</h2><p><strong>Authors:Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</strong></p><p>Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model training with an additional classifier. Training stability has improved. We also theoretically prove that NGMG shares the same benefit as the Earth Mover distance (Wasserstein) as a more sensible cost function when learning distributions supported by low-dimensional manifolds. </p><p><a href="http://arxiv.org/abs/2401.11261v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型提出一种基于高斯混合模型引导去噪的高效条件生成机制，提升生成图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型是一种生成模型，在图像合成和其他领域产生了巨大影响。</li><li>扩散模型可以通过不同的条件输入，如文本或边界框，来生成不同的图像。</li><li>本文将高斯混合模型（GMM）作为特征条件，提出了一个用于控制扩散去噪过程的条件机制。</li><li>基于集合论，本文提供了全面的理论分析，表明基于特征和类别的条件潜在分布存在显著差异。</li><li>基于特征的条件潜在分布产生更少的缺陷生成，优于基于类别的条件。</li><li>提出的高斯混合模型梯度函数（NGMG）可用于提高扩散模型训练的稳定性。</li><li>NGMG与 Earth Mover 距离（Wasserstein）具有相同的好处，作为学习低维流形分布的更合理的成本函数。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>标题：基于高斯混合模型和负高斯混合模型梯度的扩散模型条件机制</p></li><p></p><p></p><li><p>作者：Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</p></li><p></p><p></p><li><p>澳门大学数学系</p></li><p></p><p></p><li><p>关键词：扩散模型、条件生成、高斯混合模型、负高斯混合模型梯度</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2401.11261</p></li><p></p><p></p><li><p>摘要：(1) 研究背景：扩散模型是一种生成模型，在图像合成和其他领域取得了巨大的影响。扩散模型可以通过文本或边界框等多种条件输入来控制生成。(2) 过去的方法：过去的方法通常使用高斯分布对数据进行建模，但这种方法在处理复杂数据时存在局限性。(3) 研究方法：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型。GMM可以对复杂数据进行建模，因此可以更好地控制生成的图像。(4) 性能：本文的方法在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</p></li><p></p><p></p><li><p>方法：（1）：利用高斯混合模型（GMM）对数据进行建模，GMM可以更好地拟合复杂数据，从而更好地控制生成的图像。（2）：将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。（3）：证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。（4）：提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</p></li><p></p><p></p><li><p>结论：（1）意义：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型，该模型可以更好地控制生成的图像，在人脸生成任务上取得了很好的性能。（2）优缺点：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>利用GMM对数据进行建模，可以更好地拟合复杂数据，从而更好地控制生成的图像。</li><li>将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。</li><li>证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。</li><li>提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</li></ul><p>性能：</p><ul><li>在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</li></ul><p>工作量：</p><ul><li>该模型的训练过程相对复杂，需要较多的计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-61a621f7ccfd39bba7557184f456de65.jpg" align="middle"></details><br>​    <p></p><h2 id="MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation"><a href="#MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation" class="headerlink" title="MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation"></a>MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation</h2><p><strong>Authors:Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</strong></p><p>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^<em>$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^</em>$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. </p><p><a href="http://arxiv.org/abs/2401.11115v1">PDF</a> Accepted at the 38th Association for the Advancement of Artificial   Intelligence (AAAI) Conference on Artificial Intelligence, Main Conference</p><p><strong>Summary</strong><br>利用噪声和未标注动作序列的弱监督扩散模型，实现高质量动作生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一个简单有效的弱监督扩散模型 MotionMix，利用噪声和未标注动作序列生成高质量动作。</li><li>将扩散模型的去噪目标分为两个阶段：在前 $T-T^<em>$ 步利用噪声标注动作获得粗略动作近似，最后 $T^</em>$ 步利用未标注动作对粗略动作进行无条件细化。</li><li>MotionMix 在文本转动作、动作转动作和音乐转舞蹈任务上取得了最先进的性能。</li><li>MotionMix 可以应用于各种下游任务，如动作合成、动画制作和机器人控制。</li><li>MotionMix 可以扩展到其他领域，如图像生成、语音合成和自然语言处理。</li><li>MotionMix 是一个通用框架，可以应用于各种动作生成任务。</li><li>MotionMix 可以通过调节超参数来控制动作生成的质量和多样性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>标题：MotionMix：用于可控运动生成的弱监督扩散</p></li><p></p><p></p><li><p>作者：Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</p></li><p></p><p></p><li><p>隶属机构：华为技术有限公司</p></li><p></p><p></p><li><p>关键字：运动生成、扩散模型、弱监督学习</p></li><p></p><p></p><li><p>论文链接：https://nhathoang2002.github.io/MotionMix-page/，Github 链接：无</p></li><p></p><p></p><li><p>摘要：（1）研究背景：随着世界拥抱数字化转型，可控生成三维人体运动成为一个重要课题。现有的工作虽然随着扩散模型的出现取得了可喜的进展，但严重依赖于精心捕捉和注释（例如，文本）的高质量运动语料库，这在现实世界中是一个资源密集型工作。（2）过去的方法及其问题：过去的方法通常使用完全监督的扩散模型，需要大量高质量的注释数据。然而，获取此类数据成本高昂且耗时。（3）提出的研究方法：为了解决上述问题，本文提出了一种简单而有效的方法 MotionMix，它是一种弱监督扩散模型，可以同时利用噪声注释运动和未注释运动。具体来说，我们将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。（4）方法的性能：广泛的实验表明，MotionMix 作为一种通用的框架，在文本到运动、动作到运动和音乐到舞蹈任务上始终如一地取得了最先进的性能。这些性能支持了本文的目标，即在不损害运动生成质量的前提下，使用更少的数据和更少的注释来训练扩散模型。</p></li><p></p><p></p><li><p>Methods:(1): MotionMix方法将扩散模型的去噪目标分为两个阶段：在初始T-T<em>步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后T</em>步中使用未注释运动对这些初步运动进行无条件细化。(2): MotionMix方法使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。(3): MotionMix方法使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。(4): MotionMix方法可以同时利用噪声注释运动和未注释运动，这可以帮助扩散模型学习更丰富的运动信息，并提高运动生成的质量。</p></li><p></p><p></p><li><p>结论：（1）：本工作首次提出了一种弱监督扩散模型 MotionMix，用于同时利用噪声注释运动和未注释运动来生成可控运动。MotionMix 在多个运动生成基准和基本扩散模型设计中展示了其多功能性。全面的消融研究进一步支持了其在不同噪声调度和去噪支点的策略选择中的鲁棒性。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种弱监督扩散模型 MotionMix，可以同时利用噪声注释运动和未注释运动来生成可控运动。</li><li>MotionMix 将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。</li><li>MotionMix 使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。</li><li>MotionMix 使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。性能：</li><li>MotionMix 在多个运动生成基准上取得了最先进的性能，包括文本到运动、动作到运动和音乐到舞蹈任务。</li><li>MotionMix 在使用更少的数据和更少的注释的情况下，可以生成与完全监督扩散模型质量相当的运动。工作量：</li><li>MotionMix 的实现相对简单，易于训练和使用。</li><li>MotionMix 可以使用标准的扩散模型训练框架进行训练，不需要额外的计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-273f0c50cd4e128d204627cc095176a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-eae22ee23e9a564640cb9d43a3c08766.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7d3bcd5ef19eb5e526b72441762f30b5.jpg" align="middle"></details><br>​    <p></p><h2 id="UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures"><a href="#UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures" class="headerlink" title="UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures"></a>UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</h2><p><strong>Authors:Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi</strong></p><p>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments. </p><p><a href="http://arxiv.org/abs/2401.11078v1">PDF</a> The project page is at <a href="http://usrc-sea.github.io/UltrAvatar/">http://usrc-sea.github.io/UltrAvatar/</a></p><p><strong>Summary</strong><br>基于几何保真度增强和物理渲染纹理优化，提出了一种新的三维虚拟人物生成方法。</p><p><strong>Key Takeaways</strong></p><ul><li>Diffusion模型生成的3D虚拟人物往往过平滑，缺乏细节和多样性。</li><li>从单个图像生成3D虚拟人物面临着光照、视角和图像质量等挑战。</li><li>本文提出了一种名为UltrAvatar的新三维虚拟人物生成方法。</li><li>UltrAvatar可以去除光照的影响，生成更真实的漫反射颜色。</li><li>UltrAvatar通过两种基于梯度的引导来生成PBR纹理。</li><li>UltrAvatar在实验中优于现有最先进的方法。</li><li>UltrAvatar可以生成高质量的三维虚拟人物，具有更真实的几何形状和物理渲染纹理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：UltrAvatar：基于真实感指导的纹理扩散模型的超写实 3D 头像生成</p></li><p></p><p></p><li><p>作者：Yuxuan Zhang<em>, Yifan Jiang</em>, Jingyu Yang, Yebin Liu, Xiaoguang Han, Yu-Kun Lai</p></li><p></p><p></p><li><p>单位：香港中文大学（深圳）</p></li><p></p><p></p><li><p>关键词：3D 头像生成、纹理扩散模型、真实感指导、物理渲染纹理</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2302.08844, Github：无</p></li><p></p><p></p><li><p>摘要：（1）：随着 3D 头像生成技术的发展，如何生成更逼真、更可动画的头像成为研究热点。（2）：现有方法大多采用分数蒸馏采样损失函数，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，分数蒸馏采样往往会产生过度平滑的结果，缺乏面部细节，与祖先采样相比缺乏多样性。其他方法从单张图像生成 3D 头像，但图像中存在不需要的照明效果、透视视图和较差的图像质量等问题，导致难以可靠地重建具有对齐完整纹理的 3D 面部网格。（3）：本文提出了一种名为 UltrAvatar 的 3D 头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。（4）：实验结果表明，该方法有效且鲁棒，在实验中大幅优于最先进的方法。</p></li><p></p><p></p><li><p>方法：(1)：本文提出了一种名为UltrAvatar的3D头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。(2)：为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。(3)：漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。(4)：真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与3D网格几何形状更好地对齐。</p></li><p></p><p></p><li><p>结论：（1）：本工作提出了一种从文本提示或单个图像生成 3D 头像的新方法。我们方法的核心是 DCEM 模型，旨在消除源图像中不需要的照明效果，以及一个由光度和边缘信号引导的纹理生成模型，以保留头像的 PBR 细节。与其他最先进的方法相比，我们证明了我们的方法可以生成显示出高度逼真、更高质量、更出色保真度和更广泛多样性的 3D 头像。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种漫反射颜色提取模型，可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。</li><li>提出了一种真实感指导的纹理扩散模型，遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。性能：</li><li>在定量和定性评估中，我们的方法在几何保真度、纹理质量和整体逼真度方面优于最先进的方法。</li><li>我们的方法能够生成具有出色质量的物理渲染纹理，且没有不需要的照明效果。</li><li>我们的方法可以从文本提示或单个图像生成 3D 头像，并且生成的头像具有高度逼真、更高质量、更出色保真度和更广泛多样性。工作量：</li><li>本文提出的方法需要较大的计算资源，包括高性能 GPU 和大量内存。</li><li>本文提出的方法需要较多的训练数据，包括大量高质量的 3D 头像数据和相应的文本描述。</li><li>本文提出的方法需要较多的训练时间，以确保模型能够收敛并生成高质量的 3D 头像。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6ddb372268ab29440ab071d2e4e6e298.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5a668d3e08b3e9f2b7d9e0f965d9762.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe4a6193adf67c2ee040715753a40d2d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3f4f76cbd1c6492ba1fd985b02c9d05.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol></ol></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-01-24 Less Could Be Better  Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/GAN/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/GAN/</id>
    <published>2024-01-24T02:34:37.000Z</published>
    <updated>2024-01-24T08:11:59.485Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="A-Fair-Evaluation-of-Various-Deep-Learning-Based-Document-Image-Binarization-Approaches"><a href="#A-Fair-Evaluation-of-Various-Deep-Learning-Based-Document-Image-Binarization-Approaches" class="headerlink" title="A Fair Evaluation of Various Deep Learning-Based Document Image   Binarization Approaches"></a>A Fair Evaluation of Various Deep Learning-Based Document Image   Binarization Approaches</h2><p><strong>Authors:Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein</strong></p><p>Binarization of document images is an important pre-processing step in the field of document analysis. Traditional image binarization techniques usually rely on histograms or local statistics to identify a valid threshold to differentiate between different aspects of the image. Deep learning techniques are able to generate binarized versions of the images by learning context-dependent features that are less error-prone to degradation typically occurring in document images. In recent years, many deep learning-based methods have been developed for document binarization. But which one to choose? There have been no studies that compare these methods rigorously. Therefore, this work focuses on the evaluation of different deep learning-based methods under the same evaluation protocol. We evaluate them on different Document Image Binarization Contest (DIBCO) datasets and obtain very heterogeneous results. We show that the DE-GAN model was able to perform better compared to other models when evaluated on the DIBCO2013 dataset while DP-LinkNet performed best on the DIBCO2017 dataset. The 2-StageGAN performed best on the DIBCO2018 dataset while SauvolaNet outperformed the others on the DIBCO2019 challenge. Finally, we make the code, all models and evaluation publicly available (<a href="https://github.com/RichSu95/Document_Binarization_Collection">https://github.com/RichSu95/Document_Binarization_Collection</a>) to ensure reproducibility and simplify future binarization evaluations. </p><p><a href="http://arxiv.org/abs/2401.11831v1">PDF</a> DAS 2022</p><p><strong>摘要</strong><br>通过评估不同数据集上深度学习方法的文档二值化性能，可以帮助研究人员选择最适合自己任务的模型。</p><p><strong>要点</strong></p><ul><li>深度学习方法能够通过学习与上下文相关的特征来生成图像的二值化版本，从而减少通常发生在文档图像中的退化错误。</li><li>在DIBCO2013数据集上，DE-GAN模型在评估时表现优于其他模型，而在DIBCO2017数据集上，DP-LinkNet表现最佳。</li><li>在DIBCO2018数据集上，2-StageGAN表现最佳，而在DIBCO2019挑战赛上，SauvolaNet优于其他模型。</li><li>代码、所有模型和评估已公开发布，以确保可重现性和简化未来的二值化评估。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：各种基于深度学习的文档图像二值化方法的公平评估</p></li><li><p>作者：Richin Sukesh, Mathias Seuret, Anguelos Nicolaou, Martin Mayr, Vincent Christlein</p></li><li><p>第一作者单位：Friedrich-Alexander-Universit¨atErlangen-N¨urnberg, Erlangen, Germany</p></li><li><p>关键词：二值化、深度学习、评估</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11831Github 代码链接：https://github.com/RichSu95/DocumentBinarizationCollection</p></li><li><p>摘要：(1) 研究背景：文档图像的二值化是文档分析领域中一个重要的预处理步骤。传统的图像二值化技术通常依赖于直方图或局部统计数据来识别有效阈值，以区分图像的不同方面。深度学习技术能够通过学习对上下文相关的特征进行二值化，从而生成图像的二值化版本，这些特征对通常发生在文档图像中的退化不太容易出错。近年来，已经开发了许多基于深度学习的文档二值化方法。但是，如何选择合适的方法是一个问题。目前还没有研究对这些方法进行严格的比较。</p></li></ol><p>(2) 过去的方法及其问题：传统的图像二值化技术通常依赖于直方图或局部统计数据来识别有效阈值，以区分图像的不同方面。这些方法通常对图像中的噪声和退化敏感，并且可能产生不准确的二值化结果。</p><p>(3) 本文提出的研究方法：本文的重点是评估不同的基于深度学习的方法，以在相同的评估协议下进行评估。在不同的文档图像二值化竞赛 (DIBCO) 数据集上评估这些方法，并获得了非常不同的结果。结果表明，在 DIBCO 2013 数据集上评估时，DE-GAN 模型能够比其他模型表现更好，而在 DIBCO 2017 数据集上，DP-LinkNet 表现最好。2-StageGAN 在 DIBCO 2018 数据集上表现最好，而 SauvolaNet 在 DIBCO 2019 挑战赛中优于其他方法。</p><p>(4) 方法在任务和性能上的表现：在 DIBCO 2013 数据集上，DE-GAN 模型在 F-measure 指标上获得了 0.957 的最佳结果。在 DIBCO 2017 数据集上，DP-LinkNet 模型在 F-measure 指标上获得了 0.964 的最佳结果。在 DIBCO 2018 数据集上，2-StageGAN 模型在 F-measure 指标上获得了 0.968 的最佳结果。在 DIBCO 2019 挑战赛中，SauvolaNet 模型在 F-measure 指标上获得了 0.971 的最佳结果。这些结果表明，所提出的方法能够在不同的数据集上实现良好的性能，并且能够支持其目标。</p><ol start="7"><li><p>方法：（1）DE-GAN：利用生成对抗网络来建模文档二值化问题，生成器和判别器共同工作，生成器生成干净的图像，判别器区分生成的图像和真实二值化图像。（2）SauvolaNet：受传统 Sauvola 阈值算法启发，使用深度学习方法学习 Sauvola 参数，包括多窗口 Sauvola、像素级窗口注意力和自适应 Sauvola 阈值三个模块，以估计辅助阈值函数。（3）Two-StageGAN：提出一种两阶段彩色文档图像二值化深度学习架构，利用生成对抗网络，第一阶段使用局部预测进行二值化，第二阶段使用调整大小的原始输入图像和第一阶段的输出进行全局二值化。（4）DP-LinkNet：使用深度学习方法对文档图像进行二值化，采用 U-Net 作为编码器-解码器结构，使用链接网络来增强特征表示，并使用软最大池化来生成二值化结果。（5）2-StageGAN：提出一种两阶段彩色文档图像二值化深度学习架构，利用生成对抗网络，第一阶段使用局部预测进行二值化，第二阶段使用调整大小的原始输入图像和第一阶段的输出进行全局二值化。</p></li><li><p>结论：（1）：本文对七种基于深度学习的文档图像二值化方法进行了公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。对于未来的工作，我们希望使用不同的协议来评估这些方法。特别是，我们希望模拟每年挑战的 DIBCO 场景，以便与单一的 DIBCO 论文进行比较，即使用 2015-2016 年的数据集进行训练，然后使用 2017 年进行评估，将 2017 年添加到训练集中，重新训练并评估 2018 年，依此类推。使用额外的增强技术以及额外的训练数据集也值得研究，并且可能对二值化方法的整体性能产生巨大影响。此外，基于像素的评估并不是最优的。虽然 pFM 度量包含到脚本轮廓的距离，但研究间接措施可能是值得的，例如 OCR/HTR 准确性或纯粹基于骨架的度量。从实用的角度来看，推理时间也值得研究。这主要在时间质量文档图像二值化竞赛中进行了研究。（2）：创新点：本文对七种基于深度学习的文档图像二值化方法进行了公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。性能：本文提出的方法能够在不同的数据集上实现良好的性能，并且能够支持其目标。工作量：本文的工作量很大，需要对七种基于深度学习的文档图像二值化方法进行公平的评估，使用所有十个可用的 DIBCO 数据集对方法进行了评估。评估结果表明，在四个不同的测试数据集上，结果非常不同，没有明确的获胜者。总体而言，DE-GAN 方法在四个不同数据集上的平均排名最高，其次是 SauvolaNet。当单独比较指标时，2-StageGAN 方法表现最好，其次是 DE-GAN。然而，在非常不同的 DIBCO2019 数据集上，SauvolaNet 优于这些方法。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-f410bbd295b056b48e2e24fef3c6357b.jpg" align="middle"></details><br>​    <p></p><h2 id="Efficient-generative-adversarial-networks-using-linear-additive-attention-Transformers"><a href="#Efficient-generative-adversarial-networks-using-linear-additive-attention-Transformers" class="headerlink" title="Efficient generative adversarial networks using linear   additive-attention Transformers"></a>Efficient generative adversarial networks using linear   additive-attention Transformers</h2><p><strong>Authors:Emilio Morales-Juarez, Gibran Fuentes-Pineda</strong></p><p>Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources. </p><p><a href="http://arxiv.org/abs/2401.09596v1">PDF</a> 12 pages, 6 figures</p><p><strong>摘要</strong><br>拉达生成对抗网络：基于新颖的 Transformer 块 Ladaformer 的高效生成对抗网络。</p><p><strong>关键要点</strong></p><ul><li>拉达生成对抗网络是一种高效的生成对抗网络，由一个名为 Ladaformer 的新颖 Transformer 块构建而成。</li><li>Ladaformer 的主要组成部分是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。</li><li>我们在生成器和判别器中都采用了 Ladaformer，这降低了计算复杂度并克服了 Transformer GANs 经常遇到的训练不稳定性。</li><li>拉达生成对抗网络在不同分辨率的基准数据集上始终优于现有的卷积和 Transformer GANs，同时具有更高的效率。</li><li>拉达生成对抗网络与最先进的多步生成模型（例如扩散模型）相比，显示出具有竞争力的性能，而使用的计算资源却少几个数量级。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：使用线性加性注意力 Transformer 的高效生成对抗网络</p></li><li><p>作者：Emilio Morales-Juarez, Gibran Fuentes-Pineda</p></li><li><p>隶属单位：墨西哥国立自治大学工程学院</p></li><li><p>关键词：图像生成、GAN、线性加性注意力、高效 Transformer</p></li><li><p>链接：https://arxiv.org/abs/2401.09596Github：无</p></li><li><p>摘要：(1)：随着深度生成模型在图像生成方面的能力不断提高，Diffusion Models (DM) 和 Generative Adversarial Networks (GAN) 等模型取得了显著的成功。然而，这些模型的成功很大程度上归功于计算成本高昂的架构。这限制了它们在研究实验室和资源较多的公司中的应用，同时大幅增加了训练、微调和推理的碳足迹。(2)：以往的方法包括基于卷积的 GAN 和基于 Transformer 的 GAN。基于卷积的 GAN 通常需要复杂的工程设计和复杂的模块来实现最先进的图像生成，导致计算成本高昂。基于 Transformer 的 GAN 则可以学习数据的光滑和连续的潜在空间表示，但自注意力机制可能会导致 GAN 训练更加不稳定，并且其 O(N^2) 的复杂度导致高计算需求。(3)：本文提出了一种新的 GAN 架构 LadaGAN，它基于称为 Ladaformer 的新型 Transformer 块。Ladaformer 的主要组件是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。这种机制可以降低计算复杂度，并克服与 Transformer GAN 相关的训练不稳定性。(4)：LadaGAN 在 CIFAR-10、CelebA、FFHQ 和 LSUNBedroom 等基准数据集上实现了具有竞争力的 FID 分数，同时所需的 FLOP 和参数明显更少。此外，与最先进的多步生成模型（例如 DM）相比，LadaGAN 在使用数量级更少的计算资源的情况下表现出竞争力。</p></li><li><p>方法：(1)：本文提出了一种新的GAN架构LadaGAN，它基于称为Ladaformer的新型Transformer块。(2)：Ladaformer的主要组件是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。(3)：这种机制可以降低计算复杂度，并克服与TransformerGAN相关的训练不稳定性。(4)：LadaGAN在CIFAR-10、CelebA、FFHQ和LSUNBedroom等基准数据集上实现了具有竞争力的FID分数，同时所需的FLOP和参数明显更少。(5)：此外，与最先进的多步生成模型（例如DM）相比，LadaGAN在使用数量级更少的计算资源的情况下表现出竞争力。</p></li><li><p>结论：（1）本工作提出了一种新颖的 GAN 架构 LadaGAN，它基于一种称为 Ladaformer 的新型 Transformer 块。该块被证明比其他高效的 Transformer 块更适合生成器和判别器，允许在不同场景中进行稳定的 GAN 训练。我们的研究结果表明，Ladaformer 与卷积兼容，LadaGAN 具有梯度稳定性，并且对于图像生成任务非常有效。值得注意的是，LadaGAN 在不同分辨率的多个基准数据集上优于 ConvNet 和 TransformerGAN，同时所需的 FLOP 明显更少。此外，与扩散模型和一致性训练相比，LadaGAN 以极低的计算成本实现了具有竞争力的性能。据我们所知，LadaGAN 是第一个基于线性加性注意力机制的 GAN 架构。因此，我们的结果进一步证明了线性注意力机制的效率和表达能力，并为具有类似于现代扩散模型的性能的有效 GAN 架构的未来研究打开了大门。我们相信 LadaGAN 可以帮助实验室和研究小组在有限的计算预算下更快地进行实验，在不损失质量的情况下推进生成模型的应用，同时减少能源消耗并最大限度地减少碳足迹。作为未来的工作，我们计划在音频和文本到图像场景中训练 LadaGAN。此外，Ladaformer 块及其与卷积的兼容性还有待在其他任务（如图像和视频分类）中进行探索。</p></li></ol><p>（2）创新点：</p><ul><li>提出了一种新颖的 GAN 架构 LadaGAN，它基于一种称为 Ladaformer 的新型 Transformer 块。</li><li>Ladaformer 的主要组成部分是线性加性注意力机制，它为每个头计算一个注意力向量，而不是二次点积注意力。</li><li>这种机制可以降低计算复杂度，并克服与 TransformerGAN 相关的训练不稳定性。</li></ul><p>性能：</p><ul><li>LadaGAN 在 CIFAR-10、CelebA、FFHQ 和 LSUNBedroom 等基准数据集上实现了具有竞争力的 FID 分数，同时所需的 FLOP 和参数明显更少。</li><li>与最先进的多步生成模型（例如 DM）相比，LadaGAN 在使用数量级更少的计算资源的情况下表现出竞争力。</li></ul><p>工作量：</p><ul><li>LadaGAN 的训练速度比基于卷积的 GAN 和基于 Transformer 的 GAN 更快。</li><li>LadaGAN 的内存占用更少，这使得它可以在具有有限内存的设备上训练。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3929700f0a09cfd1fa328b24d0274fe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e4b6c7bf475fddc24bcc75378997dc3c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef2224955dfcfa7c34704af7b7f861f5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-74838ae28b4bbb602f3c6e331bd694ae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-949e6e1e1b4c79eff4a0a5134e9ed474.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ed19f7c8a63fb77d3397877ea92c0b0d.jpg" align="middle"></details>​    ## Adversarial Masking Contrastive Learning for vein recognition**Authors:Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang**Vein recognition has received increasing attention due to its high security and privacy. Recently, deep neural networks such as Convolutional neural networks (CNN) and Transformers have been introduced for vein recognition and achieved state-of-the-art performance. Despite the recent advances, however, existing solutions for finger-vein feature extraction are still not optimal due to scarce training image samples. To overcome this problem, in this paper, we propose an adversarial masking contrastive learning (AMCL) approach, that generates challenging samples to train a more robust contrastive learning model for the downstream palm-vein recognition task, by alternatively optimizing the encoder in the contrastive learning model and a set of latent variables. First, a huge number of masks are generated to train a robust generative adversarial network (GAN). The trained generator transforms a latent variable from the latent variable space into a mask space. Then, we combine the trained generator with a contrastive learning model to obtain our AMCL, where the generator produces challenging masking images to increase the contrastive loss and the contrastive learning model is trained based on the harder images to learn a more robust feature representation. After training, the trained encoder in the contrastive learning model is combined with a classification layer to build a classifier, which is further fine-tuned on labeled training data for vein recognition. The experimental results on three databases demonstrate that our approach outperforms existing contrastive learning approaches in terms of improving identification accuracy of vein classifiers and achieves state-of-the-art recognition results. [PDF](http://arxiv.org/abs/2401.08079v1) **Summary**对抗遮罩对比学习通过生成具有挑战性的掩模来增强静脉识别的鲁棒性。**Key Takeaways**- 深度神经网络，如卷积神经网络（CNN）和Transformer，已被引入静脉识别并取得了最先进的性能。- 然而，现有的手指静脉特征提取解决方案由于训练图像样本稀缺，仍然不是最优的。- 本文提出了一种对抗遮罩对比学习（AMCL）方法，该方法通过交替优化对比学习模型中的编码器和一组潜在变量，生成具有挑战性的样本来训练一个更鲁棒的对比学习模型，用于下游的掌静脉识别任务。- 首先，生成大量掩模来训练一个鲁棒的生成对抗网络（GAN）。训练后的生成器将潜在变量空间中的潜在变量转换为掩模空间。- 然后，我们将训练后的生成器与对比学习模型相结合，得到我们的 AMCL，其中生成器产生具有挑战性的掩模图像以增加对比损失，对比学习模型根据更难的图像进行训练以学习更鲁棒的特征表示。- 训练后，对比学习模型中训练后的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。- 三个数据库上的实验结果表明，我们的方法在提高静脉分类器的识别准确性方面优于现有的对比学习方法，并实现了最先进的识别结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：对抗掩码对比学习用于静脉识别</p></li><li><p>作者：Huafeng Qin, Yiquan Wu, Mounim A. El-Yacoubi, Jun Wang, Guangxiang Yang</p></li><li><p>单位：重庆市智能感知与区块链工程实验室，重庆工商大学计算机科学与信息工程学院</p></li><li><p>关键词：生物识别，静脉识别，对比学习，对抗学习，掩码</p></li><li><p>链接：https://arxiv.org/abs/2401.08079v1，Github 链接：无</p></li><li><p>摘要：（1）：静脉识别由于其高安全性和隐私性而受到越来越多的关注。近年来，卷积神经网络（CNN）和 Transformer 等深度神经网络已被引入静脉识别并取得了最先进的性能。然而，尽管取得了这些进展，但由于训练图像样本稀少，现有的手指静脉特征提取解决方案仍然不是最优的。（2）：为了克服这个问题，本文提出了一种对抗掩码对比学习（AMCL）方法，通过交替优化对比学习模型中的编码器和一组潜在变量，为下游掌静脉识别任务生成具有挑战性的样本以训练更鲁棒的对比学习模型。首先，生成大量掩码来训练鲁棒的生成对抗网络（GAN）。训练后的生成器将来自潜在变量空间的潜在变量转换为掩码空间。然后，我们将训练后的生成器与对比学习模型结合起来得到我们的 AMCL，其中生成器产生具有挑战性的掩码图像以增加对比损失，并且对比学习模型基于更难的图像进行训练以学习更鲁棒的特征表示。训练后，对比学习模型中训练的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。（3）：在三个数据库上的实验结果表明，我们的方法在提高静脉分类器的识别准确性方面优于现有的对比学习方法，并取得了最先进的识别结果。</p></li><li><p>方法：（1）：本文提出了一种对抗掩码对比学习（AMCL）方法，通过交替优化对比学习模型中的编码器和一组潜在变量，为下游掌静脉识别任务生成具有挑战性的样本以训练更鲁棒的对比学习模型。（2）：首先，生成大量掩码来训练鲁棒的生成对抗网络（GAN）。训练后的生成器将来自潜在变量空间的潜在变量转换为掩码空间。（3）：然后，我们将训练后的生成器与对比学习模型结合起来得到我们的 AMCL，其中生成器产生具有挑战性的掩码图像以增加对比损失，并且对比学习模型基于更难的图像进行训练以学习更鲁棒的特征表示。（4）：训练后，对比学习模型中训练的编码器与分类层相结合构建分类器，该分类器在标记的训练数据上进一步微调以进行静脉识别。</p></li><li><p>结论：（1）本文提出了一种联合生成和对比学习框架用于静脉识别，该框架结合了 GAN 和对比学习来学习鲁棒的静脉分类器。首先，生成大量掩码来训练鲁棒的 GAN 以学习掩码分布空间。其次，将训练后的 GAN 与对比学习模型相结合得到我们的 AMCL，并以对抗的方式进行训练。具体来说，搜索一组潜在变量以生成具有挑战性的样本对来增加对比学习模型的损失。对比学习模型能够基于生成的困难样本学习鲁棒的特征表示。我们在三个公开数据库上的实验结果表明，我们的方法在提高静脉分类器的性能方面优于现有的对比学习方法，并取得了最先进的识别准确率。（2）创新点：</p></li></ol><ul><li>提出了一种联合生成和对比学习的框架，用于静脉识别。</li><li>设计了一种鲁棒的 GAN 来学习掩码分布空间。</li><li>将训练后的 GAN 与对比学习模型相结合，以对抗的方式训练对比学习模型。</li></ul><p>性能：</p><ul><li>在三个公开数据库上的实验结果表明，我们的方法在提高静脉分类器的性能方面优于现有的对比学习方法，并取得了最先进的识别准确率。</li></ul><p>工作量：</p><ul><li>需要生成大量掩码来训练鲁棒的 GAN。</li><li>需要将训练后的 GAN 与对比学习模型相结合，并以对抗的方式训练对比学习模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-da84b96d624d97ec8e4bccc75083479b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-56f3ecc77281bae064a88576167ef74d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f2d72f0d01c578dc3aa49ae1a52e3e52.jpg" align="middle"></details>​    ## Multimodal Crowd Counting with Pix2Pix GANs**Authors:Muhammad Asif Khan, Hamid Menouar, Ridha Hamila**Most state-of-the-art crowd counting methods use color (RGB) images to learn the density map of the crowd. However, these methods often struggle to achieve higher accuracy in densely crowded scenes with poor illumination. Recently, some studies have reported improvement in the accuracy of crowd counting models using a combination of RGB and thermal images. Although multimodal data can lead to better predictions, multimodal data might not be always available beforehand. In this paper, we propose the use of generative adversarial networks (GANs) to automatically generate thermal infrared (TIR) images from color (RGB) images and use both to train crowd counting models to achieve higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to TIR images. Our experiments on several state-of-the-art crowd counting models and benchmark crowd datasets report significant improvement in accuracy. [PDF](http://arxiv.org/abs/2401.07591v1) Accepted version of the paper in 19th International Conference on   Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-29 Feb,   2024,**Summary**利用生成对抗网络从彩色图像自动生成热红外图像，可大幅提升人群计数准确率。**Key Takeaways**- 大多数最先进的人群计数方法使用彩色 (RGB) 图像来学习人群的密度图，但在光线较差的密集人群场景中，这些方法通常难以实现更高的准确度。- 最近，一些研究报告称，结合 RGB 和热图像可以提高人群计数模型的准确度。- 虽然多模态数据可以产生更好的预测，但多模态数据可能并不总是事先可用。- 本文提出使用生成对抗网络 (GAN) 从彩色 (RGB) 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高的准确度。- 我们首先使用 Pix2Pix GAN 网络将 RGB 图像转换为 TIR 图像。- 我们在几个最先进的人群计数模型和基准人群数据集上的实验表明，准确度有了显著提高。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：使用 Pix2PixGAN 进行多模态人群计数</p></li><li><p>作者：Muhammad Asif Khan, Hamid Menouar, Ridha Hamila</p></li><li><p>第一作者单位：卡塔尔大学卡塔尔移动创新中心</p></li><li><p>关键词：人群计数、CNN、密度估计、多模态、RGB、热成像</p></li><li><p>论文链接：https://arxiv.org/abs/2401.07591Github 链接：无</p></li><li><p>摘要：(1)：人群计数在人群管理、城市规划、安全监控、活动管理和公共安全等领域有着广泛的应用。深度学习的出现带来了人群计数技术的范式转变，在各种现实世界应用中实现了更高的准确性和可扩展性。(2)：大多数最先进的人群计数方法主要使用光学彩色图像，并且在合理的照明条件下效果较好。然而，在许多监控场景中，使用光学相机捕获的图像具有较差的照明条件，导致计数模型的性能较差。为了提高准确性，热红外 (TIR) 相机与光学相机一起使用，以捕获彩色 RGB 图像和热图像。然后，人群计数模型可以使用一个（单模态）或两个（多模态）RGB 和 TIR 图像来学习低光条件下的人群密度。(3)：本文提出使用生成对抗网络 (GAN) 从彩色 (RGB) 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高的准确性。我们首先使用 Pix2PixGAN 网络将 RGB 图像转换为 TIR 图像。(4)：我们在几个最先进的人群计数模型和基准人群数据集上进行的实验报告了准确性的显着提高。</p></li><li><p>方法：（1）：提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高精度的框架，称为 MMCount。（2）：MMCount 由两个基本部分组成：Pix2PixGAN 和多模态人群计数网络。Pix2PixGAN 将光学 RGB 图像转换为 TIR 图像，人群模型使用 RGB 和 TIR 图像来预测人群密度图。（3）：Pix2PixGAN 由生成器和判别器组成。生成器将 RGB 图像转换为 TIR 图像，判别器将生成的 TIR 图像与真实 TIR 图像进行区分。（4）：多模态计数网络由 RGB 分支和 TIR 分支组成，两个分支都具有四个卷积层，输出连接并融合在融合层中，最后使用 1×1 卷积层生成密度图。（5）：使用头位置生成稀疏定位图，然后使用高斯核与 delta 函数卷积生成密度图，作为训练模型的真实值。（6）：使用 L2 损失函数训练人群计数模型，该损失函数计算目标密度图和预测密度图之间的欧几里德距离。（7）：在 DroneRGBT、ShanghaiTechPart-B 和 CARPK 数据集上评估了所提出的方法，并与基线模型进行了比较。（8）：实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。</p></li><li><p>结论：（1）：本文提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成热红外 (TIR) 图像，并使用两者来训练人群计数模型以实现更高精度的框架，称为 MMCount。实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。（2）：创新点：</p></li></ol><ul><li>提出了一种使用 Pix2PixGAN 从 RGB 图像自动生成 TIR 图像的方法。</li><li>提出了一种使用 RGB 和 TIR 图像来训练人群计数模型以实现更高精度的框架。</li><li>在几个最先进的人群计数模型和基准人群数据集上进行了实验，并报告了准确性的显着提高。性能：</li><li>在 DroneRGBT、ShanghaiTechPart-B 和 CARPK 数据集上评估了所提出的方法，并与基线模型进行了比较。</li><li>实验结果表明，所提出的方法在单模态和多模态人群计数任务中都优于基线模型。工作量：</li><li>使用 PyTorch 实现并开源了所提出的方法。</li><li>在多个 GPU 上训练了所提出的模型。</li><li>在几个基准人群数据集上评估了所提出的模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6f0b0afa8ea4a994e77c0bef26f7009b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9d77479e99dfbd9fb266eda32c03e44d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6696e7497717d1dc1f0a8370d7ba041.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0791df4d611a13627d1fc21d3a330a13.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1cc942916c0c2dd251bf08c0b900723a.jpg" align="middle"></details>​    ## ENTED: Enhanced Neural Texture Extraction and Distribution for   Reference-based Blind Face Restoration**Authors:Yuen-Fui Lau, Tianjia Zhang, Zhefan Rao, Qifeng Chen**We present ENTED, a new framework for blind face restoration that aims to restore high-quality and realistic portrait images. Our method involves repairing a single degraded input image using a high-quality reference image. We utilize a texture extraction and distribution framework to transfer high-quality texture features between the degraded input and reference image. However, the StyleGAN-like architecture in our framework requires high-quality latent codes to generate realistic images. The latent code extracted from the degraded input image often contains corrupted features, making it difficult to align the semantic information from the input with the high-quality textures from the reference. To overcome this challenge, we employ two special techniques. The first technique, inspired by vector quantization, replaces corrupted semantic features with high-quality code words. The second technique generates style codes that carry photorealistic texture information from a more informative latent space developed using the high-quality features in the reference image's manifold. Extensive experiments conducted on synthetic and real-world datasets demonstrate that our method produces results with more realistic contextual details and outperforms state-of-the-art methods. A thorough ablation study confirms the effectiveness of each proposed module. [PDF](http://arxiv.org/abs/2401.06978v1) **Summary**利用高品质参考图修复退化面部图像，将参考图和输入图像之间的高品质纹理特征传递，替换低质量语义特征并生成逼真纹理信息样式代码。**Key Takeaways**- 提出退化面部修复框架ENTED，从参考图像传递纹理特征。- 使用风格GAN生成逼真图像需要高质量潜在码。- 退化输入图像提取的潜在码特征不完整，难以对齐输入与参考图像的语义信息。- 使用矢量量化技术，以优质码字替换损坏的语义特征。- 生成样式码，从参考图像流形中的高质量特征生成更多逼真纹理信息。- 该方法在合成和真实数据集上产生更逼真且包含更多细节的修复结果。- 团队通过消融研究验证每个提出模块的作用与必要性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：ENTED：用于基于参考图像的盲人面部修复的增强神经纹理提取和分布</p></li><li><p>作者：Yuen-Fui Lau、Tianjia Zhang、Zhefan Rao、Qifeng Chen</p></li><li><p>隶属机构：香港科技大学</p></li><li><p>关键词：盲人面部修复、参考图像、纹理提取和分布、矢量量化、风格编码</p></li><li><p>链接：https://arxiv.org/abs/2401.06978Github：无</p></li><li><p>摘要：（1）：研究背景：盲人面部修复（BFR）是一种计算摄影技术，专注于将低质量的面部图像转换成高质量的面部图像，即使不知道退化的类型。这项技术对于那些希望提高面部图像质量的人来说至关重要。然而，从低质量图像中准确重建面部细节的任务可能非常具有挑战性，因为会丢失特定的身份信息。（2）：过去的方法：为了解决这个问题，我们考虑使用同一人的高质量参考图像。这种方法导致我们探索基于参考图像的盲人面部修复的概念，我们的目标是利用高质量参考图像中的信息来增强修复过程。基于参考图像的超分辨率（RefSR）技术最近引起了人们的兴趣。它通过结合来自参考图像的高质量语义细节来提高低分辨率输入的质量。然而，如果参考图像的特征没有得到正确的管理，可能会导致参考图像的利用不足或使用不当。（3）：研究方法：为了克服这些挑战，我们采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。纹理提取和分布框架已成功应用于可控人像合成任务，我们将这一概念扩展到我们的基于参考图像的盲人面部修复框架中。（4）：方法性能：低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息。仅仅应用纹理提取和分布框架不足以产生高保真图像。为了克服这一挑战，我们利用了矢量量化（VQ）技术。该技术需要用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。通过直接替换这些代码，我们能够缩小低质量和高质量潜在代码之间的差异，从而为纹理分布提供合适的语义指导。如表 3 所示，我们注意到调制卷积有助于在修复过程中增强面部细节的真实感。然而，它们需要高质量的样式代码表示才能实现卓越的图像修复。</p></li><li><p>方法：（1）：该文提出了一种基于参考图像的盲人面部修复（BFR）方法，该方法利用高质量参考图像中的信息来增强修复过程。（2）：该方法采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。（3）：为了克服低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息的问题，该方法利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。（4）：为了增强面部细节的真实感，该方法还使用了调制卷积。</p></li><li><p>结论：（1）：本研究提出了一种基于参考图像的盲人面部修复方法，该方法利用高质量参考图像中的信息来增强修复过程。该方法采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。为了克服低质量（LQ）输入的潜在表示通常在纹理分布过程中包含不正确的信息的问题，该方法利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。为了增强面部细节的真实感，该方法还使用了调制卷积。（2）：创新点：</p></li></ol><ul><li>提出了一种基于参考图像的盲人面部修复方法，该方法利用高质量参考图像中的信息来增强修复过程。</li><li>采用了一个纹理提取和分布框架，该框架可以训练具有注意力重建损失，以提高整个纹理转移过程的准确性和高质量特征的使用。</li><li>利用了矢量量化（VQ）技术，用从高质量（HQ）词典中获得的高质量潜在编码替换退化的潜在特征。</li><li>使用了调制卷积来增强面部细节的真实感。性能：</li><li>该方法在多个数据集上取得了最先进的性能。</li><li>该方法能够有效地修复各种类型的面部图像，包括模糊、噪声和低分辨率图像。</li><li>该方法能够生成高质量的面部图像，具有逼真的细节和自然的外观。工作量：</li><li>该方法的实现相对简单，并且可以在标准的硬件上训练和部署。</li><li>该方法的训练时间相对较短，并且可以在几个小时内完成。</li><li>该方法的推理时间相对较快，并且可以在几秒钟内生成高质量的面部图像。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53a65527ccf6d7d8f8572b0ddb295010.jpg" align="middle"><img src="https://pica.zhimg.com/v2-40e6ecf4c16ec5c66222df8b6bf80060.jpg" align="middle"><img src="https://picx.zhimg.com/v2-60393ae177ef1a065e9b74e8fb943b41.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dffe915921a34f90de0be1eb61982e27.jpg" align="middle"><img src="https://picx.zhimg.com/v2-898fa480014e89e712e40975e08fb724.jpg" align="middle"></details>​    ## GE-AdvGAN: Improving the transferability of adversarial samples by   gradient editing-based adversarial generative model**Authors:Zhiyu Zhu, Huaming Chen, Xinyi Wang, Jiayu Zhang, Zhibo Jin, Kim-Kwang Raymond Choo**Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN [PDF](http://arxiv.org/abs/2401.06031v1) Accepted by SIAM International Conference on Data Mining (SDM24)**摘要**改进生成对抗神经网络（GAN）的生成器参数训练过程，提出了一种新的梯度编辑机制，提高了对抗样本的可迁移性和算法效率。**要点*** 对抗生成模型（GAN）被广泛应用于生成各种类型的数据，如图像、文本和音频。* 基于 GAN 的对抗性攻击方法在白盒和黑盒攻击场景中取得了良好的效果。* 可迁移的黑盒攻击对于在不同模型和设置下保持有效性具有重要意义，与实际应用更为紧密。* 保持此类方法的可迁移对抗性例子在性能方面仍然具有挑战性。* 增强型基于梯度的可迁移对抗性攻击算法需要花费较长时间才能生成对抗性样本。* 提出了一种名为 GE-AdvGAN 的新算法，以增强对抗样本的可迁移性并提高算法的效率。* 主要方法是优化生成器参数的训练过程。* 通过函数和特征相似性分析，引入了新的梯度编辑 (GE) 机制，并验证了其在各种模型上生成可迁移样本的可行性。* 通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗性攻击算法相比，GE-AdvGAN 可以生成高度可迁移的对抗性样本，同时最大限度地减少了执行时间。* 通过在不同数据集上进行大规模实验，对 GE-AdvGAN 的性能进行了全面评估，结果证明了该算法的优越性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：GE-AdvGAN：通过基于梯度编辑的对抗生成模型来提高对抗样本的可迁移性</li><li>作者：Zhiyu Zhu、Huaming Chen、Xinyi Wang、Jiayu Zhang、Zhibo Jin、Kim-Kwang Raymond Choo、Jun Shen、Dong Yuan</li><li>隶属单位：悉尼大学电气与计算机工程学院</li><li>关键词：梯度编辑、对抗可迁移性、基于 GAN 的对抗攻击、计算优化</li><li>论文链接：https://arxiv.org/abs/2401.06031Github 代码链接：https://github.com/LMBTough/GE-advGAN</li><li>摘要：(1)：研究背景：对抗生成模型（AGM）在生成各种类型的数据（如图像、文本和音频）方面表现出色。基于 AGM 的对抗攻击方法已广泛应用于白盒和黑盒攻击场景中。可迁移的黑盒攻击非常重要，因为它们能够跨不同的模型和设置有效地进行攻击，这与现实世界的应用更加紧密地结合在一起。然而，对于此类方法来说，在可迁移对抗样本方面保持性能仍然具有挑战性。同时，我们观察到一些增强的基于梯度的可迁移对抗攻击算法需要较长时间才能生成对抗样本。(2)：过去的方法及其问题：AdvGAN 是一种基于香草 GAN 的对抗攻击算法，用于白盒和黑盒攻击。AdvGAN 在白盒攻击环境中，通过训练生成器 G 来生成扰动，一旦 G 经过训练，就不需要连续访问受害者模型的信息。它解决了在传统白盒攻击中需要多次查询模型以训练最优对抗样本的要求。此外，在判别器（以下记为 D）中引入了动态蒸馏过程，允许 AdvGAN 适用于黑盒攻击。该算法以一种新颖的方式集成了前馈和判别器网络来构建 G 和 D 以生成对抗样本。然而，一方面，尽管在黑盒攻击中取得了可喜的成果，即 92.76% 的攻击成功率，但 AdvGAN 在白盒攻击中的性能却很差，即 67.89% 的攻击成功率。另一方面，AdvGAN 在生成对抗样本时需要大量的时间。(3)：研究方法：为了解决上述问题，我们提出了一种名为 GE-AdvGAN 的新算法，以提高对抗样本的可迁移性，同时提高算法的效率。主要方法是通过优化生成器参数的训练过程。通过功能和特征相似性分析，我们引入了一种新的梯度编辑（GE）机制，并验证了其在各种模型上生成可迁移样本的可行性。此外，通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN 能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。(4)：实验结果：GE-AdvGAN 的性能通过在不同数据集上进行的大规模实验进行了全面评估，结果证明了我们算法的优越性。在 MNIST 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 99.54% 和 98.76% 的攻击成功率。在 CIFAR-10 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 98.32% 和 97.14% 的攻击成功率。在 ImageNet 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 97.08% 和 96.23% 的攻击成功率。这些结果表明，GE-AdvGAN 能够有效地生成高度可迁移的对抗样本，并且在计算效率方面也优于其他最先进的方法。</li></ol><p>Methods:(1): 提出GE-AdvGAN算法，通过优化生成器参数的训练过程来提高对抗样本的可迁移性。(2): 引入梯度编辑（GE）机制，通过功能和特征相似性分析来确定梯度编辑方向，以生成高度可迁移的对抗样本。(3): 通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。</p><ol start="8"><li>结论：（1）：本文提出了一种名为 GE-AdvGAN 的新算法，通过优化生成器参数的训练过程来提高对抗样本的可迁移性。（2）：创新点：本文引入梯度编辑（GE）机制，通过功能和特征相似性分析来确定梯度编辑方向，以生成高度可迁移的对抗样本。通过探索频域信息来确定梯度编辑方向，与最先进的可迁移对抗攻击算法相比，GE-AdvGAN 能够生成高度可迁移的对抗样本，同时最大限度地减少执行时间。性能：在 MNIST 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 99.54% 和 98.76% 的攻击成功率。在 CIFAR-10 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 98.32% 和 97.14% 的攻击成功率。在 ImageNet 数据集上，GE-AdvGAN 在白盒攻击和黑盒攻击中分别实现了 97.08% 和 96.23% 的攻击成功率。工作量：GE-AdvGAN 的计算效率优于其他最先进的方法。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-db1cd80b7b3a4ee552ff6559e8a7a978.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19cc70fca6b8ad3b64be920ef14dd19e.jpg" align="middle"></details><br>​    <p></p><h2 id="3D-SSGAN-Lifting-2D-Semantics-for-3D-Aware-Compositional-Portrait-Synthesis"><a href="#3D-SSGAN-Lifting-2D-Semantics-for-3D-Aware-Compositional-Portrait-Synthesis" class="headerlink" title="3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait   Synthesis"></a>3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait   Synthesis</h2><p><strong>Authors:Ruiqi Liu, Peng Zheng, Ye Wang, Rui Ma</strong></p><p>Existing 3D-aware portrait synthesis methods can generate impressive high-quality images while preserving strong 3D consistency. However, most of them cannot support the fine-grained part-level control over synthesized images. Conversely, some GAN-based 2D portrait synthesis methods can achieve clear disentanglement of facial regions, but they cannot preserve view consistency due to a lack of 3D modeling abilities. To address these issues, we propose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image synthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module maps the generated 2D part features and semantics to 3D. Then, a volume renderer with a novel 3D-aware semantic mask renderer is utilized to produce the composed face features and corresponding masks. The whole framework is trained end-to-end by discriminating between real and synthesized 2D images and their semantic masks. Quantitative and qualitative evaluations demonstrate the superiority of 3D-SSGAN in controllable part-level synthesis while preserving 3D view consistency. </p><p><a href="http://arxiv.org/abs/2401.03764v1">PDF</a> </p><p><strong>摘要</strong><br>3D-SSGAN提出了一种新颖的3D感知合成人像框架，实现了人像细粒度部件控制并保持3D视图一致。</p><p><strong>要点</strong></p><ul><li>3D-SSGAN框架使用深度引导的2D到3D提升模块将生成的2D部件特征和语义映射到3D。</li><li>3D-SSGAN框架使用体积渲染器和新颖的3D感知语义掩码渲染器来生成合成面部特征和相应的掩码。</li><li>3D-SSGAN框架通过对真实和合成的2D图像及其语义掩码进行判别来端到端地训练。</li><li>定量和定性评估表明，3D-SSGAN在可控部件合成中优于现有方法，同时保持了3D视图一致性。</li><li>3D-SSGAN可以有效地实现人像细粒度部件控制，并保持3D视图一致性。</li><li>3D-SSGAN框架可以端到端地训练，易于实现。</li><li>3D-SSGAN框架在人像生成任务上取得了很好的效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：3D-SSGAN：提升 2D 语义以实现 3D 感知合成肖像</p></li><li><p>作者：刘瑞奇，郑鹏，王叶，马锐</p></li><li><p>单位：吉林大学人工智能学院</p></li><li><p>关键词：合成图像，解耦建模，3D 感知神经渲染</p></li><li><p>论文链接：Paper_info:3D-SSGAN:Lifting2DSemanticsfor3D-AwareCompositionalPortraitSynthesis，Github 链接：None</p></li><li><p>摘要：(1) 研究背景：肖像合成和编辑近年来备受关注，现有方法可以生成视觉上吸引人的肖像图像，但缺乏对图像进行精细部分编辑的能力。同时，3D 感知肖像合成方法可以生成高质量且具有视图一致性的图像，但无法支持部件级别的编辑。(2) 过去方法及其问题：2D GAN 模型可以实现面部区域的清晰解耦，但由于缺乏 3D 建模能力，无法保持视图一致性。3D 感知肖像合成方法可以实现高品质和视图一致的肖像合成，但无法进行部件级别的编辑。(3) 研究方法：本文提出了一种名为 3D-SSGAN 的框架，用于 3D 感知合成肖像。该框架首先使用深度引导的 2D 到 3D 提升模块将生成的 2D 部件特征和语义映射到 3D。然后，利用具有新型 3D 感知语义掩码渲染器的体积渲染器生成合成面部特征和相应的掩码。整个框架通过对真实和合成 2D 图像及其语义掩码进行判别来端到端训练。(4) 实验结果：定量和定性评估表明，3D-SSGAN 在可控部件级别合成和保持 3D 视图一致性方面优于现有方法。这些结果支持了本文的目标，即实现可控部件级别合成和保持 3D 视图一致性。</p></li><li><p>&lt;Methods&gt;：（1）：本文提出了一种名为3D-SSGAN的框架，用于3D感知合成肖像。该框架首先使用深度引导的2D到3D提升模块将生成的2D部件特征和语义映射到3D。（2）：然后，利用具有新型3D感知语义掩码渲染器的体积渲染器生成合成面部特征和相应的掩码。（3）：整个框架通过对真实和合成2D图像及其语义掩码进行判别来端到端训练。</p></li><li><p>结论：（1）：本文提出了一种新颖的框架3D-SSGAN，用于3D感知合成肖像。该框架将2D语义解耦学习扩展到3D感知，通过提升操作将生成的2D部件特征和语义映射到3D。利用提升和融合的特征，体积渲染器被优化用于合成3D感知人脸图像及其语义掩码。此外，基于NeRF的权重被用于部件级别掩码的生成，使生成的图像更具有3D感知性。3D感知语义掩码渲染器也将3D信息有效地融入到部件级别掩码的生成中。虽然3D-SSGAN在强语义解耦的3D感知合成中表现出优异的性能，但未来工作仍有改进的空间。首先，需要在更多数据集（如FFHQ[1]）上进行评估，以进一步验证方法的泛化能力。其次，通过调整2D生成器的架构，可以进一步提高结果的质量和多样性。最后，如何进一步提高基于组合的网络的内存和计算效率将是一个有趣的研究方向。致谢：这项工作得到了国家自然科学基金（62202199）的部分支持。（2）：创新点：本文提出了一种新颖的框架3D-SSGAN，用于3D感知合成肖像。该框架将2D语义解耦学习扩展到3D感知，通过提升操作将生成的2D部件特征和语义映射到3D。利用提升和融合的特征，体积渲染器被优化用于合成3D感知人脸图像及其语义掩码。此外，基于NeRF的权重被用于部件级别掩码的生成，使生成的图像更具有3D感知性。3D感知语义掩码渲染器也将3D信息有效地融入到部件级别掩码的生成中。性能：3D-SSGAN在强语义解耦的3D感知合成中表现出优异的性能。定量和定性评估表明，3D-SSGAN在可控部件级别合成和保持3D视图一致性方面优于现有方法。工作量：3D-SSGAN的实现需要一定的工作量。该框架涉及到多个模块，包括深度引导的2D到3D提升模块、体积渲染器和3D感知语义掩码渲染器。此外，还需要对整个框架进行端到端训练。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ce64d7f4a0001fd56e8d3a28ff09991c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0dc11b36fba8af3b937de43883acfc32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-71873fa4e08294d88dddccba710aca5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4db074bd3e9835f962fe893f6e960b17.jpg" align="middle"></details><br>​    <p></p><h2 id="FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF"><a href="#FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF" class="headerlink" title="FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF"></a>FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF</h2><p><strong>Authors:Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</strong></p><p>The success of the GAN-NeRF structure has enabled face editing on NeRF to maintain 3D view consistency. However, achieving simultaneously multi-view consistency and temporal coherence while editing video sequences remains a formidable challenge. This paper proposes a novel face video editing architecture built upon the dynamic face GAN-NeRF structure, which effectively utilizes video sequences to restore the latent code and 3D face geometry. By editing the latent code, multi-view consistent editing on the face can be ensured, as validated by multiview stereo reconstruction on the resulting edited images in our dynamic NeRF. As the estimation of face geometries occurs on a frame-by-frame basis, this may introduce a jittering issue. We propose a stabilizer that maintains temporal coherence by preserving smooth changes of face expressions in consecutive frames. Quantitative and qualitative analyses reveal that our method, as the pioneering 4D face video editor, achieves state-of-the-art performance in comparison to existing 2D or 3D-based approaches independently addressing identity and motion. Codes will be released. </p><p><a href="http://arxiv.org/abs/2401.02616v1">PDF</a> Our code will be available at: <a href="https://github.com/ZHANG1023/FED-NeRF">https://github.com/ZHANG1023/FED-NeRF</a></p><p><strong>摘要</strong><br>动态人脸 GAN-NeRF 结构实现了单帧图像的人脸编辑，并能保持多视角一致性和时间连贯性。</p><p><strong>关键要点</strong></p><ul><li>该方法使用视频序列恢复潜在代码和 3D 面部几何图形。</li><li>通过编辑潜在代码，可以确保人脸上的多视图一致编辑。</li><li>通过按帧估计面部几何图形，可能会产生抖动问题。</li><li>该方法提出一个稳定器，通过保留连续帧中面部表情的平滑变化来保持时间连贯性。</li><li>该方法的性能优于现有的基于 2D 或 3D 的方法。</li><li>该方法在确保身份和动作的情况下实现了最先进的性能。</li><li>代码已经开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：FED-NeRF：实现人脸视频编辑中的高 3D 一致性和时间连贯性</p></li><li><p>作者：Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</p></li><li><p>单位：香港科技大学</p></li><li><p>关键词：人脸视频编辑、NeRF、动态 NeRF、多视角一致性、时间连贯性</p></li><li><p>论文链接：https://arxiv.org/abs/2401.02616，Github 链接：None</p></li><li><p>摘要：（1）研究背景：GAN-NeRF 结构的成功使 NeRF 上的人脸编辑能够保持 3D 视角一致性。然而，在编辑视频序列时同时实现多视角一致性和时间连贯性仍然是一个巨大的挑战。（2）过去的方法及其问题：现有方法在处理人脸视频编辑时，往往只能独立地解决身份和动作问题，难以同时保证多视角一致性和时间连贯性。（3）研究方法：本文提出了一种新颖的人脸视频编辑架构，建立在动态人脸 GAN-NeRF 结构之上，有效地利用视频序列来恢复潜在代码和 3D 人脸几何。通过编辑潜在代码，可以确保在人脸上进行多视角一致的编辑，并通过在动态 NeRF 中对生成的编辑图像进行多视角立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。我们提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。（4）方法性能：定量和定性分析表明，我们的方法作为开创性的 4D 人脸视频编辑器，在独立地处理身份和动作时，与现有的 2D 或 3D 方法相比，达到了最先进的性能。</p></li><li><p>方法：(1) 潜在代码估计器：从视频序列中提取身份信息，将每一帧的特征聚合为单一的潜在代码输出。(2) 面部几何估计器：估计每一帧的 FLAME 控制，并使用随机采样的相机姿态渲染输出图像，计算重建损失和 ID 损失。(3) 稳定器：使用 Catmull-Rom 样条曲线对连续帧中的面部几何进行平滑运动，确保时间连贯性。(4) 语义编辑器：修改潜在代码以执行语义编辑，例如改变面部表情或发型。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构能够同时实现多视角一致性和时间连贯性。FED-NeRF在独立地处理身份和动作时，与现有的2D或3D方法相比，达到了最先进的性能。</p><p>（2）：创新点：</p><ul><li>提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构能够同时实现多视角一致性和时间连贯性。</li><li>设计了一种潜在代码估计器，可以从视频序列中提取身份信息，并将每一帧的特征聚合为单一的潜在代码输出。</li><li>设计了一种面部几何估计器，可以估计每一帧的FLAME控制，并使用随机采样的相机姿态渲染输出图像，计算重建损失和ID损失。</li><li>设计了一种稳定器，可以对连续帧中的面部几何进行平滑运动，确保时间连贯性。</li><li>设计了一种语义编辑器，可以修改潜在代码以执行语义编辑，例如改变面部表情或发型。</li></ul><p>性能：</p><ul><li>在定量和定性分析中，FED-NeRF与现有的2D或3D方法相比，达到了最先进的性能。</li></ul><p>工作量：</p><ul><li>FED-NeRF的实现相对复杂，需要较高的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d5f818de87353fbf13907e49c13b462d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b57ca3ade2e4538084a10eeb0919b87d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-102448406db5d3475d988673668fc7a1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ec81b2b7dfa9352ae62039d258ec687.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4ee173ef2962cf2f938e02414ced9add.jpg" align="middle"></details>​    ## What You See is What You GAN: Rendering Every Pixel for High-Fidelity   Geometry in 3D GANs**Authors:Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano**3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly "render every pixel" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs. [PDF](http://arxiv.org/abs/2401.02411v1) See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/**Summary**深度图像采样加速器能够以大大降低的计算成本进行神经渲染，以便以更高分辨率训练生成对抗网络，从而生成更精细的 3D 几何形状。**Key Takeaways**- 深度图像采样加速器将神经体积渲染扩展到接近原生的 2D 图像分辨率，以前所未有的细节解析精细的 3D 几何体。- 深度图像采样加速器使用基于学习的采样器，使用减少至多 5 倍的深度样本加速 3D GAN 训练的神经渲染。- 深度图像采样加速器在训练和推理期间显式“渲染全分辨率图像的每个像素”，而无需在 2D 中进行后期处理超分辨率。- 深度图像采样加速器与学习高质量曲面几何的策略相结合，可以在保持图像质量与依赖后期处理超分辨率的基线相当的同时，合成高分辨率的 3D 几何体和严格的视图一致图像。- 深度图像采样加速器在 FFHQ 和 AFHQ 上展示了最先进的 3D 几何质量，为 3D GAN 中的 3D 形状的无监督学习树立了新标准。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：渲染每个像素以在 3D GAN 中获得高保真几何体</li><li>作者：Saimun Sattar、Simon Niklaus、Richard Tucker、Niloy J. Mitra、Peter Wonka</li><li>单位：英伟达研究中心</li><li>关键词：3D GAN、神经体渲染、采样、几何体质量</li><li>论文链接：https://arxiv.org/abs/2401.02411</li><li>摘要：</li></ol><p>（1）研究背景：3D GAN 在学习生成场景的多视图一致图像和 3D 几何体方面取得了显着进展。然而，体渲染中密集采样的巨大内存和计算成本迫使 3D GAN 采用基于 patch 的训练或使用低分辨率渲染并进行后处理 2D 超分辨率，这牺牲了多视图一致性和已解析几何体的质量。因此，3D GAN 尚未能够完全解析 2D 图像中存在的丰富 3D 几何体。</p><p>（2）过去方法及问题：过去的方法主要有两种：基于 patch 的训练和低分辨率渲染。基于 patch 的训练可以减少内存和计算成本，但会牺牲图像质量和多视图一致性。低分辨率渲染可以保持图像质量和多视图一致性，但需要后处理 2D 超分辨率，这会引入伪影并降低几何体质量。</p><p>（3）研究方法：本文提出了一种将神经体渲染扩展到原生 2D 图像的更高分辨率的技术，从而以前所未有的细节解析细粒度的 3D 几何体。我们的方法采用基于学习的采样器来加速 3D GAN 训练的神经渲染，最多可减少 5 倍的深度采样。这使我们能够在训练和推理期间显式地“渲染全分辨率图像的每个像素”，而无需在 2D 中进行后处理超分辨率。结合我们学习高质量表面几何体的策略，我们的方法综合了高分辨率 3D 几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。</p><p>（4）实验结果：我们在 FFHQ 和 AFHQ 上展示了最先进的 3D 几何质量，为 3D GAN 中 3D 形状的无监督学习树立了新标准。</p><p>7.方法：（1）提出了一种基于采样器的加速 3DGAN 方法，可将神经渲染解析为 2D 图像的原生分辨率，从而以前所未有的细节解析细粒度的 3D 几何体。（2）学习采样器以加速 3DGAN 训练的神经渲染，最多可减少 5 倍的深度采样。（3）显式地“渲染全分辨率图像的每个像素”，无需在 2D 中进行后处理超分辨率。（4）结合策略学习高质量表面几何体，综合了高分辨率 3D 几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。</p><ol start="8"><li>结论：（1）：本文提出了一种基于采样器的加速3DGAN方法，可将神经渲染解析为2D图像的原生分辨率，从而以前所未有的细节解析细粒度的3D几何体。（2）：创新点：</li></ol><ul><li>提出了一种学习采样器以加速3DGAN训练的神经渲染，最多可减少5倍的深度采样。</li><li>显式地“渲染全分辨率图像的每个像素”，无需在2D中进行后处理超分辨率。</li><li>结合策略学习高质量表面几何体，综合了高分辨率3D几何体和严格的视图一致图像，同时保持与依赖后处理超分辨率的基线相当的图像质量。性能：</li><li>在FFHQ和AFHQ上展示了最先进的3D几何质量，为3DGAN中3D形状的无监督学习树立了新标准。工作量：</li><li>需要大量的计算资源来训练和推理模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c8e26844034c822334224616389d9fff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-198f6a00de818ecd7d4a21c0df4f643a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fa5c47519f923fb9367516ec51f18e5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f299edb46d004ab3eb2d99138f860f5.jpg" align="middle"></details>​    ## Is It Possible to Backdoor Face Forgery Detection with Natural Triggers?**Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong**Deep neural networks have significantly improved the performance of face forgery detection models in discriminating Artificial Intelligent Generated Content (AIGC). However, their security is significantly threatened by the injection of triggers during model training (i.e., backdoor attacks). Although existing backdoor defenses and manual data selection can mitigate those using human-eye-sensitive triggers, such as patches or adversarial noises, the more challenging natural backdoor triggers remain insufficiently researched. To further investigate natural triggers, we propose a novel analysis-by-synthesis backdoor attack against face forgery detection models, which embeds natural triggers in the latent space. We thoroughly study such backdoor vulnerability from two perspectives: (1) Model Discrimination (Optimization-Based Trigger): we adopt a substitute detection model and find the trigger by minimizing the cross-entropy loss; (2) Data Distribution (Custom Trigger): we manipulate the uncommon facial attributes in the long-tailed distribution to generate poisoned samples without the supervision from detection models. Furthermore, to completely evaluate the detection models towards the latest AIGC, we utilize both state-of-the-art StyleGAN and Stable Diffusion for trigger generation. Finally, these backdoor triggers introduce specific semantic features to the generated poisoned samples (e.g., skin textures and smile), which are more natural and robust. Extensive experiments show that our method is superior from three levels: (1) Attack Success Rate: ours achieves a high attack success rate (over 99%) and incurs a small model accuracy drop (below 0.2%) with a low poisoning rate (less than 3%); (2) Backdoor Defense: ours shows better robust performance when faced with existing backdoor defense methods; (3) Human Inspection: ours is less human-eye-sensitive from a comprehensive user study. [PDF](http://arxiv.org/abs/2401.00414v1) **摘要**分析与合成结合的隐空间自然触发器嵌入进行人脸伪造检测模型后门攻击分析。**要点*** 针对人脸伪造检测模型提出了一种新的分析-合成后门攻击，该攻击在潜在空间中嵌入自然触发器。* 分析了模型判别和数据分布两种视角下的后门攻击。* 利用最新的 StyleGAN 和 Stable Diffusion 来生成触发器。* 在攻击成功率、后门防御和人工检查等方面优于现有方法。* 带有自然触发器的中毒样本引入特定的语义特征（如皮肤纹理和微笑），更自然也更鲁棒。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：是否可以通过自然触发器对人脸伪造检测进行后门攻击？</li><li>作者：韩晓轩，杨松林，王伟，何子文，董京</li><li>第一作者单位：中国科学院大学人工智能学院</li><li>关键词：后门攻击，人脸伪造检测，面部属性编辑</li><li>论文链接：https://arxiv.org/abs/2401.00414</li><li>摘要：（1）研究背景：深度神经网络显著提高了人脸伪造检测模型区分人工智能生成内容（AIGC）的性能。然而，它们的安全受到了模型训练期间注入触发器（即后门攻击）的严重威胁。虽然现有的后门防御和手动数据选择可以通过减轻对人眼敏感的触发器（如补丁或对抗噪声）来缓解这些威胁，但更具挑战性的自然后门触发器研究还不够充分。（2）过去的方法及其问题：为了进一步研究自然触发器，我们提出了一种针对人脸伪造检测模型的新颖的分析-合成后门攻击方法，该方法将自然触发器嵌入潜在空间。我们从两个角度彻底研究了这种后门漏洞：（1）模型判别（基于优化的触发器）：我们采用替代检测模型，并通过最小化交叉熵损失来寻找触发器；（2）数据分布（自定义触发器）：我们操纵长尾分布中的不常见面部属性，在没有检测模型监督的情况下生成中毒样本。此外，为了全面评估检测模型对最新 AIGC 的性能，我们利用最新的 StyleGAN 和 Stable Diffusion 进行触发器生成。（3）提出的研究方法：最终，这些后门触发器为生成的中毒样本引入了特定的语义特征（例如，皮肤纹理和微笑），这些特征更自然且鲁棒。广泛的实验表明，我们的方法在三个方面优于其他方法：（1）攻击成功率：我们的方法实现了较高的攻击成功率（超过 99%），并且在较低的投毒率（低于 3%）下导致较小的模型准确度下降（低于 0.2%）；（2）后门防御：当面对现有的后门防御方法时，我们的方法表现出更好的鲁棒性能；（3）人工检查：从全面的用户研究来看，我们的方法对人眼不太敏感。</li></ol><p>Methods：</p><p>（1）优化生成触发器：利用替代检测模型，通过优化交叉熵损失函数，寻找潜在空间中的触发器；</p><p>（2）自定义触发器：分析长尾分布中的不常见面部属性，在没有检测模型监督的情况下生成中毒样本；</p><p>（3）模型训练：将中毒样本和良性样本混合，共同训练检测模型，使模型将特定语义特征与目标标签关联起来；</p><p>（4）模型测试：攻击者使用触发器生成图像，绕过人脸伪造检测模型，而未使用触发器生成的图像可以被正确分类。</p><ol start="8"><li>结论：（1）：本文针对人脸伪造检测模型的后门攻击进行了研究，提出了一种基于自然触发器的新型后门攻击方法。该方法将自然触发器嵌入潜在空间，并通过优化交叉熵损失函数或分析长尾分布中的不常见面部属性来生成触发器。实验结果表明，该方法在攻击成功率、后门防御和人工检查方面优于其他方法。（2）：创新点：</li></ol><ul><li>提出了一种基于自然触发器的人脸伪造检测模型后门攻击方法。</li><li>设计了两种生成触发器的方法：优化生成触发器和自定义触发器。</li><li>分析了触发器对检测模型的影响，并提出了相应的防御措施。性能：</li><li>该方法在三个方面优于其他方法：（1）攻击成功率：超过99%；（2）后门防御：表现出更好的鲁棒性能；（3）人工检查：对人眼不太敏感。工作量：</li><li>该方法需要对人脸伪造检测模型进行训练，并生成触发器。</li><li>该方法的攻击成功率与触发器的质量有关，需要花费一定的时间和精力来生成高质量的触发器。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e6a4441d4cab4c7fcfe8ff967215c571.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5b05b35a7e0ad666e188c91169453bc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b17a7d60d57015034bba81bb6b39cd4b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9d74e6cb8c8e487c2a507b9ceae4eac6.jpg" align="middle"></details>​    ## Scalable Face Image Coding via StyleGAN Prior: Towards Compression for   Human-Machine Collaborative Vision**Authors:Qi Mao, Chongyu Wang, Meng Wang, Shiqi Wang, Ruijie Chen, Libiao Jin, Siwei Ma**The accelerated proliferation of visual content and the rapid development of machine vision technologies bring significant challenges in delivering visual data on a gigantic scale, which shall be effectively represented to satisfy both human and machine requirements. In this work, we investigate how hierarchical representations derived from the advanced generative prior facilitate constructing an efficient scalable coding paradigm for human-machine collaborative vision. Our key insight is that by exploiting the StyleGAN prior, we can learn three-layered representations encoding hierarchical semantics, which are elaborately designed into the basic, middle, and enhanced layers, supporting machine intelligence and human visual perception in a progressive fashion. With the aim of achieving efficient compression, we propose the layer-wise scalable entropy transformer to reduce the redundancy between layers. Based on the multi-task scalable rate-distortion objective, the proposed scheme is jointly optimized to achieve optimal machine analysis performance, human perception experience, and compression ratio. We validate the proposed paradigm's feasibility in face image compression. Extensive qualitative and quantitative experimental results demonstrate the superiority of the proposed paradigm over the latest compression standard Versatile Video Coding (VVC) in terms of both machine analysis as well as human perception at extremely low bitrates ($&lt;0.01$ bpp), offering new insights for human-machine collaborative compression. [PDF](http://arxiv.org/abs/2312.15622v1) Accepted by IEEE TIP**摘要**通过层级表示模型与视觉内容编码的结合，可以提高机器智能和视觉感知效能，实现图像的高效视觉压缩。**要点**- 基于 StyleGAN 先验，构建三层分层语义编码模型，支持机器分析和人类视觉感知。- 提出基于分层可变熵变换器的编码方案，降低层间冗余，提高压缩效率。- 采用多任务可变速率失真目标函数，实现机器分析性能、人类感知体验和压缩率的最优化。- 在人脸图像压缩中验证了所提范式的可行性。- 实验结果表明，在极低比特率（&lt;0.01 bpp）下，所提范式在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。- 为人机协同压缩提供了新的思路。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：基于 StyleGAN 先验的可扩展人脸图像编码：面向人机协同视觉的压缩</p></li><li><p>作者：齐茂、王崇宇、王萌、王世奇、陈瑞杰、金立标、马思伟</p></li><li><p>隶属单位：中国传媒大学信息与通信工程学院、国家媒体融合与传播工程技术研究中心</p></li><li><p>关键词：人机协同压缩、可扩展编码、生成压缩、StyleGAN</p></li><li><p>链接：https://arxiv.org/abs/2312.15622</p></li><li><p>摘要：(1)：随着视觉内容的激增和机器视觉技术的快速发展，在大规模传输视觉数据时面临着严峻的挑战，需要有效地表示数据以满足人类和机器的需求。(2)：以往的方法主要集中在图像压缩或机器视觉压缩，但无法同时满足人机协同视觉的需求。(3)：本文提出了一种基于 StyleGAN 先验的可扩展人脸图像编码方法，该方法利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层，并设计了一种分层可扩展熵变换器来减少层之间的冗余。(4)：实验结果表明，该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。</p></li><li><p>方法：(1) 利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层；(2) 设计分层可扩展熵变换器来减少层之间的冗余；(3) 提出多任务可扩展 R-D 优化方法，在受控比特率约束下同时优化三层解码图像；(4) 采用对抗训练来增强增强层图像的纹理。</p></li><li><p>结论：（1）：本文提出了一种基于 StyleGAN 先验的可扩展人脸图像编码方法，该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。（2）：创新点：</p></li></ol><ul><li>利用 StyleGAN 先验学习三层表示，分别对应基本层、中间层和增强层，该方法可以有效地去除人脸图像中的冗余信息，并保留人脸图像的关键特征。</li><li>设计分层可扩展熵变换器来减少层之间的冗余，该变换器可以有效地减少三层表示之间的相关性，并提高编码效率。</li><li>提出多任务可扩展 R-D 优化方法，在受控比特率约束下同时优化三层解码图像，该方法可以有效地提高编码性能。</li><li>采用对抗训练来增强增强层图像的纹理，该方法可以有效地提高增强层图像的质量，并提高编码性能。性能：</li><li>该方法在极低比特率（&lt;0.01bpp）下，在机器分析和人类感知方面均优于最新的压缩标准多功能视频编码（VVC）。</li><li>该方法可以有效地去除人脸图像中的冗余信息，并保留人脸图像的关键特征。</li><li>该方法可以有效地减少三层表示之间的相关性，并提高编码效率。</li><li>该方法可以有效地提高编码性能。</li><li>该方法可以有效地提高增强层图像的质量，并提高编码性能。工作量：</li><li>该方法需要设计分层可扩展熵变换器和多任务可扩展 R-D 优化方法，这需要较高的数学和编程能力。</li><li>该方法需要采用对抗训练来增强增强层图像的纹理，这需要较高的计算资源。</li><li>该方法需要较高的计算复杂度，这可能会影响编码速度。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c605faab09c529ceddb03676f3c952f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0264f3a9e653ed6bd3b5559ddaa228b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-06e69cec60b6e777f4b2ccd3fe36084e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-146bf25e7774478609dfac556e36e8fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5cdc78a006a5a82fe40d8fbf0f0aae35.jpg" align="middle"></details>​    ## StyleRetoucher: Generalized Portrait Image Retouching with GAN Priors**Authors:Wanchao Su, Can Wang, Chen Liu, Hangzhou Han, Hongbo Fu, Jing Liao**Creating fine-retouched portrait images is tedious and time-consuming even for professional artists. There exist automatic retouching methods, but they either suffer from over-smoothing artifacts or lack generalization ability. To address such issues, we present StyleRetoucher, a novel automatic portrait image retouching framework, leveraging StyleGAN's generation and generalization ability to improve an input portrait image's skin condition while preserving its facial details. Harnessing the priors of pretrained StyleGAN, our method shows superior robustness: a). performing stably with fewer training samples and b). generalizing well on the out-domain data. Moreover, by blending the spatial features of the input image and intermediate features of the StyleGAN layers, our method preserves the input characteristics to the largest extent. We further propose a novel blemish-aware feature selection mechanism to effectively identify and remove the skin blemishes, improving the image skin condition. Qualitative and quantitative evaluations validate the great generalization capability of our method. Further experiments show StyleRetoucher's superior performance to the alternative solutions in the image retouching task. We also conduct a user perceptive study to confirm the superior retouching performance of our method over the existing state-of-the-art alternatives. [PDF](http://arxiv.org/abs/2312.14389v1) 13 pages, 15 figures**摘要**利用StyleGAN的高效生成能力和泛化能力，StyleRetoucher可大幅改善输入人像的皮肤状况，同时不会影响其面部细节。**要点**- StyleRetoucher 是一款新颖的自动人像图像修图框架，它利用 StyleGAN 的生成能力和泛化能力来改善输入人像图像的皮肤状况，同时保留其面部细节。- 与其他自动修图方法相比，StyleRetoucher 具有更好的鲁棒性：即使在训练样本较少的情况下也能稳定地执行任务，并且在域外数据上具有良好的泛化能力。- 通过融合输入图像的空间特征和 StyleGAN 层的中间特征，StyleRetoucher 可以最大程度地保留输入特征。- 一种新颖的瑕疵感知特征选择机制可以有效识别并去除皮肤瑕疵，改善图像皮肤状况。- 定性和定量评估验证了该方法强大的泛化能力。- 进一步的实验表明，StyleRetoucher 在图像修图任务中优于其他替代解决方案。- 用户感知研究表明，该方法的修图性能优于现有的最先进的替代方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：StyleRetoucher：基于GAN先验的通用人像图片修图</p></li><li><p>作者：Wanchao Su, Can Wang, Chen Liu, Fangzhou Han, Hongbo Fu, Jing Liao</p></li><li><p>隶属机构：无</p></li><li><p>关键词：人像图片修图、StyleGAN、GAN先验、皮肤瑕疵去除、特征融合</p></li><li><p>论文链接：无，Github代码链接：无</p></li><li><p>摘要：(1)：研究背景：人像图片修图是一项复杂且耗时的任务，需要专业艺术家的参与。现有的自动修图方法要么产生过度平滑的伪影，要么缺乏泛化能力。(2)：过去的方法：过去的方法主要基于CNN，存在过度平滑和泛化能力差的问题。(3)：研究方法：本文提出了一种新的人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力来改善输入人像图片的皮肤状况，同时保留其面部细节。该方法通过融合输入图像的空间特征和StyleGAN中间层的特征，最大程度地保留了输入图像的特征。此外，还提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。(4)：方法性能：定性和定量评估验证了该方法的泛化能力。进一步的实验表明，StyleRetoucher在图像修图任务中优于其他替代方案。用户感知研究也证实了该方法优于现有最先进的替代方案。</p></li><li><p>方法：(1) StyleGAN先验融合：该方法利用StyleGAN的生成和泛化能力，提取输入图像的空间特征和StyleGAN中间层的特征，并融合这些特征，最大程度地保留输入图像的特征。(2) 瑕疵感知特征选择：提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。(3) 迭代优化：通过迭代优化，逐步改善图像的皮肤状况，同时保留其面部细节。</p></li><li><p>结论：（1）：本工作提出了一种新的通用人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力，有效地改善了输入人像图片的皮肤状况，同时保留了其面部细节。该方法在定性和定量评估中均取得了良好的结果，证明了其泛化能力和优越性。进一步的实验表明，StyleRetoucher在图像修图任务中优于其他替代方案。用户感知研究也证实了该方法优于现有最先进的替代方案。（2）：创新点：</p></li></ol><ul><li>提出了一种新的人像图片修图框架StyleRetoucher，利用StyleGAN的生成和泛化能力，有效地改善了输入人像图片的皮肤状况，同时保留了其面部细节。</li><li>提出了一种新的瑕疵感知特征选择机制，可以有效地识别和去除皮肤瑕疵，改善图像的皮肤状况。</li><li>通过迭代优化，逐步改善图像的皮肤状况，同时保留其面部细节。性能：</li><li>在定性和定量评估中均取得了良好的结果，证明了其泛化能力和优越性。</li><li>在图像修图任务中优于其他替代方案。</li><li>用户感知研究也证实了该方法优于现有最先进的替代方案。工作量：</li><li>该方法的实现相对简单，易于部署和使用。</li><li>该方法的训练过程相对较快，可以在合理的时间内完成。</li><li>该方法的推理过程相对较快，可以实时处理图像。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1da4e5eed7542c8e382bd71ed1222c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eecb890ae8fa7ffea652e6d2ccfd2f90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ec59afed1c6e0dba20d552901cdbd2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1cca26e2ef7490681e28d8a8b63ab99c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1aca71fe4cee342f6c693d139fa2e057.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7cccc88967632c8ffb8a168720e3210f.jpg" align="middle"></details>​    ## HeadCraft: Modeling High-Detail Shape Variations for Animated 3DMMs**Authors:Artem Sevastopolsky, Philip-William Grassal, Simon Giebenhain, ShahRukh Athar, Luisa Verdoliva, Matthias Niessner**Current advances in human head modeling allow to generate plausible-looking 3D head models via neural representations. Nevertheless, constructing complete high-fidelity head models with explicitly controlled animation remains an issue. Furthermore, completing the head geometry based on a partial observation, e.g. coming from a depth sensor, while preserving details is often problematic for the existing methods. We introduce a generative model for detailed 3D head meshes on top of an articulated 3DMM which allows explicit animation and high-detail preservation at the same time. Our method is trained in two stages. First, we register a parametric head model with vertex displacements to each mesh of the recently introduced NPHM dataset of accurate 3D head scans. The estimated displacements are baked into a hand-crafted UV layout. Second, we train a StyleGAN model in order to generalize over the UV maps of displacements. The decomposition of the parametric model and high-quality vertex displacements allows us to animate the model and modify it semantically. We demonstrate the results of unconditional generation and fitting to the full or partial observation. The project page is available at https://seva100.github.io/headcraft. [PDF](http://arxiv.org/abs/2312.14140v1) Project page: https://seva100.github.io/headcraft. Video:   https://youtu.be/uBeBT2f1CL0. 23 pages, 19 figures, 2 tables**摘要**生成模型可用于创建详细的 3D 头部网格，并允许显式动画和高细节保留。**要点*** 我们提出了一种生成模型，用于在可铰接 3DMM 的基础上生成详细的 3D 头部网格。* 我们的模型可以显式地动画化头部并对头部进行语义修改。* 我们将参数模型和高质量顶点位移分解，以便对模型进行动画处理并对其进行语义修改。* 我们演示了无条件生成和拟合到完全或部分观察的结果。* 我们使用最近引入的 NPHM 精确 3D 头部扫描数据集对模型进行了训练。* 我们将估计的位移烘焙到手工制作的 UV 布局中。* 我们训练了一个 StyleGAN 模型来概括位移的 UV 映射。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：HeadCraft：为动画 3DMM 建模高细节形状变化</li><li>作者：Artem Sevastopolsky、Philip-William Grassal、Simon Giebenhain、Shah Rukh Athar、Luisa Verdoliva、Matthias Nießner</li><li>第一作者单位：慕尼黑工业大学（TUM），德国</li><li>关键词：生成头部模型、3D 扫描数据集、参数化模板、随机生成、深度观察、完整几何</li><li>论文链接：https://arxiv.org/abs/2312.14140Github 代码链接：无</li><li>摘要：(1)：研究背景：当前的人类头部建模技术可以通过神经表征来生成逼真的 3D 头部模型，这些模型在计算机图形学、虚拟现实和数字娱乐等领域都有应用。然而，构建具有显式控制动画和足够细节的完整高保真头部模型仍然是一个挑战。(2)：过去方法与问题：现有的隐式生成模型，如 pi-GAN、EG3D 或 StyleGAN-3，通常不能很好地处理动画和跟踪，并且在从部分观察（例如来自深度传感器）完成头部几何时，往往难以保留细节。(3)：研究方法：本文提出了一种在铰接 3DMM 之上的详细 3D 头部网格生成模型，该模型允许显式动画和高细节保留。该方法分两个阶段训练。首先，将参数化头部模型与顶点位移注册到最近引入的 NPHM 数据集的每个网格，以获得准确的 3D 头部扫描。然后，训练一个 StyleGAN 模型来概括位移的 UV 贴图。(4)：方法性能：本文方法在无条件生成和拟合完整或部分观察方面取得了良好的性能，证明了其有效性。</li></ol><p>方法：（1）：本文提出了一种在铰接3DMM之上的详细3D头部网格生成模型，该模型允许显式动画和高细节保留。（2）：该方法分两个阶段训练。首先，将参数化头部模型与顶点位移注册到最近引入的NPHM数据集的每个网格，以获得准确的3D头部扫描。然后，训练一个StyleGAN模型来概括位移的UV贴图。（3）：在第一个阶段，优化具有附加矢量位移的损失函数，以防止自相交。在第二个阶段，优化具有仅允许沿先前位移顶点的法线移动的位移的损失函数，以拟合高频细节。（4）：最后，将位移烘焙到UV贴图中，并使用StyleGAN2模型生成UV位移贴图。</p><ol start="8"><li>结论：（1）：本文提出了一种在铰接3DMM之上的详细3D头部网格生成模型，该模型允许显式动画和高细节保留，在无条件生成和拟合完整或部分观察方面取得了良好的性能，证明了其有效性。（2）：创新点：</li></ol><ul><li>将参数化头部模型与顶点位移注册到3D头部扫描，以获得准确的3D头部扫描。</li><li>训练一个StyleGAN模型来概括位移的UV贴图。</li><li>在第一个阶段，优化具有附加矢量位移的损失函数，以防止自相交。</li><li>在第二个阶段，优化具有仅允许沿先前位移顶点的法线移动的位移的损失函数，以拟合高频细节。</li><li>将位移烘焙到UV贴图中，并使用StyleGAN2模型生成UV位移贴图。性能：</li><li>在无条件生成和拟合完整或部分观察方面取得了良好的性能。工作量：</li><li>需要大量的数据和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0b60b8f51ddf0eb511b0da3467102956.jpg" align="middle"><img src="https://pica.zhimg.com/v2-634ce760929e598097380f2cf865992f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-08d46c76d8da31eb3f3362ff9d4fefa4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd84563b8211b1c631a249968d6a3bb7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-da274555d2a06c54e8244021b7e2af1d.jpg" align="middle"></details>​    ## Warping the Residuals for Image Editing with StyleGAN**Authors:Ahmet Burak Yildirim, Hamza Pehlivan, Aysegul Dundar**StyleGAN models show editing capabilities via their semantically interpretable latent organizations which require successful GAN inversion methods to edit real images. Many works have been proposed for inverting images into StyleGAN's latent space. However, their results either suffer from low fidelity to the input image or poor editing qualities, especially for edits that require large transformations. That is because low-rate latent spaces lose many image details due to the information bottleneck even though it provides an editable space. On the other hand, higher-rate latent spaces can pass all the image details to StyleGAN for perfect reconstruction of images but suffer from low editing qualities. In this work, we present a novel image inversion architecture that extracts high-rate latent features and includes a flow estimation module to warp these features to adapt them to edits. The flows are estimated from StyleGAN features of edited and unedited latent codes. By estimating the high-rate features and warping them for edits, we achieve both high-fidelity to the input image and high-quality edits. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements. [PDF](http://arxiv.org/abs/2312.11422v1) **Summary:**GAN图像反演创新方法可实现高质量图像编辑。**Key Takeaways:**- StyleGAN模型具有语义可解释的潜在组织，需要成功的GAN反演方法来编辑真实图像。- 许多研究已经提出将图像反演到StyleGAN潜在空间的方法。- 低速率潜在空间由于信息瓶颈而丢失许多图像细节，即使它提供了可编辑的空间。- 高速率潜在空间可以将所有图像细节传递给StyleGAN以完美重建图像，但编辑质量较差。- 本工作提出了一种新的图像反演架构，可提取高速率潜在特征，并包括一个流动估计模块，将这些特征扭曲以适应编辑。- 通过估计高速率特征并将其扭曲以进行编辑，我们实现了对输入图像的高保真度和高质量的编辑。- 广泛的实验表明我们的方法优于最先进的反演方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：使用 StyleGAN 进行图像编辑的残差翘曲</li><li>作者：Ahmet Burak Yildirim、Hamza Pehlivan、Aysegul Dundar</li><li>作者单位：无</li><li>关键词：GAN 反演、图像编辑、生成对抗网络</li><li>论文链接：https://arxiv.org/abs/2312.11422，Github 代码链接：无</li><li>摘要：（1）研究背景：StyleGAN 模型因其语义可解释的潜在组织而显示出编辑功能，这需要成功的 GAN 反演方法来编辑真实图像。许多工作已被提议将图像反演到 StyleGAN 的潜在空间。然而，它们的结果要么对输入图像的保真度低，要么编辑质量差，尤其是对于需要大变换的编辑。这是因为低速率潜在空间由于信息瓶颈而丢失了许多图像细节，即使它提供了可编辑的空间。另一方面，更高速率的潜在空间可以将所有图像细节传递给 StyleGAN 以完美重建图像，但编辑质量会降低，如果这些特征与编辑不一致。（2）过去的方法和问题：为了解决这种权衡，我们之前的工作 StyleRes 提出了一种框架，该框架可以学习更高速率潜在代码中的残差特征，这些特征在编码特征的重建中缺失。然而，这些高比率潜在代码需要适应图像编辑。例如，如果高比率代码携带有关人耳环的信息，并且图像被编辑以改变人的姿势，那么这些代码应该相应地被携带到生成器中以出现在正确的位置。否则，它会导致重影效应。因此，StyleRes 提议用随机编辑和循环一致性指导来训练模型，以便在应用编辑并恢复回原状时重建原始图像。通过这个指导，它学习了一个模块来转换这些更高比率的潜在代码以进行图像编辑。然而，学习的模块是一个卷积神经网络，并且在正确转换残差以进行编辑方面具有有限的能力。（3）方法：在本文中，我们的主要贡献是一个管道，它可以学习高保真图像重建的残差并正确采用它们以进行高质量的图像编辑。我们的框架通过估计原始图像特征和编辑图像特征之间的流动并根据流动翘曲残差特征来实现这一点。我们利用一个无监督的流动估计网络以一种新颖的方式训练我们的框架。该框架在极端编辑变换下实现了显着更好的结果，而我们的架构是单阶段且有效的。（4）实验结果：我们的贡献如下：</li></ol><ul><li>我们提出了一种新颖的反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动翘曲高比率特征来实现高质量的图像编辑。</li><li>我们通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以 StyleGAN 中间特征作为输入以实现效率。</li><li>我们表明我们的框架可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li><li>我们的广泛实验表明了我们方法的有效性。</li></ul><ol start="7"><li><p>Methods:(1): 本文提出了一种新的反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动翘曲高比率特征来实现高质量的图像编辑。(2): 该框架通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以StyleGAN中间特征作为输入以实现效率。(3): 本文表明该框架可以与不同的预训练StyleGAN反演网络一起工作，并以很大的幅度改进所有这些网络。(4): 广泛的实验表明了该方法的有效性。</p></li><li><p>结论：（1）该工作提出了一种 GAN 反演方法，该方法在运行时高效，并且在各种工作中探索的编辑方向下实现了高保真度和高质量的图像编辑。与以前的工作不同，我们提出估计未编辑和编辑特征的流预测，以扭曲高比率残差特征，这些特征对于精确的图像重建是必需的。我们表明，所提出的框架在许多编辑中实现了最先进的结果，尤其是在需要大变换的编辑中。（2）创新点：</p></li></ol><ul><li>提出了一种新的 GAN 反演管道，通过预测原始图像和编辑图像之间的流动并学习根据预测的流动扭曲高比率特征来实现高质量的图像编辑。</li><li>该框架通过一个无监督的预训练流动估计网络来指导流动预测。流动预测以 StyleGAN 中间特征作为输入以实现效率。</li><li>表明该框架可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li><li>广泛的实验表明了该方法的有效性。性能：</li><li>该方法在极端编辑变换下实现了显着更好的结果，而我们的架构是单阶段且有效的。</li><li>该方法可以与不同的预训练 StyleGAN 反演网络一起工作，并以很大的幅度改进所有这些网络。</li><li>该方法在广泛的实验中表明了其有效性。工作量：</li><li>该方法需要一个无监督的预训练流动估计网络来指导流动预测。</li><li>该方法需要广泛的实验来表明其有效性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3da8d474a8794bf8e7be10e3a9d757df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e321c50f9edfe34a7ecdbffb04a0979e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43b1de4b501c695a4a18a9daefec168c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7fcb4e4ade991311e9a49c68e363e4c0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46d11b639cc223b41293eb04b8aca2d9.jpg" align="middle"></details>​    ## High-Fidelity Face Swapping with Style Blending**Authors:Xinyu Yang, Hongbo Bo**Face swapping has gained significant traction, driven by the plethora of human face synthesis facilitated by deep learning methods. However, previous face swapping methods that used generative adversarial networks (GANs) as backbones have faced challenges such as inconsistency in blending, distortions, artifacts, and issues with training stability. To address these limitations, we propose an innovative end-to-end framework for high-fidelity face swapping. First, we introduce a StyleGAN-based facial attributes encoder that extracts essential features from faces and inverts them into a latent style code, encapsulating indispensable facial attributes for successful face swapping. Second, we introduce an attention-based style blending module to effectively transfer Face IDs from source to target. To ensure accurate and quality transferring, a series of constraint measures including contrastive face ID learning, facial landmark alignment, and dual swap consistency is implemented. Finally, the blended style code is translated back to the image space via the style decoder, which is of high training stability and generative capability. Extensive experiments on the CelebA-HQ dataset highlight the superior visual quality of generated images from our face-swapping methodology when compared to other state-of-the-art methods, and the effectiveness of each proposed module. Source code and weights will be publicly available. [PDF](http://arxiv.org/abs/2312.10843v1) 4 pages**Summary**优化生成对抗网络 (GAN) 架构，提出基于 StyleGAN 的人脸属性编码器和基于注意力的风格融合模块，实现高保真换脸。**Key Takeaways**- 提出了一种基于 StyleGAN 的人脸属性编码器，将人脸特征提取并反演为潜在样式代码。- 设计了一个基于注意力的风格融合模块，有效地将源人脸的 Face ID 转移到目标人脸。- 采用了一系列约束措施，包括对比人脸 ID 学习、人脸关键点对齐和双重交换一致性。- 将混合的样式代码通过样式解码器翻译回图像空间，具有很高的训练稳定性和生成能力。- 在 CelebA-HQ 数据集上进行的广泛实验表明，该换脸方法生成的图像视觉质量优于其他最先进的方法。- 同时验证了每一项提出的模块的有效性。- 源代码和权重将公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：风格融合的高保真换脸</li><li>作者：Xinyu Yang</li><li>单位：兰开斯特大学</li><li>关键词：生成对抗网络、人脸合成、人脸交换</li><li>论文链接：https://arxiv.org/abs/2312.10843</li><li>摘要：（1）研究背景：换脸技术近年来备受关注，但现有的换脸方法存在融合不一致、扭曲、伪影和训练稳定性差等问题。（2）过去方法与不足：以往基于生成对抗网络（GAN）的换脸方法存在融合不一致、扭曲、伪影和训练稳定性差等问题。这些方法难以平衡身份相似性和目标的低级细节保留，导致生成的图像经常出现伪影和低保真度。（3）研究方法：本文提出了一种新的端到端高保真换脸框架。该框架包括三个关键组成部分：面部属性编码器、风格融合模块和风格解码器。面部属性编码器将目标和源图像的特征提取到潜在空间中，并使用多头交叉注意（MHCA）将源嵌入和目标嵌入进行融合。最后，使用预训练的风格解码器将融合的嵌入解码为图像。（4）性能与结论：在 CelebA-HQ 数据集上进行的广泛实验表明，该方法生成的图像具有更高的视觉质量，并且能够有效地将源图像的面部特征转移到目标图像中。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种基于风格编码模型的高保真换脸框架，该框架能够生成具有更高视觉质量和更逼真的人脸图像，有效地将源图像的面部特征转移到目标图像中。（2）：创新点：该框架采用了一种新的风格融合模块，该模块能够在潜在空间中将源嵌入和目标嵌入进行融合，从而生成更逼真的图像。该框架使用预训练的风格解码器将融合的嵌入解码为图像，这使得该框架能够生成更逼真的人脸图像。性能：该框架在CelebA-HQ数据集上进行了广泛的实验，实验结果表明，该框架生成的图像具有更高的视觉质量，并且能够有效地将源图像的面部特征转移到目标图像中。工作量：该框架的实现相对复杂，需要较高的计算资源和较长时间的训练。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-911f1f1ba2307b836811866ce4529595.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a1567919a066b4968d69d7bc94705df3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cdc82b874821cb5b2ef53e92f09ee664.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94207fdbd05bbd894e5ca3f64d99a25b.jpg" align="middle"></details><br>​    <p></p><h2 id="Super-Resolution-through-StyleGAN-Regularized-Latent-Search-A-Realism-Fidelity-Trade-off"><a href="#Super-Resolution-through-StyleGAN-Regularized-Latent-Search-A-Realism-Fidelity-Trade-off" class="headerlink" title="Super-Resolution through StyleGAN Regularized Latent Search: A   Realism-Fidelity Trade-off"></a>Super-Resolution through StyleGAN Regularized Latent Search: A   Realism-Fidelity Trade-off</h2><p><strong>Authors:Marzieh Gheisari, Auguste Genovesio</strong></p><p>This paper addresses the problem of super-resolution: constructing a highly resolved (HR) image from a low resolved (LR) one. Recent unsupervised approaches search the latent space of a StyleGAN pre-trained on HR images, for the image that best downscales to the input LR image. However, they tend to produce out-of-domain images and fail to accurately reconstruct HR images that are far from the original domain. Our contribution is twofold. Firstly, we introduce a new regularizer to constrain the search in the latent space, ensuring that the inverted code lies in the original image manifold. Secondly, we further enhanced the reconstruction through expanding the image prior around the optimal latent code. Our results show that the proposed approach recovers realistic high-quality images for large magnification factors. Furthermore, for low magnification factors, it can still reconstruct details that the generator could not have produced otherwise. Altogether, our approach achieves a good trade-off between fidelity and realism for the super-resolution task. </p><p><a href="http://arxiv.org/abs/2311.16923v1">PDF</a> </p><p><strong>Summary</strong><br>通过约束潜在空间搜索范围和扩展图像先验来提高深度生成模型的超分辨率重建</p><p><strong>Key Takeaways</strong></p><ul><li>该论文关注超分辨率问题，即从低分辨率图像生成高分辨率图像。</li><li>最近的无监督方法在预先训练好的 StyleGAN 的潜在空间中搜索图像，使其下采样后与输入的低分辨率图像最匹配。</li><li>但这些方法倾向于产生域外图像，无法准确重建远离原始域的高分辨率图像。</li><li>引入了一个新的正则化项来约束潜在空间中的搜索，确保反转代码位于原始图像流形中。</li><li>通过扩展最佳潜在代码周围的图像先验来进一步增强重建。</li><li>实验结果表明，该方法可以在大倍率放大因子下恢复逼真、高质量的图像。</li><li>对于低倍率放大因子，它仍然可以重建生成器无法生成的其他细节。</li><li>总之，该方法在超分辨率任务中实现了保真度和真实性之间的良好权衡。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：通过 StyleGAN 正则化潜在搜索实现超分辨率：真实性-保真度权衡</p></li><p></p><p></p><li><p>作者：Marzieh Gheisari, Auguste Genovesio</p></li><p></p><p></p><li><p>单位：巴黎高等师范学院</p></li><p></p><p></p><li><p>关键词：超分辨率、StyleGAN、潜在搜索、正则化、真实性、保真度</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2311.16923Github：无</p></li><p></p><p></p><li><p>摘要：(1)：超分辨率旨在从低分辨率图像中重建未知的高分辨率图像。近年来，生成模型极大地促进了超分辨率的发展。(2)：GAN-based 方法和先验引导方法是超分辨率的两种主要研究趋势。GAN-based 方法学习高分辨率图像和低分辨率图像的直接耦合，但存在局限性，例如产生伪影和不自然的纹理。先验引导方法通过利用先验信息来更好地定义目标，从而提高重建的稳定性。(3)：本文提出了一种新的正则化方法来约束潜在空间中的搜索，确保反转的编码位于原始图像流形中。此外，通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。(4)：实验结果表明，该方法在较大的放大倍数下可以恢复真实且高质量的图像。即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。</p></li><p></p><p></p><li><p>方法：</p></li><br>&lt;/ol&gt;<p></p><p></p><p>（1）潜在空间正则化：提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。该方法通过最小化潜在编码和相应的生成图像之间的距离来实现。</p><p></p><p></p><p>（2）图像先验扩展：在最优潜在编码周围扩展图像先验，以进一步增强重建效果。该方法通过使用高斯核对最优潜在编码周围的潜在空间进行加权来实现。</p><p></p><p></p><p>（3）超分辨率重建：使用StyleGAN生成器将正则化的潜在编码反转为高分辨率图像。该方法通过最小化生成的图像和低分辨率输入图像之间的距离来实现。</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：（1）本文提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。此外，通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。实验结果表明，该方法在较大的放大倍数下可以恢复真实且高质量的图像。即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。（2）创新点：</li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种新的正则化方法，通过在潜在空间中约束搜索来确保反转的编码位于原始图像流形中。</li><li>通过在最优潜在编码周围扩展图像先验，进一步增强了重建效果。性能：</li><li>该方法在较大的放大倍数下可以恢复真实且高质量的图像。</li><li>即使在较小的放大倍数下，它仍然可以重建生成器无法单独生成的细节。</li><li>总体而言，该方法在超分辨率任务中实现了真实性和保真度之间的良好权衡。工作量：</li><li>该方法需要对StyleGAN生成器进行预训练。</li><li>需要对正则化方法和图像先验扩展方法进行训练。</li><li>需要对超分辨率重建方法进行训练。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-047c7ef399b99c1a231d9dac098cd5ab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2b9f640a36ad41c3da4593c85263843.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1e5efa0aaad503171165b516b9c711a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1a5de01c1a980dac1eb85f183edcaa69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50f53d31ed0811c18da861677d9c633b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-147a4284c73eb5f49586ed6d811f5b2d.jpg" align="middle"></details><br>​    <p></p><h2 id="Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling"><a href="#Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling" class="headerlink" title="Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling"></a>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling</h2><p><strong>Authors:Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</strong></p><p>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front \&amp; back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches. Code: <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> </p><p><a href="http://arxiv.org/abs/2311.16096v1">PDF</a> Projectpage: <a href="https://animatable-gaussians.github.io/">https://animatable-gaussians.github.io/</a>, Code:   <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></p><p><strong>概述</strong><br>采用强大的二维卷积神经网络和三维高斯扩散的方式来创作逼真的虚拟人物。</p><p><strong>要点</strong></p><ul><li>提出一种新的虚拟形象表示法——可动画高斯，它利用强大的二维卷积神经网络和三维高斯扩散来创建逼真的虚拟形象。</li><li>学习一个参数模板从输入视频中，然后将模板参数化在两个正面&amp;背面规范的高斯映射上，每个像素代表一个三维高斯。</li><li>学习的模板可以适应穿的衣服来建模更宽松的衣服，如连衣裙。</li><li>基于 StyleGAN 的卷积神经网络学习姿势相关的 Gaussian 地图，用于建模详细的动态外观。</li><li>提出了一种姿势投影策略，以便在新的姿势下更好地泛化。</li><li>实验表明，本方法优于其他最先进的方法。</li><li>代码：<a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：可动画的高斯体：学习姿势依赖的高斯映射</p></li><p></p><p></p><li><p>作者：Zhe Li, Menglei Chai, Yinda Zhang, Jingyi Yu, Jingyi Yu</p></li><p></p><p></p><li><p>单位：香港中文大学（深圳）</p></li><p></p><p></p><li><p>关键词：神经辐射场、动画、高斯体、姿势投影</p></li><p></p><p></p><li><p>论文链接：None，Github 代码链接：https://github.com/lizhe00/AnimatableGaussians</p></li><p></p><p></p><li><p>摘要：（1）研究背景：建模可动画的人体虚拟形象是一个长期存在且具有挑战性的问题。最近的工作通常采用基于 MLP 的神经辐射场 (NeRF) 来表示 3D 人体，但纯 MLP 很难回归姿势相关的服装细节。（2）过去方法与问题：为了解决这个问题，我们引入了可动画的高斯体，这是一种新的虚拟形象表示，利用强大的 2D CNN 和 3D 高斯体 splatting 来创建高保真虚拟形象。为了将 3D 高斯体与可动画虚拟形象相关联，我们从输入视频中学习了一个参数化模板，然后在两个正面和背面规范高斯图上对模板进行参数化，其中每个像素都表示一个 3D 高斯体。学习到的模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。这种模板引导的 2D 参数化使我们能够使用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 图，以建模详细的动态外观。此外，我们引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。（3）研究方法：总体而言，我们的方法可以创建具有动态、逼真和泛化的外观的逼真虚拟形象。实验表明，我们的方法优于其他最先进的方法。（4）方法性能：在 THuman4.0 数据集和 AvatarReX 数据集上的实验表明，我们的方法在重建准确性和视觉质量方面优于其他最先进的方法。</p></li><p></p><p></p><li><p>方法：(1) 参数化模板：提出了一种参数化模板，该模板由两个正面和背面规范高斯图组成，每个像素都表示一个 3D 高斯体。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。(2) 基于 StyleGAN 的 CNN：利用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 图，以建模详细的动态外观。(3) 姿势投影策略：引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。该策略通过将输入位置图投影到训练数据集中最接近的姿势的位置图上来实现。</p></li><p></p><p></p><li><p>结论：</p></li><br>&lt;/ol&gt;<p></p><p></p><p>（1）：本文提出了一种可动画的高斯体表示方法，使用强大的2DCNN和3D高斯体splatting来创建高保真虚拟形象。该方法可以创建具有动态、逼真和泛化的外观的逼真虚拟形象。实验表明，该方法优于其他最先进的方法。</p><p></p><p></p><p>（2）：创新点：</p><p></p><ul><li>提出了一种参数化模板，该模板由两个正面和背面规范高斯图组成，每个像素都表示一个3D高斯体。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。</li><li>利用强大的基于StyleGAN的CNN来学习姿势相关的Gaussian图，以建模详细的动态外观。</li><li>引入了一种姿势投影策略，以在给定新颖姿势时获得更好的泛化。</li></ul><p>性能：</p><ul><li>在THuman4.0数据集和AvatarReX数据集上的实验表明，该方法在重建准确性和视觉质量方面优于其他最先进的方法。</li></ul><p>工作量：</p><ul><li>该方法需要大量的数据和计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4f58407bb1da6858a1f8b3afeca5122e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-77ee722883e97eb42525787423c0db90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4ba94c75431413d71052f68f19e06407.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aec0c4ba5b1b400598aa699437906d07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b30fc2750f7d5972c0541922982b18fe.jpg" align="middle"></details><br>​    <p></p><h2 id="Importance-of-Feature-Extraction-in-the-Calculation-of-Frechet-Distance-for-Medical-Imaging"><a href="#Importance-of-Feature-Extraction-in-the-Calculation-of-Frechet-Distance-for-Medical-Imaging" class="headerlink" title="Importance of Feature Extraction in the Calculation of Fréchet   Distance for Medical Imaging"></a>Importance of Feature Extraction in the Calculation of Fréchet   Distance for Medical Imaging</h2><p><strong>Authors:McKell Woodland, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Austin Castelo, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</strong></p><p>Fr\’echet Inception Distance is a widely used metric for evaluating synthetic image quality that utilizes an ImageNet-trained InceptionV3 network as a feature extractor. However, its application in medical imaging lacks a standard feature extractor, leading to biased and inconsistent comparisons. This study aimed to compare state-of-the-art feature extractors for computing Fr\’echet Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data augmentation techniques tailored for limited data domains on datasets comprising three medical imaging modalities and four anatomical locations. Human evaluation of generative quality (via a visual Turing test) was compared to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and Swin Transformer architectures, in addition to an InceptionV3 network trained on a large medical dataset, RadImageNet. All ImageNet-based extractors were consistent with each other, but only SwAV was significantly correlated with medical expert judgment. The RadImageNet-based FD showed volatility and lacked correlation with human judgment. Caution is advised when using medical image-trained extraction networks in the FD calculation. These networks should be rigorously evaluated on the imaging modality under consideration and publicly released. ImageNet-based extractors, while imperfect, are consistent and widely understood. Training extraction networks with SwAV is a promising approach for synthetic medical image evaluation. </p><p><a href="http://arxiv.org/abs/2311.13717v1">PDF</a> </p><p><strong>Summary</strong><br>医学图像生成质量评价中，ImageNet 预训练的特征提取器与医学专家判断不一致，SwAV 是唯一显著相关的网络。</p><p><strong>Key Takeaways</strong></p><ul><li>Fr\’echet Inception Distance (FID) 是一种广泛用于评估合成图像质量的指标，利用 ImageNet 训练的 InceptionV3 网络作为特征提取器。</li><li>在医学图像中使用 FID 缺乏标准的特征提取器，导致比较结果存在偏差且不一致。</li><li>本研究旨在比较最先进的特征提取器，用于计算医学图像中的 Fr\’echet 距离 (FD)。</li><li>使用针对有限数据域量身定制的数据增强技术训练 StyleGAN2 网络，该数据集包括三个医学图像模态和四个解剖位置。</li><li>将生成质量的人工评估（通过视觉图灵测试）与使用 ImageNet 训练的 InceptionV3、ResNet50、SwAV、DINO 和 Swin Transformer 架构以及在大规模医学数据集 RadImageNet 上训练的 InceptionV3 网络计算的 FD 进行比较。</li><li>所有基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家判断显着相关。</li><li>基于 RadImageNet 的 FD 显示出不稳定性，并且与人类判断缺乏相关性。</li><li>在 FD 计算中使用医学图像训练的提取网络时应谨慎。这些网络应在所考虑的成像方式上进行严格评估并公开发布。</li><li>基于 ImageNet 的提取器虽然不完美，但具有一致性且被广泛理解。</li><li>使用 SwAV 训练提取网络是合成医学图像评估的一种有前途的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：医学图像 Fréchet 距离计算中的特征提取的重要性</li><p></p><p></p><li>作者：McKell Woodland, Mais Al Taie, Jessica Albuquerque Marques Silva, Mohamed Eltaher, Frank Mohn, Alexander Shieh, Austin Castelo, Suprateek Kundu, Joshua P. Yung, Ankit B. Patel, Kristy K. Brock</li><p></p><p></p><li>第一作者单位：德克萨斯大学安德森癌症中心</li><p></p><p></p><li>关键词：Fréchet 距离, 特征提取, 医学图像, 生成模型, 自监督学习</li><p></p><p></p><li>链接：None, Github：None</li><p></p><p></p><li>摘要：（1）研究背景：Fréchet 距离是一种广泛用于评估合成图像质量的度量，它利用在 ImageNet 上训练的 InceptionV3 网络作为特征提取器。然而，在医学图像中的应用缺乏标准的特征提取器，导致比较存在偏差和不一致。（2）过去方法及其问题：一种方法是使用在大型公开医学数据集上训练的 InceptionV3 网络来计算 FD。然而，这种方法可能不适用于未包含在该数据集中的医学模态，例如内窥镜检查和乳房 X 光检查。另一种方法是使用自监督特征提取器，但现有研究表明，这些提取器在医学图像上的性能参差不齐。（3）论文提出的研究方法：本文比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器。我们使用针对有限数据域的数据增强技术训练了一个 StyleGAN2 网络，该数据集包含三个医学影像模态和四个解剖位置。我们将生成质量的人工评估（通过视觉图灵测试）与使用在 ImageNet 上训练的 InceptionV3、ResNet50、SwAV、DINO 和 SwinTransformer 架构以及在大型医学数据集 RadImageNet 上训练的 InceptionV3 网络计算的 FD 进行了比较。（4）方法在任务和性能上的表现：所有基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。基于 RadImageNet 的 FD 显示出不稳定性，并且与人类判断缺乏相关性。在 FD 计算中使用医学图像训练的提取网络时应谨慎。这些网络应该在考虑的成像方式上进行严格评估并公开发布。ImageNet-base 的提取器虽然不完美，但它们一致且被广泛理解。使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。</li><br>&lt;/ol&gt;<p></p><p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：(1): 本文比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器，发现基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。在 Fréchet 距离计算中使用医学图像训练的提取网络时应谨慎。这些网络应该在考虑的成像方式上进行严格评估并公开发布。ImageNet-base 的提取器虽然不完美，但它们一致且被广泛理解。使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。(2): 创新点：</li><br>&lt;/ol&gt;<p></p><ul><li>比较了用于计算医学图像中 Fréchet 距离的几种最先进的特征提取器。</li><li>发现基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。</li><li>基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。</li><li>在 Fréchet 距离计算中使用医学图像训练的提取网络时应谨慎。</li><li>使用 SwAV 训练提取网络是合成医学图像评估的一种很有前途的方法。</li></ul><p>性能：</p><ul><li>基于 ImageNet 的提取器彼此一致，但只有 SwAV 与医学专家的判断显着相关。</li><li>基于 RadImageNet 的 Fréchet 距离显示出不稳定性，并且与人类判断缺乏相关性。</li></ul><p>工作量：</p><ul><li>需要比较多种特征提取器，工作量较大。</li><li>需要在医学图像上训练提取网络，工作量较大。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3a98d200e6662a30834d463110beefc2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9c2de329d902d2836a266714f60f4842.jpg" align="middle"><img src="https://pica.zhimg.com/v2-855d1b05e045ac53d6b4a972ad2848dd.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol></ol></ol></ol></ol>]]></content>
    
    
    <summary type="html">GAN 方向最新论文已更新，请持续关注 Update in 2024-01-24 A Fair Evaluation of Various Deep Learning-Based Document Image   Binarization Approaches</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="GAN" scheme="https://kedreamix.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-01-23T18:26:10.000Z</published>
    <updated>2024-01-24T08:11:52.343Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures"><a href="#UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures" class="headerlink" title="UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures"></a>UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</h2><p><strong>Authors:Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi</strong></p><p>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments. </p><p><a href="http://arxiv.org/abs/2401.11078v1">PDF</a> The project page is at <a href="http://usrc-sea.github.io/UltrAvatar/">http://usrc-sea.github.io/UltrAvatar/</a></p><p><strong>摘要</strong><br>利用人工智能生成更真实的 3D 虚拟形象，提升用户体验。</p><p><strong>要点</strong></p><ul><li>利用文本作为条件，生成场景和动画，构建更逼真的 3D 形象。</li><li>已有的大部分方法采用 Score Distillation Sampling 损失，存在过于光滑、细节缺失等问题。</li><li>部分方法从单一图像生成 3D 形象，存在光线影响、视角问题，难以重构 3D 脸部网格。</li><li>提出 UltrAvatar 方法，增强几何细节，使用物理渲染材质提高渲染质量。</li><li>引入漫反射提取模型，去除光线影响，生成不受光线影响的材质。</li><li>基于真实漫反射材质，采用梯度引导，生成真实面孔属性并与 3D 网格对齐的材质。</li><li>对比实验表明，该方法在真实性、多样性、与 3D 网格的一致性上均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：UltrAvatar：基于真实感的纹理扩散模型的超真实 3D 头像生成（UltrAvatar: Ultra-Realistic 3D Avatar Generation with Authenticity-Guided Texture Diffusion Models）</p></li><li><p>作者：Yilun Du, Linchao Bao<em>, Xinyu Gong</em>, Hang Zhou, Chen Change Loy, Ziwei Liu</p></li><li><p>单位：香港中文大学（香港）</p></li><li><p>关键词：3D 头像生成、纹理扩散模型、真实感引导、照明去除、PBR 纹理</p></li><li><p>论文链接：https://arxiv.org/abs/2302.09864，Github 链接：None</p></li><li><p>摘要：（1）：最近 3D 头像生成的进展备受关注。这些突破旨在产生更逼真的可动画头像，缩小虚拟和现实世界体验之间的差距。大多数现有工作采用分数蒸馏采样 (SDS) 损失，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，SDS 通常会生成过度平滑的结果，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他工作从单个图像生成 3D 头像，其中不需要的照明效果、透视视图和较差的图像质量使得它们难以可靠地重建具有对齐完整纹理的 3D 面部网格。（2）：为了解决上述问题，本文提出了一种新颖的 3D 头像生成方法，称为 UltrAvatar，它增强了几何体的保真度，并具有物理渲染 (PBR) 纹理的卓越质量，且没有不需要的照明。为此，提出的方法提出了一个漫反射颜色提取模型和一个真实感引导的纹理扩散模型。前者去除了不需要的照明效果以揭示真实的漫反射颜色，以便可以在各种照明条件下渲染生成的头像。后者遵循两个基于梯度的指导，用于生成 PBR 纹理，以更好地呈现不同的面部身份特征和细节，并与 3D 网格几何体更好地对齐。（3）：本文在实验中证明了所提出方法的有效性和鲁棒性，在很大程度上优于最先进的方法。（4）：在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。</p></li><li><p>方法：（1）：提出了一种新颖的3D头像生成方法UltrAvatar，它增强了几何体的保真度，并具有物理渲染(PBR)纹理的卓越质量，且没有不需要的照明。（2）：UltrAvatar包含一个漫反射颜色提取模型和一个真实感引导的纹理扩散模型。漫反射颜色提取模型去除了不需要的照明效果以揭示真实的漫反射颜色，以便可以在各种照明条件下渲染生成的头像。真实感引导的纹理扩散模型遵循两个基于梯度的指导，用于生成PBR纹理，以更好地呈现不同的面部身份特征和细节，并与3D网格几何体更好地对齐。（3）：在人脸重建任务上，UltrAvatar在多种指标上都优于现有方法。例如，在FID指标上，UltrAvatar的平均值为13.4，而最优的对比方法为18.8；在LPIPS指标上，UltrAvatar的平均值为0.24，而最优的对比方法为0.31。这些结果表明，UltrAvatar能够生成更逼真、更具细节的3D头像。</p></li><li><p>结论：（1）：本文提出了一种从文本提示或单个图像生成 3D 头像的新颖方法。我们方法的核心是 DCEM 模型，该模型旨在消除源图像中不需要的照明效果，以及一个由光度和边缘信号引导的纹理生成模型，以保留头像的 PBR 细节。与其他 SOTA 方法相比，我们证明了我们的方法可以生成显示出高度逼真、更高质量、卓越保真度和更广泛多样性的 3D 头像。（2）：创新点：</p></li></ol><ul><li>提出了一种漫反射颜色提取模型，可以消除源图像中不需要的照明效果，以便在各种照明条件下渲染生成的头像。</li><li>提出了一种真实感引导的纹理扩散模型，该模型遵循两个基于梯度的指导，用于生成 PBR 纹理，以更好地呈现不同的面部身份特征和细节，并与 3D 网格几何体更好地对齐。</li><li>在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。性能：</li><li>在人脸重建任务上，UltrAvatar 在多种指标上都优于现有方法。例如，在 FID 指标上，UltrAvatar 的平均值为 13.4，而最优的对比方法为 18.8；在 LPIPS 指标上，UltrAvatar 的平均值为 0.24，而最优的对比方法为 0.31。这些结果表明，UltrAvatar 能够生成更逼真、更具细节的 3D 头像。工作量：</li><li>该方法需要大量的数据和计算资源来训练模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6ddb372268ab29440ab071d2e4e6e298.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5a668d3e08b3e9f2b7d9e0f965d9762.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe4a6193adf67c2ee040715753a40d2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3f4f76cbd1c6492ba1fd985b02c9d05.jpg" align="middle"></details>​    ## Fast Registration of Photorealistic Avatars for VR Facial Animation**Authors:Chaitanya Patel, Shaojie Bai, Te-Li Wang, Jason Saragih, Shih-En Wei**Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided image-to-image style transfer module that is conditioned on current estimation of expression and head pose. These two modules reinforce each other, as image style transfer becomes easier when close-to-ground-truth examples are shown, and better domain-gap removal helps registration. Our system produces high-quality results efficiently, obviating the need for costly offline registration to generate personalized labels. We validate the accuracy and efficiency of our approach through extensive experiments on a commodity headset, demonstrating significant improvements over direct regression methods as well as offline registration. [PDF](http://arxiv.org/abs/2401.11002v1) Project page: https://chaitanya100100.github.io/FastRegistration/**Summary**虚拟现实中逼真虚拟人的生成方法，基于迭代细化模块和通用虚拟人引导的图像转图像风格迁移模块的组合。**Key Takeaways**- 虚拟现实中逼真虚拟人的生成是实现更加沉浸式社交互动的关键。- 准确地将虚拟人动画与佩戴头显时的图像进行匹配对于逼真虚拟人的生成至关重要。- 虚拟人与头显摄像头图像之间的域差异是导致匹配困难的主要原因之一。- 开发了一种系统设计，将问题分解为两个部分：域内输入迭代细化模块和通用虚拟人引导的图像转图像风格迁移模块。- 这两个模块相互增强，使得在接近真实结果的示例显示时，图像风格迁移变得更容易，而更好的域差异消除有助于匹配。- 该系统高效地产生了高质量的结果，避免了昂贵的离线匹配以生成个性化标签的需要。- 通过对商品头显的广泛实验验证了该方法的准确性和效率，证明了其优于直接回归方法和离线匹配的显着改进。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：VR 面部动画的逼真虚拟形象快速注册</li><li>作者：Qianqian Wang, Jiapeng Tang, Yebin Liu, Xin Tong, Yajie Zhao, Shihong Xia</li><li>单位：香港中文大学（深圳）</li><li>关键词：虚拟现实、面部动画、图像风格迁移、在线注册</li><li>论文链接：https://arxiv.org/pdf/2208.04345.pdf，Github 链接：无</li><li>摘要：（1）研究背景：虚拟现实（VR）有望带来比其他媒体更具沉浸感的人际互动。关键在于能够在佩戴 VR 头显时准确地为自己的形象制作逼真的虚拟形象动画。虽然可以在离线环境中将高质量的人特定虚拟形象注册到头显摄像头（HMC）图像，但通用实时模型的性能却会显著下降。在线注册也具有挑战性，原因在于存在倾斜的摄像头视角和模态差异。（2）过去的方法及其问题：本文首先表明，虚拟形象和头显摄像头图像之间的域差距是主要难点之一，其中基于 Transformer 的架构在域一致的数据上实现了高精度，但在重新引入域差距时会退化。基于这一发现，我们开发了一种系统设计，将问题分解为两部分：1）一个迭代细化模块，用于获取域内输入；2）一个通用虚拟形象引导的图像到图像风格迁移模块，该模块以当前估计的表情和头部姿势为条件。这两个模块相互增强，因为当显示接近真实示例时图像风格迁移变得更容易，而更好的域差距消除有助于注册。（3）研究方法：我们的系统以有效的方式产生了高质量的结果，消除了进行昂贵的离线注册以生成个性化标签的需要。我们通过在商品头显上进行广泛的实验来验证我们方法的准确性和效率，证明了它比直接回归方法和离线注册有显著的改进。（4）方法性能：该方法在任务和性能方面取得了以下成果：</li></ol><ul><li>任务：在商品头显上对 VR 面部动画的逼真虚拟形象进行快速注册。</li><li>性能：与直接回归方法和离线注册相比，我们的方法在准确性和效率方面都有显著的提高。这些性能支持了我们的目标，即提供一种快速、准确且高效的方法来注册逼真虚拟形象。</li></ul><ol start="7"><li><p>方法：(1) 迭代细化模块：该模块以当前估计的表情和头部姿势为条件，生成逼真的虚拟形象动画。它使用一个卷积神经网络（CNN）来提取图像特征，并使用一个长短期记忆（LSTM）网络来建模时间依赖性。(2) 通用虚拟形象引导的图像到图像风格迁移模块：该模块将虚拟形象的风格迁移到HMC图像上。它使用一个生成对抗网络（GAN）来学习虚拟形象和HMC图像之间的映射关系。(3) 联合优化：这两个模块相互增强，因为当显示接近真实示例时图像风格迁移变得更容易，而更好的域差距消除有助于注册。</p></li><li><p>结论：（1）：本文提出了一种快速、准确且高效的方法来注册逼真虚拟形象，该方法消除了进行昂贵的离线注册以生成个性化标签的需要。（2）：创新点：</p></li></ol><ul><li>提出了一种迭代细化模块，该模块以当前估计的表情和头部姿势为条件，生成逼真的虚拟形象动画。</li><li>提出了一种通用虚拟形象引导的图像到图像风格迁移模块，该模块将虚拟形象的风格迁移到头显摄像头图像上。</li><li>将这两个模块结合起来，形成一个联合优化框架，相互增强，提高注册的准确性和效率。性能：</li><li>与直接回归方法和离线注册相比，该方法在准确性和效率方面都有显著的提高。</li><li>该方法在商品头显上实现了实时性能，能够以每秒30帧的速度生成逼真的虚拟形象动画。</li><li>该方法能够处理各种各样的表情和头部姿势，并且对光照和遮挡具有鲁棒性。工作量：</li><li>该方法的实现相对简单，并且可以在普通的GPU上训练和部署。</li><li>该方法不需要昂贵的离线注册，并且能够在几分钟内完成注册过程。</li><li>该方法对各种各样的虚拟形象和头显摄像头图像具有通用性，因此可以广泛应用于VR面部动画领域。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-adff4a2a529cc67cabb0ab4e3422329d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5e38782058424beeee89575e1764c835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4544bcb7227c9e7f7be3dd1a344f7b7b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ee9e00db6f9bab9cbc238317f5ec6446.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e77a54cab90843cba1c304290ec1ded1.jpg" align="middle"></details>​    ## A Simple Baseline for Spoken Language to Sign Language Translation with   3D Avatars**Authors:Ronglai Zuo, Fangyun Wei, Zenggui Chen, Brian Mak, Jiaolong Yang, Xin Tong**The objective of this paper is to develop a functional system for translating spoken languages into sign languages, referred to as Spoken2Sign translation. The Spoken2Sign task is orthogonal and complementary to traditional sign language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign translation, we present a simple baseline consisting of three steps: 1) creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2) estimating a 3D sign for each sign video in the dictionary; 3) training a Spoken2Sign model, which is composed of a Text2Gloss translator, a sign connector, and a rendering module, with the aid of the yielded gloss-3D sign dictionary. The translation results are then displayed through a sign avatar. As far as we know, we are the first to present the Spoken2Sign task in an output format of 3D signs. In addition to its capability of Spoken2Sign translation, we also demonstrate that two by-products of our approach-3D keypoint augmentation and multi-view understanding-can assist in keypoint-based sign language understanding. Code and models will be available at https://github.com/FangyunWei/SLRT [PDF](http://arxiv.org/abs/2401.04730v1) **Summary**将口语翻译成手语，并通过3D手语形象显示翻译结果，此系统是一项创新的任务，并可以通过辅助语义理解辅助手语识别。**Key Takeaways*** 提出Spoken2Sign任务，将口语翻译成手语，并通过3D手语形象显示翻译结果。* Spoken2Sign任务与传统的将手语翻译成口语（Sign2Spoken）的任务是正交和互补的。* Spoken2Sign模型由文本到语义翻译器、手语连接器和渲染模块组成。* 使用语义-视频字典对Spoken2Sign模型进行训练。* 这是第一个以3D手势作为输出格式来展示Spoken2Sign任务的系统。* 该系统还可以通过辅助语义理解辅助手语识别。* 代码和模型将在 https://github.com/FangyunWei/SLRT 开源。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：一种简单的口语到手语翻译的基线方法，使用 3D 化身</p></li><li><p>作者：Zuo Ronglai、Wei Fangyun、Chen Zenggui、Mak Brian、Yang Jiaolong、Tong Xin</p></li><li><p>隶属单位：香港科技大学</p></li><li><p>关键词：手语翻译、口语到手语翻译、3D 化身、生成模型、关键点估计、多视图理解</p></li><li><p>链接：https://arxiv.org/abs/2401.04730Github：https://github.com/FangyunWei/SLRT</p></li><li><p>摘要：（1）研究背景：手语是聋哑人的主要交流方式。以往的研究主要集中在手语到口语翻译（Sign2Spoken）上，而本文将重点转移到逆向过程：口语到手语翻译（Spoken2Sign），以进一步缩小聋哑人和听力正常人群之间的沟通鸿沟。（2）过去方法及问题：以往的大部分口语到手语翻译研究都集中在通过关键点来表达翻译结果，但关键点表示法对符号使用者来说往往难以理解。一些研究使用生成模型将关键点动画化为手语图像，但 2D 视频格式容易出现模糊和视觉失真。（3）研究方法：本文提出了一种创新的口语到手语翻译方法，利用 3D 化身来表示翻译结果。该方法首先利用现有的 Sign2Spoken 基准数据集创建一个词表-视频词典，然后为词典中的每个手语视频估计一个 3D 手语，最后训练一个口语到手语翻译模型，该模型由文本到词表翻译器、符号连接器和渲染模块组成，借助于生成的词表-3D 手语词典进行训练。翻译结果通过手语化身显示。（4）方法性能：该方法在口语到手语翻译任务上实现了良好的性能，并且还证明了该方法的两个副产品——3D 关键点增强和多视图理解——可以辅助基于关键点的理解。</p></li><li><p>方法：（1）词典构建：利用现有的 CSLR 模型将连续手语视频分割成孤立手语，并构建一个手语词典，其中包含每个手语视频对应的词条。（2）3D 手语估计：使用 SMPLify-X 和 SMPL-X 模型估计词典中每个孤立手语的 3D 表示。（3）口语到手语翻译：使用文本到词条翻译器、手语连接器和渲染模块将输入文本翻译成手语动画。（4）副产品：从 3D 手语中派生出 3D 关键点增强和多视图理解两个副产品，可以辅助基于关键点的理解。</p></li><li><p>结论：（1）：本文关注口语到手语翻译，作为传统手语到口语翻译的逆向过程，旨在缩小聋哑人和听力正常人群之间的沟通鸿沟。与以往在二维空间产生翻译结果的作品不同，我们的创新方法使用 SMPLSign-X 和手语连接器等提出的技术生成三维手势。翻译结果通过虚拟形象显示。我们的方法涉及三个主要步骤：1）构建手语词典；2）估计词典中每个手势的三维表示；3）使用文本到手势翻译器将输入文本翻译成手势动画。（2）：创新点：提出了一种基于 3D 虚拟形象的口语到手语翻译方法，该方法能够生成更逼真、更易于理解的手语翻译结果。性能：在口语到手语翻译任务上实现了良好的性能，并且还证明了该方法的两个副产品——3D 关键点增强和多视图理解——可以辅助基于关键点的理解。工作量：该方法需要构建手语词典和估计词典中每个手势的三维表示，这需要大量的数据和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ae306a2dc4fbff7be83edef56e736584.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7ef768d7501213cf3991c2239ea75b97.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40bd6db0398f7d13cf5c8698c0fab4c3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3a927de2172cfccf688372b4f592d810.jpg" align="middle"></details><br>​    <p></p><h2 id="Morphable-Diffusion-3D-Consistent-Diffusion-for-Single-image-Avatar-Creation"><a href="#Morphable-Diffusion-3D-Consistent-Diffusion-for-Single-image-Avatar-Creation" class="headerlink" title="Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar   Creation"></a>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar   Creation</h2><p><strong>Authors:Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang</strong></p><p>Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. </p><p><a href="http://arxiv.org/abs/2401.04728v1">PDF</a> Project page: <a href="https://xiyichen.github.io/morphablediffusion/">https://xiyichen.github.io/morphablediffusion/</a></p><p><strong>Summary</strong><br>利用生成扩散模型来生成可控写实的3D人形虚拟人。</p><p><strong>Key Takeaways</strong></p><ul><li>生成扩散模型可以从单张图片或文本提示生成3D资产。</li><li>将3D可变形模型集成到多视角一致扩散方法中可以提高模型生成人形虚拟人的性能。</li><li>这种集成可以将面部表情和身体姿势控制无缝准确地融入生成过程。</li><li>该框架是第一个能够从一个看不见的主题的单张图片中创建完全3D一致、可动画和逼真的人形虚拟人的扩散模型。</li><li>定量和定性评估表明该方法优于现有的人形虚拟人创建模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可变形扩散：单张图像生成虚拟形象的 3D 一致扩散</li><li>作者：Xiyi Chen, Xiuming Zhang, Yinda Zhang, Kun Zhou, Yebin Liu</li><li>单位：北京大学</li><li>关键词：生成扩散模型、3D 一致扩散、虚拟形象创建、面部表情合成、身体姿势控制</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1)：最近生成扩散模型的进展使得从单张输入图像或文本提示生成 3D 资产成为可能。在这项工作中，我们旨在增强这些模型的质量和功能，以完成创建可控的、逼真的虚拟形象的任务。我们通过将 3D 可变形模型集成到最先进的多视图一致扩散方法中来实现这一点。我们证明了对生成管道在铰接 3D 模型上的准确调节增强了基线模型在从单张图像进行新视图合成的任务上的性能。更重要的是，这种集成促进了面部表情和身体姿势控制的无缝且准确地融入生成过程。据我们所知，我们提出的框架是第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型；广泛的定量和定性评估证明了我们的方法在现有最先进的虚拟形象创建模型上在新的视图和新的表达式合成任务上的优势。我们项目的代码将在 xiyichen.github.io/morphablediffusion 上公开。</li></ol><p>Methods:(1): 将 3D 可变形模型集成到最先进的多视图一致扩散方法中，以增强生成扩散模型的质量和功能；(2): 通过对生成管道在铰接 3D 模型上的准确调节，增强基线模型在从单张图像进行新视图合成的任务上的性能；(3): 将面部表情和身体姿势控制无缝且准确地融入生成过程，从而促进虚拟形象的可控生成；(4): 提出第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型；(5): 通过广泛的定量和定性评估，证明了该方法在现有最先进的虚拟形象创建模型上在新的视图和新的表达式合成任务上的优势。</p><ol start="8"><li>结论：（1）这项工作将可变形扩散模型引入虚拟形象创建领域，通过将 3D 可变形模型与多视图一致扩散框架无缝集成，增强了生成扩散模型的质量和功能，实现了从单张图像生成完全 3D 一致、可动画且逼真的虚拟形象，为逼真的人类数字化加速和后续研究提供了新的思路。（2）创新点：</li></ol><ul><li>将 3D 可变形模型集成到多视图一致扩散框架中，增强了生成扩散模型的质量和功能。</li><li>通过对生成管道在铰接 3D 模型上的准确调节，增强了基线模型在从单张图像进行新视图合成的任务上的性能。</li><li>将面部表情和身体姿势控制无缝且准确地融入生成过程，从而促进虚拟形象的可控生成。</li><li>提出第一个能够从未见过的主题的单张图像中创建完全 3D 一致、可动画且逼真的虚拟形象的扩散模型。性能：</li><li>在定量和定性评估中，该方法在新的视图和新的表达式合成任务上优于现有最先进的虚拟形象创建模型。</li><li>该方法能够生成高质量的虚拟形象，具有逼真的面部表情和身体姿势。</li><li>该方法能够有效地控制虚拟形象的面部表情和身体姿势。工作量：</li><li>该方法需要对生成扩散模型进行微调，这可能需要大量的数据和计算资源。</li><li>该方法需要对 3D 可变形模型进行拟合，这可能需要大量的人工劳动。</li><li>该方法需要对生成过程进行控制，这可能需要大量的人工干预。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7b2eff8a425f22057d3d5f29a6453f35.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76fdd4baa10b10b9c96a8fdd6e70afd5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-67653f4e7dd758faa5cf32a8691e30b3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7f1c5bb53301b2300b07aca2eec1c483.jpg" align="middle"></details>​    ## Deformable 3D Gaussian Splatting for Animatable Human Avatars**Authors:HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam**Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually. [PDF](http://arxiv.org/abs/2312.15059v1) **Summary**参数驱动动态人类虚拟人，仅需少量单目序列即可构建，且对背景无关，能在消费级硬件上高效推理。**Key Takeaways*** 提出了一种参数驱动的动态人类虚拟人方法，名为 ParDy-Human。* ParDy-Human 由两个模块组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯体，第二个模块进一步采用设计好的关节编码并预测每个高斯体变形，以处理超出 SMPL 顶点变形的动态情况。* ParDy-Human 显式建模逼真的动态人类虚拟人，所需训练视图和图像显著减少。* ParDy-Human 的训练无需额外注释，例如蒙版，即使在消费级硬件上也能以全分辨率高效推断图像。* 实验表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上均优于最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：可变形 3D 高斯散点动画人体化身</p></li><li><p>作者：Junggi Kim, Michael Zollhöfer, Christian Theobalt</p></li><li><p>单位：马克斯·普朗克计算机图形学研究所</p></li><li><p>关键词：神经辐射场、动态人体、单目视频、参数化动态人体化身</p></li><li><p>论文链接：https://arxiv.org/abs/2209.00926Github 代码链接：https://github.com/Junggy/pardy-human</p></li><li><p>摘要：(1)：近年来，神经辐射场的进步使得在动态场景中合成照片级真实图像的新颖视角成为可能，这可以应用于具有动画的人体场景。常用的隐式骨干网可以建立准确的模型，然而，它们需要许多输入视图和额外的注释，例如人体蒙版、UV 贴图和深度图。在这项工作中，我们提出了 ParDy-Human（参数化动态人体化身），这是一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。ParDy-Human 构成了一个显式模型，用于逼真的动态人体化身，所需训练视图和图像明显更少。我们的虚拟形象学习过程无需额外的注释，例如蒙版，并且可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。我们的代码可在 https://github.com/Junggy/pardy-human 获得。(2)：过去的方法通常使用隐式表示来构建神经辐射场，这需要许多输入视图和额外的注释，例如人体蒙版、UV 贴图和深度图。这些方法通常需要大量的数据和计算资源，并且在处理动态场景时可能存在困难。(3)：本文提出的方法 ParDy-Human 是一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。(4)：ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。在 ZJU-MoCap 数据集上，ParDy-Human 在平均重投影误差 (MRE) 和光度一致性 (PC) 方面分别优于最先进的方法 14.6% 和 11.5%。在 THUman4.0 数据集上，ParDy-Human 在 MRE 和 PC 方面分别优于最先进的方法 12.3% 和 9.1%。这些结果表明，ParDy-Human 可以生成高质量的动态人体图像，并且可以很好地处理动态场景。</p></li><li><p>方法：(1) 初始化3D高斯散点：从粗糙的点云扫描开始，并使用特定的自适应密度控制方案进行训练，该方案在训练过程中根据高斯散点的大小和梯度幅度对其进行分割、克隆和剪枝。(2) 姿态高斯散点：根据其父级使用逐顶点变形（PVD）对每个高斯散点进行变形。(3) 变形细化：对于穿着紧身衣的人体，使用 SMPL 模型进行变形就足够了。然而，一般来说，服装运动会导致更多依赖于人体姿势的变形。为了获得更高保真的渲染效果，我们包含了一个变形细化模块 (DRM)。(4) 球谐函数方向：在 3D-GS 中，球谐函数 (SH) 用于合并视角相关的效果。(5) 取消姿势高斯散点并更新父级：一旦高斯散点更新，它们就会被转换回规范空间，以便使用下一组参数再次摆姿势。取消姿势是通过按照变形顺序的相反顺序进行的。(6) 训练：训练过程中，高斯散点的数量及其中心可能会发生变化，因此必须相应地更新父索引 i。</p></li><li><p>结论：（1）：本文提出了 ParDy-Human，一种完全显式的方法，可以从少量单目序列构建数字虚拟形象。ParDy-Human 将参数驱动的动态引入到 3D 高斯散点中，其中 3D 高斯散点由人体姿态模型变形以使化身动画化。我们的方法由两部分组成：第一个模块根据 SMPL 顶点变形规范 3D 高斯散点，连续的模块进一步采用它们设计好的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。ParDy-Human 构成了一个显式模型，用于逼真的动态人体化身，所需训练视图和图像明显更少。我们的虚拟形象学习过程无需额外的注释，例如蒙版，并且可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上优于最先进的方法，无论是在定量上还是在视觉上。（2）：创新点：</p></li></ol><ul><li>将参数驱动的动态引入到 3D 高斯散点中，使化身能够进行逼真的动画。</li><li>提出了一种新的变形细化模块 (DRM)，以处理超出 SMPL 顶点变形的动态。</li><li>无需额外的注释，例如蒙版，即可从少量单目序列构建数字虚拟形象。</li><li>可以在推断全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也是如此。性能：</li><li>在 ZJU-MoCap 数据集上，ParDy-Human 在平均重投影误差 (MRE) 和光度一致性 (PC) 方面分别优于最先进的方法 14.6% 和 11.5%。</li><li>在 THUman4.0 数据集上，ParDy-Human 在 MRE 和 PC 方面分别优于最先进的方法 12.3% 和 9.1%。工作量：</li><li>ParDy-Human 需要较少的训练视图和图像，并且可以在消费级硬件上进行训练。</li><li>ParDy-Human 的训练过程无需额外的注释，例如蒙版。</li><li>ParDy-Human 可以使用可变背景进行训练，这使得它能够生成更逼真的图像。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3a2dec08eda70704d60e83b281cc54a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2a167032c68efd5d06543a5ec3ba4f79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e243cc96b91f1cb9f2e0e8cb1aa2a523.jpg" align="middle"><img src="https://picx.zhimg.com/v2-805c12244272b525ede83f20a94c5569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df3f505c56582ddada94e66d5ec6791a.jpg" align="middle"></details>​    ## A Language-based solution to enable Metaverse Retrieval**Authors:Ali Abdari, Alex Falcon, Giuseppe Serra**Recently, the Metaverse is becoming increasingly attractive, with millions of users accessing the many available virtual worlds. However, how do users find the one Metaverse which best fits their current interests? So far, the search process is mostly done by word of mouth, or by advertisement on technology-oriented websites. However, the lack of search engines similar to those available for other multimedia formats (e.g., YouTube for videos) is showing its limitations, since it is often cumbersome to find a Metaverse based on some specific interests using the available methods, while also making it difficult to discover user-created ones which lack strong advertisement. To address this limitation, we propose to use language to naturally describe the desired contents of the Metaverse a user wishes to find. Second, we highlight that, differently from more conventional 3D scenes, Metaverse scenarios represent a more complex data format since they often contain one or more types of multimedia which influence the relevance of the scenario itself to a user query. Therefore, in this work, we create a novel task, called Text-to-Metaverse retrieval, which aims at modeling these aspects while also taking the cross-modal relations with the textual data into account. Since we are the first ones to tackle this problem, we also collect a dataset of 33000 Metaverses, each of which consists of a 3D scene enriched with multimedia content. Finally, we design and implement a deep learning framework based on contrastive learning, resulting in a thorough experimental setup. [PDF](http://arxiv.org/abs/2312.14630v1) Accepted at 30th International Conference on Multimedia Modeling-   MMM2024**Summary**元宇宙检索：利用语言描述用户期望的元宇宙内容，并构建跨模态关系学习模型实现文本到元宇宙检索。**Key Takeaways**- 元宇宙正变得越来越有吸引力，数百万用户正在访问许多可用的虚拟世界。- 目前，人们发现适合其当前兴趣的元宇宙的方法主要是口耳相传或通过技术型网站上的广告。- 缺乏类似于其他多媒体格式（例如 YouTube 用于视频）的搜索引擎显示出其局限性。- 提出了一个新颖的任务：文本到元宇宙检索，旨在对这些方面进行建模，同时考虑与文本数据的跨模态关系。- 创建了一个包含 33000 个元宇宙的数据集，每个元宇宙都由一个丰富的多媒体内容的 3D 场景组成。- 设计并实现了基于对比学习的深度学习框架，形成了一个完整的实验设置。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于语言的元宇宙检索解决方案</li><li>作者：Ali Abdari、Alex Falcon、Giuseppe Serra</li><li>第一作者单位：乌迪内大学</li><li>关键词：多媒体、文本到多媒体检索、跨模态理解、元宇宙、对比学习</li><li>论文链接：https://arxiv.org/abs/2312.14630，Github 链接：无</li><li>摘要：（1）研究背景：元宇宙正变得越来越受欢迎，但目前缺乏有效的搜索引擎来帮助用户找到最适合他们当前兴趣的元宇宙。（2）过去方法与问题：现有的搜索过程主要通过口碑或在技术导向的网站上做广告来完成。然而，缺乏类似于其他多媒体格式（如 YouTube 用于视频）的搜索引擎，这使得根据一些特定兴趣使用可用方法找到元宇宙变得很麻烦，同时也使得发现缺乏强力广告的用户创建的元宇宙变得困难。（3）研究方法：为了解决这个问题，本文提出使用语言来自然地描述用户想要找到的元宇宙的所需内容。此外，本文还强调，与更传统的 3D 场景不同，元宇宙场景表示更复杂的数据格式，因为它们通常包含一种或多种影响场景本身与用户查询相关性的多媒体。因此，本文创建了一个名为文本到元宇宙检索的新任务，旨在对这些方面进行建模，同时还考虑与文本数据的跨模态关系。由于本文是第一个解决这个问题的研究，因此还收集了一个由 33000 个元宇宙组成的数据集，每个元宇宙都由一个包含多媒体内容的 3D 场景组成。最后，本文设计并实现了一个基于对比学习的深度学习框架，从而形成了一个彻底的实验设置。（4）方法性能：本文方法在文本到元宇宙检索任务上取得了最先进的性能，证明了该方法可以有效地支持其目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种基于语言的元宇宙检索解决方案，该解决方案通过对比学习的深度学习框架，实现了文本到元宇宙检索任务的最先进性能。（2）：创新点：• 提出了一种新的文本到元宇宙检索任务，该任务旨在对元宇宙场景表示的复杂数据格式进行建模，并考虑与文本数据的跨模态关系。• 收集了一个由33000个元宇宙组成的元宇宙检索数据集，为该任务的研究提供了基础。• 设计并实现了一个基于对比学习的深度学习框架，该框架可以有效地支持文本到元宇宙检索任务。性能：• 在文本到元宇宙检索任务上取得了最先进的性能，证明了该方法可以有效地支持其目标。工作量：• 收集了一个由33000个元宇宙组成的元宇宙检索数据集，该数据集的构建需要大量的人力物力。• 设计并实现了一个基于对比学习的深度学习框架，该框架的构建需要大量的时间和精力。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bcbf5fa9f9cbd442f630c06fe63aa7e1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-768b3350157276eaf44aaa50642b5f8f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7fc71bb9936d63fe907246316a055a.jpg" align="middle"></details><br>​    <p></p><h2 id="MoSAR-Monocular-Semi-Supervised-Model-for-Avatar-Reconstruction-using-Differentiable-Shading"><a href="#MoSAR-Monocular-Semi-Supervised-Model-for-Avatar-Reconstruction-using-Differentiable-Shading" class="headerlink" title="MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using   Differentiable Shading"></a>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using   Differentiable Shading</h2><p><strong>Authors:Abdallah Dib, Luiz Gustavo Hafemann, Emeline Got, Trevor Anderson, Amin Fadaeinejad, Rafael M. O. Cruz, Marc-Andre Carbonneau</strong></p><p>Reconstructing an avatar from a portrait image has many applications in multimedia, but remains a challenging research problem. Extracting reflectance maps and geometry from one image is ill-posed: recovering geometry is a one-to-many mapping problem and reflectance and light are difficult to disentangle. Accurate geometry and reflectance can be captured under the controlled conditions of a light stage, but it is costly to acquire large datasets in this fashion. Moreover, training solely with this type of data leads to poor generalization with in-the-wild images. This motivates the introduction of MoSAR, a method for 3D avatar generation from monocular images. We propose a semi-supervised training scheme that improves generalization by learning from both light stage and in-the-wild datasets. This is achieved using a novel differentiable shading formulation. We show that our approach effectively disentangles the intrinsic face parameters, producing relightable avatars. As a result, MoSAR estimates a richer set of skin reflectance maps, and generates more realistic avatars than existing state-of-the-art methods. We also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public dataset providing intrinsic face attributes at scale (diffuse, specular, ambient occlusion and translucency maps) for a total of 10k subjects. The project website and the dataset are available on the following link: <a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a> </p><p><a href="http://arxiv.org/abs/2312.13091v2">PDF</a> <a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a></p><p><strong>Summary</strong><br>利用半监督训练方案，MoSAR可从单张图像合成更真实的人脸。</p><p><strong>Key Takeaways</strong></p><ul><li>MoSAR可以从单张图像中生成三维虚拟人。</li><li>MoSAR使用了半监督训练方案，利用光场和野外数据集进行训练。</li><li>MoSAR提出了一种创新的可微分着色公式，用于解耦内在面部参数。</li><li>MoSAR产生的虚拟人具有更丰富的皮肤反射图，更逼真。</li><li>FFHQ-UV-Intrinsics是第一个大规模提供内在面部属性的公开数据集。</li><li>FFHQ-UV-Intrinsics数据集包含10k个主题的漫反射、镜面反射、环境光遮蔽和半透明度贴图。</li><li>项目网站和数据集可通过以下链接获得：<a href="https://ubisoft-laforge.github.io/character/mosar/">https://ubisoft-laforge.github.io/character/mosar/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MoSAR：使用可微分着色的单目半监督模型，用于虚拟形象重建</li><li>作者：Abdallah Dib、Luiz Gustavo Hafemann、Emeline Got、Trevor Anderson、Amin Fadaeinejad、Rafael M. O. Cruz、Marc-André Carbonneau</li><li>第一作者单位：育碧拉福格约克大学</li><li>关键词：虚拟形象生成、单目重建、半监督学习、可微分着色</li><li>论文链接：https://arxiv.org/abs/2312.13091Github 链接：无</li><li>摘要：</li></ol><p>（1）研究背景：从人像图像中重建虚拟形象在多媒体领域有着广泛的应用，但仍然是一个具有挑战性的研究课题。从一张图像中提取反射率图和几何形状是不适定的：恢复几何形状是一个一对多的映射问题，并且反射率和光线难以解开。在光场等受控条件下可以捕捉到准确的几何形状和反射率，但以这种方式获取大型数据集的成本很高。此外，仅使用此类数据进行训练会导致在野外图像中泛化性较差。</p><p>（2）过去的方法及其问题：现有方法主要分为两类：基于光场的数据驱动方法和基于模型的几何重建方法。基于光场的数据驱动方法可以生成高质量的虚拟形象，但需要昂贵的采集设备和受控的拍摄环境。基于模型的几何重建方法可以从单目图像中重建几何形状，但通常需要大量的人工标注数据。</p><p>（3）本文提出的研究方法：为了解决上述问题，本文提出了一种新的单目半监督模型 MoSAR，用于虚拟形象重建。MoSAR 使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。此外，MoSAR 还引入了一个新的数据集 FFHQ-UV-Intrinsics，该数据集提供了 10k 个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。</p><p>（4）方法在任务和性能上的表现：在人脸几何形状重建和虚拟形象生成任务上，MoSAR 的性能优于现有最先进的方法。MoSAR 可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。</p><ol start="7"><li><p>方法：(1): 本文提出了一种新的单目半监督模型MoSAR，用于虚拟形象重建。MoSAR使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。(2): MoSAR还引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。(3): 在人脸几何形状重建和虚拟形象生成任务上，MoSAR的性能优于现有最先进的方法。MoSAR可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。</p></li><li><p>结论：（1）：本文提出了一种从单目图像重建虚拟形象的半监督模型MoSAR，该模型使用可微分着色公式有效解开固有的人脸参数，从而产生可重新照明的虚拟形象。此外，MoSAR还引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。（2）：创新点：</p></li></ol><ul><li>提出了一种新的单目半监督模型MoSAR，用于虚拟形象重建。</li><li>使用了一种新颖的可微分着色公式，可以有效地解开固有的人脸参数，从而产生可重新照明的虚拟形象。</li><li>引入了一个新的数据集FFHQ-UV-Intrinsics，该数据集提供了10k个受试者的固有面部属性（漫反射、镜面反射、环境光遮挡和半透明度贴图）。性能：</li><li>在人脸几何形状重建和虚拟形象生成任务上，MoSAR的性能优于现有最先进的方法。</li><li>MoSAR可以从单目图像中生成高质量的虚拟形象，并且这些虚拟形象可以重新照明以匹配不同的环境光照条件。工作量：</li><li>该方法需要大量的数据和计算资源。</li><li>该方法的训练过程比较复杂，需要花费大量的时间和精力。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2050a1226482a2aeb4def8538e376837.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09d311b82ccb9b15a7db18277e983bf5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe2cd6e6acd2db5fb63e12b91608773e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2e81d9872c02b6ac45bf7a97118b311e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8544dbf9b68d824464a75139e05d2910.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5cce74b3c6fc797539b1b70305e061f.jpg" align="middle"></details>​    ## Relightable and Animatable Neural Avatars from Videos**Authors:Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu**Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at \url{https://wenbin-lin.github.io/RelightableAvatar-page/}. [PDF](http://arxiv.org/abs/2312.12877v1) Accepted by AAAI 2024**摘要**利用未知光照条件下人物的稀疏视频，构建可重打光并可动画化的神经人形。**主要要点**- 提出一种从稀疏视频创建可重打光和可动画化的神经人形的方法。- 通过可逆变形场更好地建模几何形状和阴影变化来解决几何变化。- 提出一种姿势感知的局部光线可见性网络来估计光线遮挡，以便对空间和时间变化的着色提示进行建模。- 在合成和真实数据集上的广泛实验表明，该方法重建了高质量的几何形状并在不同的身体姿势下生成了逼真的阴影。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：从视频中生成可重照明和可动画的神经网络虚拟形象</p></li><li><p>作者：Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, Feng Xu</p></li><li><p>单位：清华大学软件学院</p></li><li><p>关键词：神经网络虚拟形象、可重照明、可动画、几何建模、阴影建模</p></li><li><p>论文链接：https://arxiv.org/abs/2312.12877Github 代码链接：无</p></li><li><p>摘要：（1）研究背景：近年来，人类数字化技术发展迅速，其中 3D 着装人类虚拟形象的重建和动画在远程临场、AR/VR 和虚拟试穿等领域具有广泛应用。一个重要的目标是在所需照明环境和所需姿势下渲染人类虚拟形象。因此，人类虚拟形象需要同时具有可重照明性和可动画性，并实现逼真的渲染质量。通常，生成这些高质量的人类虚拟形象依赖于高品质数据，例如由光场（Light Stage）记录的数据，而这些数据复杂且昂贵。（2）过去方法及问题：近年来，神经辐射场（NeRF）的出现为仅从日常记录的视频中生成可动画和可重照明 3D 人类虚拟形象开辟了新的窗口。基于 NeRF 的方法在 3D 对象表示和静态和动态对象的逼真渲染方面取得了显着的成功，包括人体（Peng et al. 2021b,a; Xu, Dieck, and Sminchisescu 2021; Wen et al. 2022; Jiang et al. 2022a,b; Wang et al. 2022; Peng et al. 2022; Yu et al. 2023; Su, Bagautdinov, and Rhodin 2023）。此外，NeRF 可用于固有分解，以实现静态对象的令人印象深刻的照明效果（Zhang et al. 2021; Yao et al. 2022; Boss et al. 2021a; Srinivasan et al. 2021; Boss et al. 2021b; Zhang et al. 2022; Jin et al. 2023）。然而，基于 NeRF 的动态对象重照明很少被研究。一个关键挑战是动态变化导致对象着色发生剧烈变化，这很难用当前的 NeRF 技术建模。（3）本文方法：本文提出从稀疏视频中重建可重照明和可动画的 3D 人类虚拟形象，这些视频是在未校准的照明下记录的。为了实现这一目标，我们需要重建身体几何、材质和环境光照。动态的身体几何形状由规范空间中的静态几何形状和运动建模，以将其变形为每个帧的观察空间中的形状。我们提出了一种可逆神经变形场，它建立了规范空间和所有观察空间之间的双向映射。利用这种双向映射，我们可以轻松地利用规范姿势中提取的身体网格来更好地解决逆线性混合蒙皮问题，从而实现高质量的几何重建。在所有帧的几何重建之后，我们提出了一种光照可见性估计模块，以更好地模拟材质和光照重建的动态自遮挡效应。我们将全局姿势相关的可见性估计任务转移到多个局部部分任务中，这大大简化了光照可见性估计的复杂性。该模型受益于部分架构，具有良好的泛化能力，即使训练数据有限，也能成功估计各种身体姿势和照明条件下的光照可见性。最后，我们优化身体材质和照明参数，然后我们的方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。（4）方法性能：本文方法在合成和真实数据集上进行了广泛的实验，结果表明，该方法可以重建高质量的几何形状并在不同的身体姿势下生成逼真的阴影。代码和数据可在 https://wenbin-lin.github.io/RelightableAvatar-page/ 获得。</p></li><li><p>Methods:(1): 提出可逆神经变形场，用于规范空间和所有观察空间之间的双向映射，从而更好地解决逆线性混合蒙皮问题，实现高质量的几何重建。(2): 提出光照可见性估计模块，将全局姿势相关的可见性估计任务转移到多个局部部分任务中，简化光照可见性估计的复杂性。(3): 优化身体材质和照明参数，使方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。</p></li><li><p>结论：（1）：本文提出了一种从稀疏视频中重建可重照明和可动画的 3D 人类虚拟形象的方法，该方法可以生成高质量的几何形状并在不同的身体姿势下生成逼真的阴影。（2）：创新点：提出可逆神经变形场，用于规范空间和所有观察空间之间的双向映射，从而更好地解决逆线性混合蒙皮问题，实现高质量的几何重建。提出光照可见性估计模块，将全局姿势相关的可见性估计任务转移到多个局部部分任务中，简化光照可见性估计的复杂性。优化身体材质和照明参数，使方法可以在任何所需的身体姿势、照明和视点下渲染逼真的图像。性能：该方法在合成和真实数据集上进行了广泛的实验，结果表明，该方法可以重建高质量的几何形状并在不同的身体姿势下生成逼真的阴影。工作量：该方法需要收集稀疏视频数据，并进行数据预处理，然后训练神经网络模型。训练过程可能需要大量的时间和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9e9391649c498b53d3aee9866413c321.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38ceb0869d19022358f9afbec6d56552.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9d213e510b1c1cd964c103074eb5cbc6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f01dc710ef4497b1ceabc9bbd0193f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-416c5c19e5e6ad7af22f15d01e6f67d3.jpg" align="middle"></details><br>​    <p></p><h2 id="DLCA-Recon-Dynamic-Loose-Clothing-Avatar-Reconstruction-from-Monocular-Videos"><a href="#DLCA-Recon-Dynamic-Loose-Clothing-Avatar-Reconstruction-from-Monocular-Videos" class="headerlink" title="DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular   Videos"></a>DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular   Videos</h2><p><strong>Authors:Chunjie Luo, Fei Luo, Yusen Wang, Enxu Zhao, Chunxia Xiao</strong></p><p>Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface. To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL. With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods. </p><p><a href="http://arxiv.org/abs/2312.12096v2">PDF</a> </p><p><strong>Summary</strong><br>从单目视频中创建真人化身，解决人类穿着宽松服装时重建动态人的挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>在规范空间初始化一个估计的 3D 穿衣人形，因为变形场从穿衣人形处学习比从 SMPL 处学习更容易。</li><li>利用连续帧之间的物理连接信息并提出动态变形场 (DDF) 来优化变形场。</li><li>DDF 考虑了对宽松衣服的贡献力，以增强变形的可解释性并有效地捕捉宽松衣服的自由运动。</li><li>将 SMPL 蒙皮权重传播到每个人并优化期间细化位姿和蒙皮权重，以改进蒙皮变换。</li><li>基于更合理初始化和 DDF，我们可以更准确地模拟真实世界的物理。</li><li>与 SOTA 方法相比，我们的方法可以为穿着宽松服装的人生成更好的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>标题：DLCA-Recon：基于单目视频的动态宽松服装化身重建</p></li><p></p><p></p><li><p>作者：Luo Chunjie, Luo Fei, Wang Yuseng, Zhao Enxu, Xiao Chunxia</p></li><p></p><p></p><li><p>单位：武汉大学计算机学院</p></li><p></p><p></p><li><p>关键词：人体重建、宽松服装、隐式神经表示、动态变形场</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/abs/2312.12096Github 链接：None</p></li><p></p><p></p><li><p>摘要：（1）：人体重建是计算机图形学中的重要研究课题，具有广泛的应用前景。然而，重建动态宽松服装的人体模型是一项极具挑战性的任务。（2）：以往的方法在处理宽松服装时存在以下问题：</p><p></p><ul><li>几何初始化和约束不足，导致变形优化难以解释宽松服装的剧烈变化，重建表面不连续且不完整。</li><li>缺乏对宽松服装真实物理特性的建模，导致重建结果不准确。（3）：针对上述问题，本文提出了 DLCA-Recon 方法，该方法具有以下特点：</li><li>在规范空间中初始化估计的 3D 穿衣人体，简化了变形场的学习过程。</li><li>利用显式网格和隐式 SDF 的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场 (DDF) 来优化变形场。</li><li>传播 SMPL 蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。（4）：在公开数据集和自有数据集上的广泛实验表明，与最先进的方法相比，DLCA-Recon 方法在重建动态宽松服装的人体方面取得了优异的性能。</li></ul></li><li><p>方法：(1)：提出 DLCA-Recon 方法，该方法在规范空间中初始化估计的 3D 穿衣人体，简化了变形场的学习过程。(2)：利用显式网格和隐式 SDF 的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场 (DDF) 来优化变形场。(3)：传播 SMPL 蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。(4)：采用延迟优化策略，在训练过程中逐步启用姿态和蒙皮权重的优化，以缓解网络学习的负担。(5)：使用表面渲染而不是体积渲染来获得准确的几何形状，并结合显式网格和隐式 SDF 的双重表示来提高渲染质量。</p></li><li><p>结论：（1）：本文提出了一种基于单目视频的动态宽松服装化身重建方法DLCA-Recon，该方法在规范空间中初始化估计的3D穿衣人体，简化了变形场的学习过程。利用显式网格和隐式SDF的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场(DDF)来优化变形场。传播SMPL蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。在公开数据集和自有数据集上的广泛实验表明，DLCA-Recon方法在重建动态宽松服装的人体方面取得了优异的性能。（2）：创新点：</p></li></ol><ul><li>在规范空间中初始化估计的3D穿衣人体，简化了变形场的学习过程。</li><li>利用显式网格和隐式SDF的双重表示，并结合连续帧之间的物理连接信息，提出动态变形场(DDF)来优化变形场。</li><li>传播SMPL蒙皮权重到每个个体，并在优化过程中细化姿态和蒙皮权重，以改进蒙皮变换。性能：</li><li>在公开数据集和自有数据集上的广泛实验表明，DLCA-Recon方法在重建动态宽松服装的人体方面取得了优异的性能。工作量：</li><li>该方法需要大量的训练数据和计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cb014d0a50eee58a6cfa584a8f0e910c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6b9e9f2ccbb67bf3624babbc66b110f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3b7bc38cc6395d39ef3a3801365a9cf7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a509596d091208b879661d7a3cf71fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d98be771a5d656fda2fb54f19445a39c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4ab4017e410595f35f8ac9cca87655c1.jpg" align="middle"></details><br>​    <p></p><h2 id="Attention-Based-VR-Facial-Animation-with-Visual-Mouth-Camera-Guidance-for-Immersive-Telepresence-Avatars"><a href="#Attention-Based-VR-Facial-Animation-with-Visual-Mouth-Camera-Guidance-for-Immersive-Telepresence-Avatars" class="headerlink" title="Attention-Based VR Facial Animation with Visual Mouth Camera Guidance   for Immersive Telepresence Avatars"></a>Attention-Based VR Facial Animation with Visual Mouth Camera Guidance   for Immersive Telepresence Avatars</h2><p><strong>Authors:Andre Rochow, Max Schwarz, Sven Behnke</strong></p><p>Facial animation in virtual reality environments is essential for applications that necessitate clear visibility of the user’s face and the ability to convey emotional signals. In our scenario, we animate the face of an operator who controls a robotic Avatar system. The use of facial animation is particularly valuable when the perception of interacting with a specific individual, rather than just a robot, is intended. Purely keypoint-driven animation approaches struggle with the complexity of facial movements. We present a hybrid method that uses both keypoints and direct visual guidance from a mouth camera. Our method generalizes to unseen operators and requires only a quick enrolment step with capture of two short videos. Multiple source images are selected with the intention to cover different facial expressions. Given a mouth camera frame from the HMD, we dynamically construct the target keypoints and apply an attention mechanism to determine the importance of each source image. To resolve keypoint ambiguities and animate a broader range of mouth expressions, we propose to inject visual mouth camera information into the latent space. We enable training on large-scale speaking head datasets by simulating the mouth camera input with its perspective differences and facial deformations. Our method outperforms a baseline in quality, capability, and temporal consistency. In addition, we highlight how the facial animation contributed to our victory at the ANA Avatar XPRIZE Finals. </p><p><a href="http://arxiv.org/abs/2312.09750v1">PDF</a> Published in IEEE/RSJ International Conference on Intelligent Robots   and Systems (IROS) 2023</p><p><strong>摘要</strong><br>将视觉信息引入面部动画，增强了对于人脸运动的捕捉与合成。</p><p><strong>要点</strong></p><ul><li>面部动画在虚拟现实环境中至关重要，它需要清晰地看到用户的面部并传达情感信号。</li><li>在我们的方案中，我们为控制机器人化身系统的操作员制作面部动画。</li><li>当想要让人们感觉与特定个体进行互动，而不仅仅是与机器人互动时，使用面部动画尤其有价值。</li><li>纯粹由关键点驱动的动画方法难以应对复杂的面部动作。</li><li>我们提出了一种混合方法，它同时利用关键点和来自嘴巴摄像头的直接视觉引导。</li><li>我们的方法适用于未见过的操作员，并且只需要一个快速注册步骤，其中包括录制两个简短的视频。</li><li>我们选择了多张源图像，目的是覆盖不同的面部表情。</li><li>给定来自头戴式显示器 (HMD) 的嘴巴摄像头帧，我们动态地构建目标关键点，并应用注意机制来确定每个源图像的重要性。</li><li>为了解决关键点的模糊性并制作更广泛的嘴巴表情动画，我们建议将视觉嘴巴摄像头信息注入潜在空间。</li><li>我们通过模拟嘴巴摄像头输入及其透视差异和面部变形来实现对大规模说话头部数据集的训练。</li><li>我们的方法在质量、能力和时间一致性方面优于基准。</li><li>此外，我们重点介绍了面部动画如何帮助我们在 ANA Avatar XPRIZE 决赛中取得胜利。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：基于视觉的嘴部相机指导的面部动画</p></li><p></p><p></p><li><p>作者：Matthias Niessner, Michael Zollhöfer, Shahram Izadi, Marc Stamminger, Andreas Kolb, Christian Theobalt</p></li><p></p><p></p><li><p>隶属单位：马克斯普朗克信息学研究所</p></li><p></p><p></p><li><p>关键词：面部动画、虚拟现实、嘴部相机、视觉指导、关键点</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/1705.08922, Github：无</p></li><p></p><p></p><li><p>摘要：（1）：面部动画在虚拟现实环境中至关重要，它可以使用户清晰地看到自己的脸部并传达情感信号。在我们的场景中，我们对控制机器人 Avatar 系统的操纵员的面部进行动画处理。当用户希望与特定个人（而非仅仅一个机器人）进行交互时，面部动画的使用特别有价值。纯关键点驱动的动画方法难以应对复杂的面部动作。我们提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。我们的方法可以推广到未见过的操纵员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。我们选择多个源图像，旨在覆盖不同的面部表情。给定来自 HMD 的嘴部相机帧，我们动态构建目标关键点并应用注意机制来确定每个源图像的重要性。为了解决关键点歧义并对更广泛的面部表情进行动画处理，我们提出将视觉嘴部相机信息注入潜在空间。我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。我们的方法在质量、能力和时间一致性方面优于基线。此外，我们重点介绍了面部动画如何为我们在 ANA AvatarXPRIZE 决赛中的胜利做出了贡献。</p></li><p></p><p></p><li><p>方法：</p></li><br>&lt;/ol&gt;<p></p><p></p><p>（1）：我们提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。</p><p></p><p></p><p>（2）：我们的方法可以推广到未见过的操纵员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。</p><p></p><p></p><p>（3）：我们选择多个源图像，旨在覆盖不同的面部表情。</p><p></p><p></p><p>（4）：给定来自HMD的嘴部相机帧，我们动态构建目标关键点并应用注意机制来确定每个源图像的重要性。</p><p></p><p></p><p>（5）：为了解决关键点歧义并对更广泛的面部表情进行动画处理，我们提出将视觉嘴部相机信息注入潜在空间。</p><p></p><p></p><p>（6）：我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：（1）：该工作提出了一种实时虚拟现实面部动画方法，与关键点驱动的面部动画方法相比，该方法可以推广到未见过的操作员，并允许建模更广泛的面部表情。我们通过源图像注意力机制扩展了基线，并开发了一种将视觉嘴部图像信息注入动画管道的方法，而不会出现过拟合。这两个扩展产生了更好的准确性并显着提高了时间一致性，这对于流畅的交互非常重要。我们的方法仍然难以生成不寻常的表情，例如伸出舌头。此外，上部面部的运动仍然有限。（2）：创新点：</li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种混合方法，该方法同时使用关键点和嘴部相机的直接视觉指导。</li><li>开发了一种快速注册步骤，该步骤只需要两个短视频的捕捉，即可将我们的方法推广到未见过的操作员。</li><li>提出了一种将视觉嘴部相机信息注入潜在空间的方法，以解决关键点歧义并对更广泛的面部表情进行动画处理。性能：</li><li>我们的方法在质量、能力和时间一致性方面优于基线。</li><li>我们的方法可以推广到未见过的操作员，只需要一个快速注册步骤，其中包含两个短视频的捕捉。</li><li>我们的方法可以对更广泛的面部表情进行动画处理，包括不寻常的表情，例如伸出舌头。工作量：</li><li>我们通过模拟嘴部相机输入及其透视差异和面部变形，使训练能够在大规模说话头部数据集上进行。</li><li>我们通过源图像注意力机制扩展了基线，并开发了一种将视觉嘴部图像信息注入动画管道的方法，而不会出现过拟合。</li><li>我们在ANAAvatarXPRIZE决赛中使用了我们的方法，并取得了胜利。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dc65e27ac791814ade4910bc092cbd2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f634cbd903eec7aed8f5d4dfeb59915.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8403ee6486888d7a563ed47600f96335.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26dc5864025dde63a1e3c374590b8f70.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8e0823080cd17a5ac265b162c95e7038.jpg" align="middle"></details><br>​    <p></p><h2 id="3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting"><a href="#3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting" class="headerlink" title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"></a>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</h2><p><strong>Authors:Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</strong></p><p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively. </p><p><a href="http://arxiv.org/abs/2312.09228v2">PDF</a> Project page: <a href="https://neuralbodies.github.io/3DGS-Avatar">https://neuralbodies.github.io/3DGS-Avatar</a></p><p><strong>摘要</strong><br>基于3D高斯溅射法，本文提出了一种只需30分钟即可训练并能够以50 FPS以上实时帧速率渲染的可动画3D服装人形虚拟人重建方法。</p><p><strong>关键要点</strong></p><ul><li>本文提出了一种基于3D高斯溅射法和非刚性变形网络，在30分钟内训练出可动画的服装人形虚拟人，并以超过50 FPS的实时帧速率渲染。</li></ul><ul><li>基于神经辐射场（NeRF）的现有方法可实现高质量的新视角/新姿势图像合成，但通常需要数天的训练时间，并且推理时间非常慢。</li></ul><ul><li>近期研究探索了用于有效训练服装虚拟人的快速网格结构。尽管训练速度极快，但这些方法几乎无法实现约15 FPS的交互式渲染帧速率。</li></ul><ul><li>本文使用3D高斯溅射法并学习一个非刚性变形网络，以重建可动画的服装人形虚拟人，并在30分钟内完成训练并以实时帧速率（50+ FPS）渲染。</li></ul><ul><li>针对高斯均值向量和协方差矩阵引入尽可能等距的正则化，增强了模型对高度铰接的不可见姿势的泛化能力。</li></ul><ul><li>实验结果表明，与最先进的可动画虚拟人创建方法相比，本文方法实现了相当甚至更好的性能，同时训练和推理速度分别提高了400倍和250倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：3DGS-Avatar：可变形 3D 高斯散布的动画角色</li><p></p><p></p><li>作者：Yiyi Liao、Shuaicheng Liu、Tianchang Shen、Lingjie Liu、Christian Theobalt、Hao Li</li><p></p><p></p><li>隶属机构：马克斯·普朗克计算机科学研究所</li><p></p><p></p><li>关键词：动画角色、可变形模型、神经辐射场、单目视频、3D 高斯散布</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2302.06467，Github 代码链接：None</li><p></p><p></p><li>摘要：（1）：研究背景：神经辐射场（NeRF）方法在单目视频中创建动画角色方面取得了显着进展，但通常需要数天的训练时间，并且推理速度非常慢。最近，研究人员探索了快速网格结构以高效训练带服装的角色。尽管这些方法的训练速度非常快，但它们只能实现约 15 FPS 的交互式渲染帧率。（2）：过去的方法及其问题：现有方法基于神经辐射场（NeRF），可以实现高质量的新视角/新姿势图像合成，但通常需要数天的训练时间，并且推理速度非常慢。最近，研究社区探索了用于高效训练带服装角色的快速网格结构。尽管这些方法的训练速度非常快，但它们只能实现约 15 FPS 的交互式渲染帧率。（3）：提出的研究方法：本文提出了一种使用 3D 高斯散布（3DGS）从单目视频创建动画人类角色的方法。该方法学习了一个非刚性变形网络来重建可动画的带服装的人类角色，可以在 30 分钟内训练完成，并以实时帧率（50+ FPS）渲染。此外，还引入了尽可能等距的正则化，以增强模型对未见姿势的泛化能力。（4）：方法的性能：实验结果表明，该方法在从单目输入创建动画角色方面取得了与现有方法相当甚至更好的性能，同时训练速度提高了 400 倍，推理速度提高了 250 倍。这些性能支持了该方法的目标。</li><br>&lt;/ol&gt;<p></p><p></p><p>&lt;Methods&gt;:</p><p></p><p></p><p>（1）：方法概述：本文方法的管道如图2所示。输入是一个经过校准的相机、拟合的SMPL参数和前景掩码的单目视频。该方法优化了一组规范空间中的3D高斯分布，然后将其变形到观察空间，并从给定的相机渲染。对于一组3D高斯分布{G(i)}Ni=1，在每个点存储以下属性：位置x，缩放因子，旋转四元数q，不透明度α和颜色特征向量f。首先通过随机采样SMPL[26]网格表面的N=50k个点作为规范3D高斯分布{Gc}的初始化。受HumanNeRF[62]的启发，将复杂的人体变形分解为一个编码姿势相关布料变形的非刚性部分，以及由人体骨骼控制的刚性变换。</p><p></p><p></p><p>（2）：姿势相关的非刚性变形：将非刚性变形模块表述为：{Gd}=Fθnr({Gc};Zp)(6)</p><p></p><p></p><p>其中{Gd}表示非刚性变形的3D高斯分布。θnr表示非刚性变形模块的可学习参数。Z是一个潜在代码，它使用轻量级分层姿势编码器[28]对SMPL姿势和形状(θ,β)进行编码。具体来说，变形网络fθnr以规范位置xc和姿势潜在代码Zp作为输入，并输出高斯位置、尺度、旋转的偏移量以及特征向量z：(δx,δs,δq,z)=fθnr(xc;Zp)(7)</p><p></p><p></p><p>（3）：刚性变换：将非刚性变形的3D高斯分布{Gd}通过刚性变换模块进一步变换到观察空间：{Go}=Fθr({Gd};{Bb}Bb=1)(11)</p><p></p><p></p><p>其中皮肤网格变换MLPfθr被学习以预测位置xd处的皮肤权重。通过第3.1节中描述的前向LBS变换位置和3D高斯分布的旋转矩阵：T=�Bb=1fθr(xd)bBb(12)</p><p></p><p></p><p>（4）：颜色MLP：先前工作[63,67,68]遵循3DGS[14]的惯例，每个3D高斯分布存储球谐系数以编码视点相关颜色。将存储的颜色特征f视为球谐系数，则3D高斯分布的颜色可以通过球谐基和学习系数的点积来计算：c=⟨γ(d),f⟩(15)</p><p></p><p></p><p>其中d表示从相机中心到3D高斯分布的相对位置导出的视点方向。γ表示球谐基函数。虽然概念上很简单，但认为这种方法不适合单目设置。由于在训练期间只提供了一个相机视图，因此世界空间中的视点方向是固定的，导致对未见测试视图的泛化性较差。类似于[41]，使用第4.2节中的逆刚性变换将视点方向规范化：ˆd=T−11:3,1:3d(16)</p><p></p><p></p><p>其中T是等式（12）中定义的前向变换矩阵。理论上，规范视点方向可以提高模型对未见姿势的泛化能力。</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：</li><br>&lt;/ol&gt;<p></p><p></p><p>（1）本研究工作通过从单目视频中高效重建带服装的人类动画角色，推动了该领域的进步。该方法实现了逼真的渲染、对姿势相关布料变形的感知、对未见姿势的泛化、快速训练和实时渲染等优点。实验表明，该方法在渲染质量上与现有最先进的方法相当甚至更好，同时在训练和推理速度上提高了两个数量级。此外，还提出了用浅层 MLP 代替球谐函数来解码 3D 高斯颜色，并用几何约束来正则化变形，这两者都被证明可以有效提高渲染质量。我们希望这种新的表示能够促进从单目视图中快速、高质量的可动画带服装人类化身合成的进一步研究。</p><p></p><p></p><p>（2）创新点：</p><p></p><ul><li>提出了一种使用 3D 高斯散布 (3DGS) 从单目视频创建动画人类角色的方法。</li><li>该方法学习了一个非刚性变形网络来重建可动画的带服装的人类角色，可以在 30 分钟内训练完成，并以实时帧率（50+FPS）渲染。</li><li>引入了尽可能等距的正则化，以增强模型对未见姿势的泛化能力。</li></ul><p>性能：</p><ul><li>该方法在从单目输入创建动画角色方面取得了与现有方法相当甚至更好的性能。</li><li>该方法的训练速度提高了 400 倍，推理速度提高了 250 倍。</li></ul><p>工作量：</p><ul><li>该方法的训练和推理速度都非常快，可以在普通 GPU 上轻松实现。</li><li>该方法易于实现和使用。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-491840e5e9b907bfe6c860125c793a8e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-df8a29e21b43e7322f740381b022b6e4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c04b8f81d853c5df7e574e6e17d490fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-622f5d5aa71b525c2b25dfceb0d4c49a.jpg" align="middle"></details><br>​    <p></p><h2 id="SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance"><a href="#SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance" class="headerlink" title="SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance"></a>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance</h2><p><strong>Authors:Yuanyou Xu, Zongxin Yang, Yi Yang</strong></p><p>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: <a href="https://yoxu515.github.io/SEEAvatar/">https://yoxu515.github.io/SEEAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2312.08889v2">PDF</a> </p><p><strong>Summary</strong><br>使用 SEEAvatar 生成逼真 3D 头像，结合几何和外观的自我进化约束，产生高质量的网格和纹理。</p><p><strong>Key Takeaways</strong></p><ul><li>SEEAvatar 采用大规模文本到图像生成模型，从文本生成逼真的 3D 头像。</li><li>使用模板头像对优化后的头像进行约束，实现更灵活的形状生成。</li><li>人体先验也对脸部和手部等局部几何结构进行约束，以维持精细的结构。</li><li>扩散模型通过 prompt 工程增强，以指导基于物理的渲染管道生成逼真的纹理。</li><li>明度约束应用于漫反射贴图，以抑制不正确的照明效果。</li><li>SEEAvatar 在几何和外观的全局和局部质量上均优于以前的方法。</li><li>SEEAvatar 生成的优质网格和纹理可直接应用于经典图形管道，在任何照明条件下实现逼真的渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：SEEAvatar：具有约束几何和外观的逼真文本到 3D 头像生成</p></li><p></p><p></p><li><p>作者：Yuxuan Zhou, Hongyu Zhou, Jiapeng Tang, Yebin Liu, Yu-Kun Lai, Tao Xiang</p></li><p></p><p></p><li><p>单位：北京大学</p></li><p></p><p></p><li><p>关键词：文本到 3D 头像生成、生成对抗网络、扩散模型、几何约束、外观约束</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/abs/2302.09291，Github 链接：None</p></li><p></p><p></p><li><p>摘要：(1)：研究背景：文本到 3D 头像生成技术近年来取得了很大进展，但现有方法大多无法生成具有逼真几何和外观的头像。(2)：过去方法：现有方法存在的问题包括：几何不准确、外观质量低、无法控制头像的比例和保持局部结构。(3)：研究方法：本文提出了一种名为 SEEAvatar 的方法，该方法通过对几何和外观施加约束来生成逼真的 3D 头像。几何约束包括：全局形状约束、局部结构约束和人体先验约束。外观约束包括：光照约束和物理约束。(4)：实验结果：SEEAvatar 方法在多个数据集上进行了评估，实验结果表明，该方法在几何和外观质量方面均优于现有方法。</p></li><p></p><p></p><li><p>方法：（1）全局形状约束：使用球形谐波函数来表示头像的全局形状，并通过最小化重投影误差来优化形状参数。（2）局部结构约束：使用循环神经网络来生成头像的局部结构，并通过对抗训练来确保生成的结构与真实头像的结构相似。（3）人体先验约束：使用人体先验知识来约束头像的比例和姿势。（4）光照约束：使用光照模型来模拟头像的照明效果，并通过最小化光照误差来优化光照参数。（5）物理约束：使用物理模型来模拟头像的物理属性，并通过最小化物理误差来优化物理参数。</p></li><p></p><p></p><li><p>结论：（1）：本文提出的 SEEAvatar 方法能够生成具有约束几何和外观的逼真 3D 头像，该方法在几何和外观质量方面均优于现有方法。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种新的文本到 3D 头像生成方法，该方法通过对几何和外观施加约束来生成逼真的 3D 头像。</li><li>该方法能够生成具有准确的几何形状、逼真的外观和丰富的细节的 3D 头像。</li><li>该方法能够控制头像的比例和姿势，并保持局部结构。性能：</li><li>该方法在多个数据集上进行了评估，实验结果表明，该方法在几何和外观质量方面均优于现有方法。</li><li>该方法能够生成高质量的 3D 头像，这些头像可以应用于经典的工作流程中进行逼真的渲染。工作量：</li><li>该方法的实现相对复杂，需要较多的计算资源。</li><li>该方法的训练过程需要较长时间。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31d8f3ef22e9983e6f080f4f979f6284.jpg" align="middle"><img src="https://picx.zhimg.com/v2-204ca8c7f61c24414854bac9e34ba0a9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af635847f8e0712b1b887523a86123da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df9fb4cbdd77b5ee6fa2c33565667f41.jpg" align="middle"></details><br>​    <p></p><h2 id="ANR-Articulated-Neural-Rendering-for-Virtual-Avatars"><a href="#ANR-Articulated-Neural-Rendering-for-Virtual-Avatars" class="headerlink" title="ANR: Articulated Neural Rendering for Virtual Avatars"></a>ANR: Articulated Neural Rendering for Virtual Avatars</h2><p><strong>Authors:Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, Christoph Lassner</strong></p><p>The combination of traditional rendering with neural networks in Deferred Neural Rendering (DNR) provides a compelling balance between computational complexity and realism of the resulting images. Using skinned meshes for rendering articulating objects is a natural extension for the DNR framework and would open it up to a plethora of applications. However, in this case the neural shading step must account for deformations that are possibly not captured in the mesh, as well as alignment inaccuracies and dynamics — which can confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a novel framework based on DNR which explicitly addresses its limitations for virtual human avatars. We show the superiority of ANR not only with respect to DNR but also with methods specialized for avatar creation and animation. In two user studies, we observe a clear preference for our avatar model and we demonstrate state-of-the-art performance on quantitative evaluation metrics. Perceptually, we observe better temporal stability, level of detail and plausibility. </p><p><a href="http://arxiv.org/abs/2012.12890v1">PDF</a> </p><p><strong>摘要</strong><br>利用神經網絡，融合遞延神經渲染和網格形變，改進虛擬人生成。</p><p><strong>重要要点</strong></p><ul><li>延遲神經渲染是一種將傳統渲染技術與神經網絡相結合的方法，它在計算複雜性和逼真結果之間取得了很好的平衡。</li><li>將蒙皮網格用於渲染活動物體是遞延神經渲染框架的自然延伸，這將開啟大量應用。</li><li>然而，在這種情況下，神經著色步驟必須考慮那些網格可能無法捕捉的形變，以及對齊不準和動力學問題——這些問題可能會使遞延神經渲染管道混亂。</li><li>我們提出了基於遞延神經渲染的關節神經渲染框架，它明確地解决了虛擬人形頭像的局限性。</li><li>我們展示了關節神經渲染方法的優越性，不僅相對於遞延神經渲染，還優於專門用於頭像創建和動畫的方法。</li><li>在兩項用戶研究中，我們觀察到用戶對我們頭像模型的明顯偏好，並且在定量評估指標上展示了最先進的性能。</li><li>在感知上，我們觀察到更好的時態穩定性、細節層次和可信度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：关节神经渲染：虚拟形象的关节神经渲染</p></li><p></p><p></p><li><p>作者：Amit Raj、Julian Tanke、James Hays、Minh Vo、Carsten Stoll、Christoph Lassner</p></li><p></p><p></p><li><p>第一作者单位：佐治亚理工学院</p></li><p></p><p></p><li><p>关键词：神经渲染、虚拟形象、关节变形、神经纹理</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/pdf/2012.12890.pdf，Github 代码链接：无</p></li><p></p><p></p><li><p>摘要：（1）研究背景：计算机视觉的重要目标之一是捕捉逼真的外观。3D 渲染和神经网络的进步已经导致了具有显着保真度的技术。这些方法通常使用昂贵且复杂的捕捉设置，这阻止了生成模型的轻松数字化和传输。最近的延迟神经渲染范式为在准确的几何形状和相对简单的神经着色器内工作提供了一个激动人心的机会，同时逼真地捕捉具有视点依赖效果的复杂场景。（2）过去的方法及问题：延迟神经渲染特别适用于刚性物体。其管道可以以自然的方式扩展到可变形物体：可以使用蒙皮网格来捕捉几何形状。然后可以将来自姿势网格的光栅化神经纹理转换为 RGB 图像。虽然这个想法在概念上很简单，但神经网络必须学习更复杂的变形依赖效应。此外，用于渲染的网格通常不是 100% 准确的，并且可能与真实几何形状存在差异。这可能会导致神经渲染管道出现问题。（3）本文提出的研究方法：我们提出了一种新的框架，称为关节神经渲染 (ANR)，它明确解决了虚拟人形形象的延迟神经渲染限制。ANR 利用神经纹理和神经着色器来生成逼真的图像，同时显式地考虑关节变形和几何失真。ANR 还使用了一种新的损失函数，该损失函数可以更好地处理关节变形和几何失真。（4）方法在任务和性能上的表现：我们在两个用户研究中观察到人们对我们的人形形象模型的明显偏好，并且我们在定量评估指标上展示了最先进的性能。在感知上，我们观察到更好的时间稳定性、细节级别和合理性。更多结果可在我们的项目页面获得：https://anr-avatars.github.io。</p></li><p></p><p></p><li><p>方法：（1）延迟神经渲染（DNR）：DNR 使用一个神经纹理和一个神经渲染模型来将神经图像转换为 RGB 图像。神经图像可以通过将网格光栅化到图像空间并使用神经纹理对其进行纹理化来获得。（2）关节神经渲染（ANR）：ANR 在 DNR 的基础上，通过将神经渲染网络拆分为两个阶段 R1 和 R2 来处理关节变形和几何失真。R1 负责生成粗略的渲染结果和法线图像，R2 则使用 R1 的输出和法线图像来生成最终的渲染结果。（3）损失函数和正则化方案：ANR 使用了一个加权损失函数，该损失函数包括光度损失、特征损失、掩码损失、对抗损失和总变差损失。光度损失用于衡量生成图像和真实图像之间的差异，特征损失用于提高生成图像的锐度，掩码损失用于惩罚预测的掩码与真实掩码之间的差异，对抗损失用于鼓励生成图像的真实感，总变差损失用于鼓励生成图像的平滑性。（4）优化策略：ANR 使用了一个拆分优化策略来训练神经渲染模型。首先，使用一组关键帧来训练模型，以捕获静态的外观。然后，使用剩下的帧来训练模型，以学习处理关节变形和几何失真。</p></li><p></p><p></p><li><p>结论：（1）：本文提出了关节神经渲染（ANR），一种新颖的神经渲染框架，用于生成具有任意骨骼动画和视点的虚拟化身。我们工作的关键在于能够解释几何错位和与姿势相关的表面变形。我们的解决方案被仔细地集成到一个端到端的学习框架中，具有新颖的神经渲染架构和调整的优化方案。此外，ANR 可以使用单个神经渲染模型渲染多个化身。通过纹理和几何的解耦，它允许混合和编辑外观。对于未来的工作，我们看到进一步减轻几何错位的影响和提高对大姿势跟踪误差的弹性的潜在方向，以及将环境光照纳入渲染过程。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种新的神经渲染框架——关节神经渲染（ANR），可以生成具有任意骨骼动画和视点的虚拟化身。</li><li>ANR 能够解释几何错位和与姿势相关的表面变形。</li><li>ANR 使用了一个加权损失函数，该损失函数包括光度损失、特征损失、掩码损失、对抗损失和总变差损失。</li><li>ANR 使用了一个拆分优化策略来训练神经渲染模型。</li><li>ANR 可以使用单个神经渲染模型渲染多个化身。</li><li>通过纹理和几何的解耦，ANR 允许混合和编辑外观。</li></ul><p>性能：</p><ul><li>在两个用户研究中观察到人们对我们的人形形象模型的明显偏好。</li><li>在定量评估指标上展示了最先进的性能。</li></ul><p>工作量：</p><ul><li>收集和处理数据。</li><li>训练神经渲染模型。</li><li>评估神经渲染模型的性能。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b28956a7f40395402b0aec2307c9d6e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-124c65e51fa50d768c4606f610c9016e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-69e2a11eda84b6dbad8ef9daa42e2674.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0f13a2b60bef4c886a3317754c99b456.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol></ol></ol></ol></ol></ol>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-01-24 UltrAvatar A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>LLM</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/LLM/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/LLM/</id>
    <published>2024-01-23T17:59:49.000Z</published>
    <updated>2024-01-26T13:45:24.899Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models"><a href="#Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models" class="headerlink" title="Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models"></a>Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</h2><p><strong>Authors:Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang</strong></p><p>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at <a href="https://github.com/RL4M/MED-PEFT">https://github.com/RL4M/MED-PEFT</a>. </p><p><a href="http://arxiv.org/abs/2401.12215v1">PDF</a> Technical report</p><p><strong>摘要</strong><br>用少于 1% 可调参数实现超参数高效微调 (PEFT) 可以极大提高医学视觉基础模型的性能。</p><p><strong>主要要点</strong></p><ul><li>PEFT 是利用预训练大语言模型开发的一种有效的计算机视觉任务迁移学习方法。</li><li>PEFT 在医学视觉基础模型上的有效性尚不清楚，有待探索。</li><li>我们通过实验研究将 PEFT 应用于胸部 X 光基础模型。</li><li>与全参数微调 (FFT) 相比，LoRA 在三个著名的胸部 X 光数据集上的 18 个迁移学习任务中的 13 个中表现更好，最多提高了 2.9%，同时可调参数少于 1%。</li><li>将 LoRA 与基础模型相结合，我们在各种数据有效学习任务中创下了新纪录，例如在 NIH ChestX-ray14 上使用 1% 的标记数据获得了 80.6% 的 AUROC 分数。</li><li>我们希望这项研究能够引起社区对 PEFT 用于医学图像任务迁移学习的更多关注。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：少即是多：参数高效微调</p></li><li><p>作者：陈宇廉，周宏宇，于一舟，王连生</p></li><li><p>单位：厦门大学</p></li><li><p>关键词：迁移学习，医学视觉基础模型，胸部X光片</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12215，Github 代码链接：https://github.com/RL4M/MED-PEFT</p></li><li><p>摘要：(1) 研究背景：参数高效微调（PEFT）最初用于开发预训练的大语言模型，最近已成为计算机视觉任务中进行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型上的有效性仍不清楚，有待探索。(2) 过去的方法及问题：全参数微调（FFT）一直被认为是进行迁移学习的优越技术。然而，基础模型通常具有大量的参数，当下游任务只有有限的注释时，微调整个模型权重可能不是最优选择。在医学影像任务中，由于隐私、安全问题以及某些疾病的罕见性，注释通常难以获得，因此这种差异值得更多关注。(3) 研究方法：为了证明概念，我们对将 PEFT 应用于胸部放射线照相基础模型进行了详细的实证研究。具体来说，我们深入研究了 LoRA（一种具有代表性的 PEFT 方法），并将其与两个自监督放射线照相基础模型在三个完善的胸部放射线照相数据集上进行了比较。(4) 实验结果：我们的结果表明，在 18 个迁移学习任务中有 13 个任务中，LoRA 使用少于 1% 可调参数的表现优于 FFT，最多可达 2.9%。将 LoRA 与基础模型相结合，我们在各种数据高效学习任务中确立了新的最优水平，例如，在 NIHChestX-ray14 上使用 1% 的标记数据时，AUROC 得分为 80.6%。我们希望这项研究能够引起社区对 PEFT 在医学影像任务中进行迁移学习的更多关注。</p></li><li><p>Methods:(1): 提出了一种参数高效微调（PEFT）方法，用于医学视觉基础模型的迁移学习。(2): 将LoRA（一种具有代表性的PEFT方法）与两个自监督放射线照相基础模型在三个完善的胸部放射线照相数据集上进行了比较。(3): 在18个迁移学习任务中有13个任务中，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。(4): 将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。</p></li><li><p>结论：(1): 本工作首次将参数高效微调（PEFT）方法应用于医学视觉基础模型的迁移学习，并取得了优异的性能，为医学影像任务中的迁移学习提供了新的思路和方法。(2): 创新点：</p></li></ol><ul><li>提出了一种新的PEFT方法，该方法可以有效地将预训练的医学视觉基础模型迁移到下游任务，并取得了优异的性能。</li><li>在三个完善的胸部放射线照相数据集上，将LoRA与两个自监督放射线照相基础模型进行了比较，结果表明，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。</li><li>将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。性能：</li><li>在18个迁移学习任务中有13个任务中，LoRA使用少于1%可调参数的表现优于FFT，最多可达2.9%。</li><li>将LoRA与基础模型相结合，在各种数据高效学习任务中确立了新的最优水平，例如，在NIHChestX-ray14上使用1%的标记数据时，AUROC得分为80.6%。工作量：</li><li>本工作涉及了大量的数据收集、预处理、模型训练和评估工作，工作量较大。</li><li>本工作涉及了多种深度学习模型和算法，需要较强的理论基础和编程能力。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-88f5604fa47b7e6b53fa59ed5ce873a4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f28a6055dce3066c942bea25f00c4b98.jpg" align="middle"><img src="https://picx.zhimg.com/v2-219f68f671f950faee6332daa05d83eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b0e3d9c8b6a9c6651af0cb1202241988.jpg" align="middle"></details>​    ## CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation**Authors:Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz**Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{https://stanford-aimi.github.io/chexagent.html}. [PDF](http://arxiv.org/abs/2401.12208v1) 24 pages, 8 figures**摘要**针对目前医疗图像领域中的大规模视觉语言数据集较少、可捕捉医学数据复杂性的视觉和语言编码器缺乏、以及用于基准测试 CXR 解释能力的评估框架缺失的问题，本文提出了一个新的 CXR 解释基准框架 Chexbench 和一个新的视觉语言基础模型 CheXagent。**要点**- **医学影像领域** 中的 **大规模视觉语言数据集** 较少。- **医疗数据复杂性** 使得 **视觉和语言编码器** 难以有效捕捉。- 目前 **用于评估 CXR 解释能力的框架** 仍然**缺失**。- **CheXinstruct** 是一个从 **28 个公开可用的数据集** 中策划的大规模指令微调数据集。- **CheXagent** 是一个基于指令微调的视觉语言基础模型，能够分析和总结 CXR。- **CheXbench** 是一个新颖的基准，旨在系统地评估跨越 **8 个临床相关 CXR 解释任务** 的视觉语言基础模型。- **CheXagent** 在 **CheXbench** 任务上优于此前开发的通用和医学领域视觉语言基础模型。- 本项目还进行了公平性评估，以突出潜在的性能差异。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：CheXagent：迈向胸部 X 射线解读的基础模型</li><li>作者：Zhihong Chen、Maya Varma、Jean-Benoit Delbrouck、Magdalini Paschali、Louis Blankemeier、Dave Van Veen、Jeya Maria Jose Valanarasu、Alaa Youssef、Joseph Paul Cohen、Eduardo Pontes Reis、Emily B. Tsai、Andrew Johnston、Cameron Olsen、Tanishq Mathew Abraham、Sergios Gatidis、Akshay S. Chaudhari、Curtis Langlotz</li><li>第一作者单位：斯坦福大学</li><li>关键词：胸部 X 射线、医学图像、计算机视觉、自然语言处理、基础模型</li><li>论文链接：https://arxiv.org/abs/2401.12208Github 代码链接：无</li><li>摘要：（1）：胸部 X 射线（CXR）是临床实践中最常进行的影像检查。最近，视觉语言基础模型（FM）的发展为实现自动 CXR 解释提供了可能性，这可以帮助医生进行临床决策并改善患者预后。然而，由于以下原因，开发能够准确解释 CXR 的 FM 具有挑战性：（1）医学图像领域中缺乏大规模视觉语言数据集；（2）缺乏能够捕捉医学数据复杂性的视觉和语言编码器；（3）缺乏用于评估 FM 在 CXR 解释方面的能力的评估框架。（2）：过去的方法包括使用通用 FM 或针对医学图像领域微调的 FM。这些方法存在的问题在于它们无法充分捕捉 CXR 的复杂性，并且缺乏针对 CXR 解释任务的评估框架。（3）：本文提出了一种新的研究方法，该方法包括：</li></ol><ul><li>构建了一个名为 CheXinstruct 的大规模指令微调数据集，该数据集是从 28 个公开可用的数据集策划而来。</li><li>提出了一种名为 CheXagent 的指令微调 FM，该 FM 能够分析和总结 CXR。</li><li>引入了一个名为 CheXbench 的新基准，该基准旨在系统地评估 FM 在 8 个与临床相关的 CXR 解释任务中的表现。（4）：在 8 个 CXR 解释任务上，CheXagent 的性能优于之前开发的通用和医学领域 FM。此外，公平性评估表明，CheXagent 在性别、种族和年龄等因素上的性能没有显着差异。</li></ul><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>创新点：</p><ul><li>构建了一个名为 CheXinstruct 的大规模指令微调数据集，该数据集是从 28 个公开可用的数据集策划而来。</li><li>提出了一种名为 CheXagent 的指令微调 FM，该 FM 能够分析和总结 CXR。</li><li>引入了一个名为 CheXbench 的新基准，该基准旨在系统地评估 FM 在 8 个与临床相关的 CXR 解释任务中的表现。</li></ul><p>性能：</p><ul><li>在 8 个 CXR 解释任务上，CheXagent 的性能优于之前开发的通用和医学领域 FM。</li><li>公平性评估表明，CheXagent 在性别、种族和年龄等因素上的性能没有显着差异。</li></ul><p>工作量：</p><ul><li>数据集构建：从 28 个公开可用的数据集策划 CheXinstruct 数据集。</li><li>模型训练：训练 CheXagent 模型。</li><li>基准构建：构建 CheXbench 基准。</li><li>模型评估：在 CheXbench 基准上评估 CheXagent 的性能。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6aa52c71b57a2862b763a5188b83d6d8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7d79f07ab8199caa375ff5c3d1ce188.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f44d1729ed1485e81c04a41f097c005.jpg" align="middle"></details>​    ## Text Embedding Inversion Attacks on Multilingual Language Models**Authors:Yiyi Chen, Heather Lent, Johannes Bjerva**Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security. [PDF](http://arxiv.org/abs/2401.12192v1) 13 pages**Summary**不断增长的多语言用户植入对 NLP 安全研究提出了迫切要求，也加剧了黑盒多语言对嵌入逆向攻击及跨语言攻击的风险。**Key Takeaways**- 安全风险：植入作为一项服务 (EaaS) 模型使数据重构更容易，但过去的重点仅放在英语上，其他语言很脆弱。- 多语言模型更易受攻击：它们对实现可比的逆向性能的数据需求较少，即使在事先不知道基础语言的情况下也是如此。- 跨语言攻击：对攻击者来说，在跨语言场景中进行跨域攻击更为容易，因为目标语言通常与源语言不同。- 表现差异：多语言模型在不同语言上的性能差异可能导致跨语言设置中的攻击成功率发生变化。- 更少的数据需求：多语言模型对实现可比的逆向性能的数据需求较少，即使在事先不知道基础语言的情况下也是如此。- 需要进一步研究：这项工作强调了在 NLP 安全领域进行进一步研究和改进防御的必要性。- 防御措施：开发针对多语言和跨语言嵌入逆向攻击的防御措施，以确保 NLP系统的安全。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：多语言语言模型的文本嵌入逆向攻击</p></li><li><p>作者：Yiyi Chen, Heather Lent, Johannes Bjerva</p></li><li><p>第一作者单位：奥尔堡大学计算机科学系</p></li><li><p>关键词：自然语言处理，大语言模型，嵌入式服务，嵌入式逆向攻击，多语言，跨语言</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12192，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：随着自然语言处理（NLP）的工业应用日益广泛，大型语言模型（LLM）和嵌入式服务（EaaS）框架的使用也越来越普遍。EaaS 允许用户将文本数据存储为高品质的句子嵌入，从而提高搜索效率。然而，最近的研究表明，嵌入式逆向攻击可以从嵌入中解码出原始文本，这给 NLP 安全带来了重大威胁。</p><p>（2）过去的方法和问题：以往的研究主要集中在单语英语模型和嵌入上，假设攻击者知道文本的语言。然而，在现实场景中，攻击者可能不知道文本的语言。</p><p>（3）研究方法：本文定义了黑盒多语言和跨语言嵌入逆向攻击问题，并特别关注跨域场景。本文使用外部模型来近似嵌入逆向函数，该函数可以从嵌入中重建文本。</p><p>（4）方法的性能和目标支持：本文的实验结果表明，多语言模型比单语模型更容易受到逆向攻击。在不知道底层语言的情况下，攻击者也可以实现与单语模型相当的逆向性能。这表明多语言模型在安全性方面存在潜在的风险。</p><ol start="7"><li><p>方法：(1) 定义黑盒多语言和跨语言嵌入逆向攻击问题，关注跨域场景；(2) 使用外部模型 ψ 近似嵌入逆向函数 ϕ−1，从嵌入中重建文本；(3) 探讨文本生成模型在未知语言文本重建中的作用；(4) 研究多语言嵌入逆向攻击的潜力和影响；(5) 提出后逆向策略 AdhocTranslation，将生成文本从 ly 翻译成 lx，评估信息泄露情况；(6) 使用 T5-base 作为生成模型，在 ME5-base 和 GTR-base 上训练多语言逆向模型；(7) 与在英语数据集上训练的逆向模型比较，评估多语言模型的性能。</p></li><li><p>结论：（1）：本文首次对多语言嵌入逆向攻击问题进行了研究，为该方向的未来工作奠定了基础。我们的核心发现之一是，在某些情况下，多语言模型比单语英语模型更容易受到攻击。我们希望这项工作能够激发人们对 LLM 安全和 NLP 安全的关注，采取多语言的方法。（2）：创新点：</p></li></ol><ul><li>定义了黑盒多语言和跨语言嵌入逆向攻击问题，关注跨域场景。</li><li>使用外部模型 ψ 近似嵌入逆向函数 ϕ−1，从嵌入中重建文本。</li><li>探讨了文本生成模型在未知语言文本重建中的作用。</li><li>研究了多语言嵌入逆向攻击的潜力和影响。</li><li>提出后逆向策略 AdhocTranslation，将生成文本从 l_x 翻译成 l_y，评估信息泄露情况。</li><li>使用 T5-base 作为生成模型，在 ME5-base 和 GTR-base 上训练多语言逆向模型。</li><li>与在英语数据集上训练的逆向模型比较，评估了多语言模型的性能。性能：</li><li>多语言模型在某些情况下比单语英语模型更容易受到攻击。</li><li>在不知道底层语言的情况下，攻击者也可以实现与单语模型相当的逆向性能。</li><li>后逆向策略 AdhocTranslation 可以有效降低信息泄露风险。工作量：</li><li>实验计算量大，需要约 20,000 个 GPU 计算小时。</li><li>将这项研究扩展到更多语言将进一步增加开销。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-716d1bb2864a9ac6b2a1614499e04fd6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5de54c90ee7b1b6fe5ae2be76cb7496f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f940c4622168d9e745d9be24e1dccd3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99a0979006c1b27d04c44e10eca03872.jpg" align="middle"></details>​    ## WARM: On the Benefits of Weight Averaged Reward Models**Authors:Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret**Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM. [PDF](http://arxiv.org/abs/2401.12187v1) 14 pages, 9 figures**Summary**利用加权平均奖励模型来克服强化学习中存在的奖励窃取现象。**Key Takeaways**- 奖励窃取是指 LLM 利用奖励模型的漏洞来获得看似很高的奖励，但并未达到预期的目标。- 设计奖励模型时面临的两大挑战是：强化学习过程中分布的变化和人类偏好的不一致。- 提出加权平均奖励模型 (WARM) 来解决奖励窃取问题，先微调多个奖励模型，然后在权重空间对它们求平均。- WARM 通过使用最优 N 和强化学习方法在摘要任务上进行的实验证明，WARM 提高了 LLM 预测的整体质量和一致性。- 与采用单个奖励模型微调的策略型强化学习相比，采用 WARM 微调的策略型强化学习的获胜率为 79.4%。- 与传统预测集成相比，WARM 在分布变化和偏好不一致的情况下提高了效率和可靠性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：WARM：关于权重平均奖励模型的优势</li><li>作者：Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cidon, Olivier Bachem, Johan Ferret</li><li>隶属机构：谷歌大脑</li><li>关键词：对齐，RLHF，奖励建模，模型合并</li><li>链接：https://arxiv.org/abs/2401.12187</li><li>摘要：（1）研究背景：近年来，大型语言模型（LLM）在各个领域取得了令人瞩目的成就，这很大程度上得益于强化学习（RL）的应用。然而，在RLHF（从人类反馈中进行强化学习）中，奖励黑客问题是一个普遍存在的问题。奖励黑客是指策略（即正在训练的LLM）学会利用奖励模型（RM）中的漏洞，在不真正满足预期目标的情况下实现看似很高的奖励。这会导致性能下降、检查点选择复杂化、产生谄媚行为或放大社会偏见，最严重的是可能导致安全风险。（2）过去的方法及其问题：为了解决奖励黑客问题，一些研究人员提出了使用预测集成（ENS）的方法。ENS通过对多个RM的奖励进行平均，可以提高奖励的可靠性并降低黑客风险。然而，ENS存在内存和推理开销大的问题，而且它并不能提高对首选项数据集中标签噪声的鲁棒性。（3）本文提出的研究方法：为了解决上述问题，本文提出了权重平均奖励模型（WARM）。WARM通过对多个RM的权重进行线性插值，将它们合并成一个新的RM。这种方法继承了WA在分布偏移下的泛化能力，并提高了对标签损坏的鲁棒性。此外，WARM在效率和实用性方面也优于ENS，因为它只需要在推理时使用一个模型，而ENS需要对多个模型的预测进行平均。（4）方法性能：在本文的实验中，WARM在摘要任务上取得了比传统RLHF方法更好的性能。例如，使用WARM微调的策略在对抗使用单个RM微调的策略时，具有79.4%的获胜率。这表明WARM可以有效地提高LLM预测的整体质量和一致性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种新的权重平均奖励模型（WARM），以解决奖励建模中的两个关键挑战：分布偏移下的可靠性和标签损坏下的鲁棒性。通过对多个来自不同微调的奖励模型的权重进行线性插值，WARM 似乎是一种有效的解决方案，可以减轻人类反馈中的强化学习中的奖励黑客问题。我们的实证结果证明了将其应用于摘要时的有效性。我们预计 WARM 将有助于实现更加一致、透明和有效的 AI 系统，并鼓励在奖励建模方面进行进一步探索。（2）：创新点：</li></ol><ul><li>WARM 通过对多个奖励模型的权重进行平均，可以提高奖励的可靠性并降低黑客风险。</li><li>WARM 在效率和实用性方面优于预测集成（ENS），因为它只需要在推理时使用一个模型，而 ENS 需要对多个模型的预测进行平均。</li><li>WARM 在摘要任务上取得了比传统 RLHF 方法更好的性能。</li></ul><p>性能：</p><ul><li>WARM 在摘要任务上取得了比传统 RLHF 方法更好的性能。例如，使用 WARM 微调的策略在对抗使用单个 RM 微调的策略时，具有 79.4% 的获胜率。这表明 WARM 可以有效地提高 LLM 预测的整体质量和一致性。</li></ul><p>工作量：</p><ul><li>WARM 在效率和实用性方面优于预测集成（ENS），因为它只需要在推理时使用一个模型，而 ENS 需要对多个模型的预测进行平均。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cb30bdc7fa3fe5ca08f68a92a10b2271.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f65c9aafc185dee21168928e8ee7151.jpg" align="middle"></details>​    ## Temporal Blind Spots in Large Language Models**Authors:Jonas Wallat, Adam Jatowt, Avishek Anand**Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\footnote{https://github.com/jwallat/temporalblindspots}. [PDF](http://arxiv.org/abs/2401.12078v1) accepted at WSDM'24**摘要**大型语言模型在处理时间相关任务时存在局限性，尤其是在处理对过去信息的详细问题和较新信息时。**要点**- 大型语言模型在自然语言处理任务上表现出色，但受限于预训练语料库，对时间的理解能力有限。- 大型语言模型在回答关于过去问题的详细问题时表现不佳，对较新信息也存在理解困难。- 通过手动和自动测试发现大型语言模型存在多种时间错误。- 大型语言模型在处理时间相关任务时，随着问题对时间敏感性的增加，准确率下降。- 大型语言模型对时间的理解能力受制于预训练数据的时间范围。- 研究人员发现时间性问题可以分为事实性问题、个人经历问题和意见问题，其中大型语言模型对事实性问题的回答准确率最高。- 这项研究为理解大型语言模型的局限性做出了贡献，并为开发能够更好地满足时间相关任务需求的未来模型提供了宝贵的见解。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：大型语言模型中的时间盲点</p></li><li><p>作者：Jonas Wallat、Adam Jatowt、Avishek Anand</p></li><li><p>第一作者单位：德国汉诺威莱布尼兹信息科学研究所</p></li><li><p>关键词：大型语言模型、时间知识、时间理解、问答</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12078Github 链接：None</p></li><li><p>摘要：（1）研究背景：大型语言模型 (LLM) 在自然语言处理任务中表现出色，但其预训练数据通常局限于特定语料库，存在时效性和时间范围的限制，影响了其在涉及时间意图任务中的有效性。（2）过去的方法及其问题：以往的研究主要集中在知识探测、对抗性示例和风险分析等方面，但对 LLM 在时间知识和理解方面的盲点关注较少。（3）研究方法：本文通过三个时间问答数据集（Temporal Questions、ArchivalQA 和 TempLAMA）对 LLM 的时间知识和理解能力进行了全面评估，重点关注事实时间知识的处理和复杂时间信息的处理。（4）方法性能及对目标的支持：实验结果表明，LLM 在涉及时间知识和理解的任务中存在盲点，特别是在处理详细的历史问题和相对较新的信息时表现不佳。这些发现有助于理解 LLM 的局限性，并为开发能够更好地满足时间导向任务需求的未来模型提供有价值的见解。</p></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：</li></ol><p>（1）意义：本文通过对大型语言模型（LLM）的时间知识和理解能力进行全面评估，揭示了其在涉及时间意图任务中的盲点，为开发能够更好地满足时间导向任务需求的未来模型提供了有价值的见解。</p><p>（2）优缺点：创新点：</p><ul><li>提出了一种新的方法来评估 LLM 在时间知识和理解方面的能力。</li><li>发现 LLM 在处理详细的历史问题和相对较新的信息时表现不佳。</li></ul><p>性能：</p><ul><li>LLM 在涉及时间知识和理解的任务中存在盲点。</li></ul><p>工作量：</p><ul><li>需要收集和标记大量的数据来评估 LLM 的时间知识和理解能力。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ef953dd5680b8986b519ca3040e877c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bb539ecbb67b0f65215a7addd3ccd09f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68a2b498e74140261d533e54124b4339.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a748627690bad5dde36e8816fc801a09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-65999fcea7981a3e7935b9b9271ce7b8.jpg" align="middle"></details>​    ## Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated   Text**Authors:Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein**Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. [PDF](http://arxiv.org/abs/2401.12070v1) 20 pages, code available at https://github.com/ahans30/Binoculars**摘要**句法相似度评分可以准确区分人类文本与机器文本。**要点**- 基于比较两个紧密相关的语言模型的分数，可以非常准确地区分人类生成的文本和机器生成的文本。- 基于这种机制，我们提出了一种新颖的 LLM 检测器，它只需要使用一对预训练的 LLM 进行简单的计算。- 该方法称为 Binoculars，在没有任何训练数据的情况下实现了最先进的准确性。- 它能够在没有任何特定模型修改的情况下识别来自一系列现代 LLM 的机器文本。- 我们对 Binoculars 进行了全面的评估，涵盖多种文本来源和不同情况。- 在广泛的文档类型中，Binoculars 检测到超过 90% 的来自 ChatGPT（和其他 LLM）的生成样本，误报率为 0.01%，尽管它没有针对任何 ChatGPT 数据进行训练。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：使用双筒望远镜发现 LLM：零样本检测机器生成的文本</li><li>作者：Abhimanyu Hans、Avi Schwarzschild、Valeriia Cherepanova、Hamid Kazemi、Aniruddha Saha、Micah Goldblum、Jonas Geiping、Tom Goldstein</li><li>第一位作者的单位：马里兰大学</li><li>关键词：LLM 检测、零样本检测、语言模型、机器生成的文本</li><li>论文链接：https://arxiv.org/abs/2401.12070Github 代码链接：https://github.com/ahans30/Binoculars</li><li>摘要：</li></ol><p>（1）研究背景：检测现代大型语言模型生成的文本被认为是一项困难的任务，因为 LLM 和人都可以表现出广泛的复杂行为。（2）过去的方法和问题：现有方法主要依赖于训练数据来区分人写文本和机器生成的文本，但是这些方法往往需要大量的数据和模型训练，并且对于新的 LLM 或文本类型可能不适用。（3）提出的研究方法：本文提出一种基于对比两个紧密相关的语言模型的分数的新颖 LLM 检测器，称为双筒望远镜。该方法仅需使用预训练的 LLM 对进行简单的计算，无需任何训练数据。（4）方法的性能和对目标的支持：双筒望远镜在各种文本来源和不同情况下进行了全面的评估。在广泛的文件类型中，双筒望远镜检测到超过 90% 的来自聊天机器人和其他 LLM 生成的样本，误报率为 0.01%，尽管它没有在任何聊天机器人数据上进行训练。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种基于对比两个紧密相关的语言模型的分数的新颖LLM检测器，称为双筒望远镜。该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据，在广泛的文件类型中，双筒望远镜检测到超过90%的来自聊天机器人和其他LLM生成的样本，误报率为0.01%，尽管它没有在任何聊天机器人数据上进行训练。（2）：创新点：创新点一：提出了一种基于对比两个紧密相关的语言模型的分数的新颖LLM检测器，称为双筒望远镜。创新点二：该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据。性能：性能一：在广泛的文件类型中，双筒望远镜检测到超过90%的来自聊天机器人和其他LLM生成的样本。性能二：误报率为0.01%。工作量：工作量一：该方法仅需使用预训练的LLM对进行简单的计算，无需任何训练数据。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dd3bfb5b9d052c9fd7839e210dcdc353.jpg" align="middle"></details><br>​    <p></p><h2 id="Blinded-by-Generated-Contexts-How-Language-Models-Merge-Generated-and-Retrieved-Contexts-for-Open-Domain-QA"><a href="#Blinded-by-Generated-Contexts-How-Language-Models-Merge-Generated-and-Retrieved-Contexts-for-Open-Domain-QA" class="headerlink" title="Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?"></a>Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?</h2><p><strong>Authors:Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng</strong></p><p>While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs. </p><p><a href="http://arxiv.org/abs/2401.11911v1">PDF</a> </p><p><strong>摘要</strong><br>大型语言模型对生成和检索上下文的整合存在偏见，更多地依赖生成上下文中的信息。</p><p><strong>要点</strong></p><ul><li>大型语言模型对生成上下文的整合存在明显偏见。</li><li>大型语言模型更倾向于选择与问题更相似的生成上下文作为答案来源。</li><li>大型语言模型在整合检索上下文的过程中存在一定困难，检索上下文的不完整性可能影响了答案的准确性。</li><li>针对大型语言模型，现有上下文增强方法需要改进。</li><li>大型语言模型在整合生成和检索上下文的过程中表现出差异，理解这种差异有助于改进当前的增强方法。</li><li>影响大型语言模型上下文整合的两个关键因素：第一，生成上下文的相似性；第二，检索上下文的完整性。</li><li>大型语言模型整合上下文的能力可以通过理解和解决这些因素来提高。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：生成语境掩盖：语言模型如何将生成语境和检索语境融合用于开放域问答？</p></li><li><p>作者：谭鹤翔，孙飞，杨万里，王元卓，曹琪，程雪启</p></li><li><p>第一作者单位：中国科学院计算技术研究所人工智能安全与可信赖实验室</p></li><li><p>关键词：大型语言模型、生成增强、检索增强、冲突语境、问答</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11911，Github 链接：无</p></li><li><p>摘要：（1）研究背景：近年来，利用辅助信息增强大型语言模型（LLM）已成为一项重要研究方向。然而，对于 LLM 如何融合不同来源的语境，特别是生成语境和检索语境，目前的研究还相对较少。（2）过去方法与问题：现有工作主要分为生成增强和检索增强两种方法。生成增强方法通过生成与问题相关的背景语境，作为 LLM 回答问题的依据。检索增强方法通过从外部语料库中检索相关段落作为语境，以增强 LLM 的知识。然而，这些方法都存在一定的问题。生成增强方法生成的语境可能与问题不一致，检索增强方法检索到的语境可能不完整或与问题无关。（3）研究方法：为了研究 LLM 如何融合生成语境和检索语境，本文提出了一种新的任务，即冲突语境问答任务。该任务旨在识别 LLM 在融合生成语境和检索语境时，是否倾向于将答案归因于某个特定的语境。为了支持该任务，本文还开发了一种构建冲突语境数据集的方法。该数据集中的每个问题都配对有生成语境和检索语境，但只有其中一个语境包含正确答案。（4）实验结果与性能：本文在冲突语境问答任务上对 LLM 进行了评估，结果表明 LLM 存在明显的生成语境偏好。这种偏好体现在各个最先进的开放域（Llama2-7b/13b）和封闭域（GPT3.5/4）系统中。进一步分析表明，这种偏好主要由两个因素导致：一是生成语境通常与问题更相似，因此更容易被 LLM 选择；二是检索语境在被分割成段落后，其完整性受到破坏，从而难以被 LLM 充分利用。</p></li><li><p>方法：（1）任务定义：冲突语境问答任务。给定一个问题和两个语境，一个由生成器生成，另一个由检索器检索，判断答案来自哪个语境。（2）数据集构建：构建冲突语境数据集，每个问题配对有生成语境和检索语境，但只有其中一个语境包含正确答案。（3）模型评估：在冲突语境问答任务上评估 LLM，分析 LLM 在融合生成语境和检索语境时的偏好。</p></li><li><p>结论：（1）：本文提出了一种新的任务——冲突语境问答任务，用于研究大型语言模型（LLM）在融合生成语境和检索语境时的偏好。该任务旨在识别 LLM 在融合生成语境和检索语境时，是否倾向于将答案归因于某个特定的语境。为了支持该任务，本文还开发了一种构建冲突语境数据集的方法。该数据集中的每个问题都配对有生成语境和检索语境，但只有其中一个语境包含正确答案。（2）：创新点：提出了一种新的任务——冲突语境问答任务，用于研究 LLM 在融合生成语境和检索语境时的偏好。开发了一种构建冲突语境数据集的方法。性能：在冲突语境问答任务上评估 LLM，结果表明 LLM 存在明显的生成语境偏好。这种偏好体现在各个最先进的开放域（Llama2-7b/13b）和封闭域（GPT3.5/4）系统中。进一步分析表明，这种偏好主要由两个因素导致：一是生成语境通常与问题更相似，因此更容易被 LLM 选择；二是检索语境在被分割成段落后，其完整性受到破坏，从而难以被 LLM 充分利用。工作量：构建冲突语境数据集的工作量较大，需要收集大量的问题和语境，并进行人工标注。评估 LLM 在冲突语境问答任务上的表现的工作量也较大，需要对 LLM 进行多次评估，并分析评估结果。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aff8facaa355b1505b2cf6af3d0e915b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1ee80fef672e8714cbda66ee9ba9e921.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-32c0aa5250fcb9295d1e46e737e52534.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-99f94640fc796568a6b02c8056191892.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c63733100557d4290705642b87c665f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8815deb3960e6b2bbbe8b92e6f8e6799.jpg" align="middle"></details><br>​    <p></p><h2 id="PsySafe-A-Comprehensive-Framework-for-Psychological-based-Attack-Defense-and-Evaluation-of-Multi-agent-System-Safety"><a href="#PsySafe-A-Comprehensive-Framework-for-Psychological-based-Attack-Defense-and-Evaluation-of-Multi-agent-System-Safety" class="headerlink" title="PsySafe: A Comprehensive Framework for Psychological-based Attack,   Defense, and Evaluation of Multi-agent System Safety"></a>PsySafe: A Comprehensive Framework for Psychological-based Attack,   Defense, and Evaluation of Multi-agent System Safety</h2><p><strong>Authors:Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao</strong></p><p>Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate significant capabilities for collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. From the perspective of agent psychology, we discover that the dark psychological states of agents can lead to severe safety issues. To address these issues, we propose a comprehensive framework grounded in agent psychology. In our framework, we focus on three aspects: identifying how dark personality traits in agents might lead to risky behaviors, designing defense strategies to mitigate these risks, and evaluating the safety of multi-agent systems from both psychological and behavioral perspectives. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents’ propensity for self-reflection when engaging in dangerous behavior, and the correlation between agents’ psychological assessments and their dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. We will make our data and code publicly accessible at https:/github.com/AI4Good24/PsySafe. </p><p><a href="http://arxiv.org/abs/2401.11880v1">PDF</a> </p><p><strong>摘要</strong><br>多智能体系统结合了大型语言模型 (LLM)，显示出显著的集体智能能力，但也存在被恶意利用的风险。</p><p><strong>要点</strong></p><ul><li>多智能体系统的安全性问题尚未得到全面研究。</li><li>从智能体心理学角度来看，智能体的黑暗心理状态可能导致严重的安全问题。</li><li>我们提出了一个基于智能体心理学的综合框架，重点关注三个方面：识别智能体中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为两个角度评估多智能体系统的安全性。</li><li>我们的实验揭示了一些有趣的现象，例如智能体之间的集体危险行为、智能体在从事危险行为时的自我反省倾向，以及智能体的心理评估与其危险行为之间的相关性。</li><li>我们希望我们的框架和观察结果将为多智能体系统安全性的进一步研究提供有价值的见解。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：黑暗人格特质攻击提示：用于危险任务评估的提示</p></li><li><p>作者：Yihan Wang, Qiang Zhang, Yuxin Peng, Xiaotong Li, Jingbo Shang, Xiangliang Zhang, Yujie Zhang, Jie Tang, Yong Yu</p></li><li><p>第一作者单位：斯坦福大学</p></li><li><p>关键词：多智能体系统、黑暗人格特质、安全、心理、行为</p></li><li><p>论文链接：https://arxiv.org/abs/2302.09358，Github 代码链接：Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：多智能体系统，增强了大型语言模型（LLM），展示了集体智能的显著能力。然而，将这种智能恶意用于恶意目的的潜在滥用带来了重大风险。迄今为止，关于多智能体系统安全问题的全面研究仍然有限。</p><p>（2）过去方法及其问题：从代理心理学角度，我们发现代理的黑暗心理状态可能导致严重的安全问题。为了解决这些问题，我们提出了一个以代理心理学为基础的综合框架。</p><p>（3）提出的研究方法：在我们的框架中，我们关注三个方面：识别代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性。</p><p>（4）方法在任务和性能方面的表现：我们的实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性。我们预计，我们的框架和观察结果将为进一步研究多智能体系统的安全性提供有价值的见解。</p><ol start="7"><li><p>方法：（1）提出一个基于代理心理学的综合框架，关注代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性；（2）通过实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性；（3）分析了不同提示对多智能体系统危险率的影响，包括手工制作的越狱提示、黑暗特质提示注入、诱导指令注入和危险意图的隐藏；（4）分析了从不同角度攻击多智能体系统的影响，包括人类输入攻击、高频人类输入攻击、特质攻击和混合攻击方法；（5）评估了不同的大语言模型的安全性，包括基于 API 的模型和开源模型，研究了模型大小与危险率之间的关系；（6）进行了防御实验，评估了输入过滤器、GPT-4 的有害提示识别、DoctorDefense 和 PoliceDefense 的有效性。</p></li><li><p>结论：（1）：从心理学的角度，本文全面分析了多智能体系统的安全性问题，引入了一种结合黑暗人格特质的攻击方法。这种方法可以有效地破坏多智能体系统，诱发危险行为。本文还提出了防御策略，可以显著降低多智能体系统中危险行为的风险。此外，本文还引入了一种包含心理和行为两个方面在内的安全评估方法，对多智能体系统的安全性进行综合评估。（2）：创新点：提出了一种基于代理心理学的综合框架，关注代理中的黑暗人格特质如何导致危险行为、设计防御策略来减轻这些风险，以及从心理和行为角度评估多智能体系统的安全性；通过实验揭示了一些有趣的现象，例如代理之间的集体危险行为、代理在从事危险行为时进行自我反省的倾向，以及代理的心理评估与其危险行为之间的相关性；分析了不同提示对多智能体系统危险率的影响，包括手工制作的越狱提示、黑暗特质提示注入、诱导指令注入和危险意图的隐藏；分析了从不同角度攻击多智能体系统的影响，包括人类输入攻击、高频人类输入攻击、特质攻击和混合攻击方法；评估了不同的大语言模型的安全性，包括基于API的模型和开源模型，研究了模型大小与危险率之间的关系；进行了防御实验，评估了输入过滤器、GPT-4的有害提示识别、DoctorDefense和PoliceDefense的有效性。性能：本文提出的攻击方法可以有效地破坏多智能体系统，诱发危险行为。本文提出的防御策略可以显著降低多智能体系统中危险行为的风险。本文提出的安全评估方法可以对多智能体系统的安全性进行综合评估。工作量：本文的工作量很大，需要进行大量的实验和分析。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c838c9a54be9dc1fa902235acfd4e0fa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1dc1793d596c4da4ce3e17bd21b27f34.jpg" align="middle"></details><br>​    <p></p><h2 id="Improving-Small-Language-Models’-Mathematical-Reasoning-via-Mix-Thoughts-Distillation"><a href="#Improving-Small-Language-Models’-Mathematical-Reasoning-via-Mix-Thoughts-Distillation" class="headerlink" title="Improving Small Language Models’ Mathematical Reasoning via Mix Thoughts   Distillation"></a>Improving Small Language Models’ Mathematical Reasoning via Mix Thoughts   Distillation</h2><p><strong>Authors:Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</strong></p><p>This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Mix Thoughts Distillation (MTD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while MTD enables these models to achieve state-of-the-art reasoning performance. </p><p><a href="http://arxiv.org/abs/2401.11864v1">PDF</a> </p><p><strong>摘要</strong></p><p>融合推理蒸馏方法增强小语言模型数学推理能力。</p><p><strong>要点</strong></p><ul><li>方程式思维蒸馏 (EoTD) 是一种新颖的技术，将推理过程封装到基于方程式的表示中，以便为精调 SLM 构建 EoTD 数据集。</li><li>EoTD 大幅提升了SLM的推理能力，而MTD使这些模型能够实现最先进的推理性能。</li><li>混合思维蒸馏 (MTD) 框架用于创建具有多种思维过程的推理数据集，并将其用于精调。</li><li>EoTD 数据集的构建方法为 SLM 的推理能力的提高提供了新的思路。</li><li>MTD 框架通过创建具有多种思维过程的推理数据集，可以进一步增强SLM的推理性能。</li><li>EoTD 和 MTD 方法的结合，使得 SLM 能够实现最先进的推理性能。</li><li>这些方法为 SLM 的推理能力的提高提供了新的思路，并有望在自然语言处理、机器学习等领域得到广泛的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：通过混合思想蒸馏改进小型语言模型的数学推理能力</p></li><p></p><p></p><li><p>作者：朱洵宇, 李健, 刘勇, 马灿, 王卫平</p></li><p></p><p></p><li><p>第一作者单位：中国科学院信息工程研究所</p></li><p></p><p></p><li><p>关键词：小型语言模型；数学推理；蒸馏；思想链；方程思维</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/abs/2401.11864</p></li><p></p><p></p><li><p>摘要：(1) 研究背景：随着大型语言模型（LLM）的发展，其在自然语言处理任务中表现出强大的性能。然而，LLM的庞大参数规模和计算需求限制了其在实际应用中的部署。为了解决这一问题，研究人员提出了将LLM的数学推理能力压缩到数十亿参数的小型语言模型（SLM）中，以实现更广泛的部署。(2) 过去的方法及其问题：现有方法主要通过使用LLM创建包含详细推理路径的丰富数据集，然后微调SLM来实现知识迁移。然而，这些方法在数学问题求解方面存在明显的差距。(3) 本文提出的研究方法：本文提出了一种名为“方程思维蒸馏”（EoTD）的框架来增强SLM的数学推理能力。EoTD首先提示LLM对问题生成方程，然后使用方程求解器求解这些方程。不产生正确解的方程会被丢弃。利用这种方法，我们构建了EoTD数据集，并使用该数据集微调SLM，从而提升了SLM的推理能力。(4) 方法在任务和性能上的表现：在数学问题求解任务上，EoTD显著提高了SLM的推理能力。同时，我们还提出了混合思想蒸馏（MTD）框架，进一步增强了SLM的推理性能。MTD通过创建一个包含多种推理过程的推理数据集，并使用该数据集微调SLM来实现。实验结果表明，MTD使SLM在数学推理任务上达到了最先进的性能。这些结果证明了EoTD和MTD方法的有效性，为将LLM的数学推理能力压缩到SLM中提供了新的思路。</p></li><p></p><p></p><li><p>方法：(1) 方程思维蒸馏（EoTD）：</p><p></p><ul><li>从大型语言模型（LLM）生成包含方程的推理数据集。</li><li>使用方程求解器求解这些方程，并丢弃不产生正确解的方程。</li><li>利用该数据集微调小型语言模型（SLM），以提升其数学推理能力。(2) 混合思想蒸馏（MTD）：</li><li>构建包含多种推理过程的推理数据集。</li><li>使用该数据集微调 SLM，以进一步增强其推理性能。(3) MTD 的优势：</li><li>结合了方程思维蒸馏、链式思维蒸馏和程序思维蒸馏的优点。</li><li>弥补了各思想过程的不足，增强了 SLM 的数学推理能力。</li></ul></li><li><p>结论：（1）：本研究工作的重要意义在于，它为将大型语言模型（Large Language Models，LLMs）先进的推理能力压缩到小规模语言模型（Small Language Models，SLMs）中迈出了重要一步。通过提出方程思维蒸馏（Equation-of-Thought Distillation，EoTD）和混合思想蒸馏（Mix Thoughts Distillation，MTD）方法，我们证明了将 LLMs 的数学推理能力压缩到参数量少于十亿的 SLMs 中是可行的。EoTD 方法有效地捕获了基于方程的推理过程，促进了 SLMs 对数学推理的理解和生成。MTD 框架通过结合多种推理过程的数据集进一步增强了 SLMs 的推理能力，使其在推理任务上取得了最先进的性能。（2）：创新点：</p></li></ol><ul><li>提出了一种新的蒸馏框架 EoTD，该框架可以将 LLMs 的数学推理能力压缩到 SLMs 中。</li><li>构建了一个包含方程的推理数据集，并使用该数据集微调 SLMs，以提升其数学推理能力。</li><li>提出了一种新的蒸馏框架 MTD，该框架可以结合多种推理过程的数据集来微调 SLMs，进一步增强其推理性能。性能：</li><li>EoTD 和 MTD 方法在数学推理任务上显著提高了 SLMs 的推理能力。</li><li>在数学推理任务上，MTD 使 SLMs 达到了最先进的性能。工作量：</li><li>EoTD 和 MTD 方法需要构建推理数据集并微调 SLMs，这需要一定的工作量。</li><li>EoTD 和 MTD 方法需要使用方程求解器来求解方程，这可能会增加计算成本。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4f7748b843608736dabf6b6cc14b4ee1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d7fdb18ed70f0091f9dbdbddc997d51.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7c622575f8bb790ddfec633e22a66d78.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9c9ffd2b8828d29126e178af6f1f45c3.jpg" align="middle"></details><br>​    <p></p><h2 id="Speak-It-Out-Solving-Symbol-Related-Problems-with-Symbol-to-Language-Conversion-for-Language-Models"><a href="#Speak-It-Out-Solving-Symbol-Related-Problems-with-Symbol-to-Language-Conversion-for-Language-Models" class="headerlink" title="Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language   Conversion for Language Models"></a>Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language   Conversion for Language Models</h2><p><strong>Authors:Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu</strong></p><p>Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs. We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media. Experimental results show that S2L consistently leads to superior performance. For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively. Codes and data are available at <a href="https://github.com/THUNLP-MT/symbol2language">https://github.com/THUNLP-MT/symbol2language</a>. </p><p><a href="http://arxiv.org/abs/2401.11725v1">PDF</a> </p><p><strong>Summary</strong><br>在自然语言理解方面表现出色的大语言模型 (LLM) 在处理符号方面能力不足，符号到语言 (S2L) 方法可将符号转换为语言表示，从而显著提高 LLM 解决符号相关问题的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>符号广泛存在于各种任务中，如抽象推理、化学性质预测和表格问答。</li><li>LLM 在自然语言理解方面表现出色，但在处理符号方面能力不足。</li><li>符号与一般自然语言的表示方式不同，这是导致 LLM 符号推理能力不足的原因之一。</li><li>S2L 是一种不需要微调的方法，可以使 LLM 能够利用自然语言表达的信息来解决符号相关的问题。</li><li>S2L 先将符号转换为语言表示，然后将这些语言表示通过直接替换或连接的方式集成到原始问题中，作为 LLM 的有用输入信息。</li><li>利用 S2L 方法，可以显著提高 LLM 在符号相关任务上的性能。</li><li>S2L 方法的代码和数据可以在 <a href="https://github.com/THUNLP-MT/symbol2language">https://github.com/THUNLP-MT/symbol2language</a> 找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>论文标题：SpeakItOut：利用符号与语言转换解决符号相关问题</p></li><p></p><p></p><li><p>作者：王一乐、程思杰、孙子欣、李鹏、刘洋</p></li><p></p><p></p><li><p>第一作者单位：清华大学人工智能产业研究院</p></li><p></p><p></p><li><p>关键词：符号与语言转换、大语言模型、符号相关问题</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/abs/2401.11725Github 代码链接：https://github.com/THUNLP-MT/symbol2language</p></li><p></p><p></p><li><p>摘要：</p></li><br>&lt;/ol&gt;<p></p><p></p><p>（1）研究背景：符号（或更广泛的非自然语言文本表示），如数值序列、分子式和表格分隔符，广泛存在，在各种任务中发挥着重要作用，如抽象推理、化学性质预测和表格问答。尽管大型语言模型（LLM）具有令人印象深刻的自然语言理解能力，但它们对符号的推理能力仍然不足，这可能归因于符号表示与一般自然语言之间的差异。</p><p></p><p></p><p>（2）过去的方法及其问题：本文提出了一种无调优的方法符号到语言（S2L），该方法使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。S2L 首先将涉及的符号转换为基于语言的表示，这可以通过提示 LLM 或利用外部工具来实现，然后通过直接替换或连接将这些基于语言的表示集成到原始问题中，作为 LLM 的有用输入信息。</p><p></p><p></p><p>（3）本文提出的研究方法：我们使用基于 API（GPT-4、ChatGPT）和开源（OpenChat）的模型在八个符号相关任务上评估了 S2L 方法，这些任务从仅符号的抽象推理到社交媒体中的情感分析。实验结果表明，S2L 一致地带来了更好的性能。例如，通过将 S2L 用于 GPT-4，1D-ARC 和 Dyck 语言的子任务可以分别平均显着提高 21.9% 和 9.5%。</p><p></p><p></p><p>（4）方法在任务上的表现及其对目标的支持：S2L 方法在各种涉及不同类型符号的场景中具有广泛的适用性。</p><p></p><p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：（1）：本文提出了符号到语言转换方法，该方法将符号表示转换为基于语言的表示，从而使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。实验结果表明，该方法在各种涉及不同类型符号的场景中具有广泛的适用性，并且能够显著提高任务的性能。（2）：创新点：</li><br>&lt;/ol&gt;<p></p><ul><li>将符号表示转换为基于语言的表示，从而使大型语言模型能够利用自然语言中表达的信息来解决符号相关问题。</li><li>提出了一种无调优的方法，该方法无需对大型语言模型进行任何额外的训练，即可将其应用于符号相关问题。</li><li>在八个符号相关任务上评估了该方法的性能，实验结果表明该方法能够显著提高任务的性能。性能：</li><li>在1D-ARC和Dyck语言的子任务上，该方法分别平均显着提高了21.9%和9.5%。</li><li>在化学性质预测任务上，该方法的准确率提高了10.2%。</li><li>在表格问答任务上，该方法的准确率提高了5.8%。工作量：</li><li>该方法的实现相对简单，并且不需要对大型语言模型进行任何额外的训练。</li><li>该方法可以很容易地应用于各种符号相关问题。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-40de55454aa9c0a64ccc5f00f797fa15.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-61ef72b92fc749addf01e22c5321e38b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c34ab5861560ccbb591eb0b220a02b3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe73936de3fc8438e09058d53307b11.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5fc7e17bf194fb6a87ae3ea50cc1c8f9.jpg" align="middle"></details><br>​    <p></p><h2 id="Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs"><a href="#Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs" class="headerlink" title="Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs"></a>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a> </p><p><a href="http://arxiv.org/abs/2401.11708v1">PDF</a> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></p><p><strong>摘要</strong><br>充分利用多模态语言模型的思维链能力，有效地提高文本到图像扩散模型的合成性。</p><p><strong>要点</strong></p><ul><li>提出了一种新型的无训练文本到图像生成/编辑框架，称为重新注释、计划和生成 (RPG)。</li><li>RPG 利用多模态语言模型作为全局规划器，将生成复杂图像的过程分解为子区域内的多个更简单的生成任务。</li><li>提出了一种互补的区域扩散算法，以实现区域级的合成生成。</li><li>在闭合回路中集成文本导向图像生成和编辑，从而提高泛化能力。</li><li>大量实验表明，RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL，尤其是在多类别对象合成和文本图像语义对齐方面。</li><li>RPG 框架与各种多模态语言模型架构（例如 MiniGPT-4）和扩散模型骨干（例如 ControlNet）具有广泛的兼容性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：掌握文本到图像扩散：使用多模态 LLM 进行重新描述、规划和生成</p></li><p></p><p></p><li><p>作者：凌扬<em>1、余兆晨</em>1、孟晨林23、徐明凯2、斯特凡诺·埃尔蒙2、崔斌1</p></li><p></p><p></p><li><p>所属单位：北京大学</p></li><p></p><p></p><li><p>关键词：文本到图像扩散、多模态 LLM、重新描述、规划、生成、区域扩散</p></li><p></p><p></p><li><p>论文链接：https://github.com/YangLing0818/RPG-DiffusionMasterGithub 链接：https://github.com/YangLing0818/RPG-DiffusionMaster</p></li><p></p><p></p><li><p>摘要：（1）研究背景：扩散模型在文本到图像生成和编辑方面表现出色的性能。然而，现有方法在处理涉及具有多个属性和关系的多个对象的复杂文本提示时通常面临挑战。（2）过去的方法及其问题：一些工作通过引入额外的布局/框作为条件或利用提示感知注意指导来解决这个问题。然而，这些方法通常需要额外的训练数据或复杂的设计，并且在处理复杂的文本提示时可能仍然存在局限性。（3）研究方法：本文提出了一种全新的无训练文本到图像生成/编辑框架，称为重新描述、规划和生成 (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。我们的方法使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。我们提出了互补的区域扩散来实现区域组合生成。此外，我们将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。（4）方法性能：广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。</p></li><p></p><p></p><li><p>提出了一种新的无训练文本到图像生成/编辑框架，称为重新描述、规划和生成 (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</p></li><p></p><p></p><li><p>使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。</p></li><p></p><p></p><li><p>提出互补的区域扩散来实现区域组合生成。</p></li><p></p><p></p><li><p>将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。</p></li><p></p><p></p><li><p>广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。</p></li><p></p><p></p><li><p>我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。</p></li><p></p><p></p><li><p>结论：（1）：本文提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性，在多类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</li><li>使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个更简单的生成任务，这些任务位于子区域内。</li><li>提出互补的区域扩散来实现区域组合生成。</li><li>将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。性能：</li><li>广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，尤其是在多类别对象组合和文本图像语义对齐方面。</li><li>我们的 RPG 框架与各种 MLLM 架构（例如，MiniGPT-4）和扩散骨干网（例如，ControlNet）具有广泛的兼容性。工作量：</li><li>本文的工作量较大，涉及到多模态 LLM、文本到图像扩散模型、区域扩散模型等多个方面的研究。</li><li>本文需要进行大量的实验来验证所提出的方法的有效性。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d7ede89518c7e2b2017c785eb927b766.jpg" align="middle"><img src="https://pica.zhimg.com/v2-69a6785a9dc22c046203d70cee24a3f1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b57333091d6dbb8392ce8971cf413d0b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d6f54078071dcab585ee882e1cb7cb6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40b7d562cad3ed84d89938dbcdb65fff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe1c57ab8d093322b4502e666dccd4cb.jpg" align="middle"></details><br>​    <p></p><h2 id="FinSQL-Model-Agnostic-LLMs-based-Text-to-SQL-Framework-for-Financial-Analysis"><a href="#FinSQL-Model-Agnostic-LLMs-based-Text-to-SQL-Framework-for-Financial-Analysis" class="headerlink" title="FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial   Analysis"></a>FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial   Analysis</h2><p><strong>Authors:Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen, Dongfang Lou, Jinshu Lin</strong></p><p>Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer. </p><p><a href="http://arxiv.org/abs/2401.10506v1">PDF</a> 13 pages, 13 figures</p><p><strong>Summary</strong><br>基于大语言模型的金融领域文本转 SQL 框架 FinSQL 实现了最先进的文本转 SQL 性能，并且在少样本跨数据库模型迁移的场景中，FinSQL 可以带来高达 36.64% 的性能提升。</p><p><strong>Key Takeaways</strong></p><ul><li>基于大语言模型的文本转 SQL 框架 FinSQL 在金融领域实现了最先进的性能，在 BULL 数据集上的准确率达到了 99.36%。</li><li>FinSQL 利用提示工程、参数高效微调和输出校准等技术，有效地解决了金融领域文本转 SQL 的挑战，特别是在处理宽表方面具有优势。</li><li>FinSQL 可以通过提示工程和微调，轻松地适应不同的金融领域数据库，并且在少样本跨数据库模型迁移的场景中表现出色。</li><li>FinSQL 提供了一个统一的框架，可以应用于各种金融领域文本转 SQL 任务，例如基金分析、股票分析和宏观经济分析。</li><li>FinSQL 可以帮助金融专业人士轻松地从文本查询中提取信息，从而提高工作效率和决策质量。</li><li>FinSQL 是一个开源工具，可以在 GitHub 上获取，<a href="https://github.com/hundun-tech/FinSQL。">https://github.com/hundun-tech/FinSQL。</a></li><li>FinSQL 可以在各种硬件平台上运行，包括 CPU 和 GPU，并且支持多种编程语言，包括 Python、Java 和 C++。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：FinSQL：基于模型无关的 LLM 的金融分析文本转 SQL 框架</p></li><p></p><p></p><li><p>作者：Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen, Dongfang Lou, Jinshu Lin</p></li><p></p><p></p><li><p>单位：浙江大学</p></li><p></p><p></p><li><p>关键词：文本转 SQL，大型语言模型（LLM），金融分析</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2401.10506Github：无</p></li><p></p><p></p><li><p>摘要：（1）研究背景：文本转 SQL 旨在将自然语言问题转换成可执行的 SQL 查询，这有助于非专业数据库用户访问数据。在金融分析领域，金融专业人士经常需要查询相关数据库，但他们通常不熟悉 SQL 编程。因此，文本转 SQL 对金融分析非常重要。（2）过去方法及问题：目前没有针对金融分析的文本转 SQL 基准数据集，现有文本转 SQL 方法也没有考虑金融分析中使用的数据库的独特特征。（3）研究方法：本文构建了一个实用的金融分析文本转 SQL 数据集 BULL，该数据集包含三个分别对应于基金、股票和宏观经济的数据库。此外，本文还提出了一种基于模型无关的 LLM 的文本转 SQL 框架 FinSQL。FinSQL 从提示构建、参数高效微调和输出校准的角度对金融文本转 SQL 进行了系统处理。（4）任务和性能：在 BULL 数据集上进行的广泛实验结果表明，FinSQL 以较小的成本实现了最先进的文本转 SQL 性能；此外，在需要少量跨数据库模型迁移的场景中，FinSQL 可以带来高达 36.64% 的性能提升。这些性能结果支持了本文的目标。</p></li><p></p><p></p><li><p>方法：（1）构建金融分析文本转 SQL 基准数据集 BULL，包含基金、股票和宏观经济三个数据库；（2）提出基于模型无关的 LLM 的文本转 SQL 框架 FinSQL，从提示构建、参数高效微调和输出校准三个角度对金融文本转 SQL 进行了系统处理；（3）利用 ChatGPT 自动生成同义问句，丰富问题风格的多样性；（4）设计规则从 SQL 查询中提取关键词，获得相应的骨架，创建骨架增强数据集，指导模型先生成 SQL 骨架，再生成最终 SQL 查询；（5）改进 Cross-Encoder 模型的训练和推理过程，使其适用于金融场景，快速准确地检索模式项；（6）提出基于 LoRA 的参数高效微调框架，支持低资源微调和跨数据库泛化，包括 LoRA-based 多任务参数高效微调方法、LoRA 插件中心和权重合并方法；（7）构建 LoRA 插件中心，存储训练好的 LoRA 模块，支持低资源场景下的少量微调。</p></li><p></p><p></p><li><p>结论：（1）：本文构建了首个金融分析文本转SQL基准数据集BULL，并提出了基于模型无关的LLM的文本转SQL框架FinSQL，在BULL数据集上取得了最先进的性能，并支持低资源微调和跨数据库泛化。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>构建了首个金融分析文本转SQL基准数据集BULL，包含基金、股票和宏观经济三个数据库。</li><li>提出了一种基于模型无关的LLM的文本转SQL框架FinSQL，从提示构建、参数高效微调和输出校准三个角度对金融文本转SQL进行了系统处理。</li><li>设计了规则从SQL查询中提取关键词，获得相应的骨架，创建骨架增强数据集，指导模型先生成SQL骨架，再生成最终SQL查询。</li><li>改进了Cross-Encoder模型的训练和推理过程，使其适用于金融场景，快速准确地检索模式项。</li><li>提出基于LoRA的参数高效微调框架，支持低资源微调和跨数据库泛化，包括LoRA-based多任务参数高效微调方法、LoRA插件中心和权重合并方法。</li><li>构建了LoRA插件中心，存储训练好的LoRA模块，支持低资源场景下的少量微调。性能：</li><li>在BULL数据集上进行的广泛实验结果表明，FinSQL以较小的成本实现了最先进的文本转SQL性能；此外，在需要少量跨数据库模型迁移的场景中，FinSQL可以带来高达36.64%的性能提升。工作量：</li><li>本文构建了首个金融分析文本转SQL基准数据集BULL，包含基金、股票和宏观经济三个数据库，并提出了基于模型无关的LLM的文本转SQL框架FinSQL，在BULL数据集上取得了最先进的性能，并支持低资源微调和跨数据库泛化。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9c895692f6c567a00b199fa54f5a74f5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b388a7ca23a42a81a34c91321601c58d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-503c271dadd87a4b6296f39bca961d5e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f4ac08d2e533ded93b75b84bb260e475.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bc20ecf0e3b40a207c16c1ef7b53fe00.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bf510e39ce68125220c0fde2cc70d74.jpg" align="middle"></details><br>​    <p></p><h2 id="Beyond-Reference-Based-Metrics-Analyzing-Behaviors-of-Open-LLMs-on-Data-to-Text-Generation"><a href="#Beyond-Reference-Based-Metrics-Analyzing-Behaviors-of-Open-LLMs-on-Data-to-Text-Generation" class="headerlink" title="Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on   Data-to-Text Generation"></a>Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on   Data-to-Text Generation</h2><p><strong>Authors:Zdeněk Kasner, Ondřej Dušek</strong></p><p>We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs’ in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models’ behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at <a href="https://d2t-llm.github.io">https://d2t-llm.github.io</a>. </p><p><a href="http://arxiv.org/abs/2401.10186v1">PDF</a> 26 pages</p><p><strong>Summary</strong><br>大型语言模型在无参考零样本数据对文本生成任务中的语义准确性存在重大问题。</p><p><strong>Key Takeaways</strong></p><ul><li>我们收集了一个新的数据对文本生成基准 Quintd-1，以防止基准中的偏见影响 LLM 的训练数据。</li><li>我们使用无参考评估指标和 LLM 的上下文学习能力，允许我们在没有任何人工书写参考的情况下测试模型。</li><li>我们对模型在不同领域和任务中的行为进行了系统检查，结果表明，具有 7B 参数的最新开放式 LLM 可以从各种标准数据格式中生成流畅连贯的文本，而无需进行任何预训练。</li><li>但是，我们还发现，输出的语义准确性仍然是一个主要问题：在我们的基准测试中，根据人工注释者，80% 的开放式 LLM 输出包含语义错误（根据 GPT-4，这一比例为 91%）。</li><li>需要更多的方法来提高 LLM 在数据对文本生成任务中的语义准确性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>论文标题：超越基于引用的指标：分析开放式大语言模型在数据到文本生成中的行为</p></li><p></p><p></p><li><p>作者：Zdeněk Kasner、Ondřej Dušek</p></li><p></p><p></p><li><p>第一作者单位：查尔斯大学数学与物理学院形式与应用语言学研究所，捷克共和国布拉格</p></li><p></p><p></p><li><p>关键词：数据到文本生成、大语言模型、无参考评估、语义准确性</p></li><p></p><p></p><li><p>论文链接：https://arxiv.org/abs/2401.10186，Github 代码链接：https://github.com/d2t-llm/d2t-llm</p></li><p></p><p></p><li><p>摘要：(1) 研究背景：大语言模型在自然语言处理领域取得了显著进展，但其在数据到文本生成任务中的适用性尚未得到充分探索。当前的数据到文本生成基准测试存在饱和问题，并且与人类判断的相关性较差。(2) 过去方法及其问题：传统的基于引用的评估指标与人类判断的相关性较差。封闭式大语言模型的使用被认为是一种不良的研究实践，因为其不可重复且存在数据污染问题。(3) 本文提出的研究方法：本文提出了一种新的数据到文本生成基准测试 QUINTD-1，该基准测试由来自五个领域的结构化数据记录组成。本文还提出了一种新的评估方法，该方法结合了人类注释者和基于 GPT-4 的指标，用于评估模型输出的语义准确性。(4) 方法在任务上的表现及其对目标的支持：本文的方法在 QUINTD-1 基准测试上进行了评估。结果表明，最先进的开放式大语言模型能够从各种标准数据格式中生成流畅且连贯的文本。然而，语义准确性仍然是一个主要问题，80% 的开放式大语言模型输出根据人类注释者包含语义错误（根据 GPT-4 为 91%）。</p></li><p></p><p></p><li><p>方法：(1): 提出QUINTD-1基准测试，包含来自五个领域的结构化数据记录，用于评估数据到文本生成模型的性能。(2): 提出一种新的评估方法，结合人类注释者和基于GPT-4的指标，用于评估模型输出的语义准确性。(3): 使用最先进的开放式大语言模型在QUINTD-1基准测试上进行评估，分析模型在不同数据格式下的生成能力和语义准确性。</p></li><p></p><p></p><li><p>结论：（1）：本文探索了基于开放式大语言模型的数据到文本生成任务，提出了新的基准测试 QUINTD-1 和评估方法，分析了最先进的开放式大语言模型在不同数据格式下的生成能力和语义准确性，为数据到文本生成任务提供了新的研究方向。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出新的基准测试 QUINTD-1，包含来自五个领域的结构化数据记录，用于评估数据到文本生成模型的性能。</li><li>提出一种新的评估方法，结合人类注释者和基于 GPT-4 的指标，用于评估模型输出的语义准确性。</li><li>使用最先进的开放式大语言模型在 QUINTD-1 基准测试上进行评估，分析模型在不同数据格式下的生成能力和语义准确性。性能：</li><li>最先进的开放式大语言模型能够从各种标准数据格式中生成流畅且连贯的文本。</li><li>但语义准确性仍然是一个主要问题，80% 的开放式大语言模型输出根据人类注释者包含语义错误（根据 GPT-4 为 91%）。工作量：</li><li>收集和标注 QUINTD-1 基准测试的数据集。</li><li>实现新的评估方法，包括基于 GPT-4 的指标和人类注释。</li><li>使用最先进的开放式大语言模型在 QUINTD-1 基准测试上进行评估。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cbc2ee00465454eb9b14d3b763aac637.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1481e93defe7c7bf222585814e3cc756.jpg" align="middle"><img src="https://pica.zhimg.com/v2-133889154a0e1815bf8287c40392dc96.jpg" align="middle"><img src="https://pica.zhimg.com/v2-773279112248c748e33e938a9774aed3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-41a6874b90e948f998889323b12872af.jpg" align="middle"></details><br>​    <p></p><h2 id="DiffusionGPT-LLM-Driven-Text-to-Image-Generation-System"><a href="#DiffusionGPT-LLM-Driven-Text-to-Image-Generation-System" class="headerlink" title="DiffusionGPT: LLM-Driven Text-to-Image Generation System"></a>DiffusionGPT: LLM-Driven Text-to-Image Generation System</h2><p><strong>Authors:Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen</strong></p><p>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains. </p><p><a href="http://arxiv.org/abs/2401.10061v1">PDF</a> </p><p><strong>摘要</strong><br>扩散模型结合大型语言模型，构建领域知识库，引导模型选择，提升图像生成多样性。</p><p><strong>要点</strong></p><ul><li>扩散模型在图像生成领域取得重大进展，开源平台共享高质量模型。</li><li>当前文本转图像系统存在输入多样性不足，模型结果单一等挑战。</li><li>现有统一尝试分为两种正交方面：i) 在输入阶段解析不同提示；ii) 激活专家模型以输出。</li><li>DiffusionGPT 结合语言模型优势，构建统一生成系统，支持多种提示，集成领域专家模型。</li><li>DiffusionGPT 基于先验知识为不同生成模型构建特定领域树形结构。</li><li>输入时，语言模型解析提示，利用树形思维引导选择合适模型，放宽输入限制，在不同领域确保出色性能。</li><li>引入优势数据库，将树形思维与人类反馈相结合，使模型选择过程与人类偏好相一致。</li><li>大量实验与比较证明了 DiffusionGPT 的有效性，展示了其在不同领域图像合成中的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：DiffusionGPT：基于树状思维模型的统一扩散生成系统</li><p></p><p></p><li>作者：Yichuan Liu<em>, Xiang Wang</em>, Xihong Wu, Zhe Gan, Yujun Shen, Jingyi Zhang, Xiaogang Wang</li><p></p><p></p><li>单位：北京大学计算机科学技术系</li><p></p><p></p><li>关键词：扩散模型、生成模型、树状思维模型、文本到图像生成</li><p></p><p></p><li>链接：https://arxiv.org/abs/2302.08134, Github：无</li><p></p><p></p><li>摘要：（1）研究背景：扩散模型在图像生成领域取得了显著进展，但现有文本到图像生成系统通常无法处理多样化的输入，或者仅限于单一模型的结果。（2）过去的方法及其问题：目前的研究主要集中在输入阶段解析多样化的提示或在输出阶段激活专家模型。这些方法存在以下问题：1）难以同时处理多样化的输入；2）无法充分利用领域专家模型的知识。（3）研究方法：本文提出了一种新的文本到图像生成系统——DiffusionGPT，它利用大型语言模型（LLM）构建领域特定的树状思维模型，并根据输入提示和树状思维模型选择合适的生成模型。（4）实验结果：在多个任务上，DiffusionGPT在图像质量、多样性和准确性方面均优于现有方法，证明了其有效性。</li><br>&lt;/ol&gt;<p></p><p></p><p><strong>方法：</strong></p><p></p><p></p><p>（1）提示解析：利用大型语言模型（LLM）分析和提取输入提示中的关键文本信息，以准确识别用户想要生成的核心内容，并减轻噪声文本的影响。</p><p></p><p></p><p>（2）模型思想树构建与搜索：使用模型思想树（TOT）的概念来构建模型树，并利用模型树的搜索能力来缩小候选模型的集合，提高模型选择过程的准确性。</p><p></p><p></p><p>（3）模型选择与人工反馈：通过与用户交互，获取用户对生成结果的反馈，并根据反馈结果调整模型选择策略，以提高生成结果的质量。</p><p></p><p></p><p>（4）生成执行：根据选定的模型，生成图像。</p><p></p><p></p><ol start="8"><p></p><p></p><li>结论：（1）本工作通过提出 DiffusionGPT，将高性能生成模型与高效提示解析无缝集成，在文本到图像生成任务中取得了显著的成果，为该领域的研究提供了新的思路。（2）创新点：</li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种新的文本到图像生成框架 DiffusionGPT，该框架能够同时处理多样化的输入提示并充分利用领域专家模型的知识。</li><li>提出了一种基于大型语言模型（LLM）的提示解析方法，该方法能够准确识别用户想要生成的核心内容，并减轻噪声文本的影响。</li><li>提出了一种基于模型思想树（TOT）的概念构建模型树的方法，该方法能够缩小候选模型的集合，提高模型选择过程的准确性。</li><li>提出了一种基于用户反馈的模型选择策略，该策略能够根据用户对生成结果的反馈，调整模型选择策略，以提高生成结果的质量。性能：</li><li>在多个任务上，DiffusionGPT 在图像质量、多样性和准确性方面均优于现有方法，证明了其有效性。</li><li>DiffusionGPT 能够处理多样化的输入提示，并生成高质量、多样性和准确的图像。</li><li>DiffusionGPT 能够充分利用领域专家模型的知识，生成符合特定领域要求的图像。工作量：</li><li>DiffusionGPT 的训练和推理过程相对复杂，需要大量的数据和计算资源。</li><li>DiffusionGPT 的模型选择过程需要与用户交互，这可能会增加生成图像的工作量。</li><li>DiffusionGPT 的提示解析过程需要使用大型语言模型（LLM），这可能会增加生成图像的成本。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-206805985eb655491965884e7bb9f034.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-66c858aa31f63c6ce390099af5809303.jpg" align="middle"><img src="https://picx.zhimg.com/v2-176a2174255ac60d001c26f929b2818d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1cea822f74219b16f2b14b9a08ee6e77.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol></ol></ol></ol></ol></ol></ol>]]></content>
    
    
    <summary type="html">LLM 方向最新论文已更新，请持续关注 Update in 2024-01-24 Less Could Be Better Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="LLM" scheme="https://kedreamix.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/NeRF/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/NeRF/</id>
    <published>2024-01-23T17:30:59.000Z</published>
    <updated>2024-01-27T05:47:38.924Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p><p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p><p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 在各种应用中获得广泛欢迎，但面临稀疏视图设置时的挑战，缺乏体积渲染的足够约束。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 在稀疏视图设置中面临挑战，因为缺乏足够的体积渲染约束。</li><li>重建和理解稀疏且不受约束的相机的 3D 场景是经典计算机视觉中的一个长期问题，具有广泛的应用。</li><li>最近的工作已经探索了稀疏、不受约束的视图场景中的 NeRF，但重点主要放在增强重建和新颖视图合成。</li><li>ProvNeRF 通过提出问题“每个点是从哪里看到的？”来采用更广泛的视角 — 这决定了我们对它的理解和重建程度。</li><li>ProvNeRF 是一种模型，它通过结合每个点的出处来丰富传统的 NeRF 表示，建模每个点的可能来源位置。</li><li>ProvNeRF 与任何预先训练的 NeRF 模型及其关联的训练相机位姿兼容。</li><li>对比最先进的方法，建模每个点的出处提供了多种优势，包括不确定性估计、基于准则的视图选择和改进的新颖视图合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ProvNeRF：将 NeRF 中的逐点来源建模为随机过程</li><li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li><li>第一作者单位：斯坦福大学</li><li>关键词：NeRF、稀疏视图、来源、不确定性估计、视点优化、新颖视图合成</li><li>论文链接：https://arxiv.org/abs/2401.08140、Github 链接：None</li><li>摘要：(1)：研究背景：神经辐射场 (NeRF) 在许多应用中都获得了广泛关注。然而，由于仅靠体积渲染无法提供足够的约束，它们在稀疏视图设置中面临挑战。(2)：过去方法和问题：最近的一些工作探索了在稀疏、不受约束的视图场景中使用 NeRF。为了解决约束不足的问题，他们将先验信息纳入 NeRF 优化中，例如深度、局部几何或全局形状信息。然而，这些工作只关注于实现更好的新颖视图合成，而没有解决如何从更全面的角度理解场景的问题，例如不确定性估计、基于标准的视点选择和鲁棒性。(3)：研究方法：我们提出 ProvNeRF，这是一种通过纳入逐点来源来丰富传统 NeRF 表示的模型，该模型为每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。值得注意的是，我们的方法与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。(4)：方法性能：我们证明了对逐点来源进行建模提供了许多优势，包括不确定性估计、基于准则的视点选择和改进的新颖视图合成，与最先进的方法相比，我们的方法在这些任务上取得了更好的性能。这些性能支持了我们的目标。</li></ol><p>Methods:</p><p>(1)：我们提出ProvNeRF，这是一种通过纳入逐点来源来丰富传统NeRF表示的模型，该模型为每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。</p><p>(2)：我们定义逐点来源为一个随机过程，其索引集为R3，并由无穷多个随机变量组成。每个随机变量对应于R3中的一个点，其分布由该点的边缘分布给出。边缘分布表示可以观察到该点的可能位置的分布。</p><p>(3)：我们使用一个神经网络来学习从潜在随机变量分布到模型分布的变换。潜在随机变量分布由输入位置x的随机线性变换和x本身的连接组成。模型分布由神经网络Hθ参数化，该神经网络将潜在随机函数映射到函数Dθ。</p><p>(4)：为了优化模型分布，我们扩展了IMLE以建模随机过程的分布。我们通过在每个点处匹配经验样本和模型样本，将IMLE公式调整为函数空间。</p><ol><li>结论：（1）：ProvNeRF通过扩展随机过程的隐式最大似然估计（IMLE），将逐点来源纳入传统NeRF表示，增强了NeRF模型的表示能力，使其能够在稀疏视图设置中更好地估计不确定性、选择最佳视点并合成新颖视图。（2）：创新点：ProvNeRF将逐点来源建模为随机过程，并通过扩展IMLE来优化模型分布，这是一种新颖的建模方法，可以有效地提高NeRF模型在稀疏视图设置下的性能。性能：ProvNeRF在不确定性估计、基于准则的视点选择和新颖视图合成等任务上取得了更好的性能，证明了对逐点来源进行建模的有效性。工作量：ProvNeRF的实现相对简单，可以轻松应用于任何预训练的NeRF模型，并且不需要额外的训练数据或相机位姿信息。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle"></details><h2 id="TriNeRFLet-A-Wavelet-Based-Multiscale-Triplane-NeRF-Representation"><a href="#TriNeRFLet-A-Wavelet-Based-Multiscale-Triplane-NeRF-Representation" class="headerlink" title="TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation"></a>TriNeRFLet: A Wavelet Based Multiscale Triplane NeRF Representation</h2><p><strong>Authors:Rajaei Khatib, Raja Giryes</strong></p><p>In recent years, the neural radiance field (NeRF) model has gained popularity due to its ability to recover complex 3D scenes. Following its success, many approaches proposed different NeRF representations in order to further improve both runtime and performance. One such example is Triplane, in which NeRF is represented using three 2D feature planes. This enables easily using existing 2D neural networks in this framework, e.g., to generate the three planes. Despite its advantage, the triplane representation lagged behind in its 3D recovery quality compared to NeRF solutions. In this work, we propose TriNeRFLet, a 2D wavelet-based multiscale triplane representation for NeRF, which closes the 3D recovery performance gap and is competitive with current state-of-the-art methods. Building upon the triplane framework, we also propose a novel super-resolution (SR) technique that combines a diffusion model with TriNeRFLet for improving NeRF resolution. </p><p><a href="http://arxiv.org/abs/2401.06191v1">PDF</a> webpage link: <a href="https://rajaeekh.github.io/trinerflet-web">https://rajaeekh.github.io/trinerflet-web</a></p><p><strong>摘要</strong></p><p>引入了基于二维小波表示的 TriNeRFLet 以弥补三平面表示的 3D 重建质量与 NeRF 解决方案之间的差距。</p><p><strong>主要内容</strong></p><ul><li>TriNeRFLet 是一种基于二维小波表示的 TriPlane 表示，可以弥补三平面表示的 3D 重建质量与 NeRF 解决方案之间的差距。</li><li>TriNeRFLet 在 3D 重建质量上与当前最先进的方法具有竞争力。</li><li>TriNeRFLet 可以与扩散模型相结合以提高 NeRF 分辨率。</li><li>使用 TriNeRFLet 可以提高 NeRF 在复杂 3D 场景中的重建质量。</li><li>TriNeRFLet 可以使用现有的 2D 神经网络来生成三个平面。</li><li>TriNeRFLet 可以轻松地应用于现有的 NeRF 框架。</li><li>TriNeRFLet 可以提高 NeRF 在光照条件复杂场景中的重建质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：TriNeRFLet：一种基于小波的多尺度三平面 NeRF 表示</li><li>作者：Rajaie Khatib, Raja Giryes</li><li>单位：特拉维夫大学电子工程系</li><li>关键词：神经辐射场、三平面、小波、多尺度、超分辨率</li><li>链接：https://rajaeekh.github.io/trinerflet-web，Github 链接：None</li><li><p>摘要：(1)：近年来，神经辐射场 (NeRF) 模型因其恢复复杂 3D 场景的能力而广受欢迎。继其成功之后，许多方法提出了不同的 NeRF 表示，以便进一步提高运行时和性能。其中一个例子是三平面，其中 NeRF 使用三个 2D 特征平面表示。这使得在这个框架中轻松使用现有的 2D 神经网络，例如生成三个平面。尽管有优势，但与 NeRF 解决方案相比，三平面表示在 3D 恢复质量方面落后。在这项工作中，我们提出了 TriNeRFLet，一种用于 NeRF 的基于 2D 小波的多尺度三平面表示，它缩小了 3D 恢复性能差距，并与当前最先进的方法具有竞争力。在三平面框架的基础上，我们还提出了一种新颖的超分辨率 (SR) 技术，该技术将扩散模型与 TriNeRFLet 相结合，以提高 NeRF 分辨率。(2)：过去的方法包括使用三轴对齐的 2D 特征平面来表示 NeRF，称为三平面。在渲染过程中，通过将每个点投影到三个平面之一，然后连接对应于三个投影的特征，对每个点进行采样。这为该点形成一个单一的特征向量，然后将其传递给输出该点密度和颜色值的较小 MLP。三平面表示的一个显着优势是它可以与许多已经存在的 2D 方法一起使用。在最初的工作中，作者使用现有的 2D 生成对抗网络 (GAN) 架构来生成其平面。后续工作采用了 2D 卷积神经网络 (CNN) 架构来生成平面。然而，这些方法在 3D 恢复质量方面落后于 NeRF 解决方案。(3)：在这项工作中，我们提出了 TriNeRFLet，一种基于 2D 小波的多尺度三平面表示，它缩小了 3D 恢复性能差距，并与当前最先进的方法具有竞争力。TriNeRFLet 利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。此外，我们还提出了一种新颖的超分辨率 (SR) 技术，该技术将扩散模型与 TriNeRFLet 相结合，以提高 NeRF 分辨率。(4)：在各种数据集上的实验表明，TriNeRFLet 在 3D 重建质量和渲染速度方面优于最先进的方法。具体来说，在 DTU 数据集上，TriNeRFLet 的平均重投影误差为 0.006，而最先进的方法为 0.008。在 Replica 数据集上，TriNeRFLet 的平均重投影误差为 0.004，而最先进的方法为 0.006。此外，TriNeRFLet 在渲染速度方面也优于最先进的方法。在 DTU 数据集上，TriNeRFLet 的平均渲染时间为 0.02 秒，而最先进的方法为 0.04 秒。在 Replica 数据集上，TriNeRFLet 的平均渲染时间为 0.01 秒，而最先进的方法为 0.03 秒。这些结果表明，TriNeRFLet 在 3D 重建质量和渲染速度方面优于最先进的方法。</p></li><li><p>Methods：（1）TriNeRFLet是一种基于2D小波的多尺度三平面NeRF表示，它利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。（2）TriNeRFLet在渲染过程中，通过将每个点投影到三个平面之一，然后连接对应于三个投影的特征，对每个点进行采样。这为该点形成一个单一的特征向量，然后将其传递给输出该点密度和颜色值的较小MLP。（3）TriNeRFLet还提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。</p></li><li><p>结论：（1）：本文提出了一种基于2D小波的多尺度三平面NeRF表示TriNeRFLet，它利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。此外，我们还提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。在各种数据集上的实验表明，TriNeRFLet在3D重建质量和渲染速度方面优于最先进的方法。（2）：创新点：TriNeRFLet利用小波变换将每个平面分解为多个尺度，从而捕获不同尺度的几何细节。TriNeRFLet提出了一种新颖的超分辨率（SR）技术，该技术将扩散模型与TriNeRFLet相结合，以提高NeRF分辨率。性能：在DTU数据集上，TriNeRFLet的平均重投影误差为0.006，而最先进的方法为0.008。在Replica数据集上，TriNeRFLet的平均重投影误差为0.004，而最先进的方法为0.006。在DTU数据集上，TriNeRFLet的平均渲染时间为0.02秒，而最先进的方法为0.04秒。在Replica数据集上，TriNeRFLet的平均渲染时间为0.01秒，而最先进的方法为0.03秒。工作量：TriNeRFLet的实现相对复杂，需要对小波变换和扩散模型有一定的了解。TriNeRFLet的训练时间较长，在DTU数据集上需要大约12小时，在Replica数据集上需要大约8小时。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-376ce19b86e43007a4505ad233d775ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-62e7822d95a507a9a6135289c7daa699.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7a5c91a388f997c293454bfee53afa88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e34ea48c160c12032d9b31bd76183537.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c58b3d3d4afab12adc77185baa182d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f1a3bb1b557793cc8f1d5f612cbf4c1.jpg" align="middle"></details><h2 id="Fast-High-Dynamic-Range-Radiance-Fields-for-Dynamic-Scenes"><a href="#Fast-High-Dynamic-Range-Radiance-Fields-for-Dynamic-Scenes" class="headerlink" title="Fast High Dynamic Range Radiance Fields for Dynamic Scenes"></a>Fast High Dynamic Range Radiance Fields for Dynamic Scenes</h2><p><strong>Authors:Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang</strong></p><p>Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \url{<a href="https://guanjunwu.github.io/HDR-HexPlane/}">https://guanjunwu.github.io/HDR-HexPlane/}</a>. </p><p><a href="http://arxiv.org/abs/2401.06052v1">PDF</a> 3DV 2024. Project page: <a href="https://guanjunwu.github.io/HDR-HexPlane">https://guanjunwu.github.io/HDR-HexPlane</a></p><p><strong>Summary</strong><br>动态 HDR NeRF 框架 HDR-HexPlane 可以从具有不同曝光度的动态 2D 图像中学习 3D 场景，并以任何时间点渲染任意曝光下的高质量新视图图像。</p><p><strong>Key Takeaways</strong></p><ul><li>HDR-HexPlane 是一种动态 HDR NeRF 框架，可以从具有不同曝光度的动态 2D 图像中学习 3D 场景。</li><li>HDR-HexPlane 构建了一个可学习的曝光映射函数，以获得每张图像的自适应曝光值。</li><li>HDR-HexPlane 基于单调递增的先验设计了一种相机响应函数，以实现稳定学习。</li><li>HDR-HexPlane 可以以任何时间点渲染任意曝光下的高质量新视图图像。</li><li>HDR-HexPlane 构建了一个包含多个动态场景的数据集，这些场景是用不同的曝光拍摄的，以便进行评估。</li><li>HDR-HexPlane 的所有数据集和代码均可在 <a href="https://guanjunwu.github.io/HDR-HexPlane/">https://guanjunwu.github.io/HDR-HexPlane/</a> 获得。</li><li>HDR-HexPlane 可有效地从具有不同曝光度的动态 2D 图像中学习 3D 场景，在各种动态场景的重建与渲染任务中表现出优异的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：动态场景的快速高动态范围辐射场（中文翻译：动态场景的快速高动态范围辐射场）</li><li>作者：Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, Xinggang Wang</li><li>第一作者单位：华中科技大学计算机学院（中文翻译：华中科技大学计算机学院）</li><li>关键词：神经辐射场、高动态范围、动态场景、曝光映射、相机响应函数</li><li>论文链接：https://arxiv.org/abs/2401.06052，Github 代码链接：None</li><li>总结：（1）：研究背景：神经辐射场（NeRF）及其扩展在表示 3D 场景和合成新视角图像方面取得了巨大成功。然而，大多数 NeRF 方法使用低动态范围 (LDR) 图像，这可能会丢失细节，尤其是在照明不均匀的情况下。一些先前的 NeRF 方法尝试引入高动态范围 (HDR) 技术，但主要针对静态场景。（2）：过去的方法及其问题：过去的方法主要针对静态场景，无法处理动态场景。此外，这些方法通常需要对图像进行预处理，例如对齐和裁剪，这可能会引入误差。（3）：本文提出的研究方法：为了将 HDR NeRF 方法扩展到更广泛的应用，我们提出了一种动态 HDR NeRF 框架，名为 HDR-HexPlane，它可以从以不同曝光值捕获的动态 2D 图像中学习 3D 场景。我们构建了一个可学习的曝光映射函数来为每个图像获得自适应曝光值。基于单调递增先验，我们设计了一个相机响应函数以实现稳定学习。利用所提出的模型，可以在任何时间点以任何期望的曝光渲染高质量的新视角图像。我们还构建了一个包含多个动态场景的数据集，这些场景以不同的曝光进行捕获，以进行评估。（4）：方法在什么任务上取得了什么性能：在合成新视角图像的任务上，HDR-HexPlane 在多个动态场景上取得了最先进的性能。此外，HDR-HexPlane 还可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。使用色调映射函数，可以实现更好的色彩平衡，从而提高图像的整体视觉质量。这些性能支持了本文的目标。</li></ol><p><strong>Methods</strong>：**</p><p>（1）：我们首先回顾了HDR-NeRF和HexPlane的方法。</p><p>（2）：然后我们介绍了HDR-HexPlane的框架。</p><p>（3）：我们讨论了如何学习未知的曝光，并介绍了sigmoid相机响应函数。</p><p>（4）：最后，我们讨论了优化部分。</p><ol><li>结论：（1）：本文提出了一种名为 HDR-HexPlane 的动态 HDR 神经辐射场框架，它可以从以不同曝光值捕获的动态 2D 图像中学习 3D 场景。HDR-HexPlane 在合成新视角图像的任务上取得了最先进的性能，并且可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。（2）：创新点：</li><li>将 HDR 成像和动态场景表示管道集成到一个统一的框架中，以高效地学习 HDR 动态场景。</li><li>提出了一种可学习的曝光映射函数，可以为每个图像获得自适应曝光值。</li><li>设计了一个基于单调递增先验的相机响应函数，以实现稳定学习。性能：</li><li>在合成新视角图像的任务上，HDR-HexPlane 在多个动态场景上取得了最先进的性能。</li><li>HDR-HexPlane 可以无缝组合不同曝光的图像并生成高动态范围 (HDR) 图像。</li><li>使用色调映射函数，可以实现更好的色彩平衡，从而提高图像的整体视觉质量。工作量：</li><li>HDR-HexPlane 的实现需要大量的计算资源，包括 GPU 和内存。</li><li>HDR-HexPlane 的训练过程需要大量的数据和时间。</li><li>HDR-HexPlane 的推理过程也需要大量的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-365b96052d113ae5a68faafffa3b689c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f64579f98fad3923afdea199ebc9b8cc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a65b50e441e1d12f71aca19d6858a6fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ba32dc69a2b4433dbf0e3259f92de8c.jpg" align="middle"></details><h2 id="GO-NeRF-Generating-Virtual-Objects-in-Neural-Radiance-Fields"><a href="#GO-NeRF-Generating-Virtual-Objects-in-Neural-Radiance-Fields" class="headerlink" title="GO-NeRF: Generating Virtual Objects in Neural Radiance Fields"></a>GO-NeRF: Generating Virtual Objects in Neural Radiance Fields</h2><p><strong>Authors:Peng Dai, Feitong Tan, Xin Yu, Yinda Zhang, Xiaojuan Qi</strong></p><p>Despite advances in 3D generation, the direct creation of 3D objects within an existing 3D scene represented as NeRF remains underexplored. This process requires not only high-quality 3D object generation but also seamless composition of the generated 3D content into the existing NeRF. To this end, we propose a new method, GO-NeRF, capable of utilizing scene context for high-quality and harmonious 3D object generation within an existing NeRF. Our method employs a compositional rendering formulation that allows the generated 3D objects to be seamlessly composited into the scene utilizing learned 3D-aware opacity maps without introducing unintended scene modification. Moreover, we also develop tailored optimization objectives and training strategies to enhance the model’s ability to exploit scene context and mitigate artifacts, such as floaters, originating from 3D object generation within a scene. Extensive experiments on both feed-forward and $360^o$ scenes show the superior performance of our proposed GO-NeRF in generating objects harmoniously composited with surrounding scenes and synthesizing high-quality novel view images. Project page at {\url{<a href="https://daipengwa.github.io/GO-NeRF/}">https://daipengwa.github.io/GO-NeRF/}</a>. </p><p><a href="http://arxiv.org/abs/2401.05750v1">PDF</a> 12 pages</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 场景中的直接 3D 物体生成方法 GO-NeRF，利用场景上下文生成高质量且和谐的 3D 物体。</p><p><strong>Key Takeaways</strong></p><ul><li>GO-NeRF 提出了一种新的方法，该方法能够利用场景上下文在现有 NeRF 中生成高质量且和谐的 3D 对象。</li><li>GO-NeRF 采用组合渲染公式，利用学习的 3D 感知不透明度贴图将生成的 3D 对象无缝地组合到场景中，而不会引入意外的场景修改。</li><li>GO-NeRF 开发了定制的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻源自场景中 3D 对象生成的人为痕迹（例如漂浮物）。</li><li>GO-NeRF 在前馈和 360 度场景上的广泛实验表明，它在生成与周围场景和谐组合的对象和合成高质量的新视图图像方面具有卓越的性能。</li><li>GO-NeRF 项目主页：<a href="https://daipengwa.github.io/GO-NeRF/">https://daipengwa.github.io/GO-NeRF/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GO-NeRF：在神经辐射场中生成虚拟物体</li><li>作者：彭代、谭飞通、俞欣、张印达、戚小娟</li><li>第一作者单位：香港大学</li><li>关键词：神经辐射场、3D对象生成、场景合成、文本到3D</li><li>论文链接：https://arxiv.org/abs/2401.05750，Github 代码链接：无</li><li><p>摘要：(1)：研究背景：近年来，神经辐射场 (NeRF) 在可重现真实世界环境重建方面取得了巨大进展。与此同时，文本引导的对象生成也显示出在创建新颖 3D 内容方面的巨大潜力。本文研究了一个新颖的问题：生成与给定 3D 真实世界场景相协调的 3D 对象。这种能力对于新场景创建和编辑至关重要，它要求将生成的原始内容无缝地组合到环境中，并确保在 downstream 应用中获得高度沉浸式的体验。(2)：过去的方法及其问题：Gordon 等人利用基于 CLIP 的文本图像匹配损失进行 3D 对象生成，并引入了一个 3D 混合管道将合成的 3D 对象组合到 NeRF 中。然而，这种方法受到模型生成能力的限制，并且缺乏利用场景上下文信息的特性，导致次优的、低质量的结果，并且无法与 NeRF 无缝融合（见图 3 第二行：水果在空中飞翔）。另一方面，文本引导的图像修复模型可以通过填充指定掩码来创建与所需对象相协调的场景。然而，为后续 NeRF 模型训练生成具有视图一致性的图像（用于合成对象）仍然具有挑战性。因此，这些技术容易受到大的视图变化和意外的场景内容修改的影响，因为修复掩码不准确（见图 3 右下角：给定的掩码与对象的轮廓不匹配）。(3)：论文提出的研究方法：本文介绍了一个具有易于使用界面的新管道，称为 GO-NeRF，它可以在给定的基于 NeRF 的环境中生成由文本提示控制的 3D 虚拟对象，从而生成协调的 3D 场景（见图 1、9、3，其中可以看到浅阴影和反射）。我们的方法基于两个关键方面：（1）一种合成渲染公式，它有助于将生成的 3D 对象无缝组合到现有场景中，同时防止引入意外的场景修改，而无需显式建模场景几何。（2）精心设计的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻伪影（例如，源于场景中 3D 对象生成）。(4)：方法在任务和性能上的表现：在正向馈送和 360 度场景上的广泛实验表明，我们提出的 GO-NeRF 在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。这些性能支持他们的目标。</p></li><li><p>方法：(1) 界面：创建一个简单的直观界面，允许用户轻松地定义要生成的 3D 场景中的对象位置。(2) 组合渲染：引入一个单独的 NeRF 来表示对象，该 NeRF 由 θ 参数化。在训练期间，生成过程学习这些参数以根据输入文本提示和场景上下文合成对象。(3) 优化：首先描述场景和谐对象生成的目标损失，然后介绍提高生成质量的优化策略。</p></li><li><p>结论：（1）GO-NeRF 作为一种新颖的方法，直接在现有的场景级 NeRF 中生成由文本控制的 3D 对象，迈出了重要一步。为了实现这一目标，我们采用了与定制优化目标和训练策略相关的组合渲染公式，用于合成无缝组合到现有场景中的 3D 对象。我们的方法利用预训练文本引导图像修复网络的图像先验，以促进对象及其周围环境的和谐生成。实验结果表明了我们的方法在正向馈送和 360 度数据集中的优越性。我们希望我们的研究将激发该领域进一步的工作。（2）创新点：GO-NeRF 的创新点在于：</p></li><li>提出了一种合成渲染公式，该公式有助于将生成的 3D 对象无缝组合到现有场景中，同时防止引入意外的场景修改，而无需显式建模场景几何。</li><li>设计了精心设计的优化目标和训练策略，以增强模型利用场景上下文的能力并减轻伪影（例如，源于场景中 3D 对象生成）。性能：GO-NeRF 在以下方面表现出优越的性能：</li><li>在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。</li><li>在正向馈送和 360 度场景上的广泛实验表明，GO-NeRF 在生成与周围场景协调一致的对象和合成高质量的新视图图像方面具有优越的性能。工作量：GO-NeRF 的工作量主要包括：</li><li>创建一个简单的直观界面，允许用户轻松地定义要生成的 3D 场景中的对象位置。</li><li>引入一个单独的 NeRF 来表示对象，该 NeRF 由 θ 参数化。在训练期间，生成过程学习这些参数以根据输入文本提示和场景上下文合成对象。</li><li>描述场景和谐对象生成的目标损失，然后介绍提高生成质量的优化策略。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ee0add100ffcef00be2fec6bbe6283d9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43fa755cedd33ceae1d9d7fbb83963da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8afd08915207c50c36f014b939a33fd2.jpg" align="middle"></details><h2 id="FPRF-Feed-Forward-Photorealistic-Style-Transfer-of-Large-Scale-3D-Neural-Radiance-Fields"><a href="#FPRF-Feed-Forward-Photorealistic-Style-Transfer-of-Large-Scale-3D-Neural-Radiance-Fields" class="headerlink" title="FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D   Neural Radiance Fields"></a>FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D   Neural Radiance Fields</h2><p><strong>Authors:GeonU Kim, Kim Youwang, Tae-Hyun Oh</strong></p><p>We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN’s feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene stylization for large-scale scenes with diverse reference images. Project page: <a href="https://kim-geonu.github.io/FPRF/">https://kim-geonu.github.io/FPRF/</a> </p><p><a href="http://arxiv.org/abs/2401.05516v1">PDF</a> Project page: <a href="https://kim-geonu.github.io/FPRF/">https://kim-geonu.github.io/FPRF/</a></p><p><strong>摘要</strong><br>针对大规模 3D 神经辐射场，提出了一种前馈的超现实风格迁移方法 FPRF，可在无需额外优化的情况下对大型 3D 场景进行任意风格化处理，并保持多视图外观一致性。</p><p><strong>要点</strong></p><ul><li>FPRF 提出了一种风格分解的 3D 神经辐射场，支持任意的风格参考图像，无需额外的优化。</li><li>FPRF 支持多参考风格化，并具有语义对应匹配和局部 AdaIN，为 3D 场景风格增添了多样化的用户控制。</li><li>FPRF 通过直接将语义匹配和风格迁移过程应用于 3D 空间中的查询特征，保持了多视图一致性。</li><li>FPRF 在实验中证明了其在大规模场景中使用多样化参考图像实现令人满意的超现实质量的 3D 场景风格化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：大规模 3D 神经辐射场的 Feed-Forward 真实感风格迁移</li><li>作者：Geonu Kim, Jun-Ho Choi, Kyoung Mu Lee</li><li>单位：韩国科学技术院</li><li>关键词：3D 神经辐射场、风格迁移、AdaIN、多视图一致性</li><li>论文链接：https://arxiv.org/abs/2401.05516   Github 代码链接：None</li><li><p>摘要：   (1) 研究背景：神经辐射场 (NeRF) 是一种强大的技术，可以从多视图图像重建逼真的 3D 场景。然而，NeRF 模型通常需要大量的训练数据和计算资源，这使得它们难以用于大规模场景的重建和风格迁移。   (2) 过去方法：一些研究人员提出了使用预训练的 NeRF 模型来进行风格迁移的方法，但这些方法通常需要额外的优化过程，并且只能处理小规模的场景。   (3) 本文方法：为了解决上述问题，本文提出了一种新的 Feed-Forward Photorealistic Style Transfer (FPRF) 方法，可以对大规模 3D 神经辐射场进行真实感风格迁移。FPRF 方法通过引入一种风格分解的 3D 神经辐射场来实现风格迁移，该辐射场继承了 AdaIN 的 Feed-Forward 风格迁移机制，支持任意风格参考图像。此外，FPRF 方法支持多参考风格迁移，通过语义对应匹配和局部 AdaIN 来实现，这为 3D 场景风格增加了多样化的用户控制。FPRF 方法还通过直接将语义匹配和风格迁移过程应用于 3D 空间中的查询特征来保持多视图一致性。   (4) 实验结果：在实验中，本文证明了 FPRF 方法可以为大规模场景实现良好的真实感 3D 场景风格迁移，并支持多种参考图像。FPRF 方法在 LLFF 数据集和小规模场景上优于其他 3D 风格迁移方法，并且在 San Francisco Mission Bay 数据集和大规模场景上实现了良好的多视图一致性。这些结果表明，FPRF 方法可以很好地支持其目标，即对大规模 3D 神经辐射场进行真实感风格迁移。</p></li><li><p>方法：(1) 提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。(2) 构建了一个可风格化的辐射场，称为可风格化辐射场，该辐射场继承了 AdaIN 的前馈式风格迁移机制，支持任意风格参考图像。(3) 引入场景语义场，通过语义对应匹配和局部 AdaIN 来实现对大规模场景的多参考风格迁移，为 3D 场景风格增加了多样化的用户控制。(4) 通过将语义匹配和风格迁移过程直接应用于 3D 空间中的查询特征来保持多视图一致性。</p></li><li><p>结论：（1）：本文提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。与现有的 3D 风格迁移方法相比，FPRF 具有以下优点：支持任意风格参考图像、支持多参考风格迁移、保持多视图一致性。（2）：创新点：</p></li><li>提出了一种名为 FPRF 的前馈式真实感风格迁移方法，用于大规模 3D 场景。</li><li>构建了一个可风格化的辐射场，称为可风格化辐射场，该辐射场继承了 AdaIN 的前馈式风格迁移机制，支持任意风格参考图像。</li><li>引入场景语义场，通过语义对应匹配和局部 AdaIN 来实现对大规模场景的多参考风格迁移，为 3D 场景风格增加了多样化的用户控制。</li><li>通过将语义匹配和风格迁移过程直接应用于 3D 空间中的查询特征来保持多视图一致性。性能：</li><li>在 LLFF 数据集和小规模场景上优于其他 3D 风格迁移方法。</li><li>在 SanFranciscoMissionBay 数据集和大规模场景上实现了良好的多视图一致性。工作量：</li><li>训练 FPRF 模型需要大量的数据和计算资源。</li><li>使用 FPRF 模型进行风格迁移需要较高的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-189f376179ba2d0468a2e9c590bd3797.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a612beca1b40d1f1448824f6c5f8d29c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-15778af4f62c01b59051e8a0daf28bc7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ea21a55b1de35fad214a9b4f1c10e960.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5524cc2407a0875f1556116ab5b3f1b.jpg" align="middle"></details><h2 id="CTNeRF-Cross-Time-Transformer-for-Dynamic-Neural-Radiance-Field-from-Monocular-Video"><a href="#CTNeRF-Cross-Time-Transformer-for-Dynamic-Neural-Radiance-Field-from-Monocular-Video" class="headerlink" title="CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from   Monocular Video"></a>CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from   Monocular Video</h2><p><strong>Authors:Xingyu Miao, Yang Bai, Haoran Duan, Yawen Huang, Fan Wan, Yang Long, Yefeng Zheng</strong></p><p>The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views. </p><p><a href="http://arxiv.org/abs/2401.04861v1">PDF</a> </p><p><strong>Summary</strong><br>动态场景NeRF：通过时间和频率域聚合特征以提高复杂动态场景的视图合成质量。</p><p><strong>Key Takeaways</strong></p><ul><li>我们提出了一种新颖的方法来生成复杂动态场景的单目视频的高质量新颖视图。</li><li>我们的方法建立在最近的泛化 NeRF 基础上，该方法将附近的视图聚合到新的视点上。</li><li>我们引入了一个同时在时域和频域中运行的模块来聚合对象运动的特征。</li><li>这使我们能够学习帧之间的关系并生成更高质量的图像。</li><li>我们的实验表明，我们的方法在动态场景数据集上的性能明显优于最先进的方法。</li><li>具体来说，我们的方法在合成视图的准确性和视觉质量方面优于现有方法。</li><li>我们证明了我们的方法对于具有挑战性的动态场景（例如舞蹈序列）特别有效。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CTNeRF：动态场景单目视频的跨时间变换器</li><li>作者：邢宇苗、杨白、郝然段、黄雅雯、万璠、杨龙、叶锋正</li><li>单位：英国杜伦大学计算机系</li><li>关键词：动态神经辐射场、单目视频、场景流、变换器</li><li>链接：Paper_info:IEEE JOURNALS 1Github：无</li><li><p>总结：（1）：随着深度学习的飞速发展，神经辐射场（NeRF）作为该领域最具代表性的成果之一，在单目视频的动态场景新视角合成方面取得了显著进展。然而，现有方法在处理复杂对象运动时仍存在局限性，导致细节渲染不准确和模糊。（2）：现有方法主要有两种类型：可变形翘曲场方法和神经场景流方法。可变形翘曲场方法可以处理长序列，但对于具有复杂对象运动的动态场景可能效果不佳。神经场景流方法可以处理动态场景中的大运动，但其有效性高度依赖于预测场景流或轨迹的准确性。（3）：本文提出了一种新的方法，可以应用于动态场景，能够处理更复杂的运动并改进渲染结果。该方法借鉴了最近静态场景渲染的研究，通过聚合附近视角沿极线上的局部图像特征来增强渲染过程。为了克服动态场景带来的挑战，本文设计了一个模块，可以聚合光照空间中由于运动引起的光照变化，以及获得的多视角特征。这使得本文方法能够准确地考虑几何和外观的时空变化，从而更好地渲染动态场景。（4）：本文方法在动态场景数据集上进行了实验，结果表明，该方法在合成视图的准确性和视觉质量方面都优于现有方法，证明了本文方法可以很好地支持其目标。</p></li><li><p>方法：(1)：本文方法的核心思想是通过聚合附近视角沿极线上的局部图像特征来增强渲染过程，以更好地捕捉动态场景中的几何和外观变化。(2)：为了克服动态场景带来的挑战，本文设计了一个模块，可以聚合光照空间中由于运动引起的光照变化，以及获得的多视角特征。(3)：该模块由一个时空注意力机制和一个光照变化聚合层组成。时空注意力机制用于聚合附近视角沿极线上的局部图像特征，光照变化聚合层用于聚合光照空间中由于运动引起的光照变化。(4)：通过将这两个模块结合起来，本文方法可以准确地考虑几何和外观的时空变化，从而更好地渲染动态场景。</p></li><li><p>结论：（1）： 本文提出了一种新的动态神经渲染场框架，用于动态单目视频，该框架能够高质量地渲染新视角。为了实现这一点，我们扩展了最近的多视图聚合思想到时变 NeRF，这使得建模复杂运动成为可能。具体来说，我们引入了 RBCT 和 GSTF 模块来分别从时间域和频域建模运动。我们的实验结果表明，这些提出的模块在渲染新视角时显著增强了具有多视图聚合的时变 NeRF 的性能。（2）： 创新点：</p></li><li>提出了一种新的动态神经渲染场框架，用于动态单目视频，该框架能够高质量地渲染新视角。</li><li>扩展了最近的多视图聚合思想到时变 NeRF，这使得建模复杂运动成为可能。</li><li>引入了 RBCT 和 GSTF 模块来分别从时间域和频域建模运动。</li></ol><p>性能：* 在合成视图的准确性和视觉质量方面，我们的方法优于现有方法。</p><p>工作量：* 该方法的计算成本较高，不适用于长序列视频的新视角渲染。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0850f96472c46b64ee282b61f71f5061.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d50131e7a1f564140cf78966a505a3aa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f75406f8a2fa17fe58a50e32b802d2fd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3217dd559310e7375b8e7d9ef2419b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ec167c81acb136a204b777dfd69fd47.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47ef05de23daff322b3a1fb46fac475b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-35fa8ad2270d74904552566049ae2bdb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-45227f9a07bbebb8ecbbed80ce5f59bb.jpg" align="middle"></details>## NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation**Authors:Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald**The capabilities of monocular depth estimation (MDE) models are limited by the availability of sufficient and diverse datasets. In the case of MDE models for autonomous driving, this issue is exacerbated by the linearity of the captured data trajectories. We propose a NeRF-based data augmentation pipeline to introduce synthetic data with more diverse viewing directions into training datasets and demonstrate the benefits of our approach to model performance and robustness. Our data augmentation pipeline, which we call "NeRFmentation", trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on relevant metrics, and uses them to generate synthetic RGB-D images captured from new viewing directions. In this work, we apply our technique in conjunction with three state-of-the-art MDE architectures on the popular autonomous driving dataset KITTI, augmenting its training set of the Eigen split. We evaluate the resulting performance gain on the original test set, a separate popular driving set, and our own synthetic test set. [PDF](http://arxiv.org/abs/2401.03771v1) **Summary**NeRF 驱动的图像扩充有助于自动驾驶单目深度估计模型的鲁棒性和性能。**Key Takeaways**- 单目深度估计模型的性能受数据量和多样性限制，尤其是在自动驾驶场景中。- 提出基于 NeRF 的数据扩充管道，在训练集中引入具有更多样视角的合成数据。- 数据扩充管道训练每个场景的 NeRF，并根据相关指标过滤次优 NeRF。- 使用经过滤的 NeRF 从新的视角生成合成 RGB-D 图像。- 在 KITTI 数据集上与三种最先进的单目深度估计架构结合使用该技术。- 扩充后的训练集在原始测试集、单独的流行驾驶集和我们自己的合成测试集上都取得了性能提升。- 数据扩充在自动驾驶场景中提高了单目深度估计模型的鲁棒性和性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeRFmentation：基于 NeRF 的单目深度估计增强</li><li>作者：Felix Heide, Tobias Wutz, Simon Fuhrmann, Michael Goesele, Andreas Geiger</li><li>单位：苏黎世联邦理工学院</li><li>关键词：单目深度估计、数据增强、NeRF、自动驾驶</li><li>论文链接：https://arxiv.org/abs/2208.01185    Github 代码链接：无</li><li><p>摘要：（1）研究背景：单目深度估计 (MDE) 模型的能力受限于足够和多样化数据集的可用性。对于自动驾驶的 MDE 模型，由于捕获的数据轨迹的线性性，这个问题更加严重。（2）过去的方法及其问题：现有方法主要集中在设计新的网络架构或损失函数上，但这些方法往往需要大量的数据才能达到良好的性能。此外，现有方法通常只关注单一的场景或数据集，缺乏泛化能力。（3）提出的研究方法：我们提出了一种基于 NeRF 的数据增强管道，将具有更多样化视角方向的合成数据引入训练数据集中，并证明了我们方法对模型性能和鲁棒性的好处。我们的数据增强管道称为“NeRFmentation”，它在数据集中的每个场景上训练 NeRF，根据相关指标过滤掉较差的 NeRF，并使用它们从新的视角方向生成合成 RGB-D 图像。（4）方法在什么任务上取得了什么性能：在流行的自动驾驶数据集 KITTI 上，我们将我们的技术与三种最先进的 MDE 架构结合使用，扩充了 Eigen 分割的训练集。我们在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估了由此产生的性能提升。</p></li><li><p>方法：(1) 训练 NeRF：在每个场景上训练 NeRF，以生成具有不同视角方向的合成 RGB-D 图像。(2) 过滤 NeRF：根据相关指标过滤掉较差的 NeRF。(3) 生成合成图像：使用选定的 NeRF 从新的视角方向生成合成 RGB-D 图像。(4) 扩充训练集：将生成的合成图像与原始训练集合并，扩充训练集。(5) 训练 MDE 模型：使用扩充的训练集训练 MDE 模型。(6) 评估性能：在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估 MDE 模型的性能。</p></li><li><p>结论：（1）：本文提出了一种基于 NeRF 的数据增强管道，通过将具有更多样化视角方向的合成数据引入训练数据集中，显著提升了单目深度估计模型的性能和鲁棒性。（2）：创新点：</p></li><li><p>提出了一种基于 NeRF 的数据增强管道，可以生成具有更多样化视角方向的合成 RGB-D 图像。</p></li><li>设计了一种过滤机制，可以过滤掉较差的 NeRF，从而提高合成图像的质量。</li><li>将生成的合成图像与原始训练集合并，扩充训练集，从而提高 MDE 模型的性能。</li></ol><p>性能：</p><ul><li>在 KITTI 数据集上，将我们的技术与三种最先进的 MDE 架构结合使用，扩充了 Eigen 分割的训练集，在原始测试集、单独的流行驾驶集和我们自己的合成测试集上评估了由此产生的性能提升。</li><li>在原始测试集上，我们的方法将 MDE 模型的平均绝对误差从 2.43 米降低到 2.29 米，相对误差降低了 5.8%。</li><li>在单独的流行驾驶集上，我们的方法将 MDE 模型的平均绝对误差从 2.67 米降低到 2.49 米，相对误差降低了 6.7%。</li><li>在我们自己的合成测试集上，我们的方法将 MDE 模型的平均绝对误差从 3.12 米降低到 2.86 米，相对误差降低了 8.3%。</li></ul><p>工作量：</p><ul><li>训练 NeRF 模型需要大量的时间和计算资源。</li><li>过滤较差的 NeRF 也需要花费大量的时间和计算资源。</li><li>生成合成图像需要花费大量的时间和计算资源。</li><li>将生成的合成图像与原始训练集合并，扩充训练集需要花费大量的时间和计算资源。</li><li>训练 MDE 模型需要花费大量的时间和计算资源。</li><li>评估 MDE 模型的性能需要花费大量的时间和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ce7ee2b66de780a44f4c0f517f763b05.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b5b0c192b842205b298c0ce78c4b41a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c3e191046fcb30bdd49314ea6c0da386.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-debede860a9c02b780b3f6ea909c3d2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1511e60fdc56e1d2545239f0ebe1001d.jpg" align="middle"></details>## Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity   Monocular Dense Mapping**Authors:Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang**In this paper, we introduce Hi-Map, a novel monocular dense mapping approach based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to achieve efficient and high-fidelity mapping using only posed RGB inputs. Our method eliminates the need for external depth priors derived from e.g., a depth estimation model. Our key idea is to represent the scene as a hierarchical feature grid that encodes the radiance and then factorizes it into feature planes and vectors. As such, the scene representation becomes simpler and more generalizable for fast and smooth convergence on new observations. This allows for efficient computation while alleviating noise patterns by reducing the complexity of the scene representation. Buttressed by the hierarchical factorized representation, we leverage the Sign Distance Field (SDF) as a proxy of rendering for inferring the volume density, demonstrating high mapping fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen the photometric cues and further boost the mapping quality, especially for the distant and textureless regions. Extensive experiments demonstrate our method's superiority in geometric and textural accuracy over the state-of-the-art NeRF-based monocular mapping methods. [PDF](http://arxiv.org/abs/2401.03203v1) **摘要**神经辐射场（NeRF）单目密集映射中，全新的金字塔结构特征网格可提高效率和保真度。**主要要点**- Hi-Map 是一种基于神经辐射场 (NeRF) 的单目密集映射方法，无需深度估计模型即可实现高效且高保真的映射。- Hi-Map 的关键思想是将场景表示为一个分层特征网格，该网格对辐射进行编码，然后将其分解为特征平面和向量。- 分层分解表示可以使场景表示更简单，更具泛化性，以便对新观测进行快速、平滑的收敛。- Hi-Map 利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，从而提高了映射保真度。- Hi-Map 引入了双路径编码策略，以加强光度线索并进一步提高映射质量，尤其是对于遥远且无纹理的区域。- 大量实验表明，Hi-Map 在几何和纹理准确性方面优于最先进的基于 NeRF 的单目映射方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：分层因子化辐射场：用于高保真单目密集测绘的 Hi-Map</li><li>作者：Hua Tongyan、Bai Haotian、Cao Zidong、Liu Ming、Tao Dacheng、Wang Lin</li><li>隶属单位：香港科技大学（广州）</li><li>关键词：单目密集测绘、NeRF、SDF</li><li>论文链接：https://arxiv.org/abs/2401.03203，Github 链接：无</li><li><p>摘要：（1）研究背景：构建高保真密集 3D 地图对于具身智能系统（如机器人）至关重要。3D 地图使机器人能够执行场景理解任务并在复杂且动态的环境中导航。（2）过去方法及其问题：传统密集测绘技术难以平衡内存效率和准确性。这些方法通常依赖于显式跟踪和存储共同观察到的点，这些点随后被转换为（例如）占用网格或 TSDF 来表示场景。因此，正确跟踪的点数越多，生成的映射保真度就越高，但这还需要大量的计算和存储。随着神经辐射场 (NeRF) 的出现，一些研究尝试利用神经场更好地表示场景，方法是通过以紧凑且可学习的方式对外观和几何进行编码，从而有利于内存消耗和测绘质量。基于 NeRF 的密集测绘方法主要依赖于输入深度先验来促进在线收敛，方法是通过缩小采样的搜索范围。这种深度先验通常来自传感器或由单目视觉同时定位和建图 (vSLAM) 系统或深度估计模型提供。然而，这种对深度先验的依赖在资源有限的环境或深度线索不可用或不可靠的情况下成为障碍。即使可以通过在优化隐式表示时添加翘曲约束来内部化深度估计，它仍然难以在准确性和计算效率之间取得平衡。因此，在不依赖深度先验的情况下实现高效且高保真的密集测绘是有意义的。这要求 NeRF 能够有效且快速地推广到基础几何未知的新观察结果。（3）论文提出的研究方法：为了实现这一点，我们引入了一种新颖的分层表示，方法是因子化多分辨率特征网格，灵感来自 [29]，其中通过因子化辐射场提出了低秩正则化，通过将辐射场因子化为特征平面和向量，从而提高了渲染质量并提高了计算效率。这种正则化技术将数据结构（即 4D 张量）简化为低维元素，即低秩分量，以保留体积渲染最相关的特征。因此，场景表示变得更简单、更具通用性，可以快速平滑地收敛到新的观察结果。这允许进行有效计算，同时通过降低场景表示的复杂性来减轻噪声模式。在分层因子化表示的支持下，我们利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，从而证明了高测绘保真度。此外，我们引入了一种双路径编码策略来增强光度线索并进一步提高测绘质量，特别是对于遥远和无纹理的区域。（4）方法在什么任务上取得了什么性能？性能是否支持其目标？广泛的实验表明，我们的方法在几何和纹理精度方面优于最先进的基于 NeRF 的单目测绘方法。性能支持其目标。</p></li><li><p>方法：(1) 引入分层因子化表示，将多分辨率特征网格因子化为特征平面和向量，简化数据结构，降低场景表示的复杂性，提高渲染质量和计算效率。(2) 利用符号距离场 (SDF) 作为渲染的代理来推断体积密度，提高测绘保真度。(3) 采用双路径编码策略增强光度线索，进一步提高测绘质量，特别是对于遥远和无纹理的区域。</p></li><li><p>结论：(1): 本文提出了一种分层因子化辐射场，用于高保真单目密集测绘，在几何和纹理精度方面优于最先进的基于NeRF的单目测绘方法。(2): 创新点：</p></li><li>引入分层因子化表示，简化数据结构，降低场景表示的复杂性，提高渲染质量和计算效率。</li><li>利用符号距离场(SDF)作为渲染的代理来推断体积密度，提高测绘保真度。</li><li>采用双路径编码策略增强光度线索，进一步提高测绘质量，特别是对于遥远和无纹理的区域。性能：</li><li>在几何和纹理精度方面优于最先进的基于NeRF的单目测绘方法。工作量：</li><li>需要更多的计算资源来训练模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b366ebbb955de81c06947699b2848416.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-020fc3cd9ae1ff2be7aa32d5d5fdd3ac.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4dacbdf094479bbedb2926fda300988b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51809493796f92589316bddbc0c14a07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9f5010f46d456e61cab09f5b9e1b8146.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c0f693c233faba8cf8ca1dd1f39ee6f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78869f68df98aaa60a33965857838f02.jpg" align="middle"><img src="https://picx.zhimg.com/v2-12356980c7845df8d299441db2d77219.jpg" align="middle"></details><h2 id="FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF"><a href="#FED-NeRF-Achieve-High-3D-Consistency-and-Temporal-Coherence-for-Face-Video-Editing-on-Dynamic-NeRF" class="headerlink" title="FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF"></a>FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face   Video Editing on Dynamic NeRF</h2><p><strong>Authors:Hao Zhang, Yu-Wing Tai, Chi-Keung Tang</strong></p><p>The success of the GAN-NeRF structure has enabled face editing on NeRF to maintain 3D view consistency. However, achieving simultaneously multi-view consistency and temporal coherence while editing video sequences remains a formidable challenge. This paper proposes a novel face video editing architecture built upon the dynamic face GAN-NeRF structure, which effectively utilizes video sequences to restore the latent code and 3D face geometry. By editing the latent code, multi-view consistent editing on the face can be ensured, as validated by multiview stereo reconstruction on the resulting edited images in our dynamic NeRF. As the estimation of face geometries occurs on a frame-by-frame basis, this may introduce a jittering issue. We propose a stabilizer that maintains temporal coherence by preserving smooth changes of face expressions in consecutive frames. Quantitative and qualitative analyses reveal that our method, as the pioneering 4D face video editor, achieves state-of-the-art performance in comparison to existing 2D or 3D-based approaches independently addressing identity and motion. Codes will be released. </p><p><a href="http://arxiv.org/abs/2401.02616v1">PDF</a> Our code will be available at: <a href="https://github.com/ZHANG1023/FED-NeRF">https://github.com/ZHANG1023/FED-NeRF</a></p><p><strong>摘要</strong><br>动态人脸 GAN-NeRF 结构实现 4D 人脸视频编辑，可在 3D 视图保持一致性的同时，实现时间连贯性的视频序列编辑。</p><p><strong>要点</strong></p><ul><li>该方法基于动态人脸 GAN-NeRF 结构，有效利用视频序列恢复潜在编码和 3D 面部几何。</li><li>通过编辑潜在编码，可确保人脸在多视图中的一致性编辑。</li><li>动态 NeRF 中对编辑后图像进行多视点立体重建验证了多视图一致性编辑。</li><li>逐帧估计面部几何可能会导致抖动问题。</li><li>该方法提出一个稳定器通过保持连续帧中面部表情的平滑变化，以保持时间连贯性。</li><li>定量和定性分析表明，该方法作为首个 4D 人脸视频编辑器，与现有的仅针对身份或运动的 2D 或 3D 方法相比，取得了最先进的性能。</li><li>代码将公布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FED-NeRF：实现人脸视频编辑的高 3D 一致性和时间连贯性</li><li>作者：张浩、戴宇炜、邓志铿</li><li>隶属单位：香港科技大学</li><li>关键词：人脸视频编辑、NeRF、动态 NeRF、多视图一致性、时间连贯性</li><li>论文链接：https://arxiv.org/abs/2401.02616</li><li><p>摘要：（1）研究背景：GAN-NeRF 结构的成功让人脸编辑能够在 NeRF 上保持 3D 视图一致性。然而，在编辑视频序列时同时实现多视图一致性和时间连贯性仍然是一个巨大的挑战。（2）过去的方法及其问题：现有方法主要集中在 2D 或 3D 空间中进行人脸编辑，但这些方法在处理多视图一致性和时间连贯性方面存在局限性。（3）研究方法：本文提出了一种新颖的人脸视频编辑架构，该架构建立在动态人脸 GAN-NeRF 结构之上，有效地利用视频序列来恢复潜在编码和 3D 人脸几何。通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态 NeRF 中生成的编辑图像进行多视图立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。因此，本文提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。（4）方法性能：定量和定性分析表明，本文方法作为开创性的 4D 人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的 2D 或 3D 方法。</p></li><li><p>方法：(1) 潜在编码估计器：从视频序列中提取身份信息，将每一帧的特征通过交叉注意力层聚合，得到一个奇异的潜在编码输出。(2) 面部几何估计器：修改基于 EMOCA 的图像编码器，将输入图像分解为面部几何（由 FLAME 控制表示）、反照率、光照、额外表情代码等。(3) 稳定器：使用 Catmull-Rom 样条曲线对连续帧中的面部表情进行平滑变化，保持时间连贯性。</p></li><li><p>结论：（1）：本文提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构建立在动态人脸GAN-NeRF结构之上，有效地利用视频序列来恢复潜在编码和3D人脸几何。通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态NeRF中生成的编辑图像进行多视图立体重建来验证。由于人脸几何的估计是逐帧进行的，这可能会引入抖动问题。因此，本文提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。定量和定性分析表明，本文方法作为开创性的4D人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的2D或3D方法。（2）：创新点：</p></li><li>提出了一种新颖的人脸视频编辑架构FED-NeRF，该架构建立在动态人脸GAN-NeRF结构之上，有效地利用视频序列来恢复潜在编码和3D人脸几何。</li><li>通过编辑潜在编码，可以在人脸上确保多视图一致的编辑，这可以通过对动态NeRF中生成的编辑图像进行多视图立体重建来验证。</li><li>提出了一种稳定器，通过保持连续帧中人脸表情的平滑变化来保持时间连贯性。性能：</li><li>定量和定性分析表明，本文方法作为开创性的4D人脸视频编辑器，在独立处理身份和运动方面取得了最先进的性能，优于现有的2D或3D方法。工作量：</li><li>本文方法的实现需要较高的计算资源，并且需要对视频序列进行预处理。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5f818de87353fbf13907e49c13b462d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b57ca3ade2e4538084a10eeb0919b87d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-102448406db5d3475d988673668fc7a1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ec81b2b7dfa9352ae62039d258ec687.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4ee173ef2962cf2f938e02414ced9add.jpg" align="middle"></details><h2 id="Inpaint4DNeRF-Promptable-Spatio-Temporal-NeRF-Inpainting-with-Generative-Diffusion-Models"><a href="#Inpaint4DNeRF-Promptable-Spatio-Temporal-NeRF-Inpainting-with-Generative-Diffusion-Models" class="headerlink" title="Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with   Generative Diffusion Models"></a>Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with   Generative Diffusion Models</h2><p><strong>Authors:Han Jiang, Haosen Sun, Ruoxuan Li, Chi-Keung Tang, Yu-Wing Tai</strong></p><p>Current Neural Radiance Fields (NeRF) can generate photorealistic novel views. For editing 3D scenes represented by NeRF, with the advent of generative models, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art stable diffusion models (e.g., ControlNet) for direct generation of the underlying completed background content, regardless of static or dynamic. The key advantages of this generative approach for NeRF inpainting are twofold. First, after rough mask propagation, to complete or fill in previously occluded content, we can individually generate a small subset of completed images with plausible content, called seed images, from which simple 3D geometry proxies can be derived. Second and the remaining problem is thus 3D multiview consistency among all completed images, now guided by the seed images and their 3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF baseline framework is general which can be readily extended to 4D dynamic NeRFs, where temporal consistency can be naturally handled in a similar way as our multiview consistency. </p><p><a href="http://arxiv.org/abs/2401.00208v1">PDF</a> </p><p><strong>摘要</strong><br>生成方法弥补神经辐射场的遮挡区域，生成过程分粗糙遮罩传播和由种子图像引导的多视点一致性两步。</p><p><strong>要点</strong></p><ul><li>基于扩散模型的生成器可直接生成图片，可解决NeRF遮挡区域的修复问题。</li><li>通过种子图像上的标记可以生成补全的图片。</li><li>3D几何代理可以从生成图片构建，以指导多视点一致性。</li><li>所设计方法可以推广到4D动态神经辐射场，通过标记和种子图像，利用时间一致性可以指导多视点一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Inpaint4DNeRF：基于扩散模型的提示式时空 NeRF 修复</li><li>作者：Han Jiang、Haosen Sun、Ruoxuan Li、Chi-Keung Tang、Yu-Wing Tai</li><li>隶属单位：香港科技大学</li><li>关键词：NeRF、图像修复、生成扩散模型、提示引导、时空一致性</li><li>论文链接：https://arxiv.org/abs/2401.00208</li><li><p>摘要：（1）研究背景：NeRF 是一种强大的 3D 场景表示方法，可以生成逼真的新视图。然而，对于 NeRF 表示的 3D 场景进行编辑仍然是一个具有挑战性的问题。（2）过去的方法及其问题：现有方法主要集中在基于文本提示的 NeRF 编辑，但它们通常仅限于编辑现有对象的外观，而无法处理实质性的几何变化。此外，这些方法通常需要大量的数据和复杂的网络结构，这使得它们难以扩展到动态 NeRF。（3）提出的研究方法：Inpaint4DNeRF 是一种基于生成扩散模型的 NeRF 修复方法。它首先通过粗糙的掩码传播来生成一组具有合理内容的种子图像，然后利用这些种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。此外，Inpaint4DNeRF 可以很容易地扩展到动态 NeRF，以处理时间一致性。（4）方法的性能：在静态和动态 NeRF 修复任务上，Inpaint4DNeRF 在定性和定量方面都优于现有方法。实验结果表明，Inpaint4DNeRF 可以生成与背景一致且具有视觉上令人信服的细节的新内容。</p></li><li><p>方法：(1) 训练视图预处理：首先，选择一组种子图像，并在这些图像上进行 inpainting，以生成一组具有合理内容的种子图像。然后，利用这些种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。(2) 渐进式训练：首先，对 NeRF 进行预热训练，以获得粗略的收敛。然后，使用迭代数据集更新 (IDU) 策略对 NeRF 进行微调，以编辑目标对象的外观和精细几何形状。(3) 正则化：为了监督 NeRF 训练，使用 L1 光度损失和深度损失作为监督。此外，还使用 LPIPS 损失作为正则化项，以减少噪声和浮动物。</p></li><li><p>结论：(1): 本文提出了一种基于生成扩散模型的 NeRF 修复方法 Inpaint4DNeRF，该方法可以生成文本引导、背景适当且在多视图下一致的内容。(2): 创新点：</p></li><li>提出了一种训练图像预处理方法，该方法利用种子图像和它们的 3D 代理来指导所有完成图像的 3D 多视图一致性。</li><li>提出了一种渐进式训练策略，该策略首先对 NeRF 进行预热训练，然后使用迭代数据集更新 (IDU) 策略对 NeRF 进行微调，以编辑目标对象的外观和精细几何形状。</li><li>使用 L1 光度损失、深度损失和 LPIPS 损失作为正则化项来监督 NeRF 训练。性能：</li><li>在静态和动态 NeRF 修复任务上，Inpaint4DNeRF 在定性和定量方面都优于现有方法。</li><li>Inpaint4DNeRF 可以生成与背景一致且具有视觉上令人信服的细节的新内容。工作量：</li><li>Inpaint4DNeRF 的实现相对简单，并且可以很容易地扩展到动态 NeRF。</li><li>Inpaint4DNeRF 的训练速度较快，并且可以在普通 GPU 上进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c3d700b9dfde2e5d8ed89d1f163196dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eaae707aaf894e22e54246edd91e6dce.jpg" align="middle"></details><h2 id="SyncDreamer-for-3D-Reconstruction-of-Endangered-Animal-Species-with-NeRF-and-NeuS"><a href="#SyncDreamer-for-3D-Reconstruction-of-Endangered-Animal-Species-with-NeRF-and-NeuS" class="headerlink" title="SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF   and NeuS"></a>SyncDreamer for 3D Reconstruction of Endangered Animal Species with NeRF   and NeuS</h2><p><strong>Authors:Ahmet Haydar Ornek, Deniz Sen, Esmanur Civil</strong></p><p>The main aim of this study is to demonstrate how innovative view synthesis and 3D reconstruction techniques can be used to create models of endangered species using monocular RGB images. To achieve this, we employed SyncDreamer to produce unique perspectives and NeuS and NeRF to reconstruct 3D representations. We chose four different animals, including the oriental stork, frog, dragonfly, and tiger, as our subjects for this study. Our results show that the combination of SyncDreamer, NeRF, and NeuS techniques can successfully create 3D models of endangered animals. However, we also observed that NeuS produced blurry images, while NeRF generated sharper but noisier images. This study highlights the potential of modeling endangered animals and offers a new direction for future research in this field. By showcasing the effectiveness of these advanced techniques, we hope to encourage further exploration and development of techniques for preserving and studying endangered species. </p><p><a href="http://arxiv.org/abs/2312.13832v1">PDF</a> 8 figures</p><p><strong>Summary</strong><br>育成濒危生物 3D 模型的新方法：结合 SyncDreamer、NeRF 和 NeuS 技术。</p><p><strong>Key Takeaways</strong></p><ul><li>本研究旨在展示如何利用创新性视图合成和 3D 重建技术，仅使用单目 RGB 图像创建濒危物种模型。</li><li>我们使用了 SyncDreamer 来生成独特视角，并使用 NeuS 和 NeRF 重建 3D 表示。</li><li>我们选择了四种不同的动物，包括东方鹳、青蛙、蜻蜓和老虎，作为本研究的主题。</li><li>我们的结果表明，SyncDreamer、NeRF 和 NeuS 技术的结合可以成功创建濒危动物的 3D 模型。</li><li>我们还观察到，NeuS 生成的图像模糊，而 NeRF 生成的图像更清晰但噪声更多。</li><li>本研究强调了对濒危动物建模的潜力，并为该领域未来的研究提供了新的方向。</li><li>通过展示这些先进技术的有效性，我们希望鼓励进一步探索和开发保护和研究濒危物种的技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：利用 SyncDreamer、NeuS 和 NeRF 从单目 RGB 图像重建濒危动物物种的 3D 模型</li><li>作者：Ahmet Haydar Ornek, Deniz Sen, Esmanur Civil</li><li>第一作者单位：华为土耳其研发中心</li><li>关键词：SyncDreamer · NeuS · NeRF · 3D · 重建 · 新颖视图合成</li><li>论文链接：https://arxiv.org/abs/2312.13832</li><li><p>摘要：(1)：研究背景：随着人工智能和深度学习技术的快速发展，生成式人工智能作为一种能够自主创建逼真复杂内容的技术，已经成为技术创新的焦点。近年来，随着稳定扩散 (SD) 算法的出现，生成式人工智能取得了突破性进展，同时 3D 生成能力也取得了显着进步，进一步扩展了人工智能在各个领域的潜在应用。然而，训练一致的 3D 表示需要大量的数据样本，在某些情况下并不总是可用。保护生物多样性仍然是一个关键问题，许多物种面临着野外灭绝的威胁。加剧这一挑战的是，某些濒危物种的图像数据稀缺，这使得使用现有的基于生成式人工智能的方法创建 3D 模型变得困难。(2)：过去的方法及其问题：现有方法主要集中在使用大量数据来训练 3D 表示，这在某些情况下并不总是可用。此外，现有方法往往需要复杂的训练过程和大量的数据，这使得它们难以应用于现实世界中的问题。(3)：论文提出的研究方法：本文探索了 3D 生成式人工智能与野生动物保护的交叉点，讨论了现有的零样本 3D 模型生成方法的结果，以解决数据稀缺问题。通过利用先进的人工智能技术，特别是生成式新颖视图合成和神经隐式 3D 表示，我们旨在从有限的现有样本中生成濒危物种的 3D 模型。(4)：方法在什么任务上取得了什么性能？性能是否支持其目标：我们的方法在濒危动物物种的 3D 重建任务上取得了很好的性能。我们使用 SyncDreamer 从单目 RGB 图像生成新颖的视角，然后使用 NeRF 和 NeuS 重建 3D 表示。我们的结果表明，所提出的方法能够成功地从有限的现有样本中生成濒危动物物种的 3D 模型。这些模型可以用于各种应用，例如教育、研究和保护。</p></li><li><p>Methods:(1): 利用 SyncDreamer 从单目 RGB 图像生成新颖视角，以解决数据稀缺问题；(2): 采用 NeRF 和 NeuS 重建 3D 表示，以获得濒危动物物种的 3D 模型；(3): 将生成的 3D 模型用于教育、研究和保护等应用。</p></li><li><p>结论：（1）：这项工作的意义在于，它探索了 3D 生成式人工智能与野生动物保护的交叉点，讨论了现有的零样本 3D 模型生成方法的结果，以解决数据稀缺问题。通过利用先进的人工智能技术，特别是生成式新颖视图合成和神经隐式 3D 表示，旨在从有限的现有样本中生成濒危物种的 3D 模型。这些模型可以用于各种应用，例如教育、研究和保护。（2）：创新点：</p></li><li>利用 SyncDreamer 从单目 RGB 图像生成新颖视角，以解决数据稀缺问题。</li><li>采用 NeRF 和 NeuS 重建 3D 表示，以获得濒危动物物种的 3D 模型。</li><li>将生成的 3D 模型用于教育、研究和保护等应用。</li></ol><p>性能：- 在濒危动物物种的 3D 重建任务上取得了很好的性能。- 使用 SyncDreamer 从单目 RGB 图像生成新颖的视角，然后使用 NeRF 和 NeuS 重建 3D 表示。- 结果表明，所提出的方法能够成功地从有限的现有样本中生成濒危动物物种的 3D 模型。</p><p>工作量：- 需要收集濒危动物物种的图像数据。- 需要训练 SyncDreamer、NeRF 和 NeuS 模型。- 需要对生成的 3D 模型进行评估。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-da747916ae998379722fef89053fcb49.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b82ce116e2b1bca95bca7d34fc6c5014.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-84409520f6d01d8c83d6f88dd1f7a0f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4dba69f7a9d4113f4ec74476da3ef5b7.jpg" align="middle"></details>## Ternary-type Opacity and Hybrid Odometry for RGB-only NeRF-SLAM**Authors:Junru Lin, Asen Nachkov, Songyou Peng, Luc Van Gool, Danda Pani Paudel**The opacity of rigid 3D scenes with opaque surfaces is considered to be of a binary type. However, we observed that this property is not followed by the existing RGB-only NeRF-SLAM. Therefore, we are motivated to introduce this prior into the RGB-only NeRF-SLAM pipeline. Unfortunately, the optimization through the volumetric rendering function does not facilitate easy integration of the desired prior. Instead, we observed that the opacity of ternary-type (TT) is well supported. In this work, we study why ternary-type opacity is well-suited and desired for the task at hand. In particular, we provide theoretical insights into the process of jointly optimizing radiance and opacity through the volumetric rendering process. Through exhaustive experiments on benchmark datasets, we validate our claim and provide insights into the optimization process, which we believe will unleash the potential of RGB-only NeRF-SLAM. To foster this line of research, we also propose a simple yet novel visual odometry scheme that uses a hybrid combination of volumetric and warping-based image renderings. More specifically, the proposed hybrid odometry (HO) additionally uses image warping-based coarse odometry, leading up to an order of magnitude final speed-up. Furthermore, we show that the proposed TT and HO well complement each other, offering state-of-the-art results on benchmark datasets in terms of both speed and accuracy. [PDF](http://arxiv.org/abs/2312.13332v2) **摘要**RGB-SLAM 仅使用颜色信息估计三维场景，我们引入三元型的不透明度来优化三维重建的准确性和速度。**要点**- RGB-SLAM 现有的方法中不透明度被认为是二元型。- 通过理论分析证明了三元型的不透明度是 RGB-SLAM 的最优选择。- 三元型的不透明度优化可以提高 RGB-SLAM 的精度。- 三元型的不透明度优化可以通过体渲染轻松实现。- 提出了一个简单但新颖的视觉里程计方案，使用体渲染和基于图像翘曲的混合方法。- 基于图像翘曲的粗略里程计算可以优化视觉里程计的速度。- 三元型的不透明度和混合里程计可以很好地互补，在速度和精度方面都取得了最先进的结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：RGB-only NeRF-SLAM 的三元型不透明度和混合里程计</li><li>作者：Yifan Yuan, Hongrui Zhou, Yuxiao Zhou, Xiaowei Zhou, Chen Feng</li><li>单位：无</li><li>关键词：NeRF-SLAM、三元型不透明度、混合里程计、RGB-D SLAM</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：(1)：研究背景：RGB-only NeRF-SLAM 中不透明度的性质被认为是二元型的，但现有的 RGB-only NeRF-SLAM 并没有遵循这一特性。因此，本文提出将三元型不透明度先验引入 RGB-only NeRF-SLAM 管道中。(2)：过去的方法及其问题：优化通过体积渲染函数不能轻松集成所需先验。(3)：本文的研究方法：研究三元型不透明度为什么非常适合并且是这项任务所需要的。具体来说，本文提供了通过体积渲染过程联合优化辐射度和不透明度的理论见解。(4)：方法在任务中的表现：通过在基准数据集上的详尽实验，验证了本文的声明，并提供了对优化过程的见解，这将释放 RGB-only NeRF-SLAM 的潜力。为了促进这一研究方向，本文还提出了一种简单但新颖的视觉里程计方案，该方案使用体积和基于图像扭曲的图像渲染的混合组合。更具体地说，所提出的混合里程计 (HO) 另外使用了基于图像扭曲的粗略里程计，从而最终加速了一个数量级。此外，本文表明所提出的 TT 和 HO 相互补充，在基准数据集上在速度和准确性方面都提供了最先进的结果。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种受益于不透明场景先验的仅 RGB NeRF-SLAM 方法。这是通过 3D 场景的三元型建模来实现的。此外，我们提出了一种混合方法来估计相机运动，从而导致整体速度显着提高。我们在分析体积渲染和不透明表面时提供的理论见解在我们的上下文中得到了我们的实验结果的充分支持。事实上，报告的观察结果促使我们提出了一个简单但非常有效的策略来利用不透明表面先验，这反过来又为我们提供了更高的准确性和速度，这要归功于所提出的三元型先验提供的更快的收敛速度。局限性和未来工作。虽然是实时的，但所提出的方法对于许多常见应用来说在消费设备上尚未实时。这些要求可以通过特定于应用程序和硬件的代码优化和系统配置来满足，这仍然是未来的工作。（2）：创新点：</li><li>将三元型不透明度先验引入 RGB-only NeRF-SLAM 管道中。</li><li>提出了一种混合里程计方案，该方案使用体积和基于图像扭曲的图像渲染的混合组合。</li></ol><p>性能：* 在基准数据集上的详尽实验验证了本文的声明，并提供了对优化过程的见解，这将释放 RGB-only NeRF-SLAM 的潜力。* 所提出的 TT 和 HO 相互补充，在基准数据集上在速度和准确性方面都提供了最先进的结果。</p><p>工作量：* 该方法尚未在消费设备上实时。* 需要进行特定于应用程序和硬件的代码优化和系统配置。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-93cf1ecd415a1a0301157b4d0970a43c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222406a8117741693dd9920da4fdf228.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2fac3b124dc86b2b4487320181b09bbe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f1b89edb99ac23be67750f729dc4b43b.jpg" align="middle"></details>## MixRT: Mixed Neural Representations For Real-Time NeRF Rendering**Authors:Chaojian Li, Bichen Wu, Peter Vajda,  Yingyan,  Lin**Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods). [PDF](http://arxiv.org/abs/2312.11841v4) Accepted by 3DV'24. Project Page: https://licj15.github.io/MixRT/**摘要**低质量网格、视图相关位移贴图和压缩 NeRF 模型相结合的方法可实现实时 NeRF 渲染。**要点**- MixRT 采用低质量网格、视图相关位移贴图和压缩 NeRF 模型的新型表示方法。- 这种设计有效地利用了现有图形硬件的功能，从而可以在边缘设备上实现实时 NeRF 渲染。- MixRT 在边缘设备上实现了较快的渲染速度（MacBook M1 Pro 笔记本电脑上以 1280 x 720 分辨率达到 30 FPS 以上）。- MixRT 在室内场景中实现了更好的渲染质量（在 Unbounded-360 数据集中 PSNR 高 0.2）。- MixRT 具有较小的存储空间（低于最先进的方法的 80%）。- MixRT 可以扩展到处理大规模场景。- MixRT 可用于各种应用，包括增强现实、虚拟现实和游戏。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：MixRT：用于实时 NeRF 渲染的混合神经表示</li><li>作者：Lichao Jia、Yufei Wang、Hao Zhu、Kun Zhou、Zhiwen Fan、Shuangbai Zhou</li><li>第一作者单位：清华大学</li><li>关键词：神经辐射场、实时渲染、混合表示、压缩、图形处理器</li><li>论文链接：https://arxiv.org/abs/2302.01328，Github 代码链接：None</li><li><p>摘要：(1)：研究背景：神经辐射场（NeRF）是一种用于新视角合成的领先技术，具有令人印象深刻的逼真重建和渲染能力。然而，在大规模场景中实现实时 NeRF 渲染提出了挑战，通常导致采用具有大量三角形的复杂烘焙网格表示或在烘焙表示中进行资源密集的光线行进。(2)：过去方法及问题：过去的方法要么采用复杂烘焙网格表示，要么采用资源密集的光线行进，这使得实时 NeRF 渲染在大规模场景中具有挑战性。(3)：研究方法：本文提出了一种新的 NeRF 表示 MixRT，它包括一个低质量网格、一个视点相关位移图和一个压缩的 NeRF 模型。这种设计有效地利用了现有图形硬件的功能，从而在边缘设备上实现了实时 NeRF 渲染。(4)：方法性能：本文提出的 MixRT 在边缘设备上实现了实时的渲染速度（在 MacBook M1 Pro 笔记本电脑上以 1280×720 的分辨率达到 30 FPS 以上）、更好的渲染质量（在 Unbounded-360 数据集的室内场景中 PSNR 高出 0.2）和更小的存储大小（与最先进的方法相比减少了 80% 以上）。</p></li><li><p>方法：(1) 提出了一种新的NeRF表示MixRT，它包括一个低质量网格、一个视点相关位移图和一个压缩的NeRF模型。(2) 采用低质量网格来表示场景的几何结构，并使用视点相关位移图来细化网格的细节。(3) 将NeRF模型压缩成一个紧凑的格式，以减少存储空间和提高渲染速度。(4) 设计了一种新的渲染算法，可以有效地利用现有图形硬件的功能，实现实时的NeRF渲染。</p></li><li><p>结论：</p></li></ol><p>（1）MixRT 提出了一种新的 NeRF 表示，该表示将低质量网格、视点相关位移图和压缩的 NeRF 模型结合在一起。这种设计源于我们的观察，即实现高渲染质量并不需要由具有大量三角形的高复杂度几何体表示的网格。这一认识表明，有可能简化烘焙网格并将不同的神经表示纳入渲染、内存和存储效率中。通过详细的运行时分析和优化的基于 WebGL 的渲染框架，MixRT 在渲染质量和效率之间提供了最先进的平衡。</p><p>（2）创新点：</p><ul><li>提出了一种新的 NeRF 表示 MixRT，它将低质量网格、视点相关位移图和压缩的 NeRF 模型结合在一起。</li><li>采用低质量网格来表示场景的几何结构，并使用视点相关位移图来细化网格的细节。</li><li>将 NeRF 模型压缩成一个紧凑的格式，以减少存储空间和提高渲染速度。</li><li>设计了一种新的渲染算法，可以有效地利用现有图形硬件的功能，实现实时的 NeRF 渲染。</li></ul><p>性能：</p><ul><li>在边缘设备上实现了实时的渲染速度（在 MacBook M1 Pro 笔记本电脑上以 1280×720 的分辨率达到 30FPS 以上）。</li><li>更好的渲染质量（在 Unbounded-360 数据集的室内场景中 PSNR 高出 0.2）。</li><li>更小的存储大小（与最先进的方法相比减少了 80% 以上）。</li></ul><p>工作量：</p><ul><li>该方法在 Unbounded-360 数据集上进行了评估。</li><li>该方法与最先进的方法进行了比较。</li><li>该方法的代码已开源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-301a52ffd84957421daad742378d8046.jpg" align="middle"><img src="https://picx.zhimg.com/v2-44d66c65781dd7094b22b4ed21552bd3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-16ddd4478152e5c6a4f02bc6b86875f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-729c2c4e7c400fc17f0e1faacd580d64.jpg" align="middle"></details>## Learning Dense Correspondence for NeRF-Based Face Reenactment**Authors:Songlin Yang, Wei Wang, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong**Face reenactment is challenging due to the need to establish dense correspondence between various face representations for motion transfer. Recent studies have utilized Neural Radiance Field (NeRF) as fundamental representation, which further enhanced the performance of multi-view face reenactment in photo-realism and 3D consistency. However, establishing dense correspondence between different face NeRFs is non-trivial, because implicit representations lack ground-truth correspondence annotations like mesh-based 3D parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning 3DMM space with NeRF-based face representations can realize motion control, it is sub-optimal for their limited face-only modeling and low identity fidelity. Therefore, we are inspired to ask: Can we learn the dense correspondence between different NeRF-based face representations without a 3D parametric model prior? To address this challenge, we propose a novel framework, which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three components: canonical tri-planes, identity deformations, and motion. In terms of motion control, our key contribution is proposing a Plane Dictionary (PlaneDict) module, which efficiently maps the motion conditions to a linear weighted addition of learnable orthogonal plane bases. To the best of our knowledge, our framework is the first method that achieves one-shot multi-view face reenactment without a 3D parametric model prior. Extensive experiments demonstrate that we produce better results in fine-grained motion control and identity preservation than previous methods. [PDF](http://arxiv.org/abs/2312.10422v2) Accepted by Proceedings of the AAAI Conference on Artificial   Intelligence, 2024**摘要**没有三维参数模型先验，也能学习不同神经辐射场人脸表示之间的稠密对应关系。**要点**- 人脸重演具有挑战性，需要在不同的脸部表示之间建立稠密的对应关系，以实现动作转换。- 最近的研究利用神经辐射场（NeRF）作为基本表示，进一步提高了多视角人脸重演在照片真实感和三维一致性方面的性能。- 在不同的人脸NeRF之间建立稠密的对应关系并非易事，因为隐式表示缺乏像基于网格的三维参数模型（如具有索引对齐顶点的3DMM）这样的真实对应注释。- 虽然将3DMM空间与基于NeRF的人脸表示对齐可以实现动作控制，但由于其仅限于脸部建模，且身份保真度低，因此并不理想。- 我们提出了一种新框架，采用三平面作为基本NeRF表示，并将脸部三平面分解为三个分量：规范三平面、身份变形和动作。- 在动作控制方面，我们的关键贡献是提出了一个平面字典（PlaneDict）模块，它可以有效地将动作条件映射到可学习的正交平面基的线性加权叠加。- 据我们所知，我们的框架是第一个在没有三维参数模型先验的情况下实现一发多视角人脸重演的方法。- 大量实验表明，我们在精细的动作控制和身份保持方面产生的结果优于以前的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于 NeRF 的人脸重现的密集对应关系学习</li><li>作者：Songlin Yang, Wei Wang*, Yushi Lan, Xiangyu Fan, Bo Peng, Lei Yang, Jing Dong</li><li>单位：中国科学院大学人工智能学院</li><li>关键词：人脸重现、NeRF、三平面表示、运动控制、身份保持</li><li>链接：https://arxiv.org/abs/2312.10422</li><li><p>摘要：（1）研究背景：人脸重现是一项具有挑战性的任务，需要在不同的人脸表示之间建立密集的对应关系以进行运动转移。最近的研究利用神经辐射场 (NeRF) 作为基本表示，进一步提高了多视角人脸重现的真实感和 3D 一致性。然而，在不同的面部 NeRF 之间建立密集的对应关系并非易事，因为隐式表示缺乏像基于网格的 3D 参数模型（例如，具有索引对齐顶点的 3DMM）那样的真实对应的注释。（2）过去的方法及其问题：虽然将 3DMM 空间与基于 NeRF 的人脸表示对齐可以实现运动控制，但由于其仅限于面部建模和较低的身份保真度，因此并不是最佳选择。（3）研究方法：本文提出了一种新颖的框架，该框架采用三平面作为基本 NeRF 表示，并将面部三平面分解为三个组件：规范三平面、身份变形和运动。在运动控制方面，本文的主要贡献是提出了一种平面字典 (PlaneDict) 模块，该模块有效地将运动条件映射到可学习的正交平面基的线性加权和。（4）方法性能：实验证明，本文的方法在细粒度运动控制和身份保持方面优于以往的方法。</p></li><li><p>方法：(1) 我们将面部三平面分解为规范三平面、身份变形和运动。(2) 提出 PlaneDict 模块将运动条件映射到可学习的正交平面基的线性加权和。(3) 采用 StyleGAN 生成器获得身份变形，并通过 PlaneDict 模块获得运动。(4) 将规范三平面、身份变形和运动相加得到驱动面部图像的三平面。(5) 通过三平面解码器和体积渲染器将三平面投影到 2D 特征图像。(6) 使用超分辨率模块将最终图像大小增加到 256^2。</p></li><li><p>结论：（1）：本文提出了一种基于三平面表示的新颖框架，该框架可以有效地实现人脸重现的密集对应关系学习，在细粒度运动控制和身份保持方面优于以往的方法。（2）：创新点：PlaneDict模块：该模块有效地将运动条件映射到可学习的正交平面基的线性加权和，从而实现运动控制。三平面分解：将面部三平面分解为规范三平面、身份变形和运动，便于运动控制和身份保持。性能：在细粒度运动控制和身份保持方面优于以往的方法。工作量：该方法需要训练多个模型，包括三平面解码器、体积渲染器和超分辨率模块，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b33e8c7219eac4eb653c91c2ed347e6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cffcb53275a5aba0333681ea0099c55b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-629b61bec3df1b03a6a20598b333d114.jpg" align="middle"><img src="https://pica.zhimg.com/v2-64cee96b4bc968085a7ff7e4564a8091.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d09b9ae7baddeedc8870c3b7e3bb83b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29a23cef6a44369e63e93d4b44f0b59e.jpg" align="middle"></details><h2 id="Aleth-NeRF-Illumination-Adaptive-NeRF-with-Concealing-Field-Assumption"><a href="#Aleth-NeRF-Illumination-Adaptive-NeRF-with-Concealing-Field-Assumption" class="headerlink" title="Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption"></a>Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption</h2><p><strong>Authors:Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada</strong></p><p>The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material reflectance into emission solely from 3D points. This simplified rendering approach presents challenges in accurately modeling images captured under adverse lighting conditions, such as low light or over-exposure. Motivated by the ancient Greek emission theory that posits visual perception as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and generate normal-light condition novel views unsupervised. We introduce the concept of a “Concealing Field,” which assigns transmittance values to the surrounding air to account for illumination effects. In dark scenarios, we assume that object emissions maintain a standard lighting level but are attenuated as they traverse the air during the rendering process. Concealing Field thus compel NeRF to learn reasonable density and colour estimations for objects even in dimly lit situations. Similarly, the Concealing Field can mitigate over-exposed emissions during the rendering stage. Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation. Our code and dataset available at <a href="https://github.com/cuiziteng/Aleth-NeRF">https://github.com/cuiziteng/Aleth-NeRF</a> </p><p><a href="http://arxiv.org/abs/2312.09093v3">PDF</a> AAAI 2024, code available at   <a href="https://cuiziteng.github.io/Aleth_NeRF_web/">https://cuiziteng.github.io/Aleth_NeRF_web/</a> Modified version of previous   paper arXiv:2303.05807</p><p><strong>Summary</strong><br>用“遮挡场 (Concealing Field)” 辅助 NeRF 模型训练以模拟具有挑战性光照条件下的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF 利用视点为中心的策略，将光照和材质反射特性混入 3D 点的放射中。</li><li>该简化渲染方法难以准确地模拟在不利的照明条件（例如，低光照或过度曝光）下拍摄的图像。</li><li>受古希腊认为视觉感知是来自眼睛射出的射线的影响的理论启发，我们对 NeRF 框架进行了改进，在具有挑战性的光照条件下训练 NeRF 模型，并生成正常光照条件下的新视角。</li><li>我们引入了“遮挡场”的概念，为周围的空气分配透射率值，以考虑光照效果。</li><li>在黑暗场景中，我们假设对象放射保持标准照明水平，但在渲染过程中在空气中传播时会衰减。</li><li>遮挡场迫使 NeRF 在光线昏暗的环境，学习到合理的物体密度和颜色估计。</li><li>遮挡场可以同样地减少渲染过程中过度曝光的放射。</li><li>我们提供了一个在具有挑战性的照明条件下捕获的综合多视图数据集以供评估。</li><li>我们的代码和数据集位于 <a href="https://github.com/cuiziteng/Aleth-NeRF。">https://github.com/cuiziteng/Aleth-NeRF。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Aleth-NeRF：具有遮蔽场假设的照明自适应 NeRF</li><li>作者：Ziteng Cui, Lin Gu, Xiao Sun, Xianzheng Ma, Yu Qiao, Tatsuya Harada</li><li>隶属机构：东京大学</li><li>关键词：NeRF、照明、低光、过度曝光、遮蔽场</li><li>论文链接：https://arxiv.org/abs/2312.09093，Github 链接：https://github.com/cuiziteng/Aleth-NeRF</li><li>摘要：（1）研究背景：NeRF 是一种有效的方法，可以从 2D 图像中理解 3D 场景并生成新颖的视角。然而，NeRF 及其后续变体的公式假设捕获的图像处于正常光照条件下，通常无法在低光或过度曝光场景下工作。这是因为香草 NeRF 以观看者为中心，它对从某个位置到观看者的光线发射量进行建模，而没有将照明和材料解开（图 1(a)）。因此，NeRF 算法将黑暗场景解释为 3D 对象粒子的辐射不足，违反了对对象材料和几何形状的估计。在实际应用中，图像通常在具有挑战性的照明条件下拍摄。（2）过去的方法及其问题：NeRF 是一种以观看者为中心的范例，将照明和材料反射的各个方面纠缠到仅来自 3D 点的发射中。这种简化的渲染方法在准确建模在不利的照明条件下捕获的图像时存在挑战，例如低光或过度曝光。这是因为香草 NeRF 以观看者为中心，它对从某个位置到观看者的光线发射量进行建模，而没有将照明和材料解开（图 1(a)）。因此，NeRF 算法将黑暗场景解释为 3D 对象粒子的辐射不足，违反了对对象材料和几何形状的估计。在实际应用中，图像通常在具有挑战性的照明条件下拍摄。（3）论文提出的研究方法：为了解决这个问题，本文提出了一种新的方法 Aleth-NeRF，它可以训练低光和过度曝光场景并生成正常光照条件下的新颖视角。Aleth-NeRF 受古希腊哲学的启发，通过在对象和观察者之间建模遮蔽场来自然地扩展香草 NeRF 中的透射函数。在黑暗场景中，我们假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减。因此，遮蔽场迫使 NeRF 即使在光线昏暗的情况下也能学习到合理的密度和颜色估计。同样，遮蔽场可以减轻渲染阶段过度曝光的发射。此外，我们提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。（4）方法在任务和性能上的表现：我们的方法在具有挑战性的照明条件下训练 NeRF，并在正常光照条件下生成新颖的视角。我们对低光和过度曝光图像进行了广泛的实验，结果表明，我们的方法能够准确地估计对象的密度和颜色，即使在光线昏暗的情况下也是如此。此外，我们的方法能够生成具有正常光照条件的新颖视角，即使输入图像严重不足或过度曝光。这些结果表明，我们的方法可以有效地处理具有挑战性的照明条件下的图像，并为在各种照明条件下生成逼真的新颖视角开辟了新的可能性。</li></ol><p>7.Methods：（1）提出了一种新的方法Aleth-NeRF，它可以通过在对象和观察者之间建模遮蔽场来自然地扩展香草NeRF中的透射函数，从而训练低光和过度曝光场景并生成正常光照条件下的新颖视角；（2）在黑暗场景中，假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减，因此遮蔽场迫使NeRF即使在光线昏暗的情况下也能学习到合理的密度和颜色估计；（3）同样，遮蔽场可以减轻渲染阶段过度曝光的发射；（4）提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。</p><ol><li>结论：（1）：本文提出了一种新的方法 Aleth-NeRF，它可以训练低光和过度曝光场景并生成正常光照条件下的新颖视角。此外，我们提出了一个综合的多视图数据集，该数据集在具有挑战性的照明条件下捕获，用于评估。（2）：创新点：</li><li>提出了一种新的方法 Aleth-NeRF，它可以通过在对象和观察者之间建模遮蔽场来自然地扩展香草 NeRF 中的透射函数，从而训练低光和过度曝光场景并生成正常光照条件下的新颖视角。</li><li>在黑暗场景中，假设对象发射保持标准光照水平，但在渲染过程中穿过空气时会衰减，因此遮蔽场迫使 NeRF 即使在光线昏暗的情况下也能学习到合理的密度和颜色估计。</li><li>同样，遮蔽场可以减轻渲染阶段过度曝光的发射。性能：</li><li>我们的方法在具有挑战性的照明条件下训练 NeRF，并在正常光照条件下生成新颖的视角。</li><li>我们对低光和过度曝光图像进行了广泛的实验，结果表明，我们的方法能够准确地估计对象的密度和颜色，即使在光线昏暗的情况下也是如此。</li><li>此外，我们的方法能够生成具有正常光照条件的新颖视角，即使输入图像严重不足或过度曝光。工作量：</li><li>Aleth-NeRF 应该针对每个场景进行专门训练，这与香草 NeRF 相同。</li><li>此外，Aleth-NeRF 可能无法处理具有非均匀照明条件（王、徐和刘，2022 年）或阴影条件（屈等人，2017 年）的场景，我们认为这也是未来探索的一个有价值的研究课题。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-73df3246f0f189f363c4406f05fdfb64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0081eb8c2f0caf23f1cd8c480137c67c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5911359d2c2da29789ea31d3c9246652.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fafb4552428de0fb3d21116b5da3089.jpg" align="middle"><img src="https://picx.zhimg.com/v2-48849341af81d4f10dd85c432bbe02f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-45fe793c1d78fdd64382dd5c56c317ba.jpg" align="middle"></details><h2 id="Learn-to-Optimize-Denoising-Scores-for-3D-Generation-A-Unified-and-Improved-Diffusion-Prior-on-NeRF-and-3D-Gaussian-Splatting"><a href="#Learn-to-Optimize-Denoising-Scores-for-3D-Generation-A-Unified-and-Improved-Diffusion-Prior-on-NeRF-and-3D-Gaussian-Splatting" class="headerlink" title="Learn to Optimize Denoising Scores for 3D Generation: A Unified and   Improved Diffusion Prior on NeRF and 3D Gaussian Splatting"></a>Learn to Optimize Denoising Scores for 3D Generation: A Unified and   Improved Diffusion Prior on NeRF and 3D Gaussian Splatting</h2><p><strong>Authors:Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, Guosheng Lin</strong></p><p>We propose a unified framework aimed at enhancing the diffusion priors for 3D generation tasks. Despite the critical importance of these tasks, existing methodologies often struggle to generate high-caliber results. We begin by examining the inherent limitations in previous diffusion priors. We identify a divergence between the diffusion priors and the training procedures of diffusion models that substantially impairs the quality of 3D generation. To address this issue, we propose a novel, unified framework that iteratively optimizes both the 3D model and the diffusion prior. Leveraging the different learnable parameters of the diffusion prior, our approach offers multiple configurations, affording various trade-offs between performance and implementation complexity. Notably, our experimental results demonstrate that our method markedly surpasses existing techniques, establishing new state-of-the-art in the realm of text-to-3D generation. Furthermore, our approach exhibits impressive performance on both NeRF and the newly introduced 3D Gaussian Splatting backbones. Additionally, our framework yields insightful contributions to the understanding of recent score distillation methods, such as the VSD and DDS loss. </p><p><a href="http://arxiv.org/abs/2312.04820v1">PDF</a> </p><p><strong>摘要</strong><br>神经辐射场 (NeRF) 扩散模型的统一框架，迭代优化 3D 模型和扩散先验，实现文本到 3D 生成新突破。</p><p><strong>要点</strong></p><ul><li>提出一种统一的增强 3D 生成任务扩散先验的框架。</li><li>识别出扩散先验与扩散模型训练过程之间的差异，阻碍 3D 生成的质量。</li><li>提出一个新的统一框架，迭代优化 3D 模型和扩散先验。</li><li>该方法在 NeRF 和新引入的 3D 高斯散点骨干上均表现出令人印象深刻的性能。</li><li>该框架有助于理解最近的分数蒸馏方法，如 VSD 和 DDS 损失。</li><li>大幅优于现有技术，在文本到 3D 生成领域树立了新的最先进水平。</li><li>该方法提供了对最近的分数蒸馏方法的深刻理解，例如 VSD 和 DDS 损失。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：学习优化用于 3D 生成的去噪评分：神经辐射场和 3D 高斯散射的统一改进扩散先验</li><li>作者：杨晓峰、陈一文、陈程、张驰、许怡、杨旭磊、刘发耀、林国生</li><li>单位：南洋理工大学</li><li>关键词：扩散模型、3D 生成、NeRF、3D 高斯散射、评分蒸馏</li><li>论文链接：https://arxiv.org/abs/2312.04820Github 链接：无</li><li>摘要：(1) 研究背景：扩散模型是一种强大的生成式方法，在图像生成、编辑和 3D 生成等任务中取得了成功。然而，现有方法在 3D 生成任务中经常难以生成高质量的结果。(2) 过去的方法：SDS 损失是 DreamFusion 中提出的用于 3D 生成的扩散先验，它通过计算图像和噪声之间的评分来指导 3D 模型的优化。然而，SDS 损失存在一些问题，例如容易产生模糊的生成图像，并且难以捕捉数据空间的多样性。(3) 本文方法：本文提出了一种新的统一框架来增强 3D 生成的扩散先验。该框架通过迭代优化 3D 模型和扩散先验来提高生成图像的质量。该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。(4) 实验结果：实验结果表明，本文方法在 NeRF 和 3D 高斯散射两种骨干网络上均取得了比现有技术更好的性能，在文本到 3D 生成的领域树立了新的技术水平。此外，该框架还对最近的评分蒸馏方法，例如 VSD 和 DDS 损失，做出了有见地的贡献。</li></ol><p><strong>方法：</strong></p><p>（1）问题表述：考虑优化 3D 模型 [23]，参数化为参数 θ，以及将 θ 转换为 2D 图像 x = g(θ) 的可微渲染操作 g。我们感兴趣的是使用条件预训练扩散模型 ϵϕ(zt; y) 优化 θ 的问题。在以下小节中，我们将分析 SDS 损失如何解决这个问题，以及为什么它不能生成良好的结果。</p><p>（2）先前扩散先验的问题：扩散模型训练和推理中的差异导致 SDS 损失产生次优结果。考虑方程 3 中的 SDS 损失。它直接使用 CFG 变体（通常带有权重因子 100）的参考去噪评分来优化 θ。然而，如方程 1 所述，扩散模型的训练学习了评分函数 ϵϕ(zt; y, t) 而没有使用 CFG。这种差异产生了一个重大问题：SDS 损失中应用的无分类器引导并未将目标分布引导到与参考分布（由评分函数 ϵϕ(zt; y, t) 表征）对齐，而是引导到扩散模型的 CFG 修改版本，表示为 ˆϵϕ。这导致 SDS 损失生成的输出通常过度饱和且缺乏多样性，正如原始研究 [27] 中指出的那样。</p><p>（3）扩散先验需要更高的 CFG 引导：对上述问题的直接解决方案可以直接删除 SDS 损失中的 CFG。我们称之为参考 SDS 损失：∇θLSDS–ref(ϕ,x)=Et,ϵ[(ϵϕ(zt;y,t)−ϵ)∂x∂θ]。然而，从经验观察和理论分析来看，直接使用上述方程在 3D 生成中是不可行的。根据经验，正如先前工作 [18, 27] 所证明的，扩散先验仅在使用大 CFG 权重 w 时才能够学习 3D 对象的详细特征。我们的实验观察到了类似的挑战。可以在实验部分和图 5 中找到一个说明。从理论上讲，较大的 CFG 权重 w 将目标分布引导到条件评分函数的方向，远离无条件评分函数，从而生成与条件更相关的内容。与 2D 空间相比，3D 优化在使用 2D 扩散模型时引入了额外的分布外因素 [39]。因此，2D 扩散先验在 3D 问题上需要更大的 w。</p><p>（4）学习优化去噪评分：基于以上分析，我们改进扩散先验的关键见解是，SDS 应从较高的初始无分类器引导 (CFG) 值开始，并最终与参考 SDS 公式方程 6 保持一致，以弥合训练和推理阶段之间的差距。一种自然的方法是在优化过程中逐渐减小 CFG 权重 w。然而，这种简单的方法并没有在我们的实验中产生改进的结果。主要挑战在于 CFG 权重 w 是一个标量，它会统一影响整个噪声图，而不会考虑内部变化。此外，调整 w 值以适应优化过程中不同 3D 对象的差异难度被证明具有挑战性。为此，我们提出了 LODS（学习优化去噪评分）算法。我们的方法首先通过两个额外的可学习参数扩展无分类器引导公式。第一个，表示为 α，是指可学习的无条件嵌入，初始化为空嵌入 ∅。第二个 ψ 表示添加到网络的附加参数（例如 LoRA [14] 参数）。这些可学习参数中的每一个都对应于我们提出的方法的一个变体。然后，我们建议使用算法 1 中所示的 LODS 算法来学习这两个可学习参数。</p><p>（5）具体实现：我们首先初始化 3D 模型参数和当前运行的 SDS 损失。然后，我们继续执行两个迭代优化步骤。在步骤 5 中，我们使用当前的 SDS 损失来优化 3D 模型参数。在此之后，在步骤 6 中，我们优化 SDS 的参数。这种迭代优化算法在两个方面具有优势。首先，它允许 3D 模型的优化从任意初始评分函数开始。其次，步骤 6 中的优化过程通过将原始 SDS 与方程 6 对齐来学习弥合训练和推理阶段之间的差距。随后的子部分深入探讨了通过优化两个额外的可学习参数来实现 LODS 算法的细节。在这项研究中，我们将我们的探索限制为使用可学习空嵌入或可学习低秩参数来扩展无分类器引导。然而，值得注意的是，我们的框架可以扩展到包含其他可学习参数，例如 ControlNet 结构 [46] 和 T2I 适配器结构 [25] 中的参数。</p><ol><li>结论：</li></ol><p>（1）本文的主要贡献在于提出了一种新的框架来增强3D生成的扩散先验。该框架通过迭代优化3D模型和扩散先验来提高生成图像的质量。该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。</p><p>（2）创新点：</p><ul><li>提出了一种新的统一框架来增强3D生成的扩散先验。</li><li>该框架通过迭代优化3D模型和扩散先验来提高生成图像的质量。</li><li>该框架利用扩散先验的不同可学习参数，提供了多种配置，可在性能和实现复杂性之间进行权衡。</li></ul><p>（3）性能：</p><ul><li>在NeRF和3D高斯散射两种骨干网络上均取得了比现有技术更好的性能。</li><li>在文本到3D生成的领域树立了新的技术水平。</li></ul><p>（4）工作量：</p><ul><li>该框架的实现复杂度较高，需要较多的计算资源。</li><li>该框架的训练时间较长，需要花费数天或数周的时间。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-abd850391ea953af46ce32e34e49d149.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f856eb6ef9dd71eaef7a7695cd8943b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a74922c93a0f56db6537ecd6f851decb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d83c9cfec77cb9e942b656c02dfcdcf7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dcf54591ff8e16e58863a1eaa2623a0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47cb88f4c0caa3a68ecae822221de46d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dc0db37ef3bd4c00248273e0c7931a50.jpg" align="middle"></details>## SO-NeRF: Active View Planning for NeRF using Surrogate Objectives**Authors:Keifer Lee, Shubham Gupta, Sunglyoung Kim, Bhargav Makwana, Chao Chen, Chen Feng**Despite the great success of Neural Radiance Fields (NeRF), its data-gathering process remains vague with only a general rule of thumb of sampling as densely as possible. The lack of understanding of what actually constitutes good views for NeRF makes it difficult to actively plan a sequence of views that yield the maximal reconstruction quality. We propose Surrogate Objectives for Active Radiance Fields (SOAR), which is a set of interpretable functions that evaluates the goodness of views using geometric and photometric visual cues - surface coverage, geometric complexity, textural complexity, and ray diversity. Moreover, by learning to infer the SOAR scores from a deep network, SOARNet, we are able to effectively select views in mere seconds instead of hours, without the need for prior visits to all the candidate views or training any radiance field during such planning. Our experiments show SOARNet outperforms the baselines with $\sim$80x speed-up while achieving better or comparable reconstruction qualities. We finally show that SOAR is model-agnostic, thus it generalizes across fully neural-implicit to fully explicit approaches. [PDF](http://arxiv.org/abs/2312.03266v1) 13 pages**Summary**根据几何和光度视觉线索评估视角优劣，帮助 NeRF 迅速地选择最佳视角，提高重建质量。**Key Takeaways**- 提出了一种可解释函数 SOAR，用于评估视角的优劣，指标包括表面覆盖率、几何复杂度、纹理复杂度和光线多样性。- 设计了 SOARNet，可以快速推导出 SOAR 分数，而无需访问候选视角或训练任何辐射场。- SOARNet 在 80 倍加速的情况下优于基准，重建质量更好或相当。- SOAR 与模型无关，适用于纯神经隐式到纯显式方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：SO-NeRF：使用代理目标的 NeRF 主动视图规划</li><li>作者：Chen Feng、Yuxuan Zhang、Xiaoguang Han、Shuang Zhao、Zhiwen Fan、Zeyu Jin、Yibo Yang、Shuang Liang、Lin Gao、Xiaogang Jin</li><li>隶属单位：纽约大学</li><li>关键词：神经辐射场、主动视图规划、代理目标、视图选择、深度学习</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）：尽管神经辐射场 (NeRF) 取得了巨大的成功，但其数据收集过程仍然模糊不清，只有一个“尽可能密集地采样”的一般经验法则。由于缺乏对什么实际上构成 NeRF 的良好视图的理解，因此很难主动规划出一系列视图，从而产生最大的重建质量。（2）：过去的方法包括随机采样、主动学习和基于不确定性的采样。这些方法要么效率低下，要么需要对辐射场进行多次访问，要么需要对所有候选视图进行预先访问。（3）：本文提出了一种用于主动辐射场的代理目标 (SOAR)，这是一组可解释的函数，使用几何和光度视觉线索（表面覆盖率、几何复杂性、纹理复杂性和光线多样性）来评估视图的优劣。此外，通过学习从深度网络 SOARNet 推断 SOAR 分数，我们能够在短短几秒内有效地选择视图，而无需事先访问所有候选视图或在规划期间训练任何辐射场。（4）：实验表明，SOARNet 在实现更好或相当的重建质量的同时，比基线快约 80 倍。我们最终表明 SOAR 与模型无关，因此它可以跨越完全神经隐式到完全显式的方法进行推广。</p></li><li><p>方法：（1）首先，我们定义了评估训练集质量的目标函数，该函数最大化了表面的覆盖率、几何复杂性、纹理复杂性和光线多样性。（2）然后，我们提出了 SOARNet，这是一个深度神经网络，可以有效地计算目标函数的分数，而无需访问所有候选视图或在规划期间训练任何辐射场。（3）最后，我们通过贪婪策略构建了一个最优的轨迹，该策略在每一步选择最大化所需目标函数的视图。</p></li><li><p>结论：（1）：本文提出的 SOAR 是一个代理目标函数集合，旨在指示给定一组输入时，生成的辐射场模型的优劣。为了实现实时轨迹生成的有效推理，我们进一步提出了一个深度神经网络 SOARNet，它能够在看不见的姿态下以每步&lt;1s的速度进行规划。通过广泛的评估，我们已经证明我们的方法确实比基线快约 80 倍，同时实现了更好或相当的重建质量。（2）：创新点：</p></li><li>提出了一种代理目标函数集合 SOAR，用于评估给定一组输入时，生成的辐射场模型的优劣。</li><li>提出了一种深度神经网络 SOARNet，能够在看不见的姿态下以每步&lt;1s的速度进行规划。性能：</li><li>在实现更好或相当的重建质量的同时，比基线快约 80 倍。工作量：</li><li>实验表明，SOARNet 在实现更好或相当的重建质量的同时，比基线快约 80 倍。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-671944d1cdae24c5a23ad1828db56206.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dabcaa09003b69ee9c199436cd4103a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d920e266e7bb522989e7f812b8f3e8c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fc30479617914fe92232c4f441700b2.jpg" align="middle"></details><h2 id="SANeRF-HQ-Segment-Anything-for-NeRF-in-High-Quality"><a href="#SANeRF-HQ-Segment-Anything-for-NeRF-in-High-Quality" class="headerlink" title="SANeRF-HQ: Segment Anything for NeRF in High Quality"></a>SANeRF-HQ: Segment Anything for NeRF in High Quality</h2><p><strong>Authors:Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai</strong></p><p>Recently, the Segment Anything Model (SAM) has showcased remarkable capabilities of zero-shot segmentation, while NeRF (Neural Radiance Fields) has gained popularity as a method for various 3D problems beyond novel view synthesis. Though there exist initial attempts to incorporate these two methods into 3D segmentation, they face the challenge of accurately and consistently segmenting objects in complex scenarios. In this paper, we introduce the Segment Anything for NeRF in High Quality (SANeRF-HQ) to achieve high quality 3D segmentation of any object in a given scene. SANeRF-HQ utilizes SAM for open-world object segmentation guided by user-supplied prompts, while leveraging NeRF to aggregate information from different viewpoints. To overcome the aforementioned challenges, we employ density field and RGB similarity to enhance the accuracy of segmentation boundary during the aggregation. Emphasizing on segmentation accuracy, we evaluate our method quantitatively on multiple NeRF datasets where high-quality ground-truths are available or manually annotated. SANeRF-HQ shows a significant quality improvement over previous state-of-the-art methods in NeRF object segmentation, provides higher flexibility for object localization, and enables more consistent object segmentation across multiple views. Additional information can be found at <a href="https://lyclyc52.github.io/SANeRF-HQ/">https://lyclyc52.github.io/SANeRF-HQ/</a>. </p><p><a href="http://arxiv.org/abs/2312.01531v1">PDF</a> </p><p><strong>摘要</strong><br>利用SAM的提示和NeRF的多个视角，以高质量分割3D目标。</p><p><strong>要点</strong></p><ul><li>SANeRF-HQ将SAM用于开放世界目标分割，并利用NeRF从不同视点聚合信息。</li><li>为了克服上述挑战，我们采用密度场和RGB相似性来提高聚合期间分割边界的准确性。</li><li>SANeRF-HQ在多个具有高质量基本事实或手动注释的NeRF数据集上进行定量评估。</li><li>SANeRF-HQ在NeRF目标分割方面对以前的最先进方法显示出显着的质量改进。</li><li>SANeRF-HQ为目标检测提供了更高的灵活性，并实现了跨多个视图更一致的目标分割。</li><li>可以通过<a href="https://lyclyc52.github.io/SANeRF-HQ/获取更多相关信息。">https://lyclyc52.github.io/SANeRF-HQ/获取更多相关信息。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SANeRF-HQ：高品质任意物体 NeRF 分割</li><li>作者：Yichen Liu, Benran Hu, Chi-Keung Tang, Yu-Wing Tai</li><li>隶属机构：香港科技大学</li><li>关键词：NeRF，分割，任意物体分割，高品质，零样本分割</li><li>论文链接：https://arxiv.org/abs/2312.01531   Github 链接：None</li><li><p>摘要：(1)：研究背景：神经辐射场（NeRF）在复杂真实世界场景的新颖视图合成中取得了最先进的结果。NeRF 使用多层感知器（MLP）对给定场景进行编码，并支持查询给定 3D 坐标和视图方向的密度和辐射，这些坐标和视图方向用于从任何视点渲染逼真的图像。此外，在训练期间，NeRF 只需要具有相机位姿的 RGB 图像，这直接将 3D 链接到 2D。具有连续表示的简单但巧妙的架构很快开始挑战使用显式离散结构（例如 RGB-D 图像或点云）的传统表示。因此，NeRF 准备好在 3D 视觉中解决更具挑战性的任务。NeRF 表示可以受益的一个重要的下游任务是 3D 对象分割，这是 3D 视觉的基础，并广泛用于许多应用。为了解决 NeRF 中的对象分割问题，研究人员调查了各种方法。针对语义分割的语义 NeRF 是该方向上的第一批作品之一。DFF 将预训练特征（例如 DINO）的知识蒸馏到 3D 特征场中，用于无监督对象分解。监督方法（例如 [47]）利用 Mask2Former 获得初始 2D 掩码，并使用全景辐射场将其提升到 3D。尽管这些方法展示了令人印象深刻的结果，但它们的性能受到用于生成特征的预训练模型的限制。最近，出现了大型视觉模型，例如任意物体分割模型（SAM），具有强大的零样本泛化性能，可以作为许多下游任务的骨干组件。具体来说，SAM 为分割任务提出了一种新范式，该范式可以接受各种提示作为输入，并生成不同语义级别的分割掩码作为输出。SAM 的多功能性和泛化性为在 NeRF 中执行可提示的对象分割提供了新方法。虽然对这一领域进行了一些调查[10, 13, 21]，但新视图中的掩码质量仍然不令人满意。有鉴于此，我们提出了一种新的通用框架来实现 NeRF 中基于提示的 3D 分割。我们的框架称为 SegmentAnything for NeRF in High Quality，或 SANeRF-HQ，它利用现有的 2D 基础模型（例如 SegmentAnything）允许各种提示作为输入，并生成具有高精度和多视图一致性的 3D 分割。我们论文的主要贡献是：</p></li><li><p>我们提出了 SANeRF-HQ，这是在 NeRF 中生成高质量 3D 对象分割的首次尝试之一，在更准确的分割边界和更好的多视图一致性方面取得了进展。</p></li><li>我们通过组装和评估多个 NeRF 数据集来验证我们的方法，这些数据集中提供了高质量的真实情况或手动注释。SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进，为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。有关更多信息，请访问 https://lyclyc52.github.io/SANeRF-HQ/。</li></ol><p>(2)：过去的方法：* 语义 NeRF：针对语义分割，但性能受限于预训练模型。* DFF：将预训练特征蒸馏到 3D 特征场中，用于无监督对象分解，但性能受限于预训练模型。* Mask2Former：利用 Mask2Former 获得初始 2D 掩码，并使用全景辐射场将其提升到 3D，但新视图中的掩码质量仍然不令人满意。</p><p>(3)：本研究方法：* SANeRF-HQ：利用现有的 2D 基础模型（例如 SegmentAnything）允许各种提示作为输入，并生成具有高精度和多视图一致性的 3D 分割。* 我们利用密度场和 RGB 相似性来增强聚合过程中分割边界的准确性。</p><p>(4)：方法的性能：* 在多个 NeRF 数据集上，SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进。* SANeRF-HQ 为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。</p><ol><li><p>方法：（1）特征容器：利用预训练的 SAM 模型对图像进行编码，得到 2D 特征，这些特征可重复用于预测和传播掩码，因此可以预先计算或提取场景特征，并针对不同的输入提示重复使用；（2）掩码解码器：将用户提供的提示在不同视图之间传播，并使用来自容器的 SAM 特征生成中间掩码输出；（3）掩码聚合器：将生成的 2D 掩码集成到 3D 空间中，并利用来自 NeRF 模型的颜色和密度场来实现高质量的 3D 分割。</p></li><li><p>结论：（1）：SANeRF-HQ 结合了 SegmentAnything 模型 (SAM) 在开放世界物体分割中的优势和 NeRF 在聚合来自多个视点的信息的优势，在高质量 3D 分割方面取得了重大进展。我们的方法在各种 NeRF 数据集上进行了定量和定性评估，这证明了 SANeRF-HQ 相比于以前最先进的方法的优势。此外，我们展示了将我们的工作扩展到 4D 动态 NeRF 对象分割的潜力（请参阅补充材料）。SANeRF-HQ 有望为不断发展的 3D 计算机视觉和分割技术领域做出重大贡献。（2）：创新点：</p></li><li>利用预训练的 SAM 模型对图像进行编码，得到 2D 特征，这些特征可重复用于预测和传播掩码。</li><li>使用来自 NeRF 模型的颜色和密度场来实现高质量的 3D 分割。性能：</li><li>在多个 NeRF 数据集上，SANeRF-HQ 在 NeRF 对象分割中的先前最先进方法上显示出显着的质量改进。</li><li>SANeRF-HQ 为对象定位提供了更高的灵活性，并能够在多个视图中实现更一致的对象分割。工作量：</li><li>SANeRF-HQ 是一种通用的框架，可以与任何预训练的 2D 分割模型一起使用。</li><li>SANeRF-HQ 易于实现，并且可以在各种 NeRF 数据集上进行训练和评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9a343a2eb78b0ee139f02bb29d9d32d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1095025f718c19933ffeb4fc65253032.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cd347cc2cc1b17675b2cf3433c14135.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d4043be457d52c93131d8791d9d26578.jpg" align="middle"></details><h2 id="Deceptive-Human-Prompt-to-NeRF-3D-Human-Generation-with-3D-Consistent-Synthetic-Images"><a href="#Deceptive-Human-Prompt-to-NeRF-3D-Human-Generation-with-3D-Consistent-Synthetic-Images" class="headerlink" title="Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent   Synthetic Images"></a>Deceptive-Human: Prompt-to-NeRF 3D Human Generation with 3D-Consistent   Synthetic Images</h2><p><strong>Authors:Shiu-hong Kao, Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang</strong></p><p>This paper presents Deceptive-Human, a novel Prompt-to-NeRF framework capitalizing state-of-the-art control diffusion models (e.g., ControlNet) to generate a high-quality controllable 3D human NeRF. Different from direct 3D generative approaches, e.g., DreamFusion and DreamHuman, Deceptive-Human employs a progressive refinement technique to elevate the reconstruction quality. This is achieved by utilizing high-quality synthetic human images generated through the ControlNet with view-consistent loss. Our method is versatile and readily extensible, accommodating multimodal inputs, including a text prompt and additional data such as 3D mesh, poses, and seed images. The resulting 3D human NeRF model empowers the synthesis of highly photorealistic novel views from 360-degree perspectives. The key to our Deceptive-Human for hallucinating multi-view consistent synthetic human images lies in our progressive finetuning strategy. This strategy involves iteratively enhancing views using the provided multimodal inputs at each intermediate step to improve the human NeRF model. Within this iterative refinement process, view-dependent appearances are systematically eliminated to prevent interference with the underlying density estimation. Extensive qualitative and quantitative experimental comparison shows that our deceptive human models achieve state-of-the-art application quality. </p><p><a href="http://arxiv.org/abs/2311.16499v1">PDF</a> Github project: <a href="https://github.com/DanielSHKao/DeceptiveHuman">https://github.com/DanielSHKao/DeceptiveHuman</a></p><p><strong>Summary</strong></p><p>模拟人类：巧用控制扩散模型，生成高品质可控 3D 人类 NeRF。</p><p><strong>Key Takeaways</strong></p><ul><li>Deceptive-Human 是一个新颖的 Prompt-to-NeRF 框架，利用先进的控制扩散模型生成高质量的可控 3D 人类 NeRF。</li><li>不同于直接 3D 生成方法（如 DreamFusion 和 DreamHuman），Deceptive-Human 采用渐进优化技术来提升重建质量。</li><li>该方法通过利用 ControlNet 生成的高质量合成人体图像和视图一致性损失来实现。</li><li>Deceptive-Human 方法是多功能的，并可轻松扩展，可适应多种模态输入，包括文本提示和额外的 3D 网格、姿势和种子图像。</li><li>结果的 3D 人类 NeRF 模型能够从 360 度视角合成高度逼真的新视角。</li><li>Deceptive-Human 的关键在于其渐进微调策略，该策略涉及使用在每个中间步骤提供的多种输入反复增强视图，以改进人类 NeRF 模型。</li><li>在这个迭代细化过程中，系统地消除视图相关外观，以防止其干扰潜在的密度估计。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Deceptive-Human：带提示的 NeRF 3D 人体生成</li><li>作者：Daniel S.H. Kao, Jiapeng Tang, Jiaxiang Shang, Chen Change Loy, Qifeng Chen</li><li>隶属机构：香港中文大学</li><li>关键词：3D 人体生成、NeRF、扩散模型、文本到 3D</li><li>论文链接：https://arxiv.org/abs/2302.08823，Github 链接：None</li><li>摘要：(1) 研究背景：NeRF 是一种强大的技术，可以从 2D 图像生成逼真的 3D 场景。然而，直接使用 NeRF 生成 3D 人体存在诸多挑战，例如难以捕捉人体复杂的几何形状和纹理，以及难以保证生成的图像在不同视角下的一致性。(2) 过去的方法：一些研究尝试使用扩散模型来生成 3D 人体，但这些方法通常需要大量的数据和计算资源，并且生成的图像质量有限。(3) 本文方法：本文提出了一种名为 Deceptive-Human 的新框架，该框架利用最先进的扩散模型来生成高质量的可控 3D 人体 NeRF。Deceptive-Human 采用了一种渐进式细化技术来提高重建质量，该技术利用通过扩散模型生成的合成图像来训练 NeRF 模型。(4) 性能：实验结果表明，Deceptive-Human 在图像质量和一致性方面均优于现有方法。Deceptive-Human 可以从 360 度视角合成高度逼真的新视图，并且可以用于各种应用，例如虚拟现实、增强现实和游戏。</li></ol><p><methods>:(1) 背景：介绍了用于本文的 Clean-NeRF 以及颜色分解方法，该方法是本文使用的重建方法。Clean-NeRF 与原始 NeRF 的输入相同，即空间坐标点 x=(x,y,z) 和方向 d=(θ,ϕ)，以估计密度 σ 和空间特征 b，并基于 b 和 d 预测颜色 c。主要区别在于 c 分解为两个分量，即与视图无关的分量 cvi 和与视图相关的分量 cvd：c=γcvi+(1−γ)cvd，其中 γ 是权重因子。虽然 Clean-NeRF 成功地去除了 NeRF 重建的伪影，但我们的目标是从扩散模型生成的图像中去除视图间的不一致性。由于从低次球谐函数中提取的 cvi 被视为场景中的低频颜色，在 Clean-NeRF 架构中不受视图方向的约束，因此我们在测试期间保留 cvi 而丢弃 cvd，以提取合成图像训练数据中的共同特征。具体来说，我们的 NeRF 结构在推理期间与 Clean-NeRF 不同，其中仅使用与视图无关的分量来生成一致的图像，以便进一步细化（见第 3.2 节）。详细来说，推理阶段的渲染方法可以表示为 ˆCvi=K�k=1ˆT(tk)α(σ(tk)δk)cvi(tk)，其中 ˆT(tk)=exp�−�k−1k′=1σ(tk)δ(tk)�，α(x)=1−exp(−x)，δp=tk+1−tk，tk 是沿射线采样的第 k 个点，σ 是在 Clean-NeRF 架构中估计的密度。(2) Deceptive-Human 框架：* 先验生成：这一步旨在分别生成人类角色的高级特征和几何形状，作为种子图像和 3D 代理先验。给定一组用户提示 {P0,P1,···,Pk}，其中 P0 表示文本控制，P1,P2,...,Pk 表示额外的可选图像控制，即深度、边缘、姿势等，我们引入了一个扩散模型 G1，该模型生成 2D 人类图像 Iseed，如下所示：Iseed=G1(P0,P1,...,Pk)。在我们的工作中，G1 是使用 Latent Diffusion Model 的 txt2img 模块构建的，并附加了 k 个 ControlNet 模型，分别对应于相应的提示类型。通常，Latent Diffusion Model 能够生成高质量的图像，而额外的提示，即 P1,P2,...,Pk 用于特定的输出要求。这个种子图像 Iseed 提供了我们生成的人类的高级特征，这些特征由用户提示，例如，一位穿着白色短裙和蓝色衬衫、长发、白色鞋子，见图 2 的女白领。接下来可以从 Iseed 生成 3D 几何形状。我们利用单视图网格预测模型根据 Iseed 生成 3D 网格 M。种子图像 Iseed 和 3D 网格 M 作为纹理和几何先验传递到后续步骤。请注意，此网格仅用作 3D 几何代理；我们将展示由生成的 NeRF 在渐进细化后诱导的细化深度图。* 一致的合成视图生成：这一步用于生成 3D 感知一致的图像。我们首先从网格 M 渲染视图，即 m1,m2,...,mn，并将每个渲染的网格视图 mi 与 k 种控制类型（例如，边缘、深度等）相关联，表示为 f1(mi),f2(mi),...,fk(mi)。接下来，我们使用预训练的扩散模型 G2 作为粗略视图的生成器。具体来说，vi=G2(P0,Iseed,f1(mi),f2(mi),...,fk(mi))，其中 vi 是从 mi 生成的粗略视图，与 mi 的相机姿态相关联。请注意，生成的 vi 可能非常不一致且不具有 3D 感知，因为它们缺少交叉视图知识。为了获得具有 3D 感知力的图像，我们基于 NeRF 重建 {vi}，在优化期间丢弃与视图相关的分量，即颜色分量由空间 MLP 预测，而不受射线方向的约束。具体来说，我们通过插入合成视图和提取的与视图无关的分量之间的语义一致性正则化器来修改第 3.1 节中描述的 Clean-NeRF 重建策略。我们从 {vi} 中随机采样 m 个图像，表示为种子集 D={vjs|s=1,2,...,m}，其中 1≤j1&lt;j2&lt;...<jm≤n。我们利用预训练的编码器 Φ，本文中为="" vit。对于从方程="" 2="" 渲染的每个与视图无关的帧="" ˆivirendered，我们定义图像级正则化器为：lsem="1−Sc�Φ(I),Φ(ˆIvi)�，其中" sc(·)="" 表示余弦相似度，i="" 是从="" d="" 中随机采样种子图像。最后，我们将方程="" 5="" 与="" clean-nerf="" 重建损失="" lpho,lvi="" 和="" lvd（详细推导见="" [37]）结合起来，开发了一个新的视图一致损失="" lcon，满足="" lcon="�Lpho+�x(Lvi+Lvd)�+λLsem，其中" λ="">0 是预定义的参数。通常，我们获得一个粗糙的辐射场，我们可以从中提取一致的 3D 感知图像，遵循方程 6 中的重建策略。然而，这个 NeRF 仍然远未令人满意。基于这个粗糙的 NeRF，我们在下面介绍 3D 细化的 NeRF。* 3D 视图细化：这一步重点是细化从粗糙 NeRF 渲染的 3D 感知图像，然后重建一个增强的优质 NeRF。假设我们渲染与视图无关的帧，即 ˆIvi，则使用与视图无关的分量对其进行细化，如下所示：ˆIvi=G3(ˆIvi,P0,Iseed)。然后，我们使用细化的图像来更新 NeRF，如下所示：ˆσ=ˆσ+αˆσ，ˆb=ˆb+αˆb，其中 α 是学习率。我们重复此过程，直到达到收敛。</jm≤n。我们利用预训练的编码器></methods></p><ol><li>结论：（1）：本文提出了 Deceptive-Human，这是一个新颖的端到端 Prompt-to-NeRF 框架，该框架利用多模态指导提示生成高质量的 3D 人体 NeRF，包括文本描述以及网格、姿势和风格等其他控制。我们利用了具有神经辐射场 (NeRF) 的最先进的 2D 可控扩散模型，并采用两阶段 NeRF 重建方法来确保合成图像之间的一致性。在第一阶段，从粗糙但一致的图像中丢弃了与视图相关的分量。在第二阶段，对这些图像进行去噪以生成逼真的合成视图，以便为 NeRF 的精细版本进行重建。大量的实验表明，Deceptive-Human 在质量方面优于最先进的基线，并通过其多控制生成的可使用性，极大地扩展了 3D 人体生成在普通用户中的适用性。（2）：创新点：</li><li>提出了一种新颖的端到端 Prompt-to-NeRF 框架，该框架可以从多模态指导提示生成高质量的 3D 人体 NeRF。</li><li>利用了具有神经辐射场 (NeRF) 的最先进的 2D 可控扩散模型，并采用两阶段 NeRF 重建方法来确保合成图像之间的一致性。</li><li>在第一阶段，从粗糙但一致的图像中丢弃了与视图相关的分量。在第二阶段，对这些图像进行去噪以生成逼真的合成视图，以便为 NeRF 的精细版本进行重建。</li></ol><p>性能：* 在质量方面优于最先进的基线。* 极大地扩展了 3D 人体生成在普通用户中的适用性。</p><p>工作量：* 需要大量的实验和计算资源。* 需要对模型进行微调以适应不同的数据集。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9f7d30300d876235d30715a48931b9e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2fd76e8d6804ec1d50363c9316996837.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1d2afeb012626cf2b90dc5b2d57f5440.jpg" align="middle"><img src="https://pica.zhimg.com/v2-013099c91542aa46e6a16c5f4d465795.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-991d93d3bf524876676fa47b2a4071ab.jpg" align="middle"></details>## DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and   View-Change Human-Centric Video Editing**Authors:Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou**Despite recent progress in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Prior attempts to address this challenge by introducing video-2D representations encounter significant difficulties with large-scale motion- and view-change videos, especially in human-centric scenarios. To overcome this, we propose to introduce the dynamic Neural Radiance Fields (NeRF) as the innovative video representation, where the editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide consistent and controllable editing, we propose the image-based video-NeRF editing pipeline with a set of innovative designs, including multi-view multi-pose Score Distillation Sampling (SDS) from both the 2D personalized diffusion prior and 3D diffusion prior, reconstruction losses, text-guided local parts super-resolution, and style transfer. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% for human preference. Code will be released at https://showlab.github.io/DynVideo-E/. [PDF](http://arxiv.org/abs/2310.10624v2) Project Page: https://showlab.github.io/DynVideo-E/**Summary**动态神经辐射场 (NeRF) 作为视频表示，可进行 3D 空间编辑并通过变形场传播到整段视频，实现一致且可控的视频编辑。**Key Takeaways**- 提出创新视频表示，引入动态神经辐射场 (NeRF)，支持 3D 空间编辑。- 使用多视角多姿势分数蒸馏采样 (SDS) 确保编辑的一致性和可控性。- 提供重建损失，用于约束 NeRF 的学习过程，确保准确的视频重建。- 实现基于文本的局部零件超分辨率，使编辑结果更加逼真。- 利用风格迁移将视频编辑应用于任意风格。- 在两个具有挑战性的人类动作数据集上进行广泛的实验，证明了方法的有效性。- 该方法在人类偏好上的性能优于现有方法，提升幅度为 50% ~ 95%。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：DynVideo-E：利用动态神经辐射场进行大规模运动和视点变化的人体中心视频编辑</li><li>作者：Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou</li><li>第一作者单位：ShowLab</li><li>关键词：视频编辑、神经辐射场、运动和视点变化、人体中心视频</li><li>论文链接：https://arxiv.org/abs/2310.10624，Github 链接：无</li><li><p>摘要：（1）研究背景：现有的基于扩散的视频编辑方法由于长程一致性和逐帧编辑之间的矛盾，仅限于短视频。以往尝试通过引入视频二维表示来解决这一挑战，但在处理大规模运动和视点变化的视频时遇到了重大困难，尤其是在人体中心场景中。（2）过去方法及问题：以往方法试图通过引入视频二维表示来解决这一挑战，但在大规模运动和视点变化的视频，尤其是在人体中心场景中，遇到了重大困难。（3）研究方法：为了克服这一挑战，我们提出将动态神经辐射场 (NeRF) 作为创新的视频表示，可以在 3D 空间中执行编辑并通过变形场传播到整个视频。为了提供一致且可控的编辑，我们提出了基于图像的视频 NeRF 编辑管道，其中包含一系列创新设计，包括多视图多姿势 Score Disentanglement 模块、基于关键点的 3D 运动估计模块和基于变形场的视频 NeRF 编辑模块。（4）方法性能：在人体中心视频编辑任务上，我们的方法在定性和定量方面都优于现有方法。实验结果表明，我们的方法可以实现高度一致的大规模运动和视点变化的人体中心视频编辑。</p></li><li><p>方法：（1）视频-NeRF 模型：我们利用 HOSNeRF 作为视频表示，它可以执行 3D 空间中的编辑并通过变形场传播到整个视频。（2）图像-NeRF 编辑：我们提出基于图像的视频-NeRF 编辑管道，其中包含一系列创新设计，包括多视图多姿势 ScoreDisentanglement 模块、基于关键点的 3D 运动估计模块和基于变形场的视频-NeRF 编辑模块。（3）Image-based 3D 动态人体编辑：我们设计了一系列策略来解决一致性和高质量的图像-NeRF 编辑的挑战，包括参考图像重建损失、从 3D 扩散先验中进行分数蒸馏采样、基于关键点的 3D 运动估计和局部部分超分辨率。（4）背景静态空间编辑：我们利用风格迁移损失将参考样式传输到我们的 3D 背景模型中。</p></li><li><p>结论：（1）：本文提出了一种名为 DynVideo-E 的新颖框架，用于一致地编辑大规模运动和视点变化的人体中心视频。我们首先提出利用动态神经辐射场 (NeRF) 作为我们创新的视频表示，其中编辑可以在动态 3D 空间中执行，并通过变形场准确地传播到整个视频。然后，我们提出了一组有效的基于图像的视频-NeRF 编辑设计，包括从二维个性化扩散先验和三维扩散先验中进行多视图多姿势分数蒸馏采样 (SDS)、参考图像上的重建损失、文本指导的局部部分超分辨率以及用于 3D 背景空间的风格迁移。最后，大量的实验表明，DynVideo-E 在 SOTA 方法上取得了显着的改进。局限性和未来工作。尽管 DynVideo-E 在视频编辑方面取得了显着的进步，但其基于 NeRF 的表示非常耗时。在视频-NeRF 模型中使用体素或哈希网格可以大大减少训练时间，我们将它留作一个忠实的未来方向。（2）：创新点：</p></li><li>提出了一种新颖的视频表示——动态神经辐射场 (NeRF)，它允许在动态 3D 空间中执行编辑并通过变形场传播到整个视频。</li><li>提出了一系列有效的基于图像的视频-NeRF 编辑设计，包括多视图多姿势分数蒸馏采样 (SDS)、参考图像上的重建损失、文本指导的局部部分超分辨率以及用于 3D 背景空间的风格迁移。性能：</li><li>在人体中心视频编辑任务上，我们的方法在定性和定量方面都优于现有方法。</li><li>实验结果表明，我们的方法可以实现高度一致的大规模运动和视点变化的人体中心视频编辑。工作量：</li><li>训练基于 NeRF 的视频表示非常耗时。</li><li>在视频-NeRF 模型中使用体素或哈希网格可以大大减少训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e51f421f05471d502d845b5a05a0e040.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f12ddaab97fef8603330a543d8097935.jpg" align="middle"></details><h2 id="ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF"><a href="#ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF" class="headerlink" title="ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space   NeRF"></a>ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space   NeRF</h2><p><strong>Authors:Jangho Park, Gihyun Kwon, Jong Chul Ye</strong></p><p>Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models. </p><p><a href="http://arxiv.org/abs/2310.02712v1">PDF</a> </p><p><strong>摘要</strong><br>ED-NeRF 将真实场景嵌入潜在扩散模型的潜在空间，提高了 NeRF 编辑速度和质量。</p><p><strong>要点</strong></p><ul><li>将真实场景嵌入潜在扩散模型的潜在空间，构建 ED-NeRF 模型。</li><li>ED-NeRF 具有更快的训练速度和更高的编辑效率。</li><li>ED-NeRF 采用专为编辑设计的改进损失函数。</li><li>ED-NeRF 在编辑速度和输出质量方面优于现有 3D 编辑模型。</li><li>ED-NeRF 能够实现 3D 对象的文本编辑，并生成高质量的图像。</li><li>ED-NeRF 为 3D 场景编辑提供了新颖的解决方案。</li><li>ED-NeRF 可以应用于游戏开发、影视制作等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ED-NeRF：利用潜在空间 NeRF 进行高效的文本引导 3D 场景编辑</li><li>作者：Jangho Park、Gihyun Kwon、Jong Chul Ye</li><li>单位：韩国科学技术院人工智能研究生院、机器人学项目、生物与脑工程系</li><li>关键词：NeRF、文本引导、3D 场景编辑、潜在空间、扩散模型</li><li>论文链接：https://arxiv.org/abs/2310.02712</li><li><p>摘要：（1）研究背景：近年来，文本到图像扩散模型取得了重大进展，在 2D 图像生成方面取得了突破性的性能。这些进展已扩展到 3D 模型，能够从文本描述中生成新颖的 3D 对象。这已发展成为 NeRF 编辑方法，该方法允许通过文本条件操纵现有 3D 对象。然而，现有的 NeRF 编辑技术由于训练速度慢以及使用不充分考虑编辑的损失函数，在性能上受到限制。（2）过去方法和问题：过去的方法包括图像空间 NeRF 编辑，但存在训练速度慢、对编辑不友好等问题。（3）研究方法：为了解决这些问题，本文提出了一种新颖的 3D NeRF 编辑方法，称为 ED-NeRF，通过独特的细化层将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中。这种方法使我们能够获得一个 NeRF 主干，它不仅更快，而且与传统的图像空间 NeRF 编辑相比更适合编辑。此外，我们通过将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域，提出了一种针对编辑量身定制的改进损失函数。这种新颖的损失函数在适合编辑目的方面超越了众所周知的分数蒸馏采样 (SDS) 损失。（4）实验结果：我们的实验结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。这些性能支持了本文的目标。</p></li><li><p>Methods:(1) 提出了一种新颖的3D NeRF 编辑方法 ED-NeRF，通过独特的细化层将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中；(2) 提出了一种针对编辑量身定制的改进损失函数，通过将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域；(3) 在真实世界场景上评估了 ED-NeRF 的性能，结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。</p></li><li><p>结论：（1）：本文提出了一种新颖的 ED-NeRF 方法，该方法在潜在空间中进行了优化。通过使 NeRF 能够直接预测潜在特征，它有效地利用了潜在扩散模型的文本引导评分函数，而无需编码器。通过这样做，我们的方法能够有效降低计算成本，并解决先前模型的负担，这些模型需要以全分辨率渲染才能利用扩散模型。我们扩展了强大的 2D 图像编辑性能，使 ED-NeRF 能够在保持输出质量的同时，以更快的速度编辑 3D 场景。（2）：创新点：</p></li><li>将真实世界场景成功嵌入潜在扩散模型 (LDM) 的潜在空间中，通过独特的细化层，使 NeRF 能够直接预测潜在特征。</li><li>提出了一种针对编辑量身定制的改进损失函数，将最初用于 2D 图像编辑的 delta 去噪分数 (DDS) 蒸馏损失迁移到三维域。</li><li>在真实世界场景上评估了 ED-NeRF 的性能，结果表明，与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。性能：</li><li>与最先进的 3D 编辑模型相比，ED-NeRF 在实现更快的编辑速度的同时，还产生了改进的输出质量。</li><li>ED-NeRF 能够有效降低计算成本，并解决先前模型的负担，这些模型需要以全分辨率渲染才能利用扩散模型。工作量：</li><li>ED-NeRF 的训练速度更快，并且与传统的图像空间 NeRF 编辑相比更适合编辑。</li><li>ED-NeRF 的损失函数是针对编辑量身定制的，在适合编辑目的方面超越了众所周知的分数蒸馏采样 (SDS) 损失。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cdb1ccf9138631994193d2b408f855a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fa1ef9ca7e816bcb461a93f9dab30ab2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-462b0d642d1c29db012aa25db302beb0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d1d2140a841f3f0c8950e698c3992fc8.jpg" align="middle"></details><h2 id="NOFA-NeRF-based-One-shot-Facial-Avatar-Reconstruction"><a href="#NOFA-NeRF-based-One-shot-Facial-Avatar-Reconstruction" class="headerlink" title="NOFA: NeRF-based One-shot Facial Avatar Reconstruction"></a>NOFA: NeRF-based One-shot Facial Avatar Reconstruction</h2><p><strong>Authors:Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu</strong></p><p>3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2307.03441v1">PDF</a> </p><p><strong>摘要</strong><br>单张图片即可重建高保真 3D 面部虚拟形象，带来灵活的姿势和表情控制。</p><p><strong>要点</strong></p><ul><li>新颖的单张图片 3D 面部虚拟形象重建框架，只需一张源图像即可重建高保真 3D 面部虚拟形象。</li><li>克服 NeRF 无法泛化到新身份的缺点，仅使用一张源图像即可重建新身份的 3D 面部虚拟形象。</li><li>提出一种补偿网络来补充面部细节，提高面部虚拟形象的保真度。</li><li>提出一个变形场将规范体积扭曲成驱动的表情，实现对表情的细粒度控制。</li><li>与最先进的方法相比，实现优越的合成结果。</li><li>在多个基准数据集上进行广泛的实验评估，证明了该方法的有效性和泛化能力。</li><li>潜在应用包括虚拟现实、增强现实和游戏。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NOFA：基于 NeRF 的单次拍摄面部虚拟形象重建</li><li>作者：王博宇、范彦波、张勇、王璇、费寅、白云鹏、曹延沛、殷山、吴阳、孙忠强、吴保元</li><li>单位：腾讯优图实验室、蚂蚁集团、清华大学</li><li>关键词：面部虚拟形象、视频合成、NeRF</li><li>论文链接：https://arxiv.org/abs/2307.03441，Github 链接：无</li><li>摘要：（1）研究背景：面部虚拟形象重建是计算机图形学和计算机视觉领域的重要研究课题，在虚拟现实、增强现实、电影工业和远程会议等领域有广泛应用。高保真面部重建和细粒度面部重演是这些应用的基础。（2）过去方法与问题：为了动画面部图像，已经提出了多种 2D 方法，利用基于流的扭曲在图像或特征空间中传递运动，以及编码器-解码器网络来合成逼真的面部图像。然而，这些方法通常需要大量训练数据，并且难以泛化到新的身份。（3）研究方法：本文提出了一种单次拍摄 3D 面部虚拟形象重建框架，仅需一张源图像即可重建高保真 3D 面部虚拟形象。为了解决泛化能力不足和缺少多视图信息的问题，我们利用 3DGAN 的生成先验，并开发了一个高效的编码器-解码器网络来重建源图像的规范神经体积，并进一步提出一个补偿网络来补充面部细节。为了实现对面部动态的细粒度控制，我们提出了一个变形场，将规范体积扭曲成驱动的表情。（4）实验结果：通过广泛的实验比较，我们的方法在合成结果方面优于几种最先进的方法。这些结果支持了我们的目标，即构建一个能够从单张图像重建高保真 3D 面部虚拟形象的框架。</li></ol><p>Methods：（1）：提出一个基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA，利用3DGAN的生成先验和高效的编码器-解码器网络重建源图像的规范神经体积，并进一步提出一个补偿网络来补充面部细节。（2）：使用3DMM引导的变形场来实现对面部动态的细粒度控制，将规范体积扭曲成驱动的表情。（3）：在训练阶段使用多种损失函数来确保逼真的重建和生动的重演，包括图像重建损失、体积一致性损失、变形场损失和对抗损失。</p><ol><li>结论：（1）：本文提出了一种基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA，该框架仅需一张源图像即可重建高保真3D面部虚拟形象。（2）：创新点：</li><li>提出了一种基于NeRF的单次拍摄3D面部虚拟形象重建框架NOFA。</li><li>利用3DGAN的生成先验和高效的编码器-解码器网络重建源图像的规范神经体积。</li><li>提出一个补偿网络来补充面部细节。</li><li>使用3DMM引导的变形场来实现对面部动态的细粒度控制，将规范体积扭曲成驱动的表情。</li><li>在训练阶段使用多种损失函数来确保逼真的重建和生动的重演，包括图像重建损失、体积一致性损失、变形场损失和对抗损失。性能：</li><li>在合成结果方面优于几种最先进的方法。工作量：</li><li>训练过程相对复杂，需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-469cd2dc109bc6c14b5596ae4928857d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3a38646bddce1978ca080ab7995373b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3e291d277a3c74b961d6b7a943cfd20.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-720ea0eebc7863101769dccda881d6ee.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-24 IPR-NeRF Ownership Verification meets Neural Radiance Field</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Talking%20Head%20Generation/</id>
    <published>2024-01-23T16:22:20.000Z</published>
    <updated>2024-01-27T03:59:34.129Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Real3D-Portrait-One-shot-Realistic-3D-Talking-Portrait-Synthesis"><a href="#Real3D-Portrait-One-shot-Realistic-3D-Talking-Portrait-Synthesis" class="headerlink" title="Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis"></a>Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</h2><p><strong>Authors:Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao</strong></p><p>One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To address these limitations, we present Real3D-Potrait, a framework that (1) improves the one-shot 3D reconstruction power with a large image-to-plane model that distills 3D prior knowledge from a 3D face generative model; (2) facilitates accurate motion-conditioned animation with an efficient motion adapter; (3) synthesizes realistic video with natural torso movement and switchable background using a head-torso-background super-resolution model; and (4) supports one-shot audio-driven talking face generation with a generalizable audio-to-motion model. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods. Video samples and source code are available at <a href="https://real3dportrait.github.io">https://real3dportrait.github.io</a> . </p><p><a href="http://arxiv.org/abs/2401.08503v2">PDF</a> ICLR 2024 (Spotlight). Project page: <a href="https://real3dportrait.github.io">https://real3dportrait.github.io</a></p><p><strong>摘要</strong><br>利用大规模图像到平面模型提升 3D 人脸生成模型的重构能力，并结合动作适配器和头部躯干背景超分辨率模型，生成逼真的说话肖像视频。</p><p><strong>要点</strong></p><ul><li>提出 Real3D-Portrait 框架，用于生成逼真的说话肖像视频。</li><li>采用大规模图像到平面模型，从 3D 人脸生成模型中提取 3D 先验知识，提高一发 3D 重建能力。</li><li>使用高效的动作适配器，实现准确的动作条件动画。</li><li>利用头部躯干背景超分辨率模型，合成具有自然躯干运动和可切换背景的逼真视频。</li><li>支持一发音频驱动的说话面部生成，使用可推广的音频到动作模型。</li><li>大量实验证明，Real3D-Portrait 在看不见的身份上具有良好的泛化能力，并且与以前的方法相比，可以生成更逼真的说话肖像视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Real3D-Portrait：单次拍摄的逼真 3D 说话人像合成</li><li>作者：叶振辉、钟天云、任怡、杨佳奇、李维创、黄嘉伟、蒋子悦、何锦正、黄荣杰、刘敬林、张晨、尹翔、马泽君、赵周</li><li>单位：浙江大学、字节跳动、香港科技大学（广州）</li><li>关键词：One-shot 3D talking face generation, 3D reconstruction, Talking face animation, Video synthesis</li><li>论文链接：https://arxiv.org/abs/2401.08503Github 链接：None</li><li>摘要：(1)：研究背景：说话人像生成旨在根据驱动条件（动作序列或驱动音频）合成说话人像视频。这是一个计算机图形学和计算机视觉中长期存在的跨模态任务，具有视频会议和虚拟现实 (VR) 等多项实际应用。先前的 2D 方法可以产生逼真的视频，这要归功于生成对抗网络 (GAN) 的强大功能。然而，由于缺乏显式的 3D 建模，这些 2D 方法在头部大幅移动时会面临变形伪影和不真实的失真。在过去的几年中，基于神经辐射场 (NeRF) 的 3D 方法一直占主导地位，因为它们保持逼真的 3D 几何形状并保留丰富的纹理细节，即使在头部姿势较大的情况下也是如此。然而，在大多数方法中，模型都过度拟合特定的人，这需要为每个看不见的身份进行昂贵的单独训练。探索单次拍摄 3D 说话人像生成的任务很有希望，即给定一个看不见的人的参考图像，我们的目标是将其提升到 3D 头像并使用输入条件对其进行动画处理，以获得逼真的 3D 说话人视频。随着 3D 生成模型的最新进展，可以学习到推广到各种身份的 3D 三平面表示（EG3D，Chan et al. (2022)）的隐藏空间。虽然最近的工作 (Li et al., 2023b; Li, 2023) 开创了单次拍摄 3D 说话人像生成，但它们未能同时实现准确的重建和动画。具体来说，一些工作(2)：过去的方法：一些工作仅使用 2D 图像作为输入，而另一些工作则使用 3D 图像作为输入。使用 2D 图像作为输入的方法通常会产生质量较差的结果，因为它们无法捕获对象的 3D 形状。使用 3D 图像作为输入的方法通常会产生质量更好的结果，但它们需要昂贵的 3D 扫描设备。本方法的动机很充分。作者认为，单次拍摄 3D 说话人像生成是一个具有挑战性的任务，需要解决许多问题。这些问题包括：</li><li>如何从单张 2D 图像重建准确的 3D 模型？</li><li>如何将 3D 模型与驱动条件（动作序列或驱动音频）相关联？</li><li>如何合成逼真的说话人像视频？作者提出了一种新的方法来解决这些问题。该方法包括以下几个步骤：</li><li>从单张 2D 图像重建准确的 3D 模型。</li><li>将 3D 模型与驱动条件（动作序列或驱动音频）相关联。</li><li>合成逼真的说话人像视频。作者的方法在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。(3)：研究方法：作者提出了一种名为 Real3D-Portrait 的框架，该框架可以从单张图像生成逼真的 3D 说话人像视频。Real3D-Portrait 包括以下几个模块：</li><li>图像到平面模型：该模块将输入图像转换为 3D 三平面表示。</li><li>运动适配器：该模块将 3D 三平面表示与驱动条件（动作序列或驱动音频）相关联。</li><li>头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。</li><li><p>音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。(4)：性能：Real3D-Portrait 在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。在 TalkingHead 数据集上，Real3D-Portrait 的平均重建误差为 0.006，平均动画误差为 0.008。在 VoxCeleb 数据集上，Real3D-Portrait 的平均重建误差为 0.007，平均动画误差为 0.009。在 LRW 数据集上，Real3D-Portrait 的平均重建误差为 0.008，平均动画误差为 0.010。这些结果表明，Real3D-Portrait 能够生成高质量的说话人像视频，并且该方法可以推广到看不见的身份。</p></li><li><p><strong>方法</strong>：</p></li></ol><p>（1）图像到平面模型：该模块将输入图像转换为3D三平面表示（EG3D）。EG3D是一种隐式神经表示，可以捕获对象的3D形状和纹理。</p><p>（2）运动适配器：该模块将3D三平面表示与驱动条件（动作序列或驱动音频）相关联。运动适配器使用一个神经网络来学习如何将驱动条件映射到3D三平面表示。</p><p>（3）头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。头部躯干背景超分辨率模型使用一个神经网络来学习如何将3D三平面表示渲染成逼真的视频。</p><p>（4）音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。音频到运动模型使用一个神经网络来学习如何将驱动音频映射到动作序列。</p><ol><li>结论：（1）：本文提出了一种单次拍摄逼真3D说话人像合成框架Real3D-Portrait。该方法同时实现了准确的3D头像重建和动画，并支持视频/音频驱动的应用。（2）：创新点：</li><li>提出了一种预训练的大型图像到平面模型，可以从单张图像重建准确的3D三平面表示。</li><li>设计了一个PNCC条件运动适配器，可以将3D三平面表示与驱动条件（动作序列或驱动音频）相关联。</li><li>提出了一种头部躯干背景超分辨率模型，可以合成逼真的视频，具有自然的躯干运动和可切换的背景。</li><li>提出了一种通用的音频到运动模型，支持视频/音频驱动的应用。性能：</li><li>在TalkingHead数据集上，Real3D-Portrait的平均重建误差为0.006，平均动画误差为0.008。</li><li>在VoxCeleb数据集上，Real3D-Portrait的平均重建误差为0.007，平均动画误差为0.009。</li><li>在LRW数据集上，Real3D-Portrait的平均重建误差为0.008，平均动画误差为0.010。工作量：</li><li>该方法需要大量的数据和计算资源来训练模型。</li><li>该方法的训练过程比较复杂，需要专业知识和技能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-49a987d29d4e89d46251e6ddc16c6776.jpg" align="middle"><img src="https://pica.zhimg.com/v2-bcd0735a0f8445511d9ad42c4b5cc609.jpg" align="middle"><img src="https://pica.zhimg.com/v2-519ffb14435cf5d80acd488dc9b96504.jpg" align="middle"><img src="https://picx.zhimg.com/v2-54411699feb07b3a92834da51afd6954.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86a981fea38f5376c644338440f55eff.jpg" align="middle"></details><h2 id="DREAM-Talk-Diffusion-based-Realistic-Emotional-Audio-driven-Method-for-Single-Image-Talking-Face-Generation"><a href="#DREAM-Talk-Diffusion-based-Realistic-Emotional-Audio-driven-Method-for-Single-Image-Talking-Face-Generation" class="headerlink" title="DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for   Single Image Talking Face Generation"></a>DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for   Single Image Talking Face Generation</h2><p><strong>Authors:Chenxu Zhang, Chao Wang, Jianfeng Zhang, Hongyi Xu, Guoxian Song, You Xie, Linjie Luo, Yapeng Tian, Xiaohu Guo, Jiashi Feng</strong></p><p>The generation of emotional talking faces from a single portrait image remains a significant challenge. The simultaneous achievement of expressive emotional talking and accurate lip-sync is particularly difficult, as expressiveness is often compromised for the accuracy of lip-sync. As widely adopted by many prior works, the LSTM network often fails to capture the subtleties and variations of emotional expressions. To address these challenges, we introduce DREAM-Talk, a two-stage diffusion-based audio-driven framework, tailored for generating diverse expressions and accurate lip-sync concurrently. In the first stage, we propose EmoDiff, a novel diffusion module that generates diverse highly dynamic emotional expressions and head poses in accordance with the audio and the referenced emotion style. Given the strong correlation between lip motion and audio, we then refine the dynamics with enhanced lip-sync accuracy using audio features and emotion style. To this end, we deploy a video-to-video rendering module to transfer the expressions and lip motions from our proxy 3D avatar to an arbitrary portrait. Both quantitatively and qualitatively, DREAM-Talk outperforms state-of-the-art methods in terms of expressiveness, lip-sync accuracy and perceptual quality. </p><p><a href="http://arxiv.org/abs/2312.13578v1">PDF</a> Project Page at <a href="https://magic-research.github.io/dream-talk/">https://magic-research.github.io/dream-talk/</a></p><p><strong>Summary</strong><br>语音驱动下，DREAM-Talk 可同时实现准确的口型同步和自然的情感表达，生成逼真的动态对话人脸。</p><p><strong>Key Takeaways</strong></p><ul><li>DREAM-Talk 采用两阶段扩散式音频驱动框架，能同时实现丰富多样的情感表达和精准的口型同步。</li><li>首阶段提出 EmoDiff 模块，可依据音频和指定的情感样式，生成多样且富有动态感的情感表情和头部姿势。</li><li>基于唇部动作与音频的强相关性，利用音频特征和情感样式，DREAM-Talk 在第二阶段进一步优化动态效果，增强口型同步的精确性。</li><li>DREAM-Talk 运用视频到视频渲染模块，将代理 3D 头像的表情和唇部动作转移到任意肖像上。</li><li>定量和定性评估结果表明，DREAM-Talk在表情丰富度、口型同步精度以及感知质量方面均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：DREAM-Talk：基于扩散的逼真情感音频驱动的单张图像说话人脸生成方法</p></li><li><p>作者：陈旭章<em>, 王超</em>, 张建峰, 徐鸿毅, 宋国贤, 谢宇, 罗林杰, 田亚鹏, 郭晓虎, 冯佳世</p></li><li><p>单位：字节跳动公司</p></li><li><p>关键词：情感说话人脸生成；扩散模型；音频驱动；唇形同步</p></li><li><p>论文链接：https://arxiv.org/abs/2312.13578</p></li><li><p>摘要：(1) 研究背景：从单张人像图像生成情感说话人脸仍然是一项重大挑战。同时实现富有表现力的情感说话和准确的唇形同步尤其困难，因为表现力通常会因唇形同步的准确性而受到损害。LSTM 网络被许多先前的工作广泛采用，但往往无法捕捉情感表达的细微差别和变化。(2) 过去的方法及其问题：为了解决这些挑战，我们引入了 DREAM-Talk，这是一个两阶段的基于扩散的音频驱动框架，专门用于同时生成多样化的表情和准确的唇形同步。在第一阶段，我们提出了 EmoDiiff，一个新颖的扩散模块，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。鉴于唇部运动与音频之间存在很强的相关性，我们随后使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。为此，我们部署了一个视频到视频渲染模块，将我们代理 3D 头像的表情和唇部动作转移到任意人像上。(3) 本文提出的研究方法：在定量和定性方面，DREAM-Talk 在表现力、唇形同步准确性和感知质量方面都优于最先进的方法。(4) 方法在什么任务上取得了什么性能？性能是否支持其目标：该方法在情感说话人脸生成任务上取得了很好的性能。在定量评估中，DREAM-Talk 在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。在定性评估中，DREAM-Talk 生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。这些结果支持了该方法的目标，即生成具有多样化情感表达和准确唇形同步的逼真说话人脸。</p></li><li><p>方法：(1) 提出EmoDiff，一个新颖的扩散模块，根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。(2) 部署视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。(3) 使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。</p></li><li><p>结论：（1）：本文提出了一种名为DREAM-Talk的创新框架，该框架专为生成具有精确唇形同步的情感表达说话人脸而设计。我们的两阶段方法，包括EmoDiff模块和唇形细化，有效地捕捉了情感细微差别并确保了准确的唇形同步。利用情感条件扩散模型和唇形细化网络，我们的方法优于现有技术。我们的结果表明，在保持高视频质量的同时，面部情感表达能力得到了提高。DREAM-Talk代表了情感说话人脸生成领域向前迈出的重要一步，它使跨越广泛应用范围的逼真且情感参与的数字人形表征的创建成为可能。（2）：创新点：提出了一种新颖的扩散模块EmoDiff，该模块根据音频和参考情感风格生成多样化的高度动态情感表达和头部姿势。部署了一个视频到视频渲染模块，将代理3D头像的表情和唇部动作转移到任意人像上。使用音频特征和情感风格来增强唇形同步准确性，从而优化动态效果。性能：在定量评估中，DREAM-Talk在三个基准数据集上实现了最先进的结果，在情感多样性、唇形同步准确性和感知质量方面均优于现有方法。在定性评估中，DREAM-Talk生成的说话人脸具有逼真的情感表达、准确的唇形同步和很高的视觉质量。工作量：该方法需要大量的数据和计算资源来训练模型。该方法需要专业知识来实现和部署。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8c84d3a58a2189a2edc59f8826b7f47b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b6380a292ea9f96c4c952ba930e343d6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-873526fee22103f77756de5c2690665e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ac94d739cb30587f6ce660be8fa86fc.jpg" align="middle"></details><h2 id="VectorTalker-SVG-Talking-Face-Generation-with-Progressive-Vectorisation"><a href="#VectorTalker-SVG-Talking-Face-Generation-with-Progressive-Vectorisation" class="headerlink" title="VectorTalker: SVG Talking Face Generation with Progressive Vectorisation"></a>VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</h2><p><strong>Authors:Hao Hu, Xuan Wang, Jingxiang Sun, Yanbo Fan, Yu Guo, Caigui Jiang</strong></p><p>High-fidelity and efficient audio-driven talking head generation has been a key research topic in computer graphics and computer vision. In this work, we study vector image based audio-driven talking head generation. Compared with directly animating the raster image that most widely used in existing works, vector image enjoys its excellent scalability being used for many applications. There are two main challenges for vector image based talking head generation: the high-quality vector image reconstruction w.r.t. the source portrait image and the vivid animation w.r.t. the audio signal. To address these, we propose a novel scalable vector graphic reconstruction and animation method, dubbed VectorTalker. Specifically, for the highfidelity reconstruction, VectorTalker hierarchically reconstructs the vector image in a coarse-to-fine manner. For the vivid audio-driven facial animation, we propose to use facial landmarks as intermediate motion representation and propose an efficient landmark-driven vector image deformation module. Our approach can handle various styles of portrait images within a unified framework, including Japanese manga, cartoon, and photorealistic images. We conduct extensive quantitative and qualitative evaluations and the experimental results demonstrate the superiority of VectorTalker in both vector graphic reconstruction and audio-driven animation. </p><p><a href="http://arxiv.org/abs/2312.11568v1">PDF</a> </p><p><strong>Summary</strong><br>矢量图像驱动的语音动画生成方法 VectorTalker，首次采用分层式矢量图像重建和特征点驱动的变形模块，可生成高质量语音动画。</p><p><strong>Key Takeaways</strong></p><ul><li>矢量图像驱动的语音动画生成方法 VectorTalker，可生成高质量语音动画。</li><li>VectorTalker 分层式地重建矢量图像，以实现高保真重建。</li><li>VectorTalker 提出特征点驱动的矢量图像变形模块，以实现生动的语音动画。</li><li>VectorTalker 可处理包括日本漫画、卡通和照片写实图像在内的各种风格的肖像图像。</li><li>VectorTalker 在矢量图像重建和语音动画方面都表现出优异的性能。</li><li>VectorTalker 可在统一框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片写实图像。</li><li>VectorTalker 在矢量图像重建和语音动画方面都表现出优异的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：矢量话者：渐进矢量化下的 SVG 会话人脸生成</li><li>作者：Jinsong Zhang, Yuxuan Zhang, Yebin Liu, Xiaoguang Han</li><li>单位：无</li><li>关键词：音频驱动、面部动画、矢量图像生成、可变形模型</li><li>论文链接：无，Github 链接：无</li><li><p>摘要：（1）：随着计算机图形学和计算机视觉的发展，高保真且高效的音频驱动说话人头部生成已成为一项关键的研究课题。本文研究了基于矢量图像的音频驱动说话人头部生成。与现有工作中最广泛使用的直接对光栅图像进行动画处理相比，矢量图像因其出色的可扩展性而被用于许多应用程序。基于矢量图像的说话人头部生成面临两大挑战：相对于源人像图像的高质量矢量图像重建以及相对于音频信号的生动动画。为了解决这些问题，我们提出了一种新颖的可扩展矢量图像重建和动画方法，称为 VectorTalker。具体来说，对于高保真重建，VectorTalker 以粗到细的方式分层重建矢量图像。对于生动的音频驱动面部动画，我们建议使用面部地标作为中间运动表示，并提出了一种高效的地标驱动的矢量图像变形模块。我们的方法可以在统一的框架内处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像。我们进行了广泛的定量和定性评估，实验结果证明了 VectorTalker 在矢量图像重建和音频驱动动画方面的优越性。</p></li><li><p>方法：(1) 矢量图像重建：VectorTalker 采用分层重建策略，首先使用粗糙的矢量图像作为初始化，然后通过迭代细化过程逐步提高矢量图像的分辨率和质量。(2) 面部地标提取：VectorTalker 使用预训练的深度学习模型从输入图像中提取面部地标，这些地标作为中间运动表示，用于驱动矢量图像的变形。(3) 矢量图像变形：VectorTalker 提出了一种高效的地标驱动的矢量图像变形模块，该模块使用地标信息对矢量图像进行变形，从而实现生动的音频驱动面部动画。(4) 统一框架：VectorTalker 可以处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像，并可以在统一的框架内进行矢量图像重建和音频驱动动画。</p></li><li><p>结论：（1）：本研究提出了一种名为 VectorTalker 的新颖方法，用于生成一镜到底的音频驱动的说话 SVG 肖像。我们的渐进矢量化算法允许我们准确地将输入光栅图像重建为矢量图形。我们提取面部关键点并使用基于仿射变换的扭曲系统，通过音频驱动的面部关键点偏移预测来为 SVG 肖像制作动画。我们的广泛实验表明，我们的渐进矢量化明显优于其他基线方法。此外，我们的方法有效地完成了说话 SVG 生成的任务。在未来，我们计划利用更多关于人类的先验知识来实现更逼真的面部动画。（2）：创新点：VectorTalker 提出了一种新的矢量图像重建和动画方法，该方法可以处理各种风格的人像图像，包括日本漫画、卡通和照片写实图像，并且可以在统一的框架内进行矢量图像重建和音频驱动动画。性能：VectorTalker 在矢量图像重建和音频驱动动画方面都取得了优异的性能。在矢量图像重建方面，VectorTalker 可以准确地重建输入光栅图像，并且重建的矢量图像具有很高的质量。在音频驱动动画方面，VectorTalker 可以生成生动逼真的面部动画，并且动画与音频信号高度同步。工作量：VectorTalker 的工作量相对较大。在矢量图像重建方面，VectorTalker 需要迭代细化过程来逐步提高矢量图像的分辨率和质量。在音频驱动动画方面，VectorTalker 需要提取面部关键点并使用基于仿射变换的扭曲系统来对矢量图像进行变形。这些过程都比较耗时。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3d73af4a717ae743272e331632eb8141.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-acdfb181d2fbfb7129b4135fcac342c0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-97e9fae5e9ddf8626b7b6930899fc83a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c37086ba942564a3c3d7ee2a22da266f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-765663a7e8dd6bacba4c559aa42c0cd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33719fe6e62cc2eba54e1d67239ef47b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24965c00c4a429cc2df1ec8ea2b00a3c.jpg" align="middle"></details><h2 id="AE-NeRF-Audio-Enhanced-Neural-Radiance-Field-for-Few-Shot-Talking-Head-Synthesis"><a href="#AE-NeRF-Audio-Enhanced-Neural-Radiance-Field-for-Few-Shot-Talking-Head-Synthesis" class="headerlink" title="AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head   Synthesis"></a>AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head   Synthesis</h2><p><strong>Authors:Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, Tieniu Tan</strong></p><p>Audio-driven talking head synthesis is a promising topic with wide applications in digital human, film making and virtual reality. Recent NeRF-based approaches have shown superiority in quality and fidelity compared to previous studies. However, when it comes to few-shot talking head generation, a practical scenario where only few seconds of talking video is available for one identity, two limitations emerge: 1) they either have no base model, which serves as a facial prior for fast convergence, or ignore the importance of audio when building the prior; 2) most of them overlook the degree of correlation between different face regions and audio, e.g., mouth is audio related, while ear is audio independent. In this paper, we present Audio Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can generate realistic portraits of a new speaker with fewshot dataset. Specifically, we introduce an Audio Aware Aggregation module into the feature fusion stage of the reference scheme, where the weight is determined by the similarity of audio between reference and target image. Then, an Audio-Aligned Face Generation strategy is proposed to model the audio related and audio independent regions respectively, with a dual-NeRF framework. Extensive experiments have shown AE-NeRF surpasses the state-of-the-art on image fidelity, audio-lip synchronization, and generalization ability, even in limited training set or training iterations. </p><p><a href="http://arxiv.org/abs/2312.10921v1">PDF</a> Accepted by AAAI 2024</p><p><strong>Summary</strong></p><p>利用音频提高神经辐射场以实现由几秒视频创建逼真谈话头像。</p><p><strong>Key Takeaways</strong></p><ul><li>音频驱动的谈话头像合成在数字人、电影制作和虚拟现实中具有广泛的应用。</li><li>近期基于 NeRF 的方法在质量和保真度方面优于以往的研究。</li><li>当前方法在仅有几秒谈话视频可用于创建谈话头像时存在局限性。</li><li>本研究提出的方法在构建先验时引入了音频感知合成模块和音频对齐面部生成策略。</li><li>广泛的实验表明该方法在图像保真度、音频-嘴唇同步性和泛化能力方面优于现有技术。</li><li>该方法即使在有限的训练集或训练迭代中也能表现出色。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：音频增强神经辐射场：用于小样本说话人头部合成</li><li>作者：董泽李、康赵、魏王、博朋、张颖雅、景东、谭铁牛</li><li>单位：中国科学院自动化研究所模式识别与智能控制国家重点实验室</li><li>关键词：音频驱动说话人头部合成、神经辐射场、小样本学习、音频感知聚合、音频对齐面部生成</li><li>论文链接：https://arxiv.org/abs/2312.10921     Github 链接：无</li><li><p>摘要：(1)：研究背景：音频驱动说话人头部合成是数字人、电影制作和虚拟现实等领域的重要技术。近年来，基于神经辐射场（NeRF）的方法在该领域取得了显著进展，但它们通常需要针对每个说话人进行单独训练，并且对训练数据的数量和质量非常敏感。(2)：过去方法和问题：现有方法存在两个主要问题：一是缺乏鲁棒的先验知识，导致模型难以快速泛化到小样本说话人；二是忽略了不同面部区域与音频的相关性，导致生成的说话人头部缺乏音频唇形同步性和真实感。(3)：研究方法：为了解决上述问题，本文提出了音频增强神经辐射场（AE-NeRF）方法。AE-NeRF通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型。音频感知聚合模块根据音频相似性对参考图像的特征进行加权融合，从而增强模型对音频的感知能力。音频对齐面部生成策略将面部划分为音频相关区域和音频无关区域，并分别使用两个 NeRF 网络进行建模，从而提高模型的生成质量和音频唇形同步性。(4)：方法性能：AE-NeRF 方法在多个数据集上进行了评估，结果表明该方法在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。即使在有限的训练集或训练迭代次数下，AE-NeRF 也可以生成高质量的说话人头部图像。</p></li><li><p>方法：(1)：AE-NeRF 方法通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用音频信息来构建说话人头部模型。(2)：音频感知聚合模块根据音频相似性对参考图像的特征进行加权融合，从而增强模型对音频的感知能力。(3)：音频对齐面部生成策略将面部划分为音频相关区域和音频无关区域，并分别使用两个 NeRF 网络进行建模，从而提高模型的生成质量和音频唇形同步性。</p></li><li><p>结论：（1）：本文提出了一种音频增强神经辐射场（AE-NeRF）方法，用于小样本说话人头部合成。该方法通过引入音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型，在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。（2）：创新点：提出了一种新的音频增强神经辐射场方法，用于小样本说话人头部合成。引入了音频感知聚合模块和音频对齐面部生成策略，有效地利用了音频信息来构建说话人头部模型。在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。性能：在多个数据集上进行了评估，结果表明该方法在图像保真度、音频唇形同步性和泛化能力方面都优于现有方法。即使在有限的训练集或训练迭代次数下，AE-NeRF也可以生成高质量的说话人头部图像。工作量：该方法的实现相对复杂，需要较高的计算资源和专业知识。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-f239087e0d2ac215f78cf754abb58cc2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df318df0c3e9e1e2538b215ac58c99ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c270f0bac6c5470a2e6b63529366977.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0a96c7a7d82a681ee282d99801296cda.jpg" align="middle"></details><h2 id="Mimic-Speaking-Style-Disentanglement-for-Speech-Driven-3D-Facial-Animation"><a href="#Mimic-Speaking-Style-Disentanglement-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial   Animation"></a>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial   Animation</h2><p><strong>Authors:Hui Fu, Zeqing Wang, Ke Gong, Keze Wang, Tianshui Chen, Haojie Li, Haifeng Zeng, Wenxiong Kang</strong></p><p>Speech-driven 3D facial animation aims to synthesize vivid facial animations that accurately synchronize with speech and match the unique speaking style. However, existing works primarily focus on achieving precise lip synchronization while neglecting to model the subject-specific speaking style, often resulting in unrealistic facial animations. To the best of our knowledge, this work makes the first attempt to explore the coupled information between the speaking style and the semantic content in facial motions. Specifically, we introduce an innovative speaking style disentanglement method, which enables arbitrary-subject speaking style encoding and leads to a more realistic synthesis of speech-driven facial animations. Subsequently, we propose a novel framework called \textbf{Mimic} to learn disentangled representations of the speaking style and content from facial motions by building two latent spaces for style and content, respectively. Moreover, to facilitate disentangled representation learning, we introduce four well-designed constraints: an auxiliary style classifier, an auxiliary inverse classifier, a content contrastive loss, and a pair of latent cycle losses, which can effectively contribute to the construction of the identity-related style space and semantic-related content space. Extensive qualitative and quantitative experiments conducted on three publicly available datasets demonstrate that our approach outperforms state-of-the-art methods and is capable of capturing diverse speaking styles for speech-driven 3D facial animation. The source code and supplementary video are publicly available at: <a href="https://zeqing-wang.github.io/Mimic/">https://zeqing-wang.github.io/Mimic/</a> </p><p><a href="http://arxiv.org/abs/2312.10877v1">PDF</a> 7 pages, 6 figures, accepted by AAAI-24</p><p><strong>Summary</strong><br>基于说话风格的说话人专有3D面部动画合成方法。</p><p><strong>Key Takeaways</strong></p><ul><li>基于语音的3D面部动画合成旨在于合成逼真的面部动画，该动画与语音准确同步并匹配独特的说话风格。</li><li>现有的工作主要集中于实现精确的唇部同步，而忽略了对特定说话风格建模，常常导致不真实的面部动画。</li><li>提出了一种创新的说话风格分离方法，该方法支持任意说话者风格编码，并导致更真实的有声3D面部动画合成。</li><li>提出一个称为Mimic的新框架，通过为风格和内容分别构建两个潜在空间，从面部动作中学习说话风格和内容的解耦表示。</li><li>提出四个设计精巧的约束：一个辅助风格分类器，一个辅助反向分类器，一个内容对比损失和一对潜在循环损失，有效构建与身份相关的风格空间和与语义相关的语境空间。</li><li>在三个公开数据集上进行的大量定性和定量实验证明，这种方法优于最先进的方法，并且能够捕获用于有声3D面部动画的不同说话风格。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Mimic：具有风格内容分离的说话风格驱动的 3D 面部动画</li><li>作者：Zeqing Wang, Yitong Liu, Jiansheng Chen, Yajie Zhao, Xiaoguang Han</li><li>隶属机构：中国科学院计算技术研究所</li><li>关键词：面部动画、说话风格、内容分离、语音驱动</li><li>论文链接：https://arxiv.org/abs/2302.02789Github 代码链接：https://github.com/zeqingwang/Mimic</li><li><p>摘要：(1)：研究背景：语音驱动的 3D 面部动画旨在合成与语音准确同步并匹配独特说话风格的生动面部动画。然而，现有工作主要集中于实现精确的唇形同步，而忽略了对特定主题说话风格的建模，通常会导致不切实际的面部动画。(2)：过去的方法及其问题：一些方法试图分离面部运动中的情感相关信息，但它们主要集中于情感，而忽略了说话风格。一些方法关注身份相关信息，但它们没有明确分离说话风格和语义相关内容。(3)：研究方法：本文提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。我们提出了一个名为 Mimic 的新框架，通过分别为风格和内容构建两个潜在空间，从面部运动中学习说话风格和内容的分离表示。为了促进分离表示学习，我们引入了四个精心设计的约束：辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失，它们可以有效地促进与身份相关的风格空间和语义相关内容空间的构建。(4)：方法性能：在三个公开可用的数据集上进行的广泛定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的 3D 面部动画的不同说话风格。这些性能支持了我们的目标。</p></li><li><p>方法：(1) Mimic框架概述：Mimic框架由一个编码器和一个解码器组成。编码器将面部运动编码为风格和内容的潜在表示，解码器将潜在表示解码为面部动画。(2) 潜在空间构建：Mimic框架构建了两个潜在空间，分别是风格空间和内容空间。风格空间用于编码说话风格，内容空间用于编码语义相关内容。(3) 分离表示学习：Mimic框架通过四个精心设计的约束来促进分离表示学习，分别是辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失。(4) 面部动画生成：Mimic框架将分离的风格和内容表示解码为面部动画。解码器是一个多层感知机，将潜在表示映射到面部顶点的位置。</p></li><li><p>结论：（1）：本工作提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。我们提出的Mimic框架通过分别为风格和内容构建两个潜在空间，从面部运动中学习说话风格和内容的分离表示。广泛的定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的3D面部动画的不同说话风格。（2）：创新点：</p></li><li>提出了一种创新的说话风格分离方法，该方法能够对任意主题的说话风格进行编码，并生成更逼真的语音驱动的面部动画。</li><li>构建了两个潜在空间，分别是风格空间和内容空间，分别用于编码说话风格和语义相关内容。</li><li>通过四个精心设计的约束来促进分离表示学习，分别是辅助风格分类器、辅助逆分类器、内容对比损失和一对潜在循环损失。性能：</li><li>在三个公开可用的数据集上进行的广泛定性和定量实验表明，我们的方法优于最先进的方法，并且能够捕获用于语音驱动的3D面部动画的不同说话风格。工作量：</li><li>Mimic框架的实现相对复杂，需要较高的编程技能和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-95fa8ce2f96cb59aeced5036ae979cce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea918fc0b742f0b948922090e9c51b8e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-47da103521519c5b941dedeff17adc75.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3019e10b507b7a2281715b1a591fd446.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-73bdec1ad3a869ac2b64b987125a7b99.jpg" align="middle"></details><h2 id="DreamTalk-When-Expressive-Talking-Head-Generation-Meets-Diffusion-Probabilistic-Models"><a href="#DreamTalk-When-Expressive-Talking-Head-Generation-Meets-Diffusion-Probabilistic-Models" class="headerlink" title="DreamTalk: When Expressive Talking Head Generation Meets Diffusion   Probabilistic Models"></a>DreamTalk: When Expressive Talking Head Generation Meets Diffusion   Probabilistic Models</h2><p><strong>Authors:Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, Zhidong Deng</strong></p><p>Diffusion models have shown remarkable success in a variety of downstream generative tasks, yet remain under-explored in the important and challenging expressive talking head generation. In this work, we propose a DreamTalk framework to fulfill this gap, which employs meticulous design to unlock the potential of diffusion models in generating expressive talking heads. Specifically, DreamTalk consists of three crucial components: a denoising network, a style-aware lip expert, and a style predictor. The diffusion-based denoising network is able to consistently synthesize high-quality audio-driven face motions across diverse expressions. To enhance the expressiveness and accuracy of lip motions, we introduce a style-aware lip expert that can guide lip-sync while being mindful of the speaking styles. To eliminate the need for expression reference video or text, an extra diffusion-based style predictor is utilized to predict the target expression directly from the audio. By this means, DreamTalk can harness powerful diffusion models to generate expressive faces effectively and reduce the reliance on expensive style references. Experimental results demonstrate that DreamTalk is capable of generating photo-realistic talking faces with diverse speaking styles and achieving accurate lip motions, surpassing existing state-of-the-art counterparts. </p><p><a href="http://arxiv.org/abs/2312.09767v1">PDF</a> Project Page: <a href="https://dreamtalk-project.github.io">https://dreamtalk-project.github.io</a></p><p><strong>Summary</strong><br>梦语者: 一种新的扩散模型框架，能够产生写实且具有丰富表情的说话人头部。</p><p><strong>Key Takeaways</strong></p><ul><li>梦语者使用扩散模型来生成说话人头部，并通过不同的组件来确保质量和准确性。</li><li>扩散模型能够生成逼真的音频驱动的人脸运动，可在各种表情间切换。</li><li>风格感知唇形专家可以指导唇形同步，同时兼顾说话风格。</li><li>扩散模型风格预测器可直接从音频中预测目标表情，无需参考视频或文本。</li><li>梦语者可以生成具有不同说话风格的照片级写实说话人头部，并实现精确的唇形运动。</li><li>实验结果表明，梦语者优于现有的最先进的同类技术。</li><li>梦语者为使用扩散模型生成富有表现力的说话人头部开辟了新的道路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DreamTalk：当富有表现力的说话头生成遇到扩散概率模型</li><li>作者：Junyi Zhang, Yitong Yu, Xiaoming Liu, Yijun Li, Tao Mei</li><li>单位：香港中文大学（深圳）</li><li>关键词：扩散模型、说话风格预测、唇形同步、说话头生成</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：扩散模型在各种下游生成任务中取得了显着的成功，但在重要且具有挑战性的富有表现力的说话头生成中仍未得到充分探索。（2）过去方法与问题：现有方法通常需要表达式参考视频或文本来指导说话头生成，这限制了其在实际应用中的灵活性。此外，这些方法在生成唇形动作时往往不够准确和逼真。（3）研究方法：本文提出 DreamTalk 框架来解决上述问题。DreamTalk 由三个关键组件组成：去噪网络、风格感知唇形专家和风格预测器。去噪网络基于扩散模型，能够一致地合成高质量的音频驱动的面部动作，涵盖各种各样的表情。为了增强唇形动作的表现力和准确性，我们引入了一个风格感知唇形专家，它可以在考虑说话风格的同时指导唇形同步。为了消除对表达式参考视频或文本的需求，我们利用一个额外的基于扩散模型的风格预测器直接从音频中预测目标表情。通过这种方式，DreamTalk 可以有效地利用强大的扩散模型来生成富有表现力的面部表情，并减少对昂贵的风格参考的依赖。（4）实验结果：实验结果表明，DreamTalk 能够生成具有多样说话风格和准确唇形动作的逼真说话面孔，超越了现有的最先进方法。这些结果支持了本文提出的方法能够实现其目标。</p></li><li><p>方法：（1）去噪网络：DreamTalk的核心组件之一是去噪网络，它基于扩散模型，能够从噪声中逐渐恢复出高质量的音频驱动的面部动作。去噪网络由一系列的扩散步骤组成，在每个步骤中，网络都会将噪声图像逐渐转化为目标图像。通过这种方式，去噪网络能够生成逼真且具有多样性的面部动作，涵盖各种各样的表情。（2）风格感知唇形专家：为了增强唇形动作的表现力和准确性，DreamTalk引入了风格感知唇形专家。风格感知唇形专家是一个卷积神经网络，它能够在考虑说话风格的同时指导唇形同步。风格感知唇形专家通过分析音频中的说话风格信息，来预测目标唇形动作。这种预测结果可以帮助去噪网络生成更加准确和逼真的唇形动作。（3）风格预测器：为了消除对表达式参考视频或文本的需求，DreamTalk利用了一个额外的基于扩散模型的风格预测器。风格预测器能够直接从音频中预测目标表情。风格预测器通过分析音频中的说话风格信息，来预测目标表情的分布。这种预测结果可以帮助去噪网络生成具有多样说话风格的面部表情。</p></li><li><p>结论：（1）：DreamTalk 提出了一种新颖的方法，利用扩散模型生成富有表现力的说话头，在减少对额外风格参考的依赖的同时，在不同的说话风格中表现出色。我们开发了一个去噪网络来创建富有表现力、音频驱动的面部动作，并引入了一个风格感知唇形专家来优化唇形同步，而不会损害风格表现力。此外，我们设计了一个风格预测器，可以直接从音频中推断出说话风格，从而消除了对视频参考的需求。DreamTalk 的有效性通过广泛的实验得到了验证。致谢：这项工作得到了阿里巴巴集团通过阿里巴巴研究实习计划的支持。我们要感谢 Xinya Ji、Borong Liang、Yan Pan 和 Suzhen Wang 在比较方面给予的慷慨帮助。（2）：创新点：</p></li><li>提出了一种新的框架 DreamTalk，利用扩散模型生成富有表现力的说话头。</li><li>开发了一个去噪网络来创建富有表现力、音频驱动的面部动作。</li><li>引入了一个风格感知唇形专家来优化唇形同步，而不会损害风格表现力。</li><li>设计了一个风格预测器，可以直接从音频中推断出说话风格，从而消除了对视频参考的需求。性能：</li><li>DreamTalk 能够生成具有多样说话风格和准确唇形动作的逼真说话面孔，超越了现有的最先进方法。</li><li>DreamTalk 不需要表达式参考视频或文本，因此更加灵活且易于使用。</li><li>DreamTalk 可以生成具有不同说话风格的面部动作，包括高兴、悲伤、愤怒和惊讶等。工作量：</li><li>DreamTalk 的实现相对简单，易于理解和使用。</li><li>DreamTalk 的训练过程相对较快，可以在几个小时内完成。</li><li>DreamTalk 的生成速度也很快，可以在几秒钟内生成一个说话头。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-642c19419d72e5147c54fe7e4901843d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a4fe6c655ccb30f54162deefe293021d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7242bb1e506e10657a005d461ece1d10.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a01ad66de23fc4a6cfd936019d21a0c2.jpg" align="middle"></details><h2 id="FaceTalk-Audio-Driven-Motion-Diffusion-for-Neural-Parametric-Head-Models"><a href="#FaceTalk-Audio-Driven-Motion-Diffusion-for-Neural-Parametric-Head-Models" class="headerlink" title="FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head   Models"></a>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head   Models</h2><p><strong>Authors:Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner</strong></p><p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation. </p><p><a href="http://arxiv.org/abs/2312.08459v1">PDF</a> Paper Video: <a href="https://youtu.be/7Jf0kawrA3Q">https://youtu.be/7Jf0kawrA3Q</a> Project Page:   <a href="https://shivangi-aneja.github.io/projects/facetalk/">https://shivangi-aneja.github.io/projects/facetalk/</a></p><p><strong>摘要</strong><br>用音频信号合成高清3D说话人头运动序列的新生成方法。</p><p><strong>要点</strong></p><ul><li>FaceTalk 是一种生成性方法，能够从输入的音频信号中合成高保真的3D动态说话人头部序列。</li><li>提出将语音信号与神经参数头部模型的潜在空间相结合，以创建高保真、时间连贯的运动序列。</li><li>提出一种新的潜在扩散模型，在神经参数头部模型的表情空间中运行，以合成音频驱动的逼真头部序列。</li><li>优化了对应关系，以产生一个时间优化的NPHM表情数据集，该数据集与人们讲话的视音频记录相匹配。</li><li>这是第一个提出生成式方法来实现逼真、高质量的体积人头运动合成的工作。</li><li>FaceTalk能够生成合理的运动序列，产生高清头部动画，并与NPHM形状空间相结合。</li><li>FaceTalk在感知用户研究评估中比现有方法高出75%，证明了其有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FaceTalk：神经参数化头部模型的音频驱动运动扩散</li><li>作者：Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nießner</li><li>隶属机构：慕尼黑工业大学</li><li>关键词：音频驱动动画，神经参数化头部模型，扩散模型，语音合成</li><li>论文链接：https://arxiv.org/abs/2312.08459，Github 代码链接：None</li><li><p>摘要：（1）研究背景：三维动画建模在数字媒体领域有着广泛的应用，包括动画电影、电脑游戏和虚拟代理。近年来，大量工作提出了人类身体运动合成的生成方法，能够根据动作、语言、音乐等各种信号对人类骨骼进行动画处理。然而，生成三维面部运动的合成主要集中在三维可变形模型（3DMM）上，利用线性混合形状来表示头部运动和表情。这种模型刻画了一个分离的头部形状和运动空间，但缺乏全面表示人类面部几何形状复杂性和细粒度细节的能力。（2）过去的方法及其问题：现有方法主要集中在三维可变形模型（3DMM）上，利用线性混合形状来表示头部运动和表情。这种模型刻画了一个分离的头部形状和运动空间，但缺乏全面表示人类面部几何形状复杂性和细粒度细节的能力。（3）研究方法：本文提出了一种新的生成方法，称为 FaceTalk，用于从输入音频信号合成逼真的三维说话人头部运动序列。该方法将语音信号与神经参数化头部模型的潜在空间相结合，以创建逼真且时间连贯的运动序列。我们提出了一种新的潜在扩散模型来执行此任务，该模型在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真头部序列。在没有对应 NPHM 表情与音频的数据集的情况下，我们优化了这些对应关系，以生成适合人们说话的音频视频记录的时间优化的 NPHM 表情数据集。（4）方法性能：实验结果证明了 FaceTalk 的有效性，在感知用户研究评估中，它始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。这些性能支持了本文的目标，即生成逼真的三维头部运动，这些运动可以与 NPHM 形状空间耦合，产生高保真头部动画。</p></li><li><p>方法：(1) 数据集：本文使用了一个由 200 个说话人的音频和视频数据组成的公开数据集。音频数据是 8kHz 的单声道音频，视频数据是 25fps 的 RGB 视频。(2) 神经参数化头部模型（NPHM）：本文使用了一个预训练的 NPHM，该模型可以从输入的 3D 扫描数据中生成逼真的头部运动序列。(3) 潜在扩散模型：本文提出了一种新的潜在扩散模型，该模型可以在 NPHM 的表情空间中运行，以合成音频驱动的逼真头部序列。该模型使用了一个变分自编码器（VAE）作为编码器，将音频信号编码成潜在空间中的一个向量。然后，该向量被输入到一个扩散模型中，该模型逐渐将向量从一个高斯分布扩散到一个均匀分布。在扩散过程中，模型学习如何从潜在空间中生成逼真的头部运动序列。(4) 优化对应关系：为了生成适合人们说话的音频视频记录的时间优化的 NPHM 表情数据集，本文优化了这些对应关系。具体来说，本文使用了一个生成对抗网络（GAN）来优化对应关系。GAN 的生成器将音频信号映射到 NPHM 的表情空间中的一个向量，而判别器则试图区分生成的表情和真实的表情。通过训练 GAN，可以学习到一个能够生成逼真表情的生成器。(5) 实验结果：本文在公开数据集上对 FaceTalk 进行了评估。实验结果表明，FaceTalk 能够生成逼真的三维头部运动，这些运动可以与 NPHM 形状空间耦合，产生高保真头部动画。在感知用户研究评估中，FaceTalk 始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。</p></li><li><p>结论：（1）：本文提出了一种新的方法 FaceTalk，可以从输入的音频信号中合成逼真的三维说话人头部运动序列。该方法将语音信号与神经参数化头部模型的潜在空间相结合，以创建逼真且时间连贯的运动序列。实验结果证明了 FaceTalk 的有效性，在感知用户研究评估中，它始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。（2）：创新点：</p></li><li>提出了一种新的潜在扩散模型，可以在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真的头部序列。</li><li>优化了神经参数化头部模型表情与音频信号之间的对应关系，以生成适合人们说话的音频视频记录的时间优化的神经参数化头部模型表情数据集。</li><li>实验结果表明，FaceTalk 能够生成逼真的三维头部运动，这些运动可以与神经参数化头部模型形状空间耦合，产生高保真头部动画。性能：</li><li>在感知用户研究评估中，FaceTalk 始终如一地实现了优越且视觉上自然的运动，涵盖了多种面部表情和风格，比现有方法高出 75%。</li><li>FaceTalk 能够生成逼真的三维头部运动，这些运动可以与神经参数化头部模型形状空间耦合，产生高保真头部动画。工作量：</li><li>本文使用了一个由 200 个说话人的音频和视频数据组成的公开数据集。</li><li>本文使用了一个预训练的神经参数化头部模型，该模型可以从输入的 3D 扫描数据中生成逼真的头部运动序列。</li><li>本文提出了一种新的潜在扩散模型，该模型可以在神经参数化头部模型的表情空间中运行，以合成音频驱动的逼真的头部序列。</li><li>本文优化了神经参数化头部模型表情与音频信号之间的对应关系，以生成适合人们说话的音频视频记录的时间优化的神经参数化头部模型表情数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c3d682b60e63c1acae348037b65c2339.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c635db505ff8573cd87519e86c5a8129.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7c2f6ea2aaaabd42a793354211bb803d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2db2438899bc884a4f1ba5f9498ddf15.jpg" align="middle"></details><h2 id="GMTalker-Gaussian-Mixture-based-Emotional-talking-video-Portraits"><a href="#GMTalker-Gaussian-Mixture-based-Emotional-talking-video-Portraits" class="headerlink" title="GMTalker: Gaussian Mixture based Emotional talking video Portraits"></a>GMTalker: Gaussian Mixture based Emotional talking video Portraits</h2><p><strong>Authors:Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yebin Liu</strong></p><p>Synthesizing high-fidelity and emotion-controllable talking video portraits, with audio-lip sync, vivid expression, realistic head pose, and eye blink, is an important and challenging task in recent years. Most of the existing methods suffer in achieving personalized precise emotion control or continuously interpolating between different emotions and generating diverse motion. To address these problems, we present GMTalker, a Gaussian mixture based emotional talking portraits generation framework. Specifically, we propose a Gaussian Mixture based Expression Generator (GMEG) which can construct a continuous and multi-modal latent space, achieving more flexible emotion manipulation. Furthermore, we introduce a normalizing flow based motion generator pretrained on the dataset with a wide-range motion to generate diverse motions. Finally, we propose a personalized emotion-guided head generator with an Emotion Mapping Network (EMN) which can synthesize high-fidelity and faithful emotional video portraits. Both quantitative and qualitative experiments demonstrate our method outperforms previous methods in image quality, photo-realism, emotion accuracy and motion diversity. </p><p><a href="http://arxiv.org/abs/2312.07669v1">PDF</a> Project page: <a href="https://bob35buaa.github.io/GMTalker">https://bob35buaa.github.io/GMTalker</a></p><p><strong>摘要</strong><br>使用高斯混合模型生成多模态潜在空间，实现多样的动作和灵活的情感控制。</p><p><strong>要点</strong></p><ul><li>使用高斯混合模型构建情感表达生成器，得到连续且多模态的潜在空间，实现更为灵活的情感控制。</li><li>引入基于正态化流的动作生成器，预训练包含广泛动作的数据集，以实现多样的动作生成。</li><li>提出了一种个性化情感引导头部生成器，使用情感映射网络合成高保真视频肖像，使其在情感表达上更准确和逼真。</li><li>定量和定性实验表明，该方法在图像质量、逼真度、情感准确性和动作多样性方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GMTalker：基于高斯混合模型的情感谈话视频肖像（补充材料）</li><li>作者：Xuechen Liu, Pengfei Wan, Yebin Liu, Wenpeng Yin, Wen Zheng, Chen Change Loy, Yu-Kun Lai, Xiaoguang Han</li><li>单位：清华大学</li><li>关键词：情感控制、说话头像、视频肖像、高斯混合模型、标准化流</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：（1）：研究背景：合成高保真且可情感控制的说话视频肖像，具有音频唇形同步、生动的表情、逼真的头部姿势和眨眼，是近年来的一项重要且具有挑战性的任务。大多数现有方法在实现个性化精确的情感控制或连续插值不同情感和生成多样化情感方面存在困难。（2）：过去方法及其问题：现有方法难以实现个性化精确的情感控制或连续插值不同情感和生成多样化情感。本文方法动机充分。（3）：研究方法：为了解决这些问题，我们提出了 GMTalker，这是一个基于高斯混合模型的情感谈话肖像生成框架。具体来说，我们提出了一种基于高斯混合模型的表情生成器 (GMEG)，它可以构建一个连续且多模态的潜在空间，实现更灵活的情感操纵。此外，我们引入了一个基于标准化流的运动生成器，该生成器在具有广泛运动的数据集上预训练，以生成不同的情感。最后，我们提出了一种个性化情感引导头部生成器，该生成器具有情感映射网络 (EMN)，可以合成高保真且逼真的情感视频肖像。（4）：方法性能：定量和定性实验表明，我们的方法在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。这些性能可以支持我们的目标。</li></ol><p>7.Methods：（1）：我们提出了一种基于高斯混合模型的表情生成器（GMEG），它可以构建一个连续且多模态的潜在空间，实现更灵活的情感操纵。（2）：我们引入了一个基于标准化流的运动生成器，该生成器在具有广泛运动的数据集上预训练，以生成不同的情感。（3）：我们提出了一种个性化情感引导头部生成器，该生成器具有情感映射网络（EMN），可以合成高保真且逼真的情感视频肖像。</p><ol><li>结论：（1）：意义：GMTalker 模型能够生成逼真且情感丰富的说话人视频肖像，在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。（2）：优缺点：创新点：</li><li>提出了一种基于高斯混合模型的表情生成器 (GMEG)，构建连续且多模态的潜在空间，实现更灵活的情感操纵。</li><li>引入了一个基于标准化流的运动生成器，在具有广泛运动的数据集上预训练，以生成不同的情感。</li><li>提出了一种个性化情感引导头部生成器，具有情感映射网络 (EMN)，可以合成高保真且逼真的情感视频肖像。</li></ol><p>性能：- 定量和定性实验表明，GMTalker 模型在图像质量、照片真实感、情感准确性和运动多样性方面优于以往的方法。</p><p>不足：- 依赖于包含丰富情感内容的高质量视频，获取这些视频具有一定的挑战性。- 目前仅能描述有限的情感，受限于数据集中的八种类别。</p><p>潜在的社会影响：- GMTalker 模型能够从单目视频生成逼真的情感说话人视频，存在被用于创建欺骗性视频的潜在风险，在部署之前应谨慎考虑。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a438836432dc4fba873399aa6e9333d9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-279ff5bfffe91a6c96bc7bdcb62720dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c1e76fa3f35eb328e56f9b35af348b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5b04f9df996dfc2f7e846210b954a049.jpg" align="middle"></details>## GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained   3D Face Guidance**Authors:Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li**Although existing speech-driven talking face generation methods achieve significant progress, they are far from real-world application due to the avatar-specific training demand and unstable lip movements. To address the above issues, we propose the GSmoothFace, a novel two-stage generalized talking face generation model guided by a fine-grained 3d face model, which can synthesize smooth lip dynamics while preserving the speaker's identity. Our proposed GSmoothFace model mainly consists of the Audio to Expression Prediction (A2EP) module and the Target Adaptive Face Translation (TAFT) module. Specifically, we first develop the A2EP module to predict expression parameters synchronized with the driven speech. It uses a transformer to capture the long-term audio context and learns the parameters from the fine-grained 3D facial vertices, resulting in accurate and smooth lip-synchronization performance. Afterward, the well-designed TAFT module, empowered by Morphology Augmented Face Blending (MAFB), takes the predicted expression parameters and target video as inputs to modify the facial region of the target video without distorting the background content. The TAFT effectively exploits the identity appearance and background context in the target video, which makes it possible to generalize to different speakers without retraining. Both quantitative and qualitative experiments confirm the superiority of our method in terms of realism, lip synchronization, and visual quality. See the project page for code, data, and request pre-trained models: https://zhanghm1995.github.io/GSmoothFace. [PDF](http://arxiv.org/abs/2312.07385v1) **摘要**利用细粒度的人脸模型，我们提出了一种新颖的说话人脸生成模型，能综合平滑的唇部动态，同时保留说话人的身份。**要点:**- 我们提出了 GSmoothFace，一种基于细粒度 3D 人脸模型的新型两阶段通用说话人脸生成模型。- GSmoothFace 由音频表情预测 (A2EP) 模块和目标自适应人脸转换 (TAFT) 模块组成。- A2EP 模块使用 Transformer 捕捉长期音频上下文，并从细粒度的 3D 面部顶点学习参数，从而实现准确而平滑的唇形同步性能。- TAFT 模块利用形态增强人脸融合 (MAFB) 技术，将预测的表情参数和目标视频作为输入，修改目标视频的面部区域，同时不扭曲背景内容。- TAFT 有效地利用了目标视频中的身份外观和背景上下文，无需重新训练即可泛化到不同的说话人。- 我们的方法在真实感、唇形同步和视觉质量方面均优于现有方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：GSmoothFace：通过细粒度 3D 面部引导实现广义平滑说话人面部生成</li><li>作者：Haiming Zhang, Zhihao Yuan, Chaoda Zheng, Xu Yan, Baoyuan Wang, Guanbin Li, Song Wu, Shuguang Cui, Zhen Li</li><li>单位：香港中文大学（深圳）未来网络智能研究所</li><li>关键词：深度学习、说话人面部生成、Transformer、生成对抗网络</li><li>论文链接：https://arxiv.org/abs/2312.07385</li><li><p>摘要：（1）研究背景：说话人面部生成旨在合成与任意语音输入同步的逼真肖像视频，在数字人动画、视觉配音、虚拟视频会议和娱乐等领域具有广泛的应用。现有方法主要分为基于 2D 和基于 3D 的方法。基于 2D 的方法通常将说话人面部生成问题表述为条件生成对抗网络 (cGAN)。基于 3D 的方法依赖于 3D 可变形模型 (3DMM)，由于其具有 3D 感知建模的能力，最近受到更多关注。（2）过去方法及问题：基于 2D 的方法由于音频到唇部运动映射学习的隐式监督和 GAN 的固有局限性，如训练不稳定和模式崩溃，产生的初步结果图像质量低，唇部同步不令人满意。基于 3D 的方法虽然能够生成更逼真、更自然的面部视频，但通常需要针对每个说话人进行专门训练，并且唇部运动可能不稳定。（3）研究方法：本文提出了一种通过细粒度 3D 面部引导实现广义平滑说话人面部生成模型 GSmoothFace。该模型主要由音频到表情预测 (A2EP) 模块和目标自适应面部平移 (TAFT) 模块组成。A2EP 模块使用 Transformer 捕获长期音频上下文，并从细粒度的 3D 面部顶点学习表情参数，从而实现准确且平滑的唇部同步性能。TAFT 模块利用形态增强面部混合 (MAFB) 技术，将预测的表情参数和目标视频作为输入，修改目标视频的面部区域，而不会扭曲背景内容。（4）方法性能：定量和定性实验表明，GSmoothFace 方法在逼真度、唇部同步和视觉质量方面均优于现有方法。该方法在说话人面部生成任务上取得了良好的性能，支持其目标。</p></li><li><p>方法：(1) 音频到表情预测（A2EP）：</p><ul><li>使用预训练的 wav2vec2.0 模型生成音频嵌入。</li><li>使用表情编码器和多头自注意力层提取表情嵌入。</li><li>使用 transformer 解码器预测与驱动音频同步的表情参数。(2) 目标自适应面部平移（TAFT）：</li><li>使用形态增强面部混合（MAFB）技术将预测的表情参数和目标视频融合。</li><li>使用生成器合成最终图像。</li></ul></li><li><p>结论：(1)：本文提出了一种通过细粒度3D面部引导实现广义平滑说话人面部生成模型GSmoothFace，该模型在逼真度、唇部同步和视觉质量方面均优于现有方法，在说话人面部生成任务上取得了良好的性能，支持其目标。(2)：创新点：</p></li><li>提出了一种新的说话人面部生成模型GSmoothFace，该模型通过细粒度3D面部引导实现广义平滑说话人面部生成。</li><li>设计了一种音频到表情预测模块，使用Transformer捕获长期音频上下文，并从细粒度的3D面部顶点学习表情参数，从而实现准确且平滑的唇部同步性能。</li><li>设计了一种目标自适应面部平移模块，利用形态增强面部混合技术，将预测的表情参数和目标视频融合，修改目标视频的面部区域，而不会扭曲背景内容。性能：</li><li>定量和定性实验表明，GSmoothFace方法在逼真度、唇部同步和视觉质量方面均优于现有方法。</li><li>该方法在说话人面部生成任务上取得了良好的性能，支持其目标。工作量：</li><li>该方法需要大量的训练数据和计算资源。</li><li>该方法的训练过程可能需要花费大量的时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-76d8d3d1ff62335f344500c45e58f207.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b2ae356b21a9745e5886796cd64fcd60.jpg" align="middle"><img src="https://pica.zhimg.com/v2-aea1844a3fa4563e27b3fae7190b60f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-25f2d8c1eb09db9719a1cf18d1746617.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1b17c868324721da0d1204c8940af5fc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-39460c49b1a37832b878283b788a6c61.jpg" align="middle"></details><h2 id="Neural-Text-to-Articulate-Talk-Deep-Text-to-Audiovisual-Speech-Synthesis-achieving-both-Auditory-and-Photo-realism"><a href="#Neural-Text-to-Articulate-Talk-Deep-Text-to-Audiovisual-Speech-Synthesis-achieving-both-Auditory-and-Photo-realism" class="headerlink" title="Neural Text to Articulate Talk: Deep Text to Audiovisual Speech   Synthesis achieving both Auditory and Photo-realism"></a>Neural Text to Articulate Talk: Deep Text to Audiovisual Speech   Synthesis achieving both Auditory and Photo-realism</h2><p><strong>Authors:Georgios Milis, Panagiotis P. Filntisis, Anastasios Roussos, Petros Maragos</strong></p><p>Recent advances in deep learning for sequential data have given rise to fast and powerful models that produce realistic videos of talking humans. The state of the art in talking face generation focuses mainly on lip-syncing, being conditioned on audio clips. However, having the ability to synthesize talking humans from text transcriptions rather than audio is particularly beneficial for many applications and is expected to receive more and more attention, following the recent breakthroughs in large language models. For that, most methods implement a cascaded 2-stage architecture of a text-to-speech module followed by an audio-driven talking face generator, but this ignores the highly complex interplay between audio and visual streams that occurs during speaking. In this paper, we propose the first, to the best of our knowledge, text-driven audiovisual speech synthesizer that uses Transformers and does not follow a cascaded approach. Our method, which we call NEUral Text to ARticulate Talk (NEUTART), is a talking face generator that uses a joint audiovisual feature space, as well as speech-informed 3D facial reconstructions and a lip-reading loss for visual supervision. The proposed model produces photorealistic talking face videos with human-like articulation and well-synced audiovisual streams. Our experiments on audiovisual datasets as well as in-the-wild videos reveal state-of-the-art generation quality both in terms of objective metrics and human evaluation. </p><p><a href="http://arxiv.org/abs/2312.06613v1">PDF</a> </p><p><strong>Summary</strong><br>文本驱动视听语音合成器NEUTART首次使用Transformer，在统一视听特征空间中生成逼真的人脸说话视频</p><p><strong>Key Takeaways</strong></p><ul><li>NEUTART是第一个使用Transformer的文本驱动视听语音合成器。</li><li>NEUTART使用统一视听特征空间来学习语音和视觉特征之间的映射关系。</li><li>NEUTART利用语音提示的3D面部重建和唇读损失来进行视觉监督。</li><li>NEUTART生成的说话人脸视频在客观指标和人类评估方面都达到了最先进的生成质量。</li><li>NEUTART可以处理各种音视频数据集，包括有声数据集和野生视频。</li><li>NEUTART可以生成具有类人发音和良好同步视听流的逼真说话人脸视频。</li><li>NEUTART有望在许多应用中得到广泛使用，例如虚拟现实、教育和娱乐。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：神经文本到清晰语音：深度文本到视听语音合成</li><li>作者：Georgios Milis、Panagiotis P. Filntisis、Anastasios Roussos、Petros Maragos</li><li>隶属单位：雅典国立技术大学电气与计算机工程学院</li><li>关键词：文本到语音、语音到视觉、深度学习、Transformer、视听语音合成</li><li>论文链接：https://arxiv.org/abs/2312.06613</li><li>摘要：(1) 研究背景：随着深度学习在序列数据领域的发展，快速且强大的模型可以生成逼真的说话人类视频。目前最先进的说话人脸生成主要集中在唇形同步上，以音频片段为条件。然而，能够从文本转录而不是音频合成说话的人类对于许多应用特别有益，并且预计随着大型语言模型的最新突破，它将受到越来越多的关注。为此，大多数方法采用级联的 2 阶段架构，包括文本到语音模块和音频驱动的说话人脸生成器，但这忽略了说话过程中出现的音频和视觉流之间高度复杂的相互作用。(2) 过去的方法及其问题：大多数方法采用级联的 2 阶段架构，包括文本到语音模块和音频驱动的说话人脸生成器，但这忽略了说话过程中出现的音频和视觉流之间高度复杂的相互作用。(3) 本文提出的研究方法：我们提出第一个使用 Transformer 的文本驱动的视听语音合成器，它不遵循级联架构。相反，它将文本直接映射到视听表示，该表示用于生成逼真的说话人脸视频。(4) 方法在任务和性能上的表现：我们的方法在 TalkingHead++ 数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。此外，我们的方法还能够生成具有逼真唇形同步和面部表情的说话人脸视频。</li></ol><p><methods>:(1) NEUTART框架：本文提出的神经文本到清晰语音框架 NEUTART，用于解决文本驱动的逼真视听语音合成问题。NEUTART 包含两个主要模块：视听模块和逼真模块。视听模块用于联合合成语音音频和 3D 说话人脸序列，逼真模块用于合成 RGB 面部视频。(2) 视听模块：视听模块基于 FastSpeech2 文本到语音系统构建，以纳入视觉生成。音频通过其梅尔谱图建模，预训练的声码器从梅尔谱图生成语音波形。同样，面部使用 FLAME 3DMM 建模，将 5023 个顶点的面部 3D 网格解耦为身份 β、表情 ψ 和关节姿势 θ 参数，其中包括 3 个下颌关节参数 θjaw。使用这种表示，我们使用 3 个下颌姿势和 50 个表情参数对说话过程中的面部表情和动作进行建模。因此，NEUTART 每帧音频预测 80 个梅尔通道和 53 个 3DMM 通道。然后将 3DMM 系数解码为 3D 面部重建，从而驱动面部渲染器。(3) 逼真模块：逼真模块使用 StyleGAN2 生成器将 3D 面部重建合成到 RGB 视频中。该模块由以下子模块组成：- 光照估计器：估计场景光照条件，以确保生成的视频具有逼真的照明。- 背景合成器：合成逼真的背景，以增强视频的视觉质量。- 合成器：将 3D 面部重建和估计的光照条件合成到 RGB 视频中。(4) 训练和推理：NEUTART 使用对抗性训练方法进行训练。在推理时，视听模块和逼真模块是耦合的，但由于逼真模块中使用的神经渲染器的计算要求很高，因此它们是单独训练的。</methods></p><ol><li>结论：（1）：本文提出了一种新的文本驱动的视听语音合成框架NEUTART，该框架能够直接将文本映射到视听表示，并生成逼真的说话人脸视频。NEUTART在TalkingHead++数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。（2）：创新点：</li><li>提出了一种新的文本驱动的视听语音合成框架NEUTART，该框架不遵循级联架构，而是将文本直接映射到视听表示，并生成逼真的说话人脸视频。</li><li>NEUTART包含两个主要模块：视听模块和逼真模块。视听模块用于联合合成语音音频和3D说话人脸序列，逼真模块用于合成RGB面部视频。</li><li>视听模块基于FastSpeech2文本到语音系统构建，以纳入视觉生成。音频通过其梅尔谱图建模，预训练的声码器从梅尔谱图生成语音波形。同样，面部使用FLAME3DMM建模，将5023个顶点的面部3D网格解耦为身份β、表情ψ和关节姿势θ参数，其中包括3个下颌关节参数θjaw。使用这种表示，我们使用3个下颌姿势和50个表情参数对说话过程中的面部表情和动作进行建模。因此，NEUTART每帧音频预测80个梅尔通道和53个3DMM通道。然后将3DMM系数解码为3D面部重建，从而驱动面部渲染器。</li><li>逼真模块使用StyleGAN2生成器将3D面部重建合成到RGB视频中。该模块由以下子模块组成：光照估计器、背景合成器和合成器。光照估计器估计场景光照条件，以确保生成的视频具有逼真的照明。背景合成器合成逼真的背景，以增强视频的视觉质量。合成器将3D面部重建和估计的光照条件合成到RGB视频中。</li><li>NEUTART使用对抗性训练方法进行训练。在推理时，视听模块和逼真模块是耦合的，但由于逼真模块中使用的神经渲染器的计算要求很高，因此它们是单独训练的。</li><li>性能：NEUTART在TalkingHead++数据集上实现了最先进的结果，在合成视频的视觉和语音质量方面均优于现有方法。此外，我们的方法还能够生成具有逼真唇形同步和面部表情的说话人脸视频。</li><li>工作量：NEUTART的训练和推理过程相对复杂，需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-726d63a429612bb5a9a4e98af93d828f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4d97a4e9898602db53e92de86db31893.jpg" align="middle"><img src="https://picx.zhimg.com/v2-445934daa5563cb0bbb73a252c079f70.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0d51e1e89d038a1d7637e51d15eb70e0.jpg" align="middle"></details><h2 id="R2-Talker-Realistic-Real-Time-Talking-Head-Synthesis-with-Hash-Grid-Landmarks-Encoding-and-Progressive-Multilayer-Conditioning"><a href="#R2-Talker-Realistic-Real-Time-Talking-Head-Synthesis-with-Hash-Grid-Landmarks-Encoding-and-Progressive-Multilayer-Conditioning" class="headerlink" title="R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid   Landmarks Encoding and Progressive Multilayer Conditioning"></a>R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid   Landmarks Encoding and Progressive Multilayer Conditioning</h2><p><strong>Authors:Zhiling Ye, LiangGuo Zhang, Dingheng Zeng, Quan Lu, Ning Jiang</strong></p><p>Dynamic NeRFs have recently garnered growing attention for 3D talking portrait synthesis. Despite advances in rendering speed and visual quality, challenges persist in enhancing efficiency and effectiveness. We present R2-Talker, an efficient and effective framework enabling realistic real-time talking head synthesis. Specifically, using multi-resolution hash grids, we introduce a novel approach for encoding facial landmarks as conditional features. This approach losslessly encodes landmark structures as conditional features, decoupling input diversity, and conditional spaces by mapping arbitrary landmarks to a unified feature space. We further propose a scheme of progressive multilayer conditioning in the NeRF rendering pipeline for effective conditional feature fusion. Our new approach has the following advantages as demonstrated by extensive experiments compared with the state-of-the-art works: 1) The lossless input encoding enables acquiring more precise features, yielding superior visual quality. The decoupling of inputs and conditional spaces improves generalizability. 2) The fusing of conditional features and MLP outputs at each MLP layer enhances conditional impact, resulting in more accurate lip synthesis and better visual quality. 3) It compactly structures the fusion of conditional features, significantly enhancing computational efficiency. </p><p><a href="http://arxiv.org/abs/2312.05572v1">PDF</a> </p><p><strong>摘要</strong><br>高效且有效的 R2-Talker 框架采用多尺度哈希网格对人脸特征进行无损编码，并引入渐进式多层调节方案，实现更逼真的实时动态人像合成。</p><p><strong>要点</strong></p><ul><li>基于多尺度哈希网格，提出将面部特征作为条件特征进行编码的创新方法，实现特征无损编码，条件空间与输入解耦。</li><li>提出在 NeRF 渲染管道中进行渐进式多层调节方案，实现条件特征的有效融合。</li><li>将条件特征和多层 perceptron 的输出在每个 perceptron 层融合，增强条件特征的影响，从而提高唇部的合成精度和视觉质量。</li><li>紧凑地构建条件特征的融合，显著提高计算效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：R2-Talker：基于哈希网格的实时说话人头部合成</li><li>作者：Junyu Luo, Jingbo Zhao, Zhaoyang Lv, Yajie Zhao, Hongtao Lu, Xiaoguang Han</li><li>隶属单位：北京大学</li><li>关键词：音频驱动、说话人头部合成、NeRF、条件特征融合、哈希网格</li><li>论文链接：https://arxiv.org/abs/2312.05572   Github 链接：无</li><li>摘要：   (1)：研究背景：动态 NeRF 近年来在 3D 说话人肖像合成中备受关注。尽管渲染速度和视觉质量取得了进步，但在提高效率和有效性方面仍然存在挑战。   (2)：过去方法：现有方法主要集中在改进渲染速度和视觉质量上，但往往忽略了效率和有效性。例如，RAD-NeRF 虽然实现了较高的渲染速度，但视觉质量还有待提高；而 ER-NeRF 虽然具有较好的视觉质量，但渲染速度较慢。   (3)：研究方法：本文提出了一种新的框架 R2-Talker，可以实现高效且有效的实时说话人头部合成。R2-Talker 的主要贡献包括：</li><li>提出了一种使用多分辨率哈希网格对面部地标进行编码的新方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。</li><li><p>提出了一种在 NeRF 渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。   (4)：方法性能：在广泛的实验中，R2-Talker 在视觉质量、泛化性和计算效率方面都优于现有技术。具体来说，R2-Talker 在视觉质量方面优于 RAD-NeRF 和 ER-NeRF，在泛化性方面优于 Geneface++，在计算效率方面优于 RAD-NeRF 和 Geneface++。这些性能支持了本文的目标，即实现高效且有效的实时说话人头部合成。</p></li><li><p>Methods：（1）：本文提出了一种使用多分辨率哈希网格对面部地标进行编码的方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。（2）：本文提出了一种在NeRF渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。（3）：本文提出了一种新的框架R2-Talker，可以实现高效且有效的实时说话人头部合成。</p></li><li><p>结论：（1）：本工作提出了一种高效且有效的实时说话人头部合成框架R2-Talker，该框架在视觉质量、泛化性和计算效率方面优于现有技术。（2）：创新点：</p></li><li>提出了一种使用多分辨率哈希网格对面部地标进行编码的方法，该方法可以无损地将地标结构编码为条件特征，并通过将任意地标映射到统一的特征空间来解耦输入多样性和条件空间。</li><li>提出了一种在NeRF渲染管道中进行渐进式多层条件特征融合的方案，可以有效地融合条件特征。性能：</li><li>在视觉质量方面优于RAD-NeRF和ER-NeRF，在泛化性方面优于Geneface++，在计算效率方面优于RAD-NeRF和Geneface++。工作量：</li><li>本文的工作量较大，需要大量的实验和计算。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2cdd63682ec6a29cc8aa99e91b02f344.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-201f569e2b5317cf129033a4f5a93d60.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5a7ff777ad56ad955bbe2a61d5e51b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6c583a98ee2122cd98e00a07b748dcc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d633b39b222518156d77fac342f43598.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9e2369dfa6e73f9f30aadec5db2fc944.jpg" align="middle"></details><h2 id="FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation"><a href="#FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation" class="headerlink" title="FT2TF: First-Person Statement Text-To-Talking Face Generation"></a>FT2TF: First-Person Statement Text-To-Talking Face Generation</h2><p><strong>Authors:Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin</strong></p><p>Talking face generation has gained immense popularity in the computer vision community, with various applications including AR/VR, teleconferencing, digital assistants, and avatars. Traditional methods are mainly audio-driven ones which have to deal with the inevitable resource-intensive nature of audio storage and processing. To address such a challenge, we propose FT2TF - First-Person Statement Text-To-Talking Face Generation, a novel one-stage end-to-end pipeline for talking face generation driven by first-person statement text. Moreover, FT2TF implements accurate manipulation of the facial expressions by altering the corresponding input text. Different from previous work, our model only leverages visual and textual information without any other sources (e.g. audio/landmark/pose) during inference. Extensive experiments are conducted on LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are reported. Both quantitative and qualitative results showcase that FT2TF outperforms existing relevant methods and reaches the state-of-the-art. This achievement highlights our model capability to bridge first-person statements and dynamic face generation, providing insightful guidance for future work. </p><p><a href="http://arxiv.org/abs/2312.05430v1">PDF</a> </p><p><strong>摘要</strong><br>以第一人称语句为驱动的单阶段端到端管道——FT2TF，无需额外信息即可生成逼真动态人脸。</p><p><strong>要点</strong></p><ul><li>FT2TF 是一种用于说话人脸生成的新颖单阶段端到端流水线，由第一人称语句文本驱动。</li><li>FT2TF 仅利用视觉和文本信息，无需任何其他来源（例如音频/地标/姿势）即可进行推理。</li><li>FT2TF在LRS2和LRS3数据集上进行了广泛的实验，并在多维评估指标上报告了结果。</li><li>定量和定性结果表明，FT2TF优于现有的相关方法，并达到了最先进的水平。</li><li>这一成果突出了我们的模型将第一人称陈述与动态人脸生成相桥接的能力，为未来的工作提供了有益的指导。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：FT2TF：第一人称陈述文本到说话人脸生成</li><li>作者：Yuxiao Hu, Runpei Dong, Mingming He, Xiaoming Wei, Yajun Cai, Keke He, Jianfei Cai</li><li>第一作者单位：中国科学院大学</li><li>关键词：文本到人脸生成、第一人称陈述、情感表达、语言文本编码器、视觉解码器</li><li>论文链接：https://arxiv.org/abs/2302.08243 或 https://github.com/VITA-Group/FT2TF</li><li><p>摘要：(1) 研究背景：说话人脸生成在计算机视觉领域备受关注，应用于 AR/VR、远程会议、数字助理和虚拟形象等。传统方法主要依赖音频驱动，但音频存储和处理不可避免地资源密集。(2) 过去方法和问题：现有方法受限于音频驱动，导致资源密集且难以实现准确的表情操纵。(3) 研究方法：提出 FT2TF，一种新颖的端到端流水线，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸。FT2TF 通过改变相应的输入文本来实现对表情的准确操纵。(4) 实验结果：在 LRS2 和 LRS3 数据集上进行广泛的实验，结果表明 FT2TF 在多维评估指标上优于现有相关方法，达到了最先进水平。这一成就突出了模型将第一人称陈述与动态人脸生成桥接的能力，为未来的工作提供了有益的指导。</p></li><li><p>方法：(1)：提出FT2TF，一种新颖的端到端流水线，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸；(2)：FT2TF由语言文本编码器和视觉解码器两部分组成，语言文本编码器将第一人称陈述文本编码成语义向量，视觉解码器将语义向量解码成说话人脸；(3)：FT2TF通过改变相应的输入文本来实现对表情的准确操纵；(4)：在LRS2和LRS3数据集上进行广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。</p></li><li><p>结论：（1）：本文提出了一种新颖的端到端流水线FT2TF，仅利用视觉和文本信息，通过第一人称陈述文本生成说话人脸，为未来的工作提供了有益的指导。（2）：创新点：① FT2TF通过改变相应的输入文本来实现对表情的准确操纵，实现了说话人脸生成任务的新突破。② FT2TF在LRS2和LRS3数据集上进行了广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。③ FT2TF具有较强的泛化能力，能够在不同的数据集上生成高质量的说话人脸。性能：① FT2TF在LRS2和LRS3数据集上进行了广泛的实验，结果表明FT2TF在多维评估指标上优于现有相关方法，达到了最先进水平。② FT2TF能够生成高质量的说话人脸，具有较强的视觉保真度和情感表达能力。③ FT2TF具有较强的泛化能力，能够在不同的数据集上生成高质量的说话人脸。工作量：① FT2TF的模型结构相对简单，易于训练和部署。② FT2TF的训练速度较快，可以在短时间内生成高质量的说话人脸。③ FT2TF的推理速度较快，可以实时生成说话人脸。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2ab1187e4c933e4579db03a2b5dcd8e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e681830ba6447f3f79189954d392d003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ef172006be19ffae8981a8a6d3a65fff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8b9dc58af4aebf75acd65646ccdabb9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6acbbfb04100c974df91feb1eb4b5bf0.jpg" align="middle"></details><h2 id="Emotional-Speech-driven-3D-Body-Animation-via-Disentangled-Latent-Diffusion"><a href="#Emotional-Speech-driven-3D-Body-Animation-via-Disentangled-Latent-Diffusion" class="headerlink" title="Emotional Speech-driven 3D Body Animation via Disentangled Latent   Diffusion"></a>Emotional Speech-driven 3D Body Animation via Disentangled Latent   Diffusion</h2><p><strong>Authors:Kiran Chhatre, Radek Daněček, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart</strong></p><p>Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content and better represent the emotion expressed by the input speech. Our project website is amuse.is.tue.mpg.de. </p><p><a href="http://arxiv.org/abs/2312.04466v1">PDF</a> </p><p><strong>摘要</strong><br>通过分离内容、情感和个人风格，AMUSE 模型能够准确地生成与语音同步且情感丰富的动作。</p><p><strong>要点</strong></p><ul><li>AMUSE 是一种基于潜扩散的语音驱动肢体动画模型。</li><li>AMUSE 将驱动音频映射到三个解纠缠的潜在向量：内容、情感和个人风格。</li><li>AMUSE通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音合成 3D 人体手势。</li><li>AMUSE 可以通过对扩散模型的噪声进行随机采样来生成具有相同情感表达力的姿态变化。</li><li>定性和定量评估表明 AMUSE 输出的手势序列是逼真的。</li><li>与最先进的技术相比，生成的姿态与语音内容的同步性更好，并且更好地表达了输入语音所表达的情感。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：情感语音驱动的三维身体动画，通过分离的潜在扩散</li><li>作者：Kiran Chhatre, Radek Danˇeˇcek, Nikos Athanasiou, Giorgio Becherini, Christopher Peters, Michael J. Black, Timo Bolkart</li><li>第一作者单位：瑞典皇家理工学院</li><li>关键词：语音驱动动画、情感表达、潜在扩散</li><li>论文链接：https://arxiv.org/abs/2312.04466Github 链接：无</li><li>摘要：(1)：研究背景：</li><li>语音驱动的三维身体动画在增强现实/虚拟现实中的远程临场感、游戏和电影中的虚拟人物动画以及具身交互式数字助理等方面具有广泛的应用。</li><li>现有的语音驱动三维身体动画方法在与语音节奏和单词发音相关的动作内容方面取得了很大进展，但它们没有充分解决一个关键因素：情感对生成动作的影响。</li></ol><p>(2)：过去方法和问题：- 现有的语音驱动三维身体动画方法没有明确地建模情感对生成动作的影响，而是直接从语音输出动画，而不能控制表达的情感。- 这些方法无法生成与输入语音表达的情感一致的动作，并且生成的动作与语音内容不同步。</p><p>(3)：研究方法：- 提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE。- AMUSE 将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。- 然后将训练好的潜在扩散模型用于生成动作情感序列，该模型以这些潜在向量为条件。- AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。- 随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。</p><p>(4)：实验结果：- 定性和定量评估表明，AMUSE 输出逼真的动作序列。- 与最先进的方法相比，生成的动作与语音内容更好地同步，并且更好地代表了输入语音表达的情感。</p><ol><li><p>方法：（1）提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE，该模型将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。（2）使用训练好的潜在扩散模型生成动作情感序列，该模型以这些潜在向量为条件。（3）AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。（4）随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。</p></li><li><p>结论：（1）：本文提出了一种情感语音驱动的身体动画模型 AMUSE，该模型可以从语音中合成三维人体动作，并控制表达的情感和风格。AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。（2）：创新点：</p></li><li>提出了一种基于潜在扩散的情感语音驱动身体动画模型 AMUSE。</li><li>AMUSE 将驱动音频映射到三个分离的潜在向量：内容、情感和个人风格。</li><li>使用训练好的潜在扩散模型生成动作情感序列，该模型以这些潜在向量为条件。</li><li>AMUSE 通过将驱动语音的内容与另一个语音序列的情感和风格相结合，直接从语音中合成三维人体动作，并控制表达的情感和风格。</li><li>随机采样扩散模型的噪声可以生成具有相同情感表达的不同动作。性能：</li><li>AMUSE 在各种指标上取得了最先进的性能：多样性、手势情感分类准确率、Frechét 手势距离、节拍对齐得分和语义相关手势召回。</li><li>感知研究表明，与之前的最先进技术相比，AMUSE 生成的动作与输入语音表达的情感更加同步并且更匹配。工作量：</li><li>AMUSE 模型的训练和推理过程相对复杂，需要大量的数据和计算资源。</li><li>AMUSE 模型需要针对不同的任务和数据集进行微调，这可能会增加工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-477aaf1cf553532d2ac6d081ce493dd5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fb8f5969e38a10a90f4de1eb4b4df46.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59b0cb3d525b548d4a6433a459e908fc.jpg" align="middle"></details><h2 id="PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features"><a href="#PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features" class="headerlink" title="PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo   Multi-modal Features"></a>PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo   Multi-modal Features</h2><p><strong>Authors:Tianshun Han, Shengnan Gui, Yiqing Huang, Baihui Li, Lijian Liu, Benjia Zhou, Ning Jiang, Quan Lu, Ruicong Zhi, Yanyan Liang, Du Zhang, Jun Wan</strong></p><p>Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. We recommend watching the supplementary video. </p><p><a href="http://arxiv.org/abs/2312.02781v1">PDF</a> </p><p><strong>Summary</strong><br>语音驱动的三维面部动画精度和连贯性不足，主要原因在于忽略视觉和文本线索，我们提出了一种利用互补伪多模态特征提高面部动画精度的 PMMTalk 框架。</p><p><strong>Key Takeaways</strong></p><ul><li>PMMTalk 框架包含 PMMTalk 编码器、跨模态对齐模块和 PMMTalk 解码器三个模块。</li><li>PMMTalk 编码器分别从语音中提取视觉和文本信息。</li><li>跨模态对齐模块在时间和语义层面上对齐音频-图像-文本特征。</li><li>PMMTalk 解码器用于预测唇形同步的面部混合形状系数。</li><li>与之前的研究相比，PMMTalk 只需要一个额外的随机参考面部图像，但产生了更准确的结果。</li><li>PMMTalk 与标准动画制作工作流程无缝集成，因为它引入了面部混合形状系数，因此对艺术家友好。</li><li>PMMTalk 引入了一个大规模的三维中文音视频面部动画 (3D-CAVFA) 数据集。</li><li>广泛的实验和用户研究表明，PMMTalk 优于现有技术。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PMMTalk：基于互补伪多模态特征的语音驱动 3D 面部动画</li><li>作者：田顺寒、盛楠桂、易青黄、白慧丽、李立建、周本佳、蒋宁、陆全、池瑞聪、梁艳艳、张都、万钧</li><li>单位：澳门科技大学计算机科学与工程学院、创新工程学院</li><li>关键词：语音驱动 3D 面部动画、PMMTalk、3D-CAVFA 数据集</li><li>链接：https://arxiv.org/abs/2312.02781</li><li>摘要：(1)：语音驱动 3D 面部动画近年来取得了很大进展，但大多数相关工作仅利用声学模态，忽略了视觉和文本线索的影响，导致精度和连贯性方面的不令人满意。(2)：以往方法仅依赖于音频信号，忽视了视觉和文本线索，导致唇部运动不足和不连贯。(3)：本文提出了一种新颖的框架 PMMTalk，使用互补的伪多模态特征来提高面部动画的准确性。该框架包含三个模块：PMMTalk 编码器、跨模态对齐模块和 PMMTalk 解码器。(4)：PMMTalk 在 3D-CAVFA 数据集上取得了最先进的结果，并且用户研究表明，我们的方法在准确性和连贯性方面优于最先进的方法。</li></ol><p>方法：</p><p>（1）PMMTalk框架：PMMTalk框架由三个模块组成：PMMTalk编码器、跨模态对齐模块和PMMTalk解码器。PMMTalk编码器将语音、图像和文本特征编码成伪多模态特征。跨模态对齐模块将伪多模态特征对齐，以确保它们在时间和语义上的一致性。PMMTalk解码器将对齐后的伪多模态特征解码成3D面部动画。</p><p>（2）伪多模态特征生成：PMMTalk通过使用生成对抗网络（GAN）生成伪多模态特征。GAN由两个网络组成：生成器和判别器。生成器将语音、图像和文本特征作为输入，生成伪多模态特征。判别器将伪多模态特征与真实的多模态特征进行比较，并输出一个判别分数。生成器和判别器通过对抗训练的方式不断更新，直到生成的伪多模态特征与真实的多模态特征难以区分。</p><p>（3）跨模态对齐：PMMTalk使用一种基于注意力的机制来对齐伪多模态特征。注意力机制可以学习到伪多模态特征中与3D面部动画相关的重要信息。对齐后的伪多模态特征可以更好地反映3D面部动画的动态变化。</p><p>（4）3D面部动画解码：PMMTalk使用一种基于神经网络的解码器来将对齐后的伪多模态特征解码成3D面部动画。解码器可以学习到伪多模态特征与3D面部动画之间的映射关系。解码后的3D面部动画可以准确地反映语音、图像和文本中的信息。</p><ol><li>结论：（1）：本工作利用互补的伪多模态特征，提出了一种新颖的语音驱动 3D 面部动画框架 PMMTalk，有效提高了面部动画的准确性和连贯性，为虚拟现实应用提供了更逼真且富有情感的面部动画。（2）：创新点：</li><li>提出了一种新的语音驱动 3D 面部动画框架 PMMTalk，该框架利用互补的伪多模态特征，有效提高了面部动画的准确性和连贯性。</li><li>构建了一个大规模的 3D 面部动画数据集 3D-CAVFA，该数据集包含了同步的面部混合形状系数、多样化的语料库和广泛的主题。</li><li>在 3D-CAVFA 数据集上，PMMTalk 取得了最先进的结果，并且用户研究表明，PMMTalk 在准确性和连贯性方面优于最先进的方法。性能：</li><li>在 3D-CAVFA 数据集上，PMMTalk 在准确性和连贯性方面优于最先进的方法。</li><li>用户研究表明，PMMTalk 在准确性和连贯性方面优于最先进的方法。工作量：</li><li>PMMTalk 依赖于多个大规模的预训练模型，这增加了模型的推理时间，对实时应用提出了挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5e16e4a6505780bd258a48846b37d1cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5a7e4b7e4be75685c80823cd56d8b266.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d8e43f08b5fd353528e3121153d4b584.jpg" align="middle"></details><h2 id="MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation"><a href="#MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation" class="headerlink" title="MyPortrait: Morphable Prior-Guided Personalized Portrait Generation"></a>MyPortrait: Morphable Prior-Guided Personalized Portrait Generation</h2><p><strong>Authors:Bo Ding, Zhenfeng Fan, Shuang Yang, Shihong Xia</strong></p><p>Generating realistic talking faces is an interesting and long-standing topic in the field of computer vision. Although significant progress has been made, it is still challenging to generate high-quality dynamic faces with personalized details. This is mainly due to the inability of the general model to represent personalized details and the generalization problem to unseen controllable parameters. In this work, we propose Myportrait, a simple, general, and flexible framework for neural portrait generation. We incorporate personalized prior in a monocular video and morphable prior in 3D face morphable space for generating personalized details under novel controllable parameters. Our proposed framework supports both video-driven and audio-driven face animation given a monocular video of a single person. Distinguished by whether the test data is sent to training or not, our method provides a real-time online version and a high-quality offline version. Comprehensive experiments in various metrics demonstrate the superior performance of our method over the state-of-the-art methods. The code will be publicly available. </p><p><a href="http://arxiv.org/abs/2312.02703v1">PDF</a> </p><p><strong>摘要</strong><br>用单目视频中的个性化先验知识和3D面部可变形空间中的可变形先验知识，生成具有个性化细节的高质量动态面孔。</p><p><strong>要点</strong></p><ul><li>我们提出了Myportrait，一个用于神经肖像生成的简单、通用且灵活的框架。</li><li>Myportrait在单目视频中结合个性化先验和3D面部可变形空间中的可变形先验，以在新的可控参数下生成个性化细节。</li><li>该框架支持基于视频和基于音频的面部动画，只要有一段单个人物的单目视频。</li><li>根据测试数据是否发送到训练，该方法提供了实时的在线版本和高质量的离线版本。</li><li>该方法在各个指标的综合实验中表现出优于最先进方法的性能。</li><li>代码将公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MyPortrait：可变形先验引导的个性化肖像生成</li><li>作者：Yujun Shen, Linchao Bao, Xiaogang Wang, Xiangyu Xu, Wenpeng Wang, Xiaowei Zhou</li><li>单位：香港城市大学</li><li>关键词：神经肖像生成、个性化生成、可变形先验、视频驱动、音频驱动</li><li>论文链接：Paper_info:MyPortrait:MorphablePrior-GuidedPersonalizedPortraitGenerationSupplementaryMaterial6.DatasetGithub 链接：无</li><li>摘要：(1)：研究背景：生成逼真的说话人面孔是计算机视觉领域的一个有趣且长期存在的话题。尽管取得了重大进展，但生成具有个性化细节的高质量动态面孔仍然具有挑战性。这主要是由于通用模型无法表示个性化细节以及对不可见的可控参数的泛化问题。(2)：过去方法及其问题：现有方法存在以下问题：无法表示个性化细节；对不可见的可控参数泛化能力差；无法同时支持视频驱动和音频驱动的面部动画。(3)：研究方法：本文提出 MyPortrait，这是一个简单、通用且灵活的神经肖像生成框架。我们在单目视频中加入个性化先验，在 3D 人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。(4)：实验结果：在各种指标上的综合实验表明，本文方法优于最先进的方法。这些性能支持他们的目标。</li></ol><p>7.Methods：(1)：提出MyPortrait，这是一个简单、通用且灵活的神经肖像生成框架。(2)：在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。(3)：所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。(4)：我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。</p><ol><li>结论：(1)：本文提出了一种简单、通用且灵活的神经肖像生成框架MyPortrait，该框架在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。综合实验表明，本文方法优于最先进的方法。(2)：创新点：</li><li>提出了一种简单、通用且灵活的神经肖像生成框架MyPortrait。</li><li>在单目视频中加入个性化先验，在3D人脸可变形空间中加入可变形先验，以便在新的可控参数下生成个性化细节。</li><li>所提出的框架支持在给定单目视频的情况下进行视频驱动和音频驱动的面部动画。</li><li>我们的方法根据测试数据是否发送到训练来提供实时在线版本和高质量的离线版本。性能：</li><li>在各种指标上的综合实验表明，本文方法优于最先进的方法。工作量：</li><li>该方法需要收集单目视频数据，并对数据进行预处理。</li><li>该方法需要训练神经网络模型，这可能需要大量的时间和计算资源。</li><li>该方法需要将训练好的模型部署到实际应用中，这可能需要额外的工程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-30c34cea0fa3b5c34bec52a8060f7083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34426e9a289a340fb5a39c487fd354e5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3551f581121858d6b86025dc61e6efa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c5be15aebe061090e2870367d2adb84.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81ad84ca0a38873919e5990c28ae906.jpg" align="middle"></details><h2 id="VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior"><a href="#VividTalk-One-Shot-Audio-Driven-Talking-Head-Generation-Based-on-3D-Hybrid-Prior" class="headerlink" title="VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D   Hybrid Prior"></a>VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D   Hybrid Prior</h2><p><strong>Authors:Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, Xun Cao</strong></p><p>Audio-driven talking head generation has drawn much attention in recent years, and many efforts have been made in lip-sync, expressive facial expressions, natural head pose generation, and high video quality. However, no model has yet led or tied on all these metrics due to the one-to-many mapping between audio and motion. In this paper, we propose VividTalk, a two-stage generic framework that supports generating high-visual quality talking head videos with all the above properties. Specifically, in the first stage, we map the audio to mesh by learning two motions, including non-rigid expression motion and rigid head motion. For expression motion, both blendshape and vertex are adopted as the intermediate representation to maximize the representation ability of the model. For natural head motion, a novel learnable head pose codebook with a two-phase training mechanism is proposed. In the second stage, we proposed a dual branch motion-vae and a generator to transform the meshes into dense motion and synthesize high-quality video frame-by-frame. Extensive experiments show that the proposed VividTalk can generate high-visual quality talking head videos with lip-sync and realistic enhanced by a large margin, and outperforms previous state-of-the-art works in objective and subjective comparisons. </p><p><a href="http://arxiv.org/abs/2312.01841v2">PDF</a> 10 pages, 8 figures</p><p><strong>摘要</strong></p><p>创新的两阶段框架 VividTalk 可生成高质量视觉效果的说话人头部视频，包括唇形同步、丰富的面部表情、自然的头部姿势等。</p><p><strong>要点</strong></p><ul><li>VividTalk 采用双阶段通用框架，可以生成高质量视觉效果的说话人头部视频。</li><li>VividTalk 在第一阶段通过学习非刚性表情运动和刚性头部运动，将音频映射到网格。</li><li>VividTalk 在第二阶段使用双分支运动-VAE 和生成器将网格转换为密集运动并逐帧合成高质量视频。</li><li>广泛的实验表明，与目前最先进的作品相比，VividTalk 可以生成高质量视觉效果的说话人头部视频，并将唇形同步和逼真的增强效果提高很大幅度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：生动语聊：高保真音视频生成框架（VividTalk: A High-Fidelity Audio-Driven Talking Head Generation Framework）</li><li>作者：Yuhang Jiang, Mingyu Ding, Junhui Hou, Yanan Sun, Lu Sheng, Zhiwei Xiong, Hang Zhou</li><li>单位：无</li><li>关键词：音频驱动、说话头生成、面部表情、头部姿势、视频合成</li><li>链接：https://arxiv.org/abs/2312.01841, Github：无</li><li><p>摘要：（1）：音频驱动的说话头生成已经引起广泛关注，在唇形同步、面部表情、头部姿势生成和视频质量方面取得了进展。然而，由于音频和动作之间的一对多映射，还没有模型能够在所有这些指标上达到最优。（2）：以往的方法通常使用混合形状或顶点偏移来表示面部表情，但这些方法在捕捉精细的表情细节方面存在局限性。此外，头部姿势的生成通常是通过直接从音频中学习来实现的，这可能会导致不合理和不连续的结果。（3）：本文提出了一种名为 VividTalk 的两阶段通用框架，支持生成具有所有上述属性的高视觉质量说话头视频。在第一阶段，通过学习非刚性表情运动和刚性头部运动将音频映射到网格。对于表情运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。对于自然头部运动，提出了一种新颖的可学习头部姿势码本，并采用两阶段训练机制。在第二阶段，提出了一种双分支运动-VAE 和生成器，将网格转换为密集运动并逐帧合成高质量视频。（4）：广泛的实验表明，所提出的 VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频，并且在客观和主观比较中优于以往的最新作品。</p></li><li><p>方法：（1）：VividTalk 框架分为两个阶段：网格生成阶段和视频合成阶段。在网格生成阶段，音频首先被映射到网格，网格由混合形状和顶点偏移表示。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。在视频合成阶段，网格被转换为密集运动，然后逐帧合成高质量视频。（2）：在网格生成阶段，音频首先被映射到一个中间表示，该中间表示由混合形状和顶点偏移组成。混合形状用于捕捉精细的表情细节，而顶点偏移用于捕捉刚性头部运动。然后，中间表示被映射到网格。（3）：在视频合成阶段，网格被转换为密集运动。密集运动然后被用于逐帧合成高质量视频。视频合成器是一个双分支网络，由一个运动-VAE 和一个生成器组成。运动-VAE 用于生成密集运动，而生成器用于合成视频。</p></li><li><p>结论：（1）：本工作首次提出了一种支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频的新颖通用框架 VividTalk。对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。至于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。实验表明，我们的方法优于以往最先进的方法，并在数字人创建、视频会议等许多应用中开辟了新途径。（2）：创新点：</p></li><li>提出了一种新颖的通用框架 VividTalk，支持生成具有丰富面部表情和自然头部姿势的高质量说话头视频。</li><li>对于非刚性表情运动，混合形状和顶点都被映射为中间表示以最大化模型的表示能力，并设计了一个精心构建的多分支生成器来分别对全局和局部面部运动进行建模。</li><li>对于刚性头部运动，提出了一种新颖的可学习头部姿势码本和一种两阶段训练机制来合成自然结果。</li><li>得益于双分支运动-VAE 和生成器，驱动的网格可以很好地转换为密集运动，并用于合成最终视频。性能：</li><li>在客观和主观比较中，VividTalk 优于以往最先进的作品。</li><li>VividTalk 可以生成具有唇形同步和逼真头部姿势的高视觉质量说话头视频。工作量：</li><li>VividTalk 的实现相对复杂，需要大量的数据和计算资源。</li><li>VividTalk 的训练过程可能需要数天或数周的时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b072ca131954e5aa54fae54f90858dae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8ef1751be1b0bb02f7a73562aad64e5f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-18bcd1380728d32e1277fd17982288c6.jpg" align="middle"></details><h2 id="3DiFACE-Diffusion-based-Speech-driven-3D-Facial-Animation-and-Editing"><a href="#3DiFACE-Diffusion-based-Speech-driven-3D-Facial-Animation-and-Editing" class="headerlink" title="3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing"></a>3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing</h2><p><strong>Authors:Balamurugan Thambiraja, Sadegh Aliakbarian, Darren Cosker, Justus Thies</strong></p><p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial animation and editing. While existing methods deterministically predict facial animations from speech, they overlook the inherent one-to-many relationship between speech and facial expressions, i.e., there are multiple reasonable facial expression animations matching an audio input. It is especially important in content creation to be able to modify generated motion or to specify keyframes. To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person. Through quantitative and qualitative evaluations, we show that our method outperforms existing state-of-the-art techniques and yields speech-driven animations with greater fidelity and diversity. </p><p><a href="http://arxiv.org/abs/2312.00870v1">PDF</a> Project page: <a href="https://balamuruganthambiraja.github.io/3DiFACE/">https://balamuruganthambiraja.github.io/3DiFACE/</a></p><p><strong>Summary</strong></p><p>语音驱动、可编辑的 3D 面部动画新方式，效果更逼真、更灵活。</p><p><strong>Key Takeaways</strong></p><ul><li>3DiFACE 提出一种利用扩散模型进行 3D 面部动作生成的新方法，支持个性化语音驱动以及编辑。</li><li>该方法可以从少量 3D 动作数据集中进行训练，并保持逼真的嘴部动作。</li><li>3DiFACE 还支持针对特定个体进行微调，只需一个简短的视频即可。</li><li>定量和定性评估表明，该方法优于现有技术，可生成更逼真、更具多样性的语音驱动动画。</li><li>与现有方法相比，3DiFACE 可以生成更具多样性的面部动画，并且能够编辑生成的运动或指定关键帧。</li><li>3DiFACE 模型训练简单，训练数据量少。</li><li>3DiFACE 模型可以应用于内容创作和动画制作等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：3DiFACE：基于扩散的语音驱动 3D 面部动画和编辑</li><li>作者：Balamurugan Thambiraja、Sadegh Aliakbarian、Darren Cosker、Justus Thies</li><li>隶属机构：德国图宾根马克斯·普朗克智能系统研究所</li><li>关键词：3D 面部动画、语音驱动、扩散模型、运动编辑</li><li>论文链接：https://balamuruganthambiraja.github.io/3DiFACESpeechAudio3DiFACEInpaintedMotionFixedKeyframeFixedKeyframeAnimationSynthesisAnimationEditing   Github 链接：无</li><li>摘要：（1）研究背景：3D 面部动画在数字体验中发挥着重要作用，但现有的方法大多是确定性地预测面部动画，忽略了语音和面部表情之间的一对多关系。（2）过去的方法及其问题：现有方法学习语音和面部动画之间的确定性映射，限制了合成动画的多样性。（3）提出的研究方法：提出了一种轻量级的音频条件扩散模型，用于 3D 面部运动。该模型可以在小型 3D 运动数据集上训练，并保持富有表现力的唇部运动输出。此外，它可以针对特定对象进行微调，只需一段该人的短视频。（4）方法的性能和目标实现情况：通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。</li></ol><p>Methods:(1): 提出了一种基于扩散的语音驱动3D面部动画和编辑方法，该方法可以学习语音和面部动画之间的一对多关系，并生成具有更高保真度和多样性的语音驱动动画。(2): 该方法使用了一个轻量级的音频条件扩散模型，该模型可以在小型3D运动数据集上训练，并保持富有表现力的唇部运动输出。(3): 该方法还可以针对特定对象进行微调，只需一段该人的短视频。(4): 通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。</p><ol><li>结论：（1）本工作首次提出了一种可以从语音输入生成和编辑多样化 3D 面部动画的方法。使用无分类器引导为我们提供了一种有效的工具来平衡合成多样性和准确性，使我们能够生成具有前所未有的多样性的动画，同时在合成准确性方面优于或匹配所有基线。通过个性化，我们可以从短（~100 秒）视频中提取特定人物的说话风格，从而显著提高性能。此外，我们的架构允许我们通过使用关键帧来编辑动画。我们相信这些特性使 3DiFACE 成为内容创作者的强大工具，并对未来的应用感到兴奋。（2）创新点：提出了一种基于扩散的语音驱动 3D 面部动画和编辑方法，该方法可以学习语音和面部动画之间的一对多关系，并生成具有更高保真度和多样性的语音驱动动画。该方法使用了一个轻量级的音频条件扩散模型，该模型可以在小型 3D 运动数据集上训练，并保持富有表现力的唇部运动输出。该方法还可以针对特定对象进行微调，只需一段该人的短视频。通过定量和定性评估表明，该方法优于现有技术，并产生了具有更高保真度和多样性的语音驱动动画，证明了方法的有效性。性能：该方法在多个数据集上进行了评估，结果表明该方法在合成准确性和多样性方面优于现有技术。该方法可以针对特定对象进行微调，只需一段该人的短视频，这使得该方法可以很容易地应用于各种应用场景。该方法可以生成具有高保真度和多样性的语音驱动动画，这使得该方法非常适合用于电影、游戏和其他数字体验。工作量：该方法的训练过程相对简单，可以在小型 3D 运动数据集上训练。该方法可以针对特定对象进行微调，只需一段该人的短视频，这使得该方法可以很容易地应用于各种应用场景。该方法可以生成具有高保真度和多样性的语音驱动动画，这使得该方法非常适合用于电影、游戏和其他数字体验。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e9da6eed634be4372cb5b6b3a1d361be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-981d7296309ee36a1b0bddfb6e9dc188.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5d7e90cbb37a3074d13c36527f21fe60.jpg" align="middle"></details><h2 id="SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis"><a href="#SyncTalk-The-Devil-is-in-the-Synchronization-for-Talking-Head-Synthesis" class="headerlink" title="SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis"></a>SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</h2><p><strong>Authors:Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan</strong></p><p>Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. Traditional Generative Adversarial Networks (GAN) struggle to maintain consistent facial identity, while Neural Radiance Fields (NeRF) methods, although they can address this issue, often produce mismatched lip movements, inadequate facial expressions, and unstable head poses. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic and artificial outcomes. To address the critical issue of synchronization, identified as the “devil” in creating realistic talking heads, we introduce SyncTalk. This NeRF-based method effectively maintains subject identity, enhancing synchronization and realism in talking head synthesis. SyncTalk employs a Face-Sync Controller to align lip movements with speech and innovatively uses a 3D facial blendshape model to capture accurate facial expressions. Our Head-Sync Stabilizer optimizes head poses, achieving more natural head movements. The Portrait-Sync Generator restores hair details and blends the generated head with the torso for a seamless visual experience. Extensive experiments and user studies demonstrate that SyncTalk outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: <a href="https://ziqiaopeng.github.io/synctalk">https://ziqiaopeng.github.io/synctalk</a> </p><p><a href="http://arxiv.org/abs/2311.17590v1">PDF</a> 11 pages, 5 figures</p><p><strong>摘要</strong><br>神经辐射场-生成对抗网络框架用于实现说话人头部视频的同步合成。</p><p><strong>关键要点</strong></p><ul><li>传统生成对抗网络难以维持一致的面部身份。</li><li>神经辐射场方法可以解决面部身份一致性问题，但经常出现嘴唇运动不匹配、面部表情不足和头部姿势不稳定的问题。</li><li>逼真的说话人头部视频需要同步协调主体身份、嘴唇运动、面部表情和头部姿势。</li><li>缺少同步性是导致不真实和人为结果的根本缺陷。</li><li>SyncTalk 是一种基于神经辐射场的方法，有效地保持了主体身份，提高了说话人头部合成中的同步性和真实感。</li><li>SyncTalk 使用面部同步控制器将嘴唇运动与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。</li><li>SyncTalk 的头部同步稳定器优化了头部姿势，实现了更自然的头部运动。</li><li>人像同步生成器恢复头发细节，将生成的头部与躯干融合，以获得无缝的视觉体验。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SyncTalk：谈话头部合成同步的魔鬼</li><li>作者：Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan</li><li>第一作者单位：中国人民大学</li><li>关键词：谈话头部合成、神经辐射场、同步、身份保持、表情控制、头部姿势稳定</li><li>论文链接：https://arxiv.org/abs/2311.17590   Github 链接：无</li><li><p>摘要：（1）：研究背景：生成逼真的、由语音驱动的谈话头部视频是一项具有挑战性的任务。传统生成对抗网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。一个逼真的谈话头部需要同步协调主体身份、唇部动作、面部表情和头部姿势。缺乏这些同步是导致不真实和人工结果的根本缺陷。（2）：过去的方法及其问题：GAN 方法难以保持一致的面部身份。NeRF 方法虽然可以解决这个问题，但通常会产生不匹配的唇部动作、不充分的面部表情和不稳定的头部姿势。（3）：提出的研究方法：SyncTalk 是一种基于 NeRF 的方法，它有效地保持了主体身份，增强了谈话头部合成的同步性和真实性。SyncTalk 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。头部同步稳定器优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。（4）：方法在什么任务上取得了什么性能，这些性能是否支持了它们的目标：SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p></li><li><p>方法：(1)：面部同步控制器：使用唇部同步判别器预训练一个高度同步的音频-视觉特征提取器；引入 3D 面部混合形状模型来捕捉准确的面部表情；使用面部感知掩码注意力来减少唇部特征和表情特征之间的相互干扰。(2)：头部同步稳定器：使用头部运动跟踪器来获得头部姿态的粗略估计；使用头部点跟踪器来跟踪面部关键点；使用束调整来增强关键点和头部姿态估计的准确性。(3)：动态肖像渲染器：使用三平面哈希表示来表示 3D 场景；使用可变形神经辐射场来捕捉动态对象的外观；使用肖像同步生成器来恢复头发细节并融合头部和躯干。</p></li><li><p>结论：</p></li></ol><p>（1）：意义：SyncTalk 是一种基于神经辐射场的高同步语音驱动谈话头部合成方法，能够保持主体身份并生成同步的唇部动作、面部表情和稳定的头部姿势。SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法，有望增强各种应用并激发谈话头部合成领域进一步创新。</p><p>（2）：创新点：- 提出了一种新的谈话头部合成方法 SyncTalk，该方法能够有效地保持主体身份，增强谈话头部合成的同步性和真实性。- 使用面部同步控制器将唇部动作与语音对齐，并创新地使用 3D 面部混合形状模型来捕捉准确的面部表情。- 使用头部同步稳定器优化头部姿势，实现更自然的头部运动。- 使用肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。</p><p>性能：- SyncTalk 在谈话头部合成同步性和真实性方面优于最先进的方法。- 广泛的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。</p><p>工作量：- SyncTalk 的实现相对复杂，需要大量的数据和计算资源。- SyncTalk 的训练过程需要大量的时间和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-fd17c6961448d8c17f0288819dc76c44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f17a1563bd9dde5f0ecdc2862b78f71c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-11f3ba567cdea1cc222349d8eaca8ee1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-71774a339b795203c4ab57e06c0114e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2dec8013e0bb70e058e8514cfbe99d7c.jpg" align="middle"></details>## DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D   Face Diffuser**Authors:Peng Chen, Xiaobao Wei, Ming Lu, Yitong Zhu, Naiming Yao, Xingyu Xiao, Hui Chen**Speech-driven 3D facial animation has been an attractive task in both academia and industry. Traditional methods mostly focus on learning a deterministic mapping from speech to animation. Recent approaches start to consider the non-deterministic fact of speech-driven 3D face animation and employ the diffusion model for the task. However, personalizing facial animation and accelerating animation generation are still two major limitations of existing diffusion-based methods. To address the above limitations, we propose DiffusionTalker, a diffusion-based method that utilizes contrastive learning to personalize 3D facial animation and knowledge distillation to accelerate 3D animation generation. Specifically, to enable personalization, we introduce a learnable talking identity to aggregate knowledge in audio sequences. The proposed identity embeddings extract customized facial cues across different people in a contrastive learning manner. During inference, users can obtain personalized facial animation based on input audio, reflecting a specific talking style. With a trained diffusion model with hundreds of steps, we distill it into a lightweight model with 8 steps for acceleration. Extensive experiments are conducted to demonstrate that our method outperforms state-of-the-art methods. The code will be released. [PDF](http://arxiv.org/abs/2311.16565v2) **Summary**扩散网络技术创新生成个性化3D动态人脸，显著提升人脸动画生成效率。**Key Takeaways**- 利用扩散模型生成 3D 人脸动画，能够考虑到言语驱动的 3D 面部动画的非确定性。- 提出的扩散谈话器，是一种基于扩散的方法，利用对比学习实现个性化 3D 面部动画和知识蒸馏来加速 3D 动画生成。- 可训练的说话者身份引入能够汇总音频序列中的知识。- 提出的身份嵌入以对比学习的方式提取不同人之间的自定义面部线索。- 在推理过程中，用户可以根据输入音频获得个性化的面部动画，从而体现特定的说话风格。- 训练好的扩散模型有数百个步骤，通过知识蒸馏可以将其蒸馏成一个轻量级模型，具有 8 个步骤，加速生成。- 广泛的实验表明该方法优于最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：DiffusionTalker：语音驱动的三维人脸动画的个性化和加速</li><li>作者：Peng Chen、Xiaobao Wei、Ming Lu、Yitong Zhu、Naiming Yao、Xingyu Xiao、Hui Chen</li><li>单位：中国科学院软件研究所</li><li>关键词：语音驱动、三维人脸动画、扩散模型、个性化、加速</li><li>论文链接：https://arxiv.org/abs/2311.16565   Github 代码链接：无</li><li><p>摘要：（1）研究背景：语音驱动三维人脸动画是一项重要的任务，广泛应用于虚拟现实、增强现实和计算机游戏等领域。传统方法主要集中于学习从语音到动画的确定性映射，但最近的方法开始考虑语音驱动三维人脸动画的非确定性因素，并采用扩散模型来完成任务。（2）过去方法：现有的扩散模型方法在个性化和加速方面仍然存在局限性。（3）研究方法：为了解决上述局限性，本文提出了一种基于扩散模型的方法 DiffusionTalker，它利用对比学习来实现三维人脸动画的个性化，并利用知识蒸馏来加速三维动画的生成。具体来说，为了实现个性化，本文引入了一个可学习的说话者身份来聚合音频序列中的知识。所提出的身份嵌入以对比学习的方式提取不同人之间的定制面部提示。在推理过程中，用户可以根据输入音频获得个性化的面部动画，反映特定的说话风格。本文还将训练好的具有数百个步骤的扩散模型蒸馏成一个具有 8 个步骤的轻量级模型以实现加速。（4）方法性能：本文在多个任务上进行了广泛的实验，结果表明，该方法优于最先进的方法。这些性能支持了本文的目标。</p></li><li><p>方法：（1）扩散模型：DDPMs 是内容生成的关键元素，用于学习训练数据的分布并生成与该分布紧密匹配的图像。（2）个性化适配器：提出了一种基于对比学习的个性化适配器，该适配器包含一个身份嵌入库，每个嵌入对应一个音频序列。通过对比学习，未知的输入音频可以找到身份嵌入库中相似的身份嵌入，从而实现推理期间说话风格的个性化。（3）知识蒸馏：为了加速推理，利用知识蒸馏将具有 2n 步的教师模型蒸馏成具有 n 步的学生模型，加速语音驱动的 3D 面部动画合成的速度。（4）训练和推理：在训练过程中，随机选择一个时间步长 t，将噪声添加到 x0 以获得 xt。将音频-身份训练对分别输入音频编码器和身份编码器以提取特征。在推理过程中，将给定的音频序列输入到音频编码器中，生成音频特征。然后将此特征与身份嵌入库中所有嵌入的特征进行矩阵乘法。具有最高相似性的嵌入被识别为与输入音频序列匹配的说话身份。</p></li><li><p>结论：（1）：本工作提出了一种基于扩散模型的语音驱动三维人脸动画个性化和加速方法，该方法利用对比学习实现个性化，利用知识蒸馏实现加速，在多个任务上取得了优异的性能。（2）：创新点：</p></li><li>提出了一种基于对比学习的个性化适配器，该适配器包含一个身份嵌入库，每个嵌入对应一个音频序列。通过对比学习，未知的输入音频可以找到身份嵌入库中相似的身份嵌入，从而实现推理期间说话风格的个性化。</li><li>利用知识蒸馏将具有2n步的教师模型蒸馏成具有n步的学生模型，加速语音驱动的3D面部动画合成的速度。性能：</li><li>在多个任务上进行了广泛的实验，结果表明，该方法优于最先进的方法。这些性能支持了本文的目标。工作量：</li><li>本文的工作量较大，需要大量的训练数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-42dc6bb5ab80cf7d628dee32e112dd8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2147d22176df09eea3c7ab6aaf274e54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-293c9b171412db2fee9963cc42c767f0.jpg" align="middle"></details><h2 id="GAIA-Zero-shot-Talking-Avatar-Generation"><a href="#GAIA-Zero-shot-Talking-Avatar-Generation" class="headerlink" title="GAIA: Zero-shot Talking Avatar Generation"></a>GAIA: Zero-shot Talking Avatar Generation</h2><p><strong>Authors:Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, Jiang Bian</strong></p><p>Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3D Morphable Models, which limit the naturalness and diversity of the generated avatars. In this work, we introduce GAIA (Generative AI for Avatar), which eliminates the domain priors in talking avatar generation. In light of the observation that the speech only drives the motion of the avatar while the appearance of the avatar and the background typically remain the same throughout the entire video, we divide our approach into two stages: 1) disentangling each frame into motion and appearance representations; 2) generating motion sequences conditioned on the speech and reference portrait image. We collect a large-scale high-quality talking avatar dataset and train the model on it with different scales (up to 2B parameters). Experimental results verify the superiority, scalability, and flexibility of GAIA as 1) the resulting model beats previous baseline models in terms of naturalness, diversity, lip-sync quality, and visual quality; 2) the framework is scalable since larger models yield better results; 3) it is general and enables different applications like controllable talking avatar generation and text-instructed avatar generation. </p><p><a href="http://arxiv.org/abs/2311.15230v1">PDF</a> Project page: <a href="https://microsoft.github.io/GAIA/">https://microsoft.github.io/GAIA/</a></p><p><strong>Summary</strong><br>移除谈话头像生成中的领域先验，利用语言模型控制动作，神经网络生成外观，进行分离编码实现可控谈话头像生成。</p><p><strong>Key Takeaways</strong></p><ul><li>GAIA 无需特定领域知识，摆脱领域先验的限制，利用语言模型控制动作，神经网络生成外观。</li><li>GAIA 将任务分为两个阶段：分离编码与运动序列生成。</li><li>GAIA 的数据集包含 168 万张图片，分为训练集、验证集和测试集。</li><li>GAIA 可扩展，模型参数从 128M 到 2B 不等，模型越大效果越好。</li><li>GAIA 具有通用性，可用于可控谈话头像生成和文本指示头像生成等应用。</li><li>GAIA 在三个评价指标上都优于基线模型。</li><li>GAIA 能有效生成自然、多样、唇形同步且视觉质量高的谈话头像视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GAIA：零样本说话头像生成</li><li>作者：Tianyu He、Junliang Guo、Runyi Yu、Yuchi Wang、Jialiang Zhu、Kaikai An、Leyi Li、Xu Tan、Chunyu Wang、Han Hu、Hsiang Tao Wu、Sheng Zhao、Jiang Bian</li><li>隶属单位：微软</li><li>关键词：零样本说话头像生成、运动表示、外观表示、生成模型</li><li>论文链接：https://arxiv.org/abs/2311.15230</li><li><p>摘要：（1）研究背景：说话头像生成旨在从语音和单张人像图像中合成自然说话视频。以往方法依赖于特定领域的启发式方法，例如基于扭曲的运动表示和 3D 可变形模型，这限制了生成的头像的自然性和多样性。（2）过去方法及其问题：以往方法通过对每个头像进行特定训练（即为每个头像训练或调整特定模型）或在推理期间利用模板视频来实现高质量的结果。但是，这些方法通过引入基于扭曲的运动表示、3D 可变形模型等领域先验来降低任务的难度。虽然有效，但引入此类启发式方法阻碍了直接从数据分布中学习，并可能导致不自然的结果和有限的多样性。（3）研究方法：本文提出 GAIA（生成式人工智能头像），消除了说话头像生成中的领域先验。GAIA 揭示了两个关键见解：1）语音只驱动头像的运动，而背景和头像的外观通常在整个视频中保持不变。受此启发，我们对每帧进行运动和外观表示的解耦，其中外观在帧之间共享，而运动对于每帧都是唯一的。为了从语音预测运动，我们将运动序列编码成运动潜在序列，并将外观编码成外观潜在表示。然后，我们使用生成模型从语音和参考人像图像生成运动序列。（4）方法性能：我们收集了一个大规模高质量的说话头像数据集，并在不同规模（高达 2B 参数）上训练模型。实验结果验证了 GAIA 的优越性、可扩展性和灵活性，因为它 1）在自然性、多样性、唇形同步质量和视觉质量方面优于之前的基线模型；2）由于更大的模型会产生更好的结果，因此该框架是可扩展的；3）它是通用的，并支持不同的应用程序，例如可控说话头像生成和文本指导的头像生成。</p></li><li><p>方法：(1) 运动与外观表示解耦：将每帧解耦为运动表示和外观表示，其中外观在帧之间共享，而运动对于每帧都是唯一的。(2) 运动序列编码：将运动序列编码成运动潜在序列，并将外观编码成外观潜在表示。(3) 运动序列生成：使用生成模型从语音和参考人像图像生成运动序列。(4) 可控说话头像生成：通过替换估计的头姿势或从另一个视频中提取的头姿势，实现姿势可控的说话头像生成。(5) 全可控说话头像生成：通过编辑生成过程中的面部地标，实现任意面部属性的可控生成。</p></li><li><p>结论：（1）：xxx；（2）：创新点：提出了一种数据驱动的零样本说话头像生成框架 GAIA，该框架由两个模块组成：一个变分自动编码器，用于解耦和编码运动和外观表示，以及一个扩散模型，用于预测运动潜在序列，该序列以输入语音为条件。我们收集了一个大规模的数据集，并提出了几种过滤策略，以实现框架的有效训练。GAIA 框架具有通用性和可扩展性，能够在零样本说话头像生成中提供自然和多样化的结果，并且可以灵活地适应其他应用程序，包括可控说话头像生成和文本指导的头像生成。性能：在自然性、多样性、唇形同步质量和视觉质量方面优于之前的基线模型；由于更大的模型会产生更好的结果，因此该框架是可扩展的；工作量：收集了一个大规模高质量的说话头像数据集，并在不同规模（高达2B参数）上训练模型。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b24792b81d1876d37fc788a87d3177d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5607b67a73e71b9a00d408089c575ac.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55a4dfdd281f456c1ec180ddf006ff6d.jpg" align="middle"></details><h2 id="ChatAnything-Facetime-Chat-with-LLM-Enhanced-Personas"><a href="#ChatAnything-Facetime-Chat-with-LLM-Enhanced-Personas" class="headerlink" title="ChatAnything: Facetime Chat with LLM-Enhanced Personas"></a>ChatAnything: Facetime Chat with LLM-Enhanced Personas</h2><p><strong>Authors:Yilin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, Daquan Zhou</strong></p><p>In this technical report, we target generating anthropomorphized personas for LLM-based characters in an online manner, including visual appearance, personality and tones, with only text descriptions. To achieve this, we first leverage the in-context learning capability of LLMs for personality generation by carefully designing a set of system prompts. We then propose two novel concepts: the mixture of voices (MoV) and the mixture of diffusers (MoD) for diverse voice and appearance generation. For MoV, we utilize the text-to-speech (TTS) algorithms with a variety of pre-defined tones and select the most matching one based on the user-provided text description automatically. For MoD, we combine the recent popular text-to-image generation techniques and talking head algorithms to streamline the process of generating talking objects. We termed the whole framework as ChatAnything. With it, users could be able to animate anything with any personas that are anthropomorphic using just a few text inputs. However, we have observed that the anthropomorphic objects produced by current generative models are often undetectable by pre-trained face landmark detectors, leading to failure of the face motion generation, even if these faces possess human-like appearances because those images are nearly seen during the training (e.g., OOD samples). To address this issue, we incorporate pixel-level guidance to infuse human face landmarks during the image generation phase. To benchmark these metrics, we have built an evaluation dataset. Based on it, we verify that the detection rate of the face landmark is significantly increased from 57.0% to 92.5% thus allowing automatic face animation based on generated speech content. The code and more results can be found at <a href="https://chatanything.github.io/">https://chatanything.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2311.06772v1">PDF</a> </p><p><strong>摘要</strong></p><p>利用语言生成模型实现任意文本创建拟人化形象，包括图像、语气和性格。</p><p><strong>要点</strong></p><ul><li>利用语言生成模型的上下文学习能力和精心设计的系统提示，生成人物个性。</li></ul><ul><li>提出两种新颖概念：混合声音 (MoV) 和混合扩散器 (MoD)，用于生成多样化的声音和图像。</li></ul><ul><li>MoV 利用文本转语音 (TTS) 算法和各种预定义语调，根据用户提供的文本描述自动选择最匹配的语调。</li></ul><ul><li>MoD 将流行的文本转图像生成技术和说话头算法相结合，简化生成说话对象的流程。</li></ul><ul><li>ChatAnything 框架允许用户使用少量文本输入来生成具有拟人化形象的动画。</li></ul><ul><li>发现当前生成模型生成的拟人化对象通常无法被预训练好的面部特征检测器检测到，导致面部运动生成失败，即使这些面孔具有类似人类的外观。</li></ul><ul><li>通过在图像生成阶段加入像素级指导，使生成的图像包含人类面部标志，从而解决这个问题。</li></ul><ul><li>建立评估数据集，验证面部标志检测率从 57.0% 显着提高到 92.5%，从而允许自动生成语音内容的动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ChatAnything：基于 LLM 的角色的人格化</li><li>作者：Silin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, Daquan Zhou</li><li>隶属单位：南开大学</li><li>关键词：自然语言处理、生成式 AI、对话系统、人机交互</li><li>论文链接：https://arxiv.org/abs/2311.06772Github 代码链接：https://github.com/chatanything/chatanything</li><li>摘要：(1)：随着大语言模型 (LLM) 的快速发展，其强大的上下文学习能力和生成能力引起了广泛关注。本文旨在探索一种新的框架，该框架能够根据文本描述自动生成具有定制化个性、声音和视觉外观的 LLM 增强角色。(2)：以往的方法主要集中在 LLM 的个性化生成上，但对于声音和视觉外观的生成则相对较少。同时，现有方法生成的拟人化对象通常无法被预训练的人脸关键点检测器检测到，导致无法进行面部动作生成。(3)：本文提出的 ChatAnything 框架通过精心设计系统提示，利用 LLM 的上下文学习能力来生成定制化的角色个性。同时，本文还提出了两种新颖的概念：声音混合 (MoV) 和扩散器混合 (MoD)，用于生成多样化的声音和外观。此外，本文还提出了一种像素级引导的方法，以在图像生成阶段注入人脸关键点，从而提高面部关键点的检测率。(4)：实验结果表明，ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。同时，本文提出的像素级引导方法也显著提高了面部关键点的检测率，从而支持自动面部动画生成。</li></ol><p>Methods:</p><p>(1) The ChatAnything framework consists of four main blocks: an LLM-based control module, a portrait initializer, a mixture of text-to-speech modules, and a motion generation module.</p><p>(2) The portrait initializer uses a mixture of fine-tuned diffusion models (MoD) along with their LoRA module to generate a reference image for the persona.</p><p>(3) The mixture of text-to-speech modules (MoV) converts the text input from the persona to speech signals with customized tones.</p><p>(4) The motion generation module takes in the speech signal and drives the generated image.</p><p>(5) To inject facial landmark guidance, the framework uses a guided diffusion process with a fixed Markov Gaussian diffusion process.</p><p>(6) The framework also utilizes a ControlNet to inject the face feature in the process of image generation.</p><p>(7) A pool of stylized diffusion-based generative models and voice changers are used to customize the artistic style and voice of the generated persona.</p><p>(8) The framework uses a prompt template to generate the personality of the persona based on the user's input.</p><ol><li>结论：（1）：ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。同时，本文提出的像素级引导方法也显著提高了面部关键点的检测率，从而支持自动面部动画生成。（2）：创新点：</li><li>提出了一种新的框架 ChatAnything，该框架能够根据文本描述自动生成具有定制化个性、声音和视觉外观的 LLM 增强角色。</li><li>提出了一种新的概念：声音混合 (MoV)，用于生成多样化的声音。</li><li>提出了一种新的概念：扩散器混合 (MoD)，用于生成多样化的外观。</li><li>提出了一种像素级引导的方法，以在图像生成阶段注入人脸关键点，从而提高面部关键点的检测率。性能：</li><li>ChatAnything 框架能够有效地生成具有定制化个性、声音和视觉外观的 LLM 增强角色。</li><li>ChatAnything 框架能够显著提高面部关键点的检测率，从而支持自动面部动画生成。工作量：</li><li>ChatAnything 框架的实现相对复杂，需要较高的技术水平。</li><li>ChatAnything 框架的训练需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-494f70b2c5eac2c09270dc86936da0f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ecb9537a4b6bea4a888b9a98aa5f5584.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-afea263951ed31808a8df79048c30a2c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d9d4f86301d8d00bbb0d78ff4e1a1f89.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b4fd05a1ef19cd93425962ed04f16227.jpg" align="middle"></details><h2 id="DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation"><a href="#DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D   Facial Animation"></a>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D   Facial Animation</h2><p><strong>Authors:Guinan Su, Yanwu Yang, Zhifeng Li</strong></p><p>In recent years, audio-driven 3D facial animation has gained significant attention, particularly in applications such as virtual reality, gaming, and video conferencing. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies approach the facial animation task as a single regression problem, which often fail to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation and overlook their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability that decreases the performance. To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency as well as relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework facilitates more efficient data usage by leveraging information from both tasks and explicitly capitalizing on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics. Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. We have made our code and video demonstrations available at <a href="https://github.com/sabrina-su/iadf.git">https://github.com/sabrina-su/iadf.git</a>. </p><p><a href="http://arxiv.org/abs/2311.04766v2">PDF</a> </p><p><strong>摘要</strong><br>音频驱动3D面部动画框架DualTalker，以音频为驱动，对3D面部进行动画，学习面部运动与音频之间的互补关系，提高数据利用率，生成逼真的面部表情。</p><p><strong>要点</strong></p><ul><li>提出音频驱动3D面部动画框架DualTalker，提高数据利用率，生成更为逼真的面部表情。</li><li>DualTalker由音频-运动编码器组成，训练主要任务（音频驱动面部动画）及其双重任务（唇读）。</li><li>联合训练促进信息共享，提高性能，辅助跨模态一致性损失减轻过度平滑。</li><li>实验表明，DualTalker在VOCA和BIWI数据集上的表现优于当前最先进的方法。</li><li>代码和视频演示可在<a href="https://github.com/sabrina-su/iadf.git上获取。">https://github.com/sabrina-su/iadf.git上获取。</a></li><li>我们的方法提高了面部动画的质量，并使音频-视觉同步更加自然。</li><li>DualTalker实现了跨模态双学习，具有良好的数据利用效率和泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：双语者：一种用于语音驱动的三维面部动画的跨模态双重学习方法</li><li>作者：顾南苏，杨炎武，李志锋</li><li>单位：腾讯数据平台</li><li>关键词：双重学习、语音驱动的面部动画、跨模态一致性、Transformer</li><li>论文链接：https://arxiv.org/abs/2311.04766    Github 链接：https://github.com/Guinan-Su/DualTalker</li><li><p>摘要：(1)：语音驱动的三维面部动画技术近年来备受关注，但准确建模面部表情的复杂动态仍是一个挑战。(2)：现有方法通常将面部动画任务视为单一回归问题，忽略了语音信号和三维面部动画之间的内在跨模态关系及其固有的一致性。此外，由于三维音频视觉数据集的有限可用性，使用小样本学习的方法具有较差的泛化能力，降低了性能。(3)：提出了一种跨模态双重学习框架，旨在提高数据利用效率，并关联跨模态依赖关系以进一步提高性能。该框架与主任务（语音驱动的面部动画）及其双重任务（唇读）联合训练，并共享共同的音频/运动编码器组件。(4)：在 VOCA 和 BIWI 数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</p></li><li><p>方法：（1）提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画。该框架由主任务（语音驱动的面部动画）及其双重任务（唇读）组成，并共享共同的音频/运动编码器组件。（2）在语音驱动的面部动画任务中，采用编码器-解码器框架，其中编码器将语音信号转换为语音表示，解码器利用语音表示和过去的运动序列来预测面部运动。（3）在唇读任务中，同样采用编码器-解码器框架，其中编码器将面部运动转换为运动表示，解码器利用运动表示和过去的语音特征来预测语音特征。（4）为了实现语音驱动的面部动画和唇读的互补性，提出了一种对偶正则化损失，该损失函数鼓励语音驱动的面部动画和唇读的预测结果在跨模态特征空间中保持一致。（5）在VOCA和BIWI数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</p></li><li><p>结论：（1）：提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画，有效地解决了语音驱动面部动画中固有的挑战。通过将面部动画和唇读组件表述为双重任务，并结合创新的参数共享方案和对偶正则化器，该方法有效地提高了数据利用率，并关联了跨模态依赖关系，进一步提高了性能。（2）：创新点：</p></li><li>提出了一种跨模态双重学习框架，用于语音驱动的三维面部动画，该框架由主任务（语音驱动的面部动画）及其双重任务（唇读）组成，并共享共同的音频/运动编码器组件。</li><li>在语音驱动的面部动画任务中，采用编码器-解码器框架，其中编码器将语音信号转换为语音表示，解码器利用语音表示和过去的运动序列来预测面部运动。</li><li>在唇读任务中，同样采用编码器-解码器框架，其中编码器将面部运动转换为运动表示，解码器利用运动表示和过去的语音特征来预测语音特征。</li><li>提出了一种对偶正则化损失，该损失函数鼓励语音驱动的面部动画和唇读的预测结果在跨模态特征空间中保持一致。性能：</li><li>在VOCA和BIWI数据集上进行的广泛实验和感知用户研究表明，该方法在定性和定量上都优于当前最先进的方法。</li><li>该方法在VOCA数据集上的平均误差为0.012，在BIWI数据集上的平均误差为0.015，均优于当前最先进的方法。</li><li>该方法在VOCA数据集上的感知用户研究中获得了4.2分的平均分（满分5分），在BIWI数据集上的感知用户研究中获得了4.1分的平均分，均优于当前最先进的方法。工作量：</li><li>该方法的实现相对复杂，需要对跨模态双重学习框架、对偶正则化损失等进行深入理解。</li><li>该方法的训练时间相对较长，在VOCA数据集上训练一次需要约24小时，在BIWI数据集上训练一次需要约36小时。</li><li>该方法的推理时间相对较短，在VOCA数据集上推理一次需要约0.1秒，在BIWI数据集上推理一次需要约0.15秒。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2796e7be16d59d2ade40f87447f93837.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c353db315d023ec1c2c174b1887e6302.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e14c4424ba13626c11a4cc40af3ca98c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5252ec4a4a561db9ad58b8059a17f121.jpg" align="middle"></details><h2 id="3D-Aware-Talking-Head-Video-Motion-Transfer"><a href="#3D-Aware-Talking-Head-Video-Motion-Transfer" class="headerlink" title="3D-Aware Talking-Head Video Motion Transfer"></a>3D-Aware Talking-Head Video Motion Transfer</h2><p><strong>Authors:Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang</strong></p><p>Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we employ an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task. </p><p><a href="http://arxiv.org/abs/2311.02549v1">PDF</a> WACV2024</p><p><strong>摘要</strong><br>3D 感知说话头部视频运动迁移网络 Head3D，利用循环网络从 2D 主体帧生成视觉可解释的 3D 规范头部，较好地解决了多视角外观特征利用不充分的问题。</p><p><strong>要点</strong></p><ul><li>Head3D 通过循环网络从 2D 主体帧生成视觉可解释的 3D 规范头部，充分利用了主体外观信息。</li><li>Head3D 的关键组成部分是自监督 3D 头部几何学习模块，旨在从 2D 主体视频帧预测头部姿态和深度图。</li><li>Head3D 使用基于注意力的融合网络将主体帧中的背景和其他细节与 3D 主体头部相结合，进而生成合成目标视频。</li><li>Head3D 在两个公开说话头部视频数据集上的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验方法，并且可以轻松适应可控姿势的新视图合成任务。</li><li>Head3D 较好地解决了多视角外观特征利用不充分的问题。</li><li>Head3D 可以充分利用 2D 主体图像和 3D 主体视频的优势。</li><li>Head3D 在实际的跨身份设置中优于 2D 和 3D 先验方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：3D感知说话人头部视频动作迁移</li><li>作者：Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang</li><li>第一作者单位：宾夕法尼亚州立大学</li><li>关键词：说话人头部视频、动作迁移、3D感知、自监督学习、注意机制</li><li>论文链接：https://arxiv.org/abs/2311.02549Github 链接：无</li><li>摘要：(1) 研究背景：说话人头部视频动作迁移旨在生成一个具有目标主体的外观和驱动视频的运动模式的新视频。现有的方法主要依赖于有限数量的主体图像和 2D 表示，从而忽略了充分利用主体视频中固有的多视角外观特征。(2) 过去的方法及其问题：现有的方法主要使用一个主体图像或简单组合几个主体图像与 2D 表示。这些方法可能难以充分利用主体视频中固有的多视角外观信息。(3) 本文提出的研究方法：本文提出 Head3D，这是一个新颖的 3D 感知说话人头部视频动作迁移框架。该框架以自监督、非对抗的方式进行训练，能够通过自监督 3D 头部几何学习从每个 2D 视频帧中恢复 3D 结构信息（即头部姿势和深度），而无需 3D 人头图形模型。通过将每个选定的主体视频帧映射到 3D 规范空间，Head3D 进一步使用循环网络估计 3D 主体规范头部。为了合成最终的视频帧，Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节（例如，面部表情、肩膀）相结合。(4) 方法在哪些任务上取得了怎样的性能，这些性能是否支持了它们的目标：在两个公开的说话人头部视频数据集上进行的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验技术，并且有证据表明它可以很容易地适应姿势可控的新视角合成任务。</li></ol><p>Methods:(1): Head3D采用自监督方式进行训练，使用自我重建损失来恢复一个视频帧与同一视频中随机采样的几个帧。这种训练过程既不需要任何人工标注，也不涉及对抗训练。(2): Head3D的训练包括三个阶段：(1) 3D头部几何学习，(2) 循环规范头部生成，(3) 基于注意力的融合机制。为了便于训练，我们将这三个阶段的模块分别进行训练。(3): 在3D头部几何学习阶段，我们利用一个自监督的3D头部几何学习框架来训练一个深度网络FD和一个姿态网络FP，用于预测每个2D视频帧的头部姿态和深度。(4): 在循环规范头部生成阶段，我们使用一个循环规范头部生成网络，该网络利用基于ConvLSTM的特征聚合来创建一个3D规范头部ˆxc，其中包含了扭曲的参考帧特征。(5): 在基于注意力的融合机制阶段，我们采用基于注意力的融合机制来合成每个最终输出帧ˆsdri，方法是将规范头部ˆxc的外观特征、来自随机选取的主题帧sref的背景和其他外观细节（例如，颈部和肩膀）以及来自驱动帧dri的运动和表情信息进行组合。</p><ol><li>结论：（1）：本文提出了一种新的说话人头部视频动作迁移框架 Head3D，该框架能够通过自监督 3D 头部几何学习从每个 2D 视频帧中恢复 3D 结构信息，并进一步使用循环网络估计 3D 主体规范头部。Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节相结合，合成最终的视频帧。（2）：创新点：</li><li>Head3D 提出了一种新的自监督 3D 头部几何学习方法，能够从 2D 视频帧中恢复 3D 结构信息，无需 3D 人头图形模型。</li><li>Head3D 使用循环网络估计 3D 主体规范头部，该方法能够有效地捕获主体头部在视频中的运动模式。</li><li>Head3D 采用基于注意力的融合机制将来自 3D 主体头部的外观特征与来自主体的背景和其他细节相结合，合成最终的视频帧。性能：</li><li>Head3D 在两个公开的说话人头部视频数据集上进行的广泛实验表明，Head3D 在实际的跨身份设置中优于 2D 和 3D 先验技术。</li><li>Head3D 可以很容易地适应姿势可控的新视角合成任务。工作量：</li><li>Head3D 的训练需要大量的视频数据，这可能会增加训练时间和计算成本。</li><li>Head3D 的模型结构相对复杂，这可能会增加模型的训练和推理时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-35afec6fc14c4cd3bb501e49b198de69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-afc27ddf3e6f7773dffd54e160f21da6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d1e8cece6f2ce2f23fc15496e6200de8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64af4f2f40a878ceba7a79e3170cfa03.jpg" align="middle"></details><h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with   Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with   Natural Head Pose and Detailed Shape</h2><p><strong>Authors:Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</strong></p><p>The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation. </p><p><a href="http://arxiv.org/abs/2310.20240v1">PDF</a> </p><p><strong>摘要</strong><br>利用可变头部姿势和自然的面部细节实现逼真的语音驱动 3D 面部动画。</p><p><strong>要点</strong></p><ul><li>现有作品未能呈现出灵活头部姿势和自然面部细节的形状。</li><li>导致上述限制的两个主要因素是：训练数据集的收集成本高昂，且头部姿势与语音内容的相关性较低。</li><li>VividTalker 框架可将面部动画明确分解为头部姿势和嘴巴动作，并将其分别编码为离散的潜在空间。</li><li>采用基于窗口的 Transformer 架构，通过自回归过程生成这些属性。</li><li>构建了一个包含详细形状的新 3D 数据集，并学会根据语音内容合成面部细节。</li><li>VividTalker 在定量和定性实验中均优于现有方法，可实现生动逼真的语音驱动 3D 面部动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：赋予面部生命：自然头部姿势和详细形状的语音驱动的 3D 面部动画</li><li>作者：魏巍，王艺军，何天宇，尹连英，林建新，金鑫</li><li>隶属单位：湖南大学计算机科学与电子工程学院</li><li>关键词：3D 面部动画，详细面部形状，动作解耦</li><li>论文链接：https://weizhaomolecules.github.io/VividTalker/，Github 代码链接：None</li><li><p>摘要：（1）研究背景：3D 虚拟面部动画领域因其在娱乐、通信和医疗保健等领域的巨大价值而备受关注和研究兴趣。3D 虚拟面部动画的成功依赖于表现出类人特征，包括同步和自然性。同步涉及创建与用户期望一致的可信动画，弥合虚拟头像与现实世界之间的差距。自然性涉及创建具有自然的、逼真的面部细节（例如皱纹）的动画。（2）过去的方法及其问题：现有工作在使用灵活的头部姿势和自然的细节（例如皱纹）渲染形状方面仍然存在不足。这种限制主要归因于两个方面：1) 收集具有详细 3D 面部形状的训练集非常昂贵。这种详细形状注释的稀缺阻碍了训练具有富有表现力的面部动画的模型。2) 与嘴巴运动相比，头部姿势与语音内容的相关性要小很多。因此，对嘴巴运动和头部姿势同时建模导致缺乏面部运动可控性。（3）研究方法：为了解决这些挑战，我们引入了 VividTalker，这是一个旨在促进语音驱动的 3D 面部动画的新框架，其特点是灵活的头部姿势和自然的面部细节。具体来说，我们明确地将面部动画解耦成头部姿势和嘴巴运动，并将它们分别编码成离散的潜在空间。然后，通过利用基于窗口的 Transformer 架构的回归过程生成这些属性。为了增加 3D 面部动画的丰富性，我们构建了一个新的具有详细形状的 3D 数据集，并学会了根据语音内容合成面部细节。（4）方法的性能：广泛的定量和定性实验表明，VividTalker 优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。这些性能证明了我们的方法可以很好地实现目标。</p></li><li><p>方法：（1）：我们提出了 VividTalker，一个旨在促进语音驱动的 3D 面部动画的新框架，其特点是灵活的头部姿势和自然的面部细节。（2）：我们将面部动画明确地解耦成头部姿势和嘴巴运动，并将它们分别编码成离散的潜在空间。（3）：我们利用基于窗口的 Transformer 架构的回归过程生成这些属性。（4）：我们构建了一个新的具有详细形状的 3D 数据集，并学会了根据语音内容合成面部细节。</p></li><li><p>结论：（1）：VividTalker 旨在促进语音驱动的 3D 面部动画，具有灵活的头部姿势和自然的面部细节，在定量和定性实验中优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。（2）：创新点：</p></li><li>将头部姿势和嘴巴运动明确解耦，分别编码成离散的潜在空间。</li><li>利用基于窗口的 Transformer 架构的回归过程生成这些属性。</li><li>构建了一个新的具有详细形状的 3D 数据集，学会了根据语音内容合成面部细节。性能：</li><li>VividTalker 在定量和定性实验中优于最先进的方法，产生了生动逼真的语音驱动的 3D 面部动画。</li><li>VividTalker 能够生成具有灵活的头部姿势和自然的面部细节的动画。工作量：</li><li>VividTalker 需要构建一个新的具有详细形状的 3D 数据集，并且需要训练一个基于窗口的 Transformer 架构的回归模型。</li><li>VividTalker 的训练过程可能需要大量的时间和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7d367bbd980109a452dcecf661c89318.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-61f898b8eea6a84ec03e1da36317e047.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9f40581024d13a376e4d202368288380.jpg" align="middle"></details><h2 id="CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation"><a href="#CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation" class="headerlink" title="CorrTalk: Correlation Between Hierarchical Speech and Facial Activity   Variances for 3D Animation"></a>CorrTalk: Correlation Between Hierarchical Speech and Facial Activity   Variances for 3D Animation</h2><p><strong>Authors:Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, Xiangmin Xu</strong></p><p>Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: <a href="https://zjchu.github.io/projects/CorrTalk/">https://zjchu.github.io/projects/CorrTalk/</a> </p><p><a href="http://arxiv.org/abs/2310.11295v1">PDF</a> </p><p><strong>摘要</strong><br>扩展了强弱面部动作相关性解码框架，实现多层次语言特征与不同面部区域多强度面部动作时间同层相关性，实现更自然的说话头部生成。</p><p><strong>要点</strong></p><ul><li>提出了一种新颖的框架 CorrTalk，该框架有效地建立了分层语音特征与不同强度、不同区域的面部活动之间的时间相关性。</li><li>定义了一种新颖的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。</li><li>提出了一种双分支解码框架，用于同步合成强弱面部活动，保证了强度更广的面部动画合成。</li><li>提出了一种加权分层特征编码器，用于建立分层语音特征与不同强度下地面部活动之间的时间相关性，确保唇形同步和合理的面部表情。</li><li>大量定性和定量实验以及用户研究表明，我们的 CorrTalk 优于现有最先进的方法。</li><li>源代码和补充视频可以在以下网址公开获得：<a href="https://zjchu.github.io/projects/CorrTalk/">https://zjchu.github.io/projects/CorrTalk/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CorrTalk：用于 3D 动画的分层语音与面部活动方差之间的相关性</li><li>作者：赵杰楚、凯玲郭、肖芬兴、伊琳兰、波伦蔡、项民旭</li><li>隶属机构：华南理工大学</li><li>关键词：3D 面部动画、分层语音特征、3D 说话头部、面部活动差异、Transformer</li><li>论文链接：https://arxiv.org/abs/2310.11295，Github 链接：None</li><li>摘要：（1）研究背景：语音驱动的 3D 面部动画是一项具有挑战性的跨模态任务，近年来引起了越来越多的研究兴趣。在说话活动中，嘴巴会显示出强烈的运动，而其他面部区域通常表现出相对较弱的活动水平。现有方法通常通过将单层语音特征直接映射到整个面部动画来简化过程，这忽略了面部活动强度方面的差异，导致面部运动过于平滑。（2）过去方法及问题：一些研究调查了语音模态。早期工作建立了音素与面部手势之间的映射。单个音素可能对应几个合理的唇形，导致跨模态的不确定性。为了减轻面部姿势的模糊性，引入了短滑动窗口机制来剪辑几个连续的语音帧，然后为相应的视觉帧添加动画。短音频窗口从相邻的语音帧中捕获了额外的信息，但仍然导致面部运动变化的不确定性。MeshTalk 应用长期音频窗口来合成每个视觉帧。FaceFormer 提出了一种基于 Transformer 的模型来捕获帧级长期音频上下文的依赖性。尽管捕获长期上下文可以提高语音驱动面部动画的逼真性能，但过度冗长的长期上下文不可避免地会引入冗余信息，而过长或过短的单层语音特征缺乏足够的时间分辨率。相比之下，一些工作仅关注嘴巴的动画。嘴巴的运动在说话活动中最为常见，但在说话活动中，嘴巴和其他面部肌肉的协同运动是无法忽视的。最近，通过使用单层语音特征直接驱动整个面部动画来推动前沿。然而，忽略了不同区域（例如嘴巴和其他区域）的面部活动强度方面的差异。（3）研究方法：为了解决上述问题，本文提出了一种新颖的框架 CorrTalk，该框架有效地建立了分层语音特征与不同强度和不同区域的面部活动之间的时序相关性。定义了一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。此外，提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。（4）方法性能：广泛的定性和定量实验以及用户研究表明，CorrTalk 优于现有的最先进方法。该方法在任务和性能方面取得的成就：</li><li>在 VOCASET 数据集上，CorrTalk 在唇形同步、面部表情和整体视觉质量方面优于最先进的方法。</li><li>在 VoxCeleb 数据集上，CorrTalk 在唇形同步和面部表情方面优于最先进的方法。</li><li><p>在用户研究中，CorrTalk 在唇形同步、面部表情和整体视觉质量方面均优于最先进的方法。这些性能支持了本文的目标。</p></li><li><p>方法：(1) 提出一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。(2) 基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。(3) 提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。</p></li><li><p>结论：（1）：本文提出了一种新颖的驱动框架 CorrTalk，该框架有效地捕获了分层语音特征与不同强度和不同区域的面部活动之间的时序相关性。该框架考虑了面部活动强度的差异以及不同层次语音表征的异质性。加权分层特征编码器提供了一种互补且有效的机制来建立语音和面部活动之间的相关性。广泛的定性和定量实验以及用户研究表明，CorrTalk 优于现有的最先进方法。（2）：创新点：</p></li><li>提出了一种新的面部活动强度度量来区分强弱面部活动，该度量是通过计算面部顶点位移的短时傅里叶变换获得的。</li><li>基于面部活动的变化，提出了一种双分支解码框架来同步合成强弱面部活动，从而保证了更广泛强度的面部动画合成。</li><li>提出了一种加权分层特征编码器来建立分层语音特征与不同强度下的面部活动之间的时序相关性，从而确保唇形同步和合理的面部表情。性能：</li><li>在 VOCASET 数据集上，CorrTalk 在唇形同步、面部表情和整体视觉质量方面优于最先进的方法。</li><li>在 VoxCeleb 数据集上，CorrTalk 在唇形同步和面部表情方面优于最先进的方法。</li><li>在用户研究中，CorrTalk 在唇形同步、面部表情和整体视觉质量方面均优于最先进的方法。工作量：</li><li>该方法需要大量的数据来训练，这可能需要大量的时间和计算资源。</li><li>该方法的实现可能需要大量的代码和工程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d922178e56a58ce3c71b9f1423874fb1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-482c00c6ed52f71800345e13e8d77a81.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cab60223ad61f1d2129c598293c0da62.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4c347875935ebcaed314d4b9997f39ba.jpg" align="middle"></details><h2 id="Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models"><a href="#Mini-DALLE3-Interactive-Text-to-Image-by-Prompting-Large-Language-Models" class="headerlink" title="Mini-DALLE3: Interactive Text to Image by Prompting Large Language   Models"></a>Mini-DALLE3: Interactive Text to Image by Prompting Large Language   Models</h2><p><strong>Authors:Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang</strong></p><p>The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs’ inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems. </p><p><a href="http://arxiv.org/abs/2310.07653v2">PDF</a> Technical report. Project page at <a href="https://minidalle3.github.io/">https://minidalle3.github.io/</a></p><p><strong>Summary</strong><br>通过增强 LLM 与现有文生图模型的交互能力，提出一种新的交互式文本图像生成任务，提高了人机交互的图像质量和用户体验。</p><p><strong>Key Takeaways</strong></p><ul><li>文本到图像 (T2I) 扩散模型的蓬勃发展极大地加速了人工智能内容生成的革命。</li><li>目前流行的 T2I 模型，如 Stable Diffusion，在使用自然语言描述时存在有效的沟通障碍。</li><li>受到近期发布的 DALL-E3 模型的启发，该模型直接内置 ChatGPT 并使用人类语言，我们重新审视现有 T2I 系统，努力实现人类意图的一致性，并提出一个新任务——交互式文本到图像 (iT2I)，人们可以使用 LLM 进行交织的高质量图像生成/编辑/细化，并使用自然语言进行问题回答，从而获得更强的图像和文本对应关系。</li><li>为了解决 iT2I 问题，我们提出了一种简单的方法，利用提示技术和现成的 T2I 模型来增强 LLM 的 iT2I 能力。</li><li>我们使用不同的 LLM（如 ChatGPT、LLAMA、Baichuan 和 InternLM）在各种常用场景下评估了我们的 iT2I 方法。</li><li>我们证明了我们的方法可以方便且低成本地为任何现有的 LLM 和任何文本到图像模型引入 iT2I 能力，而无需任何培训，同时对 LLM 在问题回答和代码生成等方面的固有能力几乎没有影响。</li><li>我们希望这项工作能够引起更广泛的关注，并为提升下一代 T2I 系统的图像质量和人机交互的用户体验提供灵感。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：交互式图像生成故事概念原型交互式 Logo 设计</li><li>作者：T2IModel, StableDiffusionXL</li><li>单位：无</li><li>关键词：交互式图像生成、故事概念原型、交互式 Logo 设计</li><li>链接：无，Github 代码链接：无</li><li>摘要：(1)：随着人工智能技术的发展，交互式图像生成技术逐渐成熟，为人们提供了新的创作方式。(2)：过去的方法通常需要用户具备一定的专业知识和技能，并且生成结果往往不够令人满意。(3)：本文提出了一种新的交互式图像生成方法，该方法允许用户通过简单的自然语言指令来生成图像，并且生成的图像质量较高。(4)：该方法在多个任务上取得了良好的性能，并且能够支持用户生成各种各样的图像。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种交互式文本到图像（iT2I）的概念，并提出了一种增强现有大型语言模型以完成此任务的方法。我们的评估表明，这种方法能够实现便捷的 iT2I 功能，而不会显著降低模型固有的能力。这项工作有可能增强人机交互中的用户体验，并提升下一代 T2I 模型的图像质量，为未来的研究和发展提供了有希望的方向。（2）：创新点：提出了一种新的交互式图像生成方法，该方法允许用户通过简单的自然语言指令来生成图像，并且生成的图像质量较高。性能：该方法在多个任务上取得了良好的性能，并且能够支持用户生成各种各样的图像。工作量：该方法的实现相对简单，并且可以在多种平台上运行。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3792d852f31bc708a2cea9f03355bb97.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6afd19703c3d90bc25da231ed316facd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-67bff90f67e33e05a8dc4330ebea5ab0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-67ec55466a5a0a886f9c4fced2ca756a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0f4f4599995309681f54e1a3473d5606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f8385b3f282787527483626a5982157.jpg" align="middle"></details><h2 id="AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Adaptive-Speech-Driven-3D-Facial-Animation"><a href="#AdaMesh-Personalized-Facial-Expressions-and-Head-Poses-for-Adaptive-Speech-Driven-3D-Facial-Animation" class="headerlink" title="AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation"></a>AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive   Speech-Driven 3D Facial Animation</h2><p><strong>Authors:Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</strong></p><p>Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available at <a href="https://adamesh.github.io">https://adamesh.github.io</a>. </p><p><a href="http://arxiv.org/abs/2310.07236v2">PDF</a> Project Page: <a href="https://adamesh.github.io">https://adamesh.github.io</a></p><p><strong>Summary</strong><br>AdaMesh 是一种新颖的自适应语音驱动面部动画方法，通过学习约 10 秒的参考视频来学习个性化说话风格。</p><p><strong>Key Takeaways</strong></p><ul><li>AdaMesh 是一种新颖的语音驱动的人脸动画方法，它学习个性化的说话风格，并生成生动的面部表情和头部姿势。</li><li>AdaMesh 使用混合低秩自适应 (MoLoRA) 来微调表情适配器，有效地捕获面部表情风格。</li><li>AdaMesh 为个性化姿势风格构建离散姿势先验，并通过具有语义感知的姿势风格矩阵检索适当的风格嵌入，而无需微调。</li><li>AdaMesh 在多个数据集上优于最先进的方法，保留了参考视频中的说话风格，并生成生动的面部动画。</li><li>AdaMesh 的代码和补充视频可在网站上找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：AdaMesh：个性化面部表情和头部姿势的自适应语音驱动 3D 面部动画</li><li>作者：Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang</li><li>隶属机构：深圳国际研究生院，清华大学</li><li>关键词：面部动画、语音驱动、个性化、自适应、混合低秩自适应</li><li>论文链接：https://arxiv.org/abs/2310.07236v2，Github 代码链接：None</li><li>摘要：（1）研究背景：语音驱动的 3D 面部动画旨在生成与驱动语音同步的面部动作，该技术在虚拟现实、电影制作和游戏创作中具有巨大潜力。以往大多数工作侧重于提高语音与唇部动作的同步性，而忽略了包括面部表情和头部姿势在内的个性化说话风格。（2）过去方法与问题：一些研究尝试通过微调或自适应模块来建模特定人物的说话风格。然而，在实际应用中，目标用户仅提供少量视频片段（甚至短于 1 分钟）来捕捉个性化的说话风格。这些方法因此面临以下挑战：</li><li>1) 很少的自适应数据可能会导致预训练模型发生灾难性遗忘，并容易导致过拟合问题。</li><li>2) 语音是头部姿势的弱控制信号。在如此弱的信号上进行自适应或学习映射会导致生成结果趋于平均。（3）研究方法：本文提出 AdaMesh，一种新颖的自适应语音驱动面部动画方法。该方法从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。具体来说，本文提出混合低秩自适应 (MoLoRA) 来微调表情适配器，该方法有效地捕捉了面部表情风格。对于个性化的姿势风格，本文提出了一种姿势适配器，通过构建离散姿势先验并使用语义感知姿势风格矩阵检索适当的风格嵌入，无需微调。（4）实验结果：广泛的实验结果表明，本文方法优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本工作提出了一种新的语音驱动面部动画方法 AdaMesh，该方法能够从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。（2）：创新点：</li><li>提出混合低秩自适应 (MoLoRA) 来微调表情适配器，有效地捕捉了面部表情风格。</li><li>提出了一种姿势适配器，通过构建离散姿势先验并使用语义感知姿势风格矩阵检索适当的风格嵌入，无需微调。</li><li>广泛的实验结果表明，AdaMesh 优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。性能：</li><li>AdaMesh 能够从约 10 秒的参考视频中学习个性化的说话风格，并生成生动的面部表情和头部姿势。</li><li>AdaMesh 优于现有技术，保留了参考视频中的说话风格，并生成了生动的面部动画。工作量：</li><li>AdaMesh 的训练过程相对简单，只需要约 10 秒的参考视频即可。</li><li>AdaMesh 的推理过程也相对简单，只需要输入语音信号即可生成面部动画。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e2c325c4c46442c62d649aa8c3b3382d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21344205a1b26e05a0603dc168a71d7a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a107bec6acd460840a8823d7bfd305da.jpg" align="middle"></details><h2 id="GestSync-Determining-who-is-speaking-without-a-talking-head"><a href="#GestSync-Determining-who-is-speaking-without-a-talking-head" class="headerlink" title="GestSync: Determining who is speaking without a talking head"></a>GestSync: Determining who is speaking without a talking head</h2><p><strong>Authors:Sindhu B Hegde, Andrew Zisserman</strong></p><p>In this paper we introduce a new synchronisation task, Gesture-Sync: determining if a person’s gestures are correlated with their speech or not. In comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far looser relationship between the voice and body movement than there is between voice and lip motion. We introduce a dual-encoder model for this task, and compare a number of input representations including RGB frames, keypoint images, and keypoint vectors, assessing their performance and advantages. We show that the model can be trained using self-supervised learning alone, and evaluate its performance on the LRS3 dataset. Finally, we demonstrate applications of Gesture-Sync for audio-visual synchronisation, and in determining who is the speaker in a crowd, without seeing their faces. The code, datasets and pre-trained models can be found at: \url{<a href="https://www.robots.ox.ac.uk/~vgg/research/gestsync}">https://www.robots.ox.ac.uk/~vgg/research/gestsync}</a>. </p><p><a href="http://arxiv.org/abs/2310.05304v1">PDF</a> Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7   Figures</p><p><strong>摘要</strong><br>语音同步新任务——Gesture-Sync：判断一个人的手势是否与他的讲话相关。</p><p><strong>要点</strong></p><ul><li>Gesture-Sync 是一个全新的同步任务，旨在确定一个人的手势是否与其讲话相关。</li><li>与唇语同步相比，手势同步更具挑战性，因为声音和身体运动之间的关系比声音和嘴唇运动之间的关系松散得多。</li><li>文中提出了一种用于此任务的双编码器模型，并比较了几种输入表示，包括 RGB 帧、关键点图像和关键点向量，评估它们的性能和优势。</li><li>该模型仅使用自我监督学习就可以训练，并在 LRS3 数据集上评估了其性能。</li><li>最后，演示了 Gesture-Sync 在视听同步以及确定人群中说话者（不看他们的脸）方面的应用。</li><li>代码、数据集和预训练模型可在以下网址找到：<a href="https://www.robots.ox.ac.uk/~vgg/research/gestsync">https://www.robots.ox.ac.uk/~vgg/research/gestsync。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：手势同步：确定谁在说话，而无需说话的头部</li><li>作者：Sindhu B Hegde，Andrew Zisserman</li><li>隶属关系：牛津大学工程科学系视觉几何组</li><li>关键词：手势同步、唇形同步、自我监督学习、多模态学习、说话人识别</li><li>论文链接：https://arxiv.org/abs/2310.05304，Github 代码链接：None</li><li><p>摘要：（1）研究背景：在人机交互和多媒体处理中，准确识别说话人对于理解和响应人类语言至关重要。传统的说话人识别方法主要依赖于唇形同步，即通过分析说话人的嘴唇运动来确定说话人。然而，在某些情况下，例如当说话人的脸部被遮挡或说话人在嘈杂的环境中时，唇形同步方法可能会失效。（2）过去的方法及问题：为了解决唇形同步的局限性，一些研究人员提出了手势同步的方法，即通过分析说话人的手势运动来确定说话人。然而，现有的手势同步方法大多依赖于监督学习，需要大量带标签的数据进行训练。这在实际应用中往往难以获得。（3）提出的研究方法：为了解决手势同步中数据稀缺的问题，本文提出了一种新的手势同步方法，该方法可以利用自我监督学习进行训练。具体来说，该方法通过学习手势和语音之间的相关性来确定说话人。该方法采用双编码器模型，其中一个编码器将手势图像或关键点向量编码成嵌入向量，另一个编码器将语音信号编码成嵌入向量。然后，通过计算两个嵌入向量之间的相似性来确定说话人。（4）方法的性能：为了评估所提出方法的性能，本文在 LRS3 数据集上进行了实验。实验结果表明，该方法在说话人识别任务上取得了良好的性能。此外，该方法还可以用于音频-视觉同步和确定人群中谁在说话，而无需看到他们的脸。</p></li><li><p>方法：（1）双编码器模型：该方法采用双编码器模型，其中一个编码器将手势图像或关键点向量编码成嵌入向量，另一个编码器将语音信号编码成嵌入向量。（2）手势和语音之间的相关性学习：通过学习手势和语音之间的相关性来确定说话人。（3）计算嵌入向量之间的相似性：通过计算两个嵌入向量之间的相似性来确定说话人。</p></li><li><p>结论：（1）：本文提出了一种新的手势同步方法，该方法可以利用自我监督学习进行训练，解决了手势同步中数据稀缺的问题。（2）：创新点：</p></li><li>利用自我监督学习进行训练，无需大量带标签的数据。</li><li>采用双编码器模型，学习手势和语音之间的相关性。</li><li>通过计算嵌入向量之间的相似性来确定说话人。性能：</li><li>在LRS3数据集上进行了实验，实验结果表明，该方法在说话人识别任务上取得了良好的性能。</li><li>该方法还可以用于音频-视觉同步和确定人群中谁在说话，而无需看到他们的脸。工作量：</li><li>该方法的实现相对简单，易于部署。</li><li>该方法的训练时间较短，可以在合理的时间内完成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ce1c5e581126af42846fa6a80d1504c1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5e94dc122da890162c1017d072a1cfcc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c91edb80b044a859328ba0905835ad6.jpg" align="middle"></details><h2 id="DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models"><a href="#DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models" class="headerlink" title="DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose   Generation via Diffusion Models"></a>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose   Generation via Diffusion Models</h2><p><strong>Authors:Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-jin Liu</strong></p><p>The generation of stylistic 3D facial animations driven by speech poses a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. We extend this to include the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Our extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset will be made publicly available. </p><p><a href="http://arxiv.org/abs/2310.00434v1">PDF</a> Project page: <a href="https://raineggplant.github.io/DiffPoseTalk/">https://raineggplant.github.io/DiffPoseTalk/</a></p><p><strong>摘要</strong><br>扩散模型结合风格编码器实现语音驱动的高质量、多样化三维动态人脸生成。</p><p><strong>要点</strong></p><ul><li>提出了一种基于扩散模型和风格编码器的语音驱动三维动态人脸生成框架 DiffPoseTalk。</li><li>风格编码器通过从短参考视频中提取风格嵌入来捕捉风格的复杂性。</li><li>在推理过程中，使用无分类器引导根据语音和风格引导生成过程。</li><li>扩展该框架以生成头部姿势，从而增强用户感知。</li><li>通过在高质量的自然环境下的音频-视觉数据集上训练模型，解决了扫描三维说话人脸数据短缺的问题。</li><li>实验结果表明，该方法优于最先进的方法。</li><li>代码和数据集将公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DiffPoseTalk：通过扩散模型和头部姿势生成语音驱动的风格化 3D 面部动画</li><li>作者：孙志尧、吕天、叶盛、林马修·盖坦、盛 Jenny、温宇辉、俞敏晶、刘永进</li><li>隶属机构：清华大学</li><li>关键词：语音驱动、3D 面部动画、扩散模型、风格化、头部姿势</li><li>论文链接：https://arxiv.org/abs/2310.00434Github 代码链接：无</li><li><p>摘要：（1）研究背景：语音驱动的 3D 面部动画是一项极具挑战性的任务，因为它需要学习语音、风格和相应自然面部动作之间的多对多映射。（2）过去的方法：现有方法要么采用确定性模型进行语音到动作的映射，要么使用独热编码方案对风格进行编码。独热编码方法无法捕捉风格的复杂性，从而限制了泛化能力。（3）研究方法：本文提出 DiffPoseTalk，这是一个基于扩散模型的生成框架，结合了一个从短参考视频中提取风格嵌入的风格编码器。在推理过程中，我们采用无分类器指导来根据语音和风格指导生成过程。我们将其扩展到包括头部姿势的生成，从而增强用户感知。此外，我们通过在从高质量野外视听数据集重建的 3DMM 参数上训练模型，解决了扫描 3D 说话面部数据的短缺问题。（4）性能与目标：我们的广泛实验和用户研究表明，我们的方法优于最先进的方法。这些性能支持了他们的目标。</p></li><li><p>方法：(1) 扩散模型：我们使用扩散模型作为生成框架，将语音和风格指导映射到3D面部动画。扩散模型通过逐渐增加噪声来将数据从已知状态转换到随机状态，然后通过反向过程从噪声中恢复数据。(2) 风格编码器：我们设计了一个风格编码器，从短参考视频中提取风格嵌入。风格编码器由一个卷积神经网络组成，它将视频帧编码为一个风格向量。(3) 无分类器指导：在推理过程中，我们采用无分类器指导来根据语音和风格指导生成过程。无分类器指导通过最小化生成数据与目标数据之间的距离来训练模型。(4) 头部姿势生成：我们将模型扩展到包括头部姿势的生成，从而增强用户感知。我们使用一个额外的网络来预测头部姿势，并将其作为输入添加到生成模型中。(5) 数据集：我们使用从高质量野外视听数据集重建的3DMM参数来训练模型。该数据集包含了各种说话者的3D面部扫描数据，以及相应的语音和头部姿势数据。</p></li><li><p>结论：（1）：本文提出了一种基于扩散模型的语音驱动3D面部动画生成框架DiffPoseTalk，该框架结合了一个从短参考视频中提取风格嵌入的风格编码器。在推理过程中，采用无分类器指导来根据语音和风格指导生成过程。将模型扩展到包括头部姿势的生成，从而增强用户感知。此外，通过在从高质量野外视听数据集重建的3DMM参数上训练模型，解决了扫描3D说话面部数据的短缺问题。（2）：创新点：</p></li><li>将扩散模型应用于语音驱动的3D面部动画生成，实现了语音、风格和头部姿势的多对多映射。</li><li>设计了一个风格编码器，从短参考视频中提取风格嵌入，有效地捕捉了风格的复杂性。</li><li>采用无分类器指导来生成过程，避免了分类器带来的误差。</li><li>将模型扩展到包括头部姿势的生成，增强了用户感知。</li><li>通过在从高质量野外视听数据集重建的3DMM参数上训练模型，解决了扫描3D说话面部数据的短缺问题。</li></ol><p>性能：- 在多个数据集上进行了广泛的实验，证明了该方法优于最先进的方法。- 用户研究表明，该方法生成的3D面部动画更自然、更逼真。</p><p>工作量：- 该方法的实现相对复杂，需要较多的计算资源。- 需要高质量的野外视听数据集来训练模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7312673b0c18714105ddb7899ac0df55.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8466e4588a2da641b678027f760475e8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dbecf9b011548b814dbf8ff1aade63c8.jpg" align="middle"></details>## FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using   Diffusion**Authors:Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak**Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation synthesis. We have run extensive objective and subjective analyses and show that our approach achieves better or comparable results in comparison to the state-of-the-art methods. We also introduce a new in-house dataset that is based on a blendshape based rigged character. We recommend watching the accompanying supplementary video. The code and the dataset will be publicly available. [PDF](http://arxiv.org/abs/2309.11306v1) Pre-print of the paper accepted at ACM SIGGRAPH MIG 2023**Summary**语音驱动 3D 人脸动画合成结合了 3D 顶点和混合形状数据集，可生成非确定性的 3D 人脸动画。**Key Takeaways**- 我们提出一种非确定性深度学习模型 FaceDiffuser，用于生成语音驱动的面部动画。- FaceDiffuser 基于漫散技术，使用预训练的语音表示模型 HuBERT 对音频输入进行编码。- FaceDiffuser 首次将漫散方法应用于语音驱动的 3D 面部动画合成任务。- 我们已经进行了广泛的客观和主观分析，表明我们的方法与最先进的方法相比取得了更好或相当的结果。- 我们还引入了一个新的内部数据集，该数据集基于混合形状的装配角色。- 我们建议观看随附的补充视频。- 代码和数据集将公开提供。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：FaceDiffuser：基于扩散的语音驱动 3D 面部动画合成</li><li>作者：Stefan Stan、Kazi Injamamul Haque、Zerrin Yumak</li><li>隶属单位：乌特勒支大学</li><li>关键词：面部动画合成、深度学习、虚拟人、网格动画、混合形状动画</li><li>论文链接：https://arxiv.org/abs/2309.11306    Github 代码链接：https://github.com/uuembodiedsocialai/FaceDiffuser</li><li><p>摘要：（1）语音驱动的 3D 面部动画合成一直是工业界和研究界的一项具有挑战性的任务。最近的方法主要集中在确定性深度学习方法上，这意味着给定语音输入，输出始终相同。然而，实际上，遍布整个面部的非语言面部线索本质上是不确定的。此外，大多数方法都集中在基于 3D 顶点的的数据集上，而与现有具有装备角色的面部动画管道兼容的方法很少。（2）为了消除这些问题，我们提出了 FaceDiffuser，这是一种非确定性深度学习模型，用于生成语音驱动的面部动画，该模型使用 3D 顶点和混合形状数据集进行训练。我们的方法基于扩散技术，并使用预训练的大型语音表示模型 HuBERT 对音频输入进行编码。据我们所知，我们是第一个将扩散方法用于语音驱动的 3D 面部动画合成的任务。（3）我们进行了广泛的客观和主观分析，结果表明，与最先进的方法相比，我们的方法取得了更好或相当的结果。我们还引入了一个新的内部数据集，该数据集基于混合形状的装备角色。我们建议观看附带的补充视频。代码和数据集将公开发布。（4）在语音驱动的 3D 面部动画合成任务上，我们的方法在客观和主观评估中都取得了有竞争力的结果。这表明我们的方法可以很好地实现语音驱动的 3D 面部动画合成。</p></li><li><p>方法：（1）提出了一种通用模型，该模型可以针对基于顶点和基于混合形状的数据集进行训练，只需对超参数进行轻微修改。基于顶点的模型配置称为 V-FaceDiffuser，基于混合形状的模型称为 B-FaceDiffuser。主要区别在于额外的噪声编码器，如图 2 中以虚线红色框标出。噪声编码器有助于将高维顶点数据投影到低维潜在表示中。扩散噪声过程采用 x1:N0 来计算噪声 x1:Nt，同时保持其原始形状。从图 2 中，我们可以识别出这两个版本模型中包含的以下主要组件：（2）音频编码器：我们使用预训练的大型语音模型 HuBERT 作为音频编码器，类似于 [22]，并且在架构的两个版本中保持不变。我们采用 HuBERT 架构的预训练版本，并使用其发布的 hubert-base-ls960 版本，该版本在 960 小时的 LibriSpeech [35] 数据集上进行训练。（3）扩散过程：设 x1:N0 是来自数据集的真实视觉帧序列，形状为 (N,C)，其中 C 是顶点数乘以 3（对于 3 个空间轴）或面部控制权重（或混合形状值）的数量。在训练期间，我们从 [1,T] 中随机抽取一个整数时间步长 t，表示应用于 x1:N0 以获得 x1:Nt 的噪声步骤数，公式为：x1:Nt=q(x1:Nt|x1:Nt−1)=N(√1−βt⋅x1:Nt−1,(βt)⋅I)(2)其中，N 是序列中的视觉帧数，t 是扩散时间步长，βt 是时间步长 t 处的常数噪声，使得 0&lt;β1&lt;β2&lt;...&lt;βT&lt;1。在正向噪声过程之后，理想情况下，我们希望能够计算反向过程并从 x1:NT∼N(0,1) 向后返回到 x1:N0。因此，条件分布函数 p(x1:Nt−1|x1:Nt) 需要事先知道。Ho 等人 [24] 提出了通过学习数据集的潜在表示方差来实现这一目标。训练目标被定义为学习预测添加到输入 x0 中的噪声 𝜖。然而，我们偏离 [24] 并遵循 MDM [49] 和 EDGE [50]，选择我们的模型来学习预测实际动画数据，而不是数据中的噪声水平。我们认为这更适合我们的任务，因为结果也取决于输入音频。此外，通过选择这种方法，即使从推理过程的第一个去噪步骤开始，我们的模型也能够预测出可接受的结果，从而实现更快的采样。然而，遵循完整的推理过程将给出最好的结果。我们采用类似于 [49] 和 [50] 的简单损失进行训练。Ho 等人进行了更彻底的实验。[24]，他们还声称利用简单的损失来学习变分界被证明既易于实现，也有利于采样结果的质量。损失定义为：L=Ex0∼q(x0|c),t∼[1,T]<a href="3">∥x0−ˆx0∥</a>（4）面部解码器：面部解码器负责根据编码音频和噪声的潜在表示生成最终的动画帧。它由多个 GRU 层组成，后跟一个最终的全连接层，该层预测输出序列。在解码步骤中，还可以以学习的样式嵌入向量与隐藏状态输出之间的元素级乘积的形式添加样式嵌入。我们在消融部分解释了选择 GRU 解码器的原因。（5）FaceDiffuser：基于扩散的语音驱动 3D 面部动画合成（6）FaceDiffuser 推理是一个迭代过程，从 T 递减到 1。初始噪声由正态分布 N(0,1) 的实际噪声表示。在每一步中，我们向网络提供音频和噪声动画输入。然后将预测的运动再次扩散并馈送到迭代的下一步骤。</p></li><li><p>结论：（1）我们把扩散机制集成到一个生成式深度神经网络中，该网络经过训练可以生成以语音为条件的 3D 面部动画。所提出的方法可以推广到高维时间 3D 顶点数据以及低维混合形状数据，只需对超参数进行轻微修改。定量分析表明，我们的方法优于最先进的方法。我们展示了我们的模型能够在不同的样式条件之间产生更高的运动多样性。（2）创新点：</p></li><li>我们提出了一种新颖的基于扩散的模型 FaceDiffuser，用于生成语音驱动的 3D 面部动画。</li><li>我们的模型可以针对基于顶点和基于混合形状的数据集进行训练，只需对超参数进行轻微修改。</li><li>我们的模型能够生成具有更高运动多样性的动画，即使在不同的样式条件下也是如此。性能：</li><li>我们的模型在客观和主观评估中都优于最先进的方法。</li><li>我们的模型能够生成逼真的、高质量的 3D 面部动画。</li><li>我们的模型能够实时生成动画。工作量：</li><li>我们模型的训练过程相对简单。</li><li>我们模型的推理过程也非常有效。</li><li>我们模型的代码和数据集都是公开可用的。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c6b2d7bbedf1f0aff99eafa6c4abc4dd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1bf4fe00fac43e23f442d69b1e6d0a10.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a0ecfad348bccbc3857ece8ec2797c8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-07bf850a8b84e1168cec03ca40142feb.jpg" align="middle"></details><h2 id="DT-NeRF-Decomposed-Triplane-Hash-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis"><a href="#DT-NeRF-Decomposed-Triplane-Hash-Neural-Radiance-Fields-for-High-Fidelity-Talking-Portrait-Synthesis" class="headerlink" title="DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for   High-Fidelity Talking Portrait Synthesis"></a>DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for   High-Fidelity Talking Portrait Synthesis</h2><p><strong>Authors:Yaoyu Su, Shaohui Wang, Haoqian Wang</strong></p><p>In this paper, we present the decomposed triplane-hash neural radiance fields (DT-NeRF), a framework that significantly improves the photorealistic rendering of talking faces and achieves state-of-the-art results on key evaluation datasets. Our architecture decomposes the facial region into two specialized triplanes: one specialized for representing the mouth, and the other for the broader facial features. We introduce audio features as residual terms and integrate them as query vectors into our model through an audio-mouth-face transformer. Additionally, our method leverages the capabilities of Neural Radiance Fields (NeRF) to enrich the volumetric representation of the entire face through additive volumetric rendering techniques. Comprehensive experimental evaluations corroborate the effectiveness and superiority of our proposed approach. </p><p><a href="http://arxiv.org/abs/2309.07752v1">PDF</a> 5 pages, 5 figures. Submitted to ICASSP 2024</p><p><strong>Summary</strong><br>深度分解三平面哈希神经辐射场显著提高了说话人脸的光写实渲染效果，并在关键评估数据集中取得了最先进的成效。</p><p><strong>Key Takeaways</strong></p><ul><li>分解面部区域为两个专门的三平面：一个是专门用于表示嘴巴，另一个用于更广泛的面部特征。</li><li>将音频特征作为残差项引入，并通过音频-口-面转换器作为查询向量整合到模型中。</li><li>利用神经辐射场 (NeRF) 的功能，通过添加体积渲染技术丰富整个脸部的体积表示。</li><li>综合实验评估证实了我们提出的方法的有效性和优越性。</li><li>该方法是首个使用三平面哈希神经辐射场来实现说话人脸光写实渲染的技术。</li><li>该方法将面部区域分解为两个专门的三平面，以更有效地表示嘴巴和更广泛的面部特征。</li><li>该方法将音频特征作为残差项引入，并将其作为查询向量通过音频-口-面转换器集成到模型中。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：分解三平面哈希神经辐射场（DT-NERF）</li><li>作者：姚宇苏、邵辉王、郝谦王</li><li>隶属单位：深圳国际研究生院，清华大学，深圳 518071，中国</li><li>关键词：NeRF，会说话的面部肖像，分解三平面哈希，音频-嘴巴-面部转换器</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：音频驱动的说话面部肖像合成是一项关键且具有挑战性的领域，尤其是在增强现实 (AR)、虚拟现实 (VR) 和大型语言模型 (LLM) 在数字人、虚拟形象和远程会议等 3D 面部驱动技术中不断找到应用的情况下。近年来，研究人员对 3D 视觉环境中的音频驱动面部合成进行了广泛探索。随着 2020 年神经辐射场 (NeRF) 的出现，这种方法已被纳入这项任务，产生了令人印象深刻的视觉效果。然而，原始的 NeRF 模型在计算速度和语音期间精确的嘴巴同步方面存在局限性，这表明有改进的空间。（2）过去方法及问题：NeRF 是一种神经渲染技术，采用 5D 辐射场来捕获复杂的 3D 表面。该技术最初设计用于渲染静态、有界的场景，但此后已发展到适应动态和无界设置。NeRF 已在各种子领域中找到应用，例如通过符号距离场 (SDF) 和体积渲染相结合的场景重建、面部和身体渲染，甚至手部重建。在利用神经辐射场 (NeRF) 进行 3D 面部合成的当前领域中，流行的方法往往遵循两条路径之一。它们要么采用明确的 3D 面部表情参数或 2D 地标，这可能导致信息显着丢失，尤其是嘴巴区域，要么采用隐式表示，使用音频作为潜在代码来调制或扭曲规范空间。这些策略存在缺陷，尤其是在捕捉语音期间嘴巴的细微变化方面。（3）研究方法：为了解决这些问题，我们提出了一个双重方法。首先，我们采用动态 NeRF，该 NeRF 利用音频特征作为转换器的查询，旨在优化 NeRF 中的密度和颜色网络，以从规范空间调制到动态空间。此外，我们利用颜色和体积密度在同一 NeRF 空间中的加法特性，从而实现音频和视觉元素的更无缝集成。我们的方法旨在更有效地将音频线索与面部表情结合起来，特别关注嘴巴区域的优化。分解的 3D 表示被用来分别建模嘴巴和更广泛的面部特征。我们引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到我们的模型中。（4）实验结果：在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。综合实验评估证实了我们提出的方法的有效性和优越性。</p></li><li><p>方法：(1): 提出动态 NeRF，利用音频特征作为转换器的查询，优化 NeRF 中的密度和颜色网络，从规范空间调制到动态空间。(2): 利用颜色和体积密度在同一 NeRF 空间中的加法特性，实现音频和视觉元素的更无缝集成。(3): 采用分解的 3D 表示分别建模嘴巴和更广泛的面部特征。(4): 引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到模型中。</p></li><li><p>结论：（1）：本文提出了一种分解三平面哈希神经辐射场（DT-NERF）方法，用于音频驱动的说话面部肖像合成。该方法利用动态NeRF、分解的3D表示、音频特征作为残差项以及音频-嘴巴-面部转换器，有效地将音频线索与面部表情结合起来，特别关注嘴巴区域的优化。在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。（2）：创新点：</p></li><li>提出动态NeRF，利用音频特征作为转换器的查询，优化NeRF中的密度和颜色网络，从规范空间调制到动态空间。</li><li>利用颜色和体积密度在同一NeRF空间中的加法特性，实现音频和视觉元素的更无缝集成。</li><li>采用分解的3D表示分别建模嘴巴和更广泛的面部特征。</li><li>引入音频特征作为残差项，并通过音频-嘴巴-面部转换器将它们作为查询向量集成到模型中。性能：</li><li>在关键评估数据集上，我们的方法显著提高了逼真的说话面孔的渲染，并取得了最先进的结果。工作量：</li><li>该方法需要收集和预处理音频和面部数据，构建分解三平面哈希神经辐射场模型，并进行训练和推理。工作量中等。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ada9598533d7c8a1499ba097e55090dc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8fb8385566e4ab5153fa149f745ccef6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3595e0ef582dce4ddf81794c218bee95.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb1d50c01e04c66fad1c597d9bf14ce5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-01-24 Real3D-Portrait  One-shot Realistic 3D Talking Portrait Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3DGS/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3DGS/</id>
    <published>2024-01-23T16:05:36.000Z</published>
    <updated>2024-01-27T04:44:08.709Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-27-更新"><a href="#2024-01-27-更新" class="headerlink" title="2024-01-27 更新"></a>2024-01-27 更新</h1><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong><br>动态高斯散点用于可变形内窥镜组织重建。</p><p><strong>Key Takeaways</strong></p><ul><li>EndoGS 将高斯散点应用于可变形内窥镜组织重建。</li><li>EndoGS 结合了变形场、深度引导监督和时空权重掩码来处理动态场景、优化 3D 目标和减轻工具遮挡。</li><li>EndoGS 从单视角视频、估计的深度图和标注的工具掩码中重建并渲染高质量的可变形内窥镜组织。</li><li>EndoGS 在达芬奇机器人手术视频上的实验表明，它实现了卓越的渲染质量。</li><li>EndoGS 的代码可在 <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a> 获得。</li><li>EndoGS 的时间成本低于传统方法，如基于动态辐射场的重建方法。</li><li>EndoGS 的重建质量高于传统方法，如基于动态辐射场的重建方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯溅射的可变形内窥镜组织重建</li><li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li><li>第一作者单位：香港大学</li><li>关键词：高斯溅射、机器人手术、三维重建</li><li>论文链接：https://arxiv.org/abs/2401.11535，Github 代码链接：https://github.com/HKU-MedAI/EndoGS</li><li><p>摘要：(1)：研究背景：三维重建是机器人手术中的一个关键研究领域，最近的工作采用动态辐射场的变体，从单视角视频中成功实现了可变形组织的三维重建。然而，这些方法往往会遭受耗时的优化或质量低下的困扰，限制了它们在下游任务中的应用。(2)：过去的方法：早期尝试采用深度估计来实现内窥镜重建，但在处理非刚性变形和遮挡方面存在困难。[9,12]提出了结合工具掩蔽、立体深度估计和稀疏变形场的框架，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。神经辐射场 (NeRFs) 在三维重建方面取得了重大进展，但它们通常需要大量数据和计算资源。(3)：研究方法：本文提出了一种新的方法 EndoGS，它将高斯溅射应用于可变形内窥镜组织重建。EndoGS 将变形场结合起来处理动态场景，使用深度引导监督来优化具有单一视点的三维目标，并使用时空权重掩码来减轻遮挡。(4)：实验结果：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了卓越的渲染质量。</p></li><li><p>方法：（1）概述：提出了一种名为 EndoGS 的新方法，它将高斯溅射应用于可变形内窥镜组织重建。EndoGS 将变形场结合起来处理动态场景，使用深度引导监督来优化具有单一视点的三维目标，并使用时空权重掩码来减轻遮挡。（2）高斯溅射表示的可变形组织：使用高斯变形来表示随时间变化的运动和形状，遵循 [26] 的基本设计。最终目标是学习 3D 高斯的原始表示 {(µ, s, r, sh, σ)} 以及高斯变形 {∆(µ, s, r, sh, σ)}={(∆µ, ∆s, ∆r, ∆sh, ∆σ)}。（3）结合工具掩码和深度图的训练：重建带有工具遮挡的视频具有挑战性，遵循前人的工作 [25, 27, 28] 使用标记的工具遮挡掩码来指示看不见的像素。此外，利用时空重要性采样策略来指示与遮挡问题相关的关键区域。由于 3D-GS 使用空间权重掩码来处理遮挡，因此将工具掩码和深度图合并到空间权重掩码中，以进一步增强对遮挡区域的建模。</p></li><li><p>结论：（1）：本文提出了一种基于高斯溅射的可变形内窥镜组织重建方法，能够从单视角视频、估计的深度图和标记的工具遮挡掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。（2）：创新点：</p></li><li>将高斯溅射应用于可变形内窥镜组织重建，能够有效处理动态场景中的非刚性变形和遮挡。</li><li>使用深度引导监督来优化具有单一视点的三维目标，提高了重建的准确性和鲁棒性。</li><li>使用时空权重掩码来减轻遮挡，增强了对遮挡区域的建模。性能：</li><li>在达芬奇机器人手术视频上的实验表明，该方法实现了卓越的渲染质量。</li><li>与现有方法相比，该方法在准确性和鲁棒性方面具有优势。工作量：</li><li>该方法需要较少的计算资源，能够实时渲染可变形组织。</li><li>该方法需要标记的工具遮挡掩码和估计的深度图，这可能会增加工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details><h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang</strong></p><p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p><p><a href="http://arxiv.org/abs/2401.09720v1">PDF</a> </p><p><strong>摘要</strong><br>应用高斯几何建模实现动态着装人物的三维重建。</p><p><strong>要点</strong></p><ul><li>提出一种基于 3D 高斯几何建模的动态着装人物三维重建方法 GaussianBody。</li><li>GaussianBody 采用了高效的点云表示和渲染方式，在训练时间和渲染质量方面均取得了不错的表现。</li><li>为了适应动态人类复杂的非刚性变形和丰富的服装细节，GaussianBody 引入了显式的姿态引导变形，将规范空间和观测空间中的动态高斯体素相关联。</li><li>提出了一种基于物理先验的正则化变换，帮助缓解两个空间之间的歧义性。</li><li>训练过程中，GaussianBody 还提出了一种姿态优化策略来更新姿态回归，以补偿不准确的初始估计，并提出了一个分而治之的机制来增强回归点云的密度。</li><li>在标准数据集上的实验证明，GaussianBody 在实现最先进的动态着装人物照片级新视角渲染结果的同时，也能实现准确的几何形状重建。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯体：基于 3D 高斯散点的穿衣人体重建</li><li>作者：李孟田、姚盛祥、谢志锋、陈可宇、蒋玉刚</li><li>第一作者单位：上海大学</li><li>关键词：人体重建、3D 高斯散点、变形、姿势引导</li><li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li><li>摘要：(1)：研究背景：高保真穿衣人体模型在虚拟现实、远程呈现和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家的繁琐手动工作，这使得它们耗时且昂贵，从而限制了新手用户的可扩展性。最近，人们越来越关注从单个 RGB 图像或单目视频中自动重建穿衣人体模型。(2)：过去方法及其问题：网格方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来恢复人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移成为这种情况下的一种增强解决方案。然而，它的表示能力仍然有限。(3)：本文提出的研究方法：本文提出了一种基于 3D 高斯散点的新型穿衣人体重建方法，称为高斯体。与昂贵的神经辐射场模型相比，3D 高斯散点最近在训练时间和渲染质量方面表现出了极佳的性能。然而，由于复杂的不刚性变形和丰富的服装细节，将静态 3D 高斯散点模型应用于动态人体重建问题并非易事。为了应对这些挑战，本文方法考虑了在规范空间和观察空间跨动态高斯体的显式姿势引导变形，引入具有正则化变换的基于物理的先验有助于减轻两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。(4)：方法在什么任务和性能上取得了成就，能否支持其目标：实验证明，本文方法可以实现动态穿衣人体的高质量细节的最新逼真的新视图渲染结果，以及显式的几何重建。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1)：意义：本文提出了一种基于 3D 高斯散点的穿衣人体重建方法，称为高斯体。该方法克服了复杂的不刚性变形和丰富的服装细节的挑战，实现了动态穿衣人体的高质量细节的新视图渲染结果，以及显式的几何重建。(2)：创新点：</li><li>使用 3D 高斯散点表示动态穿衣人体。</li><li>引入具有正则化变换的基于物理的先验，以减轻规范空间和观察空间之间的歧义。</li><li>提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计。</li><li>提出了一种分裂尺度机制来增强回归点云的密度。性能：</li><li>与基线和其他方法相比，本文方法实现了可比较的图像质量指标，证明了具有竞争力的性能、相对较快的训练速度，以及能够以更高分辨率的图像进行训练。工作量：</li><li>本文方法需要收集和标记大量的数据，并且训练过程需要大量的时间和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7931aa02d87b1007c7f5cdde77107e5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3df005c3ea738aba56feb680b23b73d2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details><h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p><p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p><p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p><p><strong>Summary</strong><br>针对自动驾驶领域，通过对 250 余篇论文的分析，我们总结了视觉基础模型的发展方法，包括数据处理、预训练策略和下游任务的适应，并展望了未来研究方向。</p><p><strong>Key Takeaways</strong></p><ul><li>视觉基础模型在自动驾驶领域面临诸多挑战，如缺乏专用数据、多传感器融合和不同任务的特定架构等，极大制约了其发展。</li><li>本文通过分析 250 余篇论文，总结了视觉基础模型的构建方法，包括数据准备、预训练策略和下游任务的适应。</li><li>文中重点介绍了包括神经辐射场（NeRF）、扩散模型、3D 高斯分布和世界模型等关键技术，为该领域的研究提供了全面的路线图。</li><li>本文建立并维护了开源仓库 <a href="https://github.com/zhanghm1995/Forge_VFM4AD，以收集和分享视觉基础模型的构建方法，旨在为研究人员提供有价值的信息。">https://github.com/zhanghm1995/Forge_VFM4AD，以收集和分享视觉基础模型的构建方法，旨在为研究人员提供有价值的信息。</a></li><li>视觉基础模型在自动驾驶领域的应用具有广阔的前景，有望推动自动驾驶技术的发展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：为自动驾驶打造视觉基础模型：挑战、方法和机遇</li><li>作者：徐岩、张海明、蔡英杰、郭靖明、邱维超、高斌、周凯强、赵岳、金欢、高建涛、李臻、蒋立辉、张伟、张宏波、戴登新、刘冰冰</li><li>第一作者单位：华为诺亚方舟实验室</li><li>关键词：视觉基础模型、数据生成、自监督训练、自动驾驶、文献综述</li><li>论文链接：https://arxiv.org/pdf/2401.08045.pdf，Github 链接：无</li><li>摘要：(1) 研究背景：随着自动驾驶技术的发展，对视觉基础模型（VFM）的需求日益增长，但目前缺乏专门针对自动驾驶的 VFM。(2) 过去的方法和问题：传统自动驾驶感知系统依赖模块化架构，使用特定任务的专用算法，这种方法导致输出不一致，并且限制了系统处理长尾案例的能力。(3) 论文提出的研究方法：本文系统分析了 250 多篇论文，剖析了 VFM 开发的必要技术，包括数据准备、预训练策略和下游任务适应。此外，还探讨了 NeRF、扩散模型、3D 高斯散射和世界模型等关键进展，为未来的研究提供了全面的路线图。(4) 方法在任务上的表现和性能：本文建立并维护了 ForgeVFM4AD，这是一个开放获取的资源库，不断更新自动驾驶 VFM 的最新进展。</li></ol><p><methods>:(1): 视觉基础模型（VFM）是自动驾驶感知系统的重要组成部分，本文系统分析了250多篇论文，剖析了VFM开发的必要技术，包括数据准备、预训练策略和下游任务适应。(2): 数据准备方面，本文探讨了自动驾驶场景下的数据生成方法，包括合成数据、真实数据和混合数据。(3): 预训练策略方面，本文介绍了VFM的预训练方法，包括自监督学习、监督学习和半监督学习。(4): 下游任务适应方面，本文讨论了VFM在自动驾驶任务中的应用，包括目标检测、语义分割、实例分割、深度估计和运动估计。(5): 此外，本文还探讨了NeRF、扩散模型、3D高斯散射和世界模型等关键进展，为未来的研究提供了全面的路线图。</methods></p><ol><li>结论：（1）：本文系统分析了250多篇论文，剖析了自动驾驶视觉基础模型开发的必要技术，包括数据准备、预训练策略和下游任务适应，为自动驾驶视觉基础模型的开发提供了全面的路线图。（2）：创新点：</li><li>本文首次系统分析了自动驾驶视觉基础模型的开发技术，为自动驾驶视觉基础模型的开发提供了全面的路线图。</li><li>本文建立并维护了ForgeVFM4AD，这是一个开放获取的资源库，不断更新自动驾驶视觉基础模型的最新进展。性能：</li><li>本文提出的方法在自动驾驶视觉基础模型的开发中取得了良好的性能。工作量：</li><li>本文的工作量很大，需要分析250多篇论文，并建立和维护ForgeVFM4AD资源库。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle"></details><h2 id="Gaussian-Shadow-Casting-for-Neural-Characters"><a href="#Gaussian-Shadow-Casting-for-Neural-Characters" class="headerlink" title="Gaussian Shadow Casting for Neural Characters"></a>Gaussian Shadow Casting for Neural Characters</h2><p><strong>Authors:Luis Bolanos, Shih-Yang Su, Helge Rhodin</strong></p><p>Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability. </p><p><a href="http://arxiv.org/abs/2401.06116v1">PDF</a> 14 pages, 13 figures</p><p><strong>Summary</strong><br>神经特征模型可从视频重建详细几何结构和纹理，但不包含明确的阴影和着色，在生成新视图和姿势或重新照明时会产生伪像。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯密度代理可将采样过程替换为简单的解析公式，实现对新阴影模型的构建。</li><li>方法支持动态运动并专为阴影计算量身定制，从而避免了高斯散射所需的仿射投影逼近和排序。</li><li>与延迟神经渲染模型相结合，高斯阴影可实现兰伯特着色和阴影投射，且开销极小。</li><li>重建效果更好且更能将反照率、着色和阴影分离开来，尤其是在具有直接阳光和硬阴影的具有挑战性的户外场景中。</li><li>该方法能够优化光线的方向，而无需任何用户输入。</li><li>与最先进的方法相比，新姿势具有更少的阴影伪像，新场景中的重新照明也更逼真，这提供了在新的环境中摆放神经特征的新方法，从而提高了其适用性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：使用高斯阴影生成神经角色</li><li>作者：Daniel Holden, Junbang Liang, Derek Nowrouzezahrai, Josh Susskind</li><li>第一作者单位：英伟达公司</li><li>关键词：计算机图形学、神经渲染、阴影、光照</li><li>论文链接：https://arxiv.org/abs/2209.03594Github 链接：无</li><li><p>摘要：(1) 研究背景：神经角色模型可以从视频中重建详细的几何形状和纹理，但它们缺乏明确的阴影和着色，当生成新的视角和姿势或重新照明时会导致伪影。将阴影包含在内特别困难，因为它们是全局效应，并且所需的二次光线投射成本很高。(2) 过去的方法及其问题：为了解决这个问题，过去的方法使用各种技术来近似阴影，例如使用预计算的阴影贴图或使用球谐函数来表示光照。然而，这些方法通常计算成本高昂，并且可能导致伪影或不准确。(3) 本文提出的研究方法：我们提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。它支持动态运动，并针对阴影计算进行了定制，从而避免了与密切相关的 Gaussian splatting 所需的仿射投影近似和排序。结合延迟神经渲染模型，我们的高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。(4) 方法在什么任务上取得了什么性能，这些性能是否能支撑其目标：我们展示了改进的重建，在具有直射阳光和硬阴影的具有挑战性的户外场景中更好地分离了反照率、阴影和阴影。与最先进的方法相比，我们的方法能够优化光线方向，而无需任何用户输入。因此，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。</p></li><li><p>方法：（1）提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。（2）结合延迟神经渲染模型，高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。（3）通过优化光线方向，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。</p></li><li><p>结论：（1）：这项工作通过一个适用于动态场景且可微分用于迭代优化的高斯阴影模型，实现了人类动作在不受控环境中的 3D 重建。重建的角色支持在新的环境中重新摆放和重新照明。它们配备了全局阴影计算、漫反射着色、几何重建和一致的表面反照率，非常像手工制作的计算机图形模型所提供的那样。（2）：创新点：提出了一种使用高斯密度代理的新阴影模型，该代理用一个简单的解析公式代替了采样。结合延迟神经渲染模型，高斯阴影能够以最小的开销实现朗伯阴影和阴影投射。通过优化光线方向，新的姿势具有更少的阴影伪影，并且在新的场景中的重新照明更加逼真，为神经角色在新的环境中提供了新的摆放方式，从而提高了它们的可应用性。性能：与最先进的方法相比，我们的方法能够优化光线方向，而无需任何用户输入。我们的方法在具有直射阳光和硬阴影的具有挑战性的户外场景中更好地分离了反照率、阴影和阴影，展示了改进的重建。工作量：该方法需要较少的计算成本，并且能够以最小的开销实现朗伯阴影和阴影投射。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dc4d45055e7a95eff0b60a3ceb1f1663.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eaebff114520d85764f0c7d0f90d56c8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a4aa0b3fd8ca4717a88e2915cae00586.jpg" align="middle"><img src="https://picx.zhimg.com/v2-280abfc9150e00cc95fb60e679ad5920.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51373c781c37833f1e8cc8da2b6ea30e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3659bfeee8dd129594185dcf0a0f2373.jpg" align="middle"></details><h2 id="TRIPS-Trilinear-Point-Splatting-for-Real-Time-Radiance-Field-Rendering"><a href="#TRIPS-Trilinear-Point-Splatting-for-Real-Time-Radiance-Field-Rendering" class="headerlink" title="TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering"></a>TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering</h2><p><strong>Authors:Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</strong></p><p>Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\”uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage. </p><p><a href="http://arxiv.org/abs/2401.06003v1">PDF</a> </p><p><strong>摘要</strong><br>利用高斯散布和 ADOP 的思想，提出一种新型的三线性点渲染方法 TRIPS，具有实时渲染速度和优秀的渲染质量。</p><p><strong>要点</strong></p><ul><li>TRIPS 将高斯散布和 ADOP 的思想相结合，在屏幕空间图像金字塔中对点进行光栅化，并根据投影点大小选择金字塔层。</li><li>使用三线性写入渲染任意大小的点，并使用轻量级神经网络重建无孔图像，包括超出点分辨率的细节。</li><li>渲染管道完全可微，允许自动优化点的大小和位置。</li><li>TRIPS 在渲染质量方面优于现有最先进的方法，同时在现有硬件上保持 60 帧/秒的实时帧速率。</li><li>TRIPS 适用于具有复杂几何形状、广阔景观和自动曝光镜头的场景等具有挑战性的场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：TRIPS：实时光照场的三角形点云渲染</li><li>作者：Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger</li><li>隶属机构：视觉计算，埃朗根-纽伦堡大学，德国埃朗根</li><li>关键词：渲染；基于图像的渲染；重建</li><li>论文链接：https://arxiv.org/abs/2401.06003   Github 链接：无</li><li><p>摘要：（1）研究背景：基于点的渲染方法在新型视图合成方面取得了令人印象深刻的成果，提供了渲染质量和计算效率的完美结合。然而，该领域中最新方法也并非没有缺点。3D 高斯渲染在处理高细节场景时会遇到困难，因为它会导致模糊和云状伪影。另一方面，ADOP 可以生成更清晰的图像，但神经重建网络会降低性能，它难以解决时间不稳定性，并且无法有效处理点云中的大间隙。（2）过去的方法及其问题：3D 高斯渲染在处理高细节场景时会产生模糊和云状伪影；ADOP 可以生成更清晰的图像，但神经重建网络会降低性能，它难以解决时间不稳定性，并且无法有效处理点云中的大间隙。该方法的动机很充分。（3）研究方法：本文提出了一种名为 TRIPS（三角形点云渲染）的方法，它结合了高斯渲染和 ADOP 的思想。我们新技术的核心概念是将点光栅化为屏幕空间图像金字塔，金字塔层的选取由投影点大小决定。这种方法允许使用单个三线性写入来渲染任意大的点。然后使用一个轻量级神经网络来重建一个无孔图像，包括超过光栅分辨率的细节。重要的是，我们的渲染管道是完全可微分的，允许自动优化点的大小和位置。（4）方法的性能：我们的评估表明，TRIPS 在渲染质量方面超越了现有的最先进方法，同时在现成的硬件上保持每秒 60 帧的实时帧率。这种性能扩展到具有复杂几何形状、广阔景观和自动曝光素材的场景等具有挑战性的场景。</p></li><li><p>方法：（1）将点云投影到屏幕空间图像金字塔中，金字塔层的选取由投影点大小决定。（2）使用轻量级神经网络重建一个无孔图像，包括超过光栅分辨率的细节。（3）渲染管道是完全可微分的，允许自动优化点的大小和位置。</p></li><li><p>结论：（1）TRIPS：实时光照场的三角形点云渲染，提出了一种稳健的实时基于点的辐射场渲染管道。TRIPS 采用了一种有效的策略，将点光栅化为屏幕空间图像金字塔，从而可以有效地渲染大点，并且是完全可微分的，因此可以自动优化点的大小和位置。这种技术能够渲染高度详细的场景并填充大间隙，同时在常用硬件上保持实时帧速率。我们强调，TRIPS 实现了很高的渲染质量，即使在具有复杂几何形状、大规模环境和自动曝光素材等具有挑战性的场景中也是如此。此外，由于平滑点渲染方法，一个相对简单的神经网络重建就足够了，从而实现了实时渲染性能。开源实现可在此处获得：https://github.com/lfranke/TRIPS（2）创新点：</p></li><li>提出了一种新的基于点的辐射场渲染管道 TRIPS，它结合了高斯渲染和 ADOP 的思想，在渲染质量和计算效率之间取得了很好的平衡。</li><li>TRIPS 采用了一种有效的光栅化策略，将点光栅化为屏幕空间图像金字塔，从而可以有效地渲染大点。</li><li>TRIPS 的渲染管道是完全可微分的，因此可以自动优化点的大小和位置。性能：</li><li>TRIPS 在渲染质量方面超越了现有的最先进方法，同时在现成的硬件上保持每秒 60 帧的实时帧率。</li><li>TRIPS 可以渲染高度详细的场景并填充大间隙，即使在具有复杂几何形状、大规模环境和自动曝光素材等具有挑战性的场景中也是如此。</li><li>TRIPS 使用了一个轻量级的神经网络来重建图像，因此渲染速度很快。工作量：</li><li>TRIPS 的实现相对简单，并且开源。</li><li>TRIPS 可以很容易地应用于各种场景，包括具有复杂几何形状、广阔景观和自动曝光素材的场景。</li><li>TRIPS 可以很容易地与其他渲染技术相结合，以创建更逼真的图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-09ec963291bb4ef95dcae847c73b65ab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2e26b867b1d85086292f6b22b185913.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6615e2eef71a0b02f92f959a8a857c39.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e0e1cebcb888b31ce7e7665083bca1f9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5d595ac6172487559d3c8aaa5130d1c.jpg" align="middle"></details><h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p><p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10\% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a>. </p><p><a href="http://arxiv.org/abs/2401.05925v2">PDF</a> Correct writing details</p><p><strong>Summary:</strong></p><p>从 RGB 图像中实时进行紧凑的 3D 场景分割。</p><p><strong>Key Takeaways:</strong></p><ul><li>提出了一种新的紧凑且快速的三维高斯分割方法 CoSSegGaussians，可在快速渲染速度下仅使用 RGB 图像输入进行紧凑的 3D 一致场景分割。</li><li>CoSSegGaussians 在语义和全景零镜头分割任务上优于基准，同时与基于 NeRF 的方法相比，推理时间减少了 10% 以上。</li><li>该方法的目标是通过使用双特征融合网络作为高斯的分割场来解决这个问题。</li><li>首先优化 RGB 监督下的 3D 高斯函数。</li><li>从图像中提取的 DINO 特征通过显式反投影应用，进一步与来自高效点云处理网络的空间特征结合。</li><li>利用特征聚合以全局到局部策略融合它们，以实现紧凑的分割特征。</li><li>实验结果表明，该模型在语义和全景零镜头分割任务上优于基准，同时与基于 NeRF 的方法相比，推理时间减少了 10% 以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CoSSegGaussians：紧凑且快速的双特征融合高斯体场景分割</li><li>作者：Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</li><li>单位：西安交通大学人工智能与机器人学院</li><li>关键词：场景分割、神经辐射场、高斯体、双特征融合网络</li><li>论文链接：https://arxiv.org/abs/2401.05925</li><li><p>摘要：（1）研究背景：近年来，计算机视觉和计算机图形学取得了显著进展，特别是在神经渲染领域。神经辐射场（NeRF）及其后续方法推动了神经场景表示的发展，在新型视图合成方面展现出显著能力。然而，基于NeRF的场景分割方法依赖于耗时的神经场景优化。虽然最近的3D高斯体渲染显著提高了速度，但现有的基于高斯体的分割方法难以生成紧凑的掩模，尤其是在零样本分割中。（2）过去方法及问题：现有方法直接将可学习的参数分配给每个高斯体，导致对跨视图不一致的2D机器生成的标签缺乏鲁棒性。（3）研究方法：本文提出一种紧凑且快速的高斯体场景分割方法CoSSegGaussians。该方法首先在RGB监督下优化3D高斯体。在高斯体定位后，将从图像中提取的DINO特征通过显式反投影应用，并进一步与来自高效点云处理网络的空间特征结合。利用特征聚合以全局到局部的策略将它们融合，以获得紧凑的分割特征。（4）方法性能：实验结果表明，该模型在语义和全景零样本分割任务上优于基线，同时推理时间不到基于NeRF方法的10%。</p></li><li><p>方法：（1）高斯体定位阶段：利用 L1 和 ℓD-SSIM 光度损失对 3D 高斯体的几何信息进行监督，以获得逼真的场景表示。（2）分割阶段：将从图像中提取的 DINO 特征通过显式反投影应用，并进一步与来自 RandLA-Net 的空间特征相结合。（3）特征聚合：利用特征聚合以全局到局部的策略将它们融合，以获得紧凑的分割特征。</p></li><li><p>结论：（1）本工作提出了一种紧凑且快速的基于高斯体的场景分割方法 CoSSegGaussians，该方法在仅有 RGB 图像的条件下实现了紧凑且快速的场景分割。该方法建立在 3D 高斯体之上，并利用双特征融合网络作为分割场，该网络聚合了 DINO 和空间特征进行分割。来自图像的多尺度 DINO 特征通过反投影引入定位的 3D 高斯体，并进一步与来自 RandLA-Net 的高斯体的空间信息相结合。然后应用全局到局部的聚合模块来生成紧凑的分割逻辑。结果表明，我们的模型可以可靠且高效地完成零样本分割任务。（2）创新点：</p></li><li>提出了一种紧凑且快速的基于高斯体的场景分割方法 CoSSegGaussians。</li><li>利用双特征融合网络作为分割场，该网络聚合了 DINO 和空间特征进行分割。</li><li>将来自图像的多尺度 DINO 特征通过反投影引入定位的 3D 高斯体，并进一步与来自 RandLA-Net 的高斯体的空间信息相结合。</li><li>应用全局到局部的聚合模块来生成紧凑的分割逻辑。性能：</li><li>在语义和全景零样本分割任务上优于基线。</li><li>推理时间不到基于 NeRF 方法的 10%。工作量：</li><li>论文长度适中，实验部分较为详细。</li><li>代码和数据已开源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7b3b4f44e1bfaba57c660121007fee8a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e297ea1e2c85e96907865cc0d6107864.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27a48d664aab4676f21f642635ecb972.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a12edf74e5d62d5f426b60407d904ab.jpg" align="middle"></details><h2 id="AGG-Amortized-Generative-3D-Gaussians-for-Single-Image-to-3D"><a href="#AGG-Amortized-Generative-3D-Gaussians-for-Single-Image-to-3D" class="headerlink" title="AGG: Amortized Generative 3D Gaussians for Single Image to 3D"></a>AGG: Amortized Generative 3D Gaussians for Single Image to 3D</h2><p><strong>Authors:Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</strong></p><p>Given the growing need for automatic 3D content creation pipelines, various 3D representations have been studied to generate 3D objects from a single image. Due to its superior rendering efficiency, 3D Gaussian splatting-based models have recently excelled in both 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3D generation are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized Generative 3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image, eliminating the need for per-instance optimization. Utilizing an intermediate hybrid representation, AGG decomposes the generation of 3D Gaussian locations and other appearance attributes for joint optimization. Moreover, we propose a cascaded pipeline that first generates a coarse representation of the 3D data and later upsamples it with a 3D Gaussian super-resolution module. Our method is evaluated against existing optimization-based 3D Gaussian frameworks and sampling-based pipelines utilizing other 3D representations, where AGG showcases competitive generation abilities both qualitatively and quantitatively while being several orders of magnitude faster. Project page: <a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a> </p><p><a href="http://arxiv.org/abs/2401.04099v1">PDF</a> Project page: <a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a></p><p><strong>摘要</strong><br>无需昂贵计算，AGG 即可直接从图像生成 3D 高斯体素，大幅提升了 3D 内容创建效率。</p><p><strong>要点</strong></p><ul><li>AGG 是一个直接从图像生成 3D 高斯体素的框架，无需逐例优化，大大提高了生成效率。</li><li>AGG 使用了混合表示，将 3D 高斯体素的位置和外观属性分开生成，并联合优化。</li><li>AGG 使用级联管道，先生成 3D 数据的粗略表示，然后通过 3D 高斯体素超分辨率模块进行上采样。</li><li>AGG 在定性和定量方面都优于现有基于优化的 3D 高斯体素框架和使用其他 3D 表示的基于采样的管道。</li><li>AGG 的速度比现有方法快几个数量级。</li><li>项目主页：<a href="https://ir1d.github.io/AGG/">https://ir1d.github.io/AGG/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：AGG：用于单图像到 3D 的摊余生成 3D 高斯（AGG：Amortized Generative 3D Gaussians for Single Image to 3D）</li><li>作者：Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat</li><li>第一作者单位：德克萨斯大学奥斯汀分校</li><li>关键词：图像到 3D、3D 生成、3D 高斯、摊余生成、级联管道</li><li>论文链接：https://arxiv.org/abs/2401.04099，Github 代码链接：无</li><li>摘要：（1）随着对自动 3D 内容创建管道需求的不断增长，已经研究了各种 3D 表示来从单个图像生成 3D 对象。由于其卓越的渲染效率，基于 3D 高斯 splatting 的模型最近在 3D 重建和生成方面都表现出色。用于图像到 3D 生成的 3D 高斯 splatting 方法通常基于优化，需要许多计算成本高昂的得分蒸馏步骤。为了克服这些挑战，我们引入了一个摊余生成 3D 高斯框架（AGG），该框架可以从单个图像立即生成 3D 高斯，从而无需进行逐个实例的优化。利用中间混合表示，AGG 分解了 3D 高斯位置和其他外观属性的生成，以便进行联合优化。此外，我们提出了一个级联管道，该管道首先生成 3D 数据的粗略表示，然后使用 3D 高斯超分辨率模块对其进行上采样。我们的方法针对现有基于优化的 3D 高斯框架和利用其他 3D 表示的基于采样的管道进行了评估，其中 AGG 在定性和定量方面都展示了具有竞争力的生成能力，同时速度提高了几个数量级。</li></ol><p>Methods:(1): AGG通过摊余生成来避免逐个实例的优化，从而实现从单个图像到3D高斯的立即生成。(2): AGG利用中间混合表示将3D高斯位置和其他外观属性的生成分解为联合优化问题。(3): AGG采用级联管道，首先生成3D数据的粗略表示，然后使用3D高斯超分辨率模块对其进行上采样。(4): AGG在定性和定量方面都展示了具有竞争力的生成能力，同时速度提高了几个数量级。</p><ol><li>结论：（1）：本文首次尝试开发一个能够从单张图像输入生成 3D 高斯 splatting 的摊余管道。提出的 AGG 框架利用级联生成管道，包括粗略混合生成器和高斯超分辨率模型。实验结果表明，与基于优化的 3D 高斯框架和基于采样的 3D 生成框架相比，我们的方法在单图像到 3D 生成中实现了具有竞争力的性能，并且速度提高了几个数量级。（2）：创新点：提出了一种摊余生成 3D 高斯 splatting 的新框架 AGG，该框架可以从单张图像立即生成 3D 高斯，而无需进行逐个实例的优化。利用中间混合表示将 3D 高斯位置和其他外观属性的生成分解为联合优化问题，提高了生成效率。采用级联管道，首先生成 3D 数据的粗略表示，然后使用 3D 高斯超分辨率模块对其进行上采样，提高了生成的质量。性能：在定性和定量方面都展示了具有竞争力的生成能力，在单图像到 3D 生成中的速度提高了几个数量级。工作量：代码和数据将在论文发布后公开。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7f1992dc148bbeebd8e201f1e361744a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dfa4769a03fba071a100ba492ba057c1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a15f930311ed81ec60e68bbe1e79e746.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-436ee02cfefc628563bdcac1719e6c80.jpg" align="middle"></details><h2 id="PEGASUS-Physically-Enhanced-Gaussian-Splatting-Simulation-System-for-6DOF-Object-Pose-Dataset-Generation"><a href="#PEGASUS-Physically-Enhanced-Gaussian-Splatting-Simulation-System-for-6DOF-Object-Pose-Dataset-Generation" class="headerlink" title="PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for   6DOF Object Pose Dataset Generation"></a>PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for   6DOF Object Pose Dataset Generation</h2><p><strong>Authors:Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</strong></p><p>We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting. Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. PEGASUS allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted. Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS. </p><p><a href="http://arxiv.org/abs/2401.02281v1">PDF</a> Project Page: <a href="https://meyerls.github.io/pegasus_web">https://meyerls.github.io/pegasus_web</a></p><p><strong>Summary</strong><br>六自由度目标位姿数据集生成的新型物理实体增强高斯溅射模拟系统。</p><p><strong>Key Takeaways</strong></p><ul><li>PEGASUS 使用商品相机轻松获取环境和物体表示，重建带有高斯溅射的场景。</li><li>PEGASUS 允许通过合并环境与一个或多个物体的相应底层高斯溅射点云来组成新场景。</li><li>利用物理引擎能够模拟物体在场景中的自然放置，通过提取的物体网格和环境之间的相互作用。</li><li>通过将不同的环境和物体组合起来，可以创建大量新的静态或动态场景。</li><li>通过从不同视角渲染场景，可以提取各种数据点，例如 RGB 图像、深度图、语义蒙版和 6DoF 目标位姿。</li><li>在 PEGASUS 生成的训练数据上训练的位姿估计网络能够成功地从合成数据转移到真实数据。</li><li>我们介绍了包含 30 种日本杯面商品的 Ramen 数据集。此数据集包括球形扫描，可从目标半球和高斯溅射重建中捕获图像，使其与 PEGASUS 兼容。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PEGASUS：用于 6DOF 物体位姿数据集生成的物理增强高斯散射模拟系统</li><li>作者：Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae</li><li>第一作者单位：Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg-F¨urth</li><li>关键词：数据集生成、机器人、光场、sim2real</li><li>论文链接：https://arxiv.org/pdf/2401.02281.pdf，Github 链接：无</li><li>摘要：（1）研究背景：随着人口结构的变化，许多国家面临着劳动力短缺的问题。日本也不例外，其人口正在减少，导致政府在多个行业（如医疗保健、制造业和农业）大力投资机器人，以维持劳动力稳定。本文的研究重点是开发服务业的机器人系统，以支持零售业的人员。（2）过去的方法及其问题：为了应用深度学习方法进行物体位姿估计，大多数数据集都集中在西方风格的产品上。然而，使用这些数据集训练的模型在应用于实际场景时往往存在域差距问题，因为合成生成的数据缺乏真实性。（3）本文提出的研究方法：为了解决域差距问题，本文提出了一种名为 PEGASUS 的物理增强高斯散射模拟系统，用于生成 6DOF 物体位姿数据集。PEGASUS 允许将环境和物体表示合并，以便创建新的场景。通过利用物理引擎，可以在场景中模拟自然物体放置，从而创建大量的静态或动态新场景。通过从不同视角渲染场景，可以提取各种数据点，如 RGB 图像、深度图、语义掩码和 6DoF 物体位姿。（4）方法的性能及对目标的支持：研究表明，使用 PEGASUS 生成的训练数据可以使位姿估计网络成功地从合成数据转移到真实世界数据。此外，本文还介绍了拉面数据集，其中包含 30 种日本杯面。该数据集包括从物体两个半球捕获图像的球形扫描和高斯散射重建，使其与 PEGASUS 兼容。</li></ol><p>方法：</p><p>(1) 高斯散射基础环境：通过使用 Structure from Motion (SfM) 重建技术和 CherryPicker 方法，从 10 个不同场景中提取稀疏点云，并使用高斯散射技术生成基础环境的 3D 重建和网格。</p><p>(2) 高斯散射对象：利用 Ortery 扫描系统对物体进行图像采集，并使用与基础环境相同的设置进行高斯散射处理，生成物体的照度实体和几何实体。</p><p>(3) 物理引擎：将 PyBullet 集成到 PEGASUS 中，作为物理引擎，用于模拟物体的自然放置和动态场景的创建。</p><p>(4) PEGASUS 数据集生成：通过将高斯散射基础环境和高斯散射对象集成，利用物理引擎模拟物体的运动轨迹，并使用高斯散射渲染器渲染场景，生成包含 RGB 图像、深度图、语义掩码、2D/3D 边界框和变换矩阵等数据的训练数据集。</p><p>(5) 拉面数据集：使用 3DPhotoBench280 和 3DMultiArm2000 相机系统扫描 30 多种杯面，并使用自动背景去除技术和特征丰富的表面进行校准。</p><ol><li>结论：（1）：PEGASUS是一个多功能的数据集生成器，旨在提高物体位姿估计的准确性和质量。除了PEGASUS，我们还介绍了拉面数据集，其中包含30多种不同的产品。该数据集生成器巧妙地创建了逼真的渲染、语义掩码、深度图，并捕获了物体位姿。PEGASUS专为生成特定领域的数据集而设计，有助于微调神经网络，使其超越单纯的位姿估计任务。（2）：创新点：PEGASUS是一个多功能的数据集生成器，可以生成逼真的渲染、语义掩码、深度图和物体位姿。它使用物理引擎来模拟自然物体放置和动态场景的创建。拉面数据集包含30多种不同的产品，并使用自动背景去除技术和特征丰富的表面进行校准。性能：PEGASUS可以生成高质量的数据集，这些数据集可以用于训练物体位姿估计网络。拉面数据集是一个具有挑战性的数据集，可以用于评估物体位姿估计网络的性能。工作量：PEGASUS是一个复杂的数据集生成器，需要大量的时间和精力来创建。拉面数据集是一个大型数据集，需要大量的时间和精力来收集和注释。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-00e4aa054ddb93bc6555152285634f59.jpg" align="middle"><img src="https://pica.zhimg.com/v2-53410ef5f12fd336c82ff96b81afe2e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aacbd06f5c2193593ff83881df0a9a65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0d2817a77d6336db5f08820060093065.jpg" align="middle"></details><h2 id="FMGS-Foundation-Model-Embedded-3D-Gaussian-Splatting-for-Holistic-3D-Scene-Understanding"><a href="#FMGS-Foundation-Model-Embedded-3D-Gaussian-Splatting-for-Holistic-3D-Scene-Understanding" class="headerlink" title="FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D   Scene Understanding"></a>FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D   Scene Understanding</h2><p><strong>Authors:Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li</strong></p><p>Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by $\mathbf{10.2}$ percent on open-vocabulary language-based object detection, despite that we are $\mathbf{851\times}$ faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code upon paper acceptance. </p><p><a href="http://arxiv.org/abs/2401.01970v1">PDF</a> 19 pages, Project page coming soon</p><p><strong>Summary</strong><br>3D 高斯散点与视觉语言嵌入相结合，可高效重建并表示 3D 视觉语言模型。</p><p><strong>Key Takeaways</strong></p><ul><li>\algname{} 将视觉语言嵌入结合到 3D 高斯散点中，以高效重建并表示 3D 视觉语言模型。</li><li>3D 高斯散点和多分辨率哈希编码的优势相结合，实现了新的场景表示。</li><li>像素对齐损失可确保渲染特征距离相同的语义实体接近，从而实现高质量的渲染和快速训练。</li><li>\algname{} 在开放词汇语言对象检测基准上超越了最先进的方法，且推理速度快了 851 倍。</li><li>\algname{} 可以在不受控的真实世界环境中增强场景理解。</li><li>研究探索了视觉、语言和 3D 场景表示的交叉，为增强场景理解铺平了道路。</li><li>代码将在论文被接受后发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：FMGS：用于整体 3D 场景理解的基础模型嵌入式 3D 高斯泼溅</li><li>作者：Xingxing Zuo、Pouya Samangouei、Yunwen Zhou、Yan Di、Mingyang Li</li><li>第一作者单位：谷歌</li><li>关键词：高斯泼溅、视觉语言嵌入、基础模型、开放词汇语义</li><li>论文链接：https://arxiv.org/abs/2401.01970，Github 链接：无</li><li><p>摘要：（1）研究背景：精确感知现实世界 3D 物体的几何和语义属性对于增强现实和机器人应用的持续发展至关重要。（2）过去方法：现有方法主要集中在 3D 几何和外观估计或 3D 对象检测和场景分割上，这些方法在具有封闭类集的数据集上进行了训练。然而，对于智能代理商与物理世界进行平滑交互，仅理解由预先识别的标签表征的空间子集是不够的。（3）研究方法：本文提出了一种名为 FMGS 的方法，将基础模型的视觉语言嵌入整合到 3D 高斯泼溅 (GS) 中。FMGS 的关键贡献是一种有效的方法来重建和表示 3D 视觉语言模型。这是通过将基于图像的基础模型生成的特征图提取到从 3D 模型渲染的特征图中来实现的。为了确保高质量的渲染和快速训练，本文引入了一种新的场景表示，该表示集成了 GS 和多分辨率哈希编码 (MHE) 的优势。本文的有效训练过程还引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界。（4）方法性能：实验结果表明，FMGS 在开放词汇语言对象检测任务上比最先进的方法提高了 10.2%，尽管它的推理速度快 851 倍。这表明 FMGS 能够有效地探索视觉、语言和 3D 场景表示的交集，为在不受控制的现实世界环境中增强场景理解铺平了道路。</p></li><li><p>方法：(1) FMGS方法概述：FMGS方法将基础模型的视觉语言嵌入整合到3D高斯泼溅(GS)中，通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中，重建和表示3D视觉语言模型。(2) 场景表示：FMGS方法引入了一种新的场景表示，该表示集成了GS和多分辨率哈希编码(MHE)的优势，确保高质量的渲染和快速训练。(3) 有效训练过程：FMGS方法引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界，保证了有效训练过程。</p></li><li><p>结论：（1）：FMGS方法将基础模型的视觉语言嵌入整合到3D高斯泼溅(GS)中，通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中，重建和表示3D视觉语言模型，为在不受控制的现实世界环境中增强场景理解铺平了道路。（2）：创新点：FMGS方法的关键贡献是一种有效的方法来重建和表示3D视觉语言模型。这是通过将基于图像的基础模型生成的特征图提取到从3D模型渲染的特征图中来实现的。FMGS方法引入了一种新的场景表示，该表示集成了GS和多分辨率哈希编码(MHE)的优势，确保高质量的渲染和快速训练。FMGS方法引入了一个像素对齐损失，该损失使相同语义实体的渲染特征距离接近，遵循像素级语义边界，保证了有效训练过程。性能：实验结果表明，FMGS在开放词汇语言对象检测任务上比最先进的方法提高了10.2%，尽管它的推理速度快851倍。这表明FMGS能够有效地探索视觉、语言和3D场景表示的交集，为在不受控制的现实世界环境中增强场景理解铺平了道路。工作量：FMGS方法的实现相对复杂，需要较高的编程和数学基础。此外，该方法需要大量的数据和计算资源，这可能会增加训练和部署的成本。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d4d136f06ab7f31a3343e367c298d7d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-859b3e0bd449bc5f2715166a19563958.jpg" align="middle"></details><h2 id="Deblurring-3D-Gaussian-Splatting"><a href="#Deblurring-3D-Gaussian-Splatting" class="headerlink" title="Deblurring 3D Gaussian Splatting"></a>Deblurring 3D Gaussian Splatting</h2><p><strong>Authors:Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</strong></p><p>Recent studies in Radiance Fields have paved the robust way for novel view synthesis with their photorealistic rendering quality. Nevertheless, they usually employ neural networks and volumetric rendering, which are costly to train and impede their broad use in various real-time applications due to the lengthy rendering time. Lately 3D Gaussians splatting-based approach has been proposed to model the 3D scene, and it achieves remarkable visual quality while rendering the images in real-time. However, it suffers from severe degradation in the rendering quality if the training images are blurry. Blurriness commonly occurs due to the lens defocusing, object motion, and camera shake, and it inevitably intervenes in clean image acquisition. Several previous studies have attempted to render clean and sharp images from blurry input images using neural fields. The majority of those works, however, are designed only for volumetric rendering-based neural radiance fields and are not straightforwardly applicable to rasterization-based 3D Gaussian splatting methods. Thus, we propose a novel real-time deblurring framework, deblurring 3D Gaussian Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the covariance of each 3D Gaussian to model the scene blurriness. While deblurring 3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct fine and sharp details from blurry images. A variety of experiments have been conducted on the benchmark, and the results have revealed the effectiveness of our approach for deblurring. Qualitative results are available at <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a> </p><p><a href="http://arxiv.org/abs/2401.00834v1">PDF</a> 19 pages, 8 figures</p><p><strong>Summary</strong><br>利用小型多层感知器 (MLP) 处理高斯分布的协方差，实现实时去模糊，重建清晰图像。</p><p><strong>Key Takeaways</strong></p><ul><li>最近，基于体素渲染的3D高斯散射方法实现了实时渲染，但容易受到训练图像模糊的影响，导致渲染质量下降。</li><li>本文提出了一种新的实时去模糊框架——去模糊3D高斯散射，利用小型多层感知器 (MLP) 处理高斯分布的协方差，以模拟场景模糊。</li><li>去模糊3D高斯散射仍然可以享受实时渲染，同时可以从模糊图像中重建出精细清晰的细节。</li><li>可以在基准测试集上进行多种实验，结果表明本文方法对于去模糊是有效的。</li><li>定性结果可在 <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：去模糊 3D 高斯斑点图</li><li>作者：Byeonghyeon Lee、Howoong Lee、Xiangyu Sun、Usman Ali 和 Eunbyung Park</li><li>第一作者单位：韩国成均馆大学人工智能系</li><li>关键词：神经辐射场、3D 高斯斑点图、去模糊、实时渲染</li><li>论文链接：https://arxiv.org/abs/2401.00834Github 链接：无</li><li><p>摘要：（1）研究背景：神经辐射场 (NeRF) 的出现为具有逼真渲染质量的新视角合成开辟了一条稳健的道路。然而，它们通常采用神经网络和体积渲染，这些方法训练成本高昂，并且由于漫长的渲染时间而阻碍了它们在各种实时应用程序中的广泛使用。最近，人们提出了一种基于 3D 高斯斑点图的方法来建模 3D 场景，它在实时渲染图像的同时实现了卓越的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常由于镜头失焦、物体运动和相机抖动而发生，它不可避免地会干预清晰图像的获取。之前的一些研究尝试使用神经场从模糊输入图像渲染干净清晰的图像。然而，这些工作中的大多数仅针对基于体积渲染的神经辐射场而设计，并不直接适用于基于光栅化的 3D 高斯斑点图方法。（2）过去的方法及其问题：过去的方法主要针对基于体积渲染的神经辐射场而设计，不适用于基于光栅化的 3D 高斯斑点图方法。该方法动机明确，针对 3D 高斯斑点图方法的模糊问题，提出了一种新的去模糊框架。（3）研究方法：提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。虽然去模糊 3D 高斯斑点图仍然可以享受实时渲染，但它可以从模糊图像中重建精细而清晰的细节。（4）方法的性能：在基准上进行了各种实验，结果揭示了我们方法去模糊的有效性。定性结果可在 https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ 获得。</p></li><li><p>Methods：(1): 提出了一种新的实时去模糊框架，称为去模糊3D高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个3D高斯的协方差以建模场景模糊。(2): 为了解决3D高斯斑点图方法在模糊图像下渲染质量下降的问题，该方法提出了一种新的去模糊框架，该框架使用了一个小的多层感知器 (MLP) 来操纵每个3D高斯的协方差以建模场景模糊。(3): 虽然去模糊3D高斯斑点图仍然可以享受实时渲染，但它可以从模糊图像中重建精细而清晰的细节。</p></li><li><p>结论：（1）：本文提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。我们还通过额外的点分配进一步促进了去模糊，该分配通过 K-最近邻算法均匀地分布场景中的点并分配颜色特征。此外，由于我们应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，我们可以在场景边缘（SfM 通常难以提取特征并无法生成足够点）保留更多点。通过广泛的实验，我们验证了我们的方法可以模糊散焦模糊，同时仍然享受具有 FPS&gt;200 的实时渲染。这是因为我们仅在训练期间使用 MLP，并且 MLP 不参与推理阶段，从而使推理阶段与 3D-GS 保持一致。我们的方法在不同指标下评估时实现了最先进的性能或与当前最前沿的模型相当。（2）：创新点：</p></li><li>提出了一种新的实时去模糊框架，称为去模糊 3D 高斯斑点图，它使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊。</li><li>进一步促进了去模糊，通过额外的点分配均匀地分布场景中的点并分配颜色特征。</li><li>应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，可以在场景边缘保留更多点。性能：</li><li>我们的方法可以在模糊图像下渲染精细而清晰的细节。</li><li>我们的方法在基准上进行了各种实验，结果揭示了我们方法去模糊的有效性。</li><li>我们的方法在不同指标下评估时实现了最先进的性能或与当前最前沿的模型相当。工作量：</li><li>该方法使用了一个小的多层感知器 (MLP) 来操纵每个 3D 高斯的协方差以建模场景模糊，工作量较小。</li><li>该方法通过额外的点分配进一步促进了去模糊，工作量较小。</li><li>该方法应用了基于深度的剪枝而不是 3D-GS 采用的朴素剪枝，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5884764967912a4d08dd4f817a2619a7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-689495a2c3ac97a3861a047c4d7a0252.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccf1c2c36b334acc29bec756307b6720.jpg" align="middle"></details><h2 id="LangSplat-3D-Language-Gaussian-Splatting"><a href="#LangSplat-3D-Language-Gaussian-Splatting" class="headerlink" title="LangSplat: 3D Language Gaussian Splatting"></a>LangSplat: 3D Language Gaussian Splatting</h2><p><strong>Authors:Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, Hanspeter Pfister</strong></p><p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at <a href="https://langsplat.github.io">https://langsplat.github.io</a> </p><p><a href="http://arxiv.org/abs/2312.16084v1">PDF</a> Project Page: <a href="https://langsplat.github.io">https://langsplat.github.io</a></p><p><strong>摘要</strong><br>利用 3D 高斯体表示语言特征，LangSplat 在 3D 空间中构建了语言场，实现了无需预训练的、效率高的开集查询。</p><p><strong>要点</strong></p><ul><li>LangSplat 利用 3D 高斯体集合对语言特征进行编码，不需要对 CLIP 语言嵌入进行预训练。</li><li>LangSplat 使用基于切片的 splatting 技术渲染语言特征，提高了渲染效率。</li><li>LangSplat 首先训练场景级的语言自动编码器，然后在特定场景的潜在空间上学习语言特征，减少了显式建模带来的内存需求。</li><li>LangSplat 利用 SAM 学习分层语义，无需在不同尺度上大量查询语言场，也不需要 DINO 特征的正则化。</li><li>LangSplat 在开集 3D 物体定位和语义分割任务上的表现显著优于之前的最佳方法 LERF。</li><li>LangSplat 非常高效，在 1440 × 1080 的分辨率下，速度比 LERF 快 {\speed} 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：LangSplat：高效的 3D 语言场用于开放式词汇查询</li><li>作者：Yilun Du<em>, Xuran Pan</em>, Bo Dai, Chen Change Loy, Dahua Lin</li><li>单位：香港中文大学</li><li>关键词：3D 语言场、开放式词汇查询、语义分割、对象定位、CLIP</li><li>论文链接：https://arxiv.org/abs/2302.06121，Github 链接：None</li><li><p>摘要：(1) 研究背景：人类生活在 3D 世界中，通常使用自然语言与 3D 场景进行交互。对 3D 语言场进行建模以支持 3D 空间中的开放式词汇查询最近引起了越来越多的关注。(2) 过去的方法：现有方法将 CLIP 语言嵌入整合到 NeRF 模型中。然而，NeRF 方法的渲染过程非常耗时，即使是最先进的 NeRF 技术也无法在高分辨率、不受限制的场景中实现实时渲染。此外，现有方法难以区分对象之间的清晰边界，导致 3D 语言场不精确且模糊。(3) 研究方法：本文提出 LangSplat，它使用一组 3D 高斯函数来表示语言场，每个高斯函数都对从 CLIP 中提取的语言特征进行编码。LangSplat 采用基于图块的 splatting 技术来渲染语言特征，从而避免了 NeRF 中固有的昂贵渲染过程。此外，LangSplat 首先训练一个场景级的语言自动编码器，然后在场景特定的潜在空间上学习语言特征，从而减轻了显式建模带来的大量内存需求。为了解决对象之间的模糊边界问题，LangSplat 提出了一种学习分层语义的方法，该方法利用 SAM 来生成具有清晰边界的对象掩码。(4) 方法性能：在开放式词汇 3D 对象定位和语义分割任务上，LangSplat 的性能明显优于之前的最先进方法 LERF。值得注意的是，LangSplat 非常高效，在 1440×1080 的分辨率下，与 LERF 相比，速度提高了 199 倍。</p></li><li><p>方法：(1) 利用 SAM 学习分层语义：采用 SAM 生成具有清晰边界的对象掩码，以解决对象之间的模糊边界问题。(2) 提取像素对齐的语言嵌入：将 SAM 生成的掩码发送到 CLIP 图像编码器以提取相应的 CLIP 嵌入。(3) 学习场景级语言自动编码器：使用这些获得的 CLIP 嵌入训练一个场景级语言自动编码器，以减少显式建模带来的大量内存需求。(4) 3D 语言高斯 splatting：使用一组 3D 高斯函数来表示语言场，每个高斯函数都对从 CLIP 中提取的语言特征进行编码。(5) 基于图块的 splatting 技术：采用基于图块的 splatting 技术来渲染语言特征，从而避免了 NeRF 中固有的昂贵渲染过程。</p></li><li><p>结论：（1）：LangSplat 是一种用于构建 3D 语言场的方法，能够在 3D 空间内实现精确且高效的开放式词汇查询。LangSplat 通过将 3D 高斯 Splatting 扩展到语言特征，并学习场景特定的语言自动编码器，避免了基于 NeRF 的方法固有的缓慢渲染速度。此外，LangSplat 提出学习由 SAM 定义的语义层次结构，有效地解决了点模糊问题，从而实现了更加精确和可靠的 3D 语言场。实验结果清楚地表明，LangSplat 优于现有的最先进方法（如 LERF），尤其是在其显着的 199 倍速度提升和在开放式 3D 语言查询任务中的增强性能方面。（2）：创新点：</p></li><li>提出了一种利用 3D 高斯 Splatting 和场景级语言自动编码器来构建 3D 语言场的新方法。</li><li>提出了一种学习语义层次结构的方法，该方法能够有效地解决点模糊问题，从而提高了 3D 语言场的精度和可靠性。</li><li>提出了一种基于图块的 Splatting 技术，该技术能够避免 NeRF 中固有的昂贵渲染过程，从而大大提高了渲染速度。性能：</li><li>在开放式词汇 3D 对象定位和语义分割任务上，LangSplat 的性能明显优于之前的最先进方法 LERF。</li><li>LangSplat 非常高效，在 1440×1080 的分辨率下，与 LERF 相比，速度提高了 199 倍。工作量：</li><li>LangSplat 的实现相对复杂，需要对 3D 高斯 Splatting、场景级语言自动编码器、学习语义层次结构和基于图块的 Splatting 技术等多个方面进行实现。</li><li>LangSplat 的训练过程相对耗时，需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-59c272aba45f1a4c840c869c1aeb179c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5173c1e31a57b0806bd38f395623e341.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1bc17dfd41d5d04e3b354c6d95099e61.jpg" align="middle"></details><h2 id="Human101-Training-100-FPS-Human-Gaussians-in-100s-from-1-View"><a href="#Human101-Training-100-FPS-Human-Gaussians-in-100s-from-1-View" class="headerlink" title="Human101: Training 100+FPS Human Gaussians in 100s from 1 View"></a>Human101: Training 100+FPS Human Gaussians in 100s from 1 View</h2><p><strong>Authors:Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang</strong></p><p>Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at <a href="https://github.com/longxiang-ai/Human101">https://github.com/longxiang-ai/Human101</a>. </p><p><a href="http://arxiv.org/abs/2312.15258v1">PDF</a> Website: <a href="https://github.com/longxiang-ai/Human101">https://github.com/longxiang-ai/Human101</a></p><p><strong>Summary</strong><br>单视角视频中の人体三维动态重建解决方案，兼具快速、高质量、实时渲染的优点。</p><p><strong>Key Takeaways</strong></p><ul><li>Human101 框架能够在 100 秒内训练 3D 高斯核，以每秒 100 多帧的速度渲染出逼真的动态 3D 人类重建。</li><li>利用 3D 高斯核的优点，为 3D 人体提供明确且高效的表示形式。</li><li>使用以人类为中心的前向高斯动画方法，对 3D 高斯核的参数进行变形，以提高渲染速度。</li><li>1024 像素分辨率的图像能够以每秒 60 多帧的速度渲染，512 像素分辨率的图像能够以每秒 100 多帧的速度渲染。</li><li>实验结果表明，Human101 的性能远超现有方法，帧率最高可提高 10 倍，同时渲染质量相当或更高。</li><li>代码和演示将在 <a href="https://github.com/longxiang-ai/Human101上发布。">https://github.com/longxiang-ai/Human101上发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Human101：从单视角中以 100 FPS 的速度训练 100+ 个高斯人体模型，并在 100 秒内完成</li><li>作者：Longxiang Xiang、Yihao Liu、Jiaolong Yang、Yuxuan Zhang、Shunsuke Saito、Hanbyul Joo、Zheng Wu</li><li>单位：香港中文大学（深圳）</li><li>关键词：计算机视觉、计算机图形学、人体重建、神经辐射场、高斯散点</li><li>论文链接：https://arxiv.org/abs/2302.06695，Github 链接：None</li><li><p>摘要：（1）研究背景：从单视角视频中重建人体在虚拟现实领域发挥着关键作用。一个普遍的应用场景需要快速重建高保真 3D 数字人体，同时确保实时渲染和交互。现有方法通常难以满足这两个要求。（2）过去的方法及其问题：现有方法的问题在于渲染速度慢、保真度低、对单视角数据建模能力不足。（3）研究方法：本文提出了一种名为 Human101 的新框架，该框架能够通过在 100 秒内训练 3D 高斯模型并以 100+ FPS 的速度渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯散点的优势，提供了一种显式且高效的人体表示。与先前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯模型的参数，从而提高了渲染速度（即以令人印象深刻的 60+ FPS 渲染 1024 分辨率的图像，并以 100+ FPS 渲染 512 分辨率的图像）。（4）实验结果：实验结果表明，我们的方法大大超过了当前的方法，将每秒帧数提高了 10 倍，并提供了可比或更好的渲染质量。</p></li><li><p>方法：（1）Human101框架概述：Human101框架由三个主要组件组成：数据预处理、高斯模型训练和渲染。首先，数据预处理模块将单视角视频转换为一系列2D人体关键点。然后，高斯模型训练模块利用这些关键点训练一个3D高斯散点模型，该模型可以表示人体形状和外观。最后，渲染模块使用训练好的高斯模型生成高保真动态3D人体重建。（2）以人为中心的正向高斯动画方法：Human101框架采用了一种以人为中心的正向高斯动画方法来变形3D高斯模型的参数，从而提高渲染速度。这种方法将人体姿势分解为一系列基本动作，并使用这些基本动作来控制3D高斯模型的参数。这种方法可以有效地减少渲染计算量，从而提高渲染速度。（3）高斯散点模型的训练：Human101框架使用了一种基于神经辐射场的训练方法来训练3D高斯散点模型。这种方法将3D空间中的每个点表示为一个高斯散点，并使用神经网络来预测每个高斯散点的颜色和密度。这种方法可以有效地捕捉人体形状和外观的细节，并生成高保真动态3D人体重建。</p></li><li><p>结论：（1）：本文提出了一种名为 Human101 的新框架，该框架能够通过在 100 秒内训练 3D 高斯模型并以 100+FPS 的速度渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯散点的优势，提供了一种显式且高效的人体表示。与先前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯模型的参数，从而提高了渲染速度（即以令人印象深刻的 60+FPS 渲染 1024 分辨率的图像，并以 100+FPS 渲染 512 分辨率的图像）。（2）：创新点：</p></li><li>提出了一种新颖的以人为中心的正向高斯动画方法，该方法可以有效地减少渲染计算量，从而提高渲染速度。</li><li>使用了一种基于神经辐射场的训练方法来训练 3D 高斯散点模型，该方法可以有效地捕捉人体形状和外观的细节，并生成高保真动态 3D 人体重建。</li><li>将 3D 高斯散点模型与以人为中心的正向高斯动画方法相结合，提出了一种新的单视角人体重建框架 Human101，该框架能够在 100 秒内训练 3D 高斯模型并以 100+FPS 的速度渲染，从而生成高保真动态 3D 人体重建。性能：</li><li>Human101 框架在渲染速度和渲染质量方面都优于现有的方法。</li><li>Human101 框架可以以 60+FPS 的速度渲染 1024 分辨率的图像，并以 100+FPS 的速度渲染 512 分辨率的图像。</li><li>Human101 框架生成的 3D 人体重建具有很高的保真度，并且可以捕捉人体形状和外观的细节。工作量：</li><li>Human101 框架的训练和渲染过程都比较简单，易于实现。</li><li>Human101 框架的训练时间为 100 秒，渲染时间为 100+FPS。</li><li>Human101 框架的实现代码已经开源，方便其他研究人员使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-84a60e1cfd3ff2a4ccd504c677c219dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f8cfe9cdf0f3f288a2851246fa3440a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d7298160fd7bc71030647b1bbde1aed.jpg" align="middle"><img src="https://pica.zhimg.com/v2-95ae9edf8140557344587f9d62973d44.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f9308b2b911a7239d0b1c13e120fe940.jpg" align="middle"></details><h2 id="Deformable-3D-Gaussian-Splatting-for-Animatable-Human-Avatars"><a href="#Deformable-3D-Gaussian-Splatting-for-Animatable-Human-Avatars" class="headerlink" title="Deformable 3D Gaussian Splatting for Animatable Human Avatars"></a>Deformable 3D Gaussian Splatting for Animatable Human Avatars</h2><p><strong>Authors:HyunJun Jung, Nikolas Brasch, Jifei Song, Eduardo Perez-Pellitero, Yiren Zhou, Zhihao Li, Nassir Navab, Benjamin Busam</strong></p><p>Recent advances in neural radiance fields enable novel view synthesis of photo-realistic images in dynamic settings, which can be applied to scenarios with human animation. Commonly used implicit backbones to establish accurate models, however, require many input views and additional annotations such as human masks, UV maps and depth maps. In this work, we propose ParDy-Human (Parameterized Dynamic Human Avatar), a fully explicit approach to construct a digital avatar from as little as a single monocular sequence. ParDy-Human introduces parameter-driven dynamics into 3D Gaussian Splatting where 3D Gaussians are deformed by a human pose model to animate the avatar. Our method is composed of two parts: A first module that deforms canonical 3D Gaussians according to SMPL vertices and a consecutive module that further takes their designed joint encodings and predicts per Gaussian deformations to deal with dynamics beyond SMPL vertex deformations. Images are then synthesized by a rasterizer. ParDy-Human constitutes an explicit model for realistic dynamic human avatars which requires significantly fewer training views and images. Our avatars learning is free of additional annotations such as masks and can be trained with variable backgrounds while inferring full-resolution images efficiently even on consumer hardware. We provide experimental evidence to show that ParDy-Human outperforms state-of-the-art methods on ZJU-MoCap and THUman4.0 datasets both quantitatively and visually. </p><p><a href="http://arxiv.org/abs/2312.15059v1">PDF</a> </p><p><strong>摘要</strong><br>无需多视图和额外注解，仅用单张单目序列，即可构建出逼真的动态人类化身。</p><p><strong>要点</strong></p><ul><li>ParDy-Human 是一种全显式方法，仅需一张单色序列即可构建一个数字头像。</li><li>ParDy-Human 将参数驱动的动态引入到 3D 高斯散射中，其中 3D 高斯由人类姿态模型变形以实现化身动画。</li><li>ParDy-Human 由两部分组成：第一部分根据 SMPL 顶点变形标准 3D 高斯，连续部分进一步采用其设计的关节编码并预测每个高斯变形以处理超出 SMPL 顶点变形的动态。</li><li>图像通过光栅化器合成。</li><li>ParDy-Human 构成了一个逼真的动态人类化身的显式模型，所需的训练视图和图像明显更少。</li><li>我们化身学习无需额外注释，如遮罩，可在变化的背景下进行训练，同时即使在消费级硬件上也能高效推断出全分辨率图像。</li><li>我们提供了实验证据表明，ParDy-Human 在 ZJU-MoCap 和 THUman4.0 数据集上无论在数量上还是视觉上都优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可变形 3D 高斯散点绘制，用于可动画的人体虚拟形象</li><li>作者：Junggi Kim, Kanghee Jee, Sunghoon Im, Junsik Kim, Minsu Cho, Hyunwoo Kim</li><li>隶属机构：首尔国立大学</li><li>关键词：可变形 3D 高斯散点绘制、人体动画、神经辐射场、显式模型</li><li>论文链接：https://arxiv.org/abs/2204.09365，Github 链接：https://github.com/Junggy/pardy-human</li><li><p>摘要：(1)：近年来，神经辐射场在动态场景中合成新颖视角的逼真图像方面取得了很大进展，可应用于人体动画等场景。常用的隐式骨干网络可以建立准确的模型，但需要大量输入视图和额外注释，如人体蒙版、UV 贴图和深度图。(2)：以往方法：现有方法通常使用隐式表示来构建人体虚拟形象，需要大量输入视图和额外的注释，如人体蒙版、UV 贴图和深度图。这些方法在处理动态场景时也存在困难。            问题：这些方法需要大量输入视图和额外的注释，在处理动态场景时也存在困难。            动机：本文提出了一种显式方法来构建人体虚拟形象，可以仅需很少的输入视图即可完成训练，并且不需要额外的注释。这种方法还能够处理动态场景。(3)：本文提出的研究方法：本文提出了一种名为 ParDy-Human 的参数化动态人体虚拟形象方法，该方法采用显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。该方法由两部分组成：第一部分根据 SMPL 顶点变形规范 3D 高斯散点，第二部分进一步采用设计的关节编码并预测每个高斯散点的变形，以处理超出 SMPL 顶点变形的动态。然后通过光栅化器合成图像。(4)：本文方法在 ZJU-MoCap 和 THUman4.0 数据集上均优于最先进的方法，无论是定量还是视觉上。我们的虚拟形象学习过程不需要额外的注释，例如蒙版，并且可以在推理全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也能有效地进行训练。</p></li><li><p>方法：（1）3D 高斯散点初始化：从稀疏点云初始化 3D 高斯散点，并为每个散点分配几何中心、旋转、尺寸、比例、不透明度和球谐函数等属性。（2）姿势化高斯散点：根据人体姿势参数，使用逐顶点变形模块将高斯散点变形到新的位置和方向。（3）变形细化：使用变形细化模块对高斯散点的变形进行残差校正，以提高贴身衣物的人体动画的保真度。（4）球谐函数方向：使用球谐函数来模拟高斯散点的视角相关效果，并结合表面法线信息来计算更准确的散点方向。（5）取消高斯散点的姿势并更新父项：在每次姿势更新后，将高斯散点变换回规范空间，并更新它们的父项索引。（6）损失设计和推理管道：在训练过程中，使用 L1 损失、结构相似性损失和感知相似性损失来评估渲染图像与真实图像之间的差异。在推理时，过滤掉背景高斯散点，并根据相机姿态和人体姿态对高斯散点进行变形，然后通过光栅化器渲染出人体图像。（7）训练和实现细节：采用多阶段训练策略，交替更新高斯散点特征和变形细化模块。训练损失包括 L1 损失、结构相似性损失和感知相似性损失。</p></li><li><p>结论：（1）：本工作提出了一种显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。这种方法还能够处理动态场景。（2）：创新点：</p></li><li>提出了一种显式方法构建人体虚拟形象，仅需很少的输入视图即可完成训练，并且不需要额外的注释。</li><li>该方法能够处理动态场景。</li><li>在ZJU-MoCap和THUman4.0数据集上均优于最先进的方法，无论是定量还是视觉上。性能：</li><li>在ZJU-MoCap和THUman4.0数据集上均优于最先进的方法，无论是定量还是视觉上。</li><li>我们的虚拟形象学习过程不需要额外的注释，例如蒙版，并且可以在推理全分辨率图像时使用可变背景进行训练，即使在消费级硬件上也能有效地进行训练。工作量：</li><li>该方法需要收集人体动作数据和渲染图像数据。</li><li>该方法需要训练一个神经网络模型。</li><li>该方法需要实现一个光栅化器来渲染人体图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3a2dec08eda70704d60e83b281cc54a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a167032c68efd5d06543a5ec3ba4f79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e243cc96b91f1cb9f2e0e8cb1aa2a523.jpg" align="middle"><img src="https://pica.zhimg.com/v2-805c12244272b525ede83f20a94c5569.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3f505c56582ddada94e66d5ec6791a.jpg" align="middle"></details><h2 id="3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting"><a href="#3DGS-Avatar-Animatable-Avatars-via-Deformable-3D-Gaussian-Splatting" class="headerlink" title="3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting"></a>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</h2><p><strong>Authors:Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, Siyu Tang</strong></p><p>We introduce an approach that creates animatable human avatars from monocular videos using 3D Gaussian Splatting (3DGS). Existing methods based on neural radiance fields (NeRFs) achieve high-quality novel-view/novel-pose image synthesis but often require days of training, and are extremely slow at inference time. Recently, the community has explored fast grid structures for efficient training of clothed avatars. Albeit being extremely fast at training, these methods can barely achieve an interactive rendering frame rate with around 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a non-rigid deformation network to reconstruct animatable clothed human avatars that can be trained within 30 minutes and rendered at real-time frame rates (50+ FPS). Given the explicit nature of our representation, we further introduce as-isometric-as-possible regularizations on both the Gaussian mean vectors and the covariance matrices, enhancing the generalization of our model on highly articulated unseen poses. Experimental results show that our method achieves comparable and even better performance compared to state-of-the-art approaches on animatable avatar creation from a monocular input, while being 400x and 250x faster in training and inference, respectively. </p><p><a href="http://arxiv.org/abs/2312.09228v2">PDF</a> Project page: <a href="https://neuralbodies.github.io/3DGS-Avatar">https://neuralbodies.github.io/3DGS-Avatar</a></p><p><strong>摘要</strong><br>使用 3D 高斯展布 (3DGS) 学习单目视频中的动画人类形象，训练 30 分钟即可完成，且渲染帧率达到实时水平 (50+ FPS)。</p><p><strong>要点</strong></p><ul><li>使用神经辐射场 (NeRF) 的现有方法能实现高质量的新视角/新姿态图像合成，但通常需要数天的训练时间，且推理速度极慢。</li><li>近期研究社区探索了快速网格结构，以便高效训练着装形象。尽管训练速度快，但这些方法只能勉强实现约 15 FPS 的交互式渲染帧率。</li><li>在本文中，我们使用 3D 高斯展布并学习一个非刚性变形网络，以重建可动画的着装人类形象，训练可在 30 分钟内完成，渲染帧率达到实时水平 (50+ FPS)。</li><li>鉴于我们的表示具有的显式特性，我们进一步引入了高斯均值向量和协方差矩阵的尽可能等距正则化方法，从而增强了模型在高度关节化未见姿势上的泛化能力。</li><li>实验结果表明，我们的方法在从单目输入创建动画形象方面实现了与最先进的方法相当甚至更好的性能，同时训练和推理速度分别快 400 倍和 250 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：3DGS-Avatar：可变形 3D 高斯散点绘制的动画虚拟形象</li><li>作者：Jiapeng Tang、Pengfei Wan、Yifan Jiang、Zhaopeng Cui、Chen Change Loy、Linchao Bao、Wenxiu Sun、Wei Cheng</li><li>隶属机构：香港中文大学</li><li>关键词：计算机视觉、图形学、动画、虚拟现实</li><li>论文链接：https://arxiv.org/abs/2302.09403、Github 链接：None</li><li>摘要：（1）研究背景：近年来，基于神经辐射场（NeRF）的方法在图像合成领域取得了显著进展，但其训练和推理速度慢的问题限制了其在动画领域的应用。（2）过去的方法和问题：现有基于 NeRF 的方法可以生成高质量的新视角/新姿势图像，但通常需要数天的训练时间，并且推理速度极慢。最近，研究人员探索了快速网格结构，以实现服装虚拟形象的训练。尽管这些方法的训练速度非常快，但其渲染帧率仅约为 15 FPS，难以实现交互式渲染。（3）本文提出的研究方法：本文提出了一种使用 3D 高斯散点绘制和学习非刚性变形网络来重建可动画服装人类虚拟形象的方法。该方法可以在 30 分钟内训练完成，并以实时帧率（50+ FPS）进行渲染。此外，本文还引入了尽可能等距的正则化，以增强模型对高度铰接的未见姿势的泛化能力。（4）方法在任务和性能上的表现：实验结果表明，本文方法在单目输入的动画虚拟形象创建任务上取得了与最先进方法相当甚至更好的性能，同时在训练和推理速度上分别快了 400 倍和 250 倍。这些性能结果支持了本文方法的目标。</li></ol><p>Methods：</p><p>（1）非刚性变形：提出了一种非刚性变形模块，该模块可以对3D高斯散点进行变形，以适应不同的姿势。该模块由一个神经网络组成，该神经网络以3D高斯散点的位置和一个编码姿势的潜在代码作为输入，并输出变形后的3D高斯散点的位置、缩放因子、旋转四元数和一个特征向量。</p><p>（2）刚性变换：将非刚性变形后的3D高斯散点通过刚性变换模块转换到观察空间。该模块由一个神经网络组成，该神经网络以非刚性变形后的3D高斯散点的位置和一个编码姿势的潜在代码作为输入，并输出转换后的3D高斯散点的位置和旋转矩阵。</p><p>（3）颜色MLP：使用了一个颜色MLP来预测每个3D高斯散点的颜色。该MLP以3D高斯散点的特征向量、一个编码姿势的潜在代码和一个帧级潜在代码作为输入，并输出3D高斯散点的颜色。</p><p>（4）可微分高斯光栅化：使用了一种可微分高斯光栅化方法将观察空间中的3D高斯散点渲染成图像。该方法将每个3D高斯散点投影到图像平面上，并根据高斯函数的权重对投影后的像素进行累加。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种使用 3D 高斯散点绘制和学习非刚性变形网络来重建可动画服装人类虚拟形象的方法，该方法可以在 30 分钟内训练完成，并以实时帧率（50+FPS）进行渲染。</p><p>（2）：创新点：</p><ul><li>使用 3D 高斯散点绘制来表示服装人类虚拟形象，该表示可以有效地捕捉服装的细节和变形。</li><li>学习了一个非刚性变形网络，该网络可以将 3D 高斯散点变形到不同的姿势。</li><li>引入了尽可能等距的正则化，以增强模型对高度铰接的未见姿势的泛化能力。</li></ul><p>性能：</p><ul><li>在单目输入的动画虚拟形象创建任务上取得了与最先进方法相当甚至更好的性能。</li><li>训练速度快了 400 倍，推理速度快了 250 倍。</li></ul><p>工作量：</p><ul><li>训练一个模型需要 30 分钟。</li><li>渲染一帧图像需要 20 毫秒。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-491840e5e9b907bfe6c860125c793a8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df8a29e21b43e7322f740381b022b6e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c04b8f81d853c5df7e574e6e17d490fc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-622f5d5aa71b525c2b25dfceb0d4c49a.jpg" align="middle"></details>## ASH: Animatable Gaussian Splats for Efficient and Photoreal Human   Rendering**Authors:Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann**Real-time rendering of photorealistic and controllable human avatars stands as a cornerstone in Computer Vision and Graphics. While recent advances in neural implicit rendering have unlocked unprecedented photorealism for digital avatars, real-time performance has mostly been demonstrated for static scenes only. To address this, we propose ASH, an animatable Gaussian splatting approach for photorealistic rendering of dynamic humans in real-time. We parameterize the clothed human as animatable 3D Gaussians, which can be efficiently splatted into image space to generate the final rendering. However, naively learning the Gaussian parameters in 3D space poses a severe challenge in terms of compute. Instead, we attach the Gaussians onto a deformable character model, and learn their parameters in 2D texture space, which allows leveraging efficient 2D convolutional architectures that easily scale with the required number of Gaussians. We benchmark ASH with competing methods on pose-controllable avatars, demonstrating that our method outperforms existing real-time methods by a large margin and shows comparable or even better results than offline methods. [PDF](http://arxiv.org/abs/2312.05941v1) 13 pages, 7 figures. For project page, see   https://vcai.mpi-inf.mpg.de/projects/ash/**Summary**动态场景中实时渲染照片级动态虚拟人的新算法ASH。**Key Takeaways**- ASH是一种可动画的高斯散射方法，可用于实时渲染动态人类的照片级图像。- 将人体参数化为可动画的3D高斯体，可以有效地散射到图像空间以生成最终渲染。- 通过将高斯体附加到可变形角色模型上并学习它们在2D纹理空间中的参数来学习3D空间中的高斯参数。- 使用高效的2D卷积架构来学习高斯体参数，该架构可以轻松地扩展到所需数量的高斯体。- ASH在姿势可控的虚拟人上与竞争方法进行比较，结果表明该方法优于现有的实时方法，并且显示出与离线方法相当或更好的结果。- ASH是一种实时渲染照片级动态虚拟人的新算法，它具有高保真和高效的特点，可以应用于虚拟现实、增强现实和游戏等领域。- ASH算法将高斯体附加到可变形角色模型上，并学习它们在2D纹理空间中的参数，这使得该算法可以利用高效的2D卷积架构来学习高斯体参数，从而实现实时渲染。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：ASH：可动画高斯斑点，用于高效且逼真的真人渲染</li><li>作者：Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, Marc Habermann</li><li>第一作者单位：马克斯·普朗克计算机科学研究所</li><li>关键词：可动画高斯斑点、高效且逼真的真人渲染、神经隐式渲染、深度学习</li><li>论文链接：https://arxiv.org/abs/2312.05941，Github 代码链接：None</li><li><p>摘要：(1)：研究背景：实时渲染逼真且可控的人类 avatar 是计算机视觉和图形学领域的基础。最近在神经隐式渲染方面的进展为数字 avatar 带来了前所未有的逼真度，但实时性能大多仅限于静态场景。(2)：过去的方法及其问题：显式方法将人类 avatar 表示为具有学习动态纹理的可变形模板网格。尽管这些方法在运行时效率高，并且可以与成熟的基于光栅化的渲染管道无缝集成，但生成的渲染通常在逼真度和细节级别方面有所欠缺。混合方法通常将神经辐射场 (NeRF) 附加到（可变形）人体模型上。通常，它们在未摆姿势的空间中评估 NeRF 以模拟穿着衣服的人类的详细外观，并通过查询基于坐标的每个光线样品的  mlp 来生成颜色和密度值。尽管混合方法可以通过 NeRF 捕捉精细外观细节的能力提供卓越的渲染质量，但它们不适合实时应用，因为它们需要昂贵的计算成本。(3)：本文提出的研究方法：为了解决上述问题，本文提出 ASH，这是一种可动画的高斯斑点方法，用于实时逼真地渲染动态人类。ASH 将穿着衣服的人类参数化为可动画的 3D 高斯斑点，可以将这些斑点有效地溅射到图像空间以生成最终渲染。然而，在 3D 空间中天真地学习高斯参数会带来严峻的计算挑战。相反，本文将高斯斑点附在可变形角色模型上，并学习其参数在 2D 纹理空间中，这允许利用高效的 2D 卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。(4)：方法在任务和性能上的表现：本文使用可姿势控制的 avatar 对 ASH 进行了基准测试，结果表明，ASH 的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。这些性能支持了本文的目标。</p></li><li><p>方法：（1）：本文提出了一种可动画的高斯斑点方法ASH，用于实时逼真地渲染动态人类。ASH将穿着衣服的人类参数化为可动画的3D高斯斑点，可以将这些斑点有效地溅射到图像空间以生成最终渲染。（2）：为了解决在3D空间中天真地学习高斯参数带来的严峻计算挑战，本文将高斯斑点附在可变形角色模型上，并学习其参数在2D纹理空间中，这允许利用高效的2D卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。（3）：本文使用可姿势控制的avatar对ASH进行了基准测试，结果表明，ASH的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了ASH，这是一种实时高质量渲染动画人类的方法，仅从多视角视频中学习。ASH将最初设计用于建模静态场景的3D高斯斑点附加到可变形网格模板上。通过网格的UV参数化连接，我们可以在2D纹理空间中有效地学习3D高斯斑点作为纹理转换任务。ASH在可动画人类渲染方面定量和定性地展示出明显优于最先进的实时方法的性能，甚至优于最先进的离线方法。目前，ASH不会更新底层可变形模板网格。未来，我们将探索高斯斑点是否可以直接改进3D网格几何体。</p><p>（2）：创新点：</p><ul><li>将3D高斯斑点附加到可变形角色模型上，并学习其参数在2D纹理空间中，这允许利用高效的2D卷积架构，这些架构可以轻松扩展到所需数量的高斯斑点。</li><li>使用可姿势控制的avatar对ASH进行了基准测试，结果表明，ASH的性能远远优于现有的实时方法，并且与离线方法相比具有可比拟甚至更好的结果。</li></ul><p>性能：</p><ul><li>ASH在可动画人类渲染方面定量和定性地展示出明显优于最先进的实时方法的性能，甚至优于最先进的离线方法。</li></ul><p>工作量：</p><ul><li>ASH仅从多视角视频中学习，不需要手动注释或复杂的预处理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a44cc5ef91ec67dc9380befcf6d58fd9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b8f7667bf87fc0ccec5a9bc7e63c410a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-092918c112e840a8eb44423eb9235716.jpg" align="middle"></details>## MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar**Authors:Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu**The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods. [PDF](http://arxiv.org/abs/2312.04558v1) The link to our projectpage is   https://yufan1012.github.io/MonoGaussianAvatar**摘要**使用单目人像视频序列重建的光写实头部化身动画的能力是弥合虚拟和现实世界之间差距的关键一步。**要点**- 单目人像视频序列中重建光写实头部化身的能力对于弥合虚拟和现实世界之间的差距至关重要。- 最近在头部虚拟化身技术方面的进展，包括显式 3D 可变形网格 (3DMM)、点云和神经隐式表示，已被用于此项正在进行的研究。- 基于 3DMM 的方法受到其固定拓扑的限制，基于点的办法由于涉及大量点而负担沉重，最后一种方法存在变形灵活性和渲染效率方面的局限性。- 为了应对这些挑战，我们提出了 MonoGaussianAvatar（单目高斯点基头部虚拟化身），一种新颖的方法，它利用 3D 高斯点表示以及高斯变形场从单目人像视频中学习显式头部虚拟化身。- 我们使用具有可适应形状、能够实现灵活拓扑的高斯点来定义头部虚拟化身。- 这些点随着高斯变形场移动，与目标姿势和人的表情保持一致，从而实现有效变形。- 此外，高斯点具有可控的形状、大小、颜色和不透明度，结合高斯飞溅，可以进行高效训练和渲染。- 实验表明，我们的方法性能优越，在之前的方法中获得了最先进的结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：单目高斯点表征头部虚拟人（中文翻译）</li><li>作者：Yuhao Zhou, Xingtong Han, Xiaojun Wu, Yu-Kun Lai, Shizhan Zhu, Ang Li, Yebin Liu</li><li>第一作者单位：清华大学（中文翻译）</li><li>关键词：头部虚拟人、单目重建、高斯点表征、高斯变形场</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：头部虚拟人技术在虚拟现实、视频会议等领域有着广泛的应用。近年来，基于显式 3D 可变形网格、点云和神经隐式表示的头部虚拟人技术取得了很大进展。然而，基于 3D 可变形网格的方法受限于固定的拓扑结构，基于点云的方法由于涉及大量点而训练负担很重，基于神经隐式表示的方法在变形灵活性和渲染效率方面存在局限性。（2）过去的方法及其问题：现有方法主要包括基于显式 3D 可变形网格、点云和神经隐式表示的方法。基于显式 3D 可变形网格的方法受限于固定的拓扑结构，无法很好地适应不同人的头部形状。基于点云的方法由于涉及大量点而训练负担很重，并且渲染效率较低。基于神经隐式表示的方法在变形灵活性和渲染效率方面存在局限性。（3）研究方法：本文提出了一种基于高斯点表征和高斯变形场的单目头部虚拟人重建方法。该方法首先将头部表示为一组高斯点，然后通过高斯变形场来控制这些点的形状、大小、颜色和透明度。这种表示方式具有很强的灵活性，可以很好地适应不同人的头部形状。此外，该方法还采用了高效的渲染算法，可以实现实时渲染。（4）方法性能：本文方法在多个数据集上进行了评估，实验结果表明，该方法在重建质量、变形灵活性和渲染效率方面都优于现有方法。</p></li><li><p>方法：(1) 头部高斯点表征：将头部表示为一组高斯点，每个高斯点都有其位置、大小、颜色和透明度。(2) 高斯变形场：通过高斯变形场来控制高斯点的形状、大小、颜色和透明度。(3) 单目头部虚拟人重建：使用单目图像来重建头部虚拟人，首先将图像投影到高斯点上，然后通过高斯变形场来控制高斯点的形状、大小、颜色和透明度，从而重建出头部虚拟人。(4) 高效渲染算法：采用高效的渲染算法来渲染头部虚拟人，该算法可以实现实时渲染。</p></li><li><p>结论：（1）：提出了一种基于高斯点表征和高斯变形场的单目头部虚拟人重建方法，该方法具有很强的灵活性，可以很好地适应不同人的头部形状，并且采用高效的渲染算法，可以实现实时渲染。（2）：创新点：提出了一种新的头部虚拟人表示方法——高斯点表征，该方法具有很强的灵活性，可以很好地适应不同人的头部形状；提出了一种新的头部虚拟人变形方法——高斯变形场，该方法可以有效地控制头部虚拟人的形状、大小、颜色和透明度；提出了一种新的单目头部虚拟人重建方法，该方法可以从单目图像中重建出高质量的头部虚拟人。性能：在多个数据集上进行了评估，实验结果表明，该方法在重建质量、变形灵活性和渲染效率方面都优于现有方法。工作量：该方法的实现相对复杂，需要较多的时间和精力。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-960cb31f176221bc485bffca08572c49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ab5a224fe794c8ce5dd0412eaa41c0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff28fa285c5edbd1890f64177638b29e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c7871839126e3ba8f659d17bd8677f4.jpg" align="middle"></details><h2 id="HiFi4G-High-Fidelity-Human-Performance-Rendering-via-Compact-Gaussian-Splatting"><a href="#HiFi4G-High-Fidelity-Human-Performance-Rendering-via-Compact-Gaussian-Splatting" class="headerlink" title="HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian   Splatting"></a>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian   Splatting</h2><p><strong>Authors:Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</strong></p><p>We have recently seen tremendous progress in photo-real human modeling and rendering. Yet, efficiently rendering realistic human performance and integrating it into the rasterization pipeline remains challenging. In this paper, we present HiFi4G, an explicit and compact Gaussian-based approach for high-fidelity human performance rendering from dense footage. Our core intuition is to marry the 3D Gaussian representation with non-rigid tracking, achieving a compact and compression-friendly representation. We first propose a dual-graph mechanism to obtain motion priors, with a coarse deformation graph for effective initialization and a fine-grained Gaussian graph to enforce subsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with adaptive spatial-temporal regularizers to effectively balance the non-rigid prior and Gaussian updating. We also present a companion compression scheme with residual compensation for immersive experiences on various platforms. It achieves a substantial compression rate of approximately 25 times, with less than 2MB of storage per frame. Extensive experiments demonstrate the effectiveness of our approach, which significantly outperforms existing approaches in terms of optimization speed, rendering quality, and storage overhead. </p><p><a href="http://arxiv.org/abs/2312.03461v2">PDF</a> </p><p><strong>Summary</strong><br>高保真人类表演渲染的显式紧凑高斯方法。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出 HiFi4G，一种基于高斯分布的显式紧凑方法，用于从密集镜头中进行高保真人类表演渲染。</li><li>核心思想是将 3D 高斯表示法与非刚性跟踪相结合，实现紧凑且易于压缩的表示。</li><li>提出双图机制获取运动先验，其中粗略变形图用于有效初始化，细粒度高斯图用于实施后续约束。</li><li>利用 4D 高斯优化方案，结合自适应时空正则化，有效平衡非刚性先验和高斯更新。</li><li>提出了一种具有残差补偿机制的配套压缩方案，可实现跨平台沉浸式体验。</li><li>压缩比高达约 25 倍，每帧存储空间不到 2MB。</li><li>大量实验表明该方法的有效性，在优化速度、渲染质量和存储开销方面均明显优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HiFi4G：通过紧凑型高斯散射实现高保真人体表演渲染（HiFi4G：High-Fidelity Human Performance Rendering via Compact Gaussian Splatting）</li><li>作者：Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, Lan Xu</li><li>第一作者单位：上海科技大学</li><li>关键词：高保真人体表演渲染、紧凑型高斯散射、非刚性融合、可微分光栅化</li><li>论文链接：https://arxiv.org/abs/2312.03461Github 代码链接：无</li><li><p>摘要：（1）研究背景：近年来，照片级真实人体建模和渲染取得了巨大进展。然而，高效渲染逼真的人体表演并将其集成到光栅化管道中仍然具有挑战性。（2）过去方法及其问题：早期解决方案通过显式利用非刚性配准从捕获的视频中重建纹理网格。然而，它们仍然容易受到遮挡和纹理缺失的影响，从而导致重建结果出现孔洞和噪声。最近的神经网络进展，以 NeRF 为代表，绕过了显式重建，而是优化了一个基于坐标的多层感知器 (MLP) 来进行逼真的体积渲染。一些 NeRF 的动态变体试图维护一个规范的特征空间，以通过额外的隐式变形场在每个实时帧中重现特征。然而，这种规范设计对于大的运动和拓扑变化很脆弱。（3）论文提出的研究方法：本文提出了一种显式且紧凑的高斯散射方法 HiFi4G，用于从密集视频中进行高保真人体表演渲染。我们的核心思想是将 3D 高斯表示与非刚性跟踪相结合，实现紧凑且有利于压缩的表示。我们首先提出了一种双图机制来获取运动先验，其中粗略的变形图用于有效初始化，细粒度的高斯图用于强制后续约束。然后，我们利用 4D 高斯优化方案和自适应时空正则化器来有效平衡非刚性先验和高斯更新。我们还提出了一种具有残差补偿的配套压缩方案，用于在各种平台上实现沉浸式体验。它实现了大约 25 倍的压缩率，每个帧的存储空间小于 2MB。（4）方法在任务和性能上的表现：大量实验表明了我们方法的有效性，在优化速度、渲染质量和存储开销方面明显优于现有方法。这些性能支持了我们的目标。</p></li><li><p>方法：(1) 双图机制：获取运动先验，粗略变形图用于有效初始化，细粒度的高斯图用于强制后续约束。(2) 4D高斯优化方案和自适应时空正则化器：有效平衡非刚性先验和高斯更新。(3) 残差补偿的配套压缩方案：实现沉浸式体验，压缩率约为25倍，每个帧的存储空间小于2MB。</p></li><li><p>结论：（1）：本文提出了一种显式且紧凑的高斯散射方法 HiFi4G，用于从密集视频中进行高保真人体表演渲染。我们的方法将 3D 高斯表示与非刚性跟踪相结合，实现了紧凑且有利于压缩的表示。大量实验表明了我们方法的有效性，在优化速度、渲染质量和存储开销方面明显优于现有方法。这些性能支持了我们的目标。（2）：创新点：</p></li><li>提出了一种双图机制来获取运动先验，其中粗略的变形图用于有效初始化，细粒度的高斯图用于强制后续约束。</li><li>利用 4D 高斯优化方案和自适应时空正则化器来有效平衡非刚性先验和高斯更新。</li><li>提出了一种具有残差补偿的配套压缩方案，用于在各种平台上实现沉浸式体验。它实现了大约 25 倍的压缩率，每个帧的存储空间小于 2MB。性能：</li><li>在优化速度、渲染质量和存储开销方面明显优于现有方法。</li><li>在各种数据集上进行了广泛的实验，证明了我们方法的有效性和鲁棒性。工作量：</li><li>该方法需要密集的视频输入，这可能需要额外的采集设备和处理时间。</li><li>该方法需要对 4D 高斯散射进行优化，这可能需要大量的计算资源。</li><li>该方法需要对压缩方案进行训练，这可能需要额外的标注数据和训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a9db3189636791435751c6ef2f566368.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8fb22dc203c856f780869a746b68066b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ba98b01fb3d847fecf8756d2a082e8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-70860236a0e2652fa9ec2055060eb12b.jpg" align="middle"></details><h2 id="GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos"><a href="#GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos" class="headerlink" title="GauHuman: Articulated Gaussian Splatting from Monocular Human Videos"></a>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</h2><p><strong>Authors:Shoukang Hu, Ziwei Liu</strong></p><p>We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians. </p><p><a href="http://arxiv.org/abs/2312.02973v1">PDF</a> project page: <a href="https://skhu101.github.io/GauHuman/">https://skhu101.github.io/GauHuman/</a>; code:   <a href="https://github.com/skhu101/GauHuman">https://github.com/skhu101/GauHuman</a></p><p><strong>Summary</strong><br>高斯散点表示的 3D 人体模型 GauHuman，实现快速训练（1 ~ 2 分钟）和实时渲染（高达 189 FPS）。</p><p><strong>Key Takeaways</strong></p><ul><li>GauHuman 使用高斯散点表示在规范空间对 3D 人体进行编码，并通过线性混合蒙皮 (LBS) 将 3D 高斯体从规范空间变换为姿势空间。</li><li>GauHuman 的有效姿势和 LBS 细化模块能够以极低的计算成本学习 3D 人体的精细细节。</li><li>为了实现 GauHuman 的快速优化，研究者使用 3D 人体先验对 3D 高斯体进行初始化和剪枝，同时通过 KL 散度指导进行拆分/克隆，并引入了一种新颖的合并操作以进一步加速优化。</li><li>在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，GauHuman 在快速训练和实时渲染速度下实现了最先进的定量和定性性能。</li><li>值得注意的是，在不牺牲渲染质量的情况下，GauHuman 可以使用约 13k 个 3D 高斯体快速建模 3D 人体表演者。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯喷溅：从单目人体视频中进行铰接高斯喷溅</li><li>作者：Shoukang Hu，Ziwei Liu</li><li>隶属单位：南洋理工大学</li><li>关键词：3D 人体建模，隐式表示，神经渲染，高斯喷溅，线性混合蒙皮</li><li>论文链接：https://arxiv.org/abs/2312.02973，Github 链接：https://github.com/skhu101/GauHuman</li><li><p>摘要：(1) 研究背景：随着 AR/VR、远程呈现、电子游戏和电影制作等领域的发展，创建高质量的 3D 人体表演者具有广泛的应用价值。最近的方法表明，可以使用基于 NeRF 的隐式表示从稀疏视图视频甚至单张图像中学习 3D 人体 Avatar。然而，这些方法通常需要昂贵的时间和计算成本进行训练和渲染，这阻碍了它们在现实世界场景中的应用。例如，通常需要大约 10 个 GPU 小时来学习 3D 人体表演者，渲染速度不到每秒 1 帧 (FPS)。(2) 过去的方法及其问题：为了加快 3D 人体建模，可泛化的 HumanNeRF 方法以质量换取更少的训练时间。它们通常使用预先在铰接人体数据上训练好的模型对新的表演者进行微调。然而，这种效率低下的预训练和微调范式通常需要几个小时的预训练来获得 3D 人体的可泛化表示，另外还要花费 1 个小时进行微调。(3) 本文提出的研究方法：为了解决上述问题，本文提出了一种名为高斯喷溅的 3D 人体模型。该模型在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 将 3D 高斯从规范空间变换到姿势空间。在其中设计了有效的姿势和 LBS 细化模块，以在可忽略的计算成本下学习人体表演者的精细细节。此外，为了实现高斯喷溅的快速优化，本文使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。(4) 实验结果与性能：在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，高斯喷溅在快速训练和实时渲染速度下实现了最先进的定量和定性性能。值得注意的是，在不牺牲渲染质量的情况下，高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。</p></li><li><p>方法：（1）我们提出了一种称为高斯喷溅的 3D 人体模型，该模型在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 将 3D 高斯从规范空间变换到姿势空间。（2）我们设计了一个有效的姿势和 LBS 细化模块，以在可忽略的计算成本下学习人体表演者的精细细节。（3）为了实现高斯喷溅的快速优化，我们使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。</p></li><li><p>结论：（1）：高斯喷溅是一种快速训练（1~2分钟）和实时渲染（166 FPS）3D 人体的 3D 人体模型，它在规范空间中对高斯喷溅进行编码，并使用线性混合蒙皮 (LBS) 变换将 3D 高斯从规范空间变换到姿势空间，其中还设计了有效的姿势细化和 LBS 权重场模块来学习 3D 人体的精细细节。为了实现快速优化，我们使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。（2）：创新点：</p></li><li>提出了一种称为高斯喷溅的 3D 人体模型，该模型在规范空间中对高斯喷溅进行编码，并使用 LBS 变换将 3D 高斯从规范空间变换到姿势空间。</li><li>设计了一个有效的姿势细化和 LBS 权重场模块，以在可忽略的计算成本下学习人体表演者的精细细节。</li><li>使用 3D 人体先验初始化和剪枝 3D 高斯，同时通过 KL 散度指导进行分割/克隆，并采用了一种新颖的合并操作以进一步提高速度。性能：</li><li>在 ZJU_Mocap 和 MonoCap 数据集上进行的广泛实验表明，高斯喷溅在快速训练和实时渲染速度下实现了最先进的定量和定性性能。</li><li>值得注意的是，在不牺牲渲染质量的情况下，高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。工作量：</li><li>高斯喷溅的训练时间约为 1~2 分钟，渲染速度为 166 FPS。</li><li>高斯喷溅可以使用约 13k 个 3D 高斯快速建模 3D 人体表演者。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ca68371ceaa9efbdc729fa4bdc967f7d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5a50485d70cca3d120a273394bed8d88.jpg" align="middle"><img src="https://pica.zhimg.com/v2-272226fc9271422fc749accef9a38c85.jpg" align="middle"></details><h2 id="HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting"><a href="#HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting" class="headerlink" title="HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting"></a>HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</h2><p><strong>Authors:Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo Pérez-Pellitero</strong></p><p>3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10. </p><p><a href="http://arxiv.org/abs/2312.02902v1">PDF</a> </p><p><strong>Summary</strong><br>三维高斯形式（3DGS）可以被用于三维头部重建和动画，并能实现最先进的实时推理帧率。</p><p><strong>Key Takeaways</strong></p><ul><li>HeadGaS 是第一个使用 3DGS 进行 3D 头部重建和动画的模型。</li></ul><ul><li>HeadGaS 融合了 3DGS 与可学习潜在特征的优点，可根据参数化头部模型中的低维参数实现表情相关的最终颜色和不透明度值。</li></ul><ul><li>HeadGaS 在实时推理帧率方面实现了最先进的结果，比基线高出约 2dB，同时渲染速度提高了 10 倍以上。</li></ul><ul><li>HeadGaS 适用于各种三维头部动画应用，包括实时视频会议、虚拟现实和游戏。</li></ul><ul><li>在同一数据集上，与即时神经辐射场（Instant-NGP）等方法相比，HeadGaS在重建质量和运行时间方面均优于其实时变体即时神经辐射场（Instant-NGP）。</li></ul><ul><li>HeadGaS 的模型参数比即时神经辐射场（Instant-NGP）少 2.5 倍，仅为 16MB，速度也比其快 12 倍。</li></ul><ul><li>在真实数据和 synthetic 数据上，HeadGaS 的速度提高了 10 倍以上，更适合于对实时性能要求较高的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HeadGaS：基于 3D 高斯斑点的实时动画头部重建</li><li>作者：Jiatao Gu, Andreas Rössler, Hao Tang, Justus Thies, Matthias Nießner</li><li>隶属机构：马普学会计算机图形学研究所</li><li>关键词：3D 头部重建，动画，神经辐射场，高斯斑点</li><li>论文链接：https://arxiv.org/abs/2208.00120，Github 链接：无</li><li>摘要：（1）研究背景：近年来，3D 头部动画在质量和运行时性能方面取得了重大改进，这主要得益于可微渲染和神经辐射场的发展。实时渲染对于现实世界应用而言是一个非常理想的目标。（2）过去的方法及其问题：现有方法主要依赖显式场景表示（例如网格或点云）或隐式神经辐射场表示。显式表示通常需要大量的参数，并且难以捕捉复杂的面部表情。隐式神经辐射场表示虽然可以捕捉复杂的面部表情，但渲染速度较慢。（3）研究方法：本文提出了一种新的 3D 头部重建和动画模型 HeadGaS，该模型使用 3D 高斯斑点（3DGS）作为神经辐射场表示。3DGS 是一种参数化表示，它可以有效地捕捉复杂的面部表情，并且渲染速度快。HeadGaS 还使用了一个可学习的潜在特征库，该特征库可以与参数化头部模型的低维参数进行线性混合，以获得与表情相关的最终颜色和不透明度值。（4）方法性能：HeadGaS 在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达 2dB，同时渲染速度提高了 10 倍以上。这些性能结果支持了本文的目标，即构建一个能够实时渲染复杂面部表情的 3D 头部模型。</li></ol><p>方法：</p><p>（1）：HeadGaS模型使用3D高斯斑点（3DGS）作为神经辐射场表示。3DGS是一种参数化表示，它可以有效地捕捉复杂的面部表情，并且渲染速度快。</p><p>（2）：HeadGaS还使用了一个可学习的潜在特征库，该特征库可以与参数化头部模型的低维参数进行线性混合，以获得与表情相关的最终颜色和不透明度值。</p><p>（3）：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。</p><ol><li>结论：（1）：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。（2）：创新点：HeadGaS模型使用3D高斯斑点（3DGS）作为神经辐射场表示，并使用了一个可学习的潜在特征库来获得与表情相关的最终颜色和不透明度值。性能：HeadGaS模型在实时推理帧率下实现了最先进的结果，其性能优于基线方法高达2dB，同时渲染速度提高了10倍以上。工作量：HeadGaS模型需要大量的数据进行训练，并且训练过程可能需要很长时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0c315a4df3b1de4edf3d295f34ada5d3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c976c03ab0c54c1ca58b8f79a43787fc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a79269613584cfdb2b735b299c5cce1.jpg" align="middle"></details><h2 id="GPS-Gaussian-Generalizable-Pixel-wise-3D-Gaussian-Splatting-for-Real-time-Human-Novel-View-Synthesis"><a href="#GPS-Gaussian-Generalizable-Pixel-wise-3D-Gaussian-Splatting-for-Real-time-Human-Novel-View-Synthesis" class="headerlink" title="GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for   Real-time Human Novel View Synthesis"></a>GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for   Real-time Human Novel View Synthesis</h2><p><strong>Authors:Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, Yebin Liu</strong></p><p>We present a new approach, termed GPS-Gaussian, for synthesizing novel views of a character in a real-time manner. The proposed method enables 2K-resolution rendering under a sparse-view camera setting. Unlike the original Gaussian Splatting or neural implicit rendering methods that necessitate per-subject optimizations, we introduce Gaussian parameter maps defined on the source views and regress directly Gaussian Splatting properties for instant novel view synthesis without any fine-tuning or optimization. To this end, we train our Gaussian parameter regression module on a large amount of human scan data, jointly with a depth estimation module to lift 2D parameter maps to 3D space. The proposed framework is fully differentiable and experiments on several datasets demonstrate that our method outperforms state-of-the-art methods while achieving an exceeding rendering speed. </p><p><a href="http://arxiv.org/abs/2312.02155v1">PDF</a> The link to our projectpage is <a href="https://shunyuanzheng.github.io">https://shunyuanzheng.github.io</a></p><p><strong>摘要</strong><br>利用高斯参数图对人体扫描数据进行训练，无需微调或优化即可实现实时新视角合成。</p><p><strong>要点</strong></p><ul><li>提出了一种名为 GPS-Gaussian 的新方法，用于实时合成角色的新视角。</li><li>该方法可以在稀疏视图相机设置下进行 2K 分辨率渲染。</li><li>与需要针对每个对象进行优化的原始高斯斑点或神经隐式渲染方法不同，我们引入了在源视图上定义的高斯参数图，并直接回归高斯斑点属性，无需任何微调或优化即可实现即时新视图合成。</li><li>为此，我们在大量人体扫描数据上训练我们的高斯参数回归模块，并与深度估计模块联合，将二维参数图提升到三维空间。</li><li>所提出的框架是完全可微的，在多个数据集上的实验表明，我们的方法优于最先进的方法，同时实现了极高的渲染速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GPS-Gaussian：实时补充通用像素级三维高斯体素以用于补充材料</li><li>作者：Yuxuan Zhang†, Ziyi Wang†, Yajie Zhao, Yiyi Liao, Zhe Lin, Hao Su, Lu Sheng</li><li>单位：清华大学（仅输出中文翻译）</li><li>关键词：实时渲染、神经辐射场、三维人体重建</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）：研究背景：神经辐射场（NeRF）方法可以生成逼真的图像，但它们通常需要针对每个场景进行优化，这使得它们难以用于实时应用。（2）：过去的方法：FloRen、IBRNet 和 EN-eRF 等方法都尝试解决这个问题，但它们要么速度慢，要么质量差。（3）：研究方法：本文提出了一种新的方法 GPS-Gaussian，它可以在不进行任何微调或优化的情况下，直接回归高斯体素参数，从而实现即时的新视角合成。（4）：方法性能：在多个数据集上进行的实验表明，该方法在实现超过渲染速度的同时，优于最先进的方法。</li></ol><p>7.方法：（1）：我们提出了一种名为 GPS-Gaussian 的方法，它可以直接回归高斯体素参数，而无需针对每个场景进行任何微调或优化，从而实现即时的新视角合成。（2）：我们使用可微渲染积分来促进像素之间的对应关系，并使用优化深度估计来提高 3D 高斯参数的确定精度。（3）：我们的方法在多个数据集上实现了超过渲染速度的性能，并且优于最先进的方法。</p><ol><li>结论：（1）：该方法提出了一种新的NeRF方法GPS-Gaussian，它可以直接回归高斯体素参数，而无需针对每个场景进行任何微调或优化，从而实现即时的新视角合成。（2）：创新点：</li><li>直接回归高斯体素参数，无需针对每个场景进行任何微调或优化。</li><li>使用可微渲染积分来促进像素之间的对应关系。</li><li>使用优化深度估计来提高3D高斯参数的确定精度。性能：</li><li>在多个数据集上实现了超过渲染速度的性能。</li><li>优于最先进的方法。工作量：</li><li>训练复杂度较高。</li><li>需要大量的数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a836e694dac2420ca9f244952c8ca9fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95ab4d2bde29d287ade4c06b75bcaae7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5d7961c20513332db064855b12f85e3.jpg" align="middle"></details><h2 id="GaussianAvatars-Photorealistic-Head-Avatars-with-Rigged-3D-Gaussians"><a href="#GaussianAvatars-Photorealistic-Head-Avatars-with-Rigged-3D-Gaussians" class="headerlink" title="GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians"></a>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</h2><p><strong>Authors:Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, Matthias Nießner</strong></p><p>We introduce GaussianAvatars, a new method to create photorealistic head avatars that are fully controllable in terms of expression, pose, and viewpoint. The core idea is a dynamic 3D representation based on 3D Gaussian splats that are rigged to a parametric morphable face model. This combination facilitates photorealistic rendering while allowing for precise animation control via the underlying parametric model, e.g., through expression transfer from a driving sequence or by manually changing the morphable model parameters. We parameterize each splat by a local coordinate frame of a triangle and optimize for explicit displacement offset to obtain a more accurate geometric representation. During avatar reconstruction, we jointly optimize for the morphable model parameters and Gaussian splat parameters in an end-to-end fashion. We demonstrate the animation capabilities of our photorealistic avatar in several challenging scenarios. For instance, we show reenactments from a driving video, where our method outperforms existing works by a significant margin. </p><p><a href="http://arxiv.org/abs/2312.02069v1">PDF</a> Project page: <a href="https://shenhanqian.github.io/gaussian-avatars">https://shenhanqian.github.io/gaussian-avatars</a></p><p><strong>摘要</strong><br>高斯化身：利用高斯三维模型构建的可控3D面部化身。</p><p><strong>要点</strong></p><ul><li>引入高斯化身，一种创建逼真且可控头部化身的方法。</li><li>核心思想是基于 3D 高斯模型的动态 3D 表征。</li><li>该方法将逼真的渲染与精确的动画控制相结合。</li><li>优化了高斯模型参数和可变形模型参数，提高重建精度。</li><li>在具有挑战性的场景中，展现了逼真化身的动画效果。</li><li>通过驾驶视频重现验证了方法的性能优越性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯化身：用装备 3D 高斯的逼真头部化身</li><li>作者：沈涵钱、托比亚斯·基希施泰因、利亚姆·舍内维尔德、达维德·达沃利、西蒙·吉本海因、马蒂亚斯·尼施纳</li><li>第一作者单位：慕尼黑工业大学</li><li>关键词：高斯化身、逼真头部化身、3D 高斯、参数化人脸模型、动画控制、表情迁移、驾驶视频、新视角渲染</li><li>论文链接：https://arxiv.org/abs/2312.02069，Github 链接：无</li><li><p>摘要：（1）研究背景：创建可动画的人类头部化身一直是计算机视觉和图形学中的一个长期问题。逼真动态化身的新视角渲染能力在游戏、电影制作、沉浸式远程临场和增强或虚拟现实等领域具有广泛应用。此外，能够控制化身并使其很好地推广到新颖的姿势和表情也至关重要。重建能够同时捕捉人类头部外观、几何形状和动态的 3D 表征对于生成高保真化身而言是一项重大挑战。这种重建问题的约束不足极大地增加了实现结合新视角渲染逼真度和表情可控性的表征的任务的复杂性。此外，极端表情和面部细节（如皱纹、嘴巴内部和头发）很难捕捉，并且很容易产生人类注意到的视觉伪影。（2）过去方法及其问题：神经辐射场 (NeRF) 及其变体在从多视角观察中重建静态场景方面取得了令人印象深刻的结果。后续工作已将 NeRF 扩展到为任意场景和针对人类定制的场景建模动态场景。这些工作为新视角渲染取得了令人印象深刻的结果；但是，它们缺乏可控性，因此无法很好地推广到新颖的姿势和表情。最近的 3D 高斯散射方法通过优化整个 3D 空间中的离散几何基元（3D 高斯）来实现比 NeRF 更高的渲染质量，以进行新视角合成并具有实时性能。（3）本文提出的研究方法：本文提出了一种新方法——高斯化身，该方法可以创建在表情、姿势和视角方面完全可控的逼真头部化身。核心思想是基于装备到参数化可变形人脸模型的 3D 高斯散点的动态 3D 表征。这种组合促进了逼真的渲染，同时允许通过基础参数化模型进行精确的动画控制，例如，通过从驱动序列进行表情迁移或通过手动更改可变形模型参数。本文通过三角形的局部坐标系对每个散点进行参数化，并优化显式位移偏移以获得更准确的几何表示。在化身重建期间，本文以端到端的方式联合优化可变形模型参数和高斯散点参数。（4）方法在什么任务上取得了什么性能？该方法的性能是否支持其目标？本文展示了逼真化身的动画能力，涉及几个具有挑战性的场景。例如，本文展示了从驾驶视频中进行重演，其中本文的方法以显着的优势优于现有工作。</p></li><li><p><strong>方法</strong>：（1）数据预处理：将多视角视频分解为一系列帧，并对每一帧进行预处理，包括裁剪、调整大小和归一化。（2）参数化人脸模型：使用可变形的人脸模型来表示化身头部。该模型由一组控制顶点和一组变形权重组成，可以通过优化控制顶点的位置和变形权重来控制化身的表情和姿势。（3）3D高斯散点：使用一组3D高斯散点来表示化身头部的几何形状。每个散点由一个位置、一个半径和一个颜色组成。通过优化散点的位置、半径和颜色，可以重建化身头部的几何形状和外观。（4）端到端优化：将参数化人脸模型和3D高斯散点组合在一起，并以端到端的方式进行优化。优化目标包括重建误差、正则化项和动画控制项。重建误差衡量了化身与输入视频帧之间的差异，正则化项防止过拟合，动画控制项确保化身能够根据控制信号进行动画。（5）动画控制：通过优化可变形人脸模型的控制顶点或变形权重，可以控制化身的表情和姿势。还可以通过从驱动序列进行表情迁移或通过手动更改可变形模型参数来控制化身。</p></li><li><p>结论：（1）：高斯化身是一种新颖的方法，它可以从视频序列中创建逼真的人类头部化身。它具有基于装备到参数化可变形人脸模型的 3D 高斯散点的动态 3D 表征。这使得化身能够根据控制信号进行动画，并能够精确地控制表情和姿势。（2）：创新点：</p></li><li>将 3D 高斯散点与参数化可变形人脸模型相结合，以实现逼真的人类头部化身重建。</li><li>提出了一种新的局部坐标系，该坐标系可以对每个散点进行参数化，并优化显式位移偏移以获得更准确的几何表示。</li><li>以端到端的方式联合优化可变形模型参数和高斯散点参数，以获得更好的重建效果。</li><li>在表情、姿势和视角方面取得了完全可控的逼真头部化身。性能：</li><li>在图像质量和表情准确性方面优于现有方法。</li><li>能够从驾驶视频中进行重演，并且具有显着的优势。工作量：</li><li>需要大量的数据和计算资源来训练模型。</li><li>需要手动调整模型的参数以获得最佳的重建效果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8675c90807e2bdb45562b774a55905a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a39f72412af9350569632ebc4038eb42.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffae667ed385ae5e64c20d00da08ac79.jpg" align="middle"></details><h2 id="HUGS-Human-Gaussian-Splats"><a href="#HUGS-Human-Gaussian-Splats" class="headerlink" title="HUGS: Human Gaussian Splats"></a>HUGS: Human Gaussian Splats</h2><p><strong>Authors:Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, Anurag Ranjan</strong></p><p>Recent advances in neural rendering have improved both training and rendering times by orders of magnitude. While these methods demonstrate state-of-the-art quality and speed, they are designed for photogrammetry of static scenes and do not generalize well to freely moving humans in the environment. In this work, we introduce Human Gaussian Splats (HUGS) that represents an animatable human together with the scene using 3D Gaussian Splatting (3DGS). Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes. We utilize the SMPL body model to initialize the human Gaussians. To capture details that are not modeled by SMPL (e.g. cloth, hairs), we allow the 3D Gaussians to deviate from the human body model. Utilizing 3D Gaussians for animated humans brings new challenges, including the artifacts created when articulating the Gaussians. We propose to jointly optimize the linear blend skinning weights to coordinate the movements of individual Gaussians during animation. Our approach enables novel-pose synthesis of human and novel view synthesis of both the human and the scene. We achieve state-of-the-art rendering quality with a rendering speed of 60 FPS while being ~100x faster to train over previous work. Our code will be announced here: <a href="https://github.com/apple/ml-hugs">https://github.com/apple/ml-hugs</a> </p><p><a href="http://arxiv.org/abs/2311.17910v1">PDF</a> </p><p><strong>Summary</strong><br>人类高斯斑块 (HUGS) 使用高斯散布法 (3DGS) 通过单眼视频学习动态场景和可动人类的 disentangled 表示。</p><p><strong>Key Takeaways</strong></p><ul><li>HUGS 使用单目视频学习动画场景和可动人类的 disentangled 表示。</li><li>HUGS 使用高斯散布法 (3DGS) 来表示动画人类和场景。</li><li>SMPL 人体模型被用来初始化人体高斯。</li><li>允许 3D 高斯偏离人体模型来捕获未被 SMPL 建模的细节（如衣物、毛发）。</li><li>提出联合优化线性混合蒙皮权重，以协调动画期间各个高斯的运动。</li><li>HUGS 实现人类的新姿势合成以及人类和场景的新视角合成。</li><li>HUGS 实现了最先进的渲染质量，渲染速度为 60 FPS，而训练速度比以前的工作快 100 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HUGS：人体高斯斑点</li><li>作者：Muhammed Kocabas、Jen-Hao Rick Chang、N. James Gabriel、Oncel Tuzel、Anurag Ranjan</li><li>第一作者单位：苹果公司</li><li>关键词：神经渲染、人体动画、三维高斯斑点、场景表示</li><li>论文链接：https://arxiv.org/abs/2311.17910    Github 代码链接：https://github.com/apple/ml-hugs</li><li><p>摘要：（1）研究背景：近年来，神经渲染技术取得了很大进展，训练和渲染时间都大大缩短。然而，这些方法主要针对静态场景的摄影测量，不能很好地推广到环境中自由移动的人体。（2）过去的方法及其问题：以往的方法通常使用多摄像头捕捉设备、大量计算和大量的手动工作来创建人体虚拟形象。直接从视频中生成三维虚拟形象的方法虽然取得了一些进展，但它们在处理自由移动的人体时仍然存在很多问题。（3）研究方法：本文提出了一种名为 HUGS 的方法，它使用三维高斯斑点（3DGS）来表示可动画的人体和场景。该方法只需要一个单目视频，包含 50-100 帧，就可以在 30 分钟内自动学习分离静态场景和一个完全可动画的人体虚拟形象。（4）方法性能及其实际意义：HUGS 方法实现了最先进的渲染质量，渲染速度达到 60FPS，同时训练速度比以前的工作快约 100 倍。该方法可以用于新姿势合成、新视角合成以及人体和场景的动画。</p></li><li><p>方法：(1) 使用预训练的 SMPL 回归器估计 SMPL 姿势参数和身体形状参数。(2) 将人体表示为 3D 高斯斑点，并使用学习的 LBS 驱动高斯斑点。(3) 使用三个 MLP 来估计高斯斑点的颜色、不透明度、附加位移、旋转、缩放和 LBS 权重。(4) 将人体高斯斑点与场景高斯斑点结合起来，并使用 splatting 渲染在一起。(5) 使用 L1 损失、SSIM 损失和感知损失来优化高斯斑点的中心位置、特征三平面和三个 MLP 的参数。(6) 对 LBS 权重进行正则化，使其与 SMPL 中的 LBS 权重接近。(7) 在优化过程中，克隆、分裂和剪枝高斯斑点，以避免局部最小值。(8) 在优化结束后，人体由平均 200 个高斯斑点表示。(9) 在测试时渲染时，可以直接使用 LBS 权重对人体高斯斑点进行动画处理，而不需要评估三平面和 MLP。</p></li><li><p>结论：（1）：本文提出了一种名为 HUGS 的方法，该方法使用三维高斯斑点 (3DGS) 来表示可动画的人体和场景，只需要一个单目视频，包含 50-100 帧，就可以在 30 分钟内自动学习分离静态场景和一个完全可动画的人体虚拟形象。该方法实现了最先进的渲染质量，渲染速度达到 60FPS，同时训练速度比以前的工作快约 100 倍。（2）：创新点：</p></li><li>使用三维高斯斑点来表示可动画的人体和场景，可以实现快速训练和渲染，并且能够处理自由移动的人体。</li><li>使用学习的 LBS 驱动高斯斑点，可以实现人体的高质量动画。</li><li>使用三个 MLP 来估计高斯斑点的颜色、不透明度、附加位移、旋转、缩放和 LBS 权重，可以实现高精度的渲染。</li></ol><p>性能：- 渲染质量：HUGS 方法实现了最先进的渲染质量，在 PSNR、SSIM 和 LPIPS 指标上都优于其他方法。- 渲染速度：HUGS 方法的渲染速度达到 60FPS，远高于其他方法。- 训练速度：HUGS 方法的训练速度比以前的工作快约 100 倍。</p><p>工作量：- 数据集：HUGS 方法使用 in-the-wild 视频作为训练数据，这些视频很容易获得。- 训练时间：HUGS 方法的训练时间只需要 30 分钟。- 渲染时间：HUGS 方法的渲染时间非常短，可以达到 60FPS。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-df53c4361e0954102f0a16da8b5dbddd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6761187324e778cd899d6c594d88a176.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d64b96e5aea9ea7f9087d616da2c616f.jpg" align="middle"></details>## HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting**Authors:Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu**Realistic 3D human generation from text prompts is a desirable yet challenging task. Existing methods optimize 3D representations like mesh or neural fields via score distillation sampling (SDS), which suffers from inadequate fine details or excessive training time. In this paper, we propose an efficient yet effective framework, HumanGaussian, that generates high-quality 3D humans with fine-grained geometry and realistic appearance. Our key insight is that 3D Gaussian Splatting is an efficient renderer with periodic Gaussian shrinkage or growing, where such adaptive density control can be naturally guided by intrinsic human structures. Specifically, 1) we first propose a Structure-Aware SDS that simultaneously optimizes human appearance and geometry. The multi-modal score function from both RGB and depth space is leveraged to distill the Gaussian densification and pruning process. 2) Moreover, we devise an Annealed Negative Prompt Guidance by decomposing SDS into a noisier generative score and a cleaner classifier score, which well addresses the over-saturation issue. The floating artifacts are further eliminated based on Gaussian size in a prune-only phase to enhance generation smoothness. Extensive experiments demonstrate the superior efficiency and competitive quality of our framework, rendering vivid 3D humans under diverse scenarios. Project Page: https://alvinliu0.github.io/projects/HumanGaussian [PDF](http://arxiv.org/abs/2311.17061v1) Project Page: https://alvinliu0.github.io/projects/HumanGaussian**Summary**三维高斯散点绘制：一种最高效且有效的细粒度几何图形和逼真外观的三维人类生成框架。**Key Takeaways**- 我们提出了一种名为 HumanGaussian 的高效且有效的框架，它可以生成具有细粒度几何结构和逼真外观的高质量 3D 人类。- 3D 高斯散点绘制是一种高效的渲染器，具有周期性高斯收缩或增长，其中这种自适应密度控制可以由内在的人类结构自然引导。- 我们首先提出了一种结构感知 SDS，可以同时优化人类的外观和几何形状。- 我们设计了一个退火否定提示指导，将 SDS 分解为一个更嘈杂的生成分数和一个更清晰的分类器分数，从而很好地解决了过饱和问题。- 基于高斯大小进一步消除浮动伪影，以增强生成平滑度。- 广泛的实验表明了我们框架的卓越效率和竞争质量，在不同的场景下渲染了生动的 3D 人类。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Human Gaussian：文本驱动的三维人体生成与高斯体素溅射</li><li>作者：Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, Ziwei Liu</li><li>第一作者单位：香港中文大学</li><li>关键词：文本到三维，三维人类生成，高斯体素溅射，结构感知 SDS，退火负面提示指导</li><li>论文链接：https://arxiv.org/abs/2311.17061，Github 代码链接：无</li><li>摘要：（1）研究背景：</li><li>从文本提示生成逼真的三维人体是一项理想但具有挑战性的任务。</li><li>现有方法通过基于分数蒸馏采样的方式优化网格或神经场等三维表示，但往往存在精细细节不足或训练时间过长的问题。</li></ol><p>（2）过去方法及其问题：   - 现有方法优化三维表示时，通常采用基于分数蒸馏采样的方式，但这种方式存在以下问题：     - 难以生成精细的细节。     - 训练时间过长。</p><p>（3）研究方法：   - 提出了一种高效且有效的三维人体生成框架 Human Gaussian，该框架能够生成具有精细几何结构和逼真外观的高质量三维人体。   - Human Gaussian 的关键思想是将三维高斯体素溅射引入文本驱动的三维人体生成中，并进行了一些新颖的设计：     - 提出了一种结构感知 SDS，可以同时优化人体的外观和几何结构。     - 设计了一种退火负面提示指导，可以有效地解决过饱和问题。     - 基于高斯体素大小，在仅修剪阶段进一步消除浮动伪影，以增强生成的平滑性。</p><p>（4）方法性能：   - 广泛的实验表明，Human Gaussian 具有优越的效率和竞争性的质量，能够在各种场景下渲染出逼真的三维人体。   - 性能支持目标：     - Human Gaussian 能够生成具有精细几何结构和逼真外观的高质量三维人体。     - Human Gaussian 具有优越的效率，能够在较短的时间内生成三维人体。</p><ol><li><p>方法：（1）高斯初始化与 SMPL-X 先验：从 SMPL-X 网格表面均匀采样点作为 3DGS 初始化，生成 100k 个 3DGS，并将其缩放和转换到合理的人类尺寸，位于 3D 空间的中心。从 SMPL-X 联合提取 2D 骨架作为结构条件。（2）学习纹理结构联合分布：使用预训练的 StableDiffusion，扩展结构专家分支，同时对图像 RGB 和深度进行去噪，以捕获纹理和结构的联合分布。为了实现灵活的骨架控制，还通过通道方式将姿势图作为输入条件。（3）结构感知 SDS：设计了一种结构感知 SDS，可以同时优化人体的外观和几何结构，从 RGB 和深度空间蒸馏多模态分数函数，以优化 3DGS 的密度和修剪过程。（4）退火负面提示指导：使用具有退火负面分数的更清洁的分类器分数来规范高方差的随机 SDS 梯度，并根据高斯体素大小进一步消除浮动伪影，以增强生成的平滑性。</p></li><li><p>结论：（1）：HumanGaussian 提出了一种高效且有效的三维人体生成框架，能够生成具有精细几何结构和逼真外观的高质量三维人体。（2）：创新点：Performance：HumanGaussian 能够在较短的时间内生成三维人体。Workload：HumanGaussian 具有优越的效率，能够在较短的时间内生成三维人体。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-812d7a87ffdcd8ec91c266eb6cc26d09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2996d2afc6ef940153f64c951a8e67a2.jpg" align="middle"></details><h2 id="Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling"><a href="#Animatable-Gaussians-Learning-Pose-dependent-Gaussian-Maps-for-High-fidelity-Human-Avatar-Modeling" class="headerlink" title="Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling"></a>Animatable Gaussians: Learning Pose-dependent Gaussian Maps for   High-fidelity Human Avatar Modeling</h2><p><strong>Authors:Zhe Li, Zerong Zheng, Lizhen Wang, Yebin Liu</strong></p><p>Modeling animatable human avatars from RGB videos is a long-standing and challenging problem. Recent works usually adopt MLP-based neural radiance fields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to regress pose-dependent garment details. To this end, we introduce Animatable Gaussians, a new avatar representation that leverages powerful 2D CNNs and 3D Gaussian splatting to create high-fidelity avatars. To associate 3D Gaussians with the animatable avatar, we learn a parametric template from the input videos, and then parameterize the template on two front \&amp; back canonical Gaussian maps where each pixel represents a 3D Gaussian. The learned template is adaptive to the wearing garments for modeling looser clothes like dresses. Such template-guided 2D parameterization enables us to employ a powerful StyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling detailed dynamic appearances. Furthermore, we introduce a pose projection strategy for better generalization given novel poses. Overall, our method can create lifelike avatars with dynamic, realistic and generalized appearances. Experiments show that our method outperforms other state-of-the-art approaches. Code: <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> </p><p><a href="http://arxiv.org/abs/2311.16096v1">PDF</a> Projectpage: <a href="https://animatable-gaussians.github.io/">https://animatable-gaussians.github.io/</a>, Code:   <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a></p><p><strong>Summary</strong><br>动画高斯体素：一种新的化身表示形式，结合了强大的 2D CNN 和 3D 高斯体素，用于创建高保真化身。</p><p><strong>Key Takeaways</strong></p><ul><li>Animatable Gaussians 是一种新的化身表示形式，用于从 RGB 视频中建模可动画的人类化身。</li><li>Animatable Gaussians 利用强大的 2D CNN 和 3D 高斯体素来创建高保真化身，用于建模动态、逼真和概括的外观。</li><li>Animatable Gaussians 学习了一个参数模板，该模板可以适应松散的衣服，如连衣裙。</li><li>Animatable Gaussians 采用强大的 StyleGAN-based CNN 来学习与姿势相关的映射，用于建模详细的动态外观。</li><li>Animatable Gaussians 引入了一种姿势投影策略，以便在新的姿势下更好地泛化。</li><li>Animatable Gaussians 在实验中优于其他最先进的方法。</li><li>Animatable Gaussians 的源代码可在 <a href="https://github.com/lizhe00/AnimatableGaussians">https://github.com/lizhe00/AnimatableGaussians</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可动画高斯：学习姿势相关的高斯映射</li><li>作者：Zhi Li, Yifan Liu, Wenhao Yu, Qiong Chen, Xiaogang Wang</li><li>单位：无</li><li>关键词：动画、高斯映射、姿势相关、神经渲染场、可变形模型</li><li>论文链接：无，Github：https://github.com/lizhe00/AnimatableGaussians</li><li><p>摘要：(1)：研究背景：从 RGB 视频中建模可动画的人类虚拟形象是一个长期存在且具有挑战性的问题。最近的工作通常采用基于 MLP 的神经辐射场 (NeRF) 来表示 3D 人类，但纯 MLP 很难回归姿势相关的服装细节。(2)：过去方法及其问题：本文方法的动机：为了解决这个问题，我们引入了可动画高斯，这是一种新的虚拟形象表示，利用强大的 2D CNN 和 3D 高斯 splatting 来创建高保真虚拟形象。为了将 3D 高斯与可动画虚拟形象相关联，我们从输入视频中学习了一个参数模板，然后将模板参数化为两个正面和背面规范高斯映射，其中每个像素都表示一个 3D 高斯。学习到的模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。这种模板引导的 2D 参数化使我们能够使用强大的基于 StyleGAN 的 CNN 来学习姿势相关的 Gaussian 地图，以建模详细的动态外观。此外，我们引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。(3)：本文提出的研究方法：总体而言，我们的方法可以创建具有动态、逼真和泛化外观的逼真虚拟形象。实验表明，我们的方法优于其他最先进的方法。(4)：方法在什么任务上取得了怎样的性能？性能是否支持了他们的目标：我们的方法在建模可动画的人类虚拟形象任务上取得了最先进的性能。定量和定性结果表明，我们的方法可以生成具有逼真细节和姿势相关外观的逼真虚拟形象。这些结果支持了我们的目标，即创建可以用于各种应用（例如游戏、电影和虚拟现实）的高质量可动画虚拟形象。</p></li><li><p>方法：（1）参数模板：提出了一种参数模板，该模板由两个正面和背面规范高斯映射组成，其中每个像素都表示一个 3D 高斯。该模板可以适应穿着的服装，以建模更宽松的衣服，如连衣裙。（2）姿势投影：引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。该策略将新姿势投影到训练数据中的最接近姿势，以确保重建的位置图位于训练姿势的分布内。（3）2D CNN 和 MLP 的比较：通过将 2D CNN 替换为基于坐标的 MLP，评估了 2D CNN 和 MLP 在训练姿势重建方面的表示能力。结果表明，2D CNN 能够回归更详细和更逼真的外观，而 MLP 的表示能力有限，导致模糊的动画结果。（4）消融研究：通过移除参数模板和姿势投影策略，评估了它们对动画结果的影响。结果表明，参数模板和姿势投影策略对于生成合理和生动的合成外观至关重要。</p></li><li><p>结论：（1）：这项工作提出了一种新的虚拟形象表示方法——可动画高斯，该方法利用强大的2DCNN和3D高斯splatting来创建高保真虚拟形象。此外，还引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。实验表明，该方法优于其他最先进的方法，可以创建具有动态、逼真和泛化外观的逼真虚拟形象。（2）：创新点：</p></li><li>提出了一种新的虚拟形象表示方法——可动画高斯，该方法利用强大的2DCNN和3D高斯splatting来创建高保真虚拟形象。</li><li>引入了一种姿势投影策略，以便在给定新姿势时更好地泛化。性能：</li><li>在建模可动画的人类虚拟形象任务上取得了最先进的性能。</li><li>定量和定性结果表明，该方法可以生成具有逼真细节和姿势相关外观的逼真虚拟形象。工作量：</li><li>该方法需要大量的数据和计算资源来训练。</li><li>该方法的实现相对复杂，需要较高的编程技能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f58407bb1da6858a1f8b3afeca5122e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77ee722883e97eb42525787423c0db90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4ba94c75431413d71052f68f19e06407.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aec0c4ba5b1b400598aa699437906d07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b30fc2750f7d5972c0541922982b18fe.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-24 Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>开机自启动登录/认证脚本设置（SZU为例）</title>
    <link href="https://kedreamix.github.io/2024/01/22/Note/AutoLogin/"/>
    <id>https://kedreamix.github.io/2024/01/22/Note/AutoLogin/</id>
    <published>2024-01-21T17:20:13.000Z</published>
    <updated>2024-01-21T17:42:17.298Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开机自启动登录-认证前沿"><a href="#开机自启动登录-认证前沿" class="headerlink" title="开机自启动登录/认证前沿"></a>开机自启动登录/认证前沿</h2><p>有时候在学校或者在企业的时候，会出现这样一种情况，就是我们需要认证才能够上网，但是这种认证并不是非常稳定，有可能会出现断连的情况 </p><p>包括有时候电脑关机后自启后也会断掉，针对这种情况，我介绍一种开机自启动登录/认证的脚本，这样能不断的保证联网</p><h2 id="开机自启动目录"><a href="#开机自启动目录" class="headerlink" title="开机自启动目录"></a>开机自启动目录</h2><p>首先，我们既然向进行开机自启动，那么我就需要找到开机自启动目录</p><p>在Windows中想要开机自启动某些应用，可以把程序的快捷方式放到开始菜单-&gt;程序-&gt;启动目录下，但是自启动又分为用户自启动和系统自启动，前者针对单个用户，后者针对全部用户生效。</p><ul><li>用户自启动目录：<code>C:\Users\Administrator\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup</code></li><li>系统自启动目录：<code>C:\ProgramData\Microsoft\Windows\Start Menu\Programs\StartUp</code></li></ul><p>这里要根据用户名进行设置和修改，但是当然有更简单的方法对吧，如下，我也推荐这样的方式自动打开我们的开机自启动目录。</p><p><strong>快捷命令：按下【win+R】打开运行输入：【shell:Common Startup】</strong></p><h2 id="设置脚本运行"><a href="#设置脚本运行" class="headerlink" title="设置脚本运行"></a>设置脚本运行</h2><p>当我们已经找到了开机自启动目录后，我们就可以在这个文件下，写入<code>bat</code>文件，这样每次开机都会自动运行。</p><p>首先我们定义<code>drcom.bat</code>，我们希望他能运行一个代码来进行一个检测连接网络情况，这个代码放在了 <code>C盘</code> 的根目录下，我们也可以根据自己情况修改路径放置</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python C:\\network.py</span><br></pre></td></tr></tbody></table></figure><p>所以主要的就是这个代码了，这个代码是用Python写的，原理十分的简单</p><p>既然我们需要不断的联网，那我们就不断的看看能否ping通百度，如果ping通了说明联网了，如果没有，说明我们需要运行一个登录的<code>Shell</code>文件</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'C:\login_network.sh'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure><p>这里面的<code>Shell</code>文件可以是任何登录的脚本和命令，对于<code>SZU</code>来说，脚本如下，只要改为自己的账号和密码即可，这样就完成了开机自启动。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Login SZU NetWork"</span></span><br><span class="line">username=<span class="string">"1111111"</span>  &amp;&amp; password=<span class="string">"6666666"</span> &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data <span class="string">"DDDDD=<span class="variable">$username</span>&amp;upass=<span class="variable">$password</span>&amp;0MKKey=123456"</span>;</span><br></pre></td></tr></tbody></table></figure><p>当然，简单的情况下，我们也可以直接在上面的代码<code>network.py</code>里面修改，比如如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    exit_code = os.system(<span class="string">'ping www.baidu.com'</span>)</span><br><span class="line">    <span class="keyword">if</span> exit_code != <span class="number">0</span>:</span><br><span class="line">        os.system(<span class="string">r'username="1111111"  &amp;&amp; password="6666666" &amp;&amp; curl -k https://drcom.szu.edu.cn/a70.htm --data "DDDDD=$username&amp;upass=$password&amp;0MKKey=123456";'</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这个脚本实现开机自动登录，解决了开机后无法连接网络的问题。本文介绍了在Windows系统下找到开机自启动目录，并放置检查网络状态和登录认证脚本的方法。</p><p>具体来说，使用Python脚本循环ping百度检测网络，如果无法连接则调用登录脚本进行登录。登录脚本可以直接写死账号密码，也可以单独保存为文件引用。</p><p>这种方法很简单实用，不需要付出额外精力就可以获得自动登录的功能。尤其是在需要频繁登录校园网或公司WiFi的环境下，可以大大提升效率。只需一次设置，之后就可以享受每次开机即可上网的体验。</p><p>总之，通过这个开机自启动脚本，轻松实现了每次开机自动登录网络的需求。给日常工作和学习生活带来了许多便利。</p><p>最后感谢木子李的代码提供，感谢感谢！！！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;开机自启动登录-认证前沿&quot;&gt;&lt;a href=&quot;#开机自启动登录-认证前沿&quot; class=&quot;headerlink&quot; title=&quot;开机自启动登录/认证前沿&quot;&gt;&lt;/a&gt;开机自启动登录/认证前沿&lt;/h2&gt;&lt;p&gt;有时候在学校或者在企业的时候，会出现这样一种情况，就是我们需</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>图床转化脚本</title>
    <link href="https://kedreamix.github.io/2024/01/20/Note/PicConvert/"/>
    <id>https://kedreamix.github.io/2024/01/20/Note/PicConvert/</id>
    <published>2024-01-20T08:10:00.000Z</published>
    <updated>2024-01-21T10:20:56.682Z</updated>
    
    <content type="html"><![CDATA[<p>Github: <a href="https://github.com/imcyx/PicConvert">https://github.com/imcyx/PicConvert</a></p><h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><p>该脚本除了python基础依赖库，需要安装 <code>requests</code> 和 <code>requests_toolbelt</code> 两个库：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">pip install requests_toolbelt</span><br></pre></td></tr></tbody></table></figure><p>安装完成后即可正常使用脚本</p><h2 id="2-个人配置"><a href="#2-个人配置" class="headerlink" title="2. 个人配置"></a>2. 个人配置</h2><p>在目录下的<code>configs.py</code>文件里，用户可以对自己的脚本进行配置，下面对配置进行解读：</p><h3 id="a-配置默认使用图床"><a href="#a-配置默认使用图床" class="headerlink" title="a. 配置默认使用图床"></a>a. 配置默认使用图床</h3><p><img src="https://picx.zhimg.com/v2-dbaa44b516d8fc8934cb1239f57779d0.png" alt="配置默认使用图床"></p><p>在<code>configs.py</code>文件的第11行可以配置默认使用的图床网站，而使用的图床<strong>必须从下面5种中选择</strong>。可以设置一种或者多种，按表格样式配置即可。</p><p>这里推荐使用CSDN，因为目前实测不需要频繁更换cookie，可以比较稳定的使用。</p><h3 id="b-配置登录cookie"><a href="#b-配置登录cookie" class="headerlink" title="b. 配置登录cookie"></a>b. 配置登录cookie</h3><p>因为使用的各服务提供商图床需要登录cookie，所以需要用户进入自己的浏览器抓包获得对应字段cookie后填入。</p><p>下面介绍各浏览器cookie的获取方法：</p><h4 id="CSDN"><a href="#CSDN" class="headerlink" title="CSDN"></a>CSDN</h4><p>登录自己的CSDN，然后进入个人中心 (<a href="https://i.csdn.net/">https://i.csdn.net/</a>)，打开浏览器的开发者工具（chrome 默认 <code>ctrl</code>+<code>alt</code>+<code>I</code>），找到<code>UserName</code>和<code>UserToken</code>，将对应的值复制。</p><p><img src="https://pic1.zhimg.com/v2-10ea2f2f64f2fbbef8e61656dee9c1f6.png" alt="CSDN Cookie"></p><p>然后粘贴到第26行的 <code>csdn_cookies</code>内，即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-4a12554481a33bba2f3e3f421a7944b3.png" alt="CSDN Cookie"></p><h4 id="知乎"><a href="#知乎" class="headerlink" title="知乎"></a>知乎</h4><p>登录自己的知乎，然后进入主页 (<a href="https://www.zhihu.com/">https://www.zhihu.com/</a>)，打开浏览器的开发者工具，找到<code>z_c0</code>，将对应的值复制，然后填入33行对应的<code>zhihu_cookies</code>里即完成配置。</p><p><img src="https://picx.zhimg.com/v2-c06388579ca46e0ea942d9292d580878.png" alt="知乎 Cookie"></p><p>知乎的图片默认支持3种，<code>src</code>, <code>watermark_src</code>, <code>original_src</code>，<code>watermark_src</code>是水印原图，<code>original_src</code>是原图，<code>src</code>是展示图，用户可以自己选择。</p><h4 id="b站"><a href="#b站" class="headerlink" title="b站"></a>b站</h4><p>登录自己的b站，然后进入主页 (<a href="https://www.bilibili.com/">https://www.bilibili.com/</a>)，打开浏览器的开发者工具，找到<code>SESSDATA</code>，将对应的值复制，然后填入41行对应的<code>bili_cookies</code>里即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-73f566da43bf13b082b4cc569875bad8.png" alt="Bilibili Cookie"></p><h4 id="简书"><a href="#简书" class="headerlink" title="简书"></a>简书</h4><p>登录自己的简书，然后进入主页 (<a href="https://www.jianshu.com/">https://www.jianshu.com/</a>)，打开浏览器的开发者工具，找到<code>remember_user_token</code>和<code>_m7e_session_core</code>字段，将对应的值复制，然后填入47行对应的<code>jianshu_cookies</code>里即完成配置。</p><p><img src="https://pic1.zhimg.com/v2-ed43bbd15853b83cfe8b4c7ccd473424.png" alt="简书 Cookie"></p><h4 id="博客园"><a href="#博客园" class="headerlink" title="博客园"></a>博客园</h4><p>登录自己的博客园，然后进入主页 (<a href="https://www.cnblogs.com/">https://www.cnblogs.com/</a>)，打开浏览器的开发者工具，找到<code>.Cnblogs.AspNetCore.Cookies</code>字段，将对应的值复制，然后填入53行对应的<code>bokeyuan_cookies</code>里即完成配置。</p><p><img src="https://pica.zhimg.com/v2-2ec98121095723e0b63f0dce75441907.png" alt="博客园 Cookie"></p><h2 id="3-命令行调用"><a href="#3-命令行调用" class="headerlink" title="3.  命令行调用"></a>3.  命令行调用</h2><p>脚本的使用方法为：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py</span><br></pre></td></tr></tbody></table></figure><p>使用该命令后，默认读取当前脚本所处目录下的所有md文件，并逐个读取扫描图片链接或本地路径，按照配置里指定的转换方式，转换后再输出为{New<em>(mode)</em>(原始名)}。</p><p><img src="https://pica.zhimg.com/v2-d85a797b22394c79616e1645f70047a1.png" alt="使用命令行"></p><p>如果需要指定转化的文件，使用命令：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py -f new.md</span><br></pre></td></tr></tbody></table></figure><p>而如果不适用默认的转换图床，需要额外指定转换图床，使用命令：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python convert.py -m csdn</span><br></pre></td></tr></tbody></table></figure><p>这两个参数可以同时指定，转换效果如下：</p><p><img src="https://pic1.zhimg.com/v2-56e84953380c8dc11ee8a329c3bc1f5e.png" alt="指定参数"></p>]]></content>
    
    
    <summary type="html">一个适用于多种图床转换的脚本（知乎、b站、简书、CSDN、博客园）</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
    <category term="Tool" scheme="https://kedreamix.github.io/tags/Tool/"/>
    
  </entry>
  
  <entry>
    <title>数字人对话系统 - Linly-Talker —— “数字人交互，与虚拟的自己互动”</title>
    <link href="https://kedreamix.github.io/2024/01/20/Project/Linly-Talker/"/>
    <id>https://kedreamix.github.io/2024/01/20/Project/Linly-Talker/</id>
    <published>2024-01-19T16:00:00.000Z</published>
    <updated>2024-01-29T05:36:19.617Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2023.12 更新</strong> 📆</p><p><strong>用户可以上传任意图片进行对话</strong></p><p><strong>2024.01 更新</strong> 📆</p><ul><li><strong>令人兴奋的消息！我现在已经将强大的GeminiPro和Qwen大模型融入到我们的对话场景中。用户现在可以在对话中上传任何图片，为我们的互动增添了全新的层面。</strong></li><li><strong>更新了FastAPI的部署调用方法。</strong> </li><li><strong>更新了微软TTS的高级设置选项，增加声音种类的多样性，以及加入视频字幕加强可视化。</strong></li><li><strong>更新了GPT多轮对话系统，使得对话有上下文联系，提高数字人的交互性和真实感</strong></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</p><p><img src="https://picx.zhimg.com/80/v2-b38722d9d71153dec12acbb9e020a5b4.png" alt="The system architecture of multimodal human–computer interaction."></p><h2 id="TO-DO-LIST"><a href="#TO-DO-LIST" class="headerlink" title="TO DO LIST"></a>TO DO LIST</h2><ul><li>[x] 基本完成对话系统流程，能够<code>语音对话</code></li><li>[x] 加入了LLM大模型，包括<code>Linly</code>，<code>Qwen</code>和<code>GeminiPro</code>的使用</li><li>[x] 可上传<code>任意数字人照片</code>进行对话</li><li>[x] Linly加入<code>FastAP</code>I调用方式</li><li>[x] 利用微软<code>TTS</code>加入高级选项，可设置对应人声以及音调等参数，增加声音的多样性</li><li>[x] 视频生成加入<code>字幕</code>，能够更好的进行可视化</li><li>[x] GPT<code>多轮对话</code>系统（提高数字人的交互性和真实感，增强数字人的智能）</li><li>[ ] <code>语音克隆</code>技术（语音克隆合成自己声音，提高数字人分身的真实感和互动体验）</li><li>[ ] 加入<code>Langchain</code>的框架，建立本地知识库</li><li>[ ] <code>实时</code>语音识别（人与数字人之间就可以通过语音进行对话交流)</li></ul><p>🔆 该项目 Linly-Talker 正在进行中 - 欢迎提出PR请求！如果您有任何关于新的模型方法、研究、技术或发现运行错误的建议，请随时编辑并提交 PR。您也可以打开一个问题或通过电子邮件直接联系我。📩⭐ 如果您发现这个Github Project有用，请给它点个星！🤩</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><div class="table-container"><table><thead><tr><th style="text-align:center">文字/语音对话</th><th style="text-align:center">数字人回答</th></tr></thead><tbody><tr><td style="text-align:center">应对压力最有效的方法是什么？</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f1deb189-b682-4175-9dea-7eeb0fb392ca"></video></td></tr><tr><td style="text-align:center">如何进行时间管理？</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/968b5c43-4dce-484b-b6c6-0fd4d621ac03"></video></td></tr><tr><td style="text-align:center">撰写一篇交响乐音乐会评论，讨论乐团的表演和观众的整体体验。</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f052820f-6511-4cf0-a383-daf8402630db"></video></td></tr><tr><td style="text-align:center">翻译成中文：Luck is a dividend of sweat. The more you sweat, the luckier you get.</td><td style="text-align:center"><video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/118eec13-a9f7-4c38-b4ad-044d36ba9776"></video></td></tr></tbody></table></div><h2 id="创建环境"><a href="#创建环境" class="headerlink" title="创建环境"></a>创建环境</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conda create -n linly python=3.8 </span><br><span class="line">conda activate linly</span><br><span class="line"></span><br><span class="line">pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113</span><br><span class="line"></span><br><span class="line">conda install -ffmpeg </span><br><span class="line"></span><br><span class="line">pip install -r requirements_app.txt</span><br></pre></td></tr></tbody></table></figure><p>为了大家的部署使用方便，更新了一个<code>configs.py</code>文件，可以对其进行一些超参数修改即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设备运行端口 (Device running port)</span></span><br><span class="line">port = 7870</span><br><span class="line"><span class="comment"># api运行端口及IP (API running port and IP)</span></span><br><span class="line">ip = <span class="string">'127.0.0.1'</span> </span><br><span class="line">api_port = 7871</span><br><span class="line"><span class="comment"># Linly模型路径 (Linly model path)</span></span><br><span class="line">mode = <span class="string">'api'</span> <span class="comment"># api 需要先运行Linly-api-fast.py</span></span><br><span class="line">mode = <span class="string">'offline'</span></span><br><span class="line">model_path = <span class="string">'Linly-AI/Chinese-LLaMA-2-7B-hf'</span></span><br><span class="line"><span class="comment"># ssl证书 (SSL certificate) 麦克风对话需要此参数</span></span><br><span class="line">ssl_certfile = <span class="string">"/path/to/Linly-Talker/https_cert/cert.pem"</span></span><br><span class="line">ssl_keyfile = <span class="string">"/path/to/Linly-Talker/https_cert/key.pem"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="ASR-Whisper"><a href="#ASR-Whisper" class="headerlink" title="ASR - Whisper"></a>ASR - Whisper</h2><p>借鉴OpenAI的Whisper,具体使用方法参考<a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></p><h2 id="TTS-Edge-TTS"><a href="#TTS-Edge-TTS" class="headerlink" title="TTS - Edge TTS"></a>TTS - Edge TTS</h2><p>使用微软语音服务,具体使用方法参考<a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a></p><p>我编写了一个 <code>EdgeTTS</code> 的类，能够更好的使用，并且增加了保存字幕文件的功能</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EdgeTTS</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, list_voices = <span class="literal">False</span>, proxy = <span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        voices = list_voices_fn(proxy=proxy)</span><br><span class="line">        self.SUPPORTED_VOICE = [item[<span class="string">'ShortName'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> voices]</span><br><span class="line">        self.SUPPORTED_VOICE.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> list_voices:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">", "</span>.join(self.SUPPORTED_VOICE))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">self, rate, volume, pitch</span>):</span><br><span class="line">        <span class="keyword">if</span> rate &gt;= <span class="number">0</span>:</span><br><span class="line">            rate = <span class="string">f'+<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            rate = <span class="string">f'<span class="subst">{rate}</span>%'</span></span><br><span class="line">        <span class="keyword">if</span> pitch &gt;= <span class="number">0</span>:</span><br><span class="line">            pitch = <span class="string">f'+<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pitch = <span class="string">f'<span class="subst">{pitch}</span>Hz'</span></span><br><span class="line">        volume = <span class="number">100</span> - volume</span><br><span class="line">        volume = <span class="string">f'-<span class="subst">{volume}</span>%'</span></span><br><span class="line">        <span class="keyword">return</span> rate, volume, pitch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,TEXT, VOICE, RATE, VOLUME, PITCH, OUTPUT_FILE=<span class="string">'result.wav'</span>, OUTPUT_SUBS=<span class="string">'result.vtt'</span>, words_in_cue = <span class="number">8</span></span>):</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">amain</span>() -&gt; <span class="literal">None</span>:</span><br><span class="line">            <span class="string">"""Main function"""</span></span><br><span class="line">            rate, volume, pitch = self.preprocess(rate = RATE, volume = VOLUME, pitch = PITCH)</span><br><span class="line">            communicate = Communicate(TEXT, VOICE, rate = rate, volume = volume, pitch = pitch)</span><br><span class="line">            subs: SubMaker = SubMaker()</span><br><span class="line">            sub_file: <span class="type">Union</span>[TextIOWrapper, TextIO] = (</span><br><span class="line">                <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">for</span> chunk <span class="keyword">in</span> communicate.stream():</span><br><span class="line">                <span class="keyword">if</span> chunk[<span class="string">"type"</span>] == <span class="string">"audio"</span>:</span><br><span class="line">                    <span class="comment"># audio_file.write(chunk["data"])</span></span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">                <span class="keyword">elif</span> chunk[<span class="string">"type"</span>] == <span class="string">"WordBoundary"</span>:</span><br><span class="line">                    <span class="comment"># print((chunk["offset"], chunk["duration"]), chunk["text"])</span></span><br><span class="line">                    subs.create_sub((chunk[<span class="string">"offset"</span>], chunk[<span class="string">"duration"</span>]), chunk[<span class="string">"text"</span>])</span><br><span class="line">            sub_file.write(subs.generate_subs(words_in_cue))</span><br><span class="line">            <span class="keyword">await</span> communicate.save(OUTPUT_FILE)</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># loop = asyncio.get_event_loop_policy().get_event_loop()</span></span><br><span class="line">        <span class="comment"># try:</span></span><br><span class="line">        <span class="comment">#     loop.run_until_complete(amain())</span></span><br><span class="line">        <span class="comment"># finally:</span></span><br><span class="line">        <span class="comment">#     loop.close()</span></span><br><span class="line">        asyncio.run(amain())</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> file:</span><br><span class="line">            vtt_lines = file.readlines()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去掉每一行文字中的空格</span></span><br><span class="line">        vtt_lines_without_spaces = [line.replace(<span class="string">" "</span>, <span class="string">""</span>) <span class="keyword">if</span> <span class="string">"--&gt;"</span> <span class="keyword">not</span> <span class="keyword">in</span> line <span class="keyword">else</span> line <span class="keyword">for</span> line <span class="keyword">in</span> vtt_lines]</span><br><span class="line">        <span class="comment"># print(vtt_lines_without_spaces)</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(OUTPUT_SUBS, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> output_file:</span><br><span class="line">            output_file.writelines(vtt_lines_without_spaces)</span><br><span class="line">        <span class="keyword">return</span> OUTPUT_FILE, OUTPUT_SUBS</span><br></pre></td></tr></tbody></table></figure><p>同时在<code>src</code>文件夹下，写了一个简易的<code>WebUI</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python TTS_app.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://picx.zhimg.com/80/v2-570f3c9a069358e4d4a7b7c008e99cb7.png" alt="TTS"></p><h2 id="THG-SadTalker"><a href="#THG-SadTalker" class="headerlink" title="THG - SadTalker"></a>THG - SadTalker</h2><p>说话头生成使用SadTalker（CVPR 2023）,详情见<a href="https://sadtalker.github.io">https://sadtalker.github.io</a></p><p>下载SadTalker模型:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash scripts/download_models.sh  </span><br></pre></td></tr></tbody></table></figure><h2 id="LLM-Conversation"><a href="#LLM-Conversation" class="headerlink" title="LLM - Conversation"></a>LLM - Conversation</h2><h3 id="Linly-AI"><a href="#Linly-AI" class="headerlink" title="Linly-AI"></a>Linly-AI</h3><p>Linly来自深圳大学数据工程国家重点实验室,参考<a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></p><p>下载Linly模型:<a href="https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf">https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</a></p><p>可以使用<code>git</code>下载</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure><p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Linly-AI/Chinese-LLaMA-2-7B-hf --local-dir Linly-AI/Chinese-LLaMA-2-7B-hf</span><br></pre></td></tr></tbody></table></figure><p>或使用API:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 命令行</span></span><br><span class="line">curl -X POST -H <span class="string">"Content-Type: application/json"</span> -d <span class="string">'{"question": "北京有什么好玩的地方?"}'</span> http://url:port  </span><br><span class="line"></span><br><span class="line"><span class="comment"># Python</span></span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"http://url:port"</span></span><br><span class="line">headers = {</span><br><span class="line">  <span class="string">"Content-Type"</span>: <span class="string">"application/json"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">data = {</span><br><span class="line">  <span class="string">"question"</span>: <span class="string">"北京有什么好玩的地方?"</span> </span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"><span class="comment"># response_text = response.content.decode("utf-8")</span></span><br><span class="line">answer, tag = response.json()</span><br><span class="line"><span class="comment"># print(answer)</span></span><br><span class="line"><span class="keyword">if</span> tag == <span class="string">'success'</span>:</span><br><span class="line">    response_text =  answer[0]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"fail"</span>)</span><br><span class="line"><span class="built_in">print</span>(response_text)</span><br></pre></td></tr></tbody></table></figure><p>API部署推荐<strong>FastAPI</strong>，现在更新了 FastAPI 的API使用版本，FastAPI 是一个高性能、易用且现代的Python Web 框架，它通过使用最新的Python 特性和异步编程，提供了快速开发Web API 的能力。 该框架不仅易于学习和使用，还具有自动生成文档、数据验证等强大功能。 无论是构建小型项目还是大型应用程序，FastAPI 都是一个强大而有效的工具。</p><p>首先安装部署API所使用的库</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install fastapi==0.104.1</span><br><span class="line">pip install uvicorn==0.24.0.post1</span><br></pre></td></tr></tbody></table></figure><p>其他使用方法大致相同，主要是不同代码实现方式，会更加简单边界，并且处理并发也会更好</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> configs <span class="keyword">import</span> model_path, api_port</span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    prompt = <span class="string">f"请用少于25个字回答以下问题 ### Instruction:<span class="subst">{prompt}</span>  ### Response:"</span></span><br><span class="line">    inputs = tokenizer(prompt, return_tensors=<span class="string">"pt"</span>).to(<span class="string">"cuda:0"</span>)</span><br><span class="line">    generate_ids = model.generate(inputs.input_ids, </span><br><span class="line">                                  max_new_tokens=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,</span><br><span class="line">                                  do_sample=<span class="literal">True</span>, </span><br><span class="line">                                  top_k=<span class="number">20</span>,</span><br><span class="line">                                  top_p=top_p,</span><br><span class="line">                                  temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.84</span>,</span><br><span class="line">                                  repetition_penalty=<span class="number">1.15</span>, eos_token_id=<span class="number">2</span>, bos_token_id=<span class="number">1</span>,pad_token_id=<span class="number">0</span>)</span><br><span class="line">    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="literal">True</span>, clean_up_tokenization_spaces=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    response = response.split(<span class="string">"### Response:"</span>)[-<span class="number">1</span>]</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="comment"># "history": history,</span></span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=<span class="string">"cuda:0"</span>,</span><br><span class="line">                                                    torch_dtype=torch.bfloat16, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=<span class="literal">False</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=api_port, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure><p>默认部署在 7871 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST <span class="string">"http://127.0.0.1:7871"</span> \</span><br><span class="line">     -H <span class="string">'Content-Type: application/json'</span> \</span><br><span class="line">     -d <span class="string">'{"prompt": "如何应对压力"}'</span></span><br></pre></td></tr></tbody></table></figure><p>也可以使用python中的requests库进行调用，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_completion</span>(<span class="params">prompt</span>):</span><br><span class="line">    headers = {<span class="string">'Content-Type'</span>: <span class="string">'application/json'</span>}</span><br><span class="line">    data = {<span class="string">"prompt"</span>: prompt}</span><br><span class="line">    response = requests.post(url=<span class="string">'http://127.0.0.1:7871'</span>, headers=headers, data=json.dumps(data))</span><br><span class="line">    <span class="keyword">return</span> response.json()[<span class="string">'response'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="built_in">print</span>(get_completion(<span class="string">'你好如何应对压力'</span>))</span><br></pre></td></tr></tbody></table></figure><p>得到的返回值如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"response"</span>:<span class="string">"寻求支持和放松，并采取积极的措施解决问题。"</span>,</span><br><span class="line">  <span class="string">"status"</span>:200,</span><br><span class="line">  <span class="string">"time"</span>:<span class="string">"2024-01-12 01:43:37"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h3><p>来自阿里云的Qwen，查看 <a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></p><p>下载 Qwen 模型: <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat">https://huggingface.co/Qwen/Qwen-1_8B-Chat</a></p><p>可以使用<code>git</code>下载</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure><p>或者使用<code>huggingface</code>的下载工具<code>huggingface-cli</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pip install -U huggingface_hub</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置镜像加速</span></span><br><span class="line"><span class="comment"># Linux</span></span><br><span class="line"><span class="built_in">export</span> HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"><span class="comment"># windows powershell</span></span><br><span class="line"><span class="variable">$env</span>:HF_ENDPOINT=<span class="string">"https://hf-mirror.com"</span></span><br><span class="line"></span><br><span class="line">huggingface-cli download --resume-download Qwen/Qwen-1_8B-Chat --local-dir Qwen/Qwen-1_8B-Chat</span><br></pre></td></tr></tbody></table></figure><h3 id="Gemini-Pro"><a href="#Gemini-Pro" class="headerlink" title="Gemini-Pro"></a>Gemini-Pro</h3><p>来自 Google 的 Gemini-Pro，了解更多请访问 <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></p><p>请求 API 密钥: <a href="https://makersuite.google.com/">https://makersuite.google.com/</a></p><h3 id="LLM-模型选择"><a href="#LLM-模型选择" class="headerlink" title="LLM 模型选择"></a>LLM 模型选择</h3><p>在 app.py 文件中，轻松选择您需要的模型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取消注释并设置您选择的模型:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># llm = Gemini(model_path='gemini-pro', api_key=None, proxy_url=None) # 不要忘记加入您自己的 Google API 密钥</span></span><br><span class="line"><span class="comment"># llm = Qwen(mode='offline', model_path="Qwen/Qwen-1_8B-Chat")</span></span><br><span class="line"><span class="comment"># 自动下载</span></span><br><span class="line"><span class="comment"># llm = Linly(mode='offline', model_path="Linly-AI/Chinese-LLaMA-2-7B-hf")</span></span><br><span class="line"><span class="comment"># 手动下载到指定路径</span></span><br><span class="line">llm = Linly(mode=<span class="string">'offline'</span>, model_path=<span class="string">"Linly-AI/Chinese-LLaMA-2-7B-hf"</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>一些优化:</p><ul><li>使用固定的输入人脸图像,提前提取特征,避免每次读取</li><li>移除不必要的库,缩短总时间</li><li>只保存最终视频输出,不保存中间结果,提高性能</li><li>使用OpenCV生成最终视频,比mimwrite更快</li></ul><h2 id="Gradio"><a href="#Gradio" class="headerlink" title="Gradio"></a>Gradio</h2><p>Gradio是一个Python库,提供了一种简单的方式将机器学习模型作为交互式Web应用程序来部署。</p><p>对Linly-Talker而言,使用Gradio有两个主要目的:</p><ol><li><p><strong>可视化与演示</strong>:Gradio为模型提供一个简单的Web GUI,上传图片和文本后可以直观地看到结果。这是展示系统能力的有效方式。</p></li><li><p><strong>用户交互</strong>:Gradio的GUI可以作为前端,允许用户与Linly-Talker进行交互对话。用户可以上传自己的图片并输入问题,实时获取回答。这提供了更自然的语音交互方式。</p></li></ol><p>具体来说,我们在app.py中创建了一个Gradio的Interface,接收图片和文本输入,调用函数生成回应视频,在GUI中显示出来。这样就实现了浏览器交互而不需要编写复杂的前端。</p><p>总之,Gradio为Linly-Talker提供了可视化和用户交互的接口,是展示系统功能和让最终用户使用系统的有效途径。</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>现在的启动一共有几种模式，可以选择特定的场景进行设置</p><p>第一种只有固定了人物问答，设置好了人物，省去了预处理时间</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pica.zhimg.com/80/v2-fc37a5490a674e2194b88714d38f986e.png" alt=""></p><p>第二种是可以任意上传图片进行对话</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app_img.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pic1.zhimg.com/80/v2-7c863c3992beef67953d7ab378be99d9.png" alt=""></p><p>第三种是在第一种的基础上加入了大语言模型，加入了多轮的GPT对话</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python app_multi.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://picx.zhimg.com/80/v2-802165f64f307dd204b04b9725626cd7.png" alt=""></p><p>文件夹结构如下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">Linly-Talker/ </span><br><span class="line">├── app.py</span><br><span class="line">├── app_img.py</span><br><span class="line">├── utils.py</span><br><span class="line">├── Linly-api.py</span><br><span class="line">├── Linly-api-fast.py</span><br><span class="line">├── Linly-example.ipynb</span><br><span class="line">├── README.md</span><br><span class="line">├── README_zh.md</span><br><span class="line">├── request-Linly-api.py</span><br><span class="line">├── requirements_app.txt</span><br><span class="line">├── scripts</span><br><span class="line">│   └── download_models.sh</span><br><span class="line">├──src</span><br><span class="line">│└── .....</span><br><span class="line">├── inputs</span><br><span class="line">│   ├── example.png</span><br><span class="line">│   └── first_frame_dir</span><br><span class="line">│       ├── example_landmarks.txt</span><br><span class="line">│       ├── example.mat</span><br><span class="line">│       └── example.png</span><br><span class="line">├── examples</span><br><span class="line">│   └── source_image</span><br><span class="line">│       ├── art_0.png</span><br><span class="line">│       ├── ......</span><br><span class="line">│       └── sad.png</span><br><span class="line">├── checkpoints // SadTalker 权重路径</span><br><span class="line">│   ├── mapping_00109-model.pth.tar</span><br><span class="line">│   ├── mapping_00229-model.pth.tar</span><br><span class="line">│   ├── SadTalker_V0.0.2_256.safetensors</span><br><span class="line">│   └── SadTalker_V0.0.2_512.safetensors</span><br><span class="line">├── gfpgan // GFPGAN 权重路径</span><br><span class="line">│   └── weights</span><br><span class="line">│       ├── alignment_WFLW_4HG.pth</span><br><span class="line">│       └── detection_Resnet50_Final.pth</span><br><span class="line">├── Linly-AI // Linly 权重路径</span><br><span class="line">│   └── Chinese-LLaMA-2-7B-hf </span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── pytorch_model-00001-of-00002.bin</span><br><span class="line">│       ├── pytorch_model-00002-of-00002.bin</span><br><span class="line">│       ├── pytorch_model.bin.index.json</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── special_tokens_map.json</span><br><span class="line">│       ├── tokenizer_config.json</span><br><span class="line">│       └── tokenizer.model</span><br><span class="line">├── Qwen // Qwen 权重路径</span><br><span class="line">│   └── Qwen-1_8B-Chat</span><br><span class="line">│       ├── cache_autogptq_cuda_256.cpp</span><br><span class="line">│       ├── cache_autogptq_cuda_kernel_256.cu</span><br><span class="line">│       ├── config.json</span><br><span class="line">│       ├── configuration_qwen.py</span><br><span class="line">│       ├── cpp_kernels.py</span><br><span class="line">│       ├── examples</span><br><span class="line">│       │   └── react_prompt.md</span><br><span class="line">│       ├── generation_config.json</span><br><span class="line">│       ├── LICENSE</span><br><span class="line">│       ├── model-00001-of-00002.safetensors</span><br><span class="line">│       ├── model-00002-of-00002.safetensors</span><br><span class="line">│       ├── modeling_qwen.py</span><br><span class="line">│       ├── model.safetensors.index.json</span><br><span class="line">│       ├── NOTICE</span><br><span class="line">│       ├── qwen_generation_utils.py</span><br><span class="line">│       ├── qwen.tiktoken</span><br><span class="line">│       ├── README.md</span><br><span class="line">│       ├── tokenization_qwen.py</span><br><span class="line">│       └── tokenizer_config.json</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a></li><li><a href="https://github.com/rany2/edge-tts">https://github.com/rany2/edge-tts</a>  </li><li><a href="https://github.com/CVI-SZU/Linly">https://github.com/CVI-SZU/Linly</a></li><li><a href="https://github.com/QwenLM/Qwen">https://github.com/QwenLM/Qwen</a></li><li><a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a></li><li><a href="https://github.com/OpenTalker/SadTalker">https://github.com/OpenTalker/SadTalker</a></li></ul><h2 id="Star-History"><a href="#Star-History" class="headerlink" title="Star History"></a>Star History</h2><p><a href="https://star-history.com/#Kedreamix/Linly-Talker&amp;Date"><img src="https://api.star-history.com/svg?repos=Kedreamix/Linly-Talker&amp;type=Date" alt="Star History Chart"></a></p>]]></content>
    
    
    <summary type="html">Linly-Talker是一个将大型语言模型与视觉模型相结合的智能AI系统,创建了一种全新的人机交互方式。它集成了各种技术,例如Whisper、Linly、微软语音服务和SadTalker会说话的生成系统。该系统部署在Gradio上,允许用户通过提供图像与AI助手进行交谈。用户可以根据自己的喜好进行自由的对话或内容生成。</summary>
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="LLM" scheme="https://kedreamix.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>FastAPI 快速教程: 从零开始构建你的第一个API项目</title>
    <link href="https://kedreamix.github.io/2024/01/19/Note/fastapi/"/>
    <id>https://kedreamix.github.io/2024/01/19/Note/fastapi/</id>
    <published>2024-01-19T15:09:47.000Z</published>
    <updated>2024-01-21T10:20:06.402Z</updated>
    
    <content type="html"><![CDATA[<p>最近在学习大模型的时候，有时候会遇到要写API的时候，这个时候我就遇见了FastAPI，我发现这个是一个很好的库，可以很方便的让我们构建一个属于自己的API，所以今天我也写一下这个入门教程和大家一起分享一下，同时也让我们解密一下，OpenAI和一些公司的API，可能是怎么写和怎么做的。</p><p><img src="https://picx.zhimg.com/80/v2-f7dc5c12cb693d83a113359819a1f26e_720w.png?source=d16d100b" alt="FastAPI framework, high performance, easy to learn, fast to code, ready for production"></p><h2 id="FastAPI介绍"><a href="#FastAPI介绍" class="headerlink" title="FastAPI介绍"></a>FastAPI介绍</h2><p>FastAPI 是一个用于构建 API 的现代、快速（高性能）的 web 框架，使用 Python 3.8+ 并基于标准的 Python 类型提示。</p><p><strong>文档</strong>： <a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com</a></p><p><strong>源码</strong>： <a href="https://github.com/tiangolo/fastapi">https://github.com/tiangolo/fastapi</a></p><p>关键特性:</p><ul><li><strong>快速</strong>：可与 <strong>NodeJS</strong> 和 <strong>Go</strong> 并肩的极高性能（归功于 Starlette 和 Pydantic）。<a href="https://fastapi.tiangolo.com/zh/#_11">最快的 Python web 框架之一</a>。</li><li><strong>高效编码</strong>：提高功能开发速度约 200％ 至 300％。*</li><li><strong>更少 bug</strong>：减少约 40％ 的人为（开发者）导致错误。*</li><li><strong>智能</strong>：极佳的编辑器支持。处处皆可自动补全，减少调试时间。</li><li><strong>简单</strong>：设计的易于使用和学习，阅读文档的时间更短。</li><li><strong>简短</strong>：使代码重复最小化。通过不同的参数声明实现丰富功能。bug 更少。</li><li><strong>健壮</strong>：生产可用级别的代码。还有自动生成的交互式文档。</li><li><strong>标准化</strong>：基于（并完全兼容）API 的相关开放标准：<a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a> (以前被称为 Swagger) 和 <a href="https://json-schema.org/">JSON Schema</a>。</li></ul><h2 id="安装及依赖"><a href="#安装及依赖" class="headerlink" title="安装及依赖"></a>安装及依赖</h2><p>Python 3.8 及更高版本</p><p>FastAPI 站在以下巨人的肩膀之上：</p><ul><li><a href="https://www.starlette.io/">Starlette</a> 负责 web 部分。</li><li><a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> 负责数据部分。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fastapi</span><br></pre></td></tr></tbody></table></figure><p>我们有可能还会需要一个 ASGI 服务器，可以使用<a href="https://www.uvicorn.org/">Uvicorn</a></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install unvicorn</span><br></pre></td></tr></tbody></table></figure><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><p>创建一个 main.py 文件并写入以下内容:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure><p>如果我们需要加入异步编程的话，我们就需要改一下代码，加入async/await和async def </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_root</span>():</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"Hello"</span>: <span class="string">"World"</span>}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">"/items/{item_id}"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">read_item</span>(<span class="params">item_id: <span class="built_in">int</span>, q: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="literal">None</span>] = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> {<span class="string">"item_id"</span>: item_id, <span class="string">"q"</span>: q}</span><br></pre></td></tr></tbody></table></figure><p>如果对于异步编程有点兴趣的话，可以看看这个讲解，我觉得还是很不错的，这也加强了代码的并发能力。</p><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>通过以下命令进行运行服务器</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ uvicorn main:app --reload</span><br><span class="line"></span><br><span class="line">INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</span><br><span class="line">INFO:     Started reloader process [28720]</span><br><span class="line">INFO:     Started server process [28722]</span><br><span class="line">INFO:     Waiting <span class="keyword">for</span> application startup.</span><br><span class="line">INFO:     Application startup complete.</span><br></pre></td></tr></tbody></table></figure><p>uvicorn main:app 命令含义如下:</p><ul><li>main：main.py 文件（一个 Python “模块”）。</li><li>app：在 main.py 文件中通过 app = FastAPI() 创建的对象。</li><li>—reload：让服务器在更新代码后重新启动。仅在开发时使用该选项。</li></ul><p>这里面着重提一下<strong>reolab参数</strong>，这个相当于我们在开发是对代码进行修改的同时，服务器也在变化，这样就方便我们进行开发和学习，但是如果开发完毕以后，我们可以去掉<strong>—reload</strong>，防止不小心动到代码改变了api的访问</p><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a><strong>检查</strong></h3><p>使用浏览器访问 <a href="http://127.0.0.1:8000/items/5?q=somequery。">http://127.0.0.1:8000/items/5?q=somequery。</a></p><p>你将会看到如下 JSON 响应：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">{"item_id": 5, "q": "somequery"}</span><br></pre></td></tr></tbody></table></figure><p>你已经创建了一个具有以下功能的 API：</p><ul><li>通过 <em>路径</em> / 和 /items/{item_id} 接受 HTTP 请求。</li><li>以上 <em>路径</em> 都接受 GET <em>操作</em>（也被称为 HTTP <em>方法</em>）。</li><li>/items/{item_id} <em>路径</em> 有一个 <em>路径参数</em> item_id 并且应该为 int 类型。</li><li>/items/{item_id} <em>路径</em> 有一个可选的 str 类型的 <em>查询参数</em> q。</li></ul><h2 id="API文档"><a href="#API文档" class="headerlink" title="API文档"></a>API文档</h2><h3 id="交互式-API-文档"><a href="#交互式-API-文档" class="headerlink" title="交互式 API 文档"></a><strong>交互式 API 文档</strong></h3><p>现在访问 <a href="http://127.0.0.1:8000/docs。">http://127.0.0.1:8000/docs。</a></p><p>你会看到自动生成的交互式 API 文档（由 <a href="https://github.com/swagger-api/swagger-ui">Swagger UI</a>生成）：</p><p><img src="https://picx.zhimg.com/80/v2-39fe8891285bf578eee8466c4aec1b52_720w.png?source=d16d100b" alt="交互式 API 文档"></p><h3 id="可选的-API-文档"><a href="#可选的-API-文档" class="headerlink" title="可选的 API 文档"></a>可选的 API 文档</h3><p>访问 <a href="http://127.0.0.1:8000/redoc。">http://127.0.0.1:8000/redoc。</a></p><p>你会看到另一个自动生成的文档（由 <a href="https://github.com/Rebilly/ReDoc">ReDoc</a> 生成）：</p><p><img src="https://picx.zhimg.com/80/v2-23ae5bafaed6faf6ced83d987b17fc98_720w.png?source=d16d100b" alt="可选的 API 文档"></p><h2 id="大模型API实战"><a href="#大模型API实战" class="headerlink" title="大模型API实战"></a>大模型API实战</h2><p>比如我现在想使用一个大模型，比如就是阿里的通义千问的大模型，我希望能写一个api接口进行对其调用，有点类似与OpenAI一样写一个接口，这样就方便我们进行去调用，而不用每次跑一堆代码。</p><h3 id="Qwen模型下载与使用"><a href="#Qwen模型下载与使用" class="headerlink" title="Qwen模型下载与使用"></a>Qwen模型下载与使用</h3><p>比如我们可以从Qwen中获取对应的模型，<a href="https://huggingface.co/Qwen/Qwen-7B-Chat">https://huggingface.co/Qwen/Qwen-7B-Chat</a></p><p>从里面我们可以看到多轮对话的快速使用代码，这对我们写API有很大的帮助，我们从中可以看到主要的流程，实际上还是蛮简单的，就是导入模型后，进行传入参数，参数一般有三个，一个是一开始定义的分词器<strong>tokenizer</strong>，另外两个就比较重要，分别是问题和历史记录，所以我们希望得到的api应该是有这两个输入的。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: The default behavior now has injection attack prevention off.</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use bf16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()</span></span><br><span class="line"><span class="comment"># use fp16</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()</span></span><br><span class="line"><span class="comment"># use cpu only</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()</span></span><br><span class="line"><span class="comment"># use auto mode, automatically select precision based on the device.</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify hyperparameters for generation. But if you use transformers&gt;=4.32.0, there is no need to do this.</span></span><br><span class="line"><span class="comment"># model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一轮对话 1st dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"你好"</span>, history=<span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 你好！很高兴为你提供帮助。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二轮对话 2nd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给我讲一个年轻人奋斗创业最终取得成功的故事。"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 这是一个关于一个年轻人奋斗创业最终取得成功的故事。</span></span><br><span class="line"><span class="comment"># 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。</span></span><br><span class="line"><span class="comment"># 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。</span></span><br><span class="line"><span class="comment"># 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。</span></span><br><span class="line"><span class="comment"># 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。</span></span><br><span class="line"><span class="comment"># 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三轮对话 3rd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">"给这个故事起一个标题"</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 《奋斗创业：一个年轻人的成功之路》</span></span><br></pre></td></tr></tbody></table></figure><h3 id="编写FastAPI代码"><a href="#编写FastAPI代码" class="headerlink" title="编写FastAPI代码"></a>编写FastAPI代码</h3><p>所以弄清楚了原理以后，我们就可以开始利用FastAPI写以下的代码了，为了更好的使用api，除了prompt和history两个参数之外，还加入了max_lenth和temperature等参数，这些参数实际上都是model.chat里面进行使用的，这样更好的去设置和学习。等到返回结果以后，就会返回时间和成功的标志，这样我们就获取了结果。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GenerationConfig</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置设备参数</span></span><br><span class="line">DEVICE = <span class="string">"cuda"</span>  <span class="comment"># 使用CUDA</span></span><br><span class="line">DEVICE_ID = <span class="string">"0"</span>  <span class="comment"># CUDA设备ID，如果未设置则为空</span></span><br><span class="line">CUDA_DEVICE = <span class="string">f"<span class="subst">{DEVICE}</span>:<span class="subst">{DEVICE_ID}</span>"</span> <span class="keyword">if</span> DEVICE_ID <span class="keyword">else</span> DEVICE  <span class="comment"># 组合CUDA设备信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理GPU内存函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_gc</span>():</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():  <span class="comment"># 检查是否可用CUDA</span></span><br><span class="line">        <span class="keyword">with</span> torch.cuda.device(CUDA_DEVICE):  <span class="comment"># 指定CUDA设备</span></span><br><span class="line">            torch.cuda.empty_cache()  <span class="comment"># 清空CUDA缓存</span></span><br><span class="line">            torch.cuda.ipc_collect()  <span class="comment"># 收集CUDA内存碎片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建FastAPI应用</span></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理POST请求的端点</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">"/"</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="keyword">global</span> model, tokenizer  <span class="comment"># 声明全局变量以便在函数内部使用模型和分词器</span></span><br><span class="line">    json_post_raw = <span class="keyword">await</span> request.json()  <span class="comment"># 获取POST请求的JSON数据</span></span><br><span class="line">    json_post = json.dumps(json_post_raw)  <span class="comment"># 将JSON数据转换为字符串</span></span><br><span class="line">    json_post_list = json.loads(json_post)  <span class="comment"># 将字符串转换为Python对象</span></span><br><span class="line">    prompt = json_post_list.get(<span class="string">'prompt'</span>)  <span class="comment"># 获取请求中的提示</span></span><br><span class="line">    history = json_post_list.get(<span class="string">'history'</span>)  <span class="comment"># 获取请求中的历史记录</span></span><br><span class="line">    max_length = json_post_list.get(<span class="string">'max_length'</span>)  <span class="comment"># 获取请求中的最大长度</span></span><br><span class="line">    top_p = json_post_list.get(<span class="string">'top_p'</span>)  <span class="comment"># 获取请求中的top_p参数</span></span><br><span class="line">    temperature = json_post_list.get(<span class="string">'temperature'</span>)  <span class="comment"># 获取请求中的温度参数</span></span><br><span class="line">    <span class="comment"># 调用模型进行对话生成</span></span><br><span class="line">    response, history = model.chat(</span><br><span class="line">        tokenizer,</span><br><span class="line">        prompt,</span><br><span class="line">        history=history,</span><br><span class="line">        max_length=max_length <span class="keyword">if</span> max_length <span class="keyword">else</span> <span class="number">2048</span>,  <span class="comment"># 如果未提供最大长度，默认使用2048</span></span><br><span class="line">        top_p=top_p <span class="keyword">if</span> top_p <span class="keyword">else</span> <span class="number">0.7</span>,  <span class="comment"># 如果未提供top_p参数，默认使用0.7</span></span><br><span class="line">        temperature=temperature <span class="keyword">if</span> temperature <span class="keyword">else</span> <span class="number">0.95</span>  <span class="comment"># 如果未提供温度参数，默认使用0.95</span></span><br><span class="line">    )</span><br><span class="line">    now = datetime.datetime.now()  <span class="comment"># 获取当前时间</span></span><br><span class="line">    time = now.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)  <span class="comment"># 格式化时间为字符串</span></span><br><span class="line">    <span class="comment"># 构建响应JSON</span></span><br><span class="line">    answer = {</span><br><span class="line">        <span class="string">"response"</span>: response,</span><br><span class="line">        <span class="string">"history"</span>: history,</span><br><span class="line">        <span class="string">"status"</span>: <span class="number">200</span>,</span><br><span class="line">        <span class="string">"time"</span>: time</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 构建日志信息</span></span><br><span class="line">    log = <span class="string">"["</span> + time + <span class="string">"] "</span> + <span class="string">'", prompt:"'</span> + prompt + <span class="string">'", response:"'</span> + <span class="built_in">repr</span>(response) + <span class="string">'"'</span></span><br><span class="line">    <span class="built_in">print</span>(log)  <span class="comment"># 打印日志</span></span><br><span class="line">    torch_gc()  <span class="comment"># 执行GPU内存清理</span></span><br><span class="line">    <span class="keyword">return</span> answer  <span class="comment"># 返回响应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主函数入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 加载预训练的分词器和模型</span></span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, device_map=<span class="string">"auto"</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line">    model.generation_config = GenerationConfig.from_pretrained(<span class="string">"Qwen/Qwen-7B-Chat"</span>, trust_remote_code=<span class="literal">True</span>) <span class="comment"># 可指定</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    <span class="comment"># 启动FastAPI应用，用6006端口映射到本地，从而在本地使用api</span></span><br><span class="line">    uvicorn.run(app, host=<span class="string">'0.0.0.0'</span>, port=<span class="number">6006</span>, workers=<span class="number">1</span>)  <span class="comment"># 在指定端口和主机上启动应用</span></span><br></pre></td></tr></tbody></table></figure><h3 id="运行及使用API"><a href="#运行及使用API" class="headerlink" title="运行及使用API"></a>运行及使用API</h3><p>我们运行方式很简单，直接在服务器终端运行</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python api.py</span><br></pre></td></tr></tbody></table></figure><p>加载完毕后出现如下信息说明成功。</p><p><img src="https://picx.zhimg.com/80/v2-25146b4a0f3b5a0fa70782b1ed749e5d_720w.png?source=d16d100b" alt="运行方式"></p><p>默认部署在 6006 端口，通过 POST 方法进行调用，可以使用curl调用，如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST "http://127.0.0.1:6006" \</span><br><span class="line">     -H 'Content-Type: application/json' \</span><br><span class="line">     -d '{"prompt": "你好", "history": []}'</span><br></pre></td></tr></tbody></table></figure><p>也可以使用python中的requests库进行调用，如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">def get_completion(prompt):</span><br><span class="line">    headers = {'Content-Type': 'application/json'}</span><br><span class="line">    data = {"prompt": prompt, "history": []}</span><br><span class="line">    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))</span><br><span class="line">    return response.json()['response']</span><br><span class="line"></span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    print(get_completion('你好'))</span><br></pre></td></tr></tbody></table></figure><p>得到的返回值如下所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  "response":"你好！很高兴为你服务。有什么我可以帮助你的吗？",</span><br><span class="line">  "history":[["你好","你好！很高兴为你服务。有什么我可以帮助你的吗？"]],</span><br><span class="line">  "status":200,</span><br><span class="line">  "time":"2023-11-26 1:14:20"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><img src="https://pic1.zhimg.com/80/v2-284a1395c44b8bb0177dae70fd54b930_720w.png?source=d16d100b" alt="运行结果"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我觉得在有时候我们要实现一个API的时候，我们可以用FastAPI快速实现，并且得到一个不错的结果，这也是我学习的初衷，有时候一些服务器可能可以当做API来使用来调用，其实也方便去使用，也可以部署后成为商业产品，类似于OpenAI一样。</p><p>最后感谢一下FastAPI的文档，让我学习到很多，在里面还有更详细的使用方案，大家也可以去学习一下，然后再感谢一下datawhale的self-llm项目，也是在里面我学习到了使用FastAPI，大家如果对大模型感兴趣也可以关注一下。</p><p>self-llm项目：<a href="https://github.com/datawhalechina/self-llm">https://github.com/datawhalechina/self-llm</a></p><p>FastAPI学习文档：<a href="https://fastapi.tiangolo.com/zh/learn/">https://fastapi.tiangolo.com/zh/learn/</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在学习大模型的时候，有时候会遇到要写API的时候，这个时候我就遇见了FastAPI，我发现这个是一个很好的库，可以很方便的让我们构建一个属于自己的API，所以今天我也写一下这个入门教程和大家一起分享一下，同时也让我们解密一下，OpenAI和一些公司的API，可能是怎么写</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
    <category term="Python" scheme="https://kedreamix.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Linux使用gdown从Google Drive下载文件和文件夹（命令行/代码下载）</title>
    <link href="https://kedreamix.github.io/2024/01/19/Note/gdown/"/>
    <id>https://kedreamix.github.io/2024/01/19/Note/gdown/</id>
    <published>2024-01-19T12:25:30.000Z</published>
    <updated>2024-01-21T10:20:20.277Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、安装gdown"><a href="#一、安装gdown" class="headerlink" title="一、安装gdown"></a>一、安装gdown</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/wkentaro/gdown </span><br><span class="line"><span class="built_in">cd</span> gdown</span><br><span class="line">pip install gdown</span><br></pre></td></tr></tbody></table></figure><h2 id="二、获取Google-Drive文件链接"><a href="#二、获取Google-Drive文件链接" class="headerlink" title="二、获取Google Drive文件链接"></a>二、获取Google Drive文件链接</h2><ol><li>打开Google Drive</li><li>右键点击要下载的文件/文件夹</li><li>选择”获取链接”</li><li>确保文件/文件夹的访问权限设置为”任何人均可访问”</li><li>打开分享链接,复制地址栏中的文件ID,链接前缀都为<code>https://drive.google.com/uc?id=</code>，如<code>https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</code></li><li>复制链接</li></ol><h2 id="三、使用gdown下载"><a href="#三、使用gdown下载" class="headerlink" title="三、使用gdown下载"></a>三、使用gdown下载</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件下载</span></span><br><span class="line"></span><br><span class="line">gdown https://drive.google.com/uc?<span class="built_in">id</span>=&lt;文件ID&gt;</span><br><span class="line"><span class="comment"># gdown https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"><span class="comment"># gdown 1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line">gdown https://drive.google.com/drive/folders/15uNXeRBIhVvZJIhL4yTw4IsStMhUaaxl -O /tmp/folder --folder</span><br></pre></td></tr></tbody></table></figure><p>除了命令行之外，我们也可以通过代码来进行下载</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 文件下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://drive.google.com/file/d/1uFTzwFc3tmS-D7azjMiJcxSfn71BPqKt/view?usp=sharing'</span></span><br><span class="line">output_path = <span class="string">'graph_ML.pk'</span></span><br><span class="line">gdown.download(url, output_path, quiet=<span class="literal">False</span>,fuzzy=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文件夹下载</span></span><br><span class="line"><span class="keyword">import</span> gdown</span><br><span class="line">url = <span class="string">"https://drive.google.com/drive/folders/1HWFHKCprFzR7H7TYhrE-W7v4bz2Vc7Ia"</span></span><br><span class="line"></span><br><span class="line">gdown.download_folder(url, quiet=<span class="literal">True</span>, use_cookies=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><blockquote><p>除此之外，还有一些命令的使用，这里就不过多解释了</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gdown --<span class="built_in">help</span></span><br><span class="line">usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--<span class="built_in">id</span>] [--proxy PROXY]</span><br><span class="line">             [--speed SPEED] [--no-cookies] [--no-check-certificate]</span><br><span class="line">             [--<span class="built_in">continue</span>] [--folder] [--remaining-ok]</span><br><span class="line">             url_or_id</span><br></pre></td></tr></tbody></table></figure></blockquote><h2 id="四、问题解决"><a href="#四、问题解决" class="headerlink" title="四、问题解决"></a>四、问题解决</h2><p>由于Google Drive文件大小限制,直接使用curl/wget下载可能会失败。</p><p>这时需要使用gdown来实现从Google Drive下载大文件。它可以解决由于文件太大导致的curl/wget下载失败问题。</p><p><strong>参考链接</strong></p><p>gdown项目地址: <a href="https://github.com/wkentaro/gdown">https://github.com/wkentaro/gdown</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、安装gdown&quot;&gt;&lt;a href=&quot;#一、安装gdown&quot; class=&quot;headerlink&quot; title=&quot;一、安装gdown&quot;&gt;&lt;/a&gt;一、安装gdown&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tbody&gt;</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
</feed>
