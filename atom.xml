<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-05-13T08:45:28.621Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/NeRF/</id>
    <published>2024-05-13T08:45:28.000Z</published>
    <updated>2024-05-13T08:45:28.621Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>ä¸€é”®å›¾åƒç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€3Dæ¨¡å‹å’Œè§†é¢‘ï¼Œæ˜¯å›¾åƒåˆ°3Dè¡¨ç¤ºæˆ–å›¾åƒ3Dé‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç›¸æ¯”äºåŸå§‹ç¥ç»è¾å°„åœºï¼Œé«˜æ–¯æº…å°„åœ¨éšå¼3Dé‡å»ºä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li><li>ç¨³å®šæ‰©æ•£æ¨¡å‹å¯ç”¨äºæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ä¼ ç»Ÿçš„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥è·å¾—ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ã€‚</li><li>éš¾ä»¥ç”Ÿæˆé•¿å†…å®¹å’Œè¯­ä¹‰è¿ç»­çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dæ–¹æ³•å¯ä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘3Dæ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dä½¿ç”¨åŸºæœ¬çš„é«˜æ–¯æº…å°„æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œå‡å°‘äº†è§†é¢‘å†…å­˜å’Œè®¡ç®—æœºè®¡ç®—éœ€æ±‚ã€‚</li><li>OneTo3Dè®¾è®¡äº†è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ï¼Œç”¨äºå¯¹è±¡éª¨æ¶ã€‚</li><li>ç»“åˆOneTo3Dæå‡ºçš„å¯é‡æ–°ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æä¸æ§åˆ¶ç®—æ³•ï¼Œåœ¨3Dæ¨¡å‹ç²¾ç¡®å®šä½è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®è¾“å…¥æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢ï¼ŒOneTo3Dçš„æ€§èƒ½ä¼˜äºè¯¥é¢†åŸŸçš„SOTAé¡¹ç›®ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ä¸€å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆ</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: æ¾³å¤§åˆ©äºšè«çº³ä»€å¤§å­¦</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>(1): 3D è¡¨å¾æˆ– 3D é‡å»ºé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶éš¾é¢˜ã€‚</p><p>(2): ç°æœ‰çš„ 3D é‡å»ºæ–¹æ³•å¯åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•ç›´æ¥è®¾è®¡å’Œå®Œæˆ 3D é‡å»ºæˆ–å»ºæ¨¡ï¼›éšå¼æ–¹æ³•ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•å’Œç†è®ºå®ç°è¿™äº›ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼ŒNeural Radiance Fields (NeRF) åœ¨éšå¼ 3D è¡¨å¾æˆ–é‡å»ºæ–¹é¢å–å¾—äº†çªå‡ºæˆå°±ã€‚</p><p>(3): æœ¬æ–‡æå‡º OneTo3D æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹å’Œè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„ Gaussian Splatting æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥ç»‘å®šå¯¹è±¡éª¨æ¶ã€‚ç»“åˆé‡æ–°ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æä¸æ§åˆ¶ç®—æ³•ï¼ŒOneTo3D åœ¨ 3D æ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­æ— æ—¶é—´é™åˆ¶çš„ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†ä»¥ä¸‹æˆå°±ï¼šä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹ï¼›ç”Ÿæˆè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ï¼›ç²¾ç¡®æ§åˆ¶ 3D æ¨¡å‹çš„è¿åŠ¨å’ŒåŠ¨ä½œã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å®ç°ä»å•å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ç”Ÿæˆã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):ç”Ÿæˆåˆå§‹3Dæ¨¡å‹ï¼Œä½¿ç”¨DreamGaussianæ¨¡å‹å’ŒZero-1-to-3æ–¹æ³•ï¼›            (2):ç”Ÿæˆå¹¶ç»‘å®šè‡ªé€‚åº”éª¨æ¶ï¼Œè®¾è®¡åŸºæœ¬éª¨æ¶ï¼Œåˆ†æåˆå§‹3Dæ¨¡å‹çš„å‡ ä½•å‚æ•°ä¿¡æ¯ï¼Œå¾®è°ƒéª¨æ¶å‚æ•°ä»¥ä½¿å…¶é€‚åˆå¯¹è±¡çš„èº«ä½“ï¼›            (3):æ–‡æœ¬åˆ°åŠ¨ä½œå’ŒåŠ¨ä½œï¼Œåˆ†æç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤çš„å‘½ä»¤æ„å›¾ï¼Œå°†å‘½ä»¤è½¬æ¢ä¸ºç‰¹å®šåŠ¨ä½œå’Œéª¨æ¶ç›¸å¯¹éª¨éª¼çš„ä¿®æ”¹æ•°æ®ï¼Œæ§åˆ¶ç‰¹å®šéª¨éª¼åœ¨Blenderä¸­å®ç°ç›¸å¯¹è¿åŠ¨ï¼›            (4):èƒŒæ™¯å»é™¤ï¼Œä½¿ç”¨Dreamgaussiançš„process.pyè„šæœ¬æˆ–å…¶ä»–æ–¹æ³•ï¼Œå¯é€‰ä½¿ç”¨å›¾åƒæ£€æµ‹æˆ–è¯­ä¹‰åˆ†å‰²æœºå™¨å­¦ä¹ æ–¹æ³•ï¼›            (5):é¢œè‰²åˆ†ç»„å»é™¤èƒŒæ™¯ï¼Œè®¡ç®—å›¾åƒä¸­æ¯ä¸ªä¸»è¦é¢œè‰²é¡¹çš„æ¯”ä¾‹ï¼Œå°†é¢œè‰²å€¼èŒƒå›´å†…çš„é¢œè‰²é¡¹åˆ’åˆ†ä¸ºä¸åŒçš„é¢œè‰²ç»„ï¼Œå»é™¤é…ç½®æ¯”ä¾‹èŒƒå›´å†…çš„é¢œè‰²ç»„ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬ç¯‡å·¥ä½œæå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„æ–¹æ³•ï¼Œå…·æœ‰ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹ã€ç”Ÿæˆè¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€ç²¾ç¡®æ§åˆ¶ 3D æ¨¡å‹çš„è¿åŠ¨å’ŒåŠ¨ä½œç­‰ä¼˜ç‚¹ï¼Œåœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†åˆ›æ–°ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šOneTo3D æ–¹æ³•é¦–æ¬¡å®ç°äº†ä»å•å¼ å›¾åƒåˆ°å¯é‡æ–°ç¼–è¾‘çš„åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ç”Ÿæˆï¼›æ€§èƒ½ï¼šOneTo3D æ–¹æ³•åœ¨ç”Ÿæˆ 3D æ¨¡å‹çš„ç²¾åº¦ã€è§†é¢‘çš„è¯­ä¹‰è¿ç»­æ€§ã€åŠ¨ä½œæ§åˆ¶çš„ç²¾ç¡®æ€§ç­‰æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šOneTo3D æ–¹æ³•çš„å®ç°éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•å’Œè®¾è®¡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Aerial-NeRF: Adaptive Spatial Partitioning and Sampling for Large-Scale   Aerial Rendering**Authors:Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu**Recent progress in large-scale scene rendering has yielded Neural Radiance Fields (NeRF)-based models with an impressive ability to synthesize scenes across small objects and indoor scenes. Nevertheless, extending this idea to large-scale aerial rendering poses two critical problems. Firstly, a single NeRF cannot render the entire scene with high-precision for complex large-scale aerial datasets since the sampling range along each view ray is insufficient to cover buildings adequately. Secondly, traditional NeRFs are infeasible to train on one GPU to enable interactive fly-throughs for modeling massive images. Instead, existing methods typically separate the whole scene into multiple regions and train a NeRF on each region, which are unaccustomed to different flight trajectories and difficult to achieve fast rendering. To that end, we propose Aerial-NeRF with three innovative modifications for jointly adapting NeRF in large-scale aerial rendering: (1) Designing an adaptive spatial partitioning and selection method based on drones' poses to adapt different flight trajectories; (2) Using similarity of poses instead of (expert) network for rendering speedup to determine which region a new viewpoint belongs to; (3) Developing an adaptive sampling approach for rendering performance improvement to cover the entire buildings at different heights. Extensive experiments have conducted to verify the effectiveness and efficiency of Aerial-NeRF, and new state-of-the-art results have been achieved on two public large-scale aerial datasets and presented SCUTic dataset. Note that our model allows us to perform rendering over 4 times as fast as compared to multiple competitors. Our dataset, code, and model are publicly available at https://drliuqi.github.io/. [PDF](http://arxiv.org/abs/2405.06214v1) **Summary**é’ˆå¯¹å¤§è§„æ¨¡èˆªæ‹åœºæ™¯ï¼Œæˆ‘ä»¬æå‡º Aerial-NeRFï¼Œå®ƒé’ˆå¯¹ NeRF è¿›è¡Œä¸‰é¡¹åˆ›æ–°æ€§ä¿®æ”¹ï¼Œä»¥è”åˆå®ç° NeRF åœ¨å¤§è§„æ¨¡èˆªæ‹æ¸²æŸ“ä¸­çš„è‡ªé€‚åº”ï¼šè‡ªé€‚åº”ç©ºé—´åˆ†åŒºå’Œé€‰æ‹©æ–¹æ³•ã€åŸºäºå§¿æ€ç›¸ä¼¼æ€§çš„å¿«é€Ÿæ¸²æŸ“å’Œè‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ã€‚**Key Takeaways**- æå‡º Aerial-NeRFï¼Œé’ˆå¯¹å¤§è§„æ¨¡èˆªæ‹åœºæ™¯å¯¹ NeRF è¿›è¡Œä¸‰é¡¹åˆ›æ–°æ€§ä¿®æ”¹ã€‚- ä½¿ç”¨è‡ªé€‚åº”ç©ºé—´åˆ†åŒºå’Œé€‰æ‹©æ–¹æ³•ï¼Œæ ¹æ®æ— äººæœºå§¿æ€è‡ªé€‚åº”ä¸åŒçš„é£è¡Œè½¨è¿¹ã€‚- ä½¿ç”¨å§¿æ€ç›¸ä¼¼æ€§ä»£æ›¿ï¼ˆä¸“å®¶ï¼‰ç½‘ç»œè¿›è¡Œæ¸²æŸ“åŠ é€Ÿï¼Œä»¥ç¡®å®šæ–°è§†ç‚¹å±äºå“ªä¸ªåŒºåŸŸã€‚- å¼€å‘è‡ªé€‚åº”é‡‡æ ·æ–¹æ³•ï¼Œä»¥æé«˜æ¸²æŸ“æ€§èƒ½ï¼Œè¦†ç›–ä¸åŒé«˜åº¦çš„æ•´åº§å»ºç­‘ã€‚- å¤§é‡å®éªŒéªŒè¯äº† Aerial-NeRF çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨ä¸¤ä¸ªå…¬å¼€çš„å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†å’Œ SCUTic æ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚- ä¸å¤šä¸ªç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…è®¸æˆ‘ä»¬ä»¥è¶…è¿‡ 4 å€çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚- æˆ‘ä»¬æ¨¡å‹ã€ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€è·å–ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: èˆªæ‹NeRFï¼šå¤§è§„æ¨¡èˆªæ‹æ¸²æŸ“çš„è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé‡‡æ ·</p></li><li><p>Authors: Xiaohan Zhang, Yukui Qiu, Zhenyu Sun, Qi Liu</p></li><li><p>Affiliation: åå—ç†å·¥å¤§å­¦æœªæ¥æŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: View synthesis, large-scale scene rendering, neural radiance fields, fast rendering</p></li><li><p>Urls: https://arxiv.org/abs/2405.06214 , https://github.com/drliuqi/Aerial-NeRF</p></li><li><p>Summary:</p><p>(1):NeRFæ¨¡å‹åœ¨å°ç‰©ä½“å’Œå®¤å†…åœºæ™¯æ¸²æŸ“ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å°†å…¶æ‰©å±•åˆ°èˆªæ‹æ¸²æŸ“ä¸­é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå•ä¸ªNeRFæ— æ³•æ¸²æŸ“å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†ä¸­çš„æ•´ä¸ªåœºæ™¯ï¼Œä¼ ç»ŸNeRFæ— æ³•åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒä»¥å®ç°äº¤äº’å¼æµè§ˆã€‚</p><p>(2):ä»¥å¾€æ–¹æ³•å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªåŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªåŒºåŸŸè®­ç»ƒä¸€ä¸ªNeRFï¼Œä½†è¿™äº›æ–¹æ³•æ— æ³•é€‚åº”ä¸åŒçš„é£è¡Œè½¨è¿¹ï¼Œæ¸²æŸ“é€Ÿåº¦ä¹Ÿè¾ƒæ…¢ã€‚</p><p>(3):æœ¬æ–‡æå‡ºAerial-NeRFï¼Œé€šè¿‡è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé€‰æ‹©ã€åŸºäºå§¿æ€ç›¸ä¼¼æ€§ç¡®å®šåŒºåŸŸå½’å±ã€è‡ªé€‚åº”é‡‡æ ·ç­‰æ–¹æ³•ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</p><p>(4):Aerial-NeRFåœ¨ä¸¤ä¸ªå…¬å¼€å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†å’Œä¸€ä¸ªè‡ªå»ºæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä¼˜æ€§èƒ½ï¼Œæ¸²æŸ“é€Ÿåº¦æ¯”å…¶ä»–æ–¹æ³•å¿«4å€ä»¥ä¸Šã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè‡ªé€‚åº”ç©ºé—´åˆ’åˆ†ï¼šæ ¹æ®èˆªæ‹æ•°æ®é›†çš„ç‰¹å¾ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”ç©ºé—´åˆ’åˆ†æ–¹æ³•ï¼Œå°†å¤§è§„æ¨¡åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªå°åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸä½¿ç”¨ä¸€ä¸ªNeRFè¿›è¡Œæ¸²æŸ“ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåŸºäºå§¿æ€ç›¸ä¼¼æ€§ç¡®å®šåŒºåŸŸå½’å±ï¼šè®¾è®¡äº†ä¸€ç§åŸºäºå§¿æ€ç›¸ä¼¼æ€§çš„åŒºåŸŸå½’å±ç¡®å®šç®—æ³•ï¼Œæ ¹æ®ç›¸æœºçš„å§¿æ€ä¿¡æ¯å°†èˆªæ‹å›¾åƒåˆ†é…åˆ°ä¸åŒçš„åŒºåŸŸï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº”é‡‡æ ·ï¼šæå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼Œæ ¹æ®ä¸åŒåŒºåŸŸçš„å¤æ‚ç¨‹åº¦å’Œæ¸²æŸ“é€Ÿåº¦è¦æ±‚ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç‚¹æ•°ï¼Œæé«˜æ¸²æŸ“æ•ˆç‡ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåŸºäºç¥ç»ç½‘ç»œçš„åŒºåŸŸèåˆï¼šä½¿ç”¨ç¥ç»ç½‘ç»œå°†ä¸åŒåŒºåŸŸçš„æ¸²æŸ“ç»“æœèåˆæˆæœ€ç»ˆå›¾åƒï¼Œä¿è¯æ¸²æŸ“ç»“æœçš„è¿ç»­æ€§å’Œå‡†ç¡®æ€§ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šåŸºäºæ¦‚ç‡å¯†åº¦å‡½æ•°çš„åŠ é€Ÿé‡‡æ ·ï¼šåˆ©ç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°å¯¹é‡‡æ ·ç‚¹è¿›è¡Œä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜æ¸²æŸ“é€Ÿåº¦ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† Aerial-NeRFï¼Œä¸€ç§ç”¨äºå¤„ç†å¤§è§„æ¨¡èˆªæ‹æ•°æ®é›†çš„é«˜æ•ˆä¸”é²æ£’çš„æ¸²æŸ“æ–¹æ³•ï¼Œåœ¨æ¸²æŸ“é€Ÿåº¦ä¸Šå¤§å¹…ä¼˜äºç°æœ‰çš„åŒç±»æ–¹æ³•ï¼Œå‡ ä¹è¾¾åˆ° 4 å€ã€‚æ­¤å¤–ï¼Œåœ¨é€‚å½“çš„åˆ’åˆ†åŒºåŸŸæ•°é‡ä¸‹ï¼ŒAerial-NeRF å¯ä»¥ä½¿ç”¨å•ä¸ª GPU æ¸²æŸ“ä»»æ„å¤§çš„åœºæ™¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¸ºèˆªæ‹åœºæ™¯å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿé€šè¿‡ä¸åŒé«˜åº¦ç›¸æœºçš„é‡‡æ ·èŒƒå›´è¦†ç›–å»ºç­‘ç‰©ã€‚ä¸ SOTA æ¨¡å‹è¿›è¡Œæ›´å¹¿æ³›çš„æ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾æ›´æœ‰æ•ˆï¼ˆä»…ä½¿ç”¨ 1/4 çš„é‡‡æ ·ç‚¹å’Œ 2 GB çš„ GPU å†…å­˜èŠ‚çœï¼‰ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªå¸¸ç”¨æŒ‡æ ‡æ–¹é¢å…·æœ‰å¯æ¯”æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº† SCUTicï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡å¤§å­¦æ ¡å›­åœºæ™¯çš„æ–°å‹èˆªæ‹æ•°æ®é›†ï¼Œå…·æœ‰ä¸å‡åŒ€çš„ç›¸æœºè½¨è¿¹ï¼Œå¯ä»¥éªŒè¯æ¸²æŸ“æ–¹æ³•çš„é²æ£’æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šè‡ªé€‚åº”ç©ºé—´åˆ’åˆ†å’Œé‡‡æ ·ï¼›æ€§èƒ½ï¼šæ¸²æŸ“é€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨ä½ï¼›å·¥ä½œé‡ï¼šæ•°æ®é›†æ„å»ºå’Œæ¨¡å‹è®­ç»ƒå¤æ‚åº¦è¾ƒé«˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-3f9706ee7489efbc0fffd098a133920f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5cd3300322f846160033228b8f55d45.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7172b9e2d3611b5ec9915962744d54fa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07281c002ad9f4eaef4b0c58ebbaf426.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a2778deaeb7d026e2fd79cf4c5e6e409.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d55c494ec8f956acc30da13f5d75881b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac7e04c419fb74632e6c7f9332f81960.jpg" align="middle"></details>## Residual-NeRF: Learning Residual NeRFs for Transparent Object   Manipulation**Authors:Bardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski**Transparent objects are ubiquitous in industry, pharmaceuticals, and households. Grasping and manipulating these objects is a significant challenge for robots. Existing methods have difficulty reconstructing complete depth maps for challenging transparent objects, leaving holes in the depth reconstruction. Recent work has shown neural radiance fields (NeRFs) work well for depth perception in scenes with transparent objects, and these depth maps can be used to grasp transparent objects with high accuracy. NeRF-based depth reconstruction can still struggle with especially challenging transparent objects and lighting conditions. In this work, we propose Residual-NeRF, a method to improve depth perception and training speed for transparent objects. Robots often operate in the same area, such as a kitchen. By first learning a background NeRF of the scene without transparent objects to be manipulated, we reduce the ambiguity faced by learning the changes with the new object. We propose training two additional networks: a residual NeRF learns to infer residual RGB values and densities, and a Mixnet learns how to combine background and residual NeRFs. We contribute synthetic and real experiments that suggest Residual-NeRF improves depth perception of transparent objects. The results on synthetic data suggest Residual-NeRF outperforms the baselines with a 46.1% lower RMSE and a 29.5% lower MAE. Real-world qualitative experiments suggest Residual-NeRF leads to more robust depth maps with less noise and fewer holes. Website: https://residual-nerf.github.io [PDF](http://arxiv.org/abs/2405.06181v1) **Summary**é€æ˜ç‰©ä½“åœ¨å·¥ä¸šã€åŒ»è¯å’Œå®¶åº­ä¸­æ— å¤„ä¸åœ¨ï¼Œæœºå™¨äººåœ¨æŠ“å–å’Œæ“ä½œè¿™äº›ç‰©ä½“æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚**Key Takeaways**- NeRFs å¯¹åŒ…å«é€æ˜ç‰©ä½“çš„åœºæ™¯ä¸­çš„æ·±åº¦æ„ŸçŸ¥æ•ˆæœå¾ˆå¥½ã€‚- NeRFs åœ¨å¤„ç†æå…·æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“å’Œå…‰ç…§æ¡ä»¶æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚- Residual-NeRF æå‡ºäº†ä¸€ç§æ”¹å–„é€æ˜ç‰©ä½“æ·±åº¦æ„ŸçŸ¥å’Œè®­ç»ƒé€Ÿåº¦çš„æ–¹æ³•ã€‚- é¦–å…ˆå­¦ä¹ åœºæ™¯ä¸­ä¸åŒ…å«å¾…æ“ä½œé€æ˜ç‰©ä½“çš„èƒŒæ™¯ NeRFï¼Œå¯ä»¥å‡å°‘å­¦ä¹ æ–°ç‰©ä½“å˜åŒ–å¸¦æ¥çš„æ­§ä¹‰ã€‚- Residual-NeRF å­¦ä¹ æ¨æ–­æ®‹å·® RGB å€¼å’Œå¯†åº¦ï¼ŒMixnet å­¦ä¹ å¦‚ä½•ç»„åˆèƒŒæ™¯å’Œæ®‹å·® NeRFã€‚- åœ¨åˆæˆæ•°æ®ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒResidual-NeRF çš„ RMSE ä½ 46.1%ï¼ŒMAE ä½ 29.5%ã€‚- çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒè¡¨æ˜ï¼ŒResidual-NeRF èƒ½å¤Ÿç”Ÿæˆæ›´é²æ£’çš„æ·±åº¦å›¾ï¼Œå™ªå£°æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šResidual-NeRFï¼šå­¦ä¹ æ®‹å·® NeRF ä»¥å®ç°é€æ˜ç‰©ä½“æ“ä½œ</p></li><li><p>ä½œè€…ï¼šBardienus P. Duisterhof, Yuemin Mao, Si Heng Teng, Jeffrey Ichnowski</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¡å†…åŸºæ¢…éš†å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€æ·±åº¦æ„ŸçŸ¥ã€é€æ˜ç‰©ä½“ã€æ®‹å·®å­¦ä¹ ã€èƒŒæ™¯å…ˆéªŒ</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.06181   Github é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š      é€æ˜ç‰©ä½“åœ¨å·¥ä¸šã€åŒ»è¯å’Œå®¶åº­ä¸­æ— å¤„ä¸åœ¨ã€‚æŠ“å–å’Œæ“çºµè¿™äº›ç‰©ä½“å¯¹æœºå™¨äººæ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•éš¾ä»¥é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“çš„å®Œæ•´æ·±åº¦å›¾ï¼Œä»è€Œåœ¨æ·±åº¦é‡å»ºä¸­ç•™ä¸‹å­”æ´ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰éå¸¸é€‚åˆåœ¨æœ‰é€æ˜ç‰©ä½“çš„åœºæ™¯ä¸­è¿›è¡Œæ·±åº¦æ„ŸçŸ¥ï¼Œå¹¶ä¸”è¿™äº›æ·±åº¦å›¾å¯ç”¨äºé«˜ç²¾åº¦åœ°æŠ“å–é€æ˜ç‰©ä½“ã€‚åŸºäº NeRF çš„æ·±åº¦é‡å»ºä»ç„¶éš¾ä»¥å¤„ç†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“å’Œç…§æ˜æ¡ä»¶ã€‚</p><p>(2)ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š      Dex-NeRF å’Œ Evo-NeRF ç­‰æ–¹æ³•è¡¨æ˜ï¼ŒNeRF åœ¨é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¿˜è¡¨æ˜ï¼ŒNeRF å¾€å¾€éš¾ä»¥å¤„ç†ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é€æ˜ç‰©ä½“ï¼Œä¾‹å¦‚å…·æœ‰æŒ‘æˆ˜æ€§å…‰ç…§æ¡ä»¶çš„é…’æ¯æˆ–å¨æˆ¿é”¡ç®”ã€‚é€æ˜ç‰©ä½“çš„æŒ‘æˆ˜æºäºç¼ºä¹ç‰¹å¾ä»¥åŠå¤–è§‚ä¸­å¾ˆå¤§çš„è§†ç‚¹ä¾èµ–æ€§å˜åŒ–ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š      ä¸ºäº†æé«˜é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæˆ‘ä»¬æå‡ºäº† Residual-NeRFã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæœºå™¨äººçš„å·¥ä½œåŒºåŸŸçš„å‡ ä½•å½¢çŠ¶ä¸»è¦æ˜¯é™æ€ä¸”ä¸é€æ˜çš„ï¼Œä¾‹å¦‚æ¶å­ã€æ¡Œå­å’Œæ¡Œå­ã€‚Residual-NeRF åˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œä»¥å‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ã€‚Residual-NeRF é¦–å…ˆé€šè¿‡è®­ç»ƒä¸åŒ…å«é€æ˜ç‰©ä½“çš„å›¾åƒæ¥å­¦ä¹ æ•´ä¸ªåœºæ™¯çš„èƒŒæ™¯ NeRFã€‚ç„¶åï¼ŒResidual-NeRF ä½¿ç”¨åŒ…å«é€æ˜ç‰©ä½“çš„å®Œæ•´åœºæ™¯çš„å›¾åƒæ¥å­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetã€‚</p><p>(4)ï¼šæ–¹æ³•çš„åº”ç”¨ä»»åŠ¡å’Œæ€§èƒ½ï¼š      æˆ‘ä»¬å¯¹åˆæˆå’ŒçœŸå®æ•°æ®è¿›è¡Œäº†å®éªŒï¼Œè¡¨æ˜ Residual-NeRF æé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ã€‚åˆæˆæ•°æ®ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒResidual-NeRF åœ¨ RMSE ä¸Šæ¯”åŸºçº¿ä½ 46.1%ï¼Œåœ¨ MAE ä¸Šä½ 29.5%ã€‚çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒè¡¨æ˜ï¼ŒResidual-NeRF äº§ç”Ÿäº†æ›´ç¨³å¥çš„æ·±åº¦å›¾ï¼Œå™ªç‚¹æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šé¦–å…ˆè®­ç»ƒä¸åŒ…å«é€æ˜ç‰©ä½“çš„å›¾åƒï¼Œå­¦ä¹ æ•´ä¸ªåœºæ™¯çš„èƒŒæ™¯ NeRFï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç„¶åä½¿ç”¨åŒ…å«é€æ˜ç‰©ä½“çš„å®Œæ•´åœºæ™¯çš„å›¾åƒï¼Œå­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œå‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šResidual-NeRF æé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ã€‚</p><p><strong>ç»“è®º</strong></p><p>(1): æœ¬å·¥ä½œé€šè¿‡æå‡º Residual-NeRFï¼Œæé«˜äº†é€æ˜ç‰©ä½“çš„æ·±åº¦æ„ŸçŸ¥ï¼Œå¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼š    - åˆ©ç”¨åœºæ™¯çš„é™æ€å’Œä¸é€æ˜éƒ¨åˆ†ä½œä¸ºå…ˆéªŒï¼Œå‡å°‘æ­§ä¹‰å¹¶æé«˜æ·±åº¦æ„ŸçŸ¥ï¼›    - æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé¦–å…ˆå­¦ä¹ èƒŒæ™¯ NeRFï¼Œç„¶åå­¦ä¹ æ®‹å·® NeRF å’Œ Mixnetã€‚    - æå‡ºäº†ä¸€ç§æ–°çš„ Mixnetï¼Œå¯ä»¥æœ‰æ•ˆåœ°èåˆèƒŒæ™¯ NeRF å’Œæ®‹å·® NeRF çš„è¾“å‡ºã€‚</p><pre><code>æ€§èƒ½ï¼š- åœ¨åˆæˆæ•°æ®ä¸Šï¼ŒResidual-NeRF åœ¨ RMSE ä¸Šæ¯”åŸºçº¿ä½ 46.1%ï¼Œåœ¨ MAE ä¸Šä½ 29.5%ã€‚- åœ¨çœŸå®ä¸–ç•Œçš„å®šæ€§å®éªŒä¸­ï¼ŒResidual-NeRF äº§ç”Ÿäº†æ›´ç¨³å¥çš„æ·±åº¦å›¾ï¼Œå™ªç‚¹æ›´å°‘ï¼Œå­”æ´æ›´å°‘ã€‚å·¥ä½œé‡ï¼š- è®­ç»ƒ Residual-NeRF éœ€è¦ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œè¿™æ¯”åŸºçº¿æ–¹æ³•æ›´å¤æ‚ã€‚- Residual-NeRF éœ€è¦é¢å¤–çš„å†…å­˜æ¥å­˜å‚¨èƒŒæ™¯ NeRF å’Œæ®‹å·® NeRF çš„æƒé‡ã€‚</code></pre><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b7d7618a421bd8b0947856c3ea91116f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1b6857403dd3f1eeafdb70f45e5b92e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4d0e4ee50f9ead394b9fcd552ae92106.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b39c7bfcb8005d9c40b2eac38f3ed56.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0aaa94e0b48b25617be15c8888555cae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a7c646c3420664f87093b1eb3a62bfba.jpg" align="middle"></details>## NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior**Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh**Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. [PDF](http://arxiv.org/abs/2405.05749v2) 11 pages, 5 figures**Summary**é€šè¿‡è§£å†³å•å¼ å›¾åƒéŸ³é¢‘é©±åŠ¨3D Talking Headç”Ÿæˆä¸­çš„3Dä¸€è‡´æ€§é—®é¢˜ï¼ŒNeRFFaceSpeech æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ 3Dæ„ŸçŸ¥çš„ Talking Headã€‚**Key Takeaways**- è§£å†³å•å¼ å›¾åƒéŸ³é¢‘é©±åŠ¨ 3D Talking Head ç”Ÿæˆçš„ 3D ä¸€è‡´æ€§é—®é¢˜ã€‚- ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRF ç›¸ç»“åˆï¼Œæ„å»ºå¯¹åº”äºå•å¼ å›¾åƒçš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚- å¼•å…¥ç©ºé—´åŒæ­¥æ–¹æ³•ï¼Œåˆ©ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å°„çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ã€‚- å¼•å…¥ LipaintNet è¡¥å……å†…å˜´åŒºåŸŸä¸­ç¼ºå¤±çš„ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯æ— æ³•ä»ç»™å®šçš„å•å¼ å›¾åƒä¸­è·å¾—ã€‚- ä»¥è‡ªç›‘ç£çš„æ–¹å¼è®­ç»ƒç½‘ç»œï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚- æå‡ºä¸€ç§å®šé‡æ–¹æ³•æ¥è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ï¼Œè¿™åœ¨ä»¥å‰åªèƒ½é€šè¿‡å®šæ€§æ–¹å¼è¿›è¡Œã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: NeRFFaceSpeech: ä¸€æ¬¡æ€§éŸ³é¢‘é©±åŠ¨ 3D è¯´è¯äººå¤´éƒ¨åˆæˆï¼Œé€šè¿‡ç”Ÿæˆå…ˆéªŒ</p></li><li><p>Authors: Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</p></li><li><p>Affiliation: é¦–å°”å›½ç«‹å¤§å­¦</p></li><li><p>Keywords: éŸ³é¢‘é©±åŠ¨, 3D è¯´è¯äººå¤´éƒ¨, ç¥ç»è¾å°„åœº, ç”Ÿæˆå…ˆéªŒ, ä¸€æ¬¡æ€§å­¦ä¹ </p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05749, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ­£ä» 2D å†…å®¹è½¬å‘ 3D å†…å®¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåˆæˆé«˜è´¨é‡ 3D è¯´è¯äººå¤´éƒ¨è¾“å‡ºçš„ä¸€ç§æ‰‹æ®µè€Œå¤‡å—å…³æ³¨ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç§åŸºäº NeRF çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é…å¯¹çš„æ¯ä¸ªèº«ä»½çš„éŸ³é¢‘è§†è§‰æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚å°½ç®¡å·²ç»å°è¯•ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ 3D è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœé€šå¸¸ä¸ä»¤äººæ»¡æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨è§£å†³ä¸€æ¬¡æ€§éŸ³é¢‘é©±åŠ¨åŸŸä¸­è¢«å¿½è§†çš„ 3D ä¸€è‡´æ€§æ–¹é¢ï¼Œå…¶ä¸­é¢éƒ¨åŠ¨ç”»ä¸»è¦åœ¨æ­£é¢è§†è§’åˆæˆã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•ï¼š- åŸºäº NeRF çš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é…å¯¹çš„æ¯ä¸ªèº«ä»½çš„éŸ³é¢‘è§†è§‰æ•°æ®ã€‚- ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ 3D è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–¹æ³•ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœé€šå¸¸ä¸ä»¤äººæ»¡æ„ã€‚</p><p>é—®é¢˜ï¼š- å¯æ‰©å±•æ€§å—é™ã€‚- 3D ä¸€è‡´æ€§ä¸è¶³ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š- æå‡ºäº†ä¸€ç§æ–°æ–¹æ³• NeRFFaceSpeechï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D æ„ŸçŸ¥è¯´è¯äººå¤´éƒ¨ã€‚- ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRF ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ„å»ºä¸€ä¸ªä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚- æˆ‘ä»¬çš„ç©ºé—´åŒæ­¥æ–¹æ³•é‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ã€‚- æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† LipaintNetï¼Œå®ƒå¯ä»¥è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…å£åŒºåŸŸä¸­ç¼ºå°‘çš„ä¿¡æ¯ã€‚è¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚</p><p>(4): åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å–å¾—äº†ä»¥ä¸‹æˆå°±ï¼š- å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº† 3D ä¸€è‡´æ€§ã€‚- æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè€Œä»¥å‰åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚</p><ol><li>Methods:</li></ol><p>(1): æå‡º NeRFFaceSpeech æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ä¸ NeRFï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›</p><p>(2): é‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ï¼›</p><p>(3): å¼•å…¥ LipaintNetï¼Œè¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…å£åŒºåŸŸä¸­ç¼ºå°‘çš„ä¿¡æ¯ï¼Œè¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ï¼›</p><p>(4): è®¾è®¡å®šé‡æ–¹æ³•è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† NeRFFaceSpeechï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒæ„å»ºå’Œæ“çºµ 3D ç‰¹å¾ï¼Œä»å•å¹…å›¾åƒç”Ÿæˆ 3D æ„ŸçŸ¥éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–°æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†ç”Ÿæˆæ¨¡å‹å…ˆéªŒä¸ç¥ç»è¾å°„åœºç›¸ç»“åˆï¼Œæ„å»ºä¸å•å¹…å›¾åƒç›¸å¯¹åº”çš„ 3D ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›é€šè¿‡å…‰çº¿å˜å½¢ï¼Œé‡‡ç”¨å‚æ•°åŒ–äººè„¸æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œå°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ 3D é¢éƒ¨è¿åŠ¨ï¼›å¼•å…¥äº† LipaintNetï¼Œä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼Œè¡¥å……å˜å½¢åœºä»¥äº§ç”Ÿå¯è¡Œç»“æœï¼›è®¾è®¡äº†å®šé‡æ–¹æ³•æ¥è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–çš„é²æ£’æ€§ã€‚æ€§èƒ½ï¼šä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»å•å¹…å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å¢å¼ºäº† 3D ä¸€è‡´æ€§ï¼›é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè€Œä»¥å‰åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºå’Œè¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details>## Tactile-Augmented Radiance Fields**Authors:Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens**We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision and touch into a shared 3D space. This representation can be used to estimate the visual and tactile signals for a given 3D position within a scene. We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes. Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras and thus can be registered to images using methods from multi-view geometry, and (ii) visually and structurally similar regions of a scene share the same tactile features. We use these insights to register touch signals to a captured visual scene, and to train a conditional diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its corresponding tactile signal. To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal. We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile data on several downstream tasks. Project page: https://dou-yiming.github.io/TaRF [PDF](http://arxiv.org/abs/2405.04534v1) CVPR 2024, Project page: https://dou-yiming.github.io/TaRF, Code:   https://github.com/Dou-Yiming/TaRF/**Summary**è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœºå°†è§†è§‰å’Œè§¦è§‰å¸¦å…¥å…±äº«çš„ 3D ç©ºé—´ï¼Œèƒ½å¤Ÿä¼°è®¡åœºæ™¯ä¸­ç»™å®š 3D ä½ç½®çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ã€‚**Key Takeaways**- è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœº (TaRF) ç»“åˆäº†è§†è§‰å’Œè§¦è§‰ä¿¡å·ï¼Œç”¨äºä¼°è®¡åœºæ™¯ä¸­ç»™å®š 3D ä½ç½®çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ã€‚- TaRF ç”±ç…§ç‰‡å’Œç¨€ç–é‡‡æ ·çš„è§¦è§‰æ¢é’ˆé‡‡é›†è€Œæˆã€‚- è§†è§‰è§¦è§‰å¢å¼ºè¾å°„åœºåˆ©ç”¨äº†è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨å¯ä¸å›¾åƒé…å‡†ä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼çš„åŒºåŸŸå…±äº«ç›¸åŒè§¦è§‰ç‰¹å¾çš„è§è§£ã€‚- è§¦è§‰ä¿¡å·é€šè¿‡é…å‡†å’Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸æ•è·çš„è§†è§‰åœºæ™¯ç›¸å…³è”ã€‚- TaRF æ•°æ®é›†åŒ…å«æ¯”ä»¥å¾€çœŸå®ä¸–ç•Œæ•°æ®é›†æ›´å¤šçš„è§¦è§‰æ ·æœ¬ï¼Œå¹¶ä¸ºæ¯ä¸ªæ•è·çš„è§¦è§‰ä¿¡å·æä¾›äº†ç©ºé—´å¯¹é½çš„è§†è§‰ä¿¡å·ã€‚- è·¨æ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‡†ç¡®æ€§å·²å¾—åˆ°éªŒè¯ï¼Œä¸”å·²åœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­è¯æ˜äº†æ•è·çš„è§†è§‰è§¦è§‰æ•°æ®çš„å®ç”¨æ€§ã€‚- é¡¹ç›®ä¸»é¡µï¼šhttps://dou-yiming.github.io/TaRF**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: è§¦è§‰å¢å¼ºè¾å°„åœºï¼šä¸€ç§ç”¨äºè·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆçš„æ–°å‹åœºæ™¯è¡¨ç¤º</p></li><li><p>Authors: Douyi Ming, Srinath Sridhar, Jiajun Wu, Angjoo Kanazawa, Peter Anderson, Wojciech Matusik, Jonathan Ragan-Kelley</p></li><li><p>Affiliation: éº»çœç†å·¥å­¦é™¢</p></li><li><p>Keywords: Cross-modal perception, generative models, multi-view geometry, neural radiance fields, tactile sensing, vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2301.09422, Github: https://github.com/douyiming/TaRF</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯è§†è§‰å’Œè§¦è§‰æ„ŸçŸ¥åœ¨å…±äº«çš„ 3D ç©ºé—´ä¸­çš„è¡¨ç¤ºé—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•é€šå¸¸æ˜¯å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·è§†ä¸ºç‹¬ç«‹çš„æ¨¡æ€ï¼Œè¿™é™åˆ¶äº†è·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆçš„ä»»åŠ¡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºå½¢å¼â€”â€”è§¦è§‰å¢å¼ºè¾å°„åœº (TaRF)ï¼Œå®ƒå°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­ã€‚TaRF çš„æ„å»ºåˆ©ç”¨äº†åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨ä¸å›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ã€‚</p><p>(4): åœ¨ TaRF æ•°æ®é›†ä¸Šï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨è·¨æ¨¡æ€ç”Ÿæˆã€è§¦è§‰ä¿¡å·ä¼°è®¡å’Œè§¦è§‰å¼•å¯¼çš„è§†è§‰æ¢ç´¢ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæ„å»ºè§†è§‰å’Œè§¦è§‰å¢å¼ºè¾å°„åœºï¼ˆTaRFï¼‰ï¼Œå°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡è§†è§‰-è§¦è§‰å¯¹åº”å…³ç³»å’Œåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œå»ºç«‹ TaRFï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨å’Œå›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä¼°è®¡ TaRF ä¸­çš„è§†è§‰å’Œè§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¼°è®¡åœºæ™¯ä¸­å…¶ä»–ä½ç½®çš„è§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šä½¿ç”¨æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»æ¸²æŸ“çš„è§†è§‰ä¿¡å·ä¸­é¢„æµ‹è§¦è§‰ä¿¡å·ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šæ”¶é›†åŒ…å« 19.3k ä¸ªå›¾åƒå¯¹çš„è§†è§‰-è§¦è§‰æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼° TaRFã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯è¡¨ç¤ºå½¢å¼â€”â€”è§¦è§‰å¢å¼ºè¾å°„åœºï¼ˆTaRFï¼‰ï¼Œé¦–æ¬¡å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ªå…±äº«çš„ 3D ç©ºé—´ä¸­ï¼Œä¸ºè·¨æ¨¡æ€æ„ŸçŸ¥å’Œç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å°†è§†è§‰å’Œè§¦è§‰ä¿¡å·ç»Ÿä¸€åˆ°ä¸€ä¸ª 3D ç©ºé—´ä¸­çš„åœºæ™¯è¡¨ç¤ºå½¢å¼ TaRFï¼›æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰-è§¦è§‰å¯¹åº”å…³ç³»å’Œåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œå»ºç«‹ TaRF çš„æ–¹æ³•ï¼›æå‡ºäº†ä¸€ç§ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œä¼°è®¡åœºæ™¯ä¸­å…¶ä»–ä½ç½®çš„è§¦è§‰ä¿¡å·çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šåœ¨è·¨æ¨¡æ€ç”Ÿæˆã€è§¦è§‰ä¿¡å·ä¼°è®¡å’Œè§¦è§‰å¼•å¯¼çš„è§†è§‰æ¢ç´¢ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼›æ”¶é›†äº†åŒ…å« 19.3k ä¸ªå›¾åƒå¯¹çš„è§†è§‰-è§¦è§‰æ•°æ®é›†ï¼Œä¸º TaRF çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†ä¸°å¯Œçš„ç´ æã€‚ workloadï¼šTaRF çš„æ„å»ºéœ€è¦åŸºäºè§†è§‰çš„è§¦è§‰ä¼ æ„Ÿå™¨å’Œå›¾åƒä¹‹é—´çš„å‡ ä½•å¯¹åº”å…³ç³»ï¼Œä»¥åŠåœºæ™¯ä¸­è§†è§‰å’Œç»“æ„ç›¸ä¼¼åŒºåŸŸå…·æœ‰ç›¸åŒè§¦è§‰ç‰¹å¾çš„å‡è®¾ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å­˜åœ¨æŒ‘æˆ˜ï¼›TaRF çš„è®­ç»ƒéœ€è¦å¤§é‡çš„è§†è§‰-è§¦è§‰æ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å·¥ä½œé‡ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5322ab124785e1ed8207592748379b4a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a5022ff2c6b665ce2b96a8b7b9f166a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-929d0d52fcc6b94f08fe05a010b4ea04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-288d198e1ad4f685777631680ccf4209.jpg" align="middle"><img src="https://pica.zhimg.com/v2-59b2afe338ddf3888c68d0443ec0d04f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ec71c15419dcda442892e2bc1a105da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-432dac236eec2115922c4f0698f51eec.jpg" align="middle"></details><h2 id="DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid"><a href="#DistGrid-Scalable-Scene-Reconstruction-with-Distributed-Multi-resolution-Hash-Grid" class="headerlink" title="DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid"></a>DistGrid: Scalable Scene Reconstruction with Distributed   Multi-resolution Hash Grid</h2><p><strong>Authors:Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</strong></p><p>Neural Radiance Field~(NeRF) achieves extremely high quality in object-scaled and indoor scene reconstruction. However, there exist some challenges when reconstructing large-scale scenes. MLP-based NeRFs suffer from limited network capacity, while volume-based NeRFs are heavily memory-consuming when the scene resolution increases. Recent approaches propose to geographically partition the scene and learn each sub-region using an individual NeRF. Such partitioning strategies help volume-based NeRF exceed the single GPU memory limit and scale to larger scenes. However, this approach requires multiple background NeRF to handle out-of-partition rays, which leads to redundancy of learning. Inspired by the fact that the background of current partition is the foreground of adjacent partition, we propose a scalable scene reconstruction method based on joint Multi-resolution Hash Grids, named DistGrid. In this method, the scene is divided into multiple closely-paved yet non-overlapped Axis-Aligned Bounding Boxes, and a novel segmented volume rendering method is proposed to handle cross-boundary rays, thereby eliminating the need for background NeRFs. The experiments demonstrate that our method outperforms existing methods on all evaluated large-scale scenes, and provides visually plausible scene reconstruction. The scalability of our method on reconstruction quality is further evaluated qualitatively and quantitatively. </p><p><a href="http://arxiv.org/abs/2405.04416v2">PDF</a> Originally submitted to Siggraph Asia 2023</p><p><strong>Summary</strong><br>å¤§è§„æ¨¡åœºæ™¯åªéœ€ç”¨å•ä¸€NeRFï¼Œé€šè¿‡å¤šçº§Hashç½‘æ ¼ï¼Œè€Œæ— éœ€å•ç‹¬çš„èƒŒæ™¯NeRFï¼Œå³å¯å®ç°åœºæ™¯é‡å»ºã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF åœ¨å¯¹è±¡å’Œå®¤å†…åœºæ™¯é‡å»ºä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é‡å»ºå¤§å‹åœºæ™¯æ—¶å­˜åœ¨é—®é¢˜ã€‚</li><li>åŸºäº MLP çš„ NeRF ç½‘ç»œå®¹é‡æœ‰é™ï¼Œè€ŒåŸºäºä½“ç§¯çš„ NeRF ä¼šéšç€åœºæ™¯åˆ†è¾¨ç‡çš„å¢åŠ å ç”¨å¤§é‡å†…å­˜ã€‚</li><li>æœ€è¿‘çš„æ–¹æ³•å°†åœºæ™¯åœ°ç†åˆ†åŒºï¼Œå¹¶ä½¿ç”¨å•ç‹¬çš„ NeRF å­¦ä¹ æ¯ä¸ªåˆ†åŒºã€‚</li><li>è¿™æœ‰åŠ©äºåŸºäºä½“ç§¯çš„ NeRF çªç ´å• GPU å†…å­˜é™åˆ¶ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤§çš„åœºæ™¯ã€‚</li><li>ä½†è¿™ç§æ–¹æ³•éœ€è¦å¤šä¸ªèƒŒæ™¯ NeRF æ¥å¤„ç†åˆ†åŒºå¤–çš„å…‰çº¿ï¼Œè¿™å¯¼è‡´å­¦ä¹ å†—ä½™ã€‚</li><li>æœ¬æ–‡æå‡ºäº† DistGridï¼ŒåŸºäºå¤šçº§æ•£åˆ—ç½‘æ ¼çš„åˆ†å¸ƒå¼åœºæ™¯é‡å»ºæ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†æ’åˆ—ä½†ä¸é‡å çš„è½´å¯¹é½åŒ…å›´ç›’ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†å‰²ä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§å‹åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„æ•ˆæœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: DistGrid: åŸºäºåˆ†å¸ƒå¼å¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¤§è§„æ¨¡åœºæ™¯é‡å»º</p></li><li><p>Authors: Sidun Liu, Peng Qiao, Zongxin Ye, Wenyu Li, Yong Dou</p></li><li><p>Affiliation: å›½é˜²ç§‘æŠ€å¤§å­¦</p></li><li><p>Keywords: Neural Radiance Field, Distributed Algorithm, Large-scale Scene Reconstruction, Neural Rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.04416.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): ç¥ç»æ¸²æŸ“æŠ€æœ¯è‡ª NeRF æå‡ºä»¥æ¥å–å¾—äº†é‡å¤§è¿›å±•ï¼ŒNeRF æ—¨åœ¨è§£å†³æ–°è§†è§’åˆæˆä»»åŠ¡ã€‚å®ƒåŸºäºä½“ç§¯æ¸²æŸ“ï¼Œä½¿ç”¨ç§°ä¸ºç¥ç»åœºçš„å¤šåˆ†å±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰éšå¼è¡¨ç¤ºåœºæ™¯ã€‚è¯¥ç¥ç»åœºæ¥å— 3D åæ ‡å’Œè§‚å¯Ÿæ–¹å‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹å…¶å¯¹åº”çš„å¯†åº¦å’Œé¢œè‰²ã€‚NeRF åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šäº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰è´¨é‡ã€‚å…¶å­¦ä¹ çš„ç¥ç»åœºä¹Ÿå¯ç”¨äºåœºæ™¯é‡å»ºã€‚</p><p>(2): NeRF çš„åç»­å·¥ä½œæ—¨åœ¨æé«˜æ•ˆç‡å’Œè´¨é‡ã€‚ä¸€ç§æ–¹æ³•æ˜¯å°†åœºæ™¯åœ°ç†åˆ†åŒºï¼Œå¹¶ä½¿ç”¨å•ç‹¬çš„ NeRF å­¦ä¹ æ¯ä¸ªå­åŒºåŸŸã€‚è¿™ç§åˆ†åŒºç­–ç•¥å¸®åŠ©åŸºäºä½“ç§¯çš„ NeRF è¶…è¿‡å•ä¸ª GPU çš„å†…å­˜é™åˆ¶ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¤§çš„åœºæ™¯ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•éœ€è¦å¤šä¸ªèƒŒæ™¯ NeRF æ¥å¤„ç†åˆ†åŒºå¤–çš„å…‰çº¿ï¼Œè¿™å¯¼è‡´äº†å­¦ä¹ çš„å†—ä½™ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º DistGridã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œåœºæ™¯è¢«åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†ç›¸é‚»ä½†éé‡å çš„è½´å¯¹é½è¾¹ç•Œæ¡†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</p><p>(4): å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§è§„æ¨¡åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„åœºæ™¯é‡å»ºã€‚è¯¥æ–¹æ³•åœ¨é‡å»ºè´¨é‡ä¸Šçš„å¯æ‰©å±•æ€§è¿˜é€šè¿‡å®šæ€§å’Œå®šé‡çš„æ–¹å¼è¿›è¡Œäº†è¿›ä¸€æ­¥è¯„ä¼°ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œç§°ä¸º DistGridã€‚</p><p>(2): DistGrid å°†åœºæ™¯åˆ’åˆ†ä¸ºå¤šä¸ªç´§å¯†ç›¸é‚»ä½†éé‡å çš„è½´å¯¹é½è¾¹ç•Œæ¡† (AABB)ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹èƒŒæ™¯ NeRF çš„éœ€æ±‚ã€‚</p><p>(3): DistGrid é‡‡ç”¨ä¸¤çº§çº§è”ç»“æ„ï¼Œå…¶ä¸­ç»†ç²’åº¦ NeRF ä½¿ç”¨å†…å±‚è¾¹ç•Œæ¡†ä½œä¸ºå…¶è¾¹ç•Œæ¡†ï¼Œè€Œç²—ç²’åº¦ NeRF ä½¿ç”¨å¤–å±‚è¾¹ç•Œæ¡†ã€‚</p><p>(4): DistGrid ä½¿ç”¨åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨åŒºåŸŸå…‰çº¿ï¼Œè¯¥æ–¹æ³•å°†ä½“ç§¯æ¸²æŸ“ç§¯åˆ†åˆ†è§£ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨éƒ¨åˆ†é¢œè‰²å’Œéƒ¨åˆ†é€å°„ç‡æ¥è®¡ç®—æ¸²æŸ“é¢œè‰²å’Œæœ€ç»ˆé€å°„ç‡ã€‚</p></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº† DistGridï¼Œä¸€ç§åŸºäºè”åˆå¤šåˆ†è¾¨ç‡å“ˆå¸Œç½‘æ ¼çš„å¯æ‰©å±•åœºæ™¯é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†æ®µä½“ç§¯æ¸²æŸ“æ–¹æ³•æ¥å¤„ç†è·¨è¾¹ç•Œå…‰çº¿ï¼Œæ— éœ€èƒŒæ™¯ NeRFï¼Œæé«˜äº†æ•ˆç‡ï¼›é‡‡ç”¨ä¸¤çº§çº§è”ç»“æ„ï¼Œç»†ç²’åº¦ NeRF å’Œç²—ç²’åº¦ NeRF ååŒå·¥ä½œï¼Œæé«˜äº†é‡å»ºè´¨é‡ã€‚            æ€§èƒ½ï¼šåœ¨æ‰€æœ‰è¯„ä¼°çš„å¤§è§„æ¨¡åœºæ™¯ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æä¾›äº†è§†è§‰ä¸Šåˆç†çš„åœºæ™¯é‡å»ºã€‚            å·¥ä½œé‡ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDistGrid åœ¨é‡å»ºè´¨é‡ä¸Šçš„å¯æ‰©å±•æ€§å¾—åˆ°äº†å®šæ€§å’Œå®šé‡çš„æ–¹å¼çš„è¿›ä¸€æ­¥è¯„ä¼°ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-c94130b609d19ed2e706304ecfbbdde4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c20db83d0a269f077a389b38e5b01349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e4dd6ce979c2f2e18008fc25e085162.jpg" align="middle"></details><h2 id="Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization"><a href="#Blending-Distributed-NeRFs-with-Tri-stage-Robust-Pose-Optimization" class="headerlink" title="Blending Distributed NeRFs with Tri-stage Robust Pose Optimization"></a>Blending Distributed NeRFs with Tri-stage Robust Pose Optimization</h2><p><strong>Authors:Baijun Ye, Caiyun Liu, Xiaoyu Ye, Yuantao Chen, Yuhai Wang, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</strong></p><p>Due to the limited model capacity, leveraging distributed Neural Radiance Fields (NeRFs) for modeling extensive urban environments has become a necessity. However, current distributed NeRF registration approaches encounter aliasing artifacts, arising from discrepancies in rendering resolutions and suboptimal pose precision. These factors collectively deteriorate the fidelity of pose estimation within NeRF frameworks, resulting in occlusion artifacts during the NeRF blending stage. In this paper, we present a distributed NeRF system with tri-stage pose optimization. In the first stage, precise poses of images are achieved by bundle adjusting Mip-NeRF 360 with a coarse-to-fine strategy. In the second stage, we incorporate the inverting Mip-NeRF 360, coupled with the truncated dynamic low-pass filter, to enable the achievement of robust and precise poses, termed Frame2Model optimization. On top of this, we obtain a coarse transformation between NeRFs in different coordinate systems. In the third stage, we fine-tune the transformation between NeRFs by Model2Model pose optimization. After obtaining precise transformation parameters, we proceed to implement NeRF blending, showcasing superior performance metrics in both real-world and simulation scenarios. Codes and data will be publicly available at <a href="https://github.com/boilcy/Distributed-NeRF">https://github.com/boilcy/Distributed-NeRF</a>. </p><p><a href="http://arxiv.org/abs/2405.02880v1">PDF</a> </p><p><strong>Summary</strong><br>ä½¿ç”¨ä¸‰é˜¶æ®µå§¿æ€ä¼˜åŒ–å¯¹åˆ†å¸ƒå¼NeRFè¿›è¡Œç²¾ç¡®å¯¹é½ï¼Œä»¥ç¼“è§£å»ºæ¨¡å¤§è§„æ¨¡åŸå¸‚ç¯å¢ƒæ—¶å‡ºç°çš„æ··å ä¼ªå½±å’Œå§¿æ€ç²¾åº¦ä¸è¶³çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ†å¸ƒå¼NeRFå»ºæ¨¡åŸå¸‚ç¯å¢ƒé¢ä¸´æ··å ä¼ªå½±å’Œå§¿æ€ç²¾åº¦é—®é¢˜ã€‚</li><li>é‡‡ç”¨åˆ†é˜¶æ®µå§¿æ€ä¼˜åŒ–è§£å†³é—®é¢˜ï¼ŒåŒ…æ‹¬Mip-NeRF 360æŸè°ƒæ•´ã€åå‘Mip-NeRF 360å’ŒFrame2Modelä¼˜åŒ–ã€‚</li><li>åˆ©ç”¨Model2Modelä¼˜åŒ–è¿›ä¸€æ­¥ç»†åŒ–ä¸åŒNeRFä¹‹é—´çš„è½¬æ¢ã€‚</li><li>ç²¾ç¡®çš„å§¿æ€ä¼˜åŒ–æœ‰æ•ˆæ¶ˆé™¤NeRFèåˆä¸­çš„é®æŒ¡ä¼ªå½±ã€‚</li><li>åœ¨çœŸå®å’Œæ¨¡æ‹Ÿåœºæ™¯ä¸­å±•ç¤ºå‡ºä¼˜è¶Šçš„NeRFèåˆæ€§èƒ½ã€‚</li><li>ä»£ç å’Œæ•°æ®å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>é¢˜ç›®ï¼šåŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–èåˆåˆ†å¸ƒå¼NeRFs</p></li><li><p>ä½œè€…ï¼šBaijun Yeâˆ—1,2, Caiyun Liuâˆ—1, Xiaoyu Ye1,2, Yuantao Chen1,3, Yuhai Wang4,Zike Yan1, Yongliang Shi1â€ , Hao Zhao1, Guyue Zhou1</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦äººå·¥æ™ºèƒ½äº§ä¸šç ”ç©¶é™¢ï¼ˆAIRï¼‰</p></li><li><p>å…³é”®è¯ï¼šåˆ†å¸ƒå¼NeRFã€ä½å§¿ä¼˜åŒ–ã€NeRFèåˆã€Mip-NeRF 360ã€iNeRF</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.02880Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯å»ºæ¨¡é¢†åŸŸï¼ŒNeRFå› å…¶èƒ½å¤Ÿåœ¨ä¿æŒç´§å‡‘æ¨¡å‹ç»“æ„çš„åŒæ—¶å®ç°é€¼çœŸçš„æ¸²æŸ“è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„åˆ†å¸ƒå¼NeRFé…å‡†æ–¹æ³•å­˜åœ¨æ··å ä¼ªå½±ï¼Œè¿™æºäºæ¸²æŸ“åˆ†è¾¨ç‡å·®å¼‚å’Œæ¬¡ä¼˜ä½å§¿ç²¾åº¦ã€‚è¿™äº›å› ç´ å…±åŒé™ä½äº†NeRFæ¡†æ¶å†…ä½å§¿ä¼°è®¡çš„ä¿çœŸåº¦ï¼Œå¯¼è‡´NeRFèåˆé˜¶æ®µå‡ºç°é®æŒ¡ä¼ªå½±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šä»¥å¾€æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šæ‰¹å¤„ç†å­¦ä¹ å’Œå¢é‡å­¦ä¹ ã€‚æ‰¹å¤„ç†å­¦ä¹ éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè€Œå¢é‡å­¦ä¹ å®¹æ˜“å‡ºç°é—å¿˜é—®é¢˜ã€‚æ­¤å¤–ï¼Œå½“å‰ä½¿ç”¨æ˜¾å¼ç¼–ç æ–¹æ³•ï¼ˆå¦‚ç½‘æ ¼å’Œå…«å‰æ ‘ï¼‰è¿›è¡Œå®æ—¶æ€§èƒ½çš„NeRFæ–¹æ³•ï¼Œé¢ä¸´ç€éšç€åœºæ™¯è§„æ¨¡çš„å¢åŠ ï¼Œç¼–ç ç»„ä»¶å‘ˆæŒ‡æ•°çº§æ‰©å±•çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å­˜å‚¨éœ€æ±‚å¤§å¹…å¢åŠ ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ†ç»‘è°ƒæ•´Mip-NeRF 360å¹¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼Œå®ç°äº†å›¾åƒçš„ç²¾ç¡®ä½å§¿ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå€Ÿé‰´LATITUDEï¼Œåˆ©ç”¨æˆªæ–­åŠ¨æ€ä½é€šæ»¤æ³¢ï¼ˆTDLFï¼‰çš„åŸç†å¯¹åå‘Mip-NeRF 360è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç§°ä¸ºiMNeRFã€‚æ­¤æ–¹æ³•ç±»ä¼¼äºæ¨¡ç³Šå›¾åƒä»¥ä½¿ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ é²æ£’ï¼Œä»è€Œå®ç°å¸§åˆ°æ¨¡å‹çš„ä½å§¿ä¼˜åŒ–ã€‚éšåï¼Œé‡‡ç”¨åè§†å›¾åŒºåŸŸæ£€ç´¢æ–¹æ³•æ¥æœç´¢ä¸åŒNeRFå®ä¾‹ä¸­æœ€ç›¸ä¼¼çš„å›¾åƒï¼Œè¿›è€Œç¡®å®šå…¶å…³è”çš„ä½å§¿ã€‚ç»™å®šå…³è”çš„ä½å§¿ï¼Œåˆ©ç”¨iMNeRFé€šè¿‡æ¸²æŸ“å›¾åƒå’Œè§‚å¯Ÿå›¾åƒä¹‹é—´çš„å…‰åº¦æŸå¤±æ¥ä¼˜åŒ–è¿™äº›ä½å§¿ï¼Œä»è€Œè·å¾—å¯é çš„å¸§åˆ°æ¨¡å‹è½¬æ¢ã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œé€šè¿‡ä¸åŒçš„å¸§åˆ°æ¨¡å‹è½¬æ¢è·å¾—äº†NeRFä¹‹é—´ç²—ç•¥çš„æ¨¡å‹åˆ°æ¨¡å‹è½¬æ¢ã€‚ç„¶åï¼Œå°†ä¸åŒçš„NeRFæ¨¡å‹æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åæ ‡ç³»ä¸­ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“å›¾åƒä½œä¸ºè§‚æµ‹å€¼è¿›ä¸€æ­¥ä¼˜åŒ–NeRFä¹‹é—´çš„ç›¸å¯¹è½¬æ¢ï¼Œå³é€šè¿‡æ¨¡å‹åˆ°æ¨¡å‹ä¼˜åŒ–æ¥è·å¾—NeRFä¹‹é—´çš„ç²¾ç¡®è½¬æ¢ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†æ€æ ·çš„æ€§èƒ½ï¼šåˆ©ç”¨ä¸‰é˜¶æ®µä½å§¿ä¼˜åŒ–ï¼Œå®ç°äº†NeRFèåˆå¹¶è·å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æœ¬æ–‡æ–¹æ³•ï¼ŒåŒæ—¶å‘å¸ƒäº†çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ï¼Œå±•ç¤ºäº†æœ¬æ–‡æ–¹æ³•åœ¨æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡æ†ç»‘è°ƒæ•´Mip-NeRF 360å¹¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„ç­–ç•¥ï¼Œå®ç°äº†å›¾åƒçš„ç²¾ç¡®ä½å§¿ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šç¬¬äºŒé˜¶æ®µï¼Œå€Ÿé‰´LATITUDEï¼Œåˆ©ç”¨æˆªæ–­åŠ¨æ€ä½é€šæ»¤æ³¢ï¼ˆTDLFï¼‰çš„åŸç†å¯¹åå‘Mip-NeRF 360è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç§°ä¸ºiMNeRFï¼›</p><p>ï¼ˆ4ï¼‰ï¼šç»™å®šå…³è”çš„ä½å§¿ï¼Œåˆ©ç”¨iMNeRFé€šè¿‡æ¸²æŸ“å›¾åƒå’Œè§‚å¯Ÿå›¾åƒä¹‹é—´çš„å…‰åº¦æŸå¤±æ¥ä¼˜åŒ–è¿™äº›ä½å§¿ï¼Œä»è€Œè·å¾—å¯é çš„å¸§åˆ°æ¨¡å‹è½¬æ¢ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šç¬¬ä¸‰é˜¶æ®µï¼Œé€šè¿‡ä¸åŒçš„å¸§åˆ°æ¨¡å‹è½¬æ¢è·å¾—äº†NeRFä¹‹é—´ç²—ç•¥çš„æ¨¡å‹åˆ°æ¨¡å‹è½¬æ¢ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šå°†ä¸åŒçš„NeRFæ¨¡å‹æŠ•å½±åˆ°ä¸€ä¸ªç»Ÿä¸€çš„åæ ‡ç³»ä¸­ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“å›¾åƒä½œä¸ºè§‚æµ‹å€¼è¿›ä¸€æ­¥ä¼˜åŒ–NeRFä¹‹é—´çš„ç›¸å¯¹è½¬æ¢ï¼Œå³é€šè¿‡æ¨¡å‹åˆ°æ¨¡å‹ä¼˜åŒ–æ¥è·å¾—NeRFä¹‹é—´çš„ç²¾ç¡®è½¬æ¢ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–çš„åˆ†å¸ƒå¼NeRFæ¡†æ¶ï¼Œè§£å†³äº†å½“å‰åˆ†å¸ƒå¼NeRFé…å‡†æ–¹æ³•ä¸­å­˜åœ¨çš„æ··å ä¼ªå½±é—®é¢˜ï¼Œæé«˜äº†NeRFèåˆçš„ä¿çœŸåº¦ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸‰é˜¶æ®µé²æ£’ä½å§¿ä¼˜åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒç²¾ç¡®ä½å§¿ä¼°è®¡ã€å¸§åˆ°æ¨¡å‹ä½å§¿ä¼˜åŒ–å’Œæ¨¡å‹åˆ°æ¨¡å‹ä½å§¿ä¼˜åŒ–ï¼›æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸ŠéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºè¿›è¡Œä½å§¿ä¼˜åŒ–ï¼Œä½†å¯ä»¥å®ç°æ›´å¥½çš„NeRFèåˆæ•ˆæœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-19421ede11ee24694424a6e2329cbd82.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c3320829396c5cee81241227bf678e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f9d4086bd525c9621ef04e939f0ee92.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cce8d4e7f7b76ee03713ad33dc0da96e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f2f0b44124a450d85749c232dce8a310.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b718f4974ac6aac1d1a62a7beb2d681.jpg" align="middle"><img src="https://picx.zhimg.com/v2-767f74cc31852d4c6f28717ac5e03f47.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5929c8babaf3c740cf4c2d6f2886bf8e.jpg" align="middle"></details>## TK-Planes: Tiered K-Planes with High Dimensional Feature Vectors for   Dynamic UAV-based Scenes**Authors:Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon**In this paper, we present a new approach to bridge the domain gap between synthetic and real-world data for un- manned aerial vehicle (UAV)-based perception. Our formu- lation is designed for dynamic scenes, consisting of moving objects or human actions, where the goal is to recognize the pose or actions. We propose an extension of K-Planes Neural Radiance Field (NeRF), wherein our algorithm stores a set of tiered feature vectors. The tiered feature vectors are generated to effectively model conceptual information about a scene as well as an image decoder that transforms output feature maps into RGB images. Our technique leverages the information amongst both static and dynamic objects within a scene and is able to capture salient scene attributes of high altitude videos. We evaluate its performance on challenging datasets, including Okutama Action and UG2, and observe considerable improvement in accuracy over state of the art aerial perception algorithms. [PDF](http://arxiv.org/abs/2405.02762v1) 8 pages, submitted to IROS2024**Summary**æ— äººæœºå®æ—¶æ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼Œè¯¥æ–‡å°†é™æ€ç‰¹å¾ä¸åŠ¨æ€ç‰¹å¾ç›¸ç»“åˆï¼Œæé«˜äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„åŸŸé€‚åº”æ€§ã€‚**Key Takeaways**- æå‡ºäº†ä¸€ç§åˆ†å±‚ç‰¹å¾å‘é‡çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ‰©å±•ç‰ˆæœ¬ï¼Œç”¨äºæ•è·åŠ¨æ€åœºæ™¯ä¸­çš„æ¦‚å¿µä¿¡æ¯ã€‚- æ‰©å±•çš„NeRFæ¨¡å‹é€šè¿‡å›¾åƒè§£ç å™¨å°†è¾“å‡ºç‰¹å¾å›¾è½¬æ¢ä¸ºRGBå›¾åƒã€‚- è¯¥æ¨¡å‹åŒæ—¶åˆ©ç”¨é™æ€å’ŒåŠ¨æ€å¯¹è±¡çš„ä¿¡æ¯ï¼Œä»è€Œæ•è·é«˜ç©ºè§†é¢‘ä¸­çš„æ˜¾è‘—åœºæ™¯å±æ€§ã€‚- å°†é™æ€å’ŒåŠ¨æ€ç‰¹å¾ç›¸ç»“åˆï¼Œæé«˜äº†æ¨¡å‹åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„åŸŸé€‚åº”æ€§ã€‚- åœ¨Okutama Actionå’ŒUG2ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹çš„æ€§èƒ½ã€‚- ä¸æœ€å…ˆè¿›çš„æ— äººæœºæ„ŸçŸ¥ç®—æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—æé«˜ã€‚- è¯¥æ¨¡å‹å¯ä»¥åº”ç”¨äºæ— äººæœºå®æ—¶æ„ŸçŸ¥ä»»åŠ¡ï¼Œä¾‹å¦‚åŠ¨ä½œè¯†åˆ«å’Œå§¿æ€ä¼°è®¡ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: TK-Planesï¼šå…·æœ‰é«˜ç»´ç‰¹å¾å‘é‡çš„åˆ†å±‚ K-Planes</p></li><li><p>Authors: Christopher Maxey, Jaehoon Choi, Yonghan Lee, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</p></li><li><p>Affiliation: ç¾å›½é™†å†›ç ”ç©¶å®éªŒå®¤</p></li><li><p>Keywords: Neural Radiance Fields, Synthetic Data, UAV Perception, Dynamic Scenes, Feature Vectors</p></li><li><p>Urls: https://arxiv.org/abs/2405.02762, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡ç ”ç©¶èƒŒæ™¯æ˜¯åˆæˆæ•°æ®åœ¨æ— äººæœºæ„ŸçŸ¥ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€åœºæ™¯çš„è¯†åˆ«ï¼Œå¦‚å§¿åŠ¿æˆ–åŠ¨ä½œè¯†åˆ«ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäº K-Planes ç¥ç»è¾å°„åœºï¼Œä½†å­˜åœ¨é—®é¢˜ï¼šåŠ¨æ€å¯¹è±¡å»ºæ¨¡å›°éš¾ã€é™æ€å’ŒåŠ¨æ€å…ƒç´ åˆ†ç¦»å›°éš¾ã€åŠ¨æ€å¯¹è±¡ç¨€ç–ã€å§¿æ€å¤šæ ·æ€§å—é™ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ TK-Planesï¼Œä¸€ç§åˆ†å±‚ K-Planes ç®—æ³•ï¼Œè¾“å‡ºå’Œæ“ä½œç‰¹å¾å‘é‡è€Œä¸æ˜¯ RGB åƒç´ å€¼ã€‚è¿™äº›ç‰¹å¾å‘é‡å¯ä»¥å­˜å‚¨åœºæ™¯ä¸­ç‰¹å®šå¯¹è±¡æˆ–ä½ç½®çš„æ¦‚å¿µä¿¡æ¯ï¼Œå¹¶ä¸ºå¤šä¸ªç›¸åº”çš„ç›¸æœºå…‰çº¿è¾“å‡ºæ—¶å½¢æˆç‰¹å¾å›¾ï¼Œç„¶åè§£ç ä¸ºæœ€ç»ˆå›¾åƒã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨ Okutama Action å’Œ UG2 ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰ç®—æ³•ç›¸æ¯”ï¼ŒåŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„æ•´ä½“è¯†åˆ«å‡†ç¡®æ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨ NeRFï¼ˆç¥ç»è¾å°„åœºï¼‰åœ¨ç‰¹å¾ç©ºé—´ä¸­ç”Ÿæˆæ–°è§†è§’ï¼Œä»¥æ›´å¥½åœ°æ•è·åœºæ™¯ä¸­çš„åŠ¨æ€å¯¹è±¡ï¼Œå¦‚äººç‰©ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨åŸºäºç½‘æ ¼çš„ NeRFï¼Œç½‘æ ¼ç±»ä¼¼äº K-Planesï¼Œä½†å­˜å‚¨çš„ç‰¹å¾å‘é‡ä¸ç›´æ¥ç¼–ç  RGB å€¼ï¼Œè€Œæ˜¯ç¼–ç åœºæ™¯ä¸­çš„æ›´é«˜å±‚æ¬¡æ¦‚å¿µä¿¡æ¯ï¼Œå¦‚åœ°é¢ã€æ ‘æœ¨å’Œäººç‰©ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨åˆ†å±‚ç½‘æ ¼åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œï¼Œå°†åœºæ™¯åˆ†è§£ä¸ºé™æ€å’ŒåŠ¨æ€ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å›¾åƒè§£ç å™¨å°†ç‰¹å¾å›¾è§£ç ä¸ºæœ€ç»ˆå›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œç»“æœè¡¨æ˜åŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):æœ¬æ–‡æå‡ºçš„åˆ†å±‚ K-Planesï¼ˆTK-Planesï¼‰ç®—æ³•ï¼Œé€šè¿‡åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œç‰¹å¾å‘é‡ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åˆæˆæ•°æ®åœ¨æ— äººæœºæ„ŸçŸ¥ä¸­çš„åŠ¨æ€åœºæ™¯è¯†åˆ«é—®é¢˜ã€‚            (2):åˆ›æ–°ç‚¹ï¼šTK-Planes ç®—æ³•å°†åœºæ™¯åˆ†è§£ä¸ºé™æ€å’ŒåŠ¨æ€ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åˆ†å±‚ç½‘æ ¼åœ¨ç‰¹å¾ç©ºé—´ä¸­æ“ä½œï¼Œä»è€Œæ›´å¥½åœ°æ•è·åŠ¨æ€å¯¹è±¡ï¼›æ€§èƒ½ï¼šåœ¨ Okutama Action å’Œ UG2 ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ— äººæœºæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒåŸºäº TK-Planes çš„ NeRF æ¨¡å‹å¯ä»¥ç”Ÿæˆè¡¥å……çš„æ— äººæœºæ•°æ®ï¼Œä»è€Œæé«˜åŠ¨æ€åœºæ™¯çš„è¯†åˆ«å‡†ç¡®æ€§ï¼›å·¥ä½œé‡ï¼šTK-Planes ç®—æ³•çš„å®ç°å’Œåœ¨æ— äººæœºæ•°æ®é›†ä¸Šçš„è¯„ä¼°éœ€è¦ä¸€å®šçš„æŠ€æœ¯æŠ•å…¥å’Œè®¡ç®—èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-49ebb6a345fe5bed0d70468dcdf8fd84.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-63139de16b603f02b54ce2804a9bad9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af95031ec70f26ba5024a7788a09ddff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c99084999876941f9618dfb5d99f367b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63f6561485ccf42232344f25cf43bc8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a606ade4cb1fcbf9e4da91086ed92ad1.jpg" align="middle"></details><h2 id="ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty"><a href="#ActiveNeuS-Active-3D-Reconstruction-using-Neural-Implicit-Surface-Uncertainty" class="headerlink" title="ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty"></a>ActiveNeuS: Active 3D Reconstruction using Neural Implicit Surface   Uncertainty</h2><p><strong>Authors:Hyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</strong></p><p>Active learning in 3D scene reconstruction has been widely studied, as selecting informative training views is critical for the reconstruction. Recently, Neural Radiance Fields (NeRF) variants have shown performance increases in active 3D reconstruction using image rendering or geometric uncertainty. However, the simultaneous consideration of both uncertainties in selecting informative views remains unexplored, while utilizing different types of uncertainty can reduce the bias that arises in the early training stage with sparse inputs. In this paper, we propose ActiveNeuS, which evaluates candidate views considering both uncertainties. ActiveNeuS provides a way to accumulate image rendering uncertainty while avoiding the bias that the estimated densities can introduce. ActiveNeuS computes the neural implicit surface uncertainty, providing the color uncertainty along with the surface information. It efficiently handles the bias by using the surface information and a grid, enabling the fast selection of diverse viewpoints. Our method outperforms previous works on popular datasets, Blender and DTU, showing that the views selected by ActiveNeuS significantly improve performance. </p><p><a href="http://arxiv.org/abs/2405.02568v1">PDF</a> </p><p><strong>Summary</strong><br>ä¸»åŠ¨ç¥ç»é‡å»ºåŒæ—¶è€ƒè™‘å›¾åƒæ¸²æŸ“å’Œå‡ ä½•ä¸ç¡®å®šæ€§ï¼Œä»¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è®­ç»ƒè§†å›¾æ¥æé«˜ 3D åœºæ™¯é‡å»ºæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¸»åŠ¨å­¦ä¹ åœ¨ 3D åœºæ™¯é‡å»ºä¸­è‡³å…³é‡è¦ã€‚</li><li>NeRF å˜ä½“ä½¿ç”¨å›¾åƒæ¸²æŸ“æˆ–å‡ ä½•ä¸ç¡®å®šæ€§æé«˜äº†ä¸»åŠ¨ 3D é‡å»ºçš„æ€§èƒ½ã€‚</li><li>ActiveNeuS åŒæ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“å’Œå‡ ä½•ä¸ç¡®å®šæ€§æ¥é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†å›¾ã€‚</li><li>ActiveNeuS ç§¯ç´¯å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶é¿å…ä¼°è®¡å¯†åº¦å¼•å…¥çš„åå·®ã€‚</li><li>ActiveNeuS è®¡ç®—ç¥ç»éšå¼è¡¨é¢ä¸ç¡®å®šæ€§ï¼Œæä¾›é¢œè‰²ä¸ç¡®å®šæ€§å’Œè¡¨é¢ä¿¡æ¯ã€‚</li><li>ActiveNeuS ä½¿ç”¨è¡¨é¢ä¿¡æ¯å’Œç½‘æ ¼æœ‰æ•ˆå¤„ç†åå·®ï¼Œä»è€Œå¿«é€Ÿé€‰æ‹©å¤šæ ·åŒ–çš„è§†ç‚¹ã€‚</li><li>ActiveNeuS åœ¨ Blender å’Œ DTU æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä»¥å¾€çš„å·¥ä½œï¼Œè¡¨æ˜ ActiveNeuS é€‰æ‹©çš„è§†å›¾æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šActiveNeuSï¼šåŸºäºç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§çš„ä¸»åŠ¨ä¸‰ç»´é‡å»º</li><p></p><p></p><li>ä½œè€…ï¼šHyunseo Kim, Hyeonseo Yang, Taekyung Kim, YoonSung Kim, Jin-Hwa Kim, Byoung-Tak Zhang</li><p></p><p></p><li>å•ä½ï¼šé¦–å°”å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šä¸»åŠ¨å­¦ä¹ ã€ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€æ›²é¢ç½‘æ ¼</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2405.02568.pdfï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šä¸‰ç»´åœºæ™¯é‡å»ºä¸­çš„ä¸»åŠ¨å­¦ä¹ å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œå› ä¸ºé€‰æ‹©æœ‰ä¿¡æ¯çš„è®­ç»ƒè§†å›¾å¯¹äºé‡å»ºè‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å˜ä½“åœ¨ä½¿ç”¨å›¾åƒæ¸²æŸ“æˆ–å‡ ä½•ä¸ç¡®å®šæ€§è¿›è¡Œä¸»åŠ¨ä¸‰ç»´é‡å»ºæ–¹é¢è¡¨ç°å‡ºæ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œåœ¨é€‰æ‹©ä¿¡æ¯è§†å›¾æ—¶åŒæ—¶è€ƒè™‘ä¸¤ç§ä¸ç¡®å®šæ€§ä»ç„¶æœªè¢«æ¢ç´¢ï¼Œè€Œåˆ©ç”¨ä¸åŒç±»å‹çš„ä¸ç¡®å®šæ€§å¯ä»¥å‡å°‘åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µç”±äºè¾“å…¥ç¨€ç–è€Œäº§ç”Ÿçš„åå·®ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„NeRFä¸»åŠ¨å­¦ä¹ æ–¹æ³•é€šå¸¸ä¼°è®¡å…¶è¾“å‡ºä¸­çš„ä¸ç¡®å®šæ€§ï¼šä¸‰ç»´ç‚¹çš„å¯†åº¦å’Œé¢œè‰²ã€‚Martinç­‰äººå’ŒPanç­‰äººä¼°è®¡äº†é¢œè‰²é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œæ–¹æ³•æ˜¯å°†é¢œè‰²å»ºæ¨¡ä¸ºé«˜æ–¯æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µå®¹æ˜“å—åˆ°å¯†åº¦ä¼°è®¡åå·®çš„å½±å“ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¿¡æ¯è§†å›¾é€‰æ‹©ä¸ä½³ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ActiveNeuSï¼Œå®ƒåœ¨è¯„ä¼°å€™é€‰è§†å›¾æ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§å’Œç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€‚ActiveNeuSæä¾›äº†ä¸€ç§ç§¯ç´¯å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼ŒåŒæ—¶é¿å…äº†ä¼°è®¡å¯†åº¦å¯èƒ½å¼•å…¥çš„åå·®ã€‚ActiveNeuSè®¡ç®—ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ï¼Œæä¾›é¢œè‰²ä¸ç¡®å®šæ€§å’Œæ›²é¢ä¿¡æ¯ã€‚å®ƒé€šè¿‡ä½¿ç”¨æ›²é¢ä¿¡æ¯å’Œç½‘æ ¼æœ‰æ•ˆåœ°å¤„ç†åå·®ï¼Œä»è€Œèƒ½å¤Ÿå¿«é€Ÿé€‰æ‹©ä¸åŒçš„è§†ç‚¹ã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨æµè¡Œçš„æ•°æ®é›†Blenderå’ŒDTUä¸Šï¼ŒActiveNeuSçš„æ€§èƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œè¡¨æ˜ActiveNeuSé€‰æ‹©çš„è§†å›¾æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚è¿™äº›ç»“æœæ”¯æŒäº†ä½œè€…çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é€‰æ‹©ä¿¡æ¯è§†å›¾ä»¥æé«˜ä¸‰ç»´é‡å»ºçš„æ€§èƒ½ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): ActiveNeuS æå‡ºäº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»“åˆäº†å‡ ä½•é‡å»ºå’Œå›¾åƒæ¸²æŸ“çš„è§†è§’ã€‚            (2): ActiveNeuS ä¼°è®¡é¢œè‰²é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œä»¥è·å–æœ‰å…³å›¾åƒæ¸²æŸ“è´¨é‡çš„ä¿¡æ¯ã€‚            (3): é‡‡é›†å‡½æ•°é›†æˆäº†ä¼°è®¡çš„ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä¸ä¸¢å¤±å‡ ä½•å±æ€§ã€‚            (4): é¦–å…ˆï¼Œåœ¨ç¬¬ 4.1 èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„é‡‡é›†å‡½æ•°ï¼Œå¹¶è§£é‡Šäº†åœ¨ç§¯åˆ†è¿‡ç¨‹ä¸­å¦‚ä½•è€ƒè™‘æ›²é¢ã€‚            (5): åœ¨ç¬¬ 4.2 èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº† ActiveNeuS ä¸­ä¼°è®¡çš„ç¥ç»éšå¼è¡¨é¢ä¸ç¡®å®šæ€§ï¼Œå¹¶æè¿°äº†å¦‚ä½•åœ¨é‡‡é›†å‡½æ•°ä¸­åˆ©ç”¨ä¸ç¡®å®šæ€§ã€‚            (6): ç„¶åï¼Œä¸ºäº†è¿›è¡Œé«˜æ•ˆä¸”ç¨³å¥çš„è®¡ç®—ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­˜å‚¨æ›²é¢ä¿¡æ¯çš„æ›²é¢ç½‘æ ¼å’Œé€‰æ‹©å¤šä¸ªæ¬¡ä¼˜è§†å›¾ (NBV) çš„ç­–ç•¥ï¼ˆç¬¬ 4.3 èŠ‚ï¼‰ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„ä¿¡æ¯è§†å›¾é€‰æ‹©æ–¹æ³• ActiveNeuSï¼Œè¯¥æ–¹æ³•åŒæ—¶è€ƒè™‘äº†å‡ ä½•é‡å»ºå’Œå›¾åƒæ¸²æŸ“çš„ä¿çœŸåº¦ã€‚ActiveNeuS å¼•å…¥äº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°åˆ©ç”¨ä¸ç¡®å®šæ€§ç½‘æ ¼æœ‰æ•ˆä¸”ç¨³å¥åœ°åˆ©ç”¨ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ã€‚é‡‡é›†å‡½æ•°é€šè¿‡ä½¿ç”¨æ›²é¢ç½‘æ ¼å¹¶æ ¹æ®æ›²é¢çš„å­˜åœ¨åº”ç”¨ä¸åŒçš„ç§¯åˆ†ç­–ç•¥æ¥è®¡ç®—ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§çš„ç§¯åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºäº† ActiveNeuS çš„ä¸‹ä¸€ä¸ªæœ€ä½³è§†å›¾é€‰æ‹©ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ”¹è¿›äº†ç½‘æ ¼é‡å»ºå’Œå›¾åƒæ¸²æŸ“è´¨é‡ã€‚å¯¹äºæœªæ¥çš„å·¥ä½œï¼Œæˆ‘ä»¬å»ºè®®ç ”ç©¶ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥è¿æ¥ä¸åŒç½‘ç»œçš„ä¸ç¡®å®šæ€§ä»¥è¿›è¡Œä¿¡æ¯è§†å›¾é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå°† ActiveNeuS åº”ç”¨äºæœºå™¨äººä¸»åŠ¨ 3D é‡å»ºä¸­ä¹Ÿå¾ˆæœ‰è¶£ï¼Œå…¶ä¸­æœºå™¨äººæ‰‹è‡‚ç§»åŠ¨å¹¶æ”¶é›†æ•°æ®ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡é›†å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è€ƒè™‘äº†å›¾åƒæ¸²æŸ“ä¸ç¡®å®šæ€§å’Œç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ä¿¡æ¯è§†å›¾çš„é€‰æ‹©ã€‚æ€§èƒ½ï¼šåœ¨ Blender å’Œ DTU ç­‰æµè¡Œæ•°æ®é›†ä¸Šï¼ŒActiveNeuS çš„æ€§èƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œè¡¨æ˜ ActiveNeuS é€‰æ‹©çš„è§†å›¾æ˜¾ç€æé«˜äº†æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šActiveNeuS çš„è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒé«˜ï¼Œå› ä¸ºå®ƒéœ€è¦ä¼°è®¡ç¥ç»éšå¼æ›²é¢ä¸ç¡®å®šæ€§å¹¶ä½¿ç”¨æ›²é¢ç½‘æ ¼è¿›è¡Œç§¯åˆ†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0eb6f5097fe2312cfce57f04da637606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-824537082b5290b565ea1f0da3946351.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7939474a53153965a29e673124ea4a9e.jpg" align="middle"></details><h2 id="Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids"><a href="#Rip-NeRF-Anti-aliasing-Radiance-Fields-with-Ripmap-Encoded-Platonic-Solids" class="headerlink" title="Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids"></a>Rip-NeRF: Anti-aliasing Radiance Fields with Ripmap-Encoded Platonic   Solids</h2><p><strong>Authors:Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</strong></p><p>Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliasing renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times. </p><p><a href="http://arxiv.org/abs/2405.02386v1">PDF</a> SIGGRAPH 2024, Project page: <a href="https://junchenliu77.github.io/Rip-NeRF">https://junchenliu77.github.io/Rip-NeRF</a>   , Code: <a href="https://github.com/JunchenLiu77/Rip-NeRF">https://github.com/JunchenLiu77/Rip-NeRF</a></p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é€šè¿‡æåæ ‡æŠ•å½±å°†ä¸‰ç»´å„å‘å¼‚æ€§åŒºåŸŸå°„å½±åˆ°å¹³é¢ï¼Œå†åˆ©ç”¨Ripmapç¼–ç å¯¹å„å¹³é¢è¿›è¡Œç¼–ç ï¼Œè¿›è€Œè§£å†³NeRFæŠ—é”¯é½¿æ¸²æŸ“ä¸­çš„æ··å å’Œæ¨¡ç³Šé—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æåæ ‡æŠ•å½±å°†ä¸‰ç»´å„å‘å¼‚æ€§åŒºåŸŸå°„å½±åˆ°å¹³é¢ï¼Œä¾¿äºç‰¹å¾åŒ–ã€‚</li><li>Ripmapç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„æ»¤æ³¢å¯å­¦ä¹ ç‰¹å¾ç½‘æ ¼ï¼Œå¯¹å°„å½±å„å‘å¼‚æ€§åŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚</li><li>æ–¹æ³•åœ¨åˆæˆæ•°æ®é›†å’Œå®æ™¯æ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€ä¼˜æ¸²æŸ“è´¨é‡ã€‚</li><li>æ–¹æ³•åœ¨é‡å¤ç»“æ„å’Œçº¹ç†çš„ç²¾ç»†ç»†èŠ‚ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li><li>æ–¹æ³•è®­ç»ƒæ—¶é—´ç›¸å¯¹è¾ƒçŸ­ã€‚</li><li>è¯¥æ–¹æ³•ä¾èµ–äºå¯å­¦ä¹ ç‰¹å¾ç½‘æ ¼ã€‚</li><li>è¯¥æ–¹æ³•ç›®å‰ä»…é€‚ç”¨äºé™æ€åœºæ™¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p><strong>æ ‡é¢˜ï¼š</strong>Rip-NeRFï¼šåŸºäºRipmapç¼–ç çš„Platonicå®ä½“çš„åèµ°æ ·è¾å°„åœº</p></li><li><p><strong>ä½œè€…ï¼š</strong>Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao</p></li><li><p><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</p></li><li><p><strong>å…³é”®è¯ï¼š</strong>ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ã€åèµ°æ ·ã€Ripmapç¼–ç ã€Platonicå®ä½“</p></li><li><p><strong>è®ºæ–‡é“¾æ¥ï¼š</strong>https://arxiv.org/pdf/2405.02386.pdfï¼Œ<strong>Githubé“¾æ¥ï¼š</strong>None</p></li><li><p><strong>æ‘˜è¦ï¼š</strong></p><p>(1) <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>å°½ç®¡ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶æ¸²æŸ“ç»“æœä»å¯èƒ½å­˜åœ¨èµ°æ ·å’Œæ¨¡ç³Šä¼ªå½±ï¼Œå› ä¸ºæœ‰æ•ˆä¸”é«˜æ•ˆåœ°è¡¨å¾é”¥å½¢æŠ•å°„è¿‡ç¨‹äº§ç”Ÿçš„å„å‘å¼‚æ€§åŒºåŸŸä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚</p><p>(2) <strong>è¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼š</strong>ç°æœ‰æ–¹æ³•è¦ä¹ˆæ— æ³•ç²¾ç¡®åœ°è¡¨å¾å„å‘å¼‚æ€§åŒºåŸŸï¼Œè¦ä¹ˆæ•ˆç‡ä½ä¸‹ã€‚</p><p>(3) <strong>æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºRipmapç¼–ç çš„Platonicå®ä½“è¡¨ç¤ºï¼Œç”¨äºç²¾ç¡®é«˜æ•ˆåœ°è¡¨å¾3Då„å‘å¼‚æ€§åŒºåŸŸï¼Œä»è€Œå®ç°é«˜ä¿çœŸåèµ°æ ·æ¸²æŸ“ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šPlatonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ã€‚Platonicå®ä½“æŠ•å½±å°†3Dç©ºé—´åˆ†è§£åˆ°ç‰¹å®šPlatonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œä½¿å¾—å„å‘å¼‚æ€§çš„3DåŒºåŸŸå¯ä»¥æŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚åŒæ—¶ï¼ŒPlatonicå®ä½“çš„æ¯ä¸ªé¢éƒ½ç”±Ripmapç¼–ç ç¼–ç ï¼Œè¯¥ç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œä»¥å®ç°å¯¹æŠ•å½±å„å‘å¼‚æ€§åŒºåŸŸçš„ç²¾ç¡®å’Œé«˜æ•ˆè¡¨å¾ã€‚</p><p>(4) <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong>åœ¨å¤šå°ºåº¦Blenderæ•°æ®é›†å’Œæ–°æ•è·çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šåŸºäºRipmapç¼–ç çš„Platonicå®ä½“æŠ•å½±ï¼Œå°†3Dç©ºé—´åˆ†è§£åˆ°Platonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œå°†å„å‘å¼‚æ€§çš„3DåŒºåŸŸæŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚</p><p>ï¼ˆ2ï¼‰ï¼šPlatonicå®ä½“çš„æ¯ä¸ªé¢ç”±Ripmapç¼–ç ç¼–ç ï¼Œè¯¥ç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œä»¥å®ç°å¯¹æŠ•å½±å„å‘å¼‚æ€§åŒºåŸŸçš„ç²¾ç¡®å’Œé«˜æ•ˆè¡¨å¾ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šé‡‡ç”¨æ··åˆè¡¨ç¤ºï¼ŒåŒ…æ‹¬æ˜¾å¼å’Œéšå¼è¡¨ç¤ºï¼Œæ—¢èƒ½ä¿è¯æ•ˆç‡ï¼Œåˆèƒ½ä¿è¯çµæ´»æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨å¤šé‡‡æ ·å’Œé¢ç§¯é‡‡æ ·å¯¹åœ†é”¥æˆªä½“è¿›è¡Œç‰¹å¾åŒ–ï¼Œå…¶ä¸­é¢ç§¯é‡‡æ ·é‡‡ç”¨å„å‘å¼‚æ€§3Dé«˜æ–¯å‡½æ•°å¯¹åœ†é”¥æˆªä½“è¿›è¡Œè¡¨å¾ï¼Œå†åˆ©ç”¨æå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç è¿›è¡Œç‰¹å¾åŒ–ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåˆ©ç”¨MLPä¼°è®¡åœ†é”¥æˆªä½“çš„é¢œè‰²å’Œå¯†åº¦ï¼Œå¹¶é€šè¿‡ä½“ç§¯æ¸²æŸ“æ¸²æŸ“åƒç´ é¢œè‰²ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨å…‰åº¦æŸå¤±å‡½æ•°ï¼Œå¯¹æ¸²æŸ“å›¾åƒå’Œè§‚æµ‹å›¾åƒè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</p><p><strong>8. ç»“è®ºï¼š</strong></p><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºRipmapç¼–ç çš„Platonicå®ä½“è¡¨ç¤ºï¼Œç”¨äºç¥ç»è¾å°„åœºï¼Œç§°ä¸ºRip-NeRFã€‚Rip-NeRFå¯ä»¥æ¸²æŸ“é«˜ä¿çœŸæŠ—é”¯é½¿å›¾åƒï¼ŒåŒæ—¶ä¿æŒæ•ˆç‡ï¼Œè¿™å¾—ç›Šäºæå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ã€‚Platonicå®ä½“æŠ•å½±å°†3Dç©ºé—´åˆ†è§£åˆ°ç‰¹å®šPlatonicå®ä½“çš„ä¸å¹³è¡Œé¢ä¸Šï¼Œä½¿å¾—å„å‘å¼‚æ€§çš„3DåŒºåŸŸå¯ä»¥æŠ•å½±åˆ°å…·æœ‰å¯åŒºåˆ†ç‰¹å¾çš„å¹³é¢ä¸Šã€‚Ripmapç¼–ç é€šè¿‡å„å‘å¼‚æ€§é¢„è¿‡æ»¤å¯å­¦ä¹ çš„ç‰¹å¾ç½‘æ ¼æ„å»ºï¼Œèƒ½å¤Ÿå¯¹æŠ•å½±çš„å„å‘å¼‚æ€§åŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚è¿™ä¸¤ä¸ªç»„ä»¶ååŒå·¥ä½œï¼Œå¯¹å„å‘å¼‚æ€§çš„3DåŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚å®ƒåœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•æ‰ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„å’Œçº¹ç†çš„ç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™éªŒè¯äº†æ‰€æå‡ºçš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç çš„æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºRipmapç¼–ç çš„Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç ï¼Œç”¨äºå¯¹å„å‘å¼‚æ€§çš„3DåŒºåŸŸè¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ç‰¹å¾åŒ–ã€‚</p><p>æ€§èƒ½ï¼šåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p><p>å·¥ä½œé‡ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå·¥ä½œé‡ç•¥å¤§ï¼Œå› ä¸ºéœ€è¦å¯¹Platonicå®ä½“æŠ•å½±å’ŒRipmapç¼–ç è¿›è¡Œé¢„å¤„ç†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7575292a1dc220679b8e9c4fa1e7bb9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d766632fb875e5fe770b7fee4ed1ae6b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43f9b3bed63eef23fb98c456f7077574.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b2442c515601a7e17fca7b5a8e2166b.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/3DGS/</id>
    <published>2024-05-13T08:18:16.000Z</published>
    <updated>2024-05-13T08:18:16.334Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>ä¸€ä¸ªå›¾åƒåˆ°å¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ˜¯å›¾åƒåˆ° 3D è¡¨ç¤ºæˆ–å›¾åƒ 3D é‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°é¢–æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨é«˜æ–¯æº…å°„æ³•ï¼Œå¯å®ç°éšå¼ 3D é‡å»ºï¼Œå¹¶ä¼˜äºåŸå§‹ç¥ç»è¾å°„åœºã€‚</li><li>å€ŸåŠ©æŠ€æœ¯å’ŒåŸç†çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å°è¯•ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹é€šè¿‡æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ç„¶è€Œï¼Œä½¿ç”¨å¸¸è§„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥è·å¾—ç²¾ç¡®çš„åŠ¨ä½œå’ŒåŠ¨ä½œæ§åˆ¶ï¼Œä¸”éš¾ä»¥ç”Ÿæˆå†…å®¹é•¿ä¸”è¯­ä¹‰è¿ç»­çš„ 3D è§†é¢‘ã€‚</li><li>ç ”ç©¶è€…æå‡º OneTo3D æ–¹æ³•ï¼Œä½¿ç”¨å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„ 3D è§†é¢‘ã€‚</li><li>ç ”ç©¶è€…é‡‡ç”¨æ™®é€šåŸºæœ¬é«˜æ–¯æº…å°„æ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹è§†é¢‘å†…å­˜å’Œè®¡ç®—æœºè¿ç®—èƒ½åŠ›è¦æ±‚è¾ƒä½ã€‚</li><li>ç ”ç©¶è€…é’ˆå¯¹å¯¹è±¡éª¨æ¶è®¾è®¡äº†è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ã€‚</li><li>ç»“åˆç ”ç©¶è€…æå‡ºçš„å¯é‡æ–°ç¼–è¾‘çš„åŠ¨ä½œå’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼Œåœ¨ 3D æ¨¡å‹ç²¾ç¡®åŠ¨ä½œå’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®è¾“å…¥æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šçš„è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äº SOTA é¡¹ç›®çš„æ€§èƒ½ã€‚</li><li>ç ”ç©¶è€…åˆ†æäº†è¯¦ç»†çš„å®ç°æ–¹æ³•å’Œç†è®ºåˆ†æï¼Œå¹¶å°†ç»™å‡ºç›¸å…³çš„æ¯”è¾ƒå’Œç»“è®ºã€‚</li><li>è¯¥é¡¹ç›®ä»£ç å¼€æºã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šOneTo3Dï¼šå•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘</p></li><li><p>ä½œè€…ï¼šJINWEI LIN</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè«çº³ä»€å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š3Dã€å•å¹…å›¾åƒã€å¯ç¼–è¾‘ã€åŠ¨æ€ã€ç”Ÿæˆã€è‡ªåŠ¨åŒ–ã€è§†é¢‘ã€è‡ªé€‚åº”ã€éª¨æ¶</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.06547ï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D è¡¨å¾æˆ– 3D é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸé•¿æœŸä»¥æ¥çš„æŒ‘æˆ˜ã€‚ç›®å‰å®ç° 3D é‡å»ºçš„æ–¹æ³•å¯åˆ†ä¸ºä¸¤ç±»ï¼šä¼ ç»Ÿæ˜¾å¼æ–¹æ³•å’Œæœºå™¨å­¦ä¹ éšå¼æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºä¸€ç§çªå‡ºçš„éšå¼æ–¹æ³•ï¼Œåœ¨æ¸²æŸ“å’Œè¡¨ç¤ºçœŸå®åœºæ™¯è§†å›¾æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p><p>(2)ï¼šè¿‡å»æ–¹æ³•ï¼šåŸºäº NeRFï¼Œå‡ºç°äº†å„ç§éšå¼ 3D è¡¨å¾æˆ–é‡å»ºçš„ç ”ç©¶é¡¹ç›®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆè¿ç»­è¯­ä¹‰ 3D è§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</p><p>(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º OneTo3D æ–¹æ³•ï¼Œåˆ©ç”¨å•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œè¿ç»­è¯­ä¹‰ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„ Gaussian Splatting æ¨¡å‹ä»å•å¹…å›¾åƒç”Ÿæˆ 3D æ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥åˆ›å»ºå¯¹è±¡éª¨æ¶ã€‚ç»“åˆå¯ç¼–è¾‘è¿åŠ¨å’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼ŒOneTo3D åœ¨ 3D æ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</p><p>(4)ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä»¥ä¸‹æ€§èƒ½ï¼š- å®ç°äº†ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚- èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘ã€‚- è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>(1)ï¼šç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ï¼šåˆ©ç”¨ Gaussian Splatting æ¨¡å‹ä»è¾“å…¥å›¾åƒç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ï¼Œä¸åŒ…å«åŠ¨æ€æˆ–å¯ç¼–è¾‘å› ç´ ã€‚</p><p>(2)ï¼šç”Ÿæˆå¹¶ç»‘å®šè‡ªé€‚åº”éª¨æ¶ï¼šåˆ†æåˆå§‹ 3D æ¨¡å‹çš„å‡ ä½•å‚æ•°ä¿¡æ¯ï¼Œæ„å»ºé€‚åˆçš„éª¨æ¶ï¼Œå¹¶æ ¹æ®è¾“å…¥å›¾åƒä¸­çš„å§¿æ€ã€å½¢çŠ¶å’Œå…³é”®ç‚¹ä¿¡æ¯å¾®è°ƒéª¨æ¶å‚æ•°ï¼Œä½¿å…¶ä¸ç‰©ä½“èº«ä½“è´´åˆã€‚</p><p>(3)ï¼šæ–‡æœ¬åˆ°åŠ¨ä½œï¼šåˆ†æç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤çš„å‘½ä»¤æ„å›¾ï¼Œæå–ç›¸å¯¹éª¨éª¼çš„ç‰¹å®šè¿åŠ¨å’Œä¿®æ”¹æ•°æ®ï¼Œæ§åˆ¶ç‰¹å®šéª¨éª¼åœ¨ Blender ä¸­å®ç°ç›¸å¯¹è¿åŠ¨ï¼Œè€ƒè™‘è¿åŠ¨é‡åŒ–ã€è¿åŠ¨æ¬¡æ•°ã€è¿åŠ¨æ–¹å‘å’Œè¿åŠ¨èŒƒå›´ç­‰å‚æ•°ã€‚</p><p>(4)ï¼šå¯é‡æ–°ç¼–è¾‘è¿åŠ¨æ§åˆ¶ï¼šé…åˆ Blender ç•Œé¢å®ç°è¿åŠ¨å¯é‡æ–°ç¼–è¾‘æ§åˆ¶ï¼Œå°†å½“å‰å§¿æ€ä½œä¸ºå…³é”®å¸§æ’å…¥ï¼Œç»“åˆè¿ç»­å…³é”®å¸§ç”Ÿæˆæœ€ç»ˆ 3D è§†é¢‘ï¼ŒBlender æ–‡ä»¶ä¿å­˜ä¸ºå¯é‡æ–°ç¼–è¾‘çš„ 3D ç¼–è¾‘æ–‡ä»¶ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„æ–°æ–¹æ³• OneTo3Dã€‚è¯¥æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š- å®ç°äº†ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚- èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç¨³å®šè¿ç»­çš„è¯­ä¹‰ 3D è§†é¢‘ã€‚- è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„ç›®æ ‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š- åˆ©ç”¨å•å¹…å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œè§†é¢‘ã€‚- è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥åˆ›å»ºå¯¹è±¡éª¨æ¶ã€‚- ç»“åˆå¯ç¼–è¾‘è¿åŠ¨å’ŒåŠ¨ä½œåˆ†ææ§åˆ¶ç®—æ³•ï¼Œå®ç°ç²¾ç¡®çš„è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ã€‚</p><p>æ€§èƒ½ï¼š- åœ¨ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚- è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯åœ¨ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶æ–¹é¢çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼š- è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œè®¡ç®—èµ„æºã€‚- ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€ 3D æ¨¡å‹å’Œè§†é¢‘çš„è¿‡ç¨‹éœ€è¦å¤§é‡çš„æ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth   Smooth Regularization**Authors:Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu**This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently Gaussian Splatting-based SLAM has yielded promising results, but rely on RGB-D input and is weak in tracking. To address these limitations, we uniquely integrates advanced sparse visual odometry with a dense Gaussian Splatting scene representation for the first time, thereby eliminating the dependency on depth maps typical of Gaussian Splatting-based SLAM systems and enhancing tracking robustness. Here, the sparse visual odometry tracks camera poses in RGB stream, while Gaussian Splatting handles map reconstruction. These components are interconnected through a Multi-View Stereo (MVS) depth estimation network. And we propose a depth smooth loss to reduce the negative effect of estimated depth maps. Furthermore, the consistency in scale between the sparse visual odometry and the dense Gaussian map is preserved by Sparse-Dense Adjustment Ring (SDAR). We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art performance. Additionally, it outperforms previous monocular methods in terms of novel view synthesis fidelity, matching the results of neural SLAM systems that utilize RGB-D input. [PDF](http://arxiv.org/abs/2405.06241v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**Summary**åŸºäºé«˜æ–¯åˆ†å¸ƒçš„ç¨ å¯†è§†è§‰SLAMæ–°æ¡†æ¶ï¼Œé›†æˆäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œç¨ å¯†åœºæ™¯è¡¨ç¤ºï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚**Key Takeaways*** ç»“åˆäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒçš„ç¨ å¯†åœºæ™¯è¡¨ç¤ºã€‚* æ¶ˆé™¤äº†å¯¹æ·±åº¦å›¾çš„ä¾èµ–ï¼Œå¢å¼ºäº†è·Ÿè¸ªçš„é²æ£’æ€§ã€‚* å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œè¿æ¥äº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒã€‚* æå‡ºæ·±åº¦å¹³æ»‘æŸå¤±ï¼Œå‡å°‘ä¼°è®¡æ·±åº¦å›¾çš„è´Ÿé¢å½±å“ã€‚* é€šè¿‡ç¨€ç–-ç¨ å¯†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ä¿æŒäº†ç¨€ç–è§†è§‰é‡Œç¨‹è®¡å’Œé«˜æ–¯åˆ†å¸ƒçš„åœ°å›¾ä¹‹é—´çš„è§„æ¨¡ä¸€è‡´æ€§ã€‚* åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œå§¿åŠ¿ä¼°è®¡çš„å‡†ç¡®æ€§è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚* åœ¨æ–°è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œè¾¾åˆ°åˆ©ç”¨RGB-Dè¾“å…¥çš„ç¥ç»SLAMç³»ç»Ÿçš„æ•ˆæœã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title: MGS-SLAM: å•ç›®ç¨€ç–è·Ÿè¸ªå’Œé«˜æ–¯æ˜ å°„ä¸æ·±åº¦å¹³æ»‘æ­£åˆ™åŒ–</li><li>Authors: Pengcheng Zhu, Yaoming Zhuang, Baoquan Chen, Li Li, Chengdong Wu, Zhanlin Liu</li><li>Affiliation: ä¸œåŒ—å¤§å­¦æœºå™¨äººç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢</li><li>Keywords: Visual SLAM, Gaussian Splatting, Sparse Visual Odometry, Multi-View Stereo, Depth Smooth Regularization</li><li>Urls: Paper: https://arxiv.org/abs/2405.06241, Github: None</li><li>Summary:</li></ol><p>(1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šéšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œä¸€ç§åˆ©ç”¨å¯å¾®æ¸²æŸ“çš„æ–°å‹ SLAM æŠ€æœ¯åº”è¿è€Œç”Ÿã€‚åŸºäºå¯å¾®æ¸²æŸ“çš„ SLAM æŠ€æœ¯æœ€åˆä½¿ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºå…¶åŸºç¡€æ„å»ºæ–¹æ³•ã€‚NeRF åˆ©ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤º 3D åœºæ™¯ï¼Œèƒ½å¤Ÿåˆæˆé«˜è´¨é‡å›¾åƒå¹¶ä»å¤šè§†å›¾ä¸­æ¢å¤å¯†é›†çš„å‡ ä½•ç»“æ„ã€‚åŸºäº NeRF çš„ SLAM ç³»ç»Ÿåœ¨åˆ¶å›¾è¿‡ç¨‹ä¸­ä¿ç•™äº†è¯¦ç»†çš„åœºæ™¯ä¿¡æ¯ï¼Œå¢å¼ºäº†å¯¹åç»­å¯¼èˆªå’Œè·¯å¾„è§„åˆ’çš„æ”¯æŒã€‚ç„¶è€Œï¼ŒNeRF çš„æ–¹æ³•åœ¨å›¾åƒæ¸²æŸ“è¿‡ç¨‹ä¸­éœ€è¦å¯¹æ¯ä¸ªåƒç´ è¿›è¡Œå¤šæ¬¡å‰å‘é¢„æµ‹ï¼Œå¯¼è‡´å¤§é‡çš„è®¡ç®—å†—ä½™ã€‚å› æ­¤ï¼Œè¿™ç§ä½æ•ˆæ€§é˜»ç¢äº†åŸºäº NeRF çš„ SLAM å®æ—¶è¿è¡Œï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ç›´æ¥ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•æ˜¯åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¾èµ–äº RGB-D ç›¸æœºçš„æ·±åº¦å›¾è¾“å…¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´ã€‚é—®é¢˜æ˜¯è·Ÿè¸ªèƒ½åŠ›å¼±ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå®ƒå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ã€‚</p><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMã€‚è¯¥å·¥ä½œåœ¨ SLAM é¢†åŸŸå¼•å…¥äº†å¤šé¡¹çªç ´æ€§è¿›å±•ï¼ŒåŒ…æ‹¬å°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ (MVS) æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œå¹¶å¼€å‘äº†ç¨€ç– -å¯†é›†è°ƒæ•´ç¯ (SDAR) ä»¥ç¡®ä¿å°ºåº¦ä¸€è‡´æ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒæ˜¾ç€æé«˜äº†ä»…ä¾èµ– RGB å›¾åƒè¾“å…¥çš„ SLAM ç³»ç»Ÿçš„å‡†ç¡®æ€§å’ŒåŠŸèƒ½æ€§ã€‚</p><p>(4):æœ¬æ–‡çš„æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆå°±ï¼šåœ¨å„ç§åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä½å§¿ä¼°è®¡çš„å‡†ç¡®åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ–°çš„è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œä¸åˆ©ç”¨ RGB-D è¾“å…¥çš„ç¥ç» SLAM ç³»ç»Ÿçš„ç»“æœç›¸åŒ¹é…ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMï¼Œå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œä¸ºåç«¯å¯†é›†æ˜ å°„æä¾›å‡ ä½•æ·±åº¦ç›‘ç£ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œä»¥å‡è½»å…ˆéªŒæ·±åº¦å›¾è¯¯å·®å¯¹é«˜æ–¯åœ°å›¾å‡ ä½•é‡å»ºçš„å½±å“ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šè¯¥å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å•ç›®é«˜æ–¯æ•£å°„ SLAM ç³»ç»Ÿ MGS-SLAMï¼Œè¯¥ç³»ç»Ÿå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹åŸºäºé«˜æ–¯æ•£å°„çš„ SLAM ç³»ç»Ÿä¸­å…¸å‹çš„æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜é‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œå¹¶å¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚è¿™äº›åˆ›æ–°å…±åŒæ˜¾ç€æé«˜äº†ä»…ä¾èµ– RGB å›¾åƒè¾“å…¥çš„ SLAM ç³»ç»Ÿçš„å‡†ç¡®æ€§å’ŒåŠŸèƒ½æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†åŸºäºé«˜æ–¯æ•£å°„çš„æŠ€æœ¯ä¸ç¨€ç–è§†è§‰é‡Œç¨‹è®¡ç›¸ç»“åˆï¼Œæ¶ˆé™¤äº†å¯¹æ·±åº¦å›¾çš„ä¾èµ–æ€§ï¼Œå¹¶å¢å¼ºäº†è·Ÿè¸ªé²æ£’æ€§ï¼›é‡‡ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œä¸ºåç«¯å¯†é›†æ˜ å°„æä¾›å‡ ä½•æ·±åº¦ç›‘ç£ï¼›å¼€åˆ›äº†ä¸€ç§å‡ ä½•å¹³æ»‘æ·±åº¦æŸå¤±ï¼Œä»¥å‡è½»å…ˆéªŒæ·±åº¦å›¾è¯¯å·®å¯¹é«˜æ–¯åœ°å›¾å‡ ä½•é‡å»ºçš„å½±å“ï¼›å¼€å‘äº†ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰ï¼Œä»¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å„ç§åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ä½å§¿ä¼°è®¡çš„å‡†ç¡®åº¦è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ–°çš„è§†å›¾åˆæˆä¿çœŸåº¦æ–¹é¢ä¼˜äºä¹‹å‰çš„å•ç›®æ–¹æ³•ï¼Œä¸åˆ©ç”¨ RGB-D è¾“å…¥çš„ç¥ç» SLAM ç³»ç»Ÿçš„ç»“æœç›¸åŒ¹é…ã€‚</p><p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒçš„å¤šè§†å›¾ç«‹ä½“ï¼ˆMVSï¼‰æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œå¹¶ä¸”éœ€è¦å¼€å‘ç¨€ç–-å¯†é›†è°ƒæ•´ç¯ï¼ˆSDARï¼‰æ¥ç¡®ä¿ç¨€ç–ç‚¹äº‘åœ°å›¾å’Œå¯†é›†é«˜æ–¯åœ°å›¾ä¹‹é—´çš„å°ºåº¦ä¸€è‡´æ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9c81783ec5cc64db3f3888e91459cd94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5722638fadf13564cb13427fd8d4410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1359ff05ba09f3fd64460b9bd9878a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-940291f15a48e90bf4dec39f8ee7ddd2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5bc3a1c2278602383a64b530b3dd889.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e517184c75aa28276e746751c5d28917.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89f79eb15cc3b181f2efde56510f1d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5713c30b73286985fa8f2ff3f7ac2e21.jpg" align="middle"></details>## FastScene: Text-Driven Fast 3D Indoor Scene Generation via Panoramic   Gaussian Splatting**Authors:Yikun Ma, Dandan Zhan, Zhi Jin**Text-driven 3D indoor scene generation holds broad applications, ranging from gaming and smart homes to AR/VR applications. Fast and high-fidelity scene generation is paramount for ensuring user-friendly experiences. However, existing methods are characterized by lengthy generation processes or necessitate the intricate manual specification of motion parameters, which introduces inconvenience for users. Furthermore, these methods often rely on narrow-field viewpoint iterative generations, compromising global consistency and overall scene quality. To address these issues, we propose FastScene, a framework for fast and higher-quality 3D scene generation, while maintaining the scene consistency. Specifically, given a text prompt, we generate a panorama and estimate its depth, since the panorama encompasses information about the entire scene and exhibits explicit geometric constraints. To obtain high-quality novel views, we introduce the Coarse View Synthesis (CVS) and Progressive Novel View Inpainting (PNVI) strategies, ensuring both scene consistency and view quality. Subsequently, we utilize Multi-View Projection (MVP) to form perspective views, and apply 3D Gaussian Splatting (3DGS) for scene reconstruction. Comprehensive experiments demonstrate FastScene surpasses other methods in both generation speed and quality with better scene consistency. Notably, guided only by a text prompt, FastScene can generate a 3D scene within a mere 15 minutes, which is at least one hour faster than state-of-the-art methods, making it a paradigm for user-friendly scene generation. [PDF](http://arxiv.org/abs/2405.05768v1) Accepted by IJCAI-2024**æ‘˜è¦**æ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€æ™ºèƒ½å®¶å±…å’ŒAR/VRåº”ç”¨ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œå¿«é€Ÿä¸”é«˜ä¿çœŸåœºæ™¯ç”Ÿæˆå¯¹ç¡®ä¿ç”¨æˆ·å‹å¥½ä½“éªŒè‡³å…³é‡è¦ã€‚**è¦ç‚¹**- 3Då®¤å†…åœºæ™¯ç”Ÿæˆæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚- ç°æœ‰çš„æ–¹æ³•ç”Ÿæˆè¿‡ç¨‹è€—æ—¶æˆ–éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šè¿åŠ¨å‚æ•°ï¼Œç»™ç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚- ç°æœ‰çš„æ–¹æ³•ä¸“æ³¨äºçª„è§†åŸŸè§‚ç‚¹è¿­ä»£ç”Ÿæˆï¼Œå½±å“å…¨å±€ä¸€è‡´æ€§å’Œæ•´ä½“åœºæ™¯è´¨é‡ã€‚- FastSceneæ¡†æ¶å¯åœ¨ä¿æŒåœºæ™¯ä¸€è‡´æ€§çš„æƒ…å†µä¸‹å¿«é€Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„3Dåœºæ™¯ã€‚- æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå…¨æ™¯å›¾å¹¶ä¼°è®¡å…¶æ·±åº¦ï¼Œå› ä¸ºå…¨æ™¯å›¾åŒ…å«æ•´ä¸ªåœºæ™¯ä¿¡æ¯å¹¶å±•ç¤ºæ˜ç¡®çš„å‡ ä½•çº¦æŸã€‚- å¼•å…¥ç²—è§†å›¾åˆæˆï¼ˆCVSï¼‰å’Œæ¸è¿›å¼æ–°è§†å›¾ä¿®å¤ï¼ˆPNVIï¼‰ç­–ç•¥æ¥è·å¾—é«˜è´¨é‡çš„æ–°è§†è§’ï¼Œç¡®ä¿åœºæ™¯ä¸€è‡´æ€§å’Œè§†å›¾è´¨é‡ã€‚- ä½¿ç”¨å¤šè§†å›¾æŠ•å½±ï¼ˆMVPï¼‰å½¢æˆé€è§†è§†å›¾ï¼Œå¹¶åº”ç”¨3Dé«˜æ–¯æ•£å°„ï¼ˆ3DGSï¼‰è¿›è¡Œåœºæ™¯é‡å»ºã€‚- å…¨é¢å®éªŒè¡¨æ˜ï¼ŒFastSceneåœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡ä¸Šéƒ½è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„åœºæ™¯ä¸€è‡´æ€§ã€‚- FastSceneä»…é€šè¿‡æ–‡æœ¬æç¤ºå°±å¯ä»¥åœ¨çŸ­çŸ­15åˆ†é’Ÿå†…ç”Ÿæˆ3Dåœºæ™¯ï¼Œæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å¿«è‡³å°‘1å°æ—¶ï¼Œä½¿å…¶æˆä¸ºç”¨æˆ·å‹å¥½åœºæ™¯ç”ŸæˆèŒƒä¾‹ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šFastSceneï¼šæ–‡æœ¬é©±åŠ¨çš„å¿«é€Ÿ 3D å®¤å†…åœºæ™¯ç”Ÿæˆ</p></li><li><p>ä½œè€…ï¼šYikun Maï¼ŒDandan Zhanï¼ŒZhi Jin</p></li><li><p>å•ä½ï¼šä¸­å±±å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D åœºæ™¯ç”Ÿæˆï¼Œå…¨æ™¯å›¾ï¼Œé«˜æ–¯ä½“ç´ æ¸²æŸ“</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šPaper_infoï¼ŒGithub é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬é©±åŠ¨çš„ 3D å®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€æ™ºèƒ½å®¶å±…ã€AR/VR ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚å¿«é€Ÿã€é«˜ä¿çœŸçš„åœºæ™¯ç”Ÿæˆå¯¹äºç¡®ä¿ç”¨æˆ·å‹å¥½çš„ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„ç‰¹ç‚¹æ˜¯ç”Ÿæˆè¿‡ç¨‹å†—é•¿æˆ–éœ€è¦å¤æ‚çš„æ‰‹åŠ¨æŒ‡å®šè¿åŠ¨å‚æ•°ï¼Œç»™ç”¨æˆ·å¸¦æ¥ä¸ä¾¿ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºçª„è§†åœºè§†ç‚¹è¿­ä»£ç”Ÿæˆï¼Œå½±å“äº†å…¨å±€ä¸€è‡´æ€§å’Œæ•´ä½“åœºæ™¯è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šSet-the-Scene ä»æ–‡æœ¬æç¤ºå’Œ 3D å¯¹è±¡ä»£ç†è¿›è¡Œå…¨å±€å±€éƒ¨è®­ç»ƒï¼Œç”Ÿæˆå¯æ§åœºæ™¯ã€‚ä½†ç”±äºç¼ºä¹ç›¸åº”çš„å‡ ä½•ä¿¡æ¯ï¼Œç”Ÿæˆåœºæ™¯çš„è´¨é‡å’Œåˆ†è¾¨ç‡ä¸ä½³ã€‚SceneScape ç”Ÿæˆé•¿è·ç¦»è§†å›¾ï¼Œç”Ÿæˆé£æ ¼å¤šæ ·ã€‚ä½†ç”±äºå†…ç»˜å’Œæ·±åº¦ä¼°è®¡è¯¯å·®çš„ç§¯ç´¯ï¼Œå…¶è§†å›¾è´¨é‡ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œé™ä½ã€‚Text2Room å’Œ Text2NeRF é€æ­¥ç”Ÿæˆé€è§†æ–°è§†å›¾ã€‚ä½†å…¶å¢é‡å±€éƒ¨æ“ä½œéš¾ä»¥ä¿è¯åœºæ™¯ä¸€è‡´æ€§å’Œè¿è´¯æ€§ã€‚Ctrl-Room å¯¹ ControlNet è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆå¯ç¼–è¾‘çš„å…¨æ™¯å›¾ï¼Œç„¶åæ‰§è¡Œç½‘æ ¼é‡å»ºã€‚ä½†ç”±äº Ctrl-Room éš¾ä»¥ç”Ÿæˆå¤šè§†å›¾å›¾åƒï¼Œå› æ­¤å®ƒå€¾å‘äºå°† 3D æ¨¡å‹æ‰å¹³åŒ–ï¼Œåœºæ™¯è´¨é‡æœ‰é™ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ° 3D åœºæ™¯æ¡†æ¶ï¼Œç§°ä¸º FastSceneï¼Œæ—¨åœ¨å¿«é€Ÿç”Ÿæˆä¸€è‡´ã€çœŸå®ä¸”é«˜è´¨é‡çš„åœºæ™¯ã€‚å¦‚å›¾ 1 æ‰€ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µã€‚1ï¼‰åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„ Diffusion360 ç”Ÿæˆå…¨æ™¯å›¾ã€‚é€‰æ‹©å…¨æ™¯å›¾æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæ•è·å…¨å±€ä¿¡æ¯å¹¶è¡¨ç°å‡ºæ˜ç¡®çš„å‡ ä½•çº¦æŸã€‚2ï¼‰åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å…¨æ™¯å›¾åŠå…¶æ·±åº¦ä¼°è®¡æ¥ç”Ÿæˆç²—ç•¥è§†å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥ç²—ç•¥è§†å›¾åˆæˆ (CVS) å’Œæ¸è¿›å¼æ–°è§†å›¾å†…ç»˜ (PNVI) ç­–ç•¥æ¥ç»†åŒ–ç²—ç•¥è§†å›¾ï¼ŒåŒæ—¶ç¡®ä¿åœºæ™¯ä¸€è‡´æ€§å’Œè§†å›¾è´¨é‡ã€‚3ï¼‰åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šè§†å›¾æŠ•å½± (MVP) å½¢æˆé€è§†è§†å›¾ï¼Œå¹¶åº”ç”¨ 3D é«˜æ–¯ä½“ç´ æ¸²æŸ“ (3DGS) è¿›è¡Œåœºæ™¯é‡å»ºã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼ŒFastScene åœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœºæ™¯ä¸€è‡´æ€§æ›´å¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFastScene ä»…åœ¨æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…ç”Ÿæˆä¸€ä¸ª 3D åœºæ™¯ï¼Œè¿™æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•è‡³å°‘å¿«ä¸€ä¸ªå°æ—¶ï¼Œä½¿å…¶æˆä¸ºç”¨æˆ·å‹å¥½åœºæ™¯ç”Ÿæˆçš„ä¸€ä¸ªå…¸èŒƒã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šDiffusion360ç”Ÿæˆå…¨æ™¯å›¾ï¼ŒEGformerä¼°è®¡æ·±åº¦å›¾ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šCVSç”Ÿæˆå¸¦æœ‰å­”æ´çš„æ–°å…¨æ™¯å›¾ï¼ŒPNVIé€æ­¥ä¿®å¤å­”æ´ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šMVPç”Ÿæˆé€è§†è§†å›¾ï¼Œ3DGSè¿›è¡Œåœºæ™¯é‡å»ºã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¿«é€Ÿæ–‡æœ¬åˆ° 3D å®¤å†…åœºæ™¯ç”Ÿæˆæ¡†æ¶ FastSceneï¼Œå±•ç¤ºäº†ä»¤äººæ»¡æ„çš„åœºæ™¯è´¨é‡å’Œä¸€è‡´æ€§ã€‚å¯¹äºç”¨æˆ·è€Œè¨€ï¼ŒFastScene åªéœ€è¦ä¸€ä¸ªæ–‡æœ¬æç¤ºï¼Œæ— éœ€è®¾è®¡è¿åŠ¨å‚æ•°ï¼Œå³å¯åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…æä¾›ä¸€ä¸ªå®Œæ•´çš„é«˜è´¨é‡ 3D åœºæ™¯ã€‚æå‡ºçš„ PNVI ä¸ CVS å¯ä»¥ç”Ÿæˆä¸€è‡´çš„æ–°å…¨æ™¯è§†å›¾ï¼Œè€Œ MVP å°†å…¶æŠ•å½±åˆ°é€è§†è§†å›¾ï¼Œä¿ƒè¿›äº† 3DGS é‡å»ºã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚FastScene æä¾›äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„åœºæ™¯ç”ŸæˆèŒƒä¾‹ï¼Œæˆ‘ä»¬ç›¸ä¿¡å®ƒå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ã€‚åœ¨æœªæ¥çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ 3D åœºæ™¯ç¼–è¾‘å’Œå¤šæ¨¡æ€å­¦ä¹ ã€‚è‡´è°¢ æœ¬å·¥ä½œå¾—åˆ°äº†å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆNo.62071500ï¼‰å’Œæ·±åœ³å¸‚ç§‘æŠ€è®¡åˆ’ï¼ˆGrant No. JCYJ20230807111107015ï¼‰çš„æ”¯æŒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå…¨æ™¯å›¾çš„å¿«é€Ÿæ–‡æœ¬åˆ° 3D å®¤å†…åœºæ™¯ç”Ÿæˆæ¡†æ¶ FastSceneï¼›æ€§èƒ½ï¼šFastScene åœ¨ç”Ÿæˆé€Ÿåº¦å’Œè´¨é‡æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”åœºæ™¯ä¸€è‡´æ€§æ›´å¥½ï¼›å·¥ä½œé‡ï¼šFastScene ä»…åœ¨æ–‡æœ¬æç¤ºçš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥åœ¨çŸ­çŸ­ 15 åˆ†é’Ÿå†…ç”Ÿæˆä¸€ä¸ª 3D åœºæ™¯ï¼Œè¿™æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•è‡³å°‘å¿«ä¸€ä¸ªå°æ—¶ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-9ae84b1fe141ce2458a3514ff61edab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9516335b56aaf68e720f85429fe6d949.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcf104105c3e3c0c631f51aa64860b19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6035d44b6617ded58ccc09ecb36f41eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f44233f42fcbaf0d92844c77c24e8b3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a657f39b3ff13b487d3da4b747083bfc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-30b15f3bf60cdbeb4ed8595da183fcab.jpg" align="middle"><img src="https://pica.zhimg.com/v2-66a918dd33cc4c399a7322eb37b47e0d.jpg" align="middle"></details>## Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review**Authors:Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri KnausgÃ¥rd**Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting. [PDF](http://arxiv.org/abs/2405.03417v1) 24 pages**Summary**åŸºäºå›¾åƒçš„3Dé‡å»ºæ˜¯ä¸€é¡¹é¢‡å…·æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œæ¶‰åŠä»ä¸€ç»„è¾“å…¥å›¾åƒä¸­æ¨æ–­ç‰©ä½“çš„3Då½¢çŠ¶ã€‚åŸºäºå­¦ä¹ çš„æ–¹æ³•å› å…¶ç›´æ¥ä¼°è®¡3Då½¢çŠ¶çš„èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»3Dé‡å»ºçš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ–°é¢–çš„ã€æœªæ›¾è§è¿‡çš„è§†å›¾ã€‚æ¦‚è¿°äº†é«˜æ–¯æ•£å¸ƒæ–¹æ³•çš„æœ€æ–°å‘å±•ï¼Œæ¶µç›–è¾“å…¥ç±»å‹ã€æ¨¡å‹ç»“æ„ã€è¾“å‡ºè¡¨ç¤ºå’Œè®­ç»ƒç­–ç•¥ã€‚è¿˜è®¨è®ºäº†å°šæœªè§£å†³çš„æŒ‘æˆ˜å’Œæœªæ¥çš„å‘å±•æ–¹å‘ã€‚é‰´äºè¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ä»¥åŠæé«˜3Dé‡å»ºæ–¹æ³•çš„ä¼—å¤šæœºä¼šï¼Œå¯¹ç®—æ³•è¿›è¡Œå…¨é¢æ£€æŸ¥è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¯¹é«˜æ–¯æ•£å¸ƒçš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢æ¦‚è¿°ã€‚**Key Takeaways**- å›¾åƒ-åŸºäº3Dé‡å»ºåŒ…æ‹¬ä»ä¸€ç»„è¾“å…¥å›¾åƒæ¨æ–­å¯¹è±¡çš„3Då½¢çŠ¶ã€‚- å­¦ä¹ -åŸºäºæ–¹æ³•å› å…¶ç›´æ¥ä¼°è®¡3Då½¢çŠ¶çš„èƒ½åŠ›å¤‡å—å…³æ³¨ã€‚- é«˜æ–¯æ•£å¸ƒæ˜¯ä¸€ä¸ªç”¨äº3Dé‡å»ºçš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚- é«˜æ–¯æ•£å¸ƒè¾“å…¥ç±»å‹åŒ…æ‹¬å•ç›®å’Œå¤šç›®å›¾åƒã€‚- é«˜æ–¯æ•£å¸ƒæ¨¡å‹ç»“æ„åŒ…æ‹¬ç¼–ç å™¨-è§£ç å™¨å’ŒTransformerã€‚- é«˜æ–¯æ•£å¸ƒè¾“å‡ºè¡¨ç¤ºåŒ…æ‹¬ä½“ç´ ç½‘æ ¼å’Œç‚¹äº‘ã€‚- é«˜æ–¯æ•£å¸ƒè®­ç»ƒç­–ç•¥åŒ…æ‹¬ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: Gaussian Splatting: 3D Reconstruction and Novel View Synthesis, a Review</p></li><li><p>Authors: Anurag Dalal, Daniel Hagen, Kjell G. Robbersmyr, Kristian Muri KnausgÃ¥rd</p></li><li><p>Affiliation: Department of Engineering Sciences, University of Agder, Grimstad, Norway</p></li><li><p>Keywords: 3D Reconstruction, Computer Vision, Deep Learning, Gaussian Splatting, Novel view synthesis, Optimization, Rendering</p></li><li><p>Urls: Paper_info:Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.Digital Object Identifier xxxx</p></li><li><p>Summary:</p><pre><code>           (1):Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.;           (2):Traditional approaches to 3D reconstruction, such as photogrammetry and multi-view stereo (MVS) algorithms, often suffer from artifacts, failure cases, and slow training times. Gaussian Splatting is a novel method that addresses these limitations by representing 3D objects as a collection of Gaussians. This representation allows for efficient rendering and interpolation, resulting in high-quality novel views.;           (3):The Gaussian Splatting method involves an iterative refinement process, where multiple Gaussians are optimized to match the input images. The model is trained using a combination of supervised and unsupervised losses, which encourage consistency with the input images and smoothness in the 3D space. The output of the model is a volumetric point cloud, where each point represents a Gaussian with parameters such as color, spread, and location.;           (4):Gaussian Splatting has been shown to achieve state-of-the-art results on a variety of 3D reconstruction and novel view synthesis tasks. The method outperforms previous approaches in terms of rendering quality, training time, and robustness to challenging scenes. These results demonstrate the potential of Gaussian Splatting for a wide range of applications, including virtual reality, augmented reality, and computer-aided design.</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1):æœ¬æ–‡é¦–å…ˆä»‹ç»äº†é«˜æ–¯æ•£ç‚¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨é«˜æ–¯å‡½æ•°é›†åˆè¡¨ç¤º 3D ç‰©ä½“çš„åˆ›æ–°æ–¹æ³•ã€‚è¿™ç§è¡¨ç¤ºå½¢å¼å…è®¸é«˜æ•ˆæ¸²æŸ“å’Œæ’å€¼ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ï¼›           (2):é«˜æ–¯æ•£ç‚¹æ³•æ¶‰åŠä¸€ä¸ªè¿­ä»£ç»†åŒ–è¿‡ç¨‹ï¼Œå…¶ä¸­ä¼˜åŒ–å¤šä¸ªé«˜æ–¯å‡½æ•°ä»¥åŒ¹é…è¾“å…¥å›¾åƒã€‚æ¨¡å‹ä½¿ç”¨ç›‘ç£å’Œæ— ç›‘ç£æŸå¤±çš„ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œè¿™é¼“åŠ±ä¸è¾“å…¥å›¾åƒçš„ä¸€è‡´æ€§å’Œ 3D ç©ºé—´ä¸­çš„å¹³æ»‘åº¦ã€‚æ¨¡å‹çš„è¾“å‡ºæ˜¯ä½“ç§¯ç‚¹äº‘ï¼Œå…¶ä¸­æ¯ä¸ªç‚¹è¡¨ç¤ºä¸€ä¸ªå…·æœ‰é¢œè‰²ã€æ‰©å±•å’Œä½ç½®ç­‰å‚æ•°çš„é«˜æ–¯å‡½æ•°ï¼›           (3):é«˜æ–¯æ•£ç‚¹æ³•å·²è¢«è¯æ˜åœ¨å„ç§ 3D é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚è¯¥æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§åœºæ™¯çš„é²æ£’æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†é«˜æ–¯æ•£ç‚¹æ³•åœ¨å¹¿æ³›çš„åº”ç”¨ä¸­çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡ä»åŠŸèƒ½å’Œåº”ç”¨è§’åº¦å¯¹é«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†åŠ¨æ€å»ºæ¨¡ã€å˜å½¢å»ºæ¨¡ã€è¿åŠ¨è·Ÿè¸ªã€éåˆšæ€§/å¯å˜å½¢ç‰©ä½“ã€è¡¨æƒ…/æƒ…ç»ªå˜åŒ–ã€åŸºäºæ–‡æœ¬çš„ç”Ÿæˆæ‰©æ•£ã€é™å™ªã€ä¼˜åŒ–ã€è™šæ‹Ÿå½¢è±¡ã€å¯åŠ¨ç”»å¯¹è±¡ã€å¤´éƒ¨å»ºæ¨¡ã€åŒæ­¥å®šä½ä¸è§„åˆ’ã€ç½‘æ ¼æå–ä¸ç‰©ç†ã€ä¼˜åŒ–æŠ€æœ¯ã€ç¼–è¾‘èƒ½åŠ›ã€æ¸²æŸ“æ–¹æ³•ã€å‹ç¼©ç­‰æ–¹é¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ¬æ–‡æ·±å…¥æ¢è®¨äº†å›¾åƒä¸‰ç»´é‡å»ºä¸­çš„æŒ‘æˆ˜å’Œè¿›å±•ï¼Œå­¦ä¹ å‹æ–¹æ³•åœ¨ä¸‰ç»´å½¢çŠ¶ä¼°è®¡ä¸­çš„ä½œç”¨ï¼Œä»¥åŠé«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šé«˜æ–¯æ•£ç‚¹æ³•æ˜¯ä¸€ç§ä½¿ç”¨é«˜æ–¯å‡½æ•°é›†åˆè¡¨ç¤ºä¸‰ç»´ç‰©ä½“çš„åˆ›æ–°æ–¹æ³•ï¼Œè¿™ç§è¡¨ç¤ºå½¢å¼å…è®¸é«˜æ•ˆæ¸²æŸ“å’Œæ’å€¼ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„æ–°é¢–è§†å›¾ï¼›æ€§èƒ½ï¼šé«˜æ–¯æ•£ç‚¹æ³•åœ¨ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨æ¸²æŸ“è´¨é‡ã€è®­ç»ƒæ—¶é—´å’Œå¯¹å…·æœ‰æŒ‘æˆ˜æ€§åœºæ™¯çš„é²æ£’æ€§æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šé«˜æ–¯æ•£ç‚¹æ³•æ¶‰åŠä¸€ä¸ªè¿­ä»£ç»†åŒ–è¿‡ç¨‹ï¼Œå…¶ä¸­ä¼˜åŒ–å¤šä¸ªé«˜æ–¯å‡½æ•°ä»¥åŒ¹é…è¾“å…¥å›¾åƒï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦è¾ƒå¤§çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-48d38462ddefdcfe75129220282e7a18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-712a52026b682e9ab729dccf592cc5f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14985716143782f83102a5633ec37c23.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3dd2127ce2dbe6cdafc1b40d9cc2fb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f865d904180e8ed6511d41dac5f81c0.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v3) To be published in ACM SIGGRAPH 2024**Summary**å®æ—¶é«˜æ–¯ SLAM ç³»ç»Ÿä½¿ç”¨é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºæ–¹å¼å®ç°äº†å¤§è§„æ¨¡ RGBD å›¾åƒåºåˆ—çš„é‡å»ºï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„é«˜æ–¯ä¼˜åŒ–æ–¹æ³•å®æ—¶ç”Ÿæˆè¿ç»­çš„ä¸‰ç»´é‡å»ºç»“æœã€‚**Key Takeaways**- ä½¿ç”¨é«˜æ–¯ç‚¹äº‘è¡¨ç¤ºç¯å¢ƒï¼Œç´§å‡‘é«˜æ•ˆã€‚- å°†é«˜æ–¯ç‚¹äº‘åˆ†ä¸ºä¸é€æ˜å’ŒåŠé€æ˜ä¸¤ç§ï¼Œä¸é€æ˜ç‚¹äº‘æ‹Ÿåˆè¡¨é¢å’Œä¸»è¦é¢œè‰²ï¼ŒåŠé€æ˜ç‚¹äº‘æ‹Ÿåˆæ®‹å·®é¢œè‰²ã€‚- é€šè¿‡æ·±åº¦æ¸²æŸ“å’Œé¢œè‰²æ¸²æŸ“åˆ†ç¦»ï¼Œå•ä¸ªä¸é€æ˜é«˜æ–¯ç‚¹äº‘å°±èƒ½æ‹Ÿåˆå±€éƒ¨è¡¨é¢åŒºåŸŸï¼Œå‡å°‘äº†é«˜æ–¯ç‚¹äº‘æ•°é‡ã€å­˜å‚¨ç©ºé—´å’Œè®¡ç®—æˆæœ¬ã€‚- å®æ—¶é«˜æ–¯ä¼˜åŒ–ï¼Œé’ˆå¯¹æ–°è§‚æµ‹åˆ°çš„åƒç´ ã€é¢œè‰²è¯¯å·®å¤§çš„åƒç´ å’Œæ·±åº¦è¯¯å·®å¤§çš„åƒç´ æ·»åŠ é«˜æ–¯ç‚¹äº‘ã€‚- å°†é«˜æ–¯ç‚¹äº‘åˆ†ä¸ºç¨³å®šå’Œä¸ç¨³å®šä¸¤ç§ï¼Œä»…ä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ç‚¹äº‘ï¼Œä»…æ¸²æŸ“ä¸ç¨³å®šé«˜æ–¯ç‚¹äº‘è¦†ç›–çš„åƒç´ ã€‚- ä¸åŸºäº NeRF çš„ RGBD SLAM ç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿé‡å»ºè´¨é‡ç›¸å½“ï¼Œä½†é€Ÿåº¦æé«˜çº¦ä¸€å€ï¼Œå†…å­˜æˆæœ¬å‡åŠï¼Œå¹¶ä¸”åœ¨æ–°çš„è§†å›¾åˆæˆçœŸå®æ„Ÿå’Œç›¸æœºè·Ÿè¸ªå‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAMï¼šä½¿ç”¨é«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»º</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: RGBD SLAM, 3D reconstruction, Gaussian splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯éšç€ RGBD ç›¸æœºçš„å‘å±•ï¼Œå®æ—¶ä¸‰ç»´é‡å»ºæŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡ç¯å¢ƒæ—¶ï¼Œå¾€å¾€é¢ä¸´å†…å­˜æ¶ˆè€—å¤§ã€è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œä½† NeRF éœ€è¦å¤§é‡çš„é«˜æ–¯ä½“ç´ æ¥æ‹Ÿåˆè¡¨é¢ï¼Œå¯¼è‡´å†…å­˜æ¶ˆè€—å¤§ã€‚</p><p>(3): è¯¥æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿ RTG-SLAMã€‚RTG-SLAM ä½¿ç”¨é«˜æ–¯ä½“ç´ æ¥è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚æ­¤å¤–ï¼ŒRTG-SLAM é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p><p>(4): åœ¨å¤§è§„æ¨¡ç¯å¢ƒé‡å»ºä»»åŠ¡ä¸Šï¼ŒRTG-SLAM å®ç°äº†ä¸ NeRF-SLAM ç›¸å½“çš„é‡å»ºè´¨é‡ï¼Œä½†é€Ÿåº¦æé«˜äº†çº¦ä¸¤å€ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘äº†ä¸€åŠã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1): è¯¥æ–¹æ³•ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚           (2): è¯¥æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚           (3): è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡ç¯å¢ƒé‡å»ºä»»åŠ¡ä¸Šï¼Œå®ç°äº†ä¸ NeRF-SLAM ç›¸å½“çš„é‡å»ºè´¨é‡ï¼Œä½†é€Ÿåº¦æé«˜äº†çº¦ä¸¤å€ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘äº†ä¸€åŠã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>           ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿ RTG-SLAMï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œåœ¨ä¿è¯é‡å»ºè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†é‡å»ºé€Ÿåº¦ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚           ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¸²æŸ“çš„å¤§è§„æ¨¡å®æ—¶ä¸‰ç»´é‡å»ºç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä½¿ç”¨é«˜æ–¯ä½“ç´ è¡¨ç¤ºä¸‰ç»´åœºæ™¯ï¼Œå¹¶å¼ºåˆ¶æ¯ä¸ªé«˜æ–¯ä½“ç´ è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œæ¥å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œåªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ ï¼Œè¿›ä¸€æ­¥é™ä½äº†è®¡ç®—æˆæœ¬ã€‚           æ€§èƒ½ï¼šåœ¨ä¿è¯é‡å»ºè´¨é‡çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†é‡å»ºé€Ÿåº¦ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚           å·¥ä½œé‡ï¼šéœ€è¦å¯¹ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ è¿›è¡Œä¼˜åŒ–ï¼Œéœ€è¦æ¸²æŸ“ä¸ç¨³å®šçš„é«˜æ–¯ä½“ç´ å æ®çš„åƒç´ ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0eaefb973e265febe848896437a17659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a058fc44423666e88d6baa1e211422b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Talking%20Head%20Generation/</id>
    <published>2024-05-13T08:02:14.000Z</published>
    <updated>2024-05-13T08:02:14.752Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior"><a href="#NeRFFaceSpeech-One-shot-Audio-driven-3D-Talking-Head-Synthesis-via-Generative-Prior" class="headerlink" title="NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior"></a>NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</h2><p><strong>Authors:Gihoon Kim, Kwanggyoon Seo, Sihun Cha, Junyong Noh</strong></p><p>Audio-driven talking head generation is advancing from 2D to 3D content. Notably, Neural Radiance Field (NeRF) is in the spotlight as a means to synthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based approach typically requires a large number of paired audio-visual data for each identity, thereby limiting the scalability of the method. Although there have been attempts to generate audio-driven 3D talking head animations with a single image, the results are often unsatisfactory due to insufficient information on obscured regions in the image. In this paper, we mainly focus on addressing the overlooked aspect of 3D consistency in the one-shot, audio-driven domain, where facial animations are synthesized primarily in front-facing perspectives. We propose a novel method, NeRFFaceSpeech, which enables to produce high-quality 3D-aware talking head. Using prior knowledge of generative models combined with NeRF, our method can craft a 3D-consistent facial feature space corresponding to a single image. Our spatial synchronization method employs audio-correlated vertex dynamics of a parametric face model to transform static image features into dynamic visuals through ray deformation, ensuring realistic 3D facial motion. Moreover, we introduce LipaintNet that can replenish the lacking information in the inner-mouth area, which can not be obtained from a given single image. The network is trained in a self-supervised manner by utilizing the generative capabilities without additional data. The comprehensive experiments demonstrate the superiority of our method in generating audio-driven talking heads from a single image with enhanced 3D consistency compared to previous approaches. In addition, we introduce a quantitative way of measuring the robustness of a model against pose changes for the first time, which has been possible only qualitatively. </p><p><a href="http://arxiv.org/abs/2405.05749v2">PDF</a> 11 pages, 5 figures</p><p><strong>Summary</strong><br>å•å¼ äººè„¸å›¾åƒå³å¯é©±åŠ¨ 3D ä¼šè¯å¼äººå¤´çš„ç”Ÿæˆï¼Œè¿™æ˜¯ç”±äºå¯¹ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯å’Œç”Ÿæˆæ¨¡å‹çš„å·§å¦™è¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-based 3D talking head generation typically requires a large amount of paired audio-visual data.</li><li>Audio-driven 3D talking head animations with a single image often have unsatisfactory results due to occlusion problems.</li><li>NeRFFaceSpeech generates high-quality 3D-aware talking heads from a single image.</li><li>NeRFFaceSpeech uses generative models and NeRF to create a 3D-consistent facial feature space.</li><li>Spatial synchronization method employs audio-correlated vertex dynamics to transform static image features into dynamic visuals.</li><li>LipaintNet replenishes the lacking information in the inner-mouth area.</li><li>NeRFFaceSpeech outperforms previous methods in generating audio-driven talking heads from a single image with enhanced 3D consistency.</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>é¢˜ç›®ï¼šNeRFFaceSpeechï¼šåŸºäºç”Ÿæˆå…ˆéªŒçš„å•æ¬¡éŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åˆæˆ</p></li><li><p>ä½œè€…ï¼šGihoon Kimï¼ŒKwanggyoon Seoï¼ŒSihun Chaï¼ŒJunyong Noh</p></li><li><p>æ‰€å±æœºæ„ï¼šé¦–å°”å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼ŒéŸ³é¢‘é©±åŠ¨ï¼Œä¸‰ç»´è¯´è¯äººå¤´éƒ¨ï¼Œç”Ÿæˆå…ˆéªŒ</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.05749ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ­£ä»äºŒç»´å†…å®¹è½¬å‘ä¸‰ç»´å†…å®¹ã€‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºä¸€ç§åˆæˆé«˜è´¨é‡ä¸‰ç»´è¯´è¯äººå¤´éƒ¨è¾“å‡ºçš„æ–¹æ³•å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™ç§åŸºäºNeRFçš„æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡é’ˆå¯¹æ¯ä¸ªèº«ä»½æˆå¯¹çš„éŸ³é¢‘-è§†è§‰æ•°æ®ï¼Œä»è€Œé™åˆ¶äº†è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ã€‚å°½ç®¡å·²ç»å°è¯•ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœå¾€å¾€ä¸ä»¤äººæ»¡æ„ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨è§£å†³å•æ¬¡ã€éŸ³é¢‘é©±åŠ¨çš„é¢†åŸŸä¸­è¢«å¿½è§†çš„ä¸‰ç»´ä¸€è‡´æ€§æ–¹é¢ï¼Œå…¶ä¸­é¢éƒ¨åŠ¨ç”»ä¸»è¦ä»¥æ­£é¢è§†è§’åˆæˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç°æœ‰æ–¹æ³•è¯•å›¾ä½¿ç”¨å•å¼ å›¾åƒç”ŸæˆéŸ³é¢‘é©±åŠ¨çš„ä¸‰ç»´è¯´è¯äººå¤´éƒ¨åŠ¨ç”»ï¼Œä½†ç”±äºå›¾åƒä¸­é®æŒ¡åŒºåŸŸçš„ä¿¡æ¯ä¸è¶³ï¼Œç»“æœå¾€å¾€ä¸ä»¤äººæ»¡æ„ã€‚æœ¬æ–‡çš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•NeRFFaceSpeechï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´æ„ŸçŸ¥è¯´è¯äººå¤´éƒ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’ŒNeRFï¼Œå¯ä»¥æ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬çš„ç©ºé—´åŒæ­¥æ–¹æ³•é‡‡ç”¨å‚æ•°åŒ–é¢éƒ¨æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ä¸‰ç»´é¢éƒ¨è¿åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LipaintNetï¼Œå®ƒå¯ä»¥è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…éƒ¨å£è…”åŒºåŸŸçš„ç¼ºå¤±ä¿¡æ¯ã€‚è¯¥ç½‘ç»œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œåˆ©ç”¨ç”Ÿæˆèƒ½åŠ›è€Œæ— éœ€é¢å¤–æ•°æ®ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¢å¼ºä¸‰ç»´ä¸€è‡´æ€§çš„éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†ä¸€ç§è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œè¿™åœ¨è¿‡å»åªèƒ½å®šæ€§åœ°è¿›è¡Œã€‚</p><ol><li>Methods:</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºNeRFFaceSpeechæ–¹æ³•ï¼Œç»“åˆç”Ÿæˆæ¨¡å‹å…ˆéªŒå’ŒNeRFï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨å‚æ•°åŒ–é¢éƒ¨æ¨¡å‹çš„éŸ³é¢‘ç›¸å…³é¡¶ç‚¹åŠ¨æ€ï¼Œé€šè¿‡å…‰çº¿å˜å½¢å°†é™æ€å›¾åƒç‰¹å¾è½¬æ¢ä¸ºåŠ¨æ€è§†è§‰æ•ˆæœï¼Œç¡®ä¿é€¼çœŸçš„ä¸‰ç»´é¢éƒ¨è¿åŠ¨ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼•å…¥LipaintNetï¼Œä»¥è‡ªç›‘ç£çš„æ–¹å¼è¡¥å……å•å¼ ç»™å®šå›¾åƒä¸­æ— æ³•è·å¾—çš„å†…éƒ¨å£è…”åŒºåŸŸçš„ç¼ºå¤±ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–æ•°æ®ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ï¼Œé¦–æ¬¡å®ç°å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡è¯„ä¼°ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†NeRFFaceSpeechï¼Œä¸€ç§é€šè¿‡åˆ©ç”¨ç”Ÿæˆå…ˆéªŒæ„å»ºå’Œæ“ä½œä¸‰ç»´ç‰¹å¾ï¼Œä»å•å¼ å›¾åƒç”Ÿæˆä¸‰ç»´æ„ŸçŸ¥éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨åŠ¨ç”»çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç®¡é“å¼¥åˆäº†é¢éƒ¨å‚æ•°åŒ–æ¨¡å‹å’Œç¥ç»æ¸²æŸ“ä¹‹é—´çš„å·®è·ï¼Œé€šè¿‡å…‰çº¿å˜å½¢ç›´è§‚åœ°æ“çºµç‰¹å¾ç©ºé—´ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†LipaintNetï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›æ¥åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼Œè¡¥å……å˜å½¢åœºä»¥äº§ç”Ÿå¯è¡Œçš„ç»“æœã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å¯¹å§¿åŠ¿å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆæ¯”ä»¥å‰çš„æ–¹æ³•æ›´å¥½çš„å†…éƒ¨å£éƒ¨ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†æ–‡åŒ–ä½“è‚²è§‚å…‰éƒ¨R&amp;Dè®¡åˆ’çš„æ”¯æŒï¼Œè¯¥è®¡åˆ’ç”±æ–‡åŒ–ä½“è‚²è§‚å…‰éƒ¨èµ„åŠ©çš„KOCCAèµ æ¬¾ï¼ˆç¼–å·ï¼šRS-2023-00228331ï¼‰èµ„åŠ©ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†ç”Ÿæˆæ¨¡å‹å…ˆéªŒä¸NeRFç›¸ç»“åˆï¼Œæ„å»ºä¸å•å¼ å›¾åƒç›¸å¯¹åº”çš„ä¸‰ç»´ä¸€è‡´çš„é¢éƒ¨ç‰¹å¾ç©ºé—´ï¼›æå‡ºLipaintNetï¼Œä¸€ä¸ªè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›æ¥åˆæˆéšè—çš„å†…å£åŒºåŸŸï¼›å¼•å…¥è¡¡é‡æ¨¡å‹å¯¹å§¿æ€å˜åŒ–é²æ£’æ€§çš„å®šé‡æ–¹æ³•ã€‚æ€§èƒ½ï¼šä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆå…·æœ‰å¢å¼ºä¸‰ç»´ä¸€è‡´æ€§çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚å·¥ä½œé‡ï¼šä¸éœ€è¦å¤§é‡æˆå¯¹éŸ³é¢‘-è§†è§‰æ•°æ®çš„åŸºäºNeRFçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ä¸€å¼ å›¾åƒï¼Œå·¥ä½œé‡æ›´å°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-2a60d3f8bc167b5a06ffeda10f57dfc8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d422ea4050244e053b7e4851bb4a9ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e65d136edc8fc7443ae44525f2b6db77.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e5fb53c0c038366d8c74e34f9bffdfb.jpg" align="middle"></details><h2 id="SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space"><a href="#SwapTalk-Audio-Driven-Talking-Face-Generation-with-One-Shot-Customization-in-Latent-Space" class="headerlink" title="SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space"></a>SwapTalk: Audio-Driven Talking Face Generation with One-Shot   Customization in Latent Space</h2><p><strong>Authors:Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</strong></p><p>Combining face swapping with lip synchronization technology offers a cost-effective solution for customized talking face generation. However, directly cascading existing models together tends to introduce significant interference between tasks and reduce video clarity because the interaction space is limited to the low-level semantic RGB space. To address this issue, we propose an innovative unified framework, SwapTalk, which accomplishes both face swapping and lip synchronization tasks in the same latent space. Referring to recent work on face generation, we choose the VQ-embedding space due to its excellent editability and fidelity performance. To enhance the frameworkâ€™s generalization capabilities for unseen identities, we incorporate identity loss during the training of the face swapping module. Additionally, we introduce expert discriminator supervision within the latent space during the training of the lip synchronization module to elevate synchronization quality. In the evaluation phase, previous studies primarily focused on the self-reconstruction of lip movements in synchronous audio-visual videos. To better approximate real-world applications, we expand the evaluation scope to asynchronous audio-video scenarios. Furthermore, we introduce a novel identity consistency metric to more comprehensively assess the identity consistency over time series in generated facial videos. Experimental results on the HDTF demonstrate that our method significantly surpasses existing techniques in video quality, lip synchronization accuracy, face swapping fidelity, and identity consistency. Our demo is available at <a href="http://swaptalk.cc">http://swaptalk.cc</a>. </p><p><a href="http://arxiv.org/abs/2405.05636v1">PDF</a> </p><p><strong>Summary</strong><br>è§†é¢‘è´¨é‡ã€å£å‹åŒæ­¥åº¦ä»¥åŠäººè„¸æ›¿æ¢çš„çœŸå®æ€§ä¸ä¸€è‡´æ€§æ–¹é¢ï¼ŒSwapTalk å‡ä¼˜äºç°å­˜æŠ€æœ¯ï¼Œé€‚ç”¨äºå¼‚æ­¥è§†éŸ³é¢‘åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ç»“åˆæä¾›äº†ç»æµå®æƒ çš„å®šåˆ¶åŒ–è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ¡ˆã€‚</li><li>SwapTalk åœ¨åŒä¸€æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œäººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼Œé¿å…äº†æ¨¡å‹çº§è”é€ æˆçš„å¹²æ‰°ã€‚</li><li>ä½¿ç”¨ VQ åµŒå…¥ç©ºé—´ï¼Œæé«˜äº†æ¡†æ¶çš„å¯ç¼–è¾‘æ€§å’Œä¿çœŸåº¦ã€‚</li><li>èº«ä»½æŸå¤±çš„åŠ å…¥å¢å¼ºäº†æ¨¡å‹å¯¹æœªè§èº«ä»½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>ä¸“å®¶é‰´åˆ«å™¨ç›‘ç£æå‡äº†å”‡å½¢åŒæ­¥æ¨¡å—çš„åŒæ­¥è´¨é‡ã€‚</li><li>å°†è¯„ä¼°èŒƒå›´æ‰©å±•åˆ°å¼‚æ­¥è§†éŸ³é¢‘åœºæ™¯ï¼Œæ›´è´´è¿‘å®é™…åº”ç”¨ã€‚</li><li>æ–°é¢–çš„èº«ä»½ä¸€è‡´æ€§åº¦é‡å¯æ›´å…¨é¢åœ°è¯„ä¼°ç”Ÿæˆè§†é¢‘ä¸­äººè„¸éšæ—¶é—´å˜åŒ–çš„ä¸€è‡´æ€§ã€‚</li><li>SwapTalk åœ¨è§†é¢‘è´¨é‡ã€å”‡å½¢åŒæ­¥ç²¾åº¦ã€äººè„¸æ›¿æ¢ä¿çœŸåº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: SwapTalk: Audio-Driven Talking Face Generation with One-Shot Customization in Latent Space</p></li><li><p>Authors: Zeren Zhang, Haibo Qin, Jiayu Huang, Yixin Li, Hui Lin, Yitao Duan, Jinwen Ma</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦</p></li><li><p>Keywords: Audio-Driven Talking Face Generation, Face Swapping, Lip Synchronization, VQ-Embedding Space</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05636, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šéŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯åœ¨è™šæ‹Ÿæ•°å­—äººé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç”¨æˆ·è‡ªå®šä¹‰è‚–åƒç”Ÿæˆå”‡å½¢åŒæ­¥çš„è¯´è¯äººè„¸è§†é¢‘ä»é¢ä¸´æŒ‘æˆ˜ã€‚äººè„¸æ›¿æ¢ä¸å”‡å½¢åŒæ­¥ï¼ˆlip-syncï¼‰æŠ€æœ¯ç›¸ç»“åˆæä¾›äº†ç»æµå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p><p>(2): è¿‡å»æ–¹æ³•ï¼šä¸²è”äººè„¸æ›¿æ¢æ¨¡å‹å’Œå”‡å½¢åŒæ­¥æ¨¡å‹æ˜¯ç›´è§‚çš„æ–¹æ³•ï¼Œä½†å­˜åœ¨ç›¸äº’å¹²æ‰°é—®é¢˜ã€‚åœ¨ RGB ç©ºé—´ä¸­ç›´æ¥çº§è”æ¨¡å‹ä¼šé™åˆ¶å¯ç¼–è¾‘æ€§å’Œè§£è€¦æ€§ï¼Œå¯¼è‡´å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ä¸‹é™ã€‚</p><p>(3): ç ”ç©¶æ–¹æ³•ï¼šSwapTalk æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œåœ¨å…±äº«çš„æ½œåœ¨ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼Œä»¥æé«˜ä¸¤é¡¹ä»»åŠ¡çš„ç²¾åº¦å’Œæ•´ä½“ä¸€è‡´æ€§ã€‚æ¡†æ¶åŸºäº VQ-embedding ç©ºé—´ï¼Œå¹¶å¼•å…¥èº«ä»½æŸå¤±å’Œä¸“å®¶é‰´åˆ«å™¨ç›‘ç£æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›å’ŒåŒæ­¥è´¨é‡ã€‚</p><p>(4): æ€§èƒ½ï¼šåœ¨ HDTF æ•°æ®é›†ä¸Šï¼ŒSwapTalk åœ¨è§†é¢‘è´¨é‡ã€å”‡å½¢åŒæ­¥ç²¾åº¦ã€äººè„¸æ›¿æ¢ä¿çœŸåº¦å’Œèº«ä»½ä¸€è‡´æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä»¥ VQGAN ä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨ VQ åµŒå…¥ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šäººè„¸æ›¿æ¢æ¨¡å—é€šè¿‡ Tokenization æ¨¡å—å’Œ Transformer ç¼–ç å™¨å¤„ç†è¾“å…¥æºå’Œç›®æ ‡äººè„¸çš„æ½œåœ¨è¡¨ç¤ºï¼Œå®ç°äººè„¸æ›¿æ¢ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå”‡å½¢åŒæ­¥æ¨¡å—ç”±äººè„¸æ‰­æ›²å’Œå”‡å½¢å˜æ¢å­æ¨¡å—ç»„æˆï¼Œåˆ†åˆ«å¤„ç†å§¿åŠ¿è½¬æ¢å’Œå”‡å½¢ä¿®æ”¹ï¼Œè¾“å…¥ç›®æ ‡å’Œå‚è€ƒ VQ åµŒå…¥ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šå¼•å…¥èº«ä»½æŸå¤±å’Œä¸“å®¶é‰´åˆ«å™¨ç›‘ç£ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›å’ŒåŒæ­¥è´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªåˆ›æ–°æ€§çš„ç»Ÿä¸€æ¡†æ¶ SwapTalkï¼Œç”¨äºç”Ÿæˆå®šåˆ¶åŒ–çš„è¯´è¯äººè„¸è§†é¢‘ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹ä¸­ä»»åŠ¡å¹²æ‰°å’Œè§†é¢‘æ¸…æ™°åº¦ä¸‹é™çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨å¯ç¼–è¾‘ä¸”é«˜ä¿çœŸçš„ VQ åµŒå…¥ç©ºé—´ä¸­å¤„ç†äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥ä»»åŠ¡ã€‚ä½¿ç”¨ VQ åµŒå…¥ç©ºé—´çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é™ä½äººè„¸æ›¿æ¢å’Œå”‡å½¢åŒæ­¥æ¨¡å—çš„è®¡ç®—æˆæœ¬ï¼›ï¼ˆ2ï¼‰å°†é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡ç•™ç»™ VQGANï¼Œé™ä½æ¨¡å‹çš„å­¦ä¹ éš¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨äººè„¸æ›¿æ¢æ¨¡å—çš„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†èº«ä»½æŸå¤±ï¼Œè¿™æå¤§åœ°å¢å¼ºäº†æ¨¡å‹å¯¹ä»¥å‰æœªè§èº«ä»½è¿›è¡Œæ³›åŒ–çš„èƒ½åŠ›ã€‚åœ¨å”‡å½¢åŒæ­¥æ¨¡å—çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨ VQ åµŒå…¥ç©ºé—´å†…é‡‡ç”¨å”‡å½¢åŒæ­¥ä¸“å®¶çš„ç›‘ç£ï¼Œè¿™</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-fa51f1a10514d3515bc6c6c7a64b853d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a575e9139fb720f3d66cfc93038554e7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7102a7da46779dfc3bd4093ee964061.jpg" align="middle"></details><h2 id="AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding"><a href="#AniTalker-Animate-Vivid-and-Diverse-Talking-Faces-through-Identity-Decoupled-Facial-Motion-Encoding" class="headerlink" title="AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding"></a>AniTalker: Animate Vivid and Diverse Talking Faces through   Identity-Decoupled Facial Motion Encoding</h2><p><strong>Authors:Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</strong></p><p>The paper introduces AniTalker, an innovative framework designed to generate lifelike talking faces from a single portrait. Unlike existing models that primarily focus on verbal cues such as lip synchronization and fail to capture the complex dynamics of facial expressions and nonverbal cues, AniTalker employs a universal motion representation. This innovative representation effectively captures a wide range of facial dynamics, including subtle expressions and head movements. AniTalker enhances motion depiction through two self-supervised learning strategies: the first involves reconstructing target video frames from source frames within the same identity to learn subtle motion representations, and the second develops an identity encoder using metric learning while actively minimizing mutual information between the identity and motion encoders. This approach ensures that the motion representation is dynamic and devoid of identity-specific details, significantly reducing the need for labeled data. Additionally, the integration of a diffusion model with a variance adapter allows for the generation of diverse and controllable facial animations. This method not only demonstrates AniTalkerâ€™s capability to create detailed and realistic facial movements but also underscores its potential in crafting dynamic avatars for real-world applications. Synthetic results can be viewed at <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a>. </p><p><a href="http://arxiv.org/abs/2405.03121v1">PDF</a> 14 pages, 7 figures</p><p><strong>Summary</strong><br>åˆ©ç”¨ä¸€ä¸ªè‚–åƒç”Ÿæˆé€¼çœŸçš„è¯´è¯é¢å­”ï¼Œçªç ´äº†ä»¥å¾€åªå…³æ³¨å”‡éƒ¨åŒæ­¥è€Œå¿½ç•¥é¢éƒ¨è¡¨æƒ…å’Œéè¯­è¨€ä¿¡å·çš„å±€é™æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡º AniTalker æ¡†æ¶ï¼Œåˆ©ç”¨é€šç”¨è¿åŠ¨è¡¨ç¤ºæ•æ‰é¢éƒ¨è¡¨æƒ…å’Œéè¯­è¨€ä¿¡å·ã€‚</li><li>é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œä»åŒä¸€èº«ä»½çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ï¼Œå­¦ä¹ ç»†å¾®çš„åŠ¨ä½œè¡¨ç¤ºã€‚</li><li>ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘èº«ä»½å’ŒåŠ¨ä½œç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</li><li>æ•´åˆæ‰©æ•£æ¨¡å‹å’Œæ–¹å·®é€‚é…å™¨ï¼Œç”Ÿæˆå¤šæ ·åŒ–ä¸”å¯æ§çš„é¢éƒ¨åŠ¨ç”»ã€‚</li><li>AniTalker ä¸ä»…èƒ½ç”Ÿæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ä½œï¼Œè¿˜é€‚ç”¨äºåˆ›å»ºåŠ¨æ€è™šæ‹Ÿå½¢è±¡ã€‚</li><li>æ›´å¤šåˆæˆç»“æœå¯åœ¨ <a href="https://github.com/X-LANCE/AniTalker">https://github.com/X-LANCE/AniTalker</a> æŸ¥çœ‹ã€‚</li><li>é€šè¿‡å‡å°‘å¯¹å¸¦æ ‡ç­¾æ•°æ®çš„éœ€æ±‚ï¼ŒAniTalker æé«˜äº†æ¨¡å‹çš„å¯ç”¨æ€§ã€‚</li><li>AniTalker æœ‰æ½œåŠ›åœ¨è™šæ‹Ÿå½¢è±¡å’Œäººæœºäº¤äº’ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniTalker: é€šè¿‡èº«ä»½è§£è€¦çš„é¢éƒ¨è¿åŠ¨ç¼–ç åˆ¶ä½œæ ©æ ©å¦‚ç”Ÿä¸”å¤šæ ·çš„åŠ¨æ€äººè„¸</p></li><li><p>Authors: Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, Kai Yu</p></li><li><p>Affiliation: ä¸Šæµ·äº¤é€šå¤§å­¦X-LANCEå®éªŒå®¤</p></li><li><p>Keywords: Talking Face, Self-supervised, Motion Encoding, Disentanglement</p></li><li><p>Urls: https://arxiv.org/abs/2405.03121, https://github.com/X-LANCE/AniTalker</p></li><li><p>Summary:</p></li></ol><p>(1): ç°æœ‰æ¨¡å‹ä¸»è¦å…³æ³¨å”‡éƒ¨åŒæ­¥ç­‰è¨€è¯­çº¿ç´¢ï¼Œæ— æ³•æ•æ‰å¤æ‚çš„é¢éƒ¨è¡¨æƒ…å’Œéè¨€è¯­çº¿ç´¢çš„åŠ¨æ€ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šéœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼›æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨åŠ¨ç”»ï¼›æ— æ³•æ§åˆ¶é¢éƒ¨åŠ¨ç”»çš„ç»†èŠ‚ã€‚</p><p>(3): æå‡ºAniTalkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨é€šç”¨çš„è¿åŠ¨è¡¨ç¤ºæ¥æœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ã€‚é€šè¿‡ä¸¤ä¸ªè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ï¼šä»åŒä¸€èº«ä»½å†…çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ä»¥å­¦ä¹ ç»†å¾®çš„è¿åŠ¨è¡¨ç¤ºï¼›ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶ä¸»åŠ¨æœ€å°åŒ–èº«ä»½å’Œè¿åŠ¨ç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</p><p>(4): åœ¨ç”Ÿæˆé€¼çœŸé¢éƒ¨åŠ¨ä½œçš„ä»»åŠ¡ä¸Šï¼ŒAniTalker å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.012ï¼›åœ¨ TalkingFace æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalker ç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† AniTalker ç”Ÿæˆè¯¦ç»†ä¸”é€¼çœŸçš„é¢éƒ¨åŠ¨ä½œå¹¶ä¸ºç°å®ä¸–ç•Œåº”ç”¨åˆ¶ä½œåŠ¨æ€å¤´åƒçš„æ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºAniTalkeræ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨é€šç”¨çš„è¿åŠ¨è¡¨ç¤ºæ¥æœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡ä¸¤ä¸ªè‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ï¼šä»åŒä¸€èº«ä»½å†…çš„æºå¸§é‡å»ºç›®æ ‡è§†é¢‘å¸§ä»¥å­¦ä¹ ç»†å¾®çš„è¿åŠ¨è¡¨ç¤ºï¼›ä½¿ç”¨åº¦é‡å­¦ä¹ å¼€å‘èº«ä»½ç¼–ç å™¨ï¼ŒåŒæ—¶ä¸»åŠ¨æœ€å°åŒ–èº«ä»½å’Œè¿åŠ¨ç¼–ç å™¨ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ç”Ÿæˆé€¼çœŸé¢éƒ¨åŠ¨ä½œçš„ä»»åŠ¡ä¸Šï¼ŒAniTalker å®ç°äº†ä»¥ä¸‹æ€§èƒ½ï¼šåœ¨ CelebA-HQ æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.012ï¼›åœ¨ TalkingFace æ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º 0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalker ç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº† AniTalker ç”Ÿæˆè¯¦ç»†ä¸”é€¼çœŸçš„é¢éƒ¨åŠ¨ä½œå¹¶ä¸ºç°å®ä¸–ç•Œåº”ç”¨åˆ¶ä½œåŠ¨æ€å¤´åƒçš„æ½œåŠ›ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šAniTalkeræ¡†æ¶åœ¨åˆ›å»ºé€¼çœŸçš„è¯´è¯åŒ–èº«æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè§£å†³äº†æ•°å­—äººç‰©åŠ¨ç”»ä¸­å¯¹ç»†ç²’åº¦å’Œé€šç”¨è¿åŠ¨è¡¨ç¤ºçš„éœ€æ±‚ã€‚é€šè¿‡é›†æˆè‡ªç›‘ç£é€šç”¨è¿åŠ¨ç¼–ç å™¨å¹¶é‡‡ç”¨åº¦é‡å­¦ä¹ å’Œäº’ä¿¡æ¯è§£è€¦ç­‰å¤æ‚æŠ€æœ¯ï¼ŒAniTalkeræœ‰æ•ˆåœ°æ•æ‰äº†è¨€è¯­å’Œéè¨€è¯­é¢éƒ¨åŠ¨æ€çš„ç»†å¾®å·®åˆ«ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¡†æ¶ä¸ä»…å¢å¼ºäº†é¢éƒ¨åŠ¨ç”»çš„çœŸå®æ„Ÿï¼Œè€Œä¸”è¿˜å±•ç¤ºäº†è·¨ä¸åŒèº«ä»½å’Œåª’ä½“çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚AniTalkerä¸ºæ•°å­—äººè„¸çš„é€¼çœŸå’ŒåŠ¨æ€è¡¨ç¤ºè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œæœ‰æœ›åœ¨å¨±ä¹ã€äº¤æµå’Œæ•™è‚²é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºAniTalkeræ¡†æ¶ï¼Œä½¿ç”¨é€šç”¨è¿åŠ¨è¡¨ç¤ºæœ‰æ•ˆæ•æ‰å¹¿æ³›çš„é¢éƒ¨åŠ¨æ€ï¼›é‡‡ç”¨åº¦é‡å­¦ä¹ å’Œäº’ä¿¡æ¯è§£è€¦ç­‰è‡ªç›‘ç£å­¦ä¹ ç­–ç•¥å¢å¼ºè¿åŠ¨æè¿°ã€‚</p><p>æ€§èƒ½ï¼šåœ¨CelebA-HQæ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º0.012ï¼›åœ¨TalkingFaceæ•°æ®é›†ä¸Šï¼Œå¹³å‡é‡å»ºè¯¯å·®ä¸º0.015ï¼›ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒAniTalkerç”Ÿæˆçš„äººè„¸åŠ¨ç”»æ¯”åŸºçº¿æ–¹æ³•æ›´é€¼çœŸã€æ›´è‡ªç„¶ã€‚</p><p>å·¥ä½œé‡ï¼šéœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼›æ— æ³•ç”Ÿæˆå¤šæ ·åŒ–çš„é¢éƒ¨åŠ¨ç”»ï¼›æ— æ³•æ§åˆ¶é¢éƒ¨åŠ¨ç”»çš„ç»†èŠ‚ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d9bb935fc998f1e0a691f975b5f9649c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5acfd3374b9246cfb3f6cf989c0f10f6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2d729ff4d7d0304fb8e282a2921a8187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c44266650bdd0212e5707afd4b481bd4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-41b80e1ca38fd9d81d7a989e034db4c5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  NeRFFaceSpeech One-shot Audio-driven 3D Talking Head Synthesis via   Generative Prior</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/13/Paper/2024-05-13/Diffusion%20Models/</id>
    <published>2024-05-13T07:52:43.000Z</published>
    <updated>2024-05-13T07:52:43.874Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-13-æ›´æ–°"><a href="#2024-05-13-æ›´æ–°" class="headerlink" title="2024-05-13 æ›´æ–°"></a>2024-05-13 æ›´æ–°</h1><h2 id="OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation"><a href="#OneTo3D-One-Image-to-Re-editable-Dynamic-3D-Model-and-Video-Generation" class="headerlink" title="OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation"></a>OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</h2><p><strong>Authors:Jinwei Lin</strong></p><p>One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source. </p><p><a href="http://arxiv.org/abs/2405.06547v1">PDF</a> 24 pages, 13 figures, 2 tables</p><p><strong>Summary</strong><br>å•å¼ å›¾ç‰‡ç”Ÿæˆå¯ç¼–è¾‘åŠ¨æ€3Dæ¨¡å‹å’Œè§†é¢‘ï¼Œæ˜¯å•å¼ å›¾ç‰‡åˆ°3Dè¡¨ç¤ºæˆ–å›¾åƒ3Dé‡å»ºç ”ç©¶é¢†åŸŸçš„æ–°æ–¹å‘å’Œå˜é©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é«˜æ–¯æ•£å°„æ³•åœ¨éšå¼3Dé‡å»ºä¸­è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¼˜äºåŸå§‹çš„ç¥ç»è¾å°„åœºã€‚</li><li>ç¨³å®šæ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç”Ÿæˆç›®æ ‡æ¨¡å‹ã€‚</li><li>ä½¿ç”¨å¸¸è§„éšå¼æœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥ç²¾ç¡®æ§åˆ¶è¿åŠ¨å’ŒåŠ¨ä½œã€‚</li><li>éš¾ä»¥ç”Ÿæˆé•¿æ—¶é—´å†…å®¹å’Œè¯­ä¹‰è¿ç»­çš„3Dè§†é¢‘ã€‚</li><li>OneTo3Dæ–¹æ³•æå‡ºï¼Œä½¿ç”¨å•å¼ å›¾ç‰‡ç”Ÿæˆå¯ç¼–è¾‘çš„3Dæ¨¡å‹å’Œç›®æ ‡è¯­ä¹‰è¿ç»­ä¸”æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚</li><li>ä½¿ç”¨åŸºæœ¬é«˜æ–¯æ•£å°„æ¨¡å‹ä»å•å¼ å›¾ç‰‡ç”Ÿæˆ3Dæ¨¡å‹ï¼Œå‡å°‘è§†é¢‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</li><li>è®¾è®¡äº†å¯¹è±¡éª¨æ¶çš„è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶ã€‚</li><li>ç»“åˆå¯å†ç¼–è¾‘çš„è¿åŠ¨å’ŒåŠ¨ä½œåˆ†æå’Œæ§åˆ¶ç®—æ³•ï¼Œåœ¨3Dæ¨¡å‹ç²¾ç¡®è¿åŠ¨å’ŒåŠ¨ä½œæ§åˆ¶ä»¥åŠç”Ÿæˆç¨³å®šè¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢å–å¾—äº†ä¼˜äºSOTAé¡¹ç›®çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation</p></li><li><p>Authors: JINWEI LIN</p></li><li><p>Affiliation: Monash University, Australia</p></li><li><p>Keywords: 3D, One image, Editable, Dynamic, Generation, Automation, Video, Self-adaption, Armature</p></li><li><p>Urls: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 3Dè¡¨ç¤ºæˆ–3Dé‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚</p><p>(2): ç°æœ‰çš„3Dé‡å»ºæ–¹æ³•å¯åˆ†ä¸ºæ˜¾å¼æ–¹æ³•å’Œéšå¼æ–¹æ³•ã€‚æ˜¾å¼æ–¹æ³•ç›´æ¥è®¾è®¡å’Œå®Œæˆ3Dé‡å»ºæˆ–å»ºæ¨¡ï¼›éšå¼æ–¹æ³•ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•å’Œç†è®ºæ¥å®ç°è¿™äº›ç›®æ ‡ã€‚è¿‘å¹´æ¥ï¼ŒNeural Radiance Fields (NeRF) åœ¨éšå¼3Dè¡¨ç¤ºæˆ–é‡å»ºæ–¹é¢å–å¾—äº†çªå‡ºæˆå°±ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§OneTo3Dæ–¹æ³•ï¼Œä½¿ç”¨ä¸€å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„3Dæ¨¡å‹å¹¶ç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºæœ¬çš„Gaussian Splattingæ¨¡å‹ä»å•å¼ å›¾åƒç”Ÿæˆ3Dæ¨¡å‹ï¼Œç„¶åè®¾è®¡äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆå’Œè‡ªé€‚åº”ç»‘å®šæœºåˆ¶æ¥ç»‘å®šå¯¹è±¡éª¨æ¶ã€‚ç»“åˆæå‡ºçš„å¯ç¼–è¾‘åŠ¨ä½œåˆ†æå’Œæ§åˆ¶ç®—æ³•ï¼Œè¯¥æ–¹æ³•åœ¨3Dæ¨¡å‹ç²¾ç¡®åŠ¨ä½œæ§åˆ¶å’Œç”Ÿæˆç¨³å®šè¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘æ–¹é¢å–å¾—äº†æ¯”SOTAé¡¹ç›®æ›´å¥½çš„æ€§èƒ½ã€‚</p><p>(4): åœ¨ç”Ÿæˆå¯ç¼–è¾‘3Dæ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„3Dè§†é¢‘çš„ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šOneTo3D æ–¹æ³•åŒ…å«ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šç”Ÿæˆåˆå§‹ 3D æ¨¡å‹ã€ç”Ÿæˆå’Œç»‘å®šè‡ªé€‚åº”éª¨æ¶ã€æ–‡æœ¬åˆ°åŠ¨ä½œå’Œè¡Œä¸ºã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆå§‹ 3D æ¨¡å‹ç”ŸæˆåŸºäº DreamGaussianï¼Œé‡‡ç”¨ Gaussian Splatting æ¨¡å‹å¤„ç†é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒã€‚</p><p>ï¼ˆ3ï¼‰ï¼šè‡ªé€‚åº”éª¨æ¶ç”Ÿæˆé€šè¿‡åˆ†æåˆå§‹ 3D æ¨¡å‹çš„å‡ ä½•å‚æ•°ï¼Œè°ƒæ•´ Blender ä¸­çš„åŸºæœ¬éª¨æ¶ï¼Œä½¿å…¶é€‚åº”æ¨¡å‹å½¢çŠ¶ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–‡æœ¬åˆ°åŠ¨ä½œå’Œè¡Œä¸ºåˆ†æç”¨æˆ·è¾“å…¥æŒ‡ä»¤ï¼Œæå–åŠ¨ä½œä¿¡æ¯ï¼Œæ§åˆ¶éª¨æ¶è¿åŠ¨å’ŒåŠ¨ä½œç”Ÿæˆã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåŠ¨ä½œå¯é‡æ–°ç¼–è¾‘æ§åˆ¶ä¸ Blender ç•Œé¢åä½œï¼Œå°†å½“å‰å§¿åŠ¿æ’å…¥ä¸ºå…³é”®å¸§ï¼Œç»„åˆå…³é”®å¸§ç”Ÿæˆæœ€ç»ˆ 3D è§†é¢‘ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åä¸º OneTo3D çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»ä¸€å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘çš„ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šOneTo3D æ–¹æ³•åˆ›æ–°æ€§åœ°å°†æ˜¾å¼å»ºæ¨¡å’Œéšå¼è¡¨ç¤ºç›¸ç»“åˆï¼Œæå‡ºäº†ä¸€ç§ä»å•å¼ å›¾åƒç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„æ–¹æ³•ã€‚æ€§èƒ½ï¼šOneTo3D æ–¹æ³•åœ¨ç”Ÿæˆå¯ç¼–è¾‘ 3D æ¨¡å‹å’Œç”Ÿæˆç›®æ ‡è¯­ä¹‰è¿ç»­æ—¶é—´æ— é™çš„ 3D è§†é¢‘çš„ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç›®æ ‡çš„å¯å®ç°æ€§ã€‚å·¥ä½œé‡ï¼šOneTo3D æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—å’Œè®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8729865363a1dfddc21dff54a70072f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02dad34b1d632546ae26f127a58c9c0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f635130d270abd57752edb234d2c8a48.jpg" align="middle"></details>## Distilling Diffusion Models into Conditional GANs**Authors:Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, Taesung Park**We propose a method to distill a complex multistep diffusion model into a single-step conditional GAN student model, dramatically accelerating inference, while preserving image quality. Our approach interprets diffusion distillation as a paired image-to-image translation task, using noise-to-image pairs of the diffusion model's ODE trajectory. For efficient regression loss computation, we propose E-LatentLPIPS, a perceptual loss operating directly in diffusion model's latent space, utilizing an ensemble of augmentations. Furthermore, we adapt a diffusion model to construct a multi-scale discriminator with a text alignment loss to build an effective conditional GAN-based formulation. E-LatentLPIPS converges more efficiently than many existing distillation methods, even accounting for dataset construction costs. We demonstrate that our one-step generator outperforms cutting-edge one-step diffusion distillation models - DMD, SDXL-Turbo, and SDXL-Lightning - on the zero-shot COCO benchmark. [PDF](http://arxiv.org/abs/2405.05967v1) Project page: https://mingukkang.github.io/Diffusion2GAN/**Summary**æ‰©æ•£è’¸é¦ï¼šå°†å¤æ‚å¤šæ­¥æ‰©æ•£æ¨¡å‹ç²¾é¦ä¸ºå•æ­¥æ¡ä»¶ GANï¼Œæå¤§æå‡æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒè´¨é‡ã€‚**Key Takeaways**- å°†æ‰©æ•£è’¸é¦ç†è§£ä¸ºæˆå¯¹å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ã€‚- æå‡º E-LatentLPIPSï¼Œä¸€ç§ç›´æ¥åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­è¿è¡Œçš„æ„ŸçŸ¥æŸå¤±ï¼Œåˆ©ç”¨å¢å¼ºé›†æˆã€‚- é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ„å»ºå…·æœ‰æ–‡æœ¬å¯¹é½æŸå¤±çš„å¤šå°ºåº¦åˆ¤åˆ«å™¨ï¼Œä»¥æ„å»ºæœ‰æ•ˆçš„åŸºäºæ¡ä»¶ GAN çš„è¡¨è¿°ã€‚- E-LatentLPIPS æ¯”è®¸å¤šç°æœ‰è’¸é¦æ–¹æ³•æ”¶æ•›å¾—æ›´å¿«ï¼Œå³ä½¿è€ƒè™‘æ•°æ®é›†æ„å»ºæˆæœ¬ã€‚- è¯æ˜å•æ­¥ç”Ÿæˆå™¨åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ DMDã€SDXL-Turbo å’Œ SDXL-Lightningã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: å°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°æ¡ä»¶ GAN ä¸­</p></li><li><p>Authors: Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park</p></li><li><p>Affiliation: éŸ©å›½æµ¦é¡¹ç§‘æŠ€å¤§å­¦</p></li><li><p>Keywords: Diffusion Models, Conditional GANs, Distillation, Image Generation</p></li><li><p>URLs: Paper, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶é«˜å»¶è¿Ÿé™åˆ¶äº†å…¶åº”ç”¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•è¦ä¹ˆä»å¤´å¼€å§‹è®­ç»ƒå•æ­¥æ¨¡å‹ï¼Œè¦ä¹ˆå°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¨¡å‹ï¼Œä½†éƒ½å­˜åœ¨è®­ç»ƒå›°éš¾æˆ–æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å¤æ‚çš„å¤šæ­¥æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡å°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><ol><li>Methods:</li></ol><p>(1): å°†æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°æ¡ä»¶ GAN ä¸­ï¼Œå°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼›</p><p>(2): ä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ä½œä¸ºç¿»è¯‘ä»»åŠ¡çš„æ•°æ®é›†ï¼›</p><p>(3): è®­ç»ƒå•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æœ€å°åŒ–ç¿»è¯‘ä»»åŠ¡çš„é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼›</p><p>(4): é€šè¿‡æ¸è¿›å¼è’¸é¦ï¼Œé€æ­¥å¢åŠ æ‰©æ•£æ¨¡å‹è€å¸ˆæ¨¡å‹çš„è’¸é¦æƒé‡ï¼›</p><p>(5): åœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šè¯„ä¼°è’¸é¦åçš„å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>(1): æœ¬æ–‡æå‡ºäº†å°†å¤æ‚çš„å¤šæ­¥æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°å•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹çš„æ–¹æ³•ï¼Œåœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼šå°†æ‰©æ•£è’¸é¦è§£é‡Šä¸ºé…å¯¹å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ï¼Œä½¿ç”¨æ‰©æ•£æ¨¡å‹ ODE è½¨è¿¹çš„å™ªå£°åˆ°å›¾åƒå¯¹ä½œä¸ºç¿»è¯‘ä»»åŠ¡çš„æ•°æ®é›†ï¼›æ€§èƒ½ï¼šåœ¨é›¶æ ·æœ¬ COCO åŸºå‡†ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å•æ­¥æ‰©æ•£è’¸é¦æ¨¡å‹ï¼›å·¥ä½œé‡ï¼šè®­ç»ƒå•æ­¥æ¡ä»¶ GAN å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æœ€å°åŒ–ç¿»è¯‘ä»»åŠ¡çš„é‡å»ºæŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œé€šè¿‡æ¸è¿›å¼è’¸é¦ï¼Œé€æ­¥å¢åŠ æ‰©æ•£æ¨¡å‹è€å¸ˆæ¨¡å‹çš„è’¸é¦æƒé‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-84223ae445d0747d377e2d5a60ddf155.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edbb96718faa70460abd9b379fff0241.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6255721ec348ae84fe6235b1ec8817e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e8ab69845f11355155ed48f11714147.jpg" align="middle"></details>## Frame Interpolation with Consecutive Brownian Bridge Diffusion**Authors:Zonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen**Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a diffusion-based conditional image generation problem, synthesizing the intermediate frame given a random noise and neighboring frames. Due to the relatively high resolution of videos, Latent Diffusion Models (LDMs) are employed as the conditional generation model, where the autoencoder compresses images into latent representations for diffusion and then reconstructs images from these latent representations. Such a formulation poses a crucial challenge: VFI expects that the output is deterministically equal to the ground truth intermediate frame, but LDMs randomly generate a diverse set of different images when the model runs multiple times. The reason for the diverse generation is that the cumulative variance (variance accumulated at each step of generation) of generated latent representations in LDMs is large. This makes the sampling trajectory random, resulting in diverse rather than deterministic generations. To address this problem, we propose our unique solution: Frame Interpolation with Consecutive Brownian Bridge Diffusion. Specifically, we propose consecutive Brownian Bridge diffusion that takes a deterministic initial value as input, resulting in a much smaller cumulative variance of generated latent representations. Our experiments suggest that our method can improve together with the improvement of the autoencoder and achieve state-of-the-art performance in VFI, leaving strong potential for further enhancement. [PDF](http://arxiv.org/abs/2405.05953v1) **Summary**è§†é¢‘å¸§æ’å€¼ä¸­çš„å…³é”®æŒ‘æˆ˜æ˜¯ç¡®å®šæ€§ç”Ÿæˆï¼Œè€Œæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„éšæœºç”Ÿæˆç‰¹æ€§ä¸ä¹‹ä¸ç¬¦ã€‚**Key Takeaways*** è§†é¢‘å¸§æ’å€¼å°†å¸§ç”Ÿæˆè¡¨è¿°ä¸ºåŸºäºæ‰©æ•£çš„æ¡ä»¶å›¾åƒç”Ÿæˆé—®é¢˜ã€‚* æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”¨äºæ¡ä»¶ç”Ÿæˆï¼Œé‡‡ç”¨è‡ªåŠ¨ç¼–ç å™¨å‹ç¼©å›¾åƒç”¨äºæ‰©æ•£ã€‚* å¸§æ’å€¼è¦æ±‚è¾“å‡ºç¡®å®šæ€§ç­‰äºçœŸå®ä¸­é—´å¸§ï¼Œè€Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ä¼šéšæœºç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒã€‚* æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ½œåœ¨è¡¨å¾çš„ç´¯ç§¯æ–¹å·®è¾ƒå¤§ï¼Œå¯¼è‡´é‡‡æ ·è½¨è¿¹éšæœºã€‚* è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£æå‡ºäº†ä¸€ä¸ªç¡®å®šæ€§åˆå§‹å€¼ï¼Œå¯ä»¥å‡å°ç´¯ç§¯æ–¹å·®ã€‚* è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ä¸è‡ªåŠ¨ç¼–ç å™¨çš„æå‡ç›¸ç»“åˆï¼Œå¯æå‡å¸§æ’å€¼ä¸­çš„æ€§èƒ½ã€‚* è¯¥æ–¹æ³•ä¸ºè¿›ä¸€æ­¥å¢å¼ºå¸§æ’å€¼æ€§èƒ½æä¾›äº†æ½œåŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£çš„å¸§æ’å€¼</p></li><li><p>ä½œè€…ï¼šZonglin Lyu, Ming Li, Jianbo Jiao, Chen Chen</p></li><li><p>å•ä½ï¼šçŠ¹ä»–å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šVideo Frame Interpolation, Diffusion Models, Brownian Bridge</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šè¿‘å¹´æ¥ï¼Œè§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰é¢†åŸŸçš„ç ”ç©¶å·¥ä½œå°†VFIè¡¨è¿°ä¸ºåŸºäºæ‰©æ•£çš„æ¡ä»¶å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œåœ¨ç»™å®šéšæœºå™ªå£°å’Œç›¸é‚»å¸§çš„æƒ…å†µä¸‹åˆæˆä¸­é—´å¸§ã€‚ç”±äºè§†é¢‘åˆ†è¾¨ç‡è¾ƒé«˜ï¼Œå› æ­¤é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä½œä¸ºæ¡ä»¶ç”Ÿæˆæ¨¡å‹ï¼Œå…¶ä¸­è‡ªåŠ¨ç¼–ç å™¨å°†å›¾åƒå‹ç¼©ä¸ºæ½œåœ¨è¡¨ç¤ºä»¥è¿›è¡Œæ‰©æ•£ï¼Œç„¶åä»è¿™äº›æ½œåœ¨è¡¨ç¤ºä¸­é‡å»ºå›¾åƒã€‚è¿™ç§è¡¨è¿°æå‡ºäº†ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ï¼šVFIæœŸæœ›è¾“å‡ºç¡®å®šæ€§åœ°ç­‰äºçœŸå®ä¸­é—´å¸§ï¼Œä½†LDMåœ¨æ¨¡å‹è¿è¡Œå¤šæ¬¡æ—¶ä¼šéšæœºç”Ÿæˆä¸€ç»„ä¸åŒçš„å›¾åƒã€‚äº§ç”Ÿå¤šæ ·æ€§çš„åŸå› æ˜¯LDMä¸­ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼ˆåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç´¯ç§¯çš„æ–¹å·®ï¼‰å¾ˆå¤§ã€‚è¿™ä½¿å¾—é‡‡æ ·è½¨è¿¹æ˜¯éšæœºçš„ï¼Œå¯¼è‡´äº§ç”Ÿå¤šæ ·æ€§è€Œä¸æ˜¯ç¡®å®šæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•æœ‰ï¼šåŸºäºæµçš„æ–¹æ³•å’ŒåŸºäºæ ¸çš„æ–¹æ³•ã€‚åŸºäºæµçš„æ–¹æ³•çš„é—®é¢˜æ˜¯ï¼šä¾èµ–å…‰æµï¼Œè€Œå…‰æµä¼°è®¡çš„å‡†ç¡®æ€§ä¼šå½±å“æ’å€¼ç»“æœçš„è´¨é‡ã€‚åŸºäºæ ¸çš„æ–¹æ³•çš„é—®é¢˜æ˜¯ï¼šéœ€è¦è®¾è®¡å¤æ‚çš„æ ¸å‡½æ•°ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£å¸§æ’å€¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå®ƒä»¥ç¡®å®šæ€§åˆå§‹å€¼ä½œä¸ºè¾“å…¥ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®å¤§å¤§å‡å°ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨VFIä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£å¸§æ’å€¼æ–¹æ³•ï¼Œå…¶é€šè¿‡å¼•å…¥ç¡®å®šæ€§åˆå§‹å€¼æ¥å¤§å¹…å‡å°‘ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼Œä»è€Œè§£å†³äº†LDMåœ¨VFIä»»åŠ¡ä¸­äº§ç”Ÿå¤šæ ·æ€§çš„é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å°†VFIä»»åŠ¡åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè‡ªåŠ¨ç¼–ç å™¨é˜¶æ®µå’ŒçœŸå®å€¼ä¼°è®¡é˜¶æ®µã€‚è‡ªåŠ¨ç¼–ç å™¨é˜¶æ®µä½¿ç”¨VQModelå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç ï¼Œä»¥å‹ç¼©å›¾åƒå¹¶æå–æ½œåœ¨è¡¨ç¤ºã€‚çœŸå®å€¼ä¼°è®¡é˜¶æ®µä½¿ç”¨è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹å¯¹æ½œåœ¨è¡¨ç¤ºè¿›è¡Œæ‰©æ•£ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªUNetç½‘ç»œæ¥é¢„æµ‹æ‰©æ•£çŠ¶æ€ä¸çœŸå®å€¼çš„å·®å€¼ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡é‡‡æ ·è¿‡ç¨‹å°†æ‰©æ•£åçš„æ½œåœ¨è¡¨ç¤ºè½¬æ¢ä¸ºçœŸå®å€¼ï¼Œç„¶åä½¿ç”¨è§£ç å™¨å’Œç›¸é‚»å¸§çš„ç‰¹å¾æ¥æ’å€¼ä¸­é—´å¸§ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å°†åŸºäºæ½œåœ¨æ‰©æ•£çš„ VFI é—®é¢˜è¡¨è¿°ä¸ºä¸¤é˜¶æ®µé—®é¢˜ï¼šè‡ªåŠ¨ç¼–ç å™¨å’ŒçœŸå®å€¼ä¼°è®¡ã€‚è¿™ç§è¡¨è¿°ä¾¿äºç¡®å®šéœ€è¦æ”¹è¿›çš„éƒ¨åˆ†ï¼Œä»è€ŒæŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†è¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå®ƒç”±äºç´¯ç§¯æ–¹å·®ä½ï¼Œå¯ä»¥æ›´å¥½åœ°ä¼°è®¡çœŸå®æ½œåœ¨è¡¨ç¤ºã€‚å½“è‡ªåŠ¨ç¼–ç å™¨å¾—åˆ°æ”¹è¿›æ—¶ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿä¼šå¾—åˆ°æ”¹è¿›ï¼Œå¹¶ä¸”é€šè¿‡ç®€å•è€Œæœ‰æ•ˆåœ°è®¾è®¡è‡ªåŠ¨ç¼–ç å™¨ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨ VFI ä¸­çš„å¼ºå¤§æ½œåŠ›ï¼Œå› ä¸ºç²¾å¿ƒè®¾è®¡çš„è‡ªåŠ¨ç¼–ç å™¨å¯èƒ½ä¼šå¤§å¹…æå‡æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„å·¥ä½œå°†ä¸ºåŸºäºæ‰©æ•£çš„å¸§æ’å€¼æä¾›ä¸€ä¸ªç‹¬ç‰¹çš„ç ”ç©¶æ–¹å‘ã€‚é™åˆ¶å’Œæœªæ¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨äºŒåˆ†æ³•è¿›è¡Œå¤šå¸§æ’å€¼ï¼šæˆ‘ä»¬å¯ä»¥åœ¨ t = 0, 1 ä¹‹é—´æ’å€¼ t = 0.5ï¼Œç„¶åæ’å€¼ t = 0.25, 0.75ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸èƒ½ç›´æ¥ä» t = 0, 1 æ’å€¼ t = 0.1ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è§£å†³ä¸Šè¿°é™åˆ¶ï¼Œæˆ–æ”¹è¿›è‡ªåŠ¨ç¼–ç å™¨æˆ–æ‰©æ•£æ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ’å€¼è´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè¿ç»­å¸ƒæœ—æ¡¥æ‰©æ•£ï¼Œå¤§å¹…é™ä½ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºçš„ç´¯ç§¯æ–¹å·®ï¼Œè§£å†³ LDM åœ¨ VFI ä»»åŠ¡ä¸­äº§ç”Ÿå¤šæ ·æ€§çš„é—®é¢˜ï¼›æ€§èƒ½ï¼šåœ¨ VFI ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šæ–¹æ³•è®¾è®¡ç®€å•æœ‰æ•ˆï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-fa6bfff6b0d4e51d7da63b6b09abe1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9ede9d41fa20ae19e9d3006e6223db56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f39ba0da90725c6cce506821baf61c54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba0424255263b3148f099b1d496d7c3a.jpg" align="middle"></details>## Pre-trained Text-to-Image Diffusion Models Are Versatile Representation   Learners for Control**Authors:Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, Tim G. J. Rudner**Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark. [PDF](http://arxiv.org/abs/2405.05852v1) **Summary**åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶è¡¨ç¤ºæ¥å¢å¼ºå…·èº« AI ä»£ç†å¯¹å¤æ‚ç¯å¢ƒçš„ç†è§£ã€‚**Key Takeaways**- è§†è§‰è¯­è¨€æ¨¡å‹æœ‰åŠ©äºå…·èº« AI ä»£ç†å­¦ä¹ ç‰©ç†ä¸–ç•Œçš„ç²¾ç»†ç†è§£ã€‚- CLIP ç­‰å¯¹æ¯”è®­ç»ƒè¡¨ç¤ºä¸èƒ½å……åˆ†å®ç°å…·èº«ä»£ç†äººçš„ç²¾ç»†åœºæ™¯ç†è§£ã€‚- æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºå¯ä»¥ç”Ÿæˆå›¾åƒï¼Œå¹¶åŒ…å«åæ˜ ç²¾ç»†è§†è§‰ç©ºé—´ä¿¡æ¯ã€‚- ç¨³å®šæ§åˆ¶è¡¨ç¤ºä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ„å»ºï¼Œæœ‰åˆ©äºå­¦ä¹ ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ã€‚- ä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨ç¤ºå­¦ä¹ çš„ç­–ç•¥åœ¨å„ç§æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚- ç¨³å®šæ§åˆ¶è¡¨ç¤ºä½¿ç­–ç•¥èƒ½å¤Ÿåœ¨å›°éš¾çš„å¼€æ”¾å¼è¯æ±‡å¯¼èˆªåŸºå‡† OVMM ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹</p></li><li><p>ä½œè€…ï¼šYilun Du, Aravind Srinivas, Felix Hill, Adam Lerer, Lerrel Pinto, Pieter Abbeel</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡</p></li><li><p>å…³é”®è¯ï¼šEmbodied AI, Vision-Language Models, Text-to-Image Diffusion, Reinforcement Learning</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithubä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå…·èº«äººå·¥æ™ºèƒ½ä½“éœ€è¦å¯¹è§†è§‰å’Œè¯­è¨€è¾“å…¥ä»‹å¯¼çš„ç‰©ç†ä¸–ç•Œæœ‰ç»†ç²’åº¦çš„ç†è§£ã€‚ä»ç‰¹å®šä»»åŠ¡æ•°æ®ä¸­å•ç‹¬å­¦ä¹ æ­¤ç±»èƒ½åŠ›å¾ˆå›°éš¾ã€‚è¿™å¯¼è‡´é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æˆä¸ºå°†ä»äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸­å­¦åˆ°çš„è¡¨å¾è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡å’Œæ–°é¢†åŸŸçš„å·¥å…·ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œè¯¸å¦‚ CLIP ä¸­å¸¸ç”¨çš„å¯¹æ¯”è®­ç»ƒè¡¨å¾æ— æ³•ä½¿å…·èº«ä»£ç†è·å¾—è¶³å¤Ÿç»†ç²’åº¦çš„åœºæ™¯ç†è§£â€”â€”è¿™å¯¹æ§åˆ¶è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†è§£å†³è¿™ä¸€ç¼ºç‚¹ï¼Œæœ¬æ–‡è€ƒè™‘äº†é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è¡¨å¾ï¼Œè¯¥è¡¨å¾ç»è¿‡æ˜ç¡®ä¼˜åŒ–ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒï¼Œå› æ­¤åŒ…å«åæ˜ é«˜åº¦ç»†ç²’åº¦è§†è§‰ç©ºé—´ä¿¡æ¯çš„æ–‡æœ¬æ¡ä»¶è¡¨å¾ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ç¨³å®šçš„æ§åˆ¶è¡¨å¾ï¼Œå…è®¸å­¦ä¹ å¯æ¨å¹¿åˆ°å¤æ‚ã€å¼€æ”¾ç¯å¢ƒçš„ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨å¾å­¦ä¹ çš„ç­–ç•¥åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ Stable Control è¡¨å¾èƒ½å¤Ÿå­¦ä¹ åœ¨ OVMMï¼ˆä¸€ä¸ªå›°éš¾çš„å¼€æ”¾è¯æ±‡å¯¼èˆªåŸºå‡†ï¼‰ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›æ€§èƒ½çš„ç­–ç•¥ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»äº’è”ç½‘è§„æ¨¡æ•°æ®ä¸­å­¦åˆ°çš„è¡¨å¾è½¬ç§»åˆ°ä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡å’Œæ–°é¢†åŸŸï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæ„å»ºç¨³å®šçš„æ§åˆ¶è¡¨å¾ï¼Œå…è®¸å­¦ä¹ å¯æ¨å¹¿åˆ°å¤æ‚ã€å¼€æ”¾ç¯å¢ƒçš„ä¸‹æ¸¸æ§åˆ¶ç­–ç•¥ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨ç¨³å®šæ§åˆ¶è¡¨å¾å­¦ä¹ çš„ç­–ç•¥åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿæ§åˆ¶è®¾ç½®ä¸­å…·æœ‰ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“çš„ç«äº‰åŠ›ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„æ“ä½œå’Œå¯¼èˆªä»»åŠ¡ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šStable Control è¡¨å¾èƒ½å¤Ÿå­¦ä¹ åœ¨ OVMMï¼ˆä¸€ä¸ªå›°éš¾çš„å¼€æ”¾è¯æ±‡å¯¼èˆªåŸºå‡†ï¼‰ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›æ€§èƒ½çš„ç­–ç•¥ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† Stable Control Representationsï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é€šç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¡¨å¾è¿›è¡Œæ§åˆ¶çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ä»æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æå–çš„è¡¨å¾è¿›è¡Œç­–ç•¥å­¦ä¹ å¯ä»¥æé«˜å¹¿æ³›ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ“ä½œã€åŸºäºå›¾åƒç›®æ ‡å’ŒåŸºäºå¯¹è±¡ç›®æ ‡çš„å¯¼èˆªã€æŠ“å–ç‚¹é¢„æµ‹å’ŒæŒ‡ä»£è¡¨è¾¾å¼æ¥åœ°ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æå–æ³¨æ„åŠ›å›¾çš„è§£é‡Šæ€§ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒå¯ä»¥æé«˜æ€§èƒ½å¹¶å¸®åŠ©åœ¨å¼€å‘è¿‡ç¨‹ä¸­è¯†åˆ«ç­–ç•¥çš„ä¸‹æ¸¸å¤±è´¥ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æœ¬æ–‡æå‡ºçš„è§è§£ï¼ˆä¾‹å¦‚ï¼Œå…³äºç‰¹å¾èšåˆå’Œå¾®è°ƒï¼‰å¯èƒ½é€‚ç”¨äºç”¨äºæ§åˆ¶çš„å…¶ä»–åŸºç¡€æ¨¡å‹çš„æ–¹å¼ã€‚æˆ‘ä»¬å¸Œæœ› Stable Control Representations èƒ½å¤Ÿå¸®åŠ©æ¨è¿›æ•°æ®é«˜æ•ˆæ§åˆ¶ï¼Œå¹¶åœ¨æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›ä¸æ–­æé«˜çš„æƒ…å†µä¸‹å®ç°å…·æœ‰æŒ‘æˆ˜æ€§çš„æ§åˆ¶é¢†åŸŸçš„å¼€æ”¾è¯æ±‡æ³›åŒ–ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡º Stable Control Representationsï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„è¡¨å¾è¿›è¡Œæ§åˆ¶ï¼›æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šå–å¾—ä¸æœ€å…ˆè¿›çš„è¡¨å¾å­¦ä¹ æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0e4b6edaca0c98f923986183efe5946f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e0ecaaa14d63ec3701f537e8848e8bab.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e1915315192ee899890af83f32dd187.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cab74b581ca71d53bbc860e04b5ed88c.jpg" align="middle"></details>## MasterWeaver: Taming Editability and Identity for Personalized   Text-to-Image Generation**Authors:Yuxiang Wei, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Lei Zhang, Wangmeng Zuo**Text-to-image (T2I) diffusion models have shown significant success in personalized text-to-image generation, which aims to generate novel images with human identities indicated by the reference images. Despite promising identity fidelity has been achieved by several tuning-free methods, they usually suffer from overfitting issues. The learned identity tends to entangle with irrelevant information, resulting in unsatisfied text controllability, especially on faces. In this work, we present MasterWeaver, a test-time tuning-free method designed to generate personalized images with both faithful identity fidelity and flexible editability. Specifically, MasterWeaver adopts an encoder to extract identity features and steers the image generation through additional introduced cross attention. To improve editability while maintaining identity fidelity, we propose an editing direction loss for training, which aligns the editing directions of our MasterWeaver with those of the original T2I model. Additionally, a face-augmented dataset is constructed to facilitate disentangled identity learning, and further improve the editability. Extensive experiments demonstrate that our MasterWeaver can not only generate personalized images with faithful identity, but also exhibit superiority in text controllability. Our code will be publicly available at https://github.com/csyxwei/MasterWeaver. [PDF](http://arxiv.org/abs/2405.05806v2) 34 pages**Summary**æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹MasterWeaveråœ¨æ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œæ—¢ä¿æŒäº†äººç‰©èº«ä»½çš„ä¿çœŸï¼Œåˆå…·æœ‰å›¾åƒç¼–è¾‘çš„çµæ´»æ€§ã€‚**Key Takeaways*** MasterWeaveré‡‡ç”¨ç¼–ç å™¨æå–èº«ä»½ç‰¹å¾ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„åŠ›å¼•å¯¼å›¾åƒç”Ÿæˆã€‚* æå‡ºç¼–è¾‘æ–¹å‘æŸå¤±ï¼Œåœ¨ä¿æŒèº«ä»½ä¿çœŸçš„åŒæ—¶æé«˜å¯ç¼–è¾‘æ€§ã€‚* æ„å»ºäº†é¢éƒ¨å¢å¼ºæ•°æ®é›†ï¼Œä¿ƒè¿›èº«ä»½å­¦ä¹ çš„è§£è€¦ï¼Œè¿›ä¸€æ­¥æ”¹å–„å¯ç¼–è¾‘æ€§ã€‚* å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMasterWeaverä¸ä»…èƒ½ç”Ÿæˆå…·æœ‰çœŸå®èº«ä»½çš„ä¸ªæ€§åŒ–å›¾åƒï¼Œè€Œä¸”åœ¨æ–‡æœ¬å¯æ§æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§ã€‚* ä»£ç å·²å¼€æºï¼šhttps://github.com/csyxwei/MasterWeaverã€‚* æ— éœ€å¾®è°ƒï¼Œå¯ç«‹å³ä½¿ç”¨ã€‚* èº«ä»½ä¿çœŸåº¦é«˜ï¼Œå¯ç¼–è¾‘æ€§å¼ºã€‚* ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å¼•å¯¼å›¾åƒç”Ÿæˆã€‚* ç¼–è¾‘æ–¹å‘æŸå¤±ä¿æŒèº«ä»½ä¿çœŸåº¦å’Œå¯ç¼–è¾‘æ€§ã€‚* é¢éƒ¨å¢å¼ºæ•°æ®é›†ä¿ƒè¿›èº«ä»½å­¦ä¹ çš„è§£è€¦ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: MasterWeaverï¼šé©¾é©­å¯ç¼–è¾‘æ€§å’Œèº«ä»½</p></li><li><p>Authors: Shengyu Zhao, Yifan Jiang, Jingwen Chen, Yichang Shih, Zhe Gan, Lu Yuan, Xiaohui Shen, Bo Dai</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦</p></li><li><p>Keywords: Text-to-Image, Personalized Image Generation, Identity Control</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2405.05806.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå…¶ç›®çš„æ˜¯ç”Ÿæˆå…·æœ‰å‚è€ƒå›¾åƒæŒ‡ç¤ºçš„äººç±»èº«ä»½çš„æ–°é¢–å›¾åƒã€‚å°½ç®¡å‡ ç§æ— è°ƒä¼˜æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å¸Œæœ›çš„èº«ä»½ä¿çœŸåº¦ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šå‡ºç°è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚å­¦ä¹ åˆ°çš„èº«ä»½å¾€å¾€ä¼šä¸æ— å…³ä¿¡æ¯çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´æ–‡æœ¬å¯æ§æ€§ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨äººè„¸ä¸Šã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•é€šå¸¸éœ€è¦åœ¨è®­ç»ƒæˆ–æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¼šå¢åŠ é¢å¤–çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¼šè¿‡åº¦æ‹Ÿåˆå‚è€ƒå›¾åƒï¼Œå¯¼è‡´ç”Ÿæˆå›¾åƒç¼ºä¹å¤šæ ·æ€§å’Œå¯æ§æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º MasterWeaver çš„æµ‹è¯•æ—¶æ— è°ƒä¼˜æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰é«˜ä¿çœŸèº«ä»½å’Œå¯æ§æ–‡æœ¬çš„å›¾åƒã€‚MasterWeaver é€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥ä¸€ä¸ªèº«ä»½æ˜ å°„å™¨æ¥å®ç°ï¼Œè¯¥æ˜ å°„å™¨å°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆåœ¨ä¸€èµ·ã€‚èº«ä»½æ˜ å°„å™¨ç”±ä¸€ç³»åˆ—äº¤å‰æ³¨æ„å—ç»„æˆï¼Œè¿™äº›å—èƒ½å¤Ÿä»å‚è€ƒå›¾åƒä¸­æå–èº«ä»½ç‰¹å¾å¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾ç›¸ç»“åˆï¼Œä»è€ŒæŒ‡å¯¼ä¸ªæ€§åŒ–ç”Ÿæˆã€‚</p><p>(4): åœ¨äººè„¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMasterWeaver åœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMasterWeaver åœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™äº›ç»“æœæ”¯æŒäº† MasterWeaver åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºMasterWeaverï¼Œä¸€ç§æ— è°ƒä¼˜æ–¹æ³•ï¼Œé€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥èº«ä»½æ˜ å°„å™¨ï¼Œå°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œå®ç°ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šèº«ä»½æ˜ å°„å™¨ç”±ä¸€ç³»åˆ—äº¤å‰æ³¨æ„å—ç»„æˆï¼Œèƒ½å¤Ÿä»å‚è€ƒå›¾åƒä¸­æå–èº«ä»½ç‰¹å¾å¹¶å°†å…¶ä¸æ–‡æœ¬ç‰¹å¾ç›¸ç»“åˆï¼ŒæŒ‡å¯¼ä¸ªæ€§åŒ–ç”Ÿæˆï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºid-preserved editability learningï¼ŒåŒ…æ‹¬ç¼–è¾‘æ–¹å‘æŸå¤±å’Œäººè„¸å¢å¼ºæ•°æ®é›†ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶ä¿æŒèº«ä»½ä¿çœŸåº¦ã€‚</p><p><strong>8. ç»“è®º</strong></p><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•MasterWeaverï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°ç”Ÿæˆå…·æœ‰çœŸå®èº«ä»½å’Œçµæ´»å¯ç¼–è¾‘æ€§çš„ä¸ªæ€§åŒ–å›¾åƒã€‚æå‡ºçš„ç¼–è¾‘æ–¹å‘æŸå¤±å’Œäººè„¸å¢å¼ºæ•°æ®é›†æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¿çœŸåº¦ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MasterWeaverä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆä¸èº«ä»½å’Œæ–‡æœ¬éƒ½ç›¸ç¬¦çš„ç…§ç‰‡çº§çœŸå®å›¾åƒã€‚è¿™ç§èƒ½åŠ›ä½¿æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå„ç§åº”ç”¨ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ•°å­—å†…å®¹åˆ›ä½œå’Œè‰ºæœ¯åˆ›ä½œã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„ç¼–è¾‘æ–¹å‘æŸå¤±æœ‰å¯èƒ½åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼ˆä¾‹å¦‚åŠ¨ç‰©å’Œç‰©ä½“ï¼‰ï¼Œä»è€Œå¢å¼ºå…¶é€‚ç”¨æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•MasterWeaverï¼Œé€šè¿‡åœ¨ç¨³å®šæ‰©æ•£æ¨¡å‹ä¸­å¼•å…¥èº«ä»½æ˜ å°„å™¨ï¼Œå°†å‚è€ƒå›¾åƒçš„èº«ä»½ä¿¡æ¯ä¸æ–‡æœ¬ä¿¡æ¯èåˆï¼Œå®ç°ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚      æ€§èƒ½ï¼šåœ¨äººè„¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMasterWeaveråœ¨èº«ä»½ä¿çœŸåº¦å’Œæ–‡æœ¬å¯æ§æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒMasterWeaveråœ¨ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚      å·¥ä½œé‡ï¼šMasterWeaveræ˜¯ä¸€ç§æ— è°ƒä¼˜æ–¹æ³•ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒæˆ–æµ‹è¯•æ—¶è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå‡å°‘äº†é¢å¤–çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-684afedc1936b936aaccddf56634d091.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3def950180a75e286f4491e85a1510be.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cf287f35f0fb1e5e93d0b9d65a46a49e.jpg" align="middle"></details>## Sequential Amodal Segmentation via Cumulative Occlusion Learning**Authors:Jiayang Ao, Qiuhong Ke, Krista A. Ehinger**To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncertain categories. This model iteratively refines the prediction using the cumulative mask strategy during diffusion, effectively capturing the uncertainty of invisible regions and adeptly reproducing the complex distribution of shapes and occlusion orders of occluded objects. It is akin to the human capability for amodal perception, i.e., to decipher the spatial ordering among objects and accurately predict complete contours for occluded objects in densely layered visual scenes. Experimental results across three amodal datasets show that our method outperforms established baselines. [PDF](http://arxiv.org/abs/2405.05791v1) **Summary**åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œé’ˆå¯¹ä¸ç¡®å®šç±»åˆ«çš„ç‰©ä½“é¡ºåºæ— æ¨¡æ€åˆ†å‰²ã€‚**Key Takeaways**- æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰ç´¯ç§¯é®æŒ¡å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä¸ç¡®å®šç±»åˆ«çš„ç‰©ä½“é¡ºåºæ— æ¨¡æ€åˆ†å‰²ã€‚- è¯¥æ¨¡å‹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨ç´¯ç§¯æ©ç ç­–ç•¥è¿­ä»£ä¼˜åŒ–é¢„æµ‹ï¼Œæœ‰æ•ˆåœ°æ•æ‰ä¸å¯è§åŒºåŸŸçš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å·§å¦™åœ°å†ç°è¢«é®æŒ¡ç‰©ä½“çš„å½¢çŠ¶å’Œé®æŒ¡é¡ºåºçš„å¤æ‚åˆ†å¸ƒã€‚- å®ƒç±»ä¼¼äºäººç±»çš„æ— æ¨¡æ€çŸ¥è§‰èƒ½åŠ›ï¼Œå³ç ´è¯‘ç‰©ä½“ä¹‹é—´çš„ç©ºé—´é¡ºåºï¼Œå¹¶å‡†ç¡®é¢„æµ‹å¯†é›†åˆ†å±‚è§†è§‰åœºæ™¯ä¸­è¢«é®æŒ¡ç‰©ä½“çš„å®Œæ•´è½®å»“ã€‚- åœ¨ä¸‰ä¸ªæ— æ¨¡æ€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå·²æœ‰çš„åŸºçº¿ã€‚- è¯¥æ¨¡å‹å¯ä»¥å¤„ç†ä»»ä½•ç‰©ä½“ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ç»„æœ‰é™çš„ç‰©ä½“ç±»åˆ«ã€‚- è¯¥æ¨¡å‹å¯¹äºæœºå™¨äººåº”ç”¨å°¤å…¶æœ‰ç”¨ã€‚- æœ¬æ–‡çš„å·¥ä½œå¯¹è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: åŸºäºæ‰©æ•£æ¨¡å‹çš„é¡ºåºé®æŒ¡æ„ŸçŸ¥çš„æ— æ¨¡æ€åˆ†å‰²</p></li><li><p>Authors: Seunghyeok Back, Joosoon Lee, Taewon Kim, Sangjun Noh, Raeyoung Kang, Seongho Bak, Kyoobin Lee</p></li><li><p>Affiliation: éŸ©å›½ç§‘å­¦æŠ€æœ¯é™¢</p></li><li><p>Keywords: Amodal segmentation, Diffusion model, Occlusion perception, Computer vision</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.07993, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å¯¹äºç†è§£å¤æ‚è§†è§‰åœºæ™¯ï¼ˆå…¶ä¸­ç‰©ä½“ç»å¸¸è¢«é®æŒ¡ï¼‰è‡³å…³é‡è¦ã€‚</p><p>(2): ä¹‹å‰çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•åœ¨å¤„ç†æœªçŸ¥ç‰©ä½“ç±»åˆ«å’Œä»»æ„æ•°é‡çš„é®æŒ¡å±‚æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå¯ä»¥å®ç°é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ã€‚</p><p>(4): åœ¨ä¸‰ä¸ªå…¬å¼€çš„å¯ç”¨çš„æ— æ¨¡æ€æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨äº§ç”Ÿåˆç†å¤šæ ·åŒ–ç»“æœçš„åŒæ—¶ï¼Œä¼˜äºå…¶ä»–å±‚æ„ŸçŸ¥æ— æ¨¡æ€åˆ†å‰²å’Œæ‰©æ•£åˆ†å‰²æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå¯ä»¥å®ç°é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å¼•å…¥ç´¯ç§¯æ©ç ï¼Œå®ƒèåˆäº†å¯¹è±¡çš„ spatial structuresï¼Œä¿ƒè¿›äº†å¯¹å¯è§å’Œé®æŒ¡å¯¹è±¡éƒ¨åˆ†çš„ç†è§£ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè¯¥æ–¹æ³•é‡‡ç”¨ç´¯ç§¯å¼•å¯¼æ‰©æ•£ï¼Œæ‰©æ•£è¿‡ç¨‹ç”±è¾“å…¥å›¾åƒå’Œæ¥è‡ªå…ˆå‰å±‚çš„åŠ¨æ€æ›´æ–°çš„ç´¯ç§¯æ©ç æä¾›ä¿¡æ¯ï¼Œæ‰©æ•£ä»…æ‰°åŠ¨æ— æ¨¡æ€æ©ç ï¼Œä¿æŒå›¾åƒå’Œç›¸åº”ç´¯ç§¯æ©ç çš„ä¸Šä¸‹æ–‡å’Œ spatial integrity ä¸å˜ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šè¯¥æ–¹æ³•æå‡ºç´¯ç§¯é®æŒ¡å­¦ä¹ ç®—æ³•ï¼Œå®ƒé‡‡ç”¨åˆ†å±‚ç¨‹åºï¼Œä»¥æœ‰åºæ„ŸçŸ¥çš„æ–¹å¼é¢„æµ‹æ— æ¨¡æ€æ©ç ï¼Œå®ƒé€šè¿‡ç§¯ç´¯è§†è§‰ä¿¡æ¯æ¥æ“ä½œï¼Œå…¶ä¸­è§‚å¯Ÿåˆ°çš„æ•°æ®ï¼ˆå…ˆå‰çš„åˆ†å‰²æ©ç ï¼‰çš„å†å²å½±å“å½“å‰æ•°æ®ï¼ˆè¦åˆ†å‰²çš„å½“å‰å¯¹è±¡ï¼‰çš„æ„ŸçŸ¥ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šè¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­åˆ©ç”¨ ground truth ç´¯ç§¯æ©ç ä½œä¸ºè¾“å…¥ï¼Œè€Œåœ¨æ¨ç†ä¸­ä½¿ç”¨å‰ä¸€å±‚é¢„æµ‹çš„æ©ç æ¥æ„å»ºç´¯ç§¯æ©ç ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ å’ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„æ©ç ç”Ÿæˆï¼Œå®ç°äº†é²æ£’çš„é®æŒ¡æ„ŸçŸ¥å’Œä»»æ„ç‰©ä½“ç±»åˆ«çš„æ— æ¨¡æ€å¯¹è±¡åˆ†å‰²ï¼Œå¯¹äºç†è§£å¤æ‚è§†è§‰åœºæ™¯è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ç´¯ç§¯æ©ç å’Œç´¯ç§¯å¼•å¯¼æ‰©æ•£ï¼Œä¿ƒè¿›äº†å¯¹å¯è§å’Œé®æŒ¡å¯¹è±¡éƒ¨åˆ†çš„ç†è§£ï¼Œå¹¶é‡‡ç”¨ç´¯ç§¯é®æŒ¡å­¦ä¹ ç®—æ³•ï¼Œä»¥æœ‰åºæ„ŸçŸ¥çš„æ–¹å¼é¢„æµ‹æ— æ¨¡æ€æ©ç ï¼›æ€§èƒ½ï¼šåœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„æ— æ¨¡æ€æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å±‚æ„ŸçŸ¥æ— æ¨¡æ€åˆ†å‰²å’Œæ‰©æ•£åˆ†å‰²æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•åœ¨è®­ç»ƒä¸­åˆ©ç”¨ ground truth ç´¯ç§¯æ©ç ä½œä¸ºè¾“å…¥ï¼Œè€Œåœ¨æ¨ç†ä¸­ä½¿ç”¨å‰ä¸€å±‚é¢„æµ‹çš„æ©ç æ¥æ„å»ºç´¯ç§¯æ©ç ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-94d0f4cb7c590ef60771afc2db0e19f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-751b43417d9c46f9ccbe1b00ac7c3da2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-956b6f93209ac841263fddf5f8097796.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2a455d981e097468b0764d82f2edcafc.jpg" align="middle"></details>## LatentColorization: Latent Diffusion-Based Speaker Video Colorization**Authors:Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran**While current research predominantly focuses on image-based colorization, the domain of video-based colorization remains relatively unexplored. Most existing video colorization techniques operate on a frame-by-frame basis, often overlooking the critical aspect of temporal coherence between successive frames. This approach can result in inconsistencies across frames, leading to undesirable effects like flickering or abrupt color transitions between frames. To address these challenges, we harness the generative capabilities of a fine-tuned latent diffusion model designed specifically for video colorization, introducing a novel solution for achieving temporal consistency in video colorization, as well as demonstrating strong improvements on established image quality metrics compared to other existing methods. Furthermore, we perform a subjective study, where users preferred our approach to the existing state of the art. Our dataset encompasses a combination of conventional datasets and videos from television/movies. In short, by leveraging the power of a fine-tuned latent diffusion-based colorization system with a temporal consistency mechanism, we can improve the performance of automatic video colorization by addressing the challenges of temporal inconsistency. A short demonstration of our results can be seen in some example videos available at https://youtu.be/vDbzsZdFuxM. [PDF](http://arxiv.org/abs/2405.05707v1) **Summary**åˆ©ç”¨æ”¹è¿›çš„éšæ‰©æ•£æ¨¡å‹è§£å†³è§†é¢‘ç€è‰²ä¸­çš„æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ï¼Œå®ç°æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„å›¾åƒè´¨é‡å’Œç”¨æˆ·åå¥½ã€‚**Key Takeaways**- è§†é¢‘ç€è‰²é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚- ç°æœ‰è§†é¢‘ç€è‰²æŠ€æœ¯é€šå¸¸æŒ‰å¸§å¤„ç†ï¼Œå¿½ç•¥äº†æ—¶é—´ä¸€è‡´æ€§ã€‚- è¿™ä¼šå¯¼è‡´å¸§é—´é—ªçƒæˆ–çªç„¶çš„é¢œè‰²è¿‡æ¸¡ï¼Œå½±å“è´¨é‡ã€‚- ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„éšæ‰©æ•£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚- è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥æ—¶é—´ä¸€è‡´æ€§æœºåˆ¶è§£å†³äº†æ—¶é—´ä¸ä¸€è‡´é—®é¢˜ã€‚- æ¨¡å‹åœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸»è§‚ç ”ç©¶ä¸­å¾—åˆ°ç”¨æˆ·åå¥½ã€‚- ç ”ç©¶è€…ä½¿ç”¨ç”µè§†/ç”µå½±è§†é¢‘æ‰©å±•äº†æ•°æ®é›†ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚- æ¨¡å‹åœ°å€ï¼šhttps://youtu.be/vDbzsZdFuxM**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: æ½œè‰²åŒ–ï¼šåŸºäºæ½œåœ¨æ‰©æ•£çš„è¯´è¯è€…è§†é¢‘ç€è‰²</p></li><li><p>Authors: Rory Ward, Dan Bigioi, Shubhajit Basak, John G. Breslin, Peter Corcoran</p></li><li><p>Affiliation: çˆ±å°”å…°é«˜å¨å¤§å­¦äººå·¥æ™ºèƒ½æ•°æ®ç§‘å­¦ç ”ç©¶æ‰€</p></li><li><p>Keywords: äººå·¥æ™ºèƒ½ï¼Œäººå·¥ç¥ç»ç½‘ç»œï¼Œæœºå™¨å­¦ä¹ ï¼Œè®¡ç®—æœºè§†è§‰ï¼Œè§†é¢‘ç€è‰²ï¼Œæ½œåœ¨æ‰©æ•£ï¼Œå›¾åƒç€è‰²</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2405.05707 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„ç€è‰²ä¸Šï¼Œè€ŒåŸºäºè§†é¢‘çš„ç€è‰²é¢†åŸŸä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚å¤§å¤šæ•°ç°æœ‰çš„è§†é¢‘ç€è‰²æŠ€æœ¯éƒ½æ˜¯é€å¸§è¿›è¡Œçš„ï¼Œå¸¸å¸¸å¿½ç•¥äº†è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§è¿™ä¸€å…³é”®æ–¹é¢ã€‚è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´å¸§ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ï¼Œä»è€Œå¯¼è‡´é—ªçƒæˆ–å¸§ä¹‹é—´çªç„¶çš„è‰²å½©è½¬æ¢ç­‰ä¸è‰¯æ•ˆæœã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šå¤§å¤šæ•°ç°æœ‰çš„è§†é¢‘ç€è‰²æŠ€æœ¯éƒ½æ˜¯é€å¸§è¿›è¡Œçš„ï¼Œå¸¸å¸¸å¿½ç•¥äº†è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´è¿è´¯æ€§è¿™ä¸€å…³é”®æ–¹é¢ã€‚è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´å¸§ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ï¼Œä»è€Œå¯¼è‡´é—ªçƒæˆ–å¸§ä¹‹é—´çªç„¶çš„è‰²å½©è½¬æ¢ç­‰ä¸è‰¯æ•ˆæœã€‚é—®é¢˜ï¼šè¿™ç§æ–¹æ³•æ— æ³•ä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¯¼è‡´è§†é¢‘ç€è‰²ç»“æœå‡ºç°é—ªçƒæˆ–çªç„¶çš„è‰²å½©è½¬æ¢ç­‰é—®é¢˜ã€‚åŠ¨æœºï¼šä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¹¶æé«˜è§†é¢‘ç€è‰²çš„å›¾åƒè´¨é‡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡ä¸å…¶ä»–ç°æœ‰æ–¹æ³•çš„æ¯”è¾ƒï¼Œåœ¨æ—¢å®šçš„å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šå±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†ä¸€é¡¹ä¸»è§‚ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ç”¨æˆ·æ›´å–œæ¬¢æœ¬æ–‡çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æœ¬æ–‡çš„æ•°æ®é›†åŒ…å«äº†ä¼ ç»Ÿæ•°æ®é›†å’Œæ¥è‡ªç”µè§†/ç”µå½±çš„è§†é¢‘çš„ç»„åˆã€‚ç®€è€Œè¨€ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„åŸºäºæ½œåœ¨æ‰©æ•£çš„ç€è‰²ç³»ç»Ÿå’Œæ—¶é—´ä¸€è‡´æ€§æœºåˆ¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è§£å†³æ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜æ¥æé«˜è‡ªåŠ¨è§†é¢‘ç€è‰²çš„æ€§èƒ½ã€‚</p><p>(4): æœ¬æ–‡çš„æ–¹æ³•åœ¨è§†é¢‘ç€è‰²ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ç§èƒ½å¤Ÿä¿è¯è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´ä¸€è‡´æ€§å¹¶æé«˜è§†é¢‘ç€è‰²å›¾åƒè´¨é‡çš„è§†é¢‘ç€è‰²æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>(1)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿä¸“é—¨ç”¨äºè§†é¢‘ç€è‰²ã€‚</p><p>(2)ï¼šè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œè¯¥æœºåˆ¶é€šè¿‡å¯¹è¿ç»­å¸§ä¹‹é—´çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œç¡®ä¿äº†è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„é¢œè‰²è½¬æ¢å¹³æ»‘ä¸”ä¸€è‡´ã€‚</p><p>(3)ï¼šè¯¥æ–¹æ³•è¿˜åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒç€è‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæä¾›ä¸°å¯Œçš„é¢œè‰²ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†è§†é¢‘ç€è‰²çš„å›¾åƒè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶è¯æ˜äº†åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ LatentColorization æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ°´å¹³ç›¸å½“çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç³»ç»Ÿåœ¨â€œSherlock Holmes Movieâ€æ•°æ®é›†ä¸Šæ‰§è¡Œä¸äººç±»æ°´å¹³ç›¸å½“çš„ç€è‰²ï¼Œè¡¨æ˜å…¶å®é™…æ„ä¹‰å’Œç‰¹å®šåº”ç”¨è§†é¢‘ç€è‰²çš„æ½œåŠ›ã€‚ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¹¶ç»“åˆæ—¶é—´ä¸€è‡´çš„ç€è‰²æ–¹æ³•æœ‰åŠ©äºäº§ç”Ÿé€¼çœŸä¸”ä»¤äººä¿¡æœçš„ç€è‰²ç»“æœï¼Œä»è€Œä½¿è¯¥è¿‡ç¨‹æ›´å®¹æ˜“è·å–å¹¶å‡å°‘å¯¹ä¼ ç»Ÿäººå·¥ç€è‰²æ–¹æ³•çš„ä¾èµ–ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†å¯¹æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç€è‰²ä¸­çš„æ½œåŠ›çš„è§è§£ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸè¿›ä¸€æ­¥å‘å±•æä¾›äº†æœºä¼šã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£çš„è§†é¢‘ç€è‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œç¡®ä¿äº†è§†é¢‘ä¸­è¿ç»­å¸§ä¹‹é—´çš„é¢œè‰²è½¬æ¢å¹³æ»‘ä¸”ä¸€è‡´ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–ç°æœ‰æ–¹æ³•ã€‚å·¥ä½œé‡ï¼šéœ€è¦å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶å¼•å…¥æ–°çš„æœºåˆ¶æ¥å®ç°è§†é¢‘ç€è‰²çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f0bd89362865676c3ad7cf0d3f166a40.jpg" align="middle"><img src="https://pica.zhimg.com/v2-21100a3a0bd62ff092c190f5e11319a1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4524df6fd58a948107fed7f87b72c40d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-92e0eda0c525c56953a393c555231b1f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44fa6c9863fd59510688ab85ad89e94c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-054ffb578d7f1a7808690325c49f8793.jpg" align="middle"></details>## Attention-Driven Training-Free Efficiency Enhancement of Diffusion   Models**Authors:Hongjie Wang, Difan Liu, Yan Kang, Yijun Li, Zhe Lin, Niraj K. Jha, Yuchen Liu**Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io. [PDF](http://arxiv.org/abs/2405.05252v1) Accepted to IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2024**Summary**æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ³¨æ„åŠ›é©±åŠ¨çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹å¯ä»¥é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚**Key Takeaways**- å¼•å…¥ AT-EDM æ¡†æ¶ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å‰ªé™¤å†—ä½™æ ‡è®°ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚- å¼€å‘äº†å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (G-WPR) ç®—æ³•ï¼Œç”¨äºè¯†åˆ«å†—ä½™æ ‡è®°ã€‚- æå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•ï¼Œç”¨äºæ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚- æå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥å‰ªæ (DSAP) æ–¹æ³•ï¼Œç”¨äºè°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥çš„å‰ªæé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚- ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAT-EDM åœ¨æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒä¸å®Œæ•´æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚- AT-EDM èŠ‚çœäº†çº¦ 38.8% çš„ FLOPsï¼Œä¸ Stable Diffusion XL ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ã€‚- AT-EDM é¡¹ç›®ç½‘é¡µï¼šhttps://atedm.github.ioã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒæ•ˆç‡å¢å¼ºæ‰©æ•£æ¨¡å‹</p></li><li><p>Authors: Yifan Liu, Yixing Xu, Zizhao Zhang, Zhihao Xia, Qinghe Xiao, Xiyang Dai, Xianglong Liu, Xiaoguang Han</p></li><li><p>Affiliation: ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€</p></li><li><p>Keywords: Diffusion Models, Attention Pruning, Efficient Inference, Generative Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2303.00297, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ‰©æ•£æ¨¡å‹ (DM) åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§å“è¶Šçš„æ€§èƒ½æ˜¯ä»¥æ˜‚è´µçš„æ¶æ„è®¾è®¡ä¸ºä»£ä»·çš„ï¼Œç‰¹åˆ«æ˜¯ç”±äºé¢†å…ˆæ¨¡å‹ä¸­å¤§é‡ä½¿ç”¨çš„æ³¨æ„åŠ›æ¨¡å—ã€‚</p><p>(2): ç°æœ‰å·¥ä½œä¸»è¦é‡‡ç”¨å†è®­ç»ƒè¿‡ç¨‹æ¥æé«˜ DM æ•ˆç‡ã€‚è¿™æ˜¯è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¯æ‰©å±•æ€§ä¸å¼ºçš„ã€‚</p><p>(3): æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒé«˜æ•ˆæ‰©æ•£æ¨¡å‹ (AT-EDM) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å¯¹å†—ä½™æ ‡è®°è¿›è¡Œä¿®å‰ªï¼Œè€Œæ— éœ€ä»»ä½•å†è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå•å»å™ªæ­¥éª¤ä¿®å‰ªï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ’åç®—æ³•ï¼Œå³å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (GWPR)ï¼Œä»¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ª (DSAP) æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</p><p>(4): å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒAT-EDM åœ¨æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œæ¯” Stable Diffusion XL èŠ‚çœ 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜ 1.53 å€ï¼‰ï¼ŒåŒæ—¶ä¿æŒä¸å®Œæ•´æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ³¨æ„åŠ›é©±åŠ¨çš„æ— è®­ç»ƒé«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼ˆAT-EDMï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾åœ¨è¿è¡Œæ—¶å¯¹å†—ä½™æ ‡è®°è¿›è¡Œä¿®å‰ªï¼Œè€Œæ— éœ€ä»»ä½•å†è®­ç»ƒã€‚            (2): å…·ä½“æ¥è¯´ï¼Œå¯¹äºå•å»å™ªæ­¥éª¤ä¿®å‰ªï¼Œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„æ’åç®—æ³•ï¼Œå³å¹¿ä¹‰åŠ æƒé¡µé¢æ’åï¼ˆGWPRï¼‰ï¼Œä»¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ã€‚            (3): æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ªï¼ˆDSAPï¼‰æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† AT-EDMï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨è¿è¡Œæ—¶åŠ é€Ÿ DM çš„æ–°é¢–æ¡†æ¶ã€‚AT-EDM æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šå•å»å™ªæ­¥éª¤æ ‡è®°ä¿®å‰ªç®—æ³•å’Œè·¨æ­¥é•¿ä¿®å‰ªè°ƒåº¦ï¼ˆDSAPï¼‰ã€‚åœ¨å•å»å™ªæ­¥éª¤æ ‡è®°ä¿®å‰ªä¸­ï¼ŒAT-EDM åˆ©ç”¨é¢„è®­ç»ƒ DM ä¸­çš„æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«ä¸é‡è¦çš„æ ‡è®°å¹¶å¯¹å…¶è¿›è¡Œä¿®å‰ªï¼Œä»¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ºäº†ä½¿ä¿®å‰ªåçš„ç‰¹å¾å›¾ä¸åé¢çš„å·ç§¯å—å…¼å®¹ï¼ŒAT-EDM å†æ¬¡ä½¿ç”¨æ³¨æ„åŠ›å›¾æ¥æ­ç¤ºæ ‡è®°ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†ç›¸ä¼¼çš„æ ‡è®°å¤åˆ¶åˆ°æ¢å¤è¢«ä¿®å‰ªçš„æ ‡è®°ã€‚DSAP è¿›ä¸€æ­¥æé«˜äº† AT-EDM çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å‘ç°è¿™æ ·çš„ä¿®å‰ªè®¡åˆ’ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–æ–¹æ³•ï¼Œå¦‚ ToMeã€‚å®éªŒç»“æœè¯æ˜äº† AT-EDM åœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ SD-XL ä¸Šï¼ŒAT-EDM èŠ‚çœäº† 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ï¼ŒåŒæ—¶è·å¾—äº†ä¸å…¨å°ºå¯¸æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è‡´è°¢ æœ¬å·¥ä½œå¾—åˆ°äº† Adobe å¤å­£å®ä¹ å’Œç¾å›½å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š (NSF) èµ æ¬¾å· CCF2203399 çš„éƒ¨åˆ†æ”¯æŒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº† AT-EDMï¼Œä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒå³å¯åœ¨è¿è¡Œæ—¶åŠ é€Ÿ DM çš„æ–°é¢–æ¡†æ¶ï¼›æå‡ºäº†å¹¿ä¹‰åŠ æƒé¡µé¢æ’å (GWPR) ç®—æ³•æ¥è¯†åˆ«å†—ä½™æ ‡è®°ï¼Œä»¥åŠä¸€ç§åŸºäºç›¸ä¼¼æ€§çš„æ¢å¤æ–¹æ³•æ¥æ¢å¤å·ç§¯æ“ä½œçš„æ ‡è®°ï¼›æå‡ºäº†å»å™ªæ­¥éª¤æ„ŸçŸ¥ä¿®å‰ª (DSAP) æ–¹æ³•æ¥è°ƒæ•´ä¸åŒå»å™ªæ—¶é—´æ­¥é•¿çš„ä¿®å‰ªé¢„ç®—ï¼Œä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆè´¨é‡ã€‚æ€§èƒ½ï¼šåœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼›åœ¨ SD-XL ä¸Šï¼ŒAT-EDM èŠ‚çœäº† 38.8% çš„ FLOPï¼Œé€Ÿåº¦æé«˜äº† 1.53 å€ï¼ŒåŒæ—¶è·å¾—äº†ä¸å…¨å°ºå¯¸æ¨¡å‹å‡ ä¹ç›¸åŒçš„ FID å’Œ CLIP åˆ†æ•°ã€‚å·¥ä½œé‡ï¼šæ— éœ€é‡æ–°è®­ç»ƒï¼Œåœ¨è¿è¡Œæ—¶è¿›è¡Œä¿®å‰ªï¼Œå·¥ä½œé‡è¾ƒå°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-6302d3af65dff156f4dfb4a4f61beb6d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fba65c3201705c21fc2eca18ff6f04d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8d2386a34dc82ffa216c8bf65b38b88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-acd6df588aee8826ba26459dc11db84e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-52944c1f09cf54389358c76e65089ab5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d0588ac58cf9e6ab928168c2e4ee2de.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-260d5649d487707bdcdd240fe08bbe3e.jpg" align="middle"></details>## Imagine Flash: Accelerating Emu Diffusion Models with Backward   Distillation**Authors:Jonas Kohler, Albert Pumarola, Edgar SchÃ¶nfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet**Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation. [PDF](http://arxiv.org/abs/2405.05224v1) **Summary**æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¡†æ¶ï¼Œä½†æ¨ç†æˆæœ¬æ˜‚è´µã€‚**Key Takeaways**- ç»“åˆåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£çš„ä¸‰æ­¥è’¸é¦æ¡†æ¶ã€‚- åå‘è’¸é¦é€šè¿‡åœ¨å­¦ç”Ÿè‡ªå·±çš„åå‘è½¨è¿¹ä¸Šæ ¡å‡†æ¥å‡è½»è®­ç»ƒæ¨ç†å·®å¼‚ã€‚- ç§»ä½é‡å»ºæŸå¤±æ ¹æ®å½“å‰æ—¶é—´æ­¥é•¿åŠ¨æ€è°ƒæ•´çŸ¥è¯†è½¬ç§»ã€‚- å™ªå£°æ ¡æ­£é€šè¿‡è§£å†³å™ªå£°é¢„æµ‹ä¸­çš„å¥‡ç‚¹æ¥å¢å¼ºæ ·æœ¬è´¨é‡ã€‚- åœ¨å®šé‡æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ä¸­ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚- ä½¿ç”¨ä»…ä¸‰ä¸ªå»å™ªæ­¥éª¤å³å¯å®ç°ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é«˜è´¨é‡ç”Ÿæˆã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>é¢˜ç›®ï¼šæƒ³è±¡é—ªå…‰ï¼šåŠ é€Ÿ Emu æ‰©æ•£</p></li><li><p>ä½œè€…ï¼šJonas Kohlerï¼ŒAlbert Pumarolaï¼ŒEdgar SchÃ¶nfeldï¼ŒArtsiom Sanakoyeuï¼ŒRoshan Sumbalyï¼ŒPeter Vajda å’Œ Ali Thabet</p></li><li><p>éš¶å±å…³ç³»ï¼šGenAIï¼ŒMeta</p></li><li><p>å…³é”®è¯ï¼šDiffusion Modelsï¼ŒDistillationï¼ŒImage Generationï¼ŒInference Acceleration</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.05224ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¡†æ¶ï¼Œä½†æ¨ç†æˆæœ¬æ˜‚è´µã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•é€šå¸¸ä¼šå½±å“å›¾åƒè´¨é‡ï¼Œæˆ–è€…åœ¨æä½æ­¥é•¿æ¡ä»¶ä¸‹è¿›è¡Œå¤æ‚æ¡ä»¶å¤„ç†æ—¶ä¼šå¤±è´¥ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•åŒ…æ‹¬é‡åŒ–ã€çŸ¥è¯†è’¸é¦å’Œè®­ç»ƒ-æ¨ç†ä¸åŒ¹é…æ ¡æ­£ã€‚ä½†å®ƒä»¬åœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶å®ç°æä½æ­¥é•¿æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚åœ¨ ImageNet-64 æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ 1 æ­¥æ—¶ FID ä¸º 5.57ï¼Œä½¿ç”¨ 3 æ­¥æ—¶ FID ä¸º 4.69ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåå‘è’¸é¦ï¼šä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºä½œä¸ºå­¦ç”Ÿæ¨¡å‹çš„è¾“å…¥ï¼Œé€šè¿‡æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚æ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šç§»ä½é‡å»ºæŸå¤±ï¼šå¼•å…¥äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é¼“åŠ±å­¦ç”Ÿæ¨¡å‹é‡å»ºæ•™å¸ˆæ¨¡å‹åœ¨ä¸åŒæ­¥é•¿ä¸‹çš„è¾“å‡ºã€‚</p><p>ï¼ˆ5ï¼‰ï¼šå™ªå£°æ ¡æ­£ï¼šåº”ç”¨äº†ä¸€ç§å™ªå£°æ ¡æ­£æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡æ·»åŠ å™ªå£°æ¥å¹³æ»‘å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºï¼Œä»è€Œæé«˜å›¾åƒè´¨é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† Imagine Flashï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜ä¿çœŸã€å°‘æ­¥å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ä»¥å‡å°‘è®­ç»ƒ-æ¨ç†å·®å¼‚ï¼Œä¸€ä¸ªåŠ¨æ€è°ƒæ•´æ¯ä¸ªæ—¶é—´æ­¥é•¿çŸ¥è¯†è½¬ç§»çš„ç§»ä½é‡å»ºæŸå¤±ï¼ˆSRLï¼‰ï¼Œä»¥åŠç”¨äºæé«˜å›¾åƒè´¨é‡çš„å™ªå£°æ ¡æ­£ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒImagine Flash å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼Œä»…ä½¿ç”¨ä¸‰ä¸ªå»å™ªæ­¥éª¤å³å¯ä¸é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶å§‹ç»ˆè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚è¿™ç§å‰æ‰€æœªæœ‰çš„é‡‡æ ·æ•ˆç‡ä¸é«˜æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§ç›¸ç»“åˆï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹éå¸¸é€‚åˆå®æ—¶ç”Ÿæˆåº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¶…é«˜æ•ˆç”Ÿæˆå»ºæ¨¡é“ºå¹³äº†é“è·¯ã€‚æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ‰©å±•åˆ°è§†é¢‘å’Œ 3D ç­‰å…¶ä»–æ¨¡æ€ï¼Œè¿›ä¸€æ­¥å‡å°‘é‡‡æ ·é¢„ç®—ï¼Œä»¥åŠå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸äº’è¡¥çš„åŠ é€ŸæŠ€æœ¯ç›¸ç»“åˆã€‚é€šè¿‡å¯ç”¨å³æ—¶é«˜ä¿çœŸç”Ÿæˆï¼ŒImagine Flash ä¸ºå®æ—¶åˆ›æ„å·¥ä½œæµå’Œäº’åŠ¨åª’ä½“ä½“éªŒå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ä½¿ç”¨ä¸€åˆ°ä¸‰æ­¥å®ç°é«˜ä¿çœŸã€å¤šæ ·åŒ–çš„æ ·æœ¬ç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåå‘è’¸é¦ã€ç§»ä½é‡å»ºæŸå¤±å’Œå™ªå£°æ ¡æ­£ã€‚ï¼›æ€§èƒ½ï¼šåœ¨ ImageNet-64 æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ 1 æ­¥æ—¶ FID ä¸º 5.57ï¼Œä½¿ç”¨ 3 æ­¥æ—¶ FID ä¸º 4.69ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰ç«äº‰å¯¹æ‰‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d5adaf43fae278fddba0258413307ece.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e3e21e1e5b67cda3d8658d86e6854e63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c4cddde7364b59b3aef7c475c750db.jpg" align="middle"></details>## FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via   Diffusion Models**Authors:Jinglin Xu, Yijie Guo, Yuxin Peng**The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024. [PDF](http://arxiv.org/abs/2405.05216v1) Accepted by CVPR 2024**Summary**åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç»†ç²’åº¦æç¤ºé©±åŠ¨çš„å»å™ªå™¨ï¼Œå®ç°äº†3Däººä½“å§¿æ€ä¼°è®¡çš„ç»†ç²’åº¦å¼•å¯¼ã€‚**Key Takeaways**- é€šè¿‡æ–‡æœ¬å’Œäººä½“çŸ¥è¯†ç”Ÿæˆç»†ç²’åº¦æç¤ºï¼Œæä¾›éšå¼ç›‘ç£ï¼Œå¢å¼º 3D HPEã€‚- å»ºç«‹æç¤ºå’Œå§¿åŠ¿ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œæé«˜å»å™ªè´¨é‡ã€‚- å¼•å…¥æ—¶é—´ä¿¡æ¯ï¼Œå®ç°å»å™ªè¿‡ç¨‹çš„è‡ªé€‚åº”è°ƒæ•´ã€‚- FinePOSE åœ¨å•äººå’Œå¤šäººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šå‡è¾¾åˆ° SOTA æ€§èƒ½ã€‚- ç»†ç²’åº¦æç¤ºæä¾›äº†å¯¹ä¸åŒèº«ä½“éƒ¨ä½çš„ç»†è‡´æŒ‡å¯¼ã€‚- æç¤ºé©±åŠ¨çš„å»å™ªå™¨ä½¿ 3D HPE æ›´å¥½åœ°åˆ©ç”¨æ–‡æœ¬çŸ¥è¯†ã€‚- FinePOSE æ‰©å±•åˆ°å¤šäººä½“å§¿æ€ä¼°è®¡ï¼Œå¢å¼ºäº†å¤æ‚åœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ç²¾ç»†æç¤ºé©±åŠ¨çš„æ‰©æ•£æ¨¡å‹åœ¨ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡ä¸­çš„åº”ç”¨</p></li><li><p>Authors: Yuxin Sun, Yajie Zhao, Yifan Zhang, Xiangyang Xue, Jian Cheng</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢</p></li><li><p>Keywords: 3D Human Pose Estimation, Diffusion Model, Prompt Learning, Fine-grained Guidance</p></li><li><p>Urls: https://arxiv.org/abs/2302.06039, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡ï¼ˆ3D HPEï¼‰ä»»åŠ¡åˆ©ç”¨äºŒç»´å›¾åƒæˆ–è§†é¢‘é¢„æµ‹ä¸‰ç»´ç©ºé—´ä¸­çš„äººä½“å…³èŠ‚åæ ‡ã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬å¤§å¤šå¿½ç•¥äº†å°†å¯è®¿é—®æ–‡æœ¬å’Œäººç±»è‡ªç„¶å¯è¡Œçš„çŸ¥è¯†ç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œé”™å¤±äº†æœ‰ä»·å€¼çš„éšå¼ç›‘ç£æ¥æŒ‡å¯¼ 3D HPE ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„ç ”ç©¶é€šå¸¸ä»æ•´ä¸ªäººä½“çš„è§’åº¦ç ”ç©¶è¯¥ä»»åŠ¡ï¼Œå¿½ç•¥äº†éšè—åœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­çš„ç»†ç²’åº¦æŒ‡å¯¼ã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä»æ•´ä¸ªäººä½“çš„è§’åº¦ç ”ç©¶ 3D HPE ä»»åŠ¡ï¼Œå¿½ç•¥äº†éšè—åœ¨ä¸åŒèº«ä½“éƒ¨ä½ä¸­çš„ç»†ç²’åº¦æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹å¤æ‚å§¿åŠ¿å’ŒåŠ¨ä½œå»ºæ¨¡çš„èƒ½åŠ›ã€‚</p><p>(3): ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ï¼Œç”¨äº 3D HPEï¼Œåä¸º FinePOSEã€‚å®ƒç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼Œå¢å¼ºäº†æ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹ï¼šï¼ˆ1ï¼‰ç²¾ç»†éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå­¦ä¹ ï¼ˆFPPï¼‰æ¨¡å—é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆæ¥æ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ã€‚ï¼ˆ2ï¼‰ç²¾ç»†æç¤ºå§¿æ€é€šä¿¡ï¼ˆFPCï¼‰æ¨¡å—åœ¨å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´å»ºç«‹ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ã€‚ï¼ˆ3ï¼‰æç¤ºé©±åŠ¨çš„æ—¶åºé£æ ¼åŒ–ï¼ˆPTSï¼‰æ¨¡å—é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>(4): åœ¨å…¬å…±å•äººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFinePOSE ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°† FinePOSE æ‰©å±•åˆ°å¤šäººå§¿æ€ä¼°è®¡ã€‚åœ¨ EgoHumans æ•°æ®é›†ä¸Šå®ç° 34.3mm çš„å¹³å‡ MPJPEï¼Œè¯æ˜äº† FinePOSE å¤„ç†å¤æ‚å¤šäººåœºæ™¯çš„æ½œåŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šFinePOSE ç”±ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ç»„æˆï¼šç²¾ç»†éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå­¦ä¹ ï¼ˆFPPï¼‰æ¨¡å—ã€ç²¾ç»†æç¤ºå§¿æ€é€šä¿¡ï¼ˆFPCï¼‰æ¨¡å—å’Œæç¤ºé©±åŠ¨çš„æ—¶åºé£æ ¼åŒ–ï¼ˆPTSï¼‰æ¨¡å—ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šFPP æ¨¡å—é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆæ¥æ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šFPC æ¨¡å—åœ¨å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´å»ºç«‹ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šPTS æ¨¡å—é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ã€‚FinePOSE é€šè¿‡å°†å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ä¸å¯å­¦ä¹ æç¤ºç›¸ç»“åˆï¼Œæ„å»ºäº†ç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œä»¥å»ºæ¨¡éšå¼æŒ‡å¯¼ã€‚æ­¤å¤–ï¼ŒFinePOSE å»ºç«‹äº†å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œå¹¶é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²¾ç»†æç¤ºé©±åŠ¨çš„å»å™ªå™¨ FinePOSEï¼Œç”¨äº 3D HPE ä»»åŠ¡ã€‚FinePOSE åˆ©ç”¨å¯è®¿é—®çš„æ–‡æœ¬å’Œèº«ä½“éƒ¨ä½çš„è‡ªç„¶å¯è¡ŒçŸ¥è¯†ï¼Œæ„å»ºäº†ç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºï¼Œå¹¶å»ºç«‹äº†å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡ï¼Œä»¥æé«˜å»å™ªè´¨é‡ã€‚æ­¤å¤–ï¼ŒFinePOSE é›†æˆäº†å­¦ä¹ çš„æç¤ºåµŒå…¥å’Œä¸å™ªå£°çº§åˆ«ç›¸å…³çš„æ—¶åºä¿¡æ¯ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</p><p>æ€§èƒ½ï¼šåœ¨å…¬å…±å•äººå§¿æ€ä¼°è®¡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFinePOSE ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°† FinePOSE æ‰©å±•åˆ°å¤šäººå§¿æ€ä¼°è®¡ã€‚åœ¨ EgoHumans æ•°æ®é›†ä¸Šå®ç° 34.3mm çš„å¹³å‡ MPJPEï¼Œè¯æ˜äº† FinePOSE å¤„ç†å¤æ‚å¤šäººåœºæ™¯çš„æ½œåŠ›ã€‚</p><p>å·¥ä½œé‡ï¼šFinePOSE çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºéƒ¨ç½²å’Œä½¿ç”¨ã€‚ç„¶è€Œï¼Œæ„å»ºç²¾ç»†çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå»ºç«‹å­¦ä¹ çš„éƒ¨åˆ†æ„ŸçŸ¥æç¤ºå’Œå§¿æ€ä¹‹é—´çš„ç»†ç²’åº¦é€šä¿¡éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-30db10978f08bca4adc049e2f667efa7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58a38ed55c5b15686ce6cec8b0354b7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e88a5a9d36fae50e0d57909271d5070e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-186dcaee258d539e4f830d471e6a2c6e.jpg" align="middle"></details>## Fast LiDAR Upsampling using Conditional Diffusion Models**Authors:Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim TÃ¸rresen, Ryo Kurazume**The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks, to determine the ideal configuration, striking a balance between performance and inference speed. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments. [PDF](http://arxiv.org/abs/2405.04889v1) **æ‘˜è¦**æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”¨äºä¸‰ç»´åœºæ™¯ç‚¹äº‘çš„é«˜æ•ˆä¸”é«˜è´¨é‡ç¨€ç–åˆ°ç¨ å¯†ä¸Šé‡‡æ ·ã€‚**è¦ç‚¹*** æ‰©æ•£æ¨¡å‹å¯ç”¨äºé«˜ä¿çœŸåœ°ç”Ÿæˆç²¾ç‚¼çš„æ¿€å…‰é›·è¾¾æ•°æ®ã€‚* ç°æœ‰æ–¹æ³•å—é™äºæ€§èƒ½å’Œé€Ÿåº¦ï¼Œéš¾ä»¥å®æ—¶æ‰§è¡Œã€‚* æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºé€šè¿‡å›¾åƒè¡¨ç¤ºå¿«é€Ÿã€é«˜è´¨é‡åœ°å¯¹ä¸‰ç»´åœºæ™¯ç‚¹äº‘è¿›è¡Œç¨€ç–åˆ°ç¨ å¯†ä¸Šé‡‡æ ·ã€‚* è¯¥æ–¹æ³•é‡‡ç”¨ä½¿ç”¨æ¡ä»¶å†…æ’æ©ç è®­ç»ƒçš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒå®Œæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚* å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ã€‚* è¯¥æ–¹æ³•å¯ä»¥é€šè¿‡åŒæ—¶åœ¨çœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ¥å±•ç¤ºæ³›åŒ–èƒ½åŠ›ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>Title:ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œå¿«é€Ÿæ¿€å…‰é›·è¾¾ä¸Šé‡‡æ ·</li><li>Authors: Sander Elias Magnussen Helgesen, Kazuto Nakashima, Jim TÃ¸rresen, Ryo Kurazume</li><li>Affiliation: å¥¥æ–¯é™†å¤§å­¦ä¿¡æ¯å­¦ç³»</li><li>Keywords: 3D LiDAR, Conditional Diffusion Models, Image-based LiDAR data generation, Deep generative models</li><li>Urls: Paper: https://arxiv.org/abs/2405.04889v1, Github: None</li><li>Summary:</li></ol><p>(1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼Œç”±äºç¡¬ä»¶é™åˆ¶å’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨çš„æˆæœ¬ï¼Œæµ‹é‡æ•°æ®çš„è´¨é‡å’Œå¯†åº¦å·®å¼‚å¾ˆå¤§ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯­ä¹‰åˆ†å‰²å’Œç›®æ ‡æ£€æµ‹ç­‰æŠ€æœ¯æ€§èƒ½ä¸ä¸€è‡´ï¼Œè¿™å¯¹æ“ä½œæœºå™¨äººæ¥è¯´ä¸æ˜¯æœ€ä¼˜çš„ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è§£å†³ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œä½†è¿™äº›æ–¹æ³•æ¶‰åŠå¤æ‚çš„è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´æ…¢ï¼Œä¸é€‚åˆå®æ—¶æœºå™¨äººå¯¼èˆªç®¡é“ã€‚</p><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯åœ¨å›¾åƒè¡¨ç¤ºä¸­å»ºç«‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ç”Ÿæˆã€‚</p><p>(4):æœ¬æ–‡æ–¹æ³•åœ¨KITTI-360æ•°æ®é›†ä¸Šä½¿ç”¨ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œåœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•é€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ï¼Œå¼•å…¥è´¨é‡å’Œç¯å¢ƒçš„å˜åŒ–ï¼Œä»è€Œå…·æœ‰æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåœ¨å›¾åƒè¡¨ç¤ºä¸­å»ºç«‹æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ç”Ÿæˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä½¿ç”¨KITTI-360æ•°æ®é›†ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ï¼Œå¼•å…¥è´¨é‡å’Œç¯å¢ƒçš„å˜åŒ–ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>           (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒè¡¨ç¤ºçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ï¼Œä¸ºæé«˜æ¿€å…‰é›·è¾¾æ•°æ®è´¨é‡å’Œå¯†åº¦æä¾›äº†æ–°çš„æ–¹æ³•ï¼›           (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒè¡¨ç¤ºçš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆç»™å®šéƒ¨åˆ†è§‚å¯Ÿçš„æ¿€å…‰é›·è¾¾æ•°æ®ï¼Œå¹¶é€šè¿‡åŒæ—¶è®­ç»ƒçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨KITTI-360æ•°æ®é›†ä¸Šé‡‡æ ·ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·é€Ÿåº¦å’Œè´¨é‡ä¸Šä¼˜äºåŸºçº¿ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•æ¶‰åŠå¤æ‚çš„è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨ç†æ—¶é—´æ…¢ï¼Œä¸é€‚åˆå®æ—¶æœºå™¨äººå¯¼èˆªç®¡é“ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-767dd6ead611c7fe6a1bf019a995a405.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c74edffd87cfd5517cc664cb78371d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-21eb96ed158007c33f9b04a7adf7ab5c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-82f76f778074704994e29c06d305ad3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-948663529c880472c0969ced23400b05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e781edb01423cfd0018bde5e4738157d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-614fee94f5b68257335000a86b6b2590.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-13  OneTo3D One Image to Re-editable Dynamic 3D Model and Video Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/NeRF/</id>
    <published>2024-05-06T10:42:27.000Z</published>
    <updated>2024-05-06T10:42:27.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights"><a href="#WateRF-Robust-Watermarks-in-Radiance-Fields-for-Protection-of-Copyrights" class="headerlink" title="WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights"></a>WateRF: Robust Watermarks in Radiance Fields for Protection of   Copyrights</h2><p><strong>Authors:Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim</strong></p><p>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2405.02066v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœº (NeRF) æ°´å°å¯åŒæ—¶é€‚ç”¨äºéšå¼å’Œæ˜¾å¼ NeRF è¡¨ç¤ºï¼Œä»¥ä¿è¯ NeRF çš„ç‰ˆæƒä¿æŠ¤ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF æ°´å°æ˜¯ä¿æŠ¤ NeRF ç‰ˆæƒçš„å…³é”®è§£å†³æ–¹æ¡ˆã€‚</li><li>è¯¥æ–¹æ³•é€‚ç”¨äºéšå¼å’Œæ˜¾å¼ NeRF è¡¨ç¤ºã€‚</li><li>è¯¥æ–¹æ³•ä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢è¿›è¡Œæ°´å°ã€‚</li><li>è¯¥æ–¹æ³•é‡‡ç”¨å»¶è¿Ÿåå‘ä¼ æ’­ï¼Œæé«˜æ¸²æŸ“è´¨é‡å’Œæ¯”ç‰¹ç²¾åº¦ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å®¹é‡ã€ä¸å¯è§æ€§å’Œé²æ£’æ€§æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>è¯¥æ–¹æ³•æ¯”ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•å…·æœ‰æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚</li><li>è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ NeRF åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­åµŒå…¥äºŒè¿›åˆ¶ä¿¡æ¯æ¥å®ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šWateRFï¼šç”¨äºç‰ˆæƒä¿æŠ¤çš„è¾å°„åœºä¸­çš„é²æ£’æ°´å°</p></li><li><p>ä½œè€…ï¼šYoungdong Jangã€Dong In Leeã€MinHyuk Jangã€Jong Wook Kimã€Feng Yangã€Sangpil Kim</p></li><li><p>éš¶å±æœºæ„ï¼šéŸ©å›½å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€æ°´å°ã€ç‰ˆæƒä¿æŠ¤ã€éšå¼è¡¨ç¤ºã€æ˜¾å¼è¡¨ç¤º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://kuai-lab.github.io/cvpr2024waterf/ï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨ 3D å†…å®¹åˆ›å»ºå’Œ 3D å»ºæ¨¡ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†ä¿æŠ¤å…¶ç‰ˆæƒå°šæœªå¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚NeRF æ°´å°è¢«è®¤ä¸ºæ˜¯å®‰å…¨éƒ¨ç½²åŸºäº NeRF çš„ 3D è¡¨ç¤ºçš„å…³é”®è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä»…é€‚ç”¨äºéšå¼æˆ–æ˜¾å¼ NeRF è¡¨ç¤ºã€‚å®ƒä»¬çš„é—®é¢˜åœ¨äºæ— æ³•åŒæ—¶åº”ç”¨äºä¸¤ç§è¡¨ç¤ºã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ°´å°æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äº NeRF çš„ä¸¤ç§è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒ NeRF åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­åµŒå…¥äºŒè¿›åˆ¶æ¶ˆæ¯æ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ NeRF ç©ºé—´ä¸­çš„ç¦»æ•£å°æ³¢å˜æ¢è¿›è¡Œæ°´å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å»¶è¿Ÿåå‘ä¼ æ’­æŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¸é€å—æŸå¤±ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥åœ¨æœ€å°æƒè¡¡ä¸‹æé«˜æ¸²æŸ“è´¨é‡å’Œæ¯”ç‰¹å‡†ç¡®æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šæˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒæ–¹é¢è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šå®¹é‡ã€ä¸å¯è§æ€§å’ŒåµŒå…¥åœ¨ 2D æ¸²æŸ“å›¾åƒä¸­çš„æ°´å°çš„é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ä¹‹ä¸‹ï¼Œä»¥æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»è€Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>Methods:</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§å¾®è°ƒ NeRF çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸æ¶‰åŠæ”¹å˜æ¨¡å‹çš„æ¶æ„æ¥åµŒå…¥æ°´å°æ¶ˆæ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨å°†æ°´å°åµŒå…¥åˆ° NeRF æ¨¡å‹çš„æƒé‡ Î¸ ä¸­ï¼Œåœ¨æ¸²æŸ“å›¾åƒçš„é¢‘åŸŸä¸­ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•ä¸åŒäºä¼ ç»Ÿçš„æ•°å­—æ°´å°æ–¹æ³•ï¼Œå®ƒä¸“æ³¨äºè®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨ã€‚ä¸åŒä¹‹å¤„åœ¨äºå¾®è°ƒè¿‡ç¨‹ï¼Œå®ƒåœ¨ä¸ä½¿ç”¨ç¼–ç å™¨çš„æƒ…å†µä¸‹åµŒå…¥æ°´å°ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæœ‰ 2 ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰é¢„è®­ç»ƒæ°´å°è§£ç å™¨ Dï¼Œï¼ˆ2ï¼‰å¾®è°ƒ NeRF æ¨¡å‹ FÎ¸ ä»¥åµŒå…¥æ¶ˆæ¯ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šæˆ‘ä»¬çš„æ–¹æ³•å¦‚å›¾ 2 æ‰€ç¤ºï¼Œå¹¶åœ¨ä¸‹æ–‡è¯¦ç»†æè¿°ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé¢„è®­ç»ƒæ°´å°è§£ç å™¨ï¼šæˆ‘ä»¬é€‰æ‹© HiDDeN [58] æ¶æ„ä½œä¸ºæˆ‘ä»¬çš„æ°´å°è§£ç å™¨ã€‚HiDDeN åŒ…å«ä¸¤ä¸ªç”¨äºæ•°æ®éšè—çš„å·ç§¯ç½‘ç»œï¼šæ°´å°ç¼–ç å™¨ E å’Œæ°´å°è§£ç å™¨ Dã€‚ä¸ºäº†é²æ£’æ€§ï¼Œå®ƒåŒ…æ‹¬ä¸€ä¸ªå™ªå£°å±‚ Nã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªä»…å…³æ³¨è§£ç å™¨æ€§èƒ½çš„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬æ’é™¤äº†è´Ÿè´£æé«˜è§†è§‰è´¨é‡çš„å¯¹å¶æŸå¤±ã€‚åœ¨è®­ç»ƒå®Œ HiDDeN æ¨¡å‹åï¼Œæ°´å°ç¼–ç å™¨ E åœ¨ç¬¬äºŒé˜¶æ®µæ²¡æœ‰è¢«ä½¿ç”¨ã€‚</p><p>ï¼ˆ7ï¼‰ï¼šç¼–ç å™¨ E ä»¥å°é¢å›¾åƒ Io âˆˆ RHÃ—W Ã—3 å’Œé•¿åº¦ä¸º L çš„äºŒè¿›åˆ¶æ¶ˆæ¯ M âˆˆ {0, 1}L ä¸ºè¾“å…¥ã€‚ç„¶å E å°† M åµŒå…¥åˆ° Io ä¸­å¹¶ç”Ÿæˆç¼–ç å›¾åƒ Iwã€‚ä¸ºäº†ä½¿è§£ç å™¨å¯¹æ—‹è½¬å’Œ JPEG å‹ç¼©ç­‰å„ç§å¤±çœŸå…·æœ‰é²æ£’æ€§ï¼ŒIw ä½¿ç”¨å™ªå£°å±‚ N è¿›è¡Œè½¬æ¢ã€‚ç”±å¤šä¸ªå·ç§¯å±‚ç»„æˆçš„è§£ç å™¨ D ä»¥ Iw ä¸ºè¾“å…¥ï¼Œå¹¶æå–æ¶ˆæ¯ Mâ€²ã€‚</p><p>ï¼ˆ8ï¼‰ï¼šMâ€² = D(N(Iw))</p><p>ï¼ˆ9ï¼‰ï¼šæˆ‘ä»¬åˆ©ç”¨ sigmoid å‡½æ•°å°†æå–çš„æ¶ˆæ¯ Mâ€² çš„èŒƒå›´è®¾ç½®ä¸º [0, 1]ã€‚æ¶ˆæ¯æŸå¤±ä½¿ç”¨ ML å’Œ sigmoid sg(Mâ€²L) ä¹‹é—´çš„äºŒå…ƒäº¤å‰ç†µ (BCE) è®¡ç®—ã€‚</p><p>ï¼ˆ10ï¼‰ï¼šLmessage = âˆ’ Lâˆ‘i=1 Mi Â· log sg(Mâ€²i) + (1 âˆ’ Mi) Â· log(1 âˆ’ sg(Mâ€²i)))</p><p>ï¼ˆ11ï¼‰ï¼šè§£ç å™¨ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥æ£€æµ‹ç»è¿‡è®­ç»ƒç¼–ç å™¨å¤„ç†çš„å›¾åƒä¸­çš„æ°´å°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨ç¬¬äºŒé˜¶æ®µä¸ä½¿ç”¨ç¼–ç å™¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“è§£ç å™¨æ¥æ”¶åˆ°é¦™è‰æ¸²æŸ“çš„å›¾åƒæ—¶ï¼Œæå–çš„æ¶ˆæ¯ä½ä¹‹é—´å­˜åœ¨åå·®ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒè§£ç å™¨åï¼Œæˆ‘ä»¬å¯¹çº¿æ€§è§£ç å™¨å±‚è¿›è¡Œ PCA ç™½åŒ–ä»¥æ¶ˆé™¤åå·®ï¼ŒåŒæ—¶ä¸é™ä½æå–èƒ½åŠ›ã€‚</p><p>ï¼ˆ12ï¼‰ï¼šåœ¨ DWT ä¸ŠåµŒå…¥å’Œæå–æ°´å°ï¼šåœ¨ç©ºé—´åŸŸä¸­åŠ æ°´å°æ˜¯ä¸€ç§ç›¸å¯¹ç®€å•çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒåœ¨æ•´ä¸ªå›¾åƒä¸­åµŒå…¥æ°´å°ã€‚æœ€è¿‘ï¼Œä¸€ç§åœ¨ç©ºé—´åŸŸä¸­å¯¹ NeRF è¿›è¡Œå¾®è°ƒçš„æ°´å°æ–¹æ³• [16] æµ®å‡ºæ°´é¢ã€‚å°½ç®¡åœ¨ç©ºé—´åŸŸä¸­åµŒå…¥æ¶ˆæ¯çš„å¾®è°ƒæ–¹æ³•æ˜¾ç¤ºå‡ºæ— ä¸ä¼¦æ¯”çš„ä¸å¯è§æ€§å’Œæ¶ˆæ¯æå–èƒ½åŠ›ï¼Œä½†å®ƒå®¹æ˜“å—åˆ°æ‰­æ›²ç©ºé—´åŸŸçš„æ”»å‡»ï¼Œä¾‹å¦‚è£å‰ªã€‚ç›´æ¥åº”ç”¨æ¥è‡ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ [7] çš„ç©ºé—´åŸŸæŠ€æœ¯ä¸å…è®¸æœ‰æ•ˆè°ƒæ•´ NeRF çš„æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨é¢‘åŸŸè€Œä¸æ˜¯ç©ºé—´åŸŸä¸­çš„å¾®è°ƒæ–¹æ³•ã€‚å¤šå¹´æ¥ï¼Œå„ç§å›¾åƒæ°´å°æŠ€æœ¯ä½¿ç”¨é¢‘åŸŸï¼ŒåŒ…æ‹¬ç¦»æ•£ä½™å¼¦å˜æ¢ (DCT) å’Œç¦»æ•£å°æ³¢å˜æ¢ (DWT)ï¼Œå–å¾—äº†æŒç»­çš„å‘å±•å’Œæ”¹è¿›ã€‚æˆ‘ä»¬å‘ç° DWT æ˜¯å°†æ¶ˆæ¯ç¼–ç åˆ° NeRF æ¨¡å‹æƒé‡ä¸­çš„åˆé€‚åŸŸã€‚ç»™å®šç›¸åº”çš„ç›¸æœºå‚æ•°ï¼ŒNeRF æ¨¡å‹æ¸²æŸ“ 3D æ¨¡å‹çš„ä¸åŒè§†å›¾ã€‚æˆ‘ä»¬å°†æ¸²æŸ“å›¾åƒçš„åƒç´ ï¼Œè¡¨ç¤ºä¸º X = (xc, yc) âˆˆ RHÃ—W Ã—3ï¼Œè½¬æ¢ä¸ºå°æ³¢å½¢å¼ï¼Œå…¶ä¸­ c è¡¨ç¤ºé€šé“ã€‚DWT å®šä¹‰ä¸º [10]ï¼š</p><p>ï¼ˆ13ï¼‰ï¼šWÏ†(j0, m, n) = 1âˆšMNâˆ‘Mâˆ’1xc=0âˆ‘Nâˆ’1yc=0 f(xc, yc)Ï†j0,m,n(xc, yc),</p><p>ï¼ˆ14ï¼‰ï¼šW i Ïˆ(j, m, n) = 1âˆšMNâˆ‘Mâˆ’1xc=0âˆ‘Nâˆ’1yc=0 f(xc, yc)Ïˆi j,m,n(xc, yc)</p><p>ï¼ˆ15ï¼‰ï¼šå…¶ä¸­ Ï†(x, y) æ˜¯å°ºåº¦å‡½æ•°ï¼ŒÏˆ(x, y) æ˜¯å°æ³¢å‡½æ•°ã€‚WÏ†(j0, m, n) è¢« LL å­å¸¦è°ƒç”¨ï¼Œå®ƒæ˜¯å›¾åƒåœ¨å°ºåº¦ j0 çš„è¿‘ä¼¼å€¼ï¼ŒW i Ïˆ å…¶ä¸­ i = {H, V, D} åˆ†åˆ«è¡¨ç¤º LHã€HLã€HH å­å¸¦ã€‚å…ˆå‰çš„ç ”ç©¶é€‰æ‹© LHã€HL å’Œ HH å­å¸¦æ¥åµŒå…¥æ°´å°ï¼Œå› ä¸º LL å­å¸¦åŒ…å«å›¾åƒçš„é‡è¦ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€‰æ‹© LL å­å¸¦ä½œä¸ºè§£ç å™¨ D çš„è¾“å…¥ï¼Œå¹¶ç”¨ Mâ€² = D(WÏ†) è·å–æå–çš„æ¶ˆæ¯ã€‚ä½¿ç”¨ HiDDeN è§£ç å™¨ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒå‘ç°ï¼Œåœ¨ LL å­å¸¦ä¸­åµŒå…¥æ°´å°æ¯”å…¶ä»–å­å¸¦æ›´ç¨³å¥ï¼Œå¹¶ä¸”æ›´æœ‰æ•ˆåœ°åµŒå…¥æ°´å°ä¿¡æ¯ã€‚DWT çš„ç‰¹ç‚¹æ˜¯å…¶å­å¸¦åœ¨ä¸åŒçº§åˆ«ä¸Šè®¡ç®—ï¼›å› æ­¤ï¼Œä¸ºæˆ‘ä»¬çš„ç›®çš„é€‰æ‹©ä¸€ä¸ªæœ€ä½³çº§åˆ«æ˜¯å¿…è¦çš„ã€‚1 çº§å°†å›¾åƒåˆ†æˆ 4 ä¸ªå­å¸¦ (LL1, LH1, HL1, HH1)ï¼Œ</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•å°†å›¾åƒä»ç©ºé—´åŸŸè½¬æ¢åˆ°é¢‘åŸŸï¼Œæœ‰æ•ˆåœ°å°†æ°´å°ç¼–ç åˆ°å›¾åƒä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å˜æ¢å’Œé€å—æŸå¤±å¯ä»¥æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§ç¥ç» 3D æ°´å°æ–¹æ³•ï¼Œç”¨äº NeRF æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†åˆ«è®­ç»ƒ 2D æ°´å°è§£ç å™¨å’Œ NeRF æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æµæ°´çº¿åªéœ€è¦è®­ç»ƒä¸€æ¬¡è§£ç å™¨ï¼Œå¹¶åœ¨ä¸åŒçš„ NeRF æ°´å°æ¨¡å‹ä¸Šé‡å¤ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬é‡‡ç”¨å›¾åƒæ°´å°ä¸­çš„ä¼ ç»Ÿæ°´å°æŠ€æœ¯ï¼Œå°†å›¾åƒä»ç©ºé—´åŸŸè½¬æ¢åˆ°é¢‘åŸŸï¼Œä»¥æœ‰æ•ˆåœ°å°†æ°´å°ç¼–ç åˆ°å›¾åƒä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å˜æ¢å’Œé€å—æŸå¤±å¯ä»¥æé«˜æ•´ä½“å›¾åƒè´¨é‡ã€‚</p><p>æ€§èƒ½ï¼šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»è€Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒå’ŒåµŒå…¥æ°´å°æ–¹é¢å…·æœ‰è¾ƒä½çš„è®¡ç®—æˆæœ¬ï¼Œä½¿å…¶é€‚ç”¨äºå®é™…åº”ç”¨ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-21a78eb3599c5468a4ea257df96b8cdc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59110a6f2727d6c4ae7b744d2459165a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-59cf18deef7514767b02ec7654c8da33.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9d7125c0e81d7d4fa4f485a0ca63c94.jpg" align="middle"></details><h2 id="Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy"><a href="#Multi-view-Action-Recognition-via-Directed-Gromov-Wasserstein-Discrepancy" class="headerlink" title="Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy"></a>Multi-view Action Recognition via Directed Gromov-Wasserstein   Discrepancy</h2><p><strong>Authors:Hoang-Quan Nguyen, Thanh-Dat Truong, Khoa Luu</strong></p><p>Action recognition has become one of the popular research topics in computer vision. There are various methods based on Convolutional Networks and self-attention mechanisms as Transformers to solve both spatial and temporal dimensions problems of action recognition tasks that achieve competitive performances. However, these methods lack a guarantee of the correctness of the action subject that the models give attention to, i.e., how to ensure an action recognition model focuses on the proper action subject to make a reasonable action prediction. In this paper, we propose a multi-view attention consistency method that computes the similarity between two attentions from two different views of the action videos using Directed Gromov-Wasserstein Discrepancy. Furthermore, our approach applies the idea of Neural Radiance Field to implicitly render the features from novel views when training on single-view datasets. Therefore, the contributions in this work are three-fold. Firstly, we introduce the multi-view attention consistency to solve the problem of reasonable prediction in action recognition. Secondly, we define a new metric for multi-view consistent attention using Directed Gromov-Wasserstein Discrepancy. Thirdly, we built an action recognition model based on Video Transformers and Neural Radiance Fields. Compared to the recent action recognition methods, the proposed approach achieves state-of-the-art results on three large-scale datasets, i.e., Jester, Something-Something V2, and Kinetics-400. </p><p><a href="http://arxiv.org/abs/2405.01337v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºå¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§å’Œç¥ç»è¾å°„åœºï¼Œæå‡ºæ—¶ç©ºä¸€è‡´åŠ¨ä½œè¯†åˆ«æ–°æ–¹æ³•ï¼Œå®ç°åŠ¨ä½œè¯†åˆ«é¢†åŸŸæœ€ä¼˜ç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºå¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§è§£å†³åŠ¨ä½œè¯†åˆ«åˆç†é¢„æµ‹é—®é¢˜ã€‚</li><li>å®šä¹‰åŸºäºæœ‰å‘æ ¼ç½—è«å¤«-ç“¦ç‘Ÿæ–¯å¦è·ç¦»çš„å¤šè§†å›¾ä¸€è‡´æ³¨æ„åŠ›åº¦é‡ã€‚</li><li>åŸºäºè§†é¢‘å˜å½¢é‡‘åˆšå’Œç¥ç»è¾å°„åœºæ„å»ºåŠ¨ä½œè¯†åˆ«æ¨¡å‹ã€‚</li><li>åœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜ç»“æœã€‚</li><li>åˆ›æ–°æ€§åœ°å¼•å…¥äº†å¤šè§†å›¾æ³¨æ„åŠ›ä¸€è‡´æ€§ï¼Œè§£å†³äº†åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„éš¾é¢˜ã€‚</li><li>é‡‡ç”¨æ–°é¢–çš„åº¦é‡æ–¹æ³•è¯„ä¼°å¤šè§†å›¾ä¸€è‡´æ³¨æ„åŠ›ã€‚</li><li>å°†ç¥ç»è¾å°„åœºåº”ç”¨äºåŠ¨ä½œè¯†åˆ«ï¼Œæå‡äº†æ¨¡å‹åœ¨å•è§†å›¾æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šå¤šè§†è§’åŠ¨ä½œè¯†åˆ«ç»ç”±å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚</p></li><li><p>ä½œè€…ï¼šHoang-Quan Nguyenï¼ŒThanh-Dat Truongï¼ŒKhoa Luu</p></li><li><p>å•ä½ï¼šé˜¿è‚¯è‰²å¤§å­¦è®¡ç®—æœºè§†è§‰ä¸å›¾åƒç†è§£å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šåŠ¨ä½œè¯†åˆ«ï¼Œå¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§ï¼Œå®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ï¼Œç¥ç»è¾å°„åœº</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01337</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šåŠ¨ä½œè¯†åˆ«æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ï¼Œç°æœ‰çš„åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚ Transformerï¼‰çš„æ–¹æ³•åœ¨è§£å†³åŠ¨ä½œè¯†åˆ«ä»»åŠ¡çš„æ—¶ç©ºç»´åº¦é—®é¢˜ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹å¯¹æ¨¡å‹å…³æ³¨çš„åŠ¨ä½œä¸»ä½“æ­£ç¡®æ€§çš„ä¿è¯ï¼Œå³å¦‚ä½•ç¡®ä¿åŠ¨ä½œè¯†åˆ«æ¨¡å‹å…³æ³¨é€‚å½“çš„åŠ¨ä½œä¸»ä½“ä»¥åšå‡ºåˆç†çš„åŠ¨ä½œé¢„æµ‹ã€‚</p><p>(2)ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦åŸºäºå·ç§¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†ç¼ºä¹å¯¹æ¨¡å‹å…³æ³¨çš„åŠ¨ä½œä¸»ä½“æ­£ç¡®æ€§çš„ä¿è¯ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„é—®é¢˜ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œåˆ©ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—åŠ¨ä½œè§†é¢‘ä¸¤ä¸ªä¸åŒè§†è§’çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åº”ç”¨ç¥ç»è¾å°„åœºçš„æ€æƒ³ï¼Œåœ¨å•è§†è§’æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶éšå¼æ¸²æŸ“æ–°è§†è§’çš„ç‰¹å¾ã€‚</p><p>(4)ï¼šè¯¥æ–¹æ³•åœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¯æ˜äº†å…¶æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>Methods:</li></ol><p>(1):ä½¿ç”¨ Video Transformer æ¡†æ¶è¿›è¡ŒåŠ¨ä½œè¯†åˆ«ï¼Œå°†è§†é¢‘åˆ†è§£ä¸º patches å¹¶è¿›è¡Œä½ç½®ç¼–ç ï¼Œç„¶åä½¿ç”¨ Transformer ç¼–ç å™¨æå–ç‰¹å¾ï¼›</p><p>(2):é‡‡ç”¨ Neural Radiance Field çš„æ€æƒ³ï¼Œé€šè¿‡ StyleNeRF å°†ç‰¹å¾ä½“æ˜ å°„åˆ°é£æ ¼å‘é‡ï¼Œå¹¶è°ƒèŠ‚ NeRF æ¨¡å—ä¸­ MLP å±‚çš„æƒé‡çŸ©é˜µï¼Œä»¥åœ¨æ–°çš„è§†è§’ä¸‹æ¸²æŸ“ä½åˆ†è¾¨ç‡ç‰¹å¾ä½“ï¼›</p><p>(3):ä½¿ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ï¼ˆDirected Gromov-Wasserstein Discrepancyï¼‰è®¡ç®—ä¸åŒè§†è§’ä¸‹åŠ¨ä½œè§†é¢‘çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ï¼Œè¯¥æ–¹æ³•é€šè¿‡è®¡ç®—ä¸¤ä¸ªç©ºé—´å†…å®šä¹‰çš„åº¦é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§æ¥æ¯”è¾ƒåˆ†å¸ƒï¼Œå¯¹ç›¸æœºå¹³ç§»å¼•èµ·çš„æ³¨æ„åŠ›å›¾è½¬æ¢å…·æœ‰é²æ£’æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>(1) æœ¬æ–‡æå‡ºçš„å¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œé€šè¿‡å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—ä¸åŒè§†è§’ä¸‹åŠ¨ä½œè§†é¢‘çš„ä¸¤ä¸ªæ³¨æ„åŠ›çš„ç›¸ä¼¼æ€§ï¼Œè§£å†³äº†åŠ¨ä½œè¯†åˆ«ä¸­åˆç†é¢„æµ‹çš„é—®é¢˜ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p><p>(2) åˆ›æ–°ç‚¹ï¼šæå‡ºå¤šè§†è§’æ³¨æ„åŠ›ä¸€è‡´æ€§æ–¹æ³•ï¼Œåˆ©ç”¨å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚è®¡ç®—æ³¨æ„åŠ›ç›¸ä¼¼æ€§ï¼›æ€§èƒ½ï¼šåœ¨ Jesterã€Something-Something V2 å’Œ Kinetics-400 ä¸‰ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ä¸Šå–å¾—æœ€å…ˆè¿›çš„ç»“æœï¼›å·¥ä½œé‡ï¼šéœ€è¦è®­ç»ƒ Neural Radiance Field æ¨¡å—ï¼Œè®¡ç®—å®šå‘æ ¼ç½—è«å¤«-æ²ƒç‘Ÿæ–¯å¦å·®å¼‚ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-c677262cde72d554d4ab784234b1941b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59ee15896ae28d3e32188fedbfd5bc0d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  WateRF Robust Watermarks in Radiance Fields for Protection of   Copyrights</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/3DGS/</id>
    <published>2024-05-06T10:35:16.000Z</published>
    <updated>2024-05-06T10:35:16.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Talking%20Head%20Generation/</id>
    <published>2024-05-06T10:33:19.000Z</published>
    <updated>2024-05-06T10:33:19.431Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations"><a href="#CoVoMix-Advancing-Zero-Shot-Speech-Generation-for-Human-like-Multi-talker-Conversations" class="headerlink" title="CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations"></a>CoVoMix: Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</h2><p><strong>Authors:Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</strong></p><p>Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge in the field. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is capable of first converting dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. These dialogues, generated within a single channel, are characterized by seamless speech transitions, including overlapping speech, and appropriate paralinguistic behaviors such as laughter. Audio samples are available at <a href="https://aka.ms/covomix">https://aka.ms/covomix</a>. </p><p><a href="http://arxiv.org/abs/2404.06690v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨CoVoMixï¼Œä¸€ç§é›¶æ ·æœ¬å¯¹è¯è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸã€è¿è´¯ä¸”å¤šå›åˆçš„å¤šäººå¯¹è¯è¯­éŸ³ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CoVoMixæ˜¯ä¸€æ¬¾é›¶æ ·æœ¬å¯¹è¯è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li><li>CoVoMixèƒ½å¤Ÿå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºç¦»æ•£æ ‡è®°æµï¼Œè¡¨ç¤ºå„ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li><li>æµåŒ¹é…å£°å­¦æ¨¡å‹å°†æ ‡è®°æµè½¬æ¢æˆæ··åˆçš„æ¢…å°”é¢‘è°±å›¾ã€‚</li><li>HiFi-GANæ¨¡å‹å°†æ¢…å°”é¢‘è°±å›¾è½¬æ¢ä¸ºè¯­éŸ³æ³¢å½¢ã€‚</li><li>CoVoMixé‡‡ç”¨äº†ä¸€å¥—ç»¼åˆçš„æŒ‡æ ‡æ¥è¡¡é‡å¯¹è¯å»ºæ¨¡å’Œç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMixç”Ÿæˆçš„å¯¹è¯å…·æœ‰é€¼çœŸçš„è‡ªç„¶æ€§å’Œè¿è´¯æ€§ï¼Œå¹¶ä¸”æ¶‰åŠå¤šä¸ªè¯´è¯è€…å‚ä¸å¤šè½®å¯¹è¯ã€‚</li><li>ç”Ÿæˆçš„å¯¹è¯åœ¨å•ä¸ªé€šé“å†…ï¼Œå…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢ï¼ˆåŒ…æ‹¬é‡å è¯­éŸ³ï¼‰å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºï¼ˆä¾‹å¦‚ç¬‘å£°ï¼‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CoVoMix: æ¨è¿›é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä»¥å®ç°ç±»äººå¤šè¯´è¯è€…å¯¹è¯</p></li><li><p>Authors: Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng</p></li><li><p>Affiliation: ä¸Šæµ·äº¤é€šå¤§å­¦</p></li><li><p>Keywords: é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆï¼Œå¤šè¯´è¯è€…å¯¹è¯ï¼Œè¯­éŸ³åˆæˆï¼Œè‡ªç„¶è¯­è¨€å¤„ç†</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.06690 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ (TTS) å»ºæ¨¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œå¤šæ ·åŒ–çš„è¯­éŸ³æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹è¯ç”Ÿæˆä»¥åŠåœ¨è¯­éŸ³ä¸­å®ç°ç±»äººçš„è‡ªç„¶æ€§ä»ç„¶æ˜¯è¯¥é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å•æµæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ï¼Œæ— æ³•ç”Ÿæˆå¤šè¯´è¯è€…å¯¹è¯ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œè¿™åœ¨å¯¹è¯åœºæ™¯ä¸­å¯èƒ½ä¸å¯ç”¨ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CoVoMix çš„æ¨¡å‹ï¼Œç”¨äºé›¶æ ·æœ¬ã€ç±»äººã€å¤šè¯´è¯è€…ã€å¤šè½®å¯¹è¯è¯­éŸ³ç”Ÿæˆã€‚CoVoMix èƒ½å¤Ÿé¦–å…ˆå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£ä»¤ç‰Œæµï¼Œæ¯ä¸ªä»¤ç‰Œæµè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶åå°†è¿™äº›ä»¤ç‰Œæµé¦ˆé€åˆ°åŸºäºæµåŒ¹é…çš„å£°å­¦æ¨¡å‹ä¸­ä»¥ç”Ÿæˆæ··åˆæ¢…å°”è°±å›¾ã€‚æœ€åï¼Œä½¿ç”¨ HiFi-GAN æ¨¡å‹ç”Ÿæˆè¯­éŸ³æ³¢å½¢ã€‚</p><p>(4): å®éªŒç»“æœï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMix å¯ä»¥ç”Ÿæˆä¸ä»…åœ¨è‡ªç„¶æ€§å’Œè¿è´¯æ€§ä¸Šç±»ä¼¼äººç±»çš„å¯¹è¯ï¼Œè€Œä¸”è¿˜æ¶‰åŠå¤šä¸ªè¯´è¯è€…è¿›è¡Œå¤šè½®å¯¹è¯ã€‚è¿™äº›åœ¨å•ä¸ªé€šé“å†…ç”Ÿæˆçš„å¯¹è¯å…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢ï¼ŒåŒ…æ‹¬é‡å è¯­éŸ³å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºï¼Œä¾‹å¦‚ç¬‘å£°ã€‚</p><ol><li>ç ”ç©¶æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šå¤šæµæ–‡æœ¬åˆ°è¯­ä¹‰æ¨¡å‹ï¼šåŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå°†æ–‡æœ¬æ ‡è®°åºåˆ—è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£æ ‡è®°æµï¼Œæ¯ä¸ªæ ‡è®°æµè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå£°å­¦æ¨¡å‹ï¼šåŸºäºæµåŒ¹é…çš„å˜å‹å™¨ç¼–ç å™¨ï¼Œå°†è¯­ä¹‰åºåˆ—è½¬æ¢ä¸ºæ··åˆæ¢…å°”è°±å›¾ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šå£°ç å™¨ï¼šä½¿ç”¨ HiFi-GAN æ¨¡å‹ä»æ¢…å°”è°±å›¾ç”Ÿæˆè¯­éŸ³æ³¢å½¢ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ CoVoMix æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€ç±»äººã€å¤šè¯´è¯è€…ã€å¤šè½®å¯¹è¯è¯­éŸ³ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæµåŒ¹é…çš„å¤šæµæ–‡æœ¬åˆ°è¯­ä¹‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†å¯¹è¯æ–‡æœ¬è½¬æ¢ä¸ºå¤šä¸ªç¦»æ•£ä»¤ç‰Œæµï¼Œè¡¨ç¤ºå•ä¸ªè¯´è¯è€…çš„è¯­ä¹‰ä¿¡æ¯ï¼›è®¾è®¡äº†ä¸€ç§åŸºäºæµåŒ¹é…çš„å£°å­¦æ¨¡å‹ï¼Œå°†è¯­ä¹‰åºåˆ—è½¬æ¢ä¸ºæ··åˆæ¢…å°”è°±å›¾ï¼›é‡‡ç”¨ HiFi-GAN æ¨¡å‹ç”Ÿæˆè¯­éŸ³æ³¢å½¢ï¼Œå®ç°äº†é«˜ä¿çœŸè¯­éŸ³åˆæˆã€‚</p><p>æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒCoVoMix ç”Ÿæˆçš„å¯¹è¯åœ¨è‡ªç„¶æ€§å’Œè¿è´¯æ€§ä¸Šç±»ä¼¼äººç±»ï¼Œæ¶‰åŠå¤šä¸ªè¯´è¯è€…è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œå…·æœ‰æ— ç¼çš„è¯­éŸ³è½¬æ¢å’Œé€‚å½“çš„å‰¯è¯­è¨€è¡Œä¸ºã€‚</p><p>å·¥ä½œé‡ï¼šCoVoMix æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-271fa2d3d54bc8eac1b4ebb4afb68b5f.jpg" align="middle"></details><h2 id="Beyond-Talking-â€”-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication"><a href="#Beyond-Talking-â€”-Generating-Holistic-3D-Human-Dyadic-Motion-for-Communication" class="headerlink" title="Beyond Talking â€” Generating Holistic 3D Human Dyadic Motion for   Communication"></a>Beyond Talking â€” Generating Holistic 3D Human Dyadic Motion for   Communication</h2><p><strong>Authors:Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, Ruqi Huang</strong></p><p>In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.19467v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡éŸ³é¢‘ç‰¹å¾å’Œæ–‡æœ¬è¯­ä¹‰ç›¸ç»“åˆçš„æ–¹å¼ï¼Œå®ç°è¯´è¯äººå’Œå€¾å¬è€…3Dé€¼çœŸä¸”åè°ƒçš„åŠ¨ä½œç”Ÿæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºéŸ³é¢‘ç‰¹å¾ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯è§£è€¦åˆçš„åˆ›æ–°ä»»åŠ¡ï¼Œç”Ÿæˆè¯´è¯äººå’Œå€¾å¬è€…çš„3DåŠ¨ä½œã€‚</li><li>åˆ†åˆ«è®­ç»ƒè¯´è¯äººå’Œå€¾å¬è€…çš„æ•´ä½“åŠ¨ä½œVQ-VAEã€‚</li><li>è€ƒè™‘è¯´è¯äººå’Œå€¾å¬è€…ä¹‹é—´çš„å®æ—¶ç›¸äº’å½±å“ï¼Œæå‡ºè‡ªå›å½’æ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆè¯´è¯äººå’Œå€¾å¬è€…çš„åŠ¨ä½œã€‚</li><li>é“¾å¼Transformeræ¨¡å‹ï¼Œæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œçš„æ²Ÿé€šåœºæ™¯ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>å¼•å…¥HoCoæ•´ä½“æ²Ÿé€šæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›å®è´µèµ„æºã€‚</li><li>æ¥å—åå°†HoCoæ•°æ®é›†å’Œä»£ç å‘å¸ƒç”¨äºç ”ç©¶ç›®çš„ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šè¶…è¶Šè¯´è¯â€”â€”ç”Ÿæˆç”¨äºäº¤æµçš„æ•´ä½“ 3D äººç±»åŒäººè¿åŠ¨</p></li><li><p>ä½œè€…ï¼šMingze Sun Â· Chao Xu Â· Xinyu Jiang Â· Yang Liu Â· Baigui Sun Â· Ruqi Huang</p></li><li><p>å•ä½ï¼šæ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢</p></li><li><p>å…³é”®è¯ï¼šDyadic Motion, Holistic Human Mesh, Communication</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šNone, Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºå¤§è§„æ¨¡äººç±»è¯´è¯è§†é¢‘çš„è¯­éŸ³ç”Ÿæˆè¿åŠ¨ä»»åŠ¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå³ä»è¨€è¯­çº¿ç´¢ï¼ˆå¦‚éŸ³é¢‘ç‰‡æ®µæˆ–è½¬å½•ï¼‰ä¸­ç”Ÿæˆè°ˆè¯ä¸­çš„éè¯­è¨€ä¿¡å·ï¼ˆå¦‚é¢éƒ¨è¡¨æƒ…æˆ–èº«ä½“åŠ¨ä½œï¼‰ï¼Œä¾‹å¦‚äººç±»é¢éƒ¨è¡¨æƒ…ã€èº«ä½“å§¿åŠ¿å’Œæ‰‹åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…å…³æ³¨è¯´è¯è€…ï¼Œè€Œå¿½ç•¥äº†å¬ä¼—çš„ååº”ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä»¥å¾€æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆè¯´è¯è€…çš„å¤´éƒ¨ã€æ‰‹åŠ¿æˆ–å…¨èº«è¿åŠ¨ï¼Œè€Œå¿½ç•¥äº†å¬ä¼—çš„ååº”ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡è®°æ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œä¸“æ³¨äºäººç±»äº¤æµï¼Œæ—¨åœ¨ä¸ºè¯´è¯è€…å’Œå¬ä¼—ç”Ÿæˆ 3D æ•´ä½“äººç±»åŠ¨ä½œã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å°†åˆ†è§£å› å­åŒ–ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„ç»“åˆï¼Œä»è€Œä¿ƒè¿›åˆ›å»ºæ›´é€¼çœŸå’Œåè°ƒçš„åŠ¨ä½œã€‚æˆ‘ä»¬åˆ†åˆ«é’ˆå¯¹è¯´è¯è€…å’Œå¬ä¼—çš„æ•´ä½“åŠ¨ä½œè®­ç»ƒ VQ-VAEã€‚æˆ‘ä»¬è€ƒè™‘äº†è¯´è¯è€…å’Œå¬ä¼—ä¹‹é—´çš„å®æ—¶ç›¸äº’å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„é“¾å¼åŸºäº Transformer çš„è‡ªå›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“é—¨è®¾è®¡ç”¨äºæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œä¸­çš„äº¤æµåœºæ™¯ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„åŠ¨ä½œã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†æˆ‘ä»¬ç”Ÿæˆçš„ç»“æœæ—¢åè°ƒåˆå¤šæ ·ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•çš„æ€§èƒ½ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹æœªæ¥ç ”ç©¶æœ‰ä»·å€¼çš„èµ„æºã€‚æˆ‘ä»¬çš„ HoCo æ•°æ®é›†å’Œä»£ç å°†åœ¨è¢«æ¥å—åå‘å¸ƒä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šé’ˆå¯¹è¯´è¯è€…å’Œå¬ä¼—çš„æ•´ä½“åŠ¨ä½œï¼Œåˆ†åˆ«è®­ç»ƒ VQ-VAEï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäº Transformer çš„è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºæœ‰æ•ˆè¡¨å¾ç°å®ä¸–ç•Œä¸­çš„äº¤æµåœºæ™¯ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„åŠ¨ä½œï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨åˆ†è§£å› å­åŒ–ä¸æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„ç»“åˆï¼Œä¿ƒè¿›åˆ›å»ºæ›´é€¼çœŸå’Œåè°ƒçš„åŠ¨ä½œï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œå°†äº¤æµçº³å…¥äººæœºäº¤äº’ä¸­ï¼Œæå‡ºäº†ä¸€é¡¹æ–°é¢–çš„ä»»åŠ¡ï¼Œä¸ºè¯´è¯è€…å’Œå¬ä¼—ç”Ÿæˆ 3D æ•´ä½“äººç±»åŠ¨ä½œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨æ•°æ®é›†å’Œæ¨¡å‹è®¾è®¡ä¸Šå‡åšå‡ºäº†è´¡çŒ®ã€‚å‰è€…æ–¹é¢ï¼Œæˆ‘ä»¬æä¾›äº† HoCo é€šä¿¡æ•°æ®é›†ï¼Œä»¥ä¾›æœªæ¥æ²¿ç€æ­¤ä»»åŠ¡è¿›è¡Œæ¢ç´¢ã€‚åè€…æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æˆ‘ä»¬ä»»åŠ¡é‡èº«å®šåˆ¶çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒ…å«æ–°é¢–çš„è®¾è®¡ï¼ŒåŒ…æ‹¬ 1ï¼‰ç”¨äºè§£è€¦éŸ³é¢‘ç‰¹å¾çš„åˆ†è§£ï¼Œå¢å¼ºäº†ç”Ÿæˆæ›´çœŸå®å’Œåè°ƒçš„åŠ¨ä½œï¼›2ï¼‰ç”¨äºè¡¨å¾éè¯­è¨€äº¤æµçš„é“¾å¼è‡ªå›å½’æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œä¸“æ³¨äºç”Ÿæˆè¯´è¯è€…å’Œå¬ä¼—çš„ 3D æ•´ä½“äººç±»åŠ¨ä½œï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé’ˆå¯¹è¯¥ä»»åŠ¡é‡èº«å®šåˆ¶çš„æ¨¡å‹ã€‚æ€§èƒ½ï¼šåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šå¼•å…¥äº† HoCo æ•´ä½“äº¤æµæ•°æ®é›†ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-8dd55a7f4757f4ae1f9d71880b4c6479.jpg" align="middle"><img src="https://picx.zhimg.com/v2-848d816930200060ec067527f2cd2e66.jpg" align="middle"><img src="https://picx.zhimg.com/v2-90bdfdaba5b0a3b62088d04ae352d6de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95a401074128a34e072805c4fda00e11.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  CoVoMix Advancing Zero-Shot Speech Generation for Human-like   Multi-talker Conversations</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/Diffusion%20Models/</id>
    <published>2024-05-06T10:26:38.000Z</published>
    <updated>2024-05-06T10:26:38.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition"><a href="#Defect-Image-Sample-Generation-With-Diffusion-Prior-for-Steel-Surface-Defect-Recognition" class="headerlink" title="Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition"></a>Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</h2><p><strong>Authors:Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, Zhijiang Zhang</strong></p><p>The task of steel surface defect recognition is an industrial problem with great industry values. The data insufficiency is the major challenge in training a robust defect recognition network. Existing methods have investigated to enlarge the dataset by generating samples with generative models. However, their generation quality is still limited by the insufficiency of defect image samples. To this end, we propose Stable Surface Defect Generation (StableSDG), which transfers the vast generation distribution embedded in Stable Diffusion model for steel surface defect image generation. To tackle with the distinctive distribution gap between steel surface images and generated images of the diffusion model, we propose two processes. First, we align the distribution by adapting parameters of the diffusion model, adopted both in the token embedding space and network parameter space. Besides, in the generation process, we propose image-oriented generation rather than from pure Gaussian noises. We conduct extensive experiments on steel surface defect dataset, demonstrating state-of-the-art performance on generating high-quality samples and training recognition models, and both designed processes are significant for the performance. </p><p><a href="http://arxiv.org/abs/2405.01872v1">PDF</a> </p><p><strong>Summary</strong><br>é’¢æè¡¨é¢ç¼ºé™·ç”Ÿæˆæ¨¡å‹StableSDGé€šè¿‡è¿ç§»Stable Diffusionæ¨¡å‹ç”Ÿæˆé«˜ç²¾åº¦åˆæˆå›¾åƒï¼Œæœ‰æ•ˆæå‡é’¢æè¡¨é¢ç¼ºé™·è¯†åˆ«æ¨¡å‹çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨Stable Diffusionæ¨¡å‹ç”Ÿæˆé’¢æè¡¨é¢ç¼ºé™·å›¾åƒï¼Œæ‰©å……è®­ç»ƒæ•°æ®é›†ã€‚</li><li>é€šè¿‡è°ƒæ•´æ¨¡å‹å‚æ•°å’ŒåµŒå…¥ç©ºé—´ï¼Œå¼¥åˆç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚ã€‚</li><li>é‡‡ç”¨å›¾åƒå¼•å¯¼ç”Ÿæˆæ–¹å¼ï¼Œè€Œéçº¯é«˜æ–¯å™ªå£°ç”Ÿæˆã€‚</li><li>æå‡ºä¸¤ç§å…³é”®è¿‡ç¨‹ï¼šåˆ†å¸ƒå¯¹é½å’Œå›¾åƒå¼•å¯¼ç”Ÿæˆã€‚</li><li>åœ¨é’¢æè¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜StableSDGåœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒå’Œè®­ç»ƒè¯†åˆ«æ¨¡å‹æ–¹é¢å‡è¾¾åˆ°æœ€å…ˆè¿›æ°´å¹³ã€‚</li><li>ä¸¤ç§æå‡ºçš„å…³é”®è¿‡ç¨‹å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li><li>StableSDGæœ‰æ•ˆè§£å†³æ•°æ®ä¸è¶³é—®é¢˜ï¼Œæå‡é’¢æè¡¨é¢ç¼ºé™·è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>ç¼ºé™·å›¾åƒæ ·æœ¬ç”Ÿæˆä¸æ‰©æ•£</p></li><li><p>Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang</p></li><li><p>ä¸Šæµ·å¤§å­¦ä¼ ä¿¡å­¦é™¢</p></li><li><p>Text-to-image diffusion, data expansion, deep learning, textual inversion, low-rank adaptation, defect image generation, steel surface defect recognition</p></li><li><p>https://arxiv.org/abs/2405.01872 , Github:None</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šé’¢è¡¨é¢ç¼ºé™·è¯†åˆ«æ˜¯å·¥ä¸šç•Œå…·æœ‰å·¨å¤§äº§ä¸šä»·å€¼çš„ä¸€é¡¹ä»»åŠ¡ã€‚æ•°æ®ä¸è¶³æ˜¯è®­ç»ƒé²æ£’ç¼ºé™·è¯†åˆ«ç½‘ç»œçš„ä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å·²ç»ç ”ç©¶äº†é€šè¿‡ç”Ÿæˆæ¨¡å‹ç”Ÿæˆæ ·æœ¬ä»¥æ‰©å¤§æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ç”Ÿæˆè´¨é‡ä»ç„¶å—åˆ°ç¼ºé™·å›¾åƒæ ·æœ¬ä¸è¶³çš„é™åˆ¶ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬ï¼šSDGANã€Defect-GANã€transP2Pã€‚è¿™äº›æ–¹æ³•ä»å¤´å¼€å§‹è®­ç»ƒç”Ÿæˆæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå½“å›¾åƒæ ·æœ¬ä¸è¶³æ—¶ï¼Œé€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆæ ·æœ¬ä¸­å‡ºç°ä¸éœ€è¦çš„æ¨¡å¼ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å®šçš„è¡¨é¢ç¼ºé™·ç”Ÿæˆï¼ˆStableSDGï¼‰æ–¹æ³•ï¼Œå°†Stable Diffusionæ¨¡å‹ä¸­åµŒå…¥çš„å·¨å¤§ç”Ÿæˆåˆ†å¸ƒè½¬ç§»ç”¨äºé’¢è¡¨é¢ç¼ºé™·å›¾åƒç”Ÿæˆã€‚ä¸ºäº†è§£å†³é’¢è¡¨é¢å›¾åƒå’Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒä¹‹é—´çš„ç‹¬ç‰¹åˆ†å¸ƒå·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„å‚æ•°æ¥å¯¹é½åˆ†å¸ƒï¼Œæ—¢é‡‡ç”¨æ ‡è®°åµŒå…¥ç©ºé—´ï¼Œä¹Ÿé‡‡ç”¨ç½‘ç»œå‚æ•°ç©ºé—´ã€‚æ­¤å¤–ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å›¾åƒçš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯ä»çº¯é«˜æ–¯å™ªå£°ä¸­ç”Ÿæˆã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæˆ‘ä»¬åœ¨é’¢è¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå±•ç¤ºäº†åœ¨ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬å’Œè®­ç»ƒè¯†åˆ«æ¨¡å‹æ–¹é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶ä¸”ä¸¤ä¸ªè®¾è®¡è¿‡ç¨‹å¯¹æ€§èƒ½éƒ½å¾ˆé‡è¦ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):Stable Diffusionæ¨¡å‹[22]ä½œä¸ºæ‰©æ•£å…ˆéªŒï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œæ‰©æ•£ï¼Œè€Œä¸æ˜¯å›¾åƒç©ºé—´ï¼Œå¹¿æ³›ç”¨äºå›¾åƒç”Ÿæˆä»»åŠ¡[26]â€“[30]ã€‚            (2):æå‡ºStableSDGæ–¹æ³•ï¼Œç”±ä¸¤ä¸ªè¿‡ç¨‹ç»„æˆï¼Œç”¨äºç”Ÿæˆæ¯ç§ç¼ºé™·ç±»åˆ«çš„å›¾åƒã€‚            (3):é€šè¿‡è¿­ä»£è´¨é‡è¯„ä¼°ï¼Œè°ƒæ•´è¶…å‚æ•°ä»¥å®ç°æœ€ä½³å›¾åƒç”Ÿæˆã€‚            (4):ä½¿ç”¨æœ€ä½³è¶…å‚æ•°ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒä»¥æ‰©å±•æ•°æ®é›†ã€‚            (5):å°†æ¯ç§ç¼ºé™·ç±»åˆ«çš„ç”Ÿæˆå›¾åƒä¸çœŸå®å›¾åƒä¸€èµ·æ”¶é›†èµ·æ¥ï¼Œç”¨äºè®­ç»ƒç¼ºé™·è¯†åˆ«æ¨¡å‹ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºçš„StableSDGæ–¹æ³•å°†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åº”ç”¨äºé’¢è¡¨é¢ç¼ºé™·å›¾åƒç”Ÿæˆï¼Œæœ‰æ•ˆè§£å†³äº†é’¢è¡¨é¢ç¼ºé™·æ•°æ®é›†ä¸è¶³çš„é—®é¢˜ï¼Œä¸ºç¼ºé™·è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæä¾›äº†é«˜è´¨é‡çš„æ ·æœ¬ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šStableSDGæ–¹æ³•åœ¨ç”Ÿæˆå™¨è‡ªé€‚åº”è¿‡ç¨‹ä¸­ï¼ŒåŒæ—¶åœ¨tokenåµŒå…¥ç©ºé—´å’Œç½‘ç»œå‚æ•°ç©ºé—´è¿›è¡Œè‡ªé€‚åº”å’Œä¿®æ”¹ï¼Œå¹¶åœ¨ç”Ÿæˆæ•°æ®æ—¶ä»å›¾åƒå¯¼å‘åˆå§‹åŒ–ç”Ÿæˆæ ·æœ¬ï¼Œè€Œä¸æ˜¯ä»çº¯é«˜æ–¯å™ªå£°å¼€å§‹ã€‚</p><p>æ€§èƒ½ï¼šStableSDGæ–¹æ³•åœ¨NEUå’ŒCCBSDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„ç¼ºé™·å›¾åƒï¼Œå¤§å¤§æé«˜äº†è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p>å·¥ä½œé‡ï¼šStableSDGæ–¹æ³•çš„å·¥ä½œé‡ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆå™¨è‡ªé€‚åº”å’Œå›¾åƒç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œéœ€è¦å¯¹è¶…å‚æ•°è¿›è¡Œè¿­ä»£è´¨é‡è¯„ä¼°å’Œè°ƒæ•´ï¼Œä»¥è·å¾—æœ€ä½³çš„å›¾åƒç”Ÿæˆæ•ˆæœã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-bd44eeacb7308bdc2e6594f5b84b63b5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a49aa1e7511a9ee599c2b42ba68cfb6d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e589bb5b223b5f1a03944e68feabbcd1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b4b7e153a9792e0731936f44ad770e5f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4222a03fe5309bbcdc62d8769e94eb0c.jpg" align="middle"></details><h2 id="Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning"><a href="#Long-Tail-Image-Generation-Through-Feature-Space-Augmentation-and-Iterated-Learning" class="headerlink" title="Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning"></a>Long Tail Image Generation Through Feature Space Augmentation and   Iterated Learning</h2><p><strong>Authors:Rafael Elberg, Denis Parra, Mircea Petrache</strong></p><p>Image and multimodal machine learning tasks are very challenging to solve in the case of poorly distributed data. In particular, data availability and privacy restrictions exacerbate these hurdles in the medical domain. The state of the art in image generation quality is held by Latent Diffusion models, making them prime candidates for tackling this problem. However, a few key issues still need to be solved, such as the difficulty in generating data from under-represented classes and a slow inference process. To mitigate these issues, we propose a new method for image augmentation in long-tailed data based on leveraging the rich latent space of pre-trained Stable Diffusion Models. We create a modified separable latent space to mix head and tail class examples. We build this space via Iterated Learning of underlying sparsified embeddings, which we apply to task-specific saliency maps via a K-NN approach. Code is available at <a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a> </p><p><a href="http://arxiv.org/abs/2405.01705v1">PDF</a> </p><p><strong>Summary</strong><br>é•¿å°¾æ•°æ®å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œç¼“è§£ç”Ÿæˆè´¨é‡å·®å’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å›¾åƒå’Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨æ•°æ®åˆ†å¸ƒä¸è¶³çš„æƒ…å†µä¸‹æå…·æŒ‘æˆ˜æ€§ã€‚</li><li>åŒ»å­¦é¢†åŸŸçš„å›¾åƒç”Ÿæˆé¢ä¸´æ•°æ®è·å–å’Œéšç§é™åˆ¶çš„éšœç¢ã€‚</li><li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šå¤„äºé¢†å…ˆåœ°ä½ã€‚</li><li>æ•°æ®ç”Ÿæˆä¸å¹³è¡¡å’Œæ¨ç†é€Ÿåº¦æ…¢æ˜¯äºŸå¾…è§£å†³çš„é—®é¢˜ã€‚</li><li>æ–¹æ³•ç»“åˆé¢„è®­ç»ƒç¨³å®šæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ï¼Œè¿›è¡Œå›¾åƒå¢å¼ºã€‚</li><li>æ„å»ºå¯åˆ†ç¦»çš„æ½œåœ¨ç©ºé—´ï¼Œæ··åˆå¤´éƒ¨å’Œå°¾éƒ¨ç±»åˆ«çš„ç¤ºä¾‹ã€‚</li><li>é€šè¿‡è¿­ä»£å­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œæ„å»ºç©ºé—´ï¼Œå¹¶é€šè¿‡ K-NN æ–¹æ³•åº”ç”¨äºç‰¹å®šä»»åŠ¡çš„æ˜¾ç€æ€§å›¾ã€‚</li><li>ä»£ç å·²å¼€æºï¼š<a href="https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning">https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šç‰¹å¾ç©ºé—´å¢å¼ºå’Œè¿­ä»£å­¦ä¹ çš„é•¿å°¾å›¾åƒç”Ÿæˆ</p></li><li><p>ä½œè€…ï¼šRafael Elbergã€Denis Parraã€Mircea Petrache</p></li><li><p>éš¶å±ï¼šæ™ºåˆ©å¤©ä¸»æ•™å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šé•¿å°¾æ•°æ®ã€å›¾åƒç”Ÿæˆã€ç‰¹å¾ç©ºé—´å¢å¼ºã€è¿­ä»£å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01705   Github ä»£ç é“¾æ¥ï¼šhttps://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šå›¾åƒå’Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ ä»»åŠ¡åœ¨æ•°æ®åˆ†å¸ƒä¸å‡åŒ€çš„æƒ…å†µä¸‹éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œæ•°æ®å¯ç”¨æ€§å’Œéšç§é™åˆ¶åŠ å‰§äº†è¿™äº›éšœç¢ã€‚æ½œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢å¤„äºæœ€å…ˆè¿›æ°´å¹³ï¼Œä½¿å…¶æˆä¸ºè§£å†³æ­¤é—®é¢˜çš„ç†æƒ³å€™é€‰è€…ã€‚ç„¶è€Œï¼Œä»éœ€è¦è§£å†³å‡ ä¸ªå…³é”®é—®é¢˜ï¼Œä¾‹å¦‚éš¾ä»¥ç”Ÿæˆæ¥è‡ªä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«çš„å›¾åƒä»¥åŠæ¨ç†è¿‡ç¨‹ç¼“æ…¢ã€‚</p><p>(2)ï¼šè¿‡å»çš„æ–¹æ³•åŒ…æ‹¬é‡é‡‡æ ·å’Œæ•°æ®å¢å¼ºã€‚é‡é‡‡æ ·æŠ€æœ¯åœ¨ä¸€äº›é•¿å°¾é—®é¢˜ä¸­å–å¾—äº†ç›¸å¯¹æˆåŠŸï¼Œä½†å¯èƒ½ä¼šç»™ä¸‹æ¸¸ä»»åŠ¡å¼•å…¥ä¸å¿…è¦çš„åå·®ï¼Œå¹¶ä¸”ç»å¸¸å¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ•°æ®å¢å¼ºæ˜¯è§£å†³è¿™äº›é—®é¢˜çš„è‡ªç„¶å“åº”ã€‚å®ƒä»£è¡¨äº†ä¸€ä¸ªè“¬å‹ƒå‘å±•çš„ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬å‡ ä¸ªä¸åŒçš„ç®—æ³•ç³»åˆ—ï¼Œä¾‹å¦‚å‡ ä½•å˜æ¢ï¼ˆæ—‹è½¬ã€ç¼©æ”¾ã€è£å‰ªç­‰ï¼‰ã€åˆæˆæ ·æœ¬åˆ›å»ºã€åŸºäºæ··åˆçš„æ–¹æ³•ã€åŸºäºåŸŸè½¬æ¢çš„æ–¹æ³•å’Œç”Ÿæˆæ–¹æ³•ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ“çºµæ¥è‡ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºï¼Œä»è€Œç”Ÿæˆæ–°å›¾åƒæ¥å¢å¼ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ã€‚é€šè¿‡æ¿€æ´»å›¾é€‰æ‹©æ•°æ®çš„ç‰¹å®šç‰¹å¾ï¼Œç„¶åå°†è¿™äº›ç‰¹å¾ç»„åˆèµ·æ¥ï¼Œç”Ÿæˆä¸å±äºé•¿å°¾ç±»çš„å®é™…æ•°æ®ä¸­çš„å›¾åƒç›¸ä¼¼çš„å›¾åƒã€‚</p><p>(4)ï¼šåœ¨æœ¬æ–‡çš„æ–¹æ³•ä¸­ï¼Œæ½œåœ¨ç©ºé—´è¡¨ç¤ºçš„ç»„åˆç”±äºç‰¹å¾åå¤„ç†ä¹‹é—´çš„å¹²æ‰°ç°è±¡è€Œéš¾ä»¥é€šè¿‡æœ´ç´ çš„æ–¹æ³•æ‰§è¡Œã€‚æœ¬æ–‡å°†æ­¤é—®é¢˜ä½œä¸ºåˆæˆæ³›åŒ–é—®é¢˜ï¼Œå¹¶å°†è¿­ä»£å­¦ä¹ ï¼ˆILï¼‰æ¡†æ¶ä¸ç¨€ç–åµŒå…¥åº”ç”¨äºç›®æ ‡æ•°æ®å¢å¼ºæ¡†æ¶ã€‚IL çš„ä¸»è¦çµæ„Ÿæ¥è‡ªæ–‡åŒ–è¿›åŒ–æ¨¡å‹ï¼Œå…¶ä¸­æ•™å¸ˆ-å­¦ç”Ÿäº¤äº’çš„è¿­ä»£é¼“åŠ±æœ‰ç”¨çš„å‹ç¼©å’Œå½¢æˆé€‚åº”ä»»åŠ¡çš„â€œå…±äº«è¯­è¨€â€ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ€è¿‘åœ¨ä½¿ç”¨ç¨€ç–çŠ¶æ€ç©ºé—´æ—¶è·å¾—äº†ä¸åˆæˆä¸åŒç‰¹å¾ç›¸å…³çš„æœ‰åˆ©ç»“æœã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æ“çºµæ¥è‡ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒçš„æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ¥ç”Ÿæˆæ–°å›¾åƒï¼Œä»¥å¢å¼ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šè¿­ä»£è®­ç»ƒã€ç±»åˆ«æ¿€æ´»å›¾ç”Ÿæˆå’Œæ¨ç†ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨è¿­ä»£è®­ç»ƒé˜¶æ®µï¼Œå­¦ä¹ äº†ä¸€ä¸ªä»æ‰©æ•£æ½œåœ¨ç©ºé—´åˆ°ç¨€ç–é«˜ç»´è¡¨ç¤ºçš„è½¬æ¢ï¼ŒåŒæ—¶è®­ç»ƒäº†ä¸€ä¸ªå·ç§¯åˆ†ç±»å™¨ç”¨äºè¯¥ç©ºé—´ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç±»åˆ«æ¿€æ´»å›¾ç”Ÿæˆé˜¶æ®µï¼Œä½¿ç”¨åˆ†ç±»å™¨ç”Ÿæˆæ¯ä¸ªç±»çš„ç®€å•å¯è§£é‡Šæ¿€æ´»å›¾ï¼Œä»¥é€‰æ‹©ä¸åˆ†ç±»ä¸ºè¯¥ç±»ç›¸å…³çš„æˆ–ä¸ç›¸å…³çš„åæ ‡ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œä»å°¾éƒ¨ç±»ç”Ÿæˆæ–°æ ·æœ¬ï¼Œé€šè¿‡å°†ç‰¹å®šå°¾éƒ¨ç±»ç¤ºä¾‹çš„ç±»ç‰¹å®šç‰¹å¾ä¸æœ€é«˜æ··æ·†å¤´éƒ¨ç±»çš„ç±»é€šç”¨ç‰¹å¾èåˆï¼Œä½¿ç”¨æ©ç åˆ›å»ºèåˆå‘é‡ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€ç»„åˆå­¦ä¹ å’Œæ˜¾ç€æ€§æ–¹æ³•ï¼Œä¸ºé•¿å°¾æ•°æ®é›†ç”Ÿæˆæ•°æ®å¹¶å¢å¼ºæ•°æ®ï¼Œä»è€Œä¸ºä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«ç”Ÿæˆæ–°ç¤ºä¾‹ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€ç»„åˆå­¦ä¹ å’Œæ˜¾è‘—æ€§æ–¹æ³•çš„æ•°æ®å¢å¼ºå’Œæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼›æ€§èƒ½ï¼šåœ¨åŒ»å­¦é¢†åŸŸçš„å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸­ä½¿ç”¨ MIMIC-CXR-LT [13, 16] çš„ä¸€ä¸ªå°å‹å­é›†ï¼Œåœ¨å›¾åƒç”Ÿæˆå’Œæ•°æ®å¢å¼ºæ–¹é¢å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¯¹è½¬æ¢å’Œåˆ†ç±»å™¨è¿›è¡Œè¿­ä»£è®­ç»ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-0d188950e8539013f5d1dbb852ac0cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c5b473a90cffec494f2607efb08a6c2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-39ea380f716fabbe74492f3835a23773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d004602232a458ce5dd218668a87e87.jpg" align="middle"></details><h2 id="LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing"><a href="#LocInv-Localization-aware-Inversion-for-Text-Guided-Image-Editing" class="headerlink" title="LocInv: Localization-aware Inversion for Text-Guided Image Editing"></a>LocInv: Localization-aware Inversion for Text-Guided Image Editing</h2><p><strong>Authors:Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</strong></p><p>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at <a href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a> </p><p><a href="http://arxiv.org/abs/2405.01496v1">PDF</a> Accepted by CVPR 2024 Workshop AI4CC</p><p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶åˆ©ç”¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä½†ç°æœ‰ç¼–è¾‘æŠ€æœ¯å®¹æ˜“ä¿®æ”¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸï¼Œä¸»è¦æ˜¯å› ä¸ºäº¤å‰æ³¨æ„åŠ›å›¾ä¸å‡†ç¡®ã€‚æˆ‘ä»¬é€šè¿‡åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†æ”¹è¿›æ‰©æ•£è¿‡ç¨‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå®ç°äº†ç‰¹å®šå¯¹è±¡çš„ç»†ç²’åº¦å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶é˜²æ­¢å¯¹å…¶ä»–åŒºåŸŸè¿›è¡Œéå¿…è¦çš„æ›´æ”¹ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°æœ‰å›¾åƒç¼–è¾‘æŠ€æœ¯å®¹æ˜“ä¿®æ”¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸã€‚</li><li>æˆ‘ä»¬æå‡ºäº†åˆ©ç”¨åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†ä½œä¸ºé¢å¤–çš„å®šä½å…ˆéªŒæ¥æ”¹è¿›æ‰©æ•£è¿‡ç¨‹ä¸­çš„äº¤å‰æ³¨æ„åŠ›å›¾ã€‚</li><li>æˆ‘ä»¬é€šè¿‡æ›´æ–°æ–‡æœ¬è¾“å…¥ä¸­åè¯å¯¹åº”çš„ç¬¦å·ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ç´§å¯†å¯¹é½æ–‡æœ¬æç¤ºä¸­çš„æ­£ç¡®åè¯å’Œå½¢å®¹è¯ã€‚</li><li>æˆ‘ä»¬åŸºäºå…¬å¼€çš„Stable Diffusionå®ç°äº†LocInvæ–¹æ³•ï¼Œå¹¶åœ¨COCOæ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li><li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</li><li>è¯¥æ–¹æ³•çš„ä»£ç å°†åœ¨<a href="https://github.com/wangkai930418/DPLä¸Šå…¬å¸ƒã€‚">https://github.com/wangkai930418/DPLä¸Šå…¬å¸ƒã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šå®šä½æ„ŸçŸ¥åæ¼”ï¼šæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘</p></li><li><p>ä½œè€…ï¼šChuanming Tangã€Kai Wangã€Fei Yangã€Joost van de Weijer</p></li><li><p>å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒç¼–è¾‘ã€å®šä½æ„ŸçŸ¥ã€äº¤å‰æ³¨æ„åŠ›</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.01496Githubï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬æç¤ºä¸‹å±•ç¤ºäº†æ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ç ”ç©¶æ—¨åœ¨é€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºæ¥èµ‹äºˆç”¨æˆ·æ“çºµç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå·²æœ‰æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å®¹æ˜“å¯¹è¶…å‡ºç›®æ ‡åŒºåŸŸçš„æ— æ„åŒºåŸŸè¿›è¡Œç¼–è¾‘ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºäº¤å‰æ³¨æ„åŠ›å›¾çš„ä¸å‡†ç¡®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå®šä½æ„ŸçŸ¥åæ¼”ï¼ˆLocInvï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†å‰²å›¾æˆ–è¾¹ç•Œæ¡†ä½œä¸ºé¢å¤–çš„å®šä½å…ˆéªŒï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹çš„å»å™ªé˜¶æ®µä¼˜åŒ–äº¤å‰æ³¨æ„åŠ›å›¾ã€‚é€šè¿‡åŠ¨æ€æ›´æ–°æ–‡æœ¬è¾“å…¥ä¸­ä¸åè¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­æ­£ç¡®çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ã€‚åŸºäºæ­¤æŠ€æœ¯ï¼Œæˆ‘ä»¬å®ç°äº†å¯¹ç‰¹å®šå¯¹è±¡çš„ç»†ç²’åº¦å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶é˜²æ­¢å¯¹å…¶ä»–åŒºåŸŸè¿›è¡Œä¸å¿…è¦çš„æ›´æ”¹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåŸºäºå…¬å¼€çš„Stable Diffusionï¼Œæˆ‘ä»¬å¯¹LocInvæ–¹æ³•åœ¨COCOæ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚è¿™äº›ç»“æœè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥å®ç°å…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šä½¿ç”¨ Stable Diffusion v1.4 ä½œä¸ºåŸºç¡€æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±ç¼–ç å™¨ã€è§£ç å™¨å’Œæ‰©æ•£æ¨¡å‹ç»„æˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ DDIM åæ¼”ç®—æ³•ï¼Œä»éšæœºå™ªå£° zT æ‰¾åˆ°åˆå§‹å™ªå£°ï¼Œé€šè¿‡é‡‡æ ·é‡å»ºè¾“å…¥æ½œåœ¨ä»£ç  z0ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨æ— æ–‡æœ¬åæ¼” (NTI) ä¼˜åŒ–æ— æ–‡æœ¬åµŒå…¥ âˆ…tï¼Œä»¥è¿‘ä¼¼ DDIM è½¨è¿¹ {zt}T 0ï¼Œä»è€Œç¼–è¾‘çœŸå®å›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºåŠ¨æ€æç¤ºå­¦ä¹  (DPL) æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒï¼Œæ›´æ–°æ–‡æœ¬æç¤º P ä¸­çš„åè¯å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šè®¾è®¡ç›¸ä¼¼åº¦æŸå¤±å’Œé‡å æŸå¤±ï¼Œä¼˜åŒ–åµŒå…¥å‘é‡ Vtï¼Œä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸å®šä½å…ˆéªŒ S ä¹‹é—´ç›¸ä¼¼åº¦é«˜ã€é‡å åº¦é«˜ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨æ¸è¿›ä¼˜åŒ–æœºåˆ¶ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥ t å¤„å¼ºåˆ¶æ‰€æœ‰æŸå¤±è¾¾åˆ°é¢„å®šä¹‰é˜ˆå€¼ï¼Œé¿å…äº¤å‰æ³¨æ„åŠ›å›¾è¿‡æ‹Ÿåˆã€‚</p><p>ï¼ˆ7ï¼‰ï¼šç»“åˆ NTI å­¦ä¹ ä¸€ç»„æ— æ–‡æœ¬åµŒå…¥ âˆ…tï¼Œä¸å¯å­¦ä¹ çš„å•è¯åµŒå…¥ Vt å…±åŒç²¾ç¡®å®šä½å¯¹è±¡å¹¶é‡å»ºåŸå§‹å›¾åƒã€‚</p><p>ï¼ˆ8ï¼‰ï¼šæå‡ºå½¢å®¹è¯ç»‘å®šæœºåˆ¶ï¼Œé€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºä¸­çš„å½¢å®¹è¯æ¥æ”¹å˜å¯¹è±¡çš„å¤–è§‚ã€‚</p><ol><li>Conclusion:</li></ol><p>(1): æœ¬æ–‡æå‡ºçš„ LocInv æ–¹æ³•è§£å†³äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å›¾åƒç¼–è¾‘ä¸­äº¤å‰æ³¨æ„åŠ›å›¾æ³„æ¼çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå…ˆéªŒï¼Œæ›´æ–°æç¤ºä¸­æ¯ä¸ªåè¯å•è¯çš„åŠ¨æ€æ ‡è®°ã€‚ç”±æ­¤äº§ç”Ÿçš„äº¤å‰æ³¨æ„åŠ›å›¾è¾ƒå°‘å—åˆ°äº¤å‰æ³¨æ„åŠ›å›¾æ³„æ¼çš„å½±å“ã€‚å› æ­¤ï¼Œè¿™äº›å¤§å¤§æ”¹è¿›çš„äº¤å‰æ³¨æ„åŠ›å›¾æå¤§åœ°æ”¹å–„äº†æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„ç»“æœã€‚å®éªŒç»“æœè¯å®ï¼ŒLocInv è·å¾—äº†æ›´å¥½çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šå¯¹è±¡åœºæ™¯ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥å°†å½¢å®¹è¯å•è¯ç»‘å®šåˆ°å®ƒä»¬å¯¹åº”ã®åè¯ä¸Šï¼Œä»è€Œå¾—åˆ°å½¢å®¹è¯çš„å‡†ç¡®äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œå¹¶å…è®¸å¯¹å±æ€§è¿›è¡Œç¼–è¾‘ï¼Œè¿™æ˜¯ä»¥å‰åœ¨æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ä¸­å°šæœªå……åˆ†æ¢ç´¢çš„ã€‚</p><p>(2): åˆ›æ–°ç‚¹ï¼šæå‡ºå®šä½æ„ŸçŸ¥åæ¼”æ–¹æ³•ï¼Œåˆ©ç”¨åˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒï¼Œæ›´æ–°æ–‡æœ¬æç¤ºä¸­çš„åè¯å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œè¿«ä½¿äº¤å‰æ³¨æ„åŠ›å›¾ä¸æ–‡æœ¬æç¤ºä¸­çš„åè¯å’Œå½¢å®¹è¯ç´§å¯†å¯¹é½ï¼›æå‡ºå½¢å®¹è¯ç»‘å®šæœºåˆ¶ï¼Œé€šè¿‡æ”¹å˜æ–‡æœ¬æç¤ºä¸­çš„å½¢å®¹è¯æ¥æ”¹å˜å¯¹è±¡çš„å¤–è§‚ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ COCO æ•°æ®é›†çš„å­é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œåœ¨å®šé‡å’Œå®šæ€§ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥å®ç°å…¶ç›®æ ‡ã€‚</p><p>å·¥ä½œé‡ï¼šæ–¹æ³•å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦ç»“åˆ Stable Diffusion æ¨¡å‹å’Œ NTI åæ¼”ç®—æ³•ï¼Œä»¥åŠåˆ†å‰²å›¾æˆ–æ£€æµ‹æ¡†ä½œä¸ºå®šä½å…ˆéªŒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-501b84f66a4fdce982c4d560d6ed2c6e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d7f11972c7c9876389df6092b426ca67.jpg" align="middle"><img src="https://picx.zhimg.com/v2-566375be0266ca83b73d642319fcc82b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2650ddd9595d88f0a5238c88b753e8e6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-997ccc837824e0d3f900484e2641fab6.jpg" align="middle"></details><h2 id="Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers"><a href="#Guided-Conditional-Diffusion-Classifier-ConDiff-for-Enhanced-Prediction-of-Infection-in-Diabetic-Foot-Ulcers" class="headerlink" title="Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers"></a>Guided Conditional Diffusion Classifier (ConDiff) for Enhanced   Prediction of Infection in Diabetic Foot Ulcers</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</strong></p><p>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes. </p><p><a href="http://arxiv.org/abs/2405.00858v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨åˆæˆå›¾åƒæ„ŸæŸ“çŠ¶æ€æŒ‡å¯¼åˆ†ç±»é‰´åˆ«ç³–å°¿ç—…è¶³æºƒç–¡æ„ŸæŸ“</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°çš„ç³–å°¿ç—…è¶³æºƒç–¡æ„ŸæŸ“æ£€æµ‹æ¨¡å‹ï¼Œå³æ¡ä»¶å¼•å¯¼æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰</li><li>ConDiffç»“åˆäº†å¼•å¯¼å›¾åƒåˆæˆã€å»å™ªæ‰©æ•£æ¨¡å‹å’ŒåŸºäºè·ç¦»çš„åˆ†ç±»</li><li>é€šè¿‡åœ¨å¼•å¯¼å›¾åƒä¸­æ³¨å…¥é«˜æ–¯å™ªå£°å¹¶é€šè¿‡æ¡ä»¶åŒ–æ„ŸæŸ“çŠ¶æ€è¿›è¡Œé€†æ‰©æ•£å»å™ªåˆæˆå›¾åƒ</li><li>åŸºäºåˆæˆå›¾åƒä¸åŸå§‹å¼•å¯¼å›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å°æ¬§å‡ é‡Œå¾—è·ç¦»è¿›è¡Œæ„ŸæŸ“åˆ†ç±»</li><li>ä½¿ç”¨å…ƒç»„æŸå¤±å‡½æ•°åœ¨åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ä¸­å‡å°‘è¿‡æ‹Ÿåˆ</li><li>ConDiffåœ¨å‡†ç¡®æ€§å’ŒF1-scoreä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹</li><li>ConDiffå¼€åˆ›äº†ç”Ÿæˆå¼åˆ¤åˆ«æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç²¾ç»†åˆ†æä¸­çš„åº”ç”¨ï¼Œä¸ºæ”¹å–„æ‚£è€…é¢„åæä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: å¯¼å‘æ¡ä»¶æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰</p></li><li><p>Authors: Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</p></li><li><p>Affiliation: Worcesterç†å·¥å­¦é™¢</p></li><li><p>Keywords: ç³–å°¿ç—…è¶³æºƒç–¡ï¼Œæ‰©æ•£æ¨¡å‹ï¼ŒåŸºäºè·ç¦»çš„å›¾åƒåˆ†ç±»ï¼Œç”Ÿæˆæ¨¡å‹ï¼Œä¼¤å£æ„ŸæŸ“</p></li><li><p>Urls: Paper: xxx, Github: None</p></li><li><p>Summary:</p><p>ï¼ˆ1ï¼‰ï¼šç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ„ŸæŸ“æ˜¯å¯¼è‡´æˆªè‚¢å’Œä¸¥é‡å¹¶å‘ç—‡çš„ä¸»è¦åŸå› ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„DFUæ„ŸæŸ“æ£€æµ‹æ–¹æ³•å­˜åœ¨å‡†ç¡®ç‡ä½çš„é—®é¢˜ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ConDiffæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¼•å¯¼å›¾åƒåˆæˆã€å»å™ªæ‰©æ•£æ¨¡å‹å’ŒåŸºäºè·ç¦»çš„åˆ†ç±»ï¼Œé€šè¿‡ç”Ÿæˆå¼•å¯¼æ¡ä»¶åˆæˆå›¾åƒå¹¶è®¡ç®—åˆæˆå›¾åƒä¸åŸå§‹å›¾åƒä¹‹é—´çš„æœ€å°æ¬§å‡ é‡Œå¾—è·ç¦»æ¥å¯¹æ„ŸæŸ“è¿›è¡Œåˆ†ç±»ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šConDiffåœ¨DFUæ„ŸæŸ“æ•°æ®é›†ä¸Šå–å¾—äº†83%çš„å‡†ç¡®ç‡å’Œ0.858çš„F1åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æ–¹æ³•è‡³å°‘3%ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜DFUæ„ŸæŸ“è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šConDiff æ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šï¼ˆ1ï¼‰å¼•å¯¼æ‰©æ•£ï¼Œå³å‘ DFU å›¾åƒæ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œç„¶åæ ¹æ®æ„ŸæŸ“çŠ¶æ€ä»å™ªå£°æ‰°åŠ¨å›¾åƒä¸­é€æ­¥å»é™¤å™ªå£°ï¼Œä»¥åˆæˆæ¡ä»¶å›¾åƒï¼›ï¼ˆ2ï¼‰åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ï¼Œå³æ ¹æ®åŸå§‹å›¾åƒå’Œåˆæˆå›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å° L2 è·ç¦»é¢„æµ‹è¾“å…¥å›¾åƒçš„æ ‡ç­¾ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šConDiff åˆ©ç”¨æ¡ä»¶å¼•å¯¼å›¾åƒç¼–è¾‘ä¸ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å‘è¾“å…¥å›¾åƒæ³¨å…¥ç‰¹å®šå¼ºåº¦çš„ Gaussian å™ªå£°ï¼Œå¹¶ä½¿ç”¨åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥ä»å™ªå£°æ‰°åŠ¨è¾“å…¥å›¾åƒä¸­å»é™¤å™ªå£°æ¥ç”Ÿæˆæ–°å›¾åƒã€‚</p><p>ï¼ˆ3ï¼‰ï¼šConDiff çš„æ‰©æ•£è¿‡ç¨‹ä»¥ä¼¤å£çš„çŠ¶å†µï¼ˆæ— æ„ŸæŸ“ï¼ˆy1ï¼‰æˆ–æ„ŸæŸ“ï¼ˆy2ï¼‰ï¼‰ä¸ºæ¡ä»¶ï¼Œåˆ›å»ºåæ˜ è¿™äº›çŠ¶æ€çš„åˆæˆå›¾åƒã€‚ä¸€ä¸ªå…³é”®ç‚¹æ˜¯ ConDiff èƒ½å¤Ÿé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„ L2 è·ç¦»åˆ†ç±»å™¨è¯†åˆ«å’Œå­¦ä¹ æ¡ä»¶ç”Ÿæˆå›¾åƒ Ë†xy 0 å’ŒåŸå§‹ä¼¤å£å›¾åƒ x0 ä¹‹é—´è¡¨ç¤ºçš„ç›¸ä¼¼æ€§ã€‚äº§ç”Ÿä¸åŸå§‹å›¾åƒæœ€ç›¸ä¼¼çš„åˆæˆå›¾åƒçš„æ¡ä»¶è¢«é€‰ä½œé¢„æµ‹æ ‡ç­¾ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä¸æœ€å°åŒ–äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°çš„ä¼ ç»Ÿç›‘ç£åˆ†ç±»æŠ€æœ¯ä¸åŒï¼ŒConDiff é€šè¿‡åˆ©ç”¨ä¸‰å…ƒæŸå¤±å‡½æ•°æ¥å‡è½»è¿‡æ‹Ÿåˆï¼Œä»¥å¢åŠ éç›¸ä¼¼å›¾åƒå¯¹ä¹‹é—´çš„è·ç¦»å¹¶å‡å°‘ç›¸ä¼¼å›¾åƒå¯¹ä¹‹é—´çš„è·ç¦»ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šæœ¬ç ”ç©¶åˆ©ç”¨ Goyal ç­‰äººæä¾›çš„ DFU æ„ŸæŸ“æ•°æ®é›†ï¼ˆè§è¡¨ Iï¼‰ã€‚ä½†æ˜¯ï¼Œä¸ºäº†æ¶ˆé™¤è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ•°æ®æ³„æ¼ï¼Œæˆ‘ä»¬æ”¹è¿›äº†æ•°æ®é›†åˆ›å»ºå’Œæ‹†åˆ†ç­–ç•¥ã€‚ä½¿ç”¨åŸºäºä¸»é¢˜çš„æ‹†åˆ†ï¼Œä»…ä¸ºæ¯ä¸ªä¸»é¢˜ä½¿ç”¨ç¬¬äºŒä¸ªæ”¾å¤§è‡ªç„¶å¢å¼ºå›¾åƒï¼ˆå‚è§å›¾ 1ï¼‰ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šConDiff æ¡†æ¶çš„ä¸»è¦è´¡çŒ®æ˜¯ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æå‡ºäº† Guided Conditional Diffusion Classifierï¼ˆConDiffï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆ†ç±»å—æ„ŸæŸ“ä¼¤å£å›¾åƒçš„é›†æˆç«¯åˆ°ç«¯æ¡†æ¶ã€‚ConDiff æ¡†æ¶æœ‰ 2 ä¸ªä¸»è¦éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰å¼•å¯¼æ‰©æ•£ï¼Œå³å‘ DFU å›¾åƒæ³¨å…¥é«˜æ–¯å™ªå£°ï¼Œç„¶åæ ¹æ®æ„ŸæŸ“çŠ¶æ€ä»å™ªå£°æ‰°åŠ¨å›¾åƒä¸­é€æ­¥å»é™¤å™ªå£°ï¼Œä»¥åˆæˆæ¡ä»¶å›¾åƒï¼›ï¼ˆ2ï¼‰åŸºäºè·ç¦»çš„åˆ†ç±»å™¨ï¼Œå³æ ¹æ®åŸå§‹å›¾åƒå’Œåˆæˆå›¾åƒåœ¨åµŒå…¥ç©ºé—´ä¸­çš„æœ€å° L2 è·ç¦»é¢„æµ‹è¾“å…¥å›¾åƒçš„æ ‡ç­¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒConDiff æ˜¯ç¬¬ä¸€ä¸ªåˆ†æç»†ç²’åº¦ä¼¤å£å›¾åƒçš„ç”Ÿæˆåˆ¤åˆ«æ–¹æ³•ï¼Œä¿ƒè¿›äº†ç³–å°¿ç—…è¶³æºƒç–¡ (DFU) æ„ŸæŸ“çš„æ£€æµ‹ã€‚ï¼ˆ2ï¼‰åœ¨ DFU æ„ŸæŸ“æ•°æ®é›†çš„çœ‹ä¸è§çš„æµ‹è¯•ä¼¤å£å›¾åƒï¼ˆ148 ä¸ªå—æ„ŸæŸ“å’Œ 103 ä¸ªæœªå—æ„ŸæŸ“ï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬çš„ ConDiff æ¡†æ¶æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œæé«˜äº†ä¼¤å£æ„ŸæŸ“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œ F1 åˆ†æ•°è‡³å°‘ 3%ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åœ¨è®­ç»ƒæœŸé—´æœ€å°åŒ–ä¸‰å…ƒæŸå¤±å‡½æ•°ï¼ŒConDiff å‡å°‘äº†å¯¹ 1416 ä¸ªè®­ç»ƒå›¾åƒçš„å° DFU æ•°æ®é›†çš„è¿‡æ‹Ÿåˆã€‚ï¼ˆ4ï¼‰ç”± Score-CAM ç”Ÿæˆçš„çƒ­å›¾ç”¨äºç›´è§‚åœ°è¯´æ˜ ConDiff åœ¨å¯¹ä¼¤å£æ„ŸæŸ“çŠ¶æ€è¿›è¡Œåˆ†ç±»æ—¶ä¸“æ³¨äºæ­£ç¡®çš„ä¼¤å£å›¾åƒåŒºåŸŸã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å¼•å…¥äº†å¼•å¯¼æ¡ä»¶æ‰©æ•£åˆ†ç±»å™¨ï¼ˆConDiffï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯¹ç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰æ„ŸæŸ“è¿›è¡Œåˆ†ç±»çš„æ–°æ¡†æ¶ã€‚ConDiff ä¼˜äºä¼ ç»Ÿæ¨¡å‹è‡³å°‘ 3%ï¼Œå‡†ç¡®ç‡é«˜è¾¾ 83%ï¼ŒF1 åˆ†æ•°ä¸º 0.858ã€‚å®ƒç‹¬ç‰¹çš„æ–¹æ³•åˆ©ç”¨ä¸‰å…ƒæŸå¤±è€Œä¸æ˜¯æ ‡å‡†çš„äº¤å‰ç†µæœ€å°åŒ–ï¼Œå¢å¼ºäº†é²æ£’æ€§å’Œå‡å°‘äº†è¿‡æ‹Ÿåˆã€‚è¿™åœ¨æ•°æ®é›†é€šå¸¸å¾ˆå°çš„åŒ»å­¦æˆåƒä¸­å°¤å…¶é‡è¦ã€‚ConDiff é‡‡ç”¨æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œå‘è¾“å…¥å›¾åƒä¸­æ·»åŠ ç‰¹å®šæ•°é‡çš„é«˜æ–¯å™ªå£°ï¼Œå¹¶é‡‡ç”¨æ— åˆ†ç±»å™¨æŒ‡å¯¼çš„åå‘æ‰©æ•£ï¼Œæ ¹æ®åµŒå…¥ç©ºé—´ä¸­çš„æœ€è¿‘æ¬§å‡ é‡Œå¾—è·ç¦»å¯¹è¿™äº›å›¾åƒè¿›è¡Œè¿­ä»£ç»†åŒ–ä»¥è¿›è¡Œåˆ†ç±»ã€‚ConDiff çš„æœ‰æ•ˆæ€§è¡¨æ˜åœ¨æ”¹å–„ DFU ç®¡ç†æ–¹é¢å…·æœ‰æ˜¾ç€æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—èµ„æºæœ‰é™çš„åœ°åŒºã€‚å…¶ç²¾ç¡®çš„å®æ—¶æ„ŸæŸ“æ£€æµ‹å¯ä»¥åœ¨æ—©æœŸ DFU æ„ŸæŸ“è¯†åˆ«ä¸­å‘æŒ¥è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä»è€Œå‡å°‘è‚¢ä½“æˆªè‚¢ç­‰ä¸¥é‡å¹¶å‘ç—‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šConDiff æ˜¯ç¬¬ä¸€ä¸ªåˆ†æç»†ç²’åº¦ä¼¤å£å›¾åƒçš„ç”Ÿæˆåˆ¤åˆ«æ–¹æ³•ï¼Œä¿ƒè¿›äº† DFU æ„ŸæŸ“çš„æ£€æµ‹ï¼›æ€§èƒ½ï¼šåœ¨ DFU æ„ŸæŸ“æ•°æ®é›†çš„çœ‹ä¸è§çš„æµ‹è¯•ä¼¤å£å›¾åƒï¼ˆ148 ä¸ªå—æ„ŸæŸ“å’Œ 103 ä¸ªæœªå—æ„ŸæŸ“ï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼ŒConDiff æ¡†æ¶æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œæé«˜äº†ä¼¤å£æ„ŸæŸ“æ£€æµ‹çš„å‡†ç¡®æ€§å’Œ F1 åˆ†æ•°è‡³å°‘ 3%ï¼›å·¥ä½œé‡ï¼šé€šè¿‡åœ¨è®­ç»ƒæœŸé—´æœ€å°åŒ–ä¸‰å…ƒæŸå¤±å‡½æ•°ï¼ŒConDiff å‡å°‘äº†å¯¹ 1416 ä¸ªè®­ç»ƒå›¾åƒçš„å° DFU æ•°æ®é›†çš„è¿‡æ‹Ÿåˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f38f851b08a13cd2762a9779abb3d5dd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac1698e3895c14a21d1245d61cbbe4db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-902be7065fad826b29010fef3bd7e79b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3d5d8c1286e3aefa0a37934906ae34f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a6a38755ae54c6bfd6a3359d2197b5a2.jpg" align="middle"></details><h2 id="Obtaining-Favorable-Layouts-for-Multiple-Object-Generation"><a href="#Obtaining-Favorable-Layouts-for-Multiple-Object-Generation" class="headerlink" title="Obtaining Favorable Layouts for Multiple Object Generation"></a>Obtaining Favorable Layouts for Multiple Object Generation</h2><p><strong>Authors:Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</strong></p><p>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts. </p><p><a href="http://arxiv.org/abs/2405.00791v1">PDF</a> </p><p><strong>Summary</strong><br>éšç€æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¤šä¸»ä½“ç”Ÿæˆæˆä¸ºæ¨¡å‹å‘å±•çš„é‡è¦æ­¥éª¤ã€‚æœ¬ç ”ç©¶é’ˆå¯¹æ‰©æ•£æ¨¡å‹å¤šä¸»ä½“ç”Ÿæˆä¸­çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¼•å¯¼åŸåˆ™è¿›è¡Œå¸ƒå±€è§„åˆ’çš„æ–°æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸»ä½“ç”Ÿæˆä¸­é¢ä¸´ç€é—æ¼æˆ–åˆå¹¶ä¸»ä½“çš„é—®é¢˜ã€‚</li><li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼•å¯¼åŸåˆ™çš„å¸ƒå±€è§„åˆ’æ–¹æ³•ã€‚</li><li>è¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹åˆå§‹æå‡ºå¸ƒå±€ï¼Œç„¶åå¯¹å…¶è¿›è¡Œé‡æ–°æ’åˆ—ã€‚</li><li>å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå¾ªæå‡ºçš„é®ç½©ï¼Œå¹¶å°†æ½œåœ¨å›¾ä¸­çš„åƒç´ è¿ç§»åˆ°æ–°ä½ç½®ã€‚</li><li>å¼•å…¥äº†æ–°çš„æŸå¤±é¡¹ï¼Œä»¥å‡å°‘ XAM ç†µã€å‡å°‘ XAM ä¹‹é—´çš„é‡å å¹¶ç¡®ä¿ XAM ä¸å„è‡ªçš„é®ç½©å¯¹é½ã€‚</li><li>è¯¥æ–¹æ³•åœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´çœŸå®åœ°æ•æ‰åˆ°æ‰€éœ€æ¦‚å¿µã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: è·å¾—å¤šä¸ªå¯¹è±¡ç”Ÿæˆçš„æœ‰åˆ©å¸ƒå±€</p></li><li><p>Authors: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</p></li><li><p>Affiliation: å·´ä¼Šå…°å¤§å­¦å·¥ç¨‹å­¦é™¢</p></li><li><p>Keywords: æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ, å¤šå¯¹è±¡ç”Ÿæˆ, æ‰©æ•£æ¨¡å‹, äº¤å‰æ³¨æ„åŠ›å›¾</p></li><li><p>Paper: https://arxiv.org/abs/2405.00791 , Github: None</p></li><li><p>Summary:</p><p>(1): å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚è¿™äº›æ¨¡å‹æœ€ç»ˆæ—¨åœ¨åˆ›å»ºå¤æ‚çš„åœºæ™¯ï¼Œè§£å†³å¤šå¯¹è±¡ç”ŸæˆæŒ‘æˆ˜æ˜¯æœç€è¿™ä¸€ç›®æ ‡è¿ˆå‡ºçš„å…³é”®ä¸€æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å›¾åƒæ—¶é¢ä¸´å›°éš¾ã€‚å½“ç»™å®šåŒ…å«å¤šä¸ªå¯¹è±¡çš„æç¤ºæ—¶ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šçœç•¥ä¸€äº›å¯¹è±¡æˆ–å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ã€‚</p><p>(2): ç°æœ‰çš„æ–¹æ³•åŒ…æ‹¬ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰å¯¹ç”Ÿæˆå›¾åƒä¸­çš„ä¸åŒå¯¹è±¡è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨é—®é¢˜ï¼šå½“æç¤ºä¸­åŒ…å«å¤šä¸ªå¯¹è±¡æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šçœç•¥ä¸€äº›å¯¹è±¡æˆ–å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ã€‚è¿™æ˜¯é€šè¿‡å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå®ˆæå‡ºçš„æ©ç å¹¶é€šè¿‡å°†åƒç´ ä»æ½œåœ¨å›¾è¿ç§»åˆ°æˆ‘ä»¬ç¡®å®šçš„æ–°ä½ç½®æ¥å®ç°çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†æ–°çš„æŸå¤±é¡¹ï¼Œæ—¨åœ¨é™ä½ XAM ç†µä»¥æ›´æ¸…æ™°åœ°å®šä¹‰å¯¹è±¡çš„ç©ºé—´ï¼Œå‡å°‘ XAM ä¹‹é—´çš„é‡å ï¼Œå¹¶ç¡®ä¿ XAM ä¸å®ƒä»¬å„è‡ªçš„æ©ç å¯¹é½ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œå…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡å¼ºåˆ¶äº¤å‰æ³¨æ„åŠ›å›¾ï¼ˆXAMï¼‰éµå®ˆæå‡ºçš„æ©ç å¹¶é€šè¿‡å°†åƒç´ ä»æ½œåœ¨å›¾è¿ç§»åˆ°ç¡®å®šçš„æ–°ä½ç½®æ¥å®ç°ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå¼•å…¥æ–°çš„æŸå¤±é¡¹ï¼Œæ—¨åœ¨é™ä½ XAM ç†µä»¥æ›´æ¸…æ™°åœ°å®šä¹‰å¯¹è±¡çš„ç©ºé—´ï¼Œå‡å°‘ XAM ä¹‹é—´çš„é‡å ï¼Œå¹¶ç¡®ä¿ XAM ä¸å®ƒä»¬å„è‡ªçš„æ©ç å¯¹é½ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ï¼Œä»è€Œæ›´å¿ å®åœ°æ•æ‰åˆ°å„ç§æ–‡æœ¬æç¤ºä¸­æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºæŒ‡å¯¼åŸåˆ™çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æ‰©æ•£æ¨¡å‹æœ€åˆæå‡ºä¸€ä¸ªå¸ƒå±€ï¼Œç„¶åé‡æ–°æ’åˆ—å¸ƒå±€ç½‘æ ¼ã€‚æ€§èƒ½ï¼šåœ¨å„ç§æ–‡æœ¬æç¤ºä¸­æ›´å¿ å®åœ°æ•æ‰åˆ°æ‰€éœ€çš„æ¦‚å¿µï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å·¥ä½œé‡ï¼šéœ€è¦é’ˆå¯¹ä¸åŒçš„æ–‡æœ¬æç¤ºè¿›è¡Œå¾®è°ƒï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d3231e3375af2b14c1e49248519eaebd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-561eb2e3b9534e1fe4b30e7ef897a8b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2973412c6cc5c19507315dc2dd5efcd.jpg" align="middle"></details><h2 id="Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models"><a href="#Deep-Reward-Supervisions-for-Tuning-Text-to-Image-Diffusion-Models" class="headerlink" title="Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models"></a>Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h2><p><strong>Authors:Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2. </p><p><a href="http://arxiv.org/abs/2405.00760v1">PDF</a> N/A</p><p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆ©ç”¨ç»™å®šçš„æ¿€åŠ±å‡½æ•°è¿›è¡Œä¼˜åŒ–æ˜¯ä¸€ä¸ªé‡è¦ä½†æœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„ç ”ç©¶é¢†åŸŸã€‚ç ”ç©¶ä¸­æå‡ºæ·±åº¦æ¿€åŠ±ä¼˜åŒ–ï¼ˆDRTuneï¼‰ï¼Œç®—æ³•ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒï¼Œå¹¶ä¸”é€šè¿‡è¿­ä»£é‡‡æ ·æµç¨‹å°†æ¢¯åº¦ä¼ å›è¾“å…¥å™ªå£°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é’ˆå¯¹ä½å±‚æ¿€åŠ±ï¼Œè®­ç»ƒé‡‡æ ·æµç¨‹ä¸­çš„æ—©æœŸæ­¥éª¤è‡³å…³é‡è¦ã€‚</li><li>åœ¨å»å™ªç½‘ç»œè¾“å…¥å¤„åœæ­¢æ¢¯åº¦ï¼Œå¯ä»¥æœ‰æ•ˆå®ç°æ·±åº¦ç›‘ç£ã€‚</li><li>DRTune ç®—æ³•åœ¨å„ç§æ¿€åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ã€‚</li><li>DRTune ç®—æ³•å§‹ç»ˆä¼˜äºå…¶ä»–ç®—æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å¤±æ•ˆçš„ä½å±‚æ§åˆ¶ä¿¡å·ä¸­ã€‚</li><li>é€šè¿‡ DRTune ä¼˜åŒ– Human Preference Score v2.1ï¼Œå¯¹ Stable Diffusion XL 1.0ï¼ˆSDXL 1.0ï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œäº§ç”Ÿäº†æ›´å¥½çš„æ‰©æ•£ XL 1.0ï¼ˆFDXL 1.0ï¼‰æ¨¡å‹ã€‚</li><li>FDXL 1.0 ä¸ Midjourney v5.2 ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒè´¨é‡ï¼Œè¾¾åˆ°äº†ç›¸å½“çš„æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šæ·±åº¦å¥–åŠ±ç›‘ç£å¾®è°ƒ</p></li><li><p>ä½œè€…ï¼šXiaoshi Wu<em>1,3, Yiming Hao</em>2, Manyuan Zhang1, Keqiang Sun1, Zhaoyang Huang3, Guanglu Song4, Yu Liu4, and Hongsheng Li1,2</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤šåª’ä½“å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€æ·±åº¦å¥–åŠ±ç›‘ç£ã€å¾®è°ƒã€å›¾åƒè´¨é‡å¢å¼º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2405.00760v1</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼šä¼˜åŒ–å…·æœ‰ç»™å®šå¥–åŠ±å‡½æ•°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ªé‡è¦ä½†å°šæœªå……åˆ†æ¢ç´¢çš„ç ”ç©¶é¢†åŸŸã€‚</p><p>(2)ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æµ…å±‚ç›‘ç£ï¼Œå³ä»…ç›‘ç£é‡‡æ ·è¿‡ç¨‹çš„æ—©æœŸæ­¥éª¤ã€‚ç„¶è€Œï¼Œå¯¹äºä½çº§å¥–åŠ±ä¿¡å·ï¼Œæµ…å±‚ç›‘ç£æ•ˆæœä¸ä½³ã€‚</p><p>(3)ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºæ·±åº¦å¥–åŠ±å¾®è°ƒï¼ˆDRTuneï¼‰ç®—æ³•ï¼Œé€šè¿‡ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒå¹¶é€šè¿‡è¿­ä»£é‡‡æ ·è¿‡ç¨‹åå‘ä¼ æ’­åˆ°è¾“å…¥å™ªå£°æ¥å®ç°æ·±åº¦ç›‘ç£ã€‚</p><p>(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šDRTuneåœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ã€‚ä¸å…¶ä»–ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå§‹ç»ˆè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å‡å¤±è´¥çš„ä½çº§æ§åˆ¶ä¿¡å·æ–¹é¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨DRTuneå¾®è°ƒäº†Stable Diffusion XL 1.0ï¼ˆSDXL 1.0ï¼‰æ¨¡å‹ä»¥ä¼˜åŒ–äººç±»åå¥½è¯„åˆ†v2.1ï¼Œå¾—åˆ°äº†Favorable Diffusion XL 1.0ï¼ˆFDXL 1.0ï¼‰æ¨¡å‹ã€‚ä¸SDXL 1.0ç›¸æ¯”ï¼ŒFDXL 1.0æ˜¾ç€æé«˜äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”ä¸Midjourney v5.2ç›¸æ¯”è¾¾åˆ°äº†ç›¸å½“çš„è´¨é‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šDRTune ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç›´æ¥ç›‘ç£æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºå›¾åƒï¼Œå¹¶é€šè¿‡è¿­ä»£é‡‡æ ·è¿‡ç¨‹åå‘ä¼ æ’­åˆ°è¾“å…¥å™ªå£°æ¥å®ç°æ·±åº¦ç›‘ç£ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä¸ºäº†è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼ŒDRTune é€šè¿‡é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦æ¥è§£å†³æ”¶æ•›é—®é¢˜ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¸ºäº†æé«˜æ•ˆç‡ï¼ŒDRTune é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦ï¼Œå¹¶è®­ç»ƒæ‰€æœ‰é‡‡æ ·æ­¥éª¤çš„å­é›†ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šDRTune ç®—æ³•çš„ä¼ªä»£ç å¦‚ä¸‹ï¼š</p><p><code>è¾“å…¥ï¼šé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æƒé‡ Î¸ã€å¥–åŠ± rã€è®­ç»ƒæ—¶é—´æ­¥é•¿ Kã€æ—©æœŸåœæ­¢æ—¶é—´æ­¥é•¿èŒƒå›´ mã€‚sg è¡¨ç¤ºæ¢¯åº¦åœæ­¢æ“ä½œã€‚while not converged do    ttrain = {1, ..., K} if DRaFT-K    ttrain = {i}iâ‰¥randint(1,T ) if AlignProp    if DRTune then        # ç­‰è·æ—¶é—´æ­¥é•¿ã€‚        s = randint(1, T âˆ’ KâŒŠ T K âŒ‹)        ttrain = {s + iâŒŠ T K âŒ‹ | i = 0, 1, ..., K âˆ’ 1}    if ReFL æˆ– DRTune then        tmin = randint(1, m)    else        tmin = 0    xT âˆ¼ N(0, I)    for t = T, ..., 1 do        if DRTune then            Ë†Ïµ = ÏµÎ¸(sg(xt), t)        else            Ë†Ïµ = ÏµÎ¸(xt, t)        if t /âˆˆ ttrain then            Ë†Ïµ = sg(Ë†Ïµ)        if t == tmin then            x0 â‰ˆ intermediate_prediction(xt, Ë†Ïµ)            break        xtâˆ’1 = atxt + btË†Ïµ + ctÏµ        g = âˆ‡Î¸r(x0, c)        Î¸ â† Î¸ âˆ’ Î·g</code></p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡çš„æ„ä¹‰åœ¨äºï¼Œå®ƒè§£å†³äº†åˆ©ç”¨å¥–åŠ±æ¨¡å‹çš„åé¦ˆæ¥è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒäº†æ·±åº¦ç›‘ç£å¯¹äºä¼˜åŒ–å…¨å±€å¥–åŠ±çš„é‡è¦æ€§ï¼Œå¹¶ä½¿ç”¨åœæ­¢æ¢¯åº¦æŠ€æœ¯è§£å†³äº†æ”¶æ•›é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é€šè¿‡å¾®è°ƒ FDXL 1.0 æ¨¡å‹å±•ç¤ºäº†å¥–åŠ±è®­ç»ƒçš„æ½œåŠ›ï¼Œä»¥å®ç°ä¸ Midjourney ç›¸å½“çš„å›¾åƒè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†æ·±åº¦å¥–åŠ±å¾®è°ƒï¼ˆDRTuneï¼‰ç®—æ³•ï¼Œå®ç°äº†æ·±åº¦ç›‘ç£ï¼Œå¹¶é€šè¿‡é˜»æ­¢è¾“å…¥ xt çš„æ¢¯åº¦æ¥è§£å†³æ”¶æ•›é—®é¢˜ã€‚æ€§èƒ½ï¼šDRTune åœ¨å„ç§å¥–åŠ±æ¨¡å‹ä¸Šå¾—åˆ°äº†å¹¿æ³›è¯„ä¼°ï¼Œä¸å…¶ä»–ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå§‹ç»ˆè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æµ…å±‚ç›‘ç£æ–¹æ³•å‡å¤±è´¥çš„ä½çº§æ§åˆ¶ä¿¡å·æ–¹é¢ã€‚å·¥ä½œé‡ï¼šDRTune ç®—æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åº”ç”¨äºç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-037d3e48be185336859047a6292c8d27.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b417340b0b4ae8f5dcc966e5d18466d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d9661ec54a3560470071969dc361ea74.jpg" align="middle"><img src="https://pica.zhimg.com/v2-560c3c936da4c7000b08d87c1704852f.jpg" align="middle"></details><h2 id="Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution"><a href="#Detail-Enhancing-Framework-for-Reference-Based-Image-Super-Resolution" class="headerlink" title="Detail-Enhancing Framework for Reference-Based Image Super-Resolution"></a>Detail-Enhancing Framework for Reference-Based Image Super-Resolution</h2><p><strong>Authors:Zihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</strong></p><p>Recent years have witnessed the prosperity of reference-based image super-resolution (Ref-SR). By importing the high-resolution (HR) reference images into the single image super-resolution (SISR) approach, the ill-posed nature of this long-standing field has been alleviated with the assistance of texture transferred from reference images. Although the significant improvement in quantitative and qualitative results has verified the superiority of Ref-SR methods, the presence of misalignment before texture transfer indicates room for further performance improvement. Existing methods tend to neglect the significance of details in the context of comparison, therefore not fully leveraging the information contained within low-resolution (LR) images. In this paper, we propose a Detail-Enhancing Framework (DEF) for reference-based super-resolution, which introduces the diffusion model to generate and enhance the underlying detail in LR images. If corresponding parts are present in the reference image, our method can facilitate rigorous alignment. In cases where the reference image lacks corresponding parts, it ensures a fundamental improvement while avoiding the influence of the reference image. Extensive experiments demonstrate that our proposed method achieves superior visual results while maintaining comparable numerical outcomes. </p><p><a href="http://arxiv.org/abs/2405.00431v1">PDF</a> </p><p><strong>Summary</strong><br>å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨é€šè¿‡å¼•å…¥é«˜åˆ†è¾¨ç‡å‚è€ƒå›¾åƒæ¥ç¼“è§£å•å›¾åƒè¶…åˆ†è¾¨çš„ç—…æ€é—®é¢˜ï¼Œä½†ç”±äºçº¹ç†ä¼ è¾“å‰å­˜åœ¨é”™ä½é—®é¢˜ï¼Œä»æœ‰æå‡ç©ºé—´ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§ç»“æœä¸Šå‡æœ‰æ˜¾è‘—æå‡ã€‚</li><li>ç°æœ‰æ–¹æ³•å¿½è§†äº†æ¯”è¾ƒä¸­ç»†èŠ‚çš„é‡è¦æ€§ï¼Œæ²¡æœ‰å……åˆ†åˆ©ç”¨ä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„ä¿¡æ¯ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼Œç”¨äºå¼•ç”¨å›¾åƒè¶…åˆ†è¾¨ã€‚</li><li>å¦‚æœå‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚</li><li>å¦‚æœå‚è€ƒå›¾åƒæ²¡æœ‰å¯¹åº”éƒ¨åˆ†ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç¡®ä¿åŸºæœ¬æ”¹è¿›ï¼ŒåŒæ—¶é¿å…å‚è€ƒå›¾åƒçš„å½±å“ã€‚</li><li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¿æŒå¯æ¯”æ•°å€¼ç»“æœçš„åŒæ—¶ï¼Œè·å¾—äº†æ›´å¥½çš„è§†è§‰æ•ˆæœã€‚</li><li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰å¯¹åº”å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹æé«˜è¶…åˆ†è¾¨ç‡æ€§èƒ½ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥çµæ´»åœ°åº”ç”¨äºå„ç§å¼•ç”¨å›¾åƒè¶…åˆ†è¾¨ä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>è®ºæ–‡æ ‡é¢˜ï¼šåŸºäºå‚è€ƒå›¾åƒçš„å›¾åƒè¶…åˆ†è¾¨ç‡çš„ç»†èŠ‚å¢å¼ºæ¡†æ¶</p></li><li><p>ä½œè€…ï¼šZihan Wang, Ziliang Xiong, Hongying Tang, Xiaobing Yuan</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·å¾®ç³»ç»Ÿä¸ä¿¡æ¯æŠ€æœ¯ç ”ç©¶æ‰€</p></li><li><p>å…³é”®è¯ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå‚è€ƒå›¾åƒï¼Œç»†èŠ‚å¢å¼ºï¼Œæ‰©æ•£æ¨¡å‹</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºå‚è€ƒå›¾åƒçš„å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆRef-SRï¼‰å¾—åˆ°äº†è“¬å‹ƒå‘å±•ã€‚é€šè¿‡å°†é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å‚è€ƒå›¾åƒå¼•å…¥åˆ°å•å¹…å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ–¹æ³•ä¸­ï¼Œåœ¨å‚è€ƒå›¾åƒçº¹ç†çš„è¾…åŠ©ä¸‹ï¼Œç¼“è§£äº†è¿™ä¸€é•¿æœŸå­˜åœ¨çš„é¢†åŸŸçš„ç—…æ€æ€§è´¨ã€‚å°½ç®¡å®šé‡å’Œå®šæ€§ç»“æœçš„æ˜¾ç€æé«˜éªŒè¯äº† Ref-SR æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œä½†åœ¨çº¹ç†ä¼ è¾“ä¹‹å‰å­˜åœ¨çš„é”™ä½è¡¨æ˜è¿˜æœ‰è¿›ä¸€æ­¥æé«˜æ€§èƒ½çš„ç©ºé—´ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ¯”è¾ƒèƒŒæ™¯ä¸‹ç»†èŠ‚çš„é‡è¦æ€§ï¼Œå› æ­¤æ²¡æœ‰å……åˆ†åˆ©ç”¨ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒä¸­åŒ…å«çš„ä¿¡æ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å€¾å‘äºç®€å•åœ°å°†è¾“å…¥çš„ LR å›¾åƒè°ƒæ•´ä¸ºä¸ç›¸åº”å‚è€ƒå›¾åƒç›¸åŒçš„åˆ†è¾¨ç‡ï¼Œä¾‹å¦‚åŒä¸‰æ¬¡æ’å€¼ã€‚Lu ç­‰äººé€‰æ‹©å¯¹å‚è€ƒå›¾åƒè¿›è¡Œä¸‹é‡‡æ ·ä»¥é€‚åº”åŒ¹é…è¿‡ç¨‹ï¼Œç›®çš„æ˜¯é™ä½è®¡ç®—å¤æ‚åº¦ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£é”™ä½é—®é¢˜ï¼Œä½†å®ƒå¿½ç•¥äº†ç»†èŠ‚çš„å¢å¼ºï¼Œå¯èƒ½ä¼šç ´ååç»­çš„å›¾åƒæ¢å¤ç»“æœã€‚åœ¨å‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œç°æœ‰çš„æ–¹æ³•æ— æ³•ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œå®ƒå¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼º LR å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚å¦‚æœå‚è€ƒå›¾åƒä¸­å­˜åœ¨å¯¹åº”éƒ¨åˆ†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¿ƒè¿›ä¸¥æ ¼çš„å¯¹é½ã€‚åœ¨å‚è€ƒå›¾åƒä¸­ç¼ºå°‘å¯¹åº”éƒ¨åˆ†çš„æƒ…å†µä¸‹ï¼Œå®ƒç¡®ä¿äº†æ ¹æœ¬æ€§çš„æ”¹è¿›ï¼ŒåŒæ—¶é¿å…äº†å‚è€ƒå›¾åƒçš„å½±å“ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„è§†è§‰æ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„æ•°å€¼ç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œå®ƒå¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼º LR å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚</p><p>ï¼ˆ5ï¼‰ï¼šåˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</p><p>ï¼ˆ6ï¼‰ï¼šä½¿ç”¨ deformable convolution networkï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå’Œå¢å¼ºä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚åœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚åœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚åˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨å¯å˜å½¢å·ç§¯ç½‘ç»œï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒå›¾åƒçš„è¶…åˆ†è¾¨ç‡ç»†èŠ‚å¢å¼ºæ¡†æ¶ï¼ˆDEFï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå’Œå¢å¼ºä½åˆ†è¾¨ç‡å›¾åƒä¸­çš„åº•å±‚ç»†èŠ‚ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šç»†èŠ‚ç”Ÿæˆå’Œç»†èŠ‚è¿ç§»ã€‚åœ¨ç»†èŠ‚ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯¹è¾“å…¥å›¾åƒè¿›è¡Œä¸Šé‡‡æ ·ï¼Œä»¥è·å¾—ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒã€‚åœ¨ç»†èŠ‚è¿ç§»ä»»åŠ¡ä¸­ï¼Œé¦–å…ˆå¯¹ç»†èŠ‚å¢å¼ºçš„å›¾åƒå’Œå‚è€ƒå›¾åƒè¿›è¡Œç‰¹å¾æå–ã€‚åˆ©ç”¨ç»†èŠ‚å¢å¼ºçš„è¾“å…¥å›¾åƒæ¥è®¡ç®—å‚è€ƒå›¾åƒå’Œè¾“å…¥å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨å¯å˜å½¢å·ç§¯ç½‘ç»œï¼ˆDCNï¼‰è¿›è¡Œçº¹ç†è¿ç§»å’Œé›†æˆï¼Œä»¥è§£å†³çº¹ç†å¤±é…é—®é¢˜ã€‚æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„è§†è§‰æ•ˆæœï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¯”çš„æ•°å€¼ç»“æœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ã€‚å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œå¯å˜å½¢å·ç§¯ç½‘ç»œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-03b9463baa117efca1717d3d158fe273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c3af50396285ae462ddd151feecf5dad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a59353ef4d615d00935a00b86d496d8.jpg" align="middle"></details><h2 id="ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning"><a href="#ASAM-Boosting-Segment-Anything-Model-with-Adversarial-Tuning" class="headerlink" title="ASAM: Boosting Segment Anything Model with Adversarial Tuning"></a>ASAM: Boosting Segment Anything Model with Adversarial Tuning</h2><p><strong>Authors:Bo Li, Haoke Xiao, Lv Tang</strong></p><p>In the evolving landscape of computer vision, foundation models have emerged as pivotal tools, exhibiting exceptional adaptability to a myriad of tasks. Among these, the Segment Anything Model (SAM) by Meta AI has distinguished itself in image segmentation. However, SAM, like its counterparts, encounters limitations in specific niche applications, prompting a quest for enhancement strategies that do not compromise its inherent capabilities. This paper introduces ASAM, a novel methodology that amplifies SAMâ€™s performance through adversarial tuning. We harness the potential of natural adversarial examples, inspired by their successful implementation in natural language processing. By utilizing a stable diffusion model, we augment a subset (1%) of the SA-1B dataset, generating adversarial instances that are more representative of natural variations rather than conventional imperceptible perturbations. Our approach maintains the photorealism of adversarial examples and ensures alignment with original mask annotations, thereby preserving the integrity of the segmentation task. The fine-tuned ASAM demonstrates significant improvements across a diverse range of segmentation tasks without necessitating additional data or architectural modifications. The results of our extensive evaluations confirm that ASAM establishes new benchmarks in segmentation tasks, thereby contributing to the advancement of foundational models in computer vision. Our project page is in <a href="https://asam2024.github.io/">https://asam2024.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2405.00256v1">PDF</a> This paper is accepted by CVPR2024</p><p><strong>Summary</strong><br>Meta AIçš„ASAMé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒå¢å¼ºäº†SAMå›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¶æ„è°ƒæ•´å³å¯æå‡åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ASAM é‡‡ç”¨å¯¹æŠ—è®­ç»ƒæ¥å¢å¼º SAM å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæ— éœ€é¢å¤–æ•°æ®æˆ–æ¶æ„ä¿®æ”¹ã€‚</li><li>è‡ªç„¶å¯¹æŠ—å®ä¾‹æé«˜äº†æ¨¡å‹å¯¹è‡ªç„¶å˜åŒ–çš„é²æ£’æ€§ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ä¸å¯æ„ŸçŸ¥æ‰°åŠ¨ã€‚</li><li>ASAM ä¿æŒäº†å¯¹æŠ—å®ä¾‹çš„é€¼çœŸåº¦å¹¶ä¸åŸå§‹æ©ç æ³¨é‡Šä¿æŒä¸€è‡´ï¼Œä»è€Œä¿æŒåˆ†å‰²ä»»åŠ¡çš„å®Œæ•´æ€§ã€‚</li><li>å¾®è°ƒåçš„ ASAM åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾ç€æå‡ï¼Œåœ¨ SA-1B æ•°æ®é›†ä¸Šè¾¾åˆ° 88.2% çš„ mIoUã€‚</li><li>ASAM åœ¨ ADE20K æ•°æ®é›†ä¸Šè¾¾åˆ° 50.1% çš„ mIoUï¼Œè¶…è¿‡äº†ä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>ASAM åœ¨ COCO Stuff æ•°æ®é›†ä¸Šè¾¾åˆ° 34.6% çš„ mIoUï¼Œåœ¨ Cityscapes æ•°æ®é›†ä¸Šè¾¾åˆ° 81.2% çš„ mIoUã€‚</li><li>ASAM æ¨è¿›äº†è®¡ç®—æœºè§†è§‰ä¸­åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œè¯æ˜äº†å¯¹æŠ—æ€§è®­ç»ƒåœ¨å›¾åƒåˆ†å‰²ä¸­çš„æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ASAMï¼šåŸºäºå¯¹æŠ—è°ƒä¼˜çš„ Segment Anything æ¨¡å‹å¢å¼º</p></li><li><p>Authors: Bo Li, Haoke Xiao, Lv Tang</p></li><li><p>Affiliation: vivo Mobile Communication Co., Ltd</p></li><li><p>Keywords: Adversarial Tuning, Image Segmentation, Foundation Model, Segment Anything Model, Stable Diffusion</p></li><li><p>Urls: https://arxiv.org/abs/2405.00256, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): éšç€è®¡ç®—æœºè§†è§‰çš„å‘å±•ï¼ŒåŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºå…³é”®å·¥å…·ï¼Œå®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„é€‚åº”æ€§ã€‚å…¶ä¸­ï¼ŒMeta AI çš„ Segment Anything Model (SAM) åœ¨å›¾åƒåˆ†å‰²é¢†åŸŸè¡¨ç°çªå‡ºã€‚ç„¶è€Œï¼ŒSAM ä¸å…¶ä»–åŒç±»æ¨¡å‹ä¸€æ ·ï¼Œåœ¨ç‰¹å®šç»†åˆ†åº”ç”¨ä¸­é‡åˆ°äº†å±€é™æ€§ï¼Œè¿™ä¿ƒä½¿äººä»¬å¯»æ±‚å¢å¼ºç­–ç•¥ï¼Œè€Œä¸ä¼šæŸå®³å…¶å›ºæœ‰èƒ½åŠ›ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬å¾®è°ƒå’Œé€‚é…å™¨æ¨¡å—ï¼Œä½†å¾®è°ƒä¼šæŸå®³ SAM çš„å›ºæœ‰æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™éœ€è¦é¢å¤–çš„é€‚é…å±‚æˆ–åå¤„ç†æ¨¡å—ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• ASAMï¼Œå®ƒé€šè¿‡å¯¹æŠ—è°ƒä¼˜æ¥å¢å¼º SAM çš„æ€§èƒ½ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è‡ªç„¶å¯¹æŠ—æ ·æœ¬æˆåŠŸå®ç°çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼ºäº† SA-1B æ•°æ®é›†çš„å­é›†ï¼ˆ1%ï¼‰ï¼Œç”Ÿæˆäº†æ›´èƒ½ä»£è¡¨è‡ªç„¶å˜åŒ–çš„å¯¹æŠ—å®ä¾‹ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„ä¸å¯æ„ŸçŸ¥æ‰°åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†å¯¹æŠ—æ ·æœ¬çš„çœŸå®æ„Ÿï¼Œå¹¶ç¡®ä¿äº†ä¸åŸå§‹æ©ç æ³¨é‡Šçš„ä¸€è‡´æ€§ï¼Œä»è€Œä¿ç•™äº†åˆ†å‰²ä»»åŠ¡çš„å®Œæ•´æ€§ã€‚</p><p>(4): åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç»è¿‡å¾®è°ƒçš„ ASAM å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œæ— éœ€é¢å¤–çš„æ•°æ®æˆ–æ¶æ„ä¿®æ”¹ã€‚æˆ‘ä»¬å¹¿æ³›è¯„ä¼°çš„ç»“æœè¯å®ï¼ŒASAM åœ¨åˆ†å‰²ä»»åŠ¡ä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ï¼Œä»è€Œä¿ƒè¿›äº†è®¡ç®—æœºè§†è§‰ä¸­åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º ASAM æ–¹æ³•ï¼Œé€šè¿‡å¯¹æŠ—è°ƒä¼˜å¢å¼º SAM æ¨¡å‹çš„æ€§èƒ½ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¢å¼º SA-1B æ•°æ®é›†çš„å­é›†ï¼Œç”Ÿæˆæ›´å…·ä»£è¡¨æ€§çš„å¯¹æŠ—å®ä¾‹ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¿æŒå¯¹æŠ—æ ·æœ¬çš„çœŸå®æ„Ÿï¼Œç¡®ä¿ä¸åŸå§‹æ©ç æ³¨é‡Šçš„ä¸€è‡´æ€§ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¯„ä¼° ASAMï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡åˆ›æ–°æ€§åœ°ä½¿ç”¨å¯¹æŠ—è°ƒä¼˜ï¼Œæå‡ºçš„ ASAM æ–¹æ³•ä»£è¡¨äº† SAM çš„é‡å¤§è¿›æ­¥ã€‚åˆ©ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹å¢å¼º SA-1B æ•°æ®é›†çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†è‡ªç„¶ã€é€¼çœŸçš„å¯¹æŠ—å›¾åƒï¼Œä»è€Œå¤§å¹…æå‡äº† SAM åœ¨å„ç§ä»»åŠ¡ä¸­çš„åˆ†å‰²èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å€Ÿé‰´äº† NLP ä¸­å¯¹æŠ—è®­ç»ƒæŠ€æœ¯ï¼Œåœ¨ä¿æŒ SAM åŸç”Ÿæ¶æ„å’Œé›¶æ ·æœ¬ä¼˜åŠ¿çš„åŒæ—¶å¢å¼ºäº†å…¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒASAM ä¸ä»…åœ¨åˆ†å‰²ä»»åŠ¡ä¸­æ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œè€Œä¸”ä¿ƒè¿›äº†å¯¹æŠ—æ ·ä¾‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨å’Œç†è§£ï¼Œä¸ºæå‡å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹èƒ½åŠ›æä¾›äº†ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—å®ä¾‹ï¼Œå¢å¼º SAM çš„åˆ†å‰²èƒ½åŠ›ï¼›ä¿æŒ SAM çš„åŸç”Ÿæ¶æ„å’Œé›¶æ ·æœ¬ä¼˜åŠ¿ï¼›æ€§èƒ½ï¼šåœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ï¼›å·¥ä½œé‡ï¼šä¸å¾®è°ƒå’Œé€‚é…å™¨æ¨¡å—ç­‰å…¶ä»–å¢å¼ºç­–ç•¥ç›¸æ¯”ï¼Œå·¥ä½œé‡ç›¸å¯¹è¾ƒå°ï¼Œæ— éœ€é¢å¤–çš„æ¶æ„ä¿®æ”¹æˆ–åå¤„ç†æ¨¡å—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-e7684baf385865b289b9bd3b4babea56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7c0e935c2de944340eb5085a5356da42.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d93133fcc44a60510ee9cb1385d6be69.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  Defect Image Sample Generation With Diffusion Prior for Steel Surface   Defect Recognition</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/06/Paper/2024-05-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-06T10:03:07.000Z</published>
    <updated>2024-05-06T10:03:07.018Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-06-æ›´æ–°"><a href="#2024-05-06-æ›´æ–°" class="headerlink" title="2024-05-06 æ›´æ–°"></a>2024-05-06 æ›´æ–°</h1><h2 id="X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation"><a href="#X-Oscar-A-Progressive-Framework-for-High-quality-Text-guided-3D-Animatable-Avatar-Generation" class="headerlink" title="X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation"></a>X-Oscar: A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</h2><p><strong>Authors:Yiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</strong></p><p>Recent advancements in automatic 3D avatar generation guided by text have made significant progress. However, existing methods have limitations such as oversaturation and low-quality output. To address these challenges, we propose X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts. It follows a sequential Geometry-&gt;Texture-&gt;Animation paradigm, simplifying optimization through step-by-step generation. To tackle oversaturation, we introduce Adaptive Variational Parameter (AVP), representing avatars as an adaptive distribution during training. Additionally, we present Avatar-aware Score Distillation Sampling (ASDS), a novel technique that incorporates avatar-aware noise into rendered images for improved generation quality during optimization. Extensive evaluations confirm the superiority of X-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous project page: <a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/">https://xmu-xiaoma666.github.io/Projects/X-Oscar/</a>. </p><p><a href="http://arxiv.org/abs/2405.00954v1">PDF</a> ICML2024</p><p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºçš„ X-Oscar æ¡†æ¶å¯ä»¥ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»å¤´åƒï¼Œå®ƒé‡‡ç”¨å‡ ä½•-&gt;çº¹ç†-&gt;åŠ¨ç”»çš„é¡ºåºèŒƒå¼ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”å˜å¼‚å‚æ•°å’ŒåŸºäºå¤´åƒçš„è¯„åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯æ¥è§£å†³è¿‡é¥±å’Œå’Œä½è´¨é‡è¾“å‡ºçš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>X-Oscar æ˜¯ä¸€ä¸ªæ¸è¿›å¼æ¡†æ¶ï¼Œä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡çš„å¯åŠ¨ç”»å¤´åƒã€‚</li><li>å®ƒé‡‡ç”¨é¡ºåºçš„å‡ ä½•-&gt;çº¹ç†-&gt;åŠ¨ç”»èŒƒå¼ï¼Œç®€åŒ–äº†ä¼˜åŒ–è¿‡ç¨‹ã€‚</li><li>è‡ªé€‚åº”å˜å¼‚å‚æ•°å°†å¤´åƒè¡¨ç¤ºä¸ºè®­ç»ƒæœŸé—´çš„è‡ªé€‚åº”åˆ†å¸ƒï¼Œä»¥è§£å†³è¿‡é¥±å’Œé—®é¢˜ã€‚</li><li>åŸºäºå¤´åƒçš„è¯„åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯å°†åŸºäºå¤´åƒçš„å™ªå£°èå…¥æ¸²æŸ“å›¾åƒï¼Œä»¥æé«˜ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ç”Ÿæˆè´¨é‡ã€‚</li><li>å¹¿æ³›çš„è¯„ä¼°è¯å® X-Oscar ä¼˜äºç°æœ‰çš„æ–‡æœ¬åˆ° 3D å’Œæ–‡æœ¬åˆ°å¤´åƒæ–¹æ³•ã€‚</li><li>é¡¹ç›®ä¸»é¡µï¼š<a href="https://xmu-xiaoma666.github.io/Projects/X-Oscar/ã€‚">https://xmu-xiaoma666.github.io/Projects/X-Oscar/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>è®ºæ–‡æ ‡é¢˜ï¼šX-Oscarï¼šä¸€ä¸ªç”¨äºç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬å¼•å¯¼å¼3Då¯åŠ¨ç”»è§’è‰²çš„æ¸è¿›å¼æ¡†æ¶</li><p></p><p></p><li>ä½œè€…ï¼šYiwei Ma, Zhekai Lin, Jiayi Ji, Yijun Fan, Xiaoshuai Sun, Rongrong Ji</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå¦é—¨å¤§å­¦å¤šåª’ä½“å¯ä¿¡æ„ŸçŸ¥ä¸é«˜æ•ˆè®¡ç®—æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤</li><p></p><p></p><li>å…³é”®è¯ï¼šæ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆã€æ¸è¿›å¼ç”Ÿæˆã€è‡ªé€‚åº”å˜åˆ†å‚æ•°ã€è§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2405.00954.pdf ï¼ŒGithubä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li><p></p><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šéšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼Œ3Däººä½“é‡å»ºé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¸“æ³¨äºä»è§†è§‰çº¿ç´¢é‡å»ºäººä½“ï¼Œéš¾ä»¥æ»¡è¶³èå…¥åˆ›é€ åŠ›ã€ç¼–è¾‘å’Œæ§åˆ¶çš„éœ€æ±‚ã€‚ï¼ˆ2ï¼‰ï¼šç°æœ‰æ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆæ–¹æ³•å­˜åœ¨è¿‡é¥±å’Œå’Œè¾“å‡ºè´¨é‡ä½çš„é—®é¢˜ã€‚ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¸è¿›å¼æ¡†æ¶X-Oscarï¼Œé€šè¿‡â€œå‡ ä½•â†’çº¹ç†â†’åŠ¨ç”»â€çš„é¡ºåºç”Ÿæˆæ¨¡å¼ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”å˜åˆ†å‚æ•°å’Œè§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯ï¼Œæ¥è§£å†³è¿‡é¥±å’Œé—®é¢˜å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚ï¼ˆ4ï¼‰ï¼šåœ¨æ–‡æœ¬åˆ°3Då’Œæ–‡æœ¬åˆ°è§’è‰²ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒX-Oscaråœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ¨ç”»ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼šï¼ˆ1ï¼‰ï¼šæå‡ºæ¸è¿›å¼æ¡†æ¶X-Oscarï¼Œé€šè¿‡â€œå‡ ä½•â†’çº¹ç†â†’åŠ¨ç”»â€çš„é¡ºåºç”Ÿæˆæ¨¡å¼ï¼Œè§£å†³è¿‡é¥±å’Œé—®é¢˜å¹¶æé«˜ç”Ÿæˆè´¨é‡ï¼›ï¼ˆ2ï¼‰ï¼šå¼•å…¥è‡ªé€‚åº”å˜åˆ†å‚æ•°ï¼ˆAVPï¼‰ï¼Œé‡‡ç”¨å¯è®­ç»ƒçš„è‡ªé€‚åº”åˆ†å¸ƒè¡¨ç¤ºè™šæ‹Ÿå½¢è±¡ï¼Œè§£å†³è™šæ‹Ÿå½¢è±¡ç”Ÿæˆä¸­å¸¸è§çš„è¿‡é¥±å’Œé—®é¢˜ï¼›ï¼ˆ3ï¼‰ï¼šæå‡ºè™šæ‹Ÿå½¢è±¡æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆASDSï¼‰ï¼Œå°†å‡ ä½•æ„ŸçŸ¥å’Œå¤–è§‚æ„ŸçŸ¥å™ªå£°èå…¥å»å™ªè¿‡ç¨‹ä¸­ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥ç”Ÿæˆè™šæ‹Ÿå½¢è±¡çš„å½“å‰çŠ¶æ€ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„X-Oscaræ¡†æ¶åœ¨æ–‡æœ¬å¼•å¯¼å¼3Då¯åŠ¨ç”»è§’è‰²ç”Ÿæˆé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šX-Oscaræ¡†æ¶åˆ›æ–°æ€§åœ°æå‡ºäº†æ¸è¿›å¼ç”Ÿæˆæ¨¡å¼ã€è‡ªé€‚åº”å˜åˆ†å‚æ•°å’Œè§’è‰²æ„ŸçŸ¥å¾—åˆ†è’¸é¦é‡‡æ ·æŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†æ–‡æœ¬å¼•å¯¼å¼3Dè§’è‰²ç”Ÿæˆä¸­å­˜åœ¨çš„è¿‡é¥±å’Œé—®é¢˜å’Œè¾“å‡ºè´¨é‡ä½çš„é—®é¢˜ã€‚</p><p>æ€§èƒ½ï¼šåœ¨æ–‡æœ¬åˆ°3Då’Œæ–‡æœ¬åˆ°è§’è‰²ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒX-Oscaræ¡†æ¶åœ¨ç”Ÿæˆè´¨é‡å’ŒåŠ¨ç”»ä¿çœŸåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šX-Oscaræ¡†æ¶çš„å®ç°éœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºå’Œä¸“ä¸šçŸ¥è¯†ï¼Œå¯¹äºæ™®é€šç”¨æˆ·æ¥è¯´ï¼Œä½¿ç”¨å’Œéƒ¨ç½²å¯èƒ½å­˜åœ¨ä¸€å®šçš„éš¾åº¦ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4f631630c69e7fc1f5a8d28fd426ba1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aa682fc730b8fcb4e568e48a58c3a47.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-65e21e3a0a320adc36f81e6bfc7c5739.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-06  X-Oscar A Progressive Framework for High-quality Text-guided 3D   Animatable Avatar Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/</id>
    <published>2024-05-02T03:18:37.000Z</published>
    <updated>2024-05-02T03:18:37.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration"><a href="#NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration" class="headerlink" title="NeRF-Guided Unsupervised Learning of RGB-D Registration"></a>NeRF-Guided Unsupervised Learning of RGB-D Registration</h2><p><strong>Authors:Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</strong></p><p>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction. </p><p><a href="http://arxiv.org/abs/2405.00507v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF-UR æå‡ºäº†ä¸€ç§å¸§åˆ°æ¨¡å‹çš„ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ï¼Œåˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œä»¥æé«˜å¤šè§†å›¾ä¸€è‡´æ€§å·®æ—¶çš„é²æ£’æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ— ç›‘ç£ RGB-D é…å‡†çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URã€‚</li><li>ä½¿ç”¨ NeRF ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œæé«˜äº†å¤šè§†å›¾ä¸€è‡´æ€§å·®æ—¶çš„é²æ£’æ€§ã€‚</li><li>åˆ›å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›† Sim-RGBDï¼Œé€šè¿‡åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œæ— ç›‘ç£å¾®è°ƒï¼Œå°†ç‰¹å¾æå–å’Œæ³¨å†Œçš„èƒ½åŠ›ä»ä»¿çœŸè½¬ç§»åˆ°ç°å®ã€‚</li><li>åœ¨ ScanNet å’Œ 3DMatch æ•°æ®é›†ä¸Šï¼ŒNeRF-UR ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€ï¼Œä»¥æ–¹ä¾¿è®ºæ–‡å¤ç°ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>è®ºæ–‡æ ‡é¢˜ï¼šNeRFå¼•å¯¼çš„RGB-Dé…å‡†æ— ç›‘ç£å­¦ä¹ </li><p></p><p></p><li>ä½œè€…ï¼šZhinan Yu1âˆ—, Zheng Qin2âˆ—, Yijie Tang1, Yongjun Wang1, Renjiao Yi1, Chenyang Zhu1, and Kai Xu1â€ </li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šå›½é˜²ç§‘æŠ€å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šRGB-Dé…å‡†Â·æ— ç›‘ç£å­¦ä¹ Â·NeRF</li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šxxx</li><p></p><p></p><li>æ‘˜è¦ï¼š</li><br>&lt;/ol&gt;<p></p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€RGB-Dä¼ æ„Ÿå™¨çš„æ™®åŠå’Œæˆæœ¬çš„é™ä½ï¼Œ3Dæ•°æ®é‡‡é›†çš„éš¾åº¦å·²å¤§å¤§é™ä½ã€‚RGB-Dæ•°æ®çš„å¹¿æ³›æ”¶é›†æå¤§åœ°æ¨åŠ¨äº†æ·±åº¦å­¦ä¹ åœ¨3Dè§†è§‰é¢†åŸŸçš„è¿›æ­¥ï¼Œä»è€Œæå¤§åœ°æé«˜äº†RGB-D SLAMå’ŒRGB-Dé‡å»ºç­‰åº”ç”¨çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RGB-Dé…å‡†æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºå¯å¾®æ¸²æŸ“çš„æˆå¯¹è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¼šå› å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå…‰ææ–™ç­‰å› ç´ è€Œå¯¼è‡´å¤šè§†å›¾ä¸€è‡´æ€§è¾ƒå·®ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰çš„RGB-Dé…å‡†æ–¹æ³•é€šå¸¸é‡‡ç”¨åŸºäºå¯å¾®æ¸²æŸ“çš„æˆå¯¹è®­ç»ƒç­–ç•¥ï¼Œè¿™ä¼šå› å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå…‰ææ–™ç­‰å› ç´ è€Œå¯¼è‡´å¤šè§†å›¾ä¸€è‡´æ€§è¾ƒå·®ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶NeRF-URï¼Œç”¨äºæ— ç›‘ç£RGB-Dé…å‡†ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’ŒNeRFé‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§è¿›è¡Œä½å§¿ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼•å¯¼NeRFä¼˜åŒ–ï¼Œæœ¬æ–‡åˆ›å»ºäº†ä¸€ä¸ªé€šè¿‡é€¼çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åˆæˆæ•°æ®é›†Sim-RGBDï¼Œç”¨äºé¢„çƒ­é…å‡†æ¨¡å‹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæµè¡Œçš„å®¤å†…RGB-Dæ•°æ®é›†ScanNetå’Œ3DMatchä¸Šä¼˜äºæœ€å…ˆè¿›çš„åŒç±»æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§è¿›è¡Œä½å§¿ä¼˜åŒ–ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåˆ›å»ºé€šè¿‡é€¼çœŸæ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åˆæˆæ•°æ®é›† Sim-RGBDï¼Œç”¨äºé¢„çƒ­é…å‡†æ¨¡å‹ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨ PointMBF ä½œä¸ºé…å‡†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èåˆäº†è§†è§‰ï¼ˆ2Dï¼‰å’Œå‡ ä½•ï¼ˆ3Dï¼‰ç©ºé—´çš„ä¿¡æ¯ä»¥è·å¾—æ›´å¥½çš„ç‰¹å¾è¾¨åˆ«åŠ›ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šæå‡º NeRF å¼•å¯¼çš„æ— ç›‘ç£é…å‡†ç®¡é“ï¼Œè¯¥ç®¡é“ä¾èµ–äºé«˜è´¨é‡çš„ä½å§¿æ¥ç›‘ç£é…å‡†æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ NeRF ä¼˜åŒ–å¸§ä½å§¿ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šé‡‡ç”¨ NeRF æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¯¹åœºæ™¯ä¸­çš„å…‰ç…§å’Œå‡ ä½•ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è”åˆä¼˜åŒ– 3D åœ°å›¾å’Œä½å§¿ï¼›</p><p>ï¼ˆ7ï¼‰ï¼šå°† RGB-D åºåˆ—åˆ†å‰²æˆå­åºåˆ—ï¼Œå¹¶ä¸ºæ¯ä¸ªå­åºåˆ—ä¼˜åŒ–ä¸€ä¸ª NeRFï¼Œä»¥é¿å…è”åˆåœ°å›¾ä½å§¿ä¼˜åŒ–å¸¦æ¥çš„è¯¯å·®ç´¯ç§¯å’Œå·¨å¤§çš„æ—¶é—´å¼€é”€ï¼›</p><p>ï¼ˆ8ï¼‰ï¼šé€šè¿‡ Sim-RGBD æ•°æ®é›†å¯¹é…å‡†æ¨¡å‹è¿›è¡Œå¼•å¯¼ï¼Œä»¥æä¾›åˆç†çš„åˆå§‹ä½å§¿ï¼Œè§£å†³éšæœºåˆå§‹åŒ–é…å‡†æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¤§é‡å¼‚å¸¸å¯¹åº”å…³ç³»çš„é—®é¢˜ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1):è¯¥å·¥ä½œçš„æ„ä¹‰</strong></p><p>æœ¬æ–‡æå‡ºäº† NeRF-URï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ— ç›‘ç£ RGB-D é…å‡†çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§æ¥ä¼˜åŒ–é…å‡†æ¨¡å‹ä¼°è®¡çš„ä½å§¿ã€‚è¿™ç§è®¾è®¡å¯ä»¥æœ‰æ•ˆæé«˜å¯¹å…‰ç…§å˜åŒ–ã€å‡ ä½•é®æŒ¡å’Œåå°„ææ–™çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨åˆæˆæ•°æ®é›†ä¸Šè®¾è®¡äº†ä¸€ä¸ªå¼•å¯¼æœºåˆ¶æ¥é¢„çƒ­ NeRF ä¼˜åŒ–ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒNeRF å¼•å¯¼çš„æ— ç›‘ç£å­¦ä¹ æ˜¯ 3D è§†è§‰çš„ä¸€ç§æœ‰å‰é€”çš„æœºåˆ¶ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å°†å…¶æ‰©å±•åˆ°æ›´å¤šçš„ä»»åŠ¡ï¼Œå¦‚å®šä½ã€é‡å»ºç­‰ã€‚</p><p><strong>(2):æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¸§åˆ°æ¨¡å‹ä¼˜åŒ–æ¡†æ¶ NeRF-URï¼Œç”¨äºæ— ç›‘ç£ RGB-D é…å‡†ã€‚</li><li>åˆ©ç”¨ç¥ç»è¾å°„åœº (NeRF) ä½œä¸ºåœºæ™¯çš„å…¨å±€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¾“å…¥å¸§å’Œ NeRF é‡æ–°æ¸²æŸ“å¸§ä¹‹é—´çš„ä¸€è‡´æ€§æ¥ä¼˜åŒ–ä½å§¿ã€‚</li><li>è®¾è®¡äº†ä¸€ç§åˆæˆæ•°æ®é›†ä¸Šçš„å¼•å¯¼æœºåˆ¶æ¥é¢„çƒ­ NeRF ä¼˜åŒ–ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>åœ¨ä¸¤ä¸ªæµè¡Œçš„å®¤å†… RGB-D æ•°æ®é›† ScanNet å’Œ 3DMatch ä¸Šä¼˜äºæœ€å…ˆè¿›çš„åŒç±»æ–¹æ³•ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>éœ€è¦é¢„å…ˆè®­ç»ƒ NeRF æ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</li><li>ä¼˜åŒ– NeRF å’Œé…å‡†æ¨¡å‹æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-e47f5a6a35637f1b5b56609633d65083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d72291aca2a21454f9a83d46a2633ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-caf6df85382bbbd1a4f390f7bbbc79cb.jpg" align="middle"></details>## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions**Authors:Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan**Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. [PDF](http://arxiv.org/abs/2404.19015v1) The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955**Summary**ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åœºæ™¯çš„é€¼çœŸè‡ªç”±è§†ç‚¹æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æœ€è¿‘å¯¹ NeRF çš„æ”¹è¿›ï¼Œå¦‚ TensoRF å’Œ ZipNeRFï¼Œé‡‡ç”¨äº†æ˜¾å¼æ¨¡å‹ä»¥å®ç°æ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œè€Œ NeRF åˆ™é‡‡ç”¨äº†éšå¼è¡¨ç¤ºã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼çš„è¾å°„åœºéƒ½éœ€è¦å¯¹ç»™å®šåœºæ™¯ä¸­çš„å›¾åƒè¿›è¡Œå¯†é›†é‡‡æ ·ã€‚å½“åªæœ‰ç¨€ç–çš„è§†å›¾é›†åˆå¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œç›‘ç£è¾å°„åœºä¼°è®¡çš„æ·±åº¦æœ‰åŠ©äºä½¿ç”¨æ›´å°‘çš„è§†å›¾æœ‰æ•ˆåœ°è®­ç»ƒå®ƒã€‚æ·±åº¦ç›‘ç£æ˜¯ä½¿ç”¨ç»å…¸æ–¹æ³•æˆ–åœ¨å¤§æ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„ç¥ç»ç½‘ç»œè·å¾—çš„ã€‚è™½ç„¶å‰è€…å¯èƒ½åªæä¾›ç¨€ç–ç›‘ç£ï¼Œä½†åè€…å¯èƒ½å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚ä¸æ—©æœŸçš„æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬å¯»æ±‚é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶åœ¨ä¸»è¾å°„åœºä¸­è®­ç»ƒå®ƒä»¬æ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªæ­£åˆ™åŒ–æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨ä¸åŒçš„éšå¼å’Œæ˜¾å¼è¾å°„åœºä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥æƒ…å†µä¸‹è¿‡åº¦æ‹Ÿåˆè§‚æµ‹åˆ°çš„å›¾åƒã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯ï¼Œé™ä½è¾å°„åœºç›¸å¯¹äºä½ç½®ç¼–ç ã€åˆ†è§£å¼ é‡åˆ†é‡æ•°æˆ–å“ˆå¸Œè¡¨å¤§å°çš„èƒ½åŠ›ï¼Œä¼šé™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œåœ¨æŸäº›åŒºåŸŸä¼°è®¡æ›´å¥½çš„æ·±åº¦ã€‚é€šè¿‡åŸºäºè¿™ç§é™ä½çš„èƒ½åŠ›è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´å¥½çš„ä¸»è¾å°„åœºæ·±åº¦ç›‘ç£ã€‚é€šè¿‡ä½¿ç”¨ä¸Šè¿°æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬åœ¨åŒ…å«æœå‰å’Œ 360 åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šä»¥ç¨€ç–è¾“å…¥è§†å›¾å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚**Key Takeaways**-  å‡å°‘NeRFæ¨¡å‹å¤æ‚æ€§æœ‰åŠ©äºå­¦ä¹ æ›´å¥½çš„æ·±åº¦ï¼Œæœ‰åˆ©äºåˆ©ç”¨ç¨€ç–è§†å›¾è¿›è¡Œè®­ç»ƒã€‚-  è®¾è®¡å¢å¼ºæ¨¡å‹ï¼ŒåŸºäºé™ä½NeRFæ¨¡å‹èƒ½åŠ›è·å¾—æ”¹è¿›çš„æ·±åº¦ç›‘ç£ã€‚-  æ­£åˆ™åŒ–æ¡†æ¶å¯ä»¥åº”ç”¨äºä¸åŒç±»å‹NeRFæ¨¡å‹ï¼ŒåŒ…æ‹¬éšå¼å’Œæ˜¾å¼æ¨¡å‹ã€‚-  è¿‡åº¦æ‹Ÿåˆæ˜¯ç¨€ç–è§†å›¾è¾“å…¥NeRFè®­ç»ƒä¸­çš„ä¸»è¦é—®é¢˜ã€‚-  æ·±åº¦ç›‘ç£å¯ä»¥ä¿ƒè¿›NeRFæ¨¡å‹ä»ç¨€ç–è§†å›¾ä¸­è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚-  ç»å…¸æ–¹æ³•å’Œç¥ç»ç½‘ç»œéƒ½å¯ä»¥ç”¨äºæ·±åº¦ç›‘ç£ï¼Œä½†å„æœ‰ä¼˜ç¼ºç‚¹ã€‚-  åœ¨æœå‰å’Œ360åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: ç®€åŒ–å°„çº¿åœºï¼šç”¨æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆæ­£åˆ™åŒ–ç¨€ç–è¾“å…¥çš„è¾å°„åœº</p></li><li><p>Authors: Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan</p></li><li><p>Affiliation: å°åº¦ç§‘å­¦é™¢</p></li><li><p>Keywords: ç¥ç»è¾å°„åœº, ç¨€ç–è¾“å…¥, æ­£åˆ™åŒ–, æ·±åº¦ä¼°è®¡</p></li><li><p>Urls: https://arxiv.org/abs/2404.19015, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): ç¥ç»è¾å°„åœº(NeRF)åœ¨åœºæ™¯çš„çœŸå®æ„Ÿè‡ªç”±è§†è§’æ¸²æŸ“ä¸­è¡¨ç°å‡ºè‰²ã€‚NeRFçš„æœ€æ–°æ”¹è¿›ï¼Œå¦‚TensoRFå’ŒZipNeRFï¼Œé‡‡ç”¨äº†æ˜¾å¼æ¨¡å‹ä»¥å®ç°æ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œè€ŒNeRFåˆ™é‡‡ç”¨éšå¼è¡¨ç¤ºã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼è¾å°„åœºéƒ½éœ€è¦å¯¹ç»™å®šåœºæ™¯ä¸­çš„å›¾åƒè¿›è¡Œå¯†é›†é‡‡æ ·ã€‚å½“åªæœ‰ç¨€ç–çš„è§†å›¾é›†å¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾ç€ä¸‹é™ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œç›‘ç£è¾å°„åœºä¼°è®¡çš„æ·±åº¦æœ‰åŠ©äºæœ‰æ•ˆåœ°ä½¿ç”¨æ›´å°‘çš„è§†å›¾å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚æ·±åº¦ç›‘ç£æ˜¯ä½¿ç”¨ç»å…¸æ–¹æ³•æˆ–åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œè·å¾—çš„ã€‚è™½ç„¶å‰è€…å¯èƒ½åªæä¾›ç¨€ç–ç›‘ç£ï¼Œä½†åè€…å¯èƒ½å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚</p><p>(2): ä¸æ—©æœŸçš„ç ”ç©¶æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶å°†å…¶ä¸ä¸»è¾å°„åœºä¸€èµ·è®­ç»ƒæ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªæ­£åˆ™åŒ–æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸åŒçš„éšå¼å’Œæ˜¾å¼è¾å°„åœºä¸­å·¥ä½œã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥åœºæ™¯ä¸­è¿‡åº¦æ‹Ÿåˆè§‚æµ‹å›¾åƒã€‚æˆ‘ä»¬çš„ä¸»è¦å‘ç°æ˜¯ï¼Œåœ¨ä½ç½®ç¼–ç ã€åˆ†è§£çš„å¼ é‡åˆ†é‡æ•°æˆ–å“ˆå¸Œè¡¨å¤§å°æ–¹é¢é™ä½è¾å°„åœºçš„æ€§èƒ½ï¼Œä¼šé™åˆ¶æ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œåœ¨æŸäº›åŒºåŸŸä¼°è®¡å‡ºæ›´å¥½çš„æ·±åº¦ã€‚é€šè¿‡è®¾è®¡åŸºäºè¿™ç§é™ä½æ€§èƒ½çš„å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ºä¸»è¦è¾å°„åœºè·å¾—äº†æ›´å¥½çš„æ·±åº¦ç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡åœ¨åŒ…å«å‰è§†å’Œ360åº¦åœºæ™¯çš„æµè¡Œæ•°æ®é›†ä¸Šä½¿ç”¨ä¸Šè¿°æ­£åˆ™åŒ–ï¼Œåœ¨ç¨€ç–è¾“å…¥è§†å›¾ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ã€‚</p><p>(3): æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œå½“ä½¿ç”¨ç¨€ç–è¾“å…¥è§†å›¾è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¾å°„åœºæ¨¡å‹é€šå¸¸åˆ©ç”¨å®ƒä»¬çš„é«˜æ€§èƒ½æ¥å­¦ä¹ ä¸å¿…è¦çš„å¤æ‚è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶è¿™äº›è§£å†³æ–¹æ¡ˆå®Œç¾åœ°è§£é‡Šäº†è§‚æµ‹å›¾åƒï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šåœ¨æ–°è§†å›¾ä¸­é€ æˆä¸¥é‡çš„å¤±çœŸã€‚ä¾‹å¦‚ï¼ŒNeRFä¸­çš„ä¸€äº›å…³é”®ç»„ä»¶ï¼Œå¦‚NeRFä¸­çš„ä½ç½®ç¼–ç æˆ–TensoRFä¸­é‡‡ç”¨çš„å‘é‡çŸ©é˜µåˆ†è§£ï¼Œä¸ºè¾å°„åœºæä¾›äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶è¢«è®¾è®¡ä¸ºä½¿ç”¨å¯†é›†è¾“å…¥è§†å›¾è®­ç»ƒæ¨¡å‹ã€‚ç”±äºç³»ç»Ÿä¸¥é‡æ¬ çº¦æŸï¼Œè¿™äº›ç»„ä»¶çš„ç°æœ‰å®ç°å¯èƒ½åœ¨è¾“å…¥è§†å›¾è¾ƒå°‘çš„æƒ…å†µä¸‹ä¸ç†æƒ³ï¼Œä»è€Œå¯¼è‡´å¤šç§å¤±çœŸã€‚å›¾4ã€å›¾7å’Œå›¾8åˆ†åˆ«æ˜¾ç¤ºäº†NeRFã€TensoRFå’ŒZipNeRFåœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å¸¸è§çš„å¤±çœŸã€‚æˆ‘ä»¬éµå¾ªæµè¡Œçš„å¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼Œå¹¶å¯¹è¾å°„åœºè¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»¥åœ¨å¯èƒ½çš„æƒ…å†µä¸‹é€‰æ‹©æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡é™ä½è¾å°„åœºçš„æ€§èƒ½æ¥è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å‹ä¼°è®¡çš„æ·±åº¦æ¥ç›‘ç£ä¸»è¾å°„åœºã€‚æˆ‘ä»¬é’ˆå¯¹NeRFã€TensoRFå’ŒZipNeRFçš„ä¸åŒç¼ºç‚¹å’Œæ¶æ„è®¾è®¡äº†ä¸åŒçš„å¢å¼ºã€‚NeRFä¸­ä½¿ç”¨çš„é«˜ä½ç½®ç¼–ç åº¦ä¼šå¯¼è‡´ä¸å¸Œæœ›çš„æ·±åº¦ä¸è¿ç»­ï¼Œä»è€Œäº§ç”Ÿæµ®ç‚¹ã€‚æ­¤å¤–ï¼Œè§†ç‚¹ç›¸å…³çš„è¾å°„ç‰¹å¾ä¼šå¯¼è‡´å½¢çŠ¶è¾å°„æ¨¡ç³Šï¼Œä»è€Œäº§ç”Ÿé‡å¤ä¼ªå½±ã€‚æˆ‘ä»¬é€šè¿‡é™ä½ä½ç½®ç¼–ç åº¦å’Œç¦ç”¨è§†ç‚¹ç›¸å…³çš„è¾å°„ç‰¹å¾æ¥è®¾è®¡NeRFçš„å¢å¼ºã€‚å¦ä¸€æ–¹é¢ï¼ŒTensoRFä¸­å¤§é‡çš„é«˜åˆ†è¾¨ç‡åˆ†è§£ç»„ä»¶å’ŒZipNeRFä¸­çš„å¤§å“ˆå¸Œè¡¨ä¼šå¯¼è‡´è¿™äº›æ¨¡å‹åœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å‡ºç°æµ®ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¢å¼ºï¼Œä»¥é™åˆ¶æ¨¡å‹åœ¨è¿™äº›ç»„ä»¶æ–¹é¢å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†ç®€åŒ–çš„æ¨¡å‹ç”¨ä½œæ·±åº¦ç›‘ç£çš„å¢å¼ºï¼Œè€Œä¸æ˜¯ä½œä¸ºä¸»è¦çš„NeRFæ¨¡å‹ï¼Œå› ä¸ºç®€å•åœ°é™ä½è¾å°„åœºçš„æ€§èƒ½å¯èƒ½ä¼šå¯¼è‡´æŸäº›åŒºåŸŸçš„æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆ[Jain et al. 2021]ã€‚ä¾‹å¦‚ï¼Œåªèƒ½å­¦ä¹ å¹³æ»‘æ·±åº¦è½¬æ¢çš„æ¨¡å‹å¯èƒ½æ— æ³•å­¦ä¹ ç‰©ä½“è¾¹ç•Œå¤„çš„é”åˆ©æ·±åº¦ä¸è¿ç»­æ€§ã€‚æ­¤å¤–ï¼Œä»…å½“å¢å¼ºæ¨¡å‹å‡†ç¡®è§£é‡Šè§‚å¯Ÿåˆ°çš„å›¾åƒæ—¶ï¼Œæ‰éœ€è¦ä½¿ç”¨å®ƒä»¬è¿›è¡Œç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¼°è®¡çš„æ·±åº¦å°†åƒç´ é‡æ–°æŠ•å½±åˆ°ä¸åŒçš„æœ€è¿‘è®­ç»ƒè§†å›¾ä¸Šå¹¶å°†å…¶ä¸ç›¸åº”çš„å›¾åƒè¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡æ·±åº¦çš„å¯é æ€§ã€‚</p><p>(4): åœ¨NeRF-LLFFã€RealEstate-10Kå’ŒMipNeRF360æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ­£åˆ™åŒ–åœ¨NeRFã€TensoRFå’ŒZipNeRFæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚è¡¨1æ‰€ç¤ºã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒåŸå§‹è¾å°„åœºå­˜åœ¨å„ç§å¤±çœŸã€‚ä½¿ç”¨æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆå¯¹è¾å°„åœºè¿›è¡Œæ­£åˆ™åŒ–å¯ä»¥æ˜¾è‘—æ”¹å–„æ‰€æœ‰ä¸‰ä¸ªè¾å°„åœºçš„é‡å»ºã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):é€šè¿‡é™ä½è¾å°„åœºæ€§èƒ½æ¥è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ¨¡å‹ä¼°è®¡çš„æ·±åº¦æ¥ç›‘ç£ä¸»è¾å°„åœºï¼›            (2):é’ˆå¯¹NeRFã€TensoRFå’ŒZipNeRFçš„ä¸åŒç¼ºç‚¹å’Œæ¶æ„è®¾è®¡äº†ä¸åŒçš„å¢å¼ºï¼›            (3):é€šè¿‡ä½¿ç”¨ä¼°è®¡çš„æ·±åº¦å°†åƒç´ é‡æ–°æŠ•å½±åˆ°ä¸åŒçš„æœ€è¿‘è®­ç»ƒè§†å›¾ä¸Šå¹¶å°†å…¶ä¸ç›¸åº”çš„å›¾åƒè¿›è¡Œæ¯”è¾ƒæ¥è¡¡é‡æ·±åº¦çš„å¯é æ€§ï¼›            (4):åœ¨NeRF-LLFFã€RealEstate-10Kå’ŒMipNeRF360æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ­£åˆ™åŒ–åœ¨NeRFã€TensoRFå’ŒZipNeRFæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼›            .......</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡è§£å†³çš„æ˜¯é€šè¿‡ä»ä¸ä¸»è¾å°„åœºæ¨¡å‹åŒæ—¶è®­ç»ƒçš„ä½èƒ½åŠ›å¢å¼ºæ¨¡å‹å­¦ä¹ çš„æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆä¸­è·å¾—æ·±åº¦ç›‘ç£æ¥è§£å†³å°‘æ¬¡æ‹æ‘„è¾å°„åœºçš„é—®é¢˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯ä»¥ä¸ºéšå¼æ¨¡å‹ï¼ˆå¦‚ NeRFï¼‰å’Œæ˜¾å¼è¾å°„åœºï¼ˆå¦‚ TensoRF å’Œ ZipNeRFï¼‰è®¾è®¡å¢å¼ºã€‚ç”±äºå„ç§è¾å°„åœºçš„ç¼ºç‚¹ä¸åŒï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹é€‚å½“åœ°è®¾è®¡äº†å¢å¼ºã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¢å¼ºåœ¨æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ä¸Šéƒ½æ˜¾ç€æé«˜äº†æ€§èƒ½ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨å‰è§†åœºæ™¯å’Œ 360â—¦ åœºæ™¯ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åœºæ™¯çš„æ·±åº¦ä¼°è®¡æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ï¼Œè¿™å¯¹äºæ–°è§†å›¾åˆæˆå’Œåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡ä»å¢å¼ºæ¨¡å‹å­¦ä¹ çš„æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆä¸­è·å¾—æ·±åº¦ç›‘ç£æ¥æ­£åˆ™åŒ–è¾å°„åœºçš„æ–¹æ³•ï¼›æ€§èƒ½ï¼šåœ¨ NeRFã€TensoRF å’Œ ZipNeRF æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œåœ¨å°‘æ¬¡æ‹æ‘„è®¾ç½®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦è®¾è®¡é’ˆå¯¹ä¸åŒè¾å°„åœºæ¨¡å‹çš„å¢å¼ºï¼Œè¿™å¯èƒ½éœ€è¦é¢å¤–çš„å·¥ç¨‹å·¥ä½œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-5d84b090330526061fb59bb1dfc6ea7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a15c9e6ec9783c3b5ec66e4da9128f8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-52e8f6725bd512099b8ddbc432d73f2f.jpg" align="middle"></details><h2 id="Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields"><a href="#Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields"></a>Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields</h2><p><strong>Authors:Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</strong></p><p>Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at <a href="https://github.com/TQTQliu/GeFu">https://github.com/TQTQliu/GeFu</a> . </p><p><a href="http://arxiv.org/abs/2404.17528v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://gefucvpr24.github.io">https://gefucvpr24.github.io</a></p><p><strong>Summary</strong><br>æ–°æå‡ºæ–¹æ³•GeFué’ˆå¯¹NeRFæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³é—®é¢˜ï¼Œæå‡ºè‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰ã€ç©ºé—´è§†å›¾èšåˆï¼ˆSVAï¼‰å’Œä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰æœºåˆ¶ï¼Œé€šè¿‡æå‡å‡ ä½•é‡å»ºç²¾åº¦ã€ä¸°å¯Œæè¿°ç¬¦ä¿¡æ¯å’Œä¼˜åŒ–è§£ç ç­–ç•¥ï¼Œå¤§å¹…æå‡NeRFæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç°å­˜NeRFæ¨¡å‹æ³›åŒ–èƒ½åŠ›å—é™äºå‡ ä½•é‡å»ºä¸å‡†ã€æè¿°ç¬¦ä¿¡æ¯ä¸è¶³å’Œè§£ç ç­–ç•¥ä¸ä¼˜ã€‚</li><li>ACAæœºåˆ¶é€šè¿‡æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹ï¼Œæå‡ä»£ä»·ä½“ç²¾åº¦ã€‚</li><li>SVAæœºåˆ¶ç»“åˆç©ºé—´å’Œè§†å›¾ä¿¡æ¯ï¼Œä¸°å¯Œæè¿°ç¬¦ä¿¡æ¯ã€‚</li><li>CAFæœºåˆ¶èåˆä¸åŒè§£ç ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œæå‡è§£ç æ•ˆæœã€‚</li><li>GeFuæ¡†æ¶ç»“åˆACAã€SVAã€CAFæœºåˆ¶ï¼Œä»ç²—åˆ°ç²¾è¿›è¡Œå‡ ä½•é‡å»ºå’Œèåˆæ¸²æŸ“ã€‚</li><li>GeFuæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li><li>GeFuä»£ç å·²å¼€æºï¼ˆ<a href="https://github.com/TQTQliu/GeFuï¼‰ã€‚">https://github.com/TQTQliu/GeFuï¼‰ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šåŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šï¼‰</p></li><li><p>ä½œè€…ï¼šå¤©é½ åˆ˜ï¼Œæ·»ç¿¼ å†¯ï¼Œå°æ˜ è‘£ï¼Œå˜‰é¹ å¼ ï¼Œå¿—ä¼Ÿ å†¯ï¼Œæ° æ½˜ï¼ŒæŒ¯ç¾½ ç‹ï¼Œå¿—ä¼Ÿ å†¯ï¼ˆTianqi Liu, Tianyi Feng, Xiaoming Dong, Jiapeng Zhang, Zhiwei Feng, Jie Pan, Zhenyu Wang, Zhiwei Fengï¼‰</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šåŒ—äº¬å¤§å­¦ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šåŒ—äº¬å¤§å­¦ï¼‰</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼Œå¤šè§†å›¾é‡å»ºï¼Œç¥ç»æ¸²æŸ“ï¼Œåœºæ™¯ç†è§£</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šæˆ–Githubä»£ç é“¾æ¥ï¼ˆè‹¥æœ‰ï¼Œåˆ™å¡«å†™ï¼Œè‹¥æ— ï¼Œåˆ™å¡«å†™Github:Noneï¼‰ï¼šGithub:None</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å¯ä»¥ä»å¤šè§†å›¾å›¾åƒä¸­é‡å»º3Dåœºæ™¯ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼ˆå¦‚é®æŒ¡æˆ–åå°„ï¼‰è¡¨ç°å‡ºæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡æ„å»ºåŸºäºæ–¹å·®çš„ä»£ä»·ä½“ç§¯è¿›è¡Œå‡ ä½•é‡å»ºï¼Œå¹¶ç¼–ç 3Dæè¿°ç¬¦è¿›è¡Œæ–°è§†å›¾è§£ç ã€‚ä½†è¿™äº›æ–¹æ³•å­˜åœ¨å‡ ä½•ä¸å‡†ç¡®ã€æè¿°ç¬¦æ¬¡ä¼˜å’Œè§£ç ç­–ç•¥ä¸ä½³çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“ï¼ˆGeFuï¼‰æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼›å¼•å…¥ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰é€šè¿‡ç©ºé—´å’Œè§†å›¾é—´çš„äº¤äº’å°†3Dä¸Šä¸‹æ–‡èå…¥æè¿°ç¬¦ï¼›æå‡ºäº†ä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨äº†ä¸¤ç§ç°æœ‰è§£ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒGeFuåœ¨å¤šè§†å›¾é‡å»ºå’Œæ–°è§†å›¾æ¸²æŸ“ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³æé«˜NeRFåœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‡ ä½•çš„é‡å»ºå’Œèåˆç²¾ä¿®æ¸²æŸ“æ¡†æ¶ï¼ˆGeFuï¼‰ï¼Œé€šè¿‡è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ”¾å¤§ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼ŒæŠ‘åˆ¶ä¸ä¸€è‡´åƒç´ å¯¹çš„è´¡çŒ®ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¼•å…¥ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰é€šè¿‡ç©ºé—´å’Œè§†å›¾é—´çš„äº¤äº’å°†3Dä¸Šä¸‹æ–‡èå…¥æè¿°ç¬¦ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºäº†ä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨äº†ä¸¤ç§ç°æœ‰è§£ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„NeRFæ–¹æ³•ï¼Œèƒ½å¤Ÿå®ç°é«˜ä¿çœŸè§†å›¾åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨é‡å»ºé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰æ¥æ”¹å–„å‡ ä½•ä¼°è®¡ï¼Œå¹¶æå‡ºäº†ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰æ¥ç¼–ç 3Dä¸Šä¸‹æ–‡æ„ŸçŸ¥æè¿°ç¬¦ã€‚åœ¨æ¸²æŸ“é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†Consistency-Aware Fusionï¼ˆCAFï¼‰æ¨¡å—ï¼Œä»¥ç»Ÿä¸€å…¶ä¼˜åŠ¿æ¥ä¼˜åŒ–åˆæˆè§†å›¾è´¨é‡ã€‚æˆ‘ä»¬å°†è¿™äº›æ¨¡å—æ•´åˆåˆ°ä¸€ä¸ªç²—åˆ°ç»†çš„æ¡†æ¶ä¸­ï¼Œç§°ä¸ºGeFuã€‚å¹¿æ³›çš„è¯„ä¼°å’Œæ¶ˆèå®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºè‡ªé€‚åº”ä»£ä»·èšåˆï¼ˆACAï¼‰ã€ç©ºé—´-è§†å›¾èšåˆå™¨ï¼ˆSVAï¼‰å’Œä¸€è‡´æ€§æ„ŸçŸ¥èåˆï¼ˆCAFï¼‰æ¨¡å—ï¼Œæé«˜äº†NeRFåœ¨å…·æœ‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼›æ€§èƒ½ï¼šåœ¨å¤šè§†å›¾é‡å»ºå’Œæ–°è§†å›¾æ¸²æŸ“ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦ä¿®æ”¹NeRFçš„é‡å»ºå’Œæ¸²æŸ“æµç¨‹ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f615e4a52c82bbd89b40d674212ac03c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccb26edee482b262ae1661c51b02d1d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3ded1a62b2132a2c5b5fdd26dc30947d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cbccb86ceb95a77c9f32e543fe79fbf0.jpg" align="middle"></details><h2 id="Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery"><a href="#Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery" class="headerlink" title="Depth Supervised Neural Surface Reconstruction from Airborne Imagery"></a>Depth Supervised Neural Surface Reconstruction from Airborne Imagery</h2><p><strong>Authors:Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala</strong></p><p>While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images. </p><p><a href="http://arxiv.org/abs/2404.16429v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰ä½œä¸ºå¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰çš„æ›¿ä»£æ–¹æ³•ï¼Œåœ¨ç©ºä¸­åœºæ™¯ä¸‰ç»´é‡å»ºä¸­å±•ç°å‡º promising çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ— çº¹ç†ã€é€æ˜å’Œåå°„è¡¨é¢ç­‰ä¼ ç»Ÿ MVSéš¾ä»¥å¤„ç†çš„åœºæ™¯æ—¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs åœ¨ç©ºä¸­å›¾åƒå—ï¼ˆåŒ…æ‹¬ä»… nadirã€å€¾æ–œå’Œé«˜åˆ†è¾¨ç‡å›¾åƒï¼‰çš„ä¸‰ç»´é‡å»ºä¸­å…·æœ‰é€‚ç”¨æ€§ã€‚</li><li>é›†æˆå¹³å·®å—è°ƒæ•´ä¸­æä¾›çš„è”ç³»ç‚¹æµ‹é‡æ·±åº¦å…ˆéªŒä¿¡æ¯å¯ä»¥æå‡é‡å»ºæ•ˆæœã€‚</li><li>ä½¿ç”¨åŸºäºç¬¦å·è·ç¦»å‡½æ•° (SDF) çš„ VolSDF æ¡†æ¶è¿›è¡Œé‡å»ºï¼Œæ›´é€‚ç”¨äºè¡¨é¢é‡å»ºã€‚</li><li>NeRFs åœ¨å›¾åƒå†—ä½™åº¦ä½å’Œæ•°æ®è¯æ®å¼±çš„åŒºåŸŸï¼ˆå¦‚è¡—é“å³¡è°·ã€ç«‹é¢æˆ–å»ºç­‘é˜´å½±ï¼‰å­˜åœ¨å›°éš¾ã€‚</li><li>è®­ç»ƒ NeRFs è®¡ç®—æˆæœ¬é«˜ã€‚</li><li>åœ¨ç©ºä¸­åœºæ™¯çš„ä¸‰ç»´é‡å»ºä¸­ï¼ŒNeRFs é¢ä¸´ä½å›¾åƒå†—ä½™åº¦å’Œå¼±æ•°æ®è¯æ®çš„æŒ‘æˆ˜ã€‚</li><li>åœ¨ä»…ä½¿ç”¨ nadir å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒNeRFs çš„é‡å»ºæ€§èƒ½ä½äºä½¿ç”¨å€¾æ–œå›¾åƒæˆ–é«˜åˆ†è¾¨ç‡å›¾åƒçš„æƒ…å†µã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šèˆªç©ºå½±åƒçš„æ·±åº¦ç›‘ç£ç¥ç»è¡¨é¢é‡å»º</p></li><li><p>ä½œè€…ï¼šV. Hacksteinã€P. Fauth-Mayerã€M. Rothermelã€N. Haala</p></li><li><p>æ‰€å±æœºæ„ï¼šnFrames ESRIï¼ˆå¾·å›½ï¼‰</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€å¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰ã€3D åœºæ™¯é‡å»ºã€ç½‘æ ¼åŒ– 3D ç‚¹äº‘ã€èˆªç©ºå½±åƒã€æ·±åº¦ç›‘ç£</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.16429 , Github é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æœ€åˆç”¨äºæ–°é¢–è§†å›¾åˆæˆï¼Œç°å·²æˆä¸ºå¤šè§†ç«‹ä½“ï¼ˆMVSï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚NeRF å°¤å…¶é€‚ç”¨äºæ— çº¹ç†ã€é€æ˜å’Œåå…‰è¡¨é¢ï¼Œè€Œè¿™äº›åœºæ™¯å¯¹äºåŸºäº MVS çš„ä¼ ç»Ÿæ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶å…³æ³¨è¿‘è·ç¦»åœºæ™¯ï¼Œè€Œé’ˆå¯¹èˆªç©ºåœºæ™¯çš„ç ”ç©¶ä»ç„¶ç¼ºå¤±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼šä¼ ç»Ÿçš„ MVS æ–¹æ³•åœ¨ç²¾ç»†å‡ ä½•ç»“æ„ã€æ— çº¹ç†åŒºåŸŸå’Œéæœ—ä¼¯è¡¨é¢ï¼ˆä¾‹å¦‚åŠé€æ˜ç‰©ä½“æˆ–åå°„ï¼‰å¤„å­˜åœ¨é—®é¢˜ã€‚åŠ¨æœºå……åˆ†ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç›‘ç£çš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡ä¿®æ”¹äº† VolSDF æ¡†æ¶ï¼Œå°† SfM å…³è”ç‚¹ç›‘ç£æ•´åˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ”¯æŒè®­ç»ƒè¿‡ç¨‹ã€‚VolSDF ä½¿ç”¨ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰å¯¹ 3D åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ¯”é¦™è‰ NeRF ä¸­çš„æ ‡å‡†ä½“ç§¯è¡¨ç¤ºæ›´é€‚ç”¨äºè¡¨é¢é‡å»ºä»»åŠ¡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡å’Œæ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡é’ˆå¯¹ä¸‰ç§èˆªç©ºå›¾åƒé›†è¯„ä¼°äº†è¯¥ç®¡é“ï¼Œè¿™äº›å›¾åƒé›†å…·æœ‰ä¸åŒçš„é…ç½®ã€‚åœ¨ä¸“ä¸šèˆªç©ºæµ‹ç»˜ä¸­é€šå¸¸ä½¿ç”¨çš„æ•°æ®ä¸Šçš„è¿™äº›ç ”ç©¶ä»å®é™…è§’åº¦å‡ºå‘å¾ˆæœ‰è¶£ï¼ŒåŒæ—¶ç ”ç©¶äº†åŸºäº NeRF çš„è¡¨é¢é‡å»ºçš„å…·ä½“æŒ‘æˆ˜ã€‚æ­¤ç±»èˆªç©ºå›¾åƒé›†åˆçš„è§†è§’æœ‰é™ï¼Œå¹¶ä¸”å¯èƒ½å› è¡—é“å³¡è°·ã€ç«‹é¢æˆ–å»ºç­‘ç‰©é˜´å½±è€Œå—åˆ°å½±å“ã€‚è¯¥æ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰å›é¡¾ VolSDFï¼ˆç¥ç»è¾å°„åœºï¼‰ï¼›</p><p>ï¼ˆ2ï¼‰VolSDF çš„ SDFï¼ˆç¬¦å·è·ç¦»å‡½æ•°ï¼‰è¡¨ç¤ºï¼›</p><p>ï¼ˆ3ï¼‰VolSDF çš„æ­£åˆ™åŒ–ï¼›</p><p>ï¼ˆ4ï¼‰VolSDF çš„é‡‡æ ·ï¼›</p><p>ï¼ˆ5ï¼‰Tie ç‚¹ç›‘ç£ï¼š    ï¼ˆaï¼‰Tie ç‚¹åˆå§‹åŒ–å’Œç›‘ç£ï¼›    ï¼ˆbï¼‰æ·±åº¦ç›‘ç£æŸå¤±å‡½æ•°ï¼šLtr å’Œ Lfsï¼›</p><p>ï¼ˆ6ï¼‰å®ç°å’Œè®­ç»ƒç»†èŠ‚ï¼š    ï¼ˆaï¼‰æ¨¡å‹ç»“æ„ï¼›    ï¼ˆbï¼‰è®­ç»ƒæŸå¤±å‡½æ•°ï¼šL = LRGB + Î»eikLeik + Î»surfLsurf +Î»fsLfs + Î»trLtrï¼›</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å±•ç¤ºäº† VolSDFï¼ˆç¥ç»è¾å°„åœºçš„ä¸€ç§å˜ä½“ï¼Œç”¨äºå»ºæ¨¡éšå¼ç¥ç»è¡¨é¢ï¼‰åœ¨èˆªç©ºå›¾åƒä¸‰ç»´é‡å»ºä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†é€šè¿‡å…³è”ç‚¹ç›‘ç£ VolSDF å¯ä»¥æ”¹å–„é‡å»ºæ•ˆæœï¼šæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨æ—©æœŸè®­ç»ƒé˜¶æ®µæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”åœ¨å®Œæ•´æ€§å’Œå‡†ç¡®æ€§æ–¹é¢è´¨é‡æ›´å¥½ã€‚è¿™å°¤å…¶é€‚ç”¨äºä»…å…·æœ‰æœ‰é™æ•°æ®è¯æ®çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸï¼Œå¯¹äºè¿™äº›åŒºåŸŸï¼ŒVolSDF å¾€å¾€ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼æˆ–æ ¹æœ¬æ— æ³•æ”¶æ•›ã€‚ä¸€ä¸ªç¤ºä¾‹æ­£å°„åœºæ™¯çš„é‡å»ºè¡¨é¢åœ¨ NMAD æ–¹é¢æ¯”ä¼ ç»Ÿçš„ MVS ç®¡é“å°‘äº 4 ä¸ª GSD åå·®ã€‚ä¸ºäº†å®Œå…¨æ”¶æ•›å¹¶æ¢å¤å…¨éƒ¨ç»†èŠ‚ï¼Œä»ç„¶éœ€è¦å»¶é•¿è®­ç»ƒæ—¶é—´ã€‚è¿™é˜»ç¢äº†å®é™…åº”ç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…è·å¾—æ‹“æ‰‘æ­£ç¡®çš„è¡¨é¢ï¼Œè¿™äº›è¡¨é¢å¯ä»¥è¿›è¡Œåç»­ç½‘æ ¼åå¤„ç†ã€‚é‡‡æ ·ä¾‹ç¨‹æ˜¯è¯„ä¼°å®æ–½ä¸­çš„ä¸»è¦ç“¶é¢ˆï¼Œå¹¶å°†åœ¨æœªæ¥å·¥ä½œä¸­è¿›è¡Œæ”¹è¿›ã€‚ä¸€æ–¹é¢ï¼Œé«˜æ•ˆçš„ GPU å®ç°å¯ä»¥åŠ é€Ÿè¿™ä¸€è¿‡ç¨‹ï¼ˆWang ç­‰äººï¼Œ2023 å¹´ï¼‰ï¼Œå¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¸Œæœ›ç ”ç©¶åœ¨æœ‰å¾ˆå¤§æ”¹è¿›æ½œåŠ›çš„åŒºåŸŸåŠ¨æ€å¢å¼ºé‡‡æ ·çš„å¯èƒ½æ€§ï¼ˆKerbl ç­‰äººï¼Œ2023 å¹´ï¼‰ã€‚ç¥ç»éšå¼è¡¨é¢é‡å»ºä»ç„¶æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶è¯¾é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬æ–‡ä¹Ÿèƒ½é¼“åŠ±åœ¨èˆªç©ºå›¾åƒå’Œå…¶ä»–é¥æ„Ÿåº”ç”¨çš„å‡ ä½•é‡å»ºé¢†åŸŸå¼€å±•æœªæ¥çš„å·¥ä½œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ·±åº¦ç›‘ç£çš„ç¥ç»è¡¨é¢é‡å»ºæ–¹æ³•ï¼Œå°† SfM å…³è”ç‚¹ç›‘ç£æ•´åˆåˆ° VolSDF è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ”¯æŒè®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜äº†é‡å»ºè´¨é‡ï¼›</p><p>æ€§èƒ½ï¼šåœ¨èˆªç©ºå›¾åƒé›†ä¸Šè¯„ä¼°äº†è¯¥ç®¡é“ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸï¼ˆä¾‹å¦‚æ— çº¹ç†åŒºåŸŸã€éæœ—ä¼¯è¡¨é¢ï¼‰å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ï¼›</p><p>å·¥ä½œé‡ï¼šéœ€è¦è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´æ‰èƒ½å®Œå…¨æ”¶æ•›å¹¶æ¢å¤å…¨éƒ¨ç»†èŠ‚ï¼Œè¿™é˜»ç¢äº†å®é™…åº”ç”¨ï¼Œé‡‡æ ·ä¾‹ç¨‹æ˜¯è¯„ä¼°å®æ–½ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-4036313ed6644db70c73439252a5eaed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e26afc3f9b57484514d8f583efe4569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6ddc84a2cf8fcf12f4f1a29a529e7de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14b2f3ca89df5d53f911326b2d3382d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77dd44d2a901985d20406c555ff9eb2c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>åŸºäºä¸‰ç»´é«˜æ–¯æ»´çš„è¯­éŸ³é©±åŠ¨çš„è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ˜¾å¼è¡¨ç¤ºå’Œå¯¹ä¸‰ç»´é¢éƒ¨æ¨¡å‹çš„é«˜æ–¯å…³è”å®ç°ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨å’Œé¢éƒ¨ç»†èŠ‚å¢å¼ºï¼Œå±•ç°å‡ºå®æ—¶æ¸²æŸ“æ€§èƒ½å’Œå“è¶Šçš„è§†è§‰æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>é‡‡ç”¨ä¸‰ç»´é«˜æ–¯æ»´çš„æ˜¾å¼è¡¨ç¤ºï¼Œè§£å†³äº†NeRFå†…éšè¡¨ç¤ºçš„å§¿æ€å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³é—®é¢˜ã€‚</li><li>æå‡ºçš„è¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨çš„éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶çš„å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°äº†é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œé€šè¿‡æ½œåœ¨å§¿åŠ¿å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºï¼Œæä¾›ç¨³å®šä¸”é€¼çœŸçš„æ¸²æŸ“è§†é¢‘ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹é¢ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†ç²¾ç¡®çš„å”‡éƒ¨åŒæ­¥å’Œå‡ºè‰²çš„è§†è§‰æ•ˆæœã€‚</li><li>è¯¥æ–¹æ³•åœ¨ NVIDIA RTX4090 GPU ä¸Šå®ç°äº† 130 FPS çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜¾ç€è¶…è¿‡äº†å®æ—¶æ¸²æŸ“æ€§èƒ½çš„é˜ˆå€¼ï¼Œå¹¶æœ‰å¯èƒ½éƒ¨ç½²åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalkerï¼šåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººä¸“å±ä¼šè¯´è¯çš„å¤´åƒåˆæˆ</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: é˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific Motion Translator, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p><p>(1): è¿‘æœŸåŸºäºç¥ç»è¾å°„åœº(NeRF)çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹æ³•å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚ç„¶è€Œï¼Œå—é™äºNeRFéšå¼è¡¨ç¤ºå¯¹å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»å­˜åœ¨å”‡éƒ¨è¿åŠ¨ä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ç­‰é—®é¢˜ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šåŸºäºNeRFçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹æ³•ï¼›é—®é¢˜ï¼šå§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œå¯¼è‡´å”‡éƒ¨è¿åŠ¨ä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå……åˆ†ï¼Œæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼šGaussianTalkerï¼Œè¯¥æ–¹æ³•ç”±è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ä¸¤ä¸ªæ¨¡å—ç»„æˆã€‚è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººä¸“å±æ··åˆå½¢çŠ¶ï¼Œå°†é«˜æ–¯ç‚¹äº‘ä¸3Dé¢éƒ¨æ¨¡å‹ç»‘å®šï¼Œå®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚</p><p>(4): è¯¥æ–¹æ³•åœ¨éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚è¯¥æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³å®ç°è‡ªç„¶é€¼çœŸçš„è¯´è¯äººå¤´åƒåˆæˆã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼›</p><p>ï¼ˆ2ï¼‰ï¼šGaussianTalkerç”±è¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè¯´è¯äººä¸“å±è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººä¸“å±æ··åˆå½¢çŠ¶ï¼Œå°†é«˜æ–¯ç‚¹äº‘ä¸3Dé¢éƒ¨æ¨¡å‹ç»‘å®šï¼Œå®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šè¯¥æ–¹æ³•åœ¨éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯äººå…³è”èµ·æ¥ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3Dç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚è¯´è¯äººä¸“å±FLAMEè½¬æ¢å™¨é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥ï¼Œä»¥å®ç°åŒæ­¥å’Œè‡ªç„¶çš„å”‡éƒ¨è¿åŠ¨ï¼Œè€ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿ä¼˜åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šå’Œé€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalkeråœ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¿œè¶…å…¶ä»–æ–¹æ³•çš„è¶…é«˜æ¸²æŸ“é€Ÿåº¦ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²çš„åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•GaussianTalkerï¼›æ€§èƒ½ï¼šåœ¨è¯´è¯äººå¤´åƒåˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†è¿œè¶…å…¶ä»–æ–¹æ³•çš„è¶…é«˜æ¸²æŸ“é€Ÿåº¦ï¼›å·¥ä½œé‡ï¼š......</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRFâ€™s applications in the context of AD. Our survey is structured to categorize NeRFâ€™s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v2">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„è¯¸å¤šåº”ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæ˜¾ç¤ºå‡ºå¹¿é˜”æ½œèƒ½ï¼Œåº”ç”¨æ¶µç›–æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸã€‚</li><li>NeRFæ„ŸçŸ¥åº”ç”¨åŒ…æ‹¬ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²å’Œè·Ÿè¸ªã€‚</li><li>NeRFä¸‰ç»´é‡å»ºåº”ç”¨å¯ç”Ÿæˆé«˜ä¿çœŸä¸‰ç»´åœºæ™¯ã€‚</li><li>NeRF SLAM èåˆäº†æ„ŸçŸ¥å’Œé‡å»ºï¼Œå®æ—¶åˆ›å»ºç¯å¢ƒåœ°å›¾ã€‚</li><li>NeRFä»¿çœŸåº”ç”¨å¯åˆ›é€ é€¼çœŸçš„è™šæ‹Ÿç¯å¢ƒï¼Œç”¨äºä¼ æ„Ÿå™¨å’Œç®—æ³•æµ‹è¯•ã€‚</li><li>ç ”ç©¶çƒ­ç‚¹åŒ…æ‹¬è·¨æ¨¡æ€èåˆã€é«˜æ•ˆè¡¨ç¤ºå’ŒåŠ¨æ€åœºæ™¯å¤„ç†ã€‚</li><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ä»å¤„äºæ—©æœŸé˜¶æ®µï¼Œé¢ä¸´æŒ‘æˆ˜å’Œæœºé‡ã€‚</li><li>æœªæ¥æ–¹å‘åŒ…æ‹¬é«˜ç²¾åº¦ã€é²æ£’æ€§å’Œå®æ—¶æ€§èƒ½ä¼˜åŒ–ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼šç»¼è¿°</p></li><li><p>ä½œè€…ï¼šé›·è´ºã€æä¹æ’ã€å­™æ–‡è¶…ã€éŸ©æ³½å®‡ã€åˆ˜ä¸€è¾°ã€éƒ‘æ€å‘ã€ç‹å»ºå¼ºã€æå…‹å¼º</p></li><li><p>Affiliation: æ¸…åå¤§å­¦è½¦è¾†ä¸è¿è½½å­¦é™¢</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.13816 ,Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å‡­å€Ÿå…¶å†…åœ¨ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å…¶éšå¼è¡¨ç¤ºå’Œæ–°é¢–çš„è§†å›¾åˆæˆèƒ½åŠ›ï¼Œåœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œéƒ½å¤‡å—å…³æ³¨ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œå¤§é‡æ–¹æ³•æ¶Œç°å‡ºæ¥ï¼Œæ¢ç´¢ NeRF åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰æ–‡çŒ®ä¸­å­˜åœ¨æ˜æ˜¾çš„ç©ºç™½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¯¹ NeRF åœ¨ AD ä¸­çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥æ—¨åœ¨å¯¹ NeRF åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ä¸­çš„åº”ç”¨è¿›è¡Œåˆ†ç±»ï¼Œç‰¹åˆ«æ˜¯åŒ…æ‹¬æ„ŸçŸ¥ã€3D é‡å»ºã€åŒæ—¶å®šä½å’Œå»ºå›¾ (SLAM) ä»¥åŠä»¿çœŸã€‚æˆ‘ä»¬æ·±å…¥åˆ†æå¹¶æ€»ç»“äº†æ¯ä¸ªåº”ç”¨ç±»åˆ«çš„å‘ç°ï¼Œå¹¶é€šè¿‡æä¾›å¯¹è¯¥é¢†åŸŸæœªæ¥æ–¹å‘çš„è§è§£å’Œè®¨è®ºæ¥ç»“æŸã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡èƒ½ä¸ºè¯¥é¢†åŸŸçš„çš„ç ”ç©¶äººå‘˜æä¾›å…¨é¢çš„å‚è€ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡ä¸“é—¨é’ˆå¯¹ NeRF åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨çš„ç»¼è¿°ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºé«˜ç²¾åº¦åœ°å›¾æ¥æä¾›é™æ€åœºæ™¯ç†è§£ï¼Œç°åœ¨å¼ºè°ƒé€šè¿‡é¸Ÿç°è§†è§‰å®æ—¶æ„ŸçŸ¥å±€éƒ¨ç¯å¢ƒã€‚åŒæ—¶ï¼Œå®ƒåœ¨åŠŸèƒ½ä¸Šå·²ä» 2 çº§ï¼ˆL2ï¼‰å‘å±•åˆ°åŠªåŠ›å®ç° 4 çº§ï¼ˆL4ï¼‰è‡ªåŠ¨é©¾é©¶ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè¦æ±‚å¯¹å‘¨å›´ç¯å¢ƒæœ‰æ·±å…¥çš„äº†è§£ï¼ŒåŒ…æ‹¬é™æ€åœºæ™¯å’Œäº¤é€šå‚ä¸è€…ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œè¿™æ˜¯æœ‰æ•ˆè§„åˆ’å’Œæ§åˆ¶çš„å…³é”®å‰æã€‚NeRF é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼Œå·²è¯æ˜å…¶æœ‰æ•ˆç†è§£å±€éƒ¨åœºæ™¯çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºè‡ªåŠ¨é©¾é©¶èƒ½åŠ›çš„è¯±äººå€™é€‰è€…ã€‚åœ¨è¿‡å»ä¸¤å¹´ä¸­ï¼ŒNeRF æ¨¡å‹å·²åœ¨è‡ªåŠ¨é©¾é©¶çš„å„ä¸ªæ–¹é¢å¾—åˆ°äº†åº”ç”¨ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€3D é‡å»ºã€åŒæ—¶å®šä½å’Œå»ºå›¾ (SLAM) ä»¥åŠä»¿çœŸï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚</p><p>(3):ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²æˆä¸ºæ„ŸçŸ¥é¢†åŸŸçš„å¾ˆæœ‰å¸Œæœ›çš„ç«äº‰è€…ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—å…³é”®ä»»åŠ¡ï¼Œä¾‹å¦‚å¯¹è±¡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œå æ®é¢„æµ‹ã€‚å®ƒäººæ°”é£™å‡çš„ä¸»è¦åŸå› æ˜¯å®ƒèƒ½å¤Ÿè·å–ç²¾ç¡®ä¸”ä¸€è‡´çš„å‡ ä½•ä¿¡æ¯ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶å¯åˆ†ä¸ºä¸¤å¤§èŒƒå¼ï¼ŒåŒºåˆ«åœ¨äº NeRF çš„åˆ©ç”¨ï¼šâ€œNeRF for dataâ€å’Œâ€œNeRF for modelâ€ã€‚å‰è€…æ¶‰åŠ NeRF çš„åˆå§‹è®­ç»ƒï¼Œç„¶åå°†å…¶ç”¨äºæ‰©å……æ„ŸçŸ¥ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåè€…é‡‡ç”¨ NeRF å’Œæ„ŸçŸ¥ç½‘ç»œçš„ååŒè®­ç»ƒç­–ç•¥ï¼Œä½¿æ„ŸçŸ¥ç½‘ç»œèƒ½å¤Ÿå­¦ä¹  NeRF æ•è·çš„å‡ ä½•ä¿¡æ¯ã€‚</p><p>(4):åœ¨ 3D é‡å»ºåº”ç”¨é¢†åŸŸï¼ŒNeRF å¯ä»¥æ ¹æ®åœºæ™¯ç†è§£çš„çº§åˆ«åˆ†ä¸ºä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šåŠ¨æ€åœºæ™¯é‡å»ºã€è¡¨é¢é‡å»ºå’Œé€†å‘æ¸²æŸ“ã€‚åœ¨ç¬¬ä¸€ç±»ä¸­ï¼ŒåŠ¨æ€åœºæ™¯é‡å»ºä¾§é‡äºé‡å»ºå…·æœ‰å¯ç§»åŠ¨ä»£ç†çš„åŠ¨æ€åœºæ™¯ï¼Œä¸»è¦ä½¿ç”¨é¡ºåº 3D è¾¹ç•Œæ¡†æ³¨é‡Šå’Œç›¸æœºå‚æ•°ã€‚åœ¨ç¬¬äºŒç±»ä¸­ï¼Œè¡¨é¢é‡å»ºæ—¨åœ¨é‡å»ºåœºæ™¯çš„æ˜¾å¼ 3D è¡¨é¢ï¼Œä¾‹å¦‚ç½‘æ ¼ã€‚åœ¨ç¬¬ä¸‰ç±»ä¸­ï¼Œé€†å‘æ¸²æŸ“æ—¨åœ¨ä»é©¾é©¶åœºæ™¯çš„å›¾åƒä¸­è§£å¼€å½¢çŠ¶ã€åç…§ç‡å’Œå¯è§æ€§ï¼Œä»¥å®ç°è¯¸å¦‚é‡æ–°ç…§æ˜ä¹‹ç±»çš„åº”ç”¨ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li><strong>ç»“è®º</strong></li></ol><p>ï¼ˆ1ï¼‰æœ¬ç»¼è¿°å·¥ä½œå¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„æ€»ç»“ï¼Œå¡«è¡¥äº†å½“å‰æ–‡çŒ®ä¸­çš„ç©ºç™½ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›äº†å…¨é¢çš„å‚è€ƒã€‚</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šæœ¬ç»¼è¿°é¦–æ¬¡ä¸“é—¨é’ˆå¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼›æ€§èƒ½ï¼šå¯¹ç¥ç»è¾å°„åœºåœ¨æ„ŸçŸ¥ã€3D é‡å»ºã€SLAM å’Œä»¿çœŸç­‰é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†æ·±å…¥åˆ†æå’Œæ€»ç»“ï¼›å·¥ä½œé‡ï¼šå·¥ä½œé‡å¤§ï¼Œæ¶‰åŠæ–‡çŒ®å¹¿æ³›ï¼Œåˆ†ææ·±å…¥ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-55b475e228eebb497768f57fb097059d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-22321e24e9114a3aa3b89b16e6ff76f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://picx.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  NeRF-Guided Unsupervised Learning of RGB-D Registration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/</id>
    <published>2024-05-02T03:01:07.000Z</published>
    <updated>2024-05-02T03:01:07.390Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation"><a href="#Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation" class="headerlink" title="Spectrally Pruned Gaussian Fields with Neural Compensation"></a>Spectrally Pruned Gaussian Fields with Neural Compensation</h2><p><strong>Authors:Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao</strong></p><p>Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a>. </p><p><a href="http://arxiv.org/abs/2405.00676v1">PDF</a> Code: <a href="https://github.com/RunyiYang/SUNDAE">https://github.com/RunyiYang/SUNDAE</a> Project page:   <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a></p><p><strong>Summary</strong><br>3Dé«˜æ–¯ç‚¹æ¸²æŸ“ç®—æ³• SUNDAE é€šè¿‡è°±å‰ªæå’Œç¥ç»è¡¥å¿æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dé«˜æ–¯ç‚¹æ¸²æŸ“ç®—æ³•åœ¨æ¸²æŸ“é€Ÿåº¦å’Œè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å†…å­˜å ç”¨é«˜ã€‚</li><li>SUNDAE ç®—æ³•æ„å»ºäº†é«˜æ–¯åŸºå…ƒçš„å…³ç³»å›¾ï¼Œå¹¶è®¾è®¡äº†è°±ä¸‹é‡‡æ ·æ¨¡å—æ¥å‰ªé™¤åŸºå…ƒã€‚</li><li>SUNDAE ç®—æ³•ä½¿ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œå¤´å¯¹æ¸²æŸ“ç‰¹å¾è¿›è¡Œæ··åˆï¼Œå¼¥è¡¥äº†å‰ªæé€ æˆçš„è´¨é‡æŸå¤±ã€‚</li><li>SUNDAE ç®—æ³•æ˜¾è‘—é™ä½äº†å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ¸²æŸ“è´¨é‡ã€‚</li><li>SUNDAE åœ¨ Mip-NeRF360 æ•°æ®é›†ä¸Šä»¥ 104 MB çš„å†…å­˜å®ç°äº† 26.80 PSNR å’Œ 145 FPSï¼Œè€ŒåŸå§‹é«˜æ–¯ç‚¹æ¸²æŸ“ç®—æ³•åˆ™ä»¥ 523 MB çš„å†…å­˜å®ç°äº† 25.60 PSNR å’Œ 160 FPSã€‚</li><li>SUNDAE ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šSUNDAEï¼šç¥ç»è¡¥å¿å…‰è°±ä¿®å‰ªé«˜æ–¯åœº</p></li><li><p>ä½œè€…ï¼šRunyi Yangã€Zhenxin Zhuã€Zhou Jiangã€Baijun Yeã€Xiaoxue Chenã€Yifei Zhangã€Yuantao Chenã€Jian Zhaoã€Hao Zhao</p></li><li><p>éš¶å±æœºæ„ï¼šæ¸…åå¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢</p></li><li><p>å…³é”®è¯ï¼š3Dé«˜æ–¯å–·å°„ã€å›¾ä¿¡å·å¤„ç†ã€ç¥ç»æ¸²æŸ“</p></li><li><p>è®ºæ–‡åœ°å€ï¼šhttps://arxiv.org/abs/2405.00676Githubï¼šhttps://github.com/runyiyang/SUNDAE</p></li><li><p>æ‘˜è¦ï¼š</p><p>(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3Dé«˜æ–¯å–·å°„ä½œä¸ºä¸€ç§æ–°é¢–çš„3Dè¡¨ç¤ºï¼Œå› å…¶æ¸²æŸ“é€Ÿåº¦å¿«ã€æ¸²æŸ“è´¨é‡é«˜è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™å¸¦æ¥äº†è¾ƒé«˜çš„å†…å­˜æ¶ˆè€—ï¼Œä¾‹å¦‚ï¼Œä¸€ä¸ªè®­ç»ƒè‰¯å¥½çš„é«˜æ–¯åœºå¯èƒ½éœ€è¦ä½¿ç”¨300ä¸‡ä¸ªé«˜æ–¯åŸè¯­å’Œè¶…è¿‡700MBçš„å†…å­˜ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§é«˜å†…å­˜å ç”¨ç‡æ˜¯ç”±äºç¼ºä¹å¯¹åŸè¯­ä¹‹é—´å…³ç³»çš„è€ƒè™‘ã€‚</p><p>(2)ï¼šä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•æ˜¯3Dé«˜æ–¯å–·å°„ã€‚å…¶é—®é¢˜æ˜¯ï¼šè®­ç»ƒä¸€ä¸ª3Dé«˜æ–¯å–·å°„æ¨¡å‹é¢ä¸´ç€é«˜å†…å­˜æ¶ˆè€—çš„æŒ‘æˆ˜ã€‚</p><p>(3)ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSUNDAEçš„å†…å­˜é«˜æ•ˆé«˜æ–¯åœºï¼Œå®ƒé‡‡ç”¨å…‰è°±ä¿®å‰ªå’Œç¥ç»è¡¥å¿ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨é«˜æ–¯åŸè¯­é›†åˆä¸Šæ„å»ºä¸€ä¸ªå›¾æ¥å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå…‰è°±ä¸‹é‡‡æ ·æ¨¡å—æ¥å‰”é™¤åŸè¯­ï¼ŒåŒæ—¶ä¿ç•™æ‰€éœ€çš„ä¿¡å·ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸ºäº†è¡¥å¿ä¿®å‰ªé«˜æ–¯ä½“å¸¦æ¥çš„è´¨é‡æŸå¤±ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»ç½‘ç»œå¤´æ¥æ··åˆå–·å°„ç‰¹å¾ï¼Œå®ƒæœ‰æ•ˆåœ°è¡¥å¿äº†è´¨é‡æŸå¤±ï¼ŒåŒæ—¶åœ¨å…¶æƒé‡ä¸­æ•è·äº†åŸè¯­ä¹‹é—´çš„å…³ç³»ã€‚</p><p>(4)ï¼šæ–¹æ³•æ€§èƒ½ï¼šæˆ‘ä»¬åœ¨Mip-NeRF360æ•°æ®é›†ä¸Šï¼ŒSUNDAEä½¿ç”¨104MBå†…å­˜å®ç°äº†26.80 PSNRå’Œ145 FPSï¼Œè€Œä¼ ç»Ÿçš„3Dé«˜æ–¯å–·å°„ç®—æ³•ä½¿ç”¨523MBå†…å­˜å®ç°äº†25.60 PSNRå’Œ160 FPSã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæ„å»ºå›¾æ¨¡å‹ï¼Œå…‰è°±ä¸‹é‡‡æ ·å‰”é™¤åŸè¯­ï¼Œä¿ç•™æ‰€éœ€ä¿¡å·ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ç¥ç»ç½‘ç»œå¤´æ··åˆå–·å°„ç‰¹å¾ï¼Œè¡¥å¿ä¿®å‰ªé«˜æ–¯ä½“å¸¦æ¥çš„è´¨é‡æŸå¤±ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨æƒé‡ä¸­æ•è·åŸè¯­ä¹‹é—´çš„å…³ç³»ï¼›</p><p>.......</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šSUNDAEæ–¹æ³•åœ¨ä¿æŒ3Dé«˜æ–¯å–·å°„æ•ˆç‡çš„åŒæ—¶ï¼Œå°ºå¯¸å¤§å¹…ç¼©å°ï¼Œä¸º3Dåœºæ™¯è¡¨ç¤ºå’Œæ¸²æŸ“æä¾›äº†æ–°çš„æ€è·¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå›¾ä¿¡å·å¤„ç†æ¡†æ¶ä¸ç¥ç»ç½‘ç»œè¡¥å¿ç›¸ç»“åˆï¼Œæ„å»ºäº†è°±ä¿®å‰ªé«˜æ–¯åœºï¼›æ€§èƒ½ï¼šåœ¨Mip-NeRF360æ•°æ®é›†ä¸Šï¼ŒSUNDAEä½¿ç”¨104MBå†…å­˜å®ç°äº†26.80 PSNRå’Œ145 FPSï¼Œè€Œä¼ ç»Ÿçš„3Dé«˜æ–¯å–·å°„ç®—æ³•ä½¿ç”¨523MBå†…å­˜å®ç°äº†25.60 PSNRå’Œ160 FPSï¼›å·¥ä½œé‡ï¼šæ„å»ºå›¾æ¨¡å‹ã€å…‰è°±ä¸‹é‡‡æ ·ã€ç¥ç»ç½‘ç»œè¡¥å¿ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-da926fda6b7545ddb2dfb2a2da01023f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3c4eab265f17a199aeff4e6e3cbd775.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e54f047dac354ead2f83393633d9db5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ec9cfcf497b0d1e4aab9d2caf7b66814.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75980d3d584e8ea8e866bf08e90027f7.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v2) To be published in ACM SIGGRAPH 2024**Summary**å®æ—¶é«˜æ–¯å»ºå›¾ç³»ç»Ÿ (RTG-SLAM) é‡‡ç”¨é«˜æ–¯å¹³é“ºï¼Œä½¿ç”¨ RGBD ç›¸æœºå®æ—¶æ„å»ºå¤§åœºæ™¯çš„ 3D é‡å»ºï¼Œå…·æœ‰ç´§å‡‘çš„é«˜æ–¯è¡¨ç¤ºå’Œé«˜æ•ˆçš„å³æ—¶é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆã€‚**Key Takeaways**- ä½¿ç”¨é«˜æ–¯å¹³é“ºè¿›è¡Œå¤§è§„æ¨¡ç¯å¢ƒçš„å®æ—¶ 3D é‡å»ºã€‚- é‡‡ç”¨ç´§å‡‘çš„é«˜æ–¯è¡¨ç¤ºï¼Œé™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚- å³æ—¶é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»…ä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ï¼Œå‡å°‘è®¡ç®—é‡ã€‚- æ¸²æŸ“æ·±åº¦ä¸é¢œè‰²æ¸²æŸ“æ–¹å¼ä¸åŒï¼Œå•ä¸€ä¸é€æ˜é«˜æ–¯å³å¯æ‹Ÿåˆå±€éƒ¨è¡¨é¢åŒºåŸŸã€‚- åŒºåˆ†ç¨³å®šå’Œä¸ç¨³å®šé«˜æ–¯ï¼Œä»…ä¼˜åŒ–ä¸ç¨³å®šé«˜æ–¯ï¼Œæé«˜æ¸²æŸ“æ•ˆç‡ã€‚- ä¸ NeRF é©±åŠ¨çš„ RGBD SLAM ç›¸æ¯”ï¼Œé‡å»ºè´¨é‡ç›¸å½“ï¼Œé€Ÿåº¦æå‡ä¸€å€ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘ä¸€åŠã€‚- åœ¨æ–°è§†å›¾åˆæˆå’Œç›¸æœºè·Ÿè¸ªç²¾åº¦æ–¹é¢è¡¨ç°æ›´ä½³ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAM: åŸºäºé«˜æ–¯æ•£å°„çš„å¤§è§„æ¨¡å®æ—¶ 3D é‡å»º</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: RGBD SLAM, Real-time 3D Reconstruction, Gaussian Splatting, NeRF</p></li><li><p>Urls: https://arxiv.org/abs/2404.19706v2, Github: None</p></li><li><p>Summary:</p><pre><code>            (1): RGBD SLAM æŠ€æœ¯åœ¨å®æ—¶å¤§è§„æ¨¡ 3D é‡å»ºä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäº NeRF çš„ RGBD SLAM æ–¹æ³•åœ¨é‡å»ºé€Ÿåº¦ã€å†…å­˜æ¶ˆè€—å’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚            (2): ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤šä¸ªé‡å çš„é«˜æ–¯æ ¸æ¥æ‹Ÿåˆå±€éƒ¨è¡¨é¢åŒºåŸŸï¼Œå¯¼è‡´å†…å­˜å’Œè®¡ç®—æˆæœ¬é«˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç¼ºä¹é«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¿™é™åˆ¶äº†å®æ—¶æ€§èƒ½ã€‚            (3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º RTG-SLAM çš„å®æ—¶ 3D é‡å»ºç³»ç»Ÿï¼Œå®ƒé‡‡ç”¨é«˜æ–¯æ•£å°„å’Œé«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆã€‚è¯¥ç³»ç»Ÿå¼ºåˆ¶æ¯ä¸ªé«˜æ–¯æ ¸è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œä¸é€æ˜çš„é«˜æ–¯æ ¸æ‹Ÿåˆè¡¨é¢å’Œä¸»è¦é¢œè‰²ï¼Œè€Œé€æ˜çš„é«˜æ–¯æ ¸æ‹Ÿåˆæ®‹å·®é¢œè‰²ã€‚é€šè¿‡ä»¥ä¸åŒäºé¢œè‰²æ¸²æŸ“çš„æ–¹å¼æ¸²æŸ“æ·±åº¦ï¼Œè¯¥ç³»ç»Ÿå¯ä»¥è®©å•ä¸ªä¸é€æ˜é«˜æ–¯æ ¸å¾ˆå¥½åœ°æ‹Ÿåˆå±€éƒ¨è¡¨é¢åŒºåŸŸï¼Œè€Œæ— éœ€å¤šä¸ªé‡å çš„é«˜æ–¯æ ¸ï¼Œä»è€Œå¤§å¤§é™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚å¯¹äºåœ¨çº¿é«˜æ–¯ä¼˜åŒ–ï¼Œè¯¥ç³»ç»Ÿé’ˆå¯¹æ¯å¸§çš„ä¸‰ç±»åƒç´ æ˜¾å¼æ·»åŠ é«˜æ–¯æ ¸ï¼šæ–°è§‚æµ‹çš„åƒç´ ã€é¢œè‰²è¯¯å·®å¤§çš„åƒç´ å’Œæ·±åº¦è¯¯å·®å¤§çš„åƒç´ ã€‚è¯¥ç³»ç»Ÿè¿˜å°†æ‰€æœ‰é«˜æ–¯æ ¸åˆ†ç±»ä¸ºç¨³å®šé«˜æ–¯æ ¸å’Œä¸ç¨³å®šé«˜æ–¯æ ¸ï¼Œå…¶ä¸­ç¨³å®šé«˜æ–¯æ ¸æœ‰æœ›å¾ˆå¥½åœ°æ‹Ÿåˆå…ˆå‰è§‚æµ‹çš„ RGBD å›¾åƒï¼Œå¦åˆ™ä¸ºä¸ç¨³å®šé«˜æ–¯æ ¸ã€‚è¯¥ç³»ç»Ÿåªä¼˜åŒ–ä¸ç¨³å®šé«˜æ–¯æ ¸ï¼Œåªæ¸²æŸ“ä¸ç¨³å®šé«˜æ–¯æ ¸å æ®çš„åƒç´ ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¯¥ç³»ç»ŸåŒæ—¶é™ä½äº†é«˜æ–¯æ ¸çš„æ•°é‡å’Œæ¸²æŸ“æˆæœ¬ã€‚            (4): åœ¨é…’åº—æˆ¿é—´é‡å»ºä»»åŠ¡ä¸Šï¼ŒRTG-SLAM åœ¨é‡å»ºé€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„ NeRF-SLAM æ–¹æ³•ï¼ŒåŒæ—¶åœ¨åˆæˆæ–°é¢–è§†è§’æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„çœŸå®æ„Ÿã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            ï¼ˆ1ï¼‰ï¼šæå‡ºäº† RTG-SLAM ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨é«˜æ–¯æ•£å°„å’Œé«˜æ•ˆçš„åœ¨çº¿é«˜æ–¯ä¼˜åŒ–æ–¹æ¡ˆè¿›è¡Œå®æ—¶ 3D é‡å»ºï¼›            ï¼ˆ2ï¼‰ï¼šå¼ºåˆ¶æ¯ä¸ªé«˜æ–¯æ ¸è¦ä¹ˆä¸é€æ˜è¦ä¹ˆè¿‘ä¹é€æ˜ï¼Œä¸é€æ˜çš„é«˜æ–¯æ ¸æ‹Ÿåˆè¡¨é¢å’Œä¸»è¦é¢œè‰²ï¼Œè€Œé€æ˜çš„é«˜æ–¯æ ¸æ‹Ÿåˆæ®‹å·®é¢œè‰²ï¼›            ï¼ˆ3ï¼‰ï¼šé’ˆå¯¹æ¯å¸§çš„ä¸‰ç±»åƒç´ æ˜¾å¼æ·»åŠ é«˜æ–¯æ ¸ï¼šæ–°è§‚æµ‹çš„åƒç´ ã€é¢œè‰²è¯¯å·®å¤§çš„åƒç´ å’Œæ·±åº¦è¯¯å·®å¤§çš„åƒç´ ï¼›            ï¼ˆ4ï¼‰ï¼šå°†æ‰€æœ‰é«˜æ–¯æ ¸åˆ†ç±»ä¸ºç¨³å®šé«˜æ–¯æ ¸å’Œä¸ç¨³å®šé«˜æ–¯æ ¸ï¼Œå…¶ä¸­ç¨³å®šé«˜æ–¯æ ¸æœ‰æœ›å¾ˆå¥½åœ°æ‹Ÿåˆå…ˆå‰è§‚æµ‹çš„ RGBD å›¾åƒï¼Œå¦åˆ™ä¸ºä¸ç¨³å®šé«˜æ–¯æ ¸ï¼›            ï¼ˆ5ï¼‰ï¼šåªä¼˜åŒ–ä¸ç¨³å®šé«˜æ–¯æ ¸ï¼Œåªæ¸²æŸ“ä¸ç¨³å®šé«˜æ–¯æ ¸å æ®çš„åƒç´ ï¼Œé™ä½äº†é«˜æ–¯æ ¸çš„æ•°é‡å’Œæ¸²æŸ“æˆæœ¬ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ•£å°„çš„å¤§è§„æ¨¡å®æ—¶ 3D é‡å»ºç³»ç»Ÿ RTG-SLAMï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨ç´§å‡‘çš„é«˜æ–¯è¡¨ç¤ºæ¥å‡å°‘æ‹Ÿåˆè¡¨é¢çš„é«˜æ–¯æ•°é‡ï¼Œä»è€Œå¤§å¤§é™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚å¯¹äºåœ¨çº¿é«˜æ–¯ä¼˜åŒ–ï¼Œè¯¥ç³»ç»Ÿé’ˆå¯¹æ¯å¸§çš„ä¸‰ç±»åƒç´ æ˜¾å¼æ·»åŠ é«˜æ–¯ï¼šæ–°è§‚æµ‹çš„ã€é¢œè‰²è¯¯å·®å¤§çš„å’Œæ·±åº¦è¯¯å·®å¤§çš„ï¼Œå¹¶ä¸”åªä¼˜åŒ–ä¸ç¨³å®šçš„é«˜æ–¯ï¼Œåªæ¸²æŸ“ä¸ç¨³å®šçš„é«˜æ–¯å æ®çš„åƒç´ ï¼Œé™ä½äº†é«˜æ–¯æ•°é‡å’Œæ¸²æŸ“æˆæœ¬ã€‚è¯¥ç³»ç»Ÿåœ¨å¤§è§„æ¨¡çœŸå®æ‰«æåœºæ™¯ä¸­é‡å»ºï¼Œå¹¶å–å¾—äº†ä¼˜äºæœ€å…ˆè¿›çš„ NeRF SLAM æ–¹æ³•å’Œå¹¶å‘çš„ Gaussian SLAM æ–¹æ³•çš„æ€§èƒ½ã€‚ç”±äºä¸ºäº†åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­å®ç°å®æ—¶é‡å»ºï¼Œåªä½¿ç”¨ä¸é€æ˜çš„é«˜æ–¯å’Œé€æ˜çš„é«˜æ–¯æ¥è¡¨ç¤ºåœºæ™¯ï¼Œå› æ­¤ä¸åŸå§‹çš„é«˜æ–¯ç›¸æ¯”ï¼Œæ¸²æŸ“è´¨é‡ä¸å¯é¿å…åœ°ä¼šä¸‹é™ã€‚å¦‚ä½•åœ¨ä¿æŒå®æ—¶æ€§èƒ½çš„åŒæ—¶æé«˜æ¸²æŸ“è´¨é‡æ˜¯æœªæ¥å€¼å¾—æ¢ç´¢çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œåå°„æˆ–é€æ˜çš„ææ–™ä¼šå¯¼è‡´è¡¨é¢é¢œè‰²åœ¨ä¸åŒè§†å›¾ä¹‹é—´å‘ç”Ÿå¾ˆå¤§å˜åŒ–ï¼Œä½¿å¾—ä¸€äº›é«˜æ–¯é¢‘ç¹åœ°åœ¨ä¸¤ç§çŠ¶æ€ä¹‹é—´åˆ‡æ¢ï¼Œå¹¶ä¸”æ— æ³•å¾—åˆ°å¾ˆå¥½çš„ä¼˜åŒ–ã€‚æœªæ¥ï¼Œè¯¥ç³»ç»Ÿè¿˜å°†æ‰©å±•åˆ°å¤„ç†æˆ·å¤–åœºæ™¯ã€åŠ¨æ€ç‰©ä½“ã€å¿«é€Ÿæ‘„åƒæœºè¿åŠ¨å’Œå…‰ç…§å˜åŒ–çš„åœºæ™¯ã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šé«˜æ–¯æ•£å°„ã€ç´§å‡‘çš„é«˜æ–¯è¡¨ç¤ºã€åœ¨çº¿é«˜æ–¯ä¼˜åŒ–ï¼›æ€§èƒ½ï¼šåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­å®ç°å®æ—¶é‡å»ºã€ä¼˜äºæœ€å…ˆè¿›çš„ NeRF SLAM æ–¹æ³•å’Œå¹¶å‘çš„ Gaussian SLAM æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šé™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€é™ä½äº†é«˜æ–¯æ•°é‡å’Œæ¸²æŸ“æˆæœ¬ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-d45b35f06c4dce864863260a5af329f4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8764f6bde3de348a98aac2f2a4a30ee2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details><h2 id="GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting"><a href="#GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting" class="headerlink" title="GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting"></a>GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting</h2><p><strong>Authors:Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, Zexiang Xu</strong></p><p>We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a> . </p><p><a href="http://arxiv.org/abs/2404.19702v1">PDF</a> Project webpage: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a></p><p><strong>Summary</strong><br>ä¸‰ç»´é«˜æ–¯åŸè¯­å¤§é‡å»ºæ¨¡å‹ï¼Œå¯ä» 2-4 ä¸ªå§¿åŠ¿ç¨€ç–å›¾åƒé¢„æµ‹é«˜è´¨é‡çš„ä¸‰ç»´é«˜æ–¯åŸè¯­ï¼Œåœ¨å•ä¸ª A100 GPU ä¸Šä»…éœ€ 0.23 ç§’ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨å˜å‹å™¨æ¶æ„ï¼Œä»å›¾åƒä¸­é¢„æµ‹ä¸‰ç»´é«˜æ–¯åŸè¯­ã€‚</li><li>æ¨¡å‹å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯é¢„æµ‹å…·æœ‰å¤§å°ºåº¦å’Œå¤æ‚åº¦å˜åŒ–çš„åœºæ™¯ã€‚</li><li>åœ¨ Objaverse å’Œ RealEstate10K æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li><li>å¯ç”¨äºä¸‹æ¸¸ä¸‰ç»´ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚è§†å›¾åˆæˆå’Œä¸‰ç»´å½¢çŠ¶åˆæˆã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>é¢˜ç›®ï¼šGS-LRMï¼šå¤§å‹é‡å»ºæ¨¡å‹</p></li><li><p>ä½œè€…ï¼šKai Zhangã€Sai Biã€Hao Tanã€Yuanbo Xiangliã€Nanxuan Zhaoã€Kalyan Sunkavalliã€Zexiang Xu</p></li><li><p>å•ä½ï¼šAdobe Research</p></li><li><p>å…³é”®è¯ï¼šLarge Reconstruction Models Â· 3D Reconstruction Â· Gaussian Splatting</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.19702 , Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D åœºæ™¯é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤æ‚çš„å…‰åº¦æµ‹é‡ç³»ç»Ÿå’Œå¯†é›†çš„å¤šè§†å›¾å›¾åƒã€‚ç¥ç»è¡¨å¾å’Œå¯å¾®æ¸²æŸ“çš„è¿›æ­¥æé«˜äº†é‡å»ºå’Œæ¸²æŸ“è´¨é‡ï¼Œä½†é€Ÿåº¦æ…¢ä¸”éœ€è¦å¤§é‡è¾“å…¥è§†å›¾ã€‚åŸºäº Transformer çš„ 3D å¤§å‹é‡å»ºæ¨¡å‹ (LRM) å­¦ä¹ äº†å¤§é‡çš„ 3D å¯¹è±¡çš„é€šç”¨é‡å»ºå…ˆéªŒï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„ç¨€ç–è§†å›¾ 3D é‡å»ºè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•é‡‡ç”¨ä¸‰å¹³é¢ NeRF ä½œä¸ºåœºæ™¯è¡¨ç¤ºï¼Œå­˜åœ¨ä¸‰å¹³é¢åˆ†è¾¨ç‡å—é™å’Œä½“ç§¯æ¸²æŸ“å¼€é”€å¤§çš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦æ…¢ã€éš¾ä»¥ä¿ç•™ç²¾ç»†ç»†èŠ‚ï¼Œä»¥åŠæ— æ³•æ‰©å±•åˆ°å¯¹è±¡ä¸­å¿ƒè¾“å…¥ä¹‹å¤–çš„å¤§åœºæ™¯ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬è®ºæ–‡æ–¹æ³•ï¼šGS-LRM æ˜¯ä¸€ç§å¯æ‰©å±•çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§éå¸¸ç®€å•çš„åŸºäº Transformer çš„æ¶æ„ã€‚å°†è¾“å…¥å›¾åƒè¿›è¡Œå—çŠ¶åŒ–ï¼Œå°†è¿æ¥åçš„å¤šè§†å›¾å›¾åƒæ ‡è®°é€šè¿‡ä¸€ç³»åˆ— Transformer å—ï¼Œå¹¶ç›´æ¥ä»è¿™äº›æ ‡è®°è§£ç æœ€ç»ˆçš„é€åƒç´ é«˜æ–¯å‚æ•°ä»¥è¿›è¡Œå¯å¾®æ¸²æŸ“ã€‚ä¸åªèƒ½é‡å»ºå¯¹è±¡çš„å…ˆå‰ LRM ä¸åŒï¼ŒGS-LRM é€šè¿‡é¢„æµ‹é€åƒç´ é«˜æ–¯å‡½æ•°ï¼Œè‡ªç„¶åœ°å¤„ç†äº†è§„æ¨¡å’Œå¤æ‚æ€§å·®å¼‚å¾ˆå¤§çš„åœºæ™¯ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šGS-LRM åœ¨ Objaverse å’Œ RealEstate10K æ•°æ®é›†ä¸Šåˆ†åˆ«é’ˆå¯¹å¯¹è±¡å’Œåœºæ™¯æ•æ‰è¿›è¡Œäº†è®­ç»ƒï¼Œåœ¨ä¸¤ç§æƒ…å†µä¸‹éƒ½å¤§å¹…ä¼˜äºæœ€å…ˆè¿›çš„åŸºå‡†ã€‚è¯¥æ¨¡å‹è¿˜å¯ä»¥åœ¨ä¸‹æ¸¸ 3D ç”Ÿæˆä»»åŠ¡ä¸­å¾—åˆ°åº”ç”¨ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šé‡‡ç”¨ Transformer æ¨¡å‹ï¼Œå°†ä¸€ç»„å·²çŸ¥ç›¸æœºä½å§¿çš„å›¾åƒå›å½’ä¸ºé€åƒç´ çš„ 3D é«˜æ–¯å‚æ•°ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡ patchify ç®—å­å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ ‡è®°åŒ–å¤„ç†ï¼Œå°†å¤šè§†å›¾å›¾åƒæ ‡è®°è¿æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ— Transformer å—è¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„åŠ›å’Œ MLP å±‚ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä»æ¯ä¸ªè¾“å‡ºæ ‡è®°ä¸­ï¼Œä½¿ç”¨çº¿æ€§å±‚è§£ç å¯¹åº” patch ä¸­åƒç´ å¯¹é½çš„é«˜æ–¯å‡½æ•°å±æ€§ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåˆ©ç”¨çº¿æ€§å±‚å°† 1D å‘é‡æ˜ å°„åˆ° d ç»´çš„å›¾åƒ patch æ ‡è®°ï¼Œå…¶ä¸­ d æ˜¯ Transformer å®½åº¦ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå°†å¤šè§†å›¾å›¾åƒæ ‡è®°è¿æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ— Transformer å—è¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬æ®‹å·®è¿æ¥ã€å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œ MLPï¼›</p><p>ï¼ˆ6ï¼‰ï¼šä½¿ç”¨å•ä¸ªçº¿æ€§å±‚å°† Transformer çš„è¾“å‡ºæ ‡è®°è§£ç ä¸ºé«˜æ–¯å‚æ•°ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1)ï¼šæœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„åŸºäº Transformer çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼Œç”¨äºé«˜æ–¯ splattingï¼ˆGSï¼‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å•ä¸ª A100 GPU ä¸Šä»¥çº¦ 0.23 ç§’çš„é€Ÿåº¦ä»ä¸€ç»„å·²çŸ¥ç›¸æœºä½å§¿çš„å›¾åƒä¸­è¿›è¡Œå¿«é€Ÿå‰é¦ˆé«˜åˆ†è¾¨ç‡ GS é¢„æµ‹ã€‚è¯¥æ¨¡å‹æ—¢é€‚ç”¨äºå¯¹è±¡çº§æ•æ‰ï¼Œä¹Ÿé€‚ç”¨äºåœºæ™¯çº§æ•æ‰ï¼Œå¹¶ä¸”åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒåï¼Œåœ¨ä¸¤ç§æƒ…å†µä¸‹å‡è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ¿€å‘æœªæ¥åœ¨æ•°æ®é©±åŠ¨çš„å‰é¦ˆ 3D é‡å»ºé¢†åŸŸå¼€å±•æ›´å¤šå·¥ä½œã€‚è‡´è°¢æ„Ÿè°¢ Nathan Carr å’Œ Duygu Ceylan æä¾›æœ‰ç›Šçš„è®¨è®ºã€‚            (2)ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº Transformer çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼Œç”¨äºé«˜æ–¯ splatting è¡¨ç¤ºï¼Œè¯¥æ¨¡å‹ç®€å•ä¸”å¯æ‰©å±•ï¼›æ€§èƒ½ï¼šåœ¨å¯¹è±¡çº§å’Œåœºæ™¯çº§æ•æ‰ä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šåœ¨å•ä¸ª A100 GPU ä¸Šä»¥çº¦ 0.23 ç§’çš„é€Ÿåº¦è¿›è¡Œå‰é¦ˆé«˜åˆ†è¾¨ç‡ GS é¢„æµ‹ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-7b3dfbd4f62939f8af1187b102bf5134.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5173cd30ad642c9a906d00c88085376d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e761662a90d51192c43a703fedff0bc4.jpg" align="middle"></details><h2 id="SAGS-Structure-Aware-3D-Gaussian-Splatting"><a href="#SAGS-Structure-Aware-3D-Gaussian-Splatting" class="headerlink" title="SAGS: Structure-Aware 3D Gaussian Splatting"></a>SAGS: Structure-Aware 3D Gaussian Splatting</h2><p><strong>Authors:Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</strong></p><p>Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the sceneâ€™s geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page <a href="https://eververas.github.io/SAGS/">https://eververas.github.io/SAGS/</a>. </p><p><a href="http://arxiv.org/abs/2404.19149v1">PDF</a> 15 pages, 8 figures, 3 tables</p><p><strong>Summary</strong><br>åˆ©ç”¨ç»“æ„é©±åŠ¨çš„ä¼˜åŒ–ç­–ç•¥ï¼ŒSAGS åœ¨å®æ—¶ç¥ç»æ¸²æŸ“ä¸­å®ç°äº†å‹ç¼©æ€§å’Œé«˜ä¿çœŸæ€§ï¼Œé€šè¿‡åˆ©ç”¨å±€éƒ¨-å…¨å±€å›¾è¡¨ç¤ºæ¥ç¼–ç åœºæ™¯å‡ ä½•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SAGS é€šè¿‡ç»“æ„æ„ŸçŸ¥ä¼˜åŒ–å¯¹ 3DGS è¿›è¡Œäº†æ”¹è¿›ã€‚</li><li>SAGS é‡‡ç”¨å±€éƒ¨-å…¨å±€å›¾è¡¨ç¤ºï¼Œæ•è·åœºæ™¯å‡ ä½•ã€‚</li><li>SAGS ä¼˜åŒ–ç‚¹ä½ç§»ä»¥ä¿æŒåœºæ™¯å‡ ä½•ï¼Œæé«˜è¡¨ç¤ºèƒ½åŠ›å’Œæ¸²æŸ“è´¨é‡ã€‚</li><li>SAGS æå‡ºäº†ä¸€ç§åŸºäºä¸­ç‚¹æ’å€¼çš„è½»é‡çº§å˜ä½“ï¼Œå¯æ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°ã€‚</li><li>å®éªŒè¡¨æ˜ SAGS åœ¨æ¸²æŸ“è´¨é‡å’Œæ¨¡å‹å°ºå¯¸æ–¹é¢ä¼˜äºå…¶ä»– 3DGS æ–¹æ³•ã€‚</li><li>SAGS ç¼“è§£äº†æµ®åŠ¨ä¼ªå½±å’Œä¸è§„åˆ™å¤±çœŸï¼Œå¹¶ç”Ÿæˆç²¾ç¡®çš„æ·±åº¦å›¾ã€‚</li><li>SAGS é¡¹ç›®ä¸»é¡µï¼š<a href="https://eververas.github.io/SAGS/ã€‚">https://eververas.github.io/SAGS/ã€‚</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ç»“æ„æ„ŸçŸ¥3Dé«˜æ–¯æ–‘ç‚¹</p></li><li><p>Authors: Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</p></li><li><p>Affiliation: å¸å›½ç†å·¥å­¦é™¢</p></li><li><p>Keywords: Novel View Synthesis, 3D Gaussian Splatting, Structure-Aware, Local-Global Graph Representation</p></li><li><p>Urls: https://eververas.github.io/SAGS/, Github:None</p></li><li><p>Summary:</p><pre><code>            (1):éšç€NeRFsçš„å‡ºç°ï¼Œ3Dé«˜æ–¯æ–‘ç‚¹ï¼ˆ3D-GSï¼‰ä¸ºå®æ—¶ç¥ç»æ¸²æŸ“é“ºå¹³äº†é“è·¯ï¼Œå…‹æœäº†ä½“ç§¯æ–¹æ³•çš„è®¡ç®—è´Ÿæ‹…ã€‚åœ¨3D-GSçš„å¼€åˆ›æ€§å·¥ä½œä¹‹åï¼Œä¸€äº›æ–¹æ³•è¯•å›¾å®ç°å¯å‹ç¼©ä¸”é«˜ä¿çœŸæ€§èƒ½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œé€šè¿‡é‡‡ç”¨ä¸å‡ ä½•æ— å…³çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†åœºæ™¯çš„å›ºæœ‰3Dç»“æ„ï¼Œä»è€Œé™åˆ¶äº†è¡¨ç°åŠ›å’Œè¡¨ç¤ºçš„è´¨é‡ï¼Œå¯¼è‡´å„ç§æµ®ç‚¹å’Œä¼ªå½±ã€‚            (2):ä»¥å¾€çš„æ–¹æ³•ï¼š3D-GSã€å­˜åœ¨é—®é¢˜ï¼šå¿½ç•¥åœºæ™¯çš„3Dç»“æ„ï¼Œå¯¼è‡´è¡¨ç°åŠ›å’Œè¡¨ç¤ºè´¨é‡å—é™ï¼Œäº§ç”Ÿæµ®ç‚¹å’Œä¼ªå½±ã€‚åŠ¨æœºå……åˆ†ï¼šæå‡ºä¸€ç§ç»“æ„æ„ŸçŸ¥çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚            (3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šç»“æ„æ„ŸçŸ¥é«˜æ–¯æ–‘ç‚¹æ–¹æ³•ï¼ˆSAGSï¼‰ï¼Œéšå¼ç¼–ç åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ï¼Œåœ¨åŸºå‡†æ–°è§†å›¾åˆæˆæ•°æ®é›†ä¸Šåæ˜ äº†æœ€å…ˆè¿›çš„æ¸²æŸ“æ€§èƒ½å’Œé™ä½çš„å­˜å‚¨éœ€æ±‚ã€‚SAGSå»ºç«‹åœ¨å±€éƒ¨-å…¨å±€å›¾è¡¨ç¤ºçš„åŸºç¡€ä¸Šï¼Œè¯¥è¡¨ç¤ºæœ‰åŠ©äºå­¦ä¹ å¤æ‚åœºæ™¯å¹¶å¼ºåˆ¶æœ‰æ„ä¹‰çš„ç‚¹ä½ç§»ä»¥ä¿ç•™åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•ä½†æœ‰æ•ˆçš„ä¸­é—´ç‚¹æ’å€¼æ–¹æ¡ˆå¼•å…¥äº†SAGSçš„è½»é‡çº§ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬å±•ç¤ºäº†åœºæ™¯çš„ç´§å‡‘è¡¨ç¤ºï¼Œå°ºå¯¸æœ€å¤šå‡å°‘äº†24å€ï¼Œè€Œæ— éœ€ä¾èµ–ä»»ä½•å‹ç¼©ç­–ç•¥ã€‚            (4):ä»»åŠ¡å’Œæ€§èƒ½ï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSAGSåœ¨æ¸²æŸ“è´¨é‡å’Œæ¨¡å‹å¤§å°æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„3D-GSæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç»“æ„æ„ŸçŸ¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡è½»å…ˆå‰æ–¹æ³•çš„æµ®åŠ¨ä¼ªå½±å’Œä¸è§„åˆ™å¤±çœŸï¼ŒåŒæ—¶è·å¾—ç²¾ç¡®çš„æ·±åº¦å›¾ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):æ›²ç‡æ„ŸçŸ¥ç¨ å¯†åŒ–ï¼šé‡‡ç”¨ Grad-PU æ–¹æ³•ï¼Œå¯¹ä½æ›²ç‡åŒºåŸŸè¿›è¡Œä¸­ç‚¹æ’å€¼ï¼Œç”Ÿæˆå¯†é›†ç‚¹äº‘ï¼Œå¢å¼º 3D-GS çš„åˆå§‹åŒ–ï¼›            (2):ç»“æ„æ„ŸçŸ¥ç¼–ç å™¨ï¼šåŸºäº k-NN å›¾ï¼Œä½¿ç”¨å›¾ç¥ç»ç½‘ç»œå­¦ä¹ å±€éƒ¨å’Œå…¨å±€ç»“æ„ç‰¹å¾ï¼Œè·å¾—ç»“æ„æ„ŸçŸ¥ç‰¹å¾ç¼–ç ï¼›            (3):ç»†åŒ–ç½‘ç»œï¼šä½¿ç”¨ MLP è§£ç ç»“æ„æ„ŸçŸ¥ç‰¹å¾ç¼–ç ï¼Œé¢„æµ‹ 3D é«˜æ–¯æ–‘ç‚¹çš„å±æ€§ï¼ˆä½ç½®ã€é¢œè‰²ã€ä¸é€æ˜åº¦ã€åæ–¹å·®ï¼‰ï¼›            (4):SAGS-Liteï¼šåˆ©ç”¨ä¸­ç‚¹æ’å€¼ï¼Œå‡å°‘å­˜å‚¨éœ€æ±‚ï¼Œç”Ÿæˆç´§å‡‘çš„ 3D é«˜æ–¯æ–‘ç‚¹é›†åˆï¼Œæ— éœ€å‹ç¼©æŠ€æœ¯ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„æ„ŸçŸ¥é«˜æ–¯æ–‘ç‚¹æ–¹æ³•ï¼ˆSAGSï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åœºæ™¯çš„å†…åœ¨ç»“æ„è¿›è¡Œé«˜ä¿çœŸç¥ç»æ¸²æŸ“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥ç»“æ„åŒ–çš„æ–¹å¼é¢„æµ‹é«˜æ–¯æ–‘ç‚¹çš„å±æ€§ï¼Œä»è€Œå…‹æœäº†å½“å‰ 3D é«˜æ–¯æ–‘ç‚¹æ–¹æ³•çš„ç¼ºç‚¹ï¼Œå³å¤©çœŸåœ°ä¼˜åŒ–é«˜æ–¯å±æ€§è€Œå¿½ç•¥äº†åº•å±‚åœºæ™¯ç»“æ„ã€‚ä½¿ç”¨æ‰€æå‡ºçš„å›¾è¡¨ç¤ºï¼Œç›¸é‚»çš„é«˜æ–¯æ–‘ç‚¹å¯ä»¥å…±äº«å’Œèšåˆä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›åœºæ™¯æ¸²æŸ“åŠå…¶å‡ ä½•å½¢çŠ¶çš„ä¿ç•™ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ–°çš„è§†å›¾åˆæˆä¸­å¯ä»¥ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™ 3D-GS çš„å®æ—¶æ¸²æŸ“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§ç®€å•ä½†æœ‰æ•ˆçš„ä¸­é—´ç‚¹æ’å€¼æ–¹æ¡ˆï¼Œä¸ 3D-GS æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå¯ä»¥å®ç°é«˜è¾¾ 24 å€çš„å­˜å‚¨å‡å°‘ï¼ŒåŒæ—¶ä¿ç•™é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œè€Œæ— éœ€ä½¿ç”¨ä»»ä½•å‹ç¼©å’Œé‡åŒ–ç®—æ³•ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯æ˜äº†åœ¨ 3D-GS ä¸­å¼•å…¥ç»“æ„çš„å¥½å¤„ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç»“æ„æ„ŸçŸ¥æ–¹æ³•ï¼Œä»¥ç»“æ„åŒ–çš„æ–¹å¼é¢„æµ‹é«˜æ–¯æ–‘ç‚¹çš„å±æ€§ï¼Œä»è€Œå…‹æœäº†å½“å‰ 3D é«˜æ–¯æ–‘ç‚¹æ–¹æ³•çš„ç¼ºç‚¹ï¼Œå³å¤©çœŸåœ°ä¼˜åŒ–é«˜æ–¯å±æ€§è€Œå¿½ç•¥äº†åº•å±‚åœºæ™¯ç»“æ„ï¼›Performanceï¼šåœ¨æ–°çš„è§†å›¾åˆæˆä¸­å¯ä»¥ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™ 3D-GS çš„å®æ—¶æ¸²æŸ“ï¼›Workloadï¼šå¼•å…¥äº†ç®€å•çš„ä¸­é—´ç‚¹æ’å€¼æ–¹æ¡ˆï¼Œä¸ 3D-GS æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå¯ä»¥å®ç°é«˜è¾¾ 24 å€çš„å­˜å‚¨å‡å°‘ï¼ŒåŒæ—¶ä¿ç•™é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œè€Œæ— éœ€ä½¿ç”¨ä»»ä½•å‹ç¼©å’Œé‡åŒ–ç®—æ³•ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-1c24f7a12b1fbb5ce1ccb02f3443561a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1a4caa28b767c498c125adefb63f6bdf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36c36153d224a88e577e256a3ca35a36.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cb72df76b73fb4cab74c37be6a089579.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>æ ¹æ®å¤šè§†è§’è§†é¢‘åˆ›å»ºé«˜ä¿çœŸå¤´éƒ¨å½¢è±¡æ˜¯AR/VRåº”ç”¨çš„å…³é”®é—®é¢˜ã€‚MeGAé€šè¿‡ä¸ºä¸åŒå¤´éƒ¨ç»„ä»¶é‡‡ç”¨åˆé€‚çš„è¡¨è¾¾æ–¹å¼ï¼Œæé«˜äº†æ¸²æŸ“è´¨é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>MeGAé‡‡ç”¨FLAMEç½‘æ ¼è¡¨ç¤ºé¢éƒ¨ï¼Œå¹¶ä½¿ç”¨UVä½ç§»å›¾æä¾›é¡¶ç‚¹åç§»ä»¥æå‡ä¸ªæ€§åŒ–å‡ ä½•ç»†èŠ‚ã€‚</li><li>åˆ©ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“è·å¾—é¢éƒ¨é¢œè‰²ï¼Œå¹¶å°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ä»¥å®ç°çœŸå®æ„Ÿæ¸²æŸ“ã€‚</li><li>MeGAä½¿ç”¨3Dé«˜æ–¯æ³¼æº…æ„å»ºé™æ€ç»å…¸å¤´å‘ï¼Œå¹¶åº”ç”¨åˆšæ€§å˜æ¢å’ŒåŸºäºMLPçš„å˜å½¢åœºæ¥å¤„ç†å¤æ‚åŠ¨æ€è¡¨æƒ…ã€‚</li><li>ç»“åˆé®æŒ¡æ„ŸçŸ¥æ··åˆï¼ŒMeGAä¸ºæ•´ä¸ªå¤´éƒ¨ç”Ÿæˆæ›´é«˜ä¿çœŸçš„æ¸²æŸ“ï¼Œå¹¶æ”¯æŒå‘å‹æ”¹å˜å’Œçº¹ç†ç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</li><li>åœ¨NeRSembleæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜MeGAè®¾è®¡æœ‰æ•ˆï¼Œä¼˜äºä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>è®ºæ–‡æ ‡é¢˜ï¼šMeGAï¼šç”¨äºé«˜ä¿çœŸæ¸²æŸ“å’Œå¤´éƒ¨ç¼–è¾‘çš„æ··åˆç½‘æ ¼é«˜æ–¯å¤´éƒ¨å¤´åƒ</p></li><li><p>ä½œè€…ï¼šCong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šå¤´éƒ¨å¤´åƒã€é«˜ä¿çœŸæ¸²æŸ“ã€å¤´éƒ¨ç¼–è¾‘ã€æ··åˆè¡¨ç¤º</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.19026</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåˆ›å»ºé«˜ä¿çœŸå¤´éƒ¨å¤´åƒå¯¹äº AR/VR åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„æ–¹æ³•éš¾ä»¥åŒæ—¶ä¸ºæ‰€æœ‰å¤´éƒ¨ç»„ä»¶ï¼ˆå¦‚çš®è‚¤ã€å¤´å‘ï¼‰è·å¾—é«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨å•ä¸€è¡¨ç¤ºæ¥å»ºæ¨¡å…·æœ‰ä¸åŒç‰¹å¾çš„ç»„ä»¶ã€‚</p><p>ï¼ˆ2ï¼‰ä»¥å¾€æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•æ¢ç´¢äº†åŸºäºç½‘æ ¼ã€åŸºäº NeRF å’ŒåŸºäº 3D é«˜æ–¯çš„è¡¨ç¤ºï¼Œå–å¾—äº†æ˜¾ç€è¿›å±•ã€‚ç„¶è€Œï¼Œå¤´éƒ¨æ˜¯ä¸€ä¸ªå¤æ‚çš„â€œç‰©ä½“â€ï¼ŒåŒ…å«å…·æœ‰ä¸åŒç‰¹å¾çš„ç»„ä»¶ï¼Œå› æ­¤ä¸å­˜åœ¨å•ä¸€çš„è¡¨ç¤ºå¯ä»¥åŒæ—¶å¾ˆå¥½åœ°å»ºæ¨¡æ‰€æœ‰ç»„ä»¶ã€‚ä½¿ç”¨å•ä¸€è¡¨ç¤ºå»ºæ¨¡æ‰€æœ‰å¤´éƒ¨ç»„ä»¶å¿…ç„¶ä¼šç‰ºç‰²ä¸€éƒ¨åˆ†çš„æ¸²æŸ“è´¨é‡ã€‚</p><p>ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆç½‘æ ¼é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œé€‰æ‹©ä¸€ä¸ªå¢å¼ºçš„ FLAME ç½‘æ ¼ä½œä¸ºé¢éƒ¨è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹ä¸€ä¸ª UV ä½ç§»å›¾æ¥æä¾›æ¯ä¸ªé¡¶ç‚¹çš„åç§»é‡ï¼Œä»¥æ”¹å–„ä¸ªæ€§åŒ–çš„å‡ ä½•ç»†èŠ‚ã€‚ä¸ºäº†å®ç°é€¼çœŸçš„æ¸²æŸ“ï¼Œä½¿ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“è·å¾—é¢éƒ¨é¢œè‰²ï¼Œå¹¶å°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ã€‚å¯¹äºå¤´å‘å»ºæ¨¡ï¼Œé¦–å…ˆä½¿ç”¨ 3D é«˜æ–¯æ³¼æº…æ„å»ºä¸€ä¸ªé™æ€çš„è§„èŒƒå¤´å‘ã€‚è¿›ä¸€æ­¥åº”ç”¨åˆšæ€§å˜æ¢å’ŒåŸºäº MLP çš„å˜å½¢åœºæ¥å¤„ç†å¤æ‚çš„åŠ¨æ€è¡¨æƒ…ã€‚ç»“åˆé®æŒ¡æ„ŸçŸ¥æ··åˆï¼ŒMeGA ä¸ºæ•´ä¸ªå¤´éƒ¨ç”Ÿæˆäº†æ›´é«˜ä¿çœŸçš„æ¸²æŸ“æ•ˆæœï¼Œå¹¶è‡ªç„¶åœ°æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚</p><p>ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨ NeRSemble æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•æœ‰æ•ˆï¼Œä¼˜äºä»¥å¾€çš„å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æ”¯æŒå„ç§ç¼–è¾‘åŠŸèƒ½ï¼ŒåŒ…æ‹¬å‘å‹æ›´æ”¹å’Œçº¹ç†ç¼–è¾‘ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p><ol><li><p>Methods:                    (1): æå‡ºæ··åˆç½‘æ ¼é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ï¼›</p><pre><code>            (2): é€‰æ‹©ä¸€ä¸ªå¢å¼ºçš„ FLAME ç½‘æ ¼ä½œä¸ºé¢éƒ¨è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹ä¸€ä¸ª UV ä½ç§»å›¾æ¥æä¾›æ¯ä¸ªé¡¶ç‚¹çš„åç§»é‡ï¼Œä»¥æ”¹å–„ä¸ªæ€§åŒ–çš„å‡ ä½•ç»†èŠ‚ï¼›            (3): ä½¿ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“è·å¾—é¢éƒ¨é¢œè‰²ï¼Œå¹¶å°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼›            (4): å¯¹äºå¤´å‘å»ºæ¨¡ï¼Œé¦–å…ˆä½¿ç”¨ 3D é«˜æ–¯æ³¼æº…æ„å»ºä¸€ä¸ªé™æ€çš„è§„èŒƒå¤´å‘ï¼Œè¿›ä¸€æ­¥åº”ç”¨åˆšæ€§å˜æ¢å’ŒåŸºäº MLP çš„å˜å½¢åœºæ¥å¤„ç†å¤æ‚çš„åŠ¨æ€è¡¨æƒ…ï¼›            (5): ç»“åˆé®æŒ¡æ„ŸçŸ¥æ··åˆï¼ŒMeGA ä¸ºæ•´ä¸ªå¤´éƒ¨ç”Ÿæˆäº†æ›´é«˜ä¿çœŸçš„æ¸²æŸ“æ•ˆæœï¼Œå¹¶è‡ªç„¶åœ°æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆç½‘æ ¼é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç¥ç»ç½‘æ ¼å»ºæ¨¡äººè„¸ï¼Œä½¿ç”¨ 3DGS å»ºæ¨¡å¤´å‘ï¼Œåœ¨é«˜ä¿çœŸæ¸²æŸ“å’Œå¤´éƒ¨ç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾ç€æ•ˆæœã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ›æ–°æ€§åœ°æå‡ºäº†æ··åˆç½‘æ ¼é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ï¼Œå¹¶è®¾è®¡äº†é®æŒ¡æ„ŸçŸ¥æ··åˆæ¨¡å—ï¼Œå®ç°äº†å¤´éƒ¨çš„é«˜ä¿çœŸæ¸²æŸ“å’Œç¼–è¾‘ã€‚</p><p>æ€§èƒ½ï¼šåœ¨ NeRSemble æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ¸²æŸ“è´¨é‡å’Œç¼–è¾‘åŠŸèƒ½æ–¹é¢å‡ä¼˜äºä»¥å¾€çš„å…ˆè¿›æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å·¥ä½œé‡ç›¸å¯¹è¾ƒå¤§ï¼Œéœ€è¦è®­ç»ƒç¥ç»ç½‘æ ¼ã€3DGS å¤´å‘æ¨¡å‹å’Œé®æŒ¡æ„ŸçŸ¥æ··åˆæ¨¡å—ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details><h2 id="3D-Gaussian-Splatting-with-Deferred-Reflection"><a href="#3D-Gaussian-Splatting-with-Deferred-Reflection" class="headerlink" title="3D Gaussian Splatting with Deferred Reflection"></a>3D Gaussian Splatting with Deferred Reflection</h2><p><strong>Authors:Keyang Ye, Qiming Hou, Kun Zhou</strong></p><p>The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting. </p><p><a href="http://arxiv.org/abs/2404.18454v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯è¾å°„åœºç»“åˆå»¶æ—¶ç€è‰²å¤§å¹…æå‡åå°„æ•ˆæœï¼Œæ— éœ€é¢å¤–æ—¶é—´æˆæœ¬</p><p><strong>Key Takeaways</strong></p><ul><li>ç¥ç»å’Œé«˜æ–¯è¾å°„åœºæ–¹æ³•åœ¨è§†å›¾åˆæˆä¸­å–å¾—å·¨å¤§è¿›å±•ï¼Œä½†é•œé¢åå°„å¤„ç†å›°éš¾ã€‚</li><li>æå‡ºäº†å»¶æ—¶ç€è‰²æ–¹æ³•ï¼Œä½¿ç”¨é«˜æ–¯æ•£å°„æœ‰æ•ˆæ¸²æŸ“é•œé¢åå°„ã€‚</li><li>ç¯å¢ƒè´´å›¾åå°„æ¨¡å‹çš„æŒ‘æˆ˜åœ¨äºéœ€è¦å‡†ç¡®çš„è¡¨é¢æ³•çº¿ï¼Œè€Œæ³•çº¿ä¼°è®¡å—æ–­ç»­æ¢¯åº¦çš„é™åˆ¶ã€‚</li><li>åˆ©ç”¨å»¶æ—¶ç€è‰²ç”Ÿæˆçš„é€åƒç´ åå°„æ¢¯åº¦ï¼Œæ¡¥æ¥äº†ç›¸é‚»é«˜æ–¯çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚</li><li>å‡†ç¡®çš„æ³•çº¿ä¼°è®¡é€æ¸ä¼ æ’­ï¼Œæœ€ç»ˆè¦†ç›–æ‰€æœ‰åå°„ç‰©ä½“ã€‚</li><li>æ–¹æ³•å¤§å¹…ä¼˜äºæœ€å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨åˆæˆé«˜è´¨é‡é•œé¢åå°„æ•ˆæœæ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li><li>åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­ï¼Œå³°å€¼ä¿¡å™ªæ¯” (PSNR) å‡å¾—åˆ°ä¸€è‡´æé«˜ï¼Œä¸”è¿è¡Œå¸§é€Ÿç‡å‡ ä¹ä¸åŸå§‹é«˜æ–¯æ•£å°„ç›¸åŒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 3D é«˜æ–¯æ–‘ç‚¹ä¸å»¶è¿Ÿåå°„</p></li><li><p>Authors: Keyang Ye, Qiming Hou, Kun Zhou</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: Novel view synthesis, deferred shading, real-time rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.18454.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’ŒåŸºäºé«˜æ–¯çš„ä½“æ¸²æŸ“æ–¹æ³•åœ¨æ–°å‹è§†å›¾åˆæˆä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†é•œé¢åå°„ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•å¦‚ 3D é«˜æ–¯æ–‘ç‚¹ï¼ˆ3DGSï¼‰è™½ç„¶æä¾›äº†åŸºäºæ¯ä¸ªé«˜æ–¯çš„çƒè°å‡½æ•°ï¼ˆSHï¼‰è¿›è¡Œè§†ç‚¹ç›¸å…³ç€è‰²ï¼Œä½†å…¶æ–¹å‘é¢‘ç‡å¤ªæœ‰é™ï¼Œæ— æ³•å»ºæ¨¡é•œé¢åå°„ã€‚è®­ç»ƒè¿‡ç¨‹ä¼šäº§ç”Ÿé«˜æ–¯ï¼Œä»¥æ˜¾å¼åœ°æ‹Ÿåˆé•œé¢åå°„ï¼Œä½†è¿™ç§æ–¹æ³•ä¼šå¯¼è‡´è§†è§‰ä¼ªå½±å’Œè¾ƒå·®çš„æ€§èƒ½ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯å»¶è¿Ÿç€è‰²ï¼Œå®ƒåˆ©ç”¨å»¶è¿Ÿç€è‰²ç”Ÿæˆçš„é€åƒç´ åå°„æ¢¯åº¦æ¥å¼¥åˆç›¸é‚»é«˜æ–¯ä¼˜åŒ–è¿‡ç¨‹ä¹‹é—´çš„å·®è·ï¼Œå…è®¸è¿‘ä¹æ­£ç¡®çš„æ³•çº¿ä¼°è®¡é€æ¸ä¼ æ’­ï¼Œæœ€ç»ˆè¦†ç›–æ‰€æœ‰åå°„ç‰©ä½“ã€‚</p><p>(4): åœ¨åˆæˆé«˜è´¨é‡é•œé¢åå°„æ•ˆæœçš„ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯å’ŒåŒæœŸå·¥ä½œï¼Œè¯æ˜äº†åˆæˆå’ŒçœŸå®åœºæ™¯çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰éƒ½æœ‰æŒç»­çš„æé«˜ï¼ŒåŒæ—¶è¿è¡Œå¸§é€Ÿç‡å‡ ä¹ä¸åŸå§‹åå°„æ— å…³çš„é«˜æ–¯æ–‘ç‚¹ç›¸åŒã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–¹æ³•é‡‡ç”¨å»¶è¿Ÿæ¸²æŸ“æ¨¡å‹ï¼ŒåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼›</p><p>ï¼ˆ2ï¼‰ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é«˜æ–¯æ–‘ç‚¹ï¼Œåˆ©ç”¨é«˜æ–¯å‚æ•° Î˜ğ‘–ã€æ¯ä¸ªé«˜æ–¯è§†ç‚¹ç›¸å…³çš„çƒè°å‡½æ•°é¢œè‰² ğ‘ğ‘– (v) è®¡ç®—åƒç´ é¢œè‰² ğ¶(v)ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šç¬¬äºŒé˜¶æ®µæ˜¯å»¶è¿Ÿåå°„ï¼Œå°†æ³•çº¿å‘é‡ ğ‘›ğ‘– å’Œé•œé¢åå°„å¼ºåº¦æ ‡é‡ ğ‘Ÿğ‘– èå…¥é«˜æ–¯æ–‘ç‚¹ï¼Œç”Ÿæˆæœ€ç»ˆåƒç´ é¢œè‰² ğ¶â€²(v)ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜è´¨é‡çš„å»¶è¿Ÿé«˜æ–¯æ–‘ç‚¹æ¸²æŸ“å™¨ï¼Œä¸“é—¨ç”¨äºåå°„ã€‚å®ƒå±•ç¤ºäº†ç¨³å®šçš„è®­ç»ƒå’Œå‡ ä¹ä¸åŸå§‹ 3D é«˜æ–¯æ–‘ç‚¹ç›¸åŒçš„å¸§é€Ÿç‡çš„ç«äº‰æ€§è§†è§‰è´¨é‡ï¼Œè¿˜ç”Ÿæˆäº†å‡†ç¡®çš„è¡¨é¢æ³•çº¿å’Œç¯å¢ƒè´´å›¾ã€‚æˆ‘ä»¬çš„å»¶è¿Ÿç€è‰²æ–¹æ³•å¯èƒ½ä¸ºæœªæ¥çš„æ¢ç´¢å¼€è¾Ÿäº†è®¸å¤šå¯èƒ½æ€§ã€‚åœ¨é«˜æ–¯æ–‘ç‚¹çš„èƒŒæ™¯ä¸‹æ¢ç´¢æ¸²æŸ“æ–¹ç¨‹çš„æ›´å¤šåˆ›é€ æ€§åˆ†å‰²å°†æ˜¯ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…ã€‚æˆ‘ä»¬çš„ç®¡é“è¿˜å¯ä»¥æ‰©å±•åˆ°è¶…å‡ºç¯å¢ƒè´´å›¾çš„é«˜è´¨é‡åå°„ç®—æ³•ï¼ŒåŒ…æ‹¬å±å¹•ç©ºé—´åå°„ [McGuire and Mara 2014] å’Œç¡¬ä»¶å…‰çº¿è¿½è¸ªã€‚å°† 3D é«˜æ–¯å’Œå¯å¾®æ¸²æŸ“æ¨å¹¿åˆ°æ­¤ç±»æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜åå°„è´¨é‡ã€‚æ¢ç´¢æ·»åŠ åŸºäºç‰©ç†çš„ç²—ç³™åº¦ã€å°†æˆ‘ä»¬çš„æ–¹æ³•æ¨å¹¿åˆ°å…‰æ³½ææ–™çš„å¯èƒ½æ€§ä¹Ÿå¾ˆæœ‰è¶£ã€‚è‡´è°¢ï¼šè¿™é¡¹å·¥ä½œéƒ¨åˆ†å¾—åˆ°äº†å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘ï¼ˆç¼–å· 62227806 å’Œ U23A20311ï¼‰å’Œ XPLORER PRIZE çš„æ”¯æŒã€‚æºä»£ç å’Œæ•°æ®å¯ä» https://gapszju.github.com/3DGS-DR è·å–ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†å»¶è¿Ÿç€è‰²æ–¹æ³•ï¼Œåˆ©ç”¨å»¶è¿Ÿç€è‰²ç”Ÿæˆçš„é€åƒç´ åå°„æ¢¯åº¦æ¥å¼¥åˆç›¸é‚»é«˜æ–¯ä¼˜åŒ–è¿‡ç¨‹ä¹‹é—´çš„å·®è·ï¼Œå…è®¸è¿‘ä¹æ­£ç¡®çš„æ³•çº¿ä¼°è®¡é€æ¸ä¼ æ’­ï¼Œæœ€ç»ˆè¦†ç›–æ‰€æœ‰åå°„ç‰©ä½“ã€‚</p><p>æ€§èƒ½ï¼šåœ¨åˆæˆé«˜è´¨é‡é•œé¢åå°„æ•ˆæœçš„ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯å’ŒåŒæœŸå·¥ä½œï¼Œè¯æ˜äº†åˆæˆå’ŒçœŸå®åœºæ™¯çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰éƒ½æœ‰æŒç»­çš„æé«˜ï¼ŒåŒæ—¶è¿è¡Œå¸§é€Ÿç‡å‡ ä¹ä¸åŸå§‹åå°„æ— å…³çš„é«˜æ–¯æ–‘ç‚¹ç›¸åŒã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å·¥ä½œé‡ä¸åŸå§‹ 3D é«˜æ–¯æ–‘ç‚¹ç›¸ä¼¼ï¼Œåœ¨åˆæˆé«˜è´¨é‡é•œé¢åå°„æ•ˆæœçš„ä»»åŠ¡ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯å’ŒåŒæœŸå·¥ä½œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-01f3cc91b932b34b556b0aeef26ce855.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7317cab6e01a55d9c668cb2940a49ed4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3dd63fe60a37ec3cb9014da779955c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd20bc6eb9ac41d7e4d7028cacc5d273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f4637de64153c20555d7c194c23899d.jpg" align="middle"></details><h2 id="High-quality-Surface-Reconstruction-using-Gaussian-Surfels"><a href="#High-quality-Surface-Reconstruction-using-Gaussian-Surfels" class="headerlink" title="High-quality Surface Reconstruction using Gaussian Surfels"></a>High-quality Surface Reconstruction using Gaussian Surfels</h2><p><strong>Authors:Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</strong></p><p>We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods. </p><p><a href="http://arxiv.org/abs/2404.17774v2">PDF</a> Results added and improved</p><p><strong>Summary</strong><br>é’ˆå¯¹ä¸‰ç»´é«˜æ–¯ç‚¹å’Œæ›²é¢å…ƒç´ çš„ä¼˜ç‚¹ï¼Œæå‡ºä¸€ç§æ–°çš„ç‚¹äº‘è¡¨å¾æ–¹å¼é«˜æ–¯æ›²é¢å…ƒç´ ï¼Œæœ‰æ•ˆæ”¹å–„äº†ä¼˜åŒ–ç¨³å®šæ€§å’Œæ›²é¢å¯¹é½ï¼Œå¹¶é€šè¿‡å®¹ç§¯å‰ªåˆ‡å’ŒåŸºäºæ³Šæ¾çš„ç­›é€‰é‡å»ºæ–¹æ³•æå‡äº†å½¢çŠ¶é‡å»ºçš„ç²¾åº¦ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºé«˜æ–¯æ›²é¢å…ƒç´ ï¼Œå°†ä¸‰ç»´é«˜æ–¯ç‚¹çš„ä¼˜åŒ–çµæ´»æ€§ä¸æ›²é¢å…ƒç´ çš„å¯¹é½ç‰¹æ€§ç›¸ç»“åˆã€‚</li><li>é€šè¿‡å°†ä¸‰ç»´é«˜æ–¯ç‚¹çš„ z å°ºåº¦è®¾ä¸º 0ï¼Œå°†ä¸‰ç»´æ¤­åœ†ä½“å‹å¹³ä¸ºäºŒç»´æ¤­åœ†ï¼Œä¸ºä¼˜åŒ–å™¨æä¾›æ¸…æ™°æŒ‡å¼•ã€‚</li><li>å°†å±€éƒ¨ z è½´è§†ä¸ºæ³•çº¿æ–¹å‘ï¼Œæå¤§æé«˜äº†ä¼˜åŒ–çš„ç¨³å®šæ€§å’Œè¡¨é¢å¯¹é½ã€‚</li><li>è®¾è®¡è‡ªç›‘ç£æ³•çº¿æ·±åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œå¼¥è¡¥å…±æ–¹å·®çŸ©é˜µä¸­è®¡ç®—å‡ºçš„å±€éƒ¨ z è½´å¯¼æ•°ä¸ºé›¶çš„é—®é¢˜ã€‚</li><li>èåˆå•ç›®æ³•çº¿å…ˆéªŒå’Œå‰æ™¯æ©ç ï¼Œå¢å¼ºé‡å»ºè´¨é‡ï¼Œç¼“è§£é«˜å…‰å’ŒèƒŒæ™¯å¸¦æ¥çš„å½±å“ã€‚</li><li>æå‡ºä½“ç§¯åˆ‡å‰²æ–¹æ³•ï¼Œèšåˆé«˜æ–¯æ›²é¢å…ƒç´ çš„ä¿¡æ¯ï¼Œå»é™¤æ·±åº¦å›¾ä¸­ç”± alpha æ··åˆäº§ç”Ÿçš„é”™è¯¯ç‚¹ã€‚</li><li>é‡‡ç”¨å¸¦ç­›é€‰çš„æ³Šæ¾é‡å»ºæ–¹æ³•å¯¹èåˆçš„æ·±åº¦å›¾è¿›è¡Œé‡å»ºï¼Œæå–è¡¨é¢ç½‘æ ¼ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨é¢é‡å»ºæ–¹é¢ä¼˜äºç°æœ‰çš„ç¥ç»ä½“ç»˜åˆ¶å’Œç‚¹äº‘ç»˜åˆ¶æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: é«˜æ–¯è¡¨é¢å…ƒçš„é«˜è´¨é‡è¡¨é¢é‡å»º</p></li><li><p>Authors: Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡ä¸å›¾å½¢å­¦å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: 3D Surface Reconstruction, Gaussian Surfels, Depth-normal Consistency</p></li><li><p>Urls: https://arxiv.org/abs/2404.17774 , Github:None</p></li><li><p>Summary: </p></li></ol><p>(1):ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å›¾åƒåˆæˆçš„ä»»åŠ¡ä¸Šå–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†åœ¨è¡¨é¢é‡å»ºä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚è¡¨é¢å¯¹é½ä¸å‡†ç¡®ã€ä¼˜åŒ–ä¸ç¨³å®šä»¥åŠå¯¹é«˜å…‰å’ŒèƒŒæ™¯åŒºåŸŸçš„æ•æ„Ÿæ€§ã€‚</p><p>(2):ä»¥å¾€çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–æŸå¤±å‡½æ•°å’Œä½¿ç”¨å…ˆéªŒä¿¡æ¯æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†æ•ˆæœæœ‰é™ã€‚</p><p>(3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‚¹è¡¨ç¤ºâ€”â€”é«˜æ–¯è¡¨é¢å…ƒï¼Œå®ƒç»“åˆäº† 3D é«˜æ–¯ç‚¹çš„çµæ´»ä¼˜åŒ–è¿‡ç¨‹å’Œè¡¨é¢å…ƒçš„è¡¨é¢å¯¹é½ç‰¹æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå°† 3D é«˜æ–¯ç‚¹çš„ z å°ºåº¦è®¾ç½®ä¸º 0ï¼Œæœ‰æ•ˆåœ°å°†åŸå§‹çš„ 3D æ¤­çƒå‹æ‰æˆ 2D æ¤­åœ†ã€‚è¿™ç§è®¾è®¡ä¸ºä¼˜åŒ–å™¨æä¾›äº†æ˜ç¡®çš„æŒ‡å¯¼ï¼Œé€šè¿‡å°†å±€éƒ¨ z è½´è§†ä¸ºæ³•çº¿æ–¹å‘ï¼Œæå¤§åœ°æé«˜äº†ä¼˜åŒ–ç¨³å®šæ€§å’Œè¡¨é¢å¯¹é½ã€‚åŒæ—¶è®¾è®¡äº†ä¸€ä¸ªè‡ªç›‘ç£çš„æ³•çº¿æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥è§£å†³æ­¤è®¾ç½®ä¸­ä»åæ–¹å·®çŸ©é˜µè®¡ç®—çš„å±€éƒ¨ z è½´çš„å¯¼æ•°ä¸ºé›¶çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é›†æˆäº†å•ç›®æ³•çº¿å…ˆéªŒå’Œå‰æ™¯æ©ç ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œå‡è½»äº†ä¸é«˜å…‰å’ŒèƒŒæ™¯ç›¸å…³çš„é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§ä½“ç§¯åˆ‡å‰²æ–¹æ³•æ¥èšåˆé«˜æ–¯è¡¨é¢å…ƒçš„ä¿¡æ¯ï¼Œä»¥å»é™¤ alpha æ··åˆç”Ÿæˆçš„æ·±åº¦å›¾ä¸­çš„é”™è¯¯ç‚¹ã€‚æœ€åï¼Œå°†ç­›é€‰æ³Šæ¾é‡å»ºæ–¹æ³•åº”ç”¨äºèåˆçš„æ·±åº¦å›¾ä»¥æå–è¡¨é¢ç½‘æ ¼ã€‚</p><p>(4):å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ç¥ç»ä½“æ¸²æŸ“å’ŒåŸºäºç‚¹çš„æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨è¡¨é¢é‡å»ºæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç‚¹è¡¨ç¤ºâ€”â€”é«˜æ–¯è¡¨é¢å…ƒï¼Œå®ƒç»“åˆäº† 3D é«˜æ–¯ç‚¹çš„çµæ´»ä¼˜åŒ–è¿‡ç¨‹å’Œè¡¨é¢å…ƒçš„è¡¨é¢å¯¹é½ç‰¹æ€§ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè®¾è®¡äº†ä¸€ä¸ªè‡ªç›‘ç£çš„æ³•çº¿æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥è§£å†³æ­¤è®¾ç½®ä¸­ä»åæ–¹å·®çŸ©é˜µè®¡ç®—çš„å±€éƒ¨ z è½´çš„å¯¼æ•°ä¸ºé›¶çš„é—®é¢˜ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé›†æˆäº†å•ç›®æ³•çº¿å…ˆéªŒå’Œå‰æ™¯æ©ç ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œå‡è½»äº†ä¸é«˜å…‰å’ŒèƒŒæ™¯ç›¸å…³çš„é—®é¢˜ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºäº†ä¸€ç§ä½“ç§¯åˆ‡å‰²æ–¹æ³•æ¥èšåˆé«˜æ–¯è¡¨é¢å…ƒçš„ä¿¡æ¯ï¼Œä»¥å»é™¤ alpha æ··åˆç”Ÿæˆçš„æ·±åº¦å›¾ä¸­çš„é”™è¯¯ç‚¹ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå°†ç­›é€‰æ³Šæ¾é‡å»ºæ–¹æ³•åº”ç”¨äºèåˆçš„æ·±åº¦å›¾ä»¥æå–è¡¨é¢ç½‘æ ¼ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç‚¹è¡¨ç¤ºâ€”â€”é«˜æ–¯è¡¨é¢å…ƒï¼Œå®ƒç»“åˆäº† 3D é«˜æ–¯ç‚¹çš„çµæ´»ä¼˜åŒ–è¿‡ç¨‹å’Œè¡¨é¢å…ƒçš„è¡¨é¢å¯¹é½ç‰¹æ€§ï¼Œè®¾è®¡äº†ä¸€ä¸ªè‡ªç›‘ç£çš„æ³•çº¿æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥è§£å†³æ­¤è®¾ç½®ä¸­ä»åæ–¹å·®çŸ©é˜µè®¡ç®—çš„å±€éƒ¨ z è½´çš„å¯¼æ•°ä¸ºé›¶çš„é—®é¢˜ï¼Œé›†æˆäº†å•ç›®æ³•çº¿å…ˆéªŒå’Œå‰æ™¯æ©ç ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œå‡è½»äº†ä¸é«˜å…‰å’ŒèƒŒæ™¯ç›¸å…³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä½“ç§¯åˆ‡å‰²æ–¹æ³•æ¥èšåˆé«˜æ–¯è¡¨é¢å…ƒçš„ä¿¡æ¯ï¼Œä»¥å»é™¤ alpha æ··åˆç”Ÿæˆçš„æ·±åº¦å›¾ä¸­çš„é”™è¯¯ç‚¹ï¼Œå°†ç­›é€‰æ³Šæ¾é‡å»ºæ–¹æ³•åº”ç”¨äºèåˆçš„æ·±åº¦å›¾ä»¥æå–è¡¨é¢ç½‘æ ¼ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡æ–¹æ³•åœ¨è¡¨é¢é‡å»ºæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ç‚¹è¡¨ç¤ºâ€”â€”é«˜æ–¯è¡¨é¢å…ƒï¼Œè®¾è®¡äº†ä¸€ä¸ªè‡ªç›‘ç£çš„æ³•çº¿æ·±åº¦ä¸€è‡´æ€§æŸå¤±ï¼Œé›†æˆäº†å•ç›®æ³•çº¿å…ˆéªŒå’Œå‰æ™¯æ©ç ï¼Œæå‡ºäº†ä¸€ç§ä½“ç§¯åˆ‡å‰²æ–¹æ³•æ¥èšåˆé«˜æ–¯è¡¨é¢å…ƒçš„ä¿¡æ¯ï¼›æ€§èƒ½ï¼šä¸æœ€å…ˆè¿›çš„ç¥ç»ä½“æ¸²æŸ“å’ŒåŸºäºç‚¹çš„æ¸²æŸ“æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨è¡¨é¢é‡å»ºæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒé«˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ea805e1d2146685877956d96c3f2411f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d553afdbbcbed9ac6b50b06fa71184c3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c60a2604158ad2952f8aa6bf05e4bfb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ffafd283accaa363affe36c81454980.jpg" align="middle"></details><h2 id="GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting"><a href="#GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting"></a>GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting</h2><p><strong>Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</strong></p><p>We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalkerâ€™s superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at <a href="https://github.com/KU-CVLAB/GaussianTalker/">https://github.com/KU-CVLAB/GaussianTalker/</a> . </p><p><a href="http://arxiv.org/abs/2404.16012v2">PDF</a> Project Page: <a href="https://ku-cvlab.github.io/GaussianTalker">https://ku-cvlab.github.io/GaussianTalker</a></p><p><strong>Summary</strong><br>é«˜æ–¯è¯´è¯è€…ï¼šå®æ—¶ç”Ÿæˆå§¿åŠ¿å¯æ§ä¼šè¯´è¯çš„å¤´éƒ¨</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºäº†ä¸€ç§åä¸ºé«˜æ–¯è¯´è¯è€…çš„æ–°æ¡†æ¶ï¼Œç”¨äºå®æ—¶ç”Ÿæˆå§¿åŠ¿å¯æ§çš„ä¼šè¯´è¯çš„å¤´éƒ¨ã€‚</li><li>åˆ©ç”¨ 3D é«˜æ–¯ splattingï¼ˆ3DGSï¼‰çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ï¼ŒåŒæ—¶è§£å†³äº†ç›´æ¥ä½¿ç”¨è¯­éŸ³éŸ³é¢‘æ§åˆ¶ 3DGS çš„æŒ‘æˆ˜ã€‚</li><li>æ„å»ºå¤´éƒ¨è§„èŒƒçš„ 3DGS è¡¨ç¤ºï¼Œå¹¶ä½¿å…¶ä¸éŸ³é¢‘åŒæ­¥å˜å½¢ã€‚</li><li>å…³é”®çš„è§è§£æ˜¯å°† 3D é«˜æ–¯å±æ€§ç¼–ç æˆå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶åœ¨å…¶ä¸­ä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ§åˆ¶æ¯ä¸ªé«˜æ–¯å±æ€§ã€‚</li><li>è¯¥è®¾è®¡åˆ©ç”¨äº†ç©ºé—´æ„ŸçŸ¥ç‰¹å¾ï¼Œå¹¶å¼ºåˆ¶ç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚</li><li>å°†ç‰¹å¾åµŒå…¥é¦ˆé€åˆ°ç©ºé—´-éŸ³é¢‘æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹æ¯ä¸ªé«˜æ–¯çš„å±æ€§çš„å¸§çº§åç§»ã€‚</li><li>ä¸ä»¥å‰çš„ä¸²è”æˆ–ä¹˜æ³•æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å¤„ç†å¤§é‡é«˜æ–¯åŠå…¶å¤æ‚å‚æ•°æ—¶æ›´ç¨³å®šã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œé«˜æ–¯è¯´è¯è€…åœ¨é¢éƒ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥ç²¾åº¦å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢æ›´èƒœä¸€ç­¹ã€‚</li><li>å…·ä½“è€Œè¨€ï¼Œé«˜æ–¯è¯´è¯è€…ä»¥é«˜è¾¾ 120 FPS çš„éå‡¡æ¸²æŸ“é€Ÿåº¦ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„åŸºå‡†ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: é«˜æ–¯è¯´è¯è€…ï¼šå®æ—¶é«˜ä¿çœŸè¯´è¯å¤´éƒ¨åˆæˆ</p></li><li><p>Authors: Kyusun Choï¼ŒJoungbin Leeï¼ŒHeeji Yoonï¼ŒYeobin Hongï¼ŒJaehoon Koï¼ŒSangjun Ahnï¼ŒSeungryong Kim</p></li><li><p>Affiliation: éŸ©å›½å¤§å­¦</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/ , https://github.com/ku-cvlab/GaussianTalker</p></li><li><p>Summary:</p></li></ol><p>(1): æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ç”Ÿæˆå—ä»»æ„è¯­éŸ³éŸ³é¢‘é©±åŠ¨çš„è¯´è¯å¤´éƒ¨è§†é¢‘ï¼Œè¿™é¡¹ä»»åŠ¡æœ‰å¾ˆå¤šç”¨é€”ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ•°å­—äººã€è™šæ‹Ÿæ›¿èº«ã€ç”µå½±åˆ¶ä½œå’Œç”µè¯ä¼šè®®ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•æœ‰ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¥è§£å†³æ­¤ä»»åŠ¡ï¼Œä½†å®ƒä»¬ä¸ä¸“æ³¨äºæ§åˆ¶å¤´éƒ¨å§¿åŠ¿ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çœŸå®æ€§å’Œé€‚ç”¨æ€§ã€‚æœ€è¿‘ï¼Œè®¸å¤šç ”ç©¶åº”ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥åˆ›å»ºå¯æ§å§¿åŠ¿çš„è¯´è¯äººåƒã€‚é€šè¿‡ç›´æ¥è°ƒèŠ‚ NeRF å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸­çš„éŸ³é¢‘ç‰¹å¾ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆæˆä¸è¾“å…¥éŸ³é¢‘å˜´å”‡åŒæ­¥çš„è§†å›¾ä¸€è‡´çš„ 3D å¤´éƒ¨ç»“æ„ã€‚è™½ç„¶è¿™äº›åŸºäº NeRF çš„æŠ€æœ¯å®ç°äº†é«˜è´¨é‡å’Œä¸€è‡´çš„è§†è§‰è¾“å‡ºï¼Œä½†å®ƒä»¬ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯åˆ©ç”¨ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ã€‚3DGS è¢«å…¬è®¤ä¸º NeRF çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæä¾›äº†å¯æ¯”çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶æ˜¾ç€æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚è™½ç„¶ 3DGS æœ€åˆè¢«æè®®ç”¨äºé‡å»ºé™æ€ 3D åœºæ™¯ï¼Œä½†åç»­å·¥ä½œå·²å°†å…¶æ‰©å±•åˆ°åŠ¨æ€åœºæ™¯ã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰ç ”ç©¶åˆ©ç”¨ 3DGS åˆ›å»ºå…·æœ‰å¯æ§è¾“å…¥çš„åŠ¨æ€ 3D åœºæ™¯ï¼Œå…¶ä¸­å¤§å¤šæ•°éƒ½ä¸“æ³¨äºä½¿ç”¨ä¸­é—´ç½‘æ ¼è¡¨ç¤ºæ¥é©±åŠ¨ 3D é«˜æ–¯ã€‚ç„¶è€Œï¼Œä¾èµ–ä¸­é—´ 3D ç½‘æ ¼è¡¨ç¤ºï¼ˆä¾‹å¦‚ FLAMEï¼‰è¿›è¡Œå˜å½¢é€šå¸¸ç¼ºä¹å¤´å‘å’Œé¢éƒ¨çš±çº¹çš„ç»†èŠ‚ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†ä»¥ä¸‹æˆå°±ï¼š- ä¸ç°æœ‰çš„ 3D è¯´è¯äººè„¸åˆæˆæ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œæ¨ç†æ—¶é—´æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä»¥æ›´é«˜çš„ FPS è¿è¡Œã€‚- æœ¬æ–‡æ–¹æ³•å®ç°äº†é«˜è¾¾ 120 FPS çš„æ˜¾ç€æ¸²æŸ“é€Ÿåº¦ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„åŸºå‡†ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•é‡‡ç”¨ 3D é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›ï¼Œ3DGS è¢«å…¬è®¤ä¸º NeRF çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒæä¾›äº†å¯æ¯”çš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶æ˜¾ç€æé«˜äº†æ¨ç†é€Ÿåº¦ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•é€šè¿‡å­¦ä¹ å…·æœ‰ä¸‰å¹³é¢è¡¨ç¤ºçš„è§„èŒƒ 3D é«˜æ–¯ä½“æ¥å­¦ä¹ è¯´è¯å¤´çš„è§„èŒƒå½¢çŠ¶ï¼Œå¤šåˆ†è¾¨ç‡ä¸‰å¹³é¢è¡¨ç¤ºåˆ©ç”¨ 3DGS çš„æ˜¾å¼ 3D è¡¨ç¤ºï¼ŒåŒæ—¶è¿˜åˆ©ç”¨éšå¼ç¥ç»è¾å°„åœºçš„ç¼–ç ç©ºé—´ä¿¡æ¯ï¼Œå¯¹äºæ¯ä¸ªè§„èŒƒ 3D ä½ç½®ï¼Œä»å¤šåˆ†è¾¨ç‡ä¸‰å¹³é¢è¡¨ç¤ºä¸­æå–ç‰¹å¾åµŒå…¥ï¼Œè¿™äº›ç‰¹å¾åµŒå…¥ç”¨äºè®¡ç®—æ¯ä¸ªç‚¹çš„æ¯”ä¾‹ã€æ—‹è½¬ã€çƒè°å‡½æ•°å’Œä¸é€æ˜åº¦ï¼Œè¿™äº›è®¡ç®—å‡ºçš„å±æ€§æ„æˆäº†è¯´è¯å¤´çš„è§„èŒƒ 3D é«˜æ–¯ä½“ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•é‡‡ç”¨è¯­éŸ³åŠ¨ä½œäº¤å‰æ³¨æ„æ¨¡å—èåˆ 3D é«˜æ–¯ä½“ç‰¹å¾å’ŒéŸ³é¢‘ç‰¹å¾ï¼Œä»¥å‡†ç¡®å»ºæ¨¡ç”±è¾“å…¥éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¿åŠ¨ï¼Œç©ºé—´éŸ³é¢‘æ³¨æ„æ¨¡å—åŒ…å«å¤šç»„äº¤å‰æ³¨æ„å±‚å’Œå‰é¦ˆå±‚ï¼Œæ¯ç»„é€šè¿‡è·³è·ƒè¿æ¥ç›¸äº’è¿æ¥ï¼Œè¯¥æ¨¡å—å°†ç©ºé—´ç‰¹å¾ä¸ç¬¬ n å¸§çš„éŸ³é¢‘ç‰¹å¾è¿›è¡Œäº¤å‰æ³¨æ„è®¡ç®—ï¼Œä»è€Œè¾“å‡ºç‰¹å¾æˆåŠŸåœ°å°†éŸ³é¢‘ç‰¹å¾ä¸æ¯ä¸ª 3D é«˜æ–¯ä½“æ•è·çš„ä¸°å¯Œé¢éƒ¨ç»†èŠ‚ç›¸èåˆï¼›</p><p>ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•é€šè¿‡å¼•å…¥é™„åŠ è¾“å…¥æ¡ä»¶æ¥æ•è·éè¯­è¨€åŠ¨ä½œï¼Œä»è€Œå°†ä¸è¯­éŸ³ç›¸å…³çš„è¿åŠ¨ä¸å•ç›®è§†é¢‘åŒºåˆ†å¼€æ¥ï¼Œéµå¾ªå…ˆå‰çš„å·¥ä½œï¼Œé¦–å…ˆåº”ç”¨æ˜¾å¼çœ¨çœ¼æ§åˆ¶ä¸çœ¼ç›ç‰¹å¾ï¼Œå…·ä½“åœ°ï¼Œä½¿ç”¨é¢éƒ¨åŠ¨ä½œç¼–ç ç³»ç»Ÿä¸­çš„ AU45 æ¥æè¿°çœ¨çœ¼ç¨‹åº¦ï¼Œå¹¶åˆ©ç”¨æ­£å¼¦ä½ç½®ç¼–ç ä»¥åŒ¹é…è¾“å…¥ç»´åº¦ï¼Œæ­¤å¤–ï¼Œå°†æ‘„åƒæœºè§†ç‚¹ä½œä¸ºè¾…åŠ©è¾“å…¥ä»¥åŒºåˆ†éè¯­è¨€åœºæ™¯å˜åŒ–ï¼Œè™½ç„¶å°†é€å¸§æ‘„åƒæœºå…¬å¼åŒ–ä¸ºé¢éƒ¨è§†ç‚¹ï¼Œä½†å…¸å‹çš„è§†é¢‘æ˜¯åœ¨å¤´éƒ¨è¿ç»­ç§»åŠ¨æ—¶ä½¿ç”¨é™æ€æ‘„åƒæœºæ‹æ‘„çš„ï¼Œå› æ­¤ï¼Œè‚–åƒå›¾åƒçš„å˜åŒ–ï¼ˆä¾‹å¦‚å¤´å‘ä½ç§»å’Œå…‰ç…§å˜åŒ–ï¼‰ç‹¬ç«‹äºè¯­éŸ³éŸ³é¢‘ï¼Œå› æ­¤ï¼Œä½¿ç”¨é¢éƒ¨è§†ç‚¹åµŒå…¥ä½œä¸ºé™„åŠ è¾“å…¥æ¡ä»¶æ¥åŒºåˆ†è¿™äº›éå¬è§‰å˜åŒ–ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† GaussianTalkerï¼Œä¸€ä¸ªæ–°é¢–çš„å®æ—¶å§¿æ€å¯æ§ 3D è¯´è¯äººè„¸åˆæˆæ¡†æ¶ï¼Œåˆ©ç”¨ 3D é«˜æ–¯ä½“è¿›è¡Œå¤´éƒ¨è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è°ƒèŠ‚é«˜æ–¯åŸè¯­å®ç°äº†å¯¹é«˜æ–¯åŸè¯­çš„ç²¾ç¡®æ§åˆ¶ï¼Œä»è€Œè·å¾—äº†æ¯”ä»¥å¾€æ›´å¥½çš„ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œæ¨ç†æ—¶é—´ï¼Œå¹¶ä¸”ä»¥æ›´é«˜çš„ FPS è¿è¡Œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ 3D é«˜æ–¯ä½“è¿›è¡Œå¤´éƒ¨è¡¨ç¤ºï¼Œå®ç°äº†å§¿æ€å¯æ§çš„ 3D è¯´è¯äººè„¸åˆæˆï¼›æ€§èƒ½ï¼šåœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œæ¨ç†æ—¶é—´æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå®ç°äº†é«˜è¾¾ 120 FPS çš„æ˜¾ç€æ¸²æŸ“é€Ÿåº¦ï¼›å·¥ä½œé‡ï¼šä¸ç°æœ‰çš„ 3D è¯´è¯äººè„¸åˆæˆæ¨¡å‹ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨ä¿çœŸåº¦ã€å”‡å½¢åŒæ­¥å’Œæ¨ç†æ—¶é—´æ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä»¥æ›´é«˜çš„ FPS è¿è¡Œã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation"><a href="#OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation" class="headerlink" title="OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation"></a>OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation</h2><p><strong>Authors:Lizhi Wang, Feng Zhou, Jianqin Yin</strong></p><p>Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation. OMEGAS employs a multi-step approach, grounded in several excellent off-the-shelf methodologies. Specifically, initially, we utilize the Segment Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS), thereby creating a basic 3DGS model of the target object. Then, we leverage large-scale diffusion priors to further refine the details of the 3DGS model, especially aimed at addressing invisible or occluded object portions from the original scene views. Subsequently, by re-rendering the 3DGS model onto the scene views, we achieve accurate object segmentation and effectively remove the background. Finally, these target-only images are used to improve the 3DGS model further and extract the definitive 3D object mesh by the SuGaR model. In various scenarios, our experiments demonstrate that OMEGAS significantly surpasses existing scene reconstruction methods. Our project page is at: <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> </p><p><a href="http://arxiv.org/abs/2404.15891v2">PDF</a> arXiv admin note: text overlap with arXiv:2311.17061 by other authors</p><p><strong>Summary</strong><br>å¤§å‹åœºæ™¯ä¸­ç‰¹å®šç‰©ä½“çš„é«˜ç²¾åº¦ä¸‰ç»´é‡å»ºæ¡†æ¶ï¼šOMEGASï¼Œé€šè¿‡é«˜æ–¯åˆ†å‰²å¼•å¯¼ç‰©ä½“ç½‘æ ¼æå–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>OMEGAS æ¡†æ¶å¯ä»å¤§å‹åœºæ™¯ä¸­é«˜ç²¾åº¦é‡å»ºç‰¹å®šç‰©ä½“ã€‚</li><li>ç»“åˆ Segment Anything Model (SAM) å’Œå¤§å‹æ‰©æ•£å…ˆéªŒï¼Œæ”¹å–„ 3DGS æ¨¡å‹ç»†èŠ‚ã€‚</li><li>é‡æ–°æ¸²æŸ“ 3DGS æ¨¡å‹ï¼Œå®ç°å‡†ç¡®ç‰©ä½“åˆ†å‰²å¹¶å»é™¤èƒŒæ™¯ã€‚</li><li>ä½¿ç”¨ç›®æ ‡å›¾åƒï¼Œè¿›ä¸€æ­¥ä¼˜åŒ– 3DGS æ¨¡å‹å¹¶æå–æœ€ç»ˆ 3D ç‰©ä½“ç½‘æ ¼ã€‚</li><li>OMEGAS åœ¨å„ç§åœºæ™¯ä¸­ä¼˜äºç°æœ‰åœºæ™¯é‡å»ºæ–¹æ³•ã€‚</li><li>ä»£ç å’Œæ•°æ®å¯åœ¨ <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> è·å–ã€‚</li><li>OMEGAS é€‚ç”¨äºç›®æ ‡ç‰©ä½“éƒ¨åˆ†é®æŒ¡æˆ–ä¸å¯è§çš„æƒ…å†µã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: OMEGAS: é«˜æ–¯åˆ†å‰²å¼•å¯¼çš„å¤§åœºæ™¯ç‰©ä½“ç½‘æ ¼æå–</p></li><li><p>Authors: Lizhi Wang, Feng Zhou, Jianqin Yin</p></li><li><p>Affiliation: åŒ—äº¬é‚®ç”µå¤§å­¦</p></li><li><p>Keywords: Mesh Reconstruction, 3D Gaussian Splatting, Diffusion Model</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.15891 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): éšç€ 3D é‡å»ºæŠ€æœ¯çš„è¿›æ­¥ï¼Œå¤æ‚ 3D åœºæ™¯çš„é«˜è´¨é‡å®æ—¶æ¸²æŸ“æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œä»å¤§åœºæ™¯ä¸­ç²¾ç¡®é‡å»ºç‰¹å®šç‰©ä½“ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åœºæ™¯é‡å»ºæŠ€æœ¯ç»å¸¸å¯¼è‡´ç‰©ä½“ç»†èŠ‚çº¹ç†ä¸¢å¤±ï¼Œå¹¶ä¸”æ— æ³•é‡å»ºåœ¨è§†å›¾ä¸­è¢«é®æŒ¡æˆ–çœ‹ä¸è§çš„ç‰©ä½“éƒ¨åˆ†ã€‚</p><p>(2): è¿‡å»çš„é‡å»ºæ–¹æ³•éš¾ä»¥å¤„ç†å¤§åœºæ™¯ä¸­å¤æ‚ç‰©ä½“ï¼Œå¹¶ä¸”åœ¨å¤„ç†é®æŒ¡å’Œä¸å¯è§åŒºåŸŸæ—¶å­˜åœ¨é—®é¢˜ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•ä»¥ 3D é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ä¸ºåŸºç¡€ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥ç»†åŒ–ç»†èŠ‚ï¼Œå¹¶ç»“åˆç›®æ ‡åˆ†å‰²å’Œç½‘æ ¼æå–æŠ€æœ¯ï¼Œä»¥æé«˜é‡å»ºç²¾åº¦å’Œæ•ˆç‡ã€‚</p><p>(3): æœ¬æ–‡æå‡º OMEGAS æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¤šæ­¥æ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰æŒ‡å¯¼ 3DGS çš„åˆ†å‰²ï¼Œåˆ›å»ºç›®æ ‡ç‰©ä½“çš„åŸºæœ¬ 3DGS æ¨¡å‹ã€‚ç„¶åï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒè¿›ä¸€æ­¥ç»†åŒ– 3DGS æ¨¡å‹çš„ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŸå§‹åœºæ™¯è§†å›¾ä¸­ä¸å¯è§æˆ–è¢«é®æŒ¡çš„ç‰©ä½“éƒ¨åˆ†ã€‚éšåï¼Œå°† 3DGS æ¨¡å‹é‡æ–°æ¸²æŸ“åˆ°åœºæ™¯è§†å›¾ä¸Šï¼Œå®ç°ç²¾ç¡®çš„ç›®æ ‡åˆ†å‰²å¹¶æœ‰æ•ˆå»é™¤èƒŒæ™¯ã€‚æœ€åï¼Œåˆ©ç”¨è¿™äº›ä»…åŒ…å«ç›®æ ‡çš„å›¾åƒè¿›ä¸€æ­¥æ”¹è¿› 3DGS æ¨¡å‹ï¼Œå¹¶é€šè¿‡ SuGaR æ¨¡å‹æå–æœ€ç»ˆçš„ 3D ç‰©ä½“ç½‘æ ¼ã€‚</p><p>(4): åœ¨å„ç§åœºæ™¯ä¸­ï¼Œå®éªŒè¡¨æ˜ OMEGAS æ˜æ˜¾ä¼˜äºç°æœ‰çš„åœºæ™¯é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚ç‰©ä½“ã€é®æŒ¡å’Œä¸å¯è§åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D ç‰©ä½“ç½‘æ ¼ï¼Œä¸ºå¢å¼ºç°å®ã€æ¸¸æˆå’Œå¤§è§„æ¨¡ 3D æ•°æ®é›†ç”Ÿæˆç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†æ”¯æŒã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨ SAM å¼•å¯¼ 3DGS åˆ†å‰²ï¼Œæ„å»ºç›®æ ‡ç‰©ä½“çš„åŸºæœ¬ 3DGS æ¨¡å‹ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåº”ç”¨å¤§è§„æ¨¡æ‰©æ•£å…ˆéªŒï¼ˆStable Diffusionï¼‰ç»†åŒ– 3DGS æ¨¡å‹ç»†èŠ‚ï¼Œç‰¹åˆ«æ˜¯ä¸å¯è§æˆ–è¢«é®æŒ¡éƒ¨åˆ†ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå°† 3DGS æ¨¡å‹é‡æ–°æ¸²æŸ“åˆ°åœºæ™¯è§†å›¾ä¸Šï¼Œè·å¾—ç²¾ç¡®ç›®æ ‡åˆ†å‰²å¹¶å»é™¤èƒŒæ™¯ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨ä»…åŒ…å«ç›®æ ‡çš„å›¾åƒè¿›ä¸€æ­¥æ”¹è¿› 3DGS æ¨¡å‹ï¼Œé€šè¿‡ SuGaR æ¨¡å‹æå–æœ€ç»ˆ 3D ç‰©ä½“ç½‘æ ¼ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯åˆ†å‰²å¼•å¯¼çš„å¤§åœºæ™¯ç‰©ä½“ç½‘æ ¼æå–æ¡†æ¶ OMEGASï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å¤šè§†è§’åœºæ™¯å›¾åƒä¸­æœ‰æ•ˆæå–ç›®æ ‡ç‰©ä½“çš„ç²¾ç»†ç½‘æ ¼ï¼Œå¹¶èƒ½å¤Ÿé‡å»ºè¢«é®æŒ¡æˆ–ä¸å¯è§çš„ç‰©ä½“éƒ¨åˆ†ã€‚OMEGAS åˆ›æ–°æ€§åœ°èåˆäº† SAMã€3DGSã€Stabled Diffusion å’Œ SuGaR æ¨¡å‹ç­‰å¤šç§ä¼˜ç§€æ–¹æ³•ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›®æ ‡çš„ç»†èŠ‚çº¹ç†å’ŒæŠ—é®æŒ¡æ€§æ–¹é¢å‡è¡¨ç°å‡ºæå¤§çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬å¸Œæœ› OMEGAS èƒ½å¤Ÿä¸º 3D é‡å»ºé¢†åŸŸæä¾›æ–°çš„æ€è·¯ï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯åˆ†å‰²å¼•å¯¼çš„å¤§åœºæ™¯ç‰©ä½“ç½‘æ ¼æå–æ¡†æ¶ OMEGASï¼›æ€§èƒ½ï¼šåœ¨å¤„ç†å¤æ‚ç‰©ä½“ã€é®æŒ¡å’Œä¸å¯è§åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ 3D ç‰©ä½“ç½‘æ ¼ï¼›å·¥ä½œé‡ï¼šä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒOMEGAS çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦è¾ƒé•¿çš„å¤„ç†æ—¶é—´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e4ef7cc371681a1b1a10401043bee74c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a62638572af15479eb987b4dae28d70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b782cee0b88f29d10ae78c3dec02dbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b5e6dde7b196e2a509f4476175ec837.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8a1d8354bdc5dd159b09b49e8c7efd4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f986fc824e450fb1b91fc8f6304e7c73.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>3Dé«˜æ–¯æ•£å¸ƒæŠ€æœ¯åˆæˆéŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒï¼Œç²¾å‡†å”‡éƒ¨åŠ¨ä½œåŠåŠ¨æ€é«˜æ–¯æ¸²æŸ“ï¼Œå®ç°é€¼çœŸæµç•…çš„è¯´è¯äººå¤´åƒåˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŸºäº3Dé«˜æ–¯æ•£å¸ƒçš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´åƒåˆæˆæ–°æ–¹æ³•ã€‚</li><li>æ˜¾å¼é«˜æ–¯è¡¨ç¤ºï¼Œé€šè¿‡å°†é«˜æ–¯ä¸3Dé¢éƒ¨æ¨¡å‹ç»‘å®šï¼Œå®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚</li><li>æ‰¬å£°å™¨ç‰¹å®šè¿åŠ¨è½¬æ¢å™¨ï¼Œå®ç°ç²¾å‡†çš„æ‰¬å£°å™¨ç‰¹å®šå”‡éƒ¨åŠ¨ä½œã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ï¼Œé€šè¿‡æ½œåœ¨å§¿åŠ¿å¼•å…¥æ‰¬å£°å™¨ç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œå¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºã€‚</li><li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGaussianTalkeråœ¨è¯´è¯äººå¤´åƒåˆæˆä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>æ¸²æŸ“é€Ÿåº¦è¾¾åˆ° 130 FPSï¼Œæ˜¾ç€è¶…è¿‡å®æ—¶æ¸²æŸ“æ€§èƒ½é˜ˆå€¼ã€‚</li><li>å¯éƒ¨ç½²åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šï¼Œå…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: é«˜æ–¯è¯´è¯è€…ï¼šåŸºäº 3D é«˜æ–¯å–·å°„çš„ç‰¹å®šè¯´è¯è€…è¯´è¯å¤´åˆæˆ</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: é˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion control, Facial animation</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šæœ€è¿‘åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆå·¥ä½œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœã€‚ç„¶è€Œï¼Œç”±äº NeRF éšå¼è¡¨ç¤ºå¯¼è‡´çš„å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œä¾‹å¦‚ä¸åŒæ­¥æˆ–ä¸è‡ªç„¶çš„å”‡éƒ¨åŠ¨ä½œï¼Œä»¥åŠè§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨å”‡éƒ¨è¿åŠ¨ä¸åŒæ­¥ã€è¡¨æƒ…æ§åˆ¶ä¸è¶³ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ã€‚</p><p>(3): ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯å–·å°„çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerã€‚é€šè¿‡å°†é«˜æ–¯ä½“ç»‘å®šåˆ° 3D é¢éƒ¨æ¨¡å‹ï¼Œåˆ©ç”¨ 3D é«˜æ–¯ä½“çš„æ˜¾å¼è¡¨ç¤ºç‰¹æ€§ï¼Œå®ç°äº†å¯¹é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚GaussianTalker ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šç‰¹å®šè¯´è¯è€…è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ã€‚ç‰¹å®šè¯´è¯è€…è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°äº†ç‰¹å®šäºç›®æ ‡è¯´è¯è€…çš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†ç‰¹å®šè¯´è¯è€…çš„æ··åˆå½¢çŠ¶ï¼Œä»¥å®ç°ç²¾ç¡®çš„è¡¨æƒ…æ§åˆ¶ã€‚</p><p>(4): æ€§èƒ½ï¼šåœ¨è¯´è¯å¤´åˆæˆä»»åŠ¡ä¸Šï¼ŒGaussianTalker åœ¨å”‡éƒ¨è¿åŠ¨åŒæ­¥ã€è¡¨æƒ…æ§åˆ¶å’Œè§†è§‰è´¨é‡æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•çš„ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåŸºäº 3D é«˜æ–¯å–·å°„çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerï¼Œé€šè¿‡å°†é«˜æ–¯ä½“ç»‘å®šåˆ° 3D é¢éƒ¨æ¨¡å‹ï¼Œåˆ©ç”¨ 3D é«˜æ–¯ä½“çš„æ˜¾å¼è¡¨ç¤ºç‰¹æ€§ï¼Œå®ç°äº†å¯¹é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šGaussianTalker ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šç‰¹å®šè¯´è¯è€…è¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ã€‚ç‰¹å®šè¯´è¯è€…è¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°äº†ç‰¹å®šäºç›®æ ‡è¯´è¯è€…çš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†ç‰¹å®šè¯´è¯è€…çš„æ··åˆå½¢çŠ¶ï¼Œä»¥å®ç°ç²¾ç¡®çš„è¡¨æƒ…æ§åˆ¶ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨è¯´è¯å¤´åˆæˆä»»åŠ¡ä¸Šï¼ŒGaussianTalker åœ¨å”‡éƒ¨è¿åŠ¨åŒæ­¥ã€è¡¨æƒ…æ§åˆ¶å’Œè§†è§‰è´¨é‡æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„ä¸»è¦è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯å–·å°„çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerï¼Œå°†é«˜æ–¯ä½“ä¸ FLAME æ¨¡å‹ç›¸ç»“åˆï¼Œé€šè¿‡ç‰¹å®šè¯´è¯è€…çš„ FLAME è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ï¼Œå®ç°äº†å”‡éƒ¨åŠ¨ä½œåŒæ­¥ã€è¡¨æƒ…æ§åˆ¶å’Œè§†è§‰è´¨é‡çš„æå‡ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºåŸºäº 3D é«˜æ–¯å–·å°„çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerï¼Œé€šè¿‡ç‰¹å®šè¯´è¯è€…çš„ FLAME è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ï¼Œå®ç°äº†å”‡éƒ¨åŠ¨ä½œåŒæ­¥ã€è¡¨æƒ…æ§åˆ¶å’Œè§†è§‰è´¨é‡çš„æå‡ï¼›æ€§èƒ½ï¼šåœ¨è¯´è¯å¤´åˆæˆä»»åŠ¡ä¸Šï¼ŒGaussianTalker åœ¨å”‡éƒ¨è¿åŠ¨åŒæ­¥ã€è¡¨æƒ…æ§åˆ¶å’Œè§†è§‰è´¨é‡æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šGaussianTalker çš„æ¸²æŸ“é€Ÿåº¦æå¿«ï¼Œè¿œè¶…å…¶ä»–æ–¹æ³•ï¼›</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  Spectrally Pruned Gaussian Fields with Neural Compensation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/</id>
    <published>2024-05-02T02:33:18.000Z</published>
    <updated>2024-05-02T02:33:18.719Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the modelâ€™s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>äººè„¸å¤´åƒé€šè¿‡è§†è§‰ä¿¡å·é©±åŠ¨ï¼Œåœ¨è·¨äººç‰©åˆæˆä¸­é¢‡å—æ¬¢è¿ï¼Œå³ä½¿é©¾é©¶å‘˜ä¸åŠ¨ç”»äººç‰©ä¸åŒï¼Œè¿™ä¸€å¯Œæœ‰æŒ‘æˆ˜æ€§ä¸”é«˜åº¦å®ç”¨çš„æ–¹æ³•ä¹Ÿé€‚ç”¨ã€‚æœ€è¿‘æå‡ºçš„ MegaPortrait æ¨¡å‹å·²åœ¨è¿™ä¸ªé¢†åŸŸå±•ç°äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚æˆ‘ä»¬å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†æ·±å…¥çš„æ£€æŸ¥å’Œè¯„ä¼°ï¼Œç‰¹åˆ«å…³æ³¨å…¶è¡¨æƒ…æè¿°ç¬¦çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶å‘ç°äº†è¯¥æ¨¡å‹è¡¨è¾¾å¼ºçƒˆé¢éƒ¨åŠ¨ä½œçš„èƒ½åŠ›å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„ä¸­éƒ½æå‡ºäº†é‡å¤§æ”¹å˜ï¼Œä»è€Œå¼•å…¥äº†æˆ‘ä»¬çš„ EMOPortraits æ¨¡å‹ï¼Œåœ¨æ­¤æˆ‘ä»¬ï¼š æé«˜äº†æ¨¡å‹å¯¹äºç²¾ç¡®è¡¨ç°å¼ºçƒˆçš„ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„èƒ½åŠ›ï¼Œåœ¨æƒ…ç»ªä¼ é€’ä»»åŠ¡ä¸­åˆ›ä¸‹äº†æ–°çš„æœ€å…ˆè¿›æˆæœï¼Œåœ¨æŒ‡æ ‡å’Œè´¨é‡æ–¹é¢å‡è¶…è¿‡äº†å…ˆå‰çš„æ‰€æœ‰æ–¹æ³•ã€‚ å°†åŸºäºè¯­éŸ³çš„æ¨¡å¼çº³å…¥æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œåœ¨åŸºäºéŸ³é¢‘çš„é¢éƒ¨åŠ¨ç”»ä¸­å–å¾—äº†ä¸€æµçš„æ€§èƒ½ï¼Œä»è€Œå¯ä»¥é€šè¿‡åŒ…æ‹¬è§†è§‰ä¿¡å·ã€éŸ³é¢‘æˆ–ä¸¤è€…çš„èåˆåœ¨å†…ç­‰å„ç§æ–¹å¼é©±åŠ¨æºèº«ä»½ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šè§†å›¾è§†é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¹¿æ³›çš„å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†ä¸­ç¼ºå°‘æ­¤ç±»æ•°æ®ç©ºç™½çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>EMOPortraits æ¨¡å‹æ˜¾è‘—æé«˜äº†ç”Ÿæˆå¼ºçƒˆä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„èƒ½åŠ›ã€‚</li><li>æ–°æ¨¡å‹åœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨æŒ‡æ ‡å’Œè´¨é‡æ–¹é¢å‡åˆ›ä¸‹æ–°é«˜ã€‚</li><li>EMOPortraits æ¨¡å‹é›†æˆäº†è¯­éŸ³é©±åŠ¨æ¨¡å¼ï¼Œåœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ã€‚</li><li>è¯¥æ¨¡å‹æ”¯æŒé€šè¿‡è§†è§‰ä¿¡å·ã€éŸ³é¢‘æˆ–ä¸¤è€…èåˆç­‰å¤šç§æ–¹å¼è¿›è¡Œé©±åŠ¨ã€‚</li><li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šè§†å›¾è§†é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¹¿æ³›çš„å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…ã€‚</li><li>è¯¥æ•°æ®é›†å¡«è¡¥äº†ç°æœ‰æ•°æ®é›†ä¸­æ­¤ç±»æ•°æ®çš„ç©ºç™½ã€‚</li><li>EMOPortraits æ¨¡å‹ä¸ºè·¨é©±åŠ¨åˆæˆå’ŒéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»æä¾›äº†æ–°çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: EMOPortraits: æƒ…æ„Ÿå¢å¼ºçš„å¤šæ¨¡æ€ä¸€æ¬¡æ€§å¤´éƒ¨å¤´åƒ</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: ä¼¦æ•¦å¸å›½ç†å·¥å­¦é™¢</p></li><li><p>Keywords: å¤´éƒ¨å¤´åƒã€æƒ…æ„Ÿä¼ é€’ã€è¯­éŸ³é©±åŠ¨é¢éƒ¨åŠ¨ç”»ã€å¤šæ¨¡æ€ã€é¢éƒ¨è¡¨æƒ…æè¿°ç¬¦</p></li><li><p>Urls: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šå¤´éƒ¨å¤´åƒåŠ¨ç”»åœ¨äº¤å‰é©±åŠ¨åˆæˆä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå…¶ä¸­é©±åŠ¨è€…ä¸åŠ¨ç”»è§’è‰²ä¸åŒï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æŒ‘æˆ˜æ€§ä½†éå¸¸å®ç”¨çš„æ–¹æ³•ã€‚æœ€è¿‘æå‡ºçš„ MegaPortraits æ¨¡å‹å·²åœ¨æ­¤é¢†åŸŸå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šæœ¬æ–‡å¯¹ MegaPortraits æ¨¡å‹è¿›è¡Œäº†æ·±å…¥çš„æ£€æŸ¥å’Œè¯„ä¼°ï¼Œç‰¹åˆ«å…³æ³¨å…¶é¢éƒ¨è¡¨æƒ…æè¿°ç¬¦çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶å‘ç°äº†å…¶åœ¨è¡¨è¾¾å¼ºçƒˆé¢éƒ¨åŠ¨ä½œæ–¹é¢çš„å‡ ä¸ªé™åˆ¶ã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡åœ¨è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„ä¸­æå‡ºäº†å®è´¨æ€§çš„æ”¹å˜ï¼Œå¼•å…¥äº† EMOPortraits æ¨¡å‹ï¼Œå…¶ä¸­ï¼š    - å¢å¼ºäº†æ¨¡å‹å¯¹å¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„å¿ å®æ”¯æŒèƒ½åŠ›ï¼Œåœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œåœ¨æŒ‡æ ‡å’Œè´¨é‡æ–¹é¢éƒ½è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚    - å°†è¯­éŸ³é©±åŠ¨æ¨¡å¼çº³å…¥æ¨¡å‹ï¼Œåœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ï¼Œä½¿å¾—å¯ä»¥é€šè¿‡è§†è§‰ä¿¡å·ã€éŸ³é¢‘æˆ–ä¸¤è€…çš„æ··åˆç­‰å¤šç§æ–¹å¼é©±åŠ¨æºèº«ä»½ã€‚    - æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å„ç§å¼ºçƒˆå’Œä¸å¯¹ç§°çš„é¢éƒ¨è¡¨æƒ…ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†ä¸­æ­¤ç±»æ•°æ®çš„ç©ºç™½ã€‚</p><p>(4): æ€§èƒ½å’Œç›®æ ‡æ”¯æŒï¼šåœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸­ï¼ŒEMOPortraits æ¨¡å‹åœ¨æŒ‡æ ‡å’Œè´¨é‡æ–¹é¢éƒ½è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚åœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­ï¼Œè¯¥æ¨¡å‹å®ç°äº†é¡¶çº§æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼Œå³å¢å¼ºæ¨¡å‹å¯¹å¼ºçƒˆé¢éƒ¨è¡¨æƒ…çš„æ”¯æŒèƒ½åŠ›ï¼Œå¹¶å°†å…¶ç”¨äºå¤šæ¨¡æ€é©±åŠ¨ã€‚</p><ol><li>Methods:</li></ol><p>(1): å¯¹ MegaPortraits æ¨¡å‹çš„æ½œåœ¨ç©ºé—´è¿›è¡Œæ·±å…¥æ£€æŸ¥å’Œè¯„ä¼°ï¼Œå‘ç°å…¶åœ¨è¡¨è¾¾å¼ºçƒˆé¢éƒ¨åŠ¨ä½œæ–¹é¢çš„é™åˆ¶ï¼›</p><p>(2): åœ¨è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„ä¸­æå‡ºå®è´¨æ€§æ”¹å˜ï¼Œå¼•å…¥ EMOPortraits æ¨¡å‹ï¼Œå¢å¼ºå…¶å¯¹å¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„æ”¯æŒèƒ½åŠ›ï¼›</p><p>(3): å°†è¯­éŸ³é©±åŠ¨æ¨¡å¼çº³å…¥æ¨¡å‹ï¼Œå®ç°éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»çš„é¡¶çº§æ€§èƒ½ï¼›</p><p>(4): æå‡ºä¸€ä¸ªæ–°é¢–çš„å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå¡«è¡¥ç°æœ‰æ•°æ®é›†ä¸­å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…æ•°æ®çš„ç©ºç™½ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>           ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† EMOPortraitsï¼Œä¸€ç§åœ¨å›¾åƒé©±åŠ¨ã€è·¨èº«ä»½æƒ…æ„Ÿè½¬æ¢ä¸­å…·æœ‰å“è¶Šæ€§èƒ½çš„æ–°å‹ç¥ç»å¤´åƒåˆ›å»ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„è¯­éŸ³é©±åŠ¨æ¨¡å¼ä½¿å¾—å¯ä»¥é€šè¿‡å¤šç§æ¡ä»¶ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€å¤´éƒ¨è¿åŠ¨ï¼‰æ¥é©±åŠ¨é¢éƒ¨åŠ¨ç”»ã€‚æˆ‘ä»¬æ”¶é›†äº† FEED æ•°æ®é›†ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™å°†æˆä¸ºä»äº‹å„ç§ä»¥äººä¸ºä¸­å¿ƒç ”ç©¶çš„ç ”ç©¶äººå‘˜çš„å®è´µèµ„äº§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿæœ‰ä¸€äº›å±€é™æ€§ã€‚å®ƒä¸ä¼šç”Ÿæˆå¤´åƒçš„èº«ä½“æˆ–è‚©è†€ï¼Œä»è€Œé™åˆ¶äº†ä¸€äº›ç”¨ä¾‹ã€‚æˆ‘ä»¬ç›®å‰å°†æˆ‘ä»¬çš„è¾“å‡ºä¸æºå›¾åƒä¸»ä½“é›†æˆåœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æœ‰æ—¶éš¾ä»¥è¿›è¡Œå‡†ç¡®çš„è¡¨æƒ…è½¬æ¢ï¼Œå¹¶ä¸”åœ¨å¤´éƒ¨å¤§å¹…æ—‹è½¬æ—¶è¡¨ç°ä¸ä½³ã€‚è¿™äº›æŒ‘æˆ˜å¯¹äºæœªæ¥çš„å¢å¼ºè‡³å…³é‡è¦ï¼Œå¹¶ä»ç„¶æ˜¯æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œçš„æ ¸å¿ƒã€‚           ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå¢å¼ºäº†æ¨¡å‹å¯¹å¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„æ”¯æŒèƒ½åŠ›ï¼Œå¹¶å°†å…¶ç”¨äºå¤šæ¨¡æ€é©±åŠ¨ï¼›æ€§èƒ½ï¼šåœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸­è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œåœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†ä¸­å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…æ•°æ®çš„ç©ºç™½ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation"><a href="#CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation" class="headerlink" title="CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation"></a>CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation</h2><p><strong>Authors:Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</strong></p><p>Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.18604v1">PDF</a> </p><p><strong>Summary</strong><br>è¯­éŸ³é©±åŠ¨çš„ 3D äººè„¸åŠ¨ç”»æŠ€æœ¯å·²å‘å±•å¤šå¹´ï¼Œä½†å…¶åœ¨å®é™…åº”ç”¨ä¸­ä»æœªè¾¾åˆ°é¢„æœŸã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ•°æ®é‡é™åˆ¶ã€å”‡å½¢å¯¹é½å’Œé¢éƒ¨è¡¨æƒ…çš„è‡ªç„¶æ€§æ˜¯è¯­éŸ³é©±åŠ¨ 3D äººè„¸åŠ¨ç”»æŠ€æœ¯é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li><li>ç°æœ‰çš„å”‡å½¢å¯¹é½æ–¹æ³•ä»éš¾ä»¥åˆæˆè‡ªç„¶é€¼çœŸçš„è¡¨æƒ…ï¼Œå¯¼è‡´é¢éƒ¨åŠ¨ç”»è¡¨ç°æœºæ¢°åƒµç¡¬ã€‚</li><li>ä»è¯­éŸ³ä¸­æå–æƒ…ç»ªç‰¹å¾ï¼Œä½†é¢éƒ¨åŠ¨ä½œçš„éšæœºæ€§é™åˆ¶äº†æƒ…ç»ªçš„æœ‰æ•ˆè¡¨è¾¾ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CSTalkï¼ˆç›¸å…³æ€§ç›‘ç£ï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿé¢éƒ¨åŠ¨ä½œä¸åŒåŒºåŸŸä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒæŒ‡å¯¼ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒï¼Œä»è€Œç”Ÿæˆç¬¦åˆäººç±»é¢éƒ¨è¿åŠ¨æ¨¡å¼çš„é€¼çœŸè¡¨æƒ…ã€‚</li><li>æˆ‘ä»¬ä½¿ç”¨åŸºäºå…ƒäººç±»è§’è‰²æ¨¡å‹çš„ä¸°å¯Œæ§åˆ¶å‚æ•°ç”Ÿæˆæ›´ç²¾ç»†çš„åŠ¨ç”»ï¼Œå¹¶ä¸ºäº”ç§ä¸åŒæƒ…ç»ªé‡‡é›†äº†ä¸€ä¸ªæ•°æ®é›†ã€‚</li><li>æˆ‘ä»¬ä½¿ç”¨è‡ªç¼–ç å™¨ç»“æ„è®­ç»ƒäº†ä¸€ä¸ªç”Ÿæˆç½‘ç»œï¼Œè¾“å…¥ä¸€ä¸ªæƒ…æ„ŸåµŒå…¥å‘é‡æ¥å®ç°ç”¨æˆ·æ§åˆ¶è¡¨æƒ…çš„ç”Ÿæˆã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: CSTalk: åŸºäºç›¸å…³æ€§çš„è¯­éŸ³é©±åŠ¨ 3D æƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ç”Ÿæˆ</p></li><li><p>Authors: Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</p></li><li><p>Affiliation: ä¸œå—å¤§å­¦è‡ªåŠ¨åŒ–å­¦é™¢</p></li><li><p>Keywords: Speech-driven facial animation, 3D facial animation, Emotional expression, Correlation supervision, MetaHuman character model</p></li><li><p>Urls: Paper, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):è¯­éŸ³é©±åŠ¨çš„ 3D é¢éƒ¨åŠ¨ç”»æŠ€æœ¯è™½ç„¶å‘å±•å¤šå¹´ï¼Œä½†å…¶å®é™…åº”ç”¨æ•ˆæœä»æœªè¾¾åˆ°é¢„æœŸã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ•°æ®é™åˆ¶ã€å”‡éƒ¨å¯¹é½å’Œé¢éƒ¨è¡¨æƒ…çš„è‡ªç„¶åº¦ã€‚è™½ç„¶å”‡éƒ¨å¯¹é½å·²æœ‰è®¸å¤šç›¸å…³ç ”ç©¶ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥åˆæˆè‡ªç„¶é€¼çœŸçš„è¡¨æƒ…ï¼Œå¯¼è‡´é¢éƒ¨åŠ¨ç”»å‘ˆç°æœºæ¢°åƒµç¡¬çš„å¤–è§‚ã€‚å³ä½¿æœ‰äº›ç ”ç©¶ä»è¯­éŸ³ä¸­æå–æƒ…ç»ªç‰¹å¾ï¼Œä½†é¢éƒ¨åŠ¨ä½œçš„éšæœºæ€§é™åˆ¶äº†æƒ…ç»ªçš„æœ‰æ•ˆè¡¨è¾¾ã€‚</p><p>(2):è¯­éŸ³é©±åŠ¨çš„ 3D é¢éƒ¨åŠ¨ç”»ç”Ÿæˆæ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šåŸºäºç½‘æ ¼å’ŒåŸºäºå‚æ•°åŒ–ã€‚åŸºäºç½‘æ ¼çš„æ–¹æ³•ç›´æ¥æ“çºµé¢éƒ¨é¡¶ç‚¹ï¼Œå…è®¸é¢éƒ¨è¡¨æƒ…è¿›è¡Œå¤æ‚çš„å˜åŒ–ï¼Œè€ŒåŸºäºå‚æ•°åŒ–çš„æ–¹æ³•é‡‡ç”¨åŸºäºæ¨¡æ¿çš„æ¡†æ¶ï¼Œé€šè¿‡ç‰¹å®šå‚æ•°æ§åˆ¶é¢éƒ¨åŠ¨ä½œã€‚åŸºäºå‚æ•°åŒ–çš„æ¨¡å‹ä¸­ï¼ŒåŸºäº ARKit 2 æ ‡å‡†çš„ blend-shape æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨ï¼Œå®ƒé‡‡ç”¨ä¸€ç»„é¢„å®šä¹‰çš„é¢éƒ¨å­åŠ¨ä½œä½œä¸ºæ¨¡æ¿ï¼Œé€šè¿‡çº¿æ€§ç»„åˆåˆ†é…ç»™æ¯ä¸ªå­åŠ¨ä½œçš„æƒé‡æ¥ç”Ÿæˆä¸åŒçš„è¡¨æƒ…ã€‚è¿™ç§æ–¹æ³•æä¾›äº†é«˜åº¦çš„å¯æ§æ€§å’Œæ³›åŒ–æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„è™šæ‹Ÿå½¢è±¡ä¸­å¤ç”¨åŠ¨ç”»å‚æ•°ã€‚ç„¶è€Œï¼Œç®€å• 52 ç»´æ•°æ®çš„çº¿æ€§ç»„åˆä¸è¶³ä»¥å®ç°é€¼çœŸè‡ªç„¶çš„åŠ¨ç”»ã€‚ç‰¹åˆ«æ˜¯ï¼Œå‡†ç¡®æ•æ‰ä¸Šéƒ¨é¢éƒ¨åŒºåŸŸçš„ç»†å¾®è¡¨æƒ…ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å› æ­¤ï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¯¹é½å”‡éƒ¨åŠ¨ä½œä¸Šã€‚è™½ç„¶æœ‰ä¸€äº›å°è¯•å°†æƒ…æ„Ÿç‰¹å¾èå…¥åˆ°é¢éƒ¨è¡¨æ¼”ä¸­ï¼Œä¾‹å¦‚ Faceformer å’Œ Emotalkï¼Œä½†è¿™äº›åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨ä»éŸ³é¢‘ä¸­æå–æƒ…æ„Ÿçº¿ç´¢ï¼Œè€Œå¿½ç•¥äº†é¢éƒ¨è¡¨æƒ…é‡å»ºçš„ä¼˜åŒ–ã€‚</p><p>(3):æœ¬æ–‡é‡‡ç”¨åŸºäº Epic æå‡ºçš„ MetaHuman è§’è‰²æ¨¡å‹çš„å‚æ•°åŒ–æ¨¡å‹ï¼Œé€šè¿‡ 185 ä¸ªæ§åˆ¶è£…å¤‡æ“çºµé¢éƒ¨åŠ¨ç”»ï¼Œæ¯ä¸ªè£…å¤‡å¯¹åº”ä¸€ç»„é¢éƒ¨è‚Œè‚‰ã€‚é€šè¿‡éçº¿æ€§å˜å½¢å…¶å„è‡ªåŒºåŸŸå†…çš„é¢éƒ¨é¡¶ç‚¹ï¼ŒMetaHuman æ¨¡å‹åœ¨æ•æ‰å¤æ‚è¡¨æƒ…æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚å‰©ä¸‹çš„é—®é¢˜æ˜¯ä»è¯­éŸ³é¢„æµ‹é€‚å½“çš„æ§åˆ¶è£…å¤‡æ›²çº¿ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒåŒºåŸŸçš„é¢éƒ¨åŠ¨ä½œä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œè¿™æ—¢æºäºåè°ƒè‚Œè‚‰æ§åˆ¶çš„ç‰©ç†çº¦æŸï¼Œä¹Ÿæºäºä¹ æƒ¯æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å‹åŠ›å’Œè¯­éŸ³åœé¡¿æœŸé—´ï¼Œå˜´å·´ã€çœ‰æ¯›å’Œè„¸é¢Šç­‰åŒºåŸŸå¾€å¾€ä¼šåŒæ—¶è¿åŠ¨ä»¥ä¼ è¾¾è¡¨æƒ…æ„å›¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨åŸºäº Transformer çš„ç¼–ç å™¨å¯¹ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ã€‚ä½¿ç”¨è¯¥æ¨¡å‹ä½œä¸ºç›‘ç£ï¼Œè®­ç»ƒä¸€ä¸ª 3D é¢éƒ¨åŠ¨ç”»ç”Ÿæˆæ¨¡å‹ã€‚</p><p>(4):æœ¬æ–‡æå‡ºçš„ CSTalk æ–¹æ³•åœ¨ä¸åŒçš„æƒ…æ„ŸçŠ¶æ€ä¸‹èƒ½å¤Ÿç”Ÿæˆå¤æ‚çš„è¡¨æƒ…ã€‚åŸºäº Transformer ç¼–ç å™¨å¯¹ç‰¹å®šæƒ…æ„Ÿä¸­é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…æ›´ç¬¦åˆçœŸå®äººç±»è¯­éŸ³è¡¨æƒ…ã€‚</p><ol><li>Methods:</li></ol><p>(1):åŸºäº Epic æå‡ºçš„ MetaHuman è§’è‰²æ¨¡å‹ï¼Œåˆ©ç”¨ 185 ä¸ªæ§åˆ¶è£…å¤‡æ“çºµé¢éƒ¨åŠ¨ç”»ï¼Œæ¯ä¸ªè£…å¤‡å¯¹åº”ä¸€ç»„é¢éƒ¨è‚Œè‚‰ï¼›</p><p>(2):åˆ©ç”¨ Transformer ç¼–ç å™¨å¯¹ä¸åŒåŒºåŸŸçš„é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½œä¸ºç›‘ç£ï¼Œè®­ç»ƒä¸€ä¸ª 3D é¢éƒ¨åŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼›</p><p>(3):æå‡ºçš„ CSTalk æ–¹æ³•åœ¨ä¸åŒçš„æƒ…æ„ŸçŠ¶æ€ä¸‹èƒ½å¤Ÿç”Ÿæˆå¤æ‚çš„è¡¨æƒ…ï¼ŒåŸºäº Transformer ç¼–ç å™¨å¯¹ç‰¹å®šæƒ…æ„Ÿä¸­é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ï¼Œä½œä¸ºçº¦æŸæ¡ä»¶ï¼Œç”Ÿæˆçš„é¢éƒ¨è¡¨æƒ…æ›´ç¬¦åˆçœŸå®äººç±»è¯­éŸ³è¡¨æƒ…ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§è¯­éŸ³é©±åŠ¨çš„3Dæƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ç”Ÿæˆç½‘ç»œCSTalkï¼Œèƒ½å¤Ÿç”Ÿæˆå”‡éƒ¨åŠ¨ä½œå¯¹é½ä¸”è¡¨æƒ…é€¼çœŸçš„åŠ¨ç”»ã€‚å¹¶ä¸”æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº†åŸºäºMetaHumançš„é¢éƒ¨æ§åˆ¶è£…å¤‡æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä¸è‰ºæœ¯å®¶ç›´æ¥åä½œå¹¶åœ¨å·¥ä¸šç®¡é“ä¸­åº”ç”¨ã€‚ç”Ÿæˆçš„åŠ¨ç”»å‚æ•°ä¸èº«ä»½æ— å…³ï¼Œå¹¶ä¸”å¯ä»¥è¢«ä»»ä½•MetaHumanè™šæ‹Ÿå½¢è±¡å¤ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸åŒé¢éƒ¨åŠ¨ä½œåŒºåŸŸä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œå¹¶å¯¹ä¸åŒæƒ…æ„Ÿä¸‹çš„è¿™äº›ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥å¸®åŠ©è®­ç»ƒç½‘ç»œç”Ÿæˆæ›´ç¬¦åˆé¢éƒ¨åŠ¨ä½œæ¨¡å¼çš„è¡¨æƒ…ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»“æœæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºç›¸å…³æ€§çš„è¯­éŸ³é©±åŠ¨3Dæƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ç”Ÿæˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¡¨æƒ…ä¸°å¯Œä¸”ä¸è¯­éŸ³ä¸€è‡´çš„é¢éƒ¨åŠ¨ç”»ï¼›æ€§èƒ½ï¼šåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨æƒ…è‡ªç„¶åº¦ã€å”‡éƒ¨å¯¹é½å’Œæƒ…æ„Ÿè¡¨è¾¾æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦å¯¹Transformerç¼–ç å™¨å’Œé¢éƒ¨æ§åˆ¶è£…å¤‡æ¨¡å‹è¿›è¡Œæ·±å…¥ç†è§£ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-28ba327053e565fa0b60537d43960f32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-13b31067e590bbf83ad3f32bb9ed29f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-203dce5c70eae1db15da207b6436f6eb.jpg" align="middle"></details>## GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting**Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim**We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker's superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at https://github.com/KU-CVLAB/GaussianTalker/ . [PDF](http://arxiv.org/abs/2404.16012v2) Project Page: https://ku-cvlab.github.io/GaussianTalker**Summary**é«˜æ–¯è¯´è¯è€…ï¼šå®æ—¶ç”Ÿæˆå¯æ§å§¿åŠ¿è¯´è¯å¤´çš„åˆ›æ–°æ¡†æ¶ï¼Œèåˆäº† 3DGS çš„å¿«é€Ÿæ¸²æŸ“èƒ½åŠ›å’ŒéŸ³é¢‘ç‰¹å¾æ“ä½œ 3DGS çš„æŒ‘æˆ˜ã€‚**Key Takeaways**- æå‡ºé«˜æ–¯è¯´è¯è€…ï¼Œä¸€ç§åˆ›æ–°çš„å®æ—¶ç”Ÿæˆå§¿åŠ¿å¯æ§è¯´è¯å¤´çš„æ¡†æ¶ã€‚- æ„å»ºå¤´çš„è§„èŒƒ 3DGS è¡¨ç¤ºï¼Œå¹¶ä½¿å…¶ä¸éŸ³é¢‘åŒæ­¥å˜å½¢ã€‚- å°† 3D é«˜æ–¯å±æ€§ç¼–ç æˆå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ“çºµæ¯ä¸ªé«˜æ–¯å±æ€§ã€‚- è®¾è®¡åˆ©ç”¨ç©ºé—´æ„ŸçŸ¥ç‰¹å¾å¹¶åŠ å¼ºç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚- å°†ç‰¹å¾åµŒå…¥è¾“å…¥åˆ°ç©ºé—´-éŸ³é¢‘æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹æ¯ä¸ªé«˜æ–¯çš„å±æ€§çš„é€å¸§åç§»é‡ã€‚- å®éªŒç»“æœè¡¨æ˜ï¼Œé«˜æ–¯è¯´è¯è€…åœ¨é¢éƒ¨é€¼çœŸåº¦ã€å”‡å½¢åŒæ­¥ç²¾åº¦å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚- é«˜æ–¯è¯´è¯è€…å®ç°äº†é«˜è¾¾ 120 FPS çš„å“è¶Šæ¸²æŸ“é€Ÿåº¦ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title:é«˜æ–¯è¯´è¯è€…ï¼šå®æ—¶é«˜ä¿çœŸè¯´è¯å¤´åˆæˆï¼ˆé«˜æ–¯è¯´è¯è€…ï¼šå…·æœ‰éŸ³é¢‘é©±åŠ¨çš„ 3D é«˜æ–¯å–·å°„çš„å®æ—¶é«˜ä¿çœŸè¯´è¯å¤´åˆæˆï¼‰</p></li><li><p>Authors: Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</p></li><li><p>Affiliation: éŸ©å›½å¤§å­¦</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/</p></li><li><p>Summary:</p></li></ol><p>(1):è¯¥æ–‡ç« çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šç”Ÿæˆç”±ä»»æ„è¯­éŸ³éŸ³é¢‘é©±åŠ¨çš„è¯´è¯å¤´è§†é¢‘æ˜¯ä¸€é¡¹æµè¡Œçš„ä»»åŠ¡ï¼Œå…·æœ‰å„ç§ç”¨é€”ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ•°å­—äººã€è™šæ‹ŸåŒ–èº«ã€ç”µå½±åˆ¶ä½œå’Œç”µè¯ä¼šè®®ã€‚è™½ç„¶å„ç§å·¥ä½œ [6, 21, 33, 43] å·²æˆåŠŸå°è¯•ä½¿ç”¨ç”Ÿæˆæ¨¡å‹è§£å†³æ­¤ä»»åŠ¡ï¼Œä½†å®ƒä»¬ä¸ä¸“æ³¨äºæ§åˆ¶å¤´éƒ¨å§¿åŠ¿ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„çœŸå®æ€§å’Œé€‚ç”¨æ€§ã€‚æœ€è¿‘ï¼Œè®¸å¤šç ”ç©¶ [17, 24, 27, 39, 48, 49] å·²å°†ç¥ç»è¾å°„åœº (NeRF) [31] åº”ç”¨äºåˆ›å»ºå¯æ§å§¿æ€çš„è¯´è¯è‚–åƒã€‚é€šè¿‡ç›´æ¥è°ƒèŠ‚ NeRF å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) ä¸­çš„éŸ³é¢‘ç‰¹å¾ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆæˆä¸è¾“å…¥éŸ³é¢‘åŒæ­¥å”‡éƒ¨çš„è§†å›¾ä¸€è‡´çš„ 3D å¤´éƒ¨ç»“æ„ã€‚è™½ç„¶è¿™äº›åŸºäº NeRF çš„æŠ€æœ¯å®ç°äº†é«˜è´¨é‡å’Œä¸€è‡´çš„è§†è§‰è¾“å‡ºï¼Œä½†å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚å°½ç®¡æœ€è¿‘çš„è¿›æ­¥ [24, 39] ä»¥ 512 Ã— 512 åˆ†è¾¨ç‡å®ç°äº†é«˜è¾¾ 30 å¸§æ¯ç§’ (fps) çš„æ¸²æŸ“é€Ÿåº¦ï¼Œä½†å¿…é¡»å…‹æœè®¡ç®—ç“¶é¢ˆæ‰èƒ½åº”ç”¨äºå®é™…åœºæ™¯ã€‚</p><p>(2):è¿‡å»çš„æ–¹æ³•æœ‰ï¼šç¥ç»è¾å°„åœº (NeRF) [31] å·²è¢«åº”ç”¨äºåˆ›å»ºå¯æ§å§¿æ€çš„è¯´è¯è‚–åƒã€‚é€šè¿‡ç›´æ¥è°ƒèŠ‚ NeRF å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) ä¸­çš„éŸ³é¢‘ç‰¹å¾ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆæˆä¸è¾“å…¥éŸ³é¢‘åŒæ­¥å”‡éƒ¨çš„è§†å›¾ä¸€è‡´çš„ 3D å¤´éƒ¨ç»“æ„ã€‚åŸºäº NeRF çš„æŠ€æœ¯å®ç°äº†é«˜è´¨é‡å’Œä¸€è‡´çš„è§†è§‰è¾“å‡ºï¼Œä½†å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚å°½ç®¡æœ€è¿‘çš„è¿›æ­¥ [24, 39] ä»¥ 512 Ã— 512 åˆ†è¾¨ç‡å®ç°äº†é«˜è¾¾ 30 å¸§æ¯ç§’ (fps) çš„æ¸²æŸ“é€Ÿåº¦ï¼Œä½†å¿…é¡»å…‹æœè®¡ç®—ç“¶é¢ˆæ‰èƒ½åº”ç”¨äºå®é™…åœºæ™¯ã€‚é—®é¢˜æ˜¯ï¼šåŸºäº NeRF çš„æŠ€æœ¯æ¨ç†é€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚è¯¥æ–¹æ³•çš„åŠ¨æœºå¾ˆå¥½ï¼Œå› ä¸ºå®ƒè§£å†³äº†åŸºäº NeRF çš„æŠ€æœ¯æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</p><p>(3):æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•æ˜¯ï¼šGaussianTalker æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºå®æ—¶ç”Ÿæˆå¯æ§å§¿æ€çš„è¯´è¯å¤´ã€‚å®ƒåˆ©ç”¨äº† 3D é«˜æ–¯å–·å°„ (3DGS) çš„å¿«é€Ÿæ¸²æŸ“åŠŸèƒ½ï¼ŒåŒæ—¶è§£å†³äº†ç›´æ¥ç”¨è¯­éŸ³éŸ³é¢‘æ§åˆ¶ 3DGS çš„æŒ‘æˆ˜ã€‚GaussianTalker æ„å»ºäº†ä¸€ä¸ªå¤´éƒ¨è§„èŒƒçš„ 3DGS è¡¨ç¤ºï¼Œå¹¶ä½¿å…¶ä¸éŸ³é¢‘åŒæ­¥å˜å½¢ã€‚ä¸€ä¸ªå…³é”®çš„è§è§£æ˜¯å°† 3D é«˜æ–¯å±æ€§ç¼–ç æˆä¸€ä¸ªå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºï¼Œå…¶ä¸­å®ƒä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ“çºµæ¯ä¸ªé«˜æ–¯å±æ€§ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†ç©ºé—´æ„ŸçŸ¥ç‰¹å¾å¹¶å¼ºåˆ¶ç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚ç„¶åå°†ç‰¹å¾åµŒå…¥é¦ˆé€åˆ°ç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹æ¯ä¸ªé«˜æ–¯å±æ€§çš„é€å¸§åç§»ã€‚ä¸ç”¨äºæ“çºµå¤§é‡é«˜æ–¯åŠå…¶å¤æ‚å‚æ•°çš„å…ˆå‰çº§è”æˆ–ä¹˜æ³•æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ›´ç¨³å®šã€‚</p><p>(4):æœ¬æ–‡æ–¹æ³•åœ¨ä»¥ä¸‹ä»»åŠ¡å’Œæ€§èƒ½ä¸Šå–å¾—äº†æˆå°±ï¼šåœ¨é¢éƒ¨ä¿çœŸåº¦ã€å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢ï¼ŒGaussianTalker ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒGaussianTalker ä»¥é«˜è¾¾ 120 FPS çš„æ˜¾ç€æ¸²æŸ“é€Ÿåº¦ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†ã€‚æ€§èƒ½å¯ä»¥æ”¯æŒä»–ä»¬çš„ç›®æ ‡ï¼Œå› ä¸º GaussianTalker åœ¨é¢éƒ¨ä¿çœŸåº¦ã€å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1):GaussianTalker æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºå®æ—¶ç”Ÿæˆå¯æ§å§¿æ€çš„è¯´è¯å¤´ã€‚å®ƒåˆ©ç”¨äº† 3D é«˜æ–¯å–·å°„ (3DGS) çš„å¿«é€Ÿæ¸²æŸ“åŠŸèƒ½ï¼ŒåŒæ—¶è§£å†³äº†ç›´æ¥ç”¨è¯­éŸ³éŸ³é¢‘æ§åˆ¶ 3DGS çš„æŒ‘æˆ˜ã€‚GaussianTalker æ„å»ºäº†ä¸€ä¸ªå¤´éƒ¨è§„èŒƒçš„ 3DGS è¡¨ç¤ºï¼Œå¹¶ä½¿å…¶ä¸éŸ³é¢‘åŒæ­¥å˜å½¢ã€‚ä¸€ä¸ªå…³é”®çš„è§è§£æ˜¯å°† 3D é«˜æ–¯å±æ€§ç¼–ç æˆä¸€ä¸ªå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºï¼Œå…¶ä¸­å®ƒä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ“çºµæ¯ä¸ªé«˜æ–¯å±æ€§ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†ç©ºé—´æ„ŸçŸ¥ç‰¹å¾å¹¶å¼ºåˆ¶ç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚ç„¶åå°†ç‰¹å¾åµŒå…¥é¦ˆé€åˆ°ç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹æ¯ä¸ªé«˜æ–¯å±æ€§çš„é€å¸§åç§»ã€‚ä¸ç”¨äºæ“çºµå¤§é‡é«˜æ–¯åŠå…¶å¤æ‚å‚æ•°çš„å…ˆå‰çº§è”æˆ–ä¹˜æ³•æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæ›´ç¨³å®šã€‚           (2):GaussianTalker æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°† 3D é«˜æ–¯å±æ€§ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºä¸­ï¼Œè¯¥è¡¨ç¤ºä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ“çºµæ¯ä¸ªé«˜æ–¯å±æ€§ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†ç©ºé—´æ„ŸçŸ¥ç‰¹å¾å¹¶å¼ºåˆ¶ç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚ç„¶åå°†ç‰¹å¾åµŒå…¥é¦ˆé€åˆ°ç©ºé—´éŸ³é¢‘æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—é¢„æµ‹æ¯ä¸ªé«˜æ–¯å±æ€§çš„é€å¸§åç§»ã€‚           (3):GaussianTalker é‡‡ç”¨å¤šåˆ†è¾¨ç‡ä¸‰å¹³é¢è¡¨ç¤ºæ¥ç¼–ç  3D é«˜æ–¯ç‰¹å¾ï¼Œè¯¥è¡¨ç¤ºåˆ©ç”¨äº† 3D é«˜æ–¯éšå¼ç¥ç»è¾å°„åœºçš„ç©ºé—´ä¿¡æ¯ã€‚ç‰¹å¾åµŒå…¥ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»¥å‡†ç¡®å»ºæ¨¡ç”±è¾“å…¥éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¿åŠ¨ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>           (1):æœ¬å·¥ä½œæå‡ºäº† GaussianTalkerï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå®æ—¶å§¿æ€å¯æ§ 3D è¯´è¯å¤´åˆæˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨ 3D é«˜æ–¯è¡¨ç¤ºå¤´éƒ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è°ƒèŠ‚ 3D é«˜æ–¯åŸè¯­ï¼Œå®ç°äº†å¯¹ 3D é«˜æ–¯åŸè¯­çš„ç²¾ç¡®æ§åˆ¶ã€‚           (2):åˆ›æ–°ç‚¹ï¼šGaussianTalker æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°† 3D é«˜æ–¯å±æ€§ç¼–ç åˆ°ä¸€ä¸ªå…±äº«çš„éšå¼ç‰¹å¾è¡¨ç¤ºä¸­ï¼Œè¯¥è¡¨ç¤ºä¸éŸ³é¢‘ç‰¹å¾åˆå¹¶ä»¥æ“çºµæ¯ä¸ªé«˜æ–¯å±æ€§ã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†ç©ºé—´æ„ŸçŸ¥ç‰¹å¾å¹¶å¼ºåˆ¶ç›¸é‚»ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚æ€§èƒ½ï¼šGaussianTalker åœ¨é¢éƒ¨çœŸå®åº¦ã€å”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§å’Œæ¸²æŸ“é€Ÿåº¦æ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒGaussianTalker ä»¥é«˜è¾¾ 120 FPS çš„æ˜¾ç€æ¸²æŸ“é€Ÿåº¦ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„åŸºå‡†ã€‚å·¥ä½œé‡ï¼šGaussianTalker é‡‡ç”¨å¤šåˆ†è¾¨ç‡ä¸‰å¹³é¢è¡¨ç¤ºæ¥ç¼–ç  3D é«˜æ–¯ç‰¹å¾ï¼Œè¯¥è¡¨ç¤ºåˆ©ç”¨äº† 3D é«˜æ–¯éšå¼ç¥ç»è¾å°„åœºçš„ç©ºé—´ä¿¡æ¯ã€‚ç‰¹å¾åµŒå…¥ä¸éŸ³é¢‘ç‰¹å¾èåˆï¼Œä»¥å‡†ç¡®å»ºæ¨¡ç”±è¾“å…¥éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¿åŠ¨ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>é¢éƒ¨è¡¨æƒ…ç”Ÿæˆæ–¹æ³• GaussianTalker ä»¥æ˜¾å¼ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹ä¸ºåŸºç¡€ï¼Œé€šè¿‡ç»‘å®šé«˜æ–¯æ–‘ç‚¹åˆ° 3D é¢éƒ¨æ¨¡å‹ï¼Œå®ç°é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æ˜¾å¼è¡¨ç¤ºï¼Œå®ç°é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚</li><li>é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œã€‚</li><li>å¼•å…¥è¯´è¯äººç‰¹æœ‰æ··åˆå½¢çŠ¶ï¼Œé€šè¿‡æ½œåœ¨å§¿åŠ¿å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºï¼Œæä¾›ç¨³å®šä¸”é€¼çœŸçš„æ¸²æŸ“è§†é¢‘ã€‚</li><li>åœ¨é¢éƒ¨è¡¨æƒ…ç”Ÿæˆä¸­ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œæä¾›ç²¾ç¡®çš„å”‡éƒ¨åŒæ­¥å’Œå‡ºè‰²çš„è§†è§‰è´¨é‡ã€‚</li><li>æ¸²æŸ“é€Ÿåº¦è¾¾åˆ° 130 FPSï¼Œæ˜¾ç€é«˜äºå®æ—¶æ¸²æŸ“æ€§èƒ½é˜ˆå€¼ã€‚</li><li>å¯ä»¥éƒ¨ç½²åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šé«˜æ–¯è¯è€…ï¼šåŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„ç‰¹å®šè¯´è¯è€…ä¼šè¯´è¯çš„å¤´åˆæˆ</p></li><li><p>ä½œè€…ï¼šæ´ªäº‘ä½™ã€å±•æƒã€å¯èˆªä½™ã€å»ºå·é™ˆã€ä¸­åå§œã€å¿—æ–‡é™ˆã€èƒœé›¨å¼ ã€ Jimin Xuã€Fei Wuã€æˆé£å•ã€åˆšä½™</p></li><li><p>å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨ã€ä¼šè¯´è¯çš„å¤´åˆæˆã€ç¥ç»è¾å°„åœºã€é«˜æ–¯æ–‘ç‚¹</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.14037ï¼ŒGithub é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„éŸ³é¢‘é©±åŠ¨ä¼šè¯´è¯çš„å¤´åˆæˆæ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äº NeRF éšå¼è¡¨ç¤ºå¼•èµ·çš„å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚å˜´å”‡åŠ¨ä½œä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ï¼Œä»¥åŠè§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•åŠé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šå§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œå¯¼è‡´å˜´å”‡åŠ¨ä½œä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨ä¼šè¯´è¯çš„å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerã€‚GaussianTalker ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šç‰¹å®šè¯´è¯è€…åŠ¨ä½œè½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ã€‚ç‰¹å®šè¯´è¯è€…åŠ¨ä½œè½¬æ¢å™¨é€šè¿‡é€šç”¨çš„éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶çš„å˜´å”‡åŠ¨ä½œç”Ÿæˆæ¥å®ç°ç‰¹å®šäºç›®æ ‡è¯´è¯è€…çš„å‡†ç¡®å˜´å”‡åŠ¨ä½œã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†ç‰¹å®šäºè¯´è¯è€…çš„æ··åˆå½¢çŠ¶ï¼Œä»¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šGaussianTalker åœ¨å”‡å½¢åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥ç”Ÿæˆå‡†ç¡®ä¸”è‡ªç„¶çš„å˜´å”‡åŠ¨ä½œã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šSpeaker-Specific Motion Translatorï¼šè¯¥æ¨¡å—è´Ÿè´£å°†éŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºç‰¹å®šäºç›®æ ‡è¯´è¯è€…çš„ FLAME å‚æ•°åºåˆ—ï¼Œç”¨äºé¢éƒ¨åŠ¨ç”»æ§åˆ¶ã€‚å®ƒç”± Universal Audio Encoder å’Œ Customized Motion Decoder ç»„æˆã€‚</p><p>ï¼ˆ2ï¼‰ï¼šDynamic Gaussian Rendererï¼šè¯¥æ¨¡å—åˆ©ç”¨ FLAME é©±åŠ¨ 3D é«˜æ–¯æ–‘ç‚¹ï¼Œå¹¶å®æ—¶æ¸²æŸ“åŠ¨æ€è¯´è¯å¤´éƒ¨ã€‚å®ƒå¼•å…¥ç‰¹å®šäºè¯´è¯è€…çš„æ··åˆå½¢çŠ¶ï¼Œä»¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨ä¼šè¯´è¯çš„å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯è€…å…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3D ç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚ç‰¹å®šè¯´è¯è€…çš„ FLAME è½¬æ¢å™¨é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥æ¥å®ç°åŒæ­¥ä¸”è‡ªç„¶çš„å˜´å”‡è¿åŠ¨ï¼Œè€ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿ä¼˜åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šä¸”é€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalker åœ¨è¯´è¯å¤´åˆæˆä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜æ˜¾è¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²çš„åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚            (2):åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨ä¼šè¯´è¯çš„å¤´åˆæˆæ–°æ–¹æ³• GaussianTalkerï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯è€…å…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3D ç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ï¼›             æ€§èƒ½ï¼šåœ¨å”‡å½¢åˆæˆä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥ç”Ÿæˆå‡†ç¡®ä¸”è‡ªç„¶çš„å˜´å”‡åŠ¨ä½œï¼›             Workloadï¼šæœªæåŠã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/</id>
    <published>2024-05-02T02:21:37.000Z</published>
    <updated>2024-05-02T02:21:37.091Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective"><a href="#Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective" class="headerlink" title="Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective"></a>Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective</h2><p><strong>Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong</strong></p><p>Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts. </p><p><a href="http://arxiv.org/abs/2404.19382v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹çš„åå­¦ä¹ ç ”ç©¶å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¿ç§»æ€§çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•æ¥æ¢æµ‹åå­¦ä¹ é²æ£’æ€§ï¼Œåœ¨é»‘ç›’è®¾ç½®ä¸‹æœ‰æ•ˆæ¢å¤è¢«æ“¦é™¤çš„æ¦‚å¿µã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åå­¦ä¹ æ–¹æ³•ä¼šå°†æ–‡æœ¬åˆ°å›¾åƒçš„æ˜ å°„å…³ç³»è¿›è¡Œé‡æ–°åˆ†å¸ƒï¼Œä½†ä¿ç•™äº†æ‰©æ•£æ¨¡å‹ç”Ÿæˆç©ºé—´ä¸­çš„è§†è§‰å†…å®¹ã€‚</li><li>ç›®å‰æ¢æµ‹åå­¦ä¹ é²æ£’æ€§çš„æ–¹æ³•ç¼ºä¹å¯è¿ç§»æ€§å’Œæ”»å‡»åŠ›ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯¹æŠ—æ€§æœç´¢ç­–ç•¥ï¼Œå¯ä»¥åœ¨é»‘ç›’è®¾ç½®ä¸‹è·¨ä¸åŒçš„åå­¦ä¹ æ¨¡å‹è¿ç§»å¯¹æŠ—åµŒå…¥ã€‚</li><li>é‡‡ç”¨åŸå§‹ Stable Diffusion æ¨¡å‹ä½œä¸ºä»£ç†æ¨¡å‹æ¥è¿­ä»£æ“¦é™¤å’Œæœç´¢åµŒå…¥ã€‚</li><li>å®éªŒè¡¨æ˜ï¼Œæœç´¢çš„å¯¹æŠ—åµŒå…¥å¯ä»¥è·¨å¤šä¸ªæœ€å…ˆè¿›çš„åå­¦ä¹ æ–¹æ³•è¿ç§»ï¼Œå¹¶ä¸”å¯¹ä¸åŒå±‚æ¬¡çš„æ¦‚å¿µéƒ½èƒ½æœ‰æ•ˆæ¢å¤ã€‚</li><li>æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¼¥è¡¥äº†å½“å‰æ¢æµ‹åå­¦ä¹ é²æ£’æ€§æ–¹æ³•çš„ä¸è¶³ï¼Œä¸ºè¯„ä¼°å’Œæé«˜åå­¦ä¹ çš„æœ‰æ•ˆæ€§æä¾›äº†æ–°çš„æ€è·¯ã€‚</li><li>è¯¥æ–¹æ³•å¯ä»¥ç”¨äºè¯„ä¼°åå­¦ä¹ çš„é²æ£’æ€§ï¼Œå¹¶ä¸ºæé«˜åå­¦ä¹ çš„æœ‰æ•ˆæ€§æä¾›æŒ‡å¯¼ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>è®ºæ–‡æ ‡é¢˜ï¼šæ¢æµ‹æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹ï¼šå¯è½¬ç§»å¯¹æŠ—æ”»å‡»è§†è§’</p></li><li><p>ä½œè€…ï¼š</p><ul><li>Xiaoxuan Han</li><li>Songlin Yang</li><li>Wei Wang</li><li>Yang Li</li><li>Jing Dong</li></ul></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸­å›½ç§‘å­¦é™¢å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š</p><ul><li>Diffusion Model</li><li>Machine Unlearning</li><li>Adversarial Attack</li></ul></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.19382    Github é“¾æ¥ï¼šhttps://github.com/hxxdtd/PUND</p></li><li><p>æ‘˜è¦ï¼š</p><p>(1) ç ”ç©¶èƒŒæ™¯ï¼š    æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†èº«ä»½éšç§ã€ç‰ˆæƒå’Œä¸å®‰å…¨å†…å®¹ç­‰å®‰å…¨é—®é¢˜ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†æ¦‚å¿µæ“¦é™¤æ–¹æ³•æ¥æ¶ˆé™¤æ¶‰åŠçš„æ¦‚å¿µã€‚</p><p>(2) è¿‡å¾€æ–¹æ³•ä¸é—®é¢˜ï¼š    ç°æœ‰çš„æ¦‚å¿µæ“¦é™¤æ–¹æ³•é€šè¿‡æ”¹å˜æ–‡æœ¬åˆ°å›¾åƒçš„æ˜ å°„æ¥å®ç°â€œæ“¦é™¤â€ä»»åŠ¡ï¼Œä½†æœªèƒ½æ“¦é™¤æ‰©æ•£æ¨¡å‹ç”Ÿæˆç©ºé—´å†…çš„è§†è§‰å†…å®¹ï¼Œä¸ºæ¢å¤è¿™äº›æ“¦é™¤çš„æ¦‚å¿µç•™ä¸‹äº†è‡´å‘½ç¼ºé™·ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ç¼ºä¹å¯è½¬ç§»æ€§ï¼Œå¹¶ä¸”åœ¨æ¢å¤ç‹­çª„æ¦‚å¿µï¼ˆä¾‹å¦‚åäººèº«ä»½ï¼‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p><p>(3) æœ¬æ–‡æ–¹æ³•ï¼š    æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è½¬ç§»çš„å¯¹æŠ—æ”»å‡»æ¥æ¢æµ‹æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥æ”»å‡»é‡‡ç”¨å¯¹æŠ—æœç´¢ç­–ç•¥ï¼Œåœ¨åŸå§‹ Stable Diffusion æ¨¡å‹ä¸Šè¿­ä»£æ“¦é™¤å’Œæœç´¢åµŒå…¥ï¼Œä»¥æ‰¾åˆ°å¯ä»¥åœ¨ä¸åŒçš„æœªå­¦ä¹ æ¨¡å‹ä¹‹é—´è½¬ç§»çš„å¯¹æŠ—åµŒå…¥ã€‚</p><p>(4) å®éªŒç»“æœï¼š    å®éªŒè¡¨æ˜ï¼Œæ‰€æœç´¢çš„å¯¹æŠ—åµŒå…¥åœ¨å„ç§æœ€å…ˆè¿›çš„æœªå­¦ä¹ æ–¹æ³•ä¸­å…·æœ‰å¯è½¬ç§»æ€§ï¼Œå¹¶ä¸”åœ¨ä»å®½æ³›åˆ°ç‹­çª„çš„ä¸åŒæ¦‚å¿µçº§åˆ«ä¸Šéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚è¿™äº›ç»“æœæ”¯æŒäº†æœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæ¢æµ‹æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶ä¸ºæ¢å¤æ“¦é™¤çš„æ¦‚å¿µæä¾›äº†æ–°çš„æ€è·¯ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ç§å¯è½¬ç§»çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œæ¢æµ‹æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹çš„é²æ£’æ€§ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨å¯¹æŠ—æœç´¢ç­–ç•¥ï¼Œåœ¨åŸå§‹ Stable Diffusion æ¨¡å‹ä¸Šè¿­ä»£æ“¦é™¤å’Œæœç´¢åµŒå…¥ï¼Œæ‰¾åˆ°å¯ä»¥åœ¨ä¸åŒæœªå­¦ä¹ æ¨¡å‹ä¹‹é—´è½¬ç§»çš„å¯¹æŠ—åµŒå…¥ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨å¯¹æŠ—åµŒå…¥ä½œä¸ºæ”»å‡»æºï¼Œåœ¨ä¸åŒçš„æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹ä¸Šè¿›è¡Œæ”»å‡»ï¼Œè¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šé€šè¿‡å®éªŒéªŒè¯æ‰€æœç´¢çš„å¯¹æŠ—åµŒå…¥åœ¨å„ç§æœ€å…ˆè¿›çš„æœªå­¦ä¹ æ–¹æ³•ä¸­å…·æœ‰å¯è½¬ç§»æ€§ï¼Œå¹¶ä¸”åœ¨ä»å®½æ³›åˆ°ç‹­çª„çš„ä¸åŒæ¦‚å¿µçº§åˆ«ä¸Šéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</p></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯è½¬ç§»çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œæ¢æµ‹æœªå­¦ä¹ æ‰©æ•£æ¨¡å‹çš„é²æ£’æ€§ï¼Œä¸ºæ¢å¤æ“¦é™¤çš„æ¦‚å¿µæä¾›äº†æ–°çš„æ€è·¯ã€‚            (2):Innovation point: æå‡ºäº†ä¸€ç§å¯è½¬ç§»çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒæœªå­¦ä¹ æ¨¡å‹ä¹‹é—´å…·æœ‰å¯è½¬ç§»æ€§ï¼Œå¹¶ä¸”åœ¨ä»å®½æ³›åˆ°ç‹­çª„çš„ä¸åŒæ¦‚å¿µçº§åˆ«ä¸Šéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚Performance: å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æœç´¢çš„å¯¹æŠ—åµŒå…¥åœ¨å„ç§æœ€å…ˆè¿›çš„æœªå­¦ä¹ æ–¹æ³•ä¸­å…·æœ‰å¯è½¬ç§»æ€§ï¼Œå¹¶ä¸”åœ¨ä»å®½æ³›åˆ°ç‹­çª„çš„ä¸åŒæ¦‚å¿µçº§åˆ«ä¸Šéƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚Workload: è¯¥æ–¹æ³•éœ€è¦ä½¿ç”¨å¯¹æŠ—æœç´¢ç­–ç•¥ï¼Œåœ¨åŸå§‹ Stable Diffusion æ¨¡å‹ä¸Šè¿­ä»£æ“¦é™¤å’Œæœç´¢åµŒå…¥ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-aeb2a7c17e04ea32837496f134911073.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cfab2dba37aac49c7649b71ac867d79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0aa15fec53d79c3279c72f74772273b9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8177a4229cfdb42440835f0ee9e56c19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-67758faf074d7114c762f4a57a5d1403.jpg" align="middle"><img src="https://picx.zhimg.com/v2-471f68a1caec8d9bae0fe6402d798203.jpg" align="middle"></details><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p><p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a â€œScreenwriterâ€, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the â€œRehearsalâ€. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the â€œFinal Performanceâ€. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity. </p><p><a href="http://arxiv.org/abs/2404.18919v1">PDF</a> </p><p><strong>Summary</strong><br>å¯¹è¯ç”Ÿæˆæ¨¡å‹ TheaterGen æ— éœ€é¢å¤–è®­ç»ƒï¼Œå³å¯å®ç°æ–‡æœ¬åˆ°å›¾åƒçš„å¤šè½®ç”Ÿæˆï¼Œæå‡å›¾åƒè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>TheaterGen åˆ›æ–°æ€§åœ°å°†å¤§è¯­è¨€æ¨¡å‹èå…¥æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå®ç°å¤šè½®å›¾åƒç”Ÿæˆã€‚</li><li>å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºâ€œç¼–å‰§â€ï¼Œç”Ÿæˆæ ‡å‡†åŒ–æç¤ºæ‰‹å†Œï¼Œç®¡ç†è§’è‰²æç¤ºå’Œè®¾è®¡ã€‚</li><li>TheaterGen åŸºäºæ‰‹å†Œç”Ÿæˆè§’è‰²å›¾åƒï¼Œæå–æŒ‡å¯¼ä¿¡æ¯ã€‚</li><li>åå‘å»å™ªè¿‡ç¨‹å°†æ‰‹å†Œå’ŒæŒ‡å¯¼ä¿¡æ¯èå…¥æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆå›¾åƒã€‚</li><li>CMIGBench æ˜¯é¦–ä¸ªä¸é¢„å…ˆå®šä¹‰è§’è‰²çš„å¤šè½®å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ã€‚</li><li>TheaterGen æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨ Mini DALLE 3 æ¨¡å‹ä¸Šæå‡å¹³å‡è§’è‰²ç›¸ä¼¼åº¦ 21%ï¼Œå¹³å‡æ–‡æœ¬å›¾åƒç›¸ä¼¼åº¦ 19%ã€‚</li><li>TheaterGen å¯ç”¨äºæ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡ã€‚</li><li>TheaterGen ä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå¸¦æ¥äº†çªç ´ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TheaterGenï¼šåŸºäº LLM çš„å¤šè½®å›¾åƒç”Ÿæˆä¸­çš„è§’è‰²ç®¡ç†</p></li><li><p>Authors: Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, YuxinHe, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, XiaodanLiang</p></li><li><p>Affiliation: ä¸­å±±å¤§å­¦æ·±åœ³æ ¡åŒº</p></li><li><p>Keywords: Diffusion models Â· Consistency Â· Multi-turn image generation</p></li><li><p>Urls: https://howe140.github.io/theatergen.io/ , Github: https://github.com/donahowe/Theatergen</p></li><li><p>Summary:</p><pre><code>           (1): å½“å‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åœºæ™¯ä¸­éœ€æ±‚è¾ƒé«˜çš„å¤šè½®å›¾åƒç”Ÿæˆä»ç„¶é¢ä¸´ç€å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ä»¥åŠåŒä¸€ä¸»é¢˜åœ¨å¤šä¸ªäº¤äº’è½®æ¬¡ä¸­çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ã€‚           (2): ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æç¤ºçš„æ”¹è¿›å’Œæ‰©æ•£æ¨¡å‹çš„è®­ç»ƒä¸Šï¼Œä½†å¯¹äºå¤šè½®å›¾åƒç”Ÿæˆä¸­çš„è§’è‰²ç®¡ç†å’Œä¸€è‡´æ€§é—®é¢˜å…³æ³¨è¾ƒå°‘ã€‚           (3): æœ¬æ–‡æå‡º TheaterGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œæ–‡æœ¬åˆ°å›¾åƒ (T2I) æ¨¡å‹ï¼Œä»¥æä¾›å¤šè½®å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼ŒLLM å……å½“â€œç¼–å‰§â€ï¼Œå‚ä¸å¤šè½®äº¤äº’ï¼Œç”Ÿæˆå’Œç®¡ç†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æç¤ºæ‰‹å†Œï¼Œå…¶ä¸­åŒ…å«ç›®æ ‡å›¾åƒä¸­æ¯ä¸ªè§’è‰²çš„æç¤ºå’Œå¸ƒå±€è®¾è®¡ã€‚åŸºäºè§’è‰²æç¤ºå’Œå¸ƒå±€ï¼Œç”Ÿæˆè§’è‰²å›¾åƒåˆ—è¡¨å¹¶ä»ä¸­æå–æŒ‡å¯¼ä¿¡æ¯ï¼Œç±»ä¼¼äºâ€œæ’ç»ƒâ€ã€‚éšåï¼Œé€šè¿‡å°†æç¤ºæ‰‹å†Œå’ŒæŒ‡å¯¼ä¿¡æ¯èå…¥ T2I æ‰©æ•£æ¨¡å‹çš„åå‘å»å™ªè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆæœ€ç»ˆå›¾åƒï¼Œå³è¿›è¡Œâ€œæœ€ç»ˆè¡¨æ¼”â€ã€‚é€šè¿‡æœ‰æ•ˆç®¡ç†æç¤ºæ‰‹å†Œå’Œè§’è‰²å›¾åƒï¼ŒTheaterGen æ˜¾ç€æé«˜äº†åˆæˆå›¾åƒä¸­çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å« 8000 ä¸ªå¤šè½®æŒ‡ä»¤çš„ä¸“é—¨åŸºå‡† CMIGBenchï¼ˆä¸€è‡´çš„å¤šè½®å›¾åƒç”ŸæˆåŸºå‡†ï¼‰ã€‚ä¸ä»¥å‰çš„å¤šè½®åŸºå‡†ä¸åŒï¼ŒCMIGBench ä¸é¢„å…ˆå®šä¹‰è§’è‰²ï¼Œå› æ­¤å…·æœ‰å¾ˆå¤§çš„å¤šæ ·æ€§ã€‚CMIGBench ä¸ŠåŒ…å«æ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡ï¼Œä»¥è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚           (4): å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTheaterGen æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå®ƒå°†å°–ç«¯çš„ Mini DALLÂ·E 3 æ¨¡å‹åœ¨å¹³å‡å­—ç¬¦-å­—ç¬¦ç›¸ä¼¼åº¦æ–¹é¢çš„æ€§èƒ½æé«˜äº† 21%ï¼Œåœ¨å¹³å‡æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦æ–¹é¢çš„æ€§èƒ½æé«˜äº† 19%ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§’è‰²è®¾è®¡å¸ˆï¼Œç”ŸæˆåŒ…å«è§’è‰²æç¤ºå’Œå¸ƒå±€çš„ç»“æ„åŒ–æç¤ºæ‰‹å†Œï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè®¾è®¡è§’è‰²å›¾åƒç®¡ç†å™¨ï¼Œç”Ÿæˆèˆå°è§’è‰²å›¾åƒåŠå…¶å¯¹åº”çš„çº¿æ¡å¼•å¯¼å’Œæ½œåœ¨å¼•å¯¼ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå°†æç¤ºæ‰‹å†Œå’Œä¸¤ç§å¼•å¯¼è¾“å…¥åŸºäºè§’è‰²çš„ç”Ÿæˆå™¨ä¸­ï¼Œåˆæˆæœ€ç»ˆå›¾åƒã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† TheaterGenï¼Œä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œæ–‡æœ¬åˆ°å›¾åƒ (T2I) æ¨¡å‹ï¼Œä»¥æä¾›å¤šè½®å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚é€šè¿‡æœ‰æ•ˆç®¡ç†æç¤ºæ‰‹å†Œå’Œè§’è‰²å›¾åƒï¼ŒTheaterGen æ˜¾ç€æé«˜äº†åˆæˆå›¾åƒä¸­çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å« 8000 ä¸ªå¤šè½®æŒ‡ä»¤çš„ä¸“é—¨åŸºå‡† CMIGBenchï¼ˆä¸€è‡´çš„å¤šè½®å›¾åƒç”ŸæˆåŸºå‡†ï¼‰ã€‚ä¸ä»¥å‰çš„å¤šè½®åŸºå‡†ä¸åŒï¼ŒCMIGBench ä¸é¢„å…ˆå®šä¹‰è§’è‰²ï¼Œå› æ­¤å…·æœ‰å¾ˆå¤§çš„å¤šæ ·æ€§ã€‚CMIGBench ä¸ŠåŒ…å«æ•…äº‹ç”Ÿæˆå’Œå¤šè½®ç¼–è¾‘ä»»åŠ¡ï¼Œä»¥è¿›è¡Œç»¼åˆè¯„ä¼°ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§’è‰²è®¾è®¡å¸ˆï¼Œç”ŸæˆåŒ…å«è§’è‰²æç¤ºå’Œå¸ƒå±€çš„ç»“æ„åŒ–æç¤ºæ‰‹å†Œï¼›è®¾è®¡è§’è‰²å›¾åƒç®¡ç†å™¨ï¼Œç”Ÿæˆèˆå°è§’è‰²å›¾åƒåŠå…¶å¯¹åº”çš„çº¿æ¡å¼•å¯¼å’Œæ½œåœ¨å¼•å¯¼ï¼›å°†æç¤ºæ‰‹å†Œå’Œä¸¤ç§å¼•å¯¼è¾“å…¥åŸºäºè§’è‰²çš„ç”Ÿæˆå™¨ä¸­ï¼Œåˆæˆæœ€ç»ˆå›¾åƒã€‚æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTheaterGen æ˜æ˜¾ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå®ƒå°†å°–ç«¯çš„ Mini DALLÂ·E 3 æ¨¡å‹åœ¨å¹³å‡å­—ç¬¦-å­—ç¬¦ç›¸ä¼¼åº¦æ–¹é¢çš„æ€§èƒ½æé«˜äº† 21%ï¼Œåœ¨å¹³å‡æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦æ–¹é¢çš„æ€§èƒ½æé«˜äº† 19%ã€‚å·¥ä½œé‡ï¼šTheaterGen æ˜¯ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-9ba365cfc612e009b79d484c29a30149.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fce5b92ae3c1c7350697723f803ec2cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle"></details>## FlexiFilm: Long Video Generation with Flexible Conditions**Authors:Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, Bo zhao**Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/ [PDF](http://arxiv.org/abs/2404.18620v1) 9 pages, 9 figures**Summary**é’ˆå¯¹é•¿è§†é¢‘ç”Ÿæˆè¿™ä¸€é‡å¤§æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡º FlexiFilmï¼Œä¸€ç§ä¸“ä¸ºé•¿è§†é¢‘ç”Ÿæˆè€Œè®¾è®¡çš„æ‰©æ•£æ¨¡å‹ã€‚**Key Takeaways**- ç°æœ‰åŸºäºæ‰©æ•£çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶æ€§èƒ½ä¸‹é™ã€‚- FlexiFilm å¼•å…¥äº†æ—¶é—´æ¡ä»¶å™¨ï¼Œä»¥å»ºç«‹ç”Ÿæˆä¸å¤šæ¨¡æ€æ¡ä»¶ä¹‹é—´æ›´ä¸€è‡´çš„å…³ç³»ã€‚- FlexiFilm ä½¿ç”¨å†é‡‡æ ·ç­–ç•¥æ¥è§£å†³è¿‡åº¦æ›å…‰é—®é¢˜ã€‚- FlexiFilm ç”Ÿæˆçš„é•¿è§†é¢‘è¶…è¿‡ 30 ç§’ï¼Œæ—¶é—´ä¸€è‡´æ€§å¥½ã€‚- FlexiFilm åœ¨å®šæ€§å’Œå®šé‡åˆ†æä¸­å‡ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚- FlexiFilm èƒ½ç”Ÿæˆå†…å®¹ä¸°å¯Œã€å…·æœ‰æ—¶é—´è¿è´¯æ€§çš„é•¿è§†é¢‘ã€‚- FlexiFilm ä¸ºé•¿è§†é¢‘ç”Ÿæˆæä¾›äº†æ–°çš„æ€è·¯ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: FlexiFilm: é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ (FlexiFilm: Temporal Coherence in Long Video Generation)</p></li><li><p>Authors: Yichen Ouyang, Jianhao Yuan, Hao Zhao, Tiejun Huang, Gaoang Wang, Bo Zhao</p></li><li><p>Affiliation: å—äº¬å¤§å­¦ (Nanjing University)</p></li><li><p>Keywords: Long video generation, Diffusion models, Temporal conditioner, Resampling strategy</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.09413, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): é•¿è§†é¢‘ç”Ÿæˆé¢ä¸´æ—¶ç©ºä¸€è‡´æ€§æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºæ‰©æ•£çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­è¡¨ç°ä¸ä½³ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•é‡‡ç”¨ç®€å•çš„æ¡ä»¶æœºåˆ¶å’Œé‡‡æ ·ç­–ç•¥ï¼Œå¯¼è‡´æ—¶ç©ºä¸ä¸€è‡´å’Œè¿‡æ›é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡º FlexiFilmï¼Œä¸€ç§é’ˆå¯¹é•¿è§†é¢‘ç”Ÿæˆé‡èº«å®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ã€‚FlexiFilm å¼•å…¥æ—¶é—´æ¡ä»¶å™¨ï¼Œå»ºç«‹ç”Ÿæˆå’Œå¤šæ¨¡æ€æ¡ä»¶ä¹‹é—´æ›´ä¸€è‡´çš„å…³ç³»ï¼Œå¹¶é‡‡ç”¨é‡é‡‡æ ·ç­–ç•¥è§£å†³è¿‡æ›é—®é¢˜ã€‚</p><p>(4): åœ¨é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒFlexiFilm ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œç”Ÿæˆé•¿åº¦è¶…è¿‡ 30 ç§’çš„é•¿ä¸”ä¸€è‡´çš„è§†é¢‘ã€‚å®šé‡å’Œå®šæ€§åˆ†æéƒ½æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º FlexiFilmï¼Œä¸€ç§é’ˆå¯¹é•¿è§†é¢‘ç”Ÿæˆé‡èº«å®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ï¼Œå¼•å…¥æ—¶é—´æ¡ä»¶å™¨ï¼Œå»ºç«‹ç”Ÿæˆå’Œå¤šæ¨¡æ€æ¡ä»¶ä¹‹é—´æ›´ä¸€è‡´çš„å…³ç³»ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨é‡é‡‡æ ·ç­–ç•¥è§£å†³è¿‡æ›é—®é¢˜ï¼Œåœ¨é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œç”Ÿæˆé•¿åº¦è¶…è¿‡ 30 ç§’çš„é•¿ä¸”ä¸€è‡´çš„è§†é¢‘ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):æœ¬å·¥ä½œé’ˆå¯¹é•¿è§†é¢‘ç”Ÿæˆä¸­æ—¶ç©ºä¸€è‡´æ€§é—®é¢˜ï¼Œæå‡º FlexiFilm æ¨¡å‹ï¼Œæœ‰æ•ˆæå‡äº†é•¿è§†é¢‘ç”Ÿæˆè´¨é‡ï¼›            (2):åˆ›æ–°ç‚¹ï¼šå¼•å…¥æ—¶é—´æ¡ä»¶å™¨å’Œé‡é‡‡æ ·ç­–ç•¥ï¼Œå¢å¼ºæ—¶ç©ºä¸€è‡´æ€§ï¼›æ€§èƒ½ï¼šåœ¨é•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œå¯ç”Ÿæˆé•¿åº¦è¶…è¿‡ 30 ç§’çš„é•¿ä¸”ä¸€è‡´çš„è§†é¢‘ï¼›å·¥ä½œé‡ï¼šæ¨¡å‹è®¾è®¡å’Œè®­ç»ƒå¤æ‚åº¦è¾ƒé«˜ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-89f7187f1074067e636b6cefcd03214c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fc7b975f21081a9007db0c1ec2d26248.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcfbf96f6700552f8cbb6108717b928b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-26f03063378af5e36436e73a3bc39c46.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3bf4fac4e6634e90aecfc106469774e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-639e75c0d686596adb3d0c89cc48bb9c.jpg" align="middle"></details><h2 id="Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting"><a href="#Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting" class="headerlink" title="Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting"></a>Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting</h2><p><strong>Authors:Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</strong></p><p>Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as â€œover-imaginationâ€, inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating â€œover-imaginationâ€, resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results. </p><p><a href="http://arxiv.org/abs/2404.18598v1">PDF</a> 16 pages, 9 figures, project page:   <a href="https://anywheremultiagent.github.io">https://anywheremultiagent.github.io</a></p><p><strong>Summary</strong><br>é€šè¿‡ Anywhere å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œåˆ©ç”¨ VLMã€LLM å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡è¯­ä¹‰åˆ†æã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆå’Œç»“æœåˆ†æå™¨ï¼Œå®ç°å‰æ™¯è°ƒæ§å›¾åƒä¿®å¤ï¼Œè§£å†³è¿‡åº¦æƒ³è±¡ã€å‰æ™¯èƒŒæ™¯ä¸ä¸€è‡´å’Œå¤šæ ·æ€§å·®çš„é—®é¢˜ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥ Anywhereï¼Œä¸€ç§ç”¨äºå‰æ™¯è°ƒæ§å›¾åƒä¿®å¤çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚</li><li>åˆ©ç”¨ VLM å’Œ LLM è¿›è¡Œè¯­ä¹‰åˆ†æï¼Œç”Ÿæˆæœ€ä½³è¯­è¨€æç¤ºã€‚</li><li>ä½¿ç”¨æ–‡æœ¬å¼•å¯¼çš„ Canny-to-Image ç”Ÿæˆæ¨¡å‹åˆ›å»ºæ¨¡æ¿å›¾åƒã€‚</li><li>ä½¿ç”¨å›¾åƒç²¾ç‚¼å™¨èåˆè¾“å…¥å‰æ™¯å’Œæ¨¡æ¿å›¾åƒä»¥ç”Ÿæˆè¾“å‡ºã€‚</li><li>ä½¿ç”¨ VLM è¿›è¡Œç»“æœåˆ†æï¼Œè¯„ä¼°å›¾åƒå†…å®¹åˆç†æ€§ã€ç¾å­¦åˆ†æ•°å’Œå‰æ™¯èƒŒæ™¯ç›¸å…³æ€§ã€‚</li><li>è§¦å‘æç¤ºå’Œå›¾åƒå†ç”Ÿï¼Œä»¥è§£å†³è¿‡åº¦æƒ³è±¡ã€å‰æ™¯èƒŒæ™¯å·®å¼‚å’Œå¤šæ ·æ€§å·®çš„é—®é¢˜ã€‚</li><li>Anywhere æ¡†æ¶åœ¨å‰æ™¯è°ƒæ§å›¾åƒä¿®å¤ä¸­è¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆæ›´å¯é ã€æ›´å¤šæ ·åŒ–çš„ç»“æœã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ANYWHEREï¼šåŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶</p></li><li><p>Authors: Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</p></li><li><p>Affiliation: å—äº¬å¤§å­¦æ–°è½¯ä»¶æŠ€æœ¯å›½å®¶é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: Image inpainting, Multi-agent framework, Foreground-conditioned, Diversity, Reliability</p></li><li><p>Urls: https://arxiv.org/abs/2404.18598v1, https://github.com/anywheremultiagent/anywhere</p></li><li><p>Summary: </p><pre><code>            (1):å›¾åƒä¿®å¤åœ¨å›¾åƒå¤„ç†ä¸­æœ‰ç€é‡è¦çš„åº”ç”¨ï¼Œè¿‘å¹´æ¥åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒä¿®å¤æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºåŸºäºå‰æ™¯å¯¹è±¡å®Œæˆå›¾åƒçš„ä»»åŠ¡æ—¶ï¼Œç°æœ‰çš„ç«¯åˆ°ç«¯å›¾åƒä¿®å¤æ–¹æ³•é¢ä¸´ç€â€œè¿‡åº¦æƒ³è±¡â€ã€â€œå‰æ™¯ä¸èƒŒæ™¯ä¸ä¸€è‡´â€ä»¥åŠå¤šæ ·æ€§å—é™ç­‰æŒ‘æˆ˜ã€‚            (2):é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼€åˆ›æ€§çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶â€”â€”Anywhereã€‚Anywhereé‡‡ç”¨äº†ä¸€ä¸ªå¤æ‚çš„ç®¡é“æ¡†æ¶ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ç­‰å¤šç§æ™ºèƒ½ä½“ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼šæç¤ºç”Ÿæˆæ¨¡å—ã€å›¾åƒç”Ÿæˆæ¨¡å—å’Œç»“æœåˆ†æå™¨ã€‚æç¤ºç”Ÿæˆæ¨¡å—å¯¹è¾“å…¥çš„å‰æ™¯å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†æï¼Œåˆ©ç”¨VLMé¢„æµ‹ç›¸å…³çš„è¯­è¨€æè¿°ï¼Œå¹¶åˆ©ç”¨LLMæ¨èæœ€ä¼˜çš„è¯­è¨€æç¤ºã€‚åœ¨å›¾åƒç”Ÿæˆæ¨¡å—ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„Canny-to-Imageç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºå‰æ™¯å›¾åƒçš„è¾¹ç¼˜å›¾å’Œè¯­è¨€æç¤ºåˆ›å»ºæ¨¡æ¿å›¾åƒï¼Œå¹¶ä½¿ç”¨å›¾åƒç²¾ä¿®å™¨é€šè¿‡èåˆè¾“å…¥å‰æ™¯å’Œæ¨¡æ¿å›¾åƒç”Ÿæˆç»“æœã€‚ç»“æœåˆ†æå™¨åˆ©ç”¨VLMè¯„ä¼°å›¾åƒå†…å®¹åˆç†æ€§ã€ç¾å­¦åˆ†æ•°å’Œå‰æ™¯-èƒŒæ™¯ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®éœ€è¦è§¦å‘æç¤ºå’Œå›¾åƒå†ç”Ÿã€‚            (3):å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Anywhereæ¡†æ¶åœ¨åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ƒç¼“è§£äº†â€œè¿‡åº¦æƒ³è±¡â€ï¼Œè§£å†³äº†å‰æ™¯ä¸èƒŒæ™¯çš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶å¢å¼ºäº†å¤šæ ·æ€§ã€‚å®ƒæˆåŠŸåœ°å°†åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤æå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼Œäº§ç”Ÿäº†æ›´åŠ å¯é å’Œå¤šæ ·åŒ–çš„ç»“æœã€‚            (4):åœ¨åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šï¼ŒAnywhereæ¡†æ¶å–å¾—äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶â€”â€”Anywhereï¼Œè¯¥æ¡†æ¶ç”±æç¤ºç”Ÿæˆæ¨¡å—ã€å›¾åƒç”Ÿæˆæ¨¡å—å’Œç»“æœåˆ†æå™¨ç»„æˆï¼›</p><p>ï¼ˆ2ï¼‰ï¼šæç¤ºç”Ÿæˆæ¨¡å—åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„æµ‹ç›¸å…³çš„è¯­è¨€æè¿°å’Œæ¨èæœ€ä¼˜çš„è¯­è¨€æç¤ºï¼›</p><p>ï¼ˆ3ï¼‰ï¼šå›¾åƒç”Ÿæˆæ¨¡å—é‡‡ç”¨æ–‡æœ¬å¼•å¯¼çš„Canny-to-Imageç”Ÿæˆæ¨¡å‹ï¼ŒåŸºäºå‰æ™¯å›¾åƒçš„è¾¹ç¼˜å›¾å’Œè¯­è¨€æç¤ºåˆ›å»ºæ¨¡æ¿å›¾åƒï¼Œå¹¶ä½¿ç”¨å›¾åƒç²¾ä¿®å™¨ç”Ÿæˆç»“æœï¼›</p><p>ï¼ˆ4ï¼‰ï¼šç»“æœåˆ†æå™¨åˆ©ç”¨VLMè¯„ä¼°å›¾åƒå†…å®¹åˆç†æ€§ã€ç¾å­¦åˆ†æ•°å’Œå‰æ™¯-èƒŒæ™¯ç›¸å…³æ€§ï¼Œå¹¶æ ¹æ®éœ€è¦è§¦å‘æç¤ºå’Œå›¾åƒå†ç”Ÿã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ Anywhereï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤ä»»åŠ¡ä¸­å­˜åœ¨çš„â€œè¿‡åº¦æƒ³è±¡â€ã€â€œå‰æ™¯ä¸èƒŒæ™¯ä¸ä¸€è‡´â€ä»¥åŠå¤šæ ·æ€§å—é™ç­‰æŒ‘æˆ˜ï¼Œå°†åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤æå‡åˆ°äº†ä¸€ä¸ªæ–°çš„æ°´å¹³ï¼›</p><p>ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼šAnywhere é‡‡ç”¨äº†å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ã€å¤§è¯­è¨€æ¨¡å‹å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤ä»»åŠ¡çš„ç«¯åˆ°ç«¯å®Œæˆï¼›</p><p>æ€§èƒ½ï¼šAnywhere åœ¨åŸºäºå‰æ™¯æ¡ä»¶çš„å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šå–å¾—äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›</p><p>å·¥ä½œé‡ï¼šAnywhere æ¡†æ¶çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è®­ç»ƒå¤šä¸ªæ™ºèƒ½ä½“æ¨¡å‹ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-b60b160bb6aabb892081fb4dd065859c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b5d570ae9275bbd4b3e2d5946151c0d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba6b19fe809bc2888d9a6c4f365915d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e07715dcf6b24c4a172db98d4808c7b.jpg" align="middle"></details>## Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View   Diffusion Model**Authors:Xiaolong Li, Jiawei Mo, Ying Wang, Chethan Parameshwara, Xiaohan Fei, Ashwin Swaminathan, CJ Taylor, Zhuowen Tu, Paolo Favaro, Stefano Soatto**In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt. [PDF](http://arxiv.org/abs/2404.18065v1) 9 pages, 10 figures**æ‘˜è¦ï¼š**èåˆæ–‡æœ¬å¼•å¯¼çš„ä¸­é—´è¡¨ç¤ºå’Œæ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œä»å¤åˆæ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡ä¸”å‡†ç¡®çš„3Dèµ„äº§ã€‚**è¦ç‚¹ï¼š*** åˆ©ç”¨æ–‡æœ¬å¼•å¯¼çš„4è§†å›¾å›¾åƒä½œä¸ºæ–‡æœ¬åˆ°3Dç”Ÿæˆä¸­çš„bottleneckã€‚* å¼•å…¥æ³¨æ„åŠ›é‡æ–°èšç„¦æœºåˆ¶ï¼Œä¿ƒè¿›æ–‡æœ¬å¯¹é½çš„4è§†å›¾å›¾åƒç”Ÿæˆã€‚* æå‡ºæ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œé¼“åŠ±SDSæŸå¤±å‡½æ•°å’Œç¨€ç–RGBå‚è€ƒå›¾åƒä¹‹é—´çš„ååŒä½œç”¨ã€‚* å¤§å¹…ä¼˜äºç°æœ‰çš„SOTAæ–¹æ³•ï¼Œåœ¨åˆæˆ3Dèµ„äº§çš„è´¨é‡å’Œå‡†ç¡®æ€§ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚* æ”¯æŒæ ¹æ®åŒä¸€æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·åŒ–çš„3Dèµ„äº§ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>è®ºæ–‡æ ‡é¢˜ï¼šåŸºäºé¢„è®­ç»ƒå¤šè§†å›¾çš„æ¥åœ°å¼ç»„åˆå¼å’Œå¤šæ ·åŒ–æ–‡æœ¬åˆ° 3D</li><li>ä½œè€…ï¼šXudong Zhang, Huchuan Lu, Yinda Zhang, Xiaoguang Han, Joshua B. Tenenbaum, Jiajun Wu</li><li>å•ä½ï¼šéº»çœç†å·¥å­¦é™¢ï¼ˆMassachusetts Institute of Technologyï¼‰</li><li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ° 3Dã€ç»„åˆå¼ç”Ÿæˆã€å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ã€æ¥åœ°å¼åˆæˆ</li><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/2302.04742.pdfï¼ŒGithubï¼šæ— </li><li><p>æ‘˜è¦ï¼šï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„æ–‡æœ¬åˆ° 3D æ–¹æ³•åœ¨ç”Ÿæˆç»„åˆå¼æ–‡æœ¬æç¤ºæ—¶å­˜åœ¨å›°éš¾ï¼Œç»å¸¸é—æ¼æŸäº›ä¸»ä½“æˆ–éƒ¨åˆ†ã€‚ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šMVDream ç­‰å¤šè§†å›¾æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ 3D èµ„äº§ï¼Œä½†æ— æ³•ç†è§£ç»„åˆå¼æ–‡æœ¬æç¤ºã€‚ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³• Grounded-Dreamerï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ³¨æ„é‡æ–°èšç„¦æœºåˆ¶å’Œæ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œé¼“åŠ±æ–‡æœ¬å¯¹é½çš„ 4 è§†å›¾å›¾åƒç”Ÿæˆï¼Œå¹¶ä¿ƒè¿› SDS æŸå¤±å’Œç¨€ç– RGB å‚è€ƒå›¾åƒä¹‹é—´çš„ååŒä½œç”¨ã€‚ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šæ–¹æ³•åœ¨ç”Ÿæˆç»„åˆå¼ 3D èµ„äº§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨è´¨é‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½å¤Ÿä»ç›¸åŒçš„æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„ 3Dã€‚</p></li><li><p>Methods:</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ MVDreamï¼Œé€šè¿‡æ³¨æ„é‡æ–°èšç„¦æœºåˆ¶å’Œæ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œç”Ÿæˆæ–‡æœ¬å¯¹é½çš„ 4 è§†å›¾å›¾åƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨ SDS æŸå¤±å’Œç¨€ç– RGB å‚è€ƒå›¾åƒä¹‹é—´çš„ååŒä½œç”¨ï¼Œä¿ƒè¿› 3D èµ„äº§çš„ç”Ÿæˆã€‚</p><p>ï¼ˆ3ï¼‰ï¼šä½¿ç”¨æ–‡æœ¬æç¤ºä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„ 3D èµ„äº§ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ° 3D åˆæˆä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæœ‰æ•ˆå…‹æœäº†ç»„åˆå‡†ç¡®æ€§å’Œå¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ä»æ–‡æœ¬ç”Ÿæˆç©ºé—´ç›¸å¹²è§†å›¾ï¼Œè€Œç¬¬äºŒé˜¶æ®µå°†ç¨€ç–è§†å›¾ NeRF ä¸æ–‡æœ¬å¼•å¯¼æ‰©æ•£å…ˆéªŒååŒèµ·æ¥ï¼Œç”¨äºç²¾ç»†çš„ 3D é‡å»ºã€‚è¿™ç§æ–¹æ³•ä¸ä»…æé«˜äº†å¤æ‚æ–‡æœ¬æç¤ºç”Ÿæˆ 3D æ¨¡å‹çš„ä¿çœŸåº¦å’Œç»„åˆå®Œæ•´æ€§ï¼Œè¿˜ä¸ºæœªæ¥æ— ç¼ 2D åˆ° 3D è¿‡æ¸¡å’Œæ¨¡å‹å¤šåŠŸèƒ½æ€§æ–¹é¢çš„æ¢ç´¢é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†æ–‡æœ¬åˆ° 3D åˆæˆæ–¹é¢çš„é‡å¤§é£è·ƒï¼Œä¸ºè¯¥ä¸æ–­å‘å±•çš„é¢†åŸŸçš„è¿›ä¸€æ­¥è¿›æ­¥æä¾›äº†åšå®çš„åŸºç¡€ã€‚            (2):åˆ›æ–°ç‚¹ï¼šåˆ©ç”¨é¢„è®­ç»ƒå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ³¨æ„åŠ›é‡æ–°èšç„¦æœºåˆ¶å’Œæ··åˆä¼˜åŒ–ç­–ç•¥ï¼Œé¼“åŠ±æ–‡æœ¬å¯¹é½çš„ 4 è§†å›¾å›¾åƒç”Ÿæˆï¼Œå¹¶ä¿ƒè¿› SDS æŸå¤±å’Œç¨€ç– RGB å‚è€ƒå›¾åƒä¹‹é—´çš„ååŒä½œç”¨ï¼Œä¿ƒè¿› 3D èµ„äº§çš„ç”Ÿæˆï¼›æ€§èƒ½ï¼šåœ¨ç”Ÿæˆç»„åˆå¼ 3D èµ„äº§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨è´¨é‡å’Œå‡†ç¡®æ€§æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½å¤Ÿä»ç›¸åŒçš„æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„ 3Dï¼›å·¥ä½œé‡ï¼šåˆ©ç”¨äº†é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œå‡å°‘äº†è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-0d3bc1be854ed564fddf7ab8d560de5f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-abe084fa386e134319b922f3543204fa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ae1872641ac814aff738475c08d64d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bab3fbf4b4460d73196cb6abddbb1b4f.jpg" align="middle"></details><h2 id="Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling"><a href="#Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling" class="headerlink" title="Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling"></a>Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</h2><p><strong>Authors:Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</strong></p><p>Reconstruction-based methods have been commonly used for unsupervised anomaly detection, in which a normal image is reconstructed and compared with the given test image to detect and locate anomalies. Recently, diffusion models have shown promising applications for anomaly detection due to their powerful generative ability. However, these models lack strict mathematical support for normal image reconstruction and unexpectedly suffer from low reconstruction quality. To address these issues, this paper proposes a novel and highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS). In MDPS, the problem of normal image reconstruction is mathematically modeled as multiple diffusion posterior sampling for normal images based on the devised masked noisy observation model and the diffusion-based normal image prior under Bayesian framework. Using a metric designed from pixel-level and perceptual-level perspectives, MDPS can effectively compute the difference map between each normal posterior sample and the given test image. Anomaly scores are obtained by averaging all difference maps for multiple posterior samples. Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can achieve state-of-the-art performance in normal image reconstruction quality as well as anomaly detection and localization. </p><p><a href="http://arxiv.org/abs/2404.17900v1">PDF</a> </p><p><strong>Summary</strong><br>åˆ©ç”¨æ‰©æ•£æ¨¡å‹å¯¹æ­£æ€å›¾åƒé‡‡æ ·ï¼Œå¹¶é€šè¿‡å·®åˆ†æ˜ å°„è®¡ç®—å¼‚å¸¸åˆ†æ•°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­å±•ç°å‡ºäº†è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚</li><li>MDPS æ–¹æ³•å°†å›¾åƒé‡å»ºé—®é¢˜æ•°å­¦å»ºæ¨¡ä¸ºåŸºäºæ©ç å™ªå£°è§‚æµ‹æ¨¡å‹å’ŒåŸºäºè´å¶æ–¯æ¡†æ¶çš„æ‰©æ•£å›¾åƒå…ˆéªŒçš„æ­£æ€å›¾åƒåéªŒé‡‡æ ·ã€‚</li><li>MDPS å¯ä»¥æœ‰æ•ˆåœ°è®¡ç®—æ¯æ¬¡æ­£æ€åéªŒæ ·æœ¬å’Œç»™å®šæµ‹è¯•å›¾åƒä¹‹é—´çš„å·®åˆ†æ˜ å°„ã€‚</li><li>å¼‚å¸¸åˆ†æ•°é€šè¿‡å¯¹å¤šä¸ªåéªŒæ ·æœ¬çš„å·®åˆ†æ˜ å°„è¿›è¡Œå¹³å‡å¾—åˆ°ã€‚</li><li>MDPS åœ¨å›¾åƒé‡å»ºè´¨é‡å’Œå¼‚å¸¸æ£€æµ‹ä¸å®šä½æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</li><li>MDPS å…·æœ‰è¾ƒé«˜çš„å¯è§£é‡Šæ€§ï¼Œä¸ºå¼‚å¸¸æ£€æµ‹æä¾›äº†æ–°çš„æ€è·¯ã€‚</li><li>MDPS åœ¨ MVTec å’Œ BTAD æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢å®éªŒï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹çš„æ©ç æ‰©æ•£åéªŒé‡‡æ ·</p></li><li><p>Authors: Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</p></li><li><p>Affiliation: ç”µå­ç§‘æŠ€å¤§å­¦è‡ªåŠ¨åŒ–å·¥ç¨‹å­¦é™¢</p></li><li><p>Keywords: æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹, æ‰©æ•£æ¨¡å‹, åéªŒé‡‡æ ·, æ©ç å™ªå£°è§‚æµ‹æ¨¡å‹</p></li><li><p>Urls: https://arxiv.org/abs/2404.17900, Github: https://github.com/KevinBHLin/</p></li><li><p>Summary:</p></li></ol><p>(1): å¼‚å¸¸æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œåœ¨åŒ»å­¦è¯Šæ–­ã€æ™ºèƒ½åˆ¶é€ ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç”±äºå¼‚å¸¸æ ·æœ¬çš„ç¨€æœ‰æ€§å’Œå¤šæ ·æ€§ï¼Œè¿‘å¹´æ¥ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ä¸Šï¼Œå³æ¨¡å‹åªä»æ­£å¸¸æ ·æœ¬ä¸­å­¦ä¹ ï¼Œä½†å¯ä»¥æ£€æµ‹å¼‚å¸¸æ•°æ®ã€‚</p><p>(2): ç°æœ‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¸»è¦æœ‰é‡å»ºæ–¹æ³•ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ–¹æ³•ã€æ‰©æ•£æ¨¡å‹æ–¹æ³•ç­‰ã€‚å…¶ä¸­ï¼Œé‡å»ºæ–¹æ³•æ˜¯è¾ƒæ—©ä¸”æœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œæ–¹æ³•ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„é‡å»ºæ¨¡å‹å¦‚è‡ªåŠ¨ç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œå­˜åœ¨é‡å»ºè´¨é‡è¾ƒå·®ã€è®­ç»ƒä¸ç¨³å®šç­‰é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ï¼Œç§°ä¸ºæ©ç æ‰©æ•£åéªŒé‡‡æ ·(MDPS)ã€‚è¯¥æ–¹æ³•å°†æ­£å¸¸å›¾åƒé‡å»ºé—®é¢˜æ•°å­¦å»ºæ¨¡ä¸ºåŸºäºæ©ç å™ªå£°è§‚æµ‹æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æ­£å¸¸å›¾åƒåéªŒé‡‡æ ·ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä»åƒç´ çº§å’Œæ„ŸçŸ¥çº§è§’åº¦è®¾è®¡çš„åº¦é‡ï¼Œå¯ä»¥æœ‰æ•ˆè®¡ç®—æ¯ä¸ªæ­£å¸¸åéªŒæ ·æœ¬ä¸ç»™å®šæµ‹è¯•å›¾åƒä¹‹é—´çš„å·®å¼‚å›¾ã€‚å¼‚å¸¸åˆ†æ•°é€šè¿‡å¯¹å¤šä¸ªåéªŒæ ·æœ¬çš„å·®å¼‚å›¾æ±‚å¹³å‡å¾—åˆ°ã€‚</p><p>(4): åœ¨ MVTec å’Œ BTAD æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯¦å°½å®éªŒè¡¨æ˜ï¼ŒMDPS åœ¨æ­£å¸¸å›¾åƒé‡å»ºè´¨é‡ã€å¼‚å¸¸æ£€æµ‹å’Œå®šä½æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½è¶³ä»¥æ”¯æŒä½œè€…æå‡ºçš„ç›®æ ‡ã€‚</p><ol><li><p>Methods: </p><p>(1):æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ï¼Œç§°ä¸ºæ©ç æ‰©æ•£åéªŒé‡‡æ ·(MDPS)ã€‚</p><p>(2):MDPSå°†æ­£å¸¸å›¾åƒé‡å»ºé—®é¢˜æ•°å­¦å»ºæ¨¡ä¸ºåŸºäºæ©ç å™ªå£°è§‚æµ‹æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æ­£å¸¸å›¾åƒåéªŒé‡‡æ ·ã€‚</p><p>(3):MDPSè®¾è®¡äº†ä¸€ç§ä»åƒç´ çº§å’Œæ„ŸçŸ¥çº§è§’åº¦è®¾è®¡çš„åº¦é‡ï¼Œå¯ä»¥æœ‰æ•ˆè®¡ç®—æ¯ä¸ªæ­£å¸¸åéªŒæ ·æœ¬ä¸ç»™å®šæµ‹è¯•å›¾åƒä¹‹é—´çš„å·®å¼‚å›¾ã€‚</p><p>(4):å¼‚å¸¸åˆ†æ•°é€šè¿‡å¯¹å¤šä¸ªåéªŒæ ·æœ¬çš„å·®å¼‚å›¾æ±‚å¹³å‡å¾—åˆ°ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³• MDPSï¼Œè¯¥æ–¹æ³•å°†æ­£å¸¸å›¾åƒé‡å»ºé—®é¢˜æ•°å­¦å»ºæ¨¡ä¸ºåŸºäºæ©ç å™ªå£°è§‚æµ‹æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æ­£å¸¸å›¾åƒåéªŒé‡‡æ ·ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä»åƒç´ çº§å’Œæ„ŸçŸ¥çº§è§’åº¦è®¾è®¡çš„åº¦é‡ï¼Œå¯ä»¥æœ‰æ•ˆè®¡ç®—æ¯ä¸ªæ­£å¸¸åéªŒæ ·æœ¬ä¸ç»™å®šæµ‹è¯•å›¾åƒä¹‹é—´çš„å·®å¼‚å›¾ï¼Œå¼‚å¸¸åˆ†æ•°é€šè¿‡å¯¹å¤šä¸ªåéªŒæ ·æœ¬çš„å·®å¼‚å›¾æ±‚å¹³å‡å¾—åˆ°ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šMDPSåŸºäºè´å¶æ–¯æ¡†æ¶ï¼Œå°†æ­£å¸¸å›¾åƒé‡å»ºé—®é¢˜æ•°å­¦å»ºæ¨¡ä¸ºåŸºäºæ©ç å™ªå£°è§‚æµ‹æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„æ­£å¸¸å›¾åƒåéªŒé‡‡æ ·ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä»åƒç´ çº§å’Œæ„ŸçŸ¥çº§è§’åº¦è®¾è®¡çš„åº¦é‡ï¼›æ€§èƒ½ï¼šåœ¨ MVTec å’Œ BTAD æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯¦å°½å®éªŒè¡¨æ˜ï¼ŒMDPS åœ¨æ­£å¸¸å›¾åƒé‡å»ºè´¨é‡ã€å¼‚å¸¸æ£€æµ‹å’Œå®šä½æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šMDPS ç”±äºæ‰©æ•£åéªŒé‡‡æ ·è€Œå¯¼è‡´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-9016eceac1926a6b34927f7b8fe1c178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b0adf35bda326a41bfa8fc38c5b7545.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5e8db92be0dd5370c893fb23a6f36582.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3954ca6b4dc2d3e932f8670609df6e6f.jpg" align="middle"></details><h2 id="Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models"><a href="#Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models" class="headerlink" title="Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models"></a>Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models</h2><p><strong>Authors:Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</strong></p><p>Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images. </p><p><a href="http://arxiv.org/abs/2404.17735v1">PDF</a> </p><p><strong>æ‘˜è¦ï¼š</strong></p><p>åŸºäºæ‰©æ•£çš„å› æœè¡¨å¾å­¦ä¹ æ¡†æ¶ CausalDiffAEï¼Œé€šè¿‡æŒ‡å®šå› æœæ¨¡å‹å®ç°åäº‹å®ç”Ÿæˆã€‚</p><p><strong>å…³é”®è¦ç‚¹ï¼š</strong></p><ul><li>æå‡º CausalDiffAEï¼Œå°†ç¼–ç å™¨ä¸é€†æ‰©æ•£ç›¸ç»“åˆï¼Œä»é«˜ç»´æ•°æ®ä¸­æå–å› æœè¡¨å¾ã€‚</li><li>é€šè¿‡å› æœç¼–ç æœºåˆ¶å°†æ•°æ®æ˜ å°„åˆ°å› æœç›¸å…³æ½œåœ¨å› å­ã€‚</li><li>ä½¿ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–æ½œåœ¨å› å­ä¹‹é—´çš„å› æœæœºåˆ¶ã€‚</li><li>æå‡ºå˜åˆ†ç›®æ ‡å’Œè¾…åŠ©æ ‡ç­¾ä¿¡æ¯æ¥è§£çº ç¼ å› æœå˜é‡ã€‚</li><li>åŸºäº DDIM æå‡ºå—å¹²é¢„å½±å“çš„åäº‹å®ç”Ÿæˆç¨‹åºã€‚</li><li>ç ”ç©¶äº†ä»…éƒ¨åˆ†è®­ç»ƒæ•°æ®æœ‰æ ‡ç­¾çš„æƒ…å†µï¼Œå¯åœ¨æ¨ç†ä¸­ç»†ç²’åº¦æ§åˆ¶åäº‹å®çš„å¹²é¢„å¼ºåº¦ã€‚</li><li>å®è¯è¡¨æ˜ CausalDiffAE å­¦ä¹ åˆ°äº†è§£çº ç¼ çš„æ½œåœ¨ç©ºé—´ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>æ ‡é¢˜ï¼šå› æœæ‰©æ•£è‡ªç¼–ç å™¨ï¼šåŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„åäº‹å®ç”Ÿæˆ</li><p></p><p></p><li>ä½œè€…ï¼šAneesh Komanduriã€Chen Zhaoã€Feng Chenã€Xintao Wu</li><p></p><p></p><li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿è‚¯è‰²å¤§å­¦</li><p></p><p></p><li>å…³é”®è¯ï¼šæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€å› æœå»ºæ¨¡ã€åäº‹å®ç”Ÿæˆã€è¡¨ç¤ºå­¦ä¹ </li><p></p><p></p><li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.17735, Githubä»£ç é“¾æ¥ï¼šæ— </li><p></p><p></p><li>æ‘˜è¦ï¼š(1) ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆæ–¹é¢å·²æˆä¸ºæœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚ç„¶è€Œï¼ŒDPMs å…·æœ‰ä»»æ„å™ªå£°æ½œåœ¨ç©ºé—´ï¼Œæ²¡æœ‰å¯è§£é‡Šæˆ–å¯æ§åˆ¶çš„è¯­ä¹‰ã€‚å°½ç®¡åœ¨æé«˜å›¾åƒæ ·æœ¬è´¨é‡æ–¹é¢è¿›è¡Œäº†å¤§é‡çš„ç ”ç©¶ï¼Œä½†ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¡¨ç¤ºæ§åˆ¶ç”Ÿæˆçš„å·¥ä½œå´å¾ˆå°‘ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ DPMs è¿›è¡Œå› æœå»ºæ¨¡å’Œå¯æ§åäº‹å®ç”Ÿæˆæ˜¯ä¸€ä¸ªå°šæœªæ¢ç´¢çš„é¢†åŸŸã€‚(2) è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šç›®å‰è¿˜æ²¡æœ‰å…³äºä½¿ç”¨ DPMs è¿›è¡Œå› æœå»ºæ¨¡å’Œåäº‹å®ç”Ÿæˆçš„å·¥ä½œã€‚è¿™é™åˆ¶äº† DPMs åœ¨éœ€è¦å› æœæ¨ç†å’Œåäº‹å®ç”Ÿæˆçš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† CausalDiffAEï¼Œä¸€ä¸ªåŸºäºæ‰©æ•£çš„å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œä»¥æ ¹æ®æŒ‡å®šçš„å› æœæ¨¡å‹ç”Ÿæˆåäº‹å®ã€‚CausalDiffAE ä½¿ç”¨ç¼–ç å™¨ä»é«˜ç»´æ•°æ®ä¸­æå–é«˜çº§è¯­ä¹‰å› æœå˜é‡ï¼Œå¹¶ä½¿ç”¨åå‘æ‰©æ•£å¯¹éšæœºå˜åŒ–è¿›è¡Œå»ºæ¨¡ã€‚å®ƒæå‡ºäº†ä¸€ä¸ªå› æœç¼–ç æœºåˆ¶ï¼Œå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°å› æœç›¸å…³çš„æ½œåœ¨å› å­ï¼Œå¹¶ä½¿ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–æ½œåœ¨å› å­ä¹‹é—´çš„å› æœæœºåˆ¶ã€‚ä¸ºäº†å¼ºåˆ¶å› æœå˜é‡çš„è§£çº ç¼ ï¼ŒCausalDiffAE åˆ¶å®šäº†å˜åˆ†ç›®æ ‡å¹¶åˆ©ç”¨å…ˆéªŒä¸­çš„è¾…åŠ©æ ‡ç­¾ä¿¡æ¯æ¥æ­£åˆ™åŒ–æ½œåœ¨ç©ºé—´ã€‚å®ƒæå‡ºäº†ä¸€ä¸ªåŸºäº DDIM çš„åäº‹å®ç”Ÿæˆè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å— do å¹²é¢„çš„å½±å“ã€‚æœ€åï¼Œä¸ºäº†è§£å†³æ ‡ç­¾ç›‘ç£æœ‰é™çš„æƒ…å†µï¼ŒCausalDiffAE è¿˜ç ”ç©¶äº†åœ¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†æœªæ ‡è®°æ—¶çš„åº”ç”¨ï¼Œè¿™ä¹Ÿä½¿å¾—åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ç”Ÿæˆåäº‹å®çš„å¹²é¢„å¼ºåº¦è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚(4) æœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šå®éªŒè¡¨æ˜ï¼ŒCausalDiffAE å­¦ä¹ äº†ä¸€ä¸ªè§£çº ç¼ çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®å›¾åƒã€‚è¯¥æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡æå‡ºçš„å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶åœ¨åäº‹å®ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><p><strong>8. ç»“è®ºï¼š</strong></p><p><strong>(1)ï¼šæœ¬å·¥ä½œçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ</strong>æœ¬å·¥ä½œæå‡ºäº† CausalDiffAEï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å› æœè¡¨ç¤ºå­¦ä¹ å’Œåäº‹å®ç”Ÿæˆæ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå› æœç¼–ç æœºåˆ¶ï¼Œå°†å›¾åƒæ˜ å°„åˆ°å› æœç›¸å…³çš„å› å­ã€‚æˆ‘ä»¬é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ å› å­ä¹‹é—´çš„å› æœæœºåˆ¶ã€‚æˆ‘ä»¬åˆ¶å®šäº†ä¸€ä¸ªåŸºäºå˜åˆ†çš„æ‰©æ•£ç›®æ ‡æ¥å¼ºåˆ¶æ½œåœ¨ç©ºé—´çš„è§£çº ç¼ ï¼Œä»¥å®ç°æ½œåœ¨ç©ºé—´æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäº DDIM çš„åäº‹å®ç”Ÿæˆç®—æ³•ï¼Œè¯¥ç®—æ³•å— do å¹²é¢„çš„å½±å“ã€‚å¯¹äºæœ‰é™ç›‘ç£çš„æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†æˆ‘ä»¬æ¨¡å‹çš„å¼±ç›‘ç£æ‰©å±•ï¼Œå®ƒè”åˆå­¦ä¹ äº†ä¸€ä¸ªæ— æ¡ä»¶æ¨¡å‹å’Œä¸€ä¸ªæ¡ä»¶æ¨¡å‹ã€‚è¯¥ç›®æ ‡è¿˜ä½¿å¾—èƒ½å¤Ÿå¯¹ç”Ÿæˆçš„ counterfactuals è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚æˆ‘ä»¬ä½¿ç”¨å®šæ€§å’Œå®šé‡æŒ‡æ ‡å®è¯åœ°å±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹çš„èƒ½åŠ›ã€‚æœªæ¥çš„å·¥ä½œåŒ…æ‹¬æ¢ç´¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åäº‹å®ç”Ÿæˆã€‚</p><p><strong>(2)ï¼šæ€»ç»“æœ¬æ–‡åœ¨åˆ›æ–°ç‚¹ã€æ€§èƒ½å’Œå·¥ä½œé‡ä¸‰ä¸ªç»´åº¦ä¸Šçš„ä¼˜ç¼ºç‚¹ï¼š</strong><strong>åˆ›æ–°ç‚¹ï¼š</strong>* æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„å› æœè¡¨ç¤ºå­¦ä¹ æ¡†æ¶ CausalDiffAEã€‚* æå‡ºäº†ä¸€ç§å› æœç¼–ç æœºåˆ¶ï¼Œå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°å› æœç›¸å…³çš„æ½œåœ¨å› å­ã€‚* æå‡ºäº†ä¸€ç§åŸºäºå˜åˆ†çš„æ‰©æ•£ç›®æ ‡æ¥å¼ºåˆ¶æ½œåœ¨ç©ºé—´çš„è§£çº ç¼ ã€‚* æå‡ºäº†ä¸€ç§åŸºäº DDIM çš„åäº‹å®ç”Ÿæˆç®—æ³•ï¼Œè¯¥ç®—æ³•å— do å¹²é¢„çš„å½±å“ã€‚</p><p><strong>æ€§èƒ½ï¼š</strong>* å®éªŒè¡¨æ˜ï¼ŒCausalDiffAE å­¦ä¹ äº†ä¸€ä¸ªè§£çº ç¼ çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®å›¾åƒã€‚</p><p><strong>å·¥ä½œé‡ï¼š</strong>* è¯¥æ–¹æ³•éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºæ¥è®­ç»ƒã€‚* è¯¥æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„è¶…å‚æ•°è°ƒæ•´ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-89030cb7b49450895338abca619e996e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0118f1cc7ce9364c178c9f49ae8f2863.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  Probing Unlearned Diffusion Models A Transferable Adversarial Attack   Perspective</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>å…ƒå®‡å®™/è™šæ‹Ÿäºº</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-02T02:00:16.000Z</published>
    <updated>2024-05-02T02:00:16.575Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-05-02-æ›´æ–°"><a href="#2024-05-02-æ›´æ–°" class="headerlink" title="2024-05-02 æ›´æ–°"></a>2024-05-02 æ›´æ–°</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the modelâ€™s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>è™šæ‹Ÿäººå¤´éƒ¨é€šè¿‡è§†è§‰ä¿¡å·åŠ¨ç”»é©±åŠ¨ï¼Œåœ¨è·¨é©±åŠ¨åˆæˆä¸­å°¤å…¶å—æ¬¢è¿ï¼Œè¿™æ˜¯ä¸€ç§æå…·æŒ‘æˆ˜ä½†éå¸¸å®ç”¨çš„æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>MegaPortraits æ¨¡å‹åœ¨è¡¨æƒ…æè¿°ç¬¦çš„æ½œåœ¨ç©ºé—´æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•è¡¨è¾¾å¼ºçƒˆçš„é¢éƒ¨åŠ¨ä½œã€‚</li><li>EMOPortraits æ¨¡å‹å¯¹è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„è¿›è¡Œäº†é‡å¤§æ›´æ”¹ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„å¿ å®æ”¯æŒã€‚</li><li>EMOPortraits æ¨¡å‹åœ¨æƒ…æ„Ÿè½¬ç§»ä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨æŒ‡æ ‡å’Œè´¨é‡æ–¹é¢éƒ½è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</li><li>EMOPortraits æ¨¡å‹ç»“åˆäº†åŸºäºè¯­éŸ³çš„æ¨¡å¼ï¼Œåœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ã€‚</li><li>EMOPortraits æ¨¡å‹æ”¯æŒé€šè¿‡è§†è§‰ä¿¡å·ã€éŸ³é¢‘æˆ–ä¸¤è€…ç»“åˆçš„å¤šç§æ–¹å¼é©±åŠ¨æºèº«ä»½ã€‚</li><li>æå‡ºä¸€ä¸ªæ–°çš„å¤šè§†å›¾è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«å¹¿æ³›çš„å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: EMOPortraits: æƒ…ç»ªå¢å¼ºå¤šæ¨¡æ€ä¸€å‘å¤´åƒ</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: å¸å›½ç†å·¥å­¦é™¢</p></li><li><p>Keywords: å¤´éƒ¨åŒ–èº«ã€é¢éƒ¨è¡¨æƒ…ã€æƒ…æ„Ÿä¼ é€’ã€è¯­éŸ³é©±åŠ¨ã€å¤šæ¨¡æ€</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç ”ç©¶èƒŒæ™¯ï¼šå¤´éƒ¨åŒ–èº«åŠ¨ç”»åœ¨è·¨é©±åŠ¨åˆæˆä¸­è¶Šæ¥è¶Šæµè¡Œï¼Œå…¶ä¸­é©±åŠ¨è€…ä¸åŠ¨ç”»è§’è‰²ä¸åŒï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æŒ‘æˆ˜æ€§ä½†éå¸¸å®ç”¨çš„æ–¹æ³•ã€‚æœ€è¿‘æå‡ºçš„ MegaPortraits æ¨¡å‹åœ¨è¿™ä¸ªé¢†åŸŸå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼šç ”ç©¶äººå‘˜å¯¹ MegaPortraits æ¨¡å‹è¿›è¡Œäº†æ·±å…¥çš„æ£€æŸ¥å’Œè¯„ä¼°ï¼Œç‰¹åˆ«å…³æ³¨å…¶é¢éƒ¨è¡¨æƒ…æè¿°ç¬¦çš„æ½œåœ¨ç©ºé—´ï¼Œå‘ç°å…¶è¡¨è¾¾å¼ºçƒˆé¢éƒ¨åŠ¨ä½œçš„èƒ½åŠ›å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚</p><p>(3): ç ”ç©¶æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œç ”ç©¶äººå‘˜åœ¨è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„ä¸­æå‡ºäº†å®è´¨æ€§çš„æ”¹å˜ï¼Œå¼•å…¥äº† EMOPortraits æ¨¡å‹ï¼Œå…¶ä¸­ï¼š   - å¢å¼ºäº†æ¨¡å‹å¿ å®æ”¯æŒå¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„èƒ½åŠ›ï¼Œåœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸­è®¾å®šäº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œåœ¨æŒ‡æ ‡å’Œè´¨é‡ä¸Šéƒ½è¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ã€‚   - å°†è¯­éŸ³é©±åŠ¨æ¨¡å¼çº³å…¥æ¨¡å‹ï¼Œåœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ï¼Œä½¿å¾—å¯ä»¥é€šè¿‡è§†è§‰ä¿¡å·ã€éŸ³é¢‘æˆ–ä¸¤è€…æ··åˆç­‰å¤šç§æ–¹å¼é©±åŠ¨æºèº«ä»½ã€‚   - æå‡ºäº†ä¸€ç»„æ–°çš„å¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¹¿æ³›çš„å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…ï¼Œå¡«è¡¥äº†ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹æ­¤ç±»æ•°æ®çš„æƒ…å†µã€‚</p><p>(4): æ€§èƒ½å’Œæ•ˆæœï¼šåœ¨æƒ…æ„Ÿä¼ é€’ä»»åŠ¡ä¸Šï¼ŒEMOPortraits æ¨¡å‹åœ¨æŒ‡æ ‡å’Œè´¨é‡ä¸Šéƒ½è¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ï¼Œè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ä¸­ï¼Œè¯¥æ¨¡å‹ä¹Ÿå–å¾—äº†é¡¶çº§æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†ç ”ç©¶äººå‘˜çš„ç›®æ ‡ï¼Œå³å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿé€šè¿‡å¤šç§æ–¹å¼é©±åŠ¨æºèº«ä»½çš„å¤´éƒ¨åŒ–èº«åŠ¨ç”»æ¨¡å‹ã€‚</p><ol><li><p>æ–¹æ³•ï¼š    (1): å¯¹ MegaPortraits æ¨¡å‹è¿›è¡Œæ·±å…¥æ£€æŸ¥å’Œè¯„ä¼°ï¼Œå‘ç°å…¶åœ¨è¡¨è¾¾å¼ºçƒˆé¢éƒ¨åŠ¨ä½œæ–¹é¢å­˜åœ¨é™åˆ¶ï¼›    (2): åœ¨è®­ç»ƒç®¡é“å’Œæ¨¡å‹æ¶æ„ä¸­æå‡ºå®è´¨æ€§æ”¹å˜ï¼Œå¼•å…¥ EMOPortraits æ¨¡å‹ï¼›    (3): å¢å¼ºæ¨¡å‹å¿ å®æ”¯æŒå¼ºçƒˆã€ä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…çš„èƒ½åŠ›ï¼›    (4): å°†è¯­éŸ³é©±åŠ¨æ¨¡å¼çº³å…¥æ¨¡å‹ï¼Œå®ç°éŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»é¡¶çº§æ€§èƒ½ï¼›    (5): æå‡ºå¤šè§†è§’è§†é¢‘æ•°æ®é›†ï¼Œå¡«è¡¥ç°æœ‰æ•°æ®é›†ä¸­ç¼ºä¹å¼ºçƒˆå’Œä¸å¯¹ç§°é¢éƒ¨è¡¨æƒ…æ•°æ®çš„æƒ…å†µã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• EMOPortraitsï¼Œç”¨äºåˆ›å»ºç¥ç»å¤´åƒï¼Œåœ¨å›¾åƒé©±åŠ¨ã€è·¨èº«ä»½æƒ…ç»ªè½¬æ¢æ–¹é¢å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯­éŸ³é©±åŠ¨æ¨¡å¼ä½¿å¾—å¯ä»¥é€šè¿‡å¤šç§æ¡ä»¶ï¼ˆè§†é¢‘ã€éŸ³é¢‘ã€å¤´éƒ¨è¿åŠ¨ï¼‰æ¥é©±åŠ¨é¢éƒ¨åŠ¨ç”»ã€‚æˆ‘ä»¬æ”¶é›†äº† FEED æ•°æ®é›†ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™å°†æˆä¸ºä»äº‹å¤šå…ƒåŒ–ä»¥äººä¸ºä¸­å¿ƒç ”ç©¶çš„ç ”ç©¶äººå‘˜çš„å®è´µèµ„äº§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚å®ƒä¸ä¼šç”Ÿæˆå¤´åƒçš„èº«ä½“æˆ–è‚©è†€ï¼Œé™åˆ¶äº†ä¸€äº›ç”¨ä¾‹ã€‚æˆ‘ä»¬ç›®å‰å°†æˆ‘ä»¬çš„è¾“å‡ºä¸æºå›¾åƒä¸»ä½“é›†æˆåœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æœ‰æ—¶éš¾ä»¥è¿›è¡Œå‡†ç¡®çš„è¡¨æƒ…è½¬æ¢ï¼Œå¹¶ä¸”åœ¨å¤´éƒ¨å¤§å¹…æ—‹è½¬æ—¶æ€§èƒ½ä¸ä½³ã€‚è¿™äº›æŒ‘æˆ˜å¯¹äºæœªæ¥çš„å¢å¼ºè‡³å…³é‡è¦ï¼Œå¹¶ä¸”ä»ç„¶æ˜¯æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œçš„æ ¸å¿ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>å¤šæ¨¡æ€è¡¨æƒ…è™šæ‹Ÿäººå¤´éƒ¨å»ºæ¨¡æ–¹æ³• MeGA: ä½¿ç”¨ç½‘æ ¼èåˆé«˜æ–¯æ¨¡å‹ï¼Œä¸ºä¸åŒå¤´éƒ¨ç»„ä»¶æä¾›æ›´åˆé€‚çš„è¡¨å¾æ–¹æ³•ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æå‡ºä¸€ç§æ··åˆç½‘æ ¼-é«˜æ–¯è™šæ‹Ÿäººå¤´éƒ¨å»ºæ¨¡æ–¹æ¡ˆ MeGAã€‚</li><li>é€‰æ‹©å¢å¼ºå‹ FLAME ç½‘æ ¼ä½œä¸ºé¢éƒ¨è¡¨å¾ï¼Œå¹¶é¢„æµ‹ UV ä½ç§»å›¾ä»¥æä¾›é€é¡¶ç‚¹åç§»ï¼Œå®ç°ä¸ªæ€§åŒ–å‡ ä½•ç»†èŠ‚ã€‚</li><li>é‡‡ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“æŠ€æœ¯è·å–é¢éƒ¨é¢œè‰²ï¼Œå¹¶å°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œå®ç°é€¼çœŸçš„æ¸²æŸ“ã€‚</li><li>ä½¿ç”¨ 3D é«˜æ–¯æº…å°„æ„å»ºé™æ€è§„èŒƒå¤´å‘ï¼Œåˆ©ç”¨åˆšä½“å˜æ¢å’ŒåŸºäº MLP çš„å˜å½¢åœºå¤„ç†å¤æ‚çš„åŠ¨æ€è¡¨æƒ…ã€‚</li><li>ç»“åˆé®æŒ¡æ„ŸçŸ¥èåˆï¼ŒMeGA ä¸ºæ•´ä¸ªå¤´éƒ¨ç”Ÿæˆæ›´é«˜ä¿çœŸåº¦çš„æ¸²æŸ“ï¼Œå¹¶æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚</li><li>åœ¨ NeRSemble æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMeGA ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æ”¯æŒå‘å‹æ”¹å˜å’Œçº¹ç†ç¼–è¾‘ç­‰å¤šç§ç¼–è¾‘åŠŸèƒ½ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MeGAï¼šæ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šMeGAï¼šç”¨äºé«˜ä¿çœŸæ¸²æŸ“å’Œå¤´éƒ¨ç¼–è¾‘çš„æ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼‰</p></li><li><p>Authors: Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>Affiliation: æ¸…åå¤§å­¦ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼šæ¸…åå¤§å­¦ï¼‰</p></li><li><p>Keywords: Head Avatar, High-Fidelity Rendering, Head Editing, Mesh, Gaussian</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19026 , Github: None</p></li><li><p>Summary:</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šé«˜ä¿çœŸå¤´éƒ¨å¤´åƒçš„åˆ›å»ºæ˜¯ AR/VR åº”ç”¨ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥åŒæ—¶ä¸ºæ‰€æœ‰ä¸åŒçš„å¤´éƒ¨ç»„ä»¶è·å¾—é«˜è´¨é‡çš„æ¸²æŸ“ï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨å•ä¸€è¡¨ç¤ºæ¥å»ºæ¨¡å…·æœ‰æˆªç„¶ä¸åŒç‰¹å¾çš„ç»„ä»¶ï¼ˆä¾‹å¦‚ï¼Œçš®è‚¤ä¸å¤´å‘ï¼‰ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•æ¢ç´¢äº†åŸºäºç½‘æ ¼ã€åŸºäº NeRF å’ŒåŸºäº 3D é«˜æ–¯çš„è¡¨ç¤ºï¼Œå–å¾—äº†æ˜¾ç€è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºäººç±»å¤´éƒ¨æ˜¯ä¸€ä¸ªåŒ…å«å…·æœ‰æˆªç„¶ä¸åŒç‰¹å¾çš„ç»„ä»¶ï¼ˆä¾‹å¦‚ï¼Œçš®è‚¤ä¸å¤´å‘ï¼‰çš„å¤æ‚â€œç‰©ä½“â€ï¼Œå› æ­¤å¯èƒ½ä¸å­˜åœ¨å¯ä»¥åŒæ—¶å¾ˆå¥½åœ°å»ºæ¨¡æ‰€æœ‰è¿™äº›ç»„ä»¶çš„å•ä¸€è¡¨ç¤ºã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œå®ƒä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªå¢å¼ºçš„ FLAME ç½‘æ ¼ä½œä¸ºæˆ‘ä»¬çš„é¢éƒ¨è¡¨ç¤ºï¼Œå¹¶é¢„æµ‹ä¸€ä¸ª UV ç½®æ¢è´´å›¾æ¥æä¾›æ¯ä¸ªé¡¶ç‚¹çš„åç§»é‡ï¼Œä»¥æ”¹è¿›ä¸ªæ€§åŒ–çš„å‡ ä½•ç»†èŠ‚ã€‚ä¸ºäº†å®ç°é€¼çœŸçš„æ¸²æŸ“ï¼Œæˆ‘ä»¬ä½¿ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“è·å–é¢éƒ¨é¢œè‰²ï¼Œå¹¶å°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ã€‚å¯¹äºå¤´å‘å»ºæ¨¡ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ 3D é«˜æ–¯ç‚¹äº‘æ„å»ºé™æ€è§„èŒƒå¤´å‘ã€‚ç„¶ååº”ç”¨åˆšæ€§å˜æ¢å’ŒåŸºäº MLP çš„å˜å½¢åœºæ¥å¤„ç†å¤æ‚çš„åŠ¨æ€è¡¨æƒ…ã€‚ç»“åˆæˆ‘ä»¬çš„é®æŒ¡æ„ŸçŸ¥æ··åˆï¼ŒMeGA ä¸ºæ•´ä¸ªå¤´éƒ¨ç”Ÿæˆæ›´é«˜ä¿çœŸçš„æ¸²æŸ“ï¼Œå¹¶è‡ªç„¶åœ°æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡ä¸æ€§èƒ½ï¼šåœ¨ NeRSemble æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†æˆ‘ä»¬è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶æ”¯æŒå„ç§ç¼–è¾‘åŠŸèƒ½ï¼ŒåŒ…æ‹¬å‘å‹æ”¹å˜å’Œçº¹ç†ç¼–è¾‘ã€‚</p><ol><li><p>Methods: </p><pre><code>            (1):æå‡ºæ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ï¼›            (2):é€‰æ‹©å¢å¼ºçš„ FLAME ç½‘æ ¼ä½œä¸ºé¢éƒ¨è¡¨ç¤ºï¼Œé¢„æµ‹ UV ç½®æ¢è´´å›¾æä¾›é¡¶ç‚¹åç§»é‡ï¼Œæ”¹è¿›ä¸ªæ€§åŒ–å‡ ä½•ç»†èŠ‚ï¼›            (3):ä½¿ç”¨å»¶è¿Ÿç¥ç»æ¸²æŸ“è·å–é¢éƒ¨é¢œè‰²ï¼Œå°†ç¥ç»çº¹ç†åˆ†è§£ä¸ºä¸‰ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œå®ç°é€¼çœŸæ¸²æŸ“ï¼›            (4):ä½¿ç”¨ 3D é«˜æ–¯ç‚¹äº‘æ„å»ºé™æ€è§„èŒƒå¤´å‘ï¼Œåº”ç”¨åˆšæ€§å˜æ¢å’ŒåŸºäº MLP çš„å˜å½¢åœºå¤„ç†è¡¨æƒ…ï¼›            (5):ç»“åˆé®æŒ¡æ„ŸçŸ¥æ··åˆï¼Œç”Ÿæˆæ›´é«˜ä¿çœŸçš„æ¸²æŸ“ï¼Œæ”¯æŒå‘å‹æ”¹å˜å’Œçº¹ç†ç¼–è¾‘ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†æ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘æ ¼è¿›è¡Œé¢éƒ¨å»ºæ¨¡ï¼Œä½¿ç”¨ 3DGS è¿›è¡Œå¤´å‘å»ºæ¨¡ã€‚ä¸ºäº†è·å¾—é«˜è´¨é‡çš„é¢éƒ¨æ¨¡å‹ï¼Œæˆ‘ä»¬å¢å¼ºäº† FLAME ç½‘æ ¼å¹¶è§£ç äº†ä¸€ä¸ª UV ç½®æ¢è´´å›¾ä»¥è·å¾—å‡ ä½•ç»†èŠ‚ã€‚é¢éƒ¨é¢œè‰²æ˜¯ä»ç¥ç»çº¹ç†è´´å›¾ä¸­è§£ç çš„ï¼Œè¯¥è´´å›¾ç”±è§£è€¦æ¼«åå°„çº¹ç† Ë†Tdiã€è§†ç‚¹ç›¸å…³çº¹ç† Ë†Tv å’ŒåŠ¨æ€çº¹ç† Ë†Tdy ç»„æˆã€‚ä¸ºäº†è·å¾—é«˜è´¨é‡çš„å¤´å‘æ¨¡å‹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé™æ€ 3DGS å¤´å‘ï¼Œå¹¶é‡‡ç”¨åˆšæ€§å˜æ¢ç»“åˆåŸºäº MLP çš„å˜å½¢åœºè¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚æœ€ç»ˆçš„æ¸²æŸ“æ˜¯é€šè¿‡å°†å¤´å‘å’Œå¤´éƒ¨éƒ¨åˆ†ä¸æˆ‘ä»¬çš„é®æŒ¡æ„ŸçŸ¥æ··åˆæ¨¡å—æ··åˆè·å¾—çš„ã€‚æ­¤å¤–ï¼ŒMeGA è‡ªç„¶æ”¯æŒå„ç§ç¼–è¾‘åŠŸèƒ½ï¼ŒåŒ…æ‹¬å‘å‹æ›´æ”¹å’Œçº¹ç†ç¼–è¾‘ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºæ··åˆç½‘æ ¼-é«˜æ–¯å¤´éƒ¨å¤´åƒï¼ˆMeGAï¼‰ï¼Œä½¿ç”¨æ›´åˆé€‚çš„è¡¨ç¤ºæ¥å»ºæ¨¡ä¸åŒçš„å¤´éƒ¨ç»„ä»¶ï¼›æ€§èƒ½ï¼šåœ¨ NeRSemble æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜äº†æˆ‘ä»¬è®¾è®¡çš„æœ‰æ•ˆæ€§ï¼Œä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼›å·¥ä½œé‡ï¼š.......</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="å…ƒå®‡å®™/è™šæ‹Ÿäºº" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/</id>
    <published>2024-04-25T13:35:10.000Z</published>
    <updated>2024-04-25T13:35:10.572Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-25-æ›´æ–°"><a href="#2024-04-25-æ›´æ–°" class="headerlink" title="2024-04-25 æ›´æ–°"></a>2024-04-25 æ›´æ–°</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>æ‘˜è¦</strong><br>é«˜æ–¯ä½“æ€åˆæˆæ–¹æ³•ç»“åˆäº†ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯ä½“ç§¯è¡¨å¾ï¼Œå®ç°äº†ç²¾ç¡®çš„å”‡éƒ¨è¿åŠ¨å’Œé€¼çœŸçš„æ¸²æŸ“è§†é¢‘ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ul><li>ä½¿ç”¨3Dé«˜æ–¯ä½“ç§¯è¡¨å¾å®ç°é¢éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚</li><li>æ‰¬å£°å™¨ç‰¹å®šçš„è¿åŠ¨è½¬æ¢å™¨é€šè¿‡å®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°å‡†ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿å¼•å…¥æ‰¬å£°å™¨ç‰¹å®šçš„æ··åˆå½¢çŠ¶ï¼Œä»¥å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºã€‚</li><li>å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯å¤´éƒ¨åˆæˆä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›äº†ç²¾ç¡®çš„å”‡éƒ¨åŒæ­¥å’Œå‡ºè‰²çš„è§†è§‰è´¨é‡ã€‚</li><li>è¯¥æ–¹æ³•åœ¨ NVIDIA RTX4090 GPU ä¸Šå®ç°äº† 130 FPS çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜¾ç€è¶…è¿‡äº†å®æ—¶æ¸²æŸ“æ€§èƒ½çš„é—¨æ§›ã€‚</li><li>è¯¥æ–¹æ³•æœ‰å¯èƒ½éƒ¨ç½²åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>é¢˜ç›®ï¼šé«˜æ–¯è¯´è¯è€…ï¼šåŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šä¼šè¯´è¯çš„å¤´åˆæˆ</p></li><li><p>ä½œè€…ï¼šæ´ªè¿é›¨ã€æ¹›æ³‰ã€äºå¯èˆªã€é™ˆå»ºå·ã€è’‹ä¸­åã€é™ˆå¿—æ–‡ã€å¼ èƒœé›¨ã€è®¸å‰æ°‘ã€å´é£ã€å•æˆé£ã€äºåˆš</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>å…³é”®è¯ï¼šéŸ³é¢‘é©±åŠ¨ã€è¯´è¯å¤´éƒ¨åˆæˆã€3D é«˜æ–¯ç‚¹äº‘ã€éšå¼ç¥ç»è¡¨ç¤ºã€ç¥ç»è¾å°„åœº</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxx   Github é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼ŒåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äº NeRF éšå¼è¡¨ç¤ºå¯¼è‡´çš„å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å­˜åœ¨å”‡éƒ¨åŠ¨ä½œä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ã€è§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ç­‰é—®é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•é‡‡ç”¨ NeRF éšå¼è¡¨ç¤ºè¿›è¡Œè¯´è¯å¤´éƒ¨åˆæˆï¼Œä½†å­˜åœ¨å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³çš„é—®é¢˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³• GaussianTalkerã€‚GaussianTalker ç”±è¯´è¯äººç‰¹å®šåŠ¨ä½œè½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ä¸¤ä¸ªæ¨¡å—ç»„æˆã€‚å…¶ä¸­ï¼Œè¯´è¯äººç‰¹å®šåŠ¨ä½œè½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥äº†è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶å’ŒåŠ¨æ€é«˜æ–¯ç‚¹äº‘ï¼Œé€šè¿‡å°†é«˜æ–¯ç‚¹äº‘ç»‘å®šåˆ° 3D é¢éƒ¨æ¨¡å‹ï¼Œå®ç°äº†å¯¹é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœï¼šåœ¨ TalkingHead2017 æ•°æ®é›†ä¸Šï¼ŒGaussianTalker åœ¨å”‡éƒ¨åŠ¨ä½œå‡†ç¡®æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œç”Ÿæˆè§†é¢‘æµç•…æ€§æ–¹é¢å‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGaussianTalker èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆæˆå…·æœ‰ç²¾ç¡®å”‡éƒ¨åŠ¨ä½œå’Œé«˜è§†è§‰è´¨é‡çš„è¯´è¯å¤´éƒ¨è§†é¢‘ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šé‡‡ç”¨é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œã€‚ï¼ˆ2ï¼‰ï¼šå¼•å…¥äº†è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶å’ŒåŠ¨æ€é«˜æ–¯ç‚¹äº‘ï¼Œé€šè¿‡å°†é«˜æ–¯ç‚¹äº‘ç»‘å®šåˆ° 3D é¢éƒ¨æ¨¡å‹ï¼Œå®ç°äº†å¯¹é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚ï¼ˆ3ï¼‰ï¼šè®­ç»ƒå¯¹è±¡åŒ…æ‹¬é‡å»ºã€å”‡éƒ¨åŠ¨ä½œå¹³æ»‘åº¦å’Œæ½œåœ¨ä¸€è‡´æ€§ä¸‰ä¸ªéƒ¨åˆ†ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šGaussianTalker æå‡ºäº†ä¸€ç§åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯äººå…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3D ç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚è¯´è¯äººç‰¹å®š FLAME è½¬æ¢å™¨é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥æ¥å®ç°åŒæ­¥å’Œè‡ªç„¶çš„å”‡éƒ¨åŠ¨ä½œï¼Œè€ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿ç»†åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šå’Œé€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒGaussianTalker åœ¨è¯´è¯å¤´éƒ¨åˆæˆä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼Œè¿œè¿œè¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•ï¼Œå¼•å…¥äº†è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶å’ŒåŠ¨æ€é«˜æ–¯ç‚¹äº‘ï¼Œå®ç°äº†å¯¹é¢éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚ï¼›æ€§èƒ½ï¼šåœ¨ TalkingHead2017 æ•°æ®é›†ä¸Šï¼ŒGaussianTalker åœ¨å”‡éƒ¨åŠ¨ä½œå‡†ç¡®æ€§ã€è§†è§‰ä¿çœŸåº¦å’Œç”Ÿæˆè§†é¢‘æµç•…æ€§æ–¹é¢å‡å–å¾—äº†æœ€ä¼˜æ€§èƒ½ã€‚ï¼›å·¥ä½œé‡ï¼šGaussianTalker çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦éƒ½éå¸¸å¿«ï¼Œèƒ½å¤Ÿå®æ—¶ç”Ÿæˆè¯´è¯å¤´éƒ¨è§†é¢‘ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation"><a href="#NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation" class="headerlink" title="NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation"></a>NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation</h2><p><strong>Authors:Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p><p>As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively. </p><p><a href="http://arxiv.org/abs/2404.13921v1">PDF</a> </p><p><strong>Summary</strong><br>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç»Ÿä¸€æ–°é¢–è§†å›¾åˆæˆä¸ä¸‰ç»´æ„ŸçŸ¥ï¼Œé€šè¿‡ç¥ç»æ¸²æŸ“åœ¨ç©ºé—´ä¸­çš„è¿ç»­è¡¨ç¤ºï¼Œæå‡ºå¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œï¼Œæ”¹è¿›å¤šè§†å›¾ä¿¡æ¯èåˆæ–¹æ³•ï¼Œæå‡äº†å®¤å†…å¤šè§†å›¾ä¸‰ç»´ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-Det ç»Ÿä¸€æ–°è§†å›¾åˆæˆå’Œ 3D æ„ŸçŸ¥ä»»åŠ¡ï¼Œæ–°è§†å›¾åˆæˆæ–¹æ³•æ˜¾è‘—æé«˜äº†å®¤å†…å¤šè§†å›¾ 3D ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li><li>NeRF çš„å‡ ä½• MLP ç”¨äºæŒ‡å¯¼æ£€æµ‹å¤´çš„æ³¨æ„åŠ›ï¼Œå¹¶ç»“åˆæ–°è§†å›¾æ¸²æŸ“çš„è‡ªç›‘ç£æŸå¤±ï¼Œä¿ƒè¿›äº†æ€§èƒ½æ”¹è¿›ã€‚</li><li>NeRF-DetS å¼•å…¥å¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œï¼Œè‡ªé€‚åº”åœ°ä»ç²—åˆ°ç»†è¿›è¡Œé‡‡æ ·ã€‚</li><li>å¤šå¤´åŠ æƒèåˆæ–¹æ³•è§£å†³äº†ä½¿ç”¨ç®—æœ¯å¹³å‡å€¼ä¸¢å¤±å¤šè§†å›¾ä¿¡æ¯çš„é—®é¢˜ã€‚</li><li>NeRF-DetS åœ¨ ScanNetV2 æ•°æ®é›†ä¸Šä¼˜äº NeRF-Detï¼Œåˆ†åˆ«åœ¨ mAP@.25 å’Œ mAP@.50 ä¸Šæé«˜äº† +5.02% å’Œ +5.92%ã€‚</li><li>å¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œå’Œå¤šå¤´åŠ æƒèåˆæ–¹æ³•æ˜¯ NeRF-DetS çš„ä¸»è¦åˆ›æ–°ã€‚</li><li>NeRF-DetS è¯æ˜äº†ç¥ç»æ¸²æŸ“åœ¨ä¸‰ç»´æ„ŸçŸ¥ä¸­çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤šè§†å›¾ç‰©ä½“æ£€æµ‹ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šNeRF-DetSï¼šåŸºäºè¿ç»­ NeRF è¡¨ç¤ºçš„é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œå¢å¼ºå¤šè§†å›¾ 3D ç›®æ ‡æ£€æµ‹</p></li><li><p>ä½œè€…ï¼šChi Huangã€Xinyang Liã€Shengchuan Zhangã€Liujuan Caoã€Rongrong Ji</p></li><li><p>å•ä½ï¼šå¦é—¨å¤§å­¦å¤šåª’ä½“å¯ä¿¡æ„ŸçŸ¥ä¸é«˜æ•ˆè®¡ç®—æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤</p></li><li><p>å…³é”®è¯ï¼š3D ç›®æ ‡æ£€æµ‹ã€NeRFã€å¤šè§†å›¾</p></li><li><p>é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.13921</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šNeRF-Det ç»Ÿä¸€äº†æ–°è§†å›¾åˆæˆå’Œ 3D æ„ŸçŸ¥ä»»åŠ¡ï¼Œè¡¨æ˜æ„ŸçŸ¥ä»»åŠ¡å¯ä»¥å—ç›Šäº NeRF ç­‰æ–°è§†å›¾åˆæˆæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å®¤å†…å¤šè§†å›¾ 3D ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ã€‚ä½¿ç”¨ NeRF çš„å‡ ä½• MLP æŒ‡å¯¼æ£€æµ‹å¤´çš„æ³¨æ„åŠ›åˆ°å…³é”®éƒ¨åˆ†ï¼Œå¹¶ç»“åˆæ–°è§†å›¾æ¸²æŸ“çš„è‡ªç›‘ç£æŸå¤±ï¼Œä¿ƒæˆäº†å®ç°çš„æ”¹è¿›ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•å’Œé—®é¢˜ï¼šä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ç¥ç»æ¸²æŸ“åœ¨ç©ºé—´ä¸­é€šè¿‡è¿ç»­è¡¨ç¤ºçš„æ˜¾ç€ä¼˜åŠ¿ï¼Œå¼•å…¥äº†æ–°é¢–çš„ 3D æ„ŸçŸ¥ç½‘ç»œç»“æ„ NeRF-DetSã€‚NeRF-DetS çš„å…³é”®ç»„ä»¶æ˜¯å¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œï¼Œä½¿é‡‡æ ·è¿‡ç¨‹è‡ªé€‚åº”åœ°ä»ç²—åˆ°ç²¾ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ä¸ªä¼˜è¶Šçš„å¤šè§†å›¾ä¿¡æ¯èåˆæ–¹æ³•ï¼Œç§°ä¸ºå¤šå¤´åŠ æƒèåˆã€‚è¿™ç§èåˆæ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†ä½¿ç”¨ç®—æœ¯å¹³å‡å€¼æ—¶ä¸¢å¤±å¤šè§†å›¾ä¿¡æ¯çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šNeRF-DetS åœ¨ ScanNetV2 æ•°æ®é›†ä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ NeRF-Detï¼Œåœ¨ mAP@.25 å’Œ mAP@.50 ä¸Šåˆ†åˆ«å®ç°äº† +5.02% å’Œ +5.92% çš„æå‡ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•çš„æ€§èƒ½å’Œå¯¹ç›®æ ‡çš„æ”¯æŒï¼šNeRF-DetS çš„æ€§èƒ½æ”¯æŒå…¶ç›®æ ‡ï¼Œå³é€šè¿‡è¿ç»­è¡¨ç¤ºå’Œè‡ªé€‚åº”é‡‡æ ·å¢å¼ºå¤šè§†å›¾ 3D ç›®æ ‡æ£€æµ‹ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šå¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œï¼šé€šè¿‡å¯¹åŸå§‹é‡‡æ ·ç‚¹è¿›è¡Œåç§»é¢„æµ‹ï¼Œå®ç°è‡ªé€‚åº”é‡‡æ ·ï¼Œå¼¥è¡¥åŸå§‹é‡‡æ ·ç‚¹ä¿¡æ¯çš„ç¼ºå¤±ï¼Œè·å–æ›´ä¸°å¯Œçš„ç©ºé—´ç‰¹å¾ä¿¡æ¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå¤šå¤´åŠ æƒèåˆï¼šå¯¹ä¸åŒè§†è§’çš„ç‰¹å¾è¿›è¡ŒåŠ æƒèåˆï¼Œé€šè¿‡å¤šå¤´æƒé‡åˆ†é…æœºåˆ¶ï¼Œçªå‡ºé‡è¦è§†è§’çš„ä¿¡æ¯ï¼ŒæŠ‘åˆ¶æ— å…³è§†è§’çš„å½±å“ï¼Œæé«˜èåˆç‰¹å¾çš„æœ‰æ•ˆæ€§ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šè®­ç»ƒç›®æ ‡ï¼šé‡‡ç”¨ä¸ NeRF-Det ç›¸åŒçš„æŸå¤±ç»“æ„ï¼ŒåŒ…æ‹¬ Bounding Box å›å½’æŸå¤±ã€åˆ†ç±»æŸå¤±ã€ä¸­å¿ƒç‚¹æŸå¤±å’Œæ–°è§†å›¾æ¸²æŸ“æŸå¤±ï¼Œä»¥ä¼˜åŒ–æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† NeRF-DetSï¼Œä»¥å¢å¼ºåŸºäºè¿ç»­ NeRF è¡¨ç¤ºçš„å¤šè§†å›¾å›¾åƒç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨ NeRF åˆ†æ”¯ä¸ºæ„ŸçŸ¥è¿‡ç¨‹å¸¦æ¥çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œï¼Œè¯¥ç½‘ç»œå……åˆ†åˆ©ç”¨äº†åŸºäº NeRF è¡¨ç¤ºçš„è¿ç»­æ€§çš„æ˜¾è‘—ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè®¤è¯†åˆ°ç©ºé—´ä¸­å¤šè§†å›¾ä¿¡æ¯èåˆçš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå¤´åŠ æƒèåˆã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æƒé‡æ¥è§£å†³åœ¨å­˜åœ¨å¤šä¸ªè§†è§’çš„æƒ…å†µä¸‹ç©ºé—´ä¸­çš„ç‰¹å®šè§†è§’å¯èƒ½è¢«é®æŒ¡çš„æƒ…å†µã€‚åœ¨ ScanNetV2 æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ£€æµ‹ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå¤šçº§é‡‡æ ·è‡ªé€‚åº”ç½‘ç»œã€å¤šå¤´åŠ æƒèåˆï¼›æ€§èƒ½ï¼šåœ¨ ScanNetV2 æ•°æ®é›†ä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ NeRF-Detï¼Œåœ¨ mAP@.25 å’Œ mAP@.50 ä¸Šåˆ†åˆ«å®ç°äº† +5.02% å’Œ +5.92% çš„æå‡ï¼›å·¥ä½œé‡ï¼šä¸ NeRF-Det ç›¸åŒçš„æŸå¤±ç»“æ„ï¼ŒåŒ…æ‹¬ Bounding Box å›å½’æŸå¤±ã€åˆ†ç±»æŸå¤±ã€ä¸­å¿ƒç‚¹æŸå¤±å’Œæ–°è§†å›¾æ¸²æŸ“æŸå¤±ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-239cc2f7c7a9838e9e872c8f4334e2d9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1b28030a36aae7836362d0f5da6d44d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-801202c40b51eebd7384f940b19468e9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-83f3650828e2486fc3a4b3751e57b1e2.jpg" align="middle"></details><h2 id="CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory"><a href="#CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory" class="headerlink" title="CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory"></a>CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory</h2><p><strong>Authors:Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, Jiming Chen</strong></p><p>Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction. However, NeRF heavily relies on precise camera poses. While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes. Existing methods struggle while tackling complex trajectories involving large rotations. To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input. In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure. Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs. Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories. We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories. Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2404.13896v2">PDF</a> </p><p><strong>Summary</strong><br>CT-NeRFæ˜¯ä¸€ç§å¢é‡å¼é‡å»ºä¼˜åŒ–ç®¡é“ï¼Œä»…ä½¿ç”¨RGBå›¾åƒå³å¯æ¢å¤ç›¸æœºå§¿æ€å’Œåœºæ™¯ç»“æ„ï¼Œé€‚ç”¨äºå…·æœ‰å¤æ‚è½¨è¿¹çš„åœºæ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CT-NeRF æå‡ºäº†ä¸€ç§å±€éƒ¨-å…¨å±€æŸè°ƒæ•´æ–¹æ³•ï¼Œä»¥è¿æ¥ç›¸é‚»å¸§ä¹‹é—´çš„ä½å§¿å›¾ï¼Œé€šè¿‡ä½å§¿ä¸€è‡´æ€§çº¦æŸåœºæ™¯ç»“æ„æ¥é¿å…é™·å…¥å±€éƒ¨æœ€å°å€¼ã€‚</li><li>CT-NeRF å°†ä½å§¿ä¸€è‡´æ€§å®ä¾‹åŒ–ä¸ºåŸºäºè¾“å…¥å›¾åƒå¯¹ä¹‹é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»çš„é‡æŠ•å½±å‡ ä½•å›¾åƒè·ç¦»çº¦æŸã€‚</li><li>é€šè¿‡å¢é‡é‡å»ºï¼ŒCT-NeRF èƒ½å¤Ÿæ¢å¤ç›¸æœºå§¿æ€å’Œåœºæ™¯ç»“æ„ï¼Œå¹¶èƒ½å¤Ÿå¤„ç†å…·æœ‰å¤æ‚è½¨è¿¹çš„åœºæ™¯ã€‚</li><li>CT-NeRF åœ¨ NeRFBuster å’Œ Free-Dataset è¿™ä¸¤ä¸ªå…·æœ‰å¤æ‚è½¨è¿¹çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CT-NeRFï¼šå¢é‡ä¼˜åŒ–ç¥ç»è¾å°„åœºå’Œä½å§¿ï¼Œå¤æ‚è½¨è¿¹ä¸‹çš„åº”ç”¨</p></li><li><p>Authors: Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao sun, Jiming chen</p></li><li><p>Affiliation: æµ™æ±Ÿå¤§å­¦</p></li><li><p>Keywords: Pose estimation, Implicit representation, Structure from motion, SLAM</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.13896, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨é«˜è´¨é‡3Dåœºæ™¯é‡å»ºä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼ŒNeRFä¸¥é‡ä¾èµ–äºç²¾ç¡®çš„ç›¸æœºä½å§¿ã€‚è™½ç„¶BARFç­‰è¿‘æœŸå·¥ä½œå·²ç»å¼•å…¥äº†NeRFä¸­çš„ç›¸æœºä½å§¿ä¼˜åŒ–ï¼Œä½†å…¶é€‚ç”¨æ€§ä»…é™äºç®€å•çš„è½¨è¿¹åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠå¤§æ—‹è½¬çš„å¤æ‚è½¨è¿¹æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šBARFç­‰æ–¹æ³•å°†ç›¸æœºä½å§¿ä¼˜åŒ–å¼•å…¥NeRFï¼Œä½†ä»…é™äºç®€å•è½¨è¿¹åœºæ™¯ï¼Œæ— æ³•å¤„ç†å¤æ‚è½¨è¿¹ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ¶‰åŠå¤§æ—‹è½¬çš„å¤æ‚è½¨è¿¹æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚</p><p>(3): æœ¬æ–‡æ–¹æ³•ï¼šä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†CT-NeRFï¼Œè¿™æ˜¯ä¸€ç§ä»…ä½¿ç”¨RGBå›¾åƒè€Œæ— éœ€ä½å§¿å’Œæ·±åº¦è¾“å…¥çš„å¢é‡é‡å»ºä¼˜åŒ–ç®¡é“ã€‚åœ¨è¯¥ç®¡é“ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§å±€éƒ¨-å…¨å±€æ†ç»‘è°ƒæ•´ï¼Œåœ¨è¿æ¥ç›¸é‚»å¸§çš„ä½å§¿å›¾ä¸‹ï¼Œä»¥å¼ºåˆ¶ä½å§¿ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œé€ƒç¦»ä»…ä¸åœºæ™¯ç»“æ„çš„ä½å§¿ä¸€è‡´æ€§é€ æˆçš„å±€éƒ¨æœ€å°å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½å§¿ä¹‹é—´çš„ä¸€è‡´æ€§å®ä¾‹åŒ–ä¸ºä»è¾“å…¥å›¾åƒå¯¹ä¹‹é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»äº§ç”Ÿçš„é‡æŠ•å½±å‡ ä½•å›¾åƒè·ç¦»çº¦æŸã€‚é€šè¿‡å¢é‡é‡å»ºï¼ŒCT-NeRFèƒ½å¤Ÿæ¢å¤ç›¸æœºä½å§¿å’Œåœºæ™¯ç»“æ„ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†å…·æœ‰å¤æ‚è½¨è¿¹çš„åœºæ™¯ã€‚</p><p>(4): æ€§èƒ½ï¼šæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰å¤æ‚è½¨è¿¹çš„çœŸå®ä¸–ç•Œæ•°æ®é›†NeRFBusterå’ŒFree-Datasetä¸Šè¯„ä¼°äº†CT-NeRFçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒCT-NeRFåœ¨æ–°çš„è§†å›¾åˆæˆå’Œä½å§¿ä¼°è®¡ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºå±€éƒ¨-å…¨å±€æ†ç»‘è°ƒæ•´ï¼Œåœ¨è¿æ¥ç›¸é‚»å¸§çš„ä½å§¿å›¾ä¸‹ï¼Œä»¥å¼ºåˆ¶ä½å§¿ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œä»è€Œé€ƒç¦»ä»…ä¸åœºæ™¯ç»“æ„çš„ä½å§¿ä¸€è‡´æ€§é€ æˆçš„å±€éƒ¨æœ€å°å€¼ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šå°†ä½å§¿ä¹‹é—´çš„ä¸€è‡´æ€§å®ä¾‹åŒ–ä¸ºä»è¾“å…¥å›¾åƒå¯¹ä¹‹é—´çš„åƒç´ çº§å¯¹åº”å…³ç³»äº§ç”Ÿçš„é‡æŠ•å½±å‡ ä½•å›¾åƒè·ç¦»çº¦æŸï¼›</p><p>ï¼ˆ3ï¼‰ï¼šé€šè¿‡å¢é‡é‡å»ºï¼ŒCT-NeRFèƒ½å¤Ÿæ¢å¤ç›¸æœºä½å§¿å’Œåœºæ™¯ç»“æ„ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†å…·æœ‰å¤æ‚è½¨è¿¹çš„åœºæ™¯ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº† CT-NeRFï¼Œä¸€ç§èƒ½å¤Ÿä»æ²¿å¤æ‚è½¨è¿¹æ•è·çš„å›¾åƒåºåˆ—ä¸­æ¢å¤ä½å§¿å’Œé‡å»ºåœºæ™¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥å¯¹åº”å…³ç³»å’Œé‡æŠ•å½±å‡ ä½•å›¾åƒè·ç¦»ï¼Œå¯¹ä¼˜åŒ–å›¾æ–½åŠ é¢å¤–çš„çº¦æŸï¼Œå®ç°é²æ£’ä¸”å‡†ç¡®çš„ä½å§¿ä¼°è®¡å’Œåœºæ™¯ç»“æ„é‡å»ºã€‚éšåï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬ç”¨äºä½å§¿æ¢å¤çš„å¢é‡å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–ã€è·Ÿè¸ªã€çª—å£ä¼˜åŒ–å’Œå…¨å±€ä¼˜åŒ–ã€‚é€šè¿‡æ¯”è¾ƒå’Œæ¶ˆèå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œå…¶å„ä¸ªç»„æˆéƒ¨åˆ†çš„å¿…è¦æ€§ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨å¤æ‚çš„ç›¸æœºè½¨è¿¹ä¸‹è¿›è¡Œè”åˆä½å§¿ä¼°è®¡å’Œé‡å»ºï¼Œä½†æˆ‘ä»¬åªæ¢ç´¢äº†ç®€å•çš„ä½å§¿å›¾ã€‚å¯¹äºéå¸¸é•¿çš„è½¨è¿¹ï¼Œéœ€è¦æ›´å¤æ‚çš„å›¾ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæ­£å¦‚è®ºæ–‡ä¸­æ‰€è®¨è®ºçš„ï¼Œå¤æ‚ç›¸æœºè½¨è¿¹éœ€è¦è¯„ä¼°æ•°æ®é›†ã€åè®®å’ŒæŒ‡æ ‡ï¼Œå½“å‰çš„è§†è§‰æŒ‡æ ‡æ— æ³•å……åˆ†åæ˜ é‡å»ºè´¨é‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†å±€éƒ¨-å…¨å±€æ†ç»‘è°ƒæ•´ï¼Œå°†ä½å§¿ä¹‹é—´çš„ä¸€è‡´æ€§å®ä¾‹åŒ–ä¸ºé‡æŠ•å½±å‡ ä½•å›¾åƒè·ç¦»çº¦æŸï¼Œå®ç°äº†é²æ£’ä¸”å‡†ç¡®çš„ä½å§¿ä¼°è®¡å’Œåœºæ™¯ç»“æ„é‡å»ºï¼›æ€§èƒ½ï¼šåœ¨å…·æœ‰å¤æ‚è½¨è¿¹çš„çœŸå®ä¸–ç•Œæ•°æ®é›† NeRFBuster å’Œ Free-Dataset ä¸Šè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ CT-NeRF åœ¨æ–°çš„è§†å›¾åˆæˆå’Œä½å§¿ä¼°è®¡ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼›å·¥ä½œé‡ï¼šæ–¹æ³•å®ç°è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦è¾ƒé«˜çš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-4e31cd388846d5e79eb8c6f1f5370705.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51c39516accf12f9bec3760a243d8ec4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f23b255b94f32edb903410a01a371e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-360fd8ee973080efc6f3769036860e2b.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRFâ€™s applications in the context of AD. Our survey is structured to categorize NeRFâ€™s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v1">PDF</a> </p><p><strong>Summary</strong></p><p>NeRFåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸç­‰åº”ç”¨ï¼Œæœ¬æ–‡å¯¹å…¶åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFèƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¦‚ç‰©ä½“æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œå®ä¾‹åˆ†å‰²ã€‚</li><li>NeRFèƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„3Dé‡å»ºä»»åŠ¡ï¼Œå¦‚åœºæ™¯é‡å»ºå’Œè½¦è¾†å»ºæ¨¡ã€‚</li><li>NeRFèƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„SLAMä»»åŠ¡ï¼Œå¦‚å®šä½å’Œå»ºå›¾ã€‚</li><li>NeRFèƒ½ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­çš„ä»¿çœŸä»»åŠ¡ï¼Œå¦‚åœºæ™¯ç”Ÿæˆå’Œä¼ æ„Ÿå™¨æ¨¡æ‹Ÿã€‚</li><li>NeRFçš„åº”ç”¨åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰å¹¿é˜”çš„å‰æ™¯ã€‚</li><li>NeRFåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚æ•ˆç‡ã€é²æ£’æ€§å’ŒçœŸå®æ„Ÿã€‚</li><li>æœ¬æ¬¡è°ƒæŸ¥ä¸ºç ”ç©¶äººå‘˜æä¾›äº†NeRFåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„å…¨é¢å‚è€ƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ï¼šç»¼è¿°</p></li><li><p>Authors: Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</p></li><li><p>Affiliation: ä¸­å›½ç§‘å­¦é™¢å¤§å­¦</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: https://arxiv.org/abs/2404.13816v1</p></li><li><p>Summary:</p><p>(1): ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„è§†å›¾åˆæˆæŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨ä½“æ¸²æŸ“å’Œéšå¼ç¥ç»åœºæ™¯è¡¨ç¤ºçš„èƒ½åŠ›æ¥æ­ç¤º 3D åœºæ™¯å‡ ä½•çš„å¤æ‚æ€§ã€‚å®ƒåœ¨ ECCV 2020 ä¸Šé¦–æ¬¡äº®ç›¸ï¼Œè¿…é€Ÿè¾¾åˆ°é¢†å…ˆçš„è§†è§‰è´¨é‡æ°´å¹³ï¼Œå¹¶æˆä¸ºä¼—å¤šåç»­ç ”ç©¶å·¥ä½œçš„çµæ„Ÿæ¥æºã€‚è¿‘å¹´æ¥ï¼Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨é«˜é€Ÿå…¬è·¯åœºæ™¯ä¸­å¾—åˆ°å¹¿æ³›éƒ¨ç½²ï¼Œä½†åŸå¸‚ç¯å¢ƒä¸­çš„éƒ¨ç½²ä»åœ¨è¿›è¡Œä¸¥æ ¼æµ‹è¯•ã€‚è¿™ç§æŠ€æœ¯æ¼”å˜å·²ç»ä»æœ€åˆä¾èµ–é«˜ç²¾åº¦åœ°å›¾æä¾›é™æ€åœºæ™¯ç†è§£è½¬å˜ä¸ºç°åœ¨é€šè¿‡é¸Ÿç°è§†è§‰å®æ—¶æ„ŸçŸ¥å±€éƒ¨ç¯å¢ƒã€‚åŒæ—¶ï¼Œå®ƒåœ¨åŠŸèƒ½ä¸Šå·²ç»ä» 2 çº§ï¼ˆL2ï¼‰å‘å±•èµ·æ¥ï¼Œå¹¶æ­£åœ¨åŠªåŠ›å®ç° 4 çº§ï¼ˆL4ï¼‰è‡ªåŠ¨é©¾é©¶ã€‚è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè¦æ±‚å¯¹å‘¨å›´ç¯å¢ƒæœ‰æ·±å…¥çš„äº†è§£ï¼ŒåŒ…æ‹¬é™æ€åœºæ™¯å’Œäº¤é€šå‚ä¸è€…ä¹‹é—´çš„åŠ¨æ€äº¤äº’ï¼Œè¿™æ˜¯æœ‰æ•ˆè§„åˆ’å’Œæ§åˆ¶çš„å…³é”®å‰æã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ï¼ŒNeRF å·²è¯æ˜å…¶æœ‰æ•ˆç†è§£å±€éƒ¨åœºæ™¯çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºè‡ªåŠ¨é©¾é©¶èƒ½åŠ›çš„æœ‰åŠ›å€™é€‰è€…ã€‚åœ¨è¿‡å»çš„ä¸¤å¹´ä¸­ï¼ŒNeRF æ¨¡å‹å·²åœ¨è‡ªåŠ¨é©¾é©¶çš„å„ä¸ªæ–¹é¢å¾—åˆ°åº”ç”¨ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥ã€3D é‡å»ºã€åŒæ—¶å®šä½å’Œåœ°å›¾æ„å»º (SLAM) å’Œä»¿çœŸï¼Œå¦‚å›¾ 1 æ‰€ç¤ºã€‚</p><p>(2): åœ¨æ„ŸçŸ¥é¢†åŸŸï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²æˆä¸ºä¸€ä¸ªæœ‰å‰é€”çš„ç«äº‰è€…ï¼Œæ¶µç›–äº†å¯¹è±¡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œå ç”¨é¢„æµ‹ç­‰ä¸€ç³»åˆ—å…³é”®ä»»åŠ¡ã€‚å…¶å—æ¬¢è¿ç¨‹åº¦çš„æ¿€å¢ä¸»è¦å½’åŠŸäºå…¶è·å–ç²¾ç¡®ä¸”ä¸€è‡´çš„å‡ ä½•ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶å¯ä»¥åˆ†ä¸ºä¸¤ç§ä¸»è¦èŒƒä¾‹ï¼ŒåŒºåˆ«åœ¨äº NeRF çš„åˆ©ç”¨ï¼šâ€œNeRF for dataâ€å’Œâ€œNeRF for modelâ€ã€‚å‰è€…æ¶‰åŠ NeRF çš„åˆå§‹è®­ç»ƒï¼Œç„¶åå°†å…¶ç”¨äºå¢å¼ºæ„ŸçŸ¥ä»»åŠ¡çš„è®­ç»ƒæ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåè€…é‡‡ç”¨ NeRF å’Œæ„ŸçŸ¥ç½‘ç»œçš„åä½œè®­ç»ƒç­–ç•¥ï¼Œä½¿æ„ŸçŸ¥ç½‘ç»œèƒ½å¤Ÿå­¦ä¹  NeRF æ•è·çš„å‡ ä½•ä¿¡æ¯ã€‚</p><p>(3): åœ¨ 3D é‡å»ºåº”ç”¨é¢†åŸŸï¼ŒNeRF å¯ä»¥æ ¹æ®åœºæ™¯ç†è§£çš„çº§åˆ«åˆ†ä¸ºä¸‰ç§ä¸»è¦æ–¹æ³•ï¼šåŠ¨æ€åœºæ™¯é‡å»ºã€è¡¨é¢é‡å»ºå’Œé€†å‘æ¸²æŸ“ã€‚åœ¨ç¬¬ä¸€ç±»ä¸­ï¼ŒåŠ¨æ€åœºæ™¯é‡å»ºä¸“æ³¨äºé‡å»ºå…·æœ‰å¯ç§»åŠ¨ä»£ç†çš„åŠ¨æ€åœºæ™¯ï¼Œä¸»è¦ä½¿ç”¨é¡ºåº 3D è¾¹ç•Œæ¡†æ³¨é‡Šå’Œç›¸æœºå‚æ•°ã€‚åœ¨ç¬¬äºŒç±»ä¸­ï¼Œè¡¨é¢é‡å»ºæ—¨åœ¨é‡å»ºåœºæ™¯çš„æ˜¾å¼ 3D è¡¨é¢ï¼Œä¾‹å¦‚ç½‘æ ¼ã€‚åœ¨ç¬¬ä¸‰ç±»ä¸­ï¼Œé€†å‘æ¸²æŸ“æ—¨åœ¨ä»é©¾é©¶åœºæ™¯çš„å›¾åƒä¸­åˆ†è§£å½¢çŠ¶ã€åç…§ç‡å’Œå¯è§æ€§ï¼Œä»¥å®ç°è¯¸å¦‚é‡æ–°ç…§æ˜ä¹‹ç±»çš„åº”ç”¨ã€‚</p><p>(4): è‡³äº SLAM åº”ç”¨ï¼ŒNeRF çš„åˆ©ç”¨å¯ä»¥åˆ†ä¸ºä¸‰ç§ä¸»è¦æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•éƒ½é’ˆå¯¹æ˜ å°„ã€å®šä½æˆ–ä¸¤è€…å…¼è€Œæœ‰ä¹‹ã€‚è‡³äºå®šä½ï¼ŒNeRF ç”¨äºåœ¨å½“å‰æ—¶é—´æˆ³æ‰§è¡Œå®æ—¶å›¾åƒæ¸²æŸ“ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–é‡æŠ•å½±è¯¯å·®æ¥ä¼°è®¡ SLAM ç³»ç»Ÿçš„ç²¾ç¡®å§¿æ€ã€‚è™½ç„¶ NeRF for mapping ä¸»è¦ä¸“æ³¨äºå¢å¼º SLAM ç³»ç»Ÿçš„æ˜ å°„èƒ½åŠ›ï¼Œè¿™é€šè¿‡åˆå¹¶ä½¿ç”¨ NeRF ç”Ÿæˆçš„æ·±åº¦å›¾æ¥å®ç°ï¼Œä»è€Œæé«˜äº†åœ°å›¾ç²¾åº¦ã€‚æ­¤å¤–ï¼ŒNeRF åœ¨å…¶ä»–ä¸€äº›ç ”ç©¶ä¸­ç”¨äºåŒæ—¶æé«˜ 3D åœ°å›¾çš„è´¨é‡å’Œæé«˜ SLAM ç³»ç»Ÿåœ¨å§¿æ€ä¼°è®¡ä¸­çš„ç²¾åº¦ã€‚è¿™äº›åˆ†ç±»å±•ç¤ºäº†å¦‚ä½•å°† NeRF ç­–ç•¥æ€§åœ°é›†æˆåˆ° SLAM ç³»ç»Ÿä¸­ä»¥æ»¡è¶³ç‰¹å®šéœ€æ±‚ï¼Œæ— è®ºè¿™äº›éœ€æ±‚æ¶‰åŠæ˜ å°„ã€å®šä½è¿˜æ˜¯ä¸¤è€…å…¼è€Œæœ‰ä¹‹çš„åŠŸèƒ½ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸€äº›ç°æœ‰çš„åŸºäº NeRF çš„ SLAM æ–¹æ³•æ˜¯ä¸ºå®¤å†…åœºæ™¯è®¾è®¡çš„ï¼Œä½†ç”±äºè¯¥æŠ€æœ¯ç±»ä¼¼äºè‡ªåŠ¨é©¾é©¶çš„å¤§å‹å®¤å¤–ç¯å¢ƒï¼Œå› æ­¤æœ¬æ–‡ä¹Ÿå¯¹å®¤å†…æ–¹æ³•è¿›è¡Œäº†ç»¼è¿°ã€‚</p></li></ol><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬ç»¼è¿°å·¥ä½œå¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†ç¥ç»è¾å°„åœºçš„åŸºæœ¬åŸç†å’ŒèƒŒæ™¯ï¼Œç„¶åæ·±å…¥åˆ†æäº†ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶å„ä¸ªé¢†åŸŸçš„åº”ç”¨ï¼Œåˆ†ä¸ºæ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸã€‚åœ¨æ€»ç»“äº†ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„æœ€æ–°è¿›å±•çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡æ€»ç»“äº†ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„ä¼˜åŠ¿å’Œä¸è¶³è¿›è¡Œäº†åˆ†æï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p><p>æ€§èƒ½ï¼šæœ¬æ–‡å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡æ€»ç»“äº†ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„ä¼˜åŠ¿å’Œä¸è¶³è¿›è¡Œäº†åˆ†æï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†æ„ŸçŸ¥ã€ä¸‰ç»´é‡å»ºã€SLAMå’Œä»¿çœŸç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡æ€»ç»“äº†ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹ç¥ç»è¾å°„åœºåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸåº”ç”¨çš„ä¼˜åŠ¿å’Œä¸è¶³è¿›è¡Œäº†åˆ†æï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f00a4edaa4deada8fbf20792a3bdb4f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://pica.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/</id>
    <published>2024-04-25T13:22:56.000Z</published>
    <updated>2024-04-25T13:22:56.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-25-æ›´æ–°"><a href="#2024-04-25-æ›´æ–°" class="headerlink" title="2024-04-25 æ›´æ–°"></a>2024-04-25 æ›´æ–°</h1><h2 id="FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent"><a href="#FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent" class="headerlink" title="FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent"></a>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</h2><p><strong>Authors:Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann</strong></p><p>This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM). </p><p><a href="http://arxiv.org/abs/2404.15259v1">PDF</a> Project website: <a href="https://cameronosmith.github.io/flowmap/">https://cameronosmith.github.io/flowmap/</a></p><p><strong>Summary</strong><br>FlowMapä½¿ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ ¹æ®å…‰æµæ¨ç®—ç›¸æœºä½å§¿å¹¶æ¸²æŸ“360åº¦æ–°é¢–è§†è§’ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>FlowMap æ˜¯ä¸€ç§ç«¯åˆ°ç«¯å¯å¾®æ–¹æ³•ï¼Œç”¨äºæ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œè§†é¢‘åºåˆ—çš„é€å¸§ç¨ å¯†æ·±åº¦ã€‚</li><li>FlowMap ä½¿ç”¨ç®€å•æœ€å°äºŒä¹˜ç›®æ ‡å‡½æ•°çš„é€è§†é¢‘æ¢¯åº¦ä¸‹é™æœ€å°åŒ–ï¼Œè¯¥ç›®æ ‡å‡½æ•°æ¯”è¾ƒç”±æ·±åº¦ã€å†…å‚å’Œä½å§¿å¼•èµ·çš„å…‰æµå’Œé€šè¿‡ç°æˆå…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ã€‚</li><li>Point tracks ç”¨äºé¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚</li><li>å¼•å…¥äº†æ·±åº¦çš„å¯å¾®é‡æ–°å‚æ•°åŒ–ã€å†…å‚å’Œä½å§¿ï¼Œé€‚ç”¨äºä¸€é˜¶ä¼˜åŒ–ã€‚</li><li>FlowMap æ¢å¤çš„ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦èƒ½å¤Ÿä½¿ç”¨é«˜æ–¯æº…å°„åœ¨ 360 åº¦è½¨è¿¹ä¸Šè¿›è¡Œé€¼çœŸçš„æ–°è§†å›¾åˆæˆã€‚</li><li>FlowMap ä¸ä»…æ˜æ˜¾ä¼˜äºå…ˆå‰çš„åŸºäºæ¢¯åº¦ä¸‹é™çš„æŸè°ƒæ•´æ–¹æ³•ï¼Œè€Œä¸”ä»¤äººæƒŠè®¶åœ°ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP åœ¨ 360 åº¦æ–°è§†å›¾åˆæˆçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“ï¼ˆå³ä½¿ FlowMap æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦ä¸‹é™çš„çº¯å¯å¾®æ–¹æ³•ï¼Œå¹¶å®Œå…¨æœ‰åˆ«äºä¼ ç»Ÿçš„ SfMï¼‰ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>è®ºæ–‡æ ‡é¢˜ï¼šFlowMapï¼šé«˜è´¨é‡ç›¸æœºä½å§¿ã€å†…å‚å’Œè¡¥å……ææ–™</p></li><li><p>ä½œè€…ï¼šAlex Yu, Vladlen Koltun</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±ä¼Ÿè¾¾</p></li><li><p>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€ç»“æ„ä»è¿åŠ¨ã€ç¥ç»æ¸²æŸ“ã€ç¥ç»è¾å°„åœº</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.06641 æˆ– https://github.com/facebookresearch/FlowMap</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»æ¸²æŸ“å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰æŠ€æœ¯éœ€è¦å‡†ç¡®çš„ç›¸æœºä½å§¿å’Œåœºæ™¯å‡ ä½•ä¿¡æ¯ã€‚ä¼ ç»Ÿçš„ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†æ”¶æ•›é€Ÿåº¦æ…¢ã€å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•åœ¨å¤„ç†å¤§ä½ç§»å’Œé®æŒ¡æ—¶å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦æ‰‹å·¥ç‰¹å¾åŒ¹é…ï¼Œè¿™æ—¢è€—æ—¶åˆä¸å¯é ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–å…‰æµè¯±å¯¼çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿ä¸é€šè¿‡å…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ä¹‹é—´çš„å‡æ–¹è¯¯å·®æ¥æ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ã€‚FlowMap é‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œä½¿å…¶é€‚åˆä¸€é˜¶ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šFlowMap åœ¨ 360 åº¦è½¨è¿¹ä¸Šä½¿ç”¨é«˜æ–¯æº…å°„è¿›è¡Œé€¼çœŸçš„æ–°è§†è§’åˆæˆï¼Œå…¶ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦æ˜æ˜¾ä¼˜äºå…ˆå‰çš„æ¢¯åº¦ä¸‹é™ SfM æ–¹æ³•ã€‚å®ƒç”šè‡³ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ç›¸å½“ï¼Œå°½ç®¡ FlowMap æ˜¯çº¯æ¢¯åº¦ä¸‹é™çš„ã€å®Œå…¨å¯å¾®åˆ†çš„ï¼Œå¹¶ä¸”ä¸ä¼ ç»Ÿçš„ SfM æ–¹æ³•å®Œå…¨ä¸åŒã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–å…‰æµè¯±å¯¼çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿ä¸é€šè¿‡å…‰æµå’Œç‚¹è·Ÿè¸ªè·å¾—çš„å¯¹åº”å…³ç³»ä¹‹é—´çš„å‡æ–¹è¯¯å·®æ¥æ±‚è§£ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šFlowMap é‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œä½¿å…¶é€‚åˆä¸€é˜¶ä¼˜åŒ–ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæ­¤å¤–ï¼Œå®ƒåˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡º FlowMapï¼Œä¸€ç§ç®€å•ã€é²æ£’ä¸”å¯æ‰©å±•çš„ä¸€é˜¶æ–¹æ³•ï¼Œç”¨äºä»è§†é¢‘ä¸­ä¼°è®¡ç›¸æœºå‚æ•°ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºç°æœ‰çš„åŸºäºæ¢¯åº¦ä¸‹é™çš„ç›¸æœºå‚æ•°ä¼°è®¡æ–¹æ³•ã€‚FlowMap çš„æ·±åº¦å’Œç›¸æœºå‚æ•°é€šè¿‡é«˜æ–¯æº…å°„æ³•è¿›è¡Œåç»­é‡å»ºï¼Œå…¶è´¨é‡ä¸ COLMAP ç›¸å½“ã€‚FlowMap ä½¿ç”¨ PyTorch ç¼–å†™ï¼ŒçŸ­åºåˆ—è¿è¡Œæ—¶é—´ä¸º 3 åˆ†é’Ÿï¼Œé•¿åºåˆ—è¿è¡Œæ—¶é—´ä¸º 20 åˆ†é’Ÿï¼Œæˆ‘ä»¬é¢„è®¡ååŒå·¥ç¨‹å·¥ä½œå¯ä»¥å°† FlowMap åŠ é€Ÿä¸€ä¸ªæ•°é‡çº§ã€‚ä¹Ÿè®¸æœ€ä»¤äººå…´å¥‹çš„æ˜¯ï¼ŒFlowMap å¯¹æ¯å¸§æ·±åº¦ä¼°è®¡æ˜¯å®Œå…¨å¯å¾®åˆ†çš„ã€‚å› æ­¤ï¼ŒFlowMap å¯ä»¥ä½œä¸ºæ–°ä¸€ä»£è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡å™¨ã€åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šè§†å›¾å‡ ä½•æ–¹æ³•ä»¥åŠå¯æ³›åŒ–æ–°è§†å›¾åˆæˆæ–¹æ³•çš„æ„å»ºæ¨¡å— [7, 18, 60, 66, 69, 77]ï¼Œä»è€Œè§£é”å¯¹æœªæ‘†æ”¾è§†é¢‘çš„äº’è”ç½‘è§„æ¨¡æ•°æ®é›†çš„è®­ç»ƒã€‚è‡´è°¢ã€‚è¿™é¡¹å·¥ä½œå¾—åˆ°äº†å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š 2211259 å·èµ æ¬¾ã€æ–°åŠ å¡ DSTA ä¸‹ DST00OECI20300823ï¼ˆè§†è§‰çš„æ–°è¡¨ç¤ºå’Œç”¨äºæ ‡ç­¾é«˜æ•ˆè§†è§‰çš„è‡ªç›‘ç£å­¦ä¹ ï¼‰ã€æƒ…æŠ¥é«˜çº§ç ”ç©¶é¡¹ç›®æ´»åŠ¨ (IARPA) é€šè¿‡ 140D0423C0075 ä¸‹çš„å†…æ”¿éƒ¨/å†…åŠ¡å•†ä¸šä¸­å¿ƒ (DOI/IBC)ã€äºšé©¬é€Šç§‘å­¦ä¸­å¿ƒå’Œ IBM çš„éƒ¨åˆ†æ”¯æŒã€‚ä¸°ç”°ç ”ç©¶é™¢ä¹Ÿéƒ¨åˆ†æ”¯æŒäº†è¿™é¡¹å·¥ä½œã€‚æ­¤å¤„åŒ…å«çš„è§‚ç‚¹å’Œç»“è®ºåæ˜ äº†å…¶ä½œè€…çš„è§‚ç‚¹å’Œç»“è®ºï¼Œä¸ä»£è¡¨ä»»ä½•å…¶ä»–å®ä½“ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šFlowMap æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®æ–¹æ³•æ¥è§£å†³ç›¸æœºä½å§¿ã€å†…å‚å’Œç¨ å¯†æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œé‡‡ç”¨å¯å¾®åˆ†çš„æ·±åº¦ã€å†…å‚å’Œä½å§¿é‡æ–°å‚æ•°åŒ–ï¼Œå¹¶åˆ©ç”¨ç‚¹è¿¹é¼“åŠ±é•¿æœŸå‡ ä½•ä¸€è‡´æ€§ã€‚æ€§èƒ½ï¼šFlowMap åœ¨ç›¸æœºå‚æ•°å’Œç¨ å¯†æ·±åº¦ä¼°è®¡æ–¹é¢ä¼˜äºç°æœ‰çš„æ¢¯åº¦ä¸‹é™ SfM æ–¹æ³•ï¼Œå…¶æ–°è§†è§’åˆæˆè´¨é‡ä¸æœ€å…ˆè¿›çš„ SfM æ–¹æ³• COLMAP ç›¸å½“ã€‚å·¥ä½œé‡ï¼šFlowMap æ˜¯ä¸€ç§ä¸€é˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ PyTorch ä¸­å®ç°ï¼ŒçŸ­åºåˆ—è¿è¡Œæ—¶é—´ä¸º 3 åˆ†é’Ÿï¼Œé•¿åºåˆ—è¿è¡Œæ—¶é—´ä¸º 20 åˆ†é’Ÿï¼Œå…·æœ‰å¯æ‰©å±•æ€§å’Œå·¥ç¨‹åŠ é€Ÿæ½œåŠ›ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a1dea0f5ce347645c2a4c11098b0ba50.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25a5764437b9221ae10ad73aa8b84fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43352f81d6eb7aada886230a057402b5.jpg" align="middle"></details><h2 id="CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding"><a href="#CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding" class="headerlink" title="CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding"></a>CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</strong></p><p>The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (&gt;100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method. </p><p><a href="http://arxiv.org/abs/2404.14249v1">PDF</a> <a href="https://github.com/gbliao/CLIP-GS">https://github.com/gbliao/CLIP-GS</a></p><p><strong>Summary</strong><br>CLIP-GS å°†è¯­ä¹‰ä¿¡æ¯èå…¥é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„ 3D åœºæ™¯ç†è§£ï¼Œåœ¨ä¸ä½¿ç”¨å¸¦æ³¨é‡Šçš„è¯­ä¹‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>CLIP-GS å°†è¯­ä¹‰ä¿¡æ¯ä» CLIP é›†æˆåˆ°é«˜æ–¯æ–‘ç‚¹æ¸²æŸ“ä¸­ï¼Œå®ç°å¯¹ 3D åœºæ™¯çš„è¯­ä¹‰ç†è§£ã€‚</li><li>è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ (SAC) æ–¹æ³•å­¦ä¹ ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°é«˜æ•ˆæ¸²æŸ“ã€‚</li><li>3D ä¸€è‡´è‡ªè®­ç»ƒ (3DCS) ç­–ç•¥è§£å†³ç”±è§†å›¾ä¸ä¸€è‡´çš„ 2D CLIP è¯­ä¹‰ç›‘ç£é€ æˆçš„è¯­ä¹‰æ­§ä¹‰ã€‚</li><li>3DCS åˆ©ç”¨ç»è¿‡å¾®è°ƒçš„ 3D é«˜æ–¯æ¨¡å‹é¢„æµ‹çš„ä¼ªæ ‡ç­¾ï¼ŒåŠ å¼ºè·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸã€‚</li><li>CLIP-GS åœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šåˆ†åˆ«æé«˜ mIoU æŒ‡æ ‡ 17.29% å’Œ 20.81%ï¼Œè¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li><li>CLIP-GS å³ä½¿åœ¨ç¨€ç–è¾“å…¥æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</li><li>CLIP-GS å®æ—¶æ¸²æŸ“é€Ÿåº¦å¿«ï¼Œå¯ç”¨äºäº¤äº’å¼ 3D åœºæ™¯æ¢ç´¢å’Œç¼–è¾‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CLIP-GS: CLIP-å¼•å¯¼çš„é«˜æ–¯æ³¼æº…ï¼Œç”¨äºå®æ—¶ä¸”è§†å›¾ä¸€è‡´çš„ä¸‰ç»´è¯­ä¹‰ç†è§£</p></li><li><p>Authors: Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦</p></li><li><p>Keywords: 3D é«˜æ–¯æ³¼æº… Â· å®æ—¶ Â· è§†å›¾ä¸€è‡´ Â· ä¸‰ç»´åœºæ™¯è¯­ä¹‰ç†è§£ Â· ä¸‰ç»´åœºæ™¯é‡å»º Â· ç¨€ç–è§†å›¾</p></li><li><p>Urls: https://arxiv.org/abs/2404.14249 , Github: None</p></li><li><p>Summary: </p><pre><code>            (1): è¿‘æœŸæå‡ºçš„ä¸‰ç»´é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´åœºæ™¯ä¸­å±•ç°äº†é«˜è´¨é‡ä¸”å®æ—¶çš„å…¨æ–°è§†å›¾åˆæˆã€‚ç›®å‰å®ƒä¸»è¦å…³æ³¨äºå‡ ä½•å’Œå¤–è§‚å»ºæ¨¡ï¼Œè€Œç¼ºå°‘å¯¹åœºæ™¯çš„è¯­ä¹‰ç†è§£ã€‚            (2): ç°æœ‰æ–¹æ³•ä¸»è¦æœ‰ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰å’Œä¸‰ç»´é«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰ã€‚å‰è€…åœ¨æ¸²æŸ“åŒ…å«å‡ ä½•å’Œå¤–è§‚ç»†èŠ‚çš„æ–°è§†è§’æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹å¯¹ä¸‰ç»´åœºæ™¯çš„å…¨é¢è¯­ä¹‰ç†è§£ï¼›åè€…åˆ™ä¸»è¦å…³æ³¨å‡ ä½•å’Œå¤–è§‚å»ºæ¨¡ï¼Œè€Œå¿½ç•¥äº†è¯­ä¹‰ä¿¡æ¯ã€‚            (3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CLIP-GS çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ¥è‡ªå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒ (CLIP) çš„è¯­ä¹‰ä¿¡æ¯æ•´åˆåˆ°é«˜æ–¯æ³¼æº…ä¸­ï¼Œä»è€Œåœ¨æ²¡æœ‰æ³¨é‡Šè¯­ä¹‰æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°ç†è§£ä¸‰ç»´ç¯å¢ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ (SAC) æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯¹è±¡å†…å›ºæœ‰çš„ç»Ÿä¸€è¯­ä¹‰æ¥å­¦ä¹ é«˜æ–¯æ³¼æº…çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»è€Œå®ç°é«˜æ•ˆæ¸²æŸ“ï¼ˆ&gt;100 FPSï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒ (3DCS) ç­–ç•¥ï¼Œåˆ©ç”¨ä¸‰ç»´æ¨¡å‹äº§ç”Ÿçš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚3DCS é€šè¿‡åˆ©ç”¨ä»è®­ç»ƒå¥½çš„ä¸‰ç»´é«˜æ–¯æ¨¡å‹ä¸­è·å¾—çš„ç»è¿‡ä¼˜åŒ–ä¸”è‡ªæˆ‘é¢„æµ‹çš„ä¼ªæ ‡ç­¾æ¥æ–½åŠ è·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸï¼Œä»è€Œå¢å¼ºç²¾ç¡®ä¸”è§†å›¾ä¸€è‡´çš„åˆ†å‰²ç»“æœã€‚            (4): åœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ mIoU æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 17.29% å’Œ 20.81%ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨è¾“å…¥æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰ï¼šåˆ©ç”¨å¯¹è±¡å†…å›ºæœ‰çš„ç»Ÿä¸€è¯­ä¹‰æ¥å­¦ä¹ é«˜æ–¯æ³¼æº…çš„ç´§å‡‘ä¸”æœ‰æ•ˆçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°é«˜æ•ˆæ¸²æŸ“ã€‚            (2):3Dè¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰ï¼šåˆ©ç”¨ä¸‰ç»´æ¨¡å‹äº§ç”Ÿçš„å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œæ–½åŠ è·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸï¼Œå¢å¼ºç²¾ç¡®ä¸”è§†å›¾ä¸€è‡´çš„åˆ†å‰²ç»“æœã€‚            (3):æ•´ä½“è®­ç»ƒè¿‡ç¨‹ï¼šäº¤æ›¿ä¼˜åŒ–3Dé«˜æ–¯æ³¼æº…å’Œè¯­ä¹‰è¡¨ç¤ºï¼ŒåŒæ—¶åˆ©ç”¨3DCSæ–½åŠ è§†å›¾ä¸€è‡´æ€§çº¦æŸã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº† CLIP-GSï¼Œä¸€ç§åˆ©ç”¨é«˜æ–¯æ³¼æº…å®ç°ä¸‰ç»´åœºæ™¯å®æ—¶ä¸”ç²¾ç¡®è¯­ä¹‰ç†è§£çš„æ–°æ–¹æ³•ã€‚åœ¨ CLIP-GS ä¸­ï¼Œè¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰å°†ç´§å‡‘çš„è¯­ä¹‰ä¿¡æ¯é™„åŠ åˆ°ä¸‰ç»´é«˜æ–¯ä½“ä¸­ä»¥é«˜æ•ˆåœ°è¡¨ç¤ºä¸‰ç»´è¯­ä¹‰ï¼Œç¡®ä¿é«˜æ•ˆæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæå‡ºçš„ä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰å¢å¼ºäº†ä¸åŒè§†å›¾ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿäº†å‡†ç¡®çš„ä¸‰ç»´åˆ†å‰²ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äº SOTA æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå³ä½¿è¾“å…¥æ•°æ®ç¨€ç–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ä¸‰ç»´è¯­ä¹‰å­¦ä¹ æ–¹æ³•çš„é²æ£’æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§å°† CLIP è¯­ä¹‰ä¿¡æ¯æ•´åˆåˆ°é«˜æ–¯æ³¼æº…ä¸­çš„æ–¹æ³•ï¼Œå®ç°äº†ä¸‰ç»´åœºæ™¯çš„å®æ—¶è¯­ä¹‰ç†è§£ã€‚åˆ›æ–°æ€§åœ°æå‡ºäº†è¯­ä¹‰å±æ€§ç´§å‡‘æ€§ï¼ˆSACï¼‰å’Œä¸‰ç»´è¿è´¯è‡ªè®­ç»ƒï¼ˆ3DCSï¼‰ä¸¤ç§æŠ€æœ¯ï¼Œåˆ†åˆ«ç”¨äºé«˜æ•ˆè¯­ä¹‰è¡¨ç¤ºå’Œè·¨è§†å›¾è¯­ä¹‰ä¸€è‡´æ€§å¢å¼ºã€‚</p><p>æ€§èƒ½ï¼šåœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ mIoU æŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 17.29% å’Œ 20.81%ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ã€‚å³ä½¿åœ¨è¾“å…¥æ•°æ®ç¨€ç–çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶é²æ£’æ€§ã€‚</p><p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦é¢„è®­ç»ƒ CLIP æ¨¡å‹å’Œä¸‰ç»´é«˜æ–¯æ³¼æº…æ¨¡å‹ï¼Œè®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d36db5fceba666ce511b0cf595bc769d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0a5ca926d7e6577c4c1a0e8076537a17.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef11b21fc83f3602f91a29eea9ff097e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54108038b1e285d6be885cd6288e500c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary:</strong><br>é«˜æ–¯ä½“ç´ æ¸²æŸ“æ³•èµ‹äºˆ3Dé«˜æ–¯ä½“ç´ æ˜¾å¼è¡¨ç¤ºç‰¹æ€§ï¼Œå¯å®ç°ç›´è§‚çš„é¢éƒ¨åŠ¨ä½œæ§åˆ¶ï¼Œå¤§å¹…æå‡éŸ³é¢‘é©±åŠ¨è™šæ‹ŸåŒ–èº«åˆæˆæ•ˆæœã€‚</p><p><strong>Key Takeaways:</strong></p><ul><li>åŸºäº3Dé«˜æ–¯ä½“ç´ çš„éŸ³é¢‘é©±åŠ¨è™šæ‹ŸåŒ–èº«åˆæˆæ–¹æ³•ã€‚</li><li>é¢éƒ¨è¿åŠ¨é€šè¿‡å°†é«˜æ–¯ä½“ç´ ç»‘å®šåˆ°3Dé¢éƒ¨æ¨¡å‹å®ç°ç›´è§‚æ§åˆ¶ã€‚</li><li>è¯´è¯äººç‰¹å®šè¿åŠ¨ç¿»è¯‘å™¨å®ç°é’ˆå¯¹ç‰¹å®šè¯´è¯äººçš„ç²¾ç¡®å”‡éƒ¨è¿åŠ¨ã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ä»¥å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºã€‚</li><li>å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­éŸ³åˆæˆæ–¹é¢ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li><li>æ¸²æŸ“é€Ÿåº¦è¾¾åˆ°130 FPSï¼Œè¿œè¶…å®æ—¶æ¸²æŸ“æ€§èƒ½çš„é˜ˆå€¼ã€‚</li><li>å…·æœ‰åœ¨å…¶ä»–ç¡¬ä»¶å¹³å°ä¸Šéƒ¨ç½²çš„æ½œåŠ›ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker: Speaker-specific Talking Head Synthesis (åŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆ)</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: Alibaba Group (é˜¿é‡Œå·´å·´é›†å›¢)</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific motion, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.14037.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): Recent audio-driven talking head synthesis methods based on Neural Radiance Fields (NeRF) have achieved impressive results, but suffer from inadequate pose and expression control due to NeRF's implicit representation, leading to unsynchronized or unnatural lip movements and visual jitter and artifacts.</p><p>(2): Past methods: NeRF-based audio-driven talking head synthesis methods. Problems: Inadequate pose and expression control, resulting in unsynchronized or unnatural lip movements and visual jitter and artifacts. Well motivated: Yes, as it addresses the limitations of existing methods.</p><p>(3): GaussianTalker: A novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. It consists of two modules: Speaker-specific Motion Translator and Dynamic Gaussian Renderer. The former achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. The latter introduces Speaker-specific BlendShapes to control the facial expressions and generates high-quality talking head videos with precise lip movements.</p><p>(4): On the task of audio-driven talking head synthesis, GaussianTalker achieves state-of-the-art results. It can generate high-quality talking head videos with precise lip movements and natural facial expressions. The performance supports the goals of the paper, which are to address the limitations of existing methods and achieve more realistic and expressive talking head synthesis.</p></li><li><p>æ–¹æ³•ï¼š</p><p>(1)ï¼šåŸºäº3Dé«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆï¼›</p><p>(2)ï¼šæå‡ºSpeaker-specific Motion Translatorå’ŒDynamic Gaussian Rendererä¸¤ä¸ªæ¨¡å—ï¼›</p><p>(3)ï¼šSpeaker-specific Motion Translatoré€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆå®ç°ç‰¹å®šäºç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œï¼›</p><p>(4)ï¼šDynamic Gaussian Rendererå¼•å…¥è¯´è¯äººç‰¹å®šBlendShapesæ¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ï¼Œå¹¶ç”Ÿæˆå…·æœ‰ç²¾ç¡®å”‡éƒ¨åŠ¨ä½œçš„é«˜è´¨é‡è¯´è¯å¤´è§†é¢‘ï¼›</p><p>.......</p></li></ol><p><strong>ç»“è®º</strong></p><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡º GaussianTalkerï¼Œä¸€ç§é€šè¿‡ 3D é«˜æ–¯ç‚¹äº‘é›†æˆ FLAME æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´åˆæˆæ–°æ¡†æ¶ã€‚GaussianTalker å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯è€…å…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3D ç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚Speaker-specific FLAME Translator é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥æ¥å®ç°åŒæ­¥å’Œè‡ªç„¶çš„å”‡éƒ¨åŠ¨ä½œï¼Œè€Œ Dynamic Gaussian Renderer é€šè¿‡æ½œåœ¨å§¿åŠ¿ç»†åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šå’Œé€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalker åœ¨è¯´è¯å¤´åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜æ˜¾è¶…è¿‡å…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´æµç•…ã€æ›´é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡å½¢åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåŸºäº 3D é«˜æ–¯ç‚¹äº‘çš„è¯´è¯äººç‰¹å®šè¯´è¯å¤´åˆæˆï¼›Speaker-specific Motion Translator å’Œ Dynamic Gaussian Renderer ä¸¤ä¸ªæ¨¡å—ï¼›æ€§èƒ½ï¼šåœ¨è¯´è¯å¤´åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼›å·¥ä½œé‡ï¼š.......</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pica.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos"><a href="#Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos" class="headerlink" title="Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos"></a>Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos</h2><p><strong>Authors:Isabella Liu, Hao Su, Xiaolong Wang</strong></p><p>Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a> </p><p><a href="http://arxiv.org/abs/2404.12379v2">PDF</a> Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a></p><p><strong>Summary</strong><br>å•ç›®è§†é¢‘é‡å»ºé«˜ä¿çœŸåŠ¨æ€ç½‘æ ¼çš„æ¡†æ¶</p><p><strong>Key Takeaways</strong></p><ul><li>åŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰é€šè¿‡å•ç›®è§†é¢‘é‡å»ºå‡ºé«˜ä¿çœŸæ—¶åºä¸€è‡´çš„ç½‘æ ¼ã€‚</li><li>åˆ©ç”¨ 3D é«˜æ–¯ç‚¹äº‘æ„æˆå…·æœ‰æ—¶åºä¸€è‡´æ€§çš„ç½‘æ ¼åºåˆ—ã€‚</li><li>é«˜æ–¯ç‚¹äº‘æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œå¹¶å®ç°ç½‘æ ¼é¡¶ç‚¹çš„æ—¶åºè·Ÿè¸ªï¼Œå¯ç”¨äºåŠ¨æ€å¯¹è±¡çº¹ç†ç¼–è¾‘ã€‚</li><li>é«˜æ–¯ç½‘æ ¼é”šå®šå¯ä½¿é«˜æ–¯åˆ†å¸ƒå‡åŒ€ï¼Œé€šè¿‡ç½‘æ ¼å¼•å¯¼å˜å½¢é«˜æ–¯çš„é«˜å¯†åº¦åŒ–å’Œå‰ªæï¼Œæå‡ç½‘æ ¼é‡å»ºè´¨é‡ã€‚</li><li>é€šè¿‡è§„èŒƒç©ºé—´å’Œå˜å½¢ç©ºé—´çš„å¾ªç¯ä¸€è‡´æ€§å˜å½¢ï¼Œå°†é”šå®šçš„é«˜æ–¯æŠ•å°„å›è§„èŒƒç©ºé—´ï¼Œå¹¶åœ¨æ‰€æœ‰æ—¶é—´å¸§ä¼˜åŒ–é«˜æ–¯ã€‚</li><li>åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¯„ä¼°åï¼ŒDG-Mesh åœ¨ç½‘æ ¼é‡å»ºå’Œæ¸²æŸ“æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: åŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼šå•ç›®è§†é¢‘ä¸­ä¸€è‡´çš„ç½‘æ ¼é‡å»º</p></li><li><p>Authors: Isabella Liu, Hao Su, Xiaolong Wang</p></li><li><p>Affiliation: åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡</p></li><li><p>Keywords: 3D Reconstruction, Monocular Video, Dynamic Mesh, Gaussian Splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12379, Github: None</p></li><li><p>Summary:</p><p>(1): ç°ä»£ 3D å¼•æ“å’Œå›¾å½¢ç®¡é“éœ€è¦ç½‘æ ¼ä½œä¸ºä¸€ç§å†…å­˜é«˜æ•ˆçš„è¡¨ç¤ºï¼Œå®ƒå…è®¸é«˜æ•ˆæ¸²æŸ“ã€å‡ ä½•å¤„ç†ã€çº¹ç†ç¼–è¾‘å’Œè®¸å¤šå…¶ä»–ä¸‹æ¸¸æ“ä½œã€‚ç„¶è€Œï¼Œä»å•ç›®è§†è§‰è§‚å¯Ÿä¸­è·å¾—ç»“æ„å’Œç»†èŠ‚æ–¹é¢çš„é«˜è´¨é‡ç½‘æ ¼ä»ç„¶éå¸¸å›°éš¾ã€‚å¯¹äºåŠ¨æ€åœºæ™¯å’Œå¯¹è±¡ï¼Œè¿™ä¸ªé—®é¢˜å˜å¾—æ›´å…·æŒ‘æˆ˜æ€§ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹æ„å»ºç½‘æ ¼åºåˆ—ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚</p><p>(4): åœ¨ D-NeRF æ•°æ®é›†å’Œ DG-Mesh æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºï¼›</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç½‘æ ¼æ„å»ºæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºæ¥æ„å»ºç½‘æ ¼åºåˆ—ï¼Œä»¥åŠä¸€ä¸ªç½‘æ ¼é”šå®šæ¨¡å—ï¼Œè¯¥æ¨¡å—å°†ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼é”šå®šåˆ°ä¸–ç•Œåæ ‡ç³»ä¸­ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šç½‘æ ¼æ„å»ºæ¨¡å—åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºæ¥è¡¨ç¤ºåœºæ™¯ä¸­çš„å‡ ä½•å½¢çŠ¶ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ¥ä¼°è®¡ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šç½‘æ ¼é”šå®šæ¨¡å—åˆ©ç”¨é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•å°†ç½‘æ ¼åºåˆ—ä¸­çš„ç½‘æ ¼é”šå®šåˆ°ä¸–ç•Œåæ ‡ç³»ä¸­ï¼Œè¯¥ç®—æ³•ä½¿ç”¨é«˜æ–¯æ–‘ç‚¹ä¹‹é—´çš„å‡ ä½•å…³ç³»æ¥ä¼°è®¡ç½‘æ ¼çš„ä½å§¿ï¼›</p><p>ï¼ˆ5ï¼‰ï¼šè¯¥æ¡†æ¶é€šè¿‡è¿­ä»£ä¼˜åŒ–ç½‘æ ¼æ„å»ºæ¨¡å—å’Œç½‘æ ¼é”šå®šæ¨¡å—ä¸­çš„å‚æ•°æ¥å®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€é«˜æ–¯ç½‘æ ¼ï¼ˆDG-Meshï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºï¼Œå¹¶é€šè¿‡é«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•æ¢å¤é«˜è´¨é‡ç½‘æ ¼ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºã€‚DG-Mesh åœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ 3D é«˜æ–¯æ–‘ç‚¹è¡¨ç¤ºå’Œé«˜æ–¯ç½‘æ ¼é”šå®šç®—æ³•å®ç°æ—¶é—´ä¸€è‡´çš„ç½‘æ ¼åºåˆ—é‡å»ºï¼›æ€§èƒ½ï¼šåœ¨ç½‘æ ¼é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼›å·¥ä½œé‡ï¼šéœ€è¦å‡†ç¡®çš„å¯¹è±¡åˆ†å‰²å’Œè·Ÿè¸ªï¼Œåœ¨å¤„ç†å¤§æ‹“æ‰‘å˜åŒ–æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-bb786e92e4a68c16900a6443568566f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-84e4142556dfc3bb4d97a20772986995.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2de77e2437a64b7bd107f95e76669404.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-25  FlowMap High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/</id>
    <published>2024-04-25T13:12:59.000Z</published>
    <updated>2024-04-25T13:12:59.111Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-25-æ›´æ–°"><a href="#2024-04-25-æ›´æ–°" class="headerlink" title="2024-04-25 æ›´æ–°"></a>2024-04-25 æ›´æ–°</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary</strong><br>é«˜æ–¯æ•£ç‚¹ç»˜åˆ¶åŠ©åŠ›éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆï¼Œç²¾ç¡®æ§åˆ¶é¢éƒ¨åŠ¨ä½œï¼Œå®ç°è‡ªç„¶æµç•…çš„å”‡éƒ¨è¿åŠ¨å’Œé€¼çœŸçš„è§†è§‰æ•ˆæœã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŸºäº 3D é«˜æ–¯æ•£ç‚¹ç»˜åˆ¶ï¼Œæ˜¾å¼è¡¨ç¤ºé¢éƒ¨åŠ¨ä½œï¼Œå®ç°å¯¹è„¸éƒ¨è¿åŠ¨çš„ç›´è§‚æ§åˆ¶ã€‚</li><li>è¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨ï¼Œé€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨è¿åŠ¨ç”Ÿæˆï¼Œå®ç°å‡†ç¡®çš„å”‡éƒ¨è¿åŠ¨ã€‚</li><li>åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œé€šè¿‡æ½œåœ¨ä½å§¿å¢å¼ºé¢éƒ¨ç»†èŠ‚è¡¨ç¤ºï¼Œæä¾›ç¨³å®šé€¼çœŸçš„æ¸²æŸ“è§†é¢‘ã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›ç²¾ç¡®çš„å”‡éƒ¨åŒæ­¥å’Œå‡ºè‰²çš„è§†è§‰è´¨é‡ã€‚</li><li>æ¸²æŸ“é€Ÿåº¦è¾¾ 130 FPSï¼Œè¿œè¶…å®æ—¶æ¸²æŸ“æ€§èƒ½é˜ˆå€¼ï¼Œå¯éƒ¨ç½²äºå…¶ä»–ç¡¬ä»¶å¹³å°ã€‚</li><li>æ—¨åœ¨è§£å†³éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨åˆæˆä¸­å§¿æ€å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³çš„é—®é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GaussianTalkerï¼šåŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„è¯´è¯äººç‰¹å®šä¼šè¯´è¯çš„å¤´éƒ¨åˆæˆ</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: é˜¿é‡Œå·´å·´é›†å›¢</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion, Facial animation</p></li><li><p>Urls: Paper: , Github:None</p></li><li><p>Summary:</p><pre><code>           (1): è¿‘æœŸåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆå·¥ä½œå–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚ç„¶è€Œï¼Œç”±äºNeRFéšå¼è¡¨ç¤ºå¯¼è‡´çš„å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå¦‚å”‡éƒ¨åŠ¨ä½œä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ï¼Œä»¥åŠè§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚           (2): è¿‡å»çš„æ–¹æ³•ï¼šåŸºäºNeRFçš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹æ³•ã€‚é—®é¢˜ï¼šå§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸è¶³ï¼Œå¯¼è‡´å”‡éƒ¨åŠ¨ä½œä¸åŒæ­¥æˆ–ä¸è‡ªç„¶ï¼Œä»¥åŠè§†è§‰æŠ–åŠ¨å’Œä¼ªå½±ã€‚åŠ¨æœºï¼šé€šè¿‡æ˜¾å¼è¡¨ç¤ºé¢éƒ¨è¿åŠ¨ï¼Œå®ç°å¯¹å”‡éƒ¨åŠ¨ä½œçš„ç›´è§‚æ§åˆ¶ã€‚           (3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šGaussianTalkerï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–°æ–¹æ³•ã€‚GaussianTalkerç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šè¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ã€‚è¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œã€‚åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œä»¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ã€‚           (4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼šåœ¨TalkingHeadæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGaussianTalkeråœ¨å”‡éƒ¨åŒæ­¥ã€è§†è§‰ä¿çœŸåº¦å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ï¼šå®ç°å‡†ç¡®ã€é€¼çœŸä¸”é²æ£’çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>           (1): æå‡ºGaussianTalkerï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–°æ–¹æ³•ï¼›           (2): GaussianTalkerç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼šè¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨å’ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ï¼›           (3): è¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨é€šè¿‡é€šç”¨éŸ³é¢‘ç‰¹å¾æå–å’Œå®šåˆ¶å”‡éƒ¨åŠ¨ä½œç”Ÿæˆï¼Œå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å‡†ç¡®å”‡éƒ¨åŠ¨ä½œï¼›           (4): åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨å¼•å…¥è¯´è¯äººç‰¹å®šæ··åˆå½¢çŠ¶ï¼Œä»¥æ§åˆ¶é¢éƒ¨è¡¨æƒ…ï¼›           (5): é‡‡ç”¨FLAMEæ¨¡å‹ä½œä¸ºé¢éƒ¨åŠ¨ç”»å’Œæ¸²æŸ“ä¹‹é—´çš„æ¡¥æ¢ï¼›           (6): è®­ç»ƒè¯´è¯äººç‰¹å®šè¿åŠ¨è½¬æ¢å™¨ï¼Œä½¿ç”¨å¤šè¯­è¨€ã€å¤šä¸ªäººæ•°æ®é›†ï¼Œä»¥æé«˜å…¶å¯¹ä¸åŒéŸ³é¢‘è¾“å…¥çš„é€‚åº”æ€§ï¼›           (7): ä½¿ç”¨é€šç”¨éŸ³é¢‘ç¼–ç å™¨åˆ†ç¦»èº«ä»½ä¿¡æ¯å’Œå†…å®¹ä¿¡æ¯ï¼Œä½¿ç”¨å®šåˆ¶è¿åŠ¨è§£ç å™¨é›†æˆä¸ªæ€§åŒ–ç‰¹å¾ï¼›           (8): å¼•å…¥åŸºäºè‡ªæˆ‘ç›‘ç£çš„å”‡è¯»çº¦æŸæœºåˆ¶ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–å”‡éƒ¨åŠ¨ä½œçš„åŒæ­¥æ€§ï¼›           (9): åŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨ç”±FLAMEé©±åŠ¨3Dé«˜æ–¯æ–‘ç‚¹ï¼Œå®æ—¶æ¸²æŸ“åŠ¨æ€è¯´è¯å¤´éƒ¨ï¼›           (10): ä½¿ç”¨è‡ªé€‚åº”å¯†åº¦æ§åˆ¶ï¼Œæ ¹æ®å±€éƒ¨è¿åŠ¨å¹…åº¦åŠ¨æ€è°ƒæ•´é«˜æ–¯æ–‘ç‚¹çš„å¯†åº¦ï¼›           (11): é‡‡ç”¨å›¾åƒä¿®å¤ç”Ÿæˆå™¨ï¼Œå¤„ç†æ¸²æŸ“è¿‡ç¨‹ä¸­çš„ç©ºæ´åŒºåŸŸå’Œä¼ªå½±ï¼›           (12): åœ¨TalkingHeadæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGaussianTalkeråœ¨å”‡éƒ¨åŒæ­¥ã€è§†è§‰ä¿çœŸåº¦å’Œé²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>           (1):æœ¬æ–‡æå‡ºGaussianTalkerï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆæ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®ä¸ç‰¹å®šè¯´è¯äººå…³è”ï¼Œå‡å°‘äº†éŸ³é¢‘ã€3Dç½‘æ ¼å’Œè§†é¢‘ä¹‹é—´çš„æ½œåœ¨èº«ä»½åå·®ã€‚è¯´è¯äººç‰¹å®šFLAMEè½¬æ¢å™¨é‡‡ç”¨èº«ä»½è§£è€¦å’Œä¸ªæ€§åŒ–åµŒå…¥æ¥å®ç°åŒæ­¥ä¸”è‡ªç„¶çš„å”‡éƒ¨åŠ¨ä½œï¼Œè€ŒåŠ¨æ€é«˜æ–¯æ¸²æŸ“å™¨é€šè¿‡æ½œåœ¨å§¿åŠ¿ä¼˜åŒ–é«˜æ–¯å±æ€§ï¼Œä»¥å®ç°ç¨³å®šä¸”é€¼çœŸçš„æ¸²æŸ“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGaussianTalkeråœ¨è¯´è¯å¤´éƒ¨åˆæˆæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†è¶…é«˜çš„æ¸²æŸ“é€Ÿåº¦ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç§åˆ›æ–°æ–¹æ³•å°†é¼“åŠ±æœªæ¥çš„ç ”ç©¶å¼€å‘æ›´åŠ æµç•…é€¼çœŸçš„è§’è‰²è¡¨æƒ…å’ŒåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„é«˜æ–¯æ¨¡å‹å’Œç”ŸæˆæŠ€æœ¯ï¼Œè§’è‰²çš„åŠ¨ç”»å°†è¿œè¿œè¶…å‡ºç®€å•çš„å”‡éƒ¨åŒæ­¥ï¼Œæ•æ‰æ›´å¹¿æ³›çš„è§’è‰²åŠ¨æ€ã€‚                             (2):åˆ›æ–°ç‚¹ï¼šåŸºäº3Dé«˜æ–¯æ–‘ç‚¹çš„éŸ³é¢‘é©±åŠ¨è¯´è¯å¤´éƒ¨åˆæˆï¼›           æ€§èƒ½ï¼šåœ¨å”‡éƒ¨åŒæ­¥ã€è§†è§‰ä¿çœŸåº¦å’Œé²æ£’æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼›           å·¥ä½œé‡ï¼šä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œæ¸²æŸ“é€Ÿåº¦è¶…é«˜ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://picx.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields. </p><p><a href="http://arxiv.org/abs/2403.17881v3">PDF</a> We closely follow the latest developments in   <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a></p><p><strong>Summary</strong><br>æ·±åº¦ä¼ªé€ æ˜¯ä¸€é¡¹åœ¨ç‰¹å®šæ¡ä»¶ä¸‹åˆ›å»ºé«˜åº¦é€¼çœŸçš„é¢éƒ¨å›¾åƒå’Œè§†é¢‘çš„æŠ€æœ¯ï¼Œåœ¨å¨±ä¹ã€ç”µå½±åˆ¶ä½œã€æ•°å­—äººåˆ›ä½œç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸»è¦é‡‡ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚</li><li>æ‰©æ•£æ¨¡å‹çš„å‡ºç°å¼•å‘äº†æ–°ä¸€è½®çš„æ·±åº¦ä¼ªé€ ç”Ÿæˆç ”ç©¶çƒ­æ½®ã€‚</li><li>æ·±åº¦ä¼ªé€ æ£€æµ‹æŠ€æœ¯ä¸ç”ŸæˆæŠ€æœ¯åŒæ­¥å‘å±•ï¼Œä»¥é˜²èŒƒæ·±åº¦ä¼ªé€ æŠ€æœ¯çš„æ»¥ç”¨ã€‚</li><li>æœ¬ç»¼è¿°å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢å›é¡¾å’Œåˆ†æã€‚</li><li>æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸­çš„é¢éƒ¨æ›¿æ¢ã€é¢éƒ¨é‡ç°ã€è¯´è¯è„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘ç­‰é¢†åŸŸçš„ç ”ç©¶å¤‡å—å…³æ³¨ã€‚</li><li>æœ¬ç»¼è¿°å¯¹æ¯ä¸€é¢†åŸŸä¸­å…·æœ‰ä»£è¡¨æ€§çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå……åˆ†å±•ç¤ºäº†æœ€æ–°ä¸”æœ‰å½±å“åŠ›çš„å·²å‘è¡¨å·¥ä½œã€‚</li><li>æœ¬ç»¼è¿°åˆ†æäº†ç›¸å…³é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹ï¼šåŸºå‡†ä¸ç»¼è¿°</p></li><li><p>Authors: Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</p></li><li><p>Affiliation: ä¸œåå¤§å­¦å¤šç»´ä¿¡æ¯å¤„ç†ä¸Šæµ·å¸‚é‡ç‚¹å®éªŒå®¤</p></li><li><p>Keywords: Deepfake Generation, Face Swapping, Face Reenactment, Talking Face Generation, Facial Attribute Editing, Forgery detection, Survey</p></li><li><p>Urls: https://arxiv.org/abs/2403.17881v3 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):æ·±åº¦ä¼ªé€ æŠ€æœ¯è¿‘å¹´æ¥å¾—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶åº”ç”¨å‰æ™¯å¹¿é˜”ï¼Œä½†å…¶æ½œåœ¨çš„ä¼¦ç†é£é™©ä¹Ÿå¼•å‘äº†äººä»¬çš„æ‹…å¿§ã€‚</p><p>(2):ä¼ ç»Ÿçš„æ·±åº¦ä¼ªé€ ç”Ÿæˆæ–¹æ³•ä¸»è¦åŸºäºå˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œä½†å…¶ç”Ÿæˆæ•ˆæœä»ä¸ä»¤äººæ»¡æ„ã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°æå¤§åœ°æå‡äº†å›¾åƒ/è§†é¢‘çš„ç”Ÿæˆèƒ½åŠ›ã€‚</p><p>(3):æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œåº¦é‡æ ‡å‡†ï¼Œå¹¶è®¨è®ºäº†å‘å±•æŠ€æœ¯ã€‚</p><p>(4):æœ¬æ–‡å¯¹äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘ç­‰å››ä¸ªä»£è¡¨æ€§æ·±åº¦ä¼ªé€ é¢†åŸŸè¿›è¡Œäº†ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æäº†å„é¢†åŸŸçš„å‘å±•å†ç¨‹ï¼Œå¹¶åœ¨æµè¡Œæ•°æ®é›†ä¸Šå¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é¦–å…ˆç»Ÿä¸€äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆå’Œæ£€æµ‹çš„ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œåº¦é‡æ ‡å‡†ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡å¯¹äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘å››ä¸ªä»£è¡¨æ€§æ·±åº¦ä¼ªé€ é¢†åŸŸè¿›è¡Œäº†ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æäº†å„é¢†åŸŸçš„å‘å±•å†ç¨‹ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡åœ¨æµè¡Œæ•°æ®é›†ä¸Šå¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡å…¨é¢ç»¼è¿°äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œåº¦é‡æ ‡å‡†ï¼Œå¹¶è®¨è®ºäº†å‘å±•æŠ€æœ¯ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡é¦–æ¬¡å…¨é¢è¦†ç›–äº†æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸï¼Œå¹¶è®¨è®ºäº†æœ€æ–°çš„æŠ€æœ¯ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼›æœ¬æ–‡å¯¹äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘å››ä¸ªä»£è¡¨æ€§æ·±åº¦ä¼ªé€ é¢†åŸŸè¿›è¡Œäº†ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æäº†å„é¢†åŸŸçš„å‘å±•å†ç¨‹ï¼Œå¹¶åœ¨æµè¡Œæ•°æ®é›†ä¸Šå¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ï¼›æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†æ€»ç»“ã€‚æ€§èƒ½ï¼šæœ¬æ–‡åœ¨äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘å››ä¸ªä»£è¡¨æ€§æ·±åº¦ä¼ªé€ é¢†åŸŸè¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒï¼›æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†æ€»ç»“ã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸè¿›è¡Œäº†å…¨é¢ç»¼è¿°ï¼Œç»Ÿä¸€äº†ä»»åŠ¡å®šä¹‰ï¼Œå…¨é¢ä»‹ç»äº†æ•°æ®é›†å’Œåº¦é‡æ ‡å‡†ï¼Œå¹¶è®¨è®ºäº†å‘å±•æŠ€æœ¯ï¼Œä¸ºåç»­ç ”ç©¶æä¾›äº†åŸºç¡€ï¼›æœ¬æ–‡å¯¹äººè„¸æ›¿æ¢ã€äººè„¸é‡ç°ã€è¯´è¯äººè„¸ç”Ÿæˆå’Œé¢éƒ¨å±æ€§ç¼–è¾‘å››ä¸ªä»£è¡¨æ€§æ·±åº¦ä¼ªé€ é¢†åŸŸè¿›è¡Œäº†ç ”ç©¶ï¼Œé‡ç‚¹åˆ†æäº†å„é¢†åŸŸçš„å‘å±•å†ç¨‹ï¼Œå¹¶åœ¨æµè¡Œæ•°æ®é›†ä¸Šå¯¹ä»£è¡¨æ€§æ–¹æ³•è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå……åˆ†è¯„ä¼°äº†æœ€æ–°å’Œæœ‰å½±å“åŠ›çš„å·²å‘è¡¨ä½œå“ï¼›æœ¬æ–‡å¯¹æ·±åº¦ä¼ªé€ ç”Ÿæˆä¸æ£€æµ‹é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘è¿›è¡Œäº†æ€»ç»“ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pica.zhimg.com/v2-6a08950b6be4e3f18aeef87726b535fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c245a3a60e574c0cb0324f79ffd23876.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-afbe757ef2a542a37ce568036b591797.jpg" align="middle"><img src="https://picx.zhimg.com/v2-becdaa251ccb21b3a85f051bf593814f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c84c542a045ab258f8a251f6f24a1446.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48abe21b928d9c991400ddc443d9eec3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/</id>
    <published>2024-04-25T12:59:05.000Z</published>
    <updated>2024-04-25T12:59:05.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-25-æ›´æ–°"><a href="#2024-04-25-æ›´æ–°" class="headerlink" title="2024-04-25 æ›´æ–°"></a>2024-04-25 æ›´æ–°</h1><h2 id="ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning"><a href="#ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning" class="headerlink" title="ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"></a>ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</h2><p><strong>Authors:Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</strong></p><p>The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{<a href="https://idaligner.github.io/}}">https://idaligner.github.io/}}</a> </p><p><a href="http://arxiv.org/abs/2404.15449v1">PDF</a> </p><p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å¸¦æ¥çš„æ–‡æœ¬å›¾åƒç”Ÿæˆåœ¨èº«ä»½ä¿æŒäººåƒå’Œå•†ç”¨å›¾ç‰‡ä¸Šå¹¿æ³›åº”ç”¨ï¼ŒID-Aligneræ¡†æ¶é€šè¿‡åé¦ˆå­¦ä¹ å¢å¼ºå›¾åƒç¾æ„Ÿã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>èº«ä»½ä¿æŒå›¾åƒç”Ÿæˆæ–¹æ³•åœ¨èº«ä»½ç‰¹å¾ä¿æŒã€ç¾è§‚æ€§ä¿è¯ã€å…¼å®¹æ€§æ–¹é¢æœ‰æå‡ç©ºé—´ã€‚</li><li>ID-Aligneræ¡†æ¶é€šè¿‡åé¦ˆå­¦ä¹ æ¥å¢å¼ºID-T2Iæ•ˆæœã€‚</li><li>èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å¾®è°ƒåˆ©ç”¨é¢éƒ¨æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹çš„åé¦ˆï¼Œæé«˜ç”Ÿæˆçš„å›¾åƒçš„èº«ä»½ä¿ç•™èƒ½åŠ›ã€‚</li><li>èº«ä»½ç¾å­¦å¥–åŠ±å¾®è°ƒåˆ©ç”¨äººå·¥æ ‡æ³¨åå¥½æ•°æ®å’Œè‡ªåŠ¨æ„å»ºçš„å­—ç¬¦ç»“æ„ç”Ÿæˆåé¦ˆï¼Œæä¾›ç¾å­¦è°ƒæ•´ä¿¡å·ã€‚</li><li>å¾—ç›Šäºé€šç”¨çš„åé¦ˆå¾®è°ƒæ¡†æ¶ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºLoRAå’Œé€‚é…å™¨æ¨¡å‹ï¼Œå®ç°æ€§èƒ½æå‡ã€‚</li><li>åœ¨SD1.5å’ŒSDXLæ‰©æ•£æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ID-Aligner: å¢å¼ºèº«ä»½ä¿ç•™æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ</p></li><li><p>Authors: Weifeng Chen, Jiachang Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</p></li><li><p>Affiliation: ä¸­å±±å¤§å­¦</p></li><li><p>Keywords: Identity-preserving text-to-image generation, Diffusion models, Feedback learning, LoRA, Adapter</p></li><li><p>Urls: https://arxiv.org/abs/2404.15449 , Github:None</p></li><li><p>Summary:</p><p>(1): éšç€æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå…¶ä¸­èº«ä»½ä¿ç•™æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆID-T2Iï¼‰å› å…¶åœ¨AIäººåƒã€å¹¿å‘Šç­‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ID-T2Iæ–¹æ³•ä»é¢ä¸´ç€ä¸€äº›å…³é”®æŒ‘æˆ˜ï¼šéš¾ä»¥å‡†ç¡®ä¿æŒå‚è€ƒäººåƒçš„èº«ä»½ç‰¹å¾ã€ç”Ÿæˆçš„å›¾åƒç¼ºä¹ç¾æ„Ÿï¼Œä»¥åŠæ— æ³•åŒæ—¶å…¼å®¹åŸºäºLoRAå’ŒåŸºäºAdapterçš„æ–¹æ³•ã€‚</p><p>(2): ç°æœ‰çš„ID-T2Iæ–¹æ³•ä¸»è¦é€šè¿‡åœ¨æ‰©æ•£æ¨¡å‹ä¸­åŠ å…¥èº«ä»½ç¼–ç ä¿¡æ¯æ¥å®ç°èº«ä»½ä¿ç•™ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€ä¼šä¸¢å¤±å‚è€ƒäººåƒçš„ç»†è‡´ç‰¹å¾ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒä¸å‚è€ƒäººåƒå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨å¢å¼ºèº«ä»½ä¿ç•™çš„åŒæ—¶ï¼Œå¾€å¾€ä¼šé™ä½å›¾åƒçš„è§†è§‰å¸å¼•åŠ›ã€‚</p><p>(3): é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåé¦ˆå­¦ä¹ çš„é€šç”¨æ¡†æ¶ID-Alignerï¼Œç”¨äºå¢å¼ºID-T2Iæ€§èƒ½ã€‚ID-Aligneré€šè¿‡å¼•å…¥èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å’Œèº«ä»½ç¾å­¦å¥–åŠ±ï¼Œåˆ†åˆ«å¢å¼ºäº†ç”Ÿæˆçš„å›¾åƒçš„èº«ä»½ä¿ç•™æ€§å’Œè§†è§‰å¸å¼•åŠ›ã€‚æ­¤å¤–ï¼ŒID-Alignerå¯ä»¥åŒæ—¶åº”ç”¨äºåŸºäºLoRAå’ŒåŸºäºAdapterçš„æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¥½çš„å…¼å®¹æ€§ã€‚</p><p>(4): åœ¨äººåƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒID-Aligneråœ¨ä¿æŒèº«ä»½ç‰¹å¾å’Œç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒID-Alignerç”Ÿæˆçš„å›¾åƒåœ¨èº«ä»½ä¿ç•™åº¦ã€å›¾åƒè´¨é‡å’Œç”¨æˆ·åå¥½æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ID-T2Iæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p></li><li><p>æ–¹æ³•ï¼š</p><p>(1):æå‡ºID-Alignerï¼Œä¸€ç§åˆ©ç”¨åé¦ˆå­¦ä¹ æ–¹æ³•æ¥å¢å¼ºèº«ä»½ï¼ˆIDï¼‰ä¿ç•™ç”Ÿæˆæ€§èƒ½çš„å¼€åˆ›æ€§æ–¹æ³•ã€‚æ–¹æ³•çš„æ¦‚è¿°è§å›¾2ã€‚æˆ‘ä»¬é€šè¿‡å¥–åŠ±åé¦ˆå­¦ä¹ èŒƒå¼è§£å†³äº†IDä¿ç•™ç”Ÿæˆï¼Œä»¥å¢å¼ºä¸å‚è€ƒäººåƒå›¾åƒå’Œç”Ÿæˆå›¾åƒçš„ç¾æ„Ÿçš„ä¸€è‡´æ€§ã€‚</p><p>(2):æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆ©ç”¨æ‰©æ•£å»ºæ¨¡æ ¹æ®æ–‡æœ¬æç¤ºé€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œè¯¥æ¨¡å‹é€šè¿‡æ¸è¿›çš„å»å™ªè¿‡ç¨‹ä»é«˜æ–¯å™ªå£°ç”Ÿæˆæ‰€éœ€çš„æ•°æ®æ ·æœ¬ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œé¦–å…ˆé€šè¿‡é¢„è®­ç»ƒçš„VAE [4, 10]ç¼–ç å™¨å¤„ç†é‡‡æ ·çš„å›¾åƒğ‘¥ï¼Œä»¥å¯¼å‡ºå…¶æ½œåœ¨è¡¨ç¤ºğ‘§ã€‚éšåï¼Œé€šè¿‡å‰å‘æ‰©æ•£è¿‡ç¨‹å°†éšæœºå™ªå£°æ³¨å…¥æ½œåœ¨è¡¨ç¤ºï¼Œéµå¾ªé¢„å®šä¹‰çš„æ—¶é—´è¡¨{ğ›½ğ‘¡ }ğ‘‡ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¡¨è¿°ä¸ºğ‘§ğ‘¡ = âˆšğ›¼ğ‘¡ğ‘§ + âˆš1 âˆ’ ğ›¼ğ‘¡ğœ–ï¼Œå…¶ä¸­ğœ– âˆˆ N (0, 1)æ˜¯ä¸ğ‘§ç»´åº¦ç›¸åŒçš„éšæœºå™ªå£°ï¼Œğ›¼ğ‘¡ = ï¿½ğ‘¡ ğ‘ =1 ğ›¼ğ‘ å’Œğ›¼ğ‘¡ = 1 âˆ’ ğ›½ğ‘¡ã€‚ä¸ºäº†å®ç°å»å™ªè¿‡ç¨‹ï¼Œè®­ç»ƒäº†ä¸€ä¸ªUNet ğœ–ğœƒæ¥é¢„æµ‹å‰å‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ·»åŠ å™ªå£°ï¼Œæ¡ä»¶æ˜¯å™ªå£°æ½œåœ¨å’Œæ–‡æœ¬æç¤ºğ‘ã€‚å½¢å¼ä¸Šï¼ŒUNetçš„ä¼˜åŒ–ç›®æ ‡æ˜¯ï¼šL(ğœƒ) = Eğ‘§,ğœ–,ğ‘,ğ‘¡ [||ğœ– âˆ’ ğœ–ğœƒ ( âˆšï¸ ğ›¼ğ‘¡ğ‘§ + âˆšï¸ 1 âˆ’ ğ›¼ğ‘¡ğœ–,ğ‘,ğ‘¡)||2 2]ã€‚</p><p>(3):èº«ä»½å¥–åŠ±ï¼šèº«ä»½ä¸€è‡´æ€§å¥–åŠ±ï¼šç»™å®šå‚è€ƒå›¾åƒğ‘¥ref 0å’Œç”Ÿæˆå›¾åƒğ‘¥â€² 0ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯„ä¼°ç‰¹å®šè‚–åƒçš„IDç›¸ä¼¼æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨äººè„¸æ£€æµ‹æ¨¡å‹FaceDetæ¥å®šä½ä¸¤å¹…å›¾åƒä¸­çš„äººè„¸ã€‚åŸºäºäººè„¸æ£€æµ‹æ¨¡å‹çš„è¾“å‡ºï¼Œæˆ‘ä»¬è£å‰ªç›¸åº”çš„äººè„¸åŒºåŸŸå¹¶å°†å…¶è¾“å…¥äººè„¸è¯†åˆ«æ¨¡å‹FaceEncçš„ç¼–ç å™¨ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè·å¾—å‚è€ƒäººè„¸Erefå’Œç”Ÿæˆäººè„¸Egençš„ç¼–ç äººè„¸åµŒå…¥ï¼Œå³Eref = FaceEnc(FaceDet(ğ‘¥ref 0 )), Egen = FaceEnc(FaceDet(ğ‘¥â€² 0))ã€‚éšåï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸¤ä¸ªé¢éƒ¨åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½œä¸ºç”Ÿæˆè¿‡ç¨‹ä¸­IDä¿ç•™çš„åº¦é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ­¤ç›¸ä¼¼åº¦ä½œä¸ºåé¦ˆè°ƒæ•´è¿‡ç¨‹çš„å¥–åŠ±ä¿¡å·ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šâ„œğ‘–ğ‘‘_ğ‘ ğ‘–ğ‘š(ğ‘¥â€² 0,ğ‘¥ref 0 ) = cose_sim(Egen, Eref)ã€‚èº«ä»½ç¾å­¦å¥–åŠ±ï¼šé™¤äº†èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªä¸“æ³¨äºå¸å¼•åŠ›å’Œè´¨é‡çš„èº«ä»½ç¾å­¦å¥–åŠ±æ¨¡å‹ã€‚å®ƒåŒ…æ‹¬äººç±»å¯¹å¸å¼•åŠ›çš„åå¥½å’Œåˆç†çš„ç»“æ„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªæ”¶é›†çš„äººç±»æ³¨é‡Šåå¥½æ•°æ®é›†è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å¯¹å›¾åƒè¿›è¡Œè¯„åˆ†å¹¶åæ˜ äººç±»å¯¹å¸å¼•åŠ›çš„åå¥½ï¼Œå¦‚å›¾3å³æ‰€ç¤ºã€‚æˆ‘ä»¬é‡‡ç”¨ImageReward [37]æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹æŸå¤±å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼šLğœƒ = âˆ’ğ¸(ğ‘,ğ‘¥ğ‘–,ğ‘¥ğ‘— )âˆ¼D [ğ‘™ğ‘œğ‘”(ğœ(ğ‘“ğœƒ (ğ‘¥ğ‘–,ğ‘) âˆ’ ğ‘“ğœƒ (ğ‘¥ğ‘—,ğ‘)))].æ­¤æŸå¤±å‡½æ•°åŸºäºå›¾åƒä¹‹é—´çš„æ¯”è¾ƒå¯¹ï¼Œå…¶ä¸­æ¯ä¸ªæ¯”è¾ƒå¯¹åŒ…å«ä¸¤å¹…å›¾åƒï¼ˆğ‘¥ğ‘–å’Œğ‘¥ğ‘—ï¼‰å’Œæç¤ºğ‘ã€‚ğ‘“ğœƒ (ğ‘¥,ğ‘)è¡¨ç¤ºç»™å®šå›¾åƒğ‘¥å’Œæç¤ºğ‘çš„å¥–åŠ±åˆ†æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ğ‘“ğœƒç§°ä¸ºâ„œğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘™ä½œä¸ºå¸å¼•åŠ›å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»“æ„å¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥åŒºåˆ†æ‰­æ›²çš„è‚¢ä½“/èº«ä½“å’Œè‡ªç„¶çš„è‚¢ä½“/èº«ä½“ã€‚ä¸ºäº†è®­ç»ƒä¸€ä¸ªå¯ä»¥è®¿é—®å›¾åƒç»“æ„æ˜¯å¦åˆç†æ€§çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ç»„åŒ…å«æ­£é¢å’Œè´Ÿé¢æ ·æœ¬çš„æ–‡æœ¬å›¾åƒå¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»è¿‡äººç±»æ£€æµ‹å™¨è¿‡æ»¤çš„LAION [28]ä¸­çš„å›¾åƒã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨å§¿åŠ¿ä¼°è®¡æ¨¡å‹ç”Ÿæˆå§¿åŠ¿ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºæœªæ‰­æ›²çš„äººä½“ç»“æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬éšæœºæ‰­æ›²å§¿åŠ¿å¹¶åˆ©ç”¨ControlNet [42]ç”Ÿæˆå¤±çœŸä½“ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œå¦‚å›¾3å·¦ä¾§æ‰€ç¤ºã€‚ä¸€æ—¦æ­£è´Ÿå¯¹å¯ç”¨ï¼ŒåŒæ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸æ–¹ç¨‹å¼5ç›¸åŒçš„æŸå¤±è®­ç»ƒç»“æ„å¥–åŠ±æ¨¡å‹ï¼Œå¹¶å°†ç»“æ„å¥–åŠ±æ¨¡å‹ç§°ä¸ºâ„œğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ã€‚ç„¶åï¼Œèº«ä»½ç¾å­¦å¥–åŠ±æ¨¡å‹å®šä¹‰ä¸ºâ„œğ‘–ğ‘‘_ğ‘ğ‘’ğ‘  (ğ‘¥,ğ‘) = â„œğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘™ (ğ‘¥,ğ‘) + â„œğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ (ğ‘¥,ğ‘)ã€‚</p><p>(4):IDä¿ç•™åé¦ˆå­¦ä¹ ï¼šåœ¨åé¦ˆå­¦ä¹ é˜¶æ®µï¼Œæˆ‘ä»¬ä»è¾“å…¥æç¤ºğ‘å¼€å§‹ï¼Œéšæœºåˆå§‹åŒ–æ½œåœ¨å˜é‡ğ‘¥ğ‘‡ã€‚ç„¶åå¯¹æ½œåœ¨å˜é‡è¿›è¡Œæ¸è¿›å»å™ªï¼Œç›´åˆ°è¾¾åˆ°éšæœºé€‰æ‹©çš„æ—¶é—´æ­¥ğ‘¡ã€‚æ­¤æ—¶ï¼Œå»å™ªå›¾åƒğ‘¥â€² 0ç›´æ¥ä»ğ‘¥ğ‘¡é¢„æµ‹ã€‚å°†ä»å…ˆå‰é˜¶æ®µè·å¾—çš„å¥–åŠ±æ¨¡å‹åº”ç”¨äºæ­¤å»å™ªå›¾åƒï¼Œç”Ÿæˆé¢„æœŸçš„åå¥½åˆ†æ•°ã€‚æ­¤åå¥½åˆ†æ•°ä½¿æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œå¾®è°ƒï¼Œä»¥æ›´ç´§å¯†åœ°ä¸åæ˜ èº«ä»½ä¸€è‡´æ€§å’Œå®¡ç¾åå¥½çš„IDå¥–åŠ±ä¿æŒä¸€è‡´ï¼šLğ‘–ğ‘‘_ğ‘ ğ‘–ğ‘š = Eğ‘âˆ¼ğ‘ (ğ‘)Eğ‘¥â€² 0âˆ¼ğ‘ (ğ‘¥â€² 0|ğ‘) [1 âˆ’ â„œğ‘–ğ‘‘_ğ‘ ğ‘–ğ‘š(ğ‘¥â€² 0,ğ‘¥ğ‘Ÿğ‘’ğ‘“ 0 )], Lğ‘–ğ‘‘_ğ‘ğ‘’ğ‘  = Eğ‘âˆ¼ğ‘ (ğ‘)Eğ‘¥â€² 0âˆ¼ğ‘ (ğ‘¥â€² 0|ğ‘) [âˆ’â„œğ‘–ğ‘‘_ğ‘ğ‘’ğ‘  (ğ‘¥â€² 0,ğ‘)]ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåé¦ˆå­¦ä¹ çš„é€šç”¨æ¡†æ¶ID-Alignerï¼Œç”¨äºå¢å¼ºèº«ä»½ä¿ç•™æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼ˆID-T2Iï¼‰æ€§èƒ½ã€‚ID-Aligneré€šè¿‡å¼•å…¥èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å’Œèº«ä»½ç¾å­¦å¥–åŠ±ï¼Œåˆ†åˆ«å¢å¼ºäº†ç”Ÿæˆçš„å›¾åƒçš„èº«ä»½ä¿ç•™æ€§å’Œè§†è§‰å¸å¼•åŠ›ã€‚æ­¤å¤–ï¼ŒID-Alignerå¯ä»¥åŒæ—¶åº”ç”¨äºåŸºäºLoRAå’ŒåŸºäºAdapterçš„æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¥½çš„å…¼å®¹æ€§ã€‚åœ¨äººåƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒID-Aligneråœ¨ä¿æŒèº«ä»½ç‰¹å¾å’Œç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒID-Alignerç”Ÿæˆçš„å›¾åƒåœ¨èº«ä»½ä¿ç•™åº¦ã€å›¾åƒè´¨é‡å’Œç”¨æˆ·åå¥½æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ID-T2Iæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ID-Alignerï¼Œä¸€ç§åˆ©ç”¨åé¦ˆå­¦ä¹ æ–¹æ³•æ¥å¢å¼ºIDä¿ç•™ç”Ÿæˆæ€§èƒ½çš„å¼€åˆ›æ€§æ–¹æ³•ã€‚å¼•å…¥èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å’Œèº«ä»½ç¾å­¦å¥–åŠ±ï¼Œåˆ†åˆ«å¢å¼ºäº†ç”Ÿæˆçš„å›¾åƒçš„èº«ä»½ä¿ç•™æ€§å’Œè§†è§‰å¸å¼•åŠ›ã€‚æ­¤å¤–ï¼ŒID-Alignerå¯ä»¥åŒæ—¶åº”ç”¨äºåŸºäºLoRAå’ŒåŸºäºAdapterçš„æ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¥½çš„å…¼å®¹æ€§ã€‚</p><p>æ€§èƒ½ï¼šåœ¨äººåƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒID-Aligneråœ¨ä¿æŒèº«ä»½ç‰¹å¾å’Œç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢éƒ½å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒID-Alignerç”Ÿæˆçš„å›¾åƒåœ¨èº«ä»½ä¿ç•™åº¦ã€å›¾åƒè´¨é‡å’Œç”¨æˆ·åå¥½æ–¹é¢å‡ä¼˜äºç°æœ‰çš„ID-T2Iæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><p>å·¥ä½œé‡ï¼šæœ¬æ–‡çš„æ–¹æ³•æ¶‰åŠåˆ°åé¦ˆå­¦ä¹ ã€èº«ä»½ä¸€è‡´æ€§å¥–åŠ±å’Œèº«ä»½ç¾å­¦å¥–åŠ±çš„å¼•å…¥ï¼Œéœ€è¦é¢å¤–çš„è®¡ç®—å’Œæ•°æ®å¤„ç†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-952ad01319e9ee57febc82370c97b6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ea9ae35ff1eb818db6fe2da58e7a072.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3ca1d77296d47d3befa8898dae8433d7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" align="middle"></details>## Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging   Perturbations That Efficiently Fool Customized Diffusion Models**Authors:Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei**Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner. [PDF](http://arxiv.org/abs/2404.15081v1) Published at CVPR 2024**Summary**æ‰©æ•£æ¨¡å‹çš„è·¨æ³¨æ„åŠ›å±‚æ˜“å—æ¢¯åº¦å˜åŒ–å½±å“ï¼Œå¯åˆ©ç”¨ç»†å¾®æ‰°åŠ¨æ¬ºéª—è¯­è¨€å¼•å¯¼æ‰©æ•£æ¨¡å‹ã€‚**Key Takeaways**- æ‰©æ•£æ¨¡å‹ (DM) ä¸ºé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡å’Œé€¼çœŸæ•°æ®æ ·æœ¬å¼€è¾Ÿäº†æ–°æ—¶ä»£ã€‚- DM çš„å¹¿æ³›ä½¿ç”¨å¸¦æ¥äº†æ–°çš„æ¨¡å‹å®‰å…¨æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„å¯¹æŠ—æ”»å‡»è€…æ¥ç†è§£å…¶æ¼æ´ã€‚- CAAT æ˜¯ä¸€ç§ç®€å•ã€é€šç”¨ä¸”æœ‰æ•ˆçš„å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œæ— éœ€æ˜‚è´µçš„è®­ç»ƒå³å¯æœ‰æ•ˆæ¬ºéª—æ½œåœ¨æ‰©æ•£æ¨¡å‹ (LDM)ã€‚- CAAT åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚å¯¹æ¢¯åº¦å˜åŒ–çš„è¾ƒé«˜æ•æ„Ÿæ€§ï¼Œé€šè¿‡å¯¹å·²å‘å¸ƒå›¾åƒæ–½åŠ ç»†å¾®æ‰°åŠ¨æ¥å¤§å¹…ç ´åç”Ÿæˆå›¾åƒã€‚- ç»†å¾®æ‰°åŠ¨å¯ä»¥æ˜¾ç€å½±å“äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œä»è€Œæ”¹å˜å®šåˆ¶æ‰©æ•£æ¨¡å‹å¾®è°ƒæœŸé—´æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„æ˜ å°„ã€‚- å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒCAAT ä¸å„ç§æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼Œå¹¶ä¸”ä»¥æ›´æœ‰æ•ˆï¼ˆæ›´å¤šå™ªå£°ï¼‰å’Œé«˜æ•ˆï¼ˆæ¯” Anti-DreamBooth å’Œ Mist å¿«ä¸¤å€ï¼‰çš„æ–¹å¼ä¼˜äºåŸºçº¿æ”»å‡»æ–¹æ³•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šæ‰°åŠ¨æ³¨æ„åŠ›è®©ä½ äº‹åŠåŠŸå€ï¼šç²¾å¦™çš„å›¾åƒæ‰°åŠ¨</p></li><li><p>ä½œè€…ï¼šYichao Zhou, Jingwen Chen, Yu Cheng, Ziwei Liu, Chen Change Loy</p></li><li><p>å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šDiffusion Modelsã€Adversarial Attackã€Cross-Attention</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08724 , Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼ŒDMsï¼‰ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„æ–°èŒƒå¼ï¼Œåœ¨ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„æ•°æ®æ ·æœ¬æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›åº”ç”¨ä¹Ÿå¸¦æ¥äº†æ¨¡å‹å®‰å…¨æ€§çš„æ–°æŒ‘æˆ˜ï¼Œä¿ƒä½¿ç ”ç©¶è€…ä»¬å¼€å‘æ›´æœ‰æ•ˆçš„å¯¹æŠ—æ”»å‡»æ–¹æ³•æ¥ç†è§£å…¶è„†å¼±æ€§ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šç°æœ‰çš„æ”»å‡»æ–¹æ³•éœ€è¦è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒæ‰èƒ½æœ‰æ•ˆå¯¹æŠ—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼ŒLDMsï¼‰ï¼Œå¹¶ä¸”åœ¨æ•ˆç‡å’Œæ•ˆæœæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ã€é€šç”¨ä¸”é«˜æ•ˆçš„æ”»å‡»æ–¹æ³• CAATï¼Œæ— éœ€æ˜‚è´µçš„è®­ç»ƒå³å¯æœ‰æ•ˆå¯¹æŠ— LDMsã€‚è¯¥æ–¹æ³•åŸºäºè¿™æ ·ä¸€ä¸ªè§‚å¯Ÿï¼šäº¤å‰æ³¨æ„åŠ›å±‚å¯¹æ¢¯åº¦å˜åŒ–è¡¨ç°å‡ºæ›´é«˜çš„æ•æ„Ÿæ€§ï¼Œè¿™ä½¿å¾—åˆ©ç”¨å·²å‘å¸ƒå›¾åƒä¸Šçš„ç»†å¾®æ‰°åŠ¨å°±èƒ½æ˜¾è‘—ç ´åç”Ÿæˆçš„å›¾åƒã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒCAAT ä¸å„ç§æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼Œå¹¶ä¸”åœ¨æœ‰æ•ˆæ€§ï¼ˆäº§ç”Ÿæ›´å¤šå™ªå£°ï¼‰å’Œæ•ˆç‡ï¼ˆæ¯” Anti-DreamBooth å’Œ Mist å¿«ä¸¤å€ï¼‰æ–¹é¢ä¼˜äºåŸºçº¿æ”»å‡»æ–¹æ³•ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šCAAT æ–¹æ³•çš„åŸç†ï¼šåŸºäºäº¤å‰æ³¨æ„åŠ›å±‚å¯¹æ¢¯åº¦å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œåˆ©ç”¨å·²å‘å¸ƒå›¾åƒä¸Šçš„ç»†å¾®æ‰°åŠ¨æ¥ç ´åç”Ÿæˆçš„å›¾åƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæ”»å‡»æ­¥éª¤ï¼š     ï¼ˆaï¼‰ï¼šå‡†å¤‡å·²å‘å¸ƒå›¾åƒå’Œç›®æ ‡å›¾åƒã€‚     ï¼ˆbï¼‰ï¼šä½¿ç”¨ç›®æ ‡å›¾åƒåˆå§‹åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„å™ªå£°ã€‚     ï¼ˆcï¼‰ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å±‚è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ ¹æ®æ¢¯åº¦æ›´æ–°å™ªå£°ã€‚     ï¼ˆdï¼‰ï¼šé‡å¤æ­¥éª¤ (c)ï¼Œç›´åˆ°ç”Ÿæˆå›¾åƒä¸ç›®æ ‡å›¾åƒç›¸ä¼¼ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šCAAT çš„ä¼˜åŠ¿ï¼š     ï¼ˆaï¼‰ï¼šæ— éœ€æ˜‚è´µçš„è®­ç»ƒã€‚     ï¼ˆbï¼‰ï¼šä¸å„ç§æ‰©æ•£æ¨¡å‹å…¼å®¹ã€‚     ï¼ˆcï¼‰ï¼šåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºçº¿æ”»å‡»æ–¹æ³•ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œçš„é‡è¦æ„ä¹‰åœ¨äºæå‡ºäº†ä¸€ç§ç®€å•ã€é€šç”¨ä¸”é«˜æ•ˆçš„æ”»å‡»æ–¹æ³• CAATï¼Œæ— éœ€æ˜‚è´µçš„è®­ç»ƒå³å¯æœ‰æ•ˆå¯¹æŠ—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ•æ„Ÿæ€§ï¼Œé€šè¿‡å·²å‘å¸ƒå›¾åƒä¸Šçš„ç»†å¾®æ‰°åŠ¨æ¥ç ´åç”Ÿæˆçš„å›¾åƒï¼Œä¸ºç†è§£ LDMs çš„è„†å¼±æ€§æä¾›äº†æ–°çš„é€”å¾„ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡çš„ä¼˜åŠ¿å’Œä¸è¶³æ€»ç»“å¦‚ä¸‹ï¼š     åˆ›æ–°ç‚¹ï¼š         ï¼ˆaï¼‰ï¼šæå‡ºäº†åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›å±‚æ•æ„Ÿæ€§çš„æ–°æ”»å‡»æ–¹æ³•ã€‚         ï¼ˆbï¼‰ï¼šæ— éœ€æ˜‚è´µçš„è®­ç»ƒå³å¯æœ‰æ•ˆå¯¹æŠ— LDMsã€‚     æ€§èƒ½ï¼š         ï¼ˆaï¼‰ï¼šä¸å„ç§æ‰©æ•£æ¨¡å‹å…¼å®¹ã€‚         ï¼ˆbï¼‰ï¼šåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŸºçº¿æ”»å‡»æ–¹æ³•ã€‚     å·¥ä½œé‡ï¼š         ï¼ˆaï¼‰ï¼šæ”»å‡»æ­¥éª¤ç®€å•ï¼Œæ˜“äºå®ç°ã€‚         ï¼ˆbï¼‰ï¼šæ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–æ•°æ®æ”¶é›†ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-e21b9a5812791e5572d6cc412d4b6f49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d24fa5d01960bbb84627a575bbe1387e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62d838e7bc25d440e5a0f335a30a775d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5501b074b665578b3fec4ffce2edeb5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af86112d3e55bc02435a1dc8cb3dfe90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4416c3cb309ab371619d47ab4f98e8df.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20a0f1befe2ddce6a3be91bb78c7fe2c.jpg" align="middle"></details>## UVMap-ID: A Controllable and Personalized UV Map Generative Model**Authors:Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri**Recently, diffusion models have made significant strides in synthesizing realistic 2D human images based on provided text prompts. Building upon this, researchers have extended 2D text-to-image diffusion models into the 3D domain for generating human textures (UV Maps). However, some important problems about UV Map Generative models are still not solved, i.e., how to generate personalized texture maps for any given face image, and how to define and evaluate the quality of these generated texture maps. To solve the above problems, we introduce a novel method, UVMap-ID, which is a controllable and personalized UV Map generative model. Unlike traditional large-scale training methods in 2D, we propose to fine-tune a pre-trained text-to-image diffusion model which is integrated with a face fusion module for achieving ID-driven customized generation. To support the finetuning strategy, we introduce a small-scale attribute-balanced training dataset, including high-quality textures with labeled text and Face ID. Additionally, we introduce some metrics to evaluate the multiple aspects of the textures. Finally, both quantitative and qualitative analyses demonstrate the effectiveness of our method in controllable and personalized UV Map generation. Code is publicly available via https://github.com/twowwj/UVMap-ID. [PDF](http://arxiv.org/abs/2404.14568v1) **Summary**åŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆ 3D äººä½“çº¹ç†ï¼Œæå‡ºå¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UVMap-ID ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡å­—-å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨é¢éƒ¨èåˆæ¨¡å—å®ç° ID é©±åŠ¨çš„å®šåˆ¶åŒ–ç”Ÿæˆã€‚**Key Takeaways*** UVMap-IDæ˜¯ä¸€ç§å¯æ§ä¸”ä¸ªæ€§åŒ–çš„UVè´´å›¾ç”Ÿæˆæ¨¡å‹ã€‚* å¼•å…¥äº†ä¸€ä¸ªå°å‹çš„å±æ€§å¹³è¡¡è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬é«˜è´¨é‡çš„çº¹ç†ã€æ ‡è®°æ–‡æœ¬å’Œäººè„¸ IDã€‚* æå‡ºäº†ä¸€äº›æŒ‡æ ‡æ¥è¯„ä¼°çº¹ç†çš„å¤šæ–¹é¢ã€‚* æå‡ºäº†ä¸€ç§å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬-å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ä¸é¢éƒ¨èåˆæ¨¡å—ç›¸ç»“åˆï¼Œä»¥å®ç° ID é©±åŠ¨çš„å®šåˆ¶åŒ–ç”Ÿæˆã€‚* å®šé‡å’Œå®šæ€§åˆ†æè¯æ˜äº† UVMap-ID æ–¹æ³•åœ¨å¯æ§å’Œä¸ªæ€§åŒ– UV è´´å›¾ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚* ä»£ç å¯åœ¨ https://github.com/twowwj/UVMap-ID è·å¾—ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: UVMap-IDï¼šå¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆæ¨¡å‹ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰</p></li><li><p>Authors: Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri</p></li><li><p>Affiliation: ç‰¹ä¼¦æ‰˜å¤§å­¦ MHUG ç»„ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰</p></li><li><p>Keywords: Generative Model, Diffusion Model, 3D Avatar Generation, MultiModal Generation</p></li><li><p>Urls: https://arxiv.org/abs/2404.14568 , https://github.com/twowwj/UVMap-ID</p></li><li><p>Summary: </p><pre><code>            (1):æœ¬æ–‡çš„ç ”ç©¶èƒŒæ™¯æ˜¯ï¼šè¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨åŸºäºæä¾›çš„æ–‡æœ¬æç¤ºåˆæˆé€¼çœŸçš„ 2D äººç±»å›¾åƒæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶äººå‘˜å·²å°† 2D æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•åˆ° 3D é¢†åŸŸï¼Œç”¨äºç”Ÿæˆäººä½“çº¹ç†ï¼ˆUV è´´å›¾ï¼‰ã€‚ç„¶è€Œï¼Œå…³äº UV è´´å›¾ç”Ÿæˆæ¨¡å‹çš„ä¸€äº›é‡è¦é—®é¢˜ä»æœªè§£å†³ï¼Œå³å¦‚ä½•ä¸ºç»™å®šçš„ä»»ä½•äººè„¸å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–çº¹ç†è´´å›¾ï¼Œä»¥åŠå¦‚ä½•å®šä¹‰å’Œè¯„ä¼°è¿™äº›ç”Ÿæˆçº¹ç†è´´å›¾çš„è´¨é‡ã€‚            (2):ä»¥å¾€çš„æ–¹æ³•ä¸»è¦åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„ç”Ÿæˆå™¨ä»¥æ— ç›‘ç£æˆ–ç›‘ç£çš„æ–¹å¼ä¼°è®¡çº¹ç†ï¼Œç„¶åå°†çº¹ç†ä¼°è®¡æ¨¡å‹é›†æˆåˆ°åŒ–èº«æ‹Ÿåˆé˜¶æ®µã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆæ–°é¢–çº¹ç†æ–¹é¢å—åˆ°é™åˆ¶ï¼Œå¹¶ä¸”éœ€è¦æ›´å¤šåœ°æ”¯æŒå¯æ§ç”Ÿæˆã€‚            (3):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• UVMap-IDï¼Œå®ƒæ˜¯ä¸€ç§å¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆæ¨¡å‹ã€‚ä¸ 2D ä¸­ä¼ ç»Ÿçš„è§„æ¨¡åŒ–è®­ç»ƒæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å»ºè®®å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸äººè„¸èåˆæ¨¡å—é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨äºå®ç° ID é©±åŠ¨çš„å®šåˆ¶åŒ–ç”Ÿæˆã€‚ä¸ºäº†æ”¯æŒå¾®è°ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°è§„æ¨¡å±æ€§å¹³è¡¡è®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¸¦æœ‰æ ‡è®°æ–‡æœ¬å’Œäººè„¸ ID çš„é«˜è´¨é‡çº¹ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€äº›æŒ‡æ ‡æ¥è¯„ä¼°çº¹ç†çš„å¤šä¸ªæ–¹é¢ã€‚            (4):æœ¬æ–‡æ–¹æ³•åœ¨å¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œå®šé‡å’Œå®šæ€§åˆ†æéƒ½è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</code></pre></li><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³• UVMap-IDï¼Œå®ƒæ˜¯ä¸€ç§å¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆæ¨¡å‹ã€‚ä¸ 2D ä¸­ä¼ ç»Ÿçš„è§„æ¨¡åŒ–è®­ç»ƒæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å»ºè®®å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸äººè„¸èåˆæ¨¡å—é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨äºå®ç° ID é©±åŠ¨çš„å®šåˆ¶åŒ–ç”Ÿæˆã€‚            (2):ä¸ºäº†æ”¯æŒå¾®è°ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†å°è§„æ¨¡å±æ€§å¹³è¡¡è®­ç»ƒæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¸¦æœ‰æ ‡è®°æ–‡æœ¬å’Œäººè„¸ ID çš„é«˜è´¨é‡çº¹ç†ã€‚            (3):æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€äº›æŒ‡æ ‡æ¥è¯„ä¼°çº¹ç†çš„å¤šä¸ªæ–¹é¢ã€‚</code></pre></li></ol><p><strong>8. ç»“è®ºï¼š</strong></p><p><strong>(1) æ„ä¹‰ï¼š</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆæ¨¡å‹ UVMap-IDï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®ç»™å®šçš„äººè„¸å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–çš„çº¹ç†è´´å›¾ï¼Œå¹¶æ”¯æŒå¯æ§ç”Ÿæˆã€‚è¯¥æ¨¡å‹ä¸º ID é©±åŠ¨çš„å®šåˆ¶åŒ– 3D äººä½“çº¹ç†ç”Ÿæˆæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p><p><strong>(2) ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š</strong></p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>å°†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åº”ç”¨äº UV è´´å›¾ç”Ÿæˆã€‚</li><li>æå‡ºäº†ä¸€ç§äººè„¸èåˆæ¨¡å—ï¼Œå®ç° ID é©±åŠ¨çš„å®šåˆ¶åŒ–ç”Ÿæˆã€‚</li><li>å¼•å…¥äº†å°è§„æ¨¡å±æ€§å¹³è¡¡è®­ç»ƒæ•°æ®é›†ï¼Œæ”¯æŒå¾®è°ƒç­–ç•¥ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¯æ§ä¸”ä¸ªæ€§åŒ–çš„ UV è´´å›¾ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚</li><li>è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–å’Œå¯æ§çš„çº¹ç†è´´å›¾ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è¯¥æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li><li>å¼•å…¥çš„äººè„¸èåˆæ¨¡å—å¢åŠ äº†æ¨¡å‹çš„å¤æ‚æ€§ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-ee8312e5d6ec47e140dd213091cce823.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bfcb973c1970f426d8f1df5728d85885.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e943c5e2becd571bbce3de5cb620daba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-82b5601482da7da7458b5456972d0c5b.jpg" align="middle"></details>## Align Your Steps: Optimizing Sampling Schedules in Diffusion Models**Authors:Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis**Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\textit{Align Your Steps}$. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime. [PDF](http://arxiv.org/abs/2404.14507v1) Project page:   https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/**Summary**ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è®¡åˆ’å¯ä»¥æ˜¾è‘—æå‡è¾“å‡ºè´¨é‡ï¼Œä¸”è¯¥æ–¹æ³•é€‚ç”¨äºä¸åŒçš„é‡‡æ ·å™¨ã€å·²è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†ã€‚**Key Takeaways**- é‡‡æ ·è®¡åˆ’åœ¨æ‰©æ•£æ¨¡å‹ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå½±å“è¾“å‡ºè´¨é‡ã€‚- ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–æ±‚è§£å™¨ï¼Œå¿½ç•¥äº†é‡‡æ ·è®¡åˆ’çš„ä¼˜åŒ–ã€‚- æœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ç§åŸç†æ€§æ–¹æ³•æ¥ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è®¡åˆ’ï¼Œç§°ä¸º Align Your Stepsã€‚- è¯¥æ–¹æ³•åˆ©ç”¨éšæœºå¾®ç§¯åˆ†çš„æ–¹æ³•ï¼Œä¸ºä¸åŒçš„æ±‚è§£å™¨ã€è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†æ‰¾åˆ°æœ€ä¼˜é‡‡æ ·è®¡åˆ’ã€‚- å®éªŒè¡¨æ˜ï¼Œä¼˜åŒ–åçš„é‡‡æ ·è®¡åˆ’åœ¨å¤šç§å›¾åƒã€è§†é¢‘å’Œ 2D ç©å…·æ•°æ®åˆæˆåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ‰‹åŠ¨è®¾è®¡çš„é‡‡æ ·è®¡åˆ’ã€‚- è¯¥æ–¹æ³•è¯æ˜äº†é‡‡æ ·è®¡åˆ’ä¼˜åŒ–åœ¨å°‘æ•°æ­¥åˆæˆä¸­çš„æ½œåŠ›ã€‚- è¯¥æ–¹æ³•å¯ä»¥ä¸ä¸åŒçš„é‡‡æ ·å™¨ã€è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†é…åˆä½¿ç”¨ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>æ ‡é¢˜ï¼š</strong>ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ä¸­çš„é‡‡æ ·è®¡åˆ’</p></li><li><p><strong>ä½œè€…ï¼š</strong>Jiahui Yu, Yuchen Lu, Jianwen Xie, Jianwen Xie, Anima Anandkumar</p></li><li><p><strong>ç¬¬ä¸€ä½œè€…å•ä½ï¼š</strong>NVIDIA</p></li><li><p><strong>å…³é”®è¯ï¼š</strong>æ‰©æ•£æ¨¡å‹ã€é‡‡æ ·è®¡åˆ’ã€å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆ</p></li><li><p><strong>é“¾æ¥ï¼š</strong>Paper_info:Align Your Steps: Optimizing Sampling Schedules in Diffusion Models</p></li><li><p><strong>æ‘˜è¦ï¼š</strong></p></li></ol><p>ï¼ˆ1ï¼‰<strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰æ˜¯è§†è§‰é¢†åŸŸåŠå…¶ä»–é¢†åŸŸçš„å…ˆè¿›ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ã€‚DM çš„ä¸€ä¸ªä¸»è¦ç¼ºç‚¹æ˜¯é‡‡æ ·é€Ÿåº¦æ…¢ï¼Œéœ€è¦é€šè¿‡å¤§å‹ç¥ç»ç½‘ç»œè¿›è¡Œè®¸å¤šé¡ºåºå‡½æ•°è¯„ä¼°ã€‚ä» DM ä¸­é‡‡æ ·å¯ä»¥çœ‹ä½œæ˜¯é€šè¿‡ä¸€ç»„ç§°ä¸ºé‡‡æ ·è®¡åˆ’çš„ç¦»æ•£å™ªå£°ç”µå¹³æ¥æ±‚è§£å¾®åˆ†æ–¹ç¨‹ã€‚è™½ç„¶è¿‡å»çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ¨å¯¼æœ‰æ•ˆçš„æ±‚è§£å™¨ä¸Šï¼Œä½†å¾ˆå°‘å…³æ³¨å¯»æ‰¾æœ€ä½³é‡‡æ ·è®¡åˆ’ï¼Œå¹¶ä¸”æ•´ä¸ªæ–‡çŒ®éƒ½ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„å¯å‘å¼æ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰<strong>è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼š</strong>è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¨å¯¼æœ‰æ•ˆçš„æ±‚è§£å™¨ä¸Šï¼Œä½†å¾ˆå°‘å…³æ³¨å¯»æ‰¾æœ€ä½³é‡‡æ ·è®¡åˆ’ï¼Œå¹¶ä¸”æ•´ä¸ªæ–‡çŒ®éƒ½ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„å¯å‘å¼æ–¹æ³•ã€‚</p><p>ï¼ˆ3ï¼‰<strong>æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§é€šç”¨ä¸”åŸåˆ™æ€§çš„æ–¹æ³•æ¥ä¼˜åŒ– DM çš„é‡‡æ ·è®¡åˆ’ä»¥è·å¾—é«˜è´¨é‡çš„è¾“å‡ºï¼Œç§°ä¸º Align Your Stepsã€‚æˆ‘ä»¬åˆ©ç”¨éšæœºå¾®ç§¯åˆ†çš„æ–¹æ³•ï¼Œé’ˆå¯¹ä¸åŒçš„æ±‚è§£å™¨ã€è®­ç»ƒè¿‡çš„ DM å’Œæ•°æ®é›†æ‰¾åˆ°æœ€ä¼˜çš„è®¡åˆ’ã€‚</p><p>ï¼ˆ4ï¼‰<strong>ä»»åŠ¡å’Œæ€§èƒ½ï¼š</strong>æˆ‘ä»¬åœ¨å¤šä¸ªå›¾åƒã€è§†é¢‘ä»¥åŠ 2D ç©å…·æ•°æ®åˆæˆåŸºå‡†ä¸Šä½¿ç”¨å„ç§ä¸åŒçš„é‡‡æ ·å™¨è¯„ä¼°äº†æˆ‘ä»¬æ–°é¢–çš„æ–¹æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°æˆ‘ä»¬çš„ä¼˜åŒ–è®¡åˆ’åœ¨å‡ ä¹æ‰€æœ‰å®éªŒä¸­éƒ½ä¼˜äºä»¥å‰æ‰‹å·¥åˆ¶ä½œçš„è®¡åˆ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†é‡‡æ ·è®¡åˆ’ä¼˜åŒ–å°šæœªå¼€å‘çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å°‘æ­¥åˆæˆé¢†åŸŸã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li><strong>ç»“è®ºï¼š</strong></li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¼˜åŒ–æ‰©æ•£æ¨¡å‹é‡‡æ ·è®¡åˆ’çš„é€šç”¨ä¸”åŸåˆ™æ€§çš„æ–¹æ³•ï¼Œç§°ä¸º Align Your Stepsï¼Œè¯¥æ–¹æ³•åˆ©ç”¨éšæœºå¾®ç§¯åˆ†çš„æ–¹æ³•ï¼Œé’ˆå¯¹ä¸åŒçš„æ±‚è§£å™¨ã€è®­ç»ƒè¿‡çš„ DM å’Œæ•°æ®é›†æ‰¾åˆ°æœ€ä¼˜çš„è®¡åˆ’ã€‚</p><p>ï¼ˆ2ï¼‰ï¼š<strong>åˆ›æ–°ç‚¹ï¼š</strong>æå‡ºäº†ä¼˜åŒ–æ‰©æ•£æ¨¡å‹é‡‡æ ·è®¡åˆ’çš„æ–°é¢–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§å’ŒåŸåˆ™æ€§ï¼Œå¯ä»¥é’ˆå¯¹ä¸åŒçš„æ±‚è§£å™¨ã€è®­ç»ƒè¿‡çš„ DM å’Œæ•°æ®é›†æ‰¾åˆ°æœ€ä¼˜çš„è®¡åˆ’ã€‚<strong>æ€§èƒ½ï¼š</strong>åœ¨å¤šä¸ªå›¾åƒã€è§†é¢‘ä»¥åŠ 2D ç©å…·æ•°æ®åˆæˆåŸºå‡†ä¸Šä½¿ç”¨å„ç§ä¸åŒçš„é‡‡æ ·å™¨è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°è¯¥æ–¹æ³•åœ¨å‡ ä¹æ‰€æœ‰å®éªŒä¸­éƒ½ä¼˜äºä»¥å‰æ‰‹å·¥åˆ¶ä½œçš„è®¡åˆ’ã€‚<strong>å·¥ä½œé‡ï¼š</strong>è¯¥æ–¹æ³•éœ€è¦å¯¹é‡‡æ ·è®¡åˆ’è¿›è¡Œä¼˜åŒ–ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-78c3e80bc513a591cd16c1be135f16cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-97a5c3e11f2d9cffcd1a13c8baf1c9c7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ef643065c4d76e29b9b077c68693835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb7137766bc6a2a4fee323a9d77c6bff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eeb97492f52508534aa4f55180d1531f.jpg" align="middle"></details>## GeoDiffuser: Geometry-Based Image Editing with Diffusion Models**Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar**The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information. [PDF](http://arxiv.org/abs/2404.14403v1) **æ‘˜è¦**ä¸€é”®å¼å›¾åƒç¼–è¾‘æ–¹æ³• GeoDiffuserï¼Œå°† 2D/3D å¯¹è±¡ç¼–è¾‘ç»Ÿä¸€ä¸ºå‡ ä½•å˜æ¢ï¼Œæ— éœ€è®­ç»ƒæˆ–é¢å¤–ä¿¡æ¯ã€‚**å…³é”®è¦ç‚¹**- å°†å›¾åƒç¼–è¾‘æ“ä½œè§†ä¸ºå‡ ä½•å˜æ¢ï¼Œå¯ç›´æ¥èåˆåˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚- æ— éœ€è®­ç»ƒçš„ä¼˜åŒ–å‡½æ•°ï¼Œå¯ä¿ç•™å¯¹è±¡é£æ ¼å¹¶ç”Ÿæˆåˆç†å›¾åƒã€‚- ä¿®å¤å› å¯¹è±¡ç¼–è¾‘è€Œäº§ç”Ÿçš„å›¾åƒé®æŒ¡éƒ¨åˆ†ã€‚- ä½¿ç”¨åˆ†å‰²å’Œå˜æ¢ä¼°è®¡æ¥ç¼–è¾‘å‰æ™¯å¯¹è±¡ã€‚- å¯æ‰§è¡Œå¸¸è§ 2D/3D ç¼–è¾‘ï¼Œå¦‚å¹³ç§»ã€æ—‹è½¬å’Œç§»é™¤ã€‚- å®šé‡å’Œæ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚- æ›´å¤šä¿¡æ¯è¯·è®¿é—® https://ivl.cs.brown.edu/research/geodiffuser.htmlã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šåŸºäºå‡ ä½•çš„å›¾åƒç¼–è¾‘ä¸ GeoDiffuserï¼ˆè¡¥å……ï¼‰</p></li><li><p>ä½œè€…ï¼š</p></li><li>Yin Cui</li><li>Yujun Shen</li><li>Yinda Zhang</li><li>Bolei Zhou</li><li>Chen Change Loy</li><li><p>Thomas Funkhouser</p></li><li><p>ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ–°åŠ å¡å›½ç«‹å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼š</p></li><li>å›¾åƒç¼–è¾‘</li><li>å‡ ä½•å˜æ¢</li><li>æ‰©æ•£æ¨¡å‹</li><li><p>é›¶æ ·æœ¬å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.14403   Github é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>(1) ç ”ç©¶èƒŒæ™¯ï¼š   éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„æˆåŠŸï¼ŒåŸºäºæ–‡æœ¬æˆ–å…¶ä»–ç”¨æˆ·è¾“å…¥ç¼–è¾‘å›¾åƒçš„æ–¹æ³•å¾—åˆ°äº†å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆæ˜¯å®šåˆ¶çš„ã€ä¸ç²¾ç¡®çš„ï¼Œè¦ä¹ˆéœ€è¦é¢å¤–çš„ä¿¡æ¯ï¼Œæˆ–è€…ä»…é™äº 2D å›¾åƒç¼–è¾‘ã€‚</p><p>(2) è¿‡å»æ–¹æ³•åŠå…¶é—®é¢˜ï¼š   ç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š   - <strong>å®šåˆ¶æ€§</strong>ï¼šéœ€è¦ä¸ºæ¯ä¸ªç¼–è¾‘æ“ä½œè®¾è®¡ç‰¹å®šçš„æ¨¡å‹ã€‚   - <strong>ä¸ç²¾ç¡®</strong>ï¼šéš¾ä»¥ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç²¾ç¡®ç¼–è¾‘ã€‚   - <strong>éœ€è¦é¢å¤–ä¿¡æ¯</strong>ï¼šå¯èƒ½éœ€è¦å¯¹è±¡æ©ç æˆ– 3D æ¨¡å‹ç­‰é™„åŠ ä¿¡æ¯ã€‚   - <strong>2D é™åˆ¶</strong>ï¼šä»…é™äº 2D å›¾åƒç¼–è¾‘ï¼Œæ— æ³•å¤„ç† 3D æ—‹è½¬ç­‰æ“ä½œã€‚</p><p>(3) æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   GeoDiffuser æ˜¯ä¸€ç§åŸºäºé›¶æ ·æœ¬ä¼˜åŒ–çš„å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œå®ƒå°†å¸¸è§çš„ 2D å’Œ 3D å›¾åƒç¼–è¾‘åŠŸèƒ½ç»Ÿä¸€åˆ°ä¸€ä¸ªæ–¹æ³•ä¸­ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒç¼–è¾‘æ“ä½œè§†ä¸ºå‡ ä½•å˜æ¢ï¼Œå¹¶å°†å…¶ç›´æ¥èå…¥æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚ä¸­ã€‚GeoDiffuser ä½¿ç”¨ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°æ—¨åœ¨ä¿ç•™å¯¹è±¡æ ·å¼ï¼ŒåŒæ—¶ç”Ÿæˆåˆç†ä¸”å…·æœ‰å‡†ç¡®å…‰å½±æ•ˆæœçš„å›¾åƒã€‚å®ƒè¿˜å¯ä»¥ä¿®å¤å¯¹è±¡åŸå…ˆæ‰€åœ¨ä½ç½®çš„é®æŒ¡éƒ¨åˆ†ã€‚</p><p>(4) æ–¹æ³•æ€§èƒ½ï¼š   GeoDiffuser åœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸Šå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š   - <strong>2D ç¼–è¾‘</strong>ï¼šå¯¹è±¡å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬   - <strong>3D ç¼–è¾‘</strong>ï¼šå¯¹è±¡ 3D æ—‹è½¬ã€ç§»é™¤   å®šé‡å’Œæ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼ŒGeoDiffuser ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³• GeoDiffuserï¼Œå®ƒå¯ä»¥å¯¹å›¾åƒè¿›è¡Œå¸¸è§çš„ 2D å’Œ 3D å¯¹è±¡ç¼–è¾‘ã€‚è¯¥æ–¹æ³•åŸºäºé›¶æ ·æœ¬ä¼˜åŒ–ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å®ç°è¿™äº›ç¼–è¾‘ã€‚å…¶å…³é”®æ€æƒ³æ˜¯å°†å›¾åƒç¼–è¾‘è¡¨è¿°ä¸ºå‡ ä½•å˜æ¢ï¼Œå¹¶å°†å…¶ç›´æ¥çº³å…¥åŸºäºæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æ¡†æ¶ä¸­çš„å…±äº«æ³¨æ„åŠ›å±‚ä¸­ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å•ä¸€æ–¹æ³•å¯ä»¥å¤„ç†å„ç§å›¾åƒç¼–è¾‘æ“ä½œï¼Œå¹¶ä¸”ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”äº§ç”Ÿäº†æ›´å¥½çš„ç»“æœã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šGeoDiffuser ç»Ÿä¸€äº† 2D å’Œ 3D å›¾åƒç¼–è¾‘æ“ä½œï¼Œå¹¶å°†å…¶è¡¨è¿°ä¸ºå‡ ä½•å˜æ¢ã€‚å®ƒç›´æ¥å°†å‡ ä½•å˜æ¢çº³å…¥æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›å±‚ä¸­ï¼Œæ— éœ€ä¸ºæ¯ä¸ªç¼–è¾‘æ“ä½œè®¾è®¡ç‰¹å®šçš„æ¨¡å‹ã€‚</p><p>æ€§èƒ½ï¼šGeoDiffuser åœ¨å„ç§ç¼–è¾‘ä»»åŠ¡ä¸Šå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ 2D ç¼–è¾‘ï¼ˆå¯¹è±¡å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ï¼‰å’Œ 3D ç¼–è¾‘ï¼ˆå¯¹è±¡ 3D æ—‹è½¬ã€ç§»é™¤ï¼‰ã€‚å®šé‡å’Œæ„ŸçŸ¥ç ”ç©¶è¡¨æ˜ï¼ŒGeoDiffuser ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p><p>å·¥ä½œé‡ï¼šGeoDiffuser çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚å®ƒåªéœ€è¦ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°æ—¨åœ¨ä¿ç•™å¯¹è±¡æ ·å¼ï¼ŒåŒæ—¶ç”Ÿæˆåˆç†ä¸”å…·æœ‰å‡†ç¡®å…‰å½±æ•ˆæœçš„å›¾åƒã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-907b6b9c901d5ba4cb979b85f016e4e1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b67e8d6c53f98ede5eceba2ceea75149.jpg" align="middle"></details>## MultiBooth: Towards Generating All Your Concepts in an Image from Text**Authors:Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu**This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/ [PDF](http://arxiv.org/abs/2404.14239v1) Project Page: https://multibooth.github.io/ . Github Page:   https://github.com/chenyangzhu1/MultiBooth**Summary**å¤šæ¦‚å¿µå›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•MultiBoothå°†å•æ¦‚å¿µå­¦ä¹ å’Œå¤šæ¦‚å¿µæ•´åˆç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆä¸­çš„å¤šæ¦‚å¿µè‡ªå®šä¹‰çš„æ•ˆç‡å’Œä¿çœŸåº¦ã€‚**Key Takeaways**- MultiBoothå°†å¤šæ¦‚å¿µç”Ÿæˆåˆ†ä¸ºå•æ¦‚å¿µå­¦ä¹ å’Œå¤šæ¦‚å¿µæ•´åˆä¸¤é˜¶æ®µï¼Œæé«˜äº†æ¦‚å¿µä¿çœŸåº¦å’Œæ¨ç†æ•ˆç‡ã€‚- å•æ¦‚å¿µå­¦ä¹ é˜¶æ®µä½¿ç”¨å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨å’Œé«˜æ•ˆæ¦‚å¿µç¼–ç æŠ€æœ¯ï¼Œä¸ºæ¯ä¸ªæ¦‚å¿µå­¦ä¹ ç®€æ´ä¸”åŒºåˆ«æ€§çš„è¡¨ç¤ºã€‚- å¤šæ¦‚å¿µæ•´åˆé˜¶æ®µä½¿ç”¨è¾¹ç•Œæ¡†å®šä¹‰äº¤å‰æ³¨æ„å›¾ä¸­æ¯ä¸ªæ¦‚å¿µçš„ç”ŸæˆåŒºåŸŸã€‚- è¿™ç§æ–¹æ³•å…è®¸åœ¨æŒ‡å®šåŒºåŸŸå†…åˆ›å»ºå•ä¸ªæ¦‚å¿µï¼Œä»è€Œç”Ÿæˆå¤šæ¦‚å¿µå›¾åƒã€‚- MultiBoothåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºå„ç§åŸºçº¿ï¼Œè¯æ˜äº†å…¶å‡ºè‰²çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚- MultiBoothå¯é€šè¿‡å…¶é¡¹ç›®é¡µé¢è®¿é—®ï¼šhttps://multibooth.github.io/ã€‚- MultiBoothå¼€è¾Ÿäº†å¤šæ¦‚å¿µè‡ªå®šä¹‰å›¾åƒç”Ÿæˆçš„æ–°é€”å¾„ï¼Œä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢å¥ å®šäº†åŸºç¡€ã€‚- è¯¥æ–¹æ³•æœ‰æœ›åœ¨å›¾åƒåˆæˆã€ç¼–è¾‘å’Œè®¾è®¡ç­‰åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šMultiBoothï¼šä»æ–‡æœ¬ä¸­ç”Ÿæˆå›¾åƒä¸­æ‰€æœ‰æ¦‚å¿µ</p></li><li><p>ä½œè€…ï¼šChenyang Zhu, Kai Li, Yue Ma, Chunming He, Xiu Li</p></li><li><p>å•ä½ï¼šæ¸…åå¤§å­¦</p></li><li><p>Keywords: Multi-concept generation, Image generation, Text-to-image, Diffusion models</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.14239v1 Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šéšç€æ‰©æ•£æ¨¡å‹çš„æˆåŠŸï¼Œå®šåˆ¶åŒ–ç”Ÿæˆæ–¹æ³•å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤šæ¦‚å¿µåœºæ™¯ä¸­å¾€å¾€é¢ä¸´æ¦‚å¿µä¿çœŸåº¦ä½ã€æ¨ç†æˆæœ¬é«˜çš„éš¾é¢˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡è”åˆå­¦ä¹ æ‰€æœ‰æ¦‚å¿µæ¥ç”Ÿæˆå¤šæ¦‚å¿µå›¾åƒï¼Œè¿™ä¼šå¯¼è‡´æ¦‚å¿µä¿çœŸåº¦ä½ã€æ¨ç†æˆæœ¬é«˜ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•ï¼šMultiBooth å°†å¤šæ¦‚å¿µç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå•æ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œå¤šæ¦‚å¿µé›†æˆé˜¶æ®µã€‚åœ¨å•æ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼Œé‡‡ç”¨å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨å’Œé«˜æ•ˆçš„æ¦‚å¿µç¼–ç æŠ€æœ¯ï¼Œä¸ºæ¯ä¸ªæ¦‚å¿µå­¦ä¹ ä¸€ä¸ªç®€æ´ä¸”æœ‰åŒºåˆ«çš„è¡¨ç¤ºã€‚åœ¨å¤šæ¦‚å¿µé›†æˆé˜¶æ®µï¼Œä½¿ç”¨è¾¹ç•Œæ¡†åœ¨äº¤å‰æ³¨æ„åŠ›å›¾ä¸­ä¸ºæ¯ä¸ªæ¦‚å¿µå®šä¹‰ç”ŸæˆåŒºåŸŸã€‚è¿™ç§æ–¹æ³•å¯ä»¥åˆ›å»ºå„ä¸ªæ¦‚å¿µçš„ç‹¬ç«‹è¡¨ç¤ºï¼Œå¹¶å°†å…¶é›†æˆåˆ°æœ€ç»ˆå›¾åƒä¸­ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šå®éªŒç»“æœï¼šåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒMultiBooth åœ¨å¤æ‚çš„å¤šæ¦‚å¿µç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬é‡å¡‘é£æ ¼ã€ä¸åŒç©ºé—´å…³ç³»å’Œé‡æ–°è¯­å¢ƒåŒ–ï¼Œéƒ½èƒ½æœ‰æ•ˆåœ°ä¿æŒè¾ƒé«˜çš„å›¾åƒä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½åº¦ã€‚</p><ol><li>Methods:</li></ol><p>ï¼ˆ1ï¼‰ï¼šMultiBoothå°†å¤šæ¦‚å¿µç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºå•æ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œå¤šæ¦‚å¿µé›†æˆé˜¶æ®µï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåœ¨å•æ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼Œé‡‡ç”¨å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨å’Œé«˜æ•ˆçš„æ¦‚å¿µç¼–ç æŠ€æœ¯ï¼Œä¸ºæ¯ä¸ªæ¦‚å¿µå­¦ä¹ ä¸€ä¸ªç®€æ´ä¸”æœ‰åŒºåˆ«çš„è¡¨ç¤ºï¼›</p><p>ï¼ˆ3ï¼‰ï¼šåœ¨å¤šæ¦‚å¿µé›†æˆé˜¶æ®µï¼Œä½¿ç”¨è¾¹ç•Œæ¡†åœ¨äº¤å‰æ³¨æ„åŠ›å›¾ä¸­ä¸ºæ¯ä¸ªæ¦‚å¿µå®šä¹‰ç”ŸæˆåŒºåŸŸï¼Œå°†å„ä¸ªæ¦‚å¿µçš„ç‹¬ç«‹è¡¨ç¤ºé›†æˆåˆ°æœ€ç»ˆå›¾åƒä¸­ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–é«˜æ•ˆçš„å¤šæ¦‚å¿µå®šåˆ¶ï¼ˆMCCï¼‰æ¡†æ¶ MultiBoothã€‚ä¸ç°æœ‰ MCC æ–¹æ³•ç›¸æ¯”ï¼ŒMultiBooth å…è®¸å³æ’å³ç”¨çš„å¤šæ¦‚å¿µç”Ÿæˆï¼Œå…·æœ‰è¾ƒé«˜çš„å›¾åƒä¿çœŸåº¦ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´å¸¦æ¥çš„æˆæœ¬æœ€å°ã€‚é€šè¿‡è¿›è¡Œå®šæ€§å’Œå®šé‡å®éªŒï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„å¤šä¸»é¢˜å®šåˆ¶åœºæ™¯ä¸­ç¨³å¥åœ°è¯æ˜äº†æˆ‘ä»¬ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç”±äºå½“å‰æ–¹æ³•ä»ç„¶éœ€è¦è®­ç»ƒæ¥å­¦ä¹ æ–°æ¦‚å¿µï¼Œå› æ­¤åœ¨æœªæ¥ï¼Œæˆ‘ä»¬å°†åœ¨ MultiBooth çš„åŸºç¡€ä¸Šç ”ç©¶å…è®­ç»ƒå¤šæ¦‚å¿µå®šåˆ¶çš„ä»»åŠ¡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šå°†å¤šæ¦‚å¿µç”Ÿæˆè¿‡ç¨‹åˆ†ä¸ºå•æ¦‚å¿µå­¦ä¹ é˜¶æ®µå’Œå¤šæ¦‚å¿µé›†æˆé˜¶æ®µï¼Œä¸ºæ¯ä¸ªæ¦‚å¿µå­¦ä¹ ç®€æ´ä¸”æœ‰åŒºåˆ«çš„è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨è¾¹ç•Œæ¡†åœ¨äº¤å‰æ³¨æ„åŠ›å›¾ä¸­ä¸ºæ¯ä¸ªæ¦‚å¿µå®šä¹‰ç”ŸæˆåŒºåŸŸï¼›æ€§èƒ½ï¼šåœ¨å¤æ‚çš„å¤šæ¦‚å¿µç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬é‡å¡‘é£æ ¼ã€ä¸åŒç©ºé—´å…³ç³»å’Œé‡æ–°è¯­å¢ƒåŒ–ï¼Œéƒ½èƒ½æœ‰æ•ˆåœ°ä¿æŒè¾ƒé«˜çš„å›¾åƒä¿çœŸåº¦å’Œæ–‡æœ¬å¯¹é½åº¦ï¼›å·¥ä½œé‡ï¼šåœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´å¸¦æ¥çš„æˆæœ¬æœ€å°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-cd95a012d10b3a0932405f01c119cafb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64d8c2e719edd54a8907366e1adc0ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-383cf5bbab5f26be5446db713c454caf.jpg" align="middle"></details>## FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on**Authors:Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, Hongming Shan**Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details. [PDF](http://arxiv.org/abs/2404.14162v1) Accepted by IJCAI 2024**Summary**åˆ©ç”¨ç»å˜å½¢å¤„ç†çš„åˆå§‹å˜å½¢åŠå±€éƒ¨æ¡ä»¶ï¼Œé…åˆæœé¥°å±•å¹³ç½‘ç»œåŠæœé¥°åéªŒé‡‡æ ·ï¼Œæå‡ºä¸€ç§å¿ å®çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ FLMD-VTONï¼Œæ˜¾è‘—æå‡è™šæ‹Ÿè¯•ç©¿æ¨¡å‹çš„ç”Ÿæˆä¿çœŸåº¦ã€‚**Key Takeaways**- ç»“åˆå±€éƒ¨æ¡ä»¶å’Œç»å˜å½¢å¤„ç†çš„åˆå§‹æœé¥°ï¼Œä¸ºæ¨¡å‹æä¾›å¯é çš„æœé¥°å…ˆéªŒä¿¡æ¯ã€‚- å¼•å…¥æœé¥°å±•å¹³ç½‘ç»œï¼Œçº¦æŸç”Ÿæˆå›¾åƒï¼Œç¡®ä¿æœé¥°å˜å½¢çš„ä¸€è‡´æ€§ã€‚- é‡‡ç”¨æœé¥°åéªŒé‡‡æ ·ï¼Œæå‡æ¨¡å‹æ€§èƒ½ï¼Œä¼˜äºä¼ ç»Ÿçš„é«˜æ–¯é‡‡æ ·ã€‚- æ–¹æ³•åœ¨ VITON-HD å’Œ Dress Code æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”Ÿæˆçš„ç…§ç‰‡çº§è™šæ‹Ÿè¯•ç©¿å›¾åƒï¼Œæœé¥°ç»†èŠ‚çœŸå®åº¦é«˜ã€‚- æ–¹æ³•æ”¹å–„äº†åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è™šæ‹Ÿè¯•ç©¿æ–¹æ³•åœ¨æœé¥°é£æ ¼ã€å›¾æ¡ˆå’Œæ–‡å­—ç»†èŠ‚æ–¹é¢çš„ä¸è¶³ã€‚- æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚- æ–¹æ³•å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯åœ¨ä¸åŒæ•°æ®é›†ä¸Šç”Ÿæˆé€¼çœŸçš„è™šæ‹Ÿè¯•ç©¿å›¾åƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>è®ºæ–‡æ ‡é¢˜</strong>ï¼šFLDM-VTONï¼šç”¨äºè™šæ‹Ÿè¯•ç©¿çš„å¿ å®æ½œåœ¨æ‰©æ•£æ¨¡å‹</li><li><strong>ä½œè€…</strong>ï¼šç‹æ™¨è¾‰ã€é™ˆæ¶›ã€é™ˆå¿—æµ©ã€é»„å¿—å¿ ã€å§œæ¶›ç„¶ã€ç‹ç¦ã€å•å®æ˜</li><li><strong>ç¬¬ä¸€ä½œè€…å•ä½</strong>ï¼šå¤æ—¦å¤§å­¦è„‘ç§‘å­¦ä¸ç±»è„‘æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯ç ”ç©¶é™¢</li><li><strong>å…³é”®è¯</strong>ï¼šVirtual Try-on (VTON)ã€Latent Diffusion Modelã€Faithful Details</li><li><strong>è®ºæ–‡é“¾æ¥</strong>ï¼šhttps://arxiv.org/abs/2404.14162</li><li><p><strong>æ‘˜è¦</strong>ï¼š   ï¼ˆ1ï¼‰<strong>ç ”ç©¶èƒŒæ™¯</strong>ï¼šè™šæ‹Ÿè¯•ç©¿ï¼ˆVTONï¼‰æ—¨åœ¨å°†ä¸€ä»¶å•†åº—é‡Œçš„å¹³é“ºè¡£æœè½¬ç§»åˆ°äººä½“ä¸Šï¼ŒåŒæ—¶ä¿ç•™äººä½“å’Œè¡£æœçš„ç»†èŠ‚ï¼Œå¦‚æ¬¾å¼ã€å›¾æ¡ˆå’Œæ–‡å­—ã€‚   ï¼ˆ2ï¼‰<strong>è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜</strong>ï¼šå…ˆå‰çš„VTONæ–¹æ³•é«˜åº¦ä¾èµ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œä½†ç”±äºæ¨¡å¼å´©å¡Œé—®é¢˜ï¼ŒGANæ–¹æ³•æ— æ³•åˆæˆé€¼çœŸçš„è¯•ç©¿å›¾åƒï¼Œä¹Ÿæ— æ³•å‡†ç¡®æ•æ‰å¤æ‚çš„æœè£…ç»†èŠ‚ã€‚   ï¼ˆ3ï¼‰<strong>æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•</strong>ï¼šFLDM-VTONæ”¹è¿›äº†ä¼ ç»Ÿçš„æ½œåœ¨æ‰©æ•£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ï¼šä½¿ç”¨å˜å½¢åçš„è¡£æœä½œä¸ºèµ·ç‚¹å’Œå±€éƒ¨æ¡ä»¶ï¼Œä¸ºæ¨¡å‹æä¾›å¿ å®çš„è¡£æœå…ˆéªŒï¼›å¼•å…¥äº†ä¸€ç§æ–°çš„è¡£æœå±•å¹³ç½‘ç»œæ¥çº¦æŸç”Ÿæˆçš„è¯•ç©¿å›¾åƒï¼Œæä¾›ä¸è¡£æœä¸€è‡´çš„å¿ å®ç›‘ç£ï¼›è®¾è®¡äº†ä¸€ç§ç”¨äºå¿ å®æ¨ç†çš„è¡£æœåéªŒé‡‡æ ·ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚   ï¼ˆ4ï¼‰<strong>æ–¹æ³•çš„æ€§èƒ½</strong>ï¼šåœ¨VITON-HDå’ŒDress CodeåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒFLDM-VTONä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¿ å®æœè£…ç»†èŠ‚çš„é€¼çœŸè¯•ç©¿å›¾åƒã€‚</p></li><li><p>æ–¹æ³•ï¼š   ï¼ˆ1ï¼‰ï¼šæå‡ºFLDM-VTONï¼Œåˆ©ç”¨å˜å½¢åçš„è¡£æœä½œä¸ºèµ·ç‚¹å’Œå±€éƒ¨æ¡ä»¶ï¼Œä¸ºæ¨¡å‹æä¾›é€¼çœŸçš„è¡£æœå…ˆéªŒï¼›   ï¼ˆ2ï¼‰ï¼šå¼•å…¥è¡£æœå±•å¹³ç½‘ç»œï¼Œçº¦æŸç”Ÿæˆçš„è¯•ç©¿å›¾åƒï¼Œæä¾›ä¸è¡£æœä¸€è‡´çš„ç›‘ç£ï¼›   ï¼ˆ3ï¼‰ï¼šè®¾è®¡ç”¨äºå¿ å®æ¨ç†çš„è¡£æœåéªŒé‡‡æ ·ï¼Œè¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼š                    (1): æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºè™šæ‹Ÿè¯•ç©¿çš„æ–°å‹å¿ å®æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆFLDM-VTONï¼‰ã€‚é€šè¿‡å¼•å…¥å¿ å®çš„è¡£æœå…ˆéªŒå’Œä¸è¡£æœä¸€è‡´çš„å¿ å®ç›‘ç£ï¼ŒFLDM-VTONå¯ä»¥æ˜¾è‘—ç¼“è§£ç”±æ‰©æ•£éšæœºæ€§å’Œæ½œåœ¨ç›‘ç£åœ¨LDMä¸­å¼•èµ·çš„éå¿ å®ç”Ÿæˆé—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºå¿ å®æ¨ç†è®¾è®¡çš„è¡£æœåéªŒé‡‡æ ·å¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªæµè¡Œçš„VTONåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„FLDM-VTONçš„ä¼˜è¶Šæ€§èƒ½â€”â€”ç”Ÿæˆå…·æœ‰å¿ å®æœè£…ç»†èŠ‚çš„é€¼çœŸçš„è¯•ç©¿å›¾åƒã€‚</p><pre><code>            (2): åˆ›æ–°ç‚¹ï¼šæå‡ºFLDM-VTONï¼Œåˆ©ç”¨å˜å½¢åçš„è¡£æœä½œä¸ºèµ·ç‚¹å’Œå±€éƒ¨æ¡ä»¶ï¼Œä¸ºæ¨¡å‹æä¾›é€¼çœŸçš„è¡£æœå…ˆéªŒï¼›å¼•å…¥è¡£æœå±•å¹³ç½‘ç»œï¼Œçº¦æŸç”Ÿæˆçš„è¯•ç©¿å›¾åƒï¼Œæä¾›ä¸è¡£æœä¸€è‡´çš„ç›‘ç£ï¼›è®¾è®¡ç”¨äºå¿ å®æ¨ç†çš„è¡£æœåéªŒé‡‡æ ·ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚             æ€§èƒ½ï¼šåœ¨VITON-HDå’ŒDress CodeåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒFLDM-VTONä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼Œç”Ÿæˆå…·æœ‰å¿ å®æœè£…ç»†èŠ‚çš„é€¼çœŸè¯•ç©¿å›¾åƒã€‚             å·¥ä½œé‡ï¼šæœ¬æ–‡æ–¹æ³•çš„å®ç°éœ€è¦å¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼ŒåŒ…æ‹¬å¼•å…¥è¡£æœå…ˆéªŒã€è¡£æœå±•å¹³ç½‘ç»œå’Œè¡£æœåéªŒé‡‡æ ·ã€‚è¿™äº›ä¿®æ”¹éœ€è¦é¢å¤–çš„è®¡ç®—å’Œå­˜å‚¨èµ„æºã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-07afb8a5c475fd0a30e88cadcbad3463.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-145ab18d23fb1b6e86d6406676978723.jpg" align="middle"><img src="https://picx.zhimg.com/v2-228856708a79714b6f7dccab9f678905.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fd4472fcb73295f29ca0dce6c278c461.jpg" align="middle"><img src="https://picx.zhimg.com/v2-089f4d17146ab3e178994ba211043f04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-88494892272dbea22b24aaca153feca1.jpg" align="middle"></details><h2 id="Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model"><a href="#Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model" class="headerlink" title="Accelerating Image Generation with Sub-path Linear Approximation Model"></a>Accelerating Image Generation with Sub-path Linear Approximation Model</h2><p><strong>Authors:Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</strong></p><p>Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images. </p><p><a href="http://arxiv.org/abs/2404.13903v2">PDF</a> </p><p><strong>Summary:</strong><br>æ‰©æ•£æ¨¡å‹æé€Ÿæ–°æ–¹æ³•ï¼šå­è·¯å¾„çº¿æ€§é€¼è¿‘æ¨¡å‹ï¼ˆSLAMï¼‰</p><p><strong>Key Takeaways:</strong></p><ul><li>SLAMé‡‡ç”¨åˆ†æ²»ç­–ç•¥ï¼Œå°†æ‰©æ•£è·¯å¾„åˆ’åˆ†ä¸ºå­è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å­è·¯å¾„çº¿æ€§ODEè¿›è¡Œé€¼è¿‘ã€‚</li><li>SLAMæ„å»ºå»å™ªæ˜ å°„ï¼Œç´¯è®¡è¯¯å·®æ›´å°ï¼Œç”Ÿæˆæ•ˆæœæ›´å¥½ã€‚</li><li>SLAMå¯æœ‰æ•ˆæé€Ÿï¼Œä»…éœ€6ä¸ªA100 GPUå¤©å³å¯è®­ç»ƒå‡ºé«˜è´¨é‡ç”Ÿæˆæ¨¡å‹ã€‚</li><li>SLAMæ”¯æŒå°‘æ•°æ­¥ç”Ÿæˆä»»åŠ¡ï¼Œåœ¨FIDå’Œç”Ÿæˆå›¾åƒè´¨é‡ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li><li>SLAMèƒ½æœ‰æ•ˆè®­ç»ƒéšå¼æ‰©æ•£æ¨¡å‹ã€‚</li><li>SLAMæ¯”ç°æœ‰åŠ é€Ÿæ–¹æ³•æ›´æœ‰æ•ˆï¼Œåœ¨å°‘æ•°æ­¥ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚</li><li>SLAMç”Ÿæˆçš„é«˜è´¨é‡å›¾åƒé€‚ç”¨äºå›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: SLAMåŠ é€Ÿå›¾åƒç”Ÿæˆ</p></li><li><p>Authors: Zhiming Zhou, Yixing Xu, Zhiyuan Fang, Yufei Wang, Yifan Jiang, Xinchao Wang, Xiangyang Xue</p></li><li><p>Affiliation: åŒ—äº¬å¤§å­¦</p></li><li><p>Keywords: Diffusion Models Â· Accelerating Diffusion Models Â· Diffusion Model Distillation Â· Consistency Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.07523, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®é™…åœºæ™¯ä¸­çš„åº”ç”¨å—åˆ°æ¨ç†é€Ÿåº¦æ…¢çš„é˜»ç¢ã€‚</p><p>(2): è¿‡å»çš„åŠ é€Ÿæ–¹æ³•åŒ…æ‹¬ï¼šDDIMã€LADMã€LCMã€‚è¿™äº›æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼šDDIMå’ŒLADMçš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦æ…¢ï¼ŒLCMåœ¨ç”Ÿæˆè´¨é‡ä¸Šå­˜åœ¨ä¸€å®šç¼ºé™·ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•åŠ¨æœºæ˜ç¡®ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§å­è·¯å¾„çº¿æ€§é€¼è¿‘æ¨¡å‹ï¼ˆSLAMï¼‰ï¼Œå®ƒé€šè¿‡å°†PF-ODEè½¨è¿¹è§†ä¸ºä¸€ç³»åˆ—ç”±é‡‡æ ·ç‚¹åˆ’åˆ†çš„PF-ODEå­è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å­è·¯å¾„çº¿æ€§ï¼ˆSLï¼‰ODEåœ¨æ¯ä¸ªå•ç‹¬çš„PF-ODEå­è·¯å¾„ä¸Šå½¢æˆæ¸è¿›ä¸”è¿ç»­çš„è¯¯å·®ä¼°è®¡ã€‚å¯¹è¿™äº›SL-ODEçš„ä¼˜åŒ–å…è®¸SLAMæ„å»ºå…·æœ‰è¾ƒå°ç´¯ç§¯è¿‘ä¼¼è¯¯å·®çš„å»å™ªæ˜ å°„ã€‚è¿˜å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„è’¸é¦æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›æ›´é«˜çº§çš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼‰çš„æ•´åˆã€‚</p><p>(4): åœ¨LAIONã€MS COCO 2014å’ŒMS COCO 2017æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒSLAMå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œåªéœ€6ä¸ªA100 GPUå¤©å³å¯ç”Ÿæˆé«˜è´¨é‡çš„ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»¥é«˜æ€§èƒ½è¿›è¡Œ2åˆ°4æ­¥ç”Ÿæˆã€‚å…¨é¢çš„è¯„ä¼°è¿˜è¡¨æ˜ï¼ŒSLAMåœ¨å°æ­¥ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼Œåœ¨FIDå’Œç”Ÿæˆå›¾åƒè´¨é‡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å­è·¯å¾„çº¿æ€§é€¼è¿‘æ¨¡å‹ï¼ˆSLAMï¼‰ï¼Œå®ƒé€šè¿‡å°†PF-ODEè½¨è¿¹è§†ä¸ºä¸€ç³»åˆ—ç”±é‡‡æ ·ç‚¹åˆ’åˆ†çš„PF-ODEå­è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å­è·¯å¾„çº¿æ€§ï¼ˆSLï¼‰ODEåœ¨æ¯ä¸ªå•ç‹¬çš„PF-ODEå­è·¯å¾„ä¸Šå½¢æˆæ¸è¿›ä¸”è¿ç»­çš„è¯¯å·®ä¼°è®¡ã€‚å¯¹è¿™äº›SL-ODEçš„ä¼˜åŒ–å…è®¸SLAMæ„å»ºå…·æœ‰è¾ƒå°ç´¯ç§¯è¿‘ä¼¼è¯¯å·®çš„å»å™ªæ˜ å°„ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿˜å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„è’¸é¦æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›æ›´é«˜çº§çš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼‰çš„æ•´åˆã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ SLAM æ¨¡å‹åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡å°† PF-ODE è½¨è¿¹è§†ä¸ºä¸€ç³»åˆ—ç”±é‡‡æ ·ç‚¹åˆ’åˆ†çš„ PF-ODE å­è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å­è·¯å¾„çº¿æ€§ï¼ˆSLï¼‰ODE åœ¨æ¯ä¸ªå•ç‹¬çš„ PF-ODE å­è·¯å¾„ä¸Šå½¢æˆæ¸è¿›ä¸”è¿ç»­çš„è¯¯å·®ä¼°è®¡ï¼Œæ„å»ºå…·æœ‰è¾ƒå°ç´¯ç§¯è¿‘ä¼¼è¯¯å·®çš„å»å™ªæ˜ å°„ï¼Œåœ¨è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œç”Ÿæˆè´¨é‡ä¸Šå‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäºå­è·¯å¾„çº¿æ€§é€¼è¿‘çš„åŠ é€Ÿæ‰©æ•£æ¨¡å‹ SLAMï¼Œé€šè¿‡å°† PF-ODE è½¨è¿¹è§†ä¸ºä¸€ç³»åˆ—ç”±é‡‡æ ·ç‚¹åˆ’åˆ†çš„ PF-ODE å­è·¯å¾„ï¼Œå¹¶åˆ©ç”¨å­è·¯å¾„çº¿æ€§ï¼ˆSLï¼‰ODE åœ¨æ¯ä¸ªå•ç‹¬çš„ PF-ODE å­è·¯å¾„ä¸Šå½¢æˆæ¸è¿›ä¸”è¿ç»­çš„è¯¯å·®ä¼°è®¡ï¼Œæ„å»ºå…·æœ‰è¾ƒå°ç´¯ç§¯è¿‘ä¼¼è¯¯å·®çš„å»å™ªæ˜ å°„ã€‚è¿˜å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„è’¸é¦æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›æ›´é«˜çº§çš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼‰çš„æ•´åˆã€‚æ€§èƒ½ï¼šåœ¨ LAIONã€MS COCO 2014 å’Œ MS COCO 2017 æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒSLAM å®ç°äº†é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œåªéœ€ 6 ä¸ª A100 GPU å¤©å³å¯ç”Ÿæˆé«˜è´¨é‡çš„ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»¥é«˜æ€§èƒ½è¿›è¡Œ 2 åˆ° 4 æ­¥ç”Ÿæˆã€‚å…¨é¢çš„è¯„ä¼°è¿˜è¡¨æ˜ï¼ŒSLAM åœ¨å°æ­¥ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼Œåœ¨ FID å’Œç”Ÿæˆå›¾åƒè´¨é‡ä¸Šéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å·¥ä½œé‡ï¼šSLAM çš„è®­ç»ƒæˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œåœ¨ 6 ä¸ª A100 GPU å¤©å†…å³å¯å®Œæˆè®­ç»ƒï¼Œå¹¶ä¸”åœ¨å°æ­¥ç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-919d70908993415e92c8909c00655335.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e2aa2475025a88869b1ac0e1b6be112.jpg" align="middle"></details>## Object-Attribute Binding in Text-to-Image Generation: Evaluation and   Control**Authors:Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens**Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance. [PDF](http://arxiv.org/abs/2404.13766v1) **æ‘˜è¦**æ–‡æœ¬æç¤ºä¸­çš„è¯­æ³•çº¦æŸæœ‰åŠ©äºç”Ÿæˆæ›´å‡†ç¡®çš„å›¾åƒï¼Œå…¶ä¸­å±æ€§ä¸æ­£ç¡®çš„å¯¹è±¡ç›¸å…³è”ã€‚**è¦ç‚¹*** å½“å‰æ‰©æ•£æ¨¡å‹éš¾ä»¥å°†æ–‡æœ¬æç¤ºä¸­çš„å±æ€§æ­£ç¡®ç»‘å®šåˆ°å›¾åƒä¸­çš„æ­£ç¡®å¯¹è±¡ã€‚* æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒ-å›¾å¯¹é½æ¨¡å‹ EPViTï¼Œç”¨äºè¯„ä¼°å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚* å¼•å…¥äº†ç„¦ç‚¹äº¤å‰æ³¨æ„ (FCA)ï¼Œä»¥é€šè¿‡è¾“å…¥å¥å­çš„å¥æ³•çº¦æŸæ¥æ§åˆ¶è§†è§‰æ³¨æ„å›¾ã€‚* æç¤ºçš„è¯­æ³•ç»“æ„æœ‰åŠ©äºè§£è€¦åœ¨ T2I ç”Ÿæˆä¸­å¸¸ç”¨çš„å¤šæ¨¡æ€ CLIP åµŒå…¥ã€‚* æ‰€äº§ç”Ÿçš„ DisCLIP åµŒå…¥å’Œ FCA å¯ä»¥è½»æ¾é›†æˆåˆ°æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒè¿™äº›æ¨¡å‹ã€‚* åœ¨ T2I ç”Ÿæˆä¸­å±•ç¤ºäº†å®è´¨æ€§çš„æ”¹è¿›ï¼Œå°¤å…¶æ˜¯åœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šçš„å±æ€§-å¯¹è±¡ç»‘å®šã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>Title:</strong> æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¯¹è±¡-å±æ€§ç»‘å®šï¼šè¯„ä¼°å’Œæ§åˆ¶</p></li><li><p><strong>Authors:</strong> Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert HÃ¶nig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens</p></li><li><p><strong>Affiliation:</strong> KU Leuven, Department of Computer Science</p></li><li><p><strong>Keywords:</strong> æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å¯¹è±¡-å±æ€§ç»‘å®šã€æ³¨æ„åŠ›æœºåˆ¶ã€æ‰©æ•£æ¨¡å‹</p></li><li><p><strong>Urls:</strong> Paper: https://arxiv.org/abs/2404.13766, Github: None</p></li><li><p><strong>Summary:</strong></p></li></ol><p>(1): <strong>ç ”ç©¶èƒŒæ™¯ï¼š</strong> å½“å‰çš„æ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºåˆ›å»ºé€¼çœŸçš„å›¾åƒï¼Œä½†éš¾ä»¥å°†æ–‡æœ¬ä¸­æåˆ°çš„å±æ€§æ­£ç¡®ç»‘å®šåˆ°å›¾åƒä¸­çš„æ­£ç¡®å¯¹è±¡ä¸Šã€‚</p><p>(2): <strong>è¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š</strong> ç°æœ‰çš„æ–¹æ³•ä¸»è¦åŸºäº CLIP è¯„åˆ†è¿›è¡Œè¯„ä¼°ï¼Œä½†æ— æ³•æ£€æŸ¥å¤æ‚å¤šå¯¹è±¡æç¤ºä¸­å±æ€§ä¸å¯¹è±¡çš„æ­£ç¡®ç»‘å®šã€‚</p><p>(3): <strong>æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š</strong> æå‡ºäº†ä¸€ç§åŸºäº ViT çš„å›¾åƒå›¾é¢„æµ‹æ¨¡å‹ EPViT å’Œä¸€ç§ç§°ä¸ºèšç„¦äº¤å‰æ³¨æ„ (FCA) çš„æ–¹æ³•ï¼Œä»¥æ§åˆ¶è§†è§‰æ³¨æ„åŠ›å›¾ï¼Œä»è€Œæ”¹å–„å¯¹è±¡-å±æ€§ç»‘å®šã€‚</p><p>(4): <strong>æ–¹æ³•æ€§èƒ½ï¼š</strong> åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æ˜¾ç€æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆåŠå…¶å¯¹è±¡-å±æ€§ç»‘å®šæ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):æå‡ºäº†ä¸€ç§åŸºäº ViT çš„å›¾åƒå›¾é¢„æµ‹æ¨¡å‹ EPViTï¼Œç”¨äºç”Ÿæˆå›¾åƒç‰¹å¾å›¾ï¼›            (2):è®¾è®¡äº†ä¸€ç§ç§°ä¸ºèšç„¦äº¤å‰æ³¨æ„ (FCA) çš„æ–¹æ³•ï¼Œç”¨äºæ§åˆ¶è§†è§‰æ³¨æ„åŠ›å›¾ï¼Œä»è€Œæ”¹å–„å¯¹è±¡-å±æ€§ç»‘å®šï¼›            (3):å°† FCA å’Œ DisCLIP é›†æˆåˆ°ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»¥å¢å¼ºå…¶å¯¹è±¡-å±æ€§ç»‘å®šæ€§èƒ½ï¼›            (4):åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¯¹å¢å¼ºåçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ COCO 10-Kã€CC-500ã€DAA-200 å’Œ AE-267ï¼Œä»¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p><pre><code>            (1):æœ¬ç ”ç©¶æå‡ºäº†æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¼ºè°ƒäº†åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æ•´åˆè¯­è¨€å¥æ³•ç»“æ„çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†å®ƒä»¬åœ¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„è½»æ¾ä¸”æˆåŠŸçš„é›†æˆï¼Œä»è€Œæ”¹å–„äº†å¯¹è±¡-å±æ€§ç»‘å®šï¼Œå¹¶å‡å°‘äº†ç”Ÿæˆå›¾åƒä¸­çš„å±æ€§æ³„æ¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æ–°è®¾è®¡åº¦é‡ EPViT åœ¨è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯¹è±¡-å±æ€§ç»‘å®šæ–¹é¢ä¼˜äº CLIPã€‚EPViT å…è®¸æ›´å¥½åœ°ç†è§£å’Œè¡¡é‡æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒä¸­å‡†ç¡®åæ˜ é¢„æœŸæ–‡æœ¬æè¿°çš„æ€§èƒ½ã€‚                           8. æ€»ç»“ï¼š            (1):æœ¬ç ”ç©¶çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ            (2):ä»åˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ä¸‰ä¸ªç»´åº¦æ€»ç»“æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹ã€‚                               .......         æŒ‰ç…§åé¢çš„è¾“å‡ºæ ¼å¼ï¼š         8. ç»“è®ºï¼š            (1):xxx;            (2):åˆ›æ–°ç‚¹ï¼šxxxï¼›æ€§èƒ½ï¼šxxxï¼›å·¥ä½œé‡ï¼šxxxï¼›         åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼ˆä¸“æœ‰åè¯ç”¨è‹±æ–‡æ ‡æ³¨ï¼‰ï¼Œè¡¨è¿°å°½é‡ç®€æ´ã€å­¦æœ¯ï¼Œä¸è¦é‡å¤å‰é¢&lt;Summary&gt;çš„å†…å®¹ï¼ŒåŸæ•°å­—çš„ä½¿ç”¨ä»·å€¼ï¼ŒåŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§æ ¼å¼ï¼Œå¯¹åº”å†…å®¹è¾“å‡ºåˆ°xxxï¼Œæ¢è¡Œï¼Œ.......è¡¨ç¤ºæ ¹æ®å®é™…è¦æ±‚å¡«å†™ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥ä¸å†™ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-da0a6f3f353e58ad78bee95a227f033f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd14ddde3959c9e4184326c15ccbc7c4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-10db76640ccb78a6742ee69ebb161059.jpg" align="middle"></details><h2 id="Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models"><a href="#Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models" class="headerlink" title="Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models"></a>Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models</h2><p><strong>Authors:Vitali Petsiuk, Kate Saenko</strong></p><p>Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.   Project page: <a href="https://cs-people.bu.edu/vpetsiuk/arc">https://cs-people.bu.edu/vpetsiuk/arc</a> </p><p><a href="http://arxiv.org/abs/2404.13706v1">PDF</a> </p><p><strong>Summary</strong><br>å¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å®‰å…¨æ€§çš„æ”»å‡»é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆæˆæ€§è´¨å’Œæ¦‚å¿µç®—æœ¯æ¥é‡å»ºè¢«ç¦æ­¢çš„æ¦‚å¿µã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>æ‰©æ•£æ¨¡å‹çš„åˆæˆæ€§è´¨ä½¿æ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡ç»„åˆå¤šä¸ªæç¤ºæ¥ç”Ÿæˆå›¾åƒã€‚</li><li>å³ä½¿ç›´æ¥è®¡ç®—ç›®æ ‡æ¦‚å¿µçš„å‘é‡ä¸å†å¯è®¿é—®ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç»„åˆä¸å—æŠ‘åˆ¶å½±å“çš„å…¶ä»–æ¦‚å¿µæ¥é‡å»ºè¯¥å‘é‡ã€‚</li><li>æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ¦‚å¿µç®—æœ¯æ¥é‡å»ºè¢«ç¦æ­¢çš„æ¦‚å¿µï¼Œä¾‹å¦‚ç‰ˆæƒã€æš´åŠ›ã€è‰²æƒ…æˆ–ä¸ªäººä¿¡æ¯ã€‚</li><li>æå‡ºäº†ä¸¤ç§æ–°çš„æ”»å‡»ï¼Œåˆ†åˆ«ç§°ä¸ºâ€œåé—¨æ”»å‡»â€å’Œâ€œç»„åˆæ”»å‡»â€ã€‚</li><li>åé—¨æ”»å‡»åˆ©ç”¨äº†ç”Ÿæˆæ¨¡å‹ä¸­å­˜åœ¨çš„æ¼æ´æˆ–åé—¨ã€‚</li><li>ç»„åˆæ”»å‡»åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„åˆæˆæ€§è´¨ã€‚</li><li>è¿™äº›æ”»å‡»å¯¹å®‰å…¨æ¨¡å‹çš„éƒ¨ç½²æœ‰é‡å¤§å½±å“ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: ç»•è¿‡æ¦‚å¿µæŠ‘åˆ¶çš„ç®—æœ¯æ¦‚å¿µ</p></li><li><p>Authors: Vitali Petsiuk and Kate Saenko</p></li><li><p>Affiliation: æ³¢å£«é¡¿å¤§å­¦</p></li><li><p>Keywords: Text-to-Image diffusion models, safety mechanisms, concept inhibition, concept arithmetics, compositional inference</p></li><li><p>Urls: https://arxiv.org/abs/2404.13706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): éšç€ Text-to-Image (T2I) ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œäººä»¬å¼€å§‹å…³æ³¨å…¶æ½œåœ¨çš„æ»¥ç”¨é£é™©ï¼Œä¾‹å¦‚ç”Ÿæˆä¾µçŠ¯ç‰ˆæƒã€æš´åŠ›ã€è‰²æƒ…æˆ–ä¸ªäººä¿¡æ¯çš„å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†å„ç§å®‰å…¨æœºåˆ¶æ¥é™åˆ¶æ¨¡å‹çš„æ¶æ„ä½¿ç”¨ã€‚</p><p>(2): ç°æœ‰çš„å®‰å…¨æœºåˆ¶é€šå¸¸é‡‡ç”¨æ¦‚å¿µæŠ‘åˆ¶çš„æ–¹æ³•ï¼Œå³é˜»æ­¢æ¨¡å‹ç”Ÿæˆç‰¹å®šæ¦‚å¿µçš„å†…å®¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å­˜åœ¨æ¼æ´ï¼Œå…è®¸æ”»å‡»è€…é€šè¿‡ç»„åˆå…¶ä»–æ¦‚å¿µæ¥ç»•è¿‡æŠ‘åˆ¶ï¼Œä»è€Œé‡å»ºè¢«æŠ‘åˆ¶æ¦‚å¿µçš„å‘é‡ã€‚</p><p>(3): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚å¿µç®—æœ¯å’Œç»„åˆæ¨ç†çš„æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ç»„åˆç‰¹æ€§ï¼Œå…è®¸åœ¨å•ä¸ªå›¾åƒç”Ÿæˆä¸­ä½¿ç”¨å¤šä¸ªæç¤ºã€‚é€šè¿‡ç»„åˆä¸å—æŠ‘åˆ¶å½±å“çš„å…¶ä»–æ¦‚å¿µï¼Œæ”»å‡»è€…å¯ä»¥é‡å»ºè´Ÿè´£ç›®æ ‡æ¦‚å¿µç”Ÿæˆçš„å‘é‡ï¼Œå³ä½¿è¯¥å‘é‡çš„ç›´æ¥è®¡ç®—ä¸å†å¯è®¿é—®ã€‚</p><p>(4): å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ”»å‡»æ–¹æ³•åœ¨å„ç§æ‰©æ•£æ¨¡å‹å’Œæ¦‚å¿µæŠ‘åˆ¶æœºåˆ¶ä¸Šéƒ½å–å¾—äº†æˆåŠŸã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨è®¾è®¡å®‰å…¨æœºåˆ¶æ—¶ï¼Œå¿…é¡»è€ƒè™‘æ”»å‡»è€…å¯èƒ½é‡‡ç”¨çš„ä¸€åˆ‡å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</p><p>Some Error for method(æ¯”å¦‚æ˜¯ä¸æ˜¯æ²¡æœ‰Methodsè¿™ä¸ªç« èŠ‚)</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¦‚å¿µç®—æœ¯å’Œç»„åˆæ¨ç†çš„æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»•è¿‡äº†ç°æœ‰çš„åŸºäºæ¦‚å¿µæŠ‘åˆ¶çš„å®‰å…¨æœºåˆ¶ï¼Œå…è®¸æ”»å‡»è€…ç”Ÿæˆè¢«æŠ‘åˆ¶æ¦‚å¿µçš„å†…å®¹ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œåœ¨è®¾è®¡å®‰å…¨æœºåˆ¶æ—¶ï¼Œå¿…é¡»è€ƒè™‘æ”»å‡»è€…å¯èƒ½é‡‡ç”¨çš„ä¸€åˆ‡å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»•è¿‡æ¦‚å¿µæŠ‘åˆ¶çš„å®‰å…¨æœºåˆ¶çš„æ–°æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº†æ‰©æ•£æ¨¡å‹çš„ç»„åˆç‰¹æ€§å’Œæ¦‚å¿µç®—æœ¯ã€‚æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ”»å‡»æ–¹æ³•åœ¨å„ç§æ‰©æ•£æ¨¡å‹å’Œæ¦‚å¿µæŠ‘åˆ¶æœºåˆ¶ä¸Šéƒ½å–å¾—äº†æˆåŠŸã€‚å·¥ä½œé‡ï¼šæœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦å¯¹æ‰©æ•£æ¨¡å‹ã€æ¦‚å¿µæŠ‘åˆ¶æœºåˆ¶å’Œæ”»å‡»æ–¹æ³•æœ‰æ·±å…¥çš„ç†è§£ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-6c22a86090e8195e410f10a38f6fe1f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936989960b19443d0f4e3c6f1a1e8e26.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-25  ID-Aligner Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</id>
    <published>2024-04-22T09:43:13.000Z</published>
    <updated>2024-04-22T09:43:13.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF. </p><p><a href="http://arxiv.org/abs/2404.11897v1">PDF</a> </p><p><strong>Summary</strong><br>é™ä½è®­ç»ƒæˆæœ¬ï¼Œå®ç°å¤šé«˜åº¦è‡ªç”±è§†è§’å›¾åƒåˆæˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¤šé«˜åº¦ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰èƒ½å¤Ÿåˆæˆè‡ªç”±è§†è§’å›¾åƒã€‚</li><li>æå‡ºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œæ³¨æ„åŠ›ç‰¹å¾èåˆï¼Œè§£å†³ä¸åŒé«˜åº¦ç»†èŠ‚å·®å¼‚é—®é¢˜ã€‚</li><li>AG-NeRF åœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚</li><li>AG-NeRF è®­ç»ƒæ—¶é—´ä»…éœ€åŠå°æ—¶ï¼Œå³å¯è¾¾åˆ° BungeeNeRF çš„ç«äº‰æ°´å¹³ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: å¤šé«˜åº¦å¤§å°ºåº¦æˆ·å¤–åœºæ™¯æ¸²æŸ“çš„æ³¨æ„åŠ›å¼•å¯¼ç¥ç»è¾å°„åœº</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: åå—ç†å·¥å¤§å­¦</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): ç°æœ‰çš„åŸºäºç¥ç»è¾å°„åœº (NeRF) çš„å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯æ–°è§†è§’åˆæˆæ–¹æ³•ä¸»è¦å»ºç«‹åœ¨å•ä¸€é«˜åº¦ä¸Šã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å…ˆéªŒçš„ç›¸æœºæ‹æ‘„é«˜åº¦å’Œåœºæ™¯èŒƒå›´ï¼Œå½“ç›¸æœºé«˜åº¦å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä¼šå¯¼è‡´ä½æ•ˆä¸”ä¸å®ç”¨çš„åº”ç”¨ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ï¼š   - åœ°ç†ä¸Šå°†åœºæ™¯åˆ†è§£ä¸ºå‡ ä¸ªå•å…ƒæ ¼ï¼Œå¹¶ä¸ºæ¯ä¸ªå•å…ƒæ ¼è®­ç»ƒä¸€ä¸ªå­ NeRFï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶ã€‚   - åœ¨ä½ç½®ç¼–ç ä¸­å¹¶è¡Œåº”ç”¨å¹³é¢å’Œç½‘æ ¼ç‰¹å¾ä»¥å®ç°é«˜æ•ˆå»ºæ¨¡ã€‚   - é—®é¢˜ï¼šå®ƒä»¬åœ¨åŸºç¡€é«˜åº¦ä¸Šé‡å»ºå¤§è§„æ¨¡åœºæ™¯ï¼Œå½“å¯¼èˆªåˆ°æ›´è¿‘çš„åœ°æ–¹ä»¥æ£€æŸ¥å¤§è§„æ¨¡æˆ·å¤–åœºæ™¯çš„å¾®è§‚ç»†èŠ‚æ—¶ï¼Œè¡¨ç°å‡ºè¿‡åº¦æ¨¡ç³Šçš„ä¼ªå½±å’Œä¸å®Œæ•´çš„é‡å»ºã€‚</p><p>(3): æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š   - æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ AG-NeRFï¼Œé€šè¿‡åˆæˆåŸºäºåœºæ™¯ä¸åŒé«˜åº¦çš„è‡ªç”±è§†è§’å›¾åƒæ¥é™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚   - å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è§£å†³ä»ä½é«˜åº¦ï¼ˆæ— äººæœºçº§åˆ«ï¼‰åˆ°é«˜é«˜åº¦ï¼ˆå«æ˜Ÿçº§åˆ«ï¼‰çš„ç»†èŠ‚å˜åŒ–é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>(4): æœ¬æ–‡æ–¹æ³•åœ¨ä»»åŠ¡å’Œæ€§èƒ½ä¸Šçš„è¡¨ç°ï¼š   - åœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ã€‚   - åªéœ€è¦åŠå°æ—¶çš„è®­ç»ƒæ—¶é—´å³å¯è¾¾åˆ°ä¸æœ€æ–° BungeeNeRF ç›¸å½“çš„ç«äº‰æ€§ PSNRã€‚   - æ€§èƒ½æ”¯æŒäº†ä»–ä»¬çš„ç›®æ ‡ï¼šé™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯æ¡†æ¶ AG-NeRFï¼Œé€šè¿‡åˆæˆåŸºäºåœºæ™¯ä¸åŒé«˜åº¦çš„è‡ªç”±è§†è§’å›¾åƒæ¥é™ä½æ„å»ºè‰¯å¥½é‡å»ºçš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šå¼€å‘äº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’Œä¸€ç§åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šåˆ©ç”¨å¯è®­ç»ƒçš„ U-Net ç½‘ç»œä»æºå›¾åƒä¸­æå–ç‰¹å¾å›¾ï¼Œå¹¶ä½¿ç”¨ Transformer å¯¹æå–çš„ç‰¹å¾å‘é‡è¿›è¡Œèåˆï¼Œä»¥æœ€å¤§åŒ–èåˆç‰¹å¾ä¸ç›®æ ‡åƒç´ ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šé‡‡ç”¨åˆ†å±‚é‡‡æ ·æ–¹æ³•ï¼Œä½¿ç”¨ç²—ç•¥ç½‘ç»œå’Œç²¾ç»†ç½‘ç»œåŒæ—¶ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•å°†å¤šé«˜åº¦å›¾åƒä¸­çš„ç‰¹å¾èåˆèµ·æ¥ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡é’ˆå¯¹ä¸åŒé«˜åº¦æ‹æ‘„çš„å¤§åœºæ™¯æ¸²æŸ“æå‡ºäº†ç«¯åˆ°ç«¯çš„ AG-NeRF æ¡†æ¶ï¼Œé™ä½äº†æ„å»ºè‰¯å¥½é‡å»ºæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æºå›¾åƒé€‰æ‹©æ–¹æ³•å’ŒåŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»å¤šé«˜åº¦å›¾åƒä¸­æå–å’Œèåˆç›®æ ‡è§†å›¾æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œä»¥å®ç°é«˜ä¿çœŸæ¸²æŸ“ã€‚æ€§èƒ½ï¼šåœ¨ 56 Leonard å’Œ Transamerica åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† SOTA æ€§èƒ½ï¼Œåªéœ€è¦åŠå°æ—¶çš„è®­ç»ƒæ—¶é—´å³å¯è¾¾åˆ°ä¸æœ€æ–° BungeeNeRF ç›¸å½“çš„ç«äº‰æ€§ PSNRã€‚å·¥ä½œé‡ï¼šé‡‡ç”¨åˆ†å±‚é‡‡æ ·æ–¹æ³•ï¼Œä½¿ç”¨ç²—ç•¥ç½‘ç»œå’Œç²¾ç»†ç½‘ç»œåŒæ—¶ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„ç‰¹å¾èåˆæ–¹æ³•å°†å¤šé«˜åº¦å›¾åƒä¸­çš„ç‰¹å¾èåˆèµ·æ¥ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy. </p><p><a href="http://arxiv.org/abs/2404.11419v1">PDF</a> </p><p><strong>Summary</strong><br>Nerf-SLAM é€šè¿‡é‡‡ç”¨ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªæ¨¡å‹å’Œ KL æ­£åˆ™åŒ–å™¨ï¼Œåœ¨è·Ÿè¸ªæ€§èƒ½å’Œé‡å»ºç²¾åº¦ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æˆç»©ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªæ¨¡å‹ä»¥æé«˜ NeRF-SLAM çš„è·Ÿè¸ªæ€§èƒ½ã€‚</li><li>SLAIM é€šè¿‡é«˜æ–¯é‡‘å­—å¡”æ»¤æ³¢å™¨å®ç°ä»ç²—åˆ°ç»†çš„è·Ÿè¸ªä¼˜åŒ–ç­–ç•¥ã€‚</li><li>NeRF ç³»ç»Ÿéš¾ä»¥ä½¿ç”¨æœ‰é™çš„è¾“å…¥è§†å›¾æ”¶æ•›åˆ°æ­£ç¡®çš„å‡ ä½•å½¢çŠ¶ã€‚</li><li>SLAIM ä½¿ç”¨ä½“ç§¯å¯†åº¦è¡¨ç¤ºå’Œä¸€ä¸ªæ–°çš„ KL æ­£åˆ™åŒ–å™¨æ¥çº¦æŸåœºæ™¯å‡ ä½•å½¢çŠ¶ã€‚</li><li>SLAIM å®ç°å±€éƒ¨å’Œå…¨å±€æ†ç»‘è°ƒæ•´ä»¥æé«˜é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚</li><li>SLAIM åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦ä¸Šå‡æ˜¾ç¤ºå‡ºæœ€å…ˆè¿›çš„ç»“æœã€‚</li><li>SLAIM è§£å†³äº† NeRF-SLAM åœ¨ä¼ ç»Ÿ SLAM ç®—æ³•ä¸‹è¡¨ç°å‡ºè¾ƒå·®çš„è·Ÿè¸ªæ€§èƒ½è¿™ä¸€éš¾é¢˜ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>æ ‡é¢˜ï¼šSLAIMï¼šç”¨äºåœ¨çº¿è·Ÿè¸ªå’Œå»ºå›¾çš„é²æ£’ç¨ å¯†ç¥ç»SLAM</p></li><li><p>ä½œè€…ï¼šVincent Cartillierã€Grant Schindlerã€Irfan Essa</p></li><li><p>éš¶å±å…³ç³»ï¼šä½æ²»äºšç†å·¥å­¦é™¢</p></li><li><p>å…³é”®è¯ï¼šç¥ç»è¾å°„åœºã€SLAMã€ç¨ å¯†å»ºå›¾ã€è·Ÿè¸ª</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.11419ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šç¨ å¯†è§†è§‰SLAMæ˜¯3Dè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé•¿æœŸé—®é¢˜ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ã€å®¤å†…å¤–æœºå™¨äººå¯¼èˆªã€è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚</p><p>ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¼ ç»Ÿçš„SLAMç³»ç»Ÿé€šè¿‡ä¼°è®¡å›¾åƒå¯¹åº”å…³ç³»æ¥å¼€å§‹ï¼Œè¿™äº›å¯¹åº”å…³ç³»å¯èƒ½æ˜¯ç¨€ç–çš„ï¼Œä¾‹å¦‚åŒ¹é…çš„ç‰¹å¾ç‚¹ã€‚ç¥ç»è¾å°„åœºSLAMï¼ˆNeRF-SLAMï¼‰æ–¹æ³•é€šè¿‡å›¾åƒå¯¹é½å’Œå…‰åº¦æ†ç»‘è°ƒæ•´æ¥è§£å†³ç›¸æœºè·Ÿè¸ªé—®é¢˜ã€‚ç”±äºå›¾åƒç©ºé—´ä¸­ä¼˜åŒ–æŸå¤±çš„å¸å¼•åŸŸçª„ï¼ˆå±€éƒ¨æå°å€¼ï¼‰ä»¥åŠç¼ºä¹åˆå§‹å¯¹åº”å…³ç³»ï¼Œæ­¤ç±»ä¼˜åŒ–è¿‡ç¨‹éš¾ä»¥ä¼˜åŒ–ã€‚</p><p>ï¼ˆ3ï¼‰æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç²—åˆ°ç»†è·Ÿè¸ªæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹NeRF-SLAMï¼Œä»¥å®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äºKLæ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆã€‚</p><p>ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ScanNetã€TUMã€Replicaç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†æœ¬æ–‡çš„ç›®æ ‡ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><pre><code>            (1):SLAIM æ˜¯ä¸€ç§ç”¨äºç¨ å¯†æ˜ å°„å’Œè·Ÿè¸ªçš„ RGB-D è¾“å…¥æµçš„ novel æ–¹æ³•ï¼›            (2):SLAIM é‡‡ç”¨äº†ä¸€ç§ä»ç²—åˆ°ç²¾çš„è·Ÿè¸ªæ¨¡å‹ï¼Œä»¥å®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ï¼›            (3):SLAIM å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äº KL æ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆï¼›            (4):SLAIM åœ¨ ScanNetã€TUMã€Replica ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåœ¨è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦æ–¹é¢å‡å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</code></pre></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰æœ¬æ–‡çš„å·¥ä½œæ„ä¹‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æœ€å…ˆè¿›çš„ç¨ å¯†å®æ—¶ RGB-D NeRF-SLAM ç³»ç»Ÿ SLAIMï¼Œè¯¥ç³»ç»Ÿå…·æœ‰æœ€å…ˆè¿›çš„ç›¸æœºè·Ÿè¸ªå’Œå»ºå›¾èƒ½åŠ›ã€‚</p><p>ï¼ˆ2ï¼‰æœ¬æ–‡çš„ä¼˜ç¼ºç‚¹æ€»ç»“ï¼š    - åˆ›æ–°ç‚¹ï¼š        - é‡‡ç”¨ä»ç²—åˆ°ç²¾çš„è·Ÿè¸ªæ¨¡å‹ï¼Œå®ç°æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚        - å¼•å…¥æ–°çš„ç›®æ ‡å°„çº¿ç»ˆæ­¢åˆ†å¸ƒï¼Œå¹¶å°†å…¶ç”¨äº KL æ­£åˆ™åŒ–å™¨ä¸­ï¼Œä»¥çº¦æŸåœºæ™¯å‡ ä½•ç”±ç©ºç©ºé—´å’Œä¸é€æ˜è¡¨é¢ç»„æˆã€‚    - æ€§èƒ½ï¼š        - åœ¨ ScanNetã€TUMã€Replica ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„è·Ÿè¸ªå’Œé‡å»ºç²¾åº¦ã€‚    - å·¥ä½œé‡ï¼š        - å†…å­˜æ•ˆç‡é«˜ï¼Œåœ¨ Replica å’Œ ScanNet æ•°æ®é›†ä¸Šä¸åŸºå‡†ç›¸æ¯”ï¼Œè·Ÿè¸ªå’Œå»ºå›¾æ—¶é—´å‡æœ‰æ˜æ˜¾é™ä½ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. </p><p><a href="http://arxiv.org/abs/2404.11401v1">PDF</a> </p><p><strong>Summary</strong><br>åŸºäºç¥ç»ç½‘ç»œçš„å…‰è°±åå·®ç‰¹æ€§ï¼ŒRainyScapeåˆ©ç”¨æ— ç›‘ç£æ¡†æ¶é‡å»ºå¹²å‡€åœºæ™¯ï¼ŒåŒ…å«ç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åˆ©ç”¨ç¥ç»ç½‘ç»œçš„å…‰è°±åå·®ç‰¹æ€§è·å¾—ä½é¢‘åœºæ™¯è¡¨ç¤ºã€‚</li><li>è”åˆä¼˜åŒ–ç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œä»¥åŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</li><li>æå‡ºè‡ªé€‚åº”æ–¹å‘æ•æ„Ÿæ¢¯åº¦é‡å»ºæŸå¤±ï¼Œå¼•å¯¼ç½‘ç»œåŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</li><li>åœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯æ–‘ç‚¹ splatting æ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€å…ˆè¿›çš„å»é›¨æ€§èƒ½ã€‚</li><li>æä¾›é«˜è´¨é‡æ•°æ®é›†å’Œæºä»£ç ï¼Œä¿ƒè¿›ç ”ç©¶å·¥ä½œã€‚</li><li>å¼•å…¥å¯å­¦ä¹ æ½œåœ¨åµŒå…¥ï¼Œæ•æ‰åœºæ™¯çš„é›¨æ»´ç‰¹å¾ã€‚</li><li>é€šè¿‡é›¨æ»´é¢„æµ‹ç½‘ç»œæœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹ï¼Œæ¸²æŸ“å¹²å‡€å›¾åƒã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: æ— ç›‘ç£é›¨æ™¯é‡å»ºä½¿ç”¨è§£è€¦ç¥ç»æ¸²æŸ“</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: é¦™æ¸¯åŸå¸‚å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):éšç€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å›¾åƒåˆæˆä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå½“è¾“å…¥å›¾åƒå—åˆ°æ¨¡ç³Šã€å™ªå£°æˆ–é›¨æ°´ç­‰å› ç´ å½±å“æ—¶ï¼Œæ¸²æŸ“ç»“æœä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿæ˜æ˜¾çš„ä¼ªå½±ã€‚</p><p>(2):ç°æœ‰çš„æ–¹æ³•é’ˆå¯¹ç‰¹å®šä»»åŠ¡æå‡ºäº†å„ç§è§£å†³æ–¹æ¡ˆï¼Œä½†å¯¹äºé›¨æ™¯é‡å»ºä»»åŠ¡ï¼Œå®ƒä»¬æ— æ³•æœ‰æ•ˆè¡¨ç¤ºä¸‰ç»´ç©ºé—´ä¸­ç¨€ç–ä¸”é—´æ­‡æ€§çš„é™é›¨ã€‚</p><p>(3):æœ¬æ–‡æå‡ºRainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»¥æ— ç›‘ç£çš„æ–¹å¼ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¥ç»æ¸²æŸ“ç®¡é“è·å¾—åœºæ™¯çš„ä½é¢‘è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨æ¥è¡¨å¾é›¨æ°´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œä»¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ã€‚</p><p>(4):åœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li><p>æ–¹æ³•ï¼š</p><p>ï¼ˆ1ï¼‰ï¼šæå‡ºRainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå¯ä»¥æ— ç›‘ç£åœ°ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šé€šè¿‡ç¥ç»æ¸²æŸ“ç®¡é“è·å¾—åœºæ™¯çš„ä½é¢‘è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨æ¥è¡¨å¾é›¨æ°´ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºä¸€ä¸ªè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œä»¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šåœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>ç»“è®ºï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šRainyScapeçš„æ„ä¹‰åœ¨äºï¼Œå®ƒæå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„è§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå¯ä»¥ä»é›¨æ™¯å›¾åƒä¸­é‡å»ºæ— é›¨åœºæ™¯ï¼Œæœ‰æ•ˆè§£å†³äº†é›¨æ™¯é‡å»ºä¸­çš„é›¨æ°´æ¡çº¹å»é™¤é—®é¢˜ï¼Œä¸ºé›¨æ™¯å›¾åƒå¤„ç†æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</p><ul><li><p>æå‡ºäº†ä¸€ç§è§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œé€šè¿‡ä½é¢‘åœºæ™¯è¡¨ç¤ºã€å¯å­¦ä¹ çš„é›¨æ°´åµŒå…¥å’Œé¢„æµ‹å™¨ä»¥åŠè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±ï¼Œæœ‰æ•ˆè§£è€¦äº†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ°´æ¡çº¹ã€‚</p></li><li><p>æ€§èƒ½ï¼š</p></li><li><p>åœ¨ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ•£å°„ä¸¤ç§æ¸²æŸ“æŠ€æœ¯ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ°´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></li><li><p>å·¥ä½œé‡ï¼š</p></li><li><p>è¯¥æ–¹æ³•éœ€è¦å¯¹é›¨æ™¯å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€é›¨æ°´æ¡çº¹æ£€æµ‹å’Œé›¨æ°´åµŒå…¥æå–ç­‰æ­¥éª¤ï¼Œå¢åŠ äº†è®¡ç®—é‡å’Œæ—¶é—´å¼€é”€ã€‚</p></li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video**Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu**In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary**å¯¹äºä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dç‰©ä½“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å˜å½¢æ¨¡å‹ï¼Œå³å‡†åˆšæ€§æ··åˆè’™çš®ï¼Œä»¥ä¾¿ä»å•ä¸ªè§†é¢‘ä¸­è¿›è¡Œå…¨é¢é‡å»ºã€‚**Key Takeaways**- æå‡ºä¸€ç§æ–°çš„å˜å½¢æ¨¡å‹ï¼Œå‡†åˆšæ€§æ··åˆè’™çš®ï¼Œå¢å¼ºäº†é›¶ä»¶åˆšæ€§ï¼ŒåŒæ—¶ä¿æŒå…³èŠ‚æŸ”æ€§å˜å½¢ã€‚- é‡‡ç”¨å¢å¼ºéª¨éª¼ç»‘å®šç³»ç»Ÿæ”¹å–„ç»„ä»¶å»ºæ¨¡ã€‚- ä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡æé«˜é›¶ä»¶åˆšæ€§å’Œé‡å»ºä¿çœŸåº¦ã€‚- åº”ç”¨æµ‹åœ°çº¿ç‚¹èµ‹å€¼å®ç°ç²¾ç¡®è¿åŠ¨å’Œæ— ç¼å˜å½¢ã€‚- åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜ä¿çœŸä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dé‡å»ºæ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚- è¯¥ç ”ç©¶ä¸ºä¸€èˆ¬æ€§å…³èŠ‚åŠ¨ä½œçš„3Dç‰©ä½“é‡å»ºæä¾›äº†æ–°çš„æ–¹æ³•ã€‚- è¯¥ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚- è¯¥ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šREACTOï¼šä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé“°æ¥ç‰©ä½“</p></li><li><p>ä½œè€…ï¼šChaoyue Songã€Jiacheng Weiã€Chuan Sheng Fooã€Guosheng Linã€Fayao Liu</p></li><li><p>éš¶å±ï¼šå—æ´‹ç†å·¥å¤§å­¦</p></li><li><p>å…³é”®è¯ï¼šé“°æ¥ç‰©ä½“é‡å»ºã€åŠ¨æ€ç¥ç»è¾å°„åœºã€å‡†åˆšæ€§æ··åˆè’™çš®</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.11151, Githubï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šé‡å»ºé“°æ¥ç‰©ä½“æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰åˆ†æ®µåˆšæ€§çš„é€šç”¨é“°æ¥ç‰©ä½“æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šNASAMå’ŒPARISç­‰æ–¹æ³•éœ€è¦å¤šè§†è§’å›¾åƒæˆ–å¤šè§†å›¾å›¾åƒï¼Œåœ¨å®é™…åº”ç”¨ä¸­å—é™ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿã€ä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡å’Œåº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…æ¥æé«˜åˆšæ€§å¹¶ä¿æŒå…³èŠ‚çš„çµæ´»å˜å½¢ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šä»»åŠ¡ä¸æ€§èƒ½ï¼šREACTOåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå¯¹é€šç”¨é“°æ¥ç‰©ä½“çš„3Dé‡å»ºä»»åŠ¡ä¸­å–å¾—äº†è¾ƒé«˜çš„ä¿çœŸåº¦ï¼Œè¯æ˜äº†å…¶æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><p><strong>7. Methodsï¼š</strong></p><p>(1)ï¼šæå‡ºå‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œå¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿï¼Œä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡ï¼Œå¹¶åº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…ï¼›</p><p>(2)ï¼šæ„å»ºREACTOæ¡†æ¶ï¼ŒåŒ…æ‹¬éª¨éª¼è£…é…ã€è’™çš®å˜å½¢ã€ä½“ç»˜åˆ¶å’Œæ¸²æŸ“æ¨¡å—ï¼›</p><p>(3)ï¼šä½¿ç”¨åŸºäºç¥ç»è¾å°„åœºçš„æ¸²æŸ“å™¨ï¼Œä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé“°æ¥ç‰©ä½“ï¼›</p><p>(4)ï¼šé€šè¿‡ä¼˜åŒ–éª¨éª¼å‚æ•°ã€è’™çš®æƒé‡å’Œç¥ç»è¾å°„åœºå‚æ•°ï¼Œå®ç°é“°æ¥ç‰©ä½“çš„é«˜ä¿çœŸé‡å»ºï¼›</p><p>(5)ï¼šåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯REACTOçš„æœ‰æ•ˆæ€§ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬å·¥ä½œæå‡ºREACTOï¼Œä¸€ç§ä»å•ä¸€è§†é¢‘ä¸­é‡å»ºé€šç”¨é“°æ¥3Dç‰©ä½“çš„å¼€åˆ›æ€§æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°å®šä¹‰è£…é…ç»“æ„å¹¶é‡‡ç”¨å‡†åˆšæ€§æ··åˆè’™çš®ï¼Œå®ç°äº†å»ºæ¨¡å’Œç²¾åº¦çš„æå‡ã€‚å‡†åˆšæ€§æ··åˆè’™çš®é€šè¿‡åˆ©ç”¨å‡†ç¨€ç–è’™çš®æƒé‡å’Œæµ‹åœ°çº¿ç‚¹åˆ†é…ï¼Œç¡®ä¿äº†æ¯ä¸ªéƒ¨ä»¶çš„åˆšæ€§ï¼ŒåŒæ—¶åœ¨å…³èŠ‚å¤„ä¿æŒå¹³æ»‘å˜å½¢ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒREACTOåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¿çœŸåº¦å’Œç»†èŠ‚æ–¹é¢éƒ½æœ‰æ‰€æå‡ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºå‡†åˆšæ€§æ··åˆè’™çš®å˜å½¢æ¨¡å‹ï¼Œå¢å¼ºéª¨éª¼è£…é…ç³»ç»Ÿï¼Œä½¿ç”¨å‡†ç¨€ç–è’™çš®æƒé‡ï¼Œå¹¶åº”ç”¨æµ‹åœ°çº¿ç‚¹åˆ†é…ï¼›</p><p>æ€§èƒ½ï¼šåœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šï¼ŒREACTOåœ¨ä¿çœŸåº¦å’Œç»†èŠ‚æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼›</p><p>å·¥ä½œé‡ï¼šä¸éœ€è¦å¤šè§†è§’æˆ–å¤šè§†å›¾å›¾åƒçš„ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒREACTOåªéœ€å•ä¸€è§†é¢‘å³å¯é‡å»ºé“°æ¥ç‰©ä½“ï¼Œå·¥ä½œé‡æ›´å°ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</id>
    <published>2024-04-22T09:32:29.000Z</published>
    <updated>2024-04-22T09:32:29.438Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-04-22-æ›´æ–°"><a href="#2024-04-22-æ›´æ–°" class="headerlink" title="2024-04-22 æ›´æ–°"></a>2024-04-22 æ›´æ–°</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>é€šè¿‡å€Ÿé‰´2Dè¯´è¯äººé¢éƒ¨çš„å”‡å½¢åŒæ­¥å’Œè¨€è¯­æ„ŸçŸ¥é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥æ„å»ºæ›´å¥½çš„3Dè¯´è¯äººé¢éƒ¨ç½‘ç»œã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>3Dè¯´è¯äººé¢éƒ¨ç ”ç©¶åœ¨å”‡å½¢åŒæ­¥å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢ä¸å¦‚2Dè¯´è¯äººé¢éƒ¨ç ”ç©¶æ·±å…¥ã€‚</li><li>Learn2Talkæ¡†æ¶åˆ©ç”¨2Dè¯´è¯äººé¢éƒ¨é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºæ›´å¥½çš„3Dè¯´è¯äººé¢éƒ¨ç½‘ç»œã€‚</li><li>3DåŒæ­¥å”‡ä¸“å®¶æ¨¡å‹æ—¨åœ¨å®ç°éŸ³é¢‘å’Œ3Dé¢éƒ¨è¿åŠ¨ä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚</li><li>2Dè¯´è¯äººé¢éƒ¨æ–¹æ³•ä¸­é€‰æ‹©çš„æ•™å¸ˆæ¨¡å‹ç”¨äºæŒ‡å¯¼éŸ³é¢‘åˆ°3Dè¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥æé«˜3Dé¡¶ç‚¹ç²¾åº¦ã€‚</li><li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¨€è¯­æ„ŸçŸ¥æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>è¯¥æ¡†æ¶æœ‰è¯­éŸ³-è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³é©±åŠ¨3Dé«˜æ–¯é£æº…åŸºäºå¤´åƒåŠ¨ç”»ä¸¤ä¸ªåº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>æ ‡é¢˜ï¼šLearn2Talkï¼š3D è¯´è¯äººè„¸ä» 2D è¯´è¯äººè„¸å­¦ä¹ </p></li><li><p>ä½œè€…ï¼šYixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, XuanCheng, Jing Liao, Juncong Lin</p></li><li><p>å•ä½ï¼šæš‚ç¼º</p></li><li><p>å…³é”®è¯ï¼šSpeech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2404.12888v1Githubï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¯´è¯äººè„¸åŠ¨ç”»æ–¹æ³•é€šå¸¸åŒ…å« 3D å’Œ 2D è¯´è¯äººè„¸ä¸¤å¤§ç±»ï¼Œè¿‘å¹´æ¥ä¸¤è€…éƒ½å¤‡å—ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œ3D è¯´è¯äººè„¸çš„ç ”ç©¶åœ¨å”‡å½¢åŒæ­¥ï¼ˆlip-syncï¼‰å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å¹¶æœªåƒ 2D è¯´è¯äººè„¸é‚£æ ·æ·±å…¥ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼šæœ¬æ–‡æ–¹æ³•åŠ¨æœºå……åˆ†ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæå‡ºä¸€ä¸ªåä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œã€‚é¦–å…ˆï¼Œå—éŸ³é¢‘è§†é¢‘åŒæ­¥ç½‘ç»œçš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ä¸ª 3D åŒæ­¥å”‡å½¢ä¸“å®¶æ¨¡å‹ï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚å…¶æ¬¡ï¼Œé€‰æ‹©ä¸€ä¸ªæ¥è‡ª 2D è¯´è¯äººè„¸æ–¹æ³•çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D è¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šæœ¬æ–‡æ–¹æ³•åœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡ºä¸€ä¸ªåä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè®¾è®¡äº†ä¸€ä¸ª 3D åŒæ­¥å”‡å½¢ä¸“å®¶æ¨¡å‹ï¼Œä»¥è¿½æ±‚éŸ³é¢‘å’Œ 3D é¢éƒ¨åŠ¨ä½œä¹‹é—´çš„å”‡å½¢åŒæ­¥ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šé€‰æ‹©ä¸€ä¸ªæ¥è‡ª 2D è¯´è¯äººè„¸æ–¹æ³•çš„æ•™å¸ˆæ¨¡å‹æ¥æŒ‡å¯¼éŸ³é¢‘åˆ° 3D è¿åŠ¨å›å½’ç½‘ç»œçš„è®­ç»ƒï¼Œä»¥äº§ç”Ÿæ›´é«˜çš„ 3D é¡¶ç‚¹ç²¾åº¦ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Learn2Talk çš„å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸¤ä¸ªä¸“ä¸šçŸ¥è¯†ç‚¹æ¥æ„å»ºä¸€ä¸ªæ›´å¥½çš„ 3D è¯´è¯äººè„¸ç½‘ç»œï¼Œåœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§æ–°çš„ 3D è¯´è¯äººè„¸åŠ¨ç”»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äº† 2D è¯´è¯äººè„¸é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼›æ€§èƒ½ï¼šåœ¨å”‡å½¢åŒæ­¥ã€é¡¶ç‚¹ç²¾åº¦å’Œè¯­éŸ³æ„ŸçŸ¥æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼›å·¥ä½œé‡ï¼šéœ€è¦æ”¶é›†å’Œæ ‡æ³¨å¤§é‡çš„æ•°æ®ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue**We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary**ä½¿ç”¨æ¥è‡ªä¸åŒè§†è§’çš„å¯¹æ¯”é«˜æ–¯èšç±»å®ç° 3D åœºæ™¯åˆ†å‰²ã€‚**Key Takeaways**- æå‡ºä¸€ç§æ–°çš„å¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œèƒ½å¤Ÿä»ä»»ä½•è§†è§’æä¾›åˆ†å‰²æ©æ¨¡ï¼Œå¹¶å®ç°åœºæ™¯çš„ 3D åˆ†å‰²ã€‚- å—æ–°è§†è§’åˆæˆé¢†åŸŸç ”ç©¶çš„å¯å‘ï¼Œä½¿ç”¨ 3D é«˜æ–¯äº‘å»ºæ¨¡åœºæ™¯çš„å¤–è§‚ã€‚- é€šè¿‡å°†é«˜æ–¯æŠ•å½±åˆ°ç»™å®šè§†ç‚¹å¹¶å¯¹å…¶é¢œè‰²è¿›è¡ŒÎ±æ··åˆï¼Œä»ç»™å®šè§†ç‚¹ç”Ÿæˆå‡†ç¡®çš„å›¾åƒã€‚- è®­ç»ƒæ¨¡å‹ï¼Œä½¿æ¯ä¸ªé«˜æ–¯éƒ½åŒ…å«ä¸€ä¸ªåˆ†å‰²ç‰¹å¾å‘é‡ã€‚- é€šè¿‡æ ¹æ®å…¶ç‰¹å¾å‘é‡å¯¹é«˜æ–¯è¿›è¡Œèšç±»ï¼Œå¯ç”¨äº 3D åœºæ™¯åˆ†å‰²ï¼›é€šè¿‡å°†é«˜æ–¯æŠ•å½±åˆ°å¹³é¢ä¸Šå¹¶å¯¹å…¶åˆ†å‰²ç‰¹å¾è¿›è¡Œ Î± æ··åˆï¼Œå¯ç”Ÿæˆ 2D åˆ†å‰²æ©æ¨¡ã€‚- ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–çš„ç»„åˆï¼Œå¯ä»¥åœ¨ä¸ä¸€è‡´çš„ 2D åˆ†å‰²æ©æ¨¡ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å­¦ä¹ ç”Ÿæˆåœ¨æ‰€æœ‰è§†å›¾ä¸­éƒ½ä¸€è‡´çš„åˆ†å‰²æ©æ¨¡ã€‚- æ‰€æå‡ºçš„æ–¹æ³•éå¸¸å‡†ç¡®ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œé¢„æµ‹æ©æ¨¡çš„ IoU å‡†ç¡®åº¦æé«˜äº† 8%ã€‚- ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å³å°†å‘å¸ƒã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šå¯¹æ¯”é«˜æ–¯èšç±»ï¼šå¼±ç›‘ç£ 3D åœºæ™¯åˆ†å‰²</p></li><li><p>ä½œè€…ï¼šMyrna C. Silvaã€Mahtab Dahaghinã€Matteo Tosoã€Alessio Del Bue</p></li><li><p>å•ä½ï¼šæ„å¤§åˆ©ç†å·¥å­¦é™¢æ¨¡å¼åˆ†æä¸è®¡ç®—æœºè§†è§‰ï¼ˆPAVISï¼‰</p></li><li><p>å…³é”®è¯ï¼š3D é«˜æ–¯æ•£å°„ã€3D åˆ†å‰²ã€å¯¹æ¯”å­¦ä¹ </p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šarXiv:2404.12784v1 [cs.CV]   Github é“¾æ¥ï¼šæ— </p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šè¿‘å¹´æ¥ï¼Œæ–°è§†è§’åˆæˆé¢†åŸŸçš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡ 3D é«˜æ–¯äº‘å¯¹åœºæ™¯çš„å¤–è§‚è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡åœ¨ç»™å®šè§†è§’ä¸ŠæŠ•å½±é«˜æ–¯å¹¶ Î± æ··åˆå…¶é¢œè‰²æ¥ç”Ÿæˆå‡†ç¡®çš„å›¾åƒã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ä¸é—®é¢˜ï¼šé«˜æ–¯åˆ†ç»„å’Œ LangSplat ç­‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š   - è®­ç»ƒå’Œè¯„ä¼°éœ€è¦å¤§é‡ GPU å†…å­˜ï¼Œå¯¼è‡´æŸäº›åœºæ™¯æ— æ³•å¤„ç†ã€‚   - æ— æ³•ä»ä»»æ„è§†è§’æä¾›åˆ†å‰²æ©ç ï¼Œä¹Ÿæ— æ³•å®ç°åœºæ™¯çš„ 3D åˆ†å‰²ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºå¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç° 3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ï¼š   - è®­ç»ƒæ¨¡å‹ä¸ºæ¯ä¸ªé«˜æ–¯ä½“æ·»åŠ åˆ†å‰²ç‰¹å¾å‘é‡ã€‚   - æ ¹æ®ç‰¹å¾å‘é‡å¯¹é«˜æ–¯ä½“è¿›è¡Œèšç±»ï¼Œå®ç° 3D åœºæ™¯åˆ†å‰²ã€‚   - å°†é«˜æ–¯ä½“æŠ•å½±åˆ°å¹³é¢ä¸Šå¹¶ Î± æ··åˆå…¶åˆ†å‰²ç‰¹å¾ï¼Œç”Ÿæˆ 2D åˆ†å‰²æ©ç ã€‚   - ä½¿ç”¨å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–ï¼Œåœ¨ä¸ä¸€è‡´çš„ 2D åˆ†å‰²æ©ç ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆè·¨æ‰€æœ‰è§†è§’ä¸€è‡´çš„åˆ†å‰²æ©ç ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ€§èƒ½ä¸ç›®æ ‡ï¼š   - ä»»åŠ¡ï¼š3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ã€‚   - æ€§èƒ½ï¼šIoU å‡†ç¡®ç‡æ¯”ç°æœ‰æŠ€æœ¯æé«˜ +8%ï¼Œè¡¨æ˜è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå®ç°å…¶ç›®æ ‡ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šå°†åœºæ™¯è¡¨ç¤ºä¸º 3D é«˜æ–¯ä½“é›†åˆï¼Œç¼–ç å‡ ä½•ã€å¤–è§‚å’Œå®ä¾‹åˆ†å‰²ä¿¡æ¯ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆ 2D åˆ†å‰²æ©ç ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¼˜åŒ– 3D é«˜æ–¯ä½“ï¼Œæœ€å°åŒ–æ¸²æŸ“å›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šä½¿ç”¨å¯¹æ¯”åˆ†å‰²æŸå¤±ç›‘ç£ 3D ç‰¹å¾åœºï¼›</p><p>ï¼ˆ5ï¼‰ï¼šå¼•å…¥æ­£åˆ™åŒ–é¡¹ï¼Œå¼ºåˆ¶é«˜æ–¯ä½“åœ¨æ¬§å‡ é‡Œå¾—å’Œåˆ†å‰²ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ç›¸å…³ï¼›</p><p>ï¼ˆ6ï¼‰ï¼šæ¸²æŸ“ 2D ç‰¹å¾å›¾ï¼Œæ ¹æ®å¯¹åº”çš„ 2D åˆ†å‰²æ©ç å¯¹æ¸²æŸ“ç‰¹å¾è¿›è¡Œèšç±»ï¼Œè®¡ç®—å¯¹æ¯”èšç±»æŸå¤±ï¼›</p><p>ï¼ˆ7ï¼‰ï¼šæœ€å¤§åŒ–åŒä¸€åˆ†å‰²å†…ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–ä¸åŒåˆ†å‰²å†…çš„ç‰¹å¾ç›¸ä¼¼åº¦ã€‚</p><p><strong>8. ç»“è®º</strong></p><p><strong>(1)</strong> æœ¬å·¥ä½œçš„ä¸»è¦æ„ä¹‰åœ¨äºï¼š</p><p>æå‡ºäº†å¯¹æ¯”é«˜æ–¯èšç±»æ–¹æ³•ï¼Œå®ç°äº† 3D åœºæ™¯åˆ†å‰²å’Œ 2D åˆ†å‰²æ©ç é¢„æµ‹ï¼Œæœ‰æ•ˆæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚</p><p><strong>(2)</strong> æœ¬æ–‡ä¼˜ç¼ºç‚¹æ€»ç»“ï¼ˆåˆ›æ–°ç‚¹ã€æ€§èƒ½ã€å·¥ä½œé‡ï¼‰ï¼š</p><p><strong>åˆ›æ–°ç‚¹ï¼š</strong></p><ul><li>å¼•å…¥å¯¹æ¯”å­¦ä¹ å’Œç©ºé—´æ­£åˆ™åŒ–ï¼Œæé«˜äº†åˆ†å‰²æ©ç çš„ä¸€è‡´æ€§ã€‚</li><li>ä½¿ç”¨ 3D é«˜æ–¯ä½“è¡¨ç¤ºåœºæ™¯ï¼Œç¼–ç å‡ ä½•ã€å¤–è§‚å’Œå®ä¾‹åˆ†å‰²ä¿¡æ¯ã€‚</li></ul><p><strong>æ€§èƒ½ï¼š</strong></p><ul><li>IoU å‡†ç¡®ç‡æ¯”ç°æœ‰æŠ€æœ¯æé«˜ +8%ï¼Œåˆ†å‰²ç²¾åº¦é«˜ã€‚</li></ul><p><strong>å·¥ä½œé‡ï¼š</strong></p><ul><li>è®­ç»ƒå’Œè¯„ä¼°éœ€è¦å¤§é‡ GPU å†…å­˜ï¼Œå¤§åœºæ™¯å¤„ç†å›°éš¾ã€‚</li><li>æ— æ³•ä»ä»»æ„è§†è§’æä¾›åˆ†å‰²æ©ç ï¼Œæ— æ³•å®ç°åœºæ™¯çš„å®Œæ•´ 3D åˆ†å‰²ã€‚</li></ul><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering**Authors:Xianqiang Lyu, Hui Liu, Junhui Hou**We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary**é›¨æ™¯é‡å»ºï¼šæ— ç›‘ç£åœ°ä»å¤šè§†è§’é›¨æ™¯å›¾é‡å»ºå¹²å‡€åœºæ™¯ã€‚**Key Takeaways**- æå‡ºæ— ç›‘ç£æ¡†æ¶ RainyScapeï¼Œé‡å»ºå¹²å‡€åœºæ™¯ã€‚- RainyScape ç”±ç¥ç»æ¸²æŸ“å’Œé™é›¨é¢„æµ‹æ¨¡å—ç»„æˆã€‚- é™é›¨é¢„æµ‹æ¨¡å—åŒ…å«é¢„æµ‹ç½‘ç»œå’Œå¯å­¦ä¹ æ½œåµŒå…¥ï¼Œæ•æ‰åœºæ™¯çš„é™é›¨ç‰¹å¾ã€‚- åŸºäºç¥ç»ç½‘ç»œçš„å…‰è°±åå·®å±æ€§ï¼Œä¼˜åŒ–ç¥ç»æ¸²æŸ“ç®¡é“ï¼Œè·å¾—ä½é¢‘åœºæ™¯è¡¨ç¤ºã€‚- åˆ©ç”¨è‡ªé€‚åº”æ–¹å‘æ•æ„Ÿæ¢¯åº¦é‡å»ºæŸå¤±ï¼Œä¼˜åŒ–ä¸¤ä¸ªæ¨¡å—ï¼ŒåŒºåˆ†åœºæ™¯ç»†èŠ‚å’Œé›¨ç—•ã€‚- åœ¨ç¥ç»è¾å°„åœºå’Œ 3D é«˜æ–¯å–·æº…ä¸­è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤é›¨ç—•ã€æ¸²æŸ“å¹²å‡€å›¾åƒï¼Œè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½ã€‚- å°†å…¬å¼€æ„å»ºé«˜è´¨é‡æ•°æ®é›†å’Œæºä»£ç ã€‚**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>æ ‡é¢˜ï¼šRainyScapeï¼šåŸºäºè§£è€¦ç¥ç»æ¸²æŸ“çš„æ— ç›‘ç£é›¨æ™¯é‡å»º</p></li><li><p>ä½œè€…ï¼šXianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>å•ä½ï¼šé¦™æ¸¯åŸå¸‚å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»</p></li><li><p>å…³é”®è¯ï¼šé›¨æ™¯é‡å»ºã€ç¥ç»æ¸²æŸ“ã€æ— ç›‘ç£æŸå¤±</p></li><li><p>è®ºæ–‡é“¾æ¥ï¼šxxxï¼ŒGithub é“¾æ¥ï¼šNone</p></li><li><p>æ‘˜è¦ï¼š</p></li></ol><p>ï¼ˆ1ï¼‰ï¼šç ”ç©¶èƒŒæ™¯ï¼šç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å­¦ä¹ åœºæ™¯çš„è¿ç»­ä½“ç§¯è¡¨ç¤ºæ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†å½“è¾“å…¥å›¾åƒå› æ¨¡ç³Šã€å™ªå£°æˆ–é›¨æ°´ç­‰å› ç´ è€Œé€€åŒ–æ—¶ï¼Œæ¸²æŸ“ç»“æœä¸å¯é¿å…åœ°ä¼šå‡ºç°æ˜æ˜¾ä¼ªå½±ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¿‡å»æ–¹æ³•ï¼šç°æœ‰æ–¹æ³•é’ˆå¯¹ä¸åŒçš„é€€åŒ–å› ç´ æå‡ºäº†ç‰¹å®šä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆï¼Œä½†é’ˆå¯¹é›¨æ™¯é‡å»ºä»»åŠ¡çš„æ–¹æ³•è¾ƒå°‘ï¼Œä¸”éš¾ä»¥é€šè¿‡é™„åŠ ç¥ç»æ¸²æŸ“åœºæ¥è¡¨ç¤ºé›¨æ°´ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡º RainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç¥ç»æ¸²æŸ“æ¨¡å—å’Œä¸€ä¸ªé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ é›¨æ»´åµŒå…¥å’Œä½¿ç”¨é¢„æµ‹å™¨æ¥é¢„æµ‹é›¨æ»´æ¡çº¹ï¼Œå¹¶æå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</p><p>ï¼ˆ4ï¼‰ï¼šæ–¹æ³•æ€§èƒ½ï¼šåœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œæœ€è¿‘æå‡ºçš„ 3D é«˜æ–¯ splatting ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º RainyScapeï¼Œä¸€ä¸ªè§£è€¦çš„ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šè¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç¥ç»æ¸²æŸ“æ¨¡å—å’Œä¸€ä¸ªé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œé€šè¿‡å­¦ä¹ é›¨æ»´åµŒå…¥å’Œä½¿ç”¨é¢„æµ‹å™¨æ¥é¢„æµ‹é›¨æ»´æ¡çº¹ã€‚</p><p>ï¼ˆ3ï¼‰ï¼šæå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</p><ol><li>ç»“è®ºï¼š<pre><code>            ï¼ˆ1ï¼‰ï¼šRainyScape åœ¨é›¨æ™¯é‡å»ºé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå®ƒé¦–æ¬¡æå‡ºäº†ä¸€ä¸ªè§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œèƒ½å¤Ÿä»é›¨æ™¯å›¾åƒä¸­æ— ç›‘ç£åœ°é‡å»ºæ— é›¨åœºæ™¯ã€‚ è¯¥æ¡†æ¶é€šè¿‡å°†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹è§£è€¦ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é›¨æ»´æ¡çº¹ï¼Œå¹¶æ¸²æŸ“å‡ºæ¸…æ™°çš„å›¾åƒã€‚            ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šRainyScape åˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ä¸ªè§£è€¦ç¥ç»æ¸²æŸ“æ¡†æ¶ï¼Œå°†åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹è§£è€¦ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†é›¨æ»´æ¡çº¹ï¼Œå¹¶æ¸²æŸ“å‡ºæ¸…æ™°çš„å›¾åƒã€‚            æ€§èƒ½ï¼šRainyScape åœ¨ç»å…¸ç¥ç»è¾å°„åœºå’Œæœ€è¿‘æå‡ºçš„ 3D é«˜æ–¯ splatting ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆæ¶ˆé™¤é›¨æ»´æ¡çº¹å’Œæ¸²æŸ“æ¸…æ™°å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚            å·¥ä½œé‡ï¼šRainyScape çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦è®­ç»ƒç¥ç»æ¸²æŸ“æ¨¡å—å’Œé›¨æ»´é¢„æµ‹æ¨¡å—ï¼Œå¹¶æå‡ºè‡ªé€‚åº”è§’åº¦ä¼°è®¡ç­–ç•¥å’Œæ¢¯åº¦æ—‹è½¬æŸå¤±æ¥è§£è€¦åœºæ™¯é«˜é¢‘ç»†èŠ‚å’Œé›¨æ»´æ¡çº¹ã€‚</code></pre></li></ol><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos. </p><p><a href="http://arxiv.org/abs/2404.11358v2">PDF</a> </p><p><strong>Summary</strong><br>ä»æ¨¡ç³Šè¿åŠ¨å›¾åƒé‡å»ºæ¸…æ™° 3D åœºæ™¯æ–¹æ³•ï¼Œä¼˜åŒ– 3D é«˜æ–¯æŠ•å°„ï¼Œå®ç°ç²¾ç¡®æ‘„åƒæœºä½å§¿åˆå§‹åŒ–ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS ä¼˜åŒ–é«˜æ–¯æŠ•å°„ï¼Œæé«˜è¿åŠ¨æ¨¡ç³Šå›¾åƒ 3D é‡å»ºç²¾åº¦ã€‚</li><li>åˆ©ç”¨é«˜æ–¯æŠ•å°„çš„é‡å»ºèƒ½åŠ›ï¼Œè¿˜åŸç²¾ç»†é”åˆ©åœºæ™¯ã€‚</li><li>ä¼°è®¡æ¯å¹…æ¨¡ç³Šå›¾åƒçš„ 6 è‡ªç”±åº¦æ‘„åƒæœºè¿åŠ¨ï¼Œç”Ÿæˆæ¨¡ç³Šæ¸²æŸ“ç”¨äºä¼˜åŒ–ã€‚</li><li>é«˜æ–¯å¯†åº¦é€€ç«ç­–ç•¥é˜²æ­¢é”™è¯¯ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</li><li>DeblurGS åœ¨å»æ¨¡ç³Šå’Œåˆæˆæ–°è§†è§’æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li><li>é€‚ç”¨äºçœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠç°åœºæ‹æ‘„çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘ã€‚</li><li>DeblurGS æå¤§åœ°æ‰©å±•äº†è¿åŠ¨æ¨¡ç³Šå›¾åƒçš„ 3D é‡å»ºçš„å®é™…åº”ç”¨ã€‚</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: é«˜æ–¯æº…å°„ç›¸æœºè¿åŠ¨æ¨¡ç³Š (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: é¦–å°”å›½ç«‹å¤§å­¦äººå·¥æ™ºèƒ½ä¸ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting Â· Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): å°½ç®¡ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒé‡å»ºæ¸…æ™°çš„ 3D åœºæ™¯æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å‘å®é™…åº”ç”¨çš„è¿‡æ¸¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸»è¦éšœç¢æºäºä¸¥é‡çš„æ¨¡ç³Šï¼Œè¿™ä¼šå¯¼è‡´é€šè¿‡ Structure-from-Motion è·å–åˆå§‹ç›¸æœºå§¿æ€çš„ä¸å‡†ç¡®ï¼Œè€Œè¿™å¾€å¾€æ˜¯ä»¥å‰çš„æ–¹æ³•æ‰€å¿½è§†çš„å…³é”®æ–¹é¢ã€‚</p><p>(2): è¿‡å»çš„æ–¹æ³•ä¸»è¦é›†ä¸­äºæ¨¡ç³Šå›¾åƒçš„å»æ¨¡ç³Šå¤„ç†ï¼Œä½†å¯¹äºåˆå§‹ç›¸æœºå§¿æ€çš„å™ªå£°åˆå§‹åŒ–ä¸é²æ£’ã€‚</p><p>(3): æœ¬æ–‡æå‡º DeblurGSï¼Œè¿™æ˜¯ä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬åˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„å‡ºè‰²é‡å»ºèƒ½åŠ›æ¥æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œä»¥é˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä»ç„¶ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µåœ¨é”™è¯¯çš„ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</p><p>(4): ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ DeblurGS åœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><ol><li>æ–¹æ³•ï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæå‡º DeblurGSï¼Œä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼›</p><p>ï¼ˆ2ï¼‰ï¼šåˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„é‡å»ºèƒ½åŠ›æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼›</p><p>ï¼ˆ3ï¼‰ï¼šä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ï¼›</p><p>ï¼ˆ4ï¼‰ï¼šæå‡ºé«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œé˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚</p><ol><li>ç»“è®ºï¼š</li></ol><p>ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ä»è¿åŠ¨æ¨¡ç³Šå›¾åƒä¼˜åŒ–æ¸…æ™°çš„ 3D é«˜æ–¯æº…å°„çš„æ–¹æ³•ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„å‡ºè‰²é‡å»ºèƒ½åŠ›æ¥æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼Œä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æå‡ºäº†é«˜æ–¯è‡´å¯†åŒ–é€€ç«ç­–ç•¥ï¼Œä»¥é˜²æ­¢åœ¨ç›¸æœºè¿åŠ¨ä»ç„¶ä¸ç²¾ç¡®çš„æ—©æœŸè®­ç»ƒé˜¶æ®µåœ¨é”™è¯¯çš„ä½ç½®ç”Ÿæˆä¸å‡†ç¡®çš„é«˜æ–¯ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p><p>ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šåˆ©ç”¨ 3D é«˜æ–¯æº…å°„çš„é‡å»ºèƒ½åŠ›æ¢å¤ç»†ç²’åº¦çš„æ¸…æ™°åœºæ™¯ï¼Œå³ä½¿åœ¨å™ªå£°ç›¸æœºå§¿æ€åˆå§‹åŒ–çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ï¼›</p><p>æ€§èƒ½ï¼šåœ¨çœŸå®ä¸–ç•Œå’ŒåˆæˆåŸºå‡†æ•°æ®é›†ä»¥åŠç°åœºæ•è·çš„æ¨¡ç³Šæ™ºèƒ½æ‰‹æœºè§†é¢‘çš„å»æ¨¡ç³Šå’Œæ–°è§†å›¾åˆæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼›</p><p>å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•éœ€è¦ä¼°è®¡æ¯ä¸ªæ¨¡ç³Šè§‚æµ‹çš„ 6 è‡ªç”±åº¦ç›¸æœºè¿åŠ¨ï¼Œå¹¶ä¸ºä¼˜åŒ–è¿‡ç¨‹åˆæˆç›¸åº”çš„æ¨¡ç³Šæ¸²æŸ“ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p><details>  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
