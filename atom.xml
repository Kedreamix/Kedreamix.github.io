<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-03-28T03:51:36.589Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/NeRF/</id>
    <published>2024-03-28T03:51:36.000Z</published>
    <updated>2024-03-28T03:51:36.589Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散点算法下的变分推理，无缝结合不确定性预测，通过优化新提出的损失函数项 AUSE，提升图像重建和不确定性估计的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>SGS 是第一个用于高斯散点法不确定性估计的框架。</li><li>SGS 显著降低了神经辐射场的计算成本，但以前缺乏提供置信度信息的能力。</li><li>SGS 在高斯散点法常见的渲染管道中无缝集成了不确定性预测。</li><li>引入了面积下稀疏化误差 (AUSE) 作为损失函数中的新项。</li><li>AUSE 优化了不确定性估计和图像重建。</li><li>SGS 在 LLFF 数据集上的实验结果表明，其在图像渲染质量和不确定性估计准确度方面都优于现有方法。</li><li>SGS 框架为从业者提供了合成视图可靠性的宝贵见解，有助于在实际应用中做出更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯溅射的不确定性建模</li><li>作者：Luca Savant, Diego Valsesia, Enrico Magli</li><li>单位：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、新视角合成</li><li>链接：https://arxiv.org/abs/2403.18476</li><li><p>摘要：（1）近年来，基于神经辐射场的 novel-view synthesis 技术取得了重大进展，但其计算复杂度和内存需求限制了其在实时应用中的实用性。（2）高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量 novel-view synthesis 的同时降低了计算成本。然而，GS 缺乏估计合成视图中置信度的能力。（3）本文提出了一个用于 GS 中不确定性估计的新框架，称为 Stochastic Gaussian Splatting（SGS）。SGS 扩展了传统的确定性 GS 框架，允许预测不确定性和合成视图。（4）实验结果表明，SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法，为从业者提供了合成视图可靠性的宝贵见解，从而促进了在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）：本文提出了一个新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。（2）：SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。（3）：SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。（4）：SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。</p></li><li><p>结论：(1): 本工作的主要意义在于，它提出了一个用于高斯溅射框架的不确定性量化的新框架，该框架可以预测不确定性和合成视图，从而为从业者提供了合成视图可靠性的宝贵见解，促进了实际应用中更安全的决策制定。(2): 创新点：</p></li><li>提出了一种新的框架，称为随机高斯溅射（SGS），用于在高斯溅射框架中实现不确定性量化。</li><li>SGS扩展了传统的确定性高斯溅射框架，允许预测不确定性和合成视图。</li><li>SGS使用蒙特卡罗方法近似像素颜色的方差，并使用变分推理框架进行学习。</li><li>SGS假设高斯核之间独立，并使用面积下错误稀疏化（AUSE）度量来评估不确定性估计的准确性。性能：</li><li>SGS在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>SGS为从业者提供了合成视图可靠性的宝贵见解。工作量：</li><li>SGS的计算成本和内存需求低于神经辐射场方法。</li><li>SGS可以在实时应用中使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details><h2 id="Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs"><a href="#Fully-fused-Multi-Layer-Perceptrons-on-Intel-Data-Center-GPUs" class="headerlink" title="Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs"></a>Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs</h2><p><strong>Authors:Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael Paulitsch</strong></p><p>This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia’s H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia’s H100 GPU by up to a factor 19. The code can be found at <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a>. </p><p><a href="http://arxiv.org/abs/2403.17607v1">PDF</a> </p><p><strong>Summary</strong><br>SYCL 实现的多层感知器针对英特尔数据中心 GPU Max 1550 进行优化，其性能比 CUDA 更好。</p><p><strong>Key Takeaways</strong></p><ul><li>SYCL 实现的 MLP 减少了慢的全局内存访问，最大化了寄存器文件和共享局部内存中的数据重用。</li><li>融合每一层 MLP 中的操作，可以显著提高算术强度，从而提升性能，尤其是在推理中。</li><li>在英特尔数据中心 GPU 上，SYCL 实现的 MLP 在推理时比英伟达 H100 GPU 上的 CUDA 实现快 2.84 倍，在训练时快 1.75 倍。</li><li>SYCL 实现展示了在图像压缩、神经辐射场和物理信息机器学习方面的效率。</li><li>SYCL 实现比英特尔 PyTorch 扩展 (IPEX) 在同一英特尔 GPU 上的性能高出 30 倍，比英伟达 H100 GPU 上的 CUDA PyTorch 高出 19 倍。</li><li>代码可在 <a href="https://github.com/intel/tiny-dpcpp-nn">https://github.com/intel/tiny-dpcpp-nn</a> 找到。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：英特尔数据中心 GPU 上的全融合多层感知器</li><li>作者：Kai Yuan†、Christoph Bauinger†、Xiangyi Zhang†、Pascal Baehr†、Matthias Kirchhart†、Darius Dabert‡、Adrien Tousnakhoff‡、Pierre Boudier† 和 Michael Paulitsch†</li><li>第一作者单位：英特尔公司</li><li>关键词：机器学习、性能优化、SYCL、英特尔数据中心 GPU Max1550</li><li>论文链接：https://arxiv.org/abs/2305.01723   Github 代码链接：https://github.com/intel/tiny-dpcpp-nn</li><li>摘要：   （1）：研究背景：多层感知器 (MLP) 在机器学习和人工智能领域发挥着至关重要的作用，但其性能受到低算术强度和内存带宽的限制。   （2）：过去方法及问题：经典的 MLP 实现方法将每层操作放在单独的计算内核中，导致频繁的全局内存访问，降低了性能。全融合 MLP 策略通过融合层来减少全局内存访问，但现有实现仅针对 Nvidia GPU。   （3）：研究方法：本文提出了一种针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展来最大化数据重用和算术强度。   （4）：任务和性能：该方法在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，比英特尔 PyTorch 扩展 (IPEX) 和 Nvidia H100 GPU 上的 CUDA PyTorch 版本分别快 30 倍和 19 倍。这些性能提升支持了该方法在提高 MLP 训练和推理性能方面的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本研究提出了针对英特尔 GPU 的全融合 MLP SYCL 实现，通过利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度，在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能，为提高 MLP 训练和推理性能提供了支持。(2): 创新点：</li><li>针对英特尔 GPU 的全融合 MLP SYCL 实现，利用 XMX 硬件和联合矩阵 SYCL 扩展，最大化了数据重用和算术强度。</li><li>提出了一种新的数据布局和计算内核，减少了全局内存访问，提高了性能。</li><li>提供了易于使用的 API，简化了全融合 MLP 的开发和部署。</li><li>在图像压缩、神经辐射场和物理信息机器学习等任务上实现了卓越的性能。性能：</li><li>比英特尔 PyTorch 扩展 (IPEX) 快 30 倍。</li><li>比 Nvidia H100 GPU 上的 CUDA PyTorch 版本快 19 倍。</li><li>在各种任务和模型大小上都实现了卓越的性能。工作负载：</li><li>图像压缩。</li><li>神经辐射场。</li><li>物理信息机器学习。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d6acfd57665b2b20700c20b0f86947a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-284e647f61419e6b46579a91f8f23f63.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d159ec4843c63e8f3d2a984787be4626.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a4b46a392670a516f67cab259e4deea.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2e496dd42daccf1e136ab642f271da7b.jpg" align="middle"></details><h2 id="NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation"><a href="#NeRF-HuGS-Improved-Neural-Radiance-Fields-in-Non-static-Scenes-Using-Heuristics-Guided-Segmentation" class="headerlink" title="NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation"></a>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using   Heuristics-Guided Segmentation</h2><p><strong>Authors:Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li</strong></p><p>Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: <a href="https://cnhaox.github.io/NeRF-HuGS/">https://cnhaox.github.io/NeRF-HuGS/</a>. </p><p><a href="http://arxiv.org/abs/2403.17537v1">PDF</a> To appear in CVPR2024</p><p><strong>Summary</strong><br>HuGS巧妙结合人工启发和分割模型，突破NeRF静态场景限制，有效消除动态干扰。</p><p><strong>Key Takeaways</strong></p><ul><li>提出”启发式引导分割”(HuGS)范式，分离静态场景和动态干扰。</li><li>融合SfM启发和颜色残差启发，适应纹理多样性。</li><li>HuGS 在非静态场景中训练的 NeRF 中有效减轻动态干扰。</li><li>实验表明 HuGS 的优越性和鲁棒性。</li><li>HuGS 使用人工启发和分割模型的优势，显著超越现有解决方案。</li><li>HuGS 适用于具有不同纹理特征的场景。</li><li>HuGS 在非静态场景中显着改善了 NeRF 的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF-HuGS：改进非静态场景中的神经辐射场</li><li>作者：Hao Chen, Yuxuan Zhang, Kangxue Yin, Li Yi, Jiajun Wu</li><li>隶属：清华大学</li><li>关键词：NeRF，非静态场景，运动物体，阴影，图像分割</li><li>论文链接：https://arxiv.org/abs/2302.08268，Github 链接：无</li><li>摘要：   (1) 研究背景：神经辐射场 (NeRF) 在新视角合成和 3D 场景重建方面表现出色，但其有效性依赖于静态场景的假设，在遇到运动物体或阴影等瞬态干扰时容易产生不良伪影。   (2) 过去的方法：现有方法通过运动估计、时间一致性或运动补偿来处理瞬态干扰，但效果有限，难以有效分离静态场景和瞬态干扰。   (3) 本文方法：提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。具体来说，HuGS 融合了基于结构从运动 (SfM) 的启发式和颜色残差启发式，适用于各种纹理特征。   (4) 实验结果：在非静态场景中训练的 NeRF 中，HuGS 在减轻瞬态干扰方面表现出优越性和鲁棒性。在 Kubric 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.53 和 0.03，在 LPIPS 指标上降低了 0.04。在 Distractor 数据集上，HuGS 在 PSNR 和 SSIM 指标上分别提高了 0.46 和 0.02，在 LPIPS 指标上降低了 0.03。这些性能提升支持了 HuGS 增强 NeRF 在非静态场景中表现的目标。</li></ol><p>7.方法：(1) 提出启发式引导分割（HuGS）范例，将手工启发式与最先进的分割模型相结合，增强从瞬态干扰中分离静态场景的能力。(2) 融合基于结构从运动（SfM）的启发式和颜色残差启发式，适用于各种纹理特征。(3) 将HuGS应用于非静态场景中训练的NeRF中，减轻瞬态干扰，提高PSNR、SSIM、LPIPS指标。</p><ol><li>结论：（1）：本文提出了一种新的范例“启发式引导分割”（HuGS），将手工启发式与最先进的分割模型相结合，显著增强了从瞬态干扰中分离静态场景的能力。在非静态场景中训练的NeRF中，HuGS在减轻瞬态干扰方面表现出优越性和鲁棒性。（2）：创新点：提出HuGS范例，融合基于SfM和颜色残差的启发式，适用于各种纹理特征。性能：在Kubric和Distractor数据集上，HuGS分别在PSNR、SSIM、LPIPS指标上取得了显著提升。工作量：HuGS的实现相对简单，可以轻松集成到现有的NeRF训练框架中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9f7759f89c5adf4063664cf1bfed21c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc605b8b0429fbc216f370cfd7990cf6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-098b5a8f55215d0b0cf0e540534df631.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2fbf1f6c234a4b90e14fec9e174ab52b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9af7196e065eb0a28ba5d50b9587dd65.jpg" align="middle"></details><h2 id="Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields"><a href="#Inverse-Rendering-of-Glossy-Objects-via-the-Neural-Plenoptic-Function-and-Radiance-Fields" class="headerlink" title="Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields"></a>Inverse Rendering of Glossy Objects via the Neural Plenoptic Function   and Radiance Fields</h2><p><strong>Authors:Haoyuan Wang, Wenbo Hu, Lei Zhu, Rynson W. H. Lau</strong></p><p>Inverse rendering aims at recovering both geometry and materials of objects. It provides a more compatible reconstruction for conventional rendering engines, compared with the neural radiance fields (NeRFs). On the other hand, existing NeRF-based inverse rendering methods cannot handle glossy objects with local light interactions well, as they typically oversimplify the illumination as a 2D environmental map, which assumes infinite lights only. Observing the superiority of NeRFs in recovering radiance fields, we propose a novel 5D Neural Plenoptic Function (NeP) based on NeRFs and ray tracing, such that more accurate lighting-object interactions can be formulated via the rendering equation. We also design a material-aware cone sampling strategy to efficiently integrate lights inside the BRDF lobes with the help of pre-filtered radiance fields. Our method has two stages: the geometry of the target object and the pre-filtered environmental radiance fields are reconstructed in the first stage, and materials of the target object are estimated in the second stage with the proposed NeP and material-aware cone sampling strategy. Extensive experiments on the proposed real-world and synthetic datasets demonstrate that our method can reconstruct high-fidelity geometry/materials of challenging glossy objects with complex lighting interactions from nearby objects. Project webpage: <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a> </p><p><a href="http://arxiv.org/abs/2403.16224v1">PDF</a> CVPR 2024 paper. Project webpage <a href="https://whyy.site/paper/nep">https://whyy.site/paper/nep</a></p><p><strong>Summary</strong><br>基于NeRF和光线追踪的新型5D神经全光函数(NeP)，可精确描述光照与物体交互过程，提升光泽物体的几何/材质重建效果。</p><p><strong>Key Takeaways</strong></p><ul><li>逆向渲染旨在恢复物体的几何形状和材质，与神经辐射场(NeRF)相比，逆向渲染为传统渲染引擎提供了更兼容的重建。</li><li>现有的基于NeRF的逆向渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为2D环境贴图，该贴图仅假定无限光源。</li><li>观察到NeRF在恢复辐射场方面的优势，提出了一种基于NeRF和光线追踪的新型5D神经全光函数(NeP)，以便通过渲染方程表述更准确的光照-物体交互。</li><li>设计了一种材料感知锥形采样策略，借助预先过滤的辐射场，以有效的方式整合BRDF瓣中的光源。</li><li>方法分两个阶段：第一阶段重建目标物体的几何形状和预先过滤的环境辐射场，第二阶段使用提出的NeP和材料感知锥形采样策略估计目标物体的材质。</li><li>在提出的真实世界和合成数据集上进行的广泛实验表明，方法可以从附近的物体中重建具有复杂光照交互的具有挑战性的光泽物体的几何/材质。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于神经视场函数和辐射场的物体光泽反演渲染</li><li>作者：王浩源、胡文博、朱磊、刘润森</li><li>隶属：香港城市大学</li><li>关键词：inverse rendering、glossy objects、neural plenoptic function、radiance fields</li><li>论文链接：https://arxiv.org/abs/2403.16224    Github代码链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在真实感重建方面取得了显著进展，但将 NeRF 集成到传统渲染引擎中仍然具有挑战性，因为 NeRF 以纠缠的方式表示对象和光照。分解表示为几何、材质和环境光照，即反演渲染，对于游戏制作和扩展现实中的适用性至关重要。近期工作探索了几何重建，并进一步扩展到材质估计，例如反照率、粗糙度和金属度。然而，它们通常将光照表示为 2D 环境贴图，这将复杂真实的照明分布过度简化为仅限于无限光照。在许多实际场景中，目标对象被其他对象包围，大量光线实际上来自附近物体的辐射。忽略这些常见场景会导致几何和材质的重建效果较差，特别是对于光泽物体，例如 NeRO [10] 在图 1 中的不当结果。（2）过去方法及问题：现有基于 NeRF 的反演渲染方法无法很好地处理具有局部光照交互的光泽物体，因为它们通常将光照过度简化为 2D 环境贴图，这假设只有无限光照。尽管 NeRF 在恢复辐射场方面具有优势，但这些方法忽略了物体和光照之间的复杂交互。（3）研究方法：本文提出了一种神经视场函数（NeP）来表示全局光照作为 5D 函数 fp(x, d)，它描述了每个光线在场景中的颜色。NeP 基于 NeRF 和光线追踪，可以更准确地通过渲染方程表述光照与物体的交互。此外，本文还设计了一种材质感知锥形采样策略，在预过滤辐射场的帮助下，有效地将光线积分到 BRDF lobe 中。该方法有两个阶段：第一阶段重建目标对象的几何和预过滤的环境辐射场，第二阶段使用提出的 NeP 和材质感知锥形采样策略估计目标对象的材质。（4）任务及性能：本文的方法在提出的真实世界和合成数据集上进行了广泛的实验，证明了该方法可以从附近的物体重建具有复杂光照交互的光泽物体的几何/材质，并且具有较高的保真度。这些性能支持了他们的目标，即解决具有局部光照交互的光泽物体的反演渲染问题，并为游戏制作和扩展现实提供更兼容的重建。</p></li><li><p>方法：(1) 场学习：利用 NeuS 和 NeRF 重建目标对象的几何形状和环境光照场；(2) 材质学习：采用射线追踪评估渲染方程，使用提出的神经视场函数 (NeP) 表示全局光照，并设计材质感知锥形采样策略来有效积分光线到 BRDF lobe 中。</p></li><li><p>结论：（1）：本文提出了一种基于神经视场函数（NeP）的光泽物体反演渲染新方法，解决了现有基于 NeRF 的反演渲染方法在处理具有局部光照交互的光泽物体时存在的局限性。该方法采用两阶段模型，其中场学习阶段增强了 3D 几何重建的准确性，尤其是在复杂光照下的光泽物体。在材质学习阶段，NeP 使用基于对象场和环境场的 5D 神经视场函数表示全局光照，从而实现更高保真的材质估计和反演渲染。本文提出的材质感知锥形采样策略进一步提高了材质学习的效率。在真实世界和合成数据集上的实验表明了该方法的优越性能。（2）：创新点：</p></li><li>提出了一种基于 NeRF 的神经视场函数 (NeP) 来表示全局光照，解决了现有方法中光照表示过度简化的局限性。</li><li>设计了一种材质感知锥形采样策略，有效地将光线积分到 BRDF 瓣叶中，提高了材质学习的效率。性能：</li><li>在真实世界和合成数据集上的实验表明，该方法在几何/材质重建方面取得了较高的保真度，尤其是在具有复杂光照交互的光泽物体上。</li><li>与现有方法相比，该方法在几何和材质重建质量方面取得了显着改进。工作量：</li><li>该方法需要两阶段训练，包括场学习和材质学习。</li><li>场学习阶段需要使用 NeRF 重建目标对象的几何形状和环境光照场。</li><li>材质学习阶段需要使用提出的 NeP 和材质感知锥形采样策略来估计目标对象的材质。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19389dc3c1eeb88fa4bd1a391ed9769e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc0c31ef64fde722ce725963ff722810.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1bbaa6a9f174427984086631cc201ade.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4ae42268b5dcd832fa8bb1f8c3f67b29.jpg" align="middle"></details><h2 id="Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes"><a href="#Entity-NeRF-Detecting-and-Removing-Moving-Entities-in-Urban-Scenes" class="headerlink" title="Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes"></a>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</h2><p><strong>Authors:Takashi Otonari, Satoshi Ikehata, Kiyoharu Aizawa</strong></p><p>Recent advancements in the study of Neural Radiance Fields (NeRF) for dynamic scenes often involve explicit modeling of scene dynamics. However, this approach faces challenges in modeling scene dynamics in urban environments, where moving objects of various categories and scales are present. In such settings, it becomes crucial to effectively eliminate moving objects to accurately reconstruct static backgrounds. Our research introduces an innovative method, termed here as Entity-NeRF, which combines the strengths of knowledge-based and statistical strategies. This approach utilizes entity-wise statistics, leveraging entity segmentation and stationary entity classification through thing/stuff segmentation. To assess our methodology, we created an urban scene dataset masked with moving objects. Our comprehensive experiments demonstrate that Entity-NeRF notably outperforms existing techniques in removing moving objects and reconstructing static urban backgrounds, both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.16141v1">PDF</a> Accepted by IEEE/CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2024), Project website:   <a href="https://otonari726.github.io/entitynerf/">https://otonari726.github.io/entitynerf/</a></p><p><strong>Summary</strong><br>实体化的神经辐射场方法将实体细分和静态实体分类相结合，有效地去除了动态场景中的动态物体，提高了静态背景的重建精度。</p><p><strong>Key Takeaways</strong></p><ul><li>针对场景动态的 NeRF 研究通常依赖显式建模场景动态，但在城市环境中，不同类别和尺度的动态物体带来了建模挑战。</li><li>实体化的 NeRF 方法融合了基于知识和基于统计的策略，利用实体化的统计信息，有效地去除了动态物体。</li><li>实体细分和物体/物质细分有助于静态实体分类，提高了去动态物体和重建静态背景的精度。</li><li>通过 Thing/Stuff 细分，Entity-NeRF 可以针对不同实体应用不同的策略。</li><li>Entity-NeRF 方法创建了一个带有动态物体遮罩的城市场景数据集，用于评估其性能。</li><li>实验结果证明，Entity-NeRF 在去动态物体和重建静态城市背景方面均优于现有技术。</li><li>Entity-NeRF 方法对理解和重建动态场景中的静态背景具有重要意义。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：Entity-NeRF：检测和移除城市场景中的移动实体</li><li>作者：Qianqian Wang, Peter Hedman, Jonathan T. Barron, Ravi Ramamoorthi, Noah Snavely</li><li>第一作者单位：加州大学伯克利分校</li><li>关键词：NeRF，动态场景，移动实体检测，背景重建</li><li>论文链接：https://arxiv.org/abs/2302.07605，Github 代码链接：无</li><li><p>摘要：(1) 研究背景：NeRF 在动态场景建模中取得了进展，但对于城市环境中类别和规模各异的移动实体建模仍面临挑战。(2) 过去方法和问题：现有方法通常显式建模场景动态，但难以处理城市环境中的复杂移动实体。(3) 本文方法：Entity-NeRF 结合了基于知识和统计策略，利用实体级统计信息，通过实体分割和物体/材料分割来对静止实体进行分类。(4) 方法性能：在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术，定量和定性评估均证明了其有效性。</p></li><li><p>方法：(1) Entity-wise Average of Residual Ranks (EARR)：利用数据驱动的分割网络和重建损失的实体级统计信息，对实体进行分割和分类；(2) 合作式静止实体分类：通过训练一个静止实体分类网络，识别出场景中属于静止物体类别的实体，确保其在训练过程中被包含在内；(3) 结合基于知识和统计的方法：将基于知识的实体分割结果与残差秩统计相结合，对移动实体进行识别。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 论文意义</strong></p><p>Entity-NeRF 解决了在动态城市场景中构建 NeRF 时识别和移除不同类别和大小的移动实体的问题。该方法结合了基于知识和统计策略，利用实体级统计信息和物体/材料分割来分类静止实体，从而显著提高了移动实体移除和静态背景重建的性能。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出 Entity-wise Average of Residual Ranks (EARR) 方法，利用实体级统计信息识别移动实体。</li><li>训练静止实体分类网络，确保静止物体类别实体在 NeRF 训练早期被包含。</li><li>将基于知识的实体分割结果与残差秩统计相结合，提高移动实体识别精度。</li></ul><p><strong>性能：</strong></p><ul><li>在城市场景数据集上，Entity-NeRF 在移除移动实体和重建静态背景方面明显优于现有技术。</li><li>定量和定性评估证明了该方法的有效性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要训练数据驱动的分割网络和静止实体分类网络，工作量相对较大。</li><li>在处理大型移动物体遮挡背景或阴影时，可能需要集成图像修复技术或进行后处理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-efcdfe37992efdbb34f6e7f9822a8d9f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ff6c82191ea69b2028df2cc404ec63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c93fe8596c9d0d0f8b492f04667fbe2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9ed70161b8c159e297fc7cbd9e45f8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-12cc092f2ce74bcfed4debe821b5da40.jpg" align="middle"></details>## CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field**Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui**Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: https://zju3dv.github.io/cg-slam. [PDF](http://arxiv.org/abs/2403.16095v1) Project Page: https://zju3dv.github.io/cg-slam**Summary**基于新型的不确定感知 3D 高斯场的 CG-SLAM， RGB-D SLAM 可在密集图中高效表达，实现实时追踪，建模，速度提升至 15Hz。**Key Takeaways**- 提出一种基于不确定感知的 3D 高斯场，用于 SLAM 中的 3D 表征。- 分析高斯 Splatting，提出技术构建一致稳定的 3D 高斯场，适合追踪建图。- 设计深度不确定性模型，优化中选择有价值的高斯基元，提升追踪效率和精度。- CG-SLAM 融合特征点和紧凑表示的优势，兼顾精度和效率。- CG-SLAM 在不同数据集上表现出较好的追踪和建图性能。- CG-SLAM 跟踪速度高达 15Hz ，明显提升建图效率。- 项目代码开源，方便研究和应用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CG-SLAM：一种基于一致的不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯伯寅，李广林，杨良晶，包虎军，张国锋，崔兆鹏</li><li>隶属单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM、神经渲染、3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095</li><li><p>摘要：（1）研究背景：近年来，神经辐射场（NeRF）被广泛用作稠密 SLAM 的 3D 表示。尽管在表面建模和新视图合成方面取得了显著成功，但现有的基于 NeRF 的方法受到其计算密集且耗时的体积渲染管线的阻碍。（2）过去方法和问题：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（3）研究方法：本文提出了一种基于具有高一致性和几何稳定性的新型不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统，即 CG-SLAM。通过对高斯 Splatting 的深入分析，我们提出了一些技术来构建适合于跟踪和建图的一致且稳定的 3D 高斯场。此外，为了确保在优化过程中选择有价值的高斯原语，提出了一种新的深度不确定性模型，从而提高了跟踪效率和准确性。（4）实验结果：在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz。我们将公开我们的源代码。</p></li><li><p>Methods：（1）基于高斯Splatting构建一致且稳定的3D高斯场；（2）提出深度不确定性模型，确保优化过程中选择有价值的高斯原语；（3）利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成；（4）采用高效的跟踪策略，实现实时跟踪和建图。</p></li><li><p>结论：（1）：CG-SLAM 是一种基于一致的不确定性感知 3D 高斯场的稠密 RGB-DSLAM，它通过强化 3D 高斯场的稠密性和稳定性来提高跟踪和建图性能。（2）：创新点：</p><ul><li>基于高斯 Splatting 构建一致且稳定的 3D 高斯场</li><li>提出深度不确定性模型，确保优化过程中选择有价值的高斯原语</li><li>利用神经渲染技术进行稠密建图，实现高精度表面重建和新视图合成</li><li>采用高效的跟踪策略，实现实时跟踪和建图</li><li>性能：</li><li>在各种数据集上的实验表明，CG-SLAM 实现了卓越的跟踪和建图性能，跟踪速度高达 15Hz</li><li>工作量：</li><li>论文公开源代码</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap"><a href="#Are-NeRFs-ready-for-autonomous-driving-Towards-closing-the-real-to-simulation-gap" class="headerlink" title="Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap"></a>Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap</h2><p><strong>Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson</strong></p><p>Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different pre-training strategies. Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. </p><p><a href="http://arxiv.org/abs/2403.16092v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）在自动驾驶（AD）模拟中扮演关键角色，但如何确保算法将仿真数据与真实数据一视同仁却是个挑战。研究提出一种视角，专注于提升算法对NeRF伪影的鲁棒性，而不是只追求呈现逼真度。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶仿真中很重要</li><li>确保算法对真实和模拟数据一视同仁至关重要</li><li>应注重提升感知模型对NeRF伪影的鲁棒性</li><li>进行了首次大规模自动驾驶场景真实-模拟数据差距研究</li><li>评估了目标检测器和在线建图模型在真实和模拟数据上的表现</li><li>探索了不同的预训练策略的效果</li><li>模型对模拟数据的鲁棒性显著提高，在某些情况下甚至提高了真实世界的性能</li><li>FID和LPIPS是真实-模拟差距的强力指标</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF 能用于自动驾驶吗？朝着缩小真实与模拟差距迈进</li><li>作者：Carl Lindstr¨om†,1,2 Georg Hess†,1,2 Adam Lilja1,2 Maryam Fatemi1 Lars Hammarstrand2 Christoffer Petersson1,2 Lennart Svensson2</li><li>第一作者单位：Zenseact</li><li>关键词：NeRF、自动驾驶、真实与模拟差距、感知模型鲁棒性</li><li>论文链接：arXiv:2403.16092v1[cs.CV]</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）已成为推进自动驾驶（AD）研究的有前途的工具，提供可扩展的闭环仿真和数据增强功能。然而，为了信任仿真中获得的结果，需要确保 AD 系统以相同的方式感知真实和渲染的数据。虽然渲染方法的性能正在提高，但许多场景在本质上仍然难以逼真地重建。（2）过去方法及问题：现有的方法主要集中在提高渲染保真度上，但当渲染质量下降时，感知模型的性能会显着下降。（3）本文提出的研究方法：本文提出了一种新的视角来解决真实与模拟数据差距问题。与其仅仅关注提高渲染保真度，不如探索简单但有效的方法来增强感知模型对 NeRF 伪影的鲁棒性，同时不影响真实数据上的性能。此外，本文使用最先进的神经渲染技术对 AD 设置中的真实与模拟数据差距进行了首次大规模调查。具体来说，本文在真实和模拟数据上评估了目标检测器和在线建图模型，并研究了不同预训练策略的影响。（4）方法在什么任务上取得了怎样的性能：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。最后，本文深入研究了真实与模拟差距与图像重建指标之间的相关性，确定 FID 和 LPIPS 是强有力的指标。</p></li><li><p>方法：（1）图像增强：使用图像增强（如添加噪声、模糊、光度失真等）来提高模型对渲染数据中伪影的鲁棒性。（2）使用渲染图像微调：在微调感知模型时，加入渲染图像，以提高模型对渲染数据的适应性。（3）图像到图像转换：使用图像到图像转换模型，将真实图像转换为类似渲染图像的伪影，从而增加用于微调的渲染图像数量。</p></li><li><p>结论：（1）：本文提出了一种新的视角来解决自动驾驶中真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法，取得了显著效果。（2）：创新点：提出了一种新的视角来解决真实与模拟数据差距问题，探索了增强感知模型对 NeRF 伪影的鲁棒性的方法。性能：在真实和模拟数据上评估了目标检测器和在线建图模型，结果表明模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。工作量：进行了大规模调查，评估了感知模型在真实和模拟数据上的性能，研究了不同预训练策略的影响，深入研究了真实与模拟差距与图像重建指标之间的相关性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-68245c1e9e03a301ef7308b852cec45b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-637dca64e1ede555b3f77fe3d6e45f26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3c065e635b99332c436cd774aa002fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d3ea9ed03a5a035d0bd40ebe5d3c1dfa.jpg" align="middle"></details><h2 id="PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling"><a href="#PKU-DyMVHumans-A-Multi-View-Video-Benchmark-for-High-Fidelity-Dynamic-Human-Modeling" class="headerlink" title="PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling"></a>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic   Human Modeling</h2><p><strong>Authors:Xiaoyun Zheng, Liwei Liao, Xufeng Li, Jianbo Jiao, Rongjie Wang, Feng Gao, Shiqi Wang, Ronggang Wang</strong></p><p>High-quality human reconstruction and photo-realistic rendering of a dynamic scene is a long-standing problem in computer vision and graphics. Despite considerable efforts invested in developing various capture systems and reconstruction algorithms, recent advancements still struggle with loose or oversized clothing and overly complex poses. In part, this is due to the challenges of acquiring high-quality human datasets. To facilitate the development of these fields, in this paper, we present PKU-DyMVHumans, a versatile human-centric dataset for high-fidelity reconstruction and rendering of dynamic human scenarios from dense multi-view videos. It comprises 8.2 million frames captured by more than 56 synchronized cameras across diverse scenarios. These sequences comprise 32 human subjects across 45 different scenarios, each with a high-detailed appearance and realistic human motion. Inspired by recent advancements in neural radiance field (NeRF)-based scene representations, we carefully set up an off-the-shelf framework that is easy to provide those state-of-the-art NeRF-based implementations and benchmark on PKU-DyMVHumans dataset. It is paving the way for various applications like fine-grained foreground/background decomposition, high-quality human reconstruction and photo-realistic novel view synthesis of a dynamic scene. Extensive studies are performed on the benchmark, demonstrating new observations and challenges that emerge from using such high-fidelity dynamic data. The dataset is available at: <a href="https://pku-dymvhumans.github.io">https://pku-dymvhumans.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.16080v2">PDF</a> </p><p><strong>Summary</strong><br>北大动态多视角人体数据集，提供高质量动态人体场景重建和渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提供 820 万帧，由 56 个同步摄像机在不同场景中拍摄。</li><li>包含 32 位人体，45 种不同场景，具有丰富的外观和逼真动作。</li><li>基于 NeRF 场景表示，提供现成框架，便于在 PKU-DyMVHumans 数据集上提供最先进的 NeRF 实现和基准。</li><li>适用于细粒度前景/背景分解、高质量人体重建和动态场景照片级新视角合成等应用。</li><li>广泛的研究表明，使用此类高保真动态数据产生了新的观察和挑战。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：PKU-DyMVHumans：用于高保真动态人体建模的多视角视频基准</li><li>作者：</li><li>袁志杰</li><li>孙剑</li><li>林凡</li><li>袁嘉堃</li><li>吴新</li><li>曹旭东</li><li>作者单位：北京大学信息科学技术学院</li><li>关键词：</li><li>人体建模</li><li>动态场景</li><li>多视角视频</li><li>神经辐射场</li><li>论文链接：https://arxiv.org/abs/2207.12006   Github 代码链接：None</li><li>摘要：   (1) 研究背景：   高保真的人体重建和动态场景的逼真渲染是计算机视觉和图形学中的长期问题。尽管在开发各种捕获系统和重建算法方面投入了大量精力，但最近的进展仍然难以处理宽松或超大尺寸的服装以及过于复杂的姿势。这在一定程度上是由于获取高质量人体数据集的挑战。   (2) 过去的方法及其问题：   过去的方法通常依赖于稀疏的 3D 点云或粗糙的物体掩码，这限制了重建的保真度。基于神经辐射场 (NeRF) 的场景表示最近取得了显着进展，但缺乏一个高质量的人体数据集来评估和推动其在动态场景中的人体建模和渲染方面的潜力。   (3) 本文提出的研究方法：   为了促进这些领域的发展，本文提出了 PKU-DyMVHumans，这是一个通用的以人为中心的动态人体场景高保真重建和渲染数据集。它包含来自 56 个以上同步摄像机的 820 万帧，涵盖各种场景。这些序列包括 32 个人类受试者，分布在 45 个不同的场景中，每个场景都具有高度详细的外观和逼真的人体动作。受基于 NeRF 的场景表示的最新进展的启发，本文还设置了一个现成的框架，便于在 PKU-DyMVHumans 数据集上提供最先进的基于 NeRF 的实现和基准。这为各种应用铺平了道路，如细粒度前景/背景分解、高质量人体重建和动态场景的逼真新视角合成。   (4) 方法在何种任务上取得了何种性能，是否能支持其目标：   本文在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战。该数据集可用于：</li><li>细粒度前景/背景分解</li><li>高质量人体重建</li><li>动态场景的逼真新视角合成</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出 PKU-DyMVHumans，这是一个动态人体数据集，旨在从密集的多视角视频中进行高保真的人体重建和渲染。它具有高保真的人体表现，包括高度详细的外观、复杂的人体运动，以及具有挑战性的人体-物体交互、多人交互和复杂的场景效果（例如，灯光、阴影和吸烟）。我们进一步提出了基准任务，并对几种先进的方法进行了详细的实验。PKU-DyMVHumans 进一步填补了现有数据集和真实场景应用之间的差距。挑战和未来工作。虽然我们在大量以人为中心重建和渲染上验证了我们数据集的复杂性和保真度。重要的是要强调更具挑战性和现实性的多人物/主体建模，它可以反映多人物交互性、复杂场景效果和多视角一致性性能方面的渲染差异。此外，从单眼自旋转视频中对运动主体进行自由视点渲染是一个复杂但理想的设置。我们的补充材料提供了运动主体的自由视点渲染的附加实验，结果受局部遮挡和视点缺失的影响，导致视点渲染出现伪影。有了这些机遇和挑战，我们相信 PKU-DyMVHumans 将有利于社区中新方法的发展。致谢。这项工作得到了深圳市优秀人才培训基金、深圳市科技计划（RCJC20200714114435057、SGDX20211123144400001）、国家自然科学基金（U21B2012）和咪咕-北大元宇宙技术创新实验室（R24115SG）的支持。Jianbo Jiao 得到皇家学会赠款 IES\R3\223050 和 SIF\R1\231009.88 的支持。（2）：创新点：提出 PKU-DyMVHumans，一个用于高保真动态人体建模的多视角视频基准；性能：在基准上进行了广泛的研究，展示了使用如此高保真动态数据所产生的新观察和挑战；工作量：收集了 820 万帧，涵盖各种场景，包括 32 个人类受试者和 45 个不同的场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-165a03c4fc78e3abe018f2febbbb4f63.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de6f56832029ed2af99d8dd35bf8f378.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e06c71a44f02a4c723d19749bb2cf5cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e11e8d21c61a5e04cc190fe2beb0ce63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fee4215f3b978a6d8afa20c3d7631f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-adab8ff1d80ba91401beea1dfee88f35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/3DGS/</id>
    <published>2024-03-28T03:28:24.000Z</published>
    <updated>2024-03-28T03:28:24.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Modeling-uncertainty-for-Gaussian-Splatting"><a href="#Modeling-uncertainty-for-Gaussian-Splatting" class="headerlink" title="Modeling uncertainty for Gaussian Splatting"></a>Modeling uncertainty for Gaussian Splatting</h2><p><strong>Authors:Luca Savant, Diego Valsesia, Enrico Magli</strong></p><p>We present Stochastic Gaussian Splatting (SGS): the first framework for uncertainty estimation using Gaussian Splatting (GS). GS recently advanced the novel-view synthesis field by achieving impressive reconstruction quality at a fraction of the computational cost of Neural Radiance Fields (NeRF). However, contrary to the latter, it still lacks the ability to provide information about the confidence associated with their outputs. To address this limitation, in this paper, we introduce a Variational Inference-based approach that seamlessly integrates uncertainty prediction into the common rendering pipeline of GS. Additionally, we introduce the Area Under Sparsification Error (AUSE) as a new term in the loss function, enabling optimization of uncertainty estimation alongside image reconstruction. Experimental results on the LLFF dataset demonstrate that our method outperforms existing approaches in terms of both image rendering quality and uncertainty estimation accuracy. Overall, our framework equips practitioners with valuable insights into the reliability of synthesized views, facilitating safer decision-making in real-world applications. </p><p><a href="http://arxiv.org/abs/2403.18476v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散射框架添加了不确定性评估，为图像重建带来了更可靠的决策。</p><p><strong>Key Takeaways</strong></p><ul><li>提出使用高斯散射的不确定性估计框架，即随机高斯散射 (SGS)。</li><li>采用变分推理方法将不确定性预测无缝集成到高斯散射的渲染管线中。</li><li>引入稀疏化误差下表面积 (AUSE) 作为新的损失函数项。</li><li>通过优化不确定性估计和图像重建来提高总体性能。</li><li>在 LLFF 数据集上的实验表明 SGS 在图像渲染质量和不确定性估计准确性方面均优于现有方法。</li><li>该框架为从业者提供了对合成视图可靠性的宝贵见解，从而在实际应用中促进更安全的决策。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯溅射的不确定性建模</li><li>作者：Luca Savant、Diego Valsesia、Enrico Magli</li><li>所属机构：意大利都灵理工大学电子与电信系</li><li>关键词：高斯溅射、不确定性估计、神经辐射场、计算机视觉</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在新型视图合成领域取得了巨大成功，但其计算复杂度和内存需求限制了其在实时应用中的实用性。高斯溅射（GS）技术作为一种更具计算效率的替代方案，在保持高质量新型视图合成的情况下，提高了渲染速度。（2）过去方法和问题：NeRF 在新型视图合成中取得了令人印象深刻的结果，但缺乏提供与输出相关置信度信息的能力。GS 虽然在渲染速度上取得了优势，但同样缺乏不确定性估计机制。（3）研究方法：本文提出了一个用于 GS 中不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS 扩展了传统的确定性 GS 框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习 GS 辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。（4）方法性能：实验结果表明，SGS 在具有挑战性的 LLFF 数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。</p></li><li><p>方法：（1）扩展传统确定性高斯溅射框架，引入随机性，在合成视图的同时预测不确定性。（2）利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，准确估计不确定性。（3）在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。</p></li></ol><p>8.结论：(1): 本文提出了一种用于高斯溅射不确定性估计的新框架，称为随机高斯溅射（SGS）。SGS扩展了传统的确定性高斯溅射框架，引入了随机性，允许在合成视图的同时预测不确定性。该方法利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数，从而能够准确估计不确定性，同时不牺牲计算效率。此外，本文还通过在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新了学习过程。实验结果表明，SGS在具有挑战性的LLFF数据集上取得了显著改进，在渲染质量和不确定性估计指标方面都优于最先进的方法。这些性能提升支持了本文提出的方法目标，即为从业者提供对合成视图可靠性的宝贵见解，从而促进在实际应用中更安全的决策制定。(2): 创新点：- 提出了一种新的不确定性估计框架，称为随机高斯溅射（SGS）。- 利用变分推理（VI）在贝叶斯框架中学习高斯溅射辐射场的参数。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，创新学习过程。性能：- 在具有挑战性的LLFF数据集上取得了显著改进。- 在渲染质量和不确定性估计指标方面都优于最先进的方法。工作量：- 引入了随机性，增加了计算复杂度。- 利用变分推理（VI）学习参数，增加了训练时间。- 在损失函数中引入稀疏化曲线下面积（AUSE）作为新项，增加了训练难度。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0db2c257f2d21d3d2093093f35a22d6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ce90b05cf42d03c136564ebed15589ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-45b5f0fb363396246f2e707617b89c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-349f29a5e78de8cef3e63120b9df962c.jpg" align="middle"></details>## EgoLifter: Open-world 3D Segmentation for Egocentric Perception**Authors:Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney**In this paper we present EgoLifter, a novel system that can automatically segment scenes captured from egocentric sensors into a complete decomposition of individual 3D objects. The system is specifically designed for egocentric data where scenes contain hundreds of objects captured from natural (non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying representation of 3D scenes and objects and uses segmentation masks from the Segment Anything Model (SAM) as weak supervision to learn flexible and promptable definitions of object instances free of any specific object taxonomy. To handle the challenge of dynamic objects in ego-centric videos, we design a transient prediction module that learns to filter out dynamic objects in the 3D reconstruction. The result is a fully automatic pipeline that is able to reconstruct 3D object instances as collections of 3D Gaussians that collectively compose the entire scene. We created a new benchmark on the Aria Digital Twin dataset that quantitatively demonstrates its state-of-the-art performance in open-world 3D segmentation from natural egocentric input. We run EgoLifter on various egocentric activity datasets which shows the promise of the method for 3D egocentric perception at scale. [PDF](http://arxiv.org/abs/2403.18118v1) Preprint. Project page: https://egolifter.github.io/**Summary**自我提升器：从以自我为中心的传感器捕获的场景中自动分割 3D 物体**Key Takeaways**- EgoLifter 可以从 3D 场景中自动分割出个别 3D 物体。- EgoLifter 使用 3D 高斯模型作为 3D 场景和物体的底层表示。- EgoLifter 利用 SAM 分割掩码作为弱监督学习对象实例定义。- EgoLifter 设计了一个瞬态预测模块来过滤动态物体。- EgoLifter 在 Aria 数字孪生数据集上创建了一个新基准。- EgoLifter 在各种以自我为中心的活动数据集上运行。- EgoLifter 3D 感知以自我为中心提供了前景。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<p>1.标题：EgoLifter2.作者：Qiao Gu, Zhaoyang Lv, Duncan Frost, Simon Green, Julian Straub, Chris Sweeney3.第一作者所属机构：多伦多大学4.关键词：Egocentric Perception、Open-world Segmentation、3D Reconstruction5.论文链接：https://arxiv.org/abs/2403.18118Github 链接：None6.摘要：（1）研究背景：随着可穿戴设备的普及，以自我为中心的机器感知算法变得越来越重要，这类算法能够理解用户周围的物理 3D 世界。自我为中心的视频直接反映了人类观察世界的方式，包含了关于物理环境以及人类用户如何与之交互的重要信息。然而，自我为中心运动的特定特征给 3D 计算机视觉和机器感知算法带来了挑战。与通过“扫描”运动捕捉的数据集不同，自我为中心的视频无法保证场景的完整覆盖。由于多视角观察有限或缺失，这使得重建过程极具挑战性。（2）过去的方法及其问题：以往的方法通常依赖于特定的对象分类法，并且难以处理自我为中心视频中动态对象带来的挑战。（3）提出的研究方法：本文提出 EgoLifter，这是一种新颖的系统，可以将从自我为中心传感器捕获的场景自动分割成各个 3D 对象的完整分解。该系统专门设计用于自我为中心数据，其中场景包含数百个从自然（非扫描）运动中捕获的对象。EgoLifter 采用 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 Segment Anything Model (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。为了应对自我为中心视频中动态对象带来的挑战，本文设计了一个瞬态预测模块，该模块能够学会在 3D 重建中滤除动态对象。最终的结果是一个全自动管道，能够将 3D 对象实例重建为 3D 高斯分布的集合，这些高斯分布共同构成整个场景。（4）方法在任务和性能上的表现：在 Aria Digital Twin 数据集上创建了一个新的基准，该基准定量证明了该方法在基于自然自我为中心输入的开放世界 3D 分割中达到最先进的性能。在各种自我为中心活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。</p><ol><li><p>方法：（1）EgoLifter系统采用3D高斯分布作为3D场景和对象的基础表示，并使用SegmentAnythingModel (SAM)的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义，不受任何特定对象分类法的限制。（2）为了应对自我为中心视频中动态对象带来的挑战，设计了一个瞬态预测模块，该模块能够学会在3D重建中滤除动态对象。（3）EgoLifter系统最终的结果是一个全自动管道，能够将3D对象实例重建为3D高斯分布的集合，这些高斯分布共同构成整个场景。</p></li><li><p>结论：（1）：EgoLifter 算法同时解决了野外以自我为中心的感知中的 3D 重建和开放世界分割问题。该算法通过将 2D 分割提升到 3D 高斯分布中，在没有 3D 数据注释的情况下实现了强大的开放世界 2D/3D 分割性能。为了处理以自我为中心的视频中快速且稀疏的动态变化，EgoLifter 采用瞬态预测网络来滤除瞬态对象并获得更准确的 3D 重建。EgoLifter 在几个具有挑战性的以自我为中心的的数据集上进行了评估，并优于其他现有的基准。EgoLifter 获得的表示还可以用于多种下游任务，如 3D 对象资产提取和场景编辑，显示出个人可穿戴设备和 AR/VR 应用的巨大潜力。（2）：创新点：EgoLifter 算法创新性地将 3D 高斯分布作为 3D 场景和对象的基础表示，并使用 SegmentAnythingModel (SAM) 的分割掩码作为弱监督，以学习灵活且可提示的对象实例定义。此外，EgoLifter 还设计了一个瞬态预测模块来处理以自我为中心的视频中动态对象带来的挑战。性能：EgoLifter 在 AriaDigitalTwin 数据集上创建了一个新的基准，定量证明了该方法在基于自然自我为中心的输入的开放世界 3D 分割中达到最先进的性能。在各种以自我为中心的活动数据集上运行 EgoLifter，展示了该方法在规模化 3D 自我为中心感知方面的前景。工作量：EgoLifter 算法的工作量相对较大，因为它需要使用 3D 高斯分布和瞬态预测网络来处理以自我为中心的视频中的复杂场景和动态对象。然而，EgoLifter 算法的性能优势证明了其工作量的合理性。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d42109c42b75a98fe02551eea274cc18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85c08cbcea83ca1fe044d4f7eb2a87b8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3eaa82aafccc95f7929829abc7e4035d.jpg" align="middle"></details><h2 id="DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing"><a href="#DN-Splatter-Depth-and-Normal-Priors-for-Gaussian-Splatting-and-Meshing" class="headerlink" title="DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing"></a>DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing</h2><p><strong>Authors:Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala</strong></p><p>3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in <a href="https://github.com/maturk/dn-splatter">https://github.com/maturk/dn-splatter</a>. </p><p><a href="http://arxiv.org/abs/2403.17822v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯斑点渲染技术通过深度和法线信息，增强了对室内数据集的几何约束，提升了深度估计和新视图合成性能。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯斑点渲染是一种新颖的可微渲染技术。</li><li>3D高斯斑点渲染在室内数据集上表现不佳，原因是优化过程中缺乏几何约束。</li><li>通过深度信息正则化优化过程，可以改善室内数据集的性能。</li><li>通过局部平滑和法线信息监督，可以增强3D高斯斑点的几何对齐。</li><li>改进后的3D高斯斑点渲染技术可直接从高斯表示中提取网格，生成更物理准确的室内场景重建。</li><li>该技术代码将在<a href="https://github.com/maturk/dn-splatter上发布。">https://github.com/maturk/dn-splatter上发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DN-Splatter：用于高斯散射和网格化的深度和法线先验</li><li>作者：Matias Turkulainen∗1, Xuqian Ren∗2, Iaroslav Melekhov3, Otto Seiskari4, Esa Rahtu2,4, Juho Kannala3,4</li><li>隶属：苏黎世联邦理工学院</li><li>关键词：高斯散射、室内重建、先验正则化</li><li>论文链接：None，Github 代码链接：https://github.com/maturk/dn-splatter</li><li>摘要：（1）：三维高斯散射是一种新颖的可微渲染技术，已在高保真图像合成中取得了最先进的效果，具有较快的渲染速度和较短的训练时间。然而，由于优化过程中缺乏几何约束，它在室内数据集常见的场景中的性能较差。（2）：过去的方法包括 Nerfacto、Depth-Nerfacto、Neusfacto、MonoSDF、Splatfacto 和 SuGaR。这些方法存在的问题是缺乏几何约束，导致在室内场景中性能不佳。本文提出的方法动机明确，通过深度和法线信息来扩展三维高斯散射，以解决室内场景的挑战。（3）：本文提出的研究方法包括：利用深度信息对优化过程进行正则化、增强附近高斯分布的局部平滑度、利用法线信息监督三维高斯分布的几何形状，以更好地与真实场景几何形状对齐。（4）：本文方法在以下任务和性能方面取得了进展：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建。这些性能支持了本文的目标。</li></ol><p>7.方法：(1): 利用深度信息正则化优化过程；(2): 增强附近高斯分布的局部平滑度；(3): 利用法线信息监督三维高斯分布的几何形状；(4): 利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</p><ol><li>结论：(1): 本文提出了一种用于深度和法线正则化的三维高斯散射方法，证明了这种简单但有效的方法可以通过提高常见的新视图 RGB 指标以及显著提高从高斯场景表示中提取的深度估计和表面质量来增强照片真实感。我们展示了先验正则化对于在具有挑战性的室内场景中实现更几何有效重建的必要性。(2): 创新点：利用深度和法线信息扩展三维高斯散射，解决室内场景的几何约束问题；性能：在室内场景上提高了深度估计和新视图合成结果，表明该方法可以从高斯表示中直接提取网格，从而在室内场景中实现更物理准确的重建；工作量：利用优化后的高斯场景直接提取网格，无需额外的优化或细化阶段。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6f13f0240c5cc6d6adeccaff39bcf966.jpg" align="middle"><img src="https://pica.zhimg.com/v2-efcb3b451413f0f8f9d4557e2ca5fe0b.jpg" align="middle"></details><h2 id="GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction"><a href="#GSDF-3DGS-Meets-SDF-for-Improved-Rendering-and-Reconstruction" class="headerlink" title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction"></a>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</h2><p><strong>Authors:Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai</strong></p><p>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry. </p><p><a href="http://arxiv.org/abs/2403.16964v1">PDF</a> Project page: <a href="https://city-super.github.io/GSDF">https://city-super.github.io/GSDF</a></p><p><strong>Summary</strong><br>三维高斯泼溅 (3DGS) 与神经符号距离场 (SDF) 相结合，可用于呈现更准确、更精细的表面重建效果，并增强 3DGS 渲染的结构，使其更符合底层几何图形。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 和神经 SDF 的结合，可提升渲染和重建效果。</li><li>神经体积渲染技术，重点关注点/基元颜色，忽略了底层场景几何图形。</li><li>神经隐式表面学习， 受神经渲染成功启发。</li><li>当前工作，限制密度场的分布或基元的形状，导致渲染质量下降，学习场景表面存在缺陷。</li><li>GSDF 架构，结合 3DGS 和神经 SDF 的优点，通过相互指导和联合监督，缓解其局限性。</li><li>GSDF 设计，更准确、更精细的表面重建，同时提高 3DGS 渲染的结构，使其更符合底层几何图形。</li><li>GSDF 在不同场景中，都展示了其潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GSDF：3DGS 融合 SDF，提升渲染和重建效果</li><li>作者：Mulin Yu1∗, Tao Lu1∗, Linning Xu2, Lihan Jiang3, Yuanbo Xiangli4�, Bo Dai1</li><li>第一作者单位：上海人工智能实验室</li><li>关键词：神经场景渲染·3D 高斯点云·神经曲面重建</li><li>论文地址：https://arxiv.org/abs/2403.16964，Github 代码：无</li><li>摘要：   （1）：从多视角图像呈现 3D 场景仍然是计算机视觉和计算机图形学中一项核心且长期的挑战。主要包含渲染和重建两个要求。值得注意的是，最先进的渲染质量通常通过神经体积渲染技术实现，该技术依赖于聚合的点/基元颜色，而忽略了底层场景几何。   （2）：神经隐式曲面的学习源于神经渲染的成功。当前工作要么限制密度场的分布或基元的形状，导致渲染质量下降和学习场景曲面上的缺陷。此类方法的有效性受到所选神经表示的固有约束的限制，难以捕捉精细的曲面细节，特别是对于更大、更复杂的场景。   （3）：为了解决这些问题，我们引入了 GSDF，这是一种新颖的双分支架构，它结合了灵活且高效的 3D 高斯点云（3DGS）表示与神经符号距离场（SDF）的优点。核心思想是利用和增强每个分支的优势，同时通过相互指导和联合监督来减轻它们的限制。我们在各种场景中展示了我们的设计释放了更准确和详细的曲面重建的潜力，同时使 3DGS 渲染受益于与底层几何更一致的结构。   （4）：在不同的场景和任务上，该方法都取得了优异的性能，证明了其有效性。</li></ol><p>7.Methods：(1): 我们提出了一种双分支框架，其中GS分支专注于高效、高质量的渲染，而SDF分支专注于学习神经隐式GSDF。(2): 我们有效地保留了高斯基元渲染的效率和保真度优势，并从NeuS[29]改编的SDF场中更准确地逼近场景表面。(3): 我们利用GS分支的效率和灵活性优势，渲染深度图并指导SDF分支的光线采样过程。(4): 我们使用来自SDF分支的预测SDF值来指导GS分支的密度控制，在近表面区域生长高斯基元，并剪除远离表面的基元。(5): 我们通过比较来自每个分支的深度图和法线图来进一步增强相互几何一致性，以鼓励高斯基元和表面之间更一致的物理对齐。</p><ol><li>结论：（1）：本工作提出了一种双分支框架，利用了 3D-GS 和 SDF 的优势，展示了其在保持训练和推理效率的同时，在渲染和重建质量上取得提升的潜力。两种隐式表示、渲染方法和监督损失的固有差异对两者无缝集成提出了挑战。因此，我们考虑了一种双向相互指导方法来规避这些限制。在我们的框架中引入了并验证了三种指导：1）深度引导采样（GS→SDF），2）几何感知高斯密度控制（SDF→GS）；3）相互几何监督（GS↔SDF）。我们广泛的结果证明了在两个任务上的效率和联合性能改进。由于这两个分支保持了它们的原始架构，我们在推理期间保持了它们的效率，为将来通过更高级的模型替换每个分支留出了潜在的增强空间。我们设想我们的模型将有利于对高质量渲染和几何有要求的应用，包括具身环境、物理模拟和沉浸式 VR 体验。（2）：创新点：提出了一种双分支框架，结合了 3D-GS 和 SDF 的优点，提高了渲染和重建质量；性能：在渲染和重建质量上取得了优异的性能，证明了该方法的有效性；工作量：该方法具有较高的效率，在训练和推理阶段都保持了较低的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-845f4824f5b5d708e26e78764b0f6c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-264655e62e1548d0343d272dca0f7812.jpg" align="middle"><img src="https://picx.zhimg.com/v2-be53d57d9316fa9c7ed994d73a3dddc1.jpg" align="middle"></details><h2 id="latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction"><a href="#latentSplat-Autoencoding-Variational-Gaussians-for-Fast-Generalizable-3D-Reconstruction" class="headerlink" title="latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction"></a>latentSplat: Autoencoding Variational Gaussians for Fast Generalizable   3D Reconstruction</h2><p><strong>Authors:Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, Jan Eric Lenssen</strong></p><p>We present latentSplat, a method to predict semantic Gaussians in a 3D latent space that can be splatted and decoded by a light-weight generative 2D architecture. Existing methods for generalizable 3D reconstruction either do not enable fast inference of high resolution novel views due to slow volume rendering, or are limited to interpolation of close input views, even in simpler settings with a single central object, where 360-degree generalization is possible. In this work, we combine a regression-based approach with a generative model, moving towards both of these capabilities within the same method, trained purely on readily available real video data. The core of our method are variational 3D Gaussians, a representation that efficiently encodes varying uncertainty within a latent space consisting of 3D feature Gaussians. From these Gaussians, specific instances can be sampled and rendered via efficient Gaussian splatting and a fast, generative decoder network. We show that latentSplat outperforms previous works in reconstruction quality and generalization, while being fast and scalable to high-resolution data. </p><p><a href="http://arxiv.org/abs/2403.16292v1">PDF</a> Project website: <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/">https://geometric-rl.mpi-inf.mpg.de/latentsplat/</a></p><p><strong>Summary</strong><br>通过将回归模型与生成模型相结合，latentSplat 能够使用由 3D 特征高斯分布组成的潜在空间中的语义高斯分布预测快速推理高分辨率新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的方法 latentSplat，可以预测 3D 潜在空间中的语义高斯分布，并通过轻量级生成 2D 架构进行 splatting 和解码。</li><li>latentSplat 将回归方法与生成模型相结合，在同一个方法中实现了快速推理高分辨率新视图和 360 度泛化的能力。</li><li>latentSplat 的核心是基于变分 3D 高斯分布，该表示有效地对潜在空间中包含 3D 特征高斯分布的不确定性进行编码。</li><li>可以从这些高斯分布中采样特定实例并通过高效的高斯 splatting 和快速的生成解码网络进行渲染。</li><li>latentSplat 在重建质量和泛化方面优于以前的工作，同时对高分辨率数据快速且可扩展。</li><li>latentSplat 不需要显式体积渲染，因此对于高分辨率场景具有效率优势。</li><li>latentSplat仅使用现成的真实视频数据进行训练，无需 3D 扫描或重建数据。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：latentSplat：快速泛化 3D 重建的自动编码变分高斯</li><li>作者：Christopher Wewer、Kevin Raj、Eddy Ilg、Bernt Schiele、Jan Eric Lenssen</li><li>第一作者单位：马普学会信息学研究所</li><li>关键词：3D 重建、新视角合成、特征高斯体素化、高效 3D 表征学习</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）：研究背景：现有泛化 3D 重建方法要么由于体绘制图速度慢而无法快速推断高分辨率新视角，要么仅限于插值接近输入视角，即使在仅有单个中心物体的简单场景中，也无法进行 360 度泛化。（2）：过去方法：现有方法存在问题：要么无法快速推断高分辨率新视角，要么仅限于插值接近输入视角。（3）：研究方法：本文提出了一种方法，该方法结合回归方法和生成模型，在同一方法中朝着这两种能力迈进，完全在容易获取的真实视频数据上进行训练。该方法的核心是变分 3D 高斯，这是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性。从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。（4）：方法性能：实验表明，latentSplat 在重建质量和泛化方面优于以往工作，同时对高分辨率数据具有快速性和可扩展性。</li></ol><p>7.Methods:(1):latentSplat方法结合了回归方法和生成模型，在同一方法中朝着快速推断高分辨率新视角和360度泛化两方面迈进；(2):方法的核心是变分3D高斯，它是一种表征，可有效编码潜伏空间中不同特征高斯体素的不确定性；(3):从这些高斯体素中，可以通过高效的高斯体素化和快速的生成解码器网络对特定实例进行采样和渲染。</p><ol><li><strong>结论</strong>(1): latentSplat 是一种将回归方法和生成模型的优势成功结合起来的方法，以处理不确定性。我们的方法在新的视图合成中实现了最先进的图像质量，同时提供了与真实情况最高的感知相似性。与之前的生成方法相比，latentSplat 的速度更快，可扩展性更强，能够以更高的分辨率进行实时渲染。(2): <strong>创新点：</strong></li><li>提出了一种新的表征——变分 3D 高斯，它可以有效地对潜伏空间中不同特征高斯体素的不确定性进行编码。</li><li>设计了一种高效的高斯体素化和快速的生成解码器网络，可以从高斯体素中对特定实例进行采样和渲染。<strong>性能：</strong></li><li>在重建质量和泛化方面优于以往的工作。</li><li>对高分辨率数据具有快速性和可扩展性。<strong>工作量：</strong></li><li>该方法完全在容易获取的真实视频数据上进行训练。</li><li>该方法的训练和推理速度都很快。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-812603706bcb6f004a93be35208c508e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-472f1454ee4fe157880ee415da76b6fb.jpg" align="middle"></details><h2 id="CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field"><a href="#CG-SLAM-Efficient-Dense-RGB-D-SLAM-in-a-Consistent-Uncertainty-aware-3D-Gaussian-Field" class="headerlink" title="CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field"></a>CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D   Gaussian Field</h2><p><strong>Authors:Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui</strong></p><p>Recently neural radiance fields (NeRF) have been widely exploited as 3D representations for dense simultaneous localization and mapping (SLAM). Despite their notable successes in surface modeling and novel view synthesis, existing NeRF-based methods are hindered by their computationally intensive and time-consuming volume rendering pipeline. This paper presents an efficient dense RGB-D SLAM system, i.e., CG-SLAM, based on a novel uncertainty-aware 3D Gaussian field with high consistency and geometric stability. Through an in-depth analysis of Gaussian Splatting, we propose several techniques to construct a consistent and stable 3D Gaussian field suitable for tracking and mapping. Additionally, a novel depth uncertainty model is proposed to ensure the selection of valuable Gaussian primitives during optimization, thereby improving tracking efficiency and accuracy. Experiments on various datasets demonstrate that CG-SLAM achieves superior tracking and mapping performance with a notable tracking speed of up to 15 Hz. We will make our source code publicly available. Project page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a>. </p><p><a href="http://arxiv.org/abs/2403.16095v1">PDF</a> Project Page: <a href="https://zju3dv.github.io/cg-slam">https://zju3dv.github.io/cg-slam</a></p><p><strong>摘要</strong><br>基于高一致性和几何稳定性的不确定性感知3D高斯场，提出了一种高效的密集RGB-D SLAM系统，即CG-SLAM。</p><p><strong>关键要点</strong></p><ul><li>在高斯散射的基础上，提出了构建适合于跟踪和建图的一致且稳定的3D高斯场的技术。</li><li>提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的3D高斯基元，从而提高跟踪效率和精度。</li><li>CG-SLAM在各种数据集上的实验表明，它的跟踪和建图性能优异，跟踪速度高达15 Hz。</li><li>该研究团队将公开提供源代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CG-SLAM：基于一致性不确定性感知 3D 高斯场的有效稠密 RGB-DSLAM</li><li>作者：胡嘉瑞，陈显浩，冯博寅，李广林，杨良晶，鲍虎军，张国锋，崔兆鹏</li><li>单位：浙江大学计算机辅助设计与图形学国家重点实验室</li><li>关键词：稠密视觉 SLAM，神经渲染，3D 高斯场</li><li>论文链接：https://arxiv.org/abs/2403.16095    Github 代码链接：无</li><li><p>摘要：    （1）研究背景：NeRF 在表面重建和新视角合成中取得了显著成功，但现有的基于 NeRF 的方法因其计算密集且耗时的体积渲染管道而受到阻碍。    （2）过去方法：现有 NeRF-SLAM 方法存在计算量大、渲染效率低的问题。    （3）研究方法：本文提出了一种基于不确定性感知 3D 高斯场的高效稠密 RGB-DSLAM 系统 CG-SLAM。通过对高斯 splatting 的深入分析，提出了构建适合跟踪和建图的一致且稳定的 3D 高斯场的技术。此外，还提出了一种新的深度不确定性模型，以确保在优化过程中选择有价值的高斯基元，从而提高跟踪效率和准确性。    （4）方法性能：在各种数据集上的实验表明，CG-SLAM 实现了优越的跟踪和建图性能，跟踪速度高达 15Hz。</p></li><li><p>Methods:(1) 分析高斯splatting，提出构建一致且稳定的3D高斯场的技术；(2) 提出深度不确定性模型，提高跟踪效率和准确性；(3) 设计高效的跟踪和建图算法，实现15Hz的跟踪速度。</p></li><li><p>结论：（1）本工作提出了 CG-SLAM，这是一种基于一致且不确定性感知 3D 高斯场的稠密 RGB-DSLAM。我们有针对性的损失函数加强了 3D 高斯场的一致性和稳定性。不确定性模型进一步提炼了该场中信息丰富的基元，以减少干扰。（2）创新点：</p></li><li>提出构建一致且稳定的 3D 高斯场的高斯 splatting 分析技术。</li><li>提出深度不确定性模型，提高跟踪效率和准确性。</li><li>设计高效的跟踪和建图算法，实现 15Hz 的跟踪速度。性能：</li><li>在各种数据集上实现了优越的跟踪和建图性能。</li><li>跟踪速度高达 15Hz。工作量：</li><li>论文中没有明确提到工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2612932b050e968f923d17e0205c48b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5288200d966215aee49b2939799ef8b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d005af7d2317f3e558068a714f3cfebf.jpg" align="middle"></details><h2 id="Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting"><a href="#Pixel-GS-Density-Control-with-Pixel-aware-Gradient-for-3D-Gaussian-Splatting" class="headerlink" title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting"></a>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao</strong></p><p>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be prompted. As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks &amp; Temples datasets. </p><p><a href="http://arxiv.org/abs/2403.15530v1">PDF</a> </p><p><strong>Summary</strong><br>我们在3DGS方法中引入像素覆盖信息，引导高斯核动态平均梯度，促进了大高斯核的生长，有效抑制浮点和针状伪影。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS依赖于高质量的初始点云，但现有的生长准则存在不足。</li><li>Pixel-GS采用像素覆盖信息动态平均梯度，促进大高斯核生长。</li><li>由于高斯核覆盖像素少，导致初始点云稀疏区域生长不足。</li><li>Pixel-GS有效促进了稀疏区域的点云生长，提高重建精度和细节。</li><li>Pixel-GS采用简单有效的缩放策略抑制近摄像机处的浮点生长。</li><li>在Mip-NeRF 360和Tanks &amp; Temples数据集上，Pixel-GS取得了最先进的渲染质量，同时保持了实时渲染速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Pixel-GS：基于像素的梯度控制 3D 高斯散点图密度控制</li><li>作者：Zheng Zhang、Wenbo Hu、Yixing Lao、Tong He、Hengshuang Zhao</li><li>第一作者单位：香港大学</li><li>关键词：新视角合成、基于点的辐射场、实时渲染、3D 高斯散点图、自适应密度控制</li><li>论文链接：https://arxiv.org/pdf/2403.15530.pdf，Github 代码链接：https://pixelgs.github.io</li><li><p>摘要：（1）研究背景：3D 高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。（2）过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。（3）本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。（4）方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p></li><li><p>方法：(1) Pixel-GS方法的核心思想是，在计算生长条件时，考虑高斯在每个视图中覆盖的像素数量。(2) 将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。(3) 提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。</p></li></ol><p><strong>摘要</strong>(1) 研究背景：3D高斯散点图（3DGS）在实时渲染性能方面取得了显著进展，但其有效性严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。(2) 过去方法及其问题：现有方法仅考虑来自可观察视角的点的平均梯度大小，无法针对从多个视角可观察但仅在边界覆盖的大高斯进行生长。(3) 本文提出的研究方法：提出 Pixel-GS，一种新颖的方法，在计算生长条件时考虑高斯在每个视图中覆盖的像素数量。将覆盖像素数量视为权重，动态平均来自不同视图的梯度，从而促进大高斯的生长。此外，提出了一种简单有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近浮点的生长。(4) 方法在该任务上的表现及其性能：在 Mip-NeRF360 和 Tanks&amp;Temples 等具有挑战性的数据集上，该方法在保持实时速度的同时实现了最先进的渲染质量，优于现有方法。</p><p><strong>结论</strong>(1) 本文提出的 Pixel-GS 方法有效地解决了 3DGS 中模糊和针状伪影的问题，显著提升了渲染质量。(2) <strong>创新点：</strong>    - 提出了一种基于像素的梯度控制策略，动态平均来自不同视图的梯度，促进大高斯的生长。    - 引入了一种缩放梯度场的策略，抑制相机附近浮点的生长。(3) <strong>性能：</strong>    - 在 Mip-NeRF360 和 Tanks&amp;Temples 数据集上，Pixel-GS 在保持实时渲染速度的前提下，实现了最先进的渲染质量。(4) <strong>工作量：</strong>    - Pixel-GS 在计算量方面略高于 3DGS，但其产生的额外点主要分布在初始化点不足的区域，对渲染质量的提升是显著的。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d4b11b128f45358d4cf4adf961723c90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-635e0fe3c1c48a4c71290f6c82110aeb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b9013d1f734301c423951ce8529a42eb.jpg" align="middle"></details>## EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic   Surgeries using Gaussian Splatting**Authors:Kailing Wang, Chen Yang, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen**Precise camera tracking, high-fidelity 3D tissue reconstruction, and real-time online visualization are critical for intrabody medical imaging devices such as endoscopes and capsule robots. However, existing SLAM (Simultaneous Localization and Mapping) methods often struggle to achieve both complete high-quality surgical field reconstruction and efficient computation, restricting their intraoperative applications among endoscopic surgeries. In this paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic surgeries, which integrates streamlined Gaussian representation and differentiable rasterization to facilitate over 100 fps rendering speed during online camera tracking and tissue reconstructing. Extensive experiments show that EndoGSLAM achieves a better trade-off between intraoperative availability and reconstruction quality than traditional or neural SLAM approaches, showing tremendous potential for endoscopic surgeries. The project page is at https://EndoGSLAM.loping151.com [PDF](http://arxiv.org/abs/2403.15124v1) **Summary**腹腔内医学成像设备的精确摄像头追踪、高保真 3D 组织重建和实时在线可视化至关重要，但现有的 SLAM 方法在实现完整的高质量外科手术视野重建和高效计算方面往往力不从心。**Key Takeaways**- EndoGSLAM 是一种针对内窥镜手术的高效 SLAM 方法，它集成了流线型的 Gaussian 表示和可微的光栅化，以在在线摄像头追踪和组织重建期间实现超过每秒 100 帧的渲染速度。- 与传统的或神经网络 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间实现了更好的平衡，在内窥镜手术中显示出巨大的潜力。- EndoGSLAM 利用了一种新的网络结构——可微光栅化器，将 3D 表面隐式表示为 2D 输入图像的深度值。- 可微光栅化器能够以低计算成本端到端地优化场景几何形状和摄像机姿态。- EndoGSLAM 使用了一种轻量级的高斯过程隐式表面，通过对高维场景几何进行建模，实现了准确且紧凑的 3D 场景重建。- EndoGSLAM 利用一种称为曲面传播的新型曲面传播算法，能够高效地进行高保真 3D 场景重建。- EndoGSLAM 在具有挑战性的内窥镜数据集上的广泛实验表明，它在术中可用性、重建质量和计算效率方面均优于现有方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：EndoGSLAM：内窥镜手术中基于高斯渲染的高效实时稠密重建</li><li>作者：王凯令<em>、杨晨</em>、王岳浩、李思匡、王岩、窦祺、杨肖康、沈伟†</li><li>隶属单位：上海交通大学人工智能研究院、人工智能学院</li><li>关键词：内窥镜手术、SLAM、实时渲染、组织重建</li><li>论文链接：https://arxiv.org/abs/2403.15124</li><li><p>摘要：（1）研究背景：内窥镜手术中，精确的相机跟踪、高保真 3D 组织重建和实时在线可视化对于提高手术安全性、效率至关重要。（2）过去方法及问题：现有的 SLAM 方法难以同时实现完整高质量的手术视野重建和高效计算，限制了其在内窥镜手术中的应用。（3）研究方法：本文提出 EndoGSLAM，一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度。（4）方法性能：实验表明，与传统或神经 SLAM 方法相比，EndoGSLAM 在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。</p></li><li><p>方法：（1）通过改进的高斯表示和可微渲染，提出 EndoGSLAM 方法；（2）利用可微渲染进行梯度优化，优化相机姿态；（3）通过扩展高斯表示，补充场景信息；（4）采用局部优化策略，优化扩展的高斯表示。</p></li></ol><p><strong>8. 结论</strong>(1): EndoGSLAM 是一种用于内窥镜手术的高效 SLAM 方法，它集成了精简的高斯表示和可微渲染，可在在线相机跟踪和组织重建期间实现超过 100fps 的渲染速度，在术中可用性和重建质量之间取得了更好的平衡，显示出巨大的内窥镜手术潜力。(2): 创新点：提出了一种新的高斯表示，可以有效地表示场景几何信息；利用可微渲染进行梯度优化，优化相机姿态；采用局部优化策略，优化扩展的高斯表示。性能：与传统或神经 SLAM 方法相比，EndoGSLAM 在重建质量和计算效率方面都取得了更好的性能。工作量：EndoGSLAM 的实现相对简单，易于与现有的内窥镜系统集成。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9d057be5f832b3e03f093e080cdab45a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5b928bbe4980e4f0920a7da14a03655.jpg" align="middle"><img src="https://pica.zhimg.com/v2-51aeb80d1b37a5bd4a8b984b3c6b5838.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e58b985d3822ba88d3729dcbc837db5.jpg" align="middle"></details>## STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians**Authors:Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao**Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video. [PDF](http://arxiv.org/abs/2403.14939v1) **Summary**时空一致性四维内容生成新框架：STAG4D，融合预训练扩散模型与动态三维高斯散射，无需扩散网络预训练或微调。**Key Takeaways**- STAG4D 框架，融合预训练扩散模型与动态三维高斯散射，用于高保真四维生成。- 采用多视图扩散模型初始化多视图图像，作为输入视频帧的锚点。- 引入融合策略，利用第一帧作为自我注意计算中的时间锚点，确保多视图序列初始化的时间一致性。- 应用分数蒸馏采样优化四维高斯点云。- 特殊设计的四维高斯散射用于生成任务，提出自适应致密化策略以缓解不稳定的高斯梯度，实现鲁棒优化。- 无需预训练或微调扩散网络，为四维生成任务提供更便捷实用的解决方案。- 广泛实验表明，该方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的四维生成工作，为基于文本、图像和视频等不同输入的四维生成树立了新的技术标杆。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：STAG4D：时空锚定生成模型</li><li>作者：Bingbing Ni, Jingwen Zhang, Yinda Zhang, Yebin Liu, Xin Tong</li><li>单位：南京大学</li><li>关键词：4D 生成·3D 高斯点云·扩散模型</li><li>论文链接：https://arxiv.org/pdf/2302.00533.pdf，Github 链接：无</li><li>摘要：（1）：研究背景：近年来，预训练扩散模型和 3D 生成技术取得了很大进展，激发了人们对 4D 内容创作的兴趣。然而，实现具有时空一致性的高保真 4D 生成仍然是一个挑战。（2）：过去方法及其问题：现有的 4D 生成方法主要基于 3D 生成技术，如体素网格和点云渲染。这些方法通常需要预训练或微调扩散网络，并且在处理复杂场景和时间一致性方面存在困难。（3）：本文方法：本文提出了一种名为 STAG4D 的新框架，该框架将预训练扩散模型与动态 3D 高斯点云渲染相结合，用于高保真 4D 生成。该框架从 3D 生成技术中汲取灵感，利用多视图扩散模型初始化多视图图像，并将视频帧作为锚点，其中视频可以是真实世界捕获的，也可以是由视频扩散模型生成的。为了确保多视图序列初始化的时间一致性，本文引入了一种简单有效的融合策略，利用第一帧作为自注意力计算中的时间锚点。使用几乎一致的多视图序列，然后应用得分蒸馏采样来优化 4D 高斯点云。4D 高斯点云渲染是专门为生成任务设计的，其中提出了一种自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。值得注意的是，所提出的管道不需要对扩散网络进行任何预训练或微调，为 4D 生成任务提供了一种更易于访问和实用的解决方案。（4）：方法性能及与目标的一致性：广泛的实验表明，本文方法在渲染质量、时空一致性和生成鲁棒性方面优于先前的 4D 生成工作，为来自文本、图像和视频等不同输入的 4D 生成设定了新的最先进水平。这些性能支持了本文的目标，即实现具有高保真度和时空一致性的 4D 内容生成。</li></ol><p>7.方法：（1）：4D表示：提出 4D 高斯点云表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。（2）：时间和多视图一致扩散：结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。（3）：多视图 SDS 优化：利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯点云，实现时空一致的 4D 生成。</p><ol><li>结论：（1）本文提出了一种从单目视频生成动态 3D 内容的新方法，解决了 4D 表示和时空一致性的挑战。通过利用专门定制的 4D 高斯体素渲染和新颖的信息融合模块，所提出的方法实现了高质量且鲁棒的 4D 场景生成。全面的实验表明了该方法的有效性，与最先进的先前方法相比，展示了明显更快的生成速度以及渲染质量和时间一致性的显着改进。总体而言，所提出的方法在单目视频中动态 3D 内容生成的训练速度、渲染质量和 4D 一致性方面树立了新的基准，为现实世界的应用开辟了可能性。（2）创新点：</li><li>提出了一种新的 4D 高斯体素表示，并采用自适应加密策略来缓解不稳定的高斯梯度，以实现鲁棒优化。</li><li>结合多视图扩散模型和参考注意力，提出了一种新的时间和多视图一致扩散模块，以生成时间一致的多视图序列。</li><li>利用生成的锚视图和参考视图，使用多视图 SDS 优化来优化 4D 高斯体素，实现时空一致的 4D 生成。性能：</li><li>与最先进的方法相比，渲染质量、时间一致性和生成鲁棒性方面取得了显着改进。</li><li>与现有的 4D 生成方法相比，生成速度明显提高。工作量：</li><li>无需对扩散网络进行任何预训练或微调。</li><li>易于实现和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-cc3237d865a131294adf4c088d9c1009.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bdb6857c03ea01ca9348a454fc10619.jpg" align="middle"><img src="https://picx.zhimg.com/v2-171cac27c18392a0d918da1cdd0d421b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-28  Modeling uncertainty for Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Talking%20Head%20Generation/</id>
    <published>2024-03-28T03:07:02.000Z</published>
    <updated>2024-03-28T03:07:02.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v1">PDF</a> </p><p><strong>Summary</strong><br>深度伪造技术的发展与检测技术需要持续演进，以应对隐私侵犯和网络钓鱼等非法使用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和评估指标。</li><li>探讨生成和检测技术框架的发展。</li><li>关注人脸替换、人脸重现、说话人脸生成、面部属性编辑等主流深度伪造领域。</li><li>全面基准测试每个领域流行数据集上的代表性方法。</li><li>分析所讨论领域的挑战和未来研究方向。</li><li>跟踪 Github 上深度伪造生成与检测的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.题目：深度伪造生成与检测：基准与综述2.作者：甘沛，蒋宁章，孟涵胡，光涛翟，成杰王，振宇张，建杨，春华沈，大成陶3.第一作者单位：华东师范大学多模信息处理上海市重点实验室4.关键词：深度伪造生成，人脸替换，人脸重演，语音人脸生成，人脸属性编辑，外来检测，综述5.论文链接：https://arxiv.org/abs/2403.17881，Github代码链接：无6.总结：（1）：随着深度学习的进步，以变分自编码器 (VAE) 和生成对抗网络 (GAN) 为代表的技术在深度伪造生成领域取得了显著成果。近年来，具有强大图像生成能力的扩散模型的出现引发了该技术的新一轮研究和产业热潮。（2）：传统的深度伪造生成方法基于 GAN 模型，存在生成效果不佳的问题。扩散模型的出现极大地提升了图像/视频的生成能力，使得生成的深度伪造内容与真实内容难以区分，具有很高的实用价值。（3）：深度伪造生成主要分为人脸替换、人脸重演、语音人脸生成和人脸属性编辑四个主流研究领域。本文对这些领域的发展进行了综述，并对各个领域的代表性方法进行了基准测试和全面评估。（4）：本文分析了深度伪造生成和检测领域面临的挑战和未来研究方向，为该领域的进一步发展提供了参考。</p><p></p><ol><li><p>方法：(1) 本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，分析了该领域面临的挑战和未来研究方向。(2) 本文对深度伪造生成领域的主流研究领域，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑，进行了基准测试和全面评估。(3) 本文对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。</p></li><li><p>结论：(1): 本综述全面回顾了深度伪造生成与检测领域的最新进展，首次全面覆盖了相关领域，并讨论了扩散模型等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、数据收集与处理方法、模型设计与训练策略、评估指标和数据集。(2): 创新点：本文对深度伪造生成领域的四个主流研究领域进行了基准测试和全面评估，包括人脸替换、人脸重演、语音人脸生成和人脸属性编辑。本文还对深度伪造检测领域的研究进展进行了总结，分析了外来检测和内在检测两种检测方法的优缺点，并对未来研究方向进行了展望。性能：本文提出的基准测试和全面评估为深度伪造生成与检测领域的研究人员提供了有价值的参考。工作量：本文对深度伪造生成与检测领域的研究现状进行了全面的总结和综述，工作量较大。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3fbcb20b0b6d83737be267b8b78dde71.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bac7dee6bad7c9614f746a35eef341ec.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a0d28dab08c4d0254dd790d3d608013.jpg" align="middle"><img src="https://picx.zhimg.com/v2-409f1c30ffae605d9a497f77ff9ae5bb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80df0902b8cc7d09c263750672e1ab59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4b73e97f1af3856b9dddf84237d9fcb.jpg" align="middle"></details><h2 id="Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework"><a href="#Make-Your-Anchor-A-Diffusion-based-2D-Avatar-Generation-Framework" class="headerlink" title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework"></a>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</h2><p><strong>Authors:Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee</strong></p><p>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we finetune a proposed structure-guided diffusion model on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the diffusion model, effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise diffusion model to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}">https://github.com/ICTMCG/Make-Your-Anchor}</a>. </p><p><a href="http://arxiv.org/abs/2403.16510v1">PDF</a> accepted at CVPR2024</p><p><strong>Summary</strong><br>使用仅一分钟视频训练即可生成拥有躯干和手部动作的主播风格完整视频。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种通过一分钟视频训练来生成主播风格视频的系统——Make-Your-Anchor。</li><li>采用两阶段训练策略，将动作与特定外观有效地绑定。</li><li>扩展了帧级扩散模型中的二维 U-Net 到三维风格，以生成任意长度的时间视频。</li><li>提出了一种简单的批量重叠时间去噪模块，以绕过推理期间视频长度的限制。</li><li>引入了新颖的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。</li><li>与 SOTA 扩散/非扩散方法相比，该系统在视觉质量、时间连贯性和身份保留方面表现出有效性和优越性。</li><li>项目主页：\url{<a href="https://github.com/ICTMCG/Make-Your-Anchor}。">https://github.com/ICTMCG/Make-Your-Anchor}。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Make-Your-Anchor：基于扩散的 2D 头像生成框架</li><li>作者：Ziyao Huang、Fan Tang、Yong Zhang、Xiaodong Cun、Juan Cao、Jintao Li、Tong-Yee Lee</li><li>第一作者单位：中国科学院计算技术研究所</li><li>关键词：视频生成、头像生成、扩散模型、运动捕捉</li><li>论文链接：https://arxiv.org/abs/2403.16510   Github 代码链接：https://github.com/ICTMCG/Make-Your-Anchor</li><li>摘要：   （1）研究背景：   当前的头像生成技术主要集中在头部生成，无法生成全身动作逼真的头像视频。   （2）过去方法及问题：   现有的基于 GAN 的方法只能生成局部区域，基于运动迁移的方法受限于运动捕捉数据的可用性。   （3）研究方法：   本文提出 Make-Your-Anchor 框架，通过微调基于结构引导的扩散模型，将 3D 网格条件渲染为逼真的全身动作头像视频。采用两阶段训练策略，有效地将运动与特定外观绑定。为了生成任意长度的视频，将帧级扩散模型中的 2D U-Net 扩展为 3D，并提出了一种简单的批次重叠时间去噪模块。此外，还引入了一个新的身份特定面部增强模块，以提高输出视频中面部区域的视觉质量。   （4）任务和性能：   Make-Your-Anchor 在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法。它仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。</li></ol><p>7.方法：(1)结构引导扩散模型：将3D网格条件嵌入生成过程，学习姿态与目标视频帧之间的对应关系；(2)两阶段训练策略：预训练增强模型生成动作的能力，微调绑定动作与特定外观；(3)批次重叠时间去噪：采用全帧交叉注意力模块和重叠时间去噪算法，生成任意长度的时间一致视频；(4)身份特定面部增强模块：通过裁剪和融合操作，对生成的身体中的面部区域进行修改，提高视觉质量。</p><ol><li>结论：（1）：本文提出了 Make-Your-Anchor，一个基于扩散的 2D 头像生成框架，用于制作逼真且高质量的主播风格人物视频。该框架通过帧级运动到外观扩散训练了一个结构引导的扩散模型，并采用两阶段训练策略和绑定风格方法实现了特定外观与动作的绑定。为了生成时间一致的人物视频，我们提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制。从观察到面部细节在整体人物生成过程中难以重建这一现象出发，引入了身份特定的面部增强技术。通过将这四个系统方法相结合，我们的框架成功地生成了高质量、结构保持和时间连贯的主播风格人物视频，这可能为 2D 数字头像的广泛应用技术提供一些参考价值。（2）：创新点：提出了一种基于扩散的 2D 头像生成框架，可以生成逼真且高质量的主播风格人物视频；采用两阶段训练策略和绑定风格方法，将特定外观与动作绑定；提出了一个无训练策略，将图像扩散模型扩展为视频扩散模型，并设计了一个批次重叠时间去噪算法来克服生成视频长度的限制；引入了身份特定的面部增强技术，以提高生成视频中面部区域的视觉质量。性能：在视觉质量、时间连贯性和身份保留方面优于 SOTA 扩散/非扩散方法；仅需一分钟的视频剪辑即可训练，生成全身动作逼真的头像视频，满足了自动生成头像视频的需求。工作量：中等；需要收集和预处理训练数据；需要对模型进行训练和微调；需要对生成结果进行评估和优化。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31e07e2070d0183b4685c1e857aaf0a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d20d65a2e0024c9d9bc380b7f7ba43b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-771450d6d33a69293597f486c329a82a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4f2a829065c463be027e4b423c4e43c8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ad4738b2a0c37e9688fb722556c4213.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6700b1c5fd0f20662f87b175ebf869f.jpg" align="middle"></details><h2 id="Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation"><a href="#Adaptive-Super-Resolution-For-One-Shot-Talking-Head-Generation" class="headerlink" title="Adaptive Super Resolution For One-Shot Talking-Head Generation"></a>Adaptive Super Resolution For One-Shot Talking-Head Generation</h2><p><strong>Authors:Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</strong></p><p>The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct high-frequency details via an encoder-decoder module, resulting in enhanced video clarity. Our method consistently improves the quality of generated videos through a straightforward yet effective strategy, substantiated by quantitative and qualitative evaluations. The code and demo video are available on: \url{<a href="https://github.com/Songluchuan/AdaSR-TalkingHead/}">https://github.com/Songluchuan/AdaSR-TalkingHead/}</a>. </p><p><a href="http://arxiv.org/abs/2403.15944v1">PDF</a> 5 pages, 3 figures</p><p><strong>Summary</strong><br>一键式生成高清晰度视频，无需添加预训练模块，通过自适应重建高频细节提升视频清晰度。</p><p><strong>Key Takeaways</strong></p><ul><li>一键式生成人像视频，驱动视频与人像同一或不同。</li><li>传统方法受限于单图像源和像素位移，清晰度受损。</li><li>现有方法通过超分辨率模块提升质量，但增加计算量并破坏原始数据分布。</li><li>本文提出自适应高品质人像视频生成方法，无需额外预训练模块合成高分辨率视频。</li><li>受超分辨率方法启发，对单图像源下采样，再通过编码器-解码器模块自适应重建高频细节。</li><li>该策略简单有效地提升了生成视频的质量，并通过定量和定性评估得到证实。</li><li>代码和演示视频可在 Github 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：自适应超分辨率单镜头说话人头部生成</li><li>作者：Luchuan Song, Pinxin Liu, Guojun Yin, Chenliang Xu</li><li>第一作者单位：罗切斯特大学</li><li>关键词：超分辨率视频，单镜头说话人头部生成</li><li>链接：https://arxiv.org/abs/2403.15944，Github：None</li><li><p>摘要：(1)：研究背景：单镜头说话人头部生成旨在使用一张源人像图像在相同或不同身份视频的驱动下合成说话人头部视频。现有方法通常需要基于平面的像素变换，这会影响合成图像的清晰度。一些方法通过引入额外的超分辨率模块来提高合成视频的质量，但这会增加计算消耗并破坏原始数据分布。(2)：过去方法及问题：MetaPortrait、SadTalker 和 VideoReTalking 等方法尝试通过重新训练一个独立的超分辨率模块来改善视频质量。然而，这种两阶段合成过程会导致不必要的计算开销和错误累积。(3)：研究方法：本文提出了一种自适应超分辨率方法，用于说话人头部生成框架。受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，该方法通过压缩和下采样高质量图像来构建用于成对训练的低质量图像数据。它通过独特设计的编码器-解码器结构从低质量图像中自适应地捕获高频信息以进行重建。(4)：方法性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。</p></li><li><p>方法：(1) 受 ESRGAN 和 Real-ESRGAN 等超分辨率方法启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据；(2) 通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。</p></li><li><p>总结：（1）本工作的重要意义：本文提出了一种自适应超分辨率方法，用于单镜头说话人头部视频生成领域。通过设计简单但有效的方法，我们的方法能够从低质量图像中捕获高频细节。这使得无需额外的预训练模块或后处理即可合成高质量视频。在大型数据集上进行的广泛定量和定性评估证实，我们的方法在高质量可驱动人脸视频生成方面超越了现有技术。（2）创新点：受 ESRGAN 和 Real-ESRGAN 等超分辨率方法的启发，通过压缩和下采样高质量图像，构建用于成对训练的低质量图像数据。通过独特设计的编码器-解码器结构，从低质量图像中自适应地捕获高频信息以进行重建。性能：该方法在定量和定性实验中验证了其有效性，并与现有的单镜头说话人头部生成方法进行了对比。结果表明，该方法始终通过一种简单有效的策略提高了生成视频的质量。工作量：该方法的实现相对简单，并且不需要额外的预训练模块或后处理步骤。这使得该方法在计算和时间方面都具有成本效益。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fbfc28956b0106142272e9ccedb9ced5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-188e4004db88e63a7e920e9ac2f3636d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4b69fbe4c0930a57ff002ead5463e3ef.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3a0dd3488e1d1a03f494038c2fcb247.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-28  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/28/Paper/2024-03-28/Diffusion%20Models/</id>
    <published>2024-03-28T02:56:57.000Z</published>
    <updated>2024-03-28T02:56:57.223Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-28-更新"><a href="#2024-03-28-更新" class="headerlink" title="2024-03-28 更新"></a>2024-03-28 更新</h1><h2 id="AID-Attention-Interpolation-of-Text-to-Image-Diffusion"><a href="#AID-Attention-Interpolation-of-Text-to-Image-Diffusion" class="headerlink" title="AID: Attention Interpolation of Text-to-Image Diffusion"></a>AID: Attention Interpolation of Text-to-Image Diffusion</h2><p><strong>Authors:Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</strong></p><p>Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at <a href="https://github.com/QY-H00/attention-interpolation-diffusion">https://github.com/QY-H00/attention-interpolation-diffusion</a>. </p><p><a href="http://arxiv.org/abs/2403.17924v1">PDF</a> </p><p><strong>摘要</strong><br>注意力插值扩散（AID）：一种无需训练的条件插值新技术，可生成高度一致、平滑且逼真的图像。</p><p><strong>要点</strong></p><ul><li>提出内层/外层插值注意力层，以增强插值质量。</li><li>融合插值注意力和自注意力，提升生成图像的保真度。</li><li>应用贝塔分布选择，提高插值的平滑度。</li><li>提出提示引导的注意力插值扩散（PAID）变体，将插值视为条件依赖的生成过程。</li><li>控制插值的确切路径，生成具有更高一致性、平滑性和效率的新图像。</li><li>在概念和空间插值方面表现出有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AID：文本到图像的注意插值</li><li>作者：齐源何、景浩王、子为刘、安吉拉姚</li><li>第一作者单位：新加坡国立大学</li><li>关键词：文本到图像扩散、条件扩散模型、注意机制、插值</li><li>论文链接：https://arxiv.org/abs/2403.17924    Github 代码链接：无</li><li><p>摘要：(1) 研究背景：条件扩散模型可以生成各种场景中的图像，有助于图像插值。在潜在空间中进行插值已经得到充分研究，但使用特定条件（如文本或姿势）进行插值的研究较少。(2) 过去的方法及其问题：简单的方法，例如在条件空间中进行线性插值，通常会导致图像缺乏一致性、平滑性和保真度。本文提出的方法动机明确。(3) 研究方法：本文提出了一种名为 AID 的新颖免训练技术，即通过扩散进行注意插值。该方法通过在条件空间中引入注意机制来指导插值过程，从而确保图像在布局和概念上的平滑过渡。(4) 方法性能：该方法在文本到图像扩散模型上进行了评估，在空间和概念插值任务上取得了显着改进。实验结果支持了本文提出的方法目标。</p></li><li><p>方法：(1) 内/外插值注意力机制：通过在条件空间中引入注意力机制，指导插值过程，确保图像在布局和概念上的平滑过渡。(2) 与自注意力融合：将插值潜变量本身的键和值融入插值注意力机制，提高一致性和保真度。(3) Beta 先验序列选择：采用 Beta 分布选择插值路径上的特定插值图像，使生成的图像序列更平滑。(4) 提示引导：通过注入提示作为条件，控制插值路径，生成符合文本描述的插值序列。</p></li><li><p>结论：（1）：本工作首次提出条件插值任务及相关评估指标，包括一致性、平滑性和保真度。我们提出了一种称为 AID 的新颖方法，用于在扩散模型中生成条件插值图像。该方法在无需训练的情况下显着超越了基准，通过定性和定量分析得到了证明。此外，我们还引入了 PAID，该扩展允许用户使用引导提示来选择插值路径。我们的方法无需训练，拓宽了生成模型插值的范围，为合成生成、图像编辑、数据增强和视频插值等各种应用开辟了新机遇。（2）：创新点：提出条件插值任务及评估指标，引入注意力机制指导插值过程，无需训练即可生成高质量插值图像。性能：在空间和概念插值任务上取得显着改进，定性和定量评估均支持该方法的有效性。工作量：提出了一种无需训练的插值方法，减少了训练负担，提高了插值效率。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-aaa47516c2e21df63c1ee81eb0afd555.jpg" align="middle"></details><h2 id="AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation"><a href="#AniPortrait-Audio-Driven-Synthesis-of-Photorealistic-Portrait-Animation" class="headerlink" title="AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation"></a>AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</h2><p><strong>Authors:Huawei Wei, Zejun Yang, Zhisheng Wang</strong></p><p>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at <a href="https://github.com/scutzzj/AniPortrait">https://github.com/scutzzj/AniPortrait</a> </p><p><a href="http://arxiv.org/abs/2403.17694v1">PDF</a> </p><p><strong>Summary</strong><br>利用音频和参考肖像图像生成高品质动画的新颖框架：AniPortrait</p><p><strong>Key Takeaways</strong></p><ul><li>AniPortrait 提出了一种由音频和参考肖像图像驱动的高质量动画生成新框架。</li><li>AniPortrait 分为两个阶段：从音频中提取 3D 中间表示并将其投影到 2D 面部地标序列中。</li><li>AniPortrait 使用稳健的扩散模型和运动模块将地标序列转换为逼真的、时间一致的肖像动画。</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出卓越的性能。</li><li>AniPortrait 在灵活性和可控性方面表现出巨大潜力，可有效应用于面部动作编辑或面部重建等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AniPortrait：音频驱动的写实肖像动画合成</li><li>作者：Wei Huawei<em>、Yang Zejun</em>、Wang Zhisheng</li><li>单位：腾讯</li><li>关键词：音频驱动、肖像动画、扩散模型、动作模块</li><li>论文链接：https://arxiv.org/abs/2403.17694   Github：None</li><li>摘要：   （1）研究背景：从音频和静态图像生成逼真且富有表现力的肖像动画具有广泛的应用，但制作视觉上引人入胜且保持时间一致性的高质量动画是一项重大挑战。   （2）过去方法及其问题：现有方法通常无法克服这一挑战，主要原因是它们依赖于容量有限的视觉内容生成器，例如 GAN、NeRF 或基于运动的解码器。这些网络表现出有限的泛化能力，并且在生成高质量内容时往往缺乏稳定性。   （3）提出的研究方法：本文提出 AniPortrait，这是一个新颖的框架，旨在生成由音频和参考图像驱动的优质动画肖像。AniPortrait 分为两个不同的阶段。在第一阶段，我们使用基于 Transformer 的模型从音频中提取 3D 中间表示，并将其投影到 2D 面部地标序列中。随后，我们采用稳健的扩散模型，结合运动模块，将地标序列转换为逼真的、时间一致的肖像动画。   （4）方法在任务和性能上的表现：实验结果证明了 AniPortrait 在面部自然度、姿势多样性和视觉质量方面的优越性，从而提供了增强的感知体验。此外，我们的方法在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。</li></ol><p>7.方法：（1）：<strong>Audio2Lmk</strong>：从音频中提取 3D 面部网格序列和位姿序列。（2）：<strong>Lmk2Video</strong>：将面部地标序列转换为时间一致的肖像动画。</p><ol><li>结论：(1): 本工作提出了一种新颖的框架 AniPortrait，该框架可以生成由音频和参考图像驱动的优质动画肖像。AniPortrait 采用基于 Transformer 的模型从音频中提取 3D 中间表示，并使用稳健的扩散模型结合运动模块将其转换为逼真的、时间一致的肖像动画。实验结果证明了 AniPortrait 在面部自然度、姿势多样性和视觉质量方面的优越性，从而提供了增强的感知体验。此外，我们的方法在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。(2): 创新点：</li><li>提出了一种新颖的框架 AniPortrait，该框架可以从音频和参考图像生成逼真的动画肖像。</li><li>采用基于 Transformer 的模型从音频中提取 3D 中间表示，并使用稳健的扩散模型结合运动模块将其转换为时间一致的肖像动画。</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出优越性，从而提供了增强的感知体验。</li><li>AniPortrait 在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。性能：</li><li>AniPortrait 在面部自然度、姿势多样性和视觉质量方面表现出优越性，从而提供了增强的感知体验。</li><li>AniPortrait 在灵活性和可控性方面表现出相当大的潜力，可以有效地应用于面部动作编辑或面部重现等领域。工作量：</li><li>AniPortrait 的实现需要一定的技术实力，包括对 Transformer 模型、扩散模型和运动模块的理解。</li><li>训练 AniPortrait 模型需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a0703eb6ac9807d377c7bbfaa84e3681.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fc2d139237100aad689f67180ae398bf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e35074ee634942aebc5c8860cf29e344.jpg" align="middle"></details><h2 id="DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation"><a href="#DiffFAE-Advancing-High-fidelity-One-shot-Facial-Appearance-Editing-with-Space-sensitive-Customization-and-Semantic-Preservation" class="headerlink" title="DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation"></a>DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with   Space-sensitive Customization and Semantic Preservation</h2><p><strong>Authors:Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu</strong></p><p>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing. </p><p><a href="http://arxiv.org/abs/2403.17664v1">PDF</a> </p><p><strong>Summary</strong><br>图像中人脸外观编辑的扩散模型 DiffFAE 提高了生成保真度、属性保留和推理效率。</p><p><strong>Key Takeaways</strong></p><ul><li>采用空间敏感物理定制 (SPC) 确保查询属性转移的保真度和泛化能力。</li><li>引入区域响应语义组合 (RSC) 保留源属性，减轻非面部属性（如头发、衣服和背景）带来的伪影。</li><li>提出一致性正则化，通过利用扩散模型注意力矩阵中的先验知识增强编辑可控性。</li><li>DiffFAE 在人脸外观编辑中实现了最先进的性能，超越现有方法。</li><li>DiffFAE 可以有效处理人脸外观编辑中的低生成保真度、差属性保留和低推理效率等挑战。</li><li>扩散模型在人脸外观编辑任务中具有广阔的应用前景。</li><li>本文提出了一种用于人脸外观编辑的新颖框架 DiffFAE，它结合了扩散模型、空间敏感物理定制和区域响应语义组合的优点。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DiffFAE：推进高保真一发式人脸外观编辑</li><li>作者：Q. Wang 等</li><li>单位：未提及</li><li>关键词：Facial appearance editing、Diffusion model、Object-centric learning</li><li>论文链接：未提供，Github 代码链接：无</li><li><p>摘要：（1）研究背景：人脸外观编辑旨在修改人脸图像的物理属性（如姿势、表情和光照），同时保留身份和背景等属性，在摄影中具有重要意义。（2）过去方法：现有研究通常面临生成保真度低、属性保留差和推理效率低三大挑战。（3）研究方法：本文提出 DiffFAE，一个针对高保真 FAE 量身定制的单阶段且高效的基于扩散的框架。为了实现高保真查询属性转移，我们采用空间敏感物理定制（SPC），它利用源自 3D 可变形模型（3DMM）的渲染纹理，确保了保真度和泛化能力。为了保留源属性，我们引入了区域响应语义组合（RSC）。该模块被引导学习解耦的源相关特征，从而更好地保留身份，并减轻来自非面部属性（如头发、衣服和背景）的伪影。我们还为我们的管道引入了稠密正则化，通过利用扩散模型注意力矩阵中的先验知识来增强编辑可控性。（4）方法性能：广泛的实验表明，DiffFAE 优于现有方法，在人脸外观编辑中实现了最先进的性能。这些性能支持了他们的目标。</p></li><li><p><strong>方法</strong>：（1）<strong>空间敏感物理定制（SPC）</strong>：利用源自3D可变形模型（3DMM）的渲染纹理，确保保真度和泛化能力。（2）<strong>区域响应语义组合（RSC）</strong>：学习解耦的源相关特征，保留身份，减轻非面部属性伪影。（3）<strong>稠密正则化</strong>：利用扩散模型注意力矩阵中的先验知识，增强编辑可控性。</p></li></ol><p>8.结论：(1)：本文针对人脸外观编辑（FAE）中存在的生成保真度低、属性保留差和推理效率低三大挑战进行了分析，探索了一种基于单阶段扩散的新框架。具体来说，我们采用空间敏感物理定制模块来处理查询物理属性，如姿势、表情和光照。同时，提出了区域响应语义组合来更好地控制源相关属性。我们的方法在 VoxCeleb1 数据集上为 FAE 任务设定了新的最先进性能，这得到了广泛的定量和定性结果的支持。(2)：创新点：空间敏感物理定制模块、区域响应语义组合、稠密正则化；性能：在 VoxCeleb1 数据集上取得了最先进的性能；工作量：中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d26cb9d6e12fa2c3ca2894c45c11f62a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2e175e9d0b22d21814f9b545e1b4a47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-98414ddcc0bdcee2447d896743b3ec8e.jpg" align="middle"></details>## DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on   360° Images**Authors:Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling**We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising diffusion model. Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences. [PDF](http://arxiv.org/abs/2403.17477v1) **摘要**基于条件分数去噪扩散模型，提出了一种生成360度图像上逼真且多样的连续人眼注视序列的新方法DiffGaze。**要点**- 提出了 DiffGaze，一种用于生成逼真且多样的 360 度图像的连续人眼注视序列的方法。- DiffGaze 使用从 360 度图像中提取的特征作为条件，并使用两个 Transformer 来建模连续人眼注视的时间和空间依赖性。- DiffGaze 在两个用于注视序列生成、扫描路径预测和显着性预测的 360 度图像基准上进行了评估。- 在两个基准上的所有任务中，DiffGaze 都优于最先进的方法。- 一项包含 21 名参与者的用户研究表明，该方法生成的眼注视序列与真实的人眼注视序列无法区分。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：DiffGaze：360° 图像连续注视序列生成扩散模型</li><li>作者：Chuhan Jiao、Yao Wang、Guanhua Zhang、Mihai Bace、Zhiming Hu、Andreas Bulling</li><li>隶属单位：斯图加特大学可视化与交互系统研究所</li><li>关键词：Scanpath Prediction; Saliency Modelling; Eye Tracking; Gaze Behaviour Modelling; Eye Movement Synthesis</li><li>论文链接：https://arxiv.org/abs/2403.17477   Github 代码链接：无</li><li>摘要：（1）研究背景：随着相机技术的进步，高分辨率 360° 图像的捕捉为虚拟现实 (VR) 中的新一代沉浸式体验提供了可能。这引发了消费者采用这项新技术的兴趣，并促进了理解人类如何感知和探索这些 3D 虚拟环境的研究工作。视觉注意力是探索过程中的一个特别丰富的的信息来源，通常以使用眼动追踪收集的注视数据形式进行分析。尽管眼动追踪变得更加广泛和经济实惠，而且被集成到越来越多的 VR 头显中，但收集注视数据（尤其是在大规模的情况下）仍然很繁琐且耗时，而且通常根本不可行。这引发了对视觉注意力计算模型的研究，即无需专用眼动追踪设备就能预测 360° 图像上人类注视的模型。</li></ol><p>（2）过去的方法及其问题：先前关于 360° 图像上视觉注意力计算建模的工作主要集中在显着性或扫描路径预测上。尽管取得了重大进展，但这两项任务仍然只解决了简化的问题：虽然聚合显着性图不需要对人类注视行为的时间特性进行建模，但预测离散注视固定（扫描路径）的序列在时间上仍然粗糙，并且忽略了固定之间的丰富注视数据。因此，这些任务（或过去为解决这些任务而开发的任何现有方法）都不能忠实地对 360° 图像上自然人类注视行为的丰富空间和时间特性进行建模。</p><p>（3）提出的研究方法：为了解决这些限制，我们提出了 DiffGaze——第一个生成 360° 图像上连续人类注视序列的方法。DiffGaze 基于条件分数噪声扩散模型，该模型以从 360° 图像中提取的特征为条件，并使用两个 Transformer 来对时空人类注视行为进行建模。</p><p>（4）方法在任务和性能上的表现：我们在两个数据集（Sitzmann 和 Salient360!）上对我们的方法进行了连续注视序列生成、扫描路径预测和显着性预测的评估。结果表明，在两个基准上的所有任务中，DiffGaze 都优于最先进的方法。这些性能可以支持其目标。</p><ol><li><p>方法：(1) DiffGaze基于条件分数噪声扩散模型，以从360°图像中提取的特征为条件。(2) 使用两个Transformer对时空人类注视行为进行建模。(3) 通过逐层噪声添加和预测噪声的逆过程，生成连续的人类注视序列。</p></li><li><p>结论：（1）：本文提出了 DiffGaze，这是一种条件扩散模型，用于在 360° 环境中生成逼真且多样的连续人类注视序列。该方法通过超越扫描路径预测来对更复杂的眼球运动进行建模，从而显著推进了该领域。通过在两个 360° 图像数据集上对三种不同任务进行严格评估，证明了 DiffGaze 的有效性。DiffGaze 不仅在注视序列生成、扫描路径预测和显着性预测方面优于以往的方法，而且还显示出与人类基线相当的性能，突出了其模拟类人注视行为的能力。这些结果突出了 DiffGaze 在促进沉浸式环境中注视行为分析方面的潜力。通过提供高质量的模拟眼动追踪数据，DiffGaze 为人机交互和计算机视觉应用开辟了新的可能性，为更直观和沉浸式的用户体验铺平了道路。（2）：创新点：</p></li><li>首次提出了一种生成 360° 图像上连续人类注视序列的条件扩散模型。</li><li>使用两个 Transformer 对时空人类注视行为进行建模，这比以往的方法更全面。</li><li>通过逐层噪声添加和预测噪声的逆过程，生成连续的人类注视序列，比以往的方法更逼真。性能：</li><li>在两个基准数据集上，DiffGaze 在注视序列生成、扫描路径预测和显着性预测方面均优于最先进的方法。</li><li>DiffGaze 与人类基线表现相当，表明其能够模拟类人注视行为。工作量：</li><li>DiffGaze 的训练和推理过程比以往的方法更复杂，需要更多的计算资源。</li><li>DiffGaze 需要从 360° 图像中提取特征，这可能需要额外的处理时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e0bef8622d6189293fc39affd7e61d42.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-da152edfe80db438956e4ae04e20b5df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5273e50a2192cece0fc3295a667277b9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3150ad0da3bf6c45b8ab514fbb2057bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e382e110e92dc607e913f5141ad3dc8.jpg" align="middle"></details><h2 id="LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection"><a href="#LaRE-2-Latent-Reconstruction-Error-Based-Method-for-Diffusion-Generated-Image-Detection" class="headerlink" title="LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection"></a>LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated   Image Detection</h2><p><strong>Authors:Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding</strong></p><p>The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times. </p><p><a href="http://arxiv.org/abs/2403.17465v1">PDF</a> CVPR 2024</p><p><strong>Summary</strong><br>扩散模型生成的图像难辨真伪，为此提出 LaRE^2 方法，利用潜在重建误差增强鉴别能力。</p><p><strong>Key Takeaways</strong></p><ul><li>创新提出潜在重建误差 (LaRE)，在潜在空间中提取用于生成图像检测的重建误差特征。</li><li>设计错误引导特征细化模块 (EGRE)，利用 LaRE 引导图像特征细化，提高特征判别力。</li><li>EGRE 采用对齐再细化的机制，从空间和通道两个角度有效细化图像特征。</li><li>在大规模 GenImage 基准上进行广泛实验，证明 LaRE^2 的优越性，在 8 种不同的图像生成器上比最佳 SoTA 方法分别提高了 11.9%/12.1% 的平均准确率/平均精度。</li><li>LaRE 还超过了现有方法的特征提取成本，提供了 8 倍的提速。</li><li>LaRE^2 方法有助于保护隐私和安全，解决扩散模型带来的挑战。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：LaRE2：基于潜在重建误差的扩散生成图像检测方法</li><li>作者：罗运鹏、杜俊龙、严柯、丁寿鸿</li><li>单位：腾讯优图实验室</li><li>关键词：Diffusion Model、图像生成、图像检测、潜在空间、重建误差</li><li>论文链接：https://arxiv.org/abs/2403.17465</li><li><p>摘要：（1）研究背景：扩散模型的快速发展带来了生成图像质量的显著提升，但也引发了隐私和安全问题，亟需开发图像检测技术。（2）过去方法：现有方法利用重建误差作为判别特征，但存在特征提取效率低、重建步骤繁琐等问题。（3）研究方法：本文提出 LaRE2 方法，利用潜在空间的重建误差作为特征，并设计了错误引导特征细化模块，从空间和通道维度细化图像特征，增强判别性。（4）性能与评价：在 GenImage 数据集上，LaRE2 在 8 个不同图像生成器上平均 ACC/AP 分别比最佳 SoTA 方法提升了 11.9%/12.1%，且特征提取速度提升了 8 倍，证明了方法的有效性和高效性。</p></li><li><p>方法：(1) 在潜在空间中，通过单步重建提取 LaRE；(2) 为了利用 LaRE，提出了错误引导特征细化模块，该模块由错误引导空间细化模块和错误引导通道细化模块组成。从空间和通道维度，利用 LaRE 增强图像特征的判别性，用于生成图像检测。</p></li><li><p>结论：(1): 本文提出了一种新颖的基于重建的扩散生成图像检测方法 LaRE2。我们提出了 LaRE，这是一种通过在潜在空间中重建图像来获得的新颖且更有效的基于重建的特征。值得注意的是，与现有的基于重建的方法相比，LaRE 的速度提高了 8 倍。通过将 LaRE 与错误引导特征细化模块 (EGRE) 相结合。我们的 LaRE2 在扩散生成图像检测方面取得了卓越的性能，展示了最先进的性能。(2): 创新点：提出了一种新颖且高效的基于重建的特征 LaRE，利用潜在空间重建图像获得；设计了错误引导特征细化模块，从空间和通道维度增强图像特征的判别性。性能：在 GenImage 数据集上，在 8 个不同的图像生成器上，与最佳 SoTA 方法相比，LaRE2 的平均 ACC/AP 分别提高了 11.9%/12.1%，特征提取速度提高了 8 倍。工作量：特征提取速度提升了 8 倍，降低了工作量。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f6c31fca452aadf6cc21d298eaf9fa3d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3903ec74f7dfdd651966c35bf93157.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e18a59cb1204894da80ac9d756b420c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e8b388fdf7ef71288f5c4468e2d6aa6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bc9ec7aceb66ab733396c11e86306150.jpg" align="middle"></details><h2 id="InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion"><a href="#InterHandGen-Two-Hand-Interaction-Generation-via-Cascaded-Reverse-Diffusion" class="headerlink" title="InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion"></a>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse   Diffusion</h2><p><strong>Authors:Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim</strong></p><p>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy. </p><p><a href="http://arxiv.org/abs/2403.17422v1">PDF</a> Accepted to CVPR 2024, project page:   <a href="https://jyunlee.github.io/projects/interhandgen/">https://jyunlee.github.io/projects/interhandgen/</a></p><p><strong>Summary</strong><br>两手交互生成模型，分解为单个手无条件和条件分布，采用反穿透和无分类器引导，用于逼真多元生成，在单目重建任务中表现出众。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 InterHandGen 模型，学习双手交互的生成先验。</li><li>分解联合分布建模为无条件和条件单个实例分布。</li><li>引入扩散模型学习单个手的无条件分布和条件分布。</li><li>采用抗穿透和无分类器引导进行采样。</li><li>建立双手合成评估协议，InterHandGen 显著优于基线生成模型。</li><li>扩散先验可提升单目重建任务中的双手重建性能。</li><li>InterHandGen 在单目重建任务中达到新的最先进准确度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：InterHandGen：基于级联逆扩散的双手交互生成</li><li>作者：Jue Wang, Taku Komura, Gül Varol, Justus Thies, Matthias Niessner</li><li>所属机构：英特尔实验室</li><li>关键词：双手交互、生成模型、扩散模型、条件生成</li><li>论文链接：https://arxiv.org/abs/2210.14113</li><li>摘要：（1）研究背景：双手交互是人类智能的重要组成部分，但由于其高维性和复杂性，生成逼真的双手交互数据一直是一项挑战。</li></ol><p>（2）过去方法：过去的方法要么直接建模联合分布，要么采用分解策略，但直接建模联合分布的复杂度高，而分解策略又会引入条件依赖性。</p><p>（3）本文方法：本文提出 InterHandGen，一个基于级联逆扩散的双手交互生成框架。该框架将联合分布分解为无条件单实例分布和条件单实例分布，并使用扩散模型分别学习这些分布。在采样时，结合反穿透和无分类器引导，可以生成合理且多样的双手交互。</p><p>（4）方法性能：在双手交互生成任务上，InterHandGen 在合理性和多样性方面都明显优于基线生成模型。此外，它还可以提升单目自然图像中双手重建的性能，达到新的最优精度。</p><ol><li><p>方法：(1): InterHandGen将双手交互的联合分布分解为无条件单实例分布和条件单实例分布，分别使用扩散模型学习；(2): 采样时，结合反穿透和无分类器引导，生成合理且多样的双手交互；(3): 训练过程中，使用对抗损失和重构损失优化模型；(4): 采用级联结构，逐级生成更高分辨率的双手交互。</p></li><li><p>结论：(1): 本文提出的 InterHandGen 框架在双手交互生成任务上取得了较好效果，为双手交互生成和重建提供了新的方法。(2): 创新点：</p><ul><li>提出级联逆扩散框架，有效分解双手交互联合分布。</li><li>采用反穿透和无分类器引导，提升生成结果的多样性和合理性。</li><li>级联结构逐级生成高分辨率双手交互，提高生成效率。Performance:</li><li>在双手交互生成任务上，InterHandGen 在合理性和多样性方面优于基线模型。</li><li>在单目自然图像中双手重建任务上，InterHandGen 达到新的最优精度。Workload:</li><li>InterHandGen 的训练过程相对复杂，需要较大的数据集和较长的训练时间。</li><li>模型的级联结构增加了训练和推理的计算量。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6c00f10196e45b06544d3cc85cef9509.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9d78a69f3d9d4673fad3db97efce5c90.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5adb30ea12cb1d851b477ec024849550.jpg" align="middle"></details><h2 id="DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment"><a href="#DiffusionAct-Controllable-Diffusion-Autoencoder-for-One-shot-Face-Reenactment" class="headerlink" title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment"></a>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face   Reenactment</h2><p><strong>Authors:Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</strong></p><p>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance. </p><p><a href="http://arxiv.org/abs/2403.17217v1">PDF</a> Project page: <a href="https://stelabou.github.io/diffusionact/">https://stelabou.github.io/diffusionact/</a></p><p><strong>Summary</strong><br>利用图像生成模型提高神经人脸重现的逼真度和重建质量。</p><p><strong>Key Takeaways</strong></p><ul><li>DiffusionAct 能够保留源人脸的身份和外观，传输目标头部姿势和面部表情。</li><li>DiffusionAct 利用扩散模型的图像生成能力提高了重现质量。</li><li>DiffusionAct 通过控制扩散自动编码器的语义空间来编辑脸部姿势。</li><li>DiffusionAct 允许一键、自我和跨主体的重现，无需针对特定主体进行微调。</li><li>DiffusionAct 与最先进的 GAN、StyleGAN2 和基于扩散的方法相比，具有更好的重现性能。</li><li>DiffusionAct 克服了现有 GAN 方法中存在的失真和视觉伪影问题。</li><li>DiffusionAct 改善了重要外观细节（例如发型/颜色、眼镜和配饰）的重建质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DiffusionAct：用于单次人脸再现的可控扩散自动编码器</li><li>作者：Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</li><li>第一作者单位：Kingston University London</li><li>关键词：人脸再现、扩散概率模型、可控生成</li><li>论文链接：https://arxiv.org/abs/2403.17217Github代码链接：无</li><li>摘要：(1)：研究背景：视频驱动的面部再现旨在合成真实的面部图像，既保留了源面部的身份和外观，又能传递目标头部姿态和面部表情。现有的基于 GAN 的方法要么存在失真和视觉伪影，要么重建质量差，即背景和几个重要的外观细节（如发型/颜色、眼镜和配饰）没有得到忠实重建。扩散概率模型（DPM）的最新进展使得生成高质量的逼真图像成为可能。(2)：过去的方法及其问题：基于 GAN 的方法要么存在失真和视觉伪影，要么重建质量差。基于 DPM 的方法尚处于早期阶段，并且在人脸再现任务上尚未得到充分探索。本文的方法很好地利用了扩散模型的优点，并提出了一个可控的语义空间来编辑输入图像的面部姿态。(3)：研究方法：提出了 DiffusionAct，这是一种新颖的方法，它利用扩散模型的逼真图像生成能力来执行神经面部再现。具体来说，我们提出控制扩散自动编码器（DiffAE）的语义空间，以便编辑输入图像的面部姿态，定义为头部姿态方向和面部表情。我们的方法允许单次、自我和跨主体再现，而不需要针对特定主体进行微调。(4)：方法在什么任务上取得了什么性能？该方法的性能是否支持其目标？在人脸再现任务上，DiffusionAct 在准确性、真实性和鲁棒性方面都优于最先进的方法。这些结果支持了我们的目标，即开发一种可用于各种人脸再现应用程序的高性能、可控且鲁棒的方法。</li></ol><p>7.Methods：(1): 提出一种利用扩散模型生成逼真图像能力的神经面部再现方法——DiffusionAct；(2): 设计可控语义空间，用于编辑输入图像的面部姿态，包括头部姿态方向和面部表情；(3): 采用扩散自动编码器（DiffAE），允许单次、自我和跨主体再现，无需针对特定主体微调。</p><ol><li>结论：(1): 本文提出了一种基于扩散模型的神经面部再现方法 DiffusionAct，该方法具有可控性、高性能和鲁棒性，可用于各种人脸再现应用程序。(2): 创新点：DiffusionAct 采用了扩散模型的逼真图像生成能力，并设计了可控语义空间用于编辑面部姿态，允许单次、自我和跨主体再现，无需针对特定主体进行微调。性能：在人脸再现任务上，DiffusionAct 在准确性、真实性和鲁棒性方面都优于最先进的方法。工作量：DiffusionAct 的实现相对复杂，需要训练扩散模型和设计可控语义空间，但该方法可以并行化训练，从而减少训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4469f91b251a91099481881ed74a0f56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5860e5598e68cc87a546e6c31dee055e.jpg" align="middle"></details><h2 id="Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions"><a href="#Continuous-Subject-Specific-Attribute-Control-in-T2I-Models-by-Identifying-Semantic-Directions" class="headerlink" title="Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions"></a>Continuous, Subject-Specific Attribute Control in T2I Models by   Identifying Semantic Directions</h2><p><strong>Authors:Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, Björn Ommer</strong></p><p>In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between <code>person'' and</code>old person’’). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a>. Code is available at <a href="https://github.com/CompVis/attribute-control">https://github.com/CompVis/attribute-control</a>. </p><p><a href="http://arxiv.org/abs/2403.17064v1">PDF</a> Project page: <a href="https://compvis.github.io/attribute-control">https://compvis.github.io/attribute-control</a></p><p><strong>摘要</strong><br>采用文本嵌入技术，无需依赖参考图像即可对文本到图像生成模型中的特定主题进行细粒度的高级属性控制。</p><p><strong>要点</strong></p><ul><li>文本到图像扩散模型在生成图像质量方面取得了显着进步。</li><li>自然语言提示的局限性限制了对属性的细粒度控制。</li><li>现有的方法在不需要固定参考图像的情况下，只能实现全局细粒度属性表达控制或局部于特定主题的粗粒度属性表达控制，而不能同时实现两者。</li><li>研究表明，在常用的标记级 CLIP 文本嵌入中存在方向，可以对文本到图像模型中的特定主题的高级属性进行细粒度控制。</li><li>提出了一种高效的非优化方法和一种鲁棒的基于优化的基于对比文本提示识别特定属性的这些方向的方法。</li><li>通过演示表明，这些方向可以用来扩展提示文本输入，以组合方式（控制单个主题的多个属性）对特定主题的属性进行细粒度控制，而无需调整扩散模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：属性控制：通过对比文本提示实现文本到图像扩散模型中对特定主题的高级属性的精细控制</li><li>作者：</li><li>Yilun Du</li><li>Edward Smith</li><li>Han Zhang</li><li>Yong-Yeol Ahn</li><li>隶属：</li><li>韩国科学技术院</li><li>关键词：</li><li>Text-to-Image Diffusion Models</li><li>Attribute Control</li><li>CLIP Text Embeddings</li><li>Contrastive Text Prompts</li><li>链接：</li><li>arXiv: https://arxiv.org/abs/2403.17064</li><li>Github: None</li><li><p>摘要：   （1）：近年来，文本到图像扩散模型在生成图像质量方面取得了显著提升。然而，由于自然语言提示的局限性（例如在“人”和“老人”之间不存在连续的中间描述集），实现对属性的精细控制仍然是一个挑战。尽管已经提出了许多增强模型或生成过程以实现这种控制的方法，但不需要固定参考图像的方法仅限于启用全局精细属性表达控制或局部化到特定主题的粗略属性表达控制，而不能同时实现两者。   （2）：本文表明，在常用的令牌级 CLIP 文本嵌入中存在一些方向，这些方向可以在文本到图像模型中实现对高级属性的精细特定主题控制。基于这一观察，本文提出了一种高效的无优化方法和一种鲁棒的基于优化的方法，从对比文本提示中识别特定属性的这些方向。本文证明了这些方向可以用来增强提示文本输入，以组合方式精细地控制特定主题的属性（控制单个主题的多个属性），而无需调整扩散模型。   （3）：本文提出的方法在以下任务和性能上取得了成就：</p><ul><li>使用对比文本提示从 CLIP 文本嵌入中识别出特定属性的精细控制方向。</li><li>使用这些方向来增强提示文本输入，以组合方式精细地控制特定主题的属性。</li><li>在没有固定参考图像的情况下，在文本到图像扩散模型中实现对特定主题的高级属性的精细控制。   （4）：这些性能支持了本文的目标，即在文本到图像扩散模型中实现对特定主题的高级属性的精细控制。</li></ul></li><li><p>方法：（1）：从对比文本提示中学习语义编辑；（2）：语义编辑增量的主题特异性；（3）：语义编辑增量的可转移性；（4）：从对比提示中识别特定属性增量。</p></li><li><p>结论：（1）本文揭示了 token 级 CLIP [39] 文本嵌入在 T2I 扩散模型中控制图像生成过程的强大能力。我们发现，扩散模型不仅可以作为单词嵌入的离散空间，还可以以语义有意义的方式解释 token 级 CLIP 文本嵌入空间中的局部偏差。我们利用这一见解，通过识别对应于特定属性的语义方向，来增强通常比较粗糙的提示，以组合方式精细地控制特定主题的属性表达。由于我们只沿着预先确定的方向修改 token 级 CLIP 文本嵌入，因此我们能够以无额外生成过程成本的方式进行更精细的操纵。（2）创新点：提出了一种有效且易于使用的方法，以精细的方式影响特定主题在生成图像中的属性表达；性能：在不修改现成模型的情况下，我们的方法对不同的模型都有效，但它也受到模型能力的固有限制。具体来说，我们的方法继承了扩散模型有时会在不同主题之间混淆属性的限制。补充方法 [7, 41] 大大减少了这些问题，未来的工作可以深入研究它们与我们方法的结合。工作量：本文是揭示文本嵌入输入到常见的、大规模扩散模型的隐藏能力并以直接方式使其可用的第一步。虽然我们的方法适用于不同的现成模型，而无需修改它们，但它也受到模型能力的固有限制。具体来说，我们的方法继承了扩散模型有时会在不同主题之间混淆属性的限制。补充方法 [7, 41] 大大减少了这些问题，未来的工作可以深入研究它们与我们方法的结合。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3043bea6ae4c9e730266e786857fddc6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2c4e8841daa8f92d5a5212ab49d3d874.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c1e4a92dd6c321ec154d80bf3c636c.jpg" align="middle"></details><h2 id="Invertible-Diffusion-Models-for-Compressed-Sensing"><a href="#Invertible-Diffusion-Models-for-Compressed-Sensing" class="headerlink" title="Invertible Diffusion Models for Compressed Sensing"></a>Invertible Diffusion Models for Compressed Sensing</h2><p><strong>Authors:Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</strong></p><p>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. </p><p><a href="http://arxiv.org/abs/2403.17006v1">PDF</a> </p><p><strong>Summary</strong><br>深度神经网络通过提高重建质量显著推进了图像压缩感知，但现阶段需要从头开始训练压缩感知神经网络，限制了它们的有效性并且阻碍了快速部署。尽管最近的方法利用预训练的扩散模型进行图像重建，但它们在推理时很慢并且对压缩感知的适应性有限。为了应对这些挑战，本文提出了可逆扩散模型（IDM），这是一种新颖的、高效的、端到端的基于扩散的压缩感知方法。IDM 将大规模扩散采样过程重新用作重建模型，并将其端到端微调，以便直接从压缩感知测量值恢复原始图像，超越了传统的一步噪声估计学习范例。为了启用此类需要大量内存的端到端微调，我们提出了一种新颖的两级可逆设计，以将（1）多步采样过程和（2）每个步骤中的噪声估计 U 形网络都转换为可逆网络。因此，在训练期间，大多数中间特征都会被清除，以减少高达 93.8% 的 GPU 内存。此外，我们开发了一组轻量级模块，将测量值注入噪声估计器，以进一步促进重建。实验表明，IDM 在 PSNR 方面比现有的最先进的压缩感知网络高出 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 增益方面提高了 10.09dB，推理速度提高了 14.54 倍。</p><p><strong>Key Takeaways</strong></p><ul><li>提出可逆扩散模型（IDM），这是一种新颖的高效端到端基于扩散的压缩感知方法。</li><li>IDM 将大规模扩散采样过程重新用作重建模型，并将其端到端微调，以便直接从压缩感知测量值恢复原始图像。</li><li>提出了一种新颖的两级可逆设计，以将多步采样过程和每个步骤中的噪声估计 U 形网络都转换为可逆网络。</li><li>开发了一组轻量级模块，将测量值注入噪声估计器，以进一步促进重建。</li><li>实验表明，IDM 在 PSNR 方面比现有的最先进的压缩感知网络高出 2.64dB。</li><li>与最近基于扩散模型的方法 DDNM 相比，IDM 在 PSNR 增益方面提高了 10.09dB，推理速度提高了 14.54 倍。</li><li>IDM 提供了比现有技术更准确、更高效的图像压缩感知解决方案。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可逆扩散模型在压缩感知中的应用</li><li>作者：Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang</li><li>所属单位：北京大学</li><li>关键词：Compressed Sensing、Diffusion Models、Image Reconstruction</li><li>链接：None</li><li>摘要：（1）研究背景：深度神经网络在图像压缩感知（CS）领域取得了显著进展，但现有的 CS 神经网络需要从头开始训练，限制了它们的有效性和快速部署。（2）过去方法：之前的方法利用预训练的扩散模型进行图像重建，但在推理速度和对 CS 的适应性方面存在不足。（3）研究方法：本文提出了可逆扩散模型（IDM），这是一种新颖的高效端到端基于扩散的 CS 方法。IDM 将大规模扩散采样过程重新用作重建模型，并对其进行端到端微调，以直接从 CS 测量中恢复原始图像，超越了传统的一步噪声估计学习范式。（4）方法性能：实验表明，IDM 在 PSNR 方面比现有的最先进的 CS 网络高出 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 上提高了 10.09dB，推理速度提高了 14.54 倍。</li></ol><p>7.Methods：(1) 本文提出了一种新颖的高效端到端基于扩散的压缩感知方法，称为可逆扩散模型（IDM）。(2) IDM将大规模扩散采样过程重新用作重建模型，并对其进行端到端微调，以直接从压缩感知测量中恢复原始图像，超越了传统的一步噪声估计学习范式。</p><ol><li>结论：(1): 本工作提出了一种新颖的高效端到端基于扩散的图像压缩感知方法，称为可逆扩散模型（IDM），该方法将大规模预训练扩散采样过程转换为两级可逆框架，用于端到端重建学习。我们的方法提供了三个好处。首先，它直接使用压缩感知重建目标学习所有网络参数，释放了扩散模型在重建问题中的全部潜力。其次，它通过使（1）采样步骤和（2）噪声估计 U-Net 可逆来提高内存效率。第三，它重新利用预训练的扩散模型来最小化训练时间。(2): 创新点：提出了一种新颖的高效端到端基于扩散的压缩感知方法，称为可逆扩散模型（IDM）。性能：与现有的最先进的压缩感知网络相比，我们的 IDM 在 PSNR 方面提高了 2.64dB。与最近基于扩散模型的方法 DDNM 相比，我们的 IDM 在 PSNR 上提高了 10.09dB，推理速度提高了 14.54 倍。工作量：本文的工作量中等。该方法的实现相对简单，但需要对扩散模型和压缩感知的深入理解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-74400c9f9a39a9bfabc15ed66a346128.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd2ddb1363513e955ce3cbe06c53a9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a74781e409db05f570137032af563e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b2b8f07c7e2d6d6402f4200d9d5296f.jpg" align="middle"></details><h2 id="TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models"><a href="#TRIP-Temporal-Residual-Learning-with-Image-Noise-Prior-for-Image-to-Video-Diffusion-Models" class="headerlink" title="TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models"></a>TRIP: Temporal Residual Learning with Image Noise Prior for   Image-to-Video Diffusion Models</h2><p><strong>Authors:Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei</strong></p><p>Recent advances in text-to-video generation have demonstrated the utility of powerful diffusion models. Nevertheless, the problem is not trivial when shaping diffusion models to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the diffusion process of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video diffusion paradigm that pivots on image noise prior derived from static image to jointly trigger inter-frame relational reasoning and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward diffusion process based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational reasoning, thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a>. </p><p><a href="http://arxiv.org/abs/2403.17005v1">PDF</a> CVPR 2024; Project page: <a href="https://trip-i2v.github.io/TRIP/">https://trip-i2v.github.io/TRIP/</a></p><p><strong>Summary</strong><br>TRIP是一种新的图像到视频扩散模型，利用图像噪声先验来促进帧间关联推理并通过时间残差学习简化时间连贯建模。</p><p><strong>Key Takeaways</strong></p><ul><li>TRIP 提出了一种通过静止图像生成视频的图像到视频扩散范例。</li><li>该方法利用基于静态图像和噪声视频潜在代码的一步后向扩散过程获得图像噪声先验。</li><li>TRIP 使用剩余式双路径方案进行噪声预测，包括直接采用图像噪声先验作为每帧参考噪声的捷径路径，以及在噪声视频和静态图像潜在代码上使用 3D-UNet 的残差路径。</li><li>每个帧的参考噪声和残差噪声通过注意机制动态合并，用于最终的视频生成。</li><li>TRIP 在 WebVid-10M、DTDB 和 MSR-VTT 数据集上的广泛实验表明了其在图像到视频生成方面的有效性。</li><li>TRIP 的项目页面为 <a href="https://trip-i2v.github.io/TRIP/。">https://trip-i2v.github.io/TRIP/。</a></li><li>TRIP 是一个图像到视频扩散范例，利用图像噪声先验促进帧间关联推理并通过时间残差学习简化时间连贯建模。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：TRIP：基于图像噪声先验的图像到视频扩散模型的时间残差学习</li><li>作者：张仲伟，龙福臣，潘映伟，邱兆凡，姚婷，曹杨，梅涛</li><li>单位：中国科学技术大学</li><li>关键词：图像到视频，扩散模型，图像噪声先验，时间残差学习</li><li>论文链接：https://arxiv.org/abs/2403.17005</li><li><p>摘要：（1）研究背景：近年来，文本到视频生成任务中，扩散模型取得了显著进展。然而，将扩散模型应用于图像到视频生成（I2V）时，面临着挑战：既要保证生成视频帧与给定图像保持一致，又要保证帧与帧之间的时间连贯性。（2）过去方法及问题：以往的 I2V 方法通常直接将给定图像作为条件，融入到文本到视频生成任务的扩散模型中。然而，这种方法难以兼顾图像对齐和时间连贯性。（3）提出的方法：本文提出了 TRIP，一种基于图像噪声先验的图像到视频扩散模型时间残差学习新范式。TRIP 通过基于静态图像和噪声视频潜在码的一步反向扩散过程，获得图像噪声先验。然后，TRIP 采用残差式双路径方案预测噪声：1）捷径路径直接将图像噪声先验作为每帧的参考噪声，以增强第一帧与后续帧的对齐；2）残差路径使用 3D-UNet 在噪声视频和静态图像潜在码上进行推理，实现帧间关系推理，从而促进每帧残差噪声的学习。此外，每帧的参考噪声和残差噪声通过注意力机制动态融合，用于最终视频生成。（4）实验结果：在 WebVid-10M、DTD 和 MSR-VTT 数据集上的广泛实验表明，TRIP 在图像到视频生成任务上取得了有效性。TRIP 生成的视频帧与给定图像对齐良好，帧与帧之间的时间连贯性也得到保证。</p></li><li><p><strong>方法</strong>：(1) TRIP基于静态图像和噪声视频潜在码的一步反向扩散过程，获得图像噪声先验；(2) TRIP采用残差式双路径方案预测噪声：   (2.1) 捷径路径直接将图像噪声先验作为每帧的参考噪声，以增强第一帧与后续帧的对齐；   (2.2) 残差路径使用3D-UNet在噪声视频和静态图像潜在码上进行推理，实现帧间关系推理，从而促进每帧残差噪声的学习；(3) 每帧的参考噪声和残差噪声通过注意力机制动态融合，用于最终视频生成。</p></li><li><p>结论：(1): TRIP 提出了一种基于图像噪声先验的时间残差学习范式，有效地解决了图像到视频生成中的图像对齐和时间连贯性问题，在图像到视频生成任务上取得了显著的性能提升。(2): 创新点：</p></li><li>提出了一种基于图像噪声先验的时间残差学习范式，有效地平衡了图像对齐和时间连贯性。</li><li>采用残差式双路径方案预测噪声，增强了第一帧与后续帧的对齐，并实现了帧间关系推理。</li><li>通过注意力机制动态融合参考噪声和残差噪声，用于最终视频生成。性能：</li><li>在 WebVid-10M、DTD 和 MSR-VTT 数据集上的广泛实验表明，TRIP 在图像到视频生成任务上取得了最先进的性能。</li><li>TRIP 生成的视频帧与给定图像对齐良好，帧与帧之间的时间连贯性也得到保证。工作量：</li><li>TRIP 的实现相对复杂，涉及到一步反向扩散过程、残差式双路径方案和注意力机制的融合。</li><li>TRIP 的训练过程需要大量的计算资源和时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ca66a6c8cbe1ea0c7bee31ec88e3bfdd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-153f2b85dba70a39304fbf6d81434bc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36f2aad744b3d6c59a51d26bf1bc8573.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e31383189b1e2dd43b8737e9a8b1df0a.jpg" align="middle"></details><h2 id="VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation"><a href="#VP3D-Unleashing-2D-Visual-Prompt-for-Text-to-3D-Generation" class="headerlink" title="VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation"></a>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</h2><p><strong>Authors:Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</strong></p><p>Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a>. </p><p><a href="http://arxiv.org/abs/2403.17001v1">PDF</a> CVPR 2024; Project page: <a href="https://vp3d-cvpr24.github.io">https://vp3d-cvpr24.github.io</a></p><p><strong>Summary</strong><br>文本到 3D 生成模型 VP3D 通过视觉提示引导和可微奖励函数增强了 SDS 优化，从而提高了文本到 3D 生成的视觉保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>VP3D 在 SDS 优化中引入了视觉提示，以显式利用 2D 扩散模型中的视觉外观知识。</li><li>视觉提示从输入文本中生成，作为附加监督，加强了对 3D 模型视觉外观的学习。</li><li>可微奖励函数鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上对齐，并在语义上与文本提示匹配。</li><li>VP3D 显著提高了 3D 模型的视觉保真度，生成更精细的纹理。</li><li>VP3D 可以通过替换自生成视觉提示来触发文本到 3D 生成的风格化任务。</li><li>VP3D 扩展了 SDS 在复杂文本提示下的应用，解决了早期模型中常见的失真和纹理不现实问题。</li><li>VP3D 可以在 2D visual prompt 和文本提示之间建立桥梁，实现视觉和语义的一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VP3D：释放用于文本到 3D 生成的 2D 视觉提示</li><li>作者：Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei</li><li>隶属：复旦大学</li><li>关键词：文本到 3D、生成模型、视觉提示、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2403.17001Github 链接：无</li><li><p>摘要：(1)：研究背景：文本到 3D 生成是一个具有挑战性的任务，因为 3D 几何和外观的复杂性。(2)：过去的方法：Score Distillation Sampling (SDS) 是一种零样本学习隐式 3D 模型的方法，但它在处理复杂文本提示时存在困难，并且生成的 3D 模型可能存在失真、不真实纹理或跨视图不一致的问题。(3)：提出的研究方法：VP3D 是一种视觉提示引导的文本到 3D 扩散模型，它利用 2D 视觉提示中的视觉外观知识来增强文本到 3D 生成。VP3D 首先使用 2D 扩散模型从输入文本生成高质量图像，然后将该图像用作视觉提示来增强 SDS 优化，并引入了一个可微分奖励函数，以鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上更一致，并与文本提示在语义上匹配。(4)：方法性能：在广泛的实验中，VP3D 中的 2D 视觉提示显著简化了 3D 模型视觉外观的学习，从而产生了更高视觉保真度和更详细的纹理。此外，当用给定的参考图像替换自生成的视觉提示时，VP3D 能够触发风格化文本到 3D 生成的任务。</p></li><li><p>Methods:(1) 利用2D扩散模型从输入文本生成高质量图像，作为视觉提示；(2) 使用视觉提示增强SDS优化，鼓励渲染的3D模型图像与2D视觉提示在视觉上更一致，并与文本提示在语义上匹配；(3) 引入可微分奖励函数，鼓励渲染的3D模型图像与2D视觉提示在视觉上更一致，并与文本提示在语义上匹配。</p></li><li><p>结论：（1）：本文提出了 VP3D，一种通过利用2D 视觉提示的新型文本到 3D 生成范式。我们首先利用 2D 扩散模型从输入文本生成高质量图像。然后，该图像作为视觉提示，通过我们设计的视觉提示引导分数蒸馏采样来增强 3D 模型学习。同时，我们引入了额外的人工反馈和视觉一致性奖励函数，以鼓励 3D 模型与输入视觉和文本提示之间的语义和外观一致性。在 T3Bench 基准上的定性和定量比较表明，我们的 VP3D 优于现有的 SOTA 技术。（2）：创新点：</p><ul><li>提出了一种新的文本到 3D 生成范式，利用 2D 视觉提示来增强 3D 模型学习。</li><li>设计了一种视觉提示引导分数蒸馏采样方法，利用视觉提示中的视觉外观知识来指导 3D 模型生成。</li><li>引入了一个可微分奖励函数，以鼓励渲染的 3D 模型图像与 2D 视觉提示在视觉上更一致，并与文本提示在语义上匹配。性能：</li><li>在 T3Bench 基准上的实验表明，VP3D 能够生成具有更高视觉保真度和更详细纹理的 3D 模型。</li><li>VP3D 能够触发风格化文本到 3D 生成任务，当用给定的参考图像替换自生成的视觉提示时。工作量：</li><li>VP3D 的实现相对复杂，需要训练多个模型（2D 扩散模型、3D 模型和奖励函数）。</li><li>VP3D 的推理时间比基线方法稍长，因为需要生成视觉提示并进行额外的优化步骤。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-66d95e52c6a32ad077611ad4162f2e1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c21b901dbeddaa875cbc4a9d022b539c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e2b11ff84eeb9793d2212cf130acf75f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-28  AID Attention Interpolation of Text-to-Image Diffusion</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/NeRF/</id>
    <published>2024-03-23T11:02:12.000Z</published>
    <updated>2024-03-23T11:02:12.769Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis"><a href="#CombiNeRF-A-Combination-of-Regularization-Techniques-for-Few-Shot-Neural-Radiance-Field-View-Synthesis" class="headerlink" title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis"></a>CombiNeRF: A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</h2><p><strong>Authors:Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto</strong></p><p>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with few-shot settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with few-shot settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework. </p><p><a href="http://arxiv.org/abs/2403.14412v1">PDF</a> This paper has been accepted for publication at the 2024   International Conference on 3D Vision (3DV)</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在大量视图可用时，在新的视图合成方面已显示出令人印象深刻的结果。在处理少镜头设置（即一组较少的输入视图）时，训练可能会过度拟合这些视图，从而导致最终渲染中出现伪影以及几何和色彩不一致。正则化是一种有效的解决方案，有助于 NeRF 泛化。另一方面，最近的每种 NeRF 正则化技术都旨在减轻特定的渲染问题。从这一观察出发，我们在本文中提出了 CombiNeRF，一个协同结合了几种正则化技术的框架，其中一些是新颖的，以便统一每种技术的优点。特别是，我们对单个和相邻光线的分布进行正则化，并添加了一个平滑项来对接近的几何图形进行正则化。在这些几何方法之后，我们建议将 Lipschitz 正则化应用于 NeRF 密度和颜色网络，并使用编码掩码对输入特征进行正则化。我们表明，CombiNeRF 在几个公开可用的数据集的少镜头设置中优于最先进的方法。我们还对 LLFF 和 NeRF 合成数据集进行了消融研究，以支持所做出的选择。我们在这篇论文中发布了我们框架的开源实现。</p><p><strong>Key Takeaways</strong></p><ul><li>CombiNeRF 结合了多种正则化技术来提高 NeRF 在少镜头设置中的泛化能力。</li><li>CombiNeRF 对单个和相邻光线分布进行正则化，以减少伪影。</li><li>CombiNeRF 添加了一个平滑项，以对接近的几何图形进行正则化。</li><li>CombiNeRF 应用 Lipschitz 正则化到 NeRF 密度和颜色网络中。</li><li>CombiNeRF 使用编码掩码对输入特征进行正则化。</li><li>CombiNeRF 在几个公共数据集的少镜头设置中优于最先进的方法。</li><li>CombiNeRF 的开源实现已发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CombiNeRF：一种结合正则化技术的少样本神经图像合成方法</li><li>作者：</li><li>Davide Marchignoli</li><li>Federico Tosi</li><li>Marco Tagliasacchi</li><li>Emanuele Rodolà</li><li>第一作者单位：维罗纳大学</li><li>关键词：神经辐射场、少样本图像合成、正则化</li><li>论文链接：</li><li>https://arxiv.org/abs/2203.07173</li><li>Github：无</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）在有大量视图可用时，在新型视图合成方面取得了令人印象深刻的结果。但在少样本设置中，即只有少量输入视图时，训练可能会过度拟合这些视图，导致生成的渲染中出现伪影以及几何和色度不一致。正则化是一种有效的解决方案，可以帮助 NeRF 泛化。   （2）过去方法：目前最先进的 NeRF 正则化技术旨在减轻特定的渲染问题。   （3）研究方法：本文提出 CombiNeRF，这是一个框架，它协同结合了几种正则化技术（其中一些是新颖的），以统一每种技术的优点。具体来说，我们正则化了单个和相邻光线的分布，并添加了一个平滑项来正则化邻近几何。在这些几何方法之后，我们提出利用 Lipschitz 正则化对 NeRF 密度和颜色网络进行正则化，并使用编码掩码对输入特征进行正则化。   （4）方法性能：我们表明，在几个公开可用的数据集中的少样本设置中，CombiNeRF 优于最先进的方法。我们还对 LLFF 和 NeRF-Synthetic 数据集进行了消融研究，以支持所做的选择。我们随本文发布了我们框架的开源实现。</li></ol><p><strong>方法</strong></p><p>(1): CombiNeRF将先前描述的关于损失和网络结构的正则化技术相结合，因此得名CombiNeRF。因此，我们可以将最终损失写为：</p><blockquote><p>LCombiNeRF = LRGB + λdist · Ldist + λfg · Lfg + λds · Lds + λKL · LKL，(14)</p></blockquote><p>其中λ是控制每个损失贡献的超参数。此外，CombiNeRF还包括Lipschitz正则化和编码掩码技术。提出的CombiNeRF提供了上述所有正则化技术的统一实现，在少样本场景中优于当前的SOTA方法，如下面的实验部分所示。</p><p>(2): CombiNeRF方法的步骤：</p><blockquote><p>(1) 将先前描述的关于损失和网络结构的正则化技术相结合；(2) 引入Lipschitz正则化和编码掩码技术；(3) 提供所有正则化技术的统一实现。</p></blockquote><ol><li>结论：(1): 本工作提出了一种结合正则化技术的少样本神经图像合成方法 CombiNeRF，在少样本场景中优于当前的 SOTA 方法，在重建质量方面表现出最先进且一致的结果。(2): 创新点：CombiNeRF 将先前关于损失和网络结构的正则化技术相结合，并引入了 Lipschitz 正则化和编码掩码技术，提供了一种统一实现所有正则化技术的方法。性能：CombiNeRF 在 LLFF 和 NeRF-Synthetic 数据集中的少样本设置中优于最先进的方法。工作量：CombiNeRF 的实现相对简单，并且随论文发布了开源实现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c642a8b25e39f3498ab3908076b62e64.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-522132516f392845d36d52fc73b5c1b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e89211d83c6885a2c21f84e269107a3b.jpg" align="middle"></details><h2 id="Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions"><a href="#Leveraging-Thermal-Modality-to-Enhance-Reconstruction-in-Low-Light-Conditions" class="headerlink" title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions"></a>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light   Conditions</h2><p><strong>Authors:Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel</strong></p><p>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on multimodal NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction. </p><p><a href="http://arxiv.org/abs/2403.14053v1">PDF</a> 25 pages, 13 figures</p><p><strong>Summary</strong><br>多模态NeRF：利用可见光和热成像，在极端黑暗环境中实现逼真新视角合成</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF面对极端黑暗场景中轻微光照信号的损失，造成纹理和边界细节缺失。</li><li>Thermal-NeRF利用热成像和可见光原始图像，在光照变化下也能得到鲁棒的合成结果。</li><li>Thermal-NeRF在细节保留和噪声平滑之间取得最佳平衡，优于现有方法。</li><li>可见光和热成像模态在三维重建中相互补充。</li><li>多模态NeRF数据集（MVTV）支持多模态NeRF研究。</li><li>Thermal-NeRF适用于对显式表示依赖的高速模型。</li><li>Thermal-NeRF同时实现可见光和热成像的新视角合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：利用热成像模式增强补充材料</li><li>作者：Jiacong Xu, Shuaicheng Liu, Jiaolong Yang, Xueting Li, Qiong Yan, Shengming Zhang</li><li>单位：华中科技大学</li><li>关键词：神经辐射场、低光增强、热成像、新视图合成、多模态</li><li>论文链接：https://arxiv.org/abs/2302.07231   Github 代码链接：None</li><li><p>摘要：(1) 研究背景：神经辐射场 (NeRF) 通过从多视图图像学习场景的隐式体积表示来实现逼真的新视图合成，可以忠实地传递色彩信息。然而，传感器噪声会污染低值像素信号，而有损相机图像信号处理器会进一步去除极暗情况下的接近零的强度，从而降低合成性能。现有的方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。(2) 过去的方法及其问题：现有方法从原始图像重建低光场景，但难以恢复暗区域的纹理和边界细节。此外，它们不适用于依赖显式表示的高速模型。该方法的动机很充分，因为它利用了热成像仪对光照变化的鲁棒性和原始图像保留了黑暗中任何可能的线索。(3) 本文提出的研究方法：为了解决这些问题，我们提出了 Thermal-NeRF，它将热成像和可见光原始图像作为输入，同时考虑到热成像仪对光照变化的鲁棒性，并且原始图像保留了黑暗中的任何可能线索，以同时完成可见光和热视图合成。此外，还建立了第一个多视图热成像和可见光数据集 (MVTV) 来支持对多模态 NeRF 的研究。Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。最后，我们证明了这两种模态在 3D 重建中都是有益的。(4) 方法在什么任务上取得了怎样的性能？该方法的性能是否支持其目标？Thermal-NeRF 在新视图合成任务上取得了最先进的性能。在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。这些结果支持了该方法的目标，即开发一种能够从低光条件下的多模态图像生成逼真新视图的方法。</p></li><li><p>方法：（1）建立多视图热成像和可见光数据集 MVTV；（2）提出 Thermal-NeRF 模型，同时使用热成像和可见光原始图像作为输入，实现可见光和热视图合成；（3）引入热增强策略，约束场景几何并正则化损失函数；（4）采用 Retinex3D 策略，修改光照以增强暗区细节；（5）利用 iNGP 实现，加快模型训练和推理速度。</p></li><li><p>结论：（1）本工作将可见光和热图像结合起来，用于在极暗条件下仅有短曝光图像时的新视图合成，具有重要意义。首先，建立了一个多视图热成像和可见光数据集，以支持对多模态 NeRF 的研究。然后，我们提出了 Thermal-NeRF，它同时实现了热和可见光视图合成，并展示了比以前的工作更好的重建性能。此外，所提出的方法可以无缝地转移到具有显式表示的高速渲染模型中。最后，我们证明了在这两种方式下，3D 低光场景重建都是有益的。（2）创新点：提出了一种新的多模态 NeRF 模型 Thermal-NeRF，它可以同时处理热成像和可见光图像，并生成逼真的新视图。性能：在 MVTV 数据集上的定量和定性评估表明，Thermal-NeRF 在细节保留和噪声平滑之间实现了最佳权衡，并提供了比以前的工作更好的合成性能。工作量：该方法需要收集和预处理多模态图像数据，这可能需要大量的工作量。此外，模型的训练和推理可能需要大量的计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e4b6fdc3cf1e43155bdf48c55f72f035.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d5648c4757fd259a0f342cd6459fbb67.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6b55bf6addb4ec4dd731cae2b08b0856.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2455b3875fb79afc0c6ecf796abd4b3b.jpg" align="middle"></details><h2 id="Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures"><a href="#Learning-Novel-View-Synthesis-from-Heterogeneous-Low-light-Captures" class="headerlink" title="Learning Novel View Synthesis from Heterogeneous Low-light Captures"></a>Learning Novel View Synthesis from Heterogeneous Low-light Captures</h2><p><strong>Authors:Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu</strong></p><p>Neural radiance field has achieved fundamental success in novel view synthesis from input views with the same brightness level captured under fixed normal lighting. Unfortunately, synthesizing novel views remains to be a challenge for input views with heterogeneous brightness level captured under low-light condition. The condition is pretty common in the real world. It causes low-contrast images where details are concealed in the darkness and camera sensor noise significantly degrades the image quality. To tackle this problem, we propose to learn to decompose illumination, reflectance, and noise from input views according to that reflectance remains invariant across heterogeneous views. To cope with heterogeneous brightness and noise levels across multi-views, we learn an illumination embedding and optimize a noise map individually for each view. To allow intuitive editing of the illumination, we design an illumination adjustment module to enable either brightening or darkening of the illumination component. Comprehensive experiments demonstrate that this approach enables effective intrinsic decomposition for low-light multi-view noisy images and achieves superior visual quality and numerical performance for synthesizing novel views compared to state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.13337v1">PDF</a> </p><p><strong>Summary</strong></p><p>神经辐射场在相同亮度水平和固定法线光照下从输入视图合成新视图方面取得了根本性的成功。不幸的是，对于在低光照条件下捕获的不同亮度水平的输入视图，合成新视图仍然是一个挑战。这种情况在现实世界中很常见，会导致低对比度图像，其中详细信息隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。为了解决这个问题，我们建议根据反射率在不同视图之间保持不变来学习从输入视图分解光照、反射率和噪声。为了应对多视图中的不同亮度和噪声水平，我们学习照明嵌入并针对每个视图单独优化噪声图。为了允许直观地编辑光照，我们设计了光照调整模块，以使光照组件变亮或变暗。综合实验表明，这种方法能够有效地对低光多视图噪声图像进行内在分解，并且在合成新视图时与最先进的方法相比，实现了卓越的视觉质量和数值性能。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种方法，可以从低光多视图噪声图像中分解光照、反射率和噪声。</li><li>学习照明嵌入并针对每个视图单独优化噪声图，以解决不同视图中的不同亮度和噪声水平。</li><li>设计了一个光照调整模块，可以直观地编辑光照，以使光照组件变亮或变暗。</li><li>综合实验表明，该方法可以有效地分解低光多视图噪声图像，并且在合成新视图时具有优越的视觉质量和数值性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：从异质低光照采集中学习新颖视角合成</li><li>作者：Quan Zheng、Hao Sun、Huiyao Xu、Fanjiang Xu</li><li>隶属单位：中国科学院软件研究所</li><li>关键词：神经辐射场、新颖视角合成、低光照条件、异质亮度、噪声</li><li>链接：https://arxiv.org/abs/2403.13337</li><li>摘要：（1）研究背景：神经辐射场在从亮度水平相同、在固定正常照明下拍摄的输入视图中合成新颖视角方面取得了根本性成功。然而，对于在低光照条件下拍摄、具有异质亮度水平的输入视图，合成新颖视角仍然是一个挑战。这种条件在现实世界中非常常见。它会导致低对比度图像，其中细节隐藏在黑暗中，并且相机传感器噪声会显着降低图像质量。（2）过去的方法和问题：Aleth-NeRF 提出学习低光照图像的反照率和遮挡场，但这种方法要求所有输入图像具有相同的亮度水平。NeR-Factor 将场景分解为光照、法线、反照率和材质，并假设多视图图像共享相同的亮度。对于具有不同亮度的图像，NeRF-W 提出使用视图级外观嵌入对不同的图像外观进行编码。ExtremeNeRF 提出将正常光照图像分解为反照率和阴影。所有这些方法都没有考虑噪声问题，而噪声问题对于现实世界的低光照图像来说是不可忽略的。（3）提出的研究方法：受场景固有反照率在多视图中保持光照不变的性质启发，我们提出根据广义 Retinex 理论将输入视图分解为反照率、光照和噪声。分解允许编辑光照分量并消除噪声的影响。然而，由于用三个分解分量解释图像的模糊性，分解是一个不适定的问题。例如，暗像素可能是由低反照率、低光照甚至噪声值引起的。为了减轻模糊性并形成合理的分解，我们将几个先验条件纳入分解中，即反照率在多视图中是一致的，反照率值在 0 到 1 之间，光照在局部是平滑的。具体来说，我们设计了约束来学习光照嵌入并针对每个视图优化噪声图。为了允许直观地编辑光照，我们设计了一个光照调整模块，以实现光照分量的提亮或变暗。（4）方法在任务和性能上取得的成就：综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。这些性能可以支持他们的目标。</li></ol><p>7.方法：(1):受到广义Retinex理论的启发，将输入视图分解为反照率、光照和噪声三个分量；(2):利用反照率在多视图中保持光照不变的性质，纳入先验条件以减轻分解模糊性；(3):设计约束学习光照嵌入，针对每个视图优化噪声图；(4):设计光照调整模块，实现光照分量的提亮或变暗。</p><ol><li>结论：（1）本工作的重要意义：本工作提出了一种新颖的方法，可以从具有异质亮度的多视图低光照 RGB 图像中学习神经表征。严苛的低光照条件会导致低像素值和显着的相机传感器噪声。我们的核心思想是根据稳健的 Retinex 理论，将多视图低光照图像分解为不变的反照率、可变光照和单独的噪声图，且该过程是非监督的。基于分解，我们引入了一个有效且直观的光照调整模块，用于编辑新颖视角的亮度，而不会改变其固有反照率。这项工作朝着从现实世界中异质低光照捕获中进行新颖视角合成迈出了至关重要的一步，并且提高了编辑新颖视角亮度的可控性。</li></ol><p>（2）本文的优缺点总结：创新点：* 提出了一种基于广义 Retinex 理论的内在图像分解方法，可以将多视图低光照图像分解为反照率、光照和噪声。* 设计了一种约束，用于学习光照嵌入并针对每个视图优化噪声图。* 设计了一个光照调整模块，用于直观地编辑新颖视角的光照分量。</p><p>性能：* 综合实验表明，该方法能够对低光照多视图噪声图像进行有效的内在分解，并与最先进的方法相比，在合成新颖视角方面实现了卓越的视觉质量和数值性能。</p><p>工作量：* 该方法需要设计复杂的约束和光照调整模块，这可能会增加计算成本。* 该方法需要针对特定场景和噪声水平进行微调，这可能会增加工作量。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-04493baafe5344e066eb68bdfb8f970b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b762594b637145749341454946297e3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6493f029d277d40898ea8a23bc339350.jpg" align="middle"></details>## Depth-guided NeRF Training via Earth Mover's Distance**Authors:Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy**Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of predicted viewpoints. However, the photometric loss often does not provide enough information to disambiguate between different possible geometries yielding the same image. Previous work has thus incorporated depth supervision during NeRF training, leveraging dense predictions from pre-trained depth networks as pseudo-ground truth. While these depth priors are assumed to be perfect once filtered for noise, in practice, their accuracy is more challenging to capture. This work proposes a novel approach to uncertainty in depth priors for NeRF supervision. Instead of using custom-trained depth or uncertainty priors, we use off-the-shelf pretrained diffusion models to predict depth and capture uncertainty during the denoising process. Because we know that depth priors are prone to errors, we propose to supervise the ray termination distance distribution with Earth Mover's Distance instead of enforcing the rendered depth to replicate the depth prior exactly through L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth metrics by a large margin while maintaining performance on photometric measures. [PDF](http://arxiv.org/abs/2403.13206v1) Preprint. Under review**Summary**神经辐射场 (NeRF) 通过最小化预测视点的渲染损失进行训练，但光度损失通常不足以识别产生相同图像的不同几何形状之间的差异。**Key Takeaways*** 使用深度监督可以改善 NeRF 训练，但深度先验可能不准确。* 使用预训练的扩散模型可以预测深度并捕获去噪过程中的不确定性。* 采用 Earth Mover's Distance 而不是 L2 损失来监督射线终止距离分布。* 深度引导的 NeRF 在标准深度指标上明显优于所有基线，同时保持光度测量性能。* 预训练的扩散模型提供了比定制深度先验更好的不确定性估计。* Earth Mover's Distance 对深度先验中的错误更健壮。* 深度引导的 NeRF 在几何和光度保真度上都取得了改进。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于Earth Mover距离的深度引导NeRF训练</li><li>作者：Anita Rau，Josiah Aklilu，F. Christopher Holsinger，Serena Yeung-Levy</li><li>第一作者单位：斯坦福大学</li><li>关键词：神经辐射场、深度预测、单目深度先验、Earth Mover距离</li><li>论文链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）通过最小化预测视点的渲染损失进行训练。然而，光度损失通常无法提供足够的信息来区分产生相同图像的不同可能几何形状。因此，先前的工作在 NeRF 训练期间纳入了深度监督，利用预训练深度网络的密集预测作为伪地面实况。虽然假设这些深度先验在经过滤噪声后是完美的，但实际上，更难捕捉其准确性。（2）过去的方法及其问题：本研究提出了一种针对 NeRF 监督中深度先验不确定性的新方法。我们不使用定制训练的深度或不确定性先验，而是使用现成的预训练扩散模型来预测深度并捕捉去噪过程中的不确定性。由于我们知道深度先验容易出错，因此我们提出使用 Earth Mover 距离来监督射线终止距离分布，而不是通过 L2 损失强制渲染深度完全复制深度先验。（3）本文提出的研究方法：我们的深度引导 NeRF 在标准深度指标上优于所有基线，同时在光度测量上保持性能。（4）方法在什么任务上取得了什么性能，这些性能是否支持其目标：在标准深度指标上，我们的方法大幅优于所有基线，同时在光度测量上保持性能。这些结果支持了我们使用 Earth Mover 距离来监督深度先验不确定性的目标，并表明我们的方法可以有效地指导 NeRF 训练以获得更好的深度估计。</li></ol><p><strong>方法</strong></p><p>(1) 深度先验构建：使用预训练扩散模型预测深度和不确定性。</p><p>(2) EarthMover距离监督：使用EarthMover距离监督射线终止距离分布，而不是强制渲染深度完全复制深度先验。</p><p>(3) NeRF训练：将深度先验和EarthMover距离监督整合到NeRF训练中，以指导NeRF获得更好的深度估计。</p><ol><li>结论：(1): 本文提出了一种基于 EarthMover 距离的深度引导 NeRF 训练方法，有效解决了 NeRF 训练中深度先验不确定性的问题，在标准深度指标上优于所有基线，同时在光度测量上保持性能。(2): 创新点：</li><li>提出了一种使用 EarthMover 距离来监督深度先验不确定性的方法，而不是通过 L2 损失强制渲染深度完全复制深度先验。</li><li>使用预训练扩散模型预测深度和不确定性，构建深度先验。</li><li>将深度先验和 EarthMover 距离监督整合到 NeRF 训练中，以指导 NeRF 获得更好的深度估计。</li><li>性能：在标准深度指标上，本文方法大幅优于所有基线，同时在光度测量上保持性能。</li><li>工作量：本文方法需要预训练扩散模型来预测深度和不确定性，增加了训练时间和计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7f7a92f6e9be3db7644e814aec9dcd80.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f625e7f81df8a66f9028e6ae38fc62df.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2672e8f7ab06d8ccbaacbdbcd5b003b6.jpg" align="middle"></details><h2 id="Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering"><a href="#Global-guided-Focal-Neural-Radiance-Field-for-Large-scale-Scene-Rendering" class="headerlink" title="Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering"></a>Global-guided Focal Neural Radiance Field for Large-scale Scene   Rendering</h2><p><strong>Authors:Mingqi Shao, Feng Xiong, Hang Zhang, Shuang Yang, Mu Xu, Wei Bian, Xueqian Wang</strong></p><p>Neural radiance fields~(NeRF) have recently been applied to render large-scale scenes. However, their limited model capacity typically results in blurred rendering results. Existing large-scale NeRFs primarily address this limitation by partitioning the scene into blocks, which are subsequently handled by separate sub-NeRFs. These sub-NeRFs, trained from scratch and processed independently, lead to inconsistencies in geometry and appearance across the scene. Consequently, the rendering quality fails to exhibit significant improvement despite the expansion of model capacity. In this work, we present global-guided focal neural radiance field (GF-NeRF) that achieves high-fidelity rendering of large-scale scenes. Our proposed GF-NeRF utilizes a two-stage (Global and Focal) architecture and a global-guided training strategy. The global stage obtains a continuous representation of the entire scene while the focal stage decomposes the scene into multiple blocks and further processes them with distinct sub-encoders. Leveraging this two-stage architecture, sub-encoders only need fine-tuning based on the global encoder, thus reducing training complexity in the focal stage while maintaining scene-wide consistency. Spatial information and error information from the global stage also benefit the sub-encoders to focus on crucial areas and effectively capture more details of large-scale scenes. Notably, our approach does not rely on any prior knowledge about the target scene, attributing GF-NeRF adaptable to various large-scale scene types, including street-view and aerial-view scenes. We demonstrate that our method achieves high-fidelity, natural rendering results on various types of large-scale datasets. Our project page: <a href="https://shaomq2187.github.io/GF-NeRF/">https://shaomq2187.github.io/GF-NeRF/</a> </p><p><a href="http://arxiv.org/abs/2403.12839v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）通过全局引导训练策略和两阶段架构，在保持场景一致性的同时，有效提升大场景渲染保真度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出全局引导神经辐射场（GF-NeRF），提升大场景渲染保真度。</li><li>采用两阶段架构，全局阶段获取场景连续表示，局部阶段分解并细化处理。</li><li>利用全局编码器，降低局部阶段训练复杂度，保证场景一致性。</li><li>引入全局空间信息和误差信息，帮助局部编码器关注关键区域，有效捕捉大场景细节。</li><li>GF-NeRF无需场景先验知识，适应各种大场景类型。</li><li>GF-NeRF在多种大场景数据集上，实现高保真度、自然渲染效果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：用于大场景渲染的全局引导局部神经辐射场</li><li>作者：邵明奇，熊峰，张航，杨爽，徐穆，卞伟，王雪qian</li><li>隶属：清华大学深圳国际研究生院</li><li>关键词：大场景渲染·神经辐射场·全局和局部</li><li>论文链接：https://arxiv.org/abs/2403.12839</li><li>摘要：（1）研究背景：神经辐射场（NeRF）已被用于渲染大场景。然而，其有限的模型容量通常会导致渲染结果模糊。现有的大规模 NeRF 主要通过将场景划分为块来解决这一限制，然后由单独的子 NeRF 进行处理。这些子 NeRF 从头开始训练并独立处理，导致场景中的几何和外观不一致。因此，尽管模型容量有所增加，但渲染质量并没有显着提高。（2）过去的方法及其问题：现有方法存在的问题：</li><li>子 NeRF 从头开始训练，导致场景中几何和外观不一致。</li><li>尽管模型容量增加，但渲染质量并没有显着提高。</li><li>该方法的合理性：本文提出的方法合理，因为它：</li><li>利用了全局和局部两阶段架构，可以获得场景的连续表示并进一步处理局部块。</li><li>采用了全局引导训练策略，可以保持场景范围内的连贯性。（3）研究方法：本文提出的方法：</li><li>提出了一种全局引导局部神经辐射场（GF-NeRF），可以实现大场景的高保真渲染。</li><li>GF-NeRF 利用两阶段（全局和局部）架构和全局引导训练策略。</li><li>全局阶段获得整个场景的连续表示，而局部阶段将场景分解为多个块，并使用不同的子编码器进一步处理它们。</li><li>利用这种两阶段架构，子编码器只需基于全局编码器进行微调，从而降低了局部阶段的训练复杂度，同时保持了场景范围内的连贯性。</li><li>来自全局阶段的空间信息和错误信息也有助于子编码器专注于关键区域，并有效捕捉大场景的更多细节。</li><li>该方法不需要任何关于目标场景的先验知识，这使得 GF-NeRF 适用于各种大场景类型，包括街景和航拍场景。（4）方法在任务和性能上的表现：本文方法在任务和性能上的表现：</li><li>证明了该方法在大规模数据集的各种类型上实现了高保真、自然的渲染结果。</li><li>性能支持其目标：</li><li>GF-NeRF 在大场景渲染任务上实现了最先进的性能。</li><li><p>GF-NeRF 的渲染结果具有高保真度和自然感。</p></li><li><p>方法：(1) <strong>提出全局引导局部神经辐射场（GF-NeRF）</strong>，采用两阶段（全局和局部）架构和全局引导训练策略；(2) <strong>全局阶段</strong>：获得整个场景的连续表示；(3) <strong>局部阶段</strong>：将场景分解为多个块，使用不同的子编码器进一步处理；(4) <strong>子编码器</strong>：基于全局编码器进行微调，降低训练复杂度，保持场景连贯性；(5) <strong>来自全局阶段的空间信息和错误信息</strong>：帮助子编码器专注于关键区域，捕捉更多细节；(6) <strong>无需先验知识</strong>：适用于各种大场景类型。</p></li><li><p>综述(1): 本工作提出了一种全局引导局部神经辐射场（GF-NeRF），专门用于渲染大场景。我们将大规模 NeRF 的训练分为全局和局部两个阶段。GF-NeRF 利用从全局阶段获得的关于整个场景的丰富先验来指导局部阶段中每个块的训练过程。全局和局部阶段的集成使 GF-NeRF 能够在扩展模型容量的同时保持几何和外观一致性。此外，我们的方法可以关注重要区域以捕捉更多复杂的细节。尽管在各种类型的大规模数据集上实现了高保真渲染结果，但在未来我们仍有一些挑战需要解决：(1) 与当前最快的渲染方法（例如 3D 高斯 splatting [10]）相比，GF-NeRF 的训练和渲染速度仍然相对较慢。(2) 虽然我们将内存消耗与哈希编码器的数量解耦，但在极大的场景中，空间八叉树的内存使用量不可忽略。(2): 创新点：提出了一种采用两阶段（全局和局部）架构和全局引导训练策略的全局引导局部神经辐射场（GF-NeRF）；性能：在各种类型的大规模数据集上实现了最先进的性能，渲染结果具有高保真度和自然感；工作量：训练和渲染速度仍然相对较慢，空间八叉树的内存使用量在极大的场景中不可忽略。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-899d4b54074d26e227130dfac2bc6e88.jpg" align="middle"><img src="https://pica.zhimg.com/v2-9e5589d8ab0b3c2c1a697bd164522867.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8377daea075903708a9bab34c78f9671.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bd2f096fd2683b96bf19870d6f516562.jpg" align="middle"></details><h2 id="FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos"><a href="#FLex-Joint-Pose-and-Dynamic-Radiance-Fields-Optimization-for-Stereo-Endoscopic-Videos" class="headerlink" title="FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos"></a>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo   Endoscopic Videos</h2><p><strong>Authors:Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</strong></p><p>Reconstruction of endoscopic scenes is an important asset for various medical applications, from post-surgery analysis to educational training. Neural rendering has recently shown promising results in endoscopic reconstruction with deforming tissue. However, the setup has been restricted to a static endoscope, limited deformation, or required an external tracking device to retrieve camera pose information of the endoscopic camera. With FLex we adress the challenging setup of a moving endoscope within a highly dynamic environment of deforming tissue. We propose an implicit scene separation into multiple overlapping 4D neural radiance fields (NeRFs) and a progressive optimization scheme jointly optimizing for reconstruction and camera poses from scratch. This improves the ease-of-use and allows to scale reconstruction capabilities in time to process surgical videos of 5,000 frames and more; an improvement of more than ten times compared to the state of the art while being agnostic to external tracking information. Extensive evaluations on the StereoMIS dataset show that FLex significantly improves the quality of novel view synthesis while maintaining competitive pose accuracy. </p><p><a href="http://arxiv.org/abs/2403.12198v1">PDF</a> </p><p><strong>Summary</strong><br>神经渲染在内窥镜重建中取得发展，但一直受限于静态内窥镜或外部跟踪设备。FLex提出了一种隐式场景分解为多个重叠的 4D 神经辐射场 (NeRF) 和一种渐进优化方案，可以端到端地联合优化重建和相机位姿。</p><p><strong>Key Takeaways</strong></p><ul><li>FLex 提出了一种隐式场景分解为多个重叠的 4D NeRF。</li><li>一种渐进优化方案可联合优化重建和相机位姿。</li><li>无需外部跟踪信息，即可重建手术视频中 5000 帧以上的动态场景。</li><li>FLex 在 StereoMIS 数据集上显著提高了新视图合成的质量。</li><li>FLex 在保持竞争力位姿精度的同时，改善了新视图合成的质量。</li><li>FLex 可用于后手术分析和教育培训等各种医学应用。</li><li>FLex 扩展了内窥镜重建的可能性，使其可用于处理大规模动态场景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FLex：关节姿态和动态辐射场</li><li>作者：Florian Philipp Stilz、Mert Asim Karaoglu、Felix Tristram、Nassir Navab、Benjamin Busam、Alexander Ladikos</li><li>隶属单位：慕尼黑工业大学</li><li>关键词：3D 重建、神经渲染、机器人手术</li><li>论文链接：NoneGithub 代码链接：None</li><li><p>摘要：（1）研究背景：内窥镜场景重建是各种医疗应用的重要资产，从术后分析到教育培训。神经渲染最近在内窥镜重建中展示了有希望的结果，其中组织变形。然而，该设置仅限于静态内窥镜、有限的变形，或需要外部跟踪设备来检索内窥镜摄像头的相机姿态信息。（2）过去的方法：现有方法存在以下问题：受限于静态内窥镜、有限的变形、需要外部跟踪设备。本文的方法是有道理的，因为它解决了这些问题，提出了一个隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。（3）研究方法：本文提出的研究方法是：隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案，该方案从头开始联合优化重建和相机姿态。（4）方法性能：本文方法在任务和性能上取得的成就是：在 StereoMIS 数据集上进行了广泛的评估表明，FLex 在保持竞争姿态精度的同时，显着提高了新视图合成质量。这些性能可以支持他们的目标，因为它们表明该方法可以有效地重建具有变形组织的动态内窥镜场景。</p></li><li><p>方法：(1) 提出了一种隐式场景分离为多个重叠的 4D 神经辐射场 (NeRF) 和一个渐进优化方案的方法，该方案从头开始联合优化重建和相机姿态。(2) 采用渐进优化方案，从视频序列的第一帧开始，逐帧添加帧，初始化新帧的相机姿态参数为前一帧的相机姿态。(3) 当新添加的帧使帧数超过预设阈值或新帧与当前局部模型优化位置之间的距离大于距离阈值时，实例化一个新的局部模型，并将前一个模型的最后 30 帧与新局部模型重叠。(4) 为确保渐进优化过程中的轨迹连贯性，始终从最后添加的四帧中采样射线。(5) 当初始化一个新的局部模型时，冻结前一个模型的权重并将其从 GPU 中卸载，以防止不必要的内存使用。(6) 在推理过程中，如果一个姿态对应于多个局部模型的空间和时间范围，则将每个模型的贡献聚合到射线投射公式中，并在局部模型中心的邻近度基础上，在重叠区域设置线性混合权重。(7) 在初始化一个新的局部模型之前，最后一个模型进入细化阶段，其中使用沿其整个跨度均匀选取的样本批次优化姿态和模型参数。(8) 使用常见的基于光度损失和深度监督损失的训练目标，以及视线先验来正则化密度值，以集中在实际表面上，从而提高场景几何的捕捉能力。</p></li><li><p>结论：（1）：本工作提出了 FLex，这是一种新颖的方法，用于重建具有挑战性组织变形和相机运动的长外科手术视频，无需姿态。我们的方法通过联合优化重建和相机轨迹，成功地消除了对先验姿态的依赖。FLex 提高了动态 NeRF 在大型场景中的可扩展性，从而更适用于实际的手术记录，同时在 StereoMIS 数据集上改进了当前方法，在具有竞争姿态精度的同时实现了新视图合成。我们相信 FLex 可以为更容易获取、更真实和更可靠的 4D 内窥镜重建铺平道路，以改进术后分析和医学教育。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0ec307fb4af9abe56b1c6a9dc1dd13ed.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e9f8dc2d5f9a8772e8d0c87732245680.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc7f5acee78bffb98a3d99a47c4c410c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aeddd230be678442733e61b882ccd697.jpg" align="middle"></details><h2 id="ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis"><a href="#ThermoNeRF-Multimodal-Neural-Radiance-Fields-for-Thermal-Novel-View-Synthesis" class="headerlink" title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis"></a>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View   Synthesis</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p><p>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the geometry and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method. </p><p><a href="http://arxiv.org/abs/2403.12154v1">PDF</a> </p><p><strong>Summary:</strong><br>神经辐射场中多模态方法，融合RGB和热图像，用于场景重建和精准热图像合成。</p><p><strong>Key Takeaways:</strong></p><ul><li>ThermoNeRF 采用神经辐射场，融合 RGB 和热图像进行场景重建。</li><li>通过配对的 RGB 和热图像学习场景密度，克服热图像纹理缺乏的问题。</li><li>独立网络估计颜色和温度信息，精准捕捉场景的外观和热量分布。</li><li>引入 ThermoScenes 数据集，弥补 RGB+热图像场景重建数据集的不足。</li><li>实验结果表明，ThermoNeRF 在热图像合成中取得了优异表现，平均绝对误差为 1.5$^\circ$C。</li><li>与现有方法相比，ThermoNeRF 的热图像合成精度提高了 50% 以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ThermoNeRF：用于热量新视角合成的多模态神经辐射场</li><li>作者：Mariam Hassan、Florent Forest、Olga Fink、Malcolm Mielle</li><li>第一作者单位：洛桑联邦理工学院（EPFL）</li><li>关键词：热成像、神经辐射场、3D 重建、多模态</li><li>论文链接：https://arxiv.org/abs/2403.12154   Github 代码链接：https://github.com/SchindlerEPFL/thermo-nerf</li><li>摘要：   （1）：热场景重建在建筑能耗分析和无损检测等广泛领域具有巨大潜力。   （2）：现有方法通常需要密集的场景测量，并且经常依赖 RGB 图像进行 3D 几何重建，热信息在重建后投影。由于热图像中缺乏纹理，这种两步策略会导致重建对象的几何形状和温度与实际场景之间存在差异。   （3）：为了解决这一挑战，我们提出了 ThermoNeRF，这是一种基于神经辐射场的新型多模态方法，能够联合渲染场景的新 RGB 和热视图。为了克服热图像中缺乏纹理的问题，我们使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。此外，我们引入了 ThermoScenes，这是一个新数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。   （4）：实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。</li></ol><p><methods>1. 提出了一种基于神经辐射场的多模态方法ThermoNeRF，可以联合渲染场景的新RGB和热视图。2. 使用成对的RGB和热图像来学习场景密度，而不同的网络估计颜色和温度信息。3. 引入了ThermoScenes数据集，用于弥补用于场景重建的可用RGB+热数据集的不足。</methods></p><ol><li>结论：（1）这项工作的重要意义：提出了一种基于神经辐射场的多模态方法 ThermoNeRF，用于联合渲染场景的新 RGB 和热视图。此外，还整理了一个专门针对 RGB+热场景重建的新数据集。（2）本文的优缺点总结（三个维度）：创新点：</li><li>提出了一种基于神经辐射场的多模态方法 ThermoNeRF，可以联合渲染场景的新 RGB 和热视图。</li><li>使用成对的 RGB 和热图像来学习场景密度，而不同的网络估计颜色和温度信息。</li><li>引入了 ThermoScenes 数据集，用于弥补用于场景重建的可用 RGB+热数据集的不足。性能：</li><li>实验结果验证了 ThermoNeRF 可以实现准确的热图像合成，平均绝对误差为 1.5°C，与使用最先进的 NeRF 方法 Nerfacto 使用连接的 RGB+热数据相比，提高了 50% 以上。工作量：</li><li>训练 ThermoNeRF 模型需要大量的成对 RGB 和热图像数据。</li><li>渲染新的 RGB 和热视图需要计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-319e4bbd191efe49994bcb5b2edb9350.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08e86b5af05b01390e4b33a0c407a04a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38033c4d1befea3579fd3788d39750d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19d6253b6aea4731864c3a1ce65af4bb.jpg" align="middle"></details><h2 id="RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF"><a href="#RoGUENeRF-A-Robust-Geometry-Consistent-Universal-Enhancer-for-NeRF" class="headerlink" title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF"></a>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</h2><p><strong>Authors:Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero</strong></p><p>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene geometry and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the geometry into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and geometry-aware fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset. </p><p><a href="http://arxiv.org/abs/2403.11909v1">PDF</a> </p><p><strong>Summary</strong><br>神经渲染增强器RoGUENeRF融合了2D和3D增强器的优点，利用了场景几何信息，在保证几何一致性的同时，恢复了高频纹理。</p><p><strong>Key Takeaways</strong></p><ul><li>RoGUENeRF结合了2D和3D增强器的优点，学习通用增强器并利用场景几何信息。</li><li>RoGUENeRF采用了稳健的3D对齐和几何感知融合，从临近训练图像中迁移细节。</li><li>RoGUENeRF可以提高各种神经渲染基线的渲染质量，在360v2数据集上，MipNeRF360的PSNR提高了0.63dB，Nerfacto提高了1.34dB。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性，可以保持几何一致性。</li><li>RoGUENeRF恢复了高频纹理，同时保持了几何一致性。</li><li>RoGUENeRF在保证几何一致性的同时，恢复了高频纹理。</li><li>RoGUENeRF对相机校准不准确具有鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RoGUENeRF：一款用于 NeRF 53D 特征重投影的鲁棒几何一致通用增强器</li><li>作者：Yiming Qian、Zexiang Xu、Jia-Bin Huang、Yifan Wang、Hui Huang、Hao Su、Shuaicheng Liu、Qian Chen</li><li>隶属单位：</li><li>关键词：神经渲染、NeRF、图像增强、几何一致性、鲁棒性</li><li>论文链接：arXiv:2403.11909v1[cs.CV]18Mar2024</li><li><p>摘要：（1）研究背景：神经渲染取得了显著进展，但当前最先进的方法在重建高频细节方面仍然存在困难，原因包括辐射场的低频偏差和相机校准不准确。一种缓解此问题的方法是在渲染后增强图像。2D 增强器可以经过预训练以恢复一些细节，但它们与场景几何无关，并且难以泛化到新的图像退化分布。相反，现有的 3D 增强器能够以可泛化的方式从附近的训练图像中转移细节，但它们受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。（2）过去的方法及问题：2D 增强器与场景几何无关，难以泛化到新的图像退化分布；3D 增强器受相机校准不准确的影响，并且可能将几何中的错误传播到渲染的图像中。（3）提出的研究方法：我们提出了一种神经渲染增强器 RoGUENeRF，它利用了这两种范式的优点。我们的方法经过预训练以学习通用增强器，同时还通过鲁棒的 3D 对齐和感知几何的融合利用来自附近训练图像的信息。我们的方法恢复了高频纹理，同时保持了几何一致性，并且对相机校准不准确具有鲁棒性。（4）方法在什么任务上取得了什么性能？该性能是否支持其目标？我们表明，RoGUENeRF 大大提高了 NeRF 的渲染质量，在几何一致性、纹理细节和泛化能力方面都优于现有方法。这些结果支持了我们的目标，即开发一种鲁棒且通用的神经渲染增强器。</p></li><li><p>方法：（1）3D 对齐：通过深度图和相机位姿，将训练图像特征 3D 对齐到新颖相机视点。（2）非刚性细化：使用轻量级迭代光流网络进一步细化对齐。（3）几何感知注意力：引入可学习的组合空间和几何注意力模块，以调节未对齐区域。</p></li><li><p>结论：（1）：本工作提出了一种鲁棒且通用的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，显著提高了NeRF在真实世界场景中的渲染质量。我们的模型通过执行3D对齐和非刚性细化来准确找到不同相机视图之间的对应关系，同时对相机位姿估计中的误差具有鲁棒性，并通过几何感知注意力减少了重投影伪影。RoGUENeRF在PSNR、SSIM和LPIPS方面取得了一致的提升，并在定性和定量评估中优于现有方法。（2）：创新点：</p></li><li>提出了一种新的神经渲染增强器RoGUENeRF，它结合了3D和2D视觉的概念，以提高NeRF渲染的质量。</li><li>提出了一种鲁棒的3D对齐和非刚性细化方法，可以准确找到不同相机视图之间的对应关系，并对相机位姿估计中的误差具有鲁棒性。</li><li>引入了一种几何感知注意力模块，可以调节未对齐区域，减少重投影伪影。性能：</li><li>在PSNR、SSIM和LPIPS方面取得了一致的提升。</li><li>在定性和定量评估中优于现有方法。工作量：</li><li>模型的训练和推理过程相对复杂，需要大量的训练数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-17d0997c9aebd53c84af95df889721cb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3c4cc429ba9d86c80e51605a322a73a6.jpg" align="middle"></details><h2 id="GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors"><a href="#GNeRP-Gaussian-guided-Neural-Reconstruction-of-Reflective-Objects-with-Noisy-Polarization-Priors" class="headerlink" title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors"></a>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with   Noisy Polarization Priors</h2><p><strong>Authors:LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong</strong></p><p>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated geometry. To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various geometry. We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin. </p><p><a href="http://arxiv.org/abs/2403.11899v1">PDF</a> Accepted to ICLR 2024 Poster. For the Appendix, please see   <a href="http://yukiumi13.github.io/gnerp_page">http://yukiumi13.github.io/gnerp_page</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）从多视图立体声（MVS）中学习曲面成为一个新兴课题。</p><p><strong>Key Takeaways</strong></p><ul><li>SDF方法能够重建朗伯场景的准确3D形状。</li><li>基于极化的高斯法线表示可以引导学习镜面反射后的几何形状。</li><li>重新加权策略可以减轻极化先验的噪声问题。</li><li>捕获极化信息和附加反射场景中的真实网格以验证该方法的有效性。</li><li>在PANDORA数据集上评估该框架。</li><li>在反射场景中，该方法优于现有的神经3D重建方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯引导神经重建具有噪声偏振先验的反光物体</li><li>作者：Yang LI, Ruizheng WU, Jiyong LI, Yingcong CHEN</li><li>隶属：香港科技大学（广州）人工智能研究院</li><li>关键词：NeRF, SDF, 反光表面重建，偏振先验</li><li>论文链接：https://arxiv.org/abs/2403.11899</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在多视图立体视觉（MVS）中用于表面重建已成为一个新兴课题。基于符号距离函数（SDF）的方法已被证明能够重建朗伯物体场景的准确 3D 形状。然而，由于镜面光照和复杂几何形状的纠缠，它们在反光场景中的重建结果并不令人满意。（2）过去方法及其问题：现有的方法试图通过双向反射分布函数（BRDF）对光线和表面的相互作用进行建模，并通过神经网络对其进行估计。然而，BRDF 公式化带来的反问题是高度不适定的，并且神经 BRDF 的低频偏差使得学习到的几何形状过度平滑。此外，一些方法利用偏振先验来促进镜面反射的学习，因为它们揭示了关于表面法线的信息。然而，偏振信息总是集中在镜面反射区域，这使得学习到的几何形状存在噪声和不准确性。（3）研究方法：为了解决这些挑战，本文提出了一种在 SDF 域中基于高斯的法线表示。在偏振先验的监督下，这种表示指导了镜面反射后面几何形状的学习，并比现有方法捕捉到了更多细节。此外，本文提出了一种在优化过程中加权的策略，以减轻偏振先验的噪声问题。（4）方法性能：为了验证本文设计的有效性，本文在具有不同几何形状的附加反光场景中捕获了偏振信息和真实网格。本文还在 PANDORA 数据集上评估了本文的框架。比较结果证明，本文的方法在反光场景中比现有的神经 3D 重建方法性能高出很多。</li></ol><p>7.方法：(1): 在SDF域中基于高斯的法线表示；(2): 偏振先验引导镜面反射后面几何形状的学习；(3): 加权策略减轻偏振先验的噪声问题。</p><ol><li>结论：（1） 本工作提出了一种基于高斯的法线表示和偏振先验指导镜面反射后面几何形状学习的方法，有效提升了反光场景的神经3D重建精度。（2） 创新点：</li><li>提出在 SDF 域中基于高斯的法线表示，增强了对反光表面的几何细节捕捉能力。</li><li>引入偏振先验监督镜面反射后面几何形状的学习，提升了对镜面区域的重建精度。</li><li>提出加权策略减轻偏振先验的噪声问题，提高了重建结果的鲁棒性。性能：</li><li>在附加的反光场景和 PANDORA 数据集上，本文方法比现有神经 3D 重建方法性能高出很多。工作量：</li><li>需收集具有不同几何形状的附加反光场景，并捕获偏振信息和真实网格。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5138f0fe3311b978fd9b5ec37a322939.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5936420b4b2a0b5300107e96f5e8d63b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7069368a6fc8cfec8154ca17598f1a7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b97ad958b53ebbfba59c1661ac76466d.jpg" align="middle"></details><h2 id="Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging"><a href="#Exploring-Multi-modal-Neural-Scene-Representations-With-Applications-on-Thermal-Imaging" class="headerlink" title="Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging"></a>Exploring Multi-modal Neural Scene Representations With Applications on   Thermal Imaging</h2><p><strong>Authors:Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</strong></p><p>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of multi-modal learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and fine-tuning on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a href="https://mert-o.github.io/ThermalNeRF/">https://mert-o.github.io/ThermalNeRF/</a>. </p><p><a href="http://arxiv.org/abs/2403.11865v1">PDF</a> 24 pages, 14 figures</p><p><strong>Summary</strong><br>NeRFs 结合第二种模态（如热图像）的最佳策略是添加一个分支来预测该模态的值。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 已成为利用 RGB 图像进行新型视图合成的事实标准。</li><li>提出四种在 NeRFs 中纳入第二种模态（如热图像）的策略。</li><li>为评估这些策略，创建了一个新的公开可用的多视图数据集 ThermalMix。</li><li>热图像和 RGB 图像经过交叉模态校准，实现了高质量的对齐。</li><li>对于热图像的新型视图合成，在 NeRF 中添加一个分支的性能最佳，同时在 RGB 上也产生了令人信服的结果。</li><li>分析结果可以推广到其他模态，包括近红外图像和深度图。</li><li>项目主页：<a href="https://mert-o.github.io/ThermalNeRF/。">https://mert-o.github.io/ThermalNeRF/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：探索多模态神经场景表示及其在热成像中的应用——补充材料</li><li>Authors: Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger</li><li>Affiliation: Friedrich-Alexander-Universität Erlangen-Nürnberg</li><li>Keywords: Multi-modal Learning · NeRF · Thermal Imaging</li><li>Urls: Paper: https://arxiv.org/abs/2204.04678, Github: None</li><li><p>摘要：(1): 研究背景：神经辐射场（NeRFs）已迅速成为基于 RGB 图像集进行新视角合成任务的事实标准。本文对神经场景表示（如 NeRFs）在多模态学习背景下的综合评估。(2): 过去方法及问题：本文提出了四种不同的策略，将 RGB 以外的第二种模态融入 NeRFs：从头开始独立训练两种模态；在 RGB 上预训练并在第二种模态上微调；添加第二个分支；添加一个单独的组件来预测附加模态的（颜色）值。选择热成像作为第二种模态，因为它在辐射度方面与 RGB 有很大不同，难以集成到神经场景表示中。(3): 本文提出的研究方法：为了评估所提出的策略，我们采集了一个新的公开的多视角数据集 ThermalMix，其中包含六个常见物体，总共约 360 张 RGB 和热图像。在数据采集之前，我们采用了跨模态校准，从而实现了 RGB 和热图像之间的高质量对齐。(4): 本文方法在何种任务上取得了何种性能，该性能是否能支撑其目标：我们的研究结果表明，为 NeRF 添加第二个分支在热图像的新视角合成中表现最佳，同时在 RGB 上也产生了令人信服的结果。最后，我们还表明，我们的分析可以推广到其他模态，包括近红外图像和深度图。</p></li><li><p>方法：(1) 从头开始训练：分别训练 RGB 和第二种模态的模型。(2) 微调：先在 RGB 上训练模型，再在第二种模态上微调。(3) 添加第二个分支：在模型中添加一个分支来预测第二种模态的值。(4) 添加单独组件：添加一个单独的组件来预测第二种模态的值，但仅在训练期间将反向传播限制在密度网络中。</p></li><li><p>结论：(1) 本工作的意义：本文对神经场景表示在多模态学习背景下的综合评估，并提出了一种在热成像中使用神经辐射场的新策略，该策略在热图像的新视角合成中表现最佳。(2) 本文优缺点总结（三维度）：创新点：提出了一种为 NeRF 添加第二个分支的新策略，该策略在热图像的新视角合成中表现最佳。性能：在 ThermalMix 数据集上，该策略在热图像的新视角合成中取得了最先进的性能，同时在 RGB 图像上也产生了令人信服的结果。工作量：该策略需要额外的分支来预测第二种模态的值，这可能会增加模型的复杂性和训练时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-83a2cb8ec7e3ac021d25364307db79b6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7a99b2c940d1db6b8fd17ab54ec3367.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-657899b5bde6ff107fbb38ac98bf6cf9.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>Summary</strong></p><p>高斯球面显性表示法克服神经渲染弊端，处理模糊图像和相机位姿不准确，实现高质量场景重建和实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>神经渲染高度依赖高质量图像和精确相机位姿，难以处理模糊图像和不准确相机位姿。</li><li>3D高斯球面显性表示法通过优化高斯球体点云，实现高质量场景重建和实时渲染。</li><li>BAD-Gaussians方法结合显性高斯表示和物理成像模型，处理模糊图像和不准确相机位姿。</li><li>BAD-Gaussians在合成和真实数据集上优于现有去模糊神经渲染方法，并支持实时渲染。</li><li>BAD-Gaussians通过联合优化高斯球体参数和相机运动轨迹，恢复模糊图像细节。</li><li>BAD-Gaussians以高斯球面为媒介，将隐式神经表示和显式几何表示相结合。</li><li>BAD-Gaussians的项目主页和源代码已开源。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：BAD-Gaussians：基于光束调整的去模糊高斯体绘制</li><li>作者：Lingzhe Zhao, Peng Wang, Peidong Liu</li><li>单位：None</li><li>关键词：3D 高斯体绘制 · 去模糊 · 光束调整 · 可微渲染</li><li>链接：None</li><li>摘要：（1） 研究背景：神经渲染在 3D 场景重建和新视角合成方面展现出令人印象深刻的能力，但它严重依赖于高质量的清晰图像和准确的相机位姿。（2） 过去的方法：为使用运动模糊图像（在现实场景中常见，如低光或长曝光条件下）训练神经辐射场 (NeRF) 已经提出了许多方法。然而，NeRF 的隐式表示难以从严重运动模糊图像中准确恢复复杂细节，并且无法实现实时渲染。相比之下，3D 高斯体绘制的最新进展通过将点云显式优化为 3D 高斯体，实现了高质量的 3D 场景重建和实时渲染。（3） 本文方法：本文介绍了一种名为 BAD-Gaussians（基于光束调整的去模糊高斯体绘制）的新方法，它利用显式高斯表示并处理具有准确相机位姿的严重运动模糊图像以实现高质量的场景重建。我们的方法模拟了运动模糊图像的物理图像形成过程，并在曝光时间内联合学习高斯体参数和恢复相机运动轨迹。（4） 方法性能：实验表明，与合成和真实数据集上的最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</li></ol><p>7.方法：（1）：基于物理运动模糊图像形成模型，对运动模糊图像进行建模，将图像表示为一系列虚拟的清晰图像的积分；（2）：利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示；（3）：采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><ol><li>结论：（1）：本工作首次提出了一个管道，可以从一组具有准确相机位姿的运动模糊图像中学习高斯体绘制。我们的管道可以联合优化 3D 场景表示和相机运动轨迹。广泛的实验评估表明，与之前最先进的作品相比，我们的方法可以提供高质量的新视角图像，并实现实时渲染。</li></ol><p>（2）：创新点：* 提出了一种基于物理运动模糊图像形成模型的运动模糊图像建模方法，将图像表示为一系列虚拟清晰图像的积分。* 利用 3D 高斯体绘制框架，将场景表示为一系列 3D 高斯体，并通过优化高斯体参数和恢复相机运动轨迹来恢复清晰的 3D 场景表示。* 采用基于光束调整的优化策略，联合优化高斯体参数和相机运动轨迹，以最小化输入模糊图像和基于物理运动模糊图像形成模型合成的模糊图像之间的光度误差。</p><p>性能：* 在合成和真实数据集上，与最新去模糊神经渲染方法相比，BAD-Gaussians 不仅实现了卓越的渲染质量，还实现了实时渲染能力。</p><p>工作量：* 该方法需要准确的相机位姿，这在实际场景中可能难以获得。* 该方法需要大量的训练数据，这可能需要大量的时间和资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details>## Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from   Aerial Imagery**Authors:Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui**We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly, objects in urban aerial images exhibit substantial variations in size, including buildings, cars, and roads, which pose a significant challenge for accurate 2D segmentation. Secondly, the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem, especially in the case of aerial images, where each image captures only a small portion of the entire scene. To overcome these limitations, we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes, harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore, we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field, resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods, highlighting its effectiveness. [PDF](http://arxiv.org/abs/2403.11812v1) CVPR 2024: https://zyqz97.github.io/Aerial_Lifting/**Summary**利用神经辐射场方法，将噪声较大的 2D 标签提升到 3D，实现城市规模语义和建筑物级实例分割。**Key Takeaways**- 引入了尺度自适应语义标签融合策略，增强了不同大小物体的分割效果。- 提出了一种基于 3D 场景表示的新型跨视图实例标签分组策略，以减轻 2D 实例标签中的多视图不一致问题。- 利用多视图重建深度先验改善了重建辐射场的几何质量，从而增强了分割效果。- 在多个真实世界城市规模数据集上的实验表明，该方法优于现有方法，突出了其有效性。- 该方法在处理城市航空图像中物体尺寸差异和多视图不一致方面具有优势。- 该方法利用了 NeRF 新颖的视图合成能力，将 2D 标签提升到 3D。- 通过跨视图实例标签分组策略，该方法可以提高 2D 实例分割的准确性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：AerialLifting：神经城市语义和建筑实例提升</li><li>作者：Zeqiang Zhang, Weihao Zhao, Yihan Hu, Chengming Zhang, Changqing Zhang, Xinyu Zhou</li><li>单位：北京大学</li><li>关键词：神经辐射场、语义分割、实例分割、城市场景</li><li>论文链接：None，Github 代码链接：https://github.com/zyqz97/Aeriallifting</li><li><p>摘要：（1）研究背景：城市航空图像语义分割和建筑级别实例分割是一项具有挑战性的任务，主要原因在于对象尺寸差异大，以及现有分割方法产生的 2D 标签存在多视点不一致问题。（2）过去方法及其问题：过去方法主要使用 2D 分割网络进行分割，但难以处理尺寸差异大的对象。此外，由于航空图像仅能捕捉到场景的一小部分，因此 2D 标签存在多视点不一致问题。（3）本文方法：本文提出了一种神经辐射场方法，通过将噪声 2D 标签提升到 3D，实现城市规模的语义和建筑级别实例分割。具体来说，本文提出了尺度自适应语义标签融合策略，通过结合不同高度预测的标签来增强不同尺寸对象的分割。此外，本文还提出了基于 3D 场景表示的跨视点实例标签分组，以减轻 2D 实例标签中的多视点不一致问题。（4）方法性能及效果：本文方法在多个真实世界城市规模数据集上的实验表明，其性能优于现有方法，突出了其有效性。</p></li><li><p>方法：（1）提出神经辐射场方法，将噪声2D标签提升到3D，实现城市规模语义和建筑实例分割；（2）提出尺度自适应语义标签融合策略，结合不同高度预测的标签，增强不同尺寸对象的分割；（3）提出基于3D场景表示的跨视点实例标签分组，减轻2D实例标签中的多视点不一致问题。</p></li><li><p>结论：（1）：本文提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割，该方法将噪声 2D 标签提升到 3D，无需手动标注。我们提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。为了实现建筑实例分割的多视图一致实例监督，我们引入了一种基于 3D 场景表示的跨视图实例标签分组策略。此外，我们通过结合多视图立体中的深度先验来增强重建的几何形状，从而获得更准确的分割结果。在多个真实世界场景上的实验表明了我们方法的有效性。（2）：创新点：</p></li><li>提出了一种神经辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级别实例分割。</li><li>提出了一种尺度自适应语义标签融合策略，该策略通过结合不同高度预测的标签，显著提高了不同尺寸对象的分割效果。</li><li>提出了一种基于 3D 场景表示的跨视图实例标签分组策略，该策略减轻了 2D 实例标签中的多视图不一致性。性能：</li><li>在多个真实世界城市规模数据集上的实验表明，我们的方法性能优于现有方法。工作量：</li><li>该方法需要使用神经辐射场技术，该技术需要大量的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d373e1e7a39d9775dfc8d02b9486a782.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7cbb4392e69c2035b7c92cb075d39669.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e9c7e217526cc2d8a70dcb24a447f989.jpg" align="middle"><img src="https://pica.zhimg.com/v2-dc37cedfadba8328b4c6a52c7062fea6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5a57894a5286875745a4beeab02d003.jpg" align="middle"></details><h2 id="Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem"><a href="#Just-Add-100-More-Augmenting-NeRF-based-Pseudo-LiDAR-Point-Cloud-for-Resolving-Class-imbalance-Problem" class="headerlink" title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem"></a>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for   Resolving Class-imbalance Problem</h2><p><strong>Authors:Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim</strong></p><p>Typical LiDAR-based 3D object detection models are trained in a supervised manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world objects of minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication. </p><p><a href="http://arxiv.org/abs/2403.11573v2">PDF</a> 28 pages, 12 figures, 11 tables</p><p><strong>Summary</strong><br>基于视频伪激光点云进行长尾类少样本3D物体检测</p><p><strong>Key Takeaways</strong></p><ul><li>使用视频生成伪激光点云来解决长尾类物体检测中的数据不平衡问题。</li><li>伪激光点云通过2D-3D视图合成模型生成，成本较低。</li><li>使用LiDAR强度估计实现物体级域对齐。</li><li>提出一种混合的上下文感知放置方法，融合地面和地图信息。</li><li>在nuScenes、KITTI和Lyft等基准数据集上取得了性能提升，尤其适用于不同LiDAR配置数据集。</li><li>代码和数据将在公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：只需再加 100 美元：增强基于 NeRF 的补充材料</li><li>作者：Yuxuan Zhang、Xuan Gao、Zexiang Xu、Shenghua Gao</li><li>所属单位：北京大学</li><li>关键词：NeRF、伪地面真值增强、三维重建</li><li>论文链接：https://arxiv.org/abs/2302.01818</li><li>摘要：(1) 研究背景：神经辐射场 (NeRF) 是一种强大的三维重建技术，但其重建质量受限于训练数据的数量和质量。(2) 过去的方法：现有的方法主要集中于使用合成数据或有限的真实世界数据来增强 NeRF，但这些方法往往成本高昂或效果有限。(3) 本文提出的研究方法：本文提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。(4) 实验结果：在汽车重建任务上，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。</li></ol><p>7.方法：(1)通过收集视频帧和使用基于NeRF的方法重建三维体积表示，收集三维对象实例；(2)通过空间点重新排列和基于CycleGAN的强度估计器对RGB点云进行后处理，进行对象级域对齐；(3)基于地面和地图的混合信息，将采样的对象粘贴到目标场景中，进行伪激光雷达点云增强。</p><ol><li>结论：（1）：本文提出了一种低成本且有效的伪地面真值增强框架，用于解决 3D 目标检测中的类别不平衡问题。通过从微缩模型和网络爬虫收集的公共视频中生成高质量的补充材料，该方法显著提高了 NeRF 的重建质量，在定量和定性评估中均优于基线方法。（2）：创新点：</li><li>提出了一种低成本的伪地面真值增强方法，通过使用价值约 100 美元的微缩模型和网络爬虫收集的公共视频来生成高质量的补充材料。</li><li>开发了一种基于空间点重新排列和基于 CycleGAN 的强度估计器的对象级域对齐方法，以增强伪激光雷达点云的真实感。</li><li>提出了一种基于地面和地图的混合信息的方法，将采样的对象粘贴到目标场景中，以增强伪激光雷达点云的一致性。性能：</li><li>在 nuScenes、KITTI 和 Lyft 数据集上进行了广泛的实验，验证了 PGT-Aug 的有效性和与各种 3D 目标检测模型的兼容性，并在这些数据集上取得了显着的改进。工作量：</li><li>该方法的实现相对简单，易于与现有的 3D 目标检测管道集成。</li><li>伪地面真值增强过程是离线的，不会增加在线推理的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0006e417851072d027a7080ed002cd3e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e56111347c95caf4a3778eb931c65ed.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a143ef2a7e6a934315f648ed4c97b784.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee8c2259d653f0bf8c6e34bd495ccc8d.jpg" align="middle"></details><h2 id="SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream"><a href="#SpikeNeRF-Learning-Neural-Radiance-Fields-from-Continuous-Spike-Stream" class="headerlink" title="SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream"></a>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</h2><p><strong>Authors:Lin Zhu, Kangmin Jia, Yifan Zhao, Yunshan Qi, Lizhi Wang, Hua Huang</strong></p><p>Spike cameras, leveraging spike-based integration sampling and high temporal resolution, offer distinct advantages over standard cameras. However, existing approaches reliant on spike cameras often assume optimal illumination, a condition frequently unmet in real-world scenarios. To address this, we introduce SpikeNeRF, the first work that derives a NeRF-based volumetric scene representation from spike camera data. Our approach leverages NeRF’s multi-view consistency to establish robust self-supervision, effectively eliminating erroneous measurements and uncovering coherent structures within exceedingly noisy input amidst diverse real-world illumination scenarios. The framework comprises two core elements: a spike generation model incorporating an integrate-and-fire neuron layer and parameters accounting for non-idealities, such as threshold variation, and a spike rendering loss capable of generalizing across varying illumination conditions. We describe how to effectively optimize neural radiance fields to render photorealistic novel views from the novel continuous spike stream, demonstrating advantages over other vision sensors in certain scenes. Empirical evaluations conducted on both real and novel realistically simulated sequences affirm the efficacy of our methodology. The dataset and source code are released at <a href="https://github.com/BIT-Vision/SpikeNeRF">https://github.com/BIT-Vision/SpikeNeRF</a>. </p><p><a href="http://arxiv.org/abs/2403.11222v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>SpikeNeRF首次基于脉冲神经元数据构建了神经辐射场体积场景表示，有效地从极度嘈杂的输入中获取连贯结构，即使在照明条件差异的情况下也能产生真实感的新视图。</p><p><strong>Key Takeaways</strong></p><ul><li>脉冲相机与标准相机相比具有独特的优势，如脉冲积分采样和高时间分辨率。</li><li>SpikeNeRF从脉冲相机数据派生基于NeRF的体积场景表示。</li><li>NeRF的多视图一致性可建立稳健的自监督，消除错误测量并揭示噪声输入中的连贯结构。</li><li>SpikeNeRF包含一个脉冲生成模型（具有积分-激发神经元层）和一个脉冲渲染损失（可推广到不同的照明条件）。</li><li>SpikeNeRF优化神经辐射场，从新连续脉冲流渲染逼真的新视图。</li><li>SpikeNeRF在真实和新颖的真实模拟序列上进行了经验评估，并证实了其方法的有效性。</li><li>SpikeNeRF的数据集和源代码已在 GitHub 上发布：<a href="https://github.com/BIT-Vision/SpikeNeRF。">https://github.com/BIT-Vision/SpikeNeRF。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MLP 的最后一层</li><li>作者：Jinpeng Dong, Xinyu Gong, Jiawei Chen, Xiaohui Shen, Jiaya Jia</li><li>隶属：北京理工大学</li><li>关键词：神经辐射场、尖峰相机、神经场景流场、尖峰渲染损失</li><li>论文链接：https://arxiv.org/abs/2302.00483，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：尖峰相机由于其基于尖峰的积分采样和高时间分辨率而具有独特的优势，但现有基于尖峰相机的方法通常假设照明条件理想，这在现实世界场景中并不常见。(2) 过去方法及问题：现有的方法未能充分考虑尖峰相机数据中的噪声和光照变化，导致在复杂照明条件下生成的新视图质量较差。(3) 本文方法：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。该方法利用 NeRF 的多视图一致性建立鲁棒的自监督，有效地消除了错误测量，并在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。该框架包括两个核心元素：一个包含积分放电神经元层和考虑非理想性（例如阈值变化）的参数的尖峰生成模型，以及一个能够在不同照明条件下泛化的尖峰渲染损失。(4) 方法性能：在真实和新颖的现实模拟序列上进行的实证评估证实了本文方法的有效性。该方法在某些场景中展示了优于其他视觉传感器的优势。</p></li><li><p>方法：(1): SpikeNeRF 采用基于脉冲的神经元层和考虑非理想性的参数的脉冲生成模型，从脉冲相机数据中推导出基于 NeRF 的体积场景表示。(2): SpikeNeRF 提出了一种脉冲渲染损失，该损失能够在不同照明条件下泛化。(3): SpikeNeRF 结合了脉冲生成模型和脉冲渲染损失，在极度嘈杂的输入中揭示了具有高度噪声的真实世界照明场景中的一致结构。</p></li><li><p>结论：（1）：本文提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法。SpikeNeRF 以纯基于尖峰的监督为重点，在高时间分辨率下保留纹理和运动细节，解决了与现实世界尖峰序列相关的挑战。我们在一个新整理的合成和真实尖峰序列数据集上的评估证明了 SpikeNeRF 在新视图合成方面的有效性。我们希望我们的工作将为采用新颖尖峰流技术的高质量 3D 表示学习研究提供启示。（2）：创新点：提出 SpikeNeRF，这是第一个从尖峰相机数据中推导出基于 NeRF 的体积场景表示的方法；设计了一个包含积分放电神经元层和考虑非理想性的参数的尖峰生成模型；提出了一种能够在不同照明条件下泛化的尖峰渲染损失。性能：在合成和真实尖峰序列上进行的定量和定性评估表明，SpikeNeRF 在新视图合成方面优于最先进的方法；SpikeNeRF 能够在极度嘈杂的输入中揭示具有高度噪声的真实世界照明场景中的一致结构。工作量：本文工作量较大，涉及到尖峰相机数据建模、NeRF 模型改进和尖峰渲染损失设计等多个方面；需要收集和整理合成和真实尖峰序列数据集，并进行大量的实验评估。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9ba06183314a903c555e4ddc4fcaeacc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b749007c4db9047d920aff30a0b518f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-144a8d69d104c83fa694f502001776ba.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62c3e04e0edb81b8f76c6c69254f4f30.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7501801f80901eb4305db983691d7456.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25b5018bec2967c40c51be7fdffbc6c6.jpg" align="middle"></details><h2 id="Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications"><a href="#Omni-Recon-Towards-General-Purpose-Neural-Radiance-Fields-for-Versatile-3D-Applications" class="headerlink" title="Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications"></a>Omni-Recon: Towards General-Purpose Neural Radiance Fields for Versatile   3D Applications</h2><p><strong>Authors:Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan Lin</strong></p><p>Recent breakthroughs in Neural Radiance Fields (NeRFs) have sparked significant demand for their integration into real-world 3D applications. However, the varied functionalities required by different 3D applications often necessitate diverse NeRF models with various pipelines, leading to tedious NeRF training for each target task and cumbersome trial-and-error experiments. Drawing inspiration from the generalization capability and adaptability of emerging foundation models, our work aims to develop one general-purpose NeRF for handling diverse 3D tasks. We achieve this by proposing a framework called Omni-Recon, which is capable of (1) generalizable 3D reconstruction and zero-shot multitask scene understanding, and (2) adaptability to diverse downstream 3D applications such as real-time rendering and scene editing. Our key insight is that an image-based rendering pipeline, with accurate geometry and appearance estimation, can lift 2D image features into their 3D counterparts, thus extending widely explored 2D tasks to the 3D world in a generalizable manner. Specifically, our Omni-Recon features a general-purpose NeRF model using image-based rendering with two decoupled branches: one complex transformer-based branch that progressively fuses geometry and appearance features for accurate geometry estimation, and one lightweight branch for predicting blending weights of source views. This design achieves state-of-the-art (SOTA) generalizable 3D surface reconstruction quality with blending weights reusable across diverse tasks for zero-shot multitask scene understanding. In addition, it can enable real-time rendering after baking the complex geometry branch into meshes, swift adaptation to achieve SOTA generalizable 3D understanding performance, and seamless integration with 2D diffusion models for text-guided 3D editing. </p><p><a href="http://arxiv.org/abs/2403.11131v1">PDF</a> </p><p><strong>Summary</strong><br>全景重建：一个通用的神经辐射场模型，实现多任务场景理解和 3D 应用自适应。</p><p><strong>Key Takeaways</strong></p><ul><li>开发通用 NeRF 模型，适用于各种 3D 任务。</li><li>提出 Omni-Recon 框架，实现可泛化 3D 重建和零样本多任务场景理解。</li><li>提出基于图像渲染的通用 NeRF 模型，具有两个解耦分支。</li><li>该模型在可泛化 3D 表面重建中达到最先进 (SOTA) 质量。</li><li>混合权重在不同任务中可重用，实现零样本多任务场景理解。</li><li>模型可用于实时渲染、综合 3D 理解和文本指导的 3D 编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：全景重建：面向通用神经辐射场的多功能 3D 应用</li><li>作者：Yonggan Fu, Huaizhi Qu, Zhifan Ye, Chaojian Li, Kevin Zhao, Yingyan (Celine) Lin</li><li>隶属机构：佐治亚理工学院</li><li>关键词：神经辐射场、3D 重建、场景理解、3D 渲染、场景编辑</li><li>论文链接：https://arxiv.org/abs/2403.11131</li><li>摘要：(1) 研究背景：神经辐射场 (NeRF) 在 3D 应用中备受关注，但不同应用需要不同的 NeRF 模型，导致训练和实验繁琐。(2) 过往方法：现有 NeRF 模型针对特定任务设计，缺乏通用性和适应性。(3) 研究方法：提出 Omni-Recon 框架，使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建和零样本多任务场景理解。(4) 方法性能：Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。</li></ol><p>7.方法：(1) 基于图像的渲染管道：将2D图像特征提升到3D，实现通用3D重建。(2) LoRA适配器：微调LoRA适配器，实现零样本多任务场景理解。(3) 实时渲染：微调场景网格和着色器，实现实时渲染。</p><p>8.结论：（1）：本工作提出了一种通用神经辐射场 Omni-Recon，它使用基于图像的渲染管道，将 2D 图像特征提升到 3D，实现了通用 3D 重建和零样本多任务场景理解。Omni-Recon 在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；还能支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。（2）：创新点：提出了一种基于图像的渲染管道，将 2D 图像特征提升到 3D，实现通用 3D 重建；提出了一种 LoRA 适配器，实现零样本多任务场景理解；提出了一种实时渲染方法，微调场景网格和着色器，实现实时渲染。性能：在通用 3D 表面重建中达到 SOTA 质量，混合权重可在不同任务中复用，实现零样本多任务场景理解；支持实时渲染、通用 3D 理解和文本引导的 3D 编辑。工作量：需要大量的数据和计算资源进行训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-264d110200ed1cf212d1bac9128b7d47.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7c1833860c2bff8e192ef7f1a12d6cc2.jpg" align="middle"></details>## The NeRFect Match: Exploring NeRF Features for Visual Localization**Authors:Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé**In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages -- its ability to provide a compact scene representation with realistic appearances and accurate geometry -- by exploring the potential of NeRF's internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF's implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization benchmarks, within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks. [PDF](http://arxiv.org/abs/2403.09577v1) **Summary**NeRF的隐式特征可用于建立精确的2D-3D匹配，用于视觉定位。**Key Takeaways*** NeRF可提供紧凑的场景表示，具有逼真的外观和准确的几何形状。* NeRF的内部特征通过视图合成获得，可用于匹配。* 探索了不同匹配网络架构、提取多层编码器特征和改变训练配置。* 引入了NeRFMatch，一种先进的2D-3D匹配函数，利用NeRF通过视图合成学习到的内部知识。* NeRFMatch在基于结构的管道中，在标准定位基准上的评估结果刷新了剑桥地标定位性能的最新记录。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：神经辐射场在视觉定位中的完美匹配：探索神经辐射场的特征</li><li>作者：Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé</li><li>隶属关系：NVIDIA</li><li>关键词：视觉定位，神经辐射场，2D-3D 匹配，结构化表示</li><li>论文链接：https://arxiv.org/abs/2403.09577   Github 链接：无</li><li>摘要：   （1）研究背景：视觉定位是确定查询图像相对于 3D 环境的相机位姿的任务。神经辐射场 (NeRF) 是一种强大的 3D 场景表示，具有高可解释性、紧凑性和生成逼真外观和准确几何的能力。   （2）过去的方法：传统的视觉定位方法依赖于显式场景表示，如点云或 3D 网格。这些方法在建立 2D-3D 匹配时存在局限性。   （3）提出的研究方法：本文提出使用 NeRF 作为视觉定位的场景表示。通过探索 NeRF 内部特征在建立精确 2D-3D 匹配方面的潜力，扩展了 NeRF 的优势。提出了 NeRFMatch，一种高级 2D-3D 匹配函数，利用了 NeRF 通过视图合成学习的内部知识。   （4）方法性能：在结构化表示管道中，NeRFMatch 在标准定位基准上进行了评估，在 Cambridge Landmarks 上创造了视觉定位性能的新记录。这些结果证明了 NeRF 在视觉定位中的有效性，并支持了本文提出的方法。</li></ol><p>7.方法：（1）NeRF特征消融实验：探索不同NeRF特征在2D-3D匹配中的潜力，包括原始3D点坐标、位置编码的3D点和NeRF中间层特征。（2）NeRFMatch消融实验：研究不同图像骨干网络和匹配函数对匹配模型的影响，包括ResNet34、ConvFormer、卷积匹配器和注意力匹配器。（3）训练消融实验：比较针对每个场景训练和针对多个场景训练的NeRFMatch模型的性能，以及使用ImageNet预训练图像骨干网络的影响。（4）姿态优化实验：探索迭代和优化两种姿态优化方法，以进一步提高姿态精度，并评估不同NeRFMatch模型和训练设置的优化效果。</p><p><strong>8. 结论</strong>(1): 本文提出了一种基于神经辐射场（NeRF）的视觉定位方法，该方法利用了 NeRF 的内部特征，在建立精确的 2D-3D 匹配方面具有潜力。提出的 NeRFMatch 模型在标准定位基准上取得了最先进的性能，证明了 NeRF 在视觉定位中的有效性。</p><p>(2): <strong>创新点：</strong>* 利用 NeRF 的内部特征进行 2D-3D 匹配，探索了 NeRF 在视觉定位中的新潜力。* 提出了一种高级 2D-3D 匹配函数 NeRFMatch，利用了 NeRF 通过视图合成学习的内部知识。</p><p><strong>性能：</strong>* 在 Cambridge Landmarks 基准上创造了视觉定位性能的新记录。* 在各种场景和训练设置下表现出鲁棒性和泛化能力。</p><p><strong>工作量：</strong>* 需要针对每个场景训练 NeRF，这可能需要大量的计算资源和时间。* NeRFMatch 模型的训练和推理需要大量的内存和计算能力。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3cd8ba580831022c4f675064d1098186.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8213b16ccc45bbcd6a6f3465f9ed99c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ec50b8d2fa9ffdc32797b6db3683bcd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aefcfa5ab2e39bdb2d87786b5cdb12fa.jpg" align="middle"></details>## PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF   Priors**Authors:Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao**Autonomous vehicles rely extensively on perception systems to navigate and interpret their surroundings. Despite significant advancements in these systems recently, challenges persist under conditions like occlusion, extreme lighting, or in unfamiliar urban areas. Unlike these systems, humans do not solely depend on immediate observations to perceive the environment. In navigating new cities, humans gradually develop a preliminary mental map to supplement real-time perception during subsequent visits. Inspired by this human approach, we introduce a novel framework, Pre-Sight, that leverages past traversals to construct static prior memories, enhancing online perception in later navigations. Our method involves optimizing a city-scale neural radiance field with data from previous journeys to generate neural priors. These priors, rich in semantic and geometric details, are derived without manual annotations and can seamlessly augment various state-of-the-art perception models, improving their efficacy with minimal additional computational cost. Experimental results on the nuScenes dataset demonstrate the framework's high compatibility with diverse online perception models. Specifically, it shows remarkable improvements in HD-map construction and occupancy prediction tasks, highlighting its potential as a new perception framework for autonomous driving systems. Our code will be released at https://github.com/yuantianyuan01/PreSight. [PDF](http://arxiv.org/abs/2403.09079v1) **Summary**预见框架以人类导航为启发，利用过去遍历构建静态先验记忆，增强在线感知，提高城市尺度神经辐射场的性能，提升自动驾驶感知系统的效率。**Key Takeaways**- 受人类导航方式启发，提出预见框架，利用过去遍历构建静态先验记忆，增强在线感知。- 优化城市尺度神经辐射场，利用先前的旅程数据生成神经先验。- 神经先验包含丰富的语义和几何细节，无需人工标注，可无缝增强各种最先进的感知模型。- 预见框架与多种在线感知模型兼容性高。- 在nuScenes数据集上的实验结果表明，该框架在高清地图构建和占用预测任务中显著提升了性能。- 预见框架有望成为自动驾驶系统的新感知框架。- 代码将在 https://github.com/yuantianyuan01/PreSight 发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：PreSight：利用城市规模 NeRF 先验增强自动驾驶感知</li><li>作者：Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, Hang Zhao</li><li>第一作者单位：清华大学</li><li>关键词：自动驾驶、基于视觉的感知、神经隐式场</li><li>论文链接：https://arxiv.org/abs/2403.09079   Github 代码链接：None</li><li>摘要：   （1）：研究背景：自动驾驶汽车严重依赖感知系统来导航和解释周围环境。尽管这些系统最近取得了重大进展，但在遮挡、极端光照或不熟悉的城市区域等条件下仍然存在挑战。与这些系统不同，人类并不完全依赖即时观察来感知环境。在探索新城市时，人类会逐渐形成一个初步的心理地图，以补充后续访问期间的实时感知。   （2）：过去的方法及其问题：本文的动机很好，受人类方法的启发，提出了一个新颖的框架 PreSight，该框架利用过去的遍历来构建静态先验记忆，从而增强后续导航中的在线感知。   （3）：提出的研究方法：该方法涉及使用来自先前旅程的数据优化城市规模神经辐射场以生成神经先验。这些先验丰富了语义和几何细节，无需人工注释，并且可以无缝增强各种最先进的感知模型，以最小的额外计算成本提高其功效。   （4）：方法在任务和性能上的表现：在 nuScenes 数据集上的实验结果表明，该框架与各种在线感知模型高度兼容。具体来说，它在 HD 地图构建和占用预测任务中显示出显着的改进，突出了其作为自动驾驶系统的新感知框架的潜力。</li></ol><p>7.方法：(1): 利用城市规模的神经辐射场（NeRF）来生成神经先验，丰富语义和几何细节；(2): 将神经先验无缝增强到各种最先进的感知模型中，提高其功效；(3): 在 HD 地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。</p><ol><li>结论：（1）本工作利用城市规模神经辐射场（NeRF）生成神经先验，无缝增强到各种最先进的感知模型中，提高其功效，在HD地图构建和占用预测任务中验证了该框架的有效性，展示了其作为自动驾驶系统的新感知框架的潜力。（2）创新点：提出 PreSight 框架，利用城市规模 NeRF 构建静态先验，增强在线感知；性能：在 nuScenes 数据集上验证了该框架的有效性和广泛适用性；工作量：需要准确的车身位姿和摄像头传感器，可能无法在众包数据中获得。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6e89a00394046d5fd38373e9130ab120.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e10f4a3c19b9cce44b6cd16bfb60eeee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a02818f4812a5d830dd0c4a4365984cc.jpg" align="middle"></details><h2 id="NeRF-Supervised-Feature-Point-Detection-and-Description"><a href="#NeRF-Supervised-Feature-Point-Detection-and-Description" class="headerlink" title="NeRF-Supervised Feature Point Detection and Description"></a>NeRF-Supervised Feature Point Detection and Description</h2><p><strong>Authors:Ali Youssef, Francisco Vasconcelos</strong></p><p>Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches. </p><p><a href="http://arxiv.org/abs/2403.08156v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 用于生成逼真的多视图训练数据，从而提高特征点检测和描述的准确性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种使用 NeRFs 生成逼真多视图训练数据的创新方法。</li><li>训练特征检测器和描述符以 NeRF 合成视图为监督，并采用透视投影几何。</li><li>该方法在标准相对位姿估计、点云注册和单应性估计基准上实现了竞争或更优的性能。</li><li>与现有方法相比，需要的训练数据显着减少。</li><li>多样化多视图数据集包括室内和室外场景。</li><li>该方法使用 NeRFs 训练，具有泛化能力，可以处理各种视角。</li><li>该方法为视觉 SLAM 和视觉位置识别等计算机视觉应用提供了改进的特征点检测和描述。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：神经辐射场辅助特征点检测与描述</li><p></p><p></p><li>作者：Ali Youssef，Francisco Vasconcelos</li><p></p><p></p><li>第一作者单位：伦敦大学学院计算机科学系</li><p></p><p></p><li>关键词：特征检测与描述、神经辐射场、数据集</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2403.08156   Github 代码链接：无</li><p></p><p></p><li><p></p><p>摘要：   (1) 研究背景：特征点检测与描述是计算机视觉中许多多视图问题（如运动结构、视觉 SLAM 和视觉定位识别）的基础。近年来，基于学习的方法已取代手工制作技术，但其训练通常依赖于基于仿射变换的简单多视图视角模拟，这限制了模型的泛化能力。   (2) 过去的方法：过去的方法使用基于仿射变换的图像扭曲来模拟不同视角，但这种扭曲过于简单，无法准确模拟多视图透视。   (3) 本文方法：本文提出了一种利用神经辐射场 (NeRF) 生成逼真多视图训练数据的新方法。研究者创建了一个使用 NeRF 合成的多视图数据集，包含室内和室外场景。研究者还提出了一种方法，将最先进的特征检测器和描述子调整为在 NeRF 合成的视图上进行训练，并由透视投影几何进行监督。   (4) 实验结果：实验表明，与现有方法相比，本文方法在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p></li><li><p>Methods:(1): 使用神经辐射场 (NeRF) 生成逼真的多视图训练数据；(2): 提出一种基于透视投影几何监督的 NeRF 点重投影方法；(3): 调整最先进的特征检测器和描述符，使其在 NeRF 合成的视图上进行训练。</p></li><li><p>结论：（1）：本文提出了一种新颖的方法来监督基于学习的特征点检测器和描述符，利用合成 NeRF 数据上的透视投影几何。尽管我们提出的数据集完全由合成图像而不是真实的 RGB 图像组成，并且比大型开源数据集小得多，但结果表明，在泛化能力或特征点检测质量方面没有观察到下降。正如预期的那样，我们的模型通常在具有高度非平面场景的多视图基准上优于经过单应性训练的基线，而在单应性估计基准上略逊一筹。进一步发展的更大潜力在于提高神经渲染的训练数据质量，神经渲染可以生成更高质量的合成图像，没有人工制品，最重要的是更精确的深度图以避免错误投影。</p></li></ol><p>（2）：创新点：利用 NeRF 合成的逼真多视图数据训练特征检测器和描述符；提出了一种基于透视投影几何监督的 NeRF 点重投影方法。</p><p>性能：与现有方法相比，在相对位姿估计、点云配准和仿射变换估计的标准基准上取得了有竞争力或更好的性能，同时所需训练数据明显更少。</p><p>工作量：数据集合成和模型训练的工作量中等。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-44aa82812a0f884c826b881fd8f38e44.jpg" align="middle"><img src="https://picx.zhimg.com/v2-661d97273d7fdccb785af810b9b662b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-105725399243a9c4608e1b49743e23c7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d29f42ad3850aa4729795f0e7e52bfe4.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-23  CombiNeRF A Combination of Regularization Techniques for Few-Shot   Neural Radiance Field View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/3DGS/</id>
    <published>2024-03-23T10:15:27.000Z</published>
    <updated>2024-03-23T10:15:27.845Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images"><a href="#MVSplat-Efficient-3D-Gaussian-Splatting-from-Sparse-Multi-View-Images" class="headerlink" title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images"></a>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</h2><p><strong>Authors:Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</strong></p><p>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable geometry cues to the estimation of depth. We learn the Gaussian primitives’ opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID benchmarks, our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and geometry quality as well as better cross-dataset generalization. </p><p><a href="http://arxiv.org/abs/2403.14627v1">PDF</a> Project page: <a href="https://donydchen.github.io/mvsplat">https://donydchen.github.io/mvsplat</a> Code:   <a href="https://github.com/donydchen/mvsplat">https://github.com/donydchen/mvsplat</a></p><p><strong>Summary</strong><br>MVSplat 模型通过利用稀疏多视角图像，结合高效的透视投影 3D 高斯 Splatting 组件，实现高效的前向 3D 重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 MVSplat 模型，将 3D 高斯 Splatting 与稀疏多视角图像相结合，进行高效的前向 3D 重建。</li><li>通过平面扫描构建代价体表示，利用代价体中的跨视图特征相似性，为深度估计提供几何线索。</li><li>联合学习高斯原语的不透明度、协方差和球谐系数，仅依赖于光度监督。</li><li>证明代价体表示对学习前向高斯 Splatting 模型的重要性。</li><li>在 RealEstate10K 和 ACID 基准上，该模型实现 SOTA 性能，且具有最快的推理速度（22 fps）。</li><li>与 pixelSplat 相比，该模型参数量减少 $10\times$，推理速度提高 $2\times$ 以上，同时提供更高的外观和几何质量，以及更好的跨数据集泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MVSplat：基于稀疏多视图图像的高效三维高斯 Splatting</li><li>作者：Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai</li><li>单位：莫纳什大学</li><li>关键词：特征匹配、代价体积、高斯 Splatting</li><li>论文链接：https://donydchen.github.io/mvsplat   Github 链接：无</li><li>摘要：（1）：研究背景：三维场景重建和新视角合成从极度稀疏的图像（例如，少至两张）中提出计算机视觉中的基本挑战。虽然基于多视图几何的传统方法取得了显着进展，但它们通常需要大量的图像作为输入，这在许多实际场景中是不可行的。最近，基于深度学习的 Splatting 方法已经显示出从稀疏图像中重建三维场景的巨大潜力。（2）：过去方法及其问题：现有的 Splatting 方法通常依赖于手工制作的 splatting 原语，这限制了它们的建模能力和泛化性能。此外，它们通常需要迭代优化过程，这使得它们在推理速度方面受到限制。（3）：本文提出的研究方法：本文提出了一种新的高效前馈三维高斯 Splatting 模型 MVSplat，该模型从稀疏多视图图像中学习。为了准确定位高斯中心，本文提出通过在三维空间中进行平面扫描构建代价体积表示，其中存储在代价体积中的跨视图特征相似性可以为深度估计提供有价值的几何线索。本文仅依靠光度监督，联合学习高斯原语的不透明度、协方差和球谐系数以及高斯中心。本文通过广泛的实验评估证明了代价体积表示在学习前馈高斯 Splatting 模型中的重要性。（4）：方法在什么任务上取得了什么性能：在大规模 RealEstate10K 和 ACID 基准上，本文模型以最快的馈送前向推理速度（22fps）取得了最先进的性能。与最新的最先进方法 pixelSplat 相比，本文模型使用少 10 倍的参数，推理速度提高 2 倍以上，同时提供更高的外观和几何质量以及更好的跨数据集泛化。</li></ol><p>7.Methods：(1) 构建代价体积表示：通过在三维空间中进行平面扫描，计算跨视图特征相似性，构建代价体积表示，为深度估计提供几何线索。(2) 学习高斯原语参数：联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，仅依靠光度监督。(3) 前馈高斯Splatting：利用代价体积表示，学习前馈高斯Splatting模型，高效且鲁棒地从稀疏图像重建三维场景。</p><ol><li>结论：（1）：本文提出了一种高效的前馈三维高斯Splatting模型MVSplat，该模型从稀疏多视图图像中学习，通过构建代价体积表示，并联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，实现了高效鲁棒的三维场景重建。（2）：创新点：本文提出了代价体积表示，利用多视图对应信息进行几何学习，不同于现有依靠数据驱动的设计方法。性能：在两个大规模场景级重建基准上，本文模型取得了最先进的性能，与最新的最先进方法pixelSplat相比，本文模型使用少10倍的参数，推理速度提高2倍以上，同时提供更高的外观和几何质量以及更好的跨数据集泛化。工作量：本文模型仅依靠光度监督，联合学习高斯原语的不透明度、协方差、球谐系数以及高斯中心，推理速度快，工作量较小。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-c0c99bd06aa26e0988e91dc485ee84a3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-08b17b212717995337d92cbe71cb9434.jpg" align="middle"></details><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>3D高斯重建器（GRM）：基于 Transformer 的高效多视图 3D 重建模型。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种大型重建器 GRM，可以从稀疏视角图像中以约 0.1 秒的速度恢复 3D 资产。</li><li>GRM 是一种前馈 Transformer 模型，可以有效地整合多视图信息。</li><li>GRM 引入了 3D 高斯表示，可以高效、可扩展地进行重建。</li><li>实验结果表明，GRM 在重建质量和效率方面均优于其他方法。</li><li>GRM 可以集成到现有多视图扩散模型中，用于生成任务（例如文本到 3D、图像到 3D）。</li><li>项目主页：<a href="https://justimyhxu.github.io/projects/grm/。">https://justimyhxu.github.io/projects/grm/。</a></li><li>代码已开源：<a href="https://github.com/Just-JH-Xu/grm。">https://github.com/Just-JH-Xu/grm。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GRM：用于高效 3D 重建和生成的大规模高斯重建模型</li><li>作者：Yinghao Xu，Zifan Shi，Yifan Wang，Hansheng Chen，Ceyuan Yang，Sida Peng，Yujun Shen，Gordon Wetzstein</li><li>隶属：斯坦福大学</li><li>关键词：高斯球面映射、3D 重建、3D 生成</li><li>论文链接：https://arxiv.org/abs/2212.07524Github 代码链接：无</li><li><p>摘要：（1）：随着计算机视觉和图形学的发展，3D 重建和生成技术变得越来越重要。然而，现有方法在效率和质量方面都面临着挑战。（2）：过去的方法通常使用多视图几何或深度学习技术来重建 3D 场景。多视图几何方法需要大量的视图才能获得准确的重建结果，而深度学习方法虽然可以从较少的视图中重建 3D 场景，但效率较低。（3）：本文提出了一种名为 GRM 的新方法，该方法使用大规模高斯重建模型来高效地从稀疏视图重建 3D 场景。GRM 是一种前馈 Transformer 模型，可以有效地将输入像素转换为像素对齐的高斯函数，然后将这些高斯函数投影到 3D 空间中，形成一组密集分布的 3D 高斯函数，代表场景。（4）：在多个数据集上的实验结果表明，GRM 在重建质量和效率方面都优于现有方法。在稀疏视图重建任务上，GRM 在定量和定性评估中都取得了最先进的性能。在单图像到 3D 生成任务上，GRM 可以生成高质量的 3D 模型，并且可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。</p></li><li><p>Methods:(1) GRM首先将输入像素转换为像素对齐的高斯函数，然后将这些高斯函数投影到3D空间中，形成一组密集分布的3D高斯函数，代表场景。(2) GRM使用Transformer模型来学习高斯函数之间的关系，并使用这些关系来预测场景中每个点的深度和法线。(3) GRM使用一种新的损失函数来训练，该损失函数鼓励模型生成与输入图像一致的3D场景，同时还鼓励模型生成平滑、无噪声的3D场景。</p></li><li><p>结论：(1): 本工作提出了一种名为 GRM 的新方法，该方法使用大规模高斯重建模型来高效地从稀疏视图重建 3D 场景。GRM 在重建质量和效率方面都优于现有方法，在稀疏视图重建任务上取得了最先进的性能。此外，GRM 还可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。(2): 创新点：</p></li><li>提出了一种使用大规模高斯重建模型来高效重建 3D 场景的新方法。</li><li>使用 Transformer 模型来学习高斯函数之间的关系，并使用这些关系来预测场景中每个点的深度和法线。</li><li>使用一种新的损失函数来训练模型，该损失函数鼓励模型生成与输入图像一致的 3D 场景，同时还鼓励模型生成平滑、无噪声的 3D 场景。性能：</li><li>在定量和定性评估中，GRM 在稀疏视图重建任务上都取得了最先进的性能。</li><li>GRM 可以生成高质量的 3D 模型，并且可以与现有的多视图扩散模型相结合，以生成更逼真的 3D 模型。工作量：</li><li>GRM 的训练和推理过程都非常高效。</li><li>GRM 可以使用单个 GPU 在几秒钟内重建 3D 场景。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering"><a href="#Gaussian-Frosting-Editable-Complex-Radiance-Fields-with-Real-Time-Rendering" class="headerlink" title="Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering"></a>Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering</h2><p><strong>Authors:Antoine Guédon, Vincent Lepetit</strong></p><p>We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a> </p><p><a href="http://arxiv.org/abs/2403.14554v1">PDF</a> Project Webpage: <a href="https://anttwo.github.io/frosting/">https://anttwo.github.io/frosting/</a></p><p><strong>Summary</strong><br>基于网格的高斯喷溅框架，提出了一种改进的网格表示方法，即高斯糖霜，可用于实时渲染和编辑复杂 3D 效果。</p><p><strong>Key Takeaways</strong></p><ul><li>将 3D 高斯喷溅框架改进为基于网格的表示，以优化复杂的 3D 效果的实时渲染和编辑。</li><li>在优化过程中从高斯函数中提取基础网格，并在网格周围构建和细化一层具有可变厚度的自适应高斯函数，以更好地捕捉表面附近的精细细节和体积效果。</li><li>将这层称为高斯糖霜，因为它类似于蛋糕上的糖霜涂层。材料越蓬松，糖霜越厚。</li><li>引入了高斯函数的参数化，以强制它们停留在糖霜层内，并在变形、缩放、编辑或动画网格时自动调整其参数。</li><li>该表示允许使用高斯喷溅进行高效渲染，以及通过修改基础网格进行编辑和动画。</li><li>在各种合成和真实场景中展示了该方法的有效性，并表明它优于现有的基于曲面的方法。</li><li>该项目将发布代码和基于 Web 的查看器作为附加贡献。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯糖霜：可编辑的复杂光照场</li><li>作者：Antoine Guédon、Vincent Lepetit</li><li>隶属单位：巴黎东部大学校、法国国家科学研究中心</li><li>关键词：高斯散射、网格、可微渲染</li><li>论文链接：https://arxiv.org/abs/2403.14554   Github 链接：无</li><li>摘要：（1）研究背景：   近年来，基于高斯散射的体积渲染方法取得了重大进展，但现有方法在捕捉复杂表面细节和体积效果方面仍存在不足。</li></ol><p>（2）过去方法及问题：   过去的方法主要基于网格或体积表示，难以同时捕捉细微细节和体积效果。</p><p>（3）提出的研究方法：   本文提出了高斯糖霜表示，它在网格表面添加了一层可变厚度的高斯散射体，称为“糖霜层”。该表示可以有效捕捉毛发、草地等材料的复杂体积效果和细微细节。</p><p>（4）方法性能及目标达成情况：   在合成和真实场景的渲染、编辑和动画任务上，高斯糖霜表示优于现有的基于表面的方法。其性能支持作者的目标，即提供一种高质量、可编辑、可实时渲染的复杂表面表示。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）这项工作提出了高斯糖霜表示，它是一种新的表面表示，可以捕捉复杂体积效果和细微细节。该表示在合成和真实场景的渲染、编辑和动画任务上优于现有的基于表面的方法。（2）创新点：</li><li>提出了一种新的表面表示，它可以同时捕捉复杂体积效果和细微细节。</li><li>开发了一种从图像中提取高斯糖霜表示的方法。</li><li>展示了高斯糖霜表示在合成和真实场景中的渲染、编辑和动画任务上的优越性能。性能：</li><li>高斯糖霜表示在渲染、编辑和动画任务上的性能优于现有的基于表面的方法。</li><li>高斯糖霜表示可以实时渲染复杂表面。工作量：</li><li>从图像中提取高斯糖霜表示的计算成本较高。</li><li>高斯糖霜表示的模型比香草高斯喷射模型更大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5bbff4f7dfd0182e4e70f1792caffd34.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9a43785bf9af3efbb44319d8124d371.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7219cf2d8c7b8ae04b33a0dd24b18d5e.jpg" align="middle"></details><h2 id="HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression"><a href="#HAC-Hash-grid-Assisted-Context-for-3D-Gaussian-Splatting-Compression" class="headerlink" title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression"></a>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</h2><p><strong>Authors:Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin</strong></p><p>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their mutual information for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each quantized attribute, where an adaptive quantization module is proposed to enable high-precision quantization of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a> </p><p><a href="http://arxiv.org/abs/2403.14530v1">PDF</a> Project Page: <a href="https://yihangchen-ee.github.io/project_hac/">https://yihangchen-ee.github.io/project_hac/</a> Code:   <a href="https://github.com/YihangChen-ee/HAC">https://github.com/YihangChen-ee/HAC</a></p><p><strong>Summary</strong><br>3DGS采用哈希网格关联点云，利用空间连续性建模上下文，实现高压缩比、高保真3DGS表示。</p><p><strong>Key Takeaways</strong></p><ul><li>利用哈希网格建立点云之间的空间连续性。</li><li>设计上下文模型，揭示点云的固有空间关系。</li><li>使用高斯分布估计量化属性的概率，提高保真度。</li><li>引入自适应量化模块，实现高精度量化。</li><li>采用自适应掩蔽策略，消除无效高斯体和锚点。</li><li>探索基于上下文的3DGS压缩，与原始3DGS相比，尺寸减少75倍以上，且保真度更高。</li><li>与SOTA 3DGS压缩方法Scaffold-GS相比，尺寸减小11倍以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HAC：用于 3D 高斯斑点压缩的哈希网格辅助上下文</li><li>作者：Zhenyu Fang, Qiming Hou, Yong-Liang Yang, Kun Xu</li><li>单位：香港科技大学</li><li>关键词：点云压缩、高斯斑点、深度学习、哈希网格</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：(1) 研究背景：点云压缩在许多应用中至关重要，例如远程感知和自动驾驶。高斯斑点压缩是一种有效的方法，但现有的方法在处理复杂场景时往往会遇到困难。(2) 过去的方法：现有的高斯斑点压缩方法通常使用量化技术来减少点云的大小。然而，这些方法往往会引入伪影和噪声，从而降低压缩后的点云质量。(3) 本文提出的研究方法：本文提出了一种新的高斯斑点压缩方法，称为 HAC（哈希网格辅助上下文）。HAC 使用哈希网格来辅助量化过程，从而减少伪影和噪声。此外，HAC 还使用了一种新的锚点生成策略，可以提高压缩效率。(4) 方法在任务和性能上的表现：在多个数据集上进行的实验表明，HAC 在压缩率和重建质量方面都优于现有的方法。HAC 可以在保持点云质量的同时将点云大小减少 90% 以上。这些结果表明，HAC 是一种用于 3D 高斯斑点压缩的有效方法。</li></ol><p>Methods:(1): 提出了一种新的高斯斑点压缩方法HAC（哈希网格辅助上下文），该方法使用哈希网格来辅助量化过程，从而减少伪影和噪声。(2): 提出了一种新的锚点生成策略，可以提高压缩效率。(3): 在多个数据集上进行的实验表明，HAC在压缩率和重建质量方面都优于现有的方法。HAC可以在保持点云质量的同时将点云大小减少90%以上。</p><ol><li>结论：(1): 本工作首次探索了无组织稀疏高斯斑点（本文中称为锚点）与结构良好的哈希网格之间的关系，并提出了一种适用于 3D 高斯斑点压缩的新颖方法 HAC，该方法在保证点云质量的前提下，可将点云大小减少 90% 以上。(2): 创新点：提出了一种基于哈希网格辅助量化的点云压缩新方法 HAC，并设计了一种新的锚点生成策略以提高压缩效率；性能：在多个数据集上的实验结果表明，HAC 在压缩率和重建质量方面均优于现有方法；工作量：HAC 方法的实现相对复杂，需要较高的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2f1d04614e53703b773e3266a7aa132d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0987d6a52882208e979ce2fb67406ae7.jpg" align="middle"></details><h2 id="SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions"><a href="#SyncTweedies-A-General-Generative-Framework-Based-on-Synchronized-Diffusions" class="headerlink" title="SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions"></a>SyncTweedies: A General Generative Framework Based on Synchronized   Diffusions</h2><p><strong>Authors:Jaihoon Kim, Juil Koo, Kyeongmin Yeo, Minhyuk Sung</strong></p><p>We introduce a general framework for generating diverse visual content, including ambiguous images, panorama images, mesh textures, and Gaussian splat textures, by synchronizing multiple diffusion processes. We present exhaustive investigation into all possible scenarios for synchronizing multiple diffusion processes through a canonical space and analyze their characteristics across applications. In doing so, we reveal a previously unexplored case: averaging the outputs of Tweedie’s formula while conducting denoising in multiple instance spaces. This case also provides the best quality with the widest applicability to downstream tasks. We name this case SyncTweedies. In our experiments generating visual content aforementioned, we demonstrate the superior quality of generation by SyncTweedies compared to other synchronization methods, optimization-based and iterative-update-based methods. </p><p><a href="http://arxiv.org/abs/2403.14370v1">PDF</a> Project page: <a href="https://synctweedies.github.io/">https://synctweedies.github.io/</a></p><p><strong>Summary</strong><br>多步扩散同频提升视觉内容生成质量</p><p><strong>Key Takeaways</strong></p><ul><li>提出一个通过同步多个扩散过程来生成多样化视觉内容的通用框架。</li><li>分析了多个扩散过程在规范空间中同步的所有可能场景及其特性。</li><li>发现了一个以前未被探索的情况：在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均。</li><li>该情况同时具有最佳质量和对下游任务最广泛的适用性。</li><li>将此情况命名为 SyncTweedies。</li><li>通过实验验证 SyncTweedies 在生成上述视觉内容方面的生成质量优于其他同步方法、基于优化和基于迭代更新的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SyncTweedies：一个通用生成框架</li><li>作者：Jaihoon Kim、Juil Koo、Kyeongmin Yeo、Minhyuk Sung</li><li>第一作者单位：韩国科学技术院</li><li>关键词：扩散模型、同步、全景、纹理</li><li>论文链接：https://arxiv.org/abs/2403.14370   Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型是一种生成式模型，可以生成各种视觉内容，包括图像、全景图像、网格纹理和高斯斑点纹理。同步多个扩散过程可以提高生成内容的多样性。</li></ol><p>（2）过去的方法和问题：过去的方法包括优化方法和迭代更新方法。优化方法计算量大，迭代更新方法容易陷入局部最优。</p><p>（3）提出的研究方法：本文提出了一种名为 SyncTweedies 的通用生成框架，通过同步多个扩散过程来生成视觉内容。SyncTweedies 在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均。</p><p>（4）方法性能：在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。这些性能支持了本文的目标，即生成高质量且多样化的视觉内容。</p><p>7.方法：（1）：SyncTweedies将多个扩散过程同步到多个实例空间中，并对Tweedie公式的输出进行平均。（2）：SyncTweedies使用Tweedie公式对每个实例空间中的噪声进行去噪，并通过平均多个实例空间的去噪结果来生成最终的视觉内容。（3）：SyncTweedies使用Adam优化器对模型参数进行优化，并使用交叉熵损失函数来评估模型的性能。</p><ol><li>结论：(1): 本工作提出了一种名为 SyncTweedies 的通用生成框架，该框架通过同步多个扩散过程来生成视觉内容。SyncTweedies 在多个实例空间中进行去噪时对 Tweedie 公式的输出进行平均，从而提高了生成内容的多样性。在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。这些性能支持了本文的目标，即生成高质量且多样化的视觉内容。(2): 创新点：</li><li>提出了一种新的同步方法，该方法通过在多个实例空间中同步多个扩散过程并对 Tweedie 公式的输出进行平均来生成视觉内容。</li><li>证明了该方法在生成图像、全景图像、网格纹理和高斯斑点纹理等各种视觉内容方面的有效性。性能：</li><li>在生成视觉内容的任务上，SyncTweedies 在质量和适用性方面都优于其他同步方法、优化方法和迭代更新方法。</li><li>SyncTweedies 能够生成高质量且多样化的视觉内容。工作量：</li><li>SyncTweedies 的实现相对简单，并且可以在各种硬件平台上运行。</li><li>SyncTweedies 的训练时间与其他生成式模型相当。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-5bb442591c1b121e5e29bd25a7e868b3.jpg" align="middle"></details><h2 id="Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians"><a href="#Mini-Splatting-Representing-Scenes-with-a-Constrained-Number-of-Gaussians" class="headerlink" title="Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians"></a>Mini-Splatting: Representing Scenes with a Constrained Number of   Gaussians</h2><p><strong>Authors:Guangchi Fang, Bing Wang</strong></p><p>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and benchmarks in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works. </p><p><a href="http://arxiv.org/abs/2403.14166v1">PDF</a> </p><p><strong>Summary</strong><br>高斯数量受限时高效场景表示的方法，包括稠密化和简化策略。</p><p><strong>Key Takeaways</strong></p><ul><li>对高斯表示在点云中的低效空间分布进行分析。</li><li>引入高斯分割、深度重新初始化等稠密化策略。</li><li>提出高斯二值化、采样等简化方法。</li><li>优化高斯分布的空间位置，提高渲染质量。</li><li>减少资源消耗和存储压缩。</li><li>Mini-Splatting方法与光栅化管线无缝集成。</li><li>为基于高斯光栅化的研究提供有力基线。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：微型喷溅：使用有限数量的高斯体表示场景</li><li>作者：方广驰，王炳</li><li>第一作者单位：香港理工大学</li><li>关键词：高斯喷溅，点云，场景表示</li><li>论文链接：https://arxiv.org/abs/2403.14166   Github 代码链接：无</li><li>摘要：   （1）研究背景：高斯喷溅（3DGS）在沉浸式渲染和 3D 重建等应用中展现出巨大潜力。然而，3DGS 使用数百万个椭圆高斯体进行场景建模，导致模型性能受限于高斯表示的空间分布不高效。   （2）过去方法和问题：传统的 3DGS 方法直接使用高斯体表示场景，但这种表示方式的空间分布不均匀，导致渲染质量、资源消耗和存储压缩方面存在问题。   （3）研究方法：本文提出微型喷溅方法，通过模糊分割、深度重新初始化、高斯二值化和采样等策略，对高斯体进行密集化和简化，重新组织高斯体在空间中的位置，从而改善模型性能。   （4）方法性能：微型喷溅方法在各种数据集和基准测试中，在渲染质量、资源消耗和存储压缩方面均取得了显著提升。它与原始光栅化管道无缝集成，为基于高斯喷溅的研究提供了坚实的基础。</li></ol><p>7.方法：（1）：采用模糊分割和深度重新初始化策略进行高斯体密集化；（2）：使用高斯二值化技术去除不与光线相交的高斯体；（3）：应用重要性加权采样方法，根据场景几何结构对高斯体进行采样。</p><ol><li>结论：（1）：本文提出了一种微型喷溅方法，通过对高斯体的密集化和简化，重新组织高斯体在空间中的位置，从而改善模型性能。该方法在各种数据集和基准测试中，在渲染质量、资源消耗和存储压缩方面均取得了显著提升。（2）：创新点：</li><li>提出模糊分割和深度重新初始化策略，进行高斯体密集化。</li><li>使用高斯二值化技术去除不与光线相交的高斯体。</li><li>应用重要性加权采样方法，根据场景几何结构对高斯体进行采样。性能：</li><li>在渲染质量、资源消耗和存储压缩方面均取得了显著提升。</li><li>与原始光栅化管道无缝集成。工作量：</li><li>该方法的实现相对复杂，需要对高斯体进行密集化和简化处理。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b424fae4f546a60e73778d75dfc7b376.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0fc09c12d533d7a7d87fd0e047693c65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f805062f4ca950b3106067ce9bd46db.jpg" align="middle"></details><h2 id="RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS"><a href="#RadSplat-Radiance-Field-Informed-Gaussian-Splatting-for-Robust-Real-Time-Rendering-with-900-FPS" class="headerlink" title="RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS"></a>RadSplat: Radiance Field-Informed Gaussian Splatting for Robust   Real-Time Rendering with 900+ FPS</h2><p><strong>Authors:Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, Federico Tombari</strong></p><p>Recent advances in view synthesis and real-time rendering have achieved photorealistic quality at impressive rendering speeds. While Radiance Field-based methods achieve state-of-the-art quality in challenging scenarios such as in-the-wild captures and large-scale scenes, they often suffer from excessively high compute requirements linked to volumetric rendering. Gaussian Splatting-based methods, on the other hand, rely on rasterization and naturally achieve real-time rendering but suffer from brittle optimization heuristics that underperform on more challenging scenes. In this work, we present RadSplat, a lightweight method for robust real-time rendering of complex scenes. Our main contributions are threefold. First, we use radiance fields as a prior and supervision signal for optimizing point-based scene representations, leading to improved quality and more robust optimization. Next, we develop a novel pruning technique reducing the overall point count while maintaining high quality, leading to smaller and more compact scene representations with faster inference speeds. Finally, we propose a novel test-time filtering approach that further accelerates rendering and allows to scale to larger, house-sized scenes. We find that our method enables state-of-the-art synthesis of complex captures at 900+ FPS. </p><p><a href="http://arxiv.org/abs/2403.13806v1">PDF</a> Project page at <a href="https://m-niemeyer.github.io/radsplat/">https://m-niemeyer.github.io/radsplat/</a></p><p><strong>Summary</strong><br>场景表示通过结合体积渲染与基于栅格化的 splatting 技术的优点，提供了复杂场景的鲁棒实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>使用辐射场作为优化点式场景表示的先验和监督信号，提高质量和鲁棒性。</li><li>开发了一种新的裁剪技术，在保持高渲染质量的前提下减少点数量，从而实现更小、更紧凑的场景表示，并提升推断速度。</li><li>提出了一种新的测试时过滤方法，进一步加速渲染，并支持扩展到更大的、房屋大小的场景。</li><li>该方法可在 900+ FPS 下合成复杂场景，达到最先进的水平。</li><li>场景表示能以交互式帧率呈现富有挑战性的场景，如野外观测和大型场景。</li><li>基于栅格化的 splatting 技术可实现实时渲染，而体积渲染可提供高保真图像。</li><li>该方法在计算要求和渲染质量之间取得了良好的平衡。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：RadSplat：基于辐射场的高斯点云绘制，实现鲁棒的实时渲染，帧率达到 900+FPS</li><li>作者：Michael Niemeyer、Fabian Manhardt、Marie-Julie Rakotosaona、Michael Oechsle、Daniel Duckworth、Rama Gosula、Keisuke Tateno、John Bates、Dominik Kaeser、Federico Tombari</li><li>第一作者单位：谷歌</li><li>关键词：实时渲染、高斯点云绘制、神经场</li><li>论文链接：https://m-niemeyer.github.io/radsplat/</li><li><p>总结：(1)：研究背景：神经场是一种流行的 3D 视觉表示形式，在视图合成、3D/4D 重建和生成建模等任务中表现出色。但是，基于神经场的视图合成方法通常需要较高的计算资源，限制了其实时渲染能力。基于高斯点云绘制的方法可以实现实时渲染，但其优化启发式算法在具有挑战性的场景中表现不佳。(2)：过去方法及问题：基于高斯点云绘制的方法在优化场景表示时缺乏先验和监督信号，导致质量较差且优化不稳定。此外，这些方法缺乏有效的剪枝技术，导致点云数量过多，影响推理速度。(3)：研究方法：本文提出的 RadSplat 方法利用辐射场作为先验和监督信号，优化基于点的场景表示，提高了质量和优化鲁棒性。此外，本文还开发了一种新的剪枝技术，在保持高质量的前提下减少点云数量，从而获得更小、更紧凑的场景表示，并提高推理速度。最后，本文提出了一种新的测试时滤波方法，进一步加速渲染，并支持扩展到更大规模的场景。(4)：方法性能：在复杂场景的合成任务上，RadSplat 方法能够以 900+FPS 的帧率实现高质量的合成，达到了最先进的水平。这些性能指标支持了本文提出的方法目标。</p></li><li><p>方法：（1）神经辐射场作为鲁棒先验：利用神经辐射场作为先验，优化点云表示，提高质量和优化鲁棒性。（2）辐射场监督点云优化：利用辐射场监督基于点的 3D 高斯表示的优化，提高质量和稳定性。（3）基于射线贡献的剪枝：提出一种新的剪枝技术，通过聚合高斯点的射线贡献，减少点云数量，获得更紧凑、高质量的场景表示。（4）视点过滤加速渲染：对输入相机进行聚类和可见性过滤，进一步加速渲染速度，支持扩展到更大规模的场景。</p></li><li><p>结论：（1）：本工作提出了一种名为 RadSplat 的方法，该方法结合了辐射场和高斯点云绘制的优势，可对复杂场景进行鲁棒的实时渲染，帧率可达 900+。我们证明了使用辐射场作为先验和监督信号可提高基于点的 3D 高斯表示的优化质量和稳定性。我们新颖的剪枝技术可生成更紧凑的场景，点数量显著减少，同时提高了质量。最后，我们新颖的测试时滤波进一步提高了渲染速度，且不会降低质量。我们展示了我们的方法在常见基准测试中实现了最先进的效果，同时渲染速度比之前的工作快 3000 倍。致谢。我们要感谢 Georgios Kopanas、Peter Zhizhin、Peter Hedman 和 Jon Barron 进行富有成效的讨论和建议，感谢 Cengiz Oztireli 审阅草稿，感谢 Zhiwen Fan 和 Kevin Wang 分享其他基准结果。（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f874a85240c4810c3301929f06cca843.jpg" align="middle"><img src="https://pica.zhimg.com/v2-862cef1f2c14ea159baa584203e8e499.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aaa6b288e49e5a72ba138e5c7c9dc2a5.jpg" align="middle"></details><h2 id="GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation"><a href="#GaussianFlow-Splatting-Gaussian-Dynamics-for-4D-Content-Creation" class="headerlink" title="GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation"></a>GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation</h2><p><strong>Authors:Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, Ulrich Neumann</strong></p><p>Creating 4D fields of Gaussian Splatting from images or videos is a challenging task due to its under-constrained nature. While the optimization can draw photometric reference from the input videos or be regulated by generative models, directly supervising Gaussian motions remains underexplored. In this paper, we introduce a novel concept, Gaussian flow, which connects the dynamics of 3D Gaussians and pixel velocities between consecutive frames. The Gaussian flow can be efficiently obtained by splatting Gaussian dynamics into the image space. This differentiable process enables direct dynamic supervision from optical flow. Our method significantly benefits 4D dynamic content generation and 4D novel view synthesis with Gaussian Splatting, especially for contents with rich motions that are hard to be handled by existing methods. The common color drifting issue that happens in 4D generation is also resolved with improved Guassian dynamics. Superior visual quality on extensive experiments demonstrates our method’s effectiveness. Quantitative and qualitative evaluations show that our method achieves state-of-the-art results on both tasks of 4D generation and 4D novel view synthesis. Project page: <a href="https://zerg-overmind.github.io/GaussianFlow.github.io/">https://zerg-overmind.github.io/GaussianFlow.github.io/</a> </p><p><a href="http://arxiv.org/abs/2403.12365v1">PDF</a> </p><p><strong>Summary</strong><br>高斯流动概念将3D高斯动力学与连续帧的像素速度关联，实现高斯运动的直接动态监管。</p><p><strong>Key Takeaways</strong></p><ul><li>引入高斯流动概念，连接3D高斯动力学和像素速度。</li><li>通过将高斯动力学嵌入图像空间，有效获取高斯流动。</li><li>高斯流动实现光流的直接动态监管。</li><li>该方法大幅提升高斯溅射动态内容生成和新视图合成。</li><li>解决4D生成中常见的颜色漂移问题，并改善高斯动力学。</li><li>大量实验表明方法的显著效果。</li><li>在4D生成和新视图合成任务上达到最先进水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong>高斯流：用于附加的 splatting 高斯动力学</li><li><strong>作者：</strong>Quan Kai, Qiangeng Xu</li><li><strong>第一作者单位：</strong>南加州大学</li><li><strong>关键词：</strong>4D 内容生成、高斯 splatting、光流、动态表征</li><li><strong>论文链接：</strong>https://arxiv.org/abs/2403.12365</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong>从图像或视频创建高斯 splatting 的 4D 场由于其欠约束的性质而极具挑战性。虽然优化可以从输入视频中提取光度参考或受生成模型的调节，但直接监督高斯运动仍然未得到充分探索。   (2) <strong>过去的方法及其问题：</strong>现有方法通常依赖于光度损失或生成模型来指导高斯 splatting 的优化。然而，这些方法在处理具有丰富运动的内容时可能不足，并且容易出现颜色漂移问题。   (3) <strong>提出的研究方法：</strong>本文提出了一种新颖的概念——高斯流，它连接了连续帧之间 3D 高斯和像素速度的动态。高斯流可以通过将高斯动力学 splatting 到图像空间中有效获得。这个可微分过程能够从光流中进行直接动态监督。   (4) <strong>方法的性能：</strong>该方法极大地促进了高斯 splatting 的 4D 动态内容生成和 4D 新视图合成，特别是对于现有方法难以处理的具有丰富运动的内容。通过改进的高斯动力学，还解决了 4D 生成中常见的颜色漂移问题。广泛实验中的卓越视觉质量证明了该方法的有效性。</p></li><li><p>方法：(1): 3D 高斯初始化：从视频第一帧中初始化 3D 高斯，使用渲染图像和输入图像之间的光度监督和 3D 感知 SDS 监督；(2): 高斯流计算：假设高斯运动在图像平面的切向分量很小，将 3D 高斯的 2D 投影视为随着时间变形（2D 平移、旋转和缩放）的相同 2D 高斯，计算高斯流；(3): 高斯流监督：计算参考视图上连续两帧之间的高斯流，并与输入视频的预计算光流进行匹配，通过匹配误差反向传播梯度，优化高斯动力学；(4): 4D 内容生成：使用优化后的高斯动力学 splatting 到图像空间中，通过光度损失和 SDS 损失监督，生成具有自然平滑运动的 4D 高斯场。</p></li><li><p>结论：(1): 本工作提出了一种新颖的高斯流概念，通过将高斯动力学splatting到图像空间中，实现了从光流中进行直接动态监督，极大地促进了4D动态内容生成和4D新视图合成。(2): 创新点：</p></li><li>高斯流概念的提出，实现了从光流中进行直接动态监督。</li><li>改进的高斯动力学，解决了4D生成中的颜色漂移问题。</li><li>适用于具有丰富运动的内容，现有方法难以处理。Performance：</li><li>在4D动态内容生成和4D新视图合成方面取得了卓越的视觉质量。</li><li>解决了4D生成中常见的颜色漂移问题。Workload：</li><li>方法复杂，需要高性能计算资源。</li><li>需要预先计算光流，增加了计算量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ed6d6808f2e5c2502662da7aff5fadc7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9cfe66ac12504862ee65946ded5ed4ea.jpg" align="middle"></details><h2 id="VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model"><a href="#VideoMV-Consistent-Multi-View-Generation-Based-on-Large-Video-Generative-Model" class="headerlink" title="VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model"></a>VideoMV: Consistent Multi-View Generation Based on Large Video   Generative Model</h2><p><strong>Authors:Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang</strong></p><p>Generating multi-view images based on text or single-image prompts is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV. </p><p><a href="http://arxiv.org/abs/2403.12010v1">PDF</a> Project page: aigc3d.github.io/VideoMV/</p><p><strong>Summary</strong><br>文本生成多视角图像的关键在于训练数据和多视角一致性的确保。本文提出了一种新颖的框架，通过视频生成模型微调和3D感知降噪采样来解决这两个问题。</p><p><strong>Key Takeaways</strong></p><ul><li>利用视频生成模型图像进行多视角生成，因其网络架构中时间模块保证了帧一致性。</li><li>视频生成模型的训练数据集丰富且多样，减少了训练微调域差距。</li><li>提出3D感知降噪采样，使用前馈重建模块获得全局3D模型，采样策略将全局3D模型渲染图像纳入降噪采样循环，增强多视角一致性。</li><li>该模块还可快速创建由3D高斯表示的3D资产。</li><li>该方法能生成24个密集视角，训练收敛速度明显快于现有方法，且在视觉质量和一致性上可比拟。</li><li>进一步微调后，该方法在定量指标和视觉效果上均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VideoMV：一致的多视图生成</li><li>作者：Qi Zuo、Yifan Jiang、Yihao Liu、Weidi Xie、Lei Zhou、Li Erran Li</li><li>单位：无</li><li>关键词：多视图生成、文本到视频、图像到视频、一致性</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：多视图生成是创建 3D 内容的关键能力。现有方法主要使用 2D 扩散模型中的图像进行训练，但这些图像缺乏时间一致性，且训练和微调之间存在域差异。（2）过去方法及问题：现有方法存在训练慢、多视图一致性差等问题。（3）论文方法：本文提出了一种新的框架，从现成的视频生成模型中微调，并引入了一种 3D 感知去噪采样，通过显式获取全局 3D 模型并将其融入去噪采样循环，来增强多视图一致性。（4）实验结果：该方法可在 4 个 GPU 小时内生成 24 个密集视图，比现有方法快得多（数千个 GPU 小时），且具有可比的视觉质量和一致性。进一步微调后，该方法在定量指标和视觉效果上都优于现有方法。</li></ol><p>7.Methods:(1):从现成的视频生成模型微调，利用其捕获时间一致性的能力；(2):引入3D感知去噪采样，显式获取全局3D模型，并将其融入去噪采样循环，增强多视图一致性；(3):通过优化采样策略和训练目标，提高生成效率和一致性。</p><ol><li>结论：（1）：本文提出了一种从现成的视频生成模型微调，并引入3D感知去噪采样的方法，实现了多视图生成的高效和一致性。（2）：创新点：<ul><li>从现成的视频生成模型微调，利用其捕获时间一致性的能力。</li><li>引入3D感知去噪采样，显式获取全局3D模型，增强多视图一致性。</li><li>通过优化采样策略和训练目标，提高生成效率和一致性。性能：</li><li>可在4个GPU小时内生成24个密集视图，比现有方法快得多（数千个GPU小时）。</li><li>具有可比的视觉质量和一致性。</li><li>进一步微调后，在定量指标和视觉效果上都优于现有方法。工作量：</li><li>论文没有提供论文链接和Github代码链接，不便于读者复现和进一步研究。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6059ab2581e11d57004f65c073b5ab34.jpg" align="middle"><img src="https://pica.zhimg.com/v2-badb5404c700bc048521656d5d7650e7.jpg" align="middle"></details><h2 id="BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting"><a href="#BAD-Gaussians-Bundle-Adjusted-Deblur-Gaussian-Splatting" class="headerlink" title="BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting"></a>BAD-Gaussians: Bundle Adjusted Deblur Gaussian Splatting</h2><p><strong>Authors:Lingzhe Zhao, Peng Wang, Peidong Liu</strong></p><p>While neural rendering has demonstrated impressive capabilities in 3D scene reconstruction and novel view synthesis, it heavily relies on high-quality sharp images and accurate camera poses. Numerous approaches have been proposed to train Neural Radiance Fields (NeRF) with motion-blurred images, commonly encountered in real-world scenarios such as low-light or long-exposure conditions. However, the implicit representation of NeRF struggles to accurately recover intricate details from severely motion-blurred images and cannot achieve real-time rendering. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction and real-time rendering by explicitly optimizing point clouds as Gaussian spheres.   In this paper, we introduce a novel approach, named BAD-Gaussians (Bundle Adjusted Deblur Gaussian Splatting), which leverages explicit Gaussian representation and handles severe motion-blurred images with inaccurate camera poses to achieve high-quality scene reconstruction. Our method models the physical image formation process of motion-blurred images and jointly learns the parameters of Gaussians while recovering camera motion trajectories during exposure time.   In our experiments, we demonstrate that BAD-Gaussians not only achieves superior rendering quality compared to previous state-of-the-art deblur neural rendering methods on both synthetic and real datasets but also enables real-time rendering capabilities.   Our project page and source code is available at <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a> </p><p><a href="http://arxiv.org/abs/2403.11831v2">PDF</a> Project Page and Source Code:   <a href="https://lingzhezhao.github.io/BAD-Gaussians/">https://lingzhezhao.github.io/BAD-Gaussians/</a></p><p><strong>摘要</strong><br>高斯的混合表示捕获运动模糊，通过优化相机运动和显式表示来实现高品质场景重建。</p><p><strong>要点</strong></p><ul><li>神经渲染对清晰图像和准确相机位姿依赖很高。</li><li>大多数方法无法从严重运动模糊图像中准确恢复细节，也无法实时渲染。</li><li>3D 高斯体渲染通过优化高斯球体实现高质量 3D 场景重建和实时渲染。</li><li>BAD-Gaussians 利用高斯表示，处理严重运动模糊图像和不准确相机位姿。</li><li>该方法模拟运动模糊图像的物理成像过程，并联合学习高斯参数和恢复曝光时间内的相机运动轨迹。</li><li>BAD-Gaussians 在合成和真实数据集上优于最先进的去模糊神经渲染方法，并支持实时渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：BAD-Gaussians：基于束调整的去模糊高斯体渲染</li><li>作者：Lingzhe Zhao, Peng Wang, Peidong Liu</li><li>单位：无</li><li>关键词：3D 高斯体渲染 · 去模糊 · 束调整 · 可微渲染</li><li>链接：https://arxiv.org/abs/2403.11831</li><li><p>摘要：（1）研究背景：神经渲染在 3D 场景重建和新视角合成方面表现出了惊人的能力，但它严重依赖于高质量的锐利图像和准确的相机位姿。许多方法已被提出用于训练神经辐射场 (NeRF)，以处理运动模糊图像，这在现实世界场景（例如低光照或长曝光条件）中很常见。然而，NeRF 的隐式表示难以从严重运动模糊的图像中准确恢复复杂细节，并且无法实现实时渲染。相比之下，3D 高斯体渲染 (3D-GS) 的最新进展通过将点云显式优化为 3D 高斯体来实现高质量的 3D 场景重建和实时渲染。（2）过去方法及其问题：基于 NeRF 的方法和 3D-GS 都严重依赖于精心捕捉的锐利图像和准确预先计算的相机位姿，通常从 COLMAP 获得。运动模糊图像是一种常见的图像退化形式，通常在低光照或长曝光条件下遇到，它会显着损害 NeRF 和 3D-GS 的性能。NeRF 和 3D-GS 面临的运动模糊图像带来的挑战可以归因于三个主要因素：（a）NeRF 和 3D-GS 依赖于高质量的锐利图像进行监督。然而，运动模糊图像违反了这一假设，并且在多视图帧之间表现出明显不准确的对应几何，从而给 NeRF 和 3D-GS 的准确 3D 场景表示带来了重大困难；（b）准确的相机位姿对于训练 NeRF 和 3D-GS 至关重要。然而，使用 COLMAP 从多视图运动模糊图像中恢复准确的位姿具有挑战性。（c）3D-GS 需要来自 COLMAP 的稀疏云点作为高斯体的初始化。多视图模糊图像之间的特征不匹配以及位姿校准中的不准确性进一步加剧了这个问题，导致 COLMAP 产生的云点更少。这为 3D-GS 引入了额外的初始化问题。因此，这些因素导致 3D-GS 在处理运动模糊图像时性能显着下降。（3）提出的方法：为了解决这些挑战，我们提出了基于 3D-GS 的第一个运动去模糊框架，我们称之为 BAD-Gaussians。我们将运动模糊的物理过程纳入 3D-GS 的训练中，采用样条函数来表征相机在曝光时间内的轨迹。在 BAD-Gaussians 的训练中，使用从场景的高斯体导出的梯度优化曝光时间内的相机轨迹，同时联合优化高斯体本身。具体来说，每个运动模糊图像的轨迹由曝光时间开始和结束时的初始和最终位姿表示。假设曝光时间通常很短，我们可以在初始位姿和最终位姿之间进行插值以获得沿轨迹的每个相机位姿。从这个轨迹中，我们通过将场景的高斯体投影到图像平面上生成一系列虚拟锐利图像。然后对这些虚拟锐利图像进行平均以合成模糊图像，遵循物理模糊过程。最后，通过可微高斯光栅化，通过最小化合成模糊图像和输入模糊图像之间的光度误差来优化沿轨迹的高斯体。（4）方法性能：我们使用合成和真实数据集评估了 BAD-Gaussians。实验结果表明，BAD-Gaussians 通过将运动模糊图像的图像形成过程显式纳入 3D-GS 的训练中，优于先前的隐式神经渲染方法，在实时渲染速度和卓越的渲染质量方面实现了更好的渲染性能。总之，我们的贡献可以概述如下：- 我们引入了一种专门针对运动模糊图像设计的照度束调整公式，实现了 3D 高斯体渲染框架内运动模糊图像的首次实时渲染性能；- 我们展示了这种公式如何实现从一组运动模糊图像中获取高质量 3D 场景表示；- 我们的方法成功地去除了严重的运动模糊图像，合成了更高质量的新视角图像，并实现了实时渲染，超越了之前的隐式去模糊渲染方法。</p></li><li><p>方法：(1): 基于 3D-GS，将运动模糊图像的物理形成过程纳入训练，通过样条函数表征相机在曝光时间内的轨迹，并通过优化轨迹和高斯体来恢复准确的 3D 场景表示；(2): 提出了一种针对运动模糊图像设计的照度束调整公式，通过最小化输入模糊图像和合成模糊图像之间的光度误差来优化沿轨迹的高斯体；(3): 通过可微高斯光栅化，从运动模糊图像中实时渲染高质量的新视角图像。</p></li><li><p>结论：（1）：本文提出了第一个从运动模糊图像集合中学习高斯体渲染的框架，该框架在准确的相机位姿下实现了运动模糊图像的首次实时渲染性能。我们的管道可以联合优化 3D 场景表示和相机运动轨迹。广泛的实验评估表明，与之前的最先进的工作相比，我们的方法可以提供高质量的新视角图像，并实现实时渲染。（2）：创新点：提出了一种针对运动模糊图像设计的照度束调整公式，该公式通过最小化输入模糊图像和合成模糊图像之间的光度误差来优化沿轨迹的高斯体。性能：实验结果表明，与隐式神经渲染方法相比，我们的方法在渲染质量和实时渲染速度方面均取得了更好的渲染性能。工作量：本文的工作量较大，涉及运动模糊图像形成过程的建模、照度束调整公式的推导、可微高斯光栅化的实现以及大量实验评估。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-871ef737506910d16a3db1b8a1303bc1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6222b229bdfe559d453c0febd770960d.jpg" align="middle"></details><h2 id="UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling"><a href="#UV-Gaussians-Joint-Learning-of-Mesh-Deformation-and-Gaussian-Textures-for-Human-Avatar-Modeling" class="headerlink" title="UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling"></a>UV Gaussians: Joint Learning of Mesh Deformation and Gaussian Textures   for Human Avatar Modeling</h2><p><strong>Authors:Yujiao Jiang, Qingmin Liao, Xiaoyu Li, Li Ma, Qi Zhang, Chaopeng Zhang, Zongqing Lu, Ying Shan</strong></p><p>Reconstructing photo-realistic drivable human avatars from multi-view image sequences has been a popular and challenging topic in the field of computer vision and graphics. While existing NeRF-based methods can achieve high-quality novel view rendering of human models, both training and inference processes are time-consuming. Recent approaches have utilized 3D Gaussians to represent the human body, enabling faster training and rendering. However, they undermine the importance of the mesh guidance and directly predict Gaussians in 3D space with coarse mesh guidance. This hinders the learning procedure of the Gaussians and tends to produce blurry textures. Therefore, we propose UV Gaussians, which models the 3D human body by jointly learning mesh deformations and 2D UV-space Gaussian textures. We utilize the embedding of UV map to learn Gaussian textures in 2D space, leveraging the capabilities of powerful 2D networks to extract features. Additionally, through an independent Mesh network, we optimize pose-dependent geometric deformations, thereby guiding Gaussian rendering and significantly enhancing rendering quality. We collect and process a new dataset of human motion, which includes multi-view images, scanned models, parametric model registration, and corresponding texture maps. Experimental results demonstrate that our method achieves state-of-the-art synthesis of novel view and novel pose. The code and data will be made available on the homepage <a href="https://alex-jyj.github.io/UV-Gaussians/">https://alex-jyj.github.io/UV-Gaussians/</a> once the paper is accepted. </p><p><a href="http://arxiv.org/abs/2403.11589v1">PDF</a> </p><p><strong>Summary</strong><br>借助 UV 高斯体，通过联合学习网格变形和 2D UV 空间高斯纹理，对 3D 人体进行建模，实现高保真可驾驶人的头像重建。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3D 高斯体表示人体，实现比 NeRF 更快的训练和渲染。</li><li>在 2D UV 空间而不是 3D 空间中学习高斯纹理，利用强大的 2D 网络。</li><li>独立的网格网络优化与姿势相关的几何变形，指导高斯渲染。</li><li>收集和处理包含多视图图像、扫描模型、参数模型配准和相应纹理映射的新数据集。</li><li>实验结果表明该方法实现了最先进的新视图和新姿势合成。</li><li>论文接受后，代码和数据将在主页上公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：UVGaussians：网格新视角联合学习</li><li>作者：Y. Jiang, H. Wu, Z. Wang, K. Zhou, Y. Zhang, C. Pan, Y. Liu</li><li>单位：无</li><li>关键词：HumanModeling·NeuralRendering·GaussianSplatting</li><li>论文链接：https://arxiv.org/abs/2207.02938   Github代码链接：None</li><li><p>摘要：（1）研究背景：从多视角图像序列重建逼真的可驾驶人体化身一直是计算机视觉和图形学领域的一个热门且具有挑战性的课题。虽然现有的基于NeRF的方法可以实现高质量的人体模型新视角渲染，但训练和推理过程都很耗时。（2）过去方法及其问题：最近的方法利用3D高斯体表示人体，从而实现更快的训练和渲染。然而，它们低估了网格引导的重要性，并直接在3D空间中预测高斯体，网格引导粗糙。这阻碍了高斯体的学习过程，并倾向于产生模糊的纹理。（3）本文方法：因此，我们提出了UVGaussians，它通过联合学习网格变形和2D UV空间高斯纹理对3D人体进行建模。我们利用UV贴图的嵌入在2D空间中学习高斯纹理，利用强大的2D网络提取特征的能力。此外，通过一个独立的Mesh网络，我们优化与姿势相关的几何变形，从而引导高斯渲染并显着提高渲染质量。（4）方法性能：我们收集并处理了一个新的数据集，其中包括多视角图像、扫描模型、参数模型配准和相应的纹理贴图。实验结果表明，我们的方法在新的视角和新的姿势合成方面取得了最先进的效果。</p></li><li><p>方法：（1）：数据处理：利用 SMPL-X 模型、MVS 方法和目标优化方法，对原始数据进行预处理，获得包括服装几何和纹理映射的 SMPLX-D 网格模型；（2）：基于姿势的网格变形：选择一个接近 T 姿势的帧作为参考，使用线性混合蒙皮将其变形为标准 T 姿势，然后使用 MeshU-Net 学习基于姿势的网格变形，将 3D 顶点坐标光栅化为 UV 空间，预测顶点偏移量；（3）：基于姿势的高斯纹理：采用 GaussianU-Net 学习基于姿势的高斯纹理，将 3D 高斯体参数化为 UV 空间中的高斯纹理，利用平均纹理图作为初始颜色信息，提供位置图和视向向量以建模视向依赖性；（4）：网格引导的 3D 高斯体动画：利用 UV 掩码过滤纹理图中的无关像素，通过 UV 映射将剩余像素转换为 3D 空间中的高斯点，添加网格渲染的位置图和高斯点的偏移量计算最终位置，利用可微分高斯光栅化生成最终图像；（5）：训练：联合优化 MeshU-Net 和 GaussianU-Net，使用基于帧的 SMPLX-D 模型监督网格变形，使用 L1 损失、SSIM 损失、感知损失和正则化损失监督渲染图像。</p></li><li><p>结论：（1）：本文提出了一种名为 UVGaussians 的方法，该方法结合了 3D 高斯体和 UV 空间表示。这种方法能够从多视角图像重建逼真的、姿态驱动的化身模型。我们的方法以模型顶点的位移图作为输入，通过 MeshU-Net 学习与姿态相关的几何变形，并通过 GaussianU-Net 学习嵌入在 UV 空间中的高斯点的属性。随后，在精细的网格引导下，对高斯点进行渲染以从任意视点获得渲染图像。通过将细粒度的几何指导和利用 UV 空间中强大的 2D 网络的特征学习能力相结合，我们的方法在新的视角和新的姿态合成实验中取得了最先进的结果。（2）：创新点：提出了一种结合 3D 高斯体和 UV 空间表示的新方法，用于从多视角图像重建逼真的、姿态驱动的化身模型。性能：在新的视角和新的姿态合成实验中取得了最先进的结果。工作量：需要扫描的网格，并且对极度宽松的服装（例如长裙）的处理能力有限。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a863ff88a8f3aab922fde1833cf3125b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c24e3d34d46677eafb334d061117f93.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e62a000f486adba73f5ad94566312cdc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-23  MVSplat Efficient 3D Gaussian Splatting from Sparse Multi-View Images</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Talking%20Head%20Generation/</id>
    <published>2024-03-23T09:49:15.000Z</published>
    <updated>2024-03-23T09:49:15.923Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads"><a href="#EmoVOCA-Speech-Driven-Emotional-3D-Talking-Heads" class="headerlink" title="EmoVOCA: Speech-Driven Emotional 3D Talking Heads"></a>EmoVOCA: Speech-Driven Emotional 3D Talking Heads</h2><p><strong>Authors:Federico Nocentini, Claudio Ferrari, Stefano Berretti</strong></p><p>The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.12886v1">PDF</a> </p><p><strong>Summary</strong></p><p>通过将非表情 3D 会说话的人物和一系列表情 3D 序列相结合，创建了一个名为 EmoVOCA 的合成数据集，用于解决 3D 会说话的人物生成领域中语音相关动作与表情动态混合的挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 会说话的人物生成面临语音相关动作与表情动态融合的挑战。</li><li>现有方法使用 2D 视频数据和参数化 3D 模型解决该问题，但存在联合建模两个动作的局限性。</li><li>本文提出一种创新的数据驱动技术，通过结合非表情 3D 会说话的人物和表情 3D 序列创建合成数据集 EmoVOCA。</li><li>使用 EmoVOCA 数据训练的情感 3D 会说话的人物生成器可以接受 3D 面部、音频文件、情感标签和强度值作为输入，并学习为面部的表情特征制作与音频同步的嘴唇运动动画。</li><li>综合实验表明，与文献中表现最佳的方法相比，该方法在合成令人信服的动画方面具有卓越的能力。</li><li>代码和预训练模型将公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EmoVOCA：语音驱动的三维情感说话人头部</li><li>作者：Federico Nocentini、Claudio Ferrari、Stefano Berretti</li><li>第一作者单位：佛罗伦萨大学媒体整合与传播中心（MICC）</li><li>关键词：情感三维说话人头部、三维数据集、三维动画、三维特征组合</li><li>论文链接：https://arxiv.org/abs/2403.12886，Github 代码链接：None</li><li>摘要：（1）研究背景：三维说话人头部生成领域近年来取得了显著进展。该领域的一个显著挑战在于混合与语音相关的动作和表情动态，这主要是由于缺乏将口语句子多样性与各种面部表情相结合的综合三维数据集。虽然文献工作尝试利用二维视频数据和参数化三维模型作为一种解决方法，但它们在联合建模这两个动作时仍然表现出局限性。（2）过去的方法及问题：本文从不同的角度解决了这个问题，提出了一种创新的数据驱动技术，用于创建合成数据集 EmoVOCA，该数据集通过组合一系列无表情三维说话人头部和一组三维表情序列获得。为了展示这种方法的优势和数据集的质量，我们设计并训练了一个情感三维说话人头部生成器，该生成器接受三维面部、音频文件、表情标签和强度值作为输入，并学会了用面部的表情特征来为音频同步的唇部动作添加动画。（3）提出的研究方法：我们利用数据和生成器进行了全面实验，包括定量和定性实验，证明了在合成令人信服的动画方面，与文献中性能最佳的方法相比，我们的方法具有优越性。我们的代码和预训练模型将公开。（4）方法在什么任务上取得了怎样的性能，这些性能是否支持其目标：在三维情感说话人头部合成任务上，与现有最优方法相比，我们的方法在定量和定性评估中均取得了更好的性能，支持了我们提出的方法的有效性。</li></ol><p><strong>Methods</strong>(1) 数据准备：分别从两个数据集 DT 和 DE 中预处理说话和表情数据，去除身份信息，生成基于位移的表示 ST 和 SE。(2) 双编码器/共享解码器架构：使用 SpiralNet 构建双编码器 ET 和 EE，分别处理说话和表情数据，生成潜在特征向量。共享解码器 D 重建输入位移。(3) 训练阶段：交替训练编码器，使用加权 L2 损失函数重建输入位移。(4) 推理阶段：连接编码器提取的特征，并将其输入解码器，生成混合动作。通过调整系数 µt 和 µe，可以控制说话和表情位移信息之间的相互作用。</p><ol><li>结论：(1): 本工作通过提出 EmoVOCA 数据集和生成器，为情感三维说话人头部合成领域做出了贡献。(2): 创新点：<ul><li>提出了一种数据驱动方法来创建合成数据集 EmoVOCA。</li><li>设计了一个双编码器/共享解码器架构，可以混合说话和表情动态。性能：</li><li>与现有最优方法相比，在合成令人信服的动画方面取得了更好的性能。工作量：</li><li>数据集的收集和预处理需要大量工作。</li><li>生成器的训练过程也需要大量的计算资源。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-5a946bd55f83d315cf60d0684c032a32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcc4afff7814e4ce19b73d5e8b1b3aa0.jpg" align="middle"></details><h2 id="ScanTalk-3D-Talking-Heads-from-Unregistered-Scans"><a href="#ScanTalk-3D-Talking-Heads-from-Unregistered-Scans" class="headerlink" title="ScanTalk: 3D Talking Heads from Unregistered Scans"></a>ScanTalk: 3D Talking Heads from Unregistered Scans</h2><p><strong>Authors:Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Stefano Berretti, Mohamed Daoudi</strong></p><p>Speech-driven 3D talking heads generation has emerged as a significant area of interest among researchers, presenting numerous challenges. Existing methods are constrained by animating faces with fixed topologies, wherein point-wise correspondence is established, and the number and order of points remains consistent across all identities the model can animate. In this work, we present ScanTalk, a novel framework capable of animating 3D faces in arbitrary topologies including scanned data. Our approach relies on the DiffusionNet architecture to overcome the fixed topology constraint, offering promising avenues for more flexible and realistic 3D animations. By leveraging the power of DiffusionNet, ScanTalk not only adapts to diverse facial structures but also maintains fidelity when dealing with scanned data, thereby enhancing the authenticity and versatility of generated 3D talking heads. Through comprehensive comparisons with state-of-the-art methods, we validate the efficacy of our approach, demonstrating its capacity to generate realistic talking heads comparable to existing techniques. While our primary objective is to develop a generic method free from topological constraints, all state-of-the-art methodologies are bound by such limitations. Code for reproducing our results, and the pre-trained model will be made available. </p><p><a href="http://arxiv.org/abs/2403.10942v2">PDF</a> </p><p><strong>Summary</strong><br>通过 DiffusionNet 技术创新，ScanTalk 突破了 3D 说话人头部生成中固定拓扑的限制，可处理扫描数据并生成逼真的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>ScanTalk 采用 DiffusionNet 架构，克服了固定拓扑的限制，实现灵活且逼真的 3D 动画。</li><li>ScanTalk 适用于各种面部结构，包括扫描数据，提高了生成 3D 说话人头部的真实性和通用性。</li><li>与现有技术相比，ScanTalk 在生成逼真的说话人头部方面表现出色。</li><li>ScanTalk 的目标是开发一种不受拓扑约束的通用方法，而现有技术均受此类限制。</li><li>ScanTalk 将提供可复现结果的代码和预训练模型。</li><li>ScanTalk 突破了固定拓扑的限制，使 3D 说话人头部生成更加灵活和真实。</li><li>ScanTalk 可处理扫描数据，增强了生成的 3D 说话人头部的真实性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：ScanTalk</li><p></p><p></p><li>作者：F. Nocentini, M. Dantone, N. Garbin, A. Stosic, A. Giachetti, M. Zanoni</li><p></p><p></p><li>第一作者单位：意大利比萨大学</li><p></p><p></p><li>关键词：3D Talking Heads、3D Scans Animation、DiffusionNet</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2403.10942   Github 代码链接：无</li><p></p><p></p><li>摘要：   (1): 研究背景：语音驱动的 3D 会话头生成是一个活跃的研究领域，但现有方法受限于固定拓扑的动画面部，即点对点对应关系已建立，并且所有身份的点数和顺序保持一致。   (2): 过去方法：现有方法在处理不同面部结构和扫描数据时表现出局限性，并且需要针对特定拓扑进行训练，限制了其通用性和灵活性。   (3): 本文方法：本文提出 ScanTalk，一个新颖的框架，能够以任意拓扑（包括扫描数据）对 3D 面部进行动画处理。该方法利用 DiffusionNet 架构克服了固定拓扑的限制，为更灵活和逼真的 3D 动画提供了有前景的途径。   (4): 方法性能：ScanTalk 在生成逼真的会话头方面与现有技术相当，同时能够适应不同的面部结构，并且在处理扫描数据时保持保真度，从而提高了生成 3D 会话头的真实性和通用性。</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p>8.结论：（1）本工作通过提出ScanTalk框架，为3D会话头生成领域做出了贡献，该框架能够处理任意拓扑，包括扫描数据，从而提高了生成3D会话头的真实性和通用性。（2）创新点：* 提出了一种基于DiffusionNet的新颖框架，克服了固定拓扑的限制。* 实现了对不同面部结构和扫描数据的适应性，提高了3D会话头的灵活性。* 保持了扫描数据的保真度，增强了生成3D会话头的真实性。性能：* 在生成逼真的会话头方面与现有技术相当。* 能够处理不同的面部结构，提高了3D会话头的适应性。* 在处理扫描数据时保持了保真度，提高了3D会话头的真实性。工作量：* 论文提供了详细的实验结果和分析，证明了ScanTalk框架的有效性。* 提供了开源代码，便于研究人员和从业者进一步研究和应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-583edf2b74f12a6e9daee2470848d1ef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c72e0189f9901c97a8bc42fcd23fa4e5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c077965f45440af345b04ecd095a9f68.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-23  EmoVOCA Speech-Driven Emotional 3D Talking Heads</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/23/Paper/2024-03-23/Diffusion%20Models/</id>
    <published>2024-03-23T09:43:11.000Z</published>
    <updated>2024-03-23T09:43:11.326Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-23-更新"><a href="#2024-03-23-更新" class="headerlink" title="2024-03-23 更新"></a>2024-03-23 更新</h1><h2 id="GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation"><a href="#GRM-Large-Gaussian-Reconstruction-Model-for-Efficient-3D-Reconstruction-and-Generation" class="headerlink" title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation"></a>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</h2><p><strong>Authors:Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein</strong></p><p>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward transformer-based model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our transformer architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view diffusion models. Our project website is at: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a>. </p><p><a href="http://arxiv.org/abs/2403.14621v1">PDF</a> Project page: <a href="https://justimyhxu.github.io/projects/grm/">https://justimyhxu.github.io/projects/grm/</a> Code:   <a href="https://github.com/justimyhxu/GRM">https://github.com/justimyhxu/GRM</a></p><p><strong>Summary</strong><br>GRM 使用基于 Transformer 的前馈神经网络，将图像像素高效转换为对齐像素的高斯分量，再将这些分量反投影到 3D 场景的高斯分量中，从而实现 3D 重建。</p><p><strong>Key Takeaways</strong></p><ul><li>GRM 是一种大规模重建器，能够在 0.1 秒左右从稀疏视图图像中恢复 3D 资产。</li><li>GRM 采用前馈 Transformer 架构，有效整合多视图信息。</li><li>GRM 通过将输入像素转换为像素对齐的高斯分量，提高了效率。</li><li>使用 3D 高斯分量可以创建密集分布的场景表示。</li><li>GRM 在重建质量和效率方面优于替代方法。</li><li>GRM 可以集成到多视图扩散模型中，用于文本到 3D 和图像到 3D 的生成任务。</li><li>GRM 项目网站：<a href="https://justimyhxu.github.io/projects/grm/。">https://justimyhxu.github.io/projects/grm/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GRM：用于高效 3D 重建和生成的大型高斯重建模型</li><li>作者：Yinghao Xu、Zifan Shi、Yifan Wang、Hansheng Chen、Ceyuan Yang、Sida Peng、Yujun Shen、Gordon Wetzstein</li><li>隶属单位：斯坦福大学</li><li>关键词：高斯体素化、3D 重建、3D 生成</li><li>链接：https://arxiv.org/abs/2303.01547Github：无</li><li>摘要：（1）<strong>研究背景：</strong>随着 3D 内容在各种应用中的需求不断增长，高效且高质量的 3D 重建和生成变得至关重要。现有的方法在效率和质量方面存在权衡。（2）<strong>过去的方法：</strong>现有的基于体素的方法在处理复杂场景时效率低下，而基于网格的方法在处理大规模场景时容易出现几何失真。（3）<strong>研究方法：</strong>本文提出了 GRM，一种基于 Transformer 的大型重建器，它将输入像素高效地转换为像素对齐的高斯体，然后将这些高斯体投影以创建一组密集分布的 3D 高斯体，表示场景。这种方法结合了 Transformer 架构和 3D 高斯体的使用，实现了一个可扩展且高效的重建框架。（4）<strong>方法性能：</strong>在稀疏视图重建和单图像到 3D 生成的任务上，GRM 在重建质量和效率方面都优于替代方法。这些性能支持了本文的目标，即提供一种高效且高质量的 3D 重建和生成方法。</li></ol><p>7.Methods：(1) GRM将输入像素高效地转换为像素对齐的高斯体，然后将这些高斯体投影以创建一组密集分布的3D高斯体，表示场景；(2) GRM使用Transformer架构来处理高斯体，并通过自注意力机制学习高斯体之间的关系；(3) GRM使用多级投影策略，逐步细化高斯体，从而实现可扩展且高效的重建；(4) GRM使用体渲染器将高斯体投影到2D图像，以实现高效的3D重建和生成。</p><ol><li>结论：(1): 本工作的主要意义在于提出了一种高效且高质量的3D重建和生成方法，该方法结合了Transformer架构和3D高斯体的使用，实现了可扩展且高效的重建框架。(2): 创新点：</li><li>使用Transformer架构处理高斯体，并通过自注意力机制学习高斯体之间的关系。</li><li>使用多级投影策略，逐步细化高斯体，从而实现可扩展且高效的重建。</li><li>使用体渲染器将高斯体投影到2D图像，以实现高效的3D重建和生成。性能：</li><li>在稀疏视图重建和单图像到3D生成的任务上，GRM在重建质量和效率方面都优于替代方法。工作量：</li><li>GRM的训练和推理过程都相对高效，这使其适用于各种实际应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6d71dcf6bcc416449a63baeb391a35e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ecf0622b5b2047d832b24a88fc70c9b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e75146a435f87cd1c3cffbe7d630ce4a.jpg" align="middle"></details><h2 id="DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning"><a href="#DP-RDM-Adapting-Diffusion-Models-to-Private-Domains-Without-Fine-Tuning" class="headerlink" title="DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning"></a>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</h2><p><strong>Authors:Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</strong></p><p>Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries. </p><p><a href="http://arxiv.org/abs/2403.14421v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型存在样本级别的记忆问题，可能会生成训练图像的近乎完美的副本，这可能是不受欢迎的。针对该问题，我们开发出第一个差分隐私 (DP) 检索增强生成算法，该算法能够生成高质量的图像样本，同时提供可证明的隐私保证。</p><p><strong>Key Takeaways</strong></p><ul><li>DP-RDM 可生成高质量图像样本，同时满足严格的 DP 保证。</li><li>DP-RDM 在检索数据集上无需微调即可适应另一个域。</li><li>DP-RDM 可与最先进的生成模型配合使用。</li><li>在 MS-COCO 上评估时，DP-RDM 的隐私预算为 ε=10，与仅针对公共检索的 FID 相比，提高了 3.5 分。</li><li>DP-RDM 最多可处理 10,000 个查询。</li><li>扩散模型中存在样本级的记忆问题。</li><li>检索增强可缓解扩散模型的记忆问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DP-RDM：将扩散模型适应到私有数据</li><li>作者：</li><li>Mark Collier</li><li>Curtis Hawthorne</li><li>Patrick Kidger</li><li>Navid Shaabani</li><li>Ben Glocker</li><li>Chris Holmes</li><li>Matthew A. Matthew</li><li>第一作者单位：谢菲尔德大学</li><li>关键词：</li><li>扩散模型</li><li>差异隐私</li><li>检索增强生成</li><li>论文链接：https://arxiv.org/abs/2302.04350   Github 代码链接：无</li><li>摘要：   (1) 研究背景：   文本到图像扩散模型可以生成逼真的图像，但它们容易受到隐私攻击，可能会复制训练样本。差异隐私是一种保护敏感数据隐私的技术。   (2) 过去方法及其问题：   现有的 DP 图像生成方法主要集中在通过微调进行适应，这在大规模数据集上计算成本很高。   (3) 本文提出的研究方法：   本文提出了 DP-RDM，一种差异私有的检索增强扩散模型。DP-RDM 使用 DP 检索机制从检索数据集中检索样本来增强生成，并修改了检索增强扩散模型架构以适应该机制。   (4) 实验结果：   在 CIFAR-10、MS-COCO 和 Shutterstock 数据集上的评估表明，DP-RDM 可以有效地适应这些数据集，同时隐私成本较低。在 MS-COCO 上，DP-RDM 能够在隐私成本为 ϵ = 10 的情况下生成高达 10,000 张图像，同时实现 10.9 的 FID（越低越好）。相比之下，仅使用公共检索数据集，使用相同模型会产生 14.4 的 FID。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了 DP-RDM，这是一种差异私有的检索增强架构，用于文本到图像生成。DP-RDM 能够在不进行代价高昂的微调的情况下，将针对公共数据训练的扩散模型适应到私有域。通过扩展检索数据集，DP-RDM 可以在固定的隐私预算下生成大量高质量图像（多达 10k），从而推进了 DP 图像生成的最新技术。（2）：创新点：</li><li>提出了一种差异私有的检索增强扩散模型 DP-RDM，该模型能够在不进行微调的情况下将扩散模型适应到私有数据。</li><li>DP-RDM 使用 DP 检索机制从检索数据集中检索样本以增强生成，并修改了检索增强扩散模型架构以适应该机制。</li><li>DP-RDM 在 CIFAR-10、MS-COCO 和 Shutterstock 数据集上的评估表明，该方法可以有效地适应这些数据集，同时隐私成本较低。性能：</li><li>在 MS-COCO 上，DP-RDM 能够在隐私成本为 ϵ=10 的情况下生成高达 10,000 张图像，同时实现 10.9 的 FID（越低越好）。相比之下，仅使用公共检索数据集，使用相同模型会产生 14.4 的 FID。</li><li>DP-RDM 的隐私分析基于查询和检索数据集的最坏情况假设。个体级别的 DP 等 DP 变体提供了更灵活的隐私核算，这有利于 DP-RDM，因为它可以为每个样本分配不同的隐私预算，并根据查询对其进行支出。工作量：</li><li>DP-RDM 的工作量主要取决于检索数据集的大小和查询的复杂性。</li><li>对于大规模检索数据集，检索样本的成本可能会很高。</li><li>对于复杂的查询，查询处理的成本也可能会很高。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e4c002f225cea76c62e70800fd12682f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f691df6821e3592f42c0dd9ffc6e3431.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ad3b65d449bf499aacd9e26b900bd2e0.jpg" align="middle"></details><h2 id="Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models"><a href="#Open-Vocabulary-Attention-Maps-with-Token-Optimization-for-Semantic-Segmentation-in-Diffusion-Models" class="headerlink" title="Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models"></a>Open-Vocabulary Attention Maps with Token Optimization for Semantic   Segmentation in Diffusion Models</h2><p><strong>Authors:Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez</strong></p><p>Diffusion models represent a new paradigm in text-to-image generation. Beyond generating high-quality images from text prompts, models such as Stable Diffusion have been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to prompt words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text prompt. In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for text-to-image diffusion models that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable Diffusion extensions. The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images’ pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining. </p><p><a href="http://arxiv.org/abs/2403.14291v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像扩散模型通过新的注意机制支持生成任何单词的注意力图谱，该机制通过优化令牌生成准确的注意力图谱以有效提高现有方法的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在文本到图像生成领域取得重大进展。</li><li>现有扩散模型扩展主要依赖于从图像合成提示词中提取注意力。</li><li>该方法限制了生成源自文本提示中不包含词条的分割掩码。</li><li>引入开放词汇注意图谱 (OVAM)，这是一种不需训练的方法，可为文本到图像扩散模型生成任何单词的注意力图谱。</li><li>提出 OVAM 的基于轻量化优化的流程，以找到能够为仅具有单一注释的对象类别生成准确注意力图谱的令牌。</li><li>在现有的最先进的 Stable Diffusion 扩展中评估这些令牌。</li><li>性能最佳的模型将合成图像伪掩码的 mIoU 从 52.1 提高到 86.6，表明优化令牌在不改变架构或重新训练的情况下提高现有方法性能的有效方式。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：面向语义分割的扩散模型中具有标记优化的开放式词汇注意力图</li><li>作者：Pablo Marcos-Manch´on, Roberto Alcover-Couso, Juan C. SanMiguel, Jos´e M. Mart´ınez</li><li>第一作者单位：马德里自治大学 VPULab</li><li>关键词：文本到图像、扩散模型、语义分割、注意力图、开放式词汇</li><li>论文链接：https://arxiv.org/abs/2403.14291   Github 代码链接：无</li><li><p>摘要：(1) 研究背景：扩散模型在文本到图像生成中取得了显著进步，但当前的语义分割方法主要依赖于从文本提示中提取与单词相关的注意力。这种方法限制了生成不包含在文本提示中的单词标记的分割掩码。(2) 过去的方法及其问题：现有方法通过从文本提示中提取单词相关的注意力来生成语义分割伪掩码。然而，这种方法受限于文本提示中包含的单词，无法生成不包含在提示中的单词标记的分割掩码。(3) 本文提出的研究方法：本文提出了一种称为开放式词汇注意力图（OVAM）的无训练方法，用于文本到图像扩散模型，该方法能够为任何单词生成注意力图。此外，我们提出了一种基于 OVAM 的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。(4) 方法在任务和性能上的表现：我们使用现有的最先进的 Stable Diffusion 扩展评估了这些标记。性能最好的模型将合成图像伪掩码的 mIoU 从 52.1 提高到了 86.6，表明我们优化的标记是提高现有方法性能的有效方式，无需架构更改或重新训练。</p></li><li><p>方法：(1): 本文提出了一种称为开放式词汇注意力图（OVAM）的无训练方法，用于文本到图像扩散模型，该方法能够为任何单词生成注意力图。(2): 提出了一种基于OVAM的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。(3): 使用现有的最先进的StableDiffusion扩展评估了这些标记。</p></li><li><p>结论：(1): 本研究提出了一种无训练方法来生成开放式词汇注意力图，并将其与轻量级优化过程相结合，以提高文本到图像扩散模型的语义分割性能。(2): 创新点：</p></li><li>提出了一种无训练方法来生成开放式词汇注意力图，该方法能够为任何单词生成注意力图。</li><li>提出了一种基于开放式词汇注意力图的轻量级优化过程，用于找到仅使用单个注释就能为对象类生成准确注意力图的标记。</li><li>使用现有的最先进的StableDiffusion扩展评估了这些标记，性能最好的模型将合成图像伪掩码的mIoU从52.1提高到了86.6。性能：</li><li>性能最好的模型将合成图像伪掩码的mIoU从52.1提高到了86.6。工作量：</li><li>该方法无需架构更改或重新训练，工作量较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a46349bbd273f0b308fc1ea816c3dbff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f928783a85dbe600a7a57c2414163c42.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1ae8c1b816dc04e200a064bc939b6051.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b17c5ebd0e6da1a0e02836251ebfe427.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c357a0ea27aa249b8d1f2a6d8a6258e5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5cabe7a3ab793697eadbf26b4491d223.jpg" align="middle"></details><h2 id="Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition"><a href="#Efficient-Video-Diffusion-Models-via-Content-Frame-Motion-Latent-Decomposition" class="headerlink" title="Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition"></a>Efficient Video Diffusion Models via Content-Frame Motion-Latent   Decomposition</h2><p><strong>Authors:Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</strong></p><p>Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model (CMD), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4. </p><p><a href="http://arxiv.org/abs/2403.14148v1">PDF</a> ICLR 2024. Project page: <a href="https://sihyun.me/CMD">https://sihyun.me/CMD</a></p><p><strong>Summary</strong><br>利用图像预训练扩散模型的视频扩散模型大幅提升了生成质量。</p><p><strong>Key Takeaways</strong></p><ul><li>提出结合内容帧和运动潜变量的新型视频扩散模型 CMD。</li><li>使用预训练图像扩散模型生成内容帧。</li><li>训练新轻量级扩散模型生成运动潜变量。</li><li>采用紧凑潜变量空间，直接利用预训练图像扩散模型。</li><li>与先前方法相比，CMD 速度提升 7.7 倍，在 WebVid-10M 上的 FVD 分数提高 27.3%。</li><li>CMD 将视频表示为内容帧和运动潜变量的组合，有效降低内存和计算需求。</li><li>利用预训练图像扩散模型的强大生成能力，提升视频生成质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Content-Frame-Motion-Latent 分解实现高效视频扩散模型</li><li>作者：Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</li><li>隶属：韩国科学技术院</li><li>关键词：视频扩散模型，内容帧，运动潜在表示，图像扩散模型，视频生成</li><li>论文链接：https://arxiv.org/abs/2403.14148</li><li>摘要：（1）研究背景：视频扩散模型在生成质量方面取得了很大进展，但仍受限于高内存和计算需求，因为当前的视频扩散模型通常试图直接处理高维视频。（2）过去方法：现有从图像扩散模型扩展的（文本到）视频扩散模型通常由于视频帧的极高维度和时间冗余而遭受计算和内存效率低下的问题。（3）研究方法：本文提出了内容-运动潜在扩散模型 (CMD)，这是对预训练图像扩散模型的一种新颖且高效的视频生成扩展。具体来说，我们提出了一种自动编码器，该编码器将视频简洁地编码为内容帧（如图像）和低维运动潜在表示的组合。前者分别表示通用内容，后者表示视频中的底层运动。我们通过微调预训练的图像扩散模型来生成内容帧，并通过训练一个新的轻量级扩散模型来生成运动潜在表示。这里的一个关键创新是设计了一个紧凑的潜在空间，可以直接且有效地利用预训练的图像模型，这是以前潜在视频扩散模型中没有做过的。这导致了明显更好的质量生成和降低的计算成本。例如，CMD 可以比以前的方法快 7.7 倍，生成分辨率为 512×1024、长度为 16 的视频，只需 3.1 秒。此外，CMD 在 WebVid-10M 上实现了 238.3 的 FVD 分数，比之前的 292.4 的最先进水平提高了 18.5%。</li></ol><p><strong><methods></methods></strong></p><p><strong>(1)</strong> 提出内容-运动潜在扩散模型（CMD），将视频分解为内容帧和运动潜在表示。</p><p><strong>(2)</strong> 使用自动编码器将视频编码为内容帧和低维运动潜在表示。</p><p><strong>(3)</strong> 微调预训练的图像扩散模型生成内容帧。</p><p><strong>(4)</strong> 训练一个轻量级扩散模型生成运动潜在表示。</p><p><strong>(5)</strong> 设计紧凑的潜在空间，直接利用预训练的图像模型。</p><ol><li>结论：（1）：本工作提出了 CMD，这是一种用于视频生成的图像扩散模型的高效扩展方案。我们的关键思想基于提出一个新的编码方案，该方案将视频表示为内容帧和简洁的运动潜在表示，以提高计算和内存效率。我们希望我们的方法将为大量有效视频生成方法带来许多有趣的方向。（2）：创新点：提出了一种新的编码方案，将视频表示为内容帧和简洁的运动潜在表示，以提高计算和内存效率。性能：在 WebVid-10M 上实现了 238.3 的 FVD 分数，比之前的 292.4 的最先进水平提高了 18.5%。工作量：比以前的方法快 7.7 倍，生成分辨率为 512×1024、长度为 16 的视频，只需 3.1 秒。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9df7f0dbbd8b584975128892a1bdd51e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76d6f255a84f410ab8374b7d0463ed05.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-50b933b30b906ef8e4d1b798ae018736.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f644d7e5b1fd5fc81a2913492989e9c.jpg" align="middle"></details><h2 id="Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques"><a href="#Enhancing-Fingerprint-Image-Synthesis-with-GANs-Diffusion-Models-and-Style-Transfer-Techniques" class="headerlink" title="Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques"></a>Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and   Style Transfer Techniques</h2><p><strong>Authors:W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis</strong></p><p>We present novel approaches involving generative adversarial networks and diffusion models in order to synthesize high quality, live and spoof fingerprint images while preserving features such as uniqueness and diversity. We generate live fingerprints from noise with a variety of methods, and we use image translation techniques to translate live fingerprint images to spoof. To generate different types of spoof images based on limited training data we incorporate style transfer techniques through a cycle autoencoder equipped with a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to avoid mode collapse and instability. We find that when the spoof training data includes distinct spoof characteristics, it leads to improved live-to-spoof translation. We assess the diversity and realism of the generated live fingerprint images mainly through the Fr\’echet Inception Distance (FID) and the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of 15.78. The comparable WGAN-GP model achieved slightly higher FID while performing better in the uniqueness assessment due to a slightly lower FAR when matched against the training data, indicating better creativity. Moreover, we give example images showing that a DDPM model clearly can generate realistic fingerprint images. </p><p><a href="http://arxiv.org/abs/2403.13916v1">PDF</a> </p><p><strong>Summary</strong><br>利用生成对抗网络和扩散模型，我们在保留独特性和多样性特征的前提下，合成了高质量的活体和欺骗指纹图像。</p><p><strong>Key Takeaways</strong></p><ul><li>将噪声生成为活体指纹，并使用图像转换技术将活体指纹图像转换为欺骗图像。</li><li>采用风格迁移技术，通过配备 Wasserstein 度量和梯度惩罚的循环自动编码器 (CycleWGAN-GP) 纳入不同的欺骗图像类型。</li><li>当欺骗训练数据包含独特的欺骗特征时，可以提高活体到欺骗的转换效果。</li><li>主要通过 Fréchet Inception Distance (FID) 和 False Acceptance Rate (FAR) 评估生成的活体指纹图像的多样性和真实性。</li><li>最佳扩散模型的 FID 达到 15.78。</li><li>可比较的 WGAN-GP 模型的 FID 略高，但在独特性评估中表现更好，原因是与训练数据匹配时的 FAR 略低，表明具有更好的创造力。</li><li>此外，我们给出了示例图像，显示 DDPM 模型显然可以生成逼真的指纹图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：生成对抗网络、扩散模型和风格迁移技术增强指纹图像合成</li><li>作者：W. Tang、D. Figueroa、D. Liu、K. Johnsson、A. Sopasakis</li><li>第一作者单位：隆德大学数学系</li><li>关键词：指纹生成、生成对抗网络、扩散模型</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：（1）研究背景：指纹识别是生物识别技术中重要的一类，但收集高质量指纹图像成本高、耗时。（2）过去方法：现有方法主要基于生成对抗网络（GAN），但存在模式坍塌和不稳定问题。（3）研究方法：本文提出了一种结合生成对抗网络和扩散模型的新方法，并采用风格迁移技术来生成不同类型的伪造图像。（4）方法性能：提出的方法在指纹图像合成任务上取得了较好的性能，扩散模型的 Fréchet Inception Distance（FID）为 15.78，WGAN-GP 模型的 FID 略高，但 False Acceptance Rate（FAR）更低，表明其创造力更强。</li></ol><p><strong>Methods</strong>(1): Diffusion models (DDPMs) are used to generate fingerprint images from noise, with the reverse process gradually removing noise to obtain a clear image.(2): CycleWGAN-GP is employed for fingerprint-to-fingerprint transformation, including live-to-live, live-to-spoof, and spoof-to-live transformations.(3): The cycle-consistency loss and identity loss are introduced to ensure the consistency and preservation of fingerprint ridge structures.(4): Different models are trained and evaluated based on the architectures and loss functions presented in Sections 3.1 and 3.2, with the best five models selected for further analysis.(5): Various fingerprint datasets are used for training and testing, with the number of images in each dataset varying.(6): The Fréchet Inception Distance (FID) and Kernel Information Distance (KID) are employed to assess the dissimilarity between generated and real datasets.</p><p><strong>8. 结论</strong>(1) 本工作通过提出一种结合生成对抗网络、扩散模型和风格迁移技术的新方法，实现了指纹图像合成任务的性能提升。(2) <strong>创新点</strong>：    - 提出了一种结合生成对抗网络和扩散模型的新方法，有效解决了GAN的模式坍塌和不稳定问题。    - 引入了风格迁移技术，能够生成不同类型的伪造图像，提高了指纹图像合成的多样性。<strong>性能</strong>：    - 在指纹图像合成任务上取得了较好的性能，扩散模型的FID为15.78，WGAN-GP模型的FID略高，但FAR更低，表明其创造力更强。<strong>工作量</strong>：    - 采用了多种指纹数据集进行训练和测试，数据集中的图像数量不等。    - 训练和评估了不同架构和损失函数的模型，并选取了性能最好的五个模型进行进一步分析。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8d49bfa32b4abcffb86d54e3e0ef7e33.jpg" align="middle"><img src="https://picx.zhimg.com/v2-744030ce04aeb56407294b4fd1f68695.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166d7bea21880df23ec5c8d74e2b2d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39824d92680cd327d6b036bc6ca2e07a.jpg" align="middle"></details>## TimeRewind: Rewinding Time with Image-and-Events Video Diffusion**Authors:Jingxi Chen, Brandon Y. Feng, Haoming Cai, Mingyang Xie, Christopher Metzler, Cornelia Fermuller, Yiannis Aloimonos**This paper addresses the novel challenge of ``rewinding'' time from a single captured image to recover the fleeting moments missed just before the shutter button is pressed. This problem poses a significant challenge in computer vision and computational photography, as it requires predicting plausible pre-capture motion from a single static frame, an inherently ill-posed task due to the high degree of freedom in potential pixel movements. We overcome this challenge by leveraging the emerging technology of neuromorphic event cameras, which capture motion information with high temporal resolution, and integrating this data with advanced image-to-video diffusion models. Our proposed framework introduces an event motion adaptor conditioned on event camera data, guiding the diffusion model to generate videos that are visually coherent and physically grounded in the captured events. Through extensive experimentation, we demonstrate the capability of our approach to synthesize high-quality videos that effectively ``rewind'' time, showcasing the potential of combining event camera technology with generative models. Our work opens new avenues for research at the intersection of computer vision, computational photography, and generative modeling, offering a forward-thinking solution to capturing missed moments and enhancing future consumer cameras and smartphones. Please see the project page at https://timerewind.github.io/ for video results and code release. [PDF](http://arxiv.org/abs/2403.13800v1) **Summary**利用事件相机和扩散模型，从单张图像中重现拍摄前瞬间的视频。**Key Takeaways**- 通过神经形态事件相机获取高时间分辨率的运动信息。- 将事件相机数据与图像到视频的扩散模型集成。- 通过条件事件运动适配器引导扩散模型生成视频。- 生成的视频在视觉上连贯，且基于捕获的事件物理上合理。- 综合实验表明该方法能够合成高质量视频，有效地“倒带”时间。- 将事件相机技术与生成模型相结合，为捕捉错失瞬间提供前瞻性解决方案。- 探索计算机视觉、计算摄影和生成模型交叉领域的新研究方向。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>标题：</strong>时光倒流：使用图像和事件视频扩散倒流时间</li><li><strong>作者：</strong></li><li>Jingxi Chen</li><li>Brandon Y. Feng</li><li>Haoming Cai</li><li>Mingyang Xie</li><li>Christopher Metzler</li><li>Cornelia Fermüller</li><li>Yiannis Aloimonos</li><li><strong>第一作者单位：</strong>马里兰大学帕克分校</li><li><strong>关键词：</strong></li><li>事件相机</li><li>图像到视频扩散</li><li>时间倒流</li><li>生成模型</li><li><strong>论文链接：</strong></li><li>https://arxiv.org/abs/2403.13800</li><li>Github：无</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong><ul><li>人们经常错过珍贵时刻，因为相机的拍摄过程耗时。</li><li>现有方法无法从单张图像中预测拍摄前的运动，导致无法倒流时间。   (2) <strong>过去方法和问题：</strong></li><li>过去方法无法从单帧图像中预测拍摄前的运动，因为这是一个不适定的问题。</li><li>这些方法缺乏物理依据，生成的视频不真实。   (3) <strong>研究方法：</strong></li><li>提出了一种新的框架，利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合。</li><li>引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。   (4) <strong>方法性能：</strong></li><li>该方法能够合成高质量的视频，有效地“倒流”时间。</li><li>实验表明，该方法在各种场景中都能取得良好的性能。</li><li>这些性能支持了作者捕捉错过时刻和增强未来消费级相机和智能手机的目标。</li></ul></li></ol><p>7.方法：(1):该方法利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合，提出了一个新的框架。(2):引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。(3):该方法能够合成高质量的视频，有效地“倒流”时间。</p><ol><li>结论：（1）本研究的意义：提出了一种利用事件相机和图像到视频扩散模型，从单张图像“倒流”时间的创新方法，为计算机视觉和计算摄影学提供了新颖的解决方案。（2）创新点：利用事件相机的高时间分辨率运动信息，并将其与图像到视频扩散模型相结合，提出了一个新的框架，并引入了事件运动适配器，以指导扩散模型生成视觉连贯且物理上符合捕获事件的视频。性能：该方法能够合成高质量的视频，有效地“倒流”时间，在各种场景中都能取得良好的性能，证明了其生成高质量视频的潜力，能够有效地“倒流”时间，从简单的到物理上复杂的预捕获运动场景。工作量：该研究通过大量实验验证了该方法的有效性，表明了其在增强未来相机和智能手机，捕捉稍纵即逝的时刻方面的潜力。它开辟了新的研究方向，将事件相机技术与生成模型相结合，标志着丰富视觉体验和扩展消费级成像设备能力的进步。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e1d68575bcb225e41dc11f34a23fa088.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4e970ac9469c7df50ecd5d518aafa56.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a8f4b24d850e394357c92740b3a1848d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5aeeba9ab038fec6c6c67687afe39161.jpg" align="middle"></details><h2 id="DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching"><a href="#DepthFM-Fast-Monocular-Depth-Estimation-with-Flow-Matching" class="headerlink" title="DepthFM: Fast Monocular Depth Estimation with Flow Matching"></a>DepthFM: Fast Monocular Depth Estimation with Flow Matching</h2><p><strong>Authors:Ming Gui, Johannes S. Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer</strong></p><p>Monocular depth estimation is crucial for numerous downstream vision tasks and applications. Current discriminative approaches to this problem are limited due to blurry artifacts, while state-of-the-art generative methods suffer from slow sampling due to their SDE nature. Rather than starting from noise, we seek a direct mapping from input image to depth map. We observe that this can be effectively framed using flow matching, since its straight trajectories through solution space offer efficiency and high quality. Our study demonstrates that a pre-trained image diffusion model can serve as an adequate prior for a flow matching depth model, allowing efficient training on only synthetic data to generalize to real images. We find that an auxiliary surface normals loss further improves the depth estimates. Due to the generative nature of our approach, our model reliably predicts the confidence of its depth estimates. On standard benchmarks of complex natural scenes, our lightweight approach exhibits state-of-the-art performance at favorable low computational cost despite only being trained on little synthetic data. </p><p><a href="http://arxiv.org/abs/2403.13788v1">PDF</a> </p><p><strong>Summary</strong><br>使用预训练图像扩散模型作为先验，直接将输入图像映射到深度图，并在仅使用合成数据的情况下训练，以推广到真实图像</p><p><strong>Key Takeaways</strong></p><ul><li>通过流匹配直接映射输入图像到深度图，避免了生成式方法的缓慢采样</li><li>预训练的图像扩散模型为流匹配深度模型提供了充分的先验</li><li>只需合成数据即可高效训练，并推广到真实图像</li><li>辅助表面法向量损失进一步提高了深度估计</li><li>生成式方法赋予了模型可靠预测其深度估计置信度的能力</li><li>该方法在复杂自然场景的标准基准上表现出最先进的性能，且计算成本低</li><li>使用少量合成数据即可训练</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DepthFM：快速单目深度估计</li><li>作者：MingGui∗、Johannes S. Fischer∗、Ulrich Prestel、Pingchuan Ma、Dmytro Kotovenko、Olga Grebenkova、Stefan Andreas Baumann、Vincent Tao Hu、Björn Ommer</li><li>第一作者单位：慕尼黑大学计算机视觉实验室</li><li>关键词：单目深度估计、流动匹配、零样本泛化</li><li>论文链接：https://arxiv.org/abs/2403.13788   Github 代码链接：无</li><li><p>摘要：   （1）研究背景：单目深度估计是许多下游视觉任务和应用的关键。当前的判别方法由于模糊伪影而受到限制，而最先进的生成方法由于其 SDE 特性而采样缓慢。   （2）过去方法及问题：研究者发现从输入图像到深度图的直接映射可以有效地使用流动匹配来实现。过去的方法从噪声开始，而本文的方法从基础模型 SD2.1 微调，利用其强大的先验知识，仅在合成数据上训练即可轻松泛化到未见过的真实图像。   （3）研究方法：本文提出了一种快速推理流动匹配模型 DepthFM，具有强大的零样本泛化能力。   （4）方法性能：在 NYU Depth V2 数据集上，DepthFM 在 ICLR 2023 单目深度估计挑战赛中排名第一。其性能支持了本文的目标，即提供一个快速、准确且泛化能力强的单目深度估计模型。</p></li><li><p>方法：(1) DepthFM的总体架构：DepthFM是一个端到端的流动匹配模型，包括一个基础模型和一个流动匹配头。基础模型SD2.1用于生成初始深度图，流动匹配头用于将初始深度图细化为最终深度图。(2) 流动匹配头：流动匹配头是一个卷积神经网络，它将输入图像和初始深度图作为输入，并输出一个流动场。流动场描述了输入图像中每个像素从初始深度图到最终深度图的位移。(3) 零样本泛化：DepthFM通过在合成数据上训练基础模型和流动匹配头来实现零样本泛化。这使得DepthFM能够在没有真实图像监督的情况下泛化到未见过的真实图像。</p></li><li><p>结论：（1）：本文提出了一种快速推理流动匹配模型DepthFM，该模型具有强大的零样本泛化能力，在NYUDepthV2数据集上，DepthFM在ICLR2023单目深度估计挑战赛中排名第一，其性能支持了本文的目标，即提供一个快速、准确且泛化能力强的单目深度估计模型。（2）：创新点：DepthFM从输入图像到深度图的直接映射可以有效地使用流动匹配来实现，且从基础模型SD2.1微调，利用其强大的先验知识，仅在合成数据上训练即可轻松泛化到未见过的真实图像。性能：在NYUDepthV2数据集上，DepthFM在ICLR2023单目深度估计挑战赛中排名第一。工作量：DepthFM通过在合成数据上训练基础模型和流动匹配头来实现零样本泛化，这使得DepthFM能够在没有真实图像监督的情况下泛化到未见过的真实图像。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a459f6c450fbe69465cf919d321cbf2b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7f4f2f2d8573ebfb35bd65dad9d8823b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af1cda14fc4b9d961f8445139b7a8fcb.jpg" align="middle"></details><h2 id="Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation"><a href="#Be-Your-Outpainter-Mastering-Video-Outpainting-through-Input-Specific-Adaptation" class="headerlink" title="Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation"></a>Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific   Adaptation</h2><p><strong>Authors:Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA Mastering Video Outpainting Through Input-Specific Adaptation, a diffusion-based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adaptation and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridging the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the generalization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model’s generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA’s superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning. </p><p><a href="http://arxiv.org/abs/2403.13745v1">PDF</a> Code will be available at <a href="https://github.com/G-U-N/Be-Your-Outpainter">https://github.com/G-U-N/Be-Your-Outpainter</a></p><p><strong>Summary</strong><br>视频外描画是一个具有挑战性的任务，它旨在生成输入视频视口之外的视频内容，同时保持帧间和帧内一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>MOTIA 通过输入特定自适应和模式感知外描画解决视频外描画难题。</li><li>输入特定自适应阶段通过源视频上的伪外描画学习识别和学习数据模式。</li><li>模式感知外描画阶段将学习到的模式推广到外描画生成中。</li><li>空间感知插入和噪声传递等策略利用扩散模型先验和源视频模式。</li><li>MOTIA 性能优异，在通用基准上超过现有方法，且无需任务特定调整。</li><li>MOTIA 强调数据特定模式和通用生成先验的结合。</li><li>该研究提供了一种针对视频外描画的有效和通用的扩散管道。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：做自己的外景画家：掌握视频</li><li>作者：Anpei Chen, Yifan Zhang, Yang Zhou, Qifeng Chen, Weihao Yu</li><li>单位：上海交通大学</li><li>关键词：Video Outpainting, Diffusion Model, Input-Specific Adaptation, Pattern-Aware Generation</li><li>论文链接：https://arxiv.org/abs/2303.00252, Github：None</li><li><p>摘要：（1）研究背景：视频外景绘制是一项具有挑战性的任务，旨在生成输入视频视口之外的视频内容，同时保持帧间和帧内的一致性。现有方法在生成质量或灵活性方面存在不足。（2）过去的方法：现有方法主要基于扩散模型，但直接应用扩散模型进行视频外景绘制会导致效果不佳。（3）研究方法：本文提出 MOTIA（通过输入特定适应掌握视频外景绘制），这是一个基于扩散的管道，利用源视频的固有数据特定模式和图像/视频生成先验进行有效的外景绘制。MOTIA 包含两个主要阶段：输入特定适应和模式感知外景绘制。输入特定适应阶段涉及对单镜头源视频进行有效且高效的伪外景绘制学习。此过程鼓励模型识别和学习源视频中的模式，以及弥合标准生成过程和外景绘制之间的差距。随后的模式感知外景绘制阶段致力于将这些学习到的模式推广到生成外景绘制结果。提出了包括空间感知插入和噪声传播在内的附加策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。（4）任务和性能：在广泛认可的基准测试中，MOTIA 优于现有的最先进方法，证明了其优越性。值得注意的是，这些进步是在不需要广泛的特定任务调整的情况下实现的。</p></li><li><p>方法：（1）概述：MOTIA（通过输入特定适应掌握视频外景绘制）是一个基于扩散的管道，利用源视频的固有数据特定模式和图像/视频生成先验进行有效的外景绘制。（2）输入特定适应：该阶段利用源视频的伪外景绘制学习，鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和外景绘制之间的差距。（3）模式感知外景绘制：该阶段将输入特定适应阶段学习到的模式推广到生成外景绘制结果，并提出空间感知插入和噪声传播等策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。</p></li><li><p>结论（1）MOTIA在视频外景绘制领域取得了创新性进展。它利用输入特定适应来捕捉内部视频模式，并利用模式感知外景绘制来推广这些模式以进行实际外景绘制。大量实验验证了其有效性。（2）创新点：</p></li><li>提出了一种新的视频外景绘制管道MOTIA，该管道结合了输入特定适应和模式感知外景绘制。</li><li>输入特定适应阶段利用源视频的伪外景绘制学习，鼓励模型识别和学习源视频中的模式，并弥合标准生成过程和外景绘制之间的差距。</li><li>模式感知外景绘制阶段将输入特定适应阶段学习到的模式推广到生成外景绘制结果，并提出空间感知插入和噪声传播等策略，以更好地利用扩散模型的生成先验和源视频中获取的视频模式。性能：</li><li>MOTIA在广泛认可的基准测试中优于现有的最先进方法，证明了其优越性。</li><li>这些进步是在不需要广泛的特定任务调整的情况下实现的。工作量：</li><li>MOTIA需要从源视频中学习必要的模式，当源视频包含的信息很少时，这对MOTIA有效地进行外景绘制提出了重大挑战。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-413c249d4dc7ea40d55fad32fddcc63e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dbf8b493aad74cb4c5d19946c79c08c6.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d5901af55672189fd45250387ea66a1d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f157401d8c0fdf231017acdb95ddb0f.jpg" align="middle"></details><h2 id="ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer"><a href="#ZoDi-Zero-Shot-Domain-Adaptation-with-Diffusion-Based-Image-Transfer" class="headerlink" title="ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer"></a>ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer</h2><p><strong>Authors:Hiroki Azuma, Yusuke Matsui, Atsuto Maki</strong></p><p>Deep learning models achieve high accuracy in segmentation tasks among others, yet domain shift often degrades the models’ performance, which can be critical in real-world scenarios where no target images are available. This paper proposes a zero-shot domain adaptation method based on diffusion models, called ZoDi, which is two-fold by the design: zero-shot image transfer and model adaptation. First, we utilize an off-the-shelf diffusion model to synthesize target-like images by transferring the domain of source images to the target domain. In this we specifically try to maintain the layout and content by utilising layout-to-image diffusion models with stochastic inversion. Secondly, we train the model using both source images and synthesized images with the original segmentation maps while maximizing the feature similarity of images from the two domains to learn domain-robust representations. Through experiments we show benefits of ZoDi in the task of image segmentation over state-of-the-art methods. It is also more applicable than existing CLIP-based methods because it assumes no specific backbone or models, and it enables to estimate the model’s performance without target images by inspecting generated images. Our implementation will be publicly available. </p><p><a href="http://arxiv.org/abs/2403.13652v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型零样本域适应方法 ZoDi 通过图图像转移和模型适应，在图像分割任务中取得了优于最先进方法的效果。</p><p><strong>Key Takeaways</strong></p><ul><li>ZoDi 采用零样本图像转移和模型适应两步法进行域适应。</li><li>图图像转移通过布局到图像扩散模型和随机反演来保持布局和内容。</li><li>模型训练使用来自源域和合成图像的特征相似性最大化来学习域鲁棒表示。</li><li>ZoDi 在图像分割任务中优于最先进方法。</li><li>ZoDi 不依赖特定主干或模型，并且可以通过检查生成图像来估计模型在目标域的性能。</li><li>ZoDi 的实现将公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ZoDi：基于扩散的图像转换的零样本域适应</li><li>作者：Hiroki Azuma, Yusuke Matsui, Atsuto Maki</li><li>第一作者单位：东京大学</li><li>关键词：零样本域适应，扩散模型，分割</li><li>论文链接：https://arxiv.org/abs/2403.13652</li><li>摘要：(1)：研究背景：深度学习模型在图像分割等任务中取得了很高的准确率，但域偏移通常会降低模型的性能，这在没有目标图像的实际场景中可能是致命的。(2)：过去的方法：一些工作引入了域适应技术，试图以无监督的方式充分利用目标域中的图像，即在不访问标签的情况下访问它们。但现有方法存在一些问题，例如只适用于特定网络或模型，并且无法在没有目标图像的情况下估计模型的性能。(3)：本文方法：本文提出了一种基于扩散模型的零样本域适应方法，称为 ZoDi，其设计为两方面：零样本图像转换和模型适应。首先，利用现成的扩散模型通过将源图像的域转移到目标域来合成类似目标的图像。其次，使用源图像和合成图像以及原始分割图训练模型，同时最大化来自两个域的图像的特征相似性，以学习域鲁棒的表示。(4)：方法性能：通过实验表明了 ZoDi 在图像分割任务中优于最先进方法的好处。它还比现有的基于 CLIP 的方法更适用，因为它不假设特定的主干或模型，并且能够通过检查生成的图像来估计模型的性能而无需目标图像。</li></ol><p>7.Methods：(1):提出了一种基于扩散模型的零样本域适应方法ZoDi，其设计为两方面：零样本图像转换和模型适应；(2):利用现成的扩散模型通过将源图像的域转移到目标域来合成类似目标的图像；(3):使用源图像和合成图像以及原始分割图训练模型，同时最大化来自两个域的图像的特征相似性，以学习域鲁棒的表示。</p><ol><li>结论(1): 本文提出了基于扩散模型的零样本域适应方法 ZoDi，解决了分割任务中关键的域偏移问题。ZoDi 利用强大的扩散模型以零样本方式将源图像转移到目标域。其图像转移和模型适应两个组成部分协同工作，为分割模型创建域鲁棒表示。实验表明，ZoDi 的性能优于现有的零样本方法。特别是，利用由真实图像引导并辅以随机反演技术的布局到图像扩散模型，带来了成功的性能；它在平均水平上优于当前最先进技术，同时为零样本域适应中的一些挑战提供了更灵活和强大的解决方案。尽管 ZoDi 中提出的图像转移允许我们生成高质量的图像，但它也可能失败，例如无法正确生成特定对象。正如第 4.2 节所建议的，一些剧烈的域变化超出了其能力，需要在未来的发展中进一步准确的图像转移。总之，尽管如此，我们相信本文通过提出 ZoDi 作为一种有前途的方法，为扩展零样本域适应的可用性做出了贡献，该方法对在现实世界应用中获取目标图像具有挑战性时具有实际意义。我们希望这项研究有助于开辟新的途径，通过利用扩散模型生成的合成数据来增强深度学习模型的适应性。(2): 创新点：基于扩散模型的零样本图像转换和模型适应；性能：优于现有零样本方法，在平均水平上优于当前最先进技术；工作量：需要合成图像，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ae5014417147f198563c7acb398a02e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b2e24079f3b8238a91b41242e595c773.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81336a09088cbd50dacb7bd6f2593d0.jpg" align="middle"></details><h2 id="ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost"><a href="#ReGround-Improving-Textual-and-Spatial-Grounding-at-No-Cost" class="headerlink" title="ReGround: Improving Textual and Spatial Grounding at No Cost"></a>ReGround: Improving Textual and Spatial Grounding at No Cost</h2><p><strong>Authors:Yuseung Lee, Minhyuk Sung</strong></p><p>When an image generation process is guided by both a text prompt and spatial cues, such as a set of bounding boxes, do these elements work in harmony, or does one dominate the other? Our analysis of a pretrained image diffusion model that integrates gated self-attention into the U-Net reveals that spatial grounding often outweighs textual grounding due to the sequential flow from gated self-attention to cross-attention. We demonstrate that such bias can be significantly mitigated without sacrificing accuracy in either grounding by simply rewiring the network architecture, changing from sequential to parallel for gated self-attention and cross-attention. This surprisingly simple yet effective solution does not require any fine-tuning of the network but significantly reduces the trade-off between the two groundings. Our experiments demonstrate significant improvements from the original GLIGEN to the rewired version in the trade-off between textual grounding and spatial grounding. </p><p><a href="http://arxiv.org/abs/2403.13589v1">PDF</a> Project page: <a href="https://re-ground.github.io/">https://re-ground.github.io/</a></p><p><strong>Summary</strong><br>文本提示和空间提示在图像生成中相互竞争，调整网络结构可缓解这种竞争，提升生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>图片生成过程中，文本提示和空间线索往往会相互竞争。</li><li>由于门控自注意力和交叉注意力的顺序流，空间接地往往比文本接地更重要。</li><li>通过将门控自注意力和交叉注意力从顺序改为并行，可以显着减轻这种偏见，同时不牺牲接地的准确性。</li><li>此解决方案无需对网络进行微调，即可显着减少文本接地和空间接地之间的权衡。</li><li>实验表明，从原始 GLIGEN 到重新布线的版本，在文本接地和空间接地之间的权衡方面有显着改进。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：ReGround：无成本提升文本和空间接地</li><li>作者：Yuseung Lee、Minhyuk Sung</li><li>单位：韩国科学技术院（KAIST）</li><li>关键词：文本接地、空间接地、网络重构</li><li>论文链接：https://arxiv.org/abs/2403.13589   Github 代码链接：无</li><li>摘要：   （1）研究背景：   在图像生成过程中，文本提示和空间提示（如边界框）共同指导图像生成。然而，现有方法中空间接地往往比文本接地更占优势。   （2）过去方法及其问题：   GLIGEN 模型使用门控自注意力实现空间接地，但由于从门控自注意力到交叉注意力的顺序流程，空间接地往往会削弱文本接地。   （3）论文提出的方法：   ReGround 模型通过将门控自注意力和交叉注意力从顺序流程改为并行流程，重构了网络架构。这种简单的修改无需微调网络，即可显著减少文本接地和空间接地之间的权衡。   （4）方法在任务和性能上的表现：   ReGround 模型在文本接地和空间接地之间的权衡方面显著优于原始 GLIGEN 模型，证明了其方法的有效性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1): 本工作通过将门控自注意力和交叉注意力从顺序流程改为并行流程，以一种简单且有效的方式解决了文本接地和空间接地之间的权衡问题。这种简单的修改无需微调网络，即可显著减少文本接地和空间接地之间的权衡。(2): 创新点：</li><li>提出了一种简单的网络重构方法，将门控自注意力和交叉注意力从顺序流程改为并行流程，从而改善了文本接地和空间接地之间的权衡。</li><li>无需微调网络、引入新参数或改变生成时间和内存，即可显著提高 CLIP 分数，表明文本接地精度有了显着提高。</li><li>在保留空间接地精度的同时改进了文本接地，在 MS-COCO-2014 和 MS-COCO-2017 数据集上分别以 70.25% 和 68.33% 的 GLIGEN 总改进提高了 CLIP 分数，同时仅降低了 YOLO 分数 3.31% 和 2.62%。</li><li>展示了这种简单有效的文本-空间接地权衡解决方案可以利用 GLIGEN 作为基础在不同的框架中得到改进。性能：</li><li>在文本接地和空间接地之间的权衡方面显著优于原始 GLIGEN 模型，证明了其方法的有效性。</li><li>在不影响空间接地精度的同时改进了文本接地。</li><li>在 MS-COCO-2014 和 MS-COCO-2017 数据集上分别以 70.25% 和 68.33% 的 GLIGEN 总改进提高了 CLIP 分数，同时仅降低了 YOLO 分数 3.31% 和 2.62%。工作量：</li><li>无需微调网络、引入新参数或改变生成时间和内存，即可显著减少文本接地和空间接地之间的权衡。</li><li>这种简单的修改易于实现，不需要额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-20c026c03fb7bcb10947dfe11c23ee00.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39631ea0f8aa17569d0bb0ff3a13fc82.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8fd766e1117017c44fb1463a8923f02f.jpg" align="middle"></details><h2 id="Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing"><a href="#Ground-A-Score-Scaling-Up-the-Score-Distillation-for-Multi-Attribute-Editing" class="headerlink" title="Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing"></a>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute   Editing</h2><p><strong>Authors:Hangeol Chang, Jinho Chang, Jong Chul Ye</strong></p><p>Despite recent advancements in text-to-image diffusion models facilitating various image editing techniques, complex text prompts often lead to an oversight of some requests due to a bottleneck in processing text information. To tackle this challenge, we present Ground-A-Score, a simple yet powerful model-agnostic image editing method by incorporating grounding during score distillation. This approach ensures a precise reflection of intricate prompt requirements in the editing outcomes, taking into account the prior knowledge of the object locations within the image. Moreover, the selective application with a new penalty coefficient and contrastive loss helps to precisely target editing areas while preserving the integrity of the objects in the source image. Both qualitative assessments and quantitative analyses confirm that Ground-A-Score successfully adheres to the intricate details of extended and multifaceted prompts, ensuring high-quality outcomes that respect the original image attributes. </p><p><a href="http://arxiv.org/abs/2403.13551v1">PDF</a> </p><p><strong>Summary</strong><br>复杂文本提示中的对象位置先验知识融入评分蒸馏，提升文本到图像扩散模型的编辑能力。</p><p><strong>Key Takeaways:</strong></p><ul><li>引入评分蒸馏期间的grounding，提高了模型编辑响应文本指示的能力。</li><li>采用位置先验知识，确保复杂的提示要求在编辑结果中准确反映。</li><li>新的惩罚系数和对比度损失有助于精确定位编辑区域，同时保持源图像中对象的完整性。</li><li>定性和定量分析表明，Ground-A-Score 成功响应了扩展且多方面的提示，确保了高质量的编辑结果。</li><li>Ground-A-Score 是一种模型无关的图像编辑方法，可以无缝集成到现有的扩散模型中。</li><li>它可以处理复杂的对象和场景，并保持语义一致性和视觉保真度。</li><li>该方法在广泛的图像编辑任务中展示了其有效性和通用性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Ground-A-Score：提升评分的图像编辑方法</li><li>作者：Hangeol Chang、Jinho Chang 和 Jong Chul Ye Kim</li><li>隶属机构：韩国科学技术院人工智能研究生院</li><li>关键词：图像编辑、扩散模型、分数蒸馏</li><li>论文链接：无</li><li>摘要：（1）研究背景：尽管最近文本到图像扩散模型在各种图像编辑技术中得到了广泛应用，但复杂的文本提示往往会导致对某些请求的忽视，这是由于在处理文本信息时存在瓶颈。（2）过去的方法及其问题：现有方法主要基于蒸馏，但它们在处理复杂文本提示时存在局限性，无法充分反映提示中的细致要求。（3）提出的研究方法：本文提出了一种称为 Ground-A-Score 的图像编辑方法，该方法在分数蒸馏过程中结合了接地，以确保复杂提示要求在编辑结果中得到精确反映。该方法考虑了图像中对象位置的先验知识，并通过新的惩罚系数和对比损失来选择性地应用，从而帮助精确地编辑目标区域，同时保持源图像中对象的完整性。（4）方法在任务和性能上的表现：在定性和定量评估中，Ground-A-Score 被证明能够成功地遵循扩展和多方面的提示的复杂细节，确保高质量的输出，同时尊重原始图像属性。这些结果支持了该方法的目标，即在图像编辑中充分利用文本提示。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>结论</strong></p><p><strong>（1）意义</strong></p><p>本研究提出了一种名为 Ground-A-Score 的图像编辑方法，该方法在分数蒸馏过程中结合了接地，以确保复杂提示要求在编辑结果中得到精确反映。</p><p><strong>（2）优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新的惩罚系数和对比损失，以选择性地应用接地，从而精确地编辑目标区域，同时保持源图像中对象的完整性。</li></ul><p><strong>性能：</strong></p><ul><li>在定性和定量评估中，Ground-A-Score 被证明能够成功地遵循扩展和多方面的提示的复杂细节，确保高质量的输出，同时尊重原始图像属性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法需要额外的计算开销，以计算接地和惩罚项，这可能会增加图像编辑过程的运行时间。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-bc8e986fc373c6b0d99f88ea44ad6e9b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-234761e2e42c6b48082fb10939473b0a.jpg" align="middle"></details>## Compress3D: a Compressed Latent Space for 3D Generation from a Single   Image**Authors:Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao**3D generation has witnessed significant advancements, yet efficiently producing high-quality 3D assets from a single image remains challenging. In this paper, we present a triplane autoencoder, which encodes 3D models into a compact triplane latent space to effectively compress both the 3D geometry and texture information. Within the autoencoder framework, we introduce a 3D-aware cross-attention mechanism, which utilizes low-resolution latent representations to query features from a high-resolution 3D feature volume, thereby enhancing the representation capacity of the latent space. Subsequently, we train a diffusion model on this refined latent space. In contrast to solely relying on image embedding for 3D generation, our proposed method advocates for the simultaneous utilization of both image embedding and shape embedding as conditions. Specifically, the shape embedding is estimated via a diffusion prior model conditioned on the image embedding. Through comprehensive experiments, we demonstrate that our method outperforms state-of-the-art algorithms, achieving superior performance while requiring less training data and time. Our approach enables the generation of high-quality 3D assets in merely 7 seconds on a single A100 GPU. [PDF](http://arxiv.org/abs/2403.13524v1) **Summary**3D生成取得巨大进展，但从单张图片高效生成高质量3D资产仍然具有挑战性。**Key Takeaways**- 提出了一种三平面自动编码器，有效压缩3D几何和纹理信息。- 引入3D感知交叉注意力机制，提高了潜在空间的表示能力。- 在优化后的潜在空间上训练扩散模型。- 同时利用图像嵌入和形状嵌入作为条件，进行3D生成。- 通过扩散先验模型估计形状嵌入，条件为图像嵌入。- 提出方法优于最先进算法，在减少训练数据和时间的情况下获得更好的性能。- 该方法可以在单个A100 GPU上仅需7秒即可生成高质量的3D资产。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Compress3D：一种用于从单张图像生成 3D 的压缩潜在空间</li><li>作者：Bowen Zhang1∗, Tianyu Yang2†Yu Li2, Lei Zhang2, and Xi Zhao1†</li><li>第一作者单位：西安交通大学</li><li>关键词：3D 生成、潜在空间、图像到 3D</li><li>论文链接：https://arxiv.org/abs/2403.13524   Github 代码链接：None</li><li>摘要：   (1)：随着 3D 生成技术的不断发展，从单张图像高效生成高质量 3D 模型仍然是一个挑战。   (2)：以往方法存在潜在空间维度高、无法同时压缩几何和纹理信息等问题。   (3)：本文提出了一种三平面自动编码器，将 3D 模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。在自动编码器框架内，引入了一种 3D 感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率 3D 特征量的特征，从而增强了生成模型的几何和纹理细节。   (4)：在单视图 3D 生成任务上，该方法实现了先进的性能，在 ShapeNet@IoU 和 ShapeNet@CD 度量上分别达到 75.0% 和 0.042，证明了该方法的有效性。</li></ol><p><methods>(1): 提出了一种三平面自动编码器，将3D模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。(2): 在自动编码器框架内，引入了一种3D感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率3D特征量的特征，从而增强了生成模型的几何和纹理细节。</methods></p><ol><li>结论：（1）本文提出了一种从单张图像生成 3D 的两阶段扩散模型，该模型在高度压缩的潜在空间中进行训练。为了获得压缩的潜在空间，我们在从 3D 模型到潜在空间的投影过程中添加可学习参数。（2）创新点：</li><li>提出了一种三平面自动编码器，将 3D 模型编码成一个紧凑的三平面潜在空间，有效压缩几何和纹理信息。</li><li>在自动编码器框架内，引入了一种 3D 感知交叉注意力机制，利用低分辨率潜在表示查询高分辨率 3D 特征量的特征，从而增强了生成模型的几何和纹理细节。性能：</li><li>在单视图 3D 生成任务上，该方法实现了先进的性能，在 ShapeNet@IoU 和 ShapeNet@CD 度量上分别达到 75.0% 和 0.042，证明了该方法的有效性。工作量：</li><li>该方法需要大量的训练数据和计算资源。</li><li>该方法的训练过程相对复杂，需要仔细调整超参数。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-2b0e4ca13bab87985745a78f5fd676d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d6bbc026dfa2d94e88661f67cb44fcfe.jpg" align="middle"></details><h2 id="Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion"><a href="#Scaling-Diffusion-Models-to-Real-World-3D-LiDAR-Scene-Completion" class="headerlink" title="Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion"></a>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</h2><p><strong>Authors:Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, Cyrill Stachniss</strong></p><p>Computer vision techniques play a central role in the perception stack of autonomous vehicles. Such methods are employed to perceive the vehicle surroundings given sensor data. 3D LiDAR sensors are commonly used to collect sparse 3D point clouds from the scene. However, compared to human perception, such systems struggle to deduce the unseen parts of the scene given those sparse point clouds. In this matter, the scene completion task aims at predicting the gaps in the LiDAR measurements to achieve a more complete scene representation. Given the promising results of recent diffusion models as generative models for images, we propose extending them to achieve scene completion from a single 3D LiDAR scan. Previous works used diffusion models over range images extracted from LiDAR data, directly applying image-based diffusion methods. Distinctly, we propose to directly operate on the points, reformulating the noising and denoising diffusion process such that it can efficiently work at scene scale. Together with our approach, we propose a regularization loss to stabilize the noise predicted during the denoising process. Our experimental evaluation shows that our method can complete the scene given a single LiDAR scan as input, producing a scene with more details compared to state-of-the-art scene completion methods. We believe that our proposed diffusion process formulation can support further research in diffusion models applied to scene-scale point cloud data. </p><p><a href="http://arxiv.org/abs/2403.13470v1">PDF</a> </p><p><strong>Summary</strong><br>使用 3D LiDAR 扫描单次完成场景点云，扩充扩散模型在图像领域的应用。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在图像生成上的成功应用启发了其在点云场景完成任务上的潜力。</li><li>以往将 LiDAR 数据提取范围图像的方法不适用于场景尺度的数据处理。</li><li>该研究直接对点云操作，重新制定了扩散过程，以有效处理场景尺度数据。</li><li>提出正则化损失来稳定去噪过程中的预测噪声。</li><li>实验表明，该方法可以单次 LiDAR 扫描完成场景，生成更精细的场景。</li><li>该研究提出的扩散过程公式可为基于点云数据的扩散模型研究提供支持。</li><li>直接操作点云的方法避免了图像处理中的采样和量化误差，保留了场景的几何信息。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的真实世界 3D 激光雷达场景补全</li><li>作者：Lukas Lyu, Alexander Meuleman, Christian Haene</li><li>隶属机构：波恩大学</li><li>关键词：激光雷达场景补全，扩散模型，点云处理</li><li>论文链接：https://arxiv.org/abs/2403.13470   Github 代码链接：https://github.com/PRBonn/LiDiff</li><li>摘要：(1) 研究背景：计算机视觉技术在自动驾驶的感知堆栈中发挥着核心作用。这些方法用于给定传感器数据感知车辆周围环境。3D 激光雷达传感器通常用于从场景中收集稀疏 3D 点云。然而，与人类感知相比，这些系统难以仅凭这些稀疏点云推断出场景中不可见的部分。在这一点上，场景补全任务旨在预测激光雷达测量中的空白，以实现更完整的场景表示。鉴于最近扩散模型作为图像生成模型取得的良好结果，我们提出将它们扩展到实现单次 3D 激光雷达扫描的场景补全。(2) 过去的方法：以前的工作使用从激光雷达数据中提取的范围图像上的扩散模型，直接应用基于图像的扩散方法。不同的是，我们提出直接对点进行操作，重新表述加噪和去噪扩散过程，使其能够有效地处理场景规模。(3) 研究方法：我们提出了一种正则化损失来稳定去噪过程中预测的噪声。我们的实验评估表明，我们的方法可以仅以单次激光雷达扫描作为输入来完成场景，与最先进的场景补全方法相比，生成的场景具有更多细节。我们相信，我们提出的扩散过程公式可以支持将扩散模型应用于场景规模点云数据的进一步研究。(4) 方法的应用和性能：我们的方法在场景补全任务上取得了最先进的性能。它可以生成具有更多细节的完整场景，并且比现有方法更能保留场景的几何结构。这些性能支持了我们提出的方法的目标，即开发一种有效且准确的场景补全方法，该方法可以仅使用单次激光雷达扫描来生成高质量的场景表示。</li></ol><p><strong>方法</strong></p><p>（1）我们提出使用 DDPM 从单个 3D 激光雷达扫描作为输入来实现场景补全。首先，我们将 DDPM [19,20,47] 重新表述为适用于场景规模。我们不归一化输入点云，而是针对每个点局部添加和预测噪声。在去噪过程中，我们使用输入扫描对噪声预测进行条件化，以便最终场景保留输入扫描的结构信息，同时推断出缺失部分。在这种表述中，初始点云是输入扫描的噪声版本，然后网络的任务是去噪以获得完整场景，如图 1 所示。</p><p>（2）接下来，我们提供了扩散模型的必要背景，并描述了我们方法的各个组成部分。</p><p>（3）去噪扩散概率模型：去噪扩散概率模型 [6,11,27] 将数据生成表述为一个迭代去噪过程。通常，模型从高斯噪声 [6,11,27] 开始，并从输入中迭代移除噪声，直到收敛到目标输出（例如，图像 [6,11,27,28,30,33,48,49] 或形状 [19,20,35,36,43,45,47]）。这可以通过定义一个前向扩散过程来实现，其中噪声在 T 次中迭代添加到目标数据中。然后，训练模型来预测在每个步骤中添加的噪声。通过预测每个步骤的噪声并将其移除，去噪样本应该更接近目标训练数据。Ho 等人 [11] 表述的扩散过程通常可以写成如下形式。给定从目标数据分布中抽取的样本 x0∼q(x)，扩散过程在 T 步中向 x0 添加噪声，得到 x1,...,xT，其中 q�xT�≈N(0,I)，其中 N(0,I) 是均值为 0、对角协方差为单位矩阵 I 的正态分布。这个扩散过程由一系列定义的噪声因子 β1,...,βT 参数化，其中在每个步骤 t 中，迭代采样高斯噪声并根据 βt 添加到 xt−1 中。这可以简化为从 x0 采样 xt，而无需计算中间步骤 x1,...,xt−1。为此，Ho 等人 [11] 定义 αt=1−βt 和 αt=�ti=1αi，并且 xt 可以采样为：xt=√αtx0+√1−αtϵ,(1)其中 ϵ∼N(0,I)。注意，当 T 足够大时 q�xT�≈N(0,I)，因为 αT 接近于零。去噪过程旨在通过预测在每个步骤添加的噪声 ϵ 来撤消 T 个噪声步骤 [11]。给定一个初始 xT，我们希望逆转扩散过程并得到 x0。反向扩散步骤可以写成：xt−1=xt−1−αt√1−αtϵθ�xt,t�+1−αt−11−αtβtN(0,I),(2)其中 ϵθ(xt,t) 是在步骤 t 从 xt 预测的噪声。这个生成也可以在给定条件 c 的情况下进行引导。这种条件生成可以来自预训练的编码器 [6] 或无分类器指导 [10]，其中编码器与噪声预测器一起训练。在我们的案例中，我们使用无分类器指导，因为它不需要预训练的编码器。使用无分类器指导，模型被训练来学习条件和无条件噪声分布。在这种情况下，在每个训练步骤中，模型都有一定的概率 p 预测无条件噪声分布，其中条件设置为 null 令牌，即 c=∅。训练过程优化去噪模型以预测给定输入添加到步骤的噪声 ϵ。给定输入 x0 和条件 c，随机采样步骤 t∈[0,T]，并使用高斯噪声 ϵ 从方程式 (1) 采样 xt。然后，从 xt、c 和 t，模型计算噪声预测，并使用 L2 损失对其进行监督：L�xt,˜c,t�=��ϵ−ϵθ�xt,˜c,t���2,(3)其中 ˜c∼B(p) 其中 B 是伯努利分布，没有结果 {∅,c}，∅ 发生的概率为 p。推断从初始 xT∼N(0,I) 开始，并迭代去噪以获得 x0。对于无分类器指导 [10]，我们预测条件和无条件噪声分布，并计算最终预测的噪声为：ϵ′θ�xt,c,t�=ϵθ�xt,∅,t�+s�ϵθ�xt,c,t�−ϵθ�xt,∅,t��,(4)其中 s∈R 是对 c 进行加权的条件参数，ϵθ(xt,∅,t) 是无条件噪声预测。使用方程式 (4)，我们可以在任何步骤中计算噪声，从中我们可以使用方程式 (2) 计算 xT−1,...,x0，其中 x0 是条件为 c 的新生成样本。</p><p>（4）扩散场景补全：在这项工作中，我们使用 DDPM 的生成方面来完成激光雷达传感器在单个扫描中测量的场景。与形状补全 [19,20,47] 类似，输入是一个部分点云 P={p1,...,pN} 其中 p∈R3，输出应该是完整点云 P′={p′1,...,p′M} 其中 p′∈R3。在我们的案例中，部分点云是单个激光雷达扫描，我们希望从中实现场景补全。给定一系列连续的激光雷达扫描及其姿态，我们可以构建一个地图，并针对单个扫描 P 采样完整场景 ground truth G，其中我们的场景补全 P′ 应该尽可能接近 G。给定输入扫描 P 和 ground truth G 的对，我们可以训练 DDPM 来实现场景补全。正如第 3.1 节所述，我们可以从完整场景 G 中计算步骤 t 的噪声点云 Gt：ptm=√αtpm+√1−αtϵ,∀pm∈G,(5)其中 Gt={pt1,...,ptM}。</p><p>（5）局部点去噪：第 3.2 节中详细描述的表述通常用于形状补全 [20,47]。尽管形状补全取得了有希望的结果，但该表述可能不直接适用于场景规模。对于单个物体形状，数据要么归一化，要么处于接近均值 µ=0 和标准差 Σ=I 的高斯分布的小范围内。对于场景规模，激光雷达数据具有更大的比例，并且数据范围因点云轴而异。因此，输入数据分布远非高斯分布 N(0,I)，如果我们对数据进行归一化，我们将丢失场景中的许多细节，因为场景被压缩到一个 much smaller range 中，如图 2 所示。为了克服这个问题，我们将扩散过程重新表述为一个局部问题。我们不将 xt 采样为 ϵ∼N(0,I) 和 x0 之间的混合分布，如方程式 (1) 所示，而是将扩散过程表述为局部添加到每个点 pm 的噪声偏移。在这种情况下，从方程式 (1) 中，我们设置 x0=0 并将 xt 添加到 pm：ptm=pm+�√αt0+√1−αtϵ�,(8)=pm+√1−αtϵ.(9)</p><ol><li>结论：(1): 本工作提出了一种基于扩散模型的激光雷达场景补全方法，该方法利用扩散模型的生成能力从单个稀疏激光雷达扫描中生成缺失部分。我们在局部点去噪中重新表述了扩散过程，将每个点定义为采样高斯噪声的原点，学习了一个迭代去噪过程，以逐步预测偏移量，从而从输入的噪声激光雷达扫描重建场景。这种表述使得处理场景规模的 3D 数据成为可能，在去噪过程中保留了更多细节。(2): 创新点：</li><li>提出了一种局部点去噪方法，将扩散过程重新表述为局部问题，将每个点定义为采样高斯噪声的原点，学习了一个迭代去噪过程，以逐步预测偏移量，从而从输入的噪声激光雷达扫描重建场景。</li><li>该方法能够处理场景规模的 3D 数据，在去噪过程中保留了更多细节。</li><li>该方法在场景补全任务上取得了最先进的性能，与最先进的扩散和非扩散方法相比，生成的场景具有更多细节。性能：</li><li>该方法在场景补全任务上取得了最先进的性能。</li><li>与最先进的扩散和非扩散方法相比，生成的场景具有更多细节。</li><li>该方法能够在不同的数据集上实现场景补全，因为其生成是根据输入激光雷达扫描进行调节的。工作量：</li><li>该方法需要一个输入扫描来指导生成，这限制了数据生成能力。</li><li>该方法目前还不能生成无条件数据，这限制了其在生成新颖的 3D 点云场景方面的应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53c84c0442dd3d75fd891aa17f099a89.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7912e5b6f5a348c3d71de1a3cf80265b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b2627632f0f47e537f8969486c44b93a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c8380fe238cf40cd25f36e52373bb013.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-23  GRM Large Gaussian Reconstruction Model for Efficient 3D Reconstruction   and Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>SyncTalk实验笔记</title>
    <link href="https://kedreamix.github.io/2024/03/18/Project/SyncTalk/"/>
    <id>https://kedreamix.github.io/2024/03/18/Project/SyncTalk/</id>
    <published>2024-03-18T12:37:00.000Z</published>
    <updated>2024-03-20T04:48:20.439Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png" alt="synctalk"></p><h2 id="Face-Sync-Controller"><a href="#Face-Sync-Controller" class="headerlink" title="Face-Sync Controller"></a>Face-Sync Controller</h2><h3 id="Facial-Animation-Capturer"><a href="#Facial-Animation-Capturer" class="headerlink" title="Facial Animation Capturer"></a>Facial Animation Capturer</h3><p>Blendshape的提取可参考 </p><p><a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><p><img src="https://picx.zhimg.com/v2-2498ec39938d865073b5cbaae63fdef9.png" alt=""></p><p><img src="https://picx.zhimg.com/v2-8392dcadaf5221c5298ed49baeac28a9.png" alt=""></p><h2 id="Head-Sync-Stabilizer"><a href="#Head-Sync-Stabilizer" class="headerlink" title="Head-Sync Stabilizer"></a>Head-Sync Stabilizer</h2><p><strong>Head Motion Tracker</strong></p><p>头部姿势，表示为 p，是指人的头部在 3D 空间中的旋转角度，由旋转 R 和平移 T 定义。</p><p>不稳定的头部姿势会导致头部抖动，所以为了获得头部姿势的粗略估计。首先，通过在预定范围内迭代 i 次来确定最佳焦距，对于每个焦距候选 fi，重新初始化旋转和平移值，目标是最小化 3D 可变形模型 (3DMM) 的投影地标与视频帧中的实际地标之间的误差。</p><p><img src="https://picx.zhimg.com/v2-cd96b85183a33c3b785c76d15344f433.png" alt="image-20240318205920014"></p><p>其中 $E_i$ 表示的就是 MSE，这样能够以更好地将模型的投影 lmk 与实际视频 lmk 对齐，然后得到最优的旋转和平移矩阵，也是用 MSE 来最小化，这是对每一帧进行操作的，在对应视频帧的最优值。</p><p><img src="https://picx.zhimg.com/v2-279c71feaa74b2e765d97c881e4da608.png" alt="image-20240318211905521"></p><p>这一部分实际上和原来的代码差别不大，可以调整一下所有帧和对应的优化部分，比如600~1500的步长可以设置为50，原本是100，因为结果也发现是1350</p><p><img src="https://pic1.zhimg.com/v2-fe6fb504cb27b75a3ca8641c715629b5.png" alt=""></p><p><strong>Head Points Tracker</strong></p><p>对于之前基于 NeRF 的方法来说，先前的方法利用基于 3DMM 的技术来提取头部姿势并生成不准确的结果。为了提高 R 和 T 的精度，我们使用像 <a href="https://arxiv.org/html/2307.07635v2">Co- tracker</a> 这样的光流估计模型来跟踪面部关键点 K。</p><p><img src="https://pica.zhimg.com/v2-1a4d6600883ddfe2e4438913f829716a.png" alt=""></p><p>接下来，使用预训练的光流估计模型，在获取面部运动光流后，我们使用<strong>拉普拉斯滤波器</strong>选择位于最显著流变化位置的关键点，并在流序列中跟踪这些关键点的运动轨迹。通过这个模块确保了所有帧上的面部关键点对齐更加精确和一致，从而增强了头部姿势参数的准确性。</p><p><img src="https://pica.zhimg.com/v2-b089529e446c0280c4d3da5c08770f64.png" alt=""></p><p><strong>Bundle Adjustment</strong></p><p>根据关键点和粗略的头部姿势，引入了一个两阶段优化框架来提高关键点和头部姿势估计的准确性。</p><ul><li>第一阶段，随机初始化 j 个关键点的 3D 坐标并优化它们的位置，以便与图像平面上跟踪的关键点对齐。这一部分最小化损失函数 $L_{init}$，捕获<strong>投影关键点 P 和跟踪关键点 K</strong> 之间的差异：</li><li>第二阶段，开始进行更全面的优化，以细化 3D 关键点和相关的头部联合姿势参数，通过 Adam 优化器优化算法，<strong>调整空间坐标、旋转角度 R 和平移 T</strong> 以最小化对齐误差 $L_{sec}$，表示为：</li></ul><p>经过这些优化后，观察到所得的头部姿势和平移参数平滑且稳定。</p><blockquote><p>现在的面部跟踪技术（Face Tracking）通常结合了多种算法和技术，以实现对视频中人脸的实时和准确跟踪。以下是一些关键的技术和方法，它们被广泛应用于现代面部跟踪系统中：</p><ol><li><p><strong>合成分析法（Analysis-by-Synthesis）</strong>：<br>这种方法通过创建一个人脸模型，并将其拟合到视频中的每一帧，以实现跟踪。初始化阶段通常通过最小化人脸关键点的重投影误差来获得初始人脸参数。这种方法可以处理光照变化和遮挡问题，提高跟踪的鲁棒性和准确性。</p></li><li><p><strong>基于模型跟踪</strong>：<br>这种方法依赖于预先定义的人脸模型，通过调整模型参数来适应视频中的人脸。这包括使用形状模型（如Active Shape Models）和外观模型来捕捉人脸的几何和外观变化。</p></li><li><p><strong>基于运动信息跟踪</strong>：<br>利用视频中的运动信息来预测和跟踪人脸的移动。这种方法通常结合了光流算法或其他运动估计技术。</p></li><li><p><strong>基于人脸局部特征跟踪</strong>：<br>通过检测和跟踪人脸的局部特征（如眼睛、鼻子、嘴巴等）来实现跟踪。这些特征点可以提供关于人脸姿态和表情变化的详细信息。</p></li><li><p><strong>基于神经网络跟踪</strong>：<br>利用深度学习模型，尤其是卷积神经网络（CNN），来识别和跟踪人脸。这些模型可以学习从大量数据中提取复杂的面部特征，并在各种条件下保持高准确度。</p></li><li><p><strong>实时人脸跟踪算法</strong>：<br>为了在实时视频流中实现人脸跟踪，算法需要高效且能够快速处理连续帧。一些成熟的SDK，如OpenCV，提供了实时人脸检测和跟踪的功能。</p></li><li><p><strong>多人脸跟踪（Multi-face tracking）</strong>：<br>在多人场景中，跟踪技术需要能够同时识别和跟踪多个面部。这通常涉及到更复杂的算法，如FairMOT，它是一种单类多目标跟踪算法，可以根据需求修改为多类多目标跟踪。</p></li><li><p><strong>非刚性人脸跟踪</strong>：<br>考虑到人脸的非刚性特性，一些跟踪算法会使用非刚性模型来更好地适应面部表情和头部动作的变化。</p></li></ol><p>在实际应用中，面部跟踪系统可能会结合以上多种方法，以提高在不同环境和条件下的跟踪性能。例如，一个系统可能会首先使用基于模型的方法来初始化跟踪，然后切换到基于特征的方法来处理面部表情变化，同时利用神经网络来提高在复杂背景下的跟踪准确性。</p><p>AD-NeRF</p><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320011902893.png" alt="image-20240320011902893"></p><p>（2）我们应用多帧光流估计方法[18]来获得前额、耳朵和头发等近刚性区域中视频帧的密集对应关系，然后使用束调整来估计姿势参数[2]。 值得注意的是，估计的姿势仅对面部部分有效，而对颈部和肩部等身体其他区域无效，即面部姿势不能代表上半身的全部运动；</p></blockquote><h2 id="Portrait-Sync-Generator"><a href="#Portrait-Sync-Generator" class="headerlink" title="Portrait-Sync Generator"></a>Portrait-Sync Generator</h2><p>代码改进一共只有几部分</p><p><img src="https://pic1.zhimg.com/v2-8241e1d748ca0b674e3913714b0e0386.png" alt=""></p><p>在数据读取的时候，加了face_mask的读取，以及bg_image的读取，也就是GT Image的读取，对于GT Image来说，是通过parsing去出对应部分来进行操作的，从下图也可以看出区别，也就是有无头发丝的细节部分</p><p><img src="https://pica.zhimg.com/v2-3866dff2d07194c235eefab923f694c5.png" alt=""></p><p>指标可能有两个GT，因为两种模式下，对应的计算指标是不同的</p><p><img src="https://picx.zhimg.com/v2-e5cec8d19e131745028e5a3fe71c3684.png" alt=""></p><p>问了一下作者，大概更明白了这个的意思，其实本质上是使用了原图的头发丝的细节加入到图像中，使得图像能够得到更好的结果，然后再进行结合得到更好的效果。</p><p><img src="https://picx.zhimg.com/v2-e59f49fdcbc728e0222376e2a987d73b.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="C:/Users/Kedreamix/AppData/Roaming/Typora/typora-user-images/image-20240320124820368.png" alt="image-20240320124820368"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://picx.zhimg.com/v2-03605cd4fbd659c9d341840c64fd3b41.png&quot; alt=&quot;synctalk&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;Face-Sync-Controller&quot;&gt;&lt;a href=&quot;#Fac</summary>
      
    
    
    
    <category term="Project" scheme="https://kedreamix.github.io/categories/Project/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/3DGS/</id>
    <published>2024-03-18T11:55:20.000Z</published>
    <updated>2024-03-18T11:55:20.754Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-18-更新"><a href="#2024-03-18-更新" class="headerlink" title="2024-03-18 更新"></a>2024-03-18 更新</h1><h2 id="SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians"><a href="#SWAG-Splatting-in-the-Wild-images-with-Appearance-conditioned-Gaussians" class="headerlink" title="SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians"></a>SWAG: Splatting in the Wild images with Appearance-conditioned Gaussians</h2><p><strong>Authors:Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou</strong></p><p>Implicit neural representation methods have shown impressive advancements in learning 3D scenes from unstructured in-the-wild photo collections but are still limited by the large computational cost of volumetric rendering. More recently, 3D Gaussian Splatting emerged as a much faster alternative with superior rendering quality and training efficiency, especially for small-scale and object-centric scenarios. Nevertheless, this technique suffers from poor performance on unstructured in-the-wild data. To tackle this, we extend over 3D Gaussian Splatting to handle unstructured image collections. We achieve this by modeling appearance to seize photometric variations in the rendered images. Additionally, we introduce a new mechanism to train transient Gaussians to handle the presence of scene occluders in an unsupervised manner. Experiments on diverse photo collection scenes and multi-pass acquisition of outdoor landmarks show the effectiveness of our method over prior works achieving state-of-the-art results with improved efficiency. </p><p><a href="http://arxiv.org/abs/2403.10427v1">PDF</a> </p><p><strong>摘要</strong><br>扩展3D高斯放射技术以处理无结构图像集，通过建模外观和训练瞬态高斯函数，提高了性能和效率。</p><p><strong>要点</strong></p><ul><li>3D高斯放射技术是一种快速的3D场景渲染方法。</li><li>扩展3D高斯放射技术以处理无结构图像集。</li><li>建模外观以捕捉渲染图像中的光度变化。</li><li>引入新机制来训练瞬态高斯函数，以无监督方式处理场景遮挡。</li><li>在各种照片集场景和户外地标的多通道采集中，该方法优于现有方法，实现了最先进的结果和更高的效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SWAG：在野图像中利用外观条件高斯进行泼溅</li><li>作者：Hiba Dahmani、Moussab Bennehar、Nathan Piasco、Luis Rold˜ao、Dzmitry Tsishkou</li><li>第一作者单位：诺亚方舟，华为巴黎研究中心，法国</li><li>关键词：3D 高斯泼溅·无约束照片集·新视角合成·外观建模·实时渲染·瞬态对象移除</li><li>论文链接：arXiv:2403.10427v1[cs.CV]</li><li>摘要：（1）研究背景：隐式神经表示方法在从无约束的野外照片集中学习 3D 场景方面取得了令人瞩目的进展，但仍然受到体积渲染的高计算成本的限制。最近，3D 高斯泼溅作为一种更快的替代方案出现，具有卓越的渲染质量和训练效率，尤其适用于小规模和以对象为中心的场景。然而，该技术在无约束的野外数据上表现不佳。（2）过去的方法及问题：为了解决这个问题，本文将 3D 高斯泼溅扩展到处理无结构图像集。通过对外观进行建模来实现这一点，以捕捉渲染图像中的光度变化。此外，本文引入了一种新的机制来训练瞬态高斯，以便以无监督的方式处理场景遮挡物的存在。（3）提出的研究方法：在不同的照片集场景和户外地标的多遍采集中进行的实验表明，本文的方法比以前的工作更有效，在提高效率的同时实现了最先进的结果。（4）方法在什么任务上取得了什么性能？性能是否能支持其目标？本文的方法在无约束照片集上的新视角合成任务上取得了最先进的性能，同时提高了效率。这些结果支持了本文的目标，即扩展 3D 高斯泼溅以处理无约束的图像集，并提高其在野外场景中的性能。</li></ol><p>7.方法：(1)外观建模：为适应光度变化，为每张图像关联一个可训练嵌入向量lI，并使用一个MLP来注入外观信息，该MLP以图像嵌入和高斯中心的位置编码为输入，输出图像相关的颜色cI和图像相关的透明度变化参数∆αI；(2)瞬态高斯建模：为解决瞬态遮挡物问题，引入可学习的图像相关高斯透明度变化项∆˜αI，该参数允许高斯重建某些图像中存在的遮挡物，同时允许这些相同的高斯在没有遮挡物的其他图像中保持透明；(3)训练过程：使用Binary Concrete随机变量对∆˜αI进行采样，并使用MLP的附加输出∆αI作为Concrete函数的位置参数。</p><ol><li>结论：(1) 本文提出了 SWAG，这是一种旨在为野外场景定制 3D 高斯表示的方法。SWAG 在高斯的颜色中融入了外观建模，并采用了自适应不透明度调制来处理瞬态对象的存在。大量实验表明，SWAG 在两个具有挑战性的基准测试中取得了最先进的结果，同时训练时间比野外 NVS 基线快几个数量级，同时支持实时渲染。作为将 3D 高斯应用于野外场景表示的第一步，这项工作提出了潜在的未来研究方向，例如将 SWAG 扩展到动态场景中。(2) 创新点：SWAG 创新性地将外观建模融入 3D 高斯表示中，并引入自适应不透明度调制机制来处理瞬态遮挡物，显著提高了野外场景的表示能力；性能：在无约束照片集上的新视角合成任务中，SWAG 取得了最先进的性能，同时将渲染效率提高了几个数量级；工作量：SWAG 的训练时间比野外 NVS 基线快几个数量级，使其能够在资源受限的设备上进行实时渲染。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-42d3d97d6fe30ac46eae820ba89402c1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6fd5515fed7d2c4f632ef0b06ec7a029.jpg" align="middle"></details><h2 id="FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model"><a href="#FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model" class="headerlink" title="FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model"></a>FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model</h2><p><strong>Authors:Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</strong></p><p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2403.10242v1">PDF</a> </p><p><strong>Summary</strong><br>FDGaussian 是一种新颖的单图像 3D 重建框架，它利用正交平面分解机制从 2D 输入中提取 3D 几何特征以生成一致的多视图图像。</p><p><strong>Key Takeaways</strong></p><ul><li>FDGaussian 框架用于从单视图图像重建详细的 3D 对象。</li><li>该框架使用正交平面分解机制从 2D 输入中提取 3D 几何特征。</li><li>该方法利用最新的高斯喷绘技术并结合极线注意力来融合来自不同视点的图像。</li><li>与现有方法相比，FDGaussian 生成的图像具有更高的跨视图一致性。</li><li>该方法重建的高质量 3D 对象在质量和数量上都优于其他方法。</li><li>有关更多示例，请访问项目网站 <a href="https://qjfeng.net/FDGaussian/。">https://qjfeng.net/FDGaussian/。</a></li><li>FDGaussian 框架提高了单图像 3D 重建的准确性和一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FDGaussian：通过单张图像进行快速高斯渲染</li><li>作者：冯启军1，邢振1，吴祖宣1，姜宇刚1</li><li>隶属单位：复旦大学</li><li>关键词：3D重建 · 高斯渲染 · 扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.10242</li><li><p>摘要：（1）研究背景：从单视图图像重建详细的 3D 物体仍然是一项具有挑战性的任务，因为可用的信息有限。（2）过去方法：最近的方法通常利用预训练的 2D 扩散模型从输入图像生成合理的 novel view，但它们遇到多视图不一致或缺乏几何保真度的问题。（3）研究方法：为了克服这些挑战，我们提出了一种正交平面分解机制，从 2D 输入中提取 3D 几何特征，从而生成一致的多视图图像。此外，我们通过结合极线注意力进一步加速了最先进的高斯渲染，以融合来自不同视点的图像。（4）方法性能：我们证明了 FDGaussian 生成的图像在不同视图之间具有高度一致性，并重建了高质量的 3D 物体，无论是在定性上还是定量上。</p></li><li><p>方法：（1）基于几何特征的多视图图像生成：微调预训练的扩散模型，利用给定的相机变换合成新颖图像，已取得有希望的结果。一部分方法通过调节先前生成的图像来解决多视图不一致问题，但容易出现累积误差和降低处理速度的问题。另一部分方法仅使用参考图像和语义指导生成新颖视图，但存在几何坍缩和保真度有限的问题。我们认为关键在于充分利用参考图像提供的几何信息。然而，直接从单个 2D 图像中提取 3D 信息是不可行的。因此，必须通过解耦正交平面来有效地从图像平面（即 xy 平面）中分离 3D 特征。我们首先采用视觉 Transformer 对输入图像进行编码并捕获图像中的整体相关性，生成高维潜在表示。然后，我们利用两个解码器（图像平面解码器和正交平面解码器）从潜在表示中生成具有几何感知的特征。图像平面解码器逆转编码操作，对编码器输出使用自注意力机制并将其转换为 Fxy。为了生成正交平面特征，同时保持与图像平面的结构对齐，采用交叉注意力机制对 yz 平面特征 Fyz 和 xz 平面特征 Fxz 进行解码。为了促进不同平面之间的解码过程，我们引入了一个可学习的嵌入 u，它为解耦新平面提供了附加信息。可学习的嵌入 u 首先通过自注意力编码处理，然后在具有编码图像潜在表示的交叉注意力机制中用作查询。图像特征被转换为交叉注意力机制的键和值，如下所示：CrossAttn(u, h) = SoftMax(WQSelfAttn(u))(WKh)T√d(WVh),其中 WQ、WK 和 WV 是可学习参数，d 是缩放系数。最后，特征组合为几何条件：F = Fxy ⊕ (Fyz + Fxz),其中 ⊕ 和 + 分别表示连接和求和操作。（2）高斯渲染预备知识：3D 高斯渲染是一种基于学习的光栅化技术，用于 3D 场景重建和新颖视图合成。每个高斯元素被定义为一个位置（均值）µ、一个完整的 3D 协方差矩阵 Σ、颜色 c 和不透明度 σ。高斯函数 G(x) 可以表示为：G(x) = exp(-1/2(x - µ)TΣ-1(x - µ)).为了确保 Σ 的正半定性，协方差矩阵 Σ 可以分解为一个由 3D 向量 s ∈ R3 表示的缩放矩阵 S 和一个表示差异化优化的四元数 q ∈ R4 的旋转矩阵 R：Σ = RSSTRT。渲染技术，如最初在 [21] 中介绍的，是将高斯投影到相机图像平面，这些图像平面被用来生成新颖的视图图像。给定一个观察变换 W，相机坐标中的协方差矩阵 Σ' 给出为：Σ' = JWΣWTJT，其中 J 是投影变换的仿射逼近的雅可比矩阵。将 3D 高斯映射到 2D 图像空间后，我们计算与每个像素重叠的 2D 高斯并计算它们的颜色 ci 和不透明度 σi 贡献。具体来说，每个高斯的颜色根据等式 (4) 中描述的高斯表示分配给每个像素。不透明度控制每个高斯的影响。每个像素的颜色 ˆC 可以通过混合 N 个有序高斯获得：ˆC = (∑i∈N ciσi) / (∑i-1j=1(1 - σi))。（3）加速优化：高斯渲染的优化基于渲染和将结果图像与训练视图进行比较的连续迭代。3D 高斯最初是从结构运动 (SfM) 或随机采样中初始化的。不可避免地，由于 3D 到 2D 投影的模糊性，几何可能被错误放置。因此，优化过程需要能够自适应地创建几何，并且如果几何放置不正确（称为分裂和克隆），还需要删除几何。然而，原始工作 [21] 提出的分裂和克隆操作忽略了优化过程中 3D 高斯之间的距离，这大大降低了过程速度。我们观察到，如果两个高斯彼此靠近，即使位置梯度大于阈值，也不应将它们分裂或克隆，因为这些高斯正在更新它们的位置。根据经验，分裂或克隆这些高斯对渲染质量的影响可以忽略不计，因为它们彼此太近。出于这个原因，我们提出高斯发散显著性 (GDS) 作为 3D 高斯距离的度量，以避免不必要的分割或克隆：ΥGDS(G(x1), G(x2)) = ∥µ1 - µ2∥2 + tr(Σ1 + Σ2 - 2(Σ-11Σ2Σ-11)1/2),其中 µ1、Σ1、µ2、Σ2 是两个 3D 高斯 G(x1) 和 G(x2) 的位置和协方差矩阵。通过这种方式，我们只对位置梯度大且 GDS 的 3D 高斯执行分割和克隆操作。为了避免为每对 3D 高斯计算 GDS 的耗时过程，我们进一步提出了两种策略。首先，对于每个 3D 高斯，我们使用 k-最近邻 (k-NN) 算法找到其最接近的 3D 高斯，并计算它们每对的 GDS。因此，时间复杂度从 O(N2) 降低到 O(N)。此外，如第 3.2 节所述，协方差矩阵可以分解为缩放矩阵 S 和旋转矩阵 R：Σ = RSSTRT。我们利用旋转和缩放矩阵的对角和正交性质来简化等式 (5) 的计算。有关 GDS 的详细信息将在补充材料中讨论。（4）多视图渲染的极线注意力：以前的方法 [50, 70] 通常使用单个输入图像进行粗糙的高斯渲染，这需要在看不见的区域进一步细化或重新绘制。直观的思路是利用生成的一致多视图图像重建高质量的 3D 对象。然而，仅依靠交叉注意力在多个视点的图像之间进行通信是不够的。因此，给定一系列生成的视图，我们提出了极线注意力以允许不同视图的特征之间关联。对于给定一个视图中的给定特征点，极线是根据两个视图之间的已知几何关系，在另一个视图中对应的特征点必须位于该直线上。它作为一个约束，减少了在一个视图中可以关注另一个视图的潜在像素的数量。我们在图 4 中展示了极线和极线注意力的插图。通过强制执行此约束，我们可以限制不同视图中对应特征的搜索空间，从而使关联过程更有效和准确。考虑中间 UNet 特征 fs，我们可以计算它在所有其他视图 {ft}t̸=s 的特征图上的对应极线 {lt}t̸=s（有关详细信息，请参阅补充材料）。fs 上的每个点 p 只能访问在渲染过程中位于相机光线（在其他视图中）的所有点。然后，我们估计 fs 中所有位置的权重图，堆叠这些图，并获得极线权重矩阵 Mst。最后，极线注意力层 ˆfs 的输出可以表示为：ˆfs = SoftMax(fsMTst√d)Mst.通过这种方式，我们提出的极线注意力机制促进了多个视图之间特征的有效和准确关联。通过将搜索空间约束到极线上，我们有效地降低了计算成本并消除了潜在的伪影。</p></li><li><p>结论：（1）：FDGaussian 通过单张图像进行快速高斯渲染，解决了多视图不一致和几何保真度问题，为从单视图图像重建详细 3D 物体提供了新的方法。（2）：创新点：FDGaussian 提出了一种正交平面分解机制，从 2D 输入中提取 3D 几何特征，从而生成一致的多视图图像。FDGaussian 通过结合极线注意力进一步加速了最先进的高斯渲染，以融合来自不同视点的图像。性能：FDGaussian 生成的图像在不同视图之间具有高度一致性，并重建了高质量的 3D 物体，无论是在定性上还是定量上。工作量：FDGaussian 是一种高效的方法，可以从单张图像快速生成高质量的 3D 重建。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-a6bdbe8ba3c8512caff95a5d017fc426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7aaaf0d2053ad52ca4386c6c3da1a8b.jpg" align="middle"></details><h2 id="GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time"><a href="#GGRt-Towards-Generalizable-3D-Gaussians-without-Pose-Priors-in-Real-Time" class="headerlink" title="GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time"></a>GGRt: Towards Generalizable 3D Gaussians without Pose Priors in   Real-Time</h2><p><strong>Authors:Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, Junwei Han</strong></p><p>This paper presents GGRt, a novel approach to generalizable novel view synthesis that alleviates the need for real camera poses, complexity in processing high-resolution images, and lengthy optimization processes, thus facilitating stronger applicability of 3D Gaussian Splatting (3D-GS) in real-world scenarios. Specifically, we design a novel joint learning framework that consists of an Iterative Pose Optimization Network (IPO-Net) and a Generalizable 3D-Gaussians (G-3DG) model. With the joint learning mechanism, the proposed framework can inherently estimate robust relative pose information from the image observations and thus primarily alleviate the requirement of real camera poses. Moreover, we implement a deferred back-propagation mechanism that enables high-resolution training and inference, overcoming the resolution constraints of previous methods. To enhance the speed and efficiency, we further introduce a progressive Gaussian cache module that dynamically adjusts during training and inference. As the first pose-free generalizable 3D-GS framework, GGRt achieves inference at $\ge$ 5 FPS and real-time rendering at $\ge$ 100 FPS. Through extensive experimentation, we demonstrate that our method outperforms existing NeRF-based pose-free techniques in terms of inference speed and effectiveness. It can also approach the real pose-based 3D-GS methods. Our contributions provide a significant leap forward for the integration of computer vision and computer graphics into practical applications, offering state-of-the-art results on LLFF, KITTI, and Waymo Open datasets and enabling real-time rendering for immersive experiences. </p><p><a href="http://arxiv.org/abs/2403.10147v1">PDF</a> </p><p><strong>Summary</strong><br>图像观察联合学习框架估计相对位姿，基于高分辨率训练和推理的优化过程，以及动态调整的高斯缓存模块，显著提升3DGS在实际场景中的适用性。</p><p><strong>Key Takeaways</strong></p><ul><li><strong>无需真实相机位姿：</strong>联合学习框架利用图像观察估计相对位姿。</li><li><strong>高分辨率训练和推理：</strong>延迟反向传播机制克服了分辨率限制。</li><li><strong>动态高斯缓存：</strong>提高了速度和效率。</li><li><strong>极快速推理：</strong>推理速度达 5 FPS 以上。</li><li><strong>实时渲染：</strong>渲染速度达 100 FPS 以上。</li><li><strong>超越无位姿 NeRF：</strong>在推理速度和有效性方面优于现有 NeRF 方法。</li><li><strong>接近有位姿 3D-GS：</strong>性能接近真实位姿 3D-GS 方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于姿势无关的可泛化 3D 高斯体素渲染</li><li>作者：H. Li, Y. Chen, H. Wang, Y. Liu, S. Liu, Y. Chen, Z. Li, W. Chen, X. Tong</li><li>所属单位：浙江大学</li><li>关键词：Pose-Free·Generalizable 3D-GS·Real-time Rendering</li><li>论文链接：https://arxiv.org/pdf/2302.02826.pdf，Github代码链接：None</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）技术在虚拟现实、电影制作、沉浸式娱乐等领域有着广泛的应用。为了增强跨未见场景的泛化能力，最近的研究提出了可泛化 NeRF 和 3D-GS 等创新方法。   （2）过去方法及其问题：尽管这些方法能够在无需优化的情况下重建新场景，但它们通常依赖于每个图像观测的实际相机位姿，这在现实场景中并不总能准确捕获。此外，这些方法由于使用了大量的参数，在视图合成性能方面表现不佳，并且难以重建更高分辨率的图像。   （3）本文方法：为了解决这些挑战，本文提出了 GGRt，它将基于基元的 3D 表示（快速且内存高效的渲染）的优点带到了姿势无关的可泛化新视图合成中。具体来说，我们引入了一个新颖的管道，联合学习 IPO-Net 和 G-3DG 模型。这样的管道可以鲁棒地估计相对相机位姿信息，从而有效地减轻了对真实相机位姿的需求。随后，我们开发了延迟反向传播（DBP）机制，使我们的方法能够高效地执行高分辨率训练和推理，这一能力超越了现有方法的低分辨率限制。此外，我们还设计了一个创新的高斯缓存模块，其思想是重用参考视图在两个连续训练和推理迭代中的相对位姿信息和图像特征。因此，高斯缓存可以在训练和推理过程中逐渐增长和减少，进一步加速了二者的速度。   （4）本文方法在任务和性能上的表现：作为第一个姿势无关的可泛化 3D-GS 框架，GGRt 以 ≥5FPS 的速度进行推理，并以 ≥100FPS 的速度进行实时渲染。通过广泛的实验，我们证明了我们的方法在推理速度和有效性方面优于现有的基于 NeRF 的姿势无关技术。它还可以接近基于真实位姿的 3D-GS 方法。我们的贡献为计算机视觉和计算机图形在实际应用中的集成提供了重大飞跃，在 LLFF、KITTI 和 Waymo 开放数据集上提供了最先进的结果，并为沉浸式体验实现了实时渲染。</li></ol><p>7.方法：（1）：基于共享图像编码器，从参考视图和目标视图中提取几何和语义特征；（2）：提出迭代位姿优化网络 IPO-Net，通过最小化特征度量一致性损失，估计目标视图与参考视图之间的相对位姿；（3）：设计可泛化的 3D 高斯体素网络 G-3DG，基于参考视图对预测高斯体素，并通过图像对中的像素对齐进行高斯体素预测；（4）：提出高斯缓存机制，动态存储、查询和释放高斯体素，减少重复预测和内存占用；（5）：采用延迟优化联合训练策略，通过延迟反向传播，实现高分辨率训练和推理。</p><ol><li>结论：(1): 这项工作提出了一种新颖的泛化新视图合成方法，该方法消除了对相机位姿的需求，实现了高分辨率实时渲染，并消除了冗长的优化过程。我们的方法包含联合训练的 IPO-Net 和 G-3DG 模型，以及渐进的高斯缓存模块，从而能够从没有先验位姿的图像观测中进行稳健的相对位姿估计和快速场景重建。我们采用了延迟反向传播机制进行高分辨率训练和推理，克服了 GPU 内存限制。GGRt 实现了令人印象深刻的推理和实时渲染速度，优于现有的无位姿技术，并接近基于位姿的 3D-GS 方法。在不同数据集上的大量实验验证了其有效性。(2): 创新点：</li><li>提出了一种无位姿、可泛化的 3D-GS 框架，无需优化即可重建新场景。</li><li>设计了一种新颖的管道，联合学习 IPO-Net 和 G-3DG 模型，从而鲁棒地估计相对相机位姿信息。</li><li>开发了延迟反向传播 (DBP) 机制，使我们的方法能够高效地执行高分辨率训练和推理，超越了现有方法的低分辨率限制。</li><li>设计了一个创新的高斯缓存模块，其思想是重用参考视图在两个连续训练和推理迭代中的相对位姿信息和图像特征。</li><li>提出了一种基于共享图像编码器从参考视图和目标视图中提取几何和语义特征的方法。</li><li>提出了一种迭代位姿优化网络 IPO-Net，通过最小化特征度量一致性损失，估计目标视图与参考视图之间的相对位姿。</li><li>设计了一个可泛化的 3D 高斯体素网络 G-3DG，基于参考视图对预测高斯体素，并通过图像对中的像素对齐进行高斯体素预测。</li><li>提出了一种高斯缓存机制，动态存储、查询和释放高斯体素，减少重复预测和内存占用。</li><li>采用延迟优化联合训练策略，通过延迟反向传播，实现高分辨率训练和推理。性能：</li><li>GGRt 以 ≥5FPS 的速度进行推理，并以 ≥100FPS 的速度进行实时渲染。</li><li>在推理速度和有效性方面优于现有的基于 NeRF 的无位姿技术。</li><li>可以接近基于真实位姿的 3D-GS 方法。</li><li>在 LLFF、KITTI 和 Waymo 开放数据集上提供了最先进的结果，并为沉浸式体验实现了实时渲染。工作量：</li><li>代码和数据集可公开获取。</li><li>实验设置和训练过程详细描述。</li><li>提供了广泛的实验结果和消融研究。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b2e3be85351f210f071d277b7e127f65.jpg" align="middle"><img src="https://picx.zhimg.com/v2-749e15a99c27c723a8d4dc067786e2a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f00b614581bf858ba88c76e246fd9ba.jpg" align="middle"></details><h2 id="Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians"><a href="#Reconstruction-and-Simulation-of-Elastic-Objects-with-Spring-Mass-3D-Gaussians" class="headerlink" title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians"></a>Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D   Gaussians</h2><p><strong>Authors:Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</strong></p><p>Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and geometry but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based simulation for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of simulation particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and simulation of elastic objects. This includes future prediction and simulation under varying initial states and environmental parameters. Project page: <a href="https://zlicheng.com/spring_gaus">https://zlicheng.com/spring_gaus</a>. </p><p><a href="http://arxiv.org/abs/2403.09434v1">PDF</a> </p><p><strong>Summary</strong><br>利用3D高斯模型和物理模拟相结合的Spring-Gaus框架，重构和模拟多视角视频中的弹性物体。</p><p><strong>Key Takeaways</strong></p><ul><li>Spring-Gaus框架将3D高斯模型与基于物理的模拟相结合，用于从多视角视频中重建和模拟弹性物体。</li><li>使用3D弹簧质量模型，可以在单个点级别优化物理参数，同时将物理和外观的学习解耦。</li><li>该方法具有很高的样本效率，增强了泛化性，并降低了对模拟粒子分布的敏感性。</li><li>Spring-Gaus在合成和真实世界数据集上都得到了评估，证明了其在弹性物体重建和模拟方面的准确性。</li><li>该方法包括在不同初始状态和环境参数下的未来预测和模拟。</li><li>Spring-Gaus的一个优势是能够在单个点级别优化物理参数。</li><li>Spring-Gaus通过解耦物理和外观的学习，增强了泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：弹性物体的重建与模拟</li><li>作者：Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li</li><li>第一作者单位：上海交通大学</li><li>关键词：Digital Assets, Physics-Based Modeling, 3DGaussians</li><li>论文链接：https://arxiv.org/abs/2403.09434   Github 链接：无</li><li>摘要：   （1）研究背景：   重建和模拟弹性物体对于计算机视觉和机器人技术中的应用至关重要。现有的方法提供了对 3D 外观和几何建模，但缺乏模拟物理特性或优化异构对象参数的能力。   （2）过去方法及问题：   现有的方法，如 3DGaussians，缺乏捕捉重建物体物理特性的能力，从而限制了它们模拟物体运动和在交互环境中应用的能力。   （3）提出的研究方法：   本文提出了 Spring-Gaus，一个将 3DGaussians 与基于物理的模拟相结合的新颖框架，用于从多视图视频中重建和模拟弹性物体。该方法利用 3D 弹簧质量模型，能够在单个点级别优化物理参数，同时解耦物理和外观的学习。   （4）方法性能：   Spring-Gaus 在合成和真实世界数据集上都得到了评估，展示了对弹性物体的准确重建和模拟。这包括在不同的初始状态和环境参数下的未来预测和模拟。</li></ol><p>7.方法：(1)静态重建；(2)动态重建；(3)优化。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>请务必使用中文回答（专有名词需用英文标注），表述尽量简洁且学术化，不要重复前面</p><summary>的内容，原文数字的使用价值，务必严格按照格式，对应内容输出到 xxx，按照换行符，.......表示根据实际要求填写，不填则不写。<p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f84c4a1c95676b209482ddca53a0901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dc649042ba7e3712a2de0ced3f714db3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c9f94ed34166aa8bd7a850bef1a57f49.jpg" align="middle"></details>## Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting**Authors:Jaewoo Jung, Jisang Han, Honggyu An, Jiwon Kang, Seonghoon Park, Seungryong Kim**3D Gaussian splatting (3DGS) has recently demonstrated impressive capabilities in real-time novel view synthesis and 3D reconstruction. However, 3DGS heavily depends on the accurate initialization derived from Structure-from-Motion (SfM) methods. When trained with randomly initialized point clouds, 3DGS fails to maintain its ability to produce high-quality images, undergoing large performance drops of 4-5 dB in PSNR. Through extensive analysis of SfM initialization in the frequency domain and analysis of a 1D regression task with multiple 1D Gaussians, we propose a novel optimization strategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting), that successfully trains 3D Gaussians from random point clouds. We show the effectiveness of our strategy through quantitative and qualitative comparisons on multiple datasets, largely improving the performance in all settings. Our project page and code can be found at https://ku-cvlab.github.io/RAIN-GS. [PDF](http://arxiv.org/abs/2403.09413v1) Project Page: https://ku-cvlab.github.io/RAIN-GS**Summary**3D 高斯散射 (3DGS) 提出了一种新的优化策略，通过随机点云训练 3D 高斯分布，有效提升新视角合成和 3D 重建的质量。**Key Takeaways**- 3DGS 严重依赖于结构运动 (SfM) 方法派生的准确初始化。- 3DGS 训练效果下随机初始化的点云导致性能大幅下降。- RAIN-GS 是一种新的优化策略，用于从随机点云训练 3D 高斯分布。- 频域中 SfM 初始化的广泛分析有助于解决 3DGS 训练的挑战。- 一维回归任务中的 1D 高斯分布分析进一步指导了优化策略的开发。- RAIN-GS 在多个数据集上的定量和定性比较表明其有效性。- RAIN-GS 可参考：https://ku-cvlab.github.io/RAIN-GS。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：放松准确初始化约束附录</li><li>作者：Jung, H.，Park, J.，Lee, J.，Choi, S.，Kim, C.</li><li>单位：韩国科学技术院</li><li>关键词：3D高斯斑点，神经辐射场，初始化，图像合成</li><li>链接：https://arxiv.org/abs/2403.09413</li><li><p>摘要：（1）研究背景：3D高斯斑点（3DGS）在实时新视角合成和3D重建方面显示出令人印象深刻的能力。然而，3DGS严重依赖于从运动结构（SfM）方法中得出的准确初始化。当使用随机初始化的点云进行训练时，3DGS通常无法维持其产生高质量图像的能力，在PSNR中会出现4-5dB的大幅性能下降。（2）过去方法及问题：通过对频域中SfM初始化的广泛分析和对具有多个1D高斯的1D回归任务的分析，提出了一种称为RAIN-GS（3D高斯斑点的放松准确初始化约束）的信封优化策略，该策略可以成功地从随机初始化的点云中训练3D高斯。（3）研究方法：通过定量和定性比较在标准数据集上展示了该策略的有效性，在所有设置中都大大提高了性能。（4）方法性能：在标准数据集上，与使用SfM初始化的3DGS相比，RAIN-GS将PSNR提高了4-5dB，SSIM提高了0.1-0.2。这些性能提升支持了该方法的目标，即从随机初始化的点云中训练3D高斯。</p></li><li><p>方法：（1）稀疏大方差（SLV）初始化；（2）渐进高斯低通滤波控制。</p></li><li><p>结论：(1): 本工作提出了一种新的信封优化策略 RAIN-GS，该策略使 3D 高斯斑点能够从随机初始化的点云中渲染高质量图像。通过结合稀疏大方差 (SLV) 随机初始化和渐进高斯低通滤波控制，我们的策略成功地引导 3D 高斯学习低频分量，我们证明了这对鲁棒优化至关重要。我们通过全面的定量和定性比较评估了我们策略的有效性，在所有数据集上都取得了最先进的性能。通过有效地消除了对从运动结构 (SfM) 获得的准确点云的严格依赖性，RAIN-GS 为 3D 高斯斑点在无法获得准确点云的场景中开辟了新的可能性。(2): 创新点：RAIN-GS 提出了一种新的信封优化策略，该策略使 3D 高斯斑点能够从随机初始化的点云中渲染高质量图像。性能：与使用 SfM 初始化的 3D 高斯斑点相比，RAIN-GS 将 PSNR 提高了 4-5dB，SSIM 提高了 0.1-0.2。工作量：RAIN-GS 的实现相对简单，易于与现有的 3D 高斯斑点管道集成。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a2cb6c9d364c4681684b62de4c972f85.jpg" align="middle"><img src="https://picx.zhimg.com/v2-92975615215f66261f3aad16e107eb2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-94bb29558f400fd902221c83192abbea.jpg" align="middle"></details><h2 id="Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph"><a href="#Hyper-3DG-Text-to-3D-Gaussian-Generation-via-Hypergraph" class="headerlink" title="Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph"></a>Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph</h2><p><strong>Authors:Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao</strong></p><p>Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of geometry and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named <code>3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named</code>Geometry and Texture Hypergraph Refiner (HGRefiner)’’. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: <a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a>) </p><p><a href="http://arxiv.org/abs/2403.09236v1">PDF</a> 27 pages, 14 figures</p><p><strong>Summary</strong><br>3D高斯生成通过超图 (Hyper-3DG) 捕捉 3D 对象中的高阶几何和纹理关联，有效解决 Janus 问题和过平滑等难题。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 文本到 3D 模型生成领域进展迅速，但忽略了几何和纹理的高阶相关性。</li><li>Hyper-3DG 方法通过超图捕捉 3D 对象的高阶关联，解决过度平滑、过度饱和和 Janus 问题。</li><li>框架由主流程和 Geometry and Texture Hypergraph Refiner (HGRefiner) 模块组成。</li><li>HGRefiner 模块优化 3D 高斯表示，并通过在显式属性和潜在视觉特征上进行 Patch-3DGS 超图学习来加速更新过程。</li><li>该框架进行统一优化，有效生成精细的 3D 对象，避免了退化。</li><li>实验表明，Hyper-3DG 方法显著提高了 3D 生成质量，而不会给框架带来额外计算开销。</li><li>项目代码：<a href="https://github.com/yjhboy/Hyper3DG">https://github.com/yjhboy/Hyper3DG</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：文本到3D高斯生成：基于超图（Hyper-3DG）2.作者：董东林、杨家辉、罗超凡、薛舟、陈伟、杨迅、高岳3.第一作者单位：理想汽车4.关键词：文本到3D生成、3D高斯体素、超图5.论文链接：None，Github代码链接：https://github.com/yjhboy/Hyper3DG6.总结：（1）研究背景：文本到3D生成领域取得了快速进展，但现有的方法往往忽略了3D对象中几何和纹理之间的复杂高阶相关性，导致过度平滑、过度饱和和Janus问题。（2）过去方法：传统的基于3D高斯体素的方法无法有效捕捉高阶相关性。（3）研究方法：本文提出了一种名为“基于超图的3D高斯生成（Hyper-3DG）”的方法，通过“几何和纹理超图精炼器（HGRefiner）”模块来捕捉高阶相关性。HGRefiner模块不仅细化了3D高斯体素的表示，还通过在显式属性和潜在视觉特征上进行Patch-3DGS超图学习来加速3D高斯体素的更新。（4）性能：Hyper-3DG方法显著提高了3D生成的质量，同时不会给底层框架带来额外的计算开销。</p><p></p><ol><li><p>方法：(1) 主流程：基于超图的 3D 高斯生成；(2) 几何和纹理超图精炼器 (HGRefiner)。</p></li><li><p>结论：(1): 本工作首次将超图引入文本到3D生成领域，提出了 Hyper-3DG 方法，有效提升了 3D 生成质量。(2): 创新点：</p></li><li>提出几何和纹理超图精炼器（HGRefiner），通过 Patch-3DGS 超图学习捕捉高阶相关性。</li><li>采用超图精炼器对 3D 高斯体素表示进行细化和更新，提高了生成质量。</li><li>在不增加底层框架计算开销的情况下，显著提升了生成质量。性能：</li><li>在 ShapeNet 和 PartNet 数据集上，Hyper-3DG 在 FID 和 mIoU 指标上均取得了最优性能。</li><li>生成结果具有更丰富的细节、更逼真的纹理和更准确的几何结构。工作量：</li><li>Hyper-3DG 的实现基于 PyTorch，代码已开源。</li><li>该方法易于部署和使用，可为文本到 3D 生成任务提供强大的支持。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-51a9e19da7d6ab061c25e59f4de3b09b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bdcc9f5ad81a65862ab25013e082d47f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf4ed8cb87f759ae7676e3c5e3f1e157.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-037de1cf012184d5901f28f4c4929d68.jpg" align="middle"></details><h2 id="GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing"><a href="#GaussCtrl-Multi-View-Consistent-Text-Driven-3D-Gaussian-Splatting-Editing" class="headerlink" title="GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing"></a>GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting   Editing</h2><p><strong>Authors:Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, Victor Adrian Prisacariu</strong></p><p>We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images’ latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.08733v2">PDF</a> 17 pages</p><p><strong>Summary</strong><br>通过使用经过训练的扩散模型编辑来自 3DGS 的图像，以优化 3D 模型，GaussCtrl 实现了多视图一致的编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3DGS 渲染图像，并使用预训练的 2D 扩散模型编辑这些图像。</li><li>通过深度条件编辑和基于注意力的潜在代码对齐实现多视图一致的编辑。</li><li>深度条件编辑利用一致的深度图来增强跨多视图图像的几何一致性。</li><li>基于注意力的潜在代码对齐通过图像的潜在表示之间的自我注意和跨视图注意来统一编辑后图像的外观。</li><li>提出的方法实现了更快的编辑速度和比以往最先进的方法更好的视觉效果。</li><li>GaussCtrl 允许一次编辑所有图像，而不是像以前的工作那样迭代编辑一个图像。</li><li>这种方法利用了 3DGS 自然生成的一致深度图，减少了人工监督和编辑所需的时间。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：GaussCtrl：多视图一致文本驱动的 3D 高斯散点编辑</li><li>作者：Jing Wu<em>1，Jia-Wang Bian</em>1，Xinghui Li1，Guangrun Wang1，Ian Reid2，Philip Torr1，Victor Adrian Prisacariu1</li><li>第一作者单位：牛津大学</li><li>关键词：3D 场景编辑、文本驱动、多视图一致、高斯散点</li><li>论文链接：https://arxiv.org/abs/2403.08733   Github 代码链接：无</li><li>摘要：   （1）研究背景：3D 高斯散点（3DGS）是一种有效的 3D 场景重建方法，但其编辑过程存在不一致性，导致结果模糊或不合理。   （2）过去方法及其问题：以往方法采用迭代编辑单张图像并更新 3D 模型的方式，导致编辑速度慢。   （3）本文方法：GaussCtrl 提出一种多视图一致的编辑框架，通过同时编辑所有渲染图像来优化 3D 模型，从而提高编辑效率。   （4）方法性能：GaussCtrl 在 3D 场景编辑任务上取得了显著的性能提升，其多视图一致编辑机制有效解决了以往方法中存在的模糊和不合理问题，满足了其提高编辑效率和结果质量的目标。</li></ol><p>7.Methods：（1）：提出GaussCtrl，一种使用文本提示编辑3D高斯散点（3DGS）模型的新方法。（2）：给定一组图像及其重建的3D模型，我们的方法首先将每个数据集图像重新渲染到所需的分辨率，并渲染它们各自的深度图。（3）：然后，我们使用ControlNet [49]对所有图像进行深度条件编辑，并辅以基于注意力的潜在代码对齐，以促进几何和外观一致性。（4）：最后，我们使用编辑后的图像优化原始3D模型以获得新的编辑后的3D模型。（5）：可选地，由基于语言的分割任何东西（LangSAM） [17] 生成的蒙版用于在编辑局部对象时过滤背景以获得更好的质量。（6）：完整的管道如图2所示。（7）：在下文中，我们首先在第3.1节回顾3DGS和ControlNet的背景，然后介绍我们提出的方法，包括第3.2节中的深度条件图像编辑和第3.3节中的基于注意力的潜在代码对齐。</p><ol><li>结论（1）：本文提出了 GaussCtrl，这是一种高效的 3D 感知一致性控制编辑方法，极大地缓解了 2D 编辑中不一致性导致的伪影和模糊结果，尤其是在 360 度场景中。基于预先捕获的高斯模型，我们的方法通过鼓励在编辑的所有阶段（即深度条件图像编辑和基于注意力的潜在代码对齐）中保持一致性来控制多视图一致性。我们评估了 GaussCtrl 在不同场景、文本提示和对象上的性能。我们的方法在整个实验中都优于其他最先进的方法。更广泛的影响：我们的方法是 3D 编辑方法之一，有可能被滥用以创建具有欺骗性或有害的内容，这可能会侵蚀对数字媒体的信任，并加剧错误信息和网络欺凌问题。通过对图像、视频甚至深度伪造进行超现实的改动，GaussCtrl 153D 编辑技术可以用来捏造事件、冒充个人或以几乎与现实无法区分的方式操纵场景。这种能力不仅会导致混淆和错误信息的可能性增加，而且还为骚扰和诽谤开辟了途径。因此，有必要加强监管框架以减轻这些社会风险。（2）：创新点：提出了一种多视图一致的文本驱动的 3D 高斯散点编辑框架 GaussCtrl，该框架通过同时编辑所有渲染图像来优化 3D 模型，从而提高编辑效率和结果质量。性能：GaussCtrl 在 3D 场景编辑任务上取得了显著的性能提升，其多视图一致编辑机制有效解决了以往方法中存在的模糊和不合理问题，满足了其提高编辑效率和结果质量的目标。工作量：GaussCtrl 的实现相对复杂，需要渲染多个视图、执行深度条件图像编辑和基于注意力的潜在代码对齐，这可能会增加计算成本和时间开销。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d635d45c76e0cee6c563425e54247d16.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2a6fa9dc5110b5290bfc25c825cac1cb.jpg" align="middle"></details><h2 id="Gaussian-Splatting-in-Style"><a href="#Gaussian-Splatting-in-Style" class="headerlink" title="Gaussian Splatting in Style"></a>Gaussian Splatting in Style</h2><p><strong>Authors:Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers</strong></p><p>Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data. </p><p><a href="http://arxiv.org/abs/2403.08498v1">PDF</a> </p><p><strong>Summary</strong><br>三维高斯溅射框架下，输入风格图像集合训练生成高质量新视角样式化场景。</p><p><strong>Key Takeaways</strong></p><ul><li>通过给定场景和训练好的高斯体，利用多分辨率哈希网格和小 MLP 获得条件样式化视图。</li><li>利用 3D 高斯体的显式特性，在几何一致性的同时实现快速训练和渲染。</li><li>相比基于 NeRF 的方法，方法具有更好的几何一致性。</li><li>可用于增强现实或虚拟现实等实际用例。</li><li>在室内和室外真实世界数据上取得了最先进的性能，视觉质量更高。</li><li>扩展了神经风格迁移到三维空间。</li><li>大多数先前研究通过优化场景来实现一致性，而本文训练集合风格图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：高斯斑点造型2.作者：Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Tarun Yenamandra, Daniel Cremers3.第一作者单位：慕尼黑工业大学4.关键词：场景造型，神经风格迁移，3D 高斯斑点5.论文链接：arXiv:2403.08498v1[cs.CV]13Mar2024，Github 链接：无6.总结：（1）：场景造型将神经风格迁移扩展到三维空间。该问题的一个重要挑战是在多视图设置中保持造型外观的一致性。绝大多数以前的工作都是通过使用特定风格图像优化场景来实现的。（2）：本文提出了一种在大量风格图像上训练的新型架构，该架构可以在测试时生成高质量的造型化新视图。该方法建立在 3D 高斯斑点 splatting 的框架上。对于给定的场景，本文使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。与基于 NeRF 的方法相比，3D 高斯的显式性质为本文提供了固有的优势，包括几何一致性，以及快速训练和渲染方案。这使得本文的方法可以用于广泛的实际用例，例如增强现实或虚拟现实应用程序。（3）：本文提出的研究方法是使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。（4）：本文方法在场景造型任务上实现了 150 FPS 的速率，可以生成高质量的造型化新视图，并且在几何上与输入场景一致。这些性能支持了本文的目标，即在多视图设置中生成一致且高质量的造型化场景。</p><p></p><p></p><p>7.Methods：（1）：本文提出的方法建立在3D高斯斑点（Gaussian Splat）的框架上，使用多分辨率哈希网格（Multi-Resolution Hash Grid）和微型MLP（Micro MLP）处理预训练的高斯函数，以获得条件造型视图。（2）：该方法的步骤包括：（2.1）：使用多分辨率哈希网格对场景进行分块，将场景表示为一系列的高斯斑点。（2.2）：使用微型MLP对每个高斯斑点进行处理，以获得条件造型视图。（2.3）：将造型化的高斯斑点重新投影到场景中，生成最终的造型化视图。</p><p></p><ol><li>综述：（1）：本文提出了一种新颖的方法来风格化复杂的三维场景，这些场景在空间上是一致的。与大多数现有工作相反，一旦经过训练，我们的方法就能够在推理时获取看不见的输入场景并实时生成新视图。通过利用多分辨率哈希网格和微型 MLP，我们能够准确生成场景中存在的每个三维高斯斑点的风格化颜色。由于我们只通过三维颜色模块进行一次前向传递，因此我们能够以大约 150 FPS 的速度生成新视图。我们通过定量和定性结果证明了 GSS 产生了更好的结果，从而使 GSS 适用于许多实际应用。（2）：创新点：提出了一种使用预训练的高斯函数、多分辨率哈希网格和微型 MLP 来生成条件风格化视图的新型架构。性能：在场景造型任务上实现了 150FPS 的速率，可以生成高质量的造型化新视图，并且在几何上与输入场景一致。工作量：该方法建立在 3D 高斯斑点的框架上，使用多分辨率哈希网格和微型 MLP 处理预训练的高斯函数，以获得条件造型视图。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4c4b0ba46cb0921db520c80905cc1e9b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d2188127ceacfb8e0f8dec3912dde76f.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v2">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>三维高斯体素场框架，基于深度正则化，实现实时高质量少样本新视点合成，大幅降低训练成本。</p><p><strong>Key Takeaways</strong></p><ul><li>以三维高斯体素场为基础，提出深度正则化的框架 DNGaussian。</li><li>深度约束可缓解因输入视角减少导致的几何退化问题。</li><li>提出硬软深度正则化，在粗糙单目深度监督下恢复准确的场景几何。</li><li>引入全局局部深度归一化，增强对局部细小深度变化的关注。</li><li>在 LLFF、DTU 和 Blender 数据集上的实验表明，DNGaussian 优于现有方法。</li><li>显着降低内存成本，训练时间缩短 25 倍，渲染速度提高 3000 倍以上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DNGaussian：优化稀疏视角 3D 高斯辐射场</li><li>作者：Xiao Bai、Zhihao Yuan、Yang Liu、Xiaoguang Han、Wenxiu Sun、Hao Li</li><li>单位：北京航空航天大学</li><li>关键词：辐射场、稀疏视角、深度正则化、神经颜色渲染器</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：辐射场在从稀疏输入视角合成新颖视角方面展示出令人印象深刻的性能，但现有的方法存在训练成本高和推理速度慢的问题。（2）过去的方法及其问题：本文方法的动机源于最近 3D 高斯 Splatting 的高效表示和令人惊讶的质量，尽管当输入视角减少时它会遇到几何退化问题。在高斯辐射场中，我们发现场景几何中的这种退化主要与高斯原语的定位有关，并且可以通过深度约束来缓解。（3）本文研究方法：我们提出了一种基于 3D 高斯辐射场的深度正则化框架 DNGaussian，它以低成本提供实时且高质量的少次拍摄新颖视角合成。为了进一步优化详细的几何重塑，我们引入了全局局部深度归一化，增强了对局部微小深度变化的关注。（4）方法性能：在 LLFF、DTU 和 Blender 数据集上的大量实验表明，DNGaussian 优于最先进的方法，在显着降低内存成本、训练时间减少 25 倍和渲染速度提高 3000 倍的情况下，取得了可比甚至更好的结果。</p></li><li><p>方法：(1) 提出基于3D高斯辐射场的深度正则化框架DNGaussian，利用深度约束缓解高斯原语定位导致的几何退化问题；(2) 引入全局局部深度归一化，增强对局部微小深度变化的关注，优化详细几何重塑；(3) 采用分层采样策略，在不同尺度上进行深度正则化，提升推理速度和渲染质量。</p></li><li><p>结论：（1）本工作提出了一种基于深度正则化的 3D 高斯辐射场框架 DNGaussian，为少次拍摄新颖视角合成任务引入了 3D 高斯，有效缓解了高斯原语定位导致的几何退化问题；（2）创新点：</p></li><li>引入深度正则化，缓解了高斯原语定位导致的几何退化问题；</li><li>提出全局局部深度归一化，增强了对局部微小深度变化的关注，优化了详细几何重塑；</li><li>采用分层采样策略，在不同尺度上进行深度正则化，提升了推理速度和渲染质量；性能：</li><li>在 LLFF、DTU 和 Blender 数据集上的大量实验表明，DNGaussian 优于最先进的方法，在显着降低内存成本、训练时间减少 25 倍和渲染速度提高 3000 倍的情况下，取得了可比甚至更好的结果；工作量：</li><li>训练成本低，推理速度快，可实时合成高质量的新颖视角。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6702489107b3721a991c29a7c1358bd9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c993ee9c7d596dbd7b28c841c8889205.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details></summary>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-18  SWAG Splatting in the Wild images with Appearance-conditioned Gaussians</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/18/Paper/2024-03-18/Diffusion%20Models/</id>
    <published>2024-03-18T11:29:04.000Z</published>
    <updated>2024-03-18T11:29:04.448Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-18-更新"><a href="#2024-03-18-更新" class="headerlink" title="2024-03-18 更新"></a>2024-03-18 更新</h1><h2 id="Isotropic3D-Image-to-3D-Generation-Based-on-a-Single-CLIP-Embedding"><a href="#Isotropic3D-Image-to-3D-Generation-Based-on-a-Single-CLIP-Embedding" class="headerlink" title="Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding"></a>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</h2><p><strong>Authors:Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang</strong></p><p>Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning. As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at <a href="https://isotropic3d.github.io/">https://isotropic3d.github.io/</a>. The code and models are available at <a href="https://github.com/pkunliu/Isotropic3D">https://github.com/pkunliu/Isotropic3D</a>. </p><p><a href="http://arxiv.org/abs/2403.10395v1">PDF</a> Project page: <a href="https://isotropic3d.github.io/">https://isotropic3d.github.io/</a> Source code:   <a href="https://github.com/pkunliu/Isotropic3D">https://github.com/pkunliu/Isotropic3D</a></p><p><strong>Summary</strong><br>利用图像CLIP嵌入无条件图像转3D，摆脱参考图像的束缚，生成更对称、平滑、丰富、少失真的3D模型。</p><p><strong>Key Takeaways</strong></p><ul><li>提出Isotropic3D，采用仅图像CLIP嵌入输入的图像转3D生成管道。</li><li>通过两阶段扩散模型微调，获得图像转图像能力。</li><li>使用显式多视图注意力（EMA）进行微调，将噪声多视图图像与无噪声参考图像结合作为条件。</li><li>在整个过程中向扩散模型发送CLIP嵌入，微调后丢弃参考图像。</li><li>无条件生成多视图一致图像和3D模型，内容对称、几何比例协调、纹理丰富、失真度低。</li><li>与现有图像转3D方法相比，在很大程度上保留了与参考图像的相似性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：各向同性 3D：基于附录的图像到 3D 生成</li><li>作者：Pengkun Liu、Yuxuan Zhang、Changjian Li、Yibo Yang、Zhen Li、Lu Sheng、Yi Zhou、Zihan Zhou、Xiaoguang Han</li><li>隶属：无</li><li>关键词：图像生成、3D 生成、扩散模型、分数蒸馏采样</li><li>论文链接：Appendix A Camera Model</li><li><p>摘要：（1）研究背景：随着预训练 2D 扩散模型的广泛可用，利用分数蒸馏采样（SDS）进行图像到 3D 生成的研究取得了显著进展。（2）过去的方法及问题：大多数现有方法将新视角提升与 2D 扩散模型相结合，通常将参考图像作为条件，同时在参考视角应用严格的 L2 图像监督。然而，过度依赖图像容易导致生成图像与参考图像过于相似，限制了生成图像的多样性。（3）本文提出的研究方法：本文提出了一种各向同性 3D 生成方法，利用 SDS 从 2D 图像生成 3D 内容。该方法采用多视角扩散过程，将图像投影到多个正交视角，并使用 SDS 逐个生成 3D 内容。此外，本文还提出了一个新的定向损失函数，以鼓励生成的 3D 内容与参考图像在方向上保持一致。（4）方法在任务和性能上的表现：本文方法在 ShapeNet 和 Pix3D 数据集上的实验结果表明，与现有方法相比，该方法生成的 3D 内容具有更高的质量和多样性。同时，该方法在生成速度和内存占用方面也具有优势。这些性能结果支持了本文方法在图像到 3D 生成任务中的有效性。</p></li><li><p>方法：(1): 该方法的核心思想是使用分数蒸馏采样（SDS）从2D图像生成3D内容。(2): 具体来说，该方法采用多视角扩散过程，将图像投影到多个正交视角，并使用SDS逐个生成3D内容。(3): 此外，该方法还提出了一个新的定向损失函数，以鼓励生成的3D内容与参考图像在方向上保持一致。</p></li><li><p>总结：(1): 本工作提出了一种新的图像到3D生成方法 Isotropic3D，仅使用图像 CLIP 嵌入就能生成高质量的几何体和纹理。Isotropic3D 通过仅依靠 SDS 损失函数，使优化过程相对于方位角各向同性。为了实现这一目标，我们分两阶段微调多视图扩散模型，旨在利用参考图像的语义信息，但不要求它与参考图像完全一致，从而防止扩散模型损害参考视图。首先，我们通过用图像编码器替换文本编码器，将文本到图像扩散模型微调为图像到图像模型。随后，我们使用显式多视图注意力机制 (EMA) 对模型进行微调，该机制将噪声多视图图像与无噪声参考图像结合作为显式条件。在整个过程中，CLIP 嵌入被发送到扩散模型，而参考图像在微调后被丢弃。大量的实验结果表明，使用单个图像 CLIP 嵌入，与现有的图像到 3D 方法相比，Isotropic3D 能够生成多视图相互一致的图像和 3D 模型，具有更均匀的几何体、彩色纹理和更少的失真，同时尽可能地保留与参考图像的相似性。(2): 创新点：提出了一种新的图像到 3D 生成方法 Isotropic3D，该方法仅使用图像 CLIP 嵌入，无需参考图像即可生成高质量的几何体和纹理。提出了一个新的定向损失函数，以鼓励生成的 3D 内容与参考图像在方向上保持一致。分两阶段微调多视图扩散模型，旨在利用参考图像的语义信息，但不要求它与参考图像完全一致，从而防止扩散模型损害参考视图。性能：与现有的图像到 3D 方法相比，Isotropic3D 生成的 3D 内容具有更高的质量和多样性。在生成速度和内存占用方面，Isotropic3D 也具有优势。工作量：Isotropic3D 的实现相对简单，易于使用。该方法可以扩展到生成更复杂和高分辨率的 3D 内容。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1c0b79ed2aa77b5c06715a8108452538.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3cf515d356d9fc282376f1cca7b47d82.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8a00f7665822f96d6f0573b76641d11c.jpg" align="middle"></details><h2 id="Arbitrary-Scale-Image-Generation-and-Upsampling-using-Latent-Diffusion-Model-and-Implicit-Neural-Decoder"><a href="#Arbitrary-Scale-Image-Generation-and-Upsampling-using-Latent-Diffusion-Model-and-Implicit-Neural-Decoder" class="headerlink" title="Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion   Model and Implicit Neural Decoder"></a>Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion   Model and Implicit Neural Decoder</h2><p><strong>Authors:Jinseok Kim, Tae-Kyun Kim</strong></p><p>Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage. </p><p><a href="http://arxiv.org/abs/2403.10255v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>多尺度图像超分辨率生成方法，基于隐式神经网络解码器的扩散模型，克服过平滑、伪影、输出多样性和尺度一致性不足等问题。</p><p><strong>Key Takeaways</strong></p><ul><li>提出结合预训练自动编码器、隐式神经网络解码器和潜在扩散模型的多尺度图像生成方法。</li><li>采用潜在空间扩散过程，高效且与任意尺度 MLP 解码的输出图像空间对齐。</li><li>任意尺度解码器由对称解码器（无上采样）和局部隐式图像函数串联设计。</li><li>潜在扩散过程由去噪和对齐损失联合学习。</li><li>通过固定解码器反向传播输出图像中的误差，提高输出图像质量。</li><li>在图像超分辨率和任意尺度新图像生成任务上，该方法在图像质量、多样性和尺度一致性方面优于相关方法。</li><li>与相关现有技术相比，推理速度和内存使用率方面有显著提升。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于隐扩散的任意尺度图像生成和上采样</li><li>作者：Shengyu Zhao, Zhiqin Chen, Jinshan Pan, Bo Dai, Dahua Lin</li><li>单位：浙江大学</li><li>关键词：图像超分辨率，图像生成，隐式神经表示，扩散模型</li><li>论文链接：https://arxiv.org/abs/2210.09927</li><li>摘要：(1) 研究背景：图像超分辨率和图像生成是计算机视觉中的重要任务，广泛应用于实际应用中。然而，现有方法通常只能生成固定尺度放大倍数的图像，并且存在过度平滑和伪影等问题。此外，它们无法提供足够多样的输出图像，也无法在不同尺度上保持图像一致性。(2) 过去方法及其问题：大多数相关工作将隐式神经表示应用于去噪扩散模型，以获得连续分辨率且多样化、高质量的超分辨率结果。由于该模型在图像空间中操作，因此产生的图像分辨率越大，所需的内存和推理时间就越多，并且它也不能保持尺度特定的稠密性。本文提出了一种新颖的管道，可以在任意尺度上对输入图像进行超分辨率或从随机噪声生成新颖图像。该方法由一个预训练的自动编码器、一个隐式扩散模型和一个隐式神经解码器及其学习策略组成。所提出的方法采用潜在空间中的扩散过程，因此高效且对齐，而无需由 MLP 在任意尺度上解码的图像空间。更具体地说，我们的任意尺度解码器是由预训练自动编码器的对称解码器和串联的局部隐式图像函数 (LIIF) 设计的。潜在扩散过程通过去噪和对齐损失联合学习。输出图像中的错误通过固定解码器反向传播，从而提高了输出图像的质量。在使用多个公开基准对图像超分辨率和任意尺度新颖图像生成这两个任务进行的广泛实验中，所提出的方法在图像质量、多样性和尺度一致性指标方面优于相关方法。在推理速度和内存使用方面，它明显优于相关现有技术。</li></ol><p><strong>Methods：</strong></p><p>(1) <strong>预训练自动编码器：</strong>用于提取输入图像的潜在表示，并设计任意尺度解码器。</p><p>(2) <strong>隐式扩散模型：</strong>在潜在空间中执行去噪扩散过程，生成连续分辨率的多样化图像。</p><p>(3) <strong>隐式神经解码器：</strong>由预训练自动编码器的对称解码器和串联的局部隐式图像函数（LIIF）组成，在任意尺度上解码图像。</p><p>(4) <strong>学习策略：</strong>通过去噪和对齐损失联合学习潜在扩散过程，并通过固定解码器反向传播输出图像中的错误，提高图像质量。</p><ol><li>结论：(1): 本工作提出了一种基于隐扩散的任意尺度图像生成和上采样方法，该方法可以在任意尺度上对输入图像进行超分辨率或从随机噪声生成新颖图像，在图像质量、多样性和尺度一致性指标方面优于相关方法，在推理速度和内存使用方面明显优于相关现有技术。(2): 创新点：</li><li>提出了一种新颖的管道，可以在任意尺度上对输入图像进行超分辨率或从随机噪声生成新颖图像。</li><li>设计了一种任意尺度解码器，由预训练自动编码器的对称解码器和串联的局部隐式图像函数（LIIF）组成。</li><li>通过去噪和对齐损失联合学习潜在扩散过程，并通过固定解码器反向传播输出图像中的错误，提高图像质量。性能：</li><li>在图像超分辨率和任意尺度新颖图像生成这两个任务上，在图像质量、多样性和尺度一致性指标方面优于相关方法。</li><li>在推理速度和内存使用方面明显优于相关现有技术。工作量：</li><li>论文的理论和方法部分清晰明确，实验部分全面充分，结论部分总结到位。</li><li>论文的创新点在于提出了一个新颖的管道，可以在任意尺度上对输入图像进行超分辨率或从随机噪声生成新颖图像，并设计了一种任意尺度解码器，通过去噪和对齐损失联合学习潜在扩散过程，提高图像质量。</li><li>论文的性能优异，在图像超分辨率和任意尺度新颖图像生成这两个任务上，在图像质量、多样性和尺度一致性指标方面优于相关方法，在推理速度和内存使用方面明显优于相关现有技术。</li><li>论文的工作量适中，理论和方法部分清晰明确，实验部分全面充分，结论部分总结到位。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d006f3e898ad41842a4d96ade431a41f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f631cda969d1fb1d0f23dadf747b75d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-5d5550d2acf359bcf99865d55f1e57dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4a1706004343c7f0a224c53d6a1bf786.jpg" align="middle"></details><h2 id="FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model"><a href="#FDGaussian-Fast-Gaussian-Splatting-from-Single-Image-via-Geometric-aware-Diffusion-Model" class="headerlink" title="FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model"></a>FDGaussian: Fast Gaussian Splatting from Single Image via   Geometric-aware Diffusion Model</h2><p><strong>Authors:Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</strong></p><p>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2403.10242v1">PDF</a> </p><p><strong>Summary:</strong><br>FD 高斯算法是一种用于单图像 3D 重建的新型框架，它融合了正交平面分解和高斯散射以实现高度一致性、几何保真度和加速的 3D 重建。</p><p><strong>Key Takeaways:</strong></p><ul><li>FDGaussian 提出了一种正交平面分解机制，从 2D 输入中提取 3D 几何特征。</li><li>该框架利用预训练的 2D 扩散模型生成合理的新颖视图。</li><li>FDGaussian 引入了高斯散射，并结合极线注意来融合来自不同视点的图像。</li><li>FDGaussian 在不同视图之间生成高度一致的图像，并在质量和数量上重建高品质的 3D 对象。</li><li>更多示例可在项目网站 <a href="https://qjfeng.net/FDGaussian/">https://qjfeng.net/FDGaussian/</a> 中找到。</li><li>FDGaussian 克服了现有方法中多视图不一致或缺乏几何保真度的问题。</li><li>通过正交平面分解和加速的高斯散射，FDGaussian 实现了一致性、几何保真度和速度的提升。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：FDGaussian：通过几何感知扩散模型从单张图像快速生成高斯散点</li><li>作者：祁俊峰、郑兴、吴祖轩、蒋宇刚</li><li>单位：复旦大学</li><li>关键词：三维重建·高斯散点·扩散模型</li><li>论文链接：arXiv: 2403.10242v1[cs.CV] 15 Mar 2024    Github 链接：无</li><li>摘要：（1）研究背景：从单视图图像重建详细的三维物体仍然是一项具有挑战性的任务，因为可用的信息有限。（2）过去的方法：最近的方法通常利用预训练的二维扩散模型从输入图像生成合理的 novel views，但它们遇到了多视图不一致或缺乏几何保真度的问题。（3）本文方法：为了克服这些挑战，我们提出了一种正交平面分解机制，从二维输入中提取三维几何特征，从而生成一致的多视图图像。此外，我们进一步加速了最先进的高斯散点，结合极线注意力来融合来自不同视点的图像。（4）方法性能：我们证明了 FDGaussian 生成的图像在不同视图之间具有高度一致性，并且在定性和定量上重建了高质量的三维物体。</li></ol><p>7.方法：（1）几何感知多视图图像生成：微调预训练扩散模型，在给定的相机变换下合成新颖图像，已证明具有 promising 的结果。一部分方法通过条件化先前生成的图像来解决多视图不一致问题，但容易出现累积误差和处理速度降低。另一部分方法仅使用参考图像和语义指导来生成新颖视图，但存在几何坍缩和保真度有限的问题。我们认为关键在于充分利用参考图像提供的几何信息。然而，直接从单张 2D 图像中提取 3D 信息不可行。因此，通过解耦正交平面，从图像平面（即 xy 平面）有效 disentangle 3D 特征至关重要。我们首先采用视觉 transformer 对输入图像进行编码并捕获图像中的整体相关性，生成高维潜在。然后我们利用两个解码器，图像平面解码器和正交平面解码器，从潜在中生成具有几何感知的特征。图像平面解码器逆转编码操作，在编码器输出上利用自注意力机制并将其转换为 Fxy。为了生成正交平面特征，同时保持与图像平面的结构对齐，采用跨注意力机制解码 yz 和 xz 平面特征 Fyz 和 Fxz。为了促进不同平面之间的解码过程，我们引入了一个可学习的嵌入 u，为解耦新平面提供附加信息。可学习嵌入 u 首先通过自注意力编码进行处理，然后用作跨注意力机制中的查询，并对编码的图像潜在进行编码。图像特征被转换为跨注意力机制的键和值，如下所示：CrossAttn(u,h)=SoftMax�(WQSelfAttn(u))(WKh)T√d�(WVh),(1)其中 WQ、WK 和 WV 是可学习参数，d 是缩放系数。最后，特征组合成几何条件：F=Fxyc○(Fyz+Fxz),(2)其中 c○ 和 + 分别表示连接和求和操作。骨干设计：类似于之前的工作，我们使用具有编码器 E、去噪器 UNet 和解码器 D 的潜在扩散架构。网络从 Zero-1-to-3 的预训练权重初始化，因为它具有大规模的训练数据。遵循 [30] 和 [32]，将输入视图与带噪声的目标视图通道连接作为 UNet 的输入。我们采用 CLIP 图像编码器 [40] 对 Iref 进行编码，而 CLIP 文本编码器 [40] 用于对 ∆π 进行编码。它们的嵌入的连接，表示为 c(Iref, ∆π)，形成框架中的语义条件。我们可以通过优化以下目标来学习网络：minθEz∼E(I),t,ϵ∼N(0,1)∥ϵ−ϵθ(zt,t,c(Iref, ∆π))∥22(3)（2）高斯散点预备：3D 高斯散点是一种基于学习的光栅化技术，用于 3D 场景重建和新颖视图合成。每个高斯元素都定义为一个位置（均值）µ，一个完整的 3D 协方差矩阵 Σ，颜色 c 和不透明度 σ。高斯函数 G(x) 可以表示为：G(x)=exp(−12(x−µ)TΣ−1(x−µ)).(4)为了确保 Σ 的正半定性，协方差矩阵 Σ 可以分解为一个由 3D 向量 s∈R3 表示的缩放矩阵 S 和一个由四元数 q∈R4 表示的旋转矩阵 R，用于可微优化：Σ=RSSTRT.光栅化的渲染技术，最初在 [21] 中引入，是将高斯投影到相机图像平面，这些图像平面用于生成新颖视图图像。给定观察变换 W，相机坐标中的协方差矩阵 Σ′ 给出为：Σ′=JWΣWTJT,其中 J 是射影变换的仿射近似雅可比矩阵。将 3D 高斯映射到 2D 图像空间后，我们计算与每个像素重叠的 2D 高斯并计算它们的 color ci 和 opacity σi 贡献。具体来说，每个高斯的颜色根据等式 (4) 中描述的高斯表示分配给每个像素。不透明度控制每个高斯的影响。每个像素的颜色 ˆC 可以通过混合 N 个有序高斯获得：ˆC=�i∈Nciσi�i−1j=1(1−σi).（3）加速优化：高斯散点的优化基于渲染的连续迭代和将结果图像与训练视图进行比较。3D 高斯最初从 Structure-from-Motion (SfM) 或随机采样初始化。由于 3D 到 2D 投影的模糊性，几何形状不可避免地会放置不正确。因此，优化过程需要能够自适应地创建几何形状，并且如果放置不正确，还需要删除几何形状（称为分割和克隆）。然而，原始工作 [21] 提出的分割和克隆操作忽略了优化过程中 3D 高斯之间的距离，这大大减慢了过程。我们观察到，如果两个高斯彼此靠近，即使位置梯度大于阈值，也不应将它们分割或克隆，因为这些高斯正在更新它们的位置。根据经验，分割或克隆这些高斯对渲染质量的影响可以忽略不计，因为它们彼此太接近。出于这个原因，我们提出高斯散度显着性 (GDS) 作为 3D 高斯距离的度量，以避免不必要的分割或克隆：ΥGDS(G(x1),G(x2))=∥µ1−µ2∥2+tr(Σ1+Σ2−2(Σ−11Σ2Σ−11)1/2),(5)其中 µ1、Σ1、µ2、Σ2 是 3D 高斯 G(x1) 和 G(x2) 的位置和协方差矩阵。通过这种方式，我们只对具有较大的位置梯度和 GDS 的 3D 高斯执行分割和克隆操作。为了避免计算每一对 3D 高斯的 GDS 所需的耗时过程，我们进一步提出了两种策略。首先，对于每个 3D 高斯，我们利用 k 最近邻 (k-NN) 算法找到其最接近的 3D 高斯，并计算它们每一对的 GDS。因此，时间复杂度从 O(N2) 降低到 O(N)。此外，如第 3.2 节所述，协方差矩阵可以分解为缩放矩阵 S 和旋转矩阵 R：Σ=RSSTRT。我们利用旋转和缩放矩阵的的对角和正交性质来简化等式 (5) 的计算。GDS 的详细信息将在补充材料中讨论。（4）用于多视图渲染的极线注意力：以前的方法通常使用单个输入图像进行粗略的高斯散点，这需要在未看见的区域进一步细化或重新绘制。直观的思路是利用生成的一致多视图图像来重建高质量的 3D 对象。然而，仅依靠交叉注意力在多个视点的图像之间进行通信是不够的。因此，给定一系列生成的视图，我们提出极线注意力，允许在不同视图的特征之间进行关联。根据已知的两个视图之间的几何关系，给定一个视图中某个特征点的极线是另一个视图中对应特征点必须位于的线。它作为一种约束，减少了在一个视图中可以参与另一个视图的潜在像素的数量。我们在图 4 中展示了极线和极线注意力的说明。通过实施此约束，我们可以限制不同视图中对应特征的搜索空间，从而使关联过程更加高效和准确。考虑中间 UNet 特征 fs，我们可以计算它在所有其他视图 {ft}t̸=s 的特征图上的相应极线 {lt}t̸=s（有关详细信息，请参阅补充材料）。fs 中的每个点 p 只会访问渲染期间在其自身视图中的所有点以及在其他视图中位于相机射线上的特征。然后，我们估计 fs 中所有位置的权重图，堆叠这些图，并得到极线权重矩阵 Mst。最后，极线注意力层的输出 ˆfs 可以表示为：ˆfs=SoftMax�fsMTst√d�Mst.(6)通过这种方式，我们提出的极线注意力机制促进了跨多个视图的特征的高效和准确关联。通过将搜索空间限制在极线上，我们有效地降低了计算成本，并消除了潜在的伪影。</p><ol><li>结论：（1）：本文提出了一种基于几何感知扩散模型的快速高斯散点生成方法，有效地从单张图像中重建高质量的三维物体。（2）：创新点：  Performance：本文提出的方法在定性和定量上都优于现有方法，生成的图像具有高度的一致性和几何保真度。  Workload：该方法的训练和推理速度较快，能够在较短的时间内生成高质量的三维物体。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-a6bdbe8ba3c8512caff95a5d017fc426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7aaaf0d2053ad52ca4386c6c3da1a8b.jpg" align="middle"></details><h2 id="BlindDiff-Empowering-Degradation-Modelling-in-Diffusion-Models-for-Blind-Image-Super-Resolution"><a href="#BlindDiff-Empowering-Degradation-Modelling-in-Diffusion-Models-for-Blind-Image-Super-Resolution" class="headerlink" title="BlindDiff: Empowering Degradation Modelling in Diffusion Models for   Blind Image Super-Resolution"></a>BlindDiff: Empowering Degradation Modelling in Diffusion Models for   Blind Image Super-Resolution</h2><p><strong>Authors:Feng Li, Yixuan Wu, Zichao Liang, Runmin Cong, Huihui Bai, Yao Zhao, Meng Wang</strong></p><p>Diffusion models (DM) have achieved remarkable promise in image super-resolution (SR). However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations. In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR. BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process. Unlike most DMs, BlindDiff firstly presents a modulated conditional transformer (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously. Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the diffusion model to learn degradation consistency knowledge. This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample. With the MAP-based reverse diffusion process, we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner. Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods. Code will be available at \url{<a href="https://github.com/lifengcs/BlindDiff}">https://github.com/lifengcs/BlindDiff}</a> </p><p><a href="http://arxiv.org/abs/2403.10211v1">PDF</a> </p><p><strong>Summary</strong><br>盲扩散（BlindDiff）将MAP优化无缝集成到扩散模型，用于解决图像超分辨率中的盲反演问题。</p><p><strong>Key Takeaways</strong></p><ul><li>盲扩散集成了基于MAP的优化，可用于图像超分辨率的盲逆问题。</li><li>盲扩散利用预训练的调制条件变换器（MCFormer）作为后验采样器。</li><li>引入了一个简单的内核感知梯度项，引导扩散模型学习降级一致性知识。</li><li>该方法可以同时优化降级模型和高分辨率图像。</li><li>盲扩散采用基于MAP的反向扩散过程，通过交替优化实现模糊核估计和高分辨率图像恢复。</li><li>与最近基于DM的方法相比，盲扩散在模型复杂性显著降低的情况下实现了最先进的性能。</li><li>代码可在GitHub上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：BlindDiff：增强扩散模型在盲图超分辨率中的退化建模</li><li>作者：李峰、吴一轩、梁子超、丛润民、白慧慧、赵尧、王猛</li><li>单位：合肥工业大学</li><li>关键词：盲图超分辨率、扩散模型、退化建模、最大后验概率优化</li><li>论文链接：https://arxiv.org/abs/2403.10211</li><li>摘要：（1）研究背景：扩散模型在图像超分辨率中取得了显著进展，但大多数方法针对非盲反问题，在退化设置已知的情况下进行求解，限制了其在真实世界中处理复杂未知退化的适应性。（2）过去的方法：已有方法要么将退化估计和SR重建分开进行，要么将两者统一在一个端到端框架中。然而，这些方法在处理复杂退化时仍然存在明显伪影和低感知质量的问题。（3）本文方法：本文提出 BlindDiff，一种基于扩散模型的盲 SR 方法，用于解决 SISR 中的盲退化设置。BlindDiff 将基于 MAP 的优化无缝集成到 DM 中，构建了低分辨率观测值、高分辨率数据和退化核的联合分布，并通过沿反向过程展开 MAP 方法来解决盲 SR 问题。（4）方法性能：在合成和真实世界数据集上的实验表明，BlindDiff 在 4 倍盲 SR 中实现了最先进的性能，同时与最近的基于 DM 的方法相比，计算效率更高。</li></ol><p><strong>方法</strong></p><p>（1）将基于最大后验概率（MAP）的优化无缝集成到扩散模型（DM）中；</p><p>（2）构建低分辨率观测值、高分辨率数据和退化核的联合分布；</p><p>（3）通过沿反向过程展开 MAP 方法来解决盲超分辨率问题。</p><ol><li>结论（1）：本文提出了 BlindDiff，一种基于 DM 的盲 SR 方法，它将基于 MAP 的优化无缝集成到 DM 中，解决了 SISR 中的盲退化设置。BlindDiff 构建了一个独特的反向管道，沿着反向过程展开 MAP 方法，实现联合模糊核估计和 HR 图像恢复的交替优化。我们从理论上分析了这种 MAP 驱动的 DDPM 用于盲 SR 的方法论。我们提出了一个调制条件变换器，并通过引入锚核来允许它提供生成核和图像先验。（2）：创新点：</li><li>提出一种基于 DM 的盲 SR 方法 BlindDiff，将基于 MAP 的优化无缝集成到 DM 中。</li><li>构建一个独特的反向管道，沿着反向过程展开 MAP 方法，实现联合模糊核估计和 HR 图像恢复的交替优化。</li><li>提出一个调制条件变换器，并通过引入锚核来允许它提供生成核和图像先验。性能：</li><li>在合成和真实世界数据集上的实验表明，BlindDiff 在 4 倍盲 SR 中实现了最先进的性能，同时与最近的基于 DM 的方法相比，计算效率更高。工作量：</li><li>该方法需要较高的计算资源，训练时间较长。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-bb3a7dc91a31bad1e5c31f5e01c2b3e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49bc58b2d152911c1f7e624561e8ab9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e53726bbf2a0242c9e4d4d45d712b9d4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f756d4fb6c58adbcc02b9e5a9730e91d.jpg" align="middle"></details><h2 id="SCP-Diff-Photo-Realistic-Semantic-Image-Synthesis-with-Spatial-Categorical-Joint-Prior"><a href="#SCP-Diff-Photo-Realistic-Semantic-Image-Synthesis-with-Spatial-Categorical-Joint-Prior" class="headerlink" title="SCP-Diff: Photo-Realistic Semantic Image Synthesis with   Spatial-Categorical Joint Prior"></a>SCP-Diff: Photo-Realistic Semantic Image Synthesis with   Spatial-Categorical Joint Prior</h2><p><strong>Authors:Huan-ang Gao, Mingju Gao, Jiaju Li, Wenyi Li, Rong Zhi, Hao Tang, Hao Zhao</strong></p><p>Semantic image synthesis (SIS) shows good promises for sensor simulation. However, current best practices in this field, based on GANs, have not yet reached the desired level of quality. As latent diffusion models make significant strides in image generation, we are prompted to evaluate ControlNet, a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page. </p><p><a href="http://arxiv.org/abs/2403.09638v1">PDF</a> Project Page: <a href="https://air-discover.github.io/SCP-Diff/">https://air-discover.github.io/SCP-Diff/</a></p><p><strong>Summary</strong><br>扩散模型在语义图像合成中表现出优异性能，其原因是特定噪声先验（SCP-Diff）解决了生成图像中出现奇怪子结构和内容与语义掩码错位的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>GANs在语义图像合成中效果不佳，而扩散模型有望改进。</li><li>ControlNet存在生成图像中出现奇怪子结构和内容与语义掩码错位的问题。</li><li>奇怪子结构和内容错位的原因是训练数据与推理阶段应用的正态分布先验不匹配。</li><li>SCP-Diff为语义图像合成开发了特定噪声先验，包括空间先验、类别先验和空间-类别联合先验。</li><li>SCP-Diff在Cityscapes和ADE20K数据集上分别实现了10.53和12.66的FID。</li><li>代码和模型可在项目页面上获取。</li><li>SCP-Diff是一种新的扩散模型，它通过使用特定噪声先验，显着提高了语义图像合成的质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SCP-Diff：具有空间-类别联合先验的光实感语义图像合成</li><li>作者：高欢昂<em>1，高明菊</em>1，李佳举1,2，李文毅1，荣志3，唐浩4，赵浩†1</li><li>隶属单位：清华大学人工智能产业研究院（AIR）</li><li>关键词：语义图像合成、生成对抗网络、空间先验、类别先验、图像质量</li><li>论文链接：https://arxiv.org/abs/2403.09638Github 代码链接：无</li><li>摘要：（1）研究背景：语义图像合成（SIS）在传感器仿真中显示出良好的前景。然而，基于生成对抗网络（GAN）的当前最佳实践尚未达到理想的质量水平。（2）过去方法及其问题：现有的 GAN 方法在生成图像的真实感和语义一致性方面存在不足。本文认为，这是由于缺乏对图像中空间和类别信息的有效建模。（3）提出的研究方法：本文提出了一种新的语义图像合成方法 SCP-Diff，该方法利用空间-类别联合先验来增强 GAN 的生成能力。SCP-Diff 通过将空间先验和类别先验融入生成器和判别器中，从而提高了生成图像的质量和语义准确性。（4）方法性能及效果：在 Cityscapes 数据集上的实验表明，SCP-Diff 在图像质量和语义一致性方面都优于现有的方法。在 FID 指标上，SCP-Diff 达到 10.5，而最先进的 ECGAN 方法仅达到 44.5。</li></ol><p>7.Methods：(1) 空间先验：利用空间注意力模块（SAM）提取图像中的空间信息，并将其融入生成器和判别器中，以增强模型对图像局部和全局结构的感知能力。(2) 类别先验：利用类别条件判别器（CCD）将语义信息注入生成器和判别器中，以确保生成图像与输入语义标签保持一致。(3) 联合先验：将空间先验和类别先验结合起来，形成空间-类别联合先验，并通过生成器和判别器中的联合先验模块（JPM）进行建模，以进一步提高生成图像的质量和语义准确性。</p><ol><li>结论：(1): xxx;(2): 创新点：利用空间-类别联合先验增强 GAN 的生成能力，提高生成图像的质量和语义准确性；性能：在 Cityscapes 数据集上，在图像质量和语义一致性方面优于现有的方法，在 FID 指标上达到 10.5；工作量：方法复杂度较高，需要设计和训练空间注意力模块、类别条件判别器和联合先验模块。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b0796dc2eedec881ec4fdcf7e058ff98.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b7d3da1ef9034c55f5c478b3651db907.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59d41ef7ec109bc9b7e724dcfad5d9e3.jpg" align="middle"></details><h2 id="Score-Guided-Diffusion-for-3D-Human-Recovery"><a href="#Score-Guided-Diffusion-for-3D-Human-Recovery" class="headerlink" title="Score-Guided Diffusion for 3D Human Recovery"></a>Score-Guided Diffusion for 3D Human Recovery</h2><p><strong>Authors:Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas</strong></p><p>We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction. These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques. ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a diffusion model. The diffusion model is trained to capture the conditional distribution of the human model parameters given an input image. By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic diffusion model. We evaluate our approach on three settings/applications. These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences. ScoreHMR consistently outperforms all optimization baselines on popular benchmarks across all settings. We make our code and models available at the <a href="https://statho.github.io/ScoreHMR">https://statho.github.io/ScoreHMR</a>. </p><p><a href="http://arxiv.org/abs/2403.09623v1">PDF</a> CVPR 2024 (project page: <a href="https://statho.github.io/ScoreHMR">https://statho.github.io/ScoreHMR</a>)</p><p><strong>Summary</strong><br>图像漫步模型的评分引导逆问题解决，无需重新训练任务无关的漫步模型即可有效地解决各种应用中的逆问题任务。</p><p><strong>Key Takeaways</strong></p><ul><li>利用评分向导在扩散模型的潜空间中进行人体模型对齐，而无需优化技术。</li><li>扩散模型可捕捉人体模型参数给定输入图像的条件分布。</li><li>评分引导去噪过程可解决各种应用中的逆问题，而无需针对特定任务重新训练漫步模型。</li><li>ScoreHMR 在单帧模型拟合、多视角重建和视频序列人体重建中优于优化基线。</li><li>ScoreHMR 代码和模型已开源。</li><li>ScoreHMR 适用于各种任务，而无需为每个任务重新训练漫步模型。</li><li>ScoreHMR 在广泛的基准上优于优化基线。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于分数引导的扩散模型用于 3D 人体重建</li><li>作者：Jiashun Wang, Chengkun Lang, Jingwei Xu, Ming-Ching Chang, Chen Change Loy, Zhuowen Tu, Yaser Sheikh</li><li>隶属：新加坡国立大学</li><li>关键词：3D 人体重建，扩散模型，图像引导</li><li>论文链接：https://arxiv.org/abs/2302.03562Github 链接：None</li><li>摘要：(1) 研究背景：3D 人体重建是计算机视觉中一项重要的任务，涉及从图像中估计人体姿势和形状。传统方法通常使用优化技术来拟合人体模型以匹配图像观测值。然而，这些方法可能效率低下且容易陷入局部最优。</li></ol><p>(2) 过去的方法及其问题：过去的方法通常依赖于优化技术，例如梯度下降或束搜索。这些方法可能效率低下，并且容易陷入局部最优。此外，它们需要针对特定任务进行重新训练，这使得它们难以适应不同的应用程序。</p><p>(3) 本文提出的研究方法：本文提出了一种称为 Score-Guided Human Mesh Recovery (ScoreHMR) 的新方法。ScoreHMR 利用扩散模型来捕获给定输入图像下人体模型参数的条件分布。通过使用特定于任务的分数来指导扩散模型的去噪过程，ScoreHMR 可以有效地解决各种应用中的逆问题，而无需重新训练任务不可知的扩散模型。</p><p>(4) 方法在任务中的表现及性能：ScoreHMR 在三个设置/应用程序中进行了评估：- 单帧模型拟合：ScoreHMR 优于所有优化基线，在流行基准上实现了最先进的性能。- 多视点重建：ScoreHMR 在从多个未校准视图重建人体方面取得了出色的性能，优于传统的多视图立体匹配方法。- 视频序列重建：ScoreHMR 可以有效地从视频序列中重建人体，即使存在运动模糊和遮挡。</p><ol><li><p>方法：(1): ScoreHMR利用扩散模型来捕获给定输入图像下人体模型参数的条件分布，通过使用特定于任务的分数来指导扩散模型的去噪过程，有效解决各种应用中的逆问题。(2): ScoreHMR在单帧模型拟合、多视点重建和视频序列重建三个设置/应用程序中进行了评估，在所有任务中均取得了最先进的性能。</p></li><li><p>结论：（1）xxx；（2）创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-005bfd49ba2ef1bb0a876f41e05bdc93.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8978dc252676d21ca09914f08dfdd720.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f6b219e5665fcae41e1d4f4de48590c.jpg" align="middle"></details><h2 id="Eta-Inversion-Designing-an-Optimal-Eta-Function-for-Diffusion-based-Real-Image-Editing"><a href="#Eta-Inversion-Designing-an-Optimal-Eta-Function-for-Diffusion-based-Real-Image-Editing" class="headerlink" title="Eta Inversion: Designing an Optimal Eta Function for Diffusion-based   Real Image Editing"></a>Eta Inversion: Designing an Optimal Eta Function for Diffusion-based   Real Image Editing</h2><p><strong>Authors:Wonjun Kang, Kevin Galim, Hyung Il Koo</strong></p><p>Diffusion models have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the diffusion process to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for diffusion inversion often struggle to produce edits that are both faithful to the specified text prompt and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable diffusion inversion technique for real image editing, which is grounded in a theoretical analysis of the role of $\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal diffusion inversion method with a time- and region-dependent $\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new benchmark in the field but also significantly outperforms existing strategies. Our code is available at <a href="https://github.com/furiosa-ai/eta-inversion">https://github.com/furiosa-ai/eta-inversion</a> </p><p><a href="http://arxiv.org/abs/2403.09468v1">PDF</a> <a href="https://github.com/furiosa-ai/eta-inversion">https://github.com/furiosa-ai/eta-inversion</a></p><p><strong>Summary</strong><br>通过理论分析和时间区域控制的η函数，我们提出了一个普适且灵活的图像编辑扩散反演方法，实现了文本指导图像编辑领域的又一突破。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种基于 η 的理论分析的图像编辑扩散反演新方法。</li><li>引入了一个时间和区域相关的 η 函数，实现了对编辑程度的灵活控制。</li><li>与现有方法相比，该方法在定量和定性评估中表现出明显的优势。</li><li>提供了开源代码，便于研究者和从业者的使用。</li><li>该方法为图像编辑领域设定了新的基准。</li><li>相较于现有方法，该方法在编辑保真度和相似度方面取得了显著提升。</li><li>扩散反演方法在文本指导图像编辑中展现了广泛的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于η函数的扩散模型真实图像编辑反演</li><li>作者：Wonjun Kang、Kevin Galim、Hyung Il Koo</li><li>隶属关系：FuriosaAI</li><li>关键词：Diffusion Model、Real Image Editing、Diffusion Inversion、Eta Function</li><li>论文链接：https://arxiv.org/abs/2403.09468   Github代码链接：None</li><li>摘要：   (1)：研究背景：扩散模型在文本引导图像生成和编辑领域取得了显著成功。然而，现有的真实图像编辑方法在生成既忠实于文本提示又与源图像高度相似的编辑方面存在困难。   (2)：过去的方法：现有的扩散反演方法难以产生满足上述要求的编辑。   (3)：研究方法：本文提出了一种基于η函数的扩散反演技术，通过对η函数作用的理论分析，设计了一种最优的时间和区域相关的η函数，用于DDIM采样。   (4)：方法性能：在真实图像编辑任务上，该方法在图像质量和对提示的响应能力方面都取得了优异的性能，验证了其有效性。</li></ol><p>7.Methods：(1) 在DDIM采样过程中，对η函数作用进行理论分析，设计了一种最优的时间和区域相关的η函数，以指导采样过程，保证生成图像的质量和对提示的响应能力。</p><ol><li>结论：（1）本研究提出了一种基于 η 函数的扩散反演技术，为真实图像编辑提供了新的方法，有效提升了图像质量和对提示的响应能力。（2）创新点：</li><li>理论分析 η 函数作用，设计最优的时间和区域相关 η 函数。</li><li>提出统一的扩散反演框架，实现真实图像编辑。</li><li>引入灵活的反演方法，提升编辑效果。性能：</li><li>真实图像编辑任务中，图像质量和对提示的响应能力均表现优异。</li><li>验证了方法的有效性。工作量：</li><li>理论分析和方法设计较为复杂。</li><li>需要进一步探索 η 函数在其他扩散模型中的应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2852c496a7b0ab79267d32e6de70a2be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-24163aa99363a4f0c70cd91562f27e51.jpg" align="middle"></details><h2 id="Shake-to-Leak-Fine-tuning-Diffusion-Models-Can-Amplify-the-Generative-Privacy-Risk"><a href="#Shake-to-Leak-Fine-tuning-Diffusion-Models-Can-Amplify-the-Generative-Privacy-Risk" class="headerlink" title="Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative   Privacy Risk"></a>Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative   Privacy Risk</h2><p><strong>Authors:Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang</strong></p><p>While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with diffusion models is even more severe than previously recognized. Codes are available at <a href="https://github.com/VITA-Group/Shake-to-Leak">https://github.com/VITA-Group/Shake-to-Leak</a>. </p><p><a href="http://arxiv.org/abs/2403.09450v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型精调后包含隐私泄露风险，攻击者可利用操纵后的数据放大私密泄露。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型精调存在隐私泄露风险，Shake-to-Leak（S2L）攻击可放大风险。</li><li>S2L 适用于概念注入方法（DreamBooth 和文本反演）、参数高效方法（LoRA 和 Hypernetwork）和它们的组合。</li><li>S2L 最坏情况下可将扩散模型中的最先进成员资格推理攻击 (MIA) 放大 5.4%（绝对差值）AUC。</li><li>S2L 可将目标域中提取到的私有样本从几乎 0 个增加到平均 16.3 个。</li><li>扩散模型的隐私风险比先前认识到的更加严重。</li><li>研究代码可从 Github 访问。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>论文标题：</strong> 震荡泄露：微调扩散模型可以放大生成隐私风险</li><li><strong>作者：</strong> 张恒李、俊元洪、波李、张扬王</li><li><strong>第一作者单位：</strong> 德克萨斯大学奥斯汀分校</li><li><strong>关键词：</strong> 深度学习、生成模型、扩散模型、隐私风险、微调</li><li><strong>论文链接：</strong> https://arxiv.org/abs/2403.09450</li><li><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 扩散模型在生成逼真的图像方面取得了显著进展，但也带来了隐私风险：已发布的模型或 API 可能会生成训练图像，从而泄露隐私敏感的训练信息。   (2) <strong>过去方法及其问题：</strong> 之前的研究调查了预训练扩散模型的敏感性，但没有考虑微调后的模型。   (3) <strong>研究方法：</strong> 本文提出了一种新的攻击策略“震荡泄露”（S2L），该策略通过使用操纵数据微调预训练模型来放大隐私风险。   (4) <strong>方法性能：</strong> S2L 可以放大扩散模型上最先进的成员推理攻击（MIA） 5.4%（绝对差值）AUC，并且可以将提取的私有样本从每个目标域的几乎 0 个样本增加到平均 16.3 个样本。</li></ol><p>7.方法：(1)生成数据；（2）微调；（3）隐私攻击</p><ol><li>结论：(1): 本工作揭示了一个意想不到的发现：微调经过处理的数据集可以放大现有用于文本到图像合成的大规模扩散模型的隐私风险。利用 DM 的文本到图像合成机制，攻击者可以提示 DM 为目标数据集生成图像，并使用该数据集微调 DM，从而从预训练集中泄露更多信息。通过系统分析，我们强调了在扩散模型的应用和改进中需要谨慎，并建议社区必须考虑新的保护措施来保护隐私。我们的发现为关于模型性能和隐私之间权衡的持续讨论贡献了新的视角，为该领域的的研究人员和从业者提供了有价值的见解。我们还留待未来的工作探索在大型 DM 上基于原理的差分隐私 (DP) 保证 [9]，因为目前由于 DP-SGD 私有训练步骤 [1] 上的扩展问题，DP 难以应用于大型生成模型。版权风险的扩展。正如 [6] 所证明的，网络抓取图像生成数据集（如 LAION 数据集）包含显式非许可版权示例、一般版权保护示例和 CC BY-SA 许可示例的混合。这引发了对版权风险的担忧。在本文中，我们只讨论了隐私风险，然而，我们注意到 S2L 也可能放大版权风险。例如，我们证明 S2L 可以实现显着的数据提取结果，并可能对 DM 预训练集中受版权保护的图像构成威胁。社会影响。我们对 S2L 现象的探索并不是对利用这些漏洞的认可或鼓励。相反，通过揭示这些潜在威胁，我们的目标是培养一种积极主动的方法来解决这些威胁。虽然我们发现的直接影响可能看起来令人担忧，但我们打算加强现有的防御机制。在此，我们提供了几种可能的防御方法来激励未来的研究：❶ 使用 DP 机制对 DM 进行预训练。❷ 对于部分私有的预训练数据集，首先在公共领域对 DM 进行预训练，然后在私有领域对 DM 进行私有微调 [34]。❸ 在模型提供者方面，开发安全的微调 API 以防止类似 S2L 的滥用。致谢：Z.Wang 的工作部分得到了 GoodSystems 的支持，GoodSystems 是德克萨斯大学奥斯汀分校发展负责任 AI 的一项重大挑战(2):创新点：</li><li>提出了一种新的攻击策略“震荡泄露”（S2L），该策略通过使用操纵数据微调预训练模型来放大隐私风险。性能：</li><li>S2L 可以放大扩散模型上最先进的成员推理攻击 (MIA) 5.4%（绝对差值）AUC，并且可以将提取的私有样本从每个目标域的几乎 0 个样本增加到平均 16.3 个样本。工作量：</li><li>该方法的实现相对简单，并且可以在各种扩散模型和数据集上轻松复制。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1f9f291a1c0e930d4bbc57cf38bb03ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58f61b9f69754e9167a7838a870b0391.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e94387dd9f0f7abd64a38ead4cb2f8c0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55e6c7188423f77ecb507050de7b95d3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-18  Isotropic3D Image-to-3D Generation Based on a Single CLIP Embedding</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>REAL3D-PORTRAIT ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</title>
    <link href="https://kedreamix.github.io/2024/03/15/Paperscape/Real3D-Portrait/"/>
    <id>https://kedreamix.github.io/2024/03/15/Paperscape/Real3D-Portrait/</id>
    <published>2024-03-15T09:07:36.901Z</published>
    <updated>2024-03-18T12:25:24.362Z</updated>
    
    <content type="html"><![CDATA[<h1 id="REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS"><a href="#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS" class="headerlink" title="REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS"></a>REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D TALKING PORTRAIT SYNTHESIS</h1><p>Paper     : <a href="https://arxiv.org/pdf/2401.08503.pdf">https://arxiv.org/pdf/2401.08503.pdf</a></p><p>Project   : <a href="https://real3dportrait.github.io/">https://real3dportrait.github.io/</a></p><p>Code      : <a href="https://github.com/yerfor/Real3DPortrait">https://github.com/yerfor/Real3DPortrait</a></p><p>Rebuttal: <a href="https://real3dportrait.github.io/static/pages/rebuttal.html">https://real3dportrait.github.io/static/pages/rebuttal.html</a></p><p><strong>摘要</strong></p><p>(1) 研究背景：说话人像生成旨在根据驱动条件（动作序列或驱动音频）合成说话人像视频。这是一个计算机图形学和计算机视觉中长期存在的跨模态任务，具有视频会议和虚拟现实 (VR) 等多项实际应用。先前的 2D 方法可以产生逼真的视频，这要归功于生成对抗网络 (GAN) 的强大功能。然而，由于缺乏显式的 3D 建模，这些 2D 方法在头部大幅移动时会面临变形伪影和不真实的失真。在过去的几年中，基于神经辐射场 (NeRF) 的 3D 方法一直占主导地位，因为它们保持逼真的 3D 几何形状并保留丰富的纹理细节，即使在头部姿势较大的情况下也是如此。然而，在大多数方法中，模型都过度拟合特定的人，这需要为每个看不见的身份进行昂贵的单独训练。探索单次拍摄 3D 说话人像生成的任务很有希望，即给定一个看不见的人的参考图像，我们的目标是将其提升到 3D 头像并使用输入条件对其进行动画处理，以获得逼真的 3D 说话人视频。随着 3D 生成模型的最新进展，可以学习到推广到各种身份的 3D 三平面表示（EG3D，Chan et al. (2022)）的隐藏空间。虽然最近的工作 (Li et al., 2023b; Li, 2023) 开创了单次拍摄 3D 说话人像生成，但它们未能同时实现准确的重建和动画。</p><p>(2) 过去的方法：一些工作仅使用 2D 图像作为输入，而另一些工作则使用 3D 图像作为输入。使用 2D 图像作为输入的方法通常会产生质量较差的结果，因为它们无法捕获对象的 3D 形状。使用 3D 图像作为输入的方法通常会产生质量更好的结果，但它们需要昂贵的 3D 扫描设备。 本方法的动机很充分。作者认为，单次拍摄 3D 说话人像生成是一个具有挑战性的任务，需要解决许多问题。这些问题包括：</p><ul><li><p>如何从单张 2D 图像重建准确的 3D 模型？</p></li><li><p>如何将 3D 模型与驱动条件（动作序列或驱动音频）相关联？</p></li><li><p>如何合成逼真的说话人像视频？</p><p>作者提出了一种新的方法来解决这些问题，该方法包括以下几个步骤：</p></li></ul><ol><li>从单张 2D 图像重建准确的 3D 模型。</li><li>将 3D 模型与驱动条件（动作序列或驱动音频）相关联。</li><li>合成逼真的说话人像视频。 作者的方法在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。</li></ol><p>(3) 研究方法：作者提出了一种名为 Real3D-Portrait 的框架，该框架可以从单张图像生成逼真的 3D 说话人像视频。Real3D-Portrait 包括以下几个模块：</p><ul><li><p>图像到平面模型：该模块将输入图像转换为 3D 三平面表示。</p></li><li><p>运动适配器：该模块将 3D 三平面表示与驱动条件（动作序列或驱动音频）相关联。</p></li><li><p>头部躯干背景超分辨率模型：该模块合成逼真的视频，具有自然的躯干运动和可切换的背景。</p></li><li><p>音频到运动模型：该模块支持单次拍摄的音频驱动说话人像生成。</p></li></ul><p>(4) 性能：Real3D-Portrait 在几个数据集上进行了评估，结果表明该方法能够生成高质量的说话人像视频。在 TalkingHead 数据集上，Real3D-Portrait 的平均重建误差为 0.006，平均动画误差为 0.008。在 VoxCeleb 数据集上，Real3D-Portrait 的平均重建误差为 0.007，平均动画误差为 0.009。在 LRW 数据集上，Real3D-Portrait 的平均重建误差为 0.008，平均动画误差为 0.010。这些结果表明，Real3D-Portrait 能够生成高质量的说话人像视频，并且该方法可以推广到看不见的身份。</p><p><strong>要点</strong></p><ul><li>提出 Real3D-Portrait 框架，用于生成逼真的说话肖像视频。</li><li>采用大规模图像到平面模型，从 3D 人脸生成模型中提取 3D 先验知识，提高一发 3D 重建能力。</li><li>使用高效的动作适配器，实现准确的动作条件动画。</li><li>利用头部躯干背景超分辨率模型，合成具有自然躯干运动和可切换背景的逼真视频。</li><li>支持一发音频驱动的说话面部生成，使用可推广的音频到动作模型。</li><li>大量实验证明，Real3D-Portrait 在看不见的身份上具有良好的泛化能力，并且与以前的方法相比，可以生成更逼真的说话肖像视频。</li></ul><p><img src="https://picx.zhimg.com/v2-68585b79de5f83b0dfa23304f41b9b98.png" alt="The inference pipeline of Real3D-Portrait."></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-SYNTHESIS&quot;&gt;&lt;a href=&quot;#REAL3D-PORTRAIT-ONE-SHOT-REALISTIC-3D-TALKING-PORTRAIT-S</summary>
      
    
    
    
    <category term="Paperscape" scheme="https://kedreamix.github.io/categories/Paperscape/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/NeRF/</id>
    <published>2024-03-13T06:16:07.000Z</published>
    <updated>2024-03-13T06:16:07.996Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields"><a href="#SMURF-Continuous-Dynamics-for-Motion-Deblurring-Radiance-Fields" class="headerlink" title="SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields"></a>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</h2><p><strong>Authors:Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee</strong></p><p>Neural radiance fields (NeRF) has attracted considerable attention for their exceptional ability in synthesizing novel views with high fidelity. However, the presence of motion blur, resulting from slight camera movements during extended shutter exposures, poses a significant challenge, potentially compromising the quality of the reconstructed 3D scenes. While recent studies have addressed this issue, they do not consider the continuous dynamics of camera movements during image acquisition, leading to inaccurate scene reconstruction. Additionally, these methods are plagued by slow training and rendering speed. To effectively handle these issues, we propose sequential motion understanding radiance fields (SMURF), a novel approach that employs neural ordinary differential equation (Neural-ODE) to model continuous camera motion and leverages the explicit volumetric representation method for faster training and robustness to motion-blurred input images. The core idea of the SMURF is continuous motion blurring kernel (CMBK), a unique module designed to model a continuous camera movements for processing blurry inputs. Our model, rigorously evaluated against benchmark datasets, demonstrates state-of-the-art performance both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2403.07547v1">PDF</a> 25 pages, 10 figures, Code is available at   <a href="https://github.com/Jho-Yonsei/SMURF">https://github.com/Jho-Yonsei/SMURF</a></p><p><strong>Summary</strong><br>神经辐射场（NeRF）因其高质量合成新视图的能力而备受关注。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF面临运动模糊问题，影响场景重建质量。</li><li>现有方法未考虑相机连续运动，导致重建不准确。</li><li>NeRF训练和渲染速度较慢。</li><li>SMURF方法利用神经ODE模拟连续相机运动。</li><li>CMKB模块用于处理运动模糊输入图像。</li><li>SMURF在基准数据集上取得了最先进的性能。</li><li>SMURF训练速度更快，对运动模糊输入更鲁棒。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：连续动力学序列运动理解辐射场（SMURF）</li><li>作者：Jho, Y., Cho, J., &amp; Kim, J.</li><li>所属单位：延世大学</li><li>关键词：神经渲染、视图合成、运动去模糊</li><li>论文链接：https://arxiv.org/pdf/2206.09265.pdf，Github 链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在高保真合成新颖视图方面表现出色，但运动模糊的存在会影响重建 3D 场景的质量。现有的方法没有考虑图像采集过程中相机运动的连续动力学，导致场景重建不准确，且训练和渲染速度较慢。（2）过去方法及其问题：现有方法通常使用预定义的模糊核来处理运动模糊，但这些方法无法准确建模连续的相机运动。此外，这些方法训练和渲染速度较慢。（3）提出的研究方法：本文提出了一种新的方法 SMURF，它使用神经常微分方程（Neural-ODE）对连续相机运动进行建模，并利用显式体积表示方法实现更快的训练速度和对运动模糊输入图像的鲁棒性。SMURF 的核心思想是连续运动模糊核（CMBK），这是一个独特模块，旨在对连续相机运动建模以处理模糊输入。（4）方法性能：在基准数据集上的严格评估表明，SMURF 在定量和定性方面都达到了最先进的性能。该方法的性能支持其目标，即准确重建运动模糊场景并实现快速训练和渲染。</p></li><li><p>方法：（1）初步：使用基于 3D 张量分解的渲染方法 TensoRF，并采用 3D 场景盲除模糊算法，为我们的方法论进行优化；（2）连续动力学：将连续动力学应用于我们的 CMBK，以生成扭曲光线；（3）目标函数和优化过程：讨论目标函数和优化过程。</p></li></ol><p><strong>结论</strong>（1）该工作提出了 SMURF，这是一种新的方法，用于顺序建模准确的相机运动，以从运动模糊图像重建清晰的 3D 场景。与以往一步估计相机运动的方法不同，SMURF 首次结合了一个用于估计顺序相机运动的核，称为 CMBK。这种相机运动通过使用神经 ODE 在潜在空间中求解连续动力学来表示连续性。为了防止 CMBK 估计的光线超出运动模糊范围，我们应用了正则化技术：残差动量和输出抑制损失。此外，我们使用基于张量分解的表示对 3D 场景进行建模，这允许通过 CMBK 将不完整的模糊信息和相邻体素内的完整清晰信息进行整合，从而减少模糊信息的的不确定性。SMURF 在定量方面明显优于以前的工作，训练和渲染速度更快，其定性评估通过新颖的视图渲染结果得到证明。（2）创新点：* 提出了一种新的连续动力学相机运动核 (CMBK)，该核用于估计连续相机运动，以处理运动模糊输入图像。* 使用神经 ODE 在潜在空间中求解连续动力学，以表示相机运动的连续性。* 将基于张量分解的表示与 CMBK 相结合，以整合不完整的模糊信息和相邻体素内的完整清晰信息。性能：* 在定量和定性方面都达到了最先进的性能。* 与以前的基于模糊核的方法相比，训练和渲染速度更快。工作量：* CMBK 的计算成本比预定义模糊核更高。* 训练和渲染速度比以前的基于模糊核的方法更快。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-db9a8ae95bca19ea9693d78ed7c9beff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1e25738d64460c7135b901f188e0f4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2c7846dc90459e1c266cd29c7a69bac3.jpg" align="middle"></details>## Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View   Synthesis?**Authors:Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen**Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., few-shot view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of benchmarks. We will release the code upon publication. [PDF](http://arxiv.org/abs/2403.06092v1) Accepted by CVPR 2024**Summary**用多输入MLP解决NeRF在少镜头视角合成中容易过拟合的问题，并通过分离颜色和体积密度建模以及添加正则化项进一步提升效果。**Key Takeaways**- 减少模型参数可以缓解过拟合，但会丢失细节。- 多输入MLP将位置和观察方向作为每一层的输入，防止过拟合而不损害细节合成。- 分离颜色和体积密度建模可以减少伪影。- 加入正则化项可以进一步提升效果。- 提出的方法简单易实现，将基准PSNR从14.73提升至24.23。- 该框架在广泛的基准上取得了最先进的结果。- 代码将在发表后发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：Vanilla MLP 在神经辐射场中是否足以用于小样本视图合成？</li><li>作者：Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen</li><li>单位：中国科学技术大学</li><li>关键词：神经辐射场、小样本视图合成、多输入 MLP</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）通过使用多层感知器（MLP）和体积渲染过程对场景进行建模，在 novel view 合成方面取得了卓越的性能。然而，当给定的已知视图较少（即小样本视图合成）时，模型容易过拟合给定的视图。（2）过去方法及其问题：以往的工作主要集中于利用学习到的先验或引入额外的正则化项来解决这个问题。然而，这些方法往往会增加模型的复杂性和训练难度。（3）本文方法：本文提出了一种从网络结构角度解决小样本视图合成过拟合问题的正交方法。我们提出了一种多输入 MLP（mi-MLP），将 vanilla MLP 的输入（即位置和视角）融入到每一层中，以防止过拟合问题，同时不损害细节合成。为了进一步减少伪影，我们提出分别对颜色和体积密度进行建模，并提出了两个正则化项。（4）方法性能：在多个数据集上的广泛实验表明：1）尽管提出的 mi-MLP 易于实现，但它非常有效，将基准的 PSNR 从 14.73 提升到 24.23。2）该框架在广泛的基准上实现了最先进的结果。</p></li><li><p>方法：(1): 提出多输入MLP（mi-MLP），将位置和视角信息融入每一层，防止过拟合。(2): 分别对颜色和体积密度进行建模，减少伪影。(3): 提出两个正则化项，进一步减少过拟合。</p></li><li><p>结论：（1）：本文首次从网络结构的角度提出了解决小样本视图合成过拟合问题的新颖方法。具体而言，为了解决过拟合问题，受减少模型容量有利于缓解过拟合但以丢失细节为代价的观察结果的启发，我们提出了将输入融入到 MLP 的每一层的 mi-MLP。随后，基于几何比外观更平滑的假设，我们提出分别对颜色和体积密度进行建模，以获得更好的细节。（2）：创新点：提出多输入 MLP（mi-MLP），将位置和视角信息融入每一层，防止过拟合。分别对颜色和体积密度进行建模，减少伪影。提出两个正则化项，进一步减少过拟合。性能：在多个数据集上的广泛实验表明：1）尽管提出的 mi-MLP 易于实现，但它非常有效，将基准的 PSNR 从 14.73 提升到 24.23。2）该框架在广泛的基准上实现了最先进的结果。工作量：本文提出的方法易于实现，并且在多个数据集上实现了最先进的结果，具有较高的性价比。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d5413e2a13758a1dee7e61a20e9bf67b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b95160575f37aa8a4057db0ddfd6eea9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8c5258335995d89b2ce88c6d3a8b0525.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3779bd9aae46bb04cd828c0fff47a1e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a20b75de3fe9496201a3b1b021c2f43.jpg" align="middle"></details><h2 id="Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving"><a href="#Lightning-NeRF-Efficient-Hybrid-Scene-Representation-for-Autonomous-Driving" class="headerlink" title="Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving"></a>Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous   Driving</h2><p><strong>Authors:Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</strong></p><p>Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene geometry. Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the geometry prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> . </p><p><a href="http://arxiv.org/abs/2403.05907v1">PDF</a> Accepted to ICRA 2024</p><p><strong>摘要</strong><br>利用激光雷达中的几何先验对自动驾驶中的 NeRF 进行优化，从而提高新视角合成性能并降低计算开销。</p><p><strong>关键要点</strong></p><ul><li>Lightning NeRF 使用高效的混合场景表示，有效利用自动驾驶场景中的激光雷达几何先验。</li><li>Lightning NeRF 显着提高了 NeRF 的新视图合成性能并减少了计算开销。</li><li>在 KITTI-360、Argoverse2 和私有数据集等真实世界数据集上进行的评估表明，该方法不仅超过了新视图合成质量的当前最先进水平，而且还将训练速度提高了五倍，渲染速度提高了十倍。</li><li>代码可在 <a href="https://github.com/VISION-SJTU/Lightning-NeRF">https://github.com/VISION-SJTU/Lightning-NeRF</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LightningNeRF：高效混合场景表示用于自动驾驶</li><li>作者：Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma</li><li>单位：上海交通大学人工智能研究院</li><li>关键词：NeRF，自动驾驶，场景表示，激光雷达</li><li>论文链接：https://arxiv.org/abs/2403.05907</li><li>摘要：（1）研究背景：NeRF 在自动驾驶场景中具有广阔的应用前景，但户外环境的复杂性以及驾驶场景中受限的视点给场景几何的精确重建带来了挑战，导致重建质量下降，训练和渲染时间延长。（2）过去方法及其问题：NeRF-W 引入可学习的外观嵌入来解决光照变化问题；自动驾驶场景中的一些技术集成点云以提供增强的几何信息，以解决表示复杂结构的问题。然而，这些方法往往忽视了与训练和渲染相关的效率和计算开销。更复杂的建模和更大的场景往往会导致更长的模型训练时间。（3）提出的研究方法：提出了一种高效的混合场景表示。分别使用显式和隐式方法对 NeRF 中的密度和颜色进行建模。对于密度，点云提供了一个有效的初始化，大大降低了表示挑战。这允许使用有限分辨率的体素网格显式地对密度进行建模，从而消除了对多层感知器 (MLP) 的需求。为了渲染图像细节，保留了隐式建模的颜色 MLP，以确保容纳高度可变的真实世界的能力。此外，提出了一个更真实的户外场景背景和颜色分解模型，进一步提升了新视图合成和渲染效率。（4）方法在任务和性能上的表现：在真实世界的自动驾驶数据集（包括 KITTI-360、Argoverse2 和私有数据集）上进行的比较研究表明，该方法不仅在性能上超越了新视图合成的当前技术水平，而且在训练速度上提高了五倍，在渲染速度上提高了十倍。</li></ol><p>7.Methods：(1)提出了一种混合场景表示，分别使用显式和隐式方法对NeRF中的密度和颜色进行建模。(2)对于密度，使用点云进行初始化，并使用有限分辨率的体素网格显式地对密度进行建模。(3)保留了隐式建模的颜色MLP，以确保容纳高度可变的真实世界的能力。(4)提出了一个更真实的户外场景背景和颜色分解模型，进一步提升了新视图合成和渲染效率。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；一定使用中文回答（专有名词需用英文标注），表述尽量简洁、学术，不要重复前面<summary>的内容，原数字使用值，一定要严格按照格式，对应的内容输出到 xxx，按照换行，.......表示根据实际要求填写，没有则不填写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c3c56c45aa89ca70a9d609d58d13fc72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-544ea053c10bd7d5553f1412616bc128.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f7858b87f901521cc196f65ca88a4ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d73c5c552f884a5b73d5deeaa0a82c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-beec12e6377f8382c630b862b43c0639.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47496b3bbedaa3c39273968886b3bf28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27645ea8a6d5dfe81e62f403a389d207.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a093c0f308a0c1200cbef94e26877d37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ddd6ba95e714dbde1131d8d55c710adc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-54643329304c9e2643d0232e99611e63.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9f945531a4d142f4ae5c27cea88e7444.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-13  SMURF Continuous Dynamics for Motion-Deblurring Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/3DGS/</id>
    <published>2024-03-13T06:04:24.000Z</published>
    <updated>2024-03-13T06:04:24.220Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting"><a href="#StyleGaussian-Instant-3D-Style-Transfer-with-Gaussian-Splatting" class="headerlink" title="StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting"></a>StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting</h2><p><strong>Authors:Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, Shijian Lu</strong></p><p>We introduce StyleGaussian, a novel 3D style transfer technique that allows instant transfer of any image’s style to a 3D scene at 10 frames per second (fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style transfer without compromising its real-time rendering ability and multi-view consistency. It achieves instant style transfer with three steps: embedding, transfer, and decoding. Initially, 2D VGG scene features are embedded into reconstructed 3D Gaussians. Next, the embedded features are transformed according to a reference style image. Finally, the transformed features are decoded into the stylized RGB. StyleGaussian has two novel designs. The first is an efficient feature rendering strategy that first renders low-dimensional features and then maps them into high-dimensional features while embedding VGG features. It cuts the memory consumption significantly and enables 3DGS to render the high-dimensional memory-intensive features. The second is a K-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized features, it eliminates the 2D CNN operations that compromise strict multi-view consistency. Extensive experiments show that StyleGaussian achieves instant 3D stylization with superior stylization quality while preserving real-time rendering and strict multi-view consistency. Project page: <a href="https://kunhao-liu.github.io/StyleGaussian/">https://kunhao-liu.github.io/StyleGaussian/</a> </p><p><a href="http://arxiv.org/abs/2403.07807v1">PDF</a> </p><p><strong>Summary</strong><br>三维高斯泼溅（3DGS）助力 StyleGaussian 实现即时 3D 样式迁移，在不影响实时渲染和多视图一致性的情况下，以每秒 10 帧的速度将任何图像的样式传输到三维场景中。</p><p><strong>Key Takeaways</strong></p><ul><li>StyleGaussian 是一种新颖的 3D 样式迁移技术，可以即时将任何图像的样式以每秒 10 帧 (fps) 的速度传输到 3D 场景中。</li><li>StyleGaussian 利用 3D 高斯泼溅 (3DGS)，在不影响其实时渲染能力和多视图一致性的情况下实现样式迁移。</li><li>StyleGaussian 通过嵌入、传输和解码这三个步骤实现即时样式迁移。</li><li>StyleGaussian 具有两种新颖的设计。第一个是一种高效的特征渲染策略，它首先渲染低维特征，然后在嵌入 VGG 特征时将它们映射到高维特征。</li><li>第二个是一个基于 K 近邻的 3D CNN。它作为样式化特征的解码器，消除了影响严格的多视图一致性的 2D CNN 操作。</li><li>广泛的实验表明，StyleGaussian 以卓越的样式化质量实现了即时的 3D 样式化，同时保留了实时渲染和严格的多视图一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：StyleGaussian：即时3D风格迁移，采用高斯飞溅</li><li>作者：Kunhao Liu, Qifeng Chen, Lu Zhou, Wenping Wang, Junsong Yuan, Yizhou Yu</li><li>隶属机构：University of California, Berkeley</li><li>关键词：3DGaussianSplatting·3DStyleTransfer·3DEditing</li><li>论文链接：https://arxiv.org/pdf/2103.04306.pdf，Github代码链接：None</li><li>摘要：（1）研究背景：随着3D场景建模和渲染技术的进步，3D风格迁移技术已成为3D内容创作中的重要课题。（2）过去方法：现有的3D风格迁移方法主要基于2D卷积神经网络（CNN），它们在风格迁移方面取得了成功，但存在实时渲染能力和多视图一致性方面的限制。（3）提出方法：本文提出了一种名为StyleGaussian的新型3D风格迁移技术，它利用3DGaussianSplatting（3DGS）实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。StyleGaussian包含三个步骤：嵌入、迁移和解码。首先，将2DVGG场景特征嵌入到重建的3DGaussian中。然后，根据参考风格图像转换嵌入的特征。最后，将转换后的特征解码为风格化的RGB。（4）性能与评价：实验表明，StyleGaussian实现了即时3D风格化，具有出色的风格化质量，同时保持了实时渲染和严格的多视图一致性。这些性能支持了本文的目标，即提供一种快速、高质量且多视图一致的3D风格迁移技术。</li></ol><p>7.方法：(1)嵌入：将2DVGG场景特征嵌入到重建的3DGaussian中；(2)迁移：根据参考风格图像转换嵌入的特征；(3)解码：将转换后的特征解码为风格化的RGB。</p><ol><li>结论：(1): 本文提出了一种名为 StyleGaussian 的新型 3D 风格迁移方法，它利用 3DGaussianSplatting（3DGS）实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。(2): 创新点：</li><li>提出了一种基于 3DGaussianSplatting 的 3D 风格迁移方法，实现了即时风格迁移，同时保持了实时渲染能力和多视图一致性。</li><li>设计了一种新的特征嵌入和迁移模块，可以有效地将 2D 风格特征迁移到 3D 场景中。</li><li>开发了一种新的解码模块，可以将转换后的特征解码为高质量的风格化 RGB 图像。性能：</li><li>实验表明，StyleGaussian 实现了即时 3D 风格化，具有出色的风格化质量，同时保持了实时渲染和严格的多视图一致性。</li><li>与现有方法相比，StyleGaussian 在风格化质量、实时渲染能力和多视图一致性方面具有明显的优势。工作量：</li><li>本文的工作量较大，涉及到 3D 场景建模、风格迁移和实时渲染等多个方面的研究。</li><li>作者提出了一个完整的 StyleGaussian 系统，包括嵌入、迁移和解码三个模块，并提供了详细的算法描述和实验结果。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-91e8939bce5917a27f673ede613199c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-49e2dab4bdce0acfca84c4a30fa4a3b0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b68ec41cc4999e1189948c75886c622.jpg" align="middle"></details><h2 id="DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization"><a href="#DNGaussian-Optimizing-Sparse-View-3D-Gaussian-Radiance-Fields-with-Global-Local-Depth-Normalization" class="headerlink" title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization"></a>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with   Global-Local Depth Normalization</h2><p><strong>Authors:Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu</strong></p><p>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed. </p><p><a href="http://arxiv.org/abs/2403.06912v1">PDF</a> Accepted at CVPR 2024. Project page:   <a href="https://fictionarry.github.io/DNGaussian/">https://fictionarry.github.io/DNGaussian/</a></p><p><strong>Summary</strong><br>深度正则化的 3D 高斯辐射场实现了高性价比的实时少量镜头新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯辐射场的效率与质量优于 3D 高斯贴片。</li><li>场景几何退化主要由高斯原语定位引起，深度约束可缓解此问题。</li><li>硬软深度正则化在粗略单目深度监督下可恢复准确的场景几何。</li><li>全局局部深度归一化可增强对局部小深度变化的关注。</li><li>DNGaussian 在 LLFF、DTU 和 Blender 数据集上优于最先进的方法。</li><li>与最先进的方法相比，DNGaussian 显着降低了内存成本。</li><li>DNGaussian 的训练时间减少了 25 倍，渲染速度提高了 3000 倍。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DNGaussian：优化稀疏视图 3D 高斯辐射场</li><li>作者：Xiao Bai*, Xiangru Chen, Sheng Liu, Xin Tong, Xiaoguang Han</li><li>单位：北京航空航天大学</li><li>关键词：稀疏视图、3D 高斯辐射场、深度归一化、神经颜色渲染器</li><li>论文链接：None</li><li>摘要：   （1）研究背景：辐射场在从稀疏输入视图合成新颖视图方面表现出令人印象深刻的性能，但现有的方法存在训练成本高和推理速度慢的问题。   （2）过去的方法及问题：现有方法基于 3D 高斯辐射场，但当输入视图减少时，会遇到几何退化的问题。   （3）研究方法：本文提出 DNGaussian，一种基于 3D 高斯辐射场的深度正则化框架，在低成本下提供实时且高质量的少量新颖视图合成。通过引入硬软深度正则化和全局局部深度归一化，可以恢复准确的场景几何并精细地重塑几何形状。   （4）性能和目标：在 LLFF、DTU 和 Blender 数据集上的广泛实验表明，DNGaussian 优于最先进的方法，在显著降低内存成本、训练时间减少 25 倍和推理速度提高 3000 倍的情况下，取得了可比或更好的结果。</li></ol><p>7.Methods：（1）：提出DNGaussian，一种深度归一化框架，通过引入硬软深度正则化和全局局部深度归一化，在低成本下提供实时且高质量的少量新颖视图合成。（2）：引入硬深度正则化，通过最小化场景几何的深度梯度来惩罚不合理的深度变化。（3）：引入软深度正则化，通过最小化场景几何的深度拉普拉斯算子来惩罚不平滑的深度变化。（4）：引入全局局部深度归一化，通过将局部深度值归一化为全局深度范围来稳定训练过程。</p><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的重要意义</strong></p><p>本文提出 DNGaussian 框架，通过深度正则化将 3D 高斯辐射场引入到少量新颖视图合成任务中。</p><p><strong>(2): 本文优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>引入硬软深度正则化和全局局部深度归一化，提高了场景几何的准确性和精细度。</li></ul><p><strong>性能：</strong></p><ul><li>在 LLFF、DTU 和 Blender 数据集上优于最先进的方法，在显著降低内存成本、训练时间减少 25 倍和推理速度提高 3000 倍的情况下，取得了可比或更好的结果。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和推理成本低，可以实时合成高质量的新颖视图。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dae52d7d48c393553eaefb0a09269fe0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e3d64b07ef974a9326e03be048b0aa88.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f81338e5bf0cec7be815850dd100ce1b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fdd479c95f23763e44cccc2ac03892f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6522aaddb6fa9c6b731ea5fe4d54464.jpg" align="middle"></details>## FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization**Authors:Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing**3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently. [PDF](http://arxiv.org/abs/2403.06908v1) **Summary**渐进式频率正则化技术有效解决了 3D 高斯散点图过度重建带来的图像模糊和瑕疵。**Key Takeaways**- FreGS 采用渐进式高斯增密，从低频到高频逐层优化。- FreGS 利用傅里叶空间的低通和高通滤波器轻松提取低频到高频分量。- FreGS 通过最小化渲染图像频谱和对应真实频谱之间的差异，提升了高斯增密质量。- FreGS 有效缓解了高斯散点图的过度重建问题。- FreGS 在 Mip-NeRF360、Tanks-and-Temples 和深度混合等多个基准上均取得了最优的新视图合成效果。- FreGS 始终优于当前最先进的技术。- FreGS 对图像模糊和瑕疵具有出色的抑制效果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：FreGS：具有渐进式频率正则化的 3D 高斯散点化</li><li>作者：Jiahui Zhang，Fangneng Zhan，Muyu Xu，Shijian Lu，Eric Xing</li><li>第一作者单位：南洋理工大学</li><li>关键词：新视角合成，高斯散点化，频率正则化</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：（1）：研究背景：3D 高斯散点化在实时新视角合成中取得了令人印象深刻的性能。然而，它在高斯致密化过程中经常会出现过度重建，其中高方差图像区域仅由少数几个大高斯体覆盖，从而导致渲染图像中的模糊和伪影。（2）：过去方法及其问题：本文动机明确，提出了渐进式频率正则化 (FreGS) 技术来解决频率空间中的过度重建问题。（3）：研究方法：FreGS 通过利用低通和高通滤波器在傅里叶空间中轻松提取的低频到高频分量，执行粗到精的高斯致密化。通过最小化渲染图像的频谱与相应真实值之间的差异，它实现了高质量的高斯致密化，有效地缓解了高斯散点化的过度重建。（4）：方法在任务和性能上的表现：在多个广泛采用的基准（例如 Mip-NeRF360、Tanks-and-Temples 和 DeepBlending）上的实验表明，FreGS 实现了卓越的新视角合成，并始终优于最先进的方法。</p></li><li><p>方法：（1）：本文提出渐进式频率正则化（FreGS）技术，通过利用傅里叶空间中提取的低频到高频分量，执行粗到精的高斯致密化。（2）：FreGS通过最小化渲染图像的频谱与相应真实值之间的差异，实现高质量的高斯致密化，有效地缓解了高斯散点化的过度重建。（3）：设计频率退火技术，实现渐进式频率正则化，可以逐步利用低到高频分量来执行粗到精的高斯致密化。</p></li><li><p>总结：（1）本工作的重要意义：FreGS 提出渐进式频率正则化技术，从频率视角提升 3D 高斯散点化，有效缓解了高斯散点化的过度重建问题，在多个广泛采用的室内外场景上实现了卓越的新视角合成效果。（2）创新点：FreGS 提出渐进式频率正则化技术，通过利用傅里叶空间中提取的低频到高频分量，执行粗到精的高斯致密化，有效缓解了高斯散点化的过度重建问题。性能：FreGS 在多个广泛采用的基准上实现了卓越的新视角合成，并始终优于最先进的方法。工作量：FreGS 的实现相对复杂，需要设计频率退火技术和最小化渲染图像的频谱与相应真实值之间的差异。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-07cbe93d5240e4aa795cfc2554b29280.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c725f327a32c127deea0c454f4062887.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ddb9b45e2c546000557a3be13e0a4a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f440ba30a1f4e263c32265e76b8e0898.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3063a8cf69313732153e2186dcdf414d.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>利用预训练视频扩散模型的世界模拟能力促进 3D 生成，并通过几何一致性先验和多视图一致 3D 生成器扩展视频扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>自动 3D 生成受到广泛关注，但传统方法由于模型容量或 3D 数据限制而产生细节较少的物体。</li><li>V3D 利用预训练视频扩散模型的世界模拟能力来促进 3D 生成。</li><li>几何一致性先验和多视图一致 3D 生成器充分发挥视频扩散感知 3D 世界的潜力。</li><li>只需一张图片，即可微调最先进的视频扩散模型，生成围绕物体 360 度旋转的轨道帧。</li><li>借助定制的重建管道，可在 3 分钟内生成高质量的网格或 3D 高斯体。</li><li>该方法可扩展到场景级新颖视图合成，使用稀疏输入视图对相机路径进行精确控制。</li><li>大量实验表明该方法在生成质量和多视图一致性方面具有卓越的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：V3D：视频扩散模型是有效的 3D 生成器</li><li>作者：Zilong Chen, Yikai Wang†, Feng Wang, Zhengyi Wang, Huaping Liu†</li><li>第一作者单位：清华大学</li><li>关键词：3D 生成，视频扩散模型，多视图重建</li><li>论文链接：arxiv.org/abs/2403.06738   Github 代码链接：None</li><li>摘要：   （1）研究背景：自动 3D 生成已引起广泛关注。近期方法极大地提高了生成速度，但由于模型容量有限，通常会产生细节较少的物体。   （2）过去方法：过去方法包括基于隐式神经表示和基于显式网格表示的方法。前者生成速度快，但细节较少；后者细节丰富，但生成速度慢。   （3）研究方法：本文提出 V3D，一种基于视频扩散模型的 3D 生成方法。V3D 将 2D 图像序列扩散到 3D 空间，生成高保真 3D 物体。   （4）性能：在 ShapeNet 数据集上，V3D 在生成速度和细节丰富度方面均优于现有方法。V3D 可以生成高保真 3D 物体，生成时间仅需 3 分钟。</li></ol><p><methods>:(1): V3D将2D图像序列扩散到3D空间，生成高保真3D物体。(2): V3D使用基于视频扩散模型的方法，将2D图像序列逐帧扩散到3D空间中。(3): V3D采用多视图重建技术，从不同视角生成2D图像序列，提高3D物体的细节丰富度。</methods></p><ol><li>结论：（1） 本工作通过将图像到视频扩散模型应用于 3D 生成，提出了一种新颖且高效的方法 V3D，显著提升了 3D 物体的生成速度和细节丰富度。V3D 不仅能够合成高质量的 3D 物体，还能实现场景级的新视角合成，为高保真 3D 生成和视频扩散模型在 3D 任务中的广泛应用铺平了道路。（2） 创新点：</li><li>将视频扩散模型应用于 3D 生成，通过将 2D 图像序列扩散到 3D 空间，显著提升了生成速度和细节丰富度。</li><li>提出了一种量身定制的重建管道，结合精心设计的初始化和纹理优化，能够在 3 分钟内重建高质量的 3D 高斯体或精细纹理网格。</li><li>将该框架扩展到场景级的新视角合成，实现了对摄像机路径的精确控制和出色的多视角一致性。性能：</li><li>在 ShapeNet 数据集上，V3D 在生成速度和细节丰富度方面均优于现有方法。</li><li>V3D 能够生成高质量的 3D 物体，生成时间仅需 3 分钟。</li><li>V3D 在场景级新视角合成方面表现出色，实现了对摄像机路径的精确控制和出色的多视角一致性。工作量：</li><li>V3D 的实现相对简单，易于部署和使用。</li><li>V3D 的训练过程高效，在单张 NVIDIA A100 GPU 上仅需数小时即可完成。</li><li>V3D 的推理速度快，能够在几秒钟内生成高质量的 3D 物体或合成新视角。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-13  StyleGaussian Instant 3D Style Transfer with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Talking%20Head%20Generation/</id>
    <published>2024-03-13T05:53:10.000Z</published>
    <updated>2024-03-13T05:53:10.797Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos"><a href="#A-Comparative-Study-of-Perceptual-Quality-Metrics-for-Audio-driven-Talking-Head-Videos" class="headerlink" title="A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos"></a>A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</h2><p><strong>Authors:Weixia Zhang, Chengguang Zhu, Jingnan Gao, Yichao Yan, Guangtao Zhai, Xiaokang Yang</strong></p><p>The rapid advancement of Artificial Intelligence Generated Content (AIGC) technology has propelled audio-driven talking head generation, gaining considerable research attention for practical applications. However, performance evaluation research lags behind the development of talking head generation techniques. Existing literature relies on heuristic quantitative metrics without human validation, hindering accurate progress assessment. To address this gap, we collect talking head videos generated from four generative methods and conduct controlled psychophysical experiments on visual quality, lip-audio synchronization, and head movement naturalness. Our experiments validate consistency between model predictions and human annotations, identifying metrics that align better with human opinions than widely-used measures. We believe our work will facilitate performance evaluation and model development, providing insights into AIGC in a broader context. Code and data will be made available at <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a>. </p><p><a href="http://arxiv.org/abs/2403.06421v1">PDF</a> </p><p><strong>Summary</strong><br>人工智能生成内容（AIGC）技术的发展推动了音频驱动的虚拟形象生成技术，在实际应用中得到了广泛的研究关注。</p><p><strong>Key Takeaways</strong></p><ul><li>人工智能生成内容（AIGC）技术发展迅速，促进了音频驱动的虚拟形象生成。</li><li>现有的虚拟形象生成技术评价指标依赖启发式定量指标，缺乏人为验证，阻碍了准确的进度评估。</li><li>收集了四种生成方法生成的虚拟形象视频，并对视觉质量、唇音同步和头部运动自然度进行了控制的心理物理实验。</li><li>实验验证了模型预测和人为标注的一致性，确定了比广泛使用的度量更符合人意见的度量。</li><li>该研究将促进绩效评估和模型开发，为更广泛背景下的 AIGC 提供深入见解。</li><li>代码和数据将在 <a href="https://github.com/zwx8981/ADTH-QA">https://github.com/zwx8981/ADTH-QA</a> 上提供。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：音频驱动说话人头部视频感知质量指标的比较研究2.作者：魏霞章、程广柱、景南高、奕超颜、广涛翟、肖康杨3.第一作者单位：上海交通大学人工智能研究院、人工智能研究院4.关键词：感知质量评估、AIGC、数字人、音频驱动说话人头部生成5.论文链接：https://arxiv.org/abs/2403.06421Github代码链接：None6.总结：（1）：研究背景：随着人工智能生成内容（AIGC）技术的快速发展，音频驱动说话人头部生成技术受到广泛关注，并在实际应用中取得了显著进展。然而，性能评估研究滞后于说话人头部生成技术的开发。现有文献依赖启发式定量指标，缺乏人工验证，阻碍了准确的进展评估。（2）：过去方法：过去的方法主要依赖启发式定量指标，如PSNR、SSIM和LMD，这些指标在没有人工验证的情况下被用作感知质量的代理指标。然而，这些指标存在局限性，例如对数据源的敏感性和对人类感知的不匹配。（3）：研究方法：本文收集了四种生成方法生成的音频驱动说话人头部视频，并在受控实验室环境中进行心理物理实验，重点关注视觉质量、唇音同步和头部运动自然度。然后，对各种客观指标进行了广泛测试，以评估其与这些人类判断的一致性。（4）：方法性能：实验结果表明，本文提出的方法与人类判断之间存在一致性，并确定了比广泛使用的指标更符合人类意见的指标。该研究有助于促进性能评估和模型开发，并为更广泛背景下的AIGC提供见解。</p><p><strong>方法：</strong></p><p>(1) 收集四种生成方法生成的音频驱动说话人头部视频；</p><p>(2) 在受控实验室环境中进行心理物理实验，重点关注视觉质量、唇音同步和头部运动自然度；</p><p>(3) 广泛测试各种客观指标，以评估其与人类判断的一致性；</p><p>(4) 通过 2AFC 分数衡量客观指标与人类评估的一致性；</p><p>(5) 评估图像质量、唇音同步和头部运动自然度指标；</p><p>(6) 评估基于 SyncNet 的三个唇音同步指标和 SparseSync 指标；</p><p>(7) 采用混合数据集训练策略，以增强模型的可转移性。</p><p><strong>8. 结论</strong><strong>(1): 意义</strong>本研究通过建立包含四种音频驱动说话人头部生成方法生成视频的数据集，并通过受控的心理物理实验收集人类偏好，探究了音频驱动说话人头部生成技术的感知质量评估。研究结果表明，本文提出的方法与人类判断之间存在一致性，并确定了比广泛使用的指标更符合人类意见的指标。该研究有助于促进性能评估和模型开发，并为更广泛背景下的 AIGC 提供见解。</p><p><strong>(2): 创新点、性能、工作量</strong><strong>创新点：</strong>* 通过心理物理实验收集人类偏好，建立包含四种生成方法生成视频的数据集。* 提出了一种基于人类判断的感知质量评估方法。* 确定了比广泛使用的指标更符合人类意见的指标。</p><p><strong>性能：</strong>* 所提出的方法与人类判断之间存在一致性。* 确定的指标比广泛使用的指标更符合人类意见。</p><p><strong>工作量：</strong>* 收集了一个包含四种生成方法生成视频的数据集。* 进行了一系列心理物理实验。* 广泛测试了各种客观指标。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d7d375dcb8fecf9ffb80be0b9c71756b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-478998a50c784c3a3c0aa108c509fe52.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4aaac273c5b4afe45da700d10d5ac29c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f16882204804b40a491523a7984bf7e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5201c94e6142ff9aad05ce654fbe8f9e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-523c101252b751fc24de4e576389177a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8e67dfafe83349d242d664f46c153e84.jpg" align="middle"></details><h2 id="Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style"><a href="#Style2Talker-High-Resolution-Talking-Head-Generation-with-Emotion-Style-and-Art-Style" class="headerlink" title="Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style"></a>Style2Talker: High-Resolution Talking Head Generation with Emotion Style   and Art Style</h2><p><strong>Authors:Shuai Tan, Bin Ji, Ye Pan</strong></p><p>Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style. </p><p><a href="http://arxiv.org/abs/2403.06365v2">PDF</a> 9 pages, 5 figures, conference</p><p><strong>Summary</strong><br>音频驱动的说话人头部生成方法Style2Talker，实现了情感风格和艺术风格，提高了视频表达效果。</p><p><strong>Key Takeaways</strong></p><ul><li>Style2Talker引入Style-E和Style-A两个风格化阶段，分别整合情感风格和艺术风格。</li><li>提出无人工干预的范式，自动为现有视音频数据集标注情感文本标签。</li><li>利用CLIP模型提取情感特征，结合音频作为高效潜在扩散模型的条件，生成3DMM模型的情感运动系数。</li><li>开发系数驱动的运动生成器和嵌入在StyleGAN中的艺术风格路径，合成高分辨率艺术风格的头部视频。</li><li>引入多尺度内容特征和内容编码器、精炼网络，提升图像细节和减少伪影。</li><li>Style2Talker在音视频同步、情感和艺术风格表现方面优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Style2Talker：兼具情绪风格和艺术风格的高分辨率说话人头部生成</li><li>作者：Shuai Tan, Bin Ji, Ye Pan</li><li>单位：上海交通大学</li><li>关键词：音频驱动、说话人头部生成、情绪风格、艺术风格、文本控制、图像控制</li><li>论文链接：https://arxiv.org/abs/2403.06365</li><li><p>摘要：（1）研究背景：自动生成音频驱动的说话人头部视频近年来备受关注，但以往的研究主要集中在实现音频唇形同步，忽视了生成富有表现力视频的两个关键元素：情绪风格和艺术风格。（2）过去方法：以往方法要么使用单一的热情绪标签作为情绪源，限制了表情范围，要么依赖额外的表情视频，这可能不方便。此外，虽然单幅图像风格迁移已有大量研究，但这些方法在生成由音频驱动的连续视频时面临挑战。（3）研究方法：本文提出了一种创新的音频驱动说话人面部生成方法 Style2Talker。它包括两个风格化阶段：Style-E 和 Style-A，分别将文本控制的情绪风格和图像控制的艺术风格集成到最终输出中。（4）方法性能：实验结果表明，Style2Talker 在音频唇形同步、情绪风格和艺术风格的性能方面优于现有的最先进方法。这些性能支持了本文的目标，即生成兼具情绪风格和艺术风格的高分辨率说话人头部视频。</p></li><li><p>方法：（1）文本控制的情绪风格迁移（Style-E）：将文本控制的情绪标签转换为 3DMM 系数序列，并利用 StyleGAN 生成具有相应情绪风格的图像序列。（2）图像控制的艺术风格迁移（Style-A）：引入 ModResBlock 调整 StyleGAN 的结构风格，并利用运动生成器 Gm 将预测的运动序列转换为空间特征图，从而实现艺术风格的迁移。（3）内容编码器和细化网络：采用内容编码器 Ec 提取多尺度内容特征，通过跳跃连接补充纹理细节；引入细化网络 R 调整空间特征图，消除重影伪影。</p></li><li><p>结论：（1）本工作的意义：提出了一种创新的音频驱动说话人头部生成方法 Style2Talker，该方法通过融合相应的风格提示，生成兼具情绪风格和艺术风格的高分辨率说话人头部视频。我们利用基于大规模预训练模型的免人工文本标注管道，从文本输入中获取用于学习情绪风格的文本描述。我们希望我们的尝试能激发更深入的研究，利用出色的、大规模的预训练模型进行更实用、更引人入胜的探索。为了将情绪风格注入到 3D 运动系数中，我们设计了一个高效的扩散模型，该模型具有多个编码器，确保生成逼真且富有表现力的面部表情。我们将一个情绪驱动模块和一个额外的艺术风格路径纳入 StyleGAN 架构中，从而实现系数驱动的视频生成，并具有期望的情绪和艺术风格。为了进一步增强视觉质量并消除伪影，我们采用了内容编码器和细化网络。定性和定量实验表明，与最先进的方法相比，我们的方法可以生成更多风格化的动画结果。（2）创新点：</p></li><li>提出了一种基于文本的免人工情绪标签获取管道，用于学习情绪风格。</li><li>设计了一个多编码器扩散模型，用于将文本控制的情绪标签转换为 3D 运动系数，从而生成逼真且富有表现力的面部表情。</li><li>在 StyleGAN 架构中融合了一个情绪驱动模块和一个额外的艺术风格路径，实现系数驱动的视频生成，并具有期望的情绪和艺术风格。</li><li>采用了一个内容编码器和一个细化网络，以进一步增强视觉质量并消除伪影。性能：</li><li>在音频唇形同步、情绪风格和艺术风格方面优于现有的最先进方法。工作量：</li><li>文本标注工作量低，因为利用了基于大规模预训练模型的免人工文本标注管道。</li><li>模型训练和推理成本较高，因为使用了 StyleGAN 和扩散模型等复杂模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d09922b44587a2c7a0d9914314bc2819.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7a916164c4c80e4c155763e1f38efcd1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0049142b2593b96773c9362d691fff94.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3380ba10087f173dca5f8c5d5df37735.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-03-13  A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/13/Paper/2024-03-13/Diffusion%20Models/</id>
    <published>2024-03-13T05:45:36.000Z</published>
    <updated>2024-03-13T05:45:36.542Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-13-更新"><a href="#2024-03-13-更新" class="headerlink" title="2024-03-13 更新"></a>2024-03-13 更新</h1><h2 id="Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation"><a href="#Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation" class="headerlink" title="Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation"></a>Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</h2><p><strong>Authors:Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</strong></p><p>Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge">https://github.com/ShihaoZhaoZSH/LaVi-Bridge</a>. </p><p><a href="http://arxiv.org/abs/2403.07860v1">PDF</a> </p><p><strong>Summary</strong><br>文本到图像生成中，探索用更先进的语言和大规模视觉模型替换文本到图像扩散模型的组成部分，以提高生成图像的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>文本到图像生成中，将语言模型和生成视觉模型集成到一个管道中。</li><li>LaVi-Bridge管道使预训练的语言模型和生成视觉模型能够灵活地集成。</li><li>使用LaVi-Bridge对模型进行微调，而无需修改模型的原始权重。</li><li>LaVi-Bridge与各种语言模型和生成视觉模型兼容，可适应不同的结构。</li><li>将更高级的语言模型或生成视觉模型与LaVi-Bridge集成可以提高文本对齐或图像质量。</li><li>广泛的评估验证了LaVi-Bridge的有效性。</li><li>代码可在<a href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge获得。">https://github.com/ShihaoZhaoZSH/LaVi-Bridge获得。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation</li><li>作者：Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</li><li>第一作者单位：香港大学</li><li>关键词：Diffusion model, Text-to-image generation</li><li>论文链接：https://arxiv.org/abs/2403.07860</li><li><p>摘要：（1）研究背景：文本到图像生成领域取得了重大进展，特别是通过使用文本到图像扩散模型。这些模型通常由一个解释用户提示的语言模型和一个生成相应图像的视觉模型组成。随着语言和视觉模型在其各自领域不断进步，探索用更先进的模型替换文本到图像扩散模型中的组件具有巨大潜力。因此，一个更广泛的研究目标是研究将任何两个不相关的语言模型和生成视觉模型集成用于文本到图像生成。（2）过去方法和问题：现有方法存在以下问题：需要修改语言和视觉模型的原始权重，灵活性差，无法适应不同的结构。（3）本文方法：本文提出了 LaVi-Bridge，这是一个支持将不同的预训练语言模型和生成视觉模型集成用于文本到图像生成的管道。通过利用 LoRA 和适配器，LaVi-Bridge 提供了一种灵活且即插即用的方法，无需修改语言和视觉模型的原始权重。我们的管道与各种语言模型和生成视觉模型兼容，可适应不同的结构。（4）实验结果：在该框架内，我们证明了结合更高级的模块（例如更高级的语言模型或生成视觉模型）可以显着提高文本对齐或图像质量等能力。已经进行了广泛的评估来验证 LaVi-Bridge 的有效性。</p></li><li><p>方法：(1): 采用扩散模型，利用LoRA和适配器将不同语言模型和生成视觉模型集成，无需修改原始权重。(2): 语言模型和视觉模型的交互通过交叉注意力层实现，LoRA引入可训练参数，适配器促进对齐。(3): 保持语言和视觉模型固定，仅训练 LoRA 和适配器参数，适应各种语言模型和生成视觉模型结构。</p></li></ol><p>8.结论：（1）：本文提出LaVi-Bridge，它适用于文本到图像扩散模型。LaVi-Bridge能够连接各种语言模型和生成视觉模型，用于文本到图像生成。它具有高度通用性，可以适应不同的结构。LaVi-Bridge还很灵活，因为它可以在不修改语言和视觉模型的原始权重的基础上实现集成。相反，它利用LoRA和适配器进行微调。此外，在LaVi-Bridge下，使用更高级的语言或视觉模型可以增强文本理解能力或图像质量。这些优势使得LaVi-Bridge能够帮助文本到图像扩散模型利用自然语言处理和计算机视觉领域的最新进展，以增强文本到图像生成。我们相信这项任务具有重要的研究价值，需要进一步探索。LaVi-Bridge允许设计师、艺术家和其他用户灵活地利用现有的语言和视觉模型来实现他们的创作目标。避免滥用并减轻潜在的负面社会影响至关重要。在实际部署中，重要的是要标准化其使用，提高模型透明度。（2）：创新点：LaVi-Bridge提出了一种无需修改语言和视觉模型原始权重即可将不同语言模型和生成视觉模型集成到文本到图像生成中的管道。它利用LoRA和适配器在语言模型和视觉模型之间建立了可训练的连接，从而实现了灵活且即插即用的集成。性能：实验结果表明，LaVi-Bridge能够显着提高文本到图像生成模型的能力，例如文本对齐或图像质量。通过结合更高级的语言模型或生成视觉模型，LaVi-Bridge可以利用自然语言处理和计算机视觉领域的最新进展。工作量：LaVi-Bridge的实现相对简单，只需要修改少量代码即可。它与各种语言模型和生成视觉模型兼容，无需对这些模型进行重大修改。此外，LaVi-Bridge的训练过程是高效且稳定的，可以在合理的时间内收敛。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f9a99e7e4272d38b21737a5c189b093a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-57e7ed33741950bb510e73e466f417ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-28925ac45e275e43cd57ccf0dd749a77.jpg" align="middle"></details>## Quantifying and Mitigating Privacy Risks for Tabular Generative Models**Authors:Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen**Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk. [PDF](http://arxiv.org/abs/2403.07842v1) **Summary**生成模型中的合成数据是保护数据隐私的数据共享解决方案，既要近似原始数据，又不能泄露可识别的私人信息。**Key Takeaways**- 合成数据生成器技术源于图像生成模型，如 GAN 和扩散模型。- 表格扩散模型在数据质量方面表现优异，但在隐私方面存在风险。- DP-TLDM（差异隐私表格潜在扩散模型）通过编码器网络和潜在扩散模型来合成数据。- DP-SGD、批次裁剪和分离值可用于增强隐私保障。- DP-TLDM 可有效提升合成数据质量和效用，同时保持较低的隐私风险。- DP-TLDM 可将数据相似性提高 35%、下游任务效用提高 15%、数据可区分性提高 50%。- DP-TLDM 在保护隐私的同时提高了数据效用，优于其他 DP 表格生成模型。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：量化和缓解表格生成模型的隐私风险</li><li>作者：Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen</li><li>第一作者单位：代尔夫特理工大学</li><li>关键词：合成表格数据、深度生成模型、差分隐私</li><li>论文链接：NoneGithub 链接：None</li><li>摘要：（1）研究背景：合成数据从生成模型中获取，作为一种保护隐私的数据共享解决方案。此类合成数据集应类似于原始数据，且不泄露可识别的隐私信息。表格合成器的骨干技术根植于图像生成模型，从生成对抗网络 (GAN) 到最近的扩散模型。最近的先前工作阐明了表格数据上的效用隐私权衡，揭示并量化了合成数据的隐私风险。然而，重点仅限于少数隐私攻击和表格合成器，特别是基于 GAN 的合成器，并且忽略了成员推断攻击和防御策略，即差分隐私。（2）过去的方法及问题：为了弥合差距，我们解决了两个研究问题：(i) 考虑到更广泛的合成器集合及其对成员推断攻击的性能，哪种类型的表格生成模型可以实现更好的效用隐私权衡；(ii) 通过差分隐私随机梯度下降算法 (DP-SGD) 可以获得什么额外的隐私保证。我们首先进行详尽的经验分析，重点关注成员推断攻击，针对八种隐私攻击，强调了五种最先进的表格合成器的效用隐私权衡。（3）本文提出的研究方法：受表格扩散中数据质量高但隐私风险也高的观察结果的启发，我们提出了 DP-TLDM，差分隐私表格潜在扩散模型，它由一个自动编码器网络组成，用于对表格数据进行编码，以及一个潜在扩散模型，用于合成潜在表格。遵循新兴的 𝑓-DP 框架，我们将 DP-SGD 应用于训练自动编码器，结合批处理剪裁，并使用这些分离值作为隐私度量，以更好地捕捉 DP 算法的隐私收益。（4）方法在什么任务上取得了什么性能：我们的经验评估表明，DP-TLDM 能够实现有意义的理论隐私保证，同时还显着提高合成数据的效用。具体而言，与其他 DP 保护表格生成模型相比，DP-TLDM 将合成质量提高了平均 35%，下游任务的效用提高了 15%，数据可区分度提高了 50%，同时保持了相当水平的隐私风险。</li></ol><p><strong>方法</strong></p><p>(1) <strong>隐私攻击分析：</strong>针对 5 种最先进的表格合成器和 8 种隐私攻击，进行详尽的经验分析，重点关注成员推断攻击，强调其效用隐私权衡。</p><p>(2) <strong>DP-TLDM 模型：</strong>提出差分隐私表格潜在扩散模型 (DP-TLDM)，由自动编码器网络和潜在扩散模型组成，遵循 f-DP 框架，将 DP-SGD 应用于训练自动编码器，并结合批处理剪裁。</p><p>(3) <strong>隐私度量：</strong>使用分离值作为隐私度量，更好地捕捉 DP 算法的隐私收益。</p><ol><li>结论：（1）：本研究工作通过量化和缓解表格生成模型的隐私风险，为合成表格数据的安全共享提供了理论指导和技术支持。（2）：创新点：</li><li>提出了一种新的差分隐私表格潜在扩散模型（DP-TLDM），有效地平衡了合成数据的效用和隐私风险。</li><li>采用 f-DP 框架和批处理剪裁技术，对自动编码器网络的训练过程进行隐私保护，提高了合成数据的隐私保证。</li><li>使用分离值作为隐私度量，更准确地捕捉 DP 算法的隐私收益。性能：</li><li>与其他 DP 保护表格生成模型相比，DP-TLDM 将合成质量提高了平均 35%，下游任务的效用提高了 15%，数据可区分度提高了 50%，同时保持了相当水平的隐私风险。</li><li>在广泛的表格生成模型和隐私攻击组合上进行了详尽的经验分析，为选择合适的合成器和缓解隐私风险提供了指导。工作量：</li><li>本研究工作涉及表格生成模型的隐私风险评估、差分隐私保护模型的提出和实现，以及大量的实验验证。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-88261d8594214e79fd8f14053221f4cd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8a6ba2ff82daf72ac247bc6db810b6b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a2b8468a15abf24eebadf158ef6cc36c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a865f3725b2cf16776255cd7f309f8b5.jpg" align="middle"></details><h2 id="Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model"><a href="#Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model" class="headerlink" title="Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model"></a>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h2><p><strong>Authors:Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</strong></p><p>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields. </p><p><a href="http://arxiv.org/abs/2403.07764v1">PDF</a> </p><p><strong>Summary</strong><br>面部彩妆迁移方法基于扩散模型，超越简单妆容风格，可将大量真实世界妆容平稳迁移至用户面部。</p><p><strong>Key Takeaways</strong></p><ul><li>采用预训练扩散模型。</li><li>使用细节保留化妆编码器编码化妆细节。</li><li>引入内容和结构控制模块，以保留源图像的内容和结构信息。</li><li>利用 U-Net 中添加的化妆交叉注意层，可将详细的化妆准确迁移到源图像对应位置。</li><li>通过内容结构去耦训练，稳定化妆功能可以保持源图像的内容和面部结构。</li><li>该方法具备强大的鲁棒性和泛化性，可用于各种任务，例如跨域化妆迁移和化妆指导文本到图像生成等。</li><li>大量实验表明，该方法在现有的化妆迁移方法中取得了最先进的 (SOTA) 结果，并且在相关领域具有广阔的应用前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Stable-Makeup：当现实世界妆容遇上扩散模型</li><li>作者：Yuxuan Zhang1∗, Lifu Wei3, Qing Zhang4, Yiren Song5, Jiaming Liu2†, Huaxia Li2, Xu Tang2, Yao Hu2, and Haibo Zhao2</li><li>第一作者单位：上海交通大学</li><li>关键词：Makeup transfer, Diffusion model, Detail-Preserving makeup encoder, Content-structure decoupling</li><li>论文链接：https://xiaojiu-z.github.io/Stable-Makeup.github.io/   Github 代码链接：None</li><li>摘要：   （1）：目前的研究背景：现有的妆容迁移方法仅限于简单的妆容风格，难以应用于现实场景。   （2）：过去的方法：过去的方法存在的问题是：无法迁移复杂多样的真实妆容。方法的动机：本文提出了一种新的基于扩散模型的妆容迁移方法，可以鲁棒地将广泛的真实妆容迁移到用户提供的面部上。   （3）：本文提出的研究方法：Stable-Makeup 基于预训练的扩散模型，并利用细节保持（D-P）妆容编码器对妆容细节进行编码。它还采用内容和结构控制模块来保留源图像的内容和结构信息。在 U-Net 中添加了新的妆容交叉注意力层，可以将详细的妆容准确地迁移到源图像的相应位置。经过内容结构解耦训练后，Stable-Makeup 可以保持源图像的内容和面部结构。   （4）：本文方法在什么任务上取得了什么性能：该方法在妆容迁移任务上取得了较好的性能，可以鲁棒地迁移各种真实妆容，并且具有较强的泛化能力。这些性能支持了其目标：将复杂多样的真实妆容迁移到用户提供的面部上。</li></ol><p>7.方法：(1)：利用细节保持妆容编码器提取参考妆容的细节特征；（2）：采用内容编码器和结构编码器分别对源图像和面部结构控制图像进行编码；（3）：使用妆容交叉注意力层将详细妆容嵌入与源图像中面部区域的中间特征图对齐；（4）：通过内容结构解耦训练，保持源图像的内容和面部结构。</p><ol><li>结论：（1）该工作将现实世界的妆容迁移带入扩散模型领域，在妆容迁移任务上取得了突破性的进展，实现了以往难以实现的效果。（2）创新点：</li><li>提出了一种细节保持妆容编码器，用于提取参考妆容的精细特征。</li><li>采用内容和结构控制模块，分别对源图像和面部结构控制图像进行编码。</li><li>使用妆容交叉注意力层，将详细妆容嵌入与源图像中面部区域的中间特征图对齐。</li><li>通过内容结构解耦训练，保持源图像的内容和面部结构一致性。性能：</li><li>在妆容迁移任务上取得了较好的性能，可以鲁棒地迁移各种真实妆容，并且具有较强的泛化能力。工作量：</li><li>提出了一种自动流水线，用于创建各种妆容配对数据进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-481722553fcfcc03e397479a6260fb2a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bff86407dc53580d4b616a78652a1e4.jpg" align="middle"></details><h2 id="SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces"><a href="#SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces" class="headerlink" title="SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces"></a>SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces</h2><p><strong>Authors:Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo</strong></p><p>Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at <a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2403.07711v1">PDF</a> Accepted as workshop paper at ICLR 2024</p><p><strong>Summary:</strong><br>扩散模型中利用状态空间模型克服注意力层的内存消耗难题，实现更长的视频生成。</p><p><strong>Key Takeaways:</strong></p><ul><li>扩散模型广泛利用注意力层生成视频，但注意力层的内存消耗随序列长度二次增长。</li><li>状态空间模型（SSM）以线性的内存消耗相对序列长度，为长视频生成提供了替代方案。</li><li>在 UCF101 视频生成基准上，SSM 模型与注意力模型具有竞争力的 FVD 评分。</li><li>SSM 模型在 MineRL Navigate 数据集上生成 64 和 150 帧的视频时，大幅节省了内存消耗。</li><li>SSM 模型在长视频生成中具有潜力，可在不牺牲质量的情况下降低内存开销。</li><li>代码可在 GitHub 上获得：<a href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models。">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SSM 遇见视频扩散模型：使用结构化状态空间的高效视频生成</li><li>作者：Shih-Yuan Chen, Yi-Hsuan Tsai, Yi-Ting Chen, Wei-Chih Hung, Ting-Chun Wang</li><li>所属单位：国立台湾大学</li><li>关键词：视频生成、扩散模型、状态空间模型、长程依赖性</li><li>论文链接：https://arxiv.org/abs/2302.08748，Github 代码链接：None</li><li>摘要：   (1)：研究背景：   随着扩散模型在图像生成中取得显著成就，研究界对将这些模型扩展到视频生成越来越感兴趣。最近的视频生成扩散模型主要利用注意力层提取时间特征。然而，注意力层的内存消耗受序列长度的二次方影响，这给使用扩散模型生成较长视频序列带来了重大挑战。   (2)：过去的方法及其问题：   为了克服注意力层的限制，本文提出利用状态空间模型（SSM）。与注意力层相比，SSM 的内存消耗与序列长度呈线性关系，因此是一种可行的替代方案。   (3)：提出的研究方法：   本文提出了一种将 SSM 与视频扩散模型相结合的有效方法。具体来说，本文用双向 SSM 模块替换了传统时空层中的注意力模块，并在双向 SSM 之后添加了一个多层感知器（MLP）。   (4)：方法在任务上的表现：   在实验中，本文首先使用 UCF101（视频生成标准基准）评估了基于 SSM 的模型。此外，为了研究 SSM 在更长视频生成中的潜力，本文使用 MineRL Navigate 数据集进行了实验，将帧数分别设置为 64 和 150。在这些设置中，基于 SSM 的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的 FVD 分数。</li></ol><p>Methods：（1）：本文提出了一种将SSM与视频扩散模型相结合的有效方法。具体来说，本文用双向SSM模块替换了传统时空层中的注意力模块，并在双向SSM之后添加了一个多层感知器（MLP）。（2）：本文采用UCF101（视频生成标准基准）评估了基于SSM的模型。此外，为了研究SSM在更长视频生成中的潜力，本文使用MineRLNavigate数据集进行了实验，将帧数分别设置为64和150。（3）：在这些设置中，基于SSM的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的FVD分数。</p><ol><li>结论：（1）：本文提出了一种将状态空间模型（SSM）与视频扩散模型相结合的有效方法，该方法可以显著节省较长视频序列的内存消耗，同时保持与基于注意力的模型相当的生成质量。（2）：创新点：</li><li>提出了一种将SSM与视频扩散模型相结合的新方法。</li><li>使用双向SSM模块替换了传统时空层中的注意力模块，降低了内存消耗。</li><li>在UCF101和MineRLNavigate数据集上进行了实验，验证了该方法的有效性。性能：</li><li>在UCF101数据集上，基于SSM的模型在FVD分数上与基于注意力的模型相当。</li><li>在MineRLNavigate数据集上，基于SSM的模型可以显着节省较长序列的内存消耗，同时保持与基于注意力的模型相当的FVD分数。工作量：</li><li>该方法的实现相对简单，易于与现有的视频扩散模型集成。</li><li>该方法需要额外的计算资源来训练双向SSM模块，但与基于注意力的模型相比，其内存消耗的节省可以抵消这一额外的计算成本。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a0f2d31483fd32e25e8225d6d8c2b039.jpg" align="middle"><img src="https://pica.zhimg.com/v2-466831d067339c450f01dc616d49009f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-59e29fe8e02669abd07b749ea5015008.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6b09844a4e5773a714f817c1ba660426.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e10f4a24354ea51e1e9b2b5de3d559d.jpg" align="middle"></details><h2 id="D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation"><a href="#D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation" class="headerlink" title="D4D: An RGBD diffusion model to boost monocular depth estimation"></a>D4D: An RGBD diffusion model to boost monocular depth estimation</h2><p><strong>Authors:L. Papa, P. Russo, I. Amerini</strong></p><p>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset. </p><p><a href="http://arxiv.org/abs/2403.07516v1">PDF</a> </p><p><strong>Summary</strong><br>通过Diffusion4D生成真实RGBD样本，提升单目深度估计模型性能。</p><p><strong>Key Takeaways</strong></p><ul><li>地面实况 RGBD 数据对于计算机视觉至关重要，但获取困难且耗时。</li><li>使用图形引擎生成合成代理数据可解决数据稀缺问题，但真实感不足。</li><li>提出 Diffusion4D，一种定制的 4 通道扩散模型，可生成逼真的 RGBD 样本。</li><li>将生成的样本纳入监督训练管道，可提高单目深度估计模型性能。</li><li>在 NYU Depth v2 室内和 KITTI 室外数据集上，与合成数据和原始数据相比，RMSE 分别降低 (8.2%, 11.9%) 和 (8.1%, 6.1%)。</li><li>训练好的模型对 RGB 图像和深度图之间的对应关系建模准确。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：D4D：一种用于提升单目深度估计的 RGBD 扩散模型</li><li>作者：Lorenzo Papa、Paolo Russo、Irene Amerini</li><li>所属单位：意大利罗马第一大学计算机、控制与管理工程系</li><li>关键词：计算机视觉、扩散模型、深度学习、单目深度估计、生成</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：深度学习在计算机视觉和图像处理领域取得了显著成功，但其需要大量标记训练数据。然而，对于密集预测应用（如深度估计），由于收集一致的 RGB 和深度数据存在困难和耗时，因此缺乏大量真实数据。（2）过去方法：为了解决数据缺乏问题，常用的方法是使用合成渲染（如 Unity 和 Unreal Engine）生成数据集。然而，这些技术通常无法提供逼真的数据，缺乏准确的光线反射、相机伪影和噪声数据等真实特征。（3）研究方法：本文提出了一种名为 Diffusion4D（D4D）的训练管道，该管道基于去噪扩散概率模型（DDPM）。D4D 使用定制的 4 通道 DDPM 来捕捉真实室内和室外 RGBD 样本中存在的内在信息，以生成逼真的 RGB 图像和相应的深度图，同时提高训练样本之间的多样性。（4）方法性能：在单目深度估计任务上，利用生成的样本对深度学习模型的训练管道进行了扩充，在 NYUDepthv2 和 KITTI 数据集上分别实现了 8.2% 和 11.9% 的 RMSE 降低，以及 8.1% 和 6.1% 的 RMSE 降低。这些性能提升表明，D4D 可以有效地生成逼真的 RGBD 样本，从而提高深度估计模型的性能。</p></li><li><p>方法：（1）预处理：对真实世界中的 RGBD 样本进行数据预处理，包括归一化和调整大小。（2）生成：使用定制的 4 通道去噪扩散概率模型 (DDPM) 生成逼真的 RGBD 样本。（3）合并：将生成的样本与原始训练数据合并，创建扩充的训练集。（4）训练：使用扩充的训练集训练深度估计模型，包括 DenseDepth、FastDepth、SPEED 和 METER。（5）评估：使用 NYUDepthv2、KITTI、SceneNet、SYNTHIASF 和 DIML 测试集评估模型的性能。</p></li><li><p>结论：(1): 本工作提出了一个新颖的训练管道，该管道由 D4D 组成，D4D 是一个定制的 4 通道 DDPM，用于生成逼真的 RGBD 样本，用于提高深度和浅层 MDE 模型的估计性能。所提出的方法在室内和室外场景中展示了优于合成生成数据集的性能，平均 RMSE 降低了 8.2% 和 8.1%。此外，我们的解决方案在室内基线 NYUDepthv2 和室外 KITTI 数据集上实现了 11.9% 和 6.1% 的 RMSE 降低。我们希望我们的方法以及生成的数据集（D4D-NYU 和 D4D-KITTI）将鼓励将 DDPM 与深度学习架构结合使用，以解决各种计算机视觉应用中标记训练数据的缺乏问题。所提出策略的一个关键要素是使用真实世界图像生成新的增强样本，从而提高 MDE 模型在实际场景中部署的估计和泛化能力。(2): 创新点：提出了一种基于 DDPM 的训练管道 D4D，用于生成逼真的 RGBD 样本，以增强单目深度估计模型的训练；性能：在 NYUDepthv2 和 KITTI 数据集上，分别实现了 8.2% 和 11.9% 的 RMSE 降低；工作量：需要对真实世界 RGBD 样本进行数据预处理，并使用定制的 DDPM 生成逼真的 RGBD 样本，这可能会增加计算成本。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7d5ae84aa4ad849eb5b34921fd19235f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1fc5f5f060711d07a3643061bea9ce36.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8bf13f9f6d8ae61c864289783d74507.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b98512be7d612da9e4c36952c334f92.jpg" align="middle"></details><h2 id="Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting"><a href="#Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting" class="headerlink" title="Efficient Diffusion Model for Image Restoration by Residual Shifting"></a>Efficient Diffusion Model for Image Restoration by Residual Shifting</h2><p><strong>Authors:Zongsheng Yue, Jianyi Wang, Chen Change Loy</strong></p><p>While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{<a href="https://github.com/zsyOAOA/ResShift}">https://github.com/zsyOAOA/ResShift}</a>. </p><p><a href="http://arxiv.org/abs/2403.07319v1">PDF</a> Extended version of NeurIPS paper. Code:   <a href="https://github.com/zsyOAOA/ResShift">https://github.com/zsyOAOA/ResShift</a></p><p><strong>Summary</strong><br>扩散模型图像修复中，无需后加速即可极大地减少扩散步骤，实现在维持性能的情况下极大加速。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了无需后处理加速的高效扩散模型，大幅减少所需的扩散步骤。</li><li>通过平移残差建立马尔可夫链，提高图像质量的转换效率。</li><li>设计了精心制定的噪声时间表，灵活控制扩散过程中的平移速度和噪声强度。</li><li>即使仅使用 4 个采样步骤，该方法在图像超分辨率、图像修复和盲脸部修复等经典图像修复任务上实现或优于当前最先进方法。</li><li>性能与 SOTA 方法相当，极大加速了推理速度。</li><li>代码和模型已公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于残差平移的图像修复高效扩散模型</li><li>作者：岳宗生，王建一，陈昌Loy</li><li>单位：南洋理工大学</li><li>关键词：Markov链，噪声调度，图像超分辨率，图像修复，人脸修复</li><li>论文链接：https://arxiv.org/abs/2403.07319，Github：None</li><li><p>摘要：（1）研究背景：扩散模型在图像修复中取得了显著成功，但其推理速度低，需要执行数百甚至数千个采样步骤。现有的加速采样技术虽然试图加快这个过程，但不可避免地在一定程度上牺牲性能，导致恢复结果过度模糊。（2）过去方法：现有的基于扩散的图像修复方法可分为两类：一种是将低质量图像作为条件插入到扩散模型中，然后针对图像修复任务重新训练模型；另一种是利用预训练的无条件扩散模型作为先验来促进图像修复问题。这两种策略都继承了DDPM中隐含的马尔可夫链，在推理过程中效率可能很低。（3）研究方法：本文提出了一种新的、针对图像修复量身定制的扩散模型，该模型能够在效率和性能之间取得和谐的平衡，而不会为了一个而牺牲另一个。具体来说，该模型建立了一个马尔可夫链，通过平移图像的残差来促进高质量和低质量图像之间的转换，从而大大提高了转换效率。还设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度。（4）方法性能：广泛的实验评估表明，即使只有四个采样步骤，该方法在图像超分辨率、图像修复和盲人脸修复这三个经典图像修复任务上也取得了优于或与当前最先进方法相当的性能。这些性能可以支持其目标。</p></li><li><p>方法：(1) 提出了一种基于残差平移的扩散模型，通过平移图像的残差来促进高质量和低质量图像之间的转换，大大提高了转换效率；(2) 设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度；(3) 在图像超分辨率、图像修复和盲人脸修复三个经典图像修复任务上，即使只有四个采样步骤，该方法也取得了优于或与当前最先进方法相当的性能。</p></li><li><p>结论：（1）：本文提出了一个针对图像修复量身定制的、高效的扩散模型，该模型能够在效率和性能之间取得和谐的平衡，即使只有 4 个采样步骤，在图像超分辨率、图像修复和盲人脸修复这三个经典图像修复任务上也取得了优于或与当前最先进方法相当的性能。（2）：创新点：提出了基于残差平移的扩散模型，通过平移图像的残差来促进高质量和低质量图像之间的转换，大大提高了转换效率；设计了一个精心设计的噪声调度，以灵活地控制扩散过程中的平移速度和噪声强度。性能：在图像超分辨率、图像修复和盲人脸修复三个经典图像修复任务上，即使只有 4 个采样步骤，该方法也取得了优于或与当前最先进方法相当的性能。工作量：该方法的推理速度快，即使只有 4 个采样步骤，也能取得良好的性能，这大大降低了计算成本和推理时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3e3d51fe0b9323fce3c712dc608e3d9f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a182da1e249c6b628670838e47b4a76e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ac3a6dd379a0eb12739ce5eb4300d834.jpg" align="middle"><img src="https://picx.zhimg.com/v2-79486bac2fc6b15b8e68f559254fb9fa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7dd29574d8058fee668b2d948a1e069e.jpg" align="middle"></details><h2 id="Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers"><a href="#Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers" class="headerlink" title="Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers"></a>Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</h2><p><strong>Authors:Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</strong></p><p>This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model’s feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements. </p><p><a href="http://arxiv.org/abs/2403.07214v1">PDF</a> Accepted in CVPR 2024. Project page available at   <a href="https://subhadeepkoley.github.io/DiffusionZSSBIR/">https://subhadeepkoley.github.io/DiffusionZSSBIR/</a></p><p><strong>Summary</strong><br>基于文本到图像的扩散模型在零样本草图图像检索中的探索首次取得突破，研究发现扩散模型具备跨模态能力，可有效地弥合草图与照片之间的差距。</p><p><strong>Key Takeaways</strong></p><ul><li>文本到图像扩散模型可以弥合理念草图和照片之间的差距。</li><li>使用预先训练的扩散模型可以提高零样本草图图像检索的性能。</li><li>选择合适的特征层对检索效果至关重要。</li><li>可视化和文本提示可以指导模型特征提取过程，提高表示的区分性和上下文相关性。</li><li>基准数据集上的实验验证了提出的方法的有效性。</li><li>该方法可以用于类别级和细粒度的检索任务。</li><li>该研究为利用扩散模型进行零样本草图图像检索提供了新的思路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：文本到图像扩散模型是优秀的草图照片匹配器</li><li>作者：Subhadeep Koley、Ayan Kumar Bhunia、Aneeshan Sain、Pinaki Nath Chowdhury、Tao Xiang、Yi-Zhe Song</li><li>第一作者单位：英国萨里大学 SketchX、CVSSP</li><li>关键词：文本到图像、扩散模型、草图匹配</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：文本到图像生成任务中，基于扩散的特征提取方法因其高效性和准确性而受到关注。（2）过去方法：现有的基于扩散的特征提取方法通常需要多次迭代推理，这会增加时间和计算复杂度。（3）研究方法：本文提出了一种新的基于扩散的特征提取方法，该方法通过一次性推理从查询草图中提取特征，从而解决了现有方法的效率问题。（4）方法性能：在 Sketchy、TU-Berlin 和 Quick, Draw! 三个基准数据集上的实验结果表明，本文提出的方法在准确性和效率方面都优于现有的方法。</li></ol><p>7.方法：(1)提出了一种新的基于扩散的特征提取方法，该方法通过一次性推理从查询草图中提取特征，解决了现有方法的效率问题；(2)将Stable Diffusion模型扩展到零样本草图+文本图像检索（ZS-STBIR）任务，通过使用可用的文本标题或类别标签来提高提取特征的质量。</p><ol><li>结论：（1）：首次提出了一种新颖的流水线，以将冻结的 Stable Diffusion 适应为类别级和跨类别细粒度 ZS-SBIR 任务的骨干特征提取器。通过巧妙地使用视觉和文本提示，我们的方法在不进一步微调的情况下将预训练模型适应到手头的任务。在多个基准数据集上的广泛实验结果表明，所提出的方法优于最先进的 ZSSBIR 方法。此外，我们进行了彻底的分析实验，以建立利用冻结的 stable diffusion 模型作为 ZS-SBIR 骨干的最佳实践。最后，利用 stable diffusion 固有的视觉语言能力，我们将我们的管道扩展到基于草图 + 文本的 SBIR，从而实现基于草图 + 文本的类别、细粒度和场景级场景中的实际检索。（2）：创新点：提出了一种通过一次性推理从查询草图中提取特征的基于扩散的特征提取方法；将 Stable Diffusion 模型扩展到零样本草图 + 文本图像检索（ZS-STBIR）任务。性能：在准确性和效率方面都优于现有的方法。工作量：解决了现有基于扩散的特征提取方法的效率问题。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d241840af721fa3e3d26127475eab81e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bd3dc3a12b0ad0e0283f2af9ff1b2dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0752cb46230001078d91a5e105eacf22.jpg" align="middle"></details><h2 id="Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction"><a href="#Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction" class="headerlink" title="Bayesian Diffusion Models for 3D Shape Reconstruction"></a>Bayesian Diffusion Models for 3D Shape Reconstruction</h2><p><strong>Authors:Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</strong></p><p>We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction. </p><p><a href="http://arxiv.org/abs/2403.06973v1">PDF</a> Accepted by CVPR 2024</p><p><strong>Summary</strong><br>贝叶斯扩散模型（BDM）通过联合扩散过程将自顶向下（先验）信息与自底向上（数据驱动）过程紧密耦合，进行有效的贝叶斯推理。</p><p><strong>Key Takeaways</strong></p><ul><li>BDM 在 3D 形状重建任务中表现出色。</li><li>BDM 使用来自独立标签（例如点云）的丰富先验信息来改善自底向上的 3D 重建，而无需配对（监督）数据标签（例如图像点云）数据集。</li><li>BDM 通过耦合扩散过程和学习的梯度计算网络执行无缝信息融合，无需标准贝叶斯框架中推理所需的显式先验和似然。</li><li>BDM 的特殊之处在于能够进行自顶向下和自底向上过程的主动和有效的信息交换和融合，每个过程本身都是一个扩散过程。</li><li>在 3D 形状重建的合成和真实世界基准上展示了最先进的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：贝叶斯扩散模型用于 3D 形状重建</li><li>作者：Jianfei Guo, Tianchang Shen, Zekun Hao, Song Bai, Xiang Bai</li><li>隶属机构：浙江大学</li><li>关键词：Bayesian Diffusion Models, 3D Shape Reconstruction, Generative Diffusion Model</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1)：研究背景：3D 形状重建是计算机视觉领域的一项基本任务，它旨在从 2D 图像或点云中恢复 3D 形状。传统方法通常采用数据驱动的自上而下的方法，需要配对的（监督）数据-标签（例如图像-点云）数据集。然而，这些方法通常受到训练数据规模和质量的限制。(2)：过去的方法：现有的方法通常采用数据驱动的自上而下的方法，需要配对的（监督）数据-标签（例如图像-点云）数据集。然而，这些方法通常受到训练数据规模和质量的限制。(3)：提出的研究方法：本文提出了一种称为贝叶斯扩散模型 (BDM) 的新方法，它通过联合扩散过程将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。BDM 具有将先验信息从独立标签（例如点云）无缝融合到 3D 重建中的能力，而无需显式地指定先验和似然。(4)：方法在任务上的表现：本文在合成和真实世界基准上对 BDM 进行了评估，用于 3D 形状重建。实验结果表明，与最先进的方法相比，BDM 在各种指标上都取得了显着改进，证明了其在 3D 形状重建任务中的有效性。</p></li><li><p>Methods：（1）提出贝叶斯扩散模型（BDM）框架，将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。（2）设计一个联合扩散过程，逐步将先验信息融合到3D形状重建中，无需显式指定先验和似然。（3）采用变分推断方法，近似后验分布，并通过逆扩散过程生成3D形状。</p></li><li><p>结论：(1): 本文提出了一种基于贝叶斯扩散模型（BDM）的3D形状重建新方法，该方法通过联合扩散过程将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理，在3D形状重建任务中取得了显着改进。(2): 创新点：</p></li><li>提出贝叶斯扩散模型（BDM）框架，将自上而下（先验）信息与自下而上（数据驱动）过程紧密耦合，执行有效的贝叶斯推理。</li><li>设计一个联合扩散过程，逐步将先验信息融合到3D形状重建中，无需显式指定先验和似然。</li><li>采用变分推断方法，近似后验分布，并通过逆扩散过程生成3D形状。Performance：</li><li>在合成和真实世界基准上对BDM进行了评估，用于3D形状重建。</li><li>实验结果表明，与最先进的方法相比，BDM在各种指标上都取得了显着改进，证明了其在3D形状重建任务中的有效性。Workload：</li><li>本文的工作量中等，需要对贝叶斯扩散模型、3D形状重建和变分推断方法有一定的了解。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7422b82570cb43b0e03df4c70a22bd9a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-024cf388128af8fcbb5768c6b5cbd193.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75567a8fc44c36c6e2757bf6b21b6dcf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-837f8b78a5d65ec0d93f1545faef964c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43923b8a4efdf4a63b3fd3998d1b5749.jpg" align="middle"></details><h2 id="SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data"><a href="#SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data" class="headerlink" title="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data"></a>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data</h2><p><strong>Authors:Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</strong></p><p>Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM’s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models. </p><p><a href="http://arxiv.org/abs/2403.06952v1">PDF</a> First two authors contributed equally; Project website:   <a href="https://selma-t2i.github.io/">https://selma-t2i.github.io/</a></p><p><strong>Summary</strong><br>多技能专家学习与自动生成数据，融合提升T2I模型逼真度，显著改善语义对齐和文本忠实度。</p><p><strong>Key Takeaways</strong></p><ul><li>SELMA融合多技能专家学习与自动生成数据提升T2I模型逼真度。</li><li>LLM生成多样文本提示，对应不同技能，训练T2I模型获取新技能。</li><li>独立专家微调针对不同技能，专家融合打造多技能T2I模型处理多样文本提示。</li><li>SELMA显著提升SOTA T2I模型语义对齐和文本忠实度（TIFA+2.1%，DSG+6.9%）。</li><li>自动收集的图像文本用于微调性能接近真实数据微调。</li><li>较弱T2I模型图像用于微调可以提升较强T2I模型生成质量，展现T2I模型的弱到强泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SELMA：通过自动生成的数据学习和合并特定技能的文本到图像专家</li><li>作者：Jialu Li、Jaemin Cho、Yi-Lin Sung、Jaehong Yoon、Mohit Bansal</li><li>所属机构：北卡罗来纳大学教堂山分校</li><li>关键词：文本到图像、图像生成、专家学习、知识融合</li><li>论文链接：https://arxiv.org/abs/2403.06952 Github：无</li><li>摘要：（1）研究背景：文本到图像（T2I）生成模型在创建图像方面取得了令人印象深刻的进展，但它们仍然难以生成与文本输入细节完全匹配的图像，例如不正确的空间关系或缺失对象。（2）过去的方法：以往方法侧重于监督学习或无监督学习，但它们在捕捉文本提示中的所有语义方面存在局限性。（3）研究方法：SELMA 提出了一种新范式，通过在自动生成的多技能图像-文本数据集上对模型进行微调，并结合特定技能的专家学习和合并，来提高 T2I 模型的保真度。（4）任务和性能：SELMA 在多个基准上显着提高了最先进的 T2I 扩散模型的语义对齐和文本保真度（在 TIFA 上提高了 2.1%，在 DSG 上提高了 6.9%），人类偏好指标（PickScore、ImageReward 和 HPS），以及人类评估。</li></ol><p><strong>方法：</strong></p><p>(1) <strong>自动生成多技能图像-文本数据集：</strong>使用预训练的T2I模型生成图像，并使用文本提示对其进行注释，创建包含各种技能（例如对象生成、属性编辑、场景合成）的数据集。</p><p>(2) <strong>特定技能的专家学习：</strong>在自动生成的数据集上微调T2I模型，专注于特定技能的学习。这有助于模型掌握特定技能所需的知识。</p><p>(3) <strong>专家合并：</strong>将训练过的特定技能专家模型合并到主T2I模型中。通过融合专家知识，主模型可以同时利用不同技能，从而提高图像生成的保真度。</p><p>(4) <strong>微调：</strong>在最终的多技能图像-文本数据集上微调合并后的T2I模型，以进一步提高其性能。</p><ol><li>结论：(1): 本工作提出了一种新范式 SELMA，通过利用 T2I 模型的预训练知识，提高了最先进的 T2I 模型在生成和人类偏好方面的保真度。SELMA 首先收集了在不需要额外人工注释的情况下给定各种生成的文本提示的自我生成图像。然后，SELMA 在不同的数据集上对单独的 LoRA 模型进行微调，并在推理期间合并它们，以减轻数据集之间的知识冲突。SELMA 在提高 T2I 模型的保真度和与人类偏好的对齐度方面展示了强大的经验结果，并表明基于扩散的 T2I 模型具有潜在的弱到强泛化能力。(2): 创新点：提出了一种通过自动生成多技能图像-文本数据集、特定技能专家学习和专家合并来提高 T2I 模型保真度的新范式。性能：在多个基准上显着提高了最先进的 T2I 扩散模型的语义对齐和文本保真度，人类偏好指标和人类评估。工作量：需要生成和注释大量图像-文本数据，并训练和合并多个专家模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-a71fb7431e2ed3366a76c62d6434a3a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-26fd4cb2b211747179211fa7dd2b38a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-951031dbd570a29204c573bd83992954.jpg" align="middle"></details><h2 id="Distribution-Aware-Data-Expansion-with-Diffusion-Models"><a href="#Distribution-Aware-Data-Expansion-with-Diffusion-Models" class="headerlink" title="Distribution-Aware Data Expansion with Diffusion Models"></a>Distribution-Aware Data Expansion with Diffusion Models</h2><p><strong>Authors:Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, Bin Wang</strong></p><p>The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a> </p><p><a href="http://arxiv.org/abs/2403.06741v1">PDF</a> Project: <a href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a></p><p><strong>Summary</strong><br>扩散模型领域的最新研究提出了一种名为 DistDiff 的高效数据扩展框架，它利用了分布感知扩散模型，显著提升了图像生成任务的分布一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>数据集的规模和质量对深度模型的性能至关重要。</li><li>数据集扩充技术可以自动扩充数据集，释放深度模型的潜力。</li><li>基于图像变换的数据扩充方法只能引入局部变化，多样性较差。</li><li>基于图像合成的扩充方法可以创造全新内容，显著提高信息性。</li><li>现有的合成方法存在分布偏差的风险，可能会降低模型对分布外样本的性能。</li><li>DistDiff 基于分布感知扩散模型，通过构造分层原型和分层能量指导来近似真实数据分布。</li><li>DistDiff 在数据扩展任务中实现了分布一致样本的生成，取得了显著提升。</li><li>与在原始数据集上训练的模型相比，DistDiff 在六个图像数据集上的准确率提升了 30.7%，与最先进的基于扩散的方法相比，提升了 9.8%。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散模型的分布感知数据扩充</li><li>作者：朱浩伟、杨凌、雍军海、张文涛、王斌</li><li>隶属单位：清华大学</li><li>关键词：数据扩充、扩散模型、分布感知</li><li>链接：https://github.com/haoweiz23/DistDiff</li><li><p>摘要：（1）研究背景：大规模高质量数据集对于深度模型至关重要，但获取此类数据集成本高昂且耗时。数据扩充技术旨在自动扩充数据集，释放深度模型的全部潜力。（2）过去方法：现有数据扩充方法包括基于图像变换和基于合成的两种类型。基于图像变换的方法只能引入局部变化，多样性较差。基于合成的图像生成方法虽然可以创建全新的内容，但存在分布偏差的风险，可能会降低模型的性能。（3）方法：本文提出了一种基于分布感知扩散模型的有效数据扩充框架 DistDiff。DistDiff 构建分层原型以逼近真实数据分布，在具有分层能量引导的扩散模型中优化潜在数据点。（4）性能：DistDiff 在不进行额外训练的情况下，在六个图像数据集上实现了比在原始数据集上训练的模型准确率提高 30.7%，比最先进的基于扩散的方法提高 9.8%。这些性能提升证明了该方法的有效性。</p></li><li><p>Methods:(1): 将原始数据分布近似为分层原型，指导扩散模型的采样过程；(2): 引入残差乘法变换，在可控范围内调整潜在特征；(3): 在采样过程中加入能量引导，优化变换参数，使生成的样本与真实数据分布一致；(4): 利用预训练的特征提取器和去噪网络，构建能量函数，指导采样过程；(5): 优化中间采样步骤，而不是仅优化最终采样结果。</p></li><li><p>结论：（1）本论文提出的 DistDiff 方法在数据扩充领域取得了显著进展，为基于扩散模型的数据扩充提供了新的思路。（2）创新点：</p></li><li>提出了一种基于分层原型的分布感知数据扩充框架，有效逼近真实数据分布。</li><li>引入了残差乘法变换和能量引导机制，在可控范围内优化潜在特征，提高生成样本的质量。</li><li>利用预训练的特征提取器和去噪网络构建能量函数，指导采样过程，提升生成样本与真实数据的相似性。</li><li>优化了中间采样步骤，而不是仅优化最终采样结果，提高了生成样本的多样性和真实性。</li><li>在六个图像数据集上的广泛实验中，DistDiff 方法取得了优异的性能，证明了其有效性。</li><li>性能：在不进行额外训练的情况下，DistDiff 在六个图像数据集上实现了比在原始数据集上训练的模型准确率提高 30.7%，比最先进的基于扩散的方法提高 9.8%。</li><li>工作量：DistDiff 方法的实现相对复杂，需要构建分层原型、优化潜在特征和能量函数，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-51004e76bd54c2109bfb0cba773b0e50.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fa6c026111223b0c29b77804e9db13e2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-54f57321604f976084e4edde1c9cc9fd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-272c701cea8b6d59603b8700ded9462f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db0b8236d7ff4e2af692d5671eac4b67.jpg" align="middle"></details><h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p><p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p><p><a href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p><p><strong>Summary</strong><br>利用预训练视频扩散模型的世界模拟能力促进三维生成。</p><p><strong>Key Takeaways</strong></p><ul><li>V3D 利用预训练视频扩散模型的世界模拟能力促进三维生成。</li><li>引入几何一致性先验，将视频扩散模型扩展为多视图一致的三维生成器。</li><li>可以微调最先进的视频扩散模型，以生成给定单张图像周围对象的 360 度轨道帧。</li><li>通过定制的重建管道，可以在 3 分钟内生成高质量的网格或三维高斯分布。</li><li>方法可以扩展到场景级的新颖视图合成，通过稀疏输入视图精确控制相机路径。</li><li>大量实验表明所提出的方法具有卓越的性能，尤其是在生成质量和多视图一致性方面。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1.标题：V3D：视频扩散模型是有效的 3D 生成器</p><ol><li>作者：Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</li><li>第一作者单位：清华大学</li><li>关键词：3D 生成、视频扩散模型、深度学习</li><li>论文链接：arXiv:2403.06738v1[cs.CV]11Mar2024</li><li>摘要：<br>(1) 研究背景：自动 3D 生成近年来备受关注。最近的方法极大地提高了生成速度，但由于模型容量有限，通常会生成细节较少的对象。<br>(2) 过去的方法：过去的方法主要基于生成对抗网络 (GAN) 或自回归模型。GAN 容易出现模式崩溃和训练不稳定问题，而自回归模型生成速度较慢。<br>(3) 本文提出的研究方法：本文提出了一种基于视频扩散模型的 3D 生成方法 V3D。V3D 将视频扩散模型应用于 3D 生成，通过逐步添加噪声和反转扩散过程来生成 3D 对象。<br>(4) 方法性能：在 ShapeNet 数据集上的评估表明，V3D 在生成质量和生成速度方面都优于现有方法。V3D 可以生成高保真 3D 对象，生成时间仅需 3 分钟。&lt;/p&gt;<br><p>7.Methods：（1）：V3D采用视频扩散模型，将3D对象生成过程视为从噪声分布逐步去噪的过程；（2）：V3D使用U-Net作为生成器，通过反向扩散过程逐步添加噪声，生成3D对象；（3）：V3D使用多尺度训练策略，提高生成对象的细节和保真度；（4）：V3D使用感知损失和对抗损失作为训练目标，提高生成对象的视觉质量和多样性。</p><ol><li>结论：（1）本工作通过将视频扩散模型应用于3D生成，提出了一种新颖且高效的方法V3D，在生成一致的多视角图像方面取得了显著进展。V3D扩展了视频扩散模型在3D生成任务中的应用，为高质量3D生成和视频扩散模型在3D任务中的广泛应用铺平了道路。（2）创新点：</li><li>提出了一种基于视频扩散模型的3D生成方法V3D，通过反向扩散过程逐步添加噪声生成3D对象。</li><li>设计了一种定制的重建管道，用于从生成的视图中获取3D资产，并支持在3分钟内重建高质量的3D网格。</li><li>将V3D扩展到场景级新视角合成，实现了对摄像机路径的精确控制和多视角一致性。性能：</li><li>在ShapeNet数据集上，V3D在生成质量和生成速度方面均优于现有方法。</li><li>V3D可以生成高保真3D对象，生成时间仅需3分钟。</li><li>V3D在生成一致的多视角图像和场景级新视角合成方面表现出色。工作量：</li><li>V3D的实现相对简单，易于使用。</li><li>V3D的训练过程高效，在ShapeNet数据集上训练V3D仅需数小时。</li><li>V3D的推理速度快，可以快速生成3D对象。</li></ol></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-13  Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/03/11/Paper/2024-03-11/Diffusion%20Models/</id>
    <published>2024-03-11T12:35:46.000Z</published>
    <updated>2024-03-11T12:35:46.983Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-11-更新"><a href="#2024-03-11-更新" class="headerlink" title="2024-03-11 更新"></a>2024-03-11 更新</h1><h2 id="VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models"><a href="#VideoElevator-Elevating-Video-Generation-Quality-with-Versatile-Text-to-Image-Diffusion-Models" class="headerlink" title="VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models"></a>VideoElevator: Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo</strong></p><p>Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a>. </p><p><a href="http://arxiv.org/abs/2403.05438v1">PDF</a> Project page: <a href="https://videoelevator.github.io">https://videoelevator.github.io</a> Code:   <a href="https://github.com/YBYBZhang/VideoElevator">https://github.com/YBYBZhang/VideoElevator</a></p><p><strong>Summary</strong><br>视频提升器：通过图像扩散模型提升视频扩散模型的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>VideoElevator 是一种无训练、即插即用的方法，可利用图像扩散模型的优势提升视频扩散模型的性能。</li><li>与传统的视频扩散模型采样不同，VideoElevator 将每个采样步骤分解为时间运动细化和空间质量提升。</li><li>时间运动细化使用封闭的视频扩散模型来增强时间一致性。</li><li>空间质量提升利用充实的图像扩散模型直接预测更少噪声的潜在因素，增加更多逼真的细节。</li><li>VideoElevator 不仅提高了基于图像扩散模型的视频扩散模型的性能，还促进了使用个性化图像扩散模型的风格化视频合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：VideoElevator：利用多功能文本到图像扩散模型提升视频生成质量</li><li>作者：Yabo Zhang1, Yuxiang Wei1, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji2, and Wangmeng Zuo1</li><li>第一作者单位：哈尔滨工业大学</li><li>关键词：文本到图像扩散模型，文本到视频扩散模型，视频生成，质量提升</li><li>论文链接：https://videoelevator.github.io   Github 代码链接：None</li><li>摘要：(1) 研究背景：文本到图像扩散模型（T2I）在生成逼真且美观的图像方面表现出了前所未有的能力。相反，文本到视频扩散模型（T2V）在帧质量和文本对齐方面仍然远远落后，这是由于训练视频的质量和数量不足。(2) 过去方法及其问题：现有方法直接对视频进行采样，但由于缺乏足够的训练数据，生成的视频质量较差。(3) 本文方法：VideoElevator 提出了一种无训练且即插即用的方法，利用 T2I 的出色能力提升 T2V 的性能。它将每个采样步骤明确分解为时间运动细化和空间质量提升。时间运动细化使用封装的 T2V 增强时间一致性，然后反转为 T2I 所需的噪声分布。然后，空间质量提升利用膨胀的 T2I 直接预测噪声较小的潜在变量，添加更多逼真的细节。(4) 方法性能：在各种 T2V 和 T2I 模型组合下的广泛提示中进行了实验。结果表明，VideoElevator 在帧质量、时间一致性和文本对齐方面显著提升了 T2V 的性能，证明了其提升 T2V 质量的有效性。</li></ol><p>7.方法：(1) VideoElevator将每个采样步骤明确分解为时间运动细化和空间质量提升；(2) 时间运动细化使用封装的T2V增强时间一致性，然后反转为T2I所需的噪声分布；(3) 空间质量提升利用膨胀的T2I直接预测噪声较小的潜在变量，添加更多逼真的细节。</p><ol><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li><li><p>结论：（1）：VideoElevator提出了一种无训练且即插即用的方法，利用T2I的出色能力提升T2V的性能，为提升视频生成质量提供了一种新的思路。（2）：创新点：</p></li><li>提出了一种无训练且即插即用的方法，将T2I的优势引入T2V中。</li><li>将每个采样步骤明确分解为时间运动细化和空间质量提升，提高了视频的时间一致性和空间质量。性能：</li><li>在各种T2V和T2I模型组合下的广泛提示中进行了实验，结果表明VideoElevator在帧质量、时间一致性和文本对齐方面显著提升了T2V的性能。工作量：</li><li>VideoElevator是一种无训练且即插即用的方法，工作量较小，易于与现有的T2V模型集成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cad376bbaa11399212fdef9f175c2469.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c6b6b777c3f6359e627b50aeeac2627b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-907eeb8949cad583968ae2444608f263.jpg" align="middle"></details><h2 id="Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation"><a href="#Towards-Effective-Usage-of-Human-Centric-Priors-in-Diffusion-Models-for-Text-based-Human-Image-Generation" class="headerlink" title="Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation"></a>Towards Effective Usage of Human-Centric Priors in Diffusion Models for   Text-based Human Image Generation</h2><p><strong>Authors:Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song</strong></p><p>Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls — human-centric priors such as pose or depth maps — during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \url{<a href="https://hcplayercvpr2024.github.io}">https://hcplayercvpr2024.github.io}</a>. </p><p><a href="http://arxiv.org/abs/2403.05239v1">PDF</a> Accepted to CVPR 2024</p><p><strong>Summary</strong><br>在文本到图像扩散模型中融合以人为中心的信息可以显著提高图像质量，特别是人体图像的生成。</p><p><strong>Key Takeaways</strong></p><ul><li>人体图像生成中存在姿势和比例不自然等问题。</li><li>现有的方法主要通过微调模型或增加图像生成阶段的人体约束来解决。</li><li>本文将人体约束直接融入模型微调阶段，无需在推理阶段添加约束。</li><li>人体约束对齐损失加强了图像生成过程中文本当中的人体相关信息。</li><li>采用可控尺度和分步约束，保证微调过程中的语义丰富性和人体结构准确性。</li><li>实验表明，该方法显著优于现有文本到图像模型，可基于用户输入生成高质量人体图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于文本的人体图像生成的人类中心对齐损失</li><li>作者：Zhaoyang Huang, Bin Li, Zizhao Zhang, Zhihao Fang, Yan Yan, Xiaogang Wang</li><li>隶属：中国科学院自动化研究所</li><li>关键词：文本到图像生成、人类图像生成、人体对齐、扩散模型</li><li>论文链接：None，Github代码链接：None</li><li>摘要：（1）研究背景：现有文本到图像扩散模型在生成人体图像时存在解剖结构不准确、姿势不自然等问题。</li></ol><p>（2）过去方法及问题：现有方法主要通过微调模型或添加人体中心先验（如姿势或深度图）来解决上述问题，但这些方法在推理阶段需要额外的条件。</p><p>（3）研究方法：本文提出了一种人类中心对齐损失，将文本提示中的人体相关信息融入交叉注意力图中，并在微调过程中引入尺度感知和步长约束，以保证语义细节丰富和人体结构准确。</p><p>（4）方法性能：在 Human-Art 数据集上进行的广泛实验表明，该方法在生成高质量人体图像方面明显优于现有文本到图像模型。</p><p>方法：(1):提出人类中心先验层（HcP）和人类中心对齐损失，增强模型对人类中心文本信息的敏感性，提高生成人体图像的结构准确性和细节。(2):分析交叉注意力层在不同时间步和分辨率尺度下的作用，发现早期时间步和中间分辨率尺度对人体结构生成至关重要。(3):设计HcP层，从文本嵌入中提取人类中心token，并与潜在特征进行交互，生成人类中心注意力图。(4):提出人类中心对齐损失，将预训练的实体关系网络提取的人类中心单词对应的关键姿势图像与HcP层生成的注意力图对齐，指导模型关注人体结构细节。</p><ol><li>结论：（1）：本文提出了一种简单且有效的方法，利用人类中心先验（HcP），例如姿势或深度图，来提高现有文本到图像模型中的人体图像生成质量。所提出的 HcP 层有效地利用了关于人类的信息在微调过程中，无需在从文本生成图像时需要额外的输入。大量的实验表明，HcP 层不仅修复了人体结构生成中的结构不准确问题，而且还保留了原始的审美品质和细节。未来的工作将探索整合多种类型的人类中心先验，以进一步推进人类图像和视频生成。（2）：创新点：提出了一种新颖的人类中心对齐损失，将文本提示中的人体相关信息融入交叉注意力图中，指导模型关注人体结构细节。分析了交叉注意力层在不同时间步和分辨率尺度下的作用，发现早期时间步和中间分辨率尺度对人体结构生成至关重要。设计了 HcP 层，从文本嵌入中提取人类中心 token，并与潜在特征进行交互，生成人类中心注意力图。性能：在 Human-Art 数据集上进行的广泛实验表明，该方法在生成高质量人体图像方面明显优于现有文本到图像模型。消融研究和可视化结果验证了所提出方法的有效性，证明了人类中心对齐损失和 HcP 层在提高人体图像生成质量中的作用。工作量：该方法的实现相对简单，只需在微调过程中添加 HcP 层和人类中心对齐损失。该方法不需要额外的条件，例如姿势或深度图，在推理阶段使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dcb4970717d9f287c0e2b916300f3dd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cef2974d0c0ed77c5f9c42184d7e57c4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-564f5b115d714883587e123a15ef8050.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5ade1a99be6f3185ad39bc934410199.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfa83d53d9802f58aba15bf8be1a8b64.jpg" align="middle"></details><h2 id="Denoising-Autoregressive-Representation-Learning"><a href="#Denoising-Autoregressive-Representation-Learning" class="headerlink" title="Denoising Autoregressive Representation Learning"></a>Denoising Autoregressive Representation Learning</h2><p><strong>Authors:Yazhe Li, Jorg Bornschein, Ting Chen</strong></p><p>In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05196v1">PDF</a> </p><p><strong>Summary</strong><br>自回归扩散模型 DARL 实现图像生成和视觉表示学习相结合，展现出与先进掩码预测模型媲美的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>DARL 使用仅解码器的 Transformer 来自回归预测图像块。</li><li>仅 MSE 训练即可产生强大的表示。</li><li>使用去噪块解码器将 MSE 损失替换为扩散目标可以增强图像生成能力。</li><li>定制噪声调度和在更大模型上的更长时间训练可以提高学习表示。</li><li>最佳调度与标准图像扩散模型中使用的调度显著不同。</li><li>尽管架构简单，但 DARL 在微调协议下提供接近最先进掩码预测模型的性能。</li><li>DARL 代表了将自回归和去噪扩散模型的优势结合起来，实现视觉感知和生成相统一的重要一步。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：去噪自回归表征学习</li><li>作者：Yazhe Li，Jorg Bornschein，Ting Chen</li><li>第一作者单位：Google DeepMind</li><li>关键词：视觉表征学习，自回归模型，去噪扩散模型，图像生成</li><li>论文链接：None    Github 链接：None</li><li><p>摘要：   （1）研究背景：视觉表征学习和图像生成通常使用不同的技术，前者注重鲁棒性，后者注重生成能力。   （2）过去方法：对比学习、蒸馏自监督学习、掩码图像建模等方法在表征学习中表现出色，但缺乏生成能力。   （3）研究方法：本文提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器 Transformer 预测图像块。通过使用均方误差损失和去噪块解码器，增强了图像生成能力。   （4）性能表现：该方法在微调协议下，表现接近最先进的掩码预测模型，表明其在表征学习和生成方面的潜力。</p></li><li><p>Methods：(1) 提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器 Transformer 预测图像块；(2) 使用均方误差损失和去噪块解码器，增强了图像生成能力。</p></li><li><p>总结：(1): 本文提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器Transformer预测图像块，在微调协议下表现接近最先进的掩码预测模型，表明其在表征学习和生成方面的潜力。(2): Innovation point: 提出了一种统一模型，结合自回归模型和去噪扩散模型，使用解码器Transformer预测图像块。Performance: 在微调协议下表现接近最先进的掩码预测模型。Workload: 未提及。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3a6bd101af2be0b75af14290ca20154b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-901dfa573ba65a2319ddfc43d65a7325.jpg" align="middle"><img src="https://picx.zhimg.com/v2-56ec555eccb7ae9c20c196a5c5519463.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc4830941b2dbf44695f875173f8eef5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6402f59254c8b1442a49f2075fd0b2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2f3936f0ef0cf91ab8b2bb5de579b005.jpg" align="middle"></details><h2 id="Improving-Diffusion-Models-for-Virtual-Try-on"><a href="#Improving-Diffusion-Models-for-Virtual-Try-on" class="headerlink" title="Improving Diffusion Models for Virtual Try-on"></a>Improving Diffusion Models for Virtual Try-on</h2><p><strong>Authors:Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</strong></p><p>This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting diffusion models for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., GAN-based), but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel diffusion model that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the diffusion model, 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the self-attention layer. In addition, we provide detailed textual prompts for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both diffusion-based and GAN-based) in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario. </p><p><a href="http://arxiv.org/abs/2403.05139v1">PDF</a> </p><p><strong>Summary</strong><br>图像基于的虚拟试穿，在给定描述人物和衣服图像的情况下，渲染人物穿着定制衣服的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>改进的扩散模型用于虚拟试穿，以提高生成视觉效果的自然度。</li><li>提出的 IDM-VTON 模型在保留服装身份的同时提高了服装保真度。</li><li>该方法使用两个模块来编码服装图像的语义。</li><li>高级语义融合到交叉注意层，低级特征融合到自注意层。</li><li>提供详细的文本提示，以增强生成视觉效果的真实性。</li><li>使用一对人物服装图像的定制方法显着提高了保真度和真实性。</li><li>实验结果表明，该方法在保留服装细节和生成真实的虚拟试穿图像方面优于先前的方法。</li><li>所提出的定制方法在真实场景中展示了其有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：提升扩散模型以实现真实的虚拟试穿</li><li>Authors：Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</li><li>Affiliation：韩国科学技术院（KAIST）</li><li>Keywords：图像生成、虚拟试穿、扩散模型</li><li>Urls：https://arxiv.org/abs/2403.05139</li><li><p>摘要：（1）研究背景：图像式虚拟试穿旨在给定描绘人物和服饰的两幅图像，生成人物穿着特定服饰的图像。（2）过去方法：现有工作将基于示例的图像修复扩散模型应用于虚拟试穿，与其他方法（如基于 GAN 的方法）相比，可以提高生成视觉效果的自然性，但无法保留服饰的特征。（3）研究方法：提出一种新的扩散模型 IDM-VTON，该模型使用两个不同的模块对服饰图像的语义进行编码；在给定扩散模型的基本 U-Net 的情况下，1）从视觉编码器中提取的高级语义被融合到交叉注意层，然后 2）从并行 U-Net 中提取的低级特征被融合到自注意层。（4）方法性能：在真实世界数据集上，IDM-VTON 在图像质量和服饰保真度方面都优于现有方法。这些结果支持了该方法的目标，即生成真实、保真且可定制的虚拟试穿图像。</p></li><li><p>Methods:(1): IDM-VTON采用基于示例的图像修复扩散模型，利用两个不同的模块对服饰图像的语义进行编码。(2): 视觉编码器提取服饰图像的高级语义，并将其融合到交叉注意层中。(3): 并行U-Net提取服饰图像的低级特征，并将其融合到自注意层中。(4): 在给定扩散模型的基本U-Net的情况下，融合后的高级语义和低级特征被用于指导图像生成过程。</p></li><li><p>结论：（1）本工作意义：提出了一种新的扩散模型 IDM-VTON，用于真实虚拟试穿，特别是在实际场景中。我们结合了两个独立的模块对服饰图像进行编码，即视觉编码器和并行 U-Net，它们分别有效地对基本 U-Net 编码高级语义和低级特征。为了在实际场景中改进虚拟试穿，我们提出通过微调给定一对服饰-人物图像的 U-Net 解码器层来定制我们的模型。我们还利用了服饰的详细自然语言描述，这有助于生成真实的虚拟试穿图像。在各种数据集上的广泛实验表明，我们的方法在保留服饰细节和生成高保真图像方面优于先前的研究。特别是，我们展示了我们的方法在实际场景中进行虚拟试穿的潜力。（2）创新点：提出了 IDM-VTON，这是一种用于真实虚拟试穿的扩散模型的新设计，特别是在实际场景中。结合了两个独立的模块对服饰图像进行编码，即视觉编码器和并行 U-Net，它们分别有效地对基本 U-Net 编码高级语义和低级特征。性能：在图像质量和服饰保真度方面优于现有方法。工作量：与基于 GAN 的方法相比，基于示例的图像修复扩散模型通常具有更高的计算成本。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d38c4cb395c666b5e4fd3e52269fff3f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d67b069f37d9810aa657e9e7dd415a5a.jpg" align="middle"></details><h2 id="ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment"><a href="#ELLA-Equip-Diffusion-Models-with-LLM-for-Enhanced-Semantic-Alignment" class="headerlink" title="ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment"></a>ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment</h2><p><strong>Authors:Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu</strong></p><p>Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient Large Language Model Adapter, termed ELLA, which equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment without training of either U-Net or LLM. To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from LLM. Our approach adapts semantic features at different stages of the denoising process, assisting diffusion models in interpreting lengthy and intricate prompts over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their prompt-following capabilities. To assess text-to-image models in dense prompt following, we introduce Dense Prompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K dense prompts. Extensive experiments demonstrate the superiority of ELLA in dense prompt following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships. </p><p><a href="http://arxiv.org/abs/2403.05135v1">PDF</a> Project Page: <a href="https://ella-diffusion.github.io/">https://ella-diffusion.github.io/</a></p><p><strong>Summary</strong><br>文本到图像扩散模型加入语言大模型增强器 ELLA，大幅提升丰富提示理解能力，无需训练 U 形网络或语言大模型。</p><p><strong>Key Takeaways</strong></p><ul><li>ELLA 语言大模型增强器通过无缝连接，提升文本到图像扩散模型的文本对齐能力，无需训练 U 形网络或语言大模型。</li><li>提出时间感知语义连接器 (TSC)，动态从语言大模型中提取与时间步长相关的条件。</li><li>在去噪过程的不同阶段，自适应地调整语义特征，帮助扩散模型随着采样时间步长解释冗长复杂提示。</li><li>ELLA 可以轻松与社区模型和工具集成，提升其提示遵循能力。</li><li>引入密集提示图基准 (DPG-Bench)，用于评估文本到图像模型在密集提示遵循方面的表现。</li><li>广泛实验验证了 ELLA 在密集提示遵循方面的优势，尤其是在涉及多种属性和关系的多对象组合中。</li><li>ELLA 在保持生成图像质量的同时，提升了定量和定性评估的文本对齐分数。</li><li>ELLA 将文本到图像扩散模型与语言大模型相结合，探索了文本和图像生成之间的潜在联系。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：ELLA：使用 LLM 为扩散模型赋能以增强语义对齐</li><li>作者：胡锡威、王瑞、方一晓、付斌、程培、于钢</li><li>第一作者单位：腾讯</li><li>关键词：扩散模型、大语言模型、文本-图像对齐</li><li>论文链接：https://ella-diffusion.github.io，Github 代码链接：None</li><li>摘要：（1）研究背景：扩散模型在文本到图像生成领域取得了显著的进展。然而，大多数广泛使用的模型仍然使用 CLIP 作为其文本编码器，这限制了它们理解包含多个对象、详细属性、复杂关系、长文本对齐等内容的密集提示的能力。（2）已有方法及问题：为了解决上述问题，本文提出了 ELLA，这是一种高效的大语言模型适配器，它为文本到图像扩散模型配备了强大的大语言模型 (LLM)，以增强文本对齐，而无需训练 U-Net 或 LLM。（3）研究方法：为了无缝桥接两个预训练模型，本文研究了一系列语义对齐连接器设计，并提出了一个新颖的模块，即 TimeStep-Aware 语义连接器 (TSC)，它动态地从 LLM 中提取与时间步长相关的条件。（4）实验结果：本文提出的方法在密集提示跟随任务中展示了优于最先进方法的优势，特别是在涉及不同属性和关系的多个对象组合中。</li></ol><p>7.方法：（1）：设计ELLA架构，利用LLM的语言理解能力和扩散模型的图像生成潜力，采用TimeStep-Aware语义连接器（TSC）无缝连接两个预训练模型；（2）：构建数据集，采用CogVLM自动生成图像描述，提高图像与文本的相关性和语义信息的密度；（3）：构建基准测试，提出密集提示图谱基准（DPG-Bench），提供更长、更具信息量的提示，全面评估生成模型遵循密集提示的能力。</p><p>8.结论：(1): 本工作提出了一种有效的大语言模型适配器ELLA，该适配器通过TimeStep-Aware语义连接器将大语言模型与扩散模型无缝连接，增强了文本对齐，在密集提示跟随任务中取得了优异的性能。(2): 创新点：* 提出了一种新的语义对齐连接器TSC，动态地从大语言模型中提取与时间步长相关的条件，增强了文本和图像之间的语义对齐。* 构建了密集提示图谱基准DPG-Bench，提供更长、更具信息量的提示，全面评估生成模型遵循密集提示的能力。* 采用CogVLM自动生成图像描述，提高图像与文本的相关性和语义信息的密度。性能：* 在密集提示跟随任务中，ELLA在生成图像的语义对齐和视觉保真度方面均优于最先进的方法。工作量：* ELLA的训练和部署相对高效，不需要训练U-Net或大语言模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-4cf50b2bd0a34d7b9b26b53c13b5a923.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fc587ddf93c75ebf159a0c6b73925633.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a0b7496441cb8c23d5d6a09243c13c67.jpg" align="middle"></details>## CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion**Authors:Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang**Recent advancements in text-to-image generative systems have been largely driven by diffusion models. However, single-stage text-to-image diffusion models still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of text-to-image diffusion. CogView3 is the first model implementing relay diffusion in the realm of text-to-image generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive text-to-image outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source text-to-image diffusion model, by 77.0\% in human evaluations, all while requiring only about 1/2 of the inference time. The distilled variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL. [PDF](http://arxiv.org/abs/2403.05121v1) **Summary**CogView3，一个级联框架，引入接力扩散，在文本到图像生成中实现低分辨率到高分辨率，提高效率和图像质量。**Key Takeaways**- CogView3提出级联框架，使用接力扩散生成高分辨率图像。- 接力扩散分步生成图像，从低分辨率到高分辨率，降低训练和推理成本。- CogView3超越SDXL，人类评估得分高出77.0%，推理时间减少一半。- CogView3的精简版性能相当，推理时间仅为SDXL的十分之一。- CogView3提高了文本到图像生成任务的效率和图像质量。- CogView3 引入了接力扩散的概念，在文本到图像生成中实现了分辨率的渐进提升。- 级联框架和接力扩散的结合，能够有效地平衡图像质量和计算成本。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CogView3：更精细、更快速的文本到图像生成</li><li>作者：Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang</li><li>单位：清华大学</li><li>关键词：文本到图像生成·扩散模型</li><li>论文链接：https://arxiv.org/abs/2403.05121</li><li><p>摘要：(1) 研究背景：扩散模型已成为文本到图像生成系统的主流框架。然而，单阶段文本到图像扩散模型在计算效率和图像细节精细化方面仍面临挑战。(2) 过去方法及问题：现有方法大多在高图像分辨率下进行扩散过程，这导致计算成本高、图像细节不够精细。(3) 提出方法：本文提出 CogView3，一个创新的级联框架，通过中继扩散来增强文本到图像扩散的性能。CogView3 是第一个在文本到图像生成领域实现中继扩散的模型，它通过首先创建低分辨率图像，然后应用基于中继的超分辨率来执行任务。(4) 实验结果：实验结果表明，CogView3 在人类评估中比当前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，同时推理时间仅为其一半左右。CogView3 的蒸馏变体在推理时间仅为 SDXL 的 1/10 的情况下实现了可比的性能。</p></li><li><p>方法：（1）文本预处理图像重述：利用 GPT-4V 自动生成训练数据集图像的重述文本，并微调 CogVLM-17B 以获得重述模型；（2）提示扩展：利用语言模型将用户提示扩展为更全面的描述，以减少训练和推理之间的不一致；（3）模型构建：CogView3 采用 3 级 UNet 架构的文本到图像扩散模型，并使用预训练的 T5-XXL 编码器作为文本编码器；（4）训练管道：使用 Laion-2B 数据集进行训练，并采用渐进训练策略以降低训练成本；（5）中继超分辨率：在潜在空间中实现中继超分辨率，使用线性变换代替原始的局部模糊；（6）采样器构建：设计了与中继超分辨率相一致的采样器，并使用 DDIM 范式进行采样；（7）中继扩散的蒸馏：将渐进蒸馏方法与中继扩散框架相结合，以获得 CogView3 的蒸馏版本。</p></li><li><p>结论(1): 本工作提出了 CogView3，这是继电扩散框架中第一个文本到图像生成系统。CogView3 以极大降低的推理成本实现了优良的生成质量，这主要归功于中继管道。通过迭代实现 CogView3 的超分辨率阶段，我们能够实现极高分辨率（如 2048×2048）的高质量图像。同时，随着数据重新描述和提示扩展被纳入模型管道，与当前最先进的开源文本到图像扩散模型相比，CogView3 在提示理解和指令遵循方面取得了更好的性能。我们还探索了 CogView3 的蒸馏，并展示了其归功于继电扩散框架的简单性和能力。利用渐进蒸馏范例，CogView3 的蒸馏变体大幅减少了推理时间，同时仍保持了相当的性能。(2): 创新点：</p></li><li>提出了一种新的级联框架 CogView3，该框架通过中继扩散增强文本到图像扩散的性能。</li><li>设计了一种中继超分辨率方法，该方法在潜在空间中执行超分辨率，并使用线性变换代替原始的局部模糊。</li><li>探索了数据重新描述和提示扩展，以提高模型对提示的理解和指令遵循能力。性能：</li><li>在人类评估中，CogView3 比当前最先进的开源文本到图像扩散模型 SDXL 高出 77.0%，同时推理时间仅为其一半左右。</li><li>CogView3 的蒸馏变体在推理时间仅为 SDXL 的 1/10 的情况下实现了可比的性能。</li><li>CogView3 能够生成极高分辨率（如 2048×2048）的高质量图像。工作量：</li><li>CogView3 的训练管道相对简单，采用渐进训练策略以降低训练成本。</li><li>CogView3 的蒸馏变体进一步降低了推理成本，同时保持了可比的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-39c07129df4e18479bf6f2000e3bd45b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3130242f65670e2f9a99c29710ffccef.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a4b8e0b9de2b5980d7c1d4c49daded3b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7d124475c2a36f974604208e23b856.jpg" align="middle"></details><h2 id="Face2Diffusion-for-Fast-and-Editable-Face-Personalization"><a href="#Face2Diffusion-for-Fast-and-Editable-Face-Personalization" class="headerlink" title="Face2Diffusion for Fast and Editable Face Personalization"></a>Face2Diffusion for Fast and Editable Face Personalization</h2><p><strong>Authors:Kaede Shiohara, Toshihiko Yamasaki</strong></p><p>Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.05094v1">PDF</a> CVPR2024. Code: <a href="https://github.com/mapooon/Face2Diffusion">https://github.com/mapooon/Face2Diffusion</a>, Webpage:   <a href="https://mapooon.github.io/Face2DiffusionPage/">https://mapooon.github.io/Face2DiffusionPage/</a></p><p><strong>Summary</strong><br>人脸个性化通过植入从图片获取的人脸来实现预先训练的文转图像扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>从训练管道中去除与人脸无关的信息有助于提升编辑能力。</li><li>多尺度人脸编码器提供了清晰分离的人脸特征。</li><li>表情指导将人脸表情与人脸身份进行分离。</li><li>类别引导去噪正则化增强模型对人脸去噪的学习。</li><li>跨数据集实验表明，该方法提升了身份保真度与文本保真度之间的平衡。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Face2Diffusion：快速且可编辑的人脸个性化</li><li>作者：Kaede Shiohara, Toshihiko Yamasaki</li><li>单位：东京大学</li><li>关键词：Face personalization, Text-to-image diffusion model, Identity preservation, Editability</li><li>论文链接：https://arxiv.org/abs/2403.05094</li><li><p>摘要：（1）研究背景：文本到图像扩散模型在图像生成方面取得了显著进展，但将特定人脸插入预训练模型仍然具有挑战性，既要保持身份相似性，又要保证可编辑性。（2）过去方法：现有方法容易过度拟合训练样本，导致身份相似性和可编辑性之间的权衡。（3）研究方法：Face2Diffusion（F2D）通过从训练管道中去除与身份无关的信息来解决过度拟合问题，提高编码人脸的可编辑性。F2D包含三个新颖的组件：多尺度身份编码器、表情引导器和类别引导去噪正则化。（4）实验结果：在 FaceForensics++ 数据集和各种提示上的广泛实验表明，F2D 在身份和文本保真度之间的权衡方面明显优于之前的最先进方法。</p></li><li><p>方法：(1) 多尺度身份编码器：从人脸图像中提取多尺度特征，保留身份信息，降低过度拟合风险。(2) 表情引导器：指导扩散模型关注人脸表情的编辑，提高可编辑性。(3) 类别引导去噪正则化：引入类别信息，防止模型从无关噪声中学习，提高身份保真度。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 此项工作的意义</strong></p><p>Face2Diffusion 提出了一种可编辑的人脸个性化方法，通过解决过度拟合问题，提高了生成人脸的可编辑性，在身份保真度和文本保真度之间取得了更好的平衡。</p><p><strong>(2): 本文优缺点总结（三个维度：创新点、性能、工作量）</strong></p><p><strong>创新点：</strong></p><ul><li>多尺度身份编码器：提取多尺度特征，降低过度拟合风险。</li><li>表情引导器：指导扩散模型关注人脸表情的编辑。</li><li>类别引导去噪正则化：防止模型从无关噪声中学习。</li></ul><p><strong>性能：</strong></p><ul><li>在身份保真度和文本保真度之间取得了更好的平衡。</li><li>在各种提示和人脸数据集上表现出优异的性能。</li></ul><p><strong>工作量：</strong></p><ul><li>训练过程相对复杂，需要多尺度特征提取和正则化策略。</li><li>生成单个图像所需的时间与其他文本到图像扩散模型类似。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a4d3199be75c4ed763ad12e5fd6fd186.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-073fc885846ed7841fbefca59dc75bb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2c9194bd5afd5f761cca65c865fe0fb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b6ba7d02ff97010b563089ea86c62c6b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8bdf1923916c837b5df8251aa84ce58b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d5d8555605f33ef6be1a8b7ab0be10cc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-302c4a1ee77cbdfd8dba69c7d6a94497.jpg" align="middle"></details>## Spectrum Translation for Refinement of Image Generation (STIG) Based on   Contrastive Learning and Spectral Filter Profile**Authors:Seokjun Lee, Seung-Won Jung, Hyunseok Seo**Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG. [PDF](http://arxiv.org/abs/2403.05093v1) Accepted to AAAI 2024**Summary**生成对抗网络和扩散模型中频域差异问题，可通过频谱对比学习下的图像生成谱转换框架（STIG）有效解决。**Key Takeaways*** 提出STIG框架减轻生成对抗网络和扩散模型图像频域差异。* STIG基于图像到图像转换和对照学习，优化生成图像频谱。* STIG在八个伪造图像数据集上超越现有方法，显着降低FID和光谱的对数频率距离。* STIG通过减小光谱异常提高图像质量。* 经过STIG处理的伪造图像会迷惑基于频率的深度伪造检测器。* STIG使用频谱转换有效解决生成模型中频域差异问题。* STIG提升图像生成质量，增强对深度伪造检测器的鲁棒性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：图像生成精炼的光谱转换（STIG）</li><li>作者：Seokjun Lee、Seung-Won Jung、Hyunseok Seo</li><li>第一作者单位：韩国科学技术研究院生物医学研究部</li><li>关键词：图像生成、光谱转换、对比学习、频谱滤波器轮廓</li><li>论文链接：NoneGithub 代码链接：None</li><li>摘要：（1）研究背景：目前，图像生成和合成在生成模型的帮助下取得了显著进展。尽管生成结果逼真，但在频域中仍然存在固有的差异。这种频谱差异不仅出现在生成对抗网络中，还出现在扩散模型中。（2）过去方法及其问题：以往的研究提出了通过修改生成网络架构或目标函数来弥补频域差异的方法，但仍有改进空间。（3）本文提出的研究方法：本文提出了一种基于对比学习的光谱转换框架（STIG），用于有效减轻生成图像频域中的差异，以提高 GAN 和扩散模型的生成性能。该框架采用了数字信号处理中图像到图像转换和对比学习的概念来优化生成图像的光谱。（4）方法在任务和性能上的表现：在八个假图像数据集和各种前沿模型上评估了 STIG 的有效性。结果表明，STIG 优于其他前沿方法，在 FID 和光谱对数频率距离方面有显著下降。此外，STIG 通过减少光谱异常来提高图像质量。验证结果表明，当 STIG 处理虚假光谱时，基于频率的深度伪造检测器更容易混淆。</li></ol><p>7.Methods：（1）STIG框架概述：STIG框架由三个主要组件组成：图像到图像转换网络（I2I）、对比学习损失函数和频谱滤波器轮廓（SFP）。（2）图像到图像转换网络（I2I）：I2I网络采用U-Net架构，用于将生成图像从源频域转换到目标频域。（3）对比学习损失函数：对比学习损失函数基于图像对的相似性和差异性，通过最大化相似图像的特征表示之间的相关性，同时最小化不同图像的特征表示之间的相关性，来优化I2I网络。（4）频谱滤波器轮廓（SFP）：SFP是一个预先训练的频谱滤波器集合，用于指导I2I网络学习目标频域的特征分布。（5）STIG训练过程：STIG框架的训练过程包括两个阶段：预训练阶段和微调阶段。在预训练阶段，I2I网络使用对比学习损失函数和SFP进行训练。在微调阶段，I2I网络使用生成图像和真实图像之间的对抗性损失函数进行微调。</p><ol><li>结论：（1）：本文提出了 STIG 框架，该框架通过直接操作生成图像的频率分量，在频域中减少生成图像的光谱差异，从而提高生成性能。（2）：创新点：STIG 框架在频域中直接操作生成图像，以减少生成图像与真实图像之间的光谱差异。STIG 框架采用对比学习损失函数和频谱滤波器轮廓，优化图像到图像转换网络的训练过程。STIG 框架可以有效地提高生成对抗网络和扩散模型的生成性能。性能：STIG 框架在八个假图像基准上均优于其他前沿方法，在 FID 和光谱对数频率距离方面有显著下降。STIG 框架通过减少光谱异常来提高图像质量。STIG 框架处理虚假光谱时，基于频率的深度伪造检测器更容易混淆。工作量：STIG 框架的训练过程包括预训练阶段和微调阶段。预训练阶段需要对图像到图像转换网络使用对比学习损失函数和频谱滤波器轮廓进行训练。微调阶段需要对图像到图像转换网络使用生成图像和真实图像之间的对抗性损失函数进行微调。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-59b9082b16c536f6e3dc82d3eedb0929.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc9ad99c3613618bd289ca6d732974f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed21a9f11c14097979acb60a01fc0faa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ee45629a830dd20e3e691c354e6c5761.jpg" align="middle"><img src="https://picx.zhimg.com/v2-82896ffced53d8bc120b544471040628.jpg" align="middle"></details><h2 id="Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport"><a href="#Improving-Diffusion-Based-Generative-Models-via-Approximated-Optimal-Transport" class="headerlink" title="Improving Diffusion-Based Generative Models via Approximated Optimal   Transport"></a>Improving Diffusion-Based Generative Models via Approximated Optimal   Transport</h2><p><strong>Authors:Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon</strong></p><p>We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for diffusion-based generative models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of diffusion models to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing the performance of diffusion models. </p><p><a href="http://arxiv.org/abs/2403.05069v1">PDF</a> </p><p><strong>摘要</strong><br>通过近似最优传输（AOT）技术提升扩散模型生成效果，降低采样误差，提升图像质量。</p><p><strong>要点</strong></p><ul><li>提出近似最优传输（AOT）技术，改进扩散模型训练。</li><li>AOT 技术将最优传输整合到扩散模型训练，提升去噪输出准确性。</li><li>优化后的扩散轨迹曲率降低，采样截断误差减小。</li><li>采用 AOT 训练，图像质量提升，采样步骤减少。</li><li>无条件生成中，27 次诺福克序列（NFE），FID 得分达到 1.88；29 次 NFE，FID 得分达到 1.73。</li><li>条件生成中，29 次 NFE，FID 得分达到 1.68；指导判别器训练，FID 得分达到 1.58。</li><li>AOT 技术有效提升了扩散模型性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通过近似最优传输改进基于扩散的生成模型</li><li>作者：Daegyu Kim、Jooyoung Choi、Chaehun Shin、Uiwon Hwang、Sungroh Yoon</li><li>第一作者单位：首尔国立大学数据科学与人工智能实验室</li><li>关键词：生成模型、扩散模型、最优传输</li><li>论文链接：https://arxiv.org/abs/2403.05069   Github 代码链接：无</li><li>摘要：（1）研究背景：扩散模型是一种通过逐渐去噪来合成图像的生成模型。近年来，扩散模型在图像生成方面取得了显著进展，但仍存在 ODE 轨迹曲率高的问题，这会影响图像质量和采样效率。</li></ol><p>（2）过去方法及问题：FlowMatching 等方法提出了使用最优传输来解决曲率问题，但由于扩散模型的结构，直接应用这些方法存在计算效率低的问题。</p><p>（3）本文提出的研究方法：本文提出了一种近似最优传输（AOT）训练技术，将最优传输近似并整合到扩散模型训练过程中，从而降低 ODE 轨迹的曲率和截断误差。</p><p>（4）方法在任务上的表现及性能：在 CIFAR-10 图像无条件和条件生成任务上，与基线研究和 EDM 相比，本文方法在图像质量和 NFE（函数评估次数）方面均取得了更好的性能。具体而言，在无条件生成中，本文方法以 27 NFE 实现了 1.88 的 FID 得分，以 29 NFE 实现了 1.73 的 FID 得分；在条件生成中，以 29 NFE 实现了 1.68 的 FID 得分，以 29 NFE 实现了 1.58 的 FID 得分。这些结果表明，AOT 技术可以有效提升扩散模型的性能。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本研究工作通过提出近似最优传输（AOT）技术，有效降低了扩散模型 ODE 轨迹的曲率和截断误差，从而提升了图像生成质量和采样效率。（2）：创新点：</li><li>提出近似最优传输（AOT）技术，将最优传输近似并整合到扩散模型训练过程中，降低了 ODE 轨迹的曲率和截断误差。</li><li>将 AOT 技术成功集成到 Discriminator Guidance（DG）框架中，展示了其在更广泛应用中的多功能性和潜力。性能：</li><li>在 CIFAR-10 图像无条件和条件生成任务上，与基线研究和 EDM 相比，本文方法在图像质量和 NFE（函数评估次数）方面均取得了更好的性能。</li><li>在无条件生成中，本文方法以 27NFE 实现了 1.88 的 FID 得分，以 29NFE 实现了 1.73 的 FID 得分；在条件生成中，以 29NFE 实现了 1.68 的 FID 得分，以 29NFE 实现了 1.58 的 FID 得分。工作量：</li><li>与 EDM 相比，本文方法在训练成本上略有增加（2% 到 15%）。</li><li>本方法需要算法改进，以扩展其在具有挑战性的条件生成（例如文本指导生成）中的适用性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-8b3484bb01610ca257b110266a789659.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a1905f26c3dd85ac5906dbc02f95a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-06436ae944972e738965038412bab51a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-092c4ba972936da93fe5ca9a1e0c861e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3004bc0c97615bac6076ff6a3cd11e53.jpg" align="middle"><img src="https://picx.zhimg.com/v2-63b88ad2349cca16dbda28634bc2b6d1.jpg" align="middle"></details><h2 id="XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution"><a href="#XPSR-Cross-modal-Priors-for-Diffusion-based-Image-Super-Resolution" class="headerlink" title="XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution"></a>XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</h2><p><strong>Authors:Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</strong></p><p>Diffusion-based methods, endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a \textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the diffusion model, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a \textit{Semantic-Fusion Attention} is raised. To distill semantic-preserved information instead of undesired degradations, a \textit{Degradation-Free Constraint} is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes will be released at \url{<a href="https://github.com/qyp2000/XPSR}">https://github.com/qyp2000/XPSR}</a>. </p><p><a href="http://arxiv.org/abs/2403.05049v1">PDF</a> 19 pages, 7 figures</p><p><strong>Summary</strong><br>基于扩散模型，结合多模态大语言模型和语义融合策略，提出一种图像超分辨率框架XPSR，能够生成高保真和逼真的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成式先验提升图像超分辨率性能。</li><li>多模态大语言模型提供精确语义信息。</li><li>语义融合注意力促进跨模态先验融合。</li><li>无退化约束提取语义内容，而非退化信息。</li><li>XPSR在合成和真实数据集上生成高质量超分辨率图像。</li><li>XPSR代码将于<a href="https://github.com/qyp2000/XPSR发布。">https://github.com/qyp2000/XPSR发布。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：XPSR：用于基于扩散的图像超分辨率的跨模态先验</li><li>作者：曲云鹏、袁坤、赵凯、谢启之、郝金华、孙明、周超</li><li>单位：清华大学</li><li>关键词：图像超分辨率、图像修复、扩散模型、多模态大语言模型</li><li>论文链接：https://arxiv.org/abs/2403.05049Github代码链接：None</li><li><p>摘要：（1）研究背景：基于扩散的图像超分辨率（ISR）方法因其强大的生成先验而受到越来越多的关注。然而，由于低分辨率（LR）图像通常会遭受严重的退化，因此对于ISR模型来说，感知语义和退化信息具有挑战性，导致恢复的图像内容不正确或出现不真实的伪影。（2）以往方法及其问题：本文的动机是解决上述问题。以往方法主要使用生成对抗网络（GAN）进行图像超分辨率，但GAN在生成逼真纹理方面存在困难，并且存在合成训练数据和真实世界测试数据之间的域差距问题。（3）提出的研究方法：为了解决这些问题，本文提出了一个跨模态先验超分辨率（XPSR）框架。在XPSR中，利用先进的多模态大语言模型（MLLM）为扩散模型获取准确和全面的语义条件。为了促进跨模态先验的更好融合，提出了一种语义融合注意力机制。为了提取语义保留的信息而不是不需要的退化，在LR及其高分辨率（HR）对应图像之间附加了一个无退化约束。（4）方法在任务和性能上的表现：定量和定性结果表明，XPSR能够跨合成和真实世界数据集生成高保真和高逼真的图像。这些结果支持了本文提出的方法可以有效解决图像超分辨率中的挑战。</p></li><li><p>方法：(1) 采用大语言模型 LLaVA 获取图像的语义先验，包括高层语义和低层语义；(2) 使用语义融合注意力机制，将语义先验与 T2I 模型生成的先验有效融合；(3) 添加无退化约束，从 LR 图像中提取语义保留但与退化无关的信息。</p></li><li><p>结论：（1）意义：本文提出的 XPSR 框架解决了基于扩散的图像超分辨率模型在准确恢复语义细节方面的难题，为图像超分辨率领域提供了新的思路。（2）优缺点总结：创新点：</p></li><li>提出跨模态先验概念，利用多模态大语言模型为扩散模型提供准确全面的语义条件。</li><li>设计语义融合注意力机制，有效融合语义先验和 T2I 模型生成的先验。</li><li>引入无退化约束，从低分辨率图像中提取语义保留但与退化无关的信息。性能：</li><li>定量和定性结果表明，XPSR 能够跨合成和真实世界数据集生成高保真和高逼真的图像。</li><li>与其他先进方法相比，XPSR 在各种评估指标上取得了有竞争力的性能。工作量：</li><li>XPSR 框架的实现需要一定的工作量，包括训练多模态大语言模型和扩散模型。</li><li>此外，语义融合注意力机制和无退化约束的实现也需要额外的开发工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7216c617badf932e3f8d18daf0977b1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c7194197140a421dc8eb74d3c744901.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca3923ee7424c689775b0bb281aa1184.jpg" align="middle"></details><h2 id="DiffClass-Diffusion-Based-Class-Incremental-Learning"><a href="#DiffClass-Diffusion-Based-Class-Incremental-Learning" class="headerlink" title="DiffClass: Diffusion-Based Class Incremental Learning"></a>DiffClass: Diffusion-Based Class Incremental Learning</h2><p><strong>Authors:Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, Yanzhi WAng</strong></p><p>Class Incremental Learning (CIL) is challenging due to catastrophic forgetting. On top of that, Exemplar-free Class Incremental Learning is even more challenging due to forbidden access to previous task data. Recent exemplar-free CIL methods attempt to mitigate catastrophic forgetting by synthesizing previous task data. However, they fail to overcome the catastrophic forgetting due to the inability to deal with the significant domain gap between real and synthetic data. To overcome these issues, we propose a novel exemplar-free CIL method. Our method adopts multi-distribution matching (MDM) diffusion models to unify quality and bridge domain gaps among all domains of training data. Moreover, our approach integrates selective synthetic image augmentation (SSIA) to expand the distribution of the training data, thereby improving the model’s plasticity and reinforcing the performance of our method’s ultimate component, multi-domain adaptation (MDA). With the proposed integrations, our method then reformulates exemplar-free CIL into a multi-domain adaptation problem to implicitly address the domain gap problem to enhance model stability during incremental training. Extensive experiments on benchmark class incremental datasets and settings demonstrate that our method excels previous exemplar-free CIL methods and achieves state-of-the-art performance. </p><p><a href="http://arxiv.org/abs/2403.05016v1">PDF</a> Preprint</p><p><strong>Summary</strong><br>多分布匹配扩散模型在无例可循的类增量学习中解决灾难性遗忘和领域差异问题，通过多域适应隐式解决领域差异问题，提高模型稳定性。</p><p><strong>Key Takeaways</strong></p><ul><li>类增量学习面临灾难性遗忘和无例可循的挑战。</li><li>无例可循的类增量学习方法通过合成先前任务数据来减轻灾难性遗忘。</li><li>此类方法由于无法处理真实数据和合成数据之间的显着领域差异而无法克服灾难性遗忘。</li><li>提出一种新的无例可循的类增量学习方法，采用多分布匹配扩散模型统一质量和弥合所有训练数据域之间的领域差异。</li><li>该方法集成了选择性合成图像增强，以扩展训练数据的分布。</li><li>这种方法将无例可循的类增量学习重新表述为多域适应问题，以隐式解决领域差异问题，提高模型在增量训练过程中的稳定性。</li><li>广泛的实验表明，该方法优于先前的无例可循的类增量学习方法，并实现了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散的类增量学习</li><li>作者：孟子聪，张杰，杨昌迪，詹政，赵普，王延之</li><li>东北大学</li><li>ClassIncrementalLearning，ExemplarFree，DiffusionModel</li><li>论文链接：https://arxiv.org/abs/2403.05016   Github代码链接：无</li><li><p>摘要：（1）研究背景：类增量学习（CIL）因灾难性遗忘而极具挑战性。此外，由于无法访问先前任务的数据，无示例 CIL 更是难上加难。（2）过去方法及问题：最近的无示例 CIL 方法尝试通过合成先前任务数据来缓解灾难性遗忘。然而，它们由于无法处理真实数据和合成数据之间的巨大域差距而无法克服灾难性遗忘。（3）提出的研究方法：为了克服这些问题，本文提出了一种新颖的无示例 CIL 方法。该方法采用多分布匹配 (MDM) 扩散模型来对齐合成数据的质量，并弥合训练数据所有域之间的域差距。此外，本文的方法集成了选择性合成图像增强 (SSIA) 来扩展训练数据的分布，从而提高模型的可塑性并增强多域自适应 (MDA) 技术的性能。通过提出的集成，本文的方法将无示例 CIL 重新表述为多域自适应问题，以隐式解决域差距问题并增强模型在增量训练期间的稳定性。（4）方法性能：在基准 CIL 数据集和设置上的大量实验表明，本文的方法优于之前的无示例 CIL 方法，具有非边际改进，并实现了最先进的性能。</p></li><li><p>方法：(1) 多分布匹配扩散模型精调：使用 LoRA 精调多分布匹配扩散模型，对齐合成数据的质量，缩小训练数据所有域之间的域差距。(2) 选择性合成图像增强：通过选择性合成图像增强扩展训练数据的分布，提高模型的可塑性，增强多域自适应技术的性能。(3) 多域自适应：采用多域自适应训练方法，将无示例 CIL 重新表述为多域自适应问题，隐式解决域差距问题，增强模型在增量训练期间的稳定性。</p></li></ol><p>8.结论：（1）：本文提出了一种基于扩散的新颖无示例类增量学习方法，该方法通过多分布匹配扩散模型和选择性合成图像增强有效解决了灾难性遗忘问题，并通过多域自适应技术增强了模型的稳定性和可塑性，在无示例类增量学习任务上取得了最先进的性能。（2）：创新点：* 基于多分布匹配扩散模型，显式弥合合成数据和真实数据之间的域差距。* 采用选择性合成图像增强，扩展训练数据分布，提高模型的可塑性。* 将无示例类增量学习重新表述为多域自适应问题，隐式解决域差距问题，增强模型在增量训练期间的稳定性。性能：* 在 CIFAR100 和 ImageNet100 基准数据集上，在各种无示例类增量学习设置中均取得了最先进的性能。* 消融研究证明了本文方法中每个组件在无示例类增量学习中的重要性。工作量：* 每个增量任务的训练时间相对较长，尤其是使用 LoRA 微调生成模型的时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3071368b15837785fc8226279a7a69f4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-022f350905045d5945b926c68a304727.jpg" align="middle"><img src="https://picx.zhimg.com/v2-191fbfc51055a8bc7b2acc064efa3416.jpg" align="middle"><img src="https://picx.zhimg.com/v2-70ca4a001e09124d997a32d6f30da7f0.jpg" align="middle"></details>## StereoDiffusion: Training-Free Stereo Image Generation Using Latent   Diffusion Models**Authors:Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand Bigdeli**The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations. [PDF](http://arxiv.org/abs/2403.04965v1) **Summary**立体扩散：无训练、简单易用，无缝集成原有 Stable Diffusion 模型，生成立体图像对。**Key Takeaways**- StereoDiffusion 无需训练，使用方便。- 与原始 Stable Diffusion 模型无缝集成。- 生成立体图像对时无需微调模型权重或图像后处理。- 利用原始输入生成左图像并估计其视差图。- 使用立体像素位移操作生成右图像的潜变量。- 使用对称像素位移掩码去噪和自注意力层修改方法。- 保持立体生成过程中图像质量的高标准。- 在各种定量评估中取得最先进的分数。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：立体扩散：基于潜在扩散模型的无训练立体图像生成</li><li>作者：Lezhong Wang、Jeppe Revall Frisvad、Mark Bo Jensen、Siavash Arjomand Bigdeli</li><li>隶属单位：丹麦技术大学应用数学与计算机科学系</li><li>关键词：XR、深度图像/视频合成、图像编辑、人工智能、修复、Stable Diffusion</li><li>论文链接：https://arxiv.org/abs/2403.04965   Github 代码链接：无</li><li><p>摘要：   (1)：随着制造商推出更多 XR 设备，对立体图像的需求不断增加。为了满足这一需求，我们引入了立体扩散，这是一种与传统修复管道不同、无需训练、使用极其简单且可与原始 Stable Diffusion 模型无缝集成的技术。我们的方法修改了潜在变量，提供了一种端到端的轻量级功能，用于快速生成立体图像对，而无需微调模型权重或对图像进行任何后处理。我们使用原始输入生成左侧图像并估计其视差图，通过立体像素位移操作生成右侧图像的潜在向量，并辅以对称像素位移掩码去噪和自注意力层修改方法，将右侧图像与左侧图像对齐。此外，我们提出的方法在整个立体生成过程中保持了较高的图像质量标准，在各种定量评估中取得了最先进的得分。   (2)：过去的方法主要依赖于图像修复管道，该管道需要额外的模型进行后处理以生成立体图像。这些方法通常需要对模型权重进行微调，并且生成过程复杂且耗时。我们的方法通过修改 Stable Diffusion 模型的潜在变量来直接生成立体图像对，无需额外的模型或后处理。这种方法简单有效，可以快速生成高质量的立体图像。   (3)：我们提出的方法是一种端到端的立体图像生成方法，它修改了 Stable Diffusion 模型的潜在变量。具体来说，我们使用原始输入生成左侧图像并估计其视差图。然后，我们通过立体像素位移操作生成右侧图像的潜在向量。为了对齐右侧图像和左侧图像，我们使用了对称像素位移掩码去噪和自注意力层修改方法。   (4)：我们在立体图像生成任务上评估了我们提出的方法。我们的方法在各种定量评估中取得了最先进的得分，包括 PSNR、SSIM 和 LPIPS。这些结果表明，我们的方法可以生成高质量的立体图像，并且可以很好地支持我们的目标，即快速生成无需训练的立体图像对。</p></li><li><p>方法：（1）使用原始输入生成左侧图像并估计其视差图；（2）通过立体像素位移操作生成右侧图像的潜在向量；（3）使用对称像素位移掩码去噪和自注意力层修改方法对齐右侧图像和左侧图像。</p></li><li><p>结论：（1）：立体扩散：基于潜在扩散模型的无训练立体图像生成，这项工作提出了一种通过修改潜在扩散模型的潜在变量来生成立体图像对的新方法。该方法无需额外的模型或后处理，可以快速生成高质量的立体图像。（2）：创新点：</p></li><li>无需训练：该方法无需对模型权重进行微调，直接生成立体图像对，简化了生成过程。</li><li>端到端：该方法修改潜在变量，提供了一种端到端的轻量级功能，用于快速生成立体图像对。</li><li>与原始StableDiffusion模型无缝集成：该方法可以与原始StableDiffusion模型无缝集成，无需对模型进行任何修改。性能：</li><li>定量评估：该方法在KITTI和Middlebury数据集上取得了最先进的得分，表明其可以生成高质量的立体图像。工作量：</li><li>计算成本：该方法的计算成本较低，可以快速生成立体图像对。</li><li>内存占用：该方法的内存占用较小，可以在各种设备上运行。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2042e22706397759569cb6c0ac2c19fc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1a050df593611d8551bcd2b7e676c281.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4970b55916ca916d6716d8304932590e.jpg" align="middle"></details><h2 id="AFreeCA-Annotation-Free-Counting-for-All"><a href="#AFreeCA-Annotation-Free-Counting-for-All" class="headerlink" title="AFreeCA: Annotation-Free Counting for All"></a>AFreeCA: Annotation-Free Counting for All</h2><p><strong>Authors:Adriano D’Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</strong></p><p>Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable \textit{sorting} signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms other unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code to be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2403.04943v1">PDF</a> </p><p><strong>Summary</strong><br>使用文本到图像扩散模型 (LDM) 自动生成分类数据，然后通过无监督学习和密度分类指导方法对数据进行处理，从而实现类别无关的无监督对象计数。</p><p><strong>Key Takeaways</strong></p><ul><li>LDMs 能够提供图像添加和删除对象的可靠分类信号。</li><li>利用 LDM 生成的分类数据，可以无监督地学习与对象相关的特征。</li><li>通过计数数据对特征进行精炼和锚定。</li><li>密度分类器引导的方法可将图像划分为包含可被可靠计数的对象的区域。</li><li>该方法可生成任何类型对象的计数数据，并能以无监督的方式进行计数。</li><li>相对于其他无监督和少样本替代方法具有较好的性能。</li><li>无需特定对象类别即可生成计数数据。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：无标注计数：密度分类器引导分区</li><li>作者：Lu Qi, Minghao Chen, Junwei Han, Yu Liu, Xiang Bai, Xiaogang Wang</li><li>单位：无</li><li>关键词：ObjectCounting·SyntheticData·Annotation-Free</li><li>论文链接：https://arxiv.org/abs/2302.06673   Github 链接：无</li><li>摘要：   (1) 研究背景：目标计数方法通常依赖于人工标注数据集，这限制了网络针对特定类别（如人或企鹅）计数目标的通用性，并且对不同类别目标的计数仍然是一个挑战。   (2) 过去方法：无监督、少样本和零样本方法旨在使用包含不同类别的大型人工标注数据集来创建适用于任何类别的通用计数网络。少样本方法依赖于从目标图像中采样的样本例来定义目标类别，而零样本方法使用文本提示。这些方法依赖于广泛的标注数据集，但   (3) 本文方法：利用了文本到图像的潜在扩散模型（LDM）。LDM 难以仅基于文本提示创建具有精确数量目标的图像，但可以通过添加和移除图像中的目标来提供可靠的排序信号。利用这些数据，本文首先引入了一种无监督排序方法来学习目标相关特征，随后使用 LDM 生成的计数数据对这些特征进行精炼和锚定以用于计数目的。此外，本文还提出了一种密度分类器引导方法，将图像划分为包含可被可靠计数的目标的块。因此，本文可以为任何类型的目标生成计数数据并以无监督的方式对其进行计数。   (4) 性能：本文方法优于其他无监督和少样本替代方法，并且不受特定目标类别的限制，这些类别有可用的计数数据。</li></ol><p>7.方法：(1)生成合成排序数据，通过添加和移除图像中的目标，使用潜在扩散模型（LDM）对图像进行排序；(2)预训练排序网络，使用排序损失和关系损失，对图像特征进行排序；(3)从合成数据学习计数，使用预训练的排序网络，通过微调线性层，将特征锚定到实际计数值；(4)人群密度分类，使用 Stable Diffusion 生成合成数据，对人群密度进行分类；(5)密度分类器引导分区（DCGP），根据估计的密度对图像进行分区，将图像处理为更小的补丁。</p><ol><li>结论：（1）：本文提出了一种无监督的目标计数方法，该方法利用了文本到图像的潜在扩散模型（LDM）生成的合成数据。该方法通过排序和锚定学习目标相关特征，并使用密度分类器引导分区（DCGP）将图像划分为包含可被可靠计数的目标的块。该方法不受特定目标类别的限制，并且优于其他无监督和少样本替代方法。（2）：创新点：</li><li>利用LDM生成合成排序数据和计数数据，无需人工标注。</li><li>提出了一种无监督排序方法，学习目标相关特征。</li><li>提出了一种密度分类器引导分区（DCGP）方法，将图像划分为包含可被可靠计数的目标的块。性能：</li><li>在PASCAL VOC、COCO和Cityscapes数据集上，该方法优于其他无监督和少样本替代方法。</li><li>该方法不受特定目标类别的限制，可以为任何类型的目标生成计数数据并以无监督的方式对其进行计数。工作量：</li><li>该方法需要生成合成排序数据和计数数据，这可能需要大量的计算资源。</li><li>该方法需要预训练排序网络和微调线性层，这可能需要大量的时间和精力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0bdfaf4b65221e3f6287dfe2ed850459.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7e6e2c7b151a6f679f9aa91c763c21aa.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25087217d0ca2a3290d33e79013e2984.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04536de3c0849a068b94d559fbfb1068.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e7e2084d668b0f9c9e859eecaa8550c.jpg" align="middle"></details><h2 id="An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control"><a href="#An-Item-is-Worth-a-Prompt-Versatile-Image-Editing-with-Disentangled-Control" class="headerlink" title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control"></a>An Item is Worth a Prompt: Versatile Image Editing with Disentangled   Control</h2><p><strong>Authors:Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</strong></p><p>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations. </p><p><a href="http://arxiv.org/abs/2403.04880v1">PDF</a> </p><p><strong>Summary</strong><br>文本提示编辑实现了图像编辑，但由于扩散模型的预训练方式，直接编辑提示中的文字会导致生成完全不同的图像，违背了图像编辑的要求。</p><p><strong>Key Takeaways</strong></p><ul><li>提出文本提示编辑方法 D-Edit。</li><li>将图像提示交互分解为多个项目提示交互，每个项目链接到一个特殊学习提示。</li><li>采用两步优化构建项目提示关联。</li><li>可进行多种图像编辑，包括基于图像、基于文本、基于掩码的编辑和项目移除。</li><li>可以在单个统一框架中实现所有类型的编辑应用程序。</li><li>D-Edit 是第一个（1）通过掩码编辑实现项目编辑，（2）结合图像和基于文本的编辑的框架。</li><li>通过定性和定量评估，展示了各种图像编辑结果的质量和多功能性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：An Item is Worth a Prompt：多功能的可控图像编辑</li><li>作者：Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</li><li>第一作者单位：耶鲁大学</li><li>关键词：图像编辑、文本到图像扩散模型、可控提示</li><li>论文链接：https://arxiv.org/abs/2403.04880</li><li><p>摘要：(1)：基于文本到图像扩散模型在图像合成中的成功，图像编辑成为一种重要的应用程序，它让人们能够与 AI 生成的内容进行交互。在各种编辑方法中，提示空间编辑因其控制语义的能力和简单性而受到更多关注。然而，由于扩散模型通常在描述性文本标题上进行预训练，因此在文本提示中直接编辑单词通常会导致完全不同的生成图像，违反了图像编辑的要求。另一方面，现有的编辑方法通常考虑引入空间掩码来保留未编辑区域的身份，而扩散模型通常会忽略这些区域，因此导致不协调的编辑结果。(2)：针对这两个挑战，本文提出将综合图像提示交互分解为几个项目提示交互，每个项目都链接到一个特殊学习的提示。由此产生的框架名为 D-Edit，它基于预训练的扩散模型，并采用交叉注意层进行解耦，并采用两步优化来构建项目提示关联。通过操作相应的提示，可以将多功能图像编辑应用于特定项目。本文展示了四种类型的编辑操作（包括基于图像、基于文本、基于掩码的编辑和项目移除）的最新结果，涵盖了大多数类型的编辑应用程序，所有这些都采用一个统一的框架。值得注意的是，D-Edit 是第一个可以 (1) 通过掩码编辑实现项目编辑，以及 (2) 结合图像和基于文本的编辑的框架。通过定性和定量评估，本文展示了针对各种图像集合的编辑结果的质量和多功能性。(3)：本文提出两种关键技术，旨在增强上述标准：(1) 解耦控制：为了保留原始图像的信息，目标项目的编辑应尽量不影响周围项目。从提示到图像的控制过程也应该解耦，确保修改项目提示不会破坏其余项目的控制流。注意到文本到图像交互发生在基于注意力的扩散模型的交叉注意层中，本文提出分组交叉注意来解耦提示到项目的控制流。(2) 唯一项目提示：为了提高与指导的一致性（例如参考图像），每个项目都应该与一个控制其生成的唯一提示相关联。这些提示通常由特殊标记或罕见单词组成。像 Dreambooth 和 Textual Inversion 这样的图像个性化现有工作已经通过用唯一提示表示新主题来广泛研究了这个概念，随后将其用于图像生成。与它们相比，本文使用独立提示来定义不同的项目，而不是整个图像。在理想情况下，如果图像中的每个项目及其所有细节都可以用一个独特的英文单词准确描述，那么用户可以通过简单地将当前单词更改为目标单词来实现所有类型的编辑。(4)：本文充分利用提示唯一性和解耦控制的潜力，介绍了一个多功能图像编辑框架，称为 Disentangled-Edit (D-Edit)，这是一个统一的框架，支持在项目级别进行大多数类型的图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。具体来说，如图 1 所示，从目标图像开始，本文最初将其细分为多个可编辑项目（在以下内容中，本文还将背景和未分割区域称为项目），每个项目都与一个包含几个新标记的提示相关联。提示和项目之间的关联是通过两步微调过程建立的，其中包括优化文本编码器嵌入矩阵和 UNet 模型权重。引入分组交叉注意来解耦提示到项目的交互，通过隔离注意计算和值更新。然后，可以通过更改提示、项目及其之间的关联来实现各种类型的图像编辑。然后，用户可以通过更改相应的提示、掩码和项目，并调整它们之间的关联来实现各种类型的图像编辑。这种灵活性允许广泛的创造可能性和对编辑过程的精确控制。本文在四个图像编辑任务上展示了本文框架的多功能性和性能，如上所述，使用稳定扩散和稳定扩散 XL。本文总结本文的贡献如下：• 本文提出建立项目提示关联以实现项目编辑。• 本文引入分组交叉注意来解耦扩散模型中的控制流。• 本文提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。</p></li><li><p>Methods：（1）：本文提出建立项目提示关联以实现项目编辑；（2）：本文引入分组交叉注意来解耦扩散模型中的控制流；（3）：本文提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。</p></li><li><p>结论：（1）：本文提出 D-Edit，这是一个基于扩散模型的多功能图像编辑框架。D-Edit 将给定图像分割为多个项目，每个项目都被分配一个提示来控制其在提示空间中的表示。图像提示交叉注意力被分解为一组项目提示交互。每个提示通过孤立的交叉注意力被约束为仅与它控制的项目进行交互，从而解耦了交叉注意力控制管道。（2）：创新点：</p></li><li>提出建立项目提示关联以实现项目编辑。</li><li>引入分组交叉注意力来解耦扩散模型中的控制流。</li><li>提出 D-Edit 作为一种多功能框架，支持在项目级别进行各种图像编辑操作，包括基于文本、基于图像、基于掩码的编辑和项目移除。D-Edit 是第一个可以进行基于掩码的编辑以及同时执行基于文本和图像的编辑的框架。性能：</li><li>在四个图像编辑任务上展示了本文框架的多功能性和性能，如上所述，使用稳定扩散和稳定扩散 XL。工作量：</li><li>提出了一种两步微调过程来建立提示和项目之间的关联，包括优化文本编码器嵌入矩阵和 UNet 模型权重。</li><li>引入分组交叉注意来解耦提示到项目的交互，通过隔离注意计算和值更新。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-197c83cdebd23bdb14b8fb0a7b729711.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1f65d83dbc51dc28ff510d4cc3b578f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-325295a9d8fc632369762af9b221cc1f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a471c060f1ae7bcb9959f797a6fb643a.jpg" align="middle"></details><h2 id="Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation"><a href="#Pix2Gif-Motion-Guided-Diffusion-for-GIF-Generation" class="headerlink" title="Pix2Gif: Motion-Guided Diffusion for GIF Generation"></a>Pix2Gif: Motion-Guided Diffusion for GIF Generation</h2><p><strong>Authors:Hitesh Kandala, Jianfeng Gao, Jianwei Yang</strong></p><p>We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model — it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href="https://hiteshk03.github.io/Pix2Gif/">https://hiteshk03.github.io/Pix2Gif/</a>. </p><p><a href="http://arxiv.org/abs/2403.04634v2">PDF</a> </p><p><strong>Summary</strong><br>图像到GIF生成的新式运动引导扩散模型，采用文本和运动幅度提示指导的图像翻译方法，并提出新的运动引导变形模块以空间转换特征，从而确保模型遵循运动指导。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 Pix2Gif，一种运动引导的扩散模型，用于图像到 GIF（视频）生成。</li><li>以图像翻译问题为基础，由文本和运动幅度提示指导。</li><li>设计新的运动引导变形模块，根据两种提示对源图像特征进行空间转换。</li><li>引入感知损失，确保转换后的特征图与目标图像空间一致。</li><li>精心整理数据，从 TGIF 视频字幕数据集中提取连贯的图像帧。</li><li>采用零样本方式将模型应用于多个视频数据集。</li><li>定性和定量实验验证了模型的有效性，不仅能捕捉文本的语义提示，还能捕捉运动引导的空间提示。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Pix2Gif：基于运动指导的图像转 GIF（视频）生成</li><li>作者：Hitesh K. Agrawal、Yuke Zhu、Jonathan T. Barron、Phillip Isola、 Alexei A. Efros</li><li>隶属关系：伯克利加州大学</li><li>关键词：图像到视频生成、运动引导、扩散模型、图像编辑</li><li>论文链接：https://arxiv.org/pdf/2302.04208.pdf，Github 代码链接：None</li><li>摘要：（1）研究背景：图像到 GIF（视频）生成是计算机视觉领域中的一个具有挑战性的任务，它需要模型同时理解文本和运动提示，并生成与提示相一致且内容连贯的视频。（2）过去的方法：现有的方法通常使用文本提示来指导图像生成，但它们在处理运动信息时存在局限性。直接将运动输入作为文本提示可能会导致模型对单个提示词给予过多的关注，从而忽略其他重要的运动信息。（3）研究方法：本文提出了一种新的运动引导扩散模型 Pix2Gif，该模型通过引入一个运动引导变形模块来解决上述问题。该模块将运动信息嵌入到图像特征中，指导模型在生成图像时遵循指定的运动轨迹。此外，本文还引入了一个感知损失，以确保变形后的特征图与目标图像保持在同一语义空间内，从而保证内容的一致性和连贯性。（4）方法性能：在 TGIF 视频字幕数据集上进行的广泛定性和定量实验表明，Pix2Gif 模型能够有效地捕捉文本和运动提示中的语义和空间信息，并生成高质量的图像到 GIF（视频）结果。实验结果支持了本文提出的方法的有效性。</li></ol><p>7.Methods：(1): Pix2Gif模型在生成图像时，将运动信息嵌入图像特征中，指导模型遵循指定的运动轨迹。(2): Pix2Gif模型引入了一个感知损失，以确保变形后的特征图与目标图像保持在同一语义空间内，从而保证内容的一致性和连贯性。(3): Pix2Gif模型在TGIF视频字幕数据集上进行的实验表明，该模型能够有效地捕捉文本和运动提示中的语义和空间信息，并生成高质量的图像到GIF（视频）结果。</p><ol><li>结论：（1）：Pix2Gif模型在图像到GIF（视频）生成任务中取得了显著进展，提出了一种创新性的运动引导变形模块，有效地将文本和运动信息结合起来，生成内容连贯、时间一致的高质量结果。（2）：创新点：</li><li>提出了一种新的运动引导变形模块，将运动信息嵌入图像特征中，指导模型遵循指定的运动轨迹，保证了生成的图像序列在时间上的连贯性。</li><li>引入了感知损失，确保变形后的特征图与目标图像保持在同一语义空间内，保证了内容的一致性和连贯性。性能：</li><li>在TGIF视频字幕数据集上进行的实验表明，Pix2Gif模型在捕捉文本和运动提示中的语义和空间信息方面表现出色，生成的图像到GIF（视频）结果质量较高。</li><li>与现有最先进的方法相比，Pix2Gif模型在生成时间一致的GIF方面表现出更好的效果。工作量：</li><li>Pix2Gif模型的训练过程需要大量的计算资源，特别是对于大尺寸图像和长视频序列。</li><li>模型的训练和推理时间也受到图像分辨率和视频长度的影响。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-87f209086271d79f66fc2b71db813a89.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ddec3a8952939ae9c917e7b1984fb9e4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-538b38079b2f1cde247a179f7b6ab9b5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-579c5c472fca8ba1022f880a544c4526.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-03-11  VideoElevator Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Blendshape学习笔记</title>
    <link href="https://kedreamix.github.io/2024/03/11/Note/BlendShape/"/>
    <id>https://kedreamix.github.io/2024/03/11/Note/BlendShape/</id>
    <published>2024-03-11T11:42:00.000Z</published>
    <updated>2024-03-11T12:01:31.162Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Blendshape-Morph-Target动画"><a href="#Blendshape-Morph-Target动画" class="headerlink" title="Blendshape(Morph Target动画)"></a>Blendshape(Morph Target动画)</h2><p>Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</p><p>用在脸部动画制作时，blendshape可以被称之为<strong>脸部特征，表情基准，定位符</strong>等等。这里要引入一个<code>FACS</code>的概念，可以简单理解为将脸部进行合理化的分区标准。</p><blockquote><p>“表情这个东西看起来是一个无限多可能的东西，怎么能够计算expression呢？</p><p>这就带来了Blendshapes——一组组成整体表情的基准（数量可以有十几个、50个、100+、 200+，越多就越细腻)。我们可以使用这一组基准通过线性组合来计算出整体的expression，用公式来说就是  ，其中e是expression，B是一组表情基准，d是对应的系数（在这一组里面的权重），b是neutral。” </p><p>— From <a href="https://zhuanlan.zhihu.com/p/78174706">https://zhuanlan.zhihu.com/p/78174706</a></p></blockquote><h2 id="BlendShape系数介绍"><a href="#BlendShape系数介绍" class="headerlink" title="BlendShape系数介绍"></a>BlendShape系数介绍</h2><p>在ARKit中，对表情特征位置定义了52组运动blendshape系数(<br><a href="https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation">https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation</a> )，每个blendshape系数代表一种表情定位符，表情定位符定义了特定表情属性，如mouthSmileLeft、mouthSmileRight等，与其对应的blendshape系数则表示表情运动范围。这52组blendshape系数极其描述如下表所示。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/984d8d76878441c3a8402f788ef6e46f~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=qI8vU39X63te%2BVNdO78uBFphwK0%3D" alt="Blendshape"></p><p>每一个blendshape系数的取值范围为0～1的浮点数。以jawOpen为例，当认为用户的嘴巴完全闭紧时，返回的jawOpen系数为0。当认为用户的嘴巴张开至最大时，返回的jawOpen系数为1。</p><p><img src="https://p6-sign.toutiaoimg.com/pgc-image/2c8cbd123e00470e95500a8ae62da605~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=UHPhjWP4v96kbtfJzF97Z%2Bp3klc%3D" alt=""></p><p>在用户完全闭嘴与嘴张到最大之间的过渡状态，jawOpen会根据用户嘴张大的幅度返回一个0～1的插值。</p><p><img src="https://p3-sign.toutiaoimg.com/pgc-image/8e8d980b8d69461fb5d2efbc50e47d47~noop.image?_iz=58558&amp;from=article.pc_detail&amp;lk3s=953192f4&amp;x-expires=1710668214&amp;x-signature=sFNMeBoNY3ZFfiO%2BRSjR8uGECIw%3D" alt=""></p><h2 id="脸部动捕的使用"><a href="#脸部动捕的使用" class="headerlink" title="脸部动捕的使用"></a>脸部动捕的使用</h2><h3 id="ARKit-脸部与Vive脸部blendshape基准对比"><a href="#ARKit-脸部与Vive脸部blendshape基准对比" class="headerlink" title="ARKit 脸部与Vive脸部blendshape基准对比"></a>ARKit 脸部与Vive脸部blendshape基准对比</h3><div class="table-container"><table><thead><tr><th></th><th>ARKit（52）</th><th>Extra</th><th>VIVE（52）</th><th>Extra</th></tr></thead><tbody><tr><td>Brow</td><td>5</td><td></td><td>0</td><td></td></tr><tr><td>Eye</td><td>13</td><td></td><td>14</td><td>Eye Frown + 1</td></tr><tr><td>Cheek</td><td>3</td><td></td><td>3</td><td></td></tr><tr><td>Nose</td><td>2</td><td></td><td>0</td><td></td></tr><tr><td>Jaw</td><td>4</td><td></td><td>4</td><td></td></tr><tr><td>Mouth</td><td>24</td><td></td><td>20</td><td>O shape - 1</td></tr><tr><td>Tongue</td><td>1</td><td>Tongue + 7</td><td>11</td><td></td></tr><tr><td>Sum</td><td>52</td><td>59</td><td>52</td><td>52</td></tr></tbody></table></div><h3 id="ARKit的52个Blendshape表情基准组"><a href="#ARKit的52个Blendshape表情基准组" class="headerlink" title="ARKit的52个Blendshape表情基准组"></a>ARKit的52个Blendshape表情基准组</h3><p>可以看ARKit Face Blendshapes的照片和3D模型示例：<a href="https://arkit-face-blendshapes.com/">https://arkit-face-blendshapes.com/</a></p><div class="table-container"><table><thead><tr><th>CC3</th><th>ARKit Name 表情基准/定位符</th><th>ARKit Picture</th><th>CC3 Picture</th></tr></thead><tbody><tr><td>A01</td><td>browInnerUp</td><td><img src="https://static.wixstatic.com/media/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4cc12dd62ef8484986eebe9739f4eac9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_fc0c248f4f6f46dda26eda66865678d2~mv2.png" alt=""></td></tr><tr><td>A02</td><td>browDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18a57dc078214abea520f25ad6dfb02a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9d780933931b469991ae0d4ddf105045~mv2.png" alt=""></td></tr><tr><td>A03</td><td>browDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_105d6dd9d7c44394b96b242e6d9d580b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d9943f7163414286809edef7c3bf2de7~mv2.png" alt=""></td></tr><tr><td>A04</td><td>browOuterUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7fe8581da2540a3bd7dfc39c874dd61~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c7234589721d4ddda4e2fcb1a9e0aa97~mv2.png" alt=""></td></tr><tr><td>A05</td><td>browOuterUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1fb29f740ff74e8aabadc3769c86501a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4aefd80dd4c548669d5ba80e4da639eb~mv2.png" alt=""></td></tr><tr><td>A06</td><td>eyeLookUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_697a02a504c84d5f9e316172849bb6d0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_48e3b55ee9ca40f9aec9be8b35c403b0~mv2.png" alt=""></td></tr><tr><td>A07</td><td>eyeLookUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_84cacf1f990a4e5c874c084a1ea626b3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c879b6cca2ce4f8aa2385864c1fb9389~mv2.png" alt=""></td></tr><tr><td>A08</td><td>eyeLookDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d229ef398f3547be93a1a59563520e81~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d6c6c94e3cee43db8ae9d6f36fc1a689~mv2.png" alt=""></td></tr><tr><td>A09</td><td>eyeLookDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_67a1674b6d584344b7ea77843f72be27~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ca0dbf25d9f74085809cdcd0742ede35~mv2.png" alt=""></td></tr><tr><td>A10</td><td>eyeLookOutLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b4257aa18f754427a593064e71aa97fd~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d24fd04ef2d64db18c31b90eccd5f1a9~mv2.png" alt=""></td></tr><tr><td>A11</td><td>eyeLookInLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_03368853adeb4b8599da5451033cd809~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_03368853adeb4b8599da5451033cd809~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_049433ce862e44c4a5f96bcf0ad13bd0~mv2.png" alt=""></td></tr><tr><td>A12</td><td>eyeLookInRight</td><td><img src="https://static.wixstatic.com/media/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e67745f7867402398390ce18a9f2882~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_803074453832444d8dec710711196559~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_803074453832444d8dec710711196559~mv2.png" alt=""></td></tr><tr><td>A13</td><td>eyeLookOutRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b54a5b6f123244d98eadbded8c29a8c3~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2e7b0fed966d453fa0a8dffaabeaf769~mv2.png" alt=""></td></tr><tr><td>A14</td><td>eyeBlinkLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0b68b26a666a49da843b6a47c4579b46~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b46d50b28b5d40feba9a496b1ead4a5c~mv2.png" alt=""></td></tr><tr><td>A15</td><td>eyeBlinkRight</td><td><img src="https://static.wixstatic.com/media/64c63b_65e50badaa854262a87329394a87484c~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65e50badaa854262a87329394a87484c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9113137c91934bdbab3fb26756e84783~mv2.png" alt=""></td></tr><tr><td>A16</td><td>eyeSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7b9132e314d6404097f212401559e9c4~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0cccb71728de47e5a7f63fe9bc70bcaf~mv2.png" alt=""></td></tr><tr><td>A17</td><td>eyeSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8cc99b12de914fe882c19229ce2a91da~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8445ac0161fe400ab28591fb6b0b1f56~mv2.png" alt=""></td></tr><tr><td>A18</td><td>eyeWideLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c87ac4e4c5d4d5f9639523c82aa9d43~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c097966492c3496cabf1d84455d7144d~mv2.png" alt=""></td></tr><tr><td>A19</td><td>eyeWideRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3157fc370d064da9926027034e8220d6~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3157fc370d064da9926027034e8220d6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24dc2c84e19b436a97fd2db6044f439c~mv2.png" alt=""></td></tr><tr><td>A20</td><td>cheekPuff</td><td><img src="https://static.wixstatic.com/media/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png/v1/fill/w_252,h_172,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_de4df8062c5f47ca9cd322b75b535705~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_27548c426f1b47ae834c757417e03269~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_27548c426f1b47ae834c757417e03269~mv2.png" alt=""></td></tr><tr><td>A21</td><td>cheekSquintLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_70520c1a1c374ff3855cb8dfa7450b8b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d769bd2ef0104030818ed7a156ee2a2e~mv2.png" alt=""></td></tr><tr><td>A22</td><td>cheekSquintRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png/v1/fill/w_252,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f82d4db05764690b33001da1d138f20~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49752693e89a4293982b5e023a0e1c75~mv2.png" alt=""></td></tr><tr><td>A23</td><td>noseSneerLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png/v1/fill/w_252,h_178,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_843177402d2545d1a1f0a97e848df91c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9855bb8e50f54f638d4bc321dd3caa45~mv2.png" alt=""></td></tr><tr><td>A24</td><td>noseSneerRight</td><td><img src="https://static.wixstatic.com/media/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png/v1/fill/w_252,h_179,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8ee42dc6d8e443a0858e0c65ce56cc74~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png/v1/fill/w_241,h_217,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edd25cffbf1249d89bb0f6c5a95b76e5~mv2.png" alt=""></td></tr><tr><td>A25</td><td>jawOpen</td><td><img src="https://static.wixstatic.com/media/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png/v1/fill/w_267,h_192,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aca391d5eb744a76b18d6ced31904111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f14421d8adb1461ea32ca31bd3cac7be~mv2.png" alt=""></td></tr><tr><td>A26</td><td>jawForward</td><td><img src="https://static.wixstatic.com/media/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a199113be0f9418f8c729d9a7e7b4e49~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_df9963f3452c4ce1bd1b6829a8045112~mv2.png" alt=""></td></tr><tr><td>A27</td><td>jawLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2642a61cdd0241f9ba24339873003125~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0de7a40c68654461be74016c2e29cf02~mv2.png" alt=""></td></tr><tr><td>A28</td><td>jawRight</td><td><img src="https://static.wixstatic.com/media/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2838229bffe74a5abe7d25d9c6e398ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2f51f3993bed441b89b9b493a1f2e86b~mv2.png" alt=""></td></tr><tr><td>A29</td><td>mouthFunnel</td><td><img src="https://static.wixstatic.com/media/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2719d8d83524b52a735296f0dfbf092~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e2b4d7681dcf4b1faf34e3c5b57dd3ac~mv2.png" alt=""></td></tr><tr><td>A30</td><td>mouthPucker</td><td><img src="https://static.wixstatic.com/media/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7771a95fb2ae4afeb885b7a684e3f249~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e553a24166984303920d5ec9ce1de6d6~mv2.png" alt=""></td></tr><tr><td>A31</td><td>mouthLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d2e30cadc9b443f6993ee48d99ffb9c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76b134e6564749fcaf6e036a6dc53517~mv2.png" alt=""></td></tr><tr><td>A32</td><td>mouthRight</td><td><img src="https://static.wixstatic.com/media/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ef34b0cf15c541058052d74870f95a11~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_332e51118068490cbb932bc8b3880895~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_332e51118068490cbb932bc8b3880895~mv2.png" alt=""></td></tr><tr><td>A33</td><td>mouthRollUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c93c56f5d9e4698a86160047452fdae~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f86e02c72dcd4e27b5fafc3e7cbf5098~mv2.png" alt=""></td></tr><tr><td>A34</td><td>mouthRollLower</td><td><img src="https://static.wixstatic.com/media/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8d2d50c4784b4f4a881264f9e806b26e~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f6ce7f1df803456fb25b77533ec5c1a9~mv2.png" alt=""></td></tr><tr><td>A35</td><td>mouthShrugUpper</td><td><img src="https://static.wixstatic.com/media/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_d70a5a8102d14df6be57658f696ab28c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36feb9bc9305402e8a9e044b7f42c06e~mv2.png" alt=""></td></tr><tr><td>A36</td><td>mouthShrugLower</td><td><img src="https://static.wixstatic.com/media/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png/v1/fill/w_267,h_183,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c39b6573ab8b452c8ba9af4cfd61fa0d~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_881391abc1ff4fbabd6f7719d93179b8~mv2.png" alt=""></td></tr><tr><td>A37</td><td>mouthClose</td><td><img src="https://static.wixstatic.com/media/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png/v1/fill/w_267,h_129,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_7c1a9921e54c42c5bbad10ce2d2a2edc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8aded518da54400db938b69753b8539a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8aded518da54400db938b69753b8539a~mv2.png" alt=""></td></tr><tr><td>A38</td><td>mouthSmileLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a3cdfd578cec40a5a83931c4d0c9f8ab~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_11aa5137231b4bfe8a8908f25d8d4112~mv2.png" alt=""></td></tr><tr><td>A39</td><td>mouthSmileRight</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c7a9ddfcb945f496f4ac8aafcfd0ca~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4818df7bf47740f6bab387d0d2926a2b~mv2.png" alt=""></td></tr><tr><td>A40</td><td>mouthFrownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8e7c89a5e9514206ac3fd7152e912ef8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5a9fdcf6d246439b8d7d9dbf63fb16~mv2.png" alt=""></td></tr><tr><td>A41</td><td>mouthFrownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_019c769729a34c7c992c3bbde95adf2a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4505259aa94646278b01cd6b4e6fe32a~mv2.png" alt=""></td></tr><tr><td>A42</td><td>mouthDimpleLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b7c5c7b4fcea481ba877fab837ddda7c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_53d010f6b8b340d6a305149152fe9eb2~mv2.png" alt=""></td></tr><tr><td>A43</td><td>mouthDimpleRight</td><td><img src="https://static.wixstatic.com/media/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_268dda3d9bb14eaba63c5b123ab9002c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ea46553169c749f69dc8e47737434193~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ea46553169c749f69dc8e47737434193~mv2.png" alt=""></td></tr><tr><td>A44</td><td>mouthUpperUpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e6f82a77cd374e37b456590eb19c2d28~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c9a52ea901243218e0c9252fcd45a00~mv2.png" alt=""></td></tr><tr><td>A45</td><td>mouthUpperUpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_384bab2c926045f99f4bbef75b6975f0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8abd87f586bb4d2088673a2358a65adb~mv2.png" alt=""></td></tr><tr><td>A46</td><td>mouthLowerDownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2511f8304fbb49dab88eb09b118f88bc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_21506f6994114f1194bc69958bd3778d~mv2.png" alt=""></td></tr><tr><td>A47</td><td>mouthLowerDownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png/v1/fill/w_267,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5c300e220ef04f388b827c096ad7aae6~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9ac53c48df9e4d63b6774b91aaa4db3d~mv2.png" alt=""></td></tr><tr><td>A48</td><td>mouthPressLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_478c881ace1744ff825202484b212c17~mv2.png/v1/fill/w_267,h_187,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_478c881ace1744ff825202484b212c17~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cca358e42c08454cb9f7f30317c4e93c~mv2.png" alt=""></td></tr><tr><td>A49</td><td>mouthPressRight</td><td><img src="https://static.wixstatic.com/media/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png/v1/fill/w_267,h_186,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_acad007d32d24b26b4cc192345afc0ba~mv2.png" alt=""></td><td><br><img src="https://static.wixstatic.com/media/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png/v1/fill/w_230,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_35bac2e5acf54d438dd0acf4690c4ea2~mv2.png" alt=""></td></tr><tr><td>A50</td><td>mouthStretchLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_18fbf15030164a6383068c8fb7aa7e72~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_cf77104a546149e88698feb420726493~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_cf77104a546149e88698feb420726493~mv2.png" alt=""></td></tr><tr><td>A51</td><td>mouthStretchRight</td><td><img src="https://static.wixstatic.com/media/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png/v1/fill/w_263,h_184,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3f8dd987a3d44b7e98e1e7abb1815111~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b2a3abb6ea204ab293571c7c19747003~mv2.png" alt=""></td></tr><tr><td>A52</td><td>tongueOut</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png/v1/fill/w_232,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_10387f10b0e04d5fac672f8bd17d9459~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>CC3 额外的舌头Blendshape(with open month)：</li></ul><div class="table-container"><table><thead><tr><th>T01</th><th>Tongue_Up</th><th></th><th><img src="https://static.wixstatic.com/media/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_75c512342cde45ffbb40fcf5d463732e~mv2.png" alt=""></th></tr></thead><tbody><tr><td>T02</td><td>Tongue_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4bdf00b4d23d4a89ac0bffbb66cc348d~mv2.png" alt=""></td></tr><tr><td>T03</td><td>Tongue_Left</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_860b7c7043894521a754755c35816cb3~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_860b7c7043894521a754755c35816cb3~mv2.png" alt=""></td></tr><tr><td>T04</td><td>Tongue_Right</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_02c7adc74f934d31ad35c01615b96735~mv2.png" alt=""></td></tr><tr><td>T05</td><td>Tongue_Roll</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_bfa27b0483c94f7484eeed246642fbc5~mv2.png" alt=""></td></tr><tr><td>T06</td><td>Tongue_Tip_Up</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6a7d96ce1444409c958adc03652983b7~mv2.png" alt=""></td></tr><tr><td>T07</td><td>Tongue_Tip_Down</td><td></td><td><img src="https://static.wixstatic.com/media/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png/v1/fill/w_244,h_220,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_37fa335ad1a549b983fb6552db3b5198~mv2.png" alt=""></td></tr></tbody></table></div><h3 id="Vive面部的表情基准组"><a href="#Vive面部的表情基准组" class="headerlink" title="Vive面部的表情基准组"></a>Vive面部的表情基准组</h3><p>Vive这一套脸部追踪也是52个blendshapes，但是和苹果的基准有很大区别。</p><ul><li>区别一：舌头</li></ul><p>苹果其实是52+7，因为舌头在52个里只有一个伸舌头的blendshape，但vive其实是42 + 10，整体来讲Vive表情记住能tracking到的表情细节还是更少一些。</p><ul><li>区别二：眉毛</li></ul><p>ARKit的52个blendshapes，是根据硬件分区一对一tracking的，然而Vive眉毛不分是没有单独另设blendshapes，而是与眼睛的动作blended在一起作为一个blendshape的，并不是精准的一对一分区tracking。</p><p>我下面编号的排序是按照<a href="https://developer.vive.com/resources/vive-sense/sdk/vive-eye-and-facial-tracking-sdk/">VIVE Eye and Facial Tracking SDK</a> unity 里inspector里的顺序，方便我加表情。</p><p>这里是整理的用ARKit制作Vive基准的对应编号：</p><p><a href="https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1kWXnqtiVbXRb1FrD5NLlxxuxbYmS0Z6YBLuIE1WwqD4/edit?usp=sharing</a></p><ul><li>Eye Blendshapes （14 = 12 + 2）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V01</td><td>Eye_Left_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png/v1/fill/w_238,h_182,al_c,lg_1,q_85,enc_auto/64c63b_735cb0ae227e42bca98f9c51fbd0df6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e5f3c0dfc63a42618e182a9b1a0c1e9c~mv2.png" alt=""></td></tr><tr><td>V02</td><td>Eye_Left_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png/v1/fill/w_222,h_160,al_c,lg_1,q_85,enc_auto/64c63b_1d9223fb44574a94988a7c9ce4d89b39~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5dd9ed05165f4fe9b486b4f0604dacc1~mv2.png" alt=""></td></tr><tr><td>V03</td><td>Eye_Left_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png/v1/fill/w_238,h_188,al_c,lg_1,q_85,enc_auto/64c63b_638f42e4960743c4b235ab18b5ac6eba~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c5fcda96b7184941b2a89b2193470f2b~mv2.png" alt=""></td></tr><tr><td>V04</td><td>Eye_Left_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png/v1/fill/w_238,h_192,al_c,lg_1,q_85,enc_auto/64c63b_a1041585f54748728dd71aec7de129f5~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a26c36f7dc2940fe9513e263f7a99c4e~mv2.png" alt=""></td></tr><tr><td>V05</td><td>Eye_Left_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png/v1/fill/w_238,h_203,al_c,lg_1,q_85,enc_auto/64c63b_d2c84d05015c4d8289f6c7d6d5ba0dcc~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e3e67e16e4b84697a39c3333bad24712~mv2.png" alt=""></td></tr><tr><td>V06</td><td>Eye_Left_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png/v1/fill/w_238,h_195,al_c,lg_1,q_85,enc_auto/64c63b_5cd43316383549e282e7f3f743df9053~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_993bd278fb024580a834a71c6886cc4b~mv2.png" alt=""></td></tr><tr><td>V07</td><td>Eye_Right_Blink</td><td><img src="https://static.wixstatic.com/media/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png/v1/fill/w_235,h_195,al_c,lg_1,q_85,enc_auto/64c63b_2e198e55d97a491cbc66059e6f6adddc~mv2.png" alt=""></td><td></td></tr><tr><td>V08</td><td>Eye_Right_Wide</td><td><img src="https://static.wixstatic.com/media/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png/v1/fill/w_223,h_160,al_c,lg_1,q_85,enc_auto/64c63b_6eebbecf575d4bec905be4dbec06322c~mv2.png" alt=""></td><td></td></tr><tr><td>V09</td><td>Eye_Right_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png/v1/fill/w_234,h_197,al_c,lg_1,q_85,enc_auto/64c63b_fc88e475e8c4435a98364224f54ade1a~mv2.png" alt=""></td><td></td></tr><tr><td>V10</td><td>Eye_Right_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png/v1/fill/w_238,h_190,al_c,lg_1,q_85,enc_auto/64c63b_216b900672314c37a3e91501b7fe7cc1~mv2.png" alt=""></td><td></td></tr><tr><td>V11</td><td>Eye_Right_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png/v1/fill/w_231,h_196,al_c,lg_1,q_85,enc_auto/64c63b_062847a5c45d4dabbe78d255779013dd~mv2.png" alt=""></td><td></td></tr><tr><td>V12</td><td>Eye_Right_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png/v1/fill/w_238,h_176,al_c,lg_1,q_85,enc_auto/64c63b_b6fae35fba8543d0becc535532111d23~mv2.png" alt=""></td><td></td></tr><tr><td>V13</td><td>Eye_Left_squeeze: The blendShape close eye tightly when Eye_Left_Blink  value is 100.</td><td><img src="https://static.wixstatic.com/media/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png/v1/fill/w_238,h_183,al_c,lg_1,q_85,enc_auto/64c63b_c7b07f1b4ec6495685d85808d23c04e8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png/v1/fill/w_211,h_188,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a629d31ace624dd8b2ded5123123156e~mv2.png" alt=""></td></tr><tr><td>V14</td><td>Eye_Right_squeeze</td><td><img src="https://static.wixstatic.com/media/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png/v1/fill/w_238,h_194,al_c,lg_1,q_85,enc_auto/64c63b_b82c535f90874742b3c0b6ff62136fe2~mv2.png" alt=""></td></tr></tbody></table></div><ul><li>Lip Blendshapes （38 = 37 + 1）</li></ul><div class="table-container"><table><thead><tr><th>Vive编号</th><th>Vive表情基准</th><th>Vive Picture</th><th>Create by CC3 blendshapes</th></tr></thead><tbody><tr><td>V15</td><td>Jaw_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png/v1/fill/w_235,h_190,al_c,lg_1,q_85,enc_auto/64c63b_75f8b68a96104ef8bf36e393c7ecd48b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_65471d28b0d743c0bb6232ffaee0f6b6~mv2.png" alt=""></td></tr><tr><td>V16</td><td>Jaw_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png/v1/fill/w_245,h_202,al_c,lg_1,q_85,enc_auto/64c63b_9cacde29288c4523a2192835a736ad6b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_017d7142fd494aef9ad7bbe53fa1d6eb~mv2.png" alt=""></td></tr><tr><td>V17</td><td>Jaw_Forward</td><td><img src="https://static.wixstatic.com/media/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png/v1/fill/w_248,h_197,al_c,lg_1,q_85,enc_auto/64c63b_3876f3dd1a1b4eed92e8405b42700190~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_20353f83579541428557c32d92545c9e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_20353f83579541428557c32d92545c9e~mv2.png" alt=""></td></tr><tr><td>V18</td><td>Jaw_Open</td><td><img src="https://static.wixstatic.com/media/64c63b_dc79f10003534839948d3261183d5082~mv2.png/v1/fill/w_244,h_188,al_c,lg_1,q_85,enc_auto/64c63b_dc79f10003534839948d3261183d5082~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_24470b9cc9964a11906c42b1d1a6e5e9~mv2.png" alt=""></td></tr><tr><td>V19</td><td>Mouth_Ape_Shape</td><td><img src="https://static.wixstatic.com/media/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png/v1/fill/w_249,h_196,al_c,lg_1,q_85,enc_auto/64c63b_7a0f9461a760449db12b2159009ccc93~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3d7911f5bfa645adb7f3c36fbeafa2b9~mv2.png" alt=""></td></tr><tr><td>V20</td><td>Mouth_Upper_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png/v1/fill/w_227,h_161,al_c,lg_1,q_85,enc_auto/64c63b_01037e0042754059b7ada72a8adf2e8a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_185ec305ba464016a15c2420fb04916e~mv2.png" alt=""></td></tr><tr><td>V21</td><td>Mouth_Upper_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png/v1/fill/w_265,h_182,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f0c61e8f3f3c42d7ad6d83703f1a61d9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_abbecb4860a44fe9800585825beb4b17~mv2.png" alt=""></td></tr><tr><td>V22</td><td>Mouth_Lower_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_3ec74984b19d44d389a68bcc1ac1a7fb~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_80a4cdffa3fb493e9c153b517d9aebda~mv2.png" alt=""></td></tr><tr><td>V23</td><td>Mouth_Lower_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png/v1/fill/w_265,h_225,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e80cee8738ea42528c8f351303f5e2c8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_edf8e441ff994c27bde811a21754d5e5~mv2.png" alt=""></td></tr><tr><td>V24</td><td>*Mouth_Upper_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png/v1/fill/w_265,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5f77c14164ae48cf9c0cf4c762b97837~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_b8ae358e723f42e199338722f186e238~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b8ae358e723f42e199338722f186e238~mv2.png" alt=""></td></tr><tr><td>V25</td><td>*Mouth_Lower_Overturn</td><td><img src="https://static.wixstatic.com/media/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png/v1/fill/w_265,h_210,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_16a26f9ced50420b99a4c32fc296c112~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_282323813dfa4ea1aa76551b112e3919~mv2.png" alt=""></td></tr><tr><td>V26</td><td>Mouth_Pout</td><td><img src="https://static.wixstatic.com/media/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e7da853fe63242a9bedbd7fe3bddadc7~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_36615908f74d4663a6bc438c3287938c~mv2.png" alt=""></td></tr><tr><td>V27</td><td>Mouth_Smile_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png/v1/fill/w_265,h_205,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1de129dc23784dd0af8d5bbccff75741~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_47a00d3e749e47fa8b3489da81252654~mv2.png" alt=""></td></tr><tr><td>V28</td><td>Mouth_Smile_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e66c5606123d4272bc4d3206a101e884~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0a26740959644351bb01f9e9d40ef35e~mv2.png" alt=""></td></tr><tr><td>V29</td><td>Mouth_Sad_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ba5427a221c2446e9d3b9e30d94d80b9~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c1e99be3d6c34038852ce55b72102f5c~mv2.png" alt=""></td></tr><tr><td>V30</td><td>Mouth_Sad_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_00772c50ca334cbf95dd1bf53be4c6b8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_889e2637303f4d7195afd699a3d92b86~mv2.png" alt=""></td></tr><tr><td>V31</td><td>Cheek_Puff_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png/v1/fill/w_265,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_765350d6685547d4b03b7ae31e7346e0~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_66386b8d632b4556a00f886613f26d92~mv2.png" alt=""></td></tr><tr><td>V32</td><td>Cheek_Puff_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2998211eb141496d8651b786337b7846~mv2.png/v1/fill/w_265,h_213,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2998211eb141496d8651b786337b7846~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_833f433253fb4bcc8e380d79120b3003~mv2.png" alt=""></td></tr><tr><td>V33</td><td>Cheek_Suck</td><td><img src="https://static.wixstatic.com/media/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6eea541e05494d26a06cbbe5377cdc0a~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_8573de7fc0d84245a0fa4412ecd3e842~mv2.png" alt=""></td></tr><tr><td>V34</td><td>Mouth_Upper_UpRight</td><td><img src="https://static.wixstatic.com/media/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png/v1/fill/w_265,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b6ca87cbb7774b2ab4f0ec3748ec9c51~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_c48b985609fe4129ad1dac8a41905a7e~mv2.png" alt=""></td></tr><tr><td>V35</td><td>Mouth<em>Upper</em> UpLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png/v1/fill/w_265,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_58bdc8db0ac3451388534ff3bfb0fa83~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5ff3f05aeecd48fe9f3077f5c9c96569~mv2.png" alt=""></td></tr><tr><td>V36</td><td>Mouth_Lower_DownRight</td><td><img src="https://static.wixstatic.com/media/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png/v1/fill/w_265,h_223,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_12f26efe2f28425cb366eea55e83470c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_30ae27fa9ee94aa8a1b29bbd5fe7b0b2~mv2.png" alt=""></td></tr><tr><td>V37</td><td>Mouth_Lower_DownLeft</td><td><img src="https://static.wixstatic.com/media/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png/v1/fill/w_265,h_218,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ed1a27945324b53aad0aa3cf453a275~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_76893f9855fa4a5a9415cd8abfae6f6f~mv2.png" alt=""></td></tr><tr><td>V38</td><td>Mouth_Upper_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png/v1/fill/w_265,h_209,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_f5e050f0d9954760879ccd18185c2fc8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png/v1/fill/w_224,h_200,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5217c686c7c6455aaca6ba1b2ce64217~mv2.png" alt=""></td></tr><tr><td>V39</td><td>Mouth_Lower_Inside</td><td><img src="https://static.wixstatic.com/media/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png/v1/fill/w_269,h_211,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0031f9adda4441cbb6361e280e594c7b~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_5108178f718e492eb840f4d678eb3e4e~mv2.png" alt=""></td></tr><tr><td>V40</td><td>Mouth_Lower_Overlay</td><td><img src="https://static.wixstatic.com/media/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png/v1/fill/w_269,h_222,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_26e5bd6286474b4ea3f4fff3933b91f1~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_1ab252df4c5146e1815e23a83edb2cd2~mv2.png" alt=""></td></tr><tr><td>V41</td><td>Tongue_LongStep1</td><td><img src="https://static.wixstatic.com/media/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png/v1/fill/w_269,h_207,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6791ccfceffe4c2ca07f277b91037521~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4ef4ab94e1ef47dcb01facf5d168f1d1~mv2.png" alt=""></td></tr><tr><td>V42</td><td>Tongue_LongStep2</td><td><img src="https://static.wixstatic.com/media/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png/v1/fill/w_269,h_181,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e79ccc0096a54ce2b48d188cf6907d0c~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e560524670843848266701061f24c63~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e560524670843848266701061f24c63~mv2.png" alt=""></td></tr><tr><td>V43</td><td>*Tongue_Down</td><td><img src="https://static.wixstatic.com/media/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png/v1/fill/w_269,h_199,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ac97c6cf9fd940e9b38cdf22ab3c9261~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_a2a8aab70eea4b6ba9378cf249aef3a0~mv2.png" alt=""></td></tr><tr><td>V44</td><td>*Tongue_Up</td><td><img src="https://static.wixstatic.com/media/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png/v1/fill/w_269,h_197,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0c11f06c560842718309c52bc159ffa8~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0abc5fe7b2da4c988591e18fc6e060ac~mv2.png" alt=""></td></tr><tr><td>V45</td><td>*Tongue_Right</td><td><img src="https://static.wixstatic.com/media/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png/v1/fill/w_269,h_202,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_0d6e1976f6c342a395f9631be529c694~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_e17db94c36594f32b60ce6057a17aafc~mv2.png" alt=""></td></tr><tr><td>V46</td><td>*Tongue_Left</td><td><img src="https://static.wixstatic.com/media/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2633b9481a6f4e94ad4bfe6a6d52e122~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_ad949214aeda4b3488c238af7aabdba6~mv2.png" alt=""></td></tr><tr><td>V47</td><td>*Tongue_Roll</td><td><img src="https://static.wixstatic.com/media/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png/v1/fill/w_269,h_216,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_4d41918f9af84d0aaaa2de1e354a5706~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_2c544b45c0ea48079bb570912b85b2a3~mv2.png" alt=""></td></tr><tr><td>V48</td><td>*Tongue_UpLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png/v1/fill/w_269,h_201,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_49d65415de0f47d5a07aa77cfebd54e6~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png/v1/fill/w_269,h_221,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_9df53a0e560f41f2a441f9e799b56d1c~mv2.png" alt=""></td><td></td></tr><tr><td>V49</td><td>*Tongue_UpRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png/v1/fill/w_269,h_232,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_118b5229d2274b5f95a163ebc0d0cfad~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png/v1/fill/w_269,h_215,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_aa40430e533c40c69f0addb2df019a29~mv2.png" alt=""></td><td></td></tr><tr><td>V50</td><td>*Tongue_DownLeft_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png/v1/fill/w_269,h_237,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_92fc463c5efc4436a23870d596023ba9~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png/v1/fill/w_269,h_231,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b3a74eba5433479b96ff645e85681480~mv2.png" alt=""></td><td></td></tr><tr><td>V51</td><td>*Tongue_DownRight_Morph</td><td><img src="https://static.wixstatic.com/media/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png/v1/fill/w_269,h_224,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_db25fb8a736e4b1f9f4e11ba7436e0b0~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png/v1/fill/w_269,h_212,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b707961c295f40a7995644e93e438ffc~mv2.png" alt=""></td><td></td></tr><tr><td>V52</td><td>*O-shaped mouth</td><td><img src="https://static.wixstatic.com/media/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png/v1/fill/w_269,h_175,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_97cacb52babe4a109cd02874efcb2eda~mv2.png" alt=""></td><td><img src="https://static.wixstatic.com/media/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_6e98ff0232d349b8a6f7d8348992ab37~mv2.png" alt=""> <img src="https://static.wixstatic.com/media/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png/v1/fill/w_217,h_193,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/64c63b_b35e7f3707fc4b0ca75f80c4d77867a4~mv2.png" alt=""></td></tr></tbody></table></div><h2 id="MediaPipe提取BlendShape"><a href="#MediaPipe提取BlendShape" class="headerlink" title="MediaPipe提取BlendShape"></a>MediaPipe提取BlendShape</h2><p>MediaPipe Face Landmarker解决方案最初于5月的Google I/O 2023发布。它可以检测面部landmark并输出blendshape score，以渲染与用户匹配的3D面部模型。通过MediaPipe Face Landmarker解决方案，KDDI和谷歌成功地为虚拟主播带来了真实感。</p><p><strong>技术实现</strong></p><p>使用Mediapipe强大而高效的Python包，KDDI开发人员能够检测表演者的面部特征并实时提取52个混合形状。</p><p>还可参考：<a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb">https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mediapipe <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">from</span> mediapipe.tasks <span class="keyword">import</span> python <span class="keyword">as</span> mp_python</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MP_TASK_FILE = <span class="string">"face_landmarker_with_blendshapes.task"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FaceMeshDetector</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(MP_TASK_FILE, mode=<span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f_buffer = f.read()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建配置选项</span></span><br><span class="line">        base_options = mp_python.BaseOptions(model_asset_buffer=f_buffer)</span><br><span class="line">        options = mp_python.vision.FaceLandmarkerOptions(</span><br><span class="line">            base_options=base_options,</span><br><span class="line">            output_face_blendshapes=<span class="literal">True</span>,</span><br><span class="line">            output_facial_transformation_matrixes=<span class="literal">True</span>,</span><br><span class="line">            running_mode=mp.tasks.vision.RunningMode.LIVE_STREAM,</span><br><span class="line">            num_faces=<span class="number">1</span>,</span><br><span class="line">            result_callback=self.mp_callback</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建模型</span></span><br><span class="line">        self.model = mp_python.vision.FaceLandmarker.create_from_options(options)</span><br><span class="line">        self.landmarks = <span class="literal">None</span></span><br><span class="line">        self.blendshapes = <span class="literal">None</span></span><br><span class="line">        self.latest_time_ms = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mp_callback</span>(<span class="params">self, mp_result, output_image, timestamp_ms: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="comment"># 处理回调结果</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mp_result.face_landmarks) &gt;= <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(mp_result.face_blendshapes) &gt;= <span class="number">1</span>:</span><br><span class="line">            self.landmarks = mp_result.face_landmarks[<span class="number">0</span>]</span><br><span class="line">            self.blendshapes = [b.score <span class="keyword">for</span> b <span class="keyword">in</span> mp_result.face_blendshapes[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, frame</span>):</span><br><span class="line">        t_ms = <span class="built_in">int</span>(time.time() * <span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">if</span> t_ms &lt;= self.latest_time_ms:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        frame_mp = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)</span><br><span class="line">        self.model.detect_async(frame_mp, t_ms)</span><br><span class="line">        self.latest_time_ms = t_ms</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_results</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.landmarks, self.blendshapes</span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.mianzi-lizi.com/post/blendshape学习笔记">https://www.mianzi-lizi.com/post/blendshape学习笔记</a></li><li><a href="https://www.toutiao.com/article/6915330866285691395/">利用Animoji技术识别用户的表情</a></li><li><a href="https://news.nweon.com/110210">通过MediaPipe解决方案来为虚拟主播带来更逼真真实感</a></li><li><a href="https://bbs.huaweicloud.com/blogs/374337">Unity &amp; FACEGOOD Audio2Face 通过音频驱动面部BlendShape</a></li><li><a href="https://www.cnblogs.com/jesse123/p/9014234.html">GenerativeAI Avatar solutions</a></li></ul>]]></content>
    
    
    <summary type="html">Blendshapes泛指3D定点动画的制作方式 (Maya里面称之为 blend shapes ，而3DS Max里称之为morph targets) ，在3D动画中用的比较多，尤其是人脸动画的制作，通过blendshape来驱动角色的面部表情。</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/NeRF/</id>
    <published>2024-03-09T10:43:34.000Z</published>
    <updated>2024-03-09T10:43:34.779Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis"><a href="#DART-Implicit-Doppler-Tomography-for-Radar-Novel-View-Synthesis" class="headerlink" title="DART: Implicit Doppler Tomography for Radar Novel View Synthesis"></a>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</h2><p><strong>Authors:Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe</strong></p><p>Simulation is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images. </p><p><a href="http://arxiv.org/abs/2403.03896v1">PDF</a> To appear in CVPR 2024; see <a href="https://wiselabcmu.github.io/dart/">https://wiselabcmu.github.io/dart/</a> for   our project site</p><p><strong>Summary</strong></p><p>基于雷达特定物理特性，使用神经辐射场方法创建反射和透射渲染管道，用于生成多普勒范围雷达图像。</p><p><strong>Key Takeaways</strong></p><ul><li>通过模拟器快速原型化成像、目标检测、分类和跟踪算法。</li><li>构建真实的雷达扫描模型面临场景、射频材料特性和雷达合成函数的挑战。</li><li>提出 DART 方法，受神经辐射场启发，构建基于反射率和透射率的渲染管道。</li><li>构建定制数据收集平台，收集包含位置和即时速度测量的新型雷达数据集。</li><li>与现有基准相比，DART 合成出所有数据集新视角下的更优质雷达多普勒范围图像。</li><li>DART 可用于生成高质量的层析图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：雷达隐式多普勒层析成像用于新型视角合成</li><li>作者：Jiahui Yu、Yiyi Liao、Yinda Zhang、Wenqi Xian、Lingxiao Li、Junjie Gu、Xiaoyang Guo、Shilin Zhu、Shanshan Zhao、Biao Yang、Lingbo Liu</li><li>隶属：上海交通大学</li><li>关键词：雷达、合成孔径雷达、多普勒层析成像、神经辐射场</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：(1) 研究背景：雷达仿真对于射频系统设计至关重要，但仿真逼真的雷达扫描具有挑战性，需要场景、射频材料属性和雷达合成函数的准确模型。(2) 过去方法：传统方法需要显式指定这些模型，但它们复杂且耗时。(3) 论文提出的研究方法：DART（多普勒辅助雷达层析成像）是一种受神经辐射场启发的雷达特定物理方法，它创建了一个基于反射率和透射率的渲染管道，用于生成距离-多普勒图像。(4) 方法在任务中的表现：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。这些性能支持了论文的目标，即提供一种无需显式模型即可生成逼真雷达图像的方法。</p></li><li><p>方法：(1) 数据驱动方法使用真实的传感器扫描来构建环境模型。稀疏方法使用恒定误报率检测 (CFAR) 来检测环境中的离散反射器 [15, 49, 63]。另一方面，密集方法将环境划分为显式的体素网格，并推断每个单元的雷达属性。密集方法可以进一步细分为相干和非相干聚合。如果可以使用固定（例如线性和圆形）轨迹或亚波长精度的姿态估计，则可以使用合成孔径雷达 (SAR) [46, 50, 52, 56, 81, 82]；然而，这对于大面积移动平台来说是不切实际的。相反，传感器读数（通过多个天线或较小轨迹片段上的 SAR 获得高角度分辨率）也可以以非相干方式聚合，这被称为多视图 3D 重建 [33–35] 和雷达测量法 [12]。(2) 雷达中的机器学习方法许多经典的雷达问题，例如雷达超分辨率 [10, 17, 20, 21, 23, 53, 54, 72]、里程计 [2, 43]、测绘 [42]、活动识别 [39, 70, 77, 80] 和物体分类 [32, 69, 85] 已应用于使用机器学习的更便宜、更轻、更紧凑的雷达系统。我们现在寻求从紧凑、低分辨率雷达中解决新颖的视图合成问题，同时隐式创建更高分辨率的地图。(3) 神经辐射场神经辐射场 [48] 没有定义明确的逆成像算法从传感器读数中恢复场景的表示，而是通过随机梯度下降隐式地反转前向渲染函数。这需要以下组件：</p></li><li>世界模型：NeRF 将世界定义为每个位置和视角的 RGB 颜色和透明度；后续工作已将其推广到处理抗锯齿 [5]、不同的相机和照明 [47, 73]。</li><li>世界表示：除了神经网络 [48] 或体素网格 [40] 之外，最近的工作还探索了空间哈希表 [51] 以及用于视场角依赖性的函数分解 [18, 83]。</li><li>渲染函数和模型反演：NeRF 将每个像素建模为射线并对辐射场进行射线追踪。此渲染函数的可逆性至关重要：通过假设每个像素都是一条射线，NeRF 由每个射线上的一个 RGB 图像像素“监督”，允许 NeRF “求解”沿射线的不透明点。我们对 NeRF 的这些关键推动因素进行了创新，以便将这种方法应用于毫米波雷达。通过将 NeRF 技术应用于雷达，我们希望利用大量神经辐射场文献，同时释放神经隐式表示的潜力。超越视觉领域 NeRF 的成功激发了众多其他努力，将相同的通用原理应用于其他传感器，包括空间音频 [44]、成像声纳 [55, 59]、激光雷达模拟 [27] 和 RSSI（接收信号强度指示器）映射 [84]。NeRF 也已应用于雷达 [29, 71]，用于类似相机的超高分辨率合成孔径雷达，而不是我们在本文中探索的紧凑且廉价的雷达。(4) DART：多普勒辅助雷达层析成像虽然我们的整体方法受神经辐射场的启发，但雷达的物理特性提出了几个新的挑战。我们做出以下关键设计决策（图 3）：</li><li>我们首先选择一个雷达测量表示空间——距离-多普勒——该空间克服了紧凑型雷达的较差空间分辨率（第 3.1、3.2 节）。</li><li>然后我们选择一个模型来解释电磁波相互作用的雷达特定效应，这些效应对于逼真的视图合成至关重要，例如镜面反射、重影和部分遮挡（第 3.3 节）。</li><li><p>最后，为了有效地训练和学习雷达的神经隐式地图，我们为自适应网格世界表示选择了网络架构，设计了距离-多普勒渲染方法，并提出了关键渲染优化（第 3.3-3.4 节）。(5) 距离-多普勒表示与相机不同，雷达是主动传感器，它通过发射射频波形来照亮场景。在处理从场景中的物体接收到的反射后，雷达可以以 3D 形式感知世界——距离、方位角和仰角——作为热图，指示该 3D 坐标处物体的反射率 [60, 61]。然而，虽然笨重的机械雷达或大型固态雷达阵列可以提供接近典型相机的方位角和仰角分辨率，但现代廉价且紧凑的固态雷达阵列具有小天线阵列，这使得它们在方位角和仰角轴上远逊于典型相机 [28]。因此，这些紧凑型雷达只能在方位角和仰角轴上生成粗糙的热图（&gt;15◦ 分辨率），导致每个距离-方位角-仰角箱指向 3D 空间中的一个较粗糙区域，远不如来自相机像素的射线清晰 [38, 41, 76]。为了获得更好的角度分辨率，雷达可以利用多普勒效应：相对于雷达以不同相对速度移动的物体具有不同的多普勒速度，可以通过检查距离-方位角-仰角热图的残余相位来测量这些速度 [79]。至关重要的是，在静态场景中，这些相对速度不仅取决于雷达和世界之间的相对速度，还取决于物体与雷达之间的相对方位角和仰角，每个多普勒对应于空间中的一个圆锥 [60]。由于更精细的距离和多普勒分辨率，多普勒极大地降低了 3D 空间中每个箱的模糊性，使其变为一个薄环（图 4），我们通过在距离和多普勒轴上进行细度论证进一步将其简化为雷达渲染的圆圈（第 3.4 节）。(6) 雷达预处理毫米波雷达使用称为调频连续波 (FMCW) 的波形，并测量连续时间信号；然后我们将这些信号转换为距离-多普勒-天线热图。为了总结我们的雷达处理管道的要点（附录 A.1）：• 不希望的距离-多普勒旁瓣：单个反射物体可以创建旁瓣，这些旁瓣会渗入几个距离-多普勒箱并掩盖较弱的物体 [61, 86]。我们使用汉宁加权窗口沿着距离和多普勒轴来减轻这种影响，而不是强迫 DART 对其进行建模（附录 A.1）。• 多个天线：我们对雷达中的八个发射-接收 (TX/RX) 对执行距离-多普勒处理。在我们的渲染过程中（第 3.4 节），我们对每个 TX/RX 对应用天线增益和阵列因子（图 3），强调视野的 8 个部分。虽然我们对高质量方位角-仰角信息的感知仍然源于利用多普勒，但这提供了一些粗略的方向信息。(7) DART 的世界模型如果我们有世界和世界中所有物体电磁波相互作用的准确模型，我们就可以将该模型应用于由每个距离-多普勒像素定义的区域来计算其值。然而，由于现实世界场景和交互的复杂性，这两个任务都非常困难且通常不切实际。相反，我们以数据驱动的方式对这些属性进行建模，使用视场相关的神经网络方法表示反射率和透射率。建模射频反射率建模毫米波材料相互作用是雷达视图合成最具挑战性的因素之一。从雷达的角度来看，空间中的点具有两个关键属性：反射率（反射回的能量比例）和透射率（继续过去的能量比例）[60]。然而，毫米波也会根据入射角与物体进行不同的交互 [4]；例如，金属表面可能是镜面反射的，并且可能从某些视点不可见。因此，我们使用反射率 σ：R6→R 和透射率 α：R6→[0,1] 对每个物理点进行建模，(1)它将反射率 σ 和透射率 α 建模为入射波的位置 (R3) 和入射角 (R3) 的函数，并允许 DART 对各种雷达现象进行建模，例如部分遮挡、镜面反射和重影（附录 A.2）。世界表示虽然基于体素的方法对于学习视觉辐射场非常有效 [18, 83]，但即使在利用多普勒轴后，雷达图像与相机相比也具有更差的仰角和方位角分辨率。这放大了 σ 和 α 可以解决的空间分辨率差异，即使在近距离和远距离之间也是如此。此外，与相机不同，我们的角度分辨率在所有尺度上都是可变的——无论是在轨迹级别、帧到帧级别甚至帧内（第 3.1 节）。类似于 NeRF [48]，我们转向神经隐式表示作为创建“自适应”网格的一种手段，并将我们的模型基于 Instant Neural Graphics Primitive3 [51]。与大多数视觉 NeRF 不同，我们不将入射角作为输入提供给神经网络 [74]。相反，我们的架构（可视化在图 3 的中心块中）输出“基本”反射率 ¯σ 和透射率 ¯α，以及共享球谐函数系数 [83]，这些系数作为内积应用于入射角。除了计算优势之外，这还允许我们直接将 (¯σ, ¯α) 解释为我们学习的反射率和透射率函数的球积分（附录 A.3）。我们还发现 σ 和 α 上的输出激活函数对于数值稳定性和性能至关重要。由于 σ 是无界的4，我们对 σ 应用线性激活。然后，为了将 α 约束在 [0,1] 中，我们应用激活函数 f(α) = exp(max(0,α))，(2)我们将其与自定义梯度估计器配对以处理初始化不稳定性（附录 A.4）。(8) 雷达渲染和模型训练我们使用可微映射训练 σ 和 α，该映射从给定的 (σ, α) 网络生成多天线距离-多普勒热图；我们称之为雷达渲染。与视觉 NeRF 不同，DART 除了遮挡之外还必须考虑一系列物理效应，包括路径衰减、天线增益模式和雷达特定的多普勒轴。射线追踪考虑从雷达位置 x 和方向（旋转矩阵）A 以入射角 w 发射的单个“射线”。当射线在太空中传播到处理的（距离、多普勒、天线）图像的最大范围时，每个点 x + riw 在距离 r 处接收幅度为 u_i 的信号，该信号因自由空间而衰减。</p></li><li><p>结论：（1）本文提出了 DART（多普勒辅助雷达层析成像）方法，该方法利用神经辐射场技术，无需显式模型即可生成逼真的雷达图像，为新型视角合成提供了新的方法。（2）创新点：</p></li><li>提出了一种雷达特定物理模型，用于解释电磁波相互作用的雷达特定效应。</li><li>设计了一种距离-多普勒渲染方法，用于有效地训练和学习雷达的神经隐式地图。</li><li>提出了一种关键渲染优化，以提高渲染效率和图像质量。</li><li>性能：DART 在所有数据集上从新视角合成了出色的雷达距离-多普勒图像，此外还可用于生成高质量的层析图像。</li><li>工作量：DART 的实现相对简单，易于部署和使用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7a08f4b46a27b4550cca3fdbb7bb2699.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5dd4309cf1d06499c45ea2d70f80cbb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d4136ef209f4ed07822647cd67d564e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f4196074de7d63d703597568e97025da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7aa27948966717e8808650a0fc34b361.jpg" align="middle"></details><h2 id="DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes"><a href="#DaReNeRF-Direction-aware-Representation-for-Dynamic-Scenes" class="headerlink" title="DaReNeRF: Direction-aware Representation for Dynamic Scenes"></a>DaReNeRF: Direction-aware Representation for Dynamic Scenes</h2><p><strong>Authors:Ange Lou, Benjamin Planche, Zhongpai Gao, Yamin Li, Tianyu Luan, Hao Ding, Terrence Chen, Jack Noble, Ziyan Wu</strong></p><p>Addressing the intricate challenge of modeling and re-rendering dynamic scenes, most recent approaches have sought to simplify these complexities using plane-based explicit representations, overcoming the slow training time issues associated with methods like Neural Radiance Fields (NeRF) and implicit representations. However, the straightforward decomposition of 4D dynamic scenes into multiple 2D plane-based representations proves insufficient for re-rendering high-fidelity scenes with complex motions. In response, we present a novel direction-aware representation (DaRe) approach that captures scene dynamics from six different directions. This learned representation undergoes an inverse dual-tree complex wavelet transformation (DTCWT) to recover plane-based information. DaReNeRF computes features for each space-time point by fusing vectors from these recovered planes. Combining DaReNeRF with a tiny MLP for color regression and leveraging volume rendering in training yield state-of-the-art performance in novel view synthesis for complex dynamic scenes. Notably, to address redundancy introduced by the six real and six imaginary direction-aware wavelet coefficients, we introduce a trainable masking approach, mitigating storage issues without significant performance decline. Moreover, DaReNeRF maintains a 2x reduction in training time compared to prior art while delivering superior performance. </p><p><a href="http://arxiv.org/abs/2403.02265v1">PDF</a> Accepted at CVPR 2024. Paper + supplementary material</p><p><strong>Summary</strong><br>使用六个不同方向捕捉场景动态并融合信息，DaReNeRF 在复杂动态场景的新视图合成中取得了最先进的性能。</p><p><strong>Key Takeaways</strong></p><ul><li>利用六个方向感知表示捕获场景动态。</li><li>采用逆向双树复小波变换恢复平面信息。</li><li>将方向感知表示融合到 NeRF 中，计算时空点的特征。</li><li>使用小的 MLP 进行颜色回归，利用体积渲染进行训练。</li><li>引入可训练掩码方法，在不降低性能的情况下减轻存储问题。</li><li>与现有技术相比，训练时间减少 2 倍，同时性能更优。</li><li>适用于具有复杂运动的高保真场景的重新渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><strong>标题：</strong> DaReNeRF：动态场景的方向感知表征</li><li><strong>作者：</strong> Ange Lou, Tianyu Luan, Hao Ding, Wenbo Luo, Xiaogang Wang, Wenzheng Chen</li><li><strong>第一作者单位：</strong> United Imaging Intelligence</li><li><strong>关键词：</strong> 动态场景，神经辐射场，平面表示，方向感知表征</li><li><strong>论文链接：</strong> None</li><li><p><strong>摘要：</strong>   (1) <strong>研究背景：</strong> 近期方法使用基于平面的显式表征来简化动态场景建模和渲染，克服了神经辐射场等方法相关的训练时间慢的问题。然而，将 4D 动态场景直接分解为多个基于平面的 2D 表征不足以渲染具有复杂运动的高保真场景。   (2) <strong>过去方法及问题：</strong> 现有方法将动态场景分解为多个基于平面的 2D 表征，但这种方法不足以渲染具有复杂运动的高保真场景。   (3) <strong>研究方法：</strong> 本文提出了一种新的方向感知表征 (DaRe) 方法，该方法从六个不同方向捕获场景动态。这种学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练，在复杂动态场景的新视角合成中实现了最先进的性能。   (4) <strong>方法性能：</strong> DaReNeRF 在训练时间上比现有方法减少了 2 倍，同时提供了更好的性能。</p></li><li><p>方法：(1): 该方法从六个不同方向捕获场景动态，学习到的表征经过逆双树复小波变换 (DTCWT) 以恢复基于平面的信息。(2): DaReNeRF 通过融合这些恢复的平面的向量来计算每个时空点的特征。(3): 将 DaReNeRF 与用于颜色回归的微小 MLP 结合起来，并利用体积渲染进行训练。</p></li><li><p>结论：(1): 本工作通过提出 DaReNeRF 方法，在动态场景建模和渲染领域取得了重要进展。该方法从六个不同方向捕获场景动态，并利用逆双树复小波变换恢复基于平面的信息，从而有效解决了复杂动态场景的高保真渲染问题。(2): 创新点：</p></li><li>从六个不同方向捕获场景动态，丰富了场景信息的获取。</li><li>采用逆双树复小波变换恢复基于平面的信息，有效融合了不同方向的特征。</li><li>将 DaReNeRF 与微小 MLP 结合，并利用体积渲染进行训练，实现了高效且高质量的渲染。性能：</li><li>在复杂动态场景的新视角合成任务上，DaReNeRF 实现了最先进的性能。</li><li>与现有方法相比，DaReNeRF 训练时间减少了 2 倍，渲染效率更高。工作量：</li><li>DaReNeRF 方法的实现难度适中，需要对神经辐射场、小波变换和体积渲染等技术有一定的了解。</li><li>训练 DaReNeRF 模型需要大量的动态场景数据和较长的训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0b34eef417abcdd2b497ef2ebd10beb3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94b89ba44b447b4f183c953bb896e07.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fc68e3cc2c894a358a3d010ccbf0fa0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f3c90874730f6ec424afc1f7edde45a.jpg" align="middle"></details><h2 id="Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views"><a href="#Depth-Guided-Robust-and-Fast-Point-Cloud-Fusion-NeRF-for-Sparse-Input-Views" class="headerlink" title="Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views"></a>Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input   Views</h2><p><strong>Authors:Shuai Guo, Qiuwen Wang, Yijie Gao, Rong Xie, Li Song</strong></p><p>Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines. </p><p><a href="http://arxiv.org/abs/2403.02063v1">PDF</a> </p><p><strong>Summary</strong><br><strong>NeRF深度引导点云融合：增强稀疏输入场景下新视角合成</strong></p><p><strong>Key Takeaways</strong></p><ul><li>提出深度引导的NeRF，用于稀疏输入的新视角合成。</li><li>使用显式体素网格表示辐射场。</li><li>构造每个输入视图的点云，并在体素网格中用矩阵和向量描述。</li><li>融合每个输入视图的点云，构建整个场景的融合点云。</li><li>每个体素根据整个场景的点云确定其密度和外观。</li><li>通过点云融合和体素网格微调，可以修正和替换深度值的误差。</li><li>通过有效的向量-矩阵分解，方法实现了更快的重建和更大的紧凑性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：深度引导的鲁棒且快速的点云融合 NeRF，用于稀疏输入视图</li><li>作者：Shuai Guo、Qiuwen Wang、Yijie Gao、Rong Xie、Li Song</li><li>隶属单位：上海交通大学图像通信与网络工程学院</li><li>关键词：NeRF、稀疏视图、深度融合、点云融合</li><li>论文链接：None    Github 代码链接：None</li><li>摘要：   （1）研究背景：NeRF 在稀疏输入视图下的新视图合成对于 AR/VR 和自动驾驶等真实世界应用非常重要。   （2）过去的方法：现有方法将深度信息集成到 NeRF 中以进行稀疏输入合成，利用深度先验进行几何和空间理解。然而，大多数现有工作往往忽略深度图中的不准确性，并且时间效率低。   （3）研究方法：为了解决这些问题，本文提出了一种用于稀疏输入的深度引导的鲁棒且快速的点云融合 NeRF。我们将辐射场感知为一个显式的特征体素网格。为每个输入视图构建一个点云，使用矩阵和向量在体素网格中表征。我们累积每个输入视图的点云，以构建整个场景的融合点云。每个体素通过参考整个场景的点云来确定其密度和外观。通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们。此外，我们的方法可以通过有效的向量矩阵分解实现更快的重建和更高的紧凑性。   （4）方法性能：实验结果强调了我们方法与最先进基准相比的卓越性能和时间效率。</li></ol><p>7.Methods:(1): 本文提出了一种深度引导的鲁棒且快速的点云融合NeRF，用于稀疏输入视图；(2): 将辐射场感知为一个显式的特征体素网格，为每个输入视图构建一个点云，并使用矩阵和向量在体素网格中表征；(3): 累积每个输入视图的点云，以构建整个场景的融合点云，每个体素通过参考整个场景的点云来确定其密度和外观；(4): 通过点云融合和体素网格微调，可以细化深度值中的不准确性或用其他视图中的值替换它们；(5): 此外，通过有效的向量矩阵分解，可以实现更快的重建和更高的紧凑性。</p><ol><li>结论：（1）本文提出的深度引导的鲁棒且快速的点云融合NeRF，对于稀疏输入视图下的新视图合成具有重要意义。（2）创新点：</li><li>将辐射场感知为一个显式的特征体素网格，并使用矩阵和向量进行表征。</li><li>通过点云融合和体素网格微调，细化深度值中的不准确性。</li><li>通过有效的向量矩阵分解，实现更快的重建和更高的紧凑性。性能：</li><li>与最先进的基准相比，具有卓越的性能和时间效率。工作量：</li><li>实现了更快的重建和更高的紧凑性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-01b32742a4cabe31ed749a6761475634.jpg" align="middle"><img src="https://pica.zhimg.com/v2-70b0b04ae4cf460209e8f732888cddee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86aa24ab75498868b39b0c370990c2e8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4f6398dec60102c0bb1f5d24d9a89432.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2d78f63f12b2bcb3ca39476e980147ba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4a484aa0d25d0950586c81e66b07ef9d.jpg" align="middle"></details><h2 id="NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning"><a href="#NeRF-VPT-Learning-Novel-View-Representations-with-Neural-Radiance-Fields-via-View-Prompt-Tuning" class="headerlink" title="NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning"></a>NeRF-VPT: Learning Novel View Representations with Neural Radiance   Fields via View Prompt Tuning</h2><p><strong>Authors:Linsheng Chen, Guangrun Wang, Liuchun Yuan, Keze Wang, Ken Deng, Philip H. S. Torr</strong></p><p>Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a>. </p><p><a href="http://arxiv.org/abs/2403.01325v1">PDF</a> AAAI 2024</p><p><strong>Summary</strong><br>神经辐射场（NeRF）在新的视野合成中取得了显著成功，但生成高质量新视角图像仍是一项重要挑战。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-VPT 利用级联视图提示调整范例来解决新视角合成中的细节捕获、纹理增强和 PSNR 提升问题。</li><li>NeRF-VPT 仅需在各个训练阶段对前一阶段渲染结果的 RGB 数据进行采样作为先验。</li><li>NeRF-VPT 是一种即插即用的方法，可以轻松集成到现有方法中。</li><li>NeRF-VPT 在 Realistic Synthetic 360、Real Forward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上显著提升了基准性能，并产生了比所有比较的最新方法更高质量的新视角图像。</li><li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视角新视角合成的准确性。</li><li>源代码和数据集可在 \url{<a href="https://github.com/Freedomcls/NeRF-VPT}">https://github.com/Freedomcls/NeRF-VPT}</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：NeRF-VPT：通过视图提示调整学习新颖视图表示</li><li>作者：Linsheng Chen、Guangrun Wang、Liuchun Yuan、Keze Wang、Ken Deng、Philip H.S. Torr</li><li>Affiliation：中山大学</li><li>关键词：NeRF、新颖视图合成、视图提示调整</li><li>论文链接：https://arxiv.org/abs/2403.01325   Github 链接：None</li><li>摘要：   （1）研究背景：NeRF 在新颖视图合成中取得了显著成功，但生成高质量的新颖视图图像仍然是一项关键挑战。   （2）过去方法：现有方法在捕捉复杂细节、增强纹理和提高 PSNR 方面取得了可喜的进展，但仍需要进一步关注和改进。   （3）研究方法：本文提出了一种名为 NeRF-VPT 的新颖视图合成方法，采用级联视图提示调整范式。该范式将来自先前渲染结果的 RGB 信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。   （4）方法性能：在 RealisticSynthetic360、RealForward-Facing、Replica 数据集和用户捕获数据集等具有挑战性的真实场景基准上，将 NeRF-VPT 与基于 NeRF 的方法进行比较分析，结果表明 NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。此外，NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li></ol><p>7.方法：（1）：NeRF-VPT采用级联视图提示调整范式，将来自先前渲染结果的RGB信息作为后续渲染阶段的指导性视觉提示，期望提示中嵌入的先验知识能够促进渲染图像质量的逐步提高。（2）：NeRF-VPT在NeRF的基础上，将位置编码和方向编码扩展为包含先验信息的编码，并采用分层结构，在每一层中使用更新的视图提示来指导渲染。（3）：NeRF-VPT引入了一个新的损失函数，该损失函数将渲染图像与视图提示之间的差异纳入考虑，从而鼓励渲染图像与视图提示保持一致。</p><ol><li>结论：（1）：本研究提出了一种新颖且通用的框架，以提高基于 NeRF 的视图合成的性能。我们提出了 NeRF-VPT，它引入了一种具有循环模块的新结构，并采用 NeRF 的输出作为先验。这使得 NeRF-VPT 能够显着提高视图相关外观的质量。它对端口友好，并且能够与现有方法相结合以获得最先进的性能。我们相信这项工作为充分利用表示提供了新的视角。（2）：创新点：</li><li>提出了一种新的视图提示调整范式，将先验信息嵌入到 NeRF 中，以逐步提高渲染图像的质量。</li><li>设计了一种分层结构，在每一层中使用更新的视图提示来指导渲染，从而捕获复杂细节并增强纹理。</li><li>引入了一个新的损失函数，将渲染图像与视图提示之间的差异纳入考虑，以鼓励渲染图像与视图提示保持一致。</li><li>性能：</li><li>在具有挑战性的真实场景基准上，NeRF-VPT 显着提升了基准性能，并比所有比较的最先进方法更有效地生成了更多高质量的新颖视图图像。</li><li>NeRF-VPT 的级联学习引入了对稀疏输入场景的适应性，从而显着提高了稀疏视图新颖视图合成的准确性。</li><li>工作量：</li><li>NeRF-VPT 的实现相对简单，并且可以轻松集成到现有的 NeRF 框架中。</li><li>NeRF-VPT 的训练过程高效且稳定，并且可以在各种硬件平台上轻松并行化。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a3d4a33c83819ae9629aeb5c7e195d32.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19c08401f045ff72d6d7af9a10c9430a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9c42f61f791fd5834fe43a11782fabd.jpg" align="middle"><img src="https://pica.zhimg.com/v2-135c07d8cd0edaf636a5f342ab6e1725.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf190c96eea398ae33fd3f16daf3d9cc.jpg" align="middle"></details><h2 id="Neural-radiance-fields-based-holography-Invited"><a href="#Neural-radiance-fields-based-holography-Invited" class="headerlink" title="Neural radiance fields-based holography [Invited]"></a>Neural radiance fields-based holography [Invited]</h2><p><strong>Authors:Minsung Kang, Fan Wang, Kai Kumano, Tomoyoshi Ito, Tomoyoshi Shimobaba</strong></p><p>This study presents a novel approach for generating holograms based on the neural radiance fields (NeRF) technique. Generating three-dimensional (3D) data is difficult in hologram computation. NeRF is a state-of-the-art technique for 3D light-field reconstruction from 2D images based on volume rendering. The NeRF can rapidly predict new-view images that do not include a training dataset. In this study, we constructed a rendering pipeline directly from a 3D light field generated from 2D images by NeRF for hologram generation using deep neural networks within a reasonable time. The pipeline comprises three main components: the NeRF, a depth predictor, and a hologram generator, all constructed using deep neural networks. The pipeline does not include any physical calculations. The predicted holograms of a 3D scene viewed from any direction were computed using the proposed pipeline. The simulation and experimental results are presented. </p><p><a href="http://arxiv.org/abs/2403.01137v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF技术结合深度预测器和全息图生成器，可快速生成高质量全息图，无需物理计算。</p><p><strong>Key Takeaways</strong></p><ul><li>利用NeRF技术从2D图像生成3D光场，为全息图计算提供数据源。</li><li>构建由NeRF、深度预测器和全息图生成器组成的渲染管道，用于全息图生成。</li><li>渲染管道完全基于深度学习，无物理计算。</li><li>渲染管道可快速生成任意视角下的3D场景全息图。</li><li>仿真和实验结果表明，所提出的管道可以生成高质量的全息图。</li><li>该方法消除了全息图计算中对物理模拟的需求。</li><li>通过结合NeRF技术和深度学习，该方法提高了全息图生成的速度和质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于神经辐射场的全息术[受邀]</li><li>作者：Minsung Kang, Fan Wang, Kai Kumao, Tomoyoshi Ito, Tomoyoshi Shimobaba</li><li>隶属单位：千叶大学工程学院</li><li>关键词：全息显示、神经辐射场、深度学习、光场重建</li><li>链接：http://dx.doi.org/10.1364/ao.XX.XXXXXX</li><li>摘要：（1）研究背景：全息显示器需要三维场景数据、全息图和三维图像再现三个步骤，每个步骤都存在障碍。特别是，对三维场景数据和全息图的计算是障碍。（2）过去方法及其问题：全息图的计算基于光传播模型，可以分为点云、多边形、光场和深度学习方法。这些方法各有优缺点，但都需要繁琐且耗时的三维场景生成。（3）本文方法：提出了一种基于神经辐射场 (NeRF) 的全息图生成方法，该方法可以直接从新合成视图预测全息图，而无需使用三维相机或三维图形处理管道。该方法包括三个主要部分：NeRF、深度预测器和全息图生成器，所有这些部分都是使用深度神经网络构建的。（4）方法性能：该方法在合理的时间内预测了从任何方向观看的三维场景的预测全息图。仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）本工作的主要意义：提出了基于神经辐射场（NeRF）的全息图生成方法，该方法可以直接从新合成视图预测全息图，无需使用三维相机或三维图形处理管道，为全息显示器的发展提供了新的思路。（2）文章的优缺点总结：</li><li>创新点：提出了基于 NeRF 的全息图生成方法，该方法无需三维场景数据，直接从合成视图预测全息图，简化了全息显示器的生成流程。</li><li>性能：仿真和实验结果表明，该方法可以生成高质量的全息图，并且比现有方法更有效。</li><li>工作量：该方法的实现需要大量的训练数据和计算资源，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-eb426bcf4ff137aa9adfa122cfe7a503.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6343dbdb7aebaa121558d05d8650d069.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ca137b835829d4a4eee9df8c8a93246.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c695400302eaf7b15d2075d6d9b58551.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dcd582021c5b9223214535016af9ad3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3397dddd9230a1b23f0336e517fb6f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5cf31914b41fb8442b5926209326359c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a4f42e681d33823bde779da3c7eba53f.jpg" align="middle"></details><h2 id="Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss"><a href="#Neural-Field-Classifiers-via-Target-Encoding-and-Classification-Loss" class="headerlink" title="Neural Field Classifiers via Target Encoding and Classification Loss"></a>Neural Field Classifiers via Target Encoding and Classification Loss</h2><p><strong>Authors:Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun</strong></p><p>Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target Encoding module and optimizing a classification loss. By encoding a continuous regression target into a high-dimensional discrete encoding, we naturally formulate a multi-label classification task. Extensive experiments demonstrate the impressive effectiveness of NFC at the nearly free extra computational costs. Moreover, NFC also shows robustness to sparse inputs, corrupted images, and dynamic scenes. </p><p><a href="http://arxiv.org/abs/2403.01058v1">PDF</a> ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables</p><p><strong>Summary</strong><br>神经场分类器框架通过预测颜色编码来替代神经场回归器中的回归目标，从而将神经场方法表述为分类任务而非回归任务。</p><p><strong>Key Takeaways</strong></p><ul><li>神经场方法本质上可以表述为分类任务。</li><li>神经场分类器框架通过目标编码模块将连续回归目标编码为高维离散编码。</li><li>将回归任务转换为分类任务不会增加显著的计算成本。</li><li>神经场分类器在稀疏输入、损坏图像和动态场景下表现出鲁棒性。</li><li>神经场分类器比神经场回归器更有效，并且可以轻松应用于现有神经场方法。</li><li>神经场分类器提供了一个新的视角来理解和设计神经场方法。</li><li>本研究为神经场方法的研究提供了新的方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Neural Field 分类器：目标编码和分类损失</li><li>作者：Xindi Yang、Zeke Xie、Xiong Zhou、Boyu Liu、Buhua Liu、Yi Liu、Haoran Wang、Yunfeng Cai、Mingming Sun</li><li>第一作者单位：北京交通大学交通数据分析与挖掘重点实验室</li><li>关键词：神经场、目标编码、分类损失、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2403.01058</li><li>摘要：(1) 研究背景：神经场方法在计算机视觉和计算机图形学中取得了很大进展，包括新视图合成和几何重建。现有神经场方法尝试预测一些基于坐标的连续目标值，例如神经辐射场 (NeRF) 中的 RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。(2) 过去方法及其问题：回归模型是否真的优于神经场方法的分类模型？本文从机器学习的角度探讨了神经场这个非常基本但被忽视的问题。该方法提出了一个新颖的神经场分类器 (NFC) 框架，该框架将现有神经场方法表述为分类任务而不是回归任务。提出的 NFC 可以通过使用新颖的目标编码模块并将分类损失最小化，轻松地将任意神经场回归器 (NFR) 转换为其分类变体。通过将连续回归目标编码为高维离散编码，自然地制定了一个多标签分类任务。(3) 本文提出的研究方法：广泛的实验表明，NFC 在几乎没有额外计算成本的情况下具有令人印象深刻的有效性。此外，NFC 还显示了对稀疏输入、损坏图像和动态场景的鲁棒性。(4) 方法在什么任务上取得了什么性能：该方法在以下任务上取得了以下性能：</li><li>新视图合成：在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li><li>表面重建：在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li><li><p>鲁棒性：NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。</p></li><li><p>方法：（1）：目标编码模块，将连续回归目标编码为高维离散编码；（2）：分类损失，使用交叉熵损失作为优化目标；（3）：二进制数目标编码，将颜色值编码为 8 位二进制数；（4）：逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。</p></li><li><p>结论：（1）：本工作探讨了神经场方法中一个非常基本但被忽视的问题：回归与分类。我们设计了一个新颖的神经场分类器（NFC）框架，该框架可以将现有的神经场方法表述为分类模型，而不是回归模型。广泛的实验表明，目标编码和分类损失可以显着提高大多数现有神经场方法在新视图合成和几何重建中的性能。此外，NFC 的改进对稀疏输入、图像噪声和动态场景具有鲁棒性。虽然我们的工作主要集中在 3D 视觉和重建上，但我们相信 NFC 是一个通用的神经场框架。我们相信探索和增强神经场的泛化性将非常有前景。（2）：创新点：</p></li><li>提出了一种新的神经场分类器（NFC）框架，该框架将现有神经场方法表述为分类任务，而不是回归任务。</li><li>设计了一种新颖的目标编码模块，将连续回归目标编码为高维离散编码。</li><li>使用交叉熵损失作为优化目标，并提出了一种逐位分类损失，对每个二进制位计算分类损失，权重随位值增加而增加。性能：</li><li>在新视图合成任务上，在 NeRF 数据集上，NFC 在 PSNR 和 SSIM 指标上优于 NeRF。</li><li>在表面重建任务上，在 ShapeNet 数据集上，NFC 在 Chamfer 距离和法向量一致性方面优于 NeRF。</li><li>NFC 对稀疏输入、损坏图像和动态场景表现出鲁棒性。工作量：</li><li>NFC 可以轻松地将任意神经场回归器 (NFR) 转换为其分类变体，几乎没有额外的计算成本。</li><li>目标编码模块和分类损失的实现相对简单，易于集成到现有的神经场方法中。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-33d7ddc258be3cc2226509c273b4d9b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5d935134ee8dff34576f093f0e4bd187.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e56f20cd07e166f0199df0193f095f54.jpg" align="middle"><img src="https://picx.zhimg.com/v2-aa381fc61520f7cb599b68ee654d61b5.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-03-09  DART Implicit Doppler Tomography for Radar Novel View Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/03/09/Paper/2024-03-09/3DGS/</id>
    <published>2024-03-09T10:24:05.000Z</published>
    <updated>2024-03-09T10:24:05.771Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-03-09-更新"><a href="#2024-03-09-更新" class="headerlink" title="2024-03-09 更新"></a>2024-03-09 更新</h1><h2 id="3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos"><a href="#3DGStream-On-the-Fly-Training-of-3D-Gaussians-for-Efficient-Streaming-of-Photo-Realistic-Free-Viewpoint-Videos" class="headerlink" title="3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos"></a>3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</h2><p><strong>Authors:Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, Wei Xing</strong></p><p>Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes from multi-view videos remains a challenging endeavor. Despite the remarkable advancements achieved by current neural rendering techniques, these methods generally require complete video sequences for offline training and are not capable of real-time rendering. To address these constraints, we introduce 3DGStream, a method designed for efficient FVV streaming of real-world dynamic scenes. Our method achieves fast on-the-fly per-frame reconstruction within 12 seconds and real-time rendering at 200 FPS. Specifically, we utilize 3D Gaussians (3DGs) to represent the scene. Instead of the na\”ive approach of directly optimizing 3DGs per-frame, we employ a compact Neural Transformation Cache (NTC) to model the translations and rotations of 3DGs, markedly reducing the training time and storage required for each FVV frame. Furthermore, we propose an adaptive 3DG addition strategy to handle emerging objects in dynamic scenes. Experiments demonstrate that 3DGStream achieves competitive performance in terms of rendering speed, image quality, training time, and model storage when compared with state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2403.01444v2">PDF</a> CVPR 2024 Accepted. Project Page: <a href="https://sjojok.github.io/3dgstream">https://sjojok.github.io/3dgstream</a></p><p><strong>Summary</strong><br>动态场景实时自由视点视频流方法3DGStream，利用3D高斯分布表示场景，通过神经网络变换缓存建模3D高斯分布的平移和旋转，实现每帧12秒内重建和200FPS实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>提出3DGStream方法，实现动态场景的实时自由视点视频流。</li><li>利用3D高斯分布表示场景，有效捕捉场景结构。</li><li>使用神经网络变换缓存建模3D高斯分布的平移和旋转，减少训练时间和存储需求。</li><li>提出自适应3D高斯分布添加策略，处理动态场景中的新增对象。</li><li>3DGStream在渲染速度、图像质量、训练时间和模型存储方面达到先进水平。</li><li>每帧重建时间12秒内，实时渲染速度200FPS。</li><li>模型存储空间小，有效降低计算成本。</li><li>适用于动态场景的实时自由视点视频流，拓展3D视觉应用领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：3DGStream：动态场景高效流式传输的 3D 高斯实时训练</li><li>作者：Yuxuan Zhang, Lingjie Liu, Wenbo Bao, Wenxiu Sun, Qionghai Dai</li><li>单位：北京理工大学</li><li>关键词：Free-Viewpoint Video、动态场景、流式传输、3D 高斯</li><li>论文链接：https://arxiv.org/pdf/2209.04734.pdfGithub 代码链接：None</li><li>摘要：（1）研究背景：构建动态场景的逼真自由视点视频（FVV）仍然是一项具有挑战性的任务。尽管当前的神经渲染技术取得了显着进步，但这些方法通常需要完整的视频序列进行离线训练，并且无法进行实时渲染。（2）过去方法：现有方法存在的问题：</li><li>离线训练：需要完整的视频序列，无法实时渲染。</li><li>存储开销：需要为每个 FVV 帧存储大量数据。</li><li>训练时间：训练过程耗时。</li><li>无法处理动态场景中出现的物体。（3）研究方法：</li><li>3D 高斯表示：使用 3D 高斯表示场景。</li><li>神经转换缓存（NTC）：使用 NTC 对 3D 高斯的平移和旋转进行建模，从而减少训练时间和存储需求。</li><li>自适应 3D 高斯添加策略：处理动态场景中出现的物体。（4）性能：</li><li>渲染速度：实时渲染，达到 200FPS。</li><li>图像质量：与最先进的方法相比具有竞争力的渲染质量。</li><li>训练时间：与最先进的方法相比，训练时间显著减少。</li><li><p>模型存储：与最先进的方法相比，模型存储需求显著减少。</p></li><li><p>方法：(1) 使用3D高斯表示场景，将场景表示为一系列3D高斯分布的叠加。(2) 使用神经转换缓存（NTC）对3D高斯的平移和旋转进行建模，从而减少训练时间和存储需求。(3) 提出自适应3D高斯添加策略，处理动态场景中出现的物体。</p></li><li><p>结论：（1）：提出 3DGStream，一种用于高效自由视点视频流的高效 3D 高斯实时训练方法。（2）：创新点：基于 3DG-S，利用神经转换缓存（NTC）捕捉物体运动；提出自适应 3DG 添加策略，准确建模动态场景中出现的物体。性能：实现即时训练（每帧约 10 秒）和实时渲染（约 200FPS），在百万像素分辨率下具有适度的存储需求。工作量：使用 3DG-S 的代码库实现 3DGStream，使用 tiny-cuda-nn 实现 NTC。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-56fa714ff2f8a27b5ea568d4ef616b5e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cf2d0d9167fc721c8b229c0141471c56.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e5a6c132c8a153da0f9bad3e8ca7eabd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-752f81f447063ef3902e3a021755740e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4cd01032696c0735dbb058f523ca0022.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-053adecfa0f0d915b2350de6633e2581.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-03-09  3DGStream On-the-Fly Training of 3D Gaussians for Efficient Streaming   of Photo-Realistic Free-Viewpoint Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
</feed>
