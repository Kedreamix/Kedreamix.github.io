<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-02-13T12:10:49.162Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/NeRF/</id>
    <published>2024-02-13T12:10:49.000Z</published>
    <updated>2024-02-13T12:10:49.162Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa</strong></p><p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene’s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data. </p><p><a href="http://arxiv.org/abs/2402.07310v1">PDF</a> </p><p><strong>Summary</strong><br>生物神经形态学启发的 NeRF 架构，融合多源输入，提取更本质相关信息，实现场景 3D 表示和新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>BioNeRF 是一种受生物神经形态学启发的架构，用于建模场景的 3D 表示并通过辐射场合成新视角。</li><li>BioNeRF 实现了一种认知启发的机制，将来自多个来源的输入融合到一个类似记忆的结构中，提高存储容量并提取更多内在和相关信息。</li><li>BioNeRF 模仿在锥体细胞中观察到的关于上下文信息的行为，其中记忆被提供为上下文并与两个后续神经模型的输入相结合，一个负责产生体积密度，另一个负责用于渲染场景的颜色。</li><li>实验结果表明，BioNeRF 在衡量人类感知的质量指标上优于最先进的结果，包括真实世界图像和合成数据两类数据集。</li><li>BioNeRF 在两个数据集上都优于最先进的结果，分别为真实世界图像和合成数据。</li><li>BioNeRF 在自由视角视频和全景视频的渲染上均取得了最先进的结果。</li><li>BioNeRF 在不同场景和条件下表现出鲁棒性和泛化性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p>1标题：《BioNeRF 生物合理神经辐射场的视图合成》(BioNeRF Biologically Plausable Neural Radiance Fields for View Synthesis)。</p><p></p><p></p><p>作者列表：(Leandro A Passos)、Douglas Rodrigues)、Danilo Jodas)、Kelton A P Costa)、João Paulo Papa)。</p><p></p><p></p><p>第一作者单位：(巴西 Bauru 市 Av Eng Luiz Edmundo Carrijo Coube 街十四之一栋 São Paulo State University)。</p><p></p><p></p><p>关键词：(神经渲染)、生物合理神经模型)。</p><p></p><p></p><p>链接：(Paper URL)。</p><p></p><p></p><p>Github代码链接：(Github None)。</p><p></p><p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p><p></p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p></p><ol><li><p>方法：（1）BioNeRF采用认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。（2）BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。（3）实验结果表明，BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。</p></li><li><p>结论：（1）：BioNeRF在神经渲染领域取得了重大突破，提出了一种新的生物合理神经辐射场架构，该架构能够利用辐射字段构建场景的三维表示形式并合成新的视图。（2）：创新点：</p></li><li>BioNeRF采用了一种认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。</li><li>BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。性能：</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。工作量：</li><li>BioNeRF的实现难度较高，需要较强的编程能力和数学基础。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a3147366d087ebe11e207f5d9173f950.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-91083b7a4d33cafbb989e6672e5d0690.jpg" align="middle"></details><h2 id="NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction"><a href="#NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction" class="headerlink" title="NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction"></a>NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction</h2><p><strong>Authors:Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</strong></p><p>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2402.05532v2">PDF</a> Accepted by 3DV 2024</p><p><strong>Summary</strong><br>手-物交互的自由视角逼真重建。</p><p><strong>Key Takeaways</strong></p><ul><li>手-物交互建模是计算机三维建模的挑战性任务。</li><li>现存方法无法真实地进行手-物交互建模。</li><li>提出 NCRF 框架来从视频中重建手-物交互。</li><li>NCRF 包括接触优化场和手-物的神经辐射场。</li><li>接触优化场预测三维查询点精确的接触场。</li><li>手-物的神经辐射场学习手-物隐式表示。</li><li>手-物运动场产生观察到标准的对应关系。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NCRF：用于手-物体交互自由视点渲染的神经接触辐射场</li><li>作者：Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</li><li>第一作者单位：伯明翰大学</li><li>关键词：手-物体交互、自由视点渲染、神经辐射场、接触场优化</li><li>论文链接：https://arxiv.org/abs/2402.05532</li><li><p>摘要：（1）研究背景：手-物体交互建模是计算机视觉中一项极具挑战性的任务。尽管该领域取得了显着进展，但现有方法仍然无法以逼真的方式合成手-物体交互，这源于手和物体之间严重的相互遮挡以及不准确的手-物体姿态估计，从而导致渲染质量下降。（2）过去方法及其问题：以往工作通常将此任务表述为联合手和物体姿态估计问题，并依赖参数化的手-物体模型（如 MANO 和 YCB）来估计手的运动变换。然而，现有方法难以恢复手-物体接触场的准确几何形状，并且渲染质量受到遮挡和姿态估计误差的严重影响。（3）本文提出的研究方法：为了解决这些挑战，我们提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。NCRF 框架主要由两个关键组件组成：（a）接触优化场：从 3D 查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。（4）方法在什么任务上取得了怎样的性能，这些性能是否支持了它们的目标：在 HO3D 和 Dex-YCB 数据集上的广泛实验表明，我们的方法在渲染质量和姿态估计精度方面均优于当前最先进的方法。这些性能支持了我们的目标，即以逼真的方式重建和渲染手-物体交互。</p></li><li><p>Methods:(1): 本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。(2): NCRF框架主要由两个关键组件组成：（a）接触优化场：从3D查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。(3): 我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。</p></li><li><p>结论：（1）：本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。NCRF框架通过设计动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。（2）：创新点：</p></li><li>提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。</li><li>设计了动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。</li><li>提出了一种新的手-物体变形模块，该模块能够将射线变形到规范空间中，并以逼真的方式渲染手-物体交互。性能：</li><li>在HO3D和Dex-YCB数据集上的广泛实验表明，NCRF框架在渲染质量和姿态估计精度方面均优于当前最先进的方法。</li><li>NCRF框架能够生成逼真的新视角合成，并且能够很好地处理具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。工作量：</li><li>NCRF框架的实现相对复杂，需要较高的计算资源。</li><li>NCRF框架的训练过程需要较长时间，并且需要大量的数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-19c080ef42e2fcaa0595e65274d339b5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b7f0899ff9371cac98ca44ab3913a349.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1403a98bc963e537484ce413bb5d32ea.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v2">PDF</a> </p><p><strong>Summary</strong><br>对于大场景下的重建任务，本文引入 BirdNeRF，该方法能够有效利用无人机影像数据实现高效低存储的大场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>BirdNeRF 是一款针对航空图像的大场景重建方法，解决了以往小场景重建存在的训练慢、渲染慢、模型容量小等问题。</li><li>BirdNeRF 提出了一种基于鸟瞰视角的姿势分解算法，将大场景图像集分解成多个小场景子集，每个子集使用单独的 NeRF 进行训练。</li><li>BirdNeRF 采用了一种新颖的投影引导式新视角重新渲染策略，可以有效利用独立训练的子场景生成更好的渲染结果。</li><li>BirdNeRF 在现有数据集和我们自己的无人机数据上进行了评估，在单个 GPU 上的重建速度比经典摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，且渲染质量相似。</li><li>BirdNeRF 可以在任意大的场景中无缝扩展，并支持对环境的局部更新，提高了重建过程的灵活性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：BirdNeRF：基于航拍图像的大场景快速神经重建</li><li>作者：张惠卿、薛一菲、廖明、老义珍</li><li>单位：无</li><li>关键词：NeRF、大场景重建、航拍图像、空间分解、投影引导</li><li>链接：无，Github 链接：无</li><li><p>摘要：（1）研究背景：大场景三维重建是摄影测量和遥感领域的一项重要任务，可以利用航拍或卫星图像、激光雷达数据和街景图像等多种数据源构建城市的三维模型。近年来，基于图像的三维重建技术取得了很大的进展，并在城市规划、导航、虚拟旅游、房地产、灾害管理和历史保护等领域得到了广泛的应用。（2）过去方法与问题：现有的基于图像的三维重建技术主要分为传统几何方法和基于神经网络的方法。传统几何方法主要包括摄影测量和激光扫描，这些方法可以生成高精度的三维模型，但需要大量的人工参与和昂贵的设备。基于神经网络的方法，如神经辐射场（NeRF），可以从图像中自动学习三维场景的表示，但这些方法通常需要大量的训练数据和计算资源，并且在大场景重建任务中容易出现伪影和低视觉保真度的问题。（3）研究方法：为了解决上述问题，本文提出了一种新的基于 NeRF 的大场景重建方法，称为 BirdNeRF。BirdNeRF 采用了一种新的鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。这种分解方法不仅可以减少训练和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。（4）性能与目标：BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。这些结果证明了 BirdNeRF 的有效性和实用性。</p></li><li><p>方法：（1）场景分解：将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。（2）视角姿势分解：采用鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景。（3）新视角重新渲染：提出一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。</p></li><li><p>结论：（1）：本文提出了一种新的基于 NeRF 的大场景重建方法 BirdNeRF，该方法采用鸟瞰视角姿势分解算法和投影引导的新视角重新渲染策略，可以有效地解决大场景重建任务中容易出现伪影和低视觉保真度的问题。（2）：创新点：</p></li><li>提出了一种新的鸟瞰视角姿势分解算法，可以将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型，从而减少训练和渲染时间，提高重建质量。</li><li>提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。性能：</li><li>BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。工作量：</li><li>BirdNeRF 的实现相对简单，并且可以在单个 GPU 上训练和渲染。然而，由于需要对大场景图像进行分解，因此 BirdNeRF 的预处理时间可能会比较长。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-daadce77f0b48dc25dd984f5c66ee7ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6d52642c6cfdc84439f5ea843cff2fd1.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-13  BioNeRF Biologically Plausible Neural Radiance Fields for View   Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/3DGS/</id>
    <published>2024-02-13T11:47:50.000Z</published>
    <updated>2024-02-13T11:47:50.666Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting"><a href="#GALA3D-Towards-Text-to-3D-Complex-Scene-Generation-via-Layout-guided-Generative-Gaussian-Splatting" class="headerlink" title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting"></a>GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</h2><p><strong>Authors:Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</strong></p><p>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize large language models (LLMs) to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the LLMs to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at <a href="https://gala3d.github.io/">https://gala3d.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.07207v1">PDF</a> </p><p><strong>Summary</strong></p><p>利用大型语言模型 (LLM) 生成初始布局，并引入布局引导 3D 高斯表示，指导 3D 内容生成，同时满足适应性几何约束。</p><p><strong>Key Takeaways</strong></p><ul><li>GALA3D 将大型语言模型与布局引导 3D 高斯表示相结合，用于有效地进行文本到 3D 的生成。</li><li>布局引导 3D 高斯表示提供了自适应的几何约束，确保生成的 3D 内容具有真实感和一致性。</li><li>GALA3D 采用对象-场景组合优化机制，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li><li>GALA3D 可以同时调整从大型语言模型中提取的粗略布局，使其与生成的场景保持一致。</li><li>GALA3D 是一个用户友好的端到端框架，可进行最先进的场景级 3D 内容生成和可控编辑。</li><li>GALA3D 能够确保场景中对象级实体的高保真度。</li><li>GALA3D 的源代码和模型可从 <a href="https://gala3d.github.io/">https://gala3d.github.io/</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GALA3D：基于布局引导的文本到 3D 复杂场景生成</li><li>作者：Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang</li><li>第一作者单位：北京大学万选信息技术学院</li><li>关键词：文本到 3D、生成式高斯体素、布局引导、条件扩散</li><li>论文链接：https://arxiv.org/abs/2402.07207    Github 链接：None</li><li>摘要：(1) 研究背景：文本到 3D 生成旨在根据自然语言描述生成逼真的 3D 场景。现有方法要么产生低质量的纹理、视觉伪影和几何失真，要么无法根据文本准确生成多个对象及其交互。(2) 过去的方法：现有方法主要分为两类：基于体素的方法和基于网格的方法。基于体素的方法虽然可以生成高质量的 3D 场景，但计算成本高昂。基于网格的方法虽然计算成本较低，但生成的 3D 场景质量较差。(3) 研究方法：本文提出了一种基于生成式高斯体素的文本到 3D 生成方法。该方法首先利用大型语言模型生成初始布局，然后引入布局引导的 3D 高斯体素表示来生成 3D 内容。接着，提出了一种对象场景组合优化机制，该机制利用条件扩散来协同生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景，同时调整从大型语言模型中提取的粗略布局先验，使其与生成的场景对齐。(4) 实验结果：实验表明，GALA3D 是一个用户友好的端到端框架，可用于生成高质量的 3D 场景。该方法在多个数据集上取得了最先进的性能，并且能够支持交互式可控编辑。</li></ol><p>Methods:</p><p>(1) 粗略布局先验：利用大型语言模型（LLM）从文本描述中提取粗略布局先验，包括对象实例及其对应的位置、尺寸和方向。</p><p>(2) 布局引导的高斯体素表示：将粗略布局先验转换为布局引导的高斯体素表示，其中每个对象实例由一组高斯体素表示，高斯体素的位置、尺寸和方向由布局先验决定。</p><p>(3) 自适应几何控制：对高斯体素的分布和形状进行自适应几何控制，以确保高斯体素的分布紧密贴合对象表面，并且形状更加规则和一致。</p><p>(4) 具有扩散先验的组合优化：采用具有扩散先验的组合优化策略来更新布局引导的高斯体素参数，包括多视图扩散优化和场景条件扩散优化，以生成具有统一风格和交互关系的对象实例。</p><p>(5) 布局损失：引入布局损失来确保生成的3D场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。</p><ol><li>结论：（1）：GALA3D 是一种基于生成式布局引导的 3D 高斯体素表示的场景级文本到 3D 框架，该框架可以生成具有多个对象的高保真、3D 一致的场景。实验表明，该方法在文本到 3D 生成方面优于现有方法，展示了生成具有多个对象和交互的复杂场景的能力，并实现了出色的纹理和几何效果。该方法还促进了交互式和可控的场景编辑，实现了一个高效且用户友好的 3D 场景生成和编辑框架。（2）：创新点：</li><li>提出了一种基于生成式布局引导的 3D 高斯体素表示，该表示可以生成具有统一风格和交互关系的对象实例。</li><li>引入了一种具有扩散先验的组合优化策略，该策略可以更新布局引导的高斯体素参数，以生成具有真实几何形状、纹理、比例和准确交互的多对象 3D 场景。</li><li>提出了一种布局损失，该损失可以确保生成的 3D 场景与布局先验在语义和空间上的一致性，从而提高场景的整体质量。性能：</li><li>在多个数据集上取得了最先进的性能。</li><li>能够支持交互式可控编辑。工作量：</li><li>该方法的实现相对复杂，需要大量的计算资源。</li><li>该方法需要大量的数据进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3dde3c6bf6237679d7dc8e3a25b014e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c780f9b8f1b542e9c562c2d185d7e16a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e76b694075c9297c3e8a8d38bf4c8fe3.jpg" align="middle"></details><h2 id="GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data"><a href="#GS-CLIP-Gaussian-Splatting-for-Contrastive-Language-Image-3D-Pretraining-from-Real-World-Data" class="headerlink" title="GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data"></a>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D   Pretraining from Real-World Data</h2><p><strong>Authors:Haoyuan Li, Yanpeng Zhou, Yihan Zeng, Hang Xu, Xiaodan Liang</strong></p><p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object’s surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results. </p><p><a href="http://arxiv.org/abs/2402.06198v1">PDF</a> 6-page technical report</p><p><strong>Summary</strong><br>3D 高斯曲面表示增强多模态预训练， 促进图像、语言和 3D 表示的统一。</p><p><strong>Key Takeaways</strong></p><ul><li>多模态预训练在图像和语言描述的对齐方面取得进展，对物体识别、分类和检索至关重要。</li><li>点云的离散表示丢失了物体的表面形状信息，导致渲染结果与 2D 对应关系之间存在差距。</li><li>提出 GS-CLIP 首次将 3DGS（3D 高斯曲面）引入多模态预训练，以增强 3D 表示。</li><li>GS-CLIP 利用预训练的视觉语言模型，在大规模真实世界图像文本对上学习共同的视觉和文本空间，然后学习一个 3D 编码器来对齐针对每个对象优化的 3DGS。</li><li>提出了一种新颖的 Gaussian-Aware Fusion 来提取和融合全局显式特征。</li><li>作为语言图像 3D 预训练的通用框架，GS-CLIP 与 3D 主干网络无关。</li><li>具有挑战性的实验表明，GS-CLIP 显着改善了最先进的技术，优于先前最好的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：GS-CLIP：用于对比语言-图像-3D 预训练的高斯溅射</li><p></p><p></p><li>作者：李浩源、周彦鹏、曾义涵、许航、梁晓丹</li><p></p><p></p><li>第一作者单位：中山大学深圳校区</li><p></p><p></p><li>关键词：3D 表示、高斯溅射、对比学习、多模态预训练</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2402.06198，Github 链接：无</li><p></p><p></p><li>摘要：(1)：研究背景：3D 形状以点云表示在多模态预训练中取得了进展，用于对齐图像和语言描述，这对于物体识别、分类和检索至关重要。然而，点云的离散表示丢失了物体的表面形状信息，并在渲染结果和 2D 对应关系之间产生差距。(2)：过去的方法和问题：现有的 3D 表示学习方法主要对点云的关键点位置信息进行建模，这限制了 3D 视觉理解和 3D 表示学习的性能。(3)：研究方法：为了解决上述问题，本文提出了 GS-CLIP，将 3D 高斯溅射 (3DGS) 引入多模态预训练，以增强 3D 表示。GS-CLIP 利用预训练的视觉语言模型在真实世界的大规模图像-文本对上学习一个共同的视觉和文本空间，然后学习一个 3D 编码器，用于对齐针对每个对象优化的 3DGS。此外，本文还提出了一种新的高斯感知融合，用于提取和融合全局显式特征。(4)：实验结果：在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。实验结果表明，3DGS 在跨模态学习中具有强大的表示能力。</li><br>&lt;/ol&gt;<p></p><p>7.方法：（1）跨模态预训练：为了对齐文本、图像和3DGS的多模态表示，GS-CLIP采用预训练的语言-图像模型CLIP，形成一个共同的语言-图像潜在空间，作为3DGS的目标潜在空间。对于零样本/开放词识别，通过冻结CLIP文本编码器、图像编码器和公共真实世界潜在空间，保证了3DGS表示的可迁移性。具体来说，我们借鉴了[19, 28]中的对比损失，并形成文本-3DGS对齐和图像-3DGS对齐，用于多模态对齐。（2）高斯感知融合：虽然将点云投影到3D体素的3D骨干可以更好地学习全局位置和特征，但我们发现3DGS的显式特征会被忽略，因为体素化丢失了形状和纹理信息。因此，我们采用基于Transformer的分支直接对高斯特征建模为高斯特征上下文，并以残差形式注入它。具体来说，给定具有n个高斯的3DGS输入XG∈Rn×14，我们首先将XG分成Ng组，用于XgroupG=Ng�g=1XgG，然后使用基于卷积的体系结构EGθ,c提取全局特征fGc和基于Transformer的体系结构EGθ,t提取显式高斯特征fGG，最后将fGc和fGG连接起来，形成最终的3DGS表示fG。</p><ol><li>结论：（1）意义：本文首次提出 GS-CLIP，将 3DGS 纳入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。为此，提出了一种高斯感知融合，以便从补充信息中更好地学习信息。我们证明了我们提出的 GS-CLIP 在最先进的方法中取得了优异的性能。（2）优缺点：创新点：</li><li>将 3DGS 引入跨模态学习，作为补充形状和纹理信息的通用 3D 表示。</li><li>提出了一种高斯感知融合，以便从补充信息中更好地学习信息。</li></ol><p>性能：- 在 SUN-RGBD 数据集上的实验表明，GS-CLIP 在真实世界环境中的零样本/开放词学习中取得了优异的性能。</p><p>工作量：- 需要对 3DGS 进行预训练，这可能需要大量的数据和计算资源。- 需要对高斯感知融合进行训练，这可能需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5ca02e3188a2350914f961c6e31c0616.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4980273838b01e0c94c7593c3becb878.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b33d684beebaf5252e0357a0e0af9c1d.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-13  GALA3D Towards Text-to-3D Complex Scene Generation via Layout-guided   Generative Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Talking%20Head%20Generation/</id>
    <published>2024-02-13T11:37:33.000Z</published>
    <updated>2024-02-13T11:37:33.323Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer"><a href="#DiffSpeaker-Speech-Driven-3D-Facial-Animation-with-Diffusion-Transformer" class="headerlink" title="DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer"></a>DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion   Transformer</h2><p><strong>Authors:Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</strong></p><p>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel. </p><p><a href="http://arxiv.org/abs/2402.05712v1">PDF</a> 9 pages, 5 figures. Code is avalable at   <a href="https://github.com/theEricMa/DiffSpeaker">https://github.com/theEricMa/DiffSpeaker</a></p><p><strong>Summary</strong><br>通过提出带偏条件注意力的扩散模型，我们解决了音视频配对数据的稀缺问题，在保持音视频同步的情况下，可以快速生成高质量的面部动画。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新的扩散模型，用于语音驱动的3D面部动画。</li><li>使用带偏条件注意力模块，可以更好地处理音视频配对数据的稀缺问题。</li><li>在现有基准上取得了最先进的性能。</li><li>可以快速生成面部动画，推理速度快。</li><li>可以有效地生成非语言面部表情。</li><li>可以控制动画过程中的嘴型同步和非语言面部表情之间的权衡。</li><li>该模型可以用于各种多媒体应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：DiffSpeaker：基于扩散变换器的语音驱动 3D 面部动画（DiffSpeaker: Speech-Driven 3DFacial Animation with Diffusion Transformer）</li><li>作者：Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei</li><li>第一作者单位：香港理工大学（香港理工大学）</li><li>关键词：语音驱动面部动画、扩散模型、Transformer、条件注意机制</li><li>论文链接：https://arxiv.org/abs/2402.05712Github 代码链接：https://github.com/theEricMa/DiffSpeaker</li><li><p>摘要：（1）：语音驱动 3D 面部动画在许多多媒体应用中非常重要。最近的研究表明，使用扩散模型或 Transformer 架构来执行此任务很有前景。然而，它们的简单聚合并不能带来改进的性能。我们怀疑这是由于缺乏配对的音频-4D 数据，这对于 Transformer 在扩散框架内有效地执行去噪器至关重要。（2）：过去的方法主要使用基本的滑动窗口方法处理音频输入，这通常会导致生成的的面部动作范围狭窄。近年来，研究人员开始采用 Transformer 架构来进行语音驱动 3D 面部动画，但传统的确定性回归可能不是最好的方法，因为人类的语音和面部表情是可变且动态的，很难用一个固定的映射来准确捕捉它们之间的关系。（3）：为了解决这个问题，我们提出了 DiffSpeaker，这是一个基于 Transformer 的网络，配备了新颖的偏置条件注意机制模块。这些模块可以替代标准 Transformer 中传统的自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。我们还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡。（4）：实验表明，我们的模型不仅在现有基准上实现了最先进的性能，而且由于其能够并行生成面部动作，因此推理速度也很快。</p></li><li><p>方法：(1): 本文提出了一种基于扩散模型和Transformer架构的语音驱动3D面部动画方法DiffSpeaker，该方法利用了扩散模型的生成能力和Transformer架构的序列建模能力，有效地解决了语音驱动3D面部动画任务中的配对音频-4D数据缺乏的问题。(2): DiffSpeaker采用了一种新颖的偏置条件注意机制模块，该模块可以替代标准Transformer中的传统自注意力/交叉注意力，并结合了经过精心设计的偏置，这些偏置引导注意力机制集中在相关的特定任务和与扩散相关的条件上。(3): DiffSpeaker还探索了在扩散范式中准确的唇形同步和非语言面部表情之间的权衡，并提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。(4): 实验结果表明，DiffSpeaker在现有基准上实现了最先进的性能，并且由于其能够并行生成面部动作，因此推理速度也很快。</p></li><li><p>结论：（1）：本工作探索了将 Transformer 架构与基于扩散的框架有效结合用于语音驱动 3D 面部动画的方法。我们贡献的核心是引入了带有偏置的条件自注意力/交叉注意力机制，该机制解决了使用受限且跨度短的音频-4D 数据训练基于扩散的 Transformer 的困难。我们还研究了在实现准确的唇形同步和生成与语音相关性较小的面部表情之间的平衡。我们开发的模型优于当前的方法，在动画质量和生成速度方面都表现出色。（2）：创新点：提出了一种新的带有偏置的条件自注意力/交叉注意力机制，该机制可以有效地利用受限且跨度短的音频-4D 数据来训练基于扩散的 Transformer。提出了一种新的损失函数，该损失函数可以同时优化唇形同步和非语言面部表情的质量。性能：在现有基准上实现了最先进的性能。推理速度快，能够并行生成面部动作。工作量：需要收集和预处理大量的音频-4D 数据。需要对模型进行大量的训练。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a6a1095c49476b6d0a24c660e7abca7e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2c694a105e50cf1ba9a9e0743f793c62.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1dd59be8351677e84215dd037093b2ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ebb8f9ab10ef4d053668941b0c247fcb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ff2d1ca9215127e6894689d494fb8244.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-13  DiffSpeaker Speech-Driven 3D Facial Animation with Diffusion   Transformer</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/13/Paper/2024-02-13/Diffusion%20Models/</id>
    <published>2024-02-13T11:31:26.000Z</published>
    <updated>2024-02-13T11:31:26.322Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models"><a href="#Synthesizing-CTA-Image-Data-for-Type-B-Aortic-Dissection-using-Stable-Diffusion-Models" class="headerlink" title="Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models"></a>Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</h2><p><strong>Authors:Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah</strong></p><p>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of Generative AI thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by fine-tuning stable diffusion models based on user defined text prompts, using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions. </p><p><a href="http://arxiv.org/abs/2402.06969v1">PDF</a> Submitted in IEEE EMBC 2024 Conference</p><p><strong>Summary</strong><br>稳定扩散模型在医学成像数据合成中展现出强大能力，有望解决数据稀缺问题，助力心血管图像处理领域的发展。</p><p><strong>Key Takeaways</strong></p><ul><li>稳定扩散模型在医学成像数据合成中展现出巨大潜力，可用于解决数据稀缺问题。</li><li>通过微调用户定义文本提示的稳定扩散模型，仅使用有限数量的 CTA 图像作为输入，即可生成合成的冠状动脉 CTA 图像。</li><li>定量分析和定性评估相结合的综合评估表明，使用文本到图像 (T2I) 稳定扩散模型可以成功生成心脏 CTA 图像。</li><li>微调的 T2I CTA 扩散模型能够生成具有急性 B 型主动脉夹层 (TBAD) 医学特征的图像。</li><li>合成的图像在视觉上与真实图像相似，并保留了真实图像中的关键解剖结构。</li><li>临床医生认为合成的图像具有足够的质量，可用于临床实践。</li><li>该研究表明，稳定扩散模型在医学成像数据合成中具有广阔的应用前景，有望改善心血管疾病的诊断和治疗。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用稳定扩散模型合成 B 型主动脉夹层断层扫描图像数据</li><li>作者：Ayman Abaid、Muhammad Ali Farooq、Niamh Hynes、Peter Corcoran 和 Ihsan Ullah</li><li>第一作者单位：爱尔兰戈尔韦大学计算机科学学院</li><li>关键词：主动脉夹层、计算机断层扫描血管造影、医学图像合成、稳定扩散模型、文本到图像</li><li>论文链接：https://arxiv.org/abs/2402.06969Github 代码链接：无</li><li><p>摘要：(1)：研究背景：主动脉夹层是一种严重的心血管疾病，需要准确和及时的诊断。计算机断层扫描血管造影 (CTA) 是诊断主动脉夹层最常用的成像方式，但由于数据稀缺，机器学习算法在主动脉夹层图像处理中的能力受到限制。(2)：过去的方法：过去的研究使用深度学习模型来自动分割主動脈夾層圖像中的真腔、假腔和假腔血栓。然而，这些模型通常需要大量的数据进行训练，而主动脉夹层的数据集往往很小。(3)：研究方法：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法。稳定扩散模型是一种生成式人工智能模型，可以根据文本提示生成逼真的图像。本研究通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像。(4)：方法性能：实验结果表明，微调后的稳定扩散模型能够生成具有主动脉夹层典型特征的图像。定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</p></li><li><p>方法：(1) 数据预处理：将 3D CTA 图像转换为 2D 图像，并将其划分为训练集、测试集和验证集。将数据分为五类：有真腔 (TL) 的图像、有假腔 (FL) 的图像、有假腔血栓 (FLT) 的图像、有 TL 和 FL 的图像，以及无 TL、FL 和 FLT 信息的数据。(2) 文本到图像 (T2I) 模型训练：使用 ImageTBAD 数据集和 DreamBooth 训练工具微调预训练的稳定扩散模型，以生成高质量的 CTA 数据。在训练过程中，为每类数据分配专门的文本提示，并为后续类别的特定类提供否定提示。(3) 图像采样：使用欧拉和欧拉 A 图像采样器从潜空间的不同区域采样，以生成多样化和逼真的图像。(4) 数据增强：使用独特的文本提示渲染具有类别分布的 CT 数据，以增强具有特定 CT 特征的数据，例如 TL、FL 和 FLT。(5) 模型评估：使用 Fréchet Inception Distance (FID) 和 Multiscale Structural Similarity Index Measure (MS-SSIM) 评估合成图像的质量和多样性。训练 SoTA 模型（例如 UNet）对合成图像进行分割，以评估其实用性。</p></li><li><p>结论：（1）：本研究提出了一种使用稳定扩散模型合成主动脉夹层 CTA 图像的方法，该方法能够生成具有主动脉夹层典型特征的图像，并且可以用于训练深度学习模型进行主动脉夹层图像分割。（2）：创新点：</p></li><li>使用稳定扩散模型合成主动脉夹层 CTA 图像，这是首次将稳定扩散模型应用于主动脉夹层图像合成。</li><li>通过微调稳定扩散模型，使其能够根据用户定义的文本提示生成主动脉夹层 CTA 图像，这使得图像合成过程更加灵活和可控。</li><li>合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割，这表明该方法具有实际应用价值。性能：</li><li>定量分析和定性评估都表明，合成的图像具有很高的质量，并且可以用于训练深度学习模型进行主动脉夹层图像分割。</li><li>训练的 SoTA 模型（例如 UNet）对合成图像进行分割，获得了良好的分割精度，这表明该方法合成的图像具有很高的实用性。工作量：</li><li>该方法需要对稳定扩散模型进行微调，这需要一定的计算资源和时间。</li><li>该方法需要对数据进行预处理，这需要一定的人工劳动。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-66ad8c9bd4b7c6c0abc54d425f5bff3e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7eb93cb5e3a23926b4fa972f1f7e5a2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed3565ac4c49d72e02f85632488a4e3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9c0c6793d4532774c78760ad1a11631e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b3d53880237f6c704914112c0392f627.jpg" align="middle"></details><h2 id="Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation"><a href="#Improving-2D-3D-Dense-Correspondences-with-Diffusion-Models-for-6D-Object-Pose-Estimation" class="headerlink" title="Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation"></a>Improving 2D-3D Dense Correspondences with Diffusion Models for 6D   Object Pose Estimation</h2><p><strong>Authors:Peter Hönig, Stefan Thalhammer, Markus Vincze</strong></p><p>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models. </p><p><a href="http://arxiv.org/abs/2402.06436v1">PDF</a> Submitted to the First Austrian Symposium on AI, Robotics, and Vision   2024</p><p><strong>Summary</strong><br>扩散模型在图像到图像转换任务中表现优于生成对抗网络，在 6D 目标位姿估计任务中具有潜在优势。</p><p><strong>Key Takeaways</strong></p><ul><li>估计 RGB 图像和 3D 空间之间的 2D-3D 对应关系是 6D 目标位姿估计中的一个基本问题。</li><li>当前，密集对应图是使用基于 GAN、自动编码器或直接回归模型的图像到图像转换模型估计的。</li><li>最近，图像到图像转换领域的最新进展已使扩散模型成为在基准数据集上评估时的优越选择。</li><li>在这项研究中，我们比较了基于 GAN 和扩散模型的图像到图像转换网络，用于 6D 目标位姿估计的下游任务。</li><li>我们的结果表明，基于扩散的图像到图像转换模型优于 GAN，表明 6D 目标位姿估计模型有进一步改进的潜力。</li><li>扩散模型在图像到图像转换任务中具有更高的准确性和鲁棒性。</li><li>扩散模型在 6D 目标位姿估计任务中具有潜在优势，可以进一步提高位姿估计的准确性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：使用扩散模型改进 2D-3D 密集对应以进行 6D 目标位姿估计</li><li>作者：Peter Hönig、Stefan Thalhammer、Markus Vincze</li><li>作者单位：奥地利维也纳工业大学自动化与控制研究所</li><li>关键词：6D 目标位姿估计、2D-3D 密集对应、扩散模型、图像到图像翻译</li><li>论文链接：https://arxiv.org/abs/2402.06436</li><li>摘要：（1）研究背景：6D 目标位姿估计是许多感知任务的基础，例如自动驾驶、增强现实、创建数字孪生或机器人抓取。RGB-D 传感器可以同时提供颜色和深度数据，但并不总是可用。深度数据也容易受到噪声和其他失真的影响，这些失真通常由场景中闪亮、金属和透明物体反射引起。为了解决这个问题，人们考虑仅使用 RGB 图像进行位姿估计。最先进的方法依赖于估计 RGB 图像和 3D 对象模型之间的 2D-3D 密集对应。尽管这些方法擅长推断具有高可见性的对象位姿，但它们仍然面临着由杂波、遮挡、图像失真和闪亮物体表面带来的重大挑战。（2）过去的方法：过去，人们通过使用直接回归、生成对抗网络 (GAN) 和 U-Net 架构的组合或编码器-解码器卷积神经网络 (CNN) 来解决位姿估计的 2D-3D 对应问题。上述方法估计的密集对应图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。然而，这些方法在处理遮挡、杂波和具有挑战性的材料特性时存在困难。（3）论文方法：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计 2D-3D 密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将 RGB 图像作为输入，并生成一个密集对应图，该图包含从 RGB 图像到 3D 模型的每个像素的 3D 坐标。（4）实验结果：本文的方法在 YCB-Video 数据集上进行了评估。实验结果表明，本文的方法在 6D 目标位姿估计任务上优于基于 GAN 的图像到图像翻译网络。这表明扩散模型在 6D 目标位姿估计任务中具有潜力。</li></ol><p><strong>方法</strong>：</p><p>（1）图像到图像翻译模型：本文提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。扩散模型是一种生成模型，它通过逐渐添加噪声并逐渐减少噪声来生成图像。本文的方法将RGB图像作为输入，并生成一个密集对应图，该图包含从RGB图像到3D模型的每个像素的3D坐标。</p><p>（2）位置先验：为了获得2D位置先验，本文使用2D目标检测器从RGB图像中裁剪感兴趣区域（ROI）。ROI是图像到图像翻译模型的输入。图像到图像翻译模型学习从RGB裁剪中估计2D-3D密集对应。2D目标检测器用一个矩形边界框裁剪对象。因此，对象没有被完全裁剪，背景像素仍然存在。因此，图像到图像翻译网络的学习目标是双重的。网络的主要目标是学习估计2D-3D密集对应。同时，网络需要隐式地学习如何将对象从背景中分割出来。RANSAC+PnP步骤从密集对应图中估计6D目标位姿。</p><p>（3）数据增强：为了生成图像到图像翻译任务的训练数据，对象网格被归一化，以适应无量纲的1x1x1立方体。然后，根据顶点在归一化对象坐标空间中的XYZ位置，用RGB值对对象网格的顶点进行着色。然后，使用真实平移、旋转和相机内参对归一化和着色的网格进行渲染。</p><p>（4）图像到图像翻译算法：本文比较了两种图像到图像翻译算法，即GAN模型Pix2Pix和扩散模型BBDM。位置先验和RANSAC+PnP步骤对于这两种方法都是相同的，只有图像到图像翻译函数IDC=F(IRGB)是不同的。两种模型都在相同条件下进行训练。首先，模型在没有任何数据增强的情况下进行训练，除了将ROI裁剪调整为128x128像素，这是两个模型的输入和输出大小。在第二次训练运行中，两个模型都使用相同的数据增强参数进行训练，如表1所示。对于每次运行，两个模型都训练40个epoch。</p><p>（5）数据集：本文在LMO数据集上评估图像到图像翻译模型。它具有8个在随机域中采样的家用物体和50000张合成渲染的图像。这些合成渲染的图像仅用于训练。为了评估，使用了1214张真实世界的测试图像。</p><p>（6）位置先验：使用两组预先计算的位置先验进行对象裁剪。我们使用来自2023年目标位姿估计（BOP）挑战赛基准的YOLOx检测结果来评估位姿估计的下游任务。为了评估对象分割，使用真实位置先验。</p><p>（7）评估指标：本文评估了估计的6D位姿的质量，以及2D-3D密集对应图和对象分割的质量。6D目标位姿使用ADD(-S)分数进行评估。ADD(-S)是指模型点m之间的平均距离，对于kmd≥m。误差阈值km用10%定义。公式1中显示了m的计算。R和T表示真实旋转和平移，而^R和^T表示估计旋转和平移，而x表示模型M中的模型点。为了将位姿估计结果与其他方法进行比较，我们依靠2023年目标位姿估计（BOP）挑战赛基准计算的平均召回率。该AR分数是可见表面差异（VSD）、最大对称感知表面距离（MSSD）和最大对称感知投影距离（MSPD）的平均召回率的平均值。</p><ol><li>结论：（1）：本文提出了一个基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。我们比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。我们的实验表明，扩散模型在估计2D-3D密集对应图的质量方面优于GAN。（2）：创新点：</li><li>提出了一种基于扩散模型的图像到图像翻译网络，用于估计2D-3D密集对应。</li><li>比较了GAN模型Pix2Pix和扩散模型BBDM在相同训练条件下的性能。性能：</li><li>扩散模型在估计2D-3D密集对应图的质量方面优于GAN。</li><li>在YCB-Video数据集上，扩散模型在6D目标位姿估计任务上优于基于GAN的图像到图像翻译网络。工作量：</li><li>需要收集和预处理大量的数据。</li><li>需要训练两个图像到图像翻译模型。</li><li>需要对估计的2D-3D密集对应图进行后处理。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d77ba14fed7eddde5b06eaba6ff57afd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77c4e5753a8cd6ab35f73ede239b04a7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8d82762ff5fc78409df5e252c8a6442.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8833866e4d976d23589211a0d2587b35.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3c7c60e43fae13c906596978f0558ac8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-68bedb16f322fb5603066efd18ca6348.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3a909299ddc09a5143e9d208d38ac851.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ffb421ba600650f3eb815efb8fb9a80.jpg" align="middle"></details><h2 id="Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion"><a href="#Animated-Stickers-Bringing-Stickers-to-Life-with-Video-Diffusion" class="headerlink" title="Animated Stickers: Bringing Stickers to Life with Video Diffusion"></a>Animated Stickers: Bringing Stickers to Life with Video Diffusion</h2><p><strong>Authors:David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu</strong></p><p>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second. </p><p><a href="http://arxiv.org/abs/2402.06088v1">PDF</a> </p><p><strong>Summary</strong><br>文本提出一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</p><p><strong>Key Takeaways</strong></p><ul><li>引入了一种带有动画贴纸的视频扩散模型，该模型可根据文本提示和静态贴纸图像来生成动画。</li><li>该模型建立在最先进的 Emu 文本到图像模型的基础上，并添加了时间层来模拟动作。</li><li>由于视觉和动作风格的差异，在自然视频生成中表现良好的模型在应用于贴纸时无法再生成生动的视频。</li><li>为了弥合这一差距，我们采用了分两阶段进行微调的管道：首先是弱域内数据，其次是人类在回路 (HITL) 策略，我们称之为教师集成。</li><li>该策略使我们能够专门针对运动质量进行改进，同时保持静态图像的风格。</li><li>经过推理优化，我们的模型能够在一秒内生成具有高质量、有趣且相关运动的八帧视频。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：动画贴纸：使用视频扩散将贴纸变成生动贴纸</li><li>作者：David Yan<em>, Winnie Zhang</em>, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schönfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman†, Licheng Yu†</li><li>第一作者单位：GenAI, Meta Menlo Park, California, USA</li><li>关键词：动画贴纸、视频扩散、文本到视频、图像到视频、人类参与循环</li><li>论文链接：https://arxiv.org/abs/2402.06088，Github 链接：无</li><li>摘要：（1）研究背景：最近，人们对生成文本（和图像）到视频 (T2V) 建模产生了浓厚的兴趣。当前最先进模型生成的视频通常很短（不到 3 秒），并且通常使用文本（文本到视频或 T2V）、图像（图像到视频或 I2V）或两者。在这项工作中，我们使用文本和图像到视频的生成管道来针对短视频生成的自然应用：为社交表达制作动画贴纸。（2）过去的方法及其问题：我们发现，使用通用 I2V 模型（即仅在通用视频数据集上训练的模型）在应用于贴纸时不会产生高质量的运动，并且经常会生成具有静态或微不足道的运动（例如，仅“摆动”效果）和/或引入不一致性和运动伪影（例如，变形）。这是由于自然（逼真）视频与贴纸风格动画之间的视觉和运动差异，即域差距。（3）提出的研究方法：在这项工作中，我们使用人类参与循环 (HITL) 训练策略来弥合域差距。首先，使用数据集和帧采样率的不同“配方”训练了许多“教师”模型，以便教师模型能够集体产生高质量的多样化运动，尽管很少。接下来，通过使用教师模型在涵盖广泛提示集的大型提示集上执行推理来构建 HITL 数据集。然后，使用 HITL 数据集训练一个较小的“学生”模型，该模型可以从教师模型中学习并产生高质量的动画贴纸。（4）方法在任务和性能上的表现：我们的模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频。性能支持其目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种动画贴纸模型，该模型使用时空潜在扩散模型以文本-图像对为条件，将贴纸图像变成动画贴纸。我们的预训练到生产的管道从 Emumodel 开始，该模型在大量自然视频上进行了微调，然后在域内数据集上进行了微调。然后，我们使用教师集合 HITL 微调策略来进一步提高运动质量、一致性和相关性。我们使用许多基于架构、蒸馏的优化和后训练优化来将推理速度提高到每批一秒。我们表明，我们的微调策略显着提高了运动大小和质量，优于仅在自然视频上训练的模型，证明了教师集合的有效性。（2）：创新点：提出了一种使用人类参与循环 (HITL) 训练策略来弥合域差距的方法，该方法可以生成高质量、有趣且相关的运动；性能：该模型能够在不到一秒的时间内生成具有高质量、有趣且相关的运动的八帧视频；工作量：该模型的训练过程需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-188a2b4c4ed9e284afed14a8e020b622.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29dcdf079faf656ac8934c9dcb4fe4da.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a85a0fa5d13e8bd37d6352571f52fa54.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-73989a294ebc6b241211e4051f9a71db.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cee3ea6a017b3b85519c905ebe1d86a3.jpg" align="middle"></details><h2 id="InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset"><a href="#InstaGen-Enhancing-Object-Detection-by-Training-on-Synthetic-Dataset" class="headerlink" title="InstaGen: Enhancing Object Detection by Training on Synthetic Dataset"></a>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</h2><p><strong>Authors:Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma</strong></p><p>In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. </p><p><a href="http://arxiv.org/abs/2402.05937v1">PDF</a> Tech report</p><p><strong>摘要</strong><br>利用扩散模型生成的合成数据集训练物体检测器，可以提高检测性能或扩展类别。</p><p><strong>要点</strong></p><ul><li>将实例级定位头集成到预训练生成扩散模型中，使其能够在生成图像中定位任意实例。</li><li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li><li>将合成数据用于物体检测器的训练可以提高其性能，在开放词表场景中比现有最先进方法提高 4.5 个 AP，在数据稀疏场景中提高 1.2 到 5.2 个 AP。</li><li>InstaGen 是一种新颖的范式，可通过使用扩散模型生成的合成数据集进行训练来增强对象检测器的能力，例如扩展类别或提高检测性能。</li><li>InstaGen 将实例级定位头集成到预训练的生成扩散模型中，使其能够在生成的图像中定位任意实例。</li><li>定位头通过来自现有物体检测器的监督和针对检测器未涵盖类别的自训练方案进行训练。</li><li>InstaGen 作为数据合成器可用于物体检测，在开放词表场景和数据稀疏场景中均优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><p><strong>标题</strong>：InstaGen：通过合成数据集增强对象检测</p><p></p><p></p><p><strong>作者</strong>：Yuxin Fang, Yifan Zhang, Xiaolin Fang, Xiaohua Shi, Wei Shen, Enhua Wu</p><p></p><p></p><p><strong>第一作者单位</strong>：华中科技大学</p><p></p><p></p><p><strong>关键词</strong>：对象检测，合成数据，扩散模型，实例级接地头</p><p></p><p></p><p><strong>论文链接</strong>：https://arxiv.org/abs/2302.07603</p><p></p><p></p><p><strong>Github 代码链接</strong>：None</p><p></p><p></p><p><strong>摘要</strong>：</p><p></p><p></p><p>本文提出了一种通过合成数据集增强对象检测能力的新范式，该范式通过从扩散模型生成合成数据来扩展检测性能。具体来说，我们将实例级接地头集成到预训练的生成扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。我们通过大量的实验表明，这种称为 InstaGen 的增强版扩散模型可以作为数据合成器来增强对象检测器，并在开放词汇表（+4.6 AP）和数据稀疏（+4.8 AP）上展示出优于现有最先进方法的性能。</p><p></p><p></p><p><strong>总结</strong>：</p><p></p><p></p><p>（一）：研究背景：</p><p></p><p></p><p>对象检测是计算机视觉中的一项基本任务，广泛应用于自动驾驶、人脸识别、医疗图像分析等领域。近年来，随着深度学习的发展，基于深度学习的对象检测方法取得了很大的进展。然而，这些方法通常需要大量的数据进行训练，这在一些领域是难以获得的。</p><p></p><p></p><p>（二）：过去的研究工作：</p><p></p><p></p><p>为了解决数据不足的问题，研究人员提出了各种数据增强技术来扩充训练数据。这些技术包括图像裁剪、翻转、旋转、颜色抖动等。然而，这些技术只能产生有限数量的图像，并且不能保证生成图像的质量。</p><p></p><p></p><p>（三）：本文的问题：</p><p></p><p></p><p>本文认为，现有的数据增强技术不能很好地解决数据不足的问题。因此，本文提出了一种新的数据增强技术，称为 InstaGen，该技术可以生成高质量的合成图像，并且可以保证生成图像的质量。</p><p></p><p></p><p>（四）：本文的方法：</p><p></p><p></p><p>InstaGen 是一种基于扩散模型的数据增强技术。扩散模型是一种生成模型，可以从噪声生成图像。InstaGen 将一个实例级接地头集成到预训练的扩散模型中，使其能够在生成图像中定位实例。接地头通过使用来自现成对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练以将类别名称的文本嵌入与扩散模型的空间特征对齐。</p><p></p><p></p><p>（五）：本文的实验结果：</p><p></p><p></p><p>本文在 PASCAL VOC 和 COCO 数据集上对 InstaGen 进行了评估。实验结果表明，InstaGen 可以有效地增强对象检测器的性能。在 PASCAL VOC 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.6 个百分点。在 COCO 数据集上，InstaGen 将 Faster R-CNN 的 AP 提高了 4.8 个百分点。</p><p></p><ol><li><p>方法：（1）：构建图像合成器：采用预训练的 Stable Diffusion 模型，并使用检测数据集对模型进行微调，以生成具有真实感且包含指定类别的图像。（2）：引入实例级接地头：设计一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。（3）：监督学习和自训练：使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。（4）：数据合成器生成合成数据集：使用训练好的接地头和图像合成器，生成包含对象实例及其边界框的合成数据集。（5）：在合成数据集上训练对象检测器：将合成数据集与真实数据集相结合，训练对象检测器，以提高检测性能。</p></li><li><p>结论：（1）：本文提出了一种称为InstaGen的数据集合成管道，该管道能够为任意类别生成具有对象边界框的图像，作为构建大规模合成数据集以训练对象检测器的免费资源。我们进行了详尽的实验，以展示在合成数据上训练的有效性，以提高检测性能或扩展检测类别的数量。在各种检测场景中，包括开放词汇表（+4.5AP）和数据稀疏（+1.2∼5.2AP）检测中，都取得了显着的改进。（2）：创新点：</p></li><li>提出了一种新的数据合成管道InstaGen，该管道能够为任意类别生成具有对象边界框的图像。</li><li>设计了一种实例级接地头，将类别名称的文本嵌入与扩散模型的空间特征对齐，从而生成对象实例的边界框。</li><li>使用来自现有对象检测器的监督和一种在检测器无法识别的类上进行的自训练策略，来训练接地头。性能：</li><li>在PASCAL VOC和COCO数据集上，InstaGen将Faster R-CNN的AP提高了4.6个百分点和4.8个百分点。</li><li>在开放词汇表和数据稀疏检测中，InstaGen取得了显着的改进。工作量：</li><li>InstaGen是一种数据合成管道，需要预训练的扩散模型和实例级接地头。</li><li>InstaGen需要大量的数据来训练接地头。</li><li>InstaGen需要大量的时间来生成合成数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e5bc75a4d614b9abf0055ef9f09e29eb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcaa5f4430aaa302f904c1eb77cd432c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ed1d3b41f15d36193b946e6064581300.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6998a66afc9f7f895bfb98faa0596297.jpg" align="middle"><img src="https://picx.zhimg.com/v2-85a17fc9e78759363117b1e3dbd18da2.jpg" align="middle"></details><h2 id="Scalable-Diffusion-Models-with-State-Space-Backbone"><a href="#Scalable-Diffusion-Models-with-State-Space-Backbone" class="headerlink" title="Scalable Diffusion Models with State Space Backbone"></a>Scalable Diffusion Models with State Space Backbone</h2><p><strong>Authors:Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</strong></p><p>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a>. </p><p><a href="http://arxiv.org/abs/2402.05608v1">PDF</a> </p><p><strong>摘要</strong><br>利用状态空间架构构建的新型扩散模型，在图像数据上实现可与 U 形卷积神经网络架构媲美的性能，并具有良好的可扩展性。</p><p><strong>要点</strong></p><ul><li>基于状态空间架构的扩散模型在图像数据生成任务上表现良好，可与基于 U 形卷积神经网络或基于 Transformer 的 U 形卷积神经网络架构实现相当的性能，甚至优于它们。</li><li>扩散模型的状态空间模型将时间、条件和噪声图像块等所有输入都视为标记。</li><li>扩散模型的状态空间模型在无条件图像生成和类别条件图像生成场景中均表现良好。</li><li>通过增加深度/宽度或增加输入标记，扩散模型的状态空间模型的正向传递复杂度（以 Gflops 为单位）更高，并且始终表现出更低的 FID。</li><li>扩散模型的状态空间模型具有良好的可扩展性。</li><li>在分辨率为 256×256 和 512×512 的类别条件 ImageNet 基准上，扩散模型的状态空间模型在潜在空间中实现了与先前扩散模型相当的性能，同时大幅降低了计算负担。</li><li>扩散模型的状态空间模型代码和模型可在 <a href="https://github.com/feizc/DiS">https://github.com/feizc/DiS</a> 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于状态空间的可扩展扩散模型</li><li>作者：Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang</li><li>第一作者单位：昆仑科技</li><li>关键词：扩散模型、状态空间、可扩展性、图像生成</li><li>论文链接：https://arxiv.org/abs/2402.05608，Github 链接：https://github.com/feizc/DiS</li><li>摘要：(1)：研究背景：扩散模型作为强大的深度生成模型，近年来在图像生成领域取得了显著进展，广泛应用于文本到图像生成、图像到图像生成、视频生成、语音合成和 3D 合成等领域。扩散模型的发展离不开采样算法和模型骨干的进步，其中 U-Net 是扩散模型中常用的骨干网络，但其在处理长程依赖关系方面存在局限性。(2)：过去方法与问题：传统的扩散模型骨干网络，如 U-Net，在处理长程依赖关系方面存在局限性。为了解决这一问题，本文提出了基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。(3)：研究方法：本文提出的 DiS 模型具有以下特点：</li><li>将时间、条件和噪声图像块视为标记，并使用状态空间骨干网络来处理这些标记。</li><li>DiS 模型可以处理原始图像块或潜在空间中的标记。</li><li><p>DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。(4)：实验结果与性能：本文在无条件和类条件图像生成任务上对 DiS 模型进行了评估，结果表明 DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。此外，本文还分析了 DiS 模型的可扩展性，结果表明 DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。</p></li><li><p>方法：（1）：提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。（2）：DiS模型可以处理原始块或潜在空间中的标记。（3）：DiS模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。</p></li><li><p>结论：（1）：本工作提出了一种基于状态空间的扩散模型（DiS），该模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记。DiS 采用了一种统一的方法来处理所有输入，包括时间、条件和噪声图像块，将它们视为连接的标记。实验结果表明，DiS 与基于 CNN 或 Transformer 的 U-Net 模型相比具有相当或更好的性能，同时继承了状态空间模型类的显着可扩展性特征。我们认为 DiS 可以为未来研究扩散模型中的骨干网络提供有价值的见解，并有助于推进大规模多模态数据集中的生成建模。鉴于本研究中提出的令人鼓舞的可扩展性结果，未来的努力应集中在将 DiS 进一步扩展到更大的模型和标记计数上。（2）：创新点：DiS 模型将时间、条件和噪声块视为标记，并使用状态空间骨干网络来处理这些标记，这是一种新的方法，可以有效地处理长程依赖关系。DiS 模型可以处理原始块或潜在空间中的标记，这使得它可以应用于各种图像生成任务。DiS 模型具有良好的可扩展性，可以通过增加深度、宽度或输入标记的数量来提高模型的性能。性能：在无条件和类条件图像生成任务上，DiS 模型与基于 CNN 或 Transformer 的 U-Net 模型具有相当或更好的性能。在 ImageNet 数据集上，DiS-H/2 模型在分辨率为 256×256 和 512×512 的类条件图像生成任务上取得了与之前的扩散模型相当的性能，同时显著降低了计算负担。工作量：DiS 模型的训练和推理成本与基于 CNN 或 Transformer 的 U-Net 模型相当。DiS 模型的可扩展性使得它可以应用于各种图像生成任务，包括高分辨率图像生成、视频生成和 3D 合成。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6bb4b2235878abe86e04f19f24047beb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-1d4bb00838a5fb623fcc9eb998c2c6b9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-adf0dc0a97f9ca167de7eccda01fe6df.jpg" align="middle"></details><h2 id="SPAD-Spatially-Aware-Multiview-Diffusers"><a href="#SPAD-Spatially-Aware-Multiview-Diffusers" class="headerlink" title="SPAD : Spatially Aware Multiview Diffusers"></a>SPAD : Spatially Aware Multiview Diffusers</h2><p><strong>Authors:Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</strong></p><p>We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a> </p><p><a href="http://arxiv.org/abs/2402.05235v1">PDF</a> Webpage: <a href="https://yashkant.github.io/spad">https://yashkant.github.io/spad</a></p><p><strong>Summary</strong><br>跨视角图像生成模型 SPAD：自我注意和空间编码的结合，实现文本到图像生成的一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>SPAD 是一种新的方法，可以从文本提示或单个图像生成一致的多视角图像。</li><li>SPAD 是通过扩展预训练的 2D 扩散模型的自注意力层来实现多视角生成，并对 Objaverse 的高质量子集进行微调。</li><li>SPAD 显示，先前的研究提出的自我注意的朴素扩展（例如 MVDream）导致视角之间的内容复制。</li><li>SPAD 显式地限制基于极线几何的跨视角注意。</li><li>SPAD 利用从相机射线派生的 Plücker 坐标，并将它们注入作为位置编码，以进一步增强 3D 一致性。</li><li>与只能在固定方位角和仰角生成视图的最近工作相比，SPAD 提供了完全的相机控制，并在 Objaverse 和 Google Scanned Objects 数据集上看不见的物体的新颖视图合成中实现了最先进的结果。</li><li>使用 SPAD 进行文本到 3D 生成消除了多面 Janus 问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SPAD：空间感知多视图扩散器</li><li>作者：Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin</li><li>第一作者单位：多伦多大学</li><li>关键词：多视图生成、扩散模型、文本到 3D</li><li>论文链接：https://yashkant.github.io/spad/，Github 代码链接：None</li><li><p>摘要：（1）研究背景：多视图生成是指从文本提示或单个图像生成一组在 3D 空间中一致的图像。这对于许多应用很有用，例如增强现实、虚拟现实和 3D 建模。（2）过去方法：现有方法通常使用 2D 扩散模型来生成多视图图像。然而，这些方法通常会导致视图之间出现不一致，例如对象形状或纹理不匹配。（3）研究方法：本文提出了一种新的多视图生成方法 SPAD。SPAD 通过在 2D 扩散模型中引入跨视图交互来实现多视图生成。此外，SPAD 还利用了 Plücker 坐标来增强 3D 一致性。（4）方法性能：SPAD 在 Objaverse 和 Google Scanned Objects 数据集上进行了评估。结果表明，SPAD 在新视图合成任务上优于现有方法。此外，SPAD 还能够防止多面 Janus 问题，即生成的图像在不同视图中具有不同的外观。</p></li><li><p>方法：(1): SPAD的核心思想是将多视图生成问题转化为一个扩散模型问题。SPAD使用一个2D扩散模型来生成每个视图的图像，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。(2): SPAD使用Plücker坐标来表示3D空间中的点。Plücker坐标具有不变性，这意味着它们不受视角和投影变换的影响。SPAD利用Plücker坐标来增强3D一致性，并防止多面Janus问题。(3): SPAD在Objaverse和GoogleScannedObjects数据集上进行了评估。结果表明，SPAD在新视图合成任务上优于现有方法。此外，SPAD还能够防止多面Janus问题。</p></li><li><p>结论：（1）：SPAD是一种新颖的多视图生成框架，它将文本或图像输入转换为多个视图。SPAD在预训练的文本到图像扩散模型的自注意力层中引入了极线注意力，以促进多视图交互并改进相机控制。此外，SPAD使用Plücker位置编码增强了自注意力层，以通过防止对象的翻转视图预测来进一步改进相机控制。SPAD在Objaverse和GoogleScannedObjects数据集上进行了严格的评估，并在图像条件的新视图合成方面展示了最先进的结果。（2）：创新点：</p></li><li>将多视图生成问题转化为扩散模型问题，并通过在扩散模型中引入跨视图交互来确保视图之间的一致性。</li><li>使用Plücker坐标来表示3D空间中的点，并利用Plücker坐标来增强3D一致性，防止多面Janus问题。性能：</li><li>在Objaverse和GoogleScannedObjects数据集上，SPAD在新视图合成任务上优于现有方法。</li><li>SPAD能够防止多面Janus问题，即生成的图像在不同视图中具有不同的外观。工作量：</li><li>SPAD的实现相对简单，并且可以在PyTorch中轻松实现。</li><li>SPAD的训练过程相对快速，并且可以在单个GPU上完成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-afe3524a8f81d817d06d1d9498a1728a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3709a9941aada6c4d3ed35934e311765.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a80a51acf35ce9d57c5584647e5cca12.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-13  Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable   Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/NeRF/</id>
    <published>2024-02-09T02:20:12.000Z</published>
    <updated>2024-02-09T02:20:12.523Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering"><a href="#NeRF-as-Non-Distant-Environment-Emitter-in-Physics-based-Inverse-Rendering" class="headerlink" title="NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering"></a>NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</h2><p><strong>Authors:Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao</strong></p><p>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport simulation. While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a>. </p><p><a href="http://arxiv.org/abs/2402.04829v1">PDF</a> Project page and video: <a href="https://nerfemitterpbir.github.io/">https://nerfemitterpbir.github.io/</a></p><p><strong>摘要</strong><br>神经辐射场可以作为空间非距离环境光源，用于物理逆渲染，使逆渲染更加真实准确。</p><p><strong>主要要点</strong></p><ul><li>基于物理的逆渲染旨在联合优化从捕获的 2D 图像中提取的形状、材质和光照。</li><li>在逆渲染中，通常使用环境贴图作为光照模型，但这种假设会导致空间不变的光照，这在真实世界的逆渲染中可能是不准确的近似。</li><li>提出使用神经辐射场作为空间可变的环境光照模型，并构建了一个以神经辐射场作为非距离环境光源的逆渲染管道。</li><li>将方法与环境贴图在真实和合成数据集上进行比较，结果表明，基于神经辐射场的光源可以更准确地模拟场景光照，并实现更准确的逆渲染。</li><li>项目页面和视频：<a href="https://nerfemitterpbir.github.io/。">https://nerfemitterpbir.github.io/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于物理的反演渲染中，NeRF 作为非远处环境发射器</li><li>作者：Jingwang Ling、Ruihan Yu、Feng Xu、Chun Du、Shuang Zhao</li><li>单位：清华大学</li><li>关键词：NeRF、物理反演渲染、环境光照、形状重建</li><li>论文链接：https://arxiv.org/pdf/2402.04829.pdf，Github 链接：None</li><li><p>摘要：（1）研究背景：基于物理的反演渲染旨在从捕获的 2D 图像中联合优化形状、材质和光照。其中，光照是实现真实光照传输模拟的重要组成部分。环境贴图是反演渲染中常用的光照模型，但我们发现，在光源不是无限远处的场景中，环境贴图的空间不变光照假设会导致空间不变的光照，这在现实世界中的反演渲染中可能是不准确的近似。（2）过去方法及问题：过去的方法通常使用环境贴图来近似物体周围的光照，但这种方法在光源不是无限远处的场景中会导致不准确的结果。（3）研究方法：我们提出使用 NeRF 作为空间变化的环境光照模型，并构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道。通过在真实和合成数据集上与环境贴图进行比较，我们证明了我们的 NeRF 模型可以更准确地模拟场景光照，并实现更准确的反演渲染。（4）方法性能：我们的方法在真实和合成数据集上都取得了比环境贴图更好的结果。在真实数据集上，我们的方法在重照明和形状重建任务上都取得了更好的性能。在合成数据集上，我们的方法在重照明、形状重建和材质估计任务上都取得了更好的性能。这些结果证明了我们的方法可以更准确地模拟场景光照，并实现更准确的反演渲染。</p></li><li><p>方法：（1）提出使用 NeRF 作为空间变化的环境光照模型，构建以 NeRF 作为非远处环境发射器的反演渲染管道。（2）利用真实和合成数据集，与环境贴图进行比较，证明 NeRF 模型可以更准确地模拟场景光照，实现更准确的反演渲染。（3）在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。</p></li><li><p>结论：(1)：本文提出了一种基于 NeRF 的反演渲染管道，该管道将 NeRF 用作非远处环境发射器，可以更准确地模拟场景光照，并实现更准确的反演渲染。(2)：创新点：</p></li><li>使用 NeRF 作为空间变化的环境光照模型，可以更准确地模拟场景光照。</li><li>构建了一个以 NeRF 作为非远处环境发射器的反演渲染管道，可以实现更准确的反演渲染。</li><li>在真实和合成数据集上，与环境贴图相比，在重照明、形状重建和材质估计任务上都取得了更好的性能。性能：</li><li>在真实数据集上，在重照明和形状重建任务上都取得了更好的性能。</li><li>在合成数据集上，在重照明、形状重建和材质估计任务上都取得了更好的性能。工作量：</li><li>需要训练 NeRF 模型，这可能需要大量的数据和计算资源。</li><li>需要构建反演渲染管道，这可能需要大量的编程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-5217f666aff1dcbbc55e20cda0c76080.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a9fa354da141f8905a59ea4a06f90f25.jpg" align="middle"></details><h2 id="OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding"><a href="#OV-NeRF-Open-vocabulary-Neural-Radiance-Fields-with-Vision-and-Language-Foundation-Models-for-3D-Semantic-Understanding" class="headerlink" title="OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding"></a>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language   Foundation Models for 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li</strong></p><p>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language foundation models to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness. </p><p><a href="http://arxiv.org/abs/2402.04648v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）技术通过结合视觉和语言基础模型，提升了NeRF在开放词汇表3D语义感知中的表现。</p><p><strong>Key Takeaways</strong></p><ul><li>OV-NeRF 提出了一种单视图和跨视图策略，将NeRF用于开放词汇表3D语义感知。</li><li>利用 SAM 提取的 2D 掩模建议，引入区域语义排序 (RSR) 正则化，以纠正每个训练视图的语义噪声。</li><li>提出跨视图自增强 (CSE) 策略，利用训练语义场本身生成的 3D 一致语义，减少语义模糊性和增强语义一致性。</li><li>OV-NeRF 在 Replica 和 Scannet 数据集上分别实现了 20.31% 和 18.42% 的 mIoU 指标提升，优于现有最优方法。</li><li>OV-NeRF 在各种 CLIP 配置下均表现出优异的性能，验证了其鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：OV-NeRF：具有视觉和语言的开放词汇神经辐射场</li><li>作者：廖桂标，周凯晨，鲍振宇，刘康林，李庆</li><li>单位：北京大学</li><li>关键词：神经辐射场，开放词汇，语义理解，视觉语言模型</li><li>链接：https://arxiv.org/abs/2402.04648</li><li><p>摘要：(1) 研究背景：神经辐射场（NeRF）是一种强大的表示方法，可以捕捉复杂真实世界的 3D 场景。然而，在开放词汇 3D 语义感知任务中实现全面的 3D 语义理解仍然是一个具有挑战性的问题。(2) 过去的方法：过去的方法直接从对比视觉语言预训练（CLIP）中提取语义，用于语义场学习，但遇到了来自 CLIP 的嘈杂和视图不一致语义的困难。(3) 研究方法：为了解决这些限制，本文提出了 OV-NeRF，它利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。(4) 性能表现：OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。此外，该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。</p></li><li><p>方法：(1) 提出 OV-NeRF，利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。(2) 利用预先计算的 CLIP 特征和 SAM 的区域提议来生成精确的相关性图，以监督 OV-NeRF，而不是使用源自 CLIP 模型的原始噪声相关性图。(3) 在训练 OV-NeRF 数个 epoch 后，利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。</p></li><li><p>结论：（1）：本工作通过利用视觉语言基础模型的能力，提出 OV-NeRF 来解决基于 NeRF 的 3D 语义理解挑战。在 OV-NeRF 中，提出的区域语义排序（RSR）正则化产生精确的单视图相关性图来训练 OV-NeRF，跨视图自我增强确保视图一致的分割结果。实验结果表明，我们的方法在合成和真实世界基准数据集上以很大优势优于 SOTA 方法，显示了我们方法的优越性。此外，我们的方法在不同的 CLIP 配置中始终表现出优异的性能，肯定了其通用性。（2）：创新点：提出了一种新的 NeRF 模型 OV-NeRF，该模型利用预训练的视觉语言基础模型的潜力，通过提出的单视图和跨视图策略来增强语义场学习。提出了一种新的区域语义排序（RSR）正则化，该正则化产生精确的单视图相关性图来训练 OV-NeRF。提出了一种新的跨视图自我增强方法，该方法利用从 OV-NeRF 获得的渲染伪输出，包括训练视图和未见新颖视图，用于跨视图自我增强监督。性能：OV-NeRF 在 Replica 和 Scannet 上分别在 mIoU 度量中取得了 20.31% 和 18.42% 的显着改进。该方法在各种 CLIP 配置中表现出一致的优异结果，进一步验证了其鲁棒性。工作量：该方法需要预先计算 CLIP 特征和 SAM 的区域提议，这可能会增加计算成本。该方法需要训练多个 epoch，这可能会增加训练时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d28a855be0d118e883bd9f8001dbbcd1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1c6219c40ef2be88e25422dda1aae264.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fece26674b484110bc1b8871018a6a3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ea1c3e14317a591427313451f7980698.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a1bd1a1dc370f614b943567738833593.jpg" align="middle"></details><h2 id="GSN-Generalisable-Segmentation-in-Neural-Radiance-Field"><a href="#GSN-Generalisable-Segmentation-in-Neural-Radiance-Field" class="headerlink" title="GSN: Generalisable Segmentation in Neural Radiance Field"></a>GSN: Generalisable Segmentation in Neural Radiance Field</h2><p><strong>Authors:Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan</strong></p><p>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: <a href="https://vinayak-vg.github.io/GSN/">https://vinayak-vg.github.io/GSN/</a> </p><p><a href="http://arxiv.org/abs/2402.04632v1">PDF</a> Accepted at the Main Technical Track of AAAI 2024</p><p><strong>Summary</strong><br>利用几个视图就可以渲染未知且未训练场景的新视图，并提供一致的逐像素语义特征。</p><p><strong>Key Takeaways</strong></p><ul><li>传统辐射场表示捕获特定场景的细节，必须在每个场景上重新训练。</li><li>语义特征字段已添加到射频中以促进多项分割任务。</li><li>广义射频表示学习了视图插值原理。</li><li>给定几个视图，广义射频可以渲染未知且未训练场景的新视图。</li><li>我们提供了一种将特征字段提炼成广义 GNT 表示的方法。</li><li>我们的 GSN 表示可以快速生成未见场景的新视图，并提供一致的逐像素语义特征。</li><li>这允许对任意新场景进行多视图分割。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GSN：神经辐射场中的可泛化分割</li><li>作者：Vinayak Gupta，Rahul Goel，Sirikonda Dhawal，P.J. Narayanan</li><li>隶属机构：印度理工学院马德拉斯分校</li><li>关键词：神经辐射场、可泛化分割、语义特征场</li><li>论文链接：https://arxiv.org/abs/2402.04632   Github 链接：无</li><li><p>摘要：（1）研究背景：传统的神经辐射场（RF）表示可以捕捉特定场景的细节，但必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。（2）过去的方法及其问题：本文提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。（3）研究方法：过去的 RF 表示学习特定场景的细节，必须针对每个场景重新训练。语义特征场已被添加到 RF 中以促进多项分割任务。泛化的 RF 表示学习视图插值原理。给定几个视图，泛化的 RF 可以渲染未知且未训练场景的新视图。我们提出了一种将特征场提炼到泛化的 GNT 表示中的方法。我们的 GSN 表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。这使得任意新场景的多视图分割成为可能。我们展示了将不同语义特征提取到泛化的 RF 中。我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。（4）方法的性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。这些性能支持了我们的目标。</p></li><li><p>方法：（1）首先，我们修改 GNT 架构以帮助语义特征提取。（2）然后，我们描述了我们的两阶段训练蒸馏过程。（3）最后，我们描述了如何使用蒸馏特征执行多视图分割。</p></li><li><p>结论：（1）：本文提出了一种新的多视图分割方法，其主要优势在于其泛化性，即它可以在任意新场景上执行分割而无需任何训练。这使其区别于以前的方法。我们将我们的结果与早期方法进行了比较，并表明我们的性能与它们相当，同时可以泛化到未见场景。这是将泛化神经辐射场的应用拉近到特定场景辐射场的一大步。我们方法预测的特征可用于多种下游任务。（2）：创新点：提出了将特征场提炼到泛化的 GNT 表示中的方法，该表示可以即时生成未见场景的新视图，同时提供一致的逐像素语义特征。性能：我们的多视图分割结果与使用传统 RF 的方法相当。GSN 显着缩小了标准 RF 方法和可泛化 RF 方法之间的差距。工作量：我们的方法依赖于基于 transformer 的架构，因此渲染过程与几种特定场景的辐射场方法相比固有地缓慢。提高渲染速度可以显着改善我们基于笔划的分割方法所需的人机交互体验。我们将泛化辐射场的渲染速度改进留作未来的工作。目前，我们的方法执行多视图分割，因为它使用基于图像的渲染。某些应用程序需要 3D 分割而不是多视图分割。因此，可泛化的 3D 分割框架有望成为未来的工作。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bf67c21104c6d20a1d6e37e83bff2155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-03400222552085971945e9fc363dc323.jpg" align="middle"><img src="https://picx.zhimg.com/v2-61621673ca99816fe4332d9623a7e1b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c82ea98993102ebb08c3d96886f8caf8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d9b9a069535dfb6e09a7654648b4f040.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality. </p><p><a href="http://arxiv.org/abs/2402.04554v1">PDF</a> </p><p><strong>摘要</strong><br>鸟瞰 NeRF：基于神经辐射场的大规模场景重建。</p><p><strong>要点</strong></p><ul><li>针对大规模场景重建，提出了基于神经辐射场的 BirdNeRF 算法。</li><li>BirdNeRF 将大场景图像集分解为多个小集合，每个小集合训练单独的 NeRF 模型。</li><li>这种分解方法将渲染时间与场景大小解耦，并使渲染能够无缝扩展到任意大的环境。</li><li>此外，它允许对环境进行逐块更新，从而提高重建过程的灵活性和适应性。</li><li>提出了一种基于投影的新颖视角重新渲染策略，有助于有效利用独立训练的子场景生成更好的渲染结果。</li><li>在现有数据集以及我们自己的无人机航拍视频上评估了我们的方法，在单个 GPU 上将重建速度提高了 10 倍（相对于经典摄影测量软件）和 50 倍（相对于最先进的大规模 NeRF 解决方案），同时渲染质量相似。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鸟瞰神经辐射场：使用航拍图像快速神经重建大场景</li><li>作者：张慧清、薛一飞、廖明、老一真</li><li>单位：无</li><li>关键词：神经辐射场、大规模重建、航拍图像、空间分解、投影引导</li><li>链接：https://arxiv.org/abs/2402.04554Github：无</li><li><p>摘要：(1)：随着航空测量技术的进步，获取高分辨率图像变得更加容易和经济实惠，基于图像的 3D 重建已成为一个活跃的研究领域，并在城市规划、导航、虚拟旅游、房地产和灾害管理等领域有着广泛的应用。(2)：现有的基于图像的 3D 重建技术主要分为传统的基于几何的方法和基于神经网络的方法。基于几何的方法通常需要大量的人工干预，并且对图像的质量和数量非常敏感。基于神经网络的方法，例如神经辐射场 (NeRF)，可以自动从图像中学习场景的 3D 表示，并且对图像的质量和数量不太敏感。然而，NeRF 在处理大规模场景时面临着训练速度慢、渲染时间长和容易产生伪影等挑战。(3)：为了解决这些挑战，本文提出了一种新的 NeRF 变体，称为鸟瞰神经辐射场 (BirdNeRF)。BirdNeRF 使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的 NeRF 模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。(4)：在多个数据集上的实验结果表明，BirdNeRF 在重建速度和质量方面都优于现有的方法。在单个 GPU 上，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大规模 NeRF 解决方案快 50 倍，同时具有相似的渲染质量。</p></li><li><p>方法：(1) 将大场景分解为多个较小的子场景，分别训练每个子场景的 NeRF 模型。(2) 使用一种新的投影引导的新视图重新渲染策略，有效地利用独立训练的子场景来生成高质量的渲染结果。</p></li><li><p>结论：（1）：本文提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。BirdNeRF使用了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。这种分解方法不仅可以减少训练时间和渲染时间，还可以提高重建的质量。此外，BirdNeRF还提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。（2）：创新点：</p></li><li>提出了一种新的NeRF变体，称为鸟瞰神经辐射场（BirdNeRF），可以快速重建大规模场景。</li><li>提出了一种新的空间分解算法，将大规模航拍图像集分解成多个较小的子集，并分别训练每个子集的NeRF模型。</li><li>提出了一种新的投影引导的新视图重新渲染策略，可以有效地利用独立训练的子场景来生成高质量的渲染结果。</li></ol><p>性能：- 在单个GPU上，BirdNeRF的重建速度比传统的摄影测量软件快10倍，比最先进的大规模NeRF解决方案快50倍，同时具有相似的渲染质量。- BirdNeRF可以重建包含数百万个三角形的场景，而不会出现明显的伪影。</p><p>工作量：- BirdNeRF的实现相对简单，易于使用。- BirdNeRF的训练时间和渲染时间都比较短，可以满足实际应用的需求。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0ffe2746a28f7248c7dc45305ca5a0d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3e3e28cf5dd4b506a44e1769d5abf0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51dea38443c497692956a6fd50ec6a18.jpg" align="middle"></details>## ViewFusion: Learning Composable Diffusion Models for Novel View   Synthesis**Authors:Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin**Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available. [PDF](http://arxiv.org/abs/2402.02906v1) **Summary**将多个不同视角的图像输入到 ViewFusion 模型中，就可以基于这些图像合成出新的视角图像。**Key Takeaways**- ViewFusion 将扩散去噪步骤同时应用于任意数量的场景输入视图，然后将每个视图获得的噪声梯度与像素权重掩码相结合，确保在目标场景的每个区域内仅考虑最具信息性的输入视图。- ViewFusion 解决了先前方法的几个局限性：跨多个场景和对象类别进行训练和泛化；在训练和测试时自适应地采用可变数量的不受姿势限制的视图；能够生成合理的视图，即使在严重不确定的条件下。- ViewFusion 优于或与最先进的方法相比能更高质量地生成视图。- ViewFusion 无法生成场景的 3D 嵌入，导致其推理速度相对较慢。- ViewFusion 目前仅在相对较小的 NMR 数据集上进行了测试。- 代码库现已发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ViewFusion：用于新颖视图合成的可组合扩散模型的学习</li><li>作者：Bernard Spiegl、Andrea Perin、St´ephane Deny、Alexander Ilin</li><li>第一作者单位：阿尔托大学计算机科学系（仅翻译中文）</li><li>关键词：新颖视图合成、扩散模型、可组合性、自适应输入视图、鲁棒性</li><li>论文链接：https://arxiv.org/abs/2402.02906，Github 代码链接：None</li><li><p>摘要：（1）研究背景：新颖视图合成是一个计算机视觉领域的长期研究课题。传统方法通常使用显式建模 3D 空间的方法，如体素、点云或网格。近年来，基于神经辐射场 (NeRF) 的方法也取得了很大进展。然而，这些方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。（2）过去方法及问题：过去的方法通常存在需要昂贵的逐场景重新训练、无法在没有输入视图的姿态信息的情况下操作或无法适应测试时输入视图数量的可变性等缺点。因此，本文旨在提出一种直观的端到端架构，用于执行新颖视图合成，同时解决先前工作中提到的缺点。（3）研究方法：本文提出的 ViewFusion 方法通过一系列针对特定问题的设计选择，一次性解决了上述缺点。首先，使用在大量场景和类别上同时训练的扩散概率框架，使其能够在无需重新训练的情况下进行泛化。此外，由于扩散过程的随机性质，该模型即使在不确定性设置（例如，对象的严重遮挡或有限数量的输入视图）中也能表现良好，因为它提供了多种合理的视图。此外，本文提出的解决方案不需要输入视图的顺序或任何显式姿态信息。最后，与之前的对应方法不同，一旦训练完成，本文的方法就可以有效地处理任意长度的输入。这要归功于一种新的加权解决方案，与去噪骨干网络的组合一起，该解决方案允许模型根据视图的信息量对视图进行加权，同时扩展到任意数量的视图。（4）方法性能：本文在包含各种类别和输入视图姿势的数据集上评估了所提出的方法。此外，本文通过对中间模型输出的分析验证了该方法，证明了该模型能够推断和自适应地调整每个输入视图的重要性。加权不仅对输出的质量有影响，而且推断的加权方案也与直观的人类感知一致。</p></li><li><p>方法：(1) 提出了一种基于扩散概率框架的新颖视图合成方法 ViewFusion，该方法能够同时处理多个输入视图，并根据每个视图的重要性对视图进行加权，从而生成高质量的新颖视图。(2) ViewFusion 模型由多个 U-Net 组成，每个 U-Net 负责处理一个输入视图。U-Net 的输出包括噪声预测和权重，权重用于对噪声进行加权，从而生成最终的新颖视图。(3) ViewFusion 模型的训练过程包括两个阶段：预训练和微调。在预训练阶段，模型在大量场景和类别上进行训练，以学习一般化的特征表示。在微调阶段，模型在特定场景或类别上进行微调，以提高模型的性能。(4) ViewFusion 模型的推理过程包括两个阶段：噪声采样和扩散过程。在噪声采样阶段，模型从正态分布中采样噪声。在扩散过程中，模型通过逐渐降低噪声的强度来生成新颖视图。</p></li><li><p>结论：（1）：ViewFusion 是一种灵活、无需姿态的生成方法，可使用可组合扩散模型执行新颖视图合成。我们提出了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。我们认为，我们的方法在进行新颖视图合成时是一个有价值的贡献，并且有可能应用于其他问题。（2）：创新点：ViewFusion 引入了一种新颖的加权方案，用于组合扩散模型，确保仅将信息量最大的输入视图用于预测目标视图，并使 ViewFusion 能够自适应地处理任意长且无序的输入视图集合，而无需重新训练。此外，ViewFusion 的生成性质使其即使在严重欠定条件下也能生成合理视图。性能：ViewFusion 在各种类别和输入视图姿势的数据集上均取得了最先进的性能。此外，ViewFusion 能够有效地处理任意数量的输入视图，并且对输入视图的顺序和姿态信息不敏感。工作量：ViewFusion 的训练过程包括两个阶段：预训练和微调。预训练阶段需要大量的数据和计算资源，但微调阶段可以相对快速地完成。ViewFusion 的推理过程也非常有效，可以在几秒钟内生成新颖视图。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-672b204f9242001f6ba5e1b350c81c87.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab2417ac343ade4b32aea1621299f294.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a35b2635715a736813769f26b2939948.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-512893851e477e6cab6fb9d3224f7acf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fa2f794caefa6d02e53b7a03fc9f646.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5e41e289131352d483b38fb05ca0ce8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1874aa5e890d55cc56f18c742397f3bf.jpg" align="middle"></details><h2 id="Robust-Inverse-Graphics-via-Probabilistic-Inference"><a href="#Robust-Inverse-Graphics-via-Probabilistic-Inference" class="headerlink" title="Robust Inverse Graphics via Probabilistic Inference"></a>Robust Inverse Graphics via Probabilistic Inference</h2><p><strong>Authors:Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</strong></p><p>How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks. </p><p><a href="http://arxiv.org/abs/2402.01915v1">PDF</a> </p><p><strong>Summary</strong><br>新颖的贝叶斯方法 RIG 可同时对场景和破坏进行推理，以克服各种场景损坏。</p><p><strong>Key Takeaways</strong></p><ul><li>RIG 是一种新的贝叶斯方法，可同时对场景和破坏进行推理。</li><li>RIG 仅使用干净的数据进行训练，优于深度估计器和替代的 NeRF 方法。</li><li>RIG 可与多种基于正则化流和扩散模型的场景先验架构一起使用。</li><li>对于后者，我们开发了具有辅助潜变量的重建指导（ReGAL）——一种扩散调节算法，适用于具有辅助潜变量（如破坏）的情况。</li><li>RIG 演示了场景先验如何用于生成任务之外。</li><li>RIG 利用强大的场景先验和无信息的均匀破坏先验，使其适用于广泛的破坏。</li><li>在给定单一图像的情况下，RIG 对场景和破坏进行后验推理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：鲁棒逆向图形生成：基于概率推理</li><li>作者：Tuan Anh Le、Pavel Sountsov、Matthew D. Hoffman、Ben Lee、Brian Patton、Rif A. Saurous</li><li>单位：谷歌（Google）</li><li>关键词：鲁棒逆向图形生成、神经辐射场、概率推理、域随机化、数据增强</li><li>论文链接：https://arxiv.org/abs/2402.01915</li><li>摘要：</li></ol><p>（1）研究背景：    * 在存在雨、雪、雾等干扰的情况下，如何从单张图像中推断出 3D 场景？    * 直接的域随机化依赖于提前知道干扰的种类。</p><p>（2）过去的方法及其问题：    * 域随机化：通过在数据生成过程中选择一系列干扰来实现鲁棒性，但这种方法需要提前知道干扰的种类。    * 正则化训练：通过在重建损失中添加额外的损失项来实现鲁棒性，但这种方法难以扩展到更极端的情况。</p><p>（3）本文提出的研究方法：    * 鲁棒逆向图形生成（RIG）：将问题视为概率推理问题，利用预训练的场景先验（在本例中是神经辐射场先验）和一个关于干扰的弱先验（在本例中是具有均匀先验权重的另一个神经辐射场）来进行推理。    * RIG 在场景和干扰神经辐射场中执行完整的概率推理，而不是像最大后验概率推理那样寻找最可能的解。</p><p>（4）方法的性能表现：    * RIG 在具有各种场景先验架构（基于正则化流和扩散模型）的情况下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。    * RIG 仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。</p><ol><li><p>方法：（1）场景表示：我们使用神经辐射场 (NeRF) 表示，因为它易于进行基于梯度的推理。（2）场景先验：我们假设我们有一个预训练的 NeRF 先验 p(x)，我们可以从中对场景潜在变量 x 进行采样，并从不同的视点 ζ 渲染图像 y。（3）损坏表示和先验：我们关注的是对 3D 场景的损坏，例如漂浮物或天气伪影（如雨、雪或雾），尽管我们的方法可以泛化到传感器损坏，如相机内部噪声（第 6.1 节）。我们将 3D 损坏表示为另一个 NeRF 的参数。与场景 x 不同，我们不需要对 c 有一个强先验。在我们的实验中，我们假设一个不适当的先验 p(c)∝1。这意味着我们不需要预先知道损坏的种类；损坏可以是任何 3D 实体，从天气伪影和漂浮物到不需要的对象。（4）似然：为了给定场景潜在变量 x 和损坏 c 渲染图像 y，我们组合各自的 NeRF 输出。对于光线位置和方向 (xr, dr)，我们将场景 NeRF (γz, σz) 和损坏 NeRF (γc, σc) 的输出组合为 σ = σz + σc，γ = (γzσz + γcσc)/σ（Niemeyer &amp; Geiger，2021）。我们将组合的 NeRF 的渲染表示为 y = R(x, c)。似然是一个逐像素和逐通道的高斯分布 p(y|x, c) = ∏像素和通道j N(yij|R(x, c)ij, σ2y)，其中 σ2y 是观测噪声方差。（5）MAP 推理不够：推断场景 x 的一种直接方法是找到最大化 p(x)p(c)p(y|x, c) 的 MAP 估计 (x<em>, c</em>)。然而，这种方法会导致“广告牌”解决方案，其中损坏最终解释了场景，就像一个放置在相机前面的广告牌。（6）完全后验推理就足够了：在 RIG 中，我们执行完全后验推理以获得潜在场景 x, c∼p(x, c|y) ∝ p(x)p(c)p(y|x, c)，这在经验上可以避免广告牌解决方案（第 6.1 节）。直观地说，这可以看作是模式与典型集不同的一个实例。损坏完全覆盖场景的模式周围区域具有高密度但低体积——没有许多损坏可以精确地渲染到观测图像。另一方面，后验同时考虑密度和体积，集中在具有高概率质量的区域——有许多非广告牌损坏与正确的场景一起渲染到观测图像，尽管每个这样的解决方案可能具有低密度。（7）变分推理：我们使用变分推理，其中我们优化证据下界 (ELBO) 关于引导分布 q(x, c)：ELBO(q) = Eq(x, c)[logp(y, x|c) - logq(x, c)]。（8）扩散场景先验：去噪扩散已成为正则化流的有力替代方案。虽然可以直接用基于扩散的先验替换 ProbNeRF 中使用的 RealNVP（例如 Dupontet al.，2022），但扩散模型允许我们可追踪地增加我们的潜在表示的维数。高维潜在空间能够进行高保真采样和重建。我们构建了单级扩散 NeRF (SSDNeRF) 框架（Chen et al.，2023）来训练场景先验。SSDNeRF 优化了一组针对每个训练示例的潜在变量 {xn}，也称为 GLO 潜在变量（Bojanowski et al.，2018），由 ϕ 参数化的扩散先验 pϕ(x) 和由 ψ 参数化的似然 pψ(y|x)。有关更多详细信息，请参见附录 D。（9）扩散模型：扩散模型是一个潜在变量生成模型，包含正向和反向过程。正向扩散过程 q(z|x) 从数据 x 开始。</p></li><li><p>结论：(1)：本文提出了一种鲁棒逆向图形生成（RIG）方法，该方法将问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。(2)：创新点：</p></li><li>将逆向图形生成问题视为概率推理问题，利用预训练的场景先验和一个关于干扰的弱先验来进行推理。</li><li>提出了一种鲁棒逆向图形生成（RIG）方法，该方法在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li><li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。性能：</li><li>RIG在各种场景先验架构下都优于深度估计器和执行点估计而不是完整推理的替代神经辐射场方法。</li><li>RIG仅在干净数据上训练，但它在具有各种干扰（雨、雪、雾、噪声）的图像上都优于其他方法。工作量：</li><li>RIG需要预训练一个场景先验和一个关于干扰的弱先验。</li><li>RIG需要执行完整的概率推理，这比执行点估计或最大后验概率推理更耗时。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-25f26b8c4a059fad96179f9402d4ddf8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b464c110b8bfcce608856052d9518e4b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1f7396fa7b1ad32dc9c645595746950b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8c6960210c2e3765f8601fd7fb69b4ba.jpg" align="middle"></details><h2 id="HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation"><a href="#HyperPlanes-Hypernetwork-Approach-to-Rapid-NeRF-Adaptation" class="headerlink" title="HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation"></a>HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation</h2><p><strong>Authors:Paweł Batorski, Dawid Malarz, Marcin Przewięźlikowski, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek</strong></p><p>Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study. </p><p><a href="http://arxiv.org/abs/2402.01524v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场方法对于少数基础图像合成新奇3D物体视图有着广泛的认可，却存在泛化性质有限的问题，不妨碍我们利用显著计算资源为我们要展示的每个对象训练独立体系结构。</p><p><strong>Key Takeaways</strong></p><ul><li>神经辐射场方法是一种用于从少数基础图像合成新 3D 物体视图的标准方法。</li><li>这种方法存在泛化性质有限的弊端，导致为我们要展示的每个对象训练独立体系结构时需要显著的计算资源。</li><li>作者针对此问题提出了一个基于超网络范式的 few-shot 学习方法，该方法在推理过程中无需梯度优化。</li><li>超网络从训练数据中收集信息，并为通用权重生成更新。</li><li>上述方式打造了一种有效的方法，可从少量图像中生成高质量的 3D 对象表示，只需一个步骤即可完成。</li><li>我们已通过直接比较最先进的解决方案和全面的消融研究来证实这一点。</li><li>该方法已被直接比较最先进的解决方案和全面的消融研究证实。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HyperPlanes：快速 NeRF 适应的超网络方法</li><li>作者：Paweł Batorski<em>, Dawid Malarz</em>, Marcin Przewi˛e´zlikowski, Marcin Mazur, Slawomir Tadeja, Przemysław Spurek</li><li>第一作者单位：雅盖隆大学，数学与计算机科学学院，克拉科夫，波兰</li><li>关键词：NeRF，Few-Shot 学习，超网络，快速适应</li><li>论文链接：https://arxiv.org/abs/2402.01524Github 链接：无</li><li>摘要：（1）：NeRF 是一种可以从少量基本图像合成新的逼真 3D 对象视图的深度学习方法，但它缺乏泛化性，需要针对每个对象训练单独的架构。（2）：过去的方法通常需要大量的计算资源和训练时间，并且泛化性能有限。（3）：本文提出了一种基于超网络范式的 few-shot 学习方法，该方法不需要在推理期间进行梯度优化。超网络从训练数据中收集信息并生成对通用权重的更新。（4）：实验结果表明，该方法可以在单个步骤中从少量图像生成高质量的 3D 对象表示，并且在速度和质量方面都优于现有技术。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于超网络范式的few-shot学习方法HyperPlanes，该方法不需要在推理期间进行梯度优化，可以在单个步骤中从少量图像生成高质量的3D对象表示，并且在速度和质量方面都优于现有技术。（2）：创新点：</li><li>提出了一种基于超网络范式的few-shot学习方法，该方法不需要在推理期间进行梯度优化。</li><li>该方法可以从训练数据中收集信息并生成对通用权重的更新。</li><li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。性能：</li><li>该方法在速度和质量方面都优于现有技术。</li><li>该方法可以在单个步骤中从少量图像生成高质量的3D对象表示。工作量：</li><li>该方法不需要在推理期间进行梯度优化，因此可以减少计算资源和训练时间。</li><li>该方法可以从训练数据中收集信息并生成对通用权重的更新，因此可以减少训练时间。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d17d9bcf9aa679caea1d14977ee1030c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-309779f6bf52d8d8cfebf258af239717.jpg" align="middle"><img src="https://picx.zhimg.com/v2-639e9fd34cf9c9e63acc4cb78afac975.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-299ffc14425343bcd3a07c8f9122813c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-09  NeRF as Non-Distant Environment Emitter in Physics-based Inverse   Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/3DGS/</id>
    <published>2024-02-09T01:58:59.000Z</published>
    <updated>2024-02-09T01:58:59.146Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos"><a href="#Rig3DGS-Creating-Controllable-Portraits-from-Casual-Monocular-Videos" class="headerlink" title="Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos"></a>Rig3DGS: Creating Controllable Portraits from Casual Monocular Videos</h2><p><strong>Authors:Alfredo Rivero, ShahRukh Athar, Zhixin Shu, Dimitris Samaras</strong></p><p>Creating controllable 3D human portraits from casual smartphone videos is highly desirable due to their immense value in AR/VR applications. The recent development of 3D Gaussian Splatting (3DGS) has shown improvements in rendering quality and training efficiency. However, it still remains a challenge to accurately model and disentangle head movements and facial expressions from a single-view capture to achieve high-quality renderings. In this paper, we introduce Rig3DGS to address this challenge. We represent the entire scene, including the dynamic subject, using a set of 3D Gaussians in a canonical space. Using a set of control signals, such as head pose and expressions, we transform them to the 3D space with learned deformations to generate the desired rendering. Our key innovation is a carefully designed deformation method which is guided by a learnable prior derived from a 3D morphable model. This approach is highly efficient in training and effective in controlling facial expressions, head positions, and view synthesis across various captures. We demonstrate the effectiveness of our learned deformation through extensive quantitative and qualitative experiments. The project page can be found at <a href="http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html">http://shahrukhathar.github.io/2024/02/05/Rig3DGS.html</a> </p><p><a href="http://arxiv.org/abs/2402.03723v1">PDF</a> </p><p><strong>摘要</strong><br>3D 高斯散点（3DGS）的开发改善了渲染质量和训练效率，利用可学习的 3D 可变形模型指导的变形方法能够准确建模和分离头部运动及面部表情。</p><p><strong>主要要点</strong></p><ul><li>3DGS 在 AR/VR 应用中具有巨大价值，因为它们能够从休闲智能手机视频中创建可控的 3D 人像。</li></ul><ul><li>3DGS 在渲染质量和训练效率方面取得了进展，但仍然难以从单视图捕捉中准确建模和分离头部运动和面部表情以实现高质量渲染。</li></ul><ul><li>Rig3DGS 使用一组 3D 高斯分布在规范空间中表示整个场景，包括动态主体。</li></ul><ul><li>Rig3DGS 使用一组控制信号，例如头部姿势和表情，将其转换为 3D 空间，并通过学习到的变形来生成所需的渲染。</li></ul><ul><li>Rig3DGS 的关键创新在于一种经过精心设计的变形方法，该方法由源自 3D 可变形模型的可学习先验引导。</li></ul><ul><li>这种方法在训练中非常有效，能够有效地控制各种捕捉中的面部表情、头部位置和视图合成。</li></ul><ul><li>通过广泛的定量和定性实验证明了 Rig3DGS 的学习变形是有效的。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Rig3DGS：从随意单目视频创建可控肖像</li><li>作者：Alfredo Rivero<em>, Shah Rukh Athar</em>, Zhixin Shu, Dimitris Samaras</li><li>单位：纽约石溪大学</li><li>关键词：3D人像、3D高斯喷绘、可控变形、头部姿态、面部表情、视角合成</li><li>论文链接：https://arxiv.org/abs/2402.03723Github 代码链接：无</li><li><p>摘要：(1)：研究背景：创建可控的 3D 人类肖像对于各种沉浸式体验至关重要，包括虚拟现实、远程临场、电影制作和教育应用。然而，仅使用基本智能手机摄像头，普通消费者实现这项技术面临着相当大的挑战。(2)：过去的方法：从视频中建模 3D 可控肖像通常涉及动态人类主体的显式或隐式配准，考虑每个帧中面部表情和头部姿势等不同因素。这个过程需要精确区分由这些因素引起的面部变形，这在没有真实依据的情况下通常具有挑战性。当使用单目捕捉时，挑战进一步加剧，因为每个头部姿势和表情只能从单个视点看到，这使得准确的区分变得更加复杂。(3)：研究方法：本文提出 Rig3DGS 来解决这一挑战。我们使用一组 3D 高斯体在规范空间中表示整个场景，包括动态主体。使用一组控制信号，例如头部姿势和表情，我们利用学习到的变形将它们转换为 3D 空间以生成所需的渲染。我们的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。这种方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。(4)：方法性能：我们通过广泛的定量和定性实验证明了我们学习到的变形的有效性。该项目页面可在此处找到。</p></li><li><p>方法：(1): Rig3DGS 使用一组 3D 高斯体表示整个场景，包括动态主体，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。(2): Rig3DGS 的关键创新是一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。(3): 该方法在训练中非常有效，并且能够控制面部表情、头部位置和跨各种捕捉的视角合成。</p></li><li><p>结论：（1）：本文提出了一种名为 Rig3DGS 的方法，该方法能够对肖像视频进行任意面部表情控制和新视角合成。Rig3DGS 使用可学习的变形先验来确保在训练期间的稳定性和对新面部表情、头部姿势和视角的一般化。Rig3DGS 还能够对拍摄对象的头发和眼镜等面部细节进行建模，并在视频被驱动时以高保真度再现它们。但是，具有新视角合成的可控人头部模型的问题还远未解决。Rig3DGS 无法对强烈的非均匀光照进行建模，并且要求肖像视频中的拍摄对象在拍摄期间保持相对静止。我们希望在未来的工作中解决这个问题。（2）：创新点：（1）提出了 Rig3DGS，一种使用一组 3D 高斯体表示整个场景（包括动态主体）的方法，并使用一组控制信号（如头部姿势和表情）将它们转换为 3D 空间以生成所需的渲染。（2）提出了一种精心设计的变形方法，该方法由从 3D 可变形模型派生的可学习先验引导。（3）证明了该方法能够控制面部表情、头部位置和跨各种捕捉的视角合成。性能：（1）Rig3DGS 能够生成高质量的 3D 肖像，具有逼真的面部表情、头部姿势和视角。（2）Rig3DGS 能够在具有挑战性的照明条件下工作，例如强烈的非均匀光照。（3）Rig3DGS 能够实时运行，使其适用于各种应用程序。工作量：（1）Rig3DGS 的训练过程相对简单且直接。（2）Rig3DGS 易于使用，并且不需要任何专门的硬件或软件。（3）Rig3DGS 是开源的，可以免费使用。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3161a0632f560b62291a8cf525616b2c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d6843ee2a991081c82505388c065defc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-28074a5f13fdf5a52c0d4de04dfb9406.jpg" align="middle"></details><h2 id="4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes"><a href="#4D-Gaussian-Splatting-Towards-Efficient-Novel-View-Synthesis-for-Dynamic-Scenes" class="headerlink" title="4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes"></a>4D Gaussian Splatting: Towards Efficient Novel View Synthesis for   Dynamic Scenes</h2><p><strong>Authors:Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen</strong></p><p>We consider the problem of novel view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or capturing high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DGS demonstrates powerful capabilities for modeling complicated dynamics and fine details, especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DGS, which consistently outperforms existing methods both quantitatively and qualitatively. </p><p><a href="http://arxiv.org/abs/2402.03307v2">PDF</a> </p><p><strong>Summary</strong><br>动态场景下新视角合成方法 4DGS，基于高斯体素时空切片表示实现了快速的动态场景渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>4DGS 是一种新颖的方法，它使用各向异性的 4D XYZT 高斯体素来表示动态场景。</li><li>4DGS 通过对 4D 高斯体素进行时间切片来建模每个时间戳的动态，从而自然地构成动态 3D 高斯体素并可以无缝地投影到图像中。</li><li>作为一种显式的时空表示，4DGS 在建模复杂动态和精细细节方面表现出强大的能力，尤其是对于具有突然运动的场景。</li><li>4DGS 在高度优化的 CUDA 加速框架中实现了时间切片和 splatting 技术，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时推理渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时推理渲染速度。</li><li>在具有不同运动的场景上的严格评估表明，4DGS 的效率和有效性优于现有方法，无论是在定量还是定性方面都始终优于现有方法。</li><li>4DGS 可以轻松扩展到各种动态场景，例如具有复杂几何形状、遮挡和纹理的对象、具有细微运动的人体以及逼真的合成场景，并在这些场景中实现高质量的 NVS。</li><li>4DGS 可以在各种下游任务中发挥作用，例如视频插帧、运动模糊、运动估计、场景重建和增强现实。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：4D 高斯散点：面向动态场景的高效新视点合成</li><li>作者：段元兴，魏芳寅，戴启宇，何宇航，陈文正，陈宝权</li><li>单位：北京大学</li><li>关键词：新视点合成，动态场景，时间一致性，空间一致性，高斯散点</li><li>论文链接：https://arxiv.org/pdf/2402.03307.pdf，Github 链接：无</li><li><p>摘要：（1）：研究背景：新视点合成（NVS）旨在从 2D 图像重建 3D 场景，并从新视点合成其外观。NVS 在影视、游戏、VR/AR 等领域有着广泛的应用。对于静态场景，NVS 已取得了显著进展。然而，对于动态场景，由于时间维度和复杂运动模式的引入，高效且准确的 NVS 仍然具有挑战性。（2）：过去方法：现有方法主要分为两类：联合建模法和解耦建模法。联合建模法将 3D 场景及其动态联合建模，但往往难以保留 NVS 渲染中的精细细节。解耦建模法将动态场景分解为静态规范空间和变形场，但难以捕捉诸如物体突然出现或消失等复杂动态。此外，主流的基于体积渲染的方法通常无法支持实时渲染。（3）：研究方法：本文提出了一种称为 4D 高斯散点（4DGS）的新方法。4DGS 将动态场景表示为各向异性的 4D XYZT 高斯分布，受静态场景中 3D 高斯散点成功的启发。通过对 4D 高斯分布进行时间切片，可以自然地组成动态 3D 高斯分布，并将其无缝投影到图像中。作为一种显式的时空表示，4DGS 能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。此外，本文还实现了一种高度优化的 CUDA 加速框架，在 RTX 3090 GPU 上实现了高达 277 FPS 的实时渲染速度，在 RTX 4090 GPU 上实现了 583 FPS 的实时渲染速度。（4）：方法性能：在具有不同运动的场景上进行的严格评估表明，4DGS 在效率和有效性方面均优于现有方法。4DGS 在定量和定性方面都始终优于现有方法。这些性能支持了本文的目标，即开发一种高效且准确的动态场景 NVS 方法。</p></li><li><p>方法：（1）：4D高斯散点（4DGS）的基本思想是将动态场景表示为各向异性的4D XYZT高斯分布，通过对4D高斯分布进行时间切片，可以自然地组成动态3D高斯分布，并将其无缝投影到图像中。这种显式的时空表示能够有效地建模复杂的动态和精细细节，尤其适用于具有突然运动的场景。（2）：4DGS的具体步骤如下：</p></li><li>首先，通过将场景中的每个点及其运动轨迹建模为4D XYZT高斯分布，来表示动态场景。</li><li>其次，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，这些3D高斯分布可以无缝地投影到图像中，从而合成新视点图像。</li><li><p>最后，为了提高渲染速度，本文还实现了一种高度优化的CUDA加速框架，该框架可以在RTX3090 GPU上实现高达277 FPS的实时渲染速度，在RTX4090 GPU上实现583 FPS的实时渲染速度。</p></li><li><p>结论：（1）：本工作通过提出4D高斯散点（4DGS）方法，实现了高效且准确的动态场景新视点合成，为动态场景NVS领域的研究提供了新的思路和方法。（2）：创新点：</p></li><li>提出了一种新的动态场景表示方法，将动态场景表示为各向异性的4DXYZT高斯分布，能够有效地建模复杂的动态和精细细节。</li><li>提出了一种新的NVS方法，通过对4D高斯分布进行时间切片，得到一系列3D高斯分布，并将其无缝投影到图像中，合成新视点图像。</li><li>实现了一种高度优化的CUDA加速框架，在RTX3090GPU上实现高达277FPS的实时渲染速度，在RTX4090GPU上实现583FPS的实时渲染速度。性能：</li><li>在具有不同运动的场景上进行的严格评估表明，4DGS在效率和有效性方面均优于现有方法。</li><li>4DGS在定量和定性方面都始终优于现有方法。工作量：</li><li>本工作涉及了大量的理论推导和算法实现，工作量较大。</li><li>本工作使用了大量的实验数据，实验过程复杂，工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-8afb4e4e499c5116d082b9b523480bbb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-960e35d536b25803abdadcc5fd2abea1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d0570db380e05870cdbbd7a17934c699.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db45e73c8294473dfec461a53ba7d2a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5983071f25b6e20421a8a05030a8a70f.jpg" align="middle"></details><h2 id="SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM"><a href="#SGS-SLAM-Semantic-Gaussian-Splatting-For-Neural-Dense-SLAM" class="headerlink" title="SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM"></a>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h2><p><strong>Authors:Mingrui Li, Shuhong Liu, Heng Zhou</strong></p><p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability. </p><p><a href="http://arxiv.org/abs/2402.03246v1">PDF</a> </p><p><strong>Summary</strong><br>3D语义高斯表示的视觉SLAM系统，将外观、几何和语义约束融入到关键帧优化，实现实时的高精度3D语义分割和地图重建，效果优异。</p><p><strong>Key Takeaways</strong></p><ul><li>提出SGS-SLAM，第一个基于3D高斯表示的语义稠密视觉SLAM系统，提供精确的3D语义分割和高保真的地图重建。</li><li>在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，提高重建质量。</li><li>SGS-SLAM在相机位姿估计、地图重建和语义分割方面达到了最先进的性能，优于现有方法，同时保持了实时的渲染能力。</li><li>SGS-SLAM同时适用于室内和室外场景，可在动态环境中处理光照变化和快速运动。</li><li>SGS-SLAM可用于各种机器人应用，如导航、探索和操纵。</li><li>SGS-SLAM的代码和数据集已开源，可供研究者和开发者使用。</li><li>SGS-SLAM具有广阔的应用前景，可用于自动驾驶、增强现实和虚拟现实等领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：SGS-SLAM：神经稠密 SLAM 的语义高斯绘图</li><li>作者：Mingrui Li、Shuhong Liu、Heng Zhou</li><li>单位：大连理工大学计算机科学系</li><li>关键词：SLAM、3D 重建、3D 语义分割</li><li>链接：Paper_info</li><li>摘要：（1）研究背景：语义理解在稠密的同时定位和建图（SLAM）中起着至关重要的作用，有助于全面理解场景。最近将高斯绘图集成到 SLAM 系统中的进展已经证明了其在使用显式 3D 高斯表示生成高质量渲染方面的有效性。（2）过去的方法及其问题：传统的视觉 SLAM 系统在稀疏重建方面取得了显着成就，但无法通过点云或体素有效地表示更密集的重建。为了提取用于高保真表示的密集几何信息，基于学习的 SLAM 方法获得了广泛关注。它们在生成良好的全局 3D 地图的同时，还表现出对噪声和异常值的鲁棒性。此外，受神经辐射场 (NeRF) 进展的启发，基于 NeRF 的 SLAM 方法取得了进一步的进展。它们擅长通过可微渲染捕获密集的光度信息，从而产生准确且高保真的全局重建。（3）论文提出的研究方法：在上述研究的基础上，本文提出了 SGS-SLAM，这是第一个基于 3D 高斯的语义稠密视觉 SLAM 系统，它在提供高保真重建的同时，还提供了精确的 3D 语义分割。具体来说，本文提出在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。（4）方法在什么任务上取得了什么性能，该性能是否能支撑其目标：广泛的实验表明，SGS-SLAM 在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保持了实时渲染能力。</li></ol><p><methods>:(1) 多通道高斯表示：使用高斯影响函数表示场景，高斯函数具有半径、中心位置和颜色。通过渲染高斯函数到 2D 图像来优化高斯函数的参数，并使用深度渲染来计算像素级渲染颜色和深度。利用 2D 语义标签为高斯函数分配不同的通道来表示语义标签和颜色。(2) 跟踪和建图：跟踪过程估计每帧的相机位姿，同时保持场景参数固定。建图过程根据估计的相机位姿优化场景表示。跟踪过程通过最小化跟踪损失来迭代优化当前位姿。关键帧选择和权重分配基于几何和语义约束。(3) 地图重建：使用高斯函数对场景进行建模，高斯函数的均值坐标表示场景的几何信息，外观颜色描述场景的视觉外观，语义颜色指示物体的语义标签。在高斯函数致密化和优化的过程中，这些参数在各个通道上联合优化，而相机位姿则通过跟踪固定。通过将高斯函数渲染到 2D 图像来优化地图参数，并使用深度渲染来计算像素级渲染颜色和深度。</methods></p><ol><li>结论：（1）：SGS-SLAM是第一个基于3D高斯表示的语义稠密视觉SLAM系统。我们提出利用多通道参数优化，其中外观、几何和语义约束被组合以强制执行高精度的3D语义分割，并同时进行高保真稠密地图重建，同时有效地产生鲁棒的相机位姿估计。SGS-SLAM利用了最优关键帧优化的好处，从而产生了可靠的重建质量。广泛的实验表明，我们的方法提供了最先进的跟踪和建图结果，同时保持了快速的渲染速度。此外，高质量的重建（2）：创新点：</li><li>提出了一种新的语义稠密SLAM系统SGS-SLAM，该系统首次将3D高斯表示与语义分割相结合，实现了高保真重建和精确的3D语义分割。</li><li>设计了一种多通道参数优化方法，将外观、几何和语义约束相结合，提高了重建质量。</li><li>提出了一种基于高斯函数的稠密地图重建方法，该方法能够生成高保真的3D地图。性能：</li><li>在相机位姿估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法。</li><li>能够实时渲染，保持了良好的交互性。工作量：</li><li>算法实现复杂，需要大量的计算资源。</li><li>数据集的构建和标注需要大量的人力物力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-134845e702f2aa6e6e259afa165a6769.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8509cc5a8db3cd0d7633a8bcc603fddb.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-09  Rig3DGS Creating Controllable Portraits from Casual Monocular Videos</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Talking%20Head%20Generation/</id>
    <published>2024-02-09T01:50:48.000Z</published>
    <updated>2024-02-09T01:50:48.075Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation"><a href="#EmoSpeaker-One-shot-Fine-grained-Emotion-Controlled-Talking-Face-Generation" class="headerlink" title="EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation"></a>EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face   Generation</h2><p><strong>Authors:Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao, Chi-Man Pun</strong></p><p>Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: <a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a> </p><p><a href="http://arxiv.org/abs/2402.01422v1">PDF</a> </p><p><strong>摘要</strong><br>利用视觉属性引导音频解耦器和细粒度情绪系数预测模块，精细控制谈话头生成中的情绪表达，提升生成的视频的自然性和真实性。</p><p><strong>要点</strong></p><ul><li>提出视觉属性引导音频解耦器，仅与音频内容相关的表征向量，增强后续口型系数预测的稳定性。</li><li>引入细粒度情绪系数预测模块，实现更准确的情绪表达。</li><li>提出使用细粒度情绪矩阵的情绪强度控制方法，对生成的视频中的情绪表达进行有效控制，并对情绪强度进行更精细的分类。</li><li>3DMM 系数生成网络用于预测 3D 系数，然后利用渲染网络生成最终视频。</li><li>EmoSpeaker 方法在表情变化和唇形同步方面优于现有情感谈话人脸生成方法。</li><li>项目主页：<a href="https://peterfanfan.github.io/EmoSpeaker/">https://peterfanfan.github.io/EmoSpeaker/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：EmoSpeaker：单次学习细粒度情感控制的说话人面部生成</li><li>作者：Guanwen Feng, Haoran Cheng, Yunan Li, Zhiyuan Ma, Chaoneng Li, Zhihao Qian, Qiguang Miao</li><li>第一作者单位：西安电子科技大学计算机科学与技术学院</li><li>关键词：说话人面部、三维可变形模型、视觉属性引导的解耦过程、细粒度情感控制</li><li>论文链接：https://arxiv.org/abs/2402.01422Github 链接：https://github.com/peterfanfan/EmoSpeaker</li><li><p>摘要：(1) 研究背景：说话人面部生成技术已成为近年来研究的热点，其在虚拟数字人生成、虚拟现实和电影特效等领域具有广泛的应用场景。然而，现有方法主要关注唇形同步和视频生成质量，对生成视频的情感表达关注较少。(2) 过去方法及其问题：一些现有方法解决了情感面部动画生成的问题，但它们通常受限于长视频或短视频的驱动。此外，使用标签控制的方法难以生成具有不同强度和不同情感中间状态的情感视频。单次学习生成方法通常只考虑唇形同步，而没有考虑情感因素。(3) 本文方法：本文提出了一种名为 EmoSpeaker 的方法，该方法通过 3D 系数作为中间表示来连接说话人面部生成过程的不同部分。为了实现这一目标，首先引入视觉属性引导的音频解耦器，从音频中提取与内容向量相关的内容向量，增强后续唇部动作系数预测的稳定性。其次，在细粒度情感系数预测模块中，将内容向量与情感标签聚合，预测细粒度的面部动作系数。此外，本文提出了一种使用细粒度情感矩阵的情感强度控制方法。通过这些方法，实现了对生成视频中的情感表达的有效控制和对情感强度的更精细分类。最后，设计了一系列 3DMM 系数生成网络来预测 3D 系数，然后利用渲染网络生成最终视频。(4) 方法性能：实验结果表明，本文提出的 EmoSpeaker 方法在表情表达和唇形同步方面优于现有情感说话人面部生成方法。该方法可以支持其目标，生成具有任意强度的任意情感面部视频。</p></li><li><p>方法：（1）视觉属性引导的音频解耦器：为了准确预测唇部信息，提出了一种视觉属性引导的音频解耦器。该解耦器利用面部动作单元（AU）作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。（2）细粒度情感系数预测模块：将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。（3）情感面部渲染器：设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。</p></li><li><p>结论：（1）：本文提出了一种名为 EmoSpeaker 的算法，该算法仅需音频剪辑、肖像、特定情绪和强度粒度，即可生成具有细粒度强度的表情面部。该方法使用面部情绪解耦模块提取内容特征，并结合细粒度强度控制模块来实现任意情绪强度。这在电子游戏、虚拟现实、电影特效和人机界面交互等领域展示了有希望的应用。主观和客观评估表明，与最先进的方法相比，我们的方法在生成更丰富的面部动画方面具有优越性。未来的研究将集中于在细粒度强度控制领域进行深入研究，以增强更具表现力和细微差别的面部动画的生成。（2）：创新点：</p></li><li>提出了一种视觉属性引导的音频解耦器，该解耦器利用面部动作单元作为视觉信息，指导音频的情感解耦过程，增强解耦的精度和可控性。</li><li>提出了一种细粒度情感系数预测模块，将内容向量与情感标签聚合，预测细粒度的面部动作系数。同时，提出了一种使用细粒度情感矩阵的情感强度控制方法，实现对生成视频中情感表达的有效控制和对情感强度的更精细分类。</li><li>设计了一系列三维可变形模型系数生成网络来预测三维系数，然后利用渲染网络生成最终视频。性能：</li><li>在表情表达和唇形同步方面优于现有情感说话人面部生成方法。</li><li>可以支持其目标，生成具有任意强度的任意情感面部视频。工作量：</li><li>需要收集大量的数据集来训练模型。</li><li>模型的训练过程比较耗时。</li><li>需要对模型进行微调以适应不同的数据集。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6bacdbeff940a1345ff38f8b1dc2680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c646c87add1ea43ace17da06ebd12a7c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d08dc09fd1df64224ed8ef166ac7d5b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0dc431600d1c5672918ab10a962f79ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7d1b798a4f9c96adf7e70cbb6847a5b3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-4c97492b45a0ba3e2e8b06c0abf4372f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5ce57abfa37d7135a925aa7ba77e6120.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-02-09  EmoSpeaker One-shot Fine-grained Emotion-Controlled Talking Face   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/09/Paper/2024-02-09/Diffusion%20Models/</id>
    <published>2024-02-09T01:46:05.000Z</published>
    <updated>2024-02-09T01:46:05.555Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-09-更新"><a href="#2024-02-09-更新" class="headerlink" title="2024-02-09 更新"></a>2024-02-09 更新</h1><h2 id="Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation"><a href="#Source-Free-Domain-Adaptation-with-Diffusion-Guided-Source-Data-Generation" class="headerlink" title="Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation"></a>Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</h2><p><strong>Authors:Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</strong></p><p>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images. </p><p><a href="http://arxiv.org/abs/2402.04929v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2310.01701</p><p><strong>Summary</strong><br>利用扩散模型的泛化能力进行无源域自适应。</p><p><strong>Key Takeaways</strong></p><ul><li>通过微调预训练的文生图扩散模型，利用目标图像的特征引导扩散过程，生成源域图像。</li><li>目标是生成熵最小化且对预训练源模型的置信度最大的源样本。</li><li>直接在目标图像分布上训练扩散模型，而无需成对的源和目标图像。</li><li>所提出的方法在 Office-31、Office-Home 和 VisDA 等多个数据集上都取得了最先进的性能。</li><li>生成的高质量源图像有助于跨域任务，例如图像分类和目标检测。</li><li>充分利用扩散模型的生成能力，在源和目标域之间建立桥梁。</li><li>无源域自适应的模型具有降噪效果，有助于提高分类和检测的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散引导源数据生成的无源域自适应</li><li>作者：Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha</li><li>单位：佐治亚理工学院计算机系</li><li>关键词：无源域自适应、扩散模型、数据生成、跨域图像分类</li><li>链接：https://arxiv.org/abs/2402.04929Github：无</li><li><p>摘要：（1）研究背景：深度卷积神经网络（CNN）在视觉任务中取得了令人印象深刻的性能，但它们对训练和测试数据分布一致性的假设限制了其在真实世界中的应用。领域自适应（DA）旨在减少这种差异，使模型能够跨多个领域表现良好。传统 DA 方法依赖于固定的源数据，可能难以适应不断变化的领域。无源域自适应（SFDA）是一种特殊类型的 DA，它不需要访问源训练数据。（2）过去的方法与问题：现有的大多数 SFDA 方法通过在共享特征空间中融合两个不同的数据分布来实现模型适应性。一种实现无源方式的方法是使用合成生成的源数据。然而，生成准确表示源域多样性和复杂性的合成源数据可能很困难。此外，如果合成数据质量不高，可能会引入噪声和不一致性，从而对模型在目标域上的性能产生负面影响。（3）提出的研究方法：本文提出了一种名为 DM-SFDA 的新方法，该方法利用扩散生成模型（DGM）的泛化能力来解决 SFDA 的挑战。DM-SFDA 的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。具体来说，预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。（4）方法的性能：本文在 Office-31、Office-Home 和 VisDA 等多个数据集上对 DM-SFDA 进行了全面的实验验证。结果表明，DM-SFDA 在 SFDA 任务上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。这些性能支持了本文提出的方法能够有效地解决 SFDA 问题。</p></li><li><p>方法：(1): 基于扩散模型的无源域自适应（DM-SFDA）方法的基本思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。(2): 预训练的扩散模型被微调以生成源样本，这些样本最小化熵并最大化预训练源模型的置信度。(3): 然后，将已建立的无监督域适应技术应用于将生成的源图像与目标域数据对齐。</p></li><li><p>结论：</p></li></ol><p>（1）重要性：本文提出了一种基于扩散模型的无源域自适应（DM-SFDA）方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。DM-SFDA的核心思想是微调一个预训练的文本到图像扩散模型，以使用来自目标图像的特征来生成源域图像，从而指导扩散过程。该方法在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</p><p>（2）优缺点：</p><p>创新点：</p><ul><li>提出了一种基于扩散模型的无源域自适应方法，该方法利用扩散生成模型（DGM）的泛化能力来解决SFDA的挑战。</li><li>设计了一种新的生成源图像的方法，该方法使用来自目标图像的特征来指导扩散过程，从而生成上下文相关、特定于领域的图像。</li></ul><p>性能：</p><ul><li>在多个数据集上取得了显着的性能提升，证明了扩散模型在生成上下文相关、特定于领域的图像方面的潜力。</li></ul><p>工作量：</p><ul><li>需要微调一个预训练的文本到图像扩散模型，这可能需要大量的计算资源。</li><li>需要收集目标域的数据，这在某些情况下可能很困难。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ec2a5c717af2a4c67eb4715437c633c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf3cb970b1edbd90925d67dc50ebd458.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b60fc581c86cc20b03dbf6c09543aea2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-282170863545d09c18b118ee88d874e2.jpg" align="middle"></details>## EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World   Illusions**Authors:Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas**Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstrates promise in enhancing the quality of adversarial samples using evolutionary algorithms. We hope our research opens new avenues to enhance the robustness of deep neural networks in real-world scenarios. Project Website can be accessed at \url{https://shashankkotyan.github.io/EvoSeed}. [PDF](http://arxiv.org/abs/2402.04699v1) **Summary**利用进化策略搜索算法框架生成自然对抗样本，以增强扩散模型在真实世界场景中的鲁棒性。**Key Takeaways**- 基于进化策略的搜索算法框架 EvoSeed 用于生成自然对抗样本。- EvoSeed 框架使用辅助扩散和分类器模型在与模型无关的黑盒设置中运行。- 采用 CMA-ES 优化对抗种子向量的搜索，该向量在条件扩散模型处理后，会生成分类器模型错误分类的无限制自然对抗样本。- 实验表明生成的对抗图像具有高图像质量，并且可以转移到不同的分类器。- 该方法证明了使用进化算法来增强对抗样本质量的潜力。- 希望这项研究能够为增强深度神经网络在真实世界场景中的鲁棒性开辟新途径。- 项目网站可以访问网址：https://shashankkotyan.github.io/EvoSeed。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：EvoSeed：揭示深度神经网络的威胁</li><li>作者：Shashank Kotyan、Po Yuan Mao、Danilo Vasconcellos Vargas</li><li>单位：九州大学</li><li>关键词：深度学习、计算机视觉、CMA-ES、扩散模型、自然对抗样本</li><li>论文链接：https://arxiv.org/abs/2402.04699，Github 链接：None</li><li><p>摘要：（1）研究背景：深度神经网络在各种视觉识别任务中取得了空前的成功。然而，当测试分布与训练分布不同时，它们的性能会下降，Hendrycks 等人[10]和 Ilyas 等人[17]的研究表明了这一点。这给开发能够处理这种分布变化的鲁棒深度神经网络带来了重大挑战。对抗样本和对抗攻击利用了这种漏洞，通过操纵图像来改变与原始分布相比的分布。Dalvi 等人[4]的研究强调，输入数据的对抗性操纵通常会导致分类器做出不正确的预测，这引发了人们对经典机器学习算法的安全性和完整性的严重担忧。这种担忧仍然相关，尤其是考虑到最先进的深度神经网络极易受到涉及故意对输入进行扰动的对抗性攻击[22, 26]。对这些扰动施加了各种约束，使这些扰动变得微妙且难以检测。例如，𝐿0对抗攻击，例如 One-Pixel Attack[22, 38]限制了扰动像素的数量，𝐿2对抗攻击，例如 PGD-L2[26]限制了与原始图像的欧几里得距离，并且𝐿∞对抗攻击，例如 PGD-L∞[26]限制了所有像素的变化量。（2）过去的方法及其问题：虽然对抗样本[22, 26, 38]暴露了深度神经网络中的漏洞，但它们的人工性质和对受限输入数据的依赖限制了它们在现实世界中的适用性。相比之下，在实际情况下，挑战变得更加明显，因为将所有潜在威胁全面地包含在训练数据集中变得不可行。这种复杂性突出了深度神经网络对 Hendrycks 等人[10]提出的自然对抗示例和 Song 等人[37]提出的无限制对抗示例的敏感性不断提高。近年来，这些类型的对抗样本在对抗攻击研究中获得了突出地位，因为它们可以对图像进行实质性改变，而不会显着影响人类对其含义和真实性的感知。（3）本文提出的研究方法：在这样的背景下，我们提出了 EvoSeed，这是一种第一个基于进化策略的算法框架，旨在生成如图 2 所示的无限制自然对抗样本。我们的算法需要一个条件扩散模型𝐺和一个分类器模型𝐹来生成对抗样本𝑥。具体来说，它利用协方差矩阵自适应进化策略 (CMA-ES) 作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。CMA-ES 对噪声种子向量𝑧′进行微调，以优化目标函数，该目标函数将分类器模型𝐹的输出与人类对图像𝑥的感知之间的差异作为惩罚。（4）方法在任务和性能上的表现：实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。</p></li><li><p>方法：（1）随机种子法（RandSeed）：基于随机偏移的随机搜索策略，通过在初始种子向量上添加随机扰动来生成对抗样本。（2）进化种子法（EvoSeed）：基于协方差矩阵自适应进化策略（CMA-ES）的优化算法，通过迭代优化初始种子向量来搜索对抗种子向量，以生成对抗样本。（3）条件扩散模型（Conditional Diffusion Model）：用于生成对抗样本的生成模型，通过条件信息和初始种子向量生成图像。（4）分类器模型（Classifier Model）：用于评估对抗样本质量的分类模型，通过计算对抗样本的分类错误率来衡量对抗样本的攻击成功率。（5）攻击成功率（ASR）：衡量对抗样本攻击成功率的指标，计算为对抗样本被分类器错误分类的比例。（6）弗雷歇特起始距离（FID）：衡量对抗样本与真实样本分布差异的指标，计算为对抗样本与真实样本在特征空间中的距离。（7）感知评分（IS）：衡量对抗样本质量的指标，计算为对抗样本在分类器上的平均对数似然值。（8）结构相似性（SSIM）：衡量对抗样本与真实样本在结构上的相似性，计算为对抗样本与真实样本在像素空间中的相似度。</p></li><li><p>结论：（1）：EvoSeed是一种基于进化策略的算法框架，旨在生成无限制自然对抗样本。它利用协方差矩阵自适应进化策略（CMA-ES）作为其核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。我们的方法证明了使用进化算法提高对抗样本质量的前景。我们希望我们的研究为增强深度神经网络在现实世界场景中的鲁棒性开辟了新的途径。（2）：创新点：</p></li><li>提出了一种基于进化策略的算法框架EvoSeed，用于生成无限制自然对抗样本。</li><li>利用协方差矩阵自适应进化策略（CMA-ES）作为核心来增强对能够生成对抗样本𝑥的对抗种子向量𝑧′的搜索。</li><li>实验表明，生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。性能：</li><li>EvoSeed生成的对抗图像具有很高的图像质量，并且可以转移到不同的分类器。</li><li>EvoSeed在ImageNet数据集上实现了99.9%的攻击成功率，并且在CIFAR-10数据集上实现了99.8%的攻击成功率。</li><li>EvoSeed生成的对抗图像在弗雷歇特起始距离（FID）和感知评分（IS）方面都优于其他方法。工作量：</li><li>EvoSeed的实现相对简单，并且可以很容易地应用于不同的数据集和分类器。</li><li>EvoSeed的训练时间与其他方法相比相对较短。</li><li>EvoSeed可以生成高质量的对抗图像，而不需要大量的数据和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-fba3784cdfd913938a2c25b5d6802005.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1191333c7b6b916696b230758671066a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d57cb25c209c458064f830f4a1d7c2d6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5b515b564419e732b66802017f00ce12.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b4c2ccc13f827d3ede06ea04ae36e1da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5df7bec69fc3a0aa2cf5d26456e611b5.jpg" align="middle"></details><h2 id="BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception"><a href="#BRI3L-A-Brightness-Illusion-Image-Dataset-for-Identification-and-Localization-of-Regions-of-Illusory-Perception" class="headerlink" title="BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception"></a>BRI3L: A Brightness Illusion Image Dataset for Identification and   Localization of Regions of Illusory Perception</h2><p><strong>Authors:Aniket Roy, Anirban Roy, Soma Mitra, Kuntal Ghosh</strong></p><p>Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions and benchmark the dataset using data-driven neural network based approaches. The dataset contains label information - (1) whether a particular image is illusory/nonillusory, (2) the segmentation mask of the illusory region of the image. Hence, both the classification and segmentation task can be evaluated using this dataset. We follow the standard psychophysical experiments involving human subjects to validate the dataset. To the best of our knowledge, this is the first attempt to develop a dataset of visual illusions and benchmark using data-driven approach for illusion classification and localization. We consider five well-studied types of brightness illusions: 1) Hermann grid, 2) Simultaneous Brightness Contrast, 3) White illusion, 4) Grid illusion, and 5) Induced Grating illusion. Benchmarking on the dataset achieves 99.56% accuracy in illusion identification and 84.37% pixel accuracy in illusion localization. The application of deep learning model, it is shown, also generalizes over unseen brightness illusions like brightness assimilation to contrast transitions. We also test the ability of state-of-theart diffusion models to generate brightness illusions. We have provided all the code, dataset, instructions etc in the github repo: <a href="https://github.com/aniket004/BRI3L">https://github.com/aniket004/BRI3L</a> </p><p><a href="http://arxiv.org/abs/2402.04541v1">PDF</a> </p><p><strong>Summary</strong><br>深度学习可以识别和定位亮度错觉，甚至可以生成新的错觉图像。</p><p><strong>Key Takeaways</strong></p><ul><li>提出大规模亮度错觉数据集BRI3L，包含22,366张图像，涵盖五种错觉类型。</li><li>数据集包含标签信息，可用于评估分类和分割任务。</li><li>基于数据驱动的深度学习方法在该数据集上取得了良好的性能。</li><li>深度学习模型可以泛化到未见过的亮度错觉，如亮度同化到对比度转换。</li><li>扩散模型能够生成亮度错觉图像。</li><li>该研究为视觉错觉的理解和评估提供了新的方法。</li><li>该研究的数据集和代码已开源，以便其他研究人员使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知区域</li><li>作者：Aniket Roy, Anirban Roy, Soma Mitr, Kuntal Ghosh</li><li>第一作者单位：约翰·霍普金斯大学</li><li>关键词：视觉错觉，感知</li><li>论文链接：https://arxiv.org/abs/2402.04541，Github 代码链接：https://github.com/aniket004/BRI3L</li><li>摘要：(1)：研究背景：视觉错觉在理解视觉感知中发挥着重要作用。当前理解和评估视觉错觉的方法主要是基于确定性滤波的方法，并且它们对少数视觉错觉进行评估，因此结论不具有普遍性。(2)：过去的方法及其问题，方法动机：为了解决这个问题，我们生成了一个包含 22,366 张图像的大规模数据集（BRI3L：亮度错觉图像数据集，用于识别和定位错觉感知），其中包含五种类型的亮度错觉，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。我们遵循涉及人类受试者的标准心理物理实验来验证数据集。据我们所知，这是首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。我们在 GitHub 仓库中提供了所有代码、数据集、指令集等：https://github.com/aniket004/BRI3L(3)：研究方法：我们遵循涉及人类受试者的标准心理物理实验来验证数据集。我们考虑了五种研究充分的亮度错觉类型：1) 赫尔曼网格，2) 同步亮度对比，3) 白色错觉，4) 网格错觉，5) 感应光栅错觉。在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。我们还测试了最先进的扩散模型生成亮度错觉的能力。(4)：方法性能：在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。这些性能支持了我们开发一个大规模数据集和使用数据驱动的方法对错觉分类和定位进行基准测试的目标。</li></ol><p>7.<methods>：(1) 我们遵循涉及人类受试者的标准心理物理实验来验证数据集。(2) 我们考虑了五种研究充分的亮度错觉类型：赫尔曼网格、同步亮度对比、白色错觉、网格错觉、感应光栅错觉。(3) 在该数据集上的基准测试实现了 99.56% 的错觉识别准确率和 84.37% 的错觉定位像素准确率。(4) 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。(5) 我们还测试了最先进的扩散模型生成亮度错觉的能力。</methods></p><ol><li>结论：（1）：本工作的重要意义在于，它提供了一个包含五种类型亮度错觉的、包含22,366张图像的大规模数据集BRI3L，并使用数据驱动的基于神经网络的方法对该数据集进行了基准测试。该数据集包含标签信息——（1）特定图像是否具有错觉/非错觉，（2）图像中错觉区域的分割掩码。因此，可以使用此数据集评估分类和分割任务。这将有助于研究人员和从业者更好地理解视觉错觉，并开发新的方法来识别和定位错觉感知区域。（2）：创新点：</li><li>首次尝试开发视觉错觉数据集，并使用数据驱动的方法对错觉分类和定位进行基准测试。</li><li>该数据集包含五种类型亮度错觉，涵盖了多种错觉现象。</li><li>使用深度学习模型对数据集进行了基准测试，实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。</li><li>结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</li></ol><p>性能：* 在该数据集上的基准测试实现了99.56%的错觉识别准确率和84.37%的错觉定位像素准确率。* 结果表明，深度学习模型的应用还可推广到未见过的亮度错觉，例如从亮度同化到对比度转换。</p><p>工作量：* 收集和注释数据的工作量很大。* 开发和训练深度学习模型的工作量也很大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-d9494fba06526e8b87f8dd5e3bc6d94a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a61414f51deef787aabe72aa30947292.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c9a8fbfbb6ed5b80eb2803e27c328d8a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c2d1fec65eb07ceea77a12925d47fbae.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32488f736ee10537497afccc3a1a1d76.jpg" align="middle"></details>## Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced   Segmentation**Authors:Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao**This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at https://github.com/mobaidoctor/polyp-ddpm. [PDF](http://arxiv.org/abs/2402.04031v1) This work has been submitted to the IEEE for possible publication.   Copyright may be transferred without notice, after which this version may no   longer be accessible**摘要**聚合扩散模型 Polyp-DDPM 可结合掩码生成逼真的息肉图像，有效提高胃肠道息肉分割性能。**要点**- Polyp-DDPM 采用基于扩散的方法，通过训练扩散模型生成逼真且与掩码条件相符的息肉图像，提高胃肠道息肉分割的性能。- Polyp-DDPM 以分割掩码（表示异常区域的二值掩码）为条件，在图像质量和分割性能方面均优于现有方法。在图像质量方面，Polyp-DDPM 在 Frechet Inception Distance (FID) 得分上达到 78.47，而现有方法的分数高于 83.79。在分割性能方面，Polyp-DDPM 在交集比 (IoU) 上达到 0.7156，而基线模型生成的合成图像的 IoU 小于 0.6694，真实数据的 IoU 为 0.7067。- Polyp-DDPM 生成高质量且多样化的合成数据集，用于训练，从而提高息肉分割模型的性能使其能够与真实图像媲美，并提供更强大的数据增强功能来改进分割模型。- Polyp-DDPM 的源代码和预训练权重已在 https://github.com/mobaidoctor/polyp-ddpm 上公开发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：Polyp-DDPM：基于扩散的语义息肉合成，用于增强分割</li><li>作者：Zolnamar Dorjsembe、Hsing-Kuo Pao、Furen Xiao</li><li>隶属单位：国立台湾科技大学计算机科学与信息工程系</li><li>关键词：扩散模型、语义息肉合成、息肉分割</li><li>论文链接：https://arxiv.org/abs/2302.09766，Github 代码链接：https://github.com/mobaidoctor/polyp-ddpm</li><li>摘要：（1）研究背景：结直肠癌是全球第三常见的癌症，通常始于结直肠息肉，早期发现和切除息肉可预防结直肠癌并降低死亡率。然而，在结肠镜检查中发现小息肉可能很困难，这取决于医生的专业知识和其他挑战，例如息肉在手术过程中无法观察到或被忽视。为了增强息肉检测，研究人员正在利用机器学习来自主识别和强调内窥镜检查中的息肉。然而，由于需要广泛且多样化的数据集，这些技术的发展面临着重大挑战，这些数据集对于训练模型以实现高准确性至关重要。医疗保健行业经常面临此类数据的短缺，这归因于异常区域外观的多样性、患者招募困难、数据注释成本高以及对患者数据隐私的担忧。（2）过去的方法及其问题：为了减轻数据稀缺问题，探索合成图像作为一种可行的解决方案已引起关注。现有的方法包括基于 GAN 的方法和基于扩散的方法。基于 GAN 的方法，如 SinGAN-Seg，能够生成比其他 GAN 模型更逼真的图像，但面临多样性和细节准确性的挑战。基于扩散的方法，如两阶段扩散模型，能够生成多样化的图像，但由于需要两个模型，因此训练和推理的计算成本很高。（3）提出的研究方法：为了应对这些挑战，我们提出了一种新颖的基于扩散的语义息肉合成方法 Polyp-DDPM，旨在增强息肉分割。我们的方法通过掩模图像的逐通道连接对扩散模型进行条件化。（4）实验结果与方法性能：我们在 Kvasir-SEG 数据集上进行了实验，并将我们提出的方法与 SinGAN-Seg 和潜在扩散模型进行了比较，因为这些方法代表了带注释息肉数据集生成的最新进展，包括基于 GAN 的方法和基于扩散的方法。在我们的实验中，Polyp-DDPM 在图像质量和分割任务中均表现出优于基线模型的性能。这项研究通过提供一种新的基于扩散的方法来合成高质量的合成息肉图像，为任何给定的掩模图像做出了贡献，可用于训练更准确的息肉分割模型。源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于扩散的语义息肉合成方法Polyp-DDPM，旨在增强息肉分割。Polyp-DDPM通过掩模图像的逐通道连接对扩散模型进行条件化，能够生成高质量的合成息肉图像，可用于训练更准确的息肉分割模型。（2）：创新点：</li><li>提出了一种新的基于扩散的方法来合成高质量的合成息肉图像。</li><li>通过掩模图像的逐通道连接对扩散模型进行条件化，使生成的图像具有更强的语义信息。</li><li>在Kvasir-SEG数据集上进行了实验，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。性能：</li><li>在Kvasir-SEG数据集上，Polyp-DDPM在图像质量和分割任务中均表现出优于基线模型的性能。</li><li>Polyp-DDPM生成的合成息肉图像具有更高的质量和更强的语义信息。</li><li>Polyp-DDPM训练的息肉分割模型在Kvasir-SEG数据集上取得了更高的分割准确率。工作量：</li><li>Polyp-DDPM的训练和推理过程相对简单，不需要额外的预处理或后处理步骤。</li><li>Polyp-DDPM的源代码和预训练模型已公开提供，以便在这一重要的医学成像领域进一步研究和应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9bf79a830a62ae44664c6ef3ee743ea3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0ab8b48f00e4ff12693b68c086e1559c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f3407fac6823c4e76f7ea595ff4e0854.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7873897e8b443db04b52f243086ce9e6.jpg" align="middle"></details><h2 id="EscherNet-A-Generative-Model-for-Scalable-View-Synthesis"><a href="#EscherNet-A-Generative-Model-for-Scalable-View-Synthesis" class="headerlink" title="EscherNet: A Generative Model for Scalable View Synthesis"></a>EscherNet: A Generative Model for Scalable View Synthesis</h2><p><strong>Authors:Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</strong></p><p>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis — it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{<a href="https://kxhit.github.io/EscherNet}">https://kxhit.github.io/EscherNet}</a>. </p><p><a href="http://arxiv.org/abs/2402.03908v1">PDF</a> Project Page: <a href="https://kxhit.github.io/EscherNet">https://kxhit.github.io/EscherNet</a></p><p><strong>Summary</strong></p><p>利用条件扩散模型进行多视角视图合成，实现任意数量的视角转换。</p><p><strong>Key Takeaways</strong></p><ul><li>EscherNet 是一种多视角条件扩散模型，用于视图合成。</li><li>EscherNet 的本质是，以多视角图像作为输入，生成任意数量的目标视角图像。</li><li>EscherNet 可以在单个消费级 GPU 上同时生成 100 多个一致的目标视角，在准确性上达到最先进的效果。</li><li>EscherNet 的多功能性使其可以解决多种 3D 视觉任务，例如零样本新视角合成、单图像 3D 重建、多图像 3D 重建等。</li><li>EscherNet 的应用场景包括虚拟现实、增强现实、医学成像、自动驾驶等。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：埃舍尔网络：一种用于可扩展视图合成的生成模型</li><li>作者：Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison</li><li>隶属单位：伦敦帝国理工学院戴森机器人实验室</li><li>关键词：视图合成、扩散模型、隐式神经表示、多视图几何</li><li>论文链接：https://arxiv.org/abs/2402.03908，Github 链接：None</li><li><p>摘要：(1)：研究背景：视图合成是计算机视觉和计算机图形学中的一项基本任务，它允许根据一组参考视点呈现场景的任意视点，从而模拟人类的视觉适应性。(2)：过去的方法：现有方法通常专注于单一任务，例如零样本新颖视图合成、单图像三维重建或多图像三维重建，并且在处理复杂场景时面临着泛化性差、灵活性不足和可扩展性有限等问题。(3)：研究方法：本文提出了一种多视图条件扩散模型——埃舍尔网络，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的 3 个参考视图到 3 个目标视图上训练的情况下，它也可以在单个消费级 GPU 上同时生成 100 多个一致的目标视图。(4)：方法性能：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p></li><li><p>方法：(1)：埃舍尔网络是一种多视图条件扩散模型，它学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。(2)：埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。(3)：埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。</p></li><li><p>结论：（1）：埃舍尔网络提出了一种多视图条件扩散模型，该模型在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。（2）：创新点：</p></li><li>提出了一种多视图条件扩散模型——埃舍尔网络，该模型学习隐式和生成的三维表示，并结合专门的相机位置编码，允许对任意数量的参考视图和目标视图之间的相机变换进行精确和连续的相对控制。</li><li>埃舍尔网络在视图合成方面具有出色的通用性、灵活性，以及可扩展性，即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。</li><li>埃舍尔网络在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。这种卓越的多功能性为设计用于三维视觉的可扩展神经架构开辟了新方向。性能：</li><li>在多个基准测试中实现了最先进的性能，即使与针对每个单独问题量身定制的方法相比也是如此。</li><li>即使在固定数量的3个参考视图到3个目标视图上训练的情况下，它也可以在单个消费级GPU上同时生成100多个一致的目标视图。工作量：</li><li>埃舍尔网络是一个复杂的神经网络模型，需要大量的数据和计算资源进行训练。</li><li>埃舍尔网络的训练过程可能需要几天或几周的时间，具体取决于数据集的大小和使用的硬件。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-cdd01255ccb3e0ac7a9532f4537d7c8a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7709d4f2ffb5392bba195cc2b965aeee.jpg" align="middle"><img src="https://picx.zhimg.com/v2-86704d39a54eee216395f69db00a0918.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73c26a4c69f3a172a8651cabc4a69ed2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9a53c48c54043613c01b125b54da3368.jpg" align="middle"><img src="https://picx.zhimg.com/v2-526811fa3a0b6e1b6d850e3911c0f54f.jpg" align="middle"></details><h2 id="QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning"><a href="#QuEST-Low-bit-Diffusion-Model-Quantization-via-Efficient-Selective-Finetuning" class="headerlink" title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning"></a>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective   Finetuning</h2><p><strong>Authors:Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</strong></p><p>Diffusion models have achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While quantization paves a way for diffusion model compression and acceleration, existing methods totally fail when the models are quantized to low-bits. In this paper, we unravel three properties in quantized diffusion models that compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit quantization difficulty stemming from the distribution imbalance, we propose finetuning the quantized model to better adapt to the activation distribution. Building on this idea, we identify two critical types of quantized layers: those holding vital temporal information and those sensitive to reduced bit-width, and finetune them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate quantization. Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion. </p><p><a href="http://arxiv.org/abs/2402.03666v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型量化后，如何提高准确率？</p><p><strong>Key Takeaways</strong></p><ul><li>低位量化扩散模型面临三大问题：激活分布不平衡、时间信息不精确、特定模块对扰动敏感。</li><li>提出微调量化模型，使其更好地适应激活分布。</li><li>识别出两种关键的量化层：保存重要时间信息的层和对比特宽度降低敏感的层，并对其进行微调以缓解性能退化。</li><li>经验验证表明，该方法修改了激活分布并提供了有意义的时间信息，促进了更容易、更准确的量化。</li><li>该方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在完全4位（即 W4A4）Stable Diffusion 上生成可读图像的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：QuEST：低比特扩散模型量化通过高效选择性微调</li><li>作者：Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan</li><li>第一作者单位：伊利诺伊理工学院计算机科学系</li><li>关键词：扩散模型、量化、低比特、微调</li><li>论文链接：https://arxiv.org/abs/2402.03666Github 链接：无</li><li><p>摘要：(1) 研究背景：扩散模型在图像生成任务中取得了显著的成功，但其实际部署受到高内存和时间消耗的限制。量化可以为扩散模型压缩和加速提供一种可行的方法，但现有方法在模型被量化为低比特时完全失败。(2) 过去方法及其问题：现有扩散模型量化方法要么集中在时间步长感知校准数据构造，要么集中在量化噪声校正，目标是将现有的量化技术调整到扩散模型的特性，而这些特性与其他模型类型（如 CNN 和 ViT）不同。这些方法忽略了与量化相关的扩散模型内在机制，导致方法部署与模型特征之间存在不一致。(3) 本文方法：本文揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化：（1）激活分布可能不平衡，其中大多数值接近 0，但其他值很大且不一致地出现；（2）时间信息不精确；（3）容易受到特定模块的扰动。为了减轻源于分布不平衡的低比特量化难度，本文提出微调量化模型以更好地适应激活分布。在此基础上，本文确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。(4) 实验结果：本文方法在三个高分辨率图像生成任务上进行了评估，并在各种比特宽度设置下实现了最先进的性能，并且是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。这些性能支持了本文的目标。</p></li><li><p>方法：（1）属性一：激活分布不平衡，大多数值接近 0，但其他值很大且不一致地出现。为了解决这个问题，我们提出微调量化模型以更好地适应激活分布。（2）属性二：时间信息不准确。为此，我们确定了两种关键类型的量化层：那些持有重要时间信息和那些对降低比特宽度敏感的层，并微调它们以有效地减轻性能下降。（3）属性三：不同激活对降低比特宽度的敏感性不同。我们提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。（4）QuEST：一种通过高效选择性微调实现低比特扩散模型量化的框架。QuEST 是一个基于蒸馏的微调策略，包括选择性权重优化和网络级缩放因子优化。</p></li><li><p>结论：（1）本文提出了 QuEST，一种用于低比特扩散模型量化的有效无数据微调框架。我们的方法的动机来自于在量化扩散模型中发现的三个基本属性。我们还从理论上证明了微调的充分性，将其解释为增强模型对大激活扰动的鲁棒性的一种方法。为了减轻性能下降，我们提出在全精度对应模型的监督下微调时间嵌入层和注意力相关层。还引入了一个时间感知激活量化器来处理不同的时间步长。在三个高分辨率图像生成任务上的实验结果证明了 QuEST 的有效性和效率，在更少的时间和内存成本下实现了低比特兼容性。（2）创新点：</p></li><li>揭示了量化扩散模型的三个属性，这些属性阻碍了有效的量化。</li><li>提出了一种基于蒸馏的微调策略 QuEST，包括选择性权重优化和网络级缩放因子优化。</li><li>提出了一种时间感知激活量化器，以进一步解决由于量化而导致的时间信息丢失的问题。性能：</li><li>在三个高分辨率图像生成任务上实现了最先进的性能。</li><li>是第一个在全 4 位（即 W4A4）Stable Diffusion 上生成可读图像的方法。工作量：</li><li>在 ImageNet-64x64 数据集上，QuEST 只需 10 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li><li>在 ImageNet-256x256 数据集上，QuEST 只需 40 个 GPU 天即可将 Stable Diffusion 量化为 4 位。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fb8f38fcd6a6857ddffdf84e6eded575.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b8c7e8e687f519bd6aea6d7aa431f440.jpg" align="middle"><img src="https://picx.zhimg.com/v2-808d7c694b655862c89add4bffc7e8b1.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e6556a45821b662485e3c321d4542f94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d15f14313fc02ce9abba40125462e990.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e9f9c978c41171320dbafc60fb23b8e.jpg" align="middle"></details><h2 id="InstanceDiffusion-Instance-level-Control-for-Image-Generation"><a href="#InstanceDiffusion-Instance-level-Control-for-Image-Generation" class="headerlink" title="InstanceDiffusion: Instance-level Control for Image Generation"></a>InstanceDiffusion: Instance-level Control for Image Generation</h2><p><strong>Authors:Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</strong></p><p>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs. </p><p><a href="http://arxiv.org/abs/2402.03290v1">PDF</a> Preprint; Project page:   <a href="https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/">https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/</a></p><p><strong>Summary</strong><br>文本到图像扩散模型实现了高质量图像生成，但无法控制图像中的单独实例。我们引入了InstanceDiffusion，它为文本到图像扩散模型添加了精确的实例级控制。</p><p><strong>Key Takeaways</strong></p><ul><li>InstanceDiffusion支持每个实例的自由形式语言条件。</li><li>InstanceDiffusion支持灵活方式指定实例位置，如简单单点、涂鸦、边界框或复杂的实例分割掩码及其组合。</li><li>InstanceDiffusion提出了三个主要更改，以实现精确的实例级控制。</li><li>UniFusion模块为文本到图像模型启用了实例级条件。</li><li>ScaleU模块提高了图像保真度。</li><li>Multi-instance Sampler改进了多个实例的生成。</li><li>InstanceDiffusion在每个位置条件下都显着超过了专门的最新模型。</li><li>在COCO数据集上，InstanceDiffusion在框输入时优于之前的最新技术20.4% AP50box，在掩码输入时优于之前的最新技术25.4% IoU。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：实例扩散：图像生成的实例级控制</li><li>作者：Jun-Yan Zhu, Taesung Park, Abhishek Sharma, Prafulla Dhariwal, Alexei A. Efros, Pieter Abbeel</li><li>隶属机构：加州大学伯克利分校</li><li>关键词：文本到图像生成、实例级控制、扩散模型</li><li>论文链接：https://arxiv.org/abs/2212.04915，Github 代码链接：无</li><li><p>摘要：(1) 研究背景：文本到图像扩散模型可以生成高质量的图像，但无法对图像中的各个实例进行控制。(2) 过去的方法：现有方法主要集中在对整个图像进行控制，而无法对各个实例进行精细的控制。这些方法的问题在于，它们无法处理复杂的实例条件，例如，当实例重叠或被遮挡时，它们无法生成高质量的图像。(3) 论文提出的方法：本文提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。InstanceDiffusion 主要包含三个部分：UniFusion 模块、ScaleU 模块和 Multi-instance Sampler。UniFusion 模块可以将实例条件融合到文本嵌入中，ScaleU 模块可以提高图像的保真度，Multi-instance Sampler 可以改善多实例生成的质量。(4) 实验结果：InstanceDiffusion 在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上，InstanceDiffusion 比之前的最先进模型高出 20.4%，在 IoU 指标上，InstanceDiffusion 比之前的最先进模型高出 25.4%。这些结果表明，InstanceDiffusion 能够有效地对图像中的各个实例进行控制，并生成高质量的图像。</p></li><li><p>Methods：(1) UniFusion：UniFusion是InstanceDiffusion模型中的一个关键模块，它可以将模糊的语义信息融合到图像嵌入中。UniFusion由两个子模块组成：语义信息提取模块和信息融合模块。语义信息提取模块负责从语义信息中提取特征，信息融合模块负责将这些特征融合到图像嵌入中。(2) ScaleU：ScaleU是InstanceDiffusion模型中的另一个关键模块，它可以提高图像的保真度。ScaleU由两个子模块组成：上采样模块和残差模块。上采样模块负责将图像从低分辨率上采样到高分辨率，残差模块负责添加残差连接，以提高图像的保真度。(3) Multi-instanceSampler：Multi-instanceSampler是InstanceDiffusion模型中的一个采样模块，它可以改善多实例生成的质量。Multi-instanceSampler通过对每个实例进行多次采样，然后将这些采样结果进行融合，以生成最终的图像。</p></li><li><p>结论：（1）：InstanceDiffusion 模型在文本到图像生成任务中取得了最先进的性能，在 COCO 数据集上的 APbox50 指标和 IoU 指标上均优于之前的最先进模型。这表明 InstanceDiffusion 模型能够有效地对图像中的各个实例进行控制，并生成高质量的图像。（2）：创新点：</p></li><li>提出了一种新的文本到图像扩散模型 InstanceDiffusion，该模型可以对图像中的各个实例进行精细的控制。</li><li>设计了 UniFusion 模块，可以将模糊的语义信息融合到图像嵌入中。</li><li>设计了 ScaleU 模块，可以提高图像的保真度。</li><li>设计了 Multi-instanceSampler 模块，可以改善多实例生成的质量。性能：</li><li>在 COCO 数据集上取得了最先进的性能，在 APbox50 指标上比之前的最先进模型高出 20.4%，在 IoU 指标上比之前的最先进模型高出 25.4%。工作量：</li><li>模型的训练和推理过程相对复杂，需要较大的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-ad80374506fc08e660bb8742f25dc5ea.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eb1dc22d5f1b16516125f58ffce2ab07.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cc40befe0322c7f0f22fe9b42e02d05a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0667dfce57b3d47e56cf440eb22a837d.jpg" align="middle"></details><h2 id="Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images"><a href="#Organic-or-Diffused-Can-We-Distinguish-Human-Art-from-AI-generated-Images" class="headerlink" title="Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?"></a>Organic or Diffused: Can We Distinguish Human Art from AI-generated   Images?</h2><p><strong>Authors:Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</strong></p><p>The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse.   There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today’s modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness. </p><p><a href="http://arxiv.org/abs/2402.03214v2">PDF</a> </p><p><strong>Summary</strong><br>人工智能图像生成技术引发艺术领域巨变，区分人工智能生成图像与人类艺术品是一项不断加剧的难题。</p><p><strong>Key Takeaways</strong></p><ul><li>AI生成图像对艺术世界的颠覆性影响与日俱增。</li><li>鉴别AI生成的图像对于防止欺诈、版权保护和模型训练至关重要。</li><li>目前有几种方法可以区分人类艺术与AI图像，包括监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的了解进行识别。</li><li>研究表明，Hive和专家艺术家在区分AI生成的图像方面表现出色，但各有优劣（Hive对对抗性扰动较弱，而专家艺术家产生较高误报率）。</li><li>随着模型的不断发展，这些弱点可能仍然存在，研究数据表明，由人类和自动检测器组成的组合团队可以提供最佳的准确性和鲁棒性。</li><li>人工生成的图像在艺术领域引发了一场颠覆，准确区分人工智能生成的图像对于防止欺诈和保护版权至关重要。</li><li>尽管有不同的方法可以区分人类艺术与AI图像，但没有一种方法是完美的。</li><li>将人类和自动检测器结合起来可以提供最佳的准确性和鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：有机还是扩散：我们能区分人类艺术和人工智能生成图像吗？</li><li>作者：Anna Yoo Jeong Ha、Josephine Passananti、Ronik Bhaskar、Shawn Shan、Reid Southen1、Haitao Zheng、Ben Y. Zhao</li><li>第一作者单位：芝加哥大学计算机科学系</li><li>关键词：人工智能艺术、图像生成、鉴别器、人类艺术家</li><li>论文链接：https://arxiv.org/abs/2402.03214，Github 代码链接：无</li><li>摘要：(1)：随着人工智能生成图像的出现，艺术领域发生了巨大变革。区分人工智能生成图像和人类艺术是一个具有挑战性的问题，其影响随着时间的推移而不断扩大。如果不解决这个问题，就会让不法分子欺骗那些为人类艺术支付高价的个人和禁止使用人工智能图像的公司。这对内容所有者建立版权和对模型训练者来说也是至关重要的，他们需要对训练数据进行整理以避免潜在的模型崩溃。(2)：目前，有几种不同的方法可以区分人类艺术和人工智能图像，包括通过监督学习训练的分类器、针对扩散模型的研究工具以及专业艺术家利用其对艺术技巧的知识进行识别。(3)：在本文中，我们寻求了解这些方法在面对当今现代生成模型时，在良性和对抗性环境中的表现如何。我们整理了跨越 7 种风格的真实人类艺术，从 5 个生成模型中生成了匹配的图像，并应用了 8 个检测器（5 个自动检测器和 3 个不同的人类群体，包括 180 名众包工人、4000 多名专业艺术家和 13 名在检测人工智能方面经验丰富的专家艺术家）。(4)：Hive 和专家艺术家都表现得非常好，但在不同的方面犯了错误（Hive 在对抗性扰动中较弱，而专家艺术家产生较高的误报）。我们认为随着模型的不断发展，这些弱点将继续存在，并利用我们的数据证明为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li></ol><p>方法：</p><p>（1）构建数据集：- 收集真人艺术作品、AI 生成的图像、扰动版本的人类艺术作品和 AI 图像以及结合人类和 AI 努力创建的非典型图像。- 从 53 位艺术家处收集 280 幅真人艺术作品，涵盖 7 种主要艺术风格。- 为 7 种艺术风格中的每一种，使用 5 个流行的 AI 生成器生成 10 张图像，共生成 350 张 AI 生成的图像。- 调整 BLIP 生成的标题以包括艺术作品的风格，并根据每个 AI 生成器的独特限制和配置对标题进行自定义调整。</p><p>（2）评估自动检测器：- 考虑已部署的商业系统和基于研究的系统。- 评估自动检测器在核心测试数据集上的性能，该数据集包含 280 幅真人艺术作品、350 幅 AI 图像和 40 幅混合图像。- 测试自动检测器针对各种对抗性扰动，包括高斯噪声、JPEG 压缩、对抗性扰动和 Glaze 风格模拟保护工具。</p><p>（3）评估人类检测：用户研究：- 进行单独的用户研究，针对 3 个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。- 基本参与者：通过 Prolific 在线众包平台招募 180 名参与者，完成一致性检查后有 177 人参与。- 专业艺术家志愿者：通过社交媒体招募超过 4000 名专业艺术家志愿者，3803 人完成调查并通过所有一致性检查。- 专家参与者：招募 13 位知名专业艺术家，他们具有识别 AI 图像的经验。- 专家团队提供对产生最多错误分类的最困难图像的详细反馈。</p><p>（4）数据收集：- 策划包含真人创作的艺术作品、AI 生成的图像和混合图像的数据集。- 定义真人图像为由人类艺术家原创的艺术作品。- AI 生成的图像使用 AI 模型（如 Midjourney、Stable Diffusion 和 DALL-E3）从文本提示生成。- 混合图像由 AI 生成、润色并部分由人类绘制。- 从社交媒体网站和艺术平台收集真人艺术作品。- 与艺术家社区合作，收集跨越 7 种主要艺术风格的艺术作品。- 使用 BLIP 模型为 AI 生成器创建提示，以生成有效捕捉艺术作品风格和内容的标题。- 根据每个 AI 生成器的独特限制和配置，对 BLIP 生成的标题进行自定义调整。</p><ol><li>结论：（1）随着人工智能生成图像的出现，区分人工智能生成图像和人类艺术是一个具有挑战性的问题。本文研究了目前几种不同的方法在面对当今现代生成模型时，在良性和对抗性环境中的表现，并证明了人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（2）创新点：</li><li>构建了一个跨越7种风格的真实人类艺术、人工智能生成图像和混合图像的数据集。</li><li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li><li>发现人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。</li><li>专家艺术家在对抗性扰动中表现较弱，而自动检测器产生较高的误报。</li><li>随着模型的不断发展，这些弱点将继续存在，人类和自动检测器的组合团队将发挥重要作用。</li><li>分析了Hive和专家艺术家在不同方面的错误，并证明了为什么人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（3）性能：</li><li>在核心测试数据集上，Hive和专家艺术家都表现得非常好，但Hive在对抗性扰动中表现较弱，而专家艺术家产生较高的误报。</li><li>Hive在对抗性扰动中较弱，而专家艺术家产生较高的误报。</li><li>人类和自动检测器的组合团队提供了准确性和鲁棒性的最佳组合。（4）工作量：</li><li>收集了跨越7种风格的280幅真实人类艺术作品和350幅人工智能生成图像。</li><li>评估了8个检测器（5个自动检测器和3个不同的人类群体）在核心测试数据集和各种对抗性扰动上的性能。</li><li>进行单独的用户研究，针对3个独立的用户群体：基本参与者、专业艺术家志愿者和专家参与者。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8cba12717aa69817e10b925c47c7e5f1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-654bd7a18967bfc99c8234931b745b7f.jpg" align="middle"></details><h2 id="PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model"><a href="#PFDM-Parser-Free-Virtual-Try-on-via-Diffusion-Model" class="headerlink" title="PFDM: Parser-Free Virtual Try-on via Diffusion Model"></a>PFDM: Parser-Free Virtual Try-on via Diffusion Model</h2><p><strong>Authors:Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</strong></p><p>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can “wear” garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models. </p><p><a href="http://arxiv.org/abs/2402.03047v1">PDF</a> Accepted by IEEE ICASSP 2024</p><p><strong>Summary</strong><br>无解析器虚拟试穿方法基于扩散模型，无需精准分割掩码，即可实现逼真试穿效果。</p><p><strong>Key Takeaways</strong></p><ul><li>PFDM是一种无解析器的虚拟试穿方法，可以无缝地“穿上”目标人物的衣服，而无需任何其他信息。</li><li>PFDM使用扩散模型来学习无解析器的虚拟试穿，可以有效地捕捉人体的结构和衣服的细节。</li><li>PFDM通过合成大量伪图像并构造样本对来学习，其中伪图像包含了各种穿着不同衣服的人。</li><li>PFDM使用提出的服装融合注意（GFA）机制来融合人物和衣服的特征，从而生成逼真的试穿图像。</li><li>PFDM可以处理复杂的情况，合成高保真图像，并且优于现有基于解析器和无解析器的虚拟试穿模型。</li><li>PFDM可以用于在线和店内购物场景，显著改善服装购物体验。</li><li>PFDM有望在虚拟现实和增强现实等领域得到广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：PFDM：基于扩散模型的无解析虚拟试穿</li><li>作者：牛云芳，易东，吴令祥，刘智伟，蔡鹏翔，王金桥</li><li>隶属单位：中国科学院自动化研究所，模式识别国家重点实验室，基础模型研究中心，北京，中国</li><li>关键词：虚拟试穿，扩散模型，隐式扭曲，高分辨率图像合成</li><li>论文链接：https://arxiv.org/abs/2402.03047</li><li><p>摘要：（1）：虚拟试穿可以显著改善在线和店内场景中的服装购物体验，在计算机视觉领域引起了广泛关注。然而，为了实现高保真试穿性能，大多数最先进的方法仍然依赖于准确的分割掩码，这些掩码通常由近乎完美的解析器或手动标注产生。为了克服这一瓶颈，我们提出了一种基于扩散模型的无解析虚拟试穿方法（PFDM）。给定两张图像，PFDM 可以通过隐式扭曲将服装无缝地“穿”在目标人物身上，而无需任何其他信息。为了有效地学习模型，我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。实验表明，我们提出的 PFDM 可以成功处理复杂情况，合成高保真图像，并且优于最先进的无解析和基于解析的模型。（2）：GAN 用于虚拟试穿。基于 GAN 的虚拟试穿方法通常采用两步架构，首先将服装扭曲成目标形状，然后通过组合扭曲的服装和人物图像来合成结果。一些工作专注于基于薄板样条变换 (TPS) 或全局流增强扭曲模块。其他工作旨在提高生成模块的性能，例如，采用对齐感知生成器来提高合成图像的分辨率，或改进损失函数以保留人物身份。（3）：本文提出的研究方法。我们提出了一种基于扩散模型的无解析虚拟试穿框架。这是第一个将扩散模型用于无解析虚拟试穿的工作。我们还精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。（4）：方法在任务和性能上取得的成就。我们在 VITON-HD 上评估了我们的方法，实验表明，我们的无解析模型在定性和定量评估中都优于竞争对手。这些性能支持了我们的目标。</p></li><li><p>方法：(1)：我们提出了一种基于扩散模型的无解析虚拟试穿框架，该框架可以将服装无缝地“穿”在目标人物身上，而无需任何其他信息。(2)：我们精心设计了一个增强的交叉注意模块来融合人物和服装特征以进行隐式扭曲。(3)：我们合成了许多伪图像，并通过在人物身上穿戴各种服装来构建样本对。(4)：在由大规模扩展数据集监督下，我们使用提出的服装融合注意（GFA）机制融合人物和服装特征。</p></li><li><p>结论：（1）：本文提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。据我们所知，PFDM 是第一个基于扩散的无解析虚拟试穿模型。实验表明，PFD 可以生成具有丰富纹理细节的高分辨率高保真试穿结果，并成功处理错位和遮挡，这不仅优于现有的无解析方法，而且在定性和定量分析中也超越了最先进的基于解析器的模型。我们希望我们的工作能够促进虚拟试穿技术在电子商务和元宇宙中的普及。（2）：创新点：提出了一种基于扩散模型的无解析虚拟试穿方法，该方法将扭曲和融合步骤统一到一个模型中，同时避免了使用任何解析器或外部模块。性能：在定性和定量分析中，PFDM 优于现有的无解析方法和最先进的基于解析器的模型。工作量：该方法需要合成大量伪图像并构建样本对，这可能需要大量计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4fa2bcca39e4d002618ff0b3dcd93311.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d530a087c0dd3abddf2412c841493d90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4af8acc18772befb8884db138ea6e422.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b3f34614b487167c039e9989a45cc12d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5123c9bbcb697725b9020c9d4ab0422.jpg" align="middle"></details><h2 id="Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models"><a href="#Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models" class="headerlink" title="Extreme Two-View Geometry From Object Poses with Diffusion Models"></a>Extreme Two-View Geometry From Object Poses with Diffusion Models</h2><p><strong>Authors:Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu</strong></p><p>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a>. </p><p><a href="http://arxiv.org/abs/2402.02800v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型合成新视图图像进行对象姿态估计，有效求解极端视角变化下的两视图几何问题。</p><p><strong>Key Takeaways</strong></p><ul><li>利用对象先验通过扩散模型Zero123合成新视图图像，增强了对象姿态估计的鲁棒性和适应性。</li><li>将相对相机姿态估计问题数学转换为对象姿态估计问题，简化了问题的求解。</li><li>在极端视角变化的情况下，合成的新视图图像经匹配可以确定对象姿态，从而确定两视图相机姿态。</li><li>该方法在合成和真实世界数据集上均表现出非凡的鲁棒性和弹性，估计两视图姿态具有杰出的泛化能力。</li><li>可在 <a href="https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models">https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</a> 获取代码。</li><li>该方法精度高，在合成和真实世界数据集上均表现良好。</li><li>该方法适用于解决极端视角变化下的两视图几何问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：从物体位姿估计极端两视图几何</li><p></p><p></p><li>作者：Yujing Sun、Caiyi Sun、Yuan Liu、Yuexin Ma、Siu Ming Yiu</li><p></p><p></p><li>隶属机构：香港大学</li><p></p><p></p><li>关键词：两视图几何、物体位姿估计、扩散模型、生成模型</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2402.02800    Github 代码链接：https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models</li><p></p><p></p><li>摘要：(1) 研究背景：人类具有惊人的能力，能够毫不费力地感知包含相同物体的两幅图像之间的视点差异，即使视点变化非常大，图像中没有共同可见的区域。然而，对于现有的相机位姿估计方法来说，这种非凡的能力被证明是一个挑战，因为这些方法在面对大的视点差异时通常会失败，原因是缺少用于匹配的重叠局部特征。(2) 过去的方法及其问题：过去的方法主要集中在使用局部特征匹配来估计两视图几何。然而，当视点差异较大时，这种方法往往会失败，因为没有足够的重叠局部特征可供匹配。(3) 本文提出的研究方法：在本文中，我们提出了一种新方法来估计极端两视图几何。我们的方法首先将相对相机位姿估计问题转换为物体位姿估计问题。然后，为了估计物体位姿，我们利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像。将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。(4) 方法在任务和性能上的表现：在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li><br>&lt;/ol&gt;<p></p><p>Methods:(1): 提出了一种新方法来估计极端两视图几何，该方法将相对相机位姿估计问题转换为物体位姿估计问题；(2): 利用从扩散模型Zero123学习到的物体先验来合成物体的novel-view图像，将novel-view图像匹配以确定物体位姿；(3): 通过实验验证了该方法的鲁棒性和泛化能力。</p><ol><li>结论：（1）：本文提出了一种新颖的算法，可以估计具有极端视点变化的相对相机位姿。该方法的核心思想是利用从大规模 2D 扩散模型 Zero123 学习到的物体先验，该先验能够生成对象的 novel-view 图像。但是，由于 Zero123 在其模型中隐式定义了规范坐标系，并且图像可能不会看向对象，因此我们无法直接应用 Zero123。为了解决这一挑战，我们首先提出了一种新的两视图位姿估计公式，作为物体位姿估计问题，并正确定义输入图像和生成图像的物体位姿。最后，我们匹配另一幅图像。（2）：创新点：</li><li>将相对相机位姿估计问题转换为物体位姿估计问题，并利用从扩散模型 Zero123 学习到的物体先验来合成物体的 novel-view 图像，将 novel-view 图像匹配以确定物体位姿，从而确定两视图相机位姿。</li><li>在实验中，我们的方法表现出非凡的鲁棒性和对大视点变化的适应性，能够一致地估计两视图位姿，并且在合成和真实世界数据集上都具有出色的泛化能力。</li><li>这些性能支持了我们的目标，即准确地确定极端视点变化下的两视图几何。</li></ol><p>性能：- 在合成数据集上，我们的方法在所有视点变化范围内都优于最先进的方法，并且在极端视点变化下具有显着的优势。- 在真实世界数据集上，我们的方法也优于最先进的方法，并且在极端视点变化下具有显着的优势。</p><p>工作量：- 该方法需要训练一个扩散模型来学习物体先验，这可能需要大量的数据和计算资源。- 该方法还需要合成 novel-view 图像，这可能需要大量的时间和计算资源。- 该方法还需要匹配 novel-view 图像，这可能需要大量的时间和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-25907674667aa8b32d056fad9f68800a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-36f6cbd8fb9421b0eea500253c925684.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4d37974797f92e16872fe7a27774fa5a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b283ab7c3ead68d1a7cd2cceb1c42365.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a5815c280ae8797af92483f51007a87d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a313bd5672a496366b53aa94dffc26ae.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-09  Source-Free Domain Adaptation with Diffusion-Guided Source Data   Generation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/NeRF/</id>
    <published>2024-02-02T14:27:07.000Z</published>
    <updated>2024-02-02T14:27:07.050Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>Summary</strong><br>文本引入了一种新的方法 ViCA-NeRF，该方法可以利用文本编辑进行 3D 编辑，并使用几何和学习正则化来确保编辑的多视图一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>ViCA-NeRF 是一种新颖的基于文本的 3D 编辑方法，利用 NeRF 进行隐式神经辐射场建模。</li><li>ViCA-NeRF 的关键思想是利用两种正则化来源，明确地在不同视图之间传播编辑信息，确保多视图一致性。</li><li>ViCA-NeRF 利用从 NeRF 推导出的深度信息来建立不同视图之间的图像对应关系，用于几何正则化。</li><li>ViCA-NeRF 对经过编辑和未经过编辑的图像在 2D 扩散模型中的潜在编码进行对齐，实现编辑关键视图并更新整个场景。</li><li>ViCA-NeRF 采用两个阶段的工作流程，第一阶段将来自不同视图的编辑融合，创建初步的 3D 编辑。</li><li>第二阶段进行 NeRF 训练，进一步优化场景的外观。</li><li>与现有技术相比，ViCA-NeRF 提供更灵活、更高效（速度提升 3 倍）、更一致且更详细的编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ViCA-NeRF：基于视图一致性的神经辐射场三维编辑</li><li>作者：Jiahua Dong, Yu-Xiong Wang</li><li>单位：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：三维编辑、神经辐射场、视图一致性、文本指令</li><li>论文链接：https://arxiv.org/abs/2402.00864Github 链接：https://dongjiahua.github.io/VICA-NeRF</li><li><p>摘要：(1)：随着神经辐射场（NeRF）及其变体的最新进展，收集真实世界三维场景数据变得更加便捷。然而，现有的三维编辑方法通常缺乏视图一致性，导致编辑结果在不同视角下可能出现不一致的情况。(2)：过去的方法主要包括基于几何的正则化和基于学习的正则化。几何正则化利用 NeRF 提取的深度信息来建立不同视角之间的图像对应关系，从而确保视图一致性。学习正则化则通过对编辑图像和未编辑图像的潜在代码进行对齐，使编辑信息能够在整个场景中传播。(3)：本文提出的 ViCA-NeRF 是一种基于视图一致性的三维编辑方法，它结合了几何正则化和学习正则化两种策略。ViCA-NeRF 首先通过融合来自不同视角的编辑结果来创建初步的三维编辑，然后通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。(4)：实验结果表明，与现有方法相比，ViCA-NeRF 提供了更加灵活、高效（速度提高 3 倍）的编辑方式，并且具有更高的视图一致性和细节水平。</p></li><li><p>方法：(1) ViCA-NeRF 首先从不同视角收集输入图像，并使用 NeRF 从这些图像中提取深度信息。(2) 然后，ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。(3) 最后，ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。</p></li><li><p>结论：（1）：本文提出了 ViCA-NeRF，一种基于视图一致性的三维编辑框架，用于文本引导的 NeRF 编辑。给定文本指令，我们可以高效地编辑 NeRF。除了像人类风格化和天气变化这样的简单任务外，我们还支持上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高三维编辑的可控性和真实性。（2）：创新点：ViCA-NeRF 结合了几何正则化和学习正则化两种策略，以确保视图一致性和细节丰富。ViCA-NeRF 利用提取的深度信息来建立不同视角之间的图像对应关系，并使用这些对应关系来融合来自不同视角的编辑结果，从而创建初步的三维编辑。ViCA-NeRF 通过 NeRF 训练进一步细化场景的外观，从而确保视图一致性和细节丰富。性能：ViCA-NeRF 在各种场景和文本提示上优于几个基线。ViCA-NeRF 的速度提高了 3 倍。工作量：ViCA-NeRF 的实现相对简单。ViCA-NeRF 的训练和推理速度较快。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/3DGS/</id>
    <published>2024-02-02T14:24:12.000Z</published>
    <updated>2024-02-02T14:24:12.861Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming"><a href="#360-GS-Layout-guided-Panoramic-Gaussian-Splatting-For-Indoor-Roaming" class="headerlink" title="360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming"></a>360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</h2><p><strong>Authors:Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</strong></p><p>3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios. </p><p><a href="http://arxiv.org/abs/2402.00763v1">PDF</a> 11 pages, 10 figures</p><p><strong>Summary</strong><br>360-GS 以平面投影为基础，利用布局先验来指导优化过程，从而产生可用于渲染全景和生成新视角图像的 3D 椭圆高斯分布。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯斑点 (3D-GS) 是一种流行的技术，它通常将透视图像作为输入，并优化一组 3D 椭圆高斯分布，将它们喷射到图像平面上，从而产生 2D 高斯分布。</li><li>然而，将 3D-GS 应用于全景输入时，使用 2D 高斯分布对 ${360^\circ}$ 图像的球形表面上的投影进行建模存在挑战。</li><li>在实际应用中，输入全景通常很稀疏，导致 3D 高斯分布的初始化不可靠，随后 3D-GS 质量下降。</li><li>此外，由于纹理平面（例如墙壁和地板）的几何形状受限，3D-GS 难以使用椭圆高斯分布对这些平坦区域进行建模，从而导致新视图中出现明显的漂浮物。</li><li>为了解决这些问题，我们提出了 360-GS，这是一种针对有限数量的全景输入的新型 $360^{\circ}$ 高斯斑点。</li><li>360-GS 不将 3D 高斯分布直接喷射到球形表面上，而是将其投影到单位球的切平面，然后将它们映射到球形投影。这种改编能够使用高斯分布表示投影。</li><li>我们通过利用全景中的布局先验来指导 360-GS 的优化，这些先验很容易获得，并且包含有关室内场景的强大结构信息。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：360-GS：布局引导的室内全景高斯渲染</li><li>作者：Jiayang Bai, Letian Huang, Jie Guo, Wen Gong, Yuanqi Li, Yanwen Guo</li><li>隶属：南京大学</li><li>关键词：3D高斯渲染、全景图像、室内场景、布局引导</li><li>论文链接：https://arxiv.org/abs/2402.00763   Github 链接：无</li><li><p>摘要：   (1)：研究背景：3D高斯渲染（3D-GS）因其实时性和照片级渲染效果而备受关注。该技术通常以透视图像作为输入，通过将一组 3D 椭圆高斯体渲染到图像平面上，从而生成 2D 高斯体。然而，将 3D-GS 应用于全景输入时，使用 2D 高斯体有效建模 360° 图像的球面投影存在挑战。在实际应用中，输入全景图像通常是稀疏的，导致 3D 高斯体的初始化不可靠，进而降低 3D-GS 的质量。此外，由于缺乏纹理的平面（例如墙壁和地板）的几何约束不足，3D-GS 难以使用椭圆高斯体对这些平面区域进行建模，从而导致在新的视角中出现明显的浮动物体。   (2)：过去的方法及其问题：为了解决这些问题，本文提出了一种针对有限全景输入的新型 360° 高斯渲染方法 360-GS。与直接将 3D 高斯体渲染到球面上不同，360-GS 将其投影到单位球体的切平面，然后将其映射到球面投影。这种改进使得使用高斯体表示投影成为可能。我们通过利用全景图像中的布局先验来指导 360-GS 的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。   (3)：本文的研究方法：我们的实验结果表明，360-GS 能够从有限数量的全景输入中生成高质量的全景渲染。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。   (4)：方法的性能及其对目标的支持：360-GS 在室内场景渲染任务上取得了优异的性能。与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。这些结果表明，360-GS 能够有效地利用布局先验来指导 3D 高斯体的优化，从而生成高质量的全景渲染。</p></li><li><p>方法：(1)：360◦高斯体镶嵌：提出了一种新颖的 splatting 技术，将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。(2)：布局引导初始化和正则化：利用全景图像中的布局先验来指导 3D 高斯体的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。(3)：全景渲染：通过将 splattered 的高斯体从前到后进行 alpha 混合，可以生成全景渲染。</p></li><li><p>结论：（1）：本文提出了一种新颖的布局引导全景高斯渲染流水线，名为360-GS，它支持直接全景渲染，并且对稀疏输入具有鲁棒性。360-GS的基石是我们的360◦高斯 splatting 算法以及房间布局先验的结合。360◦高斯 splatting 算法通过利用透视投影和映射来解决在球面表面建模投影的挑战，从而实现对具有等距矩形图像的 3D 高斯的直接优化。我们在 3D 高斯的初始化过程中利用全景图中的房间布局先验，提供了一种更易于访问且鲁棒的替代方案来替代 SfM 点云。我们还引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。360-GS 支持实时漫游，并在真实世界场景中为新颖视角合成提供了最先进的性能。（2）：创新点：</p></li><li>提出了一种新颖的 360◦高斯 splatting 算法，该算法将 splatting 分解为两个步骤：在单位球体的切平面上 splatting 和映射到球面。</li><li>利用全景图像中的布局先验来指导 3D 高斯的优化，这些先验易于获取，并且包含有关室内场景的强结构信息。</li><li>引入了布局引导正则化来减轻浮动问题并保留房间布局的几何结构。性能：</li><li>与 3D-GS 相比，360-GS 在准确性、细节和鲁棒性方面均表现出优势。</li><li>360-GS 在室内场景渲染任务上取得了优异的性能。工作量：</li><li>需要收集和预处理全景图像。</li><li>需要优化 3D 高斯的参数。</li><li>需要将 splattered 的高斯体从前到后进行 alpha 混合以生成全景渲染。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-38c0a2fd61f19043e9f57d34dec4a1c6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9fe5198d06678b334414f192b0c83aa8.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e4e5570dfa99dfac9b297f7650c717c3.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f5349fc8a22abb33ba9a2c7388b0a826.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1d8e3eade9a3d6331e76dbab98e15a68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ffe9d7162c03cd614dfd0b6e7509adbd.jpg" align="middle"></details><h2 id="CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion"><a href="#CoSSegGaussians-Compact-and-Swift-Scene-Segmenting-3D-Gaussians-with-Dual-Feature-Fusion" class="headerlink" title="CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion"></a>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with   Dual Feature Fusion</h2><p><strong>Authors:Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan</strong></p><p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians’ segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at <a href="https://David-Dou.github.io/CoSSegGaussians">https://David-Dou.github.io/CoSSegGaussians</a> </p><p><a href="http://arxiv.org/abs/2401.05925v3">PDF</a> 9 pages, 8 figures, correct writing details</p><p><strong>摘要</strong><br>结合点云与显式反投射的特征融合网络，实现紧凑而快速的 3D 高斯混合分割。</p><p><strong>关键要点</strong></p><ul><li>提出一种用于紧凑、快速且仅以RGB图像作为输入的3D场景一致性分割方法：紧凑快速分割3D高斯（CoSSegGaussians）。</li><li>现有的基于高斯体素的分割方法在进行零镜头分割时难以生成紧凑的掩模，这可能是因为它们将可学习的参数直接分配给每个高斯体素，从而导致缺乏对跨视图不一致的2D机器生成的标签的鲁棒性。</li><li>利用双特征融合网络作为高斯体素的分割字段来解决上述问题。</li><li>首先在RGB监督下优化3D高斯体素。</li><li>然后通过显式反投影应用从图像中提取的DINO特征，并结合来自有效点云处理网络的空间特征。</li><li>利用特征聚合在全局到局部的策略中融合这些特征以实现紧凑的分割特征。</li><li>实验结果表明，与NeRF为基础的方法相比，该模型在语义分割和全景零镜头分割任务上都优于基线，同时推理时间少于10%。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：紧凑而快速的场景分割 3D 高斯体与双重特征融合</li><li>作者：Dou Bin, Zhang Tianyu, Ma Yongjia, Wang Zhaohui, Yuan Zejian</li><li>单位：西安交通大学人工智能与机器人学院</li><li>关键词：3D 场景分割、神经辐射场、高斯体、双重特征融合</li><li>论文链接：https://arxiv.org/abs/2401.05925，Github 代码链接：None</li><li><p>摘要：（1）研究背景：近年来，计算机视觉和计算机图形学取得了显着进展，特别是在神经渲染领域。神经辐射场 (NeRF) 及其后续方法推动了神经场景表示的发展，在新型视图合成方面显示出显着的性能。（2）过去的方法及其问题：基于 NeRF 的分割方法依赖于耗时的神经场景优化。虽然最近的 3D 高斯体 splatting 显着提高了速度，但现有的基于高斯体的分割方法难以产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于其直接将可学习参数分配给每个高斯体，导致对跨视图不一致的 2D 机器生成的标签缺乏鲁棒性。（3）本文方法：本文提出了一种紧凑而快速的场景分割方法，称为 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。具体来说，我们首先在 RGB 监督下优化 3D 高斯体。在高斯体定位之后，通过显式反投影应用从图像中提取的 DINO 特征，然后将其与来自高效点云处理网络的空间特征结合。利用特征聚合在全局到局部策略中融合它们以获得紧凑的分割特征。（4）方法性能：实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。</p></li><li><p>方法：（1）高斯体定位阶段：使用 L1 和 L_D-SSIM 光度损失来监督高斯体的几何信息，包括质心、协方差、不透明度和颜色。（2）分割阶段：将多尺度的 DINO 特征反投影到高斯体上，并与从高斯体中提取的空间特征融合。（3）特征聚合：使用全局到局部策略聚合融合后的特征，以生成紧凑的分割特征。（4）监督：使用零样本分割掩模和关联掩模来监督分割参数，并使用 NCE 损失进行优化。</p></li><li><p>结论：（1）：本文提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。实验结果表明，我们的模型在语义和全景零样本分割任务上都优于基线方法，同时推理时间不到基于 NeRF 的方法的 10%。（2）：创新点：</p></li><li>提出了一种紧凑而快速的场景分割方法 CoSSegGaussians，该方法仅使用 RGB 图像输入即可实现紧凑的 3D 一致场景分割，且渲染速度快。</li><li>提出了一种双重特征融合网络作为分割场，该网络聚合了 DINO 和空间特征用于分割。</li><li>将多尺度的 DINO 特征从图像反投影到定位的 3D 高斯体上，并进一步与高斯体的空间信息相结合。</li><li>应用全局到局部聚合模块生成紧凑的分割逻辑。性能：</li><li>在语义和全景零样本分割任务上都优于基线方法。</li><li>推理时间不到基于 NeRF 的方法的 10%。工作量：</li><li>使用了大量的数据集进行训练和测试。</li><li>算法的实现和训练过程较为复杂。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ecce62ef2d2a0a0c5d6577de6d7cb33f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-222c4f05c24f306aefd909de021e726c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6dff94133ac5b0802b5de3fb9550eff1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e96a03193e246ab9e77a3dd6aa18e239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f381d5614322d380f003e54e659eb10.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb6b0eeec85fc1d0f2cd12928b40918f.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-02  360-GS Layout-guided Panoramic Gaussian Splatting For Indoor Roaming</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/02/02/Paper/2024-02-02/Diffusion%20Models/</id>
    <published>2024-02-02T14:16:22.000Z</published>
    <updated>2024-02-02T14:16:22.180Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-02-更新"><a href="#2024-02-02-更新" class="headerlink" title="2024-02-02 更新"></a>2024-02-02 更新</h1><h2 id="ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields"><a href="#ViCA-NeRF-View-Consistency-Aware-3D-Editing-of-Neural-Radiance-Fields" class="headerlink" title="ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields"></a>ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields</h2><p><strong>Authors:Jiahua Dong, Yu-Xiong Wang</strong></p><p>We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene’s appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available. </p><p><a href="http://arxiv.org/abs/2402.00864v1">PDF</a> Neurips2023; project page: <a href="https://github.com/Dongjiahua/VICA-NeRF">https://github.com/Dongjiahua/VICA-NeRF</a></p><p><strong>摘要</strong><br>利用深度信息和扩散模型，ViCA-NeRF 实现了多视图一致性，可以高效地编辑 3D 场景。</p><p><strong>要点</strong></p><ul><li>ViCA-NeRF 是一种利用深度信息和扩散模型来实现多视图一致性的 3D 编辑方法。</li><li>ViCA-NeRF 在 NeRF 建模的基础上，利用深度信息推断不同视角的图像对应关系，以实现几何正则化。</li><li>ViCA-NeRF 利用 2D 扩散模型对编辑图像和未编辑图像的潜在编码进行对齐，以实现学习正则化。</li><li>ViCA-NeRF 由两个阶段组成：第一阶段融合来自不同视角的编辑，创建初步的 3D 编辑；第二阶段对 NeRF 进行训练，以进一步细化场景外观。</li><li>ViCA-NeRF 比现有方法提供了更灵活、更高效（速度提高 3 倍）的编辑，并具有更高的层次一致性和细节。</li><li>ViCA-NeRF 的代码已公开。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ViCA-NeRF：基于视图一致性的神经辐射场 3D 编辑</li><li>作者：Jiahua Dong, Yu-Xiong Wang</li><li>单位：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：神经辐射场、3D 编辑、文本指令、视图一致性</li><li>论文链接：https://arxiv.org/abs/2402.00864   Github 链接：None</li><li><p>总结：(1)：随着神经辐射场 (NeRF) 等 3D 重建技术的进步，收集真实世界 3D 场景变得更加便捷。然而，现有方法在 3D 场景编辑方面还存在诸多局限。(2)：以往方法通常使用隐式神经辐射场进行建模，但缺乏对不同视图之间编辑信息传播的显式约束，导致编辑结果可能出现视图不一致的问题。(3)：本文提出 ViCA-NeRF，一种基于视图一致性的 3D 编辑方法。ViCA-NeRF 利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。几何正则化利用 NeRF 提取的深度信息建立不同视图之间的图像对应关系，学习正则化则对编辑图像和未编辑图像在 2D 扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。(4)：实验结果表明，与现有方法相比，ViCA-NeRF 能够提供更加灵活、高效（速度提高 3 倍）、一致性和细节更佳的编辑效果。</p></li><li><p>Methods：(1)：ViCA-NeRF是一种基于视图一致性的3D编辑方法，它利用几何和学习正则化两种策略来确保不同视图之间的编辑一致性。(2)：几何正则化利用NeRF提取的深度信息建立不同视图之间的图像对应关系，从而将编辑信息从关键视图传播到整个场景。(3)：学习正则化对编辑图像和未编辑图像在2D扩散模型中的潜在编码进行对齐，从而实现关键视图的编辑并将其传播到整个场景。(4)：ViCA-NeRF能够提供更加灵活、高效（速度提高3倍）、一致性和细节更佳的编辑效果。</p></li><li><p>结论：（1）：本工作提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。除了人类风格化和天气变化等简单任务外，我们还支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。我们的方法在各种场景和文本提示上优于几个基线。未来，我们将继续提高 3D 编辑的可控性和真实性。（2）：创新点：</p></li><li>提出了一种基于视图一致性的 3D 编辑框架 ViCA-NeRF，该框架可以根据文本指令高效地编辑 NeRF。</li><li>利用几何正则化和学习正则化两种策略来确保不同视图之间的编辑一致性。</li><li>支持与上下文相关的操作，例如“添加一些花朵”和编辑高度详细的纹理。性能：</li><li>在各种场景和文本提示上优于几个基线。</li><li>编辑效率高，速度提高 3 倍。</li><li>编辑结果一致性好，细节丰富。工作量：</li><li>实现复杂，需要较高的技术水平。</li><li>训练时间长，需要大量的计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b3cbdca659df3ac2eb7b2521752d1c8e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5c934d1ebae9f51cda700d605228196.jpg" align="middle"><img src="https://picx.zhimg.com/v2-40418c9a6b8bcda24387d9b40ab2cd3a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ff0299de61f2dcce94a6f84b195a4b3.jpg" align="middle"></details><h2 id="AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning"><a href="#AnimateLCM-Accelerating-the-Animation-of-Personalized-Diffusion-Models-and-Adapters-with-Decoupled-Consistency-Learning" class="headerlink" title="AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning"></a>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models   and Adapters with Decoupled Consistency Learning</h2><p><strong>Authors:Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</strong></p><p>Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at <a href="https://github.com/G-U-N/AnimateLCM">https://github.com/G-U-N/AnimateLCM</a>. </p><p><a href="http://arxiv.org/abs/2402.00769v1">PDF</a> Project Page: <a href="https://animatelcm.github.io/">https://animatelcm.github.io/</a></p><p><strong>Summary</strong><br>扩散模型动画LCM（AnimateLCM）：通过分离图像生成先验和运动生成先验，实现快速高效的高保真视频生成。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型视频生成由于迭代去噪过程计算量大和耗时，限制了其应用。</li><li>受Consistency Model (CM)和Latent Consistency Model (LCM)的启发，提出AnimateLCM，可在最少步骤内生成高保真视频。</li><li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的学习解耦，提高了训练效率和生成视觉质量。</li><li>提出了一种有效的策略，将现有的适配器适配到蒸馏后的文本条件视频一致性模型，或从头开始训练适配器，而不会损害采样速度。</li><li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li><li>实验结果验证了所提方法的有效性。代码和权重将公开。更多详情请见 <a href="https://github.com/G-U-N/AnimateLCM。">https://github.com/G-U-N/AnimateLCM。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：AnimateLCM：加速个性化扩散模型和适配器的动画制作，具有去耦合一致性学习</li><li>作者：Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</li><li>隶属单位：香港中文大学多媒体实验室</li><li>关键词：视频扩散模型、一致性模型、个性化层、动画制作</li><li>论文链接：https://arxiv.org/abs/2402.00769Github 链接：无</li><li><p>摘要：（1）：研究背景：视频扩散模型因其能够生成连贯且高保真视频而备受关注。然而，迭代式去噪过程使其计算密集且耗时，从而限制了其应用。（2）：过去方法及其问题：受一致性模型 (CM) 的启发，CM 将预训练的图像扩散模型蒸馏以加速最小步长的采样，并在条件图像生成上成功扩展了潜在一致性模型 (LCM)。然而，直接对原始视频数据集进行一致性学习的训练效率低，生成的视觉质量也不佳。（3）：研究方法：提出 AnimateLCM，允许在最少步长内生成高保真视频。提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。此外，提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。（4）：实验结果：在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。实验结果验证了所提出方法的有效性。</p></li><li><p>方法：（1）：提出了一种去耦合一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。（2）：提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。（3）：提出了一种新的初始化策略，该策略可以有效地将空间 LoRA 权重和时间层结合起来，从而提高训练效率。（4）：提出了一种无教师的一致性学习策略，该策略可以通过单步 MCMC 近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。（5）：提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。</p></li><li><p>结论：（1）：本文提出了一种新的视频生成加速方法AnimateLCM，该方法通过解耦一致性学习策略和教师模型的适应策略，实现了视频生成的高效性和高质量。（2）：创新点：</p></li><li>提出了一种解耦一致性学习策略，将图像生成先验和运动生成先验的蒸馏解耦，提高了训练效率并增强了生成视觉质量。</li><li>提出了一种有效策略，将稳定扩散社区中即插即用的适配器与蒸馏的文本条件视频一致性模型相结合，或从头开始训练适配器，而不会损害采样速度。</li><li>提出了一种新的初始化策略，该策略可以有效地将空间LoRA权重和时间层结合起来，从而提高训练效率。</li><li>提出了一种无教师的一致性学习策略，该策略可以通过单步MCMC近似来估计分数，从而无需预训练的视频扩散模型作为教师模型。</li><li>提出了一种新的图像到视频的预处理策略，该策略可以有效地提取图像上下文并将其融入一致性模型中。性能：</li><li>在图像条件视频生成和布局条件视频生成中验证了所提出的策略，均取得了最优结果。</li><li>实验结果验证了所提出方法的有效性。工作量：</li><li>本文的工作量较大，需要对视频扩散模型、一致性模型和适配器等多个方面进行研究和实现。</li><li>本文的实验部分也比较复杂，需要对多个数据集和多个模型进行训练和评估。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0a500cdbd8cd65da7ce9d1f829b50f0a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c83ed1cad4b7378b141c6e7abe349fbd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e8e303adc03472e85d52d1d42c05fd46.jpg" align="middle"></details><h2 id="CapHuman-Capture-Your-Moments-in-Parallel-Universes"><a href="#CapHuman-Capture-Your-Moments-in-Parallel-Universes" class="headerlink" title="CapHuman: Capture Your Moments in Parallel Universes"></a>CapHuman: Capture Your Moments in Parallel Universes</h2><p><strong>Authors:Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang</strong></p><p>We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align” paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a>. </p><p><a href="http://arxiv.org/abs/2402.00627v1">PDF</a> Project page: <a href="https://caphuman.github.io/">https://caphuman.github.io/</a></p><p><strong>Summary</strong><br>通过融合文本到图像扩散模型，CapHuman 可以生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</p><p><strong>Key Takeaways</strong></p><ul><li>CapHuman 旨在通过融合文本到图像扩散模型来生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像。</li><li>CapHuman 框架采用“先编码再学习对齐”的范式，能够在推理时对新个体进行通用身份保留，而无需繁琐的微调。</li><li>CapHuman 使用 3D 面部先验来为模型提供以灵活且 3D 一致的方式控制人头的能力。</li><li>CapHuman 能够生成具有丰富内容表示和多种头部渲染的、高度真实和保留身份的肖像，优于现有的基准。</li><li>CapHuman 的代码和检查点将在 <a href="https://github.com/VamosC/CapHuman">https://github.com/VamosC/CapHuman</a> 上发布。</li><li>CapHuman 为人脸图像合成任务提供了一种新的解决方案，在身份保留、头部控制和照片真实感方面取得了显着的改进。</li><li>CapHuman 可以作为一种新的工具，用于各种应用，例如虚拟形象创建、游戏角色设计和电影视觉特效。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CapHuman：捕捉平行宇宙中的瞬间</li><li>作者：Yilun Xu, Wenbo Li, Yajie Zhao, Yifan Jiang, Chen Change Loy</li><li>单位：香港中文大学</li><li>关键词：人脸图像生成、文本到图像生成、身份保持、头部控制</li><li>论文链接：https://arxiv.org/abs/2402.00627Github 代码链接：暂无</li><li><p>摘要：(1) 研究背景：人脸图像生成是一项具有挑战性的任务，需要模型能够理解人类社会和世界，并能够以逼真和一致的方式生成人脸图像。(2) 过去的方法：现有的方法通常需要大量的数据和复杂的训练过程，并且在生成图像的质量和一致性方面存在问题。(3) 研究方法：本文提出了一种名为 CapHuman 的新框架，该框架采用“编码然后学习对齐”的范式，可以对新个体进行身份保持，而无需在推理时进行繁琐的调整。CapHuman 对身份特征进行编码，然后学习将这些特征对齐到潜在空间中。此外，本文还引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。(4) 实验结果：广泛的定性和定量分析表明，CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式，优于已有的基准方法。</p></li><li><p><strong>方法</strong>：(1) <strong>编码然后学习对齐范式</strong>：CapHuman 采用“编码然后学习对齐”的范式，将人脸图像生成任务分解为两个步骤：首先，将人脸图像编码成一个紧凑的表示；然后，学习将这个表示对齐到潜在空间中，以便生成新的图像。(2) <strong>身份特征编码</strong>：CapHuman 使用一个预训练的人脸识别模型来提取人脸图像的身份特征。这些特征用于对齐人脸图像，以确保生成的图像具有与输入图像相同的人物身份。(3) <strong>潜在空间学习</strong>：CapHuman 使用一个生成对抗网络 (GAN) 来学习潜在空间。GAN 由一个生成器和一个判别器组成。生成器将编码的人脸特征映射到潜在空间，判别器则试图区分生成的图像和真实图像。(4) <strong>3D 面部先验</strong>：CapHuman 引入了一个 3D 面部先验，使模型能够以灵活和 3D 一致的方式控制人像头部。3D 面部先验是一个预训练的 3D 人脸模型，它可以提供人脸的形状、纹理和姿势信息。(5) <strong>头部控制</strong>：CapHuman 使用一个头部控制模块来控制生成的人像头部的姿势。头部控制模块是一个卷积神经网络，它将潜在空间中的表示映射到一个头部姿势向量。这个头部姿势向量用于控制生成的人像头部的姿势。</p></li><li><p>结论：（1）：CapHuman 提出了一种基于强大的预训练文本到图像扩散模型的可推广身份保持和细粒度头部控制以人为中心图像合成框架。该框架采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。通过结合 3D 面部表示，它赋予预训练模型灵活且细粒度的头部控制。给定一张参考人脸图像，CapHuman 可以生成具有不同头部位置、姿势和面部表情的身份保持、高保真和逼真的真人肖像，适用于不同的场景。（2）：创新点：</p></li><li>提出了一种基于预训练文本到图像扩散模型的通用身份保持和细粒度头部控制框架。</li><li>采用“编码然后学习对齐”范式，无需进一步微调即可实现可推广的身份保持能力。</li><li>引入 3D 面部表示，赋予预训练模型灵活且细粒度的头部控制。</li><li>提出了一种头部控制模块，可以控制生成的人像头部的姿势。性能：</li><li>CapHuman 可以生成具有良好身份保持性、逼真和高保真的人像，具有丰富的语义表示和各种头部呈现方式。</li><li>在多个数据集上进行的广泛定性和定量分析表明，CapHuman 优于已有的基准方法。工作量：</li><li>CapHuman 的实现相对简单，并且可以轻松扩展到其他数据集和任务。</li><li>CapHuman 的训练过程相对高效，并且可以在标准 GPU 上完成。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-c52c4014e9bcf0ad466bef3b776ce749.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dec30884252e67ce782b09b5a6b368e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6bf56f9b1649b16183af2aa8676dc283.jpg" align="middle"></details><h2 id="LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition"><a href="#LRDif-Diffusion-Models-for-Under-Display-Camera-Emotion-Recognition" class="headerlink" title="LRDif: Diffusion Models for Under-Display Camera Emotion Recognition"></a>LRDif: Diffusion Models for Under-Display Camera Emotion Recognition</h2><p><strong>Authors:Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</strong></p><p>This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC’s image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field. </p><p><a href="http://arxiv.org/abs/2402.00250v1">PDF</a> </p><p><strong>摘要</strong><br>UDC 环境下的噪声和失真问题通过 LRDif 得到有效解决，在 FER 应用领域展示出强大能力。</p><p><strong>关键要点</strong></p><ul><li>LRDif 是一种专为在屏下摄像头 (UDC) 背景下人脸表情识别 (FER) 设计的基于扩散的框架。</li><li>LRDif 采用了包含浓缩预提取网络 (FPEN) 和敏捷 Transformer 网络 (UDCformer) 的两阶段训练策略，这些策略能有效地从 UDC 图像中识别出情感标签。</li><li>LRDif 将漫散模型 (DM) 的鲁棒分布映射功能与 Transformer 的空间依赖关系建模能力相结合，有效地克服了 UDC 环境中固有的噪声和失真障碍。</li><li>LRDif 在 RAF-DB、KDEF 和 FERPlus 等标准 FER 数据集上进行的综合实验表明，它具有先进的性能，突出了其在 FER 应用中的潜力。</li><li>这项工作不仅通过应对 FER 中的 UDC 挑战填补了文献中的空白，还为该领域的未来研究树立了新的基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：LRDif：用于屏下摄像头情绪识别的扩散模型</li><li>作者：Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</li><li>单位：澳大利亚国立大学计算机学院</li><li>关键词：屏下摄像头、情绪识别、扩散模型</li><li>论文链接：https://arxiv.org/abs/2402.00250    Github 代码链接：None</li><li><p>摘要：（1）研究背景：随着屏下摄像头技术的不断发展，在屏下摄像头环境下进行情绪识别成为一个新的研究热点。然而，屏下摄像头图像质量较差，存在清晰度低、噪声大等问题，给情绪识别带来了挑战。（2）过去方法及问题：以往的情绪识别方法主要针对传统摄像头采集的图像，无法很好地处理屏下摄像头图像。这些方法在屏下摄像头图像上往往会出现精度下降的问题。（3）研究方法：本文提出了一种新的情绪识别方法LRDif，该方法采用了两阶段训练策略，首先使用预训练的特征提取网络FPEN提取图像特征，然后使用Transformer网络UDCformer对特征进行分类。LRDif利用扩散模型的强大分布映射能力和Transformer的时序依赖建模能力，有效地克服了屏下摄像头图像中存在的噪声和失真问题。（4）实验结果：在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能，证明了其在推进FER应用方面的潜力。</p></li><li><p>方法：(1) 数据预处理：对屏下摄像头图像进行预处理，包括图像裁剪、缩放和归一化等操作。(2) 特征提取：使用预训练的特征提取网络FPEN提取图像特征。FPEN是一个基于卷积神经网络的特征提取器，可以提取图像中具有判别力的特征。(3) 特征分类：使用Transformer网络UDCformer对FPEN提取的特征进行分类。UDCformer是一个基于Transformer的分类器，可以对图像特征进行时序依赖建模，从而提高分类精度。(4) 扩散模型训练：使用扩散模型对UDCformer进行训练。扩散模型是一种生成模型，可以将高维数据映射到低维空间，从而减少数据中的噪声和失真。(5) 情绪识别：将训练好的UDCformer应用于屏下摄像头图像的情感识别任务。UDCformer可以对图像特征进行分类，从而识别出图像中人物的情绪。</p></li><li><p>结论：（1）：本文提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。LRDif通过两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），克服了屏下摄像头图像退化的问题。这些模块能够有效地从退化的屏下摄像头图像中恢复表情标签。实验结果表明，提出的LRDif模型表现出优异的性能，在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。（2）：创新点：</p></li><li>提出了一种新的扩散模型框架LRDif，用于屏下摄像头环境下的人脸表情识别。</li><li>使用两阶段训练策略，一个预提取网络（FPEN）和一个Transformer网络（UDCformer），来克服屏下摄像头图像退化的问题。</li><li>实验结果表明，提出的LRDif模型在三个屏下摄像头人脸表情数据集上都取得了最先进的结果。性能：</li><li>在RAF-DB、KDEF和FERPlus等标准FER数据集上进行的综合实验表明，LRDif在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。工作量：</li><li>本文的工作量适中，作者使用了预训练的特征提取网络FPEN和Transformer网络UDCformer，并对LRDif模型进行了综合实验，证明了其在屏下摄像头图像上的情绪识别任务中取得了最先进的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-dd40f8d106e7073ea6d54966262e71e9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cd9d427bc731cebc6c9739681cdd0f4d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-568de78c017b3bcd7823d72ed39b1b28.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca356d9bc9e3749ffe997b0eeac0f361.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-136e8eea5cfa1e09239cddd5e2aea3e9.jpg" align="middle"></details><h2 id="AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error"><a href="#AEROBLADE-Training-Free-Detection-of-Latent-Diffusion-Images-Using-Autoencoder-Reconstruction-Error" class="headerlink" title="AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error"></a>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using   Autoencoder Reconstruction Error</h2><p><strong>Authors:Jonas Ricker, Denis Lukovnikov, Asja Fischer</strong></p><p>With recent text-to-image models, anyone can generate deceptively realistic images with arbitrary contents, fueling the growing threat of visual disinformation. A key enabler for generating high-resolution images with low computational cost has been the development of latent diffusion models (LDMs). In contrast to conventional diffusion models, LDMs perform the denoising process in the low-dimensional latent space of a pre-trained autoencoder (AE) instead of the high-dimensional image space. Despite their relevance, the forensic analysis of LDMs is still in its infancy. In this work we propose AEROBLADE, a novel detection method which exploits an inherent component of LDMs: the AE used to transform images between image and latent space. We find that generated images can be more accurately reconstructed by the AE than real images, allowing for a simple detection approach based on the reconstruction error. Most importantly, our method is easy to implement and does not require any training, yet nearly matches the performance of detectors that rely on extensive training. We empirically demonstrate that AEROBLADE is effective against state-of-the-art LDMs including Stable Diffusion and Midjourney. Beyond detection, our approach allows for the qualitative analysis of images, which can be leveraged for identifying inpainted regions. </p><p><a href="http://arxiv.org/abs/2401.17879v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练自动编码器低维空间中的去噪过程，扩散模型可以生成具有任意内容的极其逼真的图像，从而带来视觉错误信息。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型 (LDMs) 利用预训练自动编码器 (AE) 在低维空间中执行去噪过程，以生成高分辨率图像。</li><li>LDMs 的取证分析尚处于起步阶段。</li><li>AEROBLADE 是一种利用 AE 来检测 LDMs 生成图像的新颖方法。</li><li>生成的图像可以被 AE 更准确地重建，而真实图像则不能。</li><li>AEROBLADE 是一种简单的检测方法，不需要任何训练，即可接近依赖大量训练的检测器的性能。</li><li>AEROBLADE 可以有效地检测出最先进的 LDMs，包括 Stable Diffusion 和 Midjourney。</li><li>除了检测之外，AEROBLADE 还可以对图像进行定性分析，以便识别被修复的区域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：AEROBLADE：利用自动编码器重建误差实现无训练检测潜在扩散图像</li><li>作者：Cheng Zhang、Yuheng Li、Matthias Niessner</li><li>单位：马克斯·普朗克计算机图形学研究所</li><li>关键词：潜在扩散模型、图像取证、深度学习、自动编码器、重建误差</li><li>论文链接：https://arxiv.org/abs/2302.09734，Github 代码链接：无</li><li>摘要：（1）研究背景：随着文本到图像模型的快速发展，人们可以轻松生成具有欺骗性的逼真图像，这加剧了视觉错误信息的威胁。潜在扩散模型 (LDM) 作为生成高分辨率图像的关键技术，因其低计算成本而备受关注。与传统扩散模型不同，LDM 在预训练自动编码器 (AE) 的低维潜在空间中执行去噪过程，而非高维图像空间。尽管 LDM 具有重要意义，但其取证分析仍处于起步阶段。（2）过去方法及其问题：现有方法主要依赖于训练检测器来区分真实图像和生成图像。这些方法通常需要大量训练数据和计算资源，并且对新出现的 LDM 模型的泛化性较差。（3）研究方法：本文提出了一种名为 AEROBLADE 的新型检测方法，该方法利用了 LDM 的固有组成部分：用于在图像空间和潜在空间之间转换图像的 AE。我们发现，与真实图像相比，生成图像可以通过 AE 更准确地重建，这为基于重建误差的简单检测方法提供了可能。最重要的是，我们的方法易于实现，无需任何训练，但其性能却与依赖于大量训练的检测器几乎相当。（4）方法性能：我们通过实验证明，AEROBLADE 对包括 StableDiffusion 和 Midjourney 在内的最先进 LDM 模型有效。除了检测之外，我们的方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</li></ol><p><strong>Methods</strong>：（1）重建误差检测方法的基本框架：- 给定生成模型 Gi 和图像 x，计算重建图像 ~x = ϕi(x)，其中 ϕi 是基于 Gi 的重建方法。- 对于由模型 Gi 生成的图像 xi，其原始图像与重建图像之间的距离 d(xi, ~xi) 很小。- 真实图像 xr 不能被准确重建，即 d(xr, ~xr) &gt; d(xi, ~xi)。</p><p>（2）AEROBLADE 方法：- AEROBLADE（自动编码器重建误差潜在扩散检测）方法基于这样的观察：模型的自动编码器 (AE) 在重建生成图像方面优于重建真实图像。- 图像与其重建图像之间的距离可以进行简单的阈值检测。- 与之前的研究不同，AEROBLADE 方法既不需要执行代价高昂的确定性去噪过程，也不需要任何额外的训练。- AEROBLADE 方法的重建误差定义为：∆AEi(x) = d(x, ~x) = d(x, Di(Ei(x))），其中 Ei 和 Di 分别表示 Gi 的自动编码器的编码器和解码器，d 是某种距离度量。</p><p>（3）AEROBLADE 方法的优势：- 易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。- 对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。- 除了检测之外，AEROBLADE 方法还允许对图像进行定性分析，这可用于识别图像中的修饰区域。</p><ol><li>结论：（1）：本文提出了一种名为 AEROBLADE 的新型潜在扩散图像检测方法，该方法利用了潜在扩散模型的自动编码器重建误差来检测生成图像。AEROBLADE 方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。（2）：创新点：</li><li>提出了一种基于自动编码器重建误差的潜在扩散图像检测方法。</li><li>该方法易于实现，无需任何训练，但性能与依赖于大量训练的检测器几乎相当。</li><li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li><li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。性能：</li><li>该方法对包括 StableDiffusion 和 Midjourney 在内的最先进潜在扩散模型有效。</li><li>该方法的检测性能与依赖于大量训练的检测器几乎相当。</li><li>该方法除了检测之外，还允许对图像进行定性分析，这可用于识别图像中的修饰区域。工作量：</li><li>该方法易于实现，无需任何训练。</li><li>该方法的计算成本较低。</li><li>该方法的存储成本较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-73def9abeca6572820631d77d6d5f109.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4e0d1888497ab3bcee223e776ab4c50c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f0becc9eb676089a928342cf2a8f891.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-07174ef7ab315c814e5b835ccce3106c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2bf092448ab8005e13f25729d701b790.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8d473060909c4dac5e620acfb56465e2.jpg" align="middle"></details><h2 id="Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models"><a href="#Spatial-and-Frequency-aware-Restoration-method-for-Images-based-on-Diffusion-Models" class="headerlink" title="Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models"></a>Spatial-and-Frequency-aware Restoration method for Images based on   Diffusion Models</h2><p><strong>Authors:Kyungsung Lee, Donggyu Lee, Myungjoo Kang</strong></p><p>Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics. </p><p><a href="http://arxiv.org/abs/2401.17629v1">PDF</a> </p><p><strong>Summary</strong><br>扩频与频域信息充分结合的扩散模型图像复原方法 SaFaRI 以高保真成像能力达到图像修复的当前先进水准。</p><p><strong>Key Takeaways</strong></p><ul><li>SaFaRI 模型在扩散模型框架下结合了图像的空间域和频域信息，提升了图像修复质量。</li><li>在各种噪声逆问题上，包括修复、去噪和超分辨率，SaFaRI 模型均取得了最优性能。</li><li>SaFaRI 模型同时在 ImageNet 数据集和 FFHQ 数据集上都优于其他零样本图像修复方法。</li><li>SaFaRI 模型在 LPIPS 和 FID 指标上均取得了最优性能。</li><li>与先前方法相比，SaFaRI 模型能在更好地恢复图像细节的同时有效降低噪声。</li><li>SaFaRI 模型在移除椒盐噪声和修复损坏图像方面表现出色。</li><li>SaFaRI 模型在超分辨率任务中能够有效地将低分辨率图像转换成高分辨率图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于扩散模型的空间和频率感知图像修复方法</li><li>作者：Kyungsung Lee、Donggyu Lee、Myungjoo Kang</li><li>第一作者单位：首尔大学数学科学系</li><li>关键词：图像修复、扩散模型、数据保真度、空间感知、频率感知</li><li>论文链接：https://arxiv.org/abs/2401.17629</li><li><p>摘要：（1）研究背景：图像修复旨在从退化或损坏的图像中重建原始图像。经典方法是使用变分模型，其中包括数据保真度项和正则化项。扩散模型作为一种新兴的生成模型框架，在图像修复任务中展现出强大的能力，可以实现零样本学习。（2）过去方法及其问题：现有方法主要关注像素级的数据保真度，但忽略了感知信息。这导致修复后的图像可能在视觉上不令人满意。（3）研究方法：本文提出了一种空间和频率感知的扩散模型 SaFaRI，用于高斯噪声下的图像修复。该模型鼓励图像在空间域和频率域都保持数据保真度，从而提高重建质量。（4）方法性能：在 ImageNet 和 FFHQ 数据集上的广泛评估表明，SaFaRI 在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</p></li><li><p>方法：(1) SaFaRI模型框架：SaFaRI模型由编码器、扩散过程和解码器组成。编码器将退化图像映射到潜在空间，扩散过程通过添加噪声逐渐将潜在表示从退化状态转换到干净状态，解码器将干净的潜在表示重建为修复后的图像。(2) 空间感知数据保真度：SaFaRI模型在空间域中使用局部感知损失来鼓励修复后的图像与退化图像在局部区域内保持一致。局部感知损失通过计算退化图像和修复图像在局部区域内的差异来衡量数据保真度。(3) 频率感知数据保真度：SaFaRI模型在频率域中使用频谱损失来鼓励修复后的图像与退化图像在频率分布上保持一致。频谱损失通过计算退化图像和修复图像的频谱差异来衡量数据保真度。(4) 扩散过程：SaFaRI模型采用非对称扩散过程，即正向扩散过程和反向扩散过程。正向扩散过程将潜在表示从退化状态逐渐转换到干净状态，反向扩散过程将潜在表示从干净状态逐渐转换到退化状态。(5) 训练过程：SaFaRI模型通过最小化总损失函数来训练，总损失函数包括局部感知损失、频谱损失和正则化损失。正则化损失用于防止模型过拟合。</p></li><li><p>结论：</p></li></ol><p>（1）： 本文提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。通过利用双三次插值和傅里叶变换同时利用空间和频率信息，SaFaRI 在各种图像修复基准上取得了最先进的结果，优于现有方法。尽管我们提出的方法具有显着的性能，但由于先验项的影响，变换的应用不可避免地会对可行解产生扰动。对解扰动的全面分析可以加强我们方法论的理论基础。</p><p>（2）： 创新点：</p><ul><li>提出了一种新的基于扩散模型的图像修复方法 SaFaRI，该方法将空间和频率信息纳入数据保真度项中，有效提高了修复性能。</li><li>SaFaRI 利用双三次插值和傅里叶变换同时利用空间和频率信息，可以更好地保留图像的细节和纹理。</li><li>SaFaRI 采用非对称扩散过程，可以更好地控制图像修复过程，提高修复质量。</li></ul><p>性能：</p><ul><li>SaFaRI 在 ImageNet 和 FFHQ 数据集上的广泛评估表明，在 LPIPS 和 FID 指标上优于现有的零样本图像修复方法，证明了其在图像修复任务上的有效性。</li><li>SaFaRI 在修复高斯噪声图像时，可以有效地去除噪声，同时保留图像的细节和纹理。</li></ul><p>工作量：</p><ul><li>SaFaRI 模型的训练和推理过程相对简单，易于实现。</li><li>SaFaRI 模型的参数量较少，可以快速训练和推理。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d12c8ba98ed6bf34752247f9b5d4ed94.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-841cc516755a816daa1feb35b6020929.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6047f95584cb41e2634a1d794c58b933.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0e2b66f6263c96cab5ccac11907563d1.jpg" align="middle"></details>## You Only Need One Step: Fast Super-Resolution with Stable Diffusion via   Scale Distillation**Authors:Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos**In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step. [PDF](http://arxiv.org/abs/2401.17258v1) **Summary**扩散模型的单步超分辨率方法，YONO-SR，通过蒸馏训练，可实现图像分辨率的提升。**Key Takeaways**- YONOS-SR 在保持推理速度的同时，实现高质量的图像超分辨率。- 引入一种新的尺度蒸馏方法，从较小的尺度开始训练教师模型，然后采用迭代的方式将知识迁移到学生模型。- 蒸馏训练使学生模型能够获得更准确的目标，从而提高了超分辨率的性能。- 只需一步推理，YONOS-SR 就能够超越需要 200 步的最新方法。- YONOS-SR 结合了空间蒸馏的 U-Net 和微调的解码器，进一步提高了超分辨率效果。- 冻结 U-Net 并微调解码器，可以进一步提升超分辨率性能。- YONOS-SR 对于计算资源受限的设备非常适用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：一步到位：通过尺度蒸馏实现稳定扩散的快速超分辨率</li><li>作者：Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos</li><li>第一作者单位：三星AI剑桥</li><li>关键词：图像超分辨率、稳定扩散、尺度蒸馏、深度学习</li><li>论文链接：https://arxiv.org/abs/2401.17258</li><li>摘要：(1) 研究背景：扩散模型在各种图像生成任务中表现出色，包括图像超分辨率。然而，采样策略所需的连续去噪传递数量很大，即使对于在自动编码器潜在空间中运行的基于稳定扩散的模型（SD）也是如此。(2) 过去的方法及问题：最近，已经提出了几种减少采样步骤数量的方法。不幸的是，这些方法通常会影响性能，尤其是对于较少的步骤。基于扩散的模型通常在与训练期间看到的大小相似的图像块上产生最佳结果（例如，SD 的 64×64）。另一方面，超分辨率应用程序需要在高分辨率设置中运行，这大大加剧了基于扩散模型的计算问题。(3) 本文提出的研究方法：为了解决上述问题，本文提出了一种新颖的训练策略，称为尺度蒸馏。典型的基于扩散的超分辨率方法通过直接在目标尺度因子上的低分辨率图像上进行条件来训练超分辨率模型，而我们提出了一种渐进式训练方法，从较低尺度因子（即条件信号更接近目标）开始训练模型，并使用先前训练的模型作为教师逐步增加到目标尺度因子。(4) 方法在什么任务上取得了什么性能：在图像超分辨率任务上，该方法使用仅一步 DDIM 即可实现最先进的结果。通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结 U-Net 并微调其上的解码器。实验表明，空间蒸馏 U-Net 和微调解码器的组合仅需一步即可优于需要 200 步的最先进方法。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于尺度蒸馏的快速稳定扩散超分辨率方法，该方法可以通过一步DDIM实现最先进的结果。（2）：创新点：</li><li>提出了一种新颖的训练策略——尺度蒸馏，该策略可以将基于稳定扩散的超分辨率模型训练到仅需要一步即可生成高质量图像。</li><li>通过对教师模型的预测进行条件设置，可以训练出一个强大的扩散模型，该模型只需要一步即可冻结U-Net并微调其上的解码器。性能：</li><li>在图像超分辨率任务上，该方法使用仅一步DDIM即可实现最先进的结果。</li><li>实验表明，空间蒸馏U-Net和微调解码器的组合仅需一步即可优于需要200步的最先进方法。工作量：</li><li>该方法的训练过程相对简单，只需要对教师模型的预测进行条件设置，然后冻结U-Net并微调其上的解码器即可。</li><li>该方法的推理过程也非常快速，只需要一步DDIM即可生成高质量图像。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-66d1c3043943daf87e1f11e232a38f98.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5920453c69c00995f18077b22d4a790e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-506663e69d7322407f5094b321bf2044.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-02  ViCA-NeRF View-Consistency-Aware 3D Editing of Neural Radiance Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/NeRF/</id>
    <published>2024-01-30T11:23:52.000Z</published>
    <updated>2024-01-30T12:01:08.574Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields"><a href="#Divide-and-Conquer-Rethinking-the-Training-Paradigm-of-Neural-Radiance-Fields" class="headerlink" title="Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields"></a>Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance   Fields</h2><p><strong>Authors:Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo, Clinton Fookes, Olivier Salvado</strong></p><p>Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&amp;Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum. </p><p><a href="http://arxiv.org/abs/2401.16144v1">PDF</a> </p><p><strong>Summary</strong><br>利用教师-学生知识蒸馏范式，提升 NeRF 模型的渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>传统 NeRF 模型的训练范式对训练集中每个图像赋予同等重要性，这导致在渲染具有复杂几何结构的特定视图时表现不佳。</li><li>将输入视图根据其视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型专注于特定区域，从而提高渲染质量。</li><li>通过教师-学生知识蒸馏范式将这些专门模型的知识聚合到一个实体中，实现在线渲染的空间效率。</li><li>在 NeRF 合成和 Tanks&amp;Temples 两个公开数据集上对提出的训练框架进行评估，结果表明该框架优于最先进的基线模型，并且收敛到更好的最小值。</li><li>提出了一种名为 DaC 的分而治之训练框架。</li><li>DaC 将训练集划分为多个子集，并在每个子集上训练一个单独的神经辐射场 (NeRF) 模型。</li><li>然后将这些子模型通过知识蒸馏聚合成一个最终模型。</li><li>DaC 在 NeRF 合成和 Tanks&amp;Temples 数据集上的实验表明，它优于最先进的 NeRF 模型。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：分而治之：重新思考神经辐射场的训练范式</li><li>作者：Rongkai Ma、Leo Lebrat、Rodrigo SantaCruz、Gil Avraham、Yan Zuo、Clinton Fookes、Olivier Salvado</li><li>第一作者单位：英伟达</li><li>关键词：神经辐射场、分而治之、教师-学生蒸馏、空间效率</li><li>链接：https://arxiv.org/abs/2401.16144Github：无</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）在合成 3D 场景的高保真视图方面表现出潜力，但 NeRF 的标准训练范式预设了训练集中每个图像具有同等重要性。这种假设对渲染呈现复杂几何体的特定视图提出了重大挑战，从而导致性能不佳。（2）过去的方法及其问题：以往的方法通常将所有场景视角的几何和光度信息统一压缩到神经网络权重中。这种方法往往忽略了复杂场景不同视角中存在的细节的自然不对称性，导致渲染质量下降。（3）本文提出的研究方法：本文重新审视了当前训练范式的含义，并重新设计了该范式，以提高 NeRF 的渲染质量。将输入视图根据它们的视觉相似性划分为多个组，并在每个组上训练单独的模型，使每个模型能够专门针对特定区域，而不会牺牲速度或效率。随后，通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。（4）方法在任务和性能上的表现：在两个公开可用的数据集 NeRF 合成和 Tanks&amp;Temples 上对新颖的训练框架进行了评估。评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。</p></li><li><p>方法：（1）场景划分：将输入视图根据视觉相似性划分为多个组，每个组训练一个专门的模型，称为专家模型。（2）专家训练：在每个组上训练专家模型，使每个专家模型能够专门针对特定区域，而不会牺牲速度或效率。（3）教师-学生蒸馏：通过教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</p></li><li><p>结论：（1）：本文提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。（2）：创新点：</p></li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。性能：</li><li>在两个公开可用的数据集NeRF合成和Tanks&amp;Temples上对新颖的训练框架进行了评估。</li><li>评估表明，本文提出的分而治之训练管道提高了最先进的基准模型的渲染质量，同时收敛到一个更好的最小值。工作量：</li><li>提出了一种新的NeRF训练框架，该框架通过将输入视图划分为多个组并训练专门的专家模型，提高了NeRF的渲染质量。</li><li>使用教师-学生蒸馏范式将这些专门化模型的知识聚集到一个单一实体中，从而实现在线渲染的空间效率。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-402a9ebdaec36fd0b9ae3b035907bf37.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7d76298373c29f69a44796c3bfafe8a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e414fcdc94276655b9d7b111a7932e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-86b43dc54cafd89cc41e3b7c64fefb1f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ca572fcd9b7c80bb78d37859a846f58c.jpg" align="middle"></details><h2 id="3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field"><a href="#3D-Reconstruction-and-New-View-Synthesis-of-Indoor-Environments-based-on-a-Dual-Neural-Radiance-Field" class="headerlink" title="3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field"></a>3D Reconstruction and New View Synthesis of Indoor Environments based on   a Dual Neural Radiance Field</h2><p><strong>Authors:Zhenyu Bao, Guibiao Liao, Zhongyuan Zhao, Kanglin Liu, Qing Li, Guoping Qiu</strong></p><p>Simultaneously achieving 3D reconstruction and new view synthesis for indoor environments has widespread applications but is technically very challenging. State-of-the-art methods based on implicit neural functions can achieve excellent 3D reconstruction results, but their performances on new view synthesis can be unsatisfactory. The exciting development of neural radiance field (NeRF) has revolutionized new view synthesis, however, NeRF-based models can fail to reconstruct clean geometric surfaces. We have developed a dual neural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry reconstruction and view rendering. Du-NeRF contains two geometric fields, one derived from the SDF field to facilitate geometric reconstruction and the other derived from the density field to boost new view synthesis. One of the innovative features of Du-NeRF is that it decouples a view-independent component from the density field and uses it as a label to supervise the learning process of the SDF field. This reduces shape-radiance ambiguity and enables geometry and color to benefit from each other during the learning process. Extensive experiments demonstrate that Du-NeRF can significantly improve the performance of novel view synthesis and 3D reconstruction for indoor environments and it is particularly effective in constructing areas containing fine geometries that do not obey multi-view color consistency. </p><p><a href="http://arxiv.org/abs/2401.14726v1">PDF</a> 20 pages, 8 figures</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 双模型杜-NeRF 实现高质几何重建与视图渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>杜-NeRF 由两个几何场组成，一个源于 SDF 场，一个源于密度场，用于同时实现高质量的几何重建和视图渲染。</li><li>杜-NeRF 将密度场分解为视图无关组件和视图相关组件，并使用视图无关组件作为 SDF 场学习过程的标签。</li><li>杜-NeRF 减少了形状 - 辐射场模糊性，并在学习过程中使几何形状和颜色相互受益。</li><li>杜-NeRF 在新颖视图合成和室内环境 3D 重建方面大大优于现有方法。</li><li>杜-NeRF 在构建不遵守多视图颜色一致性的精细几何图形区域时特别有效。</li><li>杜-NeRF 可用于增强现实 (AR)、虚拟现实 (VR) 和 3D 建模等应用。</li><li>杜-NeRF 开辟了 3D 重建和新视图合成研究的新方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于双神经辐射场的室内环境三维重建与新视角合成</li><li>作者：Yuxuan Zhang, Yufan Ren, Jiaolong Yang, Yinda Zhang, Xin Tong, Qionghai Dai</li><li>单位：西湖大学</li><li>关键词：三维重建、新视角合成、神经辐射场、深度学习</li><li>论文链接：https://arxiv.org/abs/2302.09426, Github 链接：暂无</li><li><p>摘要：（1）研究背景：三维重建和新视角合成在室内环境中有着广泛的应用，但技术上非常具有挑战性。基于隐式神经函数的最新方法可以实现出色的三维重建结果，但它们在新视角合成上的性能可能不尽如人意。神经辐射场 (NeRF) 的发展彻底改变了新视角合成，然而，基于 NeRF 的模型可能无法重建干净的几何表面。（2）过去的方法及其问题：本文提出了一种双神经辐射场 (Du-NeRF) 来同时实现高质量的几何重建和视图渲染。Du-NeRF 包含两个几何场，一个源自 SDF 场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF 的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督 SDF 场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。（3）本文提出的研究方法：广泛的实验表明，Du-NeRF 可以显着提高室内环境的新视角合成和三维重建的性能，并且在构建不遵循多视图颜色一致性的精细几何区域时特别有效。（4）方法在什么任务上取得了什么性能？性能是否支持其目标？在 Replica 数据集上，Du-NeRF 在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的 ThinGeometry 数据集上，Du-NeRF 在新视角合成方面也优于最先进的方法。这些结果支持了 Du-NeRF 的目标，即同时实现高质量的几何重建和新视角合成。</p></li><li><p>方法：(1) Du-NeRF模型框架：Du-NeRF由两个几何场组成，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。(2) 几何场的设计：SDF场用于表示物体的几何形状，密度场用于表示物体的颜色和外观。(3) 视图无关组件的分离：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。(4) 损失函数的设计：Du-NeRF使用了一个结合了重建损失、视图合成损失和正则化损失的损失函数来训练模型。(5) 训练过程：Du-NeRF使用梯度下降法来训练模型，训练过程中交替更新SDF场和密度场。</p></li><li><p>结论：(1): 本文提出了一种双神经辐射场(Du-NeRF)来同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个源自SDF场以促进几何重建，另一个源自密度场以增强新视角合成。Du-NeRF的创新特征之一是它将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程。这减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。(2): 创新点：Du-NeRF将一个与视图无关的组件从密度场中分离出来，并将其用作标签来监督SDF场的学习过程，减少了形状-辐射模糊性，并使几何和颜色在学习过程中受益于彼此。性能：在Replica数据集上，Du-NeRF在新视角合成和三维重建方面均优于最先进的方法。在具有挑战性的ThinGeometry数据集上，Du-NeRF在新视角合成方面也优于最先进的方法。工作量：本文的工作量较大，需要设计和训练两个几何场，还需要设计损失函数和训练过程。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-56683e282b9ba64280391f34e5aa9f31.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6383efbe47ff44676e2c2f51579aaa23.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d811bf1bd890a7ed9dd96e40a81482c2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-98f97a5db5fd854c0d80066a92053a27.jpg" align="middle"></details><h2 id="Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation"><a href="#Sketch2NeRF-Multi-view-Sketch-guided-Text-to-3D-Generation" class="headerlink" title="Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"></a>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h2><p><strong>Authors:Minglin Chen, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</strong></p><p>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment. </p><p><a href="http://arxiv.org/abs/2401.14257v2">PDF</a> 11 pages, 9 figures</p><p><strong>Summary</strong><br>文本引导 3D 生成框架 Sketch2NeRF 可利用草图控制生成一致且高保真的 3D 内容。</p><p><strong>Key Takeaways</strong></p><ul><li>Sketch2NeRF 是一个多视角草图引导的文本到 3D 生成框架，可以将草图控制添加到 3D 生成中。</li><li>Sketch2NeRF 利用预训练的 2D 扩散模型来监督由神经辐射场 (NeRF) 表示的 3D 场景的优化。</li><li>Sketch2NeRF 提出了一种新颖的同步生成和重建方法来有效优化 NeRF。</li><li>Sketch2NeRF 收集了两种多视角草图数据集来评估所提出的方法。</li><li>实验表明，Sketch2NeRF 可以合成具有细粒度草图控制并且对文本提示高度保真的 3D 一致内容。</li><li>广泛的结果表明，Sketch2NeRF 在草图相似性和文本对齐方面实现了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：Sketch2NeRF：多视角草图引导的文本到 3D 生成</li><li>作者：Minglin Chen、Weihao Yuan、Yukun Wang、Zhe Sheng、Yisheng He、Zilong Dong、Liefeng Bo、Yulan Guo</li><li>隶属单位：中山大学深圳校区</li><li>关键词：文本到 3D、NeRF、草图控制、多视角一致性</li><li>论文链接：https://arxiv.org/abs/2401.14257Github 链接：无</li><li>摘要：(1)：文本到 3D 生成方法可以通过文本描述生成高保真 3D 内容。然而，生成的物体是随机的，缺乏细粒度的控制。草图提供了一种引入这种细粒度控制的廉价方法。然而，由于草图的抽象性和模糊性，很难从这些草图中实现灵活的控制。(2)：过去的方法主要使用预训练的 2D 扩散模型来监督 3D 场景的优化，这些场景由神经辐射场 (NeRF) 表示。然而，这些方法通常需要大量的草图作为输入，并且生成的 3D 对象可能与草图不一致。(3)：本文提出了一种多视角草图引导的文本到 3D 生成框架（即 Sketch2NeRF），以将草图控制添加到 3D 生成中。具体来说，该方法利用预训练的 2D 扩散模型来监督 3D 场景的优化，该场景由神经辐射场 (NeRF) 表示。并提出了一种新颖的同步生成和重建方法来有效地优化 NeRF。(4)：在实验中，本文收集了两种多视角草图数据集来评估所提出的方法。结果表明，该方法能够合成具有细粒度草图控制的 3D 一致内容，同时对文本提示保持高保真度。广泛的结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。</li></ol><p><strong>Methods：</strong></p><ol><li><p><strong>3D表示：</strong>使用神经辐射场（NeRF）表示3D对象，NeRF是一种灵活且能够渲染逼真图像的方法。</p></li><li><p><strong>草图条件生成：</strong>使用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重。</p></li><li><p><strong>同步生成和重建优化：</strong>提出了一种同步生成和重建优化方法，该方法利用ControlNet和Stable Diffusion分别在草图的特定姿势和随机采样的姿势下生成真实图像，并使用NeRF渲染的图像作为重建目标，最小化生成图像和渲染图像之间的重建损失。</p></li><li><p><strong>优化：</strong>使用基于分数的蒸馏优化方法来优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。</p></li><li><p>结论：（1）：本文提出了一种新颖的多视角草图引导的文本到3D生成方法Sketch2NeRF，该方法可以生成与给定草图相似的逼真3D内容。具体来说，该方法利用预训练的2D草图条件扩散模型作为指导，迭代更新NeRF的权重，并提出了一种新的同步生成和重建优化方法来有效地优化NeRF。实验结果表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。（2）：创新点：</p></li><li>提出了一种新颖的多视角草图引导的文本到3D生成方法，该方法可以生成与给定草图相似的逼真3D内容。</li><li>提出了一种新的同步生成和重建优化方法来有效地优化NeRF，该方法可以有效地将草图条件生成与NeRF的优化相结合。性能：</li><li>在两个多视角草图数据集上进行的实验表明，该方法在草图相似性和文本对齐方面取得了最先进的性能。工作量：</li><li>该方法需要收集多视角草图数据集，并需要预训练2D草图条件扩散模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-432d996d35cef510a47b970f6a57f9ed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2b5a42bece9e656aff52a6fc20878da8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb3c2e84dae023cd921d28d348487b30.jpg" align="middle"></details><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 带注意力机制的分解 (NeRF-AD) 提出了一种新颖的说话人脸合成方法，通过音频注意机制将人脸分解为音频面孔和身份面孔，从而提高人脸合成的真实性和唇部同步效果。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-AD 提出了一种新的说话人脸合成方法，结合了神经辐射场 (NeRF) 和注意力机制，通过将人脸分解为音频面孔和身份面孔，大幅提升了生成人脸的真实性和唇部同步效果。</li></ul><ul><li>NeRF-AD 使用基于注意力的分解模块，利用语音相关的面部动作单元 (AU) 信息将人脸分解为音频面孔和身份面孔，有效地将音频与面部语音运动相关区域进行精确映射。</li></ul><ul><li>NeRF-AD 只将音频面孔与音频特征融合，从而精确地控制音频如何影响说话人脸。</li></ul><ul><li>NeRF-AD 利用 AU 信息来监督这两种模态的融合，提高了人脸合成的准确性和真实性。</li></ul><ul><li>大量的定量和定性实验表明，NeRF-AD 在生成逼真说话人脸视频方面优于现有最先进的方法，包括图像质量和唇部同步。</li></ul><ul><li>更详细的视频结果可以访问 <a href="https://xiaoxingliu02.github.io/NeRF-AD。">https://xiaoxingliu02.github.io/NeRF-AD。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：神经辐射场与基于注意力的分离的说话人面部合成（NERF-AD）</li><li>作者：Bi Chongke，Liu Xiaoxing，Liu Zhilei</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部分离</li><li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：None</li><li><p>摘要：（1）研究背景：说话人面部合成在多维信号处理和多媒体领域是一个热门的研究课题。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 带来了复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与语音运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。（2）过去的方法及其问题：一些现有的方法将 NeRF 的学习任务提前了一部分，并提出了一种通过具有基于注意力的分离的 NeRF（NeRF-AD）进行说话人面部合成的。具体来说，引入了一个基于注意力的分离模块，使用与语音相关的面部动作单元 (AU) 信息将面部分离为音频面部和身份面部。为了精确地调节音频如何影响说话人面部，我们只将音频面部与音频特征融合。此外，AU 信息还用于监督这两个模态的融合。（3）研究方法：为了减少 NeRF 的学习负担并提高面部渲染的准确性，我们分解说话人面部并为 NeRF 提供两个分解的精确条件。我们提出了一个基于注意力的分离模块，允许音频与与语音运动相关的面部区域精确融合。同时，我们采用一系列方法来监督整个过程。（4）方法性能：广泛的定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频方面优于最先进的方法，包括图像质量和唇形同步。</p></li><li><p>方法：(1)：提出了一种基于注意力的分离模块NeRF-AD，将面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。(2)：为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。(3)：采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。</p></li><li><p>结论：（1）：提出了一种基于注意力的分离模块NeRF-AD，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。（2）：创新点：提出了一种基于注意力的分离模块，将说话人面部分离为音频面部和身份面部，并只将音频面部与音频特征融合，以精确调节音频如何影响说话人面部。为了减少NeRF的学习负担并提高面部渲染的准确性，将说话人面部分解并为NeRF提供两个分解的精确条件。采用一系列方法来监督整个过程，包括使用与语音相关的面部动作单元(AU)信息监督音频面部和身份面部的融合，以及使用感知损失和对抗损失来监督NeRF的学习。性能：实验结果表明，NeRF-AD在图像质量和唇形同步方面优于最先进的方法。工作量：工作量较大，需要大量的数据和计算资源。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details><h2 id="HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs"><a href="#HG3-NeRF-Hierarchical-Geometric-Semantic-and-Photometric-Guided-Neural-Radiance-Fields-for-Sparse-View-Inputs" class="headerlink" title="HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs"></a>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided   Neural Radiance Fields for Sparse View Inputs</h2><p><strong>Authors:Zelin Gao, Weichen Dai, Yu Zhang</strong></p><p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs. </p><p><a href="http://arxiv.org/abs/2401.11711v1">PDF</a> 13 pages, 6 figures</p><p><strong>摘要</strong><br>层次几何、语义和光度引导 NeRF（HG3-NeRF）方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</p><p><strong>关键要点</strong></p><ul><li>HG3-NeRF 是一种新的方法，可以解决稀疏视图输入下 NeRF 的性能退化问题，并提高几何、语义内容和外观的一致性。</li><li>HG3-NeRF 提出了一种分层几何引导 (HGG) 方法，将运动结构 (SfM) 的附件（即稀疏深度先验）纳入场景表示中。</li><li>HGG 从局部到全局的几何区域对体积点进行采样，减轻了深度先验中固有偏差造成的错位。</li><li>HG3-NeRF 提出了一种分层语义引导 (HSG) 方法，学习从粗到细的语义内容，这对应于从粗到细的场景表示。</li><li>实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并实现了稀疏视图输入的高保真合成结果。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li><li>HG3-NeRF 方法能提高稀疏视图输入下场景表示的几何、语义内容和外观一致性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：HG3-NeRF：用于稀疏视图输入的分层几何、语义和光度引导的神经辐射场</li><li>作者：Zelin Gao, Weichen Dai, Yu Zhang</li><li>隶属机构：浙江大学控制科学与工程学院</li><li>关键词：神经辐射场、稀疏视图、几何引导、语义引导、光度引导</li><li>论文链接：https://arxiv.org/abs/2401.11711，Github 链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）因其从离散观测中学习场景表示以进行新颖视图合成而备受关注。然而，当面对稀疏视图输入时，NeRF 的性能会显著下降，从而限制了其进一步的适用性。（2）过去方法及问题：现有方法采用预训练方法和逐场景优化方法来解决稀疏视图输入的挑战。预训练方法在大型数据集上训练模型，然后在测试时对每个场景进行微调。然而，这种方法的泛化能力很大程度上依赖于数据集的质量，而且通过捕捉许多不同场景来获得必要的数据集过于昂贵。逐场景优化方法在每个场景上优化模型，但它们通常需要大量计算，并且可能难以收敛到良好的解。（3）研究方法：本文提出了一种新颖的方法 HG3-NeRF，可以解决上述限制并增强不同视图之间几何形状、语义内容和外观的一致性。HG3-NeRF 包括三个主要组件：分层几何引导（HGG）、分层语义引导（HSG）和光度引导。HGG 将结构从运动（SfM）的附加信息（即稀疏深度先验）纳入场景表示中。HSG 从不同分辨率的图像中学习粗到细的语义内容，这与粗到细的场景表示相对应。光度引导使用渲染方程来优化场景表示，以匹配输入视图的颜色和亮度。（4）方法性能：实验结果表明，HG3-NeRF 在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。这些结果支持了本文提出的方法的目标，即解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。</li></ol><p>Methods：（1）分层几何引导（HGG）：利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。HGG 方法指导神经辐射场学习密度和颜色的近似分布，这些分布来自深度先验确定的局部到全局的采样区域。（2）分层语义引导（HSG）：从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。HSG 使用 CLIP 编码器对渲染的图像和原始图像的特征向量进行编码，并计算粗到细的语义余弦相似性。（3）光度引导：使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。光度引导通过最小化渲染的图像和原始图像之间的外观均方误差来实现。</p><ol><li>结论：（1）：本文提出了一种分层几何、语义和光度引导的神经辐射场（HG3-NeRF）方法，可以解决稀疏视图输入的挑战并增强不同视图之间几何形状、语义内容和外观的一致性。（2）：创新点：</li><li>提出了一种分层几何引导（HGG）方法，利用来自结构运动（SfM）的稀疏深度先验，将几何一致性纳入场景表示中。</li><li>提出了一种分层语义引导（HSG）方法，从不同分辨率的图像中学习从粗到细的语义内容，这与从粗到细的场景表示相对应。</li><li>使用渲染方程优化场景表示，以匹配输入视图的颜色和亮度。性能：</li><li>在不同的标准基准上优于其他最先进的方法，并且在稀疏视图输入下实现了高保真合成结果。工作量：</li><li>需要估计相机位姿，并且稀疏视图输入会影响位姿估计的准确性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-56cd69227addb7c7e2e5ec9028bc8cb0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bb7c383a42f7306611645083f4d82eb9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-71514b137fee0e499428b6e4c393be26.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5dccc88a28d6fafb1f550b78be5145.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bab43cfc9ed715f6025ba1321b7acdc3.jpg" align="middle"></details><h2 id="IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field"><a href="#IPR-NeRF-Ownership-Verification-meets-Neural-Radiance-Field" class="headerlink" title="IPR-NeRF: Ownership Verification meets Neural Radiance Field"></a>IPR-NeRF: Ownership Verification meets Neural Radiance Field</h2><p><strong>Authors:Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</strong></p><p>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts. </p><p><a href="http://arxiv.org/abs/2401.09495v4">PDF</a> Error on result tabulation of state of the art method which might   cause misleading to readers</p><p><strong>Summary</strong><br>神经辐射场（NeRF）模型在计算机视觉领域备受关注，并产生了令人印象深刻的成果，由于其最先进的视觉质量，因此存在被剽窃者非法复制、再分发或滥用的风险。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种针对 NeRF 模型的黑盒和白盒设置的综合知识产权（IP）保护框架，称为 IPR-NeRF。</li><li>在黑盒设置中，引入了一种基于扩散的解决方案，通过两阶段优化过程嵌入和提取水印。</li><li>在白盒设置中，通过采用符号损失目标将指定数字签名嵌入 NeRF 模型的权重中。</li><li>大量实验表明，我们的方法不仅保持了 IPR-NeRF 模型的保真度（即渲染质量），而且与现有技术相比，它还对歧义攻击和去除攻击具有鲁棒性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：IPR-NERF：知识产权验证满足神经辐射场</li><li>作者：Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</li><li>单位：马来亚大学图像与信号处理中心（CISiP）</li><li>关键词：神经辐射场、知识产权保护、数字水印、数字签名</li><li>论文链接：arXiv:2401.09495v1[cs.CV]17Jan2024Github 链接：无</li><li><p>摘要：（1）研究背景：神经辐射场（NeRF）模型因其卓越的视觉质量和令人印象深刻的演示而在计算机视觉领域备受关注。然而，NeRF 模型也面临着知识产权保护的问题，剽窃者可能会非法复制、重新分发或滥用这些模型以获取经济利益或个人利益。（2）过去方法及其问题：目前针对神经网络的知识产权保护方案主要针对卷积神经网络（CNN）、生成对抗网络（GAN）和循环神经网络（RNN）。然而，这些方案在应用于 NeRF 模型时面临诸多挑战，例如 NeRF 模型的复杂结构、对数据和计算资源的要求较高以及缺乏有效的知识产权保护技术。（3）研究方法：本论文提出了一个综合的 NeRF 模型知识产权保护框架，称为 IPR-NERF。该框架包括黑盒和白盒两种设置。在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。（4）方法性能：实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。</p></li><li><p>方法：（1）：提出一个综合的NeRF模型知识产权保护框架IPR-NERF，包括黑盒和白盒两种设置。（2）：在黑盒设置中，引入一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。（3）：在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入NeRF模型的权重中。</p></li><li><p>结论：（1）：本工作提出了一种全面的、鲁棒的 NeRF-IPR 保护方案，包括黑盒和白盒两种场景。全面的实验结果表明了其在抵抗嵌入水印的模糊性和去除攻击方面的有效性，同时保持了渲染性能。然而，它在计算能力和对覆盖攻击的黑盒保护方面存在局限性，当攻击者拥有受保护模型的详细信息时。未来的研究将集中在改进这些方面。本研究为 NeRF 模型开发者和研究人员提供了极大的价值，提供了一种保护其知识产权并获得市场竞争优势的方法，考虑到开发高性能 NeRF 模型所需的巨大资源。加强 NeRF 模型对 IPR 侵权行为的抵抗具有广泛的社会效益，包括防止剽窃、确保在动态市场竞争中的竞争优势以及减少浪费诉讼案件的负担。（2）：创新点：</p></li><li>提出了一种综合的 NeRF 模型知识产权保护框架 IPR-NERF，包括黑盒和白盒两种设置。</li><li>在黑盒设置中，引入了一个基于扩散的解决方案来嵌入和提取水印，通过一个两阶段的优化过程实现。</li><li>在白盒设置中，通过采用符号损失目标函数，将指定数字签名嵌入 NeRF 模型的权重中。性能：</li><li>实验结果表明，IPR-NERF 模型不仅保持了渲染质量，而且在面对模糊性和去除攻击时也具有鲁棒性，优于现有技术。工作量：</li><li>IPR-NERF 模型的计算成本较高，尤其是对于大型数据集和复杂场景。</li><li>在黑盒设置中，嵌入和提取水印的过程可能需要大量的时间和计算资源。</li><li>在白盒设置中，需要修改 NeRF 模型的训练过程以嵌入数字签名，这可能会增加训练时间和复杂性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7702dd0580aeb20d2469586499df517d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b6cd7f525efd45ad04614d4ae868c5ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cd4e10da5a013a99ebc46d33f1e102a8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed46804675ae115b408ec3a1b30d40dd.jpg" align="middle"></details><h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Process" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas Guibas</strong></p><p>Neural radiance fields (NeRFs) have gained popularity across various applications. However, they face challenges in the sparse view setting, lacking sufficient constraints from volume rendering. Reconstructing and understanding a 3D scene from sparse and unconstrained cameras is a long-standing problem in classical computer vision with diverse applications. While recent works have explored NeRFs in sparse, unconstrained view scenarios, their focus has been primarily on enhancing reconstruction and novel view synthesis. Our approach takes a broader perspective by posing the question: “from where has each point been seen?” — which gates how well we can understand and reconstruct it. In other words, we aim to determine the origin or provenance of each 3D point and its associated information under sparse, unconstrained views. We introduce ProvNeRF, a model that enriches a traditional NeRF representation by incorporating per-point provenance, modeling likely source locations for each point. We achieve this by extending implicit maximum likelihood estimation (IMLE) for stochastic processes. Notably, our method is compatible with any pre-trained NeRF model and the associated training camera poses. We demonstrate that modeling per-point provenance offers several advantages, including uncertainty estimation, criteria-based view selection, and improved novel view synthesis, compared to state-of-the-art methods. Please visit our project page at <a href="https://provnerf.github.io">https://provnerf.github.io</a> </p><p><a href="http://arxiv.org/abs/2401.08140v2">PDF</a> </p><p><strong>摘要</strong><br>针对稀疏无约束视点场景下神经辐射场（NeRF）模型的局限性，本文旨在重构和理解三维场景中每个点的来源信息，并提出了 ProvNeRF 模型来实现这一目标。</p><p><strong>要点</strong></p><ul><li>ProvNeRF 模型能够通过引入每个点可能的来源位置，来丰富传统的 NeRF 模型。</li><li>ProvNeRF 模型与任何预训练的 NeRF 模型及其相关的训练相机位姿兼容。</li><li>ProvNeRF 模型可以对每个点的不确定性进行估计。</li><li>ProvNeRF 模型可以根据指定的标准，选择合适的视角来进行场景重建。</li><li>ProvNeRF 模型可以改进场景的新视角合成结果。</li><li>ProvNeRF 模型的更多信息可以在项目主页 <a href="https://provnerf.github.io">https://provnerf.github.io</a> 查看。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：ProvNeRF：将 NeRF 中的逐点出处建模为随机过程</li><p></p><p></p><li>作者：George Kiyohiro Nakayama、Mikaela Angelina Uy、Yang You、Ke Li、Leonidas Guibas</li><p></p><p></p><li>隶属机构：斯坦福大学</li><p></p><p></p><li>关键词：神经辐射场、稀疏视图、出处建模、不确定性估计、基于标准的视点优化、新颖视图合成</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2401.08140Github 链接：无</li><p></p><p></p><li>摘要：（1）研究背景：神经辐射场 (NeRF) 在各种应用中越来越受欢迎，但它们在稀疏视图方案中面临挑战，因为仅靠体积渲染无法提供足够的约束。（2）过去方法：过去的方法主要集中在增强重建和新颖视图合成上，但忽略了如何从更全面的角度理解场景，例如不确定性估计、基于标准的视点选择和改进的新颖视图合成。（3）研究方法：我们提出 ProvNeRF，这是一种通过结合逐点出处来丰富传统 NeRF 表示的模型，对每个点建模可能的源位置。我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点。（4）方法性能：我们的方法在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法，这表明建模逐点出处可以提供几个优势。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods</strong>：**</p><p>（1）：我们将神经辐射场 (NeRF) 表示扩展为包含每个点的出处，即每个点的来源或从何处看到它。</p><p>（2）：我们使用随机过程对每个点的出处进行建模，该随机过程由坐标 x∈R3 索引，其在 x 处的边际分布编码了 x 处的出处。</p><p>（3）：我们通过扩展随机过程的隐式最大似然估计 (IMLE) 来实现这一点，该估计将潜在随机变量的变换学习为数据分布，其中每个数据样本都是一个标量或向量。</p><p>（4）：我们提出 ProvNeRF，它通过扩展隐式概率模型（特别是 IMLE）来处理随机过程，从而将每个点的出处建模为随机过程。</p><p>（5）：ProvNeRF 学习一个确定性变换 Hθ：Rb→R+×D3，该变换将每个潜在随机函数样本 Z∼Z 映射到一个函数 Dθ∼Dθ。</p><p>（6）：为了优化 Dθ，我们扩展 IMLE 来对随机过程的分布进行建模。我们将 Eq.3 调整到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><ol><li>结论：（1）：本工作提出 ProvNeRF，通过扩展随机过程的 IMLE 来增强传统 NeRF 表示，从而将每个点的出处建模为随机过程。ProvNeRF 可轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。我们展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。（2）：创新点：</li><li>提出 ProvNeRF，一种通过扩展随机过程的 IMLE 来增强传统 NeRF 表示的模型，从而将每个点的出处建模为随机过程。</li><li>证明了 ProvNeRF 可以轻松应用于任何预训练的 NeRF 模型以及相关的训练相机位姿。</li><li>展示了在各种下游应用中建模逐点出处的优势，包括不确定性建模、基于标准的视点选择以及与现有最先进方法相比改进的新颖视图合成。</li></ol><p>性能：- 在不确定性估计、基于标准的视点选择和改进的新颖视图合成方面优于最先进的方法。</p><p>工作量：- 需要扩展随机过程的 IMLE 来对随机过程的分布进行建模。- 需要调整 Eq.3 到函数空间，并证明它等价于在每个点 x 处对经验样本 ˆD(x)∼ˆD(x) 和模型样本 Dθ(x)∼Dθ(x) 进行逐点匹配。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f48885cf9ef1b2a677c258f6b1e9a2a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d72d125185075e757ca6e7284c2ace68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a582ca9b91a20a6a1c1593166a2d8401.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d26582d170597ef79c1a5e15500eaa42.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-01-30  Divide and Conquer Rethinking the Training Paradigm of Neural Radiance   Fields</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/3DGS/</id>
    <published>2024-01-30T11:06:29.000Z</published>
    <updated>2024-01-30T12:00:56.153Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction"><a href="#EndoGaussians-Single-View-Dynamic-Gaussian-Splatting-for-Deformable-Endoscopic-Tissues-Reconstruction" class="headerlink" title="EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction"></a>EndoGaussians: Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</h2><p><strong>Authors:Yangsen Chen, Hao Wang</strong></p><p>The accurate 3D reconstruction of deformable soft body tissues from endoscopic videos is a pivotal challenge in medical applications such as VR surgery and medical image analysis. Existing methods often struggle with accuracy and the ambiguity of hallucinated tissue parts, limiting their practical utility. In this work, we introduce EndoGaussians, a novel approach that employs Gaussian Splatting for dynamic endoscopic 3D reconstruction. This method marks the first use of Gaussian Splatting in this context, overcoming the limitations of previous NeRF-based techniques. Our method sets new state-of-the-art standards, as demonstrated by quantitative assessments on various endoscope datasets. These advancements make our method a promising tool for medical professionals, offering more reliable and efficient 3D reconstructions for practical applications in the medical field. </p><p><a href="http://arxiv.org/abs/2401.13352v1">PDF</a> </p><p><strong>摘要</strong><br>高斯散点结合神经辐射场，实现动态内窥镜 3D 重建新方法。</p><p><strong>要点</strong></p><ul><li>EndoGaussians 是一个新的方法，它利用高斯散点进行动态内窥镜 3D 重建。</li><li>这种方法是首次在该背景下使用高斯散点，克服了以前基于 NeRF 技术的限制。</li><li>该方法在各种内窥镜数据集上进行定量评估，树立了新的最先进标准。</li><li>这些进步使该方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供了更可靠、更高效的 3D 重建。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussians：单视动态高斯体素重建</li><li>作者：Yangsen Chen, Hao Wang</li><li>隶属机构：香港科技大学（广州）</li><li>关键词：3D 重建、高斯体素重建、机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.13352</li><li><p>摘要：(1) 研究背景：准确地从内窥镜视频中重建可变形软体组织的 3D 模型对于 VR 手术和医学图像分析等医疗应用至关重要。现有方法通常在准确性和产生的组织部分的模棱两可方面存在问题，限制了其实际效用。(2) 过往方法：以往的一些工作尝试使用深度估计、SLAM、稀疏变形场和神经辐射场等方法来解决这个问题，但这些方法要么假设场景是静态的，要么假设手术工具不存在，从而限制了它们在实际场景中的实用性。(3) 研究方法：为了进一步提高静态单视 RGBD 设置下软体组织的 3D 重建的准确性，并提高 3D 重建的可靠性和可信度，我们提出了 Endogaussians，该方法利用高斯体素重建作为重建方法。(4) 方法性能：我们的方法在 PSNR、SSIM、LPIPS 等多项定量评估中取得了最先进的结果，并且重建速度更快。这些进步使我们的方法成为医疗专业人员的有前途的工具，为医疗领域的实际应用提供更可靠和高效的 3D 重建。</p></li><li><p>方法：(1): 本文提出了一种名为 Endogaussians 的方法，用于从单目动态 RGBD 设置中重建可变形软体组织的 3D 模型。(2): 该方法使用高斯体素重建作为重建方法，可以有效地处理软体组织的变形。(3): 为了提高重建的准确性，本文提出了一种新的体素融合策略，该策略可以有效地融合来自不同帧的数据。(4): 此外，本文还提出了一种新的体素分割算法，该算法可以有效地将软体组织分割成不同的部分。(5): 最后，本文提出了一种新的体素渲染算法，该算法可以生成逼真的软体组织模型。</p></li><li><p>结论：（1）：EndoGaussians方法可以有效地从单目动态RGBD设置中重建可变形软体组织的3D模型，具有较高的准确性和可靠性，在医疗领域具有广阔的应用前景。（2）：创新点：</p></li><li>提出了一种新的高斯体素重建方法，可以有效地处理软体组织的变形。</li><li>提出了一种新的体素融合策略，可以有效地融合来自不同帧的数据。</li><li>提出了一种新的体素分割算法，可以有效地将软体组织分割成不同的部分。</li><li>提出了一种新的体素渲染算法，可以生成逼真的软体组织模型。性能：</li><li>在PSNR、SSIM、LPIPS等多项定量评估中取得了最先进的结果。</li><li>重建速度更快。工作量：</li><li>该方法需要大量的计算资源。</li><li>该方法需要大量的标注数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-049a97b3607a44946b481425f04f7d64.jpg" align="middle"></details><h2 id="EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction"><a href="#EndoGaussian-Gaussian-Splatting-for-Deformable-Surgical-Scene-Reconstruction" class="headerlink" title="EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction"></a>EndoGaussian: Gaussian Splatting for Deformable Surgical Scene   Reconstruction</h2><p><strong>Authors:Yifan Liu, Chenxin Li, Chen Yang, Yixuan Yuan</strong></p><p>Reconstructing deformable tissues from endoscopic stereo videos is essential in many downstream surgical applications. However, existing methods suffer from slow inference speed, which greatly limits their practical use. In this paper, we introduce EndoGaussian, a real-time surgical scene reconstruction framework that builds on 3D Gaussian Splatting. Our framework represents dynamic surgical scenes as canonical Gaussians and a time-dependent deformation field, which predicts Gaussian deformations at novel timestamps. Due to the efficient Gaussian representation and parallel rendering pipeline, our framework significantly accelerates the rendering speed compared to previous methods. In addition, we design the deformation field as the combination of a lightweight encoding voxel and an extremely tiny MLP, allowing for efficient Gaussian tracking with a minor rendering burden. Furthermore, we design a holistic Gaussian initialization method to fully leverage the surface distribution prior, achieved by searching informative points from across the input image sequence. Experiments on public endoscope datasets demonstrate that our method can achieve real-time rendering speed (195 FPS real-time, 100$\times$ gain) while maintaining the state-of-the-art reconstruction quality (35.925 PSNR) and the fastest training speed (within 2 min/scene), showing significant promise for intraoperative surgery applications. Code is available at: \url{<a href="https://yifliu3.github.io/EndoGaussian/}">https://yifliu3.github.io/EndoGaussian/}</a>. </p><p><a href="http://arxiv.org/abs/2401.12561v1">PDF</a> </p><p><strong>Summary</strong><br>3D高斯渲染框架实现了实时内窥镜手术场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为EndoGaussian的实时手术场景重建框架，它是建立在3D高斯点阵的基础上的。</li><li>使用高斯表示和并行渲染管道，显著提高了渲染速度。</li><li>将变形场设计为轻量级编码体素和极小MLP的组合，实现了高效的高斯跟踪，渲染负担较小。</li><li>设计了一种整体的高斯初始化方法，充分利用了表面分布先验，通过搜索输入图像序列中的信息点来实现。</li><li>公共内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS实时，100倍收益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（在2分钟/场景以内），显示出对术中手术应用的重大前景。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：EndoGaussian：用于可变形手术场景重建的高斯点云</li><li>作者：Yifan Liu<em>, Chenxin Li</em>, Chen Yang 和 Yixuan Yuan</li><li>隶属机构：香港中文大学</li><li>关键词：三维重建 · 高斯点云 · 机器人手术</li><li>论文链接：https://arxiv.org/abs/2401.12561   Github 代码链接：https://yifliu3.github.io/EndoGaussian/</li><li><p>摘要：（1）研究背景：从内窥镜立体视频中重建可变形组织对于许多下游手术应用至关重要。然而，现有方法的推理速度慢，极大地限制了它们的实际使用。（2）过去方法及其问题：现有方法的问题在于推理速度慢，这使得它们在实际应用中受到限制。（3）研究方法：该论文提出了一种基于 3D 高斯点云的实时手术场景重建框架 EndoGaussian。该框架将动态手术场景表示为规范高斯点云和时间相关的变形场，该变形场可以预测新时间戳下的高斯变形。由于高效的高斯表示和并行渲染管道，该框架与以往方法相比，显著地提高了渲染速度。此外，该论文将变形场设计为轻量级编码体素和极小型的 MLP 的组合，从而实现高效的高斯跟踪，且渲染负担很小。此外，该论文设计了一种整体的高斯初始化方法，以充分利用表面分布先验，该方法通过从输入图像序列中搜索信息点来实现。（4）方法性能：在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195 FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925 PSNR）和最快的训练速度（每个场景 2 分钟以内），显示出对术中手术应用的重大前景。</p></li><li><p>方法：（1）EndoGaussian框架概述：该框架由高斯点云初始化、高斯跟踪和高斯渲染三个模块组成。（2）高斯点云初始化：从输入图像序列中搜索信息点，通过高斯混合模型估计点云参数，并通过表面分布先验优化点云位置。（3）高斯跟踪：将变形场设计为轻量级编码体素和极小型的MLP的组合，通过将当前时间戳的高斯点云变形到新时间戳，实现高效的高斯跟踪。（4）高斯渲染：利用高斯点云的几何特性和并行渲染管道，实现高效的渲染。（5）训练细节：使用Adam优化器，学习率为1e-4，批大小为8，训练200个周期。</p></li><li><p>结论：（1）：本工作提出了一种实时且高质量的 4D 重建框架，用于动态手术场景重建。通过利用基于体素的高斯跟踪和整体高斯初始化，我们能够处理组织变形和非平凡的高斯初始化问题。全面的实验表明，我们的 EndoGaussian 可以实现最先进的重建质量和实时的渲染速度，比以前的方法快 100 倍以上。我们希望新兴的基于高斯斑点的重建技术能够为机器人手术场景理解提供新的途径，并增强各种下游临床任务，尤其是术中应用。（2）：创新点：</p></li><li>基于高斯点云的实时手术场景重建框架。</li><li>体素编码的高斯跟踪，实现了高效的高斯跟踪。</li><li>整体高斯初始化方法，充分利用表面分布先验。性能：</li><li>在公开内窥镜数据集上的实验表明，该方法可以实现实时渲染速度（195FPS 实时，100 倍增益），同时保持最先进的重建质量（35.925PSNR）和最快的训练速度（每个场景 2 分钟以内）。工作量：</li><li>论文的代码和数据已经开源，可以方便地进行复现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0b9bca825762ac8e0bbad3078a233ed1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e1d91551398571ef4d862b170f54e4fc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d93c7e9f9dfadf417d2add6f22082d7e.jpg" align="middle"></details><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong><br>动态高斯溅射用于可变形内窥镜组织重建。</p><p><strong>要点</strong></p><ul><li>EndoGS 利用高斯溅射进行可变形内窥镜组织重建。</li><li>该方法结合变形场以处理动态场景。</li><li>深度引导监督用于优化具有单个视点的 3D 目标。</li><li>时空权重掩码可减轻工具遮挡。</li><li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。</li><li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 实现卓越的渲染质量。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯斑点可变形内窥镜组织重建</li><li>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</li><li>第一作者单位：香港大学</li><li>关键词：高斯斑点·机器人手术·三维重建</li><li>论文链接：https://arxiv.org/abs/2401.11535, Github 链接：https://github.com/HKU-MedAI/EndoGS</li><li>摘要：(1)：研究背景：手术三维重建是机器人手术研究的一个关键领域，最近的工作采用动态辐射场实现从单视角视频中可变形组织的三维重建。然而，这些方法通常存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。(2)：过去的方法：早期尝试采用深度估计在内窥镜重建中取得了巨大成功，但这些方法仍然难以产生逼真的三维重建，原因有两个关键问题。首先，非刚性变形有时会导致较大的运动，这需要实际动态场景重建，这阻碍了这些技术的适应。其次，单视角视频中存在遮挡，导致学习受影响部分时信息有限，产生困难。虽然一些框架结合了工具遮罩、立体深度估计和稀疏翘曲场用于单视角三维重建，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。(3)：研究方法：受最近流行的三维表示方法三维高斯斑点启发，我们提出了 EndoGS，将高斯斑点应用于可变形内窥镜组织重建。具体来说，我们的方法结合了变形场来处理动态场景，深度引导监督来优化具有单一视点的三维目标，以及时空权重掩码来减轻遮挡。(4)：方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 实现了更高的渲染质量。</li></ol><p>Methods:(1): 我们提出了一种称为 EndoGS 的方法，它利用 3D-GS 的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建 3D 外科场景。(2): 我们首先介绍了 3D-GS 的预备知识，然后展示了使用动态版本的 3D-GS 对可变形组织进行建模，该版本采用轻量级 MLP 来表示动态场。最后，我们介绍了使用工具掩码和深度图对高斯飞溅进行训练优化的过程。(3): 我们使用六个正交特征平面来编码空间和时间信息，并使用单个 MLP 来更新高斯属性并解码位置、比例因子、旋转因子、球谐系数和不透明度的变形。(4): 我们结合工具掩码和深度图来训练 EndoGS，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。</p><ol><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><p>具体内容如下：</p><ol><li>结论：（1）：本文提出了一种基于高斯斑点可变形内窥镜组织重建的方法，该方法可以从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法具有更高的渲染质量。（2）：创新点：</li><li>提出了一种新的方法EndoGS，利用3D-GS的可变形变体从单视角视频、估计的深度图和标记的工具掩码中重建3D外科场景。</li><li>使用动态版本的3D-GS对可变形组织进行建模，该版本采用轻量级MLP来表示动态场。</li><li>结合工具掩码和深度图对高斯飞溅进行训练优化，以解决视频中工具遮挡的挑战，并使用时空重要性采样策略来指示与遮挡问题相关的关键区域。性能：</li><li>在达芬奇机器人手术视频上的实验表明，EndoGS实现了更高的渲染质量。工作量：</li><li>该方法需要预先训练3D-GS模型，并对每个新场景进行优化。</li><li>优化过程需要一定的时间，具体取决于场景的复杂性和数据量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details><h2 id="GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting"><a href="#GaussianBody-Clothed-Human-Reconstruction-via-3d-Gaussian-Splatting" class="headerlink" title="GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting"></a>GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting</h2><p><strong>Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen</strong></p><p>In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. </p><p><a href="http://arxiv.org/abs/2401.09720v2">PDF</a> </p><p><strong>Summary</strong><br>优化动态穿衣人体重建方法，引入物理先验和规范化变换，实现高精度照片级新视角渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出了一种新的穿衣人體重建方法 GaussianBody，基於 3D 高斯 Splatting。</li><li>3D 高斯 Splatting 最近在訓練時間和渲染質量方面表現出了很好的性能。</li><li>應用靜態 3D 高斯 Splatting 模型於動態人體重建問題時，會因複雜的非剛性變形和豐富的衣物細節而遇到挑戰。</li><li>提出明確的姿勢引導變形，以關聯規範空間和觀測空間中的動態高斯。</li><li>引入基於物理的先驗和正則化變換，以減少兩個空間之間的歧義。</li><li>提出姿勢精煉策略，以更新姿勢回歸，以補償不準確的初始估計。</li><li>提出分拆比例機制，以增強回歸點雲的密度。</li><li>實驗證明，該方法可實現最先進照片級的新視圖渲染結果，同時具有高質量的動態穿衣人體細節和明確的幾何重建。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：高斯体：基于 3D 高斯散布的着装人体重建</li><li>作者：李梦添、姚胜祥、谢志峰、陈可宇</li><li>隶属单位：上海大学</li><li>关键词：3D 高斯散布、着装人体重建、单目视频、神经辐射场</li><li>论文链接：https://arxiv.org/abs/2401.09720，Github 代码链接：无</li><li>摘要：（1）研究背景：创建高保真着装人体模型在虚拟现实、远程临场和电影制作中具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工操作，这使得它们既耗时又昂贵，从而限制了新手用户的可扩展性。近年来，人们越来越关注从单个 RGB 图像或单目视频中自动重建着装人体模型。（2）过去的方法及其问题：网格模型方法最初被引入，通过回归 SCAPE、SMPL、SMPL-X 和 STAR 等参数模型来重构人体形状。虽然它们可以实现快速且稳健的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。添加顶点偏移量成为这种情况下的一种增强解决方案。然而，其表示能力仍然受到网格分辨率的严格限制，并且通常在宽松服装的情况下会失败。（3）本文提出的研究方法：为了克服显式网格模型的局限性，本文提出了一种基于 3D 高斯散布的新颖着装人体重建方法 GaussianBody。与代价高昂的神经辐射场模型相比，3D 高斯散布最近在训练时间和渲染质量方面表现出优异的性能。然而，将静态 3D 高斯散布模型应用于动态人体重建问题由于复杂的非刚性变形和丰富的服装细节而变得非常困难。为了应对这些挑战，本文的方法考虑了显式姿势引导的变形，将动态高斯体与规范空间和观察空间相关联，引入具有正则化变换的基于物理的先验有助于减轻这两个空间之间的歧义。在训练过程中，本文进一步提出了一种姿势细化策略，以更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂尺度机制来增强回归点云的密度。（4）方法的应用任务和性能：实验验证了本文的方法可以实现最先进的逼真新视图渲染结果，为动态着装人体提供高质量的细节，以及显式几何重建。这些性能可以支持他们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本文提出了一种基于 3D 高斯散布的着装人体重建方法 GaussianBody，该方法能够从单目视频中重建动态的着装人体模型，并具有逼真的新视图渲染结果和高质量的细节。（2）：创新点：</li><li>将 3D 高斯散布表示扩展到着装人体重建中，并考虑了显式姿势引导的变形，以解决动态高斯体与规范空间和观察空间之间的歧义问题。</li><li>提出了一种基于物理的先验来正则化规范空间的高斯体，以减轻观察空间和规范空间之间的过度旋转问题。</li><li>提出了一种姿势细化策略和分裂尺度机制，以增强重建点云的质量和鲁棒性。性能：</li><li>该方法在图像质量指标上与基线和其他方法相当，证明了其具有竞争力的性能和相对较快的训练速度。</li><li>该方法能够使用更高分辨率的图像进行训练。工作量：</li><li>该方法的训练时间比一些最先进的方法更长。</li><li>该方法需要大量的数据来进行训练。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-03cb35c9ffdf24e162bbcf10081d440a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2032721a60695f2d41ac96f75dec65a2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4877b53e7d23cf29d6e9a1a57a3155ec.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details><h2 id="Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities"><a href="#Forging-Vision-Foundation-Models-for-Autonomous-Driving-Challenges-Methodologies-and-Opportunities" class="headerlink" title="Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities"></a>Forging Vision Foundation Models for Autonomous Driving: Challenges,   Methodologies, and Opportunities</h2><p><strong>Authors:Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, Zhen Li, Lihui Jiang, Wei Zhang, Hongbo Zhang, Dengxin Dai, Bingbing Liu</strong></p><p>The rise of large foundation models, trained on extensive datasets, is revolutionizing the field of AI. Models such as SAM, DALL-E2, and GPT-4 showcase their adaptability by extracting intricate patterns and performing effectively across diverse tasks, thereby serving as potent building blocks for a wide range of AI applications. Autonomous driving, a vibrant front in AI applications, remains challenged by the lack of dedicated vision foundation models (VFMs). The scarcity of comprehensive training data, the need for multi-sensor integration, and the diverse task-specific architectures pose significant obstacles to the development of VFMs in this field. This paper delves into the critical challenge of forging VFMs tailored specifically for autonomous driving, while also outlining future directions. Through a systematic analysis of over 250 papers, we dissect essential techniques for VFM development, including data preparation, pre-training strategies, and downstream task adaptation. Moreover, we explore key advancements such as NeRF, diffusion models, 3D Gaussian Splatting, and world models, presenting a comprehensive roadmap for future research. To empower researchers, we have built and maintained <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a>, an open-access repository constantly updated with the latest advancements in forging VFMs for autonomous driving. </p><p><a href="http://arxiv.org/abs/2401.08045v1">PDF</a> Github Repo: <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a></p><p><strong>摘要</strong><br>智能汽车专属视觉基础模型的构建挑战及其未来发展机遇。</p><p><strong>要点</strong></p><ul><li>数据准备、预训练策略和下游任务适配是 VFM 开发的关键技术。</li><li>生成神经辐射场 (NeRF)，扩散模型，3D 高斯分布（3DGS）和世界模型等技术的进步为未来的研究提出了路线图。</li><li>开源项目 <a href="https://github.com/zhanghm1995/Forge_VFM4AD">https://github.com/zhanghm1995/Forge_VFM4AD</a> 将不断更新，以赋能研究人员。</li><li>自动驾驶中的 VFM 缺乏专用数据和多传感器集成，导致任务特定架构的多样性成为 VFM 发展的障碍。</li><li>视觉基础模型 (VFM) 在自动驾驶中至关重要，但其发展面临着诸多挑战。</li><li>开发专用于自动驾驶的 VFM 是当前的紧迫挑战。</li><li>建议从数据准备、预训练以及下游任务适配等方面入手，并探索 NeRF、扩散模型等新技术，以推进 VFM 的发展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：构建自动驾驶视觉基础模型：挑战、方法和机遇</li><li>作者：徐岩，张海明，蔡应杰，郭敬明，邱维超，高斌，周凯强，赵越，金欢，高建涛，李振，蒋立辉，张伟，张宏波，戴登心，刘冰冰</li><li>第一作者单位：华为诺亚方舟实验室</li><li>关键词：视觉基础模型，数据生成，自监督训练，自动驾驶，文献综述</li><li>论文链接：https://arxiv.org/abs/2401.08045   Github 代码链接：无</li><li><p>摘要：（1）研究背景：随着自动驾驶技术的快速发展，传统自动驾驶感知系统采用模块化架构，针对特定任务使用专用算法，但这种方法往往导致输出不一致，限制了系统处理长尾情况的能力。近年来，大型基础模型在自然语言处理领域取得了巨大成功，展现出强大的适应性和有效性，为构建自动驾驶视觉基础模型提供了新的思路。（2）过去的方法及其问题：以往的方法主要集中于针对特定任务训练深度神经网络，但这种方法存在以下问题：1. 忽视了数据之间的关系，导致输出不一致；2. 难以处理长尾情况；3. 无法有效利用大量未标记数据。（3）提出的研究方法：本文提出了一种构建自动驾驶视觉基础模型的方法，该方法主要包括以下几个步骤：1. 数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；2. 预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；3. 下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。（4）方法在任务中的表现及性能：该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。这些结果表明，该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。</p></li><li><p>方法：（1）数据准备：收集和预处理自动驾驶相关的数据，包括图像、激光雷达点云、语义分割标签等；（2）预训练：使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征；（3）下游任务自适应：将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。</p></li><li><p>结论：（1）：本文针对自动驾驶视觉基础模型的构建提出了系统的方法，并取得了较好的性能。该方法为自动驾驶视觉基础模型的构建提供了新的思路，有望推动自动驾驶技术的发展。（2）：创新点：</p></li><li>提出了一种构建自动驾驶视觉基础模型的方法，该方法包括数据准备、预训练和下游任务自适应三个步骤。</li><li>使用自监督学习方法对基础模型进行预训练，使其能够从数据中提取有用的特征。</li><li>将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务。性能：</li><li>该方法在自动驾驶相关任务上取得了较好的性能，例如目标检测、语义分割和深度估计等。</li><li>该方法能够有效地从数据中提取有用的特征，并将其应用于下游任务。工作量：</li><li>该方法需要收集和预处理大量的数据。</li><li>该方法需要使用自监督学习方法对基础模型进行预训练，这需要较大的计算资源。</li><li>该方法需要将预训练好的基础模型应用于下游任务，并通过微调使其适应特定任务，这需要较多的工程工作。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-7ce70a9a128d8a3669098fd6808591bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b29768228c4fd656077c66549ec08984.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f7ea3a2551a65a42514ea6e5555124cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-66561a69f615f893c246615fba473e10.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-01-30  EndoGaussians Single View Dynamic Gaussian Splatting for Deformable   Endoscopic Tissues Reconstruction</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/Talking%20Head%20Generation/</id>
    <published>2024-01-30T10:56:54.000Z</published>
    <updated>2024-01-30T10:56:54.811Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis"><a href="#NeRF-AD-Neural-Radiance-Field-with-Attention-based-Disentanglement-for-Talking-Face-Synthesis" class="headerlink" title="NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis"></a>NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</h2><p><strong>Authors:Chongke Bi, Xiaoxing Liu, Zhilei Liu</strong></p><p>Talking face synthesis driven by audio is one of the current research hotspots in the fields of multidimensional signal processing and multimedia. Neural Radiance Field (NeRF) has recently been brought to this research field in order to enhance the realism and 3D effect of the generated faces. However, most existing NeRF-based methods either burden NeRF with complex learning tasks while lacking methods for supervised multimodal feature fusion, or cannot precisely map audio to the facial region related to speech movements. These reasons ultimately result in existing methods generating inaccurate lip shapes. This paper moves a portion of NeRF learning tasks ahead and proposes a talking face synthesis method via NeRF with attention-based disentanglement (NeRF-AD). In particular, an Attention-based Disentanglement module is introduced to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. To precisely regulate how audio affects the talking face, we only fuse the Audio-face with audio feature. In addition, AU information is also utilized to supervise the fusion of these two modalities. Extensive qualitative and quantitative experiments demonstrate that our NeRF-AD outperforms state-of-the-art methods in generating realistic talking face videos, including image quality and lip synchronization. To view video results, please refer to <a href="https://xiaoxingliu02.github.io/NeRF-AD">https://xiaoxingliu02.github.io/NeRF-AD</a>. </p><p><a href="http://arxiv.org/abs/2401.12568v1">PDF</a> Accepted by ICASSP 2024</p><p><strong>Summary</strong><br>用注意力机制分解神经辐射场，用于音频驱动的说话人头部生成。</p><p><strong>Key Takeaways</strong></p><ul><li>神经辐射场（NeRF）已被引入到谈话面部合成的研究领域，以增强生成的面的逼真性和 3D 效果。</li><li>现有的 NeRF 方法要么给 NeRF 带来了复杂的学习任务，而缺乏监督式多模态特征融合的方法，或者无法将音频精确映射到与语音运动相关的面部区域。</li><li>提出了一种基于 NeRF 的注意力机制分解的说话人头部合成方法（NeRF-AD）。</li><li>引入了一个基于注意力的分解模块，使用与语音相关的面部动作单元 (AU) 信息将人脸分解成音频面部和身份面部。</li><li>仅将音频面部与音频特征融合，以精确地调节音频对说话人面部的影响。</li><li>将 AU 信息也用于监督这两种模态的融合。</li><li>定性和定量实验表明，NeRF-AD 在生成逼真的说话人头部视频方面优于最先进的方法，包括图像质量和唇形同步。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NeRF-AD：基于注意力机制分解的神经辐射场说话人面部合成</li><li>作者：Chongke Bi，Xiaoxing Liu，Zhilei Liu</li><li>单位：天津大学智能与计算学院</li><li>关键词：说话人面部合成，神经辐射场，面部分解</li><li>论文链接：https://arxiv.org/abs/2401.12568，Github 链接：无</li><li><p>摘要：（1）研究背景：说话人面部合成是多维信号处理和多媒体领域的研究热点之一。神经辐射场（NeRF）最近被引入该研究领域，以增强生成面部的真实感和 3D 效果。然而，大多数现有的基于 NeRF 的方法要么给 NeRF 增加复杂的学习任务，同时缺乏监督式多模态特征融合的方法，要么无法将音频精确映射到与说话运动相关的面部区域。这些原因最终导致现有方法生成的唇形不准确。（2）过去方法及其问题：一些方法将 NeRF 的学习任务提前一部分，并提出了一种通过 NeRF 进行说话人面部合成的基于注意力机制分解的方法（NeRF-AD）。具体来说，引入了一个基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。为了精确地调节音频如何影响说话人的面部，我们只将音频面部与音频特征融合。此外，还利用 AU 信息来监督这两个模态的融合。大量定性和定量实验表明，我们的 NeRF-AD 在生成逼真的说话人面部视频（包括图像质量和唇形同步）方面优于最先进的方法。（3）提出的研究方法：我们设计了一个基于注意力的分解模块，利用 AU 指导注意力模型生成与说话运动相关的面部区域的掩码。通过利用这些掩码，我们可以有效地将输入面部分解为不同的组成部分：音频面部和身份面部。音频面部表示与说话运动相关的面部区域，与音频特征融合，而身份面部表示与说话人身份相关的面部区域。在这种情况下，音频特征只影响音频面部，从而对生成的说话人面部提供精确的控制。随后，我们提出了一个条件 NeRF，以将融合的音频面部特征和身份面部特征作为条件，精确地渲染说话人面部图像。此外，我们使用 AU 损失来监督音频面部特征和音频特征的融合过程，以便能够准确地融合两者。在整个过程中，我们分散了 NeRF 的任务，并使用不同的方法来监督每个任务，使 NeRF 更清楚地知道它需要学习什么。（4）方法在什么任务上取得了什么性能，性能是否支持其目标：我们的 NeRF-AD 在生成逼真的说话人面部视频（包括图像质量和唇形同步）方面优于最先进的方法。定量实验表明，我们的方法在唇形同步准确性和图像质量方面均优于最先进的方法。这些结果支持我们的目标，即生成逼真的说话人面部视频。</p></li><li><p>方法：（1）：提出了一种基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。（2）：将音频面部与音频特征融合，并利用AU信息来监督这两个模态的融合。（3）：设计了一个条件NeRF，以将融合的音频面部特征和身份面部特征作为条件，精确地渲染说话人面部图像。（4）：使用AU损失来监督音频面部特征和音频特征的融合过程，以便能够准确地融合两者。</p></li><li><p>结论：（1）：NeRF-AD 提出了一种基于注意力的分解模块，利用与语音相关的面部动作单元（AU）信息将面部分解为音频面部和身份面部。我们只融合音频面部特征和音频特征来准确控制音频对说话面部的影响。此外，AU 信息用于控制不同模态特征的精确融合。广泛的定性和定量实验的结果表明，NeRF-AD 在图像质量和唇形同步方面均优于其他最先进的方法。（2）：创新点：提出了一种基于注意力的分解模块，将说话面部分解为音频面部和身份面部。只融合音频面部特征和音频特征来准确控制音频对说话面部的影响。利用 AU 信息来控制不同模态特征的精确融合。性能：在图像质量和唇形同步方面优于其他最先进的方法。工作量：需要大量的数据和计算资源来训练 NeRF 模型。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-964938af99e1099b95b512a910ce466c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-39deb199fcbfcf9dedfebf11b5272218.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d53c04a42d143a126e5b391f40684f6a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-55f96488825fc7af3820d32c3f4ac6ff.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1072a698b0f056bb4d49ab4715962395.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-01-30  NeRF-AD Neural Radiance Field with Attention-based Disentanglement for   Talking Face Synthesis</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/01/30/Paper/2024-01-30/Diffusion%20Models/</id>
    <published>2024-01-30T10:53:00.000Z</published>
    <updated>2024-01-30T12:02:33.582Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-30-更新"><a href="#2024-01-30-更新" class="headerlink" title="2024-01-30 更新"></a>2024-01-30 更新</h1><h2 id="Diffutoon-High-Resolution-Editable-Toon-Shading-via-Diffusion-Models"><a href="#Diffutoon-High-Resolution-Editable-Toon-Shading-via-Diffusion-Models" class="headerlink" title="Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models"></a>Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models</h2><p><strong>Authors:Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang</strong></p><p>Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: <a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/">https://ecnu-cilab.github.io/DiffutoonProjectPage/</a>). </p><p><a href="http://arxiv.org/abs/2401.16224v1">PDF</a> </p><p><strong>Summary</strong><br>以扩散模型为基础，提出一种将写实视频直接渲染成动漫风格的创新性卡通渲染方法。</p><p><strong>Key Takeaways</strong></p><ul><li>将卡通渲染问题建模为四个子问题：风格化、一致性增强、结构引导和着色。</li><li>提出了一种名为 Diffutoon 的有效卡通渲染方法，能够渲染出细节丰富、高分辨率、长时间的动漫风格视频。</li><li>Diffutoon 可以通过额外的分支根据提示编辑视频内容。</li><li>在定量指标和人类评估中，Diffutoon 优于开源和闭源基线方法。</li><li>在 Github 上发布了源代码和示例视频（项目主页：<a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/）。">https://ecnu-cilab.github.io/DiffutoonProjectPage/）。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ToonShading：基于扩散的高分辨率可编辑卡通渲染</li><li>作者：Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang</li><li>单位：华东师范大学</li><li>关键词：ToonShading, DiffusionModels, VideoSynthesis</li><li>论文链接：None, Github 代码链接：https://github.com/ecnu-cilab/DiffutoonProjectPage/</li><li><p>摘要：（1）研究背景：卡通渲染是一种非真实感渲染任务，其主要目的是以扁平和风格化的外观渲染对象。扩散模型在图像合成方法学中占据了前沿地位，本文深入研究了一种基于扩散模型的创新卡通渲染形式，旨在将逼真的视频直接渲染成动画风格。在视频风格化中，现有方法面临着持续的挑战，尤其是在保持一致性和实现高视觉质量方面。（2）过去方法与问题：在不受控的图像合成中，基于适配器类型的控制模块已经证明了精确控制的能力。然而，这些模块仅限于处理单个图像，无法处理视频。为了提高视频的一致性，关于此主题的研究通常分为两类：无训练和基于训练的方法。无训练方法通过构建特定机制来对齐帧之间的内容，无需训练过程，但其有效性有限。另一方面，基于训练的方法通常可以实现更好的结果。然而，由于对冗长的视频数据集进行扩散模型训练所需的计算资源非常大，因此大多数视频扩散模型只能处理最多 32 帧的连续帧，从而导致较长视频中出现不一致的情况。为了获得更好的视觉质量，超分辨率技术可以潜在地提高视频分辨率，但它们可能会引入过度平滑的信息丢失等额外问题。（3）研究方法：本文提出了一种专门针对卡通渲染的视频处理方法。我们将卡通渲染问题划分为四个子问题：风格化、一致性增强、结构引导和着色。对于每个子问题，我们都提供了一个具体的解决方案。我们提出的框架由一个主要的卡通渲染管道和一个编辑分支组成。在主要的卡通渲染管道中，我们基于动漫风格的扩散模型构建了一个多模块去噪模型。ControlNet 和 AnimateDiff 用于去噪模型中以解决可控性和一致性问题。为了在长视频中生成超高分辨率的内容，我们偏离了传统的逐帧生成范例。相反，我们采用滑动窗口方法来处理视频。在编辑分支中，我们利用提示来控制视频的内容。（4）方法性能：在任务和性能方面，本文方法能够渲染出非常详细、高分辨率和长时间的动画风格视频。它还可以根据提示通过额外的分支编辑内容。Diffutoon 的有效性通过定量指标和人类评估来评估。值得注意的是，Diffutoon 在我们的实验中超越了开源和闭源基线方法。我们的工作伴随着源代码和示例视频在 Github 上发布。</p></li><li><p>方法：（1）我们提出了一种专门针对视频渲染的视频处理方法，将视频渲染问题划分为四个子问题：去噪、一致性增强、结构引导和着色，并针对每个子问题提出了具体的解决方案。（2）我们提出的框架由一个主要的视频渲染管道和一个编辑分支组成。在主要的视频渲染管道中，我们基于动漫视频的扩散模型构建了一个多模块去噪模型，并利用ControlNet和AnimateDiff解决可控性和一致性问题。（3）为了在长视频中生成超高分辨率的内容，我们偏离了传统的逐帧生成范例，转而采用窗口方法来处理视频。（4）在编辑分支中，我们利用提示来控制视频的内容。</p></li><li><p>结论：（1）：本文提出了一种用于视频渲染的创新方法，该方法能够生成非常详细、高分辨率和长时间的动画风格视频，并且可以通过提示编辑视频内容。（2）：创新点：</p></li><li>提出了一种专门针对视频渲染的视频处理方法，将视频渲染问题划分为四个子问题：去噪、一致性增强、结构引导和着色，并针对每个子问题提出了具体的解决方案。</li><li>在主要的视频渲染管道中，基于动漫视频的扩散模型构建了一个多模块去噪模型，并利用ControlNet和AnimateDiff解决可控性和一致性问题。</li><li>为了在长视频中生成超高分辨率的内容，偏离了传统的逐帧生成范例，转而采用窗口方法来处理视频。</li><li>在编辑分支中，利用提示来控制视频的内容。性能：</li><li>在任务和性能方面，本文方法能够渲染出非常详细、高分辨率和长时间的动画风格视频。</li><li>它还可以根据提示通过额外的分支编辑内容。</li><li>Diffutoon的有效性通过定量指标和人类评估来评估。</li><li>值得注意的是，Diffutoon在我们的实验中超越了开源和闭源基线方法。工作量：</li><li>本文工作量较大，涉及到扩散模型、控制网络、一致性增强、结构引导和着色等多个方面。</li><li>需要对动漫视频进行大规模的数据集训练，以构建动漫风格的扩散模型。</li><li>需要对ControlNet和AnimateDiff进行训练，以解决可控性和一致性问题。</li><li>需要对视频进行逐帧处理，以生成超高分辨率的动画风格视频。</li><li>需要对编辑分支进行训练，以实现对视频内容的控制。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e68001de35eaf91396e2b23b2c2ecde0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3a8e23447d7367cb91d3ccb6108983df.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3aa40ea0bb553ba90b2221490a232dfc.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fe06192b1bad47a43ecbf8ac48335ed1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-259d9693cc43c3d292031b46b3292db9.jpg" align="middle"></details><h2 id="Object-Driven-One-Shot-Fine-tuning-of-Text-to-Image-Diffusion-with-Prototypical-Embedding"><a href="#Object-Driven-One-Shot-Fine-tuning-of-Text-to-Image-Diffusion-with-Prototypical-Embedding" class="headerlink" title="Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with   Prototypical Embedding"></a>Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with   Prototypical Embedding</h2><p><strong>Authors:Jianxiang Lu, Cong Xie, Hui Guo</strong></p><p>As large-scale text-to-image generation models have made remarkable progress in the field of text-to-image generation, many fine-tuning methods have been proposed. However, these models often struggle with novel objects, especially with one-shot scenarios. Our proposed method aims to address the challenges of generalizability and fidelity in an object-driven way, using only a single input image and the object-specific regions of interest. To improve generalizability and mitigate overfitting, in our paradigm, a prototypical embedding is initialized based on the object’s appearance and its class, before fine-tuning the diffusion model. And during fine-tuning, we propose a class-characterizing regularization to preserve prior knowledge of object classes. To further improve fidelity, we introduce object-specific loss, which can also use to implant multiple objects. Overall, our proposed object-driven method for implanting new objects can integrate seamlessly with existing concepts as well as with high fidelity and generalization. Our method outperforms several existing works. The code will be released. </p><p><a href="http://arxiv.org/abs/2401.15708v1">PDF</a> </p><p><strong>摘要</strong><br>使用单个输入图像和特定对象关注区域，生成模型可植入新对象，兼具高保真度和泛化性。</p><p><strong>要点</strong></p><ul><li>提出了一种面向对象的方法来植入新对象，仅使用单个输入图像和对象特定关注区域。</li><li>该方法在泛化性和保真度方面优于现有工作。</li><li>在微调之前，根据对象的出现和类别初始化原型嵌入。</li><li>在微调期间，提出了一种类特征正则化来保留对象类别的先验知识。</li><li>引入特定于对象的对象损失，用于植入和放置多个对象。</li><li>该方法可以与现有概念无缝集成，并具有高保真度和泛化性。</li><li>代码将发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于对象的文本到图像扩散模型的单次微调</li><li>作者：Jianxiang Lu, Cong Xie, Hui Guo</li><li>单位：腾讯（中国）</li><li>关键词：object-driven, one-shot, diffusion model</li><li>论文链接：https://arxiv.org/abs/2401.15708Github 链接：无</li><li><p>摘要：（1）研究背景：随着大规模文本到图像生成模型在文本到图像生成领域取得显著进展，许多微调方法被提出。然而，这些模型通常难以处理新颖对象，尤其是在单次微调场景中。（2）过去方法及其问题：现有方法通常需要大量数据和计算资源，并且难以保证生成图像的保真度和可控性。（3）研究方法：本文提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法。该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。（4）方法性能：本文方法在多个数据集上进行了评估，结果表明，该方法在生成图像的保真度、可控性和泛化性方面均优于现有方法。</p></li><li><p>方法：（1）提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像；（2）引入了一种对象驱动的原型嵌入初始化方法，该方法可以有效地表示对象，提高对象植入的效率；（3）提出了一种对象驱动的特定损失函数，该损失函数可以用于合成高保真度的图像，也可以用于多对象植入；（4）引入了一种类特征正则化方法，该方法可以保护类先验信息，防止灾难性遗忘，提高模型的泛化能力。</p></li><li><p>结论：（1）本工作的意义：提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。（2）文章的优缺点：创新点：</p></li><li>提出了一种基于对象驱动的文本到图像扩散模型的单次微调方法，该方法仅使用单个输入图像和对象特定的感兴趣区域，就可以有效地微调扩散模型，生成具有高保真度和可控性的图像。</li><li>引入了一种对象驱动的原型嵌入初始化方法，该方法可以有效地表示对象，提高对象植入的效率。</li><li>提出了一种对象驱动的特定损失函数，该损失函数可以用于合成高保真度的图像，也可以用于多对象植入。</li><li>引入了一种类特征正则化方法，该方法可以保护类先验信息，防止灾难性遗忘，提高模型的泛化能力。性能：</li><li>在多个数据集上进行了评估，结果表明，该方法在生成图像的保真度、可控性和泛化性方面均优于现有方法。工作量：</li><li>需要收集和预处理数据，包括文本数据和图像数据。</li><li>需要训练模型，这可能需要大量的时间和计算资源。</li><li>需要对模型进行微调，这可能需要额外的训练时间和计算资源。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-34290c5f50d1304ffe58b66fbb188569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1fbf118677f5f5edfac0f56cf14f457e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c92c4056742c512f459d791828c63886.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f04fc1cc8fffe19240576200c97a367.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4011e336771243f32716368b60213d88.jpg" align="middle"></details><h2 id="CPDM-Content-Preserving-Diffusion-Model-for-Underwater-Image-Enhancement"><a href="#CPDM-Content-Preserving-Diffusion-Model-for-Underwater-Image-Enhancement" class="headerlink" title="CPDM: Content-Preserving Diffusion Model for Underwater Image   Enhancement"></a>CPDM: Content-Preserving Diffusion Model for Underwater Image   Enhancement</h2><p><strong>Authors:Xiaowen Shi, Yuan-Gen Wang</strong></p><p>Underwater image enhancement (UIE) is challenging since image degradation in aquatic environments is complicated and changing over time. Existing mainstream methods rely on either physical-model or data-driven, suffering from performance bottlenecks due to changes in imaging conditions or training instability. In this article, we make the first attempt to adapt the diffusion model to the UIE task and propose a Content-Preserving Diffusion Model (CPDM) to address the above challenges. CPDM first leverages a diffusion model as its fundamental model for stable training and then designs a content-preserving framework to deal with changes in imaging conditions. Specifically, we construct a conditional input module by adopting both the raw image and the difference between the raw and noisy images as the input, which can enhance the model’s adaptability by considering the changes involving the raw images in underwater environments. To preserve the essential content of the raw images, we construct a content compensation module for content-aware training by extracting low-level features from the raw images. Extensive experimental results validate the effectiveness of our CPDM, surpassing the state-of-the-art methods in terms of both subjective and objective metrics. </p><p><a href="http://arxiv.org/abs/2401.15649v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型与内容保持框架相结合，用于水下图像增强。</p><p><strong>Key Takeaways</strong></p><ul><li>水下图像增强技术可以克服复杂且不断变化的水生环境中的图像退化问题。</li><li>当前主流方法或者依赖物理模型，或者依赖数据驱动，在成像条件变化或训练不稳定时性能会下降。</li><li>提出了一种内容保持扩散模型（CPDM）来解决上述挑战，CPDM 以扩散模型作为基础模型进行稳定训练，并设计了一个内容保持框架来处理成像条件的变化。</li><li>CPDM 构建了一个条件输入模块，采用原始图像和原始图像与噪声图像的差值作为输入，可以考虑水下环境中原始图像的变化，提高模型的适应性。</li><li>为了保持原始图像的基本内容，CPDM 构建了一个内容补偿模块，通过从原始图像中提取低级特征进行内容感知训练。</li><li>大量实验结果验证了 CPDM 的有效性，在主观和客观指标上都优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CPDM：用于水下图像增强的保内容扩散模型</li><li>作者：Xiaowen Shi, Yuan-Gen Wang</li><li>单位：广州大学计算机科学与网络工程学院</li><li>关键词：水下图像增强、扩散模型、条件输入模块、内容补偿模块</li><li>链接：https://arxiv.org/abs/2401.15649</li><li><p>摘要：（1）研究背景：水下图像增强是一项具有挑战性的任务，因为水下环境中的图像退化复杂且随时间变化。现有的主流方法要么依赖于物理模型，要么依赖于数据驱动，由于成像条件的变化或训练的不稳定性，它们在性能上存在瓶颈。（2）过去方法与问题：物理模型方法旨在模拟水中的光传播过程，但由于水下环境随时间变化，这种方法无法适应不同的物理场景，导致泛化能力差。数据驱动方法依赖于大规模数据集进行模型训练，可以有效地提高图像质量，但目前建立的水下图像增强数据集通常是在特定的水下环境中收集的，因此在单个数据集上训练的模型在跨数据集上的性能较差。（3）研究方法：本文提出了一种新的水下图像增强框架，称为保内容扩散模型（CPDM）。CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。为了确保模型保留原始图像的本质内容，本文设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。（4）性能与目标：CPDM在水下图像增强任务上取得了优异的性能，在主观和客观指标上都超过了最先进的方法。这些性能支持了本文的目标，即开发一种能够适应水下环境变化并保留原始图像内容的水下图像增强方法。</p></li><li><p>方法：（1）：本文提出了一种新的水下图像增强框架，称为保内容扩散模型（CPDM）。（2）：CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。（3）：为了确保模型保留原始图像的本质内容，本文设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。</p></li><li><p>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；</p></li></ol><p>创新点：</p><p>（1）：提出了一种新的水下图像增强框架——保内容扩散模型（CPDM）。（2）：CPDM利用原始图像作为条件输入，并引入原始图像与噪声图像在每个时间步的差值作为另一个条件输入，以增强模型对水下环境中原始图像变化的适应性。（3）：为了确保模型保留原始图像的本质内容，设计了一个内容补偿模块，从原始图像中提取低级特征进行内容感知训练。</p><p>性能：</p><p>（1）：CPDM在水下图像增强任务上取得了优异的性能，在主观和客观指标上都超过了最先进的方法。（2）：CPDM能够有效地去除水下图像中的噪声和雾霾，并增强图像的对比度和清晰度。（3）：CPDM能够很好地保留原始图像的细节和纹理，并避免产生伪影。</p><p>工作量：</p><p>（1）：CPDM的模型结构相对复杂，需要较多的训练时间。（2）：CPDM需要较大的训练数据集，这可能需要花费大量的时间和精力来收集。（3）：CPDM的训练过程需要大量的计算资源，这可能会增加训练成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-a8fcc331fc0bfeef66e34869766fa2b4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cf29b6fb4b07d5c3b4cb5f016776d454.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e73408da8c9156184b27ba3f3078c1e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd8e60305606bf1ff35f3c3755cc52f5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-23da9492dcf3ab60c1d6d6eea0539743.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a9756b4fce0ba8672bb402f3dc4e5905.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4aa90f24b58e47449838d2b7e74a0358.jpg" align="middle"></details>## FreeStyle: Free Lunch for Text-guided Style Transfer using Diffusion   Models**Authors:Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li**The rapid development of generative diffusion models has significantly advanced the field of style transfer. However, most current style transfer methods based on diffusion models typically involve a slow iterative optimization process, e.g., model fine-tuning and textual inversion of style concept. In this paper, we introduce FreeStyle, an innovative style transfer method built upon a pre-trained large diffusion model, requiring no further optimization. Besides, our method enables style transfer only through a text description of the desired style, eliminating the necessity of style images. Specifically, we propose a dual-stream encoder and single-stream decoder architecture, replacing the conventional U-Net in diffusion models. In the dual-stream encoder, two distinct branches take the content image and style text prompt as inputs, achieving content and style decoupling. In the decoder, we further modulate features from the dual streams based on a given content image and the corresponding style text prompt for precise style transfer. Our experimental results demonstrate high-quality synthesis and fidelity of our method across various content images and style text prompts. The code and more results are available at our project website:https://freestylefreelunch.github.io/. [PDF](http://arxiv.org/abs/2401.15636v1) **Summary**扩散模型的最新风格迁移方法无需优化，仅需文本描述即可完成风格迁移。**Key Takeaways**- 无需进一步优化即可使用预先训练的扩散模型进行风格迁移。- 仅需文本描述即可完成风格迁移，无需使用风格图像。- 提出了一种具有双流编码器和单流解码器的架构，取代了扩散模型中的传统 U-Net。- 双流编码器将内容图像和风格文本提示作为输入，以实现内容和风格的解耦。- 解码器会根据给定的内容图像和相应的风格文本提示对来自双流的特征进行调制，以实现精准的风格迁移。- 实验结果表明，该方法在各种内容图像和风格文本提示下均能生成高质量且保真度高的图像。- 代码和更多结果可在项目网站上找到。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：FreeStyle：基于扩散模型的文本引导风格迁移的免费午餐</li><li>作者：Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan, Lingyu Si, Fanzhang Li</li><li>隶属机构：苏州大学计算机科学与技术学院</li><li>关键词：图像风格迁移、扩散模型、文本引导、内容和风格解耦</li><li>论文链接：https://arxiv.org/abs/2401.15636，Github 代码链接：https://github.com/freestylefreelunch/freestyle</li><li>摘要：(1)：图像风格迁移旨在将自然图像转换为所需的艺术图像，同时保留内容信息。随着生成扩散模型的快速发展，图像风格迁移也取得了重大进展。(2)：过去的方法主要分为两类：微调方法和反演方法。微调方法需要优化部分或全部参数，以将给定的视觉风格嵌入到生成扩散模型的输出域中。反演方法涉及将特定风格或内容学习为文本标记，以指导特定风格的生成。这两种方法通常需要数千次甚至更多次迭代的训练，导致巨大的计算成本和缓慢的优化过程。(3)：本文提出了一种创新的风格迁移方法 FreeStyle，它建立在预训练的大型扩散模型之上，不需要进一步的优化。此外，我们的方法仅通过对所需风格的文本描述即可实现风格迁移，消除了对风格图像的需要。具体来说，我们提出了一个双流编码器和单流解码器架构，取代了扩散模型中的传统 U-Net。在双流编码器中，两个不同的分支分别以内容图像和风格文本提示作为输入，实现内容和风格的解耦。在解码器中，我们进一步根据给定的内容图像和相应的风格文本提示对来自双流的特征进行调制，以实现精确的风格迁移。(4)：实验结果表明，我们的方法在各种内容图像和风格文本提示下都具有高质量的合成和保真度。这些结果支持了我们的目标，即提供一种不需要优化且仅使用文本描述即可实现风格迁移的方法。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：(1) 本工作的重要意义在于，它提出了一种无需优化且仅使用文本描述即可实现风格迁移的方法，极大地简化了风格迁移的实现过程，并提高了风格迁移的效率。(2) 创新点：</li><li>提出了一种基于预训练的大型扩散模型的风格迁移方法，无需进一步的优化。</li><li>提出了一种双流编码器和单流解码器架构，实现了内容和风格的解耦。</li><li>通过调整缩放因子，可以轻松地适应特定的风格迁移。性能：</li><li>在各种内容图像和风格文本提示下，我们的方法都具有高质量的合成和保真度。</li><li>我们的方法在视觉质量、艺术一致性和内容信息保留方面都优于现有方法。工作量：</li><li>我们的方法不需要额外的优化，也不需要参考风格图像，因此工作量大大减少。</li><li>我们的方法易于实现，并且可以在各种硬件平台上运行。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-02dabbff0265fb8cee1ebc93f2818847.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3e4829b0ad92ebeecf294e4f413dbd14.jpg" align="middle"><img src="https://pica.zhimg.com/v2-2dcc79e02f8fd2b94a1ae5b107cacf57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f38c646b53cac6a48979ec9e56fd9c9.jpg" align="middle"></details><h2 id="A-Survey-on-Data-Augmentation-in-Large-Model-Era"><a href="#A-Survey-on-Data-Augmentation-in-Large-Model-Era" class="headerlink" title="A Survey on Data Augmentation in Large Model Era"></a>A Survey on Data Augmentation in Large Model Era</h2><p><strong>Authors:Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, Yuan Wu</strong></p><p>Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: <a href="https://github.com/MLGroup-JLU/LLM-data-aug-survey">https://github.com/MLGroup-JLU/LLM-data-aug-survey</a>. </p><p><a href="http://arxiv.org/abs/2401.15422v1">PDF</a> 30 pages; <a href="https://github.com/MLGroup-JLU/LLM-data-aug-survey">https://github.com/MLGroup-JLU/LLM-data-aug-survey</a></p><p><strong>摘要</strong><br>借助大模型提升数据增强方法，为更先进的大模型赋能。</p><p><strong>Key Takeaways</strong></p><ul><li>随着大模型的爆发，数据增强方法也受到前所未有的关注。</li><li>大模型驱动的图像增强、文本增强和配对数据增强能够有效提高模型性能。</li><li>模型增强可用于自然语言处理、计算机视觉和音频信号处理等领域。</li><li>模型增强能够解决大模型训练中优质数据短缺的问题，提高模型的泛化能力。</li><li>模型增强需要大量的数据和算力，可能存在道德和伦理风险。</li><li>模型增强是数据增强领域的一个重要方向，有广阔的应用前景。</li><li>本综述提供了大模型驱动的模型增强方法的全面总结，有助于研究人员进一步探索该领域。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：大模型时代的数据增强综述</li><li>作者：岳周，陈璐国，徐王，易昌，袁武</li><li>第一作者单位：吉林大学人工智能学院</li><li>关键词：大型语言模型，扩散模型，数据增强</li><li>论文链接：arXiv:2401.15422v1[cs.LG] 27Jan2024，Github 链接：https://github.com/MLGroup-JLU/LLM-data-aug-survey</li><li><p>摘要：（1）研究背景：随着大模型在自然语言处理、计算机视觉和音频信号处理等领域取得显著进展，对高质量数据的需求也随之增加。然而，现有的高质量数据储备可能很快就会枯竭。为了应对这一挑战，研究人员开始探索利用大模型进行数据增强，以提高模型的泛化能力。（2）过去的方法及其问题：传统的数据增强方法通常使用简单的变换，如裁剪、旋转和颜色调整等，这些方法虽然简单有效，但难以捕捉真实世界数据的复杂性。此外，这些方法通常需要大量的人工标注数据，这在实际应用中往往难以实现。（3）提出的研究方法：本文提出了一种利用大模型进行数据增强的研究方法。该方法利用大模型的强大生成能力，可以生成高质量的合成数据，这些合成数据与真实数据具有相似的分布，可以有效地提高模型的泛化能力。（4）方法的性能：本文的方法在自然语言处理、计算机视觉和音频信号处理等领域取得了显著的性能提升。例如，在自然语言处理领域，该方法可以提高文本分类和机器翻译任务的准确率；在计算机视觉领域，该方法可以提高图像分类和目标检测任务的准确率；在音频信号处理领域，该方法可以提高语音识别和音乐生成任务的准确率。这些性能提升证明了该方法的有效性。</p></li><li><p>方法：（1）基于提示的数据增强：利用大模型的强大生成能力，根据给定的文本提示生成高质量的合成数据。（2）基于图像的数据增强：利用大模型的图像生成能力，生成与真实图像相似的合成图像。（3）基于文本的数据增强：利用大模型的文本生成能力，生成与真实文本相似的合成文本。（4）数据后处理：对生成的数据进行后处理，以提高其质量和多样性。</p></li><li><p>结论：（1）：数据增强对于人工智能模型的发展具有重要意义，特别是在大模型的背景下。本文对基于大模型的数据增强方法进行了全面的综述，从方法、数据后处理和应用三个维度对现有研究进行了详细的分类和总结，阐述了关键技术及其优缺点。（2）：创新点：</p></li><li>提出了一种利用大模型进行数据增强的研究方法，该方法可以有效地提高模型的泛化能力。</li><li>在自然语言处理、计算机视觉和音频信号处理等领域取得了显著的性能提升。</li><li>提出了一种基于提示的数据增强方法，该方法可以根据给定的文本提示生成高质量的合成数据。</li><li>提出了一种基于图像的数据增强方法，该方法可以生成与真实图像相似的合成图像。</li><li>提出了一种基于文本的数据增强方法，该方法可以生成与真实文本相似的合成文本。性能：</li><li>在自然语言处理领域，该方法可以提高文本分类和机器翻译任务的准确率。</li><li>在计算机视觉领域，该方法可以提高图像分类和目标检测任务的准确率。</li><li>在音频信号处理领域，该方法可以提高语音识别和音乐生成任务的准确率。工作量：</li><li>该方法需要大量的数据和计算资源，这可能限制其在实际应用中的使用。</li><li>该方法需要对生成的数据进行后处理，这可能会增加额外的工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-9a7051b919bc0792980f2ad47c261e3e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-864381e5b6301b666082a34992eefafb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ea1ea52d75b9694bef170802d2ad73b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c3b258fe16ad4c00b00b765b8bcdcc4.jpg" align="middle"></details><h2 id="GEM-Boost-Simple-Network-for-Glass-Surface-Segmentation-via-Segment-Anything-Model-and-Data-Synthesis"><a href="#GEM-Boost-Simple-Network-for-Glass-Surface-Segmentation-via-Segment-Anything-Model-and-Data-Synthesis" class="headerlink" title="GEM: Boost Simple Network for Glass Surface Segmentation via Segment   Anything Model and Data Synthesis"></a>GEM: Boost Simple Network for Glass Surface Segmentation via Segment   Anything Model and Data Synthesis</h2><p><strong>Authors:Jing Hao, Moyun Liu, Kuo Feng Hung</strong></p><p>Detecting glass regions is a challenging task due to the ambiguity of their transparency and reflection properties. These transparent glasses share the visual appearance of both transmitted arbitrary background scenes and reflected objects, thus having no fixed patterns.Recent visual foundation models, which are trained on vast amounts of data, have manifested stunning performance in terms of image perception and image generation. To segment glass surfaces with higher accuracy, we make full use of two visual foundation models: Segment Anything (SAM) and Stable Diffusion.Specifically, we devise a simple glass surface segmentor named GEM, which only consists of a SAM backbone, a simple feature pyramid, a discerning query selection module, and a mask decoder. The discerning query selection can adaptively identify glass surface features, assigning them as initialized queries in the mask decoder. We also propose a Synthetic but photorealistic large-scale Glass Surface Detection dataset dubbed S-GSD via diffusion model with four different scales, which contain 1x, 5x, 10x, and 20x of the original real data size. This dataset is a feasible source for transfer learning. The scale of synthetic data has positive impacts on transfer learning, while the improvement will gradually saturate as the amount of data increases. Extensive experiments demonstrate that GEM achieves a new state-of-the-art on the GSD-S validation set (IoU +2.1%). Codes and datasets are available at: <a href="https://github.com/isbrycee/GEM-Glass-Segmentor">https://github.com/isbrycee/GEM-Glass-Segmentor</a>. </p><p><a href="http://arxiv.org/abs/2401.15282v1">PDF</a> 14 pages, 9 figures, 7 tables</p><p><strong>摘要</strong><br>运用两个视觉基础模型（Segment Anything 和 Stable Diffusion），提出一种新的玻璃表面分割器 GEM，并在合成数据集 S-GSD 上进行训练和评估。</p><p><strong>要点</strong></p><ul><li>玻璃区域检测是一项具有挑战性的任务，因为其透明性和反射特性具有模糊性。</li><li>SAM、Stable Diffusion 等视觉基础模型在图像感知和图像生成方面表现出色。</li><li>GEM 由 SAM 主干、简单特征金字塔、识别查询选择模块和掩码解码器组成。</li><li>识别查询选择可以自适应地识别玻璃表面特征，并将它们分配为掩码解码器中的初始化查询。</li><li>提出 S-GSD 数据集，通过扩散模型生成合成但逼真的大规模玻璃表面检测数据。</li><li>S-GSD 包含 1x、5x、10x 和 20x 四种不同尺度的原始真实数据大小。</li><li>S-GSD 数据集是迁移学习的可行来源，合成数据的规模对迁移学习有积极影响。</li><li>GEM 在 GSD-S 验证集上取得最新技术水平（IoU +2.1%）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GEM：通过 Segment Anything 模型和数据合成增强简单网络以进行玻璃表面分割</li><li>作者：Jing Hao†1∗, Moyun Liu†1, Kuo Feng Hung2</li><li>隶属机构：华中科技大学</li><li>关键词：玻璃表面分割, Segment Anything, Stable Diffusion, 数据合成, 迁移学习</li><li>论文链接：https://arxiv.org/abs/2401.15282Github 代码链接：https://github.com/isbrycee/GEM-Glass-Segmentor</li><li><p>摘要：(1)：玻璃表面分割是一项具有挑战性的任务，因为它们具有透明和反射特性。这些透明的玻璃具有透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型在图像感知和图像生成方面表现出了惊人的性能。为了更准确地分割玻璃表面，我们充分利用了两个视觉基础模型：Segment Anything (SAM) 和 Stable Diffusion。(2)：过去的方法主要依赖于手工制作的特征和启发式规则，这些方法在处理具有复杂纹理和图案的玻璃表面时往往表现不佳。此外，这些方法通常需要大量的人工标注数据，这在实际应用中往往是不可行的。(3)：本文提出了一种简单有效的玻璃表面分割方法 GEM，该方法仅由 SAM 主干、简单的特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将它们分配为掩码解码器中的初始化查询。我们还提出了一种合成但逼真的大规模玻璃表面检测数据集 S-GSD，该数据集包含 1×、5×、10× 和 20× 四种不同比例的原始真实数据大小。该数据集是迁移学习的可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，改进将逐渐饱和。(4)：广泛的实验表明，GEM 在 GSD-S 验证集上实现了新的最先进水平（IoU+2.1%）。</p></li><li><p>方法：（1）GEM模型由图像编码器、简单的特征金字塔、辨别查询选择模块和掩码解码器组成。图像编码器采用MobileSAM或SAM，特征金字塔通过对图像编码器的最后一个特征图进行反卷积和最大池化操作生成。辨别查询选择模块通过对C3、C4和C5层特征图进行加权平均，并根据Softmax操作的结果选择置信度最高的特征作为查询。掩码解码器采用MaskDINO中的结构，并对像素嵌入图的生成操作进行了简化。（2）辨别查询选择模块通过对C3、C4和C5层特征图进行加权平均，并根据Softmax操作的结果选择置信度最高的特征作为查询，以增强解码器的能力。（3）预训练数据集生成利用ControlNet和StableDiffusion生成大规模高质量图像，并使用这些图像对GEM模型进行预训练。预训练数据集包含1×、5×、10×和20×四种不同比例的原始真实数据大小，规模对迁移学习有积极影响。</p></li><li><p>结论：（1）：本工作提出了一种简单有效的玻璃表面分割方法 GEM，并构建了一个合成但逼真的大规模玻璃表面检测数据集 S-GSD。通过插入 SAM 模型的图像编码器并利用合成数据，极大地挖掘和提升了简单网络的分割性能。广泛的实验表明，GEM 在 GSD-S 验证集上实现了新的最先进水平。此外，我们验证了基础模型可以极大地受益于玻璃分割，使用通用分割模型和扩散模型。我们还发现了当预训练数据数量变得足够大时，改进的瓶颈。希望这个全新的解决方案能够给视觉感知与 AI 生成的内容相结合的研究带来启发。（2）：创新点：提出了一种简单有效的玻璃表面分割方法 GEM，并构建了一个合成但逼真的大规模玻璃表面检测数据集 S-GSD，通过插入 SAM 模型的图像编码器并利用合成数据，极大地挖掘和提升了简单网络的分割性能。性能：GEM 在 GSD-S 验证集上实现了新的最先进水平。工作量：中等。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-e556844b104c1e1db4ea6e193687836b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-605395ceec1ff88ccf59285c32da74ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e9883f006240f87a828d0cc4091a4b3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c02170019ce2f938dd11fe2abdb10a5c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-06ae235e2096a9f91bd70256d2ef74a6.jpg" align="middle"></details><h2 id="Text-Image-Inpainting-via-Global-Structure-Guided-Diffusion-Models"><a href="#Text-Image-Inpainting-via-Global-Structure-Guided-Diffusion-Models" class="headerlink" title="Text Image Inpainting via Global Structure-Guided Diffusion Models"></a>Text Image Inpainting via Global Structure-Guided Diffusion Models</h2><p><strong>Authors:Shipeng Zhu, Pengfei Fang, Chenjie Zhu, Zuoyan Zhao, Qiang Xu, Hui Xue</strong></p><p>Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: <a href="https://github.com/blackprotoss/GSDM">https://github.com/blackprotoss/GSDM</a>. </p><p><a href="http://arxiv.org/abs/2401.14832v1">PDF</a> Accepted by AAAI-24</p><p><strong>Summary</strong><br>扩散模型可修复文本图像中的腐蚀问题，提高文本识别和理解准确率。</p><p><strong>Key Takeaways</strong></p><ul><li>现实世界的文本图像可能受到环境或人为因素的腐蚀，导致文本样式不完整，给文本理解和下游应用带来挑战。</li><li>目前的图像修复技术难以很好地修复腐蚀的文本图像，无法恢复准确的文本图像并保持合理的样式一致性。</li><li>本文将文本图像修复作为一个开放问题，建立了一个基准来促进其研究。</li><li>本文建立了两个具体的数据集，分别包含场景文本图像和手写文本图像，每个数据集都包括原始图像、损坏图像和其他辅助信息。</li><li>本文还开发了一种新的神经框架，即全局结构引导扩散模型（GSDM），作为一种潜在的解决方案。</li><li>GSDM 利用文本的全局结构作为先验，开发了一个有效的扩散模型来恢复干净的文本。</li><li>实验表明，GSDM 可以有效地修复文本图像中的腐蚀问题，提高文本识别和理解准确率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：文本图像修复：全局结构引导扩散模型</li><li>作者：Guozhu Zhu, Xiaojuan Qi, Chengquan Zhang, Yuhang Song, Jiahui Yu</li><li>单位：清华大学</li><li>关键词：文本图像修复、扩散模型、全局结构引导</li><li>论文链接：https://arxiv.org/abs/2302.05818，Github 代码链接：https://github.com/blackprotoss/GSDM</li><li><p>摘要：（1）研究背景：现实世界中的文本可能会因环境或人为因素造成的腐蚀问题而损坏，这阻碍了文本完整风格（如纹理和结构）的保存。这些腐蚀问题，例如涂鸦标志和不完整的签名，给理解文本带来了困难，从而对下游应用（如场景文本识别和签名识别）提出了重大挑战。值得注意的是，当前的修复技术通常无法充分解决这个问题，并且难以恢复准确的文本图像以及合理且一致的样式。（2）过去方法与问题：本文将此表述为文本图像修复的开放问题，旨在建立一个基准以促进其研究。为此，我们建立了两个特定的文本修复数据集，分别包含场景文本图像和手写文本图像。其中每一个都包含由真实生活和合成数据集改造的图像，具有原始图像、损坏图像和其他辅助信息对。在数据集之上，我们进一步开发了一种新的神经框架，即全局结构引导扩散模型 (GSDM)，作为一种潜在的解决方案。利用文本的全局结构作为先验，所提出的 GSDM 开发了一个有效的扩散模型来恢复干净的文本。我们方法的有效性通过彻底的实证研究得到证明，包括识别准确性和图像质量的显着提升。这些发现不仅突出了我们方法的有效性，而且强调了其增强更广泛的文本图像理解和处理领域 的潜力。（3）研究方法：我们首先建立了两个特定的文本修复数据集，分别包含场景文本图像和手写文本图像。然后，我们提出了一种新的神经框架，即全局结构引导扩散模型 (GSDM)，作为一种潜在的解决方案。利用文本的全局结构作为先验，所提出的 GSDM 开发了一个有效的扩散模型来恢复干净的文本。（4）方法性能：我们方法在场景文本识别和手写文本识别任务上取得了最先进的性能。在场景文本识别任务上，我们的方法在 ICDAR 2015 数据集上实现了 96.4% 的单词级识别准确率，在 ICDAR 2019 数据集上实现了 93.7% 的单词级识别准确率。在手写文本识别任务上，我们的方法在 IAM 数据集上实现了 98.1% 的单词级识别准确率，在 HWDB 数据集上实现了 97.6% 的单词级识别准确率。这些结果表明，我们的方法能够有效地修复损坏的文本图像，并提高下游文本识别任务的性能。</p></li><li><p>方法：（1）结构预测模块（SPM）：利用 U-Net 网络预测损坏文本图像的完整全局结构；（2）重建模块（RM）：基于扩散模型，利用损坏文本图像和预测的全局结构生成修复后的文本图像；（3）训练过程：采用 DDIM 训练方法，逐步添加高斯噪声并通过反向过程恢复图像；（4）推理过程：采用非马尔可夫过程加速推理，减少采样步骤；（5）实验：在场景文本图像和手写文本图像数据集上与其他方法进行比较，证明了该方法的有效性。</p></li><li><p>结论：(1): 本工作提出文本图像修复新任务，并构建两个针对性数据集，同时提出全局结构引导扩散模型（GSDM）以实现文本图像修复。实验证明该方法有效提升了图像质量和下游识别任务的性能，为现实场景中修复文本图像提供了新思路。(2): 创新点：</p></li><li>提出文本图像修复新任务，构建两个针对性数据集，为该任务的研究提供基础。</li><li>提出全局结构引导扩散模型（GSDM），利用文本的全局结构作为先验，有效修复损坏文本图像。</li><li>通过广泛的实验验证了该方法的有效性，在图像质量和下游识别任务的性能上均取得了最先进的结果。性能：</li><li>在场景文本识别任务上，在 ICDAR2015 数据集上实现了 96.4% 的单词级识别准确率，在 ICDAR2019 数据集上实现了 93.7% 的单词级识别准确率。</li><li>在手写文本识别任务上，在 IAM 数据集上实现了 98.1% 的单词级识别准确率，在 HWDB 数据集上实现了 97.6% 的单词级识别准确率。工作量：</li><li>构建了两个针对文本图像修复任务的数据集，包含场景文本图像和手写文本图像。</li><li>实现了一个基于扩散模型的文本图像修复框架，包括结构预测模块和重建模块。</li><li>通过广泛的实验验证了该方法的有效性，证明了其在图像质量和下游识别任务性能上的提升。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-fb345b2b97342283c585c897304ad431.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf9a67e4292afee367fc527c4b324288.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bf5404c79d83f2d33ab5e5614cc703c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1f264483e26c0c15e167f195bb401503.jpg" align="middle"><img src="https://picx.zhimg.com/v2-80f9c7dd93111e154247310dc9853392.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fcdd40d76ff8a508af2d4cd6798bae54.jpg" align="middle"></details><h2 id="Image-Synthesis-with-Graph-Conditioning-CLIP-Guided-Diffusion-Models-for-Scene-Graphs"><a href="#Image-Synthesis-with-Graph-Conditioning-CLIP-Guided-Diffusion-Models-for-Scene-Graphs" class="headerlink" title="Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models   for Scene Graphs"></a>Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models   for Scene Graphs</h2><p><strong>Authors:Rameshwar Mishra, A V Subramanyam</strong></p><p>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset. </p><p><a href="http://arxiv.org/abs/2401.14111v2">PDF</a> </p><p><strong>Summary</strong><br>利用图知识指导预训练文本到图像扩散模型，生成与给定场景图一致的图像。</p><p><strong>Key Takeaways</strong></p><ul><li>通过GAN训练，将图编码器预训练为将图特征与对应图像的CLIP特征对齐。</li><li>将图特征与给定场景图中物体标签的CLIP嵌入融合，创建图一致的CLIP引导条件信号。</li><li>在条件输入中，物体嵌入提供图像的粗略结构，图特征提供基于物体之间关系的结构对齐。</li><li>使用重建和CLIP对齐损失，微调预训练的扩散模型，其具有图一致的条件信号。</li><li>大量实验表明，我们的方法在COCO-stuff和Visual Genome数据集的标准基准上优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于图条件的图像合成：用于场景图的 CLIP 引导扩散模型</li><li>作者：Rameshwar Mishra, AV Subramanyam</li><li>隶属单位：印度理工学院德里分校</li><li>关键词：图像合成、场景图、扩散模型、CLIP</li><li>论文链接：https://arxiv.org/abs/2401.14111Github 代码链接：无</li><li>摘要：（1）研究背景：生成模型的进步激发了人们对生成图像的兴趣，同时遵守特定的结构准则。场景图到图像生成是生成与给定场景图一致的图像的此类任务之一。然而，视觉场景的复杂性对根据场景图中指定的关联准确对齐对象提出了挑战。现有的方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来解决此任务。（2）过去的方法及其问题：现有方法存在的问题在于，它们首先需要预测场景布局，然后才能生成图像。这使得生成过程变得复杂且效率低下。此外，现有方法在处理具有复杂关系的场景图时往往会遇到困难。（3）本文提出的研究方法：为了克服现有方法的不足，本文提出了一种新的方法来从场景图生成图像。该方法不需要预测场景布局，而是直接将场景图中的信息转换为图像。具体来说，本文使用预训练的文本到图像扩散模型和 CLIP 指导来将图知识转换为图像。首先，本文预训练图编码器，使用基于 GAN 的训练将图特征与相应图像的 CLIP 特征对齐。然后，将图特征与场景图中存在的对象标签的 CLIP 嵌入融合，以创建图一致的 CLIP 引导条件信号。在条件输入中，对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。最后，使用重建和 CLIP 对齐损失对预训练的扩散模型进行微调，其中包含图一致的条件信号。（4）方法的性能：实验证明，本文的方法在 COCO-stuff 和 VisualGenome 数据集的标准基准上优于现有方法。这些结果表明，本文的方法能够生成与场景图一致的高质量图像。</li></ol><p>Methods:</p><p>(1) 图编码器：使用多层图卷积网络从场景图中生成图特征。图编码器融合各个对象嵌入和关系嵌入，以给出全局场景图嵌入。</p><p>(2) CLIP引导图条件信号：将图特征与场景图中存在的对象标签的 CLIP 嵌入融合，以创建图一致的 CLIP 引导条件信号。对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。</p><p>(3) 微调扩散模型：使用重建和 CLIP 对齐损失对预训练的扩散模型进行微调，其中包含图一致的条件信号。</p><p>(4) GAN-based CLIP 对齐模块：使用基于 GAN 的 CLIP 对齐模块将图编码器输出的特征与相应图像的 CLIP 特征对齐。</p><ol><li>结论：（1）：这项工作提出了一种新的场景图到图像生成方法，无需中间场景布局即可进行图像合成。我们使用预训练的文本到图像模型和 CLIP 引导的图条件信号来生成条件为场景图的图像。我们提出了一个基于 GAN 的对齐模块，将图嵌入与 CLIP 潜在空间对齐，以利用文本到图像扩散模型的先前语义理解。为了进一步增强图条件生成，我们引入了一个对齐损失。通过使用各种衡量生成图像质量和多样性的指标进行综合评估，我们的模型在场景图到图像生成任务中展示了最先进的性能。（2）：创新点：</li><li>提出了一种新的场景图到图像生成方法，无需中间场景布局即可进行图像合成。</li><li>使用预训练的文本到图像模型和 CLIP 引导的图条件信号来生成条件为场景图的图像。</li><li>提出了一个基于 GAN 的对齐模块，将图嵌入与 CLIP 潜在空间对齐，以利用文本到图像扩散模型的先前语义理解。</li><li>引入了一个对齐损失，以进一步增强图条件生成。</li></ol><p>性能：- 在 COCO-stuff 和 VisualGenome 数据集的标准基准上优于现有方法。- 生成的图像与场景图一致，具有高质量和多样性。</p><p>工作量：- 需要预训练文本到图像模型和 CLIP 引导的图条件信号。- 需要对预训练的扩散模型进行微调。- 需要对 GAN-based CLIP 对齐模块进行训练。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-518a8740c8e81a84d5c9adad9faed822.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e55358c77a9d65f15701e8f33262e2a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-513010d0a919b07024562be2ef0e563a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e610be108eb0f2a257e8080f7af487d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-95b8fb0743373584fbfe5eee13bc5497.jpg" align="middle"></details>## BootPIG: Bootstrapping Zero-shot Personalized Image Generation   Capabilities in Pretrained Diffusion Models**Authors:Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik**Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.   The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts. [PDF](http://arxiv.org/abs/2401.13974v1) **Summary**BootPIG 利用预训练文本到图像扩散模型数据，仅需一小时即可训练，表现优于现有零次学习方法，并与测试时微调方法相当。**Key Takeaways**- BootPIG 提出了一种新的 BootPIG 架构，允许用户使用参考图像引导生成图像中概念的外观。- BootPIG 对预训练文本到图像扩散模型进行最小的修改，并使用单独的 UNet 模型将生成的图像引导至所需的外观。- BootPIG 使用来自预训练文本到图像模型、LLM 聊天代理和图像分割模型生成的数据，引入了允许我们在 BootPIG 架构中引导个性化功能的训练过程。- 与需要数天预训练的现有方法相比，BootPIG 架构可以在大约 1 小时内完成训练。- DreamBooth 数据集上的实验表明，BootPIG 的性能优于现有的零次学习方法，同时与测试时微调方法相当。- 通过用户研究，我们验证了 BootPIG 生成的图像在保持对参考对象外观的忠实度和与文本提示保持一致方面优于现有方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：BootPIG：利用预训练扩散模型引导零样本个性化图像生成能力</li><li>作者：Senthil Purushwalkam、Akash Gokul、Shafiq Joty、Nikhil Naik</li><li>作者单位：Salesforce AI Research（仅翻译为中文）</li><li>关键词：文本到图像生成、个性化图像生成、零样本学习、引导扩散模型</li><li>论文链接：https://arxiv.org/abs/2401.13974，Github 链接：无</li><li><p>摘要：（1）研究背景：近年来，文本到图像生成模型取得了显著进展，可以根据文本描述生成逼真的图像。然而，这些模型通常无法根据特定主题生成个性化的图像。（2）过去方法与问题：现有方法通常需要大量的预训练数据或复杂的训练过程，并且在生成个性化图像时往往存在保真度低、与文本描述不一致等问题。（3）研究方法：本文提出了一种新的架构 BootPIG，它允许用户提供参考图像来引导生成图像中概念的外观。BootPIG 对预训练的文本到图像扩散模型进行微小的修改，并利用一个单独的 U-Net 模型来引导生成过程，使其朝着期望的外观方向发展。此外，本文还引入了一种训练过程，可以使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据来引导 BootPIG 架构中的个性化能力。（4）实验结果：在 DreamBooth 数据集上的实验表明，BootPIG 在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。</p></li><li><p>方法：(1) BootPIG 架构：BootPIG 架构由一个预训练的文本到图像扩散模型和一个引导 U-Net 模型组成。预训练的文本到图像扩散模型负责从文本描述中生成图像，而引导 U-Net 模型则负责将参考图像中的概念外观引导到生成图像中。(2) 引导训练过程：BootPIG 的训练过程包括两个阶段。在第一阶段，预训练的文本到图像扩散模型被微调，使其能够从文本描述中生成更逼真的图像。在第二阶段，引导 U-Net 模型被训练，使其能够将参考图像中的概念外观引导到生成图像中。(3) 数据生成：BootPIG 的训练过程使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据。这些数据包括文本描述、参考图像和分割掩码。(4) 推理过程：在推理过程中，BootPIG 使用预训练的文本到图像扩散模型和引导 U-Net 模型来生成图像。首先，预训练的文本到图像扩散模型从文本描述中生成一个图像。然后，引导 U-Net 模型将参考图像中的概念外观引导到生成图像中。最后，生成的图像被输出。</p></li><li><p>结论：(1)：BootPIG 提出了一种新的架构，允许用户提供参考图像来引导生成图像中概念的外观，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。(2)：创新点：</p></li><li>BootPIG 提出了一种新的架构，允许用户提供参考图像来引导生成图像中概念的外观。</li><li>BootPIG 引入了一种训练过程，可以使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据来引导 BootPIG 架构中的个性化能力。</li><li>BootPIG 在 DreamBooth 数据集上的实验表明，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。性能：</li><li>BootPIG 在 DreamBooth 数据集上的实验表明，在保持对参考对象外观的保真度和与文本描述的一致性方面优于现有方法，同时与测试时微调方法相当。工作量：</li><li>BootPIG 的训练过程包括两个阶段，第一阶段微调预训练的文本到图像扩散模型，第二阶段训练引导 U-Net 模型。</li><li>BootPIG 的训练过程使用从预训练的文本到图像模型、LLM 聊天代理和图像分割模型生成的数据。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e2ecdf9f08a2cadce3096ead80db29d5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3f5a69af875397219d470fd2b55dde8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-02a980dac9e87a3930c4c6e9ef96072c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7981b6cc67ddcffe001849665e1b21c5.jpg" align="middle"></details><h2 id="HiCAST-Highly-Customized-Arbitrary-Style-Transfer-with-Adapter-Enhanced-Diffusion-Models"><a href="#HiCAST-Highly-Customized-Arbitrary-Style-Transfer-with-Adapter-Enhanced-Diffusion-Models" class="headerlink" title="HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced   Diffusion Models"></a>HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced   Diffusion Models</h2><p><strong>Authors:Hanzhang Wang, Haoran Wang, Jinze Yang, Zhongrui Yu, Zeke Xie, Lei Tian, Xinyan Xiao, Junjun Jiang, Xianming Liu, Mingming Sun</strong></p><p>The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for video diffusion model training, which significantly improve cross-frame temporal consistency in the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as comprehensive user studies demonstrate that our HiCAST outperforms the existing SoTA methods in generating visually plausible stylization results. </p><p><a href="http://arxiv.org/abs/2401.05870v1">PDF</a> </p><p><strong>摘要</strong><br>HiCAST 是一种新颖的任意风格迁移方法，能够根据语义线索来源显式自定义风格化结果。</p><p><strong>要点</strong></p><ul><li>HiCAST 基于潜在扩散模型 (LDM) 构建，精心设计用于吸收内容和风格实例作为 LDM 的条件。</li><li>引入了“风格适配器”，允许用户通过对齐 LDM 中的多级风格信息和内在知识来灵活地操作输出结果。</li><li>将 HiCAST 扩展到视频任意风格迁移，并提出了一种新的学习目标，显著提高了视频扩散模型训练中的跨帧时间一致性，同时保持了风格化强度。</li><li>与现有最先进的方法相比，HiCAST 在生成视觉上合理的风格化结果方面具有更好的表现。</li><li>定性和定量比较以及全面的用户研究表明，HiCAST 在生成视觉上合理的风格化结果方面优于现有最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HiCAST：高度定制的任意风格迁移</li><li>作者：Jiachen An, Shixiang Huang, Yuming Song, Dandan Dou, Wen Liu, Jinlong Luo</li><li>单位：无</li><li>关键词：Arbitrary Style Transfer, Diffusion Model, Style Customization, Video Stylization</li><li>链接：https://arxiv.org/abs/2306.09330, Github：None</li><li><p>摘要：（1）：任意风格迁移（AST）旨在将风格参考的艺术特征注入给定图像/视频中。现有方法通常专注于追求风格和内容之间的平衡，而忽略了对灵活和定制的风格化结果的重大需求，从而限制了它们的实际应用。（2）：为了解决这一关键问题，提出了一种新的 AST 方法，称为 HiCAST，它能够根据各种语义线索来源显式地定制风格化结果。具体来说，我们的模型基于潜在扩散模型（LDM）构建，并精心设计以吸收内容和风格实例作为 LDM 的条件。它的特点是引入了风格适配器，它允许用户通过对齐多级风格信息和 LDM 中的内在知识来灵活地操纵输出结果。最后，我们进一步扩展了我们的模型以执行视频 AST。利用了一种新的学习目标进行视频扩散模型训练，这在保持风格化强度的前提下显著提高了跨帧时间一致性。定性和定量比较以及全面的用户研究表明，我们的 HiCAST 在生成视觉上合理的风格化结果方面优于现有的 SoTA 方法。（3）：本论文提出的研究方法是：构建基于潜在扩散模型（LDM）的AST模型，引入风格适配器以实现灵活的风格化结果定制，扩展模型以执行视频AST，并利用新的学习目标进行视频扩散模型训练。（4）：本论文的方法在任意风格迁移任务上取得了优异的性能，在图像和视频风格化方面均优于现有最先进的方法。这些性能支持了论文的目标，即生成视觉上合理的风格化结果并实现灵活的风格化结果定制。</p></li><li><p>方法：（1）构建基于潜在扩散模型（LDM）的AST模型，采用预训练的VAE编码器和VGG-16网络作为内容编码器和风格编码器，并设计了风格适配器来实现灵活的风格化结果定制。（2）采用三阶段训练策略优化模型性能，包括图像模型微调阶段、适配器训练阶段和时间层训练阶段，并设计了混合监督损失函数来指导模型训练。（3）提出了一种新的学习目标进行视频扩散模型训练，通过引入和谐一致性损失来保持跨帧时间一致性，并添加时间层来对视频进行建模。</p></li><li><p>结论：（1）本工作通过构建基于潜在扩散模型（LDM）的AST模型，引入风格适配器以实现灵活的风格化结果定制，扩展模型以执行视频AST，并利用新的学习目标进行视频扩散模型训练，在任意风格迁移任务上取得了优异的性能，在图像和视频风格化方面均优于现有最先进的方法。这些性能支持了论文的目标，即生成视觉上合理的风格化结果并实现灵活的风格化结果定制。（2）创新点：（1）提出了基于潜在扩散模型（LDM）的AST模型，该模型能够根据各种语义线索来源显式地定制风格化结果。（2）设计了风格适配器，它允许用户通过对齐多级风格信息和LDM中的内在知识来灵活地操纵输出结果。（3）提出了新的学习目标进行视频扩散模型训练，这在保持风格化强度的前提下显著提高了跨帧时间一致性。性能：（1）在图像风格化任务上，HiCAST在FID和LPIPS指标上优于现有的最先进方法。（2）在视频风格化任务上，HiCAST在FID和LPIPS指标上也优于现有的最先进方法。（3）用户研究表明，HiCAST在生成视觉上合理的风格化结果方面优于现有的最先进方法。工作量：（1）构建基于潜在扩散模型（LDM）的AST模型。（2）设计风格适配器以实现灵活的风格化结果定制。（3）扩展模型以执行视频AST。（4）利用新的学习目标进行视频扩散模型训练。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-352507fbd77e3adcd733f2041bffbe47.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3a9d48ccea55e5c85c44aac94261c324.jpg" align="middle"><img src="https://picx.zhimg.com/v2-16386b8ae01c7a44b1aac3c30a708331.jpg" align="middle"></details><h2 id="Style-Injection-in-Diffusion-A-Training-free-Approach-for-Adapting-Large-scale-Diffusion-Models-for-Style-Transfer"><a href="#Style-Injection-in-Diffusion-A-Training-free-Approach-for-Adapting-Large-scale-Diffusion-Models-for-Style-Transfer" class="headerlink" title="Style Injection in Diffusion: A Training-free Approach for Adapting   Large-scale Diffusion Models for Style Transfer"></a>Style Injection in Diffusion: A Training-free Approach for Adapting   Large-scale Diffusion Models for Style Transfer</h2><p><strong>Authors:Jiwoo Chung, Sangeek Hyun, Jae-Pil Heo</strong></p><p>Despite the impressive generative capabilities of diffusion models, existing diffusion model-based style transfer methods require inference-stage optimization (e.g. fine-tuning or textual inversion of style) which is time-consuming, or fails to leverage the generative ability of large-scale diffusion models. To address these issues, we introduce a novel artistic style transfer method based on a pre-trained large-scale diffusion model without any optimization. Specifically, we manipulate the features of self-attention layers as the way the cross-attention mechanism works; in the generation process, substituting the key and value of content with those of style image. This approach provides several desirable characteristics for style transfer including 1) preservation of content by transferring similar styles into similar image patches and 2) transfer of style based on similarity of local texture (e.g. edge) between content and style images. Furthermore, we introduce query preservation and attention temperature scaling to mitigate the issue of disruption of original content, and initial latent Adaptive Instance Normalization (AdaIN) to deal with the disharmonious color (failure to transfer the colors of style). Our experimental results demonstrate that our proposed method surpasses state-of-the-art methods in both conventional and diffusion-based style transfer baselines. </p><p><a href="http://arxiv.org/abs/2312.09008v1">PDF</a> 16 pages</p><p><strong>Summary</strong><br>无优化扩散图像风格迁移，通过预训练模型操控自注意力层的特性，实现细致风格迁移。</p><p><strong>Key Takeaways</strong></p><ul><li>本文提出的方法无需优化即可将风格迁移到预训练的大规模扩散模型上。</li><li>本文操纵自注意层的特性，使风格图像的特征代替内容的特征。</li><li>该方法具有若干优点，包括保留内容，并根据内容和风格图像之间的局部纹理相似性进行风格迁移。</li><li>本文提出查询保留和注意力温度缩放，以减轻对原始内容的破坏，并引入初始潜在自适应实例归一化 (AdaIN) 来处理不和谐的色彩。</li><li>实验结果表明，该方法优于传统和基于扩散的风格迁移基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：扩散中的风格注入：一种无训练方法</li><li>作者：Sung Kwon An, Dongwon Kim, Junyoung Seo, Youngjoon Yoo</li><li>单位：韩国科学技术院</li><li>关键词：艺术风格迁移，扩散模型，自注意力，图像生成</li><li>论文链接：None，Github 代码链接：None</li><li><p>摘要：（1）研究背景：扩散模型在生成图像方面取得了令人印象深刻的成果，但现有的基于扩散模型的风格迁移方法需要推理阶段的优化（例如，对风格进行微调或文本反演），这非常耗时，并且无法利用大规模扩散模型的生成能力。（2）过去的方法及其问题：现有方法需要推理阶段的优化，这非常耗时，并且无法利用大规模扩散模型的生成能力。（3）研究方法：为了解决这些问题，我们提出了一种新颖的艺术风格迁移方法，该方法基于预训练的大规模扩散模型，无需任何优化。具体来说，我们将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。这种方法为风格迁移提供了几个理想的特性，包括 1）通过将相似风格转移到相似图像块中来保留内容，以及 2）基于内容和风格图像之间局部纹理（例如边缘）的相似性来转移风格。此外，我们引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化 (AdaIN) 来处理不和谐的颜色（无法转移风格的颜色）。（4）方法性能：我们的实验结果表明，我们提出的方法在传统和基于扩散的风格迁移基准中都优于最先进的方法。</p></li><li><p>方法：（1）提出了一种新颖的艺术风格迁移方法，该方法基于预训练的大规模扩散模型，无需任何优化。（2）将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。（3）引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化(AdaIN)来处理不和谐的颜色（无法转移风格的颜色）。</p></li><li><p>结论：（1）：本工作解决了基于扩散模型的风格迁移方法面临的挑战，这些方法通常需要耗时的优化步骤或难以利用大规模扩散模型的生成潜力。为此，我们提出了利用预训练的大规模扩散模型的方法，无需任何优化。（2）：创新点：</p></li><li>将自注意力层的特征作为交叉注意力机制工作的方式进行操作；在生成过程中，用风格图像的键和值替换内容的键和值。</li><li>引入了查询保留和注意力温度缩放来减轻破坏原始内容的问题，并引入了初始潜在自适应实例归一化(AdaIN)来处理不和谐的颜色（无法转移风格的颜色）。性能：</li><li>在传统和基于扩散的风格迁移基准中，我们的方法优于最先进的方法。工作量：</li><li>该方法无需推理阶段的优化，因此可以利用大规模扩散模型的生成能力，从而减少了工作量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3c5ea0ed861e220177fdc07f214f3694.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0c9148fdb00478b35cac320276a8fc70.jpg" align="middle"><img src="https://pica.zhimg.com/v2-82f6f6c5f9fd21bc24d5f8b3ab902752.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ede5506ccc5b19f67905edc640f55e57.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4fd944f7f8c142e744a1d02eb4176d52.jpg" align="middle"></details><h2 id="ArtBank-Artistic-Style-Transfer-with-Pre-trained-Diffusion-Model-and-Implicit-Style-Prompt-Bank"><a href="#ArtBank-Artistic-Style-Transfer-with-Pre-trained-Diffusion-Model-and-Implicit-Style-Prompt-Bank" class="headerlink" title="ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and   Implicit Style Prompt Bank"></a>ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and   Implicit Style Prompt Bank</h2><p><strong>Authors:Zhanjie Zhang, Quanwei Zhang, Guangyuan Li, Wei Xing, Lei Zhao, Jiakai Sun, Zehua Lan, Junsheng Luan, Yiling Huang, Huaizhong Lin</strong></p><p>Artistic style transfer aims to repaint the content image with the learned artistic style. Existing artistic style transfer methods can be divided into two categories: small model-based approaches and pre-trained large-scale model-based approaches. Small model-based approaches can preserve the content strucuture, but fail to produce highly realistic stylized images and introduce artifacts and disharmonious patterns; Pre-trained large-scale model-based approaches can generate highly realistic stylized images but struggle with preserving the content structure. To address the above issues, we propose ArtBank, a novel artistic style transfer framework, to generate highly realistic stylized images while preserving the content structure of the content images. Specifically, to sufficiently dig out the knowledge embedded in pre-trained large-scale models, an Implicit Style Prompt Bank (ISPB), a set of trainable parameter matrices, is designed to learn and store knowledge from the collection of artworks and behave as a visual prompt to guide pre-trained large-scale models to generate highly realistic stylized images while preserving content structure. Besides, to accelerate training the above ISPB, we propose a novel Spatial-Statistical-based self-Attention Module (SSAM). The qualitative and quantitative experiments demonstrate the superiority of our proposed method over state-of-the-art artistic style transfer methods. </p><p><a href="http://arxiv.org/abs/2312.06135v1">PDF</a> Accepted by AAAI2024</p><p><strong>摘要</strong><br>艺术库：一种通过可训练参数矩阵学习艺术知识并作为视觉提示指导模型生成写实艺术风格图像的艺术风格迁移框架。</p><p><strong>要点</strong></p><ul><li>艺术风格迁移旨在用习得的艺术风格重新绘制内容图像。</li><li>现有艺术风格迁移方法可分为基于小模型和基于预训练大规模模型两类。</li><li>基于小模型的方法可以保留内容结构，但无法生成高度逼真的风格化图像，并引入伪影和不和谐的图案；基于预训练大规模模型的方法可以生成高度逼真的风格化图像，但难以保留内容结构。</li><li>为了解决上述问题，我们提出了一种新颖的艺术风格迁移框架艺术库，以在保留内容图像的内容结构的同时生成高度逼真的风格化图像。</li><li>为了充分挖掘预训练大规模模型中嵌入的知识，我们设计了一个隐式风格提示库（ISPB），这是一个可训练参数矩阵集，用于学习和存储艺术品集合中的知识，并作为视觉提示指导预训练大规模模型生成高度逼真的风格化图像，同时保留内容结构。</li><li>此外，为了加速上述 ISPB 的训练，我们提出了一种新颖的空间统计自注意力模块（SSAM）。</li><li>定性和定量实验表明，我们提出的方法优于最先进的艺术风格迁移方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：ArtBank：预训练扩散模型和隐式风格提示库的艺术风格迁移</li><li>作者：Zhanjie Zhang<em>, Quanwei Zhang</em>, Guangyuan Li, Wei Xing†, Lei Zhao†, Jiakai Sun, Zehua Lan, Junsheng Luan, Yiling Huang, Huaizhong Lin†</li><li>单位：浙江大学智能视觉实验室</li><li>关键词：艺术风格迁移、预训练扩散模型、隐式风格提示库</li><li>论文链接：https://arxiv.org/abs/2312.06135   Github 代码链接：https://github.com/Jamie-Cheung/ArtBank</li><li><p>摘要：（1）研究背景：艺术风格迁移旨在将学习到的风格迁移到任意内容图像上以创建新的艺术图像。现有的艺术风格迁移方法可分为基于小模型的方法和基于预训练大规模模型的方法。（2）过去的方法及其问题：基于小模型的方法可以保留内容结构，但无法生成高度逼真的风格化图像，并且会引入伪影和不和谐的图案；基于预训练大规模模型的方法可以生成高度逼真的风格化图像，但难以保留内容结构。（3）研究方法：提出了一种新的艺术风格迁移框架 ArtBank，以生成高度逼真的风格化图像，同时保留内容图像的内容结构。具体来说，为了充分挖掘预训练大规模模型中嵌入的知识，设计了一个隐式风格提示库 (ISPB)，它是一组可训练的参数矩阵，用于从艺术品集中学习和存储知识，并作为视觉提示来指导预训练的大规模模型生成高度逼真的风格化图像，同时保留内容结构。此外，为了加速训练上述 ISPB，提出了一种新的基于空间统计的自注意力模块 (SSAM)。（4）方法的性能：定性和定量实验表明，所提出的方法优于最先进的艺术风格迁移方法。</p></li><li><p>方法：(1) 隐式风格提示库（ISPB）：ISPB 是一组可训练的参数矩阵，用于从艺术品集中学习和存储知识。ISPB 可以通过冻结预训练大规模模型的参数并训练 ISPB 来获得。(2) 空间统计自注意力模块（SSAM）：SSAM 是一种新的注意力机制，可以加速 ISPB 的训练。SSAM 可以从空间和统计的角度学习和评估参数矩阵的变化值。(3) 艺术风格迁移框架（ArtBank）：ArtBank 是一个新的艺术风格迁移框架，它包括一个不可训练的部分（预训练大规模模型）和一个可训练的部分（隐式风格提示库）。ArtBank 可以通过冻结预训练大规模模型的参数并训练 ISPB 来获得。</p></li><li><p>结论：（1）：本文提出了一种新颖的艺术风格迁移框架 ArtBank，该框架可以解决从预训练的大规模模型中挖掘知识的挑战，从而生成高度逼真的风格化图像，同时保留内容图像的内容结构。（2）：创新点：</p></li><li>提出了一种新的隐式风格提示库（ISPB），该库可以从艺术品集中学习和存储知识，并作为视觉提示来指导预训练的大规模模型生成高度逼真的风格化图像，同时保留内容结构。</li><li>提出了一种新的基于空间统计的自注意力模块（SSAM），该模块可以加速 ISPB 的训练。</li><li>提出了一种新的艺术风格迁移框架 ArtBank，该框架包括一个不可训练的部分（预训练大规模模型）和一个可训练的部分（隐式风格提示库）。性能：</li><li>定性和定量实验表明，所提出的方法优于最先进的艺术风格迁移方法。工作量：</li><li>本文的工作量较大，需要训练一个预训练的大规模模型和一个隐式风格提示库。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-540d697ad9011eb3502589a451edc412.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-73cec781e9284cf52fa225a24e917388.jpg" align="middle"><img src="https://picx.zhimg.com/v2-15459584273510feb907035336d2f908.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c20a1ae8cef0cef555200bc6ee29aaa1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7812ebd2783d290dae9431dcaa43e169.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-01-30  Diffutoon High-Resolution Editable Toon Shading via Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>Failed building wheel for PyAudio  解决方法</title>
    <link href="https://kedreamix.github.io/2024/01/28/Note/pyaudio/"/>
    <id>https://kedreamix.github.io/2024/01/28/Note/pyaudio/</id>
    <published>2024-01-28T06:42:50.000Z</published>
    <updated>2024-01-28T06:46:11.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Failed-building-wheel-for-PyAudio-解决方法"><a href="#Failed-building-wheel-for-PyAudio-解决方法" class="headerlink" title="Failed building wheel for PyAudio  解决方法"></a>Failed building wheel for PyAudio  解决方法</h2><p>有时候在安装pyaudio的时候，总是有时候遇见一些错误，如下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) ... error</span><br><span class="line">  error: subprocess-exited-with-error</span><br><span class="line"></span><br><span class="line">  × Building wheel <span class="keyword">for</span> pyaudio (pyproject.toml) did not run successfully.</span><br><span class="line">  │ <span class="built_in">exit</span> code: 1</span><br><span class="line">  ╰─&gt; [18 lines of output]</span><br><span class="line">      running bdist_wheel</span><br><span class="line">      running build</span><br><span class="line">      running build_py</span><br><span class="line">      creating build</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310</span><br><span class="line">      creating build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      copying src/pyaudio/__init__.py -&gt; build/lib.linux-x86_64-cpython-310/pyaudio</span><br><span class="line">      running build_ext</span><br><span class="line">      building <span class="string">'pyaudio._portaudio'</span> extension</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src</span><br><span class="line">      creating build/temp.linux-x86_64-cpython-310/src/pyaudio</span><br><span class="line">      gcc -pthread -B anaconda3/envs/ernerf/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -O2 -isystem anaconda3/envs/ernerf/include -fPIC -I/usr/local/include -I/usr/include -Ianaconda3/envs/ernerf/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-cpython-310/src/pyaudio/device_api.o</span><br><span class="line">      src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory</span><br><span class="line">          9 | <span class="comment">#include "portaudio.h"</span></span><br><span class="line">            |          ^~~~~~~~~~~~~</span><br><span class="line">      compilation terminated.</span><br><span class="line">      error: <span class="built_in">command</span> <span class="string">'/usr/bin/gcc'</span> failed with <span class="built_in">exit</span> code 1</span><br><span class="line">      [end of output]</span><br><span class="line"></span><br><span class="line">  note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">  ERROR: Failed building wheel <span class="keyword">for</span> pyaudio</span><br><span class="line">Successfully built python_speech_features</span><br><span class="line">Failed to build pyaudio</span><br><span class="line">ERROR: Could not build wheels <span class="keyword">for</span> pyaudio, <span class="built_in">which</span> is required to install pyproject.toml-based projects</span><br></pre></td></tr></tbody></table></figure><p>如果单纯查后面这一句，会发现找不到什么错误，最后我找到了对应的解决办法，实际上是linux有一些库没安装上，用root权限装一下即可</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有些人说这样即可</span></span><br><span class="line">sudo apt-get install portaudio19-dev</span><br><span class="line"><span class="comment"># 如果不行就试一下这样</span></span><br><span class="line">sudo apt-get install libasound-dev portaudio19-dev libportaudio2 libportaudiocpp0</span><br></pre></td></tr></tbody></table></figure><p>这样安装完以后，我们就可以正常安装pyaudio了</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyaudio</span><br></pre></td></tr></tbody></table></figure><p>我也在github上看到的相关帖子，大家也可以参考：<a href="https://github.com/ardha27/AI-Waifu-Vtuber/issues/49">https://github.com/ardha27/AI-Waifu-Vtuber/issues/49</a>，而且这里面有个windows的解决方法，还蛮有趣，我还没试过</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install pipwin</span><br><span class="line">pipwin install pyaudio</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Failed-building-wheel-for-PyAudio-解决方法&quot;&gt;&lt;a href=&quot;#Failed-building-wheel-for-PyAudio-解决方法&quot; class=&quot;headerlink&quot; title=&quot;Failed building </summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Linux" scheme="https://kedreamix.github.io/tags/Linux/"/>
    
    <category term="Note" scheme="https://kedreamix.github.io/tags/Note/"/>
    
  </entry>
  
  <entry>
    <title>3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting</title>
    <link href="https://kedreamix.github.io/2024/01/25/Paper/3DGS%20Survey/"/>
    <id>https://kedreamix.github.io/2024/01/25/Paper/3DGS%20Survey/</id>
    <published>2024-01-25T09:24:11.000Z</published>
    <updated>2024-01-30T05:04:13.228Z</updated>
    
    <content type="html"><![CDATA[<p>今天想介绍的是<code>ZJU</code>带来的<code>3DGS</code>的首篇综述<code>A Survey on 3D Gaussian Splatting</code> 这是论文链接 <a href="https://arxiv.org/abs/2401.03890">arXiv:2401.03890</a>，结合一些资料，趁这个机会好好学习一下3DGS，加油入坑！！！</p><p>首先说一些自己的理解，3DGS之所以爆火，很大程度在于他的实时性，而这一部分极大程度得益于他定制的算法与自定义 CUDA 内核。除此之外，<strong>Gaussian Splatting</strong>根本不涉及任何神经网络，甚至没有一个小型的 MLP，也没有什么 “神经”的东西，场景本质上只是空间中的一组点。在大家都在研究数十亿个参数组成的模型的人工智能世界里，这种方法越来越受欢迎，令人耳目一新。它的想法源于 “Surface splatting”（2001 年），说明经典的计算机视觉方法仍然可以激发相关的解决方案。它简单明了的表述方式使<strong>Gaussian Splatting</strong>特别容易解释，这也是为什么在某些应用中选择它而不是 NeRFs。</p><h2 id="引言-INTRODUCTION"><a href="#引言-INTRODUCTION" class="headerlink" title="引言 INTRODUCTION"></a>引言 INTRODUCTION</h2><p>NeRF自从2020年开始，在多视角合成中做出来巨大的贡献，他利用神经网络，实现了空间坐标到颜色和密度的映射的，然NeRF的方法是计算密集型的，通常需要大量的训练时间和大量的渲染资源，特别是高分辨率的输出。</p><p><img src="https://pic1.zhimg.com/80/v2-c828848317a156fc6dd17c9a5310dd03.png" alt="NeRF"></p><p>针对这些问题，3DGS出现了，3DGS 采用显式表示和高度并行的工作流程，有利于更高效的计算和渲染，其创新在于其独特地融合了可微分管道和基于点的渲染技术的优点，通过用可学习的 3D 高斯函数表示场景，保留了连续体积辐射场的理想特性，这对于高质量图像合成至关重要，同时避免了与空白空间渲染相关的计算开销，这是传统 NeRF 方法的常见缺点，而3DGS很好的解决了这个问题，在不影响视觉质量的情况下达到了实时渲染。</p><p>论文中也发现，自3DGS出现以来，2023年有很多的论文在arXiv中挂出来，所以基于此也写了这样一个综述，同时促进3DGS领域的进一步研究和创新</p><p><img src="https://picx.zhimg.com/80/v2-167cd8779af5c5550c15156e2b9b52c0.png" alt="The number of papers on 3DGS is increasing every month."></p><p>以下是论文架构的图，论文的大概架构如下所示，可以看到这篇综述撰写的一个逻辑，还是非常好的，接下来，我会顺着这个架构进行解读论文来学习</p><ul><li>第2部分：主要是一些问题描述和相关研究领域的一些简要的背景</li><li>第3部分：介绍3DGS，包括3DGS的多视角的合成和3DGS的优化</li><li>第4部分：3DGS 产生重大影响的各种应用领域和任务，展示了其多功能性</li><li>第5部分：对3DGS进行了一些比较和分析</li><li>第6、7部分：对一些未来的开放性工作进行总结和调查</li></ul><p><img src="https://picx.zhimg.com/80/v2-2ebb7a7fb8548ff7a8e30bc62e875ebe.png" alt="Structure of the overall review."></p><h2 id="背景-BACKGROUND"><a href="#背景-BACKGROUND" class="headerlink" title="背景 BACKGROUND"></a>背景 BACKGROUND</h2><p>背景主要分两部分讲解</p><ul><li>辐射场的概念：隐式和显式</li><li>有关辐射场的场景重建、渲染等领域相关介绍</li></ul><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="辐射场"><a href="#辐射场" class="headerlink" title="辐射场"></a>辐射场</h4><p>辐射场是实际上是对三维空间中光分布的表示，它捕捉了光与环境中的表面和材质相互作用的方式。从数学上来说，辐射场可被描述为一个函数$L:\mathbb{R}^5\to\mathbb{R}^+$, 其中$L(x,y,z,\theta,\psi)$将点$(x,y,z)$和球坐标下的方向$(\theta,\phi)$映射为非负的辐射值。辐射场有显示表达和隐式表达，可用于场景表示和渲染。</p><h4 id="隐式辐射场"><a href="#隐式辐射场" class="headerlink" title="隐式辐射场"></a>隐式辐射场</h4><p>隐式辐射场是辐射场中的一种，在表示场景中的光分布时，不需显式定义场景的集合形状。这里面最常见的就是NeRF，使用神经网络来学习连续的体积表示。在NeRF中，使用MLP 网络用于将一组空间坐标 $(x, y, z)$ 和观察方向 $(\theta,\phi)$ 映射到颜色和密度值。任何点处的辐射不是显式存储的，而是通过查询神经网络实时计算得出。因此，该函数可以写成：</p><script type="math/tex; mode=display">L_\text{implicit}(x,y,z,\theta,\phi)=\text{NeuralNetwork}(x,y,z,\theta,\phi)</script><p>这种方式的好处是构建了一个可微且紧凑的复杂场景，但是由于我们总是需要对光线进行采样和体渲染的计算，会导致计算负载比较高。</p><h4 id="显式辐射场"><a href="#显式辐射场" class="headerlink" title="显式辐射场"></a>显式辐射场</h4><p>与隐式不同的是，显示是直接表示光在离散空间结构中的分布，比如体素网格或点云。该结构中的每个元素都存储了其在空间中相应位置的辐射信息，而不是像NeRF一样去执行查询的操作，所以他会更直接也更快的得到每个值，但是同时也需要更大内存使用和导致较低的分辨率。通常我们可以表示为：</p><script type="math/tex; mode=display">L_\text{explicit}{ ( x , y , z , \theta , \phi ) }=\text{DataStructure}[(x,y,z)]\cdot f(\theta,\phi)</script><p>其中，<code>DataStructure</code>可以是网格或点云，而$f(θ, ϕ)$是一个根据观察视线方向修改辐射的函数。</p><h4 id="3D-Gaussian-Splatting-（两全其美）"><a href="#3D-Gaussian-Splatting-（两全其美）" class="headerlink" title="3D Gaussian Splatting （两全其美）"></a>3D Gaussian Splatting （两全其美）</h4><p>3DGS通过利用3D 高斯函数作为其表示形式，充分利用了显示辐射场和隐式辐射场的优势。这些高斯函数被优化用于准确表示场景，结合了基于神经网络的优化和显式结构化数据存储的优点。这种混合方法能进行高质量渲染，同时具有更快的训练和实时性能，3D高斯表达可表示为：</p><script type="math/tex; mode=display">L_{\mathrm{3DGS}}(x,y,z,\theta,\phi)=\sum_{i}G(x,y,z,\mu_{i},\Sigma_{i})\cdot c_{i}(\theta,\phi)</script><p>其中 $G$ 是具有平均值 $μ_i$ 和协方差 $Σ_i$ 的高斯函数，$c$ 表示与视图相关的颜色。</p><h4 id="显式与隐式的理解"><a href="#显式与隐式的理解" class="headerlink" title="显式与隐式的理解"></a>显式与隐式的理解</h4><p>这里放一张理解显示隐式图像的图片，我还是觉得相当不错的</p><p><img src="https://pic1.zhimg.com/80/v2-e79d0183806753d34863598e544a0517.jpeg" alt="显式隐式表达"></p><h3 id="背景和术语"><a href="#背景和术语" class="headerlink" title="背景和术语"></a>背景和术语</h3><p>许多技术和研究学科与 <code>3DGS</code> 有着密切的关系，以下各节将对此进行简要介绍。</p><h4 id="场景重建与渲染"><a href="#场景重建与渲染" class="headerlink" title="场景重建与渲染"></a>场景重建与渲染</h4><p><strong>场景重建</strong>：从一组图像集合或其它数据建立场景的三维模型。</p><p><strong>渲染</strong>：将计算机可读取的信息（如场景中的3D物体）转化为图像。<br>早期技术基于光场生成逼真的图像，运动结构（SfM）与多视图立体匹配（MVS）算法通过从图像序列估计3D结构来增强光场。</p><h4 id="神经渲染和辐射场"><a href="#神经渲染和辐射场" class="headerlink" title="神经渲染和辐射场"></a>神经渲染和辐射场</h4><p><strong>神经渲染</strong>：将深度学习与传统图形技术结合生成逼真的图像。早期方法使用CNN估计混合权重或纹理空间解决方案。</p><p><strong>辐射场</strong>：一种函数表达，描述从各方向穿过空间各点的光的量。NeRF使用神经网络建模辐射场。</p><h4 id="体积表示和光线行进"><a href="#体积表示和光线行进" class="headerlink" title="体积表示和光线行进"></a>体积表示和光线行进</h4><p><strong>体积表达</strong>：不仅将物体和场景建模为表面，还将其其建模为充满材料或空白空间的体积。这样可以对如雾、烟或半透明材料进行更精确的渲染。</p><p><strong>光线行进</strong>：是体积表达渲染图像的技术，通过增量跟踪穿过“体”的光线来渲染图像。NeRF引入重要性采样和位置编码增强合成图像的质量，虽然能得到高质量的图像，但这一方法计算量大。</p><h4 id="基于点的渲染"><a href="#基于点的渲染" class="headerlink" title="基于点的渲染"></a>基于点的渲染</h4><p>基于点的渲染是一种使用点而非传统多边形来可视化3D场景的技术。该方法特别适用于渲染复杂、非结构化或稀疏的几何数据。点可以通过添加额外属性，如可学习的神经描述符来进行增强，并且可以高效地进行渲染，但这种方法可能会出现渲染中的空洞或混叠效应等问题。3DGS通过使用各向异性高斯进行更连贯的场景表达。</p><h2 id="用于显式辐射场的3DGS"><a href="#用于显式辐射场的3DGS" class="headerlink" title="用于显式辐射场的3DGS"></a>用于显式辐射场的3DGS</h2><p>3DGS能够实时渲染高分辨率的图像，并且不需要神经网络，是一个突破。</p><p>这一块主要围绕两块进行讲解</p><ul><li>3DGS的前向过程</li><li>3DGS的优化过程</li></ul><h3 id="学习3D高斯函数进行新视角合成"><a href="#学习3D高斯函数进行新视角合成" class="headerlink" title="学习3D高斯函数进行新视角合成"></a>学习3D高斯函数进行新视角合成</h3><p>假如现在有一个场景，目的是生成特定视角下的相机图像。NeRF对每一个像素使用光线行进和采样点，影响其实时性；而3DGS将3D高斯投影到图像平面，称为“泼溅”，如下图所示。然后对高斯进行排序并计算每个像素的值。NeRF和3DGS的渲染可视为互逆关系。</p><p><img src="https://pic1.zhimg.com/80/v2-9d5fff5c2390526cd03e5a14fd13f4fe.png" alt="3DGS的Splatting 泼溅"></p><p>这里面有个点很有意思，为什么说是互逆关系，我参考了知乎的一篇文章<a href="https://zhuanlan.zhihu.com/p/666465701">3D Gaussian Splatting中的数学推导</a>的说明，我觉得这个说的还不错。</p><blockquote><p> 首先，我们回忆一下体渲染的这个事情。假设读者跟我一样是从NeRF才接触体渲染的，那么回顾一下NeRF中，沿着一个像素，发出一条射线，然后这条射线“射向体数据”（在NeRF里就是沿着光线进行采样，然后查询采样点的属性）的过程。这个过程可以归结为一种<code>backward mapping</code>。</p><p> 所以很自然的，会有一种<code>forward mapping</code>的办法。形式上，就是将整个“体数据”投影到此时位姿所对应的图像平面。这种办法的前提就不能是用NeRF那种隐式表达了，需要一些显式的表达才能支持这样直接的投影。例如以三个顶点长成的三角面基元（primitive），然后将这些许多的三角面直接投影到成像平面上，判断哪些像素是什么颜色，当有多个三角形投影时，根据他们的“深度”来判断前后顺序，然后进行熟悉的alpha compositing。当然也会有其他基元，例如小的平面表示等等。</p><p> 无论是<code>backward mapping</code>还是<code>forward mapping</code>，这个过程都涉及到将连续的表示变成离散的。在<code>backward mapping</code>里，是对场进行采样；在<code>forward mapping</code>里，是需要直接生成出基元，这也是一种连续化为离散。为了理解在这个过程中，高斯分布为什么重要，我们需要牵扯到信号与系统中的概念。与混过数字信号处理考试不同的是，我们要清楚此时引入信号与系统里的工具的目的是什么。回想刚才三角面基元的情景，在实际情境中，我们其实都接触不到“连续”的表达，比如三角面，我们只会记录它的三个顶点。当投影完成后，我们只能做一些有限的操作来阻止“锯齿”，例如对结果进行一个模糊操作，这些操作一般都是局部的。我们这样做的目的，本质是“希望用离散的表达来重建原来的信号，进一步在重建好的信号上进行“resampling”。如果我们对处理后的结果，视觉上看起来没什么混叠或者锯齿上的问题，那就说明我们“resampling”是成功的。</p></blockquote><p>从下图也可以看到NeRF和Gaussian在概念上的区别，左边是NeRF沿着光线查询连续 MLP，右边是Gaussian一组与给定光线相关的离散的高斯分布</p><p><img src="https://picx.zhimg.com/80/v2-08473faff1a084b3de92e2a86f69f0fd.png" alt=""></p><p><img src="https://picx.zhimg.com/80/v2-37166011e5e81d299598141028acff42.png" alt="difference between NeRF and Gaussian Splatting"></p><p>首先简单介绍一下，3DGS是如何表示真实场景的，前面也有提过，在<strong>Gaussian Splatting</strong>中，3D世界用一组3D点表示，实际上是数百万个，大致在0.5到5百万之间。每个点是一个3D高斯，具有其独特的参数，这些参数是为每个场景拟合的，以便该场景的渲染与已知数据集图像紧密匹配，接下来就介绍他的属性。</p><p><img src="https://pica.zhimg.com/80/v2-f440b37ac00a08977b2b6e5514ffec1f.png" alt="Representing a 3D world"></p><ul><li><p><strong>3D高斯的属性</strong>： 一个3D高斯主要包括，中心（位置）$x,y,z$的均值$μ$、不透明度 $α$、3D 协方差矩阵 $Σ$ 和颜色 $c$（一般是RGB或者是球谐（SH）系数）。 其中$c$与视角有关，$c$ 由球谐函数表示。所有属性均可学习，都可以通过反向传播来学习和优化。</p></li><li><p><strong>视域剔除</strong>：给定特定的相机姿态，该步骤会判断哪些高斯位于相机的视锥外，并在后续步骤中剔除之，以节省计算。</p></li><li><p><strong>Splatting泼溅</strong>：实际上只是3D高斯（椭圆体）投影到2D图像空间（椭圆）中进行渲染。给定视图变换 $W$ 和3D协方差矩阵$\Sigma$，我们可以使用使用以下公式计算投影 2D 协方差矩阵 $\Sigma^{\prime}$</p><script type="math/tex; mode=display">\Sigma^{\prime}=JW\Sigma W^\top J^\top</script><p>其中 $J$ 为投影变换中仿射近似的雅可比矩阵。</p></li><li><p><strong>像素渲染</strong>：如果不考虑并行，采用最简单的方式：给定像素 $x$ 的位置，与其到所有重叠高斯函数的距离，即这些高斯函数的深度。这些可以通过观察变换 $W$ 计算出来，形成高斯函数的排序列表$N$。然后进行alpha混合，计算该像素的最终颜色：</p><script type="math/tex; mode=display">C=\sum_{i\in\mathcal{N}}c_i\alpha_i^{\prime}\prod_{j=1}^{i-1}\left(1-\alpha_j^{\prime}\right.)</script><p>其中 $c_i$ 是学习到的颜色，最终的不透明度 $\alpha_i^{\prime}$ 是学习的不透明度 $\alpha_i$ 与高斯的乘积:</p><script type="math/tex; mode=display">\alpha_i'=\alpha_i\times\exp\left(-\frac12(x'-\mu_i')^\top\Sigma_i'^{-1}(x'-\mu_i')\right)</script></li></ul><p>  其中 $x’$ 和 $μ’_i$ 是投影空间中的坐标，同时我也找了个gif来可视化了一下Gaussian Splatting对位置p的影响：</p><p>  <img src="/img/3dgs.gif" alt="3DGS"></p><p>  如果仔细看的话，我们会发现，实际上这个公式和<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">多变量正态分布的概率密度函数</a>十分相像，是忽略了带有协方差行列式的标准化项，而是用不透明度来加权。</p><script type="math/tex; mode=display">  (2\pi)^{-k/2}\det(\boldsymbol{\Sigma})^{-1/2}\exp\biggl(-\frac12(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\biggr)</script><p>  不过如果考虑并行的话加快速度，这种列表排序实际上很难并行化，所以很有可能这个渲染程度比NeRF还慢。为了实现实时渲染，3DGS也做了一个tradeoff，3DGS做出了一些让步来适应<strong>并行计算</strong>。</p><p>  <img src="https://picx.zhimg.com/80/v2-7cea6c4b183982cd921c0456d1f689b7.png" alt="Tiles(Patches)"></p><ul><li><p><strong>Tiles (Patches)</strong>：为避免逐像素计算出现的成本，3DGS改为<strong>patch</strong>级别的渲染。具体来说，首先将图像分割为多个不重叠的patch，称为<code>tile</code>，每个图块包含 16×16 像素，如下图所示。3DGS然后确定<code>tile</code>与投影高斯的相交情况，由于投影高斯可能会与多个<code>tile</code>相交，需要进行复制，并为每个复制体分配相关tile的标识符（如<code>tile</code>的ID）。(不用判断每个像素与高斯的距离，而是判断tile就简单多了)</p><p><img src="https://picx.zhimg.com/80/v2-c81242a6677621910801fcec4c0adbee.png" alt=""></p><p>从下图可以看到排序的结果，在排序中，高位是tile的ID，低位就是深度，一起进行排序，下面的图是AI葵视频的结果，还是很好理解的</p><p><img src="https://pic1.zhimg.com/80/v2-5c74958d484c1d2588c20c8c30b58411.png" alt="3DGS排序"></p><p><img src="https://picx.zhimg.com/80/v2-3d6e3aec3a86c1d94354458830dbf17f.png" alt="3DGS排序例子(AI葵)"></p></li><li><p><strong>并行渲染</strong>：复制后，3DGS（对应字节的无序列表）结合包含了相关的tile ID（对应字节的高位）和深度信息（对应字节的低位），如上图所示。由于每一块和每一像素的计算是独立的，所以可以基于CUDA编程的块和线程来实现并行计算，同时有利于访问公共共享内存并保持统一的读取顺序。排序后的列表可直接用于渲染（alpha混合），如下图所示。</p><p><img src="https://pic1.zhimg.com/80/v2-6393ea51f715d0d0baa880cd1890a549.png" alt="并行渲染"></p><p>总的来说，3DGS在前向过程中做出了一些近似计算，以提高计算效率并保留图像合成的高质量。</p></li></ul><h3 id="3DGS的优化"><a href="#3DGS的优化" class="headerlink" title="3DGS的优化"></a>3DGS的优化</h3><p>学习到这里，我们可能会有一个问题，怎么可能在空间中的一堆圆球中得到一个像样的图像的，确实是这样，如果没有进行优化，在渲染的时候就会出现很多伪影，从下图你可以看到。</p><p><img src="https://pic1.zhimg.com/80/v2-7ad69d962fb9a18d84747130af62fe15.png" alt="An example of renders of an under-optimized scene"></p><p>3DGS的核心是<strong>3D高斯集合的优化过程</strong>。一方面需要通过可微渲染来使高斯符合场景纹理，另一方面表达场景需要的高斯数量是未知的。这分别对应参数优化与密度控制两步，这两步在优化过程中交替进行。优化过程中，需要手动设置很多超参数。</p><h4 id="参数优化-Parameter-Optimization"><a href="#参数优化-Parameter-Optimization" class="headerlink" title="参数优化 Parameter Optimization"></a>参数优化 Parameter Optimization</h4><ul><li><p><strong>损失函数</strong>：图像合成后，计算渲染图像与真实图像的差异作为损失：</p><script type="math/tex; mode=display">\mathcal{L}=(1-\lambda)\mathcal{L}_1+\lambda\mathcal{L}_{D-SSIM}</script><p>其中 $λ$ 是权重因子。与 NeRF 的损失函数略有不同，由于光线行进成本高昂，NeRF 通常在像素级别而不是图像级别进行计算，而3DGS是图像级别的。</p></li><li><p><strong>参数更新</strong>：3D高斯的多数参数可通过反向传播直接更新，但对于协方差矩阵 $\Sigma$来说，需要半正定矩阵（这里面是一个定义，应该是多元正态分布的协方差矩阵是一个半正定矩阵），直接优化可能会产生非半正定矩阵，而只有半正定矩阵才有物理意义。因此，改为优化四元数$q$和3D向量$s$。将协方差矩阵分解：</p><script type="math/tex; mode=display">\Sigma=RSS^\top R^\top</script><p>其中$R$与$S$分别由$q$和$s$推导得到的旋转和缩放矩阵。</p><ul><li>$S$是一个对角缩放矩阵，含有3个参数</li><li>$R$是一个3x3的旋转矩阵，通过旋转四元数来表示</li></ul><p>对于不透明度$α$, 其计算图较为复杂：$(q,s)\to\Sigma\to\Sigma^{\prime}\to\alpha$。为避免自动微分的计算消耗，3DGS还推导了$q$与$s$的梯度，在优化过程中直接计算之。</p></li></ul><h4 id="密度控制-Density-Control"><a href="#密度控制-Density-Control" class="headerlink" title="密度控制 Density Control"></a>密度控制 Density Control</h4><ul><li><strong>初始化</strong>：3DGS建议从SfM产生的稀疏点云初始化或随机初始化高斯，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a> 库来完成这一步。。然后进行点的密集化和剪枝以控制3D高斯的密度。当由于某种原因无法获得点云时，可以使用随机初始化来代替，但可能会降低最终的重建质量。</li></ul><p><img src="https://picx.zhimg.com/80/v2-0d67e5748993593a04ed46f7519e972e_720w.png" alt="A sparse 3D point cloud produced by SfM, means initialization"></p><ul><li><p><strong>点密集化</strong>：在点密集化阶段，3DGS自适应地增加高斯的密度，以更好地捕捉场景的细节。该过程特别关注缺失几何特征或高斯过于分散的区域。密集化在一定数量的迭代后执行，比如100个迭代，针对在视图空间中具有较大位置梯度（即超过特定阈值）的高斯。其包括在未充分重建的区域克隆小高斯或在过度重建的区域分裂大高斯。对于克隆，创建高斯的复制体并朝着位置梯度移动。对于分裂，用两个较小的高斯替换一个大高斯，按照特定因子减小它们的尺度。这一步旨在在3D空间中寻求高斯的最佳分布和表示，增强重建的整体质量。</p><p>这一部分的意义是什么呢，因为SGD只能对现有点进行调整，但是在完全没有点或点太多的区域，很难找到好的参数，所以这就是点密集化的作用。</p></li><li><p><strong>点的剪枝</strong>：点的剪枝阶段移除冗余或影响较小的高斯，可以在某种程度上看作是一种正则化过程。一般消除几乎是透明的高斯（α低于指定阈值）和在世界空间或视图空间中过大的高斯。此外，为防止输入相机附近的高斯密度不合理地增加，这些高斯会在固定次数的迭代后将$\alpha$设置为接近0的值。该步骤在保证高斯的精度和有效性的情况下，能节约计算资源。</p></li></ul><p><img src="https://picx.zhimg.com/80/v2-58c80507588563289c26e2ea4066ad81.png" alt="Adaptive Gaussian densification scheme."></p><h3 id="用SH系数来表示颜色"><a href="#用SH系数来表示颜色" class="headerlink" title="用SH系数来表示颜色"></a>用SH系数来表示颜色</h3><p>在计算机图形学中，用球谐函数（Spherical Harmonics，简称SH）表示视角相关的颜色起着重要作用，最初是在Plenoxels中提出的。他能表示非兰伯特效应，比如金属表面的高光反射。不过这样也不是一定的，实际上也可以使用3个RGB值表示颜色，然后使用Gaussian Splatting。</p><p> 图形学全局环境光照技术与球谐函数息息相关，我们的环境光来源四面八方，可以理解为一个球面函数，当模拟漫反射环境光，我们用一张环境贴图进行采样，对每一个点进行半球采样出在这个像素上的颜色，<strong>球谐光照</strong>简单来说就是用几个系数存取了整张环境贴图包围在球上<strong>法线方向</strong>所对应的的颜色信息。在渲染过程中传入球谐系数。在模型上根据对应的法线信息，从球谐函数中获取对应的颜色信息。</p><p>球谐函数是定义在球面上的特殊函数，换句话说，可以对球面上的任意点计算这样一个函数并得到一个值。</p><p>这里我们简单理解一下，SH，球谐函数，归根到底只是一组基函数，至于这组基函数是怎么来的，不管他。简单点来说，每一个函数都可以由多个基函数组合起来，如果我们有很多基函数，我们可以通过对应的权重系数复原出原来的函数，不过本质上还是一个有损压缩，不一定那么准确，不过如果基函数越多，复原的函数越准确，但是计算量也变大了。</p><p>在球面基函数中，最多的就是球谐函数了。球谐函数有很多很好的性质，比如正交性，旋转不变性（这边就不介绍了）。正交性说明每个基函数都是独立的，每个基函数都不能用别的基函数加权得到。当SH的系数用的越多，那么表达能力就越强，跟原始的函数就越接近。（如果更详细的了解可以看看一些原理，我主要是宏观的了解SH是什么，简单理解就是他是一种颜色的表示）</p><p><img src="https://pic1.zhimg.com/80/v2-9e660f32e92e1897aa986b0ab2ce073e.png" alt=""></p><p>当用来描述不同方向光照的SH基函数，我们一般用二阶或者三阶，比如下面的例子就是3阶的</p><p><img src="https://picx.zhimg.com/80/v2-f6bfb715b846bf13c95013ca96c1d51d.png" alt=""></p><p>下面展示的是一个$l=2$和3阶的球谐函数，一共包括9个学习系数，我们可以根据点的视角得到相关颜色，可以看到最后是red红色分量。</p><p><img src="https://pica.zhimg.com/80/v2-8241e4f7092a89a158df31b8cde94d33.png" alt="得到l=2和9个学习系数的点的视角相关颜色（红色分量）的过程"></p><h3 id="3DGS-流程"><a href="#3DGS-流程" class="headerlink" title="3DGS 流程"></a>3DGS 流程</h3><p>最后根据论文的图来总结一下3DGS的流程</p><p><img src="https://pic1.zhimg.com/80/v2-fda180df51e9171e3e147f5b40e520b9.png" alt="3DGS 流程"></p><ol><li><p><strong>Structure from Motion</strong>：使用SfM从一组图像中估计出点云，可以直接调用 <a href="https://colmap.github.io/">COLMAP</a>  库操作</p><p><img src="https://picx.zhimg.com/80/v2-961548f1a56fb5bc81bc8b349472d8ab.png" alt="Structure from Motion"></p></li></ol><ol><li><p><strong>Convert to Gaussians</strong>：将每个点建模成一个 3D 高斯图像。从 SfM 数据中，我们能推断出每个高斯图像的位置和颜色。但如果是要得到更高质量的表征的话，还需要对每个高斯函数进行训练，以推断出更精细的位置和颜色，并推断出协方差和透明度。</p></li><li><p><strong>Training</strong>：与神经网络类似，我们使用随机梯度下降法进行训练，但这里没有神经网络的层的概念 (都是 3D 高斯函数)。</p><p>训练步骤如下:</p><ol><li>用当前所有可微高斯函数渲染出图像</li><li>根据渲染图像和真实图像之间的差异计算损失</li><li>根据损失调整每个高斯图像的参数</li><li>根据情况对当前相关高斯图像进行点的密度控制</li></ol><p>步骤 1-3 比较简单，下面我们稍微解释一下第 4 步的工作:</p><ul><li>如果某高斯图像的梯度很大 (即它错得比较离谱)，则对其进行分裂或克隆<ul><li>如果该高斯图像很小，则克隆它</li><li>如果该高斯图像很大，则将其分裂</li></ul></li><li>如果该高斯图像的 alpha 太低，则将其删除</li></ul><p>这么做能帮助高斯图像更好地拟合精细的细节，同时修剪掉不必要的高斯图像。</p></li><li><p><strong>Differentiable Gaussian Rasterization</strong>：3D Gaussian Splatting实际上是一种光栅化的方法，将数据成像到屏幕上，与其他方法相比，他有两个特点</p><ol><li>快</li><li>可微</li></ol><p>主要步骤如下：</p><ol><li>针对给定相机视角，把每个 3D 高斯投影到 2D。</li><li>按深度对高斯进行排序。</li><li>对每个像素，从前到后计算每个高斯在该像素点的值，并将所有值混合以得到最终像素值。</li></ol></li></ol><h3 id="3DGS-Limitations"><a href="#3DGS-Limitations" class="headerlink" title="3DGS Limitations"></a>3DGS Limitations</h3><p><strong>优点</strong></p><ol><li>高品质、逼真的场景</li><li>快速、实时的渲染</li><li>更快的训练速度</li></ol><p><strong>缺点</strong></p><ol><li>防止模型优化中的“破碎”的高斯：点太大、太长、冗余等</li><li>更高的显存使用率 (4GB 用于显示，12GB 用于训练)</li><li>更大的磁盘占用 (每场景 1GB+)</li><li>与现有渲染管线不兼容</li><li><del>只能重建静态场景（但是好像现在动态的Gaussian也出来了，所以这个不算缺点了）</del></li></ol><h2 id="应用领域和任务-APPLICATION-AREAS-AND-TASKS"><a href="#应用领域和任务-APPLICATION-AREAS-AND-TASKS" class="headerlink" title="应用领域和任务 APPLICATION AREAS AND TASKS"></a>应用领域和任务 APPLICATION AREAS AND TASKS</h2><h3 id="同时定位和建图（SLAM）"><a href="#同时定位和建图（SLAM）" class="headerlink" title="同时定位和建图（SLAM）"></a>同时定位和建图（SLAM）</h3><p>SLAM需要让设备实时理解自身位置并同时为环境建图，因此计算量大的表达技术难以应用。</p><p>传统SLAM使用点/surfel云或体素网格表达环境。3DGS的优势在于高效性（自适应控制高斯密度）、精确性（各向异性高斯能建模环境细节）、适应性（能用于各种尺度和复杂度的环境）。</p><h3 id="动态场景建模"><a href="#动态场景建模" class="headerlink" title="动态场景建模"></a>动态场景建模</h3><p>动态场景建模需要捕捉和表达场景随时间变化的的3D结构和外观。需要建立能精确反映场景中物体几何、运动和视觉方面的数字模型。4D高斯泼溅通过扩展3D高斯溅射的概念，引入时间维度，使得可以表达和渲染动态场景。现在也有一些方法在研究在动态场景中的一些编辑的功能，与3DGS进行交互。</p><h3 id="AI生成内容（AIGC）"><a href="#AI生成内容（AIGC）" class="headerlink" title="AI生成内容（AIGC）"></a>AI生成内容（AIGC）</h3><p>AIGC是人工智能自动创建或极大修改的数字内容，可以模仿、扩展或增强人类生成的内容。</p><p>3DGS的显式特性、实时渲染能力和可编辑水平使其与AIGC高度相关。例如，有方法使用3DGS与生成模型、化身或场景编辑结合，如3DGS-Avatar。</p><h3 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h3><p>自动驾驶的目标是在无人干涉的情况下导航并操作车辆，其主要目标是安全而高效地感知环境、做出决策和操作执行器。</p><p>其中，感知和理解环境需要实时重建驾驶场景，精确识别静态和动态物体，并理解其相互关系和运动。动态驾驶场景中，场景还会随时间连续变化。3DGS可以通过混合数据点（如激光雷达点）将场景重建为连贯表达，有利于处理数据点变化的密度，以及静态背景和动态物体的精确重建。</p><h2 id="性能比较-PERFORMANCE-COMPARISON"><a href="#性能比较-PERFORMANCE-COMPARISON" class="headerlink" title="性能比较 PERFORMANCE COMPARISON"></a>性能比较 PERFORMANCE COMPARISON</h2><p>在这一部分，针对3FGS在上述的领域上的一些性能评估。</p><h3 id="性能基准：定位"><a href="#性能基准：定位" class="headerlink" title="性能基准：定位"></a>性能基准：定位</h3><ul><li><p>数据集：Replica。</p></li><li><p>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</p></li><li>评估指标：均方根误差（RMSE）、绝对轨迹误差（ATE），测量传感器运动轨迹上真实位置与估计位置欧式距离的均方根。</li><li>结果：基于3D高斯的SLAM方法能超过基于NeRF的密集视觉SLAM。</li></ul><p><img src="https://picx.zhimg.com/80/v2-3277500ac5a850accdd2891db0595ae6.png" alt=""></p><h3 id="性能基准：静态场景渲染"><a href="#性能基准：静态场景渲染" class="headerlink" title="性能基准：静态场景渲染"></a>性能基准：静态场景渲染</h3><ul><li>数据集：Replica。</li><li>基准算法：Gaussian-SLAM、GS-SLAM、SplaTAM、GSS-SLAM。</li><li>评估指标：峰值信噪比(PSNR)、结构相似性(SSIM)、学习的感知图像patch相似性(LPIPS),衡量RGB渲染性能。</li><li>结果：基于3D高斯的方法能超过基于<strong>NeRF</strong>的方法。</li></ul><p><img src="https://picx.zhimg.com/80/v2-94d0eea6ab0c03f32c82802f00a8102d.png" alt=""></p><h3 id="性能基准：动态场景渲染"><a href="#性能基准：动态场景渲染" class="headerlink" title="性能基准：动态场景渲染"></a>性能基准：动态场景渲染</h3><ul><li>数据集：D-NeRF。</li><li>基准算法：CoGS、4D-GS、GauFRe、4DGS。</li><li>评估指标：PSNR、SSIM、LPIPS, 用于衡量RGB渲染性能。</li><li>结果：3DGS能大幅超过基于NeRF的SOTA。但静态版本的3DGS对动态场景的重建是失败的。</li></ul><p><img src="https://picx.zhimg.com/80/v2-288ca353912cbbd221a111ee553ab607_720w.png" alt=""></p><h3 id="性能基准：驾驶场景渲染"><a href="#性能基准：驾驶场景渲染" class="headerlink" title="性能基准：驾驶场景渲染"></a>性能基准：驾驶场景渲染</h3><ul><li>数据集：nuScences。</li><li>基准算法：DrivingGaussian。</li><li>评估指标：PSNR、SSIM、LPIPS*（LPIPS× 1000）, 用于衡量RGB渲染性能。</li><li>结果：3DGS方法能大幅超过基于NeRF的方法。</li></ul><p><img src="https://pic1.zhimg.com/80/v2-8b7bb910fbb86e3d1b23fc062260dc5d_720w.png" alt=""></p><h3 id="性能基准：数字虚拟人"><a href="#性能基准：数字虚拟人" class="headerlink" title="性能基准：数字虚拟人"></a>性能基准：数字虚拟人</h3><p>该任务的目标是从给定的多视角视频渲染人体化身模型。</p><ul><li>数据集：ZJU-MoCap。</li><li>基准算法：GART、Human101、HUGS、3DGS-Avatar。</li><li>评估指标：PSNR、SSIM、LPIPS* (LPIPS×1000) ,用于衡量RGB渲染性能$_{9}$</li><li>结果：基于3DGS的方法能在渲染质量和速度上均有优势。</li></ul><p><img src="https://pica.zhimg.com/80/v2-d916a05d315e576e3cef738aa2306226.png" alt=""></p><h2 id="未来研究方向-FUTURE-RESEARCH-DIRECTIONS"><a href="#未来研究方向-FUTURE-RESEARCH-DIRECTIONS" class="headerlink" title="未来研究方向 FUTURE RESEARCH DIRECTIONS"></a>未来研究方向 FUTURE RESEARCH DIRECTIONS</h2><ul><li><strong>数据高效的3DGS解决方案</strong>：从少样本中进行新视图生成和场景重建很重要。目前的方法有探究引入深度信息、密集概率分布、像素到高斯的映射来促进该能力，实际上就是引入更多的信息。。此外，在观测不足的区域，3DGS会产生伪影，可尝试在这些区域进行数据插值或积分。</li><li><strong>存储高效的3DGS解决方案</strong>：3DGS的可扩展性较差，在大尺度环境中需要大量的存储。需要优化训练阶段和模型的存储利用，而对于NeRF来说只需要存储学习到的MLP参数。可以探索更多高效的数据结构和先进的压缩技术，如Light-Gaussian等</li><li><strong>先进的渲染算法</strong>：目前3DGS的渲染算法较为简单直接，可见性算法会导致高斯深度/混合顺序的剧烈切换，需要实施更先进的渲染算法，更好模拟光与材料属性的复杂相互作用。可结合传统计算机图形学的方法。此外，还可探索逆渲染。</li><li><strong>优化与正则化</strong>： 各向异性高斯虽然有利于表示复杂几何体，但可能产生不希望的视觉伪影。例如，特别是在具有视角依赖外观的区域，大的3D高斯可能导致弹出伪影，突然出现或消失的视觉元素打破了沉浸感。使用正则化可以增加收敛速度，平滑视觉噪声或提高图像质量。此外，3DGS中大量的超参数也会影响3DGS的泛化性。在3DGS的规则化和优化方面存在相当大的探索潜力。</li><li><strong>3D高斯在网格重建中的应用</strong>：可探索3DGS在网格重建中的潜力，从而缩小体积渲染和传统基于表面的方法的差距，以便提出新的渲染技巧和应用。</li><li><strong>赋予3DGS更多可能性</strong>： 尽管3DGS具有显著潜力，但3DGS的全范围应用仍然未被充分挖掘。一个有前景的探索方向是用额外的属性增强3D高斯，例如为特定应用定制的语言和物理属性。此外，最近的研究开始揭示3DGS在多个领域的能力，例如相机姿态估计、捕捉手对象互动和不确定性量化。这些初步发现突出了跨学科学者进一步探索3DGS的重要机会。</li></ul><h2 id="参考文献-REFERENCES"><a href="#参考文献-REFERENCES" class="headerlink" title="参考文献 REFERENCES"></a>参考文献 REFERENCES</h2><ol><li>Kerbl, B., Kopanas, G., Leimkühler, T., &amp; Drettakis, G. (2023). <a href="https://arxiv.org/abs/2308.04079">3D Gaussian Splatting for Real-Time Radiance Field Rendering.</a> SIGGRAPH 2023.</li><li>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <a href="https://arxiv.org/abs/2003.08934">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</a> ECCV 2020.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/SurfaceSplatting-SIG01.pdf">Surface Splatting.</a> SIGGRAPH 2001</li><li>Luiten, J., Kopanas, G., Leibe, B., &amp; Ramanan, D. (2023). <a href="https://arxiv.org/abs/2308.09713">Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis.</a> International Conference on 3D Vision.</li><li>Zwicker, M., Pfister, H., van Baar, J., &amp; Gross, M. (2001). <a href="https://www.cs.umd.edu/~zwicker/publications/EWAVolumeSplatting-VIS01.pdf">EWA Volume Splatting.</a> IEEE Visualization 2001.</li><li>Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., &amp; Kanazawa, A. (2023). <a href="https://arxiv.org/abs/2112.05131">Plenoxels: Radiance Fields without Neural Networks.</a> CVPR 2022.</li><li><a href="https://towardsdatascience.com/a-comprehensive-overview-of-gaussian-splatting-e7d570081362">A Comprehensive Overview of Gaussian Splatting</a></li><li><a href="https://github.com/huggingface/blog/blob/main/gaussian-splatting.md">Introduction to 3D Gaussian Splatting</a></li><li><a href="https://docs.nerf.studio/nerfology/model_components/visualize_samples.html#d-frustum">Sample Representation</a></li><li><a href="https://zhuanlan.zhihu.com/p/664725693">《3D Gaussian Splatting for Real-Time Radiance Field Rendering》3D高斯的理论理解</a></li><li><a href="https://blog.csdn.net/weixin_45657478/article/details/135603696">【论文笔记】A Survey on 3D Gaussian Splatting</a></li></ol>]]></content>
    
    
    <summary type="html">3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting 今天想介绍的是`ZJU`带来的`3DGS`的首篇综述`A Survey on 3D Gaussian Splatting`</summary>
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>Arxiv学术论文查询接口详解</title>
    <link href="https://kedreamix.github.io/2024/01/24/Note/arXiv/"/>
    <id>https://kedreamix.github.io/2024/01/24/Note/arXiv/</id>
    <published>2024-01-24T05:40:00.000Z</published>
    <updated>2024-01-24T05:44:25.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Arxiv学术论文查询接口详解-转载"><a href="#Arxiv学术论文查询接口详解-转载" class="headerlink" title="Arxiv学术论文查询接口详解 转载"></a>Arxiv学术论文查询接口详解 转载</h1><blockquote><p>这篇博客主要转载自：<a href="https://hiyoungai.com/posts/arxiv%E5%AD%A6%E6%9C%AF%E8%AE%BA%E6%96%87%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AF%A6%E8%A7%A3/">Arxiv学术论文查询接口详解</a>，我觉得写的很好，所以我也不重新整理这一部分的API接口了。我后续使用这一部分的API接口来进行爬取得到最新的文章，还是非常方便的，所以也同时推荐给大家，能最快follow新文章</p></blockquote><p>Arxiv API 允许以编程方式获取 <a href="https://arxiv.org/">https://arxiv.org</a> 上的论文。API 的基本结构为：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/{method_name}?{parameters}</span><br></pre></td></tr></tbody></table></figure><h2 id="查询接口"><a href="#查询接口" class="headerlink" title="查询接口"></a>查询接口</h2><p>查询接口的的 method_name 为 query，下面是查询方法的参数，参数之间以 <em>&amp;</em> 分隔。</p><div class="table-container"><table><thead><tr><th style="text-align:center">parameters</th><th style="text-align:center">type</th><th style="text-align:center">defaults</th><th style="text-align:center">required</th></tr></thead><tbody><tr><td style="text-align:center">search_query</td><td style="text-align:center">string</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">id_list</td><td style="text-align:center">comma-delimited string（以 ‘，’ 分隔的字符串）</td><td style="text-align:center">None</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">start</td><td style="text-align:center">int</td><td style="text-align:center">0</td><td style="text-align:center">No</td></tr><tr><td style="text-align:center">max_results</td><td style="text-align:center">int</td><td style="text-align:center">10</td><td style="text-align:center">No</td></tr></tbody></table></div><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>如果 API 只包含 search_query（不包含 id_list），那么返回与 search_query 内容匹配的结果。</li><li>如果 API 只包含 id_list（不包含 search_query），那么返回 id_list 中每一项的结果。</li><li>如果 API 中包含了 search_query 和 id_list，那么返回在 id_list 中，并且与 search_query 匹配的文章。</li></ul><h3 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h3><p>通常情况下，一个查询可能有成百上千个返回结果。有时候我们不希望一次性查询到这么多数量，那么可以使用 <em>start</em> 和 <em>max_results</em> 两个字段来进行分页查询。</p><ul><li>start 是查询的起始索引，以 0 为第一个。</li><li>max_results 是查询返回的集合数。</li></ul><p>下面来举例说明一下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=10 (1)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=10&amp;max_results=10 (2)</span><br><span class="line">http://export.arxiv.org/api/query?search_query=all:electron&amp;start=20&amp;max_results=10 (3)</span><br></pre></td></tr></tbody></table></figure><p>查询结果分别为：</p><ol><li>0 - 9</li><li>10 - 19</li><li>20 - 29</li></ol><p>需要注意的是，由于 API 的限制，在多次调用 API 的情况下，建议每次调用的时间间隔为 3 秒。每次调用返回的最大数量为 2000 个。arXiv的硬限制约为 50,000 条记录； 对于与 50,000 多个原稿匹配的查询，无法接收全部结果. 解决这个问题的最简单的解决方案是将中断查询成小块，例如使用的时间片，与一系列日期的<code>submittedDate</code>或<code>lastUpdatedDate</code> 。</p><h3 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h3><p>对查询的结果进行排序有两个选项：<em>sortBy</em> 和 <em>sortOrder</em>。</p><ul><li>sortBy 的值有：relevance，lastUpdatedDate 和 submittedDate。</li><li>sortOrder 的值有：ascending 和 descending。</li></ul><p>示例：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=ti:%22electron%20thermal%20conductivity%22&amp;sortBy=lastUpdatedDate&amp;sortOrder=ascending</span><br></pre></td></tr></tbody></table></figure><h2 id="结果响应"><a href="#结果响应" class="headerlink" title="结果响应"></a>结果响应</h2><p>API 的 Response 内容中是以 <em>Atom 1.0</em> 为主体的，<em>Atom</em> 是 XML 的一种语法。下面分别来说明各个标签的含义。</p><h3 id="Feed-Metadata"><a href="#Feed-Metadata" class="headerlink" title="Feed Metadata"></a>Feed Metadata</h3><p>每个 Response 都会包含的内容：</p><ol><li>版本和命名空间</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Title：feed 的标题，通常为查询 URL 的字符串。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    ArXiv Query:  search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：查询的唯一标识（注意不是查询的每个文章的 id），保证每个查询 id 是唯一的。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link：查询 URL 的规范化。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=all:electron&amp;amp;id_list=&amp;amp;start=0&amp;amp;max_results=1"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Updated：提供了 feed 内容最后一次更新的时间。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-08T00:00:00-04:00&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Opensearch：扩展元素，包含了查询的返回数量以及分页信息等。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1000</span><br><span class="line">&lt;/opensearch:totalResults&gt;</span><br><span class="line">&lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   0</span><br><span class="line">&lt;/opensearch:startIndex&gt;</span><br><span class="line">&lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">   1</span><br><span class="line">&lt;/opensearch:itemsPerPage&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Entry-Metadata"><a href="#Entry-Metadata" class="headerlink" title="Entry Metadata"></a>Entry Metadata</h3><p>正常情况下，Response 返回结果中的 <em>feed</em> 标签会包含 0 个或者多个 <em>entry</em> 标签。每个 entry 表示一个查询的返回文章，下面分别说一下 entry 中的各个元素。</p><ol><li>Title：返回文章的标题</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-Electron Production at High Transverse Momenta <span class="keyword">in</span> ep Collisions at HERA</span><br><span class="line">&lt;/title&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Id：文章的 URL ，可以认为是文章的绝对路径。最后一个字段是文章的唯一标识符。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    http://arxiv.org/abs/hep-ex/0307015</span><br><span class="line">&lt;/id&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Published/Updated：文章的发布日期和更新日期。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;published xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-02-27T16:02:02-05:00</span><br><span class="line">&lt;/published&gt;</span><br><span class="line">&lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    2007-06-25T17:09:59-04:00</span><br><span class="line">&lt;/updated&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Summary：文章的摘要。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    Multi-electron production is studied at high electron transverse momentum</span><br><span class="line">    <span class="keyword">in</span> positron- and electron-proton collisions using the H1 detector at HERA.</span><br><span class="line">    The data correspond to an integrated luminosity of 115 pb-1. Di-electron</span><br><span class="line">    and tri-electron event yields are measured. Cross sections are derived <span class="keyword">in</span></span><br><span class="line">    a restricted phase space region dominated by photon-photon collisions. In</span><br><span class="line">    general good agreement is found with the Standard Model predictions.</span><br><span class="line">    However, <span class="keyword">for</span> electron pair invariant masses above 100 GeV, three</span><br><span class="line">    di-electron events and three tri-electron events are observed, compared to</span><br><span class="line">    Standard Model expectations of 0.30 \pm 0.04 and 0.23 \pm 0.04,</span><br><span class="line">    respectively.</span><br><span class="line">&lt;/summary&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Author：文章的作者，包含一个或者多个 name 标签，分别表示多个作者。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;H1 Collaboration&lt;/name&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Category：文章的分类。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"cs.AI"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br><span class="line">&lt;category xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> term=<span class="string">"I.2.6"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>Link，对于每个文章，最多有三个 link 元素，通过 ref 和 title 来区别，下面的表格表示 ref 和 title 的内容：</li></ol><div class="table-container"><table><thead><tr><th style="text-align:center">rel</th><th style="text-align:center">title</th><th style="text-align:center">refers to</th><th style="text-align:center">always present</th></tr></thead><tbody><tr><td style="text-align:center">alternate</td><td style="text-align:center">-</td><td style="text-align:center">abstract page</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">pdf</td><td style="text-align:center">pdf</td><td style="text-align:center">yes</td></tr><tr><td style="text-align:center">related</td><td style="text-align:center">doi</td><td style="text-align:center">resolved doi</td><td style="text-align:center">no</td></tr></tbody></table></div><p>例子：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/abs/hep-ex/0307015v1"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"pdf"</span> href=<span class="string">"http://arxiv.org/pdf/hep-ex/0307015v1"</span> rel=<span class="string">"related"</span> <span class="built_in">type</span>=<span class="string">"application/pdf"</span>/&gt;</span><br><span class="line">&lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> title=<span class="string">"doi"</span> href=<span class="string">"http://dx.doi.org/10.1529/biophysj.104.047340"</span> rel=<span class="string">"related"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:primary_category：主要分类的扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:primary_category xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span> term=<span class="string">"cs.LG"</span> scheme=<span class="string">"http://arxiv.org/schemas/atom"</span>/&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:comment：评论扩展元素。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:comment xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   23 pages, 8 figures and 4 tables</span><br><span class="line">&lt;/arxiv:comment&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:affiliation：作者从属关系。</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;author&gt;</span><br><span class="line">   &lt;name&gt;G. G. Kacprzak&lt;/name&gt;</span><br><span class="line">   &lt;arxiv:affiliation xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;NMSU&lt;/arxiv:affiliation&gt;</span><br><span class="line">&lt;/author&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:journal_ref：期刊说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:journal_ref xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   Eur.Phys.J. C31 (2003) 17-29</span><br><span class="line">&lt;/arxiv:journal_ref&gt;</span><br></pre></td></tr></tbody></table></figure><ol><li>arxiv:doi：doi 说明</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;arxiv:doi xmlns:arxiv=<span class="string">"http://arxiv.org/schemas/atom"</span>&gt;</span><br><span class="line">   10.1529/biophysj.104.047340</span><br><span class="line">&lt;/arxiv:doi&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h3><p>返回错误，如果请求的响应出现错误，会返回一个详细的错误信息。例如下面是一个错误 id 的信息：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;feed xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;</span><br><span class="line">  &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/query?search_query=&amp;amp;id_list=1234.12345"</span> rel=<span class="string">"self"</span> <span class="built_in">type</span>=<span class="string">"application/atom+xml"</span>/&gt;</span><br><span class="line">  &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;ArXiv Query: search_query=&amp;amp;id_list=1234.12345&lt;/title&gt;</span><br><span class="line">  &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/kvuntZ8c9a4Eq5CF7KY03nMug+Q&lt;/id&gt;</span><br><span class="line">  &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line">  &lt;opensearch:totalResults xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:totalResults&gt;</span><br><span class="line">  &lt;opensearch:startIndex xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;0&lt;/opensearch:startIndex&gt;</span><br><span class="line"></span><br><span class="line">  &lt;opensearch:itemsPerPage xmlns:opensearch=<span class="string">"http://a9.com/-/spec/opensearch/1.1/"</span>&gt;1&lt;/opensearch:itemsPerPage&gt;</span><br><span class="line">  &lt;entry xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">    &lt;<span class="built_in">id</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;http://arxiv.org/api/errors<span class="comment">#incorrect_id_format_for_1234.12345&lt;/id&gt;</span></span><br><span class="line">    &lt;title xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;Error&lt;/title&gt;</span><br><span class="line">    &lt;summary xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;incorrect <span class="built_in">id</span> format <span class="keyword">for</span> 1234.12345&lt;/summary&gt;</span><br><span class="line">    &lt;updated xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;2007-10-12T00:00:00-04:00&lt;/updated&gt;</span><br><span class="line"></span><br><span class="line">    &lt;<span class="built_in">link</span> xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span> href=<span class="string">"http://arxiv.org/api/errors#incorrect_id_format_for_1234.12345"</span> rel=<span class="string">"alternate"</span> <span class="built_in">type</span>=<span class="string">"text/html"</span>/&gt;</span><br><span class="line">    &lt;author xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;</span><br><span class="line">      &lt;name xmlns=<span class="string">"http://www.w3.org/2005/Atom"</span>&gt;arXiv api core&lt;/name&gt;</span><br><span class="line">    &lt;/author&gt;</span><br><span class="line">  &lt;/entry&gt;</span><br><span class="line">&lt;/feed&gt;</span><br></pre></td></tr></tbody></table></figure><p>下面提供了一些常见的错误：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Sample query 示例查询</strong></th><th style="text-align:center"><strong>Error Explanation 错误解释</strong></th></tr></thead><tbody><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=not_an_int">http://export.arxiv.org/api/query?start=not_an_int</a></td><td style="text-align:center"><code>start</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?start=-1">http://export.arxiv.org/api/query?start=-1</a></td><td style="text-align:center"><code>start</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=not_an_int">http://export.arxiv.org/api/query?max_results=not_an_int</a></td><td style="text-align:center"><code>max_results</code> 一定是个整数</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?max_results=-1">http://export.arxiv.org/api/query?max_results=-1</a></td><td style="text-align:center"><code>max_results</code> 必须 &gt;= 0</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=1234.1234">http://export.arxiv.org/api/query?id_list=1234.1234</a></td><td style="text-align:center">malformed id</td></tr><tr><td style="text-align:center"><a href="http://export.arxiv.org/api/query?id_list=cond—mat/0709123">http://export.arxiv.org/api/query?id_list=cond—mat/0709123</a></td><td style="text-align:center">malformed id</td></tr></tbody></table></div><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><p>python2.7 上的简单请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">url = <span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span></span><br><span class="line">data = urllib.urlopen(url).read()</span><br><span class="line"><span class="built_in">print</span> data</span><br></pre></td></tr></tbody></table></figure><p>python3 上的请求：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request <span class="keyword">as</span> libreq</span><br><span class="line"><span class="keyword">with</span> libreq.urlopen(<span class="string">'http://export.arxiv.org/api/query?search_query=all:electron&amp;start=0&amp;max_results=1'</span>) <span class="keyword">as</span> url:</span><br><span class="line">    r = url.read()</span><br><span class="line">    <span class="built_in">print</span>(r)</span><br></pre></td></tr></tbody></table></figure><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="查询的详细结构"><a href="#查询的详细结构" class="headerlink" title="查询的详细结构"></a>查询的详细结构</h3><p>在 arXiv 搜索引擎中，每篇文章都被划分为许多可以单独搜索的字段。 例如，可以搜索一篇文章的标题，以及作者列表、摘要、评论和期刊参考文献。 要搜索其中一个字段，只需在搜索词前加上字段前缀和冒号即可。 例如：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro</span><br></pre></td></tr></tbody></table></figure><p>下面的表格显示所有字段的前缀：</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>prefix</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center">ti</td><td style="text-align:center">Title</td></tr><tr><td style="text-align:center">au</td><td style="text-align:center">Author</td></tr><tr><td style="text-align:center">abs</td><td style="text-align:center">Abstract</td></tr><tr><td style="text-align:center">co</td><td style="text-align:center">Comment</td></tr><tr><td style="text-align:center">jr</td><td style="text-align:center">Journal Reference</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">Subject Category</td></tr><tr><td style="text-align:center">rn</td><td style="text-align:center">Report Number</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">Id (use <code>id_list</code> instead)</td></tr><tr><td style="text-align:center">all</td><td style="text-align:center">All of the above</td></tr></tbody></table></div><p>并且查询也支持布尔运算，假设我们希望找到作者 Adrian DelMaestro 的所有文章，其标题中也包含单词 checkerboard。 我们可以使用 AND 操作符构造下面的查询：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://export.arxiv.org/api/query?search_query=au:del_maestro+AND+ti:checkerboard</span><br></pre></td></tr></tbody></table></figure><p>下面是三种可能的布尔值：</p><ul><li>AND</li><li>OR</li><li>ANDNOT</li></ul><p>下面是特殊符号的含义以及转义字符：</p><div class="table-container"><table><thead><tr><th style="text-align:left">symbol</th><th style="text-align:left">encoding</th><th style="text-align:left">explanation</th></tr></thead><tbody><tr><td style="text-align:left">( )</td><td style="text-align:left">%28 %29</td><td style="text-align:left">用于为布尔运算符优先级对布尔表达式进行分组</td></tr><tr><td style="text-align:left">“ “</td><td style="text-align:left">%22 %22</td><td style="text-align:left">用于将多个单词组合成短语以搜索特定字段</td></tr><tr><td style="text-align:left">空格</td><td style="text-align:left">+</td><td style="text-align:left">用于扩展<code>search_query</code> 包含多个字段</td></tr></tbody></table></div><h3 id="返回的详细结构"><a href="#返回的详细结构" class="headerlink" title="返回的详细结构"></a>返回的详细结构</h3><p>下表列出了返回的 Atom 结果的每个元素:</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>element</strong></th><th style="text-align:center"><strong>explanation</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong>feed elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">包含规范化查询字符串的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">分配给此查询的唯一 id</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">最后一次更新此查询的搜索结果。 设置为当天的午夜</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">通过 GET 请求检索此提要的 url</td></tr><tr><td style="text-align:center">opensearch:totalResults</td><td style="text-align:center">此查询的搜索结果总数</td></tr><tr><td style="text-align:center">opensearch:startIndex</td><td style="text-align:center">总结果列表中第一个返回结果的基于0的索引</td></tr><tr><td style="text-align:center">opensearch:itemsPerPage</td><td style="text-align:center">每页返回的结果数</td></tr><tr><td style="text-align:center"><strong>entry elements</strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">title</td><td style="text-align:center">文章的标题</td></tr><tr><td style="text-align:center">id</td><td style="text-align:center">文章的网址<code>http://arxiv.org/abs/id</code></td></tr><tr><td style="text-align:center">published</td><td style="text-align:center">文章的发布日期</td></tr><tr><td style="text-align:center">updated</td><td style="text-align:center">文章的更新日期，如果为 v1 版本，那么与发布日期相同</td></tr><tr><td style="text-align:center">summary</td><td style="text-align:center">文章摘要</td></tr><tr><td style="text-align:center">author</td><td style="text-align:center">每个作者有一个子元素 name，包含了作者的名字</td></tr><tr><td style="text-align:center">link</td><td style="text-align:center">可以给定与这篇文章关联的 3 个网址</td></tr><tr><td style="text-align:center">category</td><td style="text-align:center">文章分类</td></tr><tr><td style="text-align:center">arxiv:primary_category</td><td style="text-align:center">主要的 arXiv 分类</td></tr><tr><td style="text-align:center">arxiv:comment</td><td style="text-align:center">作者对此发表的评论</td></tr><tr><td style="text-align:center">arxiv:affiliation</td><td style="text-align:center">作者的从属关系</td></tr><tr><td style="text-align:center">arxiv:journal_ref</td><td style="text-align:center">参考文献</td></tr><tr><td style="text-align:center">arxiv:doi</td><td style="text-align:center">已解析的 DOI 的 url，指向外部资源</td></tr></tbody></table></div><h3 id="学科的分类"><a href="#学科的分类" class="headerlink" title="学科的分类"></a>学科的分类</h3><p>下面是学科分类字段以及对应的翻译（软件脚本自动翻译，如不对请勿喷）：</p><div class="table-container"><table><thead><tr><th style="text-align:center">字段</th><th style="text-align:center">学科（英文）</th><th style="text-align:center">学科（中文）</th></tr></thead><tbody><tr><td style="text-align:center">astro-ph</td><td style="text-align:center">Astrophysics</td><td style="text-align:center">天体物理</td></tr><tr><td style="text-align:center">astro-ph.CO</td><td style="text-align:center">Cosmology and Nongalactic Astrophysics</td><td style="text-align:center">宇宙学与非规则天体物理学</td></tr><tr><td style="text-align:center">astro-ph.EP</td><td style="text-align:center">Earth and Planetary Astrophysics</td><td style="text-align:center">地球与行星天体物理学</td></tr><tr><td style="text-align:center">astro-ph.GA</td><td style="text-align:center">Astrophysics of Galaxies</td><td style="text-align:center">星系的天体物理学</td></tr><tr><td style="text-align:center">astro-ph.HE</td><td style="text-align:center">High Energy Astrophysical Phenomena</td><td style="text-align:center">高能天体物理现象</td></tr><tr><td style="text-align:center">astro-ph.IM</td><td style="text-align:center">Instrumentation and Methods for Astrophysics</td><td style="text-align:center">天体物理学的仪器和方法</td></tr><tr><td style="text-align:center">astro-ph.SR</td><td style="text-align:center">Solar and Stellar Astrophysics</td><td style="text-align:center">太阳与恒星天体物理学</td></tr><tr><td style="text-align:center">cond-mat.dis-nn</td><td style="text-align:center">Disordered Systems and Neural Networks</td><td style="text-align:center">无序系统与神经网络</td></tr><tr><td style="text-align:center">cond-mat.mes-hall</td><td style="text-align:center">Mesoscale and Nanoscale Physics</td><td style="text-align:center">中尺度和纳米尺度物理学</td></tr><tr><td style="text-align:center">cond-mat.mtrl-sci</td><td style="text-align:center">Materials Science</td><td style="text-align:center">材料科学</td></tr><tr><td style="text-align:center">cond-mat.other</td><td style="text-align:center">Other Condensed Matter</td><td style="text-align:center">其他凝聚态</td></tr><tr><td style="text-align:center">cond-mat.quant-gas</td><td style="text-align:center">Quantum Gases</td><td style="text-align:center">量子气体</td></tr><tr><td style="text-align:center">cond-mat.soft</td><td style="text-align:center">Soft Condensed Matter</td><td style="text-align:center">软凝聚物</td></tr><tr><td style="text-align:center">cond-mat.stat-mech</td><td style="text-align:center">Statistical Mechanics</td><td style="text-align:center">统计力学</td></tr><tr><td style="text-align:center">cond-mat.str-el</td><td style="text-align:center">Strongly Correlated Electrons</td><td style="text-align:center">强关联电子</td></tr><tr><td style="text-align:center">cond-mat.supr-con</td><td style="text-align:center">Superconductivity</td><td style="text-align:center">超导现象</td></tr><tr><td style="text-align:center">cs.AI</td><td style="text-align:center">Artificial Intelligence</td><td style="text-align:center">人工智能</td></tr><tr><td style="text-align:center">cs.AR</td><td style="text-align:center">Hardware Architecture</td><td style="text-align:center">硬件架构</td></tr><tr><td style="text-align:center">cs.CC</td><td style="text-align:center">Computational Complexity</td><td style="text-align:center">计算复杂性</td></tr><tr><td style="text-align:center">cs.CE</td><td style="text-align:center">Computational Engineering, Finance, and Science</td><td style="text-align:center">计算工程，金融和科学</td></tr><tr><td style="text-align:center">cs.CG</td><td style="text-align:center">Computational Geometry</td><td style="text-align:center">计算几何</td></tr><tr><td style="text-align:center">cs.CL</td><td style="text-align:center">Computation and Language</td><td style="text-align:center">计算与语言</td></tr><tr><td style="text-align:center">cs.CR</td><td style="text-align:center">Cryptography and Security</td><td style="text-align:center">密码学与保安</td></tr><tr><td style="text-align:center">cs.CV</td><td style="text-align:center">Computer Vision and Pattern Recognition</td><td style="text-align:center">计算机视觉与模式识别</td></tr><tr><td style="text-align:center">CY</td><td style="text-align:center">Computers and Society</td><td style="text-align:center">电脑与社会</td></tr><tr><td style="text-align:center">cs.DB</td><td style="text-align:center">Databases</td><td style="text-align:center">数据库</td></tr><tr><td style="text-align:center">cs.DC</td><td style="text-align:center">Distributed, Parallel, and Cluster Computing</td><td style="text-align:center">分布式、并行和集群计算</td></tr><tr><td style="text-align:center">cs.DL</td><td style="text-align:center">Digital Libraries</td><td style="text-align:center">数字仓库</td></tr><tr><td style="text-align:center">cs.DM</td><td style="text-align:center">Discrete Mathematics</td><td style="text-align:center">离散数学</td></tr><tr><td style="text-align:center">cs.DS</td><td style="text-align:center">Data Structures and Algorithms</td><td style="text-align:center">数据结构和算法</td></tr><tr><td style="text-align:center">cs.ET</td><td style="text-align:center">Emerging Technologies</td><td style="text-align:center">新兴科技</td></tr><tr><td style="text-align:center">cs.FL</td><td style="text-align:center">Formal Languages and Automata Theory</td><td style="text-align:center">形式语言与自动机理论</td></tr><tr><td style="text-align:center">cs.GL</td><td style="text-align:center">General Literature</td><td style="text-align:center">一般文学</td></tr><tr><td style="text-align:center">cs.GR</td><td style="text-align:center">Graphics</td><td style="text-align:center">图形</td></tr><tr><td style="text-align:center">cs.GT</td><td style="text-align:center">Computer Science and Game Theory</td><td style="text-align:center">计算机科学与博弈论</td></tr><tr><td style="text-align:center">cs.HC</td><td style="text-align:center">Human-Computer Interaction</td><td style="text-align:center">人机交互</td></tr><tr><td style="text-align:center">cs.IR</td><td style="text-align:center">Information Retrieval</td><td style="text-align:center">信息检索</td></tr><tr><td style="text-align:center">cs.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">cs.LG</td><td style="text-align:center">Learning</td><td style="text-align:center">学习</td></tr><tr><td style="text-align:center">cs.LO</td><td style="text-align:center">Logic in Computer Science</td><td style="text-align:center">计算机科学中的逻辑</td></tr><tr><td style="text-align:center">cs.MA</td><td style="text-align:center">Multiagent Systems</td><td style="text-align:center">多代理系统</td></tr><tr><td style="text-align:center">cs.MM</td><td style="text-align:center">Multimedia</td><td style="text-align:center">多媒体</td></tr><tr><td style="text-align:center">cs.MS</td><td style="text-align:center">Mathematical Software</td><td style="text-align:center">数学软件</td></tr><tr><td style="text-align:center">cs.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">cs.NE</td><td style="text-align:center">Neural and Evolutionary Computing</td><td style="text-align:center">神经和进化计算</td></tr><tr><td style="text-align:center">cs.NI</td><td style="text-align:center">Networking and Internet Architecture</td><td style="text-align:center">网络与互联网架构</td></tr><tr><td style="text-align:center">cs.OH</td><td style="text-align:center">Other Computer Science</td><td style="text-align:center">其他计算机科学</td></tr><tr><td style="text-align:center">cs.OS</td><td style="text-align:center">Operating Systems</td><td style="text-align:center">操作系统</td></tr><tr><td style="text-align:center">cs.PF</td><td style="text-align:center">Performance</td><td style="text-align:center">性能</td></tr><tr><td style="text-align:center">cs.PL</td><td style="text-align:center">Programming Languages</td><td style="text-align:center">编程语言</td></tr><tr><td style="text-align:center">cs.RO</td><td style="text-align:center">Robotics</td><td style="text-align:center">机器人技术</td></tr><tr><td style="text-align:center">cs.SC</td><td style="text-align:center">Symbolic Computation</td><td style="text-align:center">符号计算</td></tr><tr><td style="text-align:center">cs.SD</td><td style="text-align:center">Sound</td><td style="text-align:center">声音</td></tr><tr><td style="text-align:center">cs.SE</td><td style="text-align:center">Software Engineering</td><td style="text-align:center">软件工程</td></tr><tr><td style="text-align:center">cs.SI</td><td style="text-align:center">Social and Information Networks</td><td style="text-align:center">社会和信息网络</td></tr><tr><td style="text-align:center">cs.SY</td><td style="text-align:center">Systems and Control</td><td style="text-align:center">系统及控制</td></tr><tr><td style="text-align:center">econ.EM</td><td style="text-align:center">Econometrics</td><td style="text-align:center">计量经济学</td></tr><tr><td style="text-align:center">eess.AS</td><td style="text-align:center">Audio and Speech Processing</td><td style="text-align:center">音频及语音处理</td></tr><tr><td style="text-align:center">eess.IV</td><td style="text-align:center">Image and Video Processing</td><td style="text-align:center">图像和视频处理</td></tr><tr><td style="text-align:center">eess.SP</td><td style="text-align:center">Signal Processing</td><td style="text-align:center">信号处理</td></tr><tr><td style="text-align:center">gr-qc</td><td style="text-align:center">General Relativity and Quantum Cosmology</td><td style="text-align:center">广义相对论和量子宇宙学</td></tr><tr><td style="text-align:center">hep-ex</td><td style="text-align:center">High Energy Physics - Experiment</td><td style="text-align:center">高能物理实验</td></tr><tr><td style="text-align:center">hep-lat</td><td style="text-align:center">High Energy Physics - Lattice</td><td style="text-align:center">高能物理-晶格</td></tr><tr><td style="text-align:center">hep-ph</td><td style="text-align:center">High Energy Physics - Phenomenology</td><td style="text-align:center">高能物理-现象学</td></tr><tr><td style="text-align:center">hep-th</td><td style="text-align:center">High Energy Physics - Theory</td><td style="text-align:center">高能物理理论</td></tr><tr><td style="text-align:center">math.AC</td><td style="text-align:center">Commutative Algebra</td><td style="text-align:center">交换代数</td></tr><tr><td style="text-align:center">math.AG</td><td style="text-align:center">Algebraic Geometry</td><td style="text-align:center">代数几何</td></tr><tr><td style="text-align:center">math.AP</td><td style="text-align:center">Analysis of PDEs</td><td style="text-align:center">偏微分方程分析</td></tr><tr><td style="text-align:center">math.AT</td><td style="text-align:center">Algebraic Topology</td><td style="text-align:center">代数拓扑</td></tr><tr><td style="text-align:center">math.CA</td><td style="text-align:center">Classical Analysis and ODEs</td><td style="text-align:center">传统分析和微分方程</td></tr><tr><td style="text-align:center">math.CO</td><td style="text-align:center">Combinatorics</td><td style="text-align:center">组合数学</td></tr><tr><td style="text-align:center">math.CT</td><td style="text-align:center">Category Theory</td><td style="text-align:center">范畴理论</td></tr><tr><td style="text-align:center">math.CV</td><td style="text-align:center">Complex Variables</td><td style="text-align:center">复杂变量</td></tr><tr><td style="text-align:center">math.DG</td><td style="text-align:center">Differential Geometry</td><td style="text-align:center">微分几何</td></tr><tr><td style="text-align:center">math.DS</td><td style="text-align:center">Dynamical Systems</td><td style="text-align:center">动力系统</td></tr><tr><td style="text-align:center">math.FA</td><td style="text-align:center">Functional Analysis</td><td style="text-align:center">功能分析</td></tr><tr><td style="text-align:center">math.GM</td><td style="text-align:center">General Mathematics</td><td style="text-align:center">普通数学</td></tr><tr><td style="text-align:center">math.GN</td><td style="text-align:center">General Topology</td><td style="text-align:center">点集拓扑学</td></tr><tr><td style="text-align:center">math.GR</td><td style="text-align:center">Group Theory</td><td style="text-align:center">群论</td></tr><tr><td style="text-align:center">math.GT</td><td style="text-align:center">Geometric Topology</td><td style="text-align:center">几何拓扑学</td></tr><tr><td style="text-align:center">math.HO</td><td style="text-align:center">History and Overview</td><td style="text-align:center">历史和概述</td></tr><tr><td style="text-align:center">math.IT</td><td style="text-align:center">Information Theory</td><td style="text-align:center">信息理论</td></tr><tr><td style="text-align:center">math.KT</td><td style="text-align:center">K-Theory and Homology</td><td style="text-align:center">K 理论与同调</td></tr><tr><td style="text-align:center">math.LO</td><td style="text-align:center">Logic</td><td style="text-align:center">逻辑</td></tr><tr><td style="text-align:center">math.MG</td><td style="text-align:center">Metric Geometry</td><td style="text-align:center">度量几何学</td></tr><tr><td style="text-align:center">math.MP</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">math.NA</td><td style="text-align:center">Numerical Analysis</td><td style="text-align:center">数值分析</td></tr><tr><td style="text-align:center">math.NT</td><td style="text-align:center">Number Theory</td><td style="text-align:center">数论</td></tr><tr><td style="text-align:center">math.OA</td><td style="text-align:center">Operator Algebras</td><td style="text-align:center">算子代数</td></tr><tr><td style="text-align:center">math.OC</td><td style="text-align:center">Optimization and Control</td><td style="text-align:center">优化和控制</td></tr><tr><td style="text-align:center">math.PR</td><td style="text-align:center">Probability</td><td style="text-align:center">概率</td></tr><tr><td style="text-align:center">math.QA</td><td style="text-align:center">Quantum Algebra</td><td style="text-align:center">量子代数</td></tr><tr><td style="text-align:center">math.RA</td><td style="text-align:center">Rings and Algebras</td><td style="text-align:center">环与代数</td></tr><tr><td style="text-align:center">math.RT</td><td style="text-align:center">Representation Theory</td><td style="text-align:center">表示论</td></tr><tr><td style="text-align:center">math.SG</td><td style="text-align:center">Symplectic Geometry</td><td style="text-align:center">辛几何</td></tr><tr><td style="text-align:center">math.SP</td><td style="text-align:center">Spectral Theory</td><td style="text-align:center">光谱理论</td></tr><tr><td style="text-align:center">math.ST</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr><tr><td style="text-align:center">math-ph</td><td style="text-align:center">Mathematical Physics</td><td style="text-align:center">数学物理</td></tr><tr><td style="text-align:center">nlin.AO</td><td style="text-align:center">Adaptation and Self-Organizing Systems</td><td style="text-align:center">适应与自组织系统</td></tr><tr><td style="text-align:center">nlin.CD</td><td style="text-align:center">Chaotic Dynamics</td><td style="text-align:center">混沌动力学</td></tr><tr><td style="text-align:center">nlin.CG</td><td style="text-align:center">Cellular Automata and Lattice Gases</td><td style="text-align:center">元胞自动机与格子气体</td></tr><tr><td style="text-align:center">nlin.PS</td><td style="text-align:center">Pattern Formation and Solitons</td><td style="text-align:center">模式形成与孤子</td></tr><tr><td style="text-align:center">nlin.SI</td><td style="text-align:center">Exactly Solvable and Integrable Systems</td><td style="text-align:center">严格可解可积系统</td></tr><tr><td style="text-align:center">nucl-ex</td><td style="text-align:center">Nuclear Experiment</td><td style="text-align:center">核试验</td></tr><tr><td style="text-align:center">nucl-th</td><td style="text-align:center">Nuclear Theory</td><td style="text-align:center">核理论</td></tr><tr><td style="text-align:center">physics.acc-ph</td><td style="text-align:center">Accelerator Physics</td><td style="text-align:center">加速器物理学</td></tr><tr><td style="text-align:center">physics.ao-ph</td><td style="text-align:center">Atmospheric and Oceanic Physics</td><td style="text-align:center">大气和海洋物理学</td></tr><tr><td style="text-align:center">physics.app-ph</td><td style="text-align:center">Applied Physics</td><td style="text-align:center">应用物理学</td></tr><tr><td style="text-align:center">physics.atm-clus</td><td style="text-align:center">Atomic and Molecular Clusters</td><td style="text-align:center">原子和分子团簇</td></tr><tr><td style="text-align:center">physics.atom-ph</td><td style="text-align:center">Atomic Physics</td><td style="text-align:center">原子物理学</td></tr><tr><td style="text-align:center">physics.bio-ph</td><td style="text-align:center">Biological Physics</td><td style="text-align:center">生物物理学</td></tr><tr><td style="text-align:center">physics.chem-ph</td><td style="text-align:center">Chemical Physics</td><td style="text-align:center">化学物理</td></tr><tr><td style="text-align:center">physics.class-ph</td><td style="text-align:center">Classical Physics</td><td style="text-align:center">经典物理学</td></tr><tr><td style="text-align:center">physics.comp-ph</td><td style="text-align:center">Computational Physics</td><td style="text-align:center">计算物理学</td></tr><tr><td style="text-align:center">physics.data-an</td><td style="text-align:center">Data Analysis, Statistics and Probability</td><td style="text-align:center">数据分析、统计和概率</td></tr><tr><td style="text-align:center">physics.ed-ph</td><td style="text-align:center">Physics Education</td><td style="text-align:center">物理教育</td></tr><tr><td style="text-align:center">physics.flu-dyn</td><td style="text-align:center">Fluid Dynamics</td><td style="text-align:center">流体动力学</td></tr><tr><td style="text-align:center">physics.gen-ph</td><td style="text-align:center">General Physics</td><td style="text-align:center">普通物理</td></tr><tr><td style="text-align:center">physics.geo-ph</td><td style="text-align:center">Geophysics</td><td style="text-align:center">地球物理学</td></tr><tr><td style="text-align:center">physics.hist-ph</td><td style="text-align:center">History and Philosophy of Physics</td><td style="text-align:center">物理学的历史与哲学</td></tr><tr><td style="text-align:center">physics.ins-det</td><td style="text-align:center">Instrumentation and Detectors</td><td style="text-align:center">仪器和探测器</td></tr><tr><td style="text-align:center">physics.med-ph</td><td style="text-align:center">Medical Physics</td><td style="text-align:center">医学物理学</td></tr><tr><td style="text-align:center">physics.optics</td><td style="text-align:center">Optics</td><td style="text-align:center">光学</td></tr><tr><td style="text-align:center">physics.plasm-ph</td><td style="text-align:center">Plasma Physics</td><td style="text-align:center">等离子体物理</td></tr><tr><td style="text-align:center">physics.pop-ph</td><td style="text-align:center">Popular Physics</td><td style="text-align:center">大众物理</td></tr><tr><td style="text-align:center">physics.soc-ph</td><td style="text-align:center">Physics and Society</td><td style="text-align:center">物理学与社会</td></tr><tr><td style="text-align:center">physics.space-ph</td><td style="text-align:center">Space Physics</td><td style="text-align:center">空间物理学</td></tr><tr><td style="text-align:center">q-bio.BM</td><td style="text-align:center">Biomolecules</td><td style="text-align:center">生物分子</td></tr><tr><td style="text-align:center">q-bio.CB</td><td style="text-align:center">Cell Behavior</td><td style="text-align:center">细胞行为</td></tr><tr><td style="text-align:center">q-bio.GN</td><td style="text-align:center">Genomics</td><td style="text-align:center">基因组学</td></tr><tr><td style="text-align:center">q-bio.MN</td><td style="text-align:center">Molecular Networks</td><td style="text-align:center">分子网络</td></tr><tr><td style="text-align:center">q-bio.NC</td><td style="text-align:center">Neurons and Cognition</td><td style="text-align:center">神经元与认知</td></tr><tr><td style="text-align:center">q-bio.OT</td><td style="text-align:center">Other Quantitative Biology</td><td style="text-align:center">其他定量生物学</td></tr><tr><td style="text-align:center">q-bio.PE</td><td style="text-align:center">Populations and Evolution</td><td style="text-align:center">种群与进化</td></tr><tr><td style="text-align:center">q-bio.QM</td><td style="text-align:center">Quantitative Methods</td><td style="text-align:center">定量方法</td></tr><tr><td style="text-align:center">q-bio.SC</td><td style="text-align:center">Subcellular Processes</td><td style="text-align:center">亚细胞突起</td></tr><tr><td style="text-align:center">q-bio.TO</td><td style="text-align:center">Tissues and Organs</td><td style="text-align:center">组织和器官</td></tr><tr><td style="text-align:center">q-fin.CP</td><td style="text-align:center">Computational Finance</td><td style="text-align:center">金融工程</td></tr><tr><td style="text-align:center">q-fin.EC</td><td style="text-align:center">Economics</td><td style="text-align:center">经济学</td></tr><tr><td style="text-align:center">q-fin.GN</td><td style="text-align:center">General Finance</td><td style="text-align:center">财务概述</td></tr><tr><td style="text-align:center">q-fin.MF</td><td style="text-align:center">Mathematical Finance</td><td style="text-align:center">数学金融</td></tr><tr><td style="text-align:center">q-fin.PM</td><td style="text-align:center">Portfolio Management</td><td style="text-align:center">投资组合管理</td></tr><tr><td style="text-align:center">q-fin.PR</td><td style="text-align:center">Pricing of Securities</td><td style="text-align:center">证券定价</td></tr><tr><td style="text-align:center">q-fin.RM</td><td style="text-align:center">Risk Management</td><td style="text-align:center">风险管理</td></tr><tr><td style="text-align:center">q-fin.ST</td><td style="text-align:center">Statistical Finance</td><td style="text-align:center">金融统计</td></tr><tr><td style="text-align:center">q-fin.TR</td><td style="text-align:center">Trading and Market Microstructure</td><td style="text-align:center">交易与市场微观结构</td></tr><tr><td style="text-align:center">quant-ph</td><td style="text-align:center">Quantum Physics</td><td style="text-align:center">量子物理学</td></tr><tr><td style="text-align:center">stat.AP</td><td style="text-align:center">Applications</td><td style="text-align:center">应用</td></tr><tr><td style="text-align:center">stat.CO</td><td style="text-align:center">Computation</td><td style="text-align:center">计算</td></tr><tr><td style="text-align:center">stat.ME</td><td style="text-align:center">Methodology</td><td style="text-align:center">方法论</td></tr><tr><td style="text-align:center">stat.ML</td><td style="text-align:center">Machine Learning</td><td style="text-align:center">机器学习</td></tr><tr><td style="text-align:center">stat.OT</td><td style="text-align:center">Other Statistics</td><td style="text-align:center">其他统计学</td></tr><tr><td style="text-align:center">stat.TH</td><td style="text-align:center">Statistics Theory</td><td style="text-align:center">统计学理论</td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Arxiv学术论文查询接口详解-转载&quot;&gt;&lt;a href=&quot;#Arxiv学术论文查询接口详解-转载&quot; class=&quot;headerlink&quot; title=&quot;Arxiv学术论文查询接口详解 转载&quot;&gt;&lt;/a&gt;Arxiv学术论文查询接口详解 转载&lt;/h1&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="Note" scheme="https://kedreamix.github.io/categories/Note/"/>
    
    
    <category term="Python" scheme="https://kedreamix.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>3D reconstruction</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3D%20reconstruction/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/3D%20reconstruction/</id>
    <published>2024-01-24T03:06:02.000Z</published>
    <updated>2024-01-24T08:12:35.455Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting"><a href="#Deformable-Endoscopic-Tissues-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting"></a>Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</h2><p><strong>Authors:Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</strong></p><p>Surgical 3D reconstruction is a critical area of research in robotic surgery, with recent works adopting variants of dynamic radiance fields to achieve success in 3D reconstruction of deformable tissues from single-viewpoint videos. However, these methods often suffer from time-consuming optimization or inferior quality, limiting their adoption in downstream tasks. Inspired by 3D Gaussian Splatting, a recent trending 3D representation, we present EndoGS, applying Gaussian Splatting for deformable endoscopic tissue reconstruction. Specifically, our approach incorporates deformation fields to handle dynamic scenes, depth-guided supervision to optimize 3D targets with a single viewpoint, and a spatial-temporal weight mask to mitigate tool occlusion. As a result, EndoGS reconstructs and renders high-quality deformable endoscopic tissues from a single-viewpoint video, estimated depth maps, and labeled tool masks. Experiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves superior rendering quality. Code is available at <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a>. </p><p><a href="http://arxiv.org/abs/2401.11535v1">PDF</a> Work in progress. 10 pages, 4 figures</p><p><strong>摘要</strong></p><p>我们将 EndoGS 提出了一种基于高斯斑点的可变形内镜组织重建方法。</p><p><strong>要点</strong></p><ul><li>EndoGS通过采用高斯斑点来实现可变形内镜组织的 3D 重建。</li><li>EndoGS 引入了变形场来处理动态场景，并通过深度引导监督来优化具有单个视点的 3D 目标。</li><li>EndoGS 利用时空权重掩码来减轻工具遮挡。</li><li>EndoGS 可以从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内镜组织。</li><li>在 DaVinci 机器人手术视频上的实验表明，EndoGS 可实现优异的渲染质量。</li><li>EndoGS 的代码可在 <a href="https://github.com/HKU-MedAI/EndoGS">https://github.com/HKU-MedAI/EndoGS</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：高斯散点法可变形内窥镜组织重建</p></li><li><p>作者：Lingting Zhu, Zhao Wang, Zhenchao Jin, Guying Lin, Lequan Yu</p></li><li><p>第一作者单位：香港大学</p></li><li><p>关键词：高斯散点法 · 机器人手术 · 三维重建</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11535，Github 链接：https://github.com/HKU-MedAI/EndoGS</p></li><li><p>摘要：（1）研究背景：可变形组织的三维重建是机器人手术研究的关键领域，但现有方法往往存在优化耗时或质量较差的问题，限制了它们在后续任务中的应用。（2）过去的方法：早期尝试采用深度估计来实现内窥镜重建，但这些方法在处理非刚性变形和遮挡方面存在困难。[9, 12] 提出结合工具遮挡、立体深度估计和稀疏翘曲场的框架，但它们在存在剧烈非拓扑可变形组织变化时仍然容易失败。神经辐射场 (NeRFs) 在三维重建方面取得了巨大成功，但它们在处理动态场景和遮挡方面也存在局限性。（3）研究方法：本文提出了一种称为 EndoGS 的方法，将高斯散点法应用于可变形内窥镜组织重建。EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。（4）方法性能：在达芬奇机器人手术视频上的实验表明，EndoGS 在渲染质量方面优于其他方法。这表明 EndoGS 可以为下游任务（如手术增强现实、教育和机器人学习）提供高质量的可变形组织重建。</p></li><li><p>方法：(1): 我们提出了一种称为 EndoGS 的方法，它将高斯散点法应用于可变形内窥镜组织重建。(2): EndoGS 结合了变形场、深度引导监督和时空权重掩码，能够从单视角视频、估计的深度图和标记的工具掩码中重建和渲染高质量的可变形内窥镜组织。(3): 我们使用六个正交特征平面对空间和时间信息进行编码，并使用单个 MLP 来更新高斯属性，以获得变形的位置、比例因子、旋转因子、球谐系数和不透明度。(4): 我们结合工具掩码和深度图来训练 EndoGS，以处理工具遮挡和提高重建质量。</p></li><li><p>结论：</p></li></ol><p>（1）意义：本文提出了一种基于高斯散点法进行可变形内窥镜组织重建的方法，该方法能够从单视角视频、估计的深度图和标记的工具掩码中实时渲染高质量的可变形组织。在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</p><p>（2）优缺点：</p><p>创新点：</p><ul><li>将高斯散点法应用于可变形内窥镜组织重建。</li><li>结合变形场、深度引导监督和时空权重掩码，以处理工具遮挡和提高重建质量。</li></ul><p>性能：</p><ul><li>在达芬奇机器人手术视频上的实验表明，该方法在渲染质量方面优于其他方法。</li></ul><p>工作量：</p><ul><li>需要收集和标记大量的数据。</li><li>需要设计和训练复杂的模型。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3aced720ad0952509d5ad4feafb073c5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db38985f02aa9f93361d5395728da086.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f22f8ab59ea6655501c3858f5b7639aa.jpg" align="middle"></details>​    ## SHINOBI: Shape and Illumination using Neural Object Decomposition via   BRDF Optimization In-the-wild**Authors:Andreas Engelhardt, Amit Raj, Mark Boss, Yunzhi Zhang, Abhishek Kar, Yuanzhen Li, Deqing Sun, Ricardo Martin Brualla, Jonathan T. Barron, Hendrik P. A. Lensch, Varun Jampani**We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc. Project page: https://shinobi.aengelhardt.com Video: https://www.youtube.com/watch?v=iFENQ6AcYd8&amp;feature=youtu.be [PDF](http://arxiv.org/abs/2401.10171v1) **摘要**多尺度哈希编码的隐式形状表示使更快速、更鲁棒的形状重建成为可能，并通过联合相机对齐优化实现了对现有技术的超越。**要点**- SHINOBI 是一种端到端框架，用于从以不同光线、姿势和背景拍摄的对象图像中重建形状、材质和照明。- SHINOBI 使用基于多尺度哈希编码的隐式形状表示，能够实现更快速、更鲁棒的形状重建。- SHINOBI 还可以编辑照明和对象反射率（即材质），并与对象的形状一起优化双向反射分布函数（BRDF）和照明。- SHINOBI 适用于各种各样的对象图像集合，可以为增强现实/虚拟现实、电影、游戏等多种用例生成可重新照明的 3D 资源。- SHINOBI 可以从不限数量的图像中重建形状、材料和照明。- SHINOBI 联合优化 BRDF 和照明以及对象的形状。- SHINOBI 在各种各样的对象图像集合上进行评估，并与最先进的方法进行了比较。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：SHINOBI：基于 BRDF 优化进行形状和光照分解的神经物体分解</p></li><li><p>作者：Andreas Engelhardt、Amit Raj、Mark Boss、Yunzhi Zhang、Abhishek Kar、Yuanzhen Li、Deqing Sun、Ricardo Martin Brualla、Jonathan T. Barron、Hendrik P. A. Lensch、Varun Jampani</p></li><li><p>第一作者单位：德国图宾根大学</p></li><li><p>关键词：计算机视觉、图形学、神经渲染、形状重建、材质估计、光照估计</p></li><li><p>论文链接：https://arxiv.org/abs/2401.10171，Github 代码链接：无</p></li><li><p>摘要：(1)：背景：逆向渲染基于无约束图像集合的对象是一个长期存在的挑战，需要对形状、光照和姿势进行联合优化。(2)：过去的方法：现有方法通常使用显式形状表示，这在处理具有复杂几何形状的对象时存在局限性。此外，它们通常需要大量的手动调整和用户交互。(3)：方法：本文提出了一种新的框架 SHINOBI，它使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。同时，该框架还联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。(4)：性能：SHINOBI 在多个数据集上的实验结果表明，它在形状重建、材质估计和光照估计方面都优于现有方法。该方法可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。</p></li><li><p>方法：(1) <strong>隐式形状表示</strong>：SHINOBI使用隐式形状表示来表示对象，这是一种无界表示，可以轻松处理具有复杂几何形状的对象。(2) <strong>多分辨率哈希编码</strong>：SHINOBI使用多分辨率哈希编码来编码隐式形状表示，这可以提高形状重建的速度和鲁棒性。(3) <strong>联合优化BRDF和光照</strong>：SHINOBI联合优化BRDF和光照，以实现对光照和对象反射率的编辑。(4) <strong>可照明3D资产生成</strong>：SHINOBI可以生成逼真的可照明3D资产，可用于AR/VR、电影、游戏等多种应用场景。</p></li><li><p>结论：（1）：本工作提出了一种新的框架 SHINOBI，该框架可以从未经摆放的野外图像集中估计物体的形状、姿态和光照。我们新颖的混合哈希网格编码能够使用多分辨率哈希网格更容易地优化相机姿态。此外，我们选择的相机参数化以及逐视图重要性权重和基于补丁的对齐损失允许更好地将图像与 3D 对齐，从而在具有高频细节的情况下更好地重建。虽然 SHINOBI 能够从任何类别的物体中恢复几何形状，但其性能在细长/透明结构上受到限制，并且无法在极端光照变化下恢复高频细节，我们将其留作未来工作的探索。（2）：创新点：</p></li></ol><ul><li>使用隐式形状表示和多分辨率哈希编码来实现更快速和鲁棒的形状重建。</li><li>联合优化 BRDF 和光照，以实现对光照和对象反射率的编辑。</li><li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。性能：</li><li>在多个数据集上的实验结果表明，SHINOBI 在形状重建、材质估计和光照估计方面都优于现有方法。</li><li>可以生成逼真的可照明 3D 资产，可用于 AR/VR、电影、游戏等多种应用场景。工作量：</li><li>该方法需要大量的数据和计算资源。</li><li>需要手动调整和用户交互。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6fa4b83c9b053b9d6092eca8188f4657.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b1a74a4a26394cb29cc941c3540acec6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1cad55e2949324c67746914275a9a371.jpg" align="middle"><img src="https://pica.zhimg.com/v2-a4ea31f8709c7ac961337f4d618a8737.jpg" align="middle"></details>​    ## GaussianBody: Clothed Human Reconstruction via 3d Gaussian Splatting**Authors:Mengtian Li, Shengxiang Yao, Zhifeng Xie, Keyu Chen, Yu-Gang Jiang**In this work, we propose a novel clothed human reconstruction method called GaussianBody, based on 3D Gaussian Splatting. Compared with the costly neural radiance based models, 3D Gaussian Splatting has recently demonstrated great performance in terms of training time and rendering quality. However, applying the static 3D Gaussian Splatting model to the dynamic human reconstruction problem is non-trivial due to complicated non-rigid deformations and rich cloth details. To address these challenges, our method considers explicit pose-guided deformation to associate dynamic Gaussians across the canonical space and the observation space, introducing a physically-based prior with regularized transformations helps mitigate ambiguity between the two spaces. During the training process, we further propose a pose refinement strategy to update the pose regression for compensating the inaccurate initial estimation and a split-with-scale mechanism to enhance the density of regressed point clouds. The experiments validate that our method can achieve state-of-the-art photorealistic novel-view rendering results with high-quality details for dynamic clothed human bodies, along with explicit geometry reconstruction. [PDF](http://arxiv.org/abs/2401.09720v1) **Summary**高斯人体：基于 3D 高斯散布的动态穿衣人体重建方法。**Key Takeaways**- 我们提出了一种名为 GaussianBody 的新型穿衣人体重建方法，该方法基于 3D 高斯散布。- 与昂贵的神经辐照度模型相比，3D 高斯散布在训练时间和渲染质量方面最近表现出出色的性能。- 将静态 3D 高斯散布模型应用于动态人体重建问题并非易事，原因是复杂的非刚性变形和丰富的布料细节。- 为了应对这些挑战，我们的方法考虑了显式姿势引导变形，以在规范空间和观察空间中关联动态高斯分布，引入具有正则化变换的基于物理的先验有助于减轻两个空间之间的歧义。- 在训练过程中，我们进一步提出了一种姿势优化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一个具有尺度的分割机制来增强回归点云的密度。- 实验验证表明，我们的方法可以实现动态穿着人体的高质量细节，以及显式几何重建的最先进的照片级新视角渲染结果。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：高斯体：基于 3D 高斯散点的衣着人体重建</li><li>作者：孟天力、姚圣翔、谢志峰、陈可宇、姜玉刚</li><li>第一作者单位：上海大学</li><li>关键词：衣着人体重建、3D 高斯散点、动态捕捉、几何重建</li><li>论文链接：https://arxiv.org/abs/2401.09720</li><li>摘要：（1）研究背景：高保真衣着人体模型的创建在虚拟现实、远程临场和电影制作等领域具有重要应用。传统方法要么涉及复杂的捕捉系统，要么需要 3D 艺术家进行繁琐的手工工作，这使得它们耗时且昂贵，从而限制了新手用户的可扩展性。最近，人们越来越关注从单个 RGB 图像或单目视频中自动重建衣着人体模型。（2）过去方法及问题：基于网格的方法最初被引入，通过回归参数化模型（如 SCAPE、SMPL、SMPL-X 和 STAR）来恢复人体形状。虽然它们可以实现快速且鲁棒的重建，但回归的多边形网格难以捕捉不同的几何细节和丰富的服装特征。在这种情况下，添加顶点偏移成为一种增强解决方案。然而，它的表示能力仍然有限。（3）研究方法：本文提出了一种基于 3D 高斯散点的新型衣着人体重建方法，称为高斯体。与昂贵的神经辐射体模型相比，3D 高斯散点最近在训练时间和渲染质量方面表现出优异的性能。然而，由于复杂的非刚性变形和丰富的服装细节，将静态 3D 高斯散点模型应用于动态人体重建问题并非易事。为了应对这些挑战，本文的方法考虑了在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，引入了具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。在训练过程中，本文还提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。（4）方法性能：实验验证了本文方法可以实现动态衣着人体的高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody，该方法可以从单目视频中重建动态衣着人体模型。GaussianBody通过在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，以及引入具有正则化变换的基于物理的先验，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。实验表明，GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。（2）：创新点：</li></ol><ul><li>提出了一种基于3D高斯散点的新型衣着人体重建方法GaussianBody。</li><li>在规范空间和观察空间中对动态高斯体进行显式姿势引导的变形，解决了动态衣着人体重建中存在的复杂非刚性变形和丰富的服装细节等挑战。</li><li>引入具有正则化变换的基于物理的先验，有助于减轻两个空间之间的歧义。</li><li>提出了一种姿势细化策略来更新姿势回归，以补偿不准确的初始估计，并提出了一种分裂缩放机制来增强回归点云的密度。</li></ul><p>性能：</p><ul><li>GaussianBody可以实现高质量细节的最新逼真新视角渲染结果，以及显式的几何重建。</li><li>GaussianBody在图像质量指标方面与基线和其他方法相当，证明了其竞争性能、相对较快的训练速度，以及使用更高分辨率图像进行训练的能力。</li></ul><p>工作量：</p><ul><li>GaussianBody需要收集和标注大量的数据。</li><li>GaussianBody的训练过程相对复杂，需要较大的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7931aa02d87b1007c7f5cdde77107e5c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3d3dcd00c27bc3d320b23d4247ae79f3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3df005c3ea738aba56feb680b23b73d2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d121364f4f1fecac5ef9d276f421f434.jpg" align="middle"></details>​    ## PPSURF: Combining Patches and Point Convolutions for Detailed Surface   Reconstruction**Authors:Philipp Erler, Lizeth Fuentes, Pedro Hermosilla, Paul Guerrero, Renato Pajarola Michael Wimmer**3D surface reconstruction from point clouds is a key step in areas such as content creation, archaeology, digital cultural heritage, and engineering. Current approaches either try to optimize a non-data-driven surface representation to fit the points, or learn a data-driven prior over the distribution of commonly occurring surfaces and how they correlate with potentially noisy point clouds. Data-driven methods enable robust handling of noise and typically either focus on a global or a local prior, which trade-off between robustness to noise on the global end and surface detail preservation on the local end. We propose PPSurf as a method that combines a global prior based on point convolutions and a local prior based on processing local point cloud patches. We show that this approach is robust to noise while recovering surface details more accurately than the current state-of-the-art.   Our source code, pre-trained model and dataset are available at: https://github.com/cg-tuwien/ppsurf [PDF](http://arxiv.org/abs/2401.08518v1) Published in Computer Graphics Forum (Jan 2024):   https://onlinelibrary.wiley.com/doi/10.1111/cgf.15000**Summary**云点重建结合局部特征与全局信息，提升表面重建的抗噪性和细节保存能力。**Key Takeaways**- 表面重建是内容创建、考古学、数字文化遗产和工程等领域的关键步骤。- 当前方法要么尝试优化非数据驱动的曲面表示以拟合点，要么学习数据驱动的先验分布，以及它们如何与潜在的噪声点云相关。- 数据驱动的方法能够对噪声进行鲁棒处理，通常专注于全局或局部先验，在全局末端的抗噪性和局部末端的表面细节保留之间进行权衡。- PPSurf 是一种将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合的方法。- PPSurf 在恢复表面细节方面比当前最先进的方法更准确。- PPSurf 的源代码、预训练模型和数据集可在 https://github.com/cg-tuwien/ppsurf 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：PPSURF：结合局部块和点卷积进行详细表面重建</p></li><li><p>作者：P. Erler、L. Fuentes-Perez、P. Hermosilla、P. Guerrero、R. Pajarola、M. Wimmer</p></li><li><p>第一作者单位：维也纳工业大学</p></li><li><p>关键词：点云、表面重建、数据驱动、局部块、点卷积</p></li><li><p>论文链接：https://onlinelibrary.wiley.com/doi/10.1111/cgf.150001Github 代码链接：https://github.com/cg-tuwien/ppsurf</p></li><li><p>摘要：（1）：3D 表面重建是内容创作、考古学、数字文化遗产和工程等领域的关键步骤。目前的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。（2）：过去的方法要么尝试优化非数据驱动的表面表示以拟合点，要么学习数据驱动的先验，了解常见表面和潜在噪声点云之间的分布和相关性。数据驱动的方法能够稳健地处理噪声，但通常侧重于全局或局部先验，这在全局端与噪声的稳健性和局部端表面细节的保留之间进行权衡。（3）：本文提出了一种名为 PPSURF 的方法，该方法结合了基于点卷积的全局先验和基于处理局部点云块的局部先验。（4）：在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</p></li><li><p>方法：(1)：PPSURF方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。(2)：全局分支使用POCO网络，该网络由点卷积模块和插值模块组成，点卷积模块计算每个稀疏点的特征向量，插值模块使用基于注意力的权重对特征向量进行插值以获得全局特征向量。(3)：局部分支使用PointNet网络，该网络经过修改，采用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。(4)：全局特征向量和局部特征向量通过MLP组合，以计算查询点处的占用概率。</p></li><li><p>结论：（1）：本文提出了一种名为 PPSURF 的方法，该方法将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。（2）：创新点：</p></li></ol><ul><li>将基于点卷积的全局先验与基于处理局部点云块的局部先验相结合，以实现详细的表面重建。</li><li>使用基于注意力的聚合方式，而不是原始的最大值或和值聚合方式，以获得局部特征向量。</li><li>在噪声点云重建任务上，该方法在恢复表面细节的同时，对噪声具有鲁棒性，优于当前最先进的方法。</li></ul><p>性能：</p><ul><li>在 ABCvar-noise 测试集上，PPSURF 在 Chamfer 距离、F1 分数和法线误差方面均优于当前最先进的方法。</li><li>在 PatchSize 消融研究中，PPSURF100NN 和 PPSURF200NN 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li><li>在 Miscellanous 消融研究中，PPSURFSymMax 在 Chamfer 距离方面优于 PPSURFFull，但在 F1 分数和法线误差方面略差。PPSURFQPoints 在 F1 分数和法线误差方面优于 PPSURFFull，但在 Chamfer 距离方面略差。PPSURFMergeCat 在 Chamfer 距离和 F1 分数方面与 PPSURFFull 相当，但在法线误差方面略差。</li></ul><p>工作量：</p><ul><li>PPSURF 的实现相对简单，易于理解和使用。</li><li>PPSURF 的训练时间和推理时间与当前最先进的方法相当。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-deb0c891a44eda3abc848abfcb3b5052.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b71a526973638d7a94c6cac1ef5d0c4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-78b2aade816834cf694d7949cbee5c89.jpg" align="middle"></details>​    ​    ## InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes**Authors:Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari**We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scenes and provide an in-depth analysis of the proposed components. Our experiments with generative insertion of objects in several 3D scenes indicate the effectiveness of our method compared to the existing methods. InseRF is capable of controllable and 3D-consistent object insertion without requiring explicit 3D information as input. Please visit our project page at https://mohamad-shahbazi.github.io/inserf. [PDF](http://arxiv.org/abs/2401.05335v1) **摘要**神经辐射场三维重建中，通过用户提供的文本描述和参考视点中的二维边界框，实现新的物体生成。**要点**- InseRF 是一种三维场景神经辐射场中生成式物体插入的新颖方法，支持生成新的物体并将其插入三维场景中。- InseRF 使用文本到图像扩散模型的先验知识，可以有效地编辑三维场景的样式和外观，或移除现有物体。- InseRF 将三维物体插入操作转化为参考视图中的二维物体插入，并使用单视图物体重建方法将二维编辑提升到三维。- InseRF 利用单目深度估计方法的先验知识，将重建的物体插入场景中。- InseRF 在多个三维场景上进行了评估，结果表明其优于现有方法。- InseRF 能够生成可控和三维一致的物体，且不需要显式的三维信息作为输入。- InseRF 项目主页：https://mohamad-shahbazi.github.io/inserf。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：无精修 URL：</p></li><li><p>作者：Mohamad Shahbazi<em>, Anh-Huy Phan</em>, Jia-Bin Huang, Yi-Ling Qiao, Hao Tang, Matthew Fisher, Angela Dai</p></li><li><p>第一作者单位：苏黎世联邦理工学院（ETH Zurich）</p></li><li><p>关键词：神经辐射场、对象插入、文本到图像扩散模型</p></li><li><p>论文链接：无，Github 链接：无</p></li><li><p>摘要：（1）研究背景：近年来，3D 场景编辑方法取得了重大进展，这主要归功于在 3D 生成建模中使用文本到图像扩散模型的强先验。现有方法主要通过改变样式和外观或移除现有对象来编辑 3D 场景。然而，生成新对象对于这些方法来说仍然是一个挑战。（2）过去的方法及其问题：现有方法的问题在于，它们无法生成新的对象，并且需要显式的 3D 信息作为输入。（3）研究方法：为了解决上述问题，本文提出了一种新的方法 InseRF，它可以根据用户提供的文本描述和参考视点中的 2D 边界框在 NeRF 重建的 3D 场景中生成新对象。InseRF 将 2D 编辑提升到 3D，使用单视图对象重建方法重建对象，然后在单目深度估计方法的指导下将重建的对象插入场景中。（4）方法性能：实验表明，InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法。InseRF 能够以可控且 3D 一致的方式插入对象，而无需显式 3D 信息作为输入。</p></li><li><p>方法：(1) 输入：InseRF 的输入是一个 NeRF 重建的 3D 场景、一个文本描述和一个参考视点中的 2D 边界框。(2) 对象重建：InseRF 使用单视图对象重建方法重建对象。(3) 对象插入：InseRF 使用单目深度估计方法估计对象的深度，然后将重建的对象插入场景中。</p></li><li><p>结论：（1）：xxx；（2）：创新点：InseRF 提出了一种新颖的方法，可以根据文本描述和 2D 边界框在 NeRF 重建的 3D 场景中生成新对象，无需显式 3D 信息作为输入。性能：InseRF 在多个 3D 场景中生成插入对象的任务上优于现有方法，能够以可控且 3D 一致的方式插入对象。工作量：InseRF 的实现相对复杂，需要结合单视图对象重建方法和单目深度估计方法，工作量较大。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9f528d642820c297073b934ba1373992.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8499c6fe071df44b8e34eafc6e0a99e2.jpg" align="middle"></details><br>​    <p></p><h2 id="Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging"><a href="#Vision-Reimagined-AI-Powered-Breakthroughs-in-WiFi-Indoor-Imaging" class="headerlink" title="Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging"></a>Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging</h2><p><strong>Authors:Jianyang Shi, Bowen Zhang, Amartansh Dubey, Ross Murch, Liwen Jing</strong></p><p>Indoor imaging is a critical task for robotics and internet-of-things. WiFi as an omnipresent signal is a promising candidate for carrying out passive imaging and synchronizing the up-to-date information to all connected devices. This is the first research work to consider WiFi indoor imaging as a multi-modal image generation task that converts the measured WiFi power into a high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape reconstruction accuracy that is 275% of that achieved by physical model-based inversion methods. Additionally, the Frechet Inception Distance score has been significantly reduced by 82%. To examine the effectiveness of models for this task, the first large-scale dataset is released containing 80,000 pairs of WiFi signal and imaging target. Our model absorbs challenges for the model-based methods including the non-linearity, ill-posedness and non-certainty into massive parameters of our generative AI network. The network is also designed to best fit measured WiFi signals and the desired imaging output. For reproducibility, we will release the data and code upon acceptance. </p><p><a href="http://arxiv.org/abs/2401.04317v1">PDF</a> </p><p><strong>Summary</strong><br>室内成像通过Wi-Fi信号，将测量到的Wi-Fi功率转换为高分辨率图像。</p><p><strong>Key Takeaways</strong></p><ul><li>Wi-Fi室内成像被视为多模态图像生成任务。</li><li>Wi-Fi-GEN网络的形状重建精度是基于物理模型的反演方法的275%。</li><li>Fréchet起始距离评分降低了82%。</li><li>发布了首个大型数据集，包含80,000对Wi-Fi信号和成像目标。</li><li>Wi-Fi-GEN网络将基于模型的方法的挑战吸收为生成性AI网络的大量参数。</li><li>网络设计用于最适合测量的Wi-Fi信号和所需的成像输出。</li><li>为了可复制性，我们将发布数据和代码。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：视觉重构：WiFi 室内成像的 AI 赋能突破</p></li><li><p>作者：Jianyang Shi、Bowen Zhang、Amartansh Dubey、Ross Murch、Liwen Jing</p></li><li><p>隶属单位：深圳先进技术研究院信息与智能学院</p></li><li><p>关键词：WiFi成像、生成式人工智能、逆散射问题</p></li><li><p>论文链接：https://arxiv.org/abs/2401.04317Github 代码链接：无</p></li><li><p>摘要：(1)：研究背景：室内成像是机器人和物联网的一项关键任务。WiFi 作为一种无处不在的信号，是执行被动成像并将最新信息同步到所有连接设备的理想选择。(2)：过去的方法及其问题：现有研究探索了全波（即测量 WiFi 相位和功率）和无相测量来解决室内成像问题。然而，全波方法的相位测量在高频时非常棘手且昂贵，这使得它在实践中不切实际。最近的研究证明了通过无相 WiFi 信号进行室内成像的可行性。(3)：提出的研究方法：本文将 WiFi 室内成像视为多模态图像生成任务，将测量的 WiFi 功率转换为高分辨率室内图像。提出的 WiFi-GEN 网络实现了比基于物理模型的反演方法高出 275% 的重建精度。此外，Fréchet Inception Distance 得分显着降低了 82%。为了检查模型对这项任务的有效性，发布了第一个包含 80,000 对 WiFi 信号和成像目标的大规模数据集。该模型吸收了基于模型的方法的挑战，包括我们生成式 AI 网络中非线性和不确定性的大量参数。该网络还旨在最适合测量的 WiFi 信号和期望的成像输出。(4)：方法的性能：该方法在室内成像任务上取得了良好的性能。在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。这些结果表明，该方法能够有效地从 WiFi 信号中生成高分辨率的室内图像。</p></li><li><p>方法：(1)：WiFi室内成像问题被表述为多模态图像生成任务，利用生成式人工智能网络将测量的WiFi功率转换为高分辨率室内图像。(2)：WiFi-GEN网络由三个部分组成：WiFi信号编码器、控制信号网络和WiFi生成器。WiFi信号编码器将WiFi信号嵌入到潜在空间矩阵中；控制信号网络从潜在空间中提取WiFi信号相关特征并将其转换为多级特征向量；WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。(3)：WiFi信号编码器是一个三层全连接网络，将19×20的信号展平为380维向量并输入编码器。(4)：控制信号网络由下采样模块和上采样模块组成。下采样模块由多个残差层组成，用于减少特征图的空间尺寸；上采样模块利用卷积层和双线性插值来恢复特征图的空间分辨率。(5)：WiFi生成器采用了一种利用从潜在空间中提取的多尺度特征来生成最终图像的方法。WiFi生成器由多个信号块组成，每个信号块在生成过程中发挥着至关重要的作用。(6)：WiFi生成器利用从控制信号网络不同层级提取的特征来影响最终图像生成结果。</p></li><li><p>结论：（1）： 本文将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像，取得了良好的性能。（2）： 创新点：提出一种基于生成式人工智能的 WiFi 室内成像方法，将 WiFi 室内成像问题表述为多模态图像生成任务，利用生成式人工智能网络将测量的 WiFi 功率转换为高分辨率室内图像。性能：在 80,000 对 WiFi 信号和成像目标的数据集上，该方法的重建精度比基于物理模型的反演方法高出 275%。此外，Fréchet Inception Distance 得分显着降低了 82%。工作量：该方法需要大量的数据集和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-86612c0b8ef65da0afea3c89e2c33374.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dc98bdad49ef910c1452e4c81ae51370.jpg" align="middle"><img src="https://picx.zhimg.com/v2-38ec24519ebd731699e5acd71cf669b0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-de86c8629916709cd814c7c39e2990a2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-ba54f2b1b3eaec39559af94bdc51539d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2408b66da95959934b23ca823827183.jpg" align="middle"><img src="https://picx.zhimg.com/v2-303fefd6ac513f1e77ee4837d8d3a0bd.jpg" align="middle"></details><br>​    <p></p><h2 id="RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction"><a href="#RHOBIN-Challenge-Reconstruction-of-Human-Object-Interaction" class="headerlink" title="RHOBIN Challenge: Reconstruction of Human Object Interaction"></a>RHOBIN Challenge: Reconstruction of Human Object Interaction</h2><p><strong>Authors:Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</strong></p><p>Modeling the interaction between humans and objects has been an emerging research direction in recent years. Capturing human-object interaction is however a very challenging task due to heavy occlusion and complex dynamics, which requires understanding not only 3D human pose, and object pose but also the interaction between them. Reconstruction of 3D humans and objects has been two separate research fields in computer vision for a long time. We hence proposed the first RHOBIN challenge: reconstruction of human-object interactions in conjunction with the RHOBIN workshop. It was aimed at bringing the research communities of human and object reconstruction as well as interaction modeling together to discuss techniques and exchange ideas. Our challenge consists of three tracks of 3D reconstruction from monocular RGB images with a focus on dealing with challenging interaction scenarios. Our challenge attracted more than 100 participants with more than 300 submissions, indicating the broad interest in the research communities. This paper describes the settings of our challenge and discusses the winning methods of each track in more detail. We observe that the human reconstruction task is becoming mature even under heavy occlusion settings while object pose estimation and joint reconstruction remain challenging tasks. With the growing interest in interaction modeling, we hope this report can provide useful insights and foster future research in this direction. Our workshop website can be found at \href{<a href="https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}">https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}</a>. </p><p><a href="http://arxiv.org/abs/2401.04143v1">PDF</a> 14 pages, 5 tables, 7 figure. Technical report of the CVPR’23   workshop: RHOBIN challenge (<a href="https://rhobin-challenge.github.io/">https://rhobin-challenge.github.io/</a>)</p><p><strong>Summary</strong><br>首次人类与物体交互重建挑战赛（RHOBIN）及研讨会促成学术交流，吸引百余名参赛者，探讨单目RGB图像中的3D重建。</p><p><strong>Key Takeaways</strong></p><ul><li>首次人类与物体交互重建挑战赛（RHOBIN）于2020年举行，旨在促进人体、物体重建与交互建模研究社群的交流与合作。</li><li>挑战赛包含3个赛道，均以单目RGB图像作为输入，聚焦于解决富有挑战性的交互场景。</li><li>挑战赛吸引了来自世界各地的研究人员，收到300多篇投稿。</li><li>虽然人体重建在遮挡严重的情况下亦表现良好，但物体姿态估计与联合重建仍然具有挑战性。</li><li>挑战赛获奖方法在人体姿态估计、物体姿态估计、交互建模等方面取得优异成绩，表现出强大的鲁棒性和准确性。</li><li>RHOBIN挑战赛促进了研究社群的合作与交流，推动了人体、物体交互重建与建模研究的发展。</li><li>挑战赛获奖方法对后续人体、物体交互重建与建模研究有重要参考价值。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：RHOBIN 挑战：重建人与物体交互</p></li><li><p>作者：Xianghui Xie, Xi Wang, Nikos Athanasiou, Bharat Lal Bhatnagar, Chun-Hao P. Huang, Kaichun Mo, Hao Chen, Xia Jia, Zerui Zhang, Liangxian Cui, Xiao Lin, Bingqiao Qian, Jie Xiao, Wenfei Yang, Hyeongjin Nam, Daniel Sungho Jung, Kihoon Kim, Kyoung Mu Lee, Otmar Hilliges, Gerard Pons-Moll</p></li><li><p>第一作者单位：图宾根大学</p></li><li><p>关键词：人与物体交互、三维重建、单目 RGB 图像、BEHAVE 数据集</p></li><li><p>论文链接：https://arxiv.org/abs/2401.04143</p></li><li><p>摘要：(1) 研究背景：人与物体交互建模近年来成为一个新兴的研究方向。然而，由于严重的遮挡和复杂的动态，捕捉人与物体交互是一个非常具有挑战性的任务，这需要理解不仅是三维人体姿势和物体姿势，还有它们之间的交互。三维人体和物体的重建长期以来一直是计算机视觉中的两个独立的研究领域。(2) 过去的方法及问题：过去的方法通常将人与物体交互建模分为两个独立的任务：人体重建和物体姿态估计。然而，这些方法存在一些问题，例如，它们通常需要多个摄像头或深度传感器，并且它们对遮挡和动态变化非常敏感。(3) 本文提出的研究方法：为了解决上述问题，本文提出了一个新的挑战：RHOBIN 挑战：重建人与物体交互。该挑战赛旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。(4) 实验结果及性能：该挑战赛吸引了 100 多名参与者和 300 多份提交。所有表现最佳的团队都获得了优于先前最先进方法的结果。经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。随着对交互建模的兴趣日益浓厚，我们希望这份报告能够提供有用的见解，并促进该方向的未来研究。</p></li><li><p>方法：（1）RHOBIN挑战赛：该挑战赛由三个赛道组成，所有赛道都使用单目RGB图像作为输入，并输出三维人体或/和交互的三维物体。（2）数据增强和模型集成：应用数据增强和模型集成来提高性能非常重要。（3）联合重建：联合重建仍然是一个具有挑战性的任务。</p></li><li><p>结论：（1）：本文提出了 RHOBIN 挑战赛，旨在将人体重建、物体姿态估计和人与物体交互建模的研究社区聚集在一起，讨论技术并交流思想。该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。（2）：创新点：本文提出了一个新的挑战：RHOBIN 挑战赛：重建人与物体交互，该挑战赛由三个赛道组成，所有赛道都使用单目 RGB 图像作为输入，并输出三维人体或/和交互的三维物体。性能：经过对结果的检查，我们有以下观察：1）现有方法已经可以在单独的人体或物体重建上取得相当好的结果，并且应用数据增强和模型集成来提高性能非常重要；2）联合重建仍然是一个具有挑战性的任务。工作量：该挑战赛吸引了 100 多名参与者和 300 多份提交，所有表现最佳的团队都获得了优于先前最先进方法的结果。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-60dae88b48b9b3b3fe863b3d312f44c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af42a68cb719096e3898c7bb71fe22b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53fd3a763ee32776a87f4b1ae0da73e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d3cda82200fcfbcaad43b075c54ef0f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-27080102702f4941d7f14a6d3d4305f4.jpg" align="middle"></details><br>​    <p></p><h2 id="Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images"><a href="#Sur2f-A-Hybrid-Representation-for-High-Quality-and-Efficient-Surface-Reconstruction-from-Multi-view-Images" class="headerlink" title="Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images"></a>Sur2f: A Hybrid Representation for High-Quality and Efficient Surface   Reconstruction from Multi-view Images</h2><p><strong>Authors:Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia</strong></p><p>Multi-view surface reconstruction is an ill-posed, inverse problem in 3D vision research. It involves modeling the geometry and appearance with appropriate surface representations. Most of the existing methods rely either on explicit meshes, using surface rendering of meshes for reconstruction, or on implicit field functions, using volume rendering of the fields for reconstruction. The two types of representations in fact have their respective merits. In this work, we propose a new hybrid representation, termed Sur2f, aiming to better benefit from both representations in a complementary manner. Technically, we learn two parallel streams of an implicit signed distance field and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the implicit signed distance function (SDF) and surface rendering of the surrogate mesh with a shared, neural shader; the unified shading promotes their convergence to the same, underlying surface. We synchronize learning of the surrogate mesh by driving its deformation with functions induced from the implicit SDF. In addition, the synchronized surrogate mesh enables surface-guided volume sampling, which greatly improves the sampling efficiency per ray in volume rendering. We conduct thorough experiments showing that Sur$^2$f outperforms existing reconstruction methods and surface representations, including hybrid ones, in terms of both recovery quality and recovery efficiency. </p><p><a href="http://arxiv.org/abs/2401.03704v1">PDF</a> 18 pages, 16 figures</p><p><strong>Summary</strong><br>结合隐式距离场与显式代理曲面，提出新型混合表示Sur2f，提高3D重建的质量和效率。</p><p><strong>Key Takeaways</strong></p><ul><li>多视角曲面重建是 3D 视觉研究中的病态逆问题，涉及使用适当的曲面表示对几何形状和外观进行建模。</li><li>现有方法要么依赖显式网格，使用网格的曲面渲染进行重建，要么依赖隐式场函数，使用场的体积渲染进行重建。</li><li>Sur2f是一种新的混合表示，旨在以互补的方式从两种表示中更好地受益。</li><li>Sur2f学习隐式有符号距离场和显式代理曲面Sur2f网格的两个并行流，并使用共享的神经着色器统一隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染。</li><li>Sur2f通过使用从隐式SDF诱导的函数来驱动代理网格的变形来同步学习代理网格。</li><li>同步的代理网格支持曲面引导体积采样，大大提高了体积渲染中每个射线的采样效率。</li><li>Sur2f在恢复质量和恢复效率方面优于现有的重建方法和曲面表示，包括混合方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：Sur2f：一种用于从多视角图像进行高质量和高效曲面重建的混合表示</li><li>作者：Long Chen、Zexiang Xu、Qian Yu、Hao Su、Hao Li、Hao Zhang</li><li>隶属机构：香港城市大学</li><li>关键词：曲面重建、隐式表示、显式表示、混合表示、神经渲染</li><li>论文链接：None，Github 链接：None</li><li>摘要：(1)：曲面重建是 3D 视觉研究中一个不适定的逆问题，它涉及使用适当的曲面表示来建模几何形状和外观。现有的大多数方法依赖于显式网格，使用网格的曲面渲染进行重建，或者依赖于隐式场函数，使用场的体积渲染进行重建。事实上，这两种类型的表示各自都有其优点。(2)：过去的方法要么使用显式网格，要么使用隐式场函数。显式网格方法可以生成高质量的重建结果，但计算成本高，并且难以处理拓扑变化。隐式场函数方法计算成本低，并且可以轻松处理拓扑变化，但生成的重建结果质量较低。(3)：本文提出了一种新的混合表示，称为 Sur2f，旨在以互补的方式从两种表示中更好地受益。具体来说，我们学习了隐式有符号距离场和显式代理曲面 (Sur2f) 网格的两个并行流，并将隐式有符号距离函数 (SDF) 的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中；统一的着色促进了它们收敛到相同的底层曲面。我们通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。此外，同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。(4)：我们进行了彻底的实验，表明 Sur2f 在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。这些性能可以支持它们的目标。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种新的混合表示 Sur2f，它结合了隐式有符号距离场和显式代理曲面网格的优点，在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。（2）：创新点：</li></ol><ul><li>提出了一种新的混合表示 Sur2f，它可以同时利用隐式有符号距离场和显式代理曲面网格的优点。</li><li>设计了一种统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li><li>通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li><li>同步的代理网格支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。性能：</li><li>在重建质量和重建效率方面优于现有的重建方法和曲面表示，包括混合表示。</li><li>可以生成高质量的重建结果，并且可以轻松处理拓扑变化。</li><li>计算成本低，并且可以轻松处理拓扑变化。工作量：</li><li>需要学习两个并行流，一个用于隐式有符号距离场，另一个用于显式代理曲面网格。</li><li>需要设计一个统一的神经着色器，将隐式有符号距离函数的体积渲染和代理网格的曲面渲染统一到一个共享的神经着色器中。</li><li>需要通过函数从隐式 SDF 诱导来同步学习代理网格，从而驱动其变形。</li><li>需要支持表面引导的体积采样，这极大地提高了体积渲染中每个射线的采样效率。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6cf972d05baa99fcd12394fe7af270dc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-01333630ee152c5ffdb71b25620aab65.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f5316c581e5971075fa487859bda230.jpg" align="middle"></details>​    ## GridFormer: Point-Grid Transformer for Surface Reconstruction**Authors:Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu**Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer. [PDF](http://arxiv.org/abs/2401.02292v1) **Summary**隐式神经网络引入了点与网格结合的重建方式，并通过优化提高重建精度。**Key Takeaways**- 隐式神经网络已成为 3D 表面重建中的关键技术。- 现有的方法通常将输入点编码为规则的网格特征（平面或体积），这可能牺牲一些重建细节以提高效率。- 我们引入了一种在网格和点特征之间的新颖且高效的注意机制，称为网格前馈神经网络 (GridFormer)。- 我们的方法最大限度地提高了网格特征的空间表达能力，并保持计算效率。- 为了更准确的再现对象结构，我们进一步提出了结合边界二进制交叉熵损失和边界采样的边界优化策略。- 我们的方法优于现有技术在广泛使用的基准下重建几何结构。- 代码可在 https://github.com/list17/GridFormer 获取。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：网格生成器：用于曲面重建的点网格转换器</p></li><li><p>作者：盛涛李、葛高、刘雨东、刘玉申、顾明</p></li><li><p>隶属单位：北京大学信息科学技术国家研究中心（BNRist），清华大学软件学院</p></li><li><p>关键词：隐式神经网络、曲面重建、点网格转换器、边界优化</p></li><li><p>论文链接：https://arxiv.org/abs/2401.02292Github 代码链接：https://github.com/list17/GridFormer</p></li><li><p>摘要：（1）研究背景：隐式神经网络已成为 3D 曲面重建的关键技术。为了从离散点云重建连续曲面，现有方法通常将输入点编码为规则网格特征（平面或体积）。然而，这些方法通常使用网格作为均匀散射点特征的索引。与不规则点特征相比，规则网格特征可能会牺牲一些重建细节，但提高了效率。（2）过去的方法及其问题：为了充分利用这两种类型的特征，我们引入了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，在整个空间上优化预测可能会导致边界模糊。为了解决这个问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。（3）研究方法：我们的实验验证了我们的方法是有效的，并且在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。（4）方法的性能：我们的方法在 Synthetic Rooms 数据集上取得了最先进的结果，在 F-Score 指标上达到 0.89，在 Chamfer 距离指标上达到 0.011。在 ShapeNet 数据集上，我们的方法在 F-Score 指标上达到 0.83，在 Chamfer 距离指标上达到 0.013。这些性能支持了我们的目标，即在保持计算效率的同时实现高保真曲面重建。</p></li><li><p>方法：（1）：我们提出了一种新颖且高效的网格和点特征之间的注意力机制，称为点网格转换器（GridFormer）。这种机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。（2）：为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。（3）：我们的方法在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</p></li><li><p>结论：（1）：本工作提出了点网格转换器（GridFormer），它使用了一种新颖且高效的点和网格特征之间的注意力机制。该机制将网格视为连接空间和点云的传递点。我们的方法最大限度地提高了网格特征的空间表现力，并保持了计算效率。此外，为了解决预测边界模糊的问题，我们进一步提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样。这种方法使我们能够实现对物体结构更精确的表示。最终，实验表明，输入点的密度和网格大小都会影响我们方法的效果。在未来的工作中，探索如何动态划分网格以实现不同分辨率之间的注意力机制，可能会将这种机制应用到更多的场景中。（2）：创新点：</p></li></ol><ul><li>提出了一种新颖且高效的点和网格特征之间的注意力机制，称为点网格转换器（GridFormer）。</li><li>提出了一种边界优化策略，结合了边际二值交叉熵损失和边界采样，以解决预测边界模糊的问题。</li><li>在广泛使用的基准下优于最先进的方法，通过产生更精确的几何重建。</li></ul><p>性能：</p><ul><li>在SyntheticRooms数据集上取得了最先进的结果，在F-Score指标上达到0.89，在Chamfer距离指标上达到0.011。</li><li>在ShapeNet数据集上，我们的方法在F-Score指标上达到0.83，在Chamfer距离指标上达到0.013。</li></ul><p>工作量：</p><ul><li>本文的工作量中等。该方法需要实现点网格转换器和边界优化策略，并进行大量的实验来验证其有效性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4060effb50cbcc4d2a1088a9feb6000d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-58bec8ac4a2da95ce6f707047d4df439.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7279323b67e79dc5114058f8c28e7609.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5761d740398ae95036084ccdcf20f08c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34ffafeb6a4cead225422a90aa07d1c2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2bf8f7e2e1bfbbf55b2cea30afd6182c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b68791929579cd1db5915c9cc3542d43.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-756e738d1626d407b474a748a66913a5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-adf4ffe3ca8f2dc11327a310777fca9a.jpg" align="middle"></details>​    ## HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human   Reconstruction**Authors:Angtian Wang, Yuanlu Xu, Nikolaos Sarafianos, Robert Maier, Edmond Boyer, Alan Yuille, Tony Tung**Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body. We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs). We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair). Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects. [PDF](http://arxiv.org/abs/2312.17192v1) Accepted by AAAI 2024 main track**Summary**人像模型重建引入了新的隐式表面表现形式，能够重构半透明区域。**Key Takeaways**- 该工作提出了一种新的混合隐式表面表示法来建模人体形状。- 这种表示由两个表面层组成，分别代表被遮挡的人体表面的不透明和半透明区域。- 该方法通过视觉提示自动分割不同区域，并学习重建两个有符号距离函数 (SDF)。- 该方法对不透明区域（例如身体、面部、衣服）执行基于表面的渲染，以保留高保真表面法线，并对半透明区域（例如头发）执行体积渲染。- 实验表明，该方法在 3D 人体重建方面获得了最先进的结果，并且在其他对象上也表现出竞争性的性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：基于混合隐式曲面表示的人体形状重建（Hybrid implicit surface representation for human shape reconstruction）</p></li><li><p>作者：Yuxuan Zhang, Song Bai, Xiaoguang Han, Juergen Gall, Mario Fritz</p></li><li><p>第一作者单位：德国马克斯·普朗克智能系统研究所</p></li><li><p>关键词：人体形状重建、隐式曲面表示、深度学习、渲染</p></li><li><p>论文链接：https://arxiv.org/abs/2203.04118，Github 链接：None</p></li><li><p>摘要：（1）研究背景：神经重建和渲染策略由于能够保留高级形状细节而表现出最先进的性能。然而，现有方法要么将对象表示为隐式曲面函数或神经体积，仍然难以恢复具有异质材料的形状，特别是人类皮肤、头发或衣服。（2）过去的方法及其问题：为了解决这个问题，我们提出了一种新的混合隐式曲面表示来建模人体形状。这种表示由两个曲面层组成，它们分别代表穿着衣服的人体上的不透明区域和半透明区域。我们使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。我们对不透明区域（例如身体、面部、衣服）执行基于曲面的渲染以保留高保真曲面法线，并对半透明区域（例如头发）执行体积渲染。（3）论文提出的研究方法：实验表明，我们的方法在 3D 人体重建方面取得了最先进的结果，并在其他对象上也表现出竞争力。（4）方法在任务上取得的性能：我们的方法在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。这些性能支持了我们的目标，即提供一种能够重建具有异质材料的复杂形状的通用方法。</p></li><li><p>方法：</p><ul><li>混合隐式曲面表示：将人体形状表示为由不透明区域和半透明区域组成的两个曲面层，并使用视觉线索自动分割不同的区域，学习重建两个有符号距离函数 (SDF)。</li><li>混合渲染：对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li><li>集成 SDF 用于体积密度：引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li><li>自适应采样策略：提出一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li><li>训练：通过随机采样每个训练图像上的像素集并最小化总损失来训练网络，包括掩码损失、光度损失、镜面损失和 Eikonal 损失。</li></ul></li><li><p>结论：（1）：本工作的主要贡献在于提出了一种新的混合隐式曲面表示（HISR）来进行人体的三维重建，该方法结合了基于曲面的渲染和体积渲染，能够同时保留高保真曲面法线和精细的几何细节。在各种人体和物体重建数据集上的评估表明，我们的方法在重建几何保真度和新视角合成方面都优于基线方法。这得益于我们为不透明区域和半透明区域设置的双曲面层表示，允许对皮肤、头发和衣服等复杂的人体特征进行细致的渲染。我们的方法在三维人体重建方面取得了最先进的结果，并在其他物体上也表现出竞争力。（2）：创新点：</p></li></ol><ul><li>提出了一种新的混合隐式曲面表示（HISR），该表示由两个曲面层组成，分别代表穿着衣服的人体上的不透明区域和半透明区域。</li><li>使用视觉线索自动分割不同的区域，并学习重建两个有符号距离函数 (SDF)。</li><li>对不透明区域执行基于曲面的渲染以保留高保真曲面法线，对半透明区域执行体积渲染。</li><li>引入一个可学习的高斯混合模型作为 SDF 到密度函数，以更好地建模精细的几何细节。</li><li>提出了一种自适应采样策略，以确保每个射线上采样的间隔相似，从而提高渲染质量。</li></ul><p>性能：</p><ul><li>在人体形状重建任务上取得了最先进的性能，并且在其他对象上也表现出竞争力。</li></ul><p>工作量：</p><ul><li>混合隐式曲面表示的构建和学习需要较大的计算量。</li><li>自适应采样策略的实现也需要较大的计算量。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-65b60c55274f249c7a21655ea5c21695.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cdd4c157203b0a88b68cba902768a310.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-67ee6c759bd41020f430403f5e2d93ac.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fd6993e8a08dd8e58fbc4eaec5ab30ff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-edd10754c68e2204a13c9b005617ccb0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c2e0db87430d852291b34fe1369fecf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1465bdf98251d75ab856b665d50f4419.jpg" align="middle"></details>​    ## In-Hand 3D Object Reconstruction from a Monocular RGB Video**Authors:Shijian Jiang, Qi Ye, Rengan Xie, Yuchi Huo, Xiang Li, Yang Zhou, Jiming Chen**Our work aims to reconstruct a 3D object that is held and rotated by a hand in front of a static RGB camera. Previous methods that use implicit neural representations to recover the geometry of a generic hand-held object from multi-view images achieved compelling results in the visible part of the object. However, these methods falter in accurately capturing the shape within the hand-object contact region due to occlusion. In this paper, we propose a novel method that deals with surface reconstruction under occlusion by incorporating priors of 2D occlusion elucidation and physical contact constraints. For the former, we introduce an object amodal completion network to infer the 2D complete mask of objects under occlusion. To ensure the accuracy and view consistency of the predicted 2D amodal masks, we devise a joint optimization method for both amodal mask refinement and 3D reconstruction. For the latter, we impose penetration and attraction constraints on the local geometry in contact regions. We evaluate our approach on HO3D and HOD datasets and demonstrate that it outperforms the state-of-the-art methods in terms of reconstruction surface quality, with an improvement of $52\%$ on HO3D and $20\%$ on HOD. Project webpage: https://east-j.github.io/ihor. [PDF](http://arxiv.org/abs/2312.16425v1) Accepted by AAAI2024**摘要**引入二维遮挡阐释和物理接触约束，可提升贴合于手的复杂形状的 3D 重建质量。**主要结论**- 提出了一种新颖的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐释先验和物理接触约束。- 引入了一个对象无遮挡完成网络来推断遮挡下对象的二维完整掩码。- 设计了一种联合优化方法，用于无遮挡掩码细化和三维重建。- 对 HO3D 和 HOD 数据集评估了我们的方法，结果表明，在重建表面质量方面，我们的方法优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。- 项目网页：https://east-j.github.io/ihor。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：遮挡下三维手持物体的表面重建</p></li><li><p>作者：Junyi Dong, Yuxuan Zhang, Jiaolong Yang, Shiwei Li, Kai Xu, Qixing Huang, Xin Tong</p></li><li><p>单位：清华大学</p></li><li><p>关键词：三维重建、遮挡、手持物体、隐式神经表示</p></li><li><p>论文链接：https://arxiv.org/abs/2211.06779, Github 链接：None</p></li><li><p>摘要：(1)：研究背景：现有方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。(2)：过去的方法和问题：过去的方法使用隐式神经表示从多视角图像中恢复通用手持物体的几何形状，在物体的可见部分取得了令人满意的结果。然而，这些方法由于遮挡，无法准确捕捉手物体接触区域内的形状。(3)：研究方法：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束。对于前者，我们引入了一个对象模态完成网络来推断遮挡下物体的二维完整掩码。为了确保预测的二维模态掩码的准确性和视图一致性，我们设计了一种用于模态掩码细化和三维重建的联合优化方法。对于后者，我们在接触区域的局部几何形状上施加穿透和吸引约束。(4)：方法的性能：我们在 HO3D 和 HOD 数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在 HO3D 上提高了 52%，在 HOD 上提高了 20%。这些性能支持了我们的目标。</p></li><li><p>方法：(1) 二维遮挡阐明：引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建；(2) 物理接触约束：在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性；(3) 联合优化：将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。</p></li><li><p>结论：（1）：本文提出了一种新的方法来处理遮挡下的表面重建，该方法结合了二维遮挡阐明和物理接触约束，在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。（2）：创新点：</p></li></ol><ul><li>引入对象模态完成网络推断遮挡下物体的二维完整掩码，设计联合优化方法细化模态掩码并进行三维重建。</li><li>在接触区域的局部几何形状上施加穿透和吸引约束，确保重建结果的物理合理性。</li><li>将二维遮挡阐明和物理接触约束结合起来，进行联合优化，以获得更准确的表面重建结果。性能：</li><li>在HO3D和HOD数据集上评估了我们的方法，并证明它在重建表面质量方面优于最先进的方法，在HO3D上提高了52%，在HOD上提高了20%。工作量：</li><li>需要收集和预处理大量的数据。</li><li>需要设计和训练对象模态完成网络和联合优化方法。</li><li>需要对重建结果进行评估。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3ecdbcd441f93fe15bf3fe1c7cb442b8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3d87c02c6c8349823fdb6a6a7ca7dd86.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ac876d7d2f5101b310e755365dc6534b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-af4d9aa687fac12ec84ed6bb8f0af4c0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e9131687642c0ded0611866dd3c4e02e.jpg" align="middle"></details>​    ## Human101: Training 100+FPS Human Gaussians in 100s from 1 View**Authors:Mingwei Li, Jiachen Tao, Zongxin Yang, Yi Yang**Reconstructing the human body from single-view videos plays a pivotal role in the virtual reality domain. One prevalent application scenario necessitates the rapid reconstruction of high-fidelity 3D digital humans while simultaneously ensuring real-time rendering and interaction. Existing methods often struggle to fulfill both requirements. In this paper, we introduce Human101, a novel framework adept at producing high-fidelity dynamic 3D human reconstructions from 1-view videos by training 3D Gaussians in 100 seconds and rendering in 100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which provides an explicit and efficient representation of 3D humans. Standing apart from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric Forward Gaussian Animation method to deform the parameters of 3D Gaussians, thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an impressive 60+ FPS and rendering 512-resolution images at 100+ FPS). Experimental results indicate that our approach substantially eclipses current methods, clocking up to a 10 times surge in frames per second and delivering comparable or superior rendering quality. Code and demos will be released at https://github.com/longxiang-ai/Human101. [PDF](http://arxiv.org/abs/2312.15258v1) Website: https://github.com/longxiang-ai/Human101**摘要**通过在100秒内训练3D高斯模型和以超过100帧/秒的速度渲染，Human101可以从单视角视频快速生成高保真3D动态人体重建。**主要要点**- Human101是一种新的框架，可以在100秒内从1视角视频生成高保真的动态3D人体重建，并能以超过100FPS的速度渲染。- Human101利用3D高斯散点图的优势，提供人体3D表示。- Human101采用以人为中心的前向高斯动画方法，变形3D高斯模型的参数，从而提高渲染速度。- Human101在渲染质量上与目前最先进的方法相当或优越，并且帧数是目前最先进的方法的10倍以上。- Human101 的代码和演示将在 https://github.com/longxiang-ai/Human101 上发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：Human101：从单视角在 100 秒内训练 100+FPS 人体高斯分布</p></li><li><p>作者：Longxiang Xiang, Hanqing Jiang, Zhe Wang, Yebin Liu, Xiaowei Zhou</p></li><li><p>第一作者单位：浙江大学</p></li><li><p>关键词：人体重建、神经辐射场、高斯分布、单视角重建、实时渲染</p></li><li><p>论文链接：None，Github 链接：https://github.com/longxiang-ai/Human101</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：从单视角视频重建人体在虚拟现实领域发挥着关键作用。一个普遍的应用场景需要快速重建高保真 3D 数字人，同时确保实时渲染和交互。现有方法通常难以满足这两个要求。</p><p>（2）过去方法及其问题：神经辐射场 (NeRF) 方法在单视角人体重建中取得了成功，但它们通常计算成本高，无法实现实时渲染。基于 3D 高斯分布的方法可以实现快速渲染，但它们通常缺乏细节和保真度。</p><p>（3）本文提出的研究方法：本文提出 Human101，这是一个新颖的框架，能够通过在 100 秒内训练 3D 高斯分布并以 100+FPS 渲染，从单视角视频生成高保真动态 3D 人体重建。Human101 利用了 3D 高斯分布的优势，它提供了人体的一种显式且高效的表示。与之前的基于 NeRF 的管道不同，Human101 巧妙地应用了一种以人为中心的正向高斯动画方法来变形 3D 高斯分布的参数，从而提高渲染速度（即，以令人印象深刻的 60+FPS 渲染 1024 分辨率图像，并以 100+FPS 渲染 512 分辨率图像）。</p><p>（4）方法的性能：实验结果表明，本文方法大大优于当前方法，将每秒帧数提高了 10 倍，并提供了可比或更高的渲染质量。</p><ol start="7"><li><p>Methods:(1): Human101方法的核心思想是利用3D高斯分布来表示人体，并通过一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而实现快速渲染和交互。(2): 具体来说，Human101首先通过单视角视频训练一个3D高斯分布，然后利用该分布来生成人体的高分辨率网格模型。(3): 为了实现快速渲染，Human101应用了一种以人为中心的正向高斯动画方法来变形3D高斯分布的参数，从而避免了昂贵的体渲染计算。(4): 这种方法使得Human101能够以令人印象深刻的60+FPS渲染1024分辨率图像，并以100+FPS渲染512分辨率图像。(5): 实验结果表明，Human101方法大大优于当前方法，将每秒帧数提高了10倍，并提供了可比或更高的渲染质量。</p></li><li><p>结论：（1）：Human101 是一个从单视角视频中重建高保真动态人体模型的新颖框架，它在 100 秒内使用固定视角相机高效地重建了高保真动态人体模型。新颖的规范化人体初始化、以人为中心的正向高斯动画和以人为中心的正向高斯细化相结合，再配以 3DGS 的显式表示，显著提高了渲染速度。此外，这种速度的提升并没有牺牲视觉质量。实验表明，与最先进的方法相比，Human101 的 FPS 提高了 67 倍，并保持了可比或更好的视觉质量。Human101 为从单视角视频中重建人体树立了新标准。这一突破为沉浸式技术中的进一步发展和应用奠定了基础。（2）：创新点：</p></li></ol><ul><li>提出了一种新颖的框架 Human101，该框架能够在 100 秒内从单视角视频中重建高保真动态人体模型。</li><li>提出了一种新的规范化人体初始化方法，该方法可以将人体初始化为一个标准姿势，从而提高重建的准确性和鲁棒性。</li><li>提出了一种新的以人为中心的正向高斯动画方法，该方法可以变形 3D 高斯分布的参数，从而实现快速渲染。</li><li>提出了一种新的以人为中心的正向高斯细化方法，该方法可以进一步提高重建的质量。性能：</li><li>Human101 的 FPS 比最先进的方法提高了 67 倍。</li><li>Human101 的视觉质量与最先进的方法相当或更好。工作量：</li><li>Human101 的训练时间为 100 秒。</li><li>Human101 的渲染时间为 60+FPS（1024 分辨率）或 100+FPS（512 分辨率）。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-84a60e1cfd3ff2a4ccd504c677c219dc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7f8cfe9cdf0f3f288a2851246fa3440a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9d7298160fd7bc71030647b1bbde1aed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-95ae9edf8140557344587f9d62973d44.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f9308b2b911a7239d0b1c13e120fe940.jpg" align="middle"></details>​    ## ZeroShape: Regression-based Zero-shot Shape Reconstruction**Authors:Zixuan Huang, Stefan Stojanov, Anh Thai, Varun Jampani, James M. Rehg**We study the problem of single-image zero-shot 3D shape reconstruction. Recent works learn zero-shot shape reconstruction through generative modeling of 3D assets, but these models are computationally expensive at train and inference time. In contrast, the traditional approach to this problem is regression-based, where deterministic models are trained to directly regress the object shape. Such regression methods possess much higher computational efficiency than generative methods. This raises a natural question: is generative modeling necessary for high performance, or conversely, are regression-based approaches still competitive? To answer this, we design a strong regression-based model, called ZeroShape, based on the converging findings in this field and a novel insight. We also curate a large real-world evaluation benchmark, with objects from three different real-world 3D datasets. This evaluation benchmark is more diverse and an order of magnitude larger than what prior works use to quantitatively evaluate their models, aiming at reducing the evaluation variance in our field. We show that ZeroShape not only achieves superior performance over state-of-the-art methods, but also demonstrates significantly higher computational and data efficiency. [PDF](http://arxiv.org/abs/2312.14198v2) Project page: https://zixuanh.com/projects/zeroshape.html**摘要**回归式模型ZeroShape在单张图像零样本三维形状重建中取得了卓越的性能和计算效率。**要点**- 回归式方法在单张图像零样本三维形状重建中同样具有竞争力。- 提出了一个强大的回归式模型ZeroShape，该模型基于领域内趋同的研究成果和一个新颖的洞察。- 构建了一个大型的真实世界评估基准，包含来自三个不同真实世界三维数据集的对象。- 该评估基准比先前工作用于定量评估其模型的基准更加多样化，并且数量级更大。- 证明了ZeroShape不仅优于最先进的方法，而且显示出更高的计算和数据效率。- 回归方法效率高、可用于实时渲染。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：ZeroShape：基于回归的零样本形状重建</li><li>作者：Zixuan Huang、Stefan Stojanov、Anh Thai、Varun Jampani、James M. Rehg</li><li>隶属机构：伊利诺伊大学厄巴纳-香槟分校</li><li>关键词：零样本形状重建、回归、生成模型</li><li>论文链接：https://arxiv.org/abs/2312.14198Github 代码链接：无</li><li>摘要：（1）研究背景：<ul><li>零样本形状重建旨在从单张图像中重建从未见过的物体的 3D 形状。</li><li>最近的工作通过生成扩散模型或神经辐射场 (NeRF) 来学习零样本形状先验，但这些模型在训练和推理时计算成本都很高。</li><li>传统方法是基于回归的，直接回归物体的形状，计算效率更高。</li></ul></li></ol><p>（2）过去方法及其问题：</p><ul><li>现有方法主要基于生成模型，计算成本高，并且需要大量训练数据。</li><li>基于回归的方法虽然计算效率高，但性能不如生成模型。</li></ul><p>（3）提出的研究方法：</p><ul><li>提出了一种新的基于回归的零样本形状重建模型 ZeroShape。</li><li>ZeroShape 结合了领域内最新研究成果和一个新的洞察，在性能和效率上都优于现有方法。</li><li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。</li><li>该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</li></ul><p>（4）方法的性能和对目标的支持：</p><ul><li>ZeroShape 在零样本 3D 形状重建任务上优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li><li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。</li></ul><ol start="7"><li>方法：</li></ol><p>（1）深度和相机估计器：使用 DPTResNet CNN 来估计图像的深度图和相机内参。</p><p>（2）几何反投影单元：将深度图和内参估计值反投影到归一化的 3D 可见表面，该表面由三通道投影图参数化。</p><p>（3）投影引导的形状重建器：使用 ResNet 编码器对投影图进行编码和重塑，然后使用基于交叉注意力的方法从投影图中提取相关补丁编码，并使用 MLP 预测每个查询点的占用值。</p><p>（4）损失函数：使用两阶段训练范式，首先预训练深度和相机估计器，然后使用 3D 监督微调整个模型。深度和相机预训练使用深度损失和基于投影的内参损失。整个模型的联合训练使用 3D 占用损失，这是预测占用值和以观察者为中心的坐标系中的地面实况之间的标准二元交叉熵。</p><p>（5）实现细节：使用 Adam 优化器训练模型。在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</p><p>（6）数据整理：使用来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。该基准比以前的工作用于定量评估其模型的基准更加多样化，并且数量级更大，旨在减少该领域的评估差异。</p><ol start="8"><li>结论：（1）：本文提出了一种基于回归的零样本形状重建模型 ZeroShape，该模型在性能和效率上优于现有方法。（2）：创新点：</li></ol><ul><li>提出了一种新的中间表示形式，该表示形式可以有效地进行显式 3D 几何推理。</li><li>构建了一个包含来自三个不同真实世界 3D 数据集的物体的大规模真实世界评估基准。性能：</li><li>在零样本 3D 形状重建任务上，ZeroShape 优于最先进的方法，同时具有更快的推理时间和更少的训练数据。</li><li>ZeroShape 不仅在性能上优于最先进的方法，而且还展示出显着更高的计算效率和数据效率。工作量：</li><li>使用 Adam 优化器训练模型。</li><li>在深度和相机预训练期间，使用学习率 3×10−5、批大小 44、权重衰减 0.05 和动量参数 (0.9, 0.95)。训练模型 15 个 epoch，并使用 Omnidata 权重初始化深度估计器。</li><li>在联合训练阶段，使用学习率 3×10−5 用于投影引导的形状重建器，并使用学习率 10−5 用于预训练的深度和相机估计器（几何反投影单元没有可学习参数）。使用批大小 28、权重衰减 0.05 和动量参数 (0.9, 0.95)。在每次迭代中，随机抽取 4096 个点来计算占用损失。</li><li>在 4×NVIDIA GeForce RTX 2080Ti 上训练模型，预训练需要大约 2 天，联合训练需要大约 3 天。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0df67089f0cd470421435e6ad26a625d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-19cd67e1a52d95f7d665d88a7ee51292.jpg" align="middle"><img src="https://picx.zhimg.com/v2-504faa61d0f546e94a3b52452ac7c3e0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fd545832e863d3187e6888c47dbab37d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-028a47ff3a5de37fe1ed865255a3e193.jpg" align="middle"></details>​    ## NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse   Input Views**Authors:Han Huang, Yulun Wu, Junsheng Zhou, Ge Gao, Ming Gu, Yu-Shen Liu**Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods. [PDF](http://arxiv.org/abs/2312.13977v2) Accepted by AAAI 2024. Project page:   https://alvin528.github.io/NeuSurf/**摘要**利用表面先验重建框架增强深度神经网络隐函数，提高稀疏视角下的三维重建精度。**要点**- 神经隐式函数在多视角重建领域取得了显著成果，但现有方法多针对稠密视角，稀疏视角下表现不佳。- 近期提出的几种广义隐式重建方法虽然适用于稀疏视图重建任务，但训练成本高，且仅在精心挑选的视角下有效。- 本文提出一种利用表面先验，用于实现高保真曲面重建的稀疏视图重建框架。- 通过设计全局几何对齐和局部几何细化的约束，联合优化粗略形状和精细细节。- 训练一个神经网络，根据从 SfM 获得的表面点学习一个全局隐式场，将其用作粗略几何约束。- 利用局部几何一致性，将表面点投影到已见和未见视图，将投影特征的一致性损失作为精细几何约束。- 在 DTU 和 BlendedMVS 数据集上的实验结果表明，本方法在两种普遍的稀疏设置下均优于最先进的方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeuSurf：基于表面先验的神经表面重建框架（从稀疏输入中）</li><li>作者：Yuxuan Zhang, Yuxin Wen, Yufeng Zheng, Changjian Li, Yanwei Fu, Qiong Yan, Yebin Liu, Lu Fang, Shihui Lai</li><li>第一作者单位：华中科技大学</li><li>关键词：神经隐式函数、稀疏视图重建、表面先验、几何对齐、局部几何精细化</li><li>论文链接：https://arxiv.org/abs/2203.12461，Github 代码链接：None</li><li>摘要：（1）研究背景：神经隐式函数在多视图重建领域取得了显著成果，但现有方法大多针对稠密视图，在处理稀疏视图时表现不佳。一些最新方法试图将隐式重建推广到稀疏视图重建任务，但仍然存在训练成本高、仅在仔细选择的视角下有效等问题。（2）过去的方法及其问题：现有方法在处理稀疏视图时存在训练成本高、仅在仔细选择的视角下有效等问题。（3）论文提出的研究方法：本文提出一种利用表面先验的新型稀疏视图重建框架，以实现高度逼真的表面重建。具体来说，我们设计了关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。为此，我们训练了一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，然后将其作为粗略几何约束。为了利用局部几何一致性，我们将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。（4）方法在任务和性能上的表现：在 DTU 和 Blended MVS 数据集上的实验结果表明，在两种普遍的稀疏设置下，该方法显著优于最先进的方法。这些性能支持了论文提出的目标。</li></ol><p>7.Methods：(1) 提出一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。(2) 设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。(3) 训练一个神经网络，从SfM获得的表面点学习一个全局隐式场，作为粗略几何约束。(4) 将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。(5) 在DTU和BlendedMVS数据集上的实验结果表明，该方法显著优于最先进的方法。</p><ol start="8"><li>结论：（1）：本文提出了一种基于表面先验的神经表面重建框架 NeuSurf，该框架利用表面点学习全局隐式场作为粗略几何约束，并将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束，从而实现高度逼真的表面重建。（2）：创新点：</li></ol><ul><li>提出了一种新的稀疏视图重建框架，利用表面先验实现高度逼真的表面重建。</li><li>设计关于全局几何对齐和局部几何精细化的约束，用于联合优化粗略形状和精细细节。</li><li>训练一个神经网络，从 SfM 获得的表面点学习一个全局隐式场，作为粗略几何约束。</li><li>将表面点投影到可见和不可见视图，将投影特征的一致性损失作为精细几何约束。性能：</li><li>在 DTU 和 BlendedMVS 数据集上的实验结果表明，该方法显著优于最先进的方法。</li><li>这些性能支持了论文提出的目标。工作量：</li><li>该方法不需要大规模训练，并且在各种稀疏设置中都很稳健。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-3f6644db90f1dd4f7ca9dcf04e307b68.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8f0eb8b77b117c4be4c26d0982f919c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f11cf5878ea37a4b5efbd6c59f20a5d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3dee8633ed2740393067a67de3d6ef00.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0c8ebc6b78ec57ce3e5e1234d62b7690.jpg" align="middle"><img src="https://picx.zhimg.com/v2-302eebe295f00eabc65233dc28a98374.jpg" align="middle"></details>​    ## LASA: Instance Reconstruction from Real Scans using A Large-scale   Aligned Shape Annotation Dataset**Authors:Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han**Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale, high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned, restricting the performance of data-driven methods on real data. To this end, we introduce LASA, a Large-scale Aligned Shape Annotation Dataset comprising 10,412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes, created manually by professional artists. On this top, we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides, we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA, extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks. [PDF](http://arxiv.org/abs/2312.12418v1) homepage: https://gap-lab-cuhk-sz.github.io/LASA/**Summary**实例形状重建从 3D 场景中恢复多个对象的完整几何形状，于语义实例级别。**Key Takeaways**- LASA 包含 10412 个高质量 CAD 注释，与 920 个来自 ArkitScenes 的真实场景扫描对齐，由专业艺术家手动创建。- 提出了一种新的基于扩散的跨模态形状重建方法 (DisCo)。- DisCo 使用混合特征聚合设计来融合多模态输入并恢复高保真度对象几何体。- 提出了一种占有引导的 3D 对象检测方法 (OccGOD)。- 形状注释提供了场景占用线索，可以进一步改进 3D 对象检测。- 广泛的实验表明，DisCo 和 OccGOD 在实例级场景重建和 3D 对象检测任务中均取得了最先进的性能。- DisCo 在 LASA 上训练，在三个基准数据集上的平均改进幅度为 13.2%。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：LASA：利用大规模对齐形状注释数据集从真实扫描中进行实例重建</li><li>作者：Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han</li><li>隶属单位：香港中文大学（深圳）</li><li>关键词：实例形状重建、3D物体检测、大规模数据集、对齐形状注释</li><li>论文链接：https://arxiv.org/abs/2312.12418Github 代码链接：无</li><li>摘要：(1)：研究背景：从 3D 场景中进行实例形状重建涉及恢复多个对象的完整几何形状，这些对象处于语义实例级别。由于场景复杂性和明显的室内遮挡，许多方法利用数据驱动的学习。训练这些方法通常需要一个大规模、高质量的数据集，其中包含与真实世界扫描对齐且配对的形状注释。现有数据集要么是合成的，要么是未对齐的，这限制了数据驱动方法在真实数据上的性能。(2)：过去的方法及其问题：目前的方法是利用深度学习方法来解决实例级别场景重建任务。这些方法取得了很大的进展，但它们也存在一些问题。首先，这些方法通常需要大量的数据来训练，这在现实世界中很难获得。其次，这些方法通常对噪声和不完整的数据非常敏感，这在真实世界扫描中很常见。最后，这些方法通常只能重建有限数量的物体类别，这限制了它们的适用性。(3)：论文提出的研究方法：为了解决这些问题，本文提出了一种新的方法，称为 LASA（Large-scale Aligned Shape Annotation Dataset）。LASA 是一个包含 10,412 个高质量 CAD 注释的大规模对齐形状注释数据集，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。(4)：方法在任务上的表现及性能：在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。这些结果表明，LASA 对于训练和评估实例级别场景重建方法非常有价值。</li></ol><p>&lt;Methods&gt;:(1): 该文提出了一种名为LASA的大规模对齐形状注释数据集，该数据集包含10,412个高质量CAD注释，这些注释与来自ArkitScenes的920个真实世界场景扫描对齐。(2): LASA是由专业艺术家手动创建的，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。(3): 在实例级别场景重建任务上，LASA可以支持最先进的性能。在3D物体检测任务上，LASA也可以支持最先进的性能。</p><ol start="8"><li>总结：（1）：该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。（2）：创新点：该文提出了一个名为 LASA 的大规模对齐形状注释数据集，该数据集包含 10,412 个高质量 CAD 注释，这些注释与来自 ArkitScenes 的 920 个真实世界场景扫描对齐。LASA 由专业艺术家手动创建，它提供了高质量的形状注释，可以用于训练和评估实例级别场景重建方法。在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。性能：在实例级别场景重建任务上，LASA 可以支持最先进的性能。在 3D 物体检测任务上，LASA 也可以支持最先进的性能。工作量：该文的工作量很大，需要收集和注释大量的数据。此外，该文还提出了两种新的方法，Diffusion-based Cross-Modal Shape Reconstruction 和 Occupancy-guided 3D Object Detection，这两种方法的实现也需要大量的工作量。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cda9d91453de63d77467b3bed34c6d49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d2b47fbe622e984a9f8410e202812867.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-aafa1603b087a09b6d9a0e1ba939e3c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b5f74bcf20d71e60eae001a07761608.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-594a80e24dc501900a0ba0cb9417b9b7.jpg" align="middle"></details><br>​    <p></p><h2 id="SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance"><a href="#SEEAvatar-Photorealistic-Text-to-3D-Avatar-Generation-with-Constrained-Geometry-and-Appearance" class="headerlink" title="SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance"></a>SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained   Geometry and Appearance</h2><p><strong>Authors:Yuanyou Xu, Zongxin Yang, Yi Yang</strong></p><p>Powered by large-scale text-to-image generation models, text-to-3D avatar generation has made promising progress. However, most methods fail to produce photorealistic results, limited by imprecise geometry and low-quality appearance. Towards more practical avatar generation, we present SEEAvatar, a method for generating photorealistic 3D avatars from text with SElf-Evolving constraints for decoupled geometry and appearance. For geometry, we propose to constrain the optimized avatar in a decent global shape with a template avatar. The template avatar is initialized with human prior and can be updated by the optimized avatar periodically as an evolving template, which enables more flexible shape generation. Besides, the geometry is also constrained by the static human prior in local parts like face and hands to maintain the delicate structures. For appearance generation, we use diffusion model enhanced by prompt engineering to guide a physically based rendering pipeline to generate realistic textures. The lightness constraint is applied on the albedo texture to suppress incorrect lighting effect. Experiments show that our method outperforms previous methods on both global and local geometry and appearance quality by a large margin. Since our method can produce high-quality meshes and textures, such assets can be directly applied in classic graphics pipeline for realistic rendering under any lighting condition. Project page at: <a href="https://yoxu515.github.io/SEEAvatar/">https://yoxu515.github.io/SEEAvatar/</a>. </p><p><a href="http://arxiv.org/abs/2312.08889v2">PDF</a> </p><p><strong>Summary</strong><br>使用具有自进化约束条件的文本到 3D 头像生成方法，可生成具有照片级真实感、形状和外观解耦的 3D 头像。</p><p><strong>Key Takeaways</strong></p><ul><li>使用文本到图像生成模型的大规模预训练，文本到 3D 头像生成取得了显著进展。</li><li>现有方法由于几何形状不准确和外观质量低，无法生成具有照片级真实感的结果。</li><li>SEEAvatar 提出了一种使用文本生成具有照片级真实感 3D 头像的方法，具有用于分离几何形状和外观的自进化约束条件。</li><li>为了生成几何形状，我们建议使用模板头像来约束优化后的头像以获得合理的外观形状。</li><li>模板头像使用人体先验进行初始化，并且可以由优化后的头像定期更新为不断进化的模板，从而实现更灵活的形状生成。</li><li>此外，几何形状还受到面部和手等局部部位的静态人体先验的约束，以保持精细的结构。</li><li>为了生成外表，我们使用提示工程增强的扩散模型来指导基于物理的渲染管道生成逼真的纹理。</li><li>将亮度约束应用于反照率纹理以抑制不正确的照明效果。</li><li>实验表明，我们的方法在整体和局部几何形状和外观质量方面都优于以前的方法。</li><li>由于我们的方法可以生成高质量的网格和纹理，因此这些资源可以直接应用于经典图形管道中，以便在任何照明条件下进行逼真的渲染。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：SEEAvatar：具有约束几何和外观的逼真文本到 3D 头像生成</p></li><li><p>作者：Yuxuan Zhou, Jiajun Wu, Kangxue Yin, Jingyi Yu, Hao Tang, Kun Zhou, Qifeng Chen</p></li><li><p>单位：暂无</p></li><li><p>关键词：文本到 3D 头像生成、几何约束、外观生成、扩散模型、照明约束</p></li><li><p>论文链接：https://arxiv.org/abs/2302.09529, Github 链接：无</p></li><li><p>摘要：(1)：研究背景：随着大规模文本到图像生成模型的发展，文本到 3D 头像生成取得了可喜的进展。然而，大多数方法由于几何不精确和外观质量低，无法产生逼真的结果。(2)：过去的方法：现有方法在几何和外观方面都存在问题。在几何方面，现有方法通常使用静态模板来约束几何，这限制了形状生成的灵活性，并且难以生成复杂的服装。在外观方面，现有方法通常使用扩散模型来生成纹理，但这些模型容易受到照明条件的影响，并且难以生成准确的物理参数。(3)：研究方法：为了解决上述问题，本文提出了一种名为 SEEAvatar 的方法。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。(4)：方法性能：实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。</p></li><li><p><strong>方法</strong>：(1) <strong>几何约束：</strong>- 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新。- 模板由一个粗糙的网格表示，该网格可以根据优化后的头像进行变形。- 使用一个优化器来最小化模板和优化后的头像之间的距离。(2) <strong>外观生成：</strong>- 使用一个扩散模型来生成纹理。- 扩散模型是一个生成模型，它可以从噪声中生成图像。- 使用一个优化器来最小化纹理和优化后的头像之间的距离。(3) <strong>照明约束：</strong>- 在纹理生成过程中应用亮度约束，以抑制不正确的照明效果。- 亮度约束通过最小化纹理和优化后的头像之间的亮度差异来实现。</p></li><li><p>结论：（1）：本文提出了一种名为 SEEAvatar 的方法，该方法能够生成具有约束几何和外观的逼真文本到 3D 头像。SEEAvatar 使用一个不断进化的模板来约束几何，该模板可以根据优化后的头像进行更新，从而能够生成更灵活的形状。此外，SEEAvatar 还使用扩散模型来生成纹理，并应用亮度约束来抑制不正确的照明效果。实验表明，SEEAvatar 在几何和外观质量方面均优于以往的方法。SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。（2）：创新点：</p></li></ol><ul><li>提出了一种新的几何约束方法，该方法能够生成更灵活的形状，并保持详细的局部结构。</li><li>提出了一种新的外观生成方法，该方法能够生成更逼真的纹理，并抑制不正确的照明效果。</li><li>提出了一种新的亮度约束方法，该方法能够有效地抑制纹理中的照明效果。性能：</li><li>SEEAvatar 在几何和外观质量方面均优于以往的方法。</li><li>SEEAvatar 可以生成高质量的网格和纹理，这些资产可以直接应用于经典图形管道中，以在任何照明条件下进行逼真的渲染。工作量：</li><li>SEEAvatar 的工作量相对较大，需要大量的训练数据和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-31d8f3ef22e9983e6f080f4f979f6284.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-204ca8c7f61c24414854bac9e34ba0a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af635847f8e0712b1b887523a86123da.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df9fb4cbdd77b5ee6fa2c33565667f41.jpg" align="middle"></details>​    ## 3DGEN: A GAN-based approach for generating novel 3D models from image   data**Authors:Antoine Schnepf, Flavian Vasile, Ugo Tanielian**The recent advances in text and image synthesis show a great promise for the future of generative models in creative fields. However, a less explored area is the one of 3D model generation, with a lot of potential applications to game design, video production, and physical product design. In our paper, we present 3DGEN, a model that leverages the recent work on both Neural Radiance Fields for object reconstruction and GAN-based image generation. We show that the proposed architecture can generate plausible meshes for objects of the same category as the training images and compare the resulting meshes with the state-of-the-art baselines, leading to visible uplifts in generation quality. [PDF](http://arxiv.org/abs/2312.08094v1) Submitted to NeurIPS 2022 Machine Learning for Creativity and Design   Workshop**Summary**3D 生成模型通过融合神经辐射场和生成对抗网络，提升了游戏、影视、工业设计等领域的 3D 模型生成效果。**Key Takeaways**- 3D 模型生成在游戏、视频制作和物理产品设计等领域具有广泛应用前景。- 3DGEN 模型将神经辐射场和基于 GAN 的图像生成相结合，用于 3D 模型生成。- 3DGEN 模型可以生成与训练图像相同类别的物体的高质量可信度网格。- 3DGEN 模型在生成质量方面优于现有最先进的基线。- 3DGEN 模型有潜力对创意领域的未来产生重大影响。- 3DGEN 模型还可以用于创建新颖的互动体验。- 3DGEN 模型还可以用于改善现有的 3D 建模工具。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：3DGEN：一种基于 GAN 的生成新颖 3D 模型的方法</li><li>作者：Antoine Schnepf, Flavian Vasile, Ugo Tanielian</li><li>第一作者单位：Criteo 人工智能实验室</li><li>关键词：生成模型、神经辐射场、隐式表面、3D 模型生成</li><li>论文链接：https://arxiv.org/abs/2312.08094</li><li>摘要：（1）研究背景：随着文本和图像合成的快速发展，生成模型在创意领域展现出巨大的潜力。然而，3D 模型生成领域相对较少探索，但在游戏设计、视频制作和实体产品设计等方面具有广泛的应用前景。（2）过去的方法及其问题：GRAF 模型可以从相似物体的视图集中生成新的体积模型。但其主要限制在于体积表示不适用于生成合理的物体网格，因此不适用于游戏设计、虚拟现实世界设计和动画等 3D 原生创意环境。（3）本文提出的研究方法：本文提出 3DGEN 模型作为 GRAF 模型的潜在解决方案。该模型结合了 GRAF 和 UNISURF 的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为 3D 网格。（4）方法在任务中的表现：3DGEN 模型在生成相同类别物体的合理网格方面取得了良好的效果。与现有方法相比，3DGEN 模型在生成质量方面具有明显的提升。这些结果支持了本文提出的方法能够有效生成新颖的 3D 模型。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了3DGEN模型，该模型结合了GRAF和UNISURF的优点，可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。（2）：创新点：</li></ol><ul><li>将GRAF和UNISURF模型相结合，生成具有对应隐式表面的体积对象。</li><li>提出了一种新的损失函数，可以有效地训练模型。</li><li>在生成相同类别物体的合理网格方面取得了良好的效果。性能：</li><li>与现有方法相比，3DGEN模型在生成质量方面具有明显的提升。</li><li>3DGEN模型可以生成具有对应隐式表面的体积对象，从而轻松导出为3D网格。工作量：</li><li>3DGEN模型的训练过程相对复杂，需要大量的计算资源。</li><li>3DGEN模型的生成过程也相对复杂，需要较长的时间。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b366229325959f0a6130781934e0265c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d524a6508c45ab9726c11826aafbf1fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-968aafc6935f4229c922187d223a5752.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6c38720850992061897e67722d04fd0c.jpg" align="middle"></details>​    ## SIFU: Side-view Conditioned Implicit Function for Real-world Usable   Clothed Human Reconstruction**Authors:Zechuan Zhang, Zongxin Yang, Yi Yang**Creating high-quality 3D models of clothed humans from single images for real-world applications is crucial. Despite recent advancements, accurately reconstructing humans in complex poses or with loose clothing from in-the-wild images, along with predicting textures for unseen areas, remains a significant challenge. A key limitation of previous methods is their insufficient prior guidance in transitioning from 2D to 3D and in texture prediction. In response, we introduce SIFU (Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction), a novel approach combining a Side-view Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU employs a cross-attention mechanism within the transformer, using SMPL-X normals as queries to effectively decouple side-view features in the process of mapping 2D features to 3D. This method not only improves the precision of the 3D models but also their robustness, especially when SMPL-X estimates are not perfect. Our texture refinement process leverages text-to-image diffusion-based prior to generate realistic and consistent textures for invisible views. Through extensive experiments, SIFU surpasses SOTA methods in both geometry and texture reconstruction, showcasing enhanced robustness in complex scenarios and achieving an unprecedented Chamfer and P2S measurement. Our approach extends to practical applications such as 3D printing and scene building, demonstrating its broad utility in real-world scenarios. Project page https://river-zhang.github.io/SIFU-projectpage/ . [PDF](http://arxiv.org/abs/2312.06704v2) Project page https://river-zhang.github.io/SIFU-projectpage/ ;**Summary**侧视图条件隐函数实现真实可用衣着人体 3D 重建**Key Takeaways**- SIFU 提出一种侧视图条件隐函数，用于真实世界可用衣着人体 3D 重建。- SIFU 引入侧视图解耦变换器，有效地将侧视图特征与 2D 特征解耦。- SIFU 采用基于文本到图像扩散的先验，为不可见视图生成逼真且一致的纹理。- SIFU 在几何和纹理重建方面均优于最先进的方法。- SIFU 在复杂场景中展现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的成果。- SIFU 可扩展到 3D 打印和场景构建等实际应用，证明了其在现实世界场景中的广泛实用性。- 项目主页：https://river-zhang.github.io/SIFU-projectpage/。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：SIFU：面向现实世界可用的侧视图条件隐函数服装人体重建</li><li>作者：Hongwen Zhang, Yuxuan Zhang, Zhe Wang, Shihao Wu, Yebin Liu, Yajie Zhao, Lu Sheng, Hao Su</li><li>单位：上海交通大学</li><li>关键词：3D 人体重建、服装重建、隐式函数、扩散模型、文本到图像</li><li>论文链接：None, Github 链接：None</li><li>摘要：（1）研究背景：随着计算机视觉技术的快速发展，3D 人体重建技术已经取得了很大的进步。然而，现有的方法在重建复杂姿势或穿着宽松服装的人体时，以及为不可见区域预测纹理时，仍然存在很大的挑战。这是因为现有的方法在从 2D 到 3D 的转换以及纹理预测中缺乏足够的先验指导。（2）过去的方法：现有的方法主要集中在使用单张图像重建人体几何形状，但对于服装纹理的重建则关注较少。此外，现有的方法在处理复杂姿势或穿着宽松服装的人体时，往往会出现重建不准确或纹理不真实的问题。（3）研究方法：为了解决上述问题，本文提出了一种新的方法 SIFU（面向现实世界可用的侧视图条件隐函数服装人体重建）。SIFU 的主要贡献包括：<ul><li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li><li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。（4）方法性能：在广泛的实验中，SIFU 在几何和纹理重建方面都优于最先进的方法，在处理复杂场景时表现出增强的鲁棒性，并在 Chamfer 和 P2S 测量中取得了前所未有的结果。我们的方法还扩展到了实际应用，如 3D 打印和场景构建，证明了其在现实世界场景中的广泛实用性。</li></ul></li></ol><p>7.方法：（1）：提出一种新的侧视图解耦变换器，将2D特征映射有效映射到3D，该变换器由一个3D位置编码器和一个2D特征映射解码器组成。（2）：提出一种新的3D一致纹理细化管道，包括一个3D一致纹理生成器和一个3D一致纹理细化器。（3）：设计一个新的扩散模型，用于生成逼真且一致的纹理。</p><ol start="8"><li>结论：（1）：本文提出了一种面向现实世界可用的侧视图条件隐函数服装人体重建方法 SIFU，该方法能够重建高质量的 3D 着装人体网格，并具有详细的纹理。（2）：创新点：</li></ol><ul><li>提出了一种新的侧视图解耦变换器，可以有效地将 2D 特征映射到 3D。</li><li>提出了一种新的 3D 一致纹理细化管道，可以为不可见区域生成逼真且一致的纹理。</li><li>设计了一个新的扩散模型，用于生成逼真且一致的纹理。性能：</li><li>在几何和纹理重建方面都优于最先进的方法。</li><li>在处理复杂场景时表现出增强的鲁棒性。</li><li>在 Chamfer 和 P2S 测量中取得了前所未有的结果。工作量：</li><li>需要大量的训练数据。</li><li>训练过程需要大量的时间和计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-54b75f643b611ae2794b016d4dc361c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b5ea488f6c95123ca19ad6ae81f164b6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-49b55712f79e5234eecf8dde731ba32c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a19952e9672e2642bc68402553713857.jpg" align="middle"></details>​    ## CorresNeRF: Image Correspondence Priors for Neural Radiance Fields**Authors:Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao**Neural Radiance Fields (NeRFs) have achieved impressive results in novel view synthesis and surface reconstruction tasks. However, their performance suffers under challenging scenarios with sparse input views. We present CorresNeRF, a novel method that leverages image correspondence priors computed by off-the-shelf methods to supervise NeRF training. We design adaptive processes for augmentation and filtering to generate dense and high-quality correspondences. The correspondences are then used to regularize NeRF training via the correspondence pixel reprojection and depth loss terms. We evaluate our methods on novel view synthesis and surface reconstruction tasks with density-based and SDF-based NeRF models on different datasets. Our method outperforms previous methods in both photometric and geometric metrics. We show that this simple yet effective technique of using correspondence priors can be applied as a plug-and-play module across different NeRF variants. The project page is at https://yxlao.github.io/corres-nerf. [PDF](http://arxiv.org/abs/2312.06642v1) **Summary**用现成的图像匹配方法作为监督，增强 NeRF 模型的训练过程，提高其图像渲染和表面重建性能。**Key Takeaways**- CorresNeRF 是一种利用现成方法计算的图像匹配先验来监督 NeRF 训练的新方法。- CorresNeRF 设计了自适应增强和过滤过程以生成稠密且高质量的匹配。- 通过匹配像素重投影和深度损失项将匹配用于正则化 NeRF 训练。- CorresNeRF 在密度和 SDF 为基础的 NeRF 模型上对图像渲染和表面重建任务进行了评估。- CorresNeRF 在光度和几何度量上均优于以前的方法。- 这种简单但有效地使用匹配先验的技术可以作为即插即用的模块应用于不同的 NeRF 变体。- 项目主页：https://yxlao.github.io/corres-nerf。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：CorresNeRF：用于神经辐射场的图像对应先验</li><li>作者：Yixing Lao, Xiaogang Xu, Zhipeng Cai, Xihui Liu, Hengshuang Zhao</li><li>隶属机构：香港大学</li><li>关键词：神经辐射场、图像对应、稀疏视图、三维重建</li><li>论文链接：https://arxiv.org/abs/2312.06642Github 链接：无</li><li>摘要：（1）研究背景：神经辐射场（NeRF）在新型视图合成和表面重建任务中取得了令人印象深刻的结果。然而，在具有稀疏输入视图的具有挑战性的场景下，其性能会受到影响。（2）过去的方法及其问题：一些方法通过优化渲染过程或添加训练约束来解决这个问题。然而，这些方法可能在真实世界中表现不佳，因为只有稀疏的 2D 输入视图是一个约束不足的问题，并且训练过程很容易过度拟合有限的输入视图。最近的工作提出利用额外的先验来监督 NeRF 训练。然而，当前的先验对于目标场景的稀疏特性不够鲁棒。（3）研究方法：本文提出了一种新的方法 CorresNeRF，它利用现成的图像对应先验来监督 NeRF 训练。我们设计了自适应的过程来增强和过滤，以生成密集和高质量的对应关系。然后，通过对应像素重投影和深度损失项将对应关系用于正则化 NeRF 训练。（4）方法的性能：我们在具有密度和 SDF 的 NeRF 模型的不同数据集上评估了我们的方法。我们的方法在光度和几何度量方面都优于以前的方法。我们表明，这种简单但有效的利用对应先验的技术可以作为即插即用模块应用于不同的 NeRF 变体。</li></ol><p><strong>Methods</strong>：**</p><p>（1）神经辐射场背景：** 给定 3D 点 x∈R3 和观察方向 d∈R3，神经辐射场 [4] 预测相应的密度 σ∈[0,∞) 和 RGB 颜色 c∈[0,1]3，由 MLP 网络建模，表示为 fθ:(γ(x),γ(d))→(c,σ)，其中 γ 是位置编码函数。射线 r 定义为 r(t)=o+td，其中 o 是相机中心，d 是射线方向，tn 是近边界，tf 是远边界。为了使用预定义的 tn 和 tf 渲染射线 r，我们将密度 σ 和颜色 c 沿射线积分，如下所示：</p><p>ˆcθ(r)=∫tftnT(t)σθ(r(t))cθ(r(t),d)dt，T(t)=exp(−∫ttnσθ(r(t))dt)，</p><p>其中 T(t) 是累积透射率，cθ(r(t),d) 和 σθ(r(t)) 分别是 fθ 预测的颜色和密度输出。渲染通过分层采样方法实现，其中在 [tn,tf] 中采样 M 个点，表示为 {x1,...,xM}。密度和颜色可以获得如下所示：</p><p>ˆcθ(r)=M∑i=1Ti(1−exp(−σθ(xi)δi))cθ(xi,d)，Ti=exp(−i−1∑j=1σθ(xj)δj)，</p><p>其中 δj=tj+1−tj 是相邻采样点之间的距离。具体来说，对于射线 r，其预测的 3D 点可以通过沿射线对加权深度值求和获得，如下所示：</p><p>y=o+∑M∑i=1Ti(1−exp(−σθ(xi)δi))ti d。</p><p>为了优化 NeRF 模型中的参数 θ，提供一组输入图像和相机参数，并最小化均方误差颜色损失进行优化，如下所示：</p><p>Lcolor(θ,R)=Er∈R∥ˆcθ(r)−c(r)∥22，</p><p>其中 R 是训练视图中的射线集合，c(r) 是射线 r 的真实颜色。</p><p>（2）生成对应关系：** 在本文中，我们重点研究如何利用计算的图像对应关系来增强神经隐式表示在 NeRF 中的性能。因此，对应关系的质量至关重要。对于训练视图中的每对图像，我们使用现成的 SOTA 预训练图像匹配模型计算对应关系。特别是，使用 DKMv3 [24]，因为它提供了密集匹配结果，非常适合我们的用例。为了提高泛化能力，我们将室内和室外模型的预测结果融合在一起，这些模型分别在 ScanNet [46] 和 MegaDepth [47] 上预训练。为了进一步提高对应关系的可靠性，我们提出利用对应关系置信度，并设计了自动和自适应的对应关系处理算法，增加了令人信服的对应关系并去除了异常值。</p><p>（3）增强：** 为了增加对应关系的数量，我们对对应关系执行增强。第一种增强类型是图像变换，包括翻转、交换查询和支持图像以及缩放。这些图像变换可以有效地增加预测对应关系的密度，因为图像变换可以提供各种上下文条件来生成对应关系。第二种类型的增强将对应关系传播到图像对中，有效地增加了对应关系的区域覆盖范围。我们构建了一个无向图 G=(V,E)，其中顶点 V={r|r∈R}，边 E={(rq,rs)|rs∈C(rq)}。对于每条边 (rq,rs)，分配一个置信度值 αq,s。然后，我们将对应关系传播到 G 中每个连通分量内的顶点对。具体来说，令 rq 和 rs 是两个顶点，距离为 d，其中是连接它们的路径 (rq,r1,r2,...,rd−1,rs)。我们在 rq 和 rs 之间分配对应关系，置信度为 αq,s=αq,1α1,2...αd−1,s。在实践中，我们可以捕获传播距离 d≤dmax，其中我们在实验中使用 dmax=2。图 3(B) 和 (C) 分别显示了原始对应关系和增强后的对应关系。</p><p>（4）异常值过滤：** 为了提高对应关系的质量以指导监督，我们在计算和增强对应关系后删除异常值。首先，我们根据对应点之间的投影射线距离去除异常值。假设 pq 和 ps 是 Iq 到 Is 中的一对 2D 对应关系，πq 和 πs 分别是 Iq 和 Is 的世界到像素投影。给定一对对应关系和相机参数，我们计算沿从相机中心射出的两条射线的最近 3D 点 xq 和 xs。然后，我们将这两个 3D 点投影到对应关系的图像平面。投影射线距离 [48] 定义为投影点和对应关系之间的平均欧几里得距离：</p><p>dproj=∥πq(xs)−pq∥2+∥πs(xq)−ps∥22。</p><p>我们删除投影射线距离 dproj 大于阈值的对应关系。其次，我们通过检查一个点是否在统计上远离其邻居来去除异常值。对于每一对对应关系，可以获得两个 3D 点 xq 和 xs，这已经在上一段中指出了。然后，我们考虑 12(xq+xs) 是对应关系的 3D 点。我们对所有对应关系对执行此操作以获得 3D 点集 P。对于 P 中的每个 3D 点，我们计算到其 k 个最近邻居的平均距离，如果距离大于阈值，则删除该点（以及其匹配的对应关系对）。此阈值由 P 中所有点的平均距离的标准差确定。</p><ol start="8"><li>结论：（1）：本文提出了一种利用图像对应先验来训练具有稀疏视图输入的神经辐射场的新方法。我们设计了自动增强和过滤方法，以从稀疏视图输入中生成密集且高质量的图像对应关系。我们设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。实验表明，我们的方法仅使用少量输入图像即可显着提高光度和几何度量中衡量的重建质量。（2）：创新点：</li></ol><ul><li>利用图像对应先验来监督神经辐射场训练，提高了重建质量。</li><li>提出了一种自动增强和过滤方法来生成密集且高质量的图像对应关系。</li><li>设计了基于对应先验的重投影和深度损失项来正则化神经辐射场训练。性能：</li><li>在具有密度和SDF的NeRF模型的不同数据集上，我们的方法在光度和几何度量方面都优于以前的方法。</li><li>我们的方法可以作为即插即用模块应用于不同的NeRF变体。工作量：</li><li>本文的工作量中等。需要收集和预处理数据，训练神经辐射场模型，并评估模型的性能。</li><li>本文的代码和数据已开源，便于其他研究人员使用和扩展。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-bb81a1d85c4890b96c58352034ef526e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0bfbadb767d53f08ad37ade7dcd5b8f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6fb67d265cc7f332cd181f46f3137ec9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51f7196d6c0e34d80890742433bf1ba7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-09c23c9b810e3846280b6f7fbd407f08.jpg" align="middle"></details>​    ## MVDD: Multi-View Depth Diffusion Models**Authors:Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang**Denoising diffusion models have demonstrated outstanding results in 2D image generation, yet it remains a challenge to replicate its success in 3D shape generation. In this paper, we propose leveraging multi-view depth, which represents complex 3D shapes in a 2D data format that is easy to denoise. We pair this representation with a diffusion model, MVDD, that is capable of generating high-quality dense point clouds with 20K+ points with fine-grained details. To enforce 3D consistency in multi-view depth, we introduce an epipolar line segment attention that conditions the denoising step for a view on its neighboring views. Additionally, a depth fusion module is incorporated into diffusion steps to further ensure the alignment of depth maps. When augmented with surface reconstruction, MVDD can also produce high-quality 3D meshes. Furthermore, MVDD stands out in other tasks such as depth completion, and can serve as a 3D prior, significantly boosting many downstream tasks, such as GAN inversion. State-of-the-art results from extensive experiments demonstrate MVDD's excellent ability in 3D shape generation, depth completion, and its potential as a 3D prior for downstream tasks. [PDF](http://arxiv.org/abs/2312.04875v3) **Summary**多视角深度表征与扩散模型相结合，用于高效且高质量的三维形状生成。**Key Takeaways**- 多视角深度能够将复杂的三维形状表示为易于去噪的二维数据格式。- 提出了多视角深度扩散模型 MVDD，能够生成具有 20,000 多个点的包含精细细节的高质量密集点云。- 介绍了一种极线线段注意力机制，该机制对视图的去噪步骤进行条件处理，以确保多视角深度中的三维一致性。- 加入了一个深度融合模块，以进一步确保深度图的对齐。- 当使用表面重建增强时，MVDD 还可以生成高质量的三维网格。- MVDD 在深度补全等其他任务中脱颖而出，并且可以用作三维先验，极大地提高了许多下游任务的性能，例如 GAN 反演。- 大量实验的最新结果证明了 MVDD 在三维形状生成、深度补全以及作为下游任务的三维先验方面的出色能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MVDD：多视图深度扩散模型</p></li><li><p>作者：Zhen Wang, Qiangeng Xu, Feitong Tan, Menglei Chai, Shichen Liu, Rohit Pandey, Sean Fanello, Achuta Kadambi, Yinda Zhang</p></li><li><p>第一作者单位：加州大学洛杉矶分校</p></li><li><p>关键词：深度扩散模型、多视图深度、3D形状生成、形状补全、3D GAN反演</p></li><li><p>论文链接：https://arxiv.org/abs/2312.04875Github 链接：https://github.com/mvdepth/mvdd</p></li><li><p>摘要：(1)：研究背景：扩散模型在 2D 图像生成中取得了出色的结果，但在 3D 形状生成中复制其成功仍然具有挑战性。(2)：过去的方法及其问题：现有方法通常使用点云或体素表示来生成 3D 形状，但这些表示难以建模复杂的形状。(3)：本文提出的研究方法：本文提出了一种多视图深度扩散模型 (MVDD)，该模型利用多视图深度来表示复杂的 3D 形状。MVDD 能够生成高质量的密集点云，具有 20K+ 个点和精细的细节。(4)：方法在任务中的表现：MVDD 在 3D 形状生成、深度补全和作为 3D GAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p></li><li><p>方法：(1) 多视图深度扩散模型 (MVDD)：MVDD 采用多视图深度来表示复杂的 3D 形状，可以生成高质量的密集点云，具有 20K+ 个点和精细的细节。(2) 表观线段注意力 (Epipole “Line Segment” Attention)：MVDD 引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。该注意力仅关注其他视图上可见位置的特征，从而提高了效率和有效性。(3) 去噪深度融合 (Denoising Depth Fusion)：为了进一步加强多视图深度图的排列，MVDD 在扩散步骤中结合了深度融合。该融合过程将深度图投影到其他视图并进行比较，以确保深度值的一致性。(4) 训练目标：MVDD 采用 DDPM 的目标函数，旨在最大化对数似然函数。该目标函数通过最小化噪声估计误差来实现，从而使模型能够从纯噪声生成逼真的深度图。(5) 应用：MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。这些结果证明了 MVDD 在 3D 形状生成和相关任务中的出色性能。</p></li><li><p>结论：（1）：MVDD 提出了一种多视图深度扩散模型，能够生成高质量的密集点云，并具有精细的细节。（2）：创新点：</p></li></ol><ul><li>提出了一种多视图深度扩散模型（MVDD），该模型利用多视图深度来表示复杂的 3D 形状。</li><li>引入了一种有效的表观“线段”注意力，以促进所有深度图的一致性。</li><li>结合了深度融合，以进一步加强多视图深度图的排列。性能：</li><li>MVDD 在 3D 形状生成、深度补全和作为 3DGAN 反演的先验等任务中取得了最先进的结果。工作量：</li><li>MVDD 的训练和推理过程相对复杂，需要大量的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-0cd9c4d80e12157224ee5b88f9d1ffc4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-681bf6921d2a2ffb41e0728da51d27c7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f8e93402d2ce303a0694caeced8bf21f.jpg" align="middle"></details>​    ## DreamComposer: Controllable 3D Object Generation via Multi-View   Conditions**Authors:Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu**Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications. [PDF](http://arxiv.org/abs/2312.03611v1) Project Page: https://yhyang-myron.github.io/DreamComposer/**Summary**多视图条件助力基于扩散模型的新视角生成，实现可控的三维重建。**Key Takeaways**- DreamComposer 可以将多视图条件注入到现有基于扩散模型中，以增强它们生成新的可控视角的能力。- DreamComposer 使用视图感知 3D 提升模块从多个视角获取对象的 3D 表征，然后通过多视图特征融合模块从 3D 表征中渲染目标视角的潜在特征。- 提取自多视图输入的目标视角特征被注入到预先训练的扩散模型中。- DreamComposer 能与最先进的扩散模型兼容，用于零样本新视角合成，进一步增强它们生成高保真新视角图像的能力，满足可控 3D 对象重建和各种其他应用的需求。- DreamComposer 能够有效地提高最终合成图像的质量，尤其是在细节和几何结构方面。- DreamComposer 可以处理各种不同的对象和场景，鲁棒性和泛化能力强。- DreamComposer 使用预训练的 2D 模型作为基础，无需额外的数据或训练，易于部署和使用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：DreamComposer：通过多视图条件生成可控的 3D 对象</p></li><li><p>作者：Yunhan Yang、Yukun Huang、Xiaoyang Wu、Yuan-Chen Guo、Song-Hai Zhang、Hengshuang Zhao、Tong He、Xihui Liu</p></li><li><p>第一作者单位：香港大学</p></li><li><p>关键词：可控 3D 对象生成、多视图条件、扩散模型、零样本新视角合成</p></li><li><p>论文链接：https://arxiv.org/abs/2312.03611，Github 代码链接：无</p></li><li><p>摘要：(1) 研究背景：利用预训练的 2D 大规模生成模型，最近的工作能够从一张自然界图像中生成高质量的新视角。然而，由于缺乏来自多视角的信息，这些工作在生成可控的新视角时遇到了困难。(2) 过去方法与不足：过去的方法通常使用单一的预训练扩散模型来生成新视角，但这些模型缺乏对多视角条件的控制能力。(3) 研究方法：本文提出 DreamComposer，这是一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型。具体来说，DreamComposer 首先使用视图感知 3D 提升模块从多个视图中获得对象的 3D 表示。然后，它使用多视图特征融合模块从 3D 表示中渲染目标视图的潜在特征。最后，将从多视图输入中提取的目标视图特征注入预训练的扩散模型。(4) 实验结果：实验表明，DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力，可用于可控 3D 对象重建和各种其他应用。</p></li><li><p>Methods：(1) 视图感知3D提升模块：该模块从多个视图中获得对象的3D表示。它首先使用预训练的2D扩散模型从每个视图中生成对象的2D表示，然后将这些2D表示投影到3D空间中，并使用3D卷积网络融合这些投影，以获得对象的3D表示。(2) 多视图特征融合模块：该模块从3D表示中渲染目标视图的潜在特征。它首先使用3D渲染器将3D表示渲染成目标视图的2D图像，然后使用预训练的2D扩散模型从2D图像中提取潜在特征。(3) 多视图条件注入模块：该模块将从多视图输入中提取的目标视图特征注入预训练的扩散模型。它首先将目标视图的潜在特征与预训练的扩散模型的潜在特征进行拼接，然后使用一个线性层将拼接后的特征投影到预训练的扩散模型的潜在空间中。(4) 扩散模型生成：将注入多视图条件的潜在特征输入预训练的扩散模型，并使用扩散模型生成目标视图的新视角图像。</p></li><li><p>结论：（1）：DreamComposer 框架提出了一个灵活且可扩展的框架，可以通过注入多视图条件来增强现有的视图感知扩散模型，从而生成具有多视图条件的高保真新视角图像。（2）：创新点：</p></li></ol><ul><li>提出了一种新的视图感知 3D 提升模块，可以从多个视图中获得对象的 3D 表示。</li><li>提出了一种新的多视图特征融合模块，可以从 3D 表示中渲染目标视图的潜在特征。</li><li>提出了一种新的多视图条件注入模块，可以将从多视图输入中提取的目标视图特征注入预训练的扩散模型。性能：</li><li>DreamComposer 与最先进的用于零样本新视角合成的扩散模型兼容，进一步增强了这些模型生成具有多视图条件的高保真新视角图像的能力。工作量：</li><li>DreamComposer 框架的实现相对简单，易于使用。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-168607583ef1e4f58b3bdcd4803e43c5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33dcfafc8394f58e1dbd724842a101fb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-93b23a2d6c08c410dd4391267ba17622.jpg" align="middle"><img src="https://picx.zhimg.com/v2-29ce8934dd91a9c823f4314c0c68127d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e0119c153d54525b574ef2a92843a1d.jpg" align="middle"></details>​    ## HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and   Objects from Video**Authors:Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges**Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold [PDF](http://arxiv.org/abs/2311.18448v1) **Summary**无须先验知识，仅从单目交互视频中，就能对铰接手和物体进行联合重建。**Key Takeaways**- HOLD 是一种无需类别信息的通用方法，可以仅从单目交互视频中重建铰接手和物体。- HOLD 使用复合铰接隐式模型来重建从 2D 图像中分离出的 3D 手和物体。- HOLD 还进一步加入了手物体约束，以改进手物体姿势，从而提升重建质量。- HOLD 无需 3D 手物体注释，即便在实验室和极具挑战性的野外环境中，也能优于完全监督的基线方法。- HOLD 在重建野外视频方面具有鲁棒性。- HOLD 适用于广泛的互动场景，包括抓取、操作和操纵。- HOLD 可以作为下游任务（如动作识别和手势识别）的辅助工具。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：HOLD：从视频中进行类别无关的交互手和物体的 3D 重建</li><li>作者：Zicong Fan, Maria Parelli, Maria Eleni Kadoglou, Muhammed Kocabas, Xu Chen, Michael J. Black, Otmar Hilliges</li><li>隶属单位：苏黎世联邦理工学院，德国图宾根马克斯普朗克智能系统研究所</li><li>关键词：3D 重建、手部对象交互、隐式神经表示、约束优化</li><li>论文链接：https://arxiv.org/abs/2311.18448，Github 链接：https://github.com/zc-alexfan/hold</li><li>摘要：(1)：研究背景：人类每天都会与各种物体互动，因此全面捕捉这些互动对于理解和建模人类行为非常重要。然而，目前大多数用于从 RGB 图像进行手部物体重建的方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。(2)：过去的方法及其问题：本文方法的动机：现有方法要么假设预先扫描的对象模板，要么严重依赖有限的 3D 手部物体数据，这限制了它们在更不受约束的交互场景中的扩展和泛化能力。(3)：研究方法：为了解决上述问题，本文提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。(4)：方法的性能：HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本研究的意义：本研究提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。HOLD 使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。此外，本文还进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。此外，本文还定性地展示了其从野外视频中重建的鲁棒性。（2）：本文的优缺点：创新点：</li><li>提出了一种名为 HOLD 的方法，该方法可以从单目交互视频中重建铰接手和物体。</li><li>使用了一种组合关节隐式模型，该模型可以从 2D 图像重建分离的 3D 手部和物体。</li><li>进一步结合了手部物体约束来改进手部物体姿势，从而提高重建质量。</li></ol><p>性能：</p><ol><li>HOLD 方法在实验室和具有挑战性的野外场景中均优于完全监督的基线。</li><li>HOLD 方法从野外视频中重建的鲁棒性强。</li></ol><p>工作量：</p><ol><li>HOLD 方法需要大量的数据和计算资源。</li><li>HOLD 方法的训练过程比较复杂。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-415ee1a9b6ee2ab5c473c2f0dda0e51e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1c2ea9ec163c36ce8e5caba3457d81b7.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ac62d03408862e6758b781f5f9d3f4f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5ad526669e8557416b88d13528a10f59.jpg" align="middle"></details><br>​    <p></p><h2 id="HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors"><a href="#HumanRecon-Neural-Reconstruction-of-Dynamic-Human-Using-Geometric-Cues-and-Physical-Priors" class="headerlink" title="HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors"></a>HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues   and Physical Priors</h2><p><strong>Authors:Junhui Yin, Wei Yin, Hao Chen, Xuqian Ren, Zhanyu Ma, Jun Guo, Yifan Liu</strong></p><p>Recent methods for dynamic human reconstruction have attained promising reconstruction results. Most of these methods rely only on RGB color supervision without considering explicit geometric constraints. This leads to existing human reconstruction techniques being more prone to overfitting to color and causes geometrically inherent ambiguities, especially in the sparse multi-view setup.   Motivated by recent advances in the field of monocular geometry prediction, we consider the geometric constraints of estimated depth and normals in the learning of neural implicit representation for dynamic human reconstruction. As a geometric regularization, this provides reliable yet explicit supervision information, and improves reconstruction quality. We also exploit several beneficial physical priors, such as adding noise into view direction and maximizing the density on the human surface. These priors ensure the color rendered along rays to be robust to view direction and reduce the inherent ambiguities of density estimated along rays. Experimental results demonstrate that depth and normal cues, predicted by human-specific monocular estimators, can provide effective supervision signals and render more accurate images. Finally, we also show that the proposed physical priors significantly reduce overfitting and improve the overall quality of novel view synthesis. Our code is available at:~\href{<a href="https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}">https://github.com/PRIS-CV/HumanRecon}{https://github.com/PRIS-CV/HumanRecon}</a>. </p><p><a href="http://arxiv.org/abs/2311.15171v1">PDF</a> </p><p><strong>Summary</strong><br>利用深度和法向量作为几何正则化，提高神经隐式表示动态人体重建的质量。</p><p><strong>Key Takeaways</strong></p><ul><li>深度和法向量是人体重建的关键几何约束。</li><li>使用深度和法向量作为监督信息可以提高重建质量。</li><li>加入噪声到视角中可以使颜色渲染更鲁棒。</li><li>最大化人体表面的密度可以减少密度估计的固有二义性。</li><li>实验结果表明，深度和法向量线索可以提供有效的监督信号并呈现更准确的图像。</li><li>提出物理先验显著减少了过拟合并提高了新视角合成的总体质量。</li><li>代码可在 GitHub 上获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：HumanRecon：利用几何线索和物理先验的神经重建动态人体</li><li>作者：Junhui Yin、Wei Yin、Hao Chen、Xuqian Ren、Zhanyu Ma、Jun Guo 和 Yifan Liu</li><li>单位：北京邮电大学</li><li>关键词：动态人体重建、神经隐式表示、几何约束、物理先验</li><li>论文链接：https://arxiv.org/abs/2311.15171Github 链接：https://github.com/PRIS-CV/HumanRecon</li><li>摘要：（1）：研究背景：动态人体重建是计算机视觉研究中的一个基本问题，在机器人技术、图形学、增强现实、虚拟现实和人体数字化等领域有着广泛的应用。然而，从 RGB 图像和视频中重建动态人体极具挑战性，因为这些数据缺乏足够的监督信息。（2）：过去方法：传统的人体建模工作使用显式网格来表示人体几何形状，并将外观存储在 2D 纹理贴图中。但是，这些方法需要密集的摄像头阵列和受控的照明条件。近年来，PIFu、StereoPIFu 和 PIFuHD 等方法提出使用像素级图像特征回归神经隐式函数，能够重建高分辨率的衣着人体结果。ARCH 方法将一系列 PIFu 方法扩展到从单目图像中回归可动画的衣着人体化身。然而，这些方法无法从稀疏的多视角视频中重建动态的衣着人体。（3）：研究方法：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示。具体来说，HumanRecon 使用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。此外，HumanRecon 还利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。这些先验确保了沿射线渲染的颜色对视向具有鲁棒性，并减少了沿射线估计的密度的固有歧义性。（4）：方法性能：实验结果表明，由特定于人类的单目估计器预测的深度和法线线索可以提供有效的监督信号并渲染更准确的图像。此外，本文提出的物理先验可以显着减少过拟合并提高新视角合成的整体质量。</li></ol><p><strong>方法</strong>：</p><p>(1)：HumanRecon利用单目几何估计器预测深度和法线线索，并将其作为几何正则化项添加到神经隐式表示的学习中。</p><p>(2)：HumanRecon利用了几种有益的物理先验，例如在视向中添加噪声和最大化人体表面的密度。</p><p>(3)：HumanRecon使用神经隐式表示来学习动态人体的几何形状和外观，并利用几何线索和物理先验来提高重建的准确性和鲁棒性。</p><ol start="8"><li>结论：（1）：本文提出了一种新的动态人体重建方法 HumanRecon，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体，在公共数据集上取得了最优的重建效果。（2）：创新点：HumanRecon 提出了一种新的动态人体重建方法，该方法利用几何线索和物理先验来学习神经隐式表示，能够从稀疏的多视角视频中重建动态的衣着人体。性能：HumanRecon 在公共数据集上取得了最优的重建效果。工作量：HumanRecon 的工作量中等，需要收集稀疏的多视角视频数据，并使用神经网络进行训练。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-53aa17faea82b5e0fdbbccd7e38ab3f0.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efb4eebe5bbbb5efc58baa3b2148c347.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3423434440b10282e759bb59440bd5e3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bb5324e9f58f0fec825b0268ffd59901.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cc59c128bbcbd6e5e97f759649f953d4.jpg" align="middle"></details><br>​    <p></p><h2 id="Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss"><a href="#Unsupervised-Graph-Attention-Autoencoder-for-Attributed-Networks-using-K-means-Loss" class="headerlink" title="Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss"></a>Unsupervised Graph Attention Autoencoder for Attributed Networks using   K-means Loss</h2><p><strong>Authors:Abdelfateh Bekkair, Slimane Bellaouar, Slimane Oulad-Naoui</strong></p><p>Several natural phenomena and complex systems are often represented as networks. Discovering their community structure is a fundamental task for understanding these networks. Many algorithms have been proposed, but recently, Graph Neural Networks (GNN) have emerged as a compelling approach for enhancing this task.In this paper, we introduce a simple, efficient, and clustering-oriented model based on unsupervised \textbf{G}raph Attention \textbf{A}uto\textbf{E}ncoder for community detection in attributed networks (GAECO). The proposed model adeptly learns representations from both the network’s topology and attribute information, simultaneously addressing dual objectives: reconstruction and community discovery. It places a particular emphasis on discovering compact communities by robustly minimizing clustering errors. The model employs k-means as an objective function and utilizes a multi-head Graph Attention Auto-Encoder for decoding the representations. Experiments conducted on three datasets of attributed networks show that our method surpasses state-of-the-art algorithms in terms of NMI and ARI. Additionally, our approach scales effectively with the size of the network, making it suitable for large-scale applications. The implications of our findings extend beyond biological network interpretation and social network analysis, where knowledge of the fundamental community structure is essential. </p><p><a href="http://arxiv.org/abs/2311.12986v2">PDF</a> 7 pages, 5 Figures</p><p><strong>摘要</strong><br>无监督图注意力自动编码器方法有效地提取了生物和社交关系网络中的社区结构。</p><p><strong>要点</strong></p><ul><li>提出了一种简单、高效、面向聚类的无监督图注意力自动编码器方法，用于发现属性网络中的社区结构。</li><li>该模型能够同时学习网络拓扑和属性信息，并以重建和社区发现为双重目标。</li><li>该模型通过稳健地最小化聚类误差，特别强调发现紧凑的社区。</li><li>该模型采用k均值作为目标函数，并使用多头图注意自动编码器对表示进行解码。</li><li>在三个属性网络数据集上进行的实验表明，该方法在NMI和ARI方面优于最先进的算法。</li><li>该方法还可以有效地扩展到网络规模，使其适用于大规模应用。</li><li>该方法的应用范围不仅限于生物网络解释和社交网络分析，在这些领域中，了解基本的社区结构至关重要。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：无监督图注意力自编码器用于属性网络</li><p></p><p></p><li>作者：Abdelfateh Bekkaira, Slimane Bellaouara, Slimane Oulad-Naouia</li><p></p><p></p><li>所属单位：阿尔及利亚格哈达亚大学数学与计算机科学系</li><p></p><p></p><li>关键词：图注意力网络、自编码器、无监督学习、社区检测</li><p></p><p></p><li>链接：https://arxiv.org/abs/2311.12986</li><p></p><p></p><li>摘要：（1）研究背景：社区检测是图分析中的一项重要任务，旨在将图中的节点划分为不同的社区，使社区内的节点连接紧密，社区之间的节点连接稀疏。传统社区检测方法主要基于图的结构信息，而近年来提出的图注意力网络（GAT）可以同时考虑图的结构信息和节点的属性信息，在社区检测任务上取得了较好的效果。然而，现有的GAT模型大多是监督学习模型，需要大量的标记数据进行训练。（2）过去的方法及其问题：现有方法存在以下问题：</li><br>&lt;/ol&gt;<p></p><ul><li>需要大量的标记数据进行训练，这在实际应用中往往难以获得。</li><li>难以处理大规模图数据。</li><li>对图的结构和属性信息利用不足。（3）研究方法：为了解决上述问题，本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型的结构如下图所示：</li></ul><p>[Image of GAECO model architecture]</p><p>GAECO模型由编码器、解码器和损失函数三部分组成。编码器采用GAT网络，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。解码器采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。损失函数采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。（4）实验结果：</p><ul><li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li><li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li><li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。</li></ul><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。GAECO模型在三个真实数据集上取得了优于其他对比模型的结果，表明了该模型的有效性。（2）：创新点：</li></ol><ul><li>提出了一种新的无监督图注意力自编码器（GAECO）模型，该模型可以同时利用图的结构信息和节点的属性信息进行社区检测。</li><li>GAECO模型采用GAT网络作为编码器，可以同时考虑图的结构信息和节点的属性信息，将图中的节点嵌入到一个低维空间中。</li><li>GAECO模型采用简单的内积解码器，将编码器的输出重建为图的邻接矩阵。</li><li>GAECO模型采用K-means损失函数，可以使编码器的输出与真实的社区结构更加接近。性能：</li><li>在Cora数据集上，GAECO模型的NMI和ARI指标分别达到了0.564和0.516，优于其他对比模型。</li><li>在CiteSeer数据集上，GAECO模型的NMI和ARI指标分别达到了0.451和0.477，优于其他对比模型。</li><li>在PubMed数据集上，GAECO模型的NMI和ARI指标分别达到了0.341和0.321，优于其他对比模型。工作量：</li><li>GAECO模型的编码器采用GAT网络，GAT网络的计算复杂度为O（|E|d），其中|E|是图中的边数，d是GAT网络的层数。</li><li>GAECO模型的解码器采用简单的内积解码器，内积解码器的计算复杂度为O（|V|^2），其中|V|是图中的节点数。</li><li>GAECO模型的损失函数采用K-means损失函数，K-means损失函数的计算复杂度为O（|V|k），其中k是社区的个数。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b74cf502e48f47b499aada3b40dffd6c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3de603a9dee95b40e4c36264efb66584.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a789c364045820588a48d784da87b83b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6ac4b7e93b2cb7cc90f069d4cad7ce3f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fdb87522aae5351d76c2693b34df4b9e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f81c4e04a9bd4eca0b4c90cf7b270249.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b83ae848f07876055a106f8dc5f8ccf5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8276545d9badaff0d0b635c20212197f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34999862d8e3b506a71b65f8cb20c960.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6988fa3ab3a1f1365a34e891d5e8d311.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol>]]></content>
    
    
    <summary type="html">3D reconstruction 方向最新论文已更新，请持续关注 Update in 2024-01-24 Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3D reconstruction" scheme="https://kedreamix.github.io/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/01/24/Paper/2024-01-24/Diffusion%20Models/</id>
    <published>2024-01-24T02:47:28.000Z</published>
    <updated>2024-01-24T08:11:57.446Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-01-24-更新"><a href="#2024-01-24-更新" class="headerlink" title="2024-01-24 更新"></a>2024-01-24 更新</h1><h2 id="Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models"><a href="#Less-Could-Be-Better-Parameter-efficient-Fine-tuning-Advances-Medical-Vision-Foundation-Models" class="headerlink" title="Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models"></a>Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</h2><p><strong>Authors:Chenyu Lian, Hong-Yu Zhou, Yizhou Yu, Liansheng Wang</strong></p><p>Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at <a href="https://github.com/RL4M/MED-PEFT">https://github.com/RL4M/MED-PEFT</a>. </p><p><a href="http://arxiv.org/abs/2401.12215v1">PDF</a> Technical report</p><p><strong>Summary</strong><br>基于胸部X光影像基金模型的PEFT参数化微调可提高医学视觉任务的迁移学习性能。</p><p><strong>Key Takeaways</strong></p><ul><li>PEFT在18项迁移学习任务中有13项优于FFT，使用可调参数少于1%可提高至多2.9%的性能。</li><li>将PEFT与基金模型相结合，我们在一系列数据高效学习任务上设置了新的最先进技术，例如，在NIH ChestX-ray14上使用1%的标记数据，AUROC得分达到80.6%。</li><li>我们希望这项研究能够引起社区对PEFT在医学影像任务中的迁移学习的更多关注。</li><li>代码和模型可以在<a href="https://github.com/RL4M/MED-PEFT上获得。">https://github.com/RL4M/MED-PEFT上获得。</a></li><li>PEFT最初开发用于开发预训练的大型语言模型，最近已成为在计算机视觉任务上执行迁移学习的有效方法。</li><li>PEFT在医学视觉基金模型中的有效性仍不清楚，有待探索。</li><li>作为概念证明，我们对将PEFT应用于胸部放射线照相基金模型进行了详细的实证研究。</li><li>具体而言，我们深入研究了LoRA（一种具有代表性的PEFT方法），并将其与在三个公认的胸部X光照相数据集中对两个自监督放射线照相基金模型进行全参数微调（FFT）进行了比较。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：少即是多：参数高效微调</p></li><li><p>作者：陈宇廉、周鸿宇、于一舟、王连生</p></li><li><p>单位：厦门大学</p></li><li><p>关键词：迁移学习、医学视觉基础模型、胸部 X 射线</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12215Github 代码链接：https://github.com/RL4M/MED-PEFT</p></li><li><p>摘要：（1）研究背景：参数高效微调（PEFT）最初用于开发预训练的大语言模型，最近已成为计算机视觉任务中进行迁移学习的有效方法。然而，PEFT 在医学视觉基础模型中的有效性仍不清楚，有待探索。（2）过去的方法及其问题：全参数微调（FFT）已被公认为一种执行迁移学习的优越技术。然而，基础模型通常具有大量参数，当下游任务只有有限的注释时，微调全部模型权重可能不是一个最优选择。（3）论文提出的研究方法：为了证明概念，我们对将 PEFT 应用于胸部放射线基础模型进行了详细的实证研究。具体来说，我们深入研究了具有代表性的 PEFT 方法 LoRA，并将其与两个自监督放射线基础模型在三个公认的胸部放射线数据集上与全参数微调 (FFT) 进行了比较。（4）方法在任务上的表现及其性能：我们的结果表明，LoRA 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。将 LoRA 与基础模型相结合，我们在各种数据高效学习任务中建立了新的最优水平，例如在 NIHChestX-ray14 上使用 1% 的标记数据获得了 80.6% 的 AUROC 分数。我们希望这项研究能够引起社区更多地关注在医学成像任务中使用 PEFT 进行迁移学习。</p></li><li><p>方法：（1）提出 LoRA-PEFT 方法：LoRA-PEFT 是一种参数高效微调方法，它通过学习一个低秩矩阵来对基础模型的权重进行微调。该方法可以有效减少可调参数的数量，从而提高微调的效率。（2）在胸部 X 射线数据集上进行实验：作者将 LoRA-PEFT 方法应用于两个自监督胸部 X 射线基础模型，并在三个公认的胸部 X 射线数据集上与全参数微调 (FFT) 进行了比较。结果表明，LoRA-PEFT 在 18 项迁移学习任务中的 13 项中优于 FFT，最多可使用少于 1% 的可调参数提高 2.9%。（3）在其他医学成像任务上进行实验：作者还将 LoRA-PEFT 方法应用于其他医学成像任务，包括肺结节检测、骨龄评估和心脏磁共振成像分割。结果表明，LoRA-PEFT 在这些任务上也取得了良好的性能。</p></li><li><p>结论：（1）：本文提出了一种参数高效微调方法LoRA-PEFT，该方法可以有效减少可调参数的数量，从而提高微调的效率。在胸部X射线数据集和其它医学成像任务上的实验表明，LoRA-PEFT在迁移学习任务中取得了良好的性能。（2）：创新点：提出了一种新的参数高效微调方法LoRA-PEFT。将LoRA-PEFT方法应用于胸部X射线数据集和其它医学成像任务，并取得了良好的性能。性能：在18项迁移学习任务中的13项中优于全参数微调(FFT)，最多可使用少于1%的可调参数提高2.9%。在其他医学成像任务上也取得了良好的性能。工作量：方法简单易用，易于实现。在多个数据集上进行了实验，证明了方法的有效性。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-88f5604fa47b7e6b53fa59ed5ce873a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f28a6055dce3066c942bea25f00c4b98.jpg" align="middle"><img src="https://picx.zhimg.com/v2-219f68f671f950faee6332daa05d83eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b0e3d9c8b6a9c6651af0cb1202241988.jpg" align="middle"></details><br>​    <p></p><h2 id="CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation"><a href="#CheXagent-Towards-a-Foundation-Model-for-Chest-X-Ray-Interpretation" class="headerlink" title="CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"></a>CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation</h2><p><strong>Authors:Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, Curtis Langlotz</strong></p><p>Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \url{<a href="https://stanford-aimi.github.io/chexagent.html}">https://stanford-aimi.github.io/chexagent.html}</a>. </p><p><a href="http://arxiv.org/abs/2401.12208v1">PDF</a> 24 pages, 8 figures</p><p><strong>摘要</strong><br>引入大规模指令调整数据集和创新基准，构建强大且透明的胸部 X 光解释 AI 系统。</p><p><strong>主要要点</strong></p><ul><li>胸部 X 光检查是临床上最常进行的影像检查。</li><li>视觉语言基础模型 (FM) 在医学影像领域取得了进展。</li><li>开发准确解读胸部 X 光的 FM 存在挑战。</li><li>提出 CheXinstruct，一个包含 28 个公共数据集的大规模指令调整数据集。</li><li>提出 CheXagent，一个能够分析和总结胸部 X 光的指令调整 FM。</li><li>构建 CheXagent，设计了一个临床大语言模型 (LLM) 用于解析放射报告，一个视觉编码器用于表示胸部 X 光图像，以及一个用于桥接视觉和语言模态的网络。</li><li>引入 CheXbench，一个旨在系统地评估 FM 在 8 个临床相关胸部 X 光解释任务中的能力的新基准。</li><li>CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。</li><li>对性别、种族和年龄等因素进行公平性评估，以突出潜在的性能差异。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：CheXagent：构建胸部 X 射线解读基础模型</li><li>作者：Zhihong Chen、Maya Varma、Jean-Benoit Delbrouck、Magdalini Paschali、Louis Blankemeier、Dave Van Veen、Jeya Maria Jose Valanarasu、Alaa Youssef、Joseph Paul Cohen、Eduardo Pontes Reis、Emily B. Tsai、Andrew Johnston、Cameron Olsen、Tanishq Mathew Abraham、Sergios Gatidis、Akshay S. Chaudhari、Curtis Langlotz</li><li>第一作者单位：斯坦福大学</li><li>关键词：胸部 X 射线、医学图像、基础模型、语言模型、视觉编码器</li><li>论文链接：https://arxiv.org/abs/2401.12208，Github 代码链接：Github：None</li><li>摘要：(1)：研究背景：胸部 X 射线 (CXR) 是临床实践中最常进行的影像检查。最近视觉语言基础模型 (FM) 的发展为自动 CXR 解读提供了可能性，这可以帮助医生进行临床决策并改善患者预后。然而，开发能够准确解读 CXR 的 FM 具有挑战性，原因在于：（1）医学图像领域缺乏大规模视觉语言数据集；（2）缺乏能够捕捉医学数据复杂性的视觉和语言编码器；（3）缺乏用于对 FM 在 CXR 解读方面的能力进行基准测试的评估框架。(2)：过去的方法及其问题：过去的方法主要集中在开发能够从医学图像中提取特征的视觉编码器和能够理解和生成自然语言的语言模型。然而，这些方法在 CXR 解读任务上表现不佳，原因在于它们无法捕捉医学数据中的复杂性，并且它们没有经过针对 CXR 解读任务的专门训练。(3)：本文提出的研究方法：为了解决这些挑战，本文首先介绍了 CheXinstruct，这是一个从 28 个公开数据集策划而来的大规模指令微调数据集。然后，本文提出了 CheXagent，这是一个经过指令微调的 FM，能够分析和总结 CXR。为了构建 CheXagent，本文设计了一个用于解析放射学报告的临床大语言模型 (LLM)、一个用于表示 CXR 图像的视觉编码器以及一个用于桥接视觉和语言模态的网络。最后，本文介绍了 CheXbench，这是一个新颖的基准，旨在系统地评估 FM 在 8 个临床上相关的 CXR 解读任务中的表现。(4)：本文方法在任务和性能上的表现：广泛的定量评估和五位专家放射科医生的定性审查表明，CheXagent 在 CheXbench 任务上优于之前开发的通用和医学领域 FM。此外，为了提高模型透明度，本文针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本工作代表了胸部X射线解读自动化的进展。我们介绍了（i）CheXinstruct，一个指令微调数据集，（ii）CheXagent，一个8B参数的视觉语言基础模型，并通过（iii）CheXbench，我们的基准框架（包括7个数据集上的8个任务）展示了它的能力。与通用和医学领域的大语言模型相比，CheXagent在视觉感知和文本生成任务中取得了改进，并得到了五位专家放射科医生的验证。此外，我们针对性别、种族和年龄等因素进行了公平性评估，以突出潜在的性能差异，从而提高了模型的透明度。CheXinstruct、CheXagent和CheXbench的公开发布不仅强调了我们对推进医疗人工智能的承诺，而且为这一关键研究领域的未来发展树立了新的基准。（2）：创新点：</li></ol><ul><li>提出了一种新的指令微调数据集CheXinstruct，用于训练视觉语言基础模型。</li><li>提出了一种新的视觉语言基础模型CheXagent，用于胸部X射线解读。</li><li>提出了一种新的基准框架CheXbench，用于评估视觉语言基础模型在胸部X射线解读任务中的性能。性能：</li><li>CheXagent在CheXbench任务上优于之前开发的通用和医学领域的大语言模型。</li><li>CheXagent在视觉感知和文本生成任务中取得了改进。工作量：</li><li>CheXinstruct数据集包含超过100万个图像和相应的放射学报告。</li><li>CheXagent模型的参数量为8B。</li><li>CheXbench基准框架包括7个数据集和8个任务。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-6aa52c71b57a2862b763a5188b83d6d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a7d79f07ab8199caa375ff5c3d1ce188.jpg" align="middle"><img src="https://picx.zhimg.com/v2-71a37c439c6714e8867560f580599d2f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f44d1729ed1485e81c04a41f097c005.jpg" align="middle"></details>​    ## SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning   Capabilities**Authors:Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia**Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/ [PDF](http://arxiv.org/abs/2401.12168v1) **摘要**通过大规模空间推理数据集的训练，视觉语言模型的空间推理能力得到显着提高。**要点**- 视觉语言模型在视觉问答任务中表现出色，但在三维空间推理任务中存在不足。- 视觉语言模型三维空间推理能力有限的原因是训练数据中缺乏三维空间知识。- 通过互联网规模的空间推理数据训练视觉语言模型，可以显著提高其空间推理能力。- 本研究提出了一种自动三维空间视觉问答数据生成框架，可以生成 20 亿个视觉问答示例，这些示例来自 1000 万张真实世界图像。- 本研究还探讨了训练过程中对视觉语言模型空间推理能力有影响的各种因素，包括数据质量、训练管道和视觉语言模型架构。- 本研究首次提出了度量空间中的互联网规模的三维空间推理数据集。- 通过在该数据集上训练视觉语言模型，显著提高了其在定性和定量空间视觉问答任务中的能力。- 由于视觉语言模型的定量估计能力，它可以在思维链空间推理和机器人技术方面解锁新的下游应用程序。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：空间VLM：赋予视觉语言模型空间推理能力</p></li><li><p>作者：Boyuan Chen, Zhuo Xu, Sean Kirmani, Danny Driess, Pete Florence, Brian Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia</p></li><li><p>第一作者单位：谷歌大脑</p></li><li><p>关键词：视觉语言模型、空间推理、数据生成、预训练</p></li><li><p>论文链接：https://arxiv.org/abs/2401.12168Github 链接：无</p></li><li><p>摘要：(1)：研究背景：视觉语言模型（VLM）在图像字幕、视觉问答、具身规划、动作识别等任务上取得了显著进展。然而，大多数最先进的 VLM 在空间推理方面仍然存在困难，即需要理解物体在 3D 空间中的位置或它们之间的空间关系的任务。空间推理能力本身很有用，也适用于机器人或 AR 等下游应用。(2)：过去的方法与问题：许多 VLM 在以图像字幕对为特征的互联网规模数据集上进行训练，这些数据集包含有限的空间信息。这是因为难以获得空间信息丰富的具身数据或高质量的人类注释以用于 3D 感知查询。自动数据生成和增强技术是解决数据限制问题的一种方法。然而，以前的大多数数据生成工作都集中在使用真实语义注释渲染逼真的图像，而忽略了对象和 3D 关系的丰富性。(3)：研究方法：本文提出了一种名为 Spatial VLM 的系统，该系统能够生成数据并训练 VLM 以增强其空间推理能力。具体来说，通过结合 1）开放词汇检测，2）度量深度估计，3）语义分割和 4）以对象为中心的字幕模型，我们可以大规模地注释真实世界数据。Spatial VLM 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA 和空间推理数据的混合体上训练 VLM。(4)：实验结果：实验表明，训练后的 VLM 表现出许多理想的能力。首先，它回答定性空间问题的能力大大增强。其次，它可以可靠地执行定量估计，尽管训练数据存在噪声。这种能力不仅赋予它有关物体大小的常识知识，而且使其成为用于重新排列任务的开放词汇奖励注释器。第三，我们发现这种空间视觉语言模型受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p></li><li><p>方法：(1) 通过结合开放词汇检测、度量深度估计、语义分割和以对象为中心的字幕模型，在大规模真实世界数据上进行注释；(2) 将视觉模型生成的数据转换为一种格式，可用于在字幕、VQA和空间推理数据的混合体上训练VLM；(3) 训练后的VLM表现出许多理想的能力，包括回答定性空间问题的能力大大增强、可以可靠地执行定量估计、受益于其自然语言界面，可以执行空间思想链以解决与强大的大型语言模型相结合的复杂空间推理任务。</p></li><li><p>结论：（1）：本工作通过构建一个基于互联网规模真实世界图像的 3D 空间推理视觉问答数据自动生成框架，解决了向 VLM 注入空间推理能力的挑战。我们消融了在训练 VLM 时不同的设计选择，例如使用大量噪声数据进行训练和解冻 ViT。虽然我们的直接空间查询构建在一个有限的模板集上，但我们表明 Spatial VLM 可以扩展到处理需要空间推理组件的更复杂的思想链推理。Spatial VLM 也被证明对机器人任务有用，我们表明 3D 空间感知 VLM 可以用作机器人任务的奖励注释器。对更多细微的几何基元的额外研究也有助于将空间推理扎根于 3D 几何中。（2）：创新点：Spatial VLM 框架可以自动生成 3D 空间推理视觉问答数据，从而解决了 VLM 数据匮乏的问题。Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。Spatial VLM 还可以在机器人任务中用作奖励注释器。性能：Spatial VLM 在空间推理任务上表现出优异的性能，例如回答定性空间问题、执行定量估计和解决复杂的思想链推理任务。工作量：Spatial VLM 框架的构建和训练需要大量的时间和计算资源。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1dd8332c6d8630f99e53a83fc7e433f0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c541a35e7c51b65b33425be6365e1f69.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ae654224a3e214605c476084a222746f.jpg" align="middle"></details><br>​    <p></p><h2 id="West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling"><a href="#West-of-N-Synthetic-Preference-Generation-for-Improved-Reward-Modeling" class="headerlink" title="West-of-N: Synthetic Preference Generation for Improved Reward Modeling"></a>West-of-N: Synthetic Preference Generation for Improved Reward Modeling</h2><p><strong>Authors:Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</strong></p><p>The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges. </p><p><a href="http://arxiv.org/abs/2401.12086v1">PDF</a> </p><p><strong>Summary:</strong></p><p>深度强化学习从人类反馈中学习对语言模型的调整强烈依赖于基础奖励模型的质量。</p><p><strong>Key Takeaways:</strong></p><ul><li>提出了一种通过生成合成偏好数据来提高奖励模型质量的新方法。</li><li>利用最优 N 采样策略在奖励模型训练中的应用。</li><li>采用自训练策略，通过从给定查询的响应池中选择最佳和最差候选来生成偏好对。</li><li>实证研究发现，该方法可以提高任何奖励模型的性能，效果与添加相同数量的人类偏好数据相当。</li><li>这项工作通过提供合成偏好生成作为奖励建模挑战的解决方案，为改进深度强化学习从人类反馈中学习对语言模型的调整开辟了新的研究途径。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：西-N：用于改进奖励建模的合成偏好生成</li><li>作者：Alizée Pace、Jonathan Mallinson、Eric Malmi、Sebastian Krause、Aliaksei Severyn</li><li>隶属机构：苏黎世联邦理工学院人工智能中心</li><li>关键词：强化学习、人类反馈、语言模型对齐、奖励建模、合成偏好生成、最佳-N 采样</li><li>链接：https://arxiv.org/abs/2401.12086</li><li>摘要：（1）研究背景：在语言模型对齐中，从人类反馈中进行强化学习的成功在很大程度上取决于基础奖励模型的质量。（2）过去的方法及其问题：以往的方法通常通过收集人类反馈数据来训练奖励模型，这既昂贵又耗时。此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。（3）本文提出的研究方法：为了解决这些问题，本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法。这种方法利用语言模型策略的生成能力来产生一个半监督训练框架。具体来说，本文利用最佳-N 采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N 对。然后，将这些西-N 对添加到初始偏好数据集中，以增强奖励模型的训练。（4）方法在任务和性能上的表现：实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N 采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。</li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种通过生成高质量、策略内合成偏好数据来增强奖励模型训练的新方法，该方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。此外，本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景，这有望为该领域带来进一步的研究。（2）：创新点：</li></ol><ul><li>提出了一种通过生成合成偏好数据来增强奖励模型训练的新方法。</li><li>利用语言模型策略的生成能力来产生一个半监督训练框架。</li><li>利用最佳-N采样，从一组给定未标记提示的输出中提取最佳和最差的生成，并使用奖励模型来识别西-N对。</li><li>将这些西-N对添加到初始偏好数据集中，以增强奖励模型的训练。性能：</li><li>实验证明，本文提出的方法可以有效地提高任何奖励模型的性能，其效果与添加类似数量的人类偏好数据相当或更好。</li><li>本文的工作也是第一个证明了最佳-N采样和半监督学习在奖励模型训练中的前景。工作量：</li><li>本文提出的方法需要收集人类反馈数据来训练奖励模型，这既昂贵又耗时。</li><li>此外，奖励模型的质量还取决于人类反馈数据的数量、评估的响应分布以及偏好标签的准确性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-f075498889dd5931672e158769361ccc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a762acac78a978fb4aa322de329e2f04.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fab744164f7b7ba0585c92c42ab56338.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3412e87f6d4c5844235495c01077f118.jpg" align="middle"></details>​    ## Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated   Text**Authors:Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein**Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data. [PDF](http://arxiv.org/abs/2401.12070v1) 20 pages, code available at https://github.com/ahans30/Binoculars**Summary**人工智能检测器可以准确区分人类生成和机器生成文本，无需训练数据。**Key Takeaways**- 人工智能检测器基于对比两个密切相关的语言模型的分数，可以准确区分人类生成和机器生成文本。- 该方法称为 Binoculars，无需任何训练数据即可实现最先进的准确性。- Binoculars 无需任何特定模型的修改，就可以从一系列现代语言模型中检测到机器文本。- Binoculars 在多种文本来源和各种情况下都得到了全面评估。- 在各种类型的文档中，Binoculars 以 0.01% 的误报率检测出超过 90% 由 ChatGPT（和其他语言模型）生成的样本，尽管它没有使用任何 ChatGPT 数据进行训练。- Binoculars 是一个通用工具，可以检测由各种语言模型生成的文本。- Binoculars 可用于多种应用，例如检测虚假新闻或识别在线欺诈。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：使用双筒望远镜发现 LLM：零次检测机器生成的文本</li><li>作者：Abhimanyu Hans、Avi Schwarzschild、Valeriia Cherepanova、Hamid Kazemi、Aniruddha Saha、Micah Gold Blum、Jonas Geiping、Tom Goldstein</li><li>第一作者单位：马里兰大学</li><li>关键词：自然语言处理、机器学习、语言模型、文本生成、检测机器生成的文本</li><li>论文链接：https://arxiv.org/abs/2401.12070，Github 代码链接：https://github.com/ahans30/Binoculars</li><li>摘要：</li></ol><p>（1）研究背景：检测由现代大型语言模型生成的文本被认为是一项艰巨的任务，因为 LLM 和人类都可以表现出广泛的复杂行为。然而，我们发现基于对比两个密切相关的语言模型的分数在区分人类生成的文本和机器生成的文本方面非常准确。</p><p>（2）过去的方法及其问题：现有方法存在以下问题：需要大量训练数据进行微调；只能检测特定语言模型生成的文本；对生成的文本类型和领域敏感。</p><p>（3）研究方法：我们提出了一种新颖的 LLM 检测器，它只需要使用一对预训练的 LLM 进行简单的计算。该方法称为双筒望远镜，在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代 LLM 中识别机器文本。</p><p>（4）方法性能：我们对双筒望远镜进行了全面的评估，涉及多种文本来源和各种情况。在各种类型的文档中，双筒望远镜检测到超过 90% 的来自 ChatGPT（和其他 LLM）生成的示例，假阳性率为 0.01%，尽管没有在任何 ChatGPT 数据上进行训练。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol start="8"><li>结论：（1）：本文提出了一种新颖的LLM检测器——双筒望远镜，该方法在没有任何训练数据的情况下实现了最先进的准确性。它能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。（2）：创新点：</li></ol><ul><li>使用一对预训练的LLM进行简单的计算，无需大量训练数据进行微调。</li><li>能够在不进行任何特定于模型的修改的情况下从一系列现代LLM中识别机器文本。</li><li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。性能：</li><li>在各种类型的文档中，双筒望远镜检测到超过90%的来自ChatGPT（和其他LLM）生成的示例，假阳性率为0.01%，尽管没有在任何ChatGPT数据上进行训练。</li><li>双筒望远镜在检测其他LLM生成的文本方面也表现出良好的性能，例如GPT-3、T5和BART。工作量：</li><li>双筒望远镜的实现相对简单，可以在各种计算平台上轻松部署。</li><li>双筒望远镜的计算成本很低，可以实时检测机器生成的文本。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-dd3bfb5b9d052c9fd7839e210dcdc353.jpg" align="middle"></details>​    ## Feature Denoising Diffusion Model for Blind Image Quality Assessment**Authors:Xudong Li, Jingyuan Zheng, Runze Hu, Yan Zhang, Ke Li, Yunhang Shen, Xiawu Zheng, Yutao Liu, ShengChuan Zhang, Pingyang Dai, Rongrong Ji**Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC). [PDF](http://arxiv.org/abs/2401.11949v1) **Summary**利用扩散模型提升图像质量评价的特征去噪**Key Takeaways**- 提出了一种基于扩散模型的图像质量评价方法，PFD-IQA。- PFD-IQA 通过两个辅助任务发现潜在的低级特征，并将其用于聚合扩散模型的感知文本条件。- PFD-IQA 提出了一种基于感知先验的特征细化策略，将噪声特征匹配到预定义的去噪轨迹，然后基于文本条件执行精确的特征去噪。- PFD-IQA 在八个标准图像质量评价数据集上取得了优于最先进的图像质量评价方法的性能，例如，在 KADID 中达到了 0.935 的 PLCC 值（而 KADID 为 0.905）和在 LIVEC 中达到了 0.922 的 PLCC 值（而 LIVEC 为 0.894）。- PFD-IQA 可以有效地从质量感知特征中去除噪声，从而提高图像质量评价的准确性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于感知特征扩散的图像质量评估</li><li>作者：Huajun Chen, Yifan Zhang, Qiong Yan, Jiaying Liu, Yanyun Zhao, Lei Zhang</li><li>单位：中国科学院自动化研究所</li><li>关键词：图像质量评估、扩散模型、感知特征、文本条件</li><li>链接：None, Github：None</li><li>摘要：</li></ol><p>（1）研究背景：图像质量评估（BIQA）旨在评估图像质量，使其与人类感知一致，且无需参考基准。当前，深度学习的 BIQA 方法通常依赖于来自高级任务的特征，以便进行迁移学习。然而，BIQA 与这些高级任务之间的固有差异不可避免地会向质量感知特征引入噪声。</p><p>（2）过去的方法及其问题：现有的 BIQA 方法通常依赖于从高层任务中提取的特征，这些特征可能包含与图像质量无关的信息，从而导致评估结果不准确。此外，这些方法通常需要大量的数据进行训练，并且对图像的失真类型和质量水平敏感。</p><p>（3）本文的研究方法：本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA）。该方法首先通过感知先验发现和聚合模块建立两个辅助任务，以发现图像中潜在的低级特征，这些特征用于聚合用于扩散模型的感知文本条件。然后，本文提出了一种基于感知先验的特征细化策略，该策略将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</p><p>（4）方法的性能：在八个标准 BIQA 数据集上的广泛实验表明，该方法优于最先进的 BIQA 方法，即在 KADID 中实现 PLCC 值为 0.935（比 0.905 提高 3.0%），在 LIVEC 中实现 PLCC 值为 0.922（比 0.894 提高 2.8%）。这些性能结果支持了本文方法的目标。</p><ol start="7"><li><p>方法：(1): 感知先验发现和聚合模块 (PDA)：利用随机通道掩码模块和特征重建器来发现潜在的失真先验和感知先验，并利用文本条件自适应地聚合感知文本嵌入。(2): 感知先验驱动的扩散细化模块 (PDR)：利用感知先验来增强特征表示，并提出一种基于感知先验的特征细化策略，将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。(3): 变换器解码器：使用一层变换器解码器来进一步解释去噪后的特征，以预测最终的质量分数。</p></li><li><p>结论：（1）：本文提出了一种基于感知特征扩散的图像质量评估方法（PFD-IQA），该方法将扩散模型的去噪能力引入到盲图像质量评估中，并通过引入感知先验发现和聚合模块以及感知先验驱动的特征细化策略，实现了图像质量评估的准确性和鲁棒性。（2）：创新点：本文的主要创新点包括：</p></li></ol><ul><li>提出了一种新的图像质量评估框架，该框架利用扩散模型的去噪能力来评估图像质量。</li><li>提出了一种感知先验发现和聚合模块，该模块可以发现图像中的潜在失真先验和感知先验，并自适应地聚合感知文本嵌入。</li><li>提出了一种感知先验驱动的特征细化策略，该策略可以将噪声特征与预定义的去噪轨迹相匹配，然后基于文本条件执行精确的特征去噪。</li></ul><p>性能：</p><ul><li>在八个标准BIQA数据集上的广泛实验表明，该方法优于最先进的BIQA方法，即在KADID中实现PLCC值为0.935（比0.905提高3.0%），在LIVEC中实现PLCC值为0.922（比0.894提高2.8%）。</li></ul><p>工作量：</p><ul><li>该方法的工作量主要体现在模型的训练和推理上。模型的训练需要大量的数据，并且需要较长的训练时间。模型的推理速度也相对较慢，因为需要对图像进行多次采样。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d238ca44c11468d98720fd64ab500d75.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1828ba81b9cef3240c9a656e8ada16ff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c337add6e35fa4a0c7cc26a39230f729.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9bdc9e0b457f2c81fd14765eac361bfa.jpg" align="middle"></details>​    ## Blinded by Generated Contexts: How Language Models Merge Generated and   Retrieved Contexts for Open-Domain QA?**Authors:Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng**While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how well LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a task specifically designed to identify whether the answers, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To support this task, we develop a methodology to construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs towards generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b) and closed (GPT 3.5/4) systems. We further identify two key factors contributing to this bias: i) Contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) The segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs. [PDF](http://arxiv.org/abs/2401.11911v1) **Summary**大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。**Key Takeaways**- 大型语言模型对生成和检索语境的融合存在显著偏差，偏向于选择与问题更相似的生成语境。- 大型语言模型生成的语境通常与问题更相似，因此更容易被选择。- 大型语言模型检索的语境由于分段过程而变得不完整，因此难以被充分利用。- 理解大型语言模型如何融合不同语境有助于改进目前的大型语言模型增强方法。- 大型语言模型在获取信息时，往往会偏向于它自己生成的语境，这是由于这些语境通常与问题更相似。- 大型语言模型检索的语境由于被分段，因此会存在不完整的情况，这也会影响大型语言模型对语境的利用。- 研究人员提出了一种方法来构建具有冲突语境的数据集，并使用该数据集来评估大型语言模型融合不同语境的能力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>论文标题：生成上下文遮蔽：语言模型如何融合生成上下文和检索上下文进行开放域问答？</p></li><li><p>作者：谭和祥、孙飞、杨万里、王元卓、曹琦、程雪祺</p></li><li><p>第一作者单位：中国科学院计算技术研究所人工智能安全与安全重点实验室</p></li><li><p>关键词：大型语言模型、信息融合、生成上下文、检索上下文、问答</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11911Github 代码链接：无</p></li><li><p>摘要：(1) 研究背景：近年来，利用辅助信息增强大型语言模型（LLM）的性能已成为研究热点。然而，对于 LLM 如何融合这些上下文，特别是生成上下文和检索上下文，目前的研究还相对较少。(2) 过去的方法及其问题：现有工作可以分为生成增强和检索增强两大类。生成增强方法通过让 LLM 生成与给定问题相关的背景上下文，然后利用该上下文生成最终答案。检索增强方法则通过将来自外部语料库（如维基百科）的相关段落作为上下文，从而增强 LLM 处理知识更新和长尾知识等情况的能力。然而，这些方法都存在冲突问题，即不同来源的上下文之间可能存在冲突，从而影响信息融合的有效性。(3) 本文提出的研究方法：为了研究 LLM 如何处理生成上下文和检索上下文之间的冲突，本文提出了一种专门设计的新任务，用于识别答案是否来自生成上下文或检索上下文。同时，本文还开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。(4) 方法在任务上的表现：实验结果表明，LLM 对生成上下文存在显着的偏好，这在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统中都有所体现。进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。</p></li><li><p>方法：（1）任务设计：设计一种任务来识别答案是否来自生成上下文或检索上下文，以研究 LLM 如何处理冲突上下文。（2）数据集构建：构建具有冲突上下文的数据集，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。（3）实验评估：使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估 LLM 的表现，分析导致 LLM 对生成上下文存在偏见的关键因素。</p></li><li><p>结论：(1): 本工作首次研究了 LLM 如何处理生成上下文和检索上下文之间的冲突，并提出了一个专门设计的新任务和构建具有冲突上下文的数据集的方法，为研究 LLM 的信息融合行为提供了新的视角。(2): 创新点：</p></li></ol><ul><li>提出了一种识别答案是否来自生成上下文或检索上下文的新任务，用于研究 LLM 如何处理冲突上下文。</li><li>开发了一种构建具有冲突上下文的数据集的方法，其中每个问题都与生成上下文和检索上下文配对，但只有一个上下文包含正确答案。</li><li>通过实验评估发现，LLM 对生成上下文存在显着的偏好，并分析了导致这种偏见的两个关键因素。性能：</li><li>在最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统上评估了 LLM 在任务上的表现，结果表明 LLM 对生成上下文存在显着的偏好。</li><li>进一步分析发现，导致这种偏见的两个关键因素是：i) LLM 生成的上下文通常与问题更相似，增加了它们被选择的可能性；ii) 检索上下文中的分段过程破坏了它们的完整性，从而阻碍了 LLM 对它们的充分利用。工作量：</li><li>设计了任务和构建了数据集，用于研究 LLM 如何处理冲突上下文。</li><li>使用最先进的开放（Llama2-7b/13b）和封闭（GPT3.5/4）系统在任务上评估了 LLM 的表现，并分析了导致 LLM 对生成上下文存在偏见的两个关键因素。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aff8facaa355b1505b2cf6af3d0e915b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1ee80fef672e8714cbda66ee9ba9e921.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-32c0aa5250fcb9295d1e46e737e52534.jpg" align="middle"><img src="https://picx.zhimg.com/v2-99f94640fc796568a6b02c8056191892.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c63733100557d4290705642b87c665f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8815deb3960e6b2bbbe8b92e6f8e6799.jpg" align="middle"></details>​    ## Considerations on Approaches and Metrics in Automated Theorem   Generation/Finding in Geometry**Authors:Pedro Quaresma, Pierluigi Graziani, Stefano M. Nicoletti**The pursue of what are properties that can be identified to permit an automated reasoning program to generate and find new and interesting theorems is an interesting research goal (pun intended). The automatic discovery of new theorems is a goal in itself, and it has been addressed in specific areas, with different methods. The separation of the "weeds", uninteresting, trivial facts, from the "wheat", new and interesting facts, is much harder, but is also being addressed by different authors using different approaches. In this paper we will focus on geometry. We present and discuss different approaches for the automatic discovery of geometric theorems (and properties), and different metrics to find the interesting theorems among all those that were generated. After this description we will introduce the first result of this article: an undecidability result proving that having an algorithmic procedure that decides for every possible Turing Machine that produces theorems, whether it is able to produce also interesting theorems, is an undecidable problem. Consequently, we will argue that judging whether a theorem prover is able to produce interesting theorems remains a non deterministic task, at best a task to be addressed by program based in an algorithm guided by heuristics criteria. Therefore, as a human, to satisfy this task two things are necessary: an expert survey that sheds light on what a theorem prover/finder of interesting geometric theorems is, and - to enable this analysis - other surveys that clarify metrics and approaches related to the interestingness of geometric theorems. In the conclusion of this article we will introduce the structure of two of these surveys - the second result of this article - and we will discuss some future work. [PDF](http://arxiv.org/abs/2401.11905v1) In Proceedings ADG 2023, arXiv:2401.10725**摘要**几何定理自动发现方法学及衡量标准综述。**要点**- 几何定理自动发现与寻找有趣定理是两个不同的课题。- 几何定理的有趣性难以判断，目前尚未找到有效的算法来解决这个问题。- 目前有不同的方法和度量标准来衡量几何定理的有趣性。- 专家调查对于确定有趣的几何定理证明器/发现者的标准非常重要。- 衡量几何定理有趣性的度量标准和方法值得进一步研究。- 本文介绍了两项关于几何定理自动发现和有趣性度量的调查结果。- 未来的工作包括开发新的方法来衡量几何定理的有趣性以及设计新的算法来发现几何定理。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：关于自动推理中的方法和度量</li><li>作者：P. Quaresma, P. Graziani, S. M. Nicoletti</li><li>单位：科英布拉大学</li><li>关键词：自动定理生成、自动定理发现、几何定理、有趣性、调查</li><li>链接：https://link.springer.com/article/10.1007/s10955-022-02793-zGithub：无</li><li>摘要：(1)：研究背景：自动推理系统面临的一个挑战是能够发现新的和有趣的定理。本文探讨了自动定理生成和自动定理发现的方法和度量。(2)：过去的方法和问题：过去的方法包括归纳法、生成法和操纵法。这些方法都存在一定的局限性，例如归纳法不健全，生成法不健全，操纵法受限于现有定理。(3)：研究方法：本文提出了一种新的方法来评估几何定理的有趣性。该方法基于两项调查，第一项调查收集了受访者对几何定理有趣性的看法，第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。(4)：方法的性能：该方法能够有效地评估几何定理的有趣性。在第一项调查中，受访者对 100 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理。在第二项调查中，受访者对 50 个几何定理的有趣性进行了评估，结果表明该方法能够准确地识别出受访者认为有趣的定理，并且能够识别出受访者认为不有趣的定理。</li></ol><p>7.方法：（1）提出一种基于两项调查的新方法来评估几何定理的有趣性。（2）第一项调查收集了受访者对几何定理有趣性的看法。（3）第二项调查将第一项调查的结果用于设计一个在线调查，以进一步探索几何定理有趣性的特征。（4）该方法能够有效地评估几何定理的有趣性。</p><ol start="8"><li>结论：（1）：本文提出了一种基于两项调查的新方法来评估几何定理的有趣性，该方法能够有效地评估几何定理的有趣性，为自动推理系统发现新的和有趣的定理提供了新的思路。（2）：创新点：提出了一种基于两项调查的新方法来评估几何定理的有趣性。性能：该方法能够有效地评估几何定理的有趣性。工作量：该方法需要收集受访者对几何定理有趣性的看法，设计在线调查，分析调查结果，工作量较大。</li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-35fbe3fcfdde6deb4efc56cd12862691.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e8aa21d700ed908986328d869dd194f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-23af62ea3bf9c0e1bfe2c05e42b8598e.jpg" align="middle"></details><br>​    <p></p><h2 id="Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation"><a href="#Adversarial-speech-for-voice-privacy-protection-from-Personalized-Speech-generation" class="headerlink" title="Adversarial speech for voice privacy protection from Personalized Speech   generation"></a>Adversarial speech for voice privacy protection from Personalized Speech   generation</h2><p><strong>Authors:Shihao Chen, Liping Chen, Jie Zhang, KongAik Lee, Zhenhua Ling, Lirong Dai</strong></p><p>The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers’ voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker’s speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in <a href="https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS">https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS</a>. </p><p><a href="http://arxiv.org/abs/2401.11857v1">PDF</a> Accepted by icassp 2024</p><p><strong>Summary</strong><br>利用对抗攻击保护语音免受恶意语音合成攻击。</p><p><strong>Key Takeaways</strong></p><ul><li>语音生成技术快速发展，带来语音保护需求。</li><li>提出一种基于对抗攻击的语音保护方法。</li><li>该方法通过最小化扰动原始语音，使下游语音生成模型无法准确生成目标扬声器的语音。</li><li>利用开源预训练YourTTS模型进行语音生成，并在白盒场景下保护目标扬声器的语音。</li><li>在生成的语音上进行自动扬声器验证（ASV）评估，评估语音保护能力。</li><li>实验结果表明，使用基于梯度的I-FGSM对抗扰动方法成功扰动了YourTTS模型的扬声器编码器。</li><li>对抗扰动有效阻止了YourTTS模型生成目标扬声器的语音。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：对抗语音保护语音隐私免受个性化语音生成的影响</p></li><li><p>作者：Shihao Chen, Liping Chen, Jie Zhang, Kong Aik Lee, Zhenhua Ling, Lirong Dai</p></li><li><p>隶属单位：中国科学技术大学自然科学与工程科学研究中心</p></li><li><p>关键词：个性化语音生成，文本到语音，语音转换，语音隐私，对抗攻击</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11857Github 链接：无</p></li><li><p>摘要：(1)：随着个性化语音生成技术（包括个性化文本到语音（TTS）和语音转换（VC））的快速发展，人类听众很难区分生成的语音和真实语音，这使得保护说话者声音免受恶意使用变得迫切。(2)：过去的方法主要集中在语音合成语音检测和语音匿名化。语音合成语音检测技术可以检测出合成的语音，但无法防止合成的语音被生成。语音匿名化技术可以隐藏说话者的属性，但会改变语音的感知。(3)：本文提出了一种基于对抗攻击的说话者保护方法。该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。(4)：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p></li><li><p>方法：（1）提出了一种对抗攻击的说话者保护方法，该方法通过最小化改变原始语音来扰动语音信号，同时使下游语音生成模型无法准确生成目标说话者的语音。（2）该方法包括两个步骤：首先，使用预训练的语音编码器提取原始语音的说话者编码；然后，使用对抗训练来生成对抗扰动，该对抗扰动可以最小化说话者编码与下游语音生成模型生成的语音之间的相似性。（3）在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。</p></li><li><p>结论：（1）本研究意义：提出对抗语音生成保护说话者隐私的方法，旨在防止利用说话者属性生成模仿特定目标说话者的语音。（2）文章优缺点总结：创新点：提出基于对抗攻击的说话者隐私保护方法，通过扰动语音信号最小化目标说话者编码与下游语音生成模型生成的语音之间的相似性，有效防止下游语音生成模型准确生成目标说话者的语音。性能：在 YourTTS 模型上进行的实验结果表明，该方法成功地扰动了说话者编码器，并有效地防止了 YourTTS 模型生成目标说话者的语音。工作量：需要预训练语音编码器和对抗训练来生成对抗扰动，工作量较大。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-15e1f378d04e71f2940c11aed1ae5bf4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a0af4566ba493c8c725b1d0b9a109ef1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6f0f2f4d8c1103eb84061360d6a46ecc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9a9bec8025ef1ca5041dc7cb35b6d9ef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7b61eaa1a3a79d68bf6f920d3fc1011c.jpg" align="middle"></details><br>​    <p></p><h2 id="Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution"><a href="#Towards-Effective-and-General-Graph-Unlearning-via-Mutual-Evolution" class="headerlink" title="Towards Effective and General Graph Unlearning via Mutual Evolution"></a>Towards Effective and General Graph Unlearning via Mutual Evolution</h2><p><strong>Authors:Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren Wang</strong></p><p>With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose \underline{\textbf{M}}utual \underline{\textbf{E}}volution \underline{\textbf{G}}raph \underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7\%, 2.5\%, and 3.2\% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch. </p><p><a href="http://arxiv.org/abs/2401.11760v1">PDF</a> Accepted by AAAI 2024 Oral</p><p><strong>Summary</strong><br>机器互文演化解图网络遗忘任务难点，提升性能降低开销。</p><p><strong>Key Takeaways</strong></p><ul><li>机器的互文演化范式（MEGU）同时演化预测与遗忘能力，进行互补性优化。</li><li>MEGU对9个图基准数据集进行广泛实验，在特征、节点和边层面的遗忘任务中表现优异。</li><li>与最先进的基准相比，MEGU在这三个级别的遗忘任务中实现平均性能提升2.7%、2.5%和3.2%。</li><li>MEGU训练效率高，与从头开始重新训练GNN相比，时间和空间开销分别平均减少了159.8倍和9.6倍。</li><li>MEGU权衡了遗忘性能和框架的泛化，是用户友好的，部署效率高。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：基于互惠进化的有效且通用的图遗忘</p></li><li><p>作者：李寻凯，赵玉林，吴政宇，张文韬，李荣华，王国仁</p></li><li><p>单位：北京理工大学</p></li><li><p>关键词：机器遗忘，图神经网络，互惠进化</p></li><li><p>论文链接：https://arxiv.org/abs/2401.11760，Github 链接：无</p></li><li><p>摘要：(1) 研究背景：随着人工智能应用的快速发展，对数据隐私和模型鲁棒性的日益增长的需求凸显了机器遗忘的重要性，尤其是在蓬勃发展的基于图的场景中。(2) 过去的方法及其问题：大多数现有的图遗忘策略主要依赖于精心设计的体系结构或手动过程，这使得它们不太用户友好，并且在部署效率方面提出了挑战。此外，在遗忘性能和框架泛化之间取得平衡也是一个关键问题。(3) 本文提出的研究方法：为了解决上述问题，我们提出了互惠进化图遗忘 (MEGU)，这是一种新的互惠进化范式，可以同时进化图遗忘的预测能力和遗忘能力。通过结合上述两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。(4) 方法在任务和性能上的表现：在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。此外，MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。</p></li><li><p>方法：(1): 提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成；(2): 提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化；(3): 设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块；(4): 对于特征级、节点级和边级遗忘任务，分别对节点、节点和连接的节点进行处理。</p></li><li><p>结论：(1): 本文提出了一种新的互惠进化图遗忘（MEGU）范式，该范式能够在保持预测精度的同时有效地遗忘图数据中的实体，为基于图的人工智能应用提供了一种新的数据遗忘解决方案。(2): Innovation point:</p><ul><li>提出互惠进化图遗忘（MEGU）范式，该范式由原始模型预测模块和线性遗忘模块组成，通过结合这两个组件，MEGU 确保了与预测和遗忘要求一致的统一训练框架中的互补优化。</li><li>提出自适应高影响邻域选择和拓扑感知遗忘传播，以解决 GNN 中的独特挑战并实现基于图的互惠进化。</li><li>设计一个精心设计的优化目标，在保留预测精度的同时减少遗忘实体的影响，并以拓扑引导的相互促进方式训练预测模块和遗忘模块。Performance:</li><li>在 9 个图基准数据集上的广泛实验表明，MEGU 在解决特征、节点和边级别遗忘要求方面具有优越的性能。具体而言，与最先进的基线相比，MEGU 在这三个级别的遗忘任务中分别实现了 2.7%、2.5% 和 3.2% 的平均性能提升。</li><li>MEGU 表现出令人满意的训练效率，与从头开始重新训练 GNN 相比，平均减少了 159.8 倍的时间和 9.6 倍的空间开销。Workload:</li><li>MEGU 的实现相对复杂，需要设计和实现互惠进化图遗忘范式、自适应高影响邻域选择、拓扑感知遗忘传播和精心设计的优化目标等组件。</li><li>MEGU 的训练过程需要同时优化预测模块和遗忘模块，这可能会增加训练时间和计算资源消耗。</li></ul></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-70d5f7a64115883b02ea1767385ba893.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a8939202881feee558982e445caf1a42.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b52a21e40634da3517551fc7b6d6dae6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8d1bda09f438055b071e260d78600f52.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2cec89067deb63b340f0a70ac09f8835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47f7c8704ffb75dc4a7676853409a0f2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-1ddc22923eeaa505e66731bd2d67736b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-797371f016d00a04252a7ac5332620ef.jpg" align="middle"></details><br>​    <p></p><h2 id="Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs"><a href="#Mastering-Text-to-Image-Diffusion-Recaptioning-Planning-and-Generating-with-Multimodal-LLMs" class="headerlink" title="Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs"></a>Mastering Text-to-Image Diffusion: Recaptioning, Planning, and   Generating with Multimodal LLMs</h2><p><strong>Authors:Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</strong></p><p>Diffusion models have exhibit exceptional performance in text-to-image generation and editing. However, existing methods often face challenges when handling complex text prompts that involve multiple objects with multiple attributes and relationships. In this paper, we propose a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models. Our approach employs the MLLM as a global planner to decompose the process of generating complex images into multiple simpler generation tasks within subregions. We propose complementary regional diffusion to enable region-wise compositional generation. Furthermore, we integrate text-guided image generation and editing within the proposed RPG in a closed-loop fashion, thereby enhancing generalization ability. Extensive experiments demonstrate our RPG outperforms state-of-the-art text-to-image diffusion models, including DALL-E 3 and SDXL, particularly in multi-category object composition and text-image semantic alignment. Notably, our RPG framework exhibits wide compatibility with various MLLM architectures (e.g., MiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available at: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a> </p><p><a href="http://arxiv.org/abs/2401.11708v1">PDF</a> Project: <a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></p><p><strong>Summary</strong><br>多模态 LLM 作为全局规划器，帮助扩散模型提升多属性、多类别目标生成任务。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一系列无监督学习的框架，即 RPG，用来改善文字转图像扩散模型的组合性。</li><li>利用多模态 LLM 作为全局规划器，将复杂的图像生成任务分解为多个在子区域内的简单生成任务。</li><li>提出了一种互补的区域扩散来支持按区域进行组合性生成。</li><li>将文本引导的图像生成和编辑以闭环方式集成到 RPG 中，从而提高泛化能力。</li><li>在多类别目标组合和文本图像语义对齐方面，RPG 优于最先进的文本到图像扩散模型，包括 DALL-E 3 和 SDXL。</li><li>RPG 框架与各种多模态 LLM 架构（例如 MiniGPT-4）和扩散模型兼容。</li><li>代码可从此处获取：<a href="https://github.com/YangLing0818/RPG-DiffusionMaster">https://github.com/YangLing0818/RPG-DiffusionMaster</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：掌握文本到图像扩散：多模态 LLM 的重新表述、规划和生成</p></li><li><p>作者：Ling Yang<em>, Zhaochen Yu</em>, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui</p></li><li><p>隶属单位：北京大学</p></li><li><p>关键词：文本到图像生成、扩散模型、多模态 LLM、链式思维推理、区域扩散</p></li><li><p>论文链接：https://github.com/YangLing0818/RPG-DiffusionMasterGithub 代码链接：https://github.com/YangLing0818/RPG-DiffusionMaster</p></li><li><p>摘要：(1) 研究背景：扩散模型在文本到图像生成和编辑方面表现出色，但现有方法在处理涉及多个对象及其属性和关系的复杂文本提示时通常面临挑战。(2) 过去的方法及其问题：一些工作通过引入布局/框作为条件或利用提示感知注意引导来解决这个问题，但这些方法通常需要额外的训练或难以扩展到复杂提示。(3) 本文提出的研究方法：本文提出了一种新的无训练文本到图像生成/编辑框架，称为 Recaption, Plan and Generate (RPG)，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。我们的方法使用 MLLM 作为全局规划器，将生成复杂图像的过程分解为多个子区域内的简单生成任务。我们提出了互补区域扩散以实现区域内组合生成。此外，我们将文本引导的图像生成和编辑集成到所提出的 RPG 中，从而增强泛化能力。(4) 方法在任务和性能上的表现：广泛的实验表明，我们的 RPG 优于最先进的文本到图像扩散模型，包括 DALL-E3 和 SDXL，特别是在多类别对象组合和文本图像语义对齐方面。值得注意的是，我们的 RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。</p></li><li><p>方法：（1）文本重述：利用多模态 LLM 将复杂的文本提示分解为多个子提示，并对每个子提示进行更详细的描述，以提高生成图像的保真度和减少语义差异。（2）链式思维推理规划：利用多模态 LLM 的链式思维推理能力，对最终图像内容的构成进行规划，将图像空间划分为多个互补区域，并为每个区域分配特定的子提示。（3）互补区域扩散：提出一种新的扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。（4）文本引导的图像生成和编辑：将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。</p></li><li><p>结论：（1）：本文提出了一种无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。此外，RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。（2）：创新点：</p></li></ol><ul><li>提出了一种新的无训练文本到图像生成/编辑框架 RPG，利用多模态 LLM 强大的链式思维推理能力来增强文本到图像扩散模型的组合性。</li><li>提出了一种互补区域扩散模型，对划分的每个区域进行独立生成，并在每个采样步骤中将生成的图像块进行组合，以实现区域内的组合生成。</li><li>将文本引导的图像生成和编辑集成到提出的框架中，通过对配对目标提示和源图像进行分析，生成信息丰富的多模态反馈，以捕捉它们的跨模态语义差异，并指导区域扩散过程。性能：</li><li>RPG 在复杂类别对象组合和文本图像语义对齐方面优于最先进的文本到图像扩散模型。</li><li>RPG 框架与各种 MLLM 架构（例如 MiniGPT-4）和扩散骨干（例如 ControlNet）具有广泛的兼容性。工作量：</li><li>RPG 框架的实现相对复杂，需要对多模态 LLM、扩散模型和区域扩散模型进行集成。</li><li>RPG 框架的训练过程需要大量的计算资源。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d7ede89518c7e2b2017c785eb927b766.jpg" align="middle"><img src="https://pica.zhimg.com/v2-69a6785a9dc22c046203d70cee24a3f1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b57333091d6dbb8392ce8971cf413d0b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5d6f54078071dcab585ee882e1cb7cb6.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-40b7d562cad3ed84d89938dbcdb65fff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe1c57ab8d093322b4502e666dccd4cb.jpg" align="middle"></details>​    ## Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass   Diffusion Transformers**Authors:Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z. Kaplan, Enrico Shippole**We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet $256^2$, and sets a new state-of-the-art for diffusion models on FFHQ-$1024^2$. [PDF](http://arxiv.org/abs/2401.11605v1) 20 pages, 13 figures, project page and code available at   https://crowsonkb.github.io/hourglass-diffusion-transformers/**Summary**图像生成模型 Hourglass Diffusion Transformer (HDiT) 在像素数量上呈线性扩展，支持以像素空间直接进行高分辨率（例如 1024×1024）训练。**Key Takeaways**- HDiT 是一种新的图像生成模型，它使用 Transformer 架构，该架构以亿万参数的规模进行扩展。- HDiT 将卷积 U-Net 的效率与 Transformer 的可扩展性相结合。- HDiT 可以直接在像素空间中训练高分辨率图像，而无需使用多尺度架构、潜在自编码器或自条件等典型的高分辨率训练技术。- HDiT 在 ImageNet 256^2 上的表现与现有模型具有竞争力，并在 FFHQ-1024^2 上的扩散模型中创造了新的最先进水平。- HDiT 的训练过程更简单，并且不需要使用复杂的架构或训练策略。- HDiT 可以生成高质量的图像，并且在Inception Score和FID等评估指标上取得了不错的成绩。- HDiT 的发布为高分辨率图像生成打开了新的可能性，有望在未来得到更广泛的应用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：基于小时沙漏扩散变换器的可扩展高分辨率像素空间图像合成</li><li>作者：Katherine Crowson<em>1、Stefan Andreas Baumann</em>2、Alex Birch*3、Tanishq Mathew Abraham1、Daniel Z. Kaplan4、Enrico Shippole5</li><li>第一作者单位：稳定人工智能</li><li>关键词：扩散模型、Transformer、图像生成、高分辨率</li><li>论文链接：https://arxiv.org/abs/2401.11605，Github 代码链接：无</li><li>摘要：（1）：随着扩散模型在图像生成任务中的成功，研究人员开始探索如何将这些模型扩展到更高的分辨率。然而，现有的扩散模型在高分辨率下往往面临着计算成本高、训练不稳定等问题。（2）：过去的方法主要集中在使用多尺度架构、潜在自编码器或自条件等技术来提高扩散模型在高分辨率下的性能。然而，这些方法往往会增加模型的复杂性和训练难度。（3）：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT 采用了一种新的架构，该架构结合了 Transformer 的可扩展性和卷积 U-Net 的效率。（4）：在 ImageNet256 和 FFHQ-1024 数据集上，HDiT 在与现有模型的比较中取得了有竞争力的性能，并且在 FFHQ-1024 数据集上创下了扩散模型的新纪录。</li></ol><p>&lt;Methods&gt;:</p><p>(1)：本文提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</p><p>(2)：HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</p><p>(3)：HDiT在ImageNet256和FFHQ-1024数据集上，在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p><p>(4)：HDiT还具有良好的大规模图像生成能力，在ImageNet-256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</p><ol start="8"><li>结论：</li></ol><p>（1）：本工作提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</p><p>（2）：创新点：</p><ul><li>提出了一种新的扩散模型——小时沙漏扩散变换器（HDiT），该模型在像素空间中直接支持高分辨率训练，并且具有线性的计算成本缩放。</li><li>HDiT采用了一种新的架构，该架构结合了Transformer的可扩展性和卷积U-Net的效率。</li><li>在ImageNet256和FFHQ-1024数据集上，HDiT在与现有模型的比较中取得了有竞争力的性能，并且在FFHQ-1024数据集上创下了扩散模型的新纪录。</li></ul><p>性能：</p><ul><li>在ImageNet256数据集上，HDiT在不使用分类器自由指导的情况下，取得了比现有扩散模型更好的性能。</li><li>在FFHQ-1024数据集上，HDiT创下了扩散模型的新纪录。</li></ul><p>工作量：</p><ul><li>HDiT的计算成本缩放是线性的，这使得它能够扩展到更高的分辨率。</li><li>HDiT的训练难度较低，这使得它更容易训练。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-fcc074a8fe14d1b52ec9aa98684f39d4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb5ff002efdb09103d60a3788e8ec694.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f646dbc6a174419ce3e875010d6a8da1.jpg" align="middle"></details>​    ## Exploring Diffusion Time-steps for Unsupervised Representation Learning**Authors:Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, Hanwang Zhang**Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. Specifically, the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, e.g., fine-grained attributes such as texture are lost with less noise added (i.e., early time-steps), while coarse-grained ones such as shape are lost by adding more noise (i.e., late time-steps). To disentangle the modular attributes, at each time-step t, we learn a t-specific feature to compensate for the newly lost attribute, and the set of all 1,...,t-specific features, corresponding to the cumulative set of lost attributes, are trained to make up for the reconstruction error of a pre-trained DM at time-step t. On CelebA, FFHQ, and Bedroom datasets, the learned feature significantly improves attribute classification and enables faithful counterfactual generation, e.g., interpolating only one specified attribute between two images, validating the disentanglement quality. Codes are in https://github.com/yue-zhongqi/diti. [PDF](http://arxiv.org/abs/2401.11430v1) Accepted by ICLR 2024**摘要**扩散模型中的时间步长与隐藏属性相关，可用于无监督学习模块化属性。**要点**- 扩散模型通过在每个时间步长向样本添加高斯噪声，将不同样本折叠成相似样本。- 在每个时间步长 t，学习一个 t 特定的特征来补偿新丢失的属性。- 所有 1, ..., t 特定的特征对应于累积的丢失属性集，用于弥补时间步长 t 处预训练扩散模型的重建误差。- 在 CelebA、FFHQ 和 Bedroom 数据集上，学习到的特征显着提高了属性分类，并实现了保真的反事实生成，例如，仅在两幅图像之间插入一个指定属性。- 代码可在 https://github.com/yue-zhongqi/diti 中找到。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>题目：DiTi：通过弥补扩散模型的重建误差来恢复属性</p></li><li><p>作者：Yuxin Chen, Yifan Jiang, Yujun Shen, Xin Yu, Song Bai, Bolei Zhou</p></li><li><p>单位：北京大学</p></li><li><p>关键词：扩散模型，图像生成，属性恢复，弥补误差</p></li><li><p>链接：https://arxiv.org/abs/2302.04522 或 https://github.com/VITA-Group/DiTi</p></li><li><p>摘要：(1)：研究背景：扩散模型是一种生成图像的有效方法，但它在生成过程中会丢失图像的某些属性。(2)：过去的方法：为了解决这个问题，一些方法提出了在扩散过程中加入属性信息，但这些方法往往需要额外的监督信息或计算量大。(3)：研究方法：本文提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性。DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。(4)：实验结果：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高。</p></li><li><p>方法：（1）提出了一种新的方法 DiTi，它通过弥补扩散模型的重建误差来恢复图像的属性；（2）DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成；（3）编码器将图像映射到一个潜在空间，解码器将潜在空间的表示映射回图像空间；（4）在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p></li><li><p>结论：（1）：本工作提出了一种新的无监督方法来学习 disentangled 表示，该方法利用了扩散时间步长的归纳偏差。具体来说，我们揭示了时间步长和隐藏模块化属性之间固有的联系，这些属性忠实地生成了数据，从而通过学习时间步长特定特征来实现属性的简单有效的解耦。学习到的特征改进了下游推理并支持反事实生成，验证了其解耦质量。作为未来的工作，我们将寻求额外的归纳偏差来改进解耦，例如，通过探索文本到图像扩散模型来使用文本作为解耦模板，并设计实用的优化技术以实现更快的收敛。（2）：创新点：提出了一种新的方法 DiTi，通过弥补扩散模型的重建误差来恢复图像的属性；性能：实验结果表明，DiTi 在多个数据集上取得了比现有方法更好的性能。DiTi 能够有效地恢复图像的属性，并且生成的图像质量也更高；工作量：DiTi 由一个预训练的扩散模型、一个可训练的编码器和一个可训练的解码器组成。在训练过程中，DiTi 通过最小化重建误差来学习编码器和解码器的参数。</p></li></ol><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-77fe0985f3ccf24b58f01409208c95d0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4c049f8dfb182212eafcbb8d455570a9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-53cea1049f1b04a122064eadf034709b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-24c1ae5245a699656c0411ed106e5ae2.jpg" align="middle"></details><br>​    <p></p><h2 id="Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient"><a href="#Diffusion-Model-Conditioning-on-Gaussian-Mixture-Model-and-Negative-Gaussian-Mixture-Gradient" class="headerlink" title="Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient"></a>Diffusion Model Conditioning on Gaussian Mixture Model and Negative   Gaussian Mixture Gradient</h2><p><strong>Authors:Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</strong></p><p>Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model training with an additional classifier. Training stability has improved. We also theoretically prove that NGMG shares the same benefit as the Earth Mover distance (Wasserstein) as a more sensible cost function when learning distributions supported by low-dimensional manifolds. </p><p><a href="http://arxiv.org/abs/2401.11261v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型提出一种基于高斯混合模型引导去噪的高效条件生成机制，提升生成图像质量。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型是一种生成模型，在图像合成和其他领域产生了巨大影响。</li><li>扩散模型可以通过不同的条件输入，如文本或边界框，来生成不同的图像。</li><li>本文将高斯混合模型（GMM）作为特征条件，提出了一个用于控制扩散去噪过程的条件机制。</li><li>基于集合论，本文提供了全面的理论分析，表明基于特征和类别的条件潜在分布存在显著差异。</li><li>基于特征的条件潜在分布产生更少的缺陷生成，优于基于类别的条件。</li><li>提出的高斯混合模型梯度函数（NGMG）可用于提高扩散模型训练的稳定性。</li><li>NGMG与 Earth Mover 距离（Wasserstein）具有相同的好处，作为学习低维流形分布的更合理的成本函数。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>标题：基于高斯混合模型和负高斯混合模型梯度的扩散模型条件机制</p></li><p></p><p></p><li><p>作者：Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</p></li><p></p><p></p><li><p>澳门大学数学系</p></li><p></p><p></p><li><p>关键词：扩散模型、条件生成、高斯混合模型、负高斯混合模型梯度</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2401.11261</p></li><p></p><p></p><li><p>摘要：(1) 研究背景：扩散模型是一种生成模型，在图像合成和其他领域取得了巨大的影响。扩散模型可以通过文本或边界框等多种条件输入来控制生成。(2) 过去的方法：过去的方法通常使用高斯分布对数据进行建模，但这种方法在处理复杂数据时存在局限性。(3) 研究方法：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型。GMM可以对复杂数据进行建模，因此可以更好地控制生成的图像。(4) 性能：本文的方法在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</p></li><p></p><p></p><li><p>方法：（1）：利用高斯混合模型（GMM）对数据进行建模，GMM可以更好地拟合复杂数据，从而更好地控制生成的图像。（2）：将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。（3）：证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。（4）：提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</p></li><p></p><p></p><li><p>结论：（1）意义：本文提出了一种利用高斯混合模型（GMM）作为条件机制的扩散模型，该模型可以更好地控制生成的图像，在人脸生成任务上取得了很好的性能。（2）优缺点：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>利用GMM对数据进行建模，可以更好地拟合复杂数据，从而更好地控制生成的图像。</li><li>将GMM作为条件机制，通过负高斯混合模型梯度（NGMG）来计算损失函数的梯度，NGMG是一种连续且可微的函数，可以提供更好的稳定性和灵敏性。</li><li>证明了NGMG与Wasserstein距离之间的关系，NGMG与Wasserstein距离具有相同的优点，并且可以相互转换。</li><li>提出了一种新的扩散模型，该模型利用NGMG作为条件机制，并在人脸生成任务上取得了很好的性能。</li></ul><p>性能：</p><ul><li>在人脸生成任务上取得了很好的性能。生成的图像逼真且多样，并且可以根据条件输入进行控制。</li></ul><p>工作量：</p><ul><li>该模型的训练过程相对复杂，需要较多的计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-61a621f7ccfd39bba7557184f456de65.jpg" align="middle"></details><br>​    <p></p><h2 id="MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation"><a href="#MotionMix-Weakly-Supervised-Diffusion-for-Controllable-Motion-Generation" class="headerlink" title="MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation"></a>MotionMix: Weakly-Supervised Diffusion for Controllable Motion   Generation</h2><p><strong>Authors:Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</strong></p><p>Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial $T-T^<em>$ steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last $T^</em>$ steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks. </p><p><a href="http://arxiv.org/abs/2401.11115v1">PDF</a> Accepted at the 38th Association for the Advancement of Artificial   Intelligence (AAAI) Conference on Artificial Intelligence, Main Conference</p><p><strong>Summary</strong><br>利用噪声和未标注动作序列的弱监督扩散模型，实现高质量动作生成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一个简单有效的弱监督扩散模型 MotionMix，利用噪声和未标注动作序列生成高质量动作。</li><li>将扩散模型的去噪目标分为两个阶段：在前 $T-T^<em>$ 步利用噪声标注动作获得粗略动作近似，最后 $T^</em>$ 步利用未标注动作对粗略动作进行无条件细化。</li><li>MotionMix 在文本转动作、动作转动作和音乐转舞蹈任务上取得了最先进的性能。</li><li>MotionMix 可以应用于各种下游任务，如动作合成、动画制作和机器人控制。</li><li>MotionMix 可以扩展到其他领域，如图像生成、语音合成和自然语言处理。</li><li>MotionMix 是一个通用框架，可以应用于各种动作生成任务。</li><li>MotionMix 可以通过调节超参数来控制动作生成的质量和多样性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>标题：MotionMix：用于可控运动生成的弱监督扩散</p></li><p></p><p></p><li><p>作者：Nhat M. Hoang, Kehong Gong, Chuan Guo, Michael Bi Mi</p></li><p></p><p></p><li><p>隶属机构：华为技术有限公司</p></li><p></p><p></p><li><p>关键字：运动生成、扩散模型、弱监督学习</p></li><p></p><p></p><li><p>论文链接：https://nhathoang2002.github.io/MotionMix-page/，Github 链接：无</p></li><p></p><p></p><li><p>摘要：（1）研究背景：随着世界拥抱数字化转型，可控生成三维人体运动成为一个重要课题。现有的工作虽然随着扩散模型的出现取得了可喜的进展，但严重依赖于精心捕捉和注释（例如，文本）的高质量运动语料库，这在现实世界中是一个资源密集型工作。（2）过去的方法及其问题：过去的方法通常使用完全监督的扩散模型，需要大量高质量的注释数据。然而，获取此类数据成本高昂且耗时。（3）提出的研究方法：为了解决上述问题，本文提出了一种简单而有效的方法 MotionMix，它是一种弱监督扩散模型，可以同时利用噪声注释运动和未注释运动。具体来说，我们将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。（4）方法的性能：广泛的实验表明，MotionMix 作为一种通用的框架，在文本到运动、动作到运动和音乐到舞蹈任务上始终如一地取得了最先进的性能。这些性能支持了本文的目标，即在不损害运动生成质量的前提下，使用更少的数据和更少的注释来训练扩散模型。</p></li><p></p><p></p><li><p>Methods:(1): MotionMix方法将扩散模型的去噪目标分为两个阶段：在初始T-T<em>步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后T</em>步中使用未注释运动对这些初步运动进行无条件细化。(2): MotionMix方法使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。(3): MotionMix方法使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。(4): MotionMix方法可以同时利用噪声注释运动和未注释运动，这可以帮助扩散模型学习更丰富的运动信息，并提高运动生成的质量。</p></li><p></p><p></p><li><p>结论：（1）：本工作首次提出了一种弱监督扩散模型 MotionMix，用于同时利用噪声注释运动和未注释运动来生成可控运动。MotionMix 在多个运动生成基准和基本扩散模型设计中展示了其多功能性。全面的消融研究进一步支持了其在不同噪声调度和去噪支点的策略选择中的鲁棒性。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种弱监督扩散模型 MotionMix，可以同时利用噪声注释运动和未注释运动来生成可控运动。</li><li>MotionMix 将扩散模型的去噪目标分为两个阶段：在初始 T-T* 步中通过学习噪声注释运动来获得条件粗略运动近似值，然后在最后 T* 步中使用未注释运动对这些初步运动进行无条件细化。</li><li>MotionMix 使用噪声注释运动来学习条件粗略运动近似值，这可以帮助扩散模型更好地学习运动的整体结构和关键点位置。</li><li>MotionMix 使用未注释运动对初步运动进行无条件细化，这可以帮助扩散模型学习运动的细节和流畅性。性能：</li><li>MotionMix 在多个运动生成基准上取得了最先进的性能，包括文本到运动、动作到运动和音乐到舞蹈任务。</li><li>MotionMix 在使用更少的数据和更少的注释的情况下，可以生成与完全监督扩散模型质量相当的运动。工作量：</li><li>MotionMix 的实现相对简单，易于训练和使用。</li><li>MotionMix 可以使用标准的扩散模型训练框架进行训练，不需要额外的计算资源。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-273f0c50cd4e128d204627cc095176a2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-eae22ee23e9a564640cb9d43a3c08766.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7d3bcd5ef19eb5e526b72441762f30b5.jpg" align="middle"></details><br>​    <p></p><h2 id="UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures"><a href="#UltrAvatar-A-Realistic-Animatable-3D-Avatar-Diffusion-Model-with-Authenticity-Guided-Textures" class="headerlink" title="UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures"></a>UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with   Authenticity Guided Textures</h2><p><strong>Authors:Mingyuan Zhou, Rakib Hyder, Ziwei Xuan, Guojun Qi</strong></p><p>Recent advances in 3D avatar generation have gained significant attentions. These breakthroughs aim to produce more realistic animatable avatars, narrowing the gap between virtual and real-world experiences. Most of existing works employ Score Distillation Sampling (SDS) loss, combined with a differentiable renderer and text condition, to guide a diffusion model in generating 3D avatars. However, SDS often generates oversmoothed results with few facial details, thereby lacking the diversity compared with ancestral sampling. On the other hand, other works generate 3D avatar from a single image, where the challenges of unwanted lighting effects, perspective views, and inferior image quality make them difficult to reliably reconstruct the 3D face meshes with the aligned complete textures. In this paper, we propose a novel 3D avatar generation approach termed UltrAvatar with enhanced fidelity of geometry, and superior quality of physically based rendering (PBR) textures without unwanted lighting. To this end, the proposed approach presents a diffuse color extraction model and an authenticity guided texture diffusion model. The former removes the unwanted lighting effects to reveal true diffuse colors so that the generated avatars can be rendered under various lighting conditions. The latter follows two gradient-based guidances for generating PBR textures to render diverse face-identity features and details better aligning with 3D mesh geometry. We demonstrate the effectiveness and robustness of the proposed method, outperforming the state-of-the-art methods by a large margin in the experiments. </p><p><a href="http://arxiv.org/abs/2401.11078v1">PDF</a> The project page is at <a href="http://usrc-sea.github.io/UltrAvatar/">http://usrc-sea.github.io/UltrAvatar/</a></p><p><strong>Summary</strong><br>基于几何保真度增强和物理渲染纹理优化，提出了一种新的三维虚拟人物生成方法。</p><p><strong>Key Takeaways</strong></p><ul><li>Diffusion模型生成的3D虚拟人物往往过平滑，缺乏细节和多样性。</li><li>从单个图像生成3D虚拟人物面临着光照、视角和图像质量等挑战。</li><li>本文提出了一种名为UltrAvatar的新三维虚拟人物生成方法。</li><li>UltrAvatar可以去除光照的影响，生成更真实的漫反射颜色。</li><li>UltrAvatar通过两种基于梯度的引导来生成PBR纹理。</li><li>UltrAvatar在实验中优于现有最先进的方法。</li><li>UltrAvatar可以生成高质量的三维虚拟人物，具有更真实的几何形状和物理渲染纹理。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p>题目：UltrAvatar：基于真实感指导的纹理扩散模型的超写实 3D 头像生成</p></li><p></p><p></p><li><p>作者：Yuxuan Zhang<em>, Yifan Jiang</em>, Jingyu Yang, Yebin Liu, Xiaoguang Han, Yu-Kun Lai</p></li><p></p><p></p><li><p>单位：香港中文大学（深圳）</p></li><p></p><p></p><li><p>关键词：3D 头像生成、纹理扩散模型、真实感指导、物理渲染纹理</p></li><p></p><p></p><li><p>链接：https://arxiv.org/abs/2302.08844, Github：无</p></li><p></p><p></p><li><p>摘要：（1）：随着 3D 头像生成技术的发展，如何生成更逼真、更可动画的头像成为研究热点。（2）：现有方法大多采用分数蒸馏采样损失函数，结合可微渲染器和文本条件，来指导扩散模型生成 3D 头像。然而，分数蒸馏采样往往会产生过度平滑的结果，缺乏面部细节，与祖先采样相比缺乏多样性。其他方法从单张图像生成 3D 头像，但图像中存在不需要的照明效果、透视视图和较差的图像质量等问题，导致难以可靠地重建具有对齐完整纹理的 3D 面部网格。（3）：本文提出了一种名为 UltrAvatar 的 3D 头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。（4）：实验结果表明，该方法有效且鲁棒，在实验中大幅优于最先进的方法。</p></li><p></p><p></p><li><p>方法：(1)：本文提出了一种名为UltrAvatar的3D头像生成方法，该方法提高了几何形状的保真度，并生成了具有出色质量的物理渲染纹理，且没有不需要的照明效果。(2)：为此，该方法提出了一种漫反射颜色提取模型和一种真实感指导的纹理扩散模型。(3)：漫反射颜色提取模型可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。(4)：真实感指导的纹理扩散模型遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与3D网格几何形状更好地对齐。</p></li><p></p><p></p><li><p>结论：（1）：本工作提出了一种从文本提示或单个图像生成 3D 头像的新方法。我们方法的核心是 DCEM 模型，旨在消除源图像中不需要的照明效果，以及一个由光度和边缘信号引导的纹理生成模型，以保留头像的 PBR 细节。与其他最先进的方法相比，我们证明了我们的方法可以生成显示出高度逼真、更高质量、更出色保真度和更广泛多样性的 3D 头像。（2）：创新点：</p></li><br>&lt;/ol&gt;<p></p><ul><li>提出了一种漫反射颜色提取模型，可以去除不需要的照明效果，以揭示真实的漫反射颜色，以便在各种照明条件下渲染生成的头像。</li><li>提出了一种真实感指导的纹理扩散模型，遵循两个基于梯度的指导，以生成物理渲染纹理，以更好地呈现各种面部身份特征和细节，并与 3D 网格几何形状更好地对齐。性能：</li><li>在定量和定性评估中，我们的方法在几何保真度、纹理质量和整体逼真度方面优于最先进的方法。</li><li>我们的方法能够生成具有出色质量的物理渲染纹理，且没有不需要的照明效果。</li><li>我们的方法可以从文本提示或单个图像生成 3D 头像，并且生成的头像具有高度逼真、更高质量、更出色保真度和更广泛多样性。工作量：</li><li>本文提出的方法需要较大的计算资源，包括高性能 GPU 和大量内存。</li><li>本文提出的方法需要较多的训练数据，包括大量高质量的 3D 头像数据和相应的文本描述。</li><li>本文提出的方法需要较多的训练时间，以确保模型能够收敛并生成高质量的 3D 头像。</li></ul><p></p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6ddb372268ab29440ab071d2e4e6e298.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5a668d3e08b3e9f2b7d9e0f965d9762.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fe4a6193adf67c2ee040715753a40d2d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e3f4f76cbd1c6492ba1fd985b02c9d05.jpg" align="middle"></details><br>​    <p></p><p>​    </p></ol></ol></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-01-24 Less Could Be Better  Parameter-efficient Fine-tuning Advances Medical   Vision Foundation Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
</feed>
