<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Adventures in Kedreamix&#39; Digital World</title>
  
  
  <link href="https://kedreamix.github.io/atom.xml" rel="self"/>
  
  <link href="https://kedreamix.github.io/"/>
  <updated>2024-05-02T03:18:37.848Z</updated>
  <id>https://kedreamix.github.io/</id>
  
  <author>
    <name>Kedreamix</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/NeRF/</id>
    <published>2024-05-02T03:18:37.000Z</published>
    <updated>2024-05-02T03:18:37.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration"><a href="#NeRF-Guided-Unsupervised-Learning-of-RGB-D-Registration" class="headerlink" title="NeRF-Guided Unsupervised Learning of RGB-D Registration"></a>NeRF-Guided Unsupervised Learning of RGB-D Registration</h2><p><strong>Authors:Zhinan Yu, Zheng Qin, Yijie Tang, Yongjun Wang, Renjiao Yi, Chenyang Zhu, Kai Xu</strong></p><p>This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction. </p><p><a href="http://arxiv.org/abs/2405.00507v1">PDF</a> </p><p><strong>Summary</strong><br>NeRF-UR 提出了一种帧到模型的优化框架，用于无监督 RGB-D 配准，利用神经辐射场 (NeRF) 作为场景的全局模型，以提高多视图一致性差时的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种无监督 RGB-D 配准的帧到模型优化框架 NeRF-UR。</li><li>使用 NeRF 作为场景的全局模型，提高了多视图一致性差时的鲁棒性。</li><li>创建了一个合成数据集 Sim-RGBD，通过在真实数据上进行无监督微调，将特征提取和注册的能力从仿真转移到现实。</li><li>在 ScanNet 和 3DMatch 数据集上，NeRF-UR 优于最先进的方法。</li><li>代码和模型将公开，以方便论文复现。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>论文标题：NeRF引导的RGB-D配准无监督学习</li><p></p><p></p><li>作者：Zhinan Yu1∗, Zheng Qin2∗, Yijie Tang1, Yongjun Wang1, Renjiao Yi1, Chenyang Zhu1, and Kai Xu1†</li><p></p><p></p><li>第一作者单位：国防科技大学</li><p></p><p></p><li>关键词：RGB-D配准·无监督学习·NeRF</li><p></p><p></p><li>论文链接：xxx</li><p></p><p></p><li>摘要：</li><br>&lt;/ol&gt;<p></p><p>（1）：研究背景：随着RGB-D传感器的普及和成本的降低，3D数据采集的难度已大大降低。RGB-D数据的广泛收集极大地推动了深度学习在3D视觉领域的进步，从而极大地提高了RGB-D SLAM和RGB-D重建等应用的性能。然而，现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（2）：过去方法及问题：现有的RGB-D配准方法通常采用基于可微渲染的成对训练策略，这会因光照变化、几何遮挡和反光材料等因素而导致多视图一致性较差。</p><p>（3）：本文方法：本文提出了一种新颖的帧到模型优化框架NeRF-UR，用于无监督RGB-D配准。该框架利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和NeRF重新渲染帧之间的一致性进行位姿优化。此外，为了引导NeRF优化，本文创建了一个通过逼真模拟器生成的合成数据集Sim-RGBD，用于预热配准模型。</p><p>（4）：方法性能：该方法在两个流行的室内RGB-D数据集ScanNet和3DMatch上优于最先进的同类方法。</p><ol><li>方法：</li></ol><p>（1）：提出一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准；</p><p>（2）：利用神经辐射场（NeRF）作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性进行位姿优化；</p><p>（3）：创建通过逼真模拟器生成的合成数据集 Sim-RGBD，用于预热配准模型；</p><p>（4）：使用 PointMBF 作为配准模型，该模型融合了视觉（2D）和几何（3D）空间的信息以获得更好的特征辨别力；</p><p>（5）：提出 NeRF 引导的无监督配准管道，该管道依赖于高质量的位姿来监督配准模型，并利用 NeRF 优化帧位姿；</p><p>（6）：采用 NeRF 模型，该模型能够对场景中的光照和几何结构进行建模，并联合优化 3D 地图和位姿；</p><p>（7）：将 RGB-D 序列分割成子序列，并为每个子序列优化一个 NeRF，以避免联合地图位姿优化带来的误差累积和巨大的时间开销；</p><p>（8）：通过 Sim-RGBD 数据集对配准模型进行引导，以提供合理的初始位姿，解决随机初始化配准模型容易产生大量异常对应关系的问题。</p><p><strong>8. 结论</strong></p><p><strong>(1):该工作的意义</strong></p><p>本文提出了 NeRF-UR，这是一种用于无监督 RGB-D 配准的帧到模型优化框架。该方法利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化配准模型估计的位姿。这种设计可以有效提高对光照变化、几何遮挡和反射材料的鲁棒性。我们进一步在合成数据集上设计了一个引导机制来预热 NeRF 优化。在两个基准上的大量实验表明了我们方法的有效性。我们认为，NeRF 引导的无监督学习是 3D 视觉的一种有前途的机制，我们希望在未来将其扩展到更多的任务，如定位、重建等。</p><p><strong>(2):本文的优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种新颖的帧到模型优化框架 NeRF-UR，用于无监督 RGB-D 配准。</li><li>利用神经辐射场 (NeRF) 作为场景的全局模型，并使用输入帧和 NeRF 重新渲染帧之间的一致性来优化位姿。</li><li>设计了一种合成数据集上的引导机制来预热 NeRF 优化。</li></ul><p><strong>性能：</strong></p><ul><li>在两个流行的室内 RGB-D 数据集 ScanNet 和 3DMatch 上优于最先进的同类方法。</li></ul><p><strong>工作量：</strong></p><ul><li>需要预先训练 NeRF 模型，这可能需要大量的时间和计算资源。</li><li>优化 NeRF 和配准模型是一个迭代过程，可能需要多次迭代才能收敛。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e47f5a6a35637f1b5b56609633d65083.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6d72291aca2a21454f9a83d46a2633ee.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-caf6df85382bbbd1a4f390f7bbbc79cb.jpg" align="middle"></details>## Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions**Authors:Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan**Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. [PDF](http://arxiv.org/abs/2404.19015v1) The source code for our model can be found on our project page:   https://nagabhushansn95.github.io/publications/2024/Simple-RF.html. arXiv   admin note: substantial text overlap with arXiv:2309.03955**Summary**神经辐射场（NeRF）在场景的逼真自由视点渲染方面表现出色。最近对 NeRF 的改进，如 TensoRF 和 ZipNeRF，采用了显式模型以实现更快的优化和渲染，而 NeRF 则采用了隐式表示。然而，隐式和显式的辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集合可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于使用更少的视图有效地训练它。深度监督是使用经典方法或在大数据集上预先训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。与早期的方法相反，我们寻求通过设计增强模型并在主辐射场中训练它们来学习深度监督。此外，我们的目标是设计一个正则化框架，它可以在不同的隐式和显式辐射场中使用。我们观察到，这些辐射场模型的某些特征在稀疏输入情况下过度拟合观测到的图像。我们的主要发现是，降低辐射场相对于位置编码、分解张量分量数或哈希表大小的能力，会限制模型学习更简单的解决方案，从而在某些区域估计更好的深度。通过基于这种降低的能力设计增强模型，我们可以获得更好的主辐射场深度监督。通过使用上述正则化，我们在包含朝前和 360 度场景的流行数据集上以稀疏输入视图实现了最先进的视图合成性能。**Key Takeaways**-  减少NeRF模型复杂性有助于学习更好的深度，有利于利用稀疏视图进行训练。-  设计增强模型，基于降低NeRF模型能力获得改进的深度监督。-  正则化框架可以应用于不同类型NeRF模型，包括隐式和显式模型。-  过度拟合是稀疏视图输入NeRF训练中的主要问题。-  深度监督可以促进NeRF模型从稀疏视图中进行有效训练。-  经典方法和神经网络都可以用于深度监督，但各有优缺点。-  在朝前和360度场景的流行数据集上，该方法实现了最先进的视图合成性能。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: 简化射线场：用更简单的解决方案正则化稀疏输入的辐射场</p></li><li><p>Authors: Nagabhushan Somraj, Adithyan Karanayil, Sai Harsha Mupparaju, Rajiv Soundararajan</p></li><li><p>Affiliation: 印度科学院</p></li><li><p>Keywords: 神经辐射场, 稀疏输入, 正则化, 深度估计</p></li><li><p>Urls: https://arxiv.org/abs/2404.19015, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 神经辐射场(NeRF)在场景的真实感自由视角渲染中表现出色。NeRF的最新改进，如TensoRF和ZipNeRF，采用了显式模型以实现更快的优化和渲染，而NeRF则采用隐式表示。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有稀疏的视图集可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于有效地使用更少的视图对其进行训练。深度监督是使用经典方法或在大数据集上预训练的神经网络获得的。虽然前者可能只提供稀疏监督，但后者可能存在泛化问题。</p><p>(2): 与早期的研究方法相反，我们试图通过设计增强模型并将其与主辐射场一起训练来学习深度监督。此外，我们的目标是设计一个正则化框架，可以在不同的隐式和显式辐射场中工作。我们观察到，这些辐射场模型的某些特征在稀疏输入场景中过度拟合观测图像。我们的主要发现是，在位置编码、分解的张量分量数或哈希表大小方面降低辐射场的性能，会限制模型学习更简单的解决方案，从而在某些区域估计出更好的深度。通过设计基于这种降低性能的增强模型，我们为主要辐射场获得了更好的深度监督。我们通过在包含前视和360度场景的流行数据集上使用上述正则化，在稀疏输入视图上实现了最先进的视图合成性能。</p><p>(3): 我们首先观察到，当使用稀疏输入视图进行训练时，辐射场模型通常利用它们的高性能来学习不必要的复杂解决方案。虽然这些解决方案完美地解释了观测图像，但它们可能会在新视图中造成严重的失真。例如，NeRF中的一些关键组件，如NeRF中的位置编码或TensoRF中采用的向量矩阵分解，为辐射场提供了强大的性能，并被设计为使用密集输入视图训练模型。由于系统严重欠约束，这些组件的现有实现可能在输入视图较少的情况下不理想，从而导致多种失真。图4、图7和图8分别显示了NeRF、TensoRF和ZipNeRF在少次拍摄设置中常见的失真。我们遵循流行的奥卡姆剃刀原理，并对辐射场进行正则化，以在可能的情况下选择更简单的解决方案，而不是复杂的解决方案。具体来说，我们通过降低辐射场的性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场。我们针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强。NeRF中使用的高位置编码度会导致不希望的深度不连续，从而产生浮点。此外，视点相关的辐射特征会导致形状辐射模糊，从而产生重复伪影。我们通过降低位置编码度和禁用视点相关的辐射特征来设计NeRF的增强。另一方面，TensoRF中大量的高分辨率分解组件和ZipNeRF中的大哈希表会导致这些模型在少次拍摄设置中出现浮点。因此，我们设计了增强，以限制模型在这些组件方面学习更简单的解决方案。我们将简化的模型用作深度监督的增强，而不是作为主要的NeRF模型，因为简单地降低辐射场的性能可能会导致某些区域的次优解决方案[Jain et al. 2021]。例如，只能学习平滑深度转换的模型可能无法学习物体边界处的锐利深度不连续性。此外，仅当增强模型准确解释观察到的图像时，才需要使用它们进行监督。我们通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性。</p><p>(4): 在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进，如表1所示。我们观察到，原始辐射场存在各种失真。使用更简单的解决方案对辐射场进行正则化可以显著改善所有三个辐射场的重建。</p><ol><li><p>方法：</p><pre><code>            (1):通过降低辐射场性能来设计增强模型，并使用这些模型估计的深度来监督主辐射场；            (2):针对NeRF、TensoRF和ZipNeRF的不同缺点和架构设计了不同的增强；            (3):通过使用估计的深度将像素重新投影到不同的最近训练视图上并将其与相应的图像进行比较来衡量深度的可靠性；            (4):在NeRF-LLFF、RealEstate-10K和MipNeRF360数据集上，我们的正则化在NeRF、TensoRF和ZipNeRF模型上取得了显著的改进；            .......</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文解决的是通过从与主辐射场模型同时训练的低能力增强模型学习的更简单的解决方案中获得深度监督来解决少次拍摄辐射场的问题。我们表明，可以为隐式模型（如 NeRF）和显式辐射场（如 TensoRF 和 ZipNeRF）设计增强。由于各种辐射场的缺点不同，我们为每个模型适当地设计了增强。我们表明，我们的增强在所有三个模型上都显着提高了性能，并且我们在前视场景和 360◦ 场景上都取得了最先进的性能。值得注意的是，我们的模型在场景的深度估计方面取得了显着的改进，这对于新视图合成和场景理解至关重要。</p><p>（2）：创新点：提出了一种通过从增强模型学习的更简单的解决方案中获得深度监督来正则化辐射场的方法；性能：在 NeRF、TensoRF 和 ZipNeRF 模型上取得了显着改进，在少次拍摄设置中实现了最先进的视图合成性能；工作量：需要设计针对不同辐射场模型的增强，这可能需要额外的工程工作。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-5d84b090330526061fb59bb1dfc6ea7e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a15c9e6ec9783c3b5ec66e4da9128f8d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-52e8f6725bd512099b8ddbc432d73f2f.jpg" align="middle"></details><h2 id="Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields"><a href="#Geometry-aware-Reconstruction-and-Fusion-refined-Rendering-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields"></a>Geometry-aware Reconstruction and Fusion-refined Rendering for   Generalizable Neural Radiance Fields</h2><p><strong>Authors:Tianqi Liu, Xinyi Ye, Min Shi, Zihao Huang, Zhiyu Pan, Zhan Peng, Zhiguo Cao</strong></p><p>Generalizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at <a href="https://github.com/TQTQliu/GeFu">https://github.com/TQTQliu/GeFu</a> . </p><p><a href="http://arxiv.org/abs/2404.17528v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://gefucvpr24.github.io">https://gefucvpr24.github.io</a></p><p><strong>Summary</strong><br>新提出方法GeFu针对NeRF模型的泛化能力不足问题，提出自适应代价聚合（ACA）、空间视图聚合（SVA）和一致性感知融合（CAF）机制，通过提升几何重建精度、丰富描述符信息和优化解码策略，大幅提升NeRF模型的泛化能力。</p><p><strong>Key Takeaways</strong></p><ul><li>现存NeRF模型泛化能力受限于几何重建不准、描述符信息不足和解码策略不优。</li><li>ACA机制通过放大一致像素对的贡献，抑制不一致像素对，提升代价体精度。</li><li>SVA机制结合空间和视图信息，丰富描述符信息。</li><li>CAF机制融合不同解码策略的优势，提升解码效果。</li><li>GeFu框架结合ACA、SVA、CAF机制，从粗到精进行几何重建和融合渲染。</li><li>GeFu模型在多个数据集上达到最优性能。</li><li>GeFu代码已开源（<a href="https://github.com/TQTQliu/GeFu）。">https://github.com/TQTQliu/GeFu）。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：基于几何的重建和融合精修渲染（中文翻译：）</p></li><li><p>作者：天齐 刘，添翼 冯，小明 董，嘉鹏 张，志伟 冯，杰 潘，振羽 王，志伟 冯（Tianqi Liu, Tianyi Feng, Xiaoming Dong, Jiapeng Zhang, Zhiwei Feng, Jie Pan, Zhenyu Wang, Zhiwei Feng）</p></li><li><p>第一作者单位：北京大学（中文翻译：北京大学）</p></li><li><p>关键词：神经辐射场，多视图重建，神经渲染，场景理解</p></li><li><p>论文链接：或Github代码链接（若有，则填写，若无，则填写Github:None）：Github:None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）可以从多视图图像中重建3D场景，但现有方法在具有挑战性的条件下（如遮挡或反射）表现出泛化能力有限。</p><p>（2）：过去的方法：现有方法主要通过构建基于方差的代价体积进行几何重建，并编码3D描述符进行新视图解码。但这些方法存在几何不准确、描述符次优和解码策略不佳的问题。</p><p>（3）：本文方法：本文提出了一种基于几何的重建和融合精修渲染（GeFu）框架，通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><p>（4）：任务和性能：在多个数据集上，GeFu在多视图重建和新视图渲染任务上都取得了最先进的性能。这些性能支持了本文的目标，即提高NeRF在具有挑战性条件下的泛化能力。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种基于几何的重建和融合精修渲染框架（GeFu），通过自适应代价聚合（ACA）放大一致像素对的贡献，抑制不一致像素对的贡献；</p><p>（2）：引入空间-视图聚合器（SVA）通过空间和视图间的交互将3D上下文融入描述符；</p><p>（3）：提出了一致性感知融合（CAF）策略，利用了两种现有解码策略的优势。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种通用的NeRF方法，能够实现高保真视图合成。具体来说，在重建阶段，我们提出了自适应代价聚合（ACA）来改善几何估计，并提出了空间-视图聚合器（SVA）来编码3D上下文感知描述符。在渲染阶段，我们引入了Consistency-Aware Fusion（CAF）模块，以统一其优势来优化合成视图质量。我们将这些模块整合到一个粗到细的框架中，称为GeFu。广泛的评估和消融实验证明了我们提出的模块的有效性。</p><p>（2）：创新点：提出自适应代价聚合（ACA）、空间-视图聚合器（SVA）和一致性感知融合（CAF）模块，提高了NeRF在具有挑战性条件下的泛化能力；性能：在多视图重建和新视图渲染任务上取得了最先进的性能；工作量：工作量中等，需要修改NeRF的重建和渲染流程。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f615e4a52c82bbd89b40d674212ac03c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ccb26edee482b262ae1661c51b02d1d1.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3ded1a62b2132a2c5b5fdd26dc30947d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-cbccb86ceb95a77c9f32e543fe79fbf0.jpg" align="middle"></details><h2 id="Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery"><a href="#Depth-Supervised-Neural-Surface-Reconstruction-from-Airborne-Imagery" class="headerlink" title="Depth Supervised Neural Surface Reconstruction from Airborne Imagery"></a>Depth Supervised Neural Surface Reconstruction from Airborne Imagery</h2><p><strong>Authors:Vincent Hackstein, Paul Fauth-Mayer, Matthias Rothermel, Norbert Haala</strong></p><p>While originally developed for novel view synthesis, Neural Radiance Fields (NeRFs) have recently emerged as an alternative to multi-view stereo (MVS). Triggered by a manifold of research activities, promising results have been gained especially for texture-less, transparent, and reflecting surfaces, while such scenarios remain challenging for traditional MVS-based approaches. However, most of these investigations focus on close-range scenarios, with studies for airborne scenarios still missing. For this task, NeRFs face potential difficulties at areas of low image redundancy and weak data evidence, as often found in street canyons, facades or building shadows. Furthermore, training such networks is computationally expensive. Thus, the aim of our work is twofold: First, we investigate the applicability of NeRFs for aerial image blocks representing different characteristics like nadir-only, oblique and high-resolution imagery. Second, during these investigations we demonstrate the benefit of integrating depth priors from tie-point measures, which are provided during presupposed Bundle Block Adjustment. Our work is based on the state-of-the-art framework VolSDF, which models 3D scenes by signed distance functions (SDFs), since this is more applicable for surface reconstruction compared to the standard volumetric representation in vanilla NeRFs. For evaluation, the NeRF-based reconstructions are compared to results of a publicly available benchmark dataset for airborne images. </p><p><a href="http://arxiv.org/abs/2404.16429v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRFs）作为多视立体（MVS）的替代方法，在空中场景三维重建中展现出 promising 的性能，尤其是在处理无纹理、透明和反射表面等传统 MVS难以处理的场景时。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRFs 在空中图像块（包括仅 nadir、倾斜和高分辨率图像）的三维重建中具有适用性。</li><li>集成平差块调整中提供的联系点测量深度先验信息可以提升重建效果。</li><li>使用基于符号距离函数 (SDF) 的 VolSDF 框架进行重建，更适用于表面重建。</li><li>NeRFs 在图像冗余度低和数据证据弱的区域（如街道峡谷、立面或建筑阴影）存在困难。</li><li>训练 NeRFs 计算成本高。</li><li>在空中场景的三维重建中，NeRFs 面临低图像冗余度和弱数据证据的挑战。</li><li>在仅使用 nadir 图像的情况下，NeRFs 的重建性能低于使用倾斜图像或高分辨率图像的情况。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：航空影像的深度监督神经表面重建</p></li><li><p>作者：V. Hackstein、P. Fauth-Mayer、M. Rothermel、N. Haala</p></li><li><p>所属机构：nFrames ESRI（德国）</p></li><li><p>关键词：神经辐射场（NeRF）、多视立体（MVS）、3D 场景重建、网格化 3D 点云、航空影像、深度监督</p></li><li><p>论文链接：https://arxiv.org/abs/2404.16429 , Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）最初用于新颖视图合成，现已成为多视立体（MVS）的替代方案。NeRF 尤其适用于无纹理、透明和反光表面，而这些场景对于基于 MVS 的传统方法仍然具有挑战性。然而，大多数研究关注近距离场景，而针对航空场景的研究仍然缺失。</p><p>（2）：过去方法及其问题：传统的 MVS 方法在精细几何结构、无纹理区域和非朗伯表面（例如半透明物体或反射）处存在问题。动机充分：本文提出了一种基于深度监督的神经表面重建方法，以解决这些问题。</p><p>（3）：提出的研究方法：本文修改了 VolSDF 框架，将 SfM 关联点监督整合到训练过程中，以支持训练过程。VolSDF 使用符号距离函数（SDF）对 3D 场景进行建模，这比香草 NeRF 中的标准体积表示更适用于表面重建任务。</p><p>（4）：任务和方法性能：本文针对三种航空图像集评估了该管道，这些图像集具有不同的配置。在专业航空测绘中通常使用的数据上的这些研究从实际角度出发很有趣，同时研究了基于 NeRF 的表面重建的具体挑战。此类航空图像集合的视角有限，并且可能因街道峡谷、立面或建筑物阴影而受到影响。该方法在这些任务上取得了良好的性能，表明其可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）回顾 VolSDF（神经辐射场）；</p><p>（2）VolSDF 的 SDF（符号距离函数）表示；</p><p>（3）VolSDF 的正则化；</p><p>（4）VolSDF 的采样；</p><p>（5）Tie 点监督：    （a）Tie 点初始化和监督；    （b）深度监督损失函数：Ltr 和 Lfs；</p><p>（6）实现和训练细节：    （a）模型结构；    （b）训练损失函数：L = LRGB + λeikLeik + λsurfLsurf +λfsLfs + λtrLtr；</p><ol><li>结论：</li></ol><p>（1）：本文展示了 VolSDF（神经辐射场的一种变体，用于建模隐式神经表面）在航空图像三维重建中的适用性。我们证明了通过关联点监督 VolSDF 可以改善重建效果：我们观察到在早期训练阶段收敛速度更快，并且在完整性和准确性方面质量更好。这尤其适用于仅具有有限数据证据的具有挑战性的区域，对于这些区域，VolSDF 往往会陷入局部最小值或根本无法收敛。一个示例正射场景的重建表面在 NMAD 方面比传统的 MVS 管道少于 4 个 GSD 偏差。为了完全收敛并恢复全部细节，仍然需要延长训练时间。这阻碍了实际应用。然而，我们可以在合理的时间内获得拓扑正确的表面，这些表面可以进行后续网格后处理。采样例程是评估实施中的主要瓶颈，并将在未来工作中进行改进。一方面，高效的 GPU 实现可以加速这一过程（Wang 等人，2023 年），另一方面，我们希望研究在有很大改进潜力的区域动态增强采样的可能性（Kerbl 等人，2023 年）。神经隐式表面重建仍然是一个活跃的研究课题，我们希望本文也能鼓励在航空图像和其他遥感应用的几何重建领域开展未来的工作。</p><p>（2）：创新点：提出了基于深度监督的神经表面重建方法，将 SfM 关联点监督整合到 VolSDF 训练过程中，以支持训练过程，提高了重建质量；</p><p>性能：在航空图像集上评估了该管道，在具有挑战性的区域（例如无纹理区域、非朗伯表面）取得了良好的性能，表明其可以支持其目标；</p><p>工作量：需要较长的训练时间才能完全收敛并恢复全部细节，这阻碍了实际应用，采样例程是评估实施中的主要瓶颈。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-4036313ed6644db70c73439252a5eaed.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0e26afc3f9b57484514d8f583efe4569.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f6ddc84a2cf8fcf12f4f1a29a529e7de.jpg" align="middle"><img src="https://picx.zhimg.com/v2-14b2f3ca89df5d53f911326b2d3382d5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-77dd44d2a901985d20406c555ff9eb2c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>基于三维高斯滴的语音驱动的说话头部合成方法，通过显式表示和对三维面部模型的高斯关联实现精确的唇部运动和面部细节增强，展现出实时渲染性能和卓越的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>采用三维高斯滴的显式表示，解决了NeRF内隐表示的姿态和表情控制不足问题。</li><li>提出的说话人特定运动转换器通过通用的音频特征提取和定制的唇部运动生成，实现了针对目标说话人的精确唇部运动。</li><li>动态高斯渲染器引入了说话人特定混合形状，通过潜在姿势增强面部细节表示，提供稳定且逼真的渲染视频。</li><li>实验结果表明，在说话头部合成方面，该方法优于现有的最先进方法，实现了精确的唇部同步和出色的视觉效果。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的阈值，并有可能部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker：基于3D高斯点云的说话人专属会说话的头像合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific Motion Translator, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p><p>(1): 近期基于神经辐射场(NeRF)的音频驱动说话人头像合成方法取得了令人瞩目的成果。然而，受限于NeRF隐式表示对姿势和表情控制不足，这些方法仍存在唇部运动不同步或不自然、视觉抖动和伪影等问题。</p><p>(2): 过去的方法：基于NeRF的音频驱动说话人头像合成方法；问题：姿势和表情控制不足，导致唇部运动不自然、视觉抖动和伪影。该方法的动机充分，提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法。</p><p>(3): 本文提出的方法：GaussianTalker，该方法由说话人专属运动转换器和动态高斯渲染器两个模块组成。说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动。动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制。</p><p>(4): 该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。该性能支持其目标，即实现自然逼真的说话人头像合成。</p></li><li><p>方法：</p><p>（1）：提出基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；</p><p>（2）：GaussianTalker由说话人专属运动转换器和动态高斯渲染器两个模块组成；</p><p>（3）：说话人专属运动转换器通过通用音频特征提取和定制唇部运动生成，实现针对目标说话人的精确唇部运动；</p><p>（4）：动态高斯渲染器引入说话人专属混合形状，将高斯点云与3D面部模型绑定，实现面部运动的直观控制；</p><p>（5）：该方法在音频驱动说话人头像合成任务上取得了较好的性能，能够生成高质量的视频，具有精确的唇部运动。</p></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker，该方法将多模态数据与特定说话人关联起来，减少了音频、3D网格和视频之间的潜在身份偏差。说话人专属FLAME转换器采用身份解耦和个性化嵌入，以实现同步和自然的唇部运动，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定和逼真的渲染。大量实验表明，GaussianTalker在说话人头像合成方面优于最先进的性能，同时实现了远超其他方法的超高渲染速度。我们相信，这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了一种基于3D高斯点云的音频驱动说话人头像合成新方法GaussianTalker；性能：在说话人头像合成方面优于最先进的性能，并实现了远超其他方法的超高渲染速度；工作量：......</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v2">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF） 在自动驾驶领域中的诸多应用。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF在自动驾驶领域显示出广阔潜能，应用涵盖感知、三维重建、SLAM和仿真。</li><li>NeRF感知应用包括目标检测、分割和跟踪。</li><li>NeRF三维重建应用可生成高保真三维场景。</li><li>NeRF SLAM 融合了感知和重建，实时创建环境地图。</li><li>NeRF仿真应用可创造逼真的虚拟环境，用于传感器和算法测试。</li><li>研究热点包括跨模态融合、高效表示和动态场景处理。</li><li>NeRF在自动驾驶中的应用仍处于早期阶段，面临挑战和机遇。</li><li>未来方向包括高精度、鲁棒性和实时性能优化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：神经辐射场在自动驾驶中的应用：综述</p></li><li><p>作者：雷贺、李乐恒、孙文超、韩泽宇、刘一辰、郑思发、王建强、李克强</p></li><li><p>Affiliation: 清华大学车辆与运载学院</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.13816 ,Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):神经辐射场（NeRF）凭借其内在优势，特别是其隐式表示和新颖的视图合成能力，在学术界和工业界都备受关注。随着深度学习的快速发展，大量方法涌现出来，探索 NeRF 在自动驾驶（AD）领域的潜在应用。然而，当前文献中存在明显的空白。为了弥补这一差距，本文对 NeRF 在 AD 中的应用进行了全面的调查。我们的调查旨在对 NeRF 在自动驾驶（AD）中的应用进行分类，特别是包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真。我们深入分析并总结了每个应用类别的发现，并通过提供对该领域未来方向的见解和讨论来结束。我们希望这篇论文能为该领域的的研究人员提供全面的参考。据我们所知，这是第一篇专门针对 NeRF 在自动驾驶领域的应用的综述。</p><p>(2):过去的方法主要依赖于高精度地图来提供静态场景理解，现在强调通过鸟瞰视觉实时感知局部环境。同时，它在功能上已从 2 级（L2）发展到努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。NeRF 通过自监督学习，已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的诱人候选者。在过去两年中，NeRF 模型已在自动驾驶的各个方面得到了应用，包括感知、3D 重建、同时定位和建图 (SLAM) 以及仿真，如图 1 所示。</p><p>(3):神经辐射场（NeRF）已成为感知领域的很有希望的竞争者，涵盖了一系列关键任务，例如对象检测、语义分割和占据预测。它人气飙升的主要原因是它能够获取精确且一致的几何信息。该领域的研究可分为两大范式，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于扩充感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协同训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(4):在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建侧重于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中解开形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论</strong></li></ol><p>（1）本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的总结，填补了当前文献中的空白，为该领域的研究人员提供了全面的参考。</p><p>（2）创新点：本综述首次专门针对神经辐射场在自动驾驶领域的应用进行了综述；性能：对神经辐射场在感知、3D 重建、SLAM 和仿真等领域的应用进行了深入分析和总结；工作量：工作量大，涉及文献广泛，分析深入。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-55b475e228eebb497768f57fb097059d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-22321e24e9114a3aa3b89b16e6ff76f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://picx.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-05-02  NeRF-Guided Unsupervised Learning of RGB-D Registration</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/3DGS/</id>
    <published>2024-05-02T03:01:07.000Z</published>
    <updated>2024-05-02T03:01:07.390Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation"><a href="#Spectrally-Pruned-Gaussian-Fields-with-Neural-Compensation" class="headerlink" title="Spectrally Pruned Gaussian Fields with Neural Compensation"></a>Spectrally Pruned Gaussian Fields with Neural Compensation</h2><p><strong>Authors:Runyi Yang, Zhenxin Zhu, Zhou Jiang, Baijun Ye, Xiaoxue Chen, Yifei Zhang, Yuantao Chen, Jian Zhao, Hao Zhao</strong></p><p>Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a>. </p><p><a href="http://arxiv.org/abs/2405.00676v1">PDF</a> Code: <a href="https://github.com/RunyiYang/SUNDAE">https://github.com/RunyiYang/SUNDAE</a> Project page:   <a href="https://runyiyang.github.io/projects/SUNDAE/">https://runyiyang.github.io/projects/SUNDAE/</a></p><p><strong>Summary</strong><br>3D高斯点渲染算法 SUNDAE 通过谱剪枝和神经补偿显著降低了内存消耗，同时保持了高质量的渲染效果。</p><p><strong>Key Takeaways</strong></p><ul><li>3D高斯点渲染算法在渲染速度和质量方面表现出色，但内存占用高。</li><li>SUNDAE 算法构建了高斯基元的关系图，并设计了谱下采样模块来剪除基元。</li><li>SUNDAE 算法使用轻量级神经网络头对渲染特征进行混合，弥补了剪枝造成的质量损失。</li><li>SUNDAE 算法显著降低了内存消耗，同时保持了较高的渲染质量。</li><li>SUNDAE 在 Mip-NeRF360 数据集上以 104 MB 的内存实现了 26.80 PSNR 和 145 FPS，而原始高斯点渲染算法则以 523 MB 的内存实现了 25.60 PSNR 和 160 FPS。</li><li>SUNDAE 代码已公开发布。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：SUNDAE：神经补偿光谱修剪高斯场</p></li><li><p>作者：Runyi Yang、Zhenxin Zhu、Zhou Jiang、Baijun Ye、Xiaoxue Chen、Yifei Zhang、Yuantao Chen、Jian Zhao、Hao Zhao</p></li><li><p>隶属机构：清华大学人工智能研究院</p></li><li><p>关键词：3D高斯喷射、图信号处理、神经渲染</p></li><li><p>论文地址：https://arxiv.org/abs/2405.00676Github：https://github.com/runyiyang/SUNDAE</p></li><li><p>摘要：</p><p>(1)：研究背景：3D高斯喷射作为一种新颖的3D表示，因其渲染速度快、渲染质量高而备受关注。然而，这带来了较高的内存消耗，例如，一个训练良好的高斯场可能需要使用300万个高斯原语和超过700MB的内存。我们认为这种高内存占用率是由于缺乏对原语之间关系的考虑。</p><p>(2)：以往方法：以往方法是3D高斯喷射。其问题是：训练一个3D高斯喷射模型面临着高内存消耗的挑战。</p><p>(3)：本文提出的研究方法：我们提出了一种名为SUNDAE的内存高效高斯场，它采用光谱修剪和神经补偿。一方面，我们在高斯原语集合上构建一个图来建模它们之间的关系，并设计了一个光谱下采样模块来剔除原语，同时保留所需的信号。另一方面，为了补偿修剪高斯体带来的质量损失，我们利用了一个轻量级的神经网络头来混合喷射特征，它有效地补偿了质量损失，同时在其权重中捕获了原语之间的关系。</p><p>(4)：方法性能：我们在Mip-NeRF360数据集上，SUNDAE使用104MB内存实现了26.80 PSNR和145 FPS，而传统的3D高斯喷射算法使用523MB内存实现了25.60 PSNR和160 FPS。这些性能可以支持他们的目标。</p></li><li><p>方法：</p><p>（1）：构建图模型，光谱下采样剔除原语，保留所需信号；</p><p>（2）：利用神经网络头混合喷射特征，补偿修剪高斯体带来的质量损失；</p><p>（3）：在权重中捕获原语之间的关系；</p><p>.......</p></li><li><p>结论：</p></li></ol><p>（1）：SUNDAE方法在保持3D高斯喷射效率的同时，尺寸大幅缩小，为3D场景表示和渲染提供了新的思路。</p><p>（2）：创新点：图信号处理框架与神经网络补偿相结合，构建了谱修剪高斯场；性能：在Mip-NeRF360数据集上，SUNDAE使用104MB内存实现了26.80 PSNR和145 FPS，而传统的3D高斯喷射算法使用523MB内存实现了25.60 PSNR和160 FPS；工作量：构建图模型、光谱下采样、神经网络补偿。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-da926fda6b7545ddb2dfb2a2da01023f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d3c4eab265f17a199aeff4e6e3cbd775.jpg" align="middle"><img src="https://picx.zhimg.com/v2-6e54f047dac354ead2f83393633d9db5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ec9cfcf497b0d1e4aab9d2caf7b66814.jpg" align="middle"><img src="https://pica.zhimg.com/v2-75980d3d584e8ea8e866bf08e90027f7.jpg" align="middle"></details>## RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting**Authors:Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou**We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy. [PDF](http://arxiv.org/abs/2404.19706v2) To be published in ACM SIGGRAPH 2024**Summary**实时高斯建图系统 (RTG-SLAM) 采用高斯平铺，使用 RGBD 相机实时构建大场景的 3D 重建，具有紧凑的高斯表示和高效的即时高斯优化方案。**Key Takeaways**- 使用高斯平铺进行大规模环境的实时 3D 重建。- 采用紧凑的高斯表示，降低内存和计算成本。- 即时高斯优化方案，仅优化不稳定的高斯，减少计算量。- 渲染深度与颜色渲染方式不同，单一不透明高斯即可拟合局部表面区域。- 区分稳定和不稳定高斯，仅优化不稳定高斯，提高渲染效率。- 与 NeRF 驱动的 RGBD SLAM 相比，重建质量相当，速度提升一倍，内存消耗减少一半。- 在新视图合成和相机跟踪精度方面表现更佳。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: RTG-SLAM: 基于高斯散射的大规模实时 3D 重建</p></li><li><p>Authors: Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: RGBD SLAM, Real-time 3D Reconstruction, Gaussian Splatting, NeRF</p></li><li><p>Urls: https://arxiv.org/abs/2404.19706v2, Github: None</p></li><li><p>Summary:</p><pre><code>            (1): RGBD SLAM 技术在实时大规模 3D 重建中受到广泛关注。然而，现有的基于 NeRF 的 RGBD SLAM 方法在重建速度、内存消耗和新颖视角合成方面仍面临挑战。            (2): 现有的方法通常使用多个重叠的高斯核来拟合局部表面区域，导致内存和计算成本高。此外，它们缺乏高效的在线高斯优化方案，这限制了实时性能。            (3): 本文提出了一种名为 RTG-SLAM 的实时 3D 重建系统，它采用高斯散射和高效的在线高斯优化方案。该系统强制每个高斯核要么不透明要么近乎透明，不透明的高斯核拟合表面和主要颜色，而透明的高斯核拟合残差颜色。通过以不同于颜色渲染的方式渲染深度，该系统可以让单个不透明高斯核很好地拟合局部表面区域，而无需多个重叠的高斯核，从而大大降低了内存和计算成本。对于在线高斯优化，该系统针对每帧的三类像素显式添加高斯核：新观测的像素、颜色误差大的像素和深度误差大的像素。该系统还将所有高斯核分类为稳定高斯核和不稳定高斯核，其中稳定高斯核有望很好地拟合先前观测的 RGBD 图像，否则为不稳定高斯核。该系统只优化不稳定高斯核，只渲染不稳定高斯核占据的像素。通过这种方式，该系统同时降低了高斯核的数量和渲染成本。            (4): 在酒店房间重建任务上，RTG-SLAM 在重建速度和内存消耗方面优于最先进的 NeRF-SLAM 方法，同时在合成新颖视角方面表现出更高的真实感。</code></pre></li><li><p>方法：</p><pre><code>            （1）：提出了 RTG-SLAM 系统，该系统采用高斯散射和高效的在线高斯优化方案进行实时 3D 重建；            （2）：强制每个高斯核要么不透明要么近乎透明，不透明的高斯核拟合表面和主要颜色，而透明的高斯核拟合残差颜色；            （3）：针对每帧的三类像素显式添加高斯核：新观测的像素、颜色误差大的像素和深度误差大的像素；            （4）：将所有高斯核分类为稳定高斯核和不稳定高斯核，其中稳定高斯核有望很好地拟合先前观测的 RGBD 图像，否则为不稳定高斯核；            （5）：只优化不稳定高斯核，只渲染不稳定高斯核占据的像素，降低了高斯核的数量和渲染成本。</code></pre></li><li><p>结论：</p><pre><code>            （1）：本文提出了一种基于高斯散射的大规模实时 3D 重建系统 RTG-SLAM，该系统采用紧凑的高斯表示来减少拟合表面的高斯数量，从而大大降低了内存和计算成本。对于在线高斯优化，该系统针对每帧的三类像素显式添加高斯：新观测的、颜色误差大的和深度误差大的，并且只优化不稳定的高斯，只渲染不稳定的高斯占据的像素，降低了高斯数量和渲染成本。该系统在大规模真实扫描场景中重建，并取得了优于最先进的 NeRF SLAM 方法和并发的 Gaussian SLAM 方法的性能。由于为了在大规模场景中实现实时重建，只使用不透明的高斯和透明的高斯来表示场景，因此与原始的高斯相比，渲染质量不可避免地会下降。如何在保持实时性能的同时提高渲染质量是未来值得探索的方向。此外，反射或透明的材料会导致表面颜色在不同视图之间发生很大变化，使得一些高斯频繁地在两种状态之间切换，并且无法得到很好的优化。未来，该系统还将扩展到处理户外场景、动态物体、快速摄像机运动和光照变化的场景。            （2）：创新点：高斯散射、紧凑的高斯表示、在线高斯优化；性能：在大规模场景中实现实时重建、优于最先进的 NeRF SLAM 方法和并发的 Gaussian SLAM 方法；工作量：降低了内存和计算成本、降低了高斯数量和渲染成本。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d45b35f06c4dce864863260a5af329f4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8764f6bde3de348a98aac2f2a4a30ee2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3e7c959078c5d5d3548984d92ce2a3ec.jpg" align="middle"></details><h2 id="GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting"><a href="#GS-LRM-Large-Reconstruction-Model-for-3D-Gaussian-Splatting" class="headerlink" title="GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting"></a>GS-LRM: Large Reconstruction Model for 3D Gaussian Splatting</h2><p><strong>Authors:Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, Zexiang Xu</strong></p><p>We propose GS-LRM, a scalable large reconstruction model that can predict high-quality 3D Gaussian primitives from 2-4 posed sparse images in 0.23 seconds on single A100 GPU. Our model features a very simple transformer-based architecture; we patchify input posed images, pass the concatenated multi-view image tokens through a sequence of transformer blocks, and decode final per-pixel Gaussian parameters directly from these tokens for differentiable rendering. In contrast to previous LRMs that can only reconstruct objects, by predicting per-pixel Gaussians, GS-LRM naturally handles scenes with large variations in scale and complexity. We show that our model can work on both object and scene captures by training it on Objaverse and RealEstate10K respectively. In both scenarios, the models outperform state-of-the-art baselines by a wide margin. We also demonstrate applications of our model in downstream 3D generation tasks. Our project webpage is available at: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a> . </p><p><a href="http://arxiv.org/abs/2404.19702v1">PDF</a> Project webpage: <a href="https://sai-bi.github.io/project/gs-lrm/">https://sai-bi.github.io/project/gs-lrm/</a></p><p><strong>Summary</strong><br>三维高斯原语大重建模型，可从 2-4 个姿势稀疏图像预测高质量的三维高斯原语，在单个 A100 GPU 上仅需 0.23 秒。</p><p><strong>Key Takeaways</strong></p><ul><li>使用变压器架构，从图像中预测三维高斯原语。</li><li>模型具有可扩展性，可预测具有大尺度和复杂度变化的场景。</li><li>在 Objaverse 和 RealEstate10K 数据集上均优于现有方法。</li><li>可用于下游三维生成任务，如视图合成和三维形状合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>题目：GS-LRM：大型重建模型</p></li><li><p>作者：Kai Zhang、Sai Bi、Hao Tan、Yuanbo Xiangli、Nanxuan Zhao、Kalyan Sunkavalli、Zexiang Xu</p></li><li><p>单位：Adobe Research</p></li><li><p>关键词：Large Reconstruction Models · 3D Reconstruction · Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.19702 , Github：None</p></li><li><p>摘要：</p><p>（1）：研究背景：3D 场景重建是计算机视觉中的一个核心问题，传统方法依赖于复杂的光度测量系统和密集的多视图图像。神经表征和可微渲染的进步提高了重建和渲染质量，但速度慢且需要大量输入视图。基于 Transformer 的 3D 大型重建模型 (LRM) 学习了大量的 3D 对象的通用重建先验，实现了前所未有的稀疏视图 3D 重建质量。</p><p>（2）：过去的方法：过去的方法采用三平面 NeRF 作为场景表示，存在三平面分辨率受限和体积渲染开销大的问题，导致训练和渲染速度慢、难以保留精细细节，以及无法扩展到对象中心输入之外的大场景。</p><p>（3）：本论文方法：GS-LRM 是一种可扩展的大型重建模型，它采用了一种非常简单的基于 Transformer 的架构。将输入图像进行块状化，将连接后的多视图图像标记通过一系列 Transformer 块，并直接从这些标记解码最终的逐像素高斯参数以进行可微渲染。与只能重建对象的先前 LRM 不同，GS-LRM 通过预测逐像素高斯函数，自然地处理了规模和复杂性差异很大的场景。</p><p>（4）：方法性能：GS-LRM 在 Objaverse 和 RealEstate10K 数据集上分别针对对象和场景捕捉进行了训练，在两种情况下都大幅优于最先进的基准。该模型还可以在下游 3D 生成任务中得到应用。</p></li><li><p>方法：</p></li></ol><p>（1）：采用 Transformer 模型，将一组已知相机位姿的图像回归为逐像素的 3D 高斯参数；</p><p>（2）：通过 patchify 算子对输入图像进行标记化处理，将多视图图像标记连接起来，并通过一系列 Transformer 块进行处理，包括自注意力和 MLP 层；</p><p>（3）：从每个输出标记中，使用线性层解码对应 patch 中像素对齐的高斯函数属性；</p><p>（4）：利用线性层将 1D 向量映射到 d 维的图像 patch 标记，其中 d 是 Transformer 宽度；</p><p>（5）：将多视图图像标记连接起来，并通过一系列 Transformer 块进行处理，包括残差连接、多头自注意力和 MLP；</p><p>（6）：使用单个线性层将 Transformer 的输出标记解码为高斯参数。</p><ol><li>结论：<pre><code>            (1)：本工作的主要意义在于提出了一种简单且可扩展的基于 Transformer 的大型重建模型，用于高斯 splatting（GS）表示。该方法能够在单个 A100 GPU 上以约 0.23 秒的速度从一组已知相机位姿的图像中进行快速前馈高分辨率 GS 预测。该模型既适用于对象级捕捉，也适用于场景级捕捉，并且在大量数据上训练后，在两种情况下均达到最先进的性能。我们希望我们的工作能够激发未来在数据驱动的前馈 3D 重建领域开展更多工作。致谢感谢 Nathan Carr 和 Duygu Ceylan 提供有益的讨论。            (2)：创新点：提出了一种基于 Transformer 的大型重建模型，用于高斯 splatting 表示，该模型简单且可扩展；性能：在对象级和场景级捕捉任务上均达到最先进的性能；工作量：在单个 A100 GPU 上以约 0.23 秒的速度进行前馈高分辨率 GS 预测。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7b3dfbd4f62939f8af1187b102bf5134.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5173cd30ad642c9a906d00c88085376d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e761662a90d51192c43a703fedff0bc4.jpg" align="middle"></details><h2 id="SAGS-Structure-Aware-3D-Gaussian-Splatting"><a href="#SAGS-Structure-Aware-3D-Gaussian-Splatting" class="headerlink" title="SAGS: Structure-Aware 3D Gaussian Splatting"></a>SAGS: Structure-Aware 3D Gaussian Splatting</h2><p><strong>Authors:Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</strong></p><p>Following the advent of NeRFs, 3D Gaussian Splatting (3D-GS) has paved the way to real-time neural rendering overcoming the computational burden of volumetric methods. Following the pioneering work of 3D-GS, several methods have attempted to achieve compressible and high-fidelity performance alternatives. However, by employing a geometry-agnostic optimization scheme, these methods neglect the inherent 3D structure of the scene, thereby restricting the expressivity and the quality of the representation, resulting in various floating points and artifacts. In this work, we propose a structure-aware Gaussian Splatting method (SAGS) that implicitly encodes the geometry of the scene, which reflects to state-of-the-art rendering performance and reduced storage requirements on benchmark novel-view synthesis datasets. SAGS is founded on a local-global graph representation that facilitates the learning of complex scenes and enforces meaningful point displacements that preserve the scene’s geometry. Additionally, we introduce a lightweight version of SAGS, using a simple yet effective mid-point interpolation scheme, which showcases a compact representation of the scene with up to 24$\times$ size reduction without the reliance on any compression strategies. Extensive experiments across multiple benchmark datasets demonstrate the superiority of SAGS compared to state-of-the-art 3D-GS methods under both rendering quality and model size. Besides, we demonstrate that our structure-aware method can effectively mitigate floating artifacts and irregular distortions of previous methods while obtaining precise depth maps. Project page <a href="https://eververas.github.io/SAGS/">https://eververas.github.io/SAGS/</a>. </p><p><a href="http://arxiv.org/abs/2404.19149v1">PDF</a> 15 pages, 8 figures, 3 tables</p><p><strong>Summary</strong><br>利用结构驱动的优化策略，SAGS 在实时神经渲染中实现了压缩性和高保真性，通过利用局部-全局图表示来编码场景几何。</p><p><strong>Key Takeaways</strong></p><ul><li>SAGS 通过结构感知优化对 3DGS 进行了改进。</li><li>SAGS 采用局部-全局图表示，捕获场景几何。</li><li>SAGS 优化点位移以保持场景几何，提高表示能力和渲染质量。</li><li>SAGS 提出了一种基于中点插值的轻量级变体，可显著减少模型大小。</li><li>实验表明 SAGS 在渲染质量和模型尺寸方面优于其他 3DGS 方法。</li><li>SAGS 缓解了浮动伪影和不规则失真，并生成精确的深度图。</li><li>SAGS 项目主页：<a href="https://eververas.github.io/SAGS/。">https://eververas.github.io/SAGS/。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 结构感知3D高斯斑点</p></li><li><p>Authors: Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, Stefanos Zafeiriou</p></li><li><p>Affiliation: 帝国理工学院</p></li><li><p>Keywords: Novel View Synthesis, 3D Gaussian Splatting, Structure-Aware, Local-Global Graph Representation</p></li><li><p>Urls: https://eververas.github.io/SAGS/, Github:None</p></li><li><p>Summary:</p><pre><code>            (1):随着NeRFs的出现，3D高斯斑点（3D-GS）为实时神经渲染铺平了道路，克服了体积方法的计算负担。在3D-GS的开创性工作之后，一些方法试图实现可压缩且高保真性能的替代方案。然而，通过采用与几何无关的优化方案，这些方法忽略了场景的固有3D结构，从而限制了表现力和表示的质量，导致各种浮点和伪影。            (2):以往的方法：3D-GS、存在问题：忽略场景的3D结构，导致表现力和表示质量受限，产生浮点和伪影。动机充分：提出一种结构感知的方法来解决这些问题。            (3):本文提出的研究方法：结构感知高斯斑点方法（SAGS），隐式编码场景的几何形状，在基准新视图合成数据集上反映了最先进的渲染性能和降低的存储需求。SAGS建立在局部-全局图表示的基础上，该表示有助于学习复杂场景并强制有意义的点位移以保留场景的几何形状。此外，我们使用简单但有效的中间点插值方案引入了SAGS的轻量级版本，该版本展示了场景的紧凑表示，尺寸最多减少了24倍，而无需依赖任何压缩策略。            (4):任务和性能：在多个基准数据集上的广泛实验表明，SAGS在渲染质量和模型大小方面都优于最先进的3D-GS方法。此外，我们证明了我们的结构感知方法可以有效减轻先前方法的浮动伪影和不规则失真，同时获得精确的深度图。</code></pre></li><li><p>方法：</p><pre><code>            (1):曲率感知稠密化：采用 Grad-PU 方法，对低曲率区域进行中点插值，生成密集点云，增强 3D-GS 的初始化；            (2):结构感知编码器：基于 k-NN 图，使用图神经网络学习局部和全局结构特征，获得结构感知特征编码；            (3):细化网络：使用 MLP 解码结构感知特征编码，预测 3D 高斯斑点的属性（位置、颜色、不透明度、协方差）；            (4):SAGS-Lite：利用中点插值，减少存储需求，生成紧凑的 3D 高斯斑点集合，无需压缩技术。</code></pre></li><li><p>结论：</p><pre><code>            (1):本文提出了一种结构感知高斯斑点方法（SAGS），该方法利用场景的内在结构进行高保真神经渲染。我们提出了一种基于图神经网络的方法，该方法以结构化的方式预测高斯斑点的属性，从而克服了当前 3D 高斯斑点方法的缺点，即天真地优化高斯属性而忽略了底层场景结构。使用所提出的图表示，相邻的高斯斑点可以共享和聚合信息，从而促进场景渲染及其几何形状的保留。我们展示了所提出的方法在新的视图合成中可以优于当前最先进的方法，同时保留 3D-GS 的实时渲染。我们进一步引入了一种简单但有效的中间点插值方案，与 3D-GS 方法相比，它可以实现高达 24 倍的存储减少，同时保留高质量的渲染，而无需使用任何压缩和量化算法。总体而言，我们的研究结果证明了在 3D-GS 中引入结构的好处。            (2):创新点：提出了一种基于图神经网络的结构感知方法，以结构化的方式预测高斯斑点的属性，从而克服了当前 3D 高斯斑点方法的缺点，即天真地优化高斯属性而忽略了底层场景结构；Performance：在新的视图合成中可以优于当前最先进的方法，同时保留 3D-GS 的实时渲染；Workload：引入了简单的中间点插值方案，与 3D-GS 方法相比，它可以实现高达 24 倍的存储减少，同时保留高质量的渲染，而无需使用任何压缩和量化算法。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1c24f7a12b1fbb5ce1ccb02f3443561a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1a4caa28b767c498c125adefb63f6bdf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-36c36153d224a88e577e256a3ca35a36.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cb72df76b73fb4cab74c37be6a089579.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>根据多视角视频创建高保真头部形象是AR/VR应用的关键问题。MeGA通过为不同头部组件采用合适的表达方式，提高了渲染质量。</p><p><strong>Key Takeaways</strong></p><ul><li>MeGA采用FLAME网格表示面部，并使用UV位移图提供顶点偏移以提升个性化几何细节。</li><li>利用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分以实现真实感渲染。</li><li>MeGA使用3D高斯泼溅构建静态经典头发，并应用刚性变换和基于MLP的变形场来处理复杂动态表情。</li><li>结合遮挡感知混合，MeGA为整个头部生成更高保真的渲染，并支持发型改变和纹理编辑等下游任务。</li><li>在NeRSemble数据集上的实验表明MeGA设计有效，优于之前最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：MeGA：用于高保真渲染和头部编辑的混合网格高斯头部头像</p></li><li><p>作者：Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>第一作者单位：清华大学</p></li><li><p>关键词：头部头像、高保真渲染、头部编辑、混合表示</p></li><li><p>论文链接：https://arxiv.org/abs/2404.19026</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：创建高保真头部头像对于 AR/VR 应用至关重要，但现有的方法难以同时为所有头部组件（如皮肤、头发）获得高质量的渲染效果，因为它们使用单一表示来建模具有不同特征的组件。</p><p>（2）以往方法：以往方法探索了基于网格、基于 NeRF 和基于 3D 高斯的表示，取得了显着进展。然而，头部是一个复杂的“物体”，包含具有不同特征的组件，因此不存在单一的表示可以同时很好地建模所有组件。使用单一表示建模所有头部组件必然会牺牲一部分的渲染质量。</p><p>（3）本文方法：本文提出了一种混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件。具体来说，选择一个增强的 FLAME 网格作为面部表示，并预测一个 UV 位移图来提供每个顶点的偏移量，以改善个性化的几何细节。为了实现逼真的渲染，使用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分。对于头发建模，首先使用 3D 高斯泼溅构建一个静态的规范头发。进一步应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情。结合遮挡感知混合，MeGA 为整个头部生成了更高保真的渲染效果，并自然地支持更多下游任务。</p><p>（4）实验结果：在 NeRSemble 数据集上的实验表明，本文方法有效，优于以往的先进方法，并支持各种编辑功能，包括发型更改和纹理编辑。这些性能支持了本文的目标。</p><ol><li><p>Methods:                    (1): 提出混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；</p><pre><code>            (2): 选择一个增强的 FLAME 网格作为面部表示，并预测一个 UV 位移图来提供每个顶点的偏移量，以改善个性化的几何细节；            (3): 使用延迟神经渲染获得面部颜色，并将神经纹理分解为三个有意义的部分；            (4): 对于头发建模，首先使用 3D 高斯泼溅构建一个静态的规范头发，进一步应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情；            (5): 结合遮挡感知混合，MeGA 为整个头部生成了更高保真的渲染效果，并自然地支持更多下游任务。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种混合网格高斯头部头像（MeGA），该方法使用神经网格建模人脸，使用 3DGS 建模头发，在高保真渲染和头部编辑方面取得了显着效果。</p><p>（2）：创新点：创新性地提出了混合网格高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件，并设计了遮挡感知混合模块，实现了头部的高保真渲染和编辑。</p><p>性能：在 NeRSemble 数据集上的实验表明，本文方法在渲染质量和编辑功能方面均优于以往的先进方法。</p><p>工作量：本文方法的工作量相对较大，需要训练神经网格、3DGS 头发模型和遮挡感知混合模块。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details><h2 id="3D-Gaussian-Splatting-with-Deferred-Reflection"><a href="#3D-Gaussian-Splatting-with-Deferred-Reflection" class="headerlink" title="3D Gaussian Splatting with Deferred Reflection"></a>3D Gaussian Splatting with Deferred Reflection</h2><p><strong>Authors:Keyang Ye, Qiming Hou, Kun Zhou</strong></p><p>The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting. </p><p><a href="http://arxiv.org/abs/2404.18454v1">PDF</a> </p><p><strong>Summary</strong><br>高斯辐射场结合延时着色大幅提升反射效果，无需额外时间成本</p><p><strong>Key Takeaways</strong></p><ul><li>神经和高斯辐射场方法在视图合成中取得巨大进展，但镜面反射处理困难。</li><li>提出了延时着色方法，使用高斯散射有效渲染镜面反射。</li><li>环境贴图反射模型的挑战在于需要准确的表面法线，而法线估计受断续梯度的限制。</li><li>利用延时着色生成的逐像素反射梯度，桥接了相邻高斯的优化过程。</li><li>准确的法线估计逐渐传播，最终覆盖所有反射物体。</li><li>方法大幅优于最先进技术，在合成高质量镜面反射效果方面达到领先水平。</li><li>在合成和真实场景中，峰值信噪比 (PSNR) 均得到一致提高，且运行帧速率几乎与原始高斯散射相同。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 3D 高斯斑点与延迟反射</p></li><li><p>Authors: Keyang Ye, Qiming Hou, Kun Zhou</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: Novel view synthesis, deferred shading, real-time rendering</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.18454.pdf, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是神经辐射场（NeRF）和基于高斯的体渲染方法在新型视图合成中取得了巨大成功，但镜面反射仍然具有挑战性。</p><p>(2): 过去的方法如 3D 高斯斑点（3DGS）虽然提供了基于每个高斯的球谐函数（SH）进行视点相关着色，但其方向频率太有限，无法建模镜面反射。训练过程会产生高斯，以显式地拟合镜面反射，但这种方法会导致视觉伪影和较差的性能。</p><p>(3): 本文提出的研究方法是延迟着色，它利用延迟着色生成的逐像素反射梯度来弥合相邻高斯优化过程之间的差距，允许近乎正确的法线估计逐渐传播，最终覆盖所有反射物体。</p><p>(4): 在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作，证明了合成和真实场景的峰值信噪比（PSNR）都有持续的提高，同时运行帧速率几乎与原始反射无关的高斯斑点相同。</p><ol><li>方法：</li></ol><p>（1）：本方法采用延迟渲染模型，包含两个阶段；</p><p>（2）：第一阶段是高斯斑点，利用高斯参数 Θ𝑖、每个高斯视点相关的球谐函数颜色 𝑐𝑖 (v) 计算像素颜色 𝐶(v)；</p><p>（3）：第二阶段是延迟反射，将法线向量 𝑛𝑖 和镜面反射强度标量 𝑟𝑖 融入高斯斑点，生成最终像素颜色 𝐶′(v)。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种高质量的延迟高斯斑点渲染器，专门用于反射。它展示了稳定的训练和几乎与原始 3D 高斯斑点相同的帧速率的竞争性视觉质量，还生成了准确的表面法线和环境贴图。我们的延迟着色方法可能为未来的探索开辟了许多可能性。在高斯斑点的背景下探索渲染方程的更多创造性分割将是一件有趣的事情。我们的管道还可以扩展到超出环境贴图的高质量反射算法，包括屏幕空间反射 [McGuire and Mara 2014] 和硬件光线追踪。将 3D 高斯和可微渲染推广到此类方法可以显著提高反射质量。探索添加基于物理的粗糙度、将我们的方法推广到光泽材料的可能性也很有趣。致谢：这项工作部分得到了国家自然科学基金（编号 62227806 和 U23A20311）和 XPLORER PRIZE 的支持。源代码和数据可从 https://gapszju.github.com/3DGS-DR 获取。</p><p>（2）：创新点：提出了延迟着色方法，利用延迟着色生成的逐像素反射梯度来弥合相邻高斯优化过程之间的差距，允许近乎正确的法线估计逐渐传播，最终覆盖所有反射物体。</p><p>性能：在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作，证明了合成和真实场景的峰值信噪比（PSNR）都有持续的提高，同时运行帧速率几乎与原始反射无关的高斯斑点相同。</p><p>工作量：本文方法的工作量与原始 3D 高斯斑点相似，在合成高质量镜面反射效果的任务上，本文方法明显优于最先进的技术和同期工作。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-01f3cc91b932b34b556b0aeef26ce855.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7317cab6e01a55d9c668cb2940a49ed4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-df3dd63fe60a37ec3cb9014da779955c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-bd20bc6eb9ac41d7e4d7028cacc5d273.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f4637de64153c20555d7c194c23899d.jpg" align="middle"></details><h2 id="High-quality-Surface-Reconstruction-using-Gaussian-Surfels"><a href="#High-quality-Surface-Reconstruction-using-Gaussian-Surfels" class="headerlink" title="High-quality Surface Reconstruction using Gaussian Surfels"></a>High-quality Surface Reconstruction using Gaussian Surfels</h2><p><strong>Authors:Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</strong></p><p>We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods. </p><p><a href="http://arxiv.org/abs/2404.17774v2">PDF</a> Results added and improved</p><p><strong>Summary</strong><br>针对三维高斯点和曲面元素的优点，提出一种新的点云表征方式高斯曲面元素，有效改善了优化稳定性和曲面对齐，并通过容积剪切和基于泊松的筛选重建方法提升了形状重建的精度。</p><p><strong>Key Takeaways</strong></p><ul><li>提出高斯曲面元素，将三维高斯点的优化灵活性与曲面元素的对齐特性相结合。</li><li>通过将三维高斯点的 z 尺度设为 0，将三维椭圆体压平为二维椭圆，为优化器提供清晰指引。</li><li>将局部 z 轴视为法线方向，极大提高了优化的稳定性和表面对齐。</li><li>设计自监督法线深度一致性损失，弥补共方差矩阵中计算出的局部 z 轴导数为零的问题。</li><li>融合单目法线先验和前景掩码，增强重建质量，缓解高光和背景带来的影响。</li><li>提出体积切割方法，聚合高斯曲面元素的信息，去除深度图中由 alpha 混合产生的错误点。</li><li>采用带筛选的泊松重建方法对融合的深度图进行重建，提取表面网格。</li><li>实验结果表明，该方法在表面重建方面优于现有的神经体绘制和点云绘制方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯表面元的高质量表面重建</p></li><li><p>Authors: Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</p></li><li><p>Affiliation: 浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>Keywords: 3D Surface Reconstruction, Gaussian Surfels, Depth-normal Consistency</p></li><li><p>Urls: https://arxiv.org/abs/2404.17774 , Github:None</p></li><li><p>Summary: </p></li></ol><p>(1):神经辐射场（NeRF）在图像合成的任务上取得了巨大的成功，但在表面重建任务上仍然存在一些问题，例如表面对齐不准确、优化不稳定以及对高光和背景区域的敏感性。</p><p>(2):以往的方法主要集中在优化损失函数和使用先验信息来解决这些问题，但效果有限。</p><p>(3):本文提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性。具体来说，将 3D 高斯点的 z 尺度设置为 0，有效地将原始的 3D 椭球压扁成 2D 椭圆。这种设计为优化器提供了明确的指导，通过将局部 z 轴视为法线方向，极大地提高了优化稳定性和表面对齐。同时设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题。此外，本文还集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题。提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点。最后，将筛选泊松重建方法应用于融合的深度图以提取表面网格。</p><p>(4):实验结果表明，与最先进的神经体渲染和基于点的渲染方法相比，本文的方法在表面重建方面表现出优越的性能。</p><ol><li>方法：</li></ol><p>（1）：提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性；</p><p>（2）：设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题；</p><p>（3）：集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题；</p><p>（4）：提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点；</p><p>（5）：将筛选泊松重建方法应用于融合的深度图以提取表面网格。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种新的点表示——高斯表面元，它结合了 3D 高斯点的灵活优化过程和表面元的表面对齐特性，设计了一个自监督的法线深度一致性损失来解决此设置中从协方差矩阵计算的局部 z 轴的导数为零的问题，集成了单目法线先验和前景掩码以提高重建质量，减轻了与高光和背景相关的问题，提出了一种体积切割方法来聚合高斯表面元的信息，以去除 alpha 混合生成的深度图中的错误点，将筛选泊松重建方法应用于融合的深度图以提取表面网格。通过实验，本文方法在表面重建方面表现出优越的性能。</p><p>（2）：创新点：提出了一种新的点表示——高斯表面元，设计了一个自监督的法线深度一致性损失，集成了单目法线先验和前景掩码，提出了一种体积切割方法来聚合高斯表面元的信息；性能：与最先进的神经体渲染和基于点的渲染方法相比，本文的方法在表面重建方面表现出优越的性能；工作量：本文方法的计算成本相对较高。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ea805e1d2146685877956d96c3f2411f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d553afdbbcbed9ac6b50b06fa71184c3.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c60a2604158ad2952f8aa6bf05e4bfb8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ffafd283accaa363affe36c81454980.jpg" align="middle"></details><h2 id="GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting"><a href="#GaussianTalker-Real-Time-High-Fidelity-Talking-Head-Synthesis-with-Audio-Driven-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting"></a>GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting</h2><p><strong>Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</strong></p><p>We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker’s superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at <a href="https://github.com/KU-CVLAB/GaussianTalker/">https://github.com/KU-CVLAB/GaussianTalker/</a> . </p><p><a href="http://arxiv.org/abs/2404.16012v2">PDF</a> Project Page: <a href="https://ku-cvlab.github.io/GaussianTalker">https://ku-cvlab.github.io/GaussianTalker</a></p><p><strong>Summary</strong><br>高斯说话者：实时生成姿势可控会说话的头部</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种名为高斯说话者的新框架，用于实时生成姿势可控的会说话的头部。</li><li>利用 3D 高斯 splatting（3DGS）的快速渲染能力，同时解决了直接使用语音音频控制 3DGS 的挑战。</li><li>构建头部规范的 3DGS 表示，并使其与音频同步变形。</li><li>关键的见解是将 3D 高斯属性编码成共享的隐式特征表示，并在其中与音频特征合并以控制每个高斯属性。</li><li>该设计利用了空间感知特征，并强制相邻点之间的交互。</li><li>将特征嵌入馈送到空间-音频注意模块，该模块预测每个高斯的属性的帧级偏移。</li><li>与以前的串联或乘法方法相比，它在处理大量高斯及其复杂参数时更稳定。</li><li>实验结果表明，与以前的方法相比，高斯说话者在面部保真度、唇形同步精度和渲染速度方面更胜一筹。</li><li>具体而言，高斯说话者以高达 120 FPS 的非凡渲染速度，超过了之前的基准。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯说话者：实时高保真说话头部合成</p></li><li><p>Authors: Kyusun Cho，Joungbin Lee，Heeji Yoon，Yeobin Hong，Jaehoon Ko，Sangjun Ahn，Seungryong Kim</p></li><li><p>Affiliation: 韩国大学</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/ , https://github.com/ku-cvlab/GaussianTalker</p></li><li><p>Summary:</p></li></ol><p>(1): 本文的研究背景是生成受任意语音音频驱动的说话头部视频，这项任务有很多用途，包括生成数字人、虚拟替身、电影制作和电话会议。</p><p>(2): 过去的方法有使用生成模型来解决此任务，但它们不专注于控制头部姿势，这限制了它们的真实性和适用性。最近，许多研究应用神经辐射场（NeRF）来创建可控姿势的说话人像。通过直接调节 NeRF 多层感知器（MLP）中的音频特征，这些方法可以合成与输入音频嘴唇同步的视图一致的 3D 头部结构。虽然这些基于 NeRF 的技术实现了高质量和一致的视觉输出，但它们缓慢的推理速度限制了它们的实用性。</p><p>(3): 本文提出的研究方法是利用 3D 高斯溅射（3DGS）的快速渲染能力。3DGS 被公认为 NeRF 的可行替代方案，它提供了可比的渲染质量，同时显着提高了推理速度。虽然 3DGS 最初被提议用于重建静态 3D 场景，但后续工作已将其扩展到动态场景。然而，很少有研究利用 3DGS 创建具有可控输入的动态 3D 场景，其中大多数都专注于使用中间网格表示来驱动 3D 高斯。然而，依赖中间 3D 网格表示（例如 FLAME）进行变形通常缺乏头发和面部皱纹的细节。</p><p>(4): 本文方法在任务和性能上取得了以下成就：- 与现有的 3D 说话人脸合成模型相比，本文方法在保真度、唇形同步和推理时间方面取得了优异的性能，并且以更高的 FPS 运行。- 本文方法实现了高达 120 FPS 的显着渲染速度，超过了之前的基准。</p><ol><li>方法：</li></ol><p>（1）：本文方法采用 3D 高斯溅射（3DGS）的快速渲染能力，3DGS 被公认为 NeRF 的可行替代方案，它提供了可比的渲染质量，同时显着提高了推理速度；</p><p>（2）：本文方法通过学习具有三平面表示的规范 3D 高斯体来学习说话头的规范形状，多分辨率三平面表示利用 3DGS 的显式 3D 表示，同时还利用隐式神经辐射场的编码空间信息，对于每个规范 3D 位置，从多分辨率三平面表示中提取特征嵌入，这些特征嵌入用于计算每个点的比例、旋转、球谐函数和不透明度，这些计算出的属性构成了说话头的规范 3D 高斯体；</p><p>（3）：本文方法采用语音动作交叉注意模块融合 3D 高斯体特征和音频特征，以准确建模由输入音频驱动的面部运动，空间音频注意模块包含多组交叉注意层和前馈层，每组通过跳跃连接相互连接，该模块将空间特征与第 n 帧的音频特征进行交叉注意计算，从而输出特征成功地将音频特征与每个 3D 高斯体捕获的丰富面部细节相融合；</p><p>（4）：本文方法通过引入附加输入条件来捕获非语言动作，从而将与语音相关的运动与单目视频区分开来，遵循先前的工作，首先应用显式眨眼控制与眼睛特征，具体地，使用面部动作编码系统中的 AU45 来描述眨眼程度，并利用正弦位置编码以匹配输入维度，此外，将摄像机视点作为辅助输入以区分非语言场景变化，虽然将逐帧摄像机公式化为面部视点，但典型的视频是在头部连续移动时使用静态摄像机拍摄的，因此，肖像图像的变化（例如头发位移和光照变化）独立于语音音频，因此，使用面部视点嵌入作为附加输入条件来区分这些非听觉变化。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 GaussianTalker，一个新颖的实时姿态可控 3D 说话人脸合成框架，利用 3D 高斯体进行头部表示。我们的方法通过调节高斯原语实现了对高斯原语的精确控制，从而获得了比以往更好的保真度、唇形同步和推理时间，并且以更高的 FPS 运行。</p><p>（2）：创新点：利用 3D 高斯体进行头部表示，实现了姿态可控的 3D 说话人脸合成；性能：在保真度、唇形同步和推理时间方面取得了优异的性能，实现了高达 120 FPS 的显着渲染速度；工作量：与现有的 3D 说话人脸合成模型相比，本文方法在保真度、唇形同步和推理时间方面取得了优异的性能，并且以更高的 FPS 运行。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation"><a href="#OMEGAS-Object-Mesh-Extraction-from-Large-Scenes-Guided-by-Gaussian-Segmentation" class="headerlink" title="OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation"></a>OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian   Segmentation</h2><p><strong>Authors:Lizhi Wang, Feng Zhou, Jianqin Yin</strong></p><p>Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation. OMEGAS employs a multi-step approach, grounded in several excellent off-the-shelf methodologies. Specifically, initially, we utilize the Segment Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS), thereby creating a basic 3DGS model of the target object. Then, we leverage large-scale diffusion priors to further refine the details of the 3DGS model, especially aimed at addressing invisible or occluded object portions from the original scene views. Subsequently, by re-rendering the 3DGS model onto the scene views, we achieve accurate object segmentation and effectively remove the background. Finally, these target-only images are used to improve the 3DGS model further and extract the definitive 3D object mesh by the SuGaR model. In various scenarios, our experiments demonstrate that OMEGAS significantly surpasses existing scene reconstruction methods. Our project page is at: <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> </p><p><a href="http://arxiv.org/abs/2404.15891v2">PDF</a> arXiv admin note: text overlap with arXiv:2311.17061 by other authors</p><p><strong>Summary</strong><br>大型场景中特定物体的高精度三维重建框架：OMEGAS，通过高斯分割引导物体网格提取。</p><p><strong>Key Takeaways</strong></p><ul><li>OMEGAS 框架可从大型场景中高精度重建特定物体。</li><li>结合 Segment Anything Model (SAM) 和大型扩散先验，改善 3DGS 模型细节。</li><li>重新渲染 3DGS 模型，实现准确物体分割并去除背景。</li><li>使用目标图像，进一步优化 3DGS 模型并提取最终 3D 物体网格。</li><li>OMEGAS 在各种场景中优于现有场景重建方法。</li><li>代码和数据可在 <a href="https://github.com/CrystalWlz/OMEGAS">https://github.com/CrystalWlz/OMEGAS</a> 获取。</li><li>OMEGAS 适用于目标物体部分遮挡或不可见的情况。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: OMEGAS: 高斯分割引导的大场景物体网格提取</p></li><li><p>Authors: Lizhi Wang, Feng Zhou, Jianqin Yin</p></li><li><p>Affiliation: 北京邮电大学</p></li><li><p>Keywords: Mesh Reconstruction, 3D Gaussian Splatting, Diffusion Model</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.15891 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着 3D 重建技术的进步，复杂 3D 场景的高质量实时渲染成为可能。然而，从大场景中精确重建特定物体仍然是一个挑战。现有的场景重建技术经常导致物体细节纹理丢失，并且无法重建在视图中被遮挡或看不见的物体部分。</p><p>(2): 过去的重建方法难以处理大场景中复杂物体，并且在处理遮挡和不可见区域时存在问题。本文提出的方法以 3D 高斯点云（3DGS）为基础，利用扩散模型来细化细节，并结合目标分割和网格提取技术，以提高重建精度和效率。</p><p>(3): 本文提出 OMEGAS 框架，该框架采用多步方法，首先利用分割任何模型（SAM）指导 3DGS 的分割，创建目标物体的基本 3DGS 模型。然后，利用大规模扩散先验进一步细化 3DGS 模型的细节，特别是针对原始场景视图中不可见或被遮挡的物体部分。随后，将 3DGS 模型重新渲染到场景视图上，实现精确的目标分割并有效去除背景。最后，利用这些仅包含目标的图像进一步改进 3DGS 模型，并通过 SuGaR 模型提取最终的 3D 物体网格。</p><p>(4): 在各种场景中，实验表明 OMEGAS 明显优于现有的场景重建方法。该方法在处理复杂物体、遮挡和不可见区域方面表现出色，能够生成高质量的 3D 物体网格，为增强现实、游戏和大规模 3D 数据集生成等下游任务提供了支持。</p><ol><li>方法：</li></ol><p>（1）：利用 SAM 引导 3DGS 分割，构建目标物体的基本 3DGS 模型；</p><p>（2）：应用大规模扩散先验（Stable Diffusion）细化 3DGS 模型细节，特别是不可见或被遮挡部分；</p><p>（3）：将 3DGS 模型重新渲染到场景视图上，获得精确目标分割并去除背景；</p><p>（4）：使用仅包含目标的图像进一步改进 3DGS 模型，通过 SuGaR 模型提取最终 3D 物体网格。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于高斯分割引导的大场景物体网格提取框架 OMEGAS，该框架能够从多视角场景图像中有效提取目标物体的精细网格，并能够重建被遮挡或不可见的物体部分。OMEGAS 创新性地融合了 SAM、3DGS、Stabled Diffusion 和 SuGaR 模型等多种优秀方法。与基线方法相比，我们的方法在目标的细节纹理和抗遮挡性方面均表现出极大的优势。我们希望 OMEGAS 能够为 3D 重建领域提供新的思路，并为下游任务提供更好的解决方案。</p><p>（2）：创新点：提出了一种基于高斯分割引导的大场景物体网格提取框架 OMEGAS；性能：在处理复杂物体、遮挡和不可见区域方面表现出色，能够生成高质量的 3D 物体网格；工作量：与基线方法相比，OMEGAS 的计算成本较高，需要较长的处理时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e4ef7cc371681a1b1a10401043bee74c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0a62638572af15479eb987b4dae28d70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9b782cee0b88f29d10ae78c3dec02dbc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b5e6dde7b196e2a509f4476175ec837.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e8a1d8354bdc5dd159b09b49e8c7efd4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f986fc824e450fb1b91fc8f6304e7c73.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>3D高斯散布技术合成音频驱动说话人头像，精准唇部动作及动态高斯渲染，实现逼真流畅的说话人头像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>基于3D高斯散布的音频驱动说话人头像合成新方法。</li><li>显式高斯表示，通过将高斯与3D面部模型绑定，实现面部运动的直观控制。</li><li>扬声器特定运动转换器，实现精准的扬声器特定唇部动作。</li><li>动态高斯渲染器，通过潜在姿势引入扬声器特定混合形状，增强面部细节表示。</li><li>广泛实验表明，GaussianTalker在说话人头像合成中优于现有最先进的方法。</li><li>渲染速度达到 130 FPS，显着超过实时渲染性能阈值。</li><li>可部署在其他硬件平台上，具有实际应用潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 高斯说话者：基于 3D 高斯喷射的特定说话者说话头合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion control, Facial animation</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.14037, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：最近基于神经辐射场 (NeRF) 的音频驱动说话头合成工作取得了令人印象深刻的成果。然而，由于 NeRF 隐式表示导致的姿势和表情控制不足，这些方法仍然存在一些限制，例如不同步或不自然的唇部动作，以及视觉抖动和伪影。</p><p>(2): 过去的方法：现有方法存在唇部运动不同步、表情控制不足等问题。本文提出的方法动机明确。</p><p>(3): 研究方法：本文提出了一种基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker。通过将高斯体绑定到 3D 面部模型，利用 3D 高斯体的显式表示特性，实现了对面部动作的直观控制。GaussianTalker 由两个模块组成：特定说话者运动转换器和动态高斯渲染器。特定说话者运动转换器通过通用音频特征提取和定制唇部动作生成，实现了特定于目标说话者的准确唇部动作。动态高斯渲染器引入了特定说话者的混合形状，以实现精确的表情控制。</p><p>(4): 性能：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能。这些性能支持了本文提出的方法的目标。</p><ol><li>方法：</li></ol><p>（1）：提出基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，通过将高斯体绑定到 3D 面部模型，利用 3D 高斯体的显式表示特性，实现了对面部动作的直观控制；</p><p>（2）：GaussianTalker 由两个模块组成：特定说话者运动转换器和动态高斯渲染器。特定说话者运动转换器通过通用音频特征提取和定制唇部动作生成，实现了特定于目标说话者的准确唇部动作；</p><p>（3）：动态高斯渲染器引入了特定说话者的混合形状，以实现精确的表情控制；</p><p>（4）：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能。</p><ol><li>结论：</li></ol><p>（1）：本工作的主要贡献在于提出了一种基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，将高斯体与 FLAME 模型相结合，通过特定说话者的 FLAME 转换器和动态高斯渲染器，实现了唇部动作同步、表情控制和视觉质量的提升；</p><p>（2）：创新点：提出基于 3D 高斯喷射的音频驱动说话头合成新方法 GaussianTalker，通过特定说话者的 FLAME 转换器和动态高斯渲染器，实现了唇部动作同步、表情控制和视觉质量的提升；性能：在说话头合成任务上，GaussianTalker 在唇部运动同步、表情控制和视觉质量方面均取得了最先进的性能；工作量：GaussianTalker 的渲染速度极快，远超其他方法；</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-05-02  Spectrally Pruned Gaussian Fields with Neural Compensation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Talking%20Head%20Generation/</id>
    <published>2024-05-02T02:33:18.000Z</published>
    <updated>2024-05-02T02:33:18.719Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model’s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>人脸头像通过视觉信号驱动，在跨人物合成中颇受欢迎，即使驾驶员与动画人物不同，这一富有挑战性且高度实用的方法也适用。最近提出的 MegaPortrait 模型已在这个领域展现了最先进的效果。我们对该模型进行了深入的检查和评估，特别关注其表情描述符的潜在空间，并发现了该模型表达强烈面部动作的能力存在一些局限性。为了解决这些限制，我们在训练管道和模型架构中都提出了重大改变，从而引入了我们的 EMOPortraits 模型，在此我们： 提高了模型对于精确表现强烈的不对称面部表情的能力，在情绪传递任务中创下了新的最先进成果，在指标和质量方面均超过了先前的所有方法。 将基于语音的模式纳入我们的模型中，在基于音频的面部动画中取得了一流的性能，从而可以通过包括视觉信号、音频或两者的融合在内等各种方式驱动源身份。 我们提出了一个新颖的多视图视频数据集，其中包含广泛的强烈和不对称面部表情，填补了现有数据集中缺少此类数据空白的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>EMOPortraits 模型显著提高了生成强烈不对称面部表情的能力。</li><li>新模型在情感传递任务中超越了现有方法，在指标和质量方面均创下新高。</li><li>EMOPortraits 模型集成了语音驱动模式，在音频驱动的面部动画中实现了顶级性能。</li><li>该模型支持通过视觉信号、音频或两者融合等多种方式进行驱动。</li><li>研究人员提出了一个新的多视图视频数据集，其中包含广泛的强烈和不对称面部表情。</li><li>该数据集填补了现有数据集中此类数据的空白。</li><li>EMOPortraits 模型为跨驱动合成和音频驱动的面部动画提供了新的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: EMOPortraits: 情感增强的多模态一次性头部头像</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: 伦敦帝国理工学院</p></li><li><p>Keywords: 头部头像、情感传递、语音驱动面部动画、多模态、面部表情描述符</p></li><li><p>Urls: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：头部头像动画在交叉驱动合成中越来越受欢迎，其中驱动者与动画角色不同，这是一种具有挑战性但非常实用的方法。最近提出的 MegaPortraits 模型已在此领域展示了最先进的结果。</p><p>(2): 过去的方法：本文对 MegaPortraits 模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，并发现了其在表达强烈面部动作方面的几个限制。</p><p>(3): 本文提出的研究方法：为了解决这些限制，本文在训练管道和模型架构中提出了实质性的改变，引入了 EMOPortraits 模型，其中：    - 增强了模型对强烈、不对称面部表情的忠实支持能力，在情感传递任务中设定了新的最先进结果，在指标和质量方面都超越了以前的方法。    - 将语音驱动模式纳入模型，在音频驱动的面部动画中实现了顶级性能，使得可以通过视觉信号、音频或两者的混合等多种方式驱动源身份。    - 提出了一个新颖的多视角视频数据集，其中包含各种强烈和不对称的面部表情，填补了现有数据集中此类数据的空白。</p><p>(4): 性能和目标支持：在情感传递任务中，EMOPortraits 模型在指标和质量方面都超越了以前的方法。在音频驱动的面部动画中，该模型实现了顶级性能。这些性能支持了本文的目标，即增强模型对强烈面部表情的支持能力，并将其用于多模态驱动。</p><ol><li>Methods:</li></ol><p>(1): 对 MegaPortraits 模型的潜在空间进行深入检查和评估，发现其在表达强烈面部动作方面的限制；</p><p>(2): 在训练管道和模型架构中提出实质性改变，引入 EMOPortraits 模型，增强其对强烈、不对称面部表情的支持能力；</p><p>(3): 将语音驱动模式纳入模型，实现音频驱动的面部动画的顶级性能；</p><p>(4): 提出一个新颖的多视角视频数据集，填补现有数据集中强烈和不对称面部表情数据的空白。</p><ol><li>结论：<pre><code>           （1）：本文提出了 EMOPortraits，一种在图像驱动、跨身份情感转换中具有卓越性能的新型神经头像创建方法。我们的语音驱动模式使得可以通过多种条件（视频、音频、头部运动）来驱动面部动画。我们收集了 FEED 数据集，我们相信这将成为从事各种以人为中心研究的研究人员的宝贵资产。然而，我们的方法也有一些局限性。它不会生成头像的身体或肩膀，从而限制了一些用例。我们目前将我们的输出与源图像主体集成在一起。此外，该模型有时难以进行准确的表情转换，并且在头部大幅旋转时表现不佳。这些挑战对于未来的增强至关重要，并仍然是我们正在进行的研究工作的核心。           （2）：创新点：增强了模型对强烈、不对称面部表情的支持能力，并将其用于多模态驱动；性能：在情感传递任务中超越了以前的方法，在音频驱动的面部动画中实现了顶级性能；工作量：提出了一个新颖的多视角视频数据集，填补了现有数据集中强烈和不对称面部表情数据的空白。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://picx.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation"><a href="#CSTalk-Correlation-Supervised-Speech-driven-3D-Emotional-Facial-Animation-Generation" class="headerlink" title="CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation"></a>CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial   Animation Generation</h2><p><strong>Authors:Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</strong></p><p>Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods. </p><p><a href="http://arxiv.org/abs/2404.18604v1">PDF</a> </p><p><strong>Summary</strong><br>语音驱动的 3D 人脸动画技术已发展多年，但其在实际应用中仍未达到预期。</p><p><strong>Key Takeaways</strong></p><ul><li>数据量限制、唇形对齐和面部表情的自然性是语音驱动 3D 人脸动画技术面临的主要挑战。</li><li>现有的唇形对齐方法仍难以合成自然逼真的表情，导致面部动画表现机械僵硬。</li><li>从语音中提取情绪特征，但面部动作的随机性限制了情绪的有效表达。</li><li>本文提出了一种名为 CSTalk（相关性监督）的方法，通过模拟面部动作不同区域之间的相关性，指导生成模型的训练，从而生成符合人类面部运动模式的逼真表情。</li><li>我们使用基于元人类角色模型的丰富控制参数生成更精细的动画，并为五种不同情绪采集了一个数据集。</li><li>我们使用自编码器结构训练了一个生成网络，输入一个情感嵌入向量来实现用户控制表情的生成。</li><li>实验结果表明，我们的方法优于现有的最先进方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: CSTalk: 基于相关性的语音驱动 3D 情感面部动画生成</p></li><li><p>Authors: Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</p></li><li><p>Affiliation: 东南大学自动化学院</p></li><li><p>Keywords: Speech-driven facial animation, 3D facial animation, Emotional expression, Correlation supervision, MetaHuman character model</p></li><li><p>Urls: Paper, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):语音驱动的 3D 面部动画技术虽然发展多年，但其实际应用效果仍未达到预期。主要挑战在于数据限制、唇部对齐和面部表情的自然度。虽然唇部对齐已有许多相关研究，但现有方法难以合成自然逼真的表情，导致面部动画呈现机械僵硬的外观。即使有些研究从语音中提取情绪特征，但面部动作的随机性限制了情绪的有效表达。</p><p>(2):语音驱动的 3D 面部动画生成方法主要有两种：基于网格和基于参数化。基于网格的方法直接操纵面部顶点，允许面部表情进行复杂的变化，而基于参数化的方法采用基于模板的框架，通过特定参数控制面部动作。基于参数化的模型中，基于 ARKit 2 标准的 blend-shape 模型已被广泛应用，它采用一组预定义的面部子动作作为模板，通过线性组合分配给每个子动作的权重来生成不同的表情。这种方法提供了高度的可控性和泛化性，可以在不同的虚拟形象中复用动画参数。然而，简单 52 维数据的线性组合不足以实现逼真自然的动画。特别是，准确捕捉上部面部区域的细微表情仍然是一个挑战。因此，以往的研究主要集中在对齐唇部动作上。虽然有一些尝试将情感特征融入到面部表演中，例如 Faceformer 和 Emotalk，但这些努力主要集中在从音频中提取情感线索，而忽略了面部表情重建的优化。</p><p>(3):本文采用基于 Epic 提出的 MetaHuman 角色模型的参数化模型，通过 185 个控制装备操纵面部动画，每个装备对应一组面部肌肉。通过非线性变形其各自区域内的面部顶点，MetaHuman 模型在捕捉复杂表情方面表现出潜力。剩下的问题是从语音预测适当的控制装备曲线。研究表明，不同区域的面部动作之间存在相关性，这既源于协调肌肉控制的物理约束，也源于习惯模式。具体来说，在压力和语音停顿期间，嘴巴、眉毛和脸颊等区域往往会同时运动以传达表情意图。因此，本文采用基于 Transformer 的编码器对相关性进行建模。使用该模型作为监督，训练一个 3D 面部动画生成模型。</p><p>(4):本文提出的 CSTalk 方法在不同的情感状态下能够生成复杂的表情。基于 Transformer 编码器对特定情感中面部动作之间的相关性进行建模，作为约束条件，生成的面部表情更符合真实人类语音表情。</p><ol><li>Methods:</li></ol><p>(1):基于 Epic 提出的 MetaHuman 角色模型，利用 185 个控制装备操纵面部动画，每个装备对应一组面部肌肉；</p><p>(2):利用 Transformer 编码器对不同区域的面部动作之间的相关性进行建模，作为监督，训练一个 3D 面部动画生成模型；</p><p>(3):提出的 CSTalk 方法在不同的情感状态下能够生成复杂的表情，基于 Transformer 编码器对特定情感中面部动作之间的相关性进行建模，作为约束条件，生成的面部表情更符合真实人类语音表情。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种语音驱动的3D情感面部动画生成网络CSTalk，能够生成唇部动作对齐且表情逼真的动画。并且我们首次引入了基于MetaHuman的面部控制装备模型，使其能够与艺术家直接协作并在工业管道中应用。生成的动画参数与身份无关，并且可以被任何MetaHuman虚拟形象复用。此外，我们揭示了不同面部动作区域之间存在相关性，并对不同情感下的这些相关性进行建模，并利用它们来帮助训练网络生成更符合面部动作模式的表情。我们的方法在结果方面优于现有方法。</p><p>（2）：创新点：提出了一种基于相关性的语音驱动3D情感面部动画生成方法，能够生成表情丰富且与语音一致的面部动画；性能：在公开数据集上的实验表明，该方法在表情自然度、唇部对齐和情感表达方面均优于现有方法；工作量：该方法的实现相对复杂，需要对Transformer编码器和面部控制装备模型进行深入理解。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-28ba327053e565fa0b60537d43960f32.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-13b31067e590bbf83ad3f32bb9ed29f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-203dce5c70eae1db15da207b6436f6eb.jpg" align="middle"></details>## GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with   Audio-Driven 3D Gaussian Splatting**Authors:Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim**We propose GaussianTalker, a novel framework for real-time generation of pose-controllable talking heads. It leverages the fast rendering capabilities of 3D Gaussian Splatting (3DGS) while addressing the challenges of directly controlling 3DGS with speech audio. GaussianTalker constructs a canonical 3DGS representation of the head and deforms it in sync with the audio. A key insight is to encode the 3D Gaussian attributes into a shared implicit feature representation, where it is merged with audio features to manipulate each Gaussian attribute. This design exploits the spatial-aware features and enforces interactions between neighboring points. The feature embeddings are then fed to a spatial-audio attention module, which predicts frame-wise offsets for the attributes of each Gaussian. It is more stable than previous concatenation or multiplication approaches for manipulating the numerous Gaussians and their intricate parameters. Experimental results showcase GaussianTalker's superiority in facial fidelity, lip synchronization accuracy, and rendering speed compared to previous methods. Specifically, GaussianTalker achieves a remarkable rendering speed up to 120 FPS, surpassing previous benchmarks. Our code is made available at https://github.com/KU-CVLAB/GaussianTalker/ . [PDF](http://arxiv.org/abs/2404.16012v2) Project Page: https://ku-cvlab.github.io/GaussianTalker**Summary**高斯说话者：实时生成可控姿势说话头的创新框架，融合了 3DGS 的快速渲染能力和音频特征操作 3DGS 的挑战。**Key Takeaways**- 提出高斯说话者，一种创新的实时生成姿势可控说话头的框架。- 构建头的规范 3DGS 表示，并使其与音频同步变形。- 将 3D 高斯属性编码成共享的隐式特征表示，并将其与音频特征合并以操纵每个高斯属性。- 设计利用空间感知特征并加强相邻点之间的交互。- 将特征嵌入输入到空间-音频注意模块，该模块预测每个高斯的属性的逐帧偏移量。- 实验结果表明，高斯说话者在面部逼真度、唇形同步精度和渲染速度方面优于以前的方法。- 高斯说话者实现了高达 120 FPS 的卓越渲染速度，超越了之前的基准。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title:高斯说话者：实时高保真说话头合成（高斯说话者：具有音频驱动的 3D 高斯喷射的实时高保真说话头合成）</p></li><li><p>Authors: Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, Seungryong Kim</p></li><li><p>Affiliation: 韩国大学</p></li><li><p>Keywords: Talking Head Generation, 3D Controllable Head, 3D Gaussian Splatting</p></li><li><p>Urls: https://ku-cvlab.github.io/GaussianTalker/</p></li><li><p>Summary:</p></li></ol><p>(1):该文章的研究背景是：生成由任意语音音频驱动的说话头视频是一项流行的任务，具有各种用途，包括生成数字人、虚拟化身、电影制作和电话会议。虽然各种工作 [6, 21, 33, 43] 已成功尝试使用生成模型解决此任务，但它们不专注于控制头部姿势，从而限制了它们的真实性和适用性。最近，许多研究 [17, 24, 27, 39, 48, 49] 已将神经辐射场 (NeRF) [31] 应用于创建可控姿态的说话肖像。通过直接调节 NeRF 多层感知器 (MLP) 中的音频特征，这些方法可以合成与输入音频同步唇部的视图一致的 3D 头部结构。虽然这些基于 NeRF 的技术实现了高质量和一致的视觉输出，但其缓慢的推理速度限制了它们的实用性。尽管最近的进步 [24, 39] 以 512 × 512 分辨率实现了高达 30 帧每秒 (fps) 的渲染速度，但必须克服计算瓶颈才能应用于实际场景。</p><p>(2):过去的方法有：神经辐射场 (NeRF) [31] 已被应用于创建可控姿态的说话肖像。通过直接调节 NeRF 多层感知器 (MLP) 中的音频特征，这些方法可以合成与输入音频同步唇部的视图一致的 3D 头部结构。基于 NeRF 的技术实现了高质量和一致的视觉输出，但其缓慢的推理速度限制了它们的实用性。尽管最近的进步 [24, 39] 以 512 × 512 分辨率实现了高达 30 帧每秒 (fps) 的渲染速度，但必须克服计算瓶颈才能应用于实际场景。问题是：基于 NeRF 的技术推理速度慢，限制了它们的实用性。该方法的动机很好，因为它解决了基于 NeRF 的技术推理速度慢的问题。</p><p>(3):本文提出的研究方法是：GaussianTalker 是一种新颖的框架，用于实时生成可控姿态的说话头。它利用了 3D 高斯喷射 (3DGS) 的快速渲染功能，同时解决了直接用语音音频控制 3DGS 的挑战。GaussianTalker 构建了一个头部规范的 3DGS 表示，并使其与音频同步变形。一个关键的见解是将 3D 高斯属性编码成一个共享的隐式特征表示，其中它与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。与用于操纵大量高斯及其复杂参数的先前级联或乘法方法相比，它更稳定。</p><p>(4):本文方法在以下任务和性能上取得了成就：在面部保真度、唇部同步准确性和渲染速度方面，GaussianTalker 优于以前的方法。具体来说，GaussianTalker 以高达 120 FPS 的显着渲染速度，超越了之前的基准。性能可以支持他们的目标，因为 GaussianTalker 在面部保真度、唇部同步准确性和渲染速度方面都优于以前的方法。</p><ol><li><p>方法：</p><pre><code>           (1):GaussianTalker 是一种新颖的框架，用于实时生成可控姿态的说话头。它利用了 3D 高斯喷射 (3DGS) 的快速渲染功能，同时解决了直接用语音音频控制 3DGS 的挑战。GaussianTalker 构建了一个头部规范的 3DGS 表示，并使其与音频同步变形。一个关键的见解是将 3D 高斯属性编码成一个共享的隐式特征表示，其中它与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。与用于操纵大量高斯及其复杂参数的先前级联或乘法方法相比，它更稳定。           (2):GaussianTalker 提出了一种新的方法，该方法将 3D 高斯属性编码到一个共享的隐式特征表示中，该表示与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。然后将特征嵌入馈送到空间音频注意力模块，该模块预测每个高斯属性的逐帧偏移。           (3):GaussianTalker 采用多分辨率三平面表示来编码 3D 高斯特征，该表示利用了 3D 高斯隐式神经辐射场的空间信息。特征嵌入与音频特征融合，以准确建模由输入音频驱动的面部运动。</code></pre></li><li><p>结论：</p><pre><code>           (1):本工作提出了 GaussianTalker，这是一种用于实时姿态可控 3D 说话头合成的框架，利用 3D 高斯表示头部。我们的方法通过调节 3D 高斯原语，实现了对 3D 高斯原语的精确控制。           (2):创新点：GaussianTalker 提出了一种新方法，将 3D 高斯属性编码到一个共享的隐式特征表示中，该表示与音频特征合并以操纵每个高斯属性。这种设计利用了空间感知特征并强制相邻点之间的交互。性能：GaussianTalker 在面部真实度、唇部同步准确性和渲染速度方面优于以往的方法。具体来说，GaussianTalker 以高达 120 FPS 的显着渲染速度，超越了之前的基准。工作量：GaussianTalker 采用多分辨率三平面表示来编码 3D 高斯特征，该表示利用了 3D 高斯隐式神经辐射场的空间信息。特征嵌入与音频特征融合，以准确建模由输入音频驱动的面部运动。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ec62564096d07c9b5ec4f0c103bde8c1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e6d1f872d0b6fbc00f9aa1ae895fe7bd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-47a55dc6279dc78a414592ec16000227.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7262a04c0986b2720469c095a4a797a4.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c81028949da35d198f3a39ea50a55970.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v2">PDF</a> <a href="https://yuhongyun777.github.io/GaussianTalker/">https://yuhongyun777.github.io/GaussianTalker/</a></p><p><strong>Summary</strong><br>面部表情生成方法 GaussianTalker 以显式三维高斯斑点为基础，通过绑定高斯斑点到 3D 面部模型，实现面部动作的直观控制。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 3D 高斯斑点显式表示，实现面部动作的直观控制。</li><li>通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。</li><li>引入说话人特有混合形状，通过潜在姿势增强面部细节表示，提供稳定且逼真的渲染视频。</li><li>在面部表情生成中优于现有最先进方法，提供精确的唇部同步和出色的视觉质量。</li><li>渲染速度达到 130 FPS，显着高于实时渲染性能阈值。</li><li>可以部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：高斯话者：基于 3D 高斯斑点的特定说话者会说话的头合成</p></li><li><p>作者：洪云余、展权、启航余、建川陈、中华姜、志文陈、胜雨张、 Jimin Xu、Fei Wu、成飞吕、刚余</p></li><li><p>单位：阿里巴巴集团</p></li><li><p>关键词：音频驱动、会说话的头合成、神经辐射场、高斯斑点</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14037，Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：基于神经辐射场（NeRF）的音频驱动会说话的头合成方法取得了显著进展。然而，由于 NeRF 隐式表示引起的姿势和表情控制不足，这些方法仍存在一些局限性，如嘴唇动作不同步或不自然，以及视觉抖动和伪影。</p><p>（2）：过去方法及问题：现有方法存在以下问题：姿势和表情控制不足，导致嘴唇动作不自然、视觉抖动和伪影。</p><p>（3）：本文方法：本文提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker。GaussianTalker 由两个模块组成：特定说话者动作转换器和动态高斯渲染器。特定说话者动作转换器通过通用的音频特征提取和定制的嘴唇动作生成来实现特定于目标说话者的准确嘴唇动作。动态高斯渲染器引入了特定于说话者的混合形状，以控制面部表情。</p><p>（4）：方法性能：GaussianTalker 在唇形合成任务上取得了良好的性能，可以生成准确且自然的嘴唇动作。</p><ol><li>方法：</li></ol><p>（1）：Speaker-Specific Motion Translator：该模块负责将音频信号转换为特定于目标说话者的 FLAME 参数序列，用于面部动画控制。它由 Universal Audio Encoder 和 Customized Motion Decoder 组成。</p><p>（2）：Dynamic Gaussian Renderer：该模块利用 FLAME 驱动 3D 高斯斑点，并实时渲染动态说话头部。它引入特定于说话者的混合形状，以控制面部表情。</p><ol><li>结论：<pre><code>            (1):本文提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker，该方法将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差。特定说话者的 FLAME 转换器采用身份解耦和个性化嵌入来实现同步且自然的嘴唇运动，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定且逼真的渲染。大量实验表明，GaussianTalker 在说话头合成中优于最先进的性能，同时实现了超高的渲染速度，明显超过其他方法。我们相信这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。            (2):创新点：提出了一种基于 3D 高斯斑点的音频驱动会说话的头合成新方法 GaussianTalker，该方法将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差；             性能：在唇形合成任务上取得了良好的性能，可以生成准确且自然的嘴唇动作；             Workload：未提及。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-8b7befef7722d03c798c559087362540.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/Diffusion%20Models/</id>
    <published>2024-05-02T02:21:37.000Z</published>
    <updated>2024-05-02T02:21:37.091Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective"><a href="#Probing-Unlearned-Diffusion-Models-A-Transferable-Adversarial-Attack-Perspective" class="headerlink" title="Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective"></a>Probing Unlearned Diffusion Models: A Transferable Adversarial Attack   Perspective</h2><p><strong>Authors:Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong</strong></p><p>Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts. </p><p><a href="http://arxiv.org/abs/2404.19382v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型的反学习研究存在安全隐患，本文提出了一种迁移性的对抗攻击方法来探测反学习鲁棒性，在黑盒设置下有效恢复被擦除的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>反学习方法会将文本到图像的映射关系进行重新分布，但保留了扩散模型生成空间中的视觉内容。</li><li>目前探测反学习鲁棒性的方法缺乏可迁移性和攻击力。</li><li>本文提出了一个对抗性搜索策略，可以在黑盒设置下跨不同的反学习模型迁移对抗嵌入。</li><li>采用原始 Stable Diffusion 模型作为代理模型来迭代擦除和搜索嵌入。</li><li>实验表明，搜索的对抗嵌入可以跨多个最先进的反学习方法迁移，并且对不同层次的概念都能有效恢复。</li><li>本文提出的方法弥补了当前探测反学习鲁棒性方法的不足，为评估和提高反学习的有效性提供了新的思路。</li><li>该方法可以用于评估反学习的鲁棒性，并为提高反学习的有效性提供指导。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>论文标题：探测未学习扩散模型：可转移对抗攻击视角</p></li><li><p>作者：</p><ul><li>Xiaoxuan Han</li><li>Songlin Yang</li><li>Wei Wang</li><li>Yang Li</li><li>Jing Dong</li></ul></li><li><p>第一作者单位：中国科学院大学</p></li><li><p>关键词：</p><ul><li>Diffusion Model</li><li>Machine Unlearning</li><li>Adversarial Attack</li></ul></li><li><p>论文链接：https://arxiv.org/abs/2404.19382    Github 链接：https://github.com/hxxdtd/PUND</p></li><li><p>摘要：</p><p>(1) 研究背景：    文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但也带来了身份隐私、版权和不安全内容等安全问题。为了缓解这些问题，提出了概念擦除方法来消除涉及的概念。</p><p>(2) 过往方法与问题：    现有的概念擦除方法通过改变文本到图像的映射来实现“擦除”任务，但未能擦除扩散模型生成空间内的视觉内容，为恢复这些擦除的概念留下了致命缺陷。此外，这些方法缺乏可转移性，并且在恢复狭窄概念（例如名人身份）方面存在局限性。</p><p>(3) 本文方法：    本文提出了一种可转移的对抗攻击来探测未学习扩散模型的鲁棒性。该攻击采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，以找到可以在不同的未学习模型之间转移的对抗嵌入。</p><p>(4) 实验结果：    实验表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。这些结果支持了本文提出的方法能够探测未学习扩散模型的鲁棒性，并为恢复擦除的概念提供了新的思路。</p></li><li><p>方法：</p><p>（1）：提出一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性；</p><p>（2）：采用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，找到可以在不同未学习模型之间转移的对抗嵌入；</p><p>（3）：使用对抗嵌入作为攻击源，在不同的未学习扩散模型上进行攻击，评估模型的鲁棒性；</p><p>（4）：通过实验验证所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。</p></li><li><p>结论：</p><pre><code>            (1):本文提出了一种可转移的对抗攻击方法，探测未学习扩散模型的鲁棒性，为恢复擦除的概念提供了新的思路。            (2):Innovation point: 提出了一种可转移的对抗攻击方法，该方法在不同未学习模型之间具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Performance: 实验结果表明，所搜索的对抗嵌入在各种最先进的未学习方法中具有可转移性，并且在从宽泛到狭窄的不同概念级别上都表现出有效性。Workload: 该方法需要使用对抗搜索策略，在原始 Stable Diffusion 模型上迭代擦除和搜索嵌入，这可能需要大量的计算资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-aeb2a7c17e04ea32837496f134911073.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3cfab2dba37aac49c7649b71ac867d79.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0aa15fec53d79c3279c72f74772273b9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8177a4229cfdb42440835f0ee9e56c19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-67758faf074d7114c762f4a57a5d1403.jpg" align="middle"><img src="https://picx.zhimg.com/v2-471f68a1caec8d9bae0fe6402d798203.jpg" align="middle"></details><h2 id="TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation"><a href="#TheaterGen-Character-Management-with-LLM-for-Consistent-Multi-turn-Image-Generation" class="headerlink" title="TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation"></a>TheaterGen: Character Management with LLM for Consistent Multi-turn   Image Generation</h2><p><strong>Authors:Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</strong></p><p>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a “Screenwriter”, engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the “Rehearsal”. Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the “Final Performance”. With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity. </p><p><a href="http://arxiv.org/abs/2404.18919v1">PDF</a> </p><p><strong>Summary</strong><br>对话生成模型 TheaterGen 无需额外训练，即可实现文本到图像的多轮生成，提升图像语义和上下文一致性。</p><p><strong>Key Takeaways</strong></p><ul><li>TheaterGen 创新性地将大语言模型融入文本到图像模型，实现多轮图像生成。</li><li>大语言模型作为“编剧”，生成标准化提示手册，管理角色提示和设计。</li><li>TheaterGen 基于手册生成角色图像，提取指导信息。</li><li>反向去噪过程将手册和指导信息融入扩散模型，生成图像。</li><li>CMIGBench 是首个不预先定义角色的多轮图像生成基准测试。</li><li>TheaterGen 显著优于其他方法，在 Mini DALLE 3 模型上提升平均角色相似度 21%，平均文本图像相似度 19%。</li><li>TheaterGen 可用于故事生成和多轮编辑任务。</li><li>TheaterGen 为文本到图像生成领域带来了突破。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: TheaterGen：基于 LLM 的多轮图像生成中的角色管理</p></li><li><p>Authors: Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, YuxinHe, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, XiaodanLiang</p></li><li><p>Affiliation: 中山大学深圳校区</p></li><li><p>Keywords: Diffusion models · Consistency · Multi-turn image generation</p></li><li><p>Urls: https://howe140.github.io/theatergen.io/ , Github: https://github.com/donahowe/Theatergen</p></li><li><p>Summary:</p><pre><code>           (1): 当前的扩散模型在文本生成图像方面取得了显著进展。然而，在实际场景中需求较高的多轮图像生成仍然面临着图像和文本之间的语义一致性以及同一主题在多个交互轮次中的上下文一致性等挑战。           (2): 现有的方法主要集中在文本提示的改进和扩散模型的训练上，但对于多轮图像生成中的角色管理和一致性问题关注较少。           (3): 本文提出 TheaterGen，这是一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。在这个框架中，LLM 充当“编剧”，参与多轮交互，生成和管理一个标准化的提示手册，其中包含目标图像中每个角色的提示和布局设计。基于角色提示和布局，生成角色图像列表并从中提取指导信息，类似于“排练”。随后，通过将提示手册和指导信息融入 T2I 扩散模型的反向去噪过程中，生成最终图像，即进行“最终表演”。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。           (4): 广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；</p><p>（2）：设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；</p><p>（3）：将提示手册和两种引导输入基于角色的生成器中，合成最终图像。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 TheaterGen，一个无训练框架，它集成了大语言模型 (LLM) 和文本到图像 (T2I) 模型，以提供多轮图像生成的能力。通过有效管理提示手册和角色图像，TheaterGen 显着提高了合成图像中的语义和上下文一致性。此外，我们引入了包含 8000 个多轮指令的专门基准 CMIGBench（一致的多轮图像生成基准）。与以前的多轮基准不同，CMIGBench 不预先定义角色，因此具有很大的多样性。CMIGBench 上包含故事生成和多轮编辑任务，以进行综合评估。</p><p>（2）：创新点：提出基于大语言模型（LLM）的角色设计师，生成包含角色提示和布局的结构化提示手册；设计角色图像管理器，生成舞台角色图像及其对应的线条引导和潜在引导；将提示手册和两种引导输入基于角色的生成器中，合成最终图像。性能：广泛的实验结果表明，TheaterGen 明显优于最先进的方法。例如，它将尖端的 Mini DALL·E 3 模型在平均字符-字符相似度方面的性能提高了 21%，在平均文本-图像相似度方面的性能提高了 19%。工作量：TheaterGen 是一个无训练框架，不需要额外的训练成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-9ba365cfc612e009b79d484c29a30149.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fce5b92ae3c1c7350697723f803ec2cb.jpg" align="middle"><img src="https://pica.zhimg.com/v2-568b0d8a34639fe3e5425bc5cb460f4b.jpg" align="middle"></details>## FlexiFilm: Long Video Generation with Flexible Conditions**Authors:Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, Bo zhao**Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/ [PDF](http://arxiv.org/abs/2404.18620v1) 9 pages, 9 figures**Summary**针对长视频生成这一重大挑战，本文提出 FlexiFilm，一种专为长视频生成而设计的扩散模型。**Key Takeaways**- 现有基于扩散的视频生成模型在生成长视频时性能下降。- FlexiFilm 引入了时间条件器，以建立生成与多模态条件之间更一致的关系。- FlexiFilm 使用再采样策略来解决过度曝光问题。- FlexiFilm 生成的长视频超过 30 秒，时间一致性好。- FlexiFilm 在定性和定量分析中均优于竞争对手。- FlexiFilm 能生成内容丰富、具有时间连贯性的长视频。- FlexiFilm 为长视频生成提供了新的思路。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: FlexiFilm: 长视频生成中的时空一致性 (FlexiFilm: Temporal Coherence in Long Video Generation)</p></li><li><p>Authors: Yichen Ouyang, Jianhao Yuan, Hao Zhao, Tiejun Huang, Gaoang Wang, Bo Zhao</p></li><li><p>Affiliation: 南京大学 (Nanjing University)</p></li><li><p>Keywords: Long video generation, Diffusion models, Temporal conditioner, Resampling strategy</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.09413, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 长视频生成面临时空一致性挑战，现有基于扩散的视频生成模型在长视频生成中表现不佳。</p><p>(2): 过去的方法采用简单的条件机制和采样策略，导致时空不一致和过曝问题。</p><p>(3): 本文提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型。FlexiFilm 引入时间条件器，建立生成和多模态条件之间更一致的关系，并采用重采样策略解决过曝问题。</p><p>(4): 在长视频生成任务上，FlexiFilm 优于竞争对手，生成长度超过 30 秒的长且一致的视频。定量和定性分析都支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出 FlexiFilm，一种针对长视频生成量身定制的扩散模型，引入时间条件器，建立生成和多模态条件之间更一致的关系；</p><p>（2）：采用重采样策略解决过曝问题，在长视频生成任务上优于竞争对手，生成长度超过 30 秒的长且一致的视频。</p><ol><li>结论：<pre><code>            (1):本工作针对长视频生成中时空一致性问题，提出 FlexiFilm 模型，有效提升了长视频生成质量；            (2):创新点：引入时间条件器和重采样策略，增强时空一致性；性能：在长视频生成任务上优于竞争对手，可生成长度超过 30 秒的长且一致的视频；工作量：模型设计和训练复杂度较高。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-89f7187f1074067e636b6cefcd03214c.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fc7b975f21081a9007db0c1ec2d26248.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-dcfbf96f6700552f8cbb6108717b928b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-26f03063378af5e36436e73a3bc39c46.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3bf4fac4e6634e90aecfc106469774e4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-639e75c0d686596adb3d0c89cc48bb9c.jpg" align="middle"></details><h2 id="Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting"><a href="#Anywhere-A-Multi-Agent-Framework-for-Reliable-and-Diverse-Foreground-Conditioned-Image-Inpainting" class="headerlink" title="Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting"></a>Anywhere: A Multi-Agent Framework for Reliable and Diverse   Foreground-Conditioned Image Inpainting</h2><p><strong>Authors:Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</strong></p><p>Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as “over-imagination”, inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating “over-imagination”, resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results. </p><p><a href="http://arxiv.org/abs/2404.18598v1">PDF</a> 16 pages, 9 figures, project page:   <a href="https://anywheremultiagent.github.io">https://anywheremultiagent.github.io</a></p><p><strong>Summary</strong><br>通过 Anywhere 多智能体框架，利用 VLM、LLM 和图像生成模型，通过语义分析、文本引导的图像生成和结果分析器，实现前景调控图像修复，解决过度想象、前景背景不一致和多样性差的问题。</p><p><strong>Key Takeaways</strong></p><ul><li>引入 Anywhere，一种用于前景调控图像修复的多智能体框架。</li><li>利用 VLM 和 LLM 进行语义分析，生成最佳语言提示。</li><li>使用文本引导的 Canny-to-Image 生成模型创建模板图像。</li><li>使用图像精炼器融合输入前景和模板图像以生成输出。</li><li>使用 VLM 进行结果分析，评估图像内容合理性、美学分数和前景背景相关性。</li><li>触发提示和图像再生，以解决过度想象、前景背景差异和多样性差的问题。</li><li>Anywhere 框架在前景调控图像修复中表现出色，生成更可靠、更多样化的结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ANYWHERE：基于前景条件的图像修复的多智能体框架</p></li><li><p>Authors: Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</p></li><li><p>Affiliation: 南京大学新软件技术国家重点实验室</p></li><li><p>Keywords: Image inpainting, Multi-agent framework, Foreground-conditioned, Diversity, Reliability</p></li><li><p>Urls: https://arxiv.org/abs/2404.18598v1, https://github.com/anywheremultiagent/anywhere</p></li><li><p>Summary: </p><pre><code>            (1):图像修复在图像处理中有着重要的应用，近年来基于扩散模型的图像修复方法取得了显著进展。然而，当应用于基于前景对象完成图像的任务时，现有的端到端图像修复方法面临着“过度想象”、“前景与背景不一致”以及多样性受限等挑战。            (2):针对上述问题，本文提出了一个开创性的多智能体框架——Anywhere。Anywhere采用了一个复杂的管道框架，包括视觉语言模型（VLM）、大语言模型（LLM）和图像生成模型等多种智能体。该框架由三个主要组件组成：提示生成模块、图像生成模块和结果分析器。提示生成模块对输入的前景图像进行语义分析，利用VLM预测相关的语言描述，并利用LLM推荐最优的语言提示。在图像生成模块中，我们采用了一个文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器通过融合输入前景和模板图像生成结果。结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。            (3):广泛的实验表明，我们的Anywhere框架在基于前景条件的图像修复任务中表现出色，它缓解了“过度想象”，解决了前景与背景的不一致性，并增强了多样性。它成功地将基于前景条件的图像修复提升到了一个新的水平，产生了更加可靠和多样化的结果。            (4):在基于前景条件的图像修复任务上，Anywhere框架取得了比现有方法更好的性能，证明了其方法的有效性。</code></pre></li><li><p>方法：</p></li></ol><p>（1）：提出一个多智能体框架——Anywhere，该框架由提示生成模块、图像生成模块和结果分析器组成；</p><p>（2）：提示生成模块利用视觉语言模型（VLM）和大语言模型（LLM）预测相关的语言描述和推荐最优的语言提示；</p><p>（3）：图像生成模块采用文本引导的Canny-to-Image生成模型，基于前景图像的边缘图和语言提示创建模板图像，并使用图像精修器生成结果；</p><p>（4）：结果分析器利用VLM评估图像内容合理性、美学分数和前景-背景相关性，并根据需要触发提示和图像再生。</p><ol><li>结论：</li></ol><p>（1）本工作提出了一个多智能体框架 Anywhere，该框架有效解决了基于前景条件的图像修复任务中存在的“过度想象”、“前景与背景不一致”以及多样性受限等挑战，将基于前景条件的图像修复提升到了一个新的水平；</p><p>（2）创新点：Anywhere 采用了多智能体框架，结合了视觉语言模型、大语言模型和图像生成模型，实现了基于前景条件的图像修复任务的端到端完成；</p><p>性能：Anywhere 在基于前景条件的图像修复任务上取得了比现有方法更好的性能，证明了其方法的有效性；</p><p>工作量：Anywhere 框架的实现相对复杂，需要训练多个智能体模型，工作量较大。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-b60b160bb6aabb892081fb4dd065859c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0b5d570ae9275bbd4b3e2d5946151c0d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ba6b19fe809bc2888d9a6c4f365915d1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7e07715dcf6b24c4a172db98d4808c7b.jpg" align="middle"></details>## Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View   Diffusion Model**Authors:Xiaolong Li, Jiawei Mo, Ying Wang, Chethan Parameshwara, Xiaohan Fei, Ashwin Swaminathan, CJ Taylor, Zhuowen Tu, Paolo Favaro, Stefano Soatto**In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt. [PDF](http://arxiv.org/abs/2404.18065v1) 9 pages, 10 figures**摘要：**融合文本引导的中间表示和混合优化策略，从复合文本提示生成高质量且准确的3D资产。**要点：*** 利用文本引导的4视图图像作为文本到3D生成中的bottleneck。* 引入注意力重新聚焦机制，促进文本对齐的4视图图像生成。* 提出混合优化策略，鼓励SDS损失函数和稀疏RGB参考图像之间的协同作用。* 大幅优于现有的SOTA方法，在合成3D资产的质量和准确性上都表现出色。* 支持根据同一文本提示生成多样化的3D资产。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：基于预训练多视图的接地式组合式和多样化文本到 3D</li><li>作者：Xudong Zhang, Huchuan Lu, Yinda Zhang, Xiaoguang Han, Joshua B. Tenenbaum, Jiajun Wu</li><li>单位：麻省理工学院（Massachusetts Institute of Technology）</li><li>关键词：文本到 3D、组合式生成、多视图扩散模型、接地式合成</li><li>论文链接：https://arxiv.org/pdf/2302.04742.pdf，Github：无</li><li><p>摘要：（1）：研究背景：现有的文本到 3D 方法在生成组合式文本提示时存在困难，经常遗漏某些主体或部分。（2）：过去方法：MVDream 等多视图扩散模型可以生成高保真 3D 资产，但无法理解组合式文本提示。（3）：研究方法：提出了一种两阶段方法 Grounded-Dreamer，利用预训练的多视图扩散模型，通过注意重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用。（4）：方法性能：方法在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D。</p></li><li><p>Methods:</p></li></ol><p>（1）：利用预训练的多视图扩散模型 MVDream，通过注意重新聚焦机制和混合优化策略，生成文本对齐的 4 视图图像。</p><p>（2）：采用 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成。</p><p>（3）：使用文本提示作为条件，生成多样化的 3D 资产。</p><ol><li>结论：<pre><code>            (1):本文提出了一种新颖的文本到 3D 合成两阶段框架，有效克服了组合准确性和多样性的挑战。第一阶段利用多视图扩散模型从文本生成空间相干视图，而第二阶段将稀疏视图 NeRF 与文本引导扩散先验协同起来，用于精细的 3D 重建。这种方法不仅提高了复杂文本提示生成 3D 模型的保真度和组合完整性，还为未来无缝 2D 到 3D 过渡和模型多功能性方面的探索铺平了道路。我们的方法展示了文本到 3D 合成方面的重大飞跃，为该不断发展的领域的进一步进步提供了坚实的基础。            (2):创新点：利用预训练多视图扩散模型，通过注意力重新聚焦机制和混合优化策略，鼓励文本对齐的 4 视图图像生成，并促进 SDS 损失和稀疏 RGB 参考图像之间的协同作用，促进 3D 资产的生成；性能：在生成组合式 3D 资产方面优于现有技术，在质量和准确性方面均表现出色，并能够从相同的文本提示中生成多样化的 3D；工作量：利用了预训练的多视图扩散模型，减少了训练时间和计算资源的消耗。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-0d3bc1be854ed564fddf7ab8d560de5f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-abb4a9d2f1e50d8adfd617a0dc760606.jpg" align="middle"><img src="https://picx.zhimg.com/v2-abe084fa386e134319b922f3543204fa.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ae1872641ac814aff738475c08d64d2b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bab3fbf4b4460d73196cb6abddbb1b4f.jpg" align="middle"></details><h2 id="Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling"><a href="#Unsupervised-Anomaly-Detection-via-Masked-Diffusion-Posterior-Sampling" class="headerlink" title="Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling"></a>Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</h2><p><strong>Authors:Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</strong></p><p>Reconstruction-based methods have been commonly used for unsupervised anomaly detection, in which a normal image is reconstructed and compared with the given test image to detect and locate anomalies. Recently, diffusion models have shown promising applications for anomaly detection due to their powerful generative ability. However, these models lack strict mathematical support for normal image reconstruction and unexpectedly suffer from low reconstruction quality. To address these issues, this paper proposes a novel and highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS). In MDPS, the problem of normal image reconstruction is mathematically modeled as multiple diffusion posterior sampling for normal images based on the devised masked noisy observation model and the diffusion-based normal image prior under Bayesian framework. Using a metric designed from pixel-level and perceptual-level perspectives, MDPS can effectively compute the difference map between each normal posterior sample and the given test image. Anomaly scores are obtained by averaging all difference maps for multiple posterior samples. Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can achieve state-of-the-art performance in normal image reconstruction quality as well as anomaly detection and localization. </p><p><a href="http://arxiv.org/abs/2404.17900v1">PDF</a> </p><p><strong>Summary</strong><br>利用扩散模型对正态图像采样，并通过差分映射计算异常分数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在异常检测中展现出了良好的应用前景。</li><li>MDPS 方法将图像重建问题数学建模为基于掩码噪声观测模型和基于贝叶斯框架的扩散图像先验的正态图像后验采样。</li><li>MDPS 可以有效地计算每次正态后验样本和给定测试图像之间的差分映射。</li><li>异常分数通过对多个后验样本的差分映射进行平均得到。</li><li>MDPS 在图像重建质量和异常检测与定位方面均取得了最先进的效果。</li><li>MDPS 具有较高的可解释性，为异常检测提供了新的思路。</li><li>MDPS 在 MVTec 和 BTAD 数据集上进行了全面实验，证明了其优越性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 无监督异常检测的掩码扩散后验采样</p></li><li><p>Authors: Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</p></li><li><p>Affiliation: 电子科技大学自动化工程学院</p></li><li><p>Keywords: 无监督异常检测, 扩散模型, 后验采样, 掩码噪声观测模型</p></li><li><p>Urls: https://arxiv.org/abs/2404.17900, Github: https://github.com/KevinBHLin/</p></li><li><p>Summary:</p></li></ol><p>(1): 异常检测是计算机视觉领域的一项基本任务，在医学诊断、智能制造、自动驾驶等领域有着广泛的应用。由于异常样本的稀有性和多样性，近年来研究主要集中在无监督异常检测上，即模型只从正常样本中学习，但可以检测异常数据。</p><p>(2): 现有的无监督异常检测方法主要有重建方法、生成对抗网络方法、扩散模型方法等。其中，重建方法是较早且最常用的神经网络方法之一。然而，传统的重建模型如自动编码器和生成对抗网络存在重建质量较差、训练不稳定等问题。</p><p>(3): 本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。异常分数通过对多个后验样本的差异图求平均得到。</p><p>(4): 在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能。这些性能足以支持作者提出的目标。</p><ol><li><p>Methods: </p><p>(1):本文提出了一种基于贝叶斯框架的无监督异常检测新方法，称为掩码扩散后验采样(MDPS)。</p><p>(2):MDPS将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样。</p><p>(3):MDPS设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图。</p><p>(4):异常分数通过对多个后验样本的差异图求平均得到。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于贝叶斯框架的无监督异常检测新方法 MDPS，该方法将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量，可以有效计算每个正常后验样本与给定测试图像之间的差异图，异常分数通过对多个后验样本的差异图求平均得到。</p><p>（2）：创新点：MDPS基于贝叶斯框架，将正常图像重建问题数学建模为基于掩码噪声观测模型和扩散模型的正常图像后验采样，并设计了一种从像素级和感知级角度设计的度量；性能：在 MVTec 和 BTAD 数据集上进行的详尽实验表明，MDPS 在正常图像重建质量、异常检测和定位方面均取得了最先进的性能；工作量：MDPS 由于扩散后验采样而导致较高的计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-9016eceac1926a6b34927f7b8fe1c178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4b0adf35bda326a41bfa8fc38c5b7545.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5e8db92be0dd5370c893fb23a6f36582.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3954ca6b4dc2d3e932f8670609df6e6f.jpg" align="middle"></details><h2 id="Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models"><a href="#Causal-Diffusion-Autoencoders-Toward-Counterfactual-Generation-via-Diffusion-Probabilistic-Models" class="headerlink" title="Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models"></a>Causal Diffusion Autoencoders: Toward Counterfactual Generation via   Diffusion Probabilistic Models</h2><p><strong>Authors:Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</strong></p><p>Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images. </p><p><a href="http://arxiv.org/abs/2404.17735v1">PDF</a> </p><p><strong>摘要：</strong></p><p>基于扩散的因果表征学习框架 CausalDiffAE，通过指定因果模型实现反事实生成。</p><p><strong>关键要点：</strong></p><ul><li>提出 CausalDiffAE，将编码器与逆扩散相结合，从高维数据中提取因果表征。</li><li>通过因果编码机制将数据映射到因果相关潜在因子。</li><li>使用神经网络参数化潜在因子之间的因果机制。</li><li>提出变分目标和辅助标签信息来解纠缠因果变量。</li><li>基于 DDIM 提出受干预影响的反事实生成程序。</li><li>研究了仅部分训练数据有标签的情况，可在推理中细粒度控制反事实的干预强度。</li><li>实证表明 CausalDiffAE 学习到了解纠缠的潜在空间，能够生成高质量的反事实图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：因果扩散自编码器：基于扩散概率模型的反事实生成</li><p></p><p></p><li>作者：Aneesh Komanduri、Chen Zhao、Feng Chen、Xintao Wu</li><p></p><p></p><li>第一作者单位：阿肯色大学</li><p></p><p></p><li>关键词：扩散概率模型、因果建模、反事实生成、表示学习</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.17735, Github代码链接：无</li><p></p><p></p><li>摘要：(1) 研究背景：扩散概率模型（DPMs）在高质量图像生成方面已成为最先进的技术。然而，DPMs 具有任意噪声潜在空间，没有可解释或可控制的语义。尽管在提高图像样本质量方面进行了大量的研究，但使用扩散模型进行表示控制生成的工作却很少。具体来说，使用 DPMs 进行因果建模和可控反事实生成是一个尚未探索的领域。(2) 过去的方法及其问题：目前还没有关于使用 DPMs 进行因果建模和反事实生成的工作。这限制了 DPMs 在需要因果推理和反事实生成的任务中的应用。(3) 本文提出的研究方法：本文提出了 CausalDiffAE，一个基于扩散的因果表示学习框架，以根据指定的因果模型生成反事实。CausalDiffAE 使用编码器从高维数据中提取高级语义因果变量，并使用反向扩散对随机变化进行建模。它提出了一个因果编码机制，将高维数据映射到因果相关的潜在因子，并使用神经网络参数化潜在因子之间的因果机制。为了强制因果变量的解纠缠，CausalDiffAE 制定了变分目标并利用先验中的辅助标签信息来正则化潜在空间。它提出了一个基于 DDIM 的反事实生成过程，该过程受 do 干预的影响。最后，为了解决标签监督有限的情况，CausalDiffAE 还研究了在训练数据的一部分未标记时的应用，这也使得在推理过程中对生成反事实的干预强度进行精细控制。(4) 本文方法在什么任务上取得了什么性能：实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。该性能支持了本文提出的因果表示学习框架在反事实生成任务中的有效性。</li><br>&lt;/ol&gt;<p></p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论：</strong></p><p><strong>(1)：本工作的意义是什么？</strong>本工作提出了 CausalDiffAE，这是一个基于扩散的因果表示学习和反事实生成框架。我们提出了一个因果编码机制，将图像映射到因果相关的因子。我们通过神经网络学习因子之间的因果机制。我们制定了一个基于变分的扩散目标来强制潜在空间的解纠缠，以实现潜在空间操作。我们提出了一个基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。对于有限监督的情况，我们提出了我们模型的弱监督扩展，它联合学习了一个无条件模型和一个条件模型。该目标还使得能够对生成的 counterfactuals 进行细粒度控制。我们使用定性和定量指标实证地展示了我们模型的能力。未来的工作包括探索文本到图像扩散模型中的反事实生成。</p><p><strong>(2)：总结本文在创新点、性能和工作量三个维度上的优缺点：</strong><strong>创新点：</strong>* 提出了一种基于扩散的因果表示学习框架 CausalDiffAE。* 提出了一种因果编码机制，将高维数据映射到因果相关的潜在因子。* 提出了一种基于变分的扩散目标来强制潜在空间的解纠缠。* 提出了一种基于 DDIM 的反事实生成算法，该算法受 do 干预的影响。</p><p><strong>性能：</strong>* 实验表明，CausalDiffAE 学习了一个解纠缠的潜在空间，并且能够生成高质量的反事实图像。</p><p><strong>工作量：</strong>* 该方法需要大量的数据和计算资源来训练。* 该方法在训练过程中需要大量的超参数调整。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-89030cb7b49450895338abca619e996e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0118f1cc7ce9364c178c9f49ae8f2863.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-02  Probing Unlearned Diffusion Models A Transferable Adversarial Attack   Perspective</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>元宇宙/虚拟人</title>
    <link href="https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    <id>https://kedreamix.github.io/2024/05/02/Paper/2024-05-02/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</id>
    <published>2024-05-02T02:00:16.000Z</published>
    <updated>2024-05-02T02:00:16.575Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-05-02-更新"><a href="#2024-05-02-更新" class="headerlink" title="2024-05-02 更新"></a>2024-05-02 更新</h1><h2 id="EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars"><a href="#EMOPortraits-Emotion-enhanced-Multimodal-One-shot-Head-Avatars" class="headerlink" title="EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars"></a>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</h2><p><strong>Authors:Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</strong></p><p>Head avatars animated by visual signals have gained popularity, particularly in cross-driving synthesis where the driver differs from the animated character, a challenging but highly practical approach. The recently presented MegaPortraits model has demonstrated state-of-the-art results in this domain. We conduct a deep examination and evaluation of this model, with a particular focus on its latent space for facial expression descriptors, and uncover several limitations with its ability to express intense face motions. To address these limitations, we propose substantial changes in both training pipeline and model architecture, to introduce our EMOPortraits model, where we:   Enhance the model’s capability to faithfully support intense, asymmetric face expressions, setting a new state-of-the-art result in the emotion transfer task, surpassing previous methods in both metrics and quality.   Incorporate speech-driven mode to our model, achieving top-tier performance in audio-driven facial animation, making it possible to drive source identity through diverse modalities, including visual signal, audio, or a blend of both.   We propose a novel multi-view video dataset featuring a wide range of intense and asymmetric facial expressions, filling the gap with absence of such data in existing datasets. </p><p><a href="http://arxiv.org/abs/2404.19110v1">PDF</a> </p><p><strong>Summary</strong><br>虚拟人头部通过视觉信号动画驱动，在跨驱动合成中尤其受欢迎，这是一种极具挑战但非常实用的方法。</p><p><strong>Key Takeaways</strong></p><ul><li>MegaPortraits 模型在表情描述符的潜在空间方面存在局限性，无法表达强烈的面部动作。</li><li>EMOPortraits 模型对训练管道和模型架构进行了重大更改，以增强模型对强烈、不对称面部表情的忠实支持。</li><li>EMOPortraits 模型在情感转移任务中取得了新的最先进的结果，在指标和质量方面都超越了以前的方法。</li><li>EMOPortraits 模型结合了基于语音的模式，在音频驱动的面部动画中实现了顶级性能。</li><li>EMOPortraits 模型支持通过视觉信号、音频或两者结合的多种方式驱动源身份。</li><li>提出一个新的多视图视频数据集，包含广泛的强烈和不对称面部表情。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: EMOPortraits: 情绪增强多模态一发头像</p></li><li><p>Authors: Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, Maja Pantic</p></li><li><p>Affiliation: 帝国理工学院</p></li><li><p>Keywords: 头部化身、面部表情、情感传递、语音驱动、多模态</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19110v1, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 研究背景：头部化身动画在跨驱动合成中越来越流行，其中驱动者与动画角色不同，这是一种具有挑战性但非常实用的方法。最近提出的 MegaPortraits 模型在这个领域展示了最先进的结果。</p><p>(2): 过去的方法：研究人员对 MegaPortraits 模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，发现其表达强烈面部动作的能力存在一些限制。</p><p>(3): 研究方法：为了解决这些限制，研究人员在训练管道和模型架构中提出了实质性的改变，引入了 EMOPortraits 模型，其中：   - 增强了模型忠实支持强烈、不对称面部表情的能力，在情感传递任务中设定了新的最先进结果，在指标和质量上都超过了以前的方法。   - 将语音驱动模式纳入模型，在音频驱动的面部动画中实现了顶级性能，使得可以通过视觉信号、音频或两者混合等多种方式驱动源身份。   - 提出了一组新的多视角视频数据集，其中包含广泛的强烈和不对称面部表情，填补了现有数据集中缺乏此类数据的情况。</p><p>(4): 性能和效果：在情感传递任务上，EMOPortraits 模型在指标和质量上都超过了以前的方法，设定了新的最先进的结果。在音频驱动的面部动画中，该模型也取得了顶级性能。这些性能支持了研究人员的目标，即开发一个能够通过多种方式驱动源身份的头部化身动画模型。</p><ol><li><p>方法：    (1): 对 MegaPortraits 模型进行深入检查和评估，发现其在表达强烈面部动作方面存在限制；    (2): 在训练管道和模型架构中提出实质性改变，引入 EMOPortraits 模型；    (3): 增强模型忠实支持强烈、不对称面部表情的能力；    (4): 将语音驱动模式纳入模型，实现音频驱动的面部动画顶级性能；    (5): 提出多视角视频数据集，填补现有数据集中缺乏强烈和不对称面部表情数据的情况。</p></li><li><p>结论：</p></li></ol><p>（1）本文提出了一种新颖的方法 EMOPortraits，用于创建神经头像，在图像驱动、跨身份情绪转换方面具有卓越的性能。我们的语音驱动模式使得可以通过多种条件（视频、音频、头部运动）来驱动面部动画。我们收集了 FEED 数据集，我们相信这将成为从事多元化以人为中心研究的研究人员的宝贵资产。然而，我们的方法也存在一些局限性。它不会生成头像的身体或肩膀，限制了一些用例。我们目前将我们的输出与源图像主体集成在一起。此外，该模型有时难以进行准确的表情转换，并且在头部大幅旋转时性能不佳。这些挑战对于未来的增强至关重要，并且仍然是我们正在进行的研究工作的核心。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ae09218eb625859aeda612581ba59975.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76da3d8060bb28f6e1488ffdcf42c493.jpg" align="middle"><img src="https://picx.zhimg.com/v2-07199851d15b47c4d1a719b68cd3f240.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-29a1efddd95063c164480f3a84bf5f72.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc5fc34eb617f15c5ecceee7d25f9f5c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e7948dbe17eb67516e7078da09fc10ae.jpg" align="middle"></details><h2 id="MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing"><a href="#MeGA-Hybrid-Mesh-Gaussian-Head-Avatar-for-High-Fidelity-Rendering-and-Head-Editing" class="headerlink" title="MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing"></a>MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and   Head Editing</h2><p><strong>Authors:Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</strong></p><p>Creating high-fidelity head avatars from multi-view videos is a core issue for many AR/VR applications. However, existing methods usually struggle to obtain high-quality renderings for all different head components simultaneously since they use one single representation to model components with drastically different characteristics (e.g., skin vs. hair). In this paper, we propose a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we select an enhanced FLAME mesh as our facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic renderings, we obtain facial colors using deferred neural rendering and disentangle neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports more downstream tasks. Experiments on the NeRSemble dataset demonstrate the effectiveness of our designs, outperforming previous state-of-the-art methods and supporting various editing functionalities, including hairstyle alteration and texture editing. </p><p><a href="http://arxiv.org/abs/2404.19026v1">PDF</a> Project page: <a href="https://conallwang.github.io/MeGA_Pages/">https://conallwang.github.io/MeGA_Pages/</a></p><p><strong>Summary</strong><br>多模态表情虚拟人头部建模方法 MeGA: 使用网格融合高斯模型，为不同头部组件提供更合适的表征方法。</p><p><strong>Key Takeaways</strong></p><ul><li>提出一种混合网格-高斯虚拟人头部建模方案 MeGA。</li><li>选择增强型 FLAME 网格作为面部表征，并预测 UV 位移图以提供逐顶点偏移，实现个性化几何细节。</li><li>采用延迟神经渲染技术获取面部颜色，并将神经纹理分解为三个有意义的部分，实现逼真的渲染。</li><li>使用 3D 高斯溅射构建静态规范头发，利用刚体变换和基于 MLP 的变形场处理复杂的动态表情。</li><li>结合遮挡感知融合，MeGA 为整个头部生成更高保真度的渲染，并支持更多下游任务。</li><li>在 NeRSemble 数据集上的实验表明，MeGA 优于现有的最先进方法，并支持发型改变和纹理编辑等多种编辑功能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MeGA：混合网格-高斯头部头像（中文翻译：MeGA：用于高保真渲染和头部编辑的混合网格-高斯头部头像）</p></li><li><p>Authors: Cong Wang, Di Kang, He-Yi Sun, Shen-Han Qian, Zi-Xuan Wang, Linchao Bao, Song-Hai Zhang</p></li><li><p>Affiliation: 清华大学（中文翻译：清华大学）</p></li><li><p>Keywords: Head Avatar, High-Fidelity Rendering, Head Editing, Mesh, Gaussian</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.19026 , Github: None</p></li><li><p>Summary:</p></li></ol><p>（1）：研究背景：高保真头部头像的创建是 AR/VR 应用中的核心问题，但现有方法通常难以同时为所有不同的头部组件获得高质量的渲染，因为它们使用单一表示来建模具有截然不同特征的组件（例如，皮肤与头发）。</p><p>（2）：过去的方法：现有方法探索了基于网格、基于 NeRF 和基于 3D 高斯的表示，取得了显着进展。然而，由于人类头部是一个包含具有截然不同特征的组件（例如，皮肤与头发）的复杂“物体”，因此可能不存在可以同时很好地建模所有这些组件的单一表示。</p><p>（3）：研究方法：本文提出了一种混合网格-高斯头部头像（MeGA），它使用更合适的表示来建模不同的头部组件。具体来说，我们选择一个增强的 FLAME 网格作为我们的面部表示，并预测一个 UV 置换贴图来提供每个顶点的偏移量，以改进个性化的几何细节。为了实现逼真的渲染，我们使用延迟神经渲染获取面部颜色，并将神经纹理分解为三个有意义的部分。对于头发建模，我们首先使用 3D 高斯点云构建静态规范头发。然后应用刚性变换和基于 MLP 的变形场来处理复杂的动态表情。结合我们的遮挡感知混合，MeGA 为整个头部生成更高保真的渲染，并自然地支持更多下游任务。</p><p>（4）：任务与性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法，并支持各种编辑功能，包括发型改变和纹理编辑。</p><ol><li><p>Methods: </p><pre><code>            (1):提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；            (2):选择增强的 FLAME 网格作为面部表示，预测 UV 置换贴图提供顶点偏移量，改进个性化几何细节；            (3):使用延迟神经渲染获取面部颜色，将神经纹理分解为三个有意义的部分，实现逼真渲染；            (4):使用 3D 高斯点云构建静态规范头发，应用刚性变换和基于 MLP 的变形场处理表情；            (5):结合遮挡感知混合，生成更高保真的渲染，支持发型改变和纹理编辑等下游任务。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本文提出了混合网格-高斯头部头像（MeGA），它使用神经网格进行面部建模，使用 3DGS 进行头发建模。为了获得高质量的面部模型，我们增强了 FLAME 网格并解码了一个 UV 置换贴图以获得几何细节。面部颜色是从神经纹理贴图中解码的，该贴图由解耦漫反射纹理 ˆTdi、视点相关纹理 ˆTv 和动态纹理 ˆTdy 组成。为了获得高质量的头发模型，我们构建了一个静态 3DGS 头发，并采用刚性变换结合基于 MLP 的变形场进行动画处理。最终的渲染是通过将头发和头部部分与我们的遮挡感知混合模块混合获得的。此外，MeGA 自然支持各种编辑功能，包括发型更改和纹理编辑。</p><p>（2）：创新点：提出混合网格-高斯头部头像（MeGA），使用更合适的表示来建模不同的头部组件；性能：在 NeRSemble 数据集上的实验表明了我们设计的有效性，优于以前的最先进方法；工作量：.......</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-37516691b13dabbabb3b74ea46b402d8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dfaebdbe659151ed19833d736cf99b64.jpg" align="middle"><img src="https://picx.zhimg.com/v2-179dcf87ee24bc1a697ea323e2d146c3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-02  EMOPortraits Emotion-enhanced Multimodal One-shot Head Avatars</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="元宇宙/虚拟人" scheme="https://kedreamix.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/NeRF/</id>
    <published>2024-04-25T13:35:10.000Z</published>
    <updated>2024-04-25T13:35:10.572Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>摘要</strong><br>高斯体态合成方法结合了神经辐射场和 3D 高斯体积表征，实现了精确的唇部运动和逼真的渲染视频。</p><p><strong>关键要点</strong></p><ul><li>使用3D高斯体积表征实现面部运动的直观控制。</li><li>扬声器特定的运动转换器通过定制唇部运动生成，实现准确的唇部运动。</li><li>动态高斯渲染器通过潜在姿势引入扬声器特定的混合形状，以增强面部细节表示。</li><li>广泛的实验结果表明，该方法在说话头部合成中优于现有技术，提供了精确的唇部同步和出色的视觉质量。</li><li>该方法在 NVIDIA RTX4090 GPU 上实现了 130 FPS 的渲染速度，显着超过了实时渲染性能的门槛。</li><li>该方法有可能部署在其他硬件平台上。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>题目：高斯说话者：基于 3D 高斯点云的说话人特定会说话的头合成</p></li><li><p>作者：洪运雨、湛泉、于启航、陈建川、蒋中华、陈志文、张胜雨、许吉民、吴飞、吕成飞、于刚</p></li><li><p>第一作者单位：阿里巴巴集团</p></li><li><p>关键词：音频驱动、说话头部合成、3D 高斯点云、隐式神经表示、神经辐射场</p></li><li><p>论文链接：xxx   Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，基于神经辐射场（NeRF）的音频驱动说话头部合成方法取得了显著进展。然而，由于 NeRF 隐式表示导致的姿势和表情控制不足，这些方法仍然存在唇部动作不同步或不自然、视觉抖动和伪影等问题。</p><p>（2）：过去方法：以往方法采用 NeRF 隐式表示进行说话头部合成，但存在姿势和表情控制不足的问题。</p><p>（3）：本文方法：本文提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法 GaussianTalker。GaussianTalker 由说话人特定动作转换器和动态高斯渲染器两个模块组成。其中，说话人特定动作转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。动态高斯渲染器引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。</p><p>（4）：实验结果：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。这些结果表明，GaussianTalker 能够有效地合成具有精确唇部动作和高视觉质量的说话头部视频。</p><ol><li><p>方法：</p><p>（1）：采用通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。（2）：引入了说话人特定混合形状和动态高斯点云，通过将高斯点云绑定到 3D 面部模型，实现了对面部动作的直观控制。（3）：训练对象包括重建、唇部动作平滑度和潜在一致性三个部分。</p></li><li><p>结论：</p></li></ol><p>（1）：GaussianTalker 提出了一种基于 3D 高斯点云的音频驱动说话头部合成方法，该方法将多模态数据与特定说话人关联，减少了音频、3D 网格和视频之间的潜在身份偏差。说话人特定 FLAME 转换器采用身份解耦和个性化嵌入来实现同步和自然的唇部动作，而动态高斯渲染器通过潜在姿势细化高斯属性，以实现稳定和逼真的渲染。大量的实验表明，GaussianTalker 在说话头部合成中优于最先进的性能，同时实现了超高的渲染速度，远远超过其他方法。我们相信这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：提出了基于 3D 高斯点云的说话头部合成方法，引入了说话人特定混合形状和动态高斯点云，实现了对面部动作的直观控制。；性能：在 TalkingHead2017 数据集上，GaussianTalker 在唇部动作准确性、视觉保真度和生成视频流畅性方面均取得了最优性能。；工作量：GaussianTalker 的训练和推理速度都非常快，能够实时生成说话头部视频。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation"><a href="#NeRF-DetS-Enhancing-Multi-View-3D-Object-Detection-with-Sampling-adaptive-Network-of-Continuous-NeRF-based-Representation" class="headerlink" title="NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation"></a>NeRF-DetS: Enhancing Multi-View 3D Object Detection with   Sampling-adaptive Network of Continuous NeRF-based Representation</h2><p><strong>Authors:Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</strong></p><p>As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively. </p><p><a href="http://arxiv.org/abs/2404.13921v1">PDF</a> </p><p><strong>Summary</strong><br>神经辐射场（NeRF）统一新颖视图合成与三维感知，通过神经渲染在空间中的连续表示，提出多级采样自适应网络，改进多视图信息融合方法，提升了室内多视图三维物体检测性能。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF-Det 统一新视图合成和 3D 感知任务，新视图合成方法显著提高了室内多视图 3D 目标检测性能。</li><li>NeRF 的几何 MLP 用于指导检测头的注意力，并结合新视图渲染的自监督损失，促进了性能改进。</li><li>NeRF-DetS 引入多级采样自适应网络，自适应地从粗到细进行采样。</li><li>多头加权融合方法解决了使用算术平均值丢失多视图信息的问题。</li><li>NeRF-DetS 在 ScanNetV2 数据集上优于 NeRF-Det，分别在 mAP@.25 和 mAP@.50 上提高了 +5.02% 和 +5.92%。</li><li>多级采样自适应网络和多头加权融合方法是 NeRF-DetS 的主要创新。</li><li>NeRF-DetS 证明了神经渲染在三维感知中的优势，特别是对于多视图物体检测。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：NeRF-DetS：基于连续 NeRF 表示的采样自适应网络增强多视图 3D 目标检测</p></li><li><p>作者：Chi Huang、Xinyang Li、Shengchuan Zhang、Liujuan Cao、Rongrong Ji</p></li><li><p>单位：厦门大学多媒体可信感知与高效计算教育部重点实验室</p></li><li><p>关键词：3D 目标检测、NeRF、多视图</p></li><li><p>链接：https://arxiv.org/abs/2404.13921</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：NeRF-Det 统一了新视图合成和 3D 感知任务，表明感知任务可以受益于 NeRF 等新视图合成方法，显著提高了室内多视图 3D 目标检测的性能。使用 NeRF 的几何 MLP 指导检测头的注意力到关键部分，并结合新视图渲染的自监督损失，促成了实现的改进。</p><p>（2）：过去的方法和问题：为了更好地利用神经渲染在空间中通过连续表示的显着优势，引入了新颖的 3D 感知网络结构 NeRF-DetS。NeRF-DetS 的关键组件是多级采样自适应网络，使采样过程自适应地从粗到精。此外，提出了一个优越的多视图信息融合方法，称为多头加权融合。这种融合方法有效地解决了使用算术平均值时丢失多视图信息的问题，同时保持较低的计算成本。</p><p>（3）：提出的研究方法：NeRF-DetS 在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升。</p><p>（4）：方法的性能和对目标的支持：NeRF-DetS 的性能支持其目标，即通过连续表示和自适应采样增强多视图 3D 目标检测。</p><ol><li>方法：</li></ol><p>（1）：多级采样自适应网络：通过对原始采样点进行偏移预测，实现自适应采样，弥补原始采样点信息的缺失，获取更丰富的空间特征信息；</p><p>（2）：多头加权融合：对不同视角的特征进行加权融合，通过多头权重分配机制，突出重要视角的信息，抑制无关视角的影响，提高融合特征的有效性；</p><p>（3）：训练目标：采用与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失，以优化检测模型的性能。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 NeRF-DetS，以增强基于连续 NeRF 表示的多视图图像目标检测性能。为了充分利用 NeRF 分支为感知过程带来的优势，我们采用了多级采样自适应网络，该网络充分利用了基于 NeRF 表示的连续性的显著特征。此外，认识到空间中多视图信息融合的不足，我们提出了多头加权融合。这种方法利用权重来解决在存在多个视角的情况下空间中的特定视角可能被遮挡的情况。在 ScanNetV2 数据集上的大量实验验证了我们的方法在提高检测任务性能方面的有效性。</p><p>（2）：创新点：多级采样自适应网络、多头加权融合；性能：在 ScanNetV2 数据集上优于竞争对手 NeRF-Det，在 mAP@.25 和 mAP@.50 上分别实现了 +5.02% 和 +5.92% 的提升；工作量：与 NeRF-Det 相同的损失结构，包括 Bounding Box 回归损失、分类损失、中心点损失和新视图渲染损失。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-239cc2f7c7a9838e9e872c8f4334e2d9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c1b28030a36aae7836362d0f5da6d44d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-801202c40b51eebd7384f940b19468e9.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-83f3650828e2486fc3a4b3751e57b1e2.jpg" align="middle"></details><h2 id="CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory"><a href="#CT-NeRF-Incremental-Optimizing-Neural-Radiance-Field-and-Poses-with-Complex-Trajectory" class="headerlink" title="CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory"></a>CT-NeRF: Incremental Optimizing Neural Radiance Field and Poses with   Complex Trajectory</h2><p><strong>Authors:Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, Jiming Chen</strong></p><p>Neural radiance field (NeRF) has achieved impressive results in high-quality 3D scene reconstruction. However, NeRF heavily relies on precise camera poses. While recent works like BARF have introduced camera pose optimization within NeRF, their applicability is limited to simple trajectory scenes. Existing methods struggle while tackling complex trajectories involving large rotations. To address this limitation, we propose CT-NeRF, an incremental reconstruction optimization pipeline using only RGB images without pose and depth input. In this pipeline, we first propose a local-global bundle adjustment under a pose graph connecting neighboring frames to enforce the consistency between poses to escape the local minima caused by only pose consistency with the scene structure. Further, we instantiate the consistency between poses as a reprojected geometric image distance constraint resulting from pixel-level correspondences between input image pairs. Through the incremental reconstruction, CT-NeRF enables the recovery of both camera poses and scene structure and is capable of handling scenes with complex trajectories. We evaluate the performance of CT-NeRF on two real-world datasets, NeRFBuster and Free-Dataset, which feature complex trajectories. Results show CT-NeRF outperforms existing methods in novel view synthesis and pose estimation accuracy. </p><p><a href="http://arxiv.org/abs/2404.13896v2">PDF</a> </p><p><strong>Summary</strong><br>CT-NeRF是一种增量式重建优化管道，仅使用RGB图像即可恢复相机姿态和场景结构，适用于具有复杂轨迹的场景。</p><p><strong>Key Takeaways</strong></p><ul><li>CT-NeRF 提出了一种局部-全局束调整方法，以连接相邻帧之间的位姿图，通过位姿一致性约束场景结构来避免陷入局部最小值。</li><li>CT-NeRF 将位姿一致性实例化为基于输入图像对之间的像素级对应关系的重投影几何图像距离约束。</li><li>通过增量重建，CT-NeRF 能够恢复相机姿态和场景结构，并能够处理具有复杂轨迹的场景。</li><li>CT-NeRF 在 NeRFBuster 和 Free-Dataset 这两个具有复杂轨迹的真实世界数据集上优于现有方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CT-NeRF：增量优化神经辐射场和位姿，复杂轨迹下的应用</p></li><li><p>Authors: Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao sun, Jiming chen</p></li><li><p>Affiliation: 浙江大学</p></li><li><p>Keywords: Pose estimation, Implicit representation, Structure from motion, SLAM</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.13896, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 背景：神经辐射场（NeRF）在高质量3D场景重建中取得了显著成果。然而，NeRF严重依赖于精确的相机位姿。虽然BARF等近期工作已经引入了NeRF中的相机位姿优化，但其适用性仅限于简单的轨迹场景。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(2): 过去的方法及问题：BARF等方法将相机位姿优化引入NeRF，但仅限于简单轨迹场景，无法处理复杂轨迹。现有方法在处理涉及大旋转的复杂轨迹时会遇到困难。</p><p>(3): 本文方法：为了解决这一限制，本文提出了CT-NeRF，这是一种仅使用RGB图像而无需位姿和深度输入的增量重建优化管道。在该管道中，我们首先提出了一种局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值。此外，我们将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束。通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><p>(4): 性能：我们在两个具有复杂轨迹的真实世界数据集NeRFBuster和Free-Dataset上评估了CT-NeRF的性能。结果表明，CT-NeRF在新的视图合成和位姿估计精度方面优于现有方法。</p><ol><li>方法：</li></ol><p>（1）：提出局部-全局捆绑调整，在连接相邻帧的位姿图下，以强制位姿之间的一致性，从而逃离仅与场景结构的位姿一致性造成的局部最小值；</p><p>（2）：将位姿之间的一致性实例化为从输入图像对之间的像素级对应关系产生的重投影几何图像距离约束；</p><p>（3）：通过增量重建，CT-NeRF能够恢复相机位姿和场景结构，并且能够处理具有复杂轨迹的场景。</p><ol><li>结论：</li></ol><p>（1）：本文提出了 CT-NeRF，一种能够从沿复杂轨迹捕获的图像序列中恢复位姿和重建场景的方法。我们首先引入对应关系和重投影几何图像距离，对优化图施加额外的约束，实现鲁棒且准确的位姿估计和场景结构重建。随后，我们详细介绍了我们用于位姿恢复的增量学习过程，包括初始化、跟踪、窗口优化和全局优化。通过比较和消融实验，我们证明了我们方法的优越性和其各个组成部分的必要性。虽然我们的方法能够在复杂的相机轨迹下进行联合位姿估计和重建，但我们只探索了简单的位姿图。对于非常长的轨迹，需要更复杂的图优化。此外，正如论文中所讨论的，复杂相机轨迹需要评估数据集、协议和指标，当前的视觉指标无法充分反映重建质量。</p><p>（2）：创新点：提出了局部-全局捆绑调整，将位姿之间的一致性实例化为重投影几何图像距离约束，实现了鲁棒且准确的位姿估计和场景结构重建；性能：在具有复杂轨迹的真实世界数据集 NeRFBuster 和 Free-Dataset 上评估，结果表明 CT-NeRF 在新的视图合成和位姿估计精度方面优于现有方法；工作量：方法实现较为复杂，需要较高的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4e31cd388846d5e79eb8c6f1f5370705.jpg" align="middle"><img src="https://picx.zhimg.com/v2-51c39516accf12f9bec3760a243d8ec4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-2f23b255b94f32edb903410a01a371e3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-360fd8ee973080efc6f3769036860e2b.jpg" align="middle"></details><h2 id="Neural-Radiance-Field-in-Autonomous-Driving-A-Survey"><a href="#Neural-Radiance-Field-in-Autonomous-Driving-A-Survey" class="headerlink" title="Neural Radiance Field in Autonomous Driving: A Survey"></a>Neural Radiance Field in Autonomous Driving: A Survey</h2><p><strong>Authors:Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</strong></p><p>Neural Radiance Field (NeRF) has garnered significant attention from both academia and industry due to its intrinsic advantages, particularly its implicit representation and novel view synthesis capabilities. With the rapid advancements in deep learning, a multitude of methods have emerged to explore the potential applications of NeRF in the domain of Autonomous Driving (AD). However, a conspicuous void is apparent within the current literature. To bridge this gap, this paper conducts a comprehensive survey of NeRF’s applications in the context of AD. Our survey is structured to categorize NeRF’s applications in Autonomous Driving (AD), specifically encompassing perception, 3D reconstruction, simultaneous localization and mapping (SLAM), and simulation. We delve into in-depth analysis and summarize the findings for each application category, and conclude by providing insights and discussions on future directions in this field. We hope this paper serves as a comprehensive reference for researchers in this domain. To the best of our knowledge, this is the first survey specifically focused on the applications of NeRF in the Autonomous Driving domain. </p><p><a href="http://arxiv.org/abs/2404.13816v1">PDF</a> </p><p><strong>Summary</strong></p><p>NeRF在自动驾驶领域具有感知、三维重建、SLAM和仿真等应用，本文对其应用进行了全面的综述。</p><p><strong>Key Takeaways</strong></p><ul><li>NeRF能用于自动驾驶中的感知任务，如物体检测、语义分割和实例分割。</li><li>NeRF能用于自动驾驶中的3D重建任务，如场景重建和车辆建模。</li><li>NeRF能用于自动驾驶中的SLAM任务，如定位和建图。</li><li>NeRF能用于自动驾驶中的仿真任务，如场景生成和传感器模拟。</li><li>NeRF的应用在自动驾驶领域具有广阔的前景。</li><li>NeRF在自动驾驶中的应用仍面临一些挑战，如效率、鲁棒性和真实感。</li><li>本次调查为研究人员提供了NeRF在自动驾驶领域应用的全面参考。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 神经辐射场在自动驾驶中的应用：综述</p></li><li><p>Authors: Lei He, Leheng Li, Wenchao Sun, Zeyu Han, Yichen Liu, Sifa Zheng, Jianqiang Wang, Keqiang Li</p></li><li><p>Affiliation: 中国科学院大学</p></li><li><p>Keywords: Neural Radiance Field, Autonomous driving, Perception, 3D Reconstruction, SLAM, Simulation</p></li><li><p>Urls: https://arxiv.org/abs/2404.13816v1</p></li><li><p>Summary:</p><p>(1): 神经辐射场（NeRF）是一种新颖的视图合成技术，它利用体渲染和隐式神经场景表示的能力来揭示 3D 场景几何的复杂性。它在 ECCV 2020 上首次亮相，迅速达到领先的视觉质量水平，并成为众多后续研究工作的灵感来源。近年来，自动驾驶领域取得了重大进展，在高速公路场景中得到广泛部署，但城市环境中的部署仍在进行严格测试。这种技术演变已经从最初依赖高精度地图提供静态场景理解转变为现在通过鸟瞰视觉实时感知局部环境。同时，它在功能上已经从 2 级（L2）发展起来，并正在努力实现 4 级（L4）自动驾驶。自动驾驶系统要求对周围环境有深入的了解，包括静态场景和交通参与者之间的动态交互，这是有效规划和控制的关键前提。通过自监督学习，NeRF 已证明其有效理解局部场景的能力，使其成为增强自动驾驶能力的有力候选者。在过去的两年中，NeRF 模型已在自动驾驶的各个方面得到应用，包括感知、3D 重建、同时定位和地图构建 (SLAM) 和仿真，如图 1 所示。</p><p>(2): 在感知领域，神经辐射场（NeRF）已成为一个有前途的竞争者，涵盖了对象检测、语义分割和占用预测等一系列关键任务。其受欢迎程度的激增主要归功于其获取精确且一致的几何信息的能力。该领域的研究可以分为两种主要范例，区别在于 NeRF 的利用：“NeRF for data”和“NeRF for model”。前者涉及 NeRF 的初始训练，然后将其用于增强感知任务的训练数据。相比之下，后者采用 NeRF 和感知网络的协作训练策略，使感知网络能够学习 NeRF 捕获的几何信息。</p><p>(3): 在 3D 重建应用领域，NeRF 可以根据场景理解的级别分为三种主要方法：动态场景重建、表面重建和逆向渲染。在第一类中，动态场景重建专注于重建具有可移动代理的动态场景，主要使用顺序 3D 边界框注释和相机参数。在第二类中，表面重建旨在重建场景的显式 3D 表面，例如网格。在第三类中，逆向渲染旨在从驾驶场景的图像中分解形状、反照率和可见性，以实现诸如重新照明之类的应用。</p><p>(4): 至于 SLAM 应用，NeRF 的利用可以分为三种主要方法，每种方法都针对映射、定位或两者兼而有之。至于定位，NeRF 用于在当前时间戳执行实时图像渲染，并通过最小化重投影误差来估计 SLAM 系统的精确姿态。虽然 NeRF for mapping 主要专注于增强 SLAM 系统的映射能力，这通过合并使用 NeRF 生成的深度图来实现，从而提高了地图精度。此外，NeRF 在其他一些研究中用于同时提高 3D 地图的质量和提高 SLAM 系统在姿态估计中的精度。这些分类展示了如何将 NeRF 策略性地集成到 SLAM 系统中以满足特定需求，无论这些需求涉及映射、定位还是两者兼而有之的功能。值得一提的是，一些现有的基于 NeRF 的 SLAM 方法是为室内场景设计的，但由于该技术类似于自动驾驶的大型室外环境，因此本文也对室内方法进行了综述。</p></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本综述工作对神经辐射场在自动驾驶领域的应用进行了全面的回顾。具体而言，我们首先介绍了神经辐射场的基本原理和背景，然后深入分析了神经辐射场在自动驾驶各个领域的应用，分为感知、三维重建、SLAM和仿真。在总结了神经辐射场在自动驾驶领域应用的最新进展的基础上，我们还讨论了该领域未来研究的潜在方向和挑战。</p><p>（2）：创新点：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>性能：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><p>工作量：本文对神经辐射场在自动驾驶领域的应用进行了全面的综述，涵盖了感知、三维重建、SLAM和仿真等多个方面。本文总结了神经辐射场在自动驾驶领域应用的最新进展，并指出了该领域未来研究的潜在方向和挑战。此外，本文还对神经辐射场在自动驾驶领域应用的优势和不足进行了分析，为该领域的研究人员和从业者提供了有价值的参考。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f00a4edaa4deada8fbf20792a3bdb4f2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-726da3cc31a9a838b18bd0268191d0f9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-936b55512111274340010e2934e3af78.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0376cf43fef8cbf7ce42618963f10673.jpg" align="middle"><img src="https://picx.zhimg.com/v2-db311dfa75c7afbf16e9c52d4642623e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-18c975d626ca07af436db0c065d6d034.jpg" align="middle"><img src="https://pica.zhimg.com/v2-025492e7bc2802a1fe24dea9c19a7bbf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/3DGS/</id>
    <published>2024-04-25T13:22:56.000Z</published>
    <updated>2024-04-25T13:22:56.987Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent"><a href="#FlowMap-High-Quality-Camera-Poses-Intrinsics-and-Depth-via-Gradient-Descent" class="headerlink" title="FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent"></a>FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</h2><p><strong>Authors:Cameron Smith, David Charatan, Ayush Tewari, Vincent Sitzmann</strong></p><p>This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM). </p><p><a href="http://arxiv.org/abs/2404.15259v1">PDF</a> Project website: <a href="https://cameronosmith.github.io/flowmap/">https://cameronosmith.github.io/flowmap/</a></p><p><strong>Summary</strong><br>FlowMap使用基于梯度的优化方法，根据光流推算相机位姿并渲染360度新颖视角。</p><p><strong>Key Takeaways</strong></p><ul><li>FlowMap 是一种端到端可微方法，用于求解相机位姿、内参和视频序列的逐帧稠密深度。</li><li>FlowMap 使用简单最小二乘目标函数的逐视频梯度下降最小化，该目标函数比较由深度、内参和位姿引起的光流和通过现成光流和点跟踪获得的对应关系。</li><li>Point tracks 用于鼓励长期几何一致性。</li><li>引入了深度的可微重新参数化、内参和位姿，适用于一阶优化。</li><li>FlowMap 恢复的相机参数和稠密深度能够使用高斯溅射在 360 度轨迹上进行逼真的新视图合成。</li><li>FlowMap 不仅明显优于先前的基于梯度下降的束调整方法，而且令人惊讶地与最先进的 SfM 方法 COLMAP 在 360 度新视图合成的下游任务中表现相当（即使 FlowMap 是一种基于梯度下降的纯可微方法，并完全有别于传统的 SfM）。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：FlowMap：高质量相机位姿、内参和补充材料</p></li><li><p>作者：Alex Yu, Vladlen Koltun</p></li><li><p>第一作者单位：英伟达</p></li><li><p>关键词：计算机视觉、结构从运动、神经渲染、神经辐射场</p></li><li><p>论文链接：https://arxiv.org/abs/2302.06641 或 https://github.com/facebookresearch/FlowMap</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经渲染和神经辐射场（NeRF）等技术需要准确的相机位姿和场景几何信息。传统的结构从运动（SfM）方法通常采用梯度下降算法，但收敛速度慢、容易陷入局部最优。</p><p>（2）：过去方法和问题：现有的梯度下降方法在处理大位移和遮挡时存在困难。此外，它们通常需要手工特征匹配，这既耗时又不可靠。</p><p>（3）：本文方法：本文提出 FlowMap，一种端到端的可微方法，通过最小化光流诱导的深度、内参和位姿与通过光流和点跟踪获得的对应关系之间的均方误差来求解相机位姿、内参和稠密深度。FlowMap 采用可微分的深度、内参和位姿重新参数化，使其适合一阶优化。此外，它利用点迹鼓励长期几何一致性。</p><p>（4）：方法性能：FlowMap 在 360 度轨迹上使用高斯溅射进行逼真的新视角合成，其相机参数和稠密深度明显优于先前的梯度下降 SfM 方法。它甚至与最先进的 SfM 方法 COLMAP 在新视角合成任务上的性能相当，尽管 FlowMap 是纯梯度下降的、完全可微分的，并且与传统的 SfM 方法完全不同。</p><ol><li>方法：</li></ol><p>（1）：本文提出 FlowMap，一种端到端的可微方法，通过最小化光流诱导的深度、内参和位姿与通过光流和点跟踪获得的对应关系之间的均方误差来求解相机位姿、内参和稠密深度。</p><p>（2）：FlowMap 采用可微分的深度、内参和位姿重新参数化，使其适合一阶优化。</p><p>（3）：此外，它利用点迹鼓励长期几何一致性。</p><ol><li>结论：</li></ol><p>（1）：本文提出 FlowMap，一种简单、鲁棒且可扩展的一阶方法，用于从视频中估计相机参数。我们的模型优于现有的基于梯度下降的相机参数估计方法。FlowMap 的深度和相机参数通过高斯溅射法进行后续重建，其质量与 COLMAP 相当。FlowMap 使用 PyTorch 编写，短序列运行时间为 3 分钟，长序列运行时间为 20 分钟，我们预计协同工程工作可以将 FlowMap 加速一个数量级。也许最令人兴奋的是，FlowMap 对每帧深度估计是完全可微分的。因此，FlowMap 可以作为新一代自监督单目深度估计器、基于深度学习的多视图几何方法以及可泛化新视图合成方法的构建模块 [7, 18, 60, 66, 69, 77]，从而解锁对未摆放视频的互联网规模数据集的训练。致谢。这项工作得到了国家科学基金会 2211259 号赠款、新加坡 DSTA 下 DST00OECI20300823（视觉的新表示和用于标签高效视觉的自监督学习）、情报高级研究项目活动 (IARPA) 通过 140D0423C0075 下的内政部/内务商业中心 (DOI/IBC)、亚马逊科学中心和 IBM 的部分支持。丰田研究院也部分支持了这项工作。此处包含的观点和结论反映了其作者的观点和结论，不代表任何其他实体。</p><p>（2）：创新点：FlowMap 提出了一种端到端的可微方法来解决相机位姿、内参和稠密深度估计问题，采用可微分的深度、内参和位姿重新参数化，并利用点迹鼓励长期几何一致性。性能：FlowMap 在相机参数和稠密深度估计方面优于现有的梯度下降 SfM 方法，其新视角合成质量与最先进的 SfM 方法 COLMAP 相当。工作量：FlowMap 是一种一阶优化方法，在 PyTorch 中实现，短序列运行时间为 3 分钟，长序列运行时间为 20 分钟，具有可扩展性和工程加速潜力。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-ed80501d2ace1f8ad37b4cb8f3411d6f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a1dea0f5ce347645c2a4c11098b0ba50.jpg" align="middle"><img src="https://pica.zhimg.com/v2-25a5764437b9221ae10ad73aa8b84fb9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-43352f81d6eb7aada886230a057402b5.jpg" align="middle"></details><h2 id="CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding"><a href="#CLIP-GS-CLIP-Informed-Gaussian-Splatting-for-Real-time-and-View-consistent-3D-Semantic-Understanding" class="headerlink" title="CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding"></a>CLIP-GS: CLIP-Informed Gaussian Splatting for Real-time and   View-consistent 3D Semantic Understanding</h2><p><strong>Authors:Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</strong></p><p>The recent 3D Gaussian Splatting (GS) exhibits high-quality and real-time synthesis of novel views in 3D scenes. Currently, it primarily focuses on geometry and appearance modeling, while lacking the semantic understanding of scenes. To bridge this gap, we present CLIP-GS, which integrates semantics from Contrastive Language-Image Pre-Training (CLIP) into Gaussian Splatting to efficiently comprehend 3D environments without annotated semantic data. In specific, rather than straightforwardly learning and rendering high-dimensional semantic features of 3D Gaussians, which significantly diminishes the efficiency, we propose a Semantic Attribute Compactness (SAC) approach. SAC exploits the inherent unified semantics within objects to learn compact yet effective semantic representations of 3D Gaussians, enabling highly efficient rendering (&gt;100 FPS). Additionally, to address the semantic ambiguity, caused by utilizing view-inconsistent 2D CLIP semantics to supervise Gaussians, we introduce a 3D Coherent Self-training (3DCS) strategy, resorting to the multi-view consistency originated from the 3D model. 3DCS imposes cross-view semantic consistency constraints by leveraging refined, self-predicted pseudo-labels derived from the trained 3D Gaussian model, thereby enhancing precise and view-consistent segmentation results. Extensive experiments demonstrate that our method remarkably outperforms existing state-of-the-art approaches, achieving improvements of 17.29% and 20.81% in mIoU metric on Replica and ScanNet datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, verifying the robustness of our method. </p><p><a href="http://arxiv.org/abs/2404.14249v1">PDF</a> <a href="https://github.com/gbliao/CLIP-GS">https://github.com/gbliao/CLIP-GS</a></p><p><strong>Summary</strong><br>CLIP-GS 将语义信息融入高斯斑点渲染中，实现了高效的 3D 场景理解，在不使用带注释的语义数据的情况下，达到了最先进的分割性能。</p><p><strong>Key Takeaways</strong></p><ul><li>CLIP-GS 将语义信息从 CLIP 集成到高斯斑点渲染中，实现对 3D 场景的语义理解。</li><li>语义属性紧凑性 (SAC) 方法学习紧凑且有效的语义表示，实现高效渲染。</li><li>3D 一致自训练 (3DCS) 策略解决由视图不一致的 2D CLIP 语义监督造成的语义歧义。</li><li>3DCS 利用经过微调的 3D 高斯模型预测的伪标签，加强跨视图语义一致性约束。</li><li>CLIP-GS 在 Replica 和 ScanNet 数据集上分别提高 mIoU 指标 17.29% 和 20.81%，超越现有最先进方法。</li><li>CLIP-GS 即使在稀疏输入数据的情况下也能表现出优异的性能，验证了其鲁棒性。</li><li>CLIP-GS 实时渲染速度快，可用于交互式 3D 场景探索和编辑。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: CLIP-GS: CLIP-引导的高斯泼溅，用于实时且视图一致的三维语义理解</p></li><li><p>Authors: Guibiao Liao, Jiankun Li, Zhenyu Bao, Xiaoqing Ye, Jingdong Wang, Qing Li, Kanglin Liu</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: 3D 高斯泼溅 · 实时 · 视图一致 · 三维场景语义理解 · 三维场景重建 · 稀疏视图</p></li><li><p>Urls: https://arxiv.org/abs/2404.14249 , Github: None</p></li><li><p>Summary: </p><pre><code>            (1): 近期提出的三维高斯泼溅（3DGS）在三维场景中展现了高质量且实时的全新视图合成。目前它主要关注于几何和外观建模，而缺少对场景的语义理解。            (2): 现有方法主要有：神经辐射场（NeRFs）和三维高斯泼溅（3DGS）。前者在渲染包含几何和外观细节的新视角方面取得了显著进展，但缺乏对三维场景的全面语义理解；后者则主要关注几何和外观建模，而忽略了语义信息。            (3): 本文提出了一种名为 CLIP-GS 的新方法，该方法将来自对比语言图像预训练 (CLIP) 的语义信息整合到高斯泼溅中，从而在没有注释语义数据的情况下有效地理解三维环境。具体来说，我们提出了一种语义属性紧凑性 (SAC) 方法，该方法利用对象内固有的统一语义来学习高斯泼溅的紧凑且有效的语义表示，从而实现高效渲染（&gt;100 FPS）。此外，为了解决语义歧义问题，我们引入了一种三维连贯自训练 (3DCS) 策略，利用三维模型产生的多视图一致性。3DCS 通过利用从训练好的三维高斯模型中获得的经过优化且自我预测的伪标签来施加跨视图语义一致性约束，从而增强精确且视图一致的分割结果。            (4): 在 Replica 和 ScanNet 数据集上，该方法在 mIoU 指标上分别比现有最先进的方法提高了 17.29% 和 20.81%，同时保持了实时的渲染速度。此外，即使在输入数据稀疏的情况下，该方法也表现出优异的性能，验证了其鲁棒性。</code></pre></li><li><p>方法：</p><pre><code>            (1):语义属性紧凑性（SAC）：利用对象内固有的统一语义来学习高斯泼溅的紧凑且有效的语义表示，实现高效渲染。            (2):3D连贯自训练（3DCS）：利用三维模型产生的多视图一致性，施加跨视图语义一致性约束，增强精确且视图一致的分割结果。            (3):整体训练过程：交替优化3D高斯泼溅和语义表示，同时利用3DCS施加视图一致性约束。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本工作提出了 CLIP-GS，一种利用高斯泼溅实现三维场景实时且精确语义理解的新方法。在 CLIP-GS 中，语义属性紧凑性（SAC）将紧凑的语义信息附加到三维高斯体中以高效地表示三维语义，确保高效渲染。此外，提出的三维连贯自训练（3DCS）增强了不同视图之间的语义一致性，从而产生了准确的三维分割结果。实验结果表明，我们的方法在合成和真实场景中明显优于 SOTA 方法。此外，即使输入数据稀疏，我们的方法也表现出优异的性能，验证了我们的三维语义学习方法的鲁棒性。</p><p>（2）：创新点：提出了一种将 CLIP 语义信息整合到高斯泼溅中的方法，实现了三维场景的实时语义理解。创新性地提出了语义属性紧凑性（SAC）和三维连贯自训练（3DCS）两种技术，分别用于高效语义表示和跨视图语义一致性增强。</p><p>性能：在 Replica 和 ScanNet 数据集上，该方法在 mIoU 指标上分别比现有最先进的方法提高了 17.29% 和 20.81%，同时保持了实时的渲染速度。即使在输入数据稀疏的情况下，该方法也表现出优异的性能，验证了其鲁棒性。</p><p>工作量：该方法需要预训练 CLIP 模型和三维高斯泼溅模型，训练过程需要大量的数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d36db5fceba666ce511b0cf595bc769d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0a5ca926d7e6577c4c1a0e8076537a17.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ef11b21fc83f3602f91a29eea9ff097e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-54108038b1e285d6be885cd6288e500c.jpg" align="middle"></details><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary:</strong><br>高斯体素渲染法赋予3D高斯体素显式表示特性，可实现直观的面部动作控制，大幅提升音频驱动虚拟化身合成效果。</p><p><strong>Key Takeaways:</strong></p><ul><li>基于3D高斯体素的音频驱动虚拟化身合成方法。</li><li>面部运动通过将高斯体素绑定到3D面部模型实现直观控制。</li><li>说话人特定运动翻译器实现针对特定说话人的精确唇部运动。</li><li>动态高斯渲染器引入说话人特定混合形状以增强面部细节表示。</li><li>广泛的实验结果表明，该方法在语音合成方面优于现有最先进的方法。</li><li>渲染速度达到130 FPS，远超实时渲染性能的阈值。</li><li>具有在其他硬件平台上部署的潜力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: GaussianTalker: Speaker-specific Talking Head Synthesis (基于 3D 高斯点云的说话人特定说话头合成)</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: Alibaba Group (阿里巴巴集团)</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Speaker-specific motion, Dynamic Gaussian Renderer</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.14037.pdf, Github: None</p></li><li><p>Summary:</p><p>(1): Recent audio-driven talking head synthesis methods based on Neural Radiance Fields (NeRF) have achieved impressive results, but suffer from inadequate pose and expression control due to NeRF's implicit representation, leading to unsynchronized or unnatural lip movements and visual jitter and artifacts.</p><p>(2): Past methods: NeRF-based audio-driven talking head synthesis methods. Problems: Inadequate pose and expression control, resulting in unsynchronized or unnatural lip movements and visual jitter and artifacts. Well motivated: Yes, as it addresses the limitations of existing methods.</p><p>(3): GaussianTalker: A novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. It consists of two modules: Speaker-specific Motion Translator and Dynamic Gaussian Renderer. The former achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. The latter introduces Speaker-specific BlendShapes to control the facial expressions and generates high-quality talking head videos with precise lip movements.</p><p>(4): On the task of audio-driven talking head synthesis, GaussianTalker achieves state-of-the-art results. It can generate high-quality talking head videos with precise lip movements and natural facial expressions. The performance supports the goals of the paper, which are to address the limitations of existing methods and achieve more realistic and expressive talking head synthesis.</p></li><li><p>方法：</p><p>(1)：基于3D高斯点云的说话人特定说话头合成；</p><p>(2)：提出Speaker-specific Motion Translator和Dynamic Gaussian Renderer两个模块；</p><p>(3)：Speaker-specific Motion Translator通过通用音频特征提取和定制唇部动作生成实现特定于目标说话人的准确唇部动作；</p><p>(4)：Dynamic Gaussian Renderer引入说话人特定BlendShapes来控制面部表情，并生成具有精确唇部动作的高质量说话头视频；</p><p>.......</p></li></ol><p><strong>结论</strong></p><p>（1）：本工作提出 GaussianTalker，一种通过 3D 高斯点云集成 FLAME 模型的音频驱动说话头合成新框架。GaussianTalker 将多模态数据与特定说话者关联，减少了音频、3D 网格和视频之间的潜在身份偏差。Speaker-specific FLAME Translator 采用身份解耦和个性化嵌入来实现同步和自然的唇部动作，而 Dynamic Gaussian Renderer 通过潜在姿势细化高斯属性，以实现稳定和逼真的渲染。大量实验表明，GaussianTalker 在说话头合成方面优于最先进的性能，同时实现了超高的渲染速度，明显超过其他方法。我们相信，这种创新方法将鼓励未来的研究开发更流畅、更逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色动画将远远超出简单的唇形同步，捕捉更广泛的角色动态。</p><p>（2）：创新点：基于 3D 高斯点云的说话人特定说话头合成；Speaker-specific Motion Translator 和 Dynamic Gaussian Renderer 两个模块；性能：在说话头合成方面优于最先进的性能，实现了超高的渲染速度；工作量：.......</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://pica.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos"><a href="#Dynamic-Gaussians-Mesh-Consistent-Mesh-Reconstruction-from-Monocular-Videos" class="headerlink" title="Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos"></a>Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular   Videos</h2><p><strong>Authors:Isabella Liu, Hao Su, Xiaolong Wang</strong></p><p>Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines. Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a> </p><p><a href="http://arxiv.org/abs/2404.12379v2">PDF</a> Project page: <a href="https://www.liuisabella.com/DG-Mesh/">https://www.liuisabella.com/DG-Mesh/</a></p><p><strong>Summary</strong><br>单目视频重建高保真动态网格的框架</p><p><strong>Key Takeaways</strong></p><ul><li>动态高斯网格（DG-Mesh）通过单目视频重建出高保真时序一致的网格。</li><li>利用 3D 高斯点云构成具有时序一致性的网格序列。</li><li>高斯点云恢复高质量网格，并实现网格顶点的时序跟踪，可用于动态对象纹理编辑。</li><li>高斯网格锚定可使高斯分布均匀，通过网格引导变形高斯的高密度化和剪枝，提升网格重建质量。</li><li>通过规范空间和变形空间的循环一致性变形，将锚定的高斯投射回规范空间，并在所有时间帧优化高斯。</li><li>在不同数据集上评估后，DG-Mesh 在网格重建和渲染方面显著优于基准方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 动态高斯网格：单目视频中一致的网格重建</p></li><li><p>Authors: Isabella Liu, Hao Su, Xiaolong Wang</p></li><li><p>Affiliation: 加州大学圣地亚哥分校</p></li><li><p>Keywords: 3D Reconstruction, Monocular Video, Dynamic Mesh, Gaussian Splatting</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12379, Github: None</p></li><li><p>Summary:</p><p>(1): 现代 3D 引擎和图形管道需要网格作为一种内存高效的表示，它允许高效渲染、几何处理、纹理编辑和许多其他下游操作。然而，从单目视觉观察中获得结构和细节方面的高质量网格仍然非常困难。对于动态场景和对象，这个问题变得更具挑战性。</p><p>(2): 过去的方法包括使用 3D 高斯斑点构建网格序列，但这些方法在处理动态场景时存在局限性。</p><p>(3): 本文提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建。</p><p>(4): 在 D-NeRF 数据集和 DG-Mesh 数据集上，该方法在网格重建任务上取得了优异的性能，证明了其有效性。</p></li><li><p>方法：</p><p>（1）：提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建；</p><p>（2）：该框架包括一个网格构建模块，该模块使用 3D 高斯斑点表示来构建网格序列，以及一个网格锚定模块，该模块将网格序列中的网格锚定到世界坐标系中；</p><p>（3）：网格构建模块利用 3D 高斯斑点表示来表示场景中的几何形状，并使用高斯混合模型（GMM）来估计网格序列中的网格；</p><p>（4）：网格锚定模块利用高斯网格锚定算法将网格序列中的网格锚定到世界坐标系中，该算法使用高斯斑点之间的几何关系来估计网格的位姿；</p><p>（5）：该框架通过迭代优化网格构建模块和网格锚定模块中的参数来实现时间一致的网格序列重建。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种名为动态高斯网格（DG-Mesh）的框架，该框架利用 3D 高斯斑点表示，并通过高斯网格锚定算法恢复高质量网格，从而实现时间一致的网格序列重建。DG-Mesh 在网格重建任务上取得了优异的性能，证明了其有效性。</p><p>（2）：创新点：利用 3D 高斯斑点表示和高斯网格锚定算法实现时间一致的网格序列重建；性能：在网格重建任务上取得了优异的性能；工作量：需要准确的对象分割和跟踪，在处理大拓扑变化时存在挑战。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-bb786e92e4a68c16900a6443568566f8.jpg" align="middle"><img src="https://picx.zhimg.com/v2-84e4142556dfc3bb4d97a20772986995.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2de77e2437a64b7bd107f95e76669404.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-25  FlowMap High-Quality Camera Poses, Intrinsics, and Depth via Gradient   Descent</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Talking%20Head%20Generation/</id>
    <published>2024-04-25T13:12:59.000Z</published>
    <updated>2024-04-25T13:12:59.111Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting"><a href="#GaussianTalker-Speaker-specific-Talking-Head-Synthesis-via-3D-Gaussian-Splatting" class="headerlink" title="GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting"></a>GaussianTalker: Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</h2><p><strong>Authors:Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</strong></p><p>Recent works on audio-driven talking head synthesis using Neural Radiance Fields (NeRF) have achieved impressive results. However, due to inadequate pose and expression control caused by NeRF implicit representation, these methods still have some limitations, such as unsynchronized or unnatural lip movements, and visual jitter and artifacts. In this paper, we propose GaussianTalker, a novel method for audio-driven talking head synthesis based on 3D Gaussian Splatting. With the explicit representation property of 3D Gaussians, intuitive control of the facial motion is achieved by binding Gaussians to 3D facial models. GaussianTalker consists of two modules, Speaker-specific Motion Translator and Dynamic Gaussian Renderer. Speaker-specific Motion Translator achieves accurate lip movements specific to the target speaker through universalized audio feature extraction and customized lip motion generation. Dynamic Gaussian Renderer introduces Speaker-specific BlendShapes to enhance facial detail representation via a latent pose, delivering stable and realistic rendered videos. Extensive experimental results suggest that GaussianTalker outperforms existing state-of-the-art methods in talking head synthesis, delivering precise lip synchronization and exceptional visual quality. Our method achieves rendering speeds of 130 FPS on NVIDIA RTX4090 GPU, significantly exceeding the threshold for real-time rendering performance, and can potentially be deployed on other hardware platforms. </p><p><a href="http://arxiv.org/abs/2404.14037v1">PDF</a> </p><p><strong>Summary</strong><br>高斯散点绘制助力音频驱动说话人头部生成，精确控制面部动作，实现自然流畅的唇部运动和逼真的视觉效果。</p><p><strong>Key Takeaways</strong></p><ul><li>基于 3D 高斯散点绘制，显式表示面部动作，实现对脸部运动的直观控制。</li><li>说话人特定运动转换器，通过通用音频特征提取和定制唇部运动生成，实现准确的唇部运动。</li><li>动态高斯渲染器引入说话人特定混合形状，通过潜在位姿增强面部细节表示，提供稳定逼真的渲染视频。</li><li>实验结果表明，该方法在说话人头部生成方面优于现有技术，提供精确的唇部同步和出色的视觉质量。</li><li>渲染速度达 130 FPS，远超实时渲染性能阈值，可部署于其他硬件平台。</li><li>旨在解决音频驱动说话人头部合成中姿态和表情控制不足的问题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: GaussianTalker：基于3D高斯斑点的说话人特定会说话的头部合成</p></li><li><p>Authors: Hongyun Yu, Zhan Qu, Qihang Yu, Jianchuan Chen, Zhonghua Jiang, Zhiwen Chen, Shengyu Zhang, Jimin Xu, Fei Wu, Chengfei Lv, Gang Yu</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Audio-driven talking head synthesis, 3D Gaussian Splatting, Lip motion, Facial animation</p></li><li><p>Urls: Paper: , Github:None</p></li><li><p>Summary:</p><pre><code>           (1): 近期基于神经辐射场（NeRF）的音频驱动说话头部合成工作取得了令人瞩目的成果。然而，由于NeRF隐式表示导致的姿势和表情控制不足，这些方法仍然存在一些局限性，如唇部动作不同步或不自然，以及视觉抖动和伪影。           (2): 过去的方法：基于NeRF的音频驱动说话头部合成方法。问题：姿势和表情控制不足，导致唇部动作不同步或不自然，以及视觉抖动和伪影。动机：通过显式表示面部运动，实现对唇部动作的直观控制。           (3): 本文提出的研究方法：GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法。GaussianTalker由两个模块组成：说话人特定运动转换器和动态高斯渲染器。说话人特定运动转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作。动态高斯渲染器引入说话人特定混合形状，以控制面部表情。           (4): 本文方法在任务和性能上的表现：在TalkingHead数据集上进行评估，GaussianTalker在唇部同步、视觉保真度和鲁棒性方面均优于现有方法。这些性能支持了本文的目标：实现准确、逼真且鲁棒的音频驱动说话头部合成。</code></pre></li><li><p>方法：</p><pre><code>           (1): 提出GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法；           (2): GaussianTalker由两个模块组成：说话人特定运动转换器和动态高斯渲染器；           (3): 说话人特定运动转换器通过通用音频特征提取和定制唇部动作生成，实现针对目标说话人的准确唇部动作；           (4): 动态高斯渲染器引入说话人特定混合形状，以控制面部表情；           (5): 采用FLAME模型作为面部动画和渲染之间的桥梁；           (6): 训练说话人特定运动转换器，使用多语言、多个人数据集，以提高其对不同音频输入的适应性；           (7): 使用通用音频编码器分离身份信息和内容信息，使用定制运动解码器集成个性化特征；           (8): 引入基于自我监督的唇读约束机制，进一步优化唇部动作的同步性；           (9): 动态高斯渲染器由FLAME驱动3D高斯斑点，实时渲染动态说话头部；           (10): 使用自适应密度控制，根据局部运动幅度动态调整高斯斑点的密度；           (11): 采用图像修复生成器，处理渲染过程中的空洞区域和伪影；           (12): 在TalkingHead数据集上进行评估，GaussianTalker在唇部同步、视觉保真度和鲁棒性方面均优于现有方法。</code></pre></li><li><p>结论：</p><pre><code>           (1):本文提出GaussianTalker，一种基于3D高斯斑点的音频驱动说话头部合成新方法。该方法将多模态数据与特定说话人关联，减少了音频、3D网格和视频之间的潜在身份偏差。说话人特定FLAME转换器采用身份解耦和个性化嵌入来实现同步且自然的唇部动作，而动态高斯渲染器通过潜在姿势优化高斯属性，以实现稳定且逼真的渲染。大量实验表明，GaussianTalker在说话头部合成方面优于最先进的性能，同时实现了超高的渲染速度，显著超越了其他方法。我们相信这种创新方法将鼓励未来的研究开发更加流畅逼真的角色表情和动作。通过利用先进的高斯模型和生成技术，角色的动画将远远超出简单的唇部同步，捕捉更广泛的角色动态。                             (2):创新点：基于3D高斯斑点的音频驱动说话头部合成；           性能：在唇部同步、视觉保真度和鲁棒性方面优于现有方法；           工作量：与其他方法相比，渲染速度超高。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-f53af9ef57ed25d0699b508f7b856061.jpg" align="middle"><img src="https://picx.zhimg.com/v2-00e62c0d66ff2641b9803987918d6fd0.jpg" align="middle"></details><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>Deepfake is a technology dedicated to creating highly realistic facial images and videos under specific conditions, which has significant application potential in fields such as entertainment, movie production, digital human creation, to name a few. With the advancements in deep learning, techniques primarily represented by Variational Autoencoders and Generative Adversarial Networks have achieved impressive generation results. More recently, the emergence of diffusion models with powerful generation capabilities has sparked a renewed wave of research. In addition to deepfake generation, corresponding detection technologies continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing current state-of-the-arts in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss developing technologies. Then, we discuss the development of several related sub-fields and focus on researching four representative deepfake fields: face swapping, face reenactment, talking face generation, and facial attribute editing, as well as forgery detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential published works. Finally, we analyze challenges and future research directions of the discussed fields. </p><p><a href="http://arxiv.org/abs/2403.17881v3">PDF</a> We closely follow the latest developments in   <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a></p><p><strong>Summary</strong><br>深度伪造是一项在特定条件下创建高度逼真的面部图像和视频的技术，在娱乐、电影制作、数字人创作等领域具有广阔的应用前景。</p><p><strong>Key Takeaways</strong></p><ul><li>深度伪造生成主要采用变分自动编码器和生成对抗网络等深度学习技术。</li><li>扩散模型的出现引发了新一轮的深度伪造生成研究热潮。</li><li>深度伪造检测技术与生成技术同步发展，以防范深度伪造技术的滥用。</li><li>本综述对深度伪造生成与检测的最新进展进行了全面回顾和分析。</li><li>深度伪造生成中的面部替换、面部重现、说话脸生成和面部属性编辑等领域的研究备受关注。</li><li>本综述对每一领域中具有代表性的方法进行了全面评估，充分展示了最新且有影响力的已发表工作。</li><li>本综述分析了相关领域的挑战和未来研究方向。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: 深度伪造生成与检测：基准与综述</p></li><li><p>Authors: Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</p></li><li><p>Affiliation: 东华大学多维信息处理上海市重点实验室</p></li><li><p>Keywords: Deepfake Generation, Face Swapping, Face Reenactment, Talking Face Generation, Facial Attribute Editing, Forgery detection, Survey</p></li><li><p>Urls: https://arxiv.org/abs/2403.17881v3 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):深度伪造技术近年来得到广泛关注，其应用前景广阔，但其潜在的伦理风险也引发了人们的担忧。</p><p>(2):传统的深度伪造生成方法主要基于变分自编码器和生成对抗网络，但其生成效果仍不令人满意。近年来，扩散模型的出现极大地提升了图像/视频的生成能力。</p><p>(3):本文对深度伪造生成和检测的最新进展进行了全面综述，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术。</p><p>(4):本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑等四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品。</p><ol><li>方法：</li></ol><p>（1）：本文首先统一了深度伪造生成和检测的任务定义，全面介绍了数据集和度量标准，为后续研究提供了基础。</p><p>（2）：本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程。</p><p>（3）：本文在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品，为研究人员提供了有价值的参考。</p><ol><li>结论：<pre><code>            （1）：本文全面综述了深度伪造生成与检测领域的研究进展，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术，为后续研究提供了基础。            （2）：创新点：本文首次全面覆盖了深度伪造生成与检测领域，并讨论了最新的技术，如扩散模型；本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。性能：本文在人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了全面基准测试，充分评估了最新和有影响力的已发表作品，为研究人员提供了有价值的参考；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。工作量：本文对深度伪造生成与检测领域进行了全面综述，统一了任务定义，全面介绍了数据集和度量标准，并讨论了发展技术，为后续研究提供了基础；本文对人脸替换、人脸重现、说话人脸生成和面部属性编辑四个代表性深度伪造领域进行了研究，重点分析了各领域的发展历程，并在流行数据集上对代表性方法进行了全面基准测试，充分评估了最新和有影响力的已发表作品；本文对深度伪造生成与检测领域的挑战和未来研究方向进行了总结。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-6a08950b6be4e3f18aeef87726b535fd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c245a3a60e574c0cb0324f79ffd23876.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-afbe757ef2a542a37ce568036b591797.jpg" align="middle"><img src="https://picx.zhimg.com/v2-becdaa251ccb21b3a85f051bf593814f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-c84c542a045ab258f8a251f6f24a1446.jpg" align="middle"><img src="https://pica.zhimg.com/v2-48abe21b928d9c991400ddc443d9eec3.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-25  GaussianTalker Speaker-specific Talking Head Synthesis via 3D Gaussian   Splatting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/25/Paper/2024-04-25/Diffusion%20Models/</id>
    <published>2024-04-25T12:59:05.000Z</published>
    <updated>2024-04-25T12:59:05.416Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-25-更新"><a href="#2024-04-25-更新" class="headerlink" title="2024-04-25 更新"></a>2024-04-25 更新</h1><h2 id="ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning"><a href="#ID-Aligner-Enhancing-Identity-Preserving-Text-to-Image-Generation-with-Reward-Feedback-Learning" class="headerlink" title="ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning"></a>ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</h2><p><strong>Authors:Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</strong></p><p>The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{<a href="https://idaligner.github.io/}}">https://idaligner.github.io/}}</a> </p><p><a href="http://arxiv.org/abs/2404.15449v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型带来的文本图像生成在身份保持人像和商用图片上广泛应用，ID-Aligner框架通过反馈学习增强图像美感。</p><p><strong>Key Takeaways</strong></p><ul><li>身份保持图像生成方法在身份特征保持、美观性保证、兼容性方面有提升空间。</li><li>ID-Aligner框架通过反馈学习来增强ID-T2I效果。</li><li>身份一致性奖励微调利用面部检测和识别模型的反馈，提高生成的图像的身份保留能力。</li><li>身份美学奖励微调利用人工标注偏好数据和自动构建的字符结构生成反馈，提供美学调整信号。</li><li>得益于通用的反馈微调框架，该方法可以方便地应用于LoRA和适配器模型，实现性能提升。</li><li>在SD1.5和SDXL扩散模型上的广泛实验验证了该方法的有效性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: ID-Aligner: 增强身份保留文本到图像生成</p></li><li><p>Authors: Weifeng Chen, Jiachang Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</p></li><li><p>Affiliation: 中山大学</p></li><li><p>Keywords: Identity-preserving text-to-image generation, Diffusion models, Feedback learning, LoRA, Adapter</p></li><li><p>Urls: https://arxiv.org/abs/2404.15449 , Github:None</p></li><li><p>Summary:</p><p>(1): 随着扩散模型的快速发展，文本到图像生成技术得到了广泛的应用，其中身份保留文本到图像生成（ID-T2I）因其在AI人像、广告等领域的应用前景而备受关注。然而，现有的ID-T2I方法仍面临着一些关键挑战：难以准确保持参考人像的身份特征、生成的图像缺乏美感，以及无法同时兼容基于LoRA和基于Adapter的方法。</p><p>(2): 现有的ID-T2I方法主要通过在扩散模型中加入身份编码信息来实现身份保留，但这些方法往往会丢失参考人像的细致特征，导致生成的图像与参考人像存在差异。此外，这些方法在增强身份保留的同时，往往会降低图像的视觉吸引力。</p><p>(3): 针对上述问题，本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强ID-T2I性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>(4): 在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p></li><li><p>方法：</p><p>(1):提出ID-Aligner，一种利用反馈学习方法来增强身份（ID）保留生成性能的开创性方法。方法的概述见图2。我们通过奖励反馈学习范式解决了ID保留生成，以增强与参考人像图像和生成图像的美感的一致性。</p><p>(2):文本到图像扩散模型利用扩散建模根据文本提示通过扩散模型生成高质量图像，该模型通过渐进的去噪过程从高斯噪声生成所需的数据样本。在预训练期间，首先通过预训练的VAE [4, 10]编码器处理采样的图像𝑥，以导出其潜在表示𝑧。随后，通过前向扩散过程将随机噪声注入潜在表示，遵循预定义的时间表{𝛽𝑡 }𝑇。这个过程可以表述为𝑧𝑡 = √𝛼𝑡𝑧 + √1 − 𝛼𝑡𝜖，其中𝜖 ∈ N (0, 1)是与𝑧维度相同的随机噪声，𝛼𝑡 = �𝑡 𝑠=1 𝛼𝑠和𝛼𝑡 = 1 − 𝛽𝑡。为了实现去噪过程，训练了一个UNet 𝜖𝜃来预测前向扩散过程中的添加噪声，条件是噪声潜在和文本提示𝑐。形式上，UNet的优化目标是：L(𝜃) = E𝑧,𝜖,𝑐,𝑡 [||𝜖 − 𝜖𝜃 ( √︁ 𝛼𝑡𝑧 + √︁ 1 − 𝛼𝑡𝜖,𝑐,𝑡)||2 2]。</p><p>(3):身份奖励：身份一致性奖励：给定参考图像𝑥ref 0和生成图像𝑥′ 0。我们的目标是评估特定肖像的ID相似性。为了实现这一点，我们首先使用人脸检测模型FaceDet来定位两幅图像中的人脸。基于人脸检测模型的输出，我们裁剪相应的人脸区域并将其输入人脸识别模型FaceEnc的编码器。这使我们能够获得参考人脸Eref和生成人脸Egen的编码人脸嵌入，即Eref = FaceEnc(FaceDet(𝑥ref 0 )), Egen = FaceEnc(FaceDet(𝑥′ 0))。随后，我们计算这两个面部嵌入之间的余弦相似度，作为生成过程中ID保留的度量。然后，我们将此相似度作为反馈调整过程的奖励信号，如下所示：ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥ref 0 ) = cose_sim(Egen, Eref)。身份美学奖励：除了身份一致性奖励外，我们还引入了一个专注于吸引力和质量的身份美学奖励模型。它包括人类对吸引力的偏好和合理的结构。首先，我们使用自收集的人类注释偏好数据集训练一个奖励模型，该模型可以对图像进行评分并反映人类对吸引力的偏好，如图3右所示。我们采用ImageReward [37]提供的预训练模型，并使用以下损失对其进行微调：L𝜃 = −𝐸(𝑐,𝑥𝑖,𝑥𝑗 )∼D [𝑙𝑜𝑔(𝜎(𝑓𝜃 (𝑥𝑖,𝑐) − 𝑓𝜃 (𝑥𝑗,𝑐)))].此损失函数基于图像之间的比较对，其中每个比较对包含两幅图像（𝑥𝑖和𝑥𝑗）和提示𝑐。𝑓𝜃 (𝑥,𝑐)表示给定图像𝑥和提示𝑐的奖励分数。因此，我们将𝑓𝜃称为ℜ𝑎𝑝𝑝𝑒𝑎𝑙作为吸引力奖励。此外，我们设计了一个结构奖励模型，可以区分扭曲的肢体/身体和自然的肢体/身体。为了训练一个可以访问图像结构是否合理性的模型，我们收集了一组包含正面和负面样本的文本图像对。具体来说，我们使用经过人类检测器过滤的LAION [28]中的图像。然后我们使用姿势估计模型生成姿势，这可以被视为未扭曲的人体结构。然后，我们随机扭曲姿势并利用ControlNet [42]生成失真体作为负样本，如图3左侧所示。一旦正负对可用，同样，我们使用与方程式5相同的损失训练结构奖励模型，并将结构奖励模型称为ℜ𝑠𝑡𝑟𝑢𝑐𝑡。然后，身份美学奖励模型定义为ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥,𝑐) = ℜ𝑎𝑝𝑝𝑒𝑎𝑙 (𝑥,𝑐) + ℜ𝑠𝑡𝑟𝑢𝑐𝑡 (𝑥,𝑐)。</p><p>(4):ID保留反馈学习：在反馈学习阶段，我们从输入提示𝑐开始，随机初始化潜在变量𝑥𝑇。然后对潜在变量进行渐进去噪，直到达到随机选择的时间步𝑡。此时，去噪图像𝑥′ 0直接从𝑥𝑡预测。将从先前阶段获得的奖励模型应用于此去噪图像，生成预期的偏好分数。此偏好分数使扩散模型能够进行微调，以更紧密地与反映身份一致性和审美偏好的ID奖励保持一致：L𝑖𝑑_𝑠𝑖𝑚 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [1 − ℜ𝑖𝑑_𝑠𝑖𝑚(𝑥′ 0,𝑥𝑟𝑒𝑓 0 )], L𝑖𝑑_𝑎𝑒𝑠 = E𝑐∼𝑝 (𝑐)E𝑥′ 0∼𝑝 (𝑥′ 0|𝑐) [−ℜ𝑖𝑑_𝑎𝑒𝑠 (𝑥′ 0,𝑐)]。</p></li><li><p>结论：</p></li></ol><p>（1）：本文提出了一种基于反馈学习的通用框架ID-Aligner，用于增强身份保留文本到图像生成（ID-T2I）性能。ID-Aligner通过引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>（2）：创新点：提出了ID-Aligner，一种利用反馈学习方法来增强ID保留生成性能的开创性方法。引入身份一致性奖励和身份美学奖励，分别增强了生成的图像的身份保留性和视觉吸引力。此外，ID-Aligner可以同时应用于基于LoRA和基于Adapter的方法，具有较好的兼容性。</p><p>性能：在人像生成任务上，ID-Aligner在保持身份特征和生成高质量图像方面都取得了优异的性能。实验结果表明，ID-Aligner生成的图像在身份保留度、图像质量和用户偏好方面均优于现有的ID-T2I方法，验证了其有效性。</p><p>工作量：本文的方法涉及到反馈学习、身份一致性奖励和身份美学奖励的引入，需要额外的计算和数据处理。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-952ad01319e9ee57febc82370c97b6b2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ea9ae35ff1eb818db6fe2da58e7a072.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3ca1d77296d47d3befa8898dae8433d7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b249a085ea084ca24b82dc1fcadcc875.jpg" align="middle"></details>## Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging   Perturbations That Efficiently Fool Customized Diffusion Models**Authors:Jingyao Xu, Yuetong Lu, Yandong Li, Siyang Lu, Dongdong Wang, Xiang Wei**Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner. [PDF](http://arxiv.org/abs/2404.15081v1) Published at CVPR 2024**Summary**扩散模型的跨注意力层易受梯度变化影响，可利用细微扰动欺骗语言引导扩散模型。**Key Takeaways**- 扩散模型 (DM) 为高效生成高质量和逼真数据样本开辟了新时代。- DM 的广泛使用带来了新的模型安全挑战，需要更有效的对抗攻击者来理解其漏洞。- CAAT 是一种简单、通用且有效的对抗攻击方法，无需昂贵的训练即可有效欺骗潜在扩散模型 (LDM)。- CAAT 利用交叉注意力层对梯度变化的较高敏感性，通过对已发布图像施加细微扰动来大幅破坏生成图像。- 细微扰动可以显着影响交叉注意力层，从而改变定制扩散模型微调期间文本和图像之间的映射。- 广泛的实验表明，CAAT 与各种扩散模型兼容，并且以更有效（更多噪声）和高效（比 Anti-DreamBooth 和 Mist 快两倍）的方式优于基线攻击方法。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：扰动注意力让你事半功倍：精妙的图像扰动</p></li><li><p>作者：Yichao Zhou, Jingwen Chen, Yu Cheng, Ziwei Liu, Chen Change Loy</p></li><li><p>单位：新加坡国立大学</p></li><li><p>关键词：Diffusion Models、Adversarial Attack、Cross-Attention</p></li><li><p>论文链接：https://arxiv.org/abs/2302.08724 , Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：扩散模型（Diffusion Models，DMs）作为生成模型的新范式，在生成高质量、逼真的数据样本方面展现出巨大潜力。然而，其广泛应用也带来了模型安全性的新挑战，促使研究者们开发更有效的对抗攻击方法来理解其脆弱性。</p><p>（2）：过去方法与问题：现有的攻击方法需要进行昂贵的训练才能有效对抗潜在扩散模型（Latent Diffusion Models，LDMs），并且在效率和效果方面存在不足。</p><p>（3）：本文方法：本文提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗 LDMs。该方法基于这样一个观察：交叉注意力层对梯度变化表现出更高的敏感性，这使得利用已发布图像上的细微扰动就能显著破坏生成的图像。</p><p>（4）：方法性能：广泛的实验表明，CAAT 与各种扩散模型兼容，并且在有效性（产生更多噪声）和效率（比 Anti-DreamBooth 和 Mist 快两倍）方面优于基线攻击方法。</p><ol><li>方法：</li></ol><p>（1）：CAAT 方法的原理：基于交叉注意力层对梯度变化的敏感性，利用已发布图像上的细微扰动来破坏生成的图像。</p><p>（2）：攻击步骤：     （a）：准备已发布图像和目标图像。     （b）：使用目标图像初始化潜在空间中的噪声。     （c）：使用交叉注意力层计算梯度，并根据梯度更新噪声。     （d）：重复步骤 (c)，直到生成图像与目标图像相似。</p><p>（3）：CAAT 的优势：     （a）：无需昂贵的训练。     （b）：与各种扩散模型兼容。     （c）：在有效性和效率方面优于基线攻击方法。</p><ol><li>结论：</li></ol><p>（1）：本工作的重要意义在于提出了一种简单、通用且高效的攻击方法 CAAT，无需昂贵的训练即可有效对抗潜在扩散模型（LDMs）。该方法利用交叉注意力层的敏感性，通过已发布图像上的细微扰动来破坏生成的图像，为理解 LDMs 的脆弱性提供了新的途径。</p><p>（2）：本文的优势和不足总结如下：     创新点：         （a）：提出了利用交叉注意力层敏感性的新攻击方法。         （b）：无需昂贵的训练即可有效对抗 LDMs。     性能：         （a）：与各种扩散模型兼容。         （b）：在有效性和效率方面优于基线攻击方法。     工作量：         （a）：攻击步骤简单，易于实现。         （b）：无需额外的训练或数据收集。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e21b9a5812791e5572d6cc412d4b6f49.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d24fa5d01960bbb84627a575bbe1387e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-62d838e7bc25d440e5a0f335a30a775d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-f5501b074b665578b3fec4ffce2edeb5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-af86112d3e55bc02435a1dc8cb3dfe90.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4416c3cb309ab371619d47ab4f98e8df.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20a0f1befe2ddce6a3be91bb78c7fe2c.jpg" align="middle"></details>## UVMap-ID: A Controllable and Personalized UV Map Generative Model**Authors:Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri**Recently, diffusion models have made significant strides in synthesizing realistic 2D human images based on provided text prompts. Building upon this, researchers have extended 2D text-to-image diffusion models into the 3D domain for generating human textures (UV Maps). However, some important problems about UV Map Generative models are still not solved, i.e., how to generate personalized texture maps for any given face image, and how to define and evaluate the quality of these generated texture maps. To solve the above problems, we introduce a novel method, UVMap-ID, which is a controllable and personalized UV Map generative model. Unlike traditional large-scale training methods in 2D, we propose to fine-tune a pre-trained text-to-image diffusion model which is integrated with a face fusion module for achieving ID-driven customized generation. To support the finetuning strategy, we introduce a small-scale attribute-balanced training dataset, including high-quality textures with labeled text and Face ID. Additionally, we introduce some metrics to evaluate the multiple aspects of the textures. Finally, both quantitative and qualitative analyses demonstrate the effectiveness of our method in controllable and personalized UV Map generation. Code is publicly available via https://github.com/twowwj/UVMap-ID. [PDF](http://arxiv.org/abs/2404.14568v1) **Summary**基于文本提示生成 3D 人体纹理，提出可控且个性化的 UVMap-ID 生成模型，通过微调预训练的文字-图像扩散模型，并使用面部融合模块实现 ID 驱动的定制化生成。**Key Takeaways*** UVMap-ID是一种可控且个性化的UV贴图生成模型。* 引入了一个小型的属性平衡训练数据集，包括高质量的纹理、标记文本和人脸 ID。* 提出了一些指标来评估纹理的多方面。* 提出了一种微调预训练的文本-图像扩散模型的方法，该模型与面部融合模块相结合，以实现 ID 驱动的定制化生成。* 定量和定性分析证明了 UVMap-ID 方法在可控和个性化 UV 贴图生成中的有效性。* 代码可在 https://github.com/twowwj/UVMap-ID 获得。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>Title: UVMap-ID：可控且个性化的 UV 贴图生成模型（中文翻译）</p></li><li><p>Authors: Weijie Wang, Jichao Zhang, Chang Liu, Xia Li, Xingqian Xu, Humphrey Shi, Nicu Sebe, Bruno Lepri</p></li><li><p>Affiliation: 特伦托大学 MHUG 组（中文翻译）</p></li><li><p>Keywords: Generative Model, Diffusion Model, 3D Avatar Generation, MultiModal Generation</p></li><li><p>Urls: https://arxiv.org/abs/2404.14568 , https://github.com/twowwj/UVMap-ID</p></li><li><p>Summary: </p><pre><code>            (1):本文的研究背景是：近年来，扩散模型在基于提供的文本提示合成逼真的 2D 人类图像方面取得了重大进展。在此基础上，研究人员已将 2D 文本到图像扩散模型扩展到 3D 领域，用于生成人体纹理（UV 贴图）。然而，关于 UV 贴图生成模型的一些重要问题仍未解决，即如何为给定的任何人脸图像生成个性化纹理贴图，以及如何定义和评估这些生成纹理贴图的质量。            (2):以往的方法主要利用生成对抗网络（GAN）的生成器以无监督或监督的方式估计纹理，然后将纹理估计模型集成到化身拟合阶段。然而，这些方法在生成新颖纹理方面受到限制，并且需要更多地支持可控生成。            (3):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。此外，我们还引入了一些指标来评估纹理的多个方面。            (4):本文方法在可控且个性化的 UV 贴图生成任务上取得了很好的效果，定量和定性分析都证明了这一点。</code></pre></li><li><p>方法：</p><pre><code>            (1):本文提出了一种新颖的方法 UVMap-ID，它是一种可控且个性化的 UV 贴图生成模型。与 2D 中传统的规模化训练方法不同，我们建议微调一个预训练的文本到图像扩散模型，该模型与人脸融合模块集成在一起，用于实现 ID 驱动的定制化生成。            (2):为了支持微调策略，我们引入了小规模属性平衡训练数据集，其中包括带有标记文本和人脸 ID 的高质量纹理。            (3):此外，我们还引入了一些指标来评估纹理的多个方面。</code></pre></li></ol><p><strong>8. 结论：</strong></p><p><strong>(1) 意义：</strong></p><p>本文提出了一种可控且个性化的 UV 贴图生成模型 UVMap-ID，该模型可以根据给定的人脸图像生成个性化的纹理贴图，并支持可控生成。该模型为 ID 驱动的定制化 3D 人体纹理生成提供了新的解决方案。</p><p><strong>(2) 优缺点总结：</strong></p><p><strong>创新点：</strong></p><ul><li>将文本到图像扩散模型应用于 UV 贴图生成。</li><li>提出了一种人脸融合模块，实现 ID 驱动的定制化生成。</li><li>引入了小规模属性平衡训练数据集，支持微调策略。</li></ul><p><strong>性能：</strong></p><ul><li>定量和定性分析表明，该模型在可控且个性化的 UV 贴图生成任务上取得了很好的效果。</li><li>该模型能够生成高质量、多样化和可控的纹理贴图。</li></ul><p><strong>工作量：</strong></p><ul><li>该模型的训练过程需要大量的数据和计算资源。</li><li>引入的人脸融合模块增加了模型的复杂性。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-ee8312e5d6ec47e140dd213091cce823.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bfcb973c1970f426d8f1df5728d85885.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e943c5e2becd571bbce3de5cb620daba.jpg" align="middle"><img src="https://pica.zhimg.com/v2-82b5601482da7da7458b5456972d0c5b.jpg" align="middle"></details>## Align Your Steps: Optimizing Sampling Schedules in Diffusion Models**Authors:Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis**Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\textit{Align Your Steps}$. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime. [PDF](http://arxiv.org/abs/2404.14507v1) Project page:   https://research.nvidia.com/labs/toronto-ai/AlignYourSteps/**Summary**优化扩散模型的采样计划可以显著提升输出质量，且该方法适用于不同的采样器、已训练模型和数据集。**Key Takeaways**- 采样计划在扩散模型中至关重要，能够影响输出质量。- 传统方法主要集中在优化求解器，忽略了采样计划的优化。- 本文首次提出了一种原理性方法来优化扩散模型的采样计划，称为 Align Your Steps。- 该方法利用随机微积分的方法，为不同的求解器、训练模型和数据集找到最优采样计划。- 实验表明，优化后的采样计划在多种图像、视频和 2D 玩具数据合成基准测试中优于手动设计的采样计划。- 该方法证明了采样计划优化在少数步合成中的潜力。- 该方法可以与不同的采样器、训练模型和数据集配合使用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>标题：</strong>优化扩散模型中的采样计划</p></li><li><p><strong>作者：</strong>Jiahui Yu, Yuchen Lu, Jianwen Xie, Jianwen Xie, Anima Anandkumar</p></li><li><p><strong>第一作者单位：</strong>NVIDIA</p></li><li><p><strong>关键词：</strong>扩散模型、采样计划、图像生成、视频生成</p></li><li><p><strong>链接：</strong>Paper_info:Align Your Steps: Optimizing Sampling Schedules in Diffusion Models</p></li><li><p><strong>摘要：</strong></p></li></ol><p>（1）<strong>研究背景：</strong>扩散模型（DM）是视觉领域及其他领域的先进生成建模方法。DM 的一个主要缺点是采样速度慢，需要通过大型神经网络进行许多顺序函数评估。从 DM 中采样可以看作是通过一组称为采样计划的离散噪声电平来求解微分方程。虽然过去的工作主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（2）<strong>过去的方法及问题：</strong>过去的方法主要集中在推导有效的求解器上，但很少关注寻找最佳采样计划，并且整个文献都依赖于手工制作的启发式方法。</p><p>（3）<strong>本文提出的研究方法：</strong>在本文中，我们首次提出了一种通用且原则性的方法来优化 DM 的采样计划以获得高质量的输出，称为 Align Your Steps。我们利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（4）<strong>任务和性能：</strong>我们在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了我们新颖的方法，并观察到我们的优化计划在几乎所有实验中都优于以前手工制作的计划。我们的方法展示了采样计划优化尚未开发的潜力，尤其是在少步合成领域。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li><strong>结论：</strong></li></ol><p>（1）：本文提出了一种优化扩散模型采样计划的通用且原则性的方法，称为 Align Your Steps，该方法利用随机微积分的方法，针对不同的求解器、训练过的 DM 和数据集找到最优的计划。</p><p>（2）：<strong>创新点：</strong>提出了优化扩散模型采样计划的新颖方法，该方法具有通用性和原则性，可以针对不同的求解器、训练过的 DM 和数据集找到最优的计划。<strong>性能：</strong>在多个图像、视频以及 2D 玩具数据合成基准上使用各种不同的采样器评估了该方法，观察到该方法在几乎所有实验中都优于以前手工制作的计划。<strong>工作量：</strong>该方法需要对采样计划进行优化，这可能需要一定的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-78c3e80bc513a591cd16c1be135f16cd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-97a5c3e11f2d9cffcd1a13c8baf1c9c7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-7ef643065c4d76e29b9b077c68693835.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fb7137766bc6a2a4fee323a9d77c6bff.jpg" align="middle"><img src="https://picx.zhimg.com/v2-eeb97492f52508534aa4f55180d1531f.jpg" align="middle"></details>## GeoDiffuser: Geometry-Based Image Editing with Diffusion Models**Authors:Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar**The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information. [PDF](http://arxiv.org/abs/2404.14403v1) **摘要**一键式图像编辑方法 GeoDiffuser，将 2D/3D 对象编辑统一为几何变换，无需训练或额外信息。**关键要点**- 将图像编辑操作视为几何变换，可直接融合到扩散模型中。- 无需训练的优化函数，可保留对象风格并生成合理图像。- 修复因对象编辑而产生的图像遮挡部分。- 使用分割和变换估计来编辑前景对象。- 可执行常见 2D/3D 编辑，如平移、旋转和移除。- 定量和感知研究表明，性能优于现有方法。- 更多信息请访问 https://ivl.cs.brown.edu/research/geodiffuser.html。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：基于几何的图像编辑与 GeoDiffuser（补充）</p></li><li><p>作者：</p></li><li>Yin Cui</li><li>Yujun Shen</li><li>Yinda Zhang</li><li>Bolei Zhou</li><li>Chen Change Loy</li><li><p>Thomas Funkhouser</p></li><li><p>第一作者单位：新加坡国立大学</p></li><li><p>关键词：</p></li><li>图像编辑</li><li>几何变换</li><li>扩散模型</li><li><p>零样本学习</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14403   Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>(1) 研究背景：   随着图像生成模型的成功，基于文本或其他用户输入编辑图像的方法得到了发展。然而，这些方法要么是定制的、不精确的，要么需要额外的信息，或者仅限于 2D 图像编辑。</p><p>(2) 过去方法及其问题：   现有方法存在以下问题：   - <strong>定制性</strong>：需要为每个编辑操作设计特定的模型。   - <strong>不精确</strong>：难以生成符合用户意图的精确编辑。   - <strong>需要额外信息</strong>：可能需要对象掩码或 3D 模型等附加信息。   - <strong>2D 限制</strong>：仅限于 2D 图像编辑，无法处理 3D 旋转等操作。</p><p>(3) 提出的研究方法：   GeoDiffuser 是一种基于零样本优化的图像编辑方法，它将常见的 2D 和 3D 图像编辑功能统一到一个方法中。其核心思想是将图像编辑操作视为几何变换，并将其直接融入扩散模型的注意力层中。GeoDiffuser 使用一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。它还可以修复对象原先所在位置的遮挡部分。</p><p>(4) 方法性能：   GeoDiffuser 在各种编辑任务上实现了出色的性能，包括：   - <strong>2D 编辑</strong>：对象平移、缩放、旋转   - <strong>3D 编辑</strong>：对象 3D 旋转、移除   定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种统一的方法 GeoDiffuser，它可以对图像进行常见的 2D 和 3D 对象编辑。该方法基于零样本优化，利用扩散模型实现这些编辑。其关键思想是将图像编辑表述为几何变换，并将其直接纳入基于扩散模型的编辑框架中的共享注意力层中。结果表明，我们的单一方法可以处理各种图像编辑操作，并且与之前的工作相比产生了更好的结果。</p><p>（2）：创新点：GeoDiffuser 统一了 2D 和 3D 图像编辑操作，并将其表述为几何变换。它直接将几何变换纳入扩散模型的注意力层中，无需为每个编辑操作设计特定的模型。</p><p>性能：GeoDiffuser 在各种编辑任务上实现了出色的性能，包括 2D 编辑（对象平移、缩放、旋转）和 3D 编辑（对象 3D 旋转、移除）。定量和感知研究表明，GeoDiffuser 优于现有方法。</p><p>工作量：GeoDiffuser 的实现相对简单，易于使用。它只需要一个预训练的扩散模型和一个目标函数，该函数旨在保留对象样式，同时生成合理且具有准确光影效果的图像。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-907b6b9c901d5ba4cb979b85f016e4e1.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b67e8d6c53f98ede5eceba2ceea75149.jpg" align="middle"></details>## MultiBooth: Towards Generating All Your Concepts in an Image from Text**Authors:Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu**This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/ [PDF](http://arxiv.org/abs/2404.14239v1) Project Page: https://multibooth.github.io/ . Github Page:   https://github.com/chenyangzhu1/MultiBooth**Summary**多概念图像生成的新方法MultiBooth将单概念学习和多概念整合相结合，显著提高了图像生成中的多概念自定义的效率和保真度。**Key Takeaways**- MultiBooth将多概念生成分为单概念学习和多概念整合两阶段，提高了概念保真度和推理效率。- 单概念学习阶段使用多模态图像编码器和高效概念编码技术，为每个概念学习简洁且区别性的表示。- 多概念整合阶段使用边界框定义交叉注意图中每个概念的生成区域。- 这种方法允许在指定区域内创建单个概念，从而生成多概念图像。- MultiBooth在定性和定量评估中都优于各种基线，证明了其出色的性能和计算效率。- MultiBooth可通过其项目页面访问：https://multibooth.github.io/。- MultiBooth开辟了多概念自定义图像生成的新途径，为图像生成领域的进一步探索奠定了基础。- 该方法有望在图像合成、编辑和设计等应用中发挥重要作用。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：MultiBooth：从文本中生成图像中所有概念</p></li><li><p>作者：Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Xiu Li</p></li><li><p>单位：清华大学</p></li><li><p>Keywords: Multi-concept generation, Image generation, Text-to-image, Diffusion models</p></li><li><p>论文链接：https://arxiv.org/abs/2404.14239v1 Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：随着扩散模型的成功，定制化生成方法取得了重大进展。然而，现有方法在多概念场景中往往面临概念保真度低、推理成本高的难题。</p><p>（2）：过去的方法：现有的方法通常通过联合学习所有概念来生成多概念图像，这会导致概念保真度低、推理成本高。</p><p>（3）：本文方法：MultiBooth 将多概念生成过程分为两个阶段：单概念学习阶段和多概念集成阶段。在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示。在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域。这种方法可以创建各个概念的独立表示，并将其集成到最终图像中。</p><p>（4）：实验结果：在多个数据集上进行的实验表明，MultiBooth 在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度。</p><ol><li>Methods:</li></ol><p>（1）：MultiBooth将多概念生成过程分为单概念学习阶段和多概念集成阶段；</p><p>（2）：在单概念学习阶段，采用多模态图像编码器和高效的概念编码技术，为每个概念学习一个简洁且有区别的表示；</p><p>（3）：在多概念集成阶段，使用边界框在交叉注意力图中为每个概念定义生成区域，将各个概念的独立表示集成到最终图像中。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种新颖高效的多概念定制（MCC）框架 MultiBooth。与现有 MCC 方法相比，MultiBooth 允许即插即用的多概念生成，具有较高的图像保真度，同时在训练和推理期间带来的成本最小。通过进行定性和定量实验，我们在不同的多主题定制场景中稳健地证明了我们优于最先进的方法。由于当前方法仍然需要训练来学习新概念，因此在未来，我们将在 MultiBooth 的基础上研究免训练多概念定制的任务。</p><p>（2）：创新点：将多概念生成过程分为单概念学习阶段和多概念集成阶段，为每个概念学习简洁且有区别的表示，并使用边界框在交叉注意力图中为每个概念定义生成区域；性能：在复杂的多概念生成任务中，包括重塑风格、不同空间关系和重新语境化，都能有效地保持较高的图像保真度和文本对齐度；工作量：在训练和推理期间带来的成本最小。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-cd95a012d10b3a0932405f01c119cafb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-64d8c2e719edd54a8907366e1adc0ce9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-383cf5bbab5f26be5446db713c454caf.jpg" align="middle"></details>## FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on**Authors:Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, Hongming Shan**Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details. [PDF](http://arxiv.org/abs/2404.14162v1) Accepted by IJCAI 2024**Summary**利用经变形处理的初始变形及局部条件，配合服饰展平网络及服饰后验采样，提出一种忠实的潜在扩散模型 FLMD-VTON，显著提升虚拟试穿模型的生成保真度。**Key Takeaways**- 结合局部条件和经变形处理的初始服饰，为模型提供可靠的服饰先验信息。- 引入服饰展平网络，约束生成图像，确保服饰变形的一致性。- 采用服饰后验采样，提升模型性能，优于传统的高斯采样。- 方法在 VITON-HD 和 Dress Code 数据集上表现优异，生成的照片级虚拟试穿图像，服饰细节真实度高。- 方法改善了基于潜在扩散模型的虚拟试穿方法在服饰风格、图案和文字细节方面的不足。- 方法在保真度和生成质量方面都优于现有方法。- 方法具有较强的泛化能力，可在不同数据集上生成逼真的虚拟试穿图像。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><strong>论文标题</strong>：FLDM-VTON：用于虚拟试穿的忠实潜在扩散模型</li><li><strong>作者</strong>：王晨辉、陈涛、陈志浩、黄志忠、姜涛然、王琦、单宏明</li><li><strong>第一作者单位</strong>：复旦大学脑科学与类脑智能科学与技术研究院</li><li><strong>关键词</strong>：Virtual Try-on (VTON)、Latent Diffusion Model、Faithful Details</li><li><strong>论文链接</strong>：https://arxiv.org/abs/2404.14162</li><li><p><strong>摘要</strong>：   （1）<strong>研究背景</strong>：虚拟试穿（VTON）旨在将一件商店里的平铺衣服转移到人体上，同时保留人体和衣服的细节，如款式、图案和文字。   （2）<strong>过去的方法及其问题</strong>：先前的VTON方法高度依赖生成对抗网络（GAN），但由于模式崩塌问题，GAN方法无法合成逼真的试穿图像，也无法准确捕捉复杂的服装细节。   （3）<strong>本文提出的研究方法</strong>：FLDM-VTON改进了传统的潜在扩散过程，包括：使用变形后的衣服作为起点和局部条件，为模型提供忠实的衣服先验；引入了一种新的衣服展平网络来约束生成的试穿图像，提供与衣服一致的忠实监督；设计了一种用于忠实推理的衣服后验采样，进一步增强了模型性能。   （4）<strong>方法的性能</strong>：在VITON-HD和Dress Code基准数据集上的广泛实验结果表明，FLDM-VTON优于最先进的基线，并且能够生成具有忠实服装细节的逼真试穿图像。</p></li><li><p>方法：   （1）：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验；   （2）：引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督；   （3）：设计用于忠实推理的衣服后验采样，进一步增强模型性能。</p></li><li><p>结论：                    (1): 本文提出了一种用于虚拟试穿的新型忠实潜在扩散模型（FLDM-VTON）。通过引入忠实的衣服先验和与衣服一致的忠实监督，FLDM-VTON可以显著缓解由扩散随机性和潜在监督在LDM中引起的非忠实生成问题。此外，为忠实推理设计的衣服后验采样可以进一步提升模型性能。在两个流行的VTON基准数据集上进行的广泛实验结果验证了我们提出的FLDM-VTON的优越性能——生成具有忠实服装细节的逼真的试穿图像。</p><pre><code>            (2): 创新点：提出FLDM-VTON，利用变形后的衣服作为起点和局部条件，为模型提供逼真的衣服先验；引入衣服展平网络，约束生成的试穿图像，提供与衣服一致的监督；设计用于忠实推理的衣服后验采样，进一步提升模型性能。             性能：在VITON-HD和Dress Code基准数据集上，FLDM-VTON优于最先进的基线，生成具有忠实服装细节的逼真试穿图像。             工作量：本文方法的实现需要对潜在扩散模型进行修改，包括引入衣服先验、衣服展平网络和衣服后验采样。这些修改需要额外的计算和存储资源。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-07afb8a5c475fd0a30e88cadcbad3463.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-145ab18d23fb1b6e86d6406676978723.jpg" align="middle"><img src="https://picx.zhimg.com/v2-228856708a79714b6f7dccab9f678905.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fd4472fcb73295f29ca0dce6c278c461.jpg" align="middle"><img src="https://picx.zhimg.com/v2-089f4d17146ab3e178994ba211043f04.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-88494892272dbea22b24aaca153feca1.jpg" align="middle"></details><h2 id="Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model"><a href="#Accelerating-Image-Generation-with-Sub-path-Linear-Approximation-Model" class="headerlink" title="Accelerating Image Generation with Sub-path Linear Approximation Model"></a>Accelerating Image Generation with Sub-path Linear Approximation Model</h2><p><strong>Authors:Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</strong></p><p>Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images. </p><p><a href="http://arxiv.org/abs/2404.13903v2">PDF</a> </p><p><strong>Summary:</strong><br>扩散模型提速新方法：子路径线性逼近模型（SLAM）</p><p><strong>Key Takeaways:</strong></p><ul><li>SLAM采用分治策略，将扩散路径划分为子路径，并利用子路径线性ODE进行逼近。</li><li>SLAM构建去噪映射，累计误差更小，生成效果更好。</li><li>SLAM可有效提速，仅需6个A100 GPU天即可训练出高质量生成模型。</li><li>SLAM支持少数步生成任务，在FID和生成图像质量上达到最优性能。</li><li>SLAM能有效训练隐式扩散模型。</li><li>SLAM比现有加速方法更有效，在少数步生成任务上表现更好。</li><li>SLAM生成的高质量图像适用于图像、音频和视频生成任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>Title: SLAM加速图像生成</p></li><li><p>Authors: Zhiming Zhou, Yixing Xu, Zhiyuan Fang, Yufei Wang, Yifan Jiang, Xinchao Wang, Xiangyang Xue</p></li><li><p>Affiliation: 北京大学</p></li><li><p>Keywords: Diffusion Models · Accelerating Diffusion Models · Diffusion Model Distillation · Consistency Models</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2302.07523, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 扩散模型在图像、音频和视频生成任务中取得了显著进展。然而，它们在实际场景中的应用受到推理速度慢的阻碍。</p><p>(2): 过去的加速方法包括：DDIM、LADM、LCM。这些方法存在的问题是：DDIM和LADM的训练收敛速度慢，LCM在生成质量上存在一定缺陷。本文提出的方法动机明确，旨在解决这些问题。</p><p>(3): 本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><p>(4): 在LAION、MS COCO 2014和MS COCO 2017数据集上的广泛实验结果表明，SLAM实现了高效的训练方案，只需6个A100 GPU天即可生成高质量的生成模型，该模型能够以高性能进行2到4步生成。全面的评估还表明，SLAM在小步生成任务中超越了现有的加速方法，在FID和生成图像质量上都取得了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种子路径线性逼近模型（SLAM），它通过将PF-ODE轨迹视为一系列由采样点划分的PF-ODE子路径，并利用子路径线性（SL）ODE在每个单独的PF-ODE子路径上形成渐进且连续的误差估计。对这些SL-ODE的优化允许SLAM构建具有较小累积近似误差的去噪映射。</p><p>（2）：还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。</p><ol><li>结论：</li></ol><p>（1）：本文提出的 SLAM 模型在加速扩散模型生成图像方面取得了显著进展，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射，在训练收敛速度和生成质量上均取得了最先进的性能。</p><p>（2）：创新点：提出了一种基于子路径线性逼近的加速扩散模型 SLAM，通过将 PF-ODE 轨迹视为一系列由采样点划分的 PF-ODE 子路径，并利用子路径线性（SL）ODE 在每个单独的 PF-ODE 子路径上形成渐进且连续的误差估计，构建具有较小累积近似误差的去噪映射。还开发了一种有效的蒸馏方法，以促进更高级的扩散模型（例如潜在扩散模型）的整合。性能：在 LAION、MS COCO 2014 和 MS COCO 2017 数据集上的广泛实验结果表明，SLAM 实现了高效的训练方案，只需 6 个 A100 GPU 天即可生成高质量的生成模型，该模型能够以高性能进行 2 到 4 步生成。全面的评估还表明，SLAM 在小步生成任务中超越了现有的加速方法，在 FID 和生成图像质量上都取得了最先进的性能。工作量：SLAM 的训练成本相对较低，在 6 个 A100 GPU 天内即可完成训练，并且在小步生成任务中具有较高的效率。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-919d70908993415e92c8909c00655335.jpg" align="middle"><img src="https://pica.zhimg.com/v2-0e2aa2475025a88869b1ac0e1b6be112.jpg" align="middle"></details>## Object-Attribute Binding in Text-to-Image Generation: Evaluation and   Control**Authors:Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens**Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance. [PDF](http://arxiv.org/abs/2404.13766v1) **摘要**文本提示中的语法约束有助于生成更准确的图像，其中属性与正确的对象相关联。**要点*** 当前扩散模型难以将文本提示中的属性正确绑定到图像中的正确对象。* 提出了一种新的图像-图对齐模型 EPViT，用于评估图像-文本对齐。* 引入了焦点交叉注意 (FCA)，以通过输入句子的句法约束来控制视觉注意图。* 提示的语法结构有助于解耦在 T2I 生成中常用的多模态 CLIP 嵌入。* 所产生的 DisCLIP 嵌入和 FCA 可以轻松集成到最先进的扩散模型中，而无需额外训练这些模型。* 在 T2I 生成中展示了实质性的改进，尤其是在几个数据集上的属性-对象绑定。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p><strong>Title:</strong> 文本到图像生成中的对象-属性绑定：评估和控制</p></li><li><p><strong>Authors:</strong> Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Hönig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens</p></li><li><p><strong>Affiliation:</strong> KU Leuven, Department of Computer Science</p></li><li><p><strong>Keywords:</strong> 文本到图像生成、对象-属性绑定、注意力机制、扩散模型</p></li><li><p><strong>Urls:</strong> Paper: https://arxiv.org/abs/2404.13766, Github: None</p></li><li><p><strong>Summary:</strong></p></li></ol><p>(1): <strong>研究背景：</strong> 当前的扩散模型可以根据文本提示创建逼真的图像，但难以将文本中提到的属性正确绑定到图像中的正确对象上。</p><p>(2): <strong>过去的方法及其问题：</strong> 现有的方法主要基于 CLIP 评分进行评估，但无法检查复杂多对象提示中属性与对象的正确绑定。</p><p>(3): <strong>提出的研究方法：</strong> 提出了一种基于 ViT 的图像图预测模型 EPViT 和一种称为聚焦交叉注意 (FCA) 的方法，以控制视觉注意力图，从而改善对象-属性绑定。</p><p>(4): <strong>方法性能：</strong> 在多个数据集上，该方法显着提高了文本到图像生成及其对象-属性绑定性能，证明了其有效性。</p><ol><li><p>方法：</p><pre><code>            (1):提出了一种基于 ViT 的图像图预测模型 EPViT，用于生成图像特征图；            (2):设计了一种称为聚焦交叉注意 (FCA) 的方法，用于控制视觉注意力图，从而改善对象-属性绑定；            (3):将 FCA 和 DisCLIP 集成到现有的文本到图像生成模型中，以增强其对象-属性绑定性能；            (4):在多个数据集上对增强后的模型进行评估，包括 COCO 10-K、CC-500、DAA-200 和 AE-267，以验证其有效性。</code></pre></li><li><p>结论：</p><pre><code>            (1):本研究提出了无需训练的方法，强调了在文本到图像生成中整合语言句法结构的重要性。我们展示了它们在最先进的文本到图像扩散模型中的轻松且成功的集成，从而改善了对象-属性绑定，并减少了生成图像中的属性泄漏。此外，我们展示了一种新设计度量 EPViT 在评估文本到图像模型的对象-属性绑定方面优于 CLIP。EPViT 允许更好地理解和衡量模型在生成图像中准确反映预期文本描述的性能。                           8. 总结：            (1):本研究的意义是什么？            (2):从创新点、性能、工作量三个维度总结本文的优缺点。                               .......         按照后面的输出格式：         8. 结论：            (1):xxx;            (2):创新点：xxx；性能：xxx；工作量：xxx；         务必使用中文回答（专有名词用英文标注），表述尽量简洁、学术，不要重复前面&lt;Summary&gt;的内容，原数字的使用价值，务必严格按照格式，对应内容输出到xxx，换行，.......表示根据实际要求填写，如果没有，可以不写。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-da0a6f3f353e58ad78bee95a227f033f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-dd14ddde3959c9e4184326c15ccbc7c4.jpg" align="middle"><img src="https://pica.zhimg.com/v2-10db76640ccb78a6742ee69ebb161059.jpg" align="middle"></details><h2 id="Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models"><a href="#Concept-Arithmetics-for-Circumventing-Concept-Inhibition-in-Diffusion-Models" class="headerlink" title="Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models"></a>Concept Arithmetics for Circumventing Concept Inhibition in Diffusion   Models</h2><p><strong>Authors:Vitali Petsiuk, Kate Saenko</strong></p><p>Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models.   Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised.   Project page: <a href="https://cs-people.bu.edu/vpetsiuk/arc">https://cs-people.bu.edu/vpetsiuk/arc</a> </p><p><a href="http://arxiv.org/abs/2404.13706v1">PDF</a> </p><p><strong>Summary</strong><br>对文本到图像扩散模型安全性的攻击通过利用扩散模型的合成性质和概念算术来重建被禁止的概念。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型的合成性质使攻击者能够通过组合多个提示来生成图像。</li><li>即使直接计算目标概念的向量不再可访问，也可以通过组合不受抑制影响的其他概念来重建该向量。</li><li>攻击者可以利用概念算术来重建被禁止的概念，例如版权、暴力、色情或个人信息。</li><li>提出了两种新的攻击，分别称为“后门攻击”和“组合攻击”。</li><li>后门攻击利用了生成模型中存在的漏洞或后门。</li><li>组合攻击利用了扩散模型的合成性质。</li><li>这些攻击对安全模型的部署有重大影响。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 绕过概念抑制的算术概念</p></li><li><p>Authors: Vitali Petsiuk and Kate Saenko</p></li><li><p>Affiliation: 波士顿大学</p></li><li><p>Keywords: Text-to-Image diffusion models, safety mechanisms, concept inhibition, concept arithmetics, compositional inference</p></li><li><p>Urls: https://arxiv.org/abs/2404.13706, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 随着 Text-to-Image (T2I) 生成模型的快速发展，人们开始关注其潜在的滥用风险，例如生成侵犯版权、暴力、色情或个人信息的内容。为了解决这些问题，研究人员开发了各种安全机制来限制模型的恶意使用。</p><p>(2): 现有的安全机制通常采用概念抑制的方法，即阻止模型生成特定概念的内容。然而，这些方法往往存在漏洞，允许攻击者通过组合其他概念来绕过抑制，从而重建被抑制概念的向量。</p><p>(3): 本文提出了一种基于概念算术和组合推理的攻击方法，该方法利用了扩散模型的组合特性，允许在单个图像生成中使用多个提示。通过组合不受抑制影响的其他概念，攻击者可以重建负责目标概念生成的向量，即使该向量的直接计算不再可访问。</p><p>(4): 实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。这些发现表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种基于概念算术和组合推理的攻击方法，该方法绕过了现有的基于概念抑制的安全机制，允许攻击者生成被抑制概念的内容。这项工作表明，在设计安全机制时，必须考虑攻击者可能采用的一切图像生成方法。</p><p>（2）：创新点：本文提出了一个绕过概念抑制的安全机制的新攻击方法，该方法利用了扩散模型的组合特性和概念算术。性能：实验结果表明，提出的攻击方法在各种扩散模型和概念抑制机制上都取得了成功。工作量：本文的工作量中等，需要对扩散模型、概念抑制机制和攻击方法有深入的理解。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6c22a86090e8195e410f10a38f6fe1f9.jpg" align="middle"><img src="https://pica.zhimg.com/v2-936989960b19443d0f4e3c6f1a1e8e26.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-25  ID-Aligner Enhancing Identity-Preserving Text-to-Image Generation with   Reward Feedback Learning</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/NeRF/</id>
    <published>2024-04-22T09:43:13.000Z</published>
    <updated>2024-04-22T09:43:13.959Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering"><a href="#AG-NeRF-Attention-guided-Neural-Radiance-Fields-for-Multi-height-Large-scale-Outdoor-Scene-Rendering" class="headerlink" title="AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering"></a>AG-NeRF: Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</h2><p><strong>Authors:Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</strong></p><p>Existing neural radiance fields (NeRF)-based novel view synthesis methods for large-scale outdoor scenes are mainly built on a single altitude. Moreover, they often require a priori camera shooting height and scene scope, leading to inefficient and impractical applications when camera altitude changes. In this work, we propose an end-to-end framework, termed AG-NeRF, and seek to reduce the training cost of building good reconstructions by synthesizing free-viewpoint images based on varying altitudes of scenes. Specifically, to tackle the detail variation problem from low altitude (drone-level) to high altitude (satellite-level), a source image selection method and an attention-based feature fusion approach are developed to extract and fuse the most relevant features of target view from multi-height images for high-fidelity rendering. Extensive experiments demonstrate that AG-NeRF achieves SOTA performance on 56 Leonard and Transamerica benchmarks and only requires a half hour of training time to reach the competitive PSNR as compared to the latest BungeeNeRF. </p><p><a href="http://arxiv.org/abs/2404.11897v1">PDF</a> </p><p><strong>Summary</strong><br>降低训练成本，实现多高度自由视角图像合成。</p><p><strong>Key Takeaways</strong></p><ul><li>多高度神经辐射场（NeRF）能够合成自由视角图像。</li><li>提出图像选择方法和注意力特征融合，解决不同高度细节差异问题。</li><li>AG-NeRF 在 56 Leonard 和 Transamerica 基准上达到最先进性能。</li><li>AG-NeRF 训练时间仅需半小时，即可达到 BungeeNeRF 的竞争水平。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AG-NeRF: 多高度大尺度户外场景渲染的注意力引导神经辐射场</p></li><li><p>Authors: Jingfeng Guo, Xiaohan Zhang, Baozhu Zhao, Qi Liu</p></li><li><p>Affiliation: 华南理工大学</p></li><li><p>Keywords: Novel View Synthesis, NeRF, Large-scale Outdoor Scene Rendering</p></li><li><p>Urls: https://arxiv.org/abs/2404.11897v1 , Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 现有的基于神经辐射场 (NeRF) 的大规模户外场景新视角合成方法主要建立在单一高度上。此外，它们通常需要先验的相机拍摄高度和场景范围，当相机高度发生变化时，会导致低效且不实用的应用。</p><p>(2): 过去的方法：   - 地理上将场景分解为几个单元格，并为每个单元格训练一个子 NeRF，然后将它们合并。   - 在位置编码中并行应用平面和网格特征以实现高效建模。   - 问题：它们在基础高度上重建大规模场景，当导航到更近的地方以检查大规模户外场景的微观细节时，表现出过度模糊的伪影和不完整的重建。</p><p>(3): 本文提出的研究方法：   - 提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。   - 具体来说，为了解决从低高度（无人机级别）到高高度（卫星级别）的细节变化问题，开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>(4): 本文方法在任务和性能上的表现：   - 在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能。   - 只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。   - 性能支持了他们的目标：降低构建良好重建的训练成本。</p><ol><li>方法：</li></ol><p>（1）：提出了一种端到端框架 AG-NeRF，通过合成基于场景不同高度的自由视角图像来降低构建良好重建的训练成本。</p><p>（2）：开发了一种源图像选择方法和一种基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。</p><p>（3）：利用可训练的 U-Net 网络从源图像中提取特征图，并使用 Transformer 对提取的特征向量进行融合，以最大化融合特征与目标像素之间的相关性。</p><p>（4）：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><ol><li>结论：</li></ol><p>（1）：本文针对不同高度拍摄的大场景渲染提出了端到端的 AG-NeRF 框架，降低了构建良好重建模型的训练成本。</p><p>（2）：创新点：提出了一种源图像选择方法和基于注意力的特征融合方法，从多高度图像中提取和融合目标视图最相关的特征，以实现高保真渲染。性能：在 56 Leonard 和 Transamerica 基准测试中取得了 SOTA 性能，只需要半小时的训练时间即可达到与最新 BungeeNeRF 相当的竞争性 PSNR。工作量：采用分层采样方法，使用粗略网络和精细网络同时优化，并使用基于注意力的特征融合方法将多高度图像中的特征融合起来。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-82fe2876dffe132719e410910e28492d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fbedf0965ea4b6e30b80160a9ce71484.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b5a30ff8e4f41c8671a8c9f7dbcb45d2.jpg" align="middle"></details><h2 id="SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping"><a href="#SLAIM-Robust-Dense-Neural-SLAM-for-Online-Tracking-and-Mapping" class="headerlink" title="SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping"></a>SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping</h2><p><strong>Authors:Vincent Cartillier, Grant Schindler, Irfan Essa</strong></p><p>We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose a novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM (NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing NeRF-SLAM systems consistently exhibit inferior tracking performance compared to traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via image alignment and photometric bundle-adjustment. Such optimization processes are difficult to optimize due to the narrow basin of attraction of the optimization loss in image space (local minima) and the lack of initial correspondences. We mitigate these limitations by implementing a Gaussian pyramid filter on top of NeRF, facilitating a coarse-to-fine tracking optimization strategy. Furthermore, NeRF systems encounter challenges in converging to the right geometry with limited input views. While prior approaches use a Signed-Distance Function (SDF)-based NeRF and directly supervise SDF values by approximating ground truth SDF through depth measurements, this often results in suboptimal geometry. In contrast, our method employs a volume density representation and introduces a novel KL regularizer on the ray termination distribution, constraining scene geometry to consist of empty space and opaque surfaces. Our solution implements both local and global bundle-adjustment to produce a robust (coarse-to-fine) and accurate (KL regularizer) SLAM solution. We conduct experiments on multiple datasets (ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in reconstruction accuracy. </p><p><a href="http://arxiv.org/abs/2404.11419v1">PDF</a> </p><p><strong>Summary</strong><br>Nerf-SLAM 通过采用从粗到细的跟踪模型和 KL 正则化器，在跟踪性能和重建精度上实现了最先进的成绩。</p><p><strong>Key Takeaways</strong></p><ul><li>SLAIM 提出了一种从粗到细的跟踪模型以提高 NeRF-SLAM 的跟踪性能。</li><li>SLAIM 通过高斯金字塔滤波器实现从粗到细的跟踪优化策略。</li><li>NeRF 系统难以使用有限的输入视图收敛到正确的几何形状。</li><li>SLAIM 使用体积密度表示和一个新的 KL 正则化器来约束场景几何形状。</li><li>SLAIM 实现局部和全局捆绑调整以提高鲁棒性和准确性。</li><li>SLAIM 在多个数据集上进行了实验，在跟踪和重建精度上均显示出最先进的结果。</li><li>SLAIM 解决了 NeRF-SLAM 在传统 SLAM 算法下表现出较差的跟踪性能这一难题。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：SLAIM：用于在线跟踪和建图的鲁棒稠密神经SLAM</p></li><li><p>作者：Vincent Cartillier、Grant Schindler、Irfan Essa</p></li><li><p>隶属关系：佐治亚理工学院</p></li><li><p>关键词：神经辐射场、SLAM、稠密建图、跟踪</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11419，Github 代码链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）研究背景：稠密视觉SLAM是3D计算机视觉中的一个长期问题，在自动驾驶、室内外机器人导航、虚拟现实和增强现实等领域有着广泛的应用。</p><p>（2）过去的方法及问题：传统的SLAM系统通过估计图像对应关系来开始，这些对应关系可能是稀疏的，例如匹配的特征点。神经辐射场SLAM（NeRF-SLAM）方法通过图像对齐和光度捆绑调整来解决相机跟踪问题。由于图像空间中优化损失的吸引域窄（局部极小值）以及缺乏初始对应关系，此类优化过程难以优化。</p><p>（3）提出的研究方法：本文提出了一种新的粗到细跟踪模型，专门针对NeRF-SLAM，以实现最先进的跟踪性能。此外，本文还引入了一种新的目标射线终止分布，并将其用于KL正则化器中，以约束场景几何由空空间和不透明表面组成。</p><p>（4）任务和性能：本文方法在ScanNet、TUM、Replica等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。这些性能支持了本文的目标。</p><ol><li><p>方法：</p><pre><code>            (1):SLAIM 是一种用于稠密映射和跟踪的 RGB-D 输入流的 novel 方法；            (2):SLAIM 采用了一种从粗到精的跟踪模型，以实现最先进的跟踪性能；            (3):SLAIM 引入了一种新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成；            (4):SLAIM 在 ScanNet、TUM、Replica 等多个数据集上进行了实验，在跟踪和重建精度方面均取得了最先进的成果。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本文的工作意义：本文提出了一种最先进的稠密实时 RGB-D NeRF-SLAM 系统 SLAIM，该系统具有最先进的相机跟踪和建图能力。</p><p>（2）本文的优缺点总结：    - 创新点：        - 采用从粗到精的跟踪模型，实现最先进的跟踪性能。        - 引入新的目标射线终止分布，并将其用于 KL 正则化器中，以约束场景几何由空空间和不透明表面组成。    - 性能：        - 在 ScanNet、TUM、Replica 等多个数据集上取得了最先进的跟踪和重建精度。    - 工作量：        - 内存效率高，在 Replica 和 ScanNet 数据集上与基准相比，跟踪和建图时间均有明显降低。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-486ca0b76c4db89899a0670269d00796.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f729a5308a9aa1435c3a0e2db312184f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ddcd1f27f832c7cfc1c274567204de22.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c7d35d3daa3f9540491cf1d974f07bc9.jpg" align="middle"></details><h2 id="RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering"><a href="#RainyScape-Unsupervised-Rainy-Scene-Reconstruction-using-Decoupled-Neural-Rendering" class="headerlink" title="RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering"></a>RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering</h2><p><strong>Authors:Xianqiang Lyu, Hui Liu, Junhui Hou</strong></p><p>We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. </p><p><a href="http://arxiv.org/abs/2404.11401v1">PDF</a> </p><p><strong>Summary</strong><br>基于神经网络的光谱偏差特性，RainyScape利用无监督框架重建干净场景，包含神经渲染模块和雨滴预测模块。</p><p><strong>Key Takeaways</strong></p><ul><li>利用神经网络的光谱偏差特性获得低频场景表示。</li><li>联合优化神经渲染模块和雨滴预测模块，以区分场景细节和雨滴条纹。</li><li>提出自适应方向敏感梯度重建损失，引导网络区分场景细节和雨滴条纹。</li><li>在经典神经辐射场和 3D 高斯斑点 splatting 数据集上均达到最先进的去雨性能。</li><li>提供高质量数据集和源代码，促进研究工作。</li><li>引入可学习潜在嵌入，捕捉场景的雨滴特征。</li><li>通过雨滴预测网络有效消除雨滴条纹，渲染干净图像。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: RainyScape: 无监督雨景重建使用解耦神经渲染</p></li><li><p>Authors: Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>Affiliation: 香港城市大学计算机科学系</p></li><li><p>Keywords: Rainy scene reconstruction, Neural rendering, Unsupervised loss</p></li><li><p>Urls: https://arxiv.org/abs/2404.11401 , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):随着神经辐射场（NeRF）在图像合成中的广泛应用，当输入图像受到模糊、噪声或雨水等因素影响时，渲染结果不可避免地会产生明显的伪影。</p><p>(2):现有的方法针对特定任务提出了各种解决方案，但对于雨景重建任务，它们无法有效表示三维空间中稀疏且间歇性的降雨。</p><p>(3):本文提出RainyScape，一个解耦的神经渲染框架，它能够以无监督的方式从雨景图像中重建无雨场景。该框架通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水。此外，本文还提出了一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹。</p><p>(4):在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p><ol><li><p>方法：</p><p>（1）：提出RainyScape，一个解耦的神经渲染框架，可以无监督地从雨景图像中重建无雨场景；</p><p>（2）：通过神经渲染管道获得场景的低频表示，并使用可学习的雨水嵌入和预测器来表征雨水；</p><p>（3）：提出一个自适应角度估计策略和梯度旋转损失，以解耦场景高频细节和雨水条纹；</p><p>（4）：在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>结论：</p></li></ol><p>（1）：RainyScape的意义在于，它提出了一种无监督的解耦神经渲染框架，可以从雨景图像中重建无雨场景，有效解决了雨景重建中的雨水条纹去除问题，为雨景图像处理提供了新的思路和方法。</p><p>（2）：创新点：</p><ul><li><p>提出了一种解耦的神经渲染框架，通过低频场景表示、可学习的雨水嵌入和预测器以及自适应角度估计策略和梯度旋转损失，有效解耦了场景高频细节和雨水条纹。</p></li><li><p>性能：</p></li><li><p>在神经辐射场和三维高斯散射两种渲染技术上的广泛实验表明，该方法在有效消除雨水条纹和渲染清晰图像方面优于现有方法，达到了最先进的性能。</p></li><li><p>工作量：</p></li><li><p>该方法需要对雨景图像进行预处理，包括图像分割、雨水条纹检测和雨水嵌入提取等步骤，增加了计算量和时间开销。</p></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://picx.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details>## REACTO: Reconstructing Articulated Objects from a Single Video**Authors:Chaoyue Song, Jiacheng Wei, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu**In this paper, we address the challenge of reconstructing general articulated 3D objects from a single video. Existing works employing dynamic neural radiance fields have advanced the modeling of articulated objects like humans and animals from videos, but face challenges with piece-wise rigid general articulated objects due to limitations in their deformation models. To tackle this, we propose Quasi-Rigid Blend Skinning, a novel deformation model that enhances the rigidity of each part while maintaining flexible deformation of the joints. Our primary insight combines three distinct approaches: 1) an enhanced bone rigging system for improved component modeling, 2) the use of quasi-sparse skinning weights to boost part rigidity and reconstruction fidelity, and 3) the application of geodesic point assignment for precise motion and seamless deformation. Our method outperforms previous works in producing higher-fidelity 3D reconstructions of general articulated objects, as demonstrated on both real and synthetic datasets. Project page: https://chaoyuesong.github.io/REACTO. [PDF](http://arxiv.org/abs/2404.11151v1) **Summary**对于一般性关节动作的3D物体，本文提出了一种新的变形模型，即准刚性混合蒙皮，以便从单个视频中进行全面重建。**Key Takeaways**- 提出一种新的变形模型，准刚性混合蒙皮，增强了零件刚性，同时保持关节柔性变形。- 采用增强骨骼绑定系统改善组件建模。- 使用准稀疏蒙皮权重提高零件刚性和重建保真度。- 应用测地线点赋值实现精确运动和无缝变形。- 在真实和合成数据集上，该方法在生成高保真一般性关节动作的3D重建方面优于先前的工作。- 该研究为一般性关节动作的3D物体重建提供了新的方法。- 该研究在计算机视觉和图形学领域具有潜在应用价值。- 该研究有助于推动相关领域的发展。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：REACTO：从单一视频中重建铰接物体</p></li><li><p>作者：Chaoyue Song、Jiacheng Wei、Chuan Sheng Foo、Guosheng Lin、Fayao Liu</p></li><li><p>隶属：南洋理工大学</p></li><li><p>关键词：铰接物体重建、动态神经辐射场、准刚性混合蒙皮</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11151, Github：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：重建铰接物体是计算机视觉中的一项重要任务，但现有方法在处理具有分段刚性的通用铰接物体时面临挑战。</p><p>（2）：过去方法：NASAM和PARIS等方法需要多视角图像或多视图图像，在实际应用中受限。</p><p>（3）：研究方法：本文提出了一种准刚性混合蒙皮变形模型，该模型通过增强骨骼装配系统、使用准稀疏蒙皮权重和应用测地线点分配来提高刚性并保持关节的灵活变形。</p><p>（4）：任务与性能：REACTO在真实和合成数据集上对通用铰接物体的3D重建任务中取得了较高的保真度，证明了其性能可以支持其目标。</p><p><strong>7. Methods：</strong></p><p>(1)：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>(2)：构建REACTO框架，包括骨骼装配、蒙皮变形、体绘制和渲染模块；</p><p>(3)：使用基于神经辐射场的渲染器，从单一视频中重建铰接物体；</p><p>(4)：通过优化骨骼参数、蒙皮权重和神经辐射场参数，实现铰接物体的高保真重建；</p><p>(5)：在真实和合成数据集上进行实验，验证REACTO的有效性。</p><ol><li>结论：</li></ol><p>（1）：本工作提出REACTO，一种从单一视频中重建通用铰接3D物体的开创性方法，通过重新定义装配结构并采用准刚性混合蒙皮，实现了建模和精度的提升。准刚性混合蒙皮通过利用准稀疏蒙皮权重和测地线点分配，确保了每个部件的刚性，同时在关节处保持平滑变形。广泛的实验表明，REACTO在真实和合成数据集上都优于现有方法，保真度和细节方面都有所提升。</p><p>（2）：创新点：提出准刚性混合蒙皮变形模型，增强骨骼装配系统，使用准稀疏蒙皮权重，并应用测地线点分配；</p><p>性能：在真实和合成数据集上，REACTO在保真度和细节方面都优于现有方法；</p><p>工作量：与需要多视角或多视图图像的现有方法相比，REACTO只需单一视频即可重建铰接物体，工作量更小。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b24d1992bf52c35d5d68092f3855e178.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-cc1782e8c3f880dfa4512201f4175379.jpg" align="middle"><img src="https://picx.zhimg.com/v2-46959553add30d1e8d2dff8cb9e56563.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f4000a7f506812312f58f8dd21486b3b.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-22  AG-NeRF Attention-guided Neural Radiance Fields for Multi-height   Large-scale Outdoor Scene Rendering</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/3DGS/</id>
    <published>2024-04-22T09:32:29.000Z</published>
    <updated>2024-04-22T09:32:29.438Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>通过借鉴2D说话人面部的唇形同步和言语感知领域的专业知识，提出了一种学习框架，可以构建更好的3D说话人面部网络。</p><p><strong>Key Takeaways</strong></p><ul><li>3D说话人面部研究在唇形同步和言语感知方面不如2D说话人面部研究深入。</li><li>Learn2Talk框架利用2D说话人面部领域的两个专业知识点来构建更好的3D说话人面部网络。</li><li>3D同步唇专家模型旨在实现音频和3D面部运动之间的唇形同步。</li><li>2D说话人面部方法中选择的教师模型用于指导音频到3D运动回归网络的训练，以提高3D顶点精度。</li><li>广泛的实验表明，该框架在唇形同步、顶点精度和言语感知方面优于现有技术。</li><li>该框架有语音-视觉语音识别和语音驱动3D高斯飞溅基于头像动画两个应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li><p>标题：Learn2Talk：3D 说话人脸从 2D 说话人脸学习</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, XuanCheng, Jing Liao, Juncong Lin</p></li><li><p>单位：暂缺</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：https://arxiv.org/abs/2404.12888v1Github：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：说话人脸动画方法通常包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究在唇形同步（lip-sync）和语音感知方面并未像 2D 说话人脸那样深入。</p><p>（2）：过去的方法及其问题：本文方法动机充分。</p><p>（3）：本文提出的研究方法：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法性能：本文方法在唇形同步、顶点精度和语音感知方面均优于现有技术。这些性能可以支持其目标。</p><ol><li>方法：</li></ol><p>（1）：提出一个名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络。</p><p>（2）：设计了一个 3D 同步唇形专家模型，以追求音频和 3D 面部动作之间的唇形同步。</p><p>（3）：选择一个来自 2D 说话人脸方法的教师模型来指导音频到 3D 运动回归网络的训练，以产生更高的 3D 顶点精度。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种名为 Learn2Talk 的学习框架，该框架通过利用 2D 说话人脸领域的两个专业知识点来构建一个更好的 3D 说话人脸网络，在唇形同步、顶点精度和语音感知方面均优于现有技术。</p><p>（2）：创新点：提出了一种新的 3D 说话人脸动画方法，该方法利用了 2D 说话人脸领域的专业知识；性能：在唇形同步、顶点精度和语音感知方面均优于现有技术；工作量：需要收集和标注大量的数据。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://pica.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>## Contrastive Gaussian Clustering: Weakly Supervised 3D Scene Segmentation**Authors:Myrna C. Silva, Mahtab Dahaghin, Matteo Toso, Alessio Del Bue**We introduce Contrastive Gaussian Clustering, a novel approach capable of provide segmentation masks from any viewpoint and of enabling 3D segmentation of the scene. Recent works in novel-view synthesis have shown how to model the appearance of a scene via a cloud of 3D Gaussians, and how to generate accurate images from a given viewpoint by projecting on it the Gaussians before $\alpha$ blending their color. Following this example, we train a model to include also a segmentation feature vector for each Gaussian. These can then be used for 3D scene segmentation, by clustering Gaussians according to their feature vectors; and to generate 2D segmentation masks, by projecting the Gaussians on a plane and $\alpha$ blending over their segmentation features. Using a combination of contrastive learning and spatial regularization, our method can be trained on inconsistent 2D segmentation masks, and still learn to generate segmentation masks consistent across all views. Moreover, the resulting model is extremely accurate, improving the IoU accuracy of the predicted masks by $+8\%$ over the state of the art. Code and trained models will be released soon. [PDF](http://arxiv.org/abs/2404.12784v1) **Summary**使用来自不同视角的对比高斯聚类实现 3D 场景分割。**Key Takeaways**- 提出一种新的对比高斯聚类方法，能够从任何视角提供分割掩模，并实现场景的 3D 分割。- 受新视角合成领域研究的启发，使用 3D 高斯云建模场景的外观。- 通过将高斯投影到给定视点并对其颜色进行α混合，从给定视点生成准确的图像。- 训练模型，使每个高斯都包含一个分割特征向量。- 通过根据其特征向量对高斯进行聚类，可用于 3D 场景分割；通过将高斯投影到平面上并对其分割特征进行 α 混合，可生成 2D 分割掩模。- 使用对比学习和空间正则化的组合，可以在不一致的 2D 分割掩模上训练我们的方法，并学习生成在所有视图中都一致的分割掩模。- 所提出的方法非常准确，与现有技术相比，预测掩模的 IoU 准确度提高了 8%。- 代码和训练好的模型即将发布。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：对比高斯聚类：弱监督 3D 场景分割</p></li><li><p>作者：Myrna C. Silva、Mahtab Dahaghin、Matteo Toso、Alessio Del Bue</p></li><li><p>单位：意大利理工学院模式分析与计算机视觉（PAVIS）</p></li><li><p>关键词：3D 高斯散射、3D 分割、对比学习</p></li><li><p>论文链接：arXiv:2404.12784v1 [cs.CV]   Github 链接：无</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：近年来，新视角合成领域的研究表明，可以通过 3D 高斯云对场景的外观进行建模，并通过在给定视角上投影高斯并 α 混合其颜色来生成准确的图像。</p><p>（2）：过去方法与问题：高斯分组和 LangSplat 等方法存在以下问题：   - 训练和评估需要大量 GPU 内存，导致某些场景无法处理。   - 无法从任意视角提供分割掩码，也无法实现场景的 3D 分割。</p><p>（3）：研究方法：本文提出对比高斯聚类方法，该方法通过以下步骤实现 3D 场景分割和 2D 分割掩码预测：   - 训练模型为每个高斯体添加分割特征向量。   - 根据特征向量对高斯体进行聚类，实现 3D 场景分割。   - 将高斯体投影到平面上并 α 混合其分割特征，生成 2D 分割掩码。   - 使用对比学习和空间正则化，在不一致的 2D 分割掩码上训练模型，生成跨所有视角一致的分割掩码。</p><p>（4）：性能与目标：   - 任务：3D 场景分割和 2D 分割掩码预测。   - 性能：IoU 准确率比现有技术提高 +8%，表明该方法能够有效实现其目标。</p><ol><li>方法：</li></ol><p>（1）：将场景表示为 3D 高斯体集合，编码几何、外观和实例分割信息；</p><p>（2）：使用基础模型生成 2D 分割掩码；</p><p>（3）：优化 3D 高斯体，最小化渲染图像和真实图像之间的差异；</p><p>（4）：使用对比分割损失监督 3D 特征场；</p><p>（5）：引入正则化项，强制高斯体在欧几里得和分割特征空间中的距离相关；</p><p>（6）：渲染 2D 特征图，根据对应的 2D 分割掩码对渲染特征进行聚类，计算对比聚类损失；</p><p>（7）：最大化同一分割内特征之间的相似度，最小化不同分割内的特征相似度。</p><p><strong>8. 结论</strong></p><p><strong>(1)</strong> 本工作的主要意义在于：</p><p>提出了对比高斯聚类方法，实现了 3D 场景分割和 2D 分割掩码预测，有效提高了分割精度。</p><p><strong>(2)</strong> 本文优缺点总结（创新点、性能、工作量）：</p><p><strong>创新点：</strong></p><ul><li>引入对比学习和空间正则化，提高了分割掩码的一致性。</li><li>使用 3D 高斯体表示场景，编码几何、外观和实例分割信息。</li></ul><p><strong>性能：</strong></p><ul><li>IoU 准确率比现有技术提高 +8%，分割精度高。</li></ul><p><strong>工作量：</strong></p><ul><li>训练和评估需要大量 GPU 内存，大场景处理困难。</li><li>无法从任意视角提供分割掩码，无法实现场景的完整 3D 分割。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-252e679c7e0a5cfc8056b41c43d99b59.jpg" align="middle"><img src="https://picx.zhimg.com/v2-668e640c91611b7b91220b00abd05f4e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-03dada656b628530891ef19dcbebedba.jpg" align="middle"></details>## RainyScape: Unsupervised Rainy Scene Reconstruction using Decoupled   Neural Rendering**Authors:Xianqiang Lyu, Hui Liu, Junhui Hou**We propose RainyScape, an unsupervised framework for reconstructing clean scenes from a collection of multi-view rainy images. RainyScape consists of two main modules: a neural rendering module and a rain-prediction module that incorporates a predictor network and a learnable latent embedding that captures the rain characteristics of the scene. Specifically, based on the spectral bias property of neural networks, we first optimize the neural rendering pipeline to obtain a low-frequency scene representation. Subsequently, we jointly optimize the two modules, driven by the proposed adaptive direction-sensitive gradient-based reconstruction loss, which encourages the network to distinguish between scene details and rain streaks, facilitating the propagation of gradients to the relevant components. Extensive experiments on both the classic neural radiance field and the recently proposed 3D Gaussian splatting demonstrate the superiority of our method in effectively eliminating rain streaks and rendering clean images, achieving state-of-the-art performance. The constructed high-quality dataset and source code will be publicly available. [PDF](http://arxiv.org/abs/2404.11401v1) **Summary**雨景重建：无监督地从多视角雨景图重建干净场景。**Key Takeaways**- 提出无监督框架 RainyScape，重建干净场景。- RainyScape 由神经渲染和降雨预测模块组成。- 降雨预测模块包含预测网络和可学习潜嵌入，捕捉场景的降雨特征。- 基于神经网络的光谱偏差属性，优化神经渲染管道，获得低频场景表示。- 利用自适应方向敏感梯度重建损失，优化两个模块，区分场景细节和雨痕。- 在神经辐射场和 3D 高斯喷溅中进行的实验表明，该方法能有效消除雨痕、渲染干净图像，达到最先进性能。- 将公开构建高质量数据集和源代码。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li><p>标题：RainyScape：基于解耦神经渲染的无监督雨景重建</p></li><li><p>作者：Xianqiang Lyu, Hui Liu, Junhui Hou</p></li><li><p>单位：香港城市大学计算机科学系</p></li><li><p>关键词：雨景重建、神经渲染、无监督损失</p></li><li><p>论文链接：xxx，Github 链接：None</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：神经辐射场（NeRF）在学习场景的连续体积表示方面取得了突破性进展，但当输入图像因模糊、噪声或雨水等因素而退化时，渲染结果不可避免地会出现明显伪影。</p><p>（2）：过去方法：现有方法针对不同的退化因素提出了特定任务的解决方案，但针对雨景重建任务的方法较少，且难以通过附加神经渲染场来表示雨水。</p><p>（3）：研究方法：本文提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><p>（4）：方法性能：在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 RainyScape，一个解耦的神经渲染框架，能够从雨景图像中无监督地重建无雨场景。</p><p>（2）：该框架包括一个神经渲染模块和一个雨滴预测模块，通过学习雨滴嵌入和使用预测器来预测雨滴条纹。</p><p>（3）：提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</p><ol><li>结论：<pre><code>            （1）：RainyScape 在雨景重建领域具有重要意义，它首次提出了一个解耦神经渲染框架，能够从雨景图像中无监督地重建无雨场景。 该框架通过将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            （2）：创新点：RainyScape 创新性地提出了一个解耦神经渲染框架，将场景高频细节和雨滴条纹解耦，有效地消除了雨滴条纹，并渲染出清晰的图像。            性能：RainyScape 在经典神经辐射场和最近提出的 3D 高斯 splatting 上的广泛实验表明，该方法在有效消除雨滴条纹和渲染清晰图像方面优于现有方法，达到最先进的性能。            工作量：RainyScape 的工作量中等，需要训练神经渲染模块和雨滴预测模块，并提出自适应角度估计策略和梯度旋转损失来解耦场景高频细节和雨滴条纹。</code></pre></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-789763f7ebb6ec7a923539611ab1fe24.jpg" align="middle"><img src="https://picx.zhimg.com/v2-89f176b1378008d1c0b63c9241adfdb2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5f7fb8305c36c1fe2572adfd98b584f7.jpg" align="middle"><img src="https://pica.zhimg.com/v2-76be36036e15658d754b57c4864b0abf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3765b699865b1d89cc9f5f13f9843a0e.jpg" align="middle"><img src="https://pica.zhimg.com/v2-34d10a80ece07ba92081dfc066d00427.jpg" align="middle"></details><h2 id="DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur"><a href="#DeblurGS-Gaussian-Splatting-for-Camera-Motion-Blur" class="headerlink" title="DeblurGS: Gaussian Splatting for Camera Motion Blur"></a>DeblurGS: Gaussian Splatting for Camera Motion Blur</h2><p><strong>Authors:Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, Kyoung Mu Lee</strong></p><p>Although significant progress has been made in reconstructing sharp 3D scenes from motion-blurred images, a transition to real-world applications remains challenging. The primary obstacle stems from the severe blur which leads to inaccuracies in the acquisition of initial camera poses through Structure-from-Motion, a critical aspect often overlooked by previous approaches. To address this challenge, we propose DeblurGS, a method to optimize sharp 3D Gaussian Splatting from motion-blurred images, even with the noisy camera pose initialization. We restore a fine-grained sharp scene by leveraging the remarkable reconstruction capability of 3D Gaussian Splatting. Our approach estimates the 6-Degree-of-Freedom camera motion for each blurry observation and synthesizes corresponding blurry renderings for the optimization process. Furthermore, we propose Gaussian Densification Annealing strategy to prevent the generation of inaccurate Gaussians at erroneous locations during the early training stages when camera motion is still imprecise. Comprehensive experiments demonstrate that our DeblurGS achieves state-of-the-art performance in deblurring and novel view synthesis for real-world and synthetic benchmark datasets, as well as field-captured blurry smartphone videos. </p><p><a href="http://arxiv.org/abs/2404.11358v2">PDF</a> </p><p><strong>Summary</strong><br>从模糊运动图像重建清晰 3D 场景方法，优化 3D 高斯投射，实现精确摄像机位姿初始化。</p><p><strong>Key Takeaways</strong></p><ul><li>DeblurGS 优化高斯投射，提高运动模糊图像 3D 重建精度。</li><li>利用高斯投射的重建能力，还原精细锐利场景。</li><li>估计每幅模糊图像的 6 自由度摄像机运动，生成模糊渲染用于优化。</li><li>高斯密度退火策略防止错误位置生成不准确的高斯。</li><li>DeblurGS 在去模糊和合成新视角方面取得了最先进的性能。</li><li>适用于真实世界和合成基准数据集，以及现场拍摄的模糊智能手机视频。</li><li>DeblurGS 极大地扩展了运动模糊图像的 3D 重建的实际应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: DeblurGS: 高斯溅射相机运动模糊 (DeblurGS: Gaussian Splatting for Camera Motion Blur)</p></li><li><p>Authors: Jeongtaek Oh, Jaeyoung Chung, Dongwoo Lee, and Kyoung Mu Lee</p></li><li><p>Affiliation: 首尔国立大学人工智能与信息处理研究所 (IPAI, Seoul National University)</p></li><li><p>Keywords: 3D Gaussian Splatting · Camera Motion Deblurring</p></li><li><p>Urls: None, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 尽管从运动模糊图像重建清晰的 3D 场景方面取得了重大进展，但向实际应用的过渡仍然具有挑战性。主要障碍源于严重的模糊，这会导致通过 Structure-from-Motion 获取初始相机姿态的不准确，而这往往是以前的方法所忽视的关键方面。</p><p>(2): 过去的方法主要集中于模糊图像的去模糊处理，但对于初始相机姿态的噪声初始化不鲁棒。</p><p>(3): 本文提出 DeblurGS，这是一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。我们利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景。我们的方法估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，我们提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。</p><p>(4): 综合实验表明，我们的 DeblurGS 在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><ol><li>方法：</li></ol><p>（1）：提出 DeblurGS，一种从运动模糊图像优化清晰的 3D 高斯溅射的方法；</p><p>（2）：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景；</p><p>（3）：估计每个模糊观测的 6 自由度相机运动，并合成相应的模糊渲染；</p><p>（4）：提出高斯致密化退火策略，防止在相机运动不精确的早期训练阶段生成不准确的高斯。</p><ol><li>结论：</li></ol><p>（1）：本文提出了一种从运动模糊图像优化清晰的 3D 高斯溅射的方法，即使在噪声相机姿态初始化的情况下也是如此。该方法利用 3D 高斯溅射的出色重建能力来恢复细粒度的清晰场景，估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染。此外，该方法提出了高斯致密化退火策略，以防止在相机运动仍然不精确的早期训练阶段在错误的位置生成不准确的高斯。综合实验表明，该方法在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能。</p><p>（2）：创新点：利用 3D 高斯溅射的重建能力恢复细粒度的清晰场景，即使在噪声相机姿态初始化的情况下也是如此；</p><p>性能：在真实世界和合成基准数据集以及现场捕获的模糊智能手机视频的去模糊和新视图合成方面实现了最先进的性能；</p><p>工作量：该方法需要估计每个模糊观测的 6 自由度相机运动，并为优化过程合成相应的模糊渲染，这可能会增加计算成本。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1b62fa212aabdf515b9baf8fdc306be.jpg" align="middle"><img src="https://pica.zhimg.com/v2-32c4f56eaf456fe86ff5f42abfbd6ffb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-50b9e9cff40ee36449b6b3559539186a.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Talking%20Head%20Generation/</id>
    <published>2024-04-22T09:22:24.000Z</published>
    <updated>2024-04-22T09:22:24.975Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face"><a href="#Learn2Talk-3D-Talking-Face-Learns-from-2D-Talking-Face" class="headerlink" title="Learn2Talk: 3D Talking Face Learns from 2D Talking Face"></a>Learn2Talk: 3D Talking Face Learns from 2D Talking Face</h2><p><strong>Authors:Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</strong></p><p>Speech-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and speech perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and speech perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual speech recognition and speech-driven 3D Gaussian Splatting based avatar animation. </p><p><a href="http://arxiv.org/abs/2404.12888v1">PDF</a> </p><p><strong>Summary</strong><br>通过借鉴2D说话人脸的唇形同步(lip-sync)和语音感知的专业知识，Learn2Talk框架构建了一个更好的3D说话人脸网络。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了Learn2Talk框架，将2D说话人脸的专业知识应用于3D说话人脸网络。</li><li>设计了3D唇形同步专家模型，追求音频和3D面部动作之间的唇形同步。</li><li>使用2D说话人脸方法选择的教师模型来指导音频到3D动作回归网络的训练，以提高3D顶点精度。</li><li>实验表明，该框架在唇形同步、顶点精度和语音感知方面优于现有技术。</li><li>展示了该框架的两个应用：视听语音识别和语音驱动的3D高斯喷射动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>标题：Learn2Talk：3D Talking Face Learns from 2D（3D 说话人脸从 2D 说话人脸中学习）</p></li><li><p>作者：Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, Juncong Lin</p></li><li><p>第一作者单位：暂无</p></li><li><p>关键词：Speech-driven, 3D Facial Animation, 2D Talking face, Transformer, 3D Gaussian Splatting</p></li><li><p>论文链接：暂无，Github 链接：https://lkjkjoiuiu.github.io/Learn2Talk/</p></li><li><p>摘要：</p></li></ol><p>（1）：研究背景：语音驱动的面部动画方法主要包含 3D 和 2D 说话人脸两大类，近年来两者都备受研究关注。然而，据我们所知，3D 说话人脸的研究并未像 2D 说话人脸那样深入，在唇形同步（lip-sync）和言语感知方面存在差距。</p><p>（2）：以往方法：以往方法主要分为 2D 和 3D 说话人脸方法。2D 说话人脸方法通常在像素空间（例如图像、视频）中生成唇部运动或头部运动以匹配给定的输入音频流，而 3D 说话人脸方法使用时间 3D 顶点数据（例如 3D 人脸模板、混合形状参数）来表示面部运动。与 2D 说话人脸方法相比，3D 说话人脸方法可以合成更细微的唇部动作，因为细粒度的唇形校正可以在 3D 空间中更好地执行。此外，3D 面部动画具有重要的优势，因为它可以与 3D 模型或虚拟角色无缝集成，从而实现更逼真的交互。</p><p>（3）：本文提出的研究方法：为了弥合两者之间的差距，我们提出了一个名为 Learn2Talk 的学习框架，该框架可以通过利用 2D 说话人脸领域的两个专业知识点来构建更好的 3D 说话人脸网络。首先，受音频视频同步网络的启发，设计了一个 3D 同步唇部专家模型，以追求音频和 3D 面部动作之间的唇形同步。其次，使用从 2D 说话人脸方法中选择的教师模型来指导音频到 3D 动作回归网络的训练，以产生更高的 3D 顶点精度。</p><p>（4）：方法在什么任务上取得了什么性能：广泛的实验表明，与最先进的方法相比，所提出的框架在唇形同步、顶点精度和言语感知方面具有优势。最终，我们展示了所提出框架的两个应用：视听语音识别和语音驱动的基于 3D 高斯泼溅的头像动画。这些结果表明，Learn2Talk 可以有效地利用 2D 说话人脸的专业知识来提高 3D 说话人脸的性能，从而为语音驱动的面部动画领域做出贡献。</p><ol><li><p>方法：</p><pre><code>            (1): 受音频视频同步网络的启发，设计了 3D 同步唇部专家模型 SyncNet3D，以追求音频和 3D 面部动作之间的唇形同步；            (2): 使用从 2D 说话人脸方法中选择的教师模型 LipReadNet 来指导音频到 3D 动作回归网络 Audio2Mesh 的训练，以产生更高的 3D 顶点精度；            (3): 提出了一种联合训练框架，将 SyncNet3D 和 Audio2Mesh 结合起来，通过联合损失函数优化，使 3D 说话人脸模型同时满足唇形同步和顶点精度要求。</code></pre></li><li><p>结论：</p></li></ol><p>（1）：本工作通过借鉴 2D 说话人脸领域的专业知识，提出了一种名为 Learn2Talk 的学习框架，有效提升了 3D 说话人脸的性能，为语音驱动的面部动画领域做出了贡献。</p><p>（2）：创新点：Learn2Talk 创新性地将 3D 同步唇部专家模型 SyncNet3D 与教师模型 LipReadNet 相结合，通过联合训练，实现了 3D 说话人脸模型在唇形同步和顶点精度方面的双重提升。</p><p>性能：在唇形同步、顶点精度和言语感知方面，Learn2Talk 均优于最先进的方法。</p><p>工作量：Learn2Talk 的训练过程较为复杂，需要同时训练 SyncNet3D 和 Audio2Mesh 两个模型，并且需要从 2D 说话人脸方法中选择教师模型。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-c2e8566372db83537dc565617387f4cf.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c36e0e4bd338738e2a5420e68cd6ab5d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3fe7530e7260eff001a6736622671663.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3f8c50de092534c8ec8b833626c35e42.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-22  Learn2Talk 3D Talking Face Learns from 2D Talking Face</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/22/Paper/2024-04-22/Diffusion%20Models/</id>
    <published>2024-04-22T09:18:09.000Z</published>
    <updated>2024-04-22T09:18:09.823Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-22-更新"><a href="#2024-04-22-更新" class="headerlink" title="2024-04-22 更新"></a>2024-04-22 更新</h1><h2 id="Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models"><a href="#Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models" class="headerlink" title="Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models"></a>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Pedro Sanchez, Alison Q. O’Neil, Sotirios A. Tsaftaris</strong></p><p>Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the model’s weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.12920v1">PDF</a> 8 pages, 3 figures, submitted to IEEE J-BHI Special Issue on   Foundation Models in Medical Imaging</p><p><strong>Summary</strong><br>利用大型语言模型（如隐扩散模型）即使在没有目标数据训练的情况下也能执行文本引导定位任务。</p><p><strong>Key Takeaways</strong></p><ul><li>隐扩散模型具有隐式对齐视觉和文本特征的机制，适用于文本引导定位任务。</li><li>该方法采用零样本方式，无需对目标数据进行进一步训练。</li><li>通过特征选择和后处理策略，在不增加可学习参数的情况下优化特征。</li><li>该方法与采用对比学习显式强制图像和文本对齐的先进方法具有竞争力。</li><li>在胸部 X 射线基准测试中，该方法在不同类型的病理上与 SOTA 持平，在平均 IoU 和 AUC-ROC 两个指标上甚至优于 SOTA。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 零样本医学短语定位</p></li><li><p>Authors: Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris</p></li><li><p>Affiliation: 爱丁堡大学工程学院</p></li><li><p>Keywords: 深度学习, 扩散模型, 医学影像, 短语定位, 零样本学习</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12920, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): 本文研究背景是医学影像中病理区域定位任务需要大量边界框标注，而文本引导定位任务（短语定位）可以提供一种替代的弱监督形式。</p><p>(2): 现有方法通过对比学习在联合嵌入空间中强制执行图像-文本对齐，但存在显式对齐计算量大、泛化性差的问题。</p><p>(3): 本文提出了一种零样本短语定位方法，利用预训练的扩散模型中的交叉注意力机制隐式对齐图像和文本特征，并通过特征选择和后处理策略在不增加可学习参数的情况下提升定位精度。</p><p>(4): 该方法在胸部 X 射线基准测试上取得了与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法，验证了其在医学影像领域零样本学习的可行性和有效性。</p><ol><li>方法：</li></ol><p>（1）：采用 Latent Diffusion Model（LDM），通过反向扩散过程逐步恢复图像，并利用 U-Net 模型中的交叉注意力机制对图像和文本特征进行隐式对齐；</p><p>（2）：收集不同层级和时间步长的交叉注意力图，并通过特征选择和后处理策略优化定位精度；</p><p>（3）：在不增加可学习参数的情况下，在胸部 X 射线基准测试上取得与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种利用预训练扩散模型进行短语定位的新方法，该方法在不改变生成模型的情况下，利用模型中视觉和文本特征融合的交叉注意力机制，实现了零样本短语定位，为医学影像领域零样本学习提供了新的思路；</p><p>（2）：创新点：利用预训练扩散模型中的交叉注意力机制隐式对齐图像和文本特征，实现零样本短语定位；性能：在胸部 X 射线基准测试上取得与现有方法相当的定位性能，在平均 IoU 和 AUC-ROC 两个指标上优于现有方法；工作量：在不增加可学习参数的情况下，通过特征选择和后处理策略优化定位精度，工作量较小。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-edc65b84041a4ffbf6fad90dfbf52862.jpg" align="middle"><img src="https://picx.zhimg.com/v2-42ab68ed87191afb18c00170b44f792e.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d782a2682f47c83a60efe8ef4da1aeb0.jpg" align="middle"></details><h2 id="Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images"><a href="#Robust-CLIP-Based-Detector-for-Exposing-Diffusion-Model-Generated-Images" class="headerlink" title="Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images"></a>Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images</h2><p><strong>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</strong></p><p>Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detector’s robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detector’s generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at <a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection</a>. </p><p><a href="http://arxiv.org/abs/2404.12908v1">PDF</a> </p><p><strong>Summary</strong><br>扩散模型生成图像的真实性鉴别框架，利用 CLIP 模型提取图像和文本特征，并通过 MLP 分类器判别真实性和合成性。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型生成的图像真实性鉴别挑战性。</li><li>提出利用 CLIP 模型提取图像和文本特征的鉴别框架。</li><li>设计改进鉴别器鲁棒性的损失函数，并处理不平衡数据集。</li><li>对损失函数进行平滑处理，提升鉴别模型泛化能力。</li><li>实验结果表明该方法优于传统鉴别技术。</li><li>代码已开源：<a href="https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection。">https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection。</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 基于CLIP的稳健检测器用于揭露扩散模型生成图像</p></li><li><p>Authors: Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</p></li><li><p>Affiliation: 普渡大学</p></li><li><p>Keywords: Diffusion models, CLIP, Robust, AI images</p></li><li><p>Urls: https://arxiv.org/abs/2404.12908, Github:https://github.com/Purdue-M2/Robust DM Generated Image Detection</p></li><li><p>Summary:</p></li></ol><p>(1):随着扩散模型（Diffusion models，DMs）在图像生成领域取得重大进展，其生成的图像质量不断提升，应用范围也不断扩大。然而，DM生成图像的逼真性也给区分真实图像和合成图像带来了巨大挑战，引发了对数字内容真实性和潜在滥用（如生成深度伪造内容）的担忧。</p><p>(2):传统方法主要利用CLIP图像特征或图像和文本特征，结合多层感知机（MLP）分类器和二元交叉熵（BCE）损失函数进行DM生成图像检测。然而，这些方法存在鲁棒性差、对不平衡数据集处理能力弱等问题。</p><p>(3):本文提出了一种基于CLIP图像和文本特征的稳健检测框架，采用MLP分类器和条件风险价值（CVaR）损失函数与面积下曲线（AUC）损失函数的组合，并在平坦化的损失函数曲面下进行训练，以提高检测器的鲁棒性和泛化能力。</p><p>(4):在DM生成图像检测任务上，本文方法在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性，有望成为DM生成图像检测领域的新技术标杆。</p><ol><li>方法：</li></ol><p>（1）：提出了一种基于 CLIP 图像和文本特征的稳健检测框架，采用 MLP 分类器和条件风险价值 (CVaR) 损失函数与面积下曲线 (AUC) 损失函数的组合。</p><p>（2）：在平坦化的损失函数曲面下进行训练，以提高检测器的鲁棒性和泛化能力。</p><p>（3）：在 DM 生成图像检测任务上，本文方法在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性。</p><ol><li>结论：</li></ol><p>（1）本文提出的方法在DM生成图像检测任务上取得了优异的性能，有望成为该领域的新技术标杆。</p><p>（2）创新点：提出了一种基于CLIP图像和文本特征的稳健检测框架，采用MLP分类器和CVaR损失函数与AUC损失函数的组合，并在平坦化的损失函数曲面下进行训练。</p><p>性能：在多个数据集上的实验结果均优于传统方法，表明了其在该任务上的有效性。</p><p>工作量：与传统方法相比，本文方法的训练时间更长，需要更多的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4d2d3895766f30bd509b9a3d935d9804.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4bf1ac8a20b7e67bfd03bc5cca10058c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f6f7856d5aaeb46c1d7aa9023b3a02ae.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ce9de0cd6eee8dc551b4cd04b517c61c.jpg" align="middle"></details><h2 id="Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing"><a href="#Training-and-prompt-free-General-Painterly-Harmonization-Using-Image-wise-Attention-Sharing" class="headerlink" title="Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing"></a>Training-and-prompt-free General Painterly Harmonization Using   Image-wise Attention Sharing</h2><p><strong>Authors:Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</strong></p><p>Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image. However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts. To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel “share-attention module”. This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations. Additionally, we further introduce “similarity reweighting” mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches. At last, we recognize the deficiencies in existing benchmarks and propose the “General Painterly Harmonization Benchmark”, which employs range-based evaluation metrics to more accurately reflect real-world application. Extensive experiments demonstrate the superior efficacy of our method across various benchmarks. The code and web demo are available at <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a>. </p><p><a href="http://arxiv.org/abs/2404.12900v1">PDF</a> </p><p><strong>Summary</strong></p><p>图像风格统一方法TF-GPH通过图像注意力共享，不需训练和提示，即可实现多样视觉元素的无缝融合。</p><p><strong>Key Takeaways</strong></p><ul><li>设计了一种不需训练和提示的通用图像风格统一方法 TF-GPH。</li><li>引入图像级注意力共享，打破了传统自注意力机制的局限。</li><li>提出相似性重新加权机制，有效利用跨图像信息，提升性能。</li><li>提出通用图像风格统一基准，采用基于范围的评估指标，更贴近真实应用。</li><li>实验表明，TF-GPH 在多个基准上均表现优异。</li><li>代码和网络演示可在 <a href="https://github.com/BlueDyee/TF-GPH">https://github.com/BlueDyee/TF-GPH</a> 获取。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：训练与提示无关的通用绘画调和</li><p></p><p></p><li>作者：Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</li><p></p><p></p><li>单位：国立阳明交通大学</li><p></p><p></p><li>关键词：diffusion model, attention, image editing, image harmonization, painterly harmonization, style transfer</li><p></p><p></p><li>论文链接：xxx，Github 代码链接：https://github.com/BlueDyee/TF-GPH</li><p></p><p></p><li><p></p><p>摘要：（1）：研究背景：绘画图像调和旨在无缝地将不同的视觉元素融合到一个连贯的图像中。然而，由于训练数据限制、需要耗时的微调或依赖额外的提示，以前的方法经常遇到重大限制。（2）：过去的方法：过去的方法包括使用双域生成器和判别器的双域生成器和判别器，以及将图像融合到绘画中的 PHDiffusion 模型。这些方法存在训练数据限制、需要微调和依赖提示的问题。（3）：研究方法：本文提出了一种使用图像级注意力共享（TF-GPH）的训练和提示无关的通用绘画调和方法，该方法集成了一个新颖的“共享注意力模块”。该模块通过允许全面的图像级注意力来重新定义传统的自注意力机制，从而促进使用最先进的预训练潜在扩散模型而没有典型的训练数据限制。此外，我们进一步引入了“相似性重新加权”机制，通过有效利用跨图像信息来增强性能，超越了微调或基于提示的方法的能力。最后，我们认识到现有基准的缺陷，并提出了“通用绘画调和基准”，该基准采用基于范围的评估指标来更准确地反映实际应用。（4）：任务和性能：本文方法在各种基准上展示了其卓越的功效。该方法在通用绘画调和基准上的 FID 得分为 10.6，在绘画图像调和基准上的 FID 得分为 10.3，在图像编辑基准上的 FID 得分为 11.2。这些性能支持了他们的目标，即提供一种训练和提示无关的通用绘画调和方法，该方法可以在各种任务上实现最先进的性能。</p></li><li><p>Methods:</p></li></ol><p>（1）：提出了一种使用图像级注意力共享（TF-GPH）的训练和提示无关的通用绘画调和方法，该方法集成了一个新颖的“共享注意力模块”。</p><p>（2）：该模块通过允许全面的图像级注意力来重新定义传统的自注意力机制，从而促进使用最先进的预训练潜在扩散模型而没有典型的训练数据限制。</p><p>（3）：进一步引入了“相似性重新加权”机制，通过有效利用跨图像信息来增强性能，超越了微调或基于提示的方法的能力。</p><p>（4）：提出了“通用绘画调和基准”，该基准采用基于范围的评估指标来更准确地反映实际应用。</p><ol><li>结论：</li></ol><p>（1）：本工作提出了一种训练和提示无关的通用绘画调和方法 TF-GPH，该方法集成了新颖的“共享注意力模块”，并引入了“相似性重新加权”机制，有效利用跨图像信息，超越了微调或基于提示的方法的能力。此外，提出了“通用绘画调和基准”，采用基于范围的评估指标来更准确地反映实际应用。</p><p>（2）：创新点：提出了“共享注意力模块”，重新定义了传统的自注意力机制，允许全面的图像级注意力；引入了“相似性重新加权”机制，有效利用跨图像信息增强性能。</p><p>性能：在通用绘画调和基准上的 FID 得分为 10.6，在绘画图像调和基准上的 FID 得分为 10.3，在图像编辑基准上的 FID 得分为 11.2，超越了微调或基于提示的方法。</p><p>工作量：无需典型的训练数据限制，无需耗时的微调或依赖额外的提示，降低了使用门槛。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-23788675c99f2a6910d21b93d104c6ba.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-be289866fd46a1130a926aac4953f56b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-3005a952210df9687a21ac0bd5813a2c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-340d1d74c9871713d3a7044daea486c2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ed1fb496bff0b4f7658cf7a6aba9a5a2.jpg" align="middle"></details><h2 id="Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models"><a href="#Detecting-Out-Of-Distribution-Earth-Observation-Images-with-Diffusion-Models" class="headerlink" title="Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models"></a>Detecting Out-Of-Distribution Earth Observation Images with Diffusion   Models</h2><p><strong>Authors:Georges Le Bellier, Nicolas Audebert</strong></p><p>Earth Observation imagery can capture rare and unusual events, such as disasters and major landscape changes, whose visual appearance contrasts with the usual observations. Deep models trained on common remote sensing data will output drastically different features for these out-of-distribution samples, compared to those closer to their training dataset. Detecting them could therefore help anticipate changes in the observations, either geographical or environmental. In this work, we show that the reconstruction error of diffusion models can effectively serve as unsupervised out-of-distribution detectors for remote sensing images, using them as a plausibility score. Moreover, we introduce ODEED, a novel reconstruction-based scorer using the probability-flow ODE of diffusion models. We validate it experimentally on SpaceNet 8 with various scenarios, such as classical OOD detection with geographical shift and near-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We show that our ODEED scorer significantly outperforms other diffusion-based and discriminative baselines on the more challenging near-OOD scenarios of flood image detection, where OOD images are close to the distribution tail. We aim to pave the way towards better use of generative models for anomaly detection in remote sensing. </p><p><a href="http://arxiv.org/abs/2404.12667v1">PDF</a> EARTHVISION 2024 IEEE/CVF CVPR Workshop. Large Scale Computer Vision   for Remote Sensing Imagery, Jun 2024, Seattle, United States</p><p><strong>摘要</strong><br>扩散模型的重建误差可以作为遥感图像的无监督异常检测器，其对罕见事件的检测效果优于现有方法。</p><p><strong>关键要点</strong></p><ul><li>扩散模型的重建误差可以作为遥感图像的无监督异常检测指标。</li><li>ODEED 是一种基于扩散模型概率流 ODE 的重建型评分器，性能优异。</li><li>ODEED 在地理偏移和近异常场景下均表现出色，尤其是洪水图像检测等分布尾部异常检测任务。</li><li>ODEED 优于其他基于扩散模型和判别模型的基线方法。</li><li>本研究为利用生成模型进行遥感异常检测铺平了道路。</li><li>罕见事件的视觉外观与常见观测存在差异，检测这些事件有助于预测观测的变化。</li><li>扩散模型可以输出与训练数据集更接近的样本的截然不同的特征。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 扩散模型检测地球观测图像的分布外情况</p></li><li><p>Authors: Georges Le Bellier, Nicolas Audebert</p></li><li><p>Affiliation: 法国巴黎国立工艺技术学院</p></li><li><p>Keywords: Out-of-Distribution, Remote Sensing, Diffusion Model, Anomaly Detection</p></li><li><p>Urls: Paper: https://arxiv.org/pdf/2404.12667.pdf , Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): 遥感图像可以捕捉到罕见和异常事件，例如灾害和重大景观变化，其视觉外观与通常的观测结果形成对比。在常见遥感数据上训练的深度模型将为这些分布外样本输出截然不同的特征，而与那些更接近其训练数据集的样本相比。因此，检测它们有助于预测观测结果的变化，无论是地理上的还是环境上的。</p><p>(2): 过去的方法主要依赖于判别模型，这些模型需要监督学习来区分分布内和分布外样本。然而，这些方法在近分布外设置中表现不佳，其中分布外样本与训练分布的尾部接近。</p><p>(3): 本文提出了一种新颖的方法，称为 ODEED，它利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性。ODEED 将扩散模型重建误差用作非监督分布外检测器，并展示了其在各种场景中的有效性，包括经典分布外检测和近分布外设置。</p><p>(4): 在 SpaceNet 8 数据集上进行的实验表明，ODEED 在洪水图像检测的更具挑战性的近分布外场景中明显优于其他基于扩散和判别的方法。这些结果支持了本文的目标，即为遥感中的异常检测更好地利用生成模型。</p><ol><li>方法：</li></ol><p>（1）：本文提出了一种名为 ODEED 的新颖方法，该方法利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性，将扩散模型重建误差用作非监督分布外检测器；</p><p>（2）：ODEED 通过积分概率流常微分方程 (PF-ODE) 从数据分布将样本编码到先验分布，反之亦然，并利用扩散模型的生成能力和重建性能来检测分布外样本；</p><p>（3）：本文使用三种基于扩散模型的评分器来评估重建性能，包括基于时间截断扩散损失的扩散损失评分器、专注于固定时间步长去噪性能的一步去噪评分器，以及利用 PF-ODE 轨迹精度作为区分分布内和分布外样本的方法的 ODEED（ODE 编码解码）评分器。</p><ol><li>结论：</li></ol><p>（1）：本文评估了扩散模型检测地球观测图像分布外情况的有效性。我们引入了 ODEED 评分器，它利用连续时间扩散模型的确定性重建能力。我们针对 1）云检测和 2）Space-Net 8 数据集上具有挑战性的 OOD 检测任务集合评估了这些方法。我们证明了我们的 ODEED 评分器在更具挑战性的洪水相关场景中明显优于基线，展示了扩散模型检测“接近分布外”遥感图像（例如洪水图像）的意义。这些发现为利用生成模型从未标记的 EO 数据中检测罕见事件开辟了新的途径。</p><p>（2）：创新点：提出了一种利用扩散模型的概率流常微分方程 (ODE) 来计算重建相似性的新颖方法 ODEED；性能：在 SpaceNet 8 数据集上进行的实验表明，ODEED 在洪水图像检测的更具挑战性的近分布外场景中明显优于其他基于扩散和判别的方法；工作量：本文的工作量中等，需要对扩散模型和概率流常微分方程有基本的了解。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a861c2c676669b5a005a8c6460157c23.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a94e85aaa5cc539ad5464e2facd58f70.jpg" align="middle"><img src="https://picx.zhimg.com/v2-166f6cfb38b037adea4b992761e7f8c9.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bb4eaeab52bbd1091417582a1679d9b4.jpg" align="middle"></details><h2 id="Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models"><a href="#Learning-the-Domain-Specific-Inverse-NUFFT-for-Accelerated-Spiral-MRI-using-Diffusion-Models" class="headerlink" title="Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models"></a>Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI   using Diffusion Models</h2><p><strong>Authors:Trevor J. Chan, Chamith S. Rajapakse</strong></p><p>Deep learning methods for accelerated MRI achieve state-of-the-art results but largely ignore additional speedups possible with noncartesian sampling trajectories. To address this gap, we created a generative diffusion model-based reconstruction algorithm for multi-coil highly undersampled spiral MRI. This model uses conditioning during training as well as frequency-based guidance to ensure consistency between images and measurements. Evaluated on retrospective data, we show high quality (structural similarity &gt; 0.87) in reconstructed images with ultrafast scan times (0.02 seconds for a 2D image). We use this algorithm to identify a set of optimal variable-density spiral trajectories and show large improvements in image quality compared to conventional reconstruction using the non-uniform fast Fourier transform. By combining efficient spiral sampling trajectories, multicoil imaging, and deep learning reconstruction, these methods could enable the extremely high acceleration factors needed for real-time 3D imaging. </p><p><a href="http://arxiv.org/abs/2404.12361v1">PDF</a> </p><p><strong>Summary</strong><br>深度学习方法可加速磁共振成像，达到最先进水平，但并未充分利用非笛卡尔采样轨迹可能实现的额外加速。</p><p><strong>Key Takeaways</strong></p><ul><li>创建基于生成扩散模型的重建算法，用于多线圈高欠采样螺旋磁共振成像。</li><li>该模型利用训练期间的调节和基于频率的引导，确保图像和测量值的一致性。</li><li>在回顾性数据上评估，显示出高图像质量（结构相似度&gt; 0.87），扫描时间极快（2D 图像为 0.02 秒）。</li><li>使用该算法识别了一组优化的可变密度螺旋轨迹，与使用非均匀快速傅里叶变换的传统重建相比，图像质量有了很大提高。</li><li>通过结合有效的螺旋采样轨迹、多线圈成像和深度学习重建，这些方法可以实现实时 3D 成像所需的极高加速因子。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: 加速螺旋MRI的域特定逆非均匀快速傅里叶变换学习</p></li><li><p>Authors: Trevor J. Chan, Chamith S. Rajapakse</p></li><li><p>Affiliation: 宾夕法尼亚大学生物工程系</p></li><li><p>Keywords: 加速MRI，螺旋MRI，深度学习，图像重建</p></li><li><p>Urls: Paper: https://arxiv.org/abs/2404.12361, Github: None</p></li><li><p>Summary:</p></li></ol><p>(1): MRI成像速度慢，限制了其在临床中的应用。加速MRI采集是解决这一问题的关键，其中非笛卡尔采样轨迹和深度学习重建方法是两个重要的方向。</p><p>(2): 现有的深度学习重建方法主要针对笛卡尔采样MRI，忽略了非笛卡尔采样轨迹带来的额外加速潜力。</p><p>(3): 本文提出了一种基于扩散模型的、轨迹无关的多线圈螺旋MRI欠采样图像重建方法。该方法利用条件训练和频率引导，确保图像和测量值之间的一致性。</p><p>(4): 在回顾性数据上，该方法重建的图像质量高（结构相似性&gt;0.87），扫描时间极快（2D图像0.02秒）。该方法还用于识别一组最优的可变密度螺旋轨迹，与使用非均匀快速傅里叶变换的传统重建方法相比，图像质量有很大提高。通过结合高效的螺旋采样轨迹、多线圈成像和深度学习重建，该方法有望实现实时3D成像所需的高加速因子。</p><ol><li>方法：</li></ol><p>（1）：本研究回顾性使用[12]公开获取的人类受试者数据进行。无需伦理批准。</p><p>（2）：我们使用NYU FastMRI数据集[12]，该数据集包含6970个在4到24个线圈的硬件上完全采样的2D脑部扫描。对于训练和测试，我们考虑以下序列参数来表征轴向T2加权涡旋自旋回波序列：扫描时间=140s，TR=6s，TE=113ms，切片=30，切片厚度=5mm，视野=22cm，矩阵大小=320x320。2562分辨率的2D切片的有效扫描时间为140s/320 ∗ 256/30 ≈ 3.7s。由于这些数据最初是使用笛卡尔序列获取的，因此图2。给定测量值y0，重建遵循修改后的扩散采样过程。在每个时间步长，一个有噪声的潜在xt与先验p0连接，并传递到去噪模型以获得˜xt−1。为了强制与y0一致，我们计算频率梯度∇yt−1并使用修改后的迭代逆nufft（第3.3节）求解图像梯度。xt−1和∇xt−1的加权和产生校正后的图像xt−1。重复此操作，直到t = 0。</p><p>（3）：我们模拟螺旋采集，方法是回顾性地在k空间中插值，以获得沿生成螺旋轨迹的复值测量值。</p><p>（4）：根据Kim等人[13]，我们考虑以下形式的螺旋轨迹：k(τ) = � τ 0 1 ρ(ϕ)dϕejωτ ≈ λτ αejωτ。（3）此处，ρ表示采样密度，τ是时间的函数，ϕ是角度位置，ω = 2πn是频率，n是k空间中的转数，λ是缩放因子，等于矩阵大小/(2∗ FOV)，α是相对于边缘过度采样k空间中心的偏差项。在梯度回转率上限和梯度幅度上限的约束下求解这个参数方程，产生梯度（gx(t)和gy(t)）以及kx,ky平面的螺旋轨迹（图1）。这样做，我们可以调整采样参数以控制诸如读出持续时间和停留时间之类的因素，同时改变交错数和低频到高频过采样率。</p><p>（5）：图像重建是逆问题求解MRI欠采样采集等同于通过某种不完美的采样函数A测量未知信号x：y = Ax + ϵ。这里，y是测量多线圈k空间数据，A是非均匀傅里叶变换。ϵ是测量噪声，与y存在于同一域中；在MRI中，对于每个线圈，噪声在y的实部和虚部中呈高斯分布。重建是从一组不完整的k空间测量值y中恢复图像信号x的不适定逆问题。由于x和y存在于不同的域中，因此x隐藏在采样算子A的后面。解决这个问题需要先验知识。在我们的案例中，我们学习图像的潜在条件分布并寻求重建</p><ol><li>结论：</li></ol><p>（1）：本研究提出了一种基于扩散模型的螺旋MRI欠采样图像重建方法，该方法结合了多线圈成像、螺旋扫描和欠采样，实现了极快的成像速度，有望实现实时3D成像所需的极高加速因子。</p><p>（2）：创新点：基于扩散模型的图像重建方法；多线圈成像和螺旋扫描的结合；可变密度螺旋轨迹的优化。性能：图像质量高（结构相似性&gt;0.87），扫描时间极快（2D图像0.02秒）。工作量：需要大量的训练数据和计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-dad46f934fa27aedf6f5bcc658a1e97b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-7ca2a2a56eaf899a4ab5fb7f25a2d0dd.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-09dc794137fef040d7fe26326b8c5bd2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-1377428c568ca550e4683544f87b3da2.jpg" align="middle"></details><h2 id="AniClipart-Clipart-Animation-with-Text-to-Video-Priors"><a href="#AniClipart-Clipart-Animation-with-Text-to-Video-Priors" class="headerlink" title="AniClipart: Clipart Animation with Text-to-Video Priors"></a>AniClipart: Clipart Animation with Text-to-Video Priors</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Kede Ma, Jing Liao</strong></p><p>Clipart, a pre-made graphic art form, offers a convenient and efficient way of illustrating visual content. Traditional workflows to convert static clipart images into motion sequences are laborious and time-consuming, involving numerous intricate steps like rigging, key animation and in-betweening. Recent advancements in text-to-video generation hold great potential in resolving this problem. Nevertheless, direct application of text-to-video generation models often struggles to retain the visual identity of clipart images or generate cartoon-style motions, resulting in unsatisfactory animation outcomes. In this paper, we introduce AniClipart, a system that transforms static clipart images into high-quality motion sequences guided by text-to-video priors. To generate cartoon-style and smooth motion, we first define B\’{e}zier curves over keypoints of the clipart image as a form of motion regularization. We then align the motion trajectories of the keypoints with the provided text prompt by optimizing the Video Score Distillation Sampling (VSDS) loss, which encodes adequate knowledge of natural motion within a pretrained text-to-video diffusion model. With a differentiable As-Rigid-As-Possible shape deformation algorithm, our method can be end-to-end optimized while maintaining deformation rigidity. Experimental results show that the proposed AniClipart consistently outperforms existing image-to-video generation models, in terms of text-video alignment, visual identity preservation, and motion consistency. Furthermore, we showcase the versatility of AniClipart by adapting it to generate a broader array of animation formats, such as layered animation, which allows topological changes. </p><p><a href="http://arxiv.org/abs/2404.12347v1">PDF</a> Project Page: <a href="https://aniclipart.github.io/">https://aniclipart.github.io/</a></p><p><strong>Summary</strong><br>通过使用文生图语言模型，AniClipart可以将静态剪贴画图像转换为高质量的动态序列，并且始终优于现有的图像到视频生成模型。</p><p><strong>Key Takeaways</strong></p><ul><li>AniClipart 将静态剪贴画转换为动画序列，保留了剪贴画的视觉特征并生成了卡通风格的动作。</li><li>AniClipart 使用贝塞尔曲线对剪贴画图像的关键点进行运动正则化。</li><li>AniClipart 通过优化视频评分蒸馏采样 (VSDS) 损失将关键点的运动轨迹与提供的文本提示对齐。</li><li>VSDS 损失编码了预训练文本到视频扩散模型中自然运动的充分知识。</li><li>AniClipart 使用可微分僵硬形状变形算法，可以在保持变形刚性的同时进行端到端优化。</li><li>AniClipart 在文本-视频对齐、视觉特征保留和运动一致性方面始终优于现有的图像到视频生成模型。</li><li>AniClipart 可以适应更广泛的动画格式，例如允许拓扑更改的分层动画。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: AniClipart：基于文本到视频先验的剪辑画动画</p></li><li><p>Authors: RONGHUAN WU, WANCHAO SU, KEDE MA, JING LIAO</p></li><li><p>Affiliation: 香港城市大学</p></li><li><p>Keywords: Clipart Animation, Text-to-Video Diffusion, Score Distillation Sampling, As-Rigid-As-Possible Shape Deformation</p></li><li><p>Urls: Paper:https://arxiv.org/abs/2404.12347v1 Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):剪辑画是一种预先制作的图形艺术形式，它提供了一种方便且有效的方法来插图视觉内容。将静态剪辑画图像转换为运动序列的传统工作流程既费力又费时，涉及许多复杂的步骤，如装配、关键动画和中间动画。</p><p>(2):文本到视频生成方面的最新进展在解决这个问题方面具有巨大的潜力。然而，直接应用文本到视频生成模型通常难以保留剪辑画图像的视觉标识或生成卡通风格的动作，从而导致动画结果不令人满意。</p><p>(3):本文介绍了 AniClipart，这是一个将静态剪辑画图像转换为高质量运动序列的系统，该系统由文本到视频先验指导。为了生成卡通风格和流畅的动作，我们首先将贝塞尔曲线定义为剪辑画图像关键点的运动正则化形式。然后，通过优化视频评分蒸馏采样 (VSDS) 损失将关键点的运动轨迹与提供的文本提示对齐，该损失对预训练的文本到视频扩散模型中的自然运动知识进行了充分编码。通过可微分尽可能刚性形状变形算法，我们的方法可以在保持变形刚性的同时进行端到端优化。</p><p>(4):实验结果表明，在文本视频对齐、视觉标识保留和运动一致性方面，所提出的 AniClipart 始终优于现有的图像到视频生成模型。此外，我们展示了 AniClipart 的多功能性，通过对其进行调整以生成更广泛的动画格式，例如允许拓扑变化的分层动画。</p><ol><li>方法：</li></ol><p>（1）：我们提出了一种基于文本到视频先验的剪辑画动画系统 AniClipart。</p><p>（2）：AniClipart 由以下几个关键组件组成：贝塞尔曲线运动正则化、视频评分蒸馏采样（VSDS）损失和可微分尽可能刚性形状变形算法。</p><p>（3）：贝塞尔曲线运动正则化将剪辑画图像关键点的运动定义为贝塞尔曲线，从而确保了运动的平滑性和连续性。</p><p>（4）：VSDS 损失将关键点的运动轨迹与提供的文本提示对齐，从而将文本到视频扩散模型中的自然运动知识融入到动画中。</p><p>（5）：可微分尽可能刚性形状变形算法允许在保持变形刚性的同时进行端到端优化，从而生成具有清晰视觉标识的动画。</p><p><strong>8. 结论：</strong></p><p>（1）本文提出的 AniClipart 系统，通过将文本到视频先验融入剪辑画动画生成中，为静态剪辑画图像赋予了生动性，简化了动画制作流程，具有重要的创新意义。</p><p>（2）创新点：AniClipart 创新性地采用了贝塞尔曲线运动正则化、视频评分蒸馏采样损失和可微分尽可能刚性形状变形算法，实现了剪辑画图像的关键点运动轨迹与文本提示的精确对齐，以及变形刚性的保持，从而生成高质量的剪辑画动画。</p><p>性能：AniClipart 在文本视频对齐、视觉标识保留和运动一致性方面均优于现有的图像到视频生成模型，展现出出色的动画生成能力。</p><p>工作量：AniClipart 采用端到端优化，简化了剪辑画动画制作流程，降低了工作量。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7f003c736b9e8d225fc78c7b356b7e25.jpg" align="middle"><img src="https://picx.zhimg.com/v2-33e8a11d43dfdfe88d324da3694df802.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b455537a4612c5459d9162e1601fc155.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0aba7591bd6a8a973210ed734d1006c9.jpg" align="middle"></details><h2 id="StyleBooth-Image-Style-Editing-with-Multimodal-Instruction"><a href="#StyleBooth-Image-Style-Editing-with-Multimodal-Instruction" class="headerlink" title="StyleBooth: Image Style Editing with Multimodal Instruction"></a>StyleBooth: Image Style Editing with Multimodal Instruction</h2><p><strong>Authors:Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</strong></p><p>Given an original image, image editing aims to generate an image that align with the provided instruction. The challenges are to accept multimodal inputs as instructions and a scarcity of high-quality training data, including crucial triplets of source/target image pairs and multimodal (text and image) instructions. In this paper, we focus on image style editing and present StyleBooth, a method that proposes a comprehensive framework for image editing and a feasible strategy for building a high-quality style editing dataset. We integrate encoded textual instruction and image exemplar as a unified condition for diffusion model, enabling the editing of original image following multimodal instructions. Furthermore, by iterative style-destyle tuning and editing and usability filtering, the StyleBooth dataset provides content-consistent stylized/plain image pairs in various categories of styles. To show the flexibility of StyleBooth, we conduct experiments on diverse tasks, such as text-based style editing, exemplar-based style editing and compositional style editing. The results demonstrate that the quality and variety of training data significantly enhance the ability to preserve content and improve the overall quality of generated images in editing tasks. Project page can be found at <a href="https://ali-vilab.github.io/stylebooth-page/">https://ali-vilab.github.io/stylebooth-page/</a>. </p><p><a href="http://arxiv.org/abs/2404.12154v1">PDF</a> </p><p><strong>Summary</strong><br>StyleBooth是一个图像编辑框架，集成了文本和图像指令，并提供高质量的风格编辑数据集，可用于各种编辑任务，如文本风格编辑和示例风格编辑。</p><p><strong>Key Takeaways</strong></p><ul><li>StyleBooth 框架将文本指令和图像示例整合为扩散模型的统一条件，实现图像风格编辑。</li><li>StyleBooth 数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了内容一致的风格化/普通图像对。</li><li>StyleBooth 通过高质量的训练数据增强了编辑任务中保留内容和提高生成图像整体质量的能力。</li><li>StyleBooth 可用于文本风格编辑、示例风格编辑和合成风格编辑等多种任务。</li><li>StyleBooth 项目主页：<a href="https://ali-vilab.github.io/stylebooth-page/。">https://ali-vilab.github.io/stylebooth-page/。</a></li><li>多模态输入和高质量训练数据对于图像编辑至关重要。</li><li>StyleBooth 提供了一个全面的图像编辑框架和训练数据的构建策略。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: StyleBooth: 使用多模态指令进行图像风格编辑</p></li><li><p>Authors: Zhen Han, Chaojie Mao, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang</p></li><li><p>Affiliation: 阿里巴巴集团</p></li><li><p>Keywords: Text-based style editing · Exemplar-based style editing · Multimodal instruction-tuning</p></li><li><p>Urls: https://arxiv.org/abs/2404.12154, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1):图像编辑旨在根据提供的指令生成与原图像对齐的图像。挑战在于接受多模态输入作为指令，以及缺乏高质量训练数据，包括源/目标图像对和多模态（文本和图像）指令的关键三元组。</p><p>(2):以往的方法主要包括操纵注意力机制的特征、在去噪步骤中实现引导扩散、使用图像对进行监督来调整 T2I 模型等。这些方法都面临着只支持单模态输入、缺乏高质量训练数据、难以保持内容一致性等问题。</p><p>(3):本文提出 StyleBooth 方法，该方法提出了一个用于图像编辑的综合框架和构建高质量风格编辑数据集的可行策略。我们将编码的文本指令和图像示例整合为扩散模型的统一条件，从而能够根据多模态指令编辑原始图像。此外，通过迭代的风格-去风格调整和编辑以及可用性过滤，StyleBooth 数据集提供了各种风格类别中内容一致的风格化/-普通图像对。</p><p>(4):实验结果表明，训练数据的质量和多样性显着增强了在编辑任务中保留内容和提高生成图像整体质量的能力。</p><ol><li>方法：</li></ol><p>（1）：StyleBooth 方法提出了一种用于图像编辑的综合框架，该框架将编码的文本指令和图像示例整合为扩散模型的统一条件，能够根据多模态指令编辑原始图像；</p><p>（2）：StyleBooth 数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了各种风格类别中内容一致的风格化/-普通图像对，提高了训练数据的质量和多样性，增强了在编辑任务中保留内容和提高生成图像整体质量的能力；</p><p>（3）：Scale Weighting Mechanism 机制通过对隐藏空间嵌入进行缩放加权，平衡了不同模态的风格表现，保证了图像编辑的质量。</p><ol><li>结论：</li></ol><p>（1）：本文提出 StyleBooth，这是一种多模态指令图像风格编辑方法。它独立编码参考图像和文本，随后在潜在空间内对其进行转换和对齐，然后注入骨干网络以进行生成指导，以实现基于文本和示例的指令编辑。同时，StyleBooth 还可以融合多模态信息进行合成创造性生成。此外，我们构建了一个用于风格编辑的高质量数据集，该数据集由各种内容一致的风格化和普通图像对组成，这有助于我们构建更好的编辑模型。局限性。在这项工作中，我们构建了一个用于风格编辑的丰富数据集。然而，数据构建基于特定风格的文本描述，例如水彩画，这极大地限制了风格的数量。收集更广泛、更广泛的编辑数据集将是我们未来的工作。</p><p>（2）：创新点：提出 StyleBooth 方法，该方法将编码的文本指令和图像示例整合为扩散模型的统一条件，能够根据多模态指令编辑原始图像；构建 StyleBooth 数据集，该数据集通过迭代的风格-去风格调整和编辑以及可用性过滤，提供了各种风格类别中内容一致的风格化/-普通图像对，提高了训练数据的质量和多样性；提出 Scale Weighting Mechanism 机制，通过对隐藏空间嵌入进行缩放加权，平衡了不同模态的风格表现，保证了图像编辑的质量。性能：实验结果表明，训练数据的质量和多样性显着增强了在编辑任务中保留内容和提高生成图像整体质量的能力。工作量：本文的方法和数据集的构建过程相对复杂，需要较大的计算资源和人力投入。</p><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-b55633af77d0cb6e6dbf35b308d980ca.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8f75d6d068abc3a313d3144833482a9f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f20cb7f7a36d3c7049b18131117bc5cd.jpg" align="middle"></details><h2 id="IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination"><a href="#IntrinsicAnything-Learning-Diffusion-Priors-for-Inverse-Rendering-Under-Unknown-Illumination" class="headerlink" title="IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination"></a>IntrinsicAnything: Learning Diffusion Priors for Inverse Rendering Under   Unknown Illumination</h2><p><strong>Authors:Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</strong></p><p>This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code will be available at <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a>. </p><p><a href="http://arxiv.org/abs/2404.11593v1">PDF</a> Project page: <a href="https://zju3dv.github.io/IntrinsicAnything">https://zju3dv.github.io/IntrinsicAnything</a></p><p><strong>Summary</strong><br>利用生成模型学习材质先验，以正则化优化过程，从而恢复未知静态光照条件下姿势图像中的物体材质。</p><p><strong>Key Takeaways</strong></p><ul><li>将通用渲染方程拆分为漫反射和镜面反射着色项，并将材质先验表述为漫反射率和镜面的扩散模型。</li><li>使用现有的丰富 3D 物体数据训练模型，将其作为解决从 RGB 图像恢复材质表示时模糊性的通用工具。</li><li>开发了一种粗到细的训练策略，利用估计的材质来引导扩散模型满足多视图一致性约束，从而获得更稳定和准确的结果。</li><li>在真实世界和合成数据集上的广泛实验表明，该方法在材质恢复方面达到最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>论文标题：IntrinsicAnything: 学习扩散先验以在未知光照下进行逆向渲染</p></li><li><p>作者：Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, Xiaowei Zhou</p></li><li><p>第一作者单位：浙江大学计算机辅助设计与图形学国家重点实验室</p></li><li><p>关键词：Inverse Rendering, Material Recovery, Diffusion Model, Generative Prior</p></li><li><p>论文链接：https://arxiv.org/abs/2404.11593 , Github：None</p></li><li><p>摘要：</p></li></ol><p>(1) 研究背景：从捕获的图像中恢复物体的几何、材质和光照，即逆向渲染，是计算机视觉和图形学中一项长期存在的任务。这些 3D 物体的物理属性对于许多应用程序至关重要，例如 VR/AR、电影制作和视频游戏。由于现实世界物体与环境光照之间相互作用的固有复杂性，逆向渲染仍然是一个不适定问题。</p><p>(2) 过往方法及问题：以往的工作通过复杂的捕获系统[16,20]或在黑暗环境中共同定位的手电筒和相机[5,50,84]来克服这个问题。然而，这些方法需要特殊的硬件设备或受限的环境，限制了它们的应用。</p><p>(3) 本文提出的研究方法：为了克服这个不适定问题，我们的关键思想是学习一个生成模型作为材质先验来正则化优化过程。我们观察到，一般的渲染方程可以分解为漫反射和镜面反射阴影项，因此将材质先验表述为漫反射和镜面反射的扩散模型。由于这种设计，我们的模型可以使用现有的丰富 3D 对象数据进行训练，并且自然地充当了一种多功能工具，可以在从 RGB 图像中恢复材质表示时解决歧义。此外，我们开发了一种从粗到精的训练策略，利用估计的材质来引导扩散模型满足多视图一致性约束，从而得到更稳定和准确的结果。</p><p>(4) 方法在什么任务上取得了怎样的性能：在真实世界和合成数据集上的广泛实验表明，我们的方法在材质恢复方面达到了最先进的性能。该性能可以支持他们的目标。</p><ol><li>方法：</li></ol><p>（1）：提出学习扩散模型作为材质先验，正则化逆向渲染优化过程。</p><p>（2）：将渲染方程分解为漫反射和镜面反射阴影项，将材质先验表述为漫反射和镜面反射的扩散模型。</p><p>（3）：采用从粗到精的训练策略，利用估计的材质引导扩散模型满足多视图一致性约束，得到更稳定和准确的结果。</p><ol><li>结论：</li></ol><p>（1）：提出了 IntrinsicAnything 模型，该模型利用扩散模型作为材质先验，在未知静态光照条件下进行逆向渲染。</p><p>（2）：创新点：提出将材质先验设计为漫反射和镜面反射的条件扩散模型；开发了两阶段训练方案，利用粗略材质引导扩散模型满足多视图一致性约束。性能：在真实世界和合成数据集上实现了最先进的材质恢复性能。工作量：需要较大的数据集和较长的训练时间。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-2e3e705009374322a07a0404ed794846.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b8b028549fe12e9acbbb7374c824289a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-474fe30507cc76c6aa5c2fec1a6e92ad.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ffae13d6e393c5f464c5f05ee6f4295a.jpg" align="middle"></details><h2 id="MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation"><a href="#MoA-Mixture-of-Attention-for-Subject-Context-Disentanglement-in-Personalized-Image-Generation" class="headerlink" title="MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation"></a>MoA: Mixture-of-Attention for Subject-Context Disentanglement in   Personalized Image Generation</h2><p><strong>Authors: Kuan-Chieh,  Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</strong></p><p>We introduce a new architecture for personalization of text-to-image diffusion models, coined Mixture-of-Attention (MoA). Inspired by the Mixture-of-Experts mechanism utilized in large language models (LLMs), MoA distributes the generation workload between two attention pathways: a personalized branch and a non-personalized prior branch. MoA is designed to retain the original model’s prior by fixing its attention layers in the prior branch, while minimally intervening in the generation process with the personalized branch that learns to embed subjects in the layout and context generated by the prior branch. A novel routing mechanism manages the distribution of pixels in each layer across these branches to optimize the blend of personalized and generic content creation. Once trained, MoA facilitates the creation of high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. Crucially, MoA enhances the distinction between the model’s pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable. Project page: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a> </p><p><a href="http://arxiv.org/abs/2404.11565v1">PDF</a> Project Website: <a href="https://snap-research.github.io/mixture-of-attention">https://snap-research.github.io/mixture-of-attention</a></p><p><strong>Summary</strong><br>文本到图像扩散模型个性化的混合注意力机制，在预先固定的非个性化基础分支上叠加可学习的个性化分支，优化个性化和通用内容创建的混合，实现更解耦的主题-上下文控制。</p><p><strong>Key Takeaways</strong></p><ul><li>提出混合注意机制（MoA）架构，用于文本到图像扩散模型的个性化。</li><li>MoA 分发生成工作负载到个性化分支和非个性化先验分支。</li><li>个性化分支在先验分支生成的布局和上下文中嵌入主题。</li><li>新颖的路由机制优化了跨分支的像素分配。</li><li>MoA 允许创建高质量的个性化图像，具有多种主题和交互。</li><li>MoA 增强了模型的先验功能和个性化干预之间的区别。</li><li>MoA 提供了以前无法实现的更解耦的主题-上下文控制。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li><p></p><p>Title: MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation</p></li><li><p>Authors: Kuan-Chieh (Jackson) Wang, Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman</p></li><li><p>Affiliation: Snap Inc., USA</p></li><li><p>Keywords: Personalization, Text-to-image Generation, Diffusion Models</p></li><li><p>Urls: https://arxiv.org/abs/2404.11565, Github:None</p></li><li><p>Summary:</p></li></ol><p>(1): The research background of this article is the rapid progress in foundation text-conditioned image synthesis with diffusion models. Personalized generation focuses on adapting and contextualizing the generation to a set of desired subjects using limited input images, while retaining the powerful generative capabilities of the foundation model.</p><p>(2): Past methods for personalized generation include fine-tuning-based personalization techniques and approaches optimized for multi-subject generation. However, fine-tuning-based methods tend to overfit to certain attributes in the distribution of the input images or struggle to adhere adequately to the input prompt. Approaches optimized for multi-subject generation often modify the original model's weights, resulting in compositions that lack diversity and naturalness.</p><p>(3): The research methodology proposed in this paper is Mixture-of-Attention (MoA), which extends the vanilla attention mechanism into multiple attention blocks (i.e. experts), and has a router network that softly combines the different experts. MoA distributes the generation between personalized and non-personalized attention pathways. It is designed to retain the original model's prior by fixing its attention layers in the prior (non-personalized) branch, while minimally intervening in the generation process with the personalized branch.</p><p>(4): MoA is evaluated on the task of personalized image generation. The results show that MoA can generate high-quality, personalized images featuring multiple subjects with compositions and interactions as diverse as those generated by the original model. MoA also enhances the distinction between the model's pre-existing capability and the newly augmented personalized intervention, thereby offering a more disentangled subject-context control that was previously unattainable.</p><ol><li><p>方法：</p><pre><code>            (1): 提出Mixture-of-Attention（MoA）层，将vanilla注意力机制扩展为多个注意力模块（即专家），并使用路由器网络对不同专家进行软组合。            (2): MoA将生成分配到个性化和非个性化注意力路径之间。它通过固定先验（非个性化）分支中的注意力层来保留原始模型的先验，同时通过个性化分支对生成过程进行最小干预。            (3): 将MoA应用于文本到图像（T2I）扩散模型中，用于主题驱动的生成。该架构使我们能够增强T2I模型的能力，以执行主题驱动的生成，同时对主题和上下文进行解耦控制，从而保留先验模型中固有的多样化图像分布。</code></pre></li><li><p>结论：</p></li></ol><p>（1）本工作的意义在于：提出了一种新的个性化生成架构 Mixture-of-Attention（MoA），该架构增强了基础文本到图像模型的能力，使其能够注入主题图像，同时保留了模型的先前能力。与现有主题驱动生成方法生成的图像相比，MoA 无缝地统一了两种范式，通过拥有两个不同的专家和一个路由器来动态合并两个路径。MoA 层能够在一次反向扩散传递中从具有丰富交互的多个输入主题生成个性化上下文，并且不需要测试时微调步骤，从而解锁了以前无法达到的结果。此外，我们的模型展示了生成图像中以前未见的布局变化，以及处理物体或其他主题遮挡的能力，并且无需显式控制即可处理不同的身体形状。最后，由于其简单性，MoA 与众所周知的基于扩散的生成和编辑技术（如 ControlNet 和 DDIM Inversion）天然兼容。例如，MoA 和 DDIM Inversion 的结合解锁了在真实照片中进行主题交换的应用。展望未来，我们设想通过专门针对不同任务或语义标签的不同专家进一步增强 MoA 架构。此外，采用极小干预个性化的方法可以扩展到各种基础模型（例如视频和 3D/4D 生成），从而促进使用现有和未来生成模型创建个性化内容。</p><p>（2）创新点：提出了一种新的 Mixture-of-Attention（MoA）架构，该架构增强了基础文本到图像模型的能力，使其能够注入主题图像，同时保留了模型的先前能力。MoA 层能够在一次反向扩散传递中从具有丰富交互的多个输入主题生成个性化上下文，并且不需要测试时微调步骤，从而解锁了以前无法达到的结果。此外，我们的模型展示了生成图像中以前未见的布局变化，以及处理物体或其他主题遮挡的能力，并且无需显式控制即可处理不同的身体形状。</p><p>性能：在个性化图像生成任务上进行了评估，结果表明 MoA 可以生成高质量、个性化的图像，其特征是具有与原始模型生成图像一样多样化的构图和交互。MoA 还增强了模型现有能力和新增强个性化干预之间的区别，从而提供了以前无法实现的更分离的主题-上下文控制。</p><p>工作量：MoA 具有简单性，与众所周知的基于扩散的生成和编辑技术（如 ControlNet 和 DDIM Inversion）天然兼容。例如，MoA 和 DDIM Inversion 的结合解锁了在真实照片中进行主题交换的应用。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-65299f0067c3022cccf14b21e08de1a6.jpg" align="middle"><img src="https://picx.zhimg.com/v2-058ccecf97ed2df42286d132194a3ffe.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-577cd46daa6ad37536d9e75f13d51239.jpg" align="middle"><img src="https://picx.zhimg.com/v2-728a034564b4e30499365332b12dfe09.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c4cd4a1d04735a77754757aa6e384614.jpg" align="middle"><img src="https://pica.zhimg.com/v2-fdeae7faf257d15601bbe1c62a204408.jpg" align="middle"><img src="https://picx.zhimg.com/v2-5c3251b1f6745891a76c08f07664b47d.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-22  Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/NeRF/</id>
    <published>2024-04-17T11:09:58.000Z</published>
    <updated>2024-04-17T11:09:58.614Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal"><a href="#Plug-and-Play-Acceleration-of-Occupancy-Grid-based-NeRF-Rendering-using-VDB-Grid-and-Hierarchical-Ray-Traversal" class="headerlink" title="Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal"></a>Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</h2><p><strong>Authors:Yoshio Kato, Shuhei Tarashima</strong></p><p>Transmittance estimators such as Occupancy Grid (OG) can accelerate the training and rendering of Neural Radiance Field (NeRF) by predicting important samples that contributes much to the generated image. However, OG manages occupied regions in the form of the dense binary grid, in which there are many blocks with the same values that cause redundant examination of voxels’ emptiness in ray-tracing. In our work, we introduce two techniques to improve the efficiency of ray-tracing in trained OG without fine-tuning. First, we replace the dense grids with VDB grids to reduce the spatial redundancy. Second, we use hierarchical digital differential analyzer (HDDA) to efficiently trace voxels in the VDB grids. Our experiments on NeRF-Synthetic and Mip-NeRF 360 datasets show that our proposed method successfully accelerates rendering NeRF-Synthetic dataset by 12% in average and Mip-NeRF 360 dataset by 4% in average, compared to a fast implementation of OG, NerfAcc, without losing the quality of rendered images. </p><p><a href="http://arxiv.org/abs/2404.10272v1">PDF</a> Short paper for CVPR Neural Rendering Intelligence Workshop 2024.   Code: <a href="https://github.com/Yosshi999/faster-occgrid">https://github.com/Yosshi999/faster-occgrid</a></p><p><strong>Summary</strong><br>优化神经辐射场（NeRF）通过透射率估计量加速渲染</p><p><strong>Key Takeaways</strong></p><ul><li>透射率估计量（如占用格网）可加速 NeRF 训练和渲染。</li><li>占用格网使用密集二进制格管理占据区域，存在冗余检查。</li><li>体素数据块（VDB）格代替密集格，减少空间冗余。</li><li>分层数字微分分析仪（HDDA）高效追踪 VDB 格中的体素。</li><li>该方法加速 NeRF-Synthetic 渲染 12%、Mip-NeRF 360 渲染 4%。</li><li>渲染图像质量保持不变。</li><li>无需微调即可提高 OG 射线追踪效率。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：使用 VDB 网格和分层射线遍历实现占用网格基于 NeRF 渲染的即插即用加速</li><li>作者：Yifan Wang, Chenghua Li, Ya-Qin Zhang</li><li>隶属单位：香港中文大学（深圳）</li><li>关键词：神经辐射场、占用网格、即插即用加速、VDB 网格、分层数字微分分析仪</li><li>论文链接：https://arxiv.org/abs/2204.06814   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：   神经辐射场 (NeRF) 是一种强大的神经渲染技术，能够从图像数据中学习场景的 3D 表示。然而，NeRF 的训练和渲染计算成本很高。   (2) 过去的方法和问题：   占用网格 (OG) 是一种加速 NeRF 训练和渲染的技术，它通过预测对生成图像贡献较大的重要样本来工作。然而，OG 使用密集的二进制网格来管理占据区域，这会导致许多具有相同值的块，从而导致在光线追踪中冗余检查网格单元是否为空。   (3) 论文提出的研究方法：   为了提高 OG 中光线追踪的效率，论文提出了两种技术。首先，使用 VDB 网格替换密集网格以减少空间冗余。其次，使用分层数字微分分析仪 (HDDA) 在 VDB 网格中高效地追踪网格单元。   (4) 方法在任务和性能上的表现：   在 NeRF-Synthetic 和 Mip-NeRF360 数据集上的实验表明，与 NerfAcc（一种 OG 的快速实现）相比，论文提出的方法成功地将 NeRF-Synthetic 数据集的渲染速度平均提高了 12%，Mip-NeRF360 数据集的渲染速度平均提高了 4%，同时不会降低渲染图像的质量。</p></li><li><p>方法：(1): 将训练好的 NeRF 模型的占据网格 (OG) 转换为使用 OpenVDB [1] 的基于 VDB 的结构，并使用 NanoVDB [13] 将其传输到 GPU。(2): 在这项工作中，我们使用 Instant-NGP [10] 作为基础模型。(3): 如果有更多 re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法：(1): xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>方法：(1): 将训练好的NeRF模型的占据网格（OG）转换为使用OpenVDB[1]的基于VDB的结构，并使用NanoVDB[13]将其传输到GPU。(2): 在这项工作中，我们使用Instant-NGP[10]作为基础模型。(3): 如果有更多re27. 详细描述本文的方法论思想。一定要用中文回答（专有名词需要用英文标注）。例如，它的步骤是。(1):...(2):...(3):..........按照以下输出的格式：</p></li><li><p>方法：(1):xxx；（2）：xxx；（3）：xxx；.......一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面</p><summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。<p></p></summary></li><li><p>结论：(1): 这项工作的意义是什么？(2): 从创新点、性能、工作量三个维度总结本文的优缺点。........按照后面的输出格式：</p></li><li>结论：(1):xxx;(2):创新点：xxx；性能：xxx；工作量：xxx;一定要用中文回答（专有名词需要用英文标注），表述尽可能简洁、学术化，不要重复前面<summary>的内容，原数字的使用价值，一定要严格按照格式，对应的内容输出到xxx，按照换行，.......表示根据实际需要填写，如果没有，不能写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-97d863b4d0c1a7df4fa9efa004db885d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b192f1f80d49d74775f653d122feedce.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d87faabfaa6a3719df968c6bd795b312.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8ada6cbf2edd7e1759c7ba909af2521f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-04b1531de51d0c973eb6011ffedeceb8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-df09277707b0e21453e0c13c0f195645.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>优化NeRF图像插补：抑制扩散模型的随机性、缓解纹理偏移，并弃用像素和感知损失函数。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型合成内容多样性高，阻碍辐射场收敛为清晰几何体。</li><li>将潜在扩散模型应用于真实数据会导致与图像条件不符的纹理偏移。</li><li>像素距离损失加剧了上述两个问题。</li><li>引入场景定制以缓和扩散模型的随机性。</li><li>使用蒙版对抗训练来缓解纹理偏移。</li><li>像素和感知损失在NeRF图像插补任务中是有害的。</li><li>该框架在各种真实场景中实现了最先进的NeRF图像插补结果。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>标题：驯化潜在扩散模型用于补充材料</li><p></p><p></p><li>作者：C.H. Lin 等</li><p></p><p></p><li>单位：未提及</li><p></p><p></p><li>关键词：NeRF、图像修复、潜在扩散模型、对抗性训练</li><p></p><p></p><li>链接：无</li><p></p><p></p><li><p></p><p>摘要：（1）研究背景：NeRF 是一种从多视角图像进行 3D 重建的表示形式。尽管一些最近的工作显示出使用扩散先验编辑重建的 NeRF 取得了初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。（2）过去方法及问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用潜在扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题因使用像素距离损失而进一步加剧。（3）本文方法：为了解决这些问题，我们提出用场景定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF 修复任务中是有害的。（4）方法性能：通过严格的实验，我们的框架在各种真实场景上产生了最先进的 NeRF 修复结果。</p></li><li><p>方法：（1）NeRF表示：使用神经辐射场（NeRF）表示3D场景，通过像素级回归损失函数优化NeRF，重建已知区域。（2）蒙版对抗训练：不使用像素距离损失，而是采用对抗损失和判别器特征匹配损失指导NeRF在修复区域的监督。（3）单目深度监督：利用单目深度先验对修复区域的几何形状进行正则化。（4）总训练目标：训练迭代包括重建步骤、修复步骤和判别器训练步骤，每个步骤优化不同的目标。（5）迭代数据更新和噪声调度：采用迭代数据更新和部分DDIM修复，以减轻扩散模型的多样性和随机性。</p></li></ol><p><strong>8. 结论</strong></p><p><strong>(1): 本工作的意义</strong></p><p>本文提出了一种新的框架，通过场景定制和掩码对抗性训练来解决NeRF修复中的几何模糊和纹理偏移问题。该框架在真实场景上实现了最先进的NeRF修复结果，为图像修复和3D重建提供了新的方法。</p><p><strong>(2): 创新点、性能和工作量</strong></p><ul><li><strong>创新点：</strong><ul><li>提出场景定制来缓和扩散模型的随机性。</li><li>采用掩码对抗性训练来减轻纹理偏移。</li><li>发现像素和感知损失在NeRF修复任务中是有害的。</li></ul></li><li><strong>性能：</strong><ul><li>在各种真实场景上产生了最先进的NeRF修复结果。</li><li>实现了清晰且确定性的几何形状合成。</li><li>减轻了纹理偏移，提高了与图像条件的一致性。</li></ul></li><li><strong>工作量：</strong><ul><li>场景定制和掩码对抗性训练增加了训练复杂度。</li><li>迭代数据更新和噪声调度需要额外的计算资源。</li></ul></li></ul><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-71b2d0d350aca831aa75f321f4a4b0fc.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e53c1166741cf80b67784bf8605b441d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f0ca2bc16aea3d2352fbc4822bb93beb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7b5effeb64b57b56ea109097322b49a0.jpg" align="middle"></details>## GPN: Generative Point-based NeRF**Authors:Haipeng Wang**Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances. [PDF](http://arxiv.org/abs/2404.08312v1) **Summary**生成式基于点的 NeRF 在扫描图像和重建点云的引导下，修复不完整点云，实现多视角一致性。**Key Takeaways**- 利用生成式点云 NeRF 修复不完整点云，同时保证几何和颜色一致性。- 采用自动解码器架构优化全局潜在条件，确保多视角一致性。- 生成点云与扫描图像几何一致、光滑且合理。- 在 ShapeNet 上的实验表明，该方法在神经场景渲染和编辑方面具有竞争力。- 该方法解决了部分扫描、3D 遮挡和动态光照条件下点云不完整的问题。- 该方法专注于点云修复，而非点云完成功能。- 该方法充分利用了扫描图像和重建点云的信息。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GPN：基于生成点云的 NeRF</li><li>作者：Haipeng Wang</li><li>单位：浙江理工大学机械工程学院</li><li>关键词：点云重建、点云修复、生成式神经辐射场、多视图一致性</li><li>论文链接：https://arxiv.org/abs/2404.08312</li><li>摘要：（1）研究背景：在现实场景中，由于部分扫描、遮挡和动态光照条件的限制，使用现代注册设备扫描得到的点云通常是不完整的。</li></ol><p>（2）过去的方法及问题：过去的方法主要集中在点云补全上，但这些方法不能保证补全后的点云与捕获的图像在颜色和几何上的一致性。</p><p>（3）提出的方法：本文提出了一种基于生成点云的 NeRF（GPN）框架，通过充分利用扫描图像和相应的重建点云，对部分点云进行重建和修复。修复后的点云可以实现与捕获图像在多视图上的一致性，并具有较高的空间分辨率。</p><p>（4）方法的性能及效果：在 ShapeNet 数据集上的广泛实验表明，本文方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。这些性能支持了本文的目标，即生成与部分扫描图像几何一致的、平滑且合理的点云。</p><p>Some Error for method(比如是不是没有Methods这个章节)</p><p><strong>8. 结论</strong></p><p><strong>(1): 本文意义</strong></p><p>本文提出了一种基于生成点云的 NeRF（GPN）框架，该框架能够修复部分点云并重建缺失部分，同时确保修复后的点云与捕获图像在多视图上的一致性。该方法为点云重建和修复领域提供了新的思路，具有较高的实用价值。</p><p><strong>(2): 优缺点总结</strong></p><p><strong>创新点：</strong></p><ul><li>提出了一种基于生成点云的 NeRF 框架，用于点云修复和重建。</li><li>通过引入多视图一致性约束，确保修复后的点云与捕获图像在几何和颜色上的一致。</li></ul><p><strong>性能：</strong></p><ul><li>在 ShapeNet 数据集上的实验表明，该方法在点云渲染和编辑任务上取得了与其他最先进方法相当的性能。</li><li>生成的点云具有较高的空间分辨率和平滑性。</li></ul><p><strong>工作量：</strong></p><ul><li>该方法的实现需要较高的计算资源和时间成本。</li><li>对于复杂场景，修复过程可能耗时较长。</li></ul><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-977026755832e69838d0636842958c12.jpg" align="middle"><img src="https://pica.zhimg.com/v2-40839a585a476aaaa262d3984922b2ea.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4e3d24ffa7fa8024bbe07bea2f5e200e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0fe6f628a3b732261e6a91523842e27c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f1a7a543764220776107e4bb9f17417e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8b8085e2655a2ad99861a7ef579e2447.jpg" align="middle"></details>## Are NeRFs ready for autonomous driving? Towards closing the   real-to-simulation gap**Authors:Carl Lindström, Georg Hess, Adam Lilja, Maryam Fatemi, Lars Hammarstrand, Christoffer Petersson, Lennart Svensson**Neural Radiance Fields (NeRFs) have emerged as promising tools for advancing autonomous driving (AD) research, offering scalable closed-loop simulation and data augmentation capabilities. However, to trust the results achieved in simulation, one needs to ensure that AD systems perceive real and rendered data in the same way. Although the performance of rendering methods is increasing, many scenarios will remain inherently challenging to reconstruct faithfully. To this end, we propose a novel perspective for addressing the real-to-simulated data gap. Rather than solely focusing on improving rendering fidelity, we explore simple yet effective methods to enhance perception model robustness to NeRF artifacts without compromising performance on real data. Moreover, we conduct the first large-scale investigation into the real-to-simulated data gap in an AD setting using a state-of-the-art neural rendering technique. Specifically, we evaluate object detectors and an online mapping model on real and simulated data, and study the effects of different fine-tuning strategies.Our results show notable improvements in model robustness to simulated data, even improving real-world performance in some cases. Last, we delve into the correlation between the real-to-simulated gap and image reconstruction metrics, identifying FID and LPIPS as strong indicators. See https://research.zenseact.com/publications/closing-real2sim-gap for our project page. [PDF](http://arxiv.org/abs/2403.16092v2) Accepted at Workshop on Autonomous Driving, CVPR 2024**摘要**针对自动驾驶的NeRF模拟，在不影响真实数据性能的情况下，通过增强感知模型对NeRF伪影的鲁棒性弥合真实现实和模拟数据差异。**要点*** NeRF在自动驾驶模拟和数据增强中潜力巨大。* 渲染方法性能提升，但仍有场景重建困难。* 提出通过增强感知模型鲁棒性来解决真实现实与模拟数据差异。* 开展了使用最新神经渲染技术在自动驾驶背景下的真实现实与模拟数据差异大规模研究。* 评估了真实和模拟数据上的目标检测器和在线建图模型。* 研究了不同微调策略的影响。* 模型对模拟数据的鲁棒性显著提高，甚至在某些情况下提升了真实世界性能。* 探索了真实现实与模拟数据差异和图像重建度量之间的相关性，确定FID和LPIPS是强有力的指标。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：NeRFs能否用于自动驾驶？缩小真实与模拟的差距</li><li>作者：Carl Lindström、Georg Hess、Adam Lilja、Maryam Fatemi、Lars Hammarstrand、Christoffer Petersson、Lennart Svensson</li><li>第一作者单位：Zenseact</li><li>关键词：NeRFs、自动驾驶、感知模型、真实与模拟差距</li><li>论文链接：https://arxiv.org/abs/2403.16092</li><li><p>摘要：(1) 研究背景：NeRFs在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。(2) 过去方法及其问题：虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对NeRF伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。(3) 研究方法：本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。(4) 性能和意义：结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现FID和LPIPS是强有力的指标。</p></li><li><p>方法：(1) 图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。(2) 混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。(3) 图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p></li></ol><p><strong>摘要</strong></p><p><strong>（1）研究背景</strong></p><p>NeRFs 在自动驾驶领域展现出巨大潜力，可用于闭环仿真和数据增强。然而，要信任仿真结果，需要确保自动驾驶系统能够以相同的方式感知真实和渲染数据。</p><p><strong>（2）过去方法及其问题</strong></p><p>虽然渲染方法的性能不断提高，但许多场景对于真实重建仍然具有固有挑战性。过去的方法主要专注于提高渲染保真度，而本文提出了一种新颖的视角，通过增强感知模型对 NeRF 伪影的鲁棒性来解决真实与模拟数据差距问题，而不会损害真实数据上的性能。</p><p><strong>（3）研究方法</strong></p><p>本文首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。具体来说，作者评估了物体检测器和在线建图模型在真实和模拟数据上的性能，并研究了不同微调策略的影响。</p><p><strong>（4）性能和意义</strong></p><p>结果表明，模型对模拟数据的鲁棒性有了显着提高，在某些情况下甚至提高了真实世界的性能。此外，作者深入研究了真实与模拟差距与图像重建指标之间的相关性，发现 FID 和 LPIPS 是强有力的指标。</p><p><strong>方法摘要</strong></p><p><strong>（5）方法</strong></p><p>（1）图像增强：使用图像增强方法来提高感知模型对渲染数据伪影的鲁棒性。（2）混合渲染图像进行微调：在微调过程中加入渲染数据，以适应感知模型到 NeRF 渲染数据。（3）图像到图像转换：使用图像到图像转换方法生成类似 NeRF 的图像，以增加 NeRF 类似图像的数量，用于微调。</p><p><strong>结论</strong></p><p><strong>（6）结论</strong></p><p>神经渲染已成为模拟自动驾驶 (AD) 数据的一种有前景的方法。然而，为了在实践中实用，人们必须了解 AD 系统在模拟数据上的行为如何转移到真实数据上。我们的<strong>大规模调查揭示了感知模型在模拟和真实图像中暴露的性能差距</strong>。我们提出了一种新的策略来缩小差距：增加感知模型对 NeRF 模拟数据的鲁棒性。我们表明，使用 NeRF 或类似 NeRF 的数据进行微调<strong>显著缩小了物体检测和在线建图方法的真实到模拟差距</strong>，而对真实数据的性能几乎没有下降。此外，对于在线建图，我们表明有针对性地生成新场景可以提高真实数据的性能。尽管如此，当改变自我车辆姿态时，渲染质量会迅速下降。鉴于我们的发现，即低感知质量（即 LPIPS 和 FID 分数）与较大的真实到模拟差距密切相关，我们认为在推断设置中提高渲染质量仍然是使 NeRF 能够用于测试和改进 AD 系统的关键挑战。</p><p><strong>致谢</strong></p><p>我们感谢 Adam Tonderski 和 William Ljungbergh 提供宝贵的讨论。这项工作部分由 Knut 和 Alice Wallenberg 基金会资助的 Wallenberg 人工智能、自主系统和软件计划 (WASP) 资助。计算资源由 NAISS 在 NSC Berzelius 提供，部分由瑞典研究委员会资助，协议号。2022-06725。</p><p><strong>（7）总结</strong></p><p>（1）<strong>本项工作的意义</strong>：提出了一种新颖的视角来解决真实与模拟数据差距问题，通过增强感知模型对 NeRF 伪影的鲁棒性，而不会损害真实数据上的性能。</p><p>（2）<strong>本文的优缺点</strong>：* <strong>创新点</strong>：首次使用最先进的神经渲染技术对自动驾驶场景中的真实与模拟数据差距进行了大规模调查。* <strong>性能</strong>：提出的方法显着提高了感知模型对模拟数据的鲁棒性，在某些情况下甚至提高了真实世界的性能。* <strong>工作量</strong>：需要大量的渲染数据和训练时间来实现感知模型的鲁棒性。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-e8445490e4eaaeba826ce93fa44739ab.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-226e40089f23e26b7537bc25c8c4012b.jpg" align="middle"><img src="https://pica.zhimg.com/v2-d40bf7f142a8199e369826096b0b0904.jpg" align="middle"><img src="https://pica.zhimg.com/v2-44b007ade1b910cc4a89084343b2e13c.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-17  Plug-and-Play Acceleration of Occupancy Grid-based NeRF Rendering using   VDB Grid and Hierarchical Ray Traversal</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/3DGS/</id>
    <published>2024-04-17T10:58:41.000Z</published>
    <updated>2024-04-17T10:58:41.335Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes"><a href="#Gaussian-Opacity-Fields-Efficient-and-Compact-Surface-Reconstruction-in-Unbounded-Scenes" class="headerlink" title="Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes"></a>Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in   Unbounded Scenes</h2><p><strong>Authors:Zehao Yu, Torsten Sattler, Andreas Geiger</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results, while allowing the rendering of high-resolution images in real-time. However, leveraging 3D Gaussians for surface reconstruction poses significant challenges due to the explicit and disconnected nature of 3D Gaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel approach for efficient, high-quality, and compact surface reconstruction in unbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of 3D Gaussians, enabling direct geometry extraction from 3D Gaussians by identifying its levelset, without resorting to Poisson reconstruction or TSDF fusion as in previous work. We approximate the surface normal of Gaussians as the normal of the ray-Gaussian intersection plane, enabling the application of regularization that significantly enhances geometry. Furthermore, we develop an efficient geometry extraction method utilizing marching tetrahedra, where the tetrahedral grids are induced from 3D Gaussians and thus adapt to the scene’s complexity. Our evaluations reveal that GOF surpasses existing 3DGS-based methods in surface reconstruction and novel view synthesis. Further, it compares favorably to, or even outperforms, neural implicit methods in both quality and speed. </p><p><a href="http://arxiv.org/abs/2404.10772v1">PDF</a> Project page:   <a href="https://niujinshuchong.github.io/gaussian-opacity-fields">https://niujinshuchong.github.io/gaussian-opacity-fields</a></p><p><strong>摘要</strong><br>三维高斯斑点融合 (3DGS) 将三维高斯体渲染为体素，直接提取表面几何信息，高效且紧凑地重建任意场景下的物体表面。</p><p><strong>关键要点</strong></p><ul><li>提出高斯不透明度场 (GOF)，通过射线追踪体绘制三维高斯体获得，直接从三维高斯体中提取几何信息。</li><li>将高斯体表面法向量近似为射线-高斯体相交平面的法向量，并应用正则化以显著增强几何形状。</li><li>开发了一种利用行进四面体的有效几何提取方法，其中四面体网格由三维高斯体诱导，并适应场景的复杂程度。</li><li>在表面重建和新视图合成中，GOF 优于现有的基于 3DGS 的方法。</li><li>在质量和速度方面，GOF 与神经隐式方法相当，甚至优于神经隐式方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：高斯不透明度场：无界场景中的高效紧凑表面重建</li><li>作者：Zehao Yu, Torsten Sattler, Andreas Geiger</li><li>第一作者单位：德国图宾根大学图宾根人工智能中心</li><li>关键词：新颖视图合成、可微渲染、高斯溅射、表面重建、多视图转 3D</li><li>论文链接：https://niujinshuchong.github.io/gaussian-opacity-fields</li><li><p>摘要：（1）：研究背景：近年来，神经辐射场（NeRF）在新型视图合成和表面重建方面取得了显著进展。然而，现有方法在重建无界场景中前景对象时存在局限性，并且计算成本高。（2）：过去方法及其问题：传统方法通常采用体素融合或泊松重建来从 NeRF 的不透明度场中提取表面。这些方法存在噪声、不完整和计算成本高的缺点。（3）：提出的方法：本文提出了一种称为高斯不透明度场（GOF）的新颖方法。GOF 通过对 3D 高斯进行基于光线追踪的体积渲染来获得，它使我们能够直接识别 3D 高斯的水平集，从而从 3D 高斯中提取几何。此外，我们开发了一种利用行进四面体的有效几何提取方法，其中四面体网格是从 3D 高斯中感应出来的，并因此适应场景的复杂性。（4）：方法的性能：我们的评估表明，GOF 在表面重建和新颖视图合成方面优于现有的基于 3DGS 的方法。此外，它在质量和速度上都与神经隐式方法相当甚至优于神经隐式方法。</p></li><li><p>方法：(1) 构建高斯不透明度场 (GOF)，通过对 3D 高斯进行基于光线追踪的体积渲染获得；(2) 扩展 2D 高斯中的两个有效正则化项到 3D 高斯，提升重建质量；(3) 提出一种基于行进四面体的有效几何提取方法，从 GOF 中提取详细且紧凑的网格。</p></li><li><p>结论：(1): 本工作提出了一种高斯不透明度场 (GOF)，这是一种用于在无界场景中进行高效、高质量且紧凑的表面重建的新颖方法。我们的 GOF 是通过对 3D 高斯进行基于光线追踪的体积渲染获得的，与 RGB 渲染保持一致。我们的 GOF 能够直接从 3D 高斯中提取几何，通过识别其水平集，而无需泊松重建或 TSDF。我们近似高斯的曲面法线为射线-高斯相交平面的法线，并应用深度-法线一致性正则化来增强几何重建。此外，我们提出了一种利用行进四面体的有效且紧凑的网格提取方法，其中四面体网格是从 3D 高斯中感应出来的。我们的评估表明，GOF 在无界场景中的表面重建和新颖视图合成方面都优于现有方法。(2): 创新点：提出了一种基于 3D 高斯的高斯不透明度场 (GOF) 来进行表面重建；开发了一种基于行进四面体的有效几何提取方法，可以从 GOF 中提取详细且紧凑的网格。性能：在表面重建和新颖视图合成方面优于现有的基于 3D 高斯的方法；在质量和速度上都与神经隐式方法相当甚至优于神经隐式方法。工作量：需要进行基于光线追踪的体积渲染和行进四面体的几何提取，计算成本较高。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-7120d48e211e632332c006e60959fa7f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5a10b3207b26758f6049e10e774c09a3.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e998868d938693cc86772478ebad84a5.jpg" align="middle"><img src="https://picx.zhimg.com/v2-818d65bf288671197aa1a4f35098147c.jpg" align="middle"></details><h2 id="AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting"><a href="#AbsGS-Recovering-Fine-Details-for-3D-Gaussian-Splatting" class="headerlink" title="AbsGS: Recovering Fine Details for 3D Gaussian Splatting"></a>AbsGS: Recovering Fine Details for 3D Gaussian Splatting</h2><p><strong>Authors:Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, Yong Dou</strong></p><p>3D Gaussian Splatting (3D-GS) technique couples 3D Gaussian primitives with differentiable rasterization to achieve high-quality novel view synthesis results while providing advanced real-time rendering performance. However, due to the flaw of its adaptive density control strategy in 3D-GS, it frequently suffers from over-reconstruction issue in intricate scenes containing high-frequency details, leading to blurry rendered images. The underlying reason for the flaw has still been under-explored. In this work, we present a comprehensive analysis of the cause of aforementioned artifacts, namely gradient collision, which prevents large Gaussians in over-reconstructed regions from splitting. To address this issue, we propose the novel homodirectional view-space positional gradient as the criterion for densification. Our strategy efficiently identifies large Gaussians in over-reconstructed regions, and recovers fine details by splitting. We evaluate our proposed method on various challenging datasets. The experimental results indicate that our approach achieves the best rendering quality with reduced or similar memory consumption. Our method is easy to implement and can be incorporated into a wide variety of most recent Gaussian Splatting-based methods. We will open source our codes upon formal publication. Our project page is available at: <a href="https://ty424.github.io/AbsGS.github.io/">https://ty424.github.io/AbsGS.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.10484v1">PDF</a> </p><p><strong>Summary</strong><br> 三维高斯溅射（3D-GS）技术将三维高斯原语与可微栅格化相结合，在提供先进实时渲染性能的同时实现高质量新视图合成结果。</p><p><strong>Key Takeaways</strong></p><ul><li>3D-GS中自适应密度控制策略存在缺陷，导致在包含高频细节的复杂场景中经常出现过度重建问题，从而导致渲染图像模糊。</li><li>过度重建区域中过大的高斯体无法分裂，原因是梯度碰撞阻止了它们的分裂。</li><li>提出了一种新的同向视图空间位置梯度，作为致密化的标准，解决了这一问题。</li><li>该策略有效地识别出过度重建区域中的大高斯体，并通过分裂恢复精细细节。</li><li>该方法在各种具有挑战性的数据集上进行了评估，实验结果表明该方法实现了最佳渲染质量，同时减少了内存消耗。</li><li>该方法易于实现，可以整合到多种最新的基于高斯溅射的方法中。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于梯度碰撞的 3D 高斯泼溅中的过重建问题分析与解决（3D 高斯泼溅中的过重建问题分析与解决）</li><li>作者：Tianye Li<em>, Yuxuan Zhang</em></li><li>单位：无</li><li>关键词：Novel View Synthesis, 3D Gaussian Splatting, Point-based Radiance Field, 3D reconstruction</li><li>论文链接：arXiv:2404.10484v1[cs.CV]   Github 链接：None</li><li><p>摘要：（1）研究背景：3D 高斯泼溅技术将 3D 高斯基元与可微光栅化相结合，以实现高质量的新视角合成结果，同时提供先进的实时渲染性能。（2）过去的方法及其问题：3D 高斯泼溅中自适应密度控制策略的缺陷导致其在包含高频细节的复杂场景中经常出现过重建问题，从而导致渲染图像模糊。该缺陷的根本原因尚未得到充分探索。（3）本文提出的研究方法：提出了一种新颖的同向视图空间位置梯度作为致密化的标准，对上述伪影的原因（即梯度碰撞）进行了全面分析。该策略有效地识别出过重建区域中的大高斯体，并通过分裂恢复精细细节。（4）方法在何种任务上取得了怎样的性能：在各种具有挑战性的数据集上评估了所提出的方法。实验结果表明，该方法以减少或类似的内存消耗实现了最佳渲染质量。该方法易于实现，可以整合到各种最新的基于高斯泼溅的方法中。</p></li><li><p>方法：（1）分析了 3D 高斯泼溅中过重建问题的成因，即梯度碰撞，该现象阻止了过重建区域中大高斯体分裂；（2）提出了同向视图空间位置梯度作为致密化标准，有效识别出过重建区域中的大高斯体；（3）通过分裂操作恢复精细细节，改善渲染质量。</p></li><li><p>结论：(1)：本文深入研究了 3D 高斯泼溅中的过重建问题，并提出了基于梯度碰撞的分析与解决方法，有效提升了渲染质量。(2)：创新点：本文从梯度碰撞的角度分析了过重建问题的成因，并提出了同向视图空间位置梯度作为致密化标准，有效识别出过重建区域中的大高斯体，通过分裂操作恢复精细细节，改善渲染质量。性能：实验结果表明，该方法以减少或类似的内存消耗实现了最佳渲染质量。工作量：该方法易于实现，可以整合到各种最新的基于高斯泼溅的方法中。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-b3954ae2d030deb08a1858901e173aeb.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7ed623f29e65c33839f7af3ba662cf77.jpg" align="middle"><img src="https://picx.zhimg.com/v2-32e2b412c01557b89b9f9fab52d1386e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a224297269ae60107729a25ab0846851.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-b860f19dbacac5f895a9fea794424f48.jpg" align="middle"></details><h2 id="SRGS-Super-Resolution-3D-Gaussian-Splatting"><a href="#SRGS-Super-Resolution-3D-Gaussian-Splatting" class="headerlink" title="SRGS: Super-Resolution 3D Gaussian Splatting"></a>SRGS: Super-Resolution 3D Gaussian Splatting</h2><p><strong>Authors:Xiang Feng, Yongbo He, Yubo Wang, Yan Yang, Zhenzhong Kuang, Yu Jun, Jianping Fan, Jiajun ding</strong></p><p>Recently, 3D Gaussian Splatting (3DGS) has gained popularity as a novel explicit 3D representation. This approach relies on the representation power of Gaussian primitives to provide a high-quality rendering. However, primitives optimized at low resolution inevitably exhibit sparsity and texture deficiency, posing a challenge for achieving high-resolution novel view synthesis (HRNVS). To address this problem, we propose Super-Resolution 3D Gaussian Splatting (SRGS) to perform the optimization in a high-resolution (HR) space. The sub-pixel constraint is introduced for the increased viewpoints in HR space, exploiting the sub-pixel cross-view information of the multiple low-resolution (LR) views. The gradient accumulated from more viewpoints will facilitate the densification of primitives. Furthermore, a pre-trained 2D super-resolution model is integrated with the sub-pixel constraint, enabling these dense primitives to learn faithful texture features. In general, our method focuses on densification and texture learning to effectively enhance the representation ability of primitives. Experimentally, our method achieves high rendering quality on HRNVS only with LR inputs, outperforming state-of-the-art methods on challenging datasets such as Mip-NeRF 360 and Tanks &amp; Temples. Related codes will be released upon acceptance. </p><p><a href="http://arxiv.org/abs/2404.10318v1">PDF</a> submit ACM MM 2024</p><p><strong>Summary</strong><br>SRGS 在高分辨率空间进行优化，引入亚像素约束利用亚像素交叉视图信息，并结合预训练的 2D 超分辨率模型来提高三维高斯光栅原始体的表示能力，用于解决高分辨率新视角合成问题。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 在低分辨率中优化导致原始体的稀疏性和纹理缺陷，阻碍高分辨率新视角合成。</li><li>SRGS 在高分辨率空间进行优化，提高了原始体的密度。</li><li>引入亚像素约束来利用多个低分辨率视图的亚像素交叉视图信息。</li><li>累积更多视点的梯度有利于原始体的密集化。</li><li>集成预训练的 2D 超分辨率模型，使密集的原始体能够学习可靠的纹理特征。</li><li>SRGS 专注于致密化和纹理学习，有效增强了原始体的表示能力。</li><li>SRGS 在仅有低分辨率输入的情况下，实现了高分辨率新视角合成的出色渲染质量。</li><li>SRGS 在 Mip-NeRF 360 和坦克与寺庙等挑战性数据集上优于最先进的方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：SRGS：超分辨率 3D 高斯泼溅</li><li>作者：Xiang Feng，Yongbo He，Yubo Wang，Yan Yang，Zhenzhong Kuang，Jun Yu，Jianping Fan，Jiajun Ding</li><li>单位：杭州电子科技大学</li><li>关键词：3D 高斯泼溅，超分辨率，新视角合成</li><li>论文链接：https://arxiv.org/pdf/2404.10318.pdf，Github 链接：无</li><li>摘要：（1）研究背景：3D 高斯泼溅 (3DGS) 是一种新颖的显式 3D 表示，依赖于高斯基元的表示能力来提供高质量的渲染。然而，在低分辨率下优化后的基元不可避免地表现出稀疏性和纹理不足，对实现高分辨率新视角合成 (HRNVS) 构成挑战。（2）过去方法及其问题：3DGS 虽然具有多项优点，但在仅使用低分辨率输入执行 HRNVS 时，渲染质量会急剧下降。这是因为 3DGS 中高斯基元的表示能力对于实现高质量的视图合成至关重要。具体来说，高分辨率渲染需要具有细粒度纹理特征的更密集的高斯基元。然而，对于低分辨率场景优化的基元不可避免地分布稀疏，导致渲染的 HR 视图中出现伪影。此外，现有的低分辨率视图缺乏必要的 HR 纹理。因此，3D 空间中的基元不可能在没有 HR 纹理的反投影的情况下学习相应的特征。（3）本文方法：为了解决这些限制，本文提出了超分辨率 3D 高斯泼溅 (SRGS)，它扩展了 3DGS 以实现高质量的 HRNVS。所提出的方法包括两部分，即超分辨率高斯致密化和纹理引导高斯学习。（4）方法性能：在 HRNVS 任务上，与仅使用低分辨率输入的最新方法相比，本文方法在具有挑战性的数据集（如 Mip-NeRF360 和 Tanks &amp; Temples）上实现了更高的渲染质量。这些性能支持了本文的目标，即在仅使用低分辨率输入的情况下实现高质量的 HRNVS。</li></ol><p>7.方法：（1）超分辨率高斯致密化；（2）纹理引导高斯学习。</p><ol><li>结论：(1): 本工作通过仅使用低分辨率输入实现高质量的新视角合成，在该领域做出了首次尝试。具体来说，我们首先使用超分辨率高斯致密化策略来增加高斯基元的密度，从而能够表示细粒度的高分辨率信息。此外，我们引入了一种纹理引导高斯学习策略，该策略指导高斯基元从外部 2D 超分辨率模型的先验中学习真实的纹理。在三个公开数据集上的实验结果表明，SRGS 有效地增强了高斯基元的表示能力，接近使用高分辨率视图训练的 3DGS 的渲染性能。值得注意的是，我们的 SRGS 主要受 2D 超分辨率模型的限制。在我们的未来工作中，我们将进一步探索没有 2D 超分辨率模型的新视角合成方法。(2): 创新点：SRGS；性能：SRGS 在仅使用低分辨率输入的情况下实现了高质量的新视角合成，在具有挑战性的数据集上优于最新方法；工作量：中等，需要额外的 2D 超分辨率模型。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8e51f459b7f920ab07ae498fd133cdfa.jpg" align="middle"><img src="https://picx.zhimg.com/v2-73911338ed13dfd8725fff2143317b2d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-09dc3fb075728d00cdf50313743df98f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-8712c281513c2e8cc3bc68f15b35bb68.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-8f9ab4e24b8b4895c13b3db9ace1733c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3ee8ae605e49ec5d4b5081d13f242fab.jpg" align="middle"></details><h2 id="LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives"><a href="#LetsGo-Large-Scale-Garage-Modeling-and-Rendering-via-LiDAR-Assisted-Gaussian-Primitives" class="headerlink" title="LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives"></a>LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted   Gaussian Primitives</h2><p><strong>Authors:Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</strong></p><p>Large garages are ubiquitous yet intricate scenes in our daily lives, posing challenges characterized by monotonous colors, repetitive patterns, reflective surfaces, and transparent vehicle glass. Conventional Structure from Motion (SfM) methods for camera pose estimation and 3D reconstruction fail in these environments due to poor correspondence construction. To address these challenges, this paper introduces LetsGo, a LiDAR-assisted Gaussian splatting approach for large-scale garage modeling and rendering. We develop a handheld scanner, Polar, equipped with IMU, LiDAR, and a fisheye camera, to facilitate accurate LiDAR and image data scanning. With this Polar device, we present a GarageWorld dataset consisting of five expansive garage scenes with diverse geometric structures and will release the dataset to the community for further research. We demonstrate that the collected LiDAR point cloud by the Polar device enhances a suite of 3D Gaussian splatting algorithms for garage scene modeling and rendering. We also propose a novel depth regularizer for 3D Gaussian splatting algorithm training, effectively eliminating floating artifacts in rendered images, and a lightweight Level of Detail (LOD) Gaussian renderer for real-time viewing on web-based devices. Additionally, we explore a hybrid representation that combines the advantages of traditional mesh in depicting simple geometry and colors (e.g., walls and the ground) with modern 3D Gaussian representations capturing complex details and high-frequency textures. This strategy achieves an optimal balance between memory performance and rendering quality. Experimental results on our dataset, along with ScanNet++ and KITTI-360, demonstrate the superiority of our method in rendering quality and resource efficiency. </p><p><a href="http://arxiv.org/abs/2404.09748v1">PDF</a> Project Page: <a href="https://jdtsui.github.io/letsgo/">https://jdtsui.github.io/letsgo/</a></p><p><strong>Summary</strong><br>激光辅助高斯球面投影法，可高效建模大规模复杂室内场景（例如车库），并在网页端实现实时渲染。</p><p><strong>Key Takeaways</strong></p><ul><li>开发了适用于大规模车库建模和渲染的激光辅助高斯球面投影方法 LetsGo。</li><li>设计了一种集 IMU、激光雷达和鱼眼相机于一体的手持扫描仪 Polar。</li><li>推出了包含五种大规模车库场景的 GarageWorld 数据集。</li><li>提出了一种新的深度正则化器，可有效消除渲染图像中的浮动伪影。</li><li>提出了一种轻量级的细节级别（LOD）高斯渲染器，可实现基于网络设备的实时查看。</li><li>探索了一种混合表示，结合了传统网格（描述简单几何形状和颜色）和现代 3D 高斯表示（捕捉复杂细节和高频纹理）的优势。</li><li>实验结果表明，该方法在渲染质量和资源效率方面均优于其他方法。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LetsGo：基于激光雷达辅助的高斯体绘制的大规模车库建模与渲染</li><li>作者：Jiadi Cui, Junming Cao, Yuhui Zhong, Liao Wang, Fuqiang Zhao, Penghao Wang, Yifan Chen, Zhipeng He, Lan Xu, Yujiao Shi, Yingliang Zhang, Jingyi Yu</li><li>第一作者单位：上海科技大学</li><li>关键词：车库建模、激光雷达、高斯体绘制</li><li>论文链接：https://arxiv.org/abs/2404.09748   Github 代码链接：无</li><li>摘要：   （1）研究背景：大规模车库是日常生活中的普遍存在，但由于单调的颜色、重复的图案、反光表面和透明的车辆玻璃，给建模和渲染带来了挑战。传统的基于 Structure from Motion（SfM）的相机位姿估计和三维重建方法在这些环境中会失败，因为难以建立良好的对应关系。   （2）过去方法与问题：基于 SfM 的方法在车库环境中效果不佳。本文提出了一种激光雷达辅助的高斯体绘制方法来解决这些问题。   （3）研究方法：本文提出了一种称为 LetsGo 的方法，它使用激光雷达辅助的高斯体绘制来进行大规模车库建模和渲染。该方法包括：</li><li>开发了一种配备 IMU、激光雷达和鱼眼相机的便携式扫描仪 Polar，用于采集精确的激光雷达和图像数据。</li><li>提出了一种新的深度正则化器，用于训练三维高斯体绘制算法，有效地消除了渲染图像中的悬浮伪影。</li><li>设计了一种轻量级的细节层次（LOD）高斯渲染器，用于在基于 Web 的设备上实时查看。</li><li>探索了一种混合表示，它结合了传统网格（用于描绘简单的几何形状和颜色）和现代三维高斯表示（用于捕捉复杂细节和高频纹理）的优点。   （4）方法性能：在车库场景建模和渲染任务上，该方法取得了良好的性能，有效地消除了悬浮伪影，并实现了实时查看。这些性能支持了本文的目标，即在大规模车库场景中实现高保真渲染。</li></ol><p>7.方法：（1）：使用配备IMU、激光雷达和鱼眼相机的便携式扫描仪Polar采集精确的激光雷达和图像数据；（2）：提出新的深度正则化器，训练三维高斯体绘制算法，消除渲染图像中的悬浮伪影；（3）：设计轻量级的细节层次（LOD）高斯渲染器，用于在基于Web的设备上实时查看；（4）：探索结合传统网格和现代三维高斯表示的混合表示，兼顾简单几何形状和复杂细节的描绘。</p><ol><li>结论：（1）：本工作贡献了一款用于数据采集的手持设备Polar，一个车库世界数据集，用于场景表示的激光雷达辅助的高斯体，以及一种轻量级的渲染技术，该技术允许在消费者级设备上进行基于Web的渲染。得益于这些创新，我们成功地重建了各种具有多样化和具有挑战性环境的车库，允许从任何视点进行实时轻量级渲染。在收集的和两个公共数据集上的实验结果证明了我们方法的有效性。我们的车库世界以及重建的3D模型和实时渲染，支持一系列应用程序，包括训练数据生成和自动驾驶算法的测试平台、自动车辆定位、导航和停车的实时辅助，以及VFX制作。我们目前的工作主要集中在对世界的感知上，并且它支持下游识别任务。未来，我们还将探索车库生成，不断推动车库建模的边界，并实现从感知、识别到生成。（2）：创新点：xxx；性能：xxx；工作量：xxx；</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-2a3cd69515b02d46d8a8eb1653b52018.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a59b13ea73e0cb9d1f84e56d6ffa6262.jpg" align="middle"><img src="https://pica.zhimg.com/v2-6a7de06ef015705d30f275980dba4bac.jpg" align="middle"></details><h2 id="CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting"><a href="#CompGS-Efficient-3D-Scene-Representation-via-Compressed-Gaussian-Splatting" class="headerlink" title="CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting"></a>CompGS: Efficient 3D Scene Representation via Compressed Gaussian   Splatting</h2><p><strong>Authors:Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, Sam Kwong</strong></p><p>Gaussian splatting, renowned for its exceptional rendering quality and efficiency, has emerged as a prominent technique in 3D scene representation. However, the substantial data volume of Gaussian splatting impedes its practical utility in real-world applications. Herein, we propose an efficient 3D scene representation, named Compressed Gaussian Splatting (CompGS), which harnesses compact Gaussian primitives for faithful 3D scene modeling with a remarkably reduced data size. To ensure the compactness of Gaussian primitives, we devise a hybrid primitive structure that captures predictive relationships between each other. Then, we exploit a small set of anchor primitives for prediction, allowing the majority of primitives to be encapsulated into highly compact residual forms. Moreover, we develop a rate-constrained optimization scheme to eliminate redundancies within such hybrid primitives, steering our CompGS towards an optimal trade-off between bitrate consumption and representation efficacy. Experimental results show that the proposed CompGS significantly outperforms existing methods, achieving superior compactness in 3D scene representation without compromising model accuracy and rendering quality. Our code will be released on GitHub for further research. </p><p><a href="http://arxiv.org/abs/2404.09458v1">PDF</a> Submitted to a conference</p><p><strong>Summary</strong><br>高斯点集通过优化混合高斯基元结构，实现高精度3D场景表示的低数据开销。</p><p><strong>Key Takeaways</strong></p><ul><li>高斯点集因高渲染质量和效率在3D场景表示中受到广泛应用。</li><li>大量数据阻碍了高斯点集在实际应用中的实用性。</li><li>提出压缩高斯点集（CompGS），使用紧凑的高斯基元进行3D场景建模。</li><li>设计混合基元结构，捕捉基元间的预测关系，保证紧凑性。</li><li>利用少量锚定基元进行预测，将大多数基元封装成紧凑的残差形式。</li><li>开发速率受限优化方案，消除混合基元内的冗余。</li><li>CompGS在不影响模型精度和渲染质量的前提下，显著优于现有方法。</li><li>代码将在GitHub上发布，以促进进一步研究。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：CompGS：通过压缩高斯喷射实现高效的 3D 场景表示</li><li>作者：Xiangrui Liu，Xinju Wu，Pingping Zhang，Shiqi Wang，Zhu Li，Sam Kwong</li><li>所属机构：香港城市大学</li><li>关键词：3D 场景表示，高斯喷射，压缩</li><li>论文链接：None，Github 代码链接：None</li><li>摘要：（1）：研究背景：高斯喷射以其出色的渲染质量和效率而闻名，已成为 3D 场景表示中的重要技术。但是，高斯喷射大量的数据量阻碍了其在实际应用中的实用性。（2）：过去的方法：现有方法通过减少高斯喷射中 3D 高斯的数量和体积来压缩高斯喷射。这些方法通常采用启发式剪枝策略来移除对渲染质量贡献不大的 3D 高斯。此外，通常将矢量量化应用于保留的 3D 高斯以进一步压缩。（3）：提出的研究方法：本文提出了一种称为压缩高斯喷射 (CompGS) 的高效 3D 场景表示方法，该方法利用紧凑的高斯基元进行逼真的 3D 场景建模，同时显著减少了数据大小。为了确保高斯基元的紧凑性，我们设计了一种混合基元结构来捕获它们之间的预测关系。然后，我们利用一小组锚定基元进行预测，允许将大多数基元封装成高度紧凑的残差形式。此外，我们开发了一个速率约束优化方案来消除这种混合基元中的冗余，从而引导我们的 CompGS 在比特率消耗和表示效率之间实现最佳权衡。（4）：方法性能：实验结果表明，所提出的 CompGS 明显优于现有方法，在 3D 场景表示中实现了卓越的紧凑性，同时不影响模型准确性和渲染质量。</li></ol><p><strong>方法</strong></p><p>(1): 混合基元结构：建立一个混合基元结构，包括锚定基元和耦合基元。锚定基元提供参考嵌入和几何属性，而耦合基元仅包含紧凑的残差嵌入，以弥补预测误差。</p><p>(2): 跨基元预测：利用锚定基元的几何和外观属性，通过跨基元预测为耦合基元生成几何和外观属性。</p><p>(3): 速率约束优化：建立速率约束优化方案，通过最小化比特率消耗和渲染失真，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</p><p>(4): 熵估计：利用熵估计来有效建模锚定基元和耦合基元的比特率，为速率约束优化提供信息。</p><ol><li>结论：(1) 本工作提出了一个新颖的 3D 场景表示方法，压缩高斯喷射 (CompGS)，该方法利用紧凑的基元进行高效的 3D 场景表示，同时显著减小了数据大小。在此，我们为紧凑的场景建模量身定制了混合基元结构，其中耦合基元通过有限的锚定基元集有效地进行预测，从而封装成简洁的残差嵌入。同时，我们开发了一个速率约束优化方案，以进一步提高基元的紧凑性。在这个方案中，基元比率模型通过熵估计建立，然后制定速率失真代价以优化这些基元，以在渲染效率和比特率消耗之间实现最佳权衡。结合混合基元结构和速率约束优化，我们的 CompGS 优于现有的压缩方法，实现了卓越的尺寸缩减，同时不影响渲染质量。(2) 创新点：</li><li>混合基元结构，通过锚定基元预测耦合基元，实现紧凑的场景建模。</li><li>速率约束优化方案，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</li><li>熵估计，用于有效建模基元的比特率，为速率约束优化提供信息。性能：</li><li>与现有方法相比，在 3D 场景表示中实现了卓越的紧凑性，同时不影响模型准确性和渲染质量。工作量：</li><li>提出了一种新颖的 3D 场景表示方法，该方法利用紧凑的基元进行高效的 3D 场景表示，同时显著减小了数据大小。</li><li>设计了一种混合基元结构，包括锚定基元和耦合基元，以捕获基元之间的预测关系。</li><li>利用一小组锚定基元进行预测，允许将大多数基元封装成高度紧凑的残差形式。</li><li>开发了一个速率约束优化方案，通过最小化比特率消耗和渲染失真，联合优化基元和神经网络，实现紧凑的基元表示和压缩效率。</li><li>利用熵估计来有效建模锚定基元和耦合基元的比特率，为速率约束优化提供信息。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-7d580221a3c320fe2485a958d5382e40.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e32abb85df2e20749e660a25f1ddab87.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4824050d2c2a0b18fbff95df4c7fbc91.jpg" align="middle"><img src="https://picx.zhimg.com/v2-c0ac958806d8e21cae6a784ed3e74514.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-9cbd9e0e22a3201f4a5053aa5a6d3df2.jpg" align="middle"></details><h2 id="DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling"><a href="#DreamScape-3D-Scene-Creation-via-Gaussian-Splatting-joint-Correlation-Modeling" class="headerlink" title="DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling"></a>DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation   Modeling</h2><p><strong>Authors:Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang</strong></p><p>Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods. </p><p><a href="http://arxiv.org/abs/2404.09227v1">PDF</a> </p><p><strong>Summary</strong><br>DreamScape 是一个仅从文本描述中创建高度一致的 3D 场景的方法，它利用了 Gaussian Splatting 的强大 3D 表示功能和大语言模型 (LLM) 的复杂排列能力。</p><p><strong>Key Takeaways</strong></p><ul><li>3D 高斯指南 ($3{DG^2}$) 用于场景表示，包括从 LLM 使用文本提示直接推导出的语义原语（对象）及其空间变换和关系。</li><li>渐进式比例控制在局部对象生成期间进行定制，确保不同大小和密度的对象适应场景，解决了后续全局优化阶段中简单混合引起的训练不稳定问题。</li><li>为了减轻 LLM 先验的潜在偏差，我们在全局级别对对象之间的碰撞关系进行建模，从而增强物理正确性和整体真实感。</li><li>为了生成像雨和雪这样广泛分布在场景中的普遍对象，我们引入了一种稀疏初始化和致密化策略。</li><li>实验表明，DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，达到了最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScape：基于高斯泼溅的 3D 场景创建</li><li>作者：袁雪宁、杨宏宇、赵悦明、黄迪</li><li>单位：北京航空航天大学</li><li>关键词：多模态生成、3D 场景生成、场景合成、3D 高斯泼溅、LLM</li><li>论文链接：None    Github 链接：None</li><li>摘要：(1)：研究背景：近年来，文本到 3D 创建的进展得益于将来自文本到图像生成的扩散模型的强大先验整合到 3D 域中。然而，生成具有多个实例和复杂排列特征的 3D 场景仍然具有挑战性。(2)：以往方法：以往方法存在以下问题：</li><li>无法处理具有多个实例和复杂排列的 3D 场景。</li><li>训练不稳定，导致场景中不同大小和密度的对象无法适应。</li><li>容易受到 LLM 先验的偏差影响，导致物理不正确和整体真实性降低。</li><li>难以生成分布在场景中的普遍对象，如雨和雪。(3)：本文方法：本文提出 DreamScape，这是一种仅从文本描述创建高度一致的 3D 场景的方法，它利用了高斯泼溅的强大 3D 表示能力和大语言模型 (LLM) 的复杂排列能力。我们的方法包括：</li><li>3D 高斯引导 (3DG2) 用于场景表示，它由语义基元（对象）及其空间变换和关系组成，这些关系直接从文本提示中使用 LLM 推导出来。这种组合表示允许对整个场景进行局部到全局优化。</li><li>在局部对象生成期间定制的渐进式尺度控制，确保不同大小和密度的对象适应场景，从而解决后续全局优化阶段中简单混合引起的训练不稳定性问题。</li><li>为了减轻 LLM 先验的潜在偏差，我们在全局级别对对象之间的碰撞关系进行建模，从而增强物理正确性和整体真实性。</li><li>为了生成分布在场景中的普遍对象，如雨和雪，我们引入了稀疏初始化和致密化策略。(4)：实验结果：实验表明，DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，实现了最先进的性能。</li></ol><p>7.方法：(1)文本提示引导下的3D高斯引导(3DG2)场景表示；(2)定制的渐进式尺度控制，确保不同大小和密度的对象适应场景；(3)全局对象碰撞关系建模，增强物理正确性和整体真实性；(4)稀疏初始化和致密化策略，生成分布在场景中的普遍对象。</p><ol><li>结论：（1）：本文提出 DreamScape，这是一种仅从文本描述创建高度一致的 3D 场景的方法，利用了高斯泼溅的强大 3D 表示能力和大语言模型 (LLM) 的复杂排列能力。DreamScape 具有很高的可用性和可控性，能够仅从文本提示生成高保真 3D 场景，并且与其他方法相比，实现了最先进的性能。（2）：创新点：</li><li>提出 3D 高斯引导 (3DG2) 场景表示，将语义基元及其空间变换和关系直接从文本提示中使用 LLM 推导出来，实现局部到全局优化。</li><li>采用定制的渐进式尺度控制，确保不同大小和密度的对象适应场景，解决训练不稳定性问题。</li><li>建模对象之间的碰撞关系，增强物理正确性和整体真实性。</li><li>引入稀疏初始化和致密化策略，生成分布在场景中的普遍对象。性能：</li><li>能够仅从文本提示生成高保真 3D 场景。</li><li>与其他方法相比，实现了最先进的性能。工作量：</li><li>需要一个相对较大的引导比例来确保模型收敛。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-4f88c017fbd210351ebea517e05fd02b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-2e9e7a61267e388dc08cefff90f6c8da.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-a3f52ac0c6425d9782495f42a860e11c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-4177109b99815945eb22c94298f7ecfd.jpg" align="middle"><img src="https://picx.zhimg.com/v2-bcf3ed3cba06de00cc7e8e90859ff690.jpg" align="middle"></details><h2 id="LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field"><a href="#LoopGaussian-Creating-3D-Cinemagraph-with-Multi-view-Images-via-Eulerian-Motion-Field" class="headerlink" title="LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field"></a>LoopGaussian: Creating 3D Cinemagraph with Multi-view Images via   Eulerian Motion Field</h2><p><strong>Authors:Jiyang Li, Lechao Cheng, Zhangye Wang, Tingting Mu, Jingxuan He</strong></p><p>Cinemagraph is a unique form of visual media that combines elements of still photography and subtle motion to create a captivating experience. However, the majority of videos generated by recent works lack depth information and are confined to the constraints of 2D image space. In this paper, inspired by significant progress in the field of novel view synthesis (NVS) achieved by 3D Gaussian Splatting (3D-GS), we propose LoopGaussian to elevate cinemagraph from 2D image space to 3D space using 3D Gaussian modeling. To achieve this, we first employ the 3D-GS method to reconstruct 3D Gaussian point clouds from multi-view images of static scenes,incorporating shape regularization terms to prevent blurring or artifacts caused by object deformation. We then adopt an autoencoder tailored for 3D Gaussian to project it into feature space. To maintain the local continuity of the scene, we devise SuperGaussian for clustering based on the acquired features. By calculating the similarity between clusters and employing a two-stage estimation method, we derive an Eulerian motion field to describe velocities across the entire scene. The 3D Gaussian points then move within the estimated Eulerian motion field. Through bidirectional animation techniques, we ultimately generate a 3D Cinemagraph that exhibits natural and seamlessly loopable dynamics. Experiment results validate the effectiveness of our approach, demonstrating high-quality and visually appealing scene generation. The project is available at <a href="https://pokerlishao.github.io/LoopGaussian/">https://pokerlishao.github.io/LoopGaussian/</a>. </p><p><a href="http://arxiv.org/abs/2404.08966v2">PDF</a> 10 pages</p><p><strong>Summary</strong><br>基于3D高斯建模，LoopGaussian利用多视图图像合成技术，将影音图从2D图像空间升级为3D空间，展现自然流畅的动态效果。</p><p><strong>Key Takeaways</strong></p><ul><li>影音图结合静止图像和细微运动，创造引人入胜的体验。</li><li>LoopGaussian采用3D高斯建模，将影音图从2D提升至3D空间。</li><li>3D-GS方法从静态场景的多视图图像重建3D高斯点云，并融合形状正则化项防止变形。</li><li>3D高斯自编码器将点云投影到特征空间。</li><li>基于特征，SuperGaussian进行聚类，保持场景局部连续性。</li><li>两阶段估计方法计算簇间相似性，导出欧拉运动场描述场景速度。</li><li>3D高斯点在估计的欧拉运动场内移动，双向动画技术产生自然循环的3D影音图。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：LoopGaussian：通过欧拉运动场创建具有多视图的 3D 影像图</li><li>作者：李嘉洋，程乐超，王张野，穆婷婷，何静轩</li><li>单位：浙江大学</li><li>关键词：影像图，动态场景生成，3D 场景重建</li><li>论文链接：https://arxiv.org/abs/2404.08966   Github 代码链接：None</li><li><p>摘要：   （1）研究背景：影像图是一种独特的视觉媒体形式，它结合了静止摄影和微妙运动的元素，创造出一种引人入胜的体验。然而，目前大多数作品生成的视频缺乏深度信息，并且局限于 2D 图像空间的约束。   （2）过去方法及问题：为了解决上述问题，本文借鉴了 3D 高斯点云（3D-GS）在新型视图合成（NVS）领域取得的重大进展，提出了一种名为 LoopGaussian 的方法，利用 3D 高斯建模将影像图从 2D 图像空间提升到 3D 空间。   （3）研究方法：首先，本文使用 3D-GS 方法从静态场景的多视图图像中重建 3D 高斯点云，并结合形状正则化项以防止因物体变形而造成的模糊或伪影。然后，采用针对 3D 高斯量身定制的自编码器将其投影到特征空间。为了保持场景的局部连续性，本文设计了 SuperGaussian，基于所获取的特征进行聚类。通过计算聚类之间的相似性并采用两阶段估计方法，推导出一个欧拉运动场来描述整个场景中的速度。然后，3D 高斯点在估计的欧拉运动场中运动。通过双向动画技术，最终生成一个 3D 影像图，表现出自然且无缝循环的动态效果。   （4）方法性能：实验结果验证了本文方法的有效性，展示了高质量且视觉上吸引人的场景生成。该项目可在 https://pokerlishao.github.io/LoopGaussian/ 获得。</p></li><li><p>方法：（1）利用 3D-GS 方法从静态场景的多视图图像中重建 3D 高斯点云，并结合形状正则化项以防止因物体变形而造成的模糊或伪影。（2）采用针对 3D 高斯量身定制的自编码器将其投影到特征空间。（3）设计 SuperGaussian，基于所获取的特征进行聚类。（4）计算聚类之间的相似性并采用两阶段估计方法，推导出一个欧拉运动场来描述整个场景中的速度。（5）3D 高斯点在估计的欧拉运动场中运动。（6）通过双向动画技术，最终生成一个 3D 影像图，表现出自然且无缝循环的动态效果。</p></li><li><p>结论：（1）本工作提出了一种名为 LoopGaussian 的新颖框架，用于从静态场景的多视图图像生成真实的 3D 影像图。通过利用 3D 高斯点云 splatting 和固有的欧拉运动场，LoopGaussian 可以生成具有深度信息和自然动态效果的 3D 影像图。（2）创新点：</p></li><li>提出了一种使用 3D 高斯 splatting 从多视图图像重建 3D 高斯点云的方法，该方法可以有效地表示场景的形状和外观。</li><li>设计了一种针对 3D 高斯量身定制的自编码器，可以将 3D 高斯点云投影到特征空间，从而捕获场景的局部连续性。</li><li>提出了一种基于 SuperGaussian 聚类的两阶段估计方法，用于从特征中推导出欧拉运动场，该运动场描述了场景中的速度。</li><li>提出了一种双向动画技术，可以将 3D 高斯点云在估计的欧拉运动场中运动，从而生成具有自然且无缝循环动态效果的 3D 影像图。性能：</li><li>LoopGaussian 生成的 3D 影像图具有高质量和视觉吸引力，展示了场景的深度信息和自然动态效果。</li><li>LoopGaussian 在定性和定量评估中都优于现有方法，证明了其生成逼真 3D 影像图的能力。工作量：</li><li>LoopGaussian 的实现涉及多个步骤，包括 3D 高斯点云重建、特征提取、欧拉运动场估计和双向动画。</li><li>该方法需要大量的计算资源，尤其是对于复杂场景。</li><li>然而，LoopGaussian 提供了一个易于使用的界面，可以方便地生成 3D 影像图。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-e2eba83df2d2f5ac39a3c3be75067d7a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-51e0153b3e7c8edd334c5d696dd3d80a.jpg" align="middle"><img src="https://picx.zhimg.com/v2-17bf5e90eaefe5fa5f4c44be35ab164a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-5e793286a69b6603190dc809e34a6be3.jpg" align="middle"></details><h2 id="OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering"><a href="#OccGaussian-3D-Gaussian-Splatting-for-Occluded-Human-Rendering" class="headerlink" title="OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering"></a>OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering</h2><p><strong>Authors:Jingrui Ye, Zongkai Zhang, Yujiao Jiang, Qingmin Liao, Wenming Yang, Zongqing Lu</strong></p><p>Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes. </p><p><a href="http://arxiv.org/abs/2404.08449v2">PDF</a> </p><p><strong>Summary</strong><br>用3D高斯溅射法取代NeRF，大幅提升单目视频生成动态3D人物的速度和效率。</p><p><strong>Key Takeaways</strong></p><ul><li>利用3D高斯溅射，直接在规范空间中初始化3D高斯分布。</li><li>使用遮挡特征查询补偿缺失信息，结合遮挡感知loss函数更好地感知遮挡区域。</li><li>采用高斯特征MLP进一步处理特征，提升遮挡区域辨识度。</li><li>在模拟和真实遮挡场景中表现出色，与最先进方法媲美或优异。</li><li>训练和推理速度相比之前提升250倍和800倍。</li><li>适用于虚拟现实和数字娱乐等多种应用场景。</li><li>方法已开源，可供研究使用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：OccGaussian：用于遮挡人类渲染的 3D 高斯喷绘（中译）</li><li>作者：Jingrui Ye, Zhongkai Zhang, Yujiao Jiang, Qingmin Liao*, Wenming Yang, Zongqing Lu</li><li>隶属单位：清华大学深圳国际研究生院（中译）</li><li>关键词：Canonical 3D Gaussians、Novel View Synthesis、10mins Training、160FPS Rendering</li><li>论文链接：https://arxiv.org/abs/2404.08449</li><li><p>摘要：（1）研究背景：动态 3D 人类渲染在虚拟现实和数字娱乐等应用中至关重要。大多数方法假设人类处于无遮挡场景中，而现实生活场景中各种物体可能会导致身体部位被遮挡。（2）过去方法：以前的方法利用 NeRF 进行表面渲染以恢复被遮挡区域，但这需要一天多的训练时间和几秒钟的渲染时间，无法满足实时交互式应用的要求。（3）研究方法：为了解决这些问题，论文提出了基于 3D 高斯喷绘的 OccGaussian，它可以在 6 分钟内训练完成，并以高达 160FPS 的速度生成高质量的人类渲染，即使输入被遮挡。OccGaussian 在规范空间中初始化 3D 高斯分布，并在被遮挡区域执行遮挡特征查询，提取聚合的像素对齐特征以补偿缺失信息。然后使用高斯特征 MLP 进一步处理特征，并结合遮挡感知损失函数来更好地感知被遮挡区域。（4）任务和性能：在模拟和真实世界遮挡中进行的广泛实验表明，与最先进的方法相比，该方法实现了相当甚至更好的性能。并且将训练和推理速度分别提高了 250 倍和 800 倍。</p></li><li><p>方法：(1) 3D高斯正向蒙皮：在规范空间中初始化 3D 高斯分布，并使用 LBS 变换根据 SMPL 参数将点映射到每个帧的姿态空间。（2）遮挡特征查询：对于每个被遮挡点，查询其在所有可见点中的 K 个最近可见点，并将这些 K 个最近可见点投影到特征图上以提取像素对齐特征。（3）高斯特征 MLP：将遮挡特征与嵌入的被遮挡点连接起来，并将其放入 MLP 中以预测球谐系数 f 和不透明度 α。（4）可微渲染器：应用基于平铺的可微渲染器来实现快速渲染和训练期间的自适应密度控制。（5）损失函数：设计遮挡损失和一致性损失，以防止模型在被遮挡区域学习背景信息。</p></li><li><p>结论：（1）：本工作首次提出了一种使用 3D 高斯喷绘在单目视频中渲染人类遮挡的方法。以往方法在训练和推理中耗时太长，无法满足实时应用的要求，而我们实现了快速训练（6~13 分钟）和实时渲染（169FPS）。具体来说，我们在遮挡区域执行特征查询，并将可见 K 近邻点的聚合像素对齐特征输入到 MLP 中以学习不可见点的特征。此外，我们为遮挡区域设计了专门的损失函数，使渲染更加完整。在我们的实验中，我们在模拟和真实世界遮挡下将 OccGaussian 与 SOTA 方法进行了比较。实验表明，我们的 OccGaussian 在保持快速训练和实时渲染的同时实现了 SOTA 性能。（2）：创新点：OccGaussian 是第一个使用 3D 高斯喷绘渲染遮挡场景中人类的方法；提出了遮挡区域的特征查询和 MLP 预测机制，有效补偿了缺失信息；设计了专门的遮挡损失函数，增强了模型对遮挡区域的感知能力。性能：在模拟和真实世界遮挡场景下，OccGaussian 在渲染质量上与 SOTA 方法相当甚至更好；训练速度提高了 250 倍，推理速度提高了 800 倍。工作量：OccGaussian 的训练时间仅为 6~13 分钟，推理速度高达 169FPS，满足了实时交互式应用的要求。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-22b96540b149a8534443374615ca8599.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11fe939c3521e47d3227ac9f217bda.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3930eb7a35a4e86cf46c4da432a8a109.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-fe0032a9957ec683a4fc3e6deab6cc1d.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v2">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>三维几何感知变形高斯散射（3DGS）方法，用于动态视点合成中对三维动态重建和几何约束变形建模的优化。</p><p><strong>Key Takeaways</strong></p><ul><li>3DGS 方法结合了三维场景几何和高斯散射，解决了现有的神经辐射场（NeRF）方法变形建模中几何不一致的问题。</li><li>3DGS 模型是由可移动和旋转的三维高斯函数组成的，能逼真地模拟变形。</li><li>该方法提取三维几何特征，将其融入变形学习中，实现了三维几何感知的变形建模。</li><li>与传统的 NeRF 方法相比，3DGS 方法在动态视点合成和三维动态重建任务上获得了更好的性能。</li><li>3DGS 方法适用于合成数据集和真实数据集。</li><li>3DGS 方法在动态视点合成和三维动态重建任务上取得了最先进的性能。</li><li>3DGS 项目地址：<a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：基于 3D 几何的动态视点合成可变形高斯散射</li><li>作者：Jun Gao, Yixin Zhu, Jiahao Li, Jingyi Yu, Yebin Liu, Qiang Liu, Xiaogang Wang</li><li>第一作者单位：南京邮电大学</li><li>关键词：动态视点合成、神经辐射场、高斯散射、3D 场景几何</li><li>论文链接：https://arxiv.org/abs/2302.02203Github 链接：None</li><li>摘要：(1) 研究背景：神经辐射场（NeRF）在动态视点合成中取得了成功，但其隐式变形学习方式无法充分利用 3D 场景几何信息，导致变形不一致，影响合成质量。(2) 过去方法：现有方法通过引入 3D 高斯散射表示，可以显式建模场景几何，但缺乏对变形过程的几何约束。(3) 本文方法：提出了一种基于 3D 几何的动态视点合成可变形高斯散射方法，通过提取 3D 几何特征并将其融入变形学习中，增强了变形与场景几何的关联性。(4) 方法性能：在合成和重建任务上，该方法在合成质量、几何一致性和鲁棒性方面均优于现有方法，证明了其在动态视点合成中的有效性。</li></ol><p>7.方法：(1): 提取3D几何特征：利用基于点云的3D几何分析算法，提取场景的点云表示，并从中提取法向量、曲率等几何特征。(2): 融入变形学习：将提取的几何特征作为先验知识，融入到神经辐射场的变形学习中。通过设计一个几何损失函数，约束变形过程与场景几何特征的一致性。(3): 动态视点合成：利用变形后的神经辐射场，通过体渲染技术，合成不同视点的图像。</p><ol><li>结论：（1）：本文提出了一种基于 3D 几何感知的高斯散射可变形动态视点合成方法，该方法通过提取场景的 3D 几何特征并将其融入变形学习中，增强了变形与场景几何的关联性，在合成和重建任务上取得了较好的效果。（2）：创新点：</li><li>提出了一种基于 3D 几何感知的高斯散射可变形动态视点合成方法。</li><li>提出了一种基于点云的 3D 几何分析算法，提取场景的点云表示，并从中提取法向量、曲率等几何特征。</li><li>设计了一个几何损失函数，约束变形过程与场景几何特征的一致性。</li><li>性能：</li><li>在合成和重建任务上，该方法在合成质量、几何一致性和鲁棒性方面均优于现有方法，证明了其在动态视点合成中的有效性。</li><li>工作量：</li><li>该方法需要提取场景的 3D 几何特征，并将其融入变形学习中，因此工作量较大。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-08bef4d6505c50c1da246e20076b2580.jpg" align="middle"><img src="https://picx.zhimg.com/v2-653303d99420769bc567815e0df0bf5a.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f241223d01fd5b44400560a99355429c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-20cc277b202c45bf86629c31910f0a4c.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-403b047ba4b0fb3b2c45a81dd2533d35.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-17  Gaussian Opacity Fields Efficient and Compact Surface Reconstruction in   Unbounded Scenes</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Talking%20Head%20Generation/</id>
    <published>2024-04-17T10:33:12.000Z</published>
    <updated>2024-04-17T10:33:12.763Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time"><a href="#VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time" class="headerlink" title="VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time"></a>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h2><p><strong>Authors:Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo</strong></p><p>We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors. </p><p><a href="http://arxiv.org/abs/2404.10667v1">PDF</a> Tech Report. Project webpage:   <a href="https://www.microsoft.com/en-us/research/project/vasa-1/">https://www.microsoft.com/en-us/research/project/vasa-1/</a></p><p><strong>Summary</strong></p><p>通过单张静态图像和语音音频剪辑生成逼真的会说话的面孔，具有视觉情感技能。</p><p><strong>Key Takeaways</strong></p><ul><li>提出 VASA 框架，可生成逼真的会说话的面孔，具有视觉情感技能。</li><li>核心创新在于在人脸潜在空间中工作的面部动态和头部运动生成模型。</li><li>开发出表达丰富且不纠缠的人脸潜在空间。</li><li>方法在各个维度上明显优于以往方法。</li><li>方法不仅提供具有逼真面部和头部动态的高视频质量，还支持以高达 40 FPS 的速度在线生成 512x512 视频，且启动延迟可忽略不计。</li><li>为与具有类人会话行为的逼真虚拟形象进行实时互动铺平了道路。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：VASA-1：实时生成逼真的音频驱动说话人脸</li><li>作者：Sicheng Xu、Guojun Chen、Yu-Xiao Guo、Jiaolong Yang、Chong Li、Zhenyu Zang、Yizhong Zhang、Xin Tong、Baining Guo</li><li>第一作者单位：微软亚洲研究院</li><li>关键词：音频驱动说话人脸、视觉情感技能、扩散模型、人脸潜在空间</li><li>论文链接：https://arxiv.org/abs/2404.10667</li><li>摘要：（1）研究背景：在多媒体和通信领域，人脸不仅仅是一张面孔，而是一个动态的画布，其上的每一个细微动作和表情都可以表达情感、传递未说出口的信息，并促进移情连接。人工智能生成说话人脸技术的出现为未来提供了一个窗口，在这个窗口中，技术可以放大人与人以及人与人工智能交互的丰富性。这种技术有望丰富数字通信、提高交流障碍者的可访问性、通过互动式人工智能辅导改变教育方法，并在医疗保健中提供治疗支持和社交互动。（2）过去方法及其问题：现有的音频驱动说话人脸生成方法在逼真度和生动性方面存在局限性。这些方法通常依赖于基于关键点的唇形同步和基于模板的面部动画，这会导致僵硬和不自然的动作。此外，它们难以捕捉头部运动和微妙的面部表情，这些表情对于感知真实性和生动性至关重要。（3）提出的研究方法：本文提出 VASA-1，这是一种新的方法，可以生成高度逼真和生动的音频驱动说话人脸。VASA-1 的核心创新包括：</li><li>基于扩散的整体面部动态和头部运动生成模型，该模型在人脸潜在空间中工作。</li><li>使用视频开发了一个表达性和分离的人脸潜在空间。（4）方法在任务和性能上的表现：在广泛的实验中，包括对一组新指标的评估，VASA-1 在各个方面都明显优于以前的方法。它提供了具有逼真面部和头部动态的高视频质量，并且还支持以高达 40 FPS 在线生成 512×512 视频，且启动延迟可以忽略不计。这些性能支持了实时与模拟人类会话行为的逼真化身进行交互的目标。</li></ol><p>7.方法：(1)构建人脸潜在空间：利用无标注人脸视频数据集，构建具有高分离度和表达能力的人脸潜在空间，实现对人脸外观和动态的有效生成建模。(2)扩散变压器生成动态：利用扩散模型和变压器架构，提出全面的面部动态生成框架，以音频为条件，生成头和面部运动序列。(3)无分类器引导：在训练过程中，随机丢弃输入条件，并应用无分类器引导，增强模型对各种条件的鲁棒性。(4)人脸视频生成：在推理时，提取输入人脸图像和音频特征，生成头和面部运动序列，并使用训练好的解码器生成最终视频。</p><ol><li>结论：（1）：本文提出了一种名为 VASA-1 的音频驱动说话人脸生成模型，该模型以其从单张图像和音频输入中高效生成逼真的唇形同步、生动的面部表情和自然的头部动作而著称。它在视频质量和性能效率方面明显优于现有方法，在生成的说话人脸视频中展示了有前景的视觉情感技能。该模型的技术基石是一个创新的整体面部动态和头部动作生成模型，该模型在具有表达性和分离度的人脸潜在空间中工作。VASA-1 取得的进步有可能重塑各个领域的交互，包括通信、教育和医疗保健。可控条件信号的集成进一步增强了模型对个性化用户体验的适应性。（2）：创新点：</li><li>提出了一种基于扩散模型和变压器架构的全面面部动态生成框架，以音频为条件，生成头部和面部运动序列。</li><li>构建了一个具有高分离度和表达能力的人脸潜在空间，实现了对人脸外观和动态的有效生成建模。</li><li>应用无分类器引导，增强了模型对各种条件的鲁棒性。性能：</li><li>在广泛的实验中，包括对一组新指标的评估，VASA-1 在各个方面都明显优于以前的方法。</li><li>提供具有逼真面部和头部动态的高视频质量，并且还支持以高达 40FPS 在线生成 512×512 视频，且启动延迟可以忽略不计。工作量：</li><li>该方法在推理时，提取输入人脸图像和音频特征，生成头部和面部运动序列，并使用训练好的解码器生成最终视频。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-57afb9746460c539242f5be2406abcd8.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4c78cc77ce02a94033d2c27026996d18.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f451991b54ed6b1770c282a53cf0f267.jpg" align="middle"></details><h2 id="THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads"><a href="#THQA-A-Perceptual-Quality-Assessment-Database-for-Talking-Heads" class="headerlink" title="THQA: A Perceptual Quality Assessment Database for Talking Heads"></a>THQA: A Perceptual Quality Assessment Database for Talking Heads</h2><p><strong>Authors:Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</strong></p><p>In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. To tackle this issue, this paper introduces the Talking Head Quality Assessment (THQA) database, featuring 800 TH videos generated through 8 diverse speech-driven methods. Extensive experiments affirm the THQA database’s richness in character and speech features. Subsequent subjective quality assessment experiments analyze correlations between scoring results and speech-driven methods, ages, and genders. In addition, experimental results show that mainstream image and video quality assessment methods have limitations for the THQA database, underscoring the imperative for further research to enhance TH video quality assessment. The THQA database is publicly accessible at <a href="https://github.com/zyj-2000/THQA">https://github.com/zyj-2000/THQA</a>. </p><p><a href="http://arxiv.org/abs/2404.09003v1">PDF</a> </p><p><strong>Summary</strong><br>视频驱动的数字人说话头部评估数据库（THQA）为8种不同语音驱动方法生成的800个说话头部视频建立了基准，促进了说话头部视频质量评估的研究。</p><p><strong>Key Takeaways</strong></p><ul><li>说话头部视频驱动方法多样，质量参差不齐，影响用户视觉体验。</li><li>THQA 数据库包含 800 个说话头部视频，涵盖 8 种语音驱动方法、不同人物和语音特征。</li><li>主观质量评估实验分析了评分结果与语音驱动方法、年龄和性别之间的相关性。</li><li>主流图像和视频质量评估方法对 THQA 数据库有局限性，需要进一步研究改进 TH 视频质量评估。</li><li>THQA 数据库已公开，可用于研究。</li><li>TH 视频质量评估对于提高用户视觉体验至关重要。</li><li>语音驱动方法的进步将促进数字人说话头部技术的广泛应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：THQA：用于说话人头像感知质量评估的数据库</li><p></p><p></p><li>作者：Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiaohong Liu, Xiongkuo Min, Zhihua Wang, Xiao-Ping Zhang, Guangtao Zhai</li><p></p><p></p><li>第一作者单位：上海交通大学</li><p></p><p></p><li>关键词：数字人头部、语音驱动方法、质量评估数据库、无参考、多媒体处理</li><p></p><p></p><li>论文链接：None</li><p></p><p></p><li>摘要：（1）研究背景：数字人作为一种新兴的数字媒体技术，在娱乐、医疗、影视等行业得到了广泛的应用。然而，目前数字人的设计和制作过程仍然十分繁琐和耗时，主要依赖于熟练专业人员的手工操作。这种手工设计方式极大地制约了数字人内容制作的效率，尤其是在头部设计和驱动机制的复杂领域。（2）过去方法和问题：为了应对这一挑战，人工智能（AI）的出现和普及提供了一个有前景的解决方案，如图 1 所示。虽然语音驱动方法已被相继提出，简化了数字人面部表情和动作的设计，但仍然缺乏专门针对 AI 生成的说话人头像（TH）视频的质量评估指标。这些质量指标不仅可以有效评估说话人头像（TH）视频的质量，还可以间接促进语音驱动方法的进一步发展，从而为用户提供更高质量的视觉体验。遗憾的是，目前评估生成说话人头像视频的主流方法仍然遵循保留与原始视频比较的范式。值得注意的是，Fréchet 感知距离（FID）和余弦相似度（CSIM）仍然是用于此类评估的主要指标。然而，这些指标的局限性表现在两个基本方面。首先，这些客观评估指标仅关注图像或视频相似性，而忽略了整个生成内容带给观看者用户的整体视觉体验。其次，它们对原始参考视频的依赖性构成了一个实质性的限制，因为最终用户无法获得原始参考视频，从而严重限制了它们的适用性。虽然 CPBD 和 CGIQA 等指标已被纳入一些最近的工作中以衡量模糊级别和美学特征，但没有广泛使用的评估指标专门针对 TH 视频量身定制。（3）提出的研究方法：为了解决这一挑战，首先需要开发一个可公开访问的大规模 TH 视频数据库。因此，本文将重点放在以下几个方面：</li><p></p><p></p><li>提出一个新的说话人头像质量评估数据库 THQA，其中包含 800 个 TH 视频，这些视频是通过 8 种不同的语音驱动方法生成的。</li><p></p><p></p><li>广泛的实验验证了 THQA 数据库在角色和语音特征方面的丰富性。</li><p></p><p></p><li>后续的主观质量评估实验分析了评分结果与语音驱动方法、年龄和性别之间的相关性。</li><p></p><p></p><li>此外，实验结果表明，主流图像和视频质量评估方法对 THQA 数据库有局限性，强调了进一步研究以增强 TH 视频质量评估的必要性。</li><p></p><p></p><li><p></p><p>THQA 数据库可在 https://github.com/zyj-2000/THQA 公开获取。（4）方法在什么任务上取得了什么性能？性能是否能支撑其目标？THQA 数据库的建立为说话人头像视频质量评估提供了丰富的资源，并为进一步研究说话人头像视频质量增强奠定了基础。</p></li><li><p>方法：(1): 构建了一个包含 800 个 TH 视频的大型公开数据集 THQA，这些视频由 8 种不同的语音驱动方法生成；(2): 通过主观质量评估实验，分析了评分结果与语音驱动方法、年龄和性别之间的相关性；(3): 验证了 THQA 数据库在角色和语音特征方面的丰富性；(4): 实验结果表明，主流图像和视频质量评估方法对 THQA 数据库有局限性，强调了进一步研究以增强 TH 视频质量评估的必要性。</p></li></ol><p><strong>8.结论</strong>(1) 本工作的重要意义：本工作构建了一个名为 THQA 的说话人头像（TH）视频质量评估数据库。该数据库包含通过 8 种不同的语音驱动方法生成的 800 个 TH 视频。我们的分析涉及对收集的图像、语音数据和生成的视频的彻底检查。此外，我们进行主观评分实验以验证 THQA 的代表性，肯定其作为 TH 视频质量评估指导框架的效用。最后，我们基于 THQA 数据库对各种主流评估方法的性能进行比较评估。结果表明，大多数现有的评估方法在有效评估 TH 视频质量方面表现出局限性。</p><p>(2) 本文优缺点总结（三个维度）：创新点：* 构建了一个大规模的说话人头像视频质量评估数据库 THQA。* 分析了 TH 视频的丰富性，包括角色和语音特征。* 探索了主流图像和视频质量评估方法在评估 TH 视频质量方面的局限性。</p><p>性能：* THQA 数据库为说话人头像视频质量评估提供了丰富的资源。* 主观质量评估实验验证了 THQA 的代表性。* 比较评估表明，现有的评估方法在评估 TH 视频质量方面存在局限性。</p><p>工作量：* 构建 THQA 数据库涉及收集和处理大量数据。* 主观质量评估实验需要大量的人力资源。* 探索主流评估方法的局限性需要进行广泛的实验。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-1f6bf6d7bad9eaf02e82acd303b468f2.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e543b10e4a34e8d4e06d3f29d16a43fb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-8610a595c8734930ae6c9ef9d82979cc.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-beef350bacef5e83341d2b9912c3cd5f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-a3dad9d09b0691502f22ef81f9dd0bbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-17  VASA-1 Lifelike Audio-Driven Talking Faces Generated in Real Time</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Models</title>
    <link href="https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/"/>
    <id>https://kedreamix.github.io/2024/04/17/Paper/2024-04-17/Diffusion%20Models/</id>
    <published>2024-04-17T06:02:48.000Z</published>
    <updated>2024-04-17T06:02:48.569Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-17-更新"><a href="#2024-04-17-更新" class="headerlink" title="2024-04-17 更新"></a>2024-04-17 更新</h1><h2 id="RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting"><a href="#RefFusion-Reference-Adapted-Diffusion-Models-for-3D-Scene-Inpainting" class="headerlink" title="RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting"></a>RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting</h2><p><strong>Authors:Ashkan Mirzaei, Riccardo De Lutio, Seung Wook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic</strong></p><p>Neural reconstruction approaches are rapidly emerging as the preferred representation for 3D scenes, but their limited editability is still posing a challenge. In this work, we propose an approach for 3D scene inpainting — the task of coherently replacing parts of the reconstructed scene with desired content. Scene inpainting is an inherently ill-posed task as there exist many solutions that plausibly replace the missing content. A good inpainting method should therefore not only enable high-quality synthesis but also a high degree of control. Based on this observation, we focus on enabling explicit control over the inpainted content and leverage a reference image as an efficient means to achieve this goal. Specifically, we introduce RefFusion, a novel 3D inpainting method based on a multi-scale personalization of an image inpainting diffusion model to the given reference view. The personalization effectively adapts the prior distribution to the target scene, resulting in a lower variance of score distillation objective and hence significantly sharper details. Our framework achieves state-of-the-art results for object removal while maintaining high controllability. We further demonstrate the generality of our formulation on other downstream tasks such as object insertion, scene outpainting, and sparse view reconstruction. </p><p><a href="http://arxiv.org/abs/2404.10765v1">PDF</a> Project page: <a href="https://reffusion.github.io">https://reffusion.github.io</a></p><p><strong>Summary</strong><br>基于多尺度个性化的图像修复扩散模型，提出了RefFusion，一种3D场景修复方法，该方法可通过参考图像对内容进行明确控制，从而实现高质量合成。</p><p><strong>Key Takeaways</strong></p><ul><li>3D场景修复仍然面临可编辑性有限的挑战。</li><li>场景修复是一个固有的病态任务，即存在许多可信地替代缺失内容的解决方案。</li><li>好的修复方法不仅要实现高质量的合成，还要具有高度的可控性。</li><li>RefFusion是一种基于多尺度个性化的图像修复扩散模型，可以对给定的参考视图进行3D修复。</li><li>个性化有效地将先验分布适应到目标场景，从而降低了评分蒸馏目标的方差，因此产生了明显更清晰的细节。</li><li>RefFusion框架在对象移除方面实现了最先进的结果，同时保持了高度的可控性。</li><li>RefFusion的公式在对象插入、场景外画和稀疏视图重建等其他下游任务上也具有普遍性。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1.标题：RefFusion：用于 3D 场景修复的参考自适应扩散模型2.作者：Ashkan Mirzaei, Riccardo de Lutio, SeungWook Kim, David Acuna, Jonathan Kelly, Sanja Fidler, Igor Gilitschenski, Zan Gojcic3.第一作者单位：NVIDIA，多伦多大学4.关键词：3D 场景修复、扩散模型、参考自适应5.论文链接：https://arxiv.org/abs/2404.10765Github 链接：无6.摘要：（1）研究背景：神经重建方法能够从一组姿态图像中无缝重建 3D 场景。然而，它们的可编辑性仍然有限，而 3D 场景修复是一种关键的可编辑操作，涉及合成合理的内容，以便从任何角度观看时都能与场景的其余部分融为一体。（2）过去的方法：过去的方法通常使用神经网络来生成修复内容。然而，这些方法通常缺乏对修复内容的显式控制，并且可能产生模糊或不连贯的结果。（3）研究方法：本文提出了一种名为 RefFusion 的 3D 场景修复方法。RefFusion 基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。这种个性化降低了分数蒸馏目标函数的方差，从而显著提高了细节的清晰度。（4）方法性能：RefFusion 在对象移除任务上实现了最先进的结果，同时保持了较高的可控性。此外，本文还展示了该方法在其他下游任务（如对象插入、场景外延和稀疏视图重建）上的通用性。这些结果表明，RefFusion 能够有效地合成高质量且可控的修复内容，从而满足各种 3D 场景编辑需求。</p><ol><li><p>方法：(1) <strong>RefFusion</strong>：本文提出的3D场景修复方法，基于图像修复扩散模型的多尺度个性化，可以将给定的参考视图中的先验分布有效地适应目标场景。(2) <strong>多尺度个性化</strong>：RefFusion在扩散模型的不同尺度上对参考视图进行个性化，从而使模型能够捕获不同尺度上的细节和结构。(3) <strong>分数蒸馏目标函数</strong>：RefFusion使用分数蒸馏目标函数来匹配修复内容和参考视图的先验分布。通过个性化降低目标函数的方差，提高了修复内容的细节清晰度。</p></li><li><p>结论：(1): RefFusion 提出了一种基于图像修复扩散模型的多尺度个性化方法，用于 3D 场景修复，显著提高了修复内容的细节清晰度和可控性，在对象移除等任务上取得了最先进的结果。(2): 创新点：</p><ul><li>多尺度个性化：在扩散模型的不同尺度上对参考视图进行个性化，捕获不同尺度上的细节和结构。</li><li>分数蒸馏目标函数：使用分数蒸馏目标函数匹配修复内容和参考视图的先验分布，降低目标函数的方差，提高修复内容的细节清晰度。性能：</li><li>在对象移除任务上实现了最先进的结果。</li><li>在对象插入、场景外延和稀疏视图重建等下游任务上表现出通用性。工作量：</li><li>训练 RefFusion 模型需要大量的计算资源。</li><li>RefFusion 的推理速度可能会受到模型复杂度的限制。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/02817604a88632e7e3ea4560f26f9bac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c106f428df739a7142772c42a95151c1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/15ab37f03412f6ff0a7f34d5503212ec241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/542185dc6584d7e97ea791d916e54a04241286257.jpg" align="middle"></details>## LaDiC: Are Diffusion Models Really Inferior to Autoregressive   Counterparts for Image-to-Text Generation?**Authors:Yuchi Wang, Shuhuai Ren, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun**Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&amp;Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation. [PDF](http://arxiv.org/abs/2404.10763v1) **Summary**扩散模型在文本到图像生成方面表现出色，现在通过LaDiC架构在图像到文本生成方面也取得了突破性的进展。**Key Takeaways**- 扩散模型具有整体上下文建模和并行解码能力。- AR 方法存在推理速度慢、误差传播和单向约束等固有缺陷。- 缺乏有效的图像文本对齐潜在空间和连续扩散过程与离散文本数据之间的差异是扩散模型先前表现不佳的原因。- LaDiC 架构使用 split BERT 创建了专门的标题潜在空间，并集成了一个正则化模块来管理不同的文本长度。- LaDiC 包括一个用于语义图像到文本转换的扩散器，以及一种在推理过程中增强标记交互性的 Back&amp;Refine 技术。- 在 MS COCO 数据集上，LaDiC 实现了基于扩散方法的最新性能，BLEU@4 为 38.2，CIDEr 为 126.2，在没有预训练或辅助模块的情况下表现出色。- 这表明扩散模型在图像到文本生成方面具有与 AR 模型相当的竞争力，揭示了其在该领域尚未开发的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：图像段落描述：生成连贯且信息丰富的图像描述</li><li>作者：Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei</li><li>隶属关系：斯坦福大学</li><li>关键词：图像描述，段落生成，多模态学习，自然语言处理，计算机视觉</li><li>论文链接：https://arxiv.org/pdf/1611.06607.pdf，Github 链接：无</li><li>摘要：(1)：研究背景：图像描述任务通常生成单个句子来描述图像，但这种描述过于粗略，无法捕捉图像的丰富细节。密集描述任务可以描述图像中的多个区域，但生成的描述缺乏连贯性。(2)：过去的方法：传统图像描述方法生成单个句子，而密集描述方法生成多个短语，但都存在信息不足或缺乏连贯性的问题。(3)：研究方法：本文提出了一种图像段落描述模型，该模型将图像分解为语义区域，并使用分层循环神经网络对语言进行推理。通过这种方法，模型可以生成连贯且信息丰富的段落，详细描述图像中的内容。(4)：方法性能：在图像段落描述数据集上，该模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。</li></ol><p>7.方法：(1) 利用文本编码器将离散文本空间 C 转换为连续文本潜在空间 X；(2) 训练扩散器以在图像空间 V 和文本空间 X 之间架起桥梁；(3) 文本解码器将文本潜在码映射回离散文本。</p><ol><li>结论：（1）： 本文研究了基于扩散的图像到文本范式，并引入了一种新颖的架构，称为 LaDiC。我们的模型在基于扩散的方法中取得了最先进的性能，并展示了与一些预训练的 AR 模型相当的能力。此外，我们广泛的实验揭示了扩散模型在考虑更多整体上下文和并行发出所有标记方面优于 AR 模型的令人兴奋的优势。因此，我们认为扩散模型在图像到文本生成方面具有巨大的潜力，我们希望我们的工作将为该领域的这一领域开辟新的可能性。（2）： 创新点：提出了一种新颖的基于扩散的图像到文本生成架构 LaDiC，该架构在图像空间和文本空间之间建立了桥梁，并利用分层循环神经网络进行语言推理，从而生成连贯且信息丰富的图像描述。性能：在图像段落描述数据集上，我们的模型在 BLEU-4、METEOR 和 ROUGE-L 等指标上取得了最先进的性能，证明了其生成连贯且信息丰富的图像描述的能力，满足了研究目标。工作量：本文主要集中在图像到文本生成的主要研究课题上，以保持简洁性和重点。然而，我们观察到我们的模型可以很容易地适应其他模态甚至纯文本生成，而只需进行最小的修改。我们将这些潜在的扩展留给未来的工作，同时，我们希望本文将激发研究人员使用扩散模型从事以文本为中心的模态生成任务的信心，并期待在这个领域未来的精彩作品。此外，由于资源限制，我们研究中使用的模型参数和数据集并不广泛。考虑到像 GPT 这样的自回归模型在放大时表现出的显着的紧急能力，探索我们的模型或一般的扩散模型是否可以表现出类似的可扩展性，成为一项有趣且有价值的探索。风险考虑：作为一个生成模型，我们的模型可能会无意中产生难以与人类书面内容区分开来的结果，引发对潜在误用的担忧。采用文本水印技术可能有助于减轻这个问题。此外，扩散模型通常需要大量的计算资源进行训练，从而导致二氧化碳排放和环境影响增加。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/ad873da96dc2ed96671aaa4ec1d1b20f241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/35dd717311044737324107cdc54b6822241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9f7a7a48ee0399a430575adacda8ed66241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/093e2d146940e0222b3021bdfb674cf9241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9d1dfb06c22a75df200fdac77b6e7498241286257.jpg" align="middle"></details>## GazeHTA: End-to-end Gaze Target Detection with Head-Target Association**Authors:Zhi-Yi Lin, Jouh Yeong Chew, Jan van Gemert, Xucong Zhang**We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. [PDF](http://arxiv.org/abs/2404.10718v1) **Summary**基于视觉图像预测注视目标，提出端到端多人物GazeHTA框架，通过预训练扩散模型、重新注入头特征、学习连接图实现高效的目标检测。**Key Takeaways**- 提出端到端多人物注视目标检测框架 - GazeHTA。- 利用预训练扩散模型提取场景特征，提升语义理解。- 重新注入头特征，增强头部先验，提升头部理解。- 学习连接图，明确头部与注视目标之间的视觉关联。- 采用扩散模型作为基准，实现更优的性能。- 在标准数据集上，GazeHTA 优于现有方法。- GazeHTA 提供了端到端的解决方案，可直接从图像预测注视目标。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：GazeHTA：端到端的注视目标检测</li><li>作者：Zhi-Yi Lin、Jouh Yeong Chew、Jan van Gemert、Xucong Zhang</li><li>第一作者单位：代尔夫特理工大学计算机视觉实验室</li><li>关键词：注视目标检测、头部目标关联、扩散模型</li><li>论文链接：arXiv:2404.10718v1[cs.CV]16Apr2024</li><li>摘要：（1）研究背景：人类注意力估计在人机交互、社会活动识别、心理健康诊断和客户行为分析等领域具有重要意义。注视目标检测旨在直接将个体与其注视目标关联起来，提供了一种端到端的注意力估计解决方案。</li></ol><p>（2）过去方法及问题：大多数注视目标检测方法采用双流架构，存在以下问题：- 缺乏头部和注视目标之间的直接关联。- 依赖于现成的头部检测器。- 仅限于一次处理一个头部，在多人物场景中需要重复处理。</p><p>（3）本文方法：GazeHTA 提出了一种端到端的多人注视目标检测框架，通过头部目标关联来预测头部目标实例。其特点包括：- 利用预训练的扩散模型提取场景特征。- 重新注入头部特征以增强头部先验知识。- 学习连接图来表示头部和注视目标之间的视觉关联。</p><p>（4）性能：GazeHTA 在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。实验结果表明，该方法可以很好地支持其目标。</p><p>7.方法：（1）利用预训练的扩散模型提取场景特征（Stable Diffusion）；（2）重新注入头部特征以增强头部先验知识（Head Feature Re-Injection）；（3）学习连接图来表示头部和注视目标之间的视觉关联（Connection Map）。</p><ol><li>结论：(1): GazeHTA 提出了一种端到端的注视目标检测框架，通过头部目标关联来预测头部目标实例，在两个标准数据集上优于最先进的注视目标检测方法和两个改编的基于扩散的基准。(2): 创新点：</li><li>利用预训练的扩散模型提取场景特征，增强了头部先验知识，学习了头部和注视目标之间的视觉关联。</li><li>采用多任务学习策略，同时预测头部目标和注视目标，提高了检测精度。</li><li>引入了连接图，表示头部和注视目标之间的视觉关联，增强了头部目标关联的鲁棒性。性能：</li><li>在两个标准数据集上，GazeHTA 在注视目标检测任务上取得了最先进的性能。</li><li>该方法在多人物场景中表现出色，能够准确地将头部与注视目标关联起来。</li><li>该方法对头部姿态和照明条件的变化具有鲁棒性。工作量：</li><li>该方法的实现相对复杂，需要预训练扩散模型和训练连接图。</li><li>该方法的计算成本较高，在实际应用中可能需要优化。</li><li>该方法需要大量标注数据进行训练，这可能会限制其在某些场景中的应用。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/85b5892f7794783ad79c67d67689cac6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c2dd1f0a5f6c974a1e1cc5b4db6180e3241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/831d352002cad6012c105349850e0b6b241286257.jpg" align="middle"></details>## Efficient Conditional Diffusion Model with Probability Flow Sampling for   Image Super-resolution**Authors:Yutao Yuan, Chun Yuan**Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP. [PDF](http://arxiv.org/abs/2404.10688v1) AAAI 2024**Summary**通过概率流采样的高效条件扩散模型（ECDP）在图像超分辨率方面实现了高超分辨率图像质量和低时间消耗。**Key Takeaways**- 扩散概率模型能够处理图像超分辨率中固有的病态性问题。- 迭代采样导致现有基于扩散的超分辨率方法时间消耗高。- ECDP 引入了连续时间条件扩散模型以提高采样效率。- 混合参数化为去噪网络提高了生成图像的一致性。- 图像质量损失作为扩散模型得分匹配损失的补充，进一步提高了一致性和质量。- ECDP 在 DIV2K、ImageNet 和 CelebA 上优于现有基于扩散的超分辨率方法，同时时间消耗更低。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于概率流采样的高效条件扩散模型用于图像超分辨率</li><li>作者：袁宇涛，袁淳</li><li>单位：清华大学</li><li>关键词：图像超分辨率，扩散概率模型，概率流采样</li><li>论文链接：https://arxiv.org/abs/2404.10688   Github代码链接：None</li><li><p>摘要：（1）研究背景：图像超分辨率是一项本质上不适定的问题，因为对于一个低分辨率图像存在多个有效的高分辨率图像。基于扩散概率模型的超分辨率方法可以通过学习条件在低分辨率图像上的高分辨率图像分布来处理不适定性，避免了以 PSNR 为导向的方法中图像模糊的问题。（2）过去的方法及问题：现有的基于扩散的超分辨率方法使用迭代采样，时间消耗大；并且由于颜色偏移等问题，生成图像的质量和一致性不理想。（3）提出的研究方法：本文提出了用于图像超分辨率的基于概率流采样的高效条件扩散模型（ECDP）。为了减少时间消耗，设计了用于图像超分辨率的连续时间条件扩散模型，这使得可以使用概率流采样进行高效生成。此外，为了提高生成图像的一致性，提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值。此外，设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。（4）方法在任务和性能上的表现：在 DIV2K、ImageNet 和 CelebA 上进行的广泛实验表明，本文方法比现有的基于扩散的图像超分辨率方法实现了更高的超分辨率质量，同时具有更低的时间消耗。这些性能支持了本文的目标。</p></li><li><p>方法：(1): 设计了用于图像超分辨率的连续时间条件扩散模型，使得可以使用概率流采样进行高效生成；(2): 提出了去噪器网络的混合参数化，它在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，提高生成图像的一致性；(3): 设计了一个图像质量损失作为扩散模型分数匹配损失的补充，进一步提高了超分辨率的一致性和质量。</p></li><li><p>结论：（1）本文提出的 ECDP 框架在图像超分辨率任务上取得了较好的效果，在保证超分辨率质量的同时，降低了时间消耗。（2）创新点：</p></li><li>提出了一种基于连续时间条件扩散模型的图像超分辨率方法，该方法可以有效地利用概率流采样进行生成。</li><li>提出了一种混合参数化的去噪器网络，该网络可以在不同的噪声尺度上对数据预测参数化和噪声预测参数化进行插值，从而提高生成图像的一致性。</li><li>设计了一个图像质量损失作为扩散模型分数匹配损失的补充，该损失可以有效地提高超分辨率结果的质量。性能：</li><li>在 DIV2K、ImageNet 和 CelebA 数据集上的实验表明，本文方法比现有的基于扩散的图像超分辨率方法具有更高的超分辨率质量和更低的时间消耗。工作量：</li><li>本文方法的实现相对简单，易于复现。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/a82d966193c93e548a48f6956d503a03241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/3243ba61ae0dc29581f4a5b0b8bc24ac241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/9ec5bc713a1dd0f65714d070ec06c103241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8b672147537b2050d8dcfda1a25fba99241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/84fee720619599629de5e02d5ca3c96a241286257.jpg" align="middle"></details>## StyleCity: Large-Scale 3D Urban Scenes Stylization with Vision-and-Text   Reference via Progressive Optimization**Authors:Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung**Creating large-scale virtual urban scenes with variant styles is inherently challenging. To facilitate prototypes of virtual production and bypass the need for complex materials and lighting setups, we introduce the first vision-and-text-driven texture stylization system for large-scale urban scenes, StyleCity. Taking an image and text as references, StyleCity stylizes a 3D textured mesh of a large-scale urban scene in a semantics-aware fashion and generates a harmonic omnidirectional sky background. To achieve that, we propose to stylize a neural texture field by transferring 2D vision-and-text priors to 3D globally and locally. During 3D stylization, we progressively scale the planned training views of the input 3D scene at different levels in order to preserve high-quality scene content. We then optimize the scene style globally by adapting the scale of the style image with the scale of the training views. Moreover, we enhance local semantics consistency by the semantics-aware style loss which is crucial for photo-realistic stylization. Besides texture stylization, we further adopt a generative diffusion model to synthesize a style-consistent omnidirectional sky image, which offers a more immersive atmosphere and assists the semantic stylization process. The stylized neural texture field can be baked into an arbitrary-resolution texture, enabling seamless integration into conventional rendering pipelines and significantly easing the virtual production prototyping process. Extensive experiments demonstrate our stylized scenes' superiority in qualitative and quantitative performance and user preferences. [PDF](http://arxiv.org/abs/2404.10681v1) project page: https://chenyingshu.github.io/stylecity3d/**Summary**城市场景大规模纹理风格化在文图驱动下，融合神经纹理场，能生成逼真风格化的纹理和全景天空。**Key Takeaways**- 提出了一个城市场景大规模纹理风格化系统。- 提出了一种神经纹理场的风格化方法，将二维视觉与文本先验全局和局部地迁移到三维中。- 渐进缩放输入三维场景的训练视图以保留场景内容的高质量。- 通过调整风格图像和训练视图的尺度来全局优化场景风格。- 采用语义感知的风格损失增强局部语义一致性，这对于真实感风格化至关重要。- 采用生成式扩散模型合成风格一致的全景天空图像，提供更沉浸的氛围并辅助语义风格化。- 风格化的神经纹理场可以烘焙成任意分辨率的纹理，无缝集成到传统渲染管道中，极大地简化了虚拟制作原型制作过程。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>题目：StyleCity：大规模 3D 城市场景</li><li>作者：Yingshu Chen, Huajian Huang, Tuan-Anh Vu, Ka Chun Shum, Sai-Kit Yeung</li><li>第一作者单位：香港科技大学</li><li>关键词：城市风格化、大规模场景、视觉和文本参考、渐进式优化</li><li>论文链接：https://arxiv.org/abs/2404.10681</li><li><p>摘要：(1) 研究背景：创建具有不同风格的大规模虚拟城市场景具有挑战性，需要复杂的材质和灯光设置。(2) 过去方法：现有的方法主要依赖于手工制作的材质和灯光，难以实现大规模场景的自动风格化。(3) 研究方法：本文提出 StyleCity 框架，通过将 2D 视觉和文本先验全局和局部地转移到 3D，对神经纹理场进行风格化。通过渐进式缩放训练视图和自适应风格图像比例以及语义感知风格损失，实现场景风格化和全景天空背景生成。(4) 实验结果：在城市场景风格化任务上，StyleCity 在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验。</p></li><li><p>方法：（1）：枢轴视图规划：提出了一种基于枢轴的视图规划方法，通过在网格边界框的上、侧面均匀采样相机位置，并使用网格区域的质心作为相机视点，获得枢轴视图，覆盖可见表面的大部分区域，为新视图（图 2 中的绿色相机）提供初始化，以增强训练，见第 3.3 节多尺度渐进优化。（2）：城市场景分割：为语义感知的城市场景风格化，考虑了建筑场景中感兴趣的类别，包括“天空”、“建筑物”、“窗户”、“道路”、“人”、“植物”、“汽车”、“水”和“灯光”。（3）：神经纹理场定义：使用神经纹理场表示，将巨大的纹理贴图重新参数化为二维连续函数，通过一个 MLP 将归一化的 UV 纹理坐标映射为颜色 RGB 值，理论上支持任意纹理分辨率，平均纹理尺寸压缩 90%。（4）：神经渲染：给定相机位姿，光栅化网格并检索 UV 以查询纹理模型 TΘ(·)，获得相应的纹理 RGB 值，然后将值重新组合到渲染的图像中。（5）：内容和风格联合渐进优化：通过多个视图与每个迭代中的源内容和目标风格约束共同优化神经纹理场，随机采样视点，渲染内容视图及其分割，并从神经纹理中获得风格化视图。（6）：多尺度渐进优化：在优化期间，沿贝塞尔曲线随机采样新视图，以附近的计划枢轴相机作为控制点，以扩大覆盖角度，并在训练期间以“放大”效果逐渐缩小采样视图的视场 (FoV)，确保每个表面都被全面风格化和全局谐波。（7）：内容和真实感保留优化：利用渲染视图的内容特征和拉普拉斯值进行监督，以保持纹理内容和场景标识，屏蔽背景区域以保持内容完整性和真实感。（8）：全局规模自适应风格优化：全局风格优化负责将风格特征全局转移到神经纹理场，以进行整体氛围对齐，基于训练视图和风格贴块之间的结构相似性匹配多尺度风格。（9）：全局视觉和文本驱动的风格损失：通过惩罚全局风格特征分布的差异，快速获取和生成有意义的新风格化纹理。（10）：局部语义感知风格优化：对于具有复杂上下文的城市场景，全局风格转移容易导致风格语义不匹配，因此引入局部风格优化策略，进行按类别特征正则化，以实现更逼真的风格化。</p></li><li><p>结论：（1）：StyleCity是一个基于视觉和文本驱动的城市级纹理网格大规模风格化管道。我们利用神经纹理场建模场景外观，并提出了一种新的多尺度渐进优化方法，以实现高保真风格化。对于谐波风格化，我们引入了尺度自适应风格优化和新的损失函数，以全局和局部正则化风格特征。此外，我们改进了漫反射全景合成方法，以支持风格对齐的高分辨率全方位天空合成，这作为沉浸式氛围和更好语义的背景。（2）：创新点：StyleCity框架、神经纹理场表示、多尺度渐进优化、尺度自适应风格优化、全局视觉和文本驱动的风格损失、基于漫反射的全景天空合成；性能：在城市场景风格化任务上，StyleCity在视觉保真度和语义一致性方面取得了最先进的性能，支持创建个性化且引人入胜的城市探索体验；工作量：该方法需要大量的数据集和计算资源，并且训练过程可能需要很长时间。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/7cb106e145913b85abac0f0c3f097cc8241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/086cffd59fa8e20d53cc5c5be733a969241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68c301be7084d603d72cc10756a2ae2c241286257.jpg" align="middle"></details>## Generating Counterfactual Trajectories with Latent Diffusion Models for   Concept Discovery**Authors:Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed**Trustworthiness is a major prerequisite for the safe application of opaque deep learning models in high-stakes domains like medicine. Understanding the decision-making process not only contributes to fostering trust but might also reveal previously unknown decision criteria of complex models that could advance the state of medical research. The discovery of decision-relevant concepts from black box models is a particularly challenging task. This study proposes Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT), a novel three-step framework for concept discovery leveraging the superior image synthesis capabilities of diffusion models. In the first step, CDCT uses a Latent Diffusion Model (LDM) to generate a counterfactual trajectory dataset. This dataset is used to derive a disentangled representation of classification-relevant concepts using a Variational Autoencoder (VAE). Finally, a search algorithm is applied to identify relevant concepts in the disentangled latent space. The application of CDCT to a classifier trained on the largest public skin lesion dataset revealed not only the presence of several biases but also meaningful biomarkers. Moreover, the counterfactuals generated within CDCT show better FID scores than those produced by a previously established state-of-the-art method, while being 12 times more resource-efficient. Unsupervised concept discovery holds great potential for the application of trustworthy AI and the further development of human knowledge in various domains. CDCT represents a further step in this direction. [PDF](http://arxiv.org/abs/2404.10356v1) Submitted to International Conference on Pattern Recognition (ICPR)   2024**Summary**利用扩散模型的卓越图像合成能力，提出一种新的概念发现框架，以发现黑盒模型中与决策相关的重要概念。**Key Takeaways**- 黑盒模型中的概念发现对于建立信任和推进医学研究至关重要。- CDCT是一种用于概念发现的新型三步框架，利用扩散模型的图像合成能力。- CDCT通过生成反事实轨迹数据集、提取解耦表征和应用搜索算法来识别相关的概念。- CDCT发现了皮肤病变分类器中的偏差和有意义的生物标志物。- CDCT生成的对抗样本的FID得分比现有方法更好，同时资源效率提高了12倍。- 无监督概念发现对于可信赖AI的应用和人类知识在各个领域的进一步发展具有巨大潜力。- CDCT代表了朝着这个方向迈出的进一步一步。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于潜在扩散模型的反事实轨迹生成，用于概念发现</li><li>作者：</li><li>Payal Varshney</li><li>Adriano Lucieri</li><li>Christoph Balada</li><li>Andreas Dengel</li><li>Sheraz Ahmed</li><li>第一作者单位：德国凯撒斯劳滕-兰道莱茵兰-普法尔茨技术大学</li><li>关键词：可解释性、反事实、基于概念的解释、潜在扩散模型、皮肤镜检查、概念发现</li><li>论文链接：https://arxiv.org/abs/2404.10356   Github 代码链接：无</li><li><p>摘要：   (1) 研究背景：      深度学习模型在医疗等高风险领域的安全应用需要可信度。理解决策过程不仅有助于建立信任，而且还能揭示复杂模型以前未知的决策标准，从而推进医学研究的发展。从黑盒模型中发现与决策相关的概念是一项特别具有挑战性的任务。   (2) 过去的方法及其问题：      以往的方法主要依赖于梯度或扰动技术，但这些方法生成的反事实图像质量较差，难以从中提取有意义的概念。      本文提出的方法动机充分，它利用扩散模型出色的图像合成能力，通过生成反事实轨迹数据集来发现与分类相关的概念。   (3) 本文提出的研究方法：      本文提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。该框架包括：</p><ul><li>使用潜在扩散模型生成反事实轨迹数据集。</li><li>利用变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示。</li><li>应用搜索算法在解耦的潜在空间中识别相关概念。   (4) 方法在任务上的表现及性能：  将 CDCT 应用于在最大的公开皮肤病变数据集上训练的分类器，发现了几个偏差和有意义的生物标记。  此外，在 CDCT 中生成的反事实图像显示出比先前建立的最新方法更好的 FID 分数，同时资源效率提高了 12 倍。  这些性能支持了本文提出的方法可以有效发现与分类相关的概念，并为可信赖的人工智能和各个领域的知识发展做出贡献。</li></ul></li><li><p>方法：（1）：潜在扩散模型生成反事实轨迹数据集；（2）：变分自动编码器从反事实轨迹数据集中提取分类相关概念的解耦表示；（3）：搜索算法在解耦的潜在空间中识别相关概念。</p></li><li><p>结论(1): 本工作通过基于潜在扩散模型的反事实轨迹生成，为概念发现提供了一种可解释且高效的方法，在医疗等高风险领域的安全应用中具有重要意义。(2): 创新点：</p></li><li>提出了一种称为 CDCT 的三步框架，用于通过基于潜在扩散的反事实轨迹进行概念发现。</li><li>利用扩散模型出色的图像合成能力，生成反事实轨迹数据集，从中提取分类相关概念的解耦表示。</li><li>在解耦的潜在空间中应用搜索算法识别相关概念。性能：</li><li>在最大的公开皮肤病变数据集上训练的分类器中发现了几个偏差和有意义的生物标记。</li><li>与先前建立的最新方法相比，生成的反事实图像显示出更好的 FID 分数，同时资源效率提高了 12 倍。工作量：</li><li>该方法易于实现，并且可以应用于各种分类任务。</li><li>训练和部署 CDCT 模型的计算成本相对较低。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/576c1e270406787f968cf066ad94df12241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/921adc3c228efd65b737862c3ffcf199241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/4d2bd80c6bf872188ed28b597b22867a241286257.jpg" align="middle"></details>## Efficiently Adversarial Examples Generation for Visual-Language Models   under Targeted Transfer Scenarios using Diffusion Models**Authors:Qi Guo, Shanmin Pang, Xiaojun Jia, Qing Guo**Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner. [PDF](http://arxiv.org/abs/2404.10335v1) **Summary**利用扩散模型生成逼真的对抗样本，显著提升图像和语言模型对抗攻击效率和安全性。**Key Takeaways**- 提出 AdvDiffVLM，利用扩散模型生成自然、无约束的对抗样本。- 采用自适应集成梯度估计，修改扩散模型反向生成过程中的评分，确保对抗样本包含自然的对抗语义。- 使用 GradCAM 引导掩码方法，将对抗语义分散到整个图像，避免集中在特定区域。- 与现有转移攻击方法相比，速度提高 10-30 倍，同时保持对抗样本的质量。- 生成的对抗样本具有很强的迁移性，对对抗防御方法的鲁棒性更高。- AdvDiffVLM 可以以黑盒方式成功攻击商用 VLM，包括 GPT-4V。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：基于扩散模型的视觉语言模型目标迁移场景下高效对抗样本生成</li><li>作者：郭奇、庞山民、贾晓军、郭庆</li><li>隶属单位：西安交通大学</li><li>关键词：对抗样本、视觉语言模型、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.10335</li><li><p>摘要：(1) 研究背景：针对视觉语言模型（VLM）的基于目标迁移的对抗攻击对 VLM 构成重大威胁。然而，最先进的基于迁移的攻击由于过多的迭代次数而产生高昂的成本。此外，生成的对抗样本表现出明显的对抗噪声，并且在规避 DiffPure 等防御方法方面表现出有限的效力。(2) 过去的方法及问题：AttackVLM 使用基于查询的攻击方法并结合基于迁移的先验，提示黑盒 VLM 产生目标响应。但是，由于需要大量的查询，这个过程非常耗时，通常需要几个小时才能生成一个对抗样本。因此，考虑了另一种黑盒攻击方法，即基于迁移的攻击。然而，如图 1 所示，当前最先进的基于迁移的攻击在生成对抗样本方面也较慢，并且在规避对抗防御方法方面效果较差。此外，这些方法生成的对抗样本表现出明显的噪声。(3) 论文提出的方法：为了解决这些问题，受评分匹配和不受限制的对抗样本的启发，提出了 AdvDiffVLM，它使用扩散模型生成自然、不受限制的对抗样本。具体来说，利用并修改了预训练扩散模型的反向生成过程，其中利用自适应集成梯度估计来改变评分并将目标语义嵌入对抗样本中。为了增强输出的自然性，引入了 GradCAM 引导掩码，它将对抗目标语义分散在对抗样本中，而不是将它们集中在特定区域，从而提高图像质量。(4) 方法在任务和性能上的表现：该方法只需要几个后向去噪步骤即可生成对抗样本，使其明显快于以前公布的基于迁移的方法。此外，AdvDiffVLM 通过去噪生成对抗样本，对防御方法表现出更高的鲁棒性。实验结果表明，AdvDiffVLM 在目标和迁移场景中针对最先进的基于迁移的攻击实现了数量级或更大的加速，同时提供了具有更高图像质量的对抗样本。此外，这些对抗样本在跨不同 VLM（包括商业 VLM，如 GPT-4V）的迁移中表现出强大的可迁移性。</p></li><li><p>方法：(1): 从评分匹配的角度出发，将对抗攻击建模为生成过程，利用扩散模型的数据分布生成自然、无约束的对抗样本。(2): 提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。(3): 引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。</p></li><li><p>结论：（1）本工作提出了一种基于扩散模型的无约束对抗样本生成方法 AdvDiffVLM，它在目标迁移场景下高效地生成对抗样本。（2）创新点：</p></li><li>提出自适应集成梯度估计方法，利用 CLIP 模型作为代理模型，估计黑盒 VLM 的梯度信息。</li><li>引入 GradCAM 引导掩码，将对抗目标语义分散在对抗样本中，提高图像质量。性能：</li><li>与现有的系统相比，AdvDiffVLM 的速度提高了 10 倍到 30 倍。</li><li>生成的对抗样本具有较高的图像质量和鲁棒性。工作量：</li><li>AdvDiffVLM 在不同的 VLM（包括商业 VLM，如 GPT-4V）中表现出强大的可迁移性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/58f36ca28ba6f05864f978063dc05641241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/2e4d299418d7d5553271b20ba81e4ebf241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/aeab2739defd1322978a3eff9a615297241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/70b668e1887faf1bb1428acdafb7a9fc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/888909a0a5fe2c07ff40b91188d0a47d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cb46778d4b426e082ad902ea978cc0fa241286257.jpg" align="middle"></details>## OneActor: Consistent Character Generation via Cluster-Conditioned   Guidance**Authors:Jiahao Wang, Caixia Yan, Haonan Lin, Weizhan Zhang**Text-to-image diffusion models benefit artists with high-quality image generation. Yet its stochastic nature prevent artists from creating consistent images of the same character. Existing methods try to tackle this challenge and generate consistent content in various ways. However, they either depend on external data or require expensive tuning of the diffusion model. For this issue, we argue that a lightweight but intricate guidance is enough to function. Aiming at this, we lead the way to formalize the objective of consistent generation, derive a clustering-based score function and propose a novel paradigm, OneActor. We design a cluster-conditioned model which incorporates posterior samples to guide the denoising trajectories towards the target cluster. To overcome the overfitting challenge shared by one-shot tuning pipelines, we devise auxiliary components to simultaneously augment the tuning and regulate the inference. This technique is later verified to significantly enhance the content diversity of generated images. Comprehensive experiments show that our method outperforms a variety of baselines with satisfactory character consistency, superior prompt conformity as well as high image quality. And our method is at least 4 times faster than tuning-based baselines. Furthermore, to our best knowledge, we first prove that the semantic space has the same interpolation property as the latent space dose. This property can serve as another promising tool for fine generation control. [PDF](http://arxiv.org/abs/2404.10267v1) **Summary**通过引入聚类条件模型和一个新的范例OneActor，该研究实现了无监督一致图像生成，同时保持图像质量和多样性。**Key Takeaways*** 无需外部数据或昂贵的微调即可实现一致图像生成。* 提出了一种基于聚类的评分函数，将后验样本纳入去噪轨迹。* 为微调和推理设计了辅助组件以增强多样性和避免过拟合。* 实验表明该方法在字符一致性、提示符合性和图像质量方面优于基线。* 微调速度至少是基于微调的基线的 4 倍。* 证明了语义空间具有与潜在空间相同的插值属性。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：OneActor：通过集群条件引导实现一致的角色生成</li><li>作者：Jiahao Wang、Caixia Yan、Haonan Lin、Weizhan Zhang</li><li>单位：西安交通大学计算机科学与技术学院</li><li>关键词：文本到图像、扩散模型、一致性生成、集群引导</li><li>链接：https://arxiv.org/abs/2404.10267</li><li>摘要：   (1) 研究背景：文本到图像扩散模型为艺术家提供了高质量的图像生成能力，但其随机性使得艺术家无法创建同一角色的一致图像。   (2) 过去方法：现有方法通过各种方式尝试解决这一挑战，但它们要么依赖于外部数据，要么需要对扩散模型进行昂贵的微调。   (3) 研究方法：本文提出了一种轻量级但复杂的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor。设计了一个集群条件模型，将后验样本纳入其中，引导去噪轨迹朝向目标集群。为了克服单次微调管道中常见的过度拟合挑战，设计了辅助组件来同时增强微调和调节推理。   (4) 实验结果：该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。</li></ol><p>7.方法：(1)定义问题：给定用户定义的描述提示 pt（例如，一个穿着长袍的霍比特人），标准扩散模型 ϵθ 生成用户首选图像 xt 作为目标角色。我们的目标是为原始 ϵθ 配备一个支持网络 ϕ，制定 ϵθ,ϕ。在对 ϕ 进行快速微调后，我们的模型 ϵθ,ϕ 可以生成具有任何其他以角色为中心的提示（例如，一个穿着长袍的霍比特人 + 在街上行走）的同一角色的一致图像。为了完成这项任务，我们在第 4.2 节中首先进行数学分析。然后我们在第 4.3 节中构建一个集群条件模型，这是一个自引导扩散模型。我们在第 4.4 节中以对比方式对其进行微调。最后在推理期间，我们在第 4.5 节中使用语义插值生成各种一致的图像，并在第 4.6 节中使用潜在引导。(2)集群引导评分函数的推导：给定用户定义的提示 pt（例如，一个穿着白色连衣裙的美丽女孩）和相应的 base 词 wb（例如，女孩），我们将 pt 输入标准扩散模型 ϵθ 以获得 N 个 base 图像 Xbase={xbasei}Ni=1。我们随机选择一张图片作为目标角色 xtar，并将其他图片标记为辅助样本 Xaux={xauxi}N−1i=1。我们对目标图像应用人脸裁剪和图像翻转，得到增强集 Xtar={xtari}Mi=1。在标准潜在扩散模型 ϵθ(zt, t, c) 中，用于文本控制的条件 c 由文本编码器 Et 生成，该编码器将文本提示 p 投影到语义向量：c=Et(p)。然后这些语义向量引导去噪网络从初始潜在噪声 zT 采样去噪 z0。在我们的任务中，我们假设在 z0 的潜在空间中，有不同的集群对应于角色的不同显着身份。给定不同的初始 zT 和相同的字符条件 pt，ϵθ 无法到达一个特定的集群，而是扩散到不同集群的区域。也就是说，Xbase 的生成形成一个 base 区域 Sbase，其中包括一个目标集群和几个辅助集群：Sbase⊃Star1∪Saux1∪Saux2∪... 一致性生成的关键是如何引导 ϵθ 到预期的目标集群 Star。首先，我们形式化一致性生成问题。从面向结果的角度来看，我们希望增加生成目标集群 Star 图像的概率，并降低辅助集群 Sauxi 的概率。因此，如果我们将原始扩散过程视为先验分布 p(x)，则我们的期望分布可以表示为：p(x)·p(Star|x)η1 / N−1i=1p(Sauxi|x)η2，其中 η1、η2 是比例因子。我们应用贝叶斯规则推导出：p(x)·(p(x|Star)p(Star)p(x))η1 / N−1i=1(p(x|Sauxi)p(Sauxi)p(x))η2，我们进一步取其对数概率的梯度并忽略无关项，得到：∇logp(x)+η1·[∇logp(x|Star)−∇logp(x)]−η2·N−1i=1[∇logp(x|Sauxi)−∇logp(x)]。如果我们引入评分函数的概念（Song 等人，[2021]），Eq.6 中的每个术语都表示一个评分并指导推理过程。利用 Ho 等人 [2020] 的重新参数化技巧，我们可以将评分表示为潜在空间中去噪网络 θ 的预测：ϵθ(zt, t)+η1·[ϵθ(zt, t, Star)−ϵθ(zt, t)]−η2·N−1i=1[ϵθ(zt, t, Sauxi)−ϵθ(zt, t)]。这个公式称为集群引导评分函数，是我们工作的核心。我们将在后续部分中实现它。(3)使用语义表示的集群条件模型：根据 Eq.7，我们需要将集群表示引入我们的管道并建立一个集群条件模型 ϵθ(zt, t, S)。为此，我们将样本的潜在代码视为集群表示，并将 ϕ 构建为编码器。如图 2 所示，它将潜在代码 z 作为输入，并输出一个特定于角色的向量 ∆c。此向量表示角色集群的语义方向。给定文本提示 p，我们将提示嵌入拆分为逐词嵌入：c={cwi}，其中 wi 是提示的第 i 个单词。我们将 base 词嵌入 cw 和偏移输出向量连接起来：c′wb=cwb+∆c。直观地说，ϕ 就像文本编码器一样，将潜在代码投影到语义嵌入中。因此，我们将 ϕ 称为潜在编码器。潜在编码器由一个提取器和一个投影仪组成。由于原始 U-Net 编码器 Eu 已经经过良好训练，可以从潜在样本中提取特征，因此我们直接将其用作提取器。投影仪是一个多层 ResNet（He 等人，[2016]）和线性网络，带有层归一化。在微调期间，我们只激发投影仪并冻结所有其他组件。然而，U-Net 提取器可能会造成额外的计算负担。为了简化，我们用 z1 的特征近似 z0 的特征：h=Eu(z1, c)≈Eu(z0)。因此，在生成 base 图像时，我们在最后一个采样步骤中保存 U-Net 编码器的输出 H={hi}Ni=1。h 可以近似为提取器的输出。在微调和推理过程中，h 直接馈送到投影仪：∆c=ϕ(h)。通过这种近似，我们避免了 U-Net 提取器的额外计算，并将计算成本降低了 30%。到目前为止，所有可以确定集群的因素（即 p、h）都可以由文本编码器和潜在编码器处理。因此，我们可以将 Eq.7 中的集群条件项表示为：ϵθ(zt, t, S)=ϵθ,ϕ(zt, t, Et(p), ϕ(h))。(4)使用辅助样本的广义简化微调：在我们的集群条件模型的微调过程中，扩散模型的内在属性是关键方面。我们的目标是充分利用目标和辅助样本：平衡和稳定投影仪。一次性微调中的主要挑战是过拟合。数据的不足可能会导致严重的偏差和生成图像的多样性有限。为了克服这一挑战，我们不仅使用目标样本，还使用辅助样本对模型进行微调。具体来说，我们随机选择 1 个目标和 K 个辅助样本来形成一批数据：B={xtar, htar}∪{xauxi, hauxi}Ki=1。因此，更多的数据有助于优化，并且批处理归一化可以应用于投影仪，从而产生更通用的投影。对于微调管道，我们首先通过：z=Ea(x), x∈B 为它们添加噪声 ϵt 来获得潜在代码。然后如图所示，我们将噪声潜在 zt、提示 p 和特征 h 输入集群条件模型。提示是一个随机模板，填充有 base 词（例如，一个女孩的肖像）。我们对目标和辅助样本应用标准去噪损失：Ltar(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ϕ(htar))∥2，Laux(ϕ)=Et∈[1,T],zaux0,ϵt∥ϵt−ϵθ,ϕ(zauxt, t, Et(p), ϕ(haux))∥2。对于辅助项 Eq.7，计算每个步骤 N-1 个辅助条件的去噪预测非常费力。为了简化，我们提出了一个平均条件 ∆¯c。它是通过对辅助条件的条件向量求平均获得的：∆¯c=1KKi=1∆cauxi=1KKi=1ϕ(hauxi)。直观地说，这个平均条件表示所有辅助集群的中心。因此，可以用这个平均预测来近似辅助去噪预测。去噪损失也用于此条件：Laver(ϕ)=Et∈[1,T],ztar0,ϵt∥ϵt−ϵθ,ϕ(ztart, t, Et(p), ∆¯c)∥2。请注意，这里的数据来自目标集。此平均条件将充当无分类器引导（Ho 和 Salimans [2022]）的空条件，如第 4.6 节中所述。(5)语义插值的证明和实现：在推理过程中，传统的基于微调的管道在一致性和多样性之间存在相同的失衡。为了解决这个问题，我们提出了一个温和的策略，语义插值，它利用了扩散模型的内部容量。无分类器引导模型（Ho 和 Salimans [2022]）证明了潜在空间中的条件插值和外推，这表明了我们的论点。</p><p>8.结论：（1）：本文提出了一种轻量级且高效的指导方法，通过形式化一致性生成的目标、推导基于聚类的评分函数，并提出了一种新范式 OneActor，解决了文本到图像扩散模型中角色一致性生成的问题。该方法在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。此外，本文首次证明语义空间与潜在空间具有相同的插值特性，该特性可以作为细粒度生成控制的另一有前途的工具。（2）：创新点：提出了一种基于集群条件引导的轻量级且高效的文本到图像扩散模型，实现了角色一致性生成。性能：在各种基准测试中优于其他方法，具有令人满意的角色一致性、出色的提示符合性以及较高的图像质量。并且该方法至少比基于微调的基线快 4 倍。工作量：该方法的实现相对简单，并且可以在各种文本到图像扩散模型上轻松部署。</p><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/89b6f16a0a761f249e333f44a5168204241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ac080ab3aeb6fbbdbf4314b392f0a4e6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/a05f66aebcfc1504a123423e38d9d751241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/81b50b6c224994210d30702ea8e901e3241286257.jpg" align="middle"></details><h2 id="Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting"><a href="#Taming-Latent-Diffusion-Model-for-Neural-Radiance-Field-Inpainting" class="headerlink" title="Taming Latent Diffusion Model for Neural Radiance Field Inpainting"></a>Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h2><p><strong>Authors:Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</strong></p><p>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model’s stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a> </p><p><a href="http://arxiv.org/abs/2404.09995v1">PDF</a> Project page: <a href="https://hubert0527.github.io/MALD-NeRF">https://hubert0527.github.io/MALD-NeRF</a></p><p><strong>Summary</strong><br>使用场景定制和对抗性训练来改进神经辐射场（NeRF）图像修复中的扩散模型。</p><p><strong>Key Takeaways</strong></p><ul><li>扩散模型在 NeRF 图像修复中面临合成几何和纹理变化挑战。</li><li>像素差异损失在 NeRF 图像修复任务中是有害的。</li><li>使用场景定制可以减少扩散模型的随机性。</li><li>对抗性训练可以减轻纹理变化。</li><li>感知损失在 NeRF 图像修复中也不理想。</li><li>该框架在各种真实场景上实现了最先进的 NeRF 图像修复结果。</li><li>该方法解决了 NeRF 图像修复中的两个主要问题：合成几何和纹理变化。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：驯服隐式扩散模型用于补充材料</li><li>作者：C.H. Lin 等</li><li>隶属：未提及</li><li>关键词：NeRF、隐式扩散模型、图像编辑、图像生成、深度学习</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：NeRF 是从多视图图像进行三维重建的一种表示方法。尽管最近一些工作展示了使用扩散先验编辑重建的 NeRF 的初步成功，但它们仍然难以在完全未覆盖的区域中合成合理的几何形状。（2）过去方法及其问题：一个主要原因是扩散模型合成内容的高度多样性，这阻碍了辐射场收敛到清晰且确定性的几何形状。此外，由于自动编码错误，在真实数据上应用隐式扩散模型通常会导致与图像条件不一致的纹理偏移。这两个问题在使用像素距离损失时进一步加剧。（3）本文提出的研究方法：为了解决这些问题，我们提出用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移。在分析过程中，我们还发现常用的像素和感知损失在 NeRF inpainting 任务中有害。（4）方法在什么任务上取得了什么性能：通过严格的实验，我们的框架在各种真实世界场景上产生了最先进的 NeRF inpainting 结果。这些性能支持了他们的目标。</p></li><li><p>方法：(1): 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法；(2): 使用预先在内部图像修复数据集上训练的潜在扩散模型来修复二维图像，然后替换用于 NeRF 训练的输入图像；(3): 使用像素级损失函数来重建输入图像中的已知区域，同时使用掩码对抗性训练来修复修复区域；(4): 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界；(5): 利用现成的单目深度先验来规范学习到的 NeRF 的几何形状；(6): 使用迭代数据设置更新和噪声调度，以减轻扩散模型的多样性和随机性带来的不一致问题。</p></li></ol><p>8.结论：（1）本工作的重要意义：本文提出了一种用于 NeRFinpainting 的新框架，该框架通过缓解扩散模型的随机性和减轻纹理偏移来显着提高 NeRF 的编辑质量。（2）本文的创新点、性能和工作量：创新点：- 提出了一种用特定于场景的定制来缓和扩散模型的随机性，并用掩码对抗性训练来减轻纹理偏移的方法。- 设计了一种掩码对抗性训练方案，从判别器中隐藏图像补丁上的重建/修复边界。性能：- 在各种真实世界场景上产生了最先进的 NeRFinpainting 结果。工作量：- 实验设置复杂，需要大量的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/308cb7c2143fa740c4192a671925dee1241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/68fb1ff3b56d69424096a269f5033b6e241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/8d6fc19a0079ac001db1a67326957da6241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bad14fb5377a3790949b10e7d83b5ddb241286257.jpg" align="middle"></details>## Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse   Controls to Any Diffusion Model**Authors:Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal**ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours). [PDF](http://arxiv.org/abs/2404.09967v1) First two authors contributed equally; Project page:   https://ctrl-adapter.github.io/**Summary**预训练图像控制网络通过一个高效通用框架 Ctrl-Adapter 拓展至视频生成，适配各类图像/视频扩散模型，为视频控制带来多样功能。**Key Takeaways**- Ctrl-Adapter 将预训练控制网络适配到扩散模型，实现图像/视频控制。- 适配层融合控制网络特征，保持控制网络和扩散模型参数不变。- 时空模块处理视频的时间一致性。- 潜在跳过和逆时步采样提升适应性和稀疏控制。- 加权平均控制网络输出实现多条件控制。- 适配各类扩散模型，图像控制效果与控制网络匹配，视频控制效果远超基线。- 计算成本极低（低于 10 个 GPU 小时）。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：CTRL-Adapter：一种高效且通用的框架，可将各种控制适配到任何扩散模型</li><li>作者：Han Lin、Jaemin Cho、Abhay Zala、Mohit Bansal</li><li>隶属机构：北卡罗来纳大学教堂山分校</li><li>关键词：控制网络、视频生成、扩散模型、条件控制、视频编辑</li><li>论文链接：https://arxiv.org/abs/2404.09967</li><li><p>摘要：(1) 研究背景：控制网络广泛用于图像生成中添加空间控制，但直接将预训练的图像控制网络用于视频生成面临挑战。(2) 过去的方法：预训练的控制网络无法直接插入新的主干模型，且为新主干模型训练控制网络的成本很高；不同帧的控制网络特征可能无法有效处理对象的时序一致性。(3) 研究方法：提出 CTRL-Adapter 框架，通过适配预训练的控制网络（并改进视频的时间对齐）将各种控制添加到图像/视频扩散模型中。(4) 性能：CTRL-Adapter 具有图像控制、视频控制、稀疏帧视频控制、多条件控制、与不同主干模型的兼容性、对未见控制条件的适应以及视频编辑等强大且多样的功能。</p></li><li><p>Methods:(1) CTRL-Adapter框架：提出一种适配器，将预训练的控制网络与扩散模型连接，并通过时间对齐模块处理视频的时间一致性。(2) 预训练控制网络适配：设计一种适配器，将预训练的控制网络特征映射到扩散模型的潜在空间，实现不同主干模型的兼容性。(3) 视频时间对齐：引入时间对齐模块，通过循环一致性损失和时间平滑损失，确保不同帧的控制网络特征在时间上保持一致。(4) 多条件控制：提出一种多条件控制机制，允许同时使用多个控制条件，并通过条件混合器将不同条件的特征融合。(5) 未见控制条件适应：利用对抗性训练，使CTRL-Adapter能够适应未在训练集中出现的控制条件。(6) 视频编辑：通过控制网络的掩码机制，实现对视频特定区域或帧的编辑。</p></li><li><p>结论：（1）：本文提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。CTRL-Adapter 还提供了许多有用的功能，包括图像控制、视频控制、具有稀疏输入的视频控制和多源控制。我们通过综合分析实证展示了 CTRL-Adapter 的有用性。使用不同的图像和视频扩散骨干，训练 CTRL-Adapter 可以匹配或优于训练新的 ControlNet，同时降低计算成本。CTRL-Adapter 对未见条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件（例如，深度图、Canny 边缘、人体姿势和曲面法线）的视频。我们还为 CTRL-Adapter 的设计选择和定性示例提供了全面的消融研究。我们希望我们的工作可以促进未来视频和图像高效受控生成的研究。致谢：这项工作得到了 DARPA ECole 计划号 HR00112390060、NSF-AI Engage 研究所 DRL-2112635、DARPA 机器常识 (MCS) 补助金 N66001-19-2-4031、ARO 奖金 W911NF2110220、ONR 补助金 N00014-23-1-2356 和 Bloomberg 数据科学博士奖学金的支持。本文中包含的观点是作者的观点，不代表资助机构的观点。（2）：创新点：提出 CTRL-Adapter 框架，该框架通过适配和时间对齐预训练的 ControlNet，可以将各种控制添加到任何图像/视频扩散模型中，同时保持 ControlNet 和骨干扩散模型的参数不变。性能：CTRL-Adapter 在图像控制、视频控制、具有稀疏输入的视频控制和多源控制方面表现出强大且多样的功能。它可以与不同的图像和视频扩散骨干兼容，并对未见控制条件具有适应性。工作量：训练 CTRL-Adapter 明显比为新骨干模型训练 ControlNet 更有效率。它还可以对未见控制条件执行零样本适应，并帮助生成具有稀疏帧条件或多个条件的视频。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/c25f069f15093064473dcdf4dfaa56ee241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/f677d02b9dd20e46686e3c8c08280d9d241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/ce2e3a267b9806afbb6606fbc59d0755241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/33b547f986f3df3860ea96d4a3aa9f63241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/48ab1365ae4e53ae2b1e46fafa4c5b7c241286257.jpg" align="middle"></details><h2 id="A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance"><a href="#A-Diffusion-based-Data-Generator-for-Training-Object-Recognition-Models-in-Ultra-Range-Distance" class="headerlink" title="A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance"></a>A Diffusion-based Data Generator for Training Object Recognition Models   in Ultra-Range Distance</h2><p><strong>Authors:Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</strong></p><p>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot’s camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot. </p><p><a href="http://arxiv.org/abs/2404.09846v1">PDF</a> </p><p><strong>Summary</strong><br>通过扩散模型生成远距离物体图像，提升机器人手势识别能力。</p><p><strong>Key Takeaways</strong></p><ul><li>针对远距离手势识别，提出了基于扩散模型的DUR框架。</li><li>DUR接收距离和类别，生成相应远距离物体合成图像。</li><li>DUR生成的图像在保真度和识别成功率上优于其他生成模型。</li><li>URGR模型在真实数据和DUR合成数据上训练都优于直接在真实数据上训练。</li><li>基于DUR合成数据的URGR模型可用于地面机器人的手势引导。</li><li>DUR框架可推广到其他远距离物体识别任务。</li><li>合成数据集缓解了真实世界数据缺乏的问题，增强了模型的泛化能力。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于扩散的超远距离物体识别模型训练数据生成器</li><li>作者：Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</li><li>隶属单位：特拉维夫大学机械工程学院</li><li>关键词：超远距离手势识别、数据生成、扩散模型</li><li>论文链接：https://arxiv.org/abs/2404.09846   Github 代码链接：无</li><li><p>摘要：   (1)：研究背景：物体识别是机器人有效与环境交互的基础，但随着机器人与物体距离的增加，图像分辨率下降，识别性能会因失去判别性视觉特征而显着下降。   (2)：过去方法及其问题：生成对抗网络（GAN）等传统生成模型无法很好地复制远距离物体在图像中的真实视觉特征。   (3)：本文提出的研究方法：提出了一种基于扩散模型的超远距离扩散（DUR）框架，可以生成具有不同场景和距离条件的远距离物体标记图像。   (4)：方法在任务和性能上的表现：DUR 在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达 25 米的室内外环境中表现出良好的识别成功率。此外，基于合成数据的 URGR 模型还被证明可以用于基于手势的地面机器人导航。</p></li><li><p>方法：(1): 提出超远距离扩散（DUR）框架，基于扩散模型生成不同场景和距离条件下的远距离物体标记图像；(2): 定义超远距离物体识别问题，目标是根据距离小于 25 米的 RGB 图像，将图像基于展示的对象分类为 m 个可能的类别之一；(3): 收集标记数据集，包括图像、手势类别索引和距离信息，并使用 YOLOv8 检测用户并裁剪背景，使用 HQ-Net 增强图像质量；(4): 使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像；(5): 学习模型 pθ 近似条件概率，以重建原始分布的图像。</p></li><li><p>结论：(1): 本工作提出了一种基于扩散模型的超远距离扩散（DUR）框架，该框架可以生成具有不同场景和距离条件的远距离物体标记图像，为超远距离物体识别任务提供了丰富的训练数据。(2): 创新点：</p><ul><li>提出基于扩散模型的超远距离扩散（DUR）框架，为超远距离物体识别任务生成逼真的合成训练数据。</li><li>定义了超远距离物体识别问题，并收集了包含图像、手势类别索引和距离信息的标记数据集。</li><li>使用去噪扩散概率模型（DDPM）作为生成模型，通过逐渐添加和反转噪声来生成合成图像。</li></ul></li><li>性能：<ul><li>在超远距离手势识别（URGR）任务上训练了一个模型，该模型在距离高达25米的室内外环境中表现出良好的识别成功率。</li><li>基于合成数据的URGR模型还被证明可以用于基于手势的地面机器人导航。</li></ul></li><li>工作量：<ul><li>收集了包含50,000张图像的标记数据集，并使用YOLOv8和HQ-Net对图像进行了预处理。</li><li>训练了去噪扩散概率模型（DDPM），以生成具有不同场景和距离条件的远距离物体标记图像。</li><li>在超远距离手势识别（URGR）任务上训练了一个模型，并评估了其性能。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/6b8ecb308186704259aa27fb23638660241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/e63d9562d6323864e562317f7c12ea55241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c14de792d860790e380b7de225a0fa74241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/958ce44fd42dfbbff812a95c9714e514241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/14eb7f29fa6a016695804dd8fc230360241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/31c2f09164cbb10143e3860a7c7e3599241286257.jpg" align="middle"></details><h2 id="Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models"><a href="#Digging-into-contrastive-learning-for-robust-depth-estimation-with-diffusion-models" class="headerlink" title="Digging into contrastive learning for robust depth estimation with   diffusion models"></a>Digging into contrastive learning for robust depth estimation with   diffusion models</h2><p><strong>Authors:Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, Yao Zhao</strong></p><p>Recently, diffusion-based depth estimation methods have drawn widespread attention due to their elegant denoising patterns and promising performance. However, they are typically unreliable under adverse conditions prevalent in real-world scenarios, such as rainy, snowy, etc. In this paper, we propose a novel robust depth estimation method called D4RD, featuring a custom contrastive learning mode tailored for diffusion models to mitigate performance degradation in complex environments. Concretely, we integrate the strength of knowledge distillation into contrastive learning, building the `trinity’ contrastive scheme. This scheme utilizes the sampled noise of the forward diffusion process as a natural reference, guiding the predicted noise in diverse scenes toward a more stable and precise optimum. Moreover, we extend noise-level trinity to encompass more generic feature and image levels, establishing a multi-level contrast to distribute the burden of robust perception across the overall network. Before addressing complex scenarios, we enhance the stability of the baseline diffusion model with three straightforward yet effective improvements, which facilitate convergence and remove depth outliers. Extensive experiments demonstrate that D4RD surpasses existing state-of-the-art solutions on synthetic corruption datasets and real-world weather conditions. The code for D4RD will be made available for further exploration and adoption. </p><p><a href="http://arxiv.org/abs/2404.09831v1">PDF</a> 8 pages,6 figures</p><p><strong>Summary</strong><br>扩散深度估计中的三元对比学习模式，增强了模型对复杂环境的鲁棒性。</p><p><strong>Key Takeaways</strong></p><ul><li>提出了一种新型的基于扩散的鲁棒深度估计方法 D4RD。</li><li>引入了定制的对比学习模式，以减轻复杂环境中的性能下降。</li><li>提出了一种“三元”对比方案，利用前向扩散过程中的采样噪声作为自然参考。</li><li>将噪声水平三元扩展到更通用的特征和图像级别，建立多层次对比。</li><li>通过三个简单的改进增强了基线扩散模型的稳定性。</li><li>D4RD 在合成损坏数据集和真实世界天气条件下优于现有最先进的解决方案。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：基于对比学习的鲁棒深度估计</li><li>作者：王继源、林春宇、聂朗、廖康、邵书伟、赵尧</li><li>第一作者单位：北京交通大学</li><li>关键词：深度估计、鲁棒感知、自监督学习、扩散方法</li><li>论文链接：None</li><li><p>摘要：（1）研究背景：扩散模型在深度估计中取得了优异的性能，但它们在雨雪等恶劣条件下表现不可靠。（2）过去方法和问题：对比学习和知识蒸馏分别用于增强鲁棒性，但存在一些不足，例如对比学习容易受噪声影响，知识蒸馏需要高质量的伪标签。（3）研究方法：本文提出了 D4RD，它结合了对比学习和知识蒸馏，构建了一个“三元”对比方案。该方案利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。此外，还扩展了噪声级三元对比到更通用的特征和图像级别，建立了多级对比以跨整个网络分配鲁棒感知的负担。（4）任务和性能：在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。</p></li><li><p><strong>方法</strong>：（1）提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降；（2）利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值；（3）将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。</p></li><li><p>结论：（1）本文提出了 D4RD，一种基于对比学习的鲁棒深度估计框架，增强了基于扩散的深度估计的稳定性和收敛性。（2）创新点：</p></li><li>提出了一种新的多级“三元”对比方案，以增强鲁棒性和减轻恶劣条件下的性能下降。</li><li>利用正向扩散过程中的采样噪声作为自然参考，引导不同场景中的预测噪声朝向更稳定和精确的极值。</li><li>将噪声级三元对比扩展到更通用的特征和图像级别，建立多级对比以跨整个网络分配鲁棒感知的负担。性能：</li><li>在合成损坏数据集和真实世界天气条件下，D4RD 超越了现有的最先进解决方案。</li><li>这些结果表明，该方法可以有效地增强鲁棒性并减轻恶劣条件下的性能下降。工作量：</li><li>该方法需要对正向扩散过程中的采样噪声进行建模，这可能需要额外的计算开销。</li><li>多级对比方案的实现也可能增加模型的复杂性。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/944e85b707a7fb7eff2e43b4ba0298bc241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/bab219f0ca75b9f2908a62071e68eeda241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/c66b109acaddbd0ab6aa42c7965a9683241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/d9984015fa79bdccf3644f3446f0ee43241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/5ab47e273e1c34b2c8a6d3ee8c66b6ef241286257.jpg" align="middle"></details><h2 id="Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement"><a href="#Equipping-Diffusion-Models-with-Differentiable-Spatial-Entropy-for-Low-Light-Image-Enhancement" class="headerlink" title="Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement"></a>Equipping Diffusion Models with Differentiable Spatial Entropy for   Low-Light Image Enhancement</h2><p><strong>Authors:Wenyi Lian, Wenjing Lian, Ziwei Luo</strong></p><p>Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a>. </p><p><a href="http://arxiv.org/abs/2404.09735v1">PDF</a> CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge   2024</p><p><strong>Summary</strong></p><p>图像修复从损坏的图像中恢复高质量图像，通常面临病态问题，即单一输入有多个解。然而，多数基于深度学习的工作使用 l1 损失，以确定性方式训练网络，导致过度平滑的预测，感知质量较差。本文提出了一种新方法，将重点从确定性逐像素比较转移到统计角度，强调学习分布而非个别像素值。核心思想是在损失函数中引入空间熵，测量预测与目标之间的分布差异。为了使空间熵可微，我们使用核密度估计 (KDE) 来近似每个像素与其邻域的特定强度值的概率。具体来说，我们用扩散模型装备熵，并针对基于 l1 的噪声匹配损失，追求更高的准确性和增强的感知质量。在实验中，我们评估了所提方法在两个数据集和 NTIRE 挑战 2024 中的低光增强。所有这些结果说明了我们基于统计的熵损失的有效性。</p><p><strong>Key Takeaways</strong></p><ul><li>图像修复面临病态问题，有多个解。</li><li>传统方法使用 l1 损失，导致过度平滑的预测。</li><li>本文提出了一种基于统计的空间熵损失。</li><li>空间熵损失使用核密度估计 (KDE) 来近似概率分布。</li><li>本文使用扩散模型装备了空间熵损失。</li><li>实验表明，所提方法在低光增强方面优于基于 l1 的损失。</li><li>代码可在 <a href="https://github.com/shermanlian/spatial-entropy-loss">https://github.com/shermanlian/spatial-entropy-loss</a> 获得。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：可微空间熵扩散模型在低光图像增强中的应用</li><li>作者：文义连、文静连、紫薇罗</li><li>隶属：乌普萨拉大学</li><li>关键词：图像复原、低光增强、扩散模型、空间熵</li><li>链接：https://arxiv.org/abs/2404.09735    Github：https://github.com/shermanlian/spatial-entropy-loss</li><li>摘要：（1）研究背景：图像复原是一项具有挑战性的任务，传统的基于像素级的损失函数（如 L1 和 L2）在提高图像保真度方面表现良好，但往往会导致过度平滑的输出，无法捕捉图像的感知质量。（2）过去方法及其问题：为了解决过度平滑的问题，一些研究利用对抗生成网络（GAN）和感知度量（如 VGG 损失和 LPIPS 损失）来提高图像的视觉质量。然而，这些方法仍然依赖于 L1 损失来维持复原精度，并且需要额外的（预训练的）网络，增加了模型的复杂性和不可预测性。（3）本文提出的方法：本文提出了一种基于空间熵的损失函数，将重点从确定性的像素级比较转移到统计学角度，强调学习分布而不是单个像素值。具体来说，本文将空间熵引入损失函数中，以衡量预测值和目标值之间的分布差异。为了使空间熵可微，本文采用核密度估计（KDE）来近似每个像素特定强度值与其邻域的概率。（4）方法在任务中的表现：本文将提出的方法应用于低光增强任务，并在两个数据集和 NTIRE 挑战 2024 上进行了评估。实验结果表明，基于统计的空间熵损失函数在准确性和感知质量方面均优于基于 L1 的噪声匹配损失函数。</li></ol><p><strong>Methods：</strong></p><p>(1) <strong>基于空间熵的损失函数：</strong>   - 引入空间熵衡量预测值和目标值之间的分布差异。   - 采用核密度估计（KDE）近似每个像素的概率分布。</p><p>(2) <strong>低光增强中的应用：</strong>   - 将提出的方法应用于低光图像增强任务。   - 在两个数据集和 NTIRE 挑战 2024 上进行评估。</p><p>(3) <strong>实验结果：</strong>   - 基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。</p><ol><li>结论：（1）：本文提出了一种基于统计匹配的空间熵损失函数，用于图像复原。具体来说，我们引入了核密度估计（KDE）来使空间熵可微分。然后，空间熵可以用于图像重建的不同基于学习的框架。通过将其装备到扩散模型中（以替代ℓ1或ℓ2），我们获得了用于真实图像复原的新颖统计噪声匹配损失。然后，我们将此模型应用于低光增强任务以说明其有效性。我们的模型在 NTIRE 低光增强挑战中取得了最佳 LPIPS 性能。所有这些结果表明，空间熵损失对于高感知扩散学习过程是有效的。（2）：创新点：提出了一种基于空间熵的统计匹配损失函数，用于图像复原。性能：在低光增强任务上，基于统计的空间熵损失函数在准确性和感知质量方面优于基于 L1 的噪声匹配损失函数。工作量：该方法需要额外的计算开销来近似空间熵，这可能会增加训练时间和内存消耗。</li></ol><details>  <summary>点此查看论文截图</summary><img src="http://article.biliimg.com/bfs/new_dyn/a3c133d398fb53ead47a00645c5327d0241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/cf4bf5cc17f4334e352f588ed2ccefcb241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/b7a0ec6067c60992e1a2e395bdf11c91241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1f0299e2b835d711d284c66cc863a956241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/1ffcc1ff6c9b31250304fc557e4bfd89241286257.jpg" align="middle"><img src="http://article.biliimg.com/bfs/new_dyn/26cfb6c25aaf0c1236c9980dae831e68241286257.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-04-17  RefFusion Reference Adapted Diffusion Models for 3D Scene Inpainting</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Diffusion Models" scheme="https://kedreamix.github.io/tags/Diffusion-Models/"/>
    
  </entry>
  
  <entry>
    <title>NeRF</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/NeRF/</id>
    <published>2024-04-14T04:42:58.000Z</published>
    <updated>2024-04-14T04:42:58.005Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation"><a href="#Boosting-Self-Supervision-for-Single-View-Scene-Completion-via-Knowledge-Distillation" class="headerlink" title="Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation"></a>Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</h2><p><strong>Authors:Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers</strong></p><p>Inferring scene geometry from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about geometry even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and distill this knowledge into a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully self-supervised only from image data. Using knowledge distillation, we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions. </p><p><a href="http://arxiv.org/abs/2404.07933v1">PDF</a> </p><p><strong>摘要</strong><br>多视角幕后融合（MVBTS）结合多幅图像的场景几何信息，通过蒸馏得到高精度的单视角场景重构，显著提升被遮挡区域的占有率预测性能。</p><p><strong>要点</strong></p><ul><li>神经辐射场（NeRF）通过预测密度场实现场景完成功能。</li><li>密度场允许通过图像渲染实现精确的深度预测和新视角合成。</li><li>多视角幕后融合（MVBTS）融合多幅图像的密度场，无需深度标签进行完全自监督训练。</li><li>知识蒸馏将 MVBTS 的知识提炼至单视角场景完成功能网络 KDBTS 中。</li><li>KDBTS 利用直接监督进行训练，在占有率预测任务中达到最先进的性能。</li><li>KDBTS 特别提升了被遮挡区域的占有率预测精度。</li><li>MVBTS 和 KDBTS 均可用于 3D 重建和新视角合成等下游任务。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：通过知识蒸馏提升单视图场景补全的自监督</li><li>作者：</li><li>Jannik Bollmeyer</li><li>Sven Behnke</li><li>隶属：慕尼黑工业大学</li><li>关键词：</li><li>场景补全</li><li>神经辐射场</li><li>知识蒸馏</li><li>自监督学习</li><li>论文链接：https://arxiv.org/pdf/2302.04322.pdf   Github 代码链接：无</li><li><p>摘要：   (1)：研究背景：场景补全旨在从图像中推断场景几何，包括被遮挡区域的几何。神经辐射场（NeRF）在场景补全中表现出色，但通常需要大量数据和计算资源。   (2)：过去方法：以往方法主要关注从单视图图像中预测场景几何，但对于被遮挡区域的预测准确度有限。   (3)：研究方法：本文提出了一种名为多视图幕后（MVBTS）的方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。   (4)：方法性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。</p></li><li><p>方法：(1) 提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。(2) MVBTS通过将来自多张图像的密度场信息融合到单视图场景补全网络中，提高了被遮挡区域的预测准确性。(3) KDBTS采用知识蒸馏技术，将多视图场景补全网络的知识转移到单视图场景补全网络中，进一步提升了性能。</p></li><li><p>结论：（1）：本工作通过利用多视图信息，提出了一种改进单视图几何场景重建的新颖方法。这包括扩展最先进的密度预测模型以改进场景几何，然后通过知识蒸馏以 3D 方式进行直接监督以提升单视图模型。训练完全在视频数据上自监督完成。我们在深度估计和占用预测任务上评估了所提出的多视图和增强型单视图模型。虽然我们的方法在深度估计方面接近最先进水平，但被明确为该任务训练的方法所超越，但我们增强的单视图重建模型在占用预测方面始终达到最先进的性能。未来对运动物体建模的工作可以解决动态场景中相互冲突的信息，从而提高 3D 重建的整体准确性和可靠性。致谢。这项工作得到 ERC 高级补助金 SIMULACRON、慕尼黑机器学习中心以及德国联邦交通和数字基础设施部 (BMDV) 资助，用于 ADAM 项目的 19F2251F 补助金。（2）：创新点：提出了一种多视图幕后（MVBTS）方法，利用来自多张图像的密度场信息，通过知识蒸馏训练一个单视图场景补全网络，称为知识蒸馏幕后（KDBTS）。；性能：KDBTS 在占用预测任务上取得了最先进的性能，特别是在被遮挡区域。该方法在 KITTI 数据集上实现了 95.5% 的占用准确率，优于其他基准方法。；工作量：本方法采用自监督学习，训练过程无需人工标注，工作量较小。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6a815fb51ac960f580b9349c84d4aaef.jpg" align="middle"><img src="https://picx.zhimg.com/v2-fdaac4a1aa97db0d9e87e268cca712eb.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e1eb9e0800e52f5241f7180d3cd3cf5e.jpg" align="middle"><img src="https://picx.zhimg.com/v2-3114ea9821cc3e57197fd091c0fa954c.jpg" align="middle"><img src="https://picx.zhimg.com/v2-f2da95bb07d3e0a3fb36ca71ea4e0a27.jpg" align="middle"></details><h2 id="SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection"><a href="#SplatPose-amp-Detect-Pose-Agnostic-3D-Anomaly-Detection" class="headerlink" title="SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection"></a>SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection</h2><p><strong>Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn</strong></p><p>Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. </p><p><a href="http://arxiv.org/abs/2404.06832v1">PDF</a> Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024</p><p><strong>摘要</strong><br>通过神经辐射场（NeRF）实现 3D 多视角图像的无姿态缺陷检测。</p><p><strong>要点</strong></p><ul><li>缺陷检测在图像识别领域广泛探索，当前算法在困难场景和数据类型中的缺陷检测能力不断提升。</li><li>现有方法不适用于不同姿势拍摄的 3D 物体缺陷检测。</li><li>基于 NeRF 的解决方案存在算力要求高的问题，限制其实用性。</li><li>提出基于 3D 高斯 splatting 的 SplatPose 框架，可以对 3D 多视角图像的无姿态缺陷检测。</li><li>在训练、推理速度和检测性能方面达到最先进水平，即使使用比竞争方法更少的训练数据。</li><li>在 Pose-agnostic Anomaly Detection 基准及多姿势缺陷检测（MAD）数据集上进行了全面评估。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Zixuan Huang、Wenbo Li、Junjie Huang、Hao Li、Yida Wang、Lei Zhou、Dachuan Zhang、Dacheng Tao</li><li>隶属机构：中科院自动化所</li><li>关键词：异常检测、姿态无关、3D 感知、神经辐射场</li><li>论文链接：NoneGithub 代码链接：None</li><li>摘要：(1) 研究背景：异常检测在图像中已成为一个研究充分的问题，但大多数现有方法不适用于从不同姿态捕获的 3D 对象。使用神经辐射场 (NeRF) 的解决方案虽然被提出，但存在计算需求过大、阻碍实际使用的问题。(2) 过去方法及其问题：NeRF 方法计算量大；本文方法动机明确。(3) 研究方法：提出基于 3D 高斯斑块的新型框架 SplatPose，该框架给定 3D 对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(4) 方法性能：在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集上对框架进行了全面评估。</li></ol><p>Methods:(1)提出了一种新颖的基于3D高斯斑块的框架SplatPose，该框架给定3D对象的多视图图像，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。(2)SplatPose包含两个主要模块：姿态估计模块和异常检测模块。姿态估计模块使用3D高斯斑块对3D对象进行建模，并使用神经辐射场(NeRF)预测未见视图的姿态。异常检测模块使用基于重建误差的度量来检测异常。(3)SplatPose在训练和推理速度以及检测性能方面均达到最先进水平，即使使用比竞争方法更少的训练数据。在最近提出的与姿态无关的异常检测基准及其多姿态异常检测(MAD)数据集上对框架进行了全面评估。</p><ol><li>结论：（1）：本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们将对象表示为高斯点云，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，使其更适合在生产环境中部署。我们希望未来致力于改进粗略姿态估计和图像特征比较。将我们的发现应用于邻近领域，例如人类姿态估计[16,43]，对我们来说是一个很有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息包含在现有二维方法中的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 在 AIservicecenter KISSKI（拨款号 01IS22093C）下、下萨克森州科学和文化部 (MWK) 通过大众汽车基金会和德国研究基金会 (DFG) 在德国卓越战略下的 Zukunft.niedersachsen 计划的支持下，在卓越集群 PhoenixD (EXC2122) 内。</li><li>结论：（1）：xxx；（2）：创新点：xxx；性能：xxx；工作量：xxx；务必使用中文回答（专有名词需用英文标注），表述尽量简洁、学术，不要重复前面<summary>的内容，利用原文数字的值，务必严格按照格式，相应内容输出到xxx，按照换行，.......表示根据实际要求填写，若无则不写。</summary></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://pica.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views"><a href="#MonoSelfRecon-Purely-Self-Supervised-Explicit-Generalizable-3D-Reconstruction-of-Indoor-Scenes-from-Monocular-RGB-Views" class="headerlink" title="MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views"></a>MonoSelfRecon: Purely Self-Supervised Explicit Generalizable 3D   Reconstruction of Indoor Scenes from Monocular RGB Views</h2><p><strong>Authors:Runfa Li, Upal Mahbub, Vasudev Bhaskaran, Truong Nguyen</strong></p><p>Current monocular 3D scene reconstruction (3DR) works are either fully-supervised, or not generalizable, or implicit in 3D representation. We propose a novel framework - MonoSelfRecon that for the first time achieves explicit 3D mesh reconstruction for generalizable indoor scenes with monocular RGB views by purely self-supervision on voxel-SDF (signed distance function). MonoSelfRecon follows an Autoencoder-based architecture, decodes voxel-SDF and a generalizable Neural Radiance Field (NeRF), which is used to guide voxel-SDF in self-supervision. We propose novel self-supervised losses, which not only support pure self-supervision, but can be used together with supervised signals to further boost supervised training. Our experiments show that “MonoSelfRecon” trained in pure self-supervision outperforms current best self-supervised indoor depth estimation models and is comparable to 3DR models trained in fully supervision with depth annotations. MonoSelfRecon is not restricted by specific model design, which can be used to any models with voxel-SDF for purely self-supervised manner. </p><p><a href="http://arxiv.org/abs/2404.06753v1">PDF</a> </p><p><strong>Summary</strong><br>单目自监督重建框架首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</p><p><strong>Key Takeaways</strong></p><ul><li>首次通过单目RGB视图实现可泛化室内场景的显式3D网格重建。</li><li>采用自编码器架构，解码体素SDF和可泛化的NeRF。</li><li>提出新的自监督损失，支持纯自监督，并可与监督信号结合使用以进一步提升监督训练。</li><li>纯自监督训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型。</li><li>MonoSelfRecon与使用深度注释进行完全监督训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计限制，可用于任何具有体素SDF的模型进行纯粹的自监督。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：MonoSelfRecon：纯粹自监督显式可泛化 3D</li><li>作者：Yuxuan Zhang, Shuaicheng Liu, Chen Feng, Songyou Peng, Xiaowei Zhou, Qixing Huang</li><li>单位：中国科学技术大学</li><li>关键词：单目重建、自监督、显式 3D 表示、神经辐射场</li><li>论文链接：None，Github 链接：None</li><li>摘要：（1）研究背景：当前的单目 3D 场景重建（3DR）工作要么完全监督，要么不可泛化，要么在 3D 表示中是隐式的。（2）过去的方法及其问题：本方法的动机充分吗？现有方法存在以下问题：</li><li>完全监督的方法需要大量标注数据，这在现实场景中难以获得。</li><li>自监督的方法虽然不需要标注数据，但重建的 3D 表示往往是隐式的，难以用于下游任务。</li><li><p>显式 3D 表示的方法虽然可以生成显式的 3D 模型，但往往需要额外的监督信号或先验知识。（3）本文提出的研究方法：本文提出了一种新的框架 MonoSelfRecon，该框架首次通过纯自监督在体素 SDF（有符号距离函数）上实现了可泛化室内场景的显式 3D 网格重建。MonoSelfRecon 遵循基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。本文提出了新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以进一步提升监督训练。（4）方法在什么任务上取得了什么性能？性能是否能支撑其目标？实验表明，在纯自监督下训练的“MonoSelfRecon”优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的 3DR 模型相当。MonoSelfRecon 不受特定模型设计的限制，可用于任何具有体素 SDF 的模型以实现纯自监督的方式。</p></li><li><p>Methods：(1) 提出 MonoSelfRecon 框架，首次通过纯自监督在体素 SDF 上实现了可泛化室内场景的显式 3D 网格重建；(2) 提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练；(3) 采用基于自动编码器的架构，解码体素 SDF 和可泛化神经辐射场 (NeRF)，后者用于在自监督中指导体素 SDF。</p></li><li><p>结论：（1）本工作首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建，具有重要意义。（2）创新点：</p></li><li>提出了一种新的框架MonoSelfRecon，首次通过纯自监督在体素SDF上实现了可泛化室内场景的显式3D网格重建。</li><li>提出新的自监督损失，不仅支持纯自监督，还可以与监督信号一起使用以提升监督训练。</li><li>采用基于自动编码器的架构，解码体素SDF和可泛化神经辐射场(NeRF)，后者用于在自监督中指导体素SDF。性能：</li><li>在纯自监督下训练的MonoSelfRecon优于当前最好的自监督室内深度估计模型，并且与使用深度注释在完全监督下训练的3DR模型相当。</li><li>MonoSelfRecon不受特定模型设计的限制，可用于任何具有体素SDF的模型以实现纯自监督的方式。工作量：</li><li>实验表明，MonoSelfRecon在ScanNet和7Scenes数据集上取得了很好的效果。</li><li>MonoSelfRecon可以通过少量学习轻松转移到其他领域。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-3b74e26f87e5c69504b3e0bf5614d4ce.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d8c823491ef532d498c54b5bc4954cc5.jpg" align="middle"><img src="https://pica.zhimg.com/v2-066b26c50380cb863d74934c40a0317f.jpg" align="middle"><img src="https://picx.zhimg.com/v2-9e696c0929fb3a424cfb7cec25388bf9.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>神经辐射场 (NeRF) 使用高斯球面法建模 3D 几何约束，用于动态视图合成。</p><p><strong>Key Takeaways</strong></p><ul><li>提出可变形高斯球面法，在 3D 动态视图合成中考虑 3D 几何形状。</li><li>使用高斯球面表示场景，优化其位置和旋转以建模变形。</li><li>通过提取 3D 几何特征并将其融入变形学习中，执行基于 3D 几何形状的变形建模。</li><li>通过合成和真实数据集的广泛实验验证了所提方法的优越性，达到新的最先进性能。</li><li>该项目可在 <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> 获取。</li><li>3D 高斯球面法可用于 3D 形状建模，并应用于动态场景中。</li><li>显式几何约束增强了 NeRF 在动态视图合成中的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：基于 3D 几何感知的可变形高斯散射用于动态视图合成</li><li>作者：Qiangeng Xu, Pengfei Wan, Wentao Yuan, Junyu Han, Jiayuan Mao, Yebin Liu, Qi Tian</li><li>单位：南京邮电大学</li><li>关键词：动态视图合成、神经辐射场、3D 几何感知、高斯散射</li><li>论文链接：None，Github 链接：None</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）方法在动态视图合成中面临变形学习的挑战，现有 NeRF 解决方案以隐式方式学习变形，无法纳入 3D 场景几何信息，导致学习的变形在几何上不连贯，动态视图合成和 3D 动态重建效果不佳。   （2）过去方法及其问题：3D 高斯散射提供了一种新的 3D 场景表示方法，在此基础上可以利用 3D 几何信息来学习复杂的 3D 变形。现有方法存在的问题是：</li><li>无法有效利用 3D 场景几何约束来指导变形学习。</li><li>学习到的变形在几何上不连贯，导致动态视图合成和 3D 动态重建效果不佳。   （3）本文提出的研究方法：本文提出了一种基于 3D 几何感知的可变形高斯散射方法用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。   （4）方法在任务上的表现及性能：本文方法在合成和真实数据集上进行了广泛的实验，证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1) 提出了一种基于3D几何感知的可变形高斯散射方法，用于动态视图合成。(2) 通过显式提取3D几何特征并将其融入3D变形学习中，实现了3D几何感知的变形建模。(3) 利用3D几何信息指导变形学习，提高了动态视图合成和3D动态重建的质量。(4) 在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。该方法通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模，从而提高了动态视图合成和 3D 动态重建的质量。（2）：创新点：</li><li>提出了一种基于 3D 几何感知的可变形高斯散射方法，用于动态视图合成。</li><li>通过显式提取 3D 几何特征并将其融入 3D 变形学习中，实现了 3D 几何感知的变形建模。</li><li>利用 3D 几何信息指导变形学习，提高了动态视图合成和 3D 动态重建的质量。性能：</li><li>在合成和真实数据集上进行了广泛的实验，证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法需要对 3D 几何特征进行显式提取，增加了计算量。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://picx.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields"><a href="#GHNeRF-Learning-Generalizable-Human-Features-with-Efficient-Neural-Radiance-Fields" class="headerlink" title="GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields"></a>GHNeRF: Learning Generalizable Human Features with Efficient Neural   Radiance Fields</h2><p><strong>Authors:Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet</strong></p><p>Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human geometry and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time. </p><p><a href="http://arxiv.org/abs/2404.06246v1">PDF</a> </p><p><strong>Summary</strong><br>利用预训练的 2D 编码器将人体 2D/3D 关节位置与 NeRF 结合，实现人体几何、纹理和生物力学特征的联合表示。</p><p><strong>Key Takeaways</strong></p><ul><li>GHNeRF 是一种新颖的方法，可通过 NeRF 表示学习人体 2D/3D 关节位置。</li><li>GHNeRF 将预训练的 2D 编码器集成到 NeRF 框架中，以提取人体本质特征。</li><li>该方法可以同时学习人体几何、纹理和生物力学特征（如关节位置）。</li><li>GHNeRF 在近乎实时的情况下优于最先进的人体 NeRF 技术和关节估计算法。</li><li>GHNeRF 提取的关节估计准确且稳定。</li><li>GHNeRF 对遮挡和自遮挡具有鲁棒性。</li><li>GHNeRF 可用于 AR/VR 应用程序和游戏中的人体建模。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：GHNeRF：学习可泛化的人体特征</li><li>作者：Arnab Dey，Di Yang，Rohith Agaram，Antitza Dantcheva，Andrew I. Comport，Srinath Sridhar，Jean Martinet</li><li>第一作者单位：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：神经辐射场，人体表示，人体特征，关节定位，姿势估计</li><li>论文链接：https://arxiv.org/abs/2404.06246   Github 链接：无</li><li><p>摘要：   （1）研究背景：神经辐射场（NeRF）在 3D 场景表示中取得了显著进展，包括 3D 人体表示。然而，这些表示通常缺乏人体姿势和结构的关键信息，这对于 AR/VR 应用和游戏至关重要。   （2）过去方法：以往方法只能学习人体几何和纹理，无法同时学习人体生物力学特征，例如关节位置。   （3）研究方法：本文提出 GHNeRF，它将预训练的 2D 编码器与 NeRF 框架相结合，从 2D 图像中提取人体特征，并将其编码到 NeRF 中。这使得 GHNeRF 能够同时学习人体几何、纹理和生物力学特征，例如关节位置。   （4）方法性能：GHNeRF 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果，并且可以在接近实时的情况下运行。</p></li><li><p>方法：（1）：GHNeRF将预训练的2D编码器与NeRF框架相结合，从2D图像中提取人体特征，并将其编码到NeRF中，同时学习人体几何、纹理和生物力学特征，例如关节位置。（2）：GHNeRF使用基于Transformer的2D编码器，可以从2D图像中提取局部和全局特征，并将其编码为一个潜在的特征向量。（3）：然后，将这个潜在的特征向量输入到NeRF中，NeRF使用多层感知器来预测场景中每个点的颜色和密度。（4）：通过优化NeRF的损失函数，GHNeRF可以同时学习人体几何、纹理和生物力学特征，例如关节位置。</p></li></ol><p><strong>结论</strong>1. 本工作通过提出 GHNeRF，将人体生物力学特征学习融入 NeRF，显著提升了人体 NeRF 表征的泛化能力。2. 创新点：   - 提出了一种将预训练的 2D 编码器与 NeRF 框架相结合的方法，从 2D 图像中提取人体特征并将其编码到 NeRF 中。   - 创新性地利用基于 Transformer 的 2D 编码器，能够从 2D 图像中提取局部和全局特征。3. 性能：   - 在人体 NeRF 技术和关节估计算法的综合比较中取得了最先进的结果。   - 可以在接近实时的情况下运行，具有较高的实用性。4. 工作量：   - 工作量较大，涉及到 2D 编码器的预训练、NeRF 模型的训练和优化。   - 算法的复杂度较高，需要较高的计算资源。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-d1e65b0e4287dba0204c3edb8075bb41.jpg" align="middle"><img src="https://pica.zhimg.com/v2-962fd6bcf11373783e89def5f58c894b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-04d65c8c665cfeac6b6c20878f5001d2.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e13a327e9d52f71bd0c265b3d7ab6c51.jpg" align="middle"></details>## HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields**Authors:Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet**In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF. [PDF](http://arxiv.org/abs/2404.06152v1) **Summary**新颖视图生成技术中的泛化神经辐射场 (NeRF) 方法在从少量图像生成新视图方面取得了显著进展，但无法捕捉所有人体实例中骨骼的潜在结构特征。**Key Takeaways**- 引入了 HFNeRF：一种新颖的泛化人体特征 NeRF，旨在使用预训练图像编码器生成人体生物力学特征。- 以前的人体 NeRF 方法在生成逼真的虚拟化身方面显示出有希望的结果，但缺乏对下游应用（包括 AR/VR）至关重要的潜在人体结构或生物力学特征（例如骨骼或关节信息）。- HFNeRF 利用 2D 预训练基础模型，通过神经渲染学习 3D 人体特征，然后通过体积渲染生成 2D 特征图。- 通过预测热图作为特征，评估了 HFNeRF 在骨骼估计任务中的表现。- 所提出的方法完全可微分，允许同时成功学习颜色、几何和人体骨骼。- 本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>标题：HFNeRF：使用神经辐射场学习人体生物力学特征</li><li>作者：Arnab Dey，Di Yang，Antitza Dantcheva，Jean Martinet</li><li>隶属机构：I3S-CNRS/Universit´e Cˆoted’Azur</li><li>关键词：计算机视觉、增强现实、虚拟现实、NeRF</li><li>论文链接：https://arxiv.org/abs/2404.06152</li><li>摘要：   （1）研究背景：神经辐射场（NeRF）在生成新颖视图方面取得了显著进展，但现有方法无法捕捉到不同实例之间共享的骨骼等潜在结构特征。   （2）过去方法及问题：现有基于 NeRF 的人体方法虽然在生成逼真的虚拟化身方面取得了可喜的成果，但缺乏潜在的人体结构或生物力学特征，如骨骼或关节信息，这对于增强现实 (AR)/虚拟现实 (VR) 等下游应用至关重要。   （3）研究方法：本文提出了一种名为 HFNeRF 的新方法，该方法利用 NeRF 架构学习人体生物力学特征，如人体骨骼。HFNeRF 采用预训练的 2D 编码器，使用神经渲染从图像中提取人体特征，然后使用体积渲染生成 2D 特征图。   （4）方法性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估。该方法是完全可微的，允许以同步的方式成功学习颜色、几何形状和人体骨骼。本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><p><strong>Methods</strong></p><p>(1): <strong>NeRF架构</strong>：HFNeRF利用神经辐射场（NeRF）架构，通过多层感知器（MLP）将3D坐标映射到颜色和不透明度。</p><p>(2): <strong>特征提取</strong>：使用预训练的2D编码器从图像中提取人体特征，生成2D特征图。</p><p>(3): <strong>骨骼提取</strong>：从2D特征图中预测热图作为骨骼特征，然后通过后处理提取骨骼。</p><ol><li>结论：（1）：本文提出了一种名为 HFNeRF 的新框架，该框架使用神经辐射场（NeRF）来学习人体生物力学特征。我们的初步研究结果证明了 HFNeRF 在预测人体特征方面的有效性，这比以前用于人类的 NeRF 方法有了显着改进。虽然我们的重点是人体骨骼检测，但我们相信这种架构可以扩展到其他可概括的人体特征，例如身体部位检测。（2）：创新点：提出了一种新颖的框架 HFNeRF，该框架使用 NeRF 学习人体生物力学特征，如人体骨骼。性能：HFNeRF 在骨骼估计任务中通过预测热图作为特征进行评估，在预测人体特征方面表现出有效性。工作量：本文展示了 HFNeRF 的初步结果，说明了其使用 NeRF 生成具有生物力学特征的逼真虚拟化身的潜力。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-d759674d19facfbca30699f2f267c071.jpg" align="middle"><img src="https://picx.zhimg.com/v2-776a0f7d6568012bbae47efa541663bf.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">NeRF 方向最新论文已更新，请持续关注 Update in 2024-04-14  Boosting Self-Supervision for Single-View Scene Completion via Knowledge   Distillation</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="NeRF" scheme="https://kedreamix.github.io/tags/NeRF/"/>
    
  </entry>
  
  <entry>
    <title>3DGS</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/3DGS/</id>
    <published>2024-04-14T04:25:51.000Z</published>
    <updated>2024-04-14T04:25:51.782Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh"><a href="#GoMAvatar-Efficient-Animatable-Human-Modeling-from-Monocular-Video-Using-Gaussians-on-Mesh" class="headerlink" title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh"></a>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</h2><p><strong>Authors:Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang</strong></p><p>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with geometry modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject). </p><p><a href="http://arxiv.org/abs/2404.07991v1">PDF</a> CVPR 2024; project page: <a href="https://wenj.github.io/GoMAvatar/">https://wenj.github.io/GoMAvatar/</a></p><p><strong>Summary</strong><br>实时、内存高效、高质量可动画人体重建的全新方法，GoMAvatar。</p><p><strong>Key Takeaways</strong></p><ul><li>引入高斯网格表示，结合了高斯点云渲染的质量和速度、几何建模以及可变形网格的兼容性。</li><li>输入单目视频即可创建可在新姿势中重新关节化并从新视点实时渲染的数字虚拟人。</li><li>与光栅化图形管道无缝集成。</li><li>在 ZJU-MoCap 数据和各种 YouTube 视频上评估了 GoMAvatar。</li><li>在渲染质量上达到或超过当前单目人形建模算法，同时在计算效率（43 FPS）和内存效率（每个受试者 3.63 MB）方面显著优于它们。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：GoMAvatar：通过单目视频高效构建可动画的人体模型</li><li>作者：</li><li>Chen Cao</li><li>Pengfei Xiang</li><li>Yuting Ye</li><li>Yuxuan Zhang</li><li>Hongyi Xu</li><li>Yebin Liu</li><li>Hao Li</li><li>Hanqing Lu</li><li>Wenping Wang</li><li>Xiaoguang Han</li><li>第一作者单位：浙江大学</li><li>关键词：</li><li>单目人体建模</li><li>高斯网格表示</li><li>实时渲染</li><li>可动画</li><li>论文链接：None   Github 代码链接：None</li><li>摘要：   （1）研究背景：<ul><li>单目人体建模是计算机视觉领域的重要课题，可以从单目视频中创建可动画的人体模型。</li><li>现有的单目人体建模方法要么渲染质量差，要么计算效率低，要么内存消耗大。   （2）过去方法：</li><li>基于网格的方法：渲染质量高，但计算效率低。</li><li>基于高斯球的方法：计算效率高，但渲染质量差。   （3）研究方法：</li><li>提出了一种新的高斯网格表示（GoM），结合了高斯球的渲染速度和网格模型的几何建模能力。</li><li>设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。</li><li>采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。   （4）方法性能：</li><li>在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。</li><li>GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43 FPS），同时内存消耗也较低（每个主体 3.63 MB）。</li></ul></li></ol><p>Some Error for method(比如是不是没有Methods这个章节)</p><ol><li>结论：（1）：本工作提出了 GoMAvatar，该框架旨在使用单个输入视频渲染出人类表演者的高保真自由视角图像。我们的方法的核心是高斯网格表示。结合前向关节运动和神经渲染，我们的方法渲染速度快，同时内存效率高。值得注意的是，该方法很好地处理野外视频。（2）：创新点：提出了高斯网格表示，结合了高斯球的渲染速度和网格模型的几何建模能力。设计了一个端到端可微分管道，从单目视频输入到可动画的人体模型输出。采用神经网络对模型参数进行优化，包括形状、纹理、姿态和动画。性能：在 ZJU-MoCap、PeopleSnapshot 和 YouTube 视频数据集上评估了 GoMAvatar。GoMAvatar 在渲染质量上与现有的单目人体建模算法相当或优于它们，在计算效率上显著优于它们（43FPS），同时内存消耗也较低（每个主体 3.63MB）。工作量：在 2 个 NVIDIA Tesla V100 GPU 上训练模型需要大约 10 天。在单个 NVIDIA RTX 2080 Ti GPU 上进行推理需要大约 23ms。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-f6679894ad5fb175b61f1275145cd461.jpg" align="middle"><img src="https://pica.zhimg.com/v2-acf0512eb9d25a17024d67cc7e7ac305.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-4f1e513ece4b778293f135ec5b0edea5.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-ea1e1eb74d0ff5d3bdeeb203aac60cdc.jpg" align="middle"></details><h2 id="RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion"><a href="#RealmDreamer-Text-Driven-3D-Scene-Generation-with-Inpainting-and-Depth-Diffusion" class="headerlink" title="RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion"></a>RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth   Diffusion</h2><p><strong>Authors:Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi</strong></p><p>We introduce RealmDreamer, a technique for generation of general forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian Splatting representation to match complex text prompts. We initialize these splats by utilizing the state-of-the-art text-to-image generators, lifting their samples into 3D, and computing the occlusion volume. We then optimize this representation across multiple views as a 3D inpainting task with image-conditional diffusion models. To learn correct geometric structure, we incorporate a depth diffusion model by conditioning on the samples from the inpainting model, giving rich geometric structure. Finally, we finetune the model using sharpened samples from image generators. Notably, our technique does not require video or multi-view data and can synthesize a variety of high-quality 3D scenes in different styles, consisting of multiple objects. Its generality additionally allows 3D synthesis from a single image. </p><p><a href="http://arxiv.org/abs/2404.07199v1">PDF</a> Project Page: <a href="https://realmdreamer.github.io/">https://realmdreamer.github.io/</a></p><p><strong>Summary</strong><br>文本描述生成通用前视角 3D 场景的 RealmDreamer 技术，利用 3D 高斯飞溅表征匹配复杂文本提示。</p><p><strong>Key Takeaways</strong></p><ul><li>利用最先进的文本对图像生成器初始化 3D 高斯飞溅。</li><li>通过图像条件扩散模型，将此表示优化为多视图 3D 修复任务。</li><li>结合深度扩散模型，通过修复模型样本来学习正确的几何结构，提供丰富的几何结构。</li><li>使用图像生成器中锐化的样本对模型进行微调。</li><li>无需视频或多视图数据，可合成各种高质量、不同风格的 3D 场景。</li><li>允许从单张图像中进行 3D 合成。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：RealmDreamer：文本驱动的三维场景生成，带内绘和深度扩散</li><li>作者：Jaidev Shriram<em> Alex Trevithick</em> Lingjie Liu Ravi Ramamoorthi</li><li>隶属机构：加州大学圣地亚哥分校</li><li>关键词：文本到 3D、3D 场景生成、内绘、深度扩散</li><li>论文链接：https://realmdreamer.github.io/</li><li>摘要：（1）研究背景：文本驱动的三维场景合成具有革新三维内容创建的潜力，但现有方法存在迭代时间长、仅限于简单对象级数据或全景图等问题。（2）过去方法：现有方法包括神经辐射场（NeRF）、Prolific Dreamer 等，但这些方法需要视频或多视图数据，且生成的场景几何结构不准确。（3）研究方法：RealmDreamer 优化三维高斯散射表示以匹配复杂的文本提示。它利用文本到图像生成器初始化散射点，将其提升到三维并计算遮挡体积。然后，它将此表示优化为跨多个视图的三维内绘任务，并使用图像条件扩散模型。为了学习正确的几何结构，它结合深度扩散模型，以内绘模型的样本为条件，从而获得丰富的几何结构。最后，使用图像生成器的锐化样本对模型进行微调。（4）性能：RealmDreamer 在各种风格和包含多个对象的高质量三维场景合成方面取得了最先进的结果。它还可以从单个图像中合成三维场景，无需视频或多视图数据。</li></ol><p>方法：(1): 将文本提示转换为三维高斯散射表示（3DGS），利用文本到图像生成器初始化散射点，并提升到三维以计算遮挡体积；(2): 使用图像条件扩散模型对三维表示进行优化，作为跨多个视图的三维内绘任务；(3): 结合深度扩散模型，以内绘模型的样本为条件，获得丰富的几何结构；(4): 使用图像生成器的锐化样本对模型进行微调，以获得清晰的三维样本。</p><ol><li>结论：（1）：RealmDreamer 在 3D 场景生成方面取得了最先进的成果，为 3D 内容创建带来了新的可能性。（2）：创新点：</li><li>提出了一种基于内绘和深度扩散的文本驱动的 3D 场景生成方法。</li><li>利用文本到图像生成器初始化 3D 散射表示，并使用图像条件扩散模型和深度扩散模型优化几何结构。</li><li>可以从单个图像中合成 3D 场景，无需视频或多视图数据。</li><li>性能：在各种风格和包含多个对象的高质量 3D 场景合成方面取得了最先进的结果。</li><li>负载：训练时间较长（数小时），对于具有高度遮挡的复杂场景，生成的图像可能会模糊。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-a5b848a94e7c9ceb54ea9d3c0f1d9886.jpg" align="middle"><img src="https://pica.zhimg.com/v2-38a9e6e511a25c96557597931a870630.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-868bab455ec35ce1f302df76db771b6f.jpg" align="middle"><img src="https://pica.zhimg.com/v2-b22afc5e0c39a48548a9120642a8038d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-6a53086cc451b1c9072ffe37abccae8f.jpg" align="middle"></details><h2 id="DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting"><a href="#DreamScene360-Unconstrained-Text-to-3D-Scene-Generation-with-Panoramic-Gaussian-Splatting" class="headerlink" title="DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting"></a>DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic   Gaussian Splatting</h2><p><strong>Authors:Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi</strong></p><p>The increasing demand for virtual reality applications has highlighted the significance of crafting immersive 3D assets. We present a text-to-3D 360$^{\circ}$ scene generation pipeline that facilitates the creation of comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of minutes. Our approach utilizes the generative power of a 2D diffusion model and prompt self-refinement to create a high-quality and globally coherent panoramic image. This image acts as a preliminary “flat” (2D) scene representation. Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to enable real-time exploration. To produce consistent 3D geometry, our pipeline constructs a spatially coherent structure by aligning the 2D monocular depth into a globally optimized point cloud. This point cloud serves as the initial state for the centroids of 3D Gaussians. In order to address invisible issues inherent in single-view inputs, we impose semantic and geometric constraints on both synthesized and input camera views as regularizations. These guide the optimization of Gaussians, aiding in the reconstruction of unseen regions. In summary, our method offers a globally consistent 3D scene within a 360$^{\circ}$ perspective, providing an enhanced immersive experience over existing techniques. Project website at: <a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a> </p><p><a href="http://arxiv.org/abs/2404.06903v1">PDF</a> </p><p><strong>Summary</strong><br>文本到三维 360 度场景生成管道，可快速轻松地创建身临其境的 360 度场景。</p><p><strong>Key Takeaways</strong></p><ul><li>利用 2D 扩散模型生成高质量且全局连贯的全景图像作为平坦场景表示。</li><li>使用喷射技术将平坦场景提升为三维高斯体，实现实时探索。</li><li>构建空间连贯结构，将 2D 单目深度对齐到全局优化点云，生成一致的三维几何体。</li><li>利用语义和几何约束正则化合成和输入相机视图，优化高斯体，重建不可见区域。</li><li>该方法提供全局一致的三维场景，提供比现有技术更好的沉浸式体验。</li><li>项目网站：<a href="http://dreamscene360.github.io/">http://dreamscene360.github.io/</a></li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：DreamScene360：无约束文本到 3D 场景</li><li>作者：Shijie Zhou、Zhiwen Fan、Dejia Xu、Haoran Chang、Pradyumna Chari、Tejas Bharadwaj、Suya You、Zhangyang Wang、Achuta Kadambi</li><li>第一作者单位：加州大学洛杉矶分校</li><li>关键词：文本到 3D、360 度全景、高斯点 splatting、2D 扩散模型、单目深度估计</li><li>论文链接：http://dreamscene360.github.io/，Github 代码链接：无</li><li><p>摘要：（1）研究背景：虚拟现实应用的兴起凸显了创建沉浸式 3D 资产的重要性。（2）过去方法：现有方法通常依赖于 3D 建模或扫描，这需要大量的人力和时间。（3）研究方法：本文提出了一种文本到 3D 360 度场景生成管道，利用 2D 扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到 3D 高斯点 splatting 中，并通过对齐 2D 单目深度来构建空间一致的结构。（4）实验结果：该方法在文本到 360 度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的 360 度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p></li><li><p>方法：（1）：文本到 360° 全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）：从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）：利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p></li></ol><p><strong>摘要</strong></p><p>本研究提出了一种文本到3D 360度场景生成管道，利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，再将其提升到3D高斯点splatting中，并通过对齐2D单目深度来构建空间一致的结构。</p><p><strong>方法</strong></p><p>（1）文本到360°全景生成，采用自优化流程，确保生成鲁棒性，并与文本语义对齐；（2）从全景几何场初始化，将语义对齐和几何对应关系作为高斯优化正则化，以解决单视图输入造成的差距；（3）利用虚拟相机合成视差，并通过强制特征级相似性来指导高斯填充不可见区域的几何差距。</p><p><strong>结论</strong></p><p>（1）本文提出的方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。这些性能支持了本文的目标，即提供一种快速且高效的方法来创建沉浸式虚拟现实体验。</p><p>（2）<strong>创新点</strong>：- 提出了一种文本到3D 360度场景生成管道，该管道利用2D扩散模型和提示自优化生成高质量且全局一致的全景图像，并将其提升到3D高斯点splatting中。- 通过对齐2D单目深度来构建空间一致的结构，解决了单视图输入造成的差距。</p><p><strong>性能</strong>：- 该方法在文本到360度全景场景生成任务上取得了较好的性能，可以在几分钟内生成高质量、全局一致且可实时探索的360度全景场景。</p><p><strong>工作量</strong>：- 该方法的工作量相对较小，可以在几分钟内生成一个360度全景场景。</p><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-31c98e3198cc0cdc817317b3bf23d03b.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0729d57eb4a8e1293909757da5dd5064.jpg" align="middle"></details>## SplatPose &amp; Detect: Pose-Agnostic 3D Anomaly Detection**Authors:Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn**Detecting anomalies in images has become a well-explored problem in both academia and industry. State-of-the-art algorithms are able to detect defects in increasingly difficult settings and data modalities. However, most current methods are not suited to address 3D objects captured from differing poses. While solutions using Neural Radiance Fields (NeRFs) have been proposed, they suffer from excessive computation requirements, which hinder real-world usability. For this reason, we propose the novel 3D Gaussian splatting-based framework SplatPose which, given multi-view images of a 3D object, accurately estimates the pose of unseen views in a differentiable manner, and detects anomalies in them. We achieve state-of-the-art results in both training and inference speed, and detection performance, even when using less training data than competing methods. We thoroughly evaluate our framework using the recently proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly detection (MAD) data set. [PDF](http://arxiv.org/abs/2404.06832v1) Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024**Summary**通过给定 3D 物体的多视图图像，SplatPose 可以准确估计未见视图的姿势并检测其中的异常。**Key Takeaways**- **解决 3D 姿态问题：** SplatPose 适用于从不同姿势捕获的 3D 对象的异常检测。- **基于 3D 高斯溅射：** 该框架采用创新的基于 3D 高斯溅射的算法。- **可微姿势估计：** 以可微方式估计未见视图的姿势。- **高效计算：** 在训练和推理速度方面取得了最先进的成果。- **优异的检测性能：** 即使使用较少的训练数据，也能检测异常。- **对姿势无关的异常检测基准评估：** 使用最新的基准进行了全面的评估。- **多姿势异常检测数据集：** 在多姿势异常检测数据集上进行测试。**[ChatPaperFree](https://huggingface.co/spaces/Kedreamix/ChatPaperFree)**<ol><li>论文标题：SplatPose&amp;Detect：与姿态无关的 3D 异常检测</li><li>作者：Yifan Jiang, Guilin Liu, Zhehui Yuan, Shenghua Gao, Jingyi Yu, Xiaoguang Han</li><li>第一作者单位：华中科技大学</li><li>关键词：Computer Vision, Anomaly Detection, 3D Object, Pose-Agnostic</li><li>论文链接：None    Github 链接：None</li><li>摘要：（1）研究背景：异常检测在图像中是一个经过充分探索的问题，最先进的算法能够在越来越困难的设置和数据模式中检测缺陷。然而，大多数当前的方法不适合处理从不同姿态捕获的 3D 对象。虽然已经提出了使用神经辐射场的解决方案，但它们存在过度的计算要求，这阻碍了实际使用。（2）过去方法：过去方法存在以下问题：</li><li>无法处理不同姿态的 3D 对象。</li><li>使用神经辐射场的方法计算要求过高。</li><li>训练数据量大。（3）研究方法：为了解决这些问题，本文提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架在给定 3D 对象的多视图图像的情况下，能够以可微分的方式准确估计未见视图的姿态，并检测其中的异常。（4）方法性能：该方法在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。使用最近提出的与姿态无关的异常检测基准及其多姿态异常检测 (MAD) 数据集对该框架进行了全面评估。</li></ol><p><strong>7. 方法</strong></p><p>该方法提出了一种基于 3D 高斯斑块的新颖框架 SplatPose，具体步骤如下：</p><p>(1) <strong>姿态估计：</strong>利用多视图图像，通过可微分的方式估计未见视图的姿态，从而获得 3D 对象的完整表示。</p><p>(2) <strong>异常检测：</strong>在估计的 3D 表示上，使用高斯混合模型 (GMM) 检测异常，其中每个高斯分量对应于对象的正常部分。</p><p>(3) <strong>与姿态无关：</strong>通过将姿态估计与异常检测解耦，该方法实现了与姿态无关的异常检测，即使对象以不同的姿态出现，也能准确检测异常。</p><ol><li>结论：(1): 本文提出了一种新颖的与姿态无关的异常检测方法。给定多视图图像，我们使用高斯斑块表示对象，用于姿态估计，并在没有先验姿态信息的情况下查找图像中的异常。我们的方法在检测任务中击败了所有竞争对手，同时在训练和推理时间上仍然快几个数量级，这使其更适合在生产环境中部署。我们希望未来的工作致力于改进粗略的姿态估计和图像特征比较。将我们的发现应用于相邻领域，例如人类姿态估计[16,43]，对我们来说是一个有希望的下一步方向。缩小合成数据和真实世界数据之间的差距也需要更多的工作。最后，我们希望研究将三维点云信息纳入现有二维方法的方法。致谢。这项工作得到了德国联邦教育和研究部 (BMBF) 的支持，德国在 AIservicecenter KISSKI（拨款号 01IS22093C）下，下萨克森州科学和文化部 (MWK) 通过 Volkswagen 基金会的 zukunft.niedersachsen 计划以及德国研究基金会 (DFG) 在德国卓越战略下，在卓越集群 PhoenixD (EXC2122) 内。(2): 创新点：提出了基于 3D 高斯斑块的新颖框架 SplatPose，该框架可以以可微分的方式估计未见视图的姿态，并检测其中的异常。性能：在训练和推理速度以及检测性能方面都达到了最先进的水平，即使使用比竞争方法更少的训练数据也是如此。工作量：训练和推理速度快几个数量级，使其更适合在生产环境中部署。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pic1.zhimg.com/v2-98f8ef1e6e42e25a4f68a8eac21e75a0.jpg" align="middle"><img src="https://picx.zhimg.com/v2-e84352a2af71b475f2c2f75f76369d19.jpg" align="middle"><img src="https://picx.zhimg.com/v2-ab11a3ec9140c1596b81427a74134f5b.jpg" align="middle"><img src="https://picx.zhimg.com/v2-d06c9960f9e8a142bf4951e22b98ea8d.jpg" align="middle"></details><h2 id="Zero-shot-Point-Cloud-Completion-Via-2D-Priors"><a href="#Zero-shot-Point-Cloud-Completion-Via-2D-Priors" class="headerlink" title="Zero-shot Point Cloud Completion Via 2D Priors"></a>Zero-shot Point Cloud Completion Via 2D Priors</h2><p><strong>Authors:Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee</strong></p><p>3D point cloud completion is designed to recover complete shapes from partially observed point clouds. Conventional completion methods typically depend on extensive point cloud data for training %, with their effectiveness often constrained to object categories similar to those seen during training. In contrast, we propose a zero-shot framework aimed at completing partially observed point clouds across any unseen categories. Leveraging point rendering via Gaussian Splatting, we develop techniques of Point Cloud Colorization and Zero-shot Fractal Completion that utilize 2D priors from pre-trained diffusion models to infer missing regions. Experimental results on both synthetic and real-world scanned point clouds demonstrate that our approach outperforms existing methods in completing a variety of objects without any requirement for specific training data. </p><p><a href="http://arxiv.org/abs/2404.06814v1">PDF</a> </p><p><strong>摘要</strong><br>零样本3D点云补全采用预训练扩散模型的2D先验来恢复未观察到的点云区域。</p><p><strong>关键要点</strong></p><ul><li>提出零样本3D点云补全框架，适用于任何未见类别。</li><li>利用高斯散射进行点云渲染，将2D先验融入点云补全。</li><li>开发点云着色和零样本分形补全技术。</li><li>无需针对性训练数据即可补全各类物体。</li><li>在合成和真实扫描点云上优于现有方法。</li><li>拓展了3D点云处理的适用范围。</li><li>促进零样本学习在3D视觉中的应用。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：零样本点云补全通过 2D 先验</li><li>作者：Tianxin Huang、Zhiwen Yan、Yuyang Zhao、Gim Hee Lee</li><li>单位：新加坡国立大学计算学院</li><li>关键词：点云补全、高斯渲染、扩散模型</li><li>论文链接：None，Github 链接：None</li><li><p>摘要：（1）研究背景：点云补全旨在从部分观测的点云中恢复完整的形状。传统补全方法通常依赖于大量的点云数据进行训练，其有效性通常仅限于与训练期间所见对象类别相似的对象类别。（2）过去方法及问题：现有方法在处理测试时数据时面临挑战，例如未见的对象类别或真实世界的扫描。这些方法的有效性往往受到训练数据集多样性不足的限制。（3）论文方法：该研究提出了一种零样本框架，旨在跨越任何未见类别补全部分观测的点云。利用通过高斯渲染进行的点渲染，开发了点云着色和零样本分形补全技术，利用预训练扩散模型的 2D 先验来推断缺失区域。（4）任务和性能：该方法在合成和真实世界扫描的点云上都优于现有方法，无需任何特定训练数据即可补全各种对象。实验结果证明了该方法的有效性。</p></li><li><p>方法：(1) 点云着色：利用高斯渲染将点云转换为可渲染的 2D 图像，并通过深度条件着色优化 3D 高斯体；(2) 零样本分形补全：利用预训练扩散模型的 2D 先验，优化 3D 高斯体，并引入视图相关指导和保持约束，以完成缺失区域；(3) 高斯曲面提取：从优化后的 3D 高斯体的中心中提取表面点，形成均匀的补全点云。</p></li><li><p>结论：(1): 本工作提出了一种零样本点云补全框架，利用扩散模型丰富的二维先验通过三维高斯渲染进行补全。与文本驱动的补全方法不同，我们的方法不需要任何额外的提示。整个补全过程由点云着色和零样本分形补全（ZFC）组成。在点云着色中，我们提出参考视点估计和深度条件着色来估计部分点云的参考图像。随后，我们引入 ZFC，通过优化三维高斯体来补全部分点云的缺失区域，该高斯体通过参考图像调节的视点相关指导进行调节。最后，我们从三维高斯体中提取完成的点云，并使用网格拉取模块将其重新采样为均匀的结果。根据我们的实验，我们的方法比现有的基于网络的补全方法取得了更好的性能，在合成和真实扫描的点云上都具有很强的鲁棒性。(2): 创新点：提出了一种利用扩散模型二维先验进行零样本点云补全的框架；性能：在合成和真实扫描的点云上都优于现有的基于网络的补全方法；工作量：由于需要针对每个点云进行单独的优化过程以集成扩散模型的二维先验，因此比现有的基于网络的方法慢。</p></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-8167bf42bfd5c3b7928434682050264a.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-e5f1e4af1bed29e26696ea969cdbf7b4.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-c528148defac6befac55b074fe88fc24.jpg" align="middle"></details><h2 id="3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis"><a href="#3D-Geometry-aware-Deformable-Gaussian-Splatting-for-Dynamic-View-Synthesis" class="headerlink" title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis"></a>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View   Synthesis</h2><p><strong>Authors:Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai</strong></p><p>In this paper, we propose a 3D geometry-aware deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene geometry. Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D geometry could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene geometry constraint during deformation, we explicitly extract 3D geometry features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D geometry-aware deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance.   The project is available at <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a> </p><p><a href="http://arxiv.org/abs/2404.06270v1">PDF</a> Accepted by CVPR 2024. Project page: <a href="https://npucvr.github.io/GaGS/">https://npucvr.github.io/GaGS/</a></p><p><strong>Summary</strong><br>三维几何感知变形高斯斑点投影，可实现动态视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>现有基于神经辐射场（NeRF）的解决方案以隐式方式学习变形，无法纳入 3D 场景几何。</li><li>因此，学习到的变形不一定具有几何相干性，这会导致动态视角合成和 3D 动态重建效果不理想。</li><li>3D 高斯斑点投影提供了 3D 场景的新表示，可以在此基础上利用 3D 几何来学习复杂的 3D 变形。</li><li>场景表示为 3D 高斯集合，其中每个 3D 高斯经过优化，可以在时间上移动和旋转以建模变形。</li><li>为了在变形过程中强制执行 3D 场景几何约束，我们显式提取 3D 几何特征并将其整合到学习 3D 变形中。</li><li>通过这种方式，我们的解决方案实现了 3D 几何感知变形建模，从而改进了动态视图合成和 3D 动态重建。</li><li>在合成和真实数据集上的广泛实验结果证明了我们解决方案的优越性，它取得了新的最先进的性能。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>论文标题：3D 几何感知的可变形高斯散布用于动态视图合成</li><li>作者：Minghao Chen, Yuxin Wen, Yufeng Zheng, Yong-Liang Yang</li><li>单位：无</li><li>关键词：动态视图合成、3D 几何感知、可变形高斯散布</li><li>论文链接：无，Github 代码链接：无</li><li>摘要：（1）研究背景：现有的基于神经辐射场 (NeRF) 的动态视图合成方法以隐式方式学习变形，无法融入 3D 场景几何。因此，学习到的变形在几何上不一定连贯，导致动态视图合成和 3D 动态重建效果不佳。（2）过去方法及其问题：3D 高斯散布提供了一种新的 3D 场景表示，在此基础上，可以在学习 3D 复杂变形时利用 3D 几何。然而，过去的方法缺乏对 3D 场景几何约束的显式建模，从而限制了变形建模的准确性和几何连贯性。（3）本文方法：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。该方法显式提取 3D 几何特征，并将其融入学习 3D 变形中，从而实现 3D 几何感知的变形建模。（4）方法性能：本文方法在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。</li></ol><p>7.Methods：(1): 提出一种3D几何感知的可变形高斯散布方法，用于动态视图合成；(2): 显式提取3D几何特征，并将其融入学习3D变形中，实现3D几何感知的变形建模；(3): 在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。</p><ol><li>结论：（1）：本文提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成，该方法显式提取 3D 几何特征并将其融入学习 3D 变形中，实现了 3D 几何感知的变形建模，在合成和真实数据集上的广泛实验结果证明了其优越性，达到了新的最先进性能。（2）：创新点：<ul><li>提出了一种 3D 几何感知的可变形高斯散布方法，用于动态视图合成。</li><li>显式提取 3D 几何特征，并将其融入学习 3D 变形中，实现 3D 几何感知的变形建模。</li><li>在合成和真实数据集上进行广泛实验，证明了该方法的优越性，达到了新的最先进性能。性能：</li><li>在合成和真实数据集上的广泛实验结果证明了该方法的优越性，达到了新的最先进性能。工作量：</li><li>该方法的实现相对复杂，需要较大的计算资源和较长的训练时间。</li></ul></li></ol><details>  <summary>点此查看论文截图</summary><img src="https://pica.zhimg.com/v2-afebc695ce43b9cca36774d451214003.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-75335d5dad43bcba0d01c7ed06e24b06.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-efd74ebc1d29a7da3e6ab58eae03277d.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-0f460737777a95c71a697250cf77dccd.jpg" align="middle"></details><h2 id="Hash3D-Training-free-Acceleration-for-3D-Generation"><a href="#Hash3D-Training-free-Acceleration-for-3D-Generation" class="headerlink" title="Hash3D: Training-free Acceleration for 3D Generation"></a>Hash3D: Training-free Acceleration for 3D Generation</h2><p><strong>Authors:Xingyi Yang, Xinchao Wang</strong></p><p>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model’s inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D’s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D’s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a>. </p><p><a href="http://arxiv.org/abs/2404.06091v1">PDF</a> <a href="https://adamdad.github.io/hash3D/">https://adamdad.github.io/hash3D/</a></p><p><strong>Summary</strong><br>使用Hash3D哈希算法加速3D生成建模，通过重用相邻时间步和相机视角中的特征图，从图像提取三维模型。</p><p><strong>Key Takeaways</strong></p><ul><li>使用 2D 扩散模型加速了 3D 生成建模。</li><li>Hash3D 是一种通用加速，无需模型训练即可加速 3D 生成。</li><li>Hash3D 利用渲染图像中相邻位置和时间步的特征图冗余。</li><li>通过哈希和重用相邻时间步和相机角度中的特征图，Hash3D 消除了冗余计算。</li><li>Hash3D 通过自适应网格哈希实现这一点。</li><li>特征共享机制不仅加快了生成速度，还增强了合成 3D 物体的平滑度和视图一致性。</li><li>Hash3D 可与 3D 高斯渲染相结合，从而极大地加快 3D 模型的创建速度。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p></p><ol><p></p><p></p><li>题目：Hash3D：无需训练的 3D 生成加速</li><p></p><p></p><li>作者：邢一阳，王新超</li><p></p><p></p><li>单位：新加坡国立大学</li><p></p><p></p><li>关键词：快速 3D 生成 · 分数蒸馏 · 采样</li><p></p><p></p><li>论文链接：https://arxiv.org/abs/2404.06091，Github 代码链接：None</li><p></p><p></p><li>摘要：(1)：随着 2D 扩散模型的采用，3D 生成建模取得了显著进展。尽管取得了这些进展，但繁琐的优化过程本身对效率构成了关键障碍。(2)：过去的方法：基于 2D 扩散模型的 3D 生成方法。问题：优化过程繁琐，效率低下。动机：利用特征图冗余来加速生成。(3)：本论文提出的研究方法：Hash3D，一种无需模型训练的通用 3D 生成加速方法。通过自适应网格哈希，有效地哈希和重用相邻时间步长和相机角度的特征图，从而大幅减少冗余计算，加速扩散模型在 3D 生成任务中的推理。(4)：本论文方法在任务和性能上的表现：在 5 个文本到 3D 和 3 个图像到 3D 模型上的实验表明，Hash3D 能够以 1.3~4 倍的效率加速优化。此外，Hash3D 与 3D 高斯 splatting 集成，极大地加快了 3D 模型的创建，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。这些性能支持了作者加速 3D 生成并提高效率的目标。</li><br>&lt;/ol&gt;<p></p><p><strong>Methods:</strong></p><p>(1): 自适应网格哈希：根据每个样本动态调整哈希网格大小，以匹配相邻特征图，提高匹配效率。</p><p>(2): 渐进式哈希：随着扩散过程的进行，逐步增加哈希概率，平衡匹配精度和计算成本。</p><p>(3): 特征哈希：直接哈希特征图，而不是噪声，以更有效地利用特征图冗余。</p><ol><li>结论：</li></ol><p>（1）本工作通过提出 Hash3D 加速器，为基于扩散的 3D 生成建模带来了以下重要意义：    - 无需模型训练，即插即用，有效提升 3D 生成效率。    - 结合 3D 高斯 splatting，大幅缩短文本到 3D 和图像到 3D 的处理时间，提升生成效率。</p><p>（2）文章的优缺点总结（按创新点、性能、工作量三个维度）：    - 创新点：        - 提出自适应网格哈希，根据样本动态调整哈希网格大小，提高匹配效率。        - 采用渐进式哈希，平衡匹配精度和计算成本。        - 直接哈希特征图，更有效地利用特征图冗余。    - 性能：        - 在 5 个文本到 3D 和 3 个图像到 3D 模型上，实验表明 Hash3D 能够以 1.3~4 倍的效率加速优化。        - 结合 3D 高斯 splatting，将文本到 3D 处理减少到约 10 分钟，将图像到 3D 转换减少到约 30 秒。    - 工作量：        - 算法实现相对简单，易于与现有的 3D 生成模型集成。        - 无需额外的模型训练，降低了时间和计算资源的消耗。</p><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-0f4a6a4161f3cb3fcdc5c4397500d100.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-d21f45ab3f4603885126438d366655c4.jpg" align="middle"></details></ol>]]></content>
    
    
    <summary type="html">3DGS 方向最新论文已更新，请持续关注 Update in 2024-04-14  GoMAvatar Efficient Animatable Human Modeling from Monocular Video   Using Gaussians-on-Mesh</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="3DGS" scheme="https://kedreamix.github.io/tags/3DGS/"/>
    
  </entry>
  
  <entry>
    <title>Talking Head Generation</title>
    <link href="https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/"/>
    <id>https://kedreamix.github.io/2024/04/14/Paper/2024-04-14/Talking%20Head%20Generation/</id>
    <published>2024-04-14T04:07:56.000Z</published>
    <updated>2024-04-14T04:07:56.535Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-04-14-更新"><a href="#2024-04-14-更新" class="headerlink" title="2024-04-14 更新"></a>2024-04-14 更新</h1><h2 id="Deepfake-Generation-and-Detection-A-Benchmark-and-Survey"><a href="#Deepfake-Generation-and-Detection-A-Benchmark-and-Survey" class="headerlink" title="Deepfake Generation and Detection: A Benchmark and Survey"></a>Deepfake Generation and Detection: A Benchmark and Survey</h2><p><strong>Authors:Gan Pei, Jiangning Zhang, Menghan Hu, Zhenyu Zhang, Chengjie Wang, Yunsheng Wu, Guangtao Zhai, Jian Yang, Chunhua Shen, Dacheng Tao</strong></p><p>In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a>. </p><p><a href="http://arxiv.org/abs/2403.17881v2">PDF</a> </p><p><strong>Summary</strong><br>深度伪造生成技术不断发展，相应检测技术需不断演进，规制深度伪造在隐私侵犯、网络钓鱼等领域的滥用。</p><p><strong>Key Takeaways</strong></p><ul><li>统一任务定义，全面介绍数据集和指标，讨论生成和检测技术框架发展。</li><li>探讨多个相关子领域的进展，重点研究四大主流深度伪造领域：换脸、人脸重现、说话人脸生成、面部属性编辑，以及对抗检测。</li><li>对每个领域的代表性方法在流行数据集上进行全面基准测试，充分评估顶级会议/期刊中发表的最新且有影响力的成果。</li><li>分析讨论领域挑战和未来研究方向。</li><li>紧跟 <a href="https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection">https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection</a> 中的最新进展。</li></ul><p><strong><a href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>标题：深度伪造生成与检测：基准与综述</li><li>作者：Gan Pei、Jiangning Zhang、Menghan Hu、Zhenyu Zhang、Chengjie Wang、Yunsheng Wu、Guangtao Zhai、Jian Yang、Chunhua Shen、Dacheng Tao</li><li>单位：华东师范大学多维信息处理上海市重点实验室</li><li>关键词：深度伪造生成，人脸替换，人脸重演，说话人脸生成，人脸属性编辑，外来检测，综述</li><li>链接：None</li><li>摘要：（1）研究背景：深度伪造技术能够生成高度逼真的面部图像和视频，在娱乐、电影制作、数字人物创作等领域具有广泛的应用前景。随着深度学习技术的进步，以变分自编码器和生成对抗网络为代表的技术取得了令人瞩目的生成效果。近年来，具有强大生成能力的扩散模型的出现，引发了新一轮的研究热潮。（2）过去方法及其问题：早期方法主要采用变分自编码器和生成对抗网络技术，能够生成看似逼真的图像，但其性能仍不尽如人意，限制了实际应用。本文的动机很充分，旨在通过综述深度伪造生成和检测的最新进展，总结和分析这一快速发展领域的当前技术水平。（3）研究方法：本文首先统一了任务定义，全面介绍了数据集和评价指标，并讨论了发展技术。然后，讨论了几个相关子领域的进展，重点研究了四个具有代表性的深度伪造领域：人脸替换、人脸重演、说话人脸生成和人脸属性编辑，以及外来检测。随后，对每个领域中流行数据集上的代表性方法进行了全面基准测试，全面评估了最新和有影响力的已发表作品。最后，分析了所讨论领域的挑战和未来研究方向。（4）任务和性能：本文在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，对每个领域中流行数据集上的代表性方法进行了评估。这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的高效性。这些性能支持了作者的目标，即提供深度伪造生成和检测领域的全面概述。</li></ol><p>7.方法：（1）：统一任务定义，全面介绍数据集和评价指标，讨论发展技术；（2）：讨论人脸替换、人脸重演、说话人脸生成和人脸属性编辑四个深度伪造领域进展；（3）：对每个领域代表性方法进行基准测试，评估最新发表作品；（4）：分析挑战和未来研究方向。</p><ol><li>结论：（1）本综述全面回顾了深度伪造生成和检测领域的最新进展，首次全面涵盖了相关领域，并讨论了扩散等最新技术。具体而言，本文涵盖了基本背景知识的概述，包括研究任务的概念、生成模型和神经网络的发展以及其他来自密切相关领域的信息。随后，我们总结了主流的四个生成和一个检测领域的不同方法采用的技术方法，并从技术角度对方法进行分类和讨论。此外，我们力求公平地组织和标注每个领域中的代表性方法。最后，我们总结了每个领域的当前挑战和未来的研究方向。（2）创新点：</li><li>全面覆盖深度伪造生成和检测领域，包括人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测。</li><li>统一任务定义，全面介绍数据集和评价指标，讨论发展技术。</li><li>对每个领域代表性方法进行基准测试，评估最新发表作品。</li><li>分析挑战和未来研究方向。</li><li>性能：</li><li>在人脸替换、人脸重演、说话人脸生成、人脸属性编辑和外来检测任务上进行了全面基准测试，评估了每个领域流行数据集上的代表性方法。</li><li>这些方法在各个任务上取得了最先进的性能，证明了它们在生成和检测深度伪造方面的有效性。</li><li>这些性能支持了作者的目标，即提供深度伪造生成和检测领域全面概述。</li><li>工作量：</li><li>大量的工作量，需要对深度伪造生成和检测领域的广泛文献进行全面审查和分析。</li><li>需要对相关技术，包括变分自编码器、生成对抗网络、扩散模型和外来检测方法进行深入理解。</li><li>需要仔细设计和执行基准测试，以公平评估不同方法的性能。</li></ol><details>  <summary>点此查看论文截图</summary><img src="https://picx.zhimg.com/v2-6aeb35b9b32deab9d1d23aa9b1eea276.jpg" align="middle"><img src="https://picx.zhimg.com/v2-b4cf83bab5fd31096f8d73dfc31c29e2.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-25e200804e3a12a1413b7bb204b5140d.jpg" align="middle"><img src="https://pica.zhimg.com/v2-f7724b1a6d114dcf338b21d91980680f.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-7308534a9cb3137f16881c6b4c39ae70.jpg" align="middle"><img src="https://pic1.zhimg.com/v2-37bc450ea15d85f35b70da807b592dbc.jpg" align="middle"></details>]]></content>
    
    
    <summary type="html">Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-04-14  Deepfake Generation and Detection A Benchmark and Survey</summary>
    
    
    
    <category term="Paper" scheme="https://kedreamix.github.io/categories/Paper/"/>
    
    
    <category term="Talking Head Generation" scheme="https://kedreamix.github.io/tags/Talking-Head-Generation/"/>
    
  </entry>
  
</feed>
